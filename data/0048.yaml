- en: Solving Reasoning Problems with LLMs in 2023
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决 2023 年大语言模型推理问题
- en: 原文：[https://towardsdatascience.com/solving-reasoning-problems-with-llms-in-2023-6643bdfd606d?source=collection_archive---------1-----------------------#2024-01-06](https://towardsdatascience.com/solving-reasoning-problems-with-llms-in-2023-6643bdfd606d?source=collection_archive---------1-----------------------#2024-01-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/solving-reasoning-problems-with-llms-in-2023-6643bdfd606d?source=collection_archive---------1-----------------------#2024-01-06](https://towardsdatascience.com/solving-reasoning-problems-with-llms-in-2023-6643bdfd606d?source=collection_archive---------1-----------------------#2024-01-06)
- en: '[](https://medium.com/@kiddozhu?source=post_page---byline--6643bdfd606d--------------------------------)[![Zhaocheng
    Zhu](../Images/80d09cfe902ca99c97fd6cfd6e387c2f.png)](https://medium.com/@kiddozhu?source=post_page---byline--6643bdfd606d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6643bdfd606d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6643bdfd606d--------------------------------)
    [Zhaocheng Zhu](https://medium.com/@kiddozhu?source=post_page---byline--6643bdfd606d--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@kiddozhu?source=post_page---byline--6643bdfd606d--------------------------------)[![Zhaocheng
    Zhu](../Images/80d09cfe902ca99c97fd6cfd6e387c2f.png)](https://medium.com/@kiddozhu?source=post_page---byline--6643bdfd606d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6643bdfd606d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6643bdfd606d--------------------------------)
    [Zhaocheng Zhu](https://medium.com/@kiddozhu?source=post_page---byline--6643bdfd606d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6643bdfd606d--------------------------------)
    ·17 min read·Jan 6, 2024
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6643bdfd606d--------------------------------)
    ·阅读时间17分钟·2024年1月6日
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: It’s the beginning of 2024 and ChatGPT just celebrated its one-year birthday.
    One year is a super long time for the community of large language models, where
    a myriad of interesting works have taken place. Let’s revisit the progress and
    discuss topics for the coming year.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是2024年初，ChatGPT刚刚庆祝了一周年。对于大语言模型社区来说，一年是非常漫长的时间，这一年里发生了无数有趣的工作。让我们回顾一下这一年的进展，并讨论未来一年的话题。
- en: '![](../Images/d47fb72b435abfef88d39b07d7a27d8c.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d47fb72b435abfef88d39b07d7a27d8c.png)'
- en: 'The School of Agents: LLMs are retrieving knowledge from textbooks and performing
    reasoning. Image by authors & DALL·E 3.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体学院：大语言模型正在从教科书中提取知识并进行推理。图像由作者和DALL·E 3生成。
- en: '*This post was co-authored with* [*Michael Galkin*](https://twitter.com/michael_galkin)
    *(Intel AI Lab),* [*Abulhair Saparov*](https://asaparov.org/) *(New York University),*
    [*Shibo Hao*](https://twitter.com/Ber18791531) *(UC San Diego) and* [*Yihong Chen*](https://twitter.com/yihong_thu)
    *(University College London & Meta AI Research). Many insights in this post were
    formed during the fruitful discussions with* [*Emily Xue*](https://www.linkedin.com/in/yuan-emily-xue-3483012/)
    *(Google),* [*Hanjun Dai*](https://twitter.com/hanjundai) *(Google DeepMind) and*
    [*Bruno Ribeiro*](https://twitter.com/brunofmr) *(Purdue University).*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*这篇文章由* [*Michael Galkin*](https://twitter.com/michael_galkin) *(英特尔AI实验室)，*
    [*Abulhair Saparov*](https://asaparov.org/) *(纽约大学)，* [*Shibo Hao*](https://twitter.com/Ber18791531)
    *(加州大学圣地亚哥分校) 和* [*Yihong Chen*](https://twitter.com/yihong_thu) *(伦敦大学学院与Meta
    AI研究) 合著。文章中的许多见解是在与* [*Emily Xue*](https://www.linkedin.com/in/yuan-emily-xue-3483012/)
    *(谷歌)，* [*Hanjun Dai*](https://twitter.com/hanjundai) *(谷歌DeepMind) 和* [*Bruno
    Ribeiro*](https://twitter.com/brunofmr) *(普渡大学) 的富有成果的讨论中形成的。*'
- en: Table of Contents
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目录
- en: '[Introduction](#2669)'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[简介](#2669)'
- en: '[Tool Use](#0b5d)'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[工具使用](#0b5d)'
- en: 1\. [In-context learning enables using more tools](#1acd)
  id: totrans-12
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 1\. [上下文学习使得使用更多工具成为可能](#1acd)
- en: '2\. [Most used tools: code interpreters and retrievers](#89ee)'
  id: totrans-13
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2\. [最常用的工具：代码解释器和检索工具](#89ee)
- en: 3\. [Let LLMs create their own tools](#9108)
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 3\. [让大语言模型创建自己的工具](#9108)
- en: '[Reasoning](#d8e6)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[推理](#d8e6)'
- en: 1\. [Planning](#d2e7)
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 1\. [规划](#d2e7)
- en: 2\. [Self series](#3582)
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2\. [自我系列](#3582)
- en: 3\. [Evaluations and observations](#0f22)
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 3\. [评估与观察](#0f22)
- en: '[What needs to be solved in 2024?](#7e91)'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2024年需要解决的问题](#7e91)'
- en: Introduction
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: '🔥 Large language models (LLMs) must be the hottest topic in 2023\. At the NeurIPS
    conference last month, the recurring topics in social events were: 1) what research
    are we doing with/for LLMs? 2) how can my research be integrated with LLMs? 3)
    what is the best strategy to shift from XXX to LLMs? 4) what research can we do
    as a [GPU-poor](https://www.semianalysis.com/p/google-gemini-eats-the-world-gemini)
    group? The reason is that everyone was informed about the groundbreaking news
    of LLMs on X, Discord, Slack, and everywhere else.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 🔥 大语言模型（LLMs）必定是2023年最热门的话题。在上个月的NeurIPS会议上，社交活动中反复讨论的话题是：1）我们正在做什么关于LLMs的研究？
    2）我的研究如何与LLMs结合？ 3）从XXX转向LLMs的最佳策略是什么？ 4）作为一个[GPU不足](https://www.semianalysis.com/p/google-gemini-eats-the-world-gemini)的团队，我们能做什么研究？原因是每个人都通过X、Discord、Slack和其他地方获得了LLMs的突破性新闻。
- en: 'If you take a look at the language model papers on arXiv, there is a leap from
    2,837 to 11,033 in 2023, which breaks the linear trend from 2019 to 2022\. Papers
    in the past year can be roughly clustered into 3 major categories: 1️⃣ pretraining
    and alignment; 2️⃣ tool use and reasoning; 3️⃣ systems and serving. As the title
    indicates, **this post will focus on the progress of LLM research on tool use
    and reasoning.** We picked ~20 👀 mind-blowing 👀 papers and summarized their insights
    and implications. By no means can this post be a comprehensive summary of all
    the achievements made by the community. Feel free to comment on any topics we
    missed.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看arXiv上的语言模型论文，2023年的数量从2,837篇跃升至11,033篇，这打破了2019到2022年间的线性趋势。过去一年中的论文大致可以分为三大类：1️⃣
    预训练和对齐；2️⃣ 工具使用和推理；3️⃣ 系统和服务。正如标题所示，**本文将重点介绍LLM研究在工具使用和推理方面的进展。** 我们挑选了大约20篇
    👀 令人震惊的 👀 论文，并总结了它们的见解和影响。这篇文章绝不是对社区所有成就的全面总结。如果我们遗漏了任何话题，请随时评论。
- en: '![](../Images/2b76e17e1ac1e676a1b1776c4af794e4.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b76e17e1ac1e676a1b1776c4af794e4.png)'
- en: Plot made by authors & ChatGPT.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 作者和ChatGPT绘制的图表。
- en: 'This post is composed of two topics: **tool use** and **reasoning**.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本文由两个主题组成：**工具使用**和**推理**。
- en: Tool use is more about how to solve reasoning problems by equipping LLMs with
    **external** tools, such as retrievers, search engines, and code interpreters.
    While tool use is not essential for building strong AI (see Yann’s classification
    below), tool use provides a practical solution to many applications when domain-specific
    tools are easily accessible.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工具使用更多的是关于如何通过为大语言模型（LLMs）配备**外部**工具来解决推理问题，例如检索器、搜索引擎和代码解释器。虽然工具使用对于构建强大的人工智能并非必需（见下文Yann的分类），但当领域特定工具易于获取时，工具使用为许多应用提供了实际的解决方案。
- en: By contrast, reasoning focuses on solving complex problems with the **internal**
    reasoning capacities of LLMs. Research on reasoning tries to figure out the limit
    of the capabilities that LLMs possess and approaches to push that limit.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相比之下，推理则集中在利用LLM的**内部**推理能力来解决复杂问题。推理研究试图找出LLMs所具备能力的极限，并探索突破这一极限的方法。
- en: There isn’t a strict dichotomy between the two topics, as we will see in the
    rest of this post.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这两者之间并没有严格的二分法，正如我们在本文接下来的部分中将看到的那样。
- en: Yann LeCun’s classification of retrieval and reasoning.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Yann LeCun对检索与推理的分类。
- en: Tool Use
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工具使用
- en: In-context learning enables using more tools
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上下文学习使得使用更多工具成为可能
- en: '**➡️** One limitation of LLM tool usage is the necessity of sufficient human
    annotations. Whenever we want to teach an LLM to use a tool, we need enough annotated
    tool calls to finetune the LLM. In the [Toolformer](https://arxiv.org/abs/2302.04761)
    paper by Meta, the authors use in-context learning to create a model that annotates
    tool calls for the input query. This model is then used to generate tool calls
    on an unllabeled dataset. While the generations may be far from perfect, incorrect
    calls can be filtered by executing the tools and filtering the outputs based on
    the ground truth answer. The correct calls are collected and used to finetune
    the model. In this way, we can teach Transformers to use any tool based on a conventional
    dataset and merely 5 additional annotations — easy work for any engineer.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**➡️** LLM工具使用的一个限制是需要足够的人类标注。每当我们想教一个LLM使用工具时，我们需要足够的标注工具调用来对LLM进行微调。在Meta的[Toolformer](https://arxiv.org/abs/2302.04761)论文中，作者使用上下文学习创建了一个模型，能够为输入查询标注工具调用。然后，使用该模型在未标记的数据集上生成工具调用。虽然生成的调用可能远非完美，但可以通过执行工具并根据真实答案过滤输出，筛选出错误的调用。正确的调用被收集并用于微调模型。通过这种方式，我们可以基于常规数据集和仅仅5个额外标注来教会Transformer使用任何工具——这是任何工程师都能轻松完成的工作。'
- en: '![](../Images/2413879f08a9fefbd39a955d6044af62.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2413879f08a9fefbd39a955d6044af62.png)'
- en: 'Automatic annotation of tool calls. Source: [Schick et al.](https://arxiv.org/abs/2302.04761)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 工具调用的自动注释。来源：[Schick et al.](https://arxiv.org/abs/2302.04761)
- en: ➡️ [Lu et al.](https://arxiv.org/abs/2304.09842) proposed **Chameleon** 🦎 to
    compose tools for multi-step reasoning. The core idea is to use an LLM to decompose
    the question into a sequence of tools, and then generate the arguments for each
    tool call. Both steps are implemented by few-shot prompts. Such an idea is reminiscent
    of [Neural Module Networks (NMNs)](https://arxiv.org/abs/1511.02799) from 2016,
    which decompose a question into sub tasks and learn a module for each sub task.
    The main obstacle of NMNs is that they are hard to train without annotations of
    the decompositions (see [this study](https://arxiv.org/abs/1811.12889)). Fortunately,
    this is not a problem in pretrained LLMs. With in-context learning, Chameleon
    can generate different compositions of tool calls to solve a problem. [A similar
    idea on visual reasoning](https://arxiv.org/abs/2211.11559) got the best paper
    award in CVPR this year.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ [Lu et al.](https://arxiv.org/abs/2304.09842) 提出了**Chameleon** 🦎，用于组合多步推理的工具。其核心思想是使用大型语言模型（LLM）将问题分解为一系列工具调用，然后为每个工具调用生成参数。这两个步骤都是通过少量示例提示来实现的。这个想法让人联想到2016年的[神经模块网络（NMNs）](https://arxiv.org/abs/1511.02799)，它将问题分解为子任务，并为每个子任务学习一个模块。NMNs的主要障碍是，它们在没有分解注释的情况下很难训练（见[这项研究](https://arxiv.org/abs/1811.12889)）。幸运的是，在预训练的LLM中，这不是问题。通过上下文学习，Chameleon可以生成不同的工具调用组合来解决问题。[一种类似的视觉推理方法](https://arxiv.org/abs/2211.11559)在今年的CVPR上获得了最佳论文奖。
- en: '![](../Images/be14f7112ad6ccee215eb810ecf2b688.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be14f7112ad6ccee215eb810ecf2b688.png)'
- en: 'Chameleon for multi-step tool use. Source: [Lu et al.](https://arxiv.org/abs/2304.09842)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Chameleon用于多步工具使用。来源：[Lu et al.](https://arxiv.org/abs/2304.09842)
- en: ➡️ While in-context learning is highly efficient compared to traditional methods,
    it faces certain limitations, such as difficulty in managing a large array of
    tools. Addressing this, [Hao et al.](https://arxiv.org/abs/2305.11554) introduced
    **ToolkenGPT**, which augments a frozen LLM with new token embeddings specifically
    for tools, termed “toolkens”. This technique was originally used in [multilingual
    language models](https://arxiv.org/abs/1910.11856) to accomodate a new language.
    ToolkenGPT allows tool calling during inference in the same way as next word token
    prediction. It demonstrates the capacity to handle over 200 tools while being
    cost-efficient, establishing a new effectiveness-efficiency trade-off compared
    to LoRA finetuning. Similar ideas are also integrated into multi-modal LLMs for
    [robotic actions](https://arxiv.org/abs/2307.15818) and [image generation](https://arxiv.org/abs/2309.11499).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 尽管与传统方法相比，上下文学习具有较高的效率，但它也面临某些限制，例如管理大量工具的难度。为了解决这个问题，[Hao et al.](https://arxiv.org/abs/2305.11554)
    提出了**ToolkenGPT**，它通过为工具引入新的令牌嵌入（称为“toolkens”）来增强一个冻结的LLM。该技术最初在[多语言模型](https://arxiv.org/abs/1910.11856)中用于适应新的语言。ToolkenGPT允许在推理过程中像下一个词预测一样进行工具调用。它展示了处理超过200个工具的能力，同时具有成本效益，相较于LoRA微调，建立了一种新的效能与效率的平衡。类似的思路也已整合到多模态LLM中，用于[机器人动作](https://arxiv.org/abs/2307.15818)和[图像生成](https://arxiv.org/abs/2309.11499)。
- en: '![](../Images/52ecf8ab7d5d6708483fb644e9601483.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52ecf8ab7d5d6708483fb644e9601483.png)'
- en: 'ToolkenGPT for massive tool use. Source: [Hao et al.](https://arxiv.org/abs/2305.11554)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ToolkenGPT用于大规模工具使用。来源：[Hao et al.](https://arxiv.org/abs/2305.11554)
- en: 'Most used tools: code interpreters and retrievers'
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最常用的工具：代码解释器和检索器
- en: If you ask us which tools are most generally applicable to reasoning tasks,
    we would say they are **code interpreters** and **retrievers**. Code interpreters
    are probably the most expressive environment that humans have invented for logic
    and computation. Retrievers are a good complement to the parametric knowledge
    of LLMs when a question or assumed knowledge is out of their training distribution.
    Let’s see how these tools can be used by LLMs.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你问我们哪些工具最广泛适用于推理任务，我们会说它们是**代码解释器**和**检索器**。代码解释器可能是人类发明的最具表现力的逻辑与计算环境。检索器在LLM的参数化知识无法覆盖某些问题或已知假设的情况下，是一种很好的补充。让我们来看看这些工具是如何被LLM使用的。
- en: ➡️ One common failure of [chain-of-thought (CoT)](https://arxiv.org/abs/2201.11903)
    is that LLMs fail to perform arithmetic operations. In [program-aided language
    modeling (PAL)](https://arxiv.org/abs/2211.10435) and [program-of-thoughts (PoT)](https://arxiv.org/abs/2211.12588)
    prompting, the authors prompt a code language model with programs to solve math
    problems. One may insert standard chain-of-thought texts as comments in the programs.
    The final answer is then generated by executing a Python interpreter. The insight
    behind these methods is that the code interpreter provides a perfect tool for
    all kinds of calculation, reducing failure cases to only incorrect reasoning.
    Code-style prompts are also commonly used in [planning tasks](https://arxiv.org/abs/2305.16653).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ [思维链（CoT）](https://arxiv.org/abs/2201.11903)的一个常见失败是LLM无法执行算术运算。在[程序辅助语言建模（PAL）](https://arxiv.org/abs/2211.10435)和[思维程序（PoT）](https://arxiv.org/abs/2211.12588)的提示中，作者通过程序提示代码语言模型来解决数学问题。可以在程序中插入标准的思维链文本作为注释。最终的答案是通过执行Python解释器生成的。这些方法背后的洞见是，代码解释器为各种计算提供了完美的工具，将失败案例减少到仅仅是推理错误。代码风格的提示也常用于[规划任务](https://arxiv.org/abs/2305.16653)中。
- en: '![](../Images/dd4f4150c2e2a642854d8791b55c4a09.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd4f4150c2e2a642854d8791b55c4a09.png)'
- en: 'Comparison between CoT and PAL. Source: [Gao and Madaan et al.](https://arxiv.org/abs/2211.10435)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: CoT和PAL的比较。来源：[Gao和Madaan等](https://arxiv.org/abs/2211.10435)
- en: ➡️ Retrievers are commonly used as preprocessing tools for LLMs to augment the
    question with relevant documents, often referred to as [retrieval-augmented generation
    (RAG)](https://arxiv.org/abs/2005.11401). However, when it comes to multi-step
    question answering, it is challenging to select the correct documents based on
    the question alone. In the **IRCoT** proposed by [Trivedi et al.](https://arxiv.org/abs/2212.10509),
    the authors interleave thought generation and knowledge retrieval. Whenever the
    LLM generates a thought sentence, IRCoT uses the sentence to retrieve documents
    from the corpus. The documents are prepended to the prompt to augment later generations.
    Even with a weak retriever like [BM25](https://en.wikipedia.org/wiki/Okapi_BM25),
    IRCoT outperforms one-step RAG on several open-domain question answering benchmarks.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 检索器通常作为LLM的预处理工具，用于通过相关文档增强问题，通常称为[检索增强生成（RAG）](https://arxiv.org/abs/2005.11401)。然而，当涉及到多步骤问题解答时，仅凭问题本身就很难选择正确的文档。在[Trivedi
    et al.](https://arxiv.org/abs/2212.10509)提出的**IRCoT**中，作者将思维生成与知识检索交替进行。每当LLM生成一个思维句子时，IRCoT就使用该句子从语料库中检索文档。这些文档被添加到提示中，以增强后续生成。即使是像[BM25](https://en.wikipedia.org/wiki/Okapi_BM25)这样较弱的检索器，IRCoT也在多个开放领域的问答基准测试中超越了一步RAG。
- en: '![](../Images/0e047cefb88b0636fb931ccbe6ca4277.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e047cefb88b0636fb931ccbe6ca4277.png)'
- en: 'IRCoT that interleaves CoT and knowledge retrieval. Source: [Trivedi et al.](https://arxiv.org/abs/2212.10509)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 交替使用CoT和知识检索的IRCoT。来源：[Trivedi等](https://arxiv.org/abs/2212.10509)
- en: ➡️ [Yang et al.](https://arxiv.org/abs/2306.15626) presented a novel usage of
    RAG for theorem proving. They built a gym-like environment **LeanDojo** 🏯 based
    on the proof assistant Lean. [Lean](https://en.wikipedia.org/wiki/Lean_(proof_assistant))
    is an interactive programming environment where its compiler can verify whether
    the written proof proved the goal. It also includes numerous proved theorems in
    its standard libraries, similar to STL in C++. The cool thing is that because
    proofs are constructed by decomposing theorems into known premises, theorem proving
    can benefit from RAG. Given a theorem, we retrieve the relevant premises from
    the standard libraries and then ask an LLM to generate a proof step. The authors
    show that RAG requires much less training resources and generalizes better to
    novel premises.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ [Yang等](https://arxiv.org/abs/2306.15626)提出了一种RAG在定理证明中的新颖应用。他们基于证明助手Lean构建了一个类似健身房的环境**LeanDojo**
    🏯。[Lean](https://en.wikipedia.org/wiki/Lean_(proof_assistant))是一个互动编程环境，其中的编译器可以验证所写的证明是否证明了目标。它还包含了许多在标准库中已证明的定理，类似于C++中的STL。很酷的一点是，由于证明是通过将定理分解为已知前提构造的，因此定理证明可以从RAG中受益。给定一个定理，我们从标准库中检索相关的前提，然后请求LLM生成一个证明步骤。作者表明，RAG需要的训练资源要少得多，并且在新颖的前提下具有更好的泛化能力。
- en: '![](../Images/c56af9f2fb5c435c163c807a4fa4ffd3.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c56af9f2fb5c435c163c807a4fa4ffd3.png)'
- en: 'Proof of a simple logical theorem in Lean. Source: [Xena Project](https://www.youtube.com/watch?v=POHVMMG7pqE)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在Lean中证明一个简单的逻辑定理。来源：[Xena项目](https://www.youtube.com/watch?v=POHVMMG7pqE)
- en: ➡️ Finally, [DSPy](https://github.com/stanfordnlp/dspy) by [Khattab et al.](https://arxiv.org/abs/2310.03714)
    presents a novel approach towards programming LLMs where the framework can actually
    improve the prompts over time and combine prompting techniques (CoT, PoT) with
    retrieval automatically. Further, DSPy introduces *teleprompters* for optimizing
    the prompts and bootstrapping new ones. It’s hard to fit a description of DSPy
    in one paragraph — it’s not your average RAG technique, but rather an evolution
    of it.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 最后，[DSPy](https://github.com/stanfordnlp/dspy)由[Khattab et al.](https://arxiv.org/abs/2310.03714)提出了一种新的LLM编程方法，在该框架下，系统能够随着时间的推移自动优化提示，并结合提示技术（CoT,
    PoT）与检索。进一步地，DSPy引入了*提词器*来优化提示并引导新的提示生成。很难用一段话来描述DSPy——它不是普通的RAG技术，而是其进化版本。
- en: Let LLMs create their own tools
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让LLM创建自己的工具
- en: 'Tool use has an inherent limitation: it relies on the presence of tools for
    a specific task. In nature, tool use is not an exclusive skill of humans, as many
    other animals can also use tools. However, what distinguishes humans from other
    animals is the ability to **create** tools. In 2023, we’ve seen a few preliminary
    works exploring the ability of tool making in LLMs.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 工具使用有一个固有的限制：它依赖于特定任务所需工具的存在。在自然界中，工具使用并非人类的专属技能，许多其他动物也能够使用工具。然而，区别人类与其他动物的，是**创造**工具的能力。2023年，我们看到了一些初步的研究，探索了LLM中工具制作的能力。
- en: ➡️ In LLMs as tool makers (**LATM**) proposed by [Cai et al.](https://arxiv.org/abs/2305.17126),
    the authors prompt an LLM to craft tools in the form of Python functions for a
    given task. The tools are then verified on a few samples, similar to how engineers
    solve problems on LeetCode. Once some tools pass the verification test, they are
    wrapped with documentation strings generated by an LLM to describe their usage.
    At test time, an LLM is prompted to dispatch the question to one of the tools
    at hand, and execute the tool based on the usage. LATM significantly outperforms
    CoT on a wide range of reasoning tasks in BigBench.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 在由[Cai et al.](https://arxiv.org/abs/2305.17126)提出的LLM作为工具制造者（**LATM**）中，作者提示LLM为给定任务编写Python函数形式的工具。这些工具随后在一些样本上进行验证，类似于工程师在LeetCode上解决问题的方式。一旦某些工具通过验证测试，它们会被LLM生成的文档字符串包装，以描述其使用方法。在测试时，LLM会被提示将问题发送到手头的某个工具，并根据其使用方法执行该工具。LATM在BigBench的广泛推理任务中，显著优于CoT。
- en: ➡️ [Voyager](https://arxiv.org/abs/2305.16291) brought the idea of tool making
    to the world of Minecraft, and came up with incredible results. The core idea
    of Voyager is to use an LLM to propose tasks based on existing skills and world
    state. Then the LLM is prompted to synthesize code (i.e. skills) to solve the
    tasks. The skills are refined based on environment feedback and mastered skills
    are committed to an external memory. Because new skills are built on top of existing
    skills, this significantly reduced the difficulty of learning a complex skill
    (e.g. building a diamond tool in Minecraft). While the idea of learning a library
    of skills can be traced back to [DreamCoder](https://arxiv.org/abs/2006.08381),
    Voyager demonstrates the superiority of GPT-4 in searching over skills in a challenging
    open-world game. Take a look at [the fancy demos](https://voyager.minedojo.org/)
    from the paper!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ [Voyager](https://arxiv.org/abs/2305.16291)将工具制作的概念引入了Minecraft的世界，并取得了惊人的结果。Voyager的核心思想是利用LLM根据现有的技能和世界状态提出任务。接着，LLM被提示合成代码（即技能）来解决这些任务。技能基于环境反馈进行精炼，掌握的技能被保存到外部记忆中。由于新技能是在现有技能的基础上构建的，这大大降低了学习复杂技能的难度（例如，在Minecraft中制作钻石工具）。尽管学习技能库的概念可以追溯到[DreamCoder](https://arxiv.org/abs/2006.08381)，Voyager展示了GPT-4在挑战性开放世界游戏中搜索技能的优势。请查看[论文中的精彩演示](https://voyager.minedojo.org/)！
- en: '![](../Images/3dc3a1462663cecf5c056fab3a88b4d8.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3dc3a1462663cecf5c056fab3a88b4d8.png)'
- en: 'Minecraft items and skills discovered over time. Source: [Wang et al.](https://arxiv.org/abs/2305.16291)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，Minecraft中的物品和技能被发现。来源：[Wang et al.](https://arxiv.org/abs/2305.16291)
- en: '➡️ Both of the above works craft tools as code. In fact, tools can be in natural
    language, too. (shameless self-promotion) In the hypothesis-to-theories (**HtT**)
    work from [Zhu et al.](https://arxiv.org/abs/2310.07064), the authors show that
    we can use LLMs to induce a library of textual rules from a standard multi-step
    reasoning training set. The insight is that among all the rules that LLMs produce
    for different samples, rules that occur and lead to correct answers more often
    are likely to be correct. We then collect the rules and prepend them to a standard
    CoT prompt to perform deduction and get the answer. One interesting aspect about
    HtT is that it can be viewed as a novel way of learning: instead of learning model
    parameters, we learn a library of rules, which works well with black-box LLMs.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 上述两篇工作都将工具设计为代码。事实上，工具也可以是自然语言。（不嫌弃的自我推销）在 [Zhu et al.](https://arxiv.org/abs/2310.07064)
    的假设到理论 (**HtT**) 工作中，作者展示了我们可以利用 LLM 从标准的多步骤推理训练集诱导出一套文本规则库。其洞察是，在 LLM 为不同样本生成的所有规则中，发生频率较高并且导致正确答案的规则可能是正确的。然后，我们收集这些规则并将其附加到标准
    CoT 提示中以进行推理并得出答案。HtT 的一个有趣方面是，它可以被看作是一种新的学习方式：我们不是学习模型参数，而是学习一套规则库，这与黑盒 LLM 配合得非常好。
- en: '![](../Images/e30b407b2550861646906f5e0ad43916.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e30b407b2550861646906f5e0ad43916.png)'
- en: 'HtT that learns textual rules for multi-hop reasoning. Source: [Zhu et al.](https://arxiv.org/abs/2310.07064)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: HtT 学习多跳推理的文本规则。来源：[Zhu et al.](https://arxiv.org/abs/2310.07064)
- en: Reasoning
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推理
- en: Planning
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 规划
- en: One drawback for CoT-style reasoning is that LLMs have to greedily decode a
    path towards an answer. This is problematic for complex problems like math questions
    or games, since it is hard to predict a path without trial-and-error. In 2023,
    the community made some progress on this issue with new frameworks that enable
    planning with LLMs.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: CoT 风格推理的一个缺点是，LLM 必须贪婪地解码一个通向答案的路径。对于复杂的问题，如数学题或游戏，这就成了一个问题，因为没有试错就很难预测出一条路径。2023
    年，社区在这个问题上取得了一些进展，推出了新的框架，使得 LLM 可以进行规划。
- en: '➡️ If we conceptualize CoT as “system 1” reasoning — characterized by its automatic,
    unconscious nature — then a question arises: Is it feasible to replicate the more
    conscious “system 2” reasoning of humans using LLMs? This query finds relevance
    in two methodologies: [reasoning-via-planning (RAP)](https://arxiv.org/abs/2305.14992)
    and [tree-of-thoughts (ToT)](https://arxiv.org/abs/2305.10601). Both empower LLMs
    to navigate through possible reasoning steps, and to search for the optimal reasoning
    chain based on specific evaluations. RAP additionally prompts an LLM as a “world
    model”, which predicts the next states following actions. This enables the LLM
    to operate within a self-simulated world, as opposed to interacting with an external
    environment. Both algorithms are available in the [LLM Reasoners](https://github.com/Ber666/llm-reasoners/)
    library now!'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 如果我们将 CoT 概念化为“系统 1”推理——其特点是自动的、无意识的特性——那么就会出现一个问题：是否可以用 LLM 来复制更具意识的“系统
    2”推理？这个问题在两种方法中得到了回应：[推理通过规划 (RAP)](https://arxiv.org/abs/2305.14992) 和 [思维树 (ToT)](https://arxiv.org/abs/2305.10601)。这两者都使
    LLM 能够通过可能的推理步骤进行推理，并根据特定的评估搜索最优推理链。RAP 还将 LLM 提示为一个“世界模型”，它预测在行动后的下一状态。这使得 LLM
    可以在一个自我模拟的世界中操作，而不是与外部环境互动。现在这两个算法都可以在 [LLM Reasoners](https://github.com/Ber666/llm-reasoners/)
    库中找到！
- en: '![](../Images/76d337122e2af90fdd867f1c7ab0e5a8.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/76d337122e2af90fdd867f1c7ab0e5a8.png)'
- en: 'RAP that repurposes LLMs as an agent and a world model. Source: [Hao et al.](https://arxiv.org/abs/2305.14992)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: RAP 将 LLM 重新设计为一个代理和世界模型。来源：[Hao et al.](https://arxiv.org/abs/2305.14992)
- en: Self series
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Self 系列
- en: Self series are a family of techniques that replace human efforts with LLM predictions
    in the loop of LLM development. The year of 2023 has witnessed quite a few papers
    on this track. Let’s take a closer look at some representative works.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Self 系列是一类技术，通过在 LLM 开发过程中用 LLM 预测替代人工努力。2023 年见证了这一领域的若干重要论文。让我们深入了解一些具有代表性的工作。
- en: ➡️ Many people have the experience that ChatGPT doesn’t provide the desired
    output on the first trial, and this sometimes can be fixed by pointing out its
    mistake. [Self-debugging](https://arxiv.org/abs/2304.05128) and [self-refinement](https://arxiv.org/abs/2303.17651)
    automate this procedure by replacing human feedback with machine feedback. The
    feedback either comes from a program executor or an LLM that compares the generation
    with the explanation of the problem. One key observation is that the performance
    of self-refine depends on the quality of the feedback, where stronger base models
    that provide better feedback benefit more. Such iterative refinement methods have
    also been shown to be super effective in [pose estimation](https://arxiv.org/abs/1507.06550)
    and [protein structure prediction](https://www.nature.com/articles/s41586-021-03819-2),
    where it is difficult to predict the structure in a single run.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 许多人都有过ChatGPT在第一次尝试时未能提供期望输出的经历，而这有时可以通过指出其错误来修复。[自我调试](https://arxiv.org/abs/2304.05128)和[自我完善](https://arxiv.org/abs/2303.17651)通过用机器反馈替代人类反馈来自动化这个过程。反馈来自于程序执行器或一个LLM，它比较生成的内容与问题的解释。一项关键的观察是，自我完善的表现取决于反馈的质量，提供更好反馈的强大基础模型能带来更大的帮助。这类迭代完善方法在[姿态估计](https://arxiv.org/abs/1507.06550)和[蛋白质结构预测](https://www.nature.com/articles/s41586-021-03819-2)中也被证明非常有效，因为在单次运行中很难预测结构。
- en: '![](../Images/2b3c4d3580bf9a786395a972b481534c.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b3c4d3580bf9a786395a972b481534c.png)'
- en: 'Illustration of Self-Debugging. Source: [Chen et al.](https://arxiv.org/abs/2304.05128)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 自我调试的示意图。来源：[Chen et al.](https://arxiv.org/abs/2304.05128)
- en: ➡️ In the memory-of-thought (**MoT**) framework from [Li and Qiu](https://arxiv.org/abs/2305.05181),
    the authors ask an LLM to generate CoT rationales on an unlabelled dataset and
    use them for RAG. You may ask how this can be useful given that the generated
    rationales often contain errors. The key trick is to filter the rationales based
    on majority vote or entropy minimization (a similar idea is used in [Wan et al.](https://arxiv.org/abs/2305.14106)
    to filter rationales). Once we have good rationales on the unlabelled dataset,
    we dynamically retrieve few-shot examples based on the test question, which is
    shown to be much better than fixed few-shot examples. MoT can be interpreted as
    converting a parametric model to a non-parametric model without additional supervision.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 在[Li and Qiu](https://arxiv.org/abs/2305.05181)提出的记忆-思维（**MoT**）框架中，作者要求LLM在未标注数据集上生成CoT推理，并将其用于RAG。你可能会问，考虑到生成的推理通常包含错误，这如何有用呢？关键的技巧是根据多数投票或熵最小化来过滤推理（[Wan
    et al.](https://arxiv.org/abs/2305.14106)中也使用了类似的思想来过滤推理）。一旦我们在未标注数据集上获得了良好的推理，我们会基于测试问题动态地检索少样本示例，这被证明比固定的少样本示例要好得多。MoT可以解释为将一个参数化模型转化为一个非参数化模型，而无需额外的监督。
- en: '![](../Images/07dcece1803feb415c68d2727bc2c037.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07dcece1803feb415c68d2727bc2c037.png)'
- en: 'MoT that generates and recalls memory. Source: [Li and Qiu](https://arxiv.org/abs/2305.05181).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 生成和回忆记忆的MoT。来源：[Li and Qiu](https://arxiv.org/abs/2305.05181)。
- en: ➡️ Going beyond MoT, [Yasunaga et al.](https://arxiv.org/abs/2310.01714) proposed
    **analogical prompting** that eliminates the need of dumping rationales on an
    unlabeled dataset. Analogical prompting asks an LLM to recall relevant exemplars
    based on the question, and thereby generates dynamic few-shot exemplars from scratch.
    In fact, the authors found that analogical prompting is an emergent ability in
    large language models, similar to previous works on [open-domain question answering](https://arxiv.org/abs/2209.10063).
    Larger-scale LLMs can self-generate better exemplars compared to standard RAG
    solutions. Besides, this work provides a cool trick to fuse multi-step generations
    into a single prompt with markdown grammar — a godsend for prompt engineers with
    a tight budget! 💡
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 超越MoT，[Yasunaga et al.](https://arxiv.org/abs/2310.01714)提出了**类比提示**，它消除了在未标注数据集上倾倒推理的需求。类比提示要求LLM根据问题回忆相关的示例，从而从头生成动态的少样本示例。事实上，作者发现类比提示是大语言模型的一个涌现能力，类似于之前在[开放领域问答](https://arxiv.org/abs/2209.10063)中的研究。更大规模的LLM可以自我生成比标准RAG解决方案更好的示例。此外，这项工作为将多步生成合并为一个单一提示提供了一个很酷的技巧，使用Markdown语法——这对于预算紧张的提示工程师来说是一个天赐之物！💡
- en: '![](../Images/cff6c1216d04791bb9d00343530b5700.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cff6c1216d04791bb9d00343530b5700.png)'
- en: 'Analogical prompting. Source: [Yasunaga et al.](https://arxiv.org/abs/2310.01714)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 类比提示。来源：[Yasunaga et al.](https://arxiv.org/abs/2310.01714)
- en: ➡️ Are self-refine and self-generate the limit of LLM reasoning? [Yang et al.](https://arxiv.org/abs/2309.03409)
    show a more advanced usage of the reasoning abilities of LLMs — to optimize a
    prompt based on the history of generated prompts. This is a cool reinvention of
    the famous meta-learning paper “[Learning to learn by gradient descent by gradient
    descent](https://arxiv.org/abs/1606.04474)”, but all the steps here are performed
    by LLMs on text. At each step, an LLM is prompted with previous solutions and
    corresponding performance metrics and tries to predict a new solution. Notably,
    even without telling the LLM how to perform optimization, the LLM can gradually
    find better solutions that maximize the metric. Maybe this work brings prompt
    engineers one step closer to unemployment?
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 自我优化和自我生成是LLM推理的极限吗？[Yang et al.](https://arxiv.org/abs/2309.03409)展示了LLM推理能力的更高级应用——基于生成的提示历史优化提示。这是对著名元学习论文《[通过梯度下降学习学习](https://arxiv.org/abs/1606.04474)》的一个酷炫重塑，但这里的所有步骤都是由LLM在文本上执行的。在每个步骤中，LLM都会接收之前的解决方案和相应的性能指标提示，并尝试预测一个新的解决方案。值得注意的是，即使没有告诉LLM如何进行优化，LLM也能逐步找到最大化指标的更好解决方案。也许这项工作让提示工程师离失业更近了一步？
- en: '![](../Images/2c7ff40222823804440ab08816082b7d.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2c7ff40222823804440ab08816082b7d.png)'
- en: 'Performance of prompts optimized by LLM. Source: [Yang et al.](https://arxiv.org/abs/2309.03409)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 由LLM优化的提示性能。来源：[Yang et al.](https://arxiv.org/abs/2309.03409)
- en: 🔁 Probably the most eye-opening 👀 work in self series is the self-taught optimizer
    (**STOP**) by [Zelikman et al.](https://arxiv.org/abs/2310.02304) We know LLMs
    are guided by textual prompts, take texts as input and output texts. While these
    these texts are usually separate variables, what will happen if we model them
    as a single variable? In STOP, the authors draw inspiration from [self-modifying
    code](https://en.wikipedia.org/wiki/Self-modifying_code) and use a self-improvement
    prompt to improve itself.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 🔁 可能是自学系列中最具启发性的👀工作是由[Zelikman et al.](https://arxiv.org/abs/2310.02304)提出的自我优化器(**STOP**)
    。我们知道LLM是通过文本提示来引导的，接受文本作为输入并输出文本。虽然这些文本通常是分开的变量，但如果我们将它们建模为单一变量，会发生什么呢？在STOP中，作者从[自我修改代码](https://en.wikipedia.org/wiki/Self-modifying_code)中汲取灵感，使用自我改进的提示来提升自身。
- en: '![](../Images/3da91a00d4241277814ce2cfc36e8dda.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3da91a00d4241277814ce2cfc36e8dda.png)'
- en: 'The seed improver that improves itself in STOP. Source: [Zelikman et al.](https://arxiv.org/abs/2310.02304)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在STOP中改进自身的种子改进器。来源：[Zelikman et al.](https://arxiv.org/abs/2310.02304)
- en: While the seed prompt isn’t more complicated than a random search algorithm,
    with a strong LLM, one can discover many advanced meta-heuristic algorithms. Interestingly,
    GPT-4 discovers many prompting strategies that are published after the training
    cutoff for GPT-4, including [ToT](https://arxiv.org/abs/2305.10601) and [Parsel](https://arxiv.org/abs/2212.10561).
    It seems that the day when LLMs conduct research for themselves is approaching.
    One step in this direction is a recent work by [Huang et al.](https://arxiv.org/abs/2310.03302)
    showing that LLMs are capable of designing ML models for common benchmarks and
    even Kaggle challenges.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然种子提示并不比随机搜索算法更复杂，但有了强大的LLM，人们可以发现许多先进的元启发式算法。有趣的是，GPT-4发现了许多在其训练截止日期之后发布的提示策略，包括[ToT](https://arxiv.org/abs/2305.10601)和[Parsel](https://arxiv.org/abs/2212.10561)。看起来，LLM为自己进行研究的那一天即将到来。朝着这个方向迈出的一步是[Huang
    et al.](https://arxiv.org/abs/2310.03302)的最新研究，表明LLM能够为常见基准测试甚至Kaggle挑战设计机器学习模型。
- en: '![](../Images/e030d2aa8360e5a443b744b6852437af.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e030d2aa8360e5a443b744b6852437af.png)'
- en: 'Algorithms found by STOP. Source: [Zelikman et al.](https://arxiv.org/abs/2310.02304)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: STOP发现的算法。来源：[Zelikman et al.](https://arxiv.org/abs/2310.02304)
- en: Evaluations and observations
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估与观察
- en: ➡️ [Kandpal et al.](https://arxiv.org/abs/2211.08411) conducted a systematic
    study on the memorization ability of LLMs. They asked an LLM about factual questions
    from Wikipedia and found that the accuracy is highly correlated with the frequency
    of questioned entities in the pretraining documents, regardless of the scale of
    the model. By extrapolating the trend, the authors estimate that a model with
    10¹⁸ is needed to match human performance on long-tail entities — which is way
    bigger than today’s LLMs. Hence an important takeaway is to use LLM reasoning
    for tasks related to frequent knowledge, and consider RAG or other tools for tasks
    related to long-tail knowledge.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ [Kandpal等人](https://arxiv.org/abs/2211.08411)对LLMs的记忆能力进行了系统研究。他们向LLM提出了来自维基百科的事实性问题，并发现准确性与预训练文档中被提问实体的频率高度相关，无论模型的规模如何。通过推测这一趋势，作者估计，需要一个拥有10¹⁸参数的模型才能在长尾实体上与人类表现相匹配——这远远超出了当前的LLMs。因此，一个重要的启示是，对于与频繁知识相关的任务，应使用LLM推理，而对于与长尾知识相关的任务，可以考虑使用RAG或其他工具。
- en: '![](../Images/d7500b22b9573287416b537dcd301dcb.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d7500b22b9573287416b537dcd301dcb.png)'
- en: 'LLMs can hardly memorize long-tail knowledge. Source: [Kandpal et al.](https://arxiv.org/abs/2211.08411)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs几乎无法记住长尾知识。来源：[Kandpal等人](https://arxiv.org/abs/2211.08411)
- en: ➡️ As the community tries to build bigger mixtures for training LLMs, one concern
    is that LLMs may not learn to actually reason but simply to memorize the solutions
    from the training distribution, just like humans in [teaching to the test](https://en.wikipedia.org/wiki/Teaching_to_the_test).
    [Wu et al.](https://arxiv.org/abs/2307.02477) answers this concern by comparing
    the performance of GPT-4 with zero-shot CoT on 11 different tasks, each with a
    default setting and a counterfactual setting. They observe that despite LLMs performing
    better than random in the counterfactual settings, their performance is consistently
    behind that in the default settings. It remains an open question how we can train
    models to focus more on reasoning rather than memorization.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 随着社区尝试构建更大规模的混合数据集来训练LLMs，一个担忧是LLMs可能无法真正学习推理，而只是简单地记住训练分布中的解决方案，就像人类在[应试教育](https://en.wikipedia.org/wiki/Teaching_to_the_test)中一样。[Wu等人](https://arxiv.org/abs/2307.02477)通过比较GPT-4在11个不同任务上的零样本CoT表现来回应这一担忧，每个任务都有默认设置和反事实设置。他们观察到，尽管LLMs在反事实设置中表现优于随机结果，但其表现始终低于默认设置。如何训练模型更多地关注推理而非记忆，仍然是一个悬而未决的问题。
- en: '![](../Images/03f7be28616d30d4236ececb57c2aff3.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03f7be28616d30d4236ececb57c2aff3.png)'
- en: 'GPT-4 underperforms on counterfactual variants. Source: [Wu et al.](https://arxiv.org/abs/2307.02477)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4在反事实变体上的表现不佳。来源：[Wu等人](https://arxiv.org/abs/2307.02477)
- en: ➡️ [Saparov et al.](https://arxiv.org/abs/2305.15269) extended a synthetic dataset
    [PrOntoQA](https://arxiv.org/abs/2210.01240) to OOD setting to test generalization
    ability of LLMs on deductive reasoning with controlled depth, width, compositional
    structure, etc. The authors found that CoT can generalize to compositional and
    longer proofs. This is in contrast with previous conclusions on [compositional
    semantic parsing](https://arxiv.org/abs/2205.12253), possibly because deductive
    reasoning only requires composing deduction steps, while semantic parsing additionally
    deals with growing outputs. While LLMs are able to use most deduction rules, they
    require explicit demonstrations of [*proof by cases*](https://en.wikipedia.org/wiki/Disjunction_elimination)
    and [*proof by contradiction*](https://en.wikipedia.org/wiki/Proof_by_contradiction).
    There are also counterintuitive qualitative differences between in-context learning
    and supervised learning.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ [Saparov等人](https://arxiv.org/abs/2305.15269)将一个合成数据集[PrOntoQA](https://arxiv.org/abs/2210.01240)扩展到OOD设置，以测试LLMs在控制深度、宽度、组合结构等方面的推理泛化能力。作者发现，CoT能够推广到组合性和更长的证明。这与之前关于[组合语义解析](https://arxiv.org/abs/2205.12253)的结论相反，可能是因为推理只需要组合推理步骤，而语义解析还需要处理不断增长的输出。虽然LLMs能够使用大多数推理规则，但它们需要显式演示[*分情况证明*](https://en.wikipedia.org/wiki/Disjunction_elimination)和[*反证法证明*](https://en.wikipedia.org/wiki/Proof_by_contradiction)。在上下文学习和监督学习之间也存在一些违反直觉的定性差异。
- en: '![](../Images/37163127677f52ac47a0d2999eb0ebd6.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/37163127677f52ac47a0d2999eb0ebd6.png)'
- en: 'OOD generalization over deductive reasoning. Source: [Saparov et al.](https://arxiv.org/abs/2305.15269)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 关于推理的OOD泛化能力。来源：[Saparov等人](https://arxiv.org/abs/2305.15269)
- en: ➡️ Regarding the parametric knowledge in LLMs, [Berglund et al.](https://arxiv.org/abs/2309.12288)
    found a phenomenon they called the *reversal curse*. That is, LLMs trained to
    memorize “A is B” do not know that “B is A” in closed-book question answering,
    despite the fact that they can be prompted to perform deductive reasoning. This
    indicates that LLMs lack certain kinds of symmetry in its parametric knowledge,
    and it is crucial to endow them with such symmetry to enable better generalization.
    Actually, the knowledge graph community has been a leader in this area, with works
    like [double permutation equivariance](https://arxiv.org/abs/2302.01313) and [relational
    rotation](https://arxiv.org/abs/1902.10197). It would be interesting to see how
    these ideas are adapted to LLMs.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 关于LLMs中的参数化知识，[Berglund等人](https://arxiv.org/abs/2309.12288)发现了一种他们称之为*逆转诅咒*的现象。也就是说，在闭卷问答中，被训练记住“
    A是B”的LLMs并不知道“B是A”，尽管它们可以被提示进行演绎推理。这表明LLMs在其参数化知识中缺乏某些对称性，而赋予它们这种对称性对于更好的泛化至关重要。实际上，知识图谱领域一直是这一领域的领先者，像[双重排列等变性](https://arxiv.org/abs/2302.01313)和[关系旋转](https://arxiv.org/abs/1902.10197)等工作便是例证。看看这些想法如何适应LLMs将会很有趣。
- en: What needs to be solved in 2024?
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2024年需要解决什么问题？
- en: 2023 has been an exciting year for tool use and reasoning, and we expect the
    new year to be more exciting. Let’s wrap up this post with predictions from the
    authors.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年对于工具使用和推理来说是令人兴奋的一年，我们预计新的一年将更加精彩。让我们通过作者的预测来总结这篇文章。
- en: 'Zhaocheng Zhu:'
  id: totrans-101
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Zhaocheng Zhu:'
- en: 1️⃣ Reasoning with LLMs still requires ad-hoc engineering efforts for each specific
    task. By contrast, once humans acquire the skills for a task, they can quickly
    adapt the skills to similar tasks with very few or even no samples (e.g. from
    chess to poker). If we can create LLM solutions that generalize across tasks,
    it will save a lot of engineering efforts and boost the performance in low-resource
    domains.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ 使用LLMs进行推理仍然需要针对每个特定任务进行临时的工程努力。相比之下，一旦人类掌握了某项任务的技能，他们可以迅速将这些技能适应到类似的任务上，几乎不需要样本，甚至不需要样本（例如，从国际象棋到扑克牌）。如果我们能够创建跨任务泛化的LLM解决方案，这将节省大量的工程努力，并提升在低资源领域的表现。
- en: 2️⃣ Solving reasoning problems usually involves a lot of commonsense knowledge,
    ranging from math, physics to strategies like enumeration and proof by contradiction,
    if any. While LLMs may have obtained such knowledge from their training data,
    we lack precise control over the parameteric knowledge in LLMs. We would like
    to see new studies on the knowledge representations of LLMs, and techniques that
    verbalize, inject or delete knowledge in LLMs.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 解决推理问题通常涉及大量的常识性知识，从数学、物理到像枚举法和反证法这样的策略，如果有的话。虽然LLMs可能从其训练数据中获得了这些知识，但我们对LLMs中参数化知识的精确控制仍然缺乏。我们希望看到关于LLMs知识表示的新研究，以及可以在LLMs中表达、注入或删除知识的技术。
- en: 'Michael Galkin:'
  id: totrans-104
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Michael Galkin:'
- en: 1️⃣ In 2023, we saw an increasing effort to understand the basic principles
    of *what can be learned* by Transformer-based LLMs — can we actually expect LLMs
    to be able to solve any arbitrary reasoning tasks? A few famous papers like [faith
    and fate](https://arxiv.org/abs/2305.18654) and [on length generalization](https://arxiv.org/abs/2310.16028)
    suggest that the autoregressive nature of LLMs might not be the optimal way to
    approach complex reasoning. In 2024, I’d expect more efforts on understanding
    the algorithmic alignment with LLMs.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ 在2023年，我们看到对*Transformer基础的LLMs可以学习什么*基本原理的理解力度不断加大——我们是否真的可以期望LLMs能够解决任何任意的推理任务？一些著名的论文，比如[信仰与命运](https://arxiv.org/abs/2305.18654)和[关于长度泛化](https://arxiv.org/abs/2310.16028)，表明LLMs的自回归特性可能不是处理复杂推理的最佳方式。在2024年，我预计会有更多的努力理解LLMs与算法的对齐。
- en: 2️⃣ It is likely that in 2024, most open and closed foundation models will be
    multi-modal supporting vision, text, audio, and other inputs. Incorporating other
    modalities into reasoning is the natural next step.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 很可能在2024年，大多数开放和闭合的基础模型将是多模态的，支持视觉、文本、音频等多种输入。将其他模态融入推理是自然的下一步。
- en: 'Abulhair Saparov:'
  id: totrans-107
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Abulhair Saparov:'
- en: 1️⃣ I anticipate there will be more efforts to find a more mechanistic understanding
    of reasoning in LLMs. What algorithms do they use when performing reasoning tasks?
    More precisely to what extent do they exploit shortcuts or heuristics that hurt
    robustness/generalization?
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ 我预期会有更多的努力去寻找LLMs推理机制的更深层次理解。在执行推理任务时，它们使用什么算法？更准确地说，它们在多大程度上利用了会影响鲁棒性/泛化能力的捷径或启发式方法？
- en: 2️⃣ Relatedly, I would expect researchers to make progress in answering whether
    increasing the scale of LLMs and/or their training will resolve their limitations
    in reasoning, or whether these limitations are fundamental, e.g. inherent to the
    architecture.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 与此相关，我预计研究人员将会取得进展，回答一个问题：是否增加 LLM 的规模和/或其训练将解决其在推理方面的局限性，或者这些局限性是否是根本性的，例如架构固有的问题。
- en: 'Shibo Hao:'
  id: totrans-110
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Shibo Hao:'
- en: 1️⃣ Over the past year, the primary focus of LLM reasoning research has been
    on prompting and supervised fine-tuning, with some approaches, like [STaR](https://arxiv.org/abs/2203.14465),
    [Reflexion](https://arxiv.org/abs/2303.11366), and [RAP](https://arxiv.org/abs/2305.14992),
    already drawing inspiration from RL. However, we have yet to witness a breakthrough
    method that effectively employs RL to enhance an LLM’s reasoning capabilities,
    especially when compared to the advancements in RLHF for alignment.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ 在过去的一年里，LLM 推理研究的主要焦点集中在提示和监督微调上，像 [STaR](https://arxiv.org/abs/2203.14465)、[Reflexion](https://arxiv.org/abs/2303.11366)
    和 [RAP](https://arxiv.org/abs/2305.14992) 等方法，已经开始从强化学习（RL）中汲取灵感。然而，我们尚未见证一种突破性的方法，能够有效地利用强化学习提升
    LLM 的推理能力，特别是与 RLHF 在对齐方面的进展相比。
- en: 2️⃣ On the flip side, in the future, language could become the primary medium
    of expression in RL systems. The key advantage lies in the rich information of
    language compared with the traditional scalar rewards / values. The prospect of
    an LLM agent that can autonomously improve its reasoning abilities with RL (no
    need for supervision data or prompting engineering), is not only exciting but
    may also indicate a significant leap towards AGI.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 另一方面，未来语言可能成为强化学习系统中的主要表达媒介。其关键优势在于与传统的标量奖励/价值相比，语言蕴含着丰富的信息。一个能够通过强化学习自主提高推理能力的
    LLM 代理（无需监督数据或提示工程），不仅令人兴奋，还可能意味着朝着通用人工智能（AGI）迈出了重要一步。
- en: 'Yihong Chen:'
  id: totrans-113
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Yihong Chen：
- en: 1️⃣ Structured & unstructured. I assume LLMs will be gradually eating the pies
    of traditional products, which are mostly based on large databases, rules and
    piles of small classifiers. In this case, what we are referring as “LLM reasoning”
    is probably the hope that we will have a method “X” that can bridge the structured
    world, which most product data is currently living in, and the unstructured world,
    which most LLMs are living in. Knowledge graphs kind of champion the structured
    world and there have been fruity research on how to reason well on a knowledge
    graph, while LLMs are championing the unstructured world though we are still unclear
    about how they do the reasoning. They have different advantages and limitations.
    I’d expect that a nice bridge between the two would lead us to more pragmatic
    solutions for products.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ 结构化与非结构化。我认为，大型语言模型（LLMs）将逐渐吞噬传统产品的市场份额，而这些传统产品大多依赖于大型数据库、规则和大量小型分类器。在这种情况下，我们所说的“LLM
    推理”可能指的是我们期望能够有一种方法“X”，它可以弥合结构化世界（目前大多数产品数据所处的世界）与非结构化世界（大多数 LLM 所处的世界）之间的鸿沟。知识图谱在某种程度上代表了结构化世界，关于如何在知识图谱上进行有效推理的研究也很多，而
    LLM 则代表了非结构化世界，尽管我们仍不清楚它们是如何进行推理的。它们各有优劣和局限性。我预计，结构化世界与非结构化世界之间的一个有效桥梁将能为产品提供更具务实性的解决方案。
- en: 2️⃣ Sample efficiency. As Zhaocheng mentioned, current LLMs reasoning is hard
    at generalising across a large number of tasks. Instead of ad-hoc efforts, which
    are usually customised for specific problems, I would be interested in if we can
    simply pretrain a LLM that generalize with less data, similar to what’s done for
    [generalizing across multiple languages](https://arxiv.org/abs/2307.01163).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 样本效率。正如 Zhaocheng 所提到的，目前 LLM 的推理在跨大量任务的泛化方面存在困难。我对是否能够简单地预训练一个能够用更少数据泛化的
    LLM 感兴趣，类似于 [跨多种语言的泛化](https://arxiv.org/abs/2307.01163)所做的那样。
- en: 3️⃣ Reasoning inside LLMs. As Abulhair and Michael mentioned, the community
    do not have a crystal understanding about how LLMs perform reasoning, if they
    indeed are reasoning. I’d expect more efforts on reverse-engineer LLMs’ reasoning
    process, either in a [mechanistic way](https://www.lesswrong.com/posts/jLAvJt8wuSFySN975/mechanistic-interpretability-quickstart-guide)
    or other interpretability approaches.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 3️⃣ LLM 内部的推理。正如 Abulhair 和 Michael 所提到的，社区目前尚未清楚了解 LLM 是如何进行推理的，甚至是否真正在进行推理。我预计，未来会有更多的努力，去逆向工程
    LLM 的推理过程，可能是通过 [机械性方法](https://www.lesswrong.com/posts/jLAvJt8wuSFySN975/mechanistic-interpretability-quickstart-guide)
    或其他可解释性方法。
- en: Meme Time
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Meme Time
- en: Following the tradition of [Michael Galkin](https://mgalkin.medium.com/), no
    blog post is truely complete without a meme. DALL·E 3 is almost a meme wizard
    if it can spell words correctly. Guess what prompts are used for each panel?
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 延续[Michael Galkin](https://mgalkin.medium.com/)的传统，任何博客文章都不算完整，没有一个表情包。DALL·E
    3简直是表情包魔法师，如果它能正确拼写单词的话。猜猜每个面板上用的是什么提示？
- en: '![](../Images/1c0de490cd0427c4127e390af9d93917.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c0de490cd0427c4127e390af9d93917.png)'
- en: What an LLM learned and what it can reason about. Image by authors & DALL·E
    3.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 一个LLM学到了什么，它能推理什么。图片来源：作者与DALL·E 3。
- en: Read More
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 阅读更多
- en: If this blog left you wanting to learn more about LLM reasoning, take a look
    at the following awesome blog posts.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这篇博客让你想了解更多关于LLM推理的内容，看看以下这些精彩的博客文章。
- en: '[Towards Complex Reasoning: the Polaris of Large Language Models](https://www.notion.so/c2b4a51355b44764975f88e6a42d4e75?pvs=21)
    by Yao Fu.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[面向复杂推理：大语言模型的北极星](https://www.notion.so/c2b4a51355b44764975f88e6a42d4e75?pvs=21)
    作者：姚福。'
- en: '[LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/)
    by Lilian Weng.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LLM驱动的自主智能体](https://lilianweng.github.io/posts/2023-06-23-agent/) 作者：Lilian
    Weng。'
