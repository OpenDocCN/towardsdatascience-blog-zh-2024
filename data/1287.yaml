- en: 'Reinforcement Learning: Deep Q-Networks'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习：深度 Q 网络
- en: 原文：[https://towardsdatascience.com/reinforcement-learning-from-scratch-deep-q-networks-0a8d33ce165b?source=collection_archive---------1-----------------------#2024-05-23](https://towardsdatascience.com/reinforcement-learning-from-scratch-deep-q-networks-0a8d33ce165b?source=collection_archive---------1-----------------------#2024-05-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/reinforcement-learning-from-scratch-deep-q-networks-0a8d33ce165b?source=collection_archive---------1-----------------------#2024-05-23](https://towardsdatascience.com/reinforcement-learning-from-scratch-deep-q-networks-0a8d33ce165b?source=collection_archive---------1-----------------------#2024-05-23)
- en: 'Teaching a shuttle to land on the moon using Deep Q-Networks in Python: A mathematical
    deep dive into Reinforcement Learning'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用深度 Q 网络在 Python 中教一艘航天飞机降落在月球上：强化学习的数学深度解析
- en: '[](https://medium.com/@cristianleo120?source=post_page---byline--0a8d33ce165b--------------------------------)[![Cristian
    Leo](../Images/99074292e7dfda50cf50a790b8deda79.png)](https://medium.com/@cristianleo120?source=post_page---byline--0a8d33ce165b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0a8d33ce165b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0a8d33ce165b--------------------------------)
    [Cristian Leo](https://medium.com/@cristianleo120?source=post_page---byline--0a8d33ce165b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@cristianleo120?source=post_page---byline--0a8d33ce165b--------------------------------)[![Cristian
    Leo](../Images/99074292e7dfda50cf50a790b8deda79.png)](https://medium.com/@cristianleo120?source=post_page---byline--0a8d33ce165b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0a8d33ce165b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0a8d33ce165b--------------------------------)
    [Cristian Leo](https://medium.com/@cristianleo120?source=post_page---byline--0a8d33ce165b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0a8d33ce165b--------------------------------)
    ·28 min read·May 23, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0a8d33ce165b--------------------------------)
    ·28分钟阅读·2024年5月23日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/38970589a005da7933deef0ef58bbb32.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38970589a005da7933deef0ef58bbb32.png)'
- en: Image generated by DALL-E
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由 DALL-E 生成的图像
- en: In reinforcement learning (RL), Q-learning is a foundational algorithm that
    helps an agent navigate its environment by learning a policy to maximize cumulative
    rewards. It does this by updating an action-value function, which estimates the
    expected utility of taking a specific action in a given state, based on received
    rewards and future estimations (this doesn’t sound familiar? Don’t worry as we
    will go over it later together).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习（RL）中，Q 学习是一个基础算法，帮助智能体通过学习一个策略来最大化累积奖励，从而在环境中导航。它通过更新一个行动-价值函数来实现这一点，该函数根据接收到的奖励和未来的估计，估算在给定状态下采取特定行动的预期效用（这听起来不熟悉吗？别担心，我们会在接下来的部分一起讲解）。
- en: However, traditional Q-learning has its challenges. It struggles with scalability
    as the state space grows and is less effective in environments with continuous
    state and action spaces. This is where Deep Q Networks (DQNs) come in. DQNs use
    neural networks to approximate the Q-values, enabling agents to handle larger
    and more complex environments effectively.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，传统的 Q 学习存在一些挑战。随着状态空间的增大，它在扩展性上遇到困难，并且在具有连续状态和动作空间的环境中效果较差。这就是深度 Q 网络（DQNs）发挥作用的地方。DQNs
    使用神经网络来逼近 Q 值，使得智能体能够有效地处理更大和更复杂的环境。
- en: In this article, we’ll dive into Deep Q Networks. We’ll explore how DQNs overcome
    the limitations of traditional Q-learning and discuss the key components that
    make up a DQN. We’ll also walk through implementing a DQN from scratch and applying
    it to a more complex environment. By the end of this article, you’ll have a solid
    understanding of…
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将深入探讨深度 Q 网络。我们将探讨 DQN 如何克服传统 Q 学习的局限性，并讨论构成 DQN 的关键组件。我们还将逐步讲解从头开始实现一个
    DQN，并将其应用于一个更复杂的环境。通过本文的学习，您将对…
