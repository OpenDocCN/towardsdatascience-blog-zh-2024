- en: Revolutionizing Large Dataset Feature Selection with Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用强化学习彻底改变大数据集的特征选择
- en: 原文：[https://towardsdatascience.com/reinforcement-learning-for-feature-selection-be1e7eeb0acc?source=collection_archive---------2-----------------------#2024-05-19](https://towardsdatascience.com/reinforcement-learning-for-feature-selection-be1e7eeb0acc?source=collection_archive---------2-----------------------#2024-05-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/reinforcement-learning-for-feature-selection-be1e7eeb0acc?source=collection_archive---------2-----------------------#2024-05-19](https://towardsdatascience.com/reinforcement-learning-for-feature-selection-be1e7eeb0acc?source=collection_archive---------2-----------------------#2024-05-19)
- en: Leverage the power of reinforcement learning for feature selection when faced
    with very large datasets
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在面对非常大的数据集时，利用强化学习的优势进行特征选择
- en: '[](https://medium.com/@baptistelefort?source=post_page---byline--be1e7eeb0acc--------------------------------)[![Baptiste
    Lefort](../Images/f57c4077afbff9939521032fa19b7f10.png)](https://medium.com/@baptistelefort?source=post_page---byline--be1e7eeb0acc--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--be1e7eeb0acc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--be1e7eeb0acc--------------------------------)
    [Baptiste Lefort](https://medium.com/@baptistelefort?source=post_page---byline--be1e7eeb0acc--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@baptistelefort?source=post_page---byline--be1e7eeb0acc--------------------------------)[![Baptiste
    Lefort](../Images/f57c4077afbff9939521032fa19b7f10.png)](https://medium.com/@baptistelefort?source=post_page---byline--be1e7eeb0acc--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--be1e7eeb0acc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--be1e7eeb0acc--------------------------------)
    [Baptiste Lefort](https://medium.com/@baptistelefort?source=post_page---byline--be1e7eeb0acc--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--be1e7eeb0acc--------------------------------)
    ·11 min read·May 19, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--be1e7eeb0acc--------------------------------)
    ·阅读时长11分钟·2024年5月19日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Discover how reinforcement learning transforms feature selection for machine
    learning models. Learn the process, implementation, and benefits of this innovative
    approach with practical examples and a dedicated Python library.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 了解强化学习如何改变机器学习模型的特征选择。通过实际示例和专用的Python库，学习这一创新方法的过程、实现及其优势。
- en: '![](../Images/743ea5c0887670d49bff360b95359e4c.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/743ea5c0887670d49bff360b95359e4c.png)'
- en: Photo from Jared Murray on [Unplash](https://unsplash.com/photos/NSuufgf-BME)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自Jared Murray，来源于[Unplash](https://unsplash.com/photos/NSuufgf-BME)
- en: '**Feature selection is a determining step in the process of building a machine
    learning model.** Selecting the good features for the model and the task that
    we want to achieve can definitely improve the performances. Indeed, a feature
    can add some noise and then disturb the model.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征选择是构建机器学习模型过程中的一个决定性步骤。** 为模型和我们想要实现的任务选择合适的特征，确实可以提升性能。事实上，某些特征可能会引入噪声，从而干扰模型。'
- en: Also, selecting the features is especially more important if we are dealing
    with high-dimensional data set. It enables the model to learn faster and better.
    The idea is then to find the optimal number of features and the most meaningful
    ones.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，选择特征对于处理高维数据集尤其重要。它能使模型更快、更好地学习。其核心思想是找到最优的特征数量和最具意义的特征。
- en: In this article I will tackle this problem and **go beyond by introducing a
    newly implemented method for feature selection**. Although it exists many different
    feature selection processes they will not be introduced here since a lot of articles
    are already dealing with them. I will focus on the feature selection using the
    reinforcement learning strategy.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将解决这个问题，并**通过引入一种新实现的特征选择方法，进一步探讨**。尽管存在许多不同的特征选择过程，但由于很多文章已经在讨论它们，因此在这里不做介绍。我将重点介绍使用强化学习策略进行特征选择。
- en: First, the reinforcement learning and more especially the Markov Decision Process
    will be addressed. It is a very new approach in the data science domain and more
    especially for feature selection purpose. After, I will introduce an implementation
    of this and how to install and use the python library (**FSRLearning**). Finally,
    I will prove the efficiency of this implementation. Among the possible feature
    selection approaches like wrappers or filtering, **the reinforcement learning
    is the most powerful and efficient**.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，将介绍强化学习，尤其是马尔可夫决策过程。这是数据科学领域中的一种全新方法，特别是在特征选择方面。接下来，我将介绍如何实现这一方法以及如何安装和使用Python库（**FSRLearning**）。最后，我将证明这种实现的有效性。在所有可能的特征选择方法中，如包装器或过滤器，**强化学习是最强大和高效的**。
- en: The goal of this article is to emphasise on the implementation for concret and
    real-problem oriented utilisation. The theoretical aspect of this problem will
    be simplified with examples although some references will be available at the
    end.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的目标是强调针对具体实际问题的应用实现。虽然一些参考资料将在文末提供，但本文会通过示例简化该问题的理论部分。
- en: 'Reinforcement Learning : The Markov Decision Problem for feature selection'
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习：特征选择的马尔可夫决策问题
- en: It has been demonstrated that reinforcement learning (RL) technics can be very
    efficient for problems like game solving. The concept of RL is based on Markovian
    Decision Process (MDP). The point here is not to define deeply the MDP but to
    get the general idea of how it works and how it can be useful to our problem.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 已经证明，强化学习（RL）技术在解决类似游戏问题等方面非常高效。RL的概念基于马尔可夫决策过程（MDP）。这里的重点不是深入定义MDP，而是理解它的基本工作原理，以及它如何为我们的实际问题提供帮助。
- en: 'The naive idea behind RL is that an agent starts in an unknown environnement.
    This agent has to take actions to complete a task. In function of the current
    state of the agent and the action he has selected previously, the agent will be
    more inclined to choose some actions. At every new state reached and action taken,
    the agent receives a reward. Here are then the main parameters that we need to
    define for feature selection purpose:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: RL背后的直觉是，智能体从一个未知的环境开始。这个智能体必须采取行动来完成任务。根据智能体当前的状态以及他先前选择的动作，智能体将更倾向于选择某些动作。在每一个新状态到达并采取动作时，智能体都会获得奖励。以下是我们需要为特征选择定义的主要参数：
- en: What is a state ?
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是状态？
- en: What is an action ?
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是动作？
- en: What are the rewards ?
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励是什么？
- en: How do we choose an action ?
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何选择动作？
- en: 'Firstly, the state is merely a subset of features that exist in the data set.
    For example, if the data set has three features (Age, Gender, Height) plus one
    label, here will be the possible states:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，状态只是数据集中的特征子集。例如，如果数据集有三个特征（年龄、性别、身高）加上一个标签，以下将是可能的状态：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In a state, the order of the features does not matter and it will be explained
    why a little bit later in the article. We have to consider it as a set and not
    a list of features.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个状态中，特征的顺序并不重要，稍后在本文中会解释为什么。我们必须将其视为一个集合，而非特征的列表。
- en: 'Concerning the actions, from a subset we can go to any other subset with one
    not-already explored feature more than the current state. In the feature selection
    problem, an action is then selecting a not-already explored feature in the current
    state and add it to the next state. Here is a sample of possible actions:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 关于动作，从一个子集出发，我们可以通过增加一个当前状态中未曾探索的特征，转向任何其他子集。在特征选择问题中，动作就是选择一个在当前状态中未曾探索的特征，并将其添加到下一个状态。以下是一些可能的动作示例：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here is an example of impossible actions:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一些不可能的动作示例：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We have defined the states and the actions but not the reward. The reward is
    a real number that is used for evaluating the quality of a state. For example
    if a robot is trying to reach the exit of a maze and decides to go to the exit
    as his next action, then the reward associated to this action will be “good”.
    If he selects as a next action to go in a trap then the reward will be “not good”.
    The reward is a value that brought information about the previous action taken.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已定义了状态和动作，但未定义奖励。奖励是一个实数，用于评估状态的质量。例如，如果一个机器人试图到达迷宫的出口，并决定将出口作为下一个动作，那么与该动作相关的奖励将是“好”。如果他选择进入一个陷阱作为下一个动作，则奖励将是“不好”。奖励是一个值，反映了先前采取的动作的信息。
- en: 'In the problem of feature selection an interesting reward could be a value
    of accuracy that is added to the model by adding a new feature. Here is an example
    of how the reward is computed:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征选择问题中，一个有趣的奖励可能是通过添加新特征使模型的准确性得到提高的值。下面是奖励计算的一个示例：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: For each state that we visit for the first time a classifier will be trained
    with the set of features. This value is stored in the state and the training of
    the classifier, which is very costly, will only happens once even if the state
    is reached again later. The classifier does not consider the order of the feature.
    This is why we can see this problem as a graph and not a tree. In this example,
    the reward of the action of selecting Gender as a new feature for the model is
    the difference between the accuracy of the current state and the next state.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个首次访问的状态，都会用特征集训练一个分类器。这个值会被存储在状态中，并且分类器的训练是非常昂贵的，即使该状态稍后再次被访问，也只会训练一次。分类器不考虑特征的顺序。这就是为什么我们可以将此问题视为图而不是树的原因。在这个示例中，选择性别作为新特征的动作的奖励是当前状态和下一个状态准确性的差值。
- en: '![](../Images/10e93339802fdda3ba8d0913277d4826.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10e93339802fdda3ba8d0913277d4826.png)'
- en: Each state has several possible actions and rewards associated (Image by the
    author)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 每个状态都有多个可能的动作和相关的奖励（图像来自作者）。
- en: On the graph above, each feature has been mapped to a number (i.e “Age” is 1,
    “Gender” is 2 and “Height” is 3). It is totally possible to take other metrics
    to maximise to find the optimal set. In many business applications the recall
    is more considered than the accuracy.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的图中，每个特征都被映射为一个数字（例如，“Age”是1，“Gender”是2，“Height”是3）。完全可以选择其他度量来最大化以找到最优的特征集。在许多商业应用中，更看重召回率而不是准确率。
- en: The next important question is how do we select the next state from the current
    state or how do we explore our environement. We have to find the most optimal
    way to do it since it can quickly become a very complex problem. Indeed, if we
    naively explore all the possible set of features in a problem with 10 features,
    the number of states would be
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个重要问题是，我们如何从当前状态选择下一个状态，或者说如何探索我们的环境。我们必须找到最优的方式来执行此操作，因为这很快会变成一个非常复杂的问题。实际上，如果我们天真地探索一个具有10个特征的问题中的所有可能特征集，那么状态的数量将是
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The +2 is because we consider an empty state and a state that contains all the
    possible features. In this problem we would have to train the same model on all
    the states to get the set of features that maximises the accuracy. In the RL approach
    we will not have to go in all the states and to train a model every time that
    we go in an already visited state.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: +2是因为我们考虑了一个空状态和一个包含所有可能特征的状态。在这个问题中，我们必须在所有状态上训练相同的模型，以获得最大化准确性的特征集。在强化学习方法中，我们不需要遍历所有状态，并且每次进入一个已访问过的状态时不需要重新训练模型。
- en: We had to determine some stop conditions for this problem and they will be detailed
    later. For now the epsilon-greedy state selection has been chosen. The idea is
    from a current state we select the next action randomly with a probability of
    epsilon (between 0 and 1 and often around 0.2) and otherwise select the action
    that maximises a function. For feature selection the function is the average of
    reward that each feature has brought to the accuracy of the model.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须为这个问题确定一些停止条件，这些条件会在后面详细说明。现在已经选择了epsilon-贪婪状态选择方法。这个方法的思路是，从当前状态出发，我们以epsilon的概率（介于0和1之间，通常在0.2左右）随机选择下一个动作，否则选择最大化某个函数的动作。对于特征选择，那个函数就是每个特征对模型准确性贡献的平均奖励。
- en: 'The epsilon-greedy algorithm implies two steps:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: epsilon-贪婪算法包含两个步骤：
- en: 'A random phase : with a probability epsilon, we select randomly the next state
    among the possible neighbours of the current state (we can imagine either a uniform
    or a softmax selection)'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机阶段：以epsilon的概率，我们在当前状态的所有可能邻居中随机选择下一个状态（我们可以假设使用均匀选择或softmax选择）。
- en: 'A greedy phase : we select the next state such that the feature added to the
    current state has the maximal contribution of accuracy to the model. To reduce
    the time complexity, we have initialised a list containing this values for each
    feature. This list is updated every time that a feature is chosen. The update
    is very optimal thanks to the following formula:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 贪婪阶段：我们选择下一个状态，使得添加到当前状态的特征对模型准确性的贡献最大。为了减少时间复杂度，我们已经初始化了一个包含每个特征对应值的列表。每次选择一个特征时，这个列表都会更新。感谢以下公式，更新过程非常优化：
- en: '![](../Images/fe8b18d529027bc03b6f4c32265f2db9.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fe8b18d529027bc03b6f4c32265f2db9.png)'
- en: Update of the average of reward list for each feature (Image by the author)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 更新每个特征的奖励平均值列表（图片由作者提供）
- en: '*AORf* : Average of reward brought by the feature “f”'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*AORf*：特征“f”带来的奖励的平均值'
- en: '*k* : number of times that “f” has been selected'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*k*：特征“f”被选择的次数'
- en: '*V(F)* : state’s value of the set of features F (not detailed in this article
    for clarity reasons)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*V(F)*：特征集 F 的状态值（出于清晰考虑，本文未详细说明）'
- en: The global idea is to find which feature has brought the most accuracy to the
    model. That is why we need to browse different states to evaluate in many different
    environments the most global accurate value of a feature for the model.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 全局思路是找出哪个特征为模型带来了最多的准确度。这就是为什么我们需要浏览不同的状态，在多种不同的环境中评估特征对模型的全局准确值。
- en: Finally I will detail the two stop conditions. Since the goal is to minimise
    the number of state that the algorithm visits we need to be careful about them.
    The less never visited state we visit, the less amount of models we will have
    to train with different set of features. Training the model to get the accuracy
    is the most costly phase in terms of time and computation power.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我将详细说明两种停止条件。由于目标是最小化算法访问的状态数，我们需要对它们保持谨慎。我们访问的未访问过的状态越少，使用不同特征集训练模型的次数就越少。训练模型以获取准确度是时间和计算能力上最为耗费的阶段。
- en: The algorithm stops in any case in the final state which is the set containing
    all the features. We want to avoid reaching this state since it is the most expensive
    to train a model with.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 算法在最终状态停止，该状态是包含所有特征的集合。我们希望避免到达此状态，因为训练模型时最为昂贵。
- en: Also, it stops browsing the graph if a sequence of visited states see their
    values degrade successively. A threshold has been set such that after square root
    of the number of total features in the dataset, it stops exploring.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，如果一系列访问过的状态的值逐渐降低，算法将停止浏览图形。已设定一个阈值，当数据集中所有特征的总数的平方根后，算法停止探索。
- en: Now that the modelling of the problem has been explained, we will detail the
    implementation in python.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在问题的建模已经解释清楚，我们将详细说明 Python 中的实现。
- en: The python library for Feature Selection with Reinforcement Learning
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征选择的强化学习 Python 库
- en: A python library resolving this problem is available. I will explain in this
    part how it works and prove that it is an efficient strategy. Also, this article
    stands as a documentation and you will be able to use this library for your projects
    by the end of the part.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个解决此问题的 Python 库可用。我将在本部分中解释它是如何工作的，并证明它是一种高效的策略。此外，本文还作为文档，你将在本部分结束时能够将此库用于你的项目。
- en: 1\. The data pre-processing
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 数据预处理
- en: Since we need to evaluate the accuracy of a state that is visited, we need to
    feed a model with the features and the data used for this feature selection task.
    The data has to been normalised, the categorical variables encoded and have as
    few rows as possible (the smallest it is, the fastest the algorithm will be).
    Also, it’s very important to create a mapping between the features and some integers
    as explained in the previous part. This step is not mandatory but very recommended.
    The final result of this step is to get a DataFrame with all the features and
    another with the labels to predict. Below is an example with a dataset used as
    a benchmark (it can be found here [UCI Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/statlog+(australian+credit+approval))).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们需要评估访问过的状态的准确性，因此需要将特征和数据输入模型，用于此特征选择任务。数据必须进行标准化，类别变量需要编码，并且行数应尽可能少（越小越快）。此外，创建特征与整数之间的映射非常重要，正如前面部分所述。此步骤不是强制性的，但非常推荐。该步骤的最终结果是得到一个包含所有特征的
    DataFrame 和一个包含要预测标签的 DataFrame。以下是使用一个基准数据集的示例（可以在这里找到 [UCI Irvine 机器学习库](https://archive.ics.uci.edu/ml/datasets/statlog+(australian+credit+approval))）。
- en: Process the data
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 处理数据
- en: 2\. Installation and importation of the FSRLearning library
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 安装和导入 FSRLearning 库
- en: 'The second step is to install the library with pip. Here is the command to
    install it:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是通过 pip 安装库。以下是安装命令：
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To import the library the following code can be used:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 导入库时，可以使用以下代码：
- en: Import the library
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 导入库
- en: You will be able to create a feature selector simply by creating an object Feature_Selector_RL.
    Some parameters need to be filled in.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 你将能够通过创建一个 Feature_Selector_RL 对象来创建一个特征选择器。需要填写一些参数。
- en: '***feature_number*** (integer) : number of features in the DataFrame X'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***feature_number***（整数）：数据框架 X 中的特征数量'
- en: '***feature_structure*** (dictionary) : dictionary for the graph implementation'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***feature_structure***（字典）：图形实现的字典'
- en: '***eps*** (float [0; 1]) : probability of choosing a random next state, 0 is
    an only greedy algorithm and 1 only random'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***eps***（浮动 [0; 1]）：选择随机下一个状态的概率，0表示完全贪婪算法，1表示完全随机'
- en: '***alpha*** (float [0; 1]): control the rate of updates, 0 is a very not updating
    state and 1 a very updated'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***alpha***（浮动 [0; 1]）：控制更新速率，0表示几乎不更新，1表示完全更新'
- en: '***gamma*** (float [0, 1]): factor of moderation of the observation of the
    next state, 0 is a shortsighted condition and 1 it exhibits farsighted behavior'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***gamma***（浮动 [0, 1]）：观察下一个状态的调节因子，0表示目光短浅的情况，1则表现出远见行为'
- en: '***nb_iter*** (int): number of sequences to go through the graph'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***nb_iter***（整数）：遍历图形的序列数量'
- en: '***starting_state*** (“empty” or “random”) : if “empty”, the algorithm starts
    from the empty state and if “random”, the algorithm starts from a random state
    in the graph'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***starting_state***（“empty” 或 “random”）：如果为“empty”，算法从空状态开始；如果为“random”，算法从图中的随机状态开始'
- en: All the parameters can be tuned but for most of the problem only few iterations
    can be good (around 100) and the epsilon value around 0.2 is often enough. The
    starting state is useful to browse the graph more efficiently but it can be very
    dependent on the dataset and both of the values can be tested.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 所有参数都可以调节，但对于大多数问题来说，只需要少量迭代就能得到良好的结果（大约 100 次），而 epsilon 值通常设为 0.2 就足够了。起始状态有助于更高效地浏览图形，但它可能非常依赖数据集，两个值都可以进行测试。
- en: 'Finally we can initialise very simply the selector with the following code:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过以下代码非常简单地初始化选择器：
- en: Selector object initialisation
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 选择器对象初始化
- en: 'Training the algorithm is very easy on the same basis than most of the ML library:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数机器学习库的基础上，训练算法非常简单：
- en: 'Here is an example of the output:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出的一个示例：
- en: '![](../Images/6ca11e2f53ddc0befd561002a7b034fe.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6ca11e2f53ddc0befd561002a7b034fe.png)'
- en: Output of the selector (Image by the author)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 选择器的输出（图片由作者提供）
- en: 'The output is a 5-tuple as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是一个五元组，如下所示：
- en: Index of the features in the DataFrame X (like a mapping)
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据框架 X 中特征的索引（类似映射）
- en: Number or times that the feature has been observed
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征观察的次数
- en: Average of reward brought by the feature after all the iterations
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在所有迭代后，特征带来的奖励的平均值
- en: Ranking of the features from the least to the most important (here 2 is the
    least and 7 the most important feature)
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征从最不重要到最重要的排名（这里 2 是最不重要，7 是最重要的特征）
- en: Number of states globally visited
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全局访问的状态数量
- en: Another important method of this selector is to get a comparison with the RFE
    selector of Scikit-Learn. It takes as input X, y and the results of the selector.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 此选择器的另一个重要方法是与 Scikit-Learn 的 RFE 选择器进行比较。它的输入为 X、y 和选择器的结果。
- en: 'The output is a print after each step of selection of the global metric of
    RFE and FSRLearning. It also outputs a visual comparison of the accuracy of the
    model with on the x-axis the number of features selected and on the y-axis the
    accuracy. The two horizontal lines are the median of accuracy for each method.
    Here is an example:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是在每一步选择后，打印 RFE 和 FSRLearning 的全局指标。它还输出模型准确性的视觉比较，x 轴为选定特征的数量，y 轴为准确性。两条水平线分别为每种方法的准确性中位数。以下是一个示例：
- en: '![](../Images/b74a30d601f80f8b0a2970d80f8808a9.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b74a30d601f80f8b0a2970d80f8808a9.png)'
- en: Comparison between RL and RFE method (Image by the author)
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: RL 方法与 RFE 方法的比较（图片由作者提供）
- en: '[PRE6]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In this example the RL method has always given a better set of features for
    the model than RFE. We can then select with certainty among the sorted set of
    features any subset and it will give a better accuracy to the model. We can run
    several times the model and the comparator to get a very accurate estimation but
    the RL method is always better.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，RL 方法总是为模型提供比 RFE 更好的特征集。我们可以在排序后的特征集中选择任何子集，它将为模型提供更好的准确性。我们可以多次运行模型和比较器以获得非常准确的估计，但
    RL 方法总是更好。
- en: Another interesting method is the get_plot_ratio_exploration. It plots a graph
    comparing the number of already visited nodes and visited nodes in a sequence
    for a precise iteration.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的方法是 `get_plot_ratio_exploration`。它绘制了一个图表，比较已访问节点与在精确迭代中访问的节点数。
- en: '![](../Images/eaa6224563da601d84a3e70911b0e8f0.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eaa6224563da601d84a3e70911b0e8f0.png)'
- en: Comparison of visited and not visited state at each iteration (Image by the
    author)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 每次迭代中访问与未访问状态的比较（图片来自作者）
- en: Also, thanks to the second stop condition the time complexity of the algorithm
    decreases exponentially. Then even if the number of feature is big the convergence
    will be found quickly. The plot bellow is the number of times that a set of a
    certain size has been visited.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于第二个停止条件，算法的时间复杂度呈指数级下降。因此，即使特征数量很大，收敛也会迅速找到。下面的图展示了某一大小的特征集被访问的次数。
- en: '![](../Images/313c55b8da35476e077b1f932fa3529c.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/313c55b8da35476e077b1f932fa3529c.png)'
- en: Number of visited states in function of their size (Image by the author)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 根据状态大小绘制的访问状态数量（图片来自作者）
- en: In all iterations the algorithm visited a state containing 6 variables or less.
    Beyond 6 variables we can see that the number of state reached is decreasing.
    It’s a good behaviour since it is faster to train a model with small set of features
    than the big ones.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有迭代中，算法访问的状态包含不超过 6 个变量。超过 6 个变量时，我们可以看到访问的状态数量在减少。这是一个良好的行为，因为使用较小特征集训练模型比使用大特征集更快速。
- en: Conclusion and references
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论与参考文献
- en: Overall, we can see that the RL method is very efficient for maximising a metric
    of a model. It always converges quickly toward an interesting subset of features.
    Also, this method is very easy and fast to implement in ML projects with the FSRLearning
    library.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们可以看到强化学习方法对于最大化模型的度量非常高效。它总是快速收敛到一个有趣的特征子集。此外，这种方法在 ML 项目中非常容易且快速实现，使用
    FSRLearning 库即可。
- en: The github repository of the project with a complete documentation is available
    [here](https://github.com/blefo/FSRLearning).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目的 GitHub 仓库及完整文档可在 [这里](https://github.com/blefo/FSRLearning)获取。
- en: If you wish to contact me, you can do so directly on linkedin [here](https://www.linkedin.com/in/baptistelefort/)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望与我联系，可以直接在 linkedin [这里](https://www.linkedin.com/in/baptistelefort/)找到我。
- en: 'This library has been implemented with the help of these two articles:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 本库的实现得到了以下两篇文章的帮助：
- en: Sali Rasoul, Sodiq Adewole and Alphonse Akakpo, [FEATURE SELECTION USING REINFORCEMENT
    LEARNING](https://arxiv.org/pdf/2101.09460.pdf) (2021), ArXiv
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sali Rasoul, Sodiq Adewole 和 Alphonse Akakpo，[使用强化学习进行特征选择](https://arxiv.org/pdf/2101.09460.pdf)（2021），ArXiv
- en: Seyed Mehdin Hazrati Fard, Ali Hamzeh and Sattar Hashemi, [Using reinforcement
    learning to find an optimal set of features](https://www.sciencedirect.com/science/article/pii/S0898122113004495)
    (2013), ScienceDirect
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seyed Mehdin Hazrati Fard, Ali Hamzeh 和 Sattar Hashemi，[使用强化学习寻找最优特征集](https://www.sciencedirect.com/science/article/pii/S0898122113004495)（2013），ScienceDirect
