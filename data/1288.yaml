- en: 'Small Language Models: Using 3.8B Phi-3 and 8B Llama-3 Models on a PC and Raspberry
    Pi'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 小型语言模型：在 PC 和树莓派上使用 38 亿 Phi-3 和 80 亿 Llama-3 模型
- en: 原文：[https://towardsdatascience.com/small-language-models-using-3-8b-phi-3-and-8b-llama-3-models-on-a-pc-and-raspberry-pi-9ed70127fe61?source=collection_archive---------2-----------------------#2024-05-23](https://towardsdatascience.com/small-language-models-using-3-8b-phi-3-and-8b-llama-3-models-on-a-pc-and-raspberry-pi-9ed70127fe61?source=collection_archive---------2-----------------------#2024-05-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/small-language-models-using-3-8b-phi-3-and-8b-llama-3-models-on-a-pc-and-raspberry-pi-9ed70127fe61?source=collection_archive---------2-----------------------#2024-05-23](https://towardsdatascience.com/small-language-models-using-3-8b-phi-3-and-8b-llama-3-models-on-a-pc-and-raspberry-pi-9ed70127fe61?source=collection_archive---------2-----------------------#2024-05-23)
- en: Testing the models with LlamaCpp and ONNX
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 LlamaCpp 和 ONNX 测试模型
- en: '[](https://dmitryelj.medium.com/?source=post_page---byline--9ed70127fe61--------------------------------)[![Dmitrii
    Eliuseev](../Images/7c48f0c016930ead59ddb785eaf3e0e6.png)](https://dmitryelj.medium.com/?source=post_page---byline--9ed70127fe61--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9ed70127fe61--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9ed70127fe61--------------------------------)
    [Dmitrii Eliuseev](https://dmitryelj.medium.com/?source=post_page---byline--9ed70127fe61--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://dmitryelj.medium.com/?source=post_page---byline--9ed70127fe61--------------------------------)[![Dmitrii
    Eliuseev](../Images/7c48f0c016930ead59ddb785eaf3e0e6.png)](https://dmitryelj.medium.com/?source=post_page---byline--9ed70127fe61--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9ed70127fe61--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9ed70127fe61--------------------------------)
    [Dmitrii Eliuseev](https://dmitryelj.medium.com/?source=post_page---byline--9ed70127fe61--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9ed70127fe61--------------------------------)
    ·17 min read·May 23, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9ed70127fe61--------------------------------)
    ·阅读时间 17 分钟·2024 年 5 月 23 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/e1c080c33ba6ea79ec97849e1d394374.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1c080c33ba6ea79ec97849e1d394374.png)'
- en: Image by [Jelleke Vanooteghem](https://unsplash.com/@ilumire), Unsplash
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [Jelleke Vanooteghem](https://unsplash.com/@ilumire)，Unsplash
- en: Nowadays, we can observe an interesting twist in developing new AI models. For
    a long time, it has been known that bigger models are “smarter” and capable of
    doing more complex things. But they are also more computationally expensive. Big
    device manufacturers like Microsoft, Google, and Samsung have already started
    to promote new AI features to their clients, but it is clear that if millions
    of users massively use AI on their phones or laptops, the computational cloud
    costs could be enormous. What is the solution? The obvious way is to run a model
    on-device, which has advantages in latency (no network connection is required,
    and the model can be accessed immediately), privacy (no need to process user responses
    in the cloud), and, naturally, computation costs. Using local AI models is important
    not only for laptops and smartphones but also for autonomous robots, smart home
    assistants, and other edge devices.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，我们可以观察到在开发新的 AI 模型方面出现了有趣的变化。长期以来，人们一直知道较大的模型更“智能”，能够完成更复杂的任务。但它们的计算成本也更高。像微软、谷歌和三星这样的巨大设备制造商已经开始向客户推广新的
    AI 功能，但显然，如果数百万用户在他们的手机或笔记本电脑上大规模使用 AI，计算云的成本可能会非常庞大。解决方案是什么？显而易见的方法是将模型运行在设备上，这样可以在延迟（无需网络连接，模型可以立即访问）、隐私（无需在云中处理用户响应）以及自然的计算成本方面带来优势。使用本地
    AI 模型不仅对笔记本电脑和智能手机非常重要，对于自主机器人、智能家居助手和其他边缘设备也同样重要。
- en: 'At the time of making this article, at least two models were announced that
    were specially designed for on-device running:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，至少已有两个专门为设备端运行设计的模型宣布：
- en: Google’s [Gemini Nano](https://deepmind.google/technologies/gemini/nano/). The
    model was announced in December 2023; it has [two versions](https://en.wikipedia.org/wiki/Gemini_(language_model))
    with 1.8B and 3.25B parameters. According to the [developer.android.com](https://developer.android.com/ai/aicore)…
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谷歌的 [Gemini Nano](https://deepmind.google/technologies/gemini/nano/)。该模型于 2023
    年 12 月发布；它有 [两个版本](https://en.wikipedia.org/wiki/Gemini_(language_model))，分别为
    18 亿和 32.5 亿参数。根据 [developer.android.com](https://developer.android.com/ai/aicore)...
