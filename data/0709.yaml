- en: Using Self-Organizing Map To Bolster Retrieval-Augmented Generation In Large
    Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è‡ªç»„ç»‡æ˜ å°„å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/using-self-organizing-map-to-bolster-retrieval-augmented-generation-in-large-language-models-5d739ce21e9c?source=collection_archive---------3-----------------------#2024-03-16](https://towardsdatascience.com/using-self-organizing-map-to-bolster-retrieval-augmented-generation-in-large-language-models-5d739ce21e9c?source=collection_archive---------3-----------------------#2024-03-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/using-self-organizing-map-to-bolster-retrieval-augmented-generation-in-large-language-models-5d739ce21e9c?source=collection_archive---------3-----------------------#2024-03-16](https://towardsdatascience.com/using-self-organizing-map-to-bolster-retrieval-augmented-generation-in-large-language-models-5d739ce21e9c?source=collection_archive---------3-----------------------#2024-03-16)
- en: '*SOM is proposed to bolster efficient retrieval of LLM context for RAGâ€¦*'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*SOMè¢«æè®®ç”¨æ¥å¢å¼ºLLMä¸Šä¸‹æ–‡çš„é«˜æ•ˆæ£€ç´¢ï¼Œä»¥æ”¯æŒRAGâ€¦â€¦*'
- en: '[](https://murali-kashaboina.medium.com/?source=post_page---byline--5d739ce21e9c--------------------------------)[![Murali
    Kashaboina](../Images/ff1118f3c317dab87fe4b625a614fb93.png)](https://murali-kashaboina.medium.com/?source=post_page---byline--5d739ce21e9c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5d739ce21e9c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5d739ce21e9c--------------------------------)
    [Murali Kashaboina](https://murali-kashaboina.medium.com/?source=post_page---byline--5d739ce21e9c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://murali-kashaboina.medium.com/?source=post_page---byline--5d739ce21e9c--------------------------------)[![Murali
    Kashaboina](../Images/ff1118f3c317dab87fe4b625a614fb93.png)](https://murali-kashaboina.medium.com/?source=post_page---byline--5d739ce21e9c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5d739ce21e9c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5d739ce21e9c--------------------------------)
    [Murali Kashaboina](https://murali-kashaboina.medium.com/?source=post_page---byline--5d739ce21e9c--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5d739ce21e9c--------------------------------)
    Â·17 min readÂ·Mar 16, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5d739ce21e9c--------------------------------)
    Â·17åˆ†é’Ÿé˜…è¯»Â·2024å¹´3æœˆ16æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/77f2ba7f54a07f0a727c5ac6340c1a4e.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77f2ba7f54a07f0a727c5ac6340c1a4e.png)'
- en: Photo by [Werclive ğŸ‘¹](https://unsplash.com/@werclive?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼š[Werclive ğŸ‘¹](https://unsplash.com/@werclive?utm_source=medium&utm_medium=referral)æ¥è‡ª[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Background
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: èƒŒæ™¯
- en: 'Large volumes of data are used to train Large Language Models (LLM) containing
    millions and billions of model parameters with the goal of text generation, such
    as text completion, text summarization, language translations, and answering questions.
    While LLMs develop a knowledge base per se from the training data sources, there
    is always a cut-off training date post which LLM will not know any newly generated
    data. For example, the cut-off date for training OpenAIâ€™s GPT-3.5-turbo-instruct
    LLM is September 2021 (Ref: [https://platform.openai.com/docs/models/gpt-3-5-turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo)),
    and as such, GPT-3.5-turbo-instruct LLM may not answer questions on 2022, 2023,
    or 2024 events accurately. Such data not part of the LLMâ€™s original training data
    is called external data. Retrieval-Augmented Generation (RAG) is a technique meant
    to help in such cases by retrieving appropriate information contextual to the
    input prompt from authorized external sources and augmenting input so that LLM
    can generate accurate and relevant responses. Effectively, RAG forms the gateway
    between the LLM and the external data. Such augmentation eliminates the need to
    retrain or further fine-tune the LLM model.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§é‡æ•°æ®è¢«ç”¨äºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œè¿™äº›æ¨¡å‹åŒ…å«æ•°ç™¾ä¸‡ã€æ•°åäº¿ä¸ªå‚æ•°ï¼Œç›®çš„æ˜¯è¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼Œå¦‚æ–‡æœ¬è¡¥å…¨ã€æ–‡æœ¬æ‘˜è¦ã€è¯­è¨€ç¿»è¯‘å’Œé—®é¢˜å›ç­”ã€‚è™½ç„¶LLMä»è®­ç»ƒæ•°æ®ä¸­è‡ªç„¶è€Œç„¶åœ°å»ºç«‹äº†ä¸€ä¸ªçŸ¥è¯†åº“ï¼Œä½†è®­ç»ƒæ•°æ®æœ‰ä¸€ä¸ªæˆªæ­¢æ—¥æœŸï¼Œæˆªæ­¢æ—¥æœŸä¹‹åï¼ŒLLMå°†æ— æ³•äº†è§£ä»»ä½•æ–°ç”Ÿæˆçš„æ•°æ®ã€‚ä¾‹å¦‚ï¼ŒOpenAIçš„GPT-3.5-turbo-instruct
    LLMçš„è®­ç»ƒæˆªæ­¢æ—¥æœŸæ˜¯2021å¹´9æœˆï¼ˆå‚è€ƒï¼š[https://platform.openai.com/docs/models/gpt-3-5-turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo)ï¼‰ï¼Œå› æ­¤ï¼ŒGPT-3.5-turbo-instruct
    LLMå¯èƒ½æ— æ³•å‡†ç¡®å›ç­”2022å¹´ã€2023å¹´æˆ–2024å¹´çš„äº‹ä»¶ã€‚è¿™ç±»ä¸å±äºLLMåŸå§‹è®­ç»ƒæ•°æ®çš„æ•°æ®ç§°ä¸ºå¤–éƒ¨æ•°æ®ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ˜¯ä¸€ç§æŠ€æœ¯ï¼Œæ—¨åœ¨é€šè¿‡ä»æˆæƒçš„å¤–éƒ¨æ¥æºæ£€ç´¢ä¸è¾“å…¥æç¤ºç›¸å…³çš„é€‚å½“ä¿¡æ¯ï¼Œå¹¶å¢å¼ºè¾“å…¥ï¼Œä»¥ä¾¿LLMèƒ½å¤Ÿç”Ÿæˆå‡†ç¡®ä¸”ç›¸å…³çš„å“åº”ã€‚å®é™…ä¸Šï¼ŒRAGæ„æˆäº†LLMä¸å¤–éƒ¨æ•°æ®ä¹‹é—´çš„æ¡¥æ¢ã€‚è¿™ç§å¢å¼ºé¿å…äº†é‡æ–°è®­ç»ƒæˆ–è¿›ä¸€æ­¥å¾®è°ƒLLMæ¨¡å‹çš„éœ€è¦ã€‚
- en: LLMâ€™s Typical M.O.
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLMçš„å…¸å‹æ“ä½œæ¨¡å¼
- en: 'LLMs are auto-regressive, generating a new token based on the input prompt
    tokenized into a sequence of tokens. The generation of the next best token is
    probability-based and can be expressed as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: LLMæ˜¯è‡ªå›å½’çš„ï¼Œæ ¹æ®è¾“å…¥æç¤ºå°†tokenåŒ–çš„åºåˆ—ç”Ÿæˆä¸€ä¸ªæ–°çš„tokenã€‚ä¸‹ä¸€ä¸ªæœ€ä½³tokençš„ç”Ÿæˆæ˜¯åŸºäºæ¦‚ç‡çš„ï¼Œå¯ä»¥è¡¨è¾¾ä¸ºä»¥ä¸‹å½¢å¼ï¼š
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Essentially, the probability of the newly generated nth token, Yn, is conditioned
    on the probability of the occurrence of the sequence of n-1 previous tokens X
    and the learned model parameters Î¸. It should be noted here that the tokenized
    input sequence X plays a crucial role in generating the next token. In addition,
    self-attention mechanisms complement effective auto-regression, where each input
    token in the sequence computes its representation by attending to and weighing
    the importance of other tokens in the sequence. Such intricate relationships and
    dependencies among the tokens in the sequence also enable the LLM to decipher
    the most probable next-best token that â€˜gels wellâ€™ with the tokens in the input
    sequence. The LLM appends the new token to the previous tokens to form a new input
    sequence and repeats the auto-regressive process until a completion condition
    is met, such as reaching the maximum token count.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬è´¨ä¸Šï¼Œæ–°ç”Ÿæˆçš„ç¬¬nä¸ªtoken Ynçš„æ¦‚ç‡æ˜¯åŸºäºn-1ä¸ªå‰åºtokens Xä»¥åŠå­¦ä¹ åˆ°çš„æ¨¡å‹å‚æ•°Î¸çš„æ¦‚ç‡ã€‚è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒtokenåŒ–çš„è¾“å…¥åºåˆ—Xåœ¨ç”Ÿæˆä¸‹ä¸€ä¸ªtokenæ—¶èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚æ­¤å¤–ï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶æœ‰æ•ˆåœ°è¡¥å……äº†è‡ªå›å½’ï¼Œåœ¨è¿™ç§æœºåˆ¶ä¸‹ï¼Œåºåˆ—ä¸­çš„æ¯ä¸ªè¾“å…¥tokené€šè¿‡å…³æ³¨å¹¶åŠ æƒåºåˆ—ä¸­å…¶ä»–tokençš„é‡è¦æ€§æ¥è®¡ç®—å…¶è¡¨ç¤ºã€‚è¿™ç§tokenä¹‹é—´å¤æ‚çš„å…³ç³»å’Œä¾èµ–æ€§ä½¿å¾—LLMèƒ½å¤Ÿè§£ç å‡ºä¸è¾“å…¥åºåˆ—ä¸­çš„tokensâ€œå¥‘åˆâ€çš„ä¸‹ä¸€ä¸ªæœ€ä½³tokenã€‚LLMå°†æ–°ç”Ÿæˆçš„tokené™„åŠ åˆ°ä¹‹å‰çš„tokensä¸Šï¼Œå½¢æˆæ–°çš„è¾“å…¥åºåˆ—ï¼Œå¹¶é‡å¤è‡ªå›å½’è¿‡ç¨‹ï¼Œç›´åˆ°æ»¡è¶³ç»“æŸæ¡ä»¶ï¼Œä¾‹å¦‚è¾¾åˆ°æœ€å¤§tokenæ•°é‡ã€‚
- en: Such a self-attention-driven auto-regression implies that the LLM relies predominantly
    on the input sequence to generate the next best token. As long as the input sequence
    helps determine the next-best token through self-attention, the LLM continues
    in a â€˜virtuousâ€™ loop, generating coherent, comprehensible, and relevant outputs.
    On the contrary, the LLM will start relying on the model parameters if the prompt
    inputs do not help determine the next best token. In such a case, the model may
    succeed in generating the next best token if the model has been trained to contain
    sufficient â€˜knowledgeâ€™ contextual to the input prompt. Conversely, the model may
    go into a â€˜viciousâ€™ loop, generating non-coherent, incomprehensible, and possibly
    irrelevant outputs if the prompt inputs pertain to â€˜external dataâ€™ that the LLM
    has never been trained on.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§è‡ªæ³¨æ„åŠ›é©±åŠ¨çš„è‡ªå›å½’æ¨¡å‹æ„å‘³ç€ï¼ŒLLMä¸»è¦ä¾èµ–äºè¾“å…¥åºåˆ—æ¥ç”Ÿæˆä¸‹ä¸€ä¸ªæœ€ä½³çš„tokenã€‚åªè¦è¾“å…¥åºåˆ—é€šè¿‡è‡ªæ³¨æ„åŠ›å¸®åŠ©ç¡®å®šä¸‹ä¸€ä¸ªæœ€ä½³tokenï¼ŒLLMå°±ä¼šç»§ç»­åœ¨ä¸€ä¸ªâ€œè‰¯æ€§â€å¾ªç¯ä¸­ç”Ÿæˆè¿è´¯ã€æ˜“ç†è§£ä¸”ç›¸å…³çš„è¾“å‡ºã€‚ç›¸åï¼Œå¦‚æœæç¤ºè¾“å…¥æ— æ³•å¸®åŠ©ç¡®å®šä¸‹ä¸€ä¸ªæœ€ä½³tokenï¼ŒLLMå°†å¼€å§‹ä¾èµ–æ¨¡å‹å‚æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¦‚æœæ¨¡å‹å·²ç»ç»è¿‡è¶³å¤Ÿçš„è®­ç»ƒï¼Œèƒ½å¤ŸåŒ…å«ä¸è¾“å…¥æç¤ºç›¸å…³çš„â€˜çŸ¥è¯†â€™ï¼Œå®ƒå¯èƒ½æˆåŠŸç”Ÿæˆä¸‹ä¸€ä¸ªæœ€ä½³tokenã€‚ç›¸åï¼Œå¦‚æœæç¤ºè¾“å…¥æ¶‰åŠLLMä»æœªè®­ç»ƒè¿‡çš„â€˜å¤–éƒ¨æ•°æ®â€™ï¼Œæ¨¡å‹å¯èƒ½ä¼šè¿›å…¥ä¸€ä¸ªâ€˜æ¶æ€§â€™å¾ªç¯ï¼Œç”Ÿæˆä¸è¿è´¯ã€éš¾ä»¥ç†è§£ä¸”å¯èƒ½æ— å…³çš„è¾“å‡ºã€‚
- en: Various techniques tackle this issue. Prompt engineering is one of them, where
    the goal is to address the â€˜missing contextâ€™ by adjusting the prompt to enhance
    the context so that the LLM can generate relevant output. RAG is another technique
    where the goal is to specifically address the â€˜missing context due to external
    dataâ€™ by retrieving the most appropriate information contextual to the input prompt
    from external data sources in an automated manner and augmenting the prompt.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å¤šç§æŠ€æœ¯æ¥åº”å¯¹è¿™ä¸ªé—®é¢˜ã€‚æç¤ºå·¥ç¨‹å°±æ˜¯å…¶ä¸­ä¹‹ä¸€ï¼Œå…¶ç›®æ ‡æ˜¯é€šè¿‡è°ƒæ•´æç¤ºæ¥è§£å†³â€˜ç¼ºå¤±çš„ä¸Šä¸‹æ–‡â€™ï¼Œä»¥å¢å¼ºä¸Šä¸‹æ–‡ï¼Œä½¿å¾—LLMå¯ä»¥ç”Ÿæˆç›¸å…³çš„è¾“å‡ºã€‚RAGæ˜¯å¦ä¸€ç§æŠ€æœ¯ï¼Œå…¶ç›®æ ‡æ˜¯é€šè¿‡ä»å¤–éƒ¨æ•°æ®æºè‡ªåŠ¨æ£€ç´¢ä¸è¾“å…¥æç¤ºç›¸å…³çš„æœ€åˆé€‚ä¿¡æ¯ï¼Œå¹¶å¢å¼ºæç¤ºï¼Œä¸“é—¨è§£å†³ç”±äºâ€˜å¤–éƒ¨æ•°æ®â€™ç¼ºå¤±çš„ä¸Šä¸‹æ–‡ã€‚
- en: RAGâ€™s Challenge
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAGçš„æŒ‘æˆ˜
- en: 'The primary responsibility of RAG is to search and retrieve data that is contextually
    related to the input prompt from external data sources such as informational databases,
    APIs, and other document repositories like Wikipedia. A simple keyword search
    would not cut it. Instead, RAG requires a semantic search. To facilitate semantic
    search, the textual information retrieved from external sources is transformed
    into numerical representations or vectors, commonly called text embeddings, and
    stored in vector databases. There are various models or algorithms for creating
    these embeddings from text. The prompt is first transformed into its vector representation
    to search and retrieve closest matching external data vectors. Vector similarities
    (or vector distances) are then computed between the prompt vector and the previously
    stored external data vectors. The most similar or nearest vectors are sorted and
    filtered using a threshold, and their corresponding textual information is retrieved
    to augment the promptâ€™s context. The following conceptual diagram captures the
    typical interactions between different components for enabling RAG:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: RAGçš„ä¸»è¦èŒè´£æ˜¯ä»å¤–éƒ¨æ•°æ®æºï¼ˆå¦‚ä¿¡æ¯æ•°æ®åº“ã€APIå’Œå…¶ä»–æ–‡æ¡£åº“ï¼Œå¦‚ç»´åŸºç™¾ç§‘ï¼‰ä¸­æœç´¢å¹¶æ£€ç´¢ä¸è¾“å…¥æç¤ºç›¸å…³çš„ä¸Šä¸‹æ–‡æ•°æ®ã€‚ç®€å•çš„å…³é”®è¯æœç´¢æ˜¯ä¸å¤Ÿçš„ï¼Œè€Œæ˜¯éœ€è¦è¿›è¡Œè¯­ä¹‰æœç´¢ã€‚ä¸ºäº†ä¿ƒè¿›è¯­ä¹‰æœç´¢ï¼Œä»å¤–éƒ¨æºæ£€ç´¢çš„æ–‡æœ¬ä¿¡æ¯ä¼šè¢«è½¬åŒ–ä¸ºæ•°å€¼è¡¨ç¤ºæˆ–å‘é‡ï¼Œé€šå¸¸ç§°ä¸ºæ–‡æœ¬åµŒå…¥ï¼Œå¹¶å­˜å‚¨åœ¨å‘é‡æ•°æ®åº“ä¸­ã€‚å­˜åœ¨å¤šç§æ¨¡å‹æˆ–ç®—æ³•ç”¨äºä»æ–‡æœ¬ä¸­åˆ›å»ºè¿™äº›åµŒå…¥ã€‚é¦–å…ˆå°†æç¤ºè½¬åŒ–ä¸ºå…¶å‘é‡è¡¨ç¤ºï¼Œä»¥ä¾¿æœç´¢å’Œæ£€ç´¢æœ€æ¥è¿‘çš„å¤–éƒ¨æ•°æ®å‘é‡ã€‚ç„¶åè®¡ç®—æç¤ºå‘é‡ä¸å…ˆå‰å­˜å‚¨çš„å¤–éƒ¨æ•°æ®å‘é‡ä¹‹é—´çš„å‘é‡ç›¸ä¼¼åº¦ï¼ˆæˆ–å‘é‡è·ç¦»ï¼‰ã€‚æ ¹æ®ç›¸ä¼¼åº¦æ’åºå¹¶ä½¿ç”¨é˜ˆå€¼è¿‡æ»¤æœ€ç›¸ä¼¼æˆ–æœ€è¿‘çš„å‘é‡ï¼Œæœ€ç»ˆæ£€ç´¢ç›¸åº”çš„æ–‡æœ¬ä¿¡æ¯ä»¥å¢å¼ºæç¤ºçš„ä¸Šä¸‹æ–‡ã€‚ä»¥ä¸‹æ¦‚å¿µå›¾å±•ç¤ºäº†å¯ç”¨RAGæ—¶ä¸åŒç»„ä»¶ä¹‹é—´çš„å…¸å‹äº¤äº’ï¼š
- en: '![](../Images/b2ae4ffe86cb15e19dafd2de1bbfbf70.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2ae4ffe86cb15e19dafd2de1bbfbf70.png)'
- en: Conceptual View of Primary System Component Interactions for Enabling RAG â€”
    Image by Author
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ç”¨RAGçš„ä¸»è¦ç³»ç»Ÿç»„ä»¶äº¤äº’çš„æ¦‚å¿µè§†å›¾ â€” ä½œè€…æä¾›çš„å›¾ç‰‡
- en: RAGâ€™s challenge is that conducting a vector-driven semantic search is non-trivial
    and requires significant computational resources because it involves calculating
    vector similarities or distances against potentially a vast number of vectors
    within the database. Computing similarity or distance measures for each stored
    vector from a vast vector database for every input prompt will become infeasible.
    Besides, the lower the semantic match quality, the lower the LLMâ€™s generative
    output quality. Therefore, finding a way to conduct the semantic search efficiently
    becomes crucial.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: RAGé¢ä¸´çš„æŒ‘æˆ˜æ˜¯ï¼Œè¿›è¡ŒåŸºäºå‘é‡çš„è¯­ä¹‰æœç´¢å¹¶éç®€å•ä»»åŠ¡ï¼Œä¸”éœ€è¦å¤§é‡è®¡ç®—èµ„æºï¼Œå› ä¸ºå®ƒæ¶‰åŠåˆ°å¯¹å¯èƒ½æ˜¯åºå¤§æ•°æ®åº“ä¸­å¤§é‡å‘é‡è¿›è¡Œç›¸ä¼¼åº¦æˆ–è·ç¦»è®¡ç®—ã€‚å¯¹äºæ¯ä¸€ä¸ªè¾“å…¥æç¤ºï¼Œä»åºå¤§çš„å‘é‡æ•°æ®åº“ä¸­è®¡ç®—æ¯ä¸ªå­˜å‚¨å‘é‡çš„ç›¸ä¼¼åº¦æˆ–è·ç¦»å°†å˜å¾—ä¸å¯è¡Œã€‚è€Œä¸”ï¼Œè¯­ä¹‰åŒ¹é…è´¨é‡è¶Šä½ï¼ŒLLMçš„ç”Ÿæˆè¾“å‡ºè´¨é‡ä¹Ÿè¶Šä½ã€‚å› æ­¤ï¼Œæ‰¾åˆ°ä¸€ç§é«˜æ•ˆè¿›è¡Œè¯­ä¹‰æœç´¢çš„æ–¹æ³•å˜å¾—è‡³å…³é‡è¦ã€‚
- en: Solution
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è§£å†³æ–¹æ¡ˆ
- en: Several algorithmic solutions are employed to conduct efficient semantic searches.
    The typical approach of such algorithms is to group or cluster external data vectors
    as nearest neighbors and index them by mapping to such clusters. Such indexing
    is offered as a built-in capability by most vector databases. The matched clusters
    are first evaluated for the input prompt vector during semantic search. For each
    evaluated cluster, indexed vectors are selected. Similarities between the input
    prompt vector and the selected vectors are then computed. The expectation here
    is that finding the â€˜nearest neighborsâ€™ as an intermediate step reduces the number
    of similarity computations significantly. Finally, the textual information is
    retrieved corresponding to the most similar or nearest vectors filtered through
    thresholding. Algorithms such as k-Nearest Neighbors, Ball-of-Radius-R, Locality-Sensitive-Hashing,
    DBSCAN-Clustering, Tree-Like hierarchies, and Graph-Like hierarchies are typically
    implemented by vector databases to facilitate semantic searches.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¿›è¡Œé«˜æ•ˆçš„è¯­ä¹‰æœç´¢ï¼Œé‡‡ç”¨äº†å‡ ç§ç®—æ³•è§£å†³æ–¹æ¡ˆã€‚è¿™äº›ç®—æ³•çš„å…¸å‹æ–¹æ³•æ˜¯å°†å¤–éƒ¨æ•°æ®å‘é‡æŒ‰æœ€è¿‘é‚»è¿›è¡Œåˆ†ç»„æˆ–èšç±»ï¼Œå¹¶é€šè¿‡æ˜ å°„åˆ°è¿™äº›èšç±»æ¥è¿›è¡Œç´¢å¼•ã€‚å¤§å¤šæ•°å‘é‡æ•°æ®åº“æä¾›äº†è¿™ç§å†…å»ºçš„ç´¢å¼•åŠŸèƒ½ã€‚åœ¨è¯­ä¹‰æœç´¢è¿‡ç¨‹ä¸­ï¼Œé¦–å…ˆä¼šè¯„ä¼°è¾“å…¥æç¤ºå‘é‡ä¸åŒ¹é…åˆ°çš„èšç±»ã€‚å¯¹äºæ¯ä¸ªè¯„ä¼°è¿‡çš„èšç±»ï¼Œé€‰æ‹©ç›¸åº”çš„ç´¢å¼•å‘é‡ã€‚ç„¶åè®¡ç®—è¾“å…¥æç¤ºå‘é‡ä¸æ‰€é€‰å‘é‡ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚æ­¤å¤„çš„é¢„æœŸæ˜¯ï¼Œé€šè¿‡æ‰¾åˆ°â€œæœ€è¿‘é‚»â€ä½œä¸ºä¸€ä¸ªä¸­é—´æ­¥éª¤ï¼Œæ˜¾è‘—å‡å°‘äº†ç›¸ä¼¼åº¦è®¡ç®—çš„æ¬¡æ•°ã€‚æœ€åï¼Œä¾æ®é˜ˆå€¼è¿‡æ»¤ï¼Œé€šè¿‡ç›¸ä¼¼åº¦æœ€é«˜æˆ–æœ€è¿‘çš„å‘é‡æ¥æ£€ç´¢ç›¸åº”çš„æ–‡æœ¬ä¿¡æ¯ã€‚åƒk-æœ€è¿‘é‚»ã€åŠå¾„çƒä½“Rã€å±€éƒ¨æ•æ„Ÿå“ˆå¸Œã€DBSCANèšç±»ã€æ ‘çŠ¶å±‚æ¬¡ç»“æ„å’Œå›¾çŠ¶å±‚æ¬¡ç»“æ„ç­‰ç®—æ³•ï¼Œé€šå¸¸ç”±å‘é‡æ•°æ®åº“å®ç°ï¼Œç”¨ä»¥ä¿ƒè¿›è¯­ä¹‰æœç´¢ã€‚
- en: There is no one-size-fits-all solution because different families of algorithms
    have different trade-offs in terms of memory efficiency, compute efficiency, latency,
    accuracy, vector dimensionality, dataset sizing, etc. For example, clustering
    methods enable speed by narrowing the vector space for semantic search, while
    tree-like or graph-like methods offer improved accuracy for low-dimensional vector
    data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æ²¡æœ‰ä¸€ç§é€‚ç”¨äºæ‰€æœ‰æƒ…å†µçš„è§£å†³æ–¹æ¡ˆï¼Œå› ä¸ºä¸åŒç±»å‹çš„ç®—æ³•åœ¨å†…å­˜æ•ˆç‡ã€è®¡ç®—æ•ˆç‡ã€å»¶è¿Ÿã€å‡†ç¡®æ€§ã€å‘é‡ç»´åº¦ã€æ•°æ®é›†å¤§å°ç­‰æ–¹é¢æœ‰ä¸åŒçš„æƒè¡¡ã€‚ä¾‹å¦‚ï¼Œèšç±»æ–¹æ³•é€šè¿‡ç¼©å°è¯­ä¹‰æœç´¢çš„å‘é‡ç©ºé—´æ¥æé«˜é€Ÿåº¦ï¼Œè€Œç±»ä¼¼æ ‘å½¢æˆ–å›¾å½¢çš„æ–¹æ³•åˆ™ä¸ºä½ç»´å‘é‡æ•°æ®æä¾›æ›´é«˜çš„å‡†ç¡®æ€§ã€‚
- en: Self-Organizing Maps
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è‡ªç»„ç»‡æ˜ å°„
- en: A Self-Organizing Map (SOM) is a neural network-based dimensionality reduction
    algorithm developed by Teuvo Kohonen in the 1980s. It is typically used to reduce
    high-dimensional feature vectors to low-dimensional (typically two-dimensional)
    feature vectors. The core idea behind SOM is to represent high-dimensional data
    vectors as specific nodes in a low-dimensional space while retaining the vectorsâ€™
    topology in the original space. The number of nodes in the low-dimensional space
    (SOM Nodes) is fixed (hyper-parameter). The exact locations of SOM nodes are evaluated
    through multiple training epochs. The goal of the iterative training is to adjust
    the locations of the SOM nodes in the low-dimensional space so that they get mapped
    to the nearest neighboring vectors in the high-dimensional feature space. In other
    words, the goal is to map nearest-neighbor vectors in the high-dimensional space
    to SOM nodes that are also nearest neighbors in the low-dimensional space.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªç»„ç»‡æ˜ å°„ï¼ˆSOMï¼‰æ˜¯ä¸€ç§åŸºäºç¥ç»ç½‘ç»œçš„é™ç»´ç®—æ³•ï¼Œç”±Teuvo Kohonenåœ¨1980å¹´ä»£å¼€å‘ã€‚å®ƒé€šå¸¸ç”¨äºå°†é«˜ç»´ç‰¹å¾å‘é‡é™è‡³ä½ç»´ï¼ˆé€šå¸¸æ˜¯äºŒç»´ï¼‰ç‰¹å¾å‘é‡ã€‚SOMçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†é«˜ç»´æ•°æ®å‘é‡è¡¨ç¤ºä¸ºä½ç»´ç©ºé—´ä¸­çš„ç‰¹å®šèŠ‚ç‚¹ï¼ŒåŒæ—¶ä¿ç•™å‘é‡åœ¨åŸå§‹ç©ºé—´ä¸­çš„æ‹“æ‰‘ç»“æ„ã€‚ä½ç»´ç©ºé—´ä¸­çš„èŠ‚ç‚¹æ•°é‡ï¼ˆSOMèŠ‚ç‚¹ï¼‰æ˜¯å›ºå®šçš„ï¼ˆè¶…å‚æ•°ï¼‰ã€‚SOMèŠ‚ç‚¹çš„ç²¾ç¡®ä½ç½®é€šè¿‡å¤šä¸ªè®­ç»ƒå‘¨æœŸæ¥è¯„ä¼°ã€‚è¿­ä»£è®­ç»ƒçš„ç›®æ ‡æ˜¯è°ƒæ•´SOMèŠ‚ç‚¹åœ¨ä½ç»´ç©ºé—´ä¸­çš„ä½ç½®ï¼Œä½¿å…¶æ˜ å°„åˆ°é«˜ç»´ç‰¹å¾ç©ºé—´ä¸­æœ€é‚»è¿‘çš„å‘é‡ã€‚æ¢å¥è¯è¯´ï¼Œç›®æ ‡æ˜¯å°†é«˜ç»´ç©ºé—´ä¸­æœ€è¿‘é‚»çš„å‘é‡æ˜ å°„åˆ°ä½ç»´ç©ºé—´ä¸­ä¹Ÿä¸ºæœ€è¿‘é‚»çš„SOMèŠ‚ç‚¹ã€‚
- en: SOM for RAG
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SOMç”¨äºRAG
- en: 'In this write-up, I wanted to share notes and findings from my experiments
    with SOM as a possible algorithm to propel RAGâ€™s semantic search. There are three
    crucial reasons SOM could be ideal compared to other algorithms:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘æƒ³åˆ†äº«æˆ‘åœ¨å®éªŒä¸­å¯¹SOMçš„ç¬”è®°å’Œå‘ç°ï¼Œä½œä¸ºä¸€ç§æ¨åŠ¨RAGè¯­ä¹‰æœç´¢çš„å¯èƒ½ç®—æ³•ã€‚SOMç›¸æ¯”å…¶ä»–ç®—æ³•å¯èƒ½ç†æƒ³çš„ä¸‰ä¸ªå…³é”®åŸå› æ˜¯ï¼š
- en: Vectorsâ€™ high dimensionality can become a bottleneck for most other algorithms,
    such as Trees and Graphsâ€”the so-called curse of dimensionality. On the contrary,
    SOM is built for dimensionality reduction, and therefore, it can be effectively
    applied in both high-dimensional and low-dimensional scenarios.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å‘é‡çš„é«˜ç»´åº¦å¯èƒ½æˆä¸ºå¤§å¤šæ•°å…¶ä»–ç®—æ³•çš„ç“¶é¢ˆï¼Œä¾‹å¦‚æ ‘å’Œå›¾â€”â€”è¿™å°±æ˜¯æ‰€è°“çš„ç»´åº¦ç¾éš¾ã€‚ç›¸åï¼ŒSOMæ˜¯ä¸ºé™ç»´è€Œè®¾è®¡çš„ï¼Œå› æ­¤å®ƒå¯ä»¥åœ¨é«˜ç»´å’Œä½ç»´åœºæ™¯ä¸­æœ‰æ•ˆåº”ç”¨ã€‚
- en: SOM is less sensitive to random variations that may trickle into the original
    high-dimensional vector space, resulting in noise. Other algorithms can be sensitive
    to such noise, impacting the way they cluster or group high-dimensional vectors
    as nearest neighbors. Since SOM employs intermediate SOM nodes in a lower-dimensional
    vector space which get evaluated as local averages of the mapped vectors from
    the higher-dimensional space, it effectively reduces noise.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SOMå¯¹å¯èƒ½æ¸—å…¥åŸå§‹é«˜ç»´å‘é‡ç©ºé—´çš„éšæœºå˜åŒ–ä¸å¤ªæ•æ„Ÿï¼Œä»è€Œé¿å…äº†å™ªéŸ³çš„å½±å“ã€‚å…¶ä»–ç®—æ³•å¯èƒ½å¯¹è¿™äº›å™ªéŸ³æ•æ„Ÿï¼Œä»è€Œå½±å“å®ƒä»¬å°†é«˜ç»´å‘é‡èšç±»æˆ–åˆ†ç»„ä¸ºæœ€è¿‘é‚»çš„æ–¹å¼ã€‚ç”±äºSOMåœ¨ä½ç»´å‘é‡ç©ºé—´ä¸­ä½¿ç”¨ä¸­é—´çš„SOMèŠ‚ç‚¹ï¼Œè¿™äº›èŠ‚ç‚¹è¢«è¯„ä¼°ä¸ºæ˜ å°„è‡ªé«˜ç»´ç©ºé—´å‘é‡çš„å±€éƒ¨å¹³å‡å€¼ï¼Œå› æ­¤å®ƒæœ‰æ•ˆåœ°å‡å°‘äº†å™ªéŸ³ã€‚
- en: The large size of the external dataset may constrain other algorithms to create
    semantic vector spaces, which can impact semantic matching's latency and accuracy.
    On the other hand, SOM can tackle massive datasets because the number of SOM nodes
    in the low-dimensional space can be fine-tuned through a hyper-parameter proportional
    to the underlying dataset size. While training a SOM using a large dataset may
    take longer, query time mapping remains quicker once training is done.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¤–éƒ¨æ•°æ®é›†çš„åºå¤§è§„æ¨¡å¯èƒ½ä¼šé™åˆ¶å…¶ä»–ç®—æ³•åœ¨åˆ›å»ºè¯­ä¹‰å‘é‡ç©ºé—´æ—¶çš„è¡¨ç°ï¼Œè¿™å¯èƒ½ä¼šå½±å“è¯­ä¹‰åŒ¹é…çš„å»¶è¿Ÿå’Œå‡†ç¡®æ€§ã€‚å¦ä¸€æ–¹é¢ï¼ŒSOMèƒ½å¤Ÿå¤„ç†å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå› ä¸ºä½ç»´ç©ºé—´ä¸­çš„SOMèŠ‚ç‚¹æ•°é‡å¯ä»¥é€šè¿‡ä¸åº•å±‚æ•°æ®é›†å¤§å°æˆæ¯”ä¾‹çš„è¶…å‚æ•°æ¥ç²¾ç»†è°ƒæ•´ã€‚å°½ç®¡ä½¿ç”¨å¤§æ•°æ®é›†è®­ç»ƒSOMå¯èƒ½éœ€è¦æ›´é•¿çš„æ—¶é—´ï¼Œä½†è®­ç»ƒå®Œæˆåï¼ŒæŸ¥è¯¢æ˜ å°„ä»ç„¶ä¼šæ›´å¿«ã€‚
- en: 'I demonstrate a simple example of using SOM to conduct RAGâ€™s semantic search
    to augment the context for question/answer using OpenAIâ€™s GPT-3.5-turbo-instruct
    LLM. The primary reason for using OpenAIâ€™s GPT-3.5-turbo-instruct LLM is because
    the cut-off date for training OpenAIâ€™s GPT-3.5-turbo-instruct LLM is September
    2021 (Ref: [https://platform.openai.com/docs/models/gpt-3-5-turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo)),
    and as such, GPT-3.5-turbo-instruct LLM may not answer questions on 2022, 2023,
    or 2024 events accurately. Therefore, information about 2022, 2023, 0r 2024 events
    can become â€˜external dataâ€™ for OpenAIâ€™s GPT-3.5-turbo-instruct LLM. I used Wikipedia
    API as the source for such â€˜external dataâ€™ to fetch eventsâ€™ information. The following
    are the steps I used to develop and train the example, along with the sample code.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å±•ç¤ºäº†ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹ï¼Œä½¿ç”¨ SOM æ¥è¿›è¡Œ RAG çš„è¯­ä¹‰æœç´¢ï¼Œä»¥å¢å¼ºåŸºäº OpenAI GPT-3.5-turbo-instruct LLM çš„é—®ç­”ä¸Šä¸‹æ–‡ã€‚ä½¿ç”¨
    OpenAI GPT-3.5-turbo-instruct LLM çš„ä¸»è¦åŸå› æ˜¯å› ä¸º OpenAI GPT-3.5-turbo-instruct LLM çš„è®­ç»ƒæˆªæ­¢æ—¥æœŸä¸º
    2021 å¹´ 9 æœˆï¼ˆå‚è€ƒï¼š[https://platform.openai.com/docs/models/gpt-3-5-turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo)ï¼‰ï¼Œå› æ­¤ï¼ŒGPT-3.5-turbo-instruct
    LLM å¯èƒ½æ— æ³•å‡†ç¡®å›ç­” 2022 å¹´ã€2023 å¹´æˆ– 2024 å¹´çš„äº‹ä»¶é—®é¢˜ã€‚å› æ­¤ï¼Œå…³äº 2022 å¹´ã€2023 å¹´æˆ– 2024 å¹´çš„äº‹ä»¶ä¿¡æ¯å¯èƒ½æˆä¸º OpenAI
    GPT-3.5-turbo-instruct LLM çš„â€œå¤–éƒ¨æ•°æ®â€ã€‚æˆ‘ä½¿ç”¨äº† Wikipedia API ä½œä¸ºè¿™ç§â€œå¤–éƒ¨æ•°æ®â€çš„æ¥æºï¼Œæ¥è·å–äº‹ä»¶ä¿¡æ¯ã€‚ä»¥ä¸‹æ˜¯æˆ‘ç”¨æ¥å¼€å‘å’Œè®­ç»ƒç¤ºä¾‹çš„æ­¥éª¤ï¼Œä»¥åŠç¤ºä¾‹ä»£ç ã€‚
- en: 'Step 1: PyTorch-Based Kohonenâ€™s SOM implementation'
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ­¥ï¼šåŸºäº PyTorch çš„ Kohonen SOM å®ç°
- en: 'I utilized PyTorch Tensors to represent vectors and implemented Kohonenâ€™s SOM
    using PyTorch. This algorithm uses a two-dimensional lattice whose size becomes
    a hyper-parameter. The algorithmâ€™s mathematical aspects were derived from a well-crafted
    perspective with lucid explanations mentioned in the following article:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä½¿ç”¨äº† PyTorch å¼ é‡æ¥è¡¨ç¤ºå‘é‡ï¼Œå¹¶ä½¿ç”¨ PyTorch å®ç°äº† Kohonen çš„ SOMã€‚è¯¥ç®—æ³•ä½¿ç”¨ä¸€ä¸ªäºŒç»´æ ¼å­ï¼Œå…¶å¤§å°æˆä¸ºä¸€ä¸ªè¶…å‚æ•°ã€‚ç®—æ³•çš„æ•°å­¦æ–¹é¢æ¥æºäºä»¥ä¸‹æ–‡ç« ä¸­çš„æ¸…æ™°è§£é‡Šï¼š
- en: '[](http://www.ai-junkie.com/ann/som/som1.html?source=post_page-----5d739ce21e9c--------------------------------)
    [## SOM tutorial part 1'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[](http://www.ai-junkie.com/ann/som/som1.html?source=post_page-----5d739ce21e9c--------------------------------)
    [## SOM æ•™ç¨‹ç¬¬ 1 éƒ¨åˆ†'
- en: neural network tutorial in plain english
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œæ•™ç¨‹ï¼ˆé€šä¿—æ˜“æ‡‚ï¼‰
- en: www.ai-junkie.com](http://www.ai-junkie.com/ann/som/som1.html?source=post_page-----5d739ce21e9c--------------------------------)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: www.ai-junkie.com](http://www.ai-junkie.com/ann/som/som1.html?source=post_page-----5d739ce21e9c--------------------------------)
- en: The following code snippet shows the Python class for Kohonenâ€™s SOM. The complete
    code is available at [this GitHub location](https://github.com/kbmurali/som-driven-qa-rag/blob/main/kohonen_som.py).
    Itâ€™s worth noting that this implementation is standalone, so it can be used outside
    of RAG example.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ä»£ç ç‰‡æ®µå±•ç¤ºäº† Kohonen SOM çš„ Python ç±»ã€‚å®Œæ•´ä»£ç å¯åœ¨[è¿™ä¸ª GitHub é“¾æ¥](https://github.com/kbmurali/som-driven-qa-rag/blob/main/kohonen_som.py)æ‰¾åˆ°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸ªå®ç°æ˜¯ç‹¬ç«‹çš„ï¼Œå› æ­¤å¯ä»¥åœ¨
    RAG ç¤ºä¾‹ä¹‹å¤–ä½¿ç”¨ã€‚
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Step 2: SOM-Based Vector Indexer Implementation'
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬äºŒæ­¥ï¼šåŸºäº SOM çš„å‘é‡ç´¢å¼•å™¨å®ç°
- en: The vector indexer is a utility that uses Kohonenâ€™s SOM to train SOM nodes with
    data vectors from an external dataset. Its primary purpose is to map each data
    vector to the closest top-k SOM nodes, enabling efficient indexing of the data
    vectors. The following code snippet shows the train and indexing function of the
    vector indexer Python class. Its complete code is available at [this GitHub location](https://github.com/kbmurali/som-driven-qa-rag/blob/main/vector_indexer.py).
    Although its implementation is currently limited to the exampleâ€™s needs, it can
    be extended to meet other requirements.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å‘é‡ç´¢å¼•å™¨æ˜¯ä¸€ä¸ªå·¥å…·ï¼Œåˆ©ç”¨ Kohonen çš„ SOM æ¥è®­ç»ƒ SOM èŠ‚ç‚¹ï¼Œä½¿ç”¨æ¥è‡ªå¤–éƒ¨æ•°æ®é›†çš„æ•°æ®å‘é‡ã€‚å…¶ä¸»è¦ç›®çš„æ˜¯å°†æ¯ä¸ªæ•°æ®å‘é‡æ˜ å°„åˆ°æœ€æ¥è¿‘çš„ top-k
    SOM èŠ‚ç‚¹ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„æ•°æ®å‘é‡ç´¢å¼•ã€‚ä»¥ä¸‹ä»£ç ç‰‡æ®µå±•ç¤ºäº†å‘é‡ç´¢å¼•å™¨ Python ç±»çš„è®­ç»ƒå’Œç´¢å¼•åŠŸèƒ½ã€‚å…¶å®Œæ•´ä»£ç å¯åœ¨[è¿™ä¸ª GitHub é“¾æ¥](https://github.com/kbmurali/som-driven-qa-rag/blob/main/vector_indexer.py)æ‰¾åˆ°ã€‚å°½ç®¡å…¶å®ç°ç›®å‰ä»…é™äºç¤ºä¾‹çš„éœ€æ±‚ï¼Œä½†å¯ä»¥æ‰©å±•ä»¥æ»¡è¶³å…¶ä»–éœ€æ±‚ã€‚
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Step 3: OpenAI Embeddings-Based Text-To-Vector Encoder'
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰æ­¥ï¼šåŸºäº OpenAI åµŒå…¥çš„æ–‡æœ¬åˆ°å‘é‡ç¼–ç å™¨
- en: The encoderâ€™s primary function is to convert text into vector representations
    using OpenAIâ€™s text embedding API. It is worth noting that an OpenAI account and
    API key are required to use the embedding API. Upon opening an account for the
    first time, OpenAI provides complementary credit grants, which are more than enough
    to access the API for testing purposes. Below is a code snippet showcasing the
    batch encode function of the OpenAI encoder Python class. The complete code is
    available at [this GitHub location](https://github.com/kbmurali/som-driven-qa-rag/blob/main/openai_vector_encoder.py).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨çš„ä¸»è¦åŠŸèƒ½æ˜¯ä½¿ç”¨OpenAIçš„æ–‡æœ¬åµŒå…¥APIå°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨åµŒå…¥APIéœ€è¦ä¸€ä¸ªOpenAIå¸æˆ·å’ŒAPIå¯†é’¥ã€‚åœ¨é¦–æ¬¡å¼€é€šè´¦æˆ·æ—¶ï¼ŒOpenAIä¼šæä¾›å…è´¹çš„ä¿¡ç”¨é¢åº¦ï¼Œè¶³ä»¥ç”¨äºAPIæµ‹è¯•ã€‚ä»¥ä¸‹æ˜¯å±•ç¤ºOpenAIç¼–ç å™¨Pythonç±»çš„æ‰¹é‡ç¼–ç åŠŸèƒ½çš„ä»£ç ç‰‡æ®µï¼Œå®Œæ•´ä»£ç å¯åœ¨[è¿™ä¸ªGitHubä½ç½®](https://github.com/kbmurali/som-driven-qa-rag/blob/main/openai_vector_encoder.py)æ‰¾åˆ°ã€‚
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that the OpenAI vector encoder class extends a generic parent class, â€˜VectorEncoder,â€™
    that defines abstract encoding functions to be implemented through inheritance.
    It is possible to implement other types of vector encoders by inheriting from
    this parent class for the pluggability of other encoding schemes. The complete
    code for the parent vector encoder class can be found at [this GitHub location](https://github.com/kbmurali/som-driven-qa-rag/blob/main/vector_encoder_parent.py).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼ŒOpenAIå‘é‡ç¼–ç å™¨ç±»æ‰©å±•äº†ä¸€ä¸ªé€šç”¨çš„çˆ¶ç±»â€˜VectorEncoderâ€™ï¼Œè¯¥çˆ¶ç±»å®šä¹‰äº†æŠ½è±¡çš„ç¼–ç å‡½æ•°ï¼Œéœ€é€šè¿‡ç»§æ‰¿æ¥å®ç°ã€‚å¯ä»¥é€šè¿‡ç»§æ‰¿è¯¥çˆ¶ç±»å®ç°å…¶ä»–ç±»å‹çš„å‘é‡ç¼–ç å™¨ï¼Œä»è€Œå®ç°å…¶ä»–ç¼–ç æ–¹æ¡ˆçš„æ’ä»¶åŒ–ã€‚çˆ¶å‘é‡ç¼–ç å™¨ç±»çš„å®Œæ•´ä»£ç å¯ä»¥åœ¨[è¿™ä¸ªGitHubä½ç½®](https://github.com/kbmurali/som-driven-qa-rag/blob/main/vector_encoder_parent.py)æ‰¾åˆ°ã€‚
- en: 'Step 4: Wikipedia API-Driven DataSource Implementation'
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ­¥éª¤4ï¼šåŸºäºWikipedia APIçš„æ•°æ®æºå®ç°
- en: This utility class is designed to encapsulate the data retrieval logic that
    integrates with Wikipedia API. Its main function is to fetch events for a specified
    array of calendar years, format the retrieved events, and load them into a Pandas
    dataframe. The code snippet below captures the primary function of the utility
    class, while the complete code is available at [this GitHub location](https://github.com/kbmurali/som-driven-qa-rag/blob/main/wiki_datasource.py).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå·¥å…·ç±»æ—¨åœ¨å°è£…ä¸Wikipedia APIé›†æˆçš„æ•°æ®æ£€ç´¢é€»è¾‘ã€‚å®ƒçš„ä¸»è¦åŠŸèƒ½æ˜¯è·å–æŒ‡å®šæ—¥å†å¹´ä»½èŒƒå›´å†…çš„äº‹ä»¶ï¼Œæ ¼å¼åŒ–æ£€ç´¢åˆ°çš„äº‹ä»¶ï¼Œå¹¶å°†å®ƒä»¬åŠ è½½åˆ°Pandasæ•°æ®æ¡†ä¸­ã€‚ä»¥ä¸‹ä»£ç ç‰‡æ®µå±•ç¤ºäº†è¯¥å·¥å…·ç±»çš„ä¸»è¦åŠŸèƒ½ï¼Œå®Œæ•´ä»£ç å¯ä»¥åœ¨[è¿™ä¸ªGitHubä½ç½®](https://github.com/kbmurali/som-driven-qa-rag/blob/main/wiki_datasource.py)æ‰¾åˆ°ã€‚
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Step 5: SOM-Based RAG Utility Implementation'
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ­¥éª¤5ï¼šåŸºäºSOMçš„RAGå·¥å…·å®ç°
- en: The SOM-based RAG utility is a crucial element of the example implementation.
    It utilizes the vector encoder, indexer, and data source to implement the core
    logic for the underlying semantic search. The complete code for the SOM-based
    RAG utility is available at [this GitHub location](https://github.com/kbmurali/som-driven-qa-rag/blob/main/som_based_rag.py).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºSOMçš„RAGå·¥å…·æ˜¯ç¤ºä¾‹å®ç°ä¸­çš„ä¸€ä¸ªå…³é”®å…ƒç´ ã€‚å®ƒåˆ©ç”¨å‘é‡ç¼–ç å™¨ã€ç´¢å¼•å™¨å’Œæ•°æ®æºå®ç°åº•å±‚è¯­ä¹‰æœç´¢çš„æ ¸å¿ƒé€»è¾‘ã€‚åŸºäºSOMçš„RAGå·¥å…·çš„å®Œæ•´ä»£ç å¯ä»¥åœ¨[è¿™ä¸ªGitHubä½ç½®](https://github.com/kbmurali/som-driven-qa-rag/blob/main/som_based_rag.py)æ‰¾åˆ°ã€‚
- en: The utility implements three primary functions. The first function is to load
    data from an external data source and encode it into vectors, as shown in the
    following code snippet.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥å·¥å…·å®ç°äº†ä¸‰ä¸ªä¸»è¦åŠŸèƒ½ã€‚ç¬¬ä¸€ä¸ªåŠŸèƒ½æ˜¯ä»å¤–éƒ¨æ•°æ®æºåŠ è½½æ•°æ®å¹¶å°†å…¶ç¼–ç ä¸ºå‘é‡ï¼Œå¦‚ä¸‹é¢çš„ä»£ç ç‰‡æ®µæ‰€ç¤ºã€‚
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The second function is to train the SOM-based indexer to construct Kohonenâ€™s
    SOM nodes and then index the data vectors, as shown in the following code snippet.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªåŠŸèƒ½æ˜¯è®­ç»ƒåŸºäºSOMçš„ç´¢å¼•å™¨ï¼Œæ„å»ºKohonençš„SOMèŠ‚ç‚¹ï¼Œç„¶åå¯¹æ•°æ®å‘é‡è¿›è¡Œç´¢å¼•ï¼Œå¦‚ä¸‹é¢çš„ä»£ç ç‰‡æ®µæ‰€ç¤ºã€‚
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The third function is to find similar information from the previously stored
    external dataset based on a query text. This function uses the encoder to convert
    the query text into a vector and then searches through the SOM-based indexer for
    the most likely matches. This function then calculates the similarity between
    the query vector and the discovered data vectors using Cosine similarity or another
    specified similarity evaluator. Finally, this function filters the data vectors
    whose similarities are greater than or equal to the specified similarity threshold.
    The following code snippet captures the function implementation.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰ä¸ªåŠŸèƒ½æ˜¯åŸºäºæŸ¥è¯¢æ–‡æœ¬ä»å…ˆå‰å­˜å‚¨çš„å¤–éƒ¨æ•°æ®é›†æ‰¾åˆ°ç›¸ä¼¼çš„ä¿¡æ¯ã€‚è¯¥åŠŸèƒ½ä½¿ç”¨ç¼–ç å™¨å°†æŸ¥è¯¢æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡ï¼Œç„¶åé€šè¿‡åŸºäºSOMçš„ç´¢å¼•å™¨æœç´¢æœ€å¯èƒ½çš„åŒ¹é…é¡¹ã€‚æ¥ç€ï¼Œä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æˆ–å…¶ä»–æŒ‡å®šçš„ç›¸ä¼¼åº¦è¯„ä¼°å™¨è®¡ç®—æŸ¥è¯¢å‘é‡ä¸å‘ç°çš„æ•°æ®å‘é‡ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚æœ€åï¼Œè¯¥åŠŸèƒ½ç­›é€‰å‡ºä¸æŒ‡å®šç›¸ä¼¼åº¦é˜ˆå€¼å¤§äºæˆ–ç­‰äºçš„ç›¸ä¼¼åº¦çš„æ•°æ®å‘é‡ã€‚ä¸‹é¢çš„ä»£ç ç‰‡æ®µå±•ç¤ºäº†è¯¥åŠŸèƒ½çš„å®ç°ã€‚
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'An example output from a semantic search by SOM-based RAG utility function
    is shown below:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯é€šè¿‡åŸºäºSOMçš„RAGå®ç”¨å‡½æ•°è¿›è¡Œè¯­ä¹‰æœç´¢çš„ç¤ºä¾‹è¾“å‡ºï¼š
- en: '![](../Images/0495dc8a7325e42a26216f4edea1b957.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0495dc8a7325e42a26216f4edea1b957.png)'
- en: An Example Semantic Search Output â€” Image by Author
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªç¤ºä¾‹çš„è¯­ä¹‰æœç´¢è¾“å‡ºâ€”â€”ä½œè€…æä¾›çš„å›¾åƒ
- en: 'Step 6: Abstract Question/Answer ChatBot And Its OpenAI-Based Implementation'
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬6æ­¥ï¼šæŠ½è±¡é—®é¢˜/å›ç­”èŠå¤©æœºå™¨äººåŠå…¶åŸºäºOpenAIçš„å®ç°
- en: An abstract â€˜QuestionAnswerChatBotâ€™ Python class is developed to facilitate
    chatbot-like implementations. It augments the prompted question by using a standard
    instruction template and populating it with contextually similar information retrieved
    from the RAG utility.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæŠ½è±¡çš„â€˜QuestionAnswerChatBotâ€™ Pythonç±»è¢«å¼€å‘å‡ºæ¥ï¼Œä»¥ä¾¿äºç±»ä¼¼èŠå¤©æœºå™¨äººçš„å®ç°ã€‚å®ƒé€šè¿‡ä½¿ç”¨æ ‡å‡†çš„æŒ‡ä»¤æ¨¡æ¿ï¼Œå¹¶ç”¨ä»RAGå®ç”¨å·¥å…·ä¸­æ£€ç´¢åˆ°çš„è¯­å¢ƒç›¸ä¼¼ä¿¡æ¯å¡«å……å®ƒï¼Œæ¥å¢å¼ºé—®é¢˜æç¤ºã€‚
- en: The specified maximum number of new tokens limits the text size for context
    augmentation, while token counting is deferred to underlying implementations.
    In LLM economics, tokens are like currency. Each token the model processes requires
    computational resources â€” memory, processing power, and time. Thus, the more tokens
    an LLM has to process, the greater the computational cost.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æŒ‡å®šçš„æœ€å¤§æ–°æ ‡è®°æ•°é™åˆ¶äº†ä¸Šä¸‹æ–‡å¢å¼ºçš„æ–‡æœ¬å¤§å°ï¼Œè€Œæ ‡è®°è®¡æ•°åˆ™æ¨è¿Ÿåˆ°åº•å±‚å®ç°ã€‚åœ¨LLMç»æµå­¦ä¸­ï¼Œæ ‡è®°å°±åƒè´§å¸ä¸€æ ·ã€‚æ¨¡å‹å¤„ç†çš„æ¯ä¸€ä¸ªæ ‡è®°éƒ½éœ€è¦è®¡ç®—èµ„æºâ€”â€”å†…å­˜ã€å¤„ç†èƒ½åŠ›å’Œæ—¶é—´ã€‚å› æ­¤ï¼ŒLLMéœ€è¦å¤„ç†çš„æ ‡è®°è¶Šå¤šï¼Œè®¡ç®—æˆæœ¬å°±è¶Šé«˜ã€‚
- en: Finally, this class delegates prompting of the LLM model to the underlying implementation
    once the QA instruction has been populated. The following code snippet captures
    the primary function; the complete code is available at [this GitHub location](https://github.com/kbmurali/som-driven-qa-rag/blob/main/qa_chatbot.py).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œä¸€æ—¦é—®ç­”æŒ‡ä»¤è¢«å¡«å……ï¼Œè¯¥ç±»ä¼šå°†LLMæ¨¡å‹çš„æç¤ºå·¥ä½œå§”æ‰˜ç»™åº•å±‚å®ç°ã€‚ä»¥ä¸‹ä»£ç ç‰‡æ®µå±•ç¤ºäº†ä¸»è¦åŠŸèƒ½ï¼›å®Œæ•´ä»£ç å¯ä»¥åœ¨[è¿™ä¸ªGitHubä½ç½®](https://github.com/kbmurali/som-driven-qa-rag/blob/main/qa_chatbot.py)æ‰¾åˆ°ã€‚
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The Python class â€˜OpenAIQuestionAnswerChatBotâ€™ extends the abstract â€˜QuestionAnswerChatBotâ€™
    and implements the chatbot functionality using the OpenAI LLM API. The following
    code snippet shows the classâ€™s primary function. The complete code is available
    at [this GitHub location](https://github.com/kbmurali/som-driven-qa-rag/blob/main/openai_qa_chatbot.py).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Pythonç±»â€˜OpenAIQuestionAnswerChatBotâ€™æ‰©å±•äº†æŠ½è±¡ç±»â€˜QuestionAnswerChatBotâ€™ï¼Œå¹¶ä½¿ç”¨OpenAI
    LLM APIå®ç°äº†èŠå¤©æœºå™¨äººåŠŸèƒ½ã€‚ä»¥ä¸‹ä»£ç ç‰‡æ®µå±•ç¤ºäº†è¯¥ç±»çš„ä¸»è¦åŠŸèƒ½ã€‚å®Œæ•´ä»£ç å¯ä»¥åœ¨[è¿™ä¸ªGitHubä½ç½®](https://github.com/kbmurali/som-driven-qa-rag/blob/main/openai_qa_chatbot.py)æ‰¾åˆ°ã€‚
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following is an example of how a prompted question gets augmented with
    context using similar information retrieved through semantic search:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯å¦‚ä½•é€šè¿‡è¯­ä¹‰æœç´¢æ£€ç´¢åˆ°çš„ç±»ä¼¼ä¿¡æ¯æ¥å¢å¼ºé—®é¢˜æç¤ºçš„ç¤ºä¾‹ï¼š
- en: '![](../Images/77dd0ecb2e734bb5bb300ed137e03cf6.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77dd0ecb2e734bb5bb300ed137e03cf6.png)'
- en: An Example Context Augmented Question Prompt â€” Image by Author
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªç¤ºä¾‹çš„ä¸Šä¸‹æ–‡å¢å¼ºé—®é¢˜æç¤ºâ€”â€”ä½œè€…æä¾›çš„å›¾åƒ
- en: 'Step 7: Sample Questions for Testing'
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬7æ­¥ï¼šç”¨äºæµ‹è¯•çš„ç¤ºä¾‹é—®é¢˜
- en: The following are sample questions for testing the RAG using OpenAIâ€™s GPT-3.5-turbo-instruct
    LLM. They were developed to ensure that their answers pertain to events that occurred
    in 2022, 2023, and 2024.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ç”¨äºæµ‹è¯•RAGçš„ç¤ºä¾‹é—®é¢˜ï¼Œä½¿ç”¨çš„æ˜¯OpenAIçš„GPT-3.5-turbo-instruct LLMã€‚è¿™äº›é—®é¢˜æ˜¯ä¸ºäº†ç¡®ä¿å®ƒä»¬çš„ç­”æ¡ˆä¸2022å¹´ã€2023å¹´å’Œ2024å¹´å‘ç”Ÿçš„äº‹ä»¶ç›¸å…³ã€‚
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Step 8: Putting Everything Together'
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬8æ­¥ï¼šå°†æ‰€æœ‰å†…å®¹æ•´åˆåœ¨ä¸€èµ·
- en: The complete Jupyter notebook that brings all the components together can be
    found at [this GitHub location](https://github.com/kbmurali/som-driven-qa-rag/blob/main/OpenAI_Based_SOM_GPT2_Bot.ipynb).
    The following code snippet shows the initiation of the main OpenAI-based QA chatbot.
    Note that OpenAIâ€™s text embedding algorithm, â€œtext-embedding-ada-002,â€ is used
    for vector encoding. Likewise, the chatbot uses OpenAIâ€™s tokenizer, â€œcl100k_base,â€
    to count the tokens to limit the contextual text to augment the question prompt
    by leveraging the inbuilt functions of the TikToken Python library.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æ•´åˆæ‰€æœ‰ç»„ä»¶çš„å®Œæ•´Jupyterç¬”è®°æœ¬å¯ä»¥åœ¨[è¿™ä¸ªGitHubä½ç½®](https://github.com/kbmurali/som-driven-qa-rag/blob/main/OpenAI_Based_SOM_GPT2_Bot.ipynb)æ‰¾åˆ°ã€‚ä»¥ä¸‹ä»£ç ç‰‡æ®µå±•ç¤ºäº†åŸºäºOpenAIçš„ä¸»è¦é—®ç­”èŠå¤©æœºå™¨äººçš„åˆå§‹åŒ–è¿‡ç¨‹ã€‚è¯·æ³¨æ„ï¼ŒOpenAIçš„æ–‡æœ¬åµŒå…¥ç®—æ³•â€œtext-embedding-ada-002â€ç”¨äºå‘é‡ç¼–ç ã€‚åŒæ ·ï¼ŒèŠå¤©æœºå™¨äººä½¿ç”¨OpenAIçš„åˆ†è¯å™¨â€œcl100k_baseâ€æ¥è®¡ç®—æ ‡è®°æ•°ï¼Œä»¥é™åˆ¶ä¸Šä¸‹æ–‡æ–‡æœ¬ï¼Œå¹¶é€šè¿‡åˆ©ç”¨TikToken
    Pythonåº“çš„å†…ç½®å‡½æ•°å¢å¼ºé—®é¢˜æç¤ºã€‚
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The following sequence diagrams help visualize all the component interactions
    during the initialization and actual question/answering phases.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹åºåˆ—å›¾æœ‰åŠ©äºå¯è§†åŒ–åˆå§‹åŒ–å’Œå®é™…é—®ç­”é˜¶æ®µä¸­æ‰€æœ‰ç»„ä»¶çš„äº¤äº’ã€‚
- en: '![](../Images/acffe90601416d6f17621a9d2d29cddb.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/acffe90601416d6f17621a9d2d29cddb.png)'
- en: Interactions of Various Components During Initialization â€” Image by Author
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: åˆå§‹åŒ–è¿‡ç¨‹ä¸­çš„å„ç»„ä»¶äº¤äº’â€”â€”ä½œè€…æä¾›çš„å›¾åƒ
- en: '![](../Images/320862ead248a3bb9360d7ac2eea63bc.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/320862ead248a3bb9360d7ac2eea63bc.png)'
- en: Interactions of Various Components During Question/Answering â€” Image by Author
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: é—®ç­”è¿‡ç¨‹ä¸­å„ä¸ªç»„ä»¶çš„äº¤äº’ â€”â€” å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: Findings
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç ”ç©¶å‘ç°
- en: The following image captures the question/answers from OpenAIâ€™s GPT-3.5-turbo-instruct
    LLM with and without context augmentation.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹å›¾åƒå±•ç¤ºäº†åœ¨æœ‰æ— ä¸Šä¸‹æ–‡å¢å¼ºçš„æƒ…å†µä¸‹ï¼ŒOpenAIçš„GPT-3.5-turbo-instruct LLMçš„é—®ç­”ã€‚
- en: '![](../Images/e55fe942772a20c25c4906ad9d6d3a5b.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e55fe942772a20c25c4906ad9d6d3a5b.png)'
- en: OpenAIâ€™s GPT-3.5-turbo-instruct LLMâ€™s Answers With and Without Context Augmentation
    â€” Image by Author
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAIçš„GPT-3.5-turbo-instruct LLMçš„æœ‰æ— ä¸Šä¸‹æ–‡å¢å¼ºå›ç­” â€”â€” å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: Understandably, the LLM finds it challenging to answer questions about events
    that occurred after its September 2021 cut-off date. In most cases, it clearly
    responds that the questions are from a future time relative to its training cut-off
    date. On the contrary, the same LLM answers all the questions accurately to perfection
    when the context of the prompted questions is augmented with relevant information
    from years 2022, 2023, and 2024 retrieved from Wikipedia. The real credit here
    goes to the SOM that formed the basis for RAGâ€™s semantic search to retrieve and
    augment the prompted questionâ€™s context with relevant information.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥ç†è§£çš„æ˜¯ï¼ŒLLMå‘ç°å›ç­”2021å¹´9æœˆä¹‹åå‘ç”Ÿçš„äº‹ä»¶ç›¸å…³é—®é¢˜æ—¶å­˜åœ¨å›°éš¾ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå®ƒæ˜ç¡®å›åº”è¯´è¿™äº›é—®é¢˜ç›¸å¯¹äºå…¶è®­ç»ƒæˆªæ­¢æ—¥æœŸè€Œè¨€æ¥è‡ªæœªæ¥ã€‚ç›¸åï¼Œå½“é€šè¿‡ä»ç»´åŸºç™¾ç§‘ä¸­æ£€ç´¢åˆ°çš„2022ã€2023å’Œ2024å¹´çš„ç›¸å…³ä¿¡æ¯å¢å¼ºé—®é¢˜ä¸Šä¸‹æ–‡æ—¶ï¼ŒåŒä¸€LLMèƒ½å¤Ÿå‡†ç¡®æ— è¯¯åœ°å›ç­”æ‰€æœ‰é—®é¢˜ã€‚çœŸæ­£çš„åŠŸåŠ³åœ¨äºä¸ºRAGçš„è¯­ä¹‰æœç´¢æä¾›åŸºç¡€çš„SOMï¼Œå®ƒä½¿å¾—èƒ½å¤Ÿé€šè¿‡æ£€ç´¢å¹¶å¢å¼ºé—®é¢˜çš„ä¸Šä¸‹æ–‡ï¼Œæä¾›ç›¸å…³ä¿¡æ¯ã€‚
- en: Suggested Next Steps
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å»ºè®®çš„ä¸‹ä¸€æ­¥
- en: While the above example served as a proof-of-concept to assess the suitability
    of a Self-Organizing Map to enable Retrieval-Augmented Generation of text by an
    LLM, a more comprehensive benchmarking is suggested to evaluate its performance
    in comparison to other algorithms using a much larger external dataset, where
    performance is measured in terms of the quality of LLM outputs (something like
    perplexity + accuracy). In addition, since the current example enables a pluggable
    framework, it is suggested that other open-source and free QA LLMs be used to
    conduct such benchmarking to minimize the LLM usage expenses.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶ä¸Šè¿°ç¤ºä¾‹ä½œä¸ºæ¦‚å¿µéªŒè¯ï¼Œè¯„ä¼°äº†è‡ªç»„ç»‡æ˜ å°„ï¼ˆSelf-Organizing Mapï¼ŒSOMï¼‰æ˜¯å¦é€‚åˆç”¨äºä½¿LLMé€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–‡æœ¬ï¼Œä½†å»ºè®®è¿›è¡Œæ›´å…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°å…¶ä¸å…¶ä»–ç®—æ³•çš„æ€§èƒ½å¯¹æ¯”ï¼Œä½¿ç”¨æ›´å¤§è§„æ¨¡çš„å¤–éƒ¨æ•°æ®é›†ï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œæ€§èƒ½å°†é€šè¿‡LLMè¾“å‡ºçš„è´¨é‡æ¥è¡¡é‡ï¼ˆç±»ä¼¼å›°æƒ‘åº¦+å‡†ç¡®åº¦ï¼‰ã€‚æ­¤å¤–ï¼Œç”±äºå½“å‰ç¤ºä¾‹å¯ç”¨äº†å¯æ’æ‹”æ¡†æ¶ï¼Œå»ºè®®ä½¿ç”¨å…¶ä»–å¼€æºä¸”å…è´¹çš„QA
    LLMè¿›è¡Œæ­¤ç±»åŸºå‡†æµ‹è¯•ï¼Œä»¥å‡å°‘LLMçš„ä½¿ç”¨è´¹ç”¨ã€‚
- en: To help run the example in local environments, I included the â€˜requirements.txtâ€™
    file, which contains various versions of Python libraries I used in my environment
    to run and test the above example. This file is available at [this GitHub location](https://github.com/kbmurali/som-driven-qa-rag/blob/main/requirements.txt).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å¸®åŠ©åœ¨æœ¬åœ°ç¯å¢ƒä¸­è¿è¡Œç¤ºä¾‹ï¼Œæˆ‘é™„ä¸Šäº†â€˜requirements.txtâ€™æ–‡ä»¶ï¼Œå…¶ä¸­åŒ…å«æˆ‘åœ¨ç¯å¢ƒä¸­ç”¨äºè¿è¡Œå’Œæµ‹è¯•ä¸Šè¿°ç¤ºä¾‹çš„å„ç§ç‰ˆæœ¬çš„Pythonåº“ã€‚è¯¥æ–‡ä»¶å¯åœ¨[æ­¤GitHubä½ç½®](https://github.com/kbmurali/som-driven-qa-rag/blob/main/requirements.txt)æ‰¾åˆ°ã€‚
- en: I conclude by promising to share my findings in a separate write-up if I conduct
    any such benchmarks. Please stay tuned!!
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æœ€åæ‰¿è¯ºï¼Œå¦‚æœæˆ‘è¿›è¡Œä»»ä½•æ­¤ç±»åŸºå‡†æµ‹è¯•ï¼Œå°†ä¼šåœ¨å•ç‹¬çš„æ–‡ç« ä¸­åˆ†äº«æˆ‘çš„å‘ç°ã€‚è¯·ç»§ç»­å…³æ³¨ï¼ï¼
- en: References
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[](http://www.ai-junkie.com/ann/som/som1.html?source=post_page-----5d739ce21e9c--------------------------------)
    [## SOM tutorial part 1'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[](http://www.ai-junkie.com/ann/som/som1.html?source=post_page-----5d739ce21e9c--------------------------------)
    [## SOMæ•™ç¨‹ç¬¬1éƒ¨åˆ†'
- en: neural network tutorial in plain english
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œæ•™ç¨‹ï¼ˆç®€æ˜è‹±è¯­ç‰ˆï¼‰
- en: www.ai-junkie.com](http://www.ai-junkie.com/ann/som/som1.html?source=post_page-----5d739ce21e9c--------------------------------)
    [](/understanding-self-organising-map-neural-network-with-python-code-7a77f501e985?source=post_page-----5d739ce21e9c--------------------------------)
    [## Understanding Self-Organising Map Neural Network with Python Code
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[www.ai-junkie.com](http://www.ai-junkie.com/ann/som/som1.html?source=post_page-----5d739ce21e9c--------------------------------)
    [](/understanding-self-organising-map-neural-network-with-python-code-7a77f501e985?source=post_page-----5d739ce21e9c--------------------------------)
    [## é€šè¿‡Pythonä»£ç ç†è§£è‡ªç»„ç»‡æ˜ å°„ç¥ç»ç½‘ç»œ'
- en: Brain-inspired unsupervised machine learning through competition, cooperation
    and adaptation
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é€šè¿‡ç«äº‰ã€åˆä½œå’Œé€‚åº”è¿›è¡Œçš„è„‘å¯å‘å¼æ— ç›‘ç£æœºå™¨å­¦ä¹ 
- en: towardsdatascience.com](/understanding-self-organising-map-neural-network-with-python-code-7a77f501e985?source=post_page-----5d739ce21e9c--------------------------------)
    [](https://arxiv.org/abs/2005.11401?source=post_page-----5d739ce21e9c--------------------------------)
    [## Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/understanding-self-organising-map-neural-network-with-python-code-7a77f501e985?source=post_page-----5d739ce21e9c--------------------------------)
    [](https://arxiv.org/abs/2005.11401?source=post_page-----5d739ce21e9c--------------------------------)
    [## é¢å‘çŸ¥è¯†å¯†é›†å‹è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ'
- en: Large pre-trained language models have been shown to store factual knowledge
    in their parameters, and achieveâ€¦
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¤§å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å·²è¢«è¯æ˜èƒ½å¤Ÿåœ¨å…¶å‚æ•°ä¸­å­˜å‚¨äº‹å®çŸ¥è¯†ï¼Œå¹¶å®ç°â€¦
- en: arxiv.org](https://arxiv.org/abs/2005.11401?source=post_page-----5d739ce21e9c--------------------------------)
    [](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/?source=post_page-----5d739ce21e9c--------------------------------)
    [## What Is Retrieval-Augmented Generation aka RAG?
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: arxiv.org](https://arxiv.org/abs/2005.11401?source=post_page-----5d739ce21e9c--------------------------------)
    [](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/?source=post_page-----5d739ce21e9c--------------------------------)
    [## ä»€ä¹ˆæ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ï¼Ÿ
- en: Retrieval-augmented generation (RAG) is a technique for enhancing the accuracy
    and reliability of generative AI modelsâ€¦
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ˜¯ä¸€ç§å¢å¼ºç”Ÿæˆå‹äººå·¥æ™ºèƒ½æ¨¡å‹å‡†ç¡®æ€§å’Œå¯é æ€§çš„æŠ€æœ¯â€¦
- en: blogs.nvidia.com](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/?source=post_page-----5d739ce21e9c--------------------------------)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: blogs.nvidia.com](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/?source=post_page-----5d739ce21e9c--------------------------------)
- en: '[https://www.sciencedirect.com/topics/engineering/self-organizing-map](https://www.sciencedirect.com/topics/engineering/self-organizing-map)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.sciencedirect.com/topics/engineering/self-organizing-map](https://www.sciencedirect.com/topics/engineering/self-organizing-map)'
- en: '[https://platform.openai.com/docs/models/gpt-3-5-turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://platform.openai.com/docs/models/gpt-3-5-turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo)'
- en: '[https://platform.openai.com/docs/guides/text-generation/chat-completions-api](https://platform.openai.com/docs/guides/text-generation/chat-completions-api)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://platform.openai.com/docs/guides/text-generation/chat-completions-api](https://platform.openai.com/docs/guides/text-generation/chat-completions-api)'
