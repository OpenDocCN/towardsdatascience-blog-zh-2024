- en: 'Short and Sweet: Enhancing LLM Performance with Constrained Chain-of-Thought'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简洁而甜美：通过约束性思维链提升LLM表现
- en: 原文：[https://towardsdatascience.com/short-and-sweet-enhancing-llm-performance-with-constrained-chain-of-thought-c4479361d995?source=collection_archive---------4-----------------------#2024-08-07](https://towardsdatascience.com/short-and-sweet-enhancing-llm-performance-with-constrained-chain-of-thought-c4479361d995?source=collection_archive---------4-----------------------#2024-08-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/short-and-sweet-enhancing-llm-performance-with-constrained-chain-of-thought-c4479361d995?source=collection_archive---------4-----------------------#2024-08-07](https://towardsdatascience.com/short-and-sweet-enhancing-llm-performance-with-constrained-chain-of-thought-c4479361d995?source=collection_archive---------4-----------------------#2024-08-07)
- en: '|LLM|PROMPT ENGINEERING|COT|REASONING|'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '|LLM|提示工程|COT|推理|'
- en: 'Sometimes few words are enough: reducing output length for increasing accuracy'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有时候几句话就足够了：缩短输出长度以提高准确性
- en: '[](https://salvatore-raieli.medium.com/?source=post_page---byline--c4479361d995--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page---byline--c4479361d995--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c4479361d995--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c4479361d995--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page---byline--c4479361d995--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://salvatore-raieli.medium.com/?source=post_page---byline--c4479361d995--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page---byline--c4479361d995--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c4479361d995--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c4479361d995--------------------------------)
    [萨尔瓦托雷·雷耶利](https://salvatore-raieli.medium.com/?source=post_page---byline--c4479361d995--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c4479361d995--------------------------------)
    ·9 min read·Aug 7, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c4479361d995--------------------------------)
    ·9分钟阅读·2024年8月7日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/46d3ba9bc12c1d09841c8f19a65b017b.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/46d3ba9bc12c1d09841c8f19a65b017b.png)'
- en: image created by the author using AI
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者使用AI创作的图片
- en: Brevity is a great charm of eloquence. — Marcus Tullius Cicero
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 简洁是雄辩的魅力所在。——马库斯·图利乌斯·西塞罗
- en: ''
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Brevity and conciseness are the parents of correction. — Hosea Ballou
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 简洁和精炼是正确的根源。——霍西亚·巴卢
- en: '[Large language models (LLMs)](https://github.com/SalvatoreRa/tutorial/blob/main/artificial%20intelligence/FAQ.md#:~:text=Large%20Language%20Models,-What%20is%20a)
    have shown interesting capabilities in the field of reasoning. With their use,
    a new field of application has emerged: [prompt engineering](https://github.com/SalvatoreRa/tutorial/blob/main/artificial%20intelligence/FAQ.md#:~:text=What%20is%20a%20prompt%3F%20What%20is%20prompt%20engineering%3F).
    In fact, interaction with these models occurs through the use of prompts, and
    for this reason, techniques have been developed to improve these capabilities
    of LLMs.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[大语言模型（LLMs）](https://github.com/SalvatoreRa/tutorial/blob/main/artificial%20intelligence/FAQ.md#:~:text=Large%20Language%20Models,-What%20is%20a)
    在推理领域展现了有趣的能力。随着其应用的推广，出现了一个新的应用领域： [提示工程](https://github.com/SalvatoreRa/tutorial/blob/main/artificial%20intelligence/FAQ.md#:~:text=What%20is%20a%20prompt%3F%20What%20is%20prompt%20engineering%3F)。实际上，与这些模型的交互是通过使用提示进行的，因此，开发出了许多技术来提升LLM的这些能力。'
- en: '[](https://pub.towardsai.net/prompt-engineering-to-leverage-in-context-learning-in-large-language-models-72296e1f09c3?source=post_page-----c4479361d995--------------------------------)
    [## Prompt Engineering to Leverage In-Context Learning in Large Language Models'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pub.towardsai.net/prompt-engineering-to-leverage-in-context-learning-in-large-language-models-72296e1f09c3?source=post_page-----c4479361d995--------------------------------)
    [## 提升大语言模型表现的提示工程：借力上下文学习'
- en: How to modify your text prompt to obtain the best from an LLM without training
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何修改文本提示，以便在不训练的情况下从LLM中获取最佳效果
- en: pub.towardsai.net](https://pub.towardsai.net/prompt-engineering-to-leverage-in-context-learning-in-large-language-models-72296e1f09c3?source=post_page-----c4479361d995--------------------------------)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: pub.towardsai.net](https://pub.towardsai.net/prompt-engineering-to-leverage-in-context-learning-in-large-language-models-72296e1f09c3?source=post_page-----c4479361d995--------------------------------)
- en: One of the most intriguing techniques is [chain-of-thought (CoT) prompting](https://github.com/SalvatoreRa/tutorial/blob/main/artificial%20intelligence/FAQ.md#:~:text=What%20is%20Chain%2Dof%2DThought%20(CoT)%3F);
    this technique increases correctness in reasoning problems and explains how the
    model arrives at the solution (or what reasoning errors it…
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最具吸引力的技术之一是[链式思维（CoT）提示](https://github.com/SalvatoreRa/tutorial/blob/main/artificial%20intelligence/FAQ.md#:~:text=What%20is%20Chain%2Dof%2DThought%20(CoT)%3F)；该技术提高了解题过程中的正确性，并解释了模型是如何得出解决方案的（或出现推理错误的原因）。
