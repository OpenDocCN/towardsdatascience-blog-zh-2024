- en: Advanced Retrieval Techniques in a World of 2M Token Context Windows, Part 1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在2百万标记上下文窗口的世界中，先进的检索技术，第1部分
- en: 原文：[https://towardsdatascience.com/advanced-retrieval-techniques-in-a-world-of-2m-token-context-windows-pt-1-2edc0266aabe?source=collection_archive---------10-----------------------#2024-07-15](https://towardsdatascience.com/advanced-retrieval-techniques-in-a-world-of-2m-token-context-windows-pt-1-2edc0266aabe?source=collection_archive---------10-----------------------#2024-07-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/advanced-retrieval-techniques-in-a-world-of-2m-token-context-windows-pt-1-2edc0266aabe?source=collection_archive---------10-----------------------#2024-07-15](https://towardsdatascience.com/advanced-retrieval-techniques-in-a-world-of-2m-token-context-windows-pt-1-2edc0266aabe?source=collection_archive---------10-----------------------#2024-07-15)
- en: Exploring RAG techniques to improve retrieval accuracy
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索RAG技术以提高检索准确性
- en: '[](https://medium.com/@meghanheintz?source=post_page---byline--2edc0266aabe--------------------------------)[![Meghan
    Heintz](../Images/9eaae6d3d8168086d83ff7100329c51f.png)](https://medium.com/@meghanheintz?source=post_page---byline--2edc0266aabe--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2edc0266aabe--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2edc0266aabe--------------------------------)
    [Meghan Heintz](https://medium.com/@meghanheintz?source=post_page---byline--2edc0266aabe--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@meghanheintz?source=post_page---byline--2edc0266aabe--------------------------------)[![Meghan
    Heintz](../Images/9eaae6d3d8168086d83ff7100329c51f.png)](https://medium.com/@meghanheintz?source=post_page---byline--2edc0266aabe--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2edc0266aabe--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2edc0266aabe--------------------------------)
    [Meghan Heintz](https://medium.com/@meghanheintz?source=post_page---byline--2edc0266aabe--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2edc0266aabe--------------------------------)
    ·5 min read·Jul 15, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2edc0266aabe--------------------------------)
    ·5分钟阅读·2024年7月15日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/06308257f080607db3569ca0a65e320f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06308257f080607db3569ca0a65e320f.png)'
- en: Visualising AI project launched by Google DeepMind. From [Unsplash](https://unsplash.com/photos/a-bunch-of-different-colored-sprinkles-on-a-pink-background-QhDs9x7o9Jc)
    image.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由Google DeepMind发起的AI项目可视化。图片来源：[Unsplash](https://unsplash.com/photos/a-bunch-of-different-colored-sprinkles-on-a-pink-background-QhDs9x7o9Jc)。
- en: First of all, do we still care about RAG (Retrieval Augmented Generation)?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 首先，我们还关心RAG（检索增强生成）吗？
- en: Gemini Pro can handle an astonishing 2M token context compared to the paltry
    15k we were amazed by when GPT-3.5 landed. Does that mean we no longer care about
    retrieval or RAG systems? Based on [Needle-in-a-Haystack benchmarks](https://huggingface.co/papers/2407.01370),
    the answer is that while the need is diminishing, especially for Gemini models,
    advanced retrieval techniques still significantly improve performance for most
    LLMs. Benchmarking results show that long context models perform well at surfacing
    specific insights. However, they struggle when a citation is required. **That
    makes retrieval techniques especially important for use cases where citation quality
    is important (think law, journalism, and medical applications among others).**
    These tend to be higher-value applications where lacking a citation makes the
    initial insight much less useful. Additionally, while the cost of long context
    models will likely decrease, augmenting shorter content window models with retrievers
    can be a cost-effective and lower latency path to serve the same use cases. It’s
    safe to say that RAG and retrieval will stick around a while longer but maybe
    you won’t get much bang for your buck implementing a naive RAG system.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Gemini Pro能够处理惊人的2百万标记上下文，而我们曾对GPT-3.5发布时的15000标记感到震惊。这是否意味着我们不再关心检索或RAG系统？根据[针尖对麦芒基准](https://huggingface.co/papers/2407.01370)，答案是，尽管需求在减少，特别是对于Gemini模型，先进的检索技术仍然能显著提升大多数LLM的性能。基准测试结果表明，长上下文模型在提取特定见解方面表现出色。然而，当需要引用时，它们却显得力不从心。**这使得检索技术在需要引用质量至关重要的应用场景中尤为重要（比如法律、新闻和医疗应用等）。**
    这些往往是高价值的应用，缺少引用会让最初的见解变得大打折扣。此外，虽然长上下文模型的成本可能会下降，但通过检索器增强短内容窗口模型，可以是一条具有成本效益且延迟较低的路径，用于服务相同的应用场景。可以放心地说，RAG和检索技术还会存在一段时间，但可能你不会从实施一个简单的RAG系统中获得太多回报。
- en: '![](../Images/6ae67db7cd66874b13651235affe7c51.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6ae67db7cd66874b13651235affe7c51.png)'
- en: 'From [Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems](https://huggingface.co/papers/2407.01370)
    by Laban, Fabbri, Xiong, Wu in 2024\. “Summary of a Haystack results of human
    performance, RAG systems, and Long-Context LLMs. Results are reported using three
    metrics: Coverage (left), Citation (center), and Joint (right) scores. Full corresponds
    to model performance when inputting the entire Haystack, whereas Rand, Vect, LongE,
    KWs, RR3, Orac correspond to retrieval components RAG systems. Models ranked by
    Oracle Joint Score. For each model, #Wb report the average number of words per
    bullet point.”'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '来自[《Haystack总结：对长上下文LLM和RAG系统的挑战》（Summary of a Haystack: A Challenge to Long-Context
    LLMs and RAG Systems）](https://huggingface.co/papers/2407.01370)的2024年Laban、Fabbri、Xiong、Wu的研究。“Haystack结果总结，包括人类表现、RAG系统和长上下文LLM。使用三种指标报告结果：覆盖率（左）、引用（中）、联合（右）分数。Full表示输入整个Haystack时的模型表现，而Rand、Vect、LongE、KWs、RR3、Orac表示RAG系统中的检索组件。模型按Oracle联合分数排名。对于每个模型，#Wb报告每个要点的平均字数。”'
- en: So what are the retrieval techniques we should be implementing?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 那么我们应该实现哪些检索技术呢？
- en: Advanced RAG covers a range of techniques but broadly they fall under the umbrella
    of pre-retrieval query rewriting and post-retrieval re-ranking. Let’s dive in
    and learn something about each of them.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 高级RAG涵盖了一系列技术，但大体上它们都属于检索前查询重写和检索后重排序的范畴。让我们深入了解并学习每种技术。
- en: Pre-Retrieval — Query Rewriting
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检索前 — 查询重写
- en: 'Q: “What is the meaning of life?”'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 问： “生命的意义是什么？”
- en: 'A: “42”'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 答： “42”
- en: Question and answer asymmetry is a huge issue in RAG systems. A typical approach
    to simpler RAG systems is to compare the cosine similarity of the query and document
    embedding. This works when the question is nearly restated in the answer, “What’s
    Meghan’s favorite animal?”, “Meghan’s favorite animal is the giraffe.”, but we
    are rarely that lucky.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 问答不对称性是RAG系统中的一个巨大问题。对于较简单的RAG系统，一种典型的方法是比较查询与文档嵌入的余弦相似度。当问题几乎在回答中重述时，这种方法有效，例如：“梅根最喜欢的动物是什么？”，“梅根最喜欢的动物是长颈鹿。”但我们很少那么幸运。
- en: 'Here are a few techniques that can overcome this:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些可以克服此问题的技术：
- en: Rewrite-Retrieve-Read
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重写-检索-阅读
- en: The nomenclature “Rewrite-Retrieve-Read” originated from a [paper](https://arxiv.org/pdf/2305.14283)
    from the Microsoft Azure team in 2023 (although given how intuitive the technique
    is it had been used for a while). In this study, an LLM would rewrite a user query
    into a search engine-optimized query before fetching relevant context to answer
    the question.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: “重写-检索-阅读”的命名来自微软Azure团队于2023年发表的[一篇论文](https://arxiv.org/pdf/2305.14283)（尽管考虑到这一技术的直观性，它已经使用了一段时间）。在这项研究中，LLM会在获取相关上下文以回答问题之前，将用户查询重写为优化搜索引擎的查询。
- en: The key example was how this query, *“What profession do Nicholas Ray and Elia
    Kazan have in common?”* should be broken down into two queries, *“Nicholas Ray
    profession”* and *“Elia Kazan profession”*. This allows for better results because
    it’s unlikely that a single document would contain the answer to both questions.
    By splitting the query into two the retriever can more effectively retrieve relevant
    documents.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关键的例子是如何将这个查询，*“尼古拉斯·雷和埃利亚·卡赞共有的职业是什么？”* 分解为两个查询，*“尼古拉斯·雷的职业”* 和 *“埃利亚·卡赞的职业”*。这样可以获得更好的结果，因为单个文档不太可能同时包含两个问题的答案。通过将查询拆分为两个，检索器可以更有效地检索相关文档。
- en: '![](../Images/f37621b4a81d6edc33b6da094587402c.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f37621b4a81d6edc33b6da094587402c.png)'
- en: '[From Query Rewriting for Retrieval-Augmented Large Language Models](https://arxiv.org/pdf/2305.14283)
    by Ma, Gong, He, Zhao, & Duan in 2023 “(a) standard retrieve-then-read method,
    (b) LLM as a query rewriter for rewrite-retrieve-read pipeline, and (c ) trainable
    rewriter.”'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[《用于增强检索的大型语言模型的查询重写》（From Query Rewriting for Retrieval-Augmented Large Language
    Models）](https://arxiv.org/pdf/2305.14283)，由Ma、Gong、He、Zhao和Duan于2023年撰写。“(a)
    标准的先检索后阅读方法，(b) LLM作为查询重写器，用于重写-检索-阅读管道，(c) 可训练的重写器。”'
- en: Rewriting can also help overcome issues that arise from “distracted prompting”.
    Or instances where the user query has mixed concepts in their prompt and taking
    an embedding directly would result in nonsense. For example, “*Great, thanks for
    telling me who the Prime Minister of the UK is. Now tell me who the President
    of France is”* would be rewritten like *“current French president”.* This can
    help make your application more robust to a wider range of users as some will
    think a lot about how to optimally word their prompts, while others might have
    different norms.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 重写也有助于克服“分心提示”所带来的问题。比如用户查询在提示中混合了多个概念，直接采用嵌入式方法会导致毫无意义的结果。例如，“*太好了，谢谢你告诉我英国首相是谁。现在告诉我法国总统是谁*”可以被重写为*“现任法国总统”*。这可以帮助你的应用程序在面对不同的用户时更具鲁棒性，因为有些用户会深思熟虑如何优化他们的提示，而另一些用户可能有不同的习惯。
- en: Query Expansion
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查询扩展
- en: In query expansion with LLMs, the initial query can be rewritten into multiple
    reworded questions or decomposed into subquestions. Ideally, by expanding the
    query into several options, the chances of lexical overlap increase between the
    initial query and the correct document in your storage component.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用大语言模型（LLMs）进行查询扩展时，初始查询可以被重写为多个改写后的问题，或被拆解为子问题。理想情况下，通过将查询扩展为多个选项，可以增加初始查询与存储组件中正确文档之间的词汇重叠几率。
- en: Query expansion is a concept that predates the widespread usage of LLMs. Pseudo
    Relevance Feedback (PRF) is a technique that inspired some LLM researchers. In
    PRF, the top-ranked documents from an initial search to identify and weight new
    query terms. With LLMs, we rely on the creative and generative capabilities of
    the model to find new query terms. This is beneficial because LLMs are not restricted
    to the initial set of documents and can generate expansion terms not covered by
    traditional methods.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 查询扩展是一个早于大语言模型（LLMs）广泛应用的概念。伪相关反馈（PRF）是一种启发了部分LLM研究者的技术。在PRF中，通过初始搜索中排名靠前的文档来识别和加权新的查询词条。而在使用LLM时，我们依赖模型的创造性和生成能力来寻找新的查询词条。这是有益的，因为LLM不局限于初始的文档集合，可以生成传统方法无法覆盖的扩展词条。
- en: '[Corpus-Steered Query Expansion (CSQE)](https://arxiv.org/abs/2402.18031) is
    a method that marries the traditional PRF approach with the LLMs’ generative capabilities.
    The initially retrieved documents are fed back to the LLM to generate new query
    terms for the search. This technique can be especially performant for queries
    for which LLMs lacks subject knowledge.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[语料库驱动的查询扩展（CSQE）](https://arxiv.org/abs/2402.18031)是一种结合了传统伪相关反馈（PRF）方法和大语言模型生成能力的技术。最初检索到的文档被反馈给LLM，以生成新的查询词条用于搜索。这项技术在LLM缺乏某一领域知识的查询中表现尤为出色。'
- en: '![](../Images/f059b785162916053ff89d6ad95c4a65.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f059b785162916053ff89d6ad95c4a65.png)'
- en: '[From Corpus-Steered Query Expansion with Large Language Models](https://arxiv.org/abs/2402.18031)
    by Lei , Cao, Zhou , Shen, Yates in 2024\. “Overview of CSQE. Given a query Biology
    definition and the top-2 retrieved documents, CSQE utilizes an LLM to identify
    relevant document 1 and extract the key sentences from document 1 that contribute
    to the relevance. The query is then expanded by both these corpus-originated texts
    and LLM-knowledge empowered expansions (i.e., hypothetical documents that answer
    the query) to obtain the final results.”'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[来自大语言模型驱动的语料库查询扩展](https://arxiv.org/abs/2402.18031)，作者：Lei，Cao，Zhou，Shen，Yates，2024年。“CSQE概述：给定一个查询生物学定义和检索到的前两篇文档，CSQE利用LLM来识别相关文档1，并从文档1中提取对相关性贡献的关键句子。然后，查询通过这些来源于语料库的文本和LLM增强的扩展（即假设的回答查询的文档）进行扩展，以获得最终结果。”'
- en: There are limitations to both LLM-based query expansion and its predecessors
    like PRF. The most glaring of which is the assumption that the LLM generated terms
    are relevant or that the top ranked results are relevant. God forbid I am trying
    to find information about the Australian journalist Harry Potter instead of the
    famous boy wizard. Both techniques would further pull my query away from the less
    popular query subject to the more popular one making edge case queries less effective.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是基于LLM的查询扩展，还是其前身如伪相关反馈（PRF）方法，都有一定的局限性。其中最显著的局限性是假设LLM生成的词条是相关的，或者假设排名靠前的结果是相关的。天啊，如果我在寻找澳大利亚记者哈利·波特的信息，而不是著名的男巫哈利·波特，结果会怎样呢？这两种技术都会将我的查询从较不流行的查询主题拉向更流行的主题，使得边缘案例的查询效果变差。
- en: Hypothetical Query Indexes
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 假设查询索引
- en: Another way to reduce the asymmetry between questions and documents is to index
    documents with a set of LLM-generated hypothetical questions. For a given document,
    the LLM can generate questions that *could* be answered by the document. Then
    during the retrieval step, the user’s query embedding is compared to the hypothetical
    question embeddings versus the document embeddings.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种减少问题和文档之间不对称的方法是使用一组由LLM生成的假设问题对文档进行索引。对于给定的文档，LLM可以生成可能由该文档回答的问题。然后，在检索步骤中，用户的查询嵌入会与假设问题嵌入和文档嵌入进行比较。
- en: This means that we don’t need to embed the original document chunk, instead,
    we can assign the chunk a document ID and store that as metadata on the hypothetical
    question document. Generating a document ID means there is much less overhead
    when mapping many questions to one document.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们不需要嵌入原始文档片段，而是可以为该片段分配一个文档ID，并将其作为元数据存储在假设的问题文档中。生成文档ID意味着在将多个问题映射到一个文档时，开销会小得多。
- en: The clear downside to this approach is your system will be limited by the creativity
    and volume of questions you store.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的明显缺点是，你的系统将受到存储的问题的创造力和数量的限制。
- en: Hypothetical Document Embeddings — HyDE
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 假设文档嵌入 — HyDE
- en: '[HyDE](https://arxiv.org/abs/2212.10496) is the opposite of Hypothetical Query
    Indexes. Instead of generating hypothetical questions, the LLM is asked to generate
    a hypothetical document that *could* answer the question, and the embedding of
    that generated document is used to search against the real documents. The real
    document is then used to generate the response. This method showed strong improvements
    over other contemporary retriever methods when it was first introduced in 2022.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[HyDE](https://arxiv.org/abs/2212.10496)与假设查询索引相反。与生成假设问题不同，LLM被要求生成一个假设文档，该文档可能回答该问题，生成的文档嵌入被用于与真实文档进行检索。然后，真实文档用于生成回应。该方法在2022年首次推出时，相比其他当时的检索方法表现出显著的改进。'
- en: We use this concept at Dune for our natural language to SQL product. By rewriting
    user prompts as a possible caption or title for a chart that would answer the
    question, we are better able to retrieve SQL queries that can serve as context
    for the LLM to write a new query.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在Dune的自然语言到SQL产品中使用了这一概念。通过将用户的提示重写为可能回答问题的图表标题或说明，我们能够更好地检索可以作为LLM编写新查询上下文的SQL查询。
- en: '![](../Images/84cc7146e1c968806f8ef8f6a712c95f.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84cc7146e1c968806f8ef8f6a712c95f.png)'
- en: '[From Precise Zero-Shot Dense Retrieval without Relevance Labels](https://arxiv.org/abs/2212.10496)
    by Gao, Ma, Lin, Callan in 2022\. “An illustration of the HyDE model. Documents
    snippets are shown. HyDE serves all types of queries without changing the underlying
    GPT-3 and Contriever/mContriever models.”'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[《无相关标签的精确零样本密集检索》](https://arxiv.org/abs/2212.10496)，作者：Gao、Ma、Lin、Callan，发表于2022年。“HyDE模型的示意图。展示了文档片段。HyDE能处理所有类型的查询，而不改变底层的GPT-3和Contriever/mContriever模型。”'
- en: Stay tuned for part 2 on reranking methods.
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 敬请关注第二部分关于重新排序方法的内容。
