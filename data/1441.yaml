- en: 'Building LLM Apps: A Clear Step-By-Step Guide'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ„å»º LLM åº”ç”¨ï¼šæ¸…æ™°çš„é€æ­¥æŒ‡å—
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/building-llm-apps-a-clear-step-by-step-guide-1fe1e6ef60fd?source=collection_archive---------0-----------------------#2024-06-10](https://towardsdatascience.com/building-llm-apps-a-clear-step-by-step-guide-1fe1e6ef60fd?source=collection_archive---------0-----------------------#2024-06-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/building-llm-apps-a-clear-step-by-step-guide-1fe1e6ef60fd?source=collection_archive---------0-----------------------#2024-06-10](https://towardsdatascience.com/building-llm-apps-a-clear-step-by-step-guide-1fe1e6ef60fd?source=collection_archive---------0-----------------------#2024-06-10)
- en: 'Comprehensive Steps for Building LLM-Native Apps: From Initial Idea to Experimentation,
    Evaluation, and Productization'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ„å»º LLM åŸç”Ÿåº”ç”¨çš„å…¨é¢æ­¥éª¤ï¼šä»æœ€åˆçš„æ„æƒ³åˆ°å®éªŒã€è¯„ä¼°å’Œäº§å“åŒ–
- en: '[](https://medium.com/@almogbaku?source=post_page---byline--1fe1e6ef60fd--------------------------------)[![Almog
    Baku](../Images/3ac36986f6ca0ba56c8edced6ec7dd07.png)](https://medium.com/@almogbaku?source=post_page---byline--1fe1e6ef60fd--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1fe1e6ef60fd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1fe1e6ef60fd--------------------------------)
    [Almog Baku](https://medium.com/@almogbaku?source=post_page---byline--1fe1e6ef60fd--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@almogbaku?source=post_page---byline--1fe1e6ef60fd--------------------------------)[![Almog
    Baku](../Images/3ac36986f6ca0ba56c8edced6ec7dd07.png)](https://medium.com/@almogbaku?source=post_page---byline--1fe1e6ef60fd--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1fe1e6ef60fd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1fe1e6ef60fd--------------------------------)
    [Almog Baku](https://medium.com/@almogbaku?source=post_page---byline--1fe1e6ef60fd--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1fe1e6ef60fd--------------------------------)
    Â·12 min readÂ·Jun 10, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1fe1e6ef60fd--------------------------------)
    Â·12åˆ†é’Ÿé˜…è¯»Â·2024å¹´6æœˆ10æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Large Language Models (LLMs) are swiftly becoming a cornerstone of modern AI.
    Yet, there are **no established best practices**, and often, pioneers are left
    with **no clear roadmap**, needing to reinvent the wheel or getting stuck.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£è¿…é€Ÿæˆä¸ºç°ä»£äººå·¥æ™ºèƒ½çš„åŸºçŸ³ã€‚ç„¶è€Œï¼Œç›®å‰å¹¶**æ²¡æœ‰æˆç†Ÿçš„æœ€ä½³å®è·µ**ï¼Œè€Œä¸”è®¸å¤šå…ˆé©±è€…å¸¸å¸¸é¢ä¸´**æ²¡æœ‰æ¸…æ™°è·¯çº¿å›¾**çš„å›°å¢ƒï¼Œä¸å¾—ä¸é‡æ–°å‘æ˜è½®å­ï¼Œæˆ–è€…é™·å…¥å›°å¢ƒã€‚
- en: Over the past two years, Iâ€™ve helped organizations leverage LLMs to build innovative
    applications. Through this experience, I developed a ***battle-tested method***
    for creating innovative solutions (shaped by insights from the [LLM.org.il](https://llm.org.il)
    community), which Iâ€™ll share in this article.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿‡å»çš„ä¸¤å¹´é‡Œï¼Œæˆ‘å¸®åŠ©ç»„ç»‡åˆ©ç”¨ LLM æ„å»ºåˆ›æ–°åº”ç”¨ã€‚é€šè¿‡è¿™äº›ç»éªŒï¼Œæˆ‘å¼€å‘äº†ä¸€ç§***ç»å—è€ƒéªŒçš„æ–¹æ³•***ï¼Œç”¨äºåˆ›å»ºåˆ›æ–°è§£å†³æ–¹æ¡ˆï¼ˆè¿™äº›æ–¹æ³•å—åˆ°[LLM.org.il](https://llm.org.il)ç¤¾åŒºçš„è§è§£å½±å“ï¼‰ï¼Œæˆ‘å°†åœ¨æœ¬æ–‡ä¸­åˆ†äº«è¿™äº›æ–¹æ³•ã€‚
- en: This guide provides a *clear roadmap* for navigating the complex landscape of
    LLM-native development. Youâ€™ll learn how to move from ideation to experimentation,
    evaluation, and productization, unlocking your potential to create groundbreaking
    applications.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—æä¾›äº†ä¸€ä¸ª*æ¸…æ™°çš„è·¯çº¿å›¾*ï¼Œå¸®åŠ©ä½ åœ¨ LLM åŸç”Ÿå¼€å‘çš„å¤æ‚é¢†åŸŸä¸­æ‰¾åˆ°å‰è¿›çš„æ–¹å‘ã€‚ä½ å°†å­¦ä¹ å¦‚ä½•ä»æ„æ€åˆ°å®éªŒã€è¯„ä¼°å’Œäº§å“åŒ–ï¼Œé‡Šæ”¾ä½ çš„æ½œåŠ›ï¼Œåˆ›é€ å¼€åˆ›æ€§çš„åº”ç”¨ã€‚
- en: '![](../Images/36d91b22a5b41e77c6e555b9655839f9.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36d91b22a5b41e77c6e555b9655839f9.png)'
- en: (Created with Dall-E3)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ï¼ˆç”± Dall-E3 åˆ›å»ºï¼‰
- en: Why a Standardized Process is Essential
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆæ ‡å‡†åŒ–æµç¨‹è‡³å…³é‡è¦
- en: The LLM space is so dynamic that sometimes, we hear about new groundbreaking
    innovations day after day. This is quite exhilarating but also very chaotic â€”
    you may find yourself lost in the process, wondering what to do or how to bring
    your novel idea to life.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: LLM é¢†åŸŸå˜åŒ–å¦‚æ­¤ä¹‹å¿«ï¼Œæœ‰æ—¶æˆ‘ä»¬æ¯å¤©éƒ½ä¼šå¬åˆ°æ–°çš„çªç ´æ€§åˆ›æ–°ã€‚è¿™è™½ç„¶ä»¤äººæŒ¯å¥‹ï¼Œä½†ä¹Ÿå……æ»¡æ··ä¹±â€”â€”ä½ å¯èƒ½ä¼šå‘ç°è‡ªå·±è¿·å¤±åœ¨è¿‡ç¨‹å½“ä¸­ï¼Œä¸çŸ¥é“è¯¥åšä»€ä¹ˆï¼Œæˆ–è€…å¦‚ä½•å°†ä½ çš„æ–°åˆ›æ„ä»˜è¯¸å®è·µã€‚
- en: Long story short, if you are an **AI Innovator** (a manager or a practitioner)
    who wants to build LLM-native apps effectively, **this is for you**.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: é•¿è¯çŸ­è¯´ï¼Œå¦‚æœä½ æ˜¯ä¸€ä¸ª**AI åˆ›æ–°è€…**ï¼ˆæ— è®ºæ˜¯ç®¡ç†è€…è¿˜æ˜¯ä»ä¸šè€…ï¼‰ï¼Œæƒ³è¦æœ‰æ•ˆåœ°æ„å»º LLM åŸç”Ÿåº”ç”¨ï¼Œ**è¿™ç¯‡æ–‡ç« é€‚åˆä½ **ã€‚
- en: 'Implementing a standardized process helps kick off new projects and offers
    several key benefits:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å®æ–½æ ‡å‡†åŒ–æµç¨‹æœ‰åŠ©äºå¯åŠ¨æ–°é¡¹ç›®ï¼Œå¹¶å¸¦æ¥å‡ ä¸ªå…³é”®çš„å¥½å¤„ï¼š
- en: '**Standardize the process â€”** A standardized process helps align the team members
    and ensures a smooth new member onboarding process (especially in this chaos).'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ ‡å‡†åŒ–æµç¨‹ â€”â€”** æ ‡å‡†åŒ–æµç¨‹æœ‰åŠ©äºåè°ƒå›¢é˜Ÿæˆå‘˜ï¼Œç¡®ä¿é¡ºåˆ©çš„æ–°äººå…¥èŒè¿‡ç¨‹ï¼ˆç‰¹åˆ«æ˜¯åœ¨è¿™ç§æ··ä¹±ä¸­ï¼‰ã€‚'
- en: '**Defines clear milestones** â€” A straightforward way to track your work, measure
    it, and make sure you''re on the right path'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å®šä¹‰æ¸…æ™°çš„é‡Œç¨‹ç¢‘** â€”â€” ä¸€ç§ç®€å•çš„æ–¹æ³•æ¥è·Ÿè¸ªä½ çš„å·¥ä½œã€è¡¡é‡å®ƒå¹¶ç¡®ä¿ä½ èµ°åœ¨æ­£ç¡®çš„é“è·¯ä¸Šã€‚'
- en: '**Identify decision points** â€” LLM-native development is full of unknowns and
    "small experimentation" [see below]. Clear decision points make it easy to mitigate
    our risk and always stay lean with our development effort.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ç¡®å®šå†³ç­–ç‚¹** â€”â€” LLMåŸç”Ÿå¼€å‘å……æ»¡äº†æœªçŸ¥å’Œâ€œå°è§„æ¨¡å®éªŒâ€[è§ä¸‹æ–‡]ã€‚æ˜ç¡®çš„å†³ç­–ç‚¹ä½¿æˆ‘ä»¬èƒ½å¤Ÿé™ä½é£é™©ï¼Œå¹¶å§‹ç»ˆä¿æŒç²¾ç›Šå¼€å‘ã€‚'
- en: The Essential Skills of an LLM Engineer
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMå·¥ç¨‹å¸ˆçš„å¿…å¤‡æŠ€èƒ½
- en: 'Unlike any other established role in Software R&D, LLM-native development absolutely
    requires a new role: the **LLM Engineer** or the **AI Engineer.**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è½¯ä»¶ç ”å‘ä¸­çš„ä»»ä½•å…¶ä»–æ—¢å®šè§’è‰²ä¸åŒï¼ŒLLMåŸç”Ÿå¼€å‘ç»å¯¹éœ€è¦ä¸€ä¸ªæ–°çš„è§’è‰²ï¼š**LLMå·¥ç¨‹å¸ˆ** æˆ– **AIå·¥ç¨‹å¸ˆ**ã€‚
- en: 'The LLM Engineer is a unique hybrid creature that involves skills from different
    (established) roles:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: LLMå·¥ç¨‹å¸ˆæ˜¯ä¸€ä¸ªç‹¬ç‰¹çš„æ··åˆå‹è§’è‰²ï¼Œæ¶‰åŠä¸åŒï¼ˆæ—¢å®šï¼‰è§’è‰²çš„æŠ€èƒ½ï¼š
- en: '**Software Engineering skillsâ€”**Like most SWEs, most of the work involves putting
    the Lego pieces together and gluing everything together.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è½¯ä»¶å·¥ç¨‹æŠ€èƒ½â€”â€”** å°±åƒå¤§å¤šæ•°è½¯ä»¶å·¥ç¨‹å¸ˆä¸€æ ·ï¼Œå¤§éƒ¨åˆ†å·¥ä½œæ˜¯å°†ä¹é«˜ç§¯æœ¨æ‹¼å‡‘åœ¨ä¸€èµ·ï¼Œå¹¶å°†æ‰€æœ‰ä¸œè¥¿ç²˜åˆåœ¨ä¸€èµ·ã€‚'
- en: '**Research skills** â€”Properly understanding the LLM-native experimental nature
    is ***essential.*** While building "cool demo apps" is pretty accessible, the
    distance between a "cool demo" and a practical solution requires experimentation
    and agility.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç ”ç©¶æŠ€èƒ½** â€”â€” æ­£ç¡®ç†è§£LLMåŸç”Ÿå®éªŒçš„æ€§è´¨æ˜¯***è‡³å…³é‡è¦çš„ã€‚*** å°½ç®¡æ„å»ºâ€œé…·ç‚«çš„æ¼”ç¤ºåº”ç”¨â€ç›¸å¯¹å®¹æ˜“ï¼Œä½†â€œé…·ç‚«æ¼”ç¤ºâ€å’Œå®é™…è§£å†³æ–¹æ¡ˆä¹‹é—´çš„è·ç¦»éœ€è¦å®éªŒå’Œæ•æ·æ€§ã€‚'
- en: '**Deep business/product understanding â€”** Due to the fragility of the models,
    it''s essential to understand the business goals and procedures rather than sticking
    to the architecture we defined. The ability to model a manual process is a golden
    skill for LLM Engineers.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ·±å…¥ç†è§£ä¸šåŠ¡/äº§å“â€”â€”** ç”±äºæ¨¡å‹çš„è„†å¼±æ€§ï¼Œç†è§£ä¸šåŠ¡ç›®æ ‡å’Œæµç¨‹è‡³å…³é‡è¦ï¼Œè€Œä¸æ˜¯åšæŒæˆ‘ä»¬å®šä¹‰çš„æ¶æ„ã€‚èƒ½å¤Ÿå»ºæ¨¡æ‰‹åŠ¨æµç¨‹æ˜¯LLMå·¥ç¨‹å¸ˆçš„é‡‘ç‰ŒæŠ€èƒ½ã€‚'
- en: While writing this, LLM Engineering is still brand new, and **hiring can be
    very challenging**. It can be a good idea to look for candidates with a background
    in backend/data engineering or data science.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å†™è¿™ç¯‡æ–‡ç« æ—¶ï¼ŒLLMå·¥ç¨‹å­¦ä»ç„¶æ˜¯å…¨æ–°çš„é¢†åŸŸï¼Œ**æ‹›è˜å¯èƒ½éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§**ã€‚å¯»æ‰¾æœ‰åç«¯/æ•°æ®å·¥ç¨‹æˆ–æ•°æ®ç§‘å­¦èƒŒæ™¯çš„å€™é€‰äººå¯èƒ½æ˜¯ä¸€ä¸ªä¸é”™çš„ä¸»æ„ã€‚
- en: '*Software Engineers* might expect a *smoother transition*, as the experimentation
    process is much more "engineer-y" and not that "scientific" (compared to traditional
    data science work). That being said, I''ve seen many *Data Scientists* do this
    transition as well. As long as you''re okay with the fact that you''ll have to
    embrace new soft skills, you''re on the right path!'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*è½¯ä»¶å·¥ç¨‹å¸ˆ* å¯èƒ½ä¼šæœŸæœ›ä¸€ä¸ª *æ›´å¹³æ»‘çš„è¿‡æ¸¡*ï¼Œå› ä¸ºå®éªŒè¿‡ç¨‹æ›´åŠ â€œå·¥ç¨‹åŒ–â€ï¼Œè€Œä¸åƒä¼ ç»Ÿçš„æ•°æ®ç§‘å­¦å·¥ä½œé‚£æ ·â€œç§‘å­¦â€ã€‚è¯è™½å¦‚æ­¤ï¼Œæˆ‘ä¹Ÿè§è¿‡è®¸å¤š*æ•°æ®ç§‘å­¦å®¶*æˆåŠŸå®Œæˆè¿™ä¸ªè¿‡æ¸¡ã€‚åªè¦ä½ èƒ½æ¥å—ä½ å¿…é¡»æ‹¥æŠ±æ–°çš„è½¯æŠ€èƒ½è¿™ä¸€äº‹å®ï¼Œä½ å°±èµ°åœ¨äº†æ­£ç¡®çš„é“è·¯ä¸Šï¼'
- en: The Key Elements of LLM-Native Development
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMåŸç”Ÿå¼€å‘çš„å…³é”®è¦ç´ 
- en: Unlike classical backend apps (such as CRUD), there are no step-by-step recipes
    here. Like everything else in "AI," LLM-native apps require a **research and experimentation
    *mindset*.**
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ä¼ ç»Ÿçš„åç«¯åº”ç”¨ï¼ˆå¦‚CRUDï¼‰ä¸åŒï¼Œè¿™é‡Œæ²¡æœ‰ä¸€æ­¥æ­¥çš„æ“ä½œæŒ‡å—ã€‚åƒâ€œAIâ€ä¸­çš„å…¶ä»–æ‰€æœ‰äº‹ç‰©ä¸€æ ·ï¼ŒLLMåŸç”Ÿåº”ç”¨éœ€è¦ä¸€ä¸ª**ç ”ç©¶å’Œå®éªŒçš„*æ€ç»´æ–¹å¼*ã€‚**
- en: To tame the beast, you must divide and conquer by splitting your work into smaller
    experiments, trying some of them, and selecting the most promising experiment.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è¦é©¯æœè¿™åªé‡å…½ï¼Œä½ å¿…é¡»é€šè¿‡å°†å·¥ä½œæ‹†åˆ†æˆæ›´å°çš„å®éªŒæ¥å®ç°åˆ†è€Œæ²»ä¹‹ï¼Œå°è¯•å…¶ä¸­ä¸€äº›å®éªŒï¼Œå¹¶é€‰æ‹©æœ€æœ‰å‰æ™¯çš„å®éªŒã€‚
- en: I can't emphasize enough the importance of the *research mindset*.That means
    you might invest the time to explore a research vector and find out that it's
    "not possible," "not good enough," or "not worth it." That's totally okay â€” it
    means you're on the right track.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ— æ³•å¼ºè°ƒ*ç ”ç©¶å¿ƒæ€*çš„é‡è¦æ€§ã€‚ è¿™æ„å‘³ç€ä½ å¯èƒ½ä¼šæŠ•å…¥æ—¶é—´å»æ¢ç´¢ä¸€ä¸ªç ”ç©¶æ–¹å‘ï¼Œç»“æœå‘ç°å®ƒâ€œä¸å¯è¡Œâ€ã€â€œä¸å¤Ÿå¥½â€æˆ–â€œä¸å€¼å¾—â€ã€‚å®Œå…¨æ²¡å…³ç³»â€”â€”è¿™æ„å‘³ç€ä½ åœ¨æ­£ç¡®çš„è½¨é“ä¸Šã€‚
- en: '![](../Images/d80576c3df986e4e82ba49558a9d3111.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d80576c3df986e4e82ba49558a9d3111.png)'
- en: Experimenting with LLMs is the only way to build LLM-native apps (and avoid
    the snakes in the way) (Created with Dall-E3)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å®éªŒLLMæ˜¯æ„å»ºLLMåŸç”Ÿåº”ç”¨çš„å”¯ä¸€é€”å¾„ï¼ˆå¹¶ä¸”é¿å…é€”ä¸­å‡ºç°éšœç¢ï¼‰ï¼ˆç”±Dall-E3åˆ›å»ºï¼‰
- en: 'Embracing Experimentation: The Heart of the Process'
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ‹¥æŠ±å®éªŒï¼šè¿‡ç¨‹çš„æ ¸å¿ƒ
- en: Sometimes, your "experiment" will fail, then you slightly pivot your work, and
    this other experiment succeeded much better.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰æ—¶å€™ï¼Œä½ çš„â€œå®éªŒâ€ä¼šå¤±è´¥ï¼Œç„¶åä½ ä¼šç¨å¾®è°ƒæ•´å·¥ä½œï¼Œç»“æœå¦ä¸€ä¸ªå®éªŒä¼šæˆåŠŸå¾—å¤šã€‚
- en: That's precisely why, *before* designing our endgame solution, we must start
    simple and hedge our risks.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ­£æ˜¯ä¸ºä»€ä¹ˆï¼Œåœ¨è®¾è®¡æˆ‘ä»¬çš„ç»ˆæè§£å†³æ–¹æ¡ˆä¹‹å‰ï¼Œæˆ‘ä»¬å¿…é¡»ä»ç®€å•å¼€å§‹å¹¶å¯¹æˆ‘ä»¬çš„é£é™©è¿›è¡Œå¯¹å†²ã€‚
- en: '**Define a "budget" or timeframe.** Let''s see what we can do in X weeks and
    then decide how or if to continue. Usually, 2â€“4 weeks to understand basic PoC
    will be sufficient. If it looks promising â€” continue investing resources to improve
    it.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å®šä¹‰â€œé¢„ç®—â€æˆ–æ—¶é—´æ¡†æ¶ã€‚**æˆ‘ä»¬çœ‹çœ‹åœ¨Xå‘¨å†…èƒ½åšäº›ä»€ä¹ˆï¼Œç„¶åå†³å®šæ˜¯å¦ç»§ç»­ã€‚é€šå¸¸ï¼Œ2åˆ°4å‘¨çš„æ—¶é—´æ¥ç†è§£åŸºæœ¬çš„PoCå°±è¶³å¤Ÿäº†ã€‚å¦‚æœçœ‹èµ·æ¥æœ‰å¸Œæœ›â€”â€”ç»§ç»­æŠ•å…¥èµ„æºä»¥æ”¹å–„å®ƒã€‚'
- en: '**Experimentâ€”**Whether you choose a bottom-up or top-down approach for your
    experimentation phase, your goal is to maximize the result succession rate. By
    the end of the first experimentation iteration, you should have some PoC (that
    stakeholders can play with) and a baseline you achieved.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å®éªŒâ€”**æ— è®ºä½ åœ¨å®éªŒé˜¶æ®µé€‰æ‹©è‡ªä¸‹è€Œä¸Šè¿˜æ˜¯è‡ªä¸Šè€Œä¸‹çš„æ–¹æ³•ï¼Œä½ çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–ç»“æœæˆåŠŸç‡ã€‚åœ¨ç¬¬ä¸€æ¬¡å®éªŒè¿­ä»£ç»“æŸæ—¶ï¼Œä½ åº”è¯¥æ‹¥æœ‰ä¸€äº›PoCï¼ˆä¾›åˆ©ç›Šç›¸å…³è€…ä½¿ç”¨ï¼‰ä»¥åŠä½ æ‰€å–å¾—çš„åŸºå‡†ã€‚'
- en: '**Retrospective â€”** By the end of our research phase, we can understand the
    feasibility, limitations, and cost of building such an app. This helps us decide
    whether to productionize it and how to design the final product and its UX.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å›é¡¾â€”**åœ¨æˆ‘ä»¬çš„ç ”ç©¶é˜¶æ®µç»“æŸæ—¶ï¼Œæˆ‘ä»¬å¯ä»¥äº†è§£æ„å»ºæ­¤ç±»åº”ç”¨çš„å¯è¡Œæ€§ã€å±€é™æ€§å’Œæˆæœ¬ã€‚è¿™æœ‰åŠ©äºæˆ‘ä»¬å†³å®šæ˜¯å¦å°†å…¶æŠ•å…¥ç”Ÿäº§ï¼Œå¹¶è®¾è®¡æœ€ç»ˆçš„äº§å“åŠå…¶ç”¨æˆ·ä½“éªŒã€‚'
- en: '**Productization â€”** Develop a production-ready version of your project and
    integrate it with the rest of your solution by following standard SWE best practices
    and implementing a feedback and data collection mechanism.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**äº§å“åŒ–â€”**å¼€å‘é¡¹ç›®çš„ç”Ÿäº§å°±ç»ªç‰ˆæœ¬ï¼Œå¹¶é€šè¿‡éµå¾ªæ ‡å‡†çš„è½¯ä»¶å·¥ç¨‹æœ€ä½³å®è·µï¼Œå°†å…¶ä¸å…¶ä»–è§£å†³æ–¹æ¡ˆæ•´åˆï¼Œå®æ–½åé¦ˆå’Œæ•°æ®æ”¶é›†æœºåˆ¶ã€‚'
- en: '![](../Images/3584722390c58fe3144fc2d6496b099d.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3584722390c58fe3144fc2d6496b099d.png)'
- en: LLM-Native app development lifecycle (Image by author)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: LLMåŸç”Ÿåº”ç”¨å¼€å‘ç”Ÿå‘½å‘¨æœŸï¼ˆå›¾ç¤ºæ¥è‡ªä½œè€…ï¼‰
- en: 'To implement the experiment-oriented process well, we must make an informed
    decision on approaching and constructing these experiments:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æœ‰æ•ˆåœ°å®ç°é¢å‘å®éªŒçš„è¿‡ç¨‹ï¼Œæˆ‘ä»¬å¿…é¡»åšå‡ºæœ‰æ ¹æ®çš„å†³å®šæ¥æ¥è¿‘å¹¶æ„å»ºè¿™äº›å®éªŒï¼š
- en: 'Starting Lean: The Bottom-Up Approach'
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç²¾ç®€èµ·æ­¥ï¼šè‡ªä¸‹è€Œä¸Šçš„æ–¹æ³•
- en: While many early adopters quickly jump into" *State-Of-The-Art"* multichain
    agentic systems with full-fledged Langchain or something similar, I found "**The
    Bottom-Up approach**" often yields better results.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡è®¸å¤šæ—©æœŸé‡‡ç”¨è€…å¾ˆå¿«å°±è·³å…¥äº†*æœ€å…ˆè¿›çš„*å¤šé“¾æ¡ä»£ç†ç³»ç»Ÿï¼Œåƒæ˜¯å®Œæ•´çš„Langchainæˆ–ç±»ä¼¼çš„æ¡†æ¶ï¼Œä½†æˆ‘å‘ç°**è‡ªä¸‹è€Œä¸Šçš„æ–¹æ³•**å¾€å¾€èƒ½å–å¾—æ›´å¥½çš„æ•ˆæœã€‚
- en: Start lean, **very lean**, embracing the *â€œone prompt to rule them allâ€* philosophy.
    Although this strategy might seem unconventional and will likely produce bad results
    at first, it establishes a *baseline* for your system.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ç²¾ç®€å¼€å§‹ï¼Œ**éå¸¸ç²¾ç®€**ï¼Œæ‹¥æŠ±*â€œä¸€ä¸ªæç¤ºç»Ÿæ²»ä¸€åˆ‡â€*çš„å“²å­¦ã€‚å°½ç®¡è¿™ç§ç­–ç•¥çœ‹èµ·æ¥ä¸å¤ªä¼ ç»Ÿï¼Œå¹¶ä¸”ä¸€å¼€å§‹å¯èƒ½ä¼šäº§ç”Ÿä¸å¥½çš„ç»“æœï¼Œä½†å®ƒä¸ºä½ çš„ç³»ç»Ÿå»ºç«‹äº†ä¸€ä¸ª*åŸºå‡†*ã€‚
- en: From there, continuously iterate and refine your prompts, employing prompt engineering
    techniques to optimize outcomes. As you identify weaknesses in your lean solution,
    split the process by adding branches to address those shortcomings.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ä»é‚£é‡Œå¼€å§‹ï¼ŒæŒç»­è¿­ä»£å’Œä¼˜åŒ–ä½ çš„æç¤ºï¼Œè¿ç”¨æç¤ºå·¥ç¨‹æŠ€æœ¯æ¥ä¼˜åŒ–ç»“æœã€‚å½“ä½ å‘ç°ç²¾ç®€æ–¹æ¡ˆä¸­çš„å¼±ç‚¹æ—¶ï¼Œé€šè¿‡æ·»åŠ åˆ†æ”¯æ¥æ‹†åˆ†è¿‡ç¨‹ï¼Œè§£å†³è¿™äº›ä¸è¶³ã€‚
- en: While designing each "leaf" of my LLM workflow graph, or LLM-native architecture,
    I follow the[*LLM Triangle Principles*](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e)Â³
    to determine where and when to cut the branches, split them, or thicken the roots
    (by using prompt engineering techniques) and squeeze more of the lemon.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®¾è®¡æˆ‘çš„LLMå·¥ä½œæµå›¾çš„æ¯ä¸ªâ€œå¶å­â€æˆ–LLMåŸç”Ÿæ¶æ„æ—¶ï¼Œæˆ‘éµå¾ª[*LLMä¸‰è§’åŸç†*](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e)Â³æ¥å†³å®šåœ¨å“ªäº›æ—¶å€™ã€ä»¥ä½•ç§æ–¹å¼å‰ªæã€åˆ†æ”¯æˆ–åŠ ç²—æ ¹éƒ¨ï¼ˆé€šè¿‡ä½¿ç”¨æç¤ºå·¥ç¨‹æŠ€æœ¯ï¼‰ï¼Œå¹¶æŒ¤å‡ºæ›´å¤šçš„â€œæŸ æª¬æ±â€ã€‚
- en: '![](../Images/5201b05f41b5d4fed69473f9d53293e3.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5201b05f41b5d4fed69473f9d53293e3.png)'
- en: An illustration for the Bottom-Up approach (Image by author)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªä¸‹è€Œä¸Šçš„æ–¹æ³•æ’å›¾ï¼ˆå›¾ç¤ºæ¥è‡ªä½œè€…ï¼‰
- en: For example, to implement "Native language SQL querying" with the bottom-up
    approach, we'll start by naively sending the schemas to the LLM and ask it to
    generate a query.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œè¦ä½¿ç”¨è‡ªä¸‹è€Œä¸Šçš„æ–¹æ³•å®ç°â€œæœ¬åœ°è¯­è¨€SQLæŸ¥è¯¢â€ï¼Œæˆ‘ä»¬å°†ä»ç›´æ¥å‘LLMå‘é€æ¨¡å¼å¹¶è¦æ±‚å®ƒç”ŸæˆæŸ¥è¯¢å¼€å§‹ã€‚
- en: '![](../Images/1cc8092436c0a284cf6ca8b5914b78ad.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1cc8092436c0a284cf6ca8b5914b78ad.png)'
- en: A Bottom-Up approach example (Image by author)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªè‡ªä¸‹è€Œä¸Šçš„æ–¹æ³•ç¤ºä¾‹ï¼ˆå›¾ç¤ºæ¥è‡ªä½œè€…ï¼‰
- en: Usually, this does not contradict the "top-down approach" but serves as another
    step before it. This allows us to show quick wins and attract more project investment.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œè¿™å¹¶ä¸ä¸â€œè‡ªä¸Šè€Œä¸‹çš„æ–¹æ³•â€ç›¸çŸ›ç›¾ï¼Œè€Œæ˜¯ä½œä¸ºå®ƒçš„å¦ä¸€ä¸ªæ­¥éª¤ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿå±•ç¤ºå¿«é€Ÿçš„èƒœåˆ©ï¼Œå¹¶å¸å¼•æ›´å¤šé¡¹ç›®æŠ•èµ„ã€‚
- en: 'The Big Picture Upfront: The Top-Down Strategy'
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…¨å±€è§†è§’ï¼šè‡ªä¸Šè€Œä¸‹çš„ç­–ç•¥
- en: â€œWe know that LLM workflow is not easy, and to achieve our goal, we'll probably
    end up with some workflow or LLM-native architecture.â€
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œæˆ‘ä»¬çŸ¥é“ LLM å·¥ä½œæµå¹¶ä¸å®¹æ˜“ï¼Œä¸ºäº†å®ç°æˆ‘ä»¬çš„ç›®æ ‡ï¼Œæˆ‘ä»¬å¯èƒ½æœ€ç»ˆä¼šé‡‡ç”¨æŸç§å·¥ä½œæµæˆ– LLM æœ¬åœ°æ¶æ„ã€‚â€
- en: The Top-Down approach recognizes it and starts by designing the LLM-native architecture
    from day one and implementing its different steps/chains from the beginning.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªä¸Šè€Œä¸‹æ–¹æ³•è®¤è¯†åˆ°è¿™ä¸€ç‚¹ï¼Œå¹¶ä»ç¬¬ä¸€å¤©å¼€å§‹å°±è®¾è®¡ LLM æœ¬åœ°æ¶æ„ï¼Œå¹¶ä»ä¸€å¼€å§‹å°±å®æ–½å…¶ä¸åŒçš„æ­¥éª¤/é“¾æ¡ã€‚
- en: This way, you can test your workflow architecture as a whole and squeeze the
    whole lemon instead of refining each leaf separately.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è¿™ç§æ–¹å¼ï¼Œä½ å¯ä»¥æµ‹è¯•æ•´ä¸ªå·¥ä½œæµæ¶æ„ï¼Œè€Œä¸æ˜¯å•ç‹¬ä¼˜åŒ–æ¯ä¸€ç‰‡å¶å­ï¼ŒçœŸæ­£åšåˆ°æ¦¨å¹²æ•´ä¸ªæŸ æª¬ã€‚
- en: '![](../Images/c45de8272f121dbc6dc5740d4459892f.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c45de8272f121dbc6dc5740d4459892f.png)'
- en: 'Top-down approach process: design your architecture once, implement, test &
    measure (Image by author)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªä¸Šè€Œä¸‹æ–¹æ³•æµç¨‹ï¼šä¸€æ¬¡æ€§è®¾è®¡æ¶æ„ï¼Œå®æ–½ã€æµ‹è¯•å¹¶è¡¡é‡æ•ˆæœï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰
- en: 'For example, to implement "Native language SQL querying" with the top-down
    approach, we''ll start designing the architecture before even starting to code
    and then jump to the full implementation:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œè¦å®ç°â€œæœ¬åœ°è¯­è¨€ SQL æŸ¥è¯¢â€å¹¶é‡‡ç”¨è‡ªä¸Šè€Œä¸‹çš„æ–¹æ³•ï¼Œæˆ‘ä»¬ä¼šåœ¨å¼€å§‹ç¼–ç ä¹‹å‰å…ˆè®¾è®¡æ¶æ„ï¼Œç„¶åå†è·³åˆ°å®Œæ•´å®ç°ï¼š
- en: '![](../Images/59ca9496b40b497469678b254636bbcc.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/59ca9496b40b497469678b254636bbcc.png)'
- en: An example of the Top-Down approach (Image by author)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªä¸Šè€Œä¸‹æ–¹æ³•çš„ä¸€ä¸ªä¾‹å­ï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰
- en: Finding the Right Balance
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‰¾åˆ°æ­£ç¡®çš„å¹³è¡¡
- en: When you start experimenting with LLMs, you'll probably start at one of the
    extremes (overcomplicated top-down or super simple one-shot). In reality, there's
    no such a winner.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½ å¼€å§‹å°è¯•å®éªŒ LLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰æ—¶ï¼Œä½ å¯èƒ½ä¼šä»ä¸¤ä¸ªæç«¯ä¸­çš„ä¸€ä¸ªå¼€å§‹ï¼ˆè¿‡äºå¤æ‚çš„è‡ªä¸Šè€Œä¸‹æ–¹æ³•æˆ–è¶…çº§ç®€å•çš„ä¸€æ¬¡æ€§æ–¹æ³•ï¼‰ã€‚å®é™…ä¸Šï¼Œå¹¶æ²¡æœ‰ç»å¯¹çš„â€œèµ¢å®¶â€ã€‚
- en: Ideally â€” you'll define a good SoPÂ¹ and model an expert before coding and experimenting
    with the model. In reality, modeling is very hard; sometimes, you may not have
    access to such an expert.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ç†æƒ³æƒ…å†µä¸‹â€”â€”ä½ ä¼šåœ¨ç¼–ç å’Œå®éªŒæ¨¡å‹ä¹‹å‰ï¼Œå…ˆå®šä¹‰å¥½ä¸€ä¸ªå¥½çš„ SoPÂ¹ï¼Œå¹¶å»ºæ¨¡ä¸€ä¸ªä¸“å®¶ã€‚ä½†åœ¨ç°å®ä¸­ï¼Œå»ºæ¨¡éå¸¸å›°éš¾ï¼Œæœ‰æ—¶ä½ å¯èƒ½æ²¡æœ‰æ¥è§¦åˆ°è¿™æ ·çš„ä¸“å®¶ã€‚
- en: I found it challenging to land on a good architecture/SoPÂ¹ at the first shot,
    so it's worth experimenting lightly before jumping to the big guns. However, it
    doesn't mean that *everything* has to be *too lean.* If you already have a *prior
    understanding* that something *MUST* be broken into smaller pieces â€” do that.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å‘ç°è¦åœ¨ç¬¬ä¸€æ¬¡å°è¯•æ—¶æ‰¾åˆ°ä¸€ä¸ªå¥½çš„æ¶æ„/SoPÂ¹éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› æ­¤åœ¨è·³åˆ°æ›´å¤æ‚çš„æ–¹æ¡ˆä¹‹å‰ï¼Œè½»å¾®å®éªŒæ˜¯å€¼å¾—çš„ã€‚ç„¶è€Œï¼Œè¿™å¹¶ä¸æ„å‘³ç€*ä¸€åˆ‡*éƒ½å¿…é¡»*è¿‡äºç²¾ç®€*ã€‚å¦‚æœä½ å·²ç»æœ‰*å…ˆéªŒç†è§£*ï¼ŒçŸ¥é“æŸäº›ä¸œè¥¿*å¿…é¡»*è¢«æ‹†è§£æˆæ›´å°çš„éƒ¨åˆ†â€”â€”å°±å»åšã€‚
- en: You should leverage the [*LLM Triangle Principles*](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e)Â³
    and correctly model the manual process while designing your solution.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ åº”è¯¥åˆ©ç”¨ [*LLM ä¸‰è§’åŸç†*](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e)Â³ï¼Œæ­£ç¡®åœ°å»ºæ¨¡æ‰‹åŠ¨è¿‡ç¨‹ï¼ŒåŒæ—¶è®¾è®¡ä½ çš„è§£å†³æ–¹æ¡ˆã€‚
- en: 'Optimizing Your Solution: Squeezing the Lemon'
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–ä½ çš„è§£å†³æ–¹æ¡ˆï¼šæ¦¨å¹²æŸ æª¬
- en: 'During the experimentation phase, we continuously squeeze the lemon and add
    more "layers of complexity":'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®éªŒé˜¶æ®µï¼Œæˆ‘ä»¬ä¸æ–­åœ°æ¦¨å¹²æŸ æª¬å¹¶å¢åŠ æ›´å¤šçš„â€œå¤æ‚æ€§å±‚æ¬¡â€ï¼š
- en: '[**Prompt engineering techniques**](https://www.promptingguide.ai/) â€” Like
    Few Shots, Role assignment, or even Dynamic few-shot'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**æç¤ºå·¥ç¨‹æŠ€æœ¯**](https://www.promptingguide.ai/)â€”â€”å¦‚å°‘é‡æ ·æœ¬ã€è§’è‰²åˆ†é…ï¼Œç”šè‡³æ˜¯åŠ¨æ€å°‘é‡æ ·æœ¬'
- en: '**Expanding the Context Window** from simple variable information to complex
    RAG flows can help improve the results.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ‰©å±•ä¸Šä¸‹æ–‡çª—å£**ï¼Œä»ç®€å•çš„å˜é‡ä¿¡æ¯åˆ°å¤æ‚çš„ RAG æµç¨‹ï¼Œå¯ä»¥å¸®åŠ©æé«˜ç»“æœã€‚'
- en: '**Experimenting with different models** â€” Different models perform differently
    on different tasks. Also, the large LLMs are often not very cost-effective, and
    it''s worth trying more task-specific models.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å°è¯•ä¸åŒçš„æ¨¡å‹**â€”â€”ä¸åŒçš„æ¨¡å‹åœ¨ä¸åŒçš„ä»»åŠ¡ä¸Šè¡¨ç°ä¸åŒã€‚æ­¤å¤–ï¼Œå¤§å‹ LLM å¾€å¾€ä¸å…·å¤‡æˆæœ¬æ•ˆç›Šï¼Œå› æ­¤å€¼å¾—å°è¯•æ›´å¤šé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æ¨¡å‹ã€‚'
- en: '**Prompt dieting â€”** I learned that putting the SOPÂ¹ (specifically, the prompt
    and the requested output) through a "diet" usually improves latency.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æç¤ºç²¾ç®€**â€”â€”æˆ‘å‘ç°å°† SOPÂ¹ï¼ˆå…·ä½“æ¥è¯´æ˜¯æç¤ºå’Œè¯·æ±‚çš„è¾“å‡ºï¼‰è¿›è¡Œâ€œç²¾ç®€â€é€šå¸¸å¯ä»¥æé«˜å»¶è¿Ÿã€‚'
- en: By reducing the prompt size and the steps the model needs to go through, we
    can reduce both the input and output the model needs to generate. You'll be surprised,
    but prompt dieting can sometimes even improve the quality!
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: é€šè¿‡å‡å°‘æç¤ºå¤§å°å’Œæ¨¡å‹éœ€è¦ç»è¿‡çš„æ­¥éª¤ï¼Œæˆ‘ä»¬å¯ä»¥å‡å°‘æ¨¡å‹éœ€è¦ç”Ÿæˆçš„è¾“å…¥å’Œè¾“å‡ºã€‚ä½ ä¼šæ„Ÿåˆ°æƒŠè®¶ï¼Œä½†æœ‰æ—¶å€™ï¼Œæç¤ºç²¾ç®€ç”šè‡³èƒ½æé«˜è´¨é‡ï¼
- en: Be aware that the diet might also cause quality degradation, so it's important
    to set up a sanity test before doing so.
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œé¥®é£Ÿå¯èƒ½ä¹Ÿä¼šå¯¼è‡´è´¨é‡ä¸‹é™ï¼Œå› æ­¤åœ¨è¿›è¡Œä¹‹å‰è®¾ç½®ä¸€ä¸ªåˆç†æ€§æµ‹è¯•éå¸¸é‡è¦ã€‚
- en: '**Splitting the process** into smaller steps can also be very beneficial and
    make optimizing a subprocess of your SOPÂ¹ easier and feasible.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å°†è¿‡ç¨‹åˆ†è§£**æˆæ›´å°çš„æ­¥éª¤ä¹Ÿéå¸¸æœ‰åˆ©ï¼Œå¹¶èƒ½ä½¿å¾—ä¼˜åŒ– SOPÂ¹ ä¸­çš„å­è¿‡ç¨‹å˜å¾—æ›´å®¹æ˜“ä¸”å¯è¡Œã€‚'
- en: Be aware that this might increase the solution's complexity or damage the performance
    (e.g., increase the number of tokens processed). To mitigate this, aim for concise
    prompts and smaller models.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ è§£å†³æ–¹æ¡ˆçš„å¤æ‚æ€§æˆ–å½±å“æ€§èƒ½ï¼ˆä¾‹å¦‚ï¼Œå¢åŠ å¤„ç†çš„ä»¤ç‰Œæ•°ï¼‰ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€ç‚¹ï¼Œå°½é‡ä½¿ç”¨ç®€æ´çš„æç¤ºå’Œæ›´å°çš„æ¨¡å‹ã€‚
- en: As a rule of thumb, it's usually a good idea to split when a dramatic change
    of the System Prompt yields much better results for this part of the SOPÂ¹ flow.
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä¸€èˆ¬æ¥è¯´ï¼Œå½“ç³»ç»Ÿæç¤ºçš„å‰§çƒˆå˜åŒ–èƒ½ä¸º SOPÂ¹ æµç¨‹çš„è¿™ä¸€éƒ¨åˆ†å¸¦æ¥æ›´å¥½çš„ç»“æœæ—¶ï¼Œåˆ†å‰²æ˜¯ä¸ªä¸é”™çš„é€‰æ‹©ã€‚
- en: '![](../Images/0ccb287ce6adee9c671ffc63e24be4ae.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ccb287ce6adee9c671ffc63e24be4ae.png)'
- en: Squeezing the AI Lemon (Created with Dall-E3)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æŒ¤å‹ AI æŸ æª¬ï¼ˆç”± Dall-E3 åˆ›å»ºï¼‰
- en: The Anatomy of an LLM Experiment
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM å®éªŒçš„è§£å‰–å­¦
- en: 'Personally, I prefer to start *lean* with a simple Jupyter Notebook using Python,
    Pydantic, and Jinja2:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: å°±ä¸ªäººè€Œè¨€ï¼Œæˆ‘æ›´å€¾å‘äºä½¿ç”¨ä¸€ä¸ªç®€å•çš„ Jupyter Notebookï¼Œç»“åˆ Pythonã€Pydantic å’Œ Jinja2ï¼Œ*ä»¥è½»é‡çš„æ–¹å¼*å¼€å§‹ï¼š
- en: '**Use Pydantic** to define my outputs'' schema from the model.'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨ Pydantic** å®šä¹‰æ¨¡å‹è¾“å‡ºçš„æ¶æ„ã€‚'
- en: Write the **prompt template** with **Jinja2**.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ **Jinja2** ç¼–å†™ **æç¤ºæ¨¡æ¿**ã€‚
- en: '**Define a structured output** format (in **YAML**Â²). This will ensure the
    model follows the "thinking steps" and is guided by my SOP.'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å®šä¹‰ä¸€ä¸ªç»“æ„åŒ–è¾“å‡º**æ ¼å¼ï¼ˆä½¿ç”¨ **YAML**Â²ï¼‰ã€‚è¿™å°†ç¡®ä¿æ¨¡å‹éµå¾ªâ€œæ€è€ƒæ­¥éª¤â€å¹¶éµå¾ªæˆ‘çš„ SOPã€‚'
- en: '**Ensure this output** with your Pydantic validations; if needed â€” retry.'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**é€šè¿‡**ä½ çš„ Pydantic éªŒè¯ç¡®ä¿æ­¤è¾“å‡ºï¼›å¦‚æœéœ€è¦â€”â€”è¯·é‡è¯•ã€‚'
- en: '**Stabilize your work** â€” structure your code into functional units with Python
    files and packages.'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ç¨³å®šä½ çš„å·¥ä½œ**â€”â€”å°†ä»£ç ç»“æ„åŒ–ä¸ºåŠŸèƒ½å•å…ƒï¼Œä½¿ç”¨ Python æ–‡ä»¶å’ŒåŒ…ã€‚'
- en: In a broader scope, you can use different tools such as [openai-streaming](https://github.com/AlmogBaku/openai-streaming)
    to easily **utilize streaming (and tools)**, [LiteLLM](https://docs.litellm.ai/docs/)
    to have a **standardized LLM SDK** across different providers, or [vLLM](https://docs.vllm.ai/)
    to **serve open-source LLMs.**
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ›´å¹¿æ³›çš„è§’åº¦æ¥çœ‹ï¼Œä½ å¯ä»¥ä½¿ç”¨ä¸åŒçš„å·¥å…·ï¼Œæ¯”å¦‚ [openai-streaming](https://github.com/AlmogBaku/openai-streaming)
    æ¥è½»æ¾ **åˆ©ç”¨æµå¼å¤„ç†ï¼ˆå’Œå·¥å…·ï¼‰**ï¼Œ[LiteLLM](https://docs.litellm.ai/docs/) æ¥æ‹¥æœ‰ä¸€ä¸ª **æ ‡å‡†åŒ–çš„ LLM
    SDK**ï¼Œé€‚ç”¨äºä¸åŒçš„æä¾›å•†ï¼Œæˆ–è€… [vLLM](https://docs.vllm.ai/) æ¥ **æä¾›å¼€æº LLM æœåŠ¡**ã€‚
- en: Ensuring Quality with Sanity Tests and Evaluations
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é€šè¿‡å¥å…¨æ€§æµ‹è¯•å’Œè¯„ä¼°ç¡®ä¿è´¨é‡
- en: A sanity test evaluates the quality of your project and ensures that you're
    not degrading a certain success rate baseline you defined.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€é¡¹å¥å…¨æ€§æµ‹è¯•è¯„ä¼°ä½ çš„é¡¹ç›®è´¨é‡ï¼Œå¹¶ç¡®ä¿ä½ æ²¡æœ‰é™ä½å·²ç»å®šä¹‰çš„æŸä¸ªæˆåŠŸç‡åŸºå‡†ã€‚
- en: Think of your solution/prompts as a short blanket â€” if you stretch it too much,
    it might suddenly not cover some use cases it used to cover.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: æŠŠä½ çš„è§£å†³æ–¹æ¡ˆ/æç¤ºæƒ³è±¡æˆä¸€æ¡çŸ­æ¯›æ¯¯â€”â€”å¦‚æœä½ æŠŠå®ƒæ‹‰å¾—å¤ªé•¿ï¼Œå®ƒå¯èƒ½çªç„¶æ— æ³•è¦†ç›–ä»¥å‰èƒ½å¤Ÿè¦†ç›–çš„æŸäº›ç”¨ä¾‹ã€‚
- en: To do that, define a set of cases you have already covered successfully and
    ensure you keep it that way (or at least it's worth it). Thinking of it like a
    [table-driven test](https://lorenzopeppoloni.com/tabledriventestspy/) might help.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºæ­¤ï¼Œå®šä¹‰ä¸€ç»„ä½ å·²ç»æˆåŠŸè¦†ç›–çš„æ¡ˆä¾‹ï¼Œå¹¶ç¡®ä¿å®ƒä¿æŒè¿™æ ·ï¼ˆæˆ–è€…è‡³å°‘å€¼å¾—è¿™æ ·åšï¼‰ã€‚æŠŠå®ƒå½“ä½œ [è¡¨é©±åŠ¨æµ‹è¯•](https://lorenzopeppoloni.com/tabledriventestspy/)
    å¯èƒ½ä¼šæœ‰å¸®åŠ©ã€‚
- en: Evaluating the success of a "generative" solution(e.g., writing text) is much
    more complex than using LLMs for other tasks (such as categorization, entity extraction,
    etc.). For these kinds of tasks, you might want to involve a smarter model (such
    as GPT4, Claude Opus, or LLAMA3â€“70B) to act as a "judge."
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: è¯„ä¼°â€œç”Ÿæˆæ€§â€è§£å†³æ–¹æ¡ˆï¼ˆä¾‹å¦‚ï¼Œå†™æ–‡æœ¬ï¼‰çš„æˆåŠŸæ¯”ä½¿ç”¨ LLM å¤„ç†å…¶ä»–ä»»åŠ¡ï¼ˆå¦‚åˆ†ç±»ã€å®ä½“æå–ç­‰ï¼‰è¦å¤æ‚å¾—å¤šã€‚å¯¹äºè¿™äº›ä»»åŠ¡ï¼Œä½ å¯èƒ½éœ€è¦å¼•å…¥æ›´æ™ºèƒ½çš„æ¨¡å‹ï¼ˆå¦‚
    GPT4ã€Claude Opus æˆ– LLAMA3â€“70Bï¼‰ä½œä¸ºâ€œè£åˆ¤â€ã€‚
- en: 'It might also be a good idea to try and make the output include "deterministic
    parts" before the "generative" output, as these kinds of output are easier to
    test:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: å¦å¤–ï¼Œå°è¯•è®©è¾“å‡ºå…ˆåŒ…å«â€œç¡®å®šæ€§éƒ¨åˆ†â€ï¼Œå†è¾“å‡ºâ€œç”Ÿæˆéƒ¨åˆ†â€å¯èƒ½æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ï¼Œå› ä¸ºè¿™ç±»è¾“å‡ºæ›´å®¹æ˜“è¿›è¡Œæµ‹è¯•ï¼š
- en: '[PRE0]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'There are a few cutting-edge,ğŸ¤©ğŸ¤© promising solutions worth investigating. I
    found them especially relevant when evaluating RAG-based solutions: take a look
    at [DeepChecks](https://deepchecks.com/), [Ragas](https://github.com/explodinggradients/ragas),
    or [ArizeAI](https://arize.com/).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸€äº›å‰æ²¿çš„ã€ğŸ¤©ğŸ¤© æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆå€¼å¾—æ¢ç´¢ã€‚æˆ‘å‘ç°å®ƒä»¬åœ¨è¯„ä¼°åŸºäº RAG çš„è§£å†³æ–¹æ¡ˆæ—¶å°¤å…¶ç›¸å…³ï¼šå¯ä»¥çœ‹çœ‹ [DeepChecks](https://deepchecks.com/)ã€[Ragas](https://github.com/explodinggradients/ragas)
    æˆ– [ArizeAI](https://arize.com/)ã€‚
- en: 'Making Informed Decisions: The Importance of Retrospectives'
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åšå‡ºæ˜æ™ºçš„å†³ç­–ï¼šå›é¡¾çš„æ„ä¹‰
- en: After each major/time-framed experiment or milestone, we should stop and make
    **an informed decision** on how and if to proceed with this approach.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¯æ¬¡ä¸»è¦/å®šæœŸå®éªŒæˆ–é‡Œç¨‹ç¢‘åï¼Œæˆ‘ä»¬åº”è¯¥åœä¸‹æ¥å¹¶åšå‡º**æ˜æ™ºçš„å†³ç­–**ï¼Œå†³å®šæ˜¯å¦ç»§ç»­é‡‡ç”¨è¿™ç§æ–¹æ³•ã€‚
- en: At this point, your experiment will have a clear success rate baseline, and
    you'll have an idea of what needs to be improved.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ—¶ï¼Œæ‚¨çš„å®éªŒå°†æœ‰ä¸€ä¸ªæ˜ç¡®çš„æˆåŠŸç‡åŸºå‡†ï¼Œæ‚¨ä¹Ÿä¼šäº†è§£éœ€è¦æ”¹è¿›çš„åœ°æ–¹ã€‚
- en: 'This is also a good point to start discussing the productization implications
    of this solution and start with the "product work":'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æ—¶æœºæ¥å¼€å§‹è®¨è®ºè¿™ä¸ªè§£å†³æ–¹æ¡ˆçš„äº§å“åŒ–å½±å“ï¼Œå¹¶å¼€å§‹è¿›è¡Œâ€œäº§å“å·¥ä½œâ€ï¼š
- en: What will this look like within the product?
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿™åœ¨äº§å“ä¸­ä¼šæ˜¯ä»€ä¹ˆæ ·å­ï¼Ÿ
- en: What are the limitations/challenges? How would you mitigate them?
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æœ‰å“ªäº›é™åˆ¶/æŒ‘æˆ˜ï¼Ÿæ‚¨å°†å¦‚ä½•åº”å¯¹ï¼Ÿ
- en: Whatâ€™s your current latency? Is it good enough?
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç›®å‰çš„å»¶è¿Ÿæ˜¯å¤šå°‘ï¼Ÿå¤Ÿå¥½å—ï¼Ÿ
- en: What should the UX be? Which UI hacks can you use? Can [streaming](https://github.com/AlmogBaku/openai-streaming)
    help?
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç”¨æˆ·ä½“éªŒåº”å¦‚ä½•è®¾è®¡ï¼Ÿå¯ä»¥ä½¿ç”¨å“ªäº›UIæŠ€å·§ï¼Ÿ[æµåª’ä½“](https://github.com/AlmogBaku/openai-streaming)èƒ½å¦æœ‰æ‰€å¸®åŠ©ï¼Ÿ
- en: What's the estimated spending on tokens? Can we use smaller models to reduce
    spending?
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é¢„è®¡çš„ä»£å¸æ¶ˆè€—æ˜¯å¤šå°‘ï¼Ÿæˆ‘ä»¬èƒ½å¦ä½¿ç”¨æ›´å°çš„æ¨¡å‹æ¥å‡å°‘æ¶ˆè€—ï¼Ÿ
- en: What are priorities? Is any of the challenges a showstopper?
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¼˜å…ˆçº§æ˜¯ä»€ä¹ˆï¼Ÿæœ‰å“ªäº›æŒ‘æˆ˜æ˜¯ä¸å¯å¦¥åçš„ï¼Ÿ
- en: Suppose the *baseline* we achieved is â€œgood enough,â€ and we believe we can mitigate
    the problems we raised. In that case, we will continue investing in and improving
    the project while *ensuring it never degrades* and using the sanity tests.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬è¾¾æˆçš„*åŸºå‡†*â€œè¶³å¤Ÿå¥½â€ï¼Œå¹¶ä¸”æˆ‘ä»¬ç›¸ä¿¡å¯ä»¥è§£å†³æå‡ºçš„é—®é¢˜ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†ç»§ç»­æŠ•èµ„å¹¶æ”¹å–„é¡¹ç›®ï¼ŒåŒæ—¶*ç¡®ä¿å®ƒæ°¸è¿œä¸ä¼šé€€åŒ–*ï¼Œå¹¶ä½¿ç”¨ç†æ™ºæµ‹è¯•ã€‚
- en: '![](../Images/bacac79b87978cd360d070bb0f44464a.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bacac79b87978cd360d070bb0f44464a.png)'
- en: (Created with Dall-E3)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ï¼ˆç”±Dall-E3åˆ›å»ºï¼‰
- en: 'From Experiment to Product: Bringing Your Solution to Life'
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»å®éªŒåˆ°äº§å“ï¼šè®©æ‚¨çš„è§£å†³æ–¹æ¡ˆè½åœ°
- en: Last but not least, we have to productize our work. Like any other production-grade
    solution, we must implement production engineering concepts like logging, monitoring,
    dependency management, containerization, caching, etc.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åä½†åŒæ ·é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å¿…é¡»å°†æˆ‘ä»¬çš„å·¥ä½œäº§å“åŒ–ã€‚åƒä»»ä½•å…¶ä»–ç”Ÿäº§çº§è§£å†³æ–¹æ¡ˆä¸€æ ·ï¼Œæˆ‘ä»¬å¿…é¡»å®ç°ç”Ÿäº§å·¥ç¨‹æ¦‚å¿µï¼Œå¦‚æ—¥å¿—è®°å½•ã€ç›‘æ§ã€ä¾èµ–ç®¡ç†ã€å®¹å™¨åŒ–ã€ç¼“å­˜ç­‰ã€‚
- en: This is a huge world, but luckily, we can borrow many mechanisms from classical
    production engineering and even adopt many of the existing tools.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªåºå¤§çš„ä¸–ç•Œï¼Œä½†å¹¸è¿çš„æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥å€Ÿç”¨è®¸å¤šæ¥è‡ªä¼ ç»Ÿç”Ÿäº§å·¥ç¨‹çš„æœºåˆ¶ï¼Œç”šè‡³é‡‡ç”¨è®¸å¤šç°æœ‰çš„å·¥å…·ã€‚
- en: 'That being said, it''s important to take extra care of the nuances involving
    LLM-native apps:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: è¯è™½å¦‚æ­¤ï¼Œåœ¨å¤„ç†æ¶‰åŠLLMåŸç”Ÿåº”ç”¨çš„ç»†èŠ‚æ—¶è¦ç‰¹åˆ«å°å¿ƒï¼š
- en: '**Feedback loop** â€” How do we measure success? Is it simply a "thumb up/down"
    mechanism or something more sophisticated that considers the adoption of our solution?'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åé¦ˆå¾ªç¯**â€”æˆ‘ä»¬å¦‚ä½•è¡¡é‡æˆåŠŸï¼Ÿæ˜¯ä»…ä»…ä¸€ä¸ªâ€œç‚¹èµ/ç‚¹è¸©â€æœºåˆ¶ï¼Œè¿˜æ˜¯æ›´å¤æ‚çš„æœºåˆ¶ï¼Œè€ƒè™‘åˆ°æˆ‘ä»¬è§£å†³æ–¹æ¡ˆçš„é‡‡ç”¨æƒ…å†µï¼Ÿ'
- en: It is also important to collect this data; down the road, this can help us redefine
    our sanity "baseline" or fine-tune our results with [dynamic-few shots](https://arxiv.org/abs/1804.09458)
    or fine-tune the model.
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ”¶é›†è¿™äº›æ•°æ®ä¹Ÿå¾ˆé‡è¦ï¼›åœ¨æœªæ¥ï¼Œè¿™å°†å¸®åŠ©æˆ‘ä»¬é‡æ–°å®šä¹‰æˆ‘ä»¬çš„ç†æ™ºâ€œåŸºå‡†â€æˆ–é€šè¿‡[åŠ¨æ€å°‘é‡æ ·æœ¬](https://arxiv.org/abs/1804.09458)æ¥å¾®è°ƒç»“æœï¼Œæˆ–å¾®è°ƒæ¨¡å‹ã€‚
- en: '**Caching** â€” Unlike traditional SWE, caching can be very challenging when
    we involve a generative aspect in our solution. To mitigate it, explore the option
    to cache similar results(e.g., using RAG) and/or reduce the generative output
    (by having a strict output schema)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç¼“å­˜**â€”ä¸ä¼ ç»Ÿçš„è½¯ä»¶å·¥ç¨‹ä¸åŒï¼Œå½“æˆ‘ä»¬åœ¨è§£å†³æ–¹æ¡ˆä¸­å¼•å…¥ç”Ÿæˆæ€§ç‰¹å¾æ—¶ï¼Œç¼“å­˜å¯èƒ½ä¼šå˜å¾—éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œå¯ä»¥æ¢ç´¢ç¼“å­˜ç›¸ä¼¼ç»“æœï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨RAGï¼‰å’Œ/æˆ–å‡å°‘ç”Ÿæˆæ€§è¾“å‡ºï¼ˆé€šè¿‡ä¸¥æ ¼çš„è¾“å‡ºæ¶æ„ï¼‰ã€‚'
- en: '**Cost tracking** â€” Many companies find it very tempting to start with a "strong
    model" (such as GPT-4 or Opus) however - in production, the costs can quickly
    rise. Avoid being surprised on the final bill, and make sure to measure the input/output
    tokens and keep track of your workflow impact (without these practices â€” good
    luck profiling it later on)'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æˆæœ¬è·Ÿè¸ª**â€”è®¸å¤šå…¬å¸å‘ç°ï¼Œå¯åŠ¨æ—¶ä½¿ç”¨â€œå¼ºå¤§æ¨¡å‹â€ï¼ˆå¦‚GPT-4æˆ–Opusï¼‰éå¸¸æœ‰å¸å¼•åŠ›ï¼Œç„¶è€Œâ€”â€”åœ¨ç”Ÿäº§ä¸­ï¼Œæˆæœ¬å¯èƒ½è¿…é€Ÿä¸Šå‡ã€‚é¿å…åœ¨æœ€ç»ˆè´¦å•ä¸Šæ„Ÿåˆ°æƒŠè®¶ï¼Œå¹¶ç¡®ä¿è¡¡é‡è¾“å…¥/è¾“å‡ºä»£å¸å¹¶è·Ÿè¸ªå·¥ä½œæµçš„å½±å“ï¼ˆå¦‚æœæ²¡æœ‰è¿™äº›åšæ³•â€”â€”ç¨åå†è¿›è¡Œåˆ†æå¯èƒ½å°±ä¼šéå¸¸å›°éš¾ï¼‰'
- en: '**Debuggability and tracing** â€” Ensure you have set up the right tools to track
    a "buggy" input and track it throughout the process. This usually involves retaining
    the user input for later investigation and setting up [a tracing system](https://github.com/traceloop/openllmetry?tab=readme-ov-file).
    Remember: "Unlike traditional software, AI fails silently!"'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¯è°ƒè¯•æ€§å’Œè¿½è¸ª**â€”â€”ç¡®ä¿ä½ å·²ç»è®¾ç½®å¥½æ­£ç¡®çš„å·¥å…·ï¼Œä»¥è¿½è¸ªâ€œæœ‰é—®é¢˜â€çš„è¾“å…¥å¹¶è´¯ç©¿æ•´ä¸ªè¿‡ç¨‹ã€‚è¿™é€šå¸¸æ¶‰åŠä¿å­˜ç”¨æˆ·è¾“å…¥ä»¥ä¾›åç»­è°ƒæŸ¥ï¼Œå¹¶è®¾ç½®ä¸€ä¸ª[è¿½è¸ªç³»ç»Ÿ](https://github.com/traceloop/openllmetry?tab=readme-ov-file)ã€‚è®°ä½ï¼šâ€œä¸ä¼ ç»Ÿè½¯ä»¶ä¸åŒï¼ŒAIçš„å¤±è´¥æ˜¯æ‚„æ— å£°æ¯çš„ï¼â€'
- en: 'Closing Remarks: Your Role in Advancing LLM-Native Technology'
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“æŸè¯­ï¼šä½ åœ¨æ¨åŠ¨LLMåŸç”ŸæŠ€æœ¯å‘å±•ä¸­çš„è§’è‰²
- en: This might be the end of the article, but certainly not the end of our work.
    LLM-native development is an iterative process that covers more use cases, challenges,
    and features and continuously improves our LLM-native product.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯èƒ½æ˜¯æ–‡ç« çš„ç»“æŸï¼Œä½†è‚¯å®šä¸æ˜¯æˆ‘ä»¬å·¥ä½œçš„ç»ˆç»“ã€‚LLMåŸç”Ÿå¼€å‘æ˜¯ä¸€ä¸ªè¿­ä»£è¿‡ç¨‹ï¼Œæ¶‰åŠæ›´å¤šçš„ä½¿ç”¨æ¡ˆä¾‹ã€æŒ‘æˆ˜å’ŒåŠŸèƒ½ï¼Œå¹¶ä¸æ–­æ”¹è¿›æˆ‘ä»¬çš„LLMåŸç”Ÿäº§å“ã€‚
- en: As you continue your AI development journey, stay agile, experiment fearlessly,
    and keep the end-user in mind. Share your experiences and insights with the community,
    and together, we can push the boundaries of what's possible with LLM-native apps.
    Keep exploring, learning, and building â€” the possibilities are endless.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½ ç»§ç»­è¿›è¡ŒAIå¼€å‘ä¹‹æ—…æ—¶ï¼Œä¿æŒçµæ´»ï¼Œå‹‡æ•¢åœ°è¿›è¡Œå®éªŒï¼Œå¹¶å§‹ç»ˆå…³æ³¨æœ€ç»ˆç”¨æˆ·ã€‚ä¸ç¤¾åŒºåˆ†äº«ä½ çš„ç»éªŒå’Œè§è§£ï¼Œè®©æˆ‘ä»¬ä¸€èµ·æ¨åŠ¨LLMåŸç”Ÿåº”ç”¨çš„å¯èƒ½æ€§è¾¹ç•Œã€‚ç»§ç»­æ¢ç´¢ã€å­¦ä¹ å’Œæ„å»ºâ€”â€”æ— é™çš„å¯èƒ½ç­‰å¾…ç€ä½ ã€‚
- en: I hope this guide has been a valuable companion on your LLM-native development
    journey! I'd love to hear your story â€” share your triumphs and challenges in the
    comments below. ğŸ’¬
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¸Œæœ›è¿™æœ¬æŒ‡å—åœ¨ä½ è¿›è¡ŒLLMåŸç”Ÿå¼€å‘çš„è¿‡ç¨‹ä¸­æˆä¸ºäº†ä¸€ä¸ªæœ‰ä»·å€¼çš„ä¼´ä¾£ï¼æˆ‘å¾ˆæƒ³å¬å¬ä½ çš„æ•…äº‹â€”â€”åœ¨ä¸‹é¢çš„è¯„è®ºä¸­åˆ†äº«ä½ çš„æˆåŠŸä¸æŒ‘æˆ˜å§ã€‚ğŸ’¬
- en: If you find this article helpful, please give it a few **claps** ğŸ‘ on Medium
    and **share** it with your fellow AI enthusiasts. Your support means the world
    to me! ğŸŒ
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ è§‰å¾—è¿™ç¯‡æ–‡ç« å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·åœ¨Mediumä¸Šç»™å®ƒç‚¹å‡ ä¸ª**æŒå£°** ğŸ‘å¹¶**åˆ†äº«**ç»™ä½ çš„AIçˆ±å¥½è€…æœ‹å‹ä»¬ã€‚ä½ çš„æ”¯æŒå¯¹æˆ‘æ„ä¹‰é‡å¤§ï¼ğŸŒ
- en: Let's keep the conversation going â€” feel free to reach out via [email](mailto:almog.baku@gmail.com)
    or [connect on LinkedIn](https://www.linkedin.com/in/almogbaku/) ğŸ¤
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç»§ç»­å¯¹è¯â€”â€”éšæ—¶é€šè¿‡[ç”µå­é‚®ä»¶](mailto:almog.baku@gmail.com)æˆ–[LinkedInè”ç³»](https://www.linkedin.com/in/almogbaku/)
    ğŸ¤
- en: Special thanks to [Yonatan V. Levin](https://medium.com/u/8735065c2497?source=post_page---user_mention--1fe1e6ef60fd--------------------------------),
    [Gal Peretz](https://medium.com/u/532f8dc01db8?source=post_page---user_mention--1fe1e6ef60fd--------------------------------),
    [Philip Tannor](https://medium.com/u/5c5d2a69bcdb?source=post_page---user_mention--1fe1e6ef60fd--------------------------------),
    [Ori Cohen](https://medium.com/u/4dde5994e6c1?source=post_page---user_mention--1fe1e6ef60fd--------------------------------),
    [Nadav](https://medium.com/u/ed1905bd6262?source=post_page---user_mention--1fe1e6ef60fd--------------------------------),
    [Ben Huberman](https://medium.com/u/e6ad8abedec9?source=post_page---user_mention--1fe1e6ef60fd--------------------------------),
    [Carmel Barniv](https://medium.com/u/6374acbf3a05?source=post_page---user_mention--1fe1e6ef60fd--------------------------------),
    [Omri Allouche](https://medium.com/u/bf14cec4d697?source=post_page---user_mention--1fe1e6ef60fd--------------------------------),
    and [Liron Izhaki Allerhand](https://medium.com/u/251cd1007ce8?source=post_page---user_mention--1fe1e6ef60fd--------------------------------)
    for insights, feedback, and editing notes.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹åˆ«æ„Ÿè°¢[Yonatan V. Levin](https://medium.com/u/8735065c2497?source=post_page---user_mention--1fe1e6ef60fd--------------------------------)ã€[Gal
    Peretz](https://medium.com/u/532f8dc01db8?source=post_page---user_mention--1fe1e6ef60fd--------------------------------)ã€[Philip
    Tannor](https://medium.com/u/5c5d2a69bcdb?source=post_page---user_mention--1fe1e6ef60fd--------------------------------)ã€[Ori
    Cohen](https://medium.com/u/4dde5994e6c1?source=post_page---user_mention--1fe1e6ef60fd--------------------------------)ã€[Nadav](https://medium.com/u/ed1905bd6262?source=post_page---user_mention--1fe1e6ef60fd--------------------------------)ã€[Ben
    Huberman](https://medium.com/u/e6ad8abedec9?source=post_page---user_mention--1fe1e6ef60fd--------------------------------)ã€[Carmel
    Barniv](https://medium.com/u/6374acbf3a05?source=post_page---user_mention--1fe1e6ef60fd--------------------------------)ã€[Omri
    Allouche](https://medium.com/u/bf14cec4d697?source=post_page---user_mention--1fe1e6ef60fd--------------------------------)
    å’Œ [Liron Izhaki Allerhand](https://medium.com/u/251cd1007ce8?source=post_page---user_mention--1fe1e6ef60fd--------------------------------)æä¾›çš„è§è§£ã€åé¦ˆå’Œç¼–è¾‘æ„è§ã€‚
- en: '**Â¹SoP**- Standard operating procedure, a concept borrowed from [The *LLM Triangle
    Principles*](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e)Â³'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**Â¹SoP**â€”â€”æ ‡å‡†æ“ä½œç¨‹åºï¼Œè¿™ä¸€æ¦‚å¿µæ¥è‡ªäº[ã€ŠLLMä¸‰è§’åŸåˆ™ã€‹](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e)Â³'
- en: '**Â²YAML**- I found that using YAML to structure your output works much better
    with LLMs. Why? My theory is that it reduces the non-relevant tokens and behaves
    much like the native language. This article dives deep into this subject.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**Â²YAML** - æˆ‘å‘ç°ä½¿ç”¨YAMLæ¥ç»“æ„åŒ–è¾“å‡ºå¯¹äºLLMæ¥è¯´æ•ˆæœæ›´å¥½ã€‚ä¸ºä»€ä¹ˆï¼Ÿæˆ‘çš„ç†è®ºæ˜¯ï¼Œå®ƒå‡å°‘äº†ä¸ç›¸å…³çš„æ ‡è®°ï¼Œè¡Œä¸ºæ›´åƒæ˜¯åŸç”Ÿè¯­è¨€ã€‚æœ¬æ–‡æ·±å…¥æ¢è®¨äº†è¿™ä¸ªè¯é¢˜ã€‚'
- en: '**Â³**[**The LLM Triangle Principles**](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e)-
    Software design principles for designing and building LLM-native apps; Update-
    the whitepaper recently published, [you can read it here](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e).'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**Â³**[**LLMä¸‰è§’åŸç†**](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e)
    - ç”¨äºè®¾è®¡å’Œæ„å»ºLLMåŸç”Ÿåº”ç”¨çš„è½¯è®¾è®¡åŸåˆ™ï¼›æ›´æ–° - æœ€è¿‘å‘å¸ƒçš„ç™½çš®ä¹¦ï¼Œ[ä½ å¯ä»¥åœ¨è¿™é‡Œé˜…è¯»](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e)ã€‚'
