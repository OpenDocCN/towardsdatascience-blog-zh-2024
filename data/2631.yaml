- en: 'Beyond Attention: How Advanced Positional Embedding Methods Improve upon the
    Original Approach in Transformer Architecture'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越注意力机制：先进的位置嵌入方法如何在Transformer架构中改进原始方法
- en: 原文：[https://towardsdatascience.com/beyond-attention-how-advanced-positional-embedding-methods-improve-upon-the-original-transformers-90380b74d324?source=collection_archive---------3-----------------------#2024-10-29](https://towardsdatascience.com/beyond-attention-how-advanced-positional-embedding-methods-improve-upon-the-original-transformers-90380b74d324?source=collection_archive---------3-----------------------#2024-10-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/beyond-attention-how-advanced-positional-embedding-methods-improve-upon-the-original-transformers-90380b74d324?source=collection_archive---------3-----------------------#2024-10-29](https://towardsdatascience.com/beyond-attention-how-advanced-positional-embedding-methods-improve-upon-the-original-transformers-90380b74d324?source=collection_archive---------3-----------------------#2024-10-29)
- en: 'From Sinusoidal to RoPE and ALiBi: How advanced positional encodings overcome
    limitations in Transformers'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从正弦波到RoPE和ALiBi：先进的位置编码如何克服Transformer中的局限性
- en: '[](https://medium.com/@InfiniteLearningLoop?source=post_page---byline--90380b74d324--------------------------------)[![Elahe
    Aghapour](../Images/47a2023c566d50d8ecfcafdb69bb9bb7.png)](https://medium.com/@InfiniteLearningLoop?source=post_page---byline--90380b74d324--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--90380b74d324--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--90380b74d324--------------------------------)
    [Elahe Aghapour](https://medium.com/@InfiniteLearningLoop?source=post_page---byline--90380b74d324--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@InfiniteLearningLoop?source=post_page---byline--90380b74d324--------------------------------)[![Elahe
    Aghapour](../Images/47a2023c566d50d8ecfcafdb69bb9bb7.png)](https://medium.com/@InfiniteLearningLoop?source=post_page---byline--90380b74d324--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--90380b74d324--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--90380b74d324--------------------------------)
    [Elahe Aghapour](https://medium.com/@InfiniteLearningLoop?source=post_page---byline--90380b74d324--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--90380b74d324--------------------------------)
    ·9 min read·Oct 29, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--90380b74d324--------------------------------)
    ·阅读时间：9分钟·2024年10月29日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '**Authors:** [**Elahe Aghapour**](https://medium.com/u/75214fb27311?source=post_page---user_mention--90380b74d324--------------------------------)**,**
    [**Salar Rahili**](https://medium.com/u/6dff1eb2cc9f?source=post_page---user_mention--90380b74d324--------------------------------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：** [**Elahe Aghapour**](https://medium.com/u/75214fb27311?source=post_page---user_mention--90380b74d324--------------------------------)**，**
    [**Salar Rahili**](https://medium.com/u/6dff1eb2cc9f?source=post_page---user_mention--90380b74d324--------------------------------)'
- en: 'Introduction:'
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言：
- en: The exponential progress of models built in recent years is deeply connected
    with the advent of the Transformer architecture. Previously, AI scientists had
    to select architectures for each task at hand, and then optimize the hyper-parameters
    to get the best performance out of it. Another challenge limiting their potential
    was the difficulty in handling long-range dependencies of the data, surfacing
    the issues of vanishing gradients, loss of context over long sequences, and the
    inability to capture global context due to locality constraints. Additionally,
    the lack of scalability and parallelization in traditional models slowed training
    on large datasets, holding back the progress in the field.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，模型的指数级进展与Transformer架构的出现密切相关。以前，人工智能科学家需要为每个任务选择架构，然后优化超参数以获得最佳性能。另一个限制其潜力的挑战是处理数据长程依赖性的困难，导致了梯度消失、长序列中上下文丢失的问题，以及由于局部性约束无法捕获全局上下文。此外，传统模型缺乏可扩展性和并行化，减缓了大数据集的训练进程，阻碍了该领域的进展。
- en: The Transformer architecture revolutionized the field by addressing these issues
    through its self-attention mechanism. It enabled models to capture relationships
    over long sequences and efficiently understand global context, all while being
    highly parallelizable and adaptable across various modalities, such as text, images,
    and more. In the self-attention mechanism, for each token, its query is compared
    against the keys of all other tokens to compute similarity scores. These similarities
    are then used to weigh the value vectors, which ultimately decide where the current
    token should attend to. Self-attention treats all tokens as equally important
    regardless of their order, losing critical information about the sequence in which
    tokens appear, and in other words, it sees the input data as a set with no order.
    Now we need a mechanism to enforce some notion of order on the data, as natural
    language and many other types of data are inherently sequential and position-sensitive.
    This is where positional embeddings come into play. Positional embeddings encode
    the position of each token in the sequence, enabling the model to maintain awareness
    of the sequence’s structure. Various methods for encoding positional information
    have been explored, and we will cover them in this blog post.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构通过其自注意力机制解决了这些问题，彻底改变了这一领域。它使得模型能够捕捉长序列之间的关系，并高效地理解全局上下文，同时具有高度的并行化能力，并能在各种模态下适应，例如文本、图像等。在自注意力机制中，对于每个令牌，它的查询与所有其他令牌的键进行比较，以计算相似度得分。这些相似度然后用于加权值向量，最终决定当前令牌应该关注哪里。自注意力将所有令牌视为同等重要，而不考虑它们的顺序，丧失了关于令牌出现顺序的关键信息，换句话说，它将输入数据视为没有顺序的集合。现在，我们需要一种机制来强制在数据中施加某种顺序的概念，因为自然语言和许多其他类型的数据本质上是顺序性的和位置敏感的。这就是位置嵌入发挥作用的地方。位置嵌入编码了每个令牌在序列中的位置，使模型能够保持对序列结构的意识。已经探索了多种编码位置信息的方法，我们将在本博客文章中讨论这些方法。
- en: '![](../Images/998a5ddc31efe8bb315ebe9eab6c558c.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/998a5ddc31efe8bb315ebe9eab6c558c.png)'
- en: Image generated by DALL-E
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 由DALL-E生成的图像
- en: '**Attention Mechanism:**'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**注意力机制：**'
- en: 'Let *S = {wi}* for *i =1,…,N* be a sequence of *N* input tokens where *wi*
    represents the *i*-th token. Hence, the corresponding token embedding of *S* can
    be denoted as *E = {xi}* for *i =1,…,N* where *xi* is the *d*-dimensional token
    embedding vector for token *wi*. The self-attention mechanism incorporates position
    embedding into token embeddings and generates the query, key, and value representations
    as:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 设*S = {wi}*，其中*i = 1, …, N*，表示一个由*N*个输入令牌组成的序列，其中*wi*表示第*i*个令牌。因此，*S*的对应令牌嵌入可以表示为*E
    = {xi}*，其中*i = 1, …, N*，*xi*是第*i*个令牌*wi*的*d*维令牌嵌入向量。自注意力机制将位置嵌入融入令牌嵌入，并生成查询、键和值的表示形式，如下所示：
- en: '![](../Images/764b45fac9c35b4504c2338f65a6995b.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/764b45fac9c35b4504c2338f65a6995b.png)'
- en: 'Then, the attention weights is computed based on the similarity between query
    and key vectors:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，注意力权重根据查询和键向量之间的相似度计算：
- en: '![](../Images/6d9f343c6adfcbfcddf0bfb8dc532620.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d9f343c6adfcbfcddf0bfb8dc532620.png)'
- en: 'The attention weights determine how important token *n* is for token *m*. In
    the other words, how much attention token *m* should pay to token *n*. The output
    for token *m* is computed as a weighted sum of the value vectors:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力权重决定了令牌*n*对令牌*m*的重要性。换句话说，就是令牌*m*应该给予令牌*n*多少注意力。令牌*m*的输出是作为值向量的加权和计算的：
- en: '![](../Images/5303090e8fe3e310ecf8ee7321a4e822.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5303090e8fe3e310ecf8ee7321a4e822.png)'
- en: Therefore, the attention mechanism token *m* to gather information from other
    tokens in the sequence.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，注意力机制使得令牌*m*能够从序列中的其他令牌收集信息。
- en: '![](../Images/5d1d99e883ab4f86c0d399614800f9ef.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d1d99e883ab4f86c0d399614800f9ef.png)'
- en: Fig 1\. Positional encoding in transformer architecture (image from [paper](https://arxiv.org/pdf/1706.03762)).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图1. Transformer架构中的位置编码（图像来自[论文](https://arxiv.org/pdf/1706.03762)）。
- en: '**1\. Absolute Position Embedding:**'
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**1. 绝对位置嵌入：**'
- en: 'A typical choice for the equation (1) is to have:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 方程（1）的一个典型选择是：
- en: '![](../Images/1009b706a42c879f4af02733f5d9c422.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1009b706a42c879f4af02733f5d9c422.png)'
- en: Where *pi* is a *d*-dimensional vector, representing the absolute position of
    token *xi*. Sinusoidal positional encoding and learned positional encoding are
    two alternatives to generate *pi*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*pi*是一个*d*维向量，表示令牌*xi*的绝对位置。正弦位置编码和学习的位置编码是生成*pi*的两种替代方法。
- en: '**1.a Sinusoidal Positional Encoding**'
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**1.a 正弦位置编码**'
- en: 'Sinusoidal positional encoding was introduced in the “[Attention is all you
    need](https://arxiv.org/pdf/1706.03762)” paper where transformer architecture
    was proposed. Sinusoidal Positional Encoding provides a unique position representation
    for each token in the input sequence. It is based on sine and cosine functions
    with different frequencies as:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正弦位置编码在“[Attention is all you need](https://arxiv.org/pdf/1706.03762)”论文中被引入，提出了Transformer架构。正弦位置编码为输入序列中的每个token提供了一个独特的位置表示。它基于具有不同频率的正弦和余弦函数，如下所示：
- en: '![](../Images/108d3fd539207a2e549a52e501c3172e.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/108d3fd539207a2e549a52e501c3172e.png)'
- en: Where *pos* is the position of the token in the sequence, *d* is the position
    embedding dimension, and i is the dimension index (*0<=i<d*).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*pos*是token在序列中的位置，*d*是位置嵌入的维度，i是维度索引（*0<=i<d*）。
- en: 'The use of sine and cosine functions in sinusoidal positional encoding has
    a deep relationship with the **Fourier transform.** By using a range of different
    frequencies to encode positions, the Transformer creates a representation similar
    to a Fourier transform where:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 正弦位置编码中使用的正弦和余弦函数与**傅里叶变换**有着深刻的关系。通过使用不同频率的范围来编码位置，Transformer创建了一个类似于傅里叶变换的表示，其中：
- en: '**High-frequency components** (lower *i*) enable the model to capture local
    positional information. This is useful for understanding relationships between
    neighbor tokens in a sequence, such as word pairs.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高频成分**（较低的*i*）使模型能够捕捉到局部位置的关系。这对于理解序列中相邻token之间的关系非常有用，例如词对。'
- en: '**Low-frequency components** (higher *i*) capture more global patterns over
    the entire sequence. This helps the model to focus on broader relationships between
    tokens that may be far apart, such as dependencies between words in two different
    sentences.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低频成分**（较高的*i*）捕捉整个序列中的更全局的模式。这有助于模型关注可能相距较远的token之间的广泛关系，例如两句话之间的依赖关系。'
- en: This helps the model understand the relative positions of tokens by comparing
    their positional encodings. Sinusoidal positional encoding needs no additional
    training parameters while generalizing to larger sequence lengths at inference
    time. However, its expressiveness is limited.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这有助于模型通过比较位置编码来理解token之间的相对位置。正弦位置编码不需要额外的训练参数，并且在推理时能泛化到更长的序列长度。然而，它的表现力有限。
- en: '**1.b Learned Positional Encoding**'
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**1.b 学习型位置编码**'
- en: Learned positional encoding was introduced in the “[Attention is all you need](https://arxiv.org/pdf/1706.03762)”
    paper and it was applied in the [BERT](https://arxiv.org/pdf/1810.04805) and [GPT](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
    models as an alternative to Sinusoidal positional encoding. In learned positional
    encoding, each position in the sequence (e.g. first token, second token, etc)
    is assigned an embedding vector. These position embeddings are learned along with
    other transformer parameters during training. For example, if the model has a
    context length of 512 with a token embedding of size 768 (i.e. *d*=768), a learnable
    tensor of size 512*768 will be added to the other trainable parameters. This means
    the model gradually learns the best way to encode positional information for the
    specific task, such as text classification or translation.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 学习型位置编码在“[Attention is all you need](https://arxiv.org/pdf/1706.03762)”论文中被引入，并在[BERT](https://arxiv.org/pdf/1810.04805)和[GPT](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)模型中作为正弦位置编码的替代方案。对于学习型位置编码，序列中的每个位置（例如，第一个token，第二个token等）都会分配一个嵌入向量。这些位置嵌入在训练过程中与其他Transformer参数一起学习。例如，如果模型的上下文长度为512，token嵌入的维度为768（即*d*=768），则会将一个大小为512*768的可学习张量加入到其他可训练参数中。这意味着模型逐渐学习如何为特定任务（如文本分类或翻译）编码位置信息。
- en: Learned positional embedding is more expressive than sinusoidal one as the model
    can learn a position embedding, effective for its specific task. However, they
    introduce more trainable parameters which increases the model size and its computational
    cost.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 学习型位置嵌入比正弦位置嵌入更具表现力，因为模型可以学习一个特定任务有效的位置嵌入。然而，它们引入了更多的可训练参数，从而增加了模型的大小和计算成本。
- en: '**2\. Relative Positional Embeddings**'
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**2. 相对位置嵌入**'
- en: Both sinusoidal and learned position encodings focused on the absolute position
    of the token. However, the attention mechanism works by computing how important
    other tokens are for each specific token in the sequence. Hence, this process
    depends on the relative position of the tokens (how far apart they are from each
    other), rather than the absolute position of the tokens. To address the limitations
    of absolute position embedding, relative position encoding was introduced.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 正弦型和学习型位置编码侧重于标记的绝对位置。然而，注意力机制是通过计算其他标记对每个特定标记的重要性来工作的。因此，这一过程依赖于标记的相对位置（它们之间的距离），而不是标记的绝对位置。为了解决绝对位置嵌入的局限性，引入了相对位置编码。
- en: '[RelativePosEmb](https://arxiv.org/pdf/1803.02155) doesn’t add position information
    to token embeddings. Instead, it modifies the way key and value are computed at
    every layer as:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[RelativePosEmb](https://arxiv.org/pdf/1803.02155) 不将位置信息添加到标记嵌入中。相反，它修改了每一层计算键（key）和值（value）的方法，如下所示：'
- en: '![](../Images/72e5b379418a7f719d2e2f6566b6c4bd.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/72e5b379418a7f719d2e2f6566b6c4bd.png)'
- en: Here, *r = clip(m-n, Rmin, Rmax)* represents the relative distance between position
    m and n. The maximum relative position is clipped, assuming that precise relative
    position is not useful beyond a certain distance. Clipping the maximum distance
    enables the model to extrapolate at inference time, i.e. to generalize to sequence
    length not seen during training. However, this approach may miss some useful information
    from the absolute position of the token (like the position of the first token).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*r = clip(m-n, Rmin, Rmax)* 表示位置 m 和 n 之间的相对距离。最大相对位置会被裁剪，假设超出某个距离后的精确相对位置没有用处。裁剪最大距离使得模型在推理时能够外推，即能够推广到训练时未见过的序列长度。然而，这种方法可能会丢失一些来自标记绝对位置的有用信息（例如第一个标记的位置）。
- en: You may notice that *fq* lacks position embedding. That’s because we are encoding
    the relative position. In the attention formula, the query and key values are
    used to compute attention weights as equation (2) therefore we only need either
    the query or the key to include the relative position embedding.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到 *fq* 缺少位置编码。这是因为我们正在编码相对位置。在注意力公式中，查询（query）和值（key）用于计算注意力权重，如公式（2）所示，因此我们只需要查询或键之一来包含相对位置编码。
- en: This encoding has been used in many models as [Transformer-XL](https://arxiv.org/pdf/1901.02860)
    and [T5](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf). There are different
    alternatives in applying relative positional encoding that you can find in papers
    [[7]](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf) and [[8]](https://arxiv.org/pdf/2006.03654)
    .
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这种编码已在许多模型中使用，如 [Transformer-XL](https://arxiv.org/pdf/1901.02860) 和 [T5](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf)。在应用相对位置编码时，有不同的替代方法，可以在文献
    [[7]](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf) 和 [[8]](https://arxiv.org/pdf/2006.03654)
    中找到。
- en: '**3\. Rotary Positional Embedding (RoPE)**'
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**3\. 旋转位置嵌入（RoPE）**'
- en: 'Unlike previous methods, [RoPE](https://arxiv.org/pdf/2104.09864) rotates the
    vectors in a multi-dimensional space based on the position of tokens. Instead
    of adding position information to token embeddings, it modifies the way attention
    weights are computed at every layer as:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 与以往的方法不同，[RoPE](https://arxiv.org/pdf/2104.09864)基于标记的位置，在多维空间中旋转向量。它不是将位置信息添加到标记嵌入中，而是修改了每一层计算注意力权重的方式，如下所示：
- en: '![](../Images/2c2c747a497c0ec921bc2dc550a8ac7f.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2c2c747a497c0ec921bc2dc550a8ac7f.png)'
- en: 'They proposed a generalized rotation matrix to any even embedding dimensionality
    *d* as:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 他们提出了一个通用的旋转矩阵，适用于任何偶数维度的嵌入 *d*，公式如下：
- en: '![](../Images/a9b8dc8e3f08c5dbc3709a5c8eef9607.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9b8dc8e3f08c5dbc3709a5c8eef9607.png)'
- en: 'Where *θi* is pre-defined:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*θi* 是预定义的：
- en: '![](../Images/e7d44151cb5fc03ca784f80c9665741b.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7d44151cb5fc03ca784f80c9665741b.png)'
- en: 'Applying [RoPE](https://arxiv.org/pdf/2104.09864) to attention weight yields
    to:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 将 [RoPE](https://arxiv.org/pdf/2104.09864) 应用到注意力权重中得到：
- en: '![](../Images/6d510a7dbb26896bd120fdcbb45ff781.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d510a7dbb26896bd120fdcbb45ff781.png)'
- en: Note that [RoPE](https://arxiv.org/pdf/2104.09864) formulation doesn’t add position
    information to the values in the attention module. The output of the attention
    module is a weighted sum of the value vector and since position information isn’t
    added to values, the outputs of each transformer layer don’t have explicit position
    details.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，[RoPE](https://arxiv.org/pdf/2104.09864)的公式并未将位置信息添加到注意力模块的值中。注意力模块的输出是值向量的加权和，由于位置信息没有添加到值中，每个
    Transformer 层的输出没有显式的位置信息。
- en: Popular models such as [LLaMA](https://arxiv.org/pdf/2302.13971) and [GPT-NeoX](https://arxiv.org/pdf/2204.06745#cite.su2021roformer)
    are using [RoPE](https://arxiv.org/pdf/2104.09864).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 像[LLaMA](https://arxiv.org/pdf/2302.13971)和[GPT-NeoX](https://arxiv.org/pdf/2204.06745#cite.su2021roformer)这样的流行模型正在使用[RoPE](https://arxiv.org/pdf/2104.09864)。
- en: '![](../Images/042b42f1c09ae586cd90a211c1723857.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/042b42f1c09ae586cd90a211c1723857.png)'
- en: 'Fig 2: [ALiBi](https://arxiv.org/pdf/2108.12409) method visualization (image
    from [paper](https://arxiv.org/pdf/2108.12409)).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：[ALiBi](https://arxiv.org/pdf/2108.12409)方法的可视化（图像来源于[论文](https://arxiv.org/pdf/2108.12409)）。
- en: '**4\. Attention with Linear Biases (ALiBi)**'
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**4. 线性偏置注意力（ALiBi）**'
- en: '[ALiBi](https://arxiv.org/pdf/2108.12409) also does not add positional encodings
    to word embeddings; instead, it adds a penalty to attention weight scores that
    is proportional to the distance between tokens. Therefore, the attention score
    between two tokens i and j at every layer is calculated as:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[ALiBi](https://arxiv.org/pdf/2108.12409)也不将位置编码添加到词嵌入中；相反，它通过对注意力权重得分施加一个与标记之间距离成比例的惩罚。因此，两个标记i和j之间的注意力得分在每一层的计算方式如下：'
- en: '*Attention score = query_i . key_j — m.(i-j)*'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*Attention score = query_i . key_j — m.(i-j)*'
- en: 'Where *-m.(i-j)* is a penalty which is proportional to the distance between
    token *i* and *j*. The scalar *m* is a head-specific slope fixed before training
    and its values for different heads are chosen as a geometric sequence. For example,
    for 8 head, *m* might be:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*-m.(i-j)*是一个惩罚项，与标记*i*和*j*之间的距离成正比。标量*m*是一个特定于头部的斜率，在训练前就已固定，其值对于不同的头部会按照几何序列选择。例如，对于8个头部，*m*可能是：
- en: '![](../Images/7a0c79a5fea18ba0aabfa8f9b196834a.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a0c79a5fea18ba0aabfa8f9b196834a.png)'
- en: This means, the first head has a relatively large *m* so it penalizes far apart
    tokens more and focuses on recent tokens, while the 8th head has the smallest
    *m*, allowing it to attend to more distant tokens. Fig. 2 also offers visualization.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，第一个头有相对较大的*m*值，因此它对远离的标记惩罚更多，专注于最近的标记，而第八个头有最小的*m*值，允许它关注更远的标记。图2也提供了可视化展示。
- en: ALiBi is used in [BloombergGPT](https://arxiv.org/pdf/2303.17564) and [BLOOM](https://inria.hal.science/hal-03850124v1/document).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ALiBi被用于[BloombergGPT](https://arxiv.org/pdf/2303.17564)和[BLOOM](https://inria.hal.science/hal-03850124v1/document)。
- en: '**Transformer Extrapolation At Inference Time:**'
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**推理时的Transformer外推：**'
- en: Transformer extrapolation at inference time is the model’s ability to perform
    well to input sequences that are longer than those it was trained on. The transformer
    mechanism is agnostic to input length which means at inference time, it can work
    with longer sequences. However, note that the computational cost grows quadratically
    with input length even though the transformer layers themselves are agnostic to
    it.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer推理时的外推能力指的是模型在输入序列比训练时的序列更长的情况下，仍然能够良好地表现。Transformer机制对输入长度是不可知的，这意味着在推理时，它可以处理更长的序列。然而，需要注意的是，计算成本随着输入长度的增加呈二次增长，即使Transformer层本身对输入长度是不可知的。
- en: The authors of [ALiBi](https://arxiv.org/pdf/2108.12409) demonstrated that the
    bottleneck for transformer extrapolation is its position embedding method. As
    shown in Fig. 3, they compared the extrapolation capabilities of different position
    embedding methods. Since learned position embedding does not have a capability
    to encode positions greater than the training length, it has no extrapolation
    ability.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[ALiBi](https://arxiv.org/pdf/2108.12409)的作者证明了，transformer外推的瓶颈在于其位置编码方法。如图3所示，他们比较了不同位置编码方法的外推能力。由于学习到的位置编码无法编码比训练长度更长的位置，因此它没有外推能力。'
- en: '![](../Images/463e1fb96f99aff8dbb3dd6fad4ab1f4.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/463e1fb96f99aff8dbb3dd6fad4ab1f4.png)'
- en: 'Fig 3: Extrapolation: as the input sequence gets longer (x-axis), [sinusoidal](https://user.phil.hhu.de/~cwurm/wp-content/uploads/2020/01/7181-attention-is-all-you-need.pdf),
    [RoPE](https://arxiv.org/pdf/2104.09864), and [T5](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf)
    position encodings show degraded perplexity (y-axis, lower is better), while [ALiBi](https://arxiv.org/pdf/2108.12409)
    does not (image from [paper](https://arxiv.org/pdf/2108.12409)).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：外推：随着输入序列变长（x轴），[正弦](https://user.phil.hhu.de/~cwurm/wp-content/uploads/2020/01/7181-attention-is-all-you-need.pdf)、[RoPE](https://arxiv.org/pdf/2104.09864)和[T5](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf)的位置编码表现出困惑度下降（y轴，数值越低越好），而[ALiBi](https://arxiv.org/pdf/2108.12409)则没有（图像来源于[论文](https://arxiv.org/pdf/2108.12409)）。
- en: Fig. 3 shows that the sinusoidal position embedding in practice has very limited
    extrapolation capabilities. While [RoPE](https://arxiv.org/pdf/2104.09864) outperforms
    the sinusoidal one, it still does not achieve satisfactory results. The [T5](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf)
    bias method (a version of relative position embedding) leads to better extrapolation
    than both sinusoidal and [RoPE](https://arxiv.org/pdf/2104.09864) embedding. Unfortunately,
    the [T5](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf) bias is computationally
    expensive (Fig. 4). [ALiBi](https://arxiv.org/pdf/2108.12409) outperforms all
    these position embeddings with negligible (0–0.7%) memory increase.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图3显示了实际应用中，正弦位置嵌入的外推能力非常有限。尽管[RoPE](https://arxiv.org/pdf/2104.09864)优于正弦嵌入，但仍然未能取得令人满意的结果。[T5](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf)偏置方法（相对位置嵌入的一种形式）在外推方面优于正弦嵌入和[RoPE](https://arxiv.org/pdf/2104.09864)嵌入。不幸的是，[T5](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf)偏置在计算上开销较大（见图4）。[ALiBi](https://arxiv.org/pdf/2108.12409)在所有这些位置嵌入中表现最佳，且仅有轻微的（0-0.7%）内存增加。
- en: '![](../Images/5de14ba673b2dde6cc992ccadda9135d.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5de14ba673b2dde6cc992ccadda9135d.png)'
- en: 'Fig. 4: comparison of batched training, inference speed and memory use of [sinusoidal](https://user.phil.hhu.de/~cwurm/wp-content/uploads/2020/01/7181-attention-is-all-you-need.pdf),
    [RoPE](https://arxiv.org/pdf/2104.09864), [T5](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf),
    and [ALiBi](https://arxiv.org/pdf/2108.12409) position encodings (image from [paper](https://arxiv.org/pdf/2108.12409)).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：批量训练、推理速度和[正弦](https://user.phil.hhu.de/~cwurm/wp-content/uploads/2020/01/7181-attention-is-all-you-need.pdf)、[RoPE](https://arxiv.org/pdf/2104.09864)、[T5](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf)和[ALiBi](https://arxiv.org/pdf/2108.12409)位置编码的比较（图片来自[论文](https://arxiv.org/pdf/2108.12409)）。
- en: 'Conclusion:'
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论：
- en: In summary, the way positional information is being encoded in Transformer architecture
    significantly affects its ability to understand sequential data, especially its
    extrapolation at inference time. While absolute positional embedding methods provide
    positional awareness, they often struggle with Transformer extrapolation. That’s
    why newer position embeddings are proposed. Relative position encoding, RoPE,
    and ALiBi have the capability to extrapolate at inference time. As transformers
    continue to be integrated in various applications, refining position encoding
    is crucial to push the boundaries of their performance.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 总结：在Transformer架构中，位置编码方式显著影响其理解顺序数据的能力，尤其是在推理时的外推能力。虽然绝对位置嵌入方法提供了位置信息，但它们常常难以应对Transformer的外推能力。因此，提出了更新的位置嵌入方法。相对位置编码、RoPE和ALiBi具备在推理时外推的能力。随着Transformer不断在各类应用中被集成，优化位置编码对于提升其性能边界至关重要。
- en: The opinions expressed in this blog post are solely our own and do not reflect
    those of our employer.
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本文中表达的观点仅代表我们个人的看法，并不反映我们雇主的立场。
- en: '**References:**'
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**参考文献：**'
- en: '[1] Vaswani, A. “Attention is all you need.” (2017).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Vaswani, A. “Attention is all you need.” (2017)。'
- en: '[2] [BERT](https://arxiv.org/pdf/1810.04805): Devlin, Jacob. “Bert: Pre-training
    of deep bidirectional transformers for language understanding.” (2018).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [BERT](https://arxiv.org/pdf/1810.04805)：Devlin, Jacob. “Bert：用于语言理解的深度双向Transformer预训练。”
    (2018)。'
- en: '[3] [GPT](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf):
    Radford, Alec, et al. “Language models are unsupervised multitask learners.” (2019).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [GPT](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)：Radford,
    Alec 等. “语言模型是无监督的多任务学习者.” (2019)。'
- en: '[4] [RelativePosEmb](https://arxiv.org/pdf/1803.02155): Shaw, Peter, et al.
    “Self-attention with relative position representations.” (2018).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [RelativePosEmb](https://arxiv.org/pdf/1803.02155)：Shaw, Peter 等. “带有相对位置表示的自注意力.”
    (2018)。'
- en: '[5] [Transformer-XL](https://arxiv.org/pdf/1901.02860) Dai, Zihang. “Transformer-xl:
    Attentive language models beyond a fixed-length context.” (2019).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [Transformer-XL](https://arxiv.org/pdf/1901.02860) Dai, Zihang. “Transformer-xl：超越固定长度上下文的注意力语言模型。”
    (2019)。'
- en: '[6] [T5](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf): Raffel, Colin,
    et al. “Exploring the limits of transfer learning with a unified text-to-text
    transformer.” (2020).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] [T5](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf)：Raffel, Colin
    等. “通过统一的文本到文本转换器探索迁移学习的极限。” (2020)。'
- en: '[7] Raffel, Colin, et al. “Exploring the limits of transfer learning with a
    unified text-to-text transformer.” (2020)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Raffel, Colin 等. “通过统一的文本到文本转换器探索迁移学习的极限。” (2020)'
- en: '[8] He, Pengcheng, et al. “Deberta: Decoding-enhanced bert with disentangled
    attention.” (2020).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] He, Pengcheng, 等. “Deberta：解码增强的BERT模型，具有解耦注意力。”（2020年）。'
- en: '[9] [RoPE](https://arxiv.org/pdf/2104.09864): Su, Jianlin, et al. “Roformer:
    Enhanced transformer with rotary position embedding.” (2024).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] [RoPE](https://arxiv.org/pdf/2104.09864): Su, Jianlin, 等. “Roformer：具有旋转位置编码的增强型变换器。”（2024年）。'
- en: '[10] [LLaMA](https://arxiv.org/pdf/2302.13971): Touvron, Hugo, et al. “Llama:
    Open and efficient foundation language models.” (2023).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] [LLaMA](https://arxiv.org/pdf/2302.13971): Touvron, Hugo, 等. “Llama：开源且高效的基础语言模型。”（2023年）。'
- en: '[11] [GPT-NeoX](https://arxiv.org/pdf/2204.06745#cite.su2021roformer): Black,
    Sid, et al. “Gpt-neox-20b: An open-source autoregressive language model.” (2022).'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] [GPT-NeoX](https://arxiv.org/pdf/2204.06745#cite.su2021roformer): Black,
    Sid, 等. “Gpt-neox-20b：一个开源自回归语言模型。”（2022年）。'
- en: '[12] [ALiBi](https://arxiv.org/pdf/2108.12409): Press, Ofir, et al. “Train
    short, test long: Attention with linear biases enables input length extrapolation.”
    (2021).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] [ALiBi](https://arxiv.org/pdf/2108.12409): Press, Ofir, 等. “训练短，测试长：具有线性偏置的注意力机制能够进行输入长度外推。”（2021年）。'
- en: '[13] [BloombergGPT](https://arxiv.org/pdf/2303.17564): Wu, Shijie, et al. “Bloomberggpt:
    A large language model for finance.” (2023).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] [BloombergGPT](https://arxiv.org/pdf/2303.17564): Wu, Shijie, 等. “Bloomberggpt：用于金融的大型语言模型。”（2023年）。'
- en: '[14] [BLOOM](https://inria.hal.science/hal-03850124v1/document): Le Scao, Teven,
    et al. “Bloom: A 176b-parameter open-access multilingual language model.” (2023).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] [BLOOM](https://inria.hal.science/hal-03850124v1/document): Le Scao, Teven,
    等. “Bloom：一个具有1760亿参数的开源多语言语言模型。”（2023年）。'
