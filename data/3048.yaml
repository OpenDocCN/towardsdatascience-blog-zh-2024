- en: 'Understanding Deduplication Methods: Ways to Preserve the Integrity of Your
    Data'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解去重方法：保护数据完整性的方法
- en: 原文：[https://towardsdatascience.com/understanding-deduplication-methods-ways-to-preserve-the-integrity-of-your-data-545651313c12?source=collection_archive---------3-----------------------#2024-12-20](https://towardsdatascience.com/understanding-deduplication-methods-ways-to-preserve-the-integrity-of-your-data-545651313c12?source=collection_archive---------3-----------------------#2024-12-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/understanding-deduplication-methods-ways-to-preserve-the-integrity-of-your-data-545651313c12?source=collection_archive---------3-----------------------#2024-12-20](https://towardsdatascience.com/understanding-deduplication-methods-ways-to-preserve-the-integrity-of-your-data-545651313c12?source=collection_archive---------3-----------------------#2024-12-20)
- en: '*Increasing growth and data complexities have made data deduplication even
    more relevant*'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*增长和数据复杂性的增加使得数据去重变得更加重要*'
- en: '[](https://medium.com/@rendysatriadalimunthe?source=post_page---byline--545651313c12--------------------------------)[![Rendy
    Dalimunthe](../Images/efcd8304211d187271847f4ecc5fb1b1.png)](https://medium.com/@rendysatriadalimunthe?source=post_page---byline--545651313c12--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--545651313c12--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--545651313c12--------------------------------)
    [Rendy Dalimunthe](https://medium.com/@rendysatriadalimunthe?source=post_page---byline--545651313c12--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@rendysatriadalimunthe?source=post_page---byline--545651313c12--------------------------------)[![Rendy
    Dalimunthe](../Images/efcd8304211d187271847f4ecc5fb1b1.png)](https://medium.com/@rendysatriadalimunthe?source=post_page---byline--545651313c12--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--545651313c12--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--545651313c12--------------------------------)
    [Rendy Dalimunthe](https://medium.com/@rendysatriadalimunthe?source=post_page---byline--545651313c12--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--545651313c12--------------------------------)
    ·6 min read·Dec 20, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--545651313c12--------------------------------)
    ·阅读时长6分钟·2024年12月20日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Data duplication is still a problem for many organisations. Although data processing
    and storage systems have developed rapidly along with technological advances,
    the complexity of the data produced is also increasing. Moreover, with the proliferation
    of Big Data and the utilisation of cloud-based applications, today’s organisations
    must increasingly deal with fragmented data sources.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 数据重复仍然是许多组织面临的问题。尽管数据处理和存储系统随着技术进步快速发展，但所产生的数据复杂性也在增加。此外，随着大数据的普及和基于云的应用程序的使用，今天的组织必须越来越多地处理碎片化的数据源。
- en: '![](../Images/4f51c3e5611e9f4e0163f4f378461105.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f51c3e5611e9f4e0163f4f378461105.png)'
- en: 'Photo by Damir: [https://www.pexels.com/photo/serene-lakeside-reflection-with-birch-trees-29167854/](https://www.pexels.com/photo/serene-lakeside-reflection-with-birch-trees-29167854/)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由Damir拍摄： [https://www.pexels.com/photo/serene-lakeside-reflection-with-birch-trees-29167854/](https://www.pexels.com/photo/serene-lakeside-reflection-with-birch-trees-29167854/)
- en: 'Ignoring the phenomenon of the large amount of duplicated data will have a
    negative impact on the organisation. Such as:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 忽视大量重复数据的现象将对组织产生负面影响。例如：
- en: 'Disruption of the decision-making process. Unclean data can bias metrics and
    not reflect the actual conditions. For example: if there is one customer that
    is actually the same, but is represented as 2 or 3 customers data in CRM, this
    can be a distortion when projecting revenue.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策过程的中断。不干净的数据会偏差指标，无法反映实际情况。例如：如果一个客户实际上是同一个人，但在客户关系管理系统（CRM）中被表示为2或3个客户的数据，这可能会导致在预测收入时的扭曲。
- en: Swelling storage costs because every bit of data basically takes up storage
    space.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储成本膨胀，因为每一位数据都会占用存储空间。
- en: 'Disruption of customer experience. For example: if the system has to provide
    notifications or send emails to customers, it is very likely that customers whose
    data is duplicate will receive more than one notification.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户体验的中断。例如：如果系统需要向客户提供通知或发送电子邮件，数据重复的客户很可能会收到多条通知。
- en: Making the AI ​​training process less than optimal. When an organisation starts
    developing an AI solution, one of the requirements is to conduct training with
    clean data. If there is still a lot of duplicates, the data cannot be said to
    be clean and when forced to be used in AI training, it will potentially produce
    biased AI.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这使得AI训练过程不够理想。当一个组织开始开发AI解决方案时，其中一个要求是使用干净的数据进行训练。如果数据中仍然有很多重复项，那么数据就不能算是干净的，而在强制用于AI训练时，可能会产生偏见的AI。
- en: Given the crucial impact caused when an organisation does not attempt to reduce
    or eliminate data duplication, the process of data deduplication becomes increasingly
    relevant. It is also critical to ensure data quality. The growing sophistication
    and complexity of the system must be accompanied by the evolution of adequate
    deduplication techniques.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到当一个组织未尝试减少或消除数据重复时所产生的重大影响，数据去重过程变得越来越重要。确保数据质量也至关重要。系统的日益复杂性和精密性必须伴随适当的去重技术的演变。
- en: On this occasion, we will examine the 3 latest deduplication methods, which
    can be a reference for practitioners when planning the deduplication process.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个场合，我们将研究三种最新的去重方法，这些方法可以作为从业者规划去重过程的参考。
- en: '**Global Deduplication**'
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**全局去重**'
- en: It is the process of eliminating duplicate data across multiple storage locations.
    It is now common for organisations to store their data across multiple servers,
    data centers, or the cloud. Global deduplication ensures that only one copy of
    the data is stored.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这是消除多个存储位置中重复数据的过程。现在，许多组织将数据存储在多个服务器、数据中心或云端。全局去重确保仅存储数据的一个副本。
- en: This method works by creating a global index, which is a list of all existing
    data, in the form of a unique code (hash) using an algorithm such as SHA256 that
    represents each piece of data. When a new file is uploaded to a server (for example
    Server 1), the system will store a unique code for that file.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法通过创建一个全局索引来工作，全球索引是以唯一代码（哈希）的形式列出所有现有数据，使用如SHA256等算法表示每一条数据。当新文件被上传到服务器（例如服务器1）时，系统将为该文件存储一个唯一代码。
- en: On another day when a user uploads a file to Server 2, the system will compare
    the unique code of the new file with the global index. If the new file is found
    to have the same unique code/hash as the global index, then instead of continuing
    to store the same file in two places, the system will replace the duplicate file
    stored on Server 2 with a reference/pointer that points to a copy of the file
    that already exists on Server 1.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一天，当用户向服务器2上传文件时，系统将把新文件的唯一代码与全局索引进行比较。如果新文件的唯一代码/哈希与全局索引相同，那么系统将不再将相同的文件存储在两个地方，而是会用指向服务器1上已有文件副本的引用/指针替换存储在服务器2上的重复文件。
- en: With this method, storage space can clearly be saved. And if combined with Data
    Virtualisation technique then when the file is needed the system will take it
    from the original location, but all users will still feel the data is on their
    respective servers.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法，可以明显节省存储空间。如果结合数据虚拟化技术，当文件需要时，系统将从原始位置获取该文件，但所有用户仍然会感觉数据存在各自的服务器上。
- en: The illustration below shows how Global Deduplication works where each server
    only stores one copy of the original data and duplicates on other servers are
    replaced by references to the original file.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的插图展示了全局去重的工作方式，其中每个服务器只存储原始数据的一份副本，其他服务器上的重复文件则被替换为指向原始文件的引用。
- en: '![](../Images/3f4e9488dfd094fa8ae70dfc88dab168.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f4e9488dfd094fa8ae70dfc88dab168.png)'
- en: 'source: Author'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者
- en: It should be noted that the Global Deduplication method does not work in real-time,
    but post-process. Which means the method can only be applied when the file has
    entered storage.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，全局去重方法并不实时工作，而是后处理的。这意味着该方法只能在文件进入存储后应用。
- en: '**Inline Deduplication**'
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**内联去重**'
- en: Unlike Global Deduplication, this method works in real-time right when data
    is being written to the storage system. With the Inline Deduplication technique,
    duplicate data is immediately replaced with references without going through the
    physical storage process.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 与全局去重不同，这种方法在数据写入存储系统时实时工作。使用内联去重技术，重复数据会立即被替换为引用，而无需经过物理存储过程。
- en: 'The process begins when data is about to enter the system or a file is being
    uploaded, the system will immediately divide the file into several small pieces
    or chunks. Using an algorithm such as SHA-256, each chunk will then be given a
    hash value as a unique code. Example:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程从数据即将进入系统或文件上传开始，系统会立即将文件分成几个小块或数据块。使用如SHA-256等算法，每个数据块将被赋予一个哈希值作为唯一代码。例如：
- en: Chunk1 -> hashA
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Chunk1 -> hashA
- en: Chunk2-> hashB
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Chunk2 -> hashB
- en: Chunk3 -> hashC
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Chunk3 -> hashC
- en: The system will then check whether any of the chunks have hashes already in
    the storage index. If one of the chunks is found whose unique code is already
    in the storage hash, the system will not re-save the physical data from the chunk,
    but will only store a reference to the original chunk location that was previously
    stored.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 系统随后会检查各个数据块的哈希值是否已经在存储索引中。如果发现某个数据块的唯一代码已经存在于存储的哈希中，系统将不会重新保存该数据块的物理数据，而是仅存储指向之前存储的原始数据块位置的引用。
- en: While each unique chunk will be stored physically.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 每个唯一的数据块将被物理存储。
- en: Later, when a user wants to access the file, the system will rearrange the data
    from the existing chunks based on the reference, so that the complete file can
    be used by the user.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 后续，当用户想要访问文件时，系统会根据引用重新排列现有数据块中的数据，从而使用户能够使用完整的文件。
- en: Inline Deduplication is widely used by cloud service providers such as Amazon
    S3 or Google Drive. This method is very useful for optimising storage capacity.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 内联去重被亚马逊S3或Google Drive等云服务提供商广泛使用。此方法对于优化存储容量非常有用。
- en: The simple illustration below illustrates the Inline Deduplication process,
    from data chunking to how data is accessed.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 下图简要说明了内联去重过程，从数据分块到数据如何访问。
- en: '![](../Images/f2b7a8e0350a52dae6c633320882d999.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f2b7a8e0350a52dae6c633320882d999.png)'
- en: 'Source: Author'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者
- en: '**ML-Enhanced Deduplication**'
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**增强型机器学习去重**'
- en: Machine learning-powered deduplication uses AI to detect and remove duplicate
    data, even if it is not completely identical.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 基于机器学习的去重技术利用AI来检测和删除重复数据，即使它们并不完全相同。
- en: The process begins when incoming data, such as files/documents/records, are
    sent to the deduplication system for analysis. For example, the system receives
    two scanned documents that at first glance look similar but actually have subtle
    differences in layout or text format.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程从传入数据（如文件/文档/记录）发送到去重系统进行分析开始。例如，系统接收两个扫描的文档，乍一看它们看起来相似，但实际上在布局或文本格式上有细微差异。
- en: 'The system will then intelligently extract important features, usually in the
    form of metadata or visual patterns. These important features will then be analysed
    and compared for similarity. The similarity of a feature will be represented as
    a value/score. And each system/organisation can define whether data is a duplicate
    or not based on its similarity score. For example: only data with a similarity
    score above 90% can be said to be potentially duplicate.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 系统随后会智能地提取重要特征，通常以元数据或视觉模式的形式。这些重要特征会被分析并进行相似性比较。特征的相似度将以数值/分数的形式表示。每个系统/组织可以根据相似度分数来定义数据是否为重复数据。例如：只有相似度分数高于90%的数据才可以认为是潜在重复的。
- en: Based on the similarity score, the system can judge whether the data is a duplicate.
    If stated that it is a duplicate, then steps can be taken like other duplication
    methods, where for duplicate data only the reference is stored.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 基于相似度评分，系统可以判断数据是否为重复数据。如果确定为重复数据，则可以采取类似其他去重方法的步骤，对于重复数据仅存储引用。
- en: What’s interesting about ML-enhanced Deduplication is that it allows human involvement
    to validate the classification that has been done by the system. So that the system
    can continue to get smarter based on the inputs that have been learned (feedback
    loop).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 增强型机器学习去重的一个有趣之处在于，它允许人工参与验证系统已经完成的分类工作。这样，系统可以基于已学习的输入（反馈回路）不断变得更智能。
- en: However, it should be noted that unlike Inline Deduplication, ML-enhanced deduplication
    is not suitable for use in real-time. This is due to the latency factor, where
    ML takes time to extract features and process data. In addition, if forced to
    be real-time, this method requires more intensive computing resources.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然而需要注意的是，与内联去重不同，增强型机器学习去重不适合用于实时处理。这是由于延迟因素，机器学习需要时间来提取特征和处理数据。此外，如果强行要求实时处理，这种方法需要更多的计算资源。
- en: Although not real-time, the benefits it brings are still optimal, especially
    with its ability to handle unstructured or semi-structured data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然不是实时的，但它带来的好处仍然是最优的，特别是在处理非结构化或半结构化数据的能力上。
- en: The following is an illustration of the steps of ML-enhanced Deduplication along
    with examples.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是机器学习增强去重步骤的说明及示例。
- en: '![](../Images/9cee97d18c47fd9ff775e81a9583af79.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9cee97d18c47fd9ff775e81a9583af79.png)'
- en: 'Source: Author'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者
- en: From the explanation above, it is clear that organisations have many choices
    of methods according to their capabilities and needs. So there is no reason to
    not doing deduplication, especially if the data stored or handled by the organisation
    is critical data that concerns the lives of many people.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的解释可以看出，组织可以根据其能力和需求选择多种方法。因此，完全没有理由不进行去重，尤其是当组织存储或处理的数据是关乎许多人生命的关键数据时。
- en: Organisations should be able to use several consideration items to choose the
    most appropriate method. Aspects such as the purpose of deduplication, the type
    and volume of data, and the infrastructure capabilities of the organisation can
    be used for initial assessment.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 组织应能够使用若干考虑项来选择最合适的方法。去重的目的、数据类型和数据量以及组织的基础设施能力等方面可以作为初步评估的依据。
- en: It should be noted that there is flexible options that organisations can choose,
    such as a hybrid method of combining Inline Deduplication with the ML-enhanced
    counterpart. In that way, a wider benefit can be potentially obtained.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 应注意，组织可以选择灵活的选项，例如将内联去重与机器学习增强方法相结合的混合方法。通过这种方式，可以获得更广泛的收益。
- en: 'Data management regulations such as GDPR and HIPAA regulate sensitive information.
    Hence, organisations need to ensure that deduplication does not violate privacy
    policies. For example: an organisation could combine customer data from two different
    systems after detecting duplicates, without obtaining user consent. Organisations
    have to ensure this kind of thing is not happening.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管理法规，如GDPR和HIPAA，规范了敏感信息的处理。因此，组织需要确保去重过程不会违反隐私政策。例如：在发现重复数据后，组织可以合并来自两个不同系统的客户数据，而无需获得用户同意。组织必须确保此类事情不会发生。
- en: Whatever the challenge is, deduplication must still be done, and organisations
    need to make efforts from the start. Do not wait until the data gets bigger and
    deduplication efforts become more expensive and complex.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 无论面临什么挑战，去重仍然必须进行，组织需要从一开始就付出努力。不要等到数据变得更大，去重工作变得更加昂贵和复杂时才行动。
- en: Start now and reap the benefit along the way.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在就开始，并在过程中收获收益。
