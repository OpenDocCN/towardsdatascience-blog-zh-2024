- en: Unsupervised LLM Evaluations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督的LLM评估
- en: 原文：[https://towardsdatascience.com/open-ended-evaluations-with-llms-385beded97a4?source=collection_archive---------2-----------------------#2024-11-02](https://towardsdatascience.com/open-ended-evaluations-with-llms-385beded97a4?source=collection_archive---------2-----------------------#2024-11-02)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/open-ended-evaluations-with-llms-385beded97a4?source=collection_archive---------2-----------------------#2024-11-02](https://towardsdatascience.com/open-ended-evaluations-with-llms-385beded97a4?source=collection_archive---------2-----------------------#2024-11-02)
- en: '**Practitioners guide to judging outputs of large language models**'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**大型语言模型输出评估实践指南**'
- en: '[](https://medium.com/@volkot?source=post_page---byline--385beded97a4--------------------------------)[![Daniel
    Kharitonov](../Images/7d81129c1f88e4a0700462a342137227.png)](https://medium.com/@volkot?source=post_page---byline--385beded97a4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--385beded97a4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--385beded97a4--------------------------------)
    [Daniel Kharitonov](https://medium.com/@volkot?source=post_page---byline--385beded97a4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@volkot?source=post_page---byline--385beded97a4--------------------------------)[![Daniel
    Kharitonov](../Images/7d81129c1f88e4a0700462a342137227.png)](https://medium.com/@volkot?source=post_page---byline--385beded97a4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--385beded97a4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--385beded97a4--------------------------------)
    [Daniel Kharitonov](https://medium.com/@volkot?source=post_page---byline--385beded97a4--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--385beded97a4--------------------------------)
    ·12 min read·Nov 2, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--385beded97a4--------------------------------)
    ·12分钟阅读·2024年11月2日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: <TLDR>
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: <TLDR>
- en: Evaluating AI-generated outputs is critical for building robust applications
    of large language models because it allows complex AI applications to be split
    into simple stages with built-in error control.
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 评估AI生成的输出对于构建强大的大型语言模型应用至关重要，因为它可以将复杂的AI应用拆分为带有内建错误控制的简单阶段。
- en: ''
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is relatively straightforward to evaluate generative outputs in a supervised
    mode, where the “right answers” can be computed or hinted by human evaluators.
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在监督模式下评估生成输出相对简单，其中“正确答案”可以通过计算或由人工评估者提示。
- en: ''
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: At the same time, in many practical LLM applications the supervised approach
    is too restrictive, and there is a need for evaluations capable of tackling open-ended
    questions. The simplest way to build an unsupervised evaluator is to ask an LLM
    to evaluate itself. However, the ability of generative models to detect errors
    in their own output is not well understood.
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 与此同时，在许多实际的LLM应用中，监督方法过于限制性，亟需能够应对开放式问题的评估方法。构建无监督评估器的最简单方法是让LLM自我评估。然而，生成模型在检测自己输出中的错误的能力尚未得到充分理解。
- en: ''
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**We demonstrate that the quality of self-evaluations can be improved with
    iterative self-reflection**. Similar to the “Chain of Thought” technique, this
    method trades compute at inference for the robustness of the final result.'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**我们证明了自我评估的质量可以通过迭代自我反思来提高**。类似于“思维链”技术，这种方法在推理时以计算量换取最终结果的稳健性。'
- en: </TLDR>
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: </TLDR>
- en: 'Link to Google Colab notebook with examples:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 示例的Google Colab笔记本链接：
- en: '[https://colab.research.google.com/drive/1q_dChQBMbnUXZ377JVwYsjvn7lZ_7qlZ?usp=sharing](https://colab.research.google.com/drive/1q_dChQBMbnUXZ377JVwYsjvn7lZ_7qlZ?usp=sharing)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://colab.research.google.com/drive/1q_dChQBMbnUXZ377JVwYsjvn7lZ_7qlZ?usp=sharing](https://colab.research.google.com/drive/1q_dChQBMbnUXZ377JVwYsjvn7lZ_7qlZ?usp=sharing)'
- en: '![](../Images/931a0e4ac56180b865ec6e8ed07dbc43.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/931a0e4ac56180b865ec6e8ed07dbc43.png)'
- en: 'Image source: Flux 1\. Pro model prompted for “robot evaluating other robots”'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：Flux 1\. 专业版提示“机器人评估其他机器人”
- en: Introduction
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: When building processing pipelines using large language models, the often-mentioned
    issue is the quality of generated outputs. If a good evaluation process is in
    place, it can highlight cases of poor performance and trigger LLM fine-tuning,
    prompt adjustments, escalation to human agents — or all these actions at once.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建使用大型语言模型的处理管道时，常被提到的问题是生成输出的质量。如果有良好的评估过程，它可以突出表现不佳的情况，并触发LLM微调、提示调整、人工代理的介入——或者同时进行这些操作。
- en: 'Here is a typical workflow that uses evaluations for training: an LLM goes
    over the input dataset, and any output discrepancies detected by the evaluator
    are used to generate synthetic data to fine-tune the model. The application is
    deployed only when the target quality metrics are met.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个典型的使用评估进行训练的工作流程：LLM遍历输入数据集，评估器检测到的任何输出差异都用来生成合成数据以微调模型。只有当目标质量指标达到时，应用程序才会部署。
- en: '![](../Images/a18dcaad57946ae5cada9bd8d2dea6c7.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a18dcaad57946ae5cada9bd8d2dea6c7.png)'
- en: 'Image by the author: Evaluation loop for LLM fine-tuning'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像：LLM微调的评估循环
- en: Using LLM evaluators in production is very similar — except that detected discrepancies
    are usually sent to a human agent to ensure the workflow can continue despite
    raising an error flag.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中使用LLM评估器非常类似——只是检测到的差异通常会被发送给人工代理，以确保即使触发了错误标志，工作流仍然可以继续进行。
- en: 'However, building a good LLM evaluator is not trivial. The complexity of this
    problem stems from two practical restrictions:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，构建一个好的LLM评估器并非易事。这个问题的复杂性来源于两个实际的限制：
- en: '**First**, it is highly desirable to minimize human involvement in evaluations.
    For example, imagine a chatbot interacting with a user and missing a common colloquial
    pattern of ellipsis (using one word instead of the full output sentence):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**首先**，在评估中尽量减少人工参与是非常理想的。例如，想象一个聊天机器人与用户互动，并错过了一个常见的省略语模式（用一个词代替完整的输出句子）：'
- en: '*Bot:* Is that correct?'
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*机器人：* 这是正确的吗？'
- en: ''
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*User:* correct'
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*用户：* 正确'
- en: ''
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Bot:* Sorry, I didn’t get that. Please try again.'
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*机器人：* 对不起，我没有明白。请再试一次。'
- en: ''
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*User:* yes it is correct'
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*用户：* 是的，这是正确的'
- en: Given this dialog section, a human should easily highlight deficiencies in the
    chatbot’s response and suggest a fine-tuning course. However, in order to find
    this problem, an evaluator would have to read the entire dialog (which can be
    very long). This approach does not work at scale–which means we should strive
    for **evaluation without humans**.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这个对话部分，人工应该能够轻松地指出聊天机器人回答中的不足，并建议进行微调。然而，为了发现这个问题，评估者必须阅读整个对话（可能非常长）。这种方法在大规模应用中行不通——这意味着我们应该努力实现**无人工评估**。
- en: '**Second**, the process of judging the LLM output without knowing the “ground
    truth” is comparable in complexity to the original task. This means a state-of-the-art
    LLM can (at most) employ an evaluator with similar capabilities (most likely itself),
    thus raising questions about the **validity** of such evaluation.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**其次**，在不知道“真实答案”的情况下判断LLM输出的过程，其复杂度与原始任务相当。这意味着，最先进的LLM最多只能使用一个具有类似能力的评估器（很可能是它自己），从而引发关于**评估有效性**的问题。'
- en: Supervised evaluations
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督评估
- en: If we look at the well-studied to evaluate LLMs today, we will notice they mostly
    center on supervised or semi-supervised use cases.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们看一下今天广泛研究的LLM评估方法，我们会发现它们大多数集中在监督或半监督的应用场景中。
- en: If the training dataset comes with “ground truth” answers, evaluation becomes
    trivial — and can even drive optimization frameworks like [DSPy](https://github.com/stanfordnlp/dspy).
    The same is true when testing an enterprise LLM app against historical cases handled
    by human agents, where the “ground truth” equates to the judgments of those agents.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练数据集附带“真实答案”，那么评估变得非常简单——甚至可以驱动像 [DSPy](https://github.com/stanfordnlp/dspy)
    这样的优化框架。当测试企业LLM应用程序时，如果与历史案例进行对比（这些案例由人工代理处理），其中“真实答案”相当于那些代理的判断。
- en: Another opportunity to check the output against the “ground truth” comes when
    the LLM output can be formally verified on its own — such as computer code that
    can be compiled and tested. Despite the fact that a computer program can be written
    in many different ways, the correct code should pass the tests regardless of the
    chosen implementation path.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个检验输出与“真实答案”对比的机会出现在LLM输出可以被正式验证时——例如可以编译和测试的计算机代码。尽管计算机程序可以用多种不同的方式编写，但正确的代码应通过测试，无论选择哪种实现路径。
- en: Cases where the generative output cannot be formally verified usually require
    adding a human into the loop. For example, [RLHF](https://proceedings.neurips.cc/paper_files/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html)
    can be used to rate LLM outputs according to ordinal human preferences and thus
    steer the network toward complicated and nuanced policies.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 生成输出无法正式验证的情况通常需要将人工引入环路。例如，[RLHF](https://proceedings.neurips.cc/paper_files/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html)
    可以根据人类偏好的序列对LLM输出进行评分，从而引导网络走向复杂且微妙的策略。
- en: Unsupervised self-evaluations
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督自我评估
- en: Meanwhile, there are many open-ended evaluation cases where “ground truth” approach
    cannot be implemented, and RLHF is too lengthy or too costly. This explains the
    interest in unsupervised self-evaluation techniques.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，有许多开放性评估案例，无法实现“地面真实”方法，而 RLHF 太长或太昂贵。这解释了人们对无监督自我评估技术的兴趣。
- en: So, assuming we have an open-ended LLM evaluation question that would normally
    require human involvement — like “*how can this chatbot improve*” — what can be
    done to automate?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个开放性的大型语言模型评估问题，通常需要人工参与——比如“*这个聊天机器人如何改进*”——我们能做些什么来实现自动化呢？
- en: An economical evaluation harness can be built if we assume that contemporary
    large language models with rich semantic representations are inherently capable
    of self-evaluations. This means you can simply ask the model to evaluate its own
    output, or use another LLM for the same task to avoid cross-contamination in their
    training sets.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们假设当代的大型语言模型具备丰富的语义表示并且本身具有自我评估能力，那么就可以建立一个经济的评估框架。这意味着你可以直接要求模型评估其输出，或者使用另一个大型语言模型执行相同的任务，以避免它们的训练集之间的交叉污染。
- en: '**Unfortunately, a naïve implementation of LLM self-judging may fail.** There
    are two possible reasons for this.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**不幸的是，LLM 自我判断的简单实现可能会失败。** 其中有两个可能的原因。'
- en: '**The first reason** is the most obvious: an **LLM is not guaranteed to produce
    coherent outputs on subjects not reflected in its training set** or unreachable
    for retrieval tools (like web search). For example, consider this Google map snippet,
    which depicts an unincorporated town called Mineral in California and a small
    body of water (*Martin Creek*) that runs through it.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**第一个原因** 是最显而易见的：**大型语言模型不能保证在其训练集中没有反映的主题上生成连贯的输出**，或者在无法通过检索工具（如网页搜索）获取的信息上生成连贯的输出。例如，考虑这个
    Google 地图片段，它展示了加利福尼亚州一个名为 Mineral 的未合并小镇和一条小溪 (*Martin Creek*)，这条小溪流经该小镇。'
- en: '![](../Images/5bdabfab93fe992dbe76ecf0dbb730e8.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5bdabfab93fe992dbe76ecf0dbb730e8.png)'
- en: 'Mineral, CA vicinity (source: Google maps)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Mineral, CA 附近（来源：Google 地图）
- en: 'The town of Mineral does not have its own Wikipedia page, and most LLMs do
    not use GIS data in training, so when asked which creek passes through the town,
    an LLM can easily hallucinate:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Mineral 小镇没有自己的维基百科页面，而且大多数大型语言模型在训练中不使用 GIS 数据，所以当被问到哪条小溪流经该小镇时，LLM 很容易产生幻觉：
- en: '![](../Images/399f7e82f5db8d091fac0196657dd68b.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/399f7e82f5db8d091fac0196657dd68b.png)'
- en: GPT-4o hallucination example
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4o 幻觉示例
- en: '*Mill Creek* is a real water body that skirts the (much larger) census-designated
    place of which Mineral is a part. However, *Mill Creek* does not pass through
    the town of Mineral proper, so GPT-4o’s answer is incorrect.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*Mill Creek* 是一条真实的水体，绕过了 Mineral 所在的（更大的）普查指定地点。然而，*Mill Creek* 并没有穿过 Mineral
    小镇本身，因此 GPT-4o 的回答是错误的。'
- en: Intuitively, it feels like a failure in plain LLM inference due to the subject
    being outside of the knowledge set and unreachable via common tools (like web
    search) should not be easily self-detectable.
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 直观地感觉，普通的 LLM 推理失败是因为主题超出了其知识集，并且无法通过常见工具（如网页搜索）获取，且这种情况不容易自我检测。
- en: 'The **second** reason for failing self-evals is somewhat more subtle and is
    driven by **probabilistic nature of LLM outputs**. It is easy to demonstrate in
    cases where evaluation should include multiple observations. For example, consider
    this dialog between the user and chatbot:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**第二个** 使自我评估失败的原因稍微微妙一些，且由**大型语言模型输出的概率特性**驱动。它在评估需要多次观察的情况下很容易展示。例如，考虑以下用户与聊天机器人的对话：'
- en: '*Bot:* I am a chatbot to help you find the best mobile phone plan.'
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*Bot:* 我是一个聊天机器人，帮助你找到最适合的手机套餐。'
- en: ''
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*User:* I need at least 500Gb of data and priced below $50'
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*User:* 我需要至少 500GB 的数据，且价格低于 $50'
- en: ''
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Bot:* The “Ultimate Freedom” plan delivers uncapped Internet access for $79.99'
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*Bot:* “Ultimate Freedom” 套餐提供无限制的互联网访问，价格为 $79.99'
- en: ''
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*User:* This might actually work.'
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*User:* 这或许真的有效。'
- en: ''
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Bot:* Do you want to change the current plan?'
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*Bot:* 你想更改当前的套餐吗？'
- en: ''
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*User:* Is “Ultimate Freedom” domestic only ? Can I have international roaming?'
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*User:* “Ultimate Freedom” 仅限国内使用吗？我能开通国际漫游吗？'
- en: ''
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Bot:* Sorry, I didn’t get that. Please try again.'
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*Bot:* 对不起，我没听清楚。请再试一次。'
- en: ''
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*User:* I am done for now. Please do not make any changes.'
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*User:* 我现在已经好了，请不要做任何更改。'
- en: ''
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Bot:* I am a chatbot to help you find the best mobile phone plan.'
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*Bot:* 我是一个聊天机器人，帮助你找到最适合的手机套餐。'
- en: 'If we repeatedly ask an LLM to list the improvements that can be made to the
    bot based on this dialog, we will notice the output list is not stable between
    runs. One version may look like this:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们反复要求LLM列出基于这段对话可以对机器人进行的改进，我们会注意到输出列表在不同运行之间并不稳定。一个版本可能是这样的：
- en: '[PRE0]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'While another may look like this:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种可能是这样的：
- en: '[PRE1]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Even though there is significant overlap between these two answers, it is clear
    that an exhaustive list of suggestions (similar to what a human expert would produce)
    was not generated in either case.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这两个答案有很大重叠，但显然在任何一种情况下都没有生成类似于人类专家可能给出的那种详尽的建议列表。
- en: The unexpected power of self-reflection
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自我反思的意外力量
- en: Once we outline the typical failure modes for evaluations, it may seem like
    using an LLM to judge itself is a bad idea. After all, this sounds like asking
    a diligent student to re-check their own answers. Since a good student does not
    make many typos, re-checking merely reflects existing knowledge and should not
    result in improvements.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们概述了评估的典型失败模式，使用LLM来评判自己似乎是个坏主意。毕竟，这听起来像是在要求一个勤奋的学生重新检查自己的答案。因为一个好学生不会犯很多拼写错误，重新检查仅仅反映现有的知识，不应导致改进。
- en: '*However, this is where our intuition about LLMs may go awfully wrong.*'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*然而，这正是我们对LLM的直觉可能完全错误的地方。*'
- en: '**In fact, most LLMs are capable of corrective self-evaluation, even when the
    subject lies outside of their knowledge base**.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**事实上，大多数LLM都能进行纠正性自我评估，即使主题超出了它们的知识库**。'
- en: 'To illustrate this phenomenon, let us return to GPT-4o example hallucinating
    about the body of water crossing the town of Mineral*,* CA. Interestingly enough,
    this particular hallucination can be dispelled during self-evaluation:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一现象，我们回到GPT-4o的例子，其中幻觉涉及穿过矿物镇的水体*，*加州。有趣的是，这种特定的幻觉可以在自我评估过程中被消除：
- en: '![](../Images/41f9009c7df5bb633369a78802a7b35d.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41f9009c7df5bb633369a78802a7b35d.png)'
- en: Self-evaluation in GPT-4o is capable of reversing hallucinations
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4o中的自我评估能够逆转幻觉
- en: '**So where is the magic?**'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**那么，魔力在哪里呢？**'
- en: In this example, the LLM does not have the knowledge or the tools to get the
    correct answer, so it hallucinates the “most plausible” completion. However, when
    asked to evaluate itself, it arrives at the conclusion that the facts it can access
    do not corroborate previous statement. Even though GPT-4o does not know the correct
    answer, it can dismiss the incorrect one.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，LLM没有知识或工具来得到正确的答案，所以它幻觉出“最有可能”的完成。然而，当被要求自我评估时，它得出结论：它可以访问的事实与之前的陈述并不一致。即使GPT-4o不知道正确答案，它也能否定错误的答案。
- en: 'A more sophisticated model (like GPT-4o1) may be slightly harder to treat in
    the same way because it tends to produce more nuanced responses:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更复杂的模型（比如GPT-4o1）可能稍微难以以相同的方式处理，因为它倾向于生成更细致的回应：
- en: '![](../Images/021034203126a41c3695f70bef6a4b23.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/021034203126a41c3695f70bef6a4b23.png)'
- en: Hallucination in GPT-4o1 is more nuanced.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4o1中的幻觉更加细致。
- en: Instead of hallucinating a completion on the subject it cannot verify, GPT-4o1
    may choose to answer the question it was never asked — like “*Which primary body
    of water runs near Mineral, CA?”*. This evasion means that a direct self-evaluation
    prompt along the lines of “*evaluate as True or False*” may fail.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 与其在自己无法验证的主题上产生幻觉，GPT-4o1可能会选择回答一个它从未被问到的问题——比如“*矿物镇附近流经的主要水体是哪一条？*”。这种回避意味着，像“*判断对错*”这样的直接自我评估提示可能会失败。
- en: 'However, a more deliberative way of asking for self-evaluation can still be
    successful, even if it takes multiple iterations:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，以一种更加深思熟虑的方式要求自我评估，仍然可能成功，即使这需要多次迭代：
- en: '![](../Images/62ffba261147d669cf855fc565785bc2.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62ffba261147d669cf855fc565785bc2.png)'
- en: This ability of LLMs to self-reflect in an iterative way is, of course, well-known
    and is somewhat taken for granted in applications like code generation. Here we
    are just extending the same technique to self-evaluation.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs以迭代方式自我反思的能力，当然是众所周知的，并且在代码生成等应用中已经有所应用，我们这里只是将相同的技巧扩展到自我评估。
- en: The “expected” power of memoization
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 记忆化的“预期”能力
- en: The same idea of iterative reflection is also applicable to LLM tasks that tend
    to produce incomplete outputs. If we revisit the bot dialog example and allow
    an LLM to iterate on a memoized list of improvements, we will observe the model
    is rarely “satisfied” with the result at first shot.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代反思的相同思路也适用于那些倾向于生成不完整输出的LLM任务。如果我们回顾机器人对话示例，并允许LLM在记忆化改进列表上进行迭代，我们将发现模型通常不会对第一次的结果“满意”。
- en: 'In other words, if we formulate a prompt like this:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，如果我们制定一个这样的提示：
- en: '[PRE2]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then it would typically take 2–4 passes over the list of improvements until
    the LLM converges on recommendations and declares the evaluation task to be successful:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然后通常需要对改进列表进行2到4轮的检查，直到LLM得出推荐并宣布评估任务成功：
- en: '[PRE3]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: After this initial “warm-up” over one dialog, we can feed the model with more
    sample dialogs and see what happens.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一初步的“一轮热身”对话后，我们可以给模型提供更多的示例对话，看看会发生什么。
- en: 'In a manner similar to what a human evaluator would do, the GPT-4o model considers
    that many dialog samples are not worth producing new recommendations (just one
    model run is enough)–yet some may trigger much longer deliberation:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 与人类评估者的做法类似，GPT-4o模型认为许多对话样本不值得生成新的推荐（一次模型运行就足够了）——然而，有些可能会引发更长时间的深思：
- en: '![](../Images/9632e0145ef5a5bbe01f4aae0ff708e3.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9632e0145ef5a5bbe01f4aae0ff708e3.png)'
- en: Number of LLM invocations per first 50 dialogs from [ExpBot dataset](https://radar.kit.edu/radar/en/dataset/FdJmclKpjHzLfExE.ExpBot%2B-%2BA%2Bdataset%2Bof%2B79%2Bdialogs%2Bwith%2Ban%2Bexperimental%2Bcustomer%2Bservice%2Bchatbot)
    until convergence (chart by the author)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[ExpBot数据集](https://radar.kit.edu/radar/en/dataset/FdJmclKpjHzLfExE.ExpBot%2B-%2BA%2Bdataset%2Bof%2B79%2Bdialogs%2Bwith%2Ban%2Bexperimental%2Bcustomer%2Bservice%2Bchatbot)的前50个对话中的LLM调用次数，直到收敛（图表由作者提供）
- en: 'The final result will be a fairly exhaustive list of recommendations on improving
    the chatbot:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的结果将是一个相当详尽的关于改进聊天机器人的推荐列表：
- en: '[PRE4]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Some technical notes on this example:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 关于此示例的一些技术说明：
- en: For simplicity, we have combined the evaluation and generation into one prompt
    which relies on [OpenAI’s structured outputs](https://platform.openai.com/docs/guides/structured-outputs)
    to produce the desired result.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了简化，我们将评估和生成合并为一个提示，依赖于[OpenAI的结构化输出](https://platform.openai.com/docs/guides/structured-outputs)来生成期望的结果。
- en: The inherent limitation of memoization is a requirement to process samples sequentially.
    This can take some time on a long dataset, and also blocks us from using cheap
    inference via [call batching](https://cookbook.openai.com/examples/batch_processing).
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记忆化的固有限制在于需要按顺序处理样本。这在处理大型数据集时可能需要一些时间，并且还阻止我们通过[批处理调用](https://cookbook.openai.com/examples/batch_processing)使用低成本推理。
- en: To further improve the performance, we can take advantage of the fact that most
    samples in a dataset do not generate new insights. This means we can produce the
    initial list of recommendations by iterating over a small subset of samples sequentially,
    and serve the rest of the dataset in parallel via [DataChain library](https://github.com/iterative/datachain)
    (or in a batch with [OpenAI API](https://cookbook.openai.com/examples/batch_processing))
    to flag the “interesting” cases and shave 30–50% off the time (or expense) budgets
    based on your preferences.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步提高性能，我们可以利用这样一个事实：数据集中的大多数样本并不会生成新的见解。这意味着我们可以通过按顺序迭代一个小子集的样本来生成初步的推荐列表，并通过[DataChain库](https://github.com/iterative/datachain)（或通过[OpenAI
    API](https://cookbook.openai.com/examples/batch_processing)批量处理）并行服务剩余的数据集，以标记“有趣”的案例，从而节省30%到50%的时间（或费用），具体取决于您的偏好。
- en: Conclusion
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: LLMs can and should be used for unsupervised evaluations (including self-evaluations).
    The fine-print is that it requires a well-thought approach–which often resolves
    to an iterative way to improve and refine the judgements.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: LLM可以且应该用于无监督评估（包括自我评估）。关键是它需要一个经过深思熟虑的方法——这通常会发展成一种迭代的方式来改进和完善判断。
- en: 'Here is a link to the sample implementation in Google Colab:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Google Colab中示例实现的链接：
- en: '[https://colab.research.google.com/drive/1q_dChQBMbnUXZ377JVwYsjvn7lZ_7qlZ?usp=sharing](https://colab.research.google.com/drive/1q_dChQBMbnUXZ377JVwYsjvn7lZ_7qlZ?usp=sharing)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://colab.research.google.com/drive/1q_dChQBMbnUXZ377JVwYsjvn7lZ_7qlZ?usp=sharing](https://colab.research.google.com/drive/1q_dChQBMbnUXZ377JVwYsjvn7lZ_7qlZ?usp=sharing)'
