- en: 'Long-form video representation learning (Part 2: Video as sparse transformers)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长篇视频表示学习（第二部分：视频作为稀疏Transformer）
- en: 原文：[https://towardsdatascience.com/long-form-video-representation-learning-part-2-video-as-sparse-transformers-29fbd0ed9e71?source=collection_archive---------9-----------------------#2024-05-14](https://towardsdatascience.com/long-form-video-representation-learning-part-2-video-as-sparse-transformers-29fbd0ed9e71?source=collection_archive---------9-----------------------#2024-05-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/long-form-video-representation-learning-part-2-video-as-sparse-transformers-29fbd0ed9e71?source=collection_archive---------9-----------------------#2024-05-14](https://towardsdatascience.com/long-form-video-representation-learning-part-2-video-as-sparse-transformers-29fbd0ed9e71?source=collection_archive---------9-----------------------#2024-05-14)
- en: We explore novel video representations methods that are equipped with long-form
    reasoning capability. This is part II focusing on sparse video-text transformers.
    See [Part I](https://medium.com/@subarna.tripathi/long-form-video-representation-learning-part-1-video-as-graphs-c55b609d9100)
    on video as graphs. And [Part III](https://medium.com/@subarna.tripathi/long-form-video-representation-learning-part-3-latest-and-greatest-in-long-form-video-1b6dee0f5f6e)
    provides a sneak peek into our latest and greatest explorations.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们探索了具备长篇推理能力的新型视频表示方法。这是第二部分，重点介绍稀疏视频-文本Transformer。请参见关于视频作为图的[第一部分](https://medium.com/@subarna.tripathi/long-form-video-representation-learning-part-1-video-as-graphs-c55b609d9100)。而[第三部分](https://medium.com/@subarna.tripathi/long-form-video-representation-learning-part-3-latest-and-greatest-in-long-form-video-1b6dee0f5f6e)提供了我们最新研究成果的预览。
- en: '[](https://medium.com/@subarna.tripathi?source=post_page---byline--29fbd0ed9e71--------------------------------)[![Subarna
    Tripathi](../Images/0a949764464eeef40a6d3ae0d183873f.png)](https://medium.com/@subarna.tripathi?source=post_page---byline--29fbd0ed9e71--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--29fbd0ed9e71--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--29fbd0ed9e71--------------------------------)
    [Subarna Tripathi](https://medium.com/@subarna.tripathi?source=post_page---byline--29fbd0ed9e71--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@subarna.tripathi?source=post_page---byline--29fbd0ed9e71--------------------------------)[![Subarna
    Tripathi](../Images/0a949764464eeef40a6d3ae0d183873f.png)](https://medium.com/@subarna.tripathi?source=post_page---byline--29fbd0ed9e71--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--29fbd0ed9e71--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--29fbd0ed9e71--------------------------------)
    [Subarna Tripathi](https://medium.com/@subarna.tripathi?source=post_page---byline--29fbd0ed9e71--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--29fbd0ed9e71--------------------------------)
    ·6 min read·May 14, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--29fbd0ed9e71--------------------------------)
    ·6分钟阅读·2024年5月14日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: The [first blog](https://medium.com/@subarna.tripathi/long-form-video-representation-learning-part-1-video-as-graphs-c55b609d9100)
    in this series was about learning explicit sparse graph-based video representation
    methods for “long-form” video representation learning. They are effective methods;
    however, they were not end-to-end trainable. We needed to rely on other CNN or
    transformer-based feature extractors to generate the initial node embeddings.
    In this blog, our focus is to devising an end-to-end methods using transformers,
    but with the same goal of “long-form” reasoning.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本系列的[第一篇博客](https://medium.com/@subarna.tripathi/long-form-video-representation-learning-part-1-video-as-graphs-c55b609d9100)介绍了用于“长篇”视频表示学习的显式稀疏图表示方法。这些方法是有效的；然而，它们不能进行端到端的训练。我们需要依赖其他基于CNN或Transformer的特征提取器来生成初始的节点嵌入。在本篇博客中，我们的重点是使用Transformer设计端到端的方法，但目标依然是实现“长篇”推理。
- en: Sparse Video-Text Transformers
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 稀疏视频-文本Transformer
- en: As an end-to-end learnable architecture, we started exploring transformers.
    The first question we needed an answer for is that do video-text transformers
    learn to model temporal relationships across frames? We oberved that despite their
    immense capacity and the abundance of multimodal training data, recent video models
    show strong tendency towards frame-based spatial representations, while temporal
    reasoning remains largely unsolved. For example, if we shuffle the order of video
    frames in the input to the video models, the output do not change much!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种端到端可学习的架构，我们开始探索 transformers。我们首先需要解答的问题是：视频-文本 transformer 是否能学习跨帧建模时序关系？我们观察到，尽管视频模型具有巨大的容量和丰富的多模态训练数据，最近的视频模型仍然表现出强烈的基于帧的空间表示倾向，而时序推理依然未得到有效解决。例如，如果我们打乱输入视频模型的帧顺序，输出几乎没有变化！
- en: '![](../Images/d875867bb70309e5fa822c79d2b86874.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d875867bb70309e5fa822c79d2b86874.png)'
- en: Image by author
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Upon a closer investigation, we identify a few key challenges to incorporating
    multi-frame reasoning in video-language models. First, limited model size implies
    a trade-off between spatial and temporal learning (a classic example being 2D/3D
    convolutions in video CNNs). For any given dataset, optimal performance requires
    a careful balance between the two. Second, long-term video models typically have
    larger model sizes and are more prone to overfitting. Hence, for long-form video
    models, it becomes more important to carefully allocate parameters and control
    model growth. Finally, even if extending the clip length improves the results,
    it is subject to diminishing returns since the amount of information provided
    by a video clip does not grow linearly with its sampling rate. If the model size
    is not controlled, the compute increase may not justify the gains in accuracy.
    This is critical for transformer-based architectures, since self-attention mechanisms
    have a quadratic memory and time cost with respect to input length.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 经过进一步调查，我们发现将多帧推理融入视频语言模型面临几个关键挑战。首先，有限的模型规模意味着空间和时间学习之间需要做出权衡（经典例子是视频 CNN 中的
    2D/3D 卷积）。对于任何给定的数据集，最佳性能需要在二者之间找到微妙的平衡。其次，长时段视频模型通常具有更大的模型规模，并且更容易发生过拟合。因此，对于长视频模型，仔细分配参数和控制模型增长变得尤为重要。最后，即便扩展片段长度能改善结果，它也会面临收益递减的问题，因为视频片段所提供的信息量并不会随着采样率的增加而线性增长。如果不控制模型规模，计算量的增加可能无法抵消准确率提升带来的收益。这对基于
    transformer 的架构尤为重要，因为自注意力机制的内存和时间成本是输入长度的二次方。
- en: In summary, model complexity should be adjusted adaptively, depending on the
    input videos, to achieve the best trade-off between spatial representation, temporal
    representation, overfitting potential, and complexity. Since existing video-text
    models lack this ability, they either attain a suboptimal balance between spatial
    and temporal modeling, or do not learn meaningful temporal representations at
    all.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，模型复杂度应该根据输入视频自适应调整，以实现空间表示、时间表示、过拟合潜力和复杂性之间的最佳平衡。由于现有的视频-文本模型缺乏这种能力，它们要么在空间和时间建模之间达成次优平衡，要么根本没有学习到有意义的时间表示。
- en: 'What can be made “sparse” in video transformers ? Nodes and Edges:'
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视频 transformers 中可以“稀疏化”的是什么？节点和边：
- en: We argue that video-text models should learn to allocate modeling resources
    to the video data. Rather than uniformly extending the model to longer clips,
    the allocation of these resources to the relevant spatio-temporal locations of
    the video is crucial for efficient learning from long clips. For transformer models,
    this allocation is naturally performed by pruning redundant attention connections.
    We then accomplish these goals by exploring transformer sparsification techniques.
    This motivates the introduction of a *Sparse Video-Text Transformer* SViTT inspired
    by graph models. As illustrated in Figure 1, SViTT treats video tokens as graph
    vertices, and self-attention patterns as edges that connect them.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为，视频-文本模型应学习将建模资源分配给视频数据。与其均匀地扩展模型至更长的片段，不如将这些资源分配到视频的相关时空位置，这对于从长片段中高效学习至关重要。对于
    transformer 模型，这种资源分配通过修剪冗余的注意力连接自然而然地实现。我们通过探索 transformer 稀疏化技术来实现这些目标。这促使我们提出了受图模型启发的*稀疏视频-文本
    Transformer*（SViTT）。如图 1 所示，SViTT 将视频标记视为图的顶点，自注意力模式视为连接它们的边。
- en: 'We design SViTT to pursue sparsity for both: **Node** sparsity reduces to identifying
    informative tokens (e.g., corresponding to moving objects or person in the foreground)
    and pruning background feature embeddings; **edge** sparsity aims at reducing
    query-key pairs in attention module while maintaining its global reasoning capability.
    To address the diminishing returns for longer input clips, we propose to train
    SViTT with temporal sparse expansion, a curriculum learning strategy that increases
    clip length and model sparsity, in sync, at each training stage.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设计了 SViTT，旨在实现节点和边缘的稀疏性：**节点**稀疏性通过识别信息丰富的标记（例如，表示前景中移动物体或人的标记）并剪枝背景特征嵌入来实现；**边缘**稀疏性则旨在减少注意力模块中的查询-键对，同时保持其全局推理能力。为了解决长输入片段的回报递减问题，我们提出通过时间稀疏扩展进行训练，采用课程学习策略，在每个训练阶段同步增加片段长度和模型稀疏性。
- en: '![](../Images/40b9ce2c4091fb450aca4bc948cb1b3b.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40b9ce2c4091fb450aca4bc948cb1b3b.png)'
- en: '(image by author) Figure 2: (Image by author) we show the following qualitative'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: （作者提供的图片）图2：（作者提供的图片）我们展示以下定性结果
- en: 'results: (1) Left: A training sample includes a description (sentence at the
    top) and a video clip (the sequence of frames of a video), (2) Middle: video encoder’s
    layer 10 after visual token pruning; (3) Right: Multimodal encoder’s output after
    token pruning.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：（1）左侧：一个训练样本包括描述（顶部的句子）和视频片段（视频的帧序列），（2）中间：视觉标记剪枝后的视频编码器的第10层；（3）右侧：多模态编码器在标记剪枝后的输出。
- en: Applications, Evaluation and Results
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用、评估与结果
- en: SViTT is evaluated on diverse video-text benchmarks from video retrieval to
    question answering, comparing to prior art and our own dense modeling baselines.
    First, we perform a series of ablation studies to understand the benefit of sparse
    modeling in transformers. Interestingly, we find that both nodes (tokens) and
    edges (attention) can be pruned drastically at inference, with a small impact
    on test performance. In fact, token selection using cross-modal attention improves
    retrieval results by 1% without re-training. Figure 2 shows that SViTT isolates
    informative regions from background patches to facilitate efficient temporal reasoning.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: SViTT 在多种视频-文本基准测试中进行了评估，从视频检索到问答任务，比较了先前的研究成果和我们自己的稠密模型基准。首先，我们进行了一系列消融研究，以了解稀疏建模在变换器中的优势。有趣的是，我们发现，在推理时，节点（标记）和边缘（注意力）可以大幅度剪枝，而对测试性能的影响很小。实际上，使用跨模态注意力进行的标记选择，在不重新训练的情况下，能使检索结果提高1%。图2显示，SViTT
    将信息丰富的区域从背景区域中分离出来，以促进高效的时间推理。
- en: We next perform full pre-training with the sparse models and evaluate their
    downstream performance. We observe that SViTT scales well to longer input clips,
    where the accuracy of dense transformers drop due to optimization difficulties.
    On all video-text benchmarks, SViTT reports comparable or better performance than
    their dense counterparts with lower computational cost, outperforming prior arts
    including those trained with additional image-text corpora.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用稀疏模型进行完整的预训练，并评估其下游性能。我们观察到，SViTT 在处理较长输入片段时表现良好，而稠密变换器在此时由于优化困难，准确率下降。在所有视频-文本基准测试中，SViTT
    相较于稠密模型以更低的计算成本报告了可比或更好的性能，超过了包括那些使用额外图像-文本语料库训练的先前方法。
- en: '![](../Images/932fd32be2c146ab160a9bb2ffee81aa.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/932fd32be2c146ab160a9bb2ffee81aa.png)'
- en: Image by author
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: We can see from the above tables, with sparsification, immediate temporal context
    aggregation could be made 2X longer (table 2). Also see how sparsification maintains
    the final task accuracies (table 1), rather improves them.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的表格中，我们可以看到，通过稀疏化，立即的时间上下文聚合可以延长2倍（表2）。还可以看到稀疏化如何维持最终任务的准确性（表1），甚至有所提升。
- en: '![](../Images/069757dbaa4226fcc077a77d3fc1aff5.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/069757dbaa4226fcc077a77d3fc1aff5.png)'
- en: image by author
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: In the above table, we show how our proposed training paradigm helps improve
    task performance with respect to the different levels of sparsity. In table 4,
    you can see the zero-shot performance on text-to-video retrieval task on two standard
    benchmarks.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述表格中，我们展示了我们提出的训练范式如何通过不同稀疏级别来帮助提高任务性能。在表4中，您可以看到在两个标准基准上进行的文本到视频检索任务的零-shot性能。
- en: '![](../Images/89554324f077657351b4c0f9fdc161d1.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89554324f077657351b4c0f9fdc161d1.png)'
- en: image by author
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Finally, we show the results on different benchmarks on multimodal retrieval
    and video question-answering. SViTT outperforms all existing methods, and even
    required less number of pre-training pairs.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们展示了在多模态检索和视频问答的不同基准上获得的结果。SViTT 超越了所有现有方法，甚至需要的预训练对数更少。
- en: 'More details on SViTT can be found [here](http://svcl.ucsd.edu/projects/svitt/)
    . To summarize, Compared to original transformers, SViTT is 6–7 times more efficient,
    capable of 2X more context aggregation. Pre-training with SViTT improves accuracy
    SoTA on 5 benchmarks : retrieval, VideoQ&A.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 关于SViTT的更多细节可以在[这里](http://svcl.ucsd.edu/projects/svitt/)找到。总结来说，与原始的transformers相比，SViTT的效率提高了6到7倍，能够实现2倍的上下文聚合。使用SViTT进行预训练，在5个基准任务（如检索、视频问答）上提高了准确性，超越了现有的最先进技术（SoTA）。
- en: 'SViTT-Ego for egocentric videos:'
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SViTT-Ego用于以自我为中心的视频：
- en: Pretraining egocentric vision-language models has become essential to improving
    downstream egocentric video-text tasks. These egocentric foundation models commonly
    use the transformer architecture. The memory footprint of these models during
    pretraining can be substantial. Therefore, we pre-train our own sparse video-text
    transformer model, SViTT-Ego, the first sparse egocentric video-text transformer
    model integrating edge and node sparsification. We pretrain on the [EgoClip](https://proceedings.neurips.cc/paper_files/paper/2022/file/31fb284a0aaaad837d2930a610cd5e50-Paper-Conference.pdf)
    dataset and incorporate the egocentric-friendly objective EgoNCE, instead of the
    frequently used InfoNCE. Most notably, SViTT-Ego, obtains a 2.8% gain on EgoMCQ
    (intra-video) accuracy compared to the current SOTA, with no additional data augmentation
    techniques other than standard image augmentations, yet pre-trainable on memory-limited
    devices. One such visual example is shown below. We are preparing to participate
    in the EgoVis workshop at CVPR with our SViTT-ego.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练以自我为中心的视觉-语言模型已成为提升下游自我中心视频-文本任务的关键。这些自我中心的基础模型通常使用transformer架构。这些模型在预训练期间的内存占用可能相当庞大。因此，我们预训练了我们自己的稀疏视频-文本transformer模型SViTT-Ego，这是第一个集成边缘和节点稀疏化的稀疏自我中心视频-文本transformer模型。我们在[EgoClip](https://proceedings.neurips.cc/paper_files/paper/2022/file/31fb284a0aaaad837d2930a610cd5e50-Paper-Conference.pdf)数据集上进行预训练，并引入了自我中心友好的目标EgoNCE，而不是常用的InfoNCE。最显著的是，SViTT-Ego在EgoMCQ（视频内）准确度上相比当前的最先进技术提高了2.8%，且没有使用除标准图像增强外的额外数据增强技术，同时能够在内存有限的设备上进行预训练。以下展示了一个视觉示例。我们正在准备参加CVPR上的EgoVis研讨会，展示我们的SViTT-Ego。
- en: '![](../Images/0445282f443cbf2c2173ededaea88342.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0445282f443cbf2c2173ededaea88342.png)'
- en: '(image by author) Figure 3: Screenshot from the Huggingface demo of EgoMCQ'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: （图像由作者提供）图3：来自Huggingface演示的EgoMCQ截图
- en: '![](../Images/1409f52d61ca21cb9911e86979d9947d.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1409f52d61ca21cb9911e86979d9947d.png)'
- en: '(image by author) Table 7: SViTT-Ego outperforms all state-of-the-art models
    on'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: （图像由作者提供）表7：SViTT-Ego在所有最先进的模型上表现优异
- en: intra-video accuracy. When considering models trained solely on
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 视频内准确度。考虑到仅在
- en: 3.8M samples without narration augmentations, SViTT-Ego out-
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 3.8M个样本没有叙述增强，SViTT-Ego在
- en: performs all models in inter-video and intra-video accuracy
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在视频间和视频内准确度上，所有模型均表现出色
- en: '![](../Images/ad2f51266a8530be3c3c5ea54c3ff0d7.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad2f51266a8530be3c3c5ea54c3ff0d7.png)'
- en: '(Image by author) Figure 4: Given qv = 0.7, we show the following qualitative'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: （图像由作者提供）图5：给定qv = 0.7，我们展示了以下定性结果
- en: 'results with the vision encoder: row 1, shows 4 frame input; row'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 使用视觉编码器的结果：第1行，显示4帧输入；第2行
- en: 2, shows video encoder’s layer 4 after visual token pruning; row 3,
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图2，显示视频编码器的第4层在视觉标记修剪后的情况；第3行，
- en: shows video encoder’s layer 7 after visual token pruning; and row
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 显示视频编码器的第7层在视觉标记修剪后的情况；以及第3行
- en: 4, shows video encoder’s layer 10 after visual token pruning. We
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图4，显示视频编码器的第10层在视觉标记修剪后的情况。我们
- en: follow SViTT to prune visual tokens
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SViTT来修剪视觉标记
- en: 'Highlights:'
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 亮点：
- en: 'We propose, **SViTT**, a video-text architecture that unifies edge and node
    sparsity; We show its temporal modeling efficacy on video-language tasks. Compared
    to original transformers, **SViTT** is 6–7 times more efficient, capable of 2X
    more context aggregation. Pre-training with SViTT improves accuracy over SoTA
    on 5 benchmarks : retrieval, VideoQ&A. Our video-text sparse transformer work
    was first published at [CVPR 2023](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_SViTT_Temporal_Learning_of_Sparse_Video-Text_Transformers_CVPR_2023_paper.pdf).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了**SViTT**，一种视频-文本架构，统一了边缘和节点稀疏性；我们展示了它在视频语言任务中的时间建模效果。与原始的transformer相比，**SViTT**的效率提高了6到7倍，能够实现2倍的上下文聚合。使用SViTT进行预训练，在5个基准任务上（如检索、视频问答）提高了准确性，超越了当前的最先进技术（SoTA）。我们的稀疏视频-文本transformer工作首次发表于[CVPR
    2023](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_SViTT_Temporal_Learning_of_Sparse_Video-Text_Transformers_CVPR_2023_paper.pdf)。
- en: Next, we show how we are leveraging such sparse transformer for egocentric video
    understanding applications. We show our **SViTT-Ego** (built atop SViTT) outperforms
    dense transformer baselines on the EgoMCQ task with significantly lower peak memory
    and compute requirements thanks to the inherent sparsity. This shows that sparse
    architectures such as **SViTT-Ego** is a potential foundation model choice, especially
    for pretraining on memory-bound devices. Watch out for exciting news in the near
    future!
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们展示了如何利用这种稀疏变换器进行自我中心视频理解应用。我们展示了**SViTT-Ego**（基于SViTT构建）在EgoMCQ任务中相较于密集变换器基准表现更好，并且由于固有的稀疏性，峰值内存和计算需求显著更低。这表明像**SViTT-Ego**这样的稀疏架构是一个潜在的基础模型选择，特别是在内存受限设备上的预训练。请关注即将发布的激动人心的消息！
