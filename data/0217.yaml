- en: Optimizing Instance Type Selection for AI Development in Cloud Spot Markets
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化云端Spot市场中的AI开发实例类型选择
- en: 原文：[https://towardsdatascience.com/optimizing-instance-type-selection-for-ai-development-in-cloud-spot-markets-a6e94804e8f3?source=collection_archive---------9-----------------------#2024-01-22](https://towardsdatascience.com/optimizing-instance-type-selection-for-ai-development-in-cloud-spot-markets-a6e94804e8f3?source=collection_archive---------9-----------------------#2024-01-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/optimizing-instance-type-selection-for-ai-development-in-cloud-spot-markets-a6e94804e8f3?source=collection_archive---------9-----------------------#2024-01-22](https://towardsdatascience.com/optimizing-instance-type-selection-for-ai-development-in-cloud-spot-markets-a6e94804e8f3?source=collection_archive---------9-----------------------#2024-01-22)
- en: Instance Selection for Deep Learning — Part 2
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习实例选择——第二部分
- en: '[](https://chaimrand.medium.com/?source=post_page---byline--a6e94804e8f3--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page---byline--a6e94804e8f3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a6e94804e8f3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a6e94804e8f3--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page---byline--a6e94804e8f3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://chaimrand.medium.com/?source=post_page---byline--a6e94804e8f3--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page---byline--a6e94804e8f3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a6e94804e8f3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a6e94804e8f3--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page---byline--a6e94804e8f3--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a6e94804e8f3--------------------------------)
    ·9 min read·Jan 22, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a6e94804e8f3--------------------------------)
    ·9分钟阅读·2024年1月22日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/c5c97dca0d381a425ea2a55b7f74c2a4.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c5c97dca0d381a425ea2a55b7f74c2a4.png)'
- en: Photo by [Mike Enerio](https://unsplash.com/@mikeenerio?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Mike Enerio](https://unsplash.com/@mikeenerio?utm_source=medium&utm_medium=referral)
    来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: This post was written in collaboration with [Tomer Berkovich](https://www.linkedin.com/in/tomerberkovich/),
    [Yitzhak Levi](https://www.linkedin.com/in/yitzhak-levi-49a217201/), and [Max
    Rabin](https://www.linkedin.com/in/maxrabin/).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文由[Tomer Berkovich](https://www.linkedin.com/in/tomerberkovich/)、[Yitzhak Levi](https://www.linkedin.com/in/yitzhak-levi-49a217201/)和[Max
    Rabin](https://www.linkedin.com/in/maxrabin/)合作撰写。
- en: Appropriate instance selection for machine learning (ML) workloads is an important
    decision with potentially significant implications on the speed and cost of development.
    In a [previous post](/instance-selection-for-deep-learning-7463d774cff0) we expanded
    on this process, proposed a metric for making this important decision, and highlighted
    some of the many factors you should take into consideration. In this post we will
    demonstrate the opportunity for reducing AI model training costs by taking [Spot
    Instance](https://aws.amazon.com/ec2/spot/?cards.sort-by=item.additionalFields.startDateTime&cards.sort-order=asc)
    availability into account when making your cloud-based instance selection decision.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习（ML）工作负载，选择合适的实例是一个重要的决策，可能对开发的速度和成本产生重大影响。在[上一篇文章](/instance-selection-for-deep-learning-7463d774cff0)中，我们详细阐述了这一过程，提出了一个用于做出这一重要决策的度量标准，并强调了在做决策时应考虑的许多因素。在本文中，我们将展示通过在选择云端实例时考虑[Spot实例](https://aws.amazon.com/ec2/spot/?cards.sort-by=item.additionalFields.startDateTime&cards.sort-order=asc)的可用性来降低AI模型训练成本的机会。
- en: Reducing Costs Using Spot Instances
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spot实例降低成本
- en: One of the most significant opportunities for cost savings in the cloud is to
    take advantage of low cost [Amazon EC2 Spot Instances](https://aws.amazon.com/ec2/spot/?cards.sort-by=item.additionalFields.startDateTime&cards.sort-order=asc).
    Spot instances are discounted compute engines from surplus cloud service capacity.
    In exchange for the discounted price, AWS maintains the right to preempt the instance
    with little to no warning. Consequently, the relevance of Spot instance utilization
    is limited to workloads that are fault tolerant. Fortunately, through effective
    use of [model checkpointing](https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html)
    ML training workloads can be designed to be fault tolerant and to take advantage
    of the Spot instance offering. In fact, Amazon SageMaker, AWS’s managed service
    for developing ML, makes it easy to train on Spot instances by [managing the end-to-end
    Spot life-cycle](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html)
    for you.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在云计算中，最重要的节省成本的机会之一就是利用低成本的[Amazon EC2 Spot 实例](https://aws.amazon.com/ec2/spot/?cards.sort-by=item.additionalFields.startDateTime&cards.sort-order=asc)。Spot
    实例是来自多余云服务容量的折扣计算引擎。作为交换，AWS 保留在几乎没有预警的情况下中断实例的权利。因此，Spot 实例的使用仅适用于容错性强的工作负载。幸运的是，通过有效使用[模型检查点](https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html)，可以设计出具有容错能力并能够利用
    Spot 实例的机器学习训练工作负载。事实上，Amazon SageMaker，AWS 的机器学习开发管理服务，通过[管理完整的 Spot 生命周期](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html)使得在
    Spot 实例上进行训练变得更加简单。
- en: The Challenge of Anticipating Spot Instance Capacity
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测 Spot 实例容量的挑战
- en: Unfortunately, *Spot instance capacity*, which measures the availability of
    Spot instances for use, is subject to constant fluctuations and can be very difficult
    to predict. Amazon offers partial assistance in assessing the *Spot instance capacity*
    of an instance type of choice via its [Spot placement score](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-placement-score.html)
    (SPS) feature which indicates the likelihood that a Spot request will succeed
    in a given [region or availability zone (AZ)](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html).
    This is especially helpful when you have the freedom to choose to train your model
    in one of several different locations. However, the SPS feature offers no guarantees.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，*Spot 实例容量*（用于衡量 Spot 实例的可用性）会不断波动，且很难预测。Amazon 提供了部分帮助，通过[Spot placement
    score](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-placement-score.html)（SPS）功能评估所选实例类型的*Spot
    实例容量*，该功能可以指示 Spot 请求在特定[区域或可用区（AZ）](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html)成功的可能性。当你有自由选择在多个不同位置训练模型时，这个功能尤其有用。然而，SPS
    功能并不提供任何保证。
- en: When you choose to train a model on one or more Spot instances, you are taking
    the risk that your instance type of choice does not have any Spot capacity (i.e.,
    your training job will not start), or worse, that you will enter an iterative
    cycle in which your training repeatedly runs for just a small number of training
    steps and is stopped before you have made any meaningful progress — which can
    tally up your training costs without any return.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 当你选择在一个或多个 Spot 实例上训练模型时，你将面临风险，即所选实例类型可能没有任何 Spot 容量（即你的训练任务无法启动），或者更糟糕的是，你可能会进入一个反复迭代的周期，在这个周期中，训练仅仅进行了少数训练步骤，并在没有任何实质性进展的情况下被停止——这可能会增加你的训练成本而没有任何回报。
- en: Over the past couple of years, the challenges of spot instance utilization have
    been particularly acute when it comes to multi-GPU [EC2](https://aws.amazon.com/ec2/)
    instance types such as [g5.12xlarge](https://aws.amazon.com/ec2/instance-types/g5/)
    and [p4d.24xlarge](https://aws.amazon.com/ec2/instance-types/p4/). A huge increase
    in demand for powerful training accelerators (driven in part by advances in the
    field of Generative AI) combined with disruptions in the global supply chain,
    have made it virtually impossible to reliably depend on multi-GPU Spot instances
    for ML training. The natural fallback is to use the more costly [On-Demand](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-on-demand-instances.html)
    (OD) or [reserved instances](https://aws.amazon.com/ec2/pricing/reserved-instances/).
    However, in our [previous post](/instance-selection-for-deep-learning-7463d774cff0)
    we emphasized the value of considering many different alternatives for your choice
    of instance type. In this post we will demonstrate the potential gains of replacing
    multi-GPU On Demand instances with multiple single-GPU Spot instances.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年里，Spot 实例的使用挑战在多GPU [EC2](https://aws.amazon.com/ec2/) 实例类型（如 [g5.12xlarge](https://aws.amazon.com/ec2/instance-types/g5/)
    和 [p4d.24xlarge](https://aws.amazon.com/ec2/instance-types/p4/)）中尤为突出。对强大训练加速器的需求大幅增加（部分受生成式
    AI 领域进展推动），再加上全球供应链的中断，使得几乎不可能依赖多GPU Spot 实例进行 ML 训练。自然的应对措施是使用更昂贵的 [按需实例](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-on-demand-instances.html)（OD）或
    [预留实例](https://aws.amazon.com/ec2/pricing/reserved-instances/)。然而，在我们 [之前的文章](/instance-selection-for-deep-learning-7463d774cff0)
    中，我们强调了考虑多种不同备选方案来选择实例类型的价值。在本帖中，我们将展示通过将多GPU按需实例替换为多个单GPU Spot 实例所带来的潜在收益。
- en: Although our demonstration will use Amazon Web Services, similar conclusions
    can be reached on alternative cloud service platforms (CSPs). Please do not interpret
    our choice of CSP or services as an endorsement. The best option for you will
    depend on the unique details of your project. Furthermore, please take into consideration
    the possibility that the type of cost savings we will demonstrate will not reproduce
    in the case of your project and/or that the solution we propose will not be applicable
    (e.g., for some reason beyond the scope of this post). Be sure to conduct a detailed
    evaluation of the relevance and efficacy of the proposal before adapting it to
    your use case.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的演示将使用亚马逊 Web 服务（AWS），但在其他云服务平台（CSPs）上也能得出类似的结论。请不要将我们选择的 CSP 或服务解读为一种推荐。最适合您的选项将取决于项目的具体细节。此外，请考虑到我们展示的成本节省类型可能无法在您的项目中复现，和/或我们提出的解决方案可能不适用（例如，出于本帖讨论范围之外的某些原因）。在将其应用到您的用例之前，请务必对提案的相关性和有效性进行详细评估。
- en: When Multiple Single-GPU Instances are Better than a Single Multi-GPU Instance
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 当多个单GPU实例比单个多GPU实例更好时
- en: Nowadays, training AI models on multiple GPU devices in parallel — a process
    called *distributed training* — is commonplace. Setting aside instance pricing,
    when you have the choice between an instance type with multiple GPUs and multiple
    instance types with the same type of single GPUs, you would typically choose the
    multi-GPU instance. Distributed training typically requires a considerable amount
    of data communication (e.g., gradient sharing) between the GPUs. The proximity
    of the GPUs on a single instance is bound to facilitate higher network bandwidth
    and lower latency. Moreover, some multi-GPU instances include dedicated GPU-to-GPU
    inter-connections that can further accelerate the communication (e.g., [NVLink](https://www.nvidia.com/en-eu/data-center/nvlink/)
    on [p4d.24xlarge](https://aws.amazon.com/ec2/instance-types/p4/)). However, when
    Spot capacity is limited to single GPU instances, the option of training on multiple
    single GPU instances at a much lower cost becomes more compelling. At the very
    least, it warrants evaluation of its opportunity for cost-savings.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，在多个 GPU 设备上并行训练 AI 模型——这一过程被称为*分布式训练*——已经变得很常见。如果不考虑实例定价，当您在选择一个多GPU实例和多个单GPU实例（相同类型）之间做出选择时，通常会选择多GPU实例。分布式训练通常需要大量的数据通信（例如，梯度共享）在
    GPU 之间。单个实例中 GPU 的近距离位置有助于更高的网络带宽和更低的延迟。此外，一些多GPU实例还包括专用的 GPU 到 GPU 互连，可以进一步加速通信（例如，[NVLink](https://www.nvidia.com/en-eu/data-center/nvlink/)
    在 [p4d.24xlarge](https://aws.amazon.com/ec2/instance-types/p4/) 实例上）。然而，当 Spot
    容量仅限于单个 GPU 实例时，以更低的成本在多个单GPU实例上训练就显得更具吸引力。至少，这值得评估其节省成本的潜力。
- en: Optimizing Data Communication Between Multiple EC2 Instances
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化多个EC2实例之间的数据通信
- en: When distributed training runs on multiple instances, the GPUs communicate with
    one another via the network between the host machines. To optimize the speed of
    training and reduce the likelihood and/or impact of a network bottleneck, we need
    to ensure minimal network latency and maximal data throughput. These can be affected
    by a number of factors.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当分布式训练在多个实例上运行时，GPU通过宿主机之间的网络相互通信。为了优化训练速度并减少网络瓶颈的可能性和/或影响，我们需要确保最小的网络延迟和最大的数据信息流量。这些因素可能会受到多个因素的影响。
- en: Instance Collocation
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实例并置
- en: Network latency can be greatly impacted by the relative locations of the EC2
    instances. Ideally, when we request multiple cloud-based instances we would like
    them to all be collocated on the same physical rack. In practice, without appropriate
    configuration, they may not even be in the same city. In our demonstration below
    we will use a [VPC Config](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_VpcConfig.html)
    object to program an Amazon SageMaker training job to use a single subnet of an
    [Amazon Virtual Private Cloud (VPC)](https://docs.aws.amazon.com/sagemaker/latest/dg/train-vpc.html).
    This technique will ensure that all the requested training instances will be in
    the same [availability zone (AZ)](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html).
    However, collocation in the same AZ, may not suffice. Furthermore, the method
    we described involves choosing a subnet associated with one specific AZ (e.g.,
    the one with the highest [Spot placement score](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-placement-score.html)).
    A preferred API would fulfill the request in any AZ that has sufficient capacity.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 网络延迟会受到EC2实例相对位置的巨大影响。理想情况下，当我们请求多个云端实例时，希望它们都位于同一个物理机架上。实际上，如果没有适当的配置，它们甚至可能不在同一个城市。我们将在下面的示范中使用一个[VPC配置](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_VpcConfig.html)对象，通过编程将Amazon
    SageMaker训练任务指定为使用一个[Amazon虚拟私有云（VPC）](https://docs.aws.amazon.com/sagemaker/latest/dg/train-vpc.html)的单一子网。这种方法将确保所有请求的训练实例都在同一个[可用区（AZ）](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html)中。然而，仅仅在同一可用区并置可能不足以满足需求。此外，我们描述的方法涉及选择与特定可用区（例如，具有最高[Spot放置分数](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-placement-score.html)）关联的子网。理想的API应能在任何具有足够容量的可用区内满足请求。
- en: A better way to control the placement of our instances is to launch them inside
    a [placement group](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html),
    specifically a [cluster placement group](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-cluster).
    Not only will this guarantee that all of the instances will be in the same [AZ](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html),
    but it will also place them on “the same high-bisection bandwidth segment of the
    network” so as to maximize the performance of the network traffic between them.
    However, as of the time of this writing SageMaker does *not* provide the option
    to specify a [placement group](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html).
    To take advantage of placement groups we would need to use an alternative training
    service solution (as we will demonstrate below).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 控制实例位置的更好方法是将它们启动在一个[放置组](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html)中，特别是一个[集群放置组](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-cluster)。这样不仅能保证所有实例都在同一个[可用区](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html)，还会将它们放置在“网络的同一高带宽分割段”上，从而最大化它们之间网络流量的性能。然而，截至本文撰写时，SageMaker
    *不*提供指定[放置组](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html)的选项。为了利用放置组，我们需要使用替代的训练服务解决方案（如下所示）。
- en: EC2 Network **B**andwidth Constraints
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: EC2网络**带宽**约束
- en: Be sure to take into account the maximal [network bandwidth](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-network-bandwidth.html)
    supported by the EC2 instances that you choose. Note, in particular, that the
    network bandwidths associated with single-GPU machines are often documented as
    being “up to” a certain number of Gbps. Make sure to understand what that means
    and how it can impact the speed of training over time.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 确保考虑你选择的EC2实例所支持的最大[网络带宽](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-network-bandwidth.html)。特别注意，单GPU机器的网络带宽通常被描述为“最多”某个Gbps值。确保理解这意味着什么，以及它如何影响训练过程中的速度。
- en: Keep in mind that the GPU-to-GPU data communication (e.g., gradient sharing)
    might need to share the limited network bandwidth with other data flowing through
    the network such as training samples being streamed into the training instances
    or training artifacts being uploaded to persistent storage. Consider ways of reducing
    the payload of each of the categories of data to minimize the likelihood of a
    network bottleneck.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，GPU之间的数据通信（例如，梯度共享）可能需要与其他通过网络流动的数据共享有限的网络带宽，例如流入训练实例的训练样本或上传到持久存储的训练产物。考虑减少每类数据负载的方式，以最小化网络瓶颈的可能性。
- en: Elastic Fabric Adapter (EFA)
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 弹性网络适配器（EFA）
- en: A growing number of EC2 instance types support [Elastic Fabric Adapter (EFA)](https://aws.amazon.com/hpc/efa/),
    a dedicated network interface for optimizing inter-node communication. Using EFA
    can have a decisive impact on the runtime performance of your training workload.
    Note that the bandwidth on the EFA network channel is different than the documented
    bandwidth of the standard network. As of the time of this writing, detailed documentation
    of the EFA capabilities is hard to come by and it is usually best to evaluate
    its impact through trial and error. Consider using an [EC2 instance that supports
    EFA type](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html#efa-instance-types)
    when relevant.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 越来越多的EC2实例类型支持 [弹性网络适配器（EFA）](https://aws.amazon.com/hpc/efa/)，这是一种专用网络接口，用于优化节点间的通信。使用EFA可以对训练工作负载的运行时性能产生决定性影响。请注意，EFA网络通道的带宽与标准网络的带宽不同。在写作时，EFA功能的详细文档很难获得，通常最好通过试验和错误来评估其影响。在相关情况下，考虑使用支持EFA类型的[EC2实例](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html#efa-instance-types)。
- en: Toy Example
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 玩具示例
- en: We will now demonstrate the comparative price performance of training on four
    single-GPU [EC2 g5](https://aws.amazon.com/ec2/instance-types/g5/) Spot instances
    (ml.g5.2xlarge and ml.g5.4xlarge) vs. a single four-GPU On-Demand instance (ml.g5.12xlarge).
    We will use the training script below containing a Vision Transformer (ViT) backed
    classification model (trained on synthetic data).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将演示在四个单GPU [EC2 g5](https://aws.amazon.com/ec2/instance-types/g5/) Spot
    实例（ml.g5.2xlarge 和 ml.g5.4xlarge）与一个单四GPU按需实例（ml.g5.12xlarge）上的训练性能对比。我们将使用下面的训练脚本，该脚本包含一个基于
    Vision Transformer（ViT）的分类模型（在合成数据上进行训练）。
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The code block below demonstrates how we used the [SageMaker Python package](https://sagemaker.readthedocs.io/en/stable/)
    (version 2.203.1) to run our experiments. Note that for the four-instance experiments,
    we configure the use of a VPC with a single subnet, as explained above.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块演示了我们如何使用 [SageMaker Python 包](https://sagemaker.readthedocs.io/en/stable/)（版本
    2.203.1）运行实验。请注意，对于四实例的实验，我们配置了使用一个具有单一子网的VPC，如上所述。
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that our code depends on the third-party [*timm*](https://pypi.org/project/timm/)Python
    package that we point to in a requirements.txt file in the root of the source
    directory. This assumes that the VPC has been configured to [enable internet access](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html).
    Alternatively, you could define a private PyPI server (as described [here](https://aws.amazon.com/blogs/machine-learning/hosting-a-private-pypi-server-for-amazon-sagemaker-studio-notebooks-in-a-vpc/)),
    or create a custom image with your third party dependencies preinstalled (as described
    [here](/customizing-your-cloud-based-machine-learning-training-environment-part-2-b65a6cf91812)).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们的代码依赖于第三方[*timm*](https://pypi.org/project/timm/)Python 包，且我们在源代码目录根目录中的
    requirements.txt 文件中指定了该包。这假设 VPC 已配置为[启用互联网访问](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html)。另外，您可以定义一个私有的
    PyPI 服务器（如[此处](https://aws.amazon.com/blogs/machine-learning/hosting-a-private-pypi-server-for-amazon-sagemaker-studio-notebooks-in-a-vpc/)所述），或者创建一个预先安装了第三方依赖项的自定义镜像（如[此处](/customizing-your-cloud-based-machine-learning-training-environment-part-2-b65a6cf91812)所述）。
- en: Results
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: We summarize the results of our experiment in the table below. The On-Demand
    prices were taken from the [SageMaker pricing page](https://aws.amazon.com/sagemaker/pricing/)
    (as of the time of this writing, January 2024). The Spot saving values were collected
    from the reported *managed spot training savings* of the completed job. Please
    see the [EC2 Spot pricing documentation](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances-history.html)
    to get a sense for how the reported Spot savings are calculated.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在下表中总结了实验结果。按需定价来自于[SageMaker 定价页面](https://aws.amazon.com/sagemaker/pricing/)（截至本文写作时，2024年1月）。竞价节省的值是从已完成任务的*托管竞价训练节省*报告中收集的。请参阅[EC2
    竞价定价文档](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances-history.html)以了解报告的竞价节省是如何计算的。
- en: '![](../Images/3ad9e651b7ad635f5ad718a84cd2b1e7.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ad9e651b7ad635f5ad718a84cd2b1e7.png)'
- en: Experiment Results (by Author)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果（作者）
- en: Our results clearly demonstrate the potential for considerable savings when
    using four single-GPU Spot instances rather than a single four-GPU On Demand instance.
    They further demonstrate that although the cost of an On Demand g5.4xlarge instance
    type is higher, the increased CPU power and/or network bandwidth combined with
    higher Spot savings, resulted in much greater savings.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的结果清楚地表明，当使用四个单 GPU 竞价实例而不是一个四 GPU 按需实例时，节省的潜力是相当大的。进一步的结果表明，尽管按需 g5.4xlarge
    实例的成本较高，但由于增加的 CPU 功率和/或网络带宽，以及更高的竞价节省，最终导致了更大的节省。
- en: Importantly, keep in mind that the relative performance results can vary considerably
    based on the details of your job as well the Spot prices at the time that you
    run your experiments.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，请记住，相对性能结果可能会根据您的任务细节以及实验运行时的竞价价格而大幅变化。
- en: Enforcing EC2 Instance Co-location Using a Cluster Placement Group
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用集群放置组强制执行 EC2 实例协同定位
- en: In a [previous post](/a-simple-solution-for-managing-cloud-based-ml-training-c80a69c6939a)
    we described how to create a customized managed environment on top of an unmanaged
    service, such as [Amazon EC2](https://aws.amazon.com/ec2/). One of the motivating
    factors listed there was the desire to have greater control over device placement
    in a multi-instance setup, e.g., by using a [cluster placement group](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-cluster),
    as discussed above. In this section, we demonstrate the creation of a multi-node
    setup using a cluster placement group.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在[上一篇文章](/a-simple-solution-for-managing-cloud-based-ml-training-c80a69c6939a)中，我们描述了如何在未管理的服务之上创建一个自定义的管理环境，例如[Amazon
    EC2](https://aws.amazon.com/ec2/)。其中列出的一个动机因素是希望在多实例设置中对设备放置进行更大的控制，例如，使用[集群放置组](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-cluster)，如上所述。在本节中，我们展示了如何使用集群放置组创建多节点设置。
- en: 'Our code assumes the presence of a [default VPC](https://docs.aws.amazon.com/vpc/latest/userguide/default-vpc.html)
    as well as the (one-time) creation of a [cluster placement group](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-cluster),
    demonstrated here using the [AWS Python SDK](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)
    (version 1.34.23):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代码假设存在一个[默认VPC](https://docs.aws.amazon.com/vpc/latest/userguide/default-vpc.html)，并且（一次性）创建了一个[集群放置组](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-cluster)，这里演示使用了[AWS
    Python SDK](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)（版本1.34.23）：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In the code block below we use the [AWS Python SDK](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)
    to launch our Spot instances:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码块中，我们使用[AWS Python SDK](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)来启动我们的Spot实例：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Please see our [previous post](/a-simple-solution-for-managing-cloud-based-ml-training-c80a69c6939a)
    for step-by-step tips on how to extend this to an automated training solution.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅我们的[上一篇文章](/a-simple-solution-for-managing-cloud-based-ml-training-c80a69c6939a)，了解如何一步步将其扩展为自动化训练解决方案的提示。
- en: Summary
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this post, we have illustrated how demonstrating flexibility in your choice
    of training instance type can increase your ability to leverage Spot instance
    capacity and reduce the overall cost of training.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们演示了如何通过选择灵活的训练实例类型来提高利用Spot实例容量的能力，从而降低整体训练成本。
- en: As the sizes of AI models continue to grow and the costs of AI training accelerators
    continue to rise, it becomes increasingly important that we explore ways to mitigate
    training expenses. The technique outlined here is just one among several methods
    for optimizing cost performance. We encourage you to explore our [previous posts](https://chaimrand.medium.com/)
    for insights into additional opportunities in this realm.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 随着AI模型规模的不断扩大以及AI训练加速器成本的不断上涨，探索减少训练开销的方式变得愈加重要。这里介绍的技术只是优化成本性能的几种方法之一。我们鼓励你查阅我们的[上一篇文章](https://chaimrand.medium.com/)，以获取更多关于该领域其他机会的见解。
