- en: Multi-Framework AI/ML Development with Keras 3
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Keras 3 进行多框架 AI/ML 开发
- en: 原文：[https://towardsdatascience.com/multi-framework-ai-ml-development-with-keras-3-cf7be29eb23d?source=collection_archive---------3-----------------------#2024-06-16](https://towardsdatascience.com/multi-framework-ai-ml-development-with-keras-3-cf7be29eb23d?source=collection_archive---------3-----------------------#2024-06-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/multi-framework-ai-ml-development-with-keras-3-cf7be29eb23d?source=collection_archive---------3-----------------------#2024-06-16](https://towardsdatascience.com/multi-framework-ai-ml-development-with-keras-3-cf7be29eb23d?source=collection_archive---------3-----------------------#2024-06-16)
- en: All hail the return of Keras
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 欢迎 Keras 的回归
- en: '[](https://chaimrand.medium.com/?source=post_page---byline--cf7be29eb23d--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page---byline--cf7be29eb23d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--cf7be29eb23d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--cf7be29eb23d--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page---byline--cf7be29eb23d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://chaimrand.medium.com/?source=post_page---byline--cf7be29eb23d--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page---byline--cf7be29eb23d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--cf7be29eb23d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--cf7be29eb23d--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page---byline--cf7be29eb23d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--cf7be29eb23d--------------------------------)
    ·14 min read·Jun 16, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--cf7be29eb23d--------------------------------)
    ·14 分钟阅读·2024年6月16日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/2f76631d92e90d65b370eb16d967b59c.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f76631d92e90d65b370eb16d967b59c.png)'
- en: Photo by [Jose Rueda](https://unsplash.com/@notartistic?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Jose Rueda](https://unsplash.com/@notartistic?utm_source=medium&utm_medium=referral)
    提供，来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '![](../Images/60dcf4db50ea7db99db7c65748a3b9d0.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/60dcf4db50ea7db99db7c65748a3b9d0.png)'
- en: By Author
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '作者：  '
- en: Keras is Back!! First released in 2015 as a high-level Python library for training
    ML models, Keras grew in popularity due to its clean and simple APIs. Contrary
    to the ML frameworks of the time, with their awkward and clunky APIs, Keras lowered
    the entry bar for many incumbent ML developers (the author included). But somewhere
    along the way the use of Keras became virtually synonymous with TensorFlow development.
    Consequently, when developers began to turn to alternative frameworks, the relative
    popularity of Keras began to decline. But now, following a “complete rewrite”,
    Keras has returned. And with its shiny new engine and its renewed commitment to
    multi-backend support, it vies to return to its former glory.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 回来了！！Keras 最初于 2015 年发布，作为一个高阶 Python 库，用于训练机器学习模型。由于其简洁清晰的 API，Keras
    迅速获得了人气。与当时那些笨重且不灵活的机器学习框架相比，Keras 降低了许多现有机器学习开发者（包括作者本人）的入门门槛。然而，在某个时刻，Keras
    的使用几乎与 TensorFlow 开发同义。因此，当开发者开始转向其他框架时，Keras 的相对受欢迎程度开始下降。但现在，经过“完全重写”后，Keras
    已经回归。凭借其崭新的引擎和对多后端支持的重新承诺，它有望恢复昔日的辉煌。
- en: In this post we will take a new look at Keras and assess its value offering
    in the current era of AI/ML development. We will demonstrate through example its
    ease of use and make note of its shortcomings. Importantly, this post is not intended
    to be an endorsement for or against the adoption of Keras (or any other framework,
    library, service, etc.). As usual, the best decision for your project development
    will depend on a great many details, many of which are beyond the scope of this
    post.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将重新审视 Keras，并评估其在当前 AI/ML 开发时代的价值。我们将通过示例展示其易用性，并指出其不足之处。重要的是，这篇文章并非旨在支持或反对采用
    Keras（或任何其他框架、库、服务等）。像往常一样，最适合你项目开发的决策将取决于许多细节，其中很多超出了这篇文章的范围。
- en: The [recent release](https://blog.google/technology/developers/gemma-open-models/)
    of Google’s family of open sourced NLP models known as Gemma, and the inclusion
    of Keras 3 as a core component of the API, offers us an opportunity to evaluate
    Keras’s goodness and could serve as a great opportunity for its resurgence.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Google最近发布的开源NLP模型家族Gemma，以及Keras 3作为其API核心组件的加入，为我们提供了评估Keras优点的机会，也可能成为其复兴的一个绝佳机会。
- en: Why Use Keras 3?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么使用Keras 3？
- en: In our view, the most valuable feature offered by Keras 3 is its multi-framework
    support. This may surprise some readers who may recall Keras’s distinctiveness
    to be its user experience. Keras 3 advertises itself, as “simple”, “flexible”,
    and being “designed for human beings, not machines”. And indeed, it owes its early
    successes and meteoric rise in popularity to its user experience. But it is now
    2024 and there are many high-level deep learning APIs offering “reduced cognitive
    load”. In our view, the user experience, as good as it may be, is no longer a
    sufficient motivator to consider Keras over its alternatives. Its multi-framework
    support is.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们看来，Keras 3最有价值的特点是其多框架支持。这可能会让一些读者感到惊讶，他们可能会记得Keras的独特性在于其用户体验。Keras 3自我宣传为“简单”、“灵活”，并且是“为人类设计，而不是为机器设计”。的确，Keras的早期成功和急剧的流行上升，正是得益于其优秀的用户体验。但现在是2024年，许多高级深度学习API都提供了“降低认知负担”的功能。在我们看来，虽然用户体验非常好，但它已经不足以成为考虑选择Keras而不是其他替代框架的充足动因。而它的多框架支持则是。
- en: The Merits of Multi-Framework Support
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多框架支持的优点
- en: Keras 3 supports multiple backends for training and running its models. At the
    time of this writing, these include [JAX](https://jax.readthedocs.io/), [TensorFlow](https://github.com/tensorflow/tensorflow),
    and [PyTorch](https://pytorch.org/). The [Keras 3 announcement](https://keras.io/keras_3/)
    does a pretty good job of explaining the advantages of this feature. We will expand
    on the documented benefits and add some of our own flavor.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 3支持多个后端来训练和运行其模型。在本文撰写时，这些后端包括[JAX](https://jax.readthedocs.io/)、[TensorFlow](https://github.com/tensorflow/tensorflow)和[PyTorch](https://pytorch.org/)。[Keras
    3公告](https://keras.io/keras_3/)很好地解释了这一特性的优势。我们将在此基础上扩展并加入一些自己的观点。
- en: '**Avoid the difficulty of choosing an AI/ML framework:** Choosing an AI/ML
    framework is probably one of the most important decisions you will need to make
    as an ML developer. It is also one of the hardest. There are many considerations
    that need to factor into this decision. These include user experience, API coverage,
    programmability, debuggability, the formats and types of input data that are supported,
    conformance with other components on the development pipeline (e.g., restrictions
    that may be imposed by the model deployment phase), and, perhaps most importantly,
    runtime performance. As we have discussed in many of our previous posts (e.g.,
    [here](https://medium.com/p/6e407a7d2dc8#de85-799b58b79241)), AI/ML model development
    can be extremely expensive and the overall impact on cost of even the smallest
    speed-up due to the choice of framework can be dramatic. In fact, in many cases
    it may warrant the overhead of porting your model and code to a different framework
    and/or even maintaining support for multiple frameworks.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**避免选择AI/ML框架的难题：** 选择一个AI/ML框架可能是作为ML开发者你需要做出的最重要的决定之一，同时也是最困难的决定之一。这个决策需要考虑的因素有很多，包括用户体验、API覆盖范围、可编程性、可调试性、支持的输入数据的格式和类型、与开发流程中其他组件的兼容性（例如，模型部署阶段可能施加的限制），以及最重要的——运行时性能。正如我们在之前的多篇文章中讨论的那样（例如，[这里](https://medium.com/p/6e407a7d2dc8#de85-799b58b79241)），AI/ML模型开发可能非常昂贵，选择框架所带来的哪怕是最小的性能提升，也会对整体成本产生重大影响。事实上，在很多情况下，这可能值得你为将模型和代码迁移到不同的框架，甚至支持多个框架而付出额外的开销。'
- en: The problem is that it is extremely difficult, if not impossible, to know which
    framework will be most optimal for your model before you start your development.
    Moreover, even once you have committed to one framework, you will want to stay
    on top of the evolution and development of all frameworks and to continuously
    assess potential opportunities to improve your model and/or reduce the cost of
    development. The landscape of AI/ML development is extremely dynamic with optimizations
    and enhancements being designed and developed on a consistent basis. You will
    not want to fall behind.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，在你开始开发之前，几乎不可能知道哪个框架对你的模型最为优化。而且，即使你已经选择了一个框架，你也会希望紧跟所有框架的演变和发展，并持续评估改进模型和/或降低开发成本的潜在机会。AI/ML
    开发的环境非常动态，优化和增强功能正在持续不断地设计和开发中。你不希望落后于人。
- en: Keras 3 solves the framework selection problem by enabling you to develop your
    model without committing to an underlying backend. The option to toggle between
    multiple framework-backends allows you to focus on the model definition and, once
    complete, choose the backend that best suits your needs. And even as the properties
    of the ML project change or the supported frameworks evolve, Keras 3 enables you
    to easily assess the impact of changing the backend.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 3 通过允许你在不承诺底层后端的情况下开发模型，解决了框架选择问题。通过在多个框架后端之间切换的选项，你可以专注于模型定义，完成后再选择最适合你需求的后端。即使随着机器学习项目的特性变化或支持的框架发展，Keras
    3 也能让你轻松评估更换后端的影响。
- en: Putting it colloquially, you could say that Keras 3 helps humans avoid one of
    the things they hate doing most — making decisions and committing to them. But
    humor aside, AI/ML model development using Keras 3 can certainly prevent you from
    choosing and being stuck with a suboptimal framework.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通俗地说，你可以说 Keras 3 帮助人类避免做他们最讨厌做的事情之一——做出决定并坚持下去。但言归正传，使用 Keras 3 进行 AI/ML 模型开发，确实能够帮助你避免选择并被迫困在一个次优的框架中。
- en: '**Enjoy the best of all worlds:** PyTorch, TensorFlow, and JAX, each have their
    own unique advantages and differentiating properties. JAX, for example, supports
    just-in-time (JIT) compilation in which the model operators are converted into
    an intermediate computation graph and then compiled together into machine code
    specifically targeted for the underlying hardware. For many models this results
    in a considerable boost in runtime performance. On the other hand, PyTorch, which
    is typically used in a manner in which the operators are executed immediately
    (a.k.a. “eagerly”) is often considered to: have the most Pythonic interface, be
    the easiest to debug, and offer the best overall user experience. By using Keras
    3 you can enjoy the best of both worlds. You can set the backend to PyTorch during
    your initial model development and for debugging and switch to JAX for optimal
    performance when training in production mode.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**享受各个世界的最佳优势：** PyTorch、TensorFlow 和 JAX 各自具有独特的优势和差异化特性。例如，JAX 支持即时编译（JIT），在这种模式下，模型操作符会被转换成中间计算图，然后编译成专门针对底层硬件的机器码。对于许多模型来说，这通常会显著提升运行时性能。另一方面，PyTorch
    通常以立即执行操作符（即“急切执行”）的方式使用，通常被认为：具有最符合 Python 风格的接口、最容易调试，并提供最佳的整体用户体验。通过使用 Keras
    3，你可以享受两者的最佳优点。在初始模型开发和调试过程中，你可以将后端设置为 PyTorch，而在生产模式下进行训练时，可以切换到 JAX 以获得最佳性能。'
- en: '**Compatibility with the maximum number of AI accelerators and runtime environments:**
    As we have discussed in the past (e.g., [here](/instance-selection-for-deep-learning-7463d774cff0))
    our goal is to be compatible with as many AI accelerators and runtime environments
    as possible. This is especially important in an era of constrained capacity of
    AI machines in which the ability to switch between different machine types is
    a huge advantage. When you develop with Keras 3 and its multi-backend support,
    you automatically increase the number of platforms that you can potentially train
    and run your model on. For example, while you may be most accustomed to running
    in PyTorch on GPUs, by simply changing the backend to JAX you can configure your
    model to run on [Google Cloud TPUs](https://cloud.google.com/tpu?hl=en), as well
    ( — though this may depend on the details of the model).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**与最大数量的 AI 加速器和运行时环境的兼容性：** 正如我们之前所讨论的（例如，[在这里](/instance-selection-for-deep-learning-7463d774cff0)），我们的目标是与尽可能多的
    AI 加速器和运行时环境兼容。在 AI 机器容量受限的时代，能够在不同机器类型之间切换是一个巨大的优势。当你使用 Keras 3 及其多后端支持进行开发时，你将自动增加可以训练和运行你模型的平台数量。例如，虽然你可能最习惯在
    GPU 上运行 PyTorch，但只需将后端更改为 JAX，你也可以将模型配置为在 [Google Cloud TPUs](https://cloud.google.com/tpu?hl=en)
    上运行（——不过这可能取决于模型的具体细节）。'
- en: '**Increase model adoption:** If you are targeting your model for use by other
    AI/ML teams, you will increase your potential audience by supporting multiple
    frameworks. For all sorts of reasons, some teams may be limited to a specific
    ML framework. By delivering your model in Keras you remove barriers for adoption.
    A great example of this is the recent release of Google’s Gemma models which we
    will discuss in greater detail below.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**增加模型的采用率：** 如果你的目标是让其他 AI/ML 团队使用你的模型，那么通过支持多个框架，你将增加潜在的受众群体。由于各种原因，一些团队可能会局限于特定的
    ML 框架。通过在 Keras 中交付你的模型，你消除了采用的障碍。一个很好的例子是最近发布的 Google 的 Gemma 模型，我们将在下面更详细地讨论。'
- en: '**Decouple the data input pipeline from the model execution:** Some frameworks
    encourage the use of certain data storage formats and/or data loading practices.
    A classic example of this is TensorFlow’s [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord)
    data format for storing a sequence of binary records that are typically stored
    in `.tfrecord` files. While TensorFlow includes native support for parsing and
    processing data stored TFRecord files, you might find feeding them into a PyTorch
    training loop to be a bit more difficult. A preferable format for PyTorch training
    could be [WebDataset](https://pytorch.org/data/main/generated/torchdata.datapipes.iter.WebDataset.html).
    But the creation of training data can be a long process and maintaining it in
    more than one format could be prohibitively expensive. Thus, the manner in which
    your training data is stored and maintained might discourage teams from considering
    alternative frameworks.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**将数据输入管道与模型执行解耦：** 一些框架鼓励使用某些数据存储格式和/或数据加载实践。一个经典的例子是 TensorFlow 的 [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord)
    数据格式，用于存储通常以 `.tfrecord` 文件形式存储的二进制记录序列。尽管 TensorFlow 原生支持解析和处理存储在 TFRecord 文件中的数据，但你可能会发现将它们输入到
    PyTorch 的训练循环中会有些困难。PyTorch 训练的一个更合适的格式可能是 [WebDataset](https://pytorch.org/data/main/generated/torchdata.datapipes.iter.WebDataset.html)。但是，训练数据的创建可能是一个漫长的过程，而将其维护为多种格式可能会变得极其昂贵。因此，训练数据的存储和维护方式可能会使团队不愿考虑使用其他框架。'
- en: Keras 3 helps teams overcome this obstacle by completely decoupling the data
    input pipeline from the training loop. You can define your input data pipelines
    in PyTorch, TensorFlow, Numpy, Keras, and other libraries without any consideration
    for the backend that will be used in your training loop. With Keras 3, having
    your training data stored in TFRecord files is no longer a barrier to adopting
    PyTorch as a backend.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 3 帮助团队克服这一障碍，通过完全解耦数据输入管道和训练循环。你可以在 PyTorch、TensorFlow、Numpy、Keras 和其他库中定义输入数据管道，而无需考虑将用于训练循环的后端。使用
    Keras 3 后，将训练数据存储在 TFRecord 文件中不再是采用 PyTorch 作为后端的障碍。
- en: The Disadvantages of Multi-Framework Support
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多框架支持的缺点
- en: 'As with any other new SW solution on the market, it is important to be aware
    of the potential downsides of Keras 3\. A general rule of thumb in SW development
    is that the higher up the SW stack you go, the less control you have over the
    behavior and performance of your application. In AI/ML, where the degree of success
    is often determined by precise tuning of model hyperparameters, initialization
    settings, appropriate environment configuration, etc., such control could be critical.
    Here are just a few potential drawbacks to consider:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 与市场上其他任何新软件解决方案一样，了解 Keras 3 的潜在缺点非常重要。软件开发中的一个通用准则是，越是在软件堆栈的上层，你对应用程序行为和性能的控制就越少。在
    AI/ML 领域，成功的程度通常取决于模型超参数、初始化设置、环境配置等的精确调节，这样的控制可能至关重要。以下是一些需要考虑的潜在缺点：
- en: '**Potential drop in runtime performance:** Working the high level Keras APIs
    rather than directly with the framework APIs, may pose limitations on optimizing
    runtime performance. In our series of posts on the topic of [analyzing and optimizing
    the performance of PyTorch models](/pytorch-model-performance-analysis-and-optimization-10c3c5822869),
    we demonstrated a wide range of tools and techniques for increasing the speed
    of training. Sometimes these require the direct, unmediated, use of PyTorch’s
    APIs. For example, Keras’s APIs currently include very limited support for [PyTorch’s
    JIT compilation](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)
    option (via the [*jit_compile*](https://keras.io/api/models/model_training_apis/)setting).
    Another example is PyTorch’s built-in support for [scaled dot product attention](https://pytorch.org/docs/2.2/generated/torch.nn.functional.scaled_dot_product_attention.html)
    which is not supported at the Keras level (as of the time of this writing).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**运行时性能的潜在下降：** 使用高级 Keras API 而不是直接使用框架 API，可能会对优化运行时性能造成一定的限制。在我们关于[分析和优化
    PyTorch 模型性能](/pytorch-model-performance-analysis-and-optimization-10c3c5822869)的系列文章中，我们展示了各种提高训练速度的工具和技巧。有时这些技巧需要直接、不加中介地使用
    PyTorch 的 API。例如，Keras 的 API 目前对[PyTorch 的 JIT 编译](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)选项的支持非常有限（通过[*jit_compile*](https://keras.io/api/models/model_training_apis/)设置）。另一个例子是
    PyTorch 对[缩放点积注意力](https://pytorch.org/docs/2.2/generated/torch.nn.functional.scaled_dot_product_attention.html)的内置支持，而
    Keras 层面（截至本文撰写时）并不支持这一功能。'
- en: '**Limitations of cross-framework support:** Although Keras’s cross-framework
    support is extensive, you may find that it is not all-encompassing. For example,
    one gap in coverage (as of the time of this writing) is distributed training.
    Although, Keras introduces [the Keras distribution API](https://keras.io/guides/distribution/)
    to support data and model parallelism across all backends, it is currently implemented
    for the JAX backend only. To run distributed training when using other backends,
    you will need to fall back to the standard distribution APIs of the relevant framework
    (e.g., PyTorch’s [distributed data parallel API](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**跨框架支持的局限性：** 虽然 Keras 的跨框架支持非常广泛，但你可能会发现它并不是包罗万象的。例如，一个覆盖的空白（截至本文撰写时）是分布式训练。虽然
    Keras 引入了[Keras 分布式 API](https://keras.io/guides/distribution/)来支持所有后端的数据和模型并行性，但目前仅对
    JAX 后端实现了该功能。要在使用其他后端时运行分布式训练，你需要回退到相关框架的标准分布式 API（例如，PyTorch 的[分布式数据并行 API](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)）。'
- en: '**Overhead of maintaining cross-framework compatibility:** Keras 3 supports
    a wide variety of pre-built models that you can reuse (e.g., [here](https://keras.io/api/keras_nlp/models/)).
    However, inevitably, you may want to introduce your own customizations. While
    Keras 3 supports customization of the model layers, metrics, training loop and
    more, you will need to take care not to break your cross-framework compatibility.
    For example, if you create a custom layer using Keras’s backend-agnostic APIs
    (`keras.ops`), you can rest assured that multi-backend support is retained. However,
    sometimes you may choose to rely on framework-specific operations. In such cases
    maintaining cross-framework compatibility will require a dedicated implementation
    for each framework and appropriate conditional programming based on the backend
    in use. The current methods for customizing a [training step](https://keras.io/guides/custom_train_step_in_jax/)
    and a [training loop](https://keras.io/guides/writing_a_custom_training_loop_in_jax/)
    are framework-specific, meaning that they too would require dedicated implementations
    for each backend to retain cross-framework compatibility. Thus, as your model
    grows in complexity, so might the overhead required to maintain this unique capability.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**维护跨框架兼容性的开销：** Keras 3支持多种预构建模型，您可以复用它们（例如，[这里](https://keras.io/api/keras_nlp/models/)）。然而，您可能不可避免地想要引入自己的自定义功能。虽然Keras
    3支持自定义模型层、指标、训练循环等，但您需要小心避免破坏跨框架兼容性。例如，如果您使用Keras的后端无关API（`keras.ops`）创建了一个自定义层，您可以放心，它会保留多后端支持。但是，有时您可能会选择依赖于框架特定的操作。在这种情况下，保持跨框架兼容性将需要为每个框架提供专门的实现，并根据所使用的后端进行适当的条件编程。目前自定义[训练步骤](https://keras.io/guides/custom_train_step_in_jax/)和[训练循环](https://keras.io/guides/writing_a_custom_training_loop_in_jax/)的方法是框架特定的，这意味着它们同样需要为每个后端提供专门的实现，以保持跨框架兼容性。因此，随着模型复杂性的增加，维持这一独特能力所需的开销也可能增加。'
- en: We have noted just a few potential disadvantages to Keras 3 and its multi-backend
    support. You may very well likely come across others. While the multi-framework
    offering is certainly compelling, its adoption is not necessarily free of cost.
    Borrowing the name of a well-known [theorem](https://en.wikipedia.org/wiki/No_free_lunch_theorem)
    in the field of statistical inference, one could say that when it comes to choosing
    an AI/ML development methodology, there are “no free lunches”.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅列出了Keras 3及其多后端支持的一些潜在缺点，您可能还会遇到其他问题。尽管多框架的提供确实具有吸引力，但其采用并不一定是无代价的。借用统计推断领域著名[定理](https://en.wikipedia.org/wiki/No_free_lunch_theorem)的名字，可以说，当涉及到选择AI/ML开发方法时，“没有免费的午餐”。
- en: Keras 3 in Practice — A Toy Example
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras 3的实践 — 一个玩具示例
- en: As in many of our recent posts, the toy model we will define will be a [Vision
    Transformer](https://huggingface.co/docs/transformers/en/model_doc/vit) (ViT)
    backed classification model. We will rely on the reference implementation located
    in this Keras [tutorial](https://keras.io/examples/vision/image_classification_with_vision_transformer/).
    We have configured our model according to the [ViT-Base](https://deci.ai/model-zoo/vit/)
    architecture (~86 million parameters), set the [mixed_precision](https://keras.io/api/mixed_precision/)
    policy to use *bfloat16*, and defined a [PyTorchdataloader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)
    with random input data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在许多最近的文章中提到的，我们将定义的玩具模型是一个[视觉变换器](https://huggingface.co/docs/transformers/en/model_doc/vit)（ViT）支持的分类模型。我们将依赖于此Keras[教程](https://keras.io/examples/vision/image_classification_with_vision_transformer/)中的参考实现。我们已根据[ViT-Base](https://deci.ai/model-zoo/vit/)架构（约8600万个参数）配置了我们的模型，将[mixed_precision](https://keras.io/api/mixed_precision/)策略设置为使用*bfloat16*，并定义了一个带有随机输入数据的[PyTorch数据加载器](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)。
- en: 'The following block includes the configuration settings followed by definitions
    of the core ViT model components:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以下区块包含了配置设置，并紧接着是核心ViT模型组件的定义：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Using the core components, we define a ViT-backed Keras model:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 使用核心组件，我们定义了一个ViT支持的Keras模型：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the next block we define the optimizer, loss, and dataset.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个区块中，我们定义了优化器、损失函数和数据集。
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, we start the training using Keras’s [Model.fit()](https://keras.io/api/models/model_training_apis/)
    function:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用Keras的[Model.fit()](https://keras.io/api/models/model_training_apis/)函数开始训练：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We ran the script above on a Google Cloud Platform (GCP) [g2-standard-16](https://cloud.google.com/compute/docs/gpus#l4-gpus)
    VM (with a single NVIDIA L4 GPU) with a dedicated [deep learning VM image](https://cloud.google.com/deep-learning-vm/docs/release-notes)
    (common-cu121-v20240514-ubuntu-2204-py310) and installations of PyTorch (2.3.0),
    JAX (0.4.28), Keras (3.3.3), and [KerasCV](https://keras.io/keras_cv/) (0.9.0).
    Please see the [official Keras documentation](https://github.com/keras-team/keras/blob/master/README.md#installation)
    for full installation instructions. Note that we manually modified the format
    of step time reported by the [Keras progress bar](https://github.com/keras-team/keras/blob/v3.3.3/keras/src/utils/progbar.py#L228):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 Google Cloud Platform (GCP) [g2-standard-16](https://cloud.google.com/compute/docs/gpus#l4-gpus)
    虚拟机（配备单个 NVIDIA L4 GPU）上运行了上述脚本，并使用了专门的 [深度学习虚拟机镜像](https://cloud.google.com/deep-learning-vm/docs/release-notes)（common-cu121-v20240514-ubuntu-2204-py310），并安装了
    PyTorch（2.3.0）、JAX（0.4.28）、Keras（3.3.3）和 [KerasCV](https://keras.io/keras_cv/)（0.9.0）。请参阅
    [Keras 官方文档](https://github.com/keras-team/keras/blob/master/README.md#installation)获取完整的安装说明。请注意，我们手动修改了由
    [Keras 进度条](https://github.com/keras-team/keras/blob/v3.3.3/keras/src/utils/progbar.py#L228)报告的步骤时间格式：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Using the *backend* flag we were able to easily toggle between the backends
    supported by Keras and compare the runtime performance of each. For example, when
    configuring [PyTorchdataloader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)
    with 0 workers, we found that JAX backend to outperform PyTorch by ~24%. When
    setting the number of workers to 16 this drops to ~12%.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 *backend* 标志，我们能够轻松地在 Keras 支持的后端之间切换，并比较每个后端的运行时性能。例如，当配置 [PyTorch 数据加载器](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)
    并设置为 0 个工作线程时，我们发现 JAX 后端比 PyTorch 快约 24%。当将工作线程数设置为 16 时，这一差距缩小至约 12%。
- en: Custom Attention Layer
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义注意力层
- en: We now define a custom attention layer that replaces [Keras’s default attention](https://keras.io/api/layers/attention_layers/multi_head_attention/)
    computation with PyTorch’s [flash attention](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)
    implementation. Note that this will only work when the backend is set to *torch.*
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在定义了一个自定义注意力层，用 PyTorch 的 [闪电注意力](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)
    实现替换了 [Keras 的默认注意力](https://keras.io/api/layers/attention_layers/multi_head_attention/)
    计算。请注意，这仅在后端设置为 *torch* 时有效。
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The results of our experiments are summarized in the table below. Keep in mind
    that the relative performance results are likely to vary greatly based on the
    details of the model and the runtime environment.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实验的结果总结在下面的表格中。请记住，相对性能结果可能会根据模型的细节和运行时环境有很大差异。
- en: '![](../Images/bcef0995a3e1fe23cd9cca46c39e9294.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bcef0995a3e1fe23cd9cca46c39e9294.png)'
- en: ViT runtime (by Author)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ViT 运行时（作者）
- en: When using our custom attention layer, the gap between the JAX and PyTorch backends
    virtually disappears. This highlights how the use of a multi-backend solution
    could come at the expense of optimizations uniquely supported by any of the individual
    frameworks (in our example, PyTorch SDPA).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的自定义注意力层时，JAX 和 PyTorch 后端之间的差距几乎消失。这凸显了多后端解决方案的使用可能会以牺牲某些框架特有的优化为代价（在我们的示例中，是
    PyTorch 的 SDPA）。
- en: Keras 3 in Gemma
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras 3 in Gemma
- en: '[Gemma](https://ai.google.dev/gemma/?utm_source=keyword&utm_medium=referral&utm_campaign=gemma_cta&utm_content=)
    is a family of lightweight, [open source models](https://opensource.googleblog.com/2024/02/building-open-models-responsibly-gemini-era.html)
    recently [released](https://blog.google/technology/developers/gemma-open-models/)
    by Google. Keras 3 plays a prominent role in the Gemma release (e.g., see [here](https://ai.google.dev/gemma/docs/get_started))
    and its multi-framework support makes Gemma automatically accessible to AI/ML
    developers of all persuasions — PyTorch, TensorFlow, and Jax. Please see the official
    [documentation](https://keras.io/api/keras_nlp/models/gemma/) in [KerasNLP](https://keras.io/api/keras_nlp/)
    for more details on the Gemma API offering.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[Gemma](https://ai.google.dev/gemma/?utm_source=keyword&utm_medium=referral&utm_campaign=gemma_cta&utm_content=)
    是 Google 最近发布的一系列轻量级 [开源模型](https://opensource.googleblog.com/2024/02/building-open-models-responsibly-gemini-era.html)。Keras
    3 在 Gemma 发布中扮演了重要角色（例如，参见 [此处](https://ai.google.dev/gemma/docs/get_started)），其多框架支持使得
    Gemma 能够自动服务于各类 AI/ML 开发者——无论是 PyTorch、TensorFlow 还是 Jax。有关 Gemma API 的详细信息，请参阅官方
    [文档](https://keras.io/api/keras_nlp/models/gemma/) 和 [KerasNLP](https://keras.io/api/keras_nlp/)。'
- en: The following code is loosely based on the official [Gemma fine-tuning tutorial](https://ai.google.dev/gemma/docs/lora_tuning).
    In order to run the script, please follow the [necessary setup instructions](https://ai.google.dev/gemma/docs/lora_tuning#setup).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码大致基于官方的[Gemma 微调教程](https://ai.google.dev/gemma/docs/lora_tuning)。为了运行此脚本，请遵循[必要的设置说明](https://ai.google.dev/gemma/docs/lora_tuning#setup)。
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: When running the script in the same GCP environment described above, we see
    a significant (and surprising) discrepancy between the runtime performance when
    using the JAX backend (6.87 samples per second) and the runtime performance when
    using the PyTorch backend (3.01 samples per second). This is due, in part, to
    the fact that the JAX backend allows for doubling the training batch size. A deep
    dive into the causes of this discrepancy is beyond the scope of this post.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当在上述相同的 GCP 环境中运行脚本时，我们发现使用 JAX 后端（每秒 6.87 个样本）与使用 PyTorch 后端（每秒 3.01 个样本）之间的运行时性能存在显著（且令人惊讶的）差异。这部分原因在于，JAX
    后端允许将训练批次大小加倍。对这种差异原因的深入探讨超出了本文的范围。
- en: 'As in our previous example, we demonstrate one way of optimizing the PyTorch
    runtime by prepending the following configuration of the [matrix multiplication
    operations](https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html)
    to the top of our script:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前的示例所示，我们通过在脚本顶部添加以下[矩阵乘法操作](https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html)的配置来展示了一种优化
    PyTorch 运行时的方法：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This simple change results in a 29% performance boost when running with the
    PyTorch backend. Once again, we can see the impact of applying framework-specific
    optimizations. The experiment results are summarized in the table below.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的变化在使用 PyTorch 后端时带来了 29% 的性能提升。再次，我们可以看到应用框架特定优化的影响。实验结果总结在下面的表格中。
- en: '![](../Images/5aad8a9425e1f20875979fd728686855.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5aad8a9425e1f20875979fd728686855.png)'
- en: Gemma fine-tuning runtime (by Author)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Gemma 微调运行时（作者提供）
- en: Conclusion
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Our demonstrations have indicated that sticking with the backend agnostic Keras
    code could imply a meaningful runtime performance penalty. In each example, we
    have seen how a simple, framework-specific optimization had a significant impact
    on the relative performance of our chosen backends. At the same time, the arguments
    we have discussed for multi-framework AI/ML development are rather compelling.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的演示表明，坚持使用与后端无关的 Keras 代码可能会带来显著的运行时性能损失。在每个示例中，我们都看到了一个简单的框架特定优化如何对我们选择的后端的相对性能产生显著影响。同时，我们讨论的多框架
    AI/ML 开发的论点也相当有说服力。
- en: If you do choose to adopt Keras as a development framework, you may want to
    consider designing your code in a manner that includes mechanisms for applying
    and assessing framework-specific optimizations. You might also consider designing
    your development process in a way that utilizes Keras during the early stages
    of the project and, as the project matures, optimizes for the one backend that
    is revealed to be the most appropriate.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你选择采用 Keras 作为开发框架，你可能需要考虑以一种设计代码的方式来实现和评估框架特定优化的机制。你还可以考虑在项目的早期阶段使用 Keras，并随着项目的成熟，针对揭示出的最合适的后端进行优化。
- en: Summary
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this post we have explored the new and revised Keras 3 release. No longer
    an appendage to TensorFlow, Keras 3 offers the ability of framework-agnostic AI/ML
    model development. As we discussed, this capability has several significant advantages.
    However, as is often the case in the field of AI development, “there are no free
    lunches” — the added level of abstraction could mean a reduced level of control
    over the inner workings of our code which could imply slower training speed and
    higher costs. The best solution might be one that combines the use of Keras and
    its multi-framework support with dedicated mechanisms for incorporating framework-specific
    modifications.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们探讨了新发布的 Keras 3 版本。Keras 3 不再是 TensorFlow 的附属品，而是提供了框架无关的 AI/ML 模型开发能力。正如我们讨论的，这种能力有几个显著的优势。然而，正如
    AI 开发领域中常见的情况一样，“天下没有免费的午餐”——这种增加的抽象层次可能意味着我们对代码内部工作原理的控制力减弱，这可能导致训练速度变慢和成本增加。最佳的解决方案可能是结合使用
    Keras 及其多框架支持，并采用专门的机制来纳入框架特定的修改。
- en: 'Importantly, the applicability of Keras 3 to your project and the cost-best
    analysis of the investment required, will depend greatly on a wide variety of
    factors including: the target audience, the model deployment process, project
    timelines, and more. Please view this post as a mere introduction into your detailed
    exploration.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，Keras 3 在您的项目中的适用性以及所需投资的性价比分析，将在很大程度上取决于多种因素，包括：目标受众、模型部署过程、项目时间表等。请将此帖子视为您深入探索的简要介绍。
