- en: AI Model Training with JAX
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 JAX 进行 AI 模型训练
- en: 原文：[https://towardsdatascience.com/ai-model-training-with-jax-6e407a7d2dc8?source=collection_archive---------5-----------------------#2024-05-29](https://towardsdatascience.com/ai-model-training-with-jax-6e407a7d2dc8?source=collection_archive---------5-----------------------#2024-05-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/ai-model-training-with-jax-6e407a7d2dc8?source=collection_archive---------5-----------------------#2024-05-29](https://towardsdatascience.com/ai-model-training-with-jax-6e407a7d2dc8?source=collection_archive---------5-----------------------#2024-05-29)
- en: Hit the road to super-fast AI/ML development
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开启超快速的AI/ML开发之路
- en: '[](https://chaimrand.medium.com/?source=post_page---byline--6e407a7d2dc8--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page---byline--6e407a7d2dc8--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6e407a7d2dc8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6e407a7d2dc8--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page---byline--6e407a7d2dc8--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://chaimrand.medium.com/?source=post_page---byline--6e407a7d2dc8--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page---byline--6e407a7d2dc8--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6e407a7d2dc8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6e407a7d2dc8--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page---byline--6e407a7d2dc8--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6e407a7d2dc8--------------------------------)
    ·10 min read·May 29, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6e407a7d2dc8--------------------------------)
    ·10分钟阅读·2024年5月29日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/daf51b59d88858acc3ee4f29b8c04a88.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/daf51b59d88858acc3ee4f29b8c04a88.png)'
- en: Photo by [Matt Foxx](https://unsplash.com/@foxxmd?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Matt Foxx](https://unsplash.com/@foxxmd?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '![](../Images/142d8af6812f5b6fa49f682b50bceed6.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/142d8af6812f5b6fa49f682b50bceed6.png)'
- en: By Author
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 作者
- en: One of the most critical decisions you will need to make in the development
    of AI models is the choice of a machine learning development framework. Over the
    years, many libraries have vied for the lucrative title of “AI developer’s framework
    of choice”. (Remember [Caffe](https://caffe.berkeleyvision.org/) and [Theano](https://github.com/Theano/Theano)?)
    For several years [TensorFlow](https://www.tensorflow.org/) — with its emphasis
    on high-performing, graph-based computation — appeared to be the runaway leader
    (as estimated by the author based on mentions in academic papers and the strength
    of community support). Roughly around the turn of the decade, [PyTorch](https://pytorch.org/)
    — with its user-friendly Pythonic interface — seemed to have become the unquestionable
    queen. However, in recent years a new entrant has quickly grown in popularity
    and can no longer be ignored. With its sights on the coveted crown, [JAX](https://jax.readthedocs.io/en/latest/quickstart.html)
    aims to maximize the performance of AI model training and inference without compromising
    the user experience.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在AI模型开发中，最关键的决策之一就是选择机器学习开发框架。多年来，许多库争夺着“AI开发者首选框架”的这一有利头衔。（还记得 [Caffe](https://caffe.berkeleyvision.org/)
    和 [Theano](https://github.com/Theano/Theano) 吗？）在几年的时间里，[TensorFlow](https://www.tensorflow.org/)——它强调高性能、基于图形的计算——似乎一直是遥不可及的领先者（根据作者对学术论文中提及的次数和社区支持的强度的估计）。大约在十年之交，[PyTorch](https://pytorch.org/)——凭借其用户友好的
    Pythonic 接口——似乎已经成为无可争议的“女王”。然而，近年来一个新兴的框架迅速流行起来，已经无法忽视。瞄准这一宝贵的桂冠，[JAX](https://jax.readthedocs.io/en/latest/quickstart.html)
    旨在最大化 AI 模型训练和推理的性能，同时不妥协用户体验。
- en: In this post we will assess this new framework, demonstrate its use, and share
    some of our own perspectives on its advantages and drawbacks. Importantly, this
    post is *not* intended to be a JAX tutorial. To learn about JAX you are kindly
    referred to the [official documentation](https://jax.readthedocs.io/en/latest/quickstart.html)
    and the many online tutorials on ML development with JAX (e.g., [here](https://github.com/gordicaleksa/get-started-with-JAX)).
    Although our focus will be on AI model training, it should be noted that JAX has
    many additional applications in the AI/ML landscape and beyond. There are several
    high-level ML libraries built on top of JAX. In this post we will use [Flax](https://flax.readthedocs.io/en/latest/quick_start.html)
    which, as of the time of this writing appears to be the most popular.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将评估这一新框架，演示其使用，并分享我们对其优缺点的一些看法。值得注意的是，本文*并非*JAX教程。如果想了解JAX，建议您查阅[官方文档](https://jax.readthedocs.io/en/latest/quickstart.html)以及许多在线教程（例如，[这里](https://github.com/gordicaleksa/get-started-with-JAX)）。尽管我们关注的主要是AI模型训练，但需要注意的是，JAX在AI/ML领域及其他领域还有许多额外的应用。基于JAX构建了几个高级ML库。在本文中，我们将使用[Flax](https://flax.readthedocs.io/en/latest/quick_start.html)，它是截至目前看起来最受欢迎的库。
- en: Thanks to [Ohad Klein](https://www.linkedin.com/in/ohad-klein-947aaa187/?originalSubdomain=il)
    and [Yitzhak Levi](https://www.linkedin.com/in/yitzhak-levi-49a217201/) for their
    contributions to this post.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢[Ohad Klein](https://www.linkedin.com/in/ohad-klein-947aaa187/?originalSubdomain=il)和[Yitzhak
    Levi](https://www.linkedin.com/in/yitzhak-levi-49a217201/)对本文的贡献。
- en: JAX Under the Hood — XLA Compilation
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: JAX背后的秘密——XLA编译
- en: 'Let’s get this out in the open straight away: No disrespect to JAX, the real
    power of JAX comes from its use of [XLA](https://openxla.org/xla) compilation.
    The phenomenal runtime performance demonstrated with JAX, comes from the HW specific
    optimizations enabled by XLA. And many of the features and functionalities often
    associated with JAX, such as just-in-time (JIT) compilation and the “functional
    programming” paradigm, are actually derived from XLA. In fact, XLA compilation
    is hardly unique to JAX, with both [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/config/optimizer/set_jit)
    and [PyTorch](https://pytorch.org/xla/release/2.3/index.html) supporting options
    for using XLA. However, contrary to other popular frameworks, JAX was designed
    from the bottom up to use XLA. This allows for tight coupling of the design and
    implementation of their [JIT](https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html#jax.jit),
    automatic differentiation ([grad](https://jax.readthedocs.io/en/latest/_autosummary/jax.grad.html#jax.grad)),
    vectorization ([vmap](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html#jax.vmap)),
    parallelization ([pmap](https://jax.readthedocs.io/en/latest/_autosummary/jax.pmap.html)),
    sharding ([shard_map](https://jax.readthedocs.io/en/latest/notebooks/shard_map.html)),
    and other features (all of which deserve very much respect), with the underlying
    XLA library. (For contrast, see [this](https://dev-discuss.pytorch.org/t/functionalization-in-pytorch-everything-you-wanted-to-know/965)
    interesting post for a history on the “functionalization” of PyTorch.)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们马上公开一点：对JAX没有任何不尊重的意思，JAX的真正强大之处在于其使用了[XLA](https://openxla.org/xla)编译。JAX所展示的卓越运行时性能，来自于XLA启用的硬件特定优化。而许多与JAX相关的功能特性，比如即时编译（JIT）和“函数式编程”范式，实际上都源自于XLA。事实上，XLA编译并不独特于JAX，[TensorFlow](https://www.tensorflow.org/api_docs/python/tf/config/optimizer/set_jit)和[PyTorch](https://pytorch.org/xla/release/2.3/index.html)都支持使用XLA的选项。然而，与其他流行的框架不同，JAX从底层开始就是为使用XLA而设计的。这使得它可以将其设计与实现紧密结合，包括[JIT](https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html#jax.jit)、自动微分（[grad](https://jax.readthedocs.io/en/latest/_autosummary/jax.grad.html#jax.grad)）、矢量化（[vmap](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html#jax.vmap)）、并行化（[pmap](https://jax.readthedocs.io/en/latest/_autosummary/jax.pmap.html)）、分片（[shard_map](https://jax.readthedocs.io/en/latest/notebooks/shard_map.html)）和其他功能（所有这些都值得非常尊重）与底层的XLA库紧密结合。（作为对比，参见[这篇](https://dev-discuss.pytorch.org/t/functionalization-in-pytorch-everything-you-wanted-to-know/965)有趣的文章，了解PyTorch“功能化”的历史。）
- en: As discussed in [a previous post on the topic](/how-to-accelerate-your-pytorch-training-with-xla-on-aws-3d599bc8f6a9),
    the XLA JIT compiler performs a full analysis of the computation graph associated
    with the model, fuses together the successive tensor operations into single kernels,
    removes redundant graph components, and outputs machine code that is most optimal
    for the underlying accelerator. This results in a reduced number of overall machine
    level operations (FLOPS) per training step, reduced host to accelerator communication
    overhead (e.g., fewer kernels that need to be loaded into the accelerator), reduced
    memory footprint, increased utilization of the dedicated accelerator engines,
    and more.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在 [上一篇文章中讨论的那样](/how-to-accelerate-your-pytorch-training-with-xla-on-aws-3d599bc8f6a9)，XLA
    JIT 编译器对与模型相关的计算图进行全面分析，将连续的张量操作融合为单个内核，移除冗余的图组件，并输出最适合底层加速器的机器码。这导致每个训练步骤所需的机器级操作数（FLOPS）减少，主机与加速器之间的通信开销减少（例如，减少需要加载到加速器中的内核数），内存占用减少，专用加速器引擎的利用率提高，等等。
- en: In addition to the runtime performance optimization, another important feature
    of XLA is its pluggable infrastructure which enables extending its support to
    additional AI accelerators. XLA is a part of the OpenXLA project and is being
    built in collaboration by multiple actors in the field of ML.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 除了运行时性能优化，XLA 的另一个重要特性是其可插拔的基础设施，使得它能够扩展对额外 AI 加速器的支持。XLA 是 OpenXLA 项目的一部分，并正在与机器学习领域的多个参与者合作构建。
- en: At the same time, as detailed in our [previous post](/how-to-accelerate-your-pytorch-training-with-xla-on-aws-3d599bc8f6a9),
    the reliance on XLA also implies some limitations and potential pitfalls. In particular,
    many AI models, including ones with dynamic tensor shapes, may not run optimally
    in XLA. Special care needs to be taken to avoid graph breaks and graph recompilations.
    You should also consider the implications on the debuggability of your code.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，正如我们在 [上一篇文章中详细描述的](/how-to-accelerate-your-pytorch-training-with-xla-on-aws-3d599bc8f6a9)，依赖
    XLA 也意味着一些局限性和潜在的陷阱。特别是，许多 AI 模型，包括具有动态张量形状的模型，可能无法在 XLA 中达到最优运行效果。需要特别小心以避免计算图断裂和图的重新编译。你还应该考虑代码调试性的影响。
- en: JAX In Action — Toy Example
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: JAX 实战 — 简单示例
- en: 'In this section we will demonstrate how to train a toy AI model in JAX on a
    (single) GPU and compare it with PyTorch. Nowadays there are a number of high-level
    ML development platforms that include backends for multiple ML frameworks. This
    allows for comparing the performance of JAX with other frameworks. In this section
    we will use [HuggingFace](https://huggingface.co/)’s [Transformers](https://huggingface.co/docs/transformers/en/index)
    library, which includes PyTorch and JAX implementations of many common Transformer-backed
    models. More specifically, we will define a [Vision Transformer](https://huggingface.co/docs/transformers/en/model_doc/vit)
    (ViT) backed classification model using the [ViTForImageClassification](https://huggingface.co/docs/transformers/v4.41.2/en/model_doc/vit#transformers.ViTForImageClassification)
    and [FlaxViTForImageClassification](https://huggingface.co/docs/transformers/en/model_doc/vit#transformers.FlaxViTForImageClassification)
    modules for the PyTorch and JAX implementations, respectively. The code block
    below contains the model definition:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将演示如何在 JAX 中使用（单个）GPU训练一个简单的 AI 模型，并将其与 PyTorch 进行比较。如今，有许多高层次的机器学习开发平台，包含多个机器学习框架的后端。这使得我们可以将
    JAX 与其他框架的性能进行比较。在本节中，我们将使用 [HuggingFace](https://huggingface.co/) 的 [Transformers](https://huggingface.co/docs/transformers/en/index)
    库，该库包括许多常见的 Transformer 支持的模型的 PyTorch 和 JAX 实现。更具体地说，我们将定义一个基于 [Vision Transformer](https://huggingface.co/docs/transformers/en/model_doc/vit)（ViT）的分类模型，使用
    [ViTForImageClassification](https://huggingface.co/docs/transformers/v4.41.2/en/model_doc/vit#transformers.ViTForImageClassification)
    和 [FlaxViTForImageClassification](https://huggingface.co/docs/transformers/en/model_doc/vit#transformers.FlaxViTForImageClassification)
    模块，分别对应 PyTorch 和 JAX 实现。以下代码块包含了模型定义：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note, that we have chosen to disable the use of [flash attention](https://huggingface.co/docs/text-generation-inference/en/conceptual/flash_attention)
    due to the fact that this optimization is implemented for the PyTorch model only
    (as of the time of this writing).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们选择禁用 [flash attention](https://huggingface.co/docs/text-generation-inference/en/conceptual/flash_attention)
    的使用，因为该优化仅为 PyTorch 模型实现（截至本文撰写时）。
- en: 'Since our interest in this post is in runtime performance, we will train our
    model on a randomly generated dataset. We take advantage of the fact that JAX
    [supports the use of PyTorch dataloaders](https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html#data-loading-with-pytorch):'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在这篇文章中的兴趣集中在运行时性能上，因此我们将使用随机生成的数据集来训练我们的模型。我们利用JAX [支持使用 PyTorch 数据加载器](https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html#data-loading-with-pytorch)的事实：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we define our PyTorch and JAX training loops. The JAX training loop relies
    on a [Flax TrainState](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#train-state)
    object and its definition follows the [basic tutorial](https://flax.readthedocs.io/en/latest/quick_start.html#training-step)
    for training ML models in Flax:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义了 PyTorch 和 JAX 的训练循环。JAX 训练循环依赖于一个 [Flax TrainState](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#train-state)
    对象，其定义遵循 [基本教程](https://flax.readthedocs.io/en/latest/quick_start.html#training-step)，用于在
    Flax 中训练机器学习模型：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let’s now put everything together. In the script below we have included controls
    for using the graph-based JIT compilation options of PyTorch, using [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)
    and [torch_xla](https://pytorch.org/xla/release/2.3/index.html):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将一切整合在一起。在下面的脚本中，我们包含了用于使用基于图的 JIT 编译选项的控制，使用了 [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)
    和 [torch_xla](https://pytorch.org/xla/release/2.3/index.html)：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: An Important Note on Benchmark Comparisons
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于基准比较的重要说明
- en: When analyzing benchmark comparisons, it is of the utmost importance that we
    be extremely meticulous and critical about how they were conducted. This is especially
    true in the case of AI model development where a decision made based on inaccurate
    data could have extremely expensive repercussions. When comparing the runtime
    performance of training models there are a number of factors that can have a dominating
    effect on our measurements including floating type precision, matrix multiplication
    (matmul) precision, data loading methods, the use of flash/fused attention, etc.
    For example, if the default matmul precision is float32 in PyTorch and tensorfloat32
    in JAX, we cannot learn much from their performance comparison. These settings
    can be controlled via APIs such as [jax.default_matmul_precision](https://jax.readthedocs.io/en/latest/_autosummary/jax.default_matmul_precision.html)
    and [torch.set_float32_matmul_precision](https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html).
    In our script we have attempted to isolate these kinds of potential issues, but
    do not offer any guarantee that we have, in fact, succeeded.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析基准比较时，至关重要的是我们要对它们的执行过程非常谨慎和批判。尤其是在 AI 模型开发的情况下，基于不准确数据做出的决策可能会带来极其昂贵的后果。在比较训练模型的运行时性能时，有许多因素可能会对我们的测量产生主导作用，包括浮点数精度、矩阵乘法（matmul）精度、数据加载方法、是否使用闪存/融合注意力等。例如，如果
    PyTorch 的默认矩阵乘法精度是 float32，而 JAX 是 tensorfloat32，那么我们无法从它们的性能比较中获得太多有价值的信息。这些设置可以通过像
    [jax.default_matmul_precision](https://jax.readthedocs.io/en/latest/_autosummary/jax.default_matmul_precision.html)
    和 [torch.set_float32_matmul_precision](https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html)
    这样的 API 来控制。在我们的脚本中，我们尝试将这些潜在问题隔离开来，但并不能保证我们已经成功解决了所有问题。
- en: Results
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: We ran our training script on two Google Cloud VMs, a [g2-standard-16](https://cloud.google.com/compute/docs/gpus#l4-gpus)
    VM (with a single NVIDIA L4 GPU) and an [a2-highgpu-1g](https://cloud.google.com/compute/docs/gpus#a100-gpus)
    (with a single NVIDIA A100 GPU) , In each case we used a dedicated [deep learning
    VM image](https://cloud.google.com/deep-learning-vm/docs/release-notes) (common-cu121-v20240514-ubuntu-2204-py310)
    with installations of PyTorch (2.3.0), PyTorch/XLA (2.3.0), JAX (0.4.28), Flax
    (0.8.4), Optax (0.2.2), and [HuggingFace](https://huggingface.co/)’s [Transformers](https://huggingface.co/docs/transformers/en/index)
    library (4.41.1). Please see the official documentation for appropriate installation
    of [JAX](https://jax.readthedocs.io/en/latest/installation.html) and [PyTorch/XLA](https://github.com/pytorch/xla?tab=readme-ov-file#python-packages)
    for GPU.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两台Google Cloud虚拟机上运行了训练脚本，一台是[g2-standard-16](https://cloud.google.com/compute/docs/gpus#l4-gpus)虚拟机（配备单个NVIDIA
    L4 GPU），另一台是[a2-highgpu-1g](https://cloud.google.com/compute/docs/gpus#a100-gpus)虚拟机（配备单个NVIDIA
    A100 GPU）。在每种情况下，我们都使用了一个专用的[深度学习虚拟机镜像](https://cloud.google.com/deep-learning-vm/docs/release-notes)（common-cu121-v20240514-ubuntu-2204-py310），并安装了PyTorch（2.3.0）、PyTorch/XLA（2.3.0）、JAX（0.4.28）、Flax（0.8.4）、Optax（0.2.2）以及[HuggingFace](https://huggingface.co/)的[Transformers](https://huggingface.co/docs/transformers/en/index)库（4.41.1）。有关GPU环境下[JAX](https://jax.readthedocs.io/en/latest/installation.html)和[PyTorch/XLA](https://github.com/pytorch/xla?tab=readme-ov-file#python-packages)的正确安装方法，请参阅官方文档。
- en: The tables below capture the runtime results of a number of experiments. Please
    keep in mind that the comparative performance is likely to change drastically
    based on the model architecture and runtime environment. In addition, it is quite
    possible that a few small tweaks to the code could also have had a measurable
    impact on the results.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格展示了多个实验的运行时结果。请记住，比较性能可能会根据模型架构和运行环境发生显著变化。此外，代码的少量修改也有可能对结果产生可测量的影响。
- en: '![](../Images/701a8ba50a631a6814a29d74ceebcf5f.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/701a8ba50a631a6814a29d74ceebcf5f.png)'
- en: Results on NVIDIA L4 GPU (by Author)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA L4 GPU上的结果（作者提供）
- en: '![](../Images/0e1ddbccc2699b93c0a3a6b7fec44d24.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e1ddbccc2699b93c0a3a6b7fec44d24.png)'
- en: Results on NVIDIA A100 GPU (by Author)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA A100 GPU上的结果（作者提供）
- en: Although JAX appears to have demonstrated far superior performance than its
    alternatives on an L4 GPU, it came out neck-in-neck with PyTorch/XLA on A100\.
    This is not surprising given the common XLA backend. Any XLA (HLO) graph generated
    by JAX should (at least in theory) be achievable by PyTorch/XLA as well. The torch.compile
    option underwhelmed on both platforms. This is somewhat expected given our choice
    of full precision floats. As noted in a [previous post](/tips-and-tricks-for-upgrading-to-pytorch-2-3127db1d1f3d),
    the true value of torch.compile is seen when using [Automatic Mixed Precision
    (AMP)](https://pytorch.org/docs/stable/amp.html).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管JAX在L4 GPU上表现出比其替代方案更优越的性能，但在A100上它与PyTorch/XLA的表现不相上下。考虑到共同的XLA后端，这并不令人惊讶。JAX生成的任何XLA（HLO）图应该（至少在理论上）也能由PyTorch/XLA实现。torch.compile选项在两个平台上的表现都不尽如人意。鉴于我们选择了全精度浮动数，这在一定程度上是可以预见的。正如[上一篇文章](/tips-and-tricks-for-upgrading-to-pytorch-2-3127db1d1f3d)所述，torch.compile的真正价值体现在使用[自动混合精度（AMP）](https://pytorch.org/docs/stable/amp.html)时。
- en: For additional information on the performance comparison between JAX and PyTorch,
    be sure to check out the more comprehensive benchmark reports compiled by [HuggingFace](https://github.com/huggingface/transformers/tree/main/examples/flax/text-classification#runtime-evaluation),
    [Google](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/community-content/vertex_model_garden/benchmarking_reports/jax_vit_benchmarking_report.md),
    or [MLCommons](https://mlcommons.org/benchmarks/training/).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多JAX和PyTorch之间性能比较的详细信息，请务必查看[HuggingFace](https://github.com/huggingface/transformers/tree/main/examples/flax/text-classification#runtime-evaluation)、[Google](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/community-content/vertex_model_garden/benchmarking_reports/jax_vit_benchmarking_report.md)或[MLCommons](https://mlcommons.org/benchmarks/training/)编写的更全面的基准报告。
- en: So Why Use JAX?
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 那么，为什么使用JAX？
- en: 'A commonly stated motivation for training in JAX is the potential runtime performance
    optimization enabled by JIT compilation. But, given the new (PyTorch/XLA) and
    even newer (torch.compile) JIT compilation options in PyTorch, this claim could
    easily be challenged. In fact, considering the huge community of PyTorch developers
    and the numerous features that are natively supported in PyTorch and not in JAX/FLAX
    (e.g., [automatic mixed precision](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html),
    [advanced attention layers](https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html#beta-implementing-high-performance-transformers-with-scaled-dot-product-attention-sdpa),
    as of the time of this writing), one could make a strong argument *not* to take
    the time to learn JAX. However, it is our opinion that modern-day AI development
    teams must acquaint themselves with JAX and the opportunities that it offers.
    This is especially true for teams that are (like us) obsessive about utilizing
    the very latest and greatest available model training methodologies. On top of
    the potential performance benefits, here are some additional motivating factors:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在 JAX 中进行训练的一个常见动机是 JIT 编译所带来的潜在运行时性能优化。但是，考虑到 PyTorch 中的新的（PyTorch/XLA）和更新的（torch.compile）JIT
    编译选项，这一说法很容易受到挑战。事实上，考虑到 PyTorch 拥有庞大的开发者社区以及 PyTorch 中原生支持的众多功能，而 JAX/FLAX 并不支持（例如，[自动混合精度](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html)，[高级注意力层](https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html#beta-implementing-high-performance-transformers-with-scaled-dot-product-attention-sdpa)，截至本文撰写时），有人可能会强烈主张*不*值得花时间学习
    JAX。然而，我们认为现代 AI 开发团队必须熟悉 JAX 及其提供的机会。这对于那些像我们一样，执着于利用最新、最强大的模型训练方法的团队尤为重要。除了潜在的性能收益外，以下是一些额外的激励因素：
- en: Designed for XLA
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为 XLA 设计
- en: Contrary to PyTorch which underwent after-the-fact “functionalization” in the
    form of PyTorch/XLA, JAX was designed for XLA from the ground up. This implies
    that certain sequences that may appear difficult or messy in PyTorch/XLA can be
    done elegantly in JAX. A good example of this is mixing between JIT and non-JIT
    functions in your training sequence — totally straightforward in JAX but may require
    some creativity in PyTorch/XLA.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 与经过后期“功能化”的 PyTorch/XLA 相对，JAX 从一开始就是为 XLA 设计的。这意味着某些在 PyTorch/XLA 中可能显得复杂或混乱的操作，在
    JAX 中可以优雅地完成。一个很好的例子就是在训练序列中混合使用 JIT 和非 JIT 函数——在 JAX 中非常简单，但在 PyTorch/XLA 中可能需要一些创造性处理。
- en: As noted above, PyTorch/XLA and TensorFlow could — in theory — generate an XLA
    (HLO) graph that is identical to the one created by JAX (and therefore be equally
    performant). However, in practice the quality of the resulting graph will come
    down to the manner in which the framework-level implementation is translated into
    XLA. A more optimal translation will ultimately result in better runtime performance.
    Given its nativity to XLA, JAX could have the advantage over other frameworks.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，理论上 PyTorch/XLA 和 TensorFlow 可以生成一个与 JAX 创建的 XLA（HLO）图相同的图（因此也能实现相同的性能）。然而，实际上，生成的图的质量将取决于框架级实现如何转换为
    XLA。更优的转换最终将导致更好的运行时性能。由于 JAX 本身对 XLA 的原生支持，它可能相对于其他框架具有优势。
- en: Support for XLA Devices
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持 XLA 设备
- en: The XLA-friendliness of JAX makes it especially compelling to developers of
    dedicated-AI accelerators, such as the [Google Cloud TPU](https://lightning.ai/docs/pytorch/stable/accelerators/tpu.html),
    [Intel](https://developer.habana.ai/) [Gaudi](https://developer.habana.ai/), and
    [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/) chips, which
    are often exposed as “XLA devices”. Teams that train on TPU, in particular, are
    likely to find the support ecosystem for JAX to be more advanced than for PyTorch/XLA.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: JAX 的 XLA 兼容性使其特别吸引那些开发专用 AI 加速器的开发者，例如 [Google Cloud TPU](https://lightning.ai/docs/pytorch/stable/accelerators/tpu.html)、[Intel](https://developer.habana.ai/)
    [Gaudi](https://developer.habana.ai/) 和 [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/)
    芯片，这些芯片通常被称为“XLA 设备”。特别是那些在 TPU 上进行训练的团队，可能会发现 JAX 的支持生态系统比 PyTorch/XLA 更加先进。
- en: Advanced Features
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级功能
- en: In recent years, there have been a number of advanced training features that
    have been released in JAX well before its counterparts. [SPMD](https://jax.readthedocs.io/en/latest/sharded-computation.html),
    for example, an advanced technique for device parallelism offering state-of-the-art
    model sharding opportunities, was introduced in JAX a couple of years ago and
    is only recently being carried over to [PyTorch](https://pytorch.org/blog/pytorch-xla-spmd/).
    Another example is [Pallas](https://jax.readthedocs.io/en/latest/pallas/index.html)
    which (at long last) enables building custom kernels for XLA devices.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，JAX在发布一些先进的训练功能方面，比其他框架早了很多。例如，[SPMD](https://jax.readthedocs.io/en/latest/sharded-computation.html)，这是一种先进的设备并行技术，提供了最先进的模型分片机会，几年前就在JAX中推出，最近才被引入到[PyTorch](https://pytorch.org/blog/pytorch-xla-spmd/)。另一个例子是[Palas](https://jax.readthedocs.io/en/latest/pallas/index.html)，它（终于）使得可以为XLA设备构建自定义内核。
- en: Open Source Models
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开源模型
- en: As a consequence of the increasing popularity of the JAX framework, more and
    more open-source AI models are being released in JAX. Some classic examples of
    this are Google’s open-sourced [MaxText](https://github.com/google/maxtext/) (LLM)
    and [AlphaFold v2](https://github.com/google-deepmind/alphafold) (protein-structure
    prediction) models. To take full advantage of such models, you will need to either
    learn JAX, or undertake the non-trivial task of porting it to another language.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 随着JAX框架日益流行，越来越多的开源AI模型开始以JAX的形式发布。一些经典的例子包括谷歌开源的[MaxText](https://github.com/google/maxtext/)（大语言模型）和[AlphaFold
    v2](https://github.com/google-deepmind/alphafold)（蛋白质结构预测）模型。要充分利用这些模型，你需要学习JAX，或者承担将其移植到其他语言的复杂任务。
- en: It is our strong belief that these considerations warrant the inclusion of JAX
    in any ML development toolkit.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们坚信，这些考虑因素值得将JAX纳入任何机器学习开发工具包。
- en: Summary
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this post we have explored the up-and-coming JAX ML development framework.
    We described its reliance on the XLA compiler and demonstrated its use in a toy
    example. Although often noted for its speedy runtime execution, the PyTorch JIT
    compilation APIs ([torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)
    and [PyTorch/XLA](https://pytorch.org/xla/release/2.3/index.html)) support similar
    potential for performance optimization. The relative performance of each option
    will depend greatly on the details of the model and the runtime environment.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 本文我们探讨了崭露头角的JAX机器学习开发框架。我们描述了它依赖于XLA编译器，并展示了在一个简单例子中的使用。尽管PyTorch的JIT编译API（[torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)和[PyTorch/XLA](https://pytorch.org/xla/release/2.3/index.html)）也支持类似的性能优化潜力，但JAX通常因其快速的运行时执行而被提及。每种选项的相对性能将大大依赖于模型的细节和运行时环境。
- en: Importantly, each ML development framework option might have unique features,
    (such as SPMD auto-sharding in JAX and SDPA attention in PyTorch — as of the time
    of this writing) that can have a decisive impact on the comparative runtime performance.
    Thus, the best choice of framework may depend on the degree to which your model
    can benefit from these features.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，每个机器学习（ML）开发框架选项可能具有独特的功能（例如，JAX中的SPMD自动分片和PyTorch中的SDPA注意力——截至本文写作时），这些功能可能对比较运行时性能产生决定性影响。因此，框架的最佳选择可能取决于你的模型能从这些功能中获益的程度。
- en: In conclusion, as we have emphasized in many of our [previous posts](https://chaimrand.medium.com/),
    staying relevant in the constantly evolving landscape of ML development requires
    us to stay abreast of the most up-to-date tools and techniques, including the
    JAX ML development framework.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，正如我们在许多[之前的文章](https://chaimrand.medium.com/)中强调的那样，要在不断发展的机器学习开发领域中保持相关性，就需要紧跟最新的工具和技术，包括JAX机器学习开发框架。
