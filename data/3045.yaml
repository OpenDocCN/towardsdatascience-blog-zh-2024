- en: 'A New Approach to AI Safety: Layer Enhanced Classification (LEC)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一种新的AI安全方法：层增强分类（LEC）
- en: 原文：[https://towardsdatascience.com/a-new-approach-to-ai-safety-layer-enhanced-classification-lec-56141aa0f6be?source=collection_archive---------0-----------------------#2024-12-20](https://towardsdatascience.com/a-new-approach-to-ai-safety-layer-enhanced-classification-lec-56141aa0f6be?source=collection_archive---------0-----------------------#2024-12-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-new-approach-to-ai-safety-layer-enhanced-classification-lec-56141aa0f6be?source=collection_archive---------0-----------------------#2024-12-20](https://towardsdatascience.com/a-new-approach-to-ai-safety-layer-enhanced-classification-lec-56141aa0f6be?source=collection_archive---------0-----------------------#2024-12-20)
- en: '***LEC surpasses best in class models, like GPT-4o, by combining the efficiency
    of a ML classifier with the language understanding of an LLM***'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '***LEC通过将机器学习分类器的高效性与LLM的语言理解结合，超越了像GPT-4o这样的顶级模型***'
- en: '[](https://medium.com/@sandibesen?source=post_page---byline--56141aa0f6be--------------------------------)[![Sandi
    Besen](../Images/97361d97f50269f70b6621da2256bc29.png)](https://medium.com/@sandibesen?source=post_page---byline--56141aa0f6be--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--56141aa0f6be--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--56141aa0f6be--------------------------------)
    [Sandi Besen](https://medium.com/@sandibesen?source=post_page---byline--56141aa0f6be--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@sandibesen?source=post_page---byline--56141aa0f6be--------------------------------)[![Sandi
    Besen](../Images/97361d97f50269f70b6621da2256bc29.png)](https://medium.com/@sandibesen?source=post_page---byline--56141aa0f6be--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--56141aa0f6be--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--56141aa0f6be--------------------------------)
    [Sandi Besen](https://medium.com/@sandibesen?source=post_page---byline--56141aa0f6be--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--56141aa0f6be--------------------------------)
    ·8 min read·Dec 20, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--56141aa0f6be--------------------------------)
    ·8分钟阅读·2024年12月20日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'Imagine sitting in a boardroom, discussing the most transformative technology
    of our time — artificial intelligence — and realizing we’re riding a rocket with
    no reliable safety belt. The Bletchley Declaration, unveiled during the AI Safety
    Summit hosted by the UK government and backed by 29 countries, captures this sentiment
    perfectly [1]:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你正坐在会议室里，讨论着我们这个时代最具变革性的技术——人工智能，并意识到我们正乘坐一艘没有可靠安全带的火箭。由英国政府主办并得到29个国家支持的AI安全峰会上发布的《布莱奇利声明》完美地捕捉了这一情感[1]：
- en: “There is potential for serious, even catastrophic, harm, either deliberate
    or unintentional, stemming from the most significant capabilities of these AI
    models”.
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “这些AI模型最强大的能力可能会带来严重甚至灾难性的伤害，无论是故意的还是无意的。”
- en: '![](../Images/4884f6a80146d35b7392d166238c9051.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4884f6a80146d35b7392d166238c9051.png)'
- en: 'Source: Dalle3'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '来源: Dalle3'
- en: However, **existing AI safety approaches force organizations into an un-winnable
    trade-off between cost, speed, and accuracy**. Traditional machine learning classifiers
    struggle to capture the subtleties of natural language and LLM’s, while powerful,
    introduce significant computational overhead — requiring additional model calls
    that escalate costs for each AI safety check.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，**现有的AI安全方法迫使组织在成本、速度和准确性之间做出一个无法取胜的权衡**。传统的机器学习分类器难以捕捉自然语言和LLM的微妙差异，而强大的LLM引入了巨大的计算开销——需要额外的模型调用，这导致每个AI安全检查的成本上升。
- en: Our team ([Mason Sawtell](https://www.linkedin.com/in/mason-sawtell/), [Sandi
    Besen](https://www.linkedin.com/in/sandibesen/), [Tula Masterman](https://www.linkedin.com/in/tula-masterman/),
    [Jim Brown](https://www.linkedin.com/in/jim-brown-71427356/)), introduces a novel
    approach called LEC (Layer Enhanced Classification).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的团队（[Mason Sawtell](https://www.linkedin.com/in/mason-sawtell/)、[Sandi Besen](https://www.linkedin.com/in/sandibesen/)、[Tula
    Masterman](https://www.linkedin.com/in/tula-masterman/)、[Jim Brown](https://www.linkedin.com/in/jim-brown-71427356/)）提出了一种名为LEC（层增强分类）的新方法。
- en: '![](../Images/264f7de5efe28359fc11be79b7459fd4.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/264f7de5efe28359fc11be79b7459fd4.png)'
- en: 'Image by : Sandi Besen, Tula Masterman, Mason Sawtell, Jim Brown'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自：Sandi Besen, Tula Masterman, Mason Sawtell, Jim Brown
- en: '***We prove LEC combines the computational efficiency of a machine learning
    classifier with the sophisticated language understanding of an LLM — so you don’t
    have to choose between cost, speed, and accuracy. LEC surpasses best in class
    models like GPT-4o and models specifically trained for identifying unsafe content
    and prompt injections. What’s better yet, we believe LEC can be modified to tackle
    non AI safety related text classification tasks like sentiment analysis, intent
    classification, product categorization, and more.***'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '***我们证明 LEC 结合了机器学习分类器的计算效率和 LLM 的复杂语言理解——因此你不需要在成本、速度和准确性之间做出选择。LEC 超越了像 GPT-4o
    这样的顶尖模型以及专门训练用于识别不安全内容和提示注入的模型。更棒的是，我们相信 LEC 可以被修改以应对非 AI 安全相关的文本分类任务，如情感分析、意图分类、产品分类等。***'
- en: The implications are profound. Whether you’re a technology leader navigating
    the complex terrain of AI safety, a product manager mitigating potential risks,
    or an executive charting a responsible innovation strategy, our approach offers
    a scalable and adaptable solution.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这些影响是深远的。无论你是一个在 AI 安全复杂领域中航行的技术领导者，一个在缓解潜在风险的产品经理，还是一个在制定负责任创新战略的高管，我们的方法都提供了一个可扩展且适应性强的解决方案。
- en: '![](../Images/bc6f73cbbb673ecbddbc6b8e358e15c8.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc6f73cbbb673ecbddbc6b8e358e15c8.png)'
- en: 'Figure 1: An example of an adapted model inference pipeline to include LEC
    Classifiers. Image by : Sandi Besen, Tula Masterman, Mason Sawtell, Jim Brown'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：一个调整后的模型推理管道示例，包含 LEC 分类器。图片来源：Sandi Besen, Tula Masterman, Mason Sawtell,
    Jim Brown
- en: Further details can be found in the [full paper](https://arxiv.org/abs/2412.13435)’s
    pre-print on Arxiv[2] or in [Tula Masterman’s summarized article](/introducing-layer-enhanced-classification-lec-4972f4f1c79f)
    about the paper.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步的细节可以在 [完整论文](https://arxiv.org/abs/2412.13435)的预印本中找到，或者在 [Tula Masterman
    的总结文章](/introducing-layer-enhanced-classification-lec-4972f4f1c79f) 中了解更多有关论文的信息。
- en: Applying LEC to Responsible AI Use Cases
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 LEC 应用于负责任的 AI 使用案例
- en: Responsible AI has become a critical priority for technology leaders across
    the ecosystem — from model developers like Anthropic, OpenAI, Meta, Google, and
    IBM to enterprise consulting firms and AI service providers. As AI adoption accelerates,
    its importance becomes even more pronounced.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的 AI 已成为整个技术生态系统中领导者的关键优先事项——从模型开发者如 Anthropic、OpenAI、Meta、Google 和 IBM，到企业咨询公司和
    AI 服务提供商。随着 AI 采纳的加速，其重要性变得更加突出。
- en: Our research specifically targets two pivotal challenges in AI safety — content
    safety and prompt injection detection. Content safety refers to the process of
    identifying and preventing the generation of harmful, inappropriate, or potentially
    dangerous content that could pose risks to users or violate ethical guidelines.
    Prompt injection involves detecting attempts to manipulate AI systems by crafting
    input prompts designed to bypass safety mechanisms or coerce the model into producing
    unethical outputs.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究特别针对 AI 安全中的两个关键挑战——内容安全和提示注入检测。内容安全指的是识别和防止生成有害、不当或可能对用户造成危险的内容，这些内容可能会带来风险或违反伦理指南。提示注入涉及检测通过构造输入提示来操控
    AI 系统的尝试，目的是绕过安全机制或迫使模型生成不道德的输出。
- en: To advance the field of ethical AI, we applied LEC’s capabilities to real-world
    responsible AI use cases. Our hope is that this methodology will be adopted widely,
    helping to make every AI system less vulnerable to exploitation.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了推动伦理 AI 领域的发展，我们将 LEC 的能力应用于现实世界中的负责任 AI 使用案例。我们的希望是，这种方法能够广泛采用，帮助让每个 AI 系统都不那么容易受到剥削。
- en: Using LEC for Content Safety Tasks
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 LEC 进行内容安全任务
- en: We curated a content safety dataset of 5,000 examples to test LEC on both binary
    (2 categories) and multi-class (>2 categories) classification. We used the SALAD
    Data dataset from OpenSafetyLab [3] to represent unsafe content and the “LMSYS-Chat-1M”
    dataset from LMSYS, to represent safe content [4].
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们策划了一个包含 5,000 个示例的内容安全数据集，用于在二分类（2 类）和多分类（>2 类）任务中测试 LEC。我们使用了 OpenSafetyLab
    的 SALAD Data 数据集[3]来代表不安全内容，并使用了 LMSYS 的“LMSYS-Chat-1M”数据集来代表安全内容[4]。
- en: For binary classification the content is either “safe” or “unsafe”. For multi-class
    classification, content is either categorized as “safe” or assigned to a specific
    specific “unsafe” category.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二分类任务，内容要么是“安全的”，要么是“不安全的”。对于多分类任务，内容要么被分类为“安全的”，要么被分配到一个特定的“不安全”类别。
- en: We compared model’s trained using LEC to GPT-4o (widely recognized as an industry
    leader), Llama Guard 3 1B and Llama Guard 3 8B (special purpose models specifically
    trained to tackle content safety tasks). We found that the models using LEC outperformed
    all models we compared them to using as few as 20 training examples for binary
    classification and 50 training examples for multi-class classification.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用LEC训练的模型与GPT-4o（广泛被认为是行业领先者）、Llama Guard 3 1B和Llama Guard 3 8B（专门训练来处理内容安全任务的特殊目的模型）进行了比较。我们发现，使用LEC的模型在训练样本仅为20个的二分类任务和50个的多分类任务时，超越了我们比较的所有模型。
- en: The highest performing LEC model achieved a weighted F1 score (measures how
    well a system balances making correct predictions while minimizing mistakes) of
    .96 of a maximum score of 1 on the binary classification task compared to GPT-4o’s
    score of 0.82 or LlamaGuard 8B’s score of 0.71.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 表现最佳的LEC模型在二分类任务中达到了加权F1分数0.96（满分1），相比之下，GPT-4o的分数为0.82，LlamaGuard 8B的分数为0.71。
- en: '**This means that with as few as 15 examples, using LEC you can train a model
    to outperform industry leaders in identifying safe or unsafe content at a fraction
    of the computational cost.**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**这意味着，仅需15个示例，使用LEC你就能训练出一个模型，在识别安全或不安全内容时，超越行业领导者，而且计算成本极低。**'
- en: '![](../Images/bf71c4cd0d97dd59a6a287aad68f9155.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bf71c4cd0d97dd59a6a287aad68f9155.png)'
- en: 'Summary of Content safety Results. Image by : Sandi Besen, Tula Masterman,
    Mason Sawtell, Jim Brown'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 内容安全结果总结。图片来源：Sandi Besen、Tula Masterman、Mason Sawtell、Jim Brown
- en: Using LEC for Identifying Prompt Injections
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LEC识别提示注入
- en: We curated a prompt injection dataset using the SPML Chatbot Prompt Injection
    Dataset. We chose the SPML dataset because of its diversity and complexity in
    representing real-world chat bot scenarios. This dataset contained pairs of system
    and user prompts to identify user prompts that attempt to defy or manipulate the
    system prompt. This is especially relevant for businesses deploying public facing
    chatbots that are only meant to answer questions about specific domains.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用SPML Chatbot提示注入数据集策划了一个提示注入数据集。我们选择SPML数据集是因为它在展示真实世界聊天机器人场景时具有多样性和复杂性。该数据集包含了一对系统和用户的提示，用于识别那些试图违抗或操控系统提示的用户提示。这对那些部署面向公众的聊天机器人、仅用于回答特定领域问题的企业尤其相关。
- en: We compared model’s trained using LEC to GPT-4o (an industry leader) and deBERTa
    v3 Prompt Injection v2 (a model specifically trained to identify prompt injections).
    We found that the models using LEC outperformed both GPT-4o using 55 training
    examples and the the special purpose model using as few as 5 training examples.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用LEC训练的模型与GPT-4o（行业领先者）以及deBERTa v3 Prompt Injection v2（专门训练用以识别提示注入的模型）进行了比较。我们发现，使用LEC的模型在训练样本为55个的GPT-4o和训练样本仅为5个的特殊目的模型面前表现更优。
- en: The highest performing LEC model achieved a weighted F1 score of .98 of a maximum
    score of 1 compared to GPT-4o’s score of 0.92 or deBERTa v2 Prompt Injection v2’s
    score of 0.73.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 表现最佳的LEC模型在多分类任务中达到了加权F1分数0.98（满分1），相比之下，GPT-4o的分数为0.92，deBERTa v2 Prompt Injection
    v2的分数为0.73。
- en: '**This means that with as few as 5 examples, using LEC you can train a model
    to outperform industry leaders in identifying prompt injection attacks.**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**这意味着，仅需5个示例，使用LEC你就能训练出一个模型，在识别提示注入攻击时超越行业领导者。**'
- en: '![](../Images/8c579a1dc0fb1d4291fe30cd2d7bc05a.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c579a1dc0fb1d4291fe30cd2d7bc05a.png)'
- en: 'Summary of Prompt Injection Results. Image by : Sandi Besen, Tula Masterman,
    Mason Sawtell, Jim Brown'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 提示注入结果总结。图片来源：Sandi Besen、Tula Masterman、Mason Sawtell、Jim Brown
- en: Full results and experimentation implementation details can be found in the
    Arxiv preprint.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的结果和实验实现细节可以在Arxiv预印本中找到。
- en: How Your Business Can Benefit From using LEC
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你的企业如何从使用LEC中受益
- en: As organizations increasingly integrate AI into their operations, ensuring the
    safety and integrity of AI-driven interactions has become mission-critical. LEC
    provides a robust and flexible way to ensure that potentially unsafe information
    is being detected — resulting in reduce operational risk and increased end user
    trust. There are several ways that a LEC models can be incorporated into your
    AI Safety Toolkit to prevent unwanted vulnerabilities when using your AI tools
    including during LM inference, before/after LM inference, and even in multi-agent
    scenarios.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 随着组织越来越多地将AI融入其业务运营，确保AI驱动交互的安全性和完整性已经成为关键任务。LEC提供了一种强大且灵活的方式来确保潜在的不安全信息被检测到——从而减少操作风险并提高最终用户的信任度。LEC模型可以通过多种方式融入到您的AI安全工具包中，以防止在使用AI工具时出现不必要的漏洞，包括在语言模型推理过程中、推理前/后，甚至在多代理场景中。
- en: '**During LM Inference**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**语言模型推理过程中**'
- en: If you are using an open-source model or have access to the inner workings of
    the closed-source model, you can use LEC as part of your inference pipeline for
    AI safety in near real time. This means that if any safety concerns arise while
    information is traveling through the language model, generation of any output
    can be halted. An example of what this might look like can be seen in figure 1.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用开源模型或能够访问闭源模型的内部工作原理，可以将LEC作为推理流水线的一部分，实时保障AI安全。这意味着，如果在信息通过语言模型时出现任何安全问题，可以立即停止输出的生成。图1展示了这一过程的示例。
- en: '**Before / After LM Inference**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**语言模型推理前/后**'
- en: If you don’t have access to the inner workings of the language model or want
    to check for safety concerns as a separate task you can use a LEC model before
    or after calling a language model. This makes LEC compatible with closed source
    models like the Claude and GPT families.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您无法访问语言模型的内部工作原理，或者希望将安全检查作为单独的任务执行，您可以在调用语言模型之前或之后使用LEC模型。这使得LEC能够与像Claude和GPT这样的闭源模型兼容。
- en: Building a LEC Classifier into your deployment pipeline can save you from passing
    potentially harmful content into your LM and/or check for harmful content before
    an output is returned to the user.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 将LEC分类器集成到您的部署流水线中，可以避免将潜在有害的内容传递到您的语言模型中，并/或在输出返回给用户之前检查有害内容。
- en: '**Using LEC Classifiers with Agents**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用LEC分类器与代理**'
- en: 'Agentic AI systems can amplify any existing unintended actions, leading to
    a compounding effect of unintended consequences. LEC Classifiers can be used at
    different times throughout an agentic scenario to can safeguard the agent from
    either receiving or producing harmful outputs. For instance, by including LEC
    models into your agentic architecture you can:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 代理型AI系统可能会放大任何现有的非预期行为，导致不良后果的累积效应。LEC分类器可以在代理场景的不同阶段使用，以保护代理免受接收或生成有害输出的风险。例如，通过将LEC模型集成到您的代理架构中，您可以：
- en: Check that the request is ok to start working on
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查请求是否可以开始处理
- en: Ensure an invoked tool call does not violate any AI safety guidelines (e.g.,
    generating inappropriate search topics for a keyword search)
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保调用的工具不会违反任何AI安全准则（例如，为关键字搜索生成不适当的搜索主题）
- en: Make sure information returned to an agent is not harmful (e.g., results returned
    from RAG search or google search are “safe”)
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保返回给代理的信息无害（例如，从RAG搜索或谷歌搜索返回的结果是“安全的”）
- en: Validating the final response of an agent before passing it back to the user
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在将代理的最终响应返回给用户之前进行验证
- en: '**How to Implement LEC Based on Language Model Access**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**如何基于语言模型访问实现LEC**'
- en: Enterprises with access to the internal workings of models can integrate LEC
    directly within the inference pipeline, enabling continuous safety monitoring
    throughout the AI’s content generation process. When using closed-source models
    via API (as is the case with GPT-4), businesses do not have direct access to the
    underlying information needed to train a LEC model. In this scenario, LEC can
    be applied before and/or after model calls. For example, before an API call, the
    input can be screened for unsafe content. Post-call, the output can be validated
    to ensure it aligns with business safety protocols.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有访问模型内部工作的企业可以直接在推理流水线中集成LEC，从而在AI内容生成过程中实现持续的安全监控。当通过API使用闭源模型（如GPT-4）时，企业无法直接访问训练LEC模型所需的底层信息。在这种情况下，可以在模型调用之前和/或之后应用LEC。例如，在API调用之前，可以筛查输入是否包含不安全内容；调用后，可以验证输出是否符合业务安全协议。
- en: '***No matter which way you choose to implement LEC, using its powerful abilities
    provides you with superior content safety and prompt injection protection than
    existing techniques at a fraction of the time and cost.***'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '***无论你选择以何种方式实现LEC，使用其强大能力都能在比现有技术更短的时间和更低的成本下，为你提供更优的内容安全性和提示注入防护。***'
- en: Conclusion
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Layer Enhanced Classification (LEC) is the safety belt for that AI rocket ship
    we’re on.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 层级增强分类（LEC）是我们所乘坐的人工智能火箭船的安全带。
- en: 'The value proposition is clear: LEC’s AI Safety models can mitigate regulatory
    risk, help ensure brand protection, and enhance user trust in AI-driven interactions.
    It signals a new era of AI development where accuracy, speed, and cost aren’t
    competing priorities and AI safety measures can be addressed both at inference
    time, before inference time, or after inference time.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 价值主张非常明确：LEC的人工智能安全模型可以减轻监管风险，帮助确保品牌保护，并增强用户对人工智能驱动互动的信任。它标志着人工智能发展的新时代，在这个时代，准确性、速度和成本不再是相互竞争的优先事项，人工智能安全措施可以在推理时、推理前或推理后进行处理。
- en: In our content safety experiments, the highest performing **LEC model achieved
    a weighted F1 score of 0.96** out of 1 on binary classification, **significantly
    outperforming GPT-4o’s score** of 0.82 **and LlamaGuard 8B’s score** of 0.71 —
    and this was accomplished **with as few as 15 training examples**. Similarly,
    in prompt injection detection, **our top LEC model reached a weighted F1 score
    of 0.98, compared to GPT-4o’s 0.92** and deBERTa v2 Prompt Injection v2’s 0.73,
    and it was achieved with just 55 training examples. **These results not only demonstrate
    superior performance, but also highlight LEC’s remarkable ability to achieve high
    accuracy with minimal training data.**
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的内容安全实验中，表现最好的**LEC模型在二分类任务中达到了0.96的加权F1分数**，远超GPT-4o的0.82**和LlamaGuard 8B的0.71分**，而这一成绩是在**仅使用15个训练样本**的情况下取得的。同样，在提示注入检测中，**我们最优秀的LEC模型达到了0.98的加权F1分数，而GPT-4o为0.92**，deBERTa
    v2提示注入v2为0.73，且该成绩仅用了55个训练样本。**这些结果不仅展示了卓越的性能，还突显了LEC在极少训练数据下实现高准确度的非凡能力。**
- en: Although our work focused on using LEC Models for AI safety use cases, we anticipate
    that our approach can be used for a wider variety of text classification tasks.
    *We encourage the research community to use our work as a stepping stone for exploring
    what else can be achieved — further open new pathways for more intelligent, safer,
    and more trustworthy AI systems.*
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的工作集中在使用LEC模型处理人工智能安全用例上，但我们预期这种方法可以应用于更广泛的文本分类任务。*我们鼓励研究界将我们的工作作为探索更多可能性的跳板——进一步开辟更智能、更安全、更值得信赖的人工智能系统的新道路。*
- en: '*Note: The opinions expressed both in this article and paper are solely those
    of the authors and do not necessarily reflect the views or policies of their respective
    employers.*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：本文和论文中表达的意见仅代表作者本人，不一定反映其各自雇主的观点或政策。*'
- en: Interested in connecting? Drop me a DM on [Linkedin](https://www.linkedin.com/in/sandibesen/)!
    I‘m always eager to engage in food for thought and iterate on my work.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 有兴趣联系吗？可以在[Linkedin](https://www.linkedin.com/in/sandibesen/)上私信我！我总是乐于参与思想碰撞，并在我的工作中进行迭代。
- en: 'References:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 参考文献：
- en: '[1] [https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023](https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023](https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023)'
- en: '[2] [https://arxiv.org/abs/2412.13435](https://arxiv.org/abs/2412.13435)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [https://arxiv.org/abs/2412.13435](https://arxiv.org/abs/2412.13435)'
- en: '[3] [https://huggingface.co/datasets/OpenSafetyLab/Salad-Data](https://huggingface.co/datasets/OpenSafetyLab/Salad-Data)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [https://huggingface.co/datasets/OpenSafetyLab/Salad-Data](https://huggingface.co/datasets/OpenSafetyLab/Salad-Data)'
- en: '[4] [https://huggingface.co/datasets/lmsys/lmsys-chat-1m](https://huggingface.co/datasets/lmsys/lmsys-chat-1m)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [https://huggingface.co/datasets/lmsys/lmsys-chat-1m](https://huggingface.co/datasets/lmsys/lmsys-chat-1m)'
