- en: Building a Knowledge Graph From Scratch Using LLMs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零开始构建知识图谱，使用大型语言模型（LLMs）
- en: 原文：[https://towardsdatascience.com/building-a-knowledge-graph-from-scratch-using-llms-f6f677a17f07?source=collection_archive---------0-----------------------#2024-11-25](https://towardsdatascience.com/building-a-knowledge-graph-from-scratch-using-llms-f6f677a17f07?source=collection_archive---------0-----------------------#2024-11-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/building-a-knowledge-graph-from-scratch-using-llms-f6f677a17f07?source=collection_archive---------0-----------------------#2024-11-25](https://towardsdatascience.com/building-a-knowledge-graph-from-scratch-using-llms-f6f677a17f07?source=collection_archive---------0-----------------------#2024-11-25)
- en: Turn your Pandas data frame into a knowledge graph using LLMs. Build your own
    LLM graph-builder from scratch, implement LLMGraphTransformer by LangChain, and
    QA your KG.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用大型语言模型（LLMs）将你的Pandas数据框架转化为知识图谱。从零开始构建自己的LLM图谱构建器，使用LangChain实现LLMGraphTransformer，并对你的知识图谱进行问答（QA）。
- en: '[](https://medium.com/@cristianleo120?source=post_page---byline--f6f677a17f07--------------------------------)[![Cristian
    Leo](../Images/99074292e7dfda50cf50a790b8deda79.png)](https://medium.com/@cristianleo120?source=post_page---byline--f6f677a17f07--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f6f677a17f07--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f6f677a17f07--------------------------------)
    [Cristian Leo](https://medium.com/@cristianleo120?source=post_page---byline--f6f677a17f07--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@cristianleo120?source=post_page---byline--f6f677a17f07--------------------------------)[![Cristian
    Leo](../Images/99074292e7dfda50cf50a790b8deda79.png)](https://medium.com/@cristianleo120?source=post_page---byline--f6f677a17f07--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f6f677a17f07--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f6f677a17f07--------------------------------)
    [Cristian Leo](https://medium.com/@cristianleo120?source=post_page---byline--f6f677a17f07--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f6f677a17f07--------------------------------)
    ·36 min read·Nov 25, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f6f677a17f07--------------------------------)
    ·36分钟阅读·2024年11月25日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/5b9cb939817996b93d12ee568410a678.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b9cb939817996b93d12ee568410a678.png)'
- en: Knowledge Graph about 1000 Movies from Wikipedia — Image by Author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 来自维基百科的1000部电影知识图谱 — 作者提供的图片
- en: In today’s AI world, knowledge graphs are becoming increasingly important, as
    they enable many of the knowledge retrieval systems behind LLMs. Many data science
    teams across companies are investing heavily in retrieval augmented generation
    (RAG), as it’s an efficient way to improve the output accuracy of LLMs and prevent
    hallucinations.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今的人工智能世界中，知识图谱变得越来越重要，因为它们支撑了许多大型语言模型（LLMs）背后的知识检索系统。许多公司的数据科学团队正在大力投资检索增强生成（RAG），因为这是一种提高LLM输出准确性并防止幻觉生成的高效方法。
- en: 'But there is more to it; on a personal note, graph-RAG is democratizing the
    AI space. This is because before if we wanted to customize a model to a use-case
    — either for fun or business — we would have three options: pre-training the model
    providing a bigger exposure to a dataset within the industry of your use-case,
    fine-tuning the model on a specific dataset, and context prompting.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但事情远不止如此；从个人角度来看，图谱增强生成（graph-RAG）正在让人工智能领域变得更加开放和民主化。这是因为，在此之前，如果我们想要定制一个模型以适应某个应用场景——无论是为了娱乐还是商业——我们通常会有三种选择：对模型进行预训练，以便为你的应用场景所在行业的数据集提供更大的曝光，针对特定数据集进行微调，或者使用上下文提示。
- en: As for pre-training, this option is incredibly expensive and technical, and
    it’s not an option for most developers.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 至于预训练，这个选项极其昂贵且技术要求高，对于大多数开发者来说，并不是一个可行的选择。
- en: Fine-tuning is easier than pre-training, and although the cost of fine-tuning
    depends on the model and the training corpus, it’s generally a more affordable
    option. This one was…
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 微调比预训练更简单，尽管微调的成本取决于模型和训练语料库，但通常来说，它是一个更为经济的选择。这个选项是……
