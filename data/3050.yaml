- en: The 80/20 problem of generative AI — a UX research insight
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成式AI的80/20问题 — 一项用户体验研究洞察
- en: 原文：[https://towardsdatascience.com/the-80-20-problem-of-generative-ai-a-ux-research-insight-445e8aa3bbd3?source=collection_archive---------1-----------------------#2024-12-21](https://towardsdatascience.com/the-80-20-problem-of-generative-ai-a-ux-research-insight-445e8aa3bbd3?source=collection_archive---------1-----------------------#2024-12-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-80-20-problem-of-generative-ai-a-ux-research-insight-445e8aa3bbd3?source=collection_archive---------1-----------------------#2024-12-21](https://towardsdatascience.com/the-80-20-problem-of-generative-ai-a-ux-research-insight-445e8aa3bbd3?source=collection_archive---------1-----------------------#2024-12-21)
- en: '![](../Images/eb4b5aab6bb53ea2b5f2d12e9d16fdea.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb4b5aab6bb53ea2b5f2d12e9d16fdea.png)'
- en: '*Image by author*'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*图片由作者提供*'
- en: When an LLM solves a task 80% correctly, that often only amounts to 20% of the
    user value.
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 当大型语言模型（LLM）解决任务的正确率达到80%时，往往只代表用户价值的20%。
- en: '[](https://medium.com/@zombor?source=post_page---byline--445e8aa3bbd3--------------------------------)[![Zombor
    Varnagy-Toth](../Images/9e982d75a5aa08ce96e559aca6fe050c.png)](https://medium.com/@zombor?source=post_page---byline--445e8aa3bbd3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--445e8aa3bbd3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--445e8aa3bbd3--------------------------------)
    [Zombor Varnagy-Toth](https://medium.com/@zombor?source=post_page---byline--445e8aa3bbd3--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@zombor?source=post_page---byline--445e8aa3bbd3--------------------------------)[![Zombor
    Varnagy-Toth](../Images/9e982d75a5aa08ce96e559aca6fe050c.png)](https://medium.com/@zombor?source=post_page---byline--445e8aa3bbd3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--445e8aa3bbd3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--445e8aa3bbd3--------------------------------)
    [Zombor Varnagy-Toth](https://medium.com/@zombor?source=post_page---byline--445e8aa3bbd3--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--445e8aa3bbd3--------------------------------)
    ·3 min read·Dec 21, 2024
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--445e8aa3bbd3--------------------------------)
    ·阅读时间：3分钟·2024年12月21日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: The Pareto principle says if you solve a problem 20% through, you get 80% of
    the value. The opposite seems to be true for generative AI.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 帕累托原则认为，如果你解决问题的程度达到20%，你就能获得80%的价值。对于生成式AI来说，似乎情况正好相反。
- en: 'About the author: Zsombor Varnagy-Toth is a Sr UX Researcher at SAP with background
    in machine learning and cognitive science. Working with qualitative and quantitative
    data for product development.'
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 关于作者：Zsombor Varnagy-Toth 是 SAP 的高级用户体验研究员，拥有机器学习和认知科学背景。专注于使用定性和定量数据进行产品开发。
- en: I first realized this as I studied professionals writing marketing copy using
    LLMs. I observed that when these professionals start using LLMs, their enthusiasm
    quickly fades away, and most return to their old way of manually writing content.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我第一次意识到这一点是当我研究专业人士使用大型语言模型（LLMs）撰写营销文案时。我观察到，当这些专业人士开始使用LLMs时，他们的热情迅速消退，大多数人最终还是回到了手动编写内容的老路。
- en: This was an utterly surprising research finding because these professionals
    acknowledged that the AI-generated content was not bad. In fact, they found it
    unexpectedly good, say 80% good. But if that’s so, why do they still fall back
    on creating the content manually? Why not take the 80% good AI-generated content
    and just add that last 20% manually?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个令人完全惊讶的研究发现，因为这些专业人士承认，AI生成的内容并不差。事实上，他们觉得它出乎意料的好，差不多有80%的质量。但是，如果是这样，为什么他们还会选择手动创建内容呢？为什么不直接拿80%好的AI生成内容，再手动加上最后的20%？
- en: 'Here is the intuitive explanation:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是直观的解释：
- en: '*If you have a mediocre poem, you can’t just turn it into a great poem by replacing
    a few words here and there.*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你有一首平庸的诗歌，你不能通过换几个词把它变成一首伟大的诗歌。*'
- en: '*Say, you have a house that is 80% well built. It’s more or less OK, but the
    walls are not straight, and the foundations are weak. You can’t fix that with
    some additional work. You have to tear it down and start building it from the
    ground up.*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*假设你有一座房子，建得80%好。它基本上还可以，但墙壁不直，基础也很弱。你无法通过一些额外的工作来修复它。你必须将其拆除，从头开始建造。*'
- en: We investigated this phenomenon further and identified its root. For these marketing
    professionals if a piece of copy is only 80% good, there is no individual piece
    in the text they could swap that would make it 100%. For that, the whole copy
    needs to be reworked, paragraph by paragraph, sentence by sentence. Thus, going
    from AI’s 80% to 100% takes almost as much effort as going from 0% to 100% manually.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步调查了这一现象，并找到了其根源。对于这些营销专业人士来说，如果一篇文案只有80%的效果，那么在文本中没有哪个单独的部分可以替换，从而使其达到100%的效果。为此，整篇文案需要逐段逐句地重新编写。因此，从AI的80%到100%几乎需要花费与从0%到100%手动完成相同的精力。
- en: Now, this has an interesting implication. For such tasks, **the value of LLMs
    is “all or nothing.”** It either does an excellent job or it’s useless. There
    is nothing in between.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这带来了一个有趣的启示。对于此类任务，**LLM的价值是“全有或全无”**的。它要么做得非常出色，要么就没用。没有中间状态。
- en: We looked at a few different types of user tasks and figured that this reverse
    Pareto principle affects a specific class of tasks.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们查看了几种不同类型的用户任务，并发现这种逆帕累托原则影响着特定类型的任务。
- en: '**Not easily decomposable** and'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不容易分解**，并且'
- en: '**Large task size** and'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务规模大**，以及'
- en: '**100% quality is expected**'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**期望100%的质量**'
- en: If one of these conditions are not met, the reverse Pareto effect doesn’t apply.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些条件中的任何一个没有得到满足，逆帕累托效应就不适用了。
- en: 'Writing code, for example, is more composable than writing prose. Code has
    its individual parts: commands and functions that can be singled out and fixed
    independently. If AI takes the code to 80%, it really only takes about 20% extra
    effort to get to the 100% result.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 写代码，例如，比写散文更具可组合性。代码有其独立的部分：命令和函数，可以单独挑出并独立修复。如果AI将代码完成到80%，实际上只需要额外花费约20%的努力，就能达到100%的结果。
- en: As for the task size, LLMs have great utility in writing short copy, such as
    social posts. The LLM-generated short content is still “all or nothing” — it’s
    either good or worthless. However, because of the brevity of these pieces of copy,
    one can generate ten at a time and spot the best one in seconds. In other words,
    users don’t need to tackle the 80% to 100% problem — they just pick the variant
    that came out 100% in the first place.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 就任务规模而言，LLM在写短文案（例如社交媒体帖子）方面非常有用。LLM生成的短内容仍然是“全有或全无”——要么很好，要么毫无价值。然而，由于这些文案的简短性，用户可以一次生成十个，并在几秒钟内找出最好的一个。换句话说，用户无需处理80%到100%的问题——他们只需选择最初生成的100%的变体。
- en: As for quality, there are those use cases when professional grade quality is
    not a requirement. For example, a content factory may be satisfied with 80% quality
    articles.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 至于质量，有些使用场景并不要求专业级的质量。例如，一个内容工厂可能满足于80%质量的文章。
- en: What does this mean for product development?
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这对产品开发意味着什么？
- en: If you are building an LLM-powered product that deals with **large tasks** that
    are **hard to decompose** but the user is expected to produce **100% quality**,
    you must build something around the LLM that turns its 80% performance into 100%.
    It can be a sophisticated prompting approach on the backend, an additional fine-tuned
    layer, or a cognitive architecture of various tools and agents that work together
    to iron out the output. Whatever this wrapper does, that’s what gives 80% of the
    customer value. That’s where the treasure is buried, the LLM only contributes
    20%.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在构建一个处理**大任务**且**难以分解**的LLM驱动产品，而用户又要求输出**100%质量**，你必须围绕LLM构建某些东西，将其80%的表现提升到100%。这可以是后端的复杂提示方法、额外的微调层，或者是多种工具和代理共同协作的认知架构，用以完善输出。不论这个包装层做什么，它就是带来80%客户价值的所在。宝藏就埋在那里，LLM只贡献了20%。
- en: This conclusion is in line with Sequoia Capital’s [Sonya Huang’s and Pat Grady’s
    assertion](https://www.sequoiacap.com/article/generative-ais-act-o1/) that the
    next wave of value in the AI space will be created by these “last-mile application
    providers” — the wrapper companies that figure out how to jump that last mile
    that creates 80% of the value.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这一结论与红杉资本的[Sonya Huang和Pat Grady的论述](https://www.sequoiacap.com/article/generative-ais-act-o1/)一致，即AI领域的下一个价值浪潮将由这些“最后一公里应用提供商”创造——这些包装公司弄清楚如何跨越最后一公里，从而创造80%的价值。
