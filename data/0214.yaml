- en: 'AI for Groups: Build a Multi-User Chat Assistant Using 7B-Class Models'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'AI for Groups: 使用 7B 类模型构建多用户聊天助手'
- en: 原文：[https://towardsdatascience.com/ai-for-groups-build-a-multi-user-chat-assistant-using-7b-class-models-7071ca8b4aa0?source=collection_archive---------6-----------------------#2024-01-22](https://towardsdatascience.com/ai-for-groups-build-a-multi-user-chat-assistant-using-7b-class-models-7071ca8b4aa0?source=collection_archive---------6-----------------------#2024-01-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/ai-for-groups-build-a-multi-user-chat-assistant-using-7b-class-models-7071ca8b4aa0?source=collection_archive---------6-----------------------#2024-01-22](https://towardsdatascience.com/ai-for-groups-build-a-multi-user-chat-assistant-using-7b-class-models-7071ca8b4aa0?source=collection_archive---------6-----------------------#2024-01-22)
- en: Have you ever wanted to build an assistant that knows when to talk and when
    to remain silent? Learn how to do it using open-source models.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你是否曾想过构建一个能够知道何时说话、何时保持沉默的助手？了解如何使用开源模型实现这一目标。
- en: '[](https://medium.com/@jjezabek?source=post_page---byline--7071ca8b4aa0--------------------------------)[![Jan
    Jezabek, Ph.D.](../Images/eb78f321cc347a9e619b2c474e7bc192.png)](https://medium.com/@jjezabek?source=post_page---byline--7071ca8b4aa0--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7071ca8b4aa0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7071ca8b4aa0--------------------------------)
    [Jan Jezabek, Ph.D.](https://medium.com/@jjezabek?source=post_page---byline--7071ca8b4aa0--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jjezabek?source=post_page---byline--7071ca8b4aa0--------------------------------)[![Jan
    Jezabek, Ph.D.](../Images/eb78f321cc347a9e619b2c474e7bc192.png)](https://medium.com/@jjezabek?source=post_page---byline--7071ca8b4aa0--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7071ca8b4aa0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7071ca8b4aa0--------------------------------)
    [Jan Jezabek, Ph.D.](https://medium.com/@jjezabek?source=post_page---byline--7071ca8b4aa0--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7071ca8b4aa0--------------------------------)
    ·15 min read·Jan 22, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7071ca8b4aa0--------------------------------)
    ·阅读时长 15 分钟 ·2024年1月22日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Intelligent chat assistants have become a central application made possible
    by the recent generative AI progress, with ChatGPT and Bing Chat/Copilot becoming
    household names. Typically, this takes the form of a back and forth between a
    user, who provides prompts or instructions, and an assistant, who in turn provides
    responses.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 智能聊天助手已成为最近生成式 AI 进展带来的核心应用，ChatGPT 和 Bing Chat/Copilot 已成为家喻户晓的名字。通常，这种形式是用户提供提示或指令，而助手则根据这些信息进行回应。
- en: 'A scenario that has received comparatively less attention is one in which an
    assistant is a semi-active participant in a conversation between two or more users.
    Examples of such interactions are conversations between groups of friends planning
    activities together — with the assistant providing recommendations when applicable
    and staying silent otherwise — or customer support chats, with the assistant providing
    suggestions to the customer service representative. In these cases, the assistant
    is not expected to respond at every turn: It would be awkward if it regularly
    barged in during casual chit-chat between friends.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 一个相对较少关注的场景是，助手作为两个或多个用户之间对话的半主动参与者。这类互动的例子包括一群朋友一起计划活动时的对话——在适当的时候助手提供建议，其他时候保持沉默——或者是客服聊天，助手向客服代表提供建议。在这些情况下，助手并不需要每次都回应：如果它在朋友们的随意聊天中频繁插话，那会显得很尴尬。
- en: '![](../Images/2ae3c2938854108e952881ce1da19d1a.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ae3c2938854108e952881ce1da19d1a.png)'
- en: '(Image credit: DALL-E 3 with post-processing by the author to remove extra
    fingers)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: （图片来源：DALL-E 3，作者后期处理去除多余的手指）
- en: In this series I’ll go through the steps needed to build a lightweight assistant
    for this purpose using open-source LLMs. In this context “lightweight” means a
    model that requires 16GB and 8GB of GPU RAM for training and inference respectively,
    and that it can efficiently run on a CPU if needed. For this purpose, I will be
    using Llama-2-7b-hf-chat, Zephyr-7b-beta, and OpenChat-3.5-0106, which all fit
    this description.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个系列中，我将介绍使用开源 LLM（大型语言模型）构建轻量级助手所需的步骤。在这个语境中，“轻量级”指的是训练和推理分别需要 16GB 和 8GB
    GPU 内存的模型，并且如果需要，它可以高效地在 CPU 上运行。为此，我将使用 Llama-2-7b-hf-chat、Zephyr-7b-beta 和 OpenChat-3.5-0106，这些模型都符合这个描述。
- en: ChatGPT-3.5-Turbo Baseline
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ChatGPT-3.5-Turbo 基准
- en: To get a feeling for the task we’ll first implement it using ChatGPT. This will
    give us a reference point from a strong model and will give us an estimate of
    the task’s difficulty.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对任务有一个感觉，我们将首先使用 ChatGPT 实现它。这将为我们提供一个来自强大模型的参考点，并帮助我们估算任务的难度。
- en: 'Let’s think about some of the unique aspects of our use case:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们思考一下我们用例中的一些独特之处：
- en: 'We don’t want the assistant to be overzealous: It should only chime in if asked
    directly or if it has some interesting trivia to add. To this end the assistant
    needs the possibility to remain silent.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不希望助手过于热心：它应该只有在被直接问及或有一些有趣的琐事可以补充时才插话。为此，助手需要能够保持沉默的选项。
- en: There are multiple human users in the conversation. To make sense of it, we
    need to indicate which user is the speaker for each chat message.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对话中有多个用户。为了理清楚这一点，我们需要为每条聊天消息标明是哪个用户在发言。
- en: For the first aspect we need to define the mechanism for when the assistant
    chooses to remain silent. To achieve this, we’ll instruct the model to return
    “(silence)” as its response. Such a prediction can then be filtered during post-processing.
    An alternative is to ask the model to return an empty prediction, but anecdotally
    this seems not to be working reliably with some models (they are not used to staying
    silent!).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个方面，我们需要定义助手选择保持沉默的机制。为此，我们将指示模型返回“(silence)”作为响应。这样的预测可以在后处理时进行过滤。另一种方法是要求模型返回空的预测，但根据经验，这在某些模型中似乎不太可靠（它们不习惯保持沉默！）。
- en: For the second aspect, OpenAI’s API conveniently lets us provide the name of
    the participant for each message in the conversation (curiously this functionality
    is not exposed in the [Playground](https://platform.openai.com/playground)). This
    is unfortunately not true for the common open-source models (where we will need
    a workaround), but for ChatGPT we should be fine.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二个方面，OpenAI 的 API 方便地允许我们为对话中的每条消息提供参与者的姓名（有趣的是，这个功能在[Playground](https://platform.openai.com/playground)中并没有开放）。不幸的是，这对于常见的开源模型并不适用（我们需要一种解决方法），但对于
    ChatGPT 来说，我们应该没问题。
- en: 'This leaves one more crucial decision: The prompt. For our use case I’m deliberately
    picking something short and precise (it can always be adjusted if the tone of
    the responses ends up being off):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这还剩下一个至关重要的决策：提示。对于我们的用例，我故意选择了一个简短而精确的提示（如果响应的语气有偏差，随时可以调整）：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We now have everything we need, let’s give it a try. Using a chat loop as implemented
    in [this notebook](https://colab.research.google.com/drive/18Lsn3ws5H3jCEJ-loLmuL4_195BOEEoa?usp=drive_link),
    we get the following conversation:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了所需的一切，让我们试试看。使用在[这个笔记本](https://colab.research.google.com/drive/18Lsn3ws5H3jCEJ-loLmuL4_195BOEEoa?usp=drive_link)中实现的聊天循环，我们得到了以下对话：
- en: 'The initial results are encouraging if not perfect: The assistant occasionally
    chooses to remain silent (adhering to the format from the instructions) or chimes
    in with helpful information, but it also sometimes responds with unnecessary chit-chat.
    Changing the prompt to:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 初步结果令人鼓舞，尽管不完美：助手偶尔选择保持沉默（遵循指令中的格式）或提供有用的信息，但有时也会回应一些不必要的闲聊。将提示修改为：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'and inserting this reminder system message after every user message:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 并在每条用户消息后插入这个提醒系统消息：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'does not seem to make a big difference, as seen in this conversation:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这似乎没有太大区别，正如在以下对话中所看到的：
- en: 'It’s likely that the model’s performance can be improved significantly with
    more work on the prompt, but for now this is sufficient for our purposes: We have
    a baseline to compare against and we also get an indication that the problem is
    tractable, if not trivial.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的表现很可能通过对提示进行更多调整而显著改善，但现在这对于我们的目的已经足够：我们有了一个基准可以进行对比，同时也得到了一个表明问题是可解决的迹象，尽管不是简单的。
- en: Open-Source Models and Finetuning
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开源模型和微调
- en: We’ve seen that despite some hiccups, ChatGPT-3.5-Turbo is able to act as a
    semi-active participant in a group conversation. The same is unfortunately not
    true for common open-source models in the 7B parameter class, which end up responding
    at every turn. Fortunately, the great thing about open-source LLMs is that we
    can adapt them to our task via finetuning.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，尽管有一些小问题，ChatGPT-3.5-Turbo 能够充当群聊中的半主动参与者。不幸的是，对于常见的开源模型，尤其是7B 参数级别的模型，情况并非如此，它们在每个回合都回应。幸运的是，开源大型语言模型的一个优点是，我们可以通过微调将它们适配到我们的任务中。
- en: It is worth pointing out that finetuning is not applicable to every situation.
    For example, if you want to teach a model new facts, finetuning will not be the
    right tool (a better approach is Retrieval Augmented Generation). However, if
    you want to alter the tone or format of the responses (as we do here), finetuning
    is just the thing you need.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 值得指出的是，微调并不适用于每种情况。例如，如果你想教给模型新的事实，微调就不是合适的工具（一个更好的方法是检索增强生成）。然而，如果你想改变响应的语气或格式（正如我们在这里所做的），微调正是你需要的工具。
- en: Dataset Generation
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集生成
- en: 'A critical thing to decide for finetuning is the dataset. We’ll need to provide
    a set of good examples of multi-user conversations where an assistant largely
    remains silent, but occasionally chimes in with helpful information. To quickly
    bootstrap such a set, I enrolled the help of Mixtral-8x7B-Instruct-v0.1, hosted
    on replicate.com. Specifically, I generated 50 synthetic conversations using this
    prompt (along with some variations in the topic of discussion and participant
    names, see [this notebook](https://colab.research.google.com/drive/1JWAK3ecaO4EjUqIk1LBIfGQ8zV4oWmDk?usp=drive_link)
    for details):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 微调中一个关键的决定因素是数据集。我们需要提供一组良好的多用户对话示例，在这些对话中，助手大多数时间保持沉默，但偶尔会插入有帮助的信息。为了快速构建这样的数据集，我求助于Mixtral-8x7B-Instruct-v0.1，它托管在replicate.com上。具体来说，我使用以下提示生成了50个合成对话（并对讨论的主题和参与者名字进行了一些变化，详情见[这个笔记本](https://colab.research.google.com/drive/1JWAK3ecaO4EjUqIk1LBIfGQ8zV4oWmDk?usp=drive_link)）：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Obviously, the result is not a high quality, curated dataset, so using it for
    a production model is not recommended. I will discuss some ways to improve the
    dataset’s quality, as well as approaches for evaluating the resultant model in
    a subsequent article. However, the dataset is good enough for our purpose right
    now, that is to validate that a small model can be adapted for the purpose of
    a multi-user chat assistant.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，结果并不是一个高质量的精心策划的数据集，因此不建议将其用于生产模型。我将在后续文章中讨论一些改进数据集质量的方法，以及评估最终模型的方式。然而，目前该数据集已经足够满足我们的需求，即验证一个小型模型能否适应作为多用户聊天助手的任务。
- en: 'The dataset generation notebook is available here, and the generated dataset
    was uploaded to [this HuggingFace repository](https://huggingface.co/jjezabek/multi_user_chat_synthetic).
    Below is an example generated dialog:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集生成笔记本可以在此找到，生成的数据集已上传到[这个HuggingFace库](https://huggingface.co/jjezabek/multi_user_chat_synthetic)。以下是一个生成的对话示例：
- en: A Note About Chat Templates
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于聊天模板的说明
- en: When using a pretrained chat model, it is a good idea to ensure that the format
    of your input matches the one that the model had been trained with. This has become
    a bit easier with HuggingFace in September 2023 with the introduction of the *apply_chat_template*
    method of the tokenizer. This method takes care of formatting the various user,
    system and assistant prompts and responses into the required format expected by
    the model.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用预训练的聊天模型时，确保输入的格式与模型训练时的格式相匹配是个好主意。2023年9月，HuggingFace引入了*apply_chat_template*方法，这使得这个任务变得更加容易。该方法负责将各种用户、系统和助手的提示和响应格式化成模型所期望的格式。
- en: Unfortunately, not all models have been updated to have a chat template, so
    I recommend inspecting the output from *apply_chat_template* for each model and
    comparing it to the model’s documentation.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，并非所有模型都已更新以具备聊天模板，因此我建议检查每个模型的*apply_chat_template*输出，并将其与模型的文档进行对比。
- en: In the context of finetuning (as opposed to just using on off-the-shelf model
    for inference) we don’t necessarily have to follow a prescribed format. In fact,
    for non-chat models defining your own chat template is a necessity. However, for
    chat models sticking with the existing chat template is likely to make the finetuning
    task easier, resulting in fewer training steps and a smaller possibility of unwanted
    side effects (think catastrophic forgetting).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调的上下文中（与仅使用现成模型进行推理不同），我们不必严格遵循规定的格式。事实上，对于非聊天模型，定义自己的聊天模板是必须的。然而，对于聊天模型，保持现有聊天模板通常能让微调任务变得更容易，减少训练步骤，并且减少出现不良副作用的可能性（比如灾难性遗忘）。
- en: 'For the models we’ve chosen, Zephyr, Llama-7b-chat, and OpenChat-3.5, we are
    in luck: All of them have their chat templates defined correctly and *apply_chat_template*
    works as expected.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们选择的模型，Zephyr、Llama-7b-chat和OpenChat-3.5，我们很幸运：它们都有正确的聊天模板，并且*apply_chat_template*按预期工作。
- en: Finetuning
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调
- en: We are now ready to kick off the finetuning. As mentioned before, the goal is
    to fit the training into 16GB of GPU memory, allowing it to run on a single T4
    GPU (no need to hunt for the ultra-rare Pokémon… err, I mean A100s). To achieve
    this, we’ll use 4-bit quantization and LoRA. If you’re unfamiliar with these terms,
    I highly recommend [this article](https://www.mercity.ai/blog-post/guide-to-fine-tuning-llms-with-lora-and-qlora)
    as an introduction. This section will go through the main steps needed for finetuning,
    the complete training notebook can be accessed [here](https://colab.research.google.com/drive/1MDXvw8ie3XnpR0OujFhwLgTU0XIeepSv?usp=drive_link).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好开始微调了。如前所述，目标是将训练适配到16GB的GPU内存中，允许它在单个T4 GPU上运行（无需去寻找稀有的宝可梦…呃，我是说A100）。为了实现这一点，我们将使用4位量化和LoRA。如果你不熟悉这些术语，我强烈推荐[这篇文章](https://www.mercity.ai/blog-post/guide-to-fine-tuning-llms-with-lora-and-qlora)作为入门介绍。本节将介绍微调所需的主要步骤，完整的训练notebook可以通过[这里](https://colab.research.google.com/drive/1MDXvw8ie3XnpR0OujFhwLgTU0XIeepSv?usp=drive_link)访问。
- en: 'Before starting training, we need to slightly massage the synthetic dataset
    created earlier:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始训练之前，我们需要稍微调整一下之前创建的合成数据集：
- en: 'We need to add information about who the speaker is in each user turn. Remember
    the helpful *name* field in OpenAI’s API that allowed us to differentiate between
    various human speakers? It’s sadly not present in Zephyr’s, Llama’s and OpenChat’s
    chat templates. As a workaround we will just prepend “{name}: ” at the start of
    each line.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '我们需要为每个用户回合添加说话者信息。还记得OpenAI API中有用的*name*字段，它帮助我们区分不同的人工说话者吗？遗憾的是，Zephyr、Llama和OpenChat的聊天模板中没有此字段。作为一种解决方法，我们将只是在每一行的开头加上“{name}:
    ”。'
- en: We also need to add assistant lines saying “(silence)” every time the assistant
    chooses not to respond in a turn. In addition, we will also prepend “(response)”
    before each assistant line. This is not strictly necessary for the basic chat
    case but will allow us to cajole the model into answering even if it preferred
    to remain silent (this will come handy during evaluation but can also be a product
    feature).
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还需要在助手每次选择不回应时添加“(silence)”的助手行。此外，我们还将每个助手行前添加“(response)”。对于基本的对话情况，这不是绝对必要的，但它将促使模型回答，即使它本来想保持沉默（在评估过程中这会很有用，也可以作为产品功能）。
- en: Finally, we also need to apply the chat template.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们还需要应用聊天模板。
- en: 'The dataset preprocessing is implemented as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集预处理实现如下：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Note that no system prompt is included. The reason is that we’re finetuning
    a model for this one specific task, so providing the instructions to the model
    is redundant: It learns what it is supposed to do from its training. This has
    the nice side effect of both shorter training and slightly quicker inference.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，没有包含系统提示。原因是我们正在为这个特定任务微调模型，因此向模型提供指令是多余的：它通过训练学会了该做什么。这样做有一个很好的副作用——训练时间较短，推理也稍微更快。
- en: 'Having finished preparing the dataset, we now load the quantized model:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成数据集的准备工作后，我们现在加载量化模型：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We then define the adapter model (i.e. the low rank “diff” from the base model):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义适配器模型（即来自基础模型的低秩“差异”）：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'and instantiate the trainer and the training arguments:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 并实例化训练器和训练参数：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The settings used above are fairly standard (and I encourage you to tweak them
    as needed). The ones that really matter are the number of epochs, the learning
    rate, and the batch size. The above is a particular configuration that worked
    for me and might be a good starting point but is obviously not a substitute for
    a real hyperparameter search.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 上述设置相对标准（我鼓励你根据需要调整它们）。真正重要的是epoch的数量、学习率和批量大小。上述配置是一个对我有效的特定设置，可能是一个不错的起点，但显然不能替代真正的超参数搜索。
- en: 'We are now ready to instantiate the trainer and kick off the training:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好实例化训练器并启动训练：
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'That was quick, just 8 minutes on a T4! Let’s test how it does by creating
    a conversational pipeline and a loop, using the same [notebook](https://colab.research.google.com/drive/18Lsn3ws5H3jCEJ-loLmuL4_195BOEEoa?usp=drive_link)
    as for the OpenAI API case. Here is an example conversation using a model finetuned
    from OpenChat-3.5–0106:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 那真是很快，仅用了8分钟在T4上！让我们通过创建一个对话管道和循环来测试它的表现，使用与OpenAI API案例相同的[notebook](https://colab.research.google.com/drive/18Lsn3ws5H3jCEJ-loLmuL4_195BOEEoa?usp=drive_link)。下面是使用从OpenChat-3.5–0106微调的模型进行的一个示例对话：
- en: 'This is pretty encouraging: The model follows our format requirements and seems
    to make reasonable decisions on when to chime in and when to remain silent.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这真是令人鼓舞：模型遵循我们的格式要求，并且似乎在何时插入和何时保持沉默方面做出了合理的决定。
- en: So — are we done? One thing to note about the training is that the model is
    taught to predict all of the tokens in each sample, including the user messages
    and any special tokens. The following section will show how this can be suppressed.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 那么—我们完成了吗？关于训练，有一点需要注意的是，模型被教导预测每个样本中的所有标记，包括用户消息和任何特殊标记。接下来的部分将展示如何抑制这一点。
- en: Training on Completions Only
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 仅在完成任务时训练
- en: 'First things first: Why do we even care about not teaching the model to predict
    the user messages? One argument can be made on the grounds of privacy: If real
    conversations are used as training data, a model could possibly be persuaded by
    an end user to leak some of the user messages (for what it’s worth, assistant
    responses can contain sensitive information as well). A second argument is that
    trying to predict user messages is unnecessary, and as a result wasteful. This
    can mean that you will need to train for a longer time to get good results, and
    hence risk unwanted side effects (again, this is chiefly catastrophic forgetting).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要说的是：为什么我们要关心不让模型预测用户消息呢？有一个观点是基于隐私问题：如果真实对话被用作训练数据，模型可能会被最终用户说服，泄露一些用户消息（值得一提的是，助手的回答也可能包含敏感信息）。第二个理由是，尝试预测用户消息是没有必要的，因此也是浪费时间。这意味着你可能需要更长的时间来训练才能获得良好的结果，从而有可能带来不良的副作用（再说一次，这主要是灾难性遗忘）。
- en: Depending on your use case both of these arguments might be moot, and the model
    might do well with the training procedure described above. If, however, it’s not,
    or if you are just curious, I encourage you to keep reading.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的使用场景，这两个参数可能无关紧要，模型可能会在上述训练过程中表现得很好。不过，如果不是，或者你只是好奇，我鼓励你继续阅读。
- en: HuggingFace’s *trl* library provides us with a tool to solve this particular
    problem, implemented as *DataCollatorForCompletionsOnlyLM*. This collator changes
    the labels for the tokens representing user messages to an “ignore” label, meaning
    the models are not trained to predict them. The user messages are of course still
    used as context for predicting assistant messages.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: HuggingFace的*trl*库为我们提供了一个工具来解决这个特定问题，它的实现是*DataCollatorForCompletionsOnlyLM*。这个合并器将代表用户消息的标记的标签更改为“忽略”标签，意味着模型不会被训练去预测它们。当然，用户消息仍然作为预测助手消息的上下文使用。
- en: '*DataCollatorForCompletionsOnlyLM* requires us to pass two strings that it
    can use to find the start of the user messages (the *instruction_template* parameter)
    and the assistant messages (*response_template*). We can find them by inspecting
    the output of *apply_chat_template*: In the case of Zephyr, they are “<|user|>”
    and “<|assistant|>”, for Llama they are “[INST]” and “[/INST]”. Let’s try it out:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*DataCollatorForCompletionsOnlyLM*要求我们传递两个字符串，它可以用来找到用户消息的开始（*instruction_template*参数）和助手消息（*response_template*）。我们可以通过检查*apply_chat_template*的输出找到它们：在Zephyr的情况下，它们是“<|user|>”和“<|assistant|>”，在Llama的情况下，它们是“[INST]”和“[/INST]”。让我们试试看：'
- en: '[PRE11]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Uh oh, this looks bad. Essentially the trainer cannot find our template fragments
    and as a result ignores all our samples. The reason for this is explained in [this
    article](https://huggingface.co/docs/trl/sft_trainer#using-tokenids-directly-for-responsetemplate):
    Depending on the preceding context, a string like “<|user|>” can have different
    tokenized representations. Fortunately, *DataCollatorForCompletionsOnlyLM* allows
    us to pass the tokenized versions of these delimiter strings instead of the literal
    ones. In order to find these tokenized versions, we can inspect the tokenized
    output of a chat template:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀，这看起来不太妙。基本上，训练器无法找到我们的模板片段，因此忽略了我们所有的样本。原因在于[这篇文章](https://huggingface.co/docs/trl/sft_trainer#using-tokenids-directly-for-responsetemplate)中有解释：根据前面的上下文，像“<|user|>”这样的字符串可能有不同的标记化表示形式。幸运的是，*DataCollatorForCompletionsOnlyLM*允许我们传递这些分隔符字符串的标记化版本，而不是字面上的字符串。为了找到这些标记化版本，我们可以检查聊天模板的标记化输出：
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: From the output we can infer that “<|assistant|>” is tokenized as [28789, 28766,
    489, 11143, 28766, 28767], and “<|user|>” is tokenized as [28789, 28766, 1838,
    28766, 28767]. I have included the tokenized sequences for a few common models
    in the table below.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中我们可以推断出“<|assistant|>”被标记化为[28789, 28766, 489, 11143, 28766, 28767]，而“<|user|>”被标记化为[28789,
    28766, 1838, 28766, 28767]。我已经在下面的表格中列出了几种常见模型的标记化序列。
- en: 'With this in hand, we can now retry training using the updated data collator:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些信息，我们现在可以使用更新的数据收集器重新进行训练：
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This gets rid of the warning and the training loss starts decreasing. We can
    now wait for the model training to finish and upload the model to HuggingFace
    Hub.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这消除了警告，并且训练损失开始下降。我们现在可以等待模型训练完成并将模型上传到HuggingFace Hub。
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Smoke Testing
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 垃圾测试
- en: 'Let’s now see how the model is doing in practice by running [this notebook](https://colab.research.google.com/drive/18Lsn3ws5H3jCEJ-loLmuL4_195BOEEoa?usp=drive_link)
    (which can be executed locally using a consumer grade 8GB GPU). Here is an example
    conversation, again for a model finetuned from OpenChat-3.5–0106:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过运行[这个笔记本](https://colab.research.google.com/drive/18Lsn3ws5H3jCEJ-loLmuL4_195BOEEoa?usp=drive_link)来看模型在实际中的表现（该笔记本可以在配备8GB显卡的消费级GPU上本地执行）。以下是一个示例对话，同样是从OpenChat-3.5–0106微调的模型：
- en: 'So — are we done now? This depends on the goal: We do have a model that I like
    to call “syntactically competent”, meaning that it follows our defined format
    and is able to decide when to talk and when to remain silent. If the goal is a
    toy assistant, this might be sufficient. However, for any serious production use,
    there is still a fair amount of work to do, which I’ll discuss in subsequent articles.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 那么——我们现在完成了吗？这取决于目标：我们确实有一个我喜欢称之为“语法上合格”的模型，这意味着它遵循我们定义的格式，并且能够决定何时说话，何时保持沉默。如果目标是一个玩具助手，这可能就足够了。然而，对于任何严肃的生产用途，仍然需要做相当多的工作，我将在后续的文章中讨论这些内容。
- en: Follow-ups
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 后续步骤
- en: 'Let’s list some of the things that are worth consideration as follow-up steps:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们列出一些值得考虑的后续步骤：
- en: '**High quality training set:** So far, we have only used a synthetic training
    set generated by Mixtral. This set does not have too much variation and may contain
    falsehoods. It was useful for bootstrapping but is insufficient for production
    use.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高质量训练集：** 到目前为止，我们只使用了由Mixtral生成的合成训练集。这个数据集变化不大，可能包含虚假信息。它对于启动阶段很有用，但不足以用于生产用途。'
- en: '**Evaluation:** So far, we’ve only done a few smoke tests, but we don’t have
    a good grasp of how the model is performing: Is it responding truthfully, is it
    doing a good job in determining when to chime in? We also don’t know how much
    the finetuned model diverged from the base one. In a follow-up article I’ll show
    how to shed some light on these questions.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估：** 到目前为止，我们只做了一些烟雾测试，但我们并不了解模型的表现如何：它是否诚实回答，是否能够判断何时插话？我们也不知道微调后的模型与基础模型有多少偏差。在后续的文章中，我将展示如何弄清楚这些问题。'
- en: '**Context:** We cannot expect a model with just 7B parameters to be knowledgeable
    on every topic. In fact, for practical purposes, we may want to constrain the
    model to particular topics relevant to our product. To this end, we may want to
    provide contextual information to our model that is relevant to the users’ questions
    and condition the model to only answer based on this information. This approach
    is known as Retrieval Augmented Generation (RAG), and I’ll show how it can be
    applied in our multi-user setting.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**背景：** 我们不能期望一个只有7B参数的模型能够对每个话题都有所了解。事实上，出于实际用途的考虑，我们可能希望将模型限制在与我们产品相关的特定话题上。为此，我们可能希望为模型提供与用户问题相关的上下文信息，并使模型仅基于这些信息回答问题。这种方法称为检索增强生成（RAG），我将展示如何在我们的多用户环境中应用它。'
- en: Resources and Artifacts
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源和工件
- en: 'The notebooks used for training and evaluation are available on Colab: [Dataset
    generation](https://colab.research.google.com/drive/1JWAK3ecaO4EjUqIk1LBIfGQ8zV4oWmDk?usp=drive_link),
    [training](https://colab.research.google.com/drive/1MDXvw8ie3XnpR0OujFhwLgTU0XIeepSv?usp=drive_link)
    and [inference](https://colab.research.google.com/drive/18Lsn3ws5H3jCEJ-loLmuL4_195BOEEoa?usp=drive_link).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练和评估的笔记本可以在Colab上找到：[数据集生成](https://colab.research.google.com/drive/1JWAK3ecaO4EjUqIk1LBIfGQ8zV4oWmDk?usp=drive_link)，[训练](https://colab.research.google.com/drive/1MDXvw8ie3XnpR0OujFhwLgTU0XIeepSv?usp=drive_link)
    和 [推理](https://colab.research.google.com/drive/18Lsn3ws5H3jCEJ-loLmuL4_195BOEEoa?usp=drive_link)。
- en: The synthetic dataset is available [here](https://huggingface.co/jjezabek/multi_user_chat_synthetic).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 合成数据集可以在[这里](https://huggingface.co/jjezabek/multi_user_chat_synthetic)获取。
- en: Finally, the models are available on HuggingFace, finetuned from [Zephyr](https://huggingface.co/jjezabek/multi-user-chat-zephyr-7b-beta-completions-only),
    [Llama-2](https://huggingface.co/jjezabek/multi-user-chat-llama-2-7b-chat-completions-only)
    and [OpenChat-3.5](https://huggingface.co/jjezabek/multi-user-chat-openchat-3.5-0106-completions-only).
    If you are interested in the models trained on whole conversations (as opposed
    to completions only), they are available as well, finetuned from [Zephyr](https://huggingface.co/jjezabek/multi-user-chat-zephyr-7b-beta-full-conversations),
    [Llama-2](https://huggingface.co/jjezabek/multi-user-chat-llama-2-7b-chat-full-conversations)
    and [OpenChat-3.5](https://huggingface.co/jjezabek/multi-user-chat-openchat-3.5-0106-full-conversations).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这些模型可以在HuggingFace上找到，已根据[Zephyr](https://huggingface.co/jjezabek/multi-user-chat-zephyr-7b-beta-completions-only)、[Llama-2](https://huggingface.co/jjezabek/multi-user-chat-llama-2-7b-chat-completions-only)和[OpenChat-3.5](https://huggingface.co/jjezabek/multi-user-chat-openchat-3.5-0106-completions-only)进行微调。如果你对基于完整对话（而非仅仅是完成）的模型感兴趣，它们也可以找到，微调自[Zephyr](https://huggingface.co/jjezabek/multi-user-chat-zephyr-7b-beta-full-conversations)、[Llama-2](https://huggingface.co/jjezabek/multi-user-chat-llama-2-7b-chat-full-conversations)和[OpenChat-3.5](https://huggingface.co/jjezabek/multi-user-chat-openchat-3.5-0106-full-conversations)。
- en: Troubleshooting
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 故障排除
- en: Below I’m listing some pitfalls that I’ve encountered frequently during finetuning,
    these might come handy when finetuning other models.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 下面列出了一些在微调过程中我经常遇到的陷阱，这些可能在微调其他模型时也会派上用场。
- en: Pad Token
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 填充标记
- en: 'I’ve seen the pad token set to the EOS token in multiple tutorials (and also
    by default in the Zephyr model). This doesn’t play well with HuggingFace’s data
    collators though: [this line](https://github.com/huggingface/transformers/blob/976189a6df796a2ff442dd81b022626c840d8c27/src/transformers/data/data_collator.py#L752)
    in *DataCollatorForLanguageModeling* means that models are not trained to predict
    pad tokens. If the pad and EOS tokens are the same, you might end up with a model
    that continues generating tokens without stopping. My recommendation is to set
    the pad token to the UNK token if available (and distinct from EOS). Alternatively,
    you can use the tokenizer’s *add_token* method to add it to the vocabulary.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我在多个教程中看到填充标记被设置为EOS标记（在Zephyr模型中也默认如此）。然而，这与HuggingFace的数据合并器不兼容：*DataCollatorForLanguageModeling*中的[这一行](https://github.com/huggingface/transformers/blob/976189a6df796a2ff442dd81b022626c840d8c27/src/transformers/data/data_collator.py#L752)意味着模型不会被训练来预测填充标记。如果填充标记和EOS标记相同，最终可能会导致模型在生成标记时不停地继续生成。我的建议是将填充标记设置为UNK标记（如果可用，并且与EOS不同）。另外，你也可以使用分词器的*add_token*方法将其添加到词汇表中。
- en: 'In short: **Make sure the pad token is not the same as the EOS token.** Recent
    versions of HuggingFace started adding this warning, which adds visibility to
    the issue:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之：**确保填充标记与EOS标记不同。** HuggingFace的最近版本已经开始添加这个警告，以提高问题的可见性：
- en: '[PRE15]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Loss Falling to 0.0 During Training
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练过程中损失下降至0.0
- en: When using half precision floats (that is torch.float16), I’ve seen situations
    where the loss goes to 0.0 after a few steps and remains there. Specifically,
    this happens with our training notebook with the Llama-2 model. There are reports
    online of similar issues (for example [here](https://gist.github.com/younesbelkada/9f7f75c94bdc1981c8ca5cc937d4a4da?permalink_comment_id=4634125#gistcomment-4634125)),
    curiously they were resolved at that time by setting the tokenizer’s *padding_side*
    to “right”. In our case the padding is already on the right-hand side, so that
    fix does not apply.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 使用半精度浮点数（即torch.float16）时，我曾遇到过一种情况，即损失函数在几步之后降到0.0并保持在那里。具体来说，这种情况发生在我们使用Llama-2模型的训练笔记本中。网上也有类似问题的报告（例如[这里](https://gist.github.com/younesbelkada/9f7f75c94bdc1981c8ca5cc937d4a4da?permalink_comment_id=4634125#gistcomment-4634125)），有趣的是，它们当时是通过将分词器的*padding_side*设置为“right”来解决的。在我们的情况下，填充已经在右侧，因此这个解决方法并不适用。
- en: 'The workaround is to use a different type for training: Either torch.bfloat16
    (which is unavailable on older instances like T4 and V100) or torch.float32 (which
    results in a performance hit at training time, but otherwise works fine).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方法是使用不同类型的训练：可以使用torch.bfloat16（这在较旧的实例如T4和V100上不可用）或torch.float32（这会导致训练时性能下降，但在其他情况下运行正常）。
- en: '“RuntimeError: element 0 of tensors does not require grad…”'
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '“RuntimeError: element 0 of tensors does not require grad…”'
- en: 'Depending on the model, you might come across this error:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 根据模型的不同，你可能会遇到以下错误：
- en: '[PRE16]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The simple fix is to add this line after instantiating the model:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的解决方法是在实例化模型后添加这一行：
- en: '[PRE17]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
