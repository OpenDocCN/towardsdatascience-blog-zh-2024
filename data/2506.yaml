- en: 'Florence-2: Advancing Multiple Vision Tasks with a Single VLM Model'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Florence-2：通过单一VLM模型推动多个视觉任务的进展
- en: 原文：[https://towardsdatascience.com/florence-2-mastering-multiple-vision-tasks-with-a-single-vlm-model-435d251976d0?source=collection_archive---------2-----------------------#2024-10-14](https://towardsdatascience.com/florence-2-mastering-multiple-vision-tasks-with-a-single-vlm-model-435d251976d0?source=collection_archive---------2-----------------------#2024-10-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/florence-2-mastering-multiple-vision-tasks-with-a-single-vlm-model-435d251976d0?source=collection_archive---------2-----------------------#2024-10-14](https://towardsdatascience.com/florence-2-mastering-multiple-vision-tasks-with-a-single-vlm-model-435d251976d0?source=collection_archive---------2-----------------------#2024-10-14)
- en: 'A Guided Exploration of Florence-2''s Zero-Shot Capabilities: Captioning, Object
    Detection, Segmentation and OCR.'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Florence-2零样本能力的引导性探索：图像说明、物体检测、分割与OCR。
- en: '[](https://medium.com/@lihigurarie?source=post_page---byline--435d251976d0--------------------------------)[![Lihi
    Gur Arie, PhD](../Images/7a1eb30725a95159401c3672fa5f43ab.png)](https://medium.com/@lihigurarie?source=post_page---byline--435d251976d0--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--435d251976d0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--435d251976d0--------------------------------)
    [Lihi Gur Arie, PhD](https://medium.com/@lihigurarie?source=post_page---byline--435d251976d0--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@lihigurarie?source=post_page---byline--435d251976d0--------------------------------)[![Lihi
    Gur Arie, PhD](../Images/7a1eb30725a95159401c3672fa5f43ab.png)](https://medium.com/@lihigurarie?source=post_page---byline--435d251976d0--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--435d251976d0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--435d251976d0--------------------------------)
    [Lihi Gur Arie, PhD](https://medium.com/@lihigurarie?source=post_page---byline--435d251976d0--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--435d251976d0--------------------------------)
    ·7 min read·Oct 14, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--435d251976d0--------------------------------)
    ·阅读时长7分钟·2024年10月14日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d06dac442af9954dd35b05fe0f06a790.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d06dac442af9954dd35b05fe0f06a790.png)'
- en: Image annotations by Author. Original image from [Pexels](https://www.pexels.com/).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图像注释由作者提供。原图来自 [Pexels](https://www.pexels.com/)。
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: In recent years, the field of computer vision has witnessed the rise of foundation
    models that enable image annotation without the need for training custom models.
    We’ve seen models like [CLIP](/clip-creating-image-classifiers-without-data-b21c72b741fa?sk=88fdd2c1a132538015968df3f49b64b1)
    [2] for classification, [GroundingDINO](/automatic-labeling-of-object-detection-datasets-using-groundingdino-b66c486656fe?sk=7c98df89b60ea49a6de9efd5278f645e)
    [3] for object detection, and SAM [4] for segmentation — each excelling in its
    domain. But what if we had a single model capable of handling all these tasks
    together?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，计算机视觉领域见证了基础模型的崛起，这些模型无需训练定制模型即可进行图像注释。我们已经看到像[CLIP](/clip-creating-image-classifiers-without-data-b21c72b741fa?sk=88fdd2c1a132538015968df3f49b64b1)
    [2]这样的分类模型，[GroundingDINO](/automatic-labeling-of-object-detection-datasets-using-groundingdino-b66c486656fe?sk=7c98df89b60ea49a6de9efd5278f645e)
    [3]用于物体检测，和SAM [4]用于图像分割——每个模型在各自领域中表现优异。但是，如果我们有一个能够同时处理所有这些任务的单一模型呢？
- en: If you don’t have a paid Medium account, you can read for free [here](/florence-2-mastering-multiple-vision-tasks-with-a-single-vlm-model-435d251976d0?sk=e25bdee736a9aa9ace1ca80b98a036a4).
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你没有付费的Medium账户，你可以在[这里]( /florence-2-mastering-multiple-vision-tasks-with-a-single-vlm-model-435d251976d0?sk=e25bdee736a9aa9ace1ca80b98a036a4)免费阅读。
- en: In this tutorial we introduce Florence-2 [1]— a novel, open-source Vision-Language
    Model (VLM) designed to handle a diverse range of vision and multimodal tasks,
    including captioning, object detection, segmentation and OCR.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将介绍Florence-2 [1]——一个新颖的开源视觉语言模型（VLM），旨在处理多种视觉和多模态任务，包括图像说明、物体检测、分割和光学字符识别（OCR）。
- en: Accompanied by a Colab notebook, we’ll explore Florence-2’s zero-shot capabilities
    to annotate an image of an old camera.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 配套的Colab笔记本中，我们将探索Florence-2在零样本条件下标注一张老式相机图像的能力。
- en: Florence-2
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Florence-2
- en: '**Background**'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**背景**'
- en: Florence-2 was released by Microsoft in June 2024\. It was designed to perform
    multiple vision tasks within a single model. It is an open-source model, available
    on [Hugging Face](https://huggingface.co/microsoft/Florence-2-large) under the
    permissive MIT licence.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Florence-2 由微软于 2024 年 6 月发布。它被设计为在单个模型中执行多个视觉任务。它是一个开源模型，遵循宽松的 MIT 许可，可以在 [Hugging
    Face](https://huggingface.co/microsoft/Florence-2-large) 上获取。
- en: Despite its relatively small size, with versions of 0.23B & 0.77B parameters,
    Florence-2 achieves state-of-the-art (SOTA) performance. Its compact size enables
    efficient deployment on devices with limited computing resources, while ensuring
    fast inference speeds.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管其模型大小相对较小，版本有 0.23B 和 0.77B 参数，Florence-2 仍然达到了最先进（SOTA）的性能。其紧凑的大小使得它能够高效地部署在计算资源有限的设备上，同时确保快速的推理速度。
- en: The model was pre-trained on an enormous, high quality dataset called FLD-5B,
    consisting of 5.4B annotations on 126 million images. This allows Florence-2 to
    excel in zero-shot performance on many tasks without requiring additional training.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在一个庞大且高质量的数据集 FLD-5B 上进行了预训练，包含了 54 亿个标注，涉及 1.26 亿张图像。这使得 Florence-2 在许多任务中能够实现零样本性能，而无需额外训练。
- en: 'The original open-source weights of the Florence-2 model support the following
    tasks:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Florence-2 模型的原始开源权重支持以下任务：
- en: Additional unsupported tasks can be added by fine-tuning the model.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过微调模型添加额外的、不被支持的任务。
- en: '**Task Format**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**任务格式**'
- en: 'Inspired by Large Language Models (LLMs), Florence-2 was designed as a sequence-to-sequence
    model. It takes an image and text instructions as inputs, and outputs text results.
    The input or output text may represent plain text or a region in the image. The
    region format varies depending on the task:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 受到大规模语言模型（LLMs）的启发，Florence-2 被设计为一个序列到序列的模型。它接受图像和文本指令作为输入，并输出文本结果。输入或输出的文本可以表示普通文本或图像中的区域。区域格式根据任务的不同而有所变化：
- en: '**Bounding Boxes:** `''<X1><Y1><X2><Y2>’` for object detection tasks. The tokens
    represent the coordinates of the top-left and bottom-right corners of the box.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**边界框**：`''<X1><Y1><X2><Y2>’` 用于物体检测任务。标记表示框的左上角和右下角的坐标。'
- en: '**Quad Boxes**: `''<X1><Y1><X2><Y2><X3><Y3><X4><Y4>’` for text detection, using
    the coordinates of the four corners that enclose the text.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**四边形框**：`''<X1><Y1><X2><Y2><X3><Y3><X4><Y4>’` 用于文本检测，使用封闭文本的四个角的坐标。'
- en: '**Polygon**: `''<X1><Y1>...,<Xn><Yn>’` for segmentation tasks, where the coordinates
    represent the vertices of the polygon in clockwise order.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多边形**：`''<X1><Y1>...,<Xn><Yn>’` 用于分割任务，其中坐标表示多边形的顶点，按照顺时针顺序排列。'
- en: '**Architecture**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**架构**'
- en: 'Florence-2 is built using a standard encoder-decoder transformer architecture.
    Here’s how the process works:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Florence-2 基于标准的编码器-解码器 Transformer 架构构建。以下是该过程的工作原理：
- en: The input image is embedded by a DaViT vision encoder [5].
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入图像通过 DaViT 视觉编码器 [5] 嵌入。
- en: The text prompt is embedded using BART [6], utilizing an extended tokenizer
    and word embedding layer.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文本提示通过 BART [6] 嵌入，利用扩展的分词器和词嵌入层。
- en: Both the vision and text embeddings are concatenated.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 视觉和文本的嵌入被拼接在一起。
- en: These concatenated embeddings are processed by a transformer-based multi-modal
    encoder-decoder to generate the response.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些拼接后的嵌入通过基于 Transformer 的多模态编码器-解码器进行处理，以生成响应。
- en: During training, the model minimizes the cross-entropy loss, similar to standard
    language models.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练过程中，模型最小化交叉熵损失，类似于标准语言模型。
- en: '![](../Images/83b1b171555b8ba0c1e9513a11913d8f.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/83b1b171555b8ba0c1e9513a11913d8f.png)'
- en: 'An illustration of Florence-2''s architecture. Source: [link](https://arxiv.org/abs/2311.06242)*.*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Florence-2 架构的示意图。来源：[link](https://arxiv.org/abs/2311.06242)*.*
- en: Code implementation
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码实现
- en: '**Loading Florence-2 model and a sample image**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**加载 Florence-2 模型和示例图像**'
- en: 'After installing and importing the necessary libraries (as demonstrated in
    the accompanying Colab notebook), we begin by loading the Florence-2 model, processor
    and the input image of a camera:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装并导入必要的库（如随附的 Colab 笔记本所示）后，我们首先加载 Florence-2 模型、处理器和摄像头的输入图像：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Auxiliary Functions**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**辅助函数**'
- en: In this tutorial, we will use several auxiliary functions. The most important
    is the `run_example` core function, which generates a response from the Florence-2
    model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将使用几个辅助函数。最重要的函数是 `run_example` 核心函数，它从 Florence-2 模型生成响应。
- en: 'The `run_example` function combines the task prompt with any additional text
    input (if provided) into a single prompt. Using the `processor`, it generates
    text and image embeddings that serve as inputs to the model. The magic happens
    during the `model.generate` step, where the model’s response is generated. Here’s
    a breakdown of some key parameters:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`run_example` 函数将任务提示与任何附加的文本输入（如果有的话）合并成一个单一的提示。通过 `processor`，它生成文本和图像嵌入，这些作为模型的输入。在
    `model.generate` 步骤中，模型生成响应。以下是一些关键参数的拆解：'
- en: '**max_new_tokens=1024**: Sets the maximum length of the output, allowing for
    detailed responses.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**max_new_tokens=1024**：设置输出的最大长度，允许生成详细的响应。'
- en: '**do_sample=False**: Ensures a deterministic response.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**do_sample=False**：确保响应是确定性的。'
- en: '**num_beams=3**: Implements beam search with the top 3 most likely tokens at
    each step, exploring multiple potential sequences to find the best overall output.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**num_beams=3**：使用束搜索，每一步选择最可能的 3 个令牌，探索多个潜在的序列，以找到最佳的整体输出。'
- en: '**early_stopping=False**: Ensures beam search continues until all beams reach
    the maximum length or an end-of-sequence token is generated.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**early_stopping=False**：确保束搜索在所有束达到最大长度或生成结束序列标记之前继续进行。'
- en: Lastly, the model’s output is decoded and post-processed with `processor.batch_decode`
    and `processor.post_process_generation` to produce the final text response, which
    is returned by the `run_example` function.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，模型的输出会通过 `processor.batch_decode` 和 `processor.post_process_generation` 解码和后处理，生成最终的文本响应，这些响应由
    `run_example` 函数返回。
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Additionally, we utilize auxiliary functions to visualize the results (`draw_bbox`
    ,`draw_ocr_bboxes` and `draw_polygon`) and handle the conversion between bounding
    boxes formats (`convert_bbox_to_florence-2` and `convert_florence-2_to_bbox`).
    These can be explored in the attached Colab notebook.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们利用辅助函数来可视化结果（`draw_bbox`、`draw_ocr_bboxes` 和 `draw_polygon`）并处理边界框格式之间的转换（`convert_bbox_to_florence-2`
    和 `convert_florence-2_to_bbox`）。这些内容可以在附带的 Colab 笔记本中探索。
- en: Tasks
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务
- en: Florence-2 can perform a variety of visual tasks. Let’s explore some of its
    capabilities, starting with image captioning.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Florence-2 可以执行多种视觉任务。让我们从图像标题生成开始，探索它的一些功能。
- en: '***1\. Captioning Generation Related Tasks:***'
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '***1\. 标题生成相关任务:***'
- en: '**1.1 Generate Captions**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**1.1 生成标题**'
- en: Florence-2 can generate image captions at various levels of detail, using the
    `'<CAPTION>'` , `'<DETAILED_CAPTION>'` or `'<MORE_DETAILED_CAPTION>'` task prompts.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Florence-2 可以根据 `'<CAPTION>'`、`'<DETAILED_CAPTION>'` 或 `'<MORE_DETAILED_CAPTION>'`
    任务提示生成不同细节级别的图像标题。
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The model accurately describes the image and its surrounding. It even identifies
    the camera’s brand and model, demonstrating its OCR ability. However, in the `'<MORE_DETAILED_CAPTION>'`
    task there are minor inconsistencies, which is expected from a zero-shot model.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型能够准确描述图像及其周围环境，甚至识别相机的品牌和型号，展示了其 OCR 能力。然而，在 `'<MORE_DETAILED_CAPTION>'`
    任务中存在一些小的不一致，这在零-shot 模型中是可以预期的。
- en: '**1.2 Generate Caption for a Given Bounding Box**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**1.2 为给定的边界框生成标题**'
- en: Florence-2 can generate captions for specific regions of an image defined by
    bounding boxes. For this, it takes the bounding box location as input. You can
    extract the category with `'<REGION_TO_CATEGORY>'` or a description with `'<REGION_TO_DESCRIPTION>'`
    .
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Florence-2 可以为图像中特定区域（由边界框定义）生成标题。为此，它需要边界框的位置作为输入。你可以通过 `'<REGION_TO_CATEGORY>'`
    提取类别，或通过 `'<REGION_TO_DESCRIPTION>'` 提取描述。
- en: For your convenience, I added a widget to the Colab notebook that enables you
    to draw a bounding box on the image, and code to convert it to Florence-2 format.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，我在 Colab 笔记本中添加了一个小部件，允许你在图像上绘制边界框，并提供代码将其转换为 Florence-2 格式。
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In this case, the `'<REGION_TO_CATEGORY>'` identified the lens, while the `'<REGION_TO_DESCRIPTION>'`
    was less specific. However, this performance may vary with different images.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，`'<REGION_TO_CATEGORY>'` 识别了镜头，而 `'<REGION_TO_DESCRIPTION>'` 则不够具体。然而，这种表现可能会随着不同图像的变化而有所不同。
- en: '2\. Object Detection Related Tasks:'
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 物体检测相关任务：
- en: '**2.1 Generate Bounding Boxes and Text for Objects**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**2.1 生成物体的边界框和文本**'
- en: 'Florence-2 can identify densely packed regions in the image, and to provide
    their bounding box coordinates and their related labels or captions. To extract
    bounding boxes with labels, use the `’<OD>’`task prompt:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Florence-2 可以识别图像中密集的区域，并提供它们的边界框坐标以及相关的标签或标题。要提取带标签的边界框，请使用 `'<OD>'` 任务提示：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To extract bounding boxes with captions, use `''<DENSE_REGION_CAPTION>''` task
    prompt:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 要提取带有标题的边界框，请使用 `'<DENSE_REGION_CAPTION>'` 任务提示：
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/0efcbdf15dc83e62153b6a752144f800.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0efcbdf15dc83e62153b6a752144f800.png)'
- en: The image on the left shows the results of the ’<OD>’ task prompt, while the
    image on the right demonstrates ‘<DENSE_REGION_CAPTION>’
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的图像展示了‘<OD>’任务提示的结果，而右侧的图像展示了‘<DENSE_REGION_CAPTION>’的结果。
- en: '**2.2 Text Grounded Object Detection**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**2.2 文本基础的物体检测**'
- en: Florence-2 can also perform text-grounded object detection. By providing specific
    object names or descriptions as input, Florence-2 detects bounding boxes around
    the specified objects.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Florence-2还可以执行文本基础的物体检测。通过提供特定的物体名称或描述作为输入，Florence-2能够检测到围绕指定物体的边界框。
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/4a18ac8e5ecfafaded830f3246ee3953.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4a18ac8e5ecfafaded830f3246ee3953.png)'
- en: 'CAPTION_TO_PHRASE_GROUNDING task with the text input: “lens. camera. table.
    logo. flash.”'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: CAPTION_TO_PHRASE_GROUNDING任务，文本输入为：“镜头。相机。桌子。标志。闪光。”
- en: '3\. Segmentation Related Tasks:'
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. 分割相关任务：
- en: 'Florence-2 can also generate segmentation polygons grounded by text (`''<REFERRING_EXPRESSION_SEGMENTATION>''`)
    or by bounding boxes (`''<REGION_TO_SEGMENTATION>''`):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Florence-2也可以生成由文本（`'<REFERRING_EXPRESSION_SEGMENTATION>'`）或边界框（`'<REGION_TO_SEGMENTATION>'`）约束的分割多边形：
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/d3545d83b63447e4043f6d333e0c4d17.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d3545d83b63447e4043f6d333e0c4d17.png)'
- en: The image on the left shows the results of the REFERRING_EXPRESSION_SEGMENTATION
    task with ‘camera’ text as input. The image on the right demonstrates REGION_TO_SEGMENTATION
    task with a bounding box around the lens provided as input.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的图像展示了使用‘相机’文本作为输入的REFERRING_EXPRESSION_SEGMENTATION任务的结果，右侧的图像展示了使用边界框围绕镜头作为输入的REGION_TO_SEGMENTATION任务的结果。
- en: '4\. OCR Related Tasks:'
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4. OCR 相关任务：
- en: 'Florence-2 demonstrates strong OCR capabilities. It can extract text from an
    image with the `''<OCR>''` task prompt, and extract both text and its location
    with `''<OCR_WITH_REGION>''` :'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Florence-2展示了强大的OCR能力。它可以通过`'<OCR>'`任务提示从图像中提取文本，或者通过`'<OCR_WITH_REGION>'`提取文本及其位置：
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/3cab509233de43f25be60c54ce24c3a2.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3cab509233de43f25be60c54ce24c3a2.png)'
- en: Concluding Remarks
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Florence-2 is a versatile Vision-Language Model (VLM), capable of handling multiple
    vision tasks within a single model. Its zero-shot capabilities are impressive
    across diverse tasks such as image captioning, object detection, segmentation
    and OCR. While Florence-2 performs well out-of-the-box, additional fine-tuning
    can further adapt the model to new tasks or improve its performance on unique,
    custom datasets.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Florence-2是一个多功能的视觉语言模型（VLM），能够在单一模型中处理多种视觉任务。它在图像描述、物体检测、分割和OCR等多种任务中都展示了出色的零-shot能力。虽然Florence-2开箱即用效果良好，但进一步的微调可以让模型适应新任务或在独特的自定义数据集上提高性能。
- en: Thank you for reading!
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: Congratulations on making it all the way here. Click 👍 to show your appreciation
    and raise the algorithm self esteem 🤓
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你一路走到了这里。点击👍表示感谢，并提升算法的自尊心 🤓
- en: '**Want to learn more?**'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**想了解更多吗？**'
- en: '[**Explore**](https://medium.com/@lihigurarie) additional articles I’ve written'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**探索**](https://medium.com/@lihigurarie)我写的其他文章'
- en: '[**Subscribe**](https://medium.com/@lihigurarie/subscribe)to get notified when
    I publish articles'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**订阅**](https://medium.com/@lihigurarie/subscribe)以在我发布文章时收到通知'
- en: Follow me on [**Linkedin**](https://www.linkedin.com/in/lihi-gur-arie/)
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[**Linkedin**](https://www.linkedin.com/in/lihi-gur-arie/)上关注我
- en: 'Full Code as Colab notebook:'
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 完整代码，作为Colab笔记本：
- en: References
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[0] Code on Colab Notebook: [link](https://gist.github.com/Lihi-Gur-Arie/427ecce6a5c7f279d06f3910941e0145)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[0] Colab Notebook中的代码: [link](https://gist.github.com/Lihi-Gur-Arie/427ecce6a5c7f279d06f3910941e0145)'
- en: '[1] Florence-2: [Advancing a Unified Representation for a Variety of Vision
    Tasks](https://arxiv.org/pdf/2311.06242).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Florence-2: [推进统一的表示方法，以应对多种视觉任务](https://arxiv.org/pdf/2311.06242).'
- en: '[2] CLIP: [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020v1).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] CLIP: [从自然语言监督中学习可转移的视觉模型](https://arxiv.org/pdf/2103.00020v1).'
- en: '[3] Grounding DINO: [Marrying DINO with Grounded Pre-Training for Open-Set
    Object Detection](https://arxiv.org/abs/2303.05499).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Grounding DINO: [结合DINO和基础预训练进行开放集物体检测](https://arxiv.org/abs/2303.05499).'
- en: '[4] SAM2: [Segment Anything in Images and Videos](https://arxiv.org/pdf/2408.00714).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] SAM2: [图像和视频中的任何物体分割](https://arxiv.org/pdf/2408.00714).'
- en: '[5] DaViT: [Dual Attention Vision Transformers](https://arxiv.org/abs/2204.03645).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] DaViT: [双重注意力视觉变换器](https://arxiv.org/abs/2204.03645).'
- en: '[6] BART: [Denoising Sequence-to-Sequence Pre-training for Natural Language
    Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] BART: [去噪序列到序列预训练，用于自然语言生成、翻译和理解](https://arxiv.org/pdf/1910.13461).'
