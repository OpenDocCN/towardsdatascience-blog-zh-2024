- en: Meta Llama 3 Optimized CPU Inference with Hugging Face and PyTorch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Meta Llama 3 优化的CPU推理与 Hugging Face 和 PyTorch
- en: 原文：[https://towardsdatascience.com/meta-llama-3-optimized-cpu-inference-with-hugging-face-and-pytorch-9dde2926be5c?source=collection_archive---------2-----------------------#2024-04-19](https://towardsdatascience.com/meta-llama-3-optimized-cpu-inference-with-hugging-face-and-pytorch-9dde2926be5c?source=collection_archive---------2-----------------------#2024-04-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/meta-llama-3-optimized-cpu-inference-with-hugging-face-and-pytorch-9dde2926be5c?source=collection_archive---------2-----------------------#2024-04-19](https://towardsdatascience.com/meta-llama-3-optimized-cpu-inference-with-hugging-face-and-pytorch-9dde2926be5c?source=collection_archive---------2-----------------------#2024-04-19)
- en: '![](../Images/2d381d40cdfb97cc7c50ca03b4b947a3.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d381d40cdfb97cc7c50ca03b4b947a3.png)'
- en: Created with Nightcafe — Image property of Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Nightcafe创建 — 图片由作者提供
- en: Learn how to reduce model latency when deploying Meta* Llama 3 on CPUs
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解如何在部署Meta* Llama 3到CPU时减少模型延迟
- en: '[](https://eduand-alvarez.medium.com/?source=post_page---byline--9dde2926be5c--------------------------------)[![Eduardo
    Alvarez](../Images/8a51c754fdd3362aa82dee5acd2a68c5.png)](https://eduand-alvarez.medium.com/?source=post_page---byline--9dde2926be5c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9dde2926be5c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9dde2926be5c--------------------------------)
    [Eduardo Alvarez](https://eduand-alvarez.medium.com/?source=post_page---byline--9dde2926be5c--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://eduand-alvarez.medium.com/?source=post_page---byline--9dde2926be5c--------------------------------)[![Eduardo
    Alvarez](../Images/8a51c754fdd3362aa82dee5acd2a68c5.png)](https://eduand-alvarez.medium.com/?source=post_page---byline--9dde2926be5c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9dde2926be5c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9dde2926be5c--------------------------------)
    [Eduardo Alvarez](https://eduand-alvarez.medium.com/?source=post_page---byline--9dde2926be5c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9dde2926be5c--------------------------------)
    ·7 min read·Apr 19, 2024
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9dde2926be5c--------------------------------)
    ·阅读时长7分钟·2024年4月19日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: The much-anticipated [release](https://huggingface.co/blog/llama3) of Meta’s
    third-generation batch of Llama is here, and I want to ensure you know how to
    deploy this state-of-the-art (SoTA) LLM optimally. In this tutorial, we will focus
    on performing weight-only-quantization (WOQ) to compress the 8B parameter model
    and improve inference latency, but first, let’s discuss Meta Llama 3.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 万众期待的[Meta第三代Llama发布](https://huggingface.co/blog/llama3)终于来了，我希望确保你知道如何优化地部署这个最先进（SoTA）的LLM。在本教程中，我们将重点介绍执行仅权重量化（WOQ），以压缩8B参数模型并提高推理延迟，但首先，让我们讨论一下Meta
    Llama 3。
- en: Llama 3
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Llama 3
- en: To date, the Llama 3 family includes models ranging from 8B to 70B parameters,
    with more versions coming in the future. The models come with a permissive Meta
    Llama 3 [license](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/blob/main/LICENSE),
    you are encouraged to review before accepting the terms required to use them.
    This marks an exciting chapter for the Llama model family and open-source AI.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，Llama 3家族包括从8B到70B参数不等的模型，未来还会推出更多版本。模型配有宽松的Meta Llama 3[许可协议](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/blob/main/LICENSE)，鼓励在接受使用条款之前仔细审阅。这标志着Llama模型家族和开源AI的一个激动人心的篇章。
- en: Architecture
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构
- en: 'The Llama 3 is an auto-regressive LLM based on a decoder-only transformer.
    Compared to Llama 2, the Meta team has made the following notable improvements:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 3是一个基于解码器单一变压器的自回归LLM。与Llama 2相比，Meta团队做出了以下显著改进：
- en: Adoption of grouped query attention (GQA), which improves inference efficiency.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采用分组查询注意力（GQA），提高推理效率。
- en: Optimized tokenizer with a vocabulary of 128K tokens designed to encode language
    more efficiently.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化的分词器，词汇表包含128K个标记，旨在更高效地编码语言。
- en: Trained on a 15 trillion token dataset, this is 7x larger than Llama 2’s training
    dataset and includes 4x more code.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个15万亿标记的数据集上训练，这个数据集比Llama 2的训练数据集大了7倍，并且包含了4倍更多的代码。
- en: The figure below (Figure 1) is the result of `print(model)` where `model` is
    [meta-llama/Meta-Llama-3–8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct).
    In this figure, we can see that the model comprises 32 LlamaDecoderLayers composed
    of Llama Attention self-attention components. Additionally, it has LlamaMLP, LlamaRMSNorm,
    and a Linear head. We hope to learn more once the Llama 3 research paper is released.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图（图 1）为 `print(model)` 的结果，其中 `model` 为 [meta-llama/Meta-Llama-3–8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)。在该图中，我们可以看到模型由
    32 个 LlamaDecoderLayer 组成，这些层包含了 Llama Attention 自注意力组件。此外，模型还包括 LlamaMLP、LlamaRMSNorm
    和一个线性头。我们希望在 Llama 3 研究论文发布后能够了解更多信息。
- en: '![](../Images/c802b721da904eefda4260786011cf54.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c802b721da904eefda4260786011cf54.png)'
- en: Figure 1\. Output of `print(model)` showcasing the distribution of layers across
    llama-3–8B-instruct’s architecture — Image by Author
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1. `print(model)` 的输出，展示了 llama-3–8B-instruct 架构中各层的分布 — 图片来源：[作者]
- en: Language Modeling Performance
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言建模性能
- en: The model was evaluated on various industry-standard language modeling benchmarks,
    such as MMLU, GPQA, HumanEval, GSM-8K, MATH, and more. For the purpose of this
    tutorial, we will review the performance of the “Instruction Tuned Models” (Figure
    2). The most remarkable aspect of these figures is that the Llama 3 8B parameter
    model outperforms Llama 2 70B by 62% to 143% across the reported benchmarks while
    being an 88% smaller model!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型已在多个行业标准的语言建模基准测试上进行评估，如 MMLU、GPQA、HumanEval、GSM-8K、MATH 等。在本教程中，我们将回顾“指令调优模型”的表现（图
    2）。这些数据的最显著特点是，Llama 3 8B 参数模型在所报告的基准测试中超越了 Llama 2 70B 达 62% 到 143%，同时该模型的规模却比
    Llama 2 小了 88%！
- en: '![](../Images/803024fc44c78d908f4d7fb793e79fed.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/803024fc44c78d908f4d7fb793e79fed.png)'
- en: Figure 2 . Summary of Llama 3 instruction model performance metrics across the
    MMLU, GPQA, HumanEval, GSM-8K, and MATH LLM benchmarks. — Image by Author ([source](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct))
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2. Llama 3 指令模型在 MMLU、GPQA、HumanEval、GSM-8K 和 MATH LLM 基准测试中的性能指标汇总。— 图片来源：[作者](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)
- en: The increased language modeling performance, permissive licensing, and architectural
    efficiencies included with this latest Llama generation mark the beginning of
    a very exciting chapter in the generative AI space. Let’s explore how we can optimize
    inference on CPUs for scalable, low-latency deployments of Llama 3.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 增强的语言建模性能、宽松的许可协议以及此最新 Llama 代代的架构效率标志着生成式 AI 领域一个令人激动的全新篇章的开始。让我们一起探索如何优化 Llama
    3 的推理过程，以便在 CPU 上实现可扩展、低延迟的部署。
- en: Optimizing Llama 3 Inference with PyTorch
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PyTorch 优化 Llama 3 推理
- en: In a [previous article](https://medium.com/towards-data-science/improving-llm-inference-latency-on-cpus-with-model-quantization-28aefb495657),
    I covered the importance of model compression and overall inference optimization
    in developing LLM-based applications. In this tutorial, we will focus on applying
    weight-only quantization (WOQ) to [meta-llama/Meta-Llama-3–8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct).
    WOQ offers a balance between performance, latency, and accuracy, with options
    to quantize to int4 or int8\. A key component of WOQ is the dequantization step,
    which converts int4/in8 weights back to bf16 before computation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在[上一篇文章](https://medium.com/towards-data-science/improving-llm-inference-latency-on-cpus-with-model-quantization-28aefb495657)中，我讨论了模型压缩和总体推理优化在开发基于
    LLM 的应用中的重要性。在本教程中，我们将重点介绍如何对 [meta-llama/Meta-Llama-3–8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)
    应用仅权重量化（WOQ）。WOQ 提供了性能、延迟和准确性之间的平衡，并支持量化为 int4 或 int8。WOQ 的一个关键步骤是解量化，将 int4/int8
    权重转换回 bf16，再进行计算。
- en: '![](../Images/f3035f2396f92ac8f2fbe9af4294e0d2.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3035f2396f92ac8f2fbe9af4294e0d2.png)'
- en: Fig 3\. Simple illustration of weight-only quantization, with pre-quantized
    weights in orange and the quantized weights in green. Note that this depicts the
    initial quantization to int4/int8 and dequantization to fp16/bf16 for the computation
    step. — Image by Author ([source](https://medium.com/towards-data-science/improving-llm-inference-latency-on-cpus-with-model-quantization-28aefb495657))
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3. 简单示意图展示了仅权重量化，其中预量化的权重为橙色，量化后的权重为绿色。请注意，这表示初步量化为 int4/int8 以及计算步骤中的解量化为
    fp16/bf16。— 图片来源：[作者](https://medium.com/towards-data-science/improving-llm-inference-latency-on-cpus-with-model-quantization-28aefb495657)
- en: Environment Setup
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 环境设置
- en: You will need approximately 60GB of RAM to perform WOQ on Llama-3-8B-Instruct.
    This includes ~30GB to load the full model and ~30GB for peak memory during quantization.
    The WOQ Llama 3 will only consume ~10GB of RAM, meaning we can free ~50GB of RAM
    by releasing the full model from memory.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你将需要大约60GB的RAM来执行Llama-3-8B-Instruct的WOQ。这包括约30GB用于加载完整模型，以及约30GB用于量化过程中的峰值内存。WOQ
    Llama 3只会消耗大约10GB的RAM，这意味着我们可以通过将完整模型从内存中释放来释放大约50GB的RAM。
- en: '*You can run this tutorial on the* [*Intel*® *Tiber*® *Developer Cloud*](https://medium.com/r?url=https%3A%2F%2Fbit.ly%2Fllama3woq)
    *free JupyterLab* environment. This environment offers a 4th Generation Intel*®
    *Xeon*® *CPU with 224 threads and 504 GB of memory, more than enough to run this
    code.*'
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*你可以在* [*Intel*® *Tiber*® *开发者云*](https://medium.com/r?url=https%3A%2F%2Fbit.ly%2Fllama3woq)
    *免费的JupyterLab*环境中运行本教程。该环境提供了一款第四代Intel*® *Xeon*® *CPU，拥有224个线程和504GB内存，足以运行此代码。*'
- en: If running this in your own IDE, you may need to address additional dependencies
    like installing Jupyter and/or configuring a conda/python environment. Before
    getting started, ensure that you have the following dependencies installed.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在你自己的IDE中运行，你可能需要处理额外的依赖项，比如安装Jupyter和/或配置conda/python环境。在开始之前，请确保你已经安装了以下依赖项。
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Accessing and Configuring Llama 3
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 访问和配置Llama 3
- en: You will need a Hugging Face* account to access Llama 3’s model and tokenizer.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要一个Hugging Face*帐户来访问Llama 3的模型和分词器。
- en: To do so, select “Access Tokens” from your settings menu (Figure 4) and create
    a token.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，从设置菜单中选择“Access Tokens”（图4），并创建一个令牌。
- en: '![](../Images/b621f7448cbda9124a39953850909676.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b621f7448cbda9124a39953850909676.png)'
- en: Figure 4\. Snapshot of the Hugging Face token configuration console — Image
    by Author
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. Hugging Face令牌配置控制台快照 — 图片由作者提供
- en: Copy your access token and paste it into the “Token” field generated inside
    your Jupyter cell after running the following code.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 复制你的访问令牌并将其粘贴到在运行以下代码后在Jupyter单元格中生成的“Token”字段中。
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Go to [meta-llama/Meta-Llama-3–8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)
    and carefully evaluate the terms and license before providing your information
    and submitting the Llama 3 access request. Accepting the model’s terms and providing
    your information is yours and yours alone.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 访问[meta-llama/Meta-Llama-3–8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)，并在提供你的信息并提交Llama
    3访问请求之前仔细评估条款和许可。接受模型条款并提供你的信息是你自己的责任。
- en: Quantizing Llama-3–8B-Instruct with WOQ
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用WOQ对Llama-3–8B-Instruct进行量化
- en: 'We will leverage the [Intel® Extension for PyTorch](https://medium.com/r?url=https%3A%2F%2Fwww.intel.com%2Fcontent%2Fwww%2Fus%2Fen%2Fdeveloper%2Ftools%2Foneapi%2Foptimization-for-pytorch.html)*
    to apply WOQ to Llama 3\. This extension contains the latest PyTorch optimizations
    for Intel hardware. Follow these steps to quantize and perform inference with
    an optimized Llama 3 model:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用[Intel® Extension for PyTorch](https://medium.com/r?url=https%3A%2F%2Fwww.intel.com%2Fcontent%2Fwww%2Fus%2Fen%2Fdeveloper%2Ftools%2Foneapi%2Foptimization-for-pytorch.html)*来应用WOQ于Llama
    3。这一扩展包含了针对Intel硬件的最新PyTorch优化。按照这些步骤对Llama 3模型进行量化并执行推理：
- en: '**Llama 3 Model and Tokenizer:** Import the required packages and use the `AutoModelForCausalLM.from_pretrained()`
    and `AutoTokenizer.from_pretrained()` methods to load the Llama-3–8B-Instruct
    specific weights and tokenizer.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Llama 3模型和分词器：** 导入所需的包，并使用`AutoModelForCausalLM.from_pretrained()`和`AutoTokenizer.from_pretrained()`方法加载Llama-3–8B-Instruct特定的权重和分词器。'
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 2\. **Quantization Recipe Config:** Configure the WOQ quantization recipe. We
    can set the `weight_dtype` variable to the desired in-memory datatypes, choosing
    from `torch.quint4x2` or `torch.qint8` for int4 and in8, respectively. Additionally
    we can use `lowp_model` to define the dequantization precision. For now, we will
    keep this as `ipex.quantization.WoqLowpMode.None` to keep the default bf16 computation
    precision.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. **量化配置食谱：** 配置WOQ量化食谱。我们可以将`weight_dtype`变量设置为所需的内存中数据类型，分别选择`torch.quint4x2`或`torch.qint8`来表示int4和in8。此外，我们可以使用`lowp_model`来定义去量化精度。现在，我们将其保持为`ipex.quantization.WoqLowpMode.None`，以保持默认的bf16计算精度。
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We use `ipex.llm.optimize()` to apply WOQ and then `del model` to delete the
    full model from memory and free ~30GB of RAM.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`ipex.llm.optimize()`来应用WOQ，然后使用`del model`删除完整模型并释放大约30GB的RAM。
- en: 3\. **Prompting Llama 3:** Llama 3, like LLama 2, has a pre-defined prompting
    template for its instruction-tuned models. Using this template, developers can
    define specific model behavior instructions and provide user prompts and conversation
    history.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 3. **提示Llama 3：** Llama 3与Llama 2一样，拥有预定义的提示模板，用于其经过指令调优的模型。使用这个模板，开发者可以定义具体的模型行为指令，并提供用户提示和对话历史。
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We provide the required fields and then use the tokenizer to convert the entire
    template into tokens for the model.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供所需字段，然后使用分词器将整个模板转换为模型的标记。
- en: 4. **Llama 3 Inference:** For text generation, we leverage `TextStreamer` to
    generate a real-time inference stream instead of printing the entire output at
    once. This results in a more natural text generation experience for readers. We
    provide the configured streamer to `model_ipex.generate()` and other text-generation
    parameters.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 4. **Llama 3 推理：** 在文本生成方面，我们利用`TextStreamer`生成实时推理流，而不是一次性打印整个输出。这使得读者能够获得更加自然的文本生成体验。我们将配置好的流传递给`model_ipex.generate()`以及其他文本生成参数。
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Upon running this code, the model will start generating outputs. Keep in mind
    that these are unfiltered and non-guarded outputs. For real-world use cases, you
    will need to make additional post-processing considerations.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码后，模型将开始生成输出。请记住，这些是未经过滤和未加保护的输出。对于实际应用场景，你需要进行额外的后处理考虑。
- en: '![](../Images/aca89dea56a6712604806222f4b4f7a5.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aca89dea56a6712604806222f4b4f7a5.png)'
- en: Figure 5\. Streamed inference of Llama-3–8B-Instruct with WOQ mode compression
    at int4 running on the Intel Tiber Developer Cloud’s JupyterLab environment —
    Gif by Author
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图5. 在Intel Tiber Developer Cloud的JupyterLab环境中运行int4模式下压缩的Llama-3–8B-Instruct流式推理——Gif由作者提供
- en: That’s it. With less than 20 lines of code, you now have a low-latency CPU optimized
    version of the latest SoTA LLM in the ecosystem.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。不到20行代码，你就能在这个生态系统中获得最新SoTA LLM的低延迟、CPU优化版本。
- en: Considerations for Deployment
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署时的注意事项
- en: 'Depending on your inference service deployment strategy, there are a few things
    that you will want to consider:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的推理服务部署策略，有一些事项是你需要考虑的：
- en: If deploying instances of Llama 3 in containers, WOQ will offer a smaller memory
    footprint and allow you to serve multiple inference services of the model on a
    single hardware node.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果在容器中部署Llama 3实例，WOQ将提供更小的内存占用，并允许你在单一硬件节点上提供多个推理服务实例。
- en: When deploying multiple inference services, you should optimize the threads
    and memory reserved for each service instance. Leave enough additional memory
    (~4 GB) and threads (~4 threads) to handle background processes.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在部署多个推理服务时，你应该优化每个服务实例分配的线程和内存。为后台进程留出足够的额外内存（约4GB）和线程（约4个线程）。
- en: Consider saving the WOQ version of the model and storing it in a model registry
    to eliminate the need to re-quantize the model per instance deployment.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑将WOQ版本的模型保存并存储在模型注册表中，以避免在每个实例部署时重新量化模型。
- en: Conclusion and Discussion
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论与讨论
- en: Meta’s Llama 3 LLM family delivers remarkable improvements over previous generations
    with a diverse range of configurations (more coming soon). In this tutorial, we
    explored enhancing CPU inference with weight-only quantization (WOQ), a technique
    that reduces latency with minimal impacts to accuracy.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Meta的Llama 3 LLM家族相比前几代有了显著的改进，并且提供了多种配置（更多配置即将推出）。在本教程中，我们探讨了通过仅权重量化（WOQ）来增强CPU推理，这是一种在最小化准确性影响的同时减少延迟的技术。
- en: By integrating the new generation of performance-oriented Llama 3 LLMs with
    optimization techniques like WOQ, developers can unlock new possibilities for
    [GenAI](https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/training/generative-ai.html)
    applications. This combination simplifies the hardware requirements to achieve
    high-fidelity, low-latency results from LLMs integrated into new and existing
    systems.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将以性能为导向的新一代Llama 3 LLM与像WOQ这样的优化技术结合，开发者可以为[GenAI](https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/training/generative-ai.html)应用解锁新的可能性。这个组合简化了硬件需求，使LLM能够在新旧系统中集成并获得高保真、低延迟的结果。
- en: '**A few exciting things to try next would be:**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**接下来可以尝试的一些令人兴奋的事情是：**'
- en: '**Experiment with Quantization Levels:** You should test int4 and int8 quantization
    to identify the best compromise between performance and accuracy for your specific
    applications.'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**实验不同的量化级别：** 你应该测试int4和int8量化，以确定在性能和准确性之间的最佳平衡，适应你的特定应用。'
- en: '**Performance Monitoring:** It is crucial to continuously assess the performance
    and accuracy of the Llama 3 model across different real-world scenarios to ensure
    that quantization maintains the desired effectiveness.'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**性能监控：** 持续评估Llama 3模型在不同真实场景中的性能和准确性至关重要，以确保量化保持预期的有效性。'
- en: '**Test more Llamas:** Explore the entire Llama 3 family and evaluate the impact
    of WOQ and other PyTorch [quantization recipes](https://pytorch.org/docs/stable/quantization.html).'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**测试更多的Llama模型：** 探索整个Llama 3系列，并评估WOQ和其他PyTorch的[量化方法](https://pytorch.org/docs/stable/quantization.html)的影响。'
- en: '***Thank you for reading! Don’t forget to follow*** [***my profile for more
    articles***](https://eduand-alvarez.medium.com/) ***like this!***'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '***感谢阅读！别忘了关注*** [***我的个人资料以获取更多类似文章***](https://eduand-alvarez.medium.com/)
    ***！***'
- en: '**Other names and brands may be claimed as the property of others.*'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**其他名称和品牌可能被声明为他人的财产。**'
