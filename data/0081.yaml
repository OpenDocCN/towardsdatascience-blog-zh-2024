- en: From Adaline to Multilayer Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从 Adaline 到多层神经网络
- en: 原文：[https://towardsdatascience.com/from-adaline-to-multilayer-neural-networks-e115e65fee3e?source=collection_archive---------8-----------------------#2024-01-09](https://towardsdatascience.com/from-adaline-to-multilayer-neural-networks-e115e65fee3e?source=collection_archive---------8-----------------------#2024-01-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/from-adaline-to-multilayer-neural-networks-e115e65fee3e?source=collection_archive---------8-----------------------#2024-01-09](https://towardsdatascience.com/from-adaline-to-multilayer-neural-networks-e115e65fee3e?source=collection_archive---------8-----------------------#2024-01-09)
- en: Setting the foundations right
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 打好基础
- en: '[](https://medium.com/@cretanpan?source=post_page---byline--e115e65fee3e--------------------------------)[![Pan
    Cretan](../Images/8b3fbab70c0e61f7ca516d2f54b646e5.png)](https://medium.com/@cretanpan?source=post_page---byline--e115e65fee3e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e115e65fee3e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e115e65fee3e--------------------------------)
    [Pan Cretan](https://medium.com/@cretanpan?source=post_page---byline--e115e65fee3e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@cretanpan?source=post_page---byline--e115e65fee3e--------------------------------)[![Pan
    Cretan](../Images/8b3fbab70c0e61f7ca516d2f54b646e5.png)](https://medium.com/@cretanpan?source=post_page---byline--e115e65fee3e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e115e65fee3e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e115e65fee3e--------------------------------)
    [Pan Cretan](https://medium.com/@cretanpan?source=post_page---byline--e115e65fee3e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e115e65fee3e--------------------------------)
    ·23 min read·Jan 9, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e115e65fee3e--------------------------------)
    ·阅读时长 23 分钟·2024年1月9日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/b8f71ed9e1d173f61b5f8ea765b06d34.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b8f71ed9e1d173f61b5f8ea765b06d34.png)'
- en: Photo by [Konta Ferenc](https://unsplash.com/@fr3dd87?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Konta Ferenc](https://unsplash.com/@fr3dd87?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In the previous two articles we saw how we can implement a basic classifier
    based on Rosenblatt’s [perceptron](https://medium.com/towards-data-science/classification-with-rosenblatts-perceptron-e7f49e3af562)
    and how this classifier can be improved by using the adaptive linear neuron algorithm
    ([adaline](https://medium.com/towards-data-science/from-the-perceptron-to-adaline-1730e33d41c5)).
    These two articles cover the foundations before attempting to implement an artificial
    neural network with many layers. Moving from adaline to deep learning is a bigger
    leap and many machine learning practitioners will opt directly for an open source
    library like [PyTorch](https://pytorch.org/). Using such a specialised machine
    learning library is of course recommended for developing a model in production,
    but not necessarily for learning the fundamental concepts of multilayer neural
    networks. This article builds a multilayer neural network from scratch. Instead
    of solving a binary classification problem we will focus on a multiclass one.
    We will be using the sigmoid activation function after each layer, including the
    output one. Essentially we train a model that for each input, comprising a vector
    of features, produces a vector with length equal to the number of classes to be
    predicted. Each element of the output vector is in the range [0, 1] and can be
    understood as the “probability” of each class.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两篇文章中，我们看到如何基于 Rosenblatt 的 [感知机](https://medium.com/towards-data-science/classification-with-rosenblatts-perceptron-e7f49e3af562)
    实现一个基本的分类器，以及如何通过使用自适应线性神经元算法（[adaline](https://medium.com/towards-data-science/from-the-perceptron-to-adaline-1730e33d41c5)）来改进这个分类器。这两篇文章涵盖了在尝试实现一个多层人工神经网络之前的基础知识。从
    Adaline 过渡到深度学习是一个更大的飞跃，许多机器学习从业者会直接选择使用像 [PyTorch](https://pytorch.org/) 这样的开源库。使用这样的专业机器学习库，当然推荐用于开发生产中的模型，但并不一定适合用于学习多层神经网络的基本概念。本文将从零开始构建一个多层神经网络。我们将聚焦于一个多分类问题，而不是解决一个二分类问题。我们将在每一层后，包括输出层，使用
    Sigmoid 激活函数。本质上，我们训练一个模型，对于每个输入，包含一组特征的向量，输出一个长度等于待预测类别数的向量。输出向量的每个元素都在 [0, 1]
    范围内，并可以理解为每个类别的“概率”。
- en: The purpose of the article is to become comfortable with the mathematical notation
    used for describing mathematically neural networks, understand the role of the
    various matrices with weights and biases, and derive the formulas for updating
    the weights and biases to minimise the loss function. The implementation allows
    for any number of hidden layers with arbitrary dimensions. Most tutorials assume
    a fixed architecture but this article uses a carefully chosen mathematical notation
    that supports generalisation. In this way we can also run simple numerical experiments
    to examine the predictive performance as a function of the number and size of
    the hidden layers.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的目的是让读者熟悉用于描述神经网络的数学符号，理解各种矩阵（包含权重和偏差）的作用，并推导更新权重和偏差以最小化损失函数的公式。该实现允许使用任意数量的隐藏层，且每层的维度可以自定义。大多数教程假设一个固定的架构，但本文使用了精心挑选的数学符号，这些符号支持泛化。通过这种方式，我们还可以进行简单的数值实验，检验预测性能与隐藏层的数量和大小之间的关系。
- en: As in the earlier articles, I used the online [LaTeX equation editor](https://latexeditor.lagrida.com/)
    to develop the LaTeX code for the equation and then the chrome plugin [Maths Equations
    Anywhere](https://chromewebstore.google.com/detail/math-equations-anywhere/fkioioejambaepmmpepneigdadjpfamh)
    to render the equation into an image. All LaTex code is provided at the end of
    the article if you need to render it again. Getting the notation right is part
    of the journey in machine learning, and essential for understanding neural networks.
    It is vital to scrutinise the formulas, and pay attention to the various indices
    and the rules for matrix multiplication. Implementation in code becomes trivial
    once the model is correctly formulated on paper.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的文章一样，我使用了在线[LaTeX公式编辑器](https://latexeditor.lagrida.com/)来编写公式的LaTeX代码，然后使用Chrome插件[Maths
    Equations Anywhere](https://chromewebstore.google.com/detail/math-equations-anywhere/fkioioejambaepmmpepneigdadjpfamh)将公式渲染成图片。所有的LaTeX代码将在文章末尾提供，方便你再次渲染。如果你需要渲染公式，正确使用符号是机器学习中的一个关键部分，且对于理解神经网络至关重要。必须仔细审查公式，并注意各种指标和矩阵乘法的规则。模型在纸面上的正确表述一旦完成，实现代码变得非常简单。
- en: All code used in the article can be found in the accompanying [repository](https://github.com/karpanGit/myBlogs/tree/master/MultilayerNeuralNetworks).
    The article covers the following topics
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中使用的所有代码可以在附带的[代码库](https://github.com/karpanGit/myBlogs/tree/master/MultilayerNeuralNetworks)中找到。本文涵盖以下主题：
- en: ∘ [What is a multilayer neural network?](#14a0)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [什么是多层神经网络？](#14a0)
- en: ∘ [Activation](#48ed)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [激活函数](#48ed)
- en: ∘ [Loss function](#7fda)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [损失函数](#7fda)
- en: ∘ [Backpropagation](#d970)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [反向传播](#d970)
- en: ∘ [Implementation](#5f28)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [实现](#5f28)
- en: ∘ [Dataset](#2f55)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [数据集](#2f55)
- en: ∘ [Training the model](#431d)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [训练模型](#431d)
- en: ∘ [Hyperparameter tuning](#15f6)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [超参数调优](#15f6)
- en: ∘ [Conclusions](#eb5c)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [结论](#eb5c)
- en: ∘ [LaTeX code of equations used in the article](#9aa7)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [文章中使用的LaTeX公式代码](#9aa7)
- en: What is a multilayer neural network?
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是多层神经网络？
- en: This section introduces the architecture of a generalised, feedforward, fully-connected
    multilayer neural network. There are a lot of terms to go through here as we work
    our way through Figure 1 below.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了一个通用的、前馈式的、全连接的多层神经网络架构。随着我们逐步讲解下面的图1，这里有很多术语需要理解。
- en: For every prediction, the network accepts a vector of features as input
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一个预测，网络接受一个特征向量作为输入。
- en: '![](../Images/6dabd645b20fbd402155f26246eebf24.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6dabd645b20fbd402155f26246eebf24.png)'
- en: that can also be understood as a matrix with shape (1, n⁰). The network uses
    L layers and produces a vector as an output
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这也可以理解为一个形状为(1, n⁰)的矩阵。网络使用L层并产生一个向量作为输出。
- en: '![](../Images/95809ead8c78a7209cd11a22846181b9.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/95809ead8c78a7209cd11a22846181b9.png)'
- en: that can be understood as a matrix with shape (1, nᴸ) where nᴸ is the number
    of classes in the multiclass classification problem we need to solve. Every float
    in this matrix lies in the range [0, 1] and the index of the largest element corresponds
    to the predicted class. The (L) notation in the superscript is used to refer to
    a particular layer, in this case the last one.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以理解为一个形状为(1, nᴸ)的矩阵，其中nᴸ是我们需要解决的多类分类问题中的类别数。该矩阵中的每一个浮点数都位于[0, 1]的范围内，最大元素的索引对应于预测的类别。上标中的(L)符号用于指代特定层次，这里指的是最后一层。
- en: But how do we generate this prediction? Let’s focus on the first element of
    the first layer (the input is not considered a layer)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们如何生成这个预测呢？让我们集中在第一层的第一个元素（输入不算作一层）。
- en: '![](../Images/1c601c9aca303ea8b14cda3be7db7b51.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c601c9aca303ea8b14cda3be7db7b51.png)'
- en: We first compute the net input that is essentially an inner product of the input
    vector with a set of weights with the addition of a bias term. The second operation
    is the application of the activation function σ(z) to which we will return later.
    For now it is important to keep in mind that the activation function is essentially
    a scalar operation.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先计算网络输入，这本质上是输入向量与一组权重的内积，并加上一个偏置项。第二个操作是应用激活函数σ(z)，稍后我们将回到这个问题。目前需要记住的是，激活函数本质上是一个标量操作。
- en: We can compute all elements of the first layer in the same way
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以以相同的方式计算第一层的所有元素
- en: '![](../Images/0a47528924ac72826b89fc6aaadaec15.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a47528924ac72826b89fc6aaadaec15.png)'
- en: From the above we can deduce that we have introduced n¹ x n⁰ weights and n¹
    bias terms that will need to be fitted when the model is trained. These calculations
    can also be expressed in matrix form
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述内容我们可以推断，我们引入了n¹ x n⁰个权重和n¹个偏置项，这些将在模型训练时进行拟合。这些计算也可以用矩阵形式表示
- en: '![](../Images/713a0b377225ff41355e489e2cc46e0b.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/713a0b377225ff41355e489e2cc46e0b.png)'
- en: Pay close attention to the shape of the matrices. The net output is a result
    of a matrix multiplication of two matrices with shape (1, n⁰) and (n⁰, n¹) that
    results in a matrix with shape (1, n¹), to which we add another matrix with the
    bias terms that has the same (1, n¹) shape. Note that we introduced the transpose
    of the weight matrix. The activation function applies to every element of this
    matrix and hence the activated values of layer 1 are also a matrix with shape
    (1, n¹).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 注意矩阵的形状。网络输出是两个形状分别为(1, n⁰)和(n⁰, n¹)的矩阵相乘的结果，得到形状为(1, n¹)的矩阵，再加上另一个形状为(1, n¹)的偏置项矩阵。注意我们引入了权重矩阵的转置。激活函数应用于该矩阵的每个元素，因此第1层的激活值也是形状为(1,
    n¹)的矩阵。
- en: '![](../Images/6f0b228ffacfcb6ffd8fac45fc0181a7.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f0b228ffacfcb6ffd8fac45fc0181a7.png)'
- en: 'Figure 1: A general multilayer neural network with an arbitrary number of input
    features, number of output classes and number of hidden layers with different
    number of nodes (image by the Author)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：一个具有任意数量输入特征、输出类别和不同节点数隐藏层的通用多层神经网络（图像由作者提供）
- en: The above can be readily generalised for every layer in the neural network.
    Layer k accepts as input nᵏ⁻¹ values and produces nᵏ activated values
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 上述内容可以很容易地推广到神经网络中的每一层。第k层接受nᵏ⁻¹个值作为输入，并生成nᵏ个激活值
- en: '![](../Images/1a2b0d0fc68d1ef5891114ad87975343.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a2b0d0fc68d1ef5891114ad87975343.png)'
- en: Layer k introduces nᵏ x nᵏ⁻¹ weights and nᵏ bias terms that will need to be
    fitted when the model is trained. The total number of weights and bias terms is
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 第k层引入nᵏ x nᵏ⁻¹个权重和nᵏ个偏置项，这些将在模型训练时进行拟合。权重和偏置项的总数为
- en: '![](../Images/62393e822cd89c09f35328a18dcf52b7.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62393e822cd89c09f35328a18dcf52b7.png)'
- en: so if we assume an input vector with 784 elements (dimension of a low resolution
    image in gray scale), a single hidden layer with 50 nodes and 10 classes in the
    output we need to optimise 785*50+51*10 = 39,760 parameters. The number of parameters
    grows further if we increase the number of hidden layers and the number of nodes
    in these layers. Optimising an objective function with so many parameters is not
    a trivial undertaking and this is why it took some time from the time adaline
    was introduced until we discovered how to train deep networks in the mid 80s.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们假设输入向量有784个元素（低分辨率灰度图像的维度），一个隐藏层有50个节点，输出有10个类别，那么我们需要优化785*50 + 51*10
    = 39,760个参数。如果增加隐藏层的数量和每层节点的数量，参数的数量会进一步增加。优化一个包含如此多参数的目标函数并非易事，这也是为什么从adaline引入到80年代中期我们才发现如何训练深度网络。
- en: This section essentially covers what is known as the forward pass, i.e. how
    we apply a series of matrix multiplications, matrix additions and element wise
    activations to convert the input vector to an output vector. If you pay close
    attention we assumed that the input was a single sample represented as a matrix
    with shape (1, n⁰). The notation holds even if we we feed into the network a batch
    of samples represented as a matrix with shape (N, n⁰). There is only small complexity
    when it comes to the bias terms. If we focus on the first layer we sum a matrix
    with shape (N, n¹) to a bias matrix with shape (1, n¹). For this to work the bias
    matrix has its first row replicated as many times as the number of samples in
    the batch we use in the forward pass. This is such a natural operation that [NumPy](https://numpy.org/)
    does it automatically in what is called [broadcasting](/numpy-broadcasting-4c4cb9dff1e7).
    When we apply forward pass to a batch of inputs it is perhaps cleaner to use capital
    letters for all vectors that become matrices, i.e.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 本节主要介绍所谓的前向传播（forward pass），即如何应用一系列矩阵乘法、矩阵加法和逐元素激活，将输入向量转换为输出向量。如果你仔细观察，我们假设输入是一个单一样本，表示为形状为
    (1, n⁰) 的矩阵。即使我们将一个样本批次作为形状为 (N, n⁰) 的矩阵输入到网络中，这种表示法依然成立。唯一稍微复杂一些的是偏置项。如果我们关注第一层，我们将形状为
    (N, n¹) 的矩阵与形状为 (1, n¹) 的偏置矩阵相加。为了使这一步骤正常工作，偏置矩阵的第一行会被复制多次，直到与批次中样本的数量相等。这是一个非常自然的操作，[NumPy](https://numpy.org/)
    在所谓的[广播](https://numpy.org/doc/stable/reference/generated/numpy.broadcast.html)中会自动执行。当我们对一批输入应用前向传播时，可能更清晰的做法是将所有变成矩阵的向量用大写字母表示，即
- en: '![](../Images/a4fdb81338c9c192ddf1dcb2f312c2b9.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4fdb81338c9c192ddf1dcb2f312c2b9.png)'
- en: Note that I assumed that broadcasting was applied to the bias terms leading
    to a matrix with as many rows as the number of samples in the batch.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我假设广播（broadcasting）已应用于偏置项，从而导致一个矩阵的行数与批次中样本的数量相同。
- en: Operating with batches is typical with deep neural networks. We can see that
    as the number of samples N increases we will need more memory to store the various
    matrices and carry out the matrix multiplications. In addition, using only part
    of training set for updating the weights means we will be updating the parameters
    several times in each pass of the training set (epoch) leading to faster convergence.
    There is an additional benefit that is perhaps less obvious. The network uses
    activation functions that, unlike the activation in adaline, are not the identity.
    In fact they are not even linear, which makes the loss function non convex. Using
    batches introduces noise that is believed to help escaping shallow local minima.
    A suitably chosen learning rate further assists with this.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度神经网络中，通常使用批处理操作。我们可以看到，随着样本数量N的增加，我们需要更多的内存来存储各种矩阵并进行矩阵乘法运算。此外，仅使用部分训练集来更新权重意味着我们将在每次遍历训练集（epoch）时多次更新参数，从而加快收敛速度。还有一个额外的好处，可能不那么明显。网络使用的激活函数与
    Adaline 中的激活函数不同，它们不是恒等函数。实际上，它们甚至不是线性的，这使得损失函数是非凸的。使用批处理引入的噪声被认为有助于逃脱浅层局部最小值。适当选择的学习率进一步帮助实现这一目标。
- en: As a final note before we move on, the term feedforward comes from the fact
    that each layer is using as input the output of the previous layer without using
    loops that lead to the so-called recurrent neural networks.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，最后需要说明的是，“前馈”（feedforward）一词来源于每一层使用前一层的输出作为输入，而没有使用导致所谓递归神经网络的循环。
- en: Activation
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活
- en: Enabling the neural network to solve complex problem requires introducing some
    form of nonlinearity. This is achieved by using an activation function in each
    layer. There are many choices. For this article we will be using the sigmoid (logistic)
    activation function that we can visualise with
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使神经网络能够解决复杂问题需要引入某种形式的非线性。通过在每一层中使用激活函数来实现这一点。选择有很多种。本文将使用 Sigmoid（逻辑）激活函数，我们可以通过以下方式进行可视化：
- en: that produces
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 产生
- en: '![](../Images/62c64572970b8e726c169a4978ebb6e1.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62c64572970b8e726c169a4978ebb6e1.png)'
- en: 'Figure 2: Sigmoid (logistic) activation function. Image by the Author.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：Sigmoid（逻辑）激活函数。图片由作者提供。
- en: The code also includes all imports we will need throughout the article.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 代码还包括了本文中将需要的所有导入库。
- en: The activation function maps any float to the range 0 to 1\. In reality the
    sigmoid is a more suitable activation of the final layer for binary classification
    problems. For multiclass problems it would have been more appropriate to use [softmax](/sigmoid-and-softmax-functions-in-5-minutes-f516c80ea1f9)
    to normalize the output of the neural network to a probability distribution over
    predicted output classes. One way to think about this is that softmax enforces
    that post activation the sum of the entries of the output vector must add up to
    1, that is not the case with sigmoid. Another way to think about it is that the
    sigmoid essentially converts the logits (log odds) to a one-versus-all (OvA) probability.
    Still, we will use the sigmoid activation function to stay as close as possible
    to adaline because the softmax is not an element wise operation and this will
    introduce some complexities in the back propagation algorithm. I leave this as
    an exercise for the reader.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数将任何浮动值映射到 0 到 1 的范围。实际上，Sigmoid 更适合用于二分类问题的最后一层激活。对于多类别问题，使用 [softmax](/sigmoid-and-softmax-functions-in-5-minutes-f516c80ea1f9)
    将神经网络的输出归一化为预测输出类别的概率分布会更为合适。可以这样理解，softmax 强制要求激活后输出向量的所有项之和为 1，而 Sigmoid 则没有这个要求。另一种理解方式是，Sigmoid
    本质上将对数几率（log odds）转换为一对多（OvA）概率。然而，我们将继续使用 Sigmoid 激活函数，以尽量保持与 Adaline 的一致，因为
    Softmax 不是逐元素操作，这会在反向传播算法中引入一些复杂性。这个部分留给读者作为练习。
- en: Loss function
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数
- en: The loss function used for adaline was the mean square error. In practice a
    multiclass classification problem would use a multiclass cross-entropy loss. In
    order to remain as close to adaline as possible, and to facilitate the analytical
    calculation of the gradients of the loss function with respect to the parameters,
    we will stick on the mean square error loss function. Every sample in the training
    set, belongs to one of the nᴸ classes and hence the loss function can be expressed
    as
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 用于 Adaline 的损失函数是均方误差。在实际应用中，多类别分类问题通常使用多类别交叉熵损失。为了尽量保持与 Adaline 的一致性，并便于对损失函数关于参数的梯度进行解析计算，我们将坚持使用均方误差损失函数。训练集中的每个样本都属于
    nᴸ 类中的一种，因此损失函数可以表示为
- en: '![](../Images/43cd4bb53dcd48571173bf46019f5b23.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/43cd4bb53dcd48571173bf46019f5b23.png)'
- en: where the first summation is over all samples and the second over classes. The
    above implies that the known class for sample i has been converted to a one-hot-encoding,
    i.e. a matrix with shape (1, nᴸ) containing zeros apart from the element that
    corresponds to the sample class that is one. We adopted one more notation convention
    so that [j] in the superscript is used to refer to sample j. The summation above
    does not need to use all samples in the training set. In practice it will be applied
    in batches of N’ samples with N’<<N.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，第一个求和是对所有样本进行求和，第二个求和是对类别进行求和。上述公式意味着，样本 i 的已知类别已被转换为独热编码，即一个形状为 (1, nᴸ)
    的矩阵，除了对应于样本类别的元素为 1 外，其他元素都是 0。我们采用了另一种符号约定，在上标中使用 [j] 来表示样本 j。上述求和不需要使用训练集中的所有样本。实际上，它会以批量
    N’ 样本的形式应用，其中 N’<<N。
- en: Backpropagation
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反向传播
- en: The loss function is a scalar that depends on tens or hundreds of thousands
    of parameters, comprising weights and bias terms. Typically, these parameters
    are initialised with random numbers and are updated iteratively so that the loss
    function is minimised using the gradient of the loss function with regard to each
    parameter. In the case of adaline, the analytical derivation of the gradients
    was straightforward. For multilayer neural networks the derivation is more involved
    but remains tractable if we adopt a clever strategy. We enter the world of the
    back propagation but fear not. Backpropagation essentially boils down to a successive
    application of the chain differentiation rule from the right to the left.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数是一个标量，依赖于成千上万个参数，包括权重和偏差项。通常，这些参数通过随机数初始化，并通过迭代更新，以便通过损失函数相对于每个参数的梯度来最小化损失函数。在
    Adaline 的情况下，梯度的解析推导是直接的。而对于多层神经网络，推导过程则更加复杂，但如果我们采用巧妙的策略，仍然是可以处理的。我们进入了反向传播的世界，但不用担心。反向传播实际上是链式求导法则从右到左的连续应用。
- en: Let’s come back to the loss function. It depends on the activated values of
    the last layer, so we can first compute the derivatives with regard to those
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到损失函数。它依赖于最后一层的激活值，因此我们可以首先计算关于这些激活值的导数。
- en: '![](../Images/7e8f729f6d41f269acfe1bd97fc7bdb3.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e8f729f6d41f269acfe1bd97fc7bdb3.png)'
- en: The above can be understood as the (j, i) element of a derivate matrix with
    shape (N, nᴸ) and can be written in matrix form as
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 上述可以理解为导数矩阵的(j, i)元素，矩阵形状为(N, nᴸ)，可以写成矩阵形式为
- en: '![](../Images/17b39acd1aa2c9eda84bee95e10d7d5b.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17b39acd1aa2c9eda84bee95e10d7d5b.png)'
- en: 'where both matrices in the right hand side have shape (N, nᴸ). The activated
    values of the last layer are computed by applying the sigmoid activation function
    on each element of the net input matrix of the last layer. Hence, to compute the
    derivatives of the loss function with regard to each element of this net input
    matrix of the last layer we simply need to remind ourselves on how to compute
    the derivative of a nested function with the outer one being the sigmoid function:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 其中右侧的两个矩阵形状都是(N, nᴸ)。最后一层的激活值是通过对最后一层净输入矩阵的每个元素应用sigmoid激活函数计算得到的。因此，为了计算损失函数对该净输入矩阵每个元素的导数，我们只需回忆如何计算嵌套函数的导数，外层函数为sigmoid函数：
- en: '![](../Images/3184ea54722f9a63720f945d149d8099.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3184ea54722f9a63720f945d149d8099.png)'
- en: The star multiplication denotes element wise multiplication. The result of this
    formula is a matrix with shape (N, nᴸ). If you have difficulties computing the
    derivative of the sigmoid function please check [here](https://en.wikipedia.org/wiki/Logistic_function).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 星号乘法表示逐元素相乘。该公式的结果是一个形状为(N, nᴸ)的矩阵。如果你在计算sigmoid函数的导数时遇到困难，请查看[这里](https://en.wikipedia.org/wiki/Logistic_function)。
- en: We are now ready to compute the derivative of the loss function with regard
    to the weights of the L-1 layer; this is the first set of weights we encounter
    when we move from right to left
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好计算损失函数对L-1层权重的导数；这是我们从右到左移动时遇到的第一组权重
- en: '![](../Images/89b3c727cfb18427025627e303c5923c.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89b3c727cfb18427025627e303c5923c.png)'
- en: This leads to a matrix with the same shape as the weights of the L-1 layer.
    We next need to compute the derivative of the net input of the L layer with regard
    to the weights of the L-1 layer. If we pick one element of the net input matrix
    of the last layer and one of these weights we have
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了一个与L-1层的权重形状相同的矩阵。接下来，我们需要计算L层的净输入对L-1层权重的导数。如果我们选取最后一层净输入矩阵的一个元素和其中一个权重，我们有
- en: '![](../Images/02b9f932d9ab82710feaf6ac8f0fc82b.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02b9f932d9ab82710feaf6ac8f0fc82b.png)'
- en: If you have trouble to understand the above, think that for every sample j,
    the i element of the net input of the L layer only depends on the weights of the
    L-1 layer for which the first index is also i. Hence, we can eliminate one of
    the summations in the derivative
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你难以理解上述内容，可以想象对于每个样本j，L层净输入的i元素仅依赖于L-1层的权重，其中第一个索引也是i。因此，我们可以消除导数中的一个求和项。
- en: '![](../Images/0e7ebc31a8dbc7e81382424307d664b3.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e7ebc31a8dbc7e81382424307d664b3.png)'
- en: We can express all these derivatives in a matrix notation using
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用矩阵表示法来表达所有这些导数，使用
- en: '![](../Images/5e1edec3574bea4b375e321b1c82c402.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e1edec3574bea4b375e321b1c82c402.png)'
- en: Essentially the implicit summation in the matrix multiplication absorbs the
    summation over the samples. Follow along with the shapes of the multiplied matrices
    and you will see that the resulting derivative matrix has the same shape as the
    weight matrix used to calculate the net input of the L layer. Although the number
    of elements in the resulting matrix is limited to the product of the number of
    nodes of the last two layers (the shape is (nᴸ, nᴸ⁻¹)), the multiplied matrices
    are much larger and hence are typically more memory consuming. Hence, the need
    to use batches when training the model.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，矩阵乘法中的隐式求和吸收了对样本的求和。跟随乘法矩阵的形状，你会看到结果导数矩阵的形状与用于计算L层净输入的权重矩阵的形状相同。尽管结果矩阵中的元素个数仅限于最后两层节点数的乘积（形状为(nᴸ,
    nᴸ⁻¹)），但乘法矩阵要大得多，因此通常会消耗更多的内存。因此，在训练模型时需要使用批处理。
- en: The derivatives of the loss function with respect to the bias terms used for
    calculating the net input of the last layer can be computed similarly as for the
    weights to give
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数对用于计算最后一层净输入的偏置项的导数可以像对权重的导数一样计算，得到
- en: '![](../Images/3934777a724a4a9be0a877917bbf1be5.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3934777a724a4a9be0a877917bbf1be5.png)'
- en: that leads to a matrix with shape (1, nᴸ).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致一个形状为(1, nᴸ)的矩阵。
- en: We have just computed all derivatives of the loss function with regard to the
    weights and bias terms used for computing the net input of the last layer. We
    now turn our attention to the gradients with the regard to the weights and bias
    terms of the previous layer (these parameters will have the superscript index
    L-2). Hopefully we can start identifying patterns so that we can apply them to
    compute the derivates with regard to the weights and bias terms for k=0,..,L-2\.
    We could see these patterns emerge if we compute the derivative of the loss function
    with regard to the activated values of the L-1 layer. These should form a matrix
    with shape (N, nᴸ⁻¹) that is computed as
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚计算了所有关于用于计算最后一层净输入的权重和偏置项的损失函数导数。现在，我们将注意力转向前一层的权重和偏置项的梯度（这些参数将带有上标索引L-2）。希望我们能开始识别一些模式，以便将它们应用于计算关于k=0,..,L-2的权重和偏置项的导数。如果我们计算损失函数关于L-1层激活值的导数，就能看到这些模式的出现。这些导数应形成一个形状为(N,
    nᴸ⁻¹)的矩阵，计算公式如下：
- en: '![](../Images/fa8be214ae7ba029753e82575174d717.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa8be214ae7ba029753e82575174d717.png)'
- en: Once we have the derivatives of the loss with regard to the activated values
    of layer L-1 we can proceed with calculating the derivatives of the loss function
    with regard to the net input of the layer L-1 and then with regard to the weights
    and bias terms with index L-2.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们得到了损失关于L-1层激活值的导数，就可以继续计算损失函数关于L-1层净输入的导数，然后再计算损失函数关于L-2层权重和偏置项的导数。
- en: 'Let’s recap how we backpropagate by one layer. We assume we have computed the
    derivative of the loss function with regard to the weights and bias terms with
    index k and we need to compute the derivates of the loss function with regard
    to the weights and bias terms with index k-1\. We need to carry out 4 operations:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下如何通过一层进行反向传播。我们假设已经计算了损失函数关于权重和偏置项（索引为k）的导数，现在需要计算损失函数关于权重和偏置项（索引为k-1）的导数。我们需要执行四个操作：
- en: '![](../Images/e532d96ea2523945e85fa02b34245556.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e532d96ea2523945e85fa02b34245556.png)'
- en: All operations are vectorised. We can already start imaging how we could implement
    these operations in a class. My understanding is that when one uses a specialised
    library to add a fully connected linear layer with an activation function, this
    is what happens behind the scenes! It is nice not to worry about the mathematical
    notation, but my suggestion would be to go through these derivations at least
    once.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 所有操作都已向量化。我们已经可以开始设想如何在一个类中实现这些操作了。我理解的是，当使用专门的库添加一个完全连接的线性层并带有激活函数时，这背后发生的就是这些操作！不必担心数学符号非常方便，但我的建议是至少一次通过这些推导。
- en: Implementation
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现
- en: In this section we provide the implementation of a generalised, feedforward,
    multilayer neural network. The API draws some analogies to the one found in specialised
    deep learning libraries such as PyTorch
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们提供了一个通用的前馈多层神经网络的实现。这个API与专门深度学习库（如PyTorch）中的API有些相似。
- en: 'The code contains two utility functions: `sigmoid()` applies the sigmoid (logistic)
    activation function to a float (or NumPy array), and `int_to_onehot()` takes a
    list of integers with the class of each sample and returns their one-hot-encoding
    representation.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 代码包含两个实用函数：`sigmoid()`将Sigmoid（逻辑）激活函数应用于一个浮动值（或NumPy数组），而`int_to_onehot()`接受一个包含每个样本类别的整数列表，并返回它们的独热编码表示。
- en: The class `MultilayerNeuralNetClassifier` contains the neural net implementation.
    The initialisation constructor assigns random numbers to the weights and bias
    terms of each layer. As an example if we construct a neural network with `layers=[784,
    50, 10]`, we will be using 784 input features, a hidden layer with 50 nodes and
    10 classes as output. This generalised implementation allows changing both the
    number of hidden layers and the number of nodes in the hidden layers. We will
    exploit this when we do hyperparameter tuning later on. For reproducibility we
    use a seed for the random number generator to initialise the weights.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 类`MultilayerNeuralNetClassifier`包含神经网络的实现。初始化构造函数为每一层的权重和偏置项分配随机数。举个例子，如果我们构建一个神经网络，`layers=[784,
    50, 10]`，则我们将使用784个输入特征，一个包含50个节点的隐藏层，以及10个类别作为输出。这个通用的实现允许改变隐藏层的层数和每一层的节点数。稍后我们在进行超参数调整时将利用这一点。为了保证可重复性，我们使用一个随机数生成器的种子来初始化权重。
- en: The `forward` method returns the activated values for each layer as a list of
    matrices. The method works with a single sample or an array of samples. The last
    of the returned matrices contains the model predictions for the class membership
    of each sample. Once the model is trained only this matrix is used for making
    predictions. However, whilst the model is being trained we need the activated
    values for all layers as we will see below and this is why the `forward` method
    returns all of them. Assuming that the network was initialised with `layers=[784,
    50, 10]`, the `forward` method will return a list of two matrices, the first one
    with shape (N, 50) and the second one with shape (N, 10), assuming the input `x`
    has N samples, i.e. it is a matrix with shape (N, 784).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward` 方法返回每一层的激活值，作为一个矩阵列表。该方法既可以处理单个样本，也可以处理样本数组。返回的最后一个矩阵包含每个样本的类别归属的模型预测结果。一旦模型训练完成，仅使用这个矩阵来进行预测。然而，在模型训练过程中，我们需要获取所有层的激活值，正如下面所述，这也是
    `forward` 方法返回所有激活值的原因。假设网络初始化时使用了 `layers=[784, 50, 10]`，`forward` 方法将返回一个包含两个矩阵的列表，第一个矩阵的形状为
    (N, 50)，第二个矩阵的形状为 (N, 10)，假设输入 `x` 有 N 个样本，即它是一个形状为 (N, 784) 的矩阵。'
- en: The `backward` method implements backpropagation, i.e. all the analytically
    computed derivatives of the loss function as described in the previous section.
    The last layer is special because we need to compute the derivatives of the loss
    function with regard to the model output using the known classes. The first layer
    is special because we need to use the input instead of the activated values of
    the previous layer. The middle layers are all the same. We simply iterate over
    the layers backwards. The code reflects fully the analytically derived formulas.
    By using NumPy we vectorise all operations that speeds up execution. The method
    returns a tuple of two lists. The first list contains the matrices with the derivatives
    of the loss function with regard to the weights of each layer. Assuming that the
    network was initialised with `layers=[784, 50, 10]`, the list will contain two
    matrices with shapes (784, 50) and (50, 10). The second list contains the vectors
    with the derivatives of the loss function with regard to the bias terms of each
    layer. Assuming that the network was initialised with `layers=[784, 50, 10]`,
    the list will contain two vectors with shapes (50, ) and (10,).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`backward` 方法实现了反向传播，即按照上一节中描述的计算损失函数的所有解析导数。最后一层是特殊的，因为我们需要使用已知类别来计算损失函数相对于模型输出的导数。第一层是特殊的，因为我们需要使用输入值，而不是前一层的激活值。中间层则是相同的，我们只是简单地反向遍历各层。代码完全反映了解析推导的公式。通过使用
    NumPy，我们将所有操作向量化，从而加速了执行过程。该方法返回一个包含两个列表的元组。第一个列表包含与每一层权重相关的损失函数导数的矩阵。假设网络初始化时使用了
    `layers=[784, 50, 10]`，该列表将包含形状为 (784, 50) 和 (50, 10) 的两个矩阵。第二个列表包含与每一层偏置项相关的损失函数导数的向量。假设网络初始化时使用了
    `layers=[784, 50, 10]`，该列表将包含形状为 (50, ) 和 (10, ) 的两个向量。'
- en: Reflecting back on my learnings from this article, I felt that the implementation
    was straightforward. The hardest part was to come up with a robust mathematical
    notation and work out the gradients on paper. Still, it is easy to make mistakes
    that may not be easy to detect even if the optimisation seems to converge. This
    brings me to the special `backward_numerical` method. This method is used for
    neither training the model nor making predictions. It uses finite (central) differences
    to estimate the derivatives of the loss function with regard to the weights and
    bias terms of the chosen layer. The numerical derivatives can be compared with
    the analytically computed ones returned by the `backward` function to ensure that
    the implementation is correct. This method would be too slow to be used for training
    the model as it requires two forward passes for each derivative and in our trivial
    example with `layers=[784, 50, 10]` there are 39,760 such derivatives! But it
    is a lifesaver. Personally I would not have managed to debug the code without
    it. If you want to keep a key message from this article, it would be the usefulness
    of numerical differentiation for double checking your analytically derived gradients.
    We can check the correctness of the gradients with an untrained model
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我从这篇文章中学到的内容，我觉得实现过程相当直接。最难的部分是想出一个稳健的数学符号并在纸上推导梯度。不过，即使优化似乎已收敛，仍然很容易犯错误，这些错误可能不容易被察觉。这让我想到了特殊的`backward_numerical`方法。该方法既不用于训练模型，也不用于做出预测。它使用有限差分（中心差分）来估算损失函数相对于所选层的权重和偏置项的导数。通过将数值导数与`backward`函数返回的解析计算结果进行比较，可以确保实现是正确的。这个方法在训练模型时会太慢，因为它每个导数都需要进行两次前向传播，而在我们这个简单的例子中，`layers=[784,
    50, 10]`会有39,760个这样的导数！但是，它真的是救命稻草。就个人而言，如果没有它，我是无法调试代码的。如果你想从这篇文章中记住一个关键点，那就是数值微分在检查你的解析梯度时是多么有用。我们可以使用未训练的模型来检查梯度的正确性。
- en: that produces
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的
- en: '[PRE0]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Gradients look in order!
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度看起来是有序的！
- en: Dataset
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: We will need a dataset for building our first model. A famous one often used
    in pattern recognition experiments is the [MNIST handwritten digits](https://en.wikipedia.org/wiki/MNIST_database).
    We can find more details about this dataset in the OpenML dataset [repository](https://openml.org/search?type=data&status=active&id=554&sort=runs).
    All datasets in OpenML are [subject](https://openml.org/terms) to the [CC BY 4.0
    license](https://creativecommons.org/licenses/by/4.0/) that permits copying, redistributing
    and transforming the material in any medium and for any purpose.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将需要一个数据集来构建我们的第一个模型。一个在模式识别实验中常用的著名数据集是[MNIST手写数字](https://en.wikipedia.org/wiki/MNIST_database)。我们可以在OpenML数据集[仓库](https://openml.org/search?type=data&status=active&id=554&sort=runs)中找到有关该数据集的更多细节。OpenML中的所有数据集都[受](https://openml.org/terms)
    [CC BY 4.0许可证](https://creativecommons.org/licenses/by/4.0/)约束，该许可证允许在任何媒介和任何目的下复制、再分发和转化材料。
- en: The dataset contains 70,000 digit images and the corresponding labels. Conveniently,
    the digits have been size-normalized and centered in a fixed-size 28x28 image
    by computing the center of mass of the pixels, and translating the image so as
    to position this point at the center of the 28x28 field. The dataset can be conveniently
    retrieved using [scikit-learn](https://scikit-learn.org/)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含70,000张数字图像及其对应的标签。方便的是，这些数字已经通过计算像素的质心进行了大小归一化，并且将图像平移，使得这个质心点位于28x28图像的中心。该数据集可以通过[scikit-learn](https://scikit-learn.org/)方便地获取。
- en: that prints
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 打印出来
- en: '[PRE1]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We can see that each image is available as a vector with 784 integers between
    0 and 255 that were converted to floats in the range [-0.5, 0.5]. This is perhaps
    a bit different than the typical feature scaling in scikit-learn where scaling
    happens per feature rather per sample. The class labels were retrieved as strings
    and converted to integers. The dataset is reasonably balanced.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，每个图像都可以表示为一个包含784个整数的向量，范围从0到255，这些整数已转换为[-0.5, 0.5]之间的浮动值。这或许与scikit-learn中通常的特征缩放方式略有不同，后者是按特征而非样本进行缩放的。类别标签以字符串形式提取并转换为整数。该数据集在类别之间相对平衡。
- en: We next visualise ten images for each digit to obtain a feeling on the variations
    in hand writing
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将为每个数字可视化十个图像，以获得对手写变体的感觉。
- en: that produces
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的
- en: '![](../Images/71d8169c956539a18b636b779ab90a9c.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/71d8169c956539a18b636b779ab90a9c.png)'
- en: Randomly selected samples for each digit. Image by the Author.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 随机选择的每个数字样本。图片来源：作者。
- en: We can foresee that some digits may be confused by the model, e.g. the last
    9 resembles 8\. There may also be hand writing variations that are not predicted
    well, such as 7 digits written with a horizontal line in the middle, depending
    on how often such variations are represented in the training set. We now have
    a neural network implementation and a dataset to use it with. In the next section
    we will provide the necessary code for training the model before we look into
    hyperparameter tuning.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以预见到模型可能会对某些数字产生混淆，例如最后的9与8相似。也可能会出现一些手写变体未被很好预测的情况，例如7的数字写在中间有一条横线，这取决于这种变体在训练集中出现的频率。现在我们已经有了一个神经网络实现和一个数据集，接下来我们将提供训练模型所需的代码，然后再讨论超参数调优。
- en: Training the model
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: The first action we need to take is to split the dataset into a training set,
    and an external (hold-out) test set. We can readily do so using scikit-learn
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要采取的第一步是将数据集拆分为训练集和外部（留出）测试集。我们可以通过scikit-learn轻松完成这一步。
- en: We use stratification so that the percentage of each class is roughly equal
    in both the training set and the external (hold-out) dataset. The external (hold-out)
    test set contains 10,000 samples and will not be used for anything other than
    assessing the model performance. In this section we will use the 60,000 samples
    for training set without any hyperparameter tuning.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用分层抽样，以确保每个类别在训练集和外部（留出）数据集中的比例大致相等。外部（留出）测试集包含10,000个样本，仅用于评估模型性能。在本节中，我们将使用60,000个样本作为训练集，且不进行任何超参数调优。
- en: When deriving the gradients of the loss function with regard to the model parameters
    we show that it is necessary to carry out several matrix multiplications and some
    of these matrices have as many rows as the number of samples. Given that typically
    the number of samples is quite large we will need a significant amount of memory.
    To alleviate this we will be using mini batches in the same way we [used](/from-the-perceptron-to-adaline-1730e33d41c5)
    mini batches during the gradient descent optimisation of the adaline model. Typically,
    each batch can contain 100–500 samples. Reducing the batch size increases the
    convergence speed because we make more parameter updates within the the same pass
    of the training set (epoch), but we also increase the noise. We need to strike
    a balance. First we provide a generator that accepts the training set and the
    batch size and returns the batches
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算损失函数相对于模型参数的梯度时，我们会发现有必要进行多次矩阵乘法，其中一些矩阵的行数等于样本数量。鉴于样本数量通常非常大，我们将需要大量的内存。为了缓解这个问题，我们将使用小批量处理，就像我们在[使用](https://example.org/from-the-perceptron-to-adaline-1730e33d41c5)梯度下降优化Adaline模型时使用小批量处理一样。通常，每个批次可以包含100到500个样本。减小批次大小可以提高收敛速度，因为在同一轮训练集（epoch）中，我们会进行更多的参数更新，但也会增加噪声。因此，我们需要找到一个平衡点。首先，我们提供一个生成器，接受训练集和批次大小，并返回批次。
- en: The generator returns batches of equal size that by default contain 100 samples.
    The total number of samples may not be a multiple of the batch size and hence
    some samples will not be returned in a given pass through the training set. Th
    number of skipped samples is smaller than the batch size and the set of samples
    left out changes every time the generator is used, assuming we do not reset the
    random number generator. Hence, this is not critical. As we will be passing though
    the training sets multiple times in the different epochs we will eventually use
    the training set fully. The reason for using batches of a constant size is that
    we will be updating the model parameters after each batch and a small batch can
    increase the noise and prevent convergence, especially if the samples in the batch
    happen to be outliers.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 该生成器返回大小相等的批次，默认每个批次包含100个样本。样本总数可能不是批次大小的整数倍，因此在每次通过训练集时，某些样本不会被返回。被跳过的样本数量小于批次大小，而且每次使用生成器时被遗漏的样本集合会发生变化，前提是我们不重置随机数生成器。因此，这并不关键。由于我们将在不同的训练轮次中多次通过训练集，最终会完整使用训练集。使用固定大小的批次的原因是，我们会在每个批次后更新模型参数，而较小的批次可能会增加噪声并阻碍收敛，特别是当批次中的样本恰好是异常值时。
- en: When the model is initiated we expect a low accuracy that we can confirm with
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型初始化时，我们预期会得到一个较低的准确率，这一点我们可以通过以下方式确认：
- en: that gives an accuracy of approximately 9.5%. This is more or less expected
    for a reasonably balanced dataset as there are 10 classes. We now have the means
    to monitor the loss and the accuracy of each batch passed to the forward pass
    that we will exploit during training. Let’s write the final piece of code to iterate
    over the epochs and mini batches, update the model parameters and monitor how
    the loss and accuracy evolves in both the training set and external (hold-out)
    test set.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了大约9.5%的准确率。对于一个合理平衡的数据集来说，这是预期的结果，因为有10个类别。现在我们有了监控每批次的损失和准确率的方法，这将在训练过程中加以利用。让我们写下最后一段代码，迭代轮次和小批量数据，更新模型参数，并监控训练集和外部（保留）测试集中的损失和准确率的变化。
- en: Using this function training becomes a single line of code
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个函数，训练变成了一行代码
- en: that produces
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成
- en: '[PRE2]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We can see that after the ten epochs the accuracy for the training set has reached
    approximately 76%, whilst the accuracy of the external (hold-out) test set is
    slightly higher, indicating that the model has not been overfitted.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在10轮后，训练集的准确率大约达到了76%，而外部（保留）测试集的准确率略高，表明模型没有发生过拟合。
- en: The loss of the training set keeps decreasing and hence convergence has not
    been reached yet. The model allows hot starting so we could run another ten epochs
    by repeating the single line of code above. Instead, we will initiate the model
    again and run it for 100 epochs, increasing the batch size to 200 at the same
    time. We provide the complete code for doing so.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集的损失持续减少，因此尚未达到收敛。该模型支持热启动，因此我们可以通过重复上述单行代码运行另外10轮训练。相反，我们将重新初始化模型，并将其运行100轮，同时将批量大小增加到200。我们提供了执行此操作的完整代码。
- en: We first plot the training loss and its rate of change as a function of the
    epoch number
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将训练损失及其变化率绘制为与轮次（epoch）数量的函数
- en: that produces
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成
- en: '![](../Images/e3165c82de6bdca3d12babfab2e461d4.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e3165c82de6bdca3d12babfab2e461d4.png)'
- en: Training loss and its rate of change as a function of the epoch number. Image
    by the Author.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 训练损失及其变化率作为与轮次（epoch）数量的函数。图像由作者提供。
- en: We can see the model has converged reasonably well as the rate of the change
    of the training loss has become more than two orders of magnitude smaller compared
    to its value at the beginning of the training. I am not sure why we observe a
    reduction in convergence speed at around epoch 10; I can only speculate that the
    optimiser escaped a local minimum.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，模型已经合理地收敛，因为训练损失的变化率比训练开始时小了两个数量级。我不确定为什么我们在第10轮左右观察到收敛速度减慢；我只能推测优化器跳出了局部最小值。
- en: We can also plot the accuracy of the training set and the test set as a function
    of the epoch number
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将训练集和测试集的准确率绘制为与轮次（epoch）数量的函数
- en: that produces
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成
- en: '![](../Images/83e7976dde52eb1f7735294d5840e92b.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/83e7976dde52eb1f7735294d5840e92b.png)'
- en: Training set and external (hold-out) test set accuracy as a function of the
    epoch number. Image by the Author.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集和外部（保留）测试集的准确率作为轮次（epoch）数量的函数。图像由作者提供。
- en: The accuracy reaches approximately 90% after about 50 epochs for both the training
    set and external (hold-out) test set, suggesting that there is no/little overfitting.
    We just trained our first, custom built multilayer neural network with one hidden
    layer!
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 经过大约50轮训练后，训练集和外部（保留）测试集的准确率都达到了约90%，这表明没有/几乎没有过拟合。我们刚刚训练了第一个自定义构建的具有一个隐藏层的多层神经网络！
- en: Hyperparameter tuning
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数调优
- en: In this previous section we chose an arbitrary network architecture and fitted
    the model parameters. In this section we proceed with a basic hyperparameter tuning
    by varying the number of hidden layers (ranging from 1 to 3), the number of nodes
    in the hidden layers (ranging from 10 to 50 in increments of 10) and the learning
    rate (using the values 0.1, 0.2 and 0.3). We kept the batch size constant at 200
    samples per batch. Overall, we tried 45 parameter combinations. We will employ
    6-fold cross validation (not nested) which means 6 model trainings per parameter
    combination, which translates to 270 model trainings in total. In each fold we
    will be using 50,000 samples for training and 10,000 samples for measuring the
    accuracy (called validation in the code). To enhance the chances to achieve convergence
    we will perform 250 epochs for each model fitting. The total execution time was
    ~12 hours on a single processor (Intel Xeon Gold 3.5GHz). This is more or less
    what we can reasonably run on a CPU. The training speed could be increased using
    multiprocessing. In fact, the training would be way faster using a specialised
    deep learning library like PyTorch on GPUs, such as the freely available T4 GPUs
    on [Google Cola](https://colab.research.google.com)b.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们选择了一个任意的网络架构并拟合了模型参数。在本节中，我们通过改变隐藏层的数量（从1到3层）、隐藏层中节点的数量（从10到50，步长为10）以及学习率（使用0.1、0.2和0.3）进行基础的超参数调优。我们保持批量大小为每批200个样本。总的来说，我们尝试了45种参数组合。我们将使用6折交叉验证（非嵌套），这意味着每种参数组合训练6次模型，总共进行270次模型训练。在每一折中，我们将使用50,000个样本进行训练，并使用10,000个样本来测量准确率（在代码中称为验证）。为了提高收敛的可能性，我们将为每个模型拟合执行250个训练轮次。总的执行时间约为12小时，使用的是单个处理器（Intel
    Xeon Gold 3.5GHz）。这大致是我们在CPU上可以合理运行的速度。通过多进程可以提高训练速度。事实上，使用像PyTorch这样的专门深度学习库，并在[Google
    Cola](https://colab.research.google.com)上的T4 GPU上运行，训练速度会快得多。
- en: This code iterates over all hyperparameter values and folds and stores the loss
    and accuracy for both the training (50,000 samples) and validation (10,000 samples)
    in a [pandas](https://pandas.pydata.org/) dataframe. The dataframe is used to
    find the optimal hyperparameters
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码遍历所有超参数值和折叠，并在[pandas](https://pandas.pydata.org/)数据框中存储训练集（50,000个样本）和验证集（10,000个样本）的损失和准确性。该数据框用于找到最优的超参数。
- en: that produces
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生
- en: '[PRE3]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We can see that there is little benefit in increasing the number of layers.
    Perhaps we could have gained slightly better performance using a larger first
    hidden layer as the hyperparameter tuning hit the bound of 50 nodes. Some mean
    cross-validation accuracies are quite low that could be indicative of poor convergence
    (e.g. when using 3 hidden layers with 10 nodes each). We did not investigate further
    but this would be typically required before concluding on the optimal network
    geometry. I would expect that allowing for more epochs would increase accuracy
    further particular with the larger networks.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，增加层数几乎没有带来太多好处。也许通过使用更大的第一隐藏层，我们可以稍微获得更好的性能，因为超参数调优达到了50个节点的限制。一些平均交叉验证的准确率非常低，可能表明收敛性较差（例如，使用3个隐藏层，每个层10个节点时）。我们没有进一步调查，但在得出网络结构的最佳结论之前，通常需要进行这类调查。我预计，如果允许更多的训练轮次，特别是在较大的网络中，准确率可能会进一步提高。
- en: A final step is to retrain the model with all samples other than the external
    (hold-out) set that are only used for the final evaluation
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的步骤是使用除了外部（保留）集以外的所有样本重新训练模型，外部集仅用于最终评估。
- en: The last 5 epochs are
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的5个训练轮次是
- en: '[PRE4]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We achieved ~95% accuracy with the external (hold-out) test set. This is magical
    if we consider that we started with a blank piece of paper!
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在外部（保留）测试集上达到了约95%的准确率。如果考虑到我们从一张白纸开始，这真是太神奇了！
- en: Conclusions
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: This article demonstrated how we can build a multilayer, feedforward, fully
    connected neural network from scratch. The network was used for solving a multiclass
    classification problem. The implementation has been generalised to allow for any
    number of hidden layers with any number of nodes. This facilitates hyperparameter
    tuning by varying the number of layers and units in them. However, we need to
    keep in mind that the loss gradients become smaller and smaller as the depth of
    the neural network increases. This is known as the vanishing gradient problem
    and requires using specialised training algorithms once the depth exceeds a certain
    threshold, which is out of the scope of this article.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 本文展示了如何从零开始构建一个多层前馈全连接神经网络。该网络用于解决多类分类问题。实现已被通用化，可以支持任意数量的隐藏层和每层的节点数。这使得通过改变层数和节点数来调整超参数变得更加灵活。然而，我们需要牢记，随着神经网络深度的增加，损失梯度会变得越来越小。这被称为梯度消失问题，并且一旦网络深度超过某个阈值，就需要使用专门的训练算法，这超出了本文的讨论范围。
- en: Our vanilla implementation of a multilayer neural network has hopefully educational
    value. Using it in practice would require several improvements though. First of
    all, overfitting would need to be addressed, by employing some form of drop out.
    Other improvements, such as the addition of skip-connections and the variation
    of the learning rate during training, may be beneficial too. In addition, the
    network architecture itself can be optimised, e.g. by using a convolutional neural
    network that would be more appropriate for classifying images. Such improvements
    are best attempted using a specialised library like [PyTorch](https://pytorch.org/).
    When developing algorithms from scratch one needs to be wary of the time it takes
    and where to draw the line so that the endeavour remains educational without being
    extremely time consuming. I hope this article strikes a good balance in this sense.
    If you are intrigued I would recommend this [book](https://sebastianraschka.com/books/#machine-learning-with-pytorch-and-scikit-learn)
    for further study.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的原始多层神经网络实现希望具有一定的教育价值。然而，若要在实践中使用它，还需要进行一些改进。首先，需要通过采用某种形式的丢弃法来解决过拟合问题。其他改进，如添加跳跃连接和在训练过程中变动学习率，也可能有益。此外，网络架构本身也可以优化，例如使用卷积神经网络（CNN），它更适合用于图像分类。这类改进最好使用像[PyTorch](https://pytorch.org/)这样的专用库来实现。在从零开始开发算法时，需注意耗时以及如何划定边界，以便保持教育性而不会过于消耗时间。我希望本文在这方面达到了良好的平衡。如果你对这方面感兴趣，我推荐这本[书](https://sebastianraschka.com/books/#machine-learning-with-pytorch-and-scikit-learn)以供进一步学习。
- en: LaTeX code of equations used in the article
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本文中使用的LaTeX方程代码
- en: The equations used in the article can be found in the gist below, in case you
    would like to render them again.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中使用的方程可以在下面的gist中找到，如果你想重新渲染它们。
