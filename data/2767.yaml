- en: 'Gradient Boosting Regressor, Explained: A Visual Guide with Code Examples'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¢¯åº¦æå‡å›å½’å™¨è¯¦è§£ï¼šå¸¦æœ‰ä»£ç ç¤ºä¾‹çš„è§†è§‰æŒ‡å—
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/gradient-boosting-regressor-explained-a-visual-guide-with-code-examples-c098d1ae425c?source=collection_archive---------1-----------------------#2024-11-14](https://towardsdatascience.com/gradient-boosting-regressor-explained-a-visual-guide-with-code-examples-c098d1ae425c?source=collection_archive---------1-----------------------#2024-11-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/gradient-boosting-regressor-explained-a-visual-guide-with-code-examples-c098d1ae425c?source=collection_archive---------1-----------------------#2024-11-14](https://towardsdatascience.com/gradient-boosting-regressor-explained-a-visual-guide-with-code-examples-c098d1ae425c?source=collection_archive---------1-----------------------#2024-11-14)
- en: ENSEMBLE LEARNING
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é›†æˆå­¦ä¹ 
- en: Fitting to errors one booster stage at a time
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸€æ¬¡ä¸€ä¸ªå¢å¼ºé˜¶æ®µåœ°æ‹Ÿåˆè¯¯å·®
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--c098d1ae425c--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--c098d1ae425c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c098d1ae425c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c098d1ae425c--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--c098d1ae425c--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@samybaladram?source=post_page---byline--c098d1ae425c--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--c098d1ae425c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c098d1ae425c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c098d1ae425c--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--c098d1ae425c--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c098d1ae425c--------------------------------)
    Â·11 min readÂ·Nov 14, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c098d1ae425c--------------------------------)
    Â·é˜…è¯»æ—¶é—´ 11åˆ†é’ŸÂ·2024å¹´11æœˆ14æ—¥
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef?source=post_page-----c098d1ae425c--------------------------------)
    [## Decision Tree Regressor, Explained: A Visual Guide with Code Examples'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef?source=post_page-----c098d1ae425c--------------------------------)
    [## å†³ç­–æ ‘å›å½’å™¨è¯¦è§£ï¼šå¸¦æœ‰ä»£ç ç¤ºä¾‹çš„è§†è§‰æŒ‡å—'
- en: Trimming branches smartly with Cost-Complexity Pruning
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é€šè¿‡æˆæœ¬å¤æ‚åº¦ä¿®å‰ªæ™ºèƒ½åœ°ä¿®å‰ªåˆ†æ”¯
- en: towardsdatascience.com](/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef?source=post_page-----c098d1ae425c--------------------------------)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef?source=post_page-----c098d1ae425c--------------------------------)
- en: Of course, in machine learning, we want our predictions spot on. We started
    with [simple decision trees](https://medium.com/towards-data-science/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e)
    â€” they worked okay. Then came [Random Forests](https://medium.com/towards-data-science/random-forest-explained-a-visual-guide-with-code-examples-9f736a6e1b3c)
    and [AdaBoost](https://medium.com/towards-data-science/adaboost-classifier-explained-a-visual-guide-with-code-examples-fc0f25326d7b),
    which did better. But Gradient Boosting? That was a game-changer, making predictions
    way more accurate.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œåœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„é¢„æµ‹éå¸¸å‡†ç¡®ã€‚æˆ‘ä»¬ä»[ç®€å•çš„å†³ç­–æ ‘](https://medium.com/towards-data-science/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e)å¼€å§‹â€”â€”å®ƒä»¬è¿˜ä¸é”™ã€‚ç„¶åå‡ºç°äº†[éšæœºæ£®æ—](https://medium.com/towards-data-science/random-forest-explained-a-visual-guide-with-code-examples-9f736a6e1b3c)å’Œ[AdaBoost](https://medium.com/towards-data-science/adaboost-classifier-explained-a-visual-guide-with-code-examples-fc0f25326d7b)ï¼Œæ•ˆæœæ›´å¥½ã€‚ä½†æ¢¯åº¦æå‡å‘¢ï¼Ÿé‚£ç®€ç›´æ˜¯ä¸€ä¸ªæ¸¸æˆè§„åˆ™çš„æ”¹å˜ï¼Œä½¿å¾—é¢„æµ‹å˜å¾—æ›´åŠ å‡†ç¡®ã€‚
- en: 'They said, â€œWhat makes Gradient Boosting work so well is actually simple: it
    builds models one after another, where each new model focuses on fixing the mistakes
    of all previous models combined. This way of fixing errors step by step is what
    makes it special.â€ I thought itâ€™s really gonna be that simple but *every time*
    I look up Gradient Boosting, trying to understand how it works, I see the same
    thing: rows and rows of complex math formulas and ugly charts that somehow drive
    me insane. Just try it.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–ä»¬è¯´ï¼šâ€œä½¿å¾—æ¢¯åº¦æå‡å¦‚æ­¤æœ‰æ•ˆçš„åŸå› å…¶å®å¾ˆç®€å•ï¼šå®ƒé€ä¸ªæ„å»ºæ¨¡å‹ï¼Œæ¯ä¸ªæ–°æ¨¡å‹éƒ½ä¸“æ³¨äºä¿®æ­£æ‰€æœ‰ä¹‹å‰æ¨¡å‹çš„é”™è¯¯ã€‚è¿™æ ·é€æ­¥ä¿®æ­£é”™è¯¯çš„æ–¹å¼å°±æ˜¯å®ƒç‰¹åˆ«ä¹‹å¤„ã€‚â€æˆ‘æœ¬ä»¥ä¸ºè¿™çœŸçš„ä¼šè¿™ä¹ˆç®€å•ï¼Œä½†*æ¯æ¬¡*æˆ‘æŸ¥é˜…æ¢¯åº¦æå‡ï¼Œè¯•å›¾ç†è§£å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„æ—¶ï¼Œæˆ‘çœ‹åˆ°çš„å´æ˜¯ç›¸åŒçš„å†…å®¹ï¼šä¸€è¡Œè¡Œå¤æ‚çš„æ•°å­¦å…¬å¼å’Œä»¤äººå¤´ç–¼çš„å›¾è¡¨ï¼Œè¿™äº›ä¸œè¥¿æ€»æ˜¯è®©æˆ‘æŠ“ç‹‚ã€‚è¯•è¯•çœ‹å§ã€‚
- en: Letâ€™s put a stop to this and break it down in a way that actually makes sense.
    Weâ€™ll visually navigate through the training steps of Gradient Boosting, focusing
    on a regression case â€” a simpler scenario than classification â€” so we can avoid
    the confusing math. Like a multi-stage rocket shedding unnecessary weight to reach
    orbit, weâ€™ll blast away those prediction errors one residual at a time.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç»ˆç»“è¿™ä¸€ç‚¹ï¼Œå¹¶ä»¥ä¸€ç§å®é™…æœ‰æ„ä¹‰çš„æ–¹å¼æ¥åˆ†è§£å®ƒã€‚æˆ‘ä»¬å°†é€šè¿‡æ¢¯åº¦æå‡çš„è®­ç»ƒæ­¥éª¤è¿›è¡Œå¯è§†åŒ–å¯¼èˆªï¼Œä¸“æ³¨äºå›å½’æ¡ˆä¾‹â€”â€”è¿™æ˜¯ä¸€ä¸ªæ¯”åˆ†ç±»æ›´ç®€å•çš„åœºæ™¯â€”â€”è¿™æ ·æˆ‘ä»¬å°±å¯ä»¥é¿å…æ··æ·†çš„æ•°å­¦è®¡ç®—ã€‚å°±åƒå¤šçº§ç«ç®­ä¸ºäº†è¾¾åˆ°è½¨é“è€Œä¸¢å¼ƒä¸å¿…è¦çš„é‡é‡ä¸€æ ·ï¼Œæˆ‘ä»¬å°†ä¸€é¡¹ä¸€é¡¹åœ°æ¶ˆé™¤é‚£äº›é¢„æµ‹è¯¯å·®ã€‚
- en: '![](../Images/2d6078ab2a677250978f6a47f6460221.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d6078ab2a677250978f6a47f6460221.png)'
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è§†è§‰å›¾åƒï¼šä½œè€…ä½¿ç”¨ Canva Pro åˆ›å»ºã€‚ä¼˜åŒ–ä¸ºç§»åŠ¨è®¾å¤‡æ˜¾ç¤ºï¼›åœ¨æ¡Œé¢ä¸Šå¯èƒ½æ˜¾ç¤ºè¿‡å¤§ã€‚
- en: Definition
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®šä¹‰
- en: Gradient Boosting is an ensemble machine learning technique that builds a series
    of decision trees, each aimed at correcting the errors of the previous ones. Unlike
    AdaBoost, which uses shallow trees, Gradient Boosting uses deeper trees as its
    weak learners. Each new tree focuses on minimizing the residual errors â€” the differences
    between actual and predicted values â€” rather than learning directly from the original
    targets.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦æå‡æ˜¯ä¸€ç§é›†æˆæœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œå®ƒæ„å»ºä¸€ç³»åˆ—å†³ç­–æ ‘ï¼Œæ¯æ£µæ ‘æ—¨åœ¨çº æ­£å‰ä¸€æ£µæ ‘çš„é”™è¯¯ã€‚ä¸ä½¿ç”¨æµ…å±‚æ ‘çš„ AdaBoost ä¸åŒï¼Œæ¢¯åº¦æå‡ä½¿ç”¨æ›´æ·±çš„æ ‘ä½œä¸ºå…¶å¼±å­¦ä¹ å™¨ã€‚æ¯æ£µæ–°æ ‘çš„ç›®æ ‡æ˜¯æœ€å°åŒ–æ®‹å·®è¯¯å·®â€”â€”å®é™…å€¼ä¸é¢„æµ‹å€¼ä¹‹é—´çš„å·®å¼‚â€”â€”è€Œä¸æ˜¯ç›´æ¥ä»åŸå§‹ç›®æ ‡ä¸­å­¦ä¹ ã€‚
- en: For regression tasks, Gradient Boosting adds trees one after another with each
    new tree is trained to reduce the remaining errors by addressing the current residual
    errors. The final prediction is made by adding up the outputs from all the trees.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå›å½’ä»»åŠ¡ï¼Œæ¢¯åº¦æå‡é€ä¸€æ·»åŠ å†³ç­–æ ‘ï¼Œæ¯æ£µæ–°æ ‘çš„è®­ç»ƒç›®æ ‡æ˜¯é€šè¿‡è§£å†³å½“å‰çš„æ®‹å·®è¯¯å·®æ¥å‡å°‘å‰©ä½™çš„é”™è¯¯ã€‚æœ€ç»ˆçš„é¢„æµ‹æ˜¯é€šè¿‡å°†æ‰€æœ‰æ ‘çš„è¾“å‡ºç›¸åŠ å¾—åˆ°çš„ã€‚
- en: The modelâ€™s strength comes from its additive learning process â€” while each tree
    focuses on correcting the remaining errors in the ensemble, the sequential combination
    creates a powerful predictor that **progressively reduces the overall prediction
    error** by focusing on the parts of the problem where the model still struggles.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹çš„ä¼˜åŠ¿æ¥è‡ªäºå…¶åŠ æ³•å­¦ä¹ è¿‡ç¨‹â€”â€”æ¯æ£µæ ‘ä¸“æ³¨äºçº æ­£é›†æˆä¸­çš„å‰©ä½™é”™è¯¯ï¼Œè€Œé¡ºåºç»„åˆçš„æ–¹å¼ä½¿å¾—å®ƒæˆä¸ºä¸€ä¸ªå¼ºå¤§çš„é¢„æµ‹å™¨ï¼Œé€šè¿‡ä¸“æ³¨äºæ¨¡å‹ä»ç„¶éš¾ä»¥å¤„ç†çš„é—®é¢˜éƒ¨åˆ†ï¼Œ**é€æ­¥å‡å°‘æ•´ä½“é¢„æµ‹è¯¯å·®**ã€‚
- en: '![](../Images/3ac141fa57a3f0707e680165024c850a.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ac141fa57a3f0707e680165024c850a.png)'
- en: Gradient Boosting is part of the boosting family of algorithms because it builds
    trees sequentially, with each new tree trying to correct the errors of its predecessors.
    However, unlike other boosting methods, Gradient Boosting approaches the problem
    from an optimization perspective.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦æå‡æ˜¯æå‡å®¶æ—ç®—æ³•çš„ä¸€éƒ¨åˆ†ï¼Œå› ä¸ºå®ƒæ˜¯é€ä¸€æ„å»ºæ ‘çš„ï¼Œæ¯æ£µæ–°æ ‘å°è¯•çº æ­£å‰ä¸€æ£µæ ‘çš„é”™è¯¯ã€‚ç„¶è€Œï¼Œä¸å…¶ä»–æå‡æ–¹æ³•ä¸åŒï¼Œæ¢¯åº¦æå‡ä»ä¼˜åŒ–çš„è§’åº¦æ¥è§£å†³é—®é¢˜ã€‚
- en: Dataset Used
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨çš„æ•°æ®é›†
- en: Throughout this article, weâ€™ll focus on the classic golf dataset as an example
    for regression. While Gradient Boosting can handle both regression and classification
    tasks effectively, weâ€™ll concentrate on the simpler task which in this case is
    the regression â€” predicting the number of players who will show up to play golf
    based on weather conditions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ä»¥ç»å…¸çš„é«˜å°”å¤«æ•°æ®é›†ä½œä¸ºå›å½’çš„ä¾‹å­ã€‚å°½ç®¡æ¢¯åº¦æå‡å¯ä»¥æœ‰æ•ˆåœ°å¤„ç†å›å½’å’Œåˆ†ç±»ä»»åŠ¡ï¼Œä½†æˆ‘ä»¬å°†ä¸“æ³¨äºæ›´ç®€å•çš„å›å½’ä»»åŠ¡â€”â€”æ ¹æ®å¤©æ°”æ¡ä»¶é¢„æµ‹æ¥æ‰“é«˜å°”å¤«çš„ç©å®¶äººæ•°ã€‚
- en: '![](../Images/dfe988f34df90e6d6bc95d67f04271db.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dfe988f34df90e6d6bc95d67f04271db.png)'
- en: 'Columns: â€˜Overcast (one-hot-encoded into 3 columns)â€™, â€™Temperatureâ€™ (in Fahrenheit),
    â€˜Humidityâ€™ (in %), â€˜Windyâ€™ (Yes/No) and â€˜Number of Playersâ€™ (target feature)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ—ï¼šâ€˜é˜´å¤©ï¼ˆé€šè¿‡ä¸€çƒ­ç¼–ç è½¬æ¢ä¸º 3 åˆ—ï¼‰â€™ï¼Œâ€˜æ¸©åº¦â€™ï¼ˆåæ°åº¦ï¼‰ï¼Œâ€˜æ¹¿åº¦â€™ï¼ˆç™¾åˆ†æ¯”ï¼‰ï¼Œâ€˜æœ‰é£â€™ï¼ˆæ˜¯/å¦ï¼‰å’Œâ€˜ç©å®¶äººæ•°â€™ï¼ˆç›®æ ‡ç‰¹å¾ï¼‰
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Main Mechanism
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸»è¦æœºåˆ¶
- en: 'Hereâ€™s how Gradient Boosting works:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯æ¢¯åº¦æå‡çš„å·¥ä½œåŸç†ï¼š
- en: '**Initialize Model:** Start with a simple prediction, typically the mean of
    target values.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åˆå§‹åŒ–æ¨¡å‹ï¼š** ä»ä¸€ä¸ªç®€å•çš„é¢„æµ‹å¼€å§‹ï¼Œé€šå¸¸æ˜¯ç›®æ ‡å€¼çš„å¹³å‡å€¼ã€‚'
- en: '**Iterative Learning:** For a set number of iterations, compute the residuals,
    train a decision tree to predict these residuals, and add the new treeâ€™s predictions
    (scaled by the learning rate) to the running total.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è¿­ä»£å­¦ä¹ ï¼š** åœ¨è®¾å®šçš„è¿­ä»£æ¬¡æ•°å†…ï¼Œè®¡ç®—æ®‹å·®ï¼Œè®­ç»ƒä¸€æ£µå†³ç­–æ ‘æ¥é¢„æµ‹è¿™äº›æ®‹å·®ï¼Œå¹¶å°†æ–°æ ‘çš„é¢„æµ‹ç»“æœï¼ˆæŒ‰å­¦ä¹ ç‡ç¼©æ”¾ï¼‰æ·»åŠ åˆ°è¿è¡Œæ€»å’Œä¸­ã€‚'
- en: '**Build Trees on Residuals:** Each new tree focuses on the remaining errors
    from all previous iterations.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åœ¨æ®‹å·®ä¸Šæ„å»ºå†³ç­–æ ‘ï¼š** æ¯æ£µæ–°æ ‘ä¸“æ³¨äºæ‰€æœ‰å‰æœŸè¿­ä»£ä¸­çš„å‰©ä½™è¯¯å·®ã€‚'
- en: '**Final Prediction:** Sum up all tree contributions (scaled by the learning
    rate) and the initial prediction.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æœ€ç»ˆé¢„æµ‹ï¼š** æ±‡æ€»æ‰€æœ‰æ ‘çš„è´¡çŒ®ï¼ˆæŒ‰å­¦ä¹ ç‡ç¼©æ”¾ï¼‰å’Œåˆå§‹é¢„æµ‹ã€‚'
- en: '![](../Images/5aebdd14999e70e5924a74ce7ef351b5.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5aebdd14999e70e5924a74ce7ef351b5.png)'
- en: A Gradient Boosting Regressor starts with an average prediction and improves
    it through multiple trees, each one fixing the previous treesâ€™ mistakes in small
    steps, until reaching the final prediction.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦æå‡å›å½’æ¨¡å‹ä»å¹³å‡é¢„æµ‹å¼€å§‹ï¼Œé€šè¿‡å¤šæ£µæ ‘è¿›è¡Œæ”¹è¿›ï¼Œæ¯æ£µæ ‘éƒ½åœ¨å°æ­¥ä¿®æ­£å‰ä¸€æ£µæ ‘çš„é”™è¯¯ï¼Œç›´åˆ°è¾¾åˆ°æœ€ç»ˆé¢„æµ‹ã€‚
- en: Training Steps
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ­¥éª¤
- en: 'Weâ€™ll follow the standard gradient boosting approach:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†éµå¾ªæ ‡å‡†çš„æ¢¯åº¦æå‡æ–¹æ³•ï¼š
- en: '1.0\. Set Model Parameters:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 1.0\. è®¾ç½®æ¨¡å‹å‚æ•°ï¼š
- en: 'Before building any trees, we need set the core parameters that control the
    learning process:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ„å»ºä»»ä½•æ ‘ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦è®¾ç½®æ§åˆ¶å­¦ä¹ è¿‡ç¨‹çš„æ ¸å¿ƒå‚æ•°ï¼š
- en: Â· the number of trees (typically 100, but weâ€™ll choose 50) to build sequentially,
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Â· æ ‘çš„æ•°é‡ï¼ˆé€šå¸¸ä¸º100ï¼Œä½†æˆ‘ä»¬é€‰æ‹©50ï¼‰æŒ‰é¡ºåºæ„å»ºï¼Œ
- en: Â· the learning rate (typically 0.1), and
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Â· å­¦ä¹ ç‡ï¼ˆé€šå¸¸ä¸º0.1ï¼‰ï¼Œä»¥åŠ
- en: Â· the maximum depth of each tree (typically 3)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Â· æ¯æ£µæ ‘çš„æœ€å¤§æ·±åº¦ï¼ˆé€šå¸¸ä¸º3ï¼‰
- en: '![](../Images/11972f0214d401ffdf105a994791fc6b.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11972f0214d401ffdf105a994791fc6b.png)'
- en: 'A tree diagram showing our key settings: each tree will have 3 levels, and
    weâ€™ll create 50 of them while moving forward in small steps of 0.1.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ£µæ ‘çš„å›¾ç¤ºï¼Œå±•ç¤ºäº†æˆ‘ä»¬çš„å…³é”®è®¾ç½®ï¼šæ¯æ£µæ ‘å°†æœ‰3ä¸ªå±‚çº§ï¼Œæˆ‘ä»¬å°†åˆ›å»º50æ£µæ ‘ï¼Œå¹¶åœ¨æ¯æ¬¡è¿­ä»£ä¸­ä»¥0.1çš„å°æ­¥å‰è¿›ã€‚
- en: For the First Tree
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯¹äºç¬¬ä¸€æ£µæ ‘
- en: 2.0 Make an initial prediction for the label. This is typically the mean (just
    like [a dummy prediction](/dummy-regressor-explained-a-visual-guide-with-code-examples-for-beginners-4007c3d16629).)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 2.0 å¯¹æ ‡ç­¾è¿›è¡Œåˆå§‹é¢„æµ‹ã€‚é€šå¸¸è¿™æ˜¯å‡å€¼ï¼ˆå°±åƒæ˜¯[a dummy prediction](/dummy-regressor-explained-a-visual-guide-with-code-examples-for-beginners-4007c3d16629)ä¸€æ ·ã€‚ï¼‰
- en: '![](../Images/0583a69027986dee2fda863439108614.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0583a69027986dee2fda863439108614.png)'
- en: To start our predictions, we use the average value (37.43) of all our training
    data as the first guess for every case.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å¼€å§‹æˆ‘ä»¬çš„é¢„æµ‹ï¼Œæˆ‘ä»¬ä½¿ç”¨æ‰€æœ‰è®­ç»ƒæ•°æ®çš„å¹³å‡å€¼ï¼ˆ37.43ï¼‰ä½œä¸ºæ¯ä¸ªæ¡ˆä¾‹çš„ç¬¬ä¸€æ¬¡çŒœæµ‹ã€‚
- en: '2.1\. Calculate temporary residual (or pseudo-residuals):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 2.1\. è®¡ç®—ä¸´æ—¶æ®‹å·®ï¼ˆæˆ–ä¼ªæ®‹å·®ï¼‰ï¼š
- en: residual = actual value â€” predicted value
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æ®‹å·® = å®é™…å€¼ â€” é¢„æµ‹å€¼
- en: '![](../Images/5f135dbb9d7034726d771919188d31f6.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5f135dbb9d7034726d771919188d31f6.png)'
- en: Calculating the initial residuals by subtracting the mean prediction (37.43)
    from each target value in our training set.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ä»æ¯ä¸ªç›®æ ‡å€¼ä¸­å‡å»å‡å€¼é¢„æµ‹ï¼ˆ37.43ï¼‰æ¥è®¡ç®—åˆå§‹æ®‹å·®ã€‚
- en: 2.2\. Build a decision tree to **predict these residuals.** The tree building
    steps are exactly the same as in the [regression tree](https://medium.com/towards-data-science/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 2.2\. æ„å»ºå†³ç­–æ ‘ä»¥**é¢„æµ‹è¿™äº›æ®‹å·®**ã€‚æ ‘çš„æ„å»ºæ­¥éª¤ä¸[å›å½’æ ‘](https://medium.com/towards-data-science/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef)å®Œå…¨ç›¸åŒã€‚
- en: '![](../Images/77364ffaaeca757af50cfa5a758e12b1.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77364ffaaeca757af50cfa5a758e12b1.png)'
- en: The first decision tree begins its training by searching for patterns in our
    features that can best predict the calculated residuals from our initial mean
    prediction.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ£µå†³ç­–æ ‘å¼€å§‹è®­ç»ƒæ—¶ï¼Œé€šè¿‡å¯»æ‰¾ç‰¹å¾ä¸­çš„æ¨¡å¼ï¼Œæ¥é¢„æµ‹æˆ‘ä»¬åˆæ­¥å‡å€¼é¢„æµ‹çš„è®¡ç®—æ®‹å·®ã€‚
- en: a. Calculate initial MSE (Mean Squared Error) for the root node
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: a. è®¡ç®—æ ¹èŠ‚ç‚¹çš„åˆå§‹å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰
- en: '![](../Images/44736650f521e040f1c4b17edd948edc.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44736650f521e040f1c4b17edd948edc.png)'
- en: Just like in regular regression trees, we calculate the Mean Squared Error (MSE),
    but this time weâ€™re measuring the spread of residuals (around zero) instead of
    actual values (around their mean).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒå¸¸è§„å›å½’æ ‘ä¸€æ ·ï¼Œæˆ‘ä»¬è®¡ç®—å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ï¼Œä½†è¿™æ¬¡æˆ‘ä»¬æµ‹é‡çš„æ˜¯æ®‹å·®çš„åˆ†å¸ƒï¼ˆå›´ç»•é›¶ï¼‰ï¼Œè€Œä¸æ˜¯å®é™…å€¼çš„åˆ†å¸ƒï¼ˆå›´ç»•å®ƒä»¬çš„å‡å€¼ï¼‰ã€‚
- en: 'b. For each feature:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: b. å¯¹æ¯ä¸ªç‰¹å¾ï¼š
- en: Â· Sort data by feature values
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Â· æŒ‰ç‰¹å¾å€¼å¯¹æ•°æ®è¿›è¡Œæ’åº
- en: '![](../Images/b2f725b3c389f832749b26326e214e85.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2f725b3c389f832749b26326e214e85.png)'
- en: For each feature in our dataset, we sort its values and find potential split
    points between them, just as we would in a standard decision tree, to determine
    the best way to divide our residuals.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ•°æ®é›†ä¸­çš„æ¯ä¸ªç‰¹å¾ï¼Œæˆ‘ä»¬å¯¹å…¶å€¼è¿›è¡Œæ’åºå¹¶æ‰¾åˆ°æ½œåœ¨çš„åˆ†è£‚ç‚¹ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨æ ‡å‡†å†³ç­–æ ‘ä¸­æ‰€åšçš„é‚£æ ·ï¼Œæ¥ç¡®å®šæœ€å¥½çš„æ–¹å¼æ¥åˆ’åˆ†æˆ‘ä»¬çš„æ®‹å·®ã€‚
- en: 'Â· For each possible split point:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Â· å¯¹æ¯ä¸ªå¯èƒ½çš„åˆ†è£‚ç‚¹ï¼š
- en: Â·Â· Split samples into left and right groups
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Â·Â· å°†æ ·æœ¬åˆ†ä¸ºå·¦ç»„å’Œå³ç»„
- en: Â·Â· Calculate MSE for both groups
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Â·Â· è®¡ç®—ä¸¤ä¸ªç»„çš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰
- en: Â·Â· Calculate MSE reduction for this split
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Â·Â· è®¡ç®—è¿™ä¸ªåˆ†è£‚çš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰å‡å°‘é‡
- en: '![](../Images/7082a68606950c5a393205a4e8694ad0.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7082a68606950c5a393205a4e8694ad0.png)'
- en: Similar to a regular regression tree, we evaluate each split by calculating
    the weighted MSE of both groups, but here weâ€™re measuring how well the split groups
    similar residuals rather than similar target values.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼äºå¸¸è§„çš„å›å½’æ ‘ï¼Œæˆ‘ä»¬é€šè¿‡è®¡ç®—ä¸¤ç»„çš„åŠ æƒå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æ¥è¯„ä¼°æ¯æ¬¡åˆ’åˆ†ï¼Œä½†è¿™é‡Œæˆ‘ä»¬è¡¡é‡çš„æ˜¯åˆ’åˆ†åçš„ç»„å¦‚ä½•èšé›†ç›¸ä¼¼çš„æ®‹å·®ï¼Œè€Œä¸æ˜¯ç›¸ä¼¼çš„ç›®æ ‡å€¼ã€‚
- en: c. Pick the split that gives the largest MSE reduction
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: c. é€‰æ‹©èƒ½å¤Ÿå¸¦æ¥æœ€å¤§MSEé™ä½çš„åˆ†è£‚
- en: '![](../Images/26c4db7c3842cc6336ef543ffe94653d.png)![](../Images/164adc4d983357b7bd928394075c5cbb.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26c4db7c3842cc6336ef543ffe94653d.png)![](../Images/164adc4d983357b7bd928394075c5cbb.png)'
- en: The tree makes its first split using the â€œrainâ€ feature at value 0.5, dividing
    samples into two groups based on their residuals â€” this first decision will be
    refined by additional splits at deeper levels.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‘é€šè¿‡ä½¿ç”¨â€œrainâ€ç‰¹å¾ï¼ˆå€¼ä¸º0.5ï¼‰è¿›è¡Œç¬¬ä¸€æ¬¡åˆ†è£‚ï¼ŒåŸºäºæ®‹å·®å°†æ ·æœ¬åˆ†æˆä¸¤ç»„â€”â€”è¿™ä¸ªç¬¬ä¸€æ¬¡å†³ç­–å°†åœ¨æ›´æ·±å±‚æ¬¡çš„è¿›ä¸€æ­¥åˆ†è£‚ä¸­å¾—åˆ°ç²¾ç‚¼ã€‚
- en: d. Continue splitting until reaching maximum depth or minimum samples per leaf.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: d. ç»§ç»­åˆ†è£‚ï¼Œç›´åˆ°è¾¾åˆ°æœ€å¤§æ·±åº¦æˆ–æ¯ä¸ªå¶å­çš„æœ€å°æ ·æœ¬æ•°ã€‚
- en: '![](../Images/dfb7d12a8ebb84f85be346d2ab27e286.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dfb7d12a8ebb84f85be346d2ab27e286.png)'
- en: After three levels of splitting on different features, our first tree has created
    eight distinct groups, each with its own prediction for the residuals.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ç»å†äº†ä¸‰å±‚åŸºäºä¸åŒç‰¹å¾çš„åˆ†è£‚åï¼Œæˆ‘ä»¬çš„ç¬¬ä¸€æ£µæ ‘åˆ›å»ºäº†å…«ä¸ªä¸åŒçš„ç»„ï¼Œæ¯ä¸ªç»„éƒ½æœ‰è‡ªå·±çš„æ®‹å·®é¢„æµ‹å€¼ã€‚
- en: 2.3\. Calculate Leaf Values
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 2.3\. è®¡ç®—å¶å­èŠ‚ç‚¹å€¼
- en: For each leaf, find the **mean of residuals**.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªå¶å­èŠ‚ç‚¹ï¼Œè®¡ç®—**æ®‹å·®çš„å‡å€¼**ã€‚
- en: '![](../Images/fbb6b98089b4529f42201e77bbdeca1d.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fbb6b98089b4529f42201e77bbdeca1d.png)'
- en: Each leaf in our first tree contains an average of the residuals in that group
    â€” these values will be used to adjust and improve our initial mean prediction
    of 37.43.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ£µæ ‘çš„æ¯ä¸ªå¶å­èŠ‚ç‚¹åŒ…å«è¯¥ç»„æ®‹å·®çš„å¹³å‡å€¼â€”â€”è¿™äº›å€¼å°†ç”¨äºè°ƒæ•´å’Œæ”¹å–„æˆ‘ä»¬æœ€åˆçš„37.43çš„é¢„æµ‹ã€‚
- en: 2.4\. Update Predictions
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 2.4\. æ›´æ–°é¢„æµ‹
- en: Â· For each data point in the **training dataset**, determine which leaf it falls
    into based on the new tree.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Â· å¯¹äº**è®­ç»ƒæ•°æ®é›†**ä¸­çš„æ¯ä¸ªæ•°æ®ç‚¹ï¼ŒåŸºäºæ–°æ ‘ç¡®å®šå®ƒå±äºå“ªä¸ªå¶å­èŠ‚ç‚¹ã€‚
- en: '![](../Images/df18786126f2894b4b244d4fde2b9192.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df18786126f2894b4b244d4fde2b9192.png)'
- en: Running our training data through the first tree, each sample follows its own
    path based on weather features to get its predicted residual value, which will
    help correct our initial prediction.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®é€šè¿‡ç¬¬ä¸€æ£µæ ‘è¿è¡Œæ—¶ï¼Œæ¯ä¸ªæ ·æœ¬æ ¹æ®å¤©æ°”ç‰¹å¾æ²¿ç€è‡ªå·±çš„è·¯å¾„è·å–é¢„æµ‹æ®‹å·®å€¼ï¼Œè¿™å°†å¸®åŠ©ä¿®æ­£æˆ‘ä»¬æœ€åˆçš„é¢„æµ‹ã€‚
- en: Â· Multiply the new treeâ€™s predictions by the learning rate and add these scaled
    predictions to the current modelâ€™s predictions. This will be the updated prediction.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Â· å°†æ–°æ ‘çš„é¢„æµ‹ç»“æœä¹˜ä»¥å­¦ä¹ ç‡ï¼Œç„¶åå°†è¿™äº›ç¼©æ”¾åçš„é¢„æµ‹å€¼åŠ åˆ°å½“å‰æ¨¡å‹çš„é¢„æµ‹ç»“æœä¸­ã€‚è¿™å°†æ˜¯æ›´æ–°åçš„é¢„æµ‹ã€‚
- en: '![](../Images/76a4aa18c7ed15a837c54901efb09dcc.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/76a4aa18c7ed15a837c54901efb09dcc.png)'
- en: 'Our model updates its predictions by taking small steps: it adds just 10% (our
    learning rate of 0.1) of each predicted residual to our initial prediction of
    37.43, creating slightly improved predictions.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡é‡‡å–å°æ­¥è¿›æ¥æ›´æ–°é¢„æµ‹ï¼šå®ƒåªå°†æ¯ä¸ªé¢„æµ‹æ®‹å·®çš„10%ï¼ˆå­¦ä¹ ç‡ä¸º0.1ï¼‰åŠ åˆ°æˆ‘ä»¬æœ€åˆçš„37.43é¢„æµ‹å€¼ä¸Šï¼Œä»è€Œå¾—åˆ°ç¨å¾®æ”¹è¿›çš„é¢„æµ‹ã€‚
- en: For the Second Tree
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯¹äºç¬¬äºŒæ£µæ ‘
- en: 2.1\. Calculate new residuals based on current model
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 2.1\. åŸºäºå½“å‰æ¨¡å‹è®¡ç®—æ–°çš„æ®‹å·®
- en: a. Compute the difference between the target and current predictions.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: a. è®¡ç®—ç›®æ ‡é¢„æµ‹å€¼ä¸å½“å‰é¢„æµ‹å€¼ä¹‹é—´çš„å·®å¼‚ã€‚
- en: These residuals will be a bit different from the first iteration.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ®‹å·®ä¸ç¬¬ä¸€æ¬¡è¿­ä»£çš„æ®‹å·®ä¼šç•¥æœ‰ä¸åŒã€‚
- en: '![](../Images/2843e92a51f3b6bd97696bf603c17128.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2843e92a51f3b6bd97696bf603c17128.png)'
- en: After updating our predictions with the first tree, we calculate new residuals
    â€” notice how theyâ€™re slightly smaller than the original ones, showing our predictions
    are gradually improving.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´æ–°äº†ç¬¬ä¸€æ£µæ ‘çš„é¢„æµ‹åï¼Œæˆ‘ä»¬è®¡ç®—æ–°çš„æ®‹å·®â€”â€”æ³¨æ„åˆ°å®ƒä»¬æ¯”åŸæ¥çš„æ®‹å·®ç¨å¾®å°ä¸€äº›ï¼Œæ˜¾ç¤ºæˆ‘ä»¬çš„é¢„æµ‹é€æ¸å¾—åˆ°äº†æ”¹å–„ã€‚
- en: 2.2\. Build a new tree to predict these residuals. Same process as first tree,
    but targeting new residuals.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 2.2\. æ„å»ºä¸€æ£µæ–°æ ‘æ¥é¢„æµ‹è¿™äº›æ®‹å·®ã€‚è¿‡ç¨‹ä¸ç¬¬ä¸€æ£µæ ‘ç›¸åŒï¼Œä½†ç›®æ ‡æ˜¯æ–°çš„æ®‹å·®ã€‚
- en: '![](../Images/bd50d352c28d235ec429312a6fc54dd7.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd50d352c28d235ec429312a6fc54dd7.png)'
- en: Starting our second tree to predict the new, smaller residuals â€” weâ€™ll use the
    same tree-building process as before, but now weâ€™re trying to catch the errors
    our first tree missed
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å¯åŠ¨æˆ‘ä»¬çš„ç¬¬äºŒæ£µæ ‘æ¥é¢„æµ‹æ–°çš„ã€æ›´å°çš„æ®‹å·®â€”â€”æˆ‘ä»¬å°†ä½¿ç”¨ä¸ä¹‹å‰ç›¸åŒçš„æ ‘æ„å»ºè¿‡ç¨‹ï¼Œä½†è¿™æ¬¡æˆ‘ä»¬è¯•å›¾æ•æ‰ç¬¬ä¸€æ£µæ ‘é—æ¼çš„é”™è¯¯ã€‚
- en: 2.3\. Calculate the mean residuals for each leaf
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 2.3\. è®¡ç®—æ¯ä¸ªå¶å­èŠ‚ç‚¹çš„å‡å€¼æ®‹å·®
- en: '![](../Images/b5226feb595f93476c22f6f3acfb4488.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5226feb595f93476c22f6f3acfb4488.png)'
- en: The second tree follows an identical structure to our first tree with the same
    weather features and split points, but with smaller values in its leaves â€” showing
    weâ€™re fine-tuning the remaining errors.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒæ£µæ ‘ä¸ç¬¬ä¸€æ£µæ ‘çš„ç»“æ„ç›¸åŒï¼Œä½¿ç”¨ç›¸åŒçš„å¤©æ°”ç‰¹å¾å’Œåˆ†è£‚ç‚¹ï¼Œä½†å…¶å¶èŠ‚ç‚¹çš„å€¼è¾ƒå°â€”â€”è¿™è¡¨æ˜æˆ‘ä»¬æ­£åœ¨å¾®è°ƒå‰©ä½™çš„è¯¯å·®ã€‚
- en: 2.4\. Update model predictions
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 2.4\. æ›´æ–°æ¨¡å‹é¢„æµ‹
- en: Â· Multiply the new treeâ€™s predictions by the learning rate.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Â· å°†æ–°æ ‘çš„é¢„æµ‹ä¹˜ä»¥å­¦ä¹ ç‡ã€‚
- en: Â· Add the new scaled tree predictions to the running total.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Â· å°†æ–°ç¼©æ”¾è¿‡çš„æ ‘é¢„æµ‹åŠ åˆ°å½“å‰æ€»å’Œä¸­ã€‚
- en: '![](../Images/269aeb6fbb665e5f8d830fff8afb22f0.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/269aeb6fbb665e5f8d830fff8afb22f0.png)'
- en: After running our data through the second tree, we again take small steps with
    our 0.1 learning rate to update predictions, and calculate new residuals that
    are even smaller than before â€” our model is gradually learning the patterns.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å°†æ•°æ®é€šè¿‡ç¬¬äºŒæ£µæ ‘åï¼Œæˆ‘ä»¬å†æ¬¡ä»¥0.1çš„å­¦ä¹ ç‡åšå‡ºå°æ­¥è°ƒæ•´ä»¥æ›´æ–°é¢„æµ‹ï¼Œå¹¶è®¡ç®—å‡ºæ¯”ä¹‹å‰æ›´å°çš„æ®‹å·®â€”â€”æˆ‘ä»¬çš„æ¨¡å‹æ­£åœ¨é€æ¸å­¦ä¹ æ¨¡å¼ã€‚
- en: For the Third Tree onwards
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»ç¬¬ä¸‰æ£µæ ‘å¼€å§‹
- en: Repeat Steps 2.1â€“2.3 for remaining iterations. Note that each tree sees different
    residuals.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹å‰©ä½™çš„è¿­ä»£é‡å¤æ­¥éª¤2.1â€“2.3ã€‚æ³¨æ„ï¼Œæ¯æ£µæ ‘çœ‹åˆ°çš„æ®‹å·®ä¸åŒã€‚
- en: Â· Trees progressively focus on harder-to-predict patterns
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Â· å†³ç­–æ ‘é€æ¸ä¸“æ³¨äºæ›´éš¾é¢„æµ‹çš„æ¨¡å¼
- en: Â· Learning rate prevents overfitting by limiting each treeâ€™s contribution
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Â· å­¦ä¹ ç‡é€šè¿‡é™åˆ¶æ¯æ£µæ ‘çš„è´¡çŒ®æ¥é˜²æ­¢è¿‡æ‹Ÿåˆ
- en: '![](../Images/c3bec57f3cfc4d79ab75c562246a8250.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3bec57f3cfc4d79ab75c562246a8250.png)'
- en: As we build more trees, notice how the split points slowly shift and the residual
    values in the leaves get smaller â€” by tree 50, weâ€™re making tiny adjustments using
    different combinations of features compared to our first trees.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€æˆ‘ä»¬æ„å»ºæ›´å¤šçš„æ ‘ï¼Œæ³¨æ„åˆ†è£‚ç‚¹å¦‚ä½•é€æ¸å˜åŒ–ï¼Œå¶èŠ‚ç‚¹ä¸­çš„æ®‹å·®å€¼å˜å¾—æ›´å°â€”â€”åˆ°ç¬¬50æ£µæ ‘æ—¶ï¼Œæˆ‘ä»¬é€šè¿‡ä¸åŒçš„ç‰¹å¾ç»„åˆè¿›è¡Œå¾®è°ƒï¼Œè°ƒæ•´çš„å¹…åº¦æ¯”æœ€åˆçš„æ ‘è¦å°ã€‚
- en: '[PRE1]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/8a98aab772c6bdcf801764e343cb4c17.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a98aab772c6bdcf801764e343cb4c17.png)'
- en: 'Visualization from scikit-learn shows how our gradient boosting trees evolve:
    from Tree 1 making large splits with big prediction values, to Tree 50 making
    refined splits with tiny adjustments â€” each tree focuses on correcting the remaining
    errors from previous trees.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥è‡ªscikit-learnçš„å¯è§†åŒ–å±•ç¤ºäº†æˆ‘ä»¬çš„æ¢¯åº¦æå‡æ ‘å¦‚ä½•æ¼”å˜ï¼šä»æ ‘1è¿›è¡Œå¤§èŒƒå›´çš„åˆ†è£‚å¹¶ç»™å‡ºå¤§é¢„æµ‹å€¼ï¼Œåˆ°æ ‘50è¿›è¡Œç²¾ç»†çš„åˆ†è£‚å¹¶åšå‡ºå¾®å°çš„è°ƒæ•´â€”â€”æ¯æ£µæ ‘éƒ½ä¸“æ³¨äºä¿®æ­£å‰é¢æ ‘æ‰€äº§ç”Ÿçš„å‰©ä½™è¯¯å·®ã€‚
- en: Testing Step
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æµ‹è¯•æ­¥éª¤
- en: 'For predicting:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„æµ‹æ—¶ï¼š
- en: a. Start with the initial prediction (the average number of players)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: a. ä»åˆå§‹é¢„æµ‹å¼€å§‹ï¼ˆç©å®¶çš„å¹³å‡æ•°é‡ï¼‰
- en: b. Run the input through each tree to get its predicted adjustment
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: b. å°†è¾“å…¥æ•°æ®ä¼ é€’ç»™æ¯æ£µæ ‘ä»¥è·å¾—å…¶é¢„æµ‹çš„è°ƒæ•´å€¼
- en: c. Scale each treeâ€™s prediction by the learning rate.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: c. æŒ‰ç…§å­¦ä¹ ç‡ç¼©æ”¾æ¯æ£µæ ‘çš„é¢„æµ‹å€¼ã€‚
- en: d. Add all these adjustments to the initial prediction
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: d. å°†æ‰€æœ‰è¿™äº›è°ƒæ•´æ·»åŠ åˆ°åˆå§‹é¢„æµ‹ä¸­
- en: e. The sum directly gives us the predicted number of players
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: e. è¿™äº›å’Œç›´æ¥ç»™å‡ºæˆ‘ä»¬é¢„æµ‹çš„ç©å®¶æ•°é‡
- en: '![](../Images/4b7ce876ba3cdfad96da7c7e0415989f.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b7ce876ba3cdfad96da7c7e0415989f.png)'
- en: When predicting on unseen data, each tree contributes its small prediction,
    starting from 5.57 in Tree 1 down to 0.008 in Tree 50 â€” all these predictions
    are scaled by our 0.1 learning rate and added to our base prediction of 37.43
    to get the final answer.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¯¹æœªè§æ•°æ®è¿›è¡Œé¢„æµ‹æ—¶ï¼Œæ¯æ£µæ ‘éƒ½ä¼šè´¡çŒ®ä¸€ä¸ªå°çš„é¢„æµ‹å€¼ï¼Œä»æ ‘1çš„5.57å¼€å§‹ï¼Œåˆ°æ ‘50çš„0.008â€”â€”æ‰€æœ‰è¿™äº›é¢„æµ‹éƒ½è¢«æˆ‘ä»¬çš„0.1å­¦ä¹ ç‡è¿›è¡Œç¼©æ”¾ï¼Œå¹¶åŠ åˆ°æˆ‘ä»¬çš„åŸºç¡€é¢„æµ‹å€¼37.43ä¸Šï¼Œå¾—åˆ°æœ€ç»ˆçš„ç­”æ¡ˆã€‚
- en: Evaluation Step
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯„ä¼°æ­¥éª¤
- en: After building all the trees, we can evaluate the test set.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºæ‰€æœ‰æ ‘åï¼Œæˆ‘ä»¬å¯ä»¥è¯„ä¼°æµ‹è¯•é›†ã€‚
- en: '![](../Images/32381bf34c6c5780246b6047af2863bd.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/32381bf34c6c5780246b6047af2863bd.png)'
- en: Our gradient boosting model achieves an RMSE of 4.785, quite an improvement
    over [a single regression treeâ€™s 5.27](https://medium.com/towards-data-science/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef)
    â€” showing how combining many small corrections leads to better predictions than
    one complex tree!
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„æ¢¯åº¦æå‡æ¨¡å‹è¾¾åˆ°äº†4.785çš„RMSEï¼Œç›¸è¾ƒäº[aæ£µå›å½’æ ‘çš„5.27](https://medium.com/towards-data-science/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef)æœ‰äº†æ˜¾è‘—çš„æå‡â€”â€”è¿™è¡¨æ˜å°†å¤šä¸ªå°çš„è°ƒæ•´ç»„åˆèµ·æ¥ï¼Œæ¯”å•æ£µå¤æ‚çš„æ ‘æ›´èƒ½åšå‡ºå‡†ç¡®çš„é¢„æµ‹ï¼
- en: '[PRE2]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Key Parameters
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…³é”®å‚æ•°
- en: 'Here are the key parameters for Gradient Boosting, particularly in `scikit-learn`:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯æ¢¯åº¦æå‡ä¸­çš„å…³é”®å‚æ•°ï¼Œç‰¹åˆ«æ˜¯åœ¨`scikit-learn`ä¸­ï¼š
- en: '`max_depth`: The depth of trees used to model residuals. Unlike AdaBoost which
    uses stumps, Gradient Boosting works better with deeper trees (typically 3-8 levels).
    Deeper trees capture more complex patterns but risk overfitting.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_depth`ï¼šç”¨äºå»ºæ¨¡æ®‹å·®çš„æ ‘çš„æ·±åº¦ã€‚ä¸ä½¿ç”¨æ ‘æ¡©çš„AdaBoostä¸åŒï¼Œæ¢¯åº¦æå‡åœ¨æ·±å±‚æ ‘ï¼ˆé€šå¸¸ä¸º3-8å±‚ï¼‰ä¸Šæ•ˆæœæ›´å¥½ã€‚æ·±å±‚æ ‘èƒ½å¤Ÿæ•æ‰æ›´å¤æ‚çš„æ¨¡å¼ï¼Œä½†ä¹Ÿæœ‰è¿‡æ‹Ÿåˆçš„é£é™©ã€‚'
- en: '`n_estimators`: The number of trees to be used (typically 100-1000). More trees
    usually improve performance when paired with a small learning rate.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_estimators`ï¼šè¦ä½¿ç”¨çš„æ ‘çš„æ•°é‡ï¼ˆé€šå¸¸ä¸º 100-1000ï¼‰ã€‚å½“ä¸è¾ƒå°çš„å­¦ä¹ ç‡é…å¯¹æ—¶ï¼Œæ›´å¤šçš„æ ‘é€šå¸¸èƒ½æé«˜æ€§èƒ½ã€‚'
- en: '`learning_rate`: Also called "shrinkage", this scales each tree''s contribution
    (typically 0.01-0.1). Smaller values require more trees but often give better
    results by making the learning process more fine-grained.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`learning_rate`ï¼šä¹Ÿç§°ä¸ºâ€œæ”¶ç¼©â€ï¼Œç”¨äºç¼©æ”¾æ¯æ£µæ ‘çš„è´¡çŒ®ï¼ˆé€šå¸¸ä¸º 0.01-0.1ï¼‰ã€‚è¾ƒå°çš„å€¼éœ€è¦æ›´å¤šçš„æ ‘ï¼Œä½†é€šè¿‡ä½¿å­¦ä¹ è¿‡ç¨‹æ›´ç²¾ç»†åŒ–ï¼Œé€šå¸¸èƒ½è·å¾—æ›´å¥½çš„ç»“æœã€‚'
- en: '`subsample`: The fraction of samples used to train each tree (typically 0.5-0.8).
    This optional feature adds randomness that can improve robustness and reduce overfitting.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`subsample`ï¼šç”¨äºè®­ç»ƒæ¯æ£µæ ‘çš„æ ·æœ¬æ¯”ä¾‹ï¼ˆé€šå¸¸ä¸º 0.5-0.8ï¼‰ã€‚è¿™ä¸ªå¯é€‰ç‰¹æ€§å¢åŠ äº†éšæœºæ€§ï¼Œå¯ä»¥æé«˜é²æ£’æ€§å¹¶å‡å°‘è¿‡æ‹Ÿåˆã€‚'
- en: 'These parameters work together: a small learning rate needs more trees, while
    deeper trees might need a smaller learning rate to avoid overfitting.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å‚æ•°æ˜¯ç›¸äº’é…åˆå·¥ä½œçš„ï¼šè¾ƒå°çš„å­¦ä¹ ç‡éœ€è¦æ›´å¤šçš„æ ‘ï¼Œè€Œè¾ƒæ·±çš„æ ‘å¯èƒ½éœ€è¦è¾ƒå°çš„å­¦ä¹ ç‡ä»¥é¿å…è¿‡æ‹Ÿåˆã€‚
- en: Key differences from AdaBoost
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸ AdaBoost çš„å…³é”®åŒºåˆ«
- en: 'Both AdaBoost and Gradient Boosting are boosting algorithms, but the way they
    learn from their mistakes are different. Here are the key differences:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost å’Œ Gradient Boosting éƒ½æ˜¯æå‡ç®—æ³•ï¼Œä½†å®ƒä»¬ä»é”™è¯¯ä¸­å­¦ä¹ çš„æ–¹å¼ä¸åŒã€‚ä»¥ä¸‹æ˜¯å®ƒä»¬çš„å…³é”®åŒºåˆ«ï¼š
- en: '`max_depth` is typically higher (3-8) in Gradient Boosting, while AdaBoost
    prefers stumps.'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`max_depth` é€šå¸¸åœ¨ Gradient Boosting ä¸­è¾ƒé«˜ï¼ˆ3-8ï¼‰ï¼Œè€Œ AdaBoost æ›´å€¾å‘äºä½¿ç”¨æ ‘æ¡©ã€‚'
- en: No `sample_weight` updates because Gradient Boosting uses residuals instead
    of sample weighting.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ²¡æœ‰ `sample_weight` æ›´æ–°ï¼Œå› ä¸º Gradient Boosting ä½¿ç”¨æ®‹å·®è€Œä¸æ˜¯æ ·æœ¬åŠ æƒã€‚
- en: The `learning_rate` is typically much smaller (0.01-0.1) compared to AdaBoost's
    larger values (0.1-1.0).
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`learning_rate` é€šå¸¸æ¯” AdaBoost çš„è¾ƒå¤§å€¼ï¼ˆ0.1-1.0ï¼‰å°å¾—å¤šï¼ˆ0.01-0.1ï¼‰ã€‚'
- en: Initial prediction starts from the mean while AdaBoost starts from zero.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆå§‹é¢„æµ‹ä»å‡å€¼å¼€å§‹ï¼Œè€Œ AdaBoost ä»é›¶å¼€å§‹ã€‚
- en: Trees are combined through simple addition rather than weighted voting, making
    each treeâ€™s contribution more straightforward.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ ‘æ˜¯é€šè¿‡ç®€å•çš„åŠ æ³•è€Œä¸æ˜¯åŠ æƒæŠ•ç¥¨æ¥ç»„åˆçš„ï¼Œè¿™ä½¿å¾—æ¯æ£µæ ‘çš„è´¡çŒ®æ›´åŠ ç›´è§‚ã€‚
- en: Optional `subsample` parameter adds randomness, a feature not present in standard
    AdaBoost.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯é€‰çš„ `subsample` å‚æ•°å¢åŠ äº†éšæœºæ€§ï¼Œè¿™æ˜¯æ ‡å‡† AdaBoost æ‰€æ²¡æœ‰çš„ç‰¹æ€§ã€‚
- en: Pros & Cons
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¼˜ç‚¹ä¸ç¼ºç‚¹
- en: 'Pros:'
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¼˜ç‚¹ï¼š
- en: '**Step-by-Step Error Fixing:** In Gradient Boosting, each new tree focuses
    on correcting the mistakes made by the previous ones. This makes the model better
    at improving its predictions in areas where it was previously wrong.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é€æ­¥é”™è¯¯ä¿®æ­£ï¼š** åœ¨ Gradient Boosting ä¸­ï¼Œæ¯æ£µæ–°æ ‘ä¸“æ³¨äºä¿®æ­£å‰ä¸€æ£µæ ‘çš„é”™è¯¯ã€‚è¿™ä½¿å¾—æ¨¡å‹åœ¨ä¹‹å‰é”™è¯¯çš„åŒºåŸŸæ›´å¥½åœ°æ”¹è¿›é¢„æµ‹ã€‚'
- en: '**Flexible Error Measures:** Unlike AdaBoost, Gradient Boosting can optimize
    different types of error measurements (like mean absolute error, mean squared
    error, or others). This makes it adaptable to various kinds of problems.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**çµæ´»çš„è¯¯å·®åº¦é‡ï¼š** ä¸ AdaBoost ä¸åŒï¼ŒGradient Boosting å¯ä»¥ä¼˜åŒ–ä¸åŒç±»å‹çš„è¯¯å·®åº¦é‡ï¼ˆå¦‚å¹³å‡ç»å¯¹è¯¯å·®ã€å‡æ–¹è¯¯å·®ç­‰ï¼‰ã€‚è¿™ä½¿å¾—å®ƒå¯ä»¥é€‚åº”å„ç§é—®é¢˜ã€‚'
- en: '**High Accuracy:** By using more detailed trees and carefully controlling the
    learning rate, Gradient Boosting often provides more accurate results than other
    algorithms, especially for well-structured data.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é«˜å‡†ç¡®åº¦ï¼š** é€šè¿‡ä½¿ç”¨æ›´è¯¦ç»†çš„æ ‘å¹¶ä»”ç»†æ§åˆ¶å­¦ä¹ ç‡ï¼ŒGradient Boosting å¾€å¾€èƒ½æä¾›æ¯”å…¶ä»–ç®—æ³•æ›´å‡†ç¡®çš„ç»“æœï¼Œå°¤å…¶æ˜¯å¯¹äºç»“æ„è‰¯å¥½çš„æ•°æ®ã€‚'
- en: 'Cons:'
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¼ºç‚¹ï¼š
- en: '**Risk of Overfitting:** The use of deeper trees and the sequential building
    process can cause the model to fit the training data too closely, which may reduce
    its performance on new data. This requires careful tuning of tree depth, learning
    rate, and the number of trees.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è¿‡æ‹Ÿåˆçš„é£é™©ï¼š** ä½¿ç”¨æ›´æ·±çš„æ ‘å’Œé¡ºåºæ„å»ºè¿‡ç¨‹å¯èƒ½å¯¼è‡´æ¨¡å‹è¿‡åº¦æ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼Œä»è€Œé™ä½åœ¨æ–°æ•°æ®ä¸Šçš„è¡¨ç°ã€‚è¿™éœ€è¦ä»”ç»†è°ƒæ•´æ ‘çš„æ·±åº¦ã€å­¦ä¹ ç‡å’Œæ ‘çš„æ•°é‡ã€‚'
- en: '**Slow Training Process:** Like AdaBoost, trees must be built one after another,
    making it slower to train compared to algorithms that can build trees in parallel,
    like Random Forest. Each tree relies on the errors of the previous ones.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è®­ç»ƒè¿‡ç¨‹ç¼“æ…¢ï¼š** å’Œ AdaBoost ä¸€æ ·ï¼Œæ ‘å¿…é¡»ä¸€ä¸ªæ¥ä¸€ä¸ªåœ°æ„å»ºï¼Œå› æ­¤ç›¸æ¯”äºå¯ä»¥å¹¶è¡Œæ„å»ºæ ‘çš„ç®—æ³•ï¼ˆå¦‚éšæœºæ£®æ—ï¼‰ï¼Œè®­ç»ƒé€Ÿåº¦è¾ƒæ…¢ã€‚æ¯æ£µæ ‘éƒ½ä¾èµ–äºå‰ä¸€æ£µæ ‘çš„é”™è¯¯ã€‚'
- en: '**High Memory Use:** The need for deeper and more numerous trees means Gradient
    Boosting can consume more memory than simpler boosting methods such as AdaBoost.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é«˜å†…å­˜ä½¿ç”¨ï¼š** ç”±äºéœ€è¦æ›´æ·±å’Œæ›´å¤šçš„æ ‘ï¼ŒGradient Boosting çš„å†…å­˜æ¶ˆè€—å¯èƒ½æ¯”åƒ AdaBoost è¿™æ ·çš„ç®€å•æå‡æ–¹æ³•æ›´é«˜ã€‚'
- en: '**Sensitive to Settings:** The effectiveness of Gradient Boosting heavily depends
    on finding the right combination of learning rate, tree depth, and number of trees,
    which can be more complex and time-consuming than tuning simpler algorithms.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¯¹è®¾ç½®æ•æ„Ÿï¼š** æ¢¯åº¦æå‡çš„æœ‰æ•ˆæ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºæ‰¾åˆ°åˆé€‚çš„å­¦ä¹ ç‡ã€æ ‘çš„æ·±åº¦å’Œæ ‘çš„æ•°é‡çš„ç»„åˆï¼Œè¿™å¯èƒ½æ¯”è°ƒä¼˜ç®€å•ç®—æ³•æ›´å¤æ‚ä¸”è€—æ—¶ã€‚'
- en: Final Remarks
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è¯­
- en: Gradient Boosting is a major improvement in boosting algorithms. This success
    has led to popular versions like XGBoost and LightGBM, which are widely used in
    machine learning competitions and real-world applications.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦æå‡ï¼ˆGradient Boostingï¼‰æ˜¯æå‡ç®—æ³•çš„ä¸€é¡¹é‡è¦æ”¹è¿›ã€‚è¿™ä¸€æˆåŠŸå‚¬ç”Ÿäº†åƒXGBoostå’ŒLightGBMè¿™æ ·çš„æµè¡Œç‰ˆæœ¬ï¼Œå®ƒä»¬åœ¨æœºå™¨å­¦ä¹ ç«èµ›å’Œå®é™…åº”ç”¨ä¸­å¾—åˆ°äº†å¹¿æ³›ä½¿ç”¨ã€‚
- en: While Gradient Boosting requires more careful tuning than simpler algorithms
    â€” especially when adjusting the depth of decision trees, the learning rate, and
    the number of trees â€” it is very flexible and powerful. This makes it a top choice
    for problems with structured data.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æ¢¯åº¦æå‡æ¯”ç®€å•ç®—æ³•éœ€è¦æ›´ç²¾ç»†çš„è°ƒä¼˜â€”â€”å°¤å…¶æ˜¯åœ¨è°ƒæ•´å†³ç­–æ ‘æ·±åº¦ã€å­¦ä¹ ç‡å’Œæ ‘çš„æ•°é‡æ—¶â€”â€”å®ƒéå¸¸çµæ´»ä¸”å¼ºå¤§ã€‚è¿™ä½¿å¾—å®ƒæˆä¸ºç»“æ„åŒ–æ•°æ®é—®é¢˜çš„é¦–é€‰æ–¹æ³•ã€‚
- en: Gradient Boosting can handle complex relationships that simpler methods like
    AdaBoost might miss. Its continued popularity and ongoing improvements show that
    the approach of using gradients and building models step-by-step remains highly
    important in modern machine learning.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦æå‡èƒ½å¤Ÿå¤„ç†ç®€å•æ–¹æ³•ï¼ˆå¦‚AdaBoostï¼‰å¯èƒ½å¿½è§†çš„å¤æ‚å…³ç³»ã€‚å…¶æŒç»­çš„æµè¡Œå’Œä¸æ–­çš„æ”¹è¿›è¡¨æ˜ï¼Œä½¿ç”¨æ¢¯åº¦å¹¶é€æ­¥æ„å»ºæ¨¡å‹çš„æ–¹æ³•åœ¨ç°ä»£æœºå™¨å­¦ä¹ ä¸­ä¾ç„¶æä¸ºé‡è¦ã€‚
- en: ğŸŒŸ Gradient Boosting Regressor Code Summarized
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸŒŸ æ¢¯åº¦æå‡å›å½’å™¨ä»£ç æ€»ç»“
- en: '[PRE3]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Further Reading
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¿›ä¸€æ­¥é˜…è¯»
- en: For a detailed explanation of the [GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)
    and its implementation in scikit-learn, readers can refer to the official documentation,
    which provides comprehensive information on its usage and parameters.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº[æ¢¯åº¦æå‡å›å½’å™¨ï¼ˆGradientBoostingRegressorï¼‰](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)åŠå…¶åœ¨scikit-learnä¸­çš„å®ç°çš„è¯¦ç»†è§£é‡Šï¼Œè¯»è€…å¯ä»¥å‚è€ƒå®˜æ–¹æ–‡æ¡£ï¼Œè¯¥æ–‡æ¡£æä¾›äº†æœ‰å…³å…¶ä½¿ç”¨å’Œå‚æ•°çš„å…¨é¢ä¿¡æ¯ã€‚
- en: Technical Environment
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æŠ€æœ¯ç¯å¢ƒ
- en: This article uses Python 3.7 and scikit-learn 1.6\. While the concepts discussed
    are generally applicable, specific code implementations may vary slightly with
    different versions.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡ä½¿ç”¨çš„æ˜¯Python 3.7å’Œscikit-learn 1.6ç‰ˆæœ¬ã€‚è™½ç„¶æ‰€è®¨è®ºçš„æ¦‚å¿µå…·æœ‰æ™®éé€‚ç”¨æ€§ï¼Œä½†å…·ä½“çš„ä»£ç å®ç°å¯èƒ½ä¼šå› ç‰ˆæœ¬ä¸åŒè€Œç•¥æœ‰å·®å¼‚ã€‚
- en: About the Illustrations
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…³äºæ’å›¾
- en: Unless otherwise noted, all images are created by the author, incorporating
    licensed design elements from Canva Pro.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤éå¦æœ‰è¯´æ˜ï¼Œæ‰€æœ‰å›¾ç‰‡å‡ç”±ä½œè€…åˆ›ä½œï¼ŒåŒ…å«æ¥è‡ªCanva Proçš„æˆæƒè®¾è®¡å…ƒç´ ã€‚
- en: 'ğ™ğ™šğ™š ğ™¢ğ™¤ğ™§ğ™š ğ™€ğ™£ğ™¨ğ™šğ™¢ğ™—ğ™¡ğ™š ğ™‡ğ™šğ™–ğ™§ğ™£ğ™ğ™£ğ™œ ğ™ğ™šğ™§ğ™š:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğ™ğ™šğ™š ğ™¢ğ™¤ğ™§ğ™š ğ™€ğ™£ğ™¨ğ™šğ™¢ğ™—ğ™¡ğ™š ğ™‡ğ™šğ™–ğ™§ğ™£ğ™ğ™£ğ™œ ğ™ğ™šğ™§ğ™š:'
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----c098d1ae425c--------------------------------)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----c098d1ae425c--------------------------------)'
- en: Ensemble Learning
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é›†æˆå­¦ä¹ 
- en: '[View list](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----c098d1ae425c--------------------------------)4
    stories![](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[æŸ¥çœ‹åˆ—è¡¨](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----c098d1ae425c--------------------------------)4ç¯‡æ•…äº‹![](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)'
- en: 'ğ™”ğ™¤ğ™ª ğ™¢ğ™ğ™œğ™ğ™© ğ™–ğ™¡ğ™¨ğ™¤ ğ™¡ğ™ğ™ ğ™š:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğ™”ğ™¤ğ™ª ğ™¢ğ™ğ™œğ™ğ™© ğ™–ğ™¡ğ™¨ğ™¤ ğ™¡ğ™ğ™ ğ™š:'
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----c098d1ae425c--------------------------------)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----c098d1ae425c--------------------------------)'
- en: Regression Algorithms
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å›å½’ç®—æ³•
- en: '[View list](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----c098d1ae425c--------------------------------)5
    stories![A cartoon doll with pigtails and a pink hat. This â€œdummyâ€ doll, with
    its basic design and heart-adorned shirt, visually represents the concept of a
    dummy regressor in machine. Just as this toy-like figure is a simplified, static
    representation of a person, a dummy regressor is a basic models serve as baselines
    for more sophisticated analyses.](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[æŸ¥çœ‹åˆ—è¡¨](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----c098d1ae425c--------------------------------)5ä¸ªæ•…äº‹![ä¸€åªæˆ´ç€ç²‰è‰²å¸½å­ã€æ‰ç€åŒé©¬å°¾çš„å¡é€šå¨ƒå¨ƒã€‚è¿™ä¸ªâ€œå‡äººâ€å¨ƒå¨ƒï¼Œå‡­å€Ÿå…¶ç®€åŒ–çš„è®¾è®¡å’Œå¿ƒå½¢è£…é¥°çš„è¡£æœï¼Œå½¢è±¡åœ°è¡¨ç°äº†æœºå™¨å­¦ä¹ ä¸­çš„â€œå‡å›å½’å™¨â€æ¦‚å¿µã€‚å°±åƒè¿™ä¸ªç©å…·èˆ¬çš„è§’è‰²æ˜¯ä¸€ä¸ªç®€åŒ–çš„ã€é™æ€çš„äººç‰©è¡¨ç°ä¸€æ ·ï¼Œå‡å›å½’å™¨ä¹Ÿæ˜¯ä¸€ä¸ªåŸºç¡€æ¨¡å‹ï¼Œä½œä¸ºæ›´å¤æ‚åˆ†æçš„åŸºå‡†ã€‚](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)'
