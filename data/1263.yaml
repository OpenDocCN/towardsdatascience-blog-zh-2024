- en: Why LLMs are not Good for Coding — Part II
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么LLMs不适合编码 — 第二部分
- en: 原文：[https://towardsdatascience.com/llms-coding-software-development-artificial-intelligence-68f195bb2ad3?source=collection_archive---------1-----------------------#2024-05-20](https://towardsdatascience.com/llms-coding-software-development-artificial-intelligence-68f195bb2ad3?source=collection_archive---------1-----------------------#2024-05-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/llms-coding-software-development-artificial-intelligence-68f195bb2ad3?source=collection_archive---------1-----------------------#2024-05-20](https://towardsdatascience.com/llms-coding-software-development-artificial-intelligence-68f195bb2ad3?source=collection_archive---------1-----------------------#2024-05-20)
- en: Large Language Models for Coding Tasks
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于编码任务的大型语言模型
- en: '[](https://medium.com/@andvalenzuela?source=post_page---byline--68f195bb2ad3--------------------------------)[![Andrea
    Valenzuela](../Images/ddfc1534af92413fd91076f826cc49b6.png)](https://medium.com/@andvalenzuela?source=post_page---byline--68f195bb2ad3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--68f195bb2ad3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--68f195bb2ad3--------------------------------)
    [Andrea Valenzuela](https://medium.com/@andvalenzuela?source=post_page---byline--68f195bb2ad3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@andvalenzuela?source=post_page---byline--68f195bb2ad3--------------------------------)[![Andrea
    Valenzuela](../Images/ddfc1534af92413fd91076f826cc49b6.png)](https://medium.com/@andvalenzuela?source=post_page---byline--68f195bb2ad3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--68f195bb2ad3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--68f195bb2ad3--------------------------------)
    [Andrea Valenzuela](https://medium.com/@andvalenzuela?source=post_page---byline--68f195bb2ad3--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--68f195bb2ad3--------------------------------)
    ·6 min read·May 20, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--68f195bb2ad3--------------------------------)
    ·6分钟阅读·2024年5月20日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/591bff2e771516cb8f20a5bc9481bda2.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/591bff2e771516cb8f20a5bc9481bda2.png)'
- en: Self-made image.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 自制图像。
- en: 'After publishing the first article of this series, “[Why LLMs are Not Good
    for Coding](https://medium.com/towards-data-science/llms-coding-chatgpt-python-artificial-intelligence-4ea7a7bbdd93),”
    I received several comments on social media, such as:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在发布这篇系列文章的第一篇“[为什么LLMs不适合编码](https://medium.com/towards-data-science/llms-coding-chatgpt-python-artificial-intelligence-4ea7a7bbdd93)”后，我在社交媒体上收到了几条评论，例如：
- en: “I am using ChatGPT for coding and it works perfectly fine.”
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我在用ChatGPT进行编码，它工作得非常好。”
- en: ''
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: “You are wrong. Large Language Models are useful coding assistants.”
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “你错了。大型语言模型是有用的编码助手。”
- en: I was surprised by these reactions since the purpose of this article series
    is not to discourage anyone from using Large Language Models (LLMs) for coding
    but to **identify the key areas that need improvement to transform LLMs into more
    effective coding assistants.**
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我对这些反应感到惊讶，因为这篇文章系列的目的并不是劝阻任何人使用大型语言模型（LLMs）进行编码，而是**识别需要改进的关键领域，以将LLMs转变为更有效的编码助手。**
- en: While LLMs such as ChatGPT can be helpful in some instances, **they often produce
    code that could be syntactically correct but suboptimal** or even incorrect in
    functionality.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管像ChatGPT这样的LLMs在某些情况下可能有帮助，**它们经常生成语法正确但不够优化**甚至在功能上不正确的代码。
- en: In the previous article, we discussed how the tokenizer, the complexity of context
    windows when applied to code, and the nature of the training itself can influence
    the performance of these models in coding tasks.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一篇文章中，我们讨论了标记器、在应用于代码时上下文窗口的复杂性以及训练本身的性质如何影响这些模型在编码任务中的表现。
- en: 'In this second article, we will explore in more depth the type of training
    these models undergo to be used for coding tasks, as well as another reason why
    **LLMs are not inherently proficient at coding “out of the box”: the challenge
    of staying up-to-date**…'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二篇文章中，我们将更深入地探讨这些模型在进行编码任务时所接受的训练类型，以及**LLMs在“开箱即用”时天生不擅长编码的另一个原因：保持更新的挑战**…
