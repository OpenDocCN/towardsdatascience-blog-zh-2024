- en: Train/Fine-Tune Segment Anything 2 (SAM 2) in 60 Lines of Code
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用 60 行代码训练/微调 Segment Anything 2 (SAM 2)
- en: 原文：[https://towardsdatascience.com/train-fine-tune-segment-anything-2-sam-2-in-60-lines-of-code-928dd29a63b3?source=collection_archive---------0-----------------------#2024-08-03](https://towardsdatascience.com/train-fine-tune-segment-anything-2-sam-2-in-60-lines-of-code-928dd29a63b3?source=collection_archive---------0-----------------------#2024-08-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/train-fine-tune-segment-anything-2-sam-2-in-60-lines-of-code-928dd29a63b3?source=collection_archive---------0-----------------------#2024-08-03](https://towardsdatascience.com/train-fine-tune-segment-anything-2-sam-2-in-60-lines-of-code-928dd29a63b3?source=collection_archive---------0-----------------------#2024-08-03)
- en: A step-by-step tutorial for fine-tuning SAM2 for custom segmentation tasks
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一步步的教程，教你如何微调 SAM2 以完成自定义分割任务
- en: '[](https://medium.com/@sagieppel?source=post_page---byline--928dd29a63b3--------------------------------)[![Sagi
    eppel](../Images/7f02c03dbfb21891b95ddb8c52cb5fff.png)](https://medium.com/@sagieppel?source=post_page---byline--928dd29a63b3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--928dd29a63b3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--928dd29a63b3--------------------------------)
    [Sagi eppel](https://medium.com/@sagieppel?source=post_page---byline--928dd29a63b3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@sagieppel?source=post_page---byline--928dd29a63b3--------------------------------)[![Sagi
    eppel](../Images/7f02c03dbfb21891b95ddb8c52cb5fff.png)](https://medium.com/@sagieppel?source=post_page---byline--928dd29a63b3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--928dd29a63b3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--928dd29a63b3--------------------------------)
    [Sagi eppel](https://medium.com/@sagieppel?source=post_page---byline--928dd29a63b3--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--928dd29a63b3--------------------------------)
    ·13 min read·Aug 3, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--928dd29a63b3--------------------------------)
    ·13 分钟阅读·2024年8月3日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[SAM2 (Segment Anything 2)](https://ai.meta.com/blog/segment-anything-2/) is
    a new model by Meta aiming to segment anything in an image without being limited
    to specific classes or domains. What makes this model unique is the scale of data
    on which it was trained: 11 million images, and 11 billion masks. This extensive
    training makes SAM2 a powerful starting point for training on new image segmentation
    tasks.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[SAM2 (Segment Anything 2)](https://ai.meta.com/blog/segment-anything-2/) 是
    Meta 推出的一款新模型，旨在对图像中的任何内容进行分割，而不局限于特定类别或领域。该模型的独特之处在于它的训练数据规模：1100万张图片和110亿个掩码。这种广泛的训练使得
    SAM2 成为一个强大的起点，可以用于新的图像分割任务的训练。'
- en: The question you might ask is if SAM can segment anything why do we even need
    to retrain it? The answer is that SAM is very good at common objects but can perform
    rather poorly on rare or domain-specific tasks.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问，如果 SAM 可以分割任何东西，为什么还需要重新训练它？答案是，SAM 在处理常见物体时表现优异，但在处理稀有或特定领域的任务时，效果可能较差。
- en: However, even in cases where SAM gives insufficient results, it is still possible
    to significantly improve the model’s ability by fine-tuning it on new data. In
    many cases, this will take less training data and give better results then training
    a model from scratch.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使在 SAM 给出的结果不足的情况下，仍然可以通过在新数据上对模型进行微调，显著提高模型的能力。在许多情况下，这需要的训练数据更少，且效果比从头开始训练一个模型更好。
- en: This tutorial demonstrates how to fine-tune SAM2 on new data in just 60 lines
    of code (excluding comments and imports).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程展示了如何用仅 60 行代码（不包括注释和导入）在新数据上微调 SAM2。
- en: '**The full training script of the can be found in:**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**完整的训练脚本可以在以下位置找到：**'
- en: '[](https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/TRAIN.py?source=post_page-----928dd29a63b3--------------------------------)
    [## fine-tune-train_segment_anything_2_in_60_lines_of_code/TRAIN.py at main ·…'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/TRAIN.py?source=post_page-----928dd29a63b3--------------------------------)
    [## fine-tune-train_segment_anything_2_in_60_lines_of_code/TRAIN.py at main ·…'
- en: The repository provides code for training/fine tune the Meta Segment Anything
    Model 2 (SAM 2) …
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 该代码库提供了训练/微调 Meta Segment Anything Model 2 (SAM 2) 的代码……
- en: github.com](https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/TRAIN.py?source=post_page-----928dd29a63b3--------------------------------)
    ![](../Images/1bd74d767271c35cc608b7b62e43607d.png)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/TRAIN.py?source=post_page-----928dd29a63b3--------------------------------)
    ![](../Images/1bd74d767271c35cc608b7b62e43607d.png)
- en: SAM2 net diagram taken from [SAM2 GIT page](https://github.com/facebookresearch/segment-anything-2)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: SAM2网络结构图，来自[SAM2 GIT页面](https://github.com/facebookresearch/segment-anything-2)
- en: '**How Segment Anything works**'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**Segment Anything的工作原理**'
- en: The main way SAM works is by taking an image and a point in the image and predicting
    the mask of the segment that contains the point. This approach enables full image
    segmentation without human intervention and with no limits on the classes or types
    of segments (as discussed in [a previous post](https://faun.pub/train-pointer-net-for-segmenting-objects-parts-and-materials-in-60-lines-of-code-ca328be8cef2)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: SAM的主要工作原理是通过获取图像和图像中的一个点，预测包含该点的分割区域的掩膜。该方法能够在没有人工干预的情况下进行完整的图像分割，并且不受类别或分割类型的限制（如在[上一篇文章](https://faun.pub/train-pointer-net-for-segmenting-objects-parts-and-materials-in-60-lines-of-code-ca328be8cef2)中所讨论的）。
- en: 'The procedure for using SAM for full image segmentation:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SAM进行完整图像分割的步骤：
- en: Select a set of points in the image
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在图像中选择一组点
- en: Use SAM to predict the segment containing each point
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用SAM预测包含每个点的分割区域
- en: Combine the resulting segments into a single map
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将生成的分割区域合并成一个单一的地图
- en: While SAM can also utilize other inputs like masks or bounding boxes, these
    are mainly relevant for interactive segmentation involving human input. For this
    tutorial, we’ll focus on fully automatic segmentation and will only consider single
    points input.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然SAM也可以利用其他输入，如掩膜或边界框，但这些主要适用于涉及人工输入的交互式分割。对于本教程，我们将专注于完全自动化的分割，并仅考虑单点输入。
- en: More details on the model are available on the [project website.](https://ai.meta.com/blog/segment-anything-2/)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 有关该模型的更多细节，请访问[项目网站](https://ai.meta.com/blog/segment-anything-2/)
- en: '**Downloading SAM2 and setting environment**'
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**下载SAM2并设置环境**'
- en: 'The SAM2 can be downloaded from:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: SAM2可以从以下链接下载：
- en: '[](https://github.com/facebookresearch/segment-anything-2?source=post_page-----928dd29a63b3--------------------------------)
    [## GitHub - facebookresearch/segment-anything-2: The repository provides code
    for running inference…'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/facebookresearch/segment-anything-2?source=post_page-----928dd29a63b3--------------------------------)
    [## GitHub - facebookresearch/segment-anything-2: 该仓库提供了用于运行推理的代码…'
- en: The repository provides code for running inference with the Meta Segment Anything
    Model 2 (SAM 2), links for…
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 该仓库提供了用于运行Meta Segment Anything Model 2（SAM 2）推理的代码，链接如下…
- en: github.com](https://github.com/facebookresearch/segment-anything-2?source=post_page-----928dd29a63b3--------------------------------)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/facebookresearch/segment-anything-2?source=post_page-----928dd29a63b3--------------------------------)
- en: If you don’t want to copy the training code, you can also download my forked
    version that already contains the TRAIN.py script.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不想复制训练代码，你也可以下载我已经包含TRAIN.py脚本的fork版本。
- en: '[](https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code?source=post_page-----928dd29a63b3--------------------------------)
    [## GitHub - sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code:
    The repository provides…'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code?source=post_page-----928dd29a63b3--------------------------------)
    [## GitHub - sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code:
    该仓库提供了…'
- en: The repository provides code for training/fine tune the Meta Segment Anything
    Model 2 (SAM 2) …
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 该仓库提供了用于训练/微调Meta Segment Anything Model 2（SAM 2）的代码…
- en: github.com](https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code?source=post_page-----928dd29a63b3--------------------------------)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code?source=post_page-----928dd29a63b3--------------------------------)
- en: Follow the installation instructions on the github repository.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 请按照GitHub仓库中的安装说明进行操作。
- en: In general, you need Python >=3.11 and [PyTorch.](https://pytorch.org/)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你需要Python >=3.11和[PyTorch](https://pytorch.org/)
- en: 'In addition, we will use OpenCV this can be installed using:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将使用OpenCV，可以通过以下命令进行安装：
- en: '*pip install opencv-python*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*pip install opencv-python*'
- en: Downloading pre-trained model
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载预训练模型
- en: 'You also need to download the pre-trained model from:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要从以下链接下载预训练模型：
- en: '[**https://github.com/facebookresearch/segment-anything-2?tab=readme-ov-file#download-checkpoints**](https://github.com/facebookresearch/segment-anything-2?tab=readme-ov-file#download-checkpoints)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[**https://github.com/facebookresearch/segment-anything-2?tab=readme-ov-file#download-checkpoints**](https://github.com/facebookresearch/segment-anything-2?tab=readme-ov-file#download-checkpoints)'
- en: There are several models you can choose from all compatible with this tutorial.
    I recommend using the [small model](https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_small.pt)
    which is the fastest to train.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个模型你可以选择，这些模型都与本教程兼容。我建议使用[小型模型](https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_small.pt)，它是训练速度最快的。
- en: Downloading training data
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载训练数据
- en: 'For this tutorial, we will use the [LabPics1 dataset](https://zenodo.org/records/3697452/files/LabPicsV1.zip?download=1)
    for segmenting materials and liquids. You can download the dataset from this URL:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本教程，我们将使用[LabPics1数据集](https://zenodo.org/records/3697452/files/LabPicsV1.zip?download=1)来分割材料和液体。你可以从这个网址下载数据集：
- en: '[https://zenodo.org/records/3697452/files/LabPicsV1.zip?download=1](https://zenodo.org/records/3697452/files/LabPicsV1.zip?download=1)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://zenodo.org/records/3697452/files/LabPicsV1.zip?download=1](https://zenodo.org/records/3697452/files/LabPicsV1.zip?download=1)'
- en: '**Preparing the data reader**'
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**准备数据读取器**'
- en: The first thing we need to write is the data reader. This will read and prepare
    the data for the net.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要编写的第一部分是数据读取器。它将读取并准备数据以供网络使用。
- en: 'The data reader needs to produce:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 数据读取器需要生成：
- en: An image
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一张图像
- en: Masks of all the segments in the image.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图像中所有分割区域的掩膜。
- en: And a [random point inside each mask](https://faun.pub/train-pointer-net-for-segmenting-objects-parts-and-materials-in-60-lines-of-code-ca328be8cef2)
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以及一个[每个掩膜内的随机点](https://faun.pub/train-pointer-net-for-segmenting-objects-parts-and-materials-in-60-lines-of-code-ca328be8cef2)
- en: 'Lets start by loading dependencies:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从加载依赖项开始：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next we list all the images in the dataset:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来列出数据集中所有的图像：
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now for the main function that will load the training batch. The training batch
    includes: One random image, all the segmentation masks belong to this image, and
    a random point in each mask:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是加载训练批次的主要函数。训练批次包括：一张随机图像，属于此图像的所有分割掩膜，以及每个掩膜中的一个随机点：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The first part of this function is choosing a random image and loading it:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数的第一部分是选择一张随机图像并加载它：
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that OpenCV reads images as BGR while SAM expects RGB images. By using
    *[…,::-1]* we change the image from BGR to RGB.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，OpenCV读取图像时是BGR格式，而SAM期望的是RGB格式。通过使用*[…,::-1]*，我们将图像从BGR转换为RGB。
- en: SAM expects the image size to not exceed 1024, so we are going to resize the
    image and the annotation map to this size.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: SAM期望图像大小不超过1024，因此我们将把图像和注释图调整为此大小。
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: An important point here is that when resizing the annotation map (*ann_map*)
    we use *INTER_NEAREST* mode (nearest neighbors). In the annotation map, each pixel
    value is the index of the segment it belongs to. As a result, it’s important to
    use resizing methods that do not introduce new values to the map.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这里一个重要的点是，在调整注释图(*ann_map*)大小时，我们使用*INTER_NEAREST*模式（最近邻）。在注释图中，每个像素值是其所属分割区域的索引。因此，使用不会引入新值的调整方法非常重要。
- en: The next block is specific to the format of the LabPics1 dataset. The annotation
    map (*ann_map*) contains a segmentation map for the vessels in the image in one
    channel, and another map for the materials annotation in a different channel.
    We going to merge them into a single map.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个代码块特定于LabPics1数据集的格式。注释图(*ann_map*)包含图像中血管的分割图（一个通道），另一个通道则用于材料注释图。我们将把它们合并为一张单一的图。
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'What this gives us is a a map (*mat_map*) in which the value of each pixel
    is the index of the segment to which it belongs (for example: all cells with value
    3 belong to segment 3). We want to transform this into a set of binary masks (0/1)
    where each mask corresponds to a different segment. In addition, from each mask,
    we want to extract a single point.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为我们提供一个地图(*mat_map*)，其中每个像素的值是它所属的分割区域的索引（例如：所有值为3的像素属于分割区域3）。我们希望将其转换为一组二进制掩膜（0/1），每个掩膜对应一个不同的分割区域。此外，我们还希望从每个掩膜中提取一个单一的点。
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We got the image (*Img*), a list of binary masks corresponding to segments in
    the image (*masks*), and for each mask the coordinate of a single point inside
    the mask (*points*).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了图像(*Img*)，一个与图像中各个分割区域对应的二进制掩膜列表(*masks*)，以及每个掩膜内部的一个点的坐标(*points*)。
- en: '![](../Images/29c072b8bda93b285bb87851fd68ae8f.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29c072b8bda93b285bb87851fd68ae8f.png)'
- en: 'Example for a batch of training data: 1) An Image. 2) List of segments masks.
    3) For each mask a single point inside the mask (marked red). Taken from the LabPics
    dataset.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一批训练数据的示例：1) 一张图像。2) 分割掩膜列表。3) 对于每个掩膜，掩膜内的一个单一点（标记为红色）。数据来自LabPics数据集。
- en: Loading the SAM model
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载SAM模型
- en: 'Now lets load the net:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们加载网络：
- en: '[PRE7]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'First, we set the path to the model weights in: *sam2_checkpoint* parameter.We
    downloaded the weights earlier from [here](https://github.com/facebookresearch/segment-anything-2?tab=readme-ov-file#download-checkpoints).
    **“sam2_hiera_small.pt”** refer to the [small model](https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_small.pt)
    but the code will work for any model. Whichever model you choose you need to set
    the corresponding config file in the *model_cfg* parameter.The config files are
    located in the sub folder***“*sam2_configs/”** of the main repository.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们设置模型权重的路径：*sam2_checkpoint*参数。我们之前从[这里](https://github.com/facebookresearch/segment-anything-2?tab=readme-ov-file#download-checkpoints)下载了权重。**“sam2_hiera_small.pt”**指的是[小模型](https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_small.pt)，但是代码对任何模型都有效。无论你选择哪个模型，都需要在*model_cfg*参数中设置相应的配置文件。这些配置文件位于主存储库中的子文件夹***“*sam2_configs/”**。
- en: Segment Anything General structure
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Segment Anything的总体结构
- en: Before we start training we need to understand the structure of the model.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始训练之前，我们需要了解模型的结构。
- en: 'SAM is composed of three parts:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: SAM由三个部分组成：
- en: 1) Image encoder, 2) Prompt encoder, 3) Mask decoder.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 1）图像编码器，2）提示编码器，3）掩码解码器。
- en: The image encoder is responsible for processing the image and creating the image
    embedding. This is the largest component and training it will demand strong GPU.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图像编码器负责处理图像并创建图像嵌入。这是最大的组件，训练它将需要强大的GPU。
- en: The prompt encoder processes input prompt, in our case the input point.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 提示编码器处理输入的提示，在我们的例子中就是输入的点。
- en: The mask decoder takes the output of the image encoder and prompt encoder and
    produces the final segmentation masks.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码解码器接受图像编码器和提示编码器的输出，并生成最终的分割掩码。
- en: 'Setting training parameters:'
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置训练参数：
- en: 'We can enable the training of the mask decoder and prompt encoder by setting:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过设置来启用掩码解码器和提示编码器的训练：
- en: '[PRE8]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You can enable training of the image encoder by using: “***predictor.model.image_encoder.train(True)”***'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过使用“***predictor.model.image_encoder.train(True)”***来启用图像编码器的训练。
- en: This will take stronger GPU but will give the net more room to improve. If you
    choose to train the image encoder, you must scan the SAM2 code for “***no_grad”***
    commands and remove them. (***no_grad*** blocks the gradient collection, which
    saves memory but prevents training).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要更强大的GPU，但会给网络提供更多的改进空间。如果你选择训练图像编码器，必须扫描SAM2代码中的“***no_grad”***命令并将其删除。（***no_grad***会阻止梯度收集，虽然可以节省内存，但会阻止训练）。
- en: 'Next, we define the standard adamW optimizer:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义标准的adamW优化器：
- en: '[PRE9]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We also going to use mixed precision training which is just a more memory-efficient
    training strategy:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用混合精度训练，这是一种更节省内存的训练策略：
- en: '[PRE10]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Main training loop**'
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**主要训练循环**'
- en: 'Now lets build the main training loop. The first part is reading and preparing
    the data:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们构建主要的训练循环。第一部分是读取和准备数据：
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'First we cast the data to mix precision for efficient training:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将数据转换为混合精度，以便高效训练：
- en: '[PRE12]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, we use the reader function we created earlier to read training data:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用之前创建的读取器函数来读取训练数据：
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We take the image we loaded and pass it through the image encoder (the first
    part of the net):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载的图像通过图像编码器（网络的第一部分）进行处理：
- en: '[PRE14]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we process the input points using the net prompt encoder:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们使用网络提示编码器处理输入点：
- en: '[PRE15]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note that in this part we can also input boxes or masks but we are not going
    to use these options.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这一部分，我们也可以输入框或掩码，但我们不会使用这些选项。
- en: 'Now that we encoded both the prompt (points) and the image we can finally predict
    the segmentation masks:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经编码了提示（点）和图像，我们可以最终预测分割掩码：
- en: '[PRE16]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The main part in this code is the***model.sam_mask_decoder***which runs the
    mask_decoder part of the net and generates the segmentation masks (*low_res_masks*)
    and their scores (*prd_scores*).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码的主要部分是***model.sam_mask_decoder***，它运行网络的mask_decoder部分并生成分割掩码（*low_res_masks*）及其分数（*prd_scores*）。
- en: These masks are in lower resolution than the original input image and are resized
    to the original input size in the ***postprocess_masks***function.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这些掩码的分辨率低于原始输入图像，并且会在***postprocess_masks***函数中被调整到原始输入大小。
- en: 'This gives us the final prediction of the net: 3 segmentation masks (*prd_masks*)
    for each input point we used and the masks scores (*prd_scores*). *prd_masks*
    contains 3 predicted masks for each input point but we only going to use the first
    mask for each point. *prd_scores* contains a score of how good the net thinks
    each mask is (or how sure it is in the prediction).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们网络的最终预测：每个输入点对应的 3 个分割掩码（*prd_masks*）以及掩码分数（*prd_scores*）。*prd_masks* 包含每个输入点的
    3 个预测掩码，但我们只会使用每个点的第一个掩码。*prd_scores* 包含网络认为每个掩码的质量（或预测的置信度）的分数。
- en: Loss functions
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失函数
- en: Segmentation loss
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分割损失
- en: Now we have the net predictions we can calculate the loss. First, we calculate
    the segmentation loss, which means how good the predicted mask is compared to
    the ground true mask. For this, we use the standard cross entropy loss.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了网络预测，可以计算损失。首先，我们计算分割损失，即预测掩码与地面真值掩码的匹配程度。为此，我们使用标准的交叉熵损失。
- en: 'First we need to convert prediction masks (*prd_mask*) from logits into probabilities
    using the sigmoid function:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要使用 sigmoid 函数将预测掩码（*prd_mask*）从 logits 转换为概率：
- en: '[PRE17]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next we convert the ground truth mask into a torch tensor:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将地面真值掩码转换为 PyTorch 张量：
- en: '[PRE18]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Finally, we calculate the cross entropy loss (*seg_loss*) manually using the
    ground truth (*gt_mask*) and predicted probability maps (*prd_mask*):'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用地面真值（*gt_mask*）和预测概率图（*prd_mask*）手动计算交叉熵损失（*seg_loss*）：
- en: '[PRE19]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: (we add 0.0001 to prevent the log function from exploding for zero values).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: （我们加上 0.0001 来防止对零值使用对数函数时出现爆炸）。
- en: Score loss (optional)
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分数损失（可选）
- en: 'In addition to the masks, the net also predicts the score for how good each
    predicted mask is. Training this part is less important but can be useful . To
    train this part we need to first know what is the true score of each predicted
    mask. Meaning, how good the predicted mask actually is. We are going to do it
    by comparing the GT mask and the corresponding predicted mask using intersection
    over union (IOU) metrics. IOU is simply the overlap between the two masks, divided
    by the combined area of the two masks. First, we calculate the intersection between
    the predicted and GT mask (the area in which they overlap):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 除了掩码，网络还预测每个预测掩码的分数，表示掩码的质量。训练这一部分的作用较小，但也有其用处。为了训练这一部分，我们首先需要知道每个预测掩码的真实分数。也就是说，我们需要知道预测掩码的质量。我们将通过使用交并比（IOU）度量，将
    GT 掩码与相应的预测掩码进行比较，来实现这一点。IOU 只是两个掩码的重叠部分，除以两个掩码的联合面积。首先，我们计算预测掩码和 GT 掩码之间的交集（它们重叠的区域）：
- en: '[PRE20]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We use threshold *(prd_mask > 0.5)* to turn the prediction mask from probability
    to binary mask.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用阈值 *(prd_mask > 0.5)* 将预测掩码从概率转化为二进制掩码。
- en: 'Next, we get the IOU by dividing the intersection by the combined area (union)
    of the predicted and gt masks:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过将交集除以预测掩码和 GT 掩码的联合面积（并集）来获得 IOU：
- en: '[PRE21]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We going to use the IOU as the true score for each mask, and get the score loss
    as the absolute difference between the predicted scores and the IOU we just calculated.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 IOU 作为每个掩码的真实分数，并通过计算预测分数与我们刚刚计算的 IOU 之间的绝对差来获得分数损失。
- en: '[PRE22]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, we merge the segmentation loss and score loss (giving much higher
    weight to the first):'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将分割损失和分数损失合并（对第一个赋予更高的权重）：
- en: '[PRE23]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Final step: Backpropogation and saving model'
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最后一步：反向传播和保存模型
- en: 'Once we get the loss everything is completely standard. We calculate backpropogation
    and update weights using the optimizer we made earlier:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们得到损失，后续步骤就完全标准了。我们使用之前创建的优化器计算反向传播并更新权重：
- en: '[PRE24]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We also want to save the trained model once every 1000 steps:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望每 1000 步保存一次训练的模型：
- en: '[PRE25]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Since we already calculated the IOU we can display it as a moving average to
    see how well the model prediction are improving over time:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经计算了 IOU，我们可以将其作为移动平均显示，以查看模型预测随时间的改进情况：
- en: '[PRE26]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: And that it, we have trained/ fine-tuned the Segment-Anything 2 in less than
    60 lines of code (not including comments and imports). After about 25,000 steps
    you should see major improvement .
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，我们在不到 60 行代码（不包括注释和导入）内完成了 Segment-Anything 2 的训练/微调。在约 25,000 步之后，您应该会看到显著的改进。
- en: The model will be saved to “model.torch”.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 模型将被保存为“model.torch”。
- en: 'You can find the full training code at:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下位置找到完整的训练代码：
- en: '[](https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/TRAIN.py?source=post_page-----928dd29a63b3--------------------------------)
    [## fine-tune-train_segment_anything_2_in_60_lines_of_code/TRAIN.py at main ·…'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/TRAIN.py?source=post_page-----928dd29a63b3--------------------------------)
    [## fine-tune-train_segment_anything_2_in_60_lines_of_code/TRAIN.py at main ·…'
- en: The repository provides code for training/fine tune the Meta Segment Anything
    Model 2 (SAM 2) …
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 该仓库提供了用于训练/微调Meta Segment Anything Model 2 (SAM 2)的代码……
- en: github.com](https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/TRAIN.py?source=post_page-----928dd29a63b3--------------------------------)
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/TRAIN.py?source=post_page-----928dd29a63b3--------------------------------)'
- en: 'This tutorial used a single image per batch, a more efficient way is to use
    several different images per batch, The code for doing this is available at:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程使用每批次一张图像，更高效的方法是每批次使用多张不同的图像，相关代码可以在以下位置找到：
- en: '[***https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/TRAIN_multi_image_batch.py***](https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/TRAIN_multi_image_batch.py)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[***https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/TRAIN_multi_image_batch.py***](https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/TRAIN_multi_image_batch.py)'
- en: 'Inference: Loading and using the trained model:'
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推理：加载并使用训练好的模型：
- en: Now that the model as been fine-tuned, let’s use it to segment an image.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型已经完成微调，我们来使用它对一张图像进行分割。
- en: 'We going to do this using the following steps:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过以下步骤来实现：
- en: Load the model we just trained.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载我们刚刚训练好的模型。
- en: Give the model an image and a bunch of random points. For each point the net
    will predict the segment mask that contain this point and a score.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给模型提供一张图像和一堆随机点。对于每个点，网络将预测包含该点的分割掩膜以及一个分数。
- en: Take these masks and stitch them together into one segmentation map.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些掩膜拼接成一个完整的分割图。
- en: 'The full code for doing that is available at:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码可以在以下位置找到：
- en: '[](https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/TEST_Net.py?source=post_page-----928dd29a63b3--------------------------------)
    [## fine-tune-train_segment_anything_2_in_60_lines_of_code/TEST_Net.py at main
    ·…'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/TEST_Net.py?source=post_page-----928dd29a63b3--------------------------------)
    [## fine-tune-train_segment_anything_2_in_60_lines_of_code/TEST_Net.py at main
    ·…'
- en: The repository provides code for training/fine tune the Meta Segment Anything
    Model 2 (SAM 2) …
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 该仓库提供了用于训练/微调Meta Segment Anything Model 2 (SAM 2)的代码……
- en: github.com](https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/TEST_Net.py?source=post_page-----928dd29a63b3--------------------------------)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/TEST_Net.py?source=post_page-----928dd29a63b3--------------------------------)'
- en: First, we load the dependencies and cast the weights to float16 this makes the
    model much faster to run (only possible for inference).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们加载依赖项并将权重转换为float16格式，这使得模型运行速度更快（仅在推理时可行）。
- en: '[PRE27]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, we load a sample [image](https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/sample_image.jpg)
    and a mask of the image region we want to segment (download [image](https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/sample_image.jpg)/[mask](https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/sample_mask.png)):'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们加载一个示例[图像](https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/sample_image.jpg)和我们想要分割的图像区域的掩膜（下载[图像](https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/sample_image.jpg)/[掩膜](https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/sample_mask.png)）：
- en: '[PRE28]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Sample 30 random points inside the region we want to segment:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们想要分割的区域内随机采样30个点：
- en: '[PRE29]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Load the standard SAM model (same as in training)
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 加载标准的SAM模型（与训练时相同）
- en: '[PRE30]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Next, Load the weights of the model we just trained (model.torch):'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，加载我们刚刚训练的模型的权重（model.torch）：
- en: '[PRE31]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Run the fine-tuned model to predict a segmentation mask for every point we
    selected earlier:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 运行微调后的模型来预测我们之前选择的每个点的分割掩膜：
- en: '[PRE32]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Now we have a list of predicted masks and their scores. We want to somehow stitch
    them into a single consistent segmentation map. However, many of the masks overlap
    and might be inconsistent with each other.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们得到了一个预测的掩膜及其分数的列表。我们希望将它们以某种方式拼接成一个一致的分割图。然而，许多掩膜是重叠的，可能彼此不一致。
- en: 'The approach to stitching is simple:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 拼接方法很简单：
- en: 'First we will sort the predicted masks according to their predicted scores:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将根据预测分数对预测的掩模进行排序：
- en: '[PRE33]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now lets create an empty segmentation map and occupancy map:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建一个空的分割图和占用图：
- en: '[PRE34]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Next, we add the masks one by one (from high to low score) to the segmentation
    map. We only add a mask if it’s consistent with the masks that were previously
    added, which means only if the mask we want to add has less than 15% overlap with
    already occupied areas.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将按顺序（从高到低分数）将掩模添加到分割图中。只有当掩模与之前添加的掩模一致时，我们才会添加它，这意味着只有当我们想要添加的掩模与已占用区域的重叠部分小于15%时，才会添加。
- en: '[PRE35]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: And this is it.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。
- en: '*seg_mask* now contains the predicted segmentation map with different values
    for each segment and 0 for the background.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '*seg_mask*现在包含了预测的分割图，每个分割区域有不同的值，背景为0。'
- en: 'We can turn this into a color map using:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下方法将其转换为颜色图：
- en: '[PRE36]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'And display:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 并显示：
- en: '[PRE37]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![](../Images/9e5af0c935665898307a0372f325bb5b.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e5af0c935665898307a0372f325bb5b.png)'
- en: Example for segmentation results using fine-tuned SAM2\. Image from the LabPics
    dataset.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 使用微调后的SAM2进行分割结果的示例。图像来自LabPics数据集。
- en: 'The full inference code is available at:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的推理代码可以在以下位置找到：
- en: '[](https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/TEST_Net.py?source=post_page-----928dd29a63b3--------------------------------)
    [## fine-tune-train_segment_anything_2_in_60_lines_of_code/TEST_Net.py at main
    ·…'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/TEST_Net.py?source=post_page-----928dd29a63b3--------------------------------)
    [## fine-tune-train_segment_anything_2_in_60_lines_of_code/TEST_Net.py at main
    ·…'
- en: The repository provides code for training/fine tune the Meta Segment Anything
    Model 2 (SAM 2) …
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 该代码库提供了用于训练/微调Meta Segment Anything Model 2（SAM 2）的代码……
- en: github.com](https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/TEST_Net.py?source=post_page-----928dd29a63b3--------------------------------)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/TEST_Net.py?source=post_page-----928dd29a63b3--------------------------------)
- en: 'Conclusion:'
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论：
- en: That’s it, we have trained and tested SAM2 on a custom dataset. Other than changing
    the data-reader, this should work for any dataset. In many cases, this should
    be enough to give a significant improvement in performance.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样，我们已经在自定义数据集上训练和测试了SAM2。除了更改数据读取器外，这应该适用于任何数据集。在许多情况下，这应该足以显著提高性能。
- en: Finally, SAM2 can also segment and track objects in videos, but fine-tuning
    this part is for another time.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，SAM2还可以在视频中对物体进行分割和跟踪，但微调这一部分内容将在另一个时刻讨论。
- en: '**Copyright:** All images for the post are taken from the [SAM2 GIT](https://github.com/facebookresearch/segment-anything-2)
    repository (under Apache license), and [LabPics](https://zenodo.org/records/3697452)
    dataset (under MIT license). This tutorial code and nets are available under the
    Apache license.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**版权声明：** 本文中的所有图像均来自[SAM2 GIT](https://github.com/facebookresearch/segment-anything-2)代码库（采用Apache许可证），以及[LabPics](https://zenodo.org/records/3697452)数据集（采用MIT许可证）。本教程的代码和网络模型可在Apache许可证下使用。'
