- en: How Google Used Your Data to Improve their Music AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谷歌如何利用你的数据来改进他们的音乐AI
- en: 原文：[https://towardsdatascience.com/how-google-used-your-data-to-improve-their-music-ai-8948a1e85491?source=collection_archive---------10-----------------------#2024-02-28](https://towardsdatascience.com/how-google-used-your-data-to-improve-their-music-ai-8948a1e85491?source=collection_archive---------10-----------------------#2024-02-28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-google-used-your-data-to-improve-their-music-ai-8948a1e85491?source=collection_archive---------10-----------------------#2024-02-28](https://towardsdatascience.com/how-google-used-your-data-to-improve-their-music-ai-8948a1e85491?source=collection_archive---------10-----------------------#2024-02-28)
- en: MusicLM fine-tuned on user preferences
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MusicLM根据用户偏好进行了微调
- en: '[](https://medium.com/@maxhilsdorf?source=post_page---byline--8948a1e85491--------------------------------)[![Max
    Hilsdorf](../Images/01da76c553e43d5ed6b6849bdbfd00da.png)](https://medium.com/@maxhilsdorf?source=post_page---byline--8948a1e85491--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8948a1e85491--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8948a1e85491--------------------------------)
    [Max Hilsdorf](https://medium.com/@maxhilsdorf?source=post_page---byline--8948a1e85491--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@maxhilsdorf?source=post_page---byline--8948a1e85491--------------------------------)[![Max
    Hilsdorf](../Images/01da76c553e43d5ed6b6849bdbfd00da.png)](https://medium.com/@maxhilsdorf?source=post_page---byline--8948a1e85491--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8948a1e85491--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8948a1e85491--------------------------------)
    [Max Hilsdorf](https://medium.com/@maxhilsdorf?source=post_page---byline--8948a1e85491--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8948a1e85491--------------------------------)
    ·7 min read·Feb 28, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8948a1e85491--------------------------------)
    ·阅读时长7分钟·2024年2月28日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/9180fef19f159a138075f11e75b4085a.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9180fef19f159a138075f11e75b4085a.png)'
- en: Photo by [Firmbee.com](https://unsplash.com/@firmbee?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [Firmbee.com](https://unsplash.com/@firmbee?utm_source=medium&utm_medium=referral)
    由 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) 提供
- en: What is MusicLM?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是MusicLM？
- en: 'MusicLM, Google’s flagship text-to-music AI, was originally published in early
    2023\. Even in its basic version, it represented a major breakthrough and caught
    the music industry by surprise. However, a few weeks ago, MusicLM received a **significant
    update**. Here’s a side-by-side comparison for two selected prompts:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: MusicLM是谷歌的旗舰文本到音乐的AI，最初于2023年初发布。即使在其基础版本中，它也代表了一个重大的突破，并让音乐行业感到震惊。然而，几周前，MusicLM进行了**重大更新**。以下是两个选定提示的并排比较：
- en: '**Prompt: “Dance music with a melodic synth line and arpeggiation”:**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示：“带有旋律合成线和琶音的舞曲”**：'
- en: '[**Old MusicLM**](https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musiclm-7.wav):
    [https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musiclm-7.wav](https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musiclm-7.wav)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**旧版MusicLM**](https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musiclm-7.wav):
    [https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musiclm-7.wav](https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musiclm-7.wav)'
- en: '[**New MusicLM**](https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musicrlhf-ru-7.wav):
    [https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musicrlhf-ru-7.wav](https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musicrlhf-ru-7.wav)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**新版MusicLM**](https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musicrlhf-ru-7.wav):
    [https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musicrlhf-ru-7.wav](https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musicrlhf-ru-7.wav)'
- en: '**Prompt: “a nostalgic tune played by accordion band”**'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示：“由手风琴乐队演奏的怀旧旋律”**'
- en: '[**Old MusicLM**](https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musiclm-27.wav):
    [https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musiclm-27.wav](https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musiclm-27.wav)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**旧版MusicLM**](https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musiclm-27.wav):
    [https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musiclm-27.wav](https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musiclm-27.wav)'
- en: '[**New MusicLM**](https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musicrlhf-ru-27.wav):
    [https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musicrlhf-ru-27.wav](https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musicrlhf-ru-27.wav)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**新MusicLM**](https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musicrlhf-ru-27.wav)：[https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musicrlhf-ru-27.wav](https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musicrlhf-ru-27.wav)'
- en: 'This increase in quality can be attributed to a new paper by Google Research
    titled: “MusicRL: Aligning Music Generation to Human Preferenc\es”. Apparently,
    this upgrade was considered so significant that they decided to rename the model.
    However, under the hood, MusicRL is identical to MusicLM in its key architecture.
    The only difference: **Finetuning**.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '质量的提高可以归因于谷歌研究发布的一篇新论文，标题为：“MusicRL: 将音乐生成与人类偏好对齐”。显然，这次升级被认为非常重要，以至于他们决定重新命名该模型。然而，在底层，MusicRL在关键架构上与MusicLM是完全相同的。唯一的区别是：**微调**。'
- en: What is Finetuning?
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是微调？
- en: When building an AI model from scratch, it starts with zero knowledge and essentially
    does random guessing. The model then extracts useful patterns through training
    on data and starts displaying increasingly intelligent behavior as training progresses.
    One downside to this approach is that **training from scratch requires a lot of
    data**. Finetuning is the idea that an existing model is used and adapted to a
    new task, or adapted to approach the same task differently. Because the model
    already has learned the most important patterns, **much less data is required**.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从零开始构建AI模型时，它的知识是从零开始的，基本上是在进行随机猜测。然后，模型通过在数据上训练提取有用的模式，并随着训练的进行，开始表现出越来越智能的行为。这种方法的一个缺点是，**从零开始训练需要大量数据**。微调的理念是使用一个现有的模型，并将其调整到新任务上，或者调整它以不同的方式处理相同的任务。因为模型已经学习了最重要的模式，**所需的数据量要少得多**。
- en: For example, a powerful open-source LLM like Mistral7B can be trained from scratch
    by anyone, in principle. However, the amount of data required to produce even
    remotely useful outputs is gigantic. Instead, companies use the existing Mistral7B
    model and feed it a small amount of proprietary data to make it solve new tasks,
    whether that is writing SQL queries or classifying emails.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个像Mistral7B这样的强大开源LLM，原则上任何人都可以从零开始训练。然而，要产生即使是稍微有用的输出所需的数据量是巨大的。相反，公司使用现有的Mistral7B模型，并为其提供少量专有数据，以使其能够解决新的任务，无论是编写SQL查询还是分类电子邮件。
- en: The **key takeawa**y is that finetuning does not change the fundamental structure
    of the model. It only adapts its internal logic slightly to perform better on
    a specific task. Now, let’s use this knowledge to understand how Google finetuned
    MusicLM on user data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键 takeaway**是微调不会改变模型的基本结构。它只是稍微调整其内部逻辑，以便在特定任务上表现得更好。现在，让我们利用这些知识来了解谷歌是如何在用户数据上微调MusicLM的。'
- en: How Google Gathered User Data
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谷歌是如何收集用户数据的
- en: 'A few months after the MusicLM paper, a public demo was released as part of
    Google’s AI Test Kitchen. There, users could experiment with the text-to-music
    model for free. However, you might know the saying: **If the product is free,
    YOU are the product**. Unsurprisingly, Google is no exception to this rule. When
    using MusicLM’s public demo, you were occasionally confronted with two generated
    outputs and asked to state which one you prefer. Through this method, Google was
    able to gather **300,000 user preferences** within a couple of months.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在MusicLM论文发布几个月后，作为谷歌AI测试厨房的一部分，发布了一个公共演示。在那里，用户可以免费试验文本到音乐的模型。然而，你可能听过这样一句话：**如果产品是免费的，那么你就是产品**。毫不奇怪，谷歌也不例外。在使用MusicLM的公共演示时，用户偶尔会遇到两个生成的输出，并被要求表明自己更喜欢哪一个。通过这种方式，谷歌能够在几个月内收集到**300,000个用户偏好**。
- en: '![](../Images/64266a0a7a328012b4c1dbecbdaae7d6.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/64266a0a7a328012b4c1dbecbdaae7d6.png)'
- en: Example of the user preference ratings captured in the MusicLM public playground.
    Image taken from the [MusicRL paper](https://arxiv.org/pdf/2402.04229.pdf).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是用户偏好评级的示例，这些数据在MusicLM公共平台中收集。图片摘自[MusicRL论文](https://arxiv.org/pdf/2402.04229.pdf)。
- en: As you can see from the screenshot, users were **not explicitly informed** that
    their preferences would be used for machine learning. While that may feel unfair,
    it is important to note that many of our actions in the internet are being used
    for ML training, whether it is our Google search history, our Instagram likes,
    or our private Spotify playlists. In comparison to these rather personal and sensitive
    cases, music preferences on the MusicLM playground seem negligible.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从截图中可以看到，用户并没有**明确被告知**他们的偏好将被用于机器学习。虽然这可能感觉不公平，但需要注意的是，我们在互联网上的许多行为都被用来进行机器学习训练，无论是我们的谷歌搜索历史、Instagram的点赞，还是我们的私人Spotify播放列表。与这些个人且敏感的情况相比，MusicLM平台上的音乐偏好似乎微不足道。
- en: Example of User Data Collection on Linkedin Collaborative Articles
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Linkedin协作文章中的用户数据收集示例
- en: It is good to be aware that user data collection for machine learning is happening
    all the time and usually without explicit consent. If you are on Linkedin, you
    might have been invited to contribute to so-called “collaborative articles”. Essentially,
    users are invited to provide tips on questions in their domain of expertise. Here
    is an example of a [collaborative article on how to write a successful folk song](https://www.linkedin.com/advice/3/how-can-you-write-successful-folk-songs-skills-music-industry-w4i5e?trk=cah1)
    (something I didn’t know I needed).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，机器学习中的用户数据收集是随时发生的，通常是在没有明确同意的情况下。如果你在Linkedin上，你可能会被邀请参与所谓的“协作文章”。本质上，用户被邀请为自己擅长领域的问题提供建议。以下是一个关于[如何写一首成功的民谣歌曲的协作文章](https://www.linkedin.com/advice/3/how-can-you-write-successful-folk-songs-skills-music-industry-w4i5e?trk=cah1)（这是我以前从未知道我需要的）。
- en: '![](../Images/88b7b5725451f53f3869162771126314.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/88b7b5725451f53f3869162771126314.png)'
- en: Header of a [collaborative article](https://www.linkedin.com/advice/3/how-can-you-write-successful-folk-songs-skills-music-industry-w4i5e?trk=cah1)
    on songwriting. On the right side, I am asked to contribute to earn a “Top Voice”
    badge.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一篇关于[写歌的协作文章](https://www.linkedin.com/advice/3/how-can-you-write-successful-folk-songs-skills-music-industry-w4i5e?trk=cah1)的标题。在右侧，我被要求参与贡献以赚取“顶尖声音”徽章。
- en: Users are incentivized to contribute, earning them a “Top Voice” badge on the
    platform. However, my impression is that **noone actually reads these articles**.
    This leads me to believe that these thousands of question-answer pairs are being
    used by Microsoft (owner of Linkedin) to **train an expert AI system** on these
    data. If my suspicion is accurate, I would find this example much more problematic
    than Google asking users for their favorite track.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 用户们被激励参与其中，获得平台上的“顶尖声音”徽章。然而，我的印象是**其实没有人真正阅读这些文章**。这让我相信，这些成千上万的问答对正被微软（Linkedin的拥有者）用来**训练一个专家级的人工智能系统**。如果我的猜测准确，我会觉得这个例子比谷歌让用户提供他们最喜欢的曲目要更有问题。
- en: '**But back to MusicLM!**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**但让我们回到MusicLM！**'
- en: How Google Took Advantage of this User Data
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Google是如何利用这些用户数据的
- en: The next question is how Google was able to use this massive collection of user
    preferences to finetune MusicLM. The secret lies in a technique called **Reinforcement
    Learning from Human Feedback (RLHF)** which was one of the key breakthroughs of
    ChatGPT back in 2022\. In RLHF, human preferences are used to train an AI model
    that learns to imitate human preference decisions, resulting in an artificial
    human rater. Once this so-called **reward model** is trained, it can take in any
    two tracks and predict which one would most likely be preferred by human raters.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个问题是，谷歌是如何利用这个庞大的用户偏好数据集来微调MusicLM的。秘诀在于一种叫做**人类反馈强化学习（RLHF）**的技术，这是2022年ChatGPT的一个关键突破。在RLHF中，利用人类的偏好来训练一个人工智能模型，该模型学习模仿人类的偏好决策，从而生成一个人工人类评分员。一旦这个所谓的**奖励模型**被训练完成，它就可以接收任意两首曲目，并预测哪一首更有可能受到人类评分员的喜爱。
- en: With the reward model set up, MusicLM could be finetuned to maximize the predicted
    user preference of its outputs. This means that the text-to-music model generated
    thousands of tracks, each track receiving a rating from the reward model. Through
    the iterative adaptation of the model weights, MusicLM learned to generate music
    that the artificial human rater “likes”.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置好奖励模型后，MusicLM可以被微调，以最大化其输出的用户偏好预测。这意味着，文本到音乐的模型生成了成千上万的曲目，每首曲目都获得了奖励模型的评分。通过对模型权重的反复调整，MusicLM学会了生成人工评分员“喜欢”的音乐。
- en: '![](../Images/ed2b47a614f28e51438c1ed647417056.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed2b47a614f28e51438c1ed647417056.png)'
- en: RLHF explained. Image taken from [MusicRL](https://arxiv.org/abs/2402.04229)
    paper.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF 解释。图片来源于[MusicRL](https://arxiv.org/abs/2402.04229)论文。
- en: 'In addition to the finetuning on user preferences, MusicLM was also finetuned
    concerning two other criteria:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在用户偏好上的微调外，MusicLM还在另外两个标准上进行了微调：
- en: '**1\. Prompt Adherence**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**1. 提示符合度**'
- en: '[MuLan](https://research.google/pubs/mulan-a-joint-embedding-of-music-audio-and-natural-language/),
    Google’s proprietary text-to-audio embedding model was used to calculate the similarity
    between the user prompt and the generated audio. During finetuning, this adherence
    score was maximized.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[MuLan](https://research.google/pubs/mulan-a-joint-embedding-of-music-audio-and-natural-language/)，Google的专有文本到音频嵌入模型，用于计算用户提示与生成音频之间的相似度。在微调过程中，这一符合度得分被最大化。'
- en: '**2\. Audio Quality**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**2. 音频质量**'
- en: Google trained another reward model on user data to evaluate the subjective
    audio quality of its generated outputs. These user data seem to have been collected
    in separate surveys, not in MusicLM’s public demo.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Google还基于用户数据训练了另一个奖励模型，用于评估其生成输出的主观音频质量。这些用户数据似乎是在不同的调查中收集的，而不是MusicLM的公共演示中的数据。
- en: How Much Better is the New MusicLM?
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 新版MusicLM到底好多少？
- en: The new, finetuned model seems to **reliably outperform the old MusicLM**, listen
    to the samples provided on the [demo page](https://google-research.github.io/seanet/musiclm/rlhf/).
    Of course, a selected public demo can be deceiving, as the authors are incentivized
    to showcase examples that make their new model look as good as possible. Hopefully,
    we will get to test out MusicRL in a public playground, soon.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 新的、经过微调的模型似乎**在可靠性上超越了旧版MusicLM**，可以在[演示页面](https://google-research.github.io/seanet/musiclm/rlhf/)上试听提供的样本。当然，精选的公共演示可能会让人产生误导，因为作者们有动力展示那些能够使新模型看起来尽可能优秀的例子。希望我们很快能在公共平台上测试MusicRL。
- en: However, the paper also provides a **quantitative assessment** of subjective
    quality. For this, Google conducted a study and asked users to compare two tracks
    generated for the same prompt, giving each track a score from 1 to 5\. Using this
    metric with the fancy-sounding name Mean Opinion Score (MOS), we can compare not
    only the number of direct comparison wins for each model, but also calculate the
    average rater score (MOS).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，论文也提供了**主观质量的定量评估**。为此，Google进行了一项研究，要求用户比较为同一提示生成的两首曲目，并为每首曲目打分，分数范围为1到5。使用这种名为平均意见得分（MOS）的指标，我们不仅可以比较每个模型在直接比较中的获胜次数，还可以计算平均评分（MOS）。
- en: '![](../Images/645237ebf9e0b8db1060a1b315579d6e.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/645237ebf9e0b8db1060a1b315579d6e.png)'
- en: Quantitative benchmarks. Image taken from [MusicRL](https://arxiv.org/abs/2402.04229)
    paper.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 定量基准。图像来源于[MusicRL](https://arxiv.org/abs/2402.04229)论文。
- en: Here, MusicLM represents the original MusicLM model. MusicRL-R was only finetuned
    for audio quality and prompt adherence. MusicRL-U was finetuned solely on human
    feedback (the reward model). Finally, MusicRL-RU was finetuned on all three objectives.
    Unsurprisingly, **MusicRL-RU beats all other models** in direct comparison as
    well as on the average ratings.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，MusicLM代表原始的MusicLM模型。MusicRL-R仅在音频质量和提示符合度上进行了微调。MusicRL-U仅在人工反馈（奖励模型）上进行了微调。最后，MusicRL-RU在三个目标上都进行了微调。毫不奇怪，**MusicRL-RU在直接比较中以及平均评分上都超越了所有其他模型**。
- en: The paper also reports that MusicRL-RU, the fully finetuned model, beat MusicLM
    in 87% of direct comparisons. The importance of RLHF can be shown by analyzing
    the direct comparisons between MusicRL-R and MusicRL-RU. Here, the latter had
    a 66% win rate, reliably outperforming its competitor.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 论文还报告称，完全微调的模型MusicRL-RU在87%的直接比较中胜过了MusicLM。通过分析MusicRL-R和MusicRL-RU之间的直接比较，可以显示RLHF的重要性。在这里，后者的胜率为66%，可靠地超越了其竞争对手。
- en: What are the Implications of This?
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这意味着什么？
- en: Although the difference in output quality is noticeable, qualitatively as well
    as quantitatively, the new MusicLM is **still quite far from human-level outputs**
    in most cases. Even on the public demo page, many generated outputs sound odd,
    rhythmically, fail to capture key elements of the prompt or suffer from unnatural-sounding
    instruments.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管输出质量的差异是显著的，无论在定性还是定量上，新版MusicLM在大多数情况下**仍然远未达到人类水平的输出**。即使是在公共演示页面上，许多生成的输出也听起来很奇怪，节奏不对，未能捕捉到提示的关键元素，或是音色不自然。
- en: In my opinion, this paper is still significant, as it is the **first attempt
    at using RLHF for music generation**. RLHF has been used extensively in text generation
    for more than one year. But why has this taken so long? I suspect that collecting
    user feedback and finetuning the model is quite costly. Google likely released
    the public MusicLM demo with the primary intention of collecting user feedback.
    This was a smart move and gave them an edge over Meta, which has equally capable
    models, but no open platform to collect user data on.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在我看来，这篇论文仍然具有重要意义，因为它是**首次尝试使用RLHF进行音乐生成**。RLHF在文本生成中已经广泛应用超过一年。但为什么这花了这么长时间？我怀疑收集用户反馈并进行模型微调是相当昂贵的。Google可能发布了公开的MusicLM演示，主要目的是收集用户反馈。这是一个聪明的举动，让他们在竞争中领先于Meta，后者也有同样强大的模型，但没有公开平台来收集用户数据。
- en: All in all, Google has pushed itself ahead of the competition by leveraging
    proven finetuning methods borrowed from ChatGPT. While even with RLHF, the new
    MusicLM has still not reached human-level quality, Google can now maintain and
    update its reward model, **improving future generations of text-to-music models**
    with the same finetuning procedure.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，Google通过借鉴ChatGPT的经过验证的微调方法，成功地将自己推到了竞争对手的前面。尽管即便采用了RLHF，新的MusicLM仍未达到人类水平的质量，但Google现在能够保持并更新其奖励模型，**通过相同的微调程序改进未来的文本到音乐模型**。
- en: It will be interesting to see if and when other competitors like Meta or Stability
    AI will be catching up. For us as users, all of this is just **great news**! We
    get free public demos and more capable models.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 看看其他竞争者，如Meta或Stability AI，是否会赶超，什么时候赶超，应该会很有趣。对我们这些用户来说，这一切都是**好消息**！我们可以获得免费的公共演示和更强大的模型。
- en: For musicians, the pace of the current developments may feel a little threatening
    — and for good reason. I expect to see **human-level text-to-music generation
    in the next 1–3 years**. By that, I mean text-to-music AI that is at least as
    capable at producing music as ChatGPT was at writing texts when it was released.
    Musicians must learn about AI and how it can already support them in their everyday
    work. As the music industry is being disrupted once again, curiosity and flexibility
    will be the primary key to success.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于音乐人来说，当前发展的步伐可能有些让人感到威胁——而且这是有充分理由的。我预计在未来1到3年内，会看到**人类水平的文本到音乐生成**。我的意思是，文本到音乐的AI至少能像ChatGPT发布时那样，具备生成音乐的能力。音乐人必须了解AI及其如何在日常工作中已经能够支持他们。随着音乐行业再次遭到颠覆，好奇心和灵活性将是成功的关键。
- en: Interested in Music AI?
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对音乐AI感兴趣吗？
- en: 'If you liked this article, you might want to check out some of my other work:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这篇文章，你可能会想看看我的其他作品：
- en: '[“3 Music AI Breakthroughs to Expect in 2024”](https://medium.com/towards-data-science/3-music-ai-breakthroughs-to-expect-in-2024-2d945ae6b5fd).
    Medium Blog'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“2024年期待的3项音乐AI突破”](https://medium.com/towards-data-science/3-music-ai-breakthroughs-to-expect-in-2024-2d945ae6b5fd)。Medium
    博客'
- en: '[“Where is Generative AI Music Now?”](https://www.youtube.com/watch?v=OLJi1b-B0i0).
    YouTube Interview on Sync My Music'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“生成性AI音乐现状如何？”](https://www.youtube.com/watch?v=OLJi1b-B0i0)。关于Sync My Music的YouTube访谈'
- en: '[“MusicLM — Has Google Solved AI Music Generation?”](https://medium.com/towards-data-science/musiclm-has-google-solved-ai-music-generation-c6859e76bc3c)
    Medium Blog Post'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“MusicLM — Google解决了AI音乐生成问题了吗？”](https://medium.com/towards-data-science/musiclm-has-google-solved-ai-music-generation-c6859e76bc3c)
    Medium 博客文章'
- en: You can also follow me on [Linkedin](https://www.linkedin.com/in/max-hilsdorf/)
    to stay updated about new papers and trends in Music AI.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在[Linkedin](https://www.linkedin.com/in/max-hilsdorf/)上关注我，随时了解音乐AI的新论文和趋势。
- en: '**Thanks for reading this article!**'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**感谢阅读本文！**'
- en: References
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Agostinelli et al., 2023\. MusicLM: Generating Music From Text. [https://arxiv.org/abs/2301.11325](https://arxiv.org/abs/2301.11325)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Agostinelli 等人，2023年。MusicLM：从文本生成音乐。[https://arxiv.org/abs/2301.11325](https://arxiv.org/abs/2301.11325)
- en: 'Cideron et al., 2024\. MusicRL: Aligning Music Generation to Human Preferences.
    [https://arxiv.org/abs/2402.04229](https://arxiv.org/abs/2402.04229)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Cideron 等人，2024年。MusicRL：将音乐生成与人类偏好对齐。[https://arxiv.org/abs/2402.04229](https://arxiv.org/abs/2402.04229)
