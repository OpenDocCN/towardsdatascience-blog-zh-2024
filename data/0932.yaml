- en: Write-Audit-Publish for Data Lakes in Pure Python (no JVM)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用纯Python（无JVM）实现的数据湖写入-审计-发布
- en: 原文：[https://towardsdatascience.com/write-audit-publish-for-data-lakes-in-pure-python-no-jvm-25fbd971b17d?source=collection_archive---------4-----------------------#2024-04-12](https://towardsdatascience.com/write-audit-publish-for-data-lakes-in-pure-python-no-jvm-25fbd971b17d?source=collection_archive---------4-----------------------#2024-04-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/write-audit-publish-for-data-lakes-in-pure-python-no-jvm-25fbd971b17d?source=collection_archive---------4-----------------------#2024-04-12](https://towardsdatascience.com/write-audit-publish-for-data-lakes-in-pure-python-no-jvm-25fbd971b17d?source=collection_archive---------4-----------------------#2024-04-12)
- en: An open source implementation of WAP using Apache Iceberg, Lambdas, and Project
    Nessie all running entirely Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Apache Iceberg、Lambdas和Project Nessie的开源WAP实现，全部通过Python运行
- en: '[](https://medium.com/@ciro.greco?source=post_page---byline--25fbd971b17d--------------------------------)[![Ciro
    Greco](../Images/4a20e5d435998e8d8ff7aeac1f8ff60d.png)](https://medium.com/@ciro.greco?source=post_page---byline--25fbd971b17d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--25fbd971b17d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--25fbd971b17d--------------------------------)
    [Ciro Greco](https://medium.com/@ciro.greco?source=post_page---byline--25fbd971b17d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ciro.greco?source=post_page---byline--25fbd971b17d--------------------------------)[![Ciro
    Greco](../Images/4a20e5d435998e8d8ff7aeac1f8ff60d.png)](https://medium.com/@ciro.greco?source=post_page---byline--25fbd971b17d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--25fbd971b17d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--25fbd971b17d--------------------------------)
    [Ciro Greco](https://medium.com/@ciro.greco?source=post_page---byline--25fbd971b17d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--25fbd971b17d--------------------------------)
    ·9 min read·Apr 12, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--25fbd971b17d--------------------------------)
    ·阅读时间9分钟·2024年4月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/3f6c7fd882cb9f69244db1e5fe0bc7b6.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f6c7fd882cb9f69244db1e5fe0bc7b6.png)'
- en: 'Look Ma: no JVM! Photo by [Zac Ong](https://unsplash.com/@zacong?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 看，妈妈：没有JVM！图片来自[Zac Ong](https://unsplash.com/@zacong?utm_source=medium&utm_medium=referral)在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: In this blog post we provide a no-nonsense, reference implementation for Write-Audit-Publish
    (WAP) patterns on a data lake, using [Apache Iceberg](https://iceberg.apache.org/)
    as an open table format, and [Project Nessie](https://projectnessie.org/) as a
    data catalog supporting git-like semantics.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我们提供了一个简明的参考实现，用于在数据湖上实施写入-审计-发布（WAP）模式，使用[Apache Iceberg](https://iceberg.apache.org/)作为开放表格格式，并使用[Project
    Nessie](https://projectnessie.org/)作为支持类似git语义的数据目录。
- en: We chose [Nessie](https://projectnessie.org/) because its branching capabilities
    provide a good abstraction to implement a WAP design. Most importantly, we chose
    to build on [PyIceberg](https://py.iceberg.apache.org/) to eliminate the need
    for the JVM in terms of developer experience. In fact, to run the entire project,
    including the integrated applications we will only need Python and AWS.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择[Nessie](https://projectnessie.org/)是因为它的分支功能提供了一个良好的抽象，能够实现WAP设计。最重要的是，我们选择在[PyIceberg](https://py.iceberg.apache.org/)上构建，以消除开发者体验中对JVM的需求。事实上，要运行整个项目，包括集成的应用程序，我们只需要Python和AWS。
- en: While [Nessie](https://projectnessie.org/) is technically built in Java, the
    data catalog is run as a container by [AWS Lightsail](https://aws.amazon.com/lightsail/)
    in this project, we are going to interact with it only through its endpoint. Consequently,
    we can express the entire WAP logic, including the queries downstream, in Python
    only!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然[Nessie](https://projectnessie.org/)在技术上是用Java构建的，但在本项目中，数据目录通过[AWS Lightsail](https://aws.amazon.com/lightsail/)作为容器运行，我们将仅通过其端点与之交互。因此，我们可以仅使用Python表达整个WAP逻辑，包括下游查询！
- en: Because [PyIceberg](https://py.iceberg.apache.org/) is fairly new, a bunch of
    things are actually not supported out of the box. In particular, writing is still
    in early days, and branching Iceberg tables is still not supported. So what you’ll
    find here is the result of some original work we did ourselves to make branching
    Iceberg tables in [Nessie](https://projectnessie.org/) possible directly from
    Python.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于[PyIceberg](https://py.iceberg.apache.org/)相对较新，实际上很多功能并不直接支持。特别是，写入功能仍处于初期阶段，Iceberg表的分支仍不被支持。因此，您在这里看到的内容是我们自己进行的一些原创工作，以便直接从Python中使[Nessie](https://projectnessie.org/)支持Iceberg表的分支。
- en: So all this happened, more or less.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这一切或多或少地发生了。
- en: What on earth is WAP?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: WAP到底是什么？
- en: Back in 2017, Michelle Winters from Netflix [talked](https://www.youtube.com/watch?v=fXHdeBnpXrg)
    about a design pattern called Write-Audit-Publish (WAP) in data. Essentially,
    WAP is a functional design aimed at making data quality checks easy to implement
    **before** the data become available to downstream consumers.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年，Netflix的Michelle Winters在[talked](https://www.youtube.com/watch?v=fXHdeBnpXrg)中谈到了一个名为“写入-审计-发布”（Write-Audit-Publish，WAP）的数据设计模式。WAP本质上是一种功能性设计，旨在使数据质量检查在数据提供给下游消费者之前**更容易实现**。
- en: For instance, an atypical use case is data quality at ingestion. The flow will
    look like creating a staging environment and run quality tests on freshly ingested
    data, before making that data available to any downstream application.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个非典型的用例是数据摄取时的数据质量。流程看起来像是创建一个临时环境，并对新摄取的数据进行质量测试，然后再将这些数据提供给任何下游应用程序。
- en: 'As the name betrays, there are essentially three phases:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 正如名称所示，基本上有三个阶段：
- en: '***Write.*** Put the data in a location that is not accessible to consumers
    downstream (e.g. a staging environment or a branch).'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***写入。*** 将数据放在一个下游消费者无法访问的位置（例如，暂存环境或分支）。'
- en: '***Audit.*** Transform and test the data to make sure it meets the specifications
    (e.g. check whether the schema abruptly changed, or whether there are unexpected
    values, such as NULLs).'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***审计。*** 转换和测试数据，确保它符合规格（例如，检查模式是否突然变化，或是否存在意外值，如NULL）。'
- en: '***Publish.*** Put the data in the place where consumers can read it from (e.g.
    the production data lake).'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***发布。*** 将数据放在消费者可以读取的地方（例如，生产数据湖）。'
- en: '![](../Images/fed5dd1e2f2e05ee22ab00dc9438b184.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fed5dd1e2f2e05ee22ab00dc9438b184.png)'
- en: Image from the authors
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自作者
- en: This is only one example of the possible applications of WAP patterns. It is
    easy to see how it can be applied at different stages of the data life-cycle,
    from ETL and data ingestion, to complex data pipelines supporting analytics and
    ML applications.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是WAP模式应用的一种可能示例。很容易看出，它如何应用于数据生命周期的不同阶段，从ETL和数据摄取，到支持分析和机器学习应用的复杂数据管道。
- en: Despite being so useful, [WAP is still not very widespread](https://lakefs.io/blog/data-engineering-patterns-write-audit-publish/),
    and only recently companies have started thinking about it more systematically.
    The rise of open table formats and projects like [Nessie](https://projectnessie.org/)
    and [LakeFS](https://lakefs.io/) is accelerating the process, but it still a bit
    *avant garde*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管非常有用，[WAP仍然不太普及](https://lakefs.io/blog/data-engineering-patterns-write-audit-publish/)，而且直到最近，企业才开始更系统地思考它。开放表格格式和像[Nessie](https://projectnessie.org/)和[LakeFS](https://lakefs.io/)这样的项目的兴起正在加速这一过程，但它仍然有些*前卫*。
- en: In any case, it is a very good way of thinking about data and it is extremely
    useful in taming some of the most widespread problems keeping engineers up at
    night. So let’s see how we can implement it.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，它是一种非常好的数据思维方式，并且在驯服一些让工程师夜不能寐的最广泛的问题时极为有用。那么，接下来我们来看看如何实现它。
- en: WAP on a data lake in Python
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Python中的数据湖上实现WAP
- en: We are not going to have a theoretical discussion about WAP nor will we provide
    an exhaustive survey of the different ways to implement it ([Alex Merced](https://www.linkedin.com/in/alexmerced/)
    from [Dremio](https://www.dremio.com/) and [Einat Orr](https://www.linkedin.com/in/einat-orr-359ba6/)
    from [LakeFs](https://lakefs.io/) are already doing a phenomenal job at that).
    Instead, we will provide a reference implementation for WAP on a data lake.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会进行关于WAP的理论讨论，也不会提供实现它的各种方式的详尽调查（[Alex Merced](https://www.linkedin.com/in/alexmerced/)来自[Dremio](https://www.dremio.com/)和[Einat
    Orr](https://www.linkedin.com/in/einat-orr-359ba6/)来自[LakeFs](https://lakefs.io/)已经做得非常出色）。相反，我们将提供一个关于数据湖中WAP的参考实现。
- en: 👉 **So buckle up, clone the** [Repo](https://github.com/BauplanLabs/no-jvm-wap-with-iceberg)**,
    and give it a spin!**
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 👉 **系好安全带，克隆** [Repo](https://github.com/BauplanLabs/no-jvm-wap-with-iceberg)**，然后试试看！**
- en: ''
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 📌 F**or more details, please refer to the** [**README**](https://github.com/BauplanLabs/no-jvm-wap-with-iceberg/blob/main/README.md)
    **of the project.**
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 📌 **更多详情，请参考** [**README**](https://github.com/BauplanLabs/no-jvm-wap-with-iceberg/blob/main/README.md)
    **项目文档。**
- en: Architecture and workflow
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构和工作流
- en: The idea here is to simulate an ingestion workflow and implement a WAP pattern
    by branching the data lake and running a data quality test before deciding whether
    to put the data into the final table into the data lake.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的想法是模拟一个数据摄取工作流，并通过分支数据湖来实现WAP模式，在决定是否将数据放入数据湖的最终表之前，先运行数据质量测试。
- en: We use Nessie branching capabilities to get our sandboxed environment where
    data cannot be read by downstream consumers and AWS Lambda to run the WAP logic.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Nessie的分支功能来获取我们的沙箱环境，在该环境中，数据无法被下游消费者读取，同时使用AWS Lambda来运行WAP逻辑。
- en: Essentially, each time a new parquet file is uploaded, a Lambda will go up,
    create a branch in the data catalog and append the data into an Iceberg table.
    Then, a simple a simple data quality test is performed with PyIceberg to check
    whether a certain column in the table contains some NULL values.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，每当一个新的parquet文件上传时，一个Lambda函数会被触发，创建一个数据目录中的分支，并将数据追加到Iceberg表中。然后，使用PyIceberg执行一个简单的数据质量测试，检查表中的某一列是否包含NULL值。
- en: '**If the answer is yes,** the data quality test fails. The new branch will
    not be merged into the main branch of the data catalog, making the data impossible
    to be read in the main branch of data lake. Instead, an alert message is going
    to be sent to [Slack](https://slack.com/).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果答案是肯定的，**则数据质量测试失败。新的分支将不会被合并到数据目录的主分支中，从而使数据无法在数据湖的主分支中被读取。相反，警报消息将被发送到
    [Slack](https://slack.com/)。'
- en: '**If the answer is no,** and the data does not contain any NULLs, the data
    quality test is passed. The new branch will thus be merged into the *main* branch
    of the data catalog and the data will be appended in the Iceberg table in the
    data lake for other processes to read.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果答案是否定的，**并且数据中不包含任何NULL值，那么数据质量测试通过。新的分支将被合并到数据目录的*main*分支中，数据将被追加到数据湖中的Iceberg表，以便其他流程读取。'
- en: '![](../Images/0cec8ad1994f7ffe5eea60bbdbf027ea.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0cec8ad1994f7ffe5eea60bbdbf027ea.png)'
- en: 'Our WAP workflow: image from the authors'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的WAP工作流：图来自作者
- en: All data is completely synthetic and is generated automatically by simply running
    the project. Of course, we provide the possibility of choosing whether to generate
    data that complies with the data quality specifications or to generate data that
    include some NULL values.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 所有数据都是完全合成的，只需运行项目即可自动生成。当然，我们提供了选择是否生成符合数据质量规范的数据，或者生成包含NULL值的数据的可能性。
- en: 'To implement the whole end-to-end flow, we are going to use the following components:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现完整的端到端流程，我们将使用以下组件：
- en: 'Storage: [AWS S3](https://aws.amazon.com/s3/)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储：[AWS S3](https://aws.amazon.com/s3/)
- en: 'Open table format: [Apache Iceberg](https://iceberg.apache.org/)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开放表格格式：[Apache Iceberg](https://iceberg.apache.org/)
- en: 'Data catalog: [Project Nessie](https://projectnessie.org/)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据目录：[Project Nessie](https://projectnessie.org/)
- en: 'Code implementation: [PyIceberg](https://py.iceberg.apache.org/), [PyNessie](https://projectnessie.org/develop/python/)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码实现：[PyIceberg](https://py.iceberg.apache.org/)，[PyNessie](https://projectnessie.org/develop/python/)
- en: 'Serverless runtime: [Lambda](https://aws.amazon.com/lambda/)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无服务器运行时：[Lambda](https://aws.amazon.com/lambda/)
- en: 'Virtual private server: [Lightsail](https://aws.amazon.com/lightsail/)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虚拟私人服务器：[Lightsail](https://aws.amazon.com/lightsail/)
- en: 'Alerting system: [Slack](https://slack.com/)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 警报系统：[Slack](https://slack.com/)
- en: '![](../Images/50bd27c1f3c729677ca7ab2ea24ac596.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/50bd27c1f3c729677ca7ab2ea24ac596.png)'
- en: 'Project architecture: image from the authors'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 项目架构：图来自作者
- en: This project is pretty self-contained and comes with scripts to set up the entire
    infrastructure, so it requires only introductory-level familiarity with AWS and
    Python.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目是相当自包含的，带有设置整个基础设施的脚本，因此只需要对AWS和Python有基础的了解即可。
- en: 'It’s also not intended to be a production-ready solution, but rather a reference
    implementation, a starting point for more complex scenarios: the code is verbose
    and heavily commented, making it easy to modify and extend the basic concepts
    to better suit anyone’s use cases.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不打算作为一个生产级的解决方案，而是作为一个参考实现，是更复杂场景的起点：代码非常详细且有大量注释，便于修改和扩展基本概念，以更好地适应任何人的用例。
- en: Visualize
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化
- en: To visualize the results of the data quality test, we provide a very simple
    [Streamlit](https://streamlit.io/) app that can be used to see what happens when
    some new data is uploaded to first location on S3 — the one that is not available
    to downstream consumers.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化数据质量测试的结果，我们提供了一个非常简单的 [Streamlit](https://streamlit.io/) 应用，可以用来查看当一些新数据上传到
    S3 上的第一个位置时发生了什么——该位置对下游消费者不可用。
- en: We can use the app to check how many rows are in the table across the different
    branches, and for the branches other than *main*, it is easy to see in what column
    the data quality test failed and in how many rows.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用该应用检查不同分支中的表格有多少行，对于 *main* 之外的分支，可以轻松地查看数据质量测试在哪一列和多少行中失败。
- en: '![](../Images/62d00f29924ccf899b8755c6a8e842ec.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62d00f29924ccf899b8755c6a8e842ec.png)'
- en: Data quality app — this is what you see when you examine a certain upload branch
    (i.e. *emereal-keen-shame*) where a table of 3000 row was appended and did not
    pass the data quality check because one value in *my_col_1 is a NULL*. Image from
    the authors.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 数据质量应用——当你检查某个上传分支（即 *emereal-keen-shame*）时会看到这种情况，在该分支中，3000 行数据被附加进来，但由于 *my_col_1*
    中的一个值为 NULL，数据质量检查未通过。图片来自作者。
- en: From the lake to the Lakehouse
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从数据湖到 Lakehouse
- en: Once we have a WAP flow based on Iceberg, we can leverage it to implement a
    composable design for our downstream consumers. In our repo we provide instructions
    for a [Snowflake](https://www.snowflake.com/en/) integration as a way to explore
    this architectural possibility.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们基于 Iceberg 构建了 WAP 流程，就可以利用它为下游消费者实现一个可组合的设计。在我们的仓库中，我们提供了 [Snowflake](https://www.snowflake.com/en/)
    集成的说明，以便探索这一架构可能性。
- en: '![](../Images/b2208722df95aa1b1fbbeefe9c18dcc3.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2208722df95aa1b1fbbeefe9c18dcc3.png)'
- en: 'The first step towards the Lakehouse: image from the authors'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 向 Lakehouse 迈出的第一步：图片来自作者
- en: This is one of the main tenet of the [**Lakehouse**](https://www.databricks.com/sites/default/files/2020/12/cidr_lakehouse.pdf)
    architecture, conceived to be more flexible than modern data warehouses and more
    usable than traditional data lakes.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 [**Lakehouse**](https://www.databricks.com/sites/default/files/2020/12/cidr_lakehouse.pdf)
    架构的一个核心理念，它的设计目的是比现代数据仓库更灵活，同时比传统数据湖更易用。
- en: On the one hand, the Lakehouse hinges on leveraging object store to eliminate
    data redundancy and at the same time lower storage cost. On the other, it is supposed
    to provide more flexibility in choosing different compute engines for different
    purposes.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，Lakehouse 的核心是利用对象存储来消除数据冗余，并同时降低存储成本。另一方面，它应该提供更多灵活性，让用户能够为不同的目的选择不同的计算引擎。
- en: All this sounds very interesting in theory, but it also sounds very complicated
    to engineer at scale. Even a simple integration between Snowflake and an S3 bucket
    as an external volume is frankly pretty tedious.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些听起来在理论上很有趣，但在大规模工程实践中似乎非常复杂。即使是 Snowflake 和 S3 桶作为外部卷的简单集成，说实话也相当繁琐。
- en: '***And in fact, we cannot stress this enough, moving to a full Lakehouse architecture
    is a lot of work. Like a lot!***'
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***事实上，我们必须强调这一点，转向完整的 Lakehouse 架构需要大量的工作。真的很多工作！***'
- en: Having said that, even a journey of a thousand miles begins with a single step,
    so why don’t we start by reaching out the lowest hanging fruits with simple but
    very tangible practical consequences?
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，即使是千里之行，亦始于足下，那么为什么我们不从解决那些最简单但能带来实际影响的问题开始呢？
- en: 'The example in the repo showcases one of these simple use case: WAP and data
    quality tests. The WAP pattern here is a chance to move the computation required
    for data quality tests (and possibly for some ingestion ETL) **outside the data
    warehouse,** while still maintaining the possibility of taking advantage of Snowflake
    for more high value analyitcs workloads on certified artifacts. We hope that this
    post can help developers to build their own proof of concepts and use the'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 该仓库中的示例展示了其中一个简单的用例：WAP 和数据质量测试。这里的 WAP 模式提供了一个机会，将数据质量测试所需的计算（以及可能的一些摄取 ETL）**移出数据仓库**，同时仍然能够利用
    Snowflake 对经过认证的制品进行更高价值的分析工作负载。我们希望这篇文章能够帮助开发者构建自己的概念验证并加以使用。
- en: Conclusions
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'The reference implementation here proposed has several advantages:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这里提出的参考实现有几个优势：
- en: Tables are better than files
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 表格比文件更好
- en: 'Data lakes are historically hard to develop against, since the data abstractions
    are very different from those typically adopted in good old databases. Big Data
    frameworks like [Spark](https://spark.apache.org/) first provided the capabilities
    to process large amounts of raw data stored as files in different formats (e.g.
    parquet, csv, etc), but people often do not think in terms of files: they think
    it terms of tables.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖在历史上一直很难开发，因为数据抽象与传统数据库中通常采用的方式非常不同。大数据框架如 [Spark](https://spark.apache.org/)
    最早提供了处理存储为不同格式文件（例如 parquet、csv 等）的海量原始数据的能力，但人们通常并不以文件为单位进行思考：他们是以表格为单位思考的。
- en: We use an open table format for this reason. Iceberg turns the main data lake
    abstraction into tables rather than files which makes things considerably more
    intuitive. We can now use SQL query engines natively to explore the data and we
    can count on Iceberg to take care of providing correct schema evolution.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们出于这个原因使用开放表格格式。Iceberg 将主要的数据湖抽象转换为表格，而不是文件，这使得操作变得更加直观。现在我们可以原生使用 SQL 查询引擎来探索数据，并且可以依赖
    Iceberg 来提供正确的模式演进。
- en: Interoperability is good for ya
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 互操作性对我们有利
- en: Iceberg also allows for greater interoperability from an architectural point
    of view. One of the main benefits of using open table formats is that data can
    be kept in object store while high-performance SQL engines (Spark, [Trino](https://trino.io/),
    [Dremio](https://www.dremio.com/)) and Warehouses ([Snowflake](https://www.snowflake.com/en/),
    [Redshift](https://aws.amazon.com/redshift/)) can be used to query it. The fact
    that Iceberg is supported by the majority of computational engines out there has
    profound consequences for the way we can architect our data platform.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Iceberg 还从架构角度提供了更好的互操作性。使用开放表格格式的主要好处之一是数据可以保存在对象存储中，同时可以使用高性能的 SQL 引擎（如 Spark、[Trino](https://trino.io/)、[Dremio](https://www.dremio.com/)）和数据仓库（如
    [Snowflake](https://www.snowflake.com/en/)、[Redshift](https://aws.amazon.com/redshift/)）进行查询。Iceberg
    被大多数计算引擎支持这一事实，对我们如何构建数据平台有着深远的影响。
- en: As described above, our suggested integration with Snowflake is meant to show
    that one can deliberately move the computation needed for the ingestion ETL and
    the data quality tests outside of the Warehouse, and keep the the latter for large
    scale analytics jobs and last mile querying that require high performance. At
    scale, this idea can translate into significantly lower costs.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，我们建议的与 Snowflake 的集成旨在展示可以故意将用于摄取 ETL 和数据质量测试的计算移出数据仓库，并将数据仓库保留用于大规模分析作业和需要高性能的最后一公里查询。大规模应用时，这一理念可以转化为显著降低成本。
- en: Branches are useful abstractions
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分支是有用的抽象
- en: WAP pattern requires a way to write data in a location where consumers cannot
    accidentally read it. Branching semantics naturally provides a way to implement
    this, which is why we use Nessie to leverage branching semantics at the data catalog
    level. Nessie builds on Iceberg and on its time travel and table branching functionalities.
    A lot of the work done in our repo is to make Nessie work directly with Python.
    The result is that one can interact with the Nessie catalog and write Iceberg
    tables in different branches of the data catalog without a JVM based process to
    write.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: WAP 模式要求一种写入数据的位置，以避免消费者不小心读取。分支语义自然提供了一种实现此目标的方式，这也是我们使用 Nessie 在数据目录级别利用分支语义的原因。Nessie
    基于 Iceberg 并依赖其时间旅行和表格分支功能。我们在代码库中做了大量工作，使得 Nessie 可以直接与 Python 配合使用。结果是，用户可以与
    Nessie 目录交互，并在数据目录的不同分支中写入 Iceberg 表格，而无需依赖基于 JVM 的进程来执行写入。
- en: Simpler developer experience
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更简洁的开发者体验
- en: Finally, making the end-to-end experience completely Python-based simplifies
    remarkably the set up fo the system and the interaction with it. Any other system
    we are aware of would require a JVM or an additional hosted service to write back
    into Iceberg tables into different branches, while in this implementation the
    entire WAP logic can run inside one single lambda function.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，完全基于 Python 的端到端体验显著简化了系统的设置和与之交互的过程。我们所知道的任何其他系统，都需要 JVM 或额外的托管服务来将数据写回到
    Iceberg 表格中的不同分支，而在这个实现中，整个 WAP 逻辑可以在单个 lambda 函数内运行。
- en: There is nothing inherently wrong with the JVM. It is a fundamental component
    of many Big Data frameworks, providing a common API to work with platform-specific
    resources, while ensuring security and correctness. However, the JVM takes a toll
    from a developer experience perspective. Anybody who worked with Spark knows that
    JVM-based systems tend to be finicky and fail with mysterious errors. For many
    people who work in data and consider Python as their *lingua franca* the advantage
    of the JVM is paid in the coin of usability.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: JVM本身并没有什么固有的缺陷。它是许多大数据框架的核心组件，提供了一个通用API，以便与平台特定资源进行交互，同时确保安全性和正确性。然而，从开发者体验的角度来看，JVM带来了一定的代价。任何与Spark打过交道的人都知道，基于JVM的系统往往脆弱，容易出现神秘的错误。对于许多从事数据工作并将Python视为其*通用语言*的人来说，JVM的优势是以可用性为代价的。
- en: We hope more people are excited about composable designs like we are, we hope
    open standards like Iceberg and Arrow will become the norm, but most of all we
    hope this is useful.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望更多的人能像我们一样对可组合设计充满热情，我们希望像Iceberg和Arrow这样的开放标准能成为主流，但最重要的是，我们希望这对大家有所帮助。
- en: So it goes.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。
