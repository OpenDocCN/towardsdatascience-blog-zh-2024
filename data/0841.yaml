- en: 'A Humanitarian Crises Situation Report AI Assistant: Exploring LLMOps with
    Prompt Flow'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人道主义危机情况报告AI助手：探索LLMOps与Prompt Flow
- en: 原文：[https://towardsdatascience.com/a-humanitarian-crises-situation-report-ai-assistant-exploring-llmops-with-prompt-flow-32968b7a878b?source=collection_archive---------2-----------------------#2024-04-01](https://towardsdatascience.com/a-humanitarian-crises-situation-report-ai-assistant-exploring-llmops-with-prompt-flow-32968b7a878b?source=collection_archive---------2-----------------------#2024-04-01)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-humanitarian-crises-situation-report-ai-assistant-exploring-llmops-with-prompt-flow-32968b7a878b?source=collection_archive---------2-----------------------#2024-04-01](https://towardsdatascience.com/a-humanitarian-crises-situation-report-ai-assistant-exploring-llmops-with-prompt-flow-32968b7a878b?source=collection_archive---------2-----------------------#2024-04-01)
- en: '[](https://medium.com/@astrobagel?source=post_page---byline--32968b7a878b--------------------------------)[![Matthew
    Harris](../Images/4fa3264bb8a028633cd8d37093c16214.png)](https://medium.com/@astrobagel?source=post_page---byline--32968b7a878b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--32968b7a878b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--32968b7a878b--------------------------------)
    [Matthew Harris](https://medium.com/@astrobagel?source=post_page---byline--32968b7a878b--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@astrobagel?source=post_page---byline--32968b7a878b--------------------------------)[![Matthew
    Harris](../Images/4fa3264bb8a028633cd8d37093c16214.png)](https://medium.com/@astrobagel?source=post_page---byline--32968b7a878b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--32968b7a878b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--32968b7a878b--------------------------------)
    [Matthew Harris](https://medium.com/@astrobagel?source=post_page---byline--32968b7a878b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--32968b7a878b--------------------------------)
    ·17 min read·Apr 1, 2024
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--32968b7a878b--------------------------------)
    ·17分钟阅读·2024年4月1日
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: TL;DR
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: TL;DR
- en: '*There are lots of tutorials on using powerful Large Language Models (LLMs)
    for knowledge retrieval. However, if considering real-world application of these
    techniques, engineering best practices need to be applied and these should be
    extended to mitigate some of the new risks associated with LLMs, such as hallucinations.
    In this article, we explore how to implement some key areas required for operationalizing
    LLMs — such as safety, prompt engineering, grounding, and evaluation — developing
    a simple Prompt Flow to create a simple* [*demo AI assistant*](https://github.com/datakind/promptflow_devops_example)
    *for answering questions about humanitarian disasters using information from situation
    reports on the ReliefWeb platform. Prompt Flow includes a great set of tools for
    orchestrating LLM workflows, and packages such as deep eval provide ways to test
    outputs on the fly using LLMs (albeit with some caveats).*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*关于如何使用强大的大型语言模型（LLMs）进行知识检索，有许多教程。然而，如果考虑到这些技术在现实世界中的应用，就需要应用工程最佳实践，并且这些实践应该扩展以减轻与LLMs相关的一些新风险，如幻觉现象。在本文中，我们探讨了如何实现一些关键领域，来实现LLMs的操作化——如安全性、提示工程、基础扎实性和评估——并开发了一个简单的提示流，以创建一个简单的*
    [*演示AI助手*](https://github.com/datakind/promptflow_devops_example) *，用以回答关于人道主义灾难的问题，信息来源于ReliefWeb平台上的情况报告。提示流包括一套很棒的工具，用于编排LLM工作流，并且像deep
    eval这样的包提供了在运行时测试LLM输出的方法（尽管有一些注意事项）。*'
- en: Operationalizing Large Language Model Applications
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作化大型语言模型应用
- en: In a previous blog post “[Some thoughts on operationalizing LLM Applications](https://medium.com/towards-data-science/some-thoughts-on-operationalizing-llm-applications-aae3530821a8)”,
    we discussed that when launching LLM applications there are a wide range of factors
    to consider beyond the shiny new technology of generative AI. Many of the engineering
    requirements apply to any software development, such as DevOps and having a solid
    framework to monitor and evaluate performance, but other areas such as mitigating
    hallucination risk are fairly new. Any organization launching a fancy new generative
    AI application ignores these at their peril, especially in high-risk contexts
    where biased, incorrect, and missing information could have very damaging outcomes.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的博客文章“[一些关于LLM应用程序操作化的思考](https://medium.com/towards-data-science/some-thoughts-on-operationalizing-llm-applications-aae3530821a8)”中，我们讨论了在推出LLM应用程序时，除了生成型AI的炫目新技术外，还有许多因素需要考虑。许多工程要求适用于任何软件开发，如DevOps以及拥有坚实的框架来监控和评估性能，但其他领域，如减轻幻觉风险，则相对较新。任何在推出炫酷的生成型AI应用时忽视这些问题的组织，尤其是在高风险的环境中，都会冒很大风险，因为偏见、不正确和缺失的信息可能会导致非常严重的后果。
- en: '![](../Images/8a97eaba345a040471aea8b856ce7869.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a97eaba345a040471aea8b856ce7869.png)'
- en: 'Some of the key areas that should be considered before launching applications
    that use Large Language Models (LLMs). Source: [Some thoughts on operationalizing
    LLMs](https://medium.com/towards-data-science/some-thoughts-on-operationalizing-llm-applications-aae3530821a8)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 启动使用大型语言模型（LLM）应用程序之前需要考虑的一些关键领域。来源：[一些关于LLM操作化的思考](https://medium.com/towards-data-science/some-thoughts-on-operationalizing-llm-applications-aae3530821a8)
- en: Many organizations are going through this operatiuonalizing process right now
    and are trying to figure out how exactly to use new Generative AI. The good news
    is that we are in a phase where supporting products and services are beginning
    to make it a lot easier to apply solid principles for making applications safe,
    cost-effective, and accurate. [AWS Bedrock](https://aws.amazon.com/bedrock/?trk=ea449fe3-b099-4464-85dc-ff4e38a0359f&sc_channel=ps&ef_id=Cj0KCQjwqpSwBhClARIsADlZ_Tm1aI4E_wUaKbYhZTORvlr9v8k8Wr7XPefKSX6tJkMEtJxbpYDzFvMaAnQREALw_wcB%3AG%3As&s_kwcid=AL%214422%213%21692062173758%21e%21%21g%21%21aws+bedrock%2121054971963%21158684190945&gclid=Cj0KCQjwqpSwBhClARIsADlZ_Tm1aI4E_wUaKbYhZTORvlr9v8k8Wr7XPefKSX6tJkMEtJxbpYDzFvMaAnQREALw_wcB),
    [Azure Machine Learning and Studio](https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/get-started-prompt-flow?view=azureml-api-2),
    [Azure AI Studio (preview)](https://azure.microsoft.com/en-us/products/ai-studio),
    and a wide range of other vendor and open source products all make it easier to
    develop LLM solutions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 许多组织目前正在经历这一操作化过程，并试图弄清楚如何准确地使用新的生成型AI。好消息是，我们正处于一个阶段，支持性产品和服务开始让应用安全、成本效益和准确性原则的应用变得更加容易。[AWS
    Bedrock](https://aws.amazon.com/bedrock/?trk=ea449fe3-b099-4464-85dc-ff4e38a0359f&sc_channel=ps&ef_id=Cj0KCQjwqpSwBhClARIsADlZ_Tm1aI4E_wUaKbYhZTORvlr9v8k8Wr7XPefKSX6tJkMEtJxbpYDzFvMaAnQREALw_wcB%3AG%3As&s_kwcid=AL%214422%213%21692062173758%21e%21%21g%21%21aws+bedrock%2121054971963%21158684190945&gclid=Cj0KCQjwqpSwBhClARIsADlZ_Tm1aI4E_wUaKbYhZTORvlr9v8k8Wr7XPefKSX6tJkMEtJxbpYDzFvMaAnQREALw_wcB)，[Azure机器学习与Studio](https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/get-started-prompt-flow?view=azureml-api-2)，[Azure
    AI Studio（预览版）](https://azure.microsoft.com/en-us/products/ai-studio)，以及一系列其他供应商和开源产品，都使得开发LLM解决方案变得更加容易。
- en: Prompt Flow
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Prompt Flow
- en: In this article, we will focus on using Prompt Flow, an open-source project
    developed by Microsoft …
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将重点讨论使用由微软开发的开源项目Prompt Flow……
- en: '[*Prompt Flow*](https://github.com/microsoft/promptflow) *is a suite of development
    tools designed to streamline the end-to-end development cycle of LLM-based AI
    applications, from ideation, prototyping, testing, and evaluation to production
    deployment and monitoring. It makes prompt engineering much easier and enables
    you to build LLM apps with production quality.*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[*Prompt Flow*](https://github.com/microsoft/promptflow) *是一套开发工具，旨在简化基于LLM的AI应用程序的端到端开发周期，从构思、原型制作、测试、评估到生产部署和监控。它使得提示工程变得更加容易，并使你能够构建具有生产质量的LLM应用程序。*'
- en: Why Prompt Flow?
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么选择Prompt Flow？
- en: After quite a bit of personal research, [Prompt Flow](https://microsoft.github.io/promptflow/)
    has emerged as a great way to develop LLM applications in some situations because
    of the following …
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 经过大量的个人研究，[Prompt Flow](https://microsoft.github.io/promptflow/)在某些情况下已成为开发LLM应用程序的一个极好的选择，原因如下……
- en: '**Intuitive user interface.** As we shall see below, even simple LLM applications
    require complicated workflows. Prompt Flow offers a nice development user interface,
    making it easier to visualize flows, with built-in evaluation and strong [integration
    with Visual Studio Code](https://marketplace.visualstudio.com/items?itemName=prompt-flow.prompt-flow)
    supported by [solid supporting documentation](https://microsoft.github.io/promptflow/).'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**直观的用户界面**。正如我们下面所见，即使是简单的大型语言模型应用程序也需要复杂的工作流。Prompt Flow 提供了一个良好的开发用户界面，使得可视化流程更加容易，并且内置了评估功能，支持与[Visual
    Studio Code的强大集成](https://marketplace.visualstudio.com/items?itemName=prompt-flow.prompt-flow)，并且有[完善的支持文档](https://microsoft.github.io/promptflow/)。'
- en: '**Open source**. This is useful in situations where applications are being
    shipped to organizations with different infrastructure requirements. As we shall
    see below, Prompt Flow isn’t tied to any specific cloud vendor (even though it
    was developed by Microsoft) and can be deployed in several ways.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**开源**。在将应用程序部署到具有不同基础设施需求的组织时，这非常有用。正如我们下面所见，Prompt Flow 并不依赖于任何特定的云供应商（尽管它是由微软开发的），并且可以以多种方式进行部署。'
- en: '**Enterprise support in Azure.** Though open source, if you are on Azure, Prompt
    Flow is natively supported and provides a wide range of enterprise-grade features.
    Being part of Azure Machine Learning Studio and the preview Azure AI studio, it
    comes with off-the-shelf-integration for safety, observability, and deployment,
    freeing up time to focus on the business use case'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Azure 中的企业支持**。虽然是开源的，但如果你在 Azure 上，Prompt Flow 得到了本地支持，并提供广泛的企业级功能。作为 Azure
    机器学习工作室和预览版 Azure AI 工作室的一部分，它提供了现成的集成功能，涵盖了安全性、可观察性和部署，帮助节省时间专注于业务用例。'
- en: '**Easy deployment.** As mentioned above, deployment on Azure is a few clicks.
    But even if you are running locally or another cloud vendor, Prompt flow supports
    deployment using Docker'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**轻松部署**。如上所述，Azure 上的部署只需几次点击。但即使你在本地或使用其他云供应商，Prompt Flow 也支持通过 Docker 部署。'
- en: It may not be ideal for all situations of course, but if you want the best of
    both worlds — open source and enterprise support in Azure — then Prompt Flow might
    be for you.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这可能并不适用于所有情况，但如果你想要兼具开源和 Azure 企业支持的最佳体验，那么 Prompt Flow 可能适合你。
- en: An AI assistant to answer questions about active humanitarian disasters
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个回答有关当前人道主义灾难问题的 AI 助手
- en: In this article we will develop an AI assistant with Prompt Flow that can answer
    questions using information contained in humanitarian reports on the amazing [ReliefWeb
    platform](https://reliefweb.int). ReliefWeb includes content submitted by humanitarian
    organizations which provide information about what is happening on the ground
    for disasters around the world, a common format being ‘Situation Reports’. There
    can be a lot of content so being able to extract a key piece of required information
    quickly is less effort than reading through each report one by one.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将使用 Prompt Flow 开发一个 AI 助手，该助手可以使用在令人惊叹的[ReliefWeb平台](https://reliefweb.int)上包含的人道主义报告中的信息回答问题。ReliefWeb
    包含人道主义组织提交的内容，提供全球灾难发生现场的信息，常见的格式是“情况报告”。内容可能很多，因此能够快速提取所需的关键信息比逐一阅读每份报告要省力得多。
- en: '***Please note:*** *Code for this article can be found* [*here*](https://github.com/datakind/promptflow_devops_example)*,
    but it should be mentioned that it is a basic example and only meant to demonstrate
    some key concepts for operationalizing LLMs. For it to be used in production more
    work would be required around integration and querying of ReliefWeb, as well as
    including the analysis of PDF documents rather than just their HTML summaries,
    but hopefully the code provides some examples people may find useful.*'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '***请注意：*** *本文的代码可以在* [*这里*](https://github.com/datakind/promptflow_devops_example)*找到，但应该提到的是，它是一个基础示例，仅用于展示一些将大型语言模型（LLMs）投入生产的关键概念。要在生产环境中使用它，还需要更多的工作来整合和查询
    ReliefWeb，以及分析 PDF 文档而不仅仅是它们的 HTML 摘要，但希望代码能提供一些大家可能会觉得有用的示例。*'
- en: '![](../Images/0ef4b227074f426a1d77c14b29a3fabe.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ef4b227074f426a1d77c14b29a3fabe.png)'
- en: Process flow used in this article — A demonstration AI agent for answering questions
    about humanitarian disasters using information from situation reports on ReliefWeb.
    The full code can be found [here](https://github.com/datakind/promptflow_devops_example).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中使用的流程 — 一个演示的 AI 代理，用于回答有关人道主义灾难的问题，使用的信息来自 ReliefWeb 上的情况报告。完整代码可以在[这里](https://github.com/datakind/promptflow_devops_example)找到。
- en: The demo application has been set up to demonstrate the following …
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 演示应用程序已经设置好，用于展示以下内容……
- en: Content safety monitoring
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内容安全监控
- en: Orchestrating LLM tasks
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编排 LLM 任务
- en: Automated self-checking for factual accuracy and coverage
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化的事实准确性和覆盖度自我检查
- en: Batch testing of groundedness
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于事实性的批量测试
- en: Self-testing using Prompt Flow run in GitHub actions
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Prompt Flow 在 GitHub Actions 中进行自我测试
- en: Deployment
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署
- en: Setup of the demo Prompt Flow application
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 演示版 Prompt Flow 应用程序的设置
- en: The demo application for this article comes with a `requirements.txt` and runs
    with Python 3.11.4 should you want to install it in your existing environment,
    otherwise please see the setup steps below.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的演示应用程序包含一个 `requirements.txt` 文件，并且可以在 Python 3.11.4 环境下运行，如果您希望将其安装到现有环境中，请参考下面的设置步骤。
- en: If you don’t have these already, install …
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您尚未安装这些，请安装……
- en: '[Visual Studio Code](https://www.google.com/search?client=safari&rls=en&q=download+visual+studio+code&ie=UTF-8&oe=UTF-8)'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Visual Studio Code](https://www.google.com/search?client=safari&rls=en&q=download+visual+studio+code&ie=UTF-8&oe=UTF-8)'
- en: '[Prompt Flow Add-On](https://marketplace.visualstudio.com/items?itemName=prompt-flow.prompt-flow#:~:text=You%20can%20find%20your%20connections,actions%20to%20create%20your%20connections.)'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Prompt Flow 插件](https://marketplace.visualstudio.com/items?itemName=prompt-flow.prompt-flow#:~:text=You%20can%20find%20your%20connections,actions%20to%20create%20your%20connections.)'
- en: '[Miniconda](https://docs.anaconda.com/free/miniconda/miniconda-install/)'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Miniconda](https://docs.anaconda.com/free/miniconda/miniconda-install/)'
- en: Then run through the following steps …
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然后按照以下步骤操作……
- en: 4\. You will need LLM API Keys from either [OpenAI](https://platform.openai.com/docs/quickstart/account-setup)
    or [Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-services/openai/quickstart?tabs=command-line%2Cpython-new&pivots=programming-language-studio),
    as well as the deployment names of the models you want to use
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 您将需要来自 [OpenAI](https://platform.openai.com/docs/quickstart/account-setup) 或
    [Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-services/openai/quickstart?tabs=command-line%2Cpython-new&pivots=programming-language-studio)
    的 LLM API 密钥，以及您想要使用的模型的部署名称
- en: 5\. Check out the [application repo](https://github.com/datakind/promptflow_devops_example)
    which includes the Prompt Flow app in this article
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 [应用程序仓库](https://github.com/datakind/promptflow_devops_example)，其中包含本文中的
    Prompt Flow 应用程序
- en: 6\. In your repo’s top folder, copy`.env.example` to `.env` and set the API
    keys in that file
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的仓库顶层文件夹中，复制 `.env.example` 到 `.env` 并设置该文件中的 API 密钥
- en: '7\. Set up an environment at the command line, open a terminal, and in the
    repo top directory run: `conda env create -f environment.yml`. This will build
    a conda environment called `pf-rweb-demo`'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在命令行中设置环境，打开终端，在仓库的顶层目录运行：`conda env create -f environment.yml`。这将构建一个名为 `pf-rweb-demo`
    的 conda 环境
- en: 8\. Open VS Code
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 打开 VS Code
- en: 9\. Open the repo with File > Open Folder and select the repo’s top directory
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 通过“文件” > “打开文件夹”打开仓库，并选择仓库的顶层目录
- en: 10\. In VS Code, click on the Prompt flow icon — it looks like a ‘P’ on the
    left-hand bar
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在 VS Code 中，点击左侧栏的 Prompt Flow 图标 —— 它看起来像一个‘P’
- en: '![](../Images/7e9e2c8fdd769b5b2014b4681f4d8a1a.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e9e2c8fdd769b5b2014b4681f4d8a1a.png)'
- en: 11\. The first time you click on this, you should see on the upper-left, the
    message below, click on the ‘Install dependencies’ link
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次点击时，您应该在左上角看到如下消息，点击“安装依赖项”链接
- en: '![](../Images/ea9dcc0576628935939fbedcaadb3f79.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea9dcc0576628935939fbedcaadb3f79.png)'
- en: 12\. Click ‘Select Python Interpreter’ and choose the conda Python environment
    `pf-rweb-demo`you built in step 7\. Once you do this the libraries section should
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“选择 Python 解释器”，选择您在步骤 7 中创建的 conda Python 环境 `pf-rweb-demo`。完成此操作后，库部分应该会……
- en: '![](../Images/253a694c320289a68faed03bd29dee09.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/253a694c320289a68faed03bd29dee09.png)'
- en: 13\. You should now see a section called ‘Flows’ on the left-hand navigation,
    click on the ‘relief web_chat’ and select ‘Open’
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您应该能看到左侧导航栏中名为“Flows”的部分，点击“relief web_chat”并选择“打开”
- en: '![](../Images/5192abe052d52875f694e2db5af8e60e.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5192abe052d52875f694e2db5af8e60e.png)'
- en: This should open the Prompt Flow user interface …
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会打开 Prompt Flow 用户界面……
- en: '![](../Images/e21f2c34005d4a945004966b750e8b7c.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e21f2c34005d4a945004966b750e8b7c.png)'
- en: Prompt Flow user interface for the demo code for this article. The flow shows
    how to orchestrate stages in an LLM application
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 本文演示代码的 Prompt Flow 用户界面。该流程展示了如何编排 LLM 应用程序中的各个阶段
- en: 12\. Click on the ‘P’ (Prompt Flow) in the left-hand vertical bar, you should
    see a section for connections
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 点击左侧垂直栏中的‘P’（Prompt Flow），您应该能看到一个连接部分
- en: '![](../Images/160ac3ba3b1a7686f8b9fddc75f4a092.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/160ac3ba3b1a7686f8b9fddc75f4a092.png)'
- en: 13\. Click on the ‘+’ for either Azure OpenAI or OpenAI depending on which service
    you are using.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 13. 点击“+”按钮，选择您正在使用的服务（Azure OpenAI 或 OpenAI）。
- en: 14\. In the connection edit window, set the name to something reasonable, and
    if using Azure the field`api_base` to your base URL. Don’t populate the `api_key`
    as you will get prompted for this.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 14. 在连接编辑窗口中，将名称设置为合理的内容，如果使用的是 Azure，则将字段`api_base`设置为您的基础 URL。不要填充 `api_key`，因为系统会提示您输入此信息。
- en: '![](../Images/851f4f3de90e543ddfc038507c0f0f84.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/851f4f3de90e543ddfc038507c0f0f84.png)'
- en: 15\. Click the little ‘create connection’ and when prompted enter your API Key,
    your connection has now been created
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 15. 点击小的“创建连接”按钮，系统提示时输入您的 API 密钥，您的连接现在已创建
- en: 16\. If you are using Azure and called your connection azure_openai and have
    model deployments ‘gpt-4-turbo’ and ‘got-35-turbo-16k’, you should be configured,
    otherwise, click on any LLM Nodes in the Prompt Flow user interface and set the
    connection and deployment name appropriately. See below for the settings used
    for ‘extract_entities’ LLM node
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 16. 如果您使用的是 Azure，并且将连接命名为 azure_openai，且模型部署为“gpt-4-turbo”和“got-35-turbo-16k”，您应该已经配置完成，否则，请点击
    Prompt Flow 用户界面中的任何 LLM 节点，正确设置连接和部署名称。请参见下面用于“extract_entities”LLM 节点的设置。
- en: '![](../Images/65bc6591ec6791da035b060b718da550.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/65bc6591ec6791da035b060b718da550.png)'
- en: Running the demo Prompt Flow application
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行演示版 Prompt Flow 应用程序
- en: Now that you’re all set up, anytime you want to run the flow …
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已完成设置，任何时候您想要运行流程时…
- en: Open the flow as described in steps 9–11 above
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照上面第 9–11 步描述的方式打开流程
- en: Click on the little double-play icon at the top of the flow
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击流程顶部的小双播放图标
- en: '![](../Images/b3836b737f0e0b1705c4b24248b1d130.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3836b737f0e0b1705c4b24248b1d130.png)'
- en: This should run the full flow. To see the outputs you can click on any node
    and view inputs/outputs and even run individual nodes as part of debugging.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该运行完整的流程。要查看输出，您可以点击任何节点并查看输入/输出，甚至作为调试的一部分运行单独的节点。
- en: Now, let’s go through some of the main components of the application …
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看应用程序的几个主要组件……
- en: Content Safety
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内容安全
- en: 'Any chat application using LLMs should have some tests to ensure user inputs
    and LLM outputs are safe. Safety checks should cover areas such as:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 任何使用 LLM 的聊天应用程序都应进行一些测试，以确保用户输入和 LLM 输出是安全的。安全检查应涵盖以下领域：
- en: Bias
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏见
- en: Hate speech / Toxicity
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仇恨言论 / 有毒内容
- en: Self-harm
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自残
- en: Violence
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 暴力
- en: Prompt injection (hacking to get different prompt through to the LLM)
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示注入（通过黑客手段获取不同的提示进入 LLM）
- en: Intellectual property infringement
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 知识产权侵权
- en: This list is not exhaustive and not all will be applicable, depending on the
    application context, but a review should always be carried out and appropriate
    safety tests identified.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 该列表并不详尽，且并非所有项都适用，具体取决于应用程序的上下文，但应该始终进行审查，并确定适当的安全性测试。
- en: Prompt Flow comes with integration to [Azure content safety](https://azure.microsoft.com/en-us/products/ai-services/ai-content-safety)
    which covers some of the above and is very easy to implement by selecting ‘Content
    Safety’ when creating a new node in the flow. I originally configured the demo
    application to use this, but realized not everybody will have Azure so instead
    the flow includes two Python placeholder nodes `content_safety_in` and `content_safety_out`
    to illustrate where content safety checks could be applied. These do not implement
    actual safety validation in the demo application, but libraries such as [Guardrails
    AI](https://www.guardrailsai.com) and [deep eval](https://github.com/confident-ai/deepeval)
    offer a range of tests that could be used in these scripts.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Prompt Flow 集成了[Azure 内容安全](https://azure.microsoft.com/en-us/products/ai-services/ai-content-safety)，该功能涵盖了上述一些内容，并且通过在创建新节点时选择“内容安全”非常容易实现。我最初配置了演示应用程序来使用此功能，但意识到并非每个人都有
    Azure，因此该流程包括了两个 Python 占位符节点 `content_safety_in` 和 `content_safety_out`，以说明可以应用内容安全检查的位置。这些节点在演示应用程序中并未实现实际的安全验证，但像[Guardrails
    AI](https://www.guardrailsai.com)和[deep eval](https://github.com/confident-ai/deepeval)等库提供了一系列可以在这些脚本中使用的测试。
- en: The `content_safety_in`node controls the downstream flow, and will not call
    those tasks if the content is considered unsafe.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`content_safety_in`节点控制下游流程，如果内容被认为不安全，将不会调用相关任务。'
- en: Given the LLM output is heavily grounded in provided data and evaluated on the
    fly, it’s probably overkill to include a safety check on the output for this application,
    but it illustrates that there are two points safety could be enforced in an LLM
    application.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLM的输出严重依赖于提供的数据并实时评估，对于此应用程序来说，可能没有必要在输出中添加安全检查，但它展示了在LLM应用中可以执行安全性强制的两个方面。
- en: It should also be noted that Azure also provides safety filters at the LLM level
    if using Azure Model Library. This can be a convenient way to implement content
    safety without having to develop code or specify nodes in your flow, clicking
    a button and paying a little extra for a safety service can sometimes be the better
    option.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 还应注意，如果使用Azure模型库，Azure也提供LLM级别的安全过滤器。这是一种方便实现内容安全的方法，无需开发代码或指定流程中的节点，点击按钮并支付一些额外费用来获取安全服务，有时可能是更好的选择。
- en: Entity Extraction
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实体提取
- en: In order to query the ReliefWeb API it is useful to extract entities from the
    user’s question and search with those rather than the raw input. Depending on
    the remote API this can yield more appropriate situation reports for finding answers.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了查询ReliefWeb API，提取用户问题中的实体并使用这些实体进行搜索，而不是直接使用原始输入是很有用的。根据远程API的不同，这可以产生更合适的情况报告，以便找到答案。
- en: An example in the demo application is as follows …
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 演示应用程序中的一个示例如下……
- en: 'User input: “*How many children are affected by the Sudan crises?*”'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 用户输入：“*有多少儿童受到苏丹危机的影响？*”
- en: 'LLM Entities extracted:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: LLM提取的实体：
- en: '[PRE0]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'ReliefWeb API query string: “*Sudan crises*”'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ReliefWeb API查询字符串：“*苏丹危机*”
- en: This is a very basic entity extraction as we are only interested in a simple
    search query that will return results in the ReliefWeb API. The API supports more
    complex filtering and entity extraction could be extended accordingly. Other Named
    Entity Recognition techniques like [GLiNER](https://arxiv.org/abs/2311.08526)
    may improve performance.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常基础的实体提取，因为我们仅对一个简单的搜索查询感兴趣，该查询会返回ReliefWeb API中的结果。该API支持更复杂的过滤功能，实体提取可以相应扩展。像[GLiNER](https://arxiv.org/abs/2311.08526)这样的其他命名实体识别技术可能会提高性能。
- en: Getting data from the ReliefWeb API
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从ReliefWeb API获取数据
- en: Once a query string is generated, a call to the ReliefWeb API can be made. For
    the demo application we restrict the results to the top 5 most recent situation
    reports, where Python code creates the following API request …
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦生成查询字符串，就可以调用ReliefWeb API。对于演示应用程序，我们将结果限制为最新的5个情况报告，其中Python代码创建以下API请求……
- en: '[PRE1]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[ The above corresponds with [this website query](https://reliefweb.int/updates?advanced-search=%28F10%29&search=sudan+crises)
    ]'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[上述内容对应于[这个网站查询](https://reliefweb.int/updates?advanced-search=%28F10%29&search=sudan+crises)]'
- en: One thing to note about calling APIs is that they can incur costs if API results
    are processed directly by the LLM. I’ve written a little about this [here](/reframing-llm-chat-with-data-introducing-llm-assisted-data-recipes-f4096ac8c44b),
    but for small amounts of data, the above approach should suffice.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 关于调用API有一点需要注意的是，如果API结果直接由LLM处理，它们可能会产生费用。我在[这里](/reframing-llm-chat-with-data-introducing-llm-assisted-data-recipes-f4096ac8c44b)写了一些相关内容，但对于少量数据，上述方法应该足够。
- en: Summarization
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Though the focus of the demo application is on answering a specific question,
    a summary node has been included in the flow to illustrate the possibility of
    having the LLM perform more than one task. This is where Prompt Flow works well,
    in orchestrating complex multi-task processes.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管该演示应用程序的重点是回答特定问题，但在流程中已包含了一个摘要节点，以展示LLM执行多项任务的可能性。这就是Prompt Flow表现良好的地方，能够协调复杂的多任务流程。
- en: LLM summarization is an active research field and poses some interesting challenges.
    Any summarization will lose information from the original document, this is expected.
    However, controlling which information is excluded is important and will be specific
    to requirements. When summarizing a ReliefWeb situation report, it may be important
    in one scenario to ensure all metrics associated with refugee migration are accurately
    represented. Other scenarios might require that information related to infrastructure
    is the focus. The point being that a summarization prompt may need to be tailored
    to the audience’s requirements. If this is not the case, there are some useful
    general summarization prompts such as [Chain of Densit](https://arxiv.org/abs/2309.04269)y
    (CoD) which aim to capture pertinent information.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: LLM摘要是一个活跃的研究领域，并且带来了一些有趣的挑战。任何摘要都会丢失原始文档中的信息，这是预期的。然而，控制哪些信息被排除是很重要的，并且会根据需求有所不同。在总结一个ReliefWeb的情况报告时，某些场景下可能需要确保与难民迁移相关的所有指标都被准确表示。其他场景可能需要将与基础设施相关的信息作为重点。关键点在于，摘要提示可能需要根据观众的需求进行定制。如果不是这种情况，也有一些有用的通用摘要提示，比如[Chain
    of Densit](https://arxiv.org/abs/2309.04269)y (CoD)，旨在捕捉相关信息。
- en: The demo app has two summarization prompts, a very basic one …
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 演示应用程序有两个摘要提示，其中一个非常基础……
- en: '[PRE2]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As well as a variant which uses CoD …
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 以及一个使用CoD的变体……
- en: '[PRE3]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Question Answering
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题回答
- en: 'The demo app contains a node to answer the user’s original question. For this
    we used a prompt as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 演示应用程序包含一个节点来回答用户的原始问题。为此，我们使用了如下提示：
- en: '[PRE4]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This is a basic prompt which includes a request to include references and links
    with any answer.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个基本的提示，其中包括请求在任何答案中包含参考和链接。
- en: Attribution of Informational Sources
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信息来源的归属
- en: Even with validation and automatic fact-checking of LLM outputs, it is very
    important to provide attribution links to data sources used so the human can check
    themselves. In some cases it may still be useful to provide an uncertain answer
    — clearly informing the user about the uncertainty — as long as there is an information
    trail to the sources for further human validation.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 即使对LLM输出进行验证和自动事实检查，提供数据来源的归属链接依然非常重要，以便人工可以自行检查。在某些情况下，提供一个不确定的答案——明确告知用户不确定性——仍然可能是有用的，只要提供了信息来源的链路，供进一步的人工验证。
- en: In our example this means links to the situation reports which were used to
    answer the user’s question. This allows the person asking the question to jump
    to the sources and check facts themselves, as well as read additional context.
    In the demo app we have included two attribution methodologies. The first is to
    include a request in the prompt, as shown above. As with any LLM output this can
    of course result in hallucination, but as we’ll see below these can be validated.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，这意味着链接到用于回答用户问题的情况报告。这允许提问者跳转到源头，自己核对事实，并阅读更多的背景信息。在演示应用程序中，我们包含了两种归属方法。第一种是在提示中包含请求，如上所示。与任何LLM输出一样，这当然可能导致幻觉，但正如我们下面所看到的，这些可以进行验证。
- en: The second method is to simply collate a list of documents returned in the API
    call, being all the sources reviewed even if some weren’t used in the answer.
    Being able to view the full list can help identify cases where a key report was
    perhaps missed due to how the API was queried.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法是简单地整理在API调用中返回的文档列表，即所有被审阅的源，即使某些源未在答案中使用。能够查看完整列表有助于识别因API查询方式而可能错过的关键报告。
- en: Both attribution methods can be useful to the user in understanding how their
    answer was found.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 两种归属方法对用户理解答案的来源都很有帮助。
- en: Automatic Fact Checking
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动事实检查
- en: LLM information extraction, though amazing, is imperfect. Hallucinations and
    information omission are possible in situations where questions are asked of content.
    Therefore it’s key to validate the answer to ensure it isn’t presenting incorrect
    or incomplete information. Since we are essentially comparing one text (raw data
    returned from the API) with LLM-generated text (the question answer), we can also
    use LLMs to validate. With good prompt design these can be made to work well,
    if not absolutely perfectly.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: LLM信息提取虽然令人惊叹，但并不完美。在对内容提出问题时，幻觉和信息遗漏是可能发生的。因此，验证答案以确保其不提供错误或不完整的信息是至关重要的。由于我们本质上是在将一篇文本（从API返回的原始数据）与LLM生成的文本（问题回答）进行比较，我们也可以使用LLM来进行验证。通过良好的提示设计，这些方法可以很好地工作，尽管不能做到绝对完美。
- en: In the demo app we have include two methods for this to illustrate …
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在演示应用中，我们包含了两种方法来说明这一点……
- en: '**Real-time fact checking**'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**实时事实检查**'
- en: One useful library for checking LLM-generated output against the source data
    is [deep eval](https://github.com/confident-ai/deepeval), which offers a wide
    range of tests for LLM generated output. In our example we have implemented a
    faithfulness check to compare the answer with raw API output and give a score
    between 0–1 and reason for that score, where 1 is the best score possible. In
    the demo app, even if the score is less than 1 we have opted to allow the answer
    to be presented to the user, along with a warning and reasons, about the score
    being less than perfect.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 一个用于检查LLM生成的输出与源数据一致性的有用库是[deep eval](https://github.com/confident-ai/deepeval)，它提供了广泛的LLM生成输出测试。在我们的示例中，我们实现了一个忠实度检查，比较答案与原始API输出，并给出0到1之间的分数及其原因，其中1是最佳得分。在演示应用中，即使得分低于1，我们也选择允许将答案展示给用户，并附上警告和得分低于完美的原因。
- en: Here is an example answer showing the warning for a run asking the question
    “*What is the current status of internet providers in the Sudan crises?*” where
    we adjusted the answer intentionally to say the effects were minor instead of
    the true major communications outage mentioned in the source data …
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个示例答案，显示了对提问“*苏丹危机中互联网供应商的当前状态是什么？*”的警告，我们故意调整答案，表示影响是轻微的，而不是源数据中提到的重大通讯中断……
- en: 'The current status of internet providers in Sudan, as reported in the “Sudan,
    conflict — ETC Situation Report #15 Reporting period: 01/02/2024 to 29/02/2024”
    by the Emergency Telecommunications Cluster (ETC) led by the World Food Programme
    (WFP), indicates that there have been only minor disruptions to telecommunications
    infrastructure due to the conflict between the Sudanese Armed Forces (SAF) and
    the Rapid Support Forces (RSF). Here is the relevant excerpt from the report:'
  id: totrans-121
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 根据世界粮食计划署（WFP）领导的紧急电信集群（ETC）在“苏丹，冲突 — ETC情况报告#15，报告期：2024年2月1日至2024年2月29日”中报告，苏丹互联网服务提供商的当前状态显示，由于苏丹武装部队（SAF）与快速支持部队（RSF）之间的冲突，电信基础设施仅发生了轻微中断。以下是报告中的相关摘录：
- en: ''
  id: totrans-122
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: “All three of Sudan’s main internet operators were reported as offline on 07
    February in the latest of a **minor** network blackout of communications affecting
    millions of people.”
  id: totrans-123
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “根据最新报告，苏丹三大主要互联网运营商在2月7日停机，属于**轻微**的网络通讯中断，影响了数百万人的通讯。”
- en: ''
  id: totrans-124
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This information suggests that as of the reporting period, there was a **minor**
    network blackout, and the main internet operators in Sudan were offline, which
    would have a **minor** impact on communication and response efforts within the
    country.
  id: totrans-125
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这些信息表明，截至报告期，苏丹发生了**轻微**的网络中断，苏丹的主要互联网运营商处于离线状态，这可能对该国的通讯和响应工作产生**轻微**影响。
- en: ''
  id: totrans-126
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'For more details, you can access the full report on ReliefWeb: [Sudan, conflict
    — ETC Situation Report #15](https://reliefweb.int/node/4042273).'
  id: totrans-127
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '若想了解更多细节，可以访问ReliefWeb上的完整报告：[Sudan, conflict — ETC Situation Report #15](https://reliefweb.int/node/4042273)。'
- en: ''
  id: totrans-128
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Warning!** Fact checker evaluation returned a score of 0.88/1.0'
  id: totrans-129
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**警告！** 事实检查员评估返回的得分为0.88/1.0'
- en: ''
  id: totrans-130
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Reason:**'
  id: totrans-131
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**原因：**'
- en: ''
  id: totrans-132
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The score is 0.88 because the actual output incorrectly downplays the extent
    of the damage to telecommunications infrastructure in Sudan, suggesting only minor
    disruptions, whereas the retrieval context indicates there was widespread damage
    to telecommunications infrastructure and the national power grid.
  id: totrans-133
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 得分为0.88，因为实际输出错误地轻描淡写了苏丹电信基础设施损毁的程度，仅提到轻微的中断，而检索上下文则表明，电信基础设施和国家电网遭遇了广泛的损坏。
- en: Note the Warning section at the end and the associated Reason.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意末尾的警告部分及相关原因。
- en: It should however be noted, that though deep eval offers a neat way to evaluate
    LLMs, since it uses an LLM it too could sometimes suffer from hallucination. For
    the demo application performance was acceptable in re-running the same question
    20 times, but for production, it would make sense to include self-tests to evaluate
    the evaluation (!) and ensure behavior is as expected.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 然而需要注意的是，尽管深度评估提供了一种评估LLM（大语言模型）的方法，但由于它使用LLM，因此它有时也可能遭遇幻觉问题。在演示应用程序中，重新运行同一问题20次的性能是可以接受的，但在生产环境中，包含自测以评估评估结果（！）并确保行为符合预期是有意义的。
- en: '**Batch Groundedness testing**'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**批量一致性测试**'
- en: Another approach supported by Prompt Flow is the ability to create a [test file](https://github.com/datakind/promptflow_devops_example/blob/main/flows/reliefweb_chat/data.jsonl)
    with inputs and context information, which can be executed in a prompt flow batch
    run. This is analogous to software self-tests, with a twist that in evaluating
    LLMs where responses can vary slightly each time, it’s useful to use LLMs in the
    tests also. In the demo app, there is a groundedness test that does exactly this
    for batch runs, where the outputs of all tests are collated and summarized so
    that performance can be tracked over time.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Prompt Flow支持的另一种方法是能够创建一个[test文件](https://github.com/datakind/promptflow_devops_example/blob/main/flows/reliefweb_chat/data.jsonl)，其中包含输入和上下文信息，可以在Prompt
    Flow批量运行中执行。这类似于软件自检，唯一的不同是，在评估LLM时，由于每次响应可能会略有不同，因此在测试中使用LLM也非常有用。在示例应用中，有一个“合理性”测试正是为批量运行设计的，在该测试中，所有测试的输出会被整理和总结，从而可以跟踪性能变化。
- en: We have included batch test nodes in the demo app for demonstration purposes,
    but in the live applications, they wouldn’t be required and could be removed for
    improved performance.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在示例应用中包含了批量测试节点以供演示，但在实际应用中，这些节点不需要，并且可以删除以提高性能。
- en: Finally, it’s worth noting that although we can implement strategies to mitigate
    LLM-related issues, any software can have bugs. If the data being returned from
    the API doesn’t contain the required information to begin with, no amount of LLM
    magic will find the answer. For example, the data returned from ReliefWeb is heavily
    influenced by the search engine so if the best search terms aren’t used, important
    reports may not be included in the raw data. LLM fact-checking cannot control
    for this, so it’s important not to forget good old-fashioned self-tests and integration
    tests.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，值得注意的是，尽管我们可以实施策略来减轻与LLM相关的问题，但任何软件都可能存在bug。如果从API返回的数据本身就不包含所需的信息，那么再多的LLM魔力也无法找到答案。例如，从ReliefWeb返回的数据受到搜索引擎的强烈影响，因此如果没有使用最佳的搜索词，重要报告可能不会包含在原始数据中。LLM的事实核查无法解决这个问题，因此，切勿忘记传统的自检和集成测试。
- en: LLMOps
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMOps
- en: Now that we have batch tests in Prompt Flow, we can use these as part of our
    DevOps, or LLMOps, process. The demo app repo contains a set of [GitHub actions](https://github.com/datakind/promptflow_devops_example/blob/main/.github/workflows/test_deploy.yml)
    that run the tests automatically, and check the aggregated results to automatically
    confirm if the app is performing as expected. This confirmation could be used
    to control whether the application is deployed or not.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们在Prompt Flow中有了批量测试，可以将这些测试作为我们DevOps或LLMOps流程的一部分。示例应用仓库包含一组[GitHub Actions](https://github.com/datakind/promptflow_devops_example/blob/main/.github/workflows/test_deploy.yml)，它会自动运行测试，并检查聚合结果，以自动确认应用是否按预期运行。这种确认可以用来控制应用是否部署。
- en: '![](../Images/4a03fa723de3c0341fe2c1b40f2409ab.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4a03fa723de3c0341fe2c1b40f2409ab.png)'
- en: Deployment
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署
- en: Which brings us onto deployment. Prompt Flow offers easy ways to deploy, which
    is a really great feature which may save time so more effort can be put into addressing
    the user’s requirements.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这将引导我们进入部署环节。Prompt Flow提供了简便的部署方式，这是一个非常棒的功能，可以节省时间，从而将更多精力投入到解决用户需求上。
- en: The ‘Build’ option will suggest two options ‘Build as local app’ and ‘Build
    as Docker’.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: “构建”选项会建议两个选项：“作为本地应用构建”和“作为Docker构建”。
- en: '![](../Images/62e102f5f9b36c02846acf39062fe9c9.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62e102f5f9b36c02846acf39062fe9c9.png)'
- en: The first is quite useful and will launch a chat interface, but it’s only meant
    for testing and not production. The second will build a Docker container, to present
    an API app running the flow. This container could be deployed on platforms supporting
    docker and used in conjunction with a front-end chat interface such as Streamline,
    chainlit, Copilot Studio, etc. If deploying using Docker, then observability for
    how your app is used — a must for ensuring AI safety — needs to be configured
    on the service hosting the Docker container.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个选项非常有用，它会启动一个聊天界面，但仅用于测试，不能用于生产环境。第二个选项将构建一个Docker容器，以呈现一个运行流程的API应用。这个容器可以部署在支持Docker的平台上，并与前端聊天界面（如Streamline、Chainlit、Copilot
    Studio等）配合使用。如果使用Docker进行部署，那么如何观察应用的使用情况——确保AI安全的必备功能——需要在托管Docker容器的服务上进行配置。
- en: For those using Azure, the flow can be imported to Azure Machine Learning, where
    it can be managed as in VS Code. One additional feature here is that it can be
    deployed as an API with the click of a button. This is a great option because
    the deployment can be configured to include detailed observability and safety
    monitoring with very little effort, albeit with some cost.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用 Azure 的用户，流程可以导入到 Azure 机器学习中，在那里可以像在 VS Code 中一样进行管理。这里的一个额外功能是，可以通过点击按钮将其部署为
    API。这是一个很好的选择，因为部署可以配置以包含详细的可观察性和安全监控，尽管需要一些成本，但付出的努力很少。
- en: Final Thoughts
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终思考
- en: 'We have carried out a quick exploration of how to implement some important
    concepts required when operationalizing LLMs: content safety, fact checking (real-time
    and batch), fact attribution, prompt engineering, and DevOps. These were implemented
    using Prompt Flow, a powerful framework for developing LLM applications.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们快速探索了在操作化大语言模型（LLM）时需要实现的一些重要概念：内容安全、事实检查（实时和批量）、事实归属、提示工程和 DevOps。这些概念是通过
    Prompt Flow 实现的，这是一个用于开发 LLM 应用程序的强大框架。
- en: The demo application we used is only a demonstration, but it shows how complex
    simple tasks can quickly get when considering all aspects of productionizing LLM
    applications safely.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的演示应用程序只是一个展示，但它展示了在考虑到将 LLM 应用程序安全生产化的各个方面时，简单任务如何迅速变得复杂。
- en: Caveats and Trade-offs
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注意事项和权衡
- en: As with all things, there are trade-offs when implementing some of the items
    above. Adding safety tests and real-time evaluation will slow application response
    times and incur some extra costs. For me, this is an acceptable trade-off for
    ensuring solutions are safe and accurate.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 和所有事情一样，实施上述一些项目时会有权衡。增加安全性测试和实时评估会减慢应用响应时间，并带来一些额外的成本。对我而言，这是一个可以接受的权衡，因为它确保了解决方案的安全性和准确性。
- en: Also, though the LLM evaluation techniques are a great step forward in making
    applications more trustworthy and safe, using LLMs for this is not infallible
    and will sometimes fail. This can be addressed with more engineering of the LLM
    output in the demo application, as well as advances in LLM capabilities — it’s
    still a relatively new field — but it’s worth mentioning here that application
    design should include evaluation of the evaluation techniques. For example, creating
    a set of self-tests with defined context and question answers and running those
    through the evaluation workflow to give confidence it will work as expected in
    a dynamic environment.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，虽然 LLM 评估技术在使应用程序更值得信赖和安全方面迈出了重要步伐，但使用 LLM 进行评估并不是万无一失的，有时会失败。这可以通过更多地工程化演示应用程序中的
    LLM 输出，以及 LLM 能力的进展来解决——毕竟这是一个相对较新的领域——但值得在这里提到的是，应用程序设计应包括对评估技术的评估。例如，可以创建一组自我测试，定义上下文和问题答案，并将其通过评估工作流，以确保它在动态环境中按预期工作。
- en: I hope you have enjoyed this article!
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你喜欢这篇文章！
- en: References
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[Prompt Flow documentation](https://microsoft.github.io/promptflow/)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Prompt Flow 文档](https://microsoft.github.io/promptflow/)'
- en: '[ReliefWeb](https://reliefweb.int)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ReliefWeb](https://reliefweb.int)'
- en: '[From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting](https://arxiv.org/abs/2309.04269),
    Adams et al, 2023'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从稀疏到密集：使用密度提示链进行GPT-4摘要](https://arxiv.org/abs/2309.04269)，Adams 等，2023'
- en: '[GLiNER: Generalist Model for Named Entity Recognition using Bidirectional
    Transformer](https://arxiv.org/abs/2311.08526), Zaratiana et al, 2023'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GLiNER：使用双向 Transformer 的命名实体识别通用模型](https://arxiv.org/abs/2311.08526)，Zaratiana
    等，2023'
- en: The code for this article can be found [here](https://github.com/datakind/promptflow_devops_example)
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本文的代码可以在[这里](https://github.com/datakind/promptflow_devops_example)找到
- en: '*Please like this article if inclined and I’d be delighted if you followed
    me! You can find more articles* [*here*](/@astrobagel)*.*'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你喜欢这篇文章，请点赞，如果你关注我，我将非常高兴！你可以在* [*这里*](/@astrobagel)*找到更多文章*。'
