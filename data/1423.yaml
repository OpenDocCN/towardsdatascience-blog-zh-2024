- en: Scale Is All You Need for Lip-Sync?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 唇动同步仅需规模数据？
- en: 原文：[https://towardsdatascience.com/scale-is-all-you-need-for-lip-sync-0c571423f60f?source=collection_archive---------5-----------------------#2024-06-07](https://towardsdatascience.com/scale-is-all-you-need-for-lip-sync-0c571423f60f?source=collection_archive---------5-----------------------#2024-06-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/scale-is-all-you-need-for-lip-sync-0c571423f60f?source=collection_archive---------5-----------------------#2024-06-07](https://towardsdatascience.com/scale-is-all-you-need-for-lip-sync-0c571423f60f?source=collection_archive---------5-----------------------#2024-06-07)
- en: Alibaba’s EMO and Microsoft’s VASA-1 are crazy good. Let’s break down how they
    work.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 阿里巴巴的EMO和微软的VASA-1简直太强大了。让我们来拆解它们是如何工作的。
- en: '[](https://medium.com/@jacksaunders909?source=post_page---byline--0c571423f60f--------------------------------)[![Jack
    Saunders](../Images/00c752fe1c5bc03f52943238cb034ee4.png)](https://medium.com/@jacksaunders909?source=post_page---byline--0c571423f60f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0c571423f60f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0c571423f60f--------------------------------)
    [Jack Saunders](https://medium.com/@jacksaunders909?source=post_page---byline--0c571423f60f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jacksaunders909?source=post_page---byline--0c571423f60f--------------------------------)[![Jack
    Saunders](../Images/00c752fe1c5bc03f52943238cb034ee4.png)](https://medium.com/@jacksaunders909?source=post_page---byline--0c571423f60f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0c571423f60f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0c571423f60f--------------------------------)
    [Jack Saunders](https://medium.com/@jacksaunders909?source=post_page---byline--0c571423f60f--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0c571423f60f--------------------------------)
    ·11 min read·Jun 7, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0c571423f60f--------------------------------)
    ·11分钟阅读·2024年6月7日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: It’s no secret that the pace of AI research is exponentially accelerating. One
    of the biggest trends of the past couple of years has been using transformers
    to exploit huge-scale datasets. It looks like this trend has finally reached the
    field of lip-sync models. [The EMO release](https://github.com/HumanAIGC/EMO?tab=readme-ov-file)
    by Alibaba set the precedent for this (I mean look at the 200+ GitHub issues begging
    for code release). But the bar has been raised even higher with [Microsoft’s VASA-1
    last month](https://www.microsoft.com/en-us/research/project/vasa-1/).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: AI研究的进展正在以指数级速度加快，这已经不是什么秘密。过去几年中，一个最大的趋势是使用变换器模型来利用大规模数据集。看起来这个趋势终于到达了唇动同步模型领域。[阿里巴巴的EMO发布](https://github.com/HumanAIGC/EMO?tab=readme-ov-file)为此树立了先例（我的意思是，看看那些请求发布代码的200多个GitHub问题）。但是，随着[微软的VASA-1上个月发布](https://www.microsoft.com/en-us/research/project/vasa-1/)，这个门槛被进一步提高了。
- en: Demo videos from [VASA-1](https://www.microsoft.com/en-us/research/project/vasa-1/)
    and EMO. All image credits to the respective authors.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[VASA-1](https://www.microsoft.com/en-us/research/project/vasa-1/)和EMO的演示视频。所有图片的版权归各自作者所有。
- en: They’ve received a lot of hype, but so far no one has discussed what they’re
    doing. They look like almost identical works on the face of it (pun intended).
    Both take a single image and animate it using audio. Both use diffusion and both
    exploit scale to produce phenomenal results. But in actuality, there are a few
    differences under the hood. This article will take a sneak peek at how these models
    operate. We also take a look at the ethical considerations of these papers, given
    their obvious potential for misuse.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 它们受到了很多关注，但到目前为止，没有人讨论过它们的具体做法。从表面上看，它们几乎是完全相同的作品（有意为之）。两者都使用单一图像，并通过音频来动画化它。两者都使用扩散模型，并且都通过规模的利用来产生惊人的效果。但实际上，背后有一些不同之处。本文将简要探讨这些模型如何运作。我们还将探讨这些论文的伦理考虑，鉴于它们显然存在被滥用的潜力。
- en: The Data
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据
- en: 'A model can only be as good as the data it is trained on. Or, more succinctly,
    Garbage In = Garbage Out. Most existing lip sync papers make use of one or two,
    reasonably small datasets. The two papers we are discussing absolutely blow away
    the competition in this regard. Let’s break down what they use. Alibaba state
    in EMO:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一个模型的好坏，取决于它训练所使用的数据。或者更简洁地说，垃圾进，垃圾出。大多数现有的唇动同步论文使用了一到两个相对较小的数据集。我们讨论的这两篇论文在这方面绝对超越了所有竞争对手。让我们来看看它们使用了什么。阿里巴巴在EMO中提到：
- en: We collected approximately 250 hours of talking head videos from the internet
    and supplemented this with the HDTF [34] and VFHQ [31] datasets to train our models.
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们从互联网收集了大约250小时的谈话头像视频，并使用HDTF [34]和VFHQ [31]数据集对我们的模型进行了补充训练。
- en: 'Exactly what they mean by the additional 250 hours of collected data is unknown.
    However, HDTF and VFHQ are publicly available datasets, so we can break these
    down. [HDTF](https://github.com/MRzzm/HDTF) consists of 16 hours of data over
    300 subjects of 720–1080p video. [VFHQ](https://liangbinxie.github.io/projects/vfhq/)
    doesn’t mention the length of the dataset in terms of hours, but it has 15,000
    clips and takes up 1.2TB of data. If we assume each clip is, on average, at least
    10s long then this would be an additional 40 hours. This means EMO uses at least
    300 hours of data. For VASA-1 Microsoft say:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 至于他们所说的额外250小时收集的数据具体意味着什么，目前还不清楚。然而，HDTF和VFHQ是公开可用的数据集，所以我们可以进一步分析它们。[HDTF](https://github.com/MRzzm/HDTF)包含了300个受试者的16小时720到1080p的视频数据。[VFHQ](https://liangbinxie.github.io/projects/vfhq/)没有明确提到数据集的小时数，但它有15,000个视频片段，数据量为1.2TB。如果我们假设每个视频片段平均至少有10秒钟，那么这相当于增加了40小时的数据。这意味着EMO使用了至少300小时的数据。对于VASA-1，微软表示：
- en: The model is trained on VoxCeleb2 [13] and another high-resolution talk video
    dataset collected by us, which contains about 3.5K subjects.
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 该模型在VoxCeleb2 [13]和我们收集的另一个高分辨率谈话视频数据集上进行训练，该数据集包含大约3.5K个受试者。
- en: Again, the authors are being secretive about a large part of the dataset. [VoxCeleb2](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html)
    is publicly available. Looking at the accompanying paper we can see this consists
    of 2442 hours of data (that’s not a typo) across 6000 subjects albeit in a lower
    resolution than the other datasets we mentioned (360–720p). This would be ~2TB.
    Microsoft uses a dataset of 3.5k additional subjects, which I suspect are far
    higher quality and allow the model to produce high-quality video. If we assume
    this is at least 1080p and some of it is 4k, with similar durations to VoxCeleb2
    then we can expect another 5–10TB.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，作者对于数据集的大部分内容保持保密。[VoxCeleb2](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html)是公开的。通过查看相关论文，我们可以看到它包含2442小时的数据（这不是笔误），跨越6000个受试者，尽管其分辨率低于我们提到的其他数据集（360到720p）。这大约是2TB。微软使用了一个包含3.5k个额外受试者的数据集，我怀疑这些受试者的质量更高，使得模型能够生成高质量的视频。如果我们假设这些视频至少是1080p，并且其中一些是4k分辨率，且与VoxCeleb2的数据集时长相似，那么我们可以预期另有5到10TB的数据。
- en: '**For the following, I am making some educated guesses:** Alibaba likely uses
    300 hours of high-quality video (1080p or higher), while Microsoft uses ~2500
    hours of low-quality video, and maybe somewhere between 100–1000 hours of very
    high-quality video. If we try to estimate the dataset size in terms of storage
    space we find that **EMO and VASA-1 each use ~10TB of face video data to train
    their model.** For some comparisons check out the following chart:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**接下来，我将做一些有根据的猜测：**阿里巴巴可能使用了300小时的高质量视频（1080p或更高），而微软使用了大约2500小时的低质量视频，可能还使用了100到1000小时的非常高质量视频。如果我们尝试估算数据集的存储空间大小，我们发现**EMO和VASA-1每个使用大约10TB的面部视频数据来训练他们的模型**。以下是一些比较，查看下表：'
- en: '![](../Images/7e37d00962bd5486f9dcece867f90acf.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e37d00962bd5486f9dcece867f90acf.png)'
- en: A comparison of the estimated dataset sizes of several state-of-the-art talking
    head generation models. Image by me. (CC-BY)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 几个最先进的谈话头像生成模型的数据集大小估算比较。图像来自我。(CC-BY)
- en: The Models
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型
- en: Both models make use of diffusion and transformers to utilise the massive datasets.
    However, there are some key differences in how they work.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个模型都利用扩散和变换器来处理海量数据集。然而，它们在工作原理上有一些关键差异。
- en: VASA-1
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: VASA-1
- en: We can break down VASA-1 into two components. One is an image formation model
    that takes some latent representation of the facial expression and pose and produces
    a video frame. The other is a model that generates these latent pose and expression
    vectors from audio input. The image formation model
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将VASA-1分解为两个组成部分。一个是图像生成模型，它接收一些面部表情和姿势的潜在表示，并生成一个视频帧。另一个是从音频输入生成这些潜在姿势和表情向量的模型。图像生成模型
- en: '![](../Images/ac93158b0a2f00186ca3a42c6efe0e9a.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac93158b0a2f00186ca3a42c6efe0e9a.png)'
- en: The VASA-1 model diagram. The left shows the audio-to-latent generation. The
    right shows the image formation model. [Diagram from the VASA-1 paper](https://www.microsoft.com/en-us/research/project/vasa-1/).
    (CC-BY)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: VASA-1模型图。左侧显示音频到潜在向量的生成，右侧显示图像生成模型。[VASA-1论文中的图示](https://www.microsoft.com/en-us/research/project/vasa-1/)。
    (CC-BY)
- en: Image Formation Model
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像生成模型
- en: VASA-1 relies heavily on a 3D volumetric representation of the face, building
    upon previous work from Samsung called [MegaPortraits.](https://arxiv.org/abs/2207.07621)
    The idea here is to first estimate a 3D representation of a source face, warp
    it using the predicted source pose, make edits to the expression using knowledge
    of both the source and target expressions in this canonical space, and then warp
    it back using a target pose.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: VASA-1在很大程度上依赖于面部的3D体积表示，建立在三星之前的工作[MegaPortraits](https://arxiv.org/abs/2207.07621)的基础上。这里的想法是首先估计源面部的3D表示，使用预测的源姿态对其进行变形，使用源和目标表情的知识对表情进行编辑，并在这个规范空间中完成编辑后，使用目标姿态将其重新变形。
- en: '![](../Images/af31766ad997f48e2dc3f43e8063036a.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af31766ad997f48e2dc3f43e8063036a.png)'
- en: Model Diagram of the MegaPortraits approach. v is a volumetric representation
    of the face, e is an identity later descriptor, R and t are the pose (rotation
    and translation) and z is the facial expression code. [From the MegaPortraits
    paper.](https://arxiv.org/pdf/2207.07621) (CC-BY)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: MegaPortraits方法的模型图。v是面部的体积表示，e是身份的描述符，R和t是姿态（旋转和位移），z是面部表情编码。[来自MegaPortraits论文。](https://arxiv.org/pdf/2207.07621)（CC-BY）
- en: 'In more detail, this process looks as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 更详细地说，这个过程如下：
- en: Take the source image (the man in the diagram above) and predict a simple 1D
    vector which represents this man.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取源图像（上图中的男子）并预测一个简单的1D向量，代表此人。
- en: Also predict a 4D tensor (Width, Height, Depth, RGB) as a volumetric representation
    of him.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同时预测一个4D张量（宽度、高度、深度、RGB），作为该人物的体积表示。
- en: Predict for both the source and driver (the woman above) their pose and facial
    expression. **Note that only the pose estimation is pre-trained, all the others
    are trained completely from scratch.**
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测源图像和驱动图像（上面的女人）的姿态和面部表情。**请注意，只有姿态估计是预训练的，其他所有内容都是从零开始训练的。**
- en: Create two warping fields using neural networks. One converts the man's volumetric
    representation into a canonical space **(this just means a front-facing, neutral
    expression)** using our estimate of his identity, pose and facial expression.
    The other converts his canonical 3D face into a posed 3D face using estimates
    of the woman’s pose and expression, as well as the man’s identity.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用神经网络创建两个变形场。一个使用我们对其身份、姿态和面部表情的估计，将男子的体积表示转换到规范空间**（这只是意味着正面、表情中立）**。另一个使用对女性姿态和表情的估计，以及男子的身份，将他的规范3D面部转换为具有姿态的3D面部。
- en: “Render” the posed man’s face back into 2D.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将有姿态的人脸“渲染”回2D图像。
- en: For details on how exactly they do this, that is how do you project into 3D,
    how is the warping achieved and how is the 2D image created from the 3D volume,
    please refer to the [MegaPortraits paper.](https://arxiv.org/abs/2207.07621)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 有关他们是如何具体实现这一点的，即如何将图像投影到3D空间，如何实现变形以及如何从3D体积中生成2D图像，请参阅[MegaPortraits论文。](https://arxiv.org/abs/2207.07621)
- en: This highly complex process can be simplified in our minds at this point to
    just imagine a model that encodes the source in some way and then takes parameters
    for pose and expression, creating an image based on these.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们可以将这个高度复杂的过程简化为：想象一个模型，以某种方式编码源输入，然后获取姿态和表情的参数，基于这些生成图像。
- en: Audio-to-latent Generation
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 音频到潜在空间生成
- en: 'We now have a way to generate video from a sequence of expressions and pose
    latent codes. However, unlike MegaPortraits, we don’t want to control our videos
    using another person’s expressions. Instead, we want control from audio alone.
    To do this, we need to build a generative model that takes audio as input and
    outputs latent vectors. This needs to scale up to huge amounts of data, have lip-sync
    and also produce diverse and plausible head motions. Enter the diffusion transformer.
    Not familiar with these models? I don’t blame you, there are a lot of advances
    here to keep up with. I can recommend the following article:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了一种从一系列表情和姿态潜在编码生成视频的方法。然而，与MegaPortraits不同，我们不希望通过他人的表情来控制我们的视频。相反，我们希望仅通过音频来控制。为此，我们需要构建一个生成模型，输入音频，输出潜在向量。这个模型需要能够处理大量数据，具备同步口型功能，并且能够生成多样化且逼真的头部动作。进入扩散transformer。如果你对这些模型不熟悉，我不怪你，这里有很多进展需要跟上。我可以推荐以下文章：
- en: '[](/diffusion-transformer-explained-e603c4770f7e?source=post_page-----0c571423f60f--------------------------------)
    [## Diffusion Transformer Explained'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/diffusion-transformer-explained-e603c4770f7e?source=post_page-----0c571423f60f--------------------------------)
    [## 扩散Transformer解释'
- en: Exploring the architecture that brought transformers into image generation
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索将transformers引入图像生成中的架构
- en: towardsdatascience.com](/diffusion-transformer-explained-e603c4770f7e?source=post_page-----0c571423f60f--------------------------------)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/diffusion-transformer-explained-e603c4770f7e?source=post_page-----0c571423f60f--------------------------------)
- en: But in a nutshell, diffusion transformers (DiTs) replace the conventional UNET
    in image-based diffusion models with a transformer. This switch enables learning
    on data with any structure, thanks to tokenization, and it is also known to scale
    extremely well to large datasets. For example, [OpenAI’s SORA](https://openai.com/index/sora/)
    model is believed to be a diffusion transformer.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，扩散变压器（DiTs）将传统的UNET替换为图像扩散模型中的变压器。这种切换使得能够在具有任何结构的数据上进行学习，得益于标记化，并且已知它在大数据集上扩展得非常好。例如，[OpenAI的SORA](https://openai.com/index/sora/)模型被认为是一个扩散变压器。
- en: '![](../Images/a05216dfd4de90fe35d5d34a65c23fc3.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a05216dfd4de90fe35d5d34a65c23fc3.png)'
- en: The model architecture of DiT from [Scalable Diffusion Models with Transformers](https://arxiv.org/pdf/2212.09748)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: DiT模型架构来自于[可扩展的扩散模型与变压器](https://arxiv.org/pdf/2212.09748)
- en: The idea then is to start from random noise in the same shape as the latent
    vectors and gradually denoise them to produce meaningful vectors. This process
    can then be conditioned on additional signals. For our purposes, this includes
    audio, extracted into feature vectors using [Wav2Vec2](https://huggingface.co/docs/transformers/en/model_doc/wav2vec2)
    (see [FaceFormer](https://evelynfan.github.io/audio2face/) for how exactly this
    works). Additional signals are also used. We won’t go into too much detail but
    they include eye gaze direction and emotion. To ensure temporal stability, the
    previously generated motion latent codes are also used as conditioning.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程的想法是从与潜在向量形状相同的随机噪声开始，逐步去噪以生成有意义的向量。然后，这个过程可以根据额外的信号进行条件化。对于我们的目的来说，这些信号包括音频，通过[Wav2Vec2](https://huggingface.co/docs/transformers/en/model_doc/wav2vec2)提取成特征向量（详细了解可以参考[FaceFormer](https://evelynfan.github.io/audio2face/)）。还使用了其他信号。我们不会详细讨论，但它们包括眼睛凝视方向和情绪。为了确保时间稳定性，先前生成的运动潜在编码也被用作条件信号。
- en: EMO
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: EMO
- en: '![](../Images/f2f08853244950e08272af55265967f1.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f2f08853244950e08272af55265967f1.png)'
- en: The model diagram from [the EMO paper.](https://humanaigc.github.io/emote-portrait-alive/)
    (CC-BY)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[EMO论文的模型图](https://humanaigc.github.io/emote-portrait-alive/)（CC-BY）
- en: EMO takes a slightly different approach with its generation process, though
    it still relies on diffusion at its core. The model diagram looks a bit crowded,
    so I think its best to break it into smaller parts.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: EMO在其生成过程中采用了稍微不同的方法，尽管它仍然依赖于扩散模型的核心。模型图看起来有些拥挤，因此我认为最好将其分解成更小的部分。
- en: Using Stable Diffusion
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Stable Diffusion
- en: The first thing to notice is that EMO makes heavy use of the pretrained Stable
    Diffusion 1.5 model. There is a solid trend in vision at large currently towards
    building on top of this model. In the above diagram, the reference net and the
    backbone network are both instances of the SD1.5 UNET archietecture and are initialised
    with these weights. The detail is lacking, but presumably the VAE encoder and
    decoder are also taken from Stable Diffusion. The VAE components are frozen, meaning
    that all of the operations performed in the EMO model are done in the latent space
    of that VAE. The use of the same archieture and same starting weights is useful
    because it allows activations from intermediate layers to be easily taken from
    one network and used in another (they will roughly represent the same thing in
    both network).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 首先需要注意的是，EMO大量使用了预训练的Stable Diffusion 1.5模型。目前，在视觉领域有一个明显的趋势，即在此模型基础上进行构建。在上面的图示中，参考网络和骨干网络都是SD1.5
    UNET架构的实例，并且是用这些权重进行初始化的。虽然细节不多，但可以推测VAE的编码器和解码器也是来自Stable Diffusion。VAE组件被冻结，这意味着EMO模型中执行的所有操作都是在该VAE的潜在空间中完成的。使用相同的架构和相同的初始权重非常有用，因为它允许从一个网络中轻松提取中间层的激活，并将其用于另一个网络（它们在两个网络中大致表示相同的内容）。
- en: Training the First Stage
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一阶段的训练
- en: The goal of the first stage is to get a single image model that can generate
    a novel image of a person, given a reference frame of that person. This is achieved
    using a diffusion model. A basic diffusion model could be used to generate random
    images of people. In stage one, we want to, in some way, condition this generation
    process on identity. The way the authors do this is by encoding a reference image
    of a person using the reference net, and introducing the activations in each layer
    into the backbone network which is doing the diffusion. See the (poorly drawn)
    diagram below.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 第一阶段的目标是获得一个单图像模型，能够根据该人物的参考帧生成一个新颖的图像。这是通过使用扩散模型来实现的。可以使用一个基本的扩散模型来生成人物的随机图像。在第一阶段，我们希望以某种方式将生成过程与身份条件化。作者实现这一点的方法是，通过使用参考网对人物的参考图像进行编码，并将每一层的激活信息引入到进行扩散的主干网络中。请参见下方的（画得很差的）示意图。
- en: '![](../Images/f82f071f0da2819e967ce8e4b8df2eac.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f82f071f0da2819e967ce8e4b8df2eac.png)'
- en: Basic Diagram of a simplified version of stage 1\. Image by me. (CC-BY)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 第一阶段简化版的基本示意图。图像由我制作。（CC-BY）
- en: At this stage, we now have a model that can generate random frames of a person,
    given a single image of that person. We now need to control it in some way.
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在这一阶段，我们已经有了一个模型，能够根据某个人的单张图片生成随机的帧。现在，我们需要以某种方式对其进行控制。
- en: Training the Second Stage
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练第二阶段
- en: We want to control the generated frames using two signals, motion and audio.
    The audio part is the easier to explain, so I’ll cover this first.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望使用两个信号来控制生成的帧：运动和音频。音频部分较容易解释，因此我将首先讲解这一部分。
- en: '![](../Images/b43d443ec51a2aecd5ff9910ac7cc6da.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b43d443ec51a2aecd5ff9910ac7cc6da.png)'
- en: The zoomed in backbone network of EMO. Taken from [the EMO paper](https://humanaigc.github.io/emote-portrait-alive/).
    (CC-BY)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: EMO的主干网络放大图。来自[EMO论文](https://humanaigc.github.io/emote-portrait-alive/)。 (CC-BY)
- en: '**Audio:** As with VASA-1, audio is encoded in the form of wav2vec2 features.
    These are incorporated into the backbone network using cross attention. This cross
    attention replaces the text prompt cross attention already present in the Stable
    Diffusion 1.5 model.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**音频：** 和VASA-1一样，音频以wav2vec2特征的形式进行编码。这些特征通过交叉注意力机制被融入主干网络中。这种交叉注意力替代了Stable
    Diffusion 1.5模型中已经存在的文本提示交叉注意力。'
- en: '**Motion:** Motion is added using motion frames, to predict the frame at time
    t, the previous n frames are used to provide context for the motion. The motion
    frames are encoded in the same way as the reference frame. The intermediate feature
    activations of the reference net are used to condition the backbone model. The
    inclusion of these motion referenece activations is done using a specially designed
    cross-attention layer, taken from [AnimateDiff](https://animatediff.github.io/).
    From these n frames, the next f are predicted using the diffusion model.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运动：** 通过运动帧添加运动信息，在预测时间 t 的帧时，前 n 帧提供了运动的上下文。运动帧与参考帧以相同的方式进行编码。参考网的中间特征激活用于条件化主干模型。这些运动参考激活的引入是通过一个专门设计的交叉注意力层完成的，来自[AnimateDiff](https://animatediff.github.io/)。从这些
    n 帧中，下一帧 f 会通过扩散模型进行预测。'
- en: In addition to this, two other components are used. One provides a mask, taken
    as the union of all bounding boxes across the training video. This mask defines
    what region of the video is allowed to be changed. The other is a small addition
    of a speed condition is used. The pose velocity is divided into buckets (think
    slow, medium, fast) and also included. This allows us to specify the speed of
    the motion at inference time.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，还使用了两个其他组件。一个提供了一个掩码，掩码是通过对训练视频中的所有边界框取并集得到的。这个掩码定义了视频中可以被修改的区域。另一个是添加了一个速度条件。姿势速度被划分为几个桶（例如：慢速、中速、快速），并也包含在内。这使得我们能够在推理时指定运动的速度。
- en: Inference
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理
- en: 'The model is now able to take the following and produces a new set of frames:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型能够接受以下内容并生成一组新的帧：
- en: A reference frame
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考帧
- en: The previous n frames
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 之前的 n 帧
- en: The audio
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 音频
- en: The head motion speed
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 头部运动速度
- en: A bounding box of pixels that can be changed
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以被改变的像素边界框
- en: For the first frame, it is not stated, but I assume the reference frame is repeated
    and passed as the last n frames. After this point, the model is autoregressive,
    the outputs are then used as the previous frames for input.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一帧，虽然没有明确说明，但我假设参考帧会被重复，并作为最后 n 帧传递。在这一点之后，模型是自回归的，输出将作为前一帧输入。
- en: Ethics Discussion
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 伦理讨论
- en: The ethical implications of these works are, of course, very significant. They
    require only a single image in order to create very realistic synthetic content.
    This could easily be used to misrepresent people. Given the recent controversy
    surrounding [OpenAI’s use of a voice that sounds suspiciously like Scarlett Johansen
    without her consent](https://edition.cnn.com/2024/05/22/tech/openai-scarlett-johansson-lawsuit-sam-altman/index.html),
    the issue is particularly relevant at the moment. The two groups take rather different
    approaches.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工作的伦理影响当然非常重要。它们只需要一张图片就能创造出非常真实的合成内容。这很容易被用来歪曲他人形象。鉴于近期关于[OpenAI未经Scarlett
    Johansen同意使用听起来非常像她声音的事件](https://edition.cnn.com/2024/05/22/tech/openai-scarlett-johansson-lawsuit-sam-altman/index.html)，这一问题尤为相关。两组的处理方式截然不同。
- en: '**EMO**'
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**EMO**'
- en: 'The discussion in the EMO paper is very much lacking. The paper does not include
    any discussion of the ethical implications or any proposed methods of preventing
    misuse. The project page says only:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: EMO论文中的讨论非常欠缺。该论文没有讨论伦理影响或提出任何防止滥用的方法。项目页面仅写道：
- en: “This project is intended solely for academic research and effect demonstration”
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “该项目仅用于学术研究和效果展示”
- en: This seems like a very weak attempt. Furthermore, Alibaba include a GitHub repo
    which (may) make the code publicly available. It’s important to consider the pros
    and cons of doing this, [as we discuss in a previous article.](https://medium.com/towards-data-science/should-deepfakes-be-open-sourced-87d7644a0765)
    Overall, the EMO authors have not shown too much consideration for ethics.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这似乎是一次非常弱的尝试。此外，阿里巴巴还提供了一个GitHub仓库，可能会公开代码。考虑这样做的利弊是很重要的，[正如我们在上一篇文章中讨论的那样。](https://medium.com/towards-data-science/should-deepfakes-be-open-sourced-87d7644a0765)
    总体来看，EMO的作者并没有对伦理问题给予太多考虑。
- en: '**VASA-1**'
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**VASA-1**'
- en: VASA-1’s authors take a more comprehensive approach to preventing misuse. They
    include a section in the paper dedicated to this, highlighting the potential uses
    in deepfake detection as well as the positive benefits.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: VASA-1的作者采取了更全面的方法来防止滥用。他们在论文中专门有一节讨论此问题，强调了深伪检测中的潜在应用以及正面效益。
- en: '![](../Images/9470848109e9c0de02bb9ab05b1ff523.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9470848109e9c0de02bb9ab05b1ff523.png)'
- en: The ethics section from VASA-1\. Image take from the [arxiv preprint](https://arxiv.org/pdf/2404.10667).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 来自VASA-1的伦理部分。图片取自[arxiv预印本](https://arxiv.org/pdf/2404.10667)。
- en: 'In addition to this, they also include a rather interesting statement:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，他们还包含了一项相当有趣的声明：
- en: '*Note: all portrait images on this page are virtual, non-existing identities
    generated by StyleGAN2 or DALL·E-3 (except for Mona Lisa). We are exploring visual
    affective skill generation for virtual, interactive characters, NOT impersonating
    any person in the real world. This is only a research demonstration and there’s
    no product or API release plan.*'
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*注意：本页上的所有人像图像都是由StyleGAN2或DALL·E-3生成的虚拟、非真实身份（蒙娜丽莎除外）。我们正在探索虚拟互动角色的视觉情感技能生成，而非模仿任何现实世界中的人物。这仅仅是一个研究示范，并没有产品或API发布计划。*'
- en: The approach is actually one Microsoft have started to take in a few papers.
    They only create synthetic videos using synthetic people and do not release any
    of their models. Doing so prevents any possible misuse, as no real people are
    edited. However, it does raise issues around the fact that the power to create
    such videos in concentrated into the arms of big-tech companies that have the
    infrastructure to train such models.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法实际上是微软在一些论文中开始采用的。他们只使用合成人物来创建合成视频，并且不公开任何模型。这样做可以防止可能的滥用，因为没有编辑任何真实人物。然而，这也引发了一个问题：创造此类视频的能力集中在具有基础设施训练这些模型的大型科技公司手中。
- en: Further Analysis
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步分析
- en: In my opinion this line of work opens up a new set of ethical issues. While
    it had been previously possible to create fake videos of people, it usually required
    several minutes of data to train a model. This largely restricted the potential
    victims to people who create lots of video already. While this allowed for the
    creation of political misinformation, the limitations helped to stifle some other
    applications. For one, if someone creates a lot of video, it is possible to tell
    what their usual content looks like (what do they usually talk about, what their
    opinions are, etc.) and can learn to spot videos that are uncharacteristic. This
    becomes more difficult if a single image can be used. What’s more, anyone can
    become a victim of these models. Even a social media account with a profile picture
    would be enough data to build a model of a person.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在我看来，这项工作开启了一系列新的伦理问题。虽然以前可以创建假视频，但通常需要几分钟的数据来训练模型。这在很大程度上将潜在的受害者限制为已经创造大量视频的人。尽管这为制造政治虚假信息提供了可能，但这些限制帮助抑制了一些其他应用。例如，如果某人创建了大量视频，就可以通过分析其常见内容（他们通常谈论什么，持有什么观点等）来辨别出不符合其风格的视频。如果只使用一张图片，这变得更加困难。而且，任何人都可能成为这些模型的受害者。即使是一个拥有个人头像的社交媒体账户，也足够作为建立个人模型的数据。
- en: Furthermore, as a different class of “deepfake” there is not much research on
    how to detect these models. Methods that may have worked to catch video deepfake
    models would become unreliable.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，作为一种不同类型的“深度伪造”，目前对如何检测这些模型的研究并不多。以前可能有效的用于捕捉视频深度伪造模型的方法将变得不可靠。
- en: We need to ensure that the harm caused by these models is limited. Microsoft’s
    approach of limiting access and only using synthetic people helps for the short
    term. But long term we need robust regulation of the applications of these models,
    as well as reliable methods to detect content generated by them.
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们需要确保这些模型带来的危害得到限制。微软通过限制访问权限并仅使用合成人物的做法在短期内有所帮助。但从长远来看，我们需要对这些模型的应用进行强有力的监管，并且需要可靠的方法来检测它们生成的内容。
- en: Conclusion
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'Both VASA-1 and EMO are incredible papers. They both exploit diffusion models
    and large-scale datasets to produce extremely high quality video from audio and
    a single image. A few key points stand out to me:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: VASA-1和EMO都是非常出色的论文。它们都利用扩散模型和大规模数据集，从音频和一张单独的图片生成极高质量的视频。有几点关键内容让我印象深刻：
- en: It’s not quite a case of scale is all you need. Both models use clever tricks
    (VASA-1’s use of MegaPortiats and EMO’s reference & backbone nets). However, it
    does seem to be the case that **“scale is something you need”.**
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这不完全是“规模就是一切”的情况。两个模型都使用了巧妙的技巧（VASA-1使用MegaPortiats，EMO使用参考和主干网络）。然而，似乎确实存在**“规模是你需要的东西”**的情况。
- en: '**Diffusion is king.** Both of these models, as well as most state-of-the-art
    generation in vision, use diffusion. It seems that VAE’s and GAN’s are almost
    truly dead.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扩散是王道。** 这两个模型，以及大多数最先进的视觉生成模型，都使用了扩散方法。看起来变分自编码器（VAE）和生成对抗网络（GAN）几乎已经完全过时。'
- en: The field of lip-sync models is likely to become the domain of the big companies
    only soon. If trends continue, **there is no way academics will be able to build
    models** that keep up with these.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嘴唇同步模型的领域可能很快就会成为大公司的专属。如果趋势继续下去，**学术界将无法建立能够与之竞争的模型**。
