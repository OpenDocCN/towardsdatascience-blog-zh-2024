- en: 'Examining Longterm Machine Learning through ELLA and Voyager: Part 2 of Why
    LLML is the Next Game-changer of AI'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过ELLA和VOYAGER研究长期机器学习：为何LLML是AI领域下一次革命性突破的第二部分
- en: 原文：[https://towardsdatascience.com/examining-lifelong-machine-learning-through-ella-and-voyager-part-2-of-why-llml-is-next-in-ai-bea36a01f529?source=collection_archive---------14-----------------------#2024-01-17](https://towardsdatascience.com/examining-lifelong-machine-learning-through-ella-and-voyager-part-2-of-why-llml-is-next-in-ai-bea36a01f529?source=collection_archive---------14-----------------------#2024-01-17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/examining-lifelong-machine-learning-through-ella-and-voyager-part-2-of-why-llml-is-next-in-ai-bea36a01f529?source=collection_archive---------14-----------------------#2024-01-17](https://towardsdatascience.com/examining-lifelong-machine-learning-through-ella-and-voyager-part-2-of-why-llml-is-next-in-ai-bea36a01f529?source=collection_archive---------14-----------------------#2024-01-17)
- en: Understanding the power of Lifelong Learning through the Efficient Lifelong
    Learning Algorithm (ELLA) and VOYAGER
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过高效终身学习算法（ELLA）和VOYAGER理解终身学习的力量
- en: '[](https://medium.com/@almond.maj?source=post_page---byline--bea36a01f529--------------------------------)[![Anand
    Majmudar](../Images/4840cb28e81326221cebef9f540c8e12.png)](https://medium.com/@almond.maj?source=post_page---byline--bea36a01f529--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--bea36a01f529--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--bea36a01f529--------------------------------)
    [Anand Majmudar](https://medium.com/@almond.maj?source=post_page---byline--bea36a01f529--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@almond.maj?source=post_page---byline--bea36a01f529--------------------------------)[![Anand
    Majmudar](../Images/4840cb28e81326221cebef9f540c8e12.png)](https://medium.com/@almond.maj?source=post_page---byline--bea36a01f529--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--bea36a01f529--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--bea36a01f529--------------------------------)
    [Anand Majmudar](https://medium.com/@almond.maj?source=post_page---byline--bea36a01f529--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--bea36a01f529--------------------------------)
    ·9 min read·Jan 17, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--bea36a01f529--------------------------------)
    ·阅读时间：9分钟·2024年1月17日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/c09ee08b56bbe0e76c45cfe8155a8a3e.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c09ee08b56bbe0e76c45cfe8155a8a3e.png)'
- en: AI Robot Piloting Space Vessel, Generated with GPT-4
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: AI机器人驾驶太空飞船，由GPT-4生成
- en: 'I encourage you to read [Part 1: The Origins of LLML](https://medium.com/@almond.maj/the-origins-of-lifelong-ml-part-1-of-why-llml-is-the-next-game-changer-of-ai-8dacf9897143)
    if you haven’t already, where we saw the use of LLML in reinforcement learning.
    Now that we’ve covered where LLML came from, we can apply it to other areas, specifically
    supervised multi-task learning, to see some of LLML’s true power.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有阅读，[第一部分：LLML的起源](https://medium.com/@almond.maj/the-origins-of-lifelong-ml-part-1-of-why-llml-is-the-next-game-changer-of-ai-8dacf9897143)我鼓励你先读一读，在那里我们探讨了LLML在强化学习中的应用。现在，既然我们已经了解了LLML的起源，我们可以将其应用于其他领域，特别是监督学习中的多任务学习，来展示LLML的一些真正的力量。
- en: '**Supervised LLML: The Efficient Lifelong Learning Algorithm**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**监督型LLML：高效终身学习算法**'
- en: 'The Efficient Lifelong Learning Algorithm aims to train a model that will excel
    at multiple tasks at once. ELLA operates in the multi-task supervised learning
    setting, with multiple tasks T_1..T_n, with features X_1..X_n and y_1…y_n corresponding
    to each task(the dimensions of which likely vary between tasks). Our goal is to
    learn functions f_1,.., f_n where f_1: X_1 -> y_1\. Essentially, each task has
    a function that takes as input the task’s corresponding features and outputs its
    y values.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '高效终身学习算法旨在训练一个能够同时在多个任务上表现出色的模型。ELLA操作于多任务监督学习环境中，具有多个任务T_1..T_n，每个任务有对应的特征X_1..X_n和y_1…y_n（这些任务的维度可能不同）。我们的目标是学习函数f_1,..,
    f_n，其中f_1: X_1 -> y_1。基本上，每个任务都有一个函数，该函数将任务对应的特征作为输入，并输出其y值。'
- en: On a high level, ELLA maintains a shared basis of ‘knowledge’ vectors for all
    tasks, and as new tasks are encountered, ELLA uses knowledge from the basis refined
    with the data from the new task. Moreover, in learning this new task, more information
    is added to the basis, improving learning for all future tasks!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，ELLA为所有任务保持一个共享的“知识”向量基础，当遇到新任务时，ELLA利用来自新任务数据的知识来优化基础。此外，在学习这个新任务时，更多的信息被加入到这个基础中，从而提高所有未来任务的学习效果！
- en: 'Ruvolo and Eaton used ELLA in three settings: landmine detection, facial expression
    recognition, and exam score predictions! As a little taste to get you excited
    about ELLA’s power, it achieved up to a 1,000x more time-efficient algorithm on
    these datasets, sacrificing next to no performance capabilities!'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Ruvolo和Eaton在三个场景中使用了ELLA：地雷探测、面部表情识别和考试成绩预测！作为一个小小的预告，让你对ELLA的强大功能产生兴趣，它在这些数据集上实现了高达1,000倍的时间效率提升，几乎没有牺牲性能能力！
- en: Now, let’s dive into the technical details of ELLA! The first question that
    might arise when trying to derive such an algorithm is
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入探讨ELLA的技术细节！当尝试推导这样一个算法时，第一个可能出现的问题是
- en: '*How exactly do we find what information in our knowledge base is relevant
    to each task?*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们到底如何找到在知识库中与每个任务相关的信息？*'
- en: ELLA does so by modifying our f functions for each t. Instead of being a function
    f(x) = y, we now have f(x, θ_t) = y where θ_t is unique to task t, and can be
    represented by a linear combination of the knowledge base vectors. With this system,
    we now have all tasks mapped out in the *same* basis dimension, and can measure
    similarity using simple linear distance!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ELLA通过修改每个t的f函数来实现这一点。它不再是一个函数f(x) = y，而是f(x, θ_t) = y，其中θ_t是特定于任务t的，可以通过知识库向量的线性组合表示。通过这种系统，我们现在将所有任务映射到*相同*的基准维度，并可以使用简单的线性距离来衡量相似性！
- en: '*Now, how do we derive θ_t for each task?*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*现在，我们如何为每个任务推导θ_t？*'
- en: This question is the core insight of the ELLA algorithm, so let’s take a detailed
    look at it. We represent knowledge basis vectors as matrix L. Given weight vectors
    s_t, we represent each θ_t as Ls_t, the linear combination of basis vectors.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题是ELLA算法的核心洞察力，所以让我们详细看看它。我们将知识基向量表示为矩阵L。给定权重向量s_t，我们将每个θ_t表示为Ls_t，即基向量的线性组合。
- en: 'Our goal is to minimize the loss for each task while maximizing the shared
    information used between tasks. We do so with the objective function e_T we are
    trying to minimize:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是最小化每个任务的损失，同时最大化任务之间共享的信息。我们通过最小化目标函数e_T来实现这一点：
- en: '![](../Images/cc42779f3a383464b5c39bdb564c1a36.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc42779f3a383464b5c39bdb564c1a36.png)'
- en: Where ℓ is our chosen loss function.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 其中ℓ是我们选择的损失函数。
- en: Essentially, the first clause accounts for our task-specific loss, the second
    tries to minimize our weight vectors and make them sparse, and our last clause
    tries to minimize our basis vectors.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，第一个子句考虑了我们特定任务的损失，第二个子句试图最小化我们的权重向量并使其稀疏，最后一个子句试图最小化我们的基向量。
- en: '**This equation carries two inefficiencies (see if you can figure out what)!
    Our first is that our equation depends on all previous training data, (specifically
    the inner sum), which we can imagine is incredibly cumbersome. We alleviate this
    first inefficiency using a Taylor sum of approximation of the equation. Our second
    inefficiency is that we need to recompute every s_t to evaluate one instance of
    L. We eliminate this inefficiency by removing our minimization over z and instead
    computing s when t is last interacted with. I encourage you to read the original
    paper for a more detailed explanation!**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**这个方程有两个低效之处（看看你是否能找出是什么）！第一个低效之处是我们的方程依赖于所有之前的训练数据（具体来说是内层求和），我们可以想象这非常繁琐。我们通过使用泰勒级数近似来缓解这个低效问题。第二个低效之处是我们需要重新计算每一个s_t来评估L的一个实例。我们通过移除z上的最小化，并改为在t最后一次交互时计算s，从而消除了这个低效问题。我鼓励你阅读原始论文以获得更详细的解释！**'
- en: Now that we have our objective function, we want to create a method to optimize
    it!
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了目标函数，我们希望创建一种方法来优化它！
- en: In training, we’re going to treat each iteration as a unit where we receive
    a batch of training data from a single task, then compute s_t, and finally update
    L. At the start of our algorithm, we set T (our number-of-tasks counter), A, b,
    and L to zeros. Now, for each batch of data, we case based on the data is from
    a seen or unseen task.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们将每次迭代视为一个单元，其中我们从单个任务接收一批训练数据，然后计算s_t，最后更新L。在算法的开始，我们将T（任务计数器）、A、b和L初始化为零。现在，对于每批数据，我们根据数据是来自已知任务还是未知任务来进行分类处理。
- en: If we encounter data from a new task, we will add 1 to T, and initialize X_t
    and y_t for this new task, setting them equal to our current batch of X and y..
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们遇到来自新任务的数据，我们将T加1，并为这个新任务初始化X_t和y_t，将它们设置为我们当前的X和y批次。
- en: If we encounter data we’ve already seen, our process gets more complex. We again
    add our new X and y to add our new X and y to our current memory of X_t and y_t
    (by running through all data, we will have a complete set of X and y for each
    task!). We also incrementally update our A and b values negatively (I’ll explain
    this later, just remember this for now!).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们遇到已经见过的数据，我们的过程变得更加复杂。我们再次将新的X和y添加到我们当前的X_t和y_t的记忆中（通过遍历所有数据，我们将为每个任务拥有一完整的X和y集合！）。我们还会递增地更新A和b的值（我稍后会解释这一点，现在先记住这一点！）。
- en: Now we check if we want to end our training loop. We set our (θ_t, D_t) equal
    to the output of our regular learner for our batch data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们检查是否想要结束训练循环。我们将(θ_t, D_t)设置为我们常规学习器对批量数据的输出。
- en: We then check to end the loop (if we have seen all training data). If we haven’t
    ended, we move on to computing s and updating L.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们检查是否结束循环（如果我们已经看过所有训练数据）。如果没有结束，我们继续计算s并更新L。
- en: To compute s, we first compute optimal model \theta_t using only the batched
    data, which will depend on our specific task and loss function.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算s，我们首先仅使用批量数据来计算最优模型θ_t，这将依赖于我们的具体任务和损失函数。
- en: We then compute D_t, and either randomly or to one of the θ_ts initialize any
    all-zero columns of L (which occurs if a certain basis vector is unused). In linear
    regression,
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们计算D_t，并且随机或选择一个θ_t来初始化L的所有零列（如果某个基础向量未被使用，则会发生这种情况）。在线性回归中，
- en: '![](../Images/05c33698796d74a6ff1226a78f978d22.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05c33698796d74a6ff1226a78f978d22.png)'
- en: and in logistic regression
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 而在逻辑回归中
- en: '![](../Images/f28c7b108859223dbf7f62641745df10.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f28c7b108859223dbf7f62641745df10.png)'
- en: 'Then, we compute s_t using L by solving an L1-regularized regression problem:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过求解L1正则化回归问题来使用L计算s_t：
- en: '![](../Images/b28b2fbc7d15507827d04f6a00a6f1b6.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b28b2fbc7d15507827d04f6a00a6f1b6.png)'
- en: For our final step of updating L, we take
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们更新L的最后一步时，我们取
- en: '![](../Images/2ce12fb8bdf1a57f5f3b90f9e486aa51.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ce12fb8bdf1a57f5f3b90f9e486aa51.png)'
- en: ', find where the gradient is 0, then solve for L. By doing so, we increase
    the sparsity of L! We then output the updated columnwise-vectorization of L as'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ，找到梯度为0的地方，然后解L。通过这样做，我们增加了L的稀疏性！然后我们输出L的更新列向量化形式为
- en: '![](../Images/a405e3df2719964823a1c9505d48e227.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a405e3df2719964823a1c9505d48e227.png)'
- en: so as not to sum over all tasks to compute A and b, we construct them incrementally
    as each task arrives.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免对所有任务求和以计算A和b，我们在每个任务到来时逐步构建它们。
- en: Once we’ve iterated through all batch data, we’ve learned all tasks properly
    and have finished!
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们遍历完所有批次数据，就意味着我们已经正确学习了所有任务并完成了！
- en: The power of ELLA lies in many of its efficiency optimizations, primarily of
    which is its method of using θ functions to understand exactly what basis knowledge
    is useful! If you care about a more in-depth understanding of ELLA, I highly encourage
    you to check out the pseudocode and explanation in the [original paper](https://proceedings.mlr.press/v28/ruvolo13.pdf).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ELLA的强大之处在于它的许多效率优化，其中最主要的是它使用θ函数来准确理解哪些基础知识是有用的！如果你对ELLA有更深入的了解兴趣，我强烈建议你查看[原始论文](https://proceedings.mlr.press/v28/ruvolo13.pdf)中的伪代码和解释。
- en: Using ELLA as a base, we can imagine creating a generalizable AI, which can
    learn any task it’s presented with. We again have the property that the more our
    knowledge basis grows, the more ‘relevant information’ it contains, which will
    even further increase the speed of learning new tasks! It seems as if ELLA could
    be the core of one of the super-intelligent artificial learners of the future!
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ELLA作为基础，我们可以设想创建一个具有普遍性的人工智能，它可以学习任何呈现给它的任务。我们再次具有这样一个特性：随着我们的知识基础的增长，它所包含的‘相关信息’也越来越多，这将进一步加速学习新任务的速度！看起来，ELLA有可能成为未来超级智能人工学习者的核心！
- en: '**Voyager**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**旅行者**'
- en: What happens when we integrate the newest leap in AI, LLMs, with Lifelong ML?
    We get something that can beat Minecraft (This is the setting of the actual paper)!
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将人工智能的新突破LLM与终身学习结合时，会发生什么呢？我们得到的是一个能够战胜Minecraft的系统（这是实际论文的设置）！
- en: Guanzhi Wang, Yuqi Xie, and others saw the new opportunity offered by the power
    of GPT-4, and decided to combine it with ideas from lifelong learning you’ve learned
    so far to create Voyager.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 王冠志、谢宇琪等人看到了GPT-4的强大能力所带来的新机遇，并决定将其与迄今为止学到的终身学习理念结合，创造出Voyager。
- en: When it comes to learning games, typical algorithms are given predefined final
    goals and checkpoints for which they exist solely to pursue. In open-world games
    like Minecraft, however, there are many possible goals to pursue and an infinite
    amount of space to explore. What if our goal is to approximate human-like self-motivation
    combined with increased time efficiency in traditional Minecraft benchmarks, such
    as getting a diamond? Specifically, let’s say we want our agent to be able to
    decide on feasible, interesting tasks, learn and remember skills, and continue
    to explore and seek new goals in a ‘self-motivated’ way.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习游戏中，典型的算法会被赋予预定义的最终目标和检查点，存在的唯一目的就是追求这些目标。然而，在像 Minecraft 这样的开放世界游戏中，有许多可能的目标可以追求，并且有无限的空间可以探索。如果我们的目标是近似于人类般的自我激励，并结合传统
    Minecraft 基准测试中的时间效率提升，比如获取钻石？具体来说，假设我们希望我们的代理能够决定可行的、有趣的任务，学习并记住技能，并以“自我激励”的方式继续探索和寻求新目标。
- en: Towards these goals, Wang, Xie, and others created [Voyager](https://arxiv.org/pdf/2305.16291.pdf),
    which they called the first LLM-powered embodied lifelong learning agent!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这些目标，王、谢等人创建了 [Voyager](https://arxiv.org/pdf/2305.16291.pdf)，他们称之为首个基于
    LLM 的具身终身学习代理！
- en: '*How does Voyager work?*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*Voyager 是如何工作的？*'
- en: 'On a large-scale, Voyager uses GPT-4 as its main ‘intelligence function’ and
    the model itself can be separated into three parts:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模应用中，Voyager 使用 GPT-4 作为其主要的“智能功能”，该模型本身可以分为三个部分：
- en: '**Automatic curriculum:** This decides which goals to pursue, and can be thought
    of as the model’s “motivator”. Implemented with GPT-4, they instructed it to optimize
    for difficult yet feasible goals and to “discover as many diverse things as possible”
    (read the original paper to see their exact prompts). If we pass four rounds of
    our iterative prompting mechanism loop without the agent’s environment changing,
    we simply choose a new task!'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**自动化课程：** 这决定了要追求的目标，可以看作是模型的“动机”。通过 GPT-4 实现，他们指示模型优化难度较大但可行的目标，并“尽可能多地发现不同的事物”（可以阅读原文了解他们的具体提示）。如果我们在四轮迭代提示机制循环中，代理的环境没有发生变化，我们就选择一个新的任务！'
- en: '**Skill library:** a collection of executable actions such as craftStoneSword()
    or getWool() which increase in difficulty as the learner explores. This skill
    library is represented as a vector database, where keys are embedding vectors
    of GPT-3.5-generated skill descriptions, and executable skills in code form. GPT-4
    generated the code for the skills, optimized for generalizability and refined
    by feedback from the use of the skill in the agent’s environment!'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**技能库：** 一个包含可执行动作的集合，如 craftStoneSword() 或 getWool()，随着学习者的探索而逐渐增加难度。这个技能库以向量数据库的形式表示，其中键是
    GPT-3.5 生成的技能描述的嵌入向量，以及以代码形式表示的可执行技能。GPT-4 生成了技能的代码，优化了其通用性，并通过在代理环境中使用技能的反馈进行了优化！'
- en: '**Iterative prompting mechanism:** This is the element that interacts with
    the Minecraft environment. It first executes its’ interface of Minecraft to gain
    information about its current environment, for example, the items in its inventory
    and the surrounding creatures it can observe. It then prompts GPT-4 and performs
    the actions specified in the output, also offering feedback about whether the
    actions specified are impossible. This repeats until the current task (as decided
    by the automatic curriculum) is completed. At completion, we add the learned skill
    to the skill library. For example, if our task was create a stone sword, we now
    put the skill craftStoneSword() into our skill library. Finally, we ask the automatic
    curriculum for a new goal.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**迭代提示机制：** 这是与 Minecraft 环境交互的元素。它首先执行 Minecraft 的接口，获取当前环境的信息，例如它的背包中的物品和它能观察到的周围生物。然后它提示
    GPT-4 并执行输出中指定的动作，同时提供有关指定动作是否不可能的反馈。这个过程会重复，直到当前任务（由自动化课程决定）完成。完成后，我们将学到的技能添加到技能库中。例如，如果我们的任务是制作一把石剑，我们现在就将技能
    craftStoneSword() 加入到技能库中。最后，我们向自动化课程请求一个新的目标。'
- en: '*Now, where does Lifelong Learning fit into all this?*'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*那么，终身学习在这一切中扮演什么角色？*'
- en: When we encounter a new task, we query our skill database to find the top 5
    most relevant skills to the task at hand (for example, relevant skills for the
    task getDiamonds() would be craftIronPickaxe() and findCave().
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们遇到新任务时，我们查询我们的技能数据库，以找到与当前任务最相关的前 5 个技能（例如，任务 getDiamonds() 的相关技能可能是 craftIronPickaxe()
    和 findCave()）。
- en: 'Thus, we’ve used previous tasks to learn our new task more efficiently: the
    essence of lifelong learning! Through this method, Voyager continuously explores
    and grows, learning new skills that increase its frontier of possibilities, increasing
    the scale of ambition of its goals, thus increasing the powers of its newly learned
    skills, continuously!'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经利用先前的任务来更高效地学习新的任务：这就是终身学习的本质！通过这种方法，Voyager不断探索和成长，学习新的技能，拓展它的可能性边界，增加目标的雄心壮志，从而持续提高新学习技能的能力！
- en: Compared with other models like AutoGPT, ReAct, and Reflexion, Voyager discovered
    3.3x as many new items as these others, navigated distances 2.3x longer, unlocked
    wooden level 15.3x faster per prompt iteration, and was the only one to unlock
    the diamond level of the tech tree! Moreover, after training, when dropped in
    a completely new environment with no items, Voyager consistently solved prior-unseen
    tasks, while others could not solve any within 50 prompts.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 与AutoGPT、ReAct和Reflexion等其他模型相比，Voyager发现的新项目是这些模型的3.3倍，导航距离是它们的2.3倍，每次提示迭代解锁木质级别的速度是它们的15.3倍，而且是唯一一个解锁了技术树钻石级别的模型！此外，在训练之后，当被放入一个完全陌生、没有任何物品的环境时，Voyager始终能够解决以前未见过的任务，而其他模型在50次提示内都无法解决任何任务。
- en: As a display of the importance of Lifelong Learning, without the skill library,
    the model’s progress in learning new tasks plateaued after 125 iterations, whereas
    with the skill library, it kept rising at the same high rate!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示终身学习的重要性，没有技能库的情况下，模型在学习新任务时的进展在125次迭代后停滞不前，而有了技能库，它的进展以相同的高速度持续上升！
- en: Now imagine this agent applied to the real world! Imagine a learner with infinite
    time and infinite motivation that could keep increasing its possibility frontier,
    learning faster and faster the more prior knowledge it has! I hope by now I’ve
    properly illustrated the power of Lifelong Machine Learning and its capability
    to prompt the next transformation of AI!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在想象这个代理应用于现实世界！想象一个拥有无限时间和无限动力的学习者，随着拥有的先验知识越来越多，它能够不断拓展自己的可能性边界，学习得越来越快！我希望到现在为止，我已经充分展示了终身机器学习的力量以及它推动AI下一次变革的能力！
- en: If you’re interested further in LLML, I encourage you to read Zhiyuan Chen and
    Bing Liu’s [book](https://www.cs.uic.edu/~liub/lifelong-machine-learning-draft.pdf)
    which lays out the potential future paths LLML might take!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对LLML更感兴趣，我鼓励你阅读Zhiyuan Chen和Bing Liu的[书籍](https://www.cs.uic.edu/~liub/lifelong-machine-learning-draft.pdf)，它阐述了LLML可能走向的未来路径！
- en: Thank you for making it all the way here! If you’re interested, check out my
    website anandmaj.com which has my other writing, projects, and art, and follow
    me on Twitter @almondgodd.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你一直看到这里！如果你感兴趣，可以访问我的网站anandmaj.com，那里有我的其他写作、项目和艺术作品，也可以在Twitter上关注我@almondgodd。
- en: '**Original Papers and other Sources:**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**原始论文及其他资料：**'
- en: 'Eaton and Ruvolo: [Efficient Lifelong Learning Algorithm](https://www.seas.upenn.edu/~eeaton/papers/Ruvolo2013ELLA.pdf)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Eaton和Ruvolo：[高效的终身学习算法](https://www.seas.upenn.edu/~eeaton/papers/Ruvolo2013ELLA.pdf)
- en: 'Wang, Xie, et al: [Voyager](https://arxiv.org/pdf/2305.16291.pdf)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Wang、Xie等：[Voyager](https://arxiv.org/pdf/2305.16291.pdf)
- en: 'Chen and Liu, Lifelong Machine Learning (Inspired me to write this!): [https://www.cs.uic.edu/~liub/lifelong-machine-learning-draft.pdf](https://www.cs.uic.edu/~liub/lifelong-machine-learning-draft.pdf)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Chen和Liu，《终身机器学习》（启发我写这篇文章！）：[https://www.cs.uic.edu/~liub/lifelong-machine-learning-draft.pdf](https://www.cs.uic.edu/~liub/lifelong-machine-learning-draft.pdf)
- en: 'Unsupervised LL with Curricula: [https://par.nsf.gov/servlets/purl/10310051](https://par.nsf.gov/servlets/purl/10310051)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 使用课程的无监督终身学习：[https://par.nsf.gov/servlets/purl/10310051](https://par.nsf.gov/servlets/purl/10310051)
- en: 'Deep LL: [https://towardsdatascience.com/deep-lifelong-learning-drawing-inspiration-from-the-human-brain-c4518a2f4fb9](/deep-lifelong-learning-drawing-inspiration-from-the-human-brain-c4518a2f4fb9)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 深度终身学习：[https://towardsdatascience.com/deep-lifelong-learning-drawing-inspiration-from-the-human-brain-c4518a2f4fb9](/deep-lifelong-learning-drawing-inspiration-from-the-human-brain-c4518a2f4fb9)
- en: 'Neuro-inspired AI: [https://www.cell.com/neuron/pdf/S0896-6273(17)30509-3.pdf](https://www.cell.com/neuron/pdf/S0896-6273(17)30509-3.pdf)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 神经启发的AI：[https://www.cell.com/neuron/pdf/S0896-6273(17)30509-3.pdf](https://www.cell.com/neuron/pdf/S0896-6273(17)30509-3.pdf)
- en: 'Embodied LL: [https://lis.csail.mit.edu/embodied-lifelong-learning-for-decision-making/](https://lis.csail.mit.edu/embodied-lifelong-learning-for-decision-making/)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 体现终身学习的应用：[https://lis.csail.mit.edu/embodied-lifelong-learning-for-decision-making/](https://lis.csail.mit.edu/embodied-lifelong-learning-for-decision-making/)
- en: 'LL for sentiment classification: [https://arxiv.org/abs/1801.02808](https://arxiv.org/abs/1801.02808)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 用于情感分类的终身学习：[https://arxiv.org/abs/1801.02808](https://arxiv.org/abs/1801.02808)
- en: 'Lifelong Robot Learning: [https://www.sciencedirect.com/science/article/abs/pii/092188909500004Y](https://www.sciencedirect.com/science/article/abs/pii/092188909500004Y)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 终身机器人学习： [https://www.sciencedirect.com/science/article/abs/pii/092188909500004Y](https://www.sciencedirect.com/science/article/abs/pii/092188909500004Y)
- en: 'Knowledge Basis Idea: [https://arxiv.org/ftp/arxiv/papers/1206/1206.6417.pdf](https://arxiv.org/ftp/arxiv/papers/1206/1206.6417.pdf)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 知识基础理念： [https://arxiv.org/ftp/arxiv/papers/1206/1206.6417.pdf](https://arxiv.org/ftp/arxiv/papers/1206/1206.6417.pdf)
- en: 'Q-Learning: [https://link.springer.com/article/10.1007/BF00992698](https://link.springer.com/article/10.1007/BF00992698)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习： [https://link.springer.com/article/10.1007/BF00992698](https://link.springer.com/article/10.1007/BF00992698)
- en: 'AGI LLLM LLMs: [https://towardsdatascience.com/towards-agi-llms-and-foundational-models-roles-in-the-lifelong-learning-revolution-f8e56c17fa66](/towards-agi-llms-and-foundational-models-roles-in-the-lifelong-learning-revolution-f8e56c17fa66)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: AGI LLLM 大型语言模型： [https://towardsdatascience.com/towards-agi-llms-and-foundational-models-roles-in-the-lifelong-learning-revolution-f8e56c17fa66](/towards-agi-llms-and-foundational-models-roles-in-the-lifelong-learning-revolution-f8e56c17fa66)
- en: 'DEPS: [https://arxiv.org/pdf/2302.01560.pdf](https://arxiv.org/pdf/2302.01560.pdf)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: DEPS： [https://arxiv.org/pdf/2302.01560.pdf](https://arxiv.org/pdf/2302.01560.pdf)
- en: 'Voyager: [https://arxiv.org/pdf/2305.16291.pdf](https://arxiv.org/pdf/2305.16291.pdf)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Voyager： [https://arxiv.org/pdf/2305.16291.pdf](https://arxiv.org/pdf/2305.16291.pdf)
- en: 'Meta-Learning: [https://machine-learning-made-simple.medium.com/meta-learning-why-its-a-big-deal-it-s-future-for-foundation-models-and-how-to-improve-it-c70b8be2931b](https://machine-learning-made-simple.medium.com/meta-learning-why-its-a-big-deal-it-s-future-for-foundation-models-and-how-to-improve-it-c70b8be2931b)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 元学习： [https://machine-learning-made-simple.medium.com/meta-learning-why-its-a-big-deal-it-s-future-for-foundation-models-and-how-to-improve-it-c70b8be2931b](https://machine-learning-made-simple.medium.com/meta-learning-why-its-a-big-deal-it-s-future-for-foundation-models-and-how-to-improve-it-c70b8be2931b)
- en: 'Meta Reinforcement Learning Survey: [https://arxiv.org/abs/2301.08028](https://arxiv.org/abs/2301.08028)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 元强化学习调查： [https://arxiv.org/abs/2301.08028](https://arxiv.org/abs/2301.08028)
