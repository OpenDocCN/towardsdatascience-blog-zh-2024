- en: Time Series Prediction with Transformers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Transformer进行时间序列预测
- en: 原文：[https://towardsdatascience.com/time-series-prediction-with-transformers-2b64478a4cbd?source=collection_archive---------4-----------------------#2024-01-16](https://towardsdatascience.com/time-series-prediction-with-transformers-2b64478a4cbd?source=collection_archive---------4-----------------------#2024-01-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/time-series-prediction-with-transformers-2b64478a4cbd?source=collection_archive---------4-----------------------#2024-01-16](https://towardsdatascience.com/time-series-prediction-with-transformers-2b64478a4cbd?source=collection_archive---------4-----------------------#2024-01-16)
- en: A Complete Guide to Transformers in Pytorch
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 《Pytorch中的Transformer完整指南》
- en: '[](https://medium.com/@hrmnmichaels?source=post_page---byline--2b64478a4cbd--------------------------------)[![Oliver
    S](../Images/b5ee0fa2d5fb115f62e2e9dfcb92afdd.png)](https://medium.com/@hrmnmichaels?source=post_page---byline--2b64478a4cbd--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2b64478a4cbd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2b64478a4cbd--------------------------------)
    [Oliver S](https://medium.com/@hrmnmichaels?source=post_page---byline--2b64478a4cbd--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@hrmnmichaels?source=post_page---byline--2b64478a4cbd--------------------------------)[![Oliver
    S](../Images/b5ee0fa2d5fb115f62e2e9dfcb92afdd.png)](https://medium.com/@hrmnmichaels?source=post_page---byline--2b64478a4cbd--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2b64478a4cbd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2b64478a4cbd--------------------------------)
    [Oliver S](https://medium.com/@hrmnmichaels?source=post_page---byline--2b64478a4cbd--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2b64478a4cbd--------------------------------)
    ·17 min read·Jan 16, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2b64478a4cbd--------------------------------)
    ·17分钟阅读·2024年1月16日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: At the latest since the advent of [ChatGPT](https://chat.openai.com/), [Large
    Language models](https://en.wikipedia.org/wiki/Large_language_model) (LLMs) have
    created a huge hype, and are known even to those outside the AI community. Even
    though one needs to understand that [LLMs inherently are “just” sequence prediction
    models](https://www.linkedin.com/posts/yann-lecun_i-have-claimed-that-auto-regressive-llms-activity-7045908925660950528-hJGk)
    without any form of intelligence or reasoning — the achieved results are certainly
    extremely impressive, with some even talking about another step in the “AI Revolution”.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 自从[Feynman学习法](https://chat.openai.com/)的出现以来，[大型语言模型](https://en.wikipedia.org/wiki/Large_language_model)（LLMs）已掀起了巨大的热潮，甚至连AI领域以外的人也有所耳闻。尽管需要理解的是，[LLMs本质上是“仅仅”是序列预测模型](https://www.linkedin.com/posts/yann-lecun_i-have-claimed-that-auto-regressive-llms-activity-7045908925660950528-hJGk)，并不具备任何形式的智能或推理——但它们取得的成果无疑是非常令人印象深刻的，甚至有些人谈论这是“AI革命”的另一步。
- en: Essential to the success of LLMs are their core building blocks, **transformers**.
    In this post, we will give a complete guide of using them in Pytorch, with particular
    focus on time series prediction. Thanks for stopping by, and I hope you enjoy
    the ride!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs成功的关键在于其核心构建模块——**Transformer**。在本文中，我们将提供在Pytorch中使用Transformer的完整指南，特别关注时间序列预测。感谢您的到来，希望您能享受这段旅程！
- en: '![](../Images/b34cf6d50a71d4ef98f9bdb04c6a7be8.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b34cf6d50a71d4ef98f9bdb04c6a7be8.png)'
- en: Photo by [Tim Meyer](https://unsplash.com/@timmeyer?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/yellow-chevrolet-coupe-close-up-photography-GIm7wxiAZys?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Tim Meyer](https://unsplash.com/@timmeyer?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    于[Unsplash](https://unsplash.com/photos/yellow-chevrolet-coupe-close-up-photography-GIm7wxiAZys?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
- en: One could argue that all problems solved via transformers essentially are time
    series problems. While that is true, here we will put special focus to continuous
    series and data — such as predicting the spreading of diseases or forecasting
    the weather. The difference to the prominent application of Natural Language Processing
    ([NLP](https://en.wikipedia.org/wiki/Natural_language_processing)) simply (if
    this word is allowed in this context — developing a model like ChatGPT and making
    it work naturally does require a multitude of further optimization steps and tricks)
    is the continuous input space, while NLP works with discrete tokens. However,
    apart from…
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 有人可能会认为，所有通过变压器（transformers）解决的问题本质上都是时间序列问题。虽然这确实是事实，但在这里我们将特别关注连续系列和数据——例如预测疾病传播或天气预报。与自然语言处理（[NLP](https://en.wikipedia.org/wiki/Natural_language_processing)）的主要应用不同（简单来说，如果这个词在此语境下是允许的——开发一个像ChatGPT这样的模型并使其自然运行确实需要大量进一步的优化步骤和技巧），主要区别在于输入空间是连续的，而NLP使用的是离散的词元（tokens）。然而，除了……
