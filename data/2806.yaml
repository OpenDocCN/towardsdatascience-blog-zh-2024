- en: Generate 3D Images with Nvidia’s LLaMa-Mesh
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Nvidia 的 LLaMa-Mesh 生成 3D 图像
- en: 原文：[https://towardsdatascience.com/generate-3d-images-with-nvidias-llama-mesh-69a6929a4580?source=collection_archive---------10-----------------------#2024-11-19](https://towardsdatascience.com/generate-3d-images-with-nvidias-llama-mesh-69a6929a4580?source=collection_archive---------10-----------------------#2024-11-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/generate-3d-images-with-nvidias-llama-mesh-69a6929a4580?source=collection_archive---------10-----------------------#2024-11-19](https://towardsdatascience.com/generate-3d-images-with-nvidias-llama-mesh-69a6929a4580?source=collection_archive---------10-----------------------#2024-11-19)
- en: DEEP LEARNING PAPERS
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习论文
- en: 5-Minute Deep Dive into the Paper
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 论文深度剖析（5分钟速览）
- en: '[](https://varshitasher.medium.com/?source=post_page---byline--69a6929a4580--------------------------------)[![Dr.
    Varshita Sher](../Images/a3f2e9bf1dc1d8cbe018e54f9341f608.png)](https://varshitasher.medium.com/?source=post_page---byline--69a6929a4580--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--69a6929a4580--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--69a6929a4580--------------------------------)
    [Dr. Varshita Sher](https://varshitasher.medium.com/?source=post_page---byline--69a6929a4580--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://varshitasher.medium.com/?source=post_page---byline--69a6929a4580--------------------------------)[![Dr.
    Varshita Sher](../Images/a3f2e9bf1dc1d8cbe018e54f9341f608.png)](https://varshitasher.medium.com/?source=post_page---byline--69a6929a4580--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--69a6929a4580--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--69a6929a4580--------------------------------)
    [Dr. Varshita Sher](https://varshitasher.medium.com/?source=post_page---byline--69a6929a4580--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--69a6929a4580--------------------------------)
    ·5 min read·Nov 19, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--69a6929a4580--------------------------------)
    ·5分钟阅读·2024年11月19日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/54a94d7a83e9d4615209cfcd3b48eefe.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/54a94d7a83e9d4615209cfcd3b48eefe.png)'
- en: Taken from the paper
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 摘自论文
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: 'Last week, NVIDIA published a fascinating paper ([LLaMA-Mesh: Unifying 3D Mesh
    Generation with Language Models](https://arxiv.org/abs/2411.09595)) that allows
    the generation of 3D mesh objects using natural language.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '上周，NVIDIA 发布了一篇引人入胜的论文（[LLaMA-Mesh: 用语言模型统一生成 3D 网格](https://arxiv.org/abs/2411.09595)），该论文展示了如何使用自然语言生成
    3D 网格物体。'
- en: In simple words, if you can say, *"Tell me a joke*," now you can say, *"Give
    me the 3D mesh for a car,"* and it can give the output in the OBJ format (more
    on this shortly) containing the output.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，如果你能说，*“给我讲个笑话”*，那么现在你可以说，*“给我一个车的 3D 网格”*，它就能生成一个 OBJ 格式的输出文件（稍后详细说明）。
- en: If you’d like to try out few examples, you can do so here — [https://huggingface.co/spaces/Zhengyi/LLaMA-Mesh](https://huggingface.co/spaces/Zhengyi/LLaMA-Mesh)
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你想尝试一些例子，可以点击这里 —— [https://huggingface.co/spaces/Zhengyi/LLaMA-Mesh](https://huggingface.co/spaces/Zhengyi/LLaMA-Mesh)
- en: The most amazing part for me was that it did so without extending the vocabulary
    or introducing new tokens as is typical for most fine-tuning tasks.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对我来说，最令人惊讶的部分是，它在没有扩展词汇或引入新标记的情况下完成了这一点，这在大多数微调任务中是很常见的做法。
- en: '***But first, what is a 3D mesh?***'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '***但是首先，什么是 3D 网格？***'
- en: A 3D mesh is a digital representation of a 3D object that consists of vertices,
    edges, and faces.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 网格是 3D 物体的数字表示，包含顶点、边和面。
- en: For example, consider a cube. It has 8 vertices (the corners), 12 edges (the
    lines connecting the corners), and 6 faces (the square sides). This is a basic
    3D mesh representation of a cube. The cube’s vertices (`v`) define its corners,
    and the faces (`f`) describe how those corners connect to form the surfaces.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个立方体。它有 8 个顶点（角点）、12 条边（连接角点的线）和 6 个面（方形的面）。这是立方体的基本 3D 网格表示。立方体的顶点（`v`）定义了它的角点，面（`f`）描述了这些角点如何连接形成表面。
- en: Here is an example of OBJ file that represents the geometry of the 3D object
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个 OBJ 文件的示例，表示 3D 物体的几何形状
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: These numbers are then interpreted by software that will render the final image
    i.e. 3D cube. (or you can use HuggingFace spaces like [this](https://huggingface.co/spaces/Zhengyi/LLaMA-Mesh)
    to render the object)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字随后被软件解释，软件会渲染出最终图像，即 3D 立方体。（或者你也可以使用像[这个](https://huggingface.co/spaces/Zhengyi/LLaMA-Mesh)这样的
    HuggingFace 空间来渲染物体）
- en: As objects increase in complexity (compared to the simple cube above), they
    will have thousands or even millions of vertices, edges, and faces to create detailed
    shapes and textures. Additionally, they will have more dimensions to capture things
    like texture, direction it is facing, etc.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对象复杂度的增加（与上面的简单立方体相比），它们将有成千上万甚至百万个顶点、边和面来创建详细的形状和纹理。此外，它们还将有更多的维度来捕捉诸如纹理、朝向等信息。
- en: 'Realistically speaking, this is what the obj file for an everyday object (a
    bench) would look like:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 实际来说，日常物品（如长椅）的 obj 文件大概长这样：
- en: '![](../Images/17e377768d383fdca3851d49b2b687f3.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17e377768d383fdca3851d49b2b687f3.png)'
- en: Examples of obj file for different objects (Taken from paper)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 不同对象的 obj 文件示例（摘自论文）
- en: As you may have noticed from the image above, LLMs like GPT4o and LLama3.1 are
    capable, to some extent, of producing the obj file out-of-the-box. However, if
    you look at the rendered mesh image of the bench in both cases, you can see why
    fine-tuning is necessary from a quality standpoint.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如你从上面的图片中可能已经注意到的那样，LLM（如 GPT4o 和 LLama3.1）在某种程度上能够直接生成 obj 文件。然而，如果你查看长椅的渲染网格图像，你会明白为什么从质量角度看需要进行微调。
- en: How is an LLM able to work with 3D mesh?
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM 如何处理 3D 网格？
- en: It is common knowledge that LLMs understand text by converting tokens (like
    `cat`) into token ids (like `456`). Similarly, in order to work with the standard
    OBJ format, we must somehow convert the vertices coordinates which are typically
    decimals into integers.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，LLMs 通过将标记（如 `cat`）转换为标记 ID（如 `456`）来理解文本。类似地，为了处理标准的 OBJ 格式，我们也必须以某种方式将通常是小数的顶点坐标转换为整数。
- en: They use vertex quantization to achieve this in the paper and split a single
    coordinate into multiple tokens (similar to how a long word like `operational`
    would be split into two tokens — `oper` and `ational` as per [GPT4o tokenizer).](https://platform.openai.com/tokenizer)
    As expected, reducing the number of tokens to represent the decimal has a normal
    precision-cost tradeoff.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 他们在论文中使用顶点量化来实现这一点，将一个坐标分割成多个标记（类似于长单词 `operational` 会被分割成两个标记 —— `oper` 和 `ational`，如
    [GPT4o tokenizer](https://platform.openai.com/tokenizer) 所示）。正如预期的那样，减少表示小数的标记数会带来正常的精度-成本权衡。
- en: '![](../Images/5fb1a7ee1d43422858e1fc7445211069.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5fb1a7ee1d43422858e1fc7445211069.png)'
- en: To achieve vertex quantization, they scale all three axes in the mesh to the
    range (0, 64) and quantize the coordinates to the nearest integer, i.e. each of
    the 3 axes can take a value between 0 and 64 (in this case 39, 19 and 35). Finally,
    by reading and generating such a format, the LLM is able to work with 3D objects.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现顶点量化，他们将网格中的三个轴都缩放到 (0, 64) 范围，并将坐标量化为最接近的整数，即每个轴的值可以在 0 到 64 之间（在本例中为 39、19
    和 35）。最后，通过读取和生成这样的格式，LLM 就能够处理 3D 对象。
- en: What was the training procedure for LlaMa-Mesh?
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LlaMa-Mesh 的训练过程是什么？
- en: LLama-Mesh was created by fine-tuning LLama3.1–8B instruct model using the SFT
    (Supervised Fine Tuning) method to improve its mesh understanding and generation
    capabilities.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: LLama-Mesh 是通过使用 SFT（监督式微调）方法对 LLama3.1–8B 指令模型进行微调而创建的，以提高其网格理解和生成能力。
- en: 'Since it is an SFT, we need to provide it with input-output examples of Text-3D
    instructions. Here’s an example:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它是一个 SFT，我们需要为其提供文本-3D 指令的输入输出示例。以下是一个示例：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In addition to generating the 3D mesh, LLama-Mesh is also capable of interpreting
    the 3d mesh. To this end, its training data also contained several examples for
    mesh understanding and mesh generation as part of a conversation-style format.
    Here are a few examples from the dataset
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 除了生成 3D 网格，LLama-Mesh 还能够解释 3D 网格。为此，它的训练数据还包含了几个网格理解和网格生成的示例，作为对话式格式的一部分。以下是数据集中的一些示例：
- en: '![](../Images/dfc043eef6d5b257e2f48c7a48e81bec.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dfc043eef6d5b257e2f48c7a48e81bec.png)'
- en: Training dataset curated for LLama-Mesh
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为 LLama-Mesh 精心策划的训练数据集
- en: Most interesting bits from the paper
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 论文中的最有趣部分
- en: LlaMa-Mesh can communicate with both text and 3D objects **without** needing
    special tokenizers or extending the LLM’s vocabulary (thanks to the use of OBJ
    format and the vertex quantization discussed above which can effectively tokenize
    3D mesh data into discrete tokens that LLMs can process seamlessly).
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LlaMa-Mesh 可以与文本和 3D 对象 **无缝** 交互，无需特殊的标记器或扩展 LLM 的词汇量（这要归功于使用 OBJ 格式和前述的顶点量化方法，它可以有效地将
    3D 网格数据标记化为 LLM 可以无缝处理的离散标记）。
- en: '![](../Images/9c9d63bf68147f6a8d5ca8cbf283a39d.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c9d63bf68147f6a8d5ca8cbf283a39d.png)'
- en: Image taken from paper
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图片摘自论文
- en: LlaMa-Mesh can generate diverse shapes from the same input text.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LlaMa-Mesh可以从相同的输入文本生成多样化的形状。
- en: '![](../Images/9765acd06efa01af4b7afca377607486.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9765acd06efa01af4b7afca377607486.png)'
- en: Taken from paper
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 来自论文
- en: Even though the fine-tuning process slightly degraded the model’s underlying
    language understanding and reasoning capabilities (they call it out as a limitation
    imposed by the choice of instruction dataset, and size of the smaller 8B model),
    it is offset by the fact that the fine-tuned model can generate high-quality OBJ
    files for 3D mesh generation.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管微调过程略微降低了模型的语言理解和推理能力（他们将其视为由选择的指令数据集和较小的8B模型规模所带来的局限性），但这一点被微调模型能够生成高质量OBJ文件以供3D网格生成的能力所抵消。
- en: '![](../Images/a9f3f98035525faf5b3b6f84f0f917ba.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9f3f98035525faf5b3b6f84f0f917ba.png)'
- en: Comparison of the base model and fine tuned version on metrics which assess
    the model’s general knowledge, common sense reasoning, and mathematical problem-solving
    abilities (Image taken from paper)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对比基础模型和微调版本在评估模型的通用知识、常识推理和数学问题解决能力方面的指标（图片来自论文）。
- en: Why should you care about this paper?
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么你应该关注这篇论文？
- en: I am already amazed by the capabilities of large language models to generate
    human-like text, code, and reason with visual content. Adding 3D mesh to this
    list is just brilliant.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经为大语言模型生成类人文本、代码以及与视觉内容推理的能力感到惊叹。将3D网格加入这个列表简直是太聪明了。
- en: LLMs like LLaMa-Mesh have the potential to revolutionize various industries
    including gaming, education, and healthcare.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 像LLaMa-Mesh这样的LLM有潜力彻底改变包括游戏、教育和医疗在内的多个行业。
- en: It can be useful for generating realistic assets like characters, environments,
    and objects directly from text descriptions for video games.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以用于直接从文本描述生成用于视频游戏的逼真资产，如角色、环境和物体。
- en: Similarly, it can speed up the product development and ideation process as any
    company will require a design so they know what to create.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，它可以加速产品开发和创意过程，因为任何公司都需要一个设计来知道他们要创造什么。
- en: It can also be useful for architectural designs for buildings, machinery, bridges,
    and other infrastructure projects. Finally, in the edtech space, it can be used
    for embedding interactive 3D simulations within the training material.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 它还可以用于建筑物、机械、桥梁和其他基础设施项目的建筑设计。最后，在教育技术领域，它可以用于将互动3D模拟嵌入到培训材料中。
- en: The paper is a straightforward and quick read, and I highly encourage you to
    do it.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文简洁明了，阅读起来非常快速，我强烈建议你阅读它。
- en: '**Paper page** — [https://arxiv.org/pdf/2411.09595](https://arxiv.org/pdf/2411.09595)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**论文页面** — [https://arxiv.org/pdf/2411.09595](https://arxiv.org/pdf/2411.09595)'
- en: '**Code** — [https://github.com/nv-tlabs/LLaMA-Mesh](https://github.com/nv-tlabs/LLaMA-Mesh)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码** — [https://github.com/nv-tlabs/LLaMA-Mesh](https://github.com/nv-tlabs/LLaMA-Mesh)'
- en: '**Nvidia’s Blog** — [https://research.nvidia.com/labs/toronto-ai/LLaMA-Mesh/](https://research.nvidia.com/labs/toronto-ai/LLaMA-Mesh/)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**Nvidia的博客** — [https://research.nvidia.com/labs/toronto-ai/LLaMA-Mesh/](https://research.nvidia.com/labs/toronto-ai/LLaMA-Mesh/)'
