- en: Build a Document AI Pipeline for Any Type of PDF with Gemini
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Gemini ä¸ºä»»ä½•ç±»å‹çš„ PDF æ„å»ºæ–‡æ¡£ AI æµæ°´çº¿
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/build-a-document-ai-pipeline-for-any-type-of-pdf-with-gemini-9221c8e143db?source=collection_archive---------0-----------------------#2024-12-15](https://towardsdatascience.com/build-a-document-ai-pipeline-for-any-type-of-pdf-with-gemini-9221c8e143db?source=collection_archive---------0-----------------------#2024-12-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/build-a-document-ai-pipeline-for-any-type-of-pdf-with-gemini-9221c8e143db?source=collection_archive---------0-----------------------#2024-12-15](https://towardsdatascience.com/build-a-document-ai-pipeline-for-any-type-of-pdf-with-gemini-9221c8e143db?source=collection_archive---------0-----------------------#2024-12-15)
- en: Tables, Images, figures or equations are not problem anymore! Full Code provided.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¡¨æ ¼ã€å›¾ç‰‡ã€å›¾è¡¨æˆ–å…¬å¼å·²ç»ä¸å†æ˜¯é—®é¢˜ï¼æä¾›å®Œæ•´ä»£ç ã€‚
- en: '[](https://medium.com/@CVxTz?source=post_page---byline--9221c8e143db--------------------------------)[![Youness
    Mansar](../Images/b68fe2cbbe219ab0231922c7165f2b6a.png)](https://medium.com/@CVxTz?source=post_page---byline--9221c8e143db--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9221c8e143db--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9221c8e143db--------------------------------)
    [Youness Mansar](https://medium.com/@CVxTz?source=post_page---byline--9221c8e143db--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@CVxTz?source=post_page---byline--9221c8e143db--------------------------------)[![Youness
    Mansar](../Images/b68fe2cbbe219ab0231922c7165f2b6a.png)](https://medium.com/@CVxTz?source=post_page---byline--9221c8e143db--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9221c8e143db--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9221c8e143db--------------------------------)
    [Youness Mansar](https://medium.com/@CVxTz?source=post_page---byline--9221c8e143db--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9221c8e143db--------------------------------)
    Â·10 min readÂ·Dec 15, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9221c8e143db--------------------------------)
    Â·é˜…è¯»æ—¶é—´ 10 åˆ†é’ŸÂ·2024å¹´12æœˆ15æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/04f5f0506207581622a222ae56c38208.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04f5f0506207581622a222ae56c38208.png)'
- en: Photo by [Matt Noble](https://unsplash.com/@mcnoble?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼š[Matt Noble](https://unsplash.com/@mcnoble?utm_source=medium&utm_medium=referral)
    via [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Automated document processing is one of the biggest winners of the ChatGPT revolution,
    as LLMs are able to tackle a wide range of subjects and tasks in a zero-shot setting,
    meaning without in-domain labeled training data. This has made building AI-powered
    applications to process, parse, and automatically understand arbitrary documents
    much easier. Though naive approaches using LLMs are still hindered by non-text
    context, such as figures, images, and tables, this is what we will try to address
    in this blog post, with a special focus on PDFs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªåŠ¨åŒ–æ–‡æ¡£å¤„ç†æ˜¯ ChatGPT é©å‘½ä¸­çš„æœ€å¤§èµ¢å®¶ä¹‹ä¸€ï¼Œå› ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿåœ¨é›¶æ ·æœ¬ç¯å¢ƒä¸‹å¤„ç†å„ç§ä¸»é¢˜å’Œä»»åŠ¡ï¼Œè¿™æ„å‘³ç€å®ƒä»¬ä¸éœ€è¦é¢†åŸŸå†…çš„æ ‡æ³¨è®­ç»ƒæ•°æ®ã€‚è¿™ä½¿å¾—æ„å»ºåŸºäº
    AI çš„åº”ç”¨ç¨‹åºæ¥å¤„ç†ã€è§£æå’Œè‡ªåŠ¨ç†è§£ä»»æ„æ–‡æ¡£å˜å¾—æ›´åŠ å®¹æ˜“ã€‚å°½ç®¡ä½¿ç”¨ LLM çš„ç®€å•æ–¹æ³•ä»ç„¶å—åˆ°éæ–‡æœ¬ä¸Šä¸‹æ–‡çš„åˆ¶çº¦ï¼Œå¦‚å›¾è¡¨ã€å›¾ç‰‡å’Œè¡¨æ ¼ï¼Œä½†è¿™æ­£æ˜¯æˆ‘ä»¬å°†åœ¨æœ¬åšå®¢ä¸­å°è¯•è§£å†³çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å…³æ³¨
    PDF æ–‡ä»¶ã€‚
- en: At a basic level, PDFs are just a collection of characters, images, and lines
    along with their exact coordinates. They have no inherent â€œtextâ€ structure and
    were not built to be processed as text but only to be viewed as is. This is what
    makes working with them difficult, as text-only approaches fail to capture all
    the layout and visual elements in these types of documents, resulting in a significant
    loss of context and information.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä»åŸºç¡€å±‚é¢æ¥è¯´ï¼ŒPDF æ–‡ä»¶ä»…ä»…æ˜¯ç”±å­—ç¬¦ã€å›¾ç‰‡å’Œçº¿æ¡ä»¥åŠå®ƒä»¬çš„ç²¾ç¡®åæ ‡ç»„æˆã€‚å®ƒä»¬æ²¡æœ‰å†…åœ¨çš„â€œæ–‡æœ¬â€ç»“æ„ï¼Œå¹¶ä¸”å¹¶éä¸ºäº†ä½œä¸ºæ–‡æœ¬è¿›è¡Œå¤„ç†è€Œæ„å»ºï¼Œè€Œåªæ˜¯ä¸ºäº†æŸ¥çœ‹è€Œå­˜åœ¨ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆä¸å®ƒä»¬æ‰“äº¤é“ä¼šå¾ˆå›°éš¾ï¼Œå› ä¸ºä»…é™æ–‡æœ¬çš„æ–¹æ³•æ— æ³•æ•æ‰åˆ°è¿™äº›æ–‡æ¡£ä¸­çš„æ‰€æœ‰å¸ƒå±€å’Œè§†è§‰å…ƒç´ ï¼Œå¯¼è‡´å¤§é‡çš„ä¸Šä¸‹æ–‡å’Œä¿¡æ¯ä¸¢å¤±ã€‚
- en: One way to bypass this â€œtext-onlyâ€ limitation is to do heavy pre-processing
    of the document by detecting tables, images, and layout before feeding them to
    the LLM. Tables can be parsed to Markdown or JSON, images and figures can be represented
    by their captions, and the text can be fed as is. However, this approach requires
    custom models and will still result in some loss of information, so can we do
    better?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ç»•è¿‡è¿™ä¸€â€œä»…é™æ–‡æœ¬â€çš„é™åˆ¶çš„ä¸€ç§æ–¹æ³•æ˜¯ï¼Œé€šè¿‡åœ¨å°†æ–‡æ¡£è¾“å…¥åˆ° LLM ä¹‹å‰ï¼Œè¿›è¡Œå¤§é‡çš„é¢„å¤„ç†å·¥ä½œï¼Œæ£€æµ‹è¡¨æ ¼ã€å›¾ç‰‡å’Œå¸ƒå±€ã€‚è¡¨æ ¼å¯ä»¥è§£æä¸º Markdown
    æˆ– JSONï¼Œå›¾ç‰‡å’Œå›¾è¡¨å¯ä»¥é€šè¿‡å®ƒä»¬çš„æ ‡é¢˜æ¥è¡¨ç¤ºï¼Œæ–‡æœ¬åˆ™å¯ä»¥æŒ‰åŸæ ·è¾“å…¥ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•éœ€è¦è‡ªå®šä¹‰æ¨¡å‹ï¼Œå¹¶ä¸”ä»ç„¶ä¼šå¯¼è‡´ä¸€äº›ä¿¡æ¯ä¸¢å¤±ï¼Œé‚£ä¹ˆæˆ‘ä»¬èƒ½åšå¾—æ›´å¥½å—ï¼Ÿ
- en: Multimodal LLMs
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤šæ¨¡æ€ LLMs
- en: Most recent large models are now multi-modal, meaning they can process multiple
    modalities like text, code, and images. This opens the way to a simpler solution
    to our problem where one model does everything at once. So, instead of captioning
    images and parsing tables, we can just feed the page as an image and process it
    as is. Our pipeline will be able to load the PDF, extract each page as an image,
    split it into chunks (using the LLM), and index each chunk. If a chunk is retrieved,
    then the full page is included in the LLM context to perform the task. In what
    follows, we will detail how this can be implemented in practice.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘çš„å¤§å‹æ¨¡å‹å¤§å¤šæ˜¯å¤šæ¨¡æ€çš„ï¼Œæ„å‘³ç€å®ƒä»¬èƒ½å¤Ÿå¤„ç†æ–‡æœ¬ã€ä»£ç å’Œå›¾åƒç­‰å¤šç§æ¨¡æ€ã€‚è¿™ä¸ºæˆ‘ä»¬çš„é—®é¢˜æä¾›äº†æ›´ç®€å•çš„è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥è®©ä¸€ä¸ªæ¨¡å‹ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰å†…å®¹ã€‚å› æ­¤ï¼Œä»£æ›¿ä¸ºå›¾åƒåŠ ä¸Šæ ‡é¢˜å’Œè§£æè¡¨æ ¼ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥å°†é¡µé¢ä½œä¸ºå›¾åƒè¾“å…¥ï¼Œå¹¶æŒ‰åŸæ ·å¤„ç†ã€‚æˆ‘ä»¬çš„ç®¡é“å°†èƒ½å¤ŸåŠ è½½PDFï¼Œæå–æ¯ä¸€é¡µä½œä¸ºå›¾åƒï¼Œä½¿ç”¨LLMå°†å…¶æ‹†åˆ†ä¸ºå¤šä¸ªåˆ†å—ï¼Œå¹¶ä¸ºæ¯ä¸ªåˆ†å—åˆ›å»ºç´¢å¼•ã€‚å¦‚æœæŸä¸ªåˆ†å—è¢«æ£€ç´¢åˆ°ï¼Œå®Œæ•´çš„é¡µé¢å°†è¢«åŒ…å«åœ¨LLMçš„ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œä»»åŠ¡å¤„ç†ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è¯¦ç»†è¯´æ˜å¦‚ä½•åœ¨å®è·µä¸­å®ç°è¿™ä¸€ç‚¹ã€‚
- en: The Pipeline
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç®¡é“
- en: The pipeline we are implementing is a two-step process. First, we segment each
    page into significant chunks and summarize each of them. Second, we index chunks
    once then search the chunks each time we get a request and include the full context
    with each retrieved chunk in the LLM context.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®ç°çš„ç®¡é“æ˜¯ä¸€ä¸ªä¸¤æ­¥è¿‡ç¨‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†æ¯ä¸€é¡µåˆ†å‰²æˆé‡è¦çš„åˆ†å—å¹¶å¯¹æ¯ä¸ªåˆ†å—è¿›è¡Œæ‘˜è¦ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬åœ¨æ¯æ¬¡è·å–è¯·æ±‚æ—¶æ£€ç´¢è¿™äº›åˆ†å—å¹¶å°†å®Œæ•´ä¸Šä¸‹æ–‡ä¸æ¯ä¸ªæ£€ç´¢åˆ°çš„åˆ†å—ä¸€åŒåŒ…å«åœ¨LLMä¸Šä¸‹æ–‡ä¸­ã€‚
- en: 'Step 1: Page Segmentation and Summarization'
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ­¥éª¤ 1ï¼šé¡µé¢åˆ†å‰²ä¸æ‘˜è¦
- en: 'We extract the pages as images and pass each of them to the multi-modal LLM
    to segment them. Models like Gemini can understand and process page layout easily:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†é¡µé¢æå–ä¸ºå›¾åƒï¼Œå¹¶å°†æ¯ä¸ªå›¾åƒä¼ é€’ç»™å¤šæ¨¡æ€LLMè¿›è¡Œåˆ†å‰²ã€‚åƒGeminiè¿™æ ·çš„æ¨¡å‹å¯ä»¥è½»æ¾ç†è§£å¹¶å¤„ç†é¡µé¢å¸ƒå±€ï¼š
- en: '**Tables** are identified as one chunk.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è¡¨æ ¼**è¢«è§†ä¸ºä¸€ä¸ªåˆ†å—ã€‚'
- en: '**Figures** form another chunk.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å›¾å½¢**æ„æˆå¦ä¸€ä¸ªåˆ†å—ã€‚'
- en: '**Text blocks** are segmented into individual chunks.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ–‡æœ¬å—**è¢«åˆ†å‰²æˆç‹¬ç«‹çš„åˆ†å—ã€‚'
- en: â€¦
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¦
- en: For each element, the LLM generates a summary than can be embedded and indexed
    into a vector database.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªå…ƒç´ ï¼ŒLLMä¼šç”Ÿæˆä¸€ä¸ªæ‘˜è¦ï¼Œå¯ä»¥åµŒå…¥å¹¶ç´¢å¼•åˆ°å‘é‡æ•°æ®åº“ä¸­ã€‚
- en: 'Step 2: Embedding and Contextual Retrieval'
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ­¥éª¤ 2ï¼šåµŒå…¥ä¸ä¸Šä¸‹æ–‡æ£€ç´¢
- en: In this tutorial we will use text embedding only for simplicity but one improvement
    would be to use vision embeddings directly.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä»…ä½¿ç”¨æ–‡æœ¬åµŒå…¥ä»¥ç®€åŒ–æ“ä½œï¼Œä½†ä¸€ç§æ”¹è¿›æ–¹æ³•æ˜¯ç›´æ¥ä½¿ç”¨è§†è§‰åµŒå…¥ã€‚
- en: 'Each entry in the database includes:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®åº“ä¸­çš„æ¯ä¸€æ¡è®°å½•åŒ…æ‹¬ï¼š
- en: The summary of the chunk.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ†å—çš„æ‘˜è¦ã€‚
- en: The page number where it was found.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰¾åˆ°è¯¥é¡¹çš„é¡µé¢ç¼–å·ã€‚
- en: A link to the image representation of the full page for added context.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æä¾›å®Œæ•´é¡µé¢å›¾åƒçš„é“¾æ¥ä»¥å¢åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
- en: This schema allows for **local level searches** (at the chunk level) while keeping
    **track of the context** (by linking back to the full page). For example, if a
    search query retrieves an item, the Agent can include the entire page image to
    provide full layout and extra context to the LLM in order to maximize response
    quality.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¶æ„å…è®¸**å±€éƒ¨çº§åˆ«æœç´¢**ï¼ˆæŒ‰åˆ†å—çº§åˆ«ï¼‰ï¼ŒåŒæ—¶ä¿æŒ**ä¸Šä¸‹æ–‡è·Ÿè¸ª**ï¼ˆé€šè¿‡é“¾æ¥å›å®Œæ•´é¡µé¢ï¼‰ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæœç´¢æŸ¥è¯¢æ£€ç´¢åˆ°æŸä¸ªé¡¹ï¼Œä»£ç†å¯ä»¥å°†æ•´ä¸ªé¡µé¢å›¾åƒåŒ…å«è¿›æ¥ï¼Œä»¥ä¾¿å‘LLMæä¾›å®Œæ•´çš„å¸ƒå±€å’Œé¢å¤–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œæœ€å¤§åŒ–å“åº”è´¨é‡ã€‚
- en: By providing the full image, all the visual cues and important layout information
    (like images, titles, bullet pointsâ€¦ ) and neighboring items (tables, paragraph,
    â€¦) are available to the LLM at the time of generating a response.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æä¾›å®Œæ•´å›¾åƒï¼Œæ‰€æœ‰çš„è§†è§‰æç¤ºå’Œé‡è¦çš„å¸ƒå±€ä¿¡æ¯ï¼ˆå¦‚å›¾åƒã€æ ‡é¢˜ã€é¡¹ç›®ç¬¦å·â€¦ï¼‰ä»¥åŠé‚»è¿‘çš„é¡¹ï¼ˆè¡¨æ ¼ã€æ®µè½ç­‰ï¼‰éƒ½å¯ä»¥åœ¨ç”Ÿæˆå“åº”æ—¶æä¾›ç»™LLMã€‚
- en: Agents
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»£ç†
- en: 'We will implement each step as a separate, re-usable agent:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æ¯ä¸ªæ­¥éª¤å®ç°ä¸ºä¸€ä¸ªç‹¬ç«‹çš„ã€å¯é‡ç”¨çš„ä»£ç†ï¼š
- en: The first agent is for parsing, chunking, and summarization. This involves the
    segmentation of the document into significant chunks, followed by the generation
    of summaries for each of them. This agent only needs to be run once per PDF to
    preprocess the document.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªä»£ç†ç”¨äºè§£æã€åˆ†å—å’Œæ‘˜è¦ã€‚è¿™æ¶‰åŠåˆ°å°†æ–‡æ¡£åˆ†å‰²æˆé‡è¦çš„åˆ†å—ï¼Œå¹¶ä¸ºæ¯ä¸ªåˆ†å—ç”Ÿæˆæ‘˜è¦ã€‚è¿™ä¸ªä»£ç†æ¯ä¸ªPDFåªéœ€è¦è¿è¡Œä¸€æ¬¡ï¼Œç”¨äºé¢„å¤„ç†æ–‡æ¡£ã€‚
- en: The second agent manages indexing, search, and retrieval. This includes inserting
    the embedding of chunks into the vector database for efficient search. Indexing
    is performed once per document, while searches can be repeated as many times as
    needed for different queries.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªä»£ç†è´Ÿè´£ç®¡ç†ç´¢å¼•ã€æœç´¢å’Œæ£€ç´¢ã€‚è¿™åŒ…æ‹¬å°†åˆ†å—çš„åµŒå…¥æ’å…¥åˆ°å‘é‡æ•°æ®åº“ä¸­ï¼Œä»¥ä¾¿é«˜æ•ˆæœç´¢ã€‚ç´¢å¼•åœ¨æ¯ä¸ªæ–‡æ¡£ä¸­åªæ‰§è¡Œä¸€æ¬¡ï¼Œè€Œæœç´¢å¯ä»¥æ ¹æ®ä¸åŒçš„æŸ¥è¯¢é‡å¤è¿›è¡Œã€‚
- en: For both agents, we use **Gemini**, a multimodal LLM with strong vision understanding
    abilities.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä¸¤ä¸ªä»£ç†ï¼Œæˆ‘ä»¬ä½¿ç”¨**Gemini**ï¼Œä¸€ä¸ªå…·æœ‰å¼ºå¤§è§†è§‰ç†è§£èƒ½åŠ›çš„å¤šæ¨¡æ€LLMã€‚
- en: Parsing and Chunking Agent
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è§£æä¸åˆ†å—ä»£ç†
- en: 'The first agent is in charge of segmenting each page into meaningful chunks
    and summarizing each of them, following these steps:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªä»£ç†è´Ÿè´£å°†æ¯ä¸€é¡µåˆ†å‰²æˆæœ‰æ„ä¹‰çš„ç‰‡æ®µï¼Œå¹¶å¯¹æ¯ä¸ªç‰‡æ®µè¿›è¡Œæ€»ç»“ï¼ŒæŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œï¼š
- en: '**Step 1: Extracting PDF Pages as Images**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 1ï¼šå°†PDFé¡µé¢æå–ä¸ºå›¾åƒ**'
- en: We use the `pdf2image` library. The images are then encoded in Base64 format
    to simplify adding them to the LLM request.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨`pdf2image`åº“ã€‚ç„¶åå°†å›¾åƒç¼–ç ä¸ºBase64æ ¼å¼ï¼Œä»¥ç®€åŒ–å°†å…¶æ·»åŠ åˆ°LLMè¯·æ±‚ä¸­ã€‚
- en: 'Hereâ€™s the implementation:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æ˜¯å®ç°è¿‡ç¨‹ï¼š
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`extract_images_from_pdf`: Extracts each page of the PDF as a PIL image.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`extract_images_from_pdf`ï¼šå°†PDFçš„æ¯ä¸€é¡µæå–ä¸ºPILå›¾åƒã€‚'
- en: '`pil_image_to_base64_jpeg`: Converts the image into a Base64-encoded JPEG format.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`pil_image_to_base64_jpeg`ï¼šå°†å›¾åƒè½¬æ¢ä¸ºBase64ç¼–ç çš„JPEGæ ¼å¼ã€‚'
- en: '**Step 2: Chunking and Summarization**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 2ï¼šåˆ†æ®µå’Œæ€»ç»“**'
- en: 'Each image is then sent to the LLM for segmentation and summarization. We use
    structured outputs to ensure we get the predictions in the format we expect:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªå›¾åƒå°†è¢«å‘é€åˆ°LLMè¿›è¡Œåˆ†å‰²å’Œæ€»ç»“ã€‚æˆ‘ä»¬ä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºä»¥ç¡®ä¿è·å¾—æˆ‘ä»¬æœŸæœ›çš„æ ¼å¼çš„é¢„æµ‹ï¼š
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `LayoutElements` schema defines the structure of the output, with each layout
    item type (Table, Figure, â€¦ ) and its summary.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`LayoutElements`æ¨¡å¼å®šä¹‰äº†è¾“å‡ºçš„ç»“æ„ï¼Œå…¶ä¸­åŒ…æ‹¬æ¯ç§å¸ƒå±€é¡¹ç±»å‹ï¼ˆè¡¨æ ¼ã€å›¾å½¢ç­‰ï¼‰åŠå…¶æ€»ç»“ã€‚'
- en: '**Step 3: Parallel Processing of Pages**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 3ï¼šé¡µé¢çš„å¹¶è¡Œå¤„ç†**'
- en: 'Pages are processed in parallel for speed. The following method creates a list
    of tasks to handle all the page image at once since the processing is io-bound:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: é¡µé¢å¹¶è¡Œå¤„ç†ä»¥æé«˜é€Ÿåº¦ã€‚ä»¥ä¸‹æ–¹æ³•åˆ›å»ºä¸€ä¸ªä»»åŠ¡åˆ—è¡¨ï¼Œä»¥ä¾¿åŒæ—¶å¤„ç†æ‰€æœ‰é¡µé¢å›¾åƒï¼Œå› ä¸ºå¤„ç†æ˜¯IOç»‘å®šçš„ï¼š
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Each page is sent to the `find_layout_items` function as an independent task.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸€é¡µå°†ä½œä¸ºç‹¬ç«‹ä»»åŠ¡å‘é€åˆ°`find_layout_items`å‡½æ•°ã€‚
- en: '**Full workflow**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**å®Œæ•´å·¥ä½œæµç¨‹**'
- en: The agentâ€™s workflow is built using a `StateGraph`, linking the image extraction
    and layout detection steps into a unified pipeline ->
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥ä»£ç†çš„å·¥ä½œæµç¨‹ä½¿ç”¨`StateGraph`æ„å»ºï¼Œå°†å›¾åƒæå–å’Œå¸ƒå±€æ£€æµ‹æ­¥éª¤é“¾æ¥æˆä¸€ä¸ªç»Ÿä¸€çš„ç®¡é“ ->
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To run the agent on a sample PDF we do:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨ä¸€ä¸ªæ ·æœ¬PDFä¸Šè¿è¡Œä»£ç†ï¼Œæˆ‘ä»¬éœ€è¦æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This results in a parsed, segmented, and summarized representation of the PDF,
    which is the input of the second agent we will build next.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†ç”Ÿæˆä¸€ä¸ªè§£æã€åˆ†æ®µå’Œæ€»ç»“åçš„PDFè¡¨ç¤ºï¼Œå®ƒæ˜¯æˆ‘ä»¬æ¥ä¸‹æ¥è¦æ„å»ºçš„ç¬¬äºŒä¸ªä»£ç†çš„è¾“å…¥ã€‚
- en: RAG Agent
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAGä»£ç†
- en: This second agent handles the indexing and retrieval part. It saves the documents
    of the previous agent into a vector database and uses the result for retrieval.
    This can be split into two seprate steps, indexing and retrieval.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç¬¬äºŒä¸ªä»£ç†è´Ÿè´£ç´¢å¼•å’Œæ£€ç´¢éƒ¨åˆ†ã€‚å®ƒå°†å‰ä¸€ä¸ªä»£ç†çš„æ–‡æ¡£ä¿å­˜åˆ°å‘é‡æ•°æ®åº“ä¸­ï¼Œå¹¶ä½¿ç”¨ç»“æœè¿›è¡Œæ£€ç´¢ã€‚è¿™å¯ä»¥åˆ†ä¸ºä¸¤ä¸ªç‹¬ç«‹çš„æ­¥éª¤ï¼Œç´¢å¼•å’Œæ£€ç´¢ã€‚
- en: '**Step 1: Indexing the Split Document**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 1ï¼šç´¢å¼•æ‹†åˆ†æ–‡æ¡£**'
- en: 'Using the summaries generated, we vectorize them and save them in a ChromaDB
    database:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ç”Ÿæˆçš„æ€»ç»“ï¼Œæˆ‘ä»¬å°†å®ƒä»¬å‘é‡åŒ–å¹¶ä¿å­˜åˆ°ChromaDBæ•°æ®åº“ä¸­ï¼š
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `index_documents` method embeds the chunk summaries into the vector store.
    We keep metadata such as the document path and page number for later use.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`index_documents`æ–¹æ³•å°†ç‰‡æ®µæ€»ç»“åµŒå…¥åˆ°å‘é‡å­˜å‚¨ä¸­ã€‚æˆ‘ä»¬ä¿ç•™æ–‡æ¡£è·¯å¾„å’Œé¡µé¢ç¼–å·ç­‰å…ƒæ•°æ®ï¼Œä»¥å¤‡åç»­ä½¿ç”¨ã€‚'
- en: '**Step 2: Handling Questions**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 2ï¼šå¤„ç†é—®é¢˜**'
- en: When a user asks a question, the agent searches for the most relevant chunks
    in the vector store. It retrieves the summaries and corresponding page images
    for contextual understanding.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç”¨æˆ·æå‡ºé—®é¢˜æ—¶ï¼Œä»£ç†ä¼šåœ¨å‘é‡å­˜å‚¨ä¸­æœç´¢æœ€ç›¸å…³çš„ç‰‡æ®µã€‚å®ƒæ£€ç´¢æ€»ç»“å’Œç›¸åº”çš„é¡µé¢å›¾åƒä»¥ä¾¿ç†è§£ä¸Šä¸‹æ–‡ã€‚
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The retriever queries the vector store to find the chunks most relevant to the
    userâ€™s question. We then build the context for the LLM (Gemini), which combines
    text chunks and images in order to generate a response.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€ç´¢å™¨æŸ¥è¯¢å‘é‡å­˜å‚¨ä»¥æŸ¥æ‰¾ä¸ç”¨æˆ·é—®é¢˜æœ€ç›¸å…³çš„ç‰‡æ®µã€‚ç„¶åæˆ‘ä»¬ä¸ºLLMï¼ˆGeminiï¼‰æ„å»ºä¸Šä¸‹æ–‡ï¼Œç»“åˆæ–‡æœ¬ç‰‡æ®µå’Œå›¾åƒä»¥ç”Ÿæˆå›åº”ã€‚
- en: '**The full agent Workflow**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**å®Œæ•´ä»£ç†å·¥ä½œæµç¨‹**'
- en: 'The agent workflow has two stages, an indexing stage and a question answering
    stage:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£ç†çš„å·¥ä½œæµç¨‹åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼Œä¸€ä¸ªæ˜¯ç´¢å¼•é˜¶æ®µï¼Œå¦ä¸€ä¸ªæ˜¯é—®ç­”é˜¶æ®µï¼š
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Example run**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¤ºä¾‹è¿è¡Œ**'
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: With this implementation, the pipeline is complete for document processing,
    retrieval, and question answering.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æ­¤å®ç°ï¼Œç®¡é“å®Œæˆäº†æ–‡æ¡£å¤„ç†ã€æ£€ç´¢å’Œé—®ç­”åŠŸèƒ½ã€‚
- en: 'Example: Using the Document AI Pipeline'
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼šä½¿ç”¨æ–‡æ¡£AIç®¡é“
- en: Letâ€™s walk through a practical example using the document [LLM & Adaptation.pdf](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Slides/Chapter_05_Natural_Language_Processing/04-LLM%26Adaptation/LLM%20%26%20Adaptation.pdf)
    , a set of 39 slides containing text, equations, and figures (CC BY 4.0).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå®é™…ç¤ºä¾‹æ¥æ“ä½œï¼Œä½¿ç”¨æ–‡æ¡£[LLM & Adaptation.pdf](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Slides/Chapter_05_Natural_Language_Processing/04-LLM%26Adaptation/LLM%20%26%20Adaptation.pdf)ï¼Œå®ƒåŒ…å«39é¡µå¹»ç¯ç‰‡ï¼ŒåŒ…å«æ–‡æœ¬ã€å…¬å¼å’Œå›¾å½¢ï¼ˆCC
    BY 4.0ï¼‰ã€‚
- en: 'Step 1: Parsing and summarizing the Document (Agent 1)'
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬1æ­¥ï¼šè§£æå’Œæ€»ç»“æ–‡æ¡£ï¼ˆä»£ç†1ï¼‰
- en: '**Execution Time**: Parsing the 39-page document took **29 seconds**.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ‰§è¡Œæ—¶é—´**ï¼šè§£æ39é¡µæ–‡æ¡£èŠ±è´¹äº†**29ç§’**ã€‚'
- en: '**Result**: Agent 1 produces an indexed document consisting of chunk summaries
    and base64-encoded JPEG images of each page.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç»“æœ**ï¼šä»£ç†1ç”Ÿæˆäº†ä¸€ä¸ªç´¢å¼•æ–‡æ¡£ï¼Œå…¶ä¸­åŒ…å«æ¯é¡µçš„æ‘˜è¦å’ŒBase64ç¼–ç çš„JPEGå›¾ç‰‡ã€‚'
- en: 'Step 2: Questioning the Document (Agent 2)'
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬2æ­¥ï¼šè´¨ç–‘æ–‡æ¡£ï¼ˆä»£ç†2ï¼‰
- en: 'We ask the following question:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æå‡ºäº†ä»¥ä¸‹é—®é¢˜ï¼š
- en: '**â€œ**Explain LoRA, give the relevant equations**â€**'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**â€œ**è§£é‡ŠLoRAï¼Œç»™å‡ºç›¸å…³çš„å…¬å¼**â€**'
- en: 'Result:'
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“æœï¼š
- en: 'Retrieved pages:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€ç´¢çš„é¡µé¢ï¼š
- en: '![](../Images/a96c96db2579443f470a142aa9c9d1a7.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a96c96db2579443f470a142aa9c9d1a7.png)'
- en: 'Source: [LLM & Adaptation.pdf](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Slides/Chapter_05_Natural_Language_Processing/04-LLM%26Adaptation/LLM%20%26%20Adaptation.pdf)
    License CC-BY'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[LLM & Adaptation.pdf](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Slides/Chapter_05_Natural_Language_Processing/04-LLM%26Adaptation/LLM%20%26%20Adaptation.pdf)
    è®¸å¯è¯ CC-BY
- en: Response from the LLM
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¥è‡ªLLMçš„å›åº”
- en: '![](../Images/4af2dc191f5a778641c7a0e0c0b2fcb8.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4af2dc191f5a778641c7a0e0c0b2fcb8.png)'
- en: Image by author.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼šä½œè€…ã€‚
- en: The LLM was able to include equations and figures into its response by taking
    advantage of the visual context in generating a coherent and correct response
    based on the document.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: LLMèƒ½å¤Ÿé€šè¿‡åˆ©ç”¨ç”Ÿæˆè¿è´¯ä¸”æ­£ç¡®å›åº”çš„è§†è§‰ä¸Šä¸‹æ–‡ï¼Œå°†å…¬å¼å’Œå›¾å½¢çº³å…¥å…¶å›åº”ä¸­ã€‚
- en: Conclusion
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: In this quick tutorial, we saw how you can take your document AI processing
    pipeline a step further by leveraging the multi-modality of recent LLMs and using
    the full visual context available in each document, hopefully improving the quality
    of outputs that you are able to get from either your information extraction or
    RAG pipeline.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªç®€çŸ­çš„æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†å¦‚ä½•é€šè¿‡åˆ©ç”¨è¿‘æœŸLLMsçš„å¤šæ¨¡æ€æ€§ï¼Œè¿›ä¸€æ­¥æ¨è¿›ä½ çš„æ–‡æ¡£AIå¤„ç†æµç¨‹ï¼Œå¹¶åˆ©ç”¨æ¯ä¸ªæ–‡æ¡£ä¸­çš„å®Œæ•´è§†è§‰ä¸Šä¸‹æ–‡ï¼Œæ¥æå‡ä½ ä»ä¿¡æ¯æå–æˆ–RAGæµç¨‹ä¸­è·å¾—çš„è¾“å‡ºè´¨é‡ã€‚
- en: We built a stronger document segmentation step that is able to detect the important
    items like paragraphs, tables, and figures and summarize them, then used the result
    of this first step to query the collection of items and pages to give relevant
    and precise answers using Gemini. As a next step, you can try it on your use case
    and document, try to use a scalable vector database, and deploy these agents as
    part of your AI app.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ›´å¼ºå¤§çš„æ–‡æ¡£åˆ†å‰²æ­¥éª¤ï¼Œèƒ½å¤Ÿæ£€æµ‹åˆ°é‡è¦çš„é¡¹ç›®ï¼Œå¦‚æ®µè½ã€è¡¨æ ¼å’Œå›¾å½¢ï¼Œå¹¶å¯¹å…¶è¿›è¡Œæ€»ç»“ï¼Œéšåä½¿ç”¨è¿™ä¸€ç¬¬ä¸€æ­¥çš„ç»“æœæ¥æŸ¥è¯¢é¡¹ç›®å’Œé¡µé¢é›†åˆï¼Œåˆ©ç”¨Geminiç»™å‡ºç›¸å…³ä¸”ç²¾å‡†çš„ç­”æ¡ˆã€‚ä½œä¸ºä¸‹ä¸€æ­¥ï¼Œä½ å¯ä»¥åœ¨ä½ çš„ä½¿ç”¨æ¡ˆä¾‹å’Œæ–‡æ¡£ä¸Šå°è¯•ï¼Œè¯•ç€ä½¿ç”¨ä¸€ä¸ªå¯æ‰©å±•çš„å‘é‡æ•°æ®åº“ï¼Œå¹¶å°†è¿™äº›ä»£ç†éƒ¨ç½²ä¸ºä½ AIåº”ç”¨çš„ä¸€éƒ¨åˆ†ã€‚
- en: 'Full code and example are available here : [https://github.com/CVxTz/document_ai_agents](https://github.com/CVxTz/document_ai_agents)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæ•´ä»£ç å’Œç¤ºä¾‹è¯·å‚è§ï¼š[https://github.com/CVxTz/document_ai_agents](https://github.com/CVxTz/document_ai_agents)
- en: Thank you for reading ! ğŸ˜ƒ
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢é˜…è¯»ï¼ğŸ˜ƒ
