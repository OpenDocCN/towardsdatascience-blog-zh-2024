- en: 'FormulaFeatures: A Tool to Generate Highly Predictive Features for Interpretable
    Models'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FormulaFeatures：一个用于为可解释模型生成高度预测性特征的工具
- en: 原文：[https://towardsdatascience.com/formulafeatures-a-tool-to-generate-highly-predictive-features-for-interpretable-models-e18aab45e96d?source=collection_archive---------0-----------------------#2024-10-06](https://towardsdatascience.com/formulafeatures-a-tool-to-generate-highly-predictive-features-for-interpretable-models-e18aab45e96d?source=collection_archive---------0-----------------------#2024-10-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/formulafeatures-a-tool-to-generate-highly-predictive-features-for-interpretable-models-e18aab45e96d?source=collection_archive---------0-----------------------#2024-10-06](https://towardsdatascience.com/formulafeatures-a-tool-to-generate-highly-predictive-features-for-interpretable-models-e18aab45e96d?source=collection_archive---------0-----------------------#2024-10-06)
- en: Create more interpretable models by using concise, highly predictive features,
    automatically engineered based on arithmetic combinations of numeric features
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过使用简洁、高度预测性的特征，基于数值特征的算术组合自动生成，来创建更具可解释性的模型
- en: '[](https://medium.com/@wkennedy934?source=post_page---byline--e18aab45e96d--------------------------------)[![W
    Brett Kennedy](../Images/b3ce55ffd028167326c117d47c64c467.png)](https://medium.com/@wkennedy934?source=post_page---byline--e18aab45e96d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e18aab45e96d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e18aab45e96d--------------------------------)
    [W Brett Kennedy](https://medium.com/@wkennedy934?source=post_page---byline--e18aab45e96d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@wkennedy934?source=post_page---byline--e18aab45e96d--------------------------------)[![W
    Brett Kennedy](../Images/b3ce55ffd028167326c117d47c64c467.png)](https://medium.com/@wkennedy934?source=post_page---byline--e18aab45e96d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e18aab45e96d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e18aab45e96d--------------------------------)
    [W Brett Kennedy](https://medium.com/@wkennedy934?source=post_page---byline--e18aab45e96d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e18aab45e96d--------------------------------)
    ·32 min read·Oct 6, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e18aab45e96d--------------------------------)
    ·32分钟阅读·2024年10月6日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: In this article, we examine a tool called [FormulaFeatures](https://github.com/Brett-Kennedy/FormulaFeatures/tree/main).
    This is intended for use primarily with interpretable models, such as shallow
    decision trees, where having a small number of concise and highly predictive features
    can aid greatly with the interpretability and accuracy of the models.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将探讨一个名为[FormulaFeatures](https://github.com/Brett-Kennedy/FormulaFeatures/tree/main)的工具。该工具主要用于可解释模型，如浅层决策树，在这些模型中，拥有少量简洁且高度预测性的特征可以大大提高模型的可解释性和准确性。
- en: Interpretable Models in Machine Learning
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可解释模型在机器学习中的应用
- en: This article continues my series on interpretable machine learning, following
    articles on [ikNN](https://medium.com/towards-data-science/interpretable-knn-iknn-33d38402b8fc),
    [Additive Decision Trees](https://medium.com/towards-data-science/additive-decision-trees-85f2feda2223),
    [Genetic Decision Trees](https://medium.com/towards-data-science/create-stronger-decision-trees-with-bootstrapping-and-genetic-algorithms-1ae633a993c9),
    and [PRISM rules](https://medium.com/towards-data-science/prism-rules-in-python-14d2cfd801a3).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文是我关于可解释机器学习系列文章的一部分，之前的文章涉及了[ikNN](https://medium.com/towards-data-science/interpretable-knn-iknn-33d38402b8fc)、[加法决策树](https://medium.com/towards-data-science/additive-decision-trees-85f2feda2223)、[遗传决策树](https://medium.com/towards-data-science/create-stronger-decision-trees-with-bootstrapping-and-genetic-algorithms-1ae633a993c9)以及[PRISM规则](https://medium.com/towards-data-science/prism-rules-in-python-14d2cfd801a3)。
- en: 'As indicated in the previous articles (and covered there in more detail), there
    is often a strong incentive to use interpretable predictive models: each prediction
    can be well understood, and we can be confident the model will perform sensibly
    on future, unseen data.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前的文章所指出的（并且在其中做了更详细的阐述），使用可解释的预测模型往往具有强烈的驱动力：每个预测结果都能被很好地理解，并且我们可以确信该模型在未来未见数据上的表现是合理的。
- en: There are a number of models available to provide interpretable ML, although,
    unfortunately, well less than we would likely wish. There are the models described
    in the articles linked above, as well as a small number of others, for example,
    decision trees, decision tables, rule sets and rule lists (created, for example
    by [imodels](https://github.com/csinva/imodels)), [Optimal Sparse Decision Trees](https://arxiv.org/abs/1904.12847),
    GAMs (Generalized Additive Models, such as [Explainable Boosted Machines](https://interpret.ml/docs/ebm.html)),
    as well as a few other options.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 提供可解释机器学习的模型种类有很多，尽管遗憾的是，这些模型的数量远少于我们期望的。除了上述文章中描述的模型，还有一些其他模型，例如决策树、决策表、规则集和规则列表（例如由[imodels](https://github.com/csinva/imodels)创建的）、[最优稀疏决策树](https://arxiv.org/abs/1904.12847)、广义加性模型（GAMs，例如[可解释增强机](https://interpret.ml/docs/ebm.html)），以及其他一些选项。
- en: 'In general, creating predictive machine learning models that are both accurate
    and interpretable is challenging. To improve the options available for interpretable
    ML, four of the main approaches are to:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，创建既准确又可解释的预测机器学习模型是具有挑战性的。为了改善可解释机器学习的选项，四个主要的方法是：
- en: Develop additional model types
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开发更多的模型类型
- en: Improve the accuracy or interpretability of existing model types. For this,
    I’m referring to creating variations on existing model types, or the algorithms
    used to create the models, as opposed to completely novel models. For example,
    [Optimal Sparse Decision Trees](https://arxiv.org/abs/1904.12847) and [Genetic
    Decision Trees](https://medium.com/towards-data-science/create-stronger-decision-trees-with-bootstrapping-and-genetic-algorithms-1ae633a993c9)
    seek to create stronger decision trees, but in the end, are still decision trees.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提高现有模型类型的准确性或可解释性。这里指的是对现有模型类型或创建模型所使用的算法进行变体开发，而不是完全新颖的模型。例如，[最优稀疏决策树](https://arxiv.org/abs/1904.12847)和[遗传决策树](https://medium.com/towards-data-science/create-stronger-decision-trees-with-bootstrapping-and-genetic-algorithms-1ae633a993c9)旨在创建更强的决策树，但最终仍然是决策树。
- en: Provide visualizations of the data, model, and predictions made by the model.
    This is the approach taken, for example, by [ikNN](https://medium.com/towards-data-science/interpretable-knn-iknn-33d38402b8fc),
    which works by creating an ensemble of 2D kNN models (that is, ensembles of kNN
    models that each use only a single pair of features). The 2D spaces may be visualized,
    which provides a high degree of visibility into how the model works and why it
    made each prediction as it did.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供模型的数据、模型本身及其预测结果的可视化。这是例如[ikNN](https://medium.com/towards-data-science/interpretable-knn-iknn-33d38402b8fc)采用的方法，该方法通过创建2D
    kNN模型的集成（即每个kNN模型仅使用一对特征）来工作。可以将这些2D空间进行可视化，从而高度透明地展示模型是如何工作的，以及为什么做出每个预测。
- en: Improve the quality of the features that are used by the models, in order that
    models can be either more accurate or more interpretable.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 改善模型所使用特征的质量，以使模型更加准确或更加可解释。
- en: 'FormulaFeatures is used to support the last of these approaches. It was developed
    by myself to address a common issue in decision trees: they can often achieve
    a high level of accuracy, but only when grown to a large depth, which then precludes
    any interpretability. Creating new features that capture part of the function
    linking the original features to the target can allow for much more compact (and
    therefore interpretable) decision trees.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: FormulaFeatures用于支持上述的最后一种方法。它是我自己开发的，旨在解决决策树中的一个常见问题：它们往往可以实现较高的准确度，但通常需要生长到较大的深度，这会使得模型缺乏可解释性。通过创建新的特征来捕捉连接原始特征与目标之间的函数的一部分，可以使决策树更加紧凑（因此也更具可解释性）。
- en: 'The underlying idea is: for any labelled dataset, there is some true function,
    f(x) that maps the records to the target column. This function may take any number
    of forms, may be simple or complex, and may use any set of features in x. But
    regardless of the nature of f(x), by creating a model, we hope to approximate
    f(x) as well as we can given the data available. To create an interpretable model,
    we also need to do this clearly and concisely.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思想是：对于任何带标签的数据集，都存在某个真实函数f(x)，将记录映射到目标列。这个函数可以有任意形式，可能简单也可能复杂，并且可以使用x中的任何特征集。但无论f(x)的性质如何，通过创建模型，我们希望尽可能地基于现有数据逼近f(x)。为了创建一个可解释的模型，我们还需要以清晰简洁的方式做到这一点。
- en: 'If the features themselves can capture a significant part of the function,
    this can be very helpful. For example, we may have a model that predicts client
    churn and we may have features for each client including: their number of purchases
    in the last year, and the average value of their purchases in the last year. The
    true f(x), though, may be based primarily on the product of these (the total value
    of their purchases in the last year, which is found by multiplying these two features).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果特征本身能够捕捉到函数的显著部分，这将非常有帮助。例如，我们可能有一个预测客户流失的模型，并且我们为每个客户提供了包括以下内容的特征：他们在过去一年中的购买次数和购买的平均金额。然而，真实的f(x)可能主要基于这些特征的乘积（即通过这两个特征的乘积计算出的过去一年购买的总金额）。
- en: In practice, we will generally never know the true f(x), but in this case, let’s
    assume that whether a client churns in the next year is related strongly to their
    total purchases in the prior year, and not strongly to their number of purchase
    or their average size.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们通常永远无法知道真实的f(x)，但在这种情况下，我们假设客户是否在明年流失与他们前一年总购买额有很大的关系，而与他们的购买次数或平均购买金额的关系较小。
- en: We can likely build an accurate model using just the two original features,
    but a model using just the product feature will be more clear and interpretable.
    And possibly more accurate.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能仅使用这两个原始特征就能建立一个准确的模型，但如果使用仅包含乘积特征的模型，模型会更加清晰且可解释。并且可能更加准确。
- en: Example using a decision tree
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用决策树的示例
- en: 'If we have only two features, then we can view them in a 2d plot. In this case,
    we can look at just num_purc and avg_purc: the number of purchases in the last
    year per client, and their average dollar value. Assuming the true f(x) is based
    primarily on their product, the space may look like the plot below, where the
    light blue area represents client who will churn in the next year, and the dark
    blue those who will not.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只有两个特征，那么我们可以在二维图中查看它们。在这种情况下，我们可以仅查看num_purc和avg_purc：每个客户过去一年内的购买次数及其平均购买金额。假设真实的f(x)主要基于它们的产品，这个空间可能看起来像下图所示，浅蓝色区域表示将在明年流失的客户，深蓝色区域表示不会流失的客户。
- en: If using a decision tree to model this, we can create a model by dividing the
    data space recursively. The orange lines on the plot show a plausible set of splits
    a decision tree may use (for the first set of nodes) to try to predict churn.
    It may, as shown, first split on num_purc at a value of 250, then avg_purc at
    24, and so on. It would then continue to make splits in order to fit the curved
    shape of the true function. The more it makes, the closer it can come to fitting
    the true function.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用决策树来建模，我们可以通过递归地划分数据空间来创建模型。图中的橙色线条显示了一组决策树可能使用的切分（对于第一组节点）来预测流失。如图所示，决策树可能首先在num_purc的250处进行切分，然后在avg_purc的24处进行切分，依此类推。接下来，它会继续进行切分，以拟合真实函数的曲线形状。切分次数越多，它能够拟合真实函数的程度就越接近。
- en: '![](../Images/aa0c8bd9d474799c3cdb376ee63c188c.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa0c8bd9d474799c3cdb376ee63c188c.png)'
- en: 'Doing this will create a decision tree that looks something like the tree below,
    where the circles represent internal nodes, the rectangles represent the leaf
    nodes, and ellipses the sub-trees that would likely need to be grown several more
    levels deep to achieve decent accuracy. That is, this shows only a fraction of
    the full tree that would need to be grown to model this using these two features.
    We can see in the plot above as well: using axis-parallel splits, we will need
    a large number of splits to fit the boundary between the two classes well.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做会创建一个决策树，类似于下图所示，其中圆圈代表内部节点，矩形代表叶子节点，椭圆形则代表子树，这些子树可能需要再生长几个层次才能达到较好的准确性。也就是说，这里仅展示了一个完整决策树的一部分，而这个完整的决策树需要通过这两个特征来建模。我们也可以在上面的图中看到：使用轴平行切分时，我们需要大量的切分才能较好地拟合两个类别之间的边界。
- en: '![](../Images/348a0f8cd2589ba1520923269e107d4b.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/348a0f8cd2589ba1520923269e107d4b.png)'
- en: If the tree is grown sufficiently, we can likely get a strong tree in terms
    of accuracy. But, the tree will be far from interpretable.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果树已经生长得足够深，我们很可能能获得一个在准确性上非常强的树。但这个树将远非可解释。
- en: It is possible to view the decision space, as in the plot above (and this does
    make the behaviour of the model clear), but this is only feasible here because
    the space is limited to two dimensions. Normally this is impossible, and our best
    means to interpret the decision tree is to examine the tree itself. But, where
    the tree has many dozens of nodes or more, it becomes impossible to see the patterns
    it is working to capture.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 可以像上面的图表那样查看决策空间（这确实使得模型的行为变得清晰），但只有在这里空间被限制为二维时才可行。通常情况下，这是不可能的，我们解读决策树的最佳方式是检查树本身。然而，当树包含几十个节点甚至更多时，就很难看到它试图捕捉的模式。
- en: 'In this case, if we engineered a feature for num_purc * avg_purc, we could
    have a very simple decision tree, with just a single internal node, with the split
    point: num_purc * avg_purc > 25000.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，如果我们为`num_purc * avg_purc`工程化一个特征，我们可能会得到一个非常简单的决策树，只有一个内部节点，分裂点为：`num_purc
    * avg_purc > 25000`。
- en: In practice, it’s never possible to produce features that are this close to
    the true function, and it’s never possible to create a fully accurate decision
    trees with very few nodes. But it is often quite possible to engineer features
    that are closer to the true f(x) than the original features.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，永远无法生成如此接近真实函数的特征，也永远无法创建节点非常少的完全准确的决策树。但通常可以通过特征工程创建比原始特征更接近真实`f(x)`的特征。
- en: Whenever there are interactions between features, if we can capture these with
    engineered features, this will allow for more compact models.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 每当特征之间存在交互作用时，如果我们能够通过工程化特征捕捉到这些交互作用，这将有助于构建更紧凑的模型。
- en: So, with FormulaFeatures, we attempt to create features such as num_purchases
    * avg_value_of_purchases, and they can quite often be used in models such as decision
    trees to capture the true function reasonably well.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用FormulaFeatures，我们试图创建像`num_purchases * avg_value_of_purchases`这样的特征，并且它们通常可以用于决策树等模型，以合理地捕获真实的函数。
- en: As well, simply knowing that num_purchases * avg_value_of_purchases is predictive
    of the target (and that higher values are associated with lower risk of churn)
    in itself is informative. But the new feature is most useful in the context of
    seeking to make interpretable models more accurate and more interpretable.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，简单地知道`num_purchases * avg_value_of_purchases`是预测目标的关键（并且较高的值与较低的流失风险相关）本身就是一种有用的信息。但新的特征在寻求使可解释模型更准确、更具可解释性时最为有用。
- en: As we’ll describe below, FormulaFeatures also does this in a way that minimizing
    creating other features, so that only a small set of features, all relevant, are
    returned.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在下面描述的那样，FormulaFeatures也以一种最小化创建其他特征的方式做到这一点，因此只返回一小组相关的特征。
- en: Interpretable machine learning with decision trees
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可解释的机器学习与决策树
- en: With tabular data, the top-performing models for prediction problems are typically
    boosted tree-based ensembles, particularly LGBM, XGBoost, and CatBoost. It will
    vary from one prediction problem to another, but most of the time, these three
    models tend to do better than other models (and are considered, at least outside
    of AutoML approaches, the current state of the art). Other strong model types
    such as kNNs, neural networks, Bayesian Additive Regression Trees, SVMs, and others
    will also occasionally perform the best. All of these models types are, though,
    quite uninterpretable, and are effectively black-boxes.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于表格数据，预测问题的最佳模型通常是基于树的提升集成模型，特别是LGBM、XGBoost和CatBoost。虽然这会因不同的预测问题而有所不同，但大多数情况下，这三种模型往往比其他模型表现更好（并且至少在AutoML方法之外，被认为是当前的技术前沿）。其他强大的模型类型，如kNN、神经网络、贝叶斯加性回归树、SVM等，也会偶尔表现最佳。然而，所有这些模型类型都非常难以解释，实际上是黑箱模型。
- en: Unfortunately, interpretable models tend to be weaker than these with respect
    to accuracy. Sometimes, the drop in accuracy is fairly small (for example, in
    the 3rd decimal), and it’s worth sacrificing some accuracy for interpretability.
    In other cases, though, interpretable models may do substantially worse than the
    black-box alternatives. It’s difficult, for example for a single decision tree
    to compete with an ensemble of many decision trees.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，可解释的模型在准确性方面通常较弱。有时，准确度的下降非常小（例如，在第三位小数），在这种情况下，为了可解释性而牺牲一些准确性是值得的。然而，在其他情况下，可解释的模型可能比黑箱模型表现得更差。例如，对于一个单一的决策树来说，难以与多个决策树的集成模型竞争。
- en: So, it’s common to be able to create a strong black-box model, but at the same
    time for it to be challenging (or impossible) to create a strong interpretable
    model. This is the problem FormulaFeatures was designed to address. It seeks to
    capture some of logic that black-box models can represent, but in a simple, understandable
    way.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，创建一个强大的黑箱模型是很常见的，但同时要创建一个强大的可解释模型却可能具有挑战性（甚至不可能）。这就是FormulaFeatures旨在解决的问题。它试图捕捉黑箱模型能够表示的某些逻辑，但以一种简单且易于理解的方式。
- en: Much of the research done in interpretable AI focusses on decision trees, and
    relates to making decision trees more accurate and more interpretable. This is
    fairly natural, as decision trees are a model type that’s inherently straight-forward
    to understand (when sufficiently small, they are arguably as interpretable as
    any other model) and often reasonably accurate (though this is very often not
    the case).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释人工智能的许多研究集中在决策树上，并且与提高决策树的准确性和可解释性相关。这是很自然的，因为决策树是一种本质上容易理解的模型类型（当足够小的时候，决策树可以说与任何其他模型一样具有可解释性），并且通常相当准确（尽管这往往不是事实）。
- en: Other interpretable models types (e.g. logistic regression, rules, GAMs, etc.)
    are used as well, but much of the research is focused on decision trees, and so
    this article works, for the most part, with decision trees. Nevertheless, FormulaFeatures
    is not specific to decision trees, and can be useful for other interpretable models.
    In fact, it’s fairly easy to see, once we explain FormulaFeatures below, how it
    may be applied as well to ikNN, Genetic Decision Trees, Additive Decision Trees,
    rules lists, rule sets, and so on.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其他可解释模型类型（如逻辑回归、规则、广义加性模型等）也有应用，但大部分研究集中在决策树上，因此本文大部分内容都涉及决策树。然而，FormulaFeatures并非专门针对决策树，它同样可以用于其他可解释模型。事实上，一旦我们在下面解释了FormulaFeatures，您会很容易理解它如何也可以应用于ikNN、遗传决策树、加性决策树、规则列表、规则集合等。
- en: 'To be more precise with respect to decision trees, when using these for interpretable
    ML, we are looking specifically at *shallow* decision trees — trees that have
    relatively small depths, with the deepest nodes being restricted to perhaps 3,
    4, or 5 levels. This ensures two things: that shallow decision trees can provide
    both what are called *local explanations* and what are called *global explanations*.
    These are the two main concerns with interpretable ML. I’ll explain these here.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 更准确地说，在决策树的应用中，当我们使用决策树进行可解释性机器学习时，我们专门关注*浅层*决策树——这些树的深度相对较小，最深的节点可能限制在3、4或5层。这确保了两件事：首先，浅层决策树既可以提供所谓的*局部解释*，也可以提供所谓的*全局解释*。这两者是可解释性机器学习中的两个主要关注点。我将在这里解释这两者。
- en: 'With local interpretability, we want to ensure that each individual prediction
    made by the model is understandable. Here, we can examine the decision path taken
    through the tree by each record for which we generate a decision. If a path includes
    the feature num_purc * avg_purc, and the path is very short, it can be reasonably
    clear. On the other hand, a path that includes: num_purc > 250 AND avg_purc >
    24 AND num_purc < 500 AND avg_purc_50, and so on (as in the tree generated above
    without the benefit of the num_purc * avg_pur feature) can become very difficult
    to interpret.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于局部可解释性，我们希望确保模型做出的每个单独预测都是可以理解的。在这里，我们可以检查每个记录通过决策树所走的路径，以生成相应的决策。如果一条路径包含特征num_purc
    * avg_purc，并且路径非常短，那么可以合理地理解它。另一方面，如果路径包含：num_purc > 250 且 avg_purc > 24 且 num_purc
    < 500 且 avg_purc_50，等等（就像上面生成的树，但没有num_purc * avg_purc特征的帮助）则可能变得非常难以解释。
- en: With global interpretability, we want to ensure that the model as a whole is
    understandable. This allows us to see the predictions that would be made under
    any circumstances. Again, using more compact trees, and where the features themselves
    are informative, can aid with this. It’s much simpler, in this case, to see the
    big picture of how the decision tree outputs predictions.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于全局可解释性，我们希望确保整个模型是可以理解的。这使我们能够看到在任何情况下都会做出的预测。同样，使用更紧凑的树结构，并且特征本身具有信息性，可以帮助实现这一点。在这种情况下，看到决策树如何输出预测的全貌要简单得多。
- en: 'We should qualify this, though, by indicating that shallow decision trees are
    very difficult to create in a way that’s accurate for regression problems. Each
    leaf node can predict only a single value, and so a tree with n leaf nodes can
    only output, at most, n unique predictions. For regression problems, this usually
    results in high error rates: normally decision trees need to create a large number
    of leaf nodes in order to cover the full range of values that can be potentially
    predicted, with each node having reasonable precision.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们应该对这一点做出限定，指出浅层决策树在回归问题中很难以准确的方式创建。每个叶节点只能预测一个单一值，因此一个具有 n 个叶节点的树最多只能输出
    n 个不同的预测值。对于回归问题，这通常会导致较高的误差率：通常决策树需要创建大量叶节点，以涵盖所有可能预测的值范围，并且每个节点的精度要合理。
- en: Consequently, shallow decision trees tend to be practical only for classification
    problems (if there are only a small number of classes that can be predicted, it
    is quite possible to create a decision tree with not too many leaf nodes to predict
    these accurately). FormulaFeatures can be useful for use with other interpretable
    regression models, but not typically with decision trees.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，浅层决策树通常仅适用于分类问题（如果可以预测的类别数量较少，完全有可能创建一个不包含太多叶节点的决策树，准确地预测这些类别）。FormulaFeatures
    可以与其他可解释的回归模型一起使用，但通常不适用于决策树。
- en: Supervised and unsupervised feature engineering
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督式和非监督式特征工程
- en: Now that we’ve seen some of the motivation behind FormulaFeatures, we’ll take
    a look at how it works.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 FormulaFeatures 背后的部分动机，我们来看看它是如何工作的。
- en: FormulaFeatures is a form of supervised feature engineering, which is to say
    that it considers the target column when producing features, and so can generate
    features specifically useful for predicting that target. FormulaFeatures supports
    both regression & classification targets (though as indicated, when using decision
    trees, it may be that only classification targets are feasible).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: FormulaFeatures 是一种监督式特征工程方法，也就是说，它在生成特征时会考虑目标列，从而可以生成专门用于预测该目标的特征。FormulaFeatures
    支持回归和分类目标（尽管如前所述，在使用决策树时，可能只有分类目标是可行的）。
- en: Taking advantage of the target column allows it to generate only a small number
    of engineered features, each as simple or complex as necessary.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 利用目标列可以仅生成少量的工程特征，每个特征的复杂度可以根据需要进行调整。
- en: Unsupervised methods, on the other hand, do not take the target feature into
    consideration, and simply generate all possible combinations of the original features
    using some system for generating features.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，非监督方法并不考虑目标特征，而是使用某种生成特征的系统生成原始特征的所有可能组合。
- en: 'An example of this is scikit-learn’s [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html),
    which will generate all polynomial combinations of the features. If the original
    features are, say: [a, b, c], then PolynomialFeatures can create (depending on
    the parameters specified) a set of engineered features such as: [ab, ac, bc, a²,
    b², c²] — that is, it will generate all combinations of pairs of features (using
    multiplication), as well as all original features raised to the 2nd degree.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例子是 scikit-learn 的 [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)，它将生成特征的所有多项式组合。如果原始特征是，例如：[a,
    b, c]，那么 PolynomialFeatures 可以创建（根据指定的参数）一组工程特征，如：[ab, ac, bc, a², b², c²] ——也就是说，它将生成所有特征对的组合（使用乘法），以及所有原始特征的平方。
- en: Using unsupervised methods, there is very often an explosion in the number of
    features created. If we have 20 features to start with, returning just the features
    created by multiplying each pair of features would generate (20 * 19) / 2, or
    190 features (that is, 20 choose 2). If allowed to create features based on multiplying
    sets of three features, there are 20 choose 3, or 1140 of these. Allowing features
    such as a²bc, a²bc², and so on results in even more massive numbers of features
    (though with a small set of useful features being, quite possibly, among these).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用非监督方法时，通常会出现特征数量爆炸的情况。如果我们一开始有 20 个特征，仅返回通过每对特征相乘生成的特征就会产生 (20 * 19) / 2，或者说
    190 个特征（即 20 选 2）。如果允许基于三特征集相乘生成特征，则有 20 选 3，或 1140 个特征。允许生成如 a²bc、a²bc² 等特征会导致更多的特征数量膨胀（尽管在这些特征中，可能会有一些是有用的）。
- en: Supervised feature engineering methods would tend to return only a much smaller
    (and more relevant) subset of these.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 有监督的特征工程方法往往只返回这些特征的一个更小且更相关的子集。
- en: However, even within the context of supervised feature engineering (depending
    on the specific approach used), an explosion in features may still occur to some
    extent, resulting in a time consuming feature engineering process, as well as
    producing more features than can be reasonably used by any downstream tasks, such
    as prediction, clustering, or outlier detection. FormulaFeatures is optimized
    to keep both the engineering time, and the number of features returned, tractable,
    and its algorithm is designed to limit the numbers of features generated.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使在有监督的特征工程的背景下（取决于使用的具体方法），特征的爆炸性增加仍然可能在一定程度上发生，从而导致特征工程过程耗时，并产生比任何下游任务（如预测、聚类或异常检测）能够合理使用的更多特征。FormulaFeatures已优化以保持工程时间和返回特征的数量在可控范围内，其算法旨在限制生成特征的数量。
- en: Algorithm
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法
- en: The tool operates on the numeric features of a dataset. In the first iteration,
    it examines each pair of original numeric features. For each, it considers four
    potential new features based on the four basic arithmetic operations (+, -, *,
    and /). For the sake of performance, and interpretability, we limit the process
    to these four operations.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 该工具在数据集的数值特征上进行操作。在第一次迭代中，它检查每一对原始数值特征。对于每一对，它会基于四种基本算术运算（+、-、*和/）考虑四个潜在的新特征。出于性能和可解释性的考虑，我们将过程限制为这四种运算。
- en: If any perform better than both parent features (in terms of their ability to
    predict the target — described soon), then the strongest of these is added to
    the set of features. For example, if A + B and A * B are both strong features
    (both stronger than either A or B), only the stronger of these will be included.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某些特征在预测目标的能力上表现优于两个父特征（稍后将描述），那么这些特征中最强的一个会被添加到特征集合中。例如，如果A + B和A * B都是强特征（都比A或B更强），则只会包括其中更强的一个。
- en: Subsequent iterations then consider combining all features generated in the
    previous iteration will all other features, again taking the strongest of these,
    if any outperformed their two parent features. In this way, a practical number
    of new features are generated, all stronger than the previous features.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 随后的迭代将考虑将前一轮生成的所有特征与其他所有特征组合，再次选出最强的特征（如果有的话，超过它们的两个父特征）。通过这种方式，生成了一些实际可用的新特征，它们都比之前的特征强。
- en: Example stepping through the algorithm
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法示例演示
- en: Assume we start with a dataset with features A, B, and C, that Y is the target,
    and that Y is numeric (this is a regression problem).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们从一个包含特征A、B和C的数据集开始，Y是目标，且Y是数值型（这是一个回归问题）。
- en: We start by determining how predictive of the target each feature is on its
    own. The currently-available version uses R2 for regression problems and F1 (macro)
    for classification problems. We create a simple model (a classification or regression
    decision tree) using only a single feature, determine how well it predicts the
    target column, and measure this with either R2 or F1 scores.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过确定每个特征在其自身对目标的预测能力来开始。当前可用的版本对回归问题使用R2，对分类问题使用F1（宏观）。我们使用单一特征创建一个简单模型（分类或回归决策树），确定它预测目标列的效果，并通过R2或F1分数来衡量。
- en: Using a decision tree allows us to capture reasonably well the relationships
    between the feature and target — even fairly complex, non-monotonic relationships
    — where they exist.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用决策树使我们能够较好地捕捉特征与目标之间的关系——即使是相当复杂、非单调的关系——如果这些关系存在的话。
- en: Future versions will support more metrics. Using strictly R2 and F1, however,
    is not a significant limitation. While other metrics may be more relevant for
    your projects, using these metrics internally when engineering features will identify
    well the features that are strongly associated with the target, even if the strength
    of the association is not identical as it would be found using other metrics.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 未来版本将支持更多的度量标准。然而，严格使用R2和F1并不是一个显著的限制。虽然其他度量可能对你的项目更相关，但在进行特征工程时使用这些度量标准能够很好地识别出与目标强相关的特征，即使这些特征的关联强度可能与使用其他度量时的结果不完全相同。
- en: 'In this example, we begin with calculating the R2 for each original feature,
    training a decision tree using only feature A, then another using only B, and
    then again using only C. This may give the following R2 scores:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们首先计算每个原始特征的R2值，使用仅特征A训练决策树，然后使用仅特征B，再使用仅特征C。可能得到以下R2分数：
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We then consider the combinations of pairs of these, which are: A & B, A &
    C, and B & C. For each we try the four arithmetic operations: +, *, -, and /.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们考虑这些特征对的组合，它们是：A & B、A & C和B & C。对于每一对，我们尝试四种算术操作：+、*、-和/。
- en: Where there are feature interactions in f(x), it will often be that a new feature
    incorporating the relevant original features can represent the interactions well,
    and so outperform either parent feature.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果f(x)中存在特征交互，通常相关的原始特征可以通过新特征的组合来很好地表示这些交互，从而表现得比任何父特征都要好。
- en: 'When examining A & B, assume we get the following R2 scores:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当检查A & B时，假设我们得到以下R2分数：
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here there are two operations that have a higher R2 score than either parent
    feature (A or B), which are + and *. We take the highest of these, A + B, and
    add this to the set of features. We do the same for A & B and B & C. In most cases,
    no feature will be added, but often one is.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，有两个操作的R2分数高于任何父特征（A或B），它们是+和*。我们选择其中较高的A + B，并将其加入特征集。同样的操作适用于A & B和B &
    C。在大多数情况下，不会添加任何特征，但通常会添加一个。
- en: 'After the first iteration we may have:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次迭代后，我们可能会得到：
- en: '[PRE2]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We then, in the next iteration, take the two features just added, and try combining
    them with all other features, including each other.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在下一次迭代中，我们将把刚刚添加的两个特征与所有其他特征进行组合，包括彼此之间的组合。
- en: 'After this we may have:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这之后，我们可能得到：
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This continues until there is no longer improvement, or a limit specified by
    a hyperparameter, max_iterations, is reached.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程会持续，直到不再有改进，或者达到超参数`max_iterations`所指定的限制。
- en: Further pruning based on correlations
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于相关性进一步修剪
- en: At the end of each iteration, further pruning of the features is performed,
    based on correlations. The correlation among the features created during the current
    iteration is examined, and where two or more features that are highly correlated
    were created, only the strongest is kept, removing the others. This limits creating
    near-redundant features, which can become possible, especially as the features
    become more complex.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 每次迭代结束后，会根据特征之间的相关性进一步修剪特征。检查当前迭代中创建的特征之间的相关性，对于两个或多个高度相关的特征，保留最强的一个，删除其他特征。这可以避免创建近乎冗余的特征，尤其是当特征变得更加复杂时，这种情况尤为明显。
- en: 'For example: (A + B + C) / E and (A + B + D) / E may both be strong, but quite
    similar, and if so, only the stronger of these will be kept.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：(A + B + C) / E 和 (A + B + D) / E 可能都很强，但非常相似，如果是这样，只有其中更强的一个会被保留。
- en: One allowance for correlated features is made, though. In general, as the algorithm
    proceeds, more complex features are created, and these features more accurately
    capture the true relationship between the features in x and the target. But, the
    new features created may also be correlated with the features they build upon,
    which are simpler, and FormulaFeatures also seeks to favour simpler features over
    more complex, everything else equal.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 但对于相关特征，还是有一定的容许度。一般来说，随着算法的进行，会创建更多复杂的特征，这些特征更准确地捕捉x中的特征与目标之间的真实关系。然而，创建的新特征可能与其基础的较简单特征相关联，FormulaFeatures也会倾向于优先选择较简单的特征，而不是复杂的特征，其他条件相同的情况下。
- en: For example, if (A + B + C) is correlated with (A + B), both would be kept even
    if (A + B + C) is stronger, in order that the simpler (A + B) may be combined
    with other features in subsequent iterations, possibly creating features that
    are stronger still.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果(A + B + C)与(A + B)相关联，那么即使(A + B + C)更强，也会保留这两个特征，以便后续迭代中可以将较简单的(A + B)与其他特征组合，可能会创建出更强的特征。
- en: How FormulaFeatures limits the features created
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FormulaFeatures如何限制所创建的特征
- en: In the example above, we have features A, B, and C, and see that part of the
    true f(x) can be approximated with (A + B) - C.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的示例中，我们有特征A、B和C，并看到真实的f(x)的一部分可以用(A + B) - C来近似。
- en: We initially have only the original features. After the first iteration, we
    may generate (again, as in the example above) A + B and B / C, so now have five
    features.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最初只有原始特征。在第一次迭代后，我们可能生成（如上例所示）A + B和B / C，因此现在有五个特征。
- en: In the next iteration, we may generate (A + B) — C.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一次迭代中，我们可能生成(A + B) — C。
- en: 'This process is, in general, a combination of: 1) combining weak features to
    make them stronger (and more likely useful in a downstream task); as well as 2)
    combining strong features to make these even stronger, creating what are most
    likely the most predictive features.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程通常是以下两者的结合：1) 将弱特征结合起来，使其更强大（并且更可能在下游任务中有用）；以及2) 将强特征结合起来，使其更强大，创建最有可能是最具预测性的特征。
- en: But, what’s important is that this combining is done only after it’s confirmed
    that A + B is a predictive feature in itself, more so than either A or B. That
    is, we do not create (A + B) — C until we confirm that A + B is predictive. This
    ensures that, for any complex features created, each component within them is
    useful.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，重要的是，这种组合仅在确认A + B本身是一个具有预测性的特征时才会进行，而不是单独的A或B。这意味着，在确认A + B具有预测性之前，我们不会创建(A
    + B) — C。这确保了，对于任何创建的复杂特征，特征中的每个组件都是有用的。
- en: In this way, each iteration creates a more powerful set of features than the
    previous, and does so in a way that’s reliable and stable. It minimizes the effects
    of simply trying many complex combinations of features, which can easily overfit.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式，每次迭代都会创建比之前更强大的特征集，并且以一种可靠且稳定的方式进行。它最大限度地减少了简单地尝试许多复杂特征组合的效果，这种做法很容易导致过拟合。
- en: So, FormulaFeatures, executes in a principled, deliberate manner, creating only
    a small number of engineered features each step, and typically creates less features
    each iteration. As such, it, overall, favours creating features with low complexity.
    And, where complex features are generated, this can be shown to be justified.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，FormulaFeatures以一种有原则、深思熟虑的方式执行，每一步只创建少量工程化特征，并且每次迭代通常生成较少的特征。因此，总体而言，它倾向于创建低复杂度的特征。对于生成的复杂特征，可以证明这些特征是合理的。
- en: With most datasets, in the end, the features engineered are combinations of
    just two or three original features. That is, it will usually create features
    more similar to A * B than to, say, (A * B) / (C * D).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数数据集来说，最终生成的工程化特征通常是仅由两个或三个原始特征组合而成的。也就是说，它通常会生成更像A * B这样的特征，而不是像(A * B)
    / (C * D)这样的组合。
- en: In fact, to generate a features such as (A * B) / (C * D), it would need to
    demonstrate that A * B is more predictive than either A or B, that C * D is more
    predictive that C or D, and that (A * B) / (C * D) is more predictive than either
    (A * B) or (C * D). As that’s a lot of conditions, relatively few features as
    complex as (A * B) / (C * D) will tend to be created, many more like A * B.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，要生成像(A * B) / (C * D)这样的特征，需要先证明A * B比A或B更具预测性，C * D比C或D更具预测性，并且(A * B)
    / (C * D)比(A * B)或(C * D)更具预测性。由于这有许多条件，相对来说，像(A * B) / (C * D)这样复杂的特征生成的机会较少，更多的特征会像A
    * B。
- en: Using 1D decision trees internally to evaluate the features
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用1D决策树在内部评估特征。
- en: We’ll look here closer at using decision trees internally to evaluate each feature,
    both the original and the engineered features.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里更详细地讨论如何使用决策树来评估每个特征，包括原始特征和工程化特征。
- en: 'To evaluate the features, other methods are available, such as simple correlation
    tests. But creating simple, non-parametric models, and specifically decision trees,
    has a number of advantages:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估特征，还可以使用其他方法，如简单的相关性测试。但创建简单的非参数模型，特别是决策树，具有一些优势：
- en: 1D models are fast, both to train and to test, which allows the evaluation process
    to execute very quickly. We can quickly determine which engineered features are
    predictive of the target, and how predictive they are.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1D模型训练和测试都非常快速，这使得评估过程能够非常迅速地执行。我们可以快速确定哪些工程化特征能够预测目标，以及它们的预测效果如何。
- en: 1D models are simple and so may reasonably be trained on small samples of the
    data, further improving efficiency.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1D模型比较简单，因此可以合理地在小样本数据上训练，从而进一步提高效率。
- en: While 1D decision tree models are relatively simple, they can capture non-monotonic
    relationships between the features and the target, so can detect where features
    are predictive even where the relationships are complex enough to be missed by
    simpler tests, such as tests for correlation.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然1D决策树模型相对简单，但它们能够捕捉特征与目标之间的非单调关系，因此可以检测出特征的预测性，即使这些关系复杂到简单的相关性测试可能会遗漏的程度。
- en: This ensures all features useful in themselves, so supports the features being
    a form of interpretability in themselves.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这确保了所有特征在自身上都有用，因此支持这些特征本身就是一种可解释性。
- en: 'There are also some limitations of using 1D models to evaluate each feature,
    particularly: using single features precludes identifying effective combinations
    of features. This may result in missing some useful features (features that are
    not useful by themselves but are useful in combination with other features), but
    does allow the process to execute very quickly. It also ensures that all features
    produced are predictive on their own, which does aid in interpretability.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一维模型评估每个特征也存在一些局限性，特别是：使用单一特征会排除识别有效的特征组合。这可能导致错过一些有用的特征（那些单独不有用但与其他特征结合时有用的特征），但可以使得处理过程执行得非常快速。它还确保所有生成的特征本身都是可预测的，这有助于提高可解释性。
- en: 'The goal is that: where features are useful only in combination with other
    features, a new feature is created to capture this.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是：当特征仅在与其他特征结合时才有用时，创建一个新的特征来捕捉这一点。
- en: Another limitation associated with this form of feature engineering is that
    almost all engineered features will have global significance, which is often desirable,
    but it does mean the tool can miss additionally generating features that are useful
    only in specific sub-spaces. However, given that the features will be used by
    interpretable models, such as shallow decision trees, the value of features that
    are predictive in only specific sub-spaces is much lower than where more complex
    models (such as large decision trees) are used.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这种特征工程方式的另一个局限性是，几乎所有经过工程处理的特征都会具有全局意义，这通常是期望的，但这也意味着该工具可能会遗漏生成那些仅在特定子空间中有用的特征。然而，考虑到这些特征将被可解释的模型使用，比如浅层决策树，只有在特定子空间中有效的特征的价值远低于使用更复杂模型（如大型决策树）时的情况。
- en: Implications for the complexity of decision trees
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树复杂性的影响
- en: FormulaFeatures does create features that are inherently more complex than the
    original features, which does lower the interpretability of the trees (assuming
    the engineered features are used by the trees one or more times).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: FormulaFeatures 确实创建了比原始特征更复杂的特征，这确实降低了树的可解释性（假设这些工程化特征被树使用一次或多次）。
- en: At the same time, using these features can allow substantially smaller decision
    trees, resulting in a model that is, over all, more accurate and more interpretable.
    That is, even though the features used in a tree may be complex, the tree, may
    be substantially smaller (or substantially more accurate when keeping the size
    to a reasonable level), resulting in a net gain in interpretability.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，使用这些特征可以允许更小的决策树，从而使得模型总体上更加准确且易于解释。也就是说，尽管树中使用的特征可能很复杂，但树本身可能会显著更小（或者在保持大小在合理范围内时，准确性大幅提高），从而在可解释性上实现净收益。
- en: When FormulaFeatures is used with shallow decision trees, the engineered features
    generated tend to be put at the top of the trees (as these are the most powerful
    features, best able to maximize information gain). No single feature can ever
    split the data perfectly at any step, which means further splits are almost always
    necessary. Other features are used lower in the tree, which tend to be simpler
    engineered features (based only only two, or sometimes three, original features),
    or the original features. On the whole, this can produce fairly interpretable
    decision trees, and tends to limit the use of the more complex engineered features
    to a useful level.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当 FormulaFeatures 与浅层决策树结合使用时，生成的工程特征往往会被放在树的顶部（因为这些特征最强大，最能最大化信息增益）。没有任何单一特征能在任何一步完美地划分数据，这意味着几乎总是需要进一步的分裂。其他特征则被用在树的更低层次，这些特征往往是更简单的工程特征（仅基于两个，或者有时三个，原始特征），或者是原始特征。总体而言，这可以生成相当易于解释的决策树，并且往往将更复杂的工程特征的使用限制在一个有用的水平。
- en: ArithmeticFeatures
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ArithmeticFeatures
- en: To explain better some of the context for FormulaFeatures, I’ll describe another
    tool, also developed by myself, called [ArithmeticFeatures](https://github.com/Brett-Kennedy/ArithmeticFeatures),
    which is similar but somewhat simpler. We’ll then look at some of the limitations
    associated with ArithmeticFeatures that FormulaFeatures was designed to address.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地解释 FormulaFeatures 的一些背景，我将介绍另一个工具，也是我自己开发的，叫做 [ArithmeticFeatures](https://github.com/Brett-Kennedy/ArithmeticFeatures)，它类似但略为简单。接着我们将探讨
    ArithmeticFeatures 的一些局限性，而 FormulaFeatures 则是为了克服这些问题而设计的。
- en: ArithmeticFeatures is a simple tool, but one I’ve found useful in a number of
    projects. I initially created it, as it was a recurring theme that it was useful
    to generate a set of simple arithmetic combinations of the numeric features available
    for various projects I was working on. I then hosted it on [github](https://github.com/Brett-Kennedy/ArithmeticFeatures).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ArithmeticFeatures 是一个简单的工具，但我在多个项目中发现它非常有用。我最初创建它，是因为在我所从事的各种项目中，生成一组简单的算术组合来处理可用的数值特征是一个经常出现的需求。之后，我将其托管在
    [github](https://github.com/Brett-Kennedy/ArithmeticFeatures) 上。
- en: Its purpose, and its signature, are similar to scikit-learn’s PolynomialFeatures.
    It’s also an unsupervised feature engineering tool.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 它的目的和特点类似于 scikit-learn 的 PolynomialFeatures。它也是一个无监督的特征工程工具。
- en: 'Given a set of numeric features in a dataset, it generates a collection of
    new features. For each pair of numeric features, it generates four new features:
    the result of the +, -, * and / operations.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个数据集中的数值特征集，它会生成一组新的特征。对于每一对数值特征，它会生成四个新特征：加法、减法、乘法和除法操作的结果。
- en: This can generate a set of features that are useful, but also generates a very
    large set of features, and potentially redundant features, which means feature
    selection is necessary after using this.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以生成一组有用的特征，但也会生成大量的特征，且可能包含冗余特征，这意味着在使用后需要进行特征选择。
- en: 'Formula Features was designed to address the issue that, as indicated above,
    frequently occurs with unsupervised feature engineering tools including ArithmeticFeatures:
    an explosion in the numbers of features created. With no target to guide the process,
    they simply combine the numeric features in as many ways are are possible.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Formula Features 旨在解决如上所述的问题，这个问题通常出现在包括 ArithmeticFeatures 在内的无监督特征工程工具中：即特征数量的爆炸性增长。由于没有目标来引导过程，它们只是以可能的方式将数值特征进行组合。
- en: 'To quickly list the differences:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 快速列出差异：
- en: FormulaFeatures will generate far fewer features, but each that it generates
    will be known to be useful. ArithmeticFeatures provides no check as to which features
    are useful. It will generate features for every combination of original features
    and arithmetic operation.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FormulaFeatures 会生成更少的特征，但每一个生成的特征都会被确认是有用的。而 ArithmeticFeatures 则没有检查哪些特征是有用的。它会生成所有原始特征和算术操作组合的特征。
- en: FormulaFeatures will only generate features that are more predictive than either
    parent feature.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FormulaFeatures 只会生成比其父特征更具预测性的特征。
- en: For any given pair of features, FormulaFeatures will include at most one combination,
    which is the one that is most predictive of the target.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于任何给定的特征对，FormulaFeatures 最多会包含一个组合，这个组合是最能预测目标的组合。
- en: FormulaFeatures will continue looping for either a specified number of iterations,
    or so long as it is able to create more powerful features, and so can create more
    powerful features than ArithmeticFeatures, which is limited to features based
    on pairs of original features.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FormulaFeatures 会继续循环，直到达到指定的迭代次数，或者只要它能创建更强大的特征，因此能够生成比 ArithmeticFeatures
    更强的特征，后者仅限于基于原始特征对的特征。
- en: ArithmeticFeatures, as it executes only one iteration (in order to manage the
    number of features produced), is often quite limited in what it can create.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ArithmeticFeatures 由于只执行一次迭代（以管理生成的特征数量），通常在它能够创建的特征上有很大限制。
- en: 'Imagine a case where the dataset describes houses and the target feature is
    the house price. This may be related to features such as num_bedrooms, num_bathrooms
    and num_common rooms. Likely it is strongly related to the total number of rooms,
    which, let’s say, is: num_bedrooms + num_bathrooms + num_common rooms. ArithmeticFeatures,
    however is only able to produce engineered features based on pairs of original
    features, so can produce:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 假设数据集描述的是房屋，目标特征是房价。这可能与诸如 num_bedrooms、num_bathrooms 和 num_common rooms 等特征相关。很可能与房屋的总房间数强相关，假设它是：num_bedrooms
    + num_bathrooms + num_common rooms。然而，ArithmeticFeatures 只能基于原始特征对生成工程特征，因此只能生成：
- en: num_bedrooms + num_bathrooms
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: num_bedrooms + num_bathrooms
- en: num_bedrooms + num_common rooms
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: num_bedrooms + num_common rooms
- en: num_bathrooms + num_common rooms
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: num_bathrooms + num_common rooms
- en: These may be informative, but producing num_bedrooms + num_bathrooms + num_common
    rooms (as FormulaFeatures is able to do) is both more clear as a feature, and
    allows more concise trees (and other interpretable models) than using features
    based on only pairs of original features.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这些可能是有信息量的，但生成 num_bedrooms + num_bathrooms + num_common rooms（如 FormulaFeatures
    所能做到的）作为特征，不仅更清晰，而且比仅使用原始特征对的特征生成更简洁的树（和其他可解释的模型）。
- en: Another popular feature engineering tool based on arithmetic operations is [AutoFeat](https://github.com/cod3licious/autofeat),
    which works similarly to ArithmeticFeatures, and also executes in an unsupervised
    manner, so will create a very large number of features. AutoFeat is able it to
    execute for multiple iterations, creating progressively more complex features
    each iterations, but with increasing large numbers of them. As well, AutoFeat
    supports unary operations, such as square, square root, log and so on, which allows
    for features such as A²/log(B).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个基于算术运算的流行特征工程工具是 [AutoFeat](https://github.com/cod3licious/autofeat)，它与 ArithmeticFeatures
    类似，也以无监督的方式执行，因此会生成非常大量的特征。AutoFeat 可以执行多个迭代，在每次迭代中生成更复杂的特征，但数量也会增加。此外，AutoFeat
    支持一元操作，例如平方、平方根、对数等，这使得它可以生成如 A²/log(B) 这样的特征。
- en: 'So, I’ve gone over the motivations to create, and to use, FormulaFeatures over
    unsupervised feature engineering, but should also say: unsupervised methods such
    as PolynomialFeatures, ArithmeticFeatures, and AutoFeat are also often useful,
    particularly where feature selection will be performed in any case.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我已经讲过了使用 FormulaFeatures 而不是无监督特征工程的动机，但也应该提到：像 PolynomialFeatures、ArithmeticFeatures
    和 AutoFeat 这样的无监督方法通常也是有用的，特别是在任何情况下都会进行特征选择时。
- en: FormulaFeatures focuses more on interpretability (and to some extent on memory
    efficiency, but the primary motivation was interpretability), and so has a different
    purpose.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: FormulaFeatures 更注重可解释性（在某种程度上也考虑了内存效率，但主要动机是可解释性），因此它有不同的目的。
- en: Feature selection with feature engineering
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用特征工程进行特征选择
- en: Using unsupervised feature engineering tools such as PolynomialFeatures, ArithmeticFeatures,
    and AutoFeat increases the need for feature selection, but feature selection is
    generally performed in any case.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 使用像 PolynomialFeatures、ArithmeticFeatures 和 AutoFeat 这样的无监督特征工程工具增加了对特征选择的需求，但特征选择通常在任何情况下都会执行。
- en: That is, even if using a supervised feature engineering method such as FormulaFeatures,
    it will generally be useful to perform some feature selection after the feature
    engineering process. In fact, even if the feature engineering process produces
    no new features, feature selection is likely still useful simply to reduce the
    number of the original features used in the model.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，即使使用像 FormulaFeatures 这样的监督式特征工程方法，通常在特征工程过程之后进行一些特征选择仍然是有用的。事实上，即使特征工程过程没有产生新的特征，特征选择仍然可能有用，单纯是为了减少模型中使用的原始特征数量。
- en: While FormulaFeatures seeks to minimize the number of features created, it does
    not perform feature selection per se, so can generate more features than will
    be necessary for any given task. We assume the engineered features will be used,
    in most cases, for a prediction task, but the relevant features will still depend
    on the specific model used, hyperparameters, evaluation metrics, and so on, which
    FormulaFeatures cannot predict
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 FormulaFeatures 尽力减少创建的特征数量，但它本身并不执行特征选择，因此可能会生成比任何给定任务所需更多的特征。我们假设在大多数情况下，工程化的特征将用于预测任务，但相关特征仍然取决于使用的特定模型、超参数、评估指标等，而这些是
    FormulaFeatures 无法预测的。
- en: What can be relevant is that, using FormulaFeatures, as compared to many other
    feature engineering processes, the feature selection work, if performed, can be
    a much simpler process, as there will be far few features to consider. Feature
    selection can become slow and difficult when working with many features. For example,
    wrapper methods to select features become intractable.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 相关的是，与许多其他特征工程过程相比，使用 FormulaFeatures 时，特征选择工作（如果执行的话）可以变得更简单，因为需要考虑的特征会少得多。处理许多特征时，特征选择可能会变得缓慢且困难。例如，使用包装方法选择特征时会变得不可行。
- en: API Signature
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: API 签名
- en: The tool uses the fit-transform pattern, the same as that used by scikit-learn’s
    PolynomialFeatures and many other feature engineering tools (including ArithmeticFeatures).
    As such, it’s easy to substitute this tool for others to determine which is the
    most useful for any given project.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 该工具采用了 fit-transform 模式，与 scikit-learn 的 PolynomialFeatures 以及许多其他特征工程工具（包括
    ArithmeticFeatures）使用的模式相同。因此，可以轻松地将此工具替换为其他工具，以确定哪一个最适合任何给定的项目。
- en: Simple code example
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单代码示例
- en: In this example, we load the iris data set (a toy dataset provided by scikit-learn),
    split the data into train and test sets, use FormulaFeatures to engineer a set
    of additional features, and fit a Decision Tree using these.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们加载了 iris 数据集（这是一个由 scikit-learn 提供的玩具数据集），将数据拆分为训练集和测试集，使用 FormulaFeatures
    来工程化一组附加特征，并使用这些特征拟合决策树模型。
- en: This is fairly typical example. Using FormulaFeatures requires only creating
    a FormulaFeatures object, fitting it, and transforming the available data. This
    produces a new dataframe that can be used for any subsequent tasks, in this case
    to train a classification model.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当典型的示例。使用 FormulaFeatures 只需要创建一个 FormulaFeatures 对象，进行拟合并转换可用数据。这将生成一个新的数据框架，可用于后续任务，在本例中用于训练分类模型。
- en: '[PRE4]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Setting the tool to execute with verbose=1 or verbose=2 allows viewing the process
    in greater detail.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 将工具设置为 verbose=1 或 verbose=2 允许更详细地查看过程。
- en: The github page also provides a file called demo.py, which provides some examples
    using FormulaFeatures, though the signature is quite simple.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: github 页面还提供了一个名为 demo.py 的文件，里面包含了一些使用 FormulaFeatures 的示例，尽管其签名非常简单。
- en: Example getting feature scores
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取特征得分示例
- en: Getting the feature scores, which we show in this example, may be useful for
    understanding the features generated and for feature selection.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 获取特征得分（我们在此示例中展示的）可能有助于理解生成的特征以及进行特征选择。
- en: In this example, we use the gas-drift dataset from openml ([https://www.openml.org/search?type=data&sort=runs&id=1476&status=active](https://www.openml.org/search?type=data&sort=runs&id=1476&status=active),
    licensed under Creative Commons).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们使用来自 openml 的 gas-drift 数据集（[https://www.openml.org/search?type=data&sort=runs&id=1476&status=active](https://www.openml.org/search?type=data&sort=runs&id=1476&status=active)，基于
    Creative Commons 许可证）。
- en: It largely works the same as the previous example, but also makes a call to
    the display_features() API, which provides information about the features engineered.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 它与之前的示例大致相同，但还调用了 display_features() API，该 API 提供了有关工程化特征的信息。
- en: '[PRE5]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This will produce the following report, listing each feature index, F1 macro
    score, and feature name:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下报告，列出每个特征索引、F1 宏观得分和特征名称：
- en: '[PRE6]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This includes the original features (features 0 through 9) for context. In this
    example, there is a steady increase in the predictive power of the features engineered.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括了原始特征（特征 0 到 9）以提供上下文。在这个示例中，经过工程化的特征在预测能力上稳步提升。
- en: 'Plotting is also provided. In the case of regression targets, the tool presents
    a scatter plot mapping each feature to the target. In the case of classification
    targets, the tool presents a boxplot, giving the distribution of a feature broken
    down by class label. It is often the case that the original features show little
    difference in distributions per class, while engineered features can show a distinct
    difference. For example, one feature generated, (V99 / V47) — (V81 / V5) shows
    a strong separation:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 也提供了绘图功能。在回归目标的情况下，工具会呈现一个散点图，将每个特征映射到目标。在分类目标的情况下，工具会呈现一个箱线图，展示按类别标签划分的特征分布。通常情况下，原始特征在每个类别中的分布差异不大，而工程化特征则能显示出明显的差异。例如，生成的一个特征
    (V99 / V47) - (V81 / V5) 显示出强烈的分离：
- en: '![](../Images/d42772d16536ce9f26733d3b4de86ab6.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d42772d16536ce9f26733d3b4de86ab6.png)'
- en: The separation isn’t perfect, but is cleaner than with any of the original features.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 分离度虽然不完美，但比任何原始特征都更干净。
- en: This is typical of the features engineered; while each has an imperfect separation,
    each is strong, often much more so than for the original features.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是特征工程的典型表现；虽然每个特征的分离并不完美，但每个特征都很强大，通常比原始特征强得多。
- en: Test Results
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试结果
- en: Testing was performed on synthetic and real data. The tool performed very well
    on the synthetic data, though this provides more debugging and testing than meaningful
    evaluation. For real data, a set of 80 random classification datasets from OpenML
    were selected, though only those having at least two numeric features could be
    included, leaving 69 files. Testing consisted of performing a single train-test
    split on the data, then training and evaluating a model on the numeric feature
    both before and after engineering additional features.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 测试在合成数据和真实数据上进行。工具在合成数据上表现非常好，尽管这更多的是调试和测试，而非有意义的评估。对于真实数据，选择了来自 OpenML 的 80
    个随机分类数据集，但只有包含至少两个数值特征的数据集才能被纳入，最终留下了 69 个文件。测试包括对数据进行一次单一的训练-测试分割，然后在数值特征上训练并评估模型，分别在增加特征之前和之后进行。
- en: Macro F1 was used as the evaluation metric, evaluating a scikit-learn DecisionTreeClassifer
    with and without the engineered features, setting setting max_leaf_nodes = 10
    (corresponding to 10 induced rules) to ensure an interpretable model.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 使用宏观 F1 作为评估指标，评估带和不带工程化特征的 scikit-learn DecisionTreeClassifer，并设置 max_leaf_nodes
    = 10（对应 10 个生成的规则）以确保模型具有可解释性。
- en: In many cases, the tool provided no improvement, or only slight improvements,
    in the accuracy of the shallow decision trees, as is expected. No feature engineering
    technique will work in all cases. More important is that the tool led to significant
    increases inaccuracy an impressive number of times. This is without tuning or
    feature selection, which can further improve the utility of the tool.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，工具对浅层决策树的准确性没有提供改进，或者仅提供了轻微的改进，这是可以预期的。没有任何特征工程技术能在所有情况下都有效。更重要的是，工具在许多情况下显著提高了准确性，这令人印象深刻。这是在没有调优或特征选择的情况下完成的，调优和特征选择可以进一步提高工具的有效性。
- en: Using other interpretable models will give different results, possibly stronger
    or weaker than was found with shallow decision trees, which did have show quite
    strong results.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 使用其他可解释模型会得到不同的结果，可能比浅层决策树表现得更强或更弱，而浅层决策树确实表现出了相当强的结果。
- en: In these tests we found better results limiting max_iterations to 2 compared
    to 3\. This is a hyperparameter, and must be tuned for different datasets. For
    most datasets, using 2 or 3 works well, while with others, setting higher, even
    much higher (setting it to None allows the process to continue so long as it can
    produce more effective features), can work well.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些测试中，我们发现将 max_iterations 限制为 2，比设置为 3 时得到更好的结果。这是一个超参数，必须针对不同的数据集进行调优。对于大多数数据集，使用
    2 或 3 都能获得不错的结果，而对于其他数据集，设置更高，甚至更高（将其设置为 None 允许过程继续，只要能够生成更有效的特征）也可能表现良好。
- en: In most cases, the time engineering the new features was just seconds, and in
    all cases was under two minutes, even with many of the test files having hundreds
    of columns and many thousands of rows.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，工程化新特征的时间仅为几秒钟，在所有情况下都没有超过两分钟，即使许多测试文件有数百列和几千行。
- en: 'The results were:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE7]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The model performed better with, than without, Formula Features feature engineering
    49 out of 69 cases. Some noteworthy examples are:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在69个案例中，模型在有无公式特征工程的情况下表现更好，49个案例有明显改进。一些值得注意的例子包括：
- en: Japanese Vowels improved from .57 to .68
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Japanese Vowels 从 .57 提升到 .68
- en: gas-drift improved from .74 to .83
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: gas-drift 从 .74 提升到 .83
- en: hill-valley improved from .52 to .74
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: hill-valley 从 .52 提升到 .74
- en: climate-model-simulation-crashes improved from .47 to .64
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: climate-model-simulation-crashes 从 .47 提升到 .64
- en: banknote-authentication improved from .95 to .99
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: banknote-authentication 从 .95 提升到 .99
- en: page-blocks improved from .66 to .81
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: page-blocks 从 .66 提升到 .81
- en: Using Engineered Features with strong predictive models
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用具有强预测能力的模型和工程化特征
- en: We’ve looked so far primarily at shallow decision trees in this article, and
    have indicated that FormulaFeatures can also generate features useful for other
    interpretable models. But, this leaves the question of their utility with more
    powerful predictive models. On the whole, FormulaFeatures is not useful in combination
    with these tools.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，我们主要关注浅层决策树，并指出公式特征也可以生成对其他可解释模型有用的特征。但这也留下了它们在更强预测模型中的实用性问题。总体而言，公式特征与这些工具结合使用并不总是有效。
- en: For the most part, strong predictive models such as boosted tree models (e.g.,
    CatBoost, LGBM, XGBoost), will be able to infer the patterns that FormulaFeatures
    captures in any case. Though they will capture these patterns in the form of large
    numbers of decision trees, combined in an ensemble, as opposed to single features,
    the effect will be the same, and may often be stronger, as the trees are not limited
    to simple, interpretable operators (+, -, *, and /).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，强大的预测模型，如提升树模型（例如 CatBoost、LGBM、XGBoost），通常能够推断出 FormulaFeatures 捕捉的模式。虽然它们将这些模式以大量决策树的形式捕捉，并将它们组合成一个集成模型，而不是单一特征，但效果是相同的，而且通常可能更强，因为这些树并不局限于简单的、可解释的操作符（+、-、*
    和 /）。
- en: So, there may not be an appreciable gain in accuracy using engineered features
    with strong models, even where they match the true f(x) closely. It can be worth
    trying FormulaFeatures in this case, and I’ve found it helpful with some projects,
    but most often the gain is minimal.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，即使经过精心设计的特征与真实的 f(x) 非常接近，使用强模型时，可能不会在准确性上获得显著的提升。在这种情况下，尝试使用 FormulaFeatures
    可能是值得的，我在一些项目中发现它很有帮助，但大多数情况下，增益是最小的。
- en: It’s really with smaller (interpretable) models where tools such as FormulaFeatures
    become most useful.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，正是在较小的（可解释的）模型中，像 FormulaFeatures 这样的工具才变得最为有用。
- en: Working with very large numbers of original features
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理具有大量原始特征的数据
- en: One limitation of feature engineering based on arithmetic operations is that
    it can be slow where there are a very large number of original features, and it’s
    relatively common in data science to encounter tables with hundreds of features,
    or more. This affects unsupervised feature engineering methods much more severely,
    but supervised methods can also be significantly slowed down.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 基于算术运算的特征工程的一个限制是，当原始特征的数量非常庞大时，它可能会变得非常慢，在数据科学中，遇到包含数百个特征的表格是相对常见的。这个问题对无监督特征工程方法的影响更加严重，但监督方法也可能显著变慢。
- en: In these cases, creating even pairwise engineered features can also invite overfitting,
    as an enormous number of features can be produced, with some performing very well
    simply by chance.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，甚至创建成对的工程特征也可能导致过拟合，因为可以生成大量特征，其中一些仅仅因为偶然的原因表现非常好。
- en: To address this, FormulaFeatures limits the number of original columns considered
    when the input data has many columns. So, where datasets have large numbers of
    columns, only the most predictive are considered after the first iteration. The
    subsequent iterations perform as normal; there is simply some pruning of the original
    features used during this first iteration.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，FormulaFeatures 限制了在输入数据中包含大量列时考虑的原始列的数量。因此，当数据集包含大量列时，只有最具预测性的特征会在第一次迭代后被考虑。随后的迭代会正常进行；只是第一次迭代中使用的原始特征会有所修剪。
- en: Unary Functions
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一元函数
- en: By default, Formula Features does not incorporate unary functions, such as square,
    square root, or log (though it can do so if the relevant parameters are specified).
    As indicated above, some tools, such as AutoFeat also optionally support these
    operations, and they can be valuable at times.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，FormulaFeatures 不会包含一元函数，如平方、平方根或对数（尽管如果指定相关参数，它是可以执行这些操作的）。如上所述，一些工具，如
    AutoFeat，也可选择性地支持这些操作，并且它们在某些时候是有价值的。
- en: 'In some cases, it may be that a feature such as A² / B predicts the target
    better than the equivalent form without the square operator: A / B. However, including
    unary operators can lead to misleading features if not substantially correct,
    and may not significantly increase the accuracy of any models using them.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，像 A² / B 这样的特征可能比不含平方操作符的等效形式 A / B 更能预测目标。然而，如果没有足够的准确性，包含一元操作符可能会导致误导性的特征，并且可能不会显著提高任何使用这些特征的模型的准确性。
- en: When working with decision trees, so long as there is a monotonic relationship
    between the features with and without the unary functions, there will not be any
    change in the final accuracy of the model. And, most unary functions maintain
    a rank order of values (with exceptions such as sin and cos, which may reasonably
    be used where cyclical patterns are strongly suspected). For example, the values
    in A will have the same rank values as A² (assuming all values in A are positive),
    so squaring will not add any predictive power — decision trees will treat the
    features equivalently.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用决策树时，只要特征在有无一元函数的情况下保持单调关系，模型的最终准确度就不会发生变化。而且，大多数一元函数保持值的排名顺序（如正弦和余弦是例外，通常在强烈怀疑存在周期性模式时可以合理使用）。例如，A
    中的值将与 A² 中的值具有相同的排名值（假设 A 中的所有值都是正数），因此平方运算不会增加任何预测能力——决策树将等效处理这些特征。
- en: 'As well, in terms of explanatory power, simpler functions can often capture
    nearly as much of the pattern as can more complex functions: simpler function
    such as A / B are generally more comprehensible than formulas such as A² / B,
    but still convey the same idea, that it’s the ratio of the two features that’s
    relevant.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，从解释能力的角度来看，简单的函数通常可以捕捉到几乎与复杂函数相同的模式：例如，像 A / B 这样的简单函数通常比 A² / B 这样的公式更易于理解，但仍然传达相同的意思，即两个特征的比值才是相关的。
- en: Limiting the set of operators used by default also allows the process to execute
    faster and in a more regularized manner.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 限制默认使用的操作符集也可以使过程执行得更快，并且更加规范化。
- en: Coefficients
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 系数
- en: A similar argument may be made for including coefficients in engineered features.
    A feature such as 5.3A + 1.4B may capture the relationship A and B have with Y
    better than the simpler A + B, but the coefficients are often unnecessary, prone
    to be calculated incorrectly, and inscrutable even where approximately correct.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在工程特征中包含系数的问题，也可以提出类似的论点。像 5.3A + 1.4B 这样的特征可能比简单的 A + B 更好地捕捉 A 和 B 与 Y 的关系，但系数通常是多余的，容易计算错误，并且即使大致正确也难以理解。
- en: And, in the case of multiplication and division operations, the coefficients
    are most likely irrelevant (at least when used with decision trees). For example,
    5.3A * 1.4B will be functionally equivalent to A * B for most purposes, as the
    difference is a constant which can be divided out. Again, there is a monotonic
    relationship with and without the coefficients, and thus the features are equivalent
    when used with models, such as decision trees, that are concerned only with the
    ordering of feature values, not their specific values.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在乘法和除法运算的情况下，系数通常是无关紧要的（至少在与决策树一起使用时如此）。例如，5.3A * 1.4B 在大多数情况下与 A * B 在功能上是等效的，因为它们之间的差异是一个常数，可以被约掉。同样地，无论是否使用系数，都会存在单调关系，因此，当与仅关心特征值排序而非具体值的模型（如决策树）一起使用时，特征是等效的。
- en: Scaling
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缩放
- en: Scaling the features generated by FormulaFeatures is not necessary if used with
    decision trees (or similar model types such as Additive Decision Trees, rules,
    or decision tables). But, for some model types, such as SVM, kNN, ikNN, logistic
    regression, and others (including any that work based on distance calculations
    between points), the features engineered by Formula Features may be on quite different
    scales than the original features, and will need to be scaled. This is straightforward
    to do, and is simply a point to remember.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如果与决策树（或类似的模型类型，如加法决策树、规则或决策表）一起使用，则无需对 FormulaFeatures 生成的特征进行缩放。但对于某些模型类型，如支持向量机（SVM）、k最近邻（kNN）、改进kNN（ikNN）、逻辑回归等（包括任何基于点之间距离计算的模型），由
    FormulaFeatures 工程化的特征可能与原始特征在尺度上差异较大，因此需要进行缩放。这是直接可以做到的，只是需要记住这一点。
- en: Explainable Machine Learning
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可解释的机器学习
- en: In this article, we looked at interpretable models, but should indicate, at
    least quickly, FormulaFeatures can also be useful for what are called *explainable
    models* and it may be that this is actually a more important application.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们讨论了可解释模型，但至少应该快速指出，FormulaFeatures 对于所谓的*可解释模型*也非常有用，并且这可能实际上是更重要的应用。
- en: 'To explain the idea of explainability: where it is difficult or impossible
    to create interpretable models with sufficient accuracy, we often instead develop
    black-box models (e.g. boosted models or neural networks), and then create post-hoc
    explanations of the model. Doing this is referred to as *explainable AI* (or XAI).
    These explanations try to make the black-boxes more understandable. Technique
    for this include: feature importances, ALE plots, proxy models, and counterfactuals.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释可解释性的概念：当很难或不可能创建具有足够准确性的可解释模型时，我们通常会开发黑箱模型（例如提升模型或神经网络），然后为该模型创建事后解释。这样做被称为*可解释的人工智能*（或
    XAI）。这些解释试图使黑箱模型更易理解。相关技术包括：特征重要性、ALE 图、代理模型和反事实。
- en: 'These can be important tools in many contexts, but they are limited, in that
    they can provide only an approximate understanding of the model. As well, they
    may not be permissible in all environments: in some situations (for example, for
    safety, or for regulatory compliance), it can be necessary to strictly use interpretable
    models: that is, to use models where there are no questions about how the model
    behaves.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这些可以是许多场景中的重要工具，但它们也有限制，因为它们只能提供对模型的近似理解。而且，它们可能并不适用于所有环境：在某些情况下（例如出于安全性或合规性要求），可能需要严格使用可解释模型：即使用那些没有任何关于模型行为疑问的模型。
- en: 'And, even where not strictly required, it’s quite often preferable to use an
    interpretable model where possible: it’s often very useful to have a good understanding
    of the model and of the predictions made by the model.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，即使不是严格要求，通常还是更倾向于在可能的情况下使用可解释模型：对模型和模型做出的预测有较好的理解通常是非常有用的。
- en: Having said that, using black-box models and post-hoc explanations is very often
    the most suitable choice for prediction problems. As FormulaFeatures produces
    valuable features, it can support XAI, potentially making feature importances,
    plots, proxy models, or counter-factuals more interpretable.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，使用黑箱模型和事后解释通常是预测问题中最合适的选择。由于 FormulaFeatures 能够生成有价值的特征，它可以支持 XAI，从而使特征重要性、图表、代理模型或反事实更具可解释性。
- en: 'For example, it may not be feasible to use a shallow decision tree as the actual
    model, but it may be used as a proxy model: a simple, interpretable model that
    approximates the actual model. In these cases, as much as with interpretable models,
    having a good set of engineered features can make the proxy models more interpretable
    and more able to capture the behaviour of the actual model.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，可能无法将浅层决策树作为实际模型来使用，但它可以作为代理模型：一种简单、可解释的模型，近似实际模型。在这些情况下，与可解释模型一样，拥有一组良好的构造特征可以使代理模型更具可解释性，并更能捕捉实际模型的行为。
- en: Installation
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装
- en: The tool uses a [single .py](https://github.com/Brett-Kennedy/FormulaFeatures/blob/main/formula_features.py)
    file, which may be simply downloaded and used. It has no dependencies other than
    numpy, pandas, matplotlib, and seaborn (used to plot the features generated).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 该工具使用一个 [单一的 .py](https://github.com/Brett-Kennedy/FormulaFeatures/blob/main/formula_features.py)
    文件，可以直接下载并使用。除了 numpy、pandas、matplotlib 和 seaborn（用于绘制生成的特征图表）之外，没有其他依赖项。
- en: Conclusions
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: FormulaFeatures is a tool to engineer features based on arithmetic relationships
    between numeric features. The features can be informative in themselves, but are
    particularly useful when used with interpretable ML models.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: FormulaFeatures 是一个基于数值特征之间的算术关系来构造特征的工具。这些特征本身可以提供信息，但在与可解释的机器学习模型一起使用时，特别有用。
- en: While this tends to not improve the accuracy for all models, it does quite often
    improve the accuracy of interpretable models such as shallow decision trees.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种方法并不总是能提高所有模型的准确性，但它通常能提高可解释模型的准确性，例如浅层决策树。
- en: Consequently, it can be a useful tool to make it more feasible to use interpretable
    models for prediction problems — it may allow the use of interpretable models
    for problems that would otherwise be limited to black box models. And where interpretable
    models are used, it may allow these to be more accurate or interpretable. For
    example, with a classification decision tree, we may be able to achieve similar
    accuracy using fewer nodes, or may be able to achieve higher accuracy using the
    same number of nodes.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，它可以成为一个有用的工具，使得在预测问题中使用可解释模型变得更为可行——它可能允许在本应仅限于黑箱模型的问题中使用可解释模型。而且，在使用可解释模型时，它可能使这些模型更准确或更具可解释性。例如，使用分类决策树时，我们可能能够用更少的节点实现相似的准确度，或者使用相同数量的节点实现更高的准确度。
- en: FormulaFeatures can very often support interpretable ML well, but there are
    some limitations. It does not work with categorical or other non-numeric features.
    And, even with numeric features, some interactions may be difficult to capture
    using arithmetic functions. Where there is a more complex relationship between
    pairs of features and the target column, it may be more appropriate to use [ikNN](https://github.com/Brett-Kennedy/ikNN).
    This works based on nearest neighbors, so can capture relationships of arbitrary
    complexity between features and the target.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: FormulaFeatures通常能够很好地支持可解释的机器学习，但也存在一些局限性。它不适用于类别特征或其他非数值特征。而且，即使是数值特征，某些交互作用也可能难以通过算术函数捕捉。对于特征对与目标列之间存在更复杂关系的情况，使用[ikNN](https://github.com/Brett-Kennedy/ikNN)可能更为合适。该方法基于最近邻原理，因此能够捕捉特征与目标之间任意复杂度的关系。
- en: We focused on standard decision trees in this article, but for the most effective
    interpretable ML, it can be useful to try other interpretable models. It’s straightforward
    to see, for example, how the ideas here will apply directly to [Genetic Decision
    Trees](https://github.com/Brett-Kennedy/GeneticDecisionTree), which are similar
    to standard decision trees, simply created using bootstrapping and a genetic algorithm.
    Similarly for most other interpretable models.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 本文重点讨论了标准决策树，但为了实现最有效的可解释机器学习，尝试其他可解释模型也是很有用的。例如，直接看到这些方法如何应用到[遗传决策树](https://github.com/Brett-Kennedy/GeneticDecisionTree)上就很简单，这些决策树与标准决策树类似，只是通过自助法和遗传算法创建的。对大多数其他可解释模型也是如此。
- en: All images are by the author
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 所有图片均由作者提供
