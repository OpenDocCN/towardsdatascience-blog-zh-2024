- en: CLIP, LLaVA, and the Brain
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CLIP、LLaVA 和大脑
- en: 原文：[https://towardsdatascience.com/clip-llava-and-the-brain-2073dfb33d7e?source=collection_archive---------3-----------------------#2024-06-19](https://towardsdatascience.com/clip-llava-and-the-brain-2073dfb33d7e?source=collection_archive---------3-----------------------#2024-06-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/clip-llava-and-the-brain-2073dfb33d7e?source=collection_archive---------3-----------------------#2024-06-19](https://towardsdatascience.com/clip-llava-and-the-brain-2073dfb33d7e?source=collection_archive---------3-----------------------#2024-06-19)
- en: Deep Learning and the Brain
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习与大脑
- en: Insights into Multimodal Transformers from Neuroscience
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 来自神经科学的多模态变换器洞察
- en: '[](https://medium.com/@williford?source=post_page---byline--2073dfb33d7e--------------------------------)[![Jonathan
    R. Williford, PhD](../Images/63b57be5ef10621c8d48b93399b2b598.png)](https://medium.com/@williford?source=post_page---byline--2073dfb33d7e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2073dfb33d7e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2073dfb33d7e--------------------------------)
    [Jonathan R. Williford, PhD](https://medium.com/@williford?source=post_page---byline--2073dfb33d7e--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@williford?source=post_page---byline--2073dfb33d7e--------------------------------)[![Jonathan
    R. Williford, PhD](../Images/63b57be5ef10621c8d48b93399b2b598.png)](https://medium.com/@williford?source=post_page---byline--2073dfb33d7e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2073dfb33d7e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2073dfb33d7e--------------------------------)
    [Jonathan R. Williford, PhD](https://medium.com/@williford?source=post_page---byline--2073dfb33d7e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2073dfb33d7e--------------------------------)
    ·8 min read·Jun 19, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2073dfb33d7e--------------------------------)
    ·阅读时长 8 分钟 ·2024年6月19日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/727ab5bfb53e278b694bc22ca7d293aa.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/727ab5bfb53e278b694bc22ca7d293aa.png)'
- en: Image generated by the author using Dall-E 3.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由作者使用 Dall-E 3 生成。
- en: How do recent multimodal transformer networks, like CLIP (Radford et al. 2021)
    and LLaVA (Liu et al. 2023), compare to the brain? Are there similarities between
    the attention in these networks and the brain? In this article, I look at these
    transformer architectures with an eye on the similarities and differences with
    the mammalian brain.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的多模态变换器网络，如 CLIP（Radford 等，2021）和 LLaVA（Liu 等，2023），与大脑相比如何？这些网络中的注意力机制与大脑之间是否存在相似之处？在本文中，我将从相似性和差异性的角度，分析这些变换器架构与哺乳动物大脑的对比。
- en: 'What stood out to me was that vision transformers, CLIP, and LLaVA perform
    a type of processing analogous to pre-attentive visual processing in the brain.
    This processing is done in the initial feedforward visual responses to a stimulus
    before recurrence. Although a lot can be accomplished in a feedforward way, studies
    have shown that feedforward pre-attentive processing in the brain does have difficulty
    with:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 令我印象深刻的是，视觉变换器、CLIP 和 LLaVA 执行一种类似于大脑前注意性视觉处理的处理方式。这种处理发生在对刺激的初步前馈视觉响应中，在回归之前完成。虽然前馈处理可以完成很多任务，但研究表明，大脑中的前馈前注意性处理确实存在困难：
- en: Distinguishing the identity or characteristics of similar types of objects,
    especially when objects are close together or cluttered or the objects are unnatural
    or artificial (VanRullen 2007).
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 区分相似类型对象的身份或特征，尤其是当物体靠得很近或杂乱无章，或者物体是非自然或人造的（VanRullen 2007）。
- en: More complex tasks such as counting or maze or curve tracing tasks.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更复杂的任务，如计数、迷宫或曲线追踪任务。
- en: Perceiving objects that are more difficult to see, such as where it is difficult
    to perceive the boundaries of the objects.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 感知那些较难看到的物体，例如在难以分辨物体边界的情况下。
- en: In contrast to the feed-forward processing, one of the things that stands out
    with the brain is the richness in the interaction of areas, which I will discuss
    in more detail in the next section.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 与前馈处理相比，大脑的一大特点是不同区域之间的互动丰富性，我将在下一节中详细讨论这一点。
- en: Bidirectional Activity in the Brain
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大脑中的双向活动
- en: In most current deep learning architectures, activity is propagated in a single
    direction, for example, an image might be given as input to a network and then
    propagated from layer to layer until you get to a classification as the output.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数当前的深度学习架构中，活动是单向传播的，例如，图像可能作为输入提供给网络，然后从一层传播到下一层，直到得到分类作为输出。
- en: '![](../Images/f7764a163e02f6e5b19857a4dc11aace.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f7764a163e02f6e5b19857a4dc11aace.png)'
- en: 'Figure 1: A simplified diagram showing some of the feed-forward and feedback
    connections in the Macaque brain. The earlier (or lower-level) areas are whiter,
    while the later (or higher-level) areas are bluer. Image by Author.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：简化图示，展示了猕猴大脑中一些前馈和反馈连接。较早（或较低层次）的区域为白色，而较晚（或较高层次）的区域为蓝色。图像由作者提供。
- en: The brain is much more interesting than these feedforward models. In the visual
    system, a stimulus will initially propagate from lower- to higher-level visual
    areas in a feedforward fashion, then the higher-level areas will exert influence
    over the lower-level areas as depicted in Figure 1.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 大脑比这些前馈模型更加有趣。在视觉系统中，刺激最初会以前馈方式从较低层次的视觉区域传播到较高层次的视觉区域，随后更高层次的区域会对较低层次的区域施加影响，如图1所示。
- en: Some of this feedback is the conscious top-down attention that allows us to
    allocate more resources to objects and features of interest and disambiguate stimuli
    that are either complex or ambiguous. Another part of this feedback is automatic
    and allows higher-level areas to infuse the lower-level areas with information
    that would not be known in just the feedforward manner.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这种反馈的一部分是有意识的自上而下的注意力，它使我们能够为感兴趣的物体和特征分配更多的资源，并消除复杂或模糊刺激的歧义。另一部分反馈是自动的，它使得更高层次的区域能够向较低层次的区域传递通过前馈方式无法获得的信息。
- en: Conscious top-down attention is thought to support consciousness of visual stimuli.
    Without conscious access to lower-level areas that encode borders and edges, we
    wouldn’t have as spatially precise a perception of borders. Tasks like mentally
    tracing a curve or solving a maze would be impossible.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 有意识的自上而下的注意力被认为有助于视觉刺激的意识。如果无法有意识地访问编码边界和边缘的低层次区域，我们将无法对边界进行如此精确的空间感知。像是心算曲线或解迷这样的任务将变得不可能。
- en: One example of automatic unconscious feedback is border-ownership coding which
    is seen in about half of the orientation-selective neurons in visual area V2 (Zhou
    et al. 2000, Williford and von der Heydt 2013). These neurons will encode local
    information in about 40 ms and, as early as 10 ms after this initial response,
    will incorporate global context to resolve occlusions — holding the information
    about which objects are creating borders by occluding their backgrounds.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 自动无意识反馈的一个例子是边界归属编码，这在视觉区域V2中的大约一半方向选择性神经元中可见（Zhou等，2000；Williford和von der Heydt，2013）。这些神经元会在大约40毫秒内编码局部信息，并且在最初反应后的10毫秒内，便会结合全局背景来解决遮挡问题——保留关于哪些物体通过遮挡其背景来创建边界的信息。
- en: Another example of this unconscious feedback was shown by Poort et al. (2012)
    using images like that in Figure 2\. In the Macaque early visual cortex V1, neurons
    will tend to initially (within 50–75 ms of stimulus presentation) encode only
    the local features within their receptive fields (e.g., green square). However,
    after around 75 ms, they will receive feedback from the higher-level areas and
    tend to have a higher response when that texture belongs to a figure, such as
    this texture-defined figure above. This happens even when attention is drawn away
    from the figure, however, if the monkey is paying attention to the figure the
    neurons will on average respond even more.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这种无意识反馈的另一个例子是Poort等人（2012）通过图2所示的图像展示的。在猕猴初级视觉皮层V1中，神经元通常会在刺激呈现后的50-75毫秒内，首先编码它们感受野内的局部特征（例如绿色方块）。然而，在大约75毫秒后，它们会接收到来自更高层次区域的反馈，并且当纹理属于某个形状时，它们的反应会更强烈，比如上面这种纹理定义的形状。即使注意力从形状上转移，这种情况仍然会发生；然而，如果猴子集中注意力在形状上，神经元的反应通常会更强。
- en: '![](../Images/16166897fa5998f9ab7213eeddd075d0.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/16166897fa5998f9ab7213eeddd075d0.png)'
- en: 'Figure 2: Shapes defined only by texture, like the above, can be difficult
    to see in a pure “feed-forward” manner. The interaction between lower- and higher-level
    areas enables us to perceive such difficult shapes (Poort et 2012). Image by Author.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：仅由纹理定义的形状，如上图，可能很难通过纯粹的“前馈”方式来感知。低层次和高层次区域之间的互动使我们能够感知这些难以看清的形状（Poort等，2012）。图像由作者提供。
- en: One way to look at this bidirectional interaction is that each neuron greedily
    uses all available predictive signals constantly. Even higher-level areas can
    be predictive, especially when visual borders do not correspond to significant
    first-order contrast edges.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一种看待这种双向交互的方式是，每个神经元都会不断贪婪地利用所有可用的预测信号。即使是较高层次的区域也可以进行预测，特别是当视觉边界不对应显著的一阶对比边缘时。
- en: Transformers
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变压器
- en: With all the talk about attention with the introduction of transformers (Vaswani
    et al. 2017) and with the ability to generate sentences one word at a time, you
    might be led to believe that transformers are recurrent. However, there are no
    internal states kept between the steps of the transformer, only the previous output
    is provided as input. So, the recurrence is limited and does not have the bidirectionality
    that is ubiquitous in the brain. Transformers do have multi-headed attention,
    which is like being able to attend to a fixed number of things simultaneously
    (8 in the original paper). Hence, image transformers can be seen as analogous
    to pre-attentive feedforward processing with some modifications.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 随着transformer的引入（Vaswani等人，2017年）以及能够一次生成一个单词的能力，可能会让你认为transformer是递归的。然而，transformer的各个步骤之间并没有保持内部状态，只有前一个输出作为输入。因此，递归是有限的，并没有大脑中普遍存在的双向性。transformer确实具有多头注意力机制，这就像是能够同时关注一定数量的事物（原文中是8个）。因此，图像transformer可以看作是类似于带有一些修改的前注意力前馈处理。
- en: CLIP
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CLIP
- en: '![](../Images/5fd93c045205ef5f4738c2c0dbca308a.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5fd93c045205ef5f4738c2c0dbca308a.png)'
- en: 'Figure 3: CLIP trains an image and text encoder using *image caption pairs.
    I*₁ and *T*₁ are the encodings of image 1 and the corresponding caption. A contrastive
    learning loss is used to make the *I*ᵢ and *Tj* more similar when *i*=*j* and
    more dissimilar when *i*≠*j*. Weights are trained from scratch. Figure reproduced
    with permission from [Radford et al. (2021)](http://proceedings.mlr.press/v139/radford21a).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：CLIP通过*图像说明配对*训练图像和文本编码器。*I*₁和*T*₁分别是图像1和对应说明的编码。使用对比学习损失函数，使得当*i*=*j*时，*I*ᵢ和*Tj*更加相似，当*i*≠*j*时，它们更加不相似。权重是从头开始训练的。图示已获得[Radford等人(2021)](http://proceedings.mlr.press/v139/radford21a)的许可。
- en: 'Radford and colleagues from OpenAI introduced CLIP in their 2021 paper “Learning
    Transferable Visual Models from Natural Language Supervision”. The idea behind
    CLIP is simple and is shown in Figure 3\. It takes a bunch of image and caption
    pairs from the Internet and feeds the image to an image encoder and the text to
    a text encoder. It then uses a loss that brings the encoding of the image and
    the encoding of the text closer together when they are in the same pair, otherwise
    the loss increases the distance of the encodings. This is what CLIP gives you:
    the ability to compare the similarity between text and images. This does allow
    it to be used for zero-shot classification, as shown in Figure 4\. CLIP does not,
    by itself, generate text descriptions from images.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的Radford及其同事在2021年的论文《从自然语言监督中学习可转移的视觉模型》中介绍了CLIP。CLIP背后的思想非常简单，如图3所示。它从互联网上获取一堆图像和说明配对，将图像输入到图像编码器，将文本输入到文本编码器。然后，它使用一个损失函数，当图像和文本在同一对中时，使图像编码和文本编码更加接近，否则损失函数则增加它们编码之间的距离。这就是CLIP的作用：能够比较文本和图像之间的相似性。这也使得它可以用于零-shot分类，如图4所示。CLIP本身并不从图像生成文本描述。
- en: The image encoder and text encoder are independent, meaning there is no way
    for task-driven modulation to influence the image encoding. This means that the
    image encoder must encode everything that could be potentially relevant to the
    task. Typically, the resolution of the input image is small, which helps prevent
    the computation and memory requirements from exploding.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图像编码器和文本编码器是独立的，这意味着没有办法通过任务驱动的调制来影响图像编码。这意味着图像编码器必须编码所有可能与任务相关的内容。通常，输入图像的分辨率较小，这有助于防止计算和内存需求的激增。
- en: '![](../Images/450a9b45845992be3d2750574f68b548.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/450a9b45845992be3d2750574f68b548.png)'
- en: 'Figure 4: CLIP can be used for zero-shot classification. Text is created for
    each of the N classes, which are then encoded into tokens *T*1…*TN*. The image
    is then encoded, and the similarity is measured with the generated text encodings.
    The most similar text encoding is the chosen class. Figure reproduced with permission
    from [Radford et al. (2021)](http://proceedings.mlr.press/v139/radford21a).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：CLIP可以用于零-shot分类。为每个类别生成文本，然后将其编码为标记*T*1…*TN*。接着对图像进行编码，并与生成的文本编码进行相似度比较。最相似的文本编码即为选定的类别。图示已获得[Radford等人(2021)](http://proceedings.mlr.press/v139/radford21a)的许可。
- en: LLaVA
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLaVA
- en: '![](../Images/cbc5edb3e0e90bee04a87b00ca26bded.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cbc5edb3e0e90bee04a87b00ca26bded.png)'
- en: 'Figure 5: LLaVA architecture. X*v*: image, Xq: instruction/question, H*v*:
    image tokens, Hq: instruction tokens, Xa: answer, generated one token at a time.
    Image by Author, based on Figure 1 from [Liu et al. (2023)](https://doi.org/10.48550/arXiv.2304.08485).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：LLaVA 架构。X*v*：图像，Xq：指令/问题，H*v*：图像令牌，Hq：指令令牌，Xa：答案，一次生成一个令牌。图片由作者提供，基于[Liu
    et al. (2023)](https://doi.org/10.48550/arXiv.2304.08485)中的图 1。
- en: Large Language and Vision Assistant (LLaVA) (Liu et al. 2023) is a large language
    and vision architecture that extends and builds onto CLIP to add the ability to
    describe and answer questions about images. This type of architecture interests
    me because it can attempt tasks like those used in Neuroscience and Psychology.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言与视觉助手（LLaVA）（Liu et al. 2023）是一种大型语言与视觉架构，它扩展并建立在 CLIP 的基础上，加入了描述和回答图像问题的能力。这种架构让我感兴趣，因为它可以尝试一些神经科学和心理学中使用的任务。
- en: LLaVA takes the vision transformer model ViT-L/14 trained by CLIP for image
    encoding (Figure 5). The first paper uses a single linear projection matrix W
    to convert the encodings into tokens. The tokens calculated from the images Hᵥ
    and the text instructions Hq are provided as input. LLaVA can then generate the
    language response Xₐ one token at a time, appending the response so far as the
    input to the next iteration.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: LLaVA 使用由 CLIP 训练的视觉变换器模型 ViT-L/14 进行图像编码（图 5）。第一篇论文使用一个线性投影矩阵 W，将编码转换为令牌。从图像
    Hᵥ 和文本指令 Hq 计算出的令牌作为输入提供给 LLaVA。然后，LLaVA 可以一次生成一个语言回应 Xa，并将目前的回应附加到下一个迭代的输入中。
- en: I won’t go into the details of how LLaVA is trained, but it is interesting how
    they use ChatGPT to expand the caption (Xc) in Figure 5 to form instructions (Hq)
    and responses (used to train Xₐ) about an image and the use of bounding box information.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会详细讨论 LLaVA 的训练方式，但有趣的是，他们如何使用 ChatGPT 来扩展图 5 中的标题（Xc），形成关于图像的指令（Hq）和回应（用于训练
    Xa），并使用边界框信息。
- en: 'In version 1.5 of LLaVA (Liu et al. 2024), some of the improvements they made
    include:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LLaVA 的版本 1.5（Liu et al. 2024）中，他们所做的一些改进包括：
- en: The linear projection matrix W is replaced with a multilayer perceptron
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性投影矩阵 W 被一个多层感知机替代。
- en: The image resolution is increased by using an image encoder that takes images
    of size 336x336 pixels and splits the images into grids that are encoded separately
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用一个图像编码器，图像的分辨率得到了提高，该编码器接收大小为 336x336 像素的图像，并将图像拆分为网格，每个网格分别编码。
- en: Task-driven attention in the brain can dynamically allocate resources to the
    object, location, or features of interest, which allows the processing of information
    that would otherwise be overwhelmed by clutter or other objects. In LLaVA, the
    image encoder is independent of the text instructions, so to be successful it
    needs to make sure any potentially useful information is stored in the image tokens
    (Hᵥ).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 大脑中的任务驱动注意力可以动态地将资源分配给感兴趣的物体、位置或特征，这使得能够处理那些本来会被杂乱或其他物体淹没的信息。在 LLaVA 中，图像编码器与文本指令是独立的，因此为了成功，它需要确保任何潜在有用的信息都存储在图像令牌（Hᵥ）中。
- en: Conclusion
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'LLaVA and CLIP lack bidirectional and recurrence with internal states, which
    constrains their processing. This is especially true for image processing since
    image processing is done independently of the text instructions. Most convolutional
    neural networks also share these limitations. This leads me to my conjecture:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: LLaVA 和 CLIP 缺乏双向和递归的内部状态，这限制了它们的处理能力。特别是在图像处理方面，图像处理与文本指令是独立进行的。大多数卷积神经网络也有这些限制。这让我产生了我的推测：
- en: '*Conjecture: Most convolutional, vision transformer, and multimodal transformer
    networks are restricted to processing that is analogous to pre-attentive feedforward
    visual processing in the brain.*'
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*推测：大多数卷积神经网络、视觉变换器和多模态变换器网络的处理方式类似于大脑中前注意力的前馈视觉处理。*'
- en: This is not a criticism as much as an insight that can be informative. Feedforward
    processing can do a lot and is fast. However, it is not as dynamic as to what
    resources can be used to be used, which can lead to informational bottlenecks
    in cluttered scenes and is unable to encode enough information for complex tasks
    without an explosion of the size of the encodings. Creating models that work in
    a feedforward fashion is an important stepping stone because of the difficulty
    of adding recurrence and bidirectional processing.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅仅是批评，更是一种可以提供信息的洞察。前馈处理可以做很多事情，且速度很快。然而，它在可以使用哪些资源方面并不像预期的那么动态，这可能导致在杂乱场景中出现信息瓶颈，并且在没有大幅增加编码尺寸的情况下，无法为复杂任务编码足够的信息。创建前馈式工作的模型是一个重要的踏脚石，因为添加递归和双向处理的难度很大。
- en: Some networks are not limited to pre-attentive feedforward networks, but currently,
    most of the architectures lag behind those of transformers. These include long-short
    term memory models (LSTMs) and, more recently, the Mamba architecture, which has
    several benefits over transformers ([Gu and Dao 2024](http://neural.vision/blog/neuroai/CLIP-LLaVA-and-the-Brain/#ref-guMamba2024)).
    Extended LSTMs [(Beck et al. 2024](http://neural.vision/blog/neuroai/CLIP-LLaVA-and-the-Brain/#ref-beckXLSTM2024),
    [Alkin et al. 2024](http://neural.vision/blog/neuroai/CLIP-LLaVA-and-the-Brain/#ref-alkinVisionLSTM2024))
    have recently been proposed, which help close the gap between transformers and
    LSTMs. Diffusion models also have a limited type of recurrence that uses the image
    as the state between iterations.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一些网络不限于预注意力前馈网络，但目前大多数架构落后于变压器架构。这些包括长短期记忆模型（LSTM），以及最近的Mamba架构，它比变压器有多个优势（[Gu
    和 Dao 2024](http://neural.vision/blog/neuroai/CLIP-LLaVA-and-the-Brain/#ref-guMamba2024)）。扩展型LSTM（[Beck
    等 2024](http://neural.vision/blog/neuroai/CLIP-LLaVA-and-the-Brain/#ref-beckXLSTM2024)，[Alkin
    等 2024](http://neural.vision/blog/neuroai/CLIP-LLaVA-and-the-Brain/#ref-alkinVisionLSTM2024)）最近被提出，这有助于缩小变压器和LSTM之间的差距。扩散模型也有一种有限的递归类型，它使用图像作为迭代之间的状态。
- en: References
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'B. Alkin, M. Beck, K. Pöppel, S. Hochreiter, and J. Brandstetter, [Vision-LSTM:
    xLSTM as Generic Vision Backbone](http://arxiv.org/abs/2406.04303) (2024), [http://arxiv.org/abs/2406.04303](http://arxiv.org/abs/2406.04303).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 'B. Alkin, M. Beck, K. Pöppel, S. Hochreiter, 和 J. Brandstetter, [Vision-LSTM:
    xLSTM 作为通用视觉主干](http://arxiv.org/abs/2406.04303) (2024), [http://arxiv.org/abs/2406.04303](http://arxiv.org/abs/2406.04303).'
- en: 'M. Beck, K. Pöppel, M. Spanring, A. Auer, O. Prudnikova, M. Kopp, G. Klambauer,
    J. Brandstetter, and S. Hochreiter, [xLSTM: Extended Long Short-Term Memory](http://arxiv.org/abs/2405.04517)
    (2024), [http://arxiv.org/abs/2405.04517](http://arxiv.org/abs/2405.04517)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 'M. Beck, K. Pöppel, M. Spanring, A. Auer, O. Prudnikova, M. Kopp, G. Klambauer,
    J. Brandstetter, 和 S. Hochreiter, [xLSTM: 扩展型长短期记忆网络](http://arxiv.org/abs/2405.04517)
    (2024), [http://arxiv.org/abs/2405.04517](http://arxiv.org/abs/2405.04517)'
- en: 'A. Gu and T. Dao. [Mamba: Linear-Time Sequence Modeling with Selective State
    Spaces](http://arxiv.org/abs/2312.00752) (2024) [http://arxiv.org/abs/2312.00752](http://arxiv.org/abs/2312.00752)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 'A. Gu 和 T. Dao. [Mamba: 线性时间序列建模与选择性状态空间](http://arxiv.org/abs/2312.00752)
    (2024) [http://arxiv.org/abs/2312.00752](http://arxiv.org/abs/2312.00752)'
- en: H. Liu, C. Li, Y. Li, and Y. J. Lee “[Improved Baselines with Visual Instruction
    Tuning](https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Improved_Baselines_with_Visual_Instruction_Tuning_CVPR_2024_paper.html)
    (2024) Proc. of IEEE/CVF CVPR*.*
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: H. Liu, C. Li, Y. Li, 和 Y. J. Lee “[通过视觉指令调优改进的基准](https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Improved_Baselines_with_Visual_Instruction_Tuning_CVPR_2024_paper.html)
    (2024) IEEE/CVF CVPR 论文集*。
- en: H. Liu, C. Li, Q. Wu, and Y. J. Lee, [Visual Instruction Tuning](https://doi.org/10.48550/arXiv.2304.08485)
    (2023), [https://doi.org/10.48550/arXiv.2304.08485](https://doi.org/10.48550/arXiv.2304.08485)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: H. Liu, C. Li, Q. Wu, 和 Y. J. Lee, [视觉指令调优](https://doi.org/10.48550/arXiv.2304.08485)
    (2023), [https://doi.org/10.48550/arXiv.2304.08485](https://doi.org/10.48550/arXiv.2304.08485)
- en: J. Poort, F. Raudies, A. Wannig, V. A. F. Lamme, H. Neumann, and P. R. Roelfsema.
    [The Role of Attention in Figure-Ground Segregation in Areas V1 and V4 of the
    Visual Cortex](https://doi.org/10.1016/j.neuron.2012.04.032) (2012) Neuron
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: J. Poort, F. Raudies, A. Wannig, V. A. F. Lamme, H. Neumann, 和 P. R. Roelfsema.
    [注意力在视觉皮层V1和V4区中的图形与背景分离中的作用](https://doi.org/10.1016/j.neuron.2012.04.032) (2012)
    Neuron
- en: A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
    A. Askell, P. Mishkin, and J. Clark. [Learning Transferable Visual Models from
    Natural Language Supervision](http://proceedings.mlr.press/v139/radford21a) (2021)
    ICML
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
    A. Askell, P. Mishkin, 和 J. Clark. [从自然语言监督中学习可迁移的视觉模型](http://proceedings.mlr.press/v139/radford21a)
    (2021) ICML
- en: R. VanRullen, [The Power of the Feed-Forward Sweep](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2864977/)
    (2007) Advances in Cognitive Psychology
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: R. VanRullen, [《前馈波动的力量》](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2864977/)
    (2007) 《认知心理学进展》
- en: A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser,
    and I. Polosukhin, [Attention Is All You Need](https://proceedings.neurips.cc/paper/7181-attention-is-all)
    (2017) NeurIPs
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser,
    和 I. Polosukhin, [《注意力机制就是你所需要的一切》](https://proceedings.neurips.cc/paper/7181-attention-is-all)
    (2017) NeurIPs
- en: J. R. Williford and R. von der Heydt, [Border-Ownership Coding](http://scholarpedia.org/article/Border-ownership_coding)
    (2013) Scholarpedia
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: J. R. Williford 和 R. von der Heydt, [《边界所有权编码》](http://scholarpedia.org/article/Border-ownership_coding)
    (2013) Scholarpedia
- en: H. Zhou, H. S. Friedman, and R. von der Heydt. “[Coding of Border Ownership
    in Monkey Visual Cortex](https://www.jneurosci.org/content/20/17/6594.full) (2000)
    The Journal of Neuroscience
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: H. Zhou, H. S. Friedman, 和 R. von der Heydt. “[《猴子视觉皮层中的边界所有权编码》](https://www.jneurosci.org/content/20/17/6594.full)
    (2000) 《神经科学杂志》
- en: '*Originally published at* [*http://neural.vision*](http://neural.vision/blog/neuroai/CLIP-LLaVA-and-the-Brain/)
    *on June 19, 2024.*'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*原文发布于* [*http://neural.vision*](http://neural.vision/blog/neuroai/CLIP-LLaVA-and-the-Brain/)
    *于2024年6月19日。*'
