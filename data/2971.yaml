- en: 'Predicted Probability, Explained: A Visual Guide with Code Examples for Beginners'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测概率解释：带有代码示例的可视化指南，适合初学者
- en: 原文：[https://towardsdatascience.com/predicted-probability-explained-a-visual-guide-with-code-examples-for-beginners-7c34e8994ec2?source=collection_archive---------3-----------------------#2024-12-10](https://towardsdatascience.com/predicted-probability-explained-a-visual-guide-with-code-examples-for-beginners-7c34e8994ec2?source=collection_archive---------3-----------------------#2024-12-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/predicted-probability-explained-a-visual-guide-with-code-examples-for-beginners-7c34e8994ec2?source=collection_archive---------3-----------------------#2024-12-10](https://towardsdatascience.com/predicted-probability-explained-a-visual-guide-with-code-examples-for-beginners-7c34e8994ec2?source=collection_archive---------3-----------------------#2024-12-10)
- en: MODEL EVALUATION & OPTIMIZATION
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型评估与优化
- en: 7 basic classifiers reveal their prediction confidence math
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7种基本分类器揭示其预测置信度的数学原理
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--7c34e8994ec2--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--7c34e8994ec2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7c34e8994ec2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7c34e8994ec2--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--7c34e8994ec2--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@samybaladram?source=post_page---byline--7c34e8994ec2--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--7c34e8994ec2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7c34e8994ec2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7c34e8994ec2--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--7c34e8994ec2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7c34e8994ec2--------------------------------)
    ·17 min read·Dec 10, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7c34e8994ec2--------------------------------)
    ·17分钟阅读·2024年12月10日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Classification models don’t just tell you what they think the answer is — they
    also tell you **how sure** they are about that answer. This certainty is shown
    as a probability score. A high score means the model is very confident, while
    a low score means it’s uncertain about its prediction.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 分类模型不仅告诉你它们认为答案是什么——它们还告诉你**它们对这个答案的确定程度**。这种确定性通过概率分数显示出来。高分意味着模型非常自信，而低分则意味着它对预测的结果不确定。
- en: Every classification model calculates these probability scores differently.
    Simple models and complex ones each have their own specific methods to determine
    the likelihood of each possible outcome.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 每个分类模型计算这些概率分数的方式不同。简单的模型和复杂的模型各自有自己的方法来确定每种可能结果的概率。
- en: We’re going to explore seven basic classification models and visually break
    down how each one figures out its probability scores. No need for a crystal ball
    — we’ll make these probability calculations crystal clear!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨七种基本的分类模型，并直观地分析每种模型是如何计算其概率分数的。无需水晶球——我们将让这些概率计算一目了然！
- en: '![](../Images/b265adbcd86fb1261e938e663049e715.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b265adbcd86fb1261e938e663049e715.png)'
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所有视觉效果：作者使用Canva Pro创建，已针对移动设备进行优化；在桌面设备上可能会显得过大。
- en: Definition
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义
- en: Predicted probability (or “class probability”) is a number from 0 to 1 (or 0%
    to 100%) that shows how confident a model is about its answer. If the number is
    1, the model is completely sure about its answer. If it’s 0.5, the model is basically
    guessing — it’s like flipping a coin.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 预测概率（或称“类别概率”）是一个从0到1（或0%到100%）的数值，表示模型对其答案的信心水平。如果该数值为1，表示模型对其答案非常确定。如果为0.5，模型基本上是在猜测——就像抛硬币一样。
- en: Components of a Probability Score
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概率分数的组成部分
- en: 'When a model has to choose between two classes (called binary classification),
    three main rules apply:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型需要在两个类别之间做出选择时（称为二分类），有三条主要规则适用：
- en: The predicted probability must be between 0 and 1
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测概率必须介于0和1之间
- en: The chances of both options happening must add up to 1
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 两个选项发生的概率总和必须等于1
- en: A higher probability means the model is more sure about its choice
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 较高的概率意味着模型对其选择更有信心
- en: '![](../Images/36a6cda6b007e30dbfb6888351b20b6f.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36a6cda6b007e30dbfb6888351b20b6f.png)'
- en: For binary classification, when we talk about predicted probability, we usually
    mean the probability of the positive class. A higher probability means the model
    thinks the positive class is more likely, while a lower probability means it thinks
    the negative class is more likely.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二分类问题，当我们谈论预测概率时，通常是指正类的概率。更高的概率意味着模型认为正类更有可能发生，而较低的概率则意味着模型认为负类更有可能。
- en: '![](../Images/dac411b12238417f189b5c813e5b3b67.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dac411b12238417f189b5c813e5b3b67.png)'
- en: To make sure these rules are followed, models use mathematical functions to
    convert their calculations into proper probabilities. Each type of model might
    use different functions, which affects how they express their confidence levels.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为确保这些规则得到遵守，模型使用数学函数将其计算结果转换为适当的概率。每种类型的模型可能使用不同的函数，这会影响它们表达置信度的方式。
- en: Prediction vs. Probability
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测与概率
- en: In classification, a model picks the class it thinks will most likely happen
    — the one with the highest probability score. But two different models might pick
    the same class while being more or less confident about it. Their predicted probability
    scores tell us how sure each model is, even when they make the same choice.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类问题中，模型会选择它认为最有可能发生的类别——即具有最高概率分数的类别。但两个不同的模型可能会选择相同的类别，同时它们对该类别的信心程度可能有所不同。它们的预测概率分数告诉我们每个模型有多确定，即使它们做出了相同的选择。
- en: '![](../Images/003cdef1e79d01f669621bce97b57d07.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/003cdef1e79d01f669621bce97b57d07.png)'
- en: 'These different probability scores tell us something important: even when models
    pick the same class, they might understand the data differently.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这些不同的概率分数告诉我们一个重要的事实：即使模型选择了相同的类别，它们可能会以不同的方式理解数据。
- en: One model might be very sure about its choice, while another might be less confident
    — even though they made the same prediction.
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一个模型可能对其选择非常确定，而另一个模型可能信心较弱——尽管它们做出了相同的预测。
- en: 📊 Dataset Used
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 📊 使用的数据集
- en: 'To understand how predicted probability is calculated, we’ll continue with
    [the same dataset used in my previous articles on Classification Algorithms](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c).
    Our goal remains: predicting if someone will play golf based on the weather.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解预测概率是如何计算的，我们将继续使用[我之前关于分类算法的文章中使用的相同数据集](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c)。我们的目标仍然是：根据天气预测某人是否会打高尔夫。
- en: '![](../Images/1560cdcd385ca4877365575c6c84f8b3.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1560cdcd385ca4877365575c6c84f8b3.png)'
- en: 'Columns: ‘Overcast (one-hot-encoded into 3 columns)’, ’Temperature’ (in Fahrenheit),
    ‘Humidity’ (in %), ‘Windy’ (Yes/No) and ‘Play’ (Yes/No, target feature)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 列：‘Overcast（通过三列进行独热编码）’，‘Temperature’（温度，单位为华氏度），‘Humidity’（湿度，单位为%），‘Windy’（风，Yes/No）以及‘Play’（是否打球，目标特征）
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As some algorithms might need standardized values, we will also do [standard
    scaling](https://medium.com/towards-data-science/scaling-numerical-data-explained-a-visual-guide-with-code-examples-for-beginners-11676cdb45cb)
    to the numerical features and [one-hot encoding](/encoding-categorical-data-explained-a-visual-guide-with-code-example-for-beginners-b169ac4193ae)
    to the categorical features, including the target feature:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于某些算法可能需要标准化的数值，我们还将对数值特征进行[标准化缩放](https://medium.com/towards-data-science/scaling-numerical-data-explained-a-visual-guide-with-code-examples-for-beginners-11676cdb45cb)，并对分类特征，包括目标特征，进行[独热编码](/encoding-categorical-data-explained-a-visual-guide-with-code-example-for-beginners-b169ac4193ae)：
- en: '![](../Images/0b25c11e730a5be1d37aee6342ef4b31.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0b25c11e730a5be1d37aee6342ef4b31.png)'
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, let’s see how each of the following 7 classification algorithms calculates
    these probabilities:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下以下7种分类算法是如何计算这些概率的：
- en: '![](../Images/85322ccb3ef67016db657d6a2a3c02a2.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/85322ccb3ef67016db657d6a2a3c02a2.png)'
- en: Dummy Classifier Probabilities
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 哑巴分类器概率
- en: '[](/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e?source=post_page-----7c34e8994ec2--------------------------------)
    [## Dummy Classifier Explained: A Visual Guide with Code Examples for Beginners'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e?source=post_page-----7c34e8994ec2--------------------------------)
    [## 哑巴分类器解析：带代码示例的可视化指南，适合初学者'
- en: Setting the Bar in Machine Learning with Simple Baseline Models
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过简单的基准模型设定机器学习的标准
- en: towardsdatascience.com](/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e?source=post_page-----7c34e8994ec2--------------------------------)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e?source=post_page-----7c34e8994ec2--------------------------------)
- en: 'A Dummy Classifier is a prediction model that doesn’t learn patterns from data.
    Instead, it follows basic rules like: picking the most common outcome, making
    random predictions based on how often each outcome appeared in training, always
    picking one answer, or randomly choosing between options with equal chance. The
    Dummy Classifier ignores all input features and just follows these rules.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟分类器是一种不从数据中学习模式的预测模型。相反，它遵循一些基本规则，例如：选择最常见的结果，基于每个结果在训练中出现的频率做随机预测，总是选择一个答案，或在等概率的选项中随机选择。虚拟分类器忽略所有输入特征，只遵循这些规则。
- en: When this model finishes training, all it remembers is a few numbers showing
    either how often each outcome happened or the constant values it was told to use.
    It doesn’t learn anything about how features relate to outcomes.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当这个模型完成训练时，它记住的只是一些数字，显示每个结果发生的频率，或它被告知使用的常数值。它不会学习特征与结果之间的关系。
- en: '![](../Images/6dd5f768865682ac6099b8f627cd9342.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6dd5f768865682ac6099b8f627cd9342.png)'
- en: For calculating predicted probability in binary classification, the Dummy Classifier
    uses the most basic approach possible. Since it only remembered how often each
    outcome appeared in the training data, it uses these same numbers as probability
    scores for every prediction — either 0 or 1.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在二分类任务中，虚拟分类器使用最基本的方法来计算预测概率。由于它仅记住了每个结果在训练数据中出现的频率，它就将这些相同的数字作为每次预测的概率分数——要么是0，要么是1。
- en: '![](../Images/7b157b5391af9509372206c4069d9ea0.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b157b5391af9509372206c4069d9ea0.png)'
- en: These probability scores stay exactly the same for all new data, because the
    model doesn’t look at or react to any features of the new data it’s trying to
    predict.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这些概率分数对于所有新数据来说完全相同，因为该模型并不会查看或响应任何新数据的特征。
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: k-Nearest Neighbors (KNN) Probabilities
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k-最近邻（KNN）概率
- en: '[](/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1?source=post_page-----7c34e8994ec2--------------------------------)
    [## K Nearest Neighbor Classifier, Explained: A Visual Guide with Code Examples
    for Beginners'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1?source=post_page-----7c34e8994ec2--------------------------------)
    [## K 最近邻分类器详解：面向初学者的可视化指南和代码示例'
- en: The friendly neighbor approach to machine learning
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 友邻方法在机器学习中的应用
- en: towardsdatascience.com](/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1?source=post_page-----7c34e8994ec2--------------------------------)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1?source=post_page-----7c34e8994ec2--------------------------------)
- en: K-Nearest Neighbors (kNN) is a prediction model that takes a different approach
    — instead of learning rules, it keeps all training examples in memory. When it
    needs to make a prediction about new data, it measures how similar this data is
    to every stored example, finds the k most similar ones (where k is a number we
    choose), and makes its decision based on those neighbors.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: K-最近邻（kNN）是一种预测模型，它采用了不同的方法——它不是学习规则，而是将所有训练示例存储在内存中。当它需要对新数据做出预测时，它会衡量这些数据与每个存储的示例之间的相似度，找到最相似的k个（k是我们选择的数字），并基于这些邻居做出决策。
- en: When this model finishes training, all it has stored is the complete training
    dataset, the value of k we chose, and a method for measuring how similar two data
    points are (by default using Euclidean distance).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 当这个模型完成训练时，它所存储的只有完整的训练数据集、我们选择的k值以及一种衡量两个数据点相似度的方法（默认使用欧几里得距离）。
- en: '![](../Images/226337b949ff12aa339c2625a91f47df.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/226337b949ff12aa339c2625a91f47df.png)'
- en: For calculating predicted probability, kNN looks at those k most similar examples
    and counts how many belong to each class. The probability score is simply the
    number of neighbors belonging to a class divided by k.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算预测概率时，kNN会查看那些最相似的k个样本，并统计每个类别的样本数量。概率分数就是属于某一类别的邻居数量除以k。
- en: '![](../Images/d6411613689379bcd1ad1878bbb7005b.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6411613689379bcd1ad1878bbb7005b.png)'
- en: Since kNN calculates probability scores by division, it can only give certain
    specific values based on k (say, for k=5, the only possible probability scores
    are 0/5 (0%), 1/5 (20%), 2/5 (40%), 3/5 (60%), 4/5 (80%), and 5/5 (100%)). This
    means kNN can’t give as many different confidence levels as other models.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 由于kNN通过除法计算概率分数，它只能根据k给出特定的值（例如，对于k=5，唯一可能的概率分数是0/5（0%）、1/5（20%）、2/5（40%）、3/5（60%）、4/5（80%）和5/5（100%））。这意味着kNN无法像其他模型那样给出更多的置信度级别。
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Naive Bayes Probabilities
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朴素贝叶斯概率
- en: '[](/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6?source=post_page-----7c34e8994ec2--------------------------------)
    [## Bernoulli Naive Bayes, Explained: A Visual Guide with Code Examples for Beginners'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6?source=post_page-----7c34e8994ec2--------------------------------)
    [## 伯努利朴素贝叶斯解释：初学者的视觉指南与代码示例'
- en: Unlocking predictive power through Yes/No probability
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过是/否概率解锁预测能力
- en: towardsdatascience.com](/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6?source=post_page-----7c34e8994ec2--------------------------------)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6?source=post_page-----7c34e8994ec2--------------------------------)
- en: 'Naive Bayes is a prediction model that uses probability math with a “naive”
    rule: it assumes each feature affects the outcome independently. There are different
    types of Naive Bayes: Gaussian Naive Bayes works with continuous values, while
    Bernoulli Naive Bayes works with binary features. As our dataset has many 0–1
    features, we’ll focus on the Bernoulli one here.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯是一种预测模型，使用概率数学和“朴素”规则：它假设每个特征独立地影响结果。朴素贝叶斯有不同的类型：高斯朴素贝叶斯适用于连续值，而伯努利朴素贝叶斯适用于二元特征。由于我们的数据集有许多
    0 和 1 特征，下面我们将重点讲解伯努利模型。
- en: 'When this model finishes training, it remembers probability values: one value
    for how often the positive class occurs, and for each feature, values showing
    how likely different feature values appear when we have a positive outcome.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当这个模型训练完成时，它会记住概率值：一个值表示正类发生的频率，对于每个特征，值表示在正类结果下不同特征值出现的可能性。
- en: '![](../Images/287032349f3374bb29ca0948ea437c0e.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/287032349f3374bb29ca0948ea437c0e.png)'
- en: 'For calculating predicted probability, Naive Bayes multiplies several probabilities
    together: the chance of each class occurring, and the chance of seeing each feature
    value within that class. These multiplied probabilities are then normalized so
    they sum to 1, giving us the final probability scores.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算预测概率，朴素贝叶斯将多个概率相乘：每个类别发生的概率，以及在该类别中观察到每个特征值的概率。这些乘积的概率随后会进行归一化，使其和为 1，从而得到最终的概率得分。
- en: '![](../Images/64912066b621b73326a6d1ba4026bd6c.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/64912066b621b73326a6d1ba4026bd6c.png)'
- en: Since Naive Bayes uses probability math, its probability scores naturally fall
    between 0 and 1\. However, when certain features strongly point to one class over
    another, the model can give probability scores very close to 0 or 1, showing it’s
    very confident about its prediction.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 由于朴素贝叶斯使用概率数学，它的概率得分自然落在 0 和 1 之间。然而，当某些特征强烈指向某一类别时，模型可能会给出非常接近 0 或 1 的概率得分，表明它对预测非常有信心。
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Decision Tree Probabilities
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树概率
- en: '[](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=post_page-----7c34e8994ec2--------------------------------)
    [## Decision Tree Classifier, Explained: A Visual Guide with Code Examples for
    Beginners'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=post_page-----7c34e8994ec2--------------------------------)
    [## 决策树分类器解释：初学者的视觉指南与代码示例'
- en: A fresh look on our favorite upside-down tree
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们最喜欢的倒立树的新视角
- en: towardsdatascience.com](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=post_page-----7c34e8994ec2--------------------------------)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=post_page-----7c34e8994ec2--------------------------------)
- en: A Decision Tree Classifier works by creating a series of yes/no questions about
    the input data. It builds these questions one at a time, always choosing the most
    useful question that best separates the data into groups. It keeps asking questions
    until it reaches a final answer at the end of a branch.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树分类器通过针对输入数据创建一系列是/否问题来工作。它逐一构建这些问题，总是选择最有用的问题，能够最好地将数据分成不同的组。它会不断提问，直到到达每个分支的最终答案。
- en: When this model finishes training, it has created a tree where each point represents
    a question about the data. Each branch shows which way to go based on the answer,
    and at the end of each branch is information about how often each class appeared
    in the training data.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当这个模型训练完成时，它会创建一棵树，其中每个节点代表一个关于数据的问题。每个分支表示根据答案应采取的路径，而每个分支的末端则显示该类别在训练数据中出现的频率。
- en: '![](../Images/9287974aef17afe32b318ba1e212cf2f.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9287974aef17afe32b318ba1e212cf2f.png)'
- en: For calculating predicted probability, the Decision Tree follows all its questions
    for new data until it reaches the end of a branch. The probability score is based
    on how many training examples of each class ended up at that same branch during
    training.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算预测概率时，决策树会根据新数据依次回答所有问题，直到到达某一分支的末端。概率分数基于每个类别在训练期间有多少训练样本最终到达该分支。
- en: '![](../Images/e39929dd06dd01db3fe0375771fc6a53.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e39929dd06dd01db3fe0375771fc6a53.png)'
- en: Since Decision Tree probability scores come from counting training examples
    at each branch endpoint, they can only be certain values that were seen during
    training. This means the model can only give probability scores that match the
    patterns it found while learning, which limits how precise its confidence levels
    can be.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 由于决策树的概率分数来源于计数每个分支端点的训练样本，因此它们只能是训练中见过的特定值。这意味着模型只能给出与它在学习过程中发现的模式匹配的概率分数，这限制了它的置信度水平的精确度。
- en: '[PRE5]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Logistic Regression Probabilities
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归概率
- en: '[](/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505?source=post_page-----7c34e8994ec2--------------------------------)
    [## Logistic Regression, Explained: A Visual Guide with Code Examples for Beginners'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505?source=post_page-----7c34e8994ec2--------------------------------)
    [## 逻辑回归解析：初学者的视觉指南与代码示例'
- en: Finding the perfect weights to fit the data in
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 找到最适合数据的完美权重
- en: towardsdatascience.com](/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505?source=post_page-----7c34e8994ec2--------------------------------)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505?source=post_page-----7c34e8994ec2--------------------------------)
- en: A Logistic Regression model, despite its name, predicts between two classes
    using a mathematical equation. For each feature in the input data, it learns how
    important that feature is by giving it a number (weight). It also learns one extra
    number (bias) that helps make better predictions. To turn these numbers into a
    predicted probability, it uses the sigmoid function that keeps the final answer
    between 0 and 1.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管名称为逻辑回归，逻辑回归模型通过一个数学方程来预测两个类别之间的概率。对于输入数据中的每个特征，它通过赋予特征一个数字（权重）来学习该特征的重要性。它还会学习一个额外的数字（偏差），以帮助做出更好的预测。为了将这些数字转化为预测概率，它使用了一个sigmoid函数，该函数将最终答案保持在0和1之间。
- en: When this model finishes training, all it remembers is these weights — one number
    for each feature, plus the bias number. These numbers are all it needs to make
    predictions.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当该模型完成训练时，它所记住的只是这些权重——每个特征的一个数字，加上偏差值。这些数字是它进行预测所需要的全部内容。
- en: '![](../Images/74dcf333c4ede161645adab7e74ed268.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/74dcf333c4ede161645adab7e74ed268.png)'
- en: For calculating predicted probability in binary classification, Logistic Regression
    first multiplies each feature value by its weight and adds them all together,
    plus the bias. This sum could be any number, so the model uses the sigmoid function
    to convert it into a probability between 0 and 1.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在二分类中计算预测概率时，逻辑回归首先将每个特征值与其权重相乘，然后将所有结果相加，再加上偏差。这些和可能是任何数字，因此模型使用sigmoid函数将其转换为0到1之间的概率。
- en: '![](../Images/3d83f16c8d25348a9fea8afc8104ffbf.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d83f16c8d25348a9fea8afc8104ffbf.png)'
- en: Unlike other models that can only give certain specific probability scores,
    Logistic Regression can give any probability between 0 and 1\. The further the
    input data is from the point where the model switches from one class to another
    (the decision boundary), the closer the probability gets to either 0 or 1\. Data
    points near this switching point get probabilities closer to 0.5, showing the
    model is less confident about these predictions.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 与只能给出特定概率分数的其他模型不同，逻辑回归可以给出介于0和1之间的任何概率。输入数据距离模型从一个类别切换到另一个类别的点（决策边界）越远，概率就越接近0或1。接近这个切换点的数据点的概率接近0.5，显示模型对这些预测的信心较低。
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Support Vector Machine (SVM) Probabilities
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持向量机（SVM）概率
- en: '[](/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9?source=post_page-----7c34e8994ec2--------------------------------)
    [## Support Vector Classifier, Explained: A Visual Guide with Mini 2D Dataset'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9?source=post_page-----7c34e8994ec2--------------------------------)
    [## 支持向量分类器解析：使用迷你2D数据集的视觉指南'
- en: Finding the best “line” to separate the classes? Yeah, sure...
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 找到最佳的“线”来区分类别？当然...
- en: towardsdatascience.com](/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9?source=post_page-----7c34e8994ec2--------------------------------)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9?source=post_page-----7c34e8994ec2--------------------------------)'
- en: A Support Vector Machine (SVM) Classifier works by finding the best boundary
    line (or surface) that separates different classes. It focuses on the points closest
    to this boundary (called support vectors). While the basic SVM finds straight
    boundary lines, it can also create curved boundaries using mathematical functions
    called kernels.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机（SVM）分类器通过寻找最佳边界线（或面）来区分不同的类别。它关注那些离边界最近的点（即支持向量）。虽然基本的SVM找到的是直线边界，但它也可以通过使用叫做核函数的数学函数来创建弯曲的边界。
- en: 'When this model finishes training, it remembers three things: the important
    points near the boundary (support vectors), how much each point matters (weights),
    and any settings for curved boundaries (kernel parameters). Together, these define
    where and how the boundary separates the classes.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当这个模型完成训练时，它记住了三件事：边界附近的重要点（支持向量），每个点的重要性（权重），以及任何关于弯曲边界的设置（核函数参数）。这些共同定义了边界如何以及在哪里分离各个类别。
- en: '![](../Images/625ea19ad5b9567b0d3ad6c8fa7d6800.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/625ea19ad5b9567b0d3ad6c8fa7d6800.png)'
- en: For calculating predicted probability in binary classification, SVM needs an
    extra step because it wasn’t designed to give probability scores. It uses a method
    called Platt Scaling, which adds a Logistic Regression layer to convert distances
    from the boundary into probabilities. These distances go through the sigmoid function
    to get final probability scores.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在二分类中计算预测概率时，支持向量机（SVM）需要额外的步骤，因为它最初并不是为了提供概率分数而设计的。它使用一种叫做Platt Scaling的方法，通过添加一个逻辑回归层，将距离边界的距离转换为概率。这些距离经过sigmoid函数处理，得到最终的概率分数。
- en: '![](../Images/b5e58a416fce00dca108a284ebcdeb9a.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5e58a416fce00dca108a284ebcdeb9a.png)'
- en: Since SVM calculates probabilities this indirect way, the scores show how far
    points are from the boundary rather than true confidence levels. Points far from
    the boundary get probability scores closer to 0 or 1, while points near the boundary
    get scores closer to 0.5\. This means the probability scores are more about location
    relative to the boundary than the model’s actual confidence in its predictions.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 由于SVM是以这种间接的方式计算概率，得分显示的是点离边界的距离，而不是模型的真实置信度。离边界较远的点会得到接近0或1的概率分数，而靠近边界的点则会得到接近0.5的分数。这意味着概率分数更多地反映了点相对于边界的位置，而非模型对其预测的实际信心。
- en: '[PRE7]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Multilayer Perceptron Probabilities
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层感知机概率
- en: '[](/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c?source=post_page-----7c34e8994ec2--------------------------------)
    [## Multilayer Perceptron, Explained: A Visual Guide with Mini 2D Dataset'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c?source=post_page-----7c34e8994ec2--------------------------------)
    [## 多层感知机详解：带有迷你2D数据集的可视化指南'
- en: Dissecting the math (with visuals) of a tiny neural network
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 剖析一个小型神经网络的数学原理（附带图示）
- en: towardsdatascience.com](/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c?source=post_page-----7c34e8994ec2--------------------------------)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c?source=post_page-----7c34e8994ec2--------------------------------)'
- en: A Multi-Layer Perceptron (MLP) Classifier is a type of neural network that processes
    data through several layers of connected nodes (neurons). Each neuron calculates
    a weighted total of its inputs, transforms this number using a function (like
    ReLU), and sends the result to the next layer. For binary classification, the
    last layer uses the sigmoid function to give an output between 0 and 1.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知机（MLP）分类器是一种神经网络，通过几个层级的连接节点（神经元）处理数据。每个神经元计算其输入的加权总和，使用一个函数（如ReLU）对这个数值进行转换，然后将结果传递到下一层。对于二分类问题，最后一层使用sigmoid函数输出一个介于0和1之间的值。
- en: 'When this model finishes training, it remembers two main things: the connection
    strengths (weights and biases) between neurons in neighboring layers, and how
    the network is structured (how many layers and neurons are in each layer).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 当这个模型完成训练时，它记住了两件主要的事：相邻层神经元之间的连接强度（权重和偏置），以及网络的结构（每一层有多少层和神经元）。
- en: '![](../Images/ff2e6b7f72a10d9cf94c129377be153e.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff2e6b7f72a10d9cf94c129377be153e.png)'
- en: For calculating predicted probability in binary classification, the MLP moves
    data through its layers, with each layer creating more complex combinations of
    information from the previous layer. The final layer produces a number that the
    sigmoid function converts into a probability between 0 and 1.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在二分类问题中，**多层感知机**（MLP）通过其层次处理数据，每一层都根据前一层的信息组合出更复杂的特征。最终一层生成一个数字，通过**Sigmoid函数**将其转换为0到1之间的概率。
- en: '![](../Images/70865a4ff395a3529685a4435992f71f.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/70865a4ff395a3529685a4435992f71f.png)'
- en: The MLP can find more complex patterns in data than many other models because
    it combines features in advanced ways. The final probability score shows how confident
    the network is — scores close to 0 or 1 mean the network is very confident about
    its prediction, while scores near 0.5 indicate it’s uncertain.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**多层感知机**（MLP）能够比许多其他模型找到更复杂的数据模式，因为它以更先进的方式组合特征。最终的概率分数显示了网络的置信度——接近0或1的分数意味着网络对其预测非常有信心，而接近0.5的分数则表示网络的不确定性较大。'
- en: '[PRE8]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Model Comparison
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型比较
- en: 'To summarize, here’s how each classifier calculates predicted probabilities:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，以下是每个分类器计算预测概率的方式：
- en: '**Dummy Classifier**: Uses the same probability scores for all predictions,
    based only on how often each class appeared in training. Ignores all input features.'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**虚拟分类器**：对所有预测使用相同的概率分数，这些分数仅基于每个类别在训练集中出现的频率，忽略所有输入特征。'
- en: '**K-Nearest Neighbors**: The probability score is the fraction of similar neighbors
    belonging to each class. Can only give specific fractions based on k (like 3/5
    or 7/10).'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**K近邻**：概率分数是属于每个类别的相似邻居的比例。只能给出基于k的特定比例（例如3/5或7/10）。'
- en: '**Naive Bayes**: Multiplies together the initial class probability and probabilities
    of seeing each feature value, then adjusts the results to add up to 1\. Probability
    scores show how likely features are to appear in each class.'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**朴素贝叶斯**：将初始的类概率与每个特征值的概率相乘，然后调整结果使其总和为1。概率分数表示特征在每个类中出现的可能性。'
- en: '**Decision Tree**: Gives probability scores based on how often each class appeared
    in the final branches. Can only use probability values that it saw during training.'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**决策树**：根据每个类别在最终分支中出现的频率来给出概率分数。只能使用训练过程中看到的概率值。'
- en: '**Logistic Regression**: Uses the sigmoid function to convert weighted feature
    combinations into probability scores. Can give any probability between 0 and 1,
    changing smoothly based on distance from the decision boundary.'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**逻辑回归**：使用**Sigmoid函数**将加权的特征组合转换为概率分数。可以给出介于0和1之间的任意概率，且随着距离决策边界的变化平滑变化。'
- en: '**Support Vector Machine**: Needs an extra step (Platt Scaling) to create probability
    scores, using the sigmoid function to convert distances from the boundary. These
    distances determine how confident the model is.'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**支持向量机**：需要额外的步骤（Platt缩放）来创建概率分数，使用**Sigmoid函数**将距离边界的值转换为概率。这些距离决定了模型的置信度。'
- en: '**Multi-Layer Perceptron**: Processes data through multiple layers of transformations,
    ending with the sigmoid function. Creates probability scores from complex feature
    combinations, giving any value between 0 and 1.'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**多层感知机**（MLP）：通过多个层次的转换处理数据，最后通过**Sigmoid函数**输出概率分数。通过复杂的特征组合创建概率分数，给出0到1之间的任意值。'
- en: Final Remark
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最后的说明
- en: 'Looking at how each model calculates its predicted probability shows us something
    important: each model has its own way of showing how confident it is. Some models
    like the Dummy Classifier and Decision Tree can only use certain probability scores
    based on their training data. Others like Logistic Regression and Neural Networks
    can give any probability between 0 and 1, letting them be more precise about their
    uncertainty.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 观察每个模型如何计算预测概率，能揭示一个重要的信息：每个模型都有自己表达置信度的方式。有些模型，如**虚拟分类器**和**决策树**，只能使用基于其训练数据的某些概率分数。而像**逻辑回归**和**神经网络**这样的模型可以给出介于0和1之间的任意概率，使得它们在表达不确定性时更为精确。
- en: 'Here’s what’s interesting: even though all these models give us numbers between
    0 and 1, these numbers mean different things for each model. Some get their scores
    by simple counting, others by measuring distance from a boundary, and some through
    complex calculations with features. This means a 70% probability from one model
    tells us something completely different than a 70% from another model.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有个有趣的地方：尽管所有这些模型都给出介于0和1之间的数值，但这些数值对于每个模型来说含义不同。有些模型通过简单计数得到分数，有些则通过测量与边界的距离来计算，还有一些通过复杂的特征计算得到结果。这意味着一个模型给出的70%概率，和另一个模型的70%概率告诉我们的是完全不同的信息。
- en: When picking a model to use, look beyond just accuracy. Think about whether
    the way it calculates predicted probability makes sense for your specific needs.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择模型时，不要只看准确率。思考一下它计算预测概率的方式是否适合你的具体需求。
- en: 🌟 Predicted Probability Code Summarized
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🌟 预测概率代码总结
- en: '[PRE9]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Technical Environment
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术环境
- en: This article uses Python 3.7 and scikit-learn 1.5\. While the concepts discussed
    are generally applicable, specific code implementations may vary slightly with
    different versions.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 本文使用的是 Python 3.7 和 scikit-learn 1.5。虽然讨论的概念具有广泛的适用性，但具体的代码实现可能会因版本不同而有所不同。
- en: About the Illustrations
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于插图
- en: Unless otherwise noted, all images are created by the author, incorporating
    licensed design elements from Canva Pro.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图片均由作者创建，并融入了来自 Canva Pro 的授权设计元素。
- en: '𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝙈𝙤𝙙𝙚𝙡 𝙀𝙫𝙖𝙡𝙪𝙖𝙩𝙞𝙤𝙣 & 𝙊𝙥𝙩𝙞𝙢𝙞𝙯𝙖𝙩𝙞𝙤𝙣 𝙢𝙚𝙩𝙝𝙤𝙙𝙨 𝙝𝙚𝙧𝙚:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝙈𝙤𝙙𝙚𝙡 𝙀𝙫𝙖𝙡𝙪𝙖𝙩𝙞𝙤𝙣 & 𝙊𝙥𝙩𝙞𝙢𝙞𝙯𝙖𝙩𝙞𝙤𝙣 𝙢𝙚𝙩𝙝𝙤𝙙𝙨 𝙝𝙚𝙧𝙚:'
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----7c34e8994ec2--------------------------------)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----7c34e8994ec2--------------------------------)'
- en: Model Evaluation & Optimization
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型评估与优化
- en: '[View list](https://medium.com/@samybaladram/list/model-evaluation-optimization-331287896864?source=post_page-----7c34e8994ec2--------------------------------)3
    stories![](../Images/18fa82b1435fa7d5571ee54ae93a6c62.png)![](../Images/c95e89d05d1de700c631c342cd008de0.png)![](../Images/30e20e1a8ba3ced1e77644b706acd18d.png)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://medium.com/@samybaladram/list/model-evaluation-optimization-331287896864?source=post_page-----7c34e8994ec2--------------------------------)3
    个故事![](../Images/18fa82b1435fa7d5571ee54ae93a6c62.png)![](../Images/c95e89d05d1de700c631c342cd008de0.png)![](../Images/30e20e1a8ba3ced1e77644b706acd18d.png)'
- en: '𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:'
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----7c34e8994ec2--------------------------------)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----7c34e8994ec2--------------------------------)'
- en: Classification Algorithms
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类算法
- en: '[View list](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----7c34e8994ec2--------------------------------)8
    stories![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)![Samy
    Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----7c34e8994ec2--------------------------------)8
    个故事![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)![Samy
    Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----7c34e8994ec2--------------------------------)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----7c34e8994ec2--------------------------------)'
- en: Ensemble Learning
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成学习
- en: '[View list](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----7c34e8994ec2--------------------------------)4
    stories![](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----7c34e8994ec2--------------------------------)4
    个故事![](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)'
