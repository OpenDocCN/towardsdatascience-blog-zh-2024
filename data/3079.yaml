- en: Your Company Needs Small Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你的公司需要小型语言模型
- en: 原文：[https://towardsdatascience.com/your-company-needs-small-language-models-d0a223e0b6d9?source=collection_archive---------0-----------------------#2024-12-26](https://towardsdatascience.com/your-company-needs-small-language-models-d0a223e0b6d9?source=collection_archive---------0-----------------------#2024-12-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/your-company-needs-small-language-models-d0a223e0b6d9?source=collection_archive---------0-----------------------#2024-12-26](https://towardsdatascience.com/your-company-needs-small-language-models-d0a223e0b6d9?source=collection_archive---------0-----------------------#2024-12-26)
- en: '![](../Images/26913b776ae04e4f9a355fe2fa31fad8.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26913b776ae04e4f9a355fe2fa31fad8.png)'
- en: Image generated by Stable Diffusion
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 由Stable Diffusion生成的图像
- en: When specialized models outperform general-purpose models
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 当专用模型超过通用模型时
- en: '[](https://slgero.medium.com/?source=post_page---byline--d0a223e0b6d9--------------------------------)[![Sergei
    Savvov](../Images/a653eaeeec954f1a71e6341b424f009a.png)](https://slgero.medium.com/?source=post_page---byline--d0a223e0b6d9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d0a223e0b6d9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d0a223e0b6d9--------------------------------)
    [Sergei Savvov](https://slgero.medium.com/?source=post_page---byline--d0a223e0b6d9--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://slgero.medium.com/?source=post_page---byline--d0a223e0b6d9--------------------------------)[![Sergei
    Savvov](../Images/a653eaeeec954f1a71e6341b424f009a.png)](https://slgero.medium.com/?source=post_page---byline--d0a223e0b6d9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d0a223e0b6d9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d0a223e0b6d9--------------------------------)
    [Sergei Savvov](https://slgero.medium.com/?source=post_page---byline--d0a223e0b6d9--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d0a223e0b6d9--------------------------------)
    ·12 min read·Dec 26, 2024
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d0a223e0b6d9--------------------------------)
    ·12分钟阅读·2024年12月26日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: “Bigger is always better” — this principle is deeply rooted in the AI world.
    Every month, larger models are created, with more and more parameters. Companies
    are even building [$10 billion AI data centers](https://www.datacenterfrontier.com/hyperscale/article/55248311/meta-sees-10b-ai-data-center-in-louisiana-using-combo-of-clean-energy-nuclear-power)
    for them. But is it the only direction to go?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: “更大总是更好”——这一原则在AI领域根深蒂固。每个月，更多的参数和更大的模型被创造出来。公司甚至为此建立了[$100亿的AI数据中心](https://www.datacenterfrontier.com/hyperscale/article/55248311/meta-sees-10b-ai-data-center-in-louisiana-using-combo-of-clean-energy-nuclear-power)。但是，这真的是唯一的方向吗？
- en: 'At [NeurIPS 2024, Ilya Sutskever](https://www.youtube.com/watch?v=1yvBqasHLZs),
    one of OpenAI’s co-founders, shared an idea: *“Pre-training as we know it will
    unquestionably end”*. It seems the **era of scaling is coming to a close**, which
    means it’s time to focus on improving current approaches and algorithms.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在[NeurIPS 2024会议上，OpenAI的联合创始人Ilya Sutskever](https://www.youtube.com/watch?v=1yvBqasHLZs)分享了一个观点：“*我们所知的预训练将无可避免地结束*”。看起来**规模化时代即将结束**，这意味着现在是时候集中精力改进现有的方法和算法了。
- en: 'One of the most promising areas is the use of small language models (SLMs)
    with up to 10B parameters. This approach is really starting to take off in the
    industry. For example, Clem Delangue, CEO of Hugging Face, [predicts that up to
    99% of use cases could be addressed using SLMs](https://www.linkedin.com/posts/clementdelangue_ive-said-it-and-will-say-it-again-1-activity-7112524134395318273-T3z6/).
    A similar trend is evident in the [latest requests for startups by YC](https://www.ycombinator.com/rfs#summer-2024-small-fine-tuned-models-as-an-alternative-to-giant-generic-ones):'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最有前景的领域之一是使用最多10B参数的小型语言模型（SLMs）。这种方法在行业中真正开始起飞。例如，Hugging Face的CEO Clem Delangue，[预测最多99%的使用案例可以通过SLMs来解决](https://www.linkedin.com/posts/clementdelangue_ive-said-it-and-will-say-it-again-1-activity-7112524134395318273-T3z6/)。YC的[最新创业请求](https://www.ycombinator.com/rfs#summer-2024-small-fine-tuned-models-as-an-alternative-to-giant-generic-ones)中也可以看到类似的趋势：
- en: Giant generic models with a lot of parameters are very impressive. But they
    are also very costly and often come with latency and privacy challenges.
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 拥有大量参数的巨型通用模型非常令人印象深刻。但它们也非常昂贵，且往往面临延迟和隐私问题。
- en: 'In my last article “[You don’t need hosted LLMs, do you?](https://betterprogramming.pub/you-dont-need-hosted-llms-do-you-1160b2520526)”,
    I wondered if you need self-hosted models. Now I take it a step further and ask
    the question: **do you need LLMs at all?**'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在我上一篇文章“[你真的需要托管的LLM吗？](https://betterprogramming.pub/you-dont-need-hosted-llms-do-you-1160b2520526)”中，我曾经想过你是否需要自托管的模型。现在我更进一步，提出一个问题：**你根本需要LLM吗？**
- en: '![](../Images/72ad67f12c5f765e561a3aab710ca34c.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/72ad67f12c5f765e561a3aab710ca34c.png)'
- en: “Short” summary of the article.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 文章的“简短”总结。
- en: In this article, I’ll discuss why small models may be the solution your business
    needs. We’ll talk about how they can reduce costs, improve accuracy, and maintain
    control of your data. And of course, we’ll have an honest discussion about their
    limitations.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将讨论为什么小型模型可能是您企业所需的解决方案。我们将探讨它们如何减少成本、提高准确性，并保持对您数据的控制。当然，我们还将诚实地讨论它们的局限性。
- en: Cost Efficiency
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 成本效益
- en: 'The economics of LLMs is probably one of the most painful topics for businesses.
    However, the issue is much broader: it includes the need for expensive hardware,
    infrastructure costs, energy costs and environmental consequences.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）的经济学可能是企业最痛苦的话题之一。然而，这个问题远不止如此：它包括对昂贵硬件的需求、基础设施成本、能源成本和环境后果。
- en: Yes, large language models are impressive in their capabilities, but they are
    also very expensive to maintain. You may have already noticed how subscription
    prices for LLMs-based applications have risen? For example, OpenAI’s recent announcement
    of a **$200/month** Pro plan is a signal that costs are rising. And it’s likely
    that competitors will also move up to these price levels.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，大型语言模型在其能力上令人印象深刻，但它们的维护成本也非常高。您可能已经注意到基于LLM的应用程序订阅价格的上涨？例如，OpenAI最近宣布的**每月$200**专业版计划就是成本上升的信号。而且，很可能竞争对手也会将价格提高到这些水平。
- en: '![](../Images/2b41027953cd666454647c2b116ac0b3.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b41027953cd666454647c2b116ac0b3.png)'
- en: $200 for Pro plan
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 专业版计划，$200
- en: '[The Moxie robot story](https://arstechnica.com/gadgets/2024/12/startup-will-brick-800-emotional-support-robot-for-kids-without-refunds/)
    is a good example of this statement. Embodied created a great companion robot
    for kids for $800 that used the OpenAI API. Despite the success of the product
    (kids were sending 500–1000 messages a day!), the company [is shutting down](https://moxierobot.com/pages/closing-faqs)
    due to the high operational costs of the API. Now thousands of robots will become
    useless and kids will lose their friend.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[Moxie机器人故事](https://arstechnica.com/gadgets/2024/12/startup-will-brick-800-emotional-support-robot-for-kids-without-refunds/)是这一观点的一个很好的例子。Embodied为孩子们创造了一个优秀的伴侣机器人，售价为$800，使用了OpenAI
    API。尽管产品取得了成功（孩子们每天发送500到1000条消息！），但由于API的高运营成本，公司[正在关闭](https://moxierobot.com/pages/closing-faqs)。现在，成千上万的机器人将变得无用，孩子们将失去他们的朋友。'
- en: One approach is to **fine-tune a specialized Small Language Model for your specific
    domain**. Of course, it will not solve “all the problems of the world”, but it
    will perfectly cope with the task it is assigned to. For example, analyzing client
    documentation or generating specific reports. At the same time, SLMs will be more
    economical to maintain, consume fewer resources, require less data, and can run
    on much more modest hardware ([up to a smartphone](https://privatellm.app/blog/run-local-gpt-on-ios-complete-guide)).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法是**针对您的特定领域对小型语言模型进行微调**。当然，它并不能解决“世界上所有的问题”，但它能完美地应对分配给它的任务。例如，分析客户文档或生成特定报告。与此同时，小型语言模型在维护上更具经济性，消耗的资源更少，所需的数据量更小，并且可以在更加简朴的硬件上运行（[甚至是智能手机](https://privatellm.app/blog/run-local-gpt-on-ios-complete-guide)）。
- en: '![](../Images/ebee1b3fa08f2a9ac511bdd47215c967.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ebee1b3fa08f2a9ac511bdd47215c967.png)'
- en: Comparison of utilization of models with different number of parameters. [Source1](https://arxiv.org/pdf/2404.08850),
    [source2](https://adasci.org/how-much-energy-do-llms-consume-unveiling-the-power-behind-ai/),
    [source3](https://huggingface.co/blog/inference-dgx-cloud), [source4](https://llamaimodel.com/price/).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 不同参数量模型的使用对比。[来源1](https://arxiv.org/pdf/2404.08850)，[来源2](https://adasci.org/how-much-energy-do-llms-consume-unveiling-the-power-behind-ai/)，[来源3](https://huggingface.co/blog/inference-dgx-cloud)，[来源4](https://llamaimodel.com/price/)。
- en: 'And finally, let’s not forget about the environment. In the [article Carbon
    Emissions and Large Neural Network Training](https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf),
    I found some interesting statistic that amazed me: training GPT-3 with 175 billion
    parameters consumed as much electricity as the average American home consumes
    in 120 years. It also **produced 502 tons of CO₂**, which is comparable to the
    annual operation of more than a hundred gasoline cars. And that’s not counting
    inferential costs. By comparison, deploying a smaller model like the **7B would
    require 5%** of the consumption of a larger model. And what about the latest [o3
    release](https://techcrunch.com/2024/12/20/openai-announces-new-o3-model/)?'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们不能忽视环境因素。在 [《碳排放与大规模神经网络训练》](https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf)
    文章中，我发现了一些让我惊讶的统计数据：训练 GPT-3（拥有 1750 亿参数）所消耗的电力相当于美国普通家庭 120 年的用电量。它还 **排放了 502
    吨 CO₂**，相当于一百多辆汽油车一年的排放量。而这还不包括推理成本。相比之下，部署像 **7B** 这样的较小模型，仅需消耗大型模型 5% 的电力。那最新的
    [o3 版本](https://techcrunch.com/2024/12/20/openai-announces-new-o3-model/)呢？
- en: '![](../Images/e0b5c26438dab27fe0c8d676828452ac.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e0b5c26438dab27fe0c8d676828452ac.png)'
- en: Model o3 CO₂ production. [Source](https://www.linkedin.com/posts/bgamazay_openai-has-announced-o3-which-appears-to-activity-7276250095019335680-sVbW).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟 o3 CO₂ 排放。[来源](https://www.linkedin.com/posts/bgamazay_openai-has-announced-o3-which-appears-to-activity-7276250095019335680-sVbW).
- en: 💡**Hint:** don’t chase the hype. Before tackling the task, calculate the costs
    of using APIs or your own servers. Think about scaling of such a system and how
    justified the use of LLMs is.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 💡**提示：** 不要盲目跟风。在解决任务之前，先计算使用 API 或自己的服务器的成本。考虑这样的系统扩展性，以及使用 LLM 的合理性。
- en: Performance on Specialized Tasks
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 专业任务表现
- en: Now that we’ve covered the economics, let’s talk about quality. Naturally, very
    few people would want to compromise on solution accuracy just to save costs. But
    even here, SLMs have something to offer.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了经济性，接下来谈谈质量。自然，几乎没有人愿意仅仅为了节省成本而在解决方案的准确性上做出妥协。但即便如此，SLM 也有其独特的优势。
- en: '![](../Images/671c7646feb5e05b026184d19a97a9bd.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/671c7646feb5e05b026184d19a97a9bd.png)'
- en: In-domain Moderation Performance. Comparing the performance of SLMs versus LLMs
    on accuracy, recall, and precision for in-domain content moderation performance.
    Best performing SLMs outperform LLMs on accuracy and recall across all subreddits,
    while LLMs outperform SLMs on precision. [Source.](https://arxiv.org/pdf/2410.13155)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 行业内内容审核表现。比较 SLM 和 LLM 在行业内内容审核任务中的准确性、召回率和精度表现。在所有子版块中，表现最好的 SLM 在准确性和召回率上超越了
    LLM，而 LLM 在精度上则超过了 SLM。[来源](https://arxiv.org/pdf/2410.13155)
- en: 'Many studies show that for highly specialized tasks, small models can not only
    compete with large LLMs, but often outperform them. Let’s look at a few illustrative
    examples:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究表明，对于高度专业化的任务，小型模型不仅能够与大型语言模型（LLMs）竞争，甚至往往超过它们。让我们来看几个有代表性的例子：
- en: '**Medicine:** The [Diabetica-7B model](https://arxiv.org/pdf/2409.13191) (based
    on the Qwen2–7B) achieved 87.2% accuracy on diabetes-related tests, while GPT-4
    showed 79.17% and Claude-3.5–80.13%. Despite this, Diabetica-7B is dozens of times
    smaller than GPT-4 and **can run locally on a consumer GPU**.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**医学领域：** [Diabetica-7B 模型](https://arxiv.org/pdf/2409.13191)（基于 Qwen2–7B）在与糖尿病相关的测试中达到了
    87.2% 的准确率，而 GPT-4 显示为 79.17%，Claude-3.5 则为 80.13%。尽管如此，Diabetica-7B 的体积比 GPT-4
    小了几十倍，而且 **可以在消费者级 GPU 上本地运行**。'
- en: '**Legal Sector:** [An SLM with just 0.2B parameters](https://arxiv.org/pdf/2311.09825)
    achieves 77.2% accuracy in contract analysis (GPT-4 — about 82.4%). Moreover,
    for tasks like identifying “unfair” terms in user agreements, the **SLM even outperforms
    GPT-3.5 and GPT-4** on the F1 metric.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**法律领域：** 一个仅有 0.2B 参数的 [SLM](https://arxiv.org/pdf/2311.09825) 在合同分析中的准确率达到了
    77.2%（GPT-4 为约 82.4%）。此外，对于识别用户协议中的“不公平”条款等任务，**SLM 甚至在 F1 指标上超越了 GPT-3.5 和 GPT-4**。'
- en: '**Mathematical Tasks:** [Research by Google DeepMind shows](https://arxiv.org/pdf/2408.16737)
    that training a small model, Gemma2–9B, on data generated by another small model
    yields better results than training on data from the larger Gemma2–27B. Smaller
    models tend to focus better on specifics without the tendency to “trying to shine
    with all the knowledge”, which is often a trait of larger models.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数学任务：** [Google DeepMind 的研究表明](https://arxiv.org/pdf/2408.16737)，训练一个小型模型
    Gemma2–9B，使用另一个小型模型生成的数据，比使用来自更大模型 Gemma2–27B 的数据训练效果更好。较小的模型往往能更好地专注于细节，而不会像大型模型那样“试图通过所有知识来显示自己”。'
- en: '**Content Moderation:** [LLaMA 3.1 8B outperformed](https://arxiv.org/pdf/2410.13155)
    GPT-3.5 in accuracy (by 11.5%) and recall (by 25.7%) when moderating content across
    15 popular subreddits. **This was achieved even with 4-bit quantization**, which
    further reduces the model’s size.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**内容审核：**[LLaMA 3.1 8B超越了](https://arxiv.org/pdf/2410.13155)GPT-3.5，在准确性（提高了11.5%）和召回率（提高了25.7%）方面表现更好，尤其是在审核15个热门子版块的内容时。**即使在4位量化的情况下**，也成功减少了模型的大小。'
- en: '![](../Images/67a12f7e7296b11c505a401955b85fa3.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67a12f7e7296b11c505a401955b85fa3.png)'
- en: Comparison of instruction-tuned domain SLMs for QA and LLMs on PubMedQA. [Source](https://arxiv.org/pdf/2411.03350).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 针对PubMedQA的指令调优领域SLM与LLM的比较。[来源](https://arxiv.org/pdf/2411.03350)。
- en: 'I’ll go a step further and share that even classic NLP approaches often work
    surprisingly well. Let me share a personal case: I’m working on a product for
    psychological support where we process over a thousand messages from users every
    day. They can write in a chat and get a response. Each message is first classified
    into one of four categories:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我将更进一步，分享一个个人案例：我正在开发一个心理支持产品，我们每天处理来自用户的超过一千条消息。用户可以通过聊天发送消息并获得回复。每条消息首先会被分类到以下四类之一：
- en: '![](../Images/a756c82fcba8cae98609aba545fb141c.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a756c82fcba8cae98609aba545fb141c.png)'
- en: Message Classification Scheme.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 消息分类方案。
- en: '`SUPPORT` — A question about how the app works; we respond using the documentation.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SUPPORT` — 关于应用程序如何工作的提问；我们通过文档进行回答。'
- en: '`GRATITUDE` — The user thanks the bot; we simply send a “like.”'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GRATITUDE` — 用户感谢机器人；我们只需发送一个“点赞”。'
- en: '`TRY_TO_HACK` — The user requests something unrelated to the app’s purpose
    (e.g., “Write a function in Python”).'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TRY_TO_HACK` — 用户请求的内容与应用程序的目的无关（例如，“写一个Python函数”）。'
- en: '`OTHER`— All other messages, which we process further.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`OTHER` — 所有其他消息，我们会进一步处理。'
- en: 'Previously, I used GPT-3.5-turbo for classification and later switched to GPT-4o
    mini, spending a lot of time changing the prompt. However, I still encountered
    errors. So, I decided to try a classic approach: TF-IDF + a simple classifier.
    Training took less than a minute, and the Macro F1 score increased to 0.95 (compared
    to 0.92 for GPT-4o mini). The model size is just 76 MB, and when applied to 2
    million processed messages (our actual data), the cost savings were significant:
    the **GPT-based solution would have cost about $500, while the classic approach
    cost almost nothing**.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我使用了GPT-3.5-turbo进行分类，后来切换到GPT-4o mini，花费了大量时间更改提示。但是，我仍然遇到错误。因此，我决定尝试经典方法：TF-IDF
    + 简单分类器。训练时间不到一分钟，宏观F1得分提高到了0.95（相比之下，GPT-4o mini为0.92）。模型大小仅为76MB，当应用于200万条处理过的消息（我们的实际数据）时，节省的成本非常可观：**基于GPT的解决方案大约需要500美元，而经典方法几乎不需要费用**。
- en: '![](../Images/299ede5a52ee02c3ce6ef99f7f448dc2.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/299ede5a52ee02c3ce6ef99f7f448dc2.png)'
- en: 'Accuracy, speed and cost comparison table: GPT-4o mini vs TF-IDF model.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 准确性、速度和成本比较表：GPT-4o mini与TF-IDF模型。
- en: And there are several such “small” and simple tasks in our product. I believe
    you might find the same in your company. Of course, large models are great for
    a quick start, especially when there’s no labeled data and requirements are changing.
    But for well-defined, stable tasks where accuracy and minimal costs are key, specialized
    and simple models (including classic methods) can often be a more effective solution.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的产品中有许多这样的“小”且简单的任务。我相信你们的公司也可能会发现类似的情况。当然，大模型适合快速启动，特别是在没有标注数据且需求不断变化的情况下。但对于那些定义明确、稳定的任务，准确性和最小成本是关键，专用且简单的模型（包括经典方法）往往能提供更有效的解决方案。
- en: 💡**Hint:** use LLMs for prototyping, and then, once the task becomes clear and
    stable, switch to smaller, cheaper, and more accurate models. This hybrid approach
    helps maintain high quality, significantly reduce costs, and avoid the redundancy
    of general-purpose models.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 💡**提示：**使用LLM进行原型设计，当任务变得清晰且稳定时，再切换到更小、更便宜、更精确的模型。这种混合方法有助于保持高质量，显著降低成本，并避免通用模型的冗余。
- en: Security, Privacy and Regulatory
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安全性、隐私和合规性
- en: Using LLMs through APIs, you’re handing over sensitive data to external providers,
    increasing the risk of leaks and complicating compliance with strict regulations
    like HIPAA, GDPR, and CCPA. OpenAI’s recent announcement about plans to introduce
    advertising only highlights these risks. **Your company not only loses full control
    over its data but also becomes dependent on third-party SLAs.**
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通过API使用LLMs，你实际上是将敏感数据交给外部提供商，增加了数据泄漏的风险，并使得遵守严格法规（如HIPAA、GDPR和CCPA）变得更加复杂。OpenAI最近关于引入广告的公告进一步凸显了这些风险。**你的公司不仅失去了对数据的完全控制，还变得依赖于第三方的服务水平协议（SLA）。**
- en: Certainly, it’s possible to run a LLM locally, but the cost of deployment and
    scaling (hundreds of gigabytes of memory, multiple GPUs) often exceeds reasonable
    economic limits and makes it difficult to quickly adapt to new regulatory requirements.
    And you can forget about launching it on low-end hardware.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，可以在本地运行大型语言模型（LLM），但其部署和扩展的成本（数百GB的内存、多个GPU）往往超出合理的经济限制，并且使得快速适应新的监管要求变得困难。你可以忘记在低端硬件上启动它。
- en: '![](../Images/99295d3081eef53a84bd486fdd456dd9.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99295d3081eef53a84bd486fdd456dd9.png)'
- en: Comparison of Cloud API Risks and on-device slm benefits.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 云API风险与设备上SLM优势比较
- en: 'And this is where the “small guys” come back into play:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是“小型模型”重新发挥作用的地方：
- en: 1\. Simplified Audits
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 简化审计
- en: The smaller size of SLMs lowers the barrier for conducting audits, verification,
    and customization to meet specific regulations. It’s easier to understand how
    the model processes data, implement your own encryption or logging, and show auditors
    that information never leaves a trusted environment. As the founder of a healthcare
    company, I know how challenging and crucial this task can be.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: SLM较小的体积降低了进行审计、验证和定制以满足特定法规的门槛。更容易理解模型如何处理数据，实现自定义加密或日志记录，并向审计员展示信息从未离开过受信任的环境。作为一家医疗公司创始人，我深知这一任务有多么具有挑战性和重要性。
- en: '**2\.** Running on Isolated and low-end hardware'
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2.** 在隔离和低端硬件上运行'
- en: 'LLMs are difficult to efficiently “deploy” in an isolated network segment or
    on a smartphone. SLMs, however, with their lower computational requirements, can
    operate almost anywhere: from a local server in a private network to a doctor’s
    or inspector’s device. [According to IDC](https://blogs.idc.com/2024/07/05/the-rise-of-gen-ai-smartphones/)
    forecasts, **by 2028, over 900 million smartphones will be capable of running
    generative AI models locally**.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在隔离的网络段或智能手机上高效“部署”十分困难。然而，考虑到其较低的计算需求，简化语言模型（SLMs）几乎可以在任何地方运行：从私有网络中的本地服务器到医生或检查员的设备。[根据IDC的预测](https://blogs.idc.com/2024/07/05/the-rise-of-gen-ai-smartphones/)，**到2028年，超过9亿部智能手机将能够本地运行生成性AI模型**。
- en: '**3\.** New Regulations Updates and Adaptation'
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3.** 新法规更新与适应'
- en: Regulations and laws change frequently — compact models can be fine-tuned or
    adjusted in hours rather than days. This enables a quick response to new requirements
    without the need for large-scale infrastructure upgrades, which are typical for
    big LLMs.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 法规和法律经常变化——紧凑型模型可以在数小时内进行微调或调整，而不是几天。这使得能够快速响应新的要求，而不需要进行大规模的基础设施升级，这是大规模LLMs的常见特征。
- en: '**4\. Distributed Security Architecture**'
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**4. 分布式安全架构**'
- en: 'Unlike the monolithic architecture of LLMs, where all security components are
    “baked” into one large model, SLMs enable the creation of a distributed security
    system. Each component:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 与LLMs的单体架构不同，LLMs将所有安全组件“集成”到一个大型模型中，而SLMs则允许创建一个分布式安全系统。每个组件：
- en: Specializes in a specific task.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 专注于特定任务。
- en: Can be independently updated and tested.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以独立更新和测试。
- en: Scales separately from the others.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与其他组件分开扩展。
- en: 'For example, a medical application could use a cascade of three models:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个医疗应用可以使用三个模型的级联：
- en: '**Privacy Guardian (2B)** — masks personal data.'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**隐私守护者（2B）**——掩盖个人数据。'
- en: '**Medical Validator (3B)** — ensures medical accuracy.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**医学验证器（3B）**——确保医学准确性。'
- en: '**Compliance Checker (1B)** — monitors HIPAA compliance.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**合规检查器（1B）**——监控HIPAA合规性。'
- en: '**Smaller models are easier to verify and update**, making the overall architecture
    more flexible and reliable.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**较小的模型更容易验证和更新**，使整体架构更加灵活和可靠。'
- en: '![](../Images/c73556307d7ff9a0f482dd5e882ed0da.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c73556307d7ff9a0f482dd5e882ed0da.png)'
- en: Comparison of Data Privacy Features.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 数据隐私特性比较
- en: 💡**Hint:** consider using SLMs if you operate in a heavily regulated field.
    Pay close attention to data transfer policies and the frequency of changes in
    the regulatory landscape. I recommend use SLMs if your professional domain is
    in healthcare, finance, or law.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 💡**提示：**如果你所在的领域受到严格监管，可以考虑使用SLMs。密切关注数据传输政策和监管环境变化的频率。如果你的专业领域是医疗、金融或法律，我推荐使用SLMs。
- en: 'AI Agents: The Perfect Use Case'
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI代理：完美的应用场景
- en: Remember the old [Unix philosophy, “Do one thing and do it well”](https://en.wikipedia.org/wiki/Unix_philosophy)?
    It seems we’re returning to this principle, now in the context of AI.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得旧的[Unix哲学：“做一件事，并把它做得很好”](https://en.wikipedia.org/wiki/Unix_philosophy)吗？现在似乎我们又回到了这一原则，只不过是在人工智能的背景下。
- en: Ilya Sutskever’s recent statement at NeurIPS that “Pre-training as we know it
    will unquestionably end” and that the next generation of models will be “agentic
    in real ways” only confirms this trend. Y Combinator goes even further, predicting
    that [**AI agents could create a market 10 times larger than SaaS**](https://www.youtube.com/watch?v=ASABxNenD_U).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Ilya Sutskever最近在NeurIPS上的声明——“我们所知道的预训练将毫无疑问地结束”以及下代模型将“以真实方式具备代理能力”——只会进一步确认这一趋势。Y
    Combinator甚至预测，[**AI代理可能创造一个比SaaS市场大10倍的市场**](https://www.youtube.com/watch?v=ASABxNenD_U)。
- en: For example, already [12% of enterprise solutions use agent-based architecture](https://menlovc.com/2024-the-state-of-generative-ai-in-the-enterprise/).
    Moreover, analysts predict that agents will be the next wave of AI-transformation
    that can affect not only the $400-billion software market, but also the **$10-trillion
    U.S. services economy**.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，目前已有[12%的企业解决方案采用基于代理的架构](https://menlovc.com/2024-the-state-of-generative-ai-in-the-enterprise/)。此外，分析师预测，代理将成为AI转型的下一个浪潮，不仅能影响4000亿美元的软件市场，还能影响**10万亿美元的美国服务经济**。
- en: And SMLs are ideal candidates for this role. Perhaps one model is quite limited,
    but a swarm of such models — can solve complex tasks piece by piece. **Faster,
    higher quality and cheaper.**
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 而SMLs是这个角色的理想候选者。也许单个模型非常有限，但这样的多个模型——可以逐步解决复杂的任务。**更快、更高质量、更便宜。**
- en: 'Let’s take a concrete example: imagine you are building a system to analyze
    financial documents. Instead of using one large model, you can break the task
    into several specialized agents:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个具体的例子：假设你正在构建一个系统来分析金融文档。你可以将任务拆分成多个专门的代理，而不是使用一个大模型：
- en: '![](../Images/b9593e6888e27316001c963ec8232ee1.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b9593e6888e27316001c963ec8232ee1.png)'
- en: The example flow of information between specialized agents.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 专门代理之间的信息流动示例。
- en: 'And this approach is not only more cost-effective but also more reliable: each
    agent focuses on what it does best. **Cheaper. Faster. Better.** Yes, I’m repeating
    it again.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 而这种方法不仅更具成本效益，还更可靠：每个代理专注于自己最擅长的领域。**更便宜、更快、更好。**是的，我再次重复。
- en: 'To back this up, let me name a few companies:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持这一点，让我列举几个公司：
- en: '[**H Company**](https://www.hcompany.ai/) raised $100M in a seed round to develop
    a multi-agent system based on SLMs (2–3B parameters). Their agent Runner H (3B)
    achieves 67% task completion success compared to Anthropic’s Computer Use at 52%,
    all **with significantly lower costs**.'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**H公司**](https://www.hcompany.ai/)在种子轮融资中筹集了1亿美元，旨在开发基于SLMs的多代理系统（2-3B参数）。他们的代理Runner
    H（3B）完成任务的成功率为67%，相比之下，Anthropic的Computer Use为52%，**且成本显著更低**。'
- en: '[**Liquid AI**](https://www.liquid.ai/) recently secured $250M in funding,
    focusing on building efficient enterprise models. Their model (1.3B parameters)
    has outperformed all existing models of similar size. Meanwhile, their LFM-3B
    delivers performance on par with 7B and even 13B models **while requiring less
    memory.**'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**Liquid AI**](https://www.liquid.ai/)最近获得了2.5亿美元的融资，专注于构建高效的企业模型。他们的模型（1.3B参数）超越了所有类似规模的现有模型。同时，他们的LFM-3B在性能上与7B甚至13B模型不相上下，**同时需要更少的内存**。'
- en: '[**Cohere**](https://cohere.com/) launched Command R7B, a specialized model
    for RAG applications that can even **run on a CPU**. The model supports 23 languages
    and integrates with external tools, showing best-in-class results for reasoning
    and question-answering tasks.'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**Cohere**](https://cohere.com/)推出了Command R7B，这是一款专门针对RAG应用的模型，甚至可以**在CPU上运行**。该模型支持23种语言，并与外部工具集成，在推理和问答任务中表现出最顶级的结果。'
- en: '**YOUR COMPANY NAME** could also join this list. I’m not just saying that —
    in [Reforma Health](https://reforma.health/), the company I’m working on, is developing
    specialized SLMs for various medical domains. This decision was driven by the
    need to comply with HIPAA requirements and the specifics of medical information
    processing. Our experience shows that highly **specialized SLMs can be a significant
    competitive advantage**, especially in regulated domains.'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**贵公司名称**也可以加入这个名单。我不仅仅是这么说的——在我工作的公司[Reforma Health](https://reforma.health/)，我们正在开发针对各种医疗领域的专用SLM。做出这一决定是因为需要遵守HIPAA规定以及医疗信息处理的具体要求。我们的经验表明，高度**专用的SLM可以成为显著的竞争优势**，尤其是在受监管的领域。'
- en: 'These examples highlight the following:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子突显了以下几点：
- en: '**Investors believe** in the future of specialized small models.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**投资者相信**专用小型模型的未来。'
- en: '**Enterprise clients are willing to pay** for efficient solutions that don’t
    require sending data to external providers.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**企业客户愿意支付**高效的解决方案，而不需要将数据发送给外部供应商。'
- en: The market is shifting towards **“smart” specialized agents** instead of relying
    on “universal” large models.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 市场正在转向**“智能”专用代理**，而不再依赖于“通用”大型模型。
- en: 💡**Hint:** start by identifying repetitive tasks in your project. These are
    the best candidates for developing specialized SLM agents. This approach will
    help you avoid overpaying for the excessive power of LLMs and achieve greater
    control over the process.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 💡**提示：**从识别项目中的重复任务开始。这些任务最适合开发专用的SLM代理。通过这种方式，你将避免为LLM的过度能力支付过多费用，并能更好地掌控流程。
- en: Potential Limitations of SLMs Compared to LLMs
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SLM与LLM相比的潜在局限性
- en: Although I’ve spent this entire article praising small models, it’s fair to
    point out their limitations as well.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我在整篇文章中都在赞扬小型模型，但公平地说，也应指出它们的局限性。
- en: 1\. Limited Task Flexibility
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 任务灵活性的局限
- en: The most significant limitation of SLMs is their narrow specialization. Unlike
    LLMs, which can handle a wide range of tasks, SLMs succeed only in the specific
    tasks for which they have been trained. For example, in medicine, [Diabetica-7B
    outperformed LLMs](https://arxiv.org/pdf/2409.13191) in diabetes-related tests,
    but other medical disciplines required additional fine-tuning or a new architecture.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: SLM的最大限制是其狭窄的专用性。与可以处理广泛任务的LLM不同，SLM仅在它们经过训练的特定任务中表现出色。例如，在医学领域，[Diabetica-7B在糖尿病相关测试中优于LLM](https://arxiv.org/pdf/2409.13191)，但其他医学领域则需要额外的微调或全新的架构。
- en: '![](../Images/22382712a4e184c3025a715a8ed608f2.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22382712a4e184c3025a715a8ed608f2.png)'
- en: 'LLMs vs SLMs: Flexibility vs Specialization.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: LLM与SLM：灵活性与专用性。
- en: 2\. Context Window Limitations
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2. 上下文窗口的限制
- en: 'Unlike large models that reach up to 1M tokens ([Gemini 2.0](https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/)),
    SLMs have shorter contexts. Even though recent advances in small LLaMA 3.2 models
    (3B, 1B) having a context length of 128k tokens, [the effective context length
    is often not as claimed](https://arxiv.org/pdf/2410.18745): models often lose
    the “connection” between the beginning and the end of the text. For example, SLMs
    cannot efficiently process voluminous medical histories of patients over several
    years or large legal documents.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 与可以处理高达1M tokens的较大模型（如[Gemini 2.0](https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/)）不同，SLM的上下文长度较短。尽管近期小型LLaMA
    3.2模型（3B, 1B）已具有128k tokens的上下文长度，[但有效的上下文长度往往并非如宣传所称](https://arxiv.org/pdf/2410.18745)：模型通常无法保持文本开头与结尾之间的“联系”。例如，SLM无法高效处理多年的病人医疗历史或大量法律文件。
- en: '![](../Images/3bce47a45b59ed7cdc06638f6f63d2d2.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3bce47a45b59ed7cdc06638f6f63d2d2.png)'
- en: Comparison of maximum context length for different models.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 不同模型最大上下文长度的比较。
- en: 3\. Emergence Capabilities Gap
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. 新兴能力差距
- en: 'Many “emergent abilities” only [appear when a model reaches a certain size
    threshold](https://arxiv.org/pdf/2001.08361). SLMs **typically don’t hit the parameter
    levels required for advanced logical reasoning or deep contextual understanding**.
    [A study by Google Research](https://arxiv.org/pdf/2408.16737) demonstrates this
    with math word problems: while small models struggle with basic arithmetic, larger
    models suddenly demonstrate complex mathematical reasoning skills.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 许多“新兴能力”只有在模型达到某个规模门槛时才会[显现](https://arxiv.org/pdf/2001.08361)。SLM**通常无法达到进行高级逻辑推理或深度上下文理解所需的参数水平**。[谷歌研究的一项研究](https://arxiv.org/pdf/2408.16737)通过数学问题展示了这一点：小型模型在基础算术方面存在困难，而较大模型则能突然展示出复杂的数学推理能力。
- en: However, [recent research by Hugging Face shows](https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute)
    that **test-time compute scaling** can partially bridge this gap. Using strategies
    like **iterative self-refinement** or employing a **reward model**, small models
    can “think longer” on complex problems. For example, with extended generation
    time, small models (1B and 3B) outperformed their larger counterparts (8B and
    70B) on the MATH-500 benchmark.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，[Hugging Face 最近的研究表明](https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute)
    **测试时计算扩展**可以部分弥合这一差距。通过使用**迭代自我优化**或采用**奖励模型**等策略，小型模型可以在复杂问题上“思考更长时间”。例如，在延长生成时间的情况下，小型模型（1B和3B）在MATH-500基准测试中超过了它们的大型模型（8B和70B）。
- en: 💡**Hint:** If you work in an environment where tasks change weekly, require
    analyzing large documents, or involve solving complex logical problems, larger
    LLMs are often more reliable and versatile.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 💡**提示：** 如果您在一个任务每周都在变化、需要分析大量文档或涉及解决复杂逻辑问题的环境中工作，较大的LLM通常更可靠和多功能。
- en: Closing thoughts
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结束语
- en: As with choosing between O[penAI and self-hosted LLMs in my previous article](https://medium.com/better-programming/you-dont-need-hosted-llms-do-you-1160b2520526),
    there is no one-size-fits-all solution here. If your task involves constant changes,
    lacks precise specialization, or requires rapid prototyping, LLMs will offer an
    easy start.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 与我在上一篇文章中讨论的选择[OpenAI与自托管LLM之间的平衡](https://medium.com/better-programming/you-dont-need-hosted-llms-do-you-1160b2520526)一样，这里也没有“一刀切”的解决方案。如果您的任务涉及持续变化、缺乏精确的专业化，或需要快速原型开发，LLM将提供一个轻松的起点。
- en: However, over time, as your goal become more clearer, moving to compact, specialized
    **SLM agents can significantly reduce costs, improve accuracy, and simplify compliance
    with regulatory requirements**.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着时间的推移，当您的目标变得更加明确时，转向紧凑、专业化的**SLM 代理可以显著降低成本、提高准确性，并简化合规性要求**。
- en: '![](../Images/12891117c3be9a5f526a54a48e569a6d.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12891117c3be9a5f526a54a48e569a6d.png)'
- en: Moving from rapid prototyping at LLM to an optimized SLM agent ecosystem.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 从LLM的快速原型开发转向优化的SLM代理生态系统。
- en: SLMs aren’t a paradigm shift for the sake of trends but a pragmatic approach
    that allows you to solve specific problems more accurately and cost-effectively
    without overpaying for unnecessary functionality. You don’t need to completely
    abandon LLMs — **you can gradually replace only some components with SLMs** or
    even classic NLP methods. It all depends on your metrics, budget, and the nature
    of your task.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: SLM 并非为了跟随潮流而改变范式，而是一种务实的方法，使您能够更准确、经济地解决特定问题，而无需为不必要的功能支付过多费用。您不需要完全放弃LLM——**您可以逐步用SLM**或甚至经典的NLP方法替换其中的某些组件。所有这些取决于您的指标、预算和任务的性质。
- en: 'A good example of this is IBM, which employs a [multimodel strategy](https://www.ibm.com/products/watsonx-ai/foundation-models),
    combining smaller models for different tasks. As they point out:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的例子是 IBM，它采用了[多模型策略](https://www.ibm.com/products/watsonx-ai/foundation-models)，将较小的模型用于不同的任务。正如他们所指出的：
- en: Bigger is not always better, as specialized models outperform general-purpose
    models with lower infrastructure requirements.
  id: totrans-117
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 更大并不总是更好，因为专业化模型在基础设施要求较低的情况下，通常超过通用模型的表现。
- en: 'In the end, the **key to success is to adapt**. Start with a large model, evaluate
    where it performs best, and then optimize your architecture to avoid overpaying
    for unnecessary capabilities and compromising data privacy. This approach allows
    you to combine the best of both worlds: the flexibility and versatility of LLMs
    during the initial stages, and the precise, cost-effective performance of SLMs
    for a mature product.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，**成功的关键是适应**。从一个大型模型开始，评估它在何处表现最佳，然后优化您的架构，以避免为不必要的能力支付过高费用，并确保数据隐私不受影响。这种方法使您能够结合两者的优势：在初期阶段，LLM的灵活性和多功能性，以及在成熟产品阶段，SLM的精确和成本效益。
- en: If you have any questions or suggestions, feel free to connect on [LinkedIn](https://www.linkedin.com/in/sergey-savvov/).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有任何问题或建议，请随时在[LinkedIn](https://www.linkedin.com/in/sergey-savvov/)上与我联系。
- en: '***Disclaimer****: The information in the article is current as of December
    2024, but please be aware that changes may occur thereafter.*'
  id: totrans-120
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***免责声明***：本文中的信息截至2024年12月，但请注意此后可能会发生变化。'
- en: ''
  id: totrans-121
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Unless otherwise noted, all images are by the author.*'
  id: totrans-122
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*除非另有说明，所有图片均由作者提供。*'
