- en: Document Extraction Is GenAI’s Killer App
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文档提取是GenAI的杀手级应用
- en: 原文：[https://towardsdatascience.com/document-extraction-is-genais-killer-app-9e9c816e6caf?source=collection_archive---------0-----------------------#2024-08-13](https://towardsdatascience.com/document-extraction-is-genais-killer-app-9e9c816e6caf?source=collection_archive---------0-----------------------#2024-08-13)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/document-extraction-is-genais-killer-app-9e9c816e6caf?source=collection_archive---------0-----------------------#2024-08-13](https://towardsdatascience.com/document-extraction-is-genais-killer-app-9e9c816e6caf?source=collection_archive---------0-----------------------#2024-08-13)
- en: The future is here and you don’t get killer robots. You get great automation
    for tedious office work.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 未来已经到来，你不会看到致命的机器人。你会看到用于繁琐办公室工作的优秀自动化。
- en: '[](https://urimerhav.medium.com/?source=post_page---byline--9e9c816e6caf--------------------------------)[![Uri
    Merhav](../Images/d2fbdefa4b6e76357ffba91b0f5268a2.png)](https://urimerhav.medium.com/?source=post_page---byline--9e9c816e6caf--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9e9c816e6caf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9e9c816e6caf--------------------------------)
    [Uri Merhav](https://urimerhav.medium.com/?source=post_page---byline--9e9c816e6caf--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://urimerhav.medium.com/?source=post_page---byline--9e9c816e6caf--------------------------------)[![Uri
    Merhav](../Images/d2fbdefa4b6e76357ffba91b0f5268a2.png)](https://urimerhav.medium.com/?source=post_page---byline--9e9c816e6caf--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9e9c816e6caf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9e9c816e6caf--------------------------------)
    [Uri Merhav](https://urimerhav.medium.com/?source=post_page---byline--9e9c816e6caf--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9e9c816e6caf--------------------------------)
    ·9 min read·Aug 13, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9e9c816e6caf--------------------------------)
    ·阅读时间9分钟·2024年8月13日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Almost a decade ago, I worked as a Machine Learning Engineer at LinkedIn’s illustrious
    data standardization team. From the day I joined to the day I left, we still couldn’t
    automatically read a person’s profile and reliably understand someone’s seniority
    and job title across all languages and regions.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎十年前，我在LinkedIn的著名数据标准化团队担任机器学习工程师。从我加入的那一天到离开的那一天，我们仍然无法自动读取一个人的个人资料，并可靠地理解某人在所有语言和地区中的职级和职位。
- en: This looks simple at first glance. “software engineer” is clear enough, right?
    How about someone who just writes “associate”? It might be a low seniority retail
    worker, if they’re in Walmart, or a high ranking lawyer if they work in a law
    firm. But you probably knew that — do you know what’s a [Java Fresher](https://www.indeed.com/q-java-fresher-jobs.html)?
    What’s [*Freiwilliges Soziales Jahr*](https://en.wikipedia.org/wiki/Voluntary_social_year)?
    This isn’t just about knowing the German language — it translates to “Voluntary
    Social year”. But what’s a good standard title to represent this role? If you
    had a large list of known job titles, where would you map it?
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，这看起来很简单。“软件工程师”已经足够明确了，对吧？那如果有人只写“助理”呢？如果他在沃尔玛，那可能是一个低级零售工人；如果他在律师事务所，那可能是一个高级律师。但你大概已经知道了——你知道什么是[Java
    Fresher](https://www.indeed.com/q-java-fresher-jobs.html)吗？什么是[*Freiwilliges Soziales
    Jahr*](https://en.wikipedia.org/wiki/Voluntary_social_year)？这不仅仅是了解德语——它翻译为“志愿社会年”。但是，什么是代表这一角色的标准职称呢？如果你有一大堆已知的职位名称，你会把它映射到哪里？
- en: I joined LinkedIn, I left LinkedIn. We made progress, but making sense of even
    the most simple regular texts — a person’s résumé, was elusive.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我加入了LinkedIn，我离开了LinkedIn。我们取得了一些进展，但即使是最简单的常规文本——一个人的简历，依然难以理解。
- en: Very hard becomes trivial
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非常困难变得简单
- en: You probably won’t be shocked to learn that this problem is trivial for an LLM
    like GPT-4
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能不会感到惊讶地发现，这个问题对于像GPT-4这样的LLM来说是微不足道的
- en: '![](../Images/aecaaab3fe2a139fe32b077ff5839d5b.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aecaaab3fe2a139fe32b077ff5839d5b.png)'
- en: 'Easy peasy for GPT (source: me. and GPT)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对GPT来说轻松简单（来源：我和GPT）
- en: But wait, we’re a company, not a guy in a chat terminal, we need structured
    outputs.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 等等，我们是公司，不是一个聊天终端上的人，我们需要结构化的输出。
- en: '![](../Images/018900c647e69c4c7e8eaa916628d35c.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/018900c647e69c4c7e8eaa916628d35c.png)'
- en: '(source: GPT)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：GPT）
- en: Ah, that’s better. You can repeat this exercise with the most nuanced and culture-specific
    questions. Even better, you can repeat this exercise when you get an entire person’s
    profile, that gives you more context, and with code, which gives you the ability
    to use the results consistently in a business setting, and not only as a one off
    chat. With some more work you can coerce the results into a standard taxonomy
    of allowable job titles, which would make it indexable. It’s not an exaggeration
    to say if you copy & paste all of a person’s resume and prompt GPT just right,
    you will exceed the best results obtainable by some pretty smart people a decade
    ago, who worked at this for years.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 啊，这样好多了。你可以对那些最复杂且具有文化特定性的提问进行重复练习。更棒的是，当你得到一个完整的个人资料时，你可以重复这个练习，这样你就能获得更多的背景信息；使用代码时，你可以在商业环境中稳定地使用这些结果，而不仅仅是进行一次性的聊天。通过更多的工作，你可以将结果强制转换为一个标准的可接受职位标题分类法，这样它就能被索引。毫不夸张地说，如果你复制并粘贴某个人的全部简历并正确提示GPT，你将超越十年前一些相当聪明的人的最佳成果，这些人花了多年时间在这方面工作。
- en: High Value Office Work == Understanding Documents
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高价值办公室工作 == 理解文档
- en: 'The specific example of standardizing reumès is interesting, but it stays limited
    to where tech has always been hard at work — at a tech website that naturally
    applies AI tools. I think there’s a deeper opportunity here. A large percent of
    the world’s GDP is office work that boils down to expert human intelligence being
    applied to extract insights from a document repeatedly, with context. Here are
    some examples at increasing complexity:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化简历的具体例子很有趣，但它仍然局限于技术一直在努力的领域——一个自然应用人工智能工具的技术网站。我认为这里有更深层的机会。全球GDP的大部分来自办公室工作，这些工作归结为将专家级人类智能应用于从文档中反复提取洞见，且需要考虑背景信息。以下是一些复杂度逐渐增加的例子：
- en: Expense management is reading an invoice and converting it to a standardized
    view of what was paid, when, in what currency, and for which expense category.
    Potentially this decision is informed by background information about the business,
    the person making the expense, etc.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 费用管理就是读取发票并将其转化为标准化视图，显示支付了什么、何时支付、使用了哪种货币以及属于哪个费用类别。可能这个决策是基于关于业务、报销人等背景信息做出的。
- en: Healthcare claim adjudication is the process of reading a tangled mess of invoices
    and clinican notes and saying “ok so all told there was a single chest X-ray with
    a bunch of duplicates, it cost $800, and it maps to category 1-C in the health
    insurance policy”.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 医疗保险理赔裁定过程就是阅读一堆杂乱的发票和临床记录，并判断：“好吧，总共有一次胸部X光检查，包含了一些重复项目，总费用为800美元，且它对应健康保险政策中的1-C类别”。
- en: A loan underwriter might look at a bunch of bank statements from an applicants
    and answer a sequence of questions. Again, this is complex only because the inputs
    are all over the place. The actual decision making is something like “What’s the
    average inflow and outflow of cash, how much of it is going towards loan repayment,
    and which portion of it is one-off vs actual recurring revenue”.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 贷款核准员可能会查看一堆申请人的银行账单并回答一系列问题。同样，这之所以复杂，仅仅因为输入信息杂乱无章。实际的决策过程像是：“现金的平均流入和流出是多少，多少用于贷款偿还，其中有多少是一时性的支出，多少是实际的经常性收入”。
- en: Reasoning about text is LLM’s home turf
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于文本推理是LLM的强项。
- en: 'By now LLMs are notorious for being prone to hallucinations, a.k.a making shit
    up. The reality is more nuanced: hallucinations are in fact **a** **predictable
    result** in some settings, and are pretty much **guaranteed not to happen** in
    others.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，大型语言模型（LLMs）因易发生幻觉（即胡乱编造）而臭名昭著。现实情况更为复杂：幻觉实际上在某些情境下是**可预测的结果**，而在其他情况下则几乎**可以保证不会发生**。
- en: 'The place where hallucinations occur is when you ask it to answer factual questions
    and expect the model to just “know” the answer from its innate knowledge about
    the world. LLMs are bad and introspecting about what they know about the world
    — it’s more like a very happy accident that they can do this at all. They weren’t
    explicitly trained for that task. What they were trained for is to generate a
    predictable completion of text sequences. When an LLM is grounded against an input
    text and needs to answer questions about the content of that text, **it does not
    hallucinate.** If you copy & paste this blog post into chatGPT and ask does it
    teach you how to cook a an American Apple Pie, you will get the right result 100%
    of the time. For an LLM this is a very predictable task, where it sees a chunk
    of text, and tries to predict how a competent data analyst would fill a set of
    predefined fields with predefined outcomes, one of which is `{“is cooking discussed”:
    false}`.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '产生幻觉的地方是当你让它回答事实性问题，并期望模型仅凭其对世界的固有知识“知道”答案时。大语言模型（LLM）在自我反思自己对世界的知识方面表现得很差——它们能做到这一点更多是一个非常偶然的结果。它们并没有专门为此任务进行训练。它们的训练目标是生成可预测的文本序列补全。当LLM被绑定到输入文本并需要回答关于该文本内容的问题时，**它不会产生幻觉**。如果你将这篇博客文章复制并粘贴到chatGPT中，问它是否教你做美国苹果派，你会100%得到正确答案。对于LLM来说，这是一个非常可预测的任务，它看到一段文本，并尝试预测一个有能力的数据分析师如何用预定义的字段和预定义的结果来填写，这些结果之一是`{"is
    cooking discussed": false}`。'
- en: 'Previously as an AI consultant, we’ve repeatedly solved projects that involved
    extracting information from documents. Turns out there’s a lot of utility there
    in insurance, finance, etc. There was a large disconnect between what our clients
    feared (“LLMs hellucinate”) vs. what actually destroyed us (we didn’t extract
    the table correctly and all errors stem from there). LLMs did fail — when we failed
    them present it with the input text in a clean and unambiguous way. There are
    two necessary ingredients to build automatic pipelines that reason about documents:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 以前作为AI顾问，我们多次解决涉及从文档中提取信息的项目。事实证明，在保险、金融等领域这方面有很多用途。客户之间对LLM的恐惧（“LLM会产生幻觉”）与实际上摧毁我们的原因（我们没有正确提取表格，所有错误都源于此）之间存在很大的差距。LLM确实失败了——当我们没有以清晰且不含歧义的方式呈现输入文本时，它们会失败。构建能够推理文档的自动化管道需要两个必要的成分：
- en: '**Perfect Text extraction** that converts the input document into clean, understandable
    plain text. That means handling tables, checkmarks, hand-written comments, variable
    document layout etc. The entire complexity of a real world form needs to convert
    into a clean plaintext that makes sense in an LLM’s mind.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**完美的文本提取**，将输入文档转换为干净、易懂的纯文本。这意味着需要处理表格、勾选框、手写注释、可变文档布局等。现实世界表单的复杂性需要转化为LLM能够理解的清晰纯文本。'
- en: '**Robust Schemas** that define exactly what outputs you’re looking from a given
    document type, how to handle edge cases, what data format to use, etc.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**健壮的架构**，明确规定你希望从给定文档类型中获得哪些输出，如何处理边缘案例，使用什么数据格式等等。'
- en: Text extraction is trickier than first meets the eye
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本提取比看起来要复杂得多
- en: 'Here’s what causes LLMs to crash and burn, and get ridiculously bad outputs:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是导致LLM崩溃和产生荒谬输出的原因：
- en: The input has complex formatting like a double column layout, and you copy &
    pasted in text from e.g. a PDF from left to right, taking sentences completely
    out of context.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入中有复杂的格式，例如双列布局，你从例如PDF中从左到右复制并粘贴文本，完全把句子从上下文中剥离开来。
- en: The input has checkboxes, checkmarks, hand scribbled annotations, and you missed
    them altogether in conversion to text
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入中有复选框、勾选标记、手写注释，而你在转换为文本时完全忽略了这些
- en: 'Even worse: you thought you can get around converting to text, and hope to
    just paste a picture of a document and have GPT reason about it on its own. THIS
    gets your into hallucination city. Just ask GPT to transcribe an image of a table
    with some empty cells and you’ll se it happily going apeshit and making stuff
    up willy nilly.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更糟糕的是：你认为可以绕过转换为文本的步骤，指望只是粘贴一张文档的图片，让GPT自行推理。这会让你进入幻觉之城。只需让GPT转录一张带有空白单元格的表格图片，你就会看到它高兴地胡乱发挥，乱编一些东西。
- en: 'It always helps to remember what a crazy mess goes on in real world documents.
    Here’s a casual tax form:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 时常记住现实世界文档的混乱程度是很有帮助的。这里有一份随意的税表：
- en: '![](../Images/427b0482e504398b3c2761391d9b1482.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/427b0482e504398b3c2761391d9b1482.png)'
- en: Of course real tax forms have all these fields filled out, often in handwriting
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，真实的税表上这些字段通常是填写完整的，且常常是手写的
- en: Or here’s my resumè
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 或者这是我的简历
- en: '![](../Images/66cd7e1b0b1bf5c98cad07d1c48456ea.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66cd7e1b0b1bf5c98cad07d1c48456ea.png)'
- en: 'Source: my resume'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：我的简历
- en: Or a publicly available example lab report (this is a front page result from
    Google)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 或者一个公开的示例实验报告（这是Google的首页结果）
- en: '![](../Images/127803e926e7457e11d51d5c864d5b33.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/127803e926e7457e11d51d5c864d5b33.png)'
- en: 'Source: [research gate](https://www.researchgate.net/figure/The-table-below-is-a-summary-of-laboratory-results-for-blood-Full-blood-count-FBC_tbl1_370627188),
    public domain image'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[research gate](https://www.researchgate.net/figure/The-table-below-is-a-summary-of-laboratory-results-for-blood-Full-blood-count-FBC_tbl1_370627188)，公共领域图像
- en: The absolute worst thing you can do, by the way, is ask GPT’s multimodal capabilities
    to transcribe a table. Try it if you dare — it looks right at first glance, and
    absolutely makes random stuff up for some table cells, takes things completely
    out of context, etc.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，最糟糕的事情就是让GPT的多模态能力来转录一个表格。如果你敢试试——一开始看起来没问题，但它会为某些表格单元格随意编造内容，完全脱离上下文等。
- en: If something’s wrong with the world, build a SaaS company to fix it
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如果这个世界有问题，那就建立一个SaaS公司来解决它。
- en: When tasked with understanding these kinds of documents, my cofounder [Nitai
    Dean](https://medium.com/u/47700d6c6e7b?source=post_page---user_mention--9e9c816e6caf--------------------------------)
    and I were befuddled that there aren’t any off-the-shelf solutions for making
    sense of these texts.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们需要理解这些类型的文档时，我和我的联合创始人[Nitai Dean](https://medium.com/u/47700d6c6e7b?source=post_page---user_mention--9e9c816e6caf--------------------------------)感到困惑，因为没有现成的解决方案可以帮助我们理解这些文本。
- en: Some people claim to solve it, like AWS Textract. But they make numerous mistakes
    on any complex document we’ve tested on. Then you have the long tail of small
    things that are necessary, like recognizing checkmarks, radio button, crossed
    out text, handwriting scribbles on a form, etc etc.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人声称能够解决这个问题，比如AWS Textract。但在我们测试的任何复杂文档中，它们都会犯很多错误。接着就是那些必须处理的小细节，比如识别勾选框、单选按钮、删除线文本、表单上的手写涂鸦等。
- en: So, we built [Docupanda.io](https://www.docupanda.io/) — which first generates
    a clean text representation of any page you throw at it. On the left hand you’ll
    see the original document, and on the right you can see the text output
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们建立了[Docupanda.io](https://www.docupanda.io/)——它首先生成任何你输入的页面的清晰文本表示。左边是原始文档，右边是文本输出。
- en: '![](../Images/803c1c01bb2650c7931f990593190f4b.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/803c1c01bb2650c7931f990593190f4b.png)'
- en: 'Source: docupanda.io'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：docupanda.io
- en: 'Tables are similarly handled. Under the hood we just convert the tables into
    huuman and LLM-readable markdown format:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 表格也类似地处理。在背后，我们只是将表格转换为人类和LLM可读的Markdown格式：
- en: '![](../Images/6dfd9a9d4449d5fb10c093660ca8736e.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6dfd9a9d4449d5fb10c093660ca8736e.png)'
- en: 'Source: docupanda.io'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：docupanda.io
- en: The last piece to making sense of data with LLMs is generating and adhering
    to rigid output formats. It’s great that we can make AI mold its output into a
    json, but in order to apply rules, reasoning, queries etc on data — we need to
    make it behave in a regular way. The data needs to conform to a predefined set
    of slots which we’ll fill up with content. In the data world we call that a **Schema**.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LLM来理解数据的最后一块拼图是生成并遵循严格的输出格式。虽然我们可以让AI将输出格式化为json，但为了对数据应用规则、推理、查询等——我们需要让它以规律的方式运作。数据需要符合预定义的插槽集，我们将用内容填充这些插槽。在数据领域，我们称之为**模式**。
- en: Building Schemas is a trial an error process… That an LLM can do
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建模式是一个试错过程……这是一个LLM可以做的事情。
- en: The reason we need a schema, is that data is useless without regularity. If
    we’re processing patient records, and they map to “male” “Male” “m” and “M” —
    we’re doing a terrible job.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要模式的原因是，数据没有规律性是毫无意义的。如果我们在处理患者记录，并且它们映射到“male”、“Male”、“m”和“M”——那我们做得很糟糕。
- en: So how do you build a schema? In a textbook, you might build a schema by sitting
    long and hard and staring at the wall, and defining that what you want to extract.
    You sit there, mull over your healthcare data operation and go “I want to extract
    patient name, date, gendfer and their physician’s name. Oh and gender must be
    M/F/Other.”
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何构建一个模式呢？在教科书中，你可能通过长时间地坐在那里，盯着墙壁，来定义你想要提取的内容。你坐在那里，思考你的医疗数据操作，并说“我想提取患者姓名、日期、性别和他们的医生姓名。哦，性别必须是
    M/F/Other。”
- en: In real life, in order to define what to extract from documents, you freaking
    stare at your documents… a lot. You start off with something like the above, but
    then you look at documents and see that one of them has a LIST of physicians instead
    of one. And some of them also list an address for the physicians. And some addresses
    have a unit number and a building number, so maybe you need a slot for that. On
    and on it goes.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实中，为了定义从文档中提取什么内容，你得盯着文档看……很多。你一开始可能像上面那样，但是当你看文档时，你会发现其中一个文档列出了多个医生的名单，而不是一个。还有些文档也列出了医生的地址。一些地址包含单元号和楼栋号，所以你可能需要为此预留一个字段。事情就是这么一件接一件地发生。
- en: What we came to realize is that being able to define exactly what’s all the
    things you want to extract, is both non-trivial, difficult, and very solvable
    with AI.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们意识到，能够准确定义你想要提取的所有内容，是既不简单、又困难，但使用 AI 是完全可以解决的。
- en: 'That’s a key piece of DocuPanda. Rather than just asking an LLM to improvise
    an output for every document, we’ve built the mechanism that lets you:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 DocuPanda 的关键部分。我们不是仅仅要求一个 LLM 为每个文档即兴生成输出，而是建立了一个机制，让你可以：
- en: Specify what things you need to get from a document in free language
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用自由语言指定你需要从文档中提取的内容。
- en: Have our AI map over *many* documents and figure out a schema that answers all
    the questions and accommodates the kinks and irregularities observed in actual
    documents.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们的 AI 映射 *多个* 文档，并找出一个能回答所有问题、并适应实际文档中观察到的漏洞和不规则之处的模式。
- en: Change the schema with feedback to adjust it to your business needs
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据反馈调整模式，以适应你的业务需求。
- en: What you end up with is a powerful JSON schema — a template that says exactly
    what you want to extract from every document, and maps over hundreds of thousands
    of them, extracting answers to all of them, while obeying rules like always extracting
    dates in the same format, respecting a set of predefined categories, etc.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最终你得到的是一个强大的 JSON 模式 —— 一个模板，明确指出你想从每个文档中提取的内容，并且能够处理数十万份文档，从中提取所有答案，同时遵循像始终以相同格式提取日期、尊重一组预定义类别等规则。
- en: '![](../Images/d650427366300fef5c2dad18c7eaae96.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d650427366300fef5c2dad18c7eaae96.png)'
- en: 'Source: docupanda.io'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：docupanda.io
- en: Plenty More!
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多！
- en: 'Like with any rabbit hole, there’s always more stuff than first meets the eye.
    As time went by, we’ve discovered that more things are needed:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 就像任何兔子洞一样，总是有比最初看到的更多的东西。随着时间的推移，我们发现需要更多的东西：
- en: Often organizations have to deal with an incoming stream of anonymous documents,
    so we automatically classify them and decide what schema to apply to them
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组织经常需要处理大量匿名文档，因此我们会自动对它们进行分类，并决定应用哪种模式。
- en: Documents are sometimes a concatenation of many documents, and you need an intelligent
    solution to break apart a very long documents into its atomic, seperate components
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档有时是多个文档的拼接，你需要一个智能的解决方案将一篇非常长的文档拆分成其原子化的独立组件。
- en: Querying for the right documents using the generated results is super useful
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用生成的结果查询正确的文档非常有用。
- en: If there’s one takeaway from this post, it’s that you should look into harnessing
    LLMs to make sense of documents in a regular way. If there’s two takeawways, it’s
    that you should also try out [Docupanda.io](https://www.docupanda.io/). The reason
    I’m building it is that I believe in it. Maybe that’s a good enough reason to
    give it a go?
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果从这篇文章中能得到一个启示，那就是你应该研究如何利用 LLM 来以规范的方式理解文档。如果有两个启示，那就是你也应该试试 [Docupanda.io](https://www.docupanda.io/)。我之所以构建它，是因为我相信它。也许这就足够成为尝试它的理由？
- en: '![](../Images/efa501bf785c4e8f347589ffa7ba0728.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/efa501bf785c4e8f347589ffa7ba0728.png)'
- en: 'A future office worker (Source: unsplash.com)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的办公室工作人员（来源：unsplash.com）
