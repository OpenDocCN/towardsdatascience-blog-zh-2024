- en: Deploying your Llama Model via vLLM using SageMaker Endpoint
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过vLLM使用SageMaker端点部署Llama模型
- en: 原文：[https://towardsdatascience.com/deploying-your-llama-model-via-vllm-using-sagemaker-endpoint-f02b424da124?source=collection_archive---------8-----------------------#2024-09-12](https://towardsdatascience.com/deploying-your-llama-model-via-vllm-using-sagemaker-endpoint-f02b424da124?source=collection_archive---------8-----------------------#2024-09-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/deploying-your-llama-model-via-vllm-using-sagemaker-endpoint-f02b424da124?source=collection_archive---------8-----------------------#2024-09-12](https://towardsdatascience.com/deploying-your-llama-model-via-vllm-using-sagemaker-endpoint-f02b424da124?source=collection_archive---------8-----------------------#2024-09-12)
- en: Leveraging AWS’s MLOps platform to serve your LLM models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用AWS的MLOps平台为LLM模型提供服务
- en: '[](https://medium.com/@teosiyang?source=post_page---byline--f02b424da124--------------------------------)[![Jake
    Teo](../Images/9687f43822fab69befb750a8ec58516d.png)](https://medium.com/@teosiyang?source=post_page---byline--f02b424da124--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f02b424da124--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f02b424da124--------------------------------)
    [Jake Teo](https://medium.com/@teosiyang?source=post_page---byline--f02b424da124--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@teosiyang?source=post_page---byline--f02b424da124--------------------------------)[![Jake
    Teo](../Images/9687f43822fab69befb750a8ec58516d.png)](https://medium.com/@teosiyang?source=post_page---byline--f02b424da124--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f02b424da124--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f02b424da124--------------------------------)
    [Jake Teo](https://medium.com/@teosiyang?source=post_page---byline--f02b424da124--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f02b424da124--------------------------------)
    ·8 min read·Sep 12, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f02b424da124--------------------------------)
    ·8分钟阅读·2024年9月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a52a22e39707bd0217ce2e9ce9454883.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a52a22e39707bd0217ce2e9ce9454883.png)'
- en: Instances in an MLOps workflow that require an inference endpoint (created by
    author).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 需要推理端点的MLOps工作流中的实例（由作者创建）。
- en: In any machine learning project, the goal is to train a model that can be used
    by others to derive a good prediction. To do that, the model needs to be served
    for inference. Several parts in this workflow require this inference endpoint,
    namely, for model evaluation, before releasing it to the development, staging,
    and finally production environment for the end-users to consume.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何机器学习项目中，目标是训练一个可以供他人使用，以得出良好预测的模型。为了实现这一目标，需要提供模型进行推理。在这个工作流中，几个部分需要这个推理端点，即在模型评估时，模型在发布到开发、预发布环境，最后到生产环境供最终用户使用之前。
- en: In this article, I will demonstrate how to deploy the latest LLM and serving
    technologies, namely Llama and vLLM, using AWS’s SageMaker endpoint and its DJL
    image. What are these components and how do they make up an inference endpoint?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将演示如何使用AWS的SageMaker端点和其DJL镜像，部署最新的LLM和服务技术，即Llama和vLLM。这些组件是什么，它们如何组成推理端点？
- en: '![](../Images/c015c60e62b49d2c87dc9949dcd2dd07.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c015c60e62b49d2c87dc9949dcd2dd07.png)'
- en: How each of these components together serves the model in AWS. SageMaker endpoint
    is the GPU instance, DJL is the template Docker image, and vLLM is the model server
    (created by author).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 各个组件如何协同工作，服务于AWS中的模型。SageMaker端点是GPU实例，DJL是模板Docker镜像，vLLM是模型服务器（由作者创建）。
- en: '**SageMaker** is an AWS service that consists of a large suite of tools and
    services to manage a machine learning lifecycle. Its inference service is known
    as SageMaker endpoint. Under the hood, it is essentially a virtual machine self-managed
    by AWS.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**SageMaker**是AWS提供的一项服务，包含一整套工具和服务，用于管理机器学习生命周期。其推理服务被称为SageMaker端点。在底层，它本质上是由AWS自主管理的虚拟机。'
- en: '**DJL** (Deep Java Library) is an open-source library developed by AWS used
    to develop LLM inference docker images, including vLLM [2]. This image is used
    in…'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**DJL**（Deep Java Library）是由AWS开发的开源库，用于开发LLM推理Docker镜像，包括vLLM[2]。该镜像用于…'
