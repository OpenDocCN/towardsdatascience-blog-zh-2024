- en: 'PySpark Explained: Four Ways to Create and Populate DataFrames'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark 解析：创建和填充 DataFrame 的四种方式
- en: 原文：[https://towardsdatascience.com/pyspark-explained-four-ways-to-create-and-populate-dataframes-31f3e4322ad9?source=collection_archive---------3-----------------------#2024-07-04](https://towardsdatascience.com/pyspark-explained-four-ways-to-create-and-populate-dataframes-31f3e4322ad9?source=collection_archive---------3-----------------------#2024-07-04)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/pyspark-explained-four-ways-to-create-and-populate-dataframes-31f3e4322ad9?source=collection_archive---------3-----------------------#2024-07-04](https://towardsdatascience.com/pyspark-explained-four-ways-to-create-and-populate-dataframes-31f3e4322ad9?source=collection_archive---------3-----------------------#2024-07-04)
- en: '![](../Images/49e4e3686f9183a0297651c636080f94.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/49e4e3686f9183a0297651c636080f94.png)'
- en: Image by AI (Dalle-3)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由 AI（Dalle-3）生成
- en: 'From CSVs to databases: loading data into PySpark DataFrames'
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从 CSV 到数据库：将数据加载到 PySpark DataFrame 中
- en: '[](https://medium.com/@thomas_reid?source=post_page---byline--31f3e4322ad9--------------------------------)[![Thomas
    Reid](../Images/c1b4e5f577272633ba07e5dbfd21c02d.png)](https://medium.com/@thomas_reid?source=post_page---byline--31f3e4322ad9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--31f3e4322ad9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--31f3e4322ad9--------------------------------)
    [Thomas Reid](https://medium.com/@thomas_reid?source=post_page---byline--31f3e4322ad9--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@thomas_reid?source=post_page---byline--31f3e4322ad9--------------------------------)[![Thomas
    Reid](../Images/c1b4e5f577272633ba07e5dbfd21c02d.png)](https://medium.com/@thomas_reid?source=post_page---byline--31f3e4322ad9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--31f3e4322ad9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--31f3e4322ad9--------------------------------)
    [Thomas Reid](https://medium.com/@thomas_reid?source=post_page---byline--31f3e4322ad9--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--31f3e4322ad9--------------------------------)
    ·11 min read·Jul 4, 2024
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--31f3e4322ad9--------------------------------)
    ·11分钟阅读·2024年7月4日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: When using PySpark, especially if you have a background in SQL, one of the first
    things you’ll want to do is get the data you want to process into a DataFrame.
    Once the data is in a DataFrame, it’s easy to create a temporary view (or permanent
    table) from the DataFrame. At that stage, all of PySpark SQL's rich set of operations
    becomes available for you to use to further explore and process the data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 PySpark 时，特别是如果你有 SQL 背景，首先你可能想做的事情就是将你想处理的数据放入 DataFrame。一旦数据在 DataFrame
    中，就可以很容易地从 DataFrame 创建一个临时视图（或永久表）。在这个阶段，PySpark SQL 提供的丰富操作集就可以供你使用，帮助你进一步探索和处理数据。
- en: Since many standard SQL skills are easily transferable to PySpark SQL, it’s
    crucial to prepare your data for direct use with PySpark SQL as early as possible
    in your processing pipeline. Doing this should be a top priority for efficient
    data handling and analysis.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 由于许多标准的 SQL 技能可以轻松转移到 PySpark SQL，因此在处理管道中尽早准备数据以直接使用 PySpark SQL 至关重要。这样做应该是高效数据处理和分析的首要任务。
- en: You don’t ***have*** to do this of course, as anything you can do with PySpark
    SQL on views or tables can be done directly on DataFrames too using the API. But
    as someone who is far more comfortable using SQL than the DataFrame API, my goto
    process when using Spark has always been,
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你不***一定***非得这么做，因为你可以使用 PySpark SQL 对视图或表执行的所有操作也可以直接在 DataFrame 上通过 API
    完成。但作为一个对使用 SQL 比对使用 DataFrame API 更加熟悉的人，我在使用 Spark 时的首选流程一直是，
- en: '**input data -> DataFrame-> temporary view-> SQL processing**'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**输入数据 -> DataFrame -> 临时视图 -> SQL 处理**'
- en: To help you with this process, this article will discuss the first part of this
    pipeline, i.e. getting your data into DataFrames, by showcasing four of…
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助你完成这个过程，本文将讨论这个管道的第一部分，即将数据放入 DataFrame，并展示四种方法……
