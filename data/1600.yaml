- en: From Vision Transformers to Masked Autoencoders in 5 Minutes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从视觉变换器到掩码自编码器，5分钟搞定
- en: 原文：[https://towardsdatascience.com/from-vision-transformers-to-masked-autoencoders-in-5-minutes-cfd2fa1664ac?source=collection_archive---------3-----------------------#2024-06-28](https://towardsdatascience.com/from-vision-transformers-to-masked-autoencoders-in-5-minutes-cfd2fa1664ac?source=collection_archive---------3-----------------------#2024-06-28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/from-vision-transformers-to-masked-autoencoders-in-5-minutes-cfd2fa1664ac?source=collection_archive---------3-----------------------#2024-06-28](https://towardsdatascience.com/from-vision-transformers-to-masked-autoencoders-in-5-minutes-cfd2fa1664ac?source=collection_archive---------3-----------------------#2024-06-28)
- en: A Straightforward Guide on How NLP Tasks Generalize to Computer Vision
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然语言处理任务如何泛化到计算机视觉的简单指南
- en: '[](https://essamwissam.medium.com/?source=post_page---byline--cfd2fa1664ac--------------------------------)[![Essam
    Wisam](../Images/6320ce88ba2e5d56d70ce3e0f97ceb1d.png)](https://essamwissam.medium.com/?source=post_page---byline--cfd2fa1664ac--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--cfd2fa1664ac--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--cfd2fa1664ac--------------------------------)
    [Essam Wisam](https://essamwissam.medium.com/?source=post_page---byline--cfd2fa1664ac--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://essamwissam.medium.com/?source=post_page---byline--cfd2fa1664ac--------------------------------)[![Essam
    Wisam](../Images/6320ce88ba2e5d56d70ce3e0f97ceb1d.png)](https://essamwissam.medium.com/?source=post_page---byline--cfd2fa1664ac--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--cfd2fa1664ac--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--cfd2fa1664ac--------------------------------)
    [Essam Wisam](https://essamwissam.medium.com/?source=post_page---byline--cfd2fa1664ac--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--cfd2fa1664ac--------------------------------)
    ·7 min read·Jun 28, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--cfd2fa1664ac--------------------------------)
    ·阅读时长7分钟·2024年6月28日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Nearly all natural language processing tasks, from language modeling and masked
    word prediction to translation and question-answering, were revolutionized with
    the debut of the transformer architecture in 2017\. It should come as no surprise,
    that within just 2–3 years, transformers were also employed in computer vision
    tasks where they also showed outstanding results. In this story, we explore two
    fundamental architectures that enabled transformers to break into the world of
    computer vision.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有的自然语言处理任务，从语言建模、掩码词预测到翻译和问答，都在2017年变换器架构首次亮相时经历了革命性变化。毫不奇怪，在短短2至3年内，变换器也被应用到计算机视觉任务中，并且在这些任务中展现了卓越的表现。在这篇文章中，我们探讨了两种基础架构，使得变换器能够突破进入计算机视觉领域。
- en: Table of Contents
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目录
- en: · [The Vision Transformer](#c206)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: · [视觉变换器](#c206)
- en: ∘ [Key Idea](#c302)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [关键思想](#c302)
- en: ∘ [Operation](#98e9)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [操作](#98e9)
- en: ∘ [Hybrid Architecture](#ae5c)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [混合架构](#ae5c)
- en: ∘ [Loss of Structure](#af38)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [结构的丧失](#af38)
- en: ∘ [Results](#6ed9)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [结果](#6ed9)
- en: ∘ [Self-supervised Learning by Masking](#7607)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [自监督学习通过掩码](#7607)
- en: · [Masked Autoencoder Vision Transformer](#e126)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: · [掩码自编码器视觉变换器](#e126)
- en: ∘ [Key Idea](#2db8)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [关键思想](#2db8)
- en: ∘ [Architecture](#3393)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [架构](#3393)
- en: ∘ [Final Remark and Example](#f78c)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [最终备注与示例](#f78c)
- en: The Vision Transformer
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视觉变换器
- en: '![](../Images/f02bd048ad2f7f140d55716a7d214624.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f02bd048ad2f7f140d55716a7d214624.png)'
- en: 'Image from Paper: “An Image is Worth 16x16 Words: Transformers for Image Recognition
    at Scale”'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 论文中的图片：“一张图片值16x16个词：用于大规模图像识别的变换器”
- en: Key Idea
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键思想
- en: 'The vision transformer is simply meant to generalize the [standard transformer](https://medium.com/@essamwissam/a-systematic-explanation-of-transformers-db82e039b913)
    architecture to process and learn from image input. There is a key idea about
    the architecture that the authors were transparent enough to highlight:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉变换器的目的是将[标准变换器](https://medium.com/@essamwissam/a-systematic-explanation-of-transformers-db82e039b913)架构推广到处理和学习图像输入。关于该架构有一个关键思想，作者在文中非常透明地进行了强调：
- en: “Inspired by the Transformer scaling successes in NLP, we experiment with applying
    a standard Transformer directly to images, with the fewest possible modifications.”
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “受变换器在自然语言处理领域扩展成功的启发，我们尝试将标准变换器直接应用于图像，尽可能少地做修改。”
- en: Operation
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作
- en: 'It’s valid to take “*fewest possible modifications”* quite literally because
    they pretty much make zero modifications. What they actuall modify is input structure:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 将“*尽可能少的修改*”字面理解是合理的，因为他们几乎没有做任何修改。实际上他们修改的是输入结构：
- en: In NLP, the transformer encoder takes a ***sequence of one-hot vectors*** (or
    equivalently token indices) that ***represent the input sentence/paragraph***
    and returns a sequence of contextual embedding vectors that could be used for
    a further tasks (e.g., classification)
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在NLP中，transformer编码器接收一个***one-hot向量序列***（或者等价的令牌索引），这些向量***代表输入句子/段落***，并返回一个上下文嵌入向量序列，后者可以用于进一步的任务（例如分类）。
- en: To generalize the CV, the vision transformer takes a ***sequence of patch vectors***
    that represent the ***input image*** and returns a sequence of contextual embedding
    vectors that could be used for a further tasks (e.g., classification)
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了推广CV，视觉transformer接收一个***patch向量序列***，这些向量代表了***输入图像***，并返回一个上下文嵌入向量序列，后者可以用于进一步的任务（例如分类）。
- en: 'In particular, suppose the input images have dimensions (n,n,3) to pass this
    as an input to the transformer, what the vision transformer does is:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，假设输入图像的维度是(n,n,3)，并将其作为输入传递给transformer，视觉transformer所做的是：
- en: Divides it into k² patches for some k (e.g., k=3) as in the figure above.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将图像分成k²个patch，其中k是某个值（例如，k=3），如上图所示。
- en: Now each patch will be (n/k,n/k,3) the next step is to flatten each patch into
    a vector
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在每个patch的维度将是(n/k,n/k,3)，下一步是将每个patch展平为一个向量。
- en: 'The patch vector will be of dimensionality 3*(n/k)*(n/k). For example, if the
    image is (900,900,3) and we use k=3 then a patch vector will have dimensionality
    300*300*3 representing the pixel values in the flattened patch. In the paper,
    authors use k=16\. Hence, the paper’s name “An Image is Worth 16x16 Words: Transformers
    for Image Recognition at Scale” instead of feeding a one-hot vector representing
    the word they represent a vector pixels representing a patch of the image.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 'patch向量的维度将是3*(n/k)*(n/k)。例如，如果图像的维度是(900,900,3)，并且我们使用k=3，那么一个patch向量的维度将是300*300*3，表示展平后的patch中的像素值。在论文中，作者使用了k=16。因此，论文的标题是“An
    Image is Worth 16x16 Words: Transformers for Image Recognition at Scale”，而不是输入一个表示单词的one-hot向量，它们代表的是表示图像patch的像素向量。'
- en: '**The rest of the operations remains as in the original transformer encoder:**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**其余操作与原始transformer编码器相同：**'
- en: These patch vectors pass by a trainable embedding layer
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些patch向量会经过一个可训练的嵌入层。
- en: Positional embeddings are added to each vector to maintain a sense of spatial
    information in the image
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个向量中添加位置嵌入，以保持图像中的空间信息。
- en: The output is *num_patches* encoder representations (one for each patch) which
    could be used for classification on the patch or image level
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出是*num_patches*个编码表示（每个patch一个），这些表示可以用于对patch或图像层面的分类。
- en: More often (and as in the paper), a CLS token is prepended the representation
    corresponding to that is used to make a prediction over the whole image (similar
    to BERT)
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更常见的是（如论文中所示），一个CLS标记被加到表示序列的最前面，这个标记用于对整个图像进行预测（类似于BERT）。
- en: '**How about the transformer decoder?**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**那transformer解码器怎么样？**'
- en: Well, remember it’s just like the transformer encoder; the difference is that
    it uses masked self-attention instead of self-attention (but the same input signature
    remains). In any case, you should expect to seldom use a decoder-only transformer
    architecture because simply predicting the next patch may not a task of great
    interest.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，这就像transformer编码器一样；不同之处在于，它使用了掩码自注意力而不是自注意力（但输入签名保持不变）。无论如何，你应该预期很少使用仅解码器的transformer架构，因为简单地预测下一个patch可能不是一个很感兴趣的任务。
- en: Hybrid Architecture
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混合架构
- en: Authors also mentions that it’s possible to start with a CNN feature map instead
    of the image itself to form a hybrid architecture (CNN feeding output to vision
    transformer). In this case, we think of the input as a generic (n,n,p) feature
    map and a patch vector will have dimensions (n/k)*(n/k)*p.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 作者还提到，完全可以从CNN特征图开始，而不是直接从图像开始，形成一种混合架构（CNN将输出馈送给视觉transformer）。在这种情况下，我们可以把输入看作是一个通用的(n,n,p)特征图，而一个patch向量的维度将是(n/k)*(n/k)*p。
- en: Loss of Structure
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结构丢失
- en: It may cross your mind that this architecture shouldn’t be so good because it
    treated the image as a linear structure when it isn’t. The author try to depict
    that this is intentional by mentioning
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，这种架构不应该那么好，因为它把图像当作线性结构来处理，但实际上并非如此。作者通过提到这一点，试图表明这是故意为之。
- en: “The two-dimensional neighborhood structure is used very sparingly…position
    embeddings at initialization time carry no information about the 2D positions
    of the patches and all spatial relations between the patches have to be learned
    from scratch”
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “二维邻域结构的使用非常有限……在初始化时，位置嵌入不包含任何关于补丁的二维位置信息，所有补丁之间的空间关系必须从头开始学习。”
- en: We will see that the transformer is able to learn this as evidenced by its good
    performance in their experiments and more importantly the architecture in the
    next paper.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到，变换器（transformer）能够学习这一点，这一点在他们的实验中得到了验证，更重要的是，下一篇论文中的架构也证明了这一点。
- en: Results
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: 'The main verdict from the results is that vision transformers tend to not outperform
    CNN-based models for small datasets but approach or outperofrm CNN-based models
    for larger datasets and either way require significantly less compute:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的主要结论是，对于小数据集，视觉变换器通常不如基于CNN的模型，但对于较大数据集，它们能够接近或超越CNN模型，并且无论如何都需要显著更少的计算资源：
- en: '![](../Images/7e699867cbb8ab95893959a4509a990f.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e699867cbb8ab95893959a4509a990f.png)'
- en: 'Table from Paper: “An Image is Worth 16x16 Words: Transformers for Image Recognition
    at Scale”.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 来自论文的表格：“一张图片价值16x16个词：用于大规模图像识别的变换器”。
- en: Here we see that for the JFT-300M dataset (which has 300M images), the ViT models
    pre-trained on the dataset outperform ResNet-based baselines while taking substantially
    less computational resources to pre-train. As can be seen the larget vision transformer
    they used (ViT-Huge with 632M parameters and k=16) used about 25% of the compute
    used for the ResNet based model and still outperformed it. The performance doesn’t
    even downgrade that much with ViT-Large using only <6.8% of the compute.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到对于JFT-300M数据集（包含3亿张图片），在该数据集上预训练的ViT模型表现超过了基于ResNet的基准模型，同时预训练所需的计算资源明显更少。如图所示，他们使用的最大视觉变换器（ViT-Huge，拥有6.32亿参数，k=16）所需的计算量仅为ResNet基准模型的25%，且仍然优于其性能。即使是ViT-Large，仅使用不到6.8%的计算资源，其性能也几乎没有下降。
- en: Meanwhile, others also expose results where the ResNet performed significantly
    better when trained on ImageNet-1K which has just 1.3M images.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，其他研究也展示了ResNet在训练时在ImageNet-1K（仅包含130万张图片）上的表现显著更好。
- en: Self-supervised Learning by Masking
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过遮罩进行的自监督学习
- en: Authors performed a preliminary exploration on masked patch prediction for self-supervision,
    mimicking the masked language modeling task used in BERT (i.e., masking out patches
    and attempting to predict them).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们对遮罩补丁预测进行了初步探索，模仿了BERT中使用的遮罩语言模型任务（即，遮罩出一些补丁并尝试预测它们）。
- en: “We employ the masked patch prediction objective for preliminary self-supervision
    experiments. To do so we corrupt 50% of patch embeddings by either replacing their
    embeddings with a learnable [mask] embedding (80%), a random other patch embedding
    (10%) or just keeping them as is (10%).”
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我们使用遮罩补丁预测目标进行初步的自监督实验。为此，我们将50%的补丁嵌入破坏，通过以下方式之一：将其嵌入替换为可学习的[遮罩]嵌入（80%）、随机替换为其他补丁嵌入（10%），或者保持原样（10%）。”
- en: With self-supervised pre-training, their smaller ViT-Base/16 model achieves
    79.9% accuracy on ImageNet, a significant improvement of 2% to training from scratch.
    But still 4% behind supervised pre-training.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 通过自监督预训练，他们的小型ViT-Base/16模型在ImageNet上的准确率达到了79.9%，相比从零开始训练提升了2%，但仍然比监督预训练低4%。
- en: Masked Autoencoder Vision Transformer
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 遮罩自编码器视觉变换器
- en: '![](../Images/d7cf73cf4ebc0d818ec829d32a024a11.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d7cf73cf4ebc0d818ec829d32a024a11.png)'
- en: 'Image from Paper: Masked Autoencoders Are Scalable Vision Learners'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 来自论文的图片：《Masked Autoencoders Are Scalable Vision Learners》
- en: Key Idea
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键思想
- en: As we have seen from the vision transformer paper, the gains from pretraining
    by masking patches in input images were not as significant as in ordinary NLP
    where masked pretraining can lead to state-of-the-art results in some fine-tuning
    tasks.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们从视觉变换器的论文中看到的，通过遮罩输入图像的补丁进行预训练的收益并不像在普通NLP任务中那样显著，在NLP中，遮罩预训练可以在某些微调任务中带来最先进的结果。
- en: This paper proposes a vision transformer architecture involving an encoder and
    a decoder that when pretrained with masking results in significant improvements
    over the base vision transformer model (as much as 6% improvement compared to
    training a base size vision transformer in a supervised fashion).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了一种视觉变换器架构，涉及一个编码器和解码器，当通过遮罩进行预训练时，能显著提高基准视觉变换器模型的表现（与使用监督训练的基准模型相比，提升高达6%）。
- en: '![](../Images/027d64201111d63dce657b96d4ad9c3c.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/027d64201111d63dce657b96d4ad9c3c.png)'
- en: 'Image from Paper: Masked Autoencoders Are Scalable Vision Learners'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 来自论文的图像：*Masked Autoencoders Are Scalable Vision Learners*
- en: This is some sample (input, output, true labels). It’s an autoencoder in the
    sense that it tried to reconstruct the input while filling the missing patches.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一些示例（输入、输出、真实标签）。它是一个自编码器，旨在重建输入的同时填充缺失的补丁。
- en: Architecture
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构
- en: Their **encoder** is simply the ordinary vision transformer encoder we explained
    earlier. In training and inference, it takes only the “observed” patches.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的**编码器**实际上是我们之前解释的普通视觉变换器编码器。在训练和推理过程中，它只处理“观察到的”补丁。
- en: 'Meanwhile, their **decoder** is also simply the ordinary vision transformer
    encoder but it takes:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，他们的**解码器**实际上是普通的视觉变换器编码器，但它需要：
- en: Masked token vectors for the missing patches
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺失补丁的遮蔽标记向量
- en: Encoder output vectors for the known patches
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已知补丁的编码器输出向量
- en: So for an image [ [ A, B, X], [C, X, X], [X, D, E]] where X denotes a missing
    patch, the decoder will take the sequence of patch vectors [Enc(A), Enc(B), Vec(X),
    Vec(X), Vec(X), Enc(D), Enc(E)]. Enc returns the encoder output vector given the
    patch vector and X is a vector to represent missing token.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 所以对于一个图像 [ [ A, B, X], [C, X, X], [X, D, E]]，其中X表示缺失的补丁，解码器将处理补丁向量的序列 [Enc(A),
    Enc(B), Vec(X), Vec(X), Vec(X), Enc(D), Enc(E)]。Enc返回给定补丁向量的编码器输出向量，X是表示缺失标记的向量。
- en: The **last layer** in the decoder is a linear layer that maps the contextual
    embeddings (produced by the vision transformer encoder in the decoder) to a vector
    of length equal to the patch size. The loss function is mean squared error which
    squares the difference between the original patch vector and the predicted one
    by this layer. In the loss function, we only look at the decoder predictions due
    to masked tokens and ignore the ones corresponding the present ones (i.e., Dec(A),.
    Dec(B), Dec(C), etc.).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**解码器中的最后一层**是一个线性层，它将上下文嵌入（由解码器中的视觉变换器编码器生成）映射到一个长度等于补丁大小的向量。损失函数是均方误差，它计算原始补丁向量与此层预测补丁向量之间的差值平方。在损失函数中，我们只关注解码器对遮蔽标记的预测，忽略与已存在的标记（即Dec(A)、Dec(B)、Dec(C)等）对应的部分。'
- en: Final Remark and Example
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最后的评论和示例
- en: 'It may be surprising that the authors suggest masking about 75% of the patches
    in the images; BERT would mask only about 15% of the words. They justify like
    so:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 可能令人惊讶的是，作者建议遮蔽图像中约75%的补丁，而BERT仅会遮蔽大约15%的单词。他们是这样辩解的：
- en: Images,are natural signals with heavy spatial redundancy — e.g., a missing patch
    can be recovered from neighboring patches with little high-level understanding
    of parts, objects, and scenes. To overcome this difference and encourage learning
    useful features, we mask a very high portion of random patches.
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 图像是具有大量空间冗余的自然信号——例如，缺失的补丁可以通过相邻的补丁恢复，而无需高层次的理解部分、物体和场景。为了克服这一差异并鼓励学习有用的特征，我们会遮蔽大量的随机补丁。
- en: Want to try it out yourself? Checkout this [demo notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/ViTMAE/ViT_MAE_visualization_demo.ipynb)
    by NielsRogge.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 想自己试试吗？查看NielsRogge提供的[演示笔记本](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/ViTMAE/ViT_MAE_visualization_demo.ipynb)。
- en: This is all for this story. We went through a journey to understand how fundamental
    transformer models generalize to the computer vision world. Hope you have found
    it clear, insighful and worth your time.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是这个故事的全部内容。我们经历了一段旅程，理解了基本的变换器模型如何推广到计算机视觉领域。希望你觉得它清晰、有洞察力，且值得你的时间。
- en: 'References:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 参考文献：
- en: '[1] Dosovitskiy, A. *et al.* (2021) *An image is worth 16x16 words: Transformers
    for image recognition at scale*, *arXiv.org*. Available at: [https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)
    (Accessed: 28 June 2024).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Dosovitskiy, A. *et al.* (2021) *An image is worth 16x16 words: Transformers
    for image recognition at scale*, *arXiv.org*。可在以下网址访问：[https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)（访问时间：2024年6月28日）。'
- en: '[2] He, K. *et al.* (2021) *Masked autoencoders are scalable vision learners*,
    *arXiv.org*. Available at: [https://arxiv.org/abs/2111.06377](https://arxiv.org/abs/2111.06377)
    (Accessed: 28 June 2024).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] He, K. *et al.* (2021) *Masked autoencoders are scalable vision learners*,
    *arXiv.org*。可在以下网址访问：[https://arxiv.org/abs/2111.06377](https://arxiv.org/abs/2111.06377)（访问时间：2024年6月28日）。'
