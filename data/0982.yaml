- en: 'Neural Speed: Fast Inference on CPU for 4-bit Large Language Models'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Neural Speed：针对4位大型语言模型的CPU快速推理
- en: 原文：[https://towardsdatascience.com/neural-speed-fast-inference-on-cpu-for-4-bit-large-language-models-0d611978f399?source=collection_archive---------2-----------------------#2024-04-18](https://towardsdatascience.com/neural-speed-fast-inference-on-cpu-for-4-bit-large-language-models-0d611978f399?source=collection_archive---------2-----------------------#2024-04-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/neural-speed-fast-inference-on-cpu-for-4-bit-large-language-models-0d611978f399?source=collection_archive---------2-----------------------#2024-04-18](https://towardsdatascience.com/neural-speed-fast-inference-on-cpu-for-4-bit-large-language-models-0d611978f399?source=collection_archive---------2-----------------------#2024-04-18)
- en: Up to 40x faster than llama.cpp?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比llama.cpp快最多40倍？
- en: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--0d611978f399--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--0d611978f399--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0d611978f399--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0d611978f399--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--0d611978f399--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--0d611978f399--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--0d611978f399--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0d611978f399--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0d611978f399--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--0d611978f399--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0d611978f399--------------------------------)
    ·5 min read·Apr 18, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0d611978f399--------------------------------)
    ·5分钟阅读·2024年4月18日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/1517e7fb1e45ea750af91efd6cc45765.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1517e7fb1e45ea750af91efd6cc45765.png)'
- en: Generate with DALL-E
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DALL-E生成
- en: Running large language models (LLMs) on consumer hardware can be challenging.
    If the LLM doesn’t fit on the GPU memory, quantization is usually applied to reduce
    its size. However, even after quantization, the model might still be too large
    to fit on the GPU. An alternative is to run it on the CPU RAM using a framework
    optimized for [CPU inference such as llama.cpp](https://medium.com/@bnjmn_marie/gguf-quantization-for-fast-and-memory-efficient-inference-on-your-cpu-d10fbe58fbca).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在消费级硬件上运行大型语言模型（LLM）可能是一个挑战。如果LLM无法适应GPU内存，通常会应用量化技术来减小其大小。然而，即使经过量化，模型仍然可能过大，无法适应GPU。另一种方法是使用优化过的框架，将其在CPU
    RAM上运行，[例如llama.cpp](https://medium.com/@bnjmn_marie/gguf-quantization-for-fast-and-memory-efficient-inference-on-your-cpu-d10fbe58fbca)这样的CPU推理框架。
- en: Intel is also working on accelerating inference on the CPU. They propose a framework,
    [Intel’s extension for Transformers](https://github.com/intel/intel-extension-for-transformers),
    built on top of Hugging Face Transformers and easy to use to exploit the CPU.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Intel也在致力于加速CPU上的推理。他们提出了一种框架，[Intel为Transformers提供的扩展](https://github.com/intel/intel-extension-for-transformers)，该框架建立在Hugging
    Face Transformers之上，易于使用，能够充分利用CPU。
- en: With [Neural Speed](https://github.com/intel/neural-speed) (Apache 2.0 license),
    which relies on Intel’s extension for Transformers, Intel further accelerates
    inference for 4-bit LLMs on CPUs. According to Intel, using this framework can
    [make inference up to 40x faster than llama.cpp](https://github.com/intel/neural-speed?tab=readme-ov-file#key-features).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[Neural Speed](https://github.com/intel/neural-speed)（Apache 2.0许可证），该框架依赖于Intel为Transformers提供的扩展，Intel进一步加速了在CPU上对4位LLM的推理。根据Intel的说法，使用此框架可以[使推理速度比llama.cpp快40倍](https://github.com/intel/neural-speed?tab=readme-ov-file#key-features)。
- en: In this article, I review the main optimizations Neural Speed brings. I show
    how to use it and benchmark the inference throughput. I also compare it with llama.cpp.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我回顾了Neural Speed带来的主要优化。我展示了如何使用它并基准测试推理吞吐量。我还将其与llama.cpp进行了比较。
- en: Neural Speed’s Inference Optimizations for 4-bit LLMs
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Neural Speed对4位LLM的推理优化
- en: 'At NeurIPS 2023, Intel presented the main optimizations for inference on CPUs:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在2023年NeurIPS大会上，Intel展示了在CPU上进行推理的主要优化：
