- en: 'Bias-Variance Tradeoff, Explained: A Visual Guide with Code Examples for Beginners'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏差-方差权衡解析：为初学者提供的带有代码示例的视觉指南
- en: 原文：[https://towardsdatascience.com/bias-variance-tradeoff-explained-a-visual-guide-with-code-examples-for-beginners-9521871f728a?source=collection_archive---------4-----------------------#2024-11-25](https://towardsdatascience.com/bias-variance-tradeoff-explained-a-visual-guide-with-code-examples-for-beginners-9521871f728a?source=collection_archive---------4-----------------------#2024-11-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/bias-variance-tradeoff-explained-a-visual-guide-with-code-examples-for-beginners-9521871f728a?source=collection_archive---------4-----------------------#2024-11-25](https://towardsdatascience.com/bias-variance-tradeoff-explained-a-visual-guide-with-code-examples-for-beginners-9521871f728a?source=collection_archive---------4-----------------------#2024-11-25)
- en: MODEL EVALUATION & OPTIMIZATION
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型评估与优化
- en: How underfitting and overfitting fight over your models
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 欠拟合与过拟合如何在你的模型上“斗争”
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--9521871f728a--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--9521871f728a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9521871f728a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9521871f728a--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--9521871f728a--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@samybaladram?source=post_page---byline--9521871f728a--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--9521871f728a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9521871f728a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9521871f728a--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--9521871f728a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9521871f728a--------------------------------)
    ·20 min read·Nov 25, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9521871f728a--------------------------------)
    ·阅读时长 20 分钟·2024年11月25日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'Every time someone builds a prediction model, they face these classic problems:
    underfitting and overfitting. The model cannot be too simple, yet it also cannot
    be too complex. The interaction between these two forces is known as the bias-variance
    tradeoff, and it affects every predictive model out there.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 每当有人构建预测模型时，他们都会面临这些经典问题：欠拟合和过拟合。模型不能太简单，但也不能过于复杂。这两者之间的互动被称为偏差-方差权衡，它影响着所有预测模型。
- en: 'The thing about this topic of “bias-variance tradeoff” is that whenever you
    try to look up these terms online, you’ll find lots of articles with these perfect
    curves on graphs. Yes, they explain the basic idea — but they miss something important:
    they focus too much on theory, not enough on real-world problems, and rarely show
    what happens when you work with actual data.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 关于“偏差-方差权衡”这一主题的问题是，每当你尝试在线查找这些术语时，你会发现很多文章展示了完美的图表曲线。是的，它们解释了基本概念——但它们忽略了一个重要的点：它们过于关注理论，而不够关注现实世界中的问题，且很少展示在处理实际数据时会发生什么。
- en: Here, instead of theoretical examples, we’ll work with a real dataset and build
    actual models. Step by step, we’ll see exactly how models fail, what underfitting
    and overfitting look like in practice, and why finding the right balance matters.
    Let’s stop this fight between bias and variance, and find a fair middle ground.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将不使用理论示例，而是使用一个真实数据集并构建实际的模型。一步一步地，我们将确切地看到模型是如何失败的，欠拟合和过拟合在实践中是怎样表现的，以及为什么找到正确的平衡如此重要。让我们停止偏差和方差之间的斗争，找到一个公平的中间地带。
- en: '![](../Images/bf28840b0f2c53d90e3c33fda840385f.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bf28840b0f2c53d90e3c33fda840385f.png)'
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所有视觉效果：作者使用Canva Pro创建。已优化为移动端显示，可能在桌面端显示过大。
- en: What is Bias-Variance Tradeoff?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是偏差-方差权衡？
- en: Before we start, to avoid confusion, let’s make things clear about the terms
    **bias** and **variance** that we are using here in machine learning. These words
    get used differently in many places in math and data science.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，为了避免混淆，让我们澄清一下在这里机器学习中使用的“**偏差**”与“**方差**”这两个术语。这些词在数学和数据科学的许多领域中的使用方式是不同的。
- en: Bias can mean several things. [In statistics](https://en.wikipedia.org/wiki/Bias_(statistics)),
    it means how far off our calculations are from the true answer, and [in data science](https://en.wikipedia.org/wiki/Selection_bias),
    it can mean unfair treatment of certain groups. Even in the for other part of
    machine learning which [in neural networks](https://www.turing.com/kb/necessity-of-bias-in-neural-networks),
    it’s a special number that helps the network learn
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差可以有多种含义。[在统计学中](https://en.wikipedia.org/wiki/Bias_(statistics))，它表示我们的计算与真实答案之间的偏离程度，而[在数据科学中](https://en.wikipedia.org/wiki/Selection_bias)，它可以指对某些群体的不公平对待。即使是在机器学习的另一部分，[在神经网络中](https://www.turing.com/kb/necessity-of-bias-in-neural-networks)，它是一个帮助网络学习的特殊数字。
- en: Variance also has different meanings. [In statistics](https://en.wikipedia.org/wiki/Variance),
    it tells us how spread out numbers are from their average and [in scientific experiments](https://www.creative-wisdom.com/teaching/WBI/variance_control.shtml),
    it shows how much results change each time we repeat them.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 方差也有不同的含义。 [在统计学中](https://en.wikipedia.org/wiki/Variance)，它告诉我们数字与其平均值之间的分散程度，而[在科学实验中](https://www.creative-wisdom.com/teaching/WBI/variance_control.shtml)，它表示每次我们重复实验时结果的变化程度。
- en: But in machine learning’s “bias-variance tradeoff,” these words have special
    meanings.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 但在机器学习的“偏差-方差权衡”中，这些词语有特殊的含义。
- en: '**Bias** means how well a model can learn patterns. When we say a model has
    high bias, we mean it’s too simple and keeps making the same mistakes over and
    over.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**偏差**指的是模型学习模式的能力。当我们说一个模型有高偏差时，我们的意思是它太简单了，并且不断重复同样的错误。'
- en: '**Variance** here means how much your model’s answers change when you give
    it different training data. When we say high variance, we mean the modelchanges
    its answers too much when we show it new data.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**方差**在这里指的是当你给模型不同的训练数据时，它的答案会发生多大变化。当我们说方差很高时，我们的意思是模型在看到新数据时，其答案变化过大。'
- en: 'The “**bias-variance tradeoff**” is not something we can measure exactly with
    numbers. Instead, it helps us understand how our model is working: If a model
    has high bias, it does poorly on both training data and test data, an if a model
    has high variance, it does very well on training data but poorly on test data.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: “**偏差-方差权衡**”并不是我们可以通过数字精确测量的东西。相反，它帮助我们理解我们的模型是如何工作的：如果一个模型有很高的偏差，它在训练数据和测试数据上的表现都不好；而如果一个模型有很高的方差，它在训练数据上表现很好，但在测试数据上表现较差。
- en: This helps us fix our models when they’re not working well. Let’s set up our
    problem and data set to see how to apply this concept.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这帮助我们修复模型在表现不佳时的问题。让我们设置我们的任务和数据集，看看如何应用这个概念。
- en: ⛳️ Setting Up Our Problem
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ⛳️ 设置我们的任务
- en: Training and Test Dataset
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练与测试数据集
- en: 'Say, you own a golf course and now you’re trying to predict how many players
    will show up on a given day. You have collected the data about the weather: starting
    from the general outlook until the details of temperature and humidity. You want
    to use these weather conditions to predict how many players will come.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你拥有一个高尔夫球场，现在你试图预测某一天有多少球员会到场。你已经收集了关于天气的数据：从一般的天气概况到温度和湿度的详细信息。你想利用这些天气条件来预测将会有多少球员到来。
- en: '![](../Images/62b6715e07ff01c02029294d84cd18f9.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62b6715e07ff01c02029294d84cd18f9.png)'
- en: 'Columns: ‘Outlook (sunny, overcast, rain)’, ’Temperature’ (in Fahrenheit),
    ‘Humidity’ (in %), ‘Windy’ (Yes/No) and ‘Number of Players’ (target feature)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 列：‘天气概况（晴天、阴天、雨天）’，‘温度’（华氏度），‘湿度’（百分比），‘风力’（是/否）和‘球员人数’（目标特征）
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This might sound simple, but there’s a catch. We only have information from
    28 different days — that’s not a lot! And to make things even trickier, we need
    to split this data into two parts: 14 days to help our model learn (we call this
    training data), and 14 days to test if our model actually works (test data).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来很简单，但有一个问题。我们只有28天的数据——这不多！为了让事情变得更复杂，我们需要将这些数据分为两部分：14天的数据用来帮助我们的模型学习（我们称之为训练数据），而剩下的14天用来测试我们的模型是否有效（测试数据）。
- en: '![](../Images/385ccc5503447fe191d0d6c2e3a5fc50.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/385ccc5503447fe191d0d6c2e3a5fc50.png)'
- en: The first 14 dataset will be used to train the model, while the final 14 will
    be used to test the model.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 前14个数据集将用于训练模型，而最后14个将用于测试模型。
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Think about how hard this is. There are so many possible combination of weather
    conditions. It can be sunny & humid, sunny & cool, rainy & windy, overcast & cool,
    or other combinations. With only 14 days of training data, we definitely won’t
    see every possible weather combination. But our model still needs to make good
    predictions for any weather condition it might encounter.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 想一想这有多难。天气条件的组合有很多种。它可以是阳光明媚且潮湿、阳光明媚且凉爽、下雨且有风、阴天且凉爽，或其他组合。只有 14 天的训练数据，我们肯定无法看到每一种可能的天气组合。但我们的模型仍然需要对它可能遇到的任何天气条件做出准确预测。
- en: This is where our challenge begins. If we make our model too simple — like only
    looking at temperature — it will miss important details like wind and rain. That’s
    not good enough. But if we make it too complex — trying to account for every tiny
    weather change — it might think that one random quiet day during a rainy week
    means rain actually brings more players. With only 14 training examples, it’s
    easy for our model to get confused.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们挑战的开始。如果我们让模型过于简单——比如只关注温度——它将忽略像风和雨这样的重要细节。这样是不够的。但如果我们让模型过于复杂——试图考虑每一个微小的天气变化——它可能会认为在一个多雨的周里，某个随机的安静日子意味着雨水实际上带来了更多的玩家。只有
    14 个训练样本时，模型很容易变得混淆。
- en: 'And here’s the thing: unlike many examples you see online, our data isn’t perfect.
    Some days might have similar weather but different player counts. Maybe there
    was a local event that day, or maybe it was a holiday — but our weather data can’t
    tell us that. This is exactly what makes real-world prediction problems tricky.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有个问题：与许多你在网上看到的例子不同，我们的数据并不完美。有些日子天气相似，但玩家数量不同。也许那天有本地活动，或者那天是节假日——但我们的天气数据无法告诉我们这些。这正是现实世界预测问题的复杂性所在。
- en: 'So before we get into building models, take a moment to appreciate what we’re
    trying to do:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在我们开始构建模型之前，先花点时间了解我们正在尝试做的事情：
- en: Using just 14 examples to create a model that can predict player counts for
    ANY weather condition, even ones it hasn’t seen before.
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用仅有的 14 个例子来创建一个可以预测任何天气条件下玩家数量的模型，即使是它之前没见过的天气条件。
- en: This is the kind of real challenge that makes the bias-variance trade-off so
    important to understand.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是使得偏差-方差权衡如此重要的问题。
- en: Model Complexity
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型复杂度
- en: For our predictions, we’ll use decision tree regressors with varying depth (if
    you want to learn how this works, check out my article on [decision tree basics](/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef)).
    What matters for our discussion is how complex we let this model become.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的预测，我们将使用深度不同的决策树回归器（如果你想了解这个是如何工作的，可以查看我关于[决策树基础](https://example.org/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef)的文章）。对我们讨论来说，重要的是我们让这个模型变得多复杂。
- en: '![](../Images/bf5db4005f606e72d8e76bceb6b6a41b.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bf5db4005f606e72d8e76bceb6b6a41b.png)'
- en: We will train the decision trees using the whole training dataset. The depth
    of the tree is set first to stop the tree from growing up to a certain depth.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用整个训练数据集来训练决策树。树的深度首先设置，以防止树生长到某一深度。
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We’ll control the model’s complexity using its depth — from depth 1 (simplest)
    to depth 5 (most complex).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过控制模型的深度来控制模型的复杂度——从深度 1（最简单）到深度 5（最复杂）。
- en: '![](../Images/a67bdc7fd280e92d694e0663903f4123.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a67bdc7fd280e92d694e0663903f4123.png)'
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/01f44ef4c0388791ef9aacbfd5b7082c.png)![](../Images/8fa8694a1819b9ef1a4b11f46a4ab28d.png)![](../Images/7a3031d067133f5aee51d4887838a3fc.png)![](../Images/708ba80f3954ee07c52c534e06007255.png)![](../Images/9d4c754e23142f5685ea75c0ef9cc311.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01f44ef4c0388791ef9aacbfd5b7082c.png)![](../Images/8fa8694a1819b9ef1a4b11f46a4ab28d.png)![](../Images/7a3031d067133f5aee51d4887838a3fc.png)![](../Images/708ba80f3954ee07c52c534e06007255.png)![](../Images/9d4c754e23142f5685ea75c0ef9cc311.png)'
- en: 'Why these complexity levels matter:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这些复杂度级别很重要：
- en: 'Depth 1: Extremely simple — creates just a few different predictions'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度 1：极其简单 — 只创建几种不同的预测
- en: 'Depth 2: Slightly more flexible — can create more varied predictions'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度 2：稍微灵活一些 — 可以创建更多样化的预测
- en: 'Depth 3: Moderate complexity — getting close to too many rules'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度 3：适度复杂度 — 接近过多规则
- en: 'Depth 4–5: Highest complexity — nearly one rule per training example'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度 4–5：最高复杂度 — 每个训练样本几乎有一个规则
- en: Notice something interesting? Our most complex model (depth 5) creates almost
    as many different prediction rules as we have training examples. When a model
    starts making unique rules for almost every training example, it’s a clear sign
    we’ve made it too complex for our small dataset.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 发现了什么有趣的事情吗？我们最复杂的模型（深度5）几乎为每一个训练示例创建了不同的预测规则。当一个模型开始为几乎每个训练示例都生成独特的规则时，这是一个明确的信号，说明我们已经将模型做得对我们的小数据集来说过于复杂。
- en: Throughout the next sections, we’ll see how these different complexity levels
    perform on our golf course data, and why finding the right complexity is crucial
    for making reliable predictions.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将看到这些不同复杂度的模型在我们高尔夫球场数据上的表现，并探讨为什么找到合适的复杂度对做出可靠预测至关重要。
- en: What Makes a Model “Good”?
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么样的模型“好”？
- en: Prediction Errors
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测误差
- en: The main goal in prediction is to make guesses as close to the truth as possible.
    We need a way to measure errors that sees guessing too high or too low as equally
    bad. A prediction 10 units above the real answer is just as wrong as one 10 units
    below it.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 预测的主要目标是使猜测尽可能接近真实值。我们需要一种衡量误差的方式，这种方式把高估或低估视为同样的不正确。预测值比真实答案高10个单位和低10个单位是一样错误的。
- en: This is why we use **Root Mean Square Error (RMSE)** as our measurement. RMSE
    gives us the typical size of our prediction errors. If RMSE is 7, our predictions
    are usually off by about 7 units. If it’s 3, we’re usually off by about 3 units.
    A lower RMSE means better predictions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们使用**均方根误差（RMSE）**作为衡量标准的原因。RMSE告诉我们预测误差的典型大小。如果RMSE是7，意味着我们的预测通常会偏离真实值约7个单位。如果是3，通常偏离约3个单位。较低的RMSE意味着更好的预测效果。
- en: '![](../Images/4b69f4c62599ba2ba173037469adb4b4.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b69f4c62599ba2ba173037469adb4b4.png)'
- en: In the simple 5-point dataset above, we can say our prediction is roughly off
    by 3 people.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的简单5点数据集中，我们可以说我们的预测大约偏离3个人。
- en: When measuring model performance, we always calculate two different errors.
    First is the training error — how well the model performs on the data it learned
    from. Second is the test error — how well it performs on new data it has never
    seen. This test error is crucial because it tells us how well our model will work
    in real-world situations where it faces new data.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在衡量模型性能时，我们通常会计算两种不同的误差。首先是训练误差——即模型在它学习过的数据上的表现。其次是测试误差——即模型在它从未见过的新数据上的表现。这个测试误差非常关键，因为它告诉我们模型在面对现实世界中新的数据时的表现如何。
- en: ⛳️ Looking at Our Golf Course Predictions
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ⛳️ 查看我们的高尔夫球场预测
- en: 'In our golf course case, we’re trying to predict daily player counts based
    on weather conditions. We have data from 28 different days, which we split into
    two equal parts:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的高尔夫球场案例中，我们试图根据天气状况预测每日的玩家数量。我们拥有来自28个不同日期的数据，并将其分成两部分：
- en: 'Training data: Records from 14 days that our model uses to learn patterns'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据：模型用来学习模式的14天记录
- en: 'Test data: Records from 14 different days that we keep hidden from our model'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试数据：我们隐藏在模型之外的14天不同记录
- en: Using the models we made, let’s test both the training data and the test data,
    and also calculating their RMSE.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们构建的模型，让我们测试训练数据和测试数据，并计算它们的RMSE。
- en: '![](../Images/dc2e2893ad9d5e16348b6abb679078ca.png)![](../Images/4c345cc4d52de24c83c3d9d4c1759c5b.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc2e2893ad9d5e16348b6abb679078ca.png)![](../Images/4c345cc4d52de24c83c3d9d4c1759c5b.png)'
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/413369456a72d5c176d6baa2f6a16e5b.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/413369456a72d5c176d6baa2f6a16e5b.png)'
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/857cc086116969099a9091f30891f2db.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/857cc086116969099a9091f30891f2db.png)'
- en: 'Looking at these numbers, we can already see some interesting patterns: As
    we make our models more complex, they get better and better at predicting player
    counts for days they’ve seen before — to the point where our most complex model
    makes perfect predictions on training data.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些数字来看，我们已经能够看到一些有趣的模式：随着我们让模型变得越来越复杂，它们在预测之前已经见过的日期的玩家数量时变得越来越准确——直到我们最复杂的模型在训练数据上做出了完美的预测。
- en: '![](../Images/e92e88824480d1a8c42c20133d07f189.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e92e88824480d1a8c42c20133d07f189.png)'
- en: But the real test is how well they predict player counts for new days. Here,
    we see something different. While adding some complexity helps (the test error
    keeps getting better from depth 1 to depth 3), making the model too complex (depth
    4–5) actually starts making things worse again.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 但真正的考验是它们在预测新日期的玩家数量时的表现。在这里，我们看到了一些不同的情况。尽管增加一些复杂性有助于提高表现（从深度1到深度3，测试误差不断改善），但是将模型做得过于复杂（深度4-5）实际上会开始导致效果变差。
- en: '![](../Images/d91133e82e8ea3802cc41540d83ca708.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d91133e82e8ea3802cc41540d83ca708.png)'
- en: 'This difference between training and test performance (from being off by 3–4
    players to being off by 9 players) shows a fundamental challenge in prediction:
    performing well on new, unseen situations is much harder than performing well
    on familiar ones. Even with our best performing model, we see this gap between
    training and test performance.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和测试表现之间的差异（从误差为3-4名玩家到误差为9名玩家）揭示了预测中的一个根本挑战：在新的、未见过的情况中表现良好，比在熟悉的情况下表现好要难得多。即使是我们表现最好的模型，也能看到训练和测试表现之间的差距。
- en: '![](../Images/d18915b0493f91277de71fb1301381f2.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d18915b0493f91277de71fb1301381f2.png)'
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we’ll explore the two main ways models can fail: through consistently
    inaccurate predictions (bias) or through wildly inconsistent predictions (variance).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨模型失败的两种主要方式：通过持续不准确的预测（偏差）或通过极其不一致的预测（方差）。
- en: Understanding Bias (When Models Underfit)
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解偏差（当模型欠拟合时）
- en: What is Bias?
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是偏差？
- en: Bias happens when a model underfits the data by being too simple to capture
    important patterns. A model with high bias consistently makes large errors because
    it’s missing key relationships. Think of it as being consistently wrong in a predictable
    way.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差发生在模型通过过于简单无法捕捉到重要模式时。具有高偏差的模型会持续犯大错，因为它错过了关键的关系。可以把它理解为以可预测的方式始终错误。
- en: 'When a model underfits, it shows specific behaviors:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型欠拟合时，会表现出以下特定的行为：
- en: Similar sized errors across different predictions
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同预测中的误差相似
- en: Training error is high
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练误差很高
- en: Test error is also high
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试误差也很高
- en: Training and test errors are close to each other
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和测试误差接近
- en: High bias and underfitting are signs that our model needs to be more complex
    — it needs to pay attention to more patterns in the data. But how do we spot this
    problem? We look at both training and test errors. If both errors are high and
    similar to each other, we likely have a bias problem.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 高偏差和欠拟合表明我们的模型需要更加复杂——它需要关注数据中的更多模式。但我们如何发现这个问题呢？我们查看训练和测试的误差。如果两个误差都很高，并且彼此相似，我们很可能遇到偏差问题。
- en: ⛳️ Looking at Our Simple Golf Course Model
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ⛳️ 查看我们的简单高尔夫球场模型
- en: 'Let’s examine our simplest model’s performance (depth 1):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下我们最简单模型（深度为1）的表现：
- en: '![](../Images/a60afb362ddeb2e1d3f7d3170c422b45.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a60afb362ddeb2e1d3f7d3170c422b45.png)'
- en: 'Training RMSE: 16.13'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练RMSE：16.13
- en: On average, it’s off by about 16 players even for days it trained on
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 平均而言，即使是它训练过的日期，误差也大约为16名玩家。
- en: 'Test RMSE: 13.26'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试RMSE：13.26
- en: For new days, it’s off by about 13 players
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于新的日期，模型预测误差大约为13名玩家。
- en: These numbers tell an important story. First, notice how high both errors are.
    Being off by 13–16 players is a lot when many days see between 20–80 players.
    Second, while the test error is higher (as we’d expect), both errors are notably
    large.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字讲述了一个重要的故事。首先，注意到这两个误差都很高。误差为13-16名玩家，在很多日期的玩家数量在20到80之间时，这个误差很大。其次，尽管测试误差更高（如我们所料），但两者的误差都明显较大。
- en: 'Looking deeper at what’s happening:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 深入分析正在发生的情况：
- en: '![](../Images/4fb3f69c9d775e008d7a9fe5365f1556.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4fb3f69c9d775e008d7a9fe5365f1556.png)'
- en: With depth 1, our model can only make one split decision. It might just split
    days based on whether it is raining or not, creating only two possible predictions
    for player counts. This means many different weather conditions get lumped together
    with the same prediction.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在深度为1时，我们的模型只能做出一个分裂决策。它可能只是根据是否下雨来分裂日期，从而只创建两种可能的玩家数量预测。这意味着许多不同的天气条件被归类到相同的预测中。
- en: 'The errors follow clear patterns:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些误差遵循明显的模式：
- en: '- On hot, humid days: The model predicts too many players because it only sees
    whether it is raining or not'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 在炎热、潮湿的日子里：模型预测的玩家数过多，因为它只看是否下雨'
- en: '- On cool, perfect days: The model predicts too few players because it ignores
    great playing conditions'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 在凉爽、完美的日子里：模型预测的玩家数过少，因为它忽略了良好的比赛条件。'
- en: Most telling is how similar the training and test errors are. Both are high,
    which means even when predicting days it trained on, the model does poorly. This
    is the clearest sign of high bias — the model is too simple to even capture the
    patterns in its training data.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最具说明性的是训练误差和测试误差的相似性。两者都很高，这意味着即使是在模型训练过的日期上进行预测，模型也表现得很差。这是高偏差的最明显迹象——模型过于简单，甚至无法捕捉到其训练数据中的模式。
- en: 'This is the key problem with underfitting: the model lacks the complexity needed
    to capture important combinations of weather conditions that affect player turnout.
    Each prediction is wrong in predictable ways because the model simply can’t account
    for more than one weather factor at a time.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这是欠拟合的关键问题：模型缺乏捕捉影响玩家人数的重要天气条件组合的复杂性。每个预测都是以可预测的方式错误，因为模型根本无法同时考虑多个天气因素。
- en: 'The solution seems obvious: make the model more complex so it can look at multiple
    weather conditions together. But as we’ll see in the next section, this creates
    its own problems.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案看起来很明显：使模型更加复杂，以便它能够同时考虑多种天气条件。但正如我们在下一节将看到的，这会带来一些新的问题。
- en: Understanding Variance (When Models Overfit)
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解方差（当模型过拟合时）
- en: What is Variance?
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是方差？
- en: Variance occurs when a model overfits by becoming too complex and overly sensitive
    to small changes in the data. While an underfit model ignores important patterns,
    an overfit model does the opposite — it treats every tiny detail as if it were
    an important pattern.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 方差发生在模型过拟合时，它变得过于复杂，并对数据中的小变化过于敏感。虽然欠拟合模型忽视了重要的模式，但过拟合模型则相反——它把每一个微小的细节都当作重要的模式来处理。
- en: 'A model that’s overfitting shows these behaviors:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 一个过拟合的模型表现出以下特点：
- en: Very small errors on training data
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据中的非常小的错误
- en: Much larger errors on test data
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试数据中的更大误差
- en: A big gap between training and test errors
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和测试误差之间的巨大差距
- en: Predictions that change dramatically with small data changes
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着数据变化剧烈变化的预测
- en: This problem is especially dangerous with small datasets. When we only have
    a few examples to learn from, an overfit model might perfectly memorize all of
    them without learning the true patterns that matter.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集很小的时候，这个问题尤其危险。当我们只有几个示例供学习时，过拟合模型可能会完美地记住所有这些示例，而没有学习到真正重要的模式。
- en: ⛳️ Looking at Our Complex Golf Course Model
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ⛳️ 看看我们复杂的高尔夫球场模型
- en: 'Let’s examine our most complex model’s performance (depth 5):'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来检视一下我们最复杂模型（深度为5）的表现：
- en: '![](../Images/29cbb043dbe43ddb37a06a163e20d381.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29cbb043dbe43ddb37a06a163e20d381.png)'
- en: 'Training RMSE: 0.00'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练RMSE：0.00
- en: Perfect predictions! Not a single error on training data
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 完美的预测！训练数据中没有一个错误
- en: 'Test RMSE: 9.14'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试RMSE：9.14
- en: But on new days, it’s off by about 9–10 players
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 但是在新的一天，它的预测偏差大约是9到10个玩家
- en: These numbers reveal a classic case of overfitting. The training error of zero
    means our model learned to predict the exact number of players for every single
    day it trained on. Sounds great, right? But look at the test error — it’s much
    higher. This huge gap between training and test performance (from 0 to 9–10 players)
    is a red flag.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字揭示了一个典型的过拟合案例。零的训练误差意味着我们的模型学会了预测每个它训练过的特定日期的玩家人数。听起来不错，对吧？但是看看测试误差——它高得多。训练和测试表现之间的巨大差距（从0到9-10个玩家）是一个红旗。
- en: 'Looking deeper at what’s happening:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 更深入地看一下发生了什么：
- en: '![](../Images/47825e3568da33adba6f9cacd2f9844d.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47825e3568da33adba6f9cacd2f9844d.png)'
- en: 'With depth 5, our model creates extremely specific rules. For example:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在深度为5时，我们的模型会创建非常具体的规则。例如：
- en: '- If it’s not rainy AND temperature is 76°F AND humidity is 80% AND it’s windy
    → predict exactly 70 players'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 如果没有下雨并且温度是76°F并且湿度是80%并且有风 → 预测恰好70个玩家'
- en: Each rule is based on just one or two days from our training data.
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个规则仅基于我们训练数据中的一两天。
- en: When the model sees slightly different conditions in the test data, it gets
    confused.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当模型在测试数据中看到稍微不同的条件时，它会感到困惑。
- en: This is very similar to our first rule above, but the model might predict a
    completely different number
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这与我们上面的第一个规则非常相似，但模型可能会预测一个完全不同的数字
- en: With only 14 training examples, each training day gets its own highly specific
    set of rules. The model isn’t learning general patterns about how weather affects
    player counts — it’s just memorizing what happened on each specific day.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在只有14个训练示例的情况下，每个训练日都会有自己非常具体的一组规则。模型并没有学习天气如何影响玩家人数的普遍模式——它只是记住了每个特定日子发生了什么。
- en: What’s particularly interesting is that while this overfit model does much better
    than our underfit model (test error 9.15), it’s actually worse than our moderately
    complex model. This shows how adding too much complexity can start hurting our
    predictions, even if the training performance looks perfect.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 特别有趣的是，尽管这个过拟合模型比我们的欠拟合模型表现得更好（测试误差9.15），但它实际上比我们适度复杂的模型更差。这表明，增加过多的复杂性可能会开始损害我们的预测，尽管训练表现看起来是完美的。
- en: 'This is the fundamental challenge of overfitting: the model becomes so focused
    on making perfect predictions for the training data that it fails to learn the
    general patterns that would help it predict new situations well. It’s especially
    problematic when working with small datasets like ours, where creating a unique
    rule for each training example leaves us with no way to handle new situations
    reliably.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是过拟合的根本挑战：模型变得过于专注于对训练数据做出完美预测，以至于无法学习能够帮助其预测新情况的通用模式。当处理像我们这样的小数据集时，尤其是有问题的，因为为每个训练样本创建一个独特的规则会使我们无法可靠地处理新情况。
- en: Finding the Balance
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 寻找平衡
- en: The Core Problem
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 核心问题
- en: Now we’ve seen both problems — underfitting and overfitting — let’s look at
    what happens when we try to fix them. This is where the real challenge of the
    bias-variance trade-off becomes clear.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了两个问题——欠拟合和过拟合——让我们看看当我们尝试解决它们时会发生什么。这就是偏差-方差权衡的真正挑战所在。
- en: 'Looking at our models’ performance as we made them more complex:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 看看我们在使模型变得更复杂时，它们的表现：
- en: '![](../Images/5ab580aa2868077d0063910ba7fe1884.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5ab580aa2868077d0063910ba7fe1884.png)'
- en: 'These numbers tell an important story. As we made our model more complex:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字讲述了一个重要的故事。随着我们使模型变得更复杂：
- en: Training error kept getting better (16.3 → 6.7 → 3.6 → 1.1 → 0.0)
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练误差持续改善（16.3 → 6.7 → 3.6 → 1.1 → 0.0）
- en: Test error improved significantly at first (13.3 → 10.1 → 7.3)
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试误差最初显著改善（13.3 → 10.1 → 7.3）
- en: But then test error got slightly worse (7.3 → 8.8 → 9.1)
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 但随后测试误差略微变差（7.3 → 8.8 → 9.1）
- en: Why This Happens
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么会发生这种情况
- en: This pattern isn’t a coincidence — it’s the fundamental nature of the bias-variance
    trade-off.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模式不是巧合——它是偏差-方差权衡的基本特性。
- en: 'When we make a model more complex:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使模型变得更复杂时：
- en: It becomes less likely to underfit the training data (bias decreases)
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不太可能欠拟合训练数据（偏差减少）
- en: But it becomes more likely to overfit to small changes (variance increases)
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 但它变得更容易对小的变化发生过拟合（方差增加）
- en: 'Our golf course data shows this clearly:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的高尔夫球场数据清晰地展示了这一点：
- en: The depth 1 model underfit badly — it could only split days into two groups,
    leading to large errors everywhere
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 深度 1 模型欠拟合严重——它只能将天数分为两组，导致到处都有大的误差
- en: Adding complexity helped — depth 2 could consider more weather combinations,
    and depth 3 found even better patterns
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加复杂性有所帮助——深度 2 可以考虑更多的天气组合，而深度 3 发现了更好的模式
- en: But depth 4 started to overfit — creating unique rules for nearly every training
    day
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 但是深度 4 开始出现过拟合——为几乎每个训练天创建独特的规则
- en: 'The sweet spot came with our depth 3 model:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳点出现在我们的深度 3 模型中：
- en: '![](../Images/65d6d8150cad95ca942bb23e71884aa5.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/65d6d8150cad95ca942bb23e71884aa5.png)'
- en: This model is complex enough to avoid underfitting while simple enough to avoid
    overfitting. It has the best test performance (RMSE 7.13) of all our models.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型足够复杂，避免了欠拟合，同时又足够简单，避免了过拟合。它在所有模型中具有最佳的测试表现（RMSE 7.13）。
- en: The Real-World Impact
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 现实世界的影响
- en: 'With our golf course predictions, this trade-off has real consequences:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的高尔夫球场预测中，这种权衡有着真实的后果：
- en: 'Depth 1: Underfits by only looking at temperature, missing crucial information
    about rain or wind'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度 1：通过只看温度来欠拟合，错过了关于雨量或风速的关键信息
- en: 'Depth 2: Can combine two factors, like temperature AND rain'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度 2：可以结合两个因素，如温度和雨量
- en: 'Depth 3: Can find patterns like “warm, low humidity, and not rainy means high
    turnout”'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度 3：能够发现类似“温暖、低湿度、没有雨意味着高出勤率”的模式
- en: 'Depth 4–5: Overfits with unreliable rules like “exactly 76°F with 80% humidity
    on a windy day means exactly 70 players”'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度 4-5：通过不可靠的规则过拟合，如“在风大的日子里，温度恰好是 76°F，湿度是 80%，意味着恰好 70 名球员”
- en: This is why finding the right balance matters. With just 14 training examples,
    every decision about model complexity has big impacts. Our depth 3 model isn’t
    perfect — being off by 7 players on average isn’t ideal. But it’s much better
    than underfitting with depth 1 (off by 13 players) or overfitting with depth 4
    (giving wildly different predictions for very similar weather conditions).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么找到正确的平衡很重要。只有 14 个训练样本，每个关于模型复杂度的决策都会产生很大影响。我们的深度 3 模型并不完美——平均偏差 7 名球员并不理想。但它比深度
    1 的欠拟合（偏差 13 名球员）或深度 4 的过拟合（对非常相似的天气条件给出截然不同的预测）要好得多。
- en: How to Choose the Right Balance
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何选择正确的平衡
- en: The Basic Approach
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本方法
- en: When picking the best model, looking at training and test errors isn’t enough.
    Why? Because our test data is limited — with only 14 test examples, we might get
    lucky or unlucky with how well our model performs on those specific days.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择最佳模型时，仅查看训练误差和测试误差是不够的。为什么？因为我们的测试数据有限——只有14个测试样本，我们可能会因为特定的几天模型表现好或者不好而感到幸运或不幸运。
- en: 'A better way to test our models is called **cross-validation**. Instead of
    using just one split of training and test data, we try different splits. Each
    time we:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 测试模型的更好方法叫做**交叉验证**。与其仅使用一次训练和测试数据拆分，我们尝试不同的拆分。每次我们：
- en: Pick different samples as training data
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择不同的样本作为训练数据
- en: Train our model
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练我们的模型
- en: Test on the samples we didn’t use for training
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在未用于训练的样本上测试
- en: Record the errors
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记录误差
- en: By doing this multiple times, we can understand better how well our model really
    works.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 通过多次这样做，我们可以更好地理解我们的模型真正的表现如何。
- en: ⛳️ What We Found With Our Golf Course Data
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ⛳️ 我们从高尔夫球场数据中发现的结果
- en: Let’s look at how our different models performed across multiple training splits
    using cross-validation. Given our small dataset of just 14 training examples,
    we used K-fold cross-validation with k=7, meaning each validation fold had 2 samples.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下不同模型在多个训练拆分中的交叉验证表现。鉴于我们仅有14个训练样本，我们使用了K折交叉验证，k=7，这意味着每个验证折叠有2个样本。
- en: '![](../Images/e385c08896e206e43fcd8e9de12d97f9.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e385c08896e206e43fcd8e9de12d97f9.png)'
- en: 'While this is a small validation size, it allows us to maximize our training
    data while still getting meaningful cross-validation estimates:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这是一个较小的验证集，但它让我们能够最大化我们的训练数据，同时仍然获得有意义的交叉验证估计：
- en: '[PRE7]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/457b97b29c2a97b2dbc1c62c05b5f659.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/457b97b29c2a97b2dbc1c62c05b5f659.png)'
- en: 'Simple Model (depth 1):'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 简单模型（深度1）：
- en: '- CV Mean RMSE: 20.28 (±12.90)'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '- CV均值RMSE：20.28（±12.90）'
- en: '- Shows high variation in cross-validation (±12.90)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '- 在交叉验证中显示出较大的波动（±12.90）'
- en: '- Consistently poor performance across different data splits'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '- 不同数据拆分下表现 consistently 较差'
- en: 'Slightly Flexible Model (depth 2):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 稍微灵活的模型（深度2）：
- en: '- CV Mean RMSE: 17.35 (±11.00)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '- CV均值RMSE：17.35（±11.00）'
- en: '- Lower average error than depth 1'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '- 比深度1的平均误差低'
- en: '- Still shows considerable variation in cross-validation'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '- 交叉验证中仍然显示出相当大的波动'
- en: '- Some improvement in predictive power'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '- 预测能力有所提升'
- en: 'Moderate Complexity Model (depth 3):'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 中等复杂度模型（深度3）：
- en: '- CV Mean RMSE: 16.16 (±9.26)'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '- CV均值RMSE：16.16（±9.26）'
- en: '- More stable cross-validation performance'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '- 更稳定的交叉验证表现'
- en: '- Shows good improvement over simpler models'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '- 比更简单的模型有明显改善'
- en: '- Best balance of stability and accuracy'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '- 稳定性和准确性的最佳平衡'
- en: 'Complex Model (depth 4):'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂模型（深度4）：
- en: '- CV Mean RMSE: 16.10 (±12.33)'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '- CV均值RMSE：16.10（±12.33）'
- en: '- Very similar mean to depth 3'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '- 与深度3的均值非常相似'
- en: '- Larger variation in CV suggests less stable predictions'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '- 交叉验证中的较大波动表明预测不够稳定'
- en: '- Starting to show signs of overfitting'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '- 开始显示出过拟合的迹象'
- en: 'Very Complex Model (depth 5):'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 非常复杂的模型（深度5）：
- en: '- CV Mean RMSE: 16.59 (±11.73)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '- CV均值RMSE：16.59（±11.73）'
- en: '- CV performance starts to worsen'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '- CV性能开始恶化'
- en: '- High variation continues'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '- 高波动继续'
- en: '- Clear sign of overfitting beginning to occur'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '- 明显的过拟合迹象开始出现'
- en: '![](../Images/853f4e7f0dd7ac08f000f411fc7c5d4c.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/853f4e7f0dd7ac08f000f411fc7c5d4c.png)'
- en: 'This cross-validation shows us something important: while our depth 3 model
    achieved the best test performance in our earlier analysis, the cross-validation
    results reveal that model performance can vary significantly. The high standard
    deviations (ranging from ±9.26 to ±12.90 players) across all models show that
    with such a small dataset, any single split of the data might give us misleading
    results. This is why cross-validation is so important — it helps us see the true
    performance of our models beyond just one lucky or unlucky split.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这个交叉验证展示了一个重要的点：尽管我们之前的分析中深度3模型在测试性能上表现最好，但交叉验证结果揭示了模型性能的波动。所有模型中较高的标准差（从±9.26到±12.90不等）表明，在这样一个小的数据集上，任何一个数据拆分可能都会给我们带来误导性的结果。这也是交叉验证如此重要的原因——它帮助我们看到模型的真实表现，而不仅仅是一个幸运或不幸运的拆分结果。
- en: How to Make This Decision in Practice
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何在实践中做出这一决定
- en: 'Based on our results, here’s how we can find the right model balance:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的结果，以下是我们如何找到合适模型平衡的方法：
- en: '**Start Simple**'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**从简单开始**'
- en: Start with the most basic model you can build. Check how well it works on both
    your training data and test data. If it performs poorly on both, that’s okay!
    It just means your model needs to be a bit more complex to capture the important
    patterns.
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从你能构建的最基础模型开始。检查它在训练数据和测试数据上的表现。如果在这两者上都表现不佳，那也没关系！这只是说明你的模型需要稍微复杂一些，以便捕捉到重要的模式。
- en: '**Gradually Add Complexity**'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**逐渐增加复杂性**'
- en: Now slowly make your model more sophisticated, one step at a time. Watch how
    the performance changes with each adjustment. When you see it starting to do worse
    on new data, that’s your signal to stop — you’ve found the right balance of complexity.
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，逐步使你的模型变得更加复杂，注意每次调整后的性能变化。当你发现模型在新数据上的表现开始变差时，那就是信号，告诉你该停止了——你已经找到了合适的复杂性平衡。
- en: '**Watch for Warning Signs**'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**注意警告信号**'
- en: 'Keep an eye out for problems: If your model does extremely well on training
    data but poorly on new data, it’s too complex. If it does badly on all data, it’s
    too simple. If its performance changes a lot between different data splits, you’ve
    probably made it too complex.'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 留意潜在问题：如果你的模型在训练数据上表现极好，但在新数据上表现很差，那就说明模型太复杂。如果在所有数据上表现都很差，那说明模型太简单。如果模型在不同数据分割间的表现差异很大，那你可能做得太复杂了。
- en: '**Consider Your Data Size**'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**考虑数据规模**'
- en: When you don’t have much data (like our 14 examples), keep your model simple.
    You can’t expect a model to make perfect predictions with very few examples to
    learn from. With small datasets, it’s better to have a simple model that works
    consistently than a complex one that’s unreliable.
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当你没有太多数据（比如我们的14个样本）时，保持模型简单。你不能指望在非常少的样本上训练出一个能够做出完美预测的模型。对于小数据集，拥有一个稳定的简单模型比一个不可靠的复杂模型要好。
- en: Whenever we make prediction model, our goal isn’t to get perfect predictions
    — it’s to get reliable, useful predictions that will work well on new data. With
    our golf course dataset, being off by 6–7 players on average isn’t perfect, but
    it’s much better than being off by 11–12 players (too simple) or having wildly
    unreliable predictions (too complex).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们构建预测模型时，我们的目标不是获得完美的预测——而是获得可靠、有效的预测，这些预测将在新数据上表现良好。对于我们的高尔夫球场数据集，平均预测偏差为6-7名球员虽然不是完美的，但远比偏差为11-12名球员（过于简单）或预测极度不可靠（过于复杂）要好得多。
- en: Key Takeaways
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键要点
- en: Quick Ways to Spot Problems
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 快速识别问题的方法
- en: 'Let’s wrap up what we’ve learned about building prediction models that actually
    work. Here are the key signs that tell you if your model is underfitting or overfitting:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下我们关于构建实际有效的预测模型所学到的知识。以下是一些关键迹象，可以告诉你模型是过拟合还是欠拟合：
- en: '![](../Images/ed6d8ea301f30aa4d80257ec7e1326fb.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed6d8ea301f30aa4d80257ec7e1326fb.png)'
- en: '**Signs of Underfitting (Too Simple):**'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '**欠拟合的迹象（过于简单）：**'
- en: When a model underfits, the training error will be high (like our depth 1 model’s
    16.13 RMSE). Similarly, the test error will be high (13.26 RMSE). The gap between
    these errors is small (16.13 vs 13.26), which tells us that the model is always
    performing poorly. This kind of model is too simple to capture existing real relationships.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型欠拟合时，训练误差将很高（就像我们深度为1的模型，其RMSE为16.13）。同样，测试误差也会很高（13.26 RMSE）。这两个误差之间的差距很小（16.13与13.26），这告诉我们模型一直表现不佳。这样的模型过于简单，无法捕捉到真实的关系。
- en: '**Signs of Overfitting (Too Complex):**'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '**过拟合的迹象（过于复杂）：**'
- en: An overfit model shows a very different pattern. You’ll see very low training
    error (like our depth 5 model’s 0.00 RMSE) but much higher test error (9.15 RMSE).
    This large gap between training and test performance (0.00 vs 9.15) is a sign
    that the model is easily distracted by noise in the training data and it is just
    memorizing the specific examples it was trained on.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合模型显示出完全不同的模式。你会看到非常低的训练误差（就像我们深度为5的模型，其RMSE为0.00），但测试误差却高得多（9.15 RMSE）。训练和测试表现之间的巨大差距（0.00与9.15）是一个信号，表明模型很容易被训练数据中的噪声干扰，它仅仅是在记忆它所训练过的特定示例。
- en: '**Signs of a Good Balance (Like our depth 3 model):**'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '**良好平衡的迹象（如我们的深度3模型）：**'
- en: 'A well-balanced model shows more promising characteristics. The training error
    is reasonably low (3.16 RMSE) and while the test error is higher (7.33 RMSE),
    it’s our best overall performance. The gap between training and test error exists
    but isn’t extreme (3.16 vs 7.33). This tells us the model has found the sweet
    spot: it’s complex enough to capture real patterns in the data while being simple
    enough to avoid getting distracted by noise. This balance between underfitting
    and overfitting is exactly what we’re looking for in a reliable model.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 一个平衡良好的模型表现出更有前景的特征。训练误差相对较低（3.16 RMSE），尽管测试误差较高（7.33 RMSE），但这是我们最好的整体表现。训练误差和测试误差之间的差距存在，但并不极端（3.16
    vs 7.33）。这告诉我们，模型找到了甜蜜点：它足够复杂，能够捕捉到数据中的真实模式，同时又足够简单，避免了被噪声干扰。欠拟合和过拟合之间的这种平衡正是我们在可靠模型中所追求的。
- en: Final Remarks
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最后备注
- en: The bias-variance trade-off isn’t just theory. It has real impacts on real predictions
    including in our golf course example before. The goal here isn’t to eliminate
    either underfitting or overfitting completely, because that’s impossible. What
    we want is to find the sweet spot where your model is complex enough to avoid
    underfitting and catch real patterns while being simple enough to avoid overfitting
    to random noise.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差-方差权衡不仅仅是理论。它对实际预测有着真实的影响，包括我们之前的高尔夫球场示例。这里的目标不是完全消除欠拟合或过拟合，因为那是不可能的。我们想要的是找到那个甜蜜点，在这个点上，模型足够复杂以避免欠拟合并捕捉到真实的模式，同时又足够简单，以避免对随机噪声过拟合。
- en: At the end, a model that’s consistently off by a little is often more useful
    than one that overfits — occasionally perfect but usually way off.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，一个始终略有误差的模型通常比一个过拟合的模型更有用——后者偶尔完美，但通常误差较大。
- en: In the real world, reliability matters more than perfection.
  id: totrans-224
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在现实世界中，可靠性比完美更为重要。
- en: Technical Environment
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术环境
- en: This article uses Python 3.7 and scikit-learn 1.6\. While the concepts discussed
    are generally applicable, specific code implementations may vary slightly with
    different versions.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 本文使用的是Python 3.7和scikit-learn 1.6。虽然所讨论的概念通常适用，但不同版本之间的具体代码实现可能会略有不同。
- en: About the Illustrations
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于插图
- en: Unless otherwise noted, all images are created by the author, incorporating
    licensed design elements from Canva Pro.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，否则所有图片均由作者创作，结合了Canva Pro的授权设计元素。
- en: '𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝙈𝙤𝙙𝙚𝙡 𝙀𝙫𝙖𝙡𝙪𝙖𝙩𝙞𝙤𝙣 & 𝙊𝙥𝙩𝙞𝙢𝙞𝙯𝙖𝙩𝙞𝙤𝙣 𝙝𝙚𝙧𝙚:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝙈𝙤𝙙𝙚𝙡 𝙀𝙫𝙖𝙡𝙪𝙖𝙩𝙞𝙤𝙣 & 𝙊𝙥𝙩𝙞𝙢𝙞𝙯𝙖𝙩𝙞𝙤𝙣 𝙝𝙚𝙧𝙚:'
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----9521871f728a--------------------------------)'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----9521871f728a--------------------------------)'
- en: Model Evaluation & Optimization
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型评估与优化
- en: '[View list](https://medium.com/@samybaladram/list/model-evaluation-optimization-331287896864?source=post_page-----9521871f728a--------------------------------)3
    stories![](../Images/18fa82b1435fa7d5571ee54ae93a6c62.png)![](../Images/c95e89d05d1de700c631c342cd008de0.png)![](../Images/30e20e1a8ba3ced1e77644b706acd18d.png)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://medium.com/@samybaladram/list/model-evaluation-optimization-331287896864?source=post_page-----9521871f728a--------------------------------)3个故事![](../Images/18fa82b1435fa7d5571ee54ae93a6c62.png)![](../Images/c95e89d05d1de700c631c342cd008de0.png)![](../Images/30e20e1a8ba3ced1e77644b706acd18d.png)'
- en: '𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:'
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----9521871f728a--------------------------------)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----9521871f728a--------------------------------)'
- en: Classification Algorithms
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类算法
- en: '[View list](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----9521871f728a--------------------------------)8
    stories![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----9521871f728a--------------------------------)8个故事![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)'
