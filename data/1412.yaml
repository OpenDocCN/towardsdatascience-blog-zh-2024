- en: SageMaker vs Vertex AI for Model Inference
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SageMaker 与 Vertex AI 在模型推理方面的对比
- en: 原文：[https://towardsdatascience.com/sagemaker-vs-vertex-ai-for-model-inference-ef0d503cee76?source=collection_archive---------2-----------------------#2024-06-06](https://towardsdatascience.com/sagemaker-vs-vertex-ai-for-model-inference-ef0d503cee76?source=collection_archive---------2-----------------------#2024-06-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/sagemaker-vs-vertex-ai-for-model-inference-ef0d503cee76?source=collection_archive---------2-----------------------#2024-06-06](https://towardsdatascience.com/sagemaker-vs-vertex-ai-for-model-inference-ef0d503cee76?source=collection_archive---------2-----------------------#2024-06-06)
- en: Comparing the AWS and GCP fully-managed services for ML workflows
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较 AWS 和 GCP 在机器学习工作流中的全托管服务
- en: '[](https://medium.com/@turc.raluca?source=post_page---byline--ef0d503cee76--------------------------------)[![Julia
    Turc](../Images/1ca27d7db36799dec53b8daf4099f5cb.png)](https://medium.com/@turc.raluca?source=post_page---byline--ef0d503cee76--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ef0d503cee76--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ef0d503cee76--------------------------------)
    [Julia Turc](https://medium.com/@turc.raluca?source=post_page---byline--ef0d503cee76--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@turc.raluca?source=post_page---byline--ef0d503cee76--------------------------------)[![Julia
    Turc](../Images/1ca27d7db36799dec53b8daf4099f5cb.png)](https://medium.com/@turc.raluca?source=post_page---byline--ef0d503cee76--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ef0d503cee76--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ef0d503cee76--------------------------------)
    [Julia Turc](https://medium.com/@turc.raluca?source=post_page---byline--ef0d503cee76--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ef0d503cee76--------------------------------)
    ·12 min read·Jun 6, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ef0d503cee76--------------------------------)
    ·12 分钟阅读·2024年6月6日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: If you’re in that exciting stage of product development where you’re looking
    to deploy your first AI models to production, then take a moment to enjoy this
    clean slate. The decisions you’re about to make might influence the future of
    your company, or at least its technical debt going forward. No pressure :) Or
    at least that’s what I tell myself, now that I am starting to lay down the technical
    foundations of our company.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正处在产品开发的激动人心的阶段，准备将首个 AI 模型部署到生产环境中，那么不妨花点时间享受这一张白纸的自由。你即将做出的决策可能会影响公司未来的发展，或者至少会影响未来的技术债务。没有压力
    :) 或者至少这是我告诉自己的话，因为我现在正开始为我们公司奠定技术基础。
- en: At [Storia](https://storia.ai/), we build and deploy a lot of AI models, so
    efficient model serving is top of mind. We did a deep dive into two of the most
    prominent services, [SageMaker](https://aws.amazon.com/pm/sagemaker/) and [Vertex
    AI](https://cloud.google.com/vertex-ai?hl=en), and are sharing our takeaways here.
    We liked SageMaker better for our use case. While we tried to stay impartial for
    the sake of making the best decision for our company, who knows what sort of biases
    are creeping in. I spent many years at Google, my cofounder at Amazon. Both are
    offering us free credits through their startup programs and Amazon welcomed us
    into their [generative AI accelerator](https://aws-startup-lofts.com/amer/program/accelerators/generative-ai)
    last year.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [Storia](https://storia.ai/) ，我们构建并部署了大量 AI 模型，因此高效的模型服务至关重要。我们深入研究了两个最主要的服务，[SageMaker](https://aws.amazon.com/pm/sagemaker/)
    和 [Vertex AI](https://cloud.google.com/vertex-ai?hl=en)，并在此分享我们的经验。对于我们的使用场景，我们更喜欢
    SageMaker。尽管我们尽力保持公正，以便为公司做出最佳决策，但谁知道有哪些偏见在悄悄潜入呢。我曾在 Google 工作多年，我的联合创始人在 Amazon。两家公司都通过它们的初创企业项目向我们提供免费积分，去年
    Amazon 还邀请我们加入了它们的 [生成性 AI 加速器](https://aws-startup-lofts.com/amer/program/accelerators/generative-ai)。
- en: 'TL;DR: SageMaker wins overall. If you’re starting from scratch and have no
    affinity for one cloud provider over another (because of free credits, existing
    lock-in, or strong familiarity with their tooling), just go for SageMaker. However,
    if GCP already has you enthralled, stay there: Vertex AI is putting up a good
    enough fight.'
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: TL;DR：SageMaker 在整体表现上占优。如果你从零开始，并且对任何一个云提供商没有偏好（因为免费积分、现有锁定或者对其工具的强烈熟悉），那就选择
    SageMaker。然而，如果 GCP 已经让你深深着迷，还是留在那里吧：Vertex AI 提供了足够强有力的竞争。
- en: '![](../Images/3e1eee4fed051c6b3ae2fa1569a74d04.png)![](../Images/239206169cd8817b3efb60141b58dddc.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e1eee4fed051c6b3ae2fa1569a74d04.png)![](../Images/239206169cd8817b3efb60141b58dddc.png)'
- en: 'Photos from [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
    (left: [Christian Wiediger](https://unsplash.com/@christianw?utm_source=medium&utm_medium=referral),
    right: [Kai Wenzel](https://unsplash.com/@kai_wenzel?utm_source=medium&utm_medium=referral))'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)的照片（左图：[Christian
    Wiediger](https://unsplash.com/@christianw?utm_source=medium&utm_medium=referral)，右图：[Kai
    Wenzel](https://unsplash.com/@kai_wenzel?utm_source=medium&utm_medium=referral)）
- en: What are SageMaker and Vertex AI?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是SageMaker和Vertex AI？
- en: '[SageMaker](https://aws.amazon.com/pm/sagemaker/) and [Vertex AI](https://cloud.google.com/vertex-ai?hl=en)
    are two competing services from AWS and GCP for training and serving machine learning
    models. They wrap around cloud primitives (virtual machines, accelerators and
    storage) to streamline the process of building and deploying ML models. Their
    goal is to prevent developers from manually and repeatedly setting up operations
    that are common across most ML workflows.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[SageMaker](https://aws.amazon.com/pm/sagemaker/)和[Vertex AI](https://cloud.google.com/vertex-ai?hl=en)是来自AWS和GCP的两项竞争服务，用于训练和部署机器学习模型。它们通过云原生组件（虚拟机、加速器和存储）来简化构建和部署ML模型的过程。它们的目标是防止开发者手动并反复设置在大多数机器学习工作流中常见的操作。'
- en: 'For instance, building a training pipeline requires a few universal steps:
    placing the training data in a storage system, bringing up one or more accelerator-enabled
    virtual machines, ensuring they are not bottlenecked by I/O (i.e. more time is
    spent propagating gradients than reading training data), checkpointing and evaluating
    regularly, etc.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，构建训练管道需要一些通用步骤：将训练数据存储到一个存储系统中，启动一个或多个启用了加速器的虚拟机，确保它们不被I/O瓶颈限制（即，更多时间花费在传播梯度而不是读取训练数据上），定期进行检查点保存和评估等。
- en: SageMaker and Vertex AI enable developers to set up such involved workflows
    with a mere configuration file or a few bash commands. The result is a self-healing
    system that accomplishes the task without much monitoring needed. This is why
    they are often referred to as *fully managed services.*
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker和Vertex AI使开发者仅通过配置文件或几个bash命令便可设置如此复杂的工作流。其结果是一个自愈系统，能够在无需大量监控的情况下完成任务。这就是为什么它们常被称为*完全托管服务*的原因。
- en: SageMaker and Vertex AI for model inference
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SageMaker和Vertex AI在模型推理中的应用
- en: In this article, we compare SageMaker and Vertex AI from the lens of *model
    inference* in particular.Here, their main value proposition is to ensure that
    (a) the inference server is always up and running, and (b) it *autoscales* based
    on incoming traffic. The latter is particularly relevant in today’s era of large
    models that require powerful accelerators. Since GPUs are scarce and expensive,
    we cannot afford to have them sit idle, so we need to bring them up and down based
    on the amount of traffic.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们特别从*模型推理*的角度比较了SageMaker和Vertex AI。在这里，它们的主要价值主张是确保（a）推理服务器始终保持运行，以及（b）根据传入的流量*自动扩展*。后者在今天的大型模型时代尤其重要，这些模型需要强大的加速器。由于GPU稀缺且价格昂贵，我们无法承受它们处于闲置状态，因此需要根据流量的多少来动态地启用或停用它们。
- en: While we focus on inference in this article, it’s worth acknowledging these
    services encompass many other parts of the workflow. Notably, in addition to support
    for model training, they both include notebook-centric offerings for data scientists
    to analyze the training data (see [SageMaker Notebooks](https://aws.amazon.com/sagemaker/notebooks/)
    and [Vertex AI Notebooks](https://cloud.google.com/vertex-ai-notebooks?hl=en)).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本文重点讨论推理，但值得注意的是，这些服务涵盖了工作流的许多其他部分。特别是，除了支持模型训练外，它们还包括以笔记本为中心的功能，供数据科学家分析训练数据（参见[SageMaker
    Notebooks](https://aws.amazon.com/sagemaker/notebooks/)和[Vertex AI Notebooks](https://cloud.google.com/vertex-ai-notebooks?hl=en)）。
- en: Developer workflow
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开发者工作流
- en: 'When using SageMaker or VertexAI for model deployment, developers are expected
    to perform the following three steps:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用SageMaker或VertexAI进行模型部署时，开发者需要执行以下三个步骤：
- en: Create a model.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建模型。
- en: Configure an endpoint.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置端点。
- en: Deploy the model to the endpoint.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型部署到端点。
- en: These operations can be performed via the web interface, cloud-specific CLIs,
    or cloud-specific SDKs for various programming languages.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作可以通过网页界面、特定于云平台的CLI或支持多种编程语言的云平台SDK来执行。
- en: Creating a model
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建模型
- en: Creating a model boils down to supplying a [Docker](https://www.docker.com/)
    image for an HTTP server that responds to requests for (1) loading model artifacts
    in memory, (2) making predictions and (3) health checks. Beyond this contract,
    SageMaker and Vertex AI are relatively unopinionated about what they serve, treating
    models as black boxes that need to be kept up and running while responding to
    prediction requests.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 创建模型归结为提供一个 [Docker](https://www.docker.com/) 镜像，用于一个 HTTP 服务器，响应以下请求：（1）将模型工件加载到内存中，（2）进行预测，以及（3）健康检查。除了这一契约之外，SageMaker
    和 Vertex AI 对它们服务的内容相对不做限制，将模型视为需要保持运行并响应预测请求的黑盒。
- en: Both SageMaker and Vertex AI offer *prebuilt images* for various ML frameworks
    (PyTorch, Tensorflow, Scikit-learn, etc.) and built-in algorithms. For instance,
    if you simply want to run text-to-image generation with SDXL 1.0, you can grab
    the image from [Amazon’s Marketplace](https://aws.amazon.com/marketplace/pp/prodview-pe7wqwehghdtm?sr=0-3&ref_=beagle&applicationId=AWSMPContessa)
    or from [Google Cloud’s Model Garden](https://console.cloud.google.com/vertex-ai/publishers/stability-ai/model-garden/stable-diffusion-xl-base?rapt=AEjHL4MQfisT9c5bHE515GsX5Ytk7xRpC6SOyHvaxy3NpApsXgU1UjiMuTlQX8StsIi1PG0K8kSbOXSHJNg-j6baE4RUpxhPi4Asmv2W-3Nao5_FKiafEsg&project=textify-988c3).
    Alternatively, they also both support *custom images* that allow developers to
    write their own serving logic and define their own runtime environment, as long
    as the container exposes an HTTP server with the three endpoints mentioned above.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 和 Vertex AI 都提供适用于各种机器学习框架（如 PyTorch、TensorFlow、Scikit-learn 等）和内置算法的*预构建镜像*。例如，如果你只是想使用
    SDXL 1.0 运行文本到图像的生成，你可以从 [Amazon Marketplace](https://aws.amazon.com/marketplace/pp/prodview-pe7wqwehghdtm?sr=0-3&ref_=beagle&applicationId=AWSMPContessa)
    或 [Google Cloud’s Model Garden](https://console.cloud.google.com/vertex-ai/publishers/stability-ai/model-garden/stable-diffusion-xl-base?rapt=AEjHL4MQfisT9c5bHE515GsX5Ytk7xRpC6SOyHvaxy3NpApsXgU1UjiMuTlQX8StsIi1PG0K8kSbOXSHJNg-j6baE4RUpxhPi4Asmv2W-3Nao5_FKiafEsg&project=textify-988c3)
    获取镜像。或者，它们也都支持*自定义镜像*，允许开发者编写自己的服务逻辑并定义自己的运行时环境，只要容器暴露一个包含上述三个端点的 HTTP 服务器。
- en: Configuring an endpoint
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置端点
- en: 'An endpoint configuration associates a model with a set of runtime constraints:
    machine and accelerator type to run on, minimum and maximum amount of resources
    to be consumed, and how to handle autoscaling (what metric to monitor and above
    what threshold to trigger).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 端点配置将模型与一组运行时约束关联起来：运行的机器和加速器类型、要消耗的资源的最小和最大数量，以及如何处理自动伸缩（监控哪个指标，以及超过什么阈值触发）。
- en: Deploying a model
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署模型
- en: Once these configurations are in place, the developer gives the final green
    light. SageMaker and Vertex AI then provision the necessary machines, run the
    container, and call the initial model load method exposed by the inference server.
    Then, throughout the lifetime of the container, they make regular health checks
    and restart the container when necessary. Based on traffic, they scale up and
    down in an attempt to minimize resource consumption and maximize throughput.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这些配置完成，开发者就可以发出最终的绿灯。然后，SageMaker 和 Vertex AI 会分配所需的机器，运行容器，并调用推理服务器暴露的初始模型加载方法。之后，在容器的整个生命周期内，它们会定期进行健康检查，并在必要时重新启动容器。根据流量情况，它们会进行伸缩，以尽量减少资源消耗并最大化吞吐量。
- en: How do SageMaker and Vertex AI compare?
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SageMaker 和 Vertex AI 如何比较？
- en: 'Verdict: SageMaker wins overall. If you’re starting from scratch and have no
    affinity for one cloud provider over the other (free credits, existing lock-in,
    or strong familiarity with their tooling), just go for SageMaker. However, if
    GCP already has you enthralled, stay there: Vertex AI is putting up a good enough
    fight.'
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 结论：总体而言，SageMaker 更胜一筹。如果你是从零开始，并且对云服务提供商没有特别偏好（没有免费信用、现有的锁定或对工具的熟悉），就选择 SageMaker。然而，如果
    GCP 已经让你深深着迷，那就留在 GCP：Vertex AI 在这场竞争中表现得相当不错。
- en: Many times, the answer to this kind of question is *“It depends”*. But this
    is not one of those times. At least in the context of model serving, SageMaker
    wins by far on most dimensions. Compared to Vertex AI, SageMaker is generally
    more feature-rich and flexible, without losing sight of its original goal of making
    ML workflows *easy*. This, coupled with AWS’s general customer obsession (which
    translates into faster customer support and more free credits for startups) makes
    SageMaker a better choice overall.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 很多时候，类似问题的答案是*“视情况而定”*。但这次情况不同。至少在模型服务的背景下，SageMaker 在大多数维度上远远领先。与 Vertex AI
    相比，SageMaker 通常功能更丰富且更灵活，同时不会偏离其让机器学习工作流程*简便*的初衷。再加上 AWS 的普遍客户至上（这转化为更快的客户支持和更多的初创公司免费信用）使得
    SageMaker 总体上是一个更好的选择。
- en: That being said, Vertex AI can be good enough if your use case is not very sophisticated.
    If you have a good enough reason to prefer GCP (perhaps you’re already locked
    in, or have more free credits there), Vertex AI might work for you just fine.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，如果你的使用案例不太复杂，Vertex AI 可能足够好了。如果你有充分的理由选择 GCP（也许你已经被锁定，或者在那里有更多的免费信用），Vertex
    AI 可能会非常适合你。
- en: Autoscaling
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动扩展
- en: SageMaker offers more flexibility when configuring autoscaling. In contrast
    to Vertex AI, it can scale based on QPS instead of resource usage.
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 与 Vertex AI 相比，SageMaker 在配置自动扩展时提供了更多的灵活性。它可以基于 QPS（每秒查询数）而非资源使用情况进行扩展。
- en: In the context of model inference, autoscaling is one of the main value propositions
    of fully managed services like SageMaker and Vertex AI. When your traffic increases,
    they provision extra machines. When it decreases, they remove unnecessary instances.
    This is particularly important in today’s world, where most models run on accelerators
    that are too expensive to be kept idle. However, adjusting the allocated resources
    based on traffic is a non-trivial task.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型推理的背景下，自动扩展是像 SageMaker 和 Vertex AI 这样的完全托管服务的主要价值主张之一。当流量增加时，它们会分配额外的机器；当流量减少时，它们会移除不必要的实例。这在当今世界尤其重要，因为大多数模型都运行在过于昂贵的加速器上，不能让它们处于空闲状态。然而，根据流量调整分配的资源是一项复杂的任务。
- en: '**Why is autoscaling difficult?**'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**为什么自动扩展如此困难？**'
- en: A big hinderance is that scaling up is notinstantaneous. When an extra GPU is
    needed, the system will provision a new VM, download the Docker image, start the
    container, and download model artifacts. This can take anywhere between 3 to 8
    minutes, depending on the specifics of your deployment. Since it can’t react to
    fluctuations in traffic quickly enough, the system needs to predict traffic spikes
    ahead of time by leveraging past information.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一个主要的障碍是扩展不是即时的。当需要额外的 GPU 时，系统将配置一个新的虚拟机，下载 Docker 镜像，启动容器，并下载模型工件。这可能需要 3
    到 8 分钟不等，具体取决于你的部署细节。由于它无法快速响应流量波动，系统需要通过利用过去的信息提前预测流量激增。
- en: '**How SageMaker wins the autoscaling game**'
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**SageMaker 如何在自动扩展中获胜**'
- en: 'SageMaker offers three types of autoscaling (see [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-prerequisites.html)):
    (1) *target tracking* (tracks a designated metric — like CPU usage — and scales
    up when a predefined threshold is exceeded), (2) *step scaling* (supports more
    complex logic based on multiple tracked metrics) and (3) *scheduled scaling* (allows
    you to hard-code specific times when you expect a traffic increase).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 提供三种类型的自动扩展（见[文档](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-prerequisites.html)）：(1)
    *目标追踪*（追踪指定的指标—如 CPU 使用率—当超出预定阈值时扩展），(2) *步进扩展*（支持基于多个追踪指标的更复杂逻辑），以及 (3) *定时扩展*（允许你硬编码特定时间段，当你预期流量增加时使用）。
- en: 'The recommended method is *target tracking*: you pick any metric from [Amazon
    CloudWatch](https://aws.amazon.com/pm/cloudwatch/) (or even define a custom one!)
    and the value that should trigger scaling. There are metrics that reflect resource
    utilization (e.g. CPU / GPU memory or cycles) and also metrics that measure traffic
    (e.g. `InvocationsPerInstance` or `ApproximateBacklogSizePerInstance`).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐的方法是*目标追踪*：你可以从[Amazon CloudWatch](https://aws.amazon.com/pm/cloudwatch/)中选择任何一个指标（甚至可以定义一个自定义指标！），以及应触发扩展的值。有一些指标反映了资源的利用率（例如
    CPU / GPU 内存或周期），也有一些指标衡量流量（例如`InvocationsPerInstance`或`ApproximateBacklogSizePerInstance`）。
- en: 'In contrast, Vertex AI provides a lot less control (see [documentation](https://cloud.google.com/vertex-ai/docs/general/deployment#scaling)).
    The only option is target tracking, restricted to [two metrics](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/DedicatedResources#autoscalingmetricspec):
    CPU utilization and GPU duty cycles. Note that there is no metric that directly
    reflects traffic. This is very inconvenient when your model cannot serve multiple
    requests concurrently (i.e., neither batching nor multi-threading is possible).
    Without this ability, your CPU or GPU is operating in one of two modes: either
    *0% utilization* (no requests), or a fixed *x% utilization* (one or more requests).
    In this binary reality, CPU or GPU usage does not reflect the true load and is
    not a good trigger for scaling. Your only option is to scale up whenever utilization
    is somewhere between *0%* and *x%*, with the added complexity that *x* is accelerator-dependent:
    if you switch from an NVIDIA T4 to an A100, you’ll have to manually lower the
    threshold.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，Vertex AI 提供的控制选项要少得多（请参见[文档](https://cloud.google.com/vertex-ai/docs/general/deployment#scaling)）。唯一的选项是目标追踪，仅限于[两个指标](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/DedicatedResources#autoscalingmetricspec)：CPU
    利用率和 GPU 占用周期。需要注意的是，没有直接反映流量的指标。当你的模型不能并发处理多个请求时（即不支持批处理或多线程），这会非常不方便。在这种情况下，CPU
    或 GPU 处于两种模式之一：要么是 *0% 利用率*（没有请求），要么是固定的 *x% 利用率*（一个或多个请求）。在这种二元现实中，CPU 或 GPU
    的使用率并不能反映真实的负载，也不是一个好的扩展触发器。你的唯一选择是在利用率介于 *0%* 和 *x%* 之间时进行扩展，且有一个额外的复杂性，即 *x*
    是依赖于加速器的：如果你从 NVIDIA T4 切换到 A100，你需要手动调整阈值。
- en: For some extra drama, Vertex AI cannot scale down to zero ([see issue](https://issuetracker.google.com/issues/206042974));
    at least one machine must keep running. However, SageMaker allows completely removing
    all instances for their [asynchronous endpoints](https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference.html)
    (more on this in the next section).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 增添一些戏剧性的是，Vertex AI 无法扩展到零实例（[见问题](https://issuetracker.google.com/issues/206042974)）；至少需要保持一台机器在运行。然而，SageMaker
    允许完全移除其[异步端点](https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference.html)的所有实例（下一节将进一步讨论此问题）。
- en: Perhaps the only saving grace for GCP is that it allows you to easily track
    the autoscaling behavior on their web console, whereas AWS provides no information
    whatsoever on their web portal (and you’ll have to resort to a bash command in
    a for loop to monitor it).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 也许 GCP 唯一的优势是它允许你在其 Web 控制台中轻松跟踪自动扩展行为，而 AWS 在其 Web 门户上完全没有提供相关信息（你只能通过 bash
    命令循环来监控它）。
- en: Synchronous vs asynchronous predictions
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 同步预测与异步预测
- en: SageMaker supports both synchronous calls (which block waiting until the prediction
    is complete) and asynchronous calls (which immediately return a URL that will
    hold the output once ready). Vertex AI solely supports the former.
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: SageMaker 支持同步调用（会阻塞，直到预测完成）和异步调用（会立即返回一个 URL，预测结果准备好后可以通过该 URL 获取）。而 Vertex
    AI 仅支持前者。
- en: 'By default, SageMaker and Vertex AI endpoints are *synchronous —* the caller
    is blocked waiting until the prediction is complete. While this is the easiest
    client/server communication model to wrap your head around, it can be inconvenient
    when the model has high latency. Both services will simply timeout after 60 seconds:
    if a single model call takes longer than that, SageMaker / Vertex AI will simply
    return a *timeout* response. Note that this includes wait times as well. Say that
    the client issues two requests simultaneously, and each request takes 45 seconds
    to resolve. If your model doesn’t support parallelism (e.g. via batching), then
    the second request will timeout (since it would need 90 seconds to get resolved).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，SageMaker 和 Vertex AI 的端点都是 *同步的* —— 调用者会被阻塞，直到预测完成。虽然这种客户端/服务器通信模型是最容易理解的，但当模型存在高延迟时可能会不方便。这两个服务都将在
    60 秒后超时：如果单次模型调用超过此时间，SageMaker / Vertex AI 会返回一个 *超时* 响应。需要注意的是，这包括等待时间。假设客户端同时发出两个请求，每个请求需要
    45 秒来解决。如果你的模型不支持并行处理（例如通过批处理），那么第二个请求会超时（因为它需要 90 秒来解决）。
- en: To work around this issue, SageMaker supports [*asynchronous endpoints*](https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference.html)
    — they immediately respond to the client with an S3 URL; the model output will
    be placed there when completed. It is up to the client to poll the S3 location
    until available. Since requests are placed in a (best-effort) FIFO queue, the
    time out is extended to 15 minutes (as opposed to 60 seconds). Unfortunately,
    Vertex AI does not support asynchronous endpoints; you would have to implement
    your own queuing and retry logic if you don’t want your requests to simply be
    dropped after 60 seconds.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了绕过这个问题，SageMaker 支持 [*异步端点*](https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference.html)
    —— 它们会立即通过 S3 URL 响应客户端；当模型输出完成时，将被放置在该位置。客户端需要轮询 S3 位置，直到该结果可用。由于请求被放入一个（尽力而为的）FIFO
    队列，超时时间被延长至 15 分钟（而不是 60 秒）。不幸的是，Vertex AI 不支持异步端点；如果不希望请求在 60 秒后被丢弃，您需要实现自己的排队和重试逻辑。
- en: Note that both SageMaker and Vertex AI support *batch predictions*, which are
    asynchronous. These are not suitable for live traffic, but rather batch jobs (i.e.,
    running offline predictions over an entire dataset).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，SageMaker 和 Vertex AI 都支持 *批量预测*，它们是异步的。这些预测不适合实时流量，而是适合批处理作业（即对整个数据集进行离线预测）。
- en: Multi-model endpoints (MMEs)
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多模型端点（MMEs）
- en: SageMaker fully supports multi-model endpoints that share resources among models.
    Vertex AI’s multi-model endpoints solely share the URL, and don’t translate to
    any cost savings.
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: SageMaker 完全支持共享资源的多模型端点。Vertex AI 的多模型端点仅共享 URL，并不会带来任何成本节省。
- en: Sometimes you want to deploy more than just one model. Maybe you have an entire
    pipeline where each step requires a different model, like for [language-guided
    image editing](https://pytorch.org/blog/amazon-sagemaker-w-torchserve/). Or maybe
    you have a collection of independent models with a power law usage (2–3 of them
    are used frequently, and the long tail only occasionally). Allocating a dedicated
    machine to each model can get prohibitively expensive. To this end, SageMaker
    offers [*multi-model endpoints*](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html),
    which share the same container and resources among your models. They don’t need
    to all fit into memory; SageMaker can swap them in and out on demand based on
    which one is currently requested. The trade-off is the occasional cold start (i.e.
    if the requested model is not already in memory, your client will have to wait
    until SageMaker swaps it in). This is tolerable when you have a long tail of rarely
    used models.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候你需要部署的不止一个模型。也许你有一个完整的管道，每个步骤都需要不同的模型，比如 [基于语言的图像编辑](https://pytorch.org/blog/amazon-sagemaker-w-torchserve/)。或者，也许你有一组独立的模型，并且它们的使用遵循幂律分布（其中
    2-3 个模型经常被使用，而长尾部分则偶尔使用）。为每个模型分配一台专用机器可能会变得过于昂贵。为此，SageMaker 提供了 [*多模型端点*](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html)，它们在模型之间共享同一个容器和资源。这些模型不需要全部装入内存；SageMaker
    可以根据当前请求的模型动态调入调出。这样做的折衷是偶尔的冷启动（即，如果请求的模型不在内存中，客户端将不得不等待，直到 SageMaker 调入该模型）。当你有一组很少使用的模型时，这种情况是可以接受的。
- en: One constraint of SageMaker multi-model endpoints is that they require all models
    to use the same framework (PyTorch, Tensorflow etc.). However, [multi-container
    endpoints](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-container-endpoints.html)
    alleviate this restriction.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 多模型端点的一个限制是它要求所有模型使用相同的框架（如 PyTorch、Tensorflow 等）。然而，[多容器端点](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-container-endpoints.html)缓解了这一限制。
- en: While Vertex AI officially allows you to deploy multiple models to an endpoint
    (see [documentation](https://cloud.google.com/vertex-ai/docs/general/deployment#models-endpoint)),
    resources are actually associated with the model, not the endpoint. You don’t
    get the same advantage of sharing resources and reducing costs, but the mere convenience
    of being able to gradually transition traffic from a model v1 to a model v2 without
    changing the endpoint URL. Actually sharing resources is only possible for Tensorflow
    models that use a pre-built container, which is quite restrictive (see [documentation](https://cloud.google.com/vertex-ai/docs/predictions/model-co-hosting)).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Vertex AI 正式允许你将多个模型部署到一个端点（参见[文档](https://cloud.google.com/vertex-ai/docs/general/deployment#models-endpoint)），但资源实际上是与模型相关联的，而不是与端点相关联的。因此，你无法像在
    AWS 上那样共享资源来降低成本，但你可以方便地将流量从模型 v1 平滑过渡到模型 v2，而无需更改端点 URL。真正的资源共享仅适用于使用预构建容器的 TensorFlow
    模型，这有些限制（参见[文档](https://cloud.google.com/vertex-ai/docs/predictions/model-co-hosting)）。
- en: Quotas and GPU availability
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配额和 GPU 可用性
- en: 'When it comes to quota and accelerator availability, both providers have their
    own quirks, which are out-shadowed by the same fundamental challenge: GPUs are
    expensive.'
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在配额和加速器可用性方面，两个提供商各有其独特之处，但它们面临着相同的根本挑战：GPU 成本昂贵。
- en: With GCP, you can get access to (and pay for) a single A100 GPU. However, AWS
    forces you to rent 8 at a time (which, depending on your needs, might be an overkill).
    This situation is particular to A100s and doesn’t apply to lower-tier GPUs; you’re
    free to request a single GPU of any other type on AWS.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 GCP 上，你可以获得（并为此付费）一个 A100 GPU。但 AWS 强制要求你一次租用 8 个（这可能会根据你的需求显得有些过剩）。这种情况仅适用于
    A100 GPU，并不适用于低层次的 GPU；你可以自由请求任何其他类型的单个 GPU。
- en: Within GCP, quotas for VMs can be reused for Vertex AI. In other words, you
    only have to ask for that hot A100 once. However, AWS manages EC2 and SageMaker
    quotas separately (read more about [AWS service quotas](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html)),
    so make sure to request the quota for the right service.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 GCP 内，VM 的配额可以用于 Vertex AI。换句话说，你只需要请求一次那个热门的 A100 GPU。然而，AWS 会将 EC2 和 SageMaker
    的配额分别管理（了解更多关于[AWS 服务配额](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html)的信息），所以一定要确保请求正确服务的配额。
- en: While we have dedicated customer support from both providers (GCP via their
    [startup program](https://cloud.google.com/startup/?hl=en) and AWS via their [generative
    AI accelerator](https://aws-startup-lofts.com/amer/program/accelerators/generative-ai)),
    the AWS representatives are generally much more responsive, which also translates
    to quota requests being resolved quicker.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管我们可以从两家提供商处获得专门的客户支持（GCP 通过他们的[初创企业计划](https://cloud.google.com/startup/?hl=en)，AWS
    通过他们的[生成式 AI 加速器](https://aws-startup-lofts.com/amer/program/accelerators/generative-ai)），但
    AWS 的代表通常响应更快，这也意味着配额请求会更快解决。
- en: Limitations
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 限制
- en: 'In the previous sections, we discussed what limitations the two services have
    with respect to each other. There are, however, limitations that are common among
    the two:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，我们讨论了两种服务彼此之间的限制。然而，两者之间也存在一些共同的限制：
- en: '**Payload restrictions**. The model response payload has a maximum size for
    both services: 1.5 MB for public Vertex AI endpoints, 6 MB for synchronous SageMaker
    endpoints, and 1 GB for asynchronous endpoints ([source 1](https://cloud.google.com/vertex-ai/docs/predictions/get-online-predictions),
    [source 2](https://docs.aws.amazon.com/sagemaker/latest/dg/hosting-faqs.html)).'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**负载限制**。这两种服务的模型响应负载都有最大大小限制：Vertex AI 公共端点为 1.5 MB，SageMaker 同步端点为 6 MB，异步端点为
    1 GB（[source 1](https://cloud.google.com/vertex-ai/docs/predictions/get-online-predictions)，[source
    2](https://docs.aws.amazon.com/sagemaker/latest/dg/hosting-faqs.html)）。'
- en: '**Timeouts**. Prediction requests will be eventually dropped by both services:
    60 seconds for Vertex AI and synchronous SageMaker endpoints, 15 minutes for asynchronous
    SageMaker endpoints ([source 1](https://cloud.google.com/vertex-ai/docs/predictions/get-online-predictions),
    [source 2](https://docs.aws.amazon.com/sagemaker/latest/dg/hosting-faqs.html)).'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**超时**。预测请求最终会被两种服务丢弃：Vertex AI 的超时为 60 秒，SageMaker 的同步端点为 60 秒，SageMaker 的异步端点为
    15 分钟（[source 1](https://cloud.google.com/vertex-ai/docs/predictions/get-online-predictions)，[source
    2](https://docs.aws.amazon.com/sagemaker/latest/dg/hosting-faqs.html)）。'
- en: '**Scaling down to 0**. This is not supported by Vertex AI and synchronous SageMaker
    endpoints, but it is possible with SageMaker asynchronous endpoints.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**缩减到 0**。Vertex AI 和同步的 SageMaker 端点不支持此功能，但 SageMaker 的异步端点支持。'
- en: '**Attaching a shared file system**. Neither SageMaker nor Vertex AI allow mounting
    an external file storage system ([EFS](https://aws.amazon.com/efs/) or [FSx](https://aws.amazon.com/fsx/)
    in AWS and [Filestore](https://cloud.google.com/filestore?hl=en) in GCP). This
    could be convenient to store and share model artifacts across server replicas
    or implement tricks [like this one](https://cloud.google.com/blog/products/containers-kubernetes/stable-diffusion-containers-on-gke)
    for saving space in your Docker image (and reducing launch time). Note that they
    do support access to regular object storage (S3 and GCS).'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**附加共享文件系统**。SageMaker和Vertex AI都不允许挂载外部文件存储系统（AWS中的[EFS](https://aws.amazon.com/efs/)或[FSx](https://aws.amazon.com/fsx/)以及GCP中的[Filestore](https://cloud.google.com/filestore?hl=en)）。这对于存储和共享跨服务器副本的模型工件，或实现像[这个](https://cloud.google.com/blog/products/containers-kubernetes/stable-diffusion-containers-on-gke)这样的技巧以节省Docker镜像空间（并减少启动时间）可能很有用。需要注意的是，它们确实支持访问常规对象存储（S3和GCS）。'
- en: Summary
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'A lot has been said, so here is a neat table that compresses it all:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 已经说了很多，这里有一个简洁的表格，将所有内容压缩在一起：
- en: '![](../Images/5f43e208910b9b59dc8d0385fc44a7ab.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5f43e208910b9b59dc8d0385fc44a7ab.png)'
- en: Image by author. ✅ = supported, ❌ = not supported, ⚠️ = limited support.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。✅ = 支持，❌ = 不支持，⚠️ = 有限支持。
- en: Alternatives
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 替代方案
- en: SageMaker and Vertex are the most popular solutions for model serving and can
    satisfy most use cases. If you’re not happy with either, then you’ll have to do
    a little introspection. Do you want more flexibility? Do you want simplicity at
    the cost of even less flexibility? Or perhaps you just want to reduce cost at
    the expense of cold starts?
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker和Vertex是最受欢迎的模型服务解决方案，能够满足大多数用例。如果你对其中任何一个不满意，那么你可能需要做一些自我反思。你是想要更多的灵活性吗？你是想要以牺牲更多灵活性为代价的简化吗？还是你只是想要通过减少冷启动来降低成本？
- en: If flexibility is what you’re craving, then there’s probably no way of avoiding
    [Kubernetes](https://kubernetes.io/) — Amazon’s [EKS](https://aws.amazon.com/eks/)
    and Google’s [GKE](https://cloud.google.com/kubernetes-engine?hl=en) are managed
    Kubernetes services that might be a good start. The additional advantage is that
    Kubernetes is cloud-agnostic, so you can reuse the same configuration on AWS /
    GCP / Azure with an infrastructure automation tool like [Terraform](https://www.terraform.io/).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你渴望灵活性，那么可能无法避免使用[Kubernetes](https://kubernetes.io/)——亚马逊的[EKS](https://aws.amazon.com/eks/)和谷歌的[GKE](https://cloud.google.com/kubernetes-engine?hl=en)是托管的Kubernetes服务，可能是一个不错的起点。额外的优势是Kubernetes是与云平台无关的，因此你可以在AWS
    / GCP / Azure上重复使用相同的配置，配合像[Terraform](https://www.terraform.io/)这样的基础设施自动化工具。
- en: In contrast, if you’re aiming for simplicity, there are services like [Replicate](https://replicate.com/),
    [Baseten](https://www.baseten.co/), [Modal](https://modal.com/) or [Mystic](https://www.mystic.ai/)
    that are one level of abstraction above SageMaker and Vertex. They come with different
    trade-offs; for instance, Replicate makes it extremely easy to bring up model
    endpoints during the experimentation phase, but struggle with significant cold
    starts.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，如果你追求简便，有一些服务如[Replicate](https://replicate.com/)、[Baseten](https://www.baseten.co/)、[Modal](https://modal.com/)或[Mystic](https://www.mystic.ai/)，它们比SageMaker和Vertex多了一层抽象。它们有不同的权衡；例如，Replicate让你在实验阶段极其容易地创建模型端点，但在冷启动方面存在显著的困难。
- en: Contact
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联系方式
- en: '*If you’re thinking of efficient model serving, we want to hear from you! You
    can find me on Twitter* [*@juliarturc*](https://x.com/juliarturc) *or* [*LinkedIn*](https://www.linkedin.com/in/iulia-raluca-turc/)*.*'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你正在考虑高效的模型服务，我们很想听听你的意见！你可以在Twitter上找到我* [*@juliarturc*](https://x.com/juliarturc)
    *或在* [*LinkedIn*](https://www.linkedin.com/in/iulia-raluca-turc/) *联系我。*'
- en: Further reading
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入阅读
- en: '[Official SageMaker documentation](https://docs.aws.amazon.com/sagemaker/)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[官方SageMaker文档](https://docs.aws.amazon.com/sagemaker/)'
- en: '[Official Vertex AI documentation](https://cloud.google.com/vertex-ai/docs/predictions/overview)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[官方Vertex AI文档](https://cloud.google.com/vertex-ai/docs/predictions/overview)'
- en: '[Accelerate AI models on GPU using Amazon SageMaker multi-model endpoints with
    TorchServe, saving up to 75% on inference costs](https://pytorch.org/blog/amazon-sagemaker-w-torchserve/)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用Amazon SageMaker多模型端点与TorchServe加速GPU上的AI模型，节省多达75%的推理成本](https://pytorch.org/blog/amazon-sagemaker-w-torchserve/)'
- en: '[Serving Machine Learning models with Google Vertex AI](https://medium.com/google-cloud/serving-machine-learning-models-with-google-vertex-ai-5d9644ededa3)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用Google Vertex AI服务机器学习模型](https://medium.com/google-cloud/serving-machine-learning-models-with-google-vertex-ai-5d9644ededa3)'
- en: '[Improving launch time of Stable Diffusion on Google Kubernetes Engine (GKE)
    by 4X](https://cloud.google.com/blog/products/containers-kubernetes/stable-diffusion-containers-on-gke)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过 Google Kubernetes Engine (GKE) 将 Stable Diffusion 启动时间提高 4 倍](https://cloud.google.com/blog/products/containers-kubernetes/stable-diffusion-containers-on-gke)'
