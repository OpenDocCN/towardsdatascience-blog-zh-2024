- en: Optimizing Small Language Models on a Free T4 GPU
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在免费的T4 GPU上优化小型语言模型
- en: 原文：[https://towardsdatascience.com/optimizing-small-language-models-on-a-free-t4-gpu-008c37700d57?source=collection_archive---------7-----------------------#2024-01-30](https://towardsdatascience.com/optimizing-small-language-models-on-a-free-t4-gpu-008c37700d57?source=collection_archive---------7-----------------------#2024-01-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/optimizing-small-language-models-on-a-free-t4-gpu-008c37700d57?source=collection_archive---------7-----------------------#2024-01-30](https://towardsdatascience.com/optimizing-small-language-models-on-a-free-t4-gpu-008c37700d57?source=collection_archive---------7-----------------------#2024-01-30)
- en: Comprehensive Guide to Fine-Tuning Phi-2 using Direct Preference Optimization
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用直接偏好优化（DPO）微调Phi-2的全面指南
- en: '[](https://medium.com/@yanli.liu?source=post_page---byline--008c37700d57--------------------------------)[![Yanli
    Liu](../Images/31342655ab635eb38e3ce501235f1b89.png)](https://medium.com/@yanli.liu?source=post_page---byline--008c37700d57--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--008c37700d57--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--008c37700d57--------------------------------)
    [Yanli Liu](https://medium.com/@yanli.liu?source=post_page---byline--008c37700d57--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@yanli.liu?source=post_page---byline--008c37700d57--------------------------------)[![Yanli
    Liu](../Images/31342655ab635eb38e3ce501235f1b89.png)](https://medium.com/@yanli.liu?source=post_page---byline--008c37700d57--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--008c37700d57--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--008c37700d57--------------------------------)
    [Yanli Liu](https://medium.com/@yanli.liu?source=post_page---byline--008c37700d57--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--008c37700d57--------------------------------)
    ·11 min read·Jan 30, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--008c37700d57--------------------------------)
    ·阅读时长11分钟·2024年1月30日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/658323d7ef569529110ababc07d7f772.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/658323d7ef569529110ababc07d7f772.png)'
- en: Photo by [Donald Wu](https://unsplash.com/@donaldwuid?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自[Donald Wu](https://unsplash.com/@donaldwuid?utm_source=medium&utm_medium=referral)在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: “Small” Language Models (LLMs) are rapidly becoming a game-changer in the field
    of artificial intelligence.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: “小型”语言模型（LLMs）正在迅速成为人工智能领域的一项变革性技术。
- en: Unlike the traditional LLMs which require significant computational resources,
    these models are much smaller and more efficient. While their performance would
    be that of the larger ones, they can easily operate on standard devices such as
    laptops, and even go to the edge. This also means that they can be easily customized
    and integrated for use on your data set.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的语言模型（LLMs）需要大量计算资源不同，这些模型要小得多且更高效。尽管它们的性能可以与较大的模型相媲美，但它们能够轻松在标准设备（如笔记本电脑）上运行，甚至可以部署到边缘设备。这也意味着它们可以轻松定制并集成到你的数据集上使用。
- en: In this article, I will first explain the basics and inner workings of the model
    fine-tuning and alignment processes. Then, I’ll guide you through the process
    of preference fine-tuning Phi 2, a small LLM with 2 billion parameters, using
    a novel approach called Direct Preference Optimization (DPO).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将首先解释模型微调和对齐过程的基础知识和内部工作原理。然后，我将引导你通过使用一种名为直接偏好优化（Direct Preference Optimization，DPO）的创新方法对2亿参数的小型语言模型Phi
    2进行偏好微调的过程。
- en: Thanks to the small size of the model and optimization techniques such as quantization
    and QLoRA, we’ll be able to perform this process through Google Colab using the
    free T4 GPU! This requires some adaptation of the settings and hyperparameters
    used by Hugging Face to train its Zephyr 7B model.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型体积小且采用了量化和QLoRA等优化技术，我们将能够通过使用免费的T4 GPU在Google Colab中执行此过程！这需要一些调整Hugging
    Face用于训练其Zephyr 7B模型的设置和超参数。
- en: 'Table of Contents:'
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目录：
- en: '**Why We Need Fine-Tuning**…'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**为什么我们需要微调**…'
