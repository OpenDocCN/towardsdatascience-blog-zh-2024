- en: How to Implement State-of-the-Art Masked AutoEncoders (MAE)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现最先进的掩码自编码器（MAE）
- en: 原文：[https://towardsdatascience.com/how-to-implement-state-of-the-art-masked-autoencoders-mae-6f454b736087?source=collection_archive---------9-----------------------#2024-09-16](https://towardsdatascience.com/how-to-implement-state-of-the-art-masked-autoencoders-mae-6f454b736087?source=collection_archive---------9-----------------------#2024-09-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-implement-state-of-the-art-masked-autoencoders-mae-6f454b736087?source=collection_archive---------9-----------------------#2024-09-16](https://towardsdatascience.com/how-to-implement-state-of-the-art-masked-autoencoders-mae-6f454b736087?source=collection_archive---------9-----------------------#2024-09-16)
- en: '**A Step-by-Step Guide to Building MAE with Vision Transformers**'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**使用视觉变换器（Vision Transformers）构建 MAE 的一步步指南**'
- en: '[](https://medium.com/@francoisporcher?source=post_page---byline--6f454b736087--------------------------------)[![François
    Porcher](../Images/9ddb233f8cadbd69026bd79e2bd62dea.png)](https://medium.com/@francoisporcher?source=post_page---byline--6f454b736087--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6f454b736087--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6f454b736087--------------------------------)
    [François Porcher](https://medium.com/@francoisporcher?source=post_page---byline--6f454b736087--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@francoisporcher?source=post_page---byline--6f454b736087--------------------------------)[![François
    Porcher](../Images/9ddb233f8cadbd69026bd79e2bd62dea.png)](https://medium.com/@francoisporcher?source=post_page---byline--6f454b736087--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6f454b736087--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6f454b736087--------------------------------)
    [François Porcher](https://medium.com/@francoisporcher?source=post_page---byline--6f454b736087--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6f454b736087--------------------------------)
    ·7 min read·Sep 16, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6f454b736087--------------------------------)
    ·阅读时间：7分钟·2024年9月16日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Hi everyone! For those who do not know me yet, my name is Francois, I am a Research
    Scientist at Meta. I have a passion for explaining advanced AI concepts and making
    them more accessible.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 大家好！对于还不认识我的朋友们，我叫 Francois，我是 Meta 的研究科学家。我热衷于解释先进的 AI 概念，并使其更加易于理解。
- en: 'Today, I’m excited to delve into one of the most significant breakthroughs
    in Computer Vision post-Vision Transformers: **Masked Autoencoders (MAE).** This
    article serves as the practical implementation companion to my previous post:
    [The Ultimate Guide to Masked Autoencoders (MAE)](https://medium.com/ai-advances/the-ultimate-guide-to-masked-autoencoders-mae-87aa6883ff47)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，我很高兴深入探讨计算机视觉领域，继视觉变换器（Vision Transformers）之后最重要的突破之一：**掩码自编码器（MAE）**。本文作为我之前文章的实践实现伴随教程：[掩码自编码器（MAE）终极指南](https://medium.com/ai-advances/the-ultimate-guide-to-masked-autoencoders-mae-87aa6883ff47)
- en: 'For the following tutorial, we will use the code available on this github repository:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下教程中，我们将使用此 GitHub 仓库中的代码：
- en: '[](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----6f454b736087--------------------------------)
    [## GitHub - FrancoisPorcher/awesome-ai-tutorials: The best collection of AI tutorials
    to make you a…'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----6f454b736087--------------------------------)
    [## GitHub - FrancoisPorcher/awesome-ai-tutorials: 最佳 AI 教程合集，帮助你成为数据科学的高手…'
- en: The best collection of AI tutorials to make you a boss of Data Science! - FrancoisPorcher/awesome-ai-tutorials
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最佳的 AI 教程合集，帮助你成为数据科学的高手！ - FrancoisPorcher/awesome-ai-tutorials
- en: github.com](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----6f454b736087--------------------------------)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----6f454b736087--------------------------------)
- en: 'Here is a brief reminder of how it works:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这里简要提醒一下其工作原理：
- en: '![](../Images/4bb371ea36f5a017424c55d6032f01dd.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4bb371ea36f5a017424c55d6032f01dd.png)'
- en: Image from article [MAE are Scalable Learners](https://arxiv.org/abs/2111.06377)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来自文章 [MAE 是可扩展的学习器](https://arxiv.org/abs/2111.06377)
- en: '**Here’s how the methodology works:**'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**以下是该方法的工作原理：**'
- en: 1\. The image is split into patches.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 图像被分割成多个补丁。
- en: 2\. A subset of these patches is randomly masked.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 这些补丁的一个子集被随机掩盖。
- en: 3\. Only the visible patches are fed into the encoder **(this is crucial).**
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 仅将可见的补丁输入到编码器中 **（这至关重要）**。
