- en: Building a Research Agent That Can Write to Google Docs (Part 1)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建一个能够写入 Google Docs 的研究代理（第一部分）
- en: 原文：[https://towardsdatascience.com/building-a-research-agent-that-can-write-to-google-docs-part-1-4b49ea05a292?source=collection_archive---------6-----------------------#2024-11-20](https://towardsdatascience.com/building-a-research-agent-that-can-write-to-google-docs-part-1-4b49ea05a292?source=collection_archive---------6-----------------------#2024-11-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/building-a-research-agent-that-can-write-to-google-docs-part-1-4b49ea05a292?source=collection_archive---------6-----------------------#2024-11-20](https://towardsdatascience.com/building-a-research-agent-that-can-write-to-google-docs-part-1-4b49ea05a292?source=collection_archive---------6-----------------------#2024-11-20)
- en: '![](../Images/467c2bf3a8c599809df9567803dcb8fe.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/467c2bf3a8c599809df9567803dcb8fe.png)'
- en: Dalle-3’s interpretation of “A quirky AI assistant hard at work checking documents”.
    Image generated by the author.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Dalle-3对“一个古怪的AI助手在辛勤工作中检查文档”的诠释。图片由作者生成。
- en: A tool that might help with your homework
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可能帮助你完成作业的工具
- en: '[](https://medium.com/@rmartinshort?source=post_page---byline--4b49ea05a292--------------------------------)[![Robert
    Martin-Short](../Images/e3910071b72a914255b185b850579a5a.png)](https://medium.com/@rmartinshort?source=post_page---byline--4b49ea05a292--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4b49ea05a292--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4b49ea05a292--------------------------------)
    [Robert Martin-Short](https://medium.com/@rmartinshort?source=post_page---byline--4b49ea05a292--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@rmartinshort?source=post_page---byline--4b49ea05a292--------------------------------)[![Robert
    Martin-Short](../Images/e3910071b72a914255b185b850579a5a.png)](https://medium.com/@rmartinshort?source=post_page---byline--4b49ea05a292--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4b49ea05a292--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4b49ea05a292--------------------------------)
    [Robert Martin-Short](https://medium.com/@rmartinshort?source=post_page---byline--4b49ea05a292--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4b49ea05a292--------------------------------)
    ·15 min read·Nov 20, 2024
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[面向数据科学](https://towardsdatascience.com/?source=post_page---byline--4b49ea05a292--------------------------------)
    ·阅读时间15分钟·2024年11月20日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '***This article is the first of a two part series where we use LangGraph and
    Tavily to build a simple research agent, which writes and refines short articles.
    To keep track of the plans, articles and comments it generates we add the ability
    to programmatically create and edit Google Docs. In this article we focus on the
    agent, leaving the docs connection to the second article. You can find all the
    relevant code*** [***here***](https://github.com/rmartinshort/research_assist)***.***'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '***本文是两部分系列的第一篇，我们将使用LangGraph和Tavily构建一个简单的研究代理，能够编写并优化简短的文章。为了跟踪代理生成的计划、文章和评论，我们添加了通过编程方式创建和编辑Google
    Docs的功能。在本文中，我们将重点介绍代理部分，将Google Docs连接的部分留到第二篇文章中。你可以在[***这里***](https://github.com/rmartinshort/research_assist)***找到所有相关代码。***'
- en: Large Language Models (LLMs) are quickly finding use in all sorts of applications
    relevant to analysts and researchers, especially when it comes to the extraction,
    organization and summarization of text information. The community — both commercial
    and open source — is also making it increasingly easy to build and scale so-called
    “agentic” applications, in which the LLM assumes the role of a (hopefully) skilled
    analyst and makes semi-autonomous decisions. In a chatbot application, for example,
    if the user asks a complex or multi-step query the LLM might need to design a
    plan of action, correctly query multiple external tools — perhaps calculators,
    web searchers, vector databases etc — assemble the results and generate an answer.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）正在快速应用于与分析师和研究人员相关的各种场景，特别是在文本信息的提取、组织和总结方面。无论是商业界还是开源社区，都在不断简化构建和扩展所谓“代理型”应用程序的过程，在这些应用程序中，LLM充当（希望是）熟练的分析师角色，并做出半自主决策。例如，在一个聊天机器人应用程序中，如果用户提出一个复杂或多步骤的问题，LLM可能需要设计一个行动计划，正确查询多个外部工具——可能是计算器、网页搜索工具、向量数据库等——汇总结果并生成答案。
- en: Systems like this are often said to use the [ReAct framework](https://www.promptingguide.ai/techniques/react)
    of prompt engineering, which stands for “Reasoning-Action”. Basically, the structure
    and sequence of prompts forces the LLM to answer the question in very methodical
    fashion, first by articulating a thought (typically a plan of attack), carrying
    out an action, then making an observation of the result. In agentic systems, this
    process can continue iteratively until the LLM decides that it’s come to an acceptable
    answer.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这种系统通常被认为使用了[ReAct框架](https://www.promptingguide.ai/techniques/react)的提示工程方法，其中“ReAct”代表“Reasoning-Action”（推理-行动）。基本上，提示的结构和顺序迫使LLM以非常有条理的方式回答问题，首先通过表达一个思想（通常是攻击计划），然后执行一个行动，再观察结果。在代理系统中，这个过程可以不断迭代，直到LLM认为已经得出了一个可接受的答案。
- en: In this series of articles, we’ll use the [LangGraph](https://www.langchain.com/langgraph)
    library and [Tavily](https://tavily.com/) search tool to build a simple research
    assistant that demonstrates some of these concepts and might even be useful for
    those of us looking to generate quick, well written reports about any subject.
    Our agent will be inspired by the plan -> research -> write -> submit -> review
    -> revise cycle that happens in peer-reviewed research, and you can take a look
    at the prompts for these different sections [here](https://github.com/rmartinshort/research_assist/blob/main/research_assist/researcher/prompts.py).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一系列文章中，我们将使用[LangGraph](https://www.langchain.com/langgraph)库和[Tavily](https://tavily.com/)搜索工具，构建一个简单的研究助手，展示一些这些概念，并且可能对我们那些希望快速生成关于任何主题的简洁、写得好的报告的人有帮助。我们的代理将受到同行评审研究中的计划
    -> 研究 -> 写作 -> 提交 -> 审阅 -> 修订周期的启发，你可以在[这里](https://github.com/rmartinshort/research_assist/blob/main/research_assist/researcher/prompts.py)查看这些不同部分的提示。
- en: To make the system feel more complete, we’ll also add the ability to automatically
    add the material generated to a Google Doc, which is explored in [part 2](/building-a-research-assistant-that-can-write-to-google-docs-part-2-ac9dcacff4ff).
    This should be considered as more of an add-on than an integrated component of
    the agent, but it is interesting in its own right and so could also be read as
    a stand-alone article.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让系统感觉更完整，我们还将添加将生成的材料自动添加到Google文档的功能，详细内容请参见[第2部分](/building-a-research-assistant-that-can-write-to-google-docs-part-2-ac9dcacff4ff)。这应该被视为一个附加功能，而不是代理的集成组件，但它本身也很有趣，因此也可以作为一篇独立的文章阅读。
- en: 1\. What should our research assistant do?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 我们的研究助手应该做些什么？
- en: Before looking at how we can build this assistant and what it means for it to
    be “agentic”, we should think briefly about what we’d like it to do. The goal
    is to build a system that can plan and write short, informative articles about
    a given topic, then improve its own work through review and revision.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在看看我们如何构建这个助手以及它的“代理性”意味着什么之前，我们应该简要思考一下我们希望它做些什么。目标是构建一个可以计划并撰写关于给定主题的简短、信息丰富的文章，然后通过审阅和修订来改进自己的工作的系统。
- en: Why? Mainly this is just an exploration of technology, but the use of LLMs as
    semi-autonomous researchers is an active field of investigation and is yielding
    interesting projects such as [GPT-researcher](https://github.com/assafelovic/gpt-researcher).
    They have the potential to speed up the work of analysts, students, authors and
    researchers — though of course if the goal is human learning, there is no substitute
    for careful reading, note taking and discussion, which AI cannot replace.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么？主要这只是一次技术探索，但将LLM用作半自主研究员是一个活跃的研究领域，并且产生了一些有趣的项目，如[GPT-researcher](https://github.com/assafelovic/gpt-researcher)。它们有潜力加速分析师、学生、作家和研究人员的工作——尽管当然，如果目标是人类学习，仔细阅读、做笔记和讨论是不可替代的，AI无法取而代之。
- en: LLMs like GPT4, Anthropic Claude Sonnet, Meta Llama 3, Google Gemini Pro etc.
    can already write great articles out of the box with just a single prompt. However,
    these LLMs have knowledge cutoffs and so need access to additional tools in order
    to fetch the latest information, such as news about current events. There are
    plenty of services — notably tools like Perplexity, ChatGPT (now accessible via
    chat.com) and Google’s AI overview that already have this ability, but they are
    geared more towards providing quick summaries than polished research reports.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 像GPT4、Anthropic Claude Sonnet、Meta Llama 3、Google Gemini Pro等大型语言模型（LLM）已经能够通过一个简单的提示写出很棒的文章。然而，这些LLM存在知识截止问题，因此需要访问额外的工具来获取最新信息，比如关于时事新闻的内容。有许多服务——特别是像Perplexity、ChatGPT（现在可以通过chat.com访问）和Google的AI概览这些工具，它们已经具备了这种能力，但它们更倾向于提供快速总结，而不是精心编写的研究报告。
- en: Here, we’re making the assumption that multiple iterations of review and revision
    will improve the quality of an article generated by an LLM. This is certainly
    how it works in human writing. Our assistant will have the following components,
    each with its own instruction prompt
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们假设多次的审阅和修改将提升LLM生成文章的质量。这当然是人类写作中的工作方式。我们的助手将有以下几个组件，每个组件都有自己的指令提示。
- en: '**Planner.** Turns a poorly defined task into a structured article plan'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规划者。** 将一个定义不清的任务转化为结构化的文章计划'
- en: '**Researcher.** Takes the plan and searches the internet for relevant content.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**研究员。** 接受计划并在互联网上搜索相关内容。'
- en: '**Writer.** Uses the plan, retrieved content and it own knowledge to write
    the report'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**写作者。** 利用计划、检索到的内容和自身知识来撰写报告'
- en: '**Reviewer.** Reads the report and offers constructive criticism'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**审阅者。** 阅读报告并提供建设性的批评'
- en: '**Editor.** Reads the report and the reviewer’s criticism and decides if the
    report needs to be revised. If so, the report is sent back to the researcher and
    writer stages.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编辑。** 阅读报告和审阅者的批评，决定报告是否需要修改。如果需要，报告将被送回研究员和写作者阶段。'
- en: In our implementation each of these components will be calling the same LLM,
    namely GPT4o-mini, but in a real application they could just as easily use different,
    more specialized models.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实现中，这些组件将调用相同的LLM，即GPT4o-mini，但在实际应用中，它们完全可以使用不同的、更专业的模型。
- en: The output will be a well-written, informative report — preferably with references
    — that we can programmatically drop into a Google doc for safe keeping. It’s easy
    to modify the “personality” or our researcher by adapting the prompts. The editor
    is particularly important, because it’s the gatekeeper for the end of the process.
    If we make our editor very strict, the system might need to loop through many
    revisions to get accepted. To what extent will a stricter editor improve the quality
    of the result? That’s a very interesting question which, as they say, is beyond
    the scope of the current work!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将是一份写得很好的、信息丰富的报告——最好附带参考文献——我们可以程序化地将其放入Google文档中保存。通过调整提示，可以轻松修改我们研究员的“个性”。编辑特别重要，因为它是整个过程的把关人。如果我们让编辑非常严格，系统可能需要经过多次修改才能被接受。严格的编辑在多大程度上能提高结果质量？这是一个非常有趣的问题，正如他们所说，这超出了当前工作的范围！
- en: 2\. Structure of the agent
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 代理的结构
- en: Our research assistant is based heavily on the example described in this [excellent
    short course about LangGraph](https://www.deeplearning.ai/short-courses/ai-agents-in-langgraph/).
    LangGraph is an LLM orchestration library that attempts to make it easier for
    us to design and build reliable agents. For an in-depth comparison of LangGraph
    and LangChain, I recommend this excellent [article](/ai-agent-workflows-a-complete-guide-on-whether-to-build-with-langgraph-or-langchain-117025509fa0).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究助手在很大程度上基于[这门关于LangGraph的优秀短期课程](https://www.deeplearning.ai/short-courses/ai-agents-in-langgraph/)。LangGraph是一个LLM编排库，旨在让我们更容易设计和构建可靠的代理。关于LangGraph与LangChain的深入对比，我推荐这篇优秀的[文章](/ai-agent-workflows-a-complete-guide-on-whether-to-build-with-langgraph-or-langchain-117025509fa0)。
- en: What exactly is an agent? It appears that the community has not yet settled
    on a definition, but at least broadly speaking we might say that an agent is a
    [multi-step system where an LLM is allowed to make meaningful decisions about
    the outcome](https://blog.langchain.dev/what-is-an-agent/). This makes it more
    complex (and potentially more unpredictable) than a chain, which is just a predefined
    set of LLM calls one after the other.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 代理究竟是什么？似乎社区尚未达成统一定义，但至少广义上来说，我们可以说代理是一个[多步骤的系统，在这个系统中，LLM被允许对结果做出有意义的决策](https://blog.langchain.dev/what-is-an-agent/)。这使得它比链条更复杂（并且可能更不可预测），链条只是预先定义的一组LLM调用按顺序执行。
- en: In an agent framework, the LLM has some autonomy over how to solve the problem
    it’s given, perhaps by choosing the appropriate tool to call or deciding when
    to stop refining a solution once it’s good enough. In that sense the LLM becomes
    more of the brain of the system, acting more like a human analyst than just an
    API call. One interesting challenge here is that while agents might be free to
    make decisions, they are usually embedded within or interact with traditional
    software systems that require structured inputs and outputs. It’s therefore very
    important to force the agent to return its answers in the way that these other
    systems understand, regardless of the decision it makes.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在代理框架中，LLM对于如何解决其所给定的问题有一定的自主性，可能通过选择合适的工具来调用，或者决定在解决方案足够好时何时停止改进。从这个意义上讲，LLM更像是系统的大脑，更像一个人类分析师，而不仅仅是一个API调用。这里的一个有趣挑战是，尽管代理可以自由做出决策，但它们通常嵌入在或与传统的软件系统交互，这些系统需要结构化的输入和输出。因此，迫使代理以这些其他系统能够理解的方式返回答案非常重要，无论它做出了什么决策。
- en: For a more in-depth discussion of agents in the context of LangGraph, this [documentation](https://langchain-ai.github.io/langgraph/concepts/#graphs)
    is very helpful. Our research agent will be quite a simple one (partly because
    I am still learning this material too!) but hopefully could be a stepping stone
    towards something more sophisticated.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在LangGraph上下文中讨论代理的更深入内容，这份[文档](https://langchain-ai.github.io/langgraph/concepts/#graphs)非常有帮助。我们的研究代理将是一个相当简单的代理（部分原因是我也在学习这些材料！），但希望它能成为通向更复杂系统的一块垫脚石。
- en: In LangGraph we define the logic of our system as a graph, which consists of
    nodes and edges. Nodes are where LLM calls are made, and edges pass information
    from one node to the next. Edges can be conditional, meaning that they can direct
    information to different nodes depending on what decision is made. Information
    is passed between nodes in a structured format defined by a state.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在LangGraph中，我们将系统的逻辑定义为一个图，其中包含节点和边。节点是进行LLM调用的地方，边则将信息从一个节点传递到下一个节点。边可以是有条件的，意味着它们可以根据做出的决策将信息指向不同的节点。信息以由状态定义的结构化格式在节点之间传递。
- en: Our research assistant has a single stage called `AgentState` and it looks like
    this
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究助手只有一个阶段，叫做`AgentState`，看起来像这样：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This is where all the information relevant to our problem gets stored, and can
    be updated by LLM action inside a node of the graph.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这是存储与我们问题相关的所有信息的地方，并且可以通过LLM在图的某个节点内部进行更新。
- en: Now we can define some nodes. In the code, all the nodes are kept within the
    `AgentNodes` class, which is just a way I found helpful to group them. For example
    the planner node looks like this
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以定义一些节点。在代码中，所有节点都保存在`AgentNodes`类中，这只是我发现的一个有用的方式来对它们进行分组。例如，规划节点看起来像这样：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note how it takes in an `AgentState` and returns a modification to one of its
    components, namely the text for the research plan. When this node is run, the
    plan is updated.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 注意它如何接收一个`AgentState`并返回对其某个组件的修改，即研究计划的文本。当这个节点被运行时，计划会被更新。
- en: The code inside the node function uses standard LangChain syntax. `self.model`
    is an instance of `ChatOpenAI`, which looks like this
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 节点函数中的代码使用标准的LangChain语法。`self.model`是`ChatOpenAI`的一个实例，像这样：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The prompt consists of a system message from the `ResearchPlanPrompt` dataclass
    concatenated with the “task” element of the AgentState, which is the research
    topic provided by the user. The plan prompt looks like this.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个提示由来自`ResearchPlanPrompt`数据类的系统消息与AgentState的“task”元素拼接而成，后者是用户提供的研究课题。计划提示看起来像这样。
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Similar nodes need to be made for the following tasks
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 需要为以下任务创建类似的节点：
- en: '**Conduct research**. This is where we use an LLM to convert the research task
    into a series of queries, then use the Tavily search tool to find their answers
    online and save this under “content” in the AgentStage. This process is discussed
    in more detail in section 2'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**进行研究**。在这里，我们使用LLM将研究任务转换为一系列查询，然后使用Tavily搜索工具在线查找答案并将其保存在AgentStage的“content”下。此过程将在第2节中详细讨论。'
- en: '**Write the report**. Here we make use of the task name, the research plan,
    the research content and any previous reviewer comments to actually write the
    research report. This gets saved under “draft” in the AgentState. Whenever this
    runs, the `revision_number` indicator gets updated.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**撰写报告**。在这里，我们利用任务名称、研究计划、研究内容以及任何先前的审稿人评论来实际撰写研究报告。这些内容会保存在AgentState的“draft”下。每次运行时，`revision_number`指示器都会更新。'
- en: '**Review the report.** Call the LLM to critique the research report and save
    the review under “critique”'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**审查报告。** 调用LLM来批评研究报告，并将审查保存到“critique”下。'
- en: '**Conduct more research in response to the critique**. This is going to take
    in the original draft and the review and generate some more queries for Tavily
    that should help the system address the reviewer comments. Once again, this information
    is saved under “content”'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**根据反馈进行更多的研究**。这将处理原始草稿和审查意见，并为Tavily生成更多的查询，帮助系统解决审稿人评论。再一次，这些信息会保存在“content”下。'
- en: '**Make a decision** about whether or not the report satisfies the reviewer’s
    comments. This is done by the LLM with the guidance of the editor prompt, which
    instructs it to make a yes/no decision on the article and explain its reasoning.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**做出决策**，判断报告是否满足审稿人的评论。LLM会根据编辑提示的指导做出是/否决策，并解释其推理过程。'
- en: '**Dummy nodes** for rejecting or accepting the research. Once we get to either
    of these, we can end the flow. The final research report can then be extracted
    from the AgentState'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**虚拟节点**，用于拒绝或接受研究。一旦我们到达这两个节点中的任何一个，我们就可以结束流程。最终的研究报告可以从AgentState中提取。'
- en: 'We need to make a conditional edge in the graph at the editor node: If the
    editor says yes, we go to the accepted node. If no, we go back to the review node.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在图中的编辑节点处创建一个条件边：如果编辑器选择是，我们进入已接受节点。如果选择否，我们返回审查节点。
- en: To define this logic, we need to make a function to run inside the conditional
    edge. I have chosen to put this in an AgentEdges class, but this is not a requirement.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了定义此逻辑，我们需要创建一个函数在条件边内运行。我选择将其放入一个AgentEdges类中，但这不是必须的。
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In code, the entire graph setup looks like this
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，整个图的设置如下所示
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Before data can flow through a graph, the graph must be compiled. My understanding
    from the docs is that just runs some simple checks on the structured of the graph
    and returns a `CompiledGraph` object, which has methods like `stream` and `invoke.`These
    allow you to pass inputs to the start node, which is defined using `set_entry_point`
    in the code above.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据可以流经图之前，图必须被编译。从文档中的理解来看，它只是对图的结构做一些简单检查，并返回一个`CompiledGraph`对象，该对象具有像`stream`和`invoke`这样的函数。这些方法允许你将输入传递给起始节点，起始节点通过上面的代码中的`set_entry_point`来定义。
- en: When building these graphs, it can be very helpful to visualize all the nodes
    and edges in a notebook. This can be done with the following command
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建这些图时，将所有节点和边可视化在笔记本中非常有帮助。这可以通过以下命令实现。
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[LangGraph offers a few different ways of drawing the graph](https://langchain-ai.github.io/langgraph/how-tos/visualization/),
    depending on what visualization package you have installed. I’m using pygraphviz,
    which can be installed on an m-series mac using the following command'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[LangGraph提供了几种不同的绘制图形的方式](https://langchain-ai.github.io/langgraph/how-tos/visualization/)，具体取决于你安装的可视化包。我使用的是pygraphviz，可以通过以下命令在M系列Mac上安装。'
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/234af656591507969c2639052529c85b.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/234af656591507969c2639052529c85b.png)'
- en: Visualization of the control flow for our agent. Nodes are where LLM calls occur,
    while edges indicate the flow of information. Image generated by the author.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们代理的控制流可视化。节点是LLM调用发生的地方，而边表示信息流动。图像由作者生成。
- en: How do we test our agent? The simplest way would just be to call invoke with
    initial values of some of the components of AgentState (i.e. task, max_revisions
    and revision number), which enter the graph at the entry point node.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何测试代理？最简单的方法是使用一些AgentState组件的初始值（例如任务、最大修订次数和修订号）调用`invoke`，这些值将进入图的入口节点。
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: After some time (can be several minutes if the max_revisions is set to be large)
    this will return a dictionary of the agent state with all the components filled
    in. I’m using gpt4o-mini for this and the results are very impressive, although
    the extent to which adding the “review” and “editor” components really help to
    improve the quality of the article could be debated and we’ll return to that in
    section 3.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 经过一段时间（如果`max_revisions`设置为较大值，可能需要几分钟），这将返回一个包含所有组件的代理状态字典。我正在使用gpt4o-mini进行此操作，结果非常令人印象深刻，尽管关于“审查”和“编辑器”组件是否能真正提高文章质量的问题仍有争议，我们将在第三节中讨论这个问题。
- en: What if we want more insight into the inputs and outputs of the nodes at each
    stage of the graph? This is essential for debugging and explainable as the graph
    grows or if we’re hoping to deploy something like this in production. Thankfully
    LangGraph has some great tools here, which are covered under the [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/)
    and [streaming](https://langchain-ai.github.io/langgraph/concepts/streaming/)
    sections of its documentation. A minimal implementation looks something like this,
    where we are using an in memory store to keep track of the updates the come out
    of each stage of the graph.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望更深入了解图中每个阶段节点的输入和输出怎么办？这是调试和解释性非常重要的，特别是在图形变得越来越复杂或我们希望将其部署到生产环境时。幸运的是，LangGraph提供了一些很棒的工具，这些工具在其文档的[持久性](https://langchain-ai.github.io/langgraph/concepts/persistence/)和[流式传输](https://langchain-ai.github.io/langgraph/concepts/streaming/)部分中有介绍。一个最小的实现可能类似于下面的样子，在这里我们使用内存存储来跟踪图的每个阶段的更新。
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: More sophisticated applications would access the store from inside the nodes
    themselves, allowing a chatbot to recall previous conversations with a given user
    for example. Here we’re just using the memory to save the outputs of each of the
    nodes, which can then be viewed for debugging purposes. We’ll explore that a bit
    more in the final section.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 更复杂的应用程序将从节点内部访问存储，允许聊天机器人回忆与某个用户的先前对话。例如，在这里我们只是使用内存来保存每个节点的输出，这些输出可以用于调试目的查看。我们将在最后一节中进一步探讨这个问题。
- en: '**3\. What’s in the “*do_research*” node? The power of Tavily search**'
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**3\. “*do_research*”节点中有什么？Tavily搜索的强大功能**'
- en: Perhaps the most interesting parts of the control flow above are the `do_research`and
    `research_revise` nodes. Inside both of these nodes we are using an LLM to generate
    some web search queries relevant to the task, and then we’re using the [Tavily](https://docs.tavily.com/docs/welcome)
    API to actually conduct the search. Tavily is a relatively new service that offers
    a search engine optimized for AI agents. Practically what this means is that the
    service returns search results as chunks of relevant text from websites, rather
    than just a list of urls (which would need to be scraped and parsed) as in the
    case of typical search engine APIs.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 也许上述控制流中最有趣的部分是`do_research`和`research_revise`节点。在这两个节点中，我们使用LLM生成与任务相关的一些网页搜索查询，然后我们使用[Tavily](https://docs.tavily.com/docs/welcome)
    API实际进行搜索。Tavily是一个相对较新的服务，提供针对AI代理优化的搜索引擎。实际上，这意味着该服务返回来自网站的相关文本块，而不是像典型的搜索引擎API那样仅返回网址列表（这些网址需要被抓取和解析）。
- en: Under the hood, Tavily is likely using web scrapers and LLMs to extract content
    relevant to the user’s search, but all of that is abstracted away. You can sign
    up [here](https://app.tavily.com/home) for Tavily’s free “Researcher” plan which
    gives 1000 free API calls. Unfortunately after that you’d need to pay a monthly
    fee to keep using it, which is likely only worth it for business use cases.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在背后，Tavily很可能使用网页抓取工具和LLM来提取与用户搜索相关的内容，但所有这些都被抽象化了。你可以在[这里](https://app.tavily.com/home)注册Tavily的免费“研究员”计划，获得1000次免费的API调用。不幸的是，超过此次数后，你需要支付月费才能继续使用，可能只有在商业用例中才值得这样做。
- en: Lets see an example using the code very similar to what’s going on inside `AgentNodes.research_plan_node`
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个使用与`AgentNodes.research_plan_node`中非常相似的代码的例子。
- en: '[PRE10]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This generates 5 search queries relevant to the task we defined, which look
    like this
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成5个与我们定义的任务相关的搜索查询，结果如下所示：
- en: '[PRE11]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Next we can call Tavily search on each of these queries
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以对这些查询中的每一个调用Tavily搜索。
- en: '[PRE12]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This provides a nicely formatted result with url, title and text chunk.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这将提供一个格式良好的结果，包含网址、标题和文本块。
- en: '![](../Images/d53f7c7858080ce176276d07bcf8a46c.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d53f7c7858080ce176276d07bcf8a46c.png)'
- en: Example results from a Tavily search. Image generated by the author.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 来自Tavily搜索的示例结果。图片由作者生成。
- en: This is a very powerful and easy to use search tool that can give LLM applications
    access to the web without the need for extra work!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常强大且易于使用的搜索工具，可以让LLM应用程序访问网络，而无需额外的工作！
- en: In our researcher agent, we’re currently only using the content field, which
    we extract and append to a list which is passed into the AgentState. That information
    then gets injected into the prompt thats used for the writer node, hence allowing
    the LLM to have access to it when generating the report.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的研究员代理中，我们目前只使用内容字段，并将其提取并附加到一个列表中，该列表被传递到AgentState中。然后，这些信息会被注入到用于写作节点的提示中，从而允许LLM在生成报告时访问这些信息。
- en: There is a lot more you can do with Tavily search, but be aware that experimenting
    with it will quickly burn through your free API calls. In fact, for our report
    writing task there are many applications where Tavily calls probably aren’t necessary
    (i.e. the LLM already has sufficient knowledge to write the report), so I would
    recommend adding an additional conditional edge that allows the system to bypass
    the `do_research` and `research_revise` nodes if it determines that a web search
    is not needed. I will likely update the repo with this change soon.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Tavily 搜索可以做的事情还很多，但要注意，实验使用它会迅速消耗你的免费 API 调用。事实上，对于我们的报告写作任务，有很多应用场景 Tavily
    调用可能不是必须的（即 LLM 已经有足够的知识来写报告），所以我建议添加一个额外的条件边，使系统在判断不需要进行网络搜索时跳过 `do_research`
    和 `research_revise` 节点。我可能很快会在仓库中更新这个修改。
- en: '**4\. Walk through an example**'
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**4. 演示一个例子**'
- en: To solidify everything we just learned, let’s walk through an example of the
    researcher in action, using the same task as above.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了巩固我们刚刚学到的内容，让我们通过一个实际的例子来演示研究人员的工作，使用与上面相同的任务。
- en: First, we import the libraries and set up our LLM and searcher models
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入库并设置我们的 LLM 和搜索模型
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now we can run the agent on a task and give it a maximum number of revisions.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以在任务上运行代理，并给它一个最大的修订次数。
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now the agent will run its task, which might take about a minute. Logging has
    been added to show what it’s doing and importantly, the results are being saved
    to the `in_memory_store` , which we saw at the end of section 2.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在代理将运行它的任务，这可能需要大约一分钟。已经添加了日志记录以显示它正在做什么，重要的是，结果正在保存到 `in_memory_store` 中，我们在第二部分末尾看到了它。
- en: The final report is accessible in a few ways. Its stored in the result list
    and can be visualized in a notebook like this
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最终报告有几种方式可以访问。它存储在结果列表中，可以像这样在笔记本中可视化。
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Its also stored in the agent’s memory along with all the other outputs. We can
    access it as follows
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 它也存储在代理的记忆中，和所有其他输出一起。我们可以按照以下方式访问它。
- en: '[PRE16]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The report itself is about 1300 words long — a bit too much to copy here — but
    I’ve pasted it into the repo [here](https://github.com/rmartinshort/research_assist/tree/main/research_assist/examples).
    We can also take a look at what the editor thought of it after one round of revision
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 报告本身大约有 1300 字——有点多，无法在这里复制——但我已经将其粘贴到了仓库的[这里](https://github.com/rmartinshort/research_assist/tree/main/research_assist/examples)。我们也可以看看编辑器在经过一轮修订后的看法。
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: It seems the editor was satisfied!
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来编辑器很满意！
- en: For debugging purposes, we probably need to read though all the other outputs
    though. This can be painful to do in a notebook so in the next article we’ll discuss
    how they can be programmatically dropped into Google Docs. Thanks for making it
    to the end and [we’ll pick up in part 2](/building-a-research-assistant-that-can-write-to-google-docs-part-2-ac9dcacff4ff)!
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调试，我们可能需要阅读其他所有输出。不过，在笔记本中做这件事可能会很痛苦，所以在下一篇文章中，我们将讨论如何将这些输出程序化地插入到 Google
    Docs 中。感谢你坚持看到最后，[我们将在第二部分继续](https://building-a-research-assistant-that-can-write-to-google-docs-part-2-ac9dcacff4ff)！
- en: The author is unaffiliated with any of the tools discussed in this article.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 作者与本文讨论的任何工具都没有任何关联。
