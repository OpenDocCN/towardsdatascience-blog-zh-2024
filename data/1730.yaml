- en: The LLM Triangle Principles to Architect Reliable AI Apps
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建可靠AI应用程序的LLM三角原则
- en: 原文：[https://towardsdatascience.com/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e?source=collection_archive---------0-----------------------#2024-07-16](https://towardsdatascience.com/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e?source=collection_archive---------0-----------------------#2024-07-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e?source=collection_archive---------0-----------------------#2024-07-16](https://towardsdatascience.com/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e?source=collection_archive---------0-----------------------#2024-07-16)
- en: Software design principles for thoughtfully designing reliable, high-performing
    LLM applications. A framework to bridge the gap between potential and production-grade
    performance.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 软件设计原则，用于精心设计可靠且高性能的LLM应用程序。这是一个弥合潜力与生产级性能之间差距的框架。
- en: '[](https://medium.com/@almogbaku?source=post_page---byline--d3753dd8542e--------------------------------)[![Almog
    Baku](../Images/3ac36986f6ca0ba56c8edced6ec7dd07.png)](https://medium.com/@almogbaku?source=post_page---byline--d3753dd8542e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d3753dd8542e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d3753dd8542e--------------------------------)
    [Almog Baku](https://medium.com/@almogbaku?source=post_page---byline--d3753dd8542e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@almogbaku?source=post_page---byline--d3753dd8542e--------------------------------)[![Almog
    Baku](../Images/3ac36986f6ca0ba56c8edced6ec7dd07.png)](https://medium.com/@almogbaku?source=post_page---byline--d3753dd8542e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d3753dd8542e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d3753dd8542e--------------------------------)
    [Almog Baku](https://medium.com/@almogbaku?source=post_page---byline--d3753dd8542e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d3753dd8542e--------------------------------)
    ·16 min read·Jul 16, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d3753dd8542e--------------------------------)
    ·阅读时间16分钟·2024年7月16日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Large Language Models (LLMs) hold immense potential, but developing reliable
    production-grade applications remains challenging. After building dozens of LLM
    systems, I’ve distilled the formula for success into 3+1 fundamental principles
    that any team can apply.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）具有巨大的潜力，但开发可靠的生产级应用程序仍然具有挑战性。经过构建数十个LLM系统后，我将成功的公式提炼为3+1个基本原则，任何团队都可以应用。
- en: “LLM-Native apps are 10% sophisticated model, and 90% experimenting data-driven
    engineering work.”
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “LLM本地应用程序是10%的复杂模型，和90%的实验性数据驱动的工程工作。”
- en: Building production-ready LLM applications requires *careful engineering practices*.
    When users cannot interact *directly* with the LLM, the prompt must be meticulously
    composed to cover all nuances, as *iterative user feedback may be unavailable*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 构建生产就绪的LLM应用程序需要*细致的工程实践*。当用户无法*直接*与LLM互动时，提示语必须精心编写，以涵盖所有细节，因为*迭代的用户反馈可能不可用*。
- en: Introducing the LLM Triangle Principles
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍LLM三角原则
- en: The LLM Triangle Principles encapsulate the essential guidelines for building
    effective LLM-native apps. They provide a solid conceptual framework, guide developers
    in constructing robust and reliable LLM-native applications, and offer direction
    and support.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: LLM三角原则概括了构建有效LLM本地应用程序的基本指南。它们提供了一个坚实的概念框架，引导开发人员构建健壮且可靠的LLM本地应用程序，并提供方向和支持。
- en: '![](../Images/6a29d388a95ba8611ab9ea94a2a9665d.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a29d388a95ba8611ab9ea94a2a9665d.png)'
- en: An optimal LLM Usage is achieved by optimizing the three prominent principles
    through the lens of the SOP. (Image by author)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 通过SOP的视角来优化三个突出原则，可以实现最佳的LLM使用。（图片来源：作者）
- en: The Key Apices
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键顶点
- en: The LLM Triangle Principles introduces four programming principles to help you
    design and build LLM-Native apps.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: LLM三角原则介绍了四个编程原则，帮助您设计和构建LLM本地应用程序。
- en: 'The first principle is the *Standard Operating Procedure (****SOP****).* The
    SOP guides the three apices of our triangle: ***Model***, ***Engineering Techniques*,**
    and ***Contextual Data***.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个原则是*标准操作程序（****SOP****）*。SOP引导着我们三角形的三个顶点：***模型***、***工程技术***和***上下文数据***。
- en: Optimizing the three apices principles *through the lens* of the **SOP is the
    key** **to ensuring a high-performing** LLM-native app.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通过**SOP的视角**来优化三个顶点的原则是**确保高性能**LLM本地应用的关键。
- en: 1\. Standard Operating Procedure (SOP)
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1. 标准操作程序（SOP）
- en: '[**S**tandard **O**perating **P**rocedure (SOP)](https://en.wikipedia.org/wiki/Standard_operating_procedure)
    is a well-known terminology in the industrial world. It’s a set of step-by-step
    instructions compiled by large organizations to help their workers carry out routine
    operations while maintaining high-quality and similar results each time. This
    practically turns inexperienced or low-skilled workers into experts by writing
    detailed instructions.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[**标准操作程序（SOP）**](https://en.wikipedia.org/wiki/Standard_operating_procedure)是工业界的一个著名术语。它是一套由大组织编写的逐步指令，帮助工人完成日常操作，并确保每次都能保持高质量和相似的结果。这实际上通过编写详细的指令，将没有经验或技术不熟练的工人转变为专家。'
- en: The LLM Triangle Principles borrow the SOP paradigm and encourage you to **consider
    the model as an inexperienced/unskilled worker**. We can ensure higher-quality
    results by “teaching” the model how an expert would perform this task.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: LLM三角原则借鉴了SOP范式，并鼓励你**将模型视为一个经验不足/技术不熟练的工作人员**。通过“教导”模型专家如何完成任务，我们可以确保更高质量的结果。
- en: '![](../Images/3c52e5439c5a35d0aa068834dbcf132b.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c52e5439c5a35d0aa068834dbcf132b.png)'
- en: The SOP ***guiding*** principle. (image by author)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: SOP ***指导***原理。（图片来源：作者）
- en: “Without an SOP, even the most powerful LLM will fail to deliver consistently
    high-quality results.”
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “没有SOP，即使是最强大的LLM也无法持续交付高质量的结果。”
- en: When thinking about the **SOP *guiding* principle**, we should identify what
    techniques will help us implement the SOP most effectively.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在思考**SOP *指导*原理**时，我们应该识别哪些技术将帮助我们最有效地实施SOP。
- en: 1.1\. Cognitive modeling
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1. 认知建模
- en: To create an SOP, we need to take our best-performing workers (domain experts),
    model how they think and work to achieve the same results, and write down everything
    they do.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个SOP，我们需要选出表现最好的工人（领域专家），建模他们如何思考和工作以实现相同的结果，并将他们所做的每一步都记录下来。
- en: After editing and formalizing it, we’ll have detailed instructions to help every
    inexperienced or low-skilled worker succeed and yield excellent work.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在编辑和正式化后，我们将拥有详细的指令，帮助每一位经验不足或技能较低的工人取得成功，并产出卓越的工作成果。
- en: Like humans, it’s essential to *reduce the cognitive load* of the task by simplifying
    or splitting it. Following a simple step-by-step instruction is more straightforward
    than a lengthy, complex procedure.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 像人类一样，*通过简化或分解任务来减轻认知负担*是至关重要的。遵循简单的逐步指令比繁琐复杂的程序更直接。
- en: During this process, we identify the hidden [*implicit cognition*](https://en.wikipedia.org/wiki/Implicit_cognition)
    *“jumps”* — the small, unconscious steps experts take that significantly impact
    the outcome. These subtle, unconscious, often unspoken assumptions or decisions
    can substantially affect the final result.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，我们识别出隐藏的[*隐性认知*](https://en.wikipedia.org/wiki/Implicit_cognition) *“跳跃”*
    —— 专家在无意识中采取的微小步骤，这些步骤会显著影响结果。这些细微的、无意识的、通常未言明的假设或决策，可能会大大影响最终结果。
- en: '![](../Images/6e8abc91b025fdeea3cfe961d6f6111e.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6e8abc91b025fdeea3cfe961d6f6111e.png)'
- en: An example of an “implicit cognition jump.” (Image by author)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一个“隐性认知跳跃”的示例。（图片来源：作者）
- en: 'For example, let’s say we want to model an SQL analyst. We’ll start by interviewing
    them and ask them a few questions, such as:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们想要建立一个SQL分析师的模型。我们将通过面谈他们并询问几个问题，例如：
- en: What do you do when you are asked to analyze a business problem?
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你被要求分析一个商业问题时，你会怎么做？
- en: How do you make sure your solution meets the request?
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你如何确保你的解决方案满足需求？
- en: <reflecting the process as we understand to the interviewee>
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <将我们理解的过程反馈给面试对象>
- en: Does this accurately capture your process? <getting corrections>
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这准确地反映了你的流程吗？ <获取修正>
- en: Etc.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等等。
- en: '![](../Images/4f537cd288b3ad06ed31c92d586b5af4.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f537cd288b3ad06ed31c92d586b5af4.png)'
- en: An example of the cognitive process that the analyst does and how to model it.
    (Image by author)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 分析师进行的认知过程的一个示例，以及如何建模它。（图片来源：作者）
- en: The implicit cognition process takes many shapes and forms; a typical example
    is a “domain-specific definition.” For example, “bestseller” might be a prominent
    term for our domain expert, but not for everyone else.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 隐性认知过程有许多表现形式；一个典型的例子是“领域特定定义”。例如，“畅销书”可能是我们领域专家的一个显著术语，但对其他人来说却不一定如此。
- en: '![](../Images/66542e24426dcd8ad82a02786a45e3fc.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66542e24426dcd8ad82a02786a45e3fc.png)'
- en: Expanding the implicit cognition process in our SQL analyst example. (Image
    by author)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展我们SQL分析师示例中的隐性认知过程。（图片来源：作者）
- en: Eventually, we’ll have a full SOP “recipe” that allows us to emulate our top-performing
    analyst.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们将拥有一个完整的SOP“配方”，使我们能够模仿我们顶尖的分析员。
- en: When mapping out these complex processes, it can be helpful to visualize them
    as a graph. This is especially helpful when the process is nuanced and involves
    many steps, conditions, and splits.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在绘制这些复杂的过程时，将它们可视化为图形可能会非常有帮助。当过程非常细致，并且涉及许多步骤、条件和分支时，这尤其有用。
- en: '![](../Images/d292b6c118ddeba744f0a0967b1a39d6.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d292b6c118ddeba744f0a0967b1a39d6.png)'
- en: The “SQL Analyst SOP” includes all the required technical steps, visualized
    as a graph. (Image by author)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: “SQL分析员SOP”包括所有必要的技术步骤，以图形方式呈现。（图片由作者提供）
- en: Our final solution should mimic the steps defined in the SOP. In this stage,
    try to ignore the implementation—later, you can implement it across one or many
    steps/chains throughout our solution.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最终解决方案应模仿SOP中定义的步骤。在此阶段，尽量忽略实现细节——稍后，你可以在我们的解决方案中的一个或多个步骤/链条中实现它。
- en: Unlike the rest of the principles, the cognitive modeling (SOP writing) is the
    *only standalone process*. It’s highly recommended that you model your process
    before writing code. That being said, while implementing it, you might go back
    and change it based on new insights or understandings you gained.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他原则不同，认知建模（SOP编写）是*唯一独立的过程*。强烈建议在编写代码之前先对你的过程进行建模。话虽如此，在实施过程中，你可能会根据获得的新见解或理解，回头对其进行修改。
- en: Now that we understand the importance of creating a well-defined SOP, that guides
    our *business understanding* of the problem, let’s explore how we can effectively
    implement it using various engineering techniques.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了制定清晰SOP的重要性，它指导了我们对问题的*业务理解*，接下来让我们探索如何利用各种工程技巧有效地实现它。
- en: 2\. Engineering Techniques
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 工程技巧
- en: '[Engineering Techniques](https://www.promptingguide.ai/) help you practically
    implement your SOP and get the most out of the model. When thinking about the
    **Engineering Techniques principle**, we should consider what tools(techniques)
    in our toolbox can help us implement and shape our SOP and assist the model in
    communicating well with us.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[工程技巧](https://www.promptingguide.ai/)帮助你实际实施SOP并从模型中获得最大收益。在思考**工程技巧原则**时，我们应该考虑哪些工具（技巧）能帮助我们实施和塑造SOP，并协助模型与我们有效沟通。'
- en: '![](../Images/d6a821a075378b6d2e1cdd1d807de0fd.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6a821a075378b6d2e1cdd1d807de0fd.png)'
- en: The Engineering Techniques principle. (Image by author)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 工程技巧原则。（图片由作者提供）
- en: Some engineering techniques are only implemented in the prompt layer, while
    many require a software layer to be effective, and some combine both layers.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一些工程技巧仅在提示层中实现，而许多技巧则需要在软件层中才能生效，另外一些则结合了这两者。
- en: '![](../Images/ddc3397969ad3a8ea0111ff3c09cae12.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ddc3397969ad3a8ea0111ff3c09cae12.png)'
- en: Engineering Techniques Layers. (Image by author)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 工程技巧层。（图片由作者提供）
- en: 'While many small nuances and techniques are discovered daily, I’ll cover two
    primary techniques: workflow/chains and agents.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然每天都会发现许多小细节和技巧，但我将涵盖两种主要技巧：工作流/链条和代理。
- en: 2.1\. LLM-Native architectures (aka flow engineering or chains)
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1\. LLM原生架构（也叫流工程或链条）
- en: The LLM-Native Architecture describes the agentic flow your app is going through
    to yield the task’s result.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: LLM原生架构描述了你的应用程序执行任务结果的代理流。
- en: Each step in our flow is a standalone process that must occur to achieve our
    task. Some steps will be performed simply by deterministic code; for some, we
    will use an LLM (agent).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们流程中的每个步骤都是一个独立的过程，必须执行才能实现我们的任务。一些步骤将仅通过确定性代码执行；对于某些步骤，我们将使用LLM（代理）。
- en: 'To do that, we can reflect on the Standard Operating Procedure (SOP) we drew
    and think:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们可以反思我们绘制的标准操作程序（SOP），并思考：
- en: Which SOP steps should we glue together to the same agent? And what steps should
    we split as different agents?
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应该将哪些SOP步骤组合到同一个代理中？又应该将哪些步骤拆分为不同的代理？
- en: What SOP steps should be executed in a standalone manner (but they might be
    fed with information from previous steps)?
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪些SOP步骤应该独立执行（但它们可能会接收来自前一步骤的信息）？
- en: What SOP steps can we perform in a deterministic code?
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以在确定性代码中执行哪些SOP步骤？
- en: Etc.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等等。
- en: '![](../Images/45e4b1ee6d0a0a2ace59885dbeedee4e.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/45e4b1ee6d0a0a2ace59885dbeedee4e.png)'
- en: An LLM-Native Architecture example for “Wikipedia writer” based on a given SOP.
    (Image by author)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 基于给定SOP的“维基百科写作员”LLM原生架构示例。（图片由作者提供）
- en: 'Before navigating to the next step in our architecture/graph, we should define
    its key properties:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入我们架构/图表的下一步之前，我们应该定义其关键属性：
- en: '**Inputs and outputs** — What is the signature of this step? What is required
    before we can take an action? (this can also serve as an output format for an
    agent)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入和输出** — 这一步的签名是什么？我们在采取行动之前需要什么？（这也可以作为代理的输出格式）'
- en: '**Quality assurances—**What makes the response “good enough”? Are there cases
    that require human intervention in the loop? What kinds of assertions can we configure?'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**质量保证**— 什么样的响应才算是“足够好”？是否有需要人工干预的情况？我们可以配置什么样的断言？'
- en: '**Autonomous level** — How much control do we need over the result’s quality?
    What range of use cases can this stage handle? In other words, how much can we
    trust the model to work independently at this point?'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自主级别** — 我们需要对结果质量的控制有多大？这一阶段能处理什么范围的用例？换句话说，在这一点上，我们能多大程度上信任模型独立工作？'
- en: '**Triggers** — What is the next step? What defines the next step?'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**触发器** — 下一步是什么？是什么决定了下一步？'
- en: '**Non-functional** — What’s the required latency? Do we need special business
    monitoring here?'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非功能性** — 需要什么样的延迟？我们是否需要在这里进行特别的业务监控？'
- en: '**Failover control** — What kind of failures(systematic and agentic) can occur?
    What are our fallbacks?'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**故障转移控制** — 可能发生什么样的故障（系统性和代理性）？我们的备选方案是什么？'
- en: '**State management** — Do we need a special state management mechanism? How
    do we retrieve/save states (define the indexing key)? Do we need persistence storage?
    What are the different usages of this state(e.g., cache, logging, etc.)?'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态管理** — 我们需要特殊的状态管理机制吗？我们如何检索/保存状态（定义索引键）？我们需要持久存储吗？这个状态有哪些不同的使用场景（例如，缓存、日志记录等）？'
- en: Etc.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等等。
- en: 2.2\. What are agents?
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2\. 什么是代理？
- en: An LLM agent is a standalone component of an LLM-Native architecture that involves
    calling an LLM.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 代理是 LLM-Native 架构中的独立组件，涉及调用 LLM。
- en: It’s an instance of LLM usage with the prompt containing the context. Not all
    agents are equal — Some will use “tools,” some won’t; some might be used “just
    once” in the flow, while others can be called recursively or multiple times, carrying
    the previous input and outputs.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个包含上下文的 LLM 使用实例。并非所有代理都是相同的——有些会使用“工具”，有些则不会；有些可能只在流程中“使用一次”，而其他的则可以递归调用或多次调用，携带之前的输入和输出。
- en: 2.2.1\. Agents with tools
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2.1\. 带工具的代理
- en: Some LLM agents can use “tools” — predefined functions for tasks like calculations
    or web searches. The agent outputs instructions specifying the tool and input,
    which the application executes, returning the result to the agent.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 一些 LLM 代理可以使用“工具”——用于执行任务如计算或网页搜索的预定义函数。代理输出指定工具和输入的指令，应用程序执行这些指令，并将结果返回给代理。
- en: 'To understand the concept, let’s look at a simple prompt implementation for
    tool calling. This can work even with models not natively trained to call tools:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这个概念，我们来看看一个简单的提示实现，看看如何调用工具。即使是没有原生训练来调用工具的模型也能这样工作：
- en: '[PRE0]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: It’s important to distinguish between agents with tools (hence *autonomous agents*)
    and agents whose output can lead to performing an action.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 区分带工具的代理（因此是*自主代理*）和能够导致执行动作的代理非常重要。
- en: “Autonomous agents are agents that have the ability to generate a way to accomplish
    the task.”
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “自主代理是具有生成完成任务方式能力的代理。”
- en: Autonomous agents are *given the right* to **decide** if they should act and
    with what action. In contrast, a (nonautonomous) agent simply “processes” our
    request(e.g., classification), and based on this process, our deterministic code
    performs an action, and the model has zero control over that.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 自主代理被*赋予权力*来**决定**是否采取行动以及采取什么行动。相比之下，（非自主）代理仅仅“处理”我们的请求（例如，分类），并基于这一过程，我们的确定性代码执行动作，模型对此没有任何控制。
- en: '![](../Images/f0e9e53e6f52e3df3279712dc420d130.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f0e9e53e6f52e3df3279712dc420d130.png)'
- en: An autonomous agent VS agent that triggers an action. (Image by author)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 自主代理与触发动作的代理。（图片来自作者）
- en: As we increase the agent’s autonomy in planning and executing tasks, we enhance
    its decision-making capabilities but potentially reduce control over output quality.
    Although this might look like a magical solution to make it more “smart” or “advanced,”
    it comes with the cost of losing control over the quality.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们提高代理在规划和执行任务中的自主性，我们增强了其决策能力，但可能会降低对输出质量的控制。虽然这看起来像是一种神奇的解决方案，使其变得更加“智能”或“先进”，但这也意味着我们将失去对质量的控制。
- en: '![](../Images/6f127e24bc282742d3b4399a1ba9ba57.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f127e24bc282742d3b4399a1ba9ba57.png)'
- en: The tradeoffs of an autonomous agent. (Image by author)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 自主代理的权衡。（图片来自作者）
- en: Beware the allure of fully autonomous agents. While their architecture might
    look appealing and simpler, using it for everything (or as the initial PoC) might
    be very deceiving from the “real production” cases. Autonomous agents are hard
    to debug and unpredictable(response with unstable quality), which makes them unusable
    for production.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 警惕完全自主代理的诱惑。尽管它们的架构看起来可能很有吸引力且更简单，但将其用于一切（或作为初始的PoC）可能会从“真实生产”案例中产生误导。自主代理很难调试且不可预测（响应质量不稳定），这使得它们无法用于生产。
- en: Currently, agents (without implicit guidance) are not very good at planning
    complex processes and usually skip essential steps. For example, in our “Wikipedia
    writer” use-case, they’ll just start writing and skip the systematic process.
    This makes agents (and autonomous agents especially) only as good as the model,
    or more accurately — only as good as the data they were trained on relative to
    your task.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，代理（没有隐式引导）在规划复杂流程方面并不擅长，通常会跳过关键步骤。例如，在我们的“维基百科写作”用例中，它们会直接开始写作，跳过系统化的过程。这使得代理（特别是自主代理）仅能像模型一样好，或者更准确地说——仅能像它们相对于你的任务所训练的的数据一样好。
- en: Instead of giving the agent (or a swarm of agents) the liberty to do everything
    end-to-end, try to hedge their task to a specific region of your flow/SOP that
    requires this kind of agility or creativity. This can yield higher-quality results
    because you can enjoy both worlds.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 与其让代理（或一群代理）自由地做所有的端到端任务，不如尝试将它们的任务限定在你的流程/SOP的特定区域，这些区域需要这种灵活性或创造性。这可以产生更高质量的结果，因为你可以享受两者的优点。
- en: 'An excellent example is [AlphaCodium](https://www.codium.ai/blog/alphacodium-state-of-the-art-code-generation-for-code-contests/):
    By combining a structured flow with different agents (including a novel agent
    that iteratively writes and tests code), they increased GPT-4 accuracy (pass@5)
    on CodeContests from 19% to 44%.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的例子是[AlphaCodium](https://www.codium.ai/blog/alphacodium-state-of-the-art-code-generation-for-code-contests/)：通过将结构化流程与不同的代理（包括一个迭代编写和测试代码的新型代理）结合，他们将GPT-4在CodeContests上的准确率（pass@5）从19%提高到44%。
- en: '![](../Images/e388535058a331dbe448aefe597b45f5.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e388535058a331dbe448aefe597b45f5.png)'
- en: AlphaCodium’s LLM Architecture. (Image by the curtesy [Codium.ai](https://www.codium.ai/))
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaCodium的LLM架构。（图片由[Codium.ai](https://www.codium.ai/)提供）
- en: 'While engineering techniques lay the groundwork for implementing our SOP and
    optimizing LLM-native applications, we must also carefully consider another critical
    component of the LLM Triangle: the model itself.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然工程技术为实施我们的SOP和优化LLM原生应用奠定了基础，但我们还必须仔细考虑LLM三角形中的另一个关键组成部分：模型本身。
- en: 3\. Model
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 模型
- en: The model we choose is a critical component of our project’s success—a large
    one (such as GPT-4 or Claude Opus) might yield better results but be quite costly
    at scale, while a smaller model might be less “smart” but help with the budget.
    When thinking about the **Model principle**, we should aim to identify our constraints
    and goals and what kind of model can help us fulfill them.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择的模型是项目成功的关键组成部分——一个大型模型（如GPT-4或Claude Opus）可能会产生更好的结果，但在大规模应用时成本非常高，而一个较小的模型可能不那么“智能”，但可以帮助节省预算。在思考**模型原则**时，我们应当确定我们的约束和目标，并找出哪种类型的模型可以帮助我们实现这些目标。
- en: '![](../Images/b2141352ce97abf24254c35db07b7d46.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2141352ce97abf24254c35db07b7d46.png)'
- en: The Model principle. (Image by author)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 模型原则。（图片由作者提供）
- en: “Not all LLMs are created equal. Match the model to the mission.”
  id: totrans-102
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “并非所有LLM都是平等的。将模型与任务匹配。”
- en: The truth is that we don’t always need the largest model; it depends on the
    task. To find the right match, we must have an [experimental process](/building-llm-apps-a-clear-step-by-step-guide-1fe1e6ef60fd)
    and try multiple variations of our solution.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 事实是，我们并不总是需要最大的模型；这取决于任务。为了找到合适的匹配，我们必须有一个[实验过程](/building-llm-apps-a-clear-step-by-step-guide-1fe1e6ef60fd)，并尝试多种变体的解决方案。
- en: It helps to look at our “inexperienced worker” analogy — a very “smart” worker
    with many academic credentials probably will succeed in some tasks easily. Still,
    they might be overqualified for the job, and hiring a “cheaper” candidate will
    be much more cost-effective.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 借鉴我们的“经验不足的工人”类比——一个非常“聪明”的工人拥有许多学术资质，可能会轻松完成某些任务，但他们可能对这份工作来说资历过高，雇佣一个“更便宜”的候选人将更加具有成本效益。
- en: 'When considering a model, we should define and compare solutions based on the
    tradeoffs we are willing to take:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑一个模型时，我们应根据愿意承受的权衡来定义和比较解决方案：
- en: '**Task Complexity** — Simpler tasks (such as summarization) are easier to complete
    with smaller models, while reasoning usually requires larger models.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务复杂性** — 较简单的任务（如摘要生成）可以用较小的模型更容易完成，而推理通常需要更大的模型。'
- en: '**Inference infrastructure** — Should it run on the cloud or edge devices?
    The model size might impact a small phone, but it can be tolerated for cloud-serving.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推理基础设施** — 它应该在云端运行还是在边缘设备上运行？模型的大小可能对小型手机产生影响，但对于云端服务而言可以容忍。'
- en: '**Pricing** — What price can we tolerate? Is it cost-effective considering
    the business impact and predicated usage?'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定价** — 我们能够接受什么价格？从商业影响和预测使用角度来看，这是否具有成本效益？'
- en: '**Latency** — As the model grows larger, the latency grows as well.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**延迟** — 随着模型的增大，延迟也会增加。'
- en: '**Labeled data** — Do we have data we can use immediately to enrich the model
    with examples or relevant information that is not trained upon?'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标注数据** — 我们是否有可以立即使用的数据，用来通过示例或相关信息丰富模型，而这些数据并未经过训练？'
- en: In many cases, until you have the “in-house expertise,” it helps to pay a little
    extra for an experienced worker — the same applies to LLMs.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，直到你拥有“内部专业知识”，聘请一个有经验的工作人员可能会有助益 —— 对LLM也是如此。
- en: If you don’t have *labeled data*, start with a stronger (larger) model, *collect
    data*, and then utilize it to empower a model using a few-shot or fine-tuning.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有*标注数据*，可以从一个更强（更大的）模型开始，*收集数据*，然后利用它通过少量样本或微调来赋能模型。
- en: 3.1\. Fine-tuning a model
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1\. 微调模型
- en: 'There are a few aspects that you must consider before resorting to fine-tune
    a model:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在对模型进行微调之前，有几个方面是必须考虑的：
- en: '**Privacy** — Your data might include pieces of private information that must
    be kept from the model. You must anonymize your data to avoid legal liabilities
    if your data contains private information.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐私** — 你的数据可能包含必须从模型中隔离的私人信息。如果数据中包含私人信息，你必须对数据进行匿名化，以避免法律责任。'
- en: '**Laws, Compliance, and Data Rights** — Some legal questions can be raised
    when training a model. For example, the OpenAI terms-of-use policy prevents you
    from training a model without OpenAI using generated responses. Another typical
    example is complying with the GDPR’s laws, which require a “right for revocation,”
    where a user can require the company to remove information from the system. This
    raises legal questions about whether the model should be retrained or not.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**法律、合规性与数据权益** — 在训练模型时可能会出现一些法律问题。例如，OpenAI的使用条款政策禁止你在没有OpenAI的情况下训练一个模型，使用生成的响应。另一个典型的例子是遵守GDPR法律，要求提供“撤销权”，用户可以要求公司从系统中删除信息。这就引发了关于是否需要重新训练模型的法律问题。'
- en: '**Updating latency —** The latency or data cutoff is much higher when training
    a model. Unlike embedding the new information via the context (see “4\. Contextual
    Data” section below), which provides immediate latency, training the model is
    a long process that takes time. Due to that, models are retrained less often.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更新延迟** — 训练模型时，延迟或数据截止时间要高得多。与通过上下文嵌入新信息（见下文“4. 上下文数据”部分）提供即时延迟不同，训练模型是一个需要时间的长期过程。因此，模型的重新训练频率较低。'
- en: '**Development and operation —** Implementing a reproducible, scalable, and
    monitored fine-tuning pipeline is essential while continuously evaluating the
    results’ performance. This complex process requires constant maintenance.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开发与运维** — 实施一个可重复、可扩展且可监控的微调管道至关重要，同时需要持续评估结果的性能。这个复杂的过程需要不断的维护。'
- en: '**Cost —** Retraining is considered expensive due to its complexity and the
    highly intensive resources(GPUs) required per training.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本** — 由于重新训练的复杂性和每次训练所需的大量资源（如GPU），重新训练被认为是昂贵的。'
- en: The ability of LLMs to act as *in-context learners* and the fact that the newer
    models support a much larger context window simplify our implementation dramatically
    and can provide excellent results even without fine-tuning. Due to the complexity
    of fine-tuning, using it as a last resort or skipping it entirely is recommended.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的能力，作为*上下文学习者*，以及新模型支持更大上下文窗口的事实，极大简化了我们的实现，并且即使没有微调也能提供优秀的结果。由于微调的复杂性，建议将其作为最后的手段，或完全跳过。
- en: Conversely, fine-tuning models for specific tasks (e.g., structured JSON output)
    or domain-specific language can be highly efficient. A small, task-specific model
    can be highly effective and much cheaper in inference than large LLMs. Choose
    your solution wisely, and assess all the relevant considerations before escalating
    to LLM training.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，为特定任务（例如，结构化 JSON 输出）或特定领域语言对模型进行微调可能会非常高效。一个小型的、任务特定的模型可能非常有效，并且在推理时比大型
    LLM 更便宜。明智地选择你的解决方案，并在决定进行 LLM 训练之前，评估所有相关的考虑因素。
- en: “Even the most powerful model requires relevant and well-structured contextual
    data to shine.”
  id: totrans-122
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “即使是最强大的模型，也需要相关且结构良好的上下文数据才能发挥作用。”
- en: 4\. Contextual Data
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4. 上下文数据
- en: '***LLMs are in-context learners.*** That means that by providing task-specific
    information, the LLM agent can help us to perform it *without* special training
    or fine-tuning. This enables us to “teach” new knowledge or skills easily. When
    thinking about the **Contextual Data principle**, we should aim to organize and
    model the available data and how to compose it within our prompt.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '***LLMs 是上下文学习者。*** 这意味着通过提供特定任务的信息，LLM 代理可以帮助我们执行任务，*无需*特殊的训练或微调。这使得我们能够轻松地“教授”新知识或技能。在考虑**上下文数据原则**时，我们应该目标是组织和建模可用数据，并在提示中如何组合这些数据。'
- en: '![](../Images/879503f2a13a6e3cc811e6bb8ba4ecc5.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/879503f2a13a6e3cc811e6bb8ba4ecc5.png)'
- en: The Contextual Data principle. (Image by author)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文数据原则。（图片由作者提供）
- en: 'To compose our context, we include the relevant (contextual) information within
    the prompt we send to the LLM. There are two kinds of contexts we can use:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建我们的上下文，我们将相关的（上下文）信息包含在发送给 LLM 的提示中。我们可以使用两种类型的上下文：
- en: '**Embedded contexts** — embedded information pieces provided as part of the
    prompt.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**嵌入式上下文** — 作为提示的一部分提供的嵌入信息'
- en: '[PRE1]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Attachment contexts** — A list of information pieces glues by the beginning/end
    of the prompt'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**附件上下文** — 一组由提示开始/结束连接的信息'
- en: '[PRE2]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Contexts are usually implemented using a “prompt template” (such as [jinja2](https://jinja.palletsprojects.com/en/3.1.x/)
    or [mustache](https://mustache.github.io/) or simply native [formatting literal
    strings](https://docs.python.org/3/reference/lexical_analysis.html#formatted-string-literals));
    this way, we can compose them elegantly while keeping the essence of our prompt:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文通常使用“提示模板”来实现（例如，[jinja2](https://jinja.palletsprojects.com/en/3.1.x/) 或
    [mustache](https://mustache.github.io/) 或简单的原生[格式化字面量字符串](https://docs.python.org/3/reference/lexical_analysis.html#formatted-string-literals)）；这样，我们可以优雅地组合它们，同时保持提示的核心内容：
- en: '[PRE3]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 4.1\. Few-shot learning
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1. 少量学习
- en: Few-shot learning is a powerful way to “teach” LLMs by example without requiring
    extensive fine-tuning. Providing a few representative examples in the prompt can
    guide the model in understanding the desired format, style, or task.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 少量学习是一种强大的方式，通过示例“教” LLM，而无需进行大量微调。提供一些具有代表性的示例可以指导模型理解期望的格式、风格或任务。
- en: For instance, if we want the LLM to generate email responses, we could include
    a few examples of well-written responses in the prompt. This helps the model learn
    the preferred structure and tone.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们希望大型语言模型（LLM）生成电子邮件回复，可以在提示中包含一些写得很好的回复示例。这有助于模型学习首选的结构和语气。
- en: We can use diverse examples to help the model catch different corner cases or
    nuances and learn from them. Therefore, it’s essential to include a variety of
    examples that cover a range of scenarios your application might encounter.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用多样化的示例，帮助模型捕捉到不同的边界情况或细微差别，并从中学习。因此，包含涵盖应用程序可能遇到的各种场景的多种示例是至关重要的。
- en: As your application grows, you may consider implementing “[Dynamic few-shot](https://arxiv.org/abs/1804.09458),”
    which involves programmatically selecting the most relevant examples for each
    input. While it increases your implementation complexity, it ensures the model
    receives the most appropriate guidance for each case, significantly improving
    performance across a wide range of tasks without costly fine-tuning.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 随着应用程序的增长，您可能会考虑实现“[动态少量学习](https://arxiv.org/abs/1804.09458)”，该方法涉及程序化地为每个输入选择最相关的示例。虽然这增加了实现的复杂性，但它确保模型为每种情况提供最合适的指导，从而显著提高了在广泛任务中的性能，而无需昂贵的微调。
- en: 4.2\. Retrieval Augmented Generation
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2. 检索增强生成
- en: '[Retrieval Augmented Generation (RAG)](https://www.promptingguide.ai/techniques/rag)
    is a technique for retrieving relevant documents for additional context before
    generating a response. It’s like giving the LLM a quick peek at specific reference
    material to help inform its answer. This keeps responses current and factual without
    needing to retrain the model.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[检索增强生成（RAG）](https://www.promptingguide.ai/techniques/rag)是一种在生成响应之前检索相关文档以提供额外上下文的技术。它就像是给LLM一个快速浏览特定参考材料的机会，以帮助它形成答案。这使得响应保持最新且准确，而无需重新训练模型。'
- en: For instance, on a support chatbot application, RAG could pull relevant help-desk
    wiki pages to inform the LLM’s answers.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在支持聊天应用中，RAG可以提取相关的帮助台Wiki页面来帮助LLM回答问题。
- en: This approach helps LLMs *stay current* and *reduces hallucinations* by grounding
    responses in retrieved facts. RAG is particularly handy for tasks that require
    updated or specialized knowledge without retraining the entire model.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法通过将回答建立在检索到的事实基础上，帮助LLM *保持最新* 并 *减少幻觉*。RAG对于需要更新或专业知识的任务特别有用，而不需要重新训练整个模型。
- en: For example, suppose we are building a support chat for our product. In that
    case, we can use RAG to retrieve a relevant document from our helpdesk wiki, then
    provide it to an LLM agent and ask it to compose an answer based on the question
    and provide a document.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们正在为我们的产品构建支持聊天。在这种情况下，我们可以使用RAG从帮助台Wiki中检索相关文档，然后将其提供给LLM代理，并要求它基于问题生成回答并提供文档。
- en: 'There are three key pieces to look at while implementing RAG:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现RAG时，有三个关键点需要关注：
- en: '**Retrieval mechanism** — While the traditional implementation of RAG involves
    retrieving a relevant document using a vector similarity search, sometimes it’s
    better or cheaper to use simpler methods such as keyword-based search (like [BM-25](https://en.wikipedia.org/wiki/Okapi_BM25)).'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检索机制**—传统的RAG实现方式是通过向量相似度搜索来检索相关文档，但有时使用更简单的方法，如基于关键词的搜索（如[BM-25](https://en.wikipedia.org/wiki/Okapi_BM25)），可能更好或更便宜。'
- en: '**Indexed data structure —**Indexing the entire document naively, without preprocessing,
    may limit the effectiveness of the retrieval process. Sometimes, we want to add
    a data preparation step, such as preparing a list of questions and answers based
    on the document.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**索引数据结构—**天真地索引整个文档，而不进行预处理，可能会限制检索过程的效果。有时，我们需要添加一个数据准备步骤，例如基于文档准备一份问题和答案的列表。'
- en: '**Metadata—**Storing relevant metadata allows for more efficient referencing
    and filtering of information (e.g., narrowing down wiki pages to only those related
    to the user’s specific product inquiry). This extra data layer streamlines the
    retrieval process.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**元数据—**存储相关的元数据可以更高效地引用和筛选信息（例如，将Wiki页面缩小到仅与用户特定产品查询相关的页面）。这层额外的数据结构可以简化检索过程。'
- en: 4.3\. Providing relevant context
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3\. 提供相关上下文
- en: The context information relevant to your agent can vary. Although it may seem
    beneficial, providing the model (like the “unskilled worker”) with too much information
    can be overwhelming and irrelevant to the task. Theoretically, this causes the
    model to learn irrelevant information (or token connections), which can lead to
    confusion and [hallucinations](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 与您的代理相关的上下文信息可能有所不同。虽然看起来提供过多的信息可能有益，但如果给模型（比如“非熟练工人”）提供太多信息，可能会让其感到不知所措且与任务无关。从理论上讲，这会导致模型学习无关的信息（或令牌连接），从而可能导致混淆和[幻觉](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence))。
- en: When Gemini 1.5 was released and introduced as an LLM that could process up
    to 10M tokens, some practitioners questioned whether the context was still an
    issue. While it’s a fantastic accomplishment, especially for some use cases (such
    as chat with PDFs), it’s still limited, especially when reasoning over various
    documents.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 当Gemini 1.5发布并作为一个能够处理最多1000万个令牌的LLM时，一些从业者质疑上下文是否仍然是一个问题。虽然这是一个了不起的成就，特别是对于某些用例（例如与PDF聊天），但它仍然有限，特别是在推理多个文档时。
- en: Compacting the prompt and providing the LLM agent with only relevant information
    is crucial. This reduces the processing power the model invests in irrelevant
    tokens, improves the quality, optimizes the latency, and reduces the cost.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩提示并仅向LLM代理提供相关信息至关重要。这可以减少模型在无关令牌上的处理能力，提升质量，优化延迟，并降低成本。
- en: There are many tricks to improve the relevancy of the provided context, most
    of which relate to how you store and catalog your data.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多技巧可以提高提供上下文的相关性，其中大多数与如何存储和分类数据有关。
- en: For RAG applications, it’s handy to add a data preparation that shapes the information
    you store (e.g., questions and answers based on the document, then providing the
    LLM agent only with the answer; this way, the agent gets a summarized and shorter
    context), and use re-ranking algorithms on top of the retrieved documents to refine
    the results.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 对于RAG应用程序，增加一种数据准备方法是非常有用的，它能塑造您存储的信息（例如，基于文档的问答，然后仅向LLM代理提供答案；这样，代理获得的是一个总结的、较短的上下文），并在检索到的文档上使用重新排序算法来细化结果。
- en: “Data fuels the engine of LLM-native applications. A strategic design of contextual
    data unlocks their true potential.”
  id: totrans-154
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “数据为LLM原生应用程序提供动力。上下文数据的战略性设计释放了它们的真正潜力。”
- en: Conclusion and Implications
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论与启示
- en: The LLM Triangle Principles provide a structured approach to developing high-quality
    LLM-native applications, addressing the gap between LLMs’ enormous potential and
    real-world implementation challenges. Developers can create more reliable and
    effective LLM-powered solutions by focusing on 3+1 key principles—the **Model**,
    **Engineering Techniques**, and **Contextual Data**—all guided by a well-defined
    **SOP**.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: LLM三角原则提供了一种结构化的方法，用于开发高质量的LLM原生应用程序，解决了LLM巨大潜力与实际应用挑战之间的差距。开发人员可以通过关注3+1个关键原则——**模型**、**工程技术**和**上下文数据**——并由明确定义的**SOP**指导，来创建更可靠和有效的LLM驱动解决方案。
- en: '![](../Images/6559fd3272a486ef7e1bd7e6edd445e5.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6559fd3272a486ef7e1bd7e6edd445e5.png)'
- en: The LLM Triangle Principles. (Image by author)
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: LLM三角原则。（图片由作者提供）
- en: Key takeaways
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键要点
- en: '**Start with a clear SOP**: Model your expert’s cognitive process to create
    a step-by-step guide for your LLM application. Use it as a guide while thinking
    of the other principles.'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**从明确的SOP开始**：模拟专家的认知过程，为您的LLM应用程序创建一个逐步指南。在考虑其他原则时，使用它作为指导。'
- en: '**Choose the right model**: Balance capabilities with cost, and consider starting
    with larger models before potentially moving to smaller, fine-tuned ones.'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择合适的模型**：平衡能力与成本，并考虑先使用更大的模型，再可能转向较小的、微调过的模型。'
- en: '**Leverage engineering techniques**: Implement LLM-native architectures and
    use agents strategically to optimize performance and maintain control. Experiment
    with different prompt techniques to find the most effective prompt for your case.'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**利用工程技术**：实现LLM原生架构，并战略性地使用代理来优化性能和保持控制。尝试不同的提示技术，找到最适合您案例的提示。'
- en: '**Provide relevant context**: Use in-context learning, including RAG, when
    appropriate, but be cautious of overwhelming the model with irrelevant information.'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**提供相关上下文**：在适当时使用上下文学习，包括RAG，但要小心不要用无关的信息让模型感到过载。'
- en: '**Iterate and experiment**: Finding the right solution often requires testing
    and refining your work. I recommend reading and implementing the [“Building LLM
    Apps: A Clear Step-By-Step Guide”](/building-llm-apps-a-clear-step-by-step-guide-1fe1e6ef60fd)
    tips for a detailed LLM-Native development process guide.'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**迭代和实验**：找到正确的解决方案通常需要测试和完善您的工作。我建议阅读并实施[《构建LLM应用程序：清晰的逐步指南》](/building-llm-apps-a-clear-step-by-step-guide-1fe1e6ef60fd)，以获得详细的LLM原生开发过程指南。'
- en: By applying the LLM Triangle Principles, organizations can move beyond a simple
    proof-of-concept and develop robust, production-ready LLM applications that truly
    harness the power of this transformative technology.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用LLM三角原则，组织可以超越简单的概念验证，开发出强大、可投入生产的LLM应用程序，真正发挥这一变革性技术的力量。
- en: If you find this whitepaper helpful, please give it a few **claps** 👏 on Medium
    and **share** it with your fellow AI enthusiasts. Your support means the world
    to me! 🌍
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您觉得这篇白皮书对您有帮助，请在Medium上给它一些**掌声** 👏，并**分享**给您的AI爱好者朋友们。您的支持对我意义重大！🌍
- en: Let’s keep the conversation going — feel free to reach out via [email](mailto:almog.baku@gmail.com)
    or [connect on LinkedIn](https://www.linkedin.com/in/almogbaku/) 🤝
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续交流——随时通过[电子邮件](mailto:almog.baku@gmail.com)或[在LinkedIn上联系](https://www.linkedin.com/in/almogbaku/)
    🤝
- en: Special thanks to [Gal Peretz](https://medium.com/u/532f8dc01db8?source=post_page---user_mention--d3753dd8542e--------------------------------),
    [Gad Benram](https://medium.com/u/b45fa95a7293?source=post_page---user_mention--d3753dd8542e--------------------------------),
    [Liron Izhaki Allerhand](https://medium.com/u/251cd1007ce8?source=post_page---user_mention--d3753dd8542e--------------------------------),
    [Itamar Friedman](https://medium.com/u/bcd07dca5f93?source=post_page---user_mention--d3753dd8542e--------------------------------),
    [Lee Twito](https://medium.com/u/241b56ab4bf?source=post_page---user_mention--d3753dd8542e--------------------------------),
    [Ofir Ziv](https://medium.com/u/2db20f6d91e8?source=post_page---user_mention--d3753dd8542e--------------------------------),
    [Philip Tannor](https://medium.com/u/5c5d2a69bcdb?source=post_page---user_mention--d3753dd8542e--------------------------------),
    [Yair Livne](https://medium.com/u/6f8924605cf6?source=post_page---user_mention--d3753dd8542e--------------------------------)
    and [Shai Alon](https://medium.com/u/fa1f5e83a0c8?source=post_page---user_mention--d3753dd8542e--------------------------------)
    for insights, feedback, and editing notes.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 特别感谢[Gal Peretz](https://medium.com/u/532f8dc01db8?source=post_page---user_mention--d3753dd8542e--------------------------------)、[Gad
    Benram](https://medium.com/u/b45fa95a7293?source=post_page---user_mention--d3753dd8542e--------------------------------)、[Liron
    Izhaki Allerhand](https://medium.com/u/251cd1007ce8?source=post_page---user_mention--d3753dd8542e--------------------------------)、[Itamar
    Friedman](https://medium.com/u/bcd07dca5f93?source=post_page---user_mention--d3753dd8542e--------------------------------)、[Lee
    Twito](https://medium.com/u/241b56ab4bf?source=post_page---user_mention--d3753dd8542e--------------------------------)、[Ofir
    Ziv](https://medium.com/u/2db20f6d91e8?source=post_page---user_mention--d3753dd8542e--------------------------------)、[Philip
    Tannor](https://medium.com/u/5c5d2a69bcdb?source=post_page---user_mention--d3753dd8542e--------------------------------)、[Yair
    Livne](https://medium.com/u/6f8924605cf6?source=post_page---user_mention--d3753dd8542e--------------------------------)和[Shai
    Alon](https://medium.com/u/fa1f5e83a0c8?source=post_page---user_mention--d3753dd8542e--------------------------------)提供的见解、反馈和编辑意见。
