- en: A Crash Course of Planning for Perception Engineers in Autonomous Driving
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动驾驶感知工程师规划速成课程
- en: 原文：[https://towardsdatascience.com/a-crash-course-of-planning-for-perception-engineers-in-autonomous-driving-ede324d78717?source=collection_archive---------1-----------------------#2024-06-30](https://towardsdatascience.com/a-crash-course-of-planning-for-perception-engineers-in-autonomous-driving-ede324d78717?source=collection_archive---------1-----------------------#2024-06-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-crash-course-of-planning-for-perception-engineers-in-autonomous-driving-ede324d78717?source=collection_archive---------1-----------------------#2024-06-30](https://towardsdatascience.com/a-crash-course-of-planning-for-perception-engineers-in-autonomous-driving-ede324d78717?source=collection_archive---------1-----------------------#2024-06-30)
- en: The fundamentals of planning and decision-making
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 规划与决策的基础
- en: '[](https://medium.com/@patrickllgc?source=post_page---byline--ede324d78717--------------------------------)[![Patrick
    Langechuan Liu](../Images/fecbf85146a9bde21e6b2251538ddd65.png)](https://medium.com/@patrickllgc?source=post_page---byline--ede324d78717--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ede324d78717--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ede324d78717--------------------------------)
    [Patrick Langechuan Liu](https://medium.com/@patrickllgc?source=post_page---byline--ede324d78717--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@patrickllgc?source=post_page---byline--ede324d78717--------------------------------)[![Patrick
    Langechuan Liu](../Images/fecbf85146a9bde21e6b2251538ddd65.png)](https://medium.com/@patrickllgc?source=post_page---byline--ede324d78717--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ede324d78717--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ede324d78717--------------------------------)
    [Patrick Langechuan Liu](https://medium.com/@patrickllgc?source=post_page---byline--ede324d78717--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ede324d78717--------------------------------)
    ·44 min read·Jun 30, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ede324d78717--------------------------------)
    ·44分钟阅读·2024年6月30日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/5098127e65432ebcafff9731d7e5c451.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5098127e65432ebcafff9731d7e5c451.png)'
- en: AlphaGo, ChatGPT and FSD (image credit [Elena Popova](https://unsplash.com/@elenapopova?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash),
    [Karthik Sridasyam](https://unsplash.com/@karthik1324?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    and [Jonathan Kemper](https://unsplash.com/@jupp?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/a-computer-screen-with-a-text-description-on-it-5yuRImxKOcU?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaGo、ChatGPT 和 FSD（图片来源：[Elena Popova](https://unsplash.com/@elenapopova?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)、[Karthik
    Sridasyam](https://unsplash.com/@karthik1324?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    和 [Jonathan Kemper](https://unsplash.com/@jupp?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    于[Unsplash](https://unsplash.com/photos/a-computer-screen-with-a-text-description-on-it-5yuRImxKOcU?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
- en: A classical modular autonomous driving system typically consists of perception,
    prediction, planning, and control. Until around 2023, AI (artificial intelligence)
    or ML (machine learning) primarily enhanced **perception** in most mass-production
    autonomous driving systems, with its influence diminishing in downstream components.
    In stark contrast to the low integration of AI in the planning stack, end-to-end
    perception systems (such as the [BEV, or birds-eye-view perception pipeline](https://medium.com/towards-data-science/monocular-bev-perception-with-transformers-in-autonomous-driving-c41e4a893944?sk=b7498a375cf92533e8242b1837f29af4))
    have been deployed in [mass production vehicles](https://medium.com/towards-data-science/bev-perception-in-mass-production-autonomous-driving-c6e3f1e46ae0?sk=8963783161435815fa1b0957fd325d39).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的模块化自动驾驶系统通常由感知、预测、规划和控制组成。直到大约2023年，AI（人工智能）或ML（机器学习）主要增强了大多数量产自动驾驶系统中的**感知**功能，其在下游组件中的影响逐渐减弱。与规划堆栈中AI集成较低形成鲜明对比的是，端到端感知系统（例如[BEV或鸟瞰图感知管道](https://medium.com/towards-data-science/monocular-bev-perception-with-transformers-in-autonomous-driving-c41e4a893944?sk=b7498a375cf92533e8242b1837f29af4)）已经在[量产汽车](https://medium.com/towards-data-science/bev-perception-in-mass-production-autonomous-driving-c6e3f1e46ae0?sk=8963783161435815fa1b0957fd325d39)中部署。
- en: '![](../Images/4f4a37398d69465caf9d19fc89e145ad.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f4a37398d69465caf9d19fc89e145ad.png)'
- en: Classical modular design of an autonomous driving stack, 2023 and prior (Chart
    created by author)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年及之前的经典模块化自动驾驶堆栈设计（图表由作者制作）
- en: There are multiple reasons for this. A classical stack based on a human-crafted
    framework is more explainable and can be iterated faster to fix field test issues
    (within hours) compared to machine learning-driven features (which could take
    days or weeks). However, it does not make sense to let readily available human
    driving data sit idle. Moreover, increasing computing power is more scalable than
    expanding the engineering team.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这是有多个原因的。基于人工构建框架的经典栈比基于机器学习的特征更具可解释性，并且能够更快地进行迭代，以解决现场测试中的问题（几个小时内解决），而基于机器学习的特征可能需要几天或几周的时间才能完成。然而，让现成的人工驾驶数据闲置是没有意义的。此外，增加计算能力比扩展工程团队更具可扩展性。
- en: Fortunately, there has been a strong trend in both academia and industry to
    change this situation. First, downstream modules are becoming increasingly data-driven
    and may also be integrated via different interfaces, such as the one proposed
    in [CVPR 2023’s best paper, UniAD](https://arxiv.org/abs/2212.10156). Moreover,
    driven by the ever-growing wave of Generative AI, a single unified vision-language-action
    (VLA) model shows great potential for handling complex robotics tasks ([RT-2](https://robotics-transformer2.github.io/assets/rt2.pdf)
    in academia, TeslaBot and 1X in industry) and autonomous driving ([GAIA-1](https://arxiv.org/abs/2309.17080),
    [DriveVLM](https://arxiv.org/abs/2402.12289) in academia, and Wayve AI driver,
    Tesla FSD in industry). This brings the toolsets of AI and data-driven development
    from the perception stack to the planning stack.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，学术界和工业界都有一个强烈的趋势，正在改变这种情况。首先，下游模块正变得越来越数据驱动，并且可能通过不同的接口进行集成，比如在[CVPR 2023最佳论文UniAD](https://arxiv.org/abs/2212.10156)中提出的接口。此外，受到日益增长的生成式AI浪潮的推动，一个单一的统一视觉-语言-动作（VLA）模型在处理复杂的机器人任务中显示出巨大的潜力（学术界的[RT-2](https://robotics-transformer2.github.io/assets/rt2.pdf)，工业界的TeslaBot和1X）以及自动驾驶（学术界的[GAIA-1](https://arxiv.org/abs/2309.17080)、[DriveVLM](https://arxiv.org/abs/2402.12289)，工业界的Wayve
    AI driver和Tesla FSD）。这将AI和数据驱动开发的工具集从感知层带入了规划层。
- en: This blog post aims to introduce the problem settings, existing methodologies,
    and challenges of the planning stack, in the form of a crash course for perception
    engineers. As a perception engineer, I finally had some time over the past couple
    of weeks to systematically learn the classical planning stack, and I would like
    to share what I learned. I will also share my thoughts on how AI can help from
    the perspective of an AI practitioner.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇博客文章旨在介绍规划栈的问题设置、现有方法以及挑战，以感知工程师的速成课程形式进行呈现。作为一名感知工程师，过去几周我终于有时间系统地学习经典的规划栈，并且我想分享我所学到的内容。我还会从AI实践者的角度分享我的思考，探讨AI如何提供帮助。
- en: The intended audience for this post is AI practitioners who work in the field
    of autonomous driving, in particular, perception engineers.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的目标读者是从事自动驾驶领域的AI实践者，特别是感知工程师。
- en: The article is a bit long (11100 words), and the table of contents below will
    most likely help those who want to do quick ctrl+F searches with the keywords.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章有点长（11100字），下面的目录将最有可能帮助那些想要快速使用关键词进行ctrl+F搜索的人。
- en: The Chinese version is available on [Zhihu知乎](https://zhuanlan.zhihu.com/p/706193528).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 中文版本可以在[知乎知乎](https://zhuanlan.zhihu.com/p/706193528)上找到。
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Why learn planning?
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么要学习规划？
- en: 'This brings us to an interesting question: why learn planning, especially the
    classical stack, in the era of AI?'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了一个有趣的问题：为什么在AI时代，尤其是经典的规划栈值得学习？
- en: From a problem-solving perspective, understanding your customers’ challenges
    better will enable you, as a perception engineer, to serve your downstream customers
    more effectively, even if your main focus remains on perception work.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从问题解决的角度来看，更好地理解客户的挑战，将使你作为一名感知工程师能够更有效地为下游客户服务，即使你的主要工作仍然集中在感知领域。
- en: Machine learning is a tool, not a solution. The most efficient way to solve
    problems is to combine new tools with domain knowledge, especially those with
    solid mathematical formulations. Domain knowledge-inspired learning methods are
    likely to be more data-efficient. As planning transitions from rule-based to ML-based
    systems, even with early prototypes and products of end-to-end systems hitting
    the road, there is a need for engineers who can deeply understand both the fundamentals
    of planning and machine learning. Despite these changes, classical and learning
    methods will likely continue to coexist for a considerable period, perhaps shifting
    from an 8:2 to a 2:8 ratio. It is almost essential for engineers working in this
    field to understand both worlds.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是一个工具，而不是一个解决方案。解决问题的最有效方法是将新工具与领域知识结合起来，特别是那些具有扎实数学公式的领域知识。受领域知识启发的学习方法可能会更具数据效率。随着规划从基于规则的系统向基于机器学习的系统过渡，即使端到端系统的早期原型和产品已经上路，仍然需要能够深入理解规划和机器学习基础的工程师。尽管发生了这些变化，传统方法和学习方法可能会在相当长的一段时间内继续共存，比例可能会从8:2转变为2:8。对于在这一领域工作的工程师来说，理解这两个世界几乎是必不可少的。
- en: From a value-driven development perspective, understanding the limitations of
    classical methods is crucial. This insight allows you to effectively utilize new
    ML tools to design a system that addresses current issues and delivers immediate
    impact.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 从价值驱动的开发角度来看，理解传统方法的局限性至关重要。这个洞察力使你能够有效地利用新的机器学习工具来设计一个解决当前问题并带来即时影响的系统。
- en: Additionally, planning is a critical part of all autonomous agents, not just
    in autonomous driving. Understanding what planning is and how it works will enable
    more ML talents to work on this exciting topic and contribute to the development
    of truly autonomous agents, whether they are cars or other forms of automation.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，规划是所有自主智能体中的关键部分，不仅仅是自动驾驶。理解什么是规划以及它如何运作，将使更多的机器学习人才参与到这一激动人心的话题中，推动真正自主智能体的发展，无论是汽车还是其他形式的自动化。
- en: What is planning?
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是规划？
- en: The problem formulation
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题表述
- en: As the “brain” of autonomous vehicles, the planning system is crucial for the
    safe and efficient driving of vehicles. The goal of the planner is to generate
    trajectories that are safe, comfortable, and efficiently progressing towards the
    goal. In other words, safety, comfort, and efficiency are the three key objectives
    for planning.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 作为自动驾驶车辆的“大脑”，规划系统对于车辆的安全高效驾驶至关重要。规划者的目标是生成既安全、舒适，又能高效推进到目标的轨迹。换句话说，安全、舒适和效率是规划的三个关键目标。
- en: As input to the planning systems, all perception outputs are required, including
    static road structures, dynamic road agents, free space generated by occupancy
    networks, and traffic wait conditions. The planning system must also ensure vehicle
    comfort by monitoring acceleration and jerk for smooth trajectories, while considering
    interaction and traffic courtesy.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 作为规划系统的输入，所有感知输出都是必需的，包括静态路面结构、动态道路代理、占据网络生成的空闲空间和交通等待情况。规划系统还必须通过监控加速度和加速度变化来确保车辆的舒适性，以实现平滑的轨迹，同时考虑互动和交通礼让。
- en: The planning systems generate trajectories in the format of a sequence of waypoints
    for the ego vehicle’s low-level controller to track. Specifically, these waypoints
    represent the future positions of the ego vehicle at a series of fixed time stamps.
    For example, each point might be 0.4 seconds apart, covering an 8-second planning
    horizon, resulting in a total of 20 waypoints.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 规划系统生成的轨迹是以一系列航路点的形式表示的，供自车的低层控制器跟踪。具体来说，这些航路点代表了自车在一系列固定时间戳下的未来位置。例如，每个航路点可能相隔0.4秒，覆盖8秒的规划范围，总共生成20个航路点。
- en: A classical planning stack roughly consists of global route planning, local
    behavior planning, and local trajectory planning. Global route planning provides
    a road-level path from the start point to the end point on a global map. Local
    behavior planning decides on a semantic driving action type (e.g., car following,
    nudging, side passing, yielding, and overtaking) for the next several seconds.
    Based on the decided behavior type from the behavior planning module, local trajectory
    planning generates a short-term trajectory. The global route planning is typically
    provided by a map service once navigation is set and is beyond the scope of this
    post. We will focus on behavior planning and trajectory planning from now on.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一个经典的规划堆栈大致由全局路线规划、局部行为规划和局部轨迹规划组成。全局路线规划提供从起点到终点的路面级路径，该路径基于全局地图。局部行为规划决定接下来几秒钟内的语义驾驶动作类型（例如，跟车、轻推、侧方超车、让行和超车）。基于行为规划模块决定的行为类型，局部轨迹规划生成短期轨迹。全局路线规划通常由地图服务提供，一旦设置了导航，它就不在本文讨论的范围内。接下来，我们将重点讨论行为规划和轨迹规划。
- en: Behavior planning and trajectory generation can work explicitly in tandem or
    be combined into a single process. In explicit methods, behavior planning and
    trajectory generation are distinct processes operating within a hierarchical framework,
    working at different frequencies, with behavior planning at 1–5 Hz and trajectory
    planning at 10–20 Hz. Despite being highly efficient most of the time, adapting
    to different scenarios may require significant modifications and fine-tuning.
    More advanced planning systems combine the two into a single optimization problem.
    This approach ensures feasibility and optimality without any compromise.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 行为规划和轨迹生成可以明确地协同工作，也可以合并成一个单一过程。在显式方法中，行为规划和轨迹生成是两个不同的过程，它们在一个层次结构框架内运行，工作频率不同，其中行为规划的频率为1–5
    Hz，轨迹规划的频率为10–20 Hz。尽管大多数时候效率很高，但适应不同场景可能需要进行重大修改和微调。更高级的规划系统将两者合并成一个单一的优化问题。这种方法确保了可行性和最优性，不会做出任何妥协。
- en: '![](../Images/84bae81967ad3e62c3fd2330dc51aa15.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84bae81967ad3e62c3fd2330dc51aa15.png)'
- en: 'Classification of planning design approaches (source: [Fluid Dynamics Planner](https://arxiv.org/abs/2406.05708))'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 规划设计方法的分类（来源：[Fluid Dynamics Planner](https://arxiv.org/abs/2406.05708))
- en: The Glossary of Planning
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 规划术语表
- en: You might have noticed that the terminology used in the above section and the
    image do not completely match. There is no standard terminology that everyone
    uses. Across both academia and industry, it is not uncommon for engineers to use
    different names to refer to the same concept and the same name to refer to different
    concepts. This indicates that planning in autonomous driving is still under active
    development and has not fully converged.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，上述部分中使用的术语与图像并不完全匹配。没有统一的标准术语被广泛采用。在学术界和工业界，工程师使用不同的名称来指代同一概念，或用相同的名称指代不同的概念，这并不罕见。这表明自动驾驶中的规划仍在积极发展中，并未完全统一。
- en: Here, I list the notation used in this post and briefly explain other notions
    present in the literature.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我列出了本文中使用的符号，并简要解释了文献中出现的其他概念。
- en: 'Planning: A top-level concept, parallel to control, that generates trajectory
    waypoints. Together, planning and control are jointly referred to as PnC (planning
    and control).'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规划：一个顶级概念，与控制平行，用于生成轨迹的关键点。规划与控制一起被统称为PnC（规划与控制）。
- en: 'Control: A top-level concept that takes in trajectory waypoints and generates
    high-frequency steering, throttle, and brake commands for actuators to execute.
    Control is relatively well-established compared to other areas and is beyond the
    scope of this post, despite the common notion of PnC.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制：一个顶级概念，接收轨迹关键点并生成高频率的转向、油门和刹车命令，供执行器执行。与其他领域相比，控制相对成熟，尽管PnC的常见观点存在，但它超出了本文的范围。
- en: 'Prediction: A top-level concept that predicts the future trajectories of traffic
    agents other than the ego vehicle. Prediction can be considered a lightweight
    planner for other agents and is also called motion prediction.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测：一个顶级概念，用于预测除自车外的交通参与者的未来轨迹。预测可以看作是其他交通参与者的轻量级规划器，也称为运动预测。
- en: 'Behavior Planning: A module that produces high-level semantic actions (e.g.,
    lane change, overtake) and typically generates a coarse trajectory. It is also
    known as task planning or decision making, particularly in the context of interactions.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行为规划：一个生成高级语义动作（如车道变换、超车）的模块，通常会生成粗略的轨迹。它也被称为任务规划或决策制定，特别是在交互的上下文中。
- en: 'Motion Planning: A module that takes in semantic actions and produces smooth,
    feasible trajectory waypoints for the duration of the planning horizon for control
    to execute. It is also referred to as trajectory planning.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运动规划：一个模块，接收语义动作，并生成平滑、可行的轨迹航点，供控制在规划时域内执行。它也被称为轨迹规划。
- en: 'Trajectory Planning: Another term for motion planning.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 轨迹规划：运动规划的另一种说法。
- en: 'Decision Making: Behavior planning with a focus on interactions. Without ego-agent
    interaction, it is simply referred to as behavior planning. It is also known as
    tactical decision making.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策制定：专注于交互的行为规划。如果没有自车与其他主体的交互，则仅称为行为规划。它也被称为战术决策制定。
- en: 'Route Planning: Finds the preferred route over road networks, also known as
    mission planning.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 路径规划：在道路网络上找到首选路线，也称为任务规划。
- en: 'Model-Based Approach: In planning, this refers to manually crafted frameworks
    used in the classical planning stack, as opposed to neural network models. Model-based
    methods contrast with learning-based methods.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于模型的方法：在规划中，指的是手工构建的框架，这些框架用于经典的规划栈，与神经网络模型相对。基于模型的方法与基于学习的方法形成对比。
- en: 'Multimodality: In the context of planning, this typically refers to multiple
    intentions. This contrasts with multimodality in the context of multimodal sensor
    inputs to perception or multimodal large language models (such as VLM or VLA).'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多模态：在规划的上下文中，通常指的是多个意图。这与感知中的多模态传感器输入或多模态大型语言模型（如VLM或VLA）中的多模态有所不同。
- en: 'Reference Line: A local (several hundred meters) and coarse path based on global
    routing information and the current state of the ego vehicle.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考线：基于全局路线信息和自车当前状态的局部（几百米范围内）且粗略的路径。
- en: 'Frenet Coordinates: A coordinate system based on a reference line. Frenet simplifies
    a curvy path in Cartesian coordinates to a straight tunnel model. See below for
    a more detailed introduction.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弗涅坐标：基于参考线的坐标系统。弗涅坐标将笛卡尔坐标系中的曲线路径简化为一条直线隧道模型。以下会有更详细的介绍。
- en: 'Trajectory: A 3D spatiotemporal curve, in the form of (x, y, t) in Cartesian
    coordinates or (s, l, t) in Frenet coordinates. A trajectory is composed of both
    path and speed.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 轨迹：三维时空曲线，表示为笛卡尔坐标中的（x, y, t）或弗涅坐标中的（s, l, t）。轨迹由路径和速度组成。
- en: 'Path: A 2D spatial curve, in the form of (x, y) in Cartesian coordinates or
    (s, l) in Frenet coordinates.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 路径：二维空间曲线，表示为笛卡尔坐标中的（x, y）或弗涅坐标中的（s, l）。
- en: 'Semantic Action: A high-level abstraction of action (e.g., car following, nudge,
    side pass, yield, overtake) with clear human intention. Also referred to as intention,
    policy, maneuver, or primitive motion.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义动作：一种高级抽象的动作（如跟车、轻推、侧超车、让行、超车），具有明确的人类意图。也被称为意图、策略、机动或原始运动。
- en: 'Action: A term with no fixed meaning. It can refer to the output of control
    (high-frequency steering, throttle, and brake commands for actuators to execute)
    or the output of planning (trajectory waypoints). Semantic action refers to the
    output of behavior prediction.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动作：一个没有固定含义的术语。它可以指代控制的输出（执行器所执行的高频率转向、油门和刹车指令），也可以指代规划的输出（轨迹的航点）。语义动作指的是行为预测的输出。
- en: 'Different literature may use various notations and concepts. Here are some
    examples:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的文献可能使用不同的符号和概念。以下是一些示例：
- en: 'Decision Making System: Sometimes includes planning and control as well. (source:
    [A Survey of Motion Planning and Control Techniques for Self-driving Urban Vehicles](https://arxiv.org/pdf/1604.07446),
    and [BEVGPT](https://arxiv.org/abs/2310.10357))'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策系统：有时也包括规划和控制。（来源：[自动驾驶城市车辆运动规划与控制技术综述](https://arxiv.org/pdf/1604.07446)，以及[BEVGPT](https://arxiv.org/abs/2310.10357)）
- en: 'Motion Planning: Sometimes is the top-level planning concept and includes behavior
    planning and trajectory planning. (source: [Towards A General-Purpose Motion Planning
    for Autonomous Vehicles Using Fluid Dynamics](https://arxiv.org/abs/2406.05708)).'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运动规划：有时是顶层的规划概念，包含行为规划和轨迹规划。（来源：[基于流体动力学的通用运动规划方法应用于自动驾驶车辆](https://arxiv.org/abs/2406.05708)）
- en: 'Planning: Sometimes includes behavior planning, motion planning, and also route
    planning.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规划：有时包括行为规划、运动规划以及路线规划。
- en: These variations illustrate the diversity in terminology and the evolving nature
    of the field.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这些变化展示了术语的多样性以及该领域不断发展的特点。
- en: Behavior Planning
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 行为规划
- en: As a machine learning engineer, you may notice that the behavior planning module
    is a heavily manually crafted intermediate module. There is no consensus on the
    exact form and content of its output. Concretely, the output of behavior planning
    can be a reference path or object labeling on ego maneuvers (e.g., pass from the
    left or right-hand side, pass or yield). The term “semantic action” has no strict
    definition and no fixed methods.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一名机器学习工程师，你可能会注意到行为规划模块是一个 heavily 依赖人工设计的中间模块。对于其输出的具体形式和内容并没有共识。具体来说，行为规划的输出可以是参考路径或在自我机动过程中对目标物体的标注（例如，从左侧或右侧超车，超车或让行）。术语“语义动作”并没有严格的定义，也没有固定的方法。
- en: The decoupling of behavior planning and motion planning increases efficiency
    in solving the extremely high-dimensional action space of autonomous vehicles.
    The actions of an autonomous vehicle need to be reasoned at typically 10 Hz or
    more (time resolution in waypoints), and most of these actions are relatively
    straightforward, like going straight. After decoupling, the behavior planning
    layer only needs to reason about future scenarios at a relatively coarse resolution,
    while the motion planning layer operates in the local solution space based on
    the decision made by behavior planning. Another benefit of behavior planning is
    converting non-convex optimization to convex optimization, which we will discuss
    further below.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 行为规划与运动规划的解耦提高了在解决自动驾驶车辆极高维度动作空间时的效率。自动驾驶车辆的行为需要在通常为10Hz或更高的频率下推理（时间分辨率以路径点为单位），且大多数动作相对简单，如直行。解耦后，行为规划层只需以相对粗糙的分辨率推理未来场景，而运动规划层则基于行为规划做出的决策，在局部解空间中进行操作。行为规划的另一个好处是将非凸优化问题转化为凸优化问题，下面我们将进一步讨论这一点。
- en: Frenet vs Cartesian systems
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 弗雷内坐标系与笛卡尔坐标系的对比
- en: The Frenet coordinate system is a widely adopted system that merits its own
    introduction section. The Frenet frame simplifies trajectory planning by independently
    managing lateral and longitudinal movements relative to a reference path. The
    sss coordinate represents longitudinal displacement (distance along the road),
    while the lll (or ddd) coordinate represents lateral displacement (side position
    relative to the reference path).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 弗雷内坐标系是一种广泛采用的坐标系统，值得单独设立一个介绍部分。弗雷内框架通过独立地管理相对于参考路径的横向和纵向运动，简化了轨迹规划。sss坐标表示纵向位移（沿道路的距离），而lll（或ddd）坐标表示横向位移（相对于参考路径的侧向位置）。
- en: Frenet simplifies a curvy path in Cartesian coordinates to a straight tunnel
    model. This transformation converts non-linear road boundary constraints on curvy
    roads into linear ones, significantly simplifying the subsequent optimization
    problems. Additionally, humans perceive longitudinal and lateral movements differently,
    and the Frenet frame allows for separate and more flexible optimization of these
    movements.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 弗雷内坐标系将笛卡尔坐标系中的曲线路径简化为一个直线隧道模型。这种转换将曲线路段上的非线性道路边界约束转化为线性约束，从而显著简化后续的优化问题。此外，人类对纵向和横向运动的感知不同，弗雷内坐标系允许对这些运动进行独立且更灵活的优化。
- en: '![](../Images/557bbcb9cf482212a57762342fd2058c.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/557bbcb9cf482212a57762342fd2058c.png)'
- en: 'Schematics on the conversion from Cartesian frame to Frenet frame (source:
    [Cartesian Planner](https://ieeexplore.ieee.org/document/9703250))'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从笛卡尔坐标系到弗雷内坐标系的转换示意图（来源：[笛卡尔规划器](https://ieeexplore.ieee.org/document/9703250)）
- en: The Frenet coordinate system requires a clean, structured road graph with low
    curvature lanes. In practice, it is preferred for structured roads with small
    curvature, such as highways or city expressways. However, the issues with the
    Frenet coordinate system are amplified with increasing reference line curvature,
    so it should be used cautiously on structured roads with high curvature, like
    city intersections with guide lines.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 弗雷内坐标系需要清晰、结构化的道路图，且道路曲率较小。实际应用中，更适合用于曲率较小的结构化道路，如高速公路或城市快速路。然而，当参考线的曲率增加时，弗雷内坐标系的问题会被放大，因此在曲率较大的结构化道路上，如带导向线的城市交叉口，使用时应谨慎。
- en: For unstructured roads, such as ports, mining areas, parking lots, or intersections
    without guidelines, the more flexible Cartesian coordinate system is recommended.
    The Cartesian system is better suited for these environments because it can handle
    higher curvature and less structured scenarios more effectively.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于没有结构的道路，如港口、矿区、停车场或没有指引线的交叉口，推荐使用更灵活的笛卡尔坐标系统。笛卡尔系统更适合这些环境，因为它能更有效地处理更高的曲率和更少结构化的场景。
- en: Classical tools — the troika of planning
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 经典工具——规划的三驾马车
- en: Planning in autonomous driving involves computing a trajectory from an initial
    high-dimensional state (including position, time, velocity, acceleration, and
    jerk) to a target subspace, ensuring all constraints are satisfied. Searching,
    sampling, and optimization are the three most widely used tools for planning.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶中的规划涉及从初始的高维状态（包括位置、时间、速度、加速度和震动）计算一条轨迹到目标子空间，确保满足所有约束条件。搜索、采样和优化是规划中最广泛使用的三种工具。
- en: Searching
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 搜索
- en: Classical graph-search methods are popular in planning and are used in route/mission
    planning on structured roads or directly in motion planning to find the best path
    in unstructured environments (such as parking or urban intersections, especially
    mapless scenarios). There is a clear evolution path, from Dijkstra’s algorithm
    to A* (A-star), and further to hybrid A*.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的图搜索方法在规划中很受欢迎，并且广泛应用于结构化道路上的路线/任务规划，或直接用于无结构环境（如停车场或城市交叉口，尤其是无地图场景）的运动规划，寻找最佳路径。其发展路径明确，从
    Dijkstra 算法到 A*（A-star），再到混合 A*。
- en: Dijkstra’s algorithm explores all possible paths to find the shortest one, making
    it a blind (uninformed) search algorithm. It is a systematic method that guarantees
    the optimal path, but it is inefficient to deploy. As shown in the chart below,
    it explores almost all directions. Essentially, Dijkstra’s algorithm is a breadth-first
    search (BFS) weighted by movement costs. To improve efficiency, we can use information
    about the location of the target to trim down the search space.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Dijkstra 算法通过探索所有可能的路径来找到最短路径，使其成为一种盲目（无信息）的搜索算法。它是一种系统的方法，保证最优路径，但部署时效率较低。如下面的图表所示，它几乎会探索所有方向。本质上，Dijkstra
    算法是加权的广度优先搜索（BFS）。为了提高效率，我们可以利用目标位置的信息来缩小搜索空间。
- en: '![](../Images/90d0ec4849efafff5dd561d12527be42.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90d0ec4849efafff5dd561d12527be42.png)'
- en: 'Visualization of Dijkstra’s algorithm and A-star search (Source: [PathFinding.js](https://qiao.github.io/PathFinding.js/visual/),
    example inspired by [RedBlobGames](https://www.redblobgames.com/pathfinding/a-star/introduction.html))'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Dijkstra 算法和 A* 搜索的可视化（来源：[PathFinding.js](https://qiao.github.io/PathFinding.js/visual/)，示例灵感来自于
    [RedBlobGames](https://www.redblobgames.com/pathfinding/a-star/introduction.html)）
- en: The A* algorithm uses heuristics to prioritize paths that appear to be leading
    closer to the goal, making it more efficient. It combines the cost so far (Dijkstra)
    with the cost to go (heuristics, essentially greedy best-first). A* only guarantees
    the shortest path if the heuristic is admissible and consistent. If the heuristic
    is poor, A* can perform worse than the Dijkstra baseline and may degenerate into
    a greedy best-first search.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: A* 算法使用启发式方法优先考虑看似更接近目标的路径，从而提高效率。它将当前的成本（Dijkstra）与到达目标的成本（启发式，本质上是贪婪的最佳优先搜索）结合起来。只有当启发式方法是可接受且一致的时，A*
    才能保证最短路径。如果启发式方法不佳，A* 的性能可能会比 Dijkstra 基准差，甚至可能退化为贪婪最佳优先搜索。
- en: In the specific application of autonomous driving, the hybrid A* algorithm further
    improves A* by considering vehicle kinematics. A* may not satisfy kinematic constraints
    and cannot be tracked accurately (e.g., the steering angle is typically within
    40 degrees). While A* operates in grid space for both state and action, hybrid
    A* separates them, maintaining the state in the grid but allowing continuous action
    according to kinematics.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动驾驶的具体应用中，混合 A* 算法通过考虑车辆运动学进一步改进了 A*。A* 可能无法满足运动学约束，也无法准确跟踪路径（例如，转向角度通常在 40
    度以内）。尽管 A* 在网格空间中对状态和动作都进行操作，混合 A* 将它们分开，保持状态在网格中，但根据运动学允许连续的动作。
- en: Analytical expansion (shot to goal) is another key innovation proposed by hybrid
    A*. A natural enhancement to A* is to connect the most recently explored nodes
    to the goal using a non-colliding straight line. If this is possible, we have
    found the solution. In hybrid A*, this straight line is replaced by Dubins and
    Reeds-Shepp (RS) curves, which comply with vehicle kinematics. This early stopping
    method strikes a balance between optimality and feasibility by focusing more on
    feasibility for the further side.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 解析扩展（射击到目标）是混合 A* 提出的另一个关键创新。A* 的一种自然增强方法是将最近探索的节点与目标连接，使用不碰撞的直线。如果可能的话，我们就找到了这个解。在混合
    A* 中，这条直线被替换为符合车辆动力学的杜宾曲线和里德-谢普（RS）曲线。这种提前停止方法通过更多关注远端的可行性，平衡了最优性和可行性。
- en: Hybrid A* is used heavily in parking scenarios and mapless urban intersections.
    Here is a very nice video showcasing how it works in a parking scenario.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 混合 A* 在停车场场景和无地图的城市交叉口中被广泛使用。这里有一个非常好的视频展示了它如何在停车场场景中工作。
- en: '![](../Images/3f58234e1f2851acba82d22fabb261e1.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f58234e1f2851acba82d22fabb261e1.png)'
- en: 'Hybrid A-star algorithm with analytical expansion (source: the [2010 IJRR Hybrid
    A-star paper](https://www.semanticscholar.org/paper/Path-Planning-for-Autonomous-Vehicles-in-Unknown-Dolgov-Thrun/0e8c927d9c2c46b87816a0f8b7b8b17ed1263e9c)
    and [2012 Udacity class](https://www.youtube.com/watch?v=qXZt-B7iUyw&ab_channel=Udacity)
    )'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 带有解析扩展的混合 A* 算法（来源：[2010 IJRR 混合 A* 论文](https://www.semanticscholar.org/paper/Path-Planning-for-Autonomous-Vehicles-in-Unknown-Dolgov-Thrun/0e8c927d9c2c46b87816a0f8b7b8b17ed1263e9c)
    和 [2012 Udacity 课程](https://www.youtube.com/watch?v=qXZt-B7iUyw&ab_channel=Udacity)）
- en: Sampling
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 采样
- en: Another popular method of planning is sampling. The well-known Monte Carlo method
    is a random sampling method. In essence, sampling involves selecting many candidates
    randomly or according to a prior, and then selecting the best one according to
    a defined cost. For sampling-based methods, the fast evaluation of many options
    is critical, as it directly impacts the real-time performance of the autonomous
    driving system.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个流行的规划方法是采样。著名的蒙特卡罗方法是一种随机采样方法。本质上，采样涉及随机或根据先验选择许多候选项，然后根据定义的成本选择最优的那个。对于基于采样的方法，快速评估许多选项至关重要，因为它直接影响自动驾驶系统的实时性能。
- en: Large Language Models (LLMs) essentially provide samples, and there needs to
    be an evaluator with a defined cost that aligns with human preferences. This evaluation
    process ensures that the selected output meets the desired criteria and quality
    standards. This process is basically the so-called alignment.
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）本质上提供样本，并且需要一个评估器，其定义的成本与人类偏好对齐。这个评估过程确保所选择的输出符合期望的标准和质量。这一过程基本上就是所谓的对齐。
- en: Sampling can occur in a parameterized solution space if we already know the
    analytical solution to a given problem or subproblem. For example, typically we
    want to minimize the time integral of the square of jerk (the third derivative
    of position p(t)), indicated by the triple dots over p, where one dot represents
    one order derivative with respect to time), among other criteria.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们已经知道给定问题或子问题的解析解，则可以在参数化的解空间中进行采样。例如，通常我们希望最小化冲击的平方的时间积分（位置 p(t) 的三阶导数，表示为
    p 上的三点符号，其中一个点代表相对于时间的一个阶导数），这是在其他标准下进行的。
- en: '![](../Images/e328957af9c4e4da4e980f475582b866.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e328957af9c4e4da4e980f475582b866.png)'
- en: 'Minimizing squared jerk for driving comfort (source: [Werling et al](https://www.semanticscholar.org/paper/Optimal-trajectory-generation-for-dynamic-street-in-Werling-Ziegler/6bda8fc13bda8cffb3bb426a73ce5c12cc0a1760),
    ICRA 2010)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化平方冲击以提高驾驶舒适性（来源：[Werling 等人](https://www.semanticscholar.org/paper/Optimal-trajectory-generation-for-dynamic-street-in-Werling-Ziegler/6bda8fc13bda8cffb3bb426a73ce5c12cc0a1760)，ICRA
    2010）
- en: It can be mathematically proven that quintic (5th order) polynomials provide
    the jerk-optimal connection between two states in a position-velocity-acceleration
    space, even when additional cost terms are considered. By sampling in this parameter
    space of quintic polynomials, we can find the one with the minimum cost to get
    the approximate solution. The cost takes into account factors such as speed, acceleration,
    jerk limit, and collision checks. This approach essentially solves the optimization
    problem through sampling.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过数学证明，五次（5阶）多项式在位置-速度-加速度空间中提供了两种状态之间的冲击最优连接，即使在考虑额外的成本项时也是如此。通过在这个五次多项式的参数空间中进行采样，我们可以找到一个具有最小成本的解，从而获得近似解。成本考虑了诸如速度、加速度、冲击限制和碰撞检查等因素。这种方法本质上通过采样来解决优化问题。
- en: '![](../Images/22c50c86d7b5b1fd2a459d17fafece5e.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22c50c86d7b5b1fd2a459d17fafece5e.png)'
- en: 'Sampling of lateral movement time profiles (source: [Werling et al](https://www.semanticscholar.org/paper/Optimal-trajectory-generation-for-dynamic-street-in-Werling-Ziegler/6bda8fc13bda8cffb3bb426a73ce5c12cc0a1760),
    ICRA 2010)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 横向运动时间配置文件的采样（来源：[Werling等人](https://www.semanticscholar.org/paper/Optimal-trajectory-generation-for-dynamic-street-in-Werling-Ziegler/6bda8fc13bda8cffb3bb426a73ce5c12cc0a1760)，ICRA
    2010）
- en: Sampling-based methods have inspired numerous ML papers, including CoverNet,
    Lift-Splat-Shoot, NMP, and MP3\. These methods replace mathematically sound quintic
    polynomials with human driving behavior, utilizing a large database. The evaluation
    of trajectories can be easily parallelized, which further supports the use of
    sampling-based methods. This approach effectively leverages a vast amount of expert
    demonstrations to mimic human-like driving behavior, while avoiding random sampling
    of acceleration and steering profiles.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 基于采样的方法启发了众多机器学习论文，包括CoverNet、Lift-Splat-Shoot、NMP和MP3。这些方法用大量数据库中的人类驾驶行为取代了数学上合理的五次多项式。轨迹的评估可以轻松并行化，这进一步支持了基于采样的方法。该方法有效地利用了大量专家示范，模仿了类似人类的驾驶行为，同时避免了对加速度和转向配置文件的随机采样。
- en: '![](../Images/98b82507bc9c5bf06c32603d8a6d7823.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/98b82507bc9c5bf06c32603d8a6d7823.png)'
- en: 'Sampling from human-driving data for data-driven planning methods (source:
    [NMP](http://www.cs.toronto.edu/~wenjie/papers/cvpr19/nmp.pdf), [CoverNet](https://arxiv.org/abs/1911.10298)
    and [Lift-splat-shoot](https://arxiv.org/abs/2008.05711))'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 从人类驾驶数据中进行采样以支持数据驱动的规划方法（来源：[NMP](http://www.cs.toronto.edu/~wenjie/papers/cvpr19/nmp.pdf)，[CoverNet](https://arxiv.org/abs/1911.10298)
    和 [Lift-splat-shoot](https://arxiv.org/abs/2008.05711)）
- en: Optimization
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化
- en: Optimization finds the best solution to a problem by maximizing or minimizing
    a specific objective function under given constraints. In neural network training,
    a similar principle is followed using gradient descent and backpropagation to
    adjust the network’s weights. However, in optimization tasks outside of neural
    networks, models are usually less complex, and more effective methods than gradient
    descent are often employed. For example, while gradient descent can be applied
    to Quadratic Programming, it is generally not the most efficient method.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 优化通过在给定约束下最大化或最小化特定的目标函数来找到问题的最佳解决方案。在神经网络训练中，采用类似的原理，使用梯度下降和反向传播调整网络的权重。然而，在神经网络之外的优化任务中，模型通常较为简单，且常常采用比梯度下降更有效的方法。例如，虽然梯度下降可以应用于二次规划，但它通常不是最有效的方法。
- en: In autonomous driving, the planning cost to optimize typically considers dynamic
    objects for obstacle avoidance, static road structures for following lanes, navigation
    information to ensure the correct route, and ego status to evaluate smoothness.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动驾驶中，优化规划成本通常会考虑动态物体以避免障碍物，静态道路结构以跟随车道，导航信息以确保正确路线，以及自车状态以评估平顺性。
- en: Optimization can be categorized into convex and non-convex types. The key distinction
    is that in a convex optimization scenario, there is only one global optimum, which
    is also the local optimum. This characteristic makes it unaffected by the initial
    solution to the optimization problems. For non-convex optimization, the initial
    solution matters a lot, as illustrated in the chart below.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 优化可以分为凸优化和非凸优化两类。其主要区别在于，在凸优化场景中，只有一个全局最优解，而这个解也是局部最优解。这一特性使得优化问题不受初始解的影响。对于非凸优化，初始解非常重要，如下图所示。
- en: '![](../Images/697da6b0533847e3cebf23f289d27301.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/697da6b0533847e3cebf23f289d27301.png)'
- en: 'Convex vs non-convex optimization (source: [Stanford course materials](https://stanford.edu/~pilanci/papers/TALK_Sketching.pdf))'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 凸优化与非凸优化（来源：[斯坦福课程资料](https://stanford.edu/~pilanci/papers/TALK_Sketching.pdf)）
- en: Since planning involves highly non-convex optimization with many local optima,
    it heavily depends on the initial solution. Additionally, convex optimization
    typically runs much faster and is therefore preferred for onboard real-time applications
    such as autonomous driving. A typical approach is to use convex optimization in
    conjunction with other methods to outline a convex solution space first. This
    is the mathematical foundation behind separating behavior planning and motion
    planning, where finding a good initial solution is the role of behavior planning.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 由于规划涉及高度非凸优化，且存在多个局部最优解，因此它严重依赖于初始解。此外，凸优化通常运行得更快，因此更适用于诸如自动驾驶等车载实时应用。一个典型的方法是将凸优化与其他方法结合使用，首先勾画出一个凸的解空间。这就是将行为规划与运动规划分开的数学基础，在这个过程中，找到一个良好的初始解是行为规划的职责。
- en: Take obstacle avoidance as a concrete example, which typically introduces non-convex
    problems. If we know the nudging direction, then it becomes a convex optimization
    problem, with the obstacle position acting as a lower or upper bound constraint
    for the optimization problem. If we don’t know the nudging direction, we need
    to decide first which direction to nudge, making the problem a convex one for
    motion planning to solve. This nudging direction decision falls under behavior
    planning.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 以避障为例，它通常会引入非凸问题。如果我们知道推动方向，那么这就变成了一个凸优化问题，障碍物的位置作为约束条件成为优化问题的下界或上界。如果我们不知道推动方向，我们需要先决定哪个方向来推动，这使得问题变成了一个由运动规划来解决的凸问题。这个推动方向的决定属于行为规划范畴。
- en: Of course, we can do direct optimization of non-convex optimization problems
    with tools such as projected gradient descent, alternating minimization, particle
    swarm optimization (PSO), and genetic algorithms. However, this is beyond the
    scope of this post.
  id: totrans-100
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当然，我们也可以通过直接优化非凸优化问题来解决，比如使用投影梯度下降、交替最小化、粒子群优化（PSO）和遗传算法等工具。然而，这超出了本文的讨论范围。
- en: '![](../Images/6a71f4bf5ba9b067048f77546c92d93b.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a71f4bf5ba9b067048f77546c92d93b.png)'
- en: A convex path planning problem vs a non-convex one (chart made by author)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 凸路径规划问题与非凸路径规划问题（作者制作的图表）
- en: '![](../Images/5aec290676543117fa55a96f20ec808c.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5aec290676543117fa55a96f20ec808c.png)'
- en: The solution process of the convex vs non-convex path planning problem (chart
    made by author)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 凸与非凸路径规划问题的解决过程（作者制作的图表）
- en: How do we make such decisions? We can use the aforementioned search or sampling
    methods to address non-convex problems. Sampling-based methods scatter many options
    across the parameter space, effectively handling non-convex issues similarly to
    searching.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何做出这样的决策呢？我们可以使用前述的搜索或采样方法来解决非凸问题。基于采样的方法通过在参数空间中分散多个选项，类似于搜索，有效地处理非凸问题。
- en: You may also question why deciding which direction to nudge from is enough to
    guarantee the problem space is convex. To explain this, we need to discuss topology.
    In path space, similar feasible paths can transform continuously into each other
    without obstacle interference. These similar paths, grouped as “homotopy classes”
    in the formal language of topology, can all be explored using a single initial
    solution homotopic to them. All these paths form a driving corridor, illustrated
    as the red or green shaded area in the image above. For a 3D spatiotemporal case,
    please refer to the [QCraft tech blog](https://zhuanlan.zhihu.com/p/551381336).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 你也许会质疑，为什么仅仅决定从哪个方向推动就足够保证问题空间是凸的。为了说明这一点，我们需要讨论拓扑学。在路径空间中，相似的可行路径可以在没有障碍物干扰的情况下连续地互相转化。这些相似路径在拓扑学的正式语言中被归为“同伦类”，它们都可以通过一个初始解来探索，这个初始解与这些路径是同伦的。所有这些路径共同形成了一个驾驶通道，如上图中的红色或绿色阴影区域所示。对于三维时空情况，请参考[QCraft技术博客](https://zhuanlan.zhihu.com/p/551381336)。
- en: We can utilize the [*Generalized Voronoi diagram*](https://zhuanlan.zhihu.com/p/551381336)
    to enumerate all homotopy classes, which roughly corresponds to the different
    decision paths available to us. However, this topic delves into advanced mathematical
    concepts that are beyond the scope of this blog post.
  id: totrans-107
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们可以利用[*广义Voronoi图*](https://zhuanlan.zhihu.com/p/551381336)来列举所有同伦类，这大致对应于我们可用的不同决策路径。然而，这个话题涉及到高级数学概念，超出了本文的范围。
- en: The key to solving optimization problems efficiently lies in the capabilities
    of the optimization solver. Typically, a solver requires approximately 10 milliseconds
    to plan a trajectory. If we can boost this efficiency by tenfold, it can significantly
    impact algorithm design. This exact improvement was highlighted during Tesla AI
    Day 2022\. A similar enhancement has occurred in perception systems, transitioning
    from 2D perception to Bird’s Eye View (BEV) as available computing power scaled
    up tenfold. With a more efficient optimizer, more options can be calculated and
    evaluated, thereby reducing the importance of the decision-making process. However,
    engineering an efficient optimization solver demands substantial engineering resources.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 高效解决优化问题的关键在于优化求解器的能力。通常，求解器需要大约10毫秒来规划一条轨迹。如果我们能够将这一效率提高十倍，将对算法设计产生重大影响。这个改进在2022年特斯拉AI日中得到了突出展示。类似的改进也出现在感知系统中，随着可用计算能力提升十倍，从2D感知转向了鸟瞰图（BEV）。通过更高效的优化器，可以计算和评估更多选项，从而减少决策过程的重要性。然而，设计一个高效的优化求解器需要大量的工程资源。
- en: Every time compute scales up by 10x, algorithm will evolve to next generation.
  id: totrans-109
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 每次计算能力提升10倍时，算法将进化到下一代。
- en: — — The unverified law of algorithm evolution
  id: totrans-110
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: — — 未验证的算法进化定律
- en: Industry Practices of Planning
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 规划的行业实践
- en: A key differentiator in various planning systems is whether they are spatiotemporally
    decoupled. Concretely, spatiotemporally decoupled methods plan in spatial dimensions
    first to generate a path, and then plan the speed profile along this path. This
    approach is also known as path-speed decoupling.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 各种规划系统的关键区别在于它们是否是时空解耦的。具体来说，时空解耦方法首先在空间维度上进行规划以生成路径，然后在该路径上规划速度轮廓。这种方法也被称为路径-速度解耦。
- en: Path-speed decoupling is often referred to as lateral-longitudinal (lat-long)
    decoupling, where lateral (lat) planning corresponds to path planning and longitudinal
    (long) planning corresponds to speed planning. This terminology seems to originate
    from the Frenet coordinate system explained earlier.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 路径-速度解耦通常被称为横向-纵向（lat-long）解耦，其中横向（lat）规划对应路径规划，纵向（long）规划对应速度规划。这一术语似乎源自前面解释的弗雷内坐标系。
- en: Decoupled solutions are easier to implement and can solve about 95% of issues.
    In contrast, coupled solutions have a higher theoretical performance ceiling but
    are more challenging to implement. They involve more parameters to tune and require
    a more principled approach to parameter tuning.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 解耦方案更易于实现，能够解决大约95%的问题。相比之下，联合方案具有更高的理论性能上限，但实现起来更具挑战性。它们涉及更多的参数调节，并且需要更有原则的参数调节方法。
- en: '![](../Images/4f7f92a26aabf6071ff72ba10662c0c5.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f7f92a26aabf6071ff72ba10662c0c5.png)'
- en: 'The comparison of decoupled and joint planning (source: made by the author,
    inspired by [Qcraft](https://www.xchuxing.com/article/63767))'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 解耦与联合规划的比较（来源：由作者制作，灵感来源于[Qcraft](https://www.xchuxing.com/article/63767)）
- en: '![](../Images/80caac84205c7a3876e6207a9d7e841f.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80caac84205c7a3876e6207a9d7e841f.png)'
- en: Pros and cons of decoupled vs joint spatiotemporal planning (chart made by author)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 解耦与联合时空规划的优缺点（图表由作者制作）
- en: Path-speed decoupled planning
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 路径-速度解耦规划
- en: We can take [Baidu Apollo EM planner](https://arxiv.org/abs/1807.08048) as an
    example of a system that uses path-speed decoupled planning.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以以[Baidu Apollo EM规划器](https://arxiv.org/abs/1807.08048)为例，展示使用路径-速度解耦规划的系统。
- en: 'The EM planner significantly reduces computational complexity by transforming
    a three-dimensional station-lateral-speed problem into two two-dimensional problems:
    station-lateral and station-speed. At the core of Apollo’s EM planner is an iterative
    Expectation-Maximization (EM) step, consisting of path optimization and speed
    optimization. Each step is divided into an E-step (projection and formulation
    in a 2D state space) and an M-step (optimization in the 2D state space). The E-step
    involves projecting the 3D problem into either a Frenet SL frame or an ST speed
    tracking frame.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: EM规划器通过将三维的站点-横向-速度问题转化为两个二维问题：站点-横向和站点-速度，从而显著降低了计算复杂度。Apollo的EM规划器的核心是一个迭代的期望最大化（EM）步骤，由路径优化和速度优化组成。每个步骤被分为E步骤（在二维状态空间中的投影和公式化）和M步骤（在二维状态空间中的优化）。E步骤涉及将三维问题投影到弗雷内SL框架或ST速度跟踪框架中。
- en: '![](../Images/dc607d36c1a73faf84d32ee4fd982b79.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc607d36c1a73faf84d32ee4fd982b79.png)'
- en: 'The EM iteration in Apollo EM planner (source: [Baidu Apollo EM planner](https://arxiv.org/abs/1807.08048)
    )'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Apollo EM 规划器中的EM迭代（来源：[百度Apollo EM规划器](https://arxiv.org/abs/1807.08048)）
- en: 'The M-step (maximization step) in both path and speed optimization involves
    solving non-convex optimization problems. For path optimization, this means deciding
    whether to nudge an object on the left or right side, while for speed optimization,
    it involves deciding whether to overtake or yield to a dynamic object crossing
    the path. The Apollo EM planner addresses these non-convex optimization challenges
    using a two-step process: Dynamic Programming (DP) followed by Quadratic Programming
    (QP).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在路径和速度优化中的M步（最大化步骤）涉及解决非凸优化问题。对于路径优化，这意味着决定是否将物体推向左侧或右侧；而对于速度优化，则涉及决定是超车还是对动态物体让行。Apollo
    EM 规划器通过两步法解决这些非凸优化挑战：动态规划（DP）和二次规划（QP）。
- en: DP uses a sampling or searching algorithm to generate a rough initial solution,
    effectively pruning the non-convex space into a convex space. QP then takes the
    coarse DP results as input and optimizes them within the convex space provided
    by DP. In essence, DP focuses on feasibility, and QP refines the solution to achieve
    optimality within the convex constraints.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 动态规划（DP）使用采样或搜索算法生成粗略的初步解，有效地将非凸空间修剪成凸空间。然后，二次规划（QP）将粗略的DP结果作为输入，在DP提供的凸空间内进行优化。本质上，DP关注可行性，而QP则在凸约束下对解进行精炼，以实现最优解。
- en: In our defined terminology, Path DP corresponds to lateral BP, Path QP to lateral
    MP, Speed DP to longitudinal BP, and Speed QP to longitudinal MP. Thus, the process
    involves conducting BP (Basic Planning) followed by MP (Master Planning) in both
    the path and speed steps.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们定义的术语中，路径DP对应于横向基本规划（BP），路径QP对应于横向主规划（MP），速度DP对应于纵向基本规划（BP），速度QP对应于纵向主规划（MP）。因此，该过程涉及在路径和速度步骤中先进行基本规划（BP），然后进行主规划（MP）。
- en: '![](../Images/f32f0b7181bbaae16ff39a41c4366104.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f32f0b7181bbaae16ff39a41c4366104.png)'
- en: A full autonomous driving stack with path-speed decoupled planning (chart made
    by author)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 一个完整的自动驾驶系统，采用路径-速度解耦规划（图表由作者制作）
- en: Joint spatiotemporal planning
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 联合时空规划
- en: Although decoupled planning can resolve 95% of cases in autonomous driving,
    the remaining 5% involve challenging dynamic interactions where a decoupled solution
    often results in suboptimal trajectories. In these complex scenarios, demonstrating
    intelligence is crucial, making it a very hot topic in the field.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然解耦规划可以解决自动驾驶中95%的情况，但剩余的5%涉及具有挑战性的动态交互，在这些情况下，解耦解往往导致次优的轨迹。在这些复杂场景中，展现智能是至关重要的，这也使其成为该领域的一个热点话题。
- en: For example, in narrow-space passing, the optimal behavior might be to either
    decelerate to yield or accelerate to pass. Such behaviors are not achievable within
    the decoupled solution space and require joint optimization. Joint optimization
    allows for a more integrated approach, considering both path and speed simultaneously
    to handle intricate dynamic interactions effectively.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在狭窄空间通过时，最优行为可能是减速让行或加速超车。这样的行为在解耦的解空间内无法实现，需要联合优化。联合优化允许更综合的方法，能够同时考虑路径和速度，从而有效处理复杂的动态交互。
- en: '![](../Images/d32a376deca659417fe15fe58533a45e.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d32a376deca659417fe15fe58533a45e.png)'
- en: A full autonomous driving stack with joint spatiotemporal planning (chart made
    by author)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 一个完整的自动驾驶系统，采用联合时空规划（图表由作者制作）
- en: However, there are significant challenges in joint spatiotemporal planning.
    Firstly, solving the non-convex problem directly in a higher-dimensional state
    space is more challenging and time-consuming than using a decoupled solution.
    Secondly, considering interactions in spatiotemporal joint planning is even more
    complex. We will cover this topic in more detail later when we discuss decision-making.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，联合时空规划面临重大挑战。首先，在更高维的状态空间中直接解决非凸问题比使用解耦方案更加困难且耗时。其次，在时空联合规划中考虑交互性则更加复杂。我们将在后续讨论决策时更详细地探讨这一主题。
- en: 'Here we introduce two solving methods: brute force search and constructing
    a spatiotemporal corridor for optimization.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们介绍了两种求解方法：暴力搜索和构建时空走廊进行优化。
- en: Brute force search occurs directly in 3D spatiotemporal space (2D in space and
    1D in time), and can be performed in either XYT (Cartesian) or SLT (Frenet) coordinates.
    We will take SLT as an example. SLT space is long and flat, similar to an energy
    bar. It is elongated in the L dimension and flat in the ST face. For brute force
    search, we can use hybrid A-star, with the cost being a combination of progress
    cost and cost to go. During optimization, we must conform to search constraints
    that prevent reversing in both the s and t dimensions.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 粗暴搜索直接发生在三维时空空间中（空间为二维，时间为一维），可以在XYT（笛卡尔坐标）或SLT（弗雷内坐标）坐标系中执行。我们以SLT为例。SLT空间长而平坦，类似于一个能量条。它在L维度上拉长，在ST面上平坦。对于粗暴搜索，我们可以使用混合A星算法，代价是进度代价与前进代价的组合。在优化过程中，我们必须遵守搜索约束，防止在s和t维度上发生反向移动。
- en: '![](../Images/f2dd701764070281b4288f1bd789052f.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f2dd701764070281b4288f1bd789052f.png)'
- en: 'Overtake by lane change in spatiotemporal lattice (source: [Spatiotemporal
    optimization with A*](https://www.qichegongcheng.com/CN/abstract/abstract1500.shtml))'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 空间时间晶格中的变道超车（来源：[基于A*的时空优化](https://www.qichegongcheng.com/CN/abstract/abstract1500.shtml)）
- en: Another method is constructing a spatiotemporal corridor, essentially a curve
    with the footprint of a car winding through a 3D spatiotemporal state space (SLT,
    for example). The [SSC (spatiotemporal semantic corridor, RAL 2019)](https://arxiv.org/abs/1906.09788),
    encodes requirements given by semantic elements into a semantic corridor, generating
    a safe trajectory accordingly. The semantic corridor consists of a series of mutually
    connected collision-free cubes with dynamical constraints posed by the semantic
    elements in the spatiotemporal domain. Within each cube, it becomes a convex optimization
    problem that can be solved using Quadratic Programming (QP).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是构建时空走廊，本质上是一个曲线，表示汽车的足迹在三维时空状态空间中盘旋（例如SLT）。[SSC（时空语义走廊，RAL 2019）](https://arxiv.org/abs/1906.09788)将语义元素提供的需求编码到语义走廊中，从而生成安全轨迹。语义走廊由一系列互相连接的无碰撞立方体组成，这些立方体的动态约束由时空域中的语义元素施加。在每个立方体内，它变成一个凸优化问题，可以使用二次规划（QP）来求解。
- en: SSC still requires a BP (Behavior Planning) module to provide a coarse driving
    trajectory. Complex semantic elements of the environment are projected into the
    spatiotemporal domain concerning the reference lane. [EPSILON (TRO 2021)](https://arxiv.org/abs/2108.07993),
    showcases a system where SSC serves as the motion planner working in tandem with
    a behavior planner. In the next section, we will discuss behavior planning, especially
    focusing on interaction. In this context, behavior planning is usually referred
    to as decision making.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: SSC仍然需要一个BP（行为规划）模块来提供粗略的驾驶轨迹。环境中的复杂语义元素相对于参考车道投影到时空域中。[EPSILON（TRO 2021）](https://arxiv.org/abs/2108.07993)展示了一个系统，其中SSC作为与行为规划器协同工作的运动规划器。在下一节中，我们将讨论行为规划，特别是专注于交互。在这种情况下，行为规划通常被称为决策制定。
- en: '![](../Images/abad96e9b684fca4f58bce59ddb353e6.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/abad96e9b684fca4f58bce59ddb353e6.png)'
- en: 'An illustration of the spatiotemporal corridor (source: [SSC](https://arxiv.org/abs/1906.09788))'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 时空走廊的示意图（来源：[SSC](https://arxiv.org/abs/1906.09788)）
- en: Decision making
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策制定
- en: What and why?
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么和为什么？
- en: Decision making in autonomous driving is essentially behavior planning, but
    with a focus on interaction with other traffic agents. The assumption is that
    other agents are mostly rational and will respond to our behavior in a predictable
    manner, which we can describe as “noisily rational.”
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶中的决策制定本质上是行为规划，但更侧重于与其他交通主体的交互。假设其他主体大多数是理性的，并且会以可预测的方式响应我们的行为，这种情况可以描述为“有噪声的理性”。
- en: People may question the necessity of decision making when advanced planning
    tools are available. However, two key aspects — uncertainty and interaction —
    introduce a probabilistic nature to the environment, primarily due to the presence
    of dynamic objects. Interaction is the most challenging part of autonomous driving,
    distinguishing it from general robotics. Autonomous vehicles must not only navigate
    but also anticipate and react to the behavior of other agents, making robust decision-making
    essential for safety and efficiency.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 当先进的规划工具可用时，人们可能会质疑决策制定的必要性。然而，有两个关键因素——不确定性和交互——使得环境具有了概率性质，主要是由于动态物体的存在。交互是自动驾驶中最具挑战性的部分，它将自动驾驶与一般机器人技术区分开来。自动驾驶汽车不仅要导航，还必须预测并应对其他主体的行为，因此，稳健的决策制定对安全性和效率至关重要。
- en: In a deterministic (purely geometric) world without interaction, decision making
    would be unnecessary, and planning through searching, sampling, and optimization
    would suffice. Brute force searching in the 3D XYT space could serve as a general
    solution.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有互动的确定性（纯几何）世界中，决策将是多余的，通过搜索、采样和优化进行规划就足够了。在三维XYT空间中进行暴力搜索可以作为一种通用解决方案。
- en: In most classical autonomous driving stacks, a prediction-then-plan approach
    is adopted, assuming zero-order interaction between the ego vehicle and other
    vehicles. This approach treats prediction outputs as deterministic, requiring
    the ego vehicle to react accordingly. This leads to overly conservative behavior,
    exemplified by the “freezing robot” problem. In such cases, prediction fills the
    entire spatiotemporal space, preventing actions like lane changes in crowded conditions
    — something humans manage more effectively.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数经典自动驾驶系统中，采用预测-然后规划的方法，假设自车与其他车辆之间是零阶互动。该方法将预测结果视为确定性的，要求自车做出相应的反应。这会导致过于保守的行为，典型例子就是“冻结机器人”问题。在这种情况下，预测填充了整个时空空间，阻止了像在拥挤条件下变道这样的动作——这是人类能更有效管理的事情。
- en: To handle stochastic strategies, Markov Decision Processes (MDP) or Partially
    Observable Markov Decision Processes (POMDP) frameworks are essential. These approaches
    shift the focus from geometry to probability, addressing chaotic uncertainty.
    By assuming that traffic agents behave rationally or at least noisily rationally,
    decision making can help create a safe driving corridor in the otherwise chaotic
    spatiotemporal space.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理随机策略，马尔可夫决策过程（MDP）或部分可观测马尔可夫决策过程（POMDP）框架是必不可少的。这些方法将重点从几何转向概率，解决混乱的不确定性。通过假设交通参与者理性行为或至少是嘈杂理性，决策可以帮助在本应混乱的时空空间中创建安全的行驶通道。
- en: Among the three overarching goals of planning — safety, comfort, and efficiency
    — decision making primarily enhances efficiency. Conservative actions can maximize
    safety and comfort, but effective negotiation with other road agents, achievable
    through decision making, is essential for optimal efficiency. Effective decision
    making also displays intelligence.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在规划的三个主要目标——安全、舒适性和效率——中，决策主要提升效率。保守的行为可以最大化安全性和舒适性，但通过决策有效地与其他道路参与者协商，对于实现最优效率至关重要。有效的决策也展现了智能。
- en: MDP and POMDP
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MDP和POMDP
- en: We will first introduce Markov Decision Processes (MDP) and Partially Observable
    Markov Decision Processes (POMDP), followed by their systematic solutions, such
    as value iteration and policy iteration.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先介绍马尔可夫决策过程（MDP）和部分可观测马尔可夫决策过程（POMDP），接着介绍它们的系统化解法，如价值迭代和策略迭代。
- en: A Markov Process (MP) is a type of stochastic process that deals with dynamic
    random phenomena, unlike static probability. In a Markov Process, the future state
    depends only on the current state, making it sufficient for prediction. For autonomous
    driving, the relevant state may only include the last second of data, and we can
    expand the state space to allow for a shorter history window (for example, instead
    of a longer history of positions, we can use a shorter history window of position,
    velocity and acceleration, etc).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫过程（MP）是一种处理动态随机现象的随机过程，不同于静态概率。在马尔可夫过程中，未来状态仅依赖于当前状态，因此仅凭当前状态就足够进行预测。对于自动驾驶而言，相关的状态可能只包括最后一秒的数据，我们可以扩展状态空间，允许使用更短的历史窗口（例如，取代更长的位置信息历史，可以使用更短的位置信息、速度、加速度等历史窗口）。
- en: 'A Markov Decision Process (MDP) extends a Markov Process to include decision-making
    by introducing action. MDPs model decision-making where outcomes are partly random
    and partly controlled by the decision maker or agent. An MDP can be modeled with
    five factors:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程（MDP）是通过引入行动扩展了马尔可夫过程，涵盖了决策过程。MDP模型中的决策过程，其结果既有部分随机性，也有部分由决策者或智能体控制。一个MDP可以通过五个因素来建模：
- en: 'State (S): The state of the environment.'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 状态（S）：环境的状态。
- en: 'Action (A): The actions the agent can take to affect the environment.'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 行动（A）：智能体可以采取的、影响环境的动作。
- en: 'Reward (R): The reward the environment provides to the agent as a result of
    the action.'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 奖励（R）：环境因行动而给予智能体的奖励。
- en: 'Transition Probability (P): The probability of transitioning from the old state
    to a new state upon the agent’s action.'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转移概率（P）：智能体采取行动后，从旧状态转移到新状态的概率。
- en: 'Gamma (γ): A discount factor for future rewards.'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 折扣因子（γ）：未来奖励的折扣因子。
- en: This is also the common framework used by reinforcement learning (RL), which
    is essentially an MDP. The goal of MDP or RL is to maximize the cumulative reward
    received in the long run. This requires the agent to make good decisions given
    a state from the environment, according to a policy.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是强化学习（RL）使用的常见框架，RL本质上是一个MDP。MDP或RL的目标是在长期内最大化累积回报。这要求智能体根据策略，在给定的环境状态下做出正确的决策。
- en: A policy, π, is a mapping from each state, s ∈ S, and action, a ∈ A(s), to the
    probability π(a|s) of taking action a when in state s. MDP or RL studies the problem
    of how to derive the optimal policy.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 策略π是一个映射，将每个状态s ∈ S和动作a ∈ A(s)映射到在状态s下采取动作a的概率π(a|s)。MDP或强化学习研究如何推导出最优策略的问题。
- en: '![](../Images/498d6c7687a59b14b330d3801aead9cf.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/498d6c7687a59b14b330d3801aead9cf.png)'
- en: 'The agent-environment interface in MDP and RL (source: [Reinforcement Learning:
    An Introduction](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf))'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: MDP和RL中的智能体-环境接口（来源：[强化学习：导论](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)）
- en: A Partially Observable Markov Decision Process (POMDP) adds an extra layer of
    complexity by recognizing that states cannot be directly observed but rather inferred
    through observations. In a POMDP, the agent maintains a belief — a probability
    distribution over possible states — to estimate the state of the environment.
    Autonomous driving scenarios are better represented by POMDPs due to their inherent
    uncertainties and the partial observability of the environment. An MDP can be
    considered a special case of a POMDP where the observation perfectly reveals the
    state.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 部分可观察的马尔可夫决策过程（POMDP）通过认识到状态不能直接观察到，而是通过观察来推测，增加了额外的复杂性。在POMDP中，智能体保持一种信念——对可能状态的概率分布——来估计环境的状态。由于环境的不确定性和部分可观察性，自动驾驶场景通常由POMDP更好地表示。MDP可以看作是POMDP的一个特例，其中观察能够完美地揭示状态。
- en: '![](../Images/826bad80b9b9275cb7ed42022798660f.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/826bad80b9b9275cb7ed42022798660f.png)'
- en: 'MDP vs POMDP (source: [POMDPs as stochastic contingent planning](https://www.researchgate.net/figure/MDP-and-POMDP-visualization_fig1_374986767))'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: MDP与POMDP（来源：[POMDPs作为随机应急规划](https://www.researchgate.net/figure/MDP-and-POMDP-visualization_fig1_374986767)）
- en: POMDPs can actively collect information, leading to actions that gather necessary
    data, demonstrating the intelligent behavior of these models. This capability
    is particularly valuable in scenarios like waiting at intersections, where gathering
    information about other vehicles’ intentions and the state of the traffic light
    is crucial for making safe and efficient decisions.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: POMDPs可以主动收集信息，从而采取收集必要数据的行动，展现了这些模型的智能行为。这一能力在一些场景中尤为重要，比如在交叉口等待时，收集其他车辆意图和交通信号灯状态的信息对做出安全和高效的决策至关重要。
- en: Value iteration and Policy iteration
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 值迭代与策略迭代
- en: '**Value iteration** and **policy iteration** are systematic methods for solving
    MDP or POMDP problems. While these methods are not commonly used in real-world
    applications due to their complexity, understanding them provides insight into
    exact solutions and how they can be simplified in practice, such as using MCTS
    in AlphaGo or MPDM in autonomous driving.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**值迭代**和**策略迭代**是解决MDP或POMDP问题的系统方法。尽管由于其复杂性，这些方法在实际应用中不常用，但理解它们能够帮助我们洞察精确解的获取方式，以及如何在实践中简化这些方法，比如在AlphaGo中使用MCTS，或者在自动驾驶中使用MPDM。'
- en: 'To find the best policy in an MDP, we must assess the potential or expected
    reward from a state, or more specifically, from an action taken in that state.
    This expected reward includes not just the immediate reward but also all future
    rewards, formally known as the return or cumulative discounted reward. (For a
    deeper understanding, refer to “[Reinforcement Learning: An Introduction](https://medium.com/p/ede324d78717/edit#:~:text=https%3A//web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf),”
    often considered the definitive guide on the subject.)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在MDP中找到最佳策略，我们必须评估一个状态的潜在或期望回报，或者更具体地说，评估在该状态下采取某个动作的回报。这个期望回报不仅包括即时回报，还包括所有未来的回报，正式称为回报或累积折扣回报。（若要深入了解，请参考“[强化学习：导论](https://medium.com/p/ede324d78717/edit#:~:text=https%3A//web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)”，这本书常被视为该领域的权威指南。）
- en: The value function (V) characterizes the quality of states by summing the expected
    returns. The action-value function (Q) assesses the quality of actions for a given
    state. Both functions are defined according to a given policy. The Bellman Optimality
    Equation states that an *optimal* policy will choose the action that *maximizes*
    the immediate reward plus the expected future rewards from the resulting new states.
    In simple terms, the Bellman Optimality Equation advises considering both the
    immediate reward and the future consequences of an action. For example, when switching
    jobs, consider not only the immediate pay raise (R) but also the future value
    (S’) the new position offers.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 价值函数（V）通过求和期望回报来描述状态的质量。动作价值函数（Q）则评估在特定状态下执行某一动作的质量。这两个函数都是根据给定策略来定义的。贝尔曼最优性方程表明，*最优*策略将选择能够*最大化*即时奖励以及从结果状态中获得的期望未来奖励的动作。简单来说，贝尔曼最优性方程建议考虑动作的即时奖励以及未来的后果。例如，在换工作时，不仅要考虑即时的加薪（R），还要考虑新职位带来的未来价值（S’）。
- en: '![](../Images/b78b1653df30f808436b6e3d5b3f3137.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b78b1653df30f808436b6e3d5b3f3137.png)'
- en: Bellman’s equation of optimality (chart made by author)
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼最优性方程（图表由作者制作）
- en: It is relatively straightforward to extract the optimal policy from the Bellman
    Optimality Equation once the optimal value function is available. But how do we
    find this optimal value function? This is where value iteration comes to the rescue.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦获得最优值函数，从贝尔曼最优性方程中提取最优策略是相对简单的。那么我们如何找到这个最优值函数呢？这时，价值迭代就派上用场了。
- en: '![](../Images/e68bbaeaec158ca39554728869f81b09.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e68bbaeaec158ca39554728869f81b09.png)'
- en: Extract best policy from optimal values (chart made by author)
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 从最优值中提取最佳策略（图表由作者制作）
- en: Value iteration finds the best policy by repeatedly updating the value of each
    state until it stabilizes. This process is derived by turning the Bellman Optimality
    Equation into an update rule. Essentially, we use the optimal future picture to
    guide the iteration toward it. In plain language, “fake it until you make it!”
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 价值迭代通过反复更新每个状态的值，直到其稳定，从而找到最佳策略。这个过程是通过将贝尔曼最优性方程转化为更新规则来实现的。从本质上讲，我们使用最优的未来图景来引导迭代朝着它前进。通俗来说，就是“装作成功直到你成功！”
- en: '![](../Images/288f8fed1d74fe07a5ff0a778685e236.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/288f8fed1d74fe07a5ff0a778685e236.png)'
- en: Update value functions under the guidance of Bellman’s Equation (chart made
    by author)
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝尔曼方程的指导下更新价值函数（图表由作者制作）
- en: Value iteration is guaranteed to converge for finite state spaces, regardless
    of the initial values assigned to the states (for a detailed proof, please refer
    to the [Bible of RL](https://medium.com/p/ede324d78717/edit#:~:text=https%3A//medium.com/p/ede324d78717/edit%23%3A~%3Atext%3Dhttps%253A//web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)).
    If the discount factor gamma is set to 0, meaning we only consider immediate rewards,
    the value iteration will converge after just one iteration. A smaller gamma leads
    to faster convergence because the horizon of consideration is shorter, though
    it may not always be the best option for solving concrete problems. Balancing
    the discount factor is a key aspect of engineering practice.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 价值迭代在有限状态空间内保证收敛，无论初始状态值如何（详细证明请参见[强化学习圣经](https://medium.com/p/ede324d78717/edit#:~:text=https%3A//medium.com/p/ede324d78717/edit%23%3A~%3Atext%3Dhttps%253A//web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)）。如果折扣因子
    gamma 设置为 0，意味着我们只考虑即时奖励，价值迭代将在一次迭代后收敛。较小的 gamma 会导致更快的收敛，因为考虑的视野更短，尽管这不一定总是解决具体问题的最佳选择。平衡折扣因子是工程实践中的关键方面。
- en: One might ask how this works if all states are initialized to zero. The immediate
    reward in the Bellman Equation is crucial for bringing in additional information
    and breaking the initial symmetry. Think about the states that immediately lead
    to the goal state; their value propagates through the state space like a virus.
    In plain language, it’s about making small wins, frequently.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 有人可能会问，如果所有状态都初始化为零，这个方法如何有效。贝尔曼方程中的即时奖励对于引入额外信息并打破初始对称性至关重要。想一想那些能直接到达目标状态的状态，它们的价值像病毒一样传播在状态空间中。通俗来说，这就是不断地取得小胜利。
- en: '![](../Images/3e3a80128bd6e391639059888b443e77.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e3a80128bd6e391639059888b443e77.png)'
- en: 'Value and policy functions interact until they converge to optimum together
    (source: [Reinforcement Learning: An Introduction](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf))'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 值函数和策略函数相互作用，直到它们一起收敛到最优值（来源：[强化学习：导论](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)）
- en: However, value iteration also suffers from inefficiency. It requires taking
    the optimal action at each iteration by considering all possible actions, similar
    to Dijkstra’s algorithm. While it demonstrates feasibility as a basic approach,
    it is typically not practical for real-world applications.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，值迭代也存在低效的问题。它需要在每次迭代中通过考虑所有可能的行动来选择最优行为，类似于Dijkstra算法。尽管作为一种基本方法它是可行的，但通常不适用于现实世界的应用。
- en: '![](../Images/341299e3acf45f2fcba47891bf78fc9c.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/341299e3acf45f2fcba47891bf78fc9c.png)'
- en: The contrast of Bellman Equation and Bellman Optimality Equation (chart made
    by author)
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼方程与贝尔曼最优方程的对比（图表由作者制作）
- en: Policy iteration improves on this by taking actions according to the current
    policy and updating it based on the Bellman Equation (not the Bellman Optimality
    Equation). Policy iteration decouples policy evaluation from policy improvement,
    making it a much faster solution. Each step is taken based on a given policy instead
    of exploring all possible actions to find the one that maximizes the objective.
    Although each iteration of policy iteration can be more computationally intensive
    due to the policy evaluation step, it generally results in a faster convergence
    overall.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 策略迭代在此基础上进行改进，依据当前策略采取行动，并根据贝尔曼方程（而非贝尔曼最优方程）进行更新。策略迭代将策略评估与策略改进解耦，使其成为一种更快速的解决方案。每一步都是根据给定的策略来执行，而不是探索所有可能的行动以找到最大化目标的行动。尽管策略迭代的每次迭代可能因策略评估步骤而计算量较大，但总体上它能更快地收敛。
- en: In simple terms, if you can only fully evaluate the consequence of one action,
    it’s better to use your own judgment and do your best with the current information
    available.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，如果你只能完全评估某个行为的结果，最好还是依赖自己的判断，尽力利用当前可用的信息。
- en: AlphaGo and MCTS — when nets meet trees
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AlphaGo与蒙特卡洛树搜索（MCTS）——当网络遇见树
- en: We have all heard the unbelievable story of AlphaGo beating the best human player
    in 2016\. AlphaGo formulates the gameplay of Go as an MDP and solves it with Monte
    Carlo Tree Search (MCTS). But why not use value iteration or policy iteration?
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都听过AlphaGo在2016年击败世界顶级围棋选手的惊人故事。AlphaGo将围棋的游戏规则视为马尔可夫决策过程（MDP），并通过蒙特卡洛树搜索（MCTS）进行求解。那么，为什么不使用值迭代或策略迭代呢？
- en: Value iteration and policy iteration are systematic, iterative methods that
    solve MDP problems. However, even with improved policy iteration, it still requires
    performing time-consuming operations to update the value of every state. A standard
    19x19 Go board has roughly [2e170 possible states](https://senseis.xmp.net/?NumberOfPossibleGoGames=).
    This vast number of states makes it intractable to solve with traditional value
    iteration or policy iteration techniques.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 值迭代和策略迭代是解决MDP问题的系统性迭代方法。然而，即使是改进后的策略迭代，仍然需要执行耗时的操作来更新每个状态的值。标准的19x19围棋棋盘大约有[2e170种可能状态](https://senseis.xmp.net/?NumberOfPossibleGoGames=)。如此庞大的状态空间使得传统的值迭代或策略迭代方法无法解决。
- en: AlphaGo and its successors use a [Monte Carlo tree search](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search)
    (MCTS) algorithm to find their moves, guided by a value network and a policy network,
    trained on both human and computer play. Let’s take a look at vanilla MCTS first.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaGo及其后续版本使用[蒙特卡洛树搜索](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search)（MCTS）算法来寻找最佳走法，并通过值网络和策略网络进行指导，这些网络通过人类和计算机对局的数据进行训练。我们先来了解一下基础的MCTS。
- en: '![](../Images/2103789656021072b20fd0ceb6e0d8cb.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2103789656021072b20fd0ceb6e0d8cb.png)'
- en: 'The four steps of MCTS by AlphaGo, combining both value network and policy
    network (source: [AlphaGo](https://www.nature.com/articles/nature16961), Nature
    2016)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaGo的MCTS四个步骤，结合了值网络和策略网络（来源：[AlphaGo](https://www.nature.com/articles/nature16961)，《自然》2016）
- en: '**Monte Carlo Tree Search (MCTS)** is a method for policy estimation that focuses
    on decision-making from the current state. One iteration involves a four-step
    process: selection, expansion, simulation (or evaluation), and backup.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '**蒙特卡洛树搜索（MCTS）**是一种用于策略估计的方法，侧重于从当前状态进行决策。一次迭代涉及四个步骤：选择、扩展、模拟（或评估）和备份。'
- en: '**Selection**: The algorithm follows the most promising path based on previous
    simulations until it reaches a leaf node, a position not yet fully explored.'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择**：算法根据之前的模拟结果，遵循最有前景的路径，直到到达一个叶节点，即尚未完全探索的位置。'
- en: '**Expansion**: One or more child nodes are added to represent possible next
    moves from the leaf node.'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**扩展**：添加一个或多个子节点，表示从叶节点出发的可能下一步。'
- en: '**Simulation (Evaluation)**: The algorithm plays out a random game from the
    new node until the end, known as a “rollout.” This assesses the potential outcome
    from the expanded node by simulating random moves until a terminal state is reached.'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模拟（评估）**：算法从新节点开始，随机进行一局游戏，直到结束，这称为“回放”。这通过模拟随机动作直到达到终止状态，评估从扩展节点的潜在结果。'
- en: '**Backup**: The algorithm updates the values of the nodes on the path taken
    based on the game’s result. If the outcome is a win, the value of the nodes increases;
    if it is a loss, the value decreases. This process propagates the result of the
    rollout back up the tree, refining the policy based on simulated outcomes.'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**回传**：算法根据游戏结果更新路径上节点的值。如果结果是胜利，节点的值增加；如果是失败，节点的值减少。这个过程将回放的结果向上传播，基于模拟的结果细化策略。'
- en: After a given number of iterations, MCTS provides the percentage frequency with
    which immediate actions were selected from the root during simulations. During
    inference, the action with the most visits is selected. Here is [an interactive
    illustration of MTCS](https://vgarciasc.github.io/mcts-viz/) with the game of
    tic-tac-toe for simplicity.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在一定次数的迭代后，MCTS提供了在模拟过程中从根节点选择即时动作的百分比频率。在推理时，会选择访问次数最多的动作。这里是一个简单的[MTCS互动示意图](https://vgarciasc.github.io/mcts-viz/)展示了井字游戏。
- en: MCTS in AlphaGo is enhanced by two neural networks. **Value Network** evaluates
    the winning rate from a given state (board configuration). **Policy Network**
    evaluates the action distribution for all possible moves. These neural networks
    improve MCTS by reducing the effective depth and breadth of the search tree. The
    policy network helps in sampling actions, focusing the search on promising moves,
    while the value network provides a more accurate evaluation of positions, reducing
    the need for extensive rollouts. This combination allows AlphaGo to perform efficient
    and effective searches in the vast state space of Go.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaGo中的MCTS通过两个神经网络得到增强。**值网络**评估给定状态（棋盘配置）的胜率。**策略网络**评估所有可能动作的分布。这些神经网络通过减少搜索树的有效深度和广度来改进MCTS。策略网络有助于采样动作，集中搜索有前景的动作，而值网络提供了对位置的更准确评估，减少了广泛回放的需求。这种组合使得AlphaGo能够在围棋的广阔状态空间中进行高效且有效的搜索。
- en: '![](../Images/f006e297e1acba4bad8d39b92f318b69.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f006e297e1acba4bad8d39b92f318b69.png)'
- en: 'The policy network and value network of AlphaGo (source: [AlphaGo](https://www.nature.com/articles/nature16961),
    Nature 2016)'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaGo的策略网络和值网络（来源：[AlphaGo](https://www.nature.com/articles/nature16961)，Nature
    2016）
- en: In the **expansion** step, the policy network samples the most likely positions,
    effectively pruning the breadth of the search space. In the **evaluation** step,
    the value network provides an instinctive scoring of the position, while a faster,
    lightweight policy network performs rollouts until the game ends to collect rewards.
    MCTS then uses a weighted sum of the evaluations from both networks to make the
    final assessment.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在**扩展**步骤中，策略网络对最可能的位置进行采样，有效地修剪了搜索空间的广度。在**评估**步骤中，值网络对位置进行本能的评分，而更快速、轻量级的策略网络则执行回放，直到游戏结束并收集奖励。然后，MCTS使用两个网络评估的加权总和来做出最终评估。
- en: Note that a single evaluation of the value network approaches the accuracy of
    Monte Carlo rollouts using the RL policy network but with 15,000 times less computation.
    This mirrors the fast-slow system design, akin to intuition versus reasoning,
    or [System 1 versus System 2](https://medium.com/p/ede324d78717/edit#:~:text=https%3A//en.wikipedia.org/wiki/Thinking%2C_Fast_and_Slow)
    as described by Nobel laureate Daniel Kahneman. Similar designs can be observed
    in more recent works, such as [DriveVLM](https://arxiv.org/abs/2402.12289).
  id: totrans-205
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意，单次对值网络的评估接近使用RL策略网络的蒙特卡罗回放的准确度，但计算量少了15,000倍。这类似于快-慢系统的设计，类似于诺贝尔奖得主丹尼尔·卡尼曼所描述的[系统1与系统2](https://medium.com/p/ede324d78717/edit#:~:text=https%3A//en.wikipedia.org/wiki/Thinking%2C_Fast_and_Slow)，也可见于最近的工作中，如[DriveVLM](https://arxiv.org/abs/2402.12289)。
- en: ''
  id: totrans-206
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To be exact, AlphaGo incorporates two slow-fast systems at different levels.
    On the macro level, the policy network selects moves while the faster rollout
    policy network evaluates these moves. On the micro level, the faster rollout policy
    network can be approximated by a value network that directly predicts the winning
    rate of board positions.
  id: totrans-207
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 准确地说，AlphaGo在不同的层次上结合了两种慢速-快速系统。在宏观层面，策略网络选择棋步，而更快的回滚策略网络评估这些棋步。在微观层面，更快的回滚策略网络可以通过一个价值网络来近似，该网络直接预测棋盘位置的胜率。
- en: What can we learn from AlphaGo for autonomous driving? AlphaGo demonstrates
    the importance of extracting an excellent policy using a robust world model (simulation).
    Similarly, autonomous driving requires a highly accurate simulation to effectively
    leverage algorithms similar to those used by AlphaGo. This approach underscores
    the value of combining strong policy networks with detailed, precise simulations
    to enhance decision-making and optimize performance in complex, dynamic environments.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从AlphaGo中为自动驾驶学到什么？AlphaGo展示了使用强大的世界模型（仿真）提取优秀策略的重要性。同样，自动驾驶也需要一个高精度的仿真，以有效地利用类似于AlphaGo的算法。这一方法强调了将强大的策略网络与详细、精确的仿真相结合，以增强决策和优化复杂动态环境中的表现的价值。
- en: MPDM (and successors) in autonomous driving
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MPDM（及其后继研究）在自动驾驶中的应用
- en: In the game of Go, all states are immediately available to both players, making
    it a perfect information game where observation equals state. This allows the
    game to be characterized by an MDP process. In contrast, autonomous driving is
    a POMDP process, as the states can only be estimated through observation.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在围棋游戏中，所有状态对两位玩家都是立即可用的，这使得它成为一个完备信息游戏，其中观察等于状态。这使得游戏可以通过MDP过程来表征。相比之下，自动驾驶是一个POMDP过程，因为状态只能通过观察来估计。
- en: POMDPs connect perception and planning in a principled way. The typical solution
    for a POMDP is similar to that for an MDP, with a limited lookahead. However,
    the main challenges lie in the curse of dimensionality (explosion in state space)
    and the complex interactions with other agents. To make real-time progress tractable,
    domain-specific assumptions are typically made to simplify the POMDP problem.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: POMDP通过一种原理性的方法将感知和规划联系起来。POMDP的典型解决方案类似于MDP的解决方案，通过有限的前瞻来实现。然而，主要的挑战在于维度灾难（状态空间爆炸）和与其他代理的复杂交互。为了使实时进展可行，通常会做出特定领域的假设来简化POMDP问题。
- en: '[MPDM](https://ieeexplore.ieee.org/document/7139412) (and the [two](https://www.roboticsproceedings.org/rss11/p43.pdf)
    [follow-ups](https://link.springer.com/article/10.1007/s10514-017-9619-z), and
    [the white paper](https://maymobility.com/resources/autonomy-at-scale-white-paper/))
    is one pioneering study in this direction. MPDM reduces the POMDP to a closed-loop
    forward simulation of a finite, discrete set of semantic-level policies, rather
    than evaluating every possible control input for every vehicle. This approach
    addresses the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)
    by focusing on a manageable number of meaningful policies, allowing for effective
    real-time decision-making in autonomous driving scenarios.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[MPDM](https://ieeexplore.ieee.org/document/7139412)（以及[两个](https://www.roboticsproceedings.org/rss11/p43.pdf)
    [后续研究](https://link.springer.com/article/10.1007/s10514-017-9619-z)，和[白皮书](https://maymobility.com/resources/autonomy-at-scale-white-paper/)）是这一方向上的一项开创性研究。MPDM将POMDP简化为对有限、离散的语义级策略集合进行闭环前向仿真，而不是为每辆车评估所有可能的控制输入。这一方法通过关注可管理的、有意义的策略，解决了[维度灾难](https://en.wikipedia.org/wiki/Curse_of_dimensionality)问题，从而实现了在自动驾驶场景中的有效实时决策。'
- en: '![](../Images/1e479cccd693fbe81acaa387068e7102.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e479cccd693fbe81acaa387068e7102.png)'
- en: 'Semantic actions help control the curse of dimensionality (source: [EPSILON](https://arxiv.org/abs/2108.07993))'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 语义动作有助于控制维度灾难（来源：[EPSILON](https://arxiv.org/abs/2108.07993)）
- en: 'The assumptions of MPDM are twofold. First, much of the decision-making by
    human drivers involves discrete high-level semantic actions (e.g., slowing, accelerating,
    lane-changing, stopping). These actions are referred to as policies in this context.
    The second implicit assumption concerns other agents: other vehicles will make
    reasonably safe decisions. Once a vehicle’s policy is decided, its action (trajectory)
    is determined.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: MPDM的假设有两个方面。首先，许多人类驾驶员的决策涉及离散的高级语义动作（例如减速、加速、变道、停车）。在此背景下，这些动作被称为策略。第二个隐含假设是关于其他代理的：其他车辆将做出合理安全的决策。一旦车辆的策略确定，其行动（轨迹）也随之确定。
- en: '![](../Images/0a6e9de32f012cfd6c27c556e6912cf2.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a6e9de32f012cfd6c27c556e6912cf2.png)'
- en: The framework of MPDM (chart created by author)
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: MPDM框架（图表由作者创建）
- en: MPDM first selects one policy for the ego vehicle from many options (hence the
    “multi-policy” in its name) and selects one policy for each nearby agent based
    on their respective predictions. It then performs forward simulation (similar
    to a fast rollout in MCTS). The best interaction scenario after evaluation is
    then passed on to motion planning, such as the Spatiotemporal Semantic Corridor
    (SCC) mentioned in the joint spatiotemporal planning session.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: MPDM首先从多个选项中为自车选择一个策略（因此其名称中有“多策略”一词），并根据各自的预测为每个附近的代理选择一个策略。接着进行前向仿真（类似于MCTS中的快速展开）。经过评估后，最好的交互场景会传递给运动规划，例如在联合时空规划会议中提到的时空语义走廊（SCC）。
- en: MPDM enables intelligent and human-like behavior, such as actively cutting into
    dense traffic flow even when there is no sufficient gap present. This is not possible
    with a predict-then-plan pipeline, which does not explicitly consider interactions.
    The prediction module in MPDM is tightly integrated with the behavior planning
    model through forward simulation.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: MPDM使得智能和类人行为成为可能，例如，即使没有足够的空隙，依然能够主动切入密集的交通流中。这是基于预测-再规划管道无法实现的，因为它并未明确考虑交互。MPDM中的预测模块通过前向仿真与行为规划模型紧密集成。
- en: MPDM assumes a single policy throughout the decision horizon (10 seconds). Essentially,
    MPDM adopts an MCTS approach with one layer deep and super wide, considering all
    possible agent predictions. This leaves room for improvement, inspiring many follow-up
    works such as EUDM, EPSILON, and MARC. For example, EUDM considers more flexible
    ego policies and assigns a policy tree with a depth of four, with each policy
    covering a time duration of 2 seconds over an 8-second decision horizon. To compensate
    for the extra computation induced by the increased tree depth, EUDM performs more
    efficient width pruning by guided branching, identifying critical scenarios and
    key vehicles. This approach explores a more balanced policy tree.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: MPDM假设在整个决策时域（10秒）内使用单一策略。实质上，MPDM采用了一种MCTS方法，具有一层深度和超宽的结构，考虑了所有可能的代理预测。这为改进留下了空间，启发了许多后续工作，如EUDM、EPSILON和MARC。例如，EUDM考虑了更灵活的自我策略，并分配了一个深度为四的策略树，每个策略涵盖2秒的时间段，整个决策时域为8秒。为了弥补因树深增加带来的额外计算量，EUDM通过引导分支执行更高效的宽度修剪，识别关键场景和重要车辆。这种方法探索了一个更平衡的策略树。
- en: The forward simulation in MPDM and EUDM uses very simplistic driver models (Intelligent
    driver model or IDM for longitudinal simulation, and Pure Pursuit or PP for lateral
    simulation). MPDM points out that high fidelity realism matters less than the
    closed-loop nature itself, as long as policy-level decisions are not affected
    by low-level action execution inaccuracies.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: MPDM和EUDM中的前向仿真使用了非常简单的驾驶员模型（纵向仿真使用智能驾驶员模型IDM，横向仿真使用纯追踪算法PP）。MPDM指出，高保真度的真实感不如闭环特性本身重要，只要策略层决策不受低级动作执行不准确性的影响。
- en: '![](../Images/7cfe5b2544f6f640ad9958c221f248b7.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7cfe5b2544f6f640ad9958c221f248b7.png)'
- en: The conceptual diagram of decision making, where prediction, BP and MP integrates
    tightly (chart created by author)
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 决策制定的概念图，其中预测、行为规划（BP）和运动规划（MP）紧密集成（图表由作者创建）
- en: Contingency planning in the context of autonomous driving involves generating
    multiple potential trajectories to account for various possible future scenarios.
    A key motivating example is that experienced drivers anticipate multiple future
    scenarios and always plan for a safe backup plan. This anticipatory approach leads
    to a smoother driving experience, even when cars perform sudden cut-ins into the
    ego lane.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶中的应急规划涉及生成多种潜在轨迹，以应对各种可能的未来场景。一个关键的激励性例子是，经验丰富的驾驶员会预见到多种未来情景，并始终为安全备选方案做计划。这种预见性方法能够带来更平稳的驾驶体验，即使在车辆突然切入自车道时也能保持平稳。
- en: A critical aspect of contingency planning is deferring the decision bifurcation
    point. This means delaying the point at which different potential trajectories
    diverge, allowing the ego vehicle more time to gather information and respond
    to different outcomes. By doing so, the vehicle can make more informed decisions,
    resulting in smoother and more confident driving behaviors, similar to those of
    an experienced driver.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 应急规划的一个关键方面是推迟决策分岔点。这意味着延迟不同潜在轨迹分歧的点，为自车提供更多时间来收集信息并应对不同的结果。通过这样做，车辆可以做出更明智的决策，从而实现更平稳、更自信的驾驶行为，类似于经验丰富的驾驶员。
- en: '![](../Images/6bed42bd618a07b194d8e27fc1327ca7.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6bed42bd618a07b194d8e27fc1327ca7.png)'
- en: 'Risk-aware contingency planning (source: [MARC](https://arxiv.org/abs/2308.12021),
    RAL 2023)'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 风险意识应急规划（来源：[MARC](https://arxiv.org/abs/2308.12021)，RAL 2023）
- en: MARC also combines behavior planning and motion planning together. This extends
    the notion and utility of forward simulation. In other words, MPDM and EUDM still
    uses policy tree for high level behavior planning and rely on other motion planning
    pipelines such as [semantic spatiotemporal corridors (SSC)](https://arxiv.org/abs/1906.09788),
    due to the fact that ego motion in the policy tree is still characterized by heavily
    quantized behavior bucket. MARC extends this by keeping the quantized behavior
    for agents other than ego but uses more refined motion planning directly in the
    forward rollout. In a way it is a hybrid approach, where hybrid carries a similar
    meaning to that in hybrid A*, a mix of discrete and continuous.
  id: totrans-228
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: MARC也将行为规划和运动规划结合在一起。这扩展了前向仿真的概念和实用性。换句话说，MPDM和EUDM仍然使用策略树进行高层次行为规划，并依赖其他运动规划管道，如[语义时空通道（SSC）](https://arxiv.org/abs/1906.09788)，这是因为在策略树中的自车运动仍然由高度量化的行为桶来表征。MARC通过保持除自车外其他代理的量化行为，但在前向展开中直接使用更精细的运动规划来扩展这一点。从某种意义上说，这是一种混合方法，其中“混合”与混合A*中的含义相似，结合了离散和连续。
- en: One possible drawback of MPDM and all its follow-up works is their reliance
    on simple policies designed for highway-like structured environments, such as
    lane keeping and lane changing. This reliance may limit the capability of forward
    simulation to handle complex interactions. To address this, following the example
    of MPDM, the key to making POMDPs more effective is to simplify the action and
    state space through the growth of a high-level policy tree. It might be possible
    to create a more flexible policy tree, for example, by enumerating spatiotemporal
    relative position tags to all relative objects and then performing guided branching.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: MPDM及其所有后续工作可能的一个缺点是，它们依赖于为类似高速公路结构的环境设计的简单策略，例如车道保持和变道。这种依赖可能限制了前向仿真在处理复杂交互时的能力。为了解决这个问题，跟随MPDM的例子，使POMDPs更有效的关键是通过高层次策略树的扩展来简化动作和状态空间。通过列举所有相对物体的时空相对位置标签，然后执行引导分支，可能能够创建一个更灵活的策略树。
- en: Industry practices of decision making
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策制定的行业实践
- en: Decision-making remains a hot topic in current research. Even classical optimization
    methods have not been fully explored yet. Machine learning methods could shine
    and have a disruptive impact, especially with the advent of Large Language Models
    (LLMs), empowered by techniques like Chain of Thought (CoT) or Monte Carlo Tree
    Search (MCTS).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 决策制定仍然是当前研究的热门话题。即使是经典的优化方法也未被完全探索。机器学习方法有可能大放异彩，产生颠覆性影响，尤其是在大型语言模型（LLM）问世后，借助如思维链（CoT）或蒙特卡洛树搜索（MCTS）等技术。
- en: Trees
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 树
- en: Trees are systematic ways to perform decision-making. Tesla AI Day 2021 and
    2022 showcased their decision-making capabilities, heavily influenced by AlphaGo
    and the subsequent MuZero, to address highly complex interactions.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 树是进行决策的系统化方法。特斯拉2021年和2022年的AI日展示了它们的决策能力，深受AlphaGo及其后继版本MuZero的影响，用以处理高度复杂的交互。
- en: At a high level, Tesla’s approach follows behavior planning (decision making)
    followed by motion planning. It searches for a convex corridor first and then
    feeds it into continuous optimization, using spatiotemporal joint planning. This
    approach effectively addresses scenarios such as narrow passing, a typical bottleneck
    for path-speed decoupled planning.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，特斯拉的方法遵循行为规划（决策制定）后接运动规划。它首先寻找一个凸形通道，然后将其输入到连续优化中，采用时空联合规划。这种方法有效地解决了狭窄通过等场景，这是路径-速度解耦规划的典型瓶颈。
- en: '![](../Images/ee5c3ba674a8e3a6622efd7ad84d89f8.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee5c3ba674a8e3a6622efd7ad84d89f8.png)'
- en: 'Neural network heuristics guided MCTS (source: [Tesla AI Day 2021](https://youtu.be/j0z4FweCy4M?t=4514))'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络启发式引导的蒙特卡罗树搜索（MCTS）（来源：[Tesla AI Day 2021](https://youtu.be/j0z4FweCy4M?t=4514)）
- en: Tesla also adopts a hybrid system that combines data-driven and physics-based
    checks. Starting with defined goals, Tesla’s system generates seed trajectories
    and evaluates key scenarios. It then branches out to create more scenario variants,
    such as asserting or yielding to a traffic agent. Such an interaction search over
    the policy tree is showcased in the presentations of the years [2021](https://pic1.zhimg.com/80/v2-ede609df14524ce4a9c23689367f791c_1440w.webp)
    and [2022](https://pic2.zhimg.com/v2-48022fc4ee278be6f730d3b2ee587b19_r.png).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 特斯拉还采用了一种混合系统，结合了数据驱动和基于物理的检查。从定义的目标开始，特斯拉的系统生成种子轨迹并评估关键场景。然后，它会分支创建更多的场景变体，如对交通代理进行让步或请求。这样的政策树交互搜索在[2021年](https://pic1.zhimg.com/80/v2-ede609df14524ce4a9c23689367f791c_1440w.webp)和[2022年](https://pic2.zhimg.com/v2-48022fc4ee278be6f730d3b2ee587b19_r.png)的展示中有所展示。
- en: One highlight of Tesla’s use of machine learning is the acceleration of tree
    search via trajectory optimization. For each node, Tesla uses physics-based optimization
    and a neural planner, achieving a 10 ms vs. 100 µs time frame — resulting in a
    10x to 100x improvement. The neural network is trained with expert demonstrations
    and offline optimizers.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 特斯拉在机器学习应用中的一个亮点是通过轨迹优化加速树搜索。对于每个节点，特斯拉使用基于物理的优化和神经网络规划器，实现了10毫秒与100微秒的时间差——带来了10倍到100倍的性能提升。神经网络通过专家演示和离线优化器进行训练。
- en: Trajectory scoring is performed by combining classical physics-based checks
    (such as collision checks and comfort analysis) with neural network evaluators
    that predict intervention likelihood and rate human-likeness. This scoring helps
    prune the search space, focusing computation on the most promising outcomes.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 轨迹评分通过将经典物理基础检查（如碰撞检查和舒适度分析）与预测干预可能性和人类相似度的神经网络评估器相结合来完成。这种评分有助于修剪搜索空间，将计算集中在最有前景的结果上。
- en: While many argue that machine learning should be applied to high-level decision-making,
    Tesla uses ML fundamentally to accelerate optimization and, consequently, tree
    search.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然许多人认为机器学习应该应用于高层决策，但特斯拉基本上使用机器学习来加速优化，从而加速树搜索。
- en: The Monte Carlo Tree Search (MCTS) method appears to be an ultimate tool for
    decision-making. Interestingly, those studying Large Language Models (LLMs) are
    trying to incorporate MCTS into LLMs, while those working on autonomous driving
    are attempting to replace MCTS with LLMs.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡罗树搜索（MCTS）方法似乎是决策的终极工具。有趣的是，研究大规模语言模型（LLM）的人们正在尝试将MCTS融入LLM，而从事自动驾驶的人们则在尝试用LLM取代MCTS。
- en: As of roughly two years ago, Tesla’s technology followed this approach. However,
    since March 2024, Tesla’s Full Self-Driving (FSD) has switched to a more end-to-end
    approach, significantly different from their earlier methods.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 大约两年前，特斯拉的技术采用了这种方法。然而，从2024年3月开始，特斯拉的全自动驾驶（FSD）切换到了更加端到端的方法，明显不同于他们之前的做法。
- en: No trees
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无树
- en: We can still consider interactions without implicitly growing trees. Ad-hoc
    logics can be implemented to perform one-order interaction between prediction
    and planning. Even one-order interaction can already generate good behavior, as
    demonstrated by TuSimple. MPDM, in its original form, is essentially one-order
    interaction, but executed in a more principled and extendable way.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然可以考虑不隐式生成树的交互。可以实现临时逻辑，以在预测与规划之间进行一级交互。即使是一级交互，也能产生良好的行为，正如TuSimple所展示的那样。MPDM的原始形式本质上就是一级交互，但它以更有原则和可扩展的方式执行。
- en: '![](../Images/dd068780f3173683cf8bc15e030b0665.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd068780f3173683cf8bc15e030b0665.png)'
- en: 'Multi-order interaction between prediction and planning (source: [TuSImple
    AI day](https://www.bilibili.com/video/BV1Em4y1u7P7/?vd_source=73e03d3e246aa3ec284f028c7dcf0fa7),
    in Chinese, translated by author)'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 预测与规划之间的多级交互（来源：[TuSimple AI Day](https://www.bilibili.com/video/BV1Em4y1u7P7/?vd_source=73e03d3e246aa3ec284f028c7dcf0fa7)，中文，由作者翻译）
- en: TuSimple has also demonstrated the capability to perform contingency planning,
    similar to the approach proposed in [MARC](https://arxiv.org/abs/2308.12021) (though
    MARC can also accommodate a customized risk preference).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: TuSimple也展示了执行应急规划的能力，类似于[MARC](https://arxiv.org/abs/2308.12021)中提出的方法（尽管MARC也能适应定制的风险偏好）。
- en: '![](../Images/a06c89c942531828ed5dda56ef58dc38.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a06c89c942531828ed5dda56ef58dc38.png)'
- en: 'Contingency planning (source: [TuSImple AI day](https://www.bilibili.com/video/BV1Em4y1u7P7/?vd_source=73e03d3e246aa3ec284f028c7dcf0fa7),
    in Chinese, translated by author)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 应急规划（来源：[TuSImple AI day](https://www.bilibili.com/video/BV1Em4y1u7P7/?vd_source=73e03d3e246aa3ec284f028c7dcf0fa7)，中文，作者翻译）
- en: Self-Reflections
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自我反思
- en: After learning the basic building blocks of classical planning systems, including
    behavior planning, motion planning, and the principled way to handle interaction
    through decision-making, I have been reflecting on potential bottlenecks in the
    system and how machine learning (ML) and neural networks (NN) may help. I am documenting
    my thought process here for future reference and for others who may have similar
    questions. Note that the information in this section may contain personal biases
    and speculations.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习了经典规划系统的基本构建块之后，包括行为规划、运动规划和通过决策来处理交互的有原则方法，我一直在反思系统中潜在的瓶颈以及机器学习（ML）和神经网络（NN）如何提供帮助。我在此记录下我的思考过程，以便将来参考，也为其他可能有类似问题的人提供帮助。请注意，本节中的信息可能包含个人偏见和推测。
- en: Why NN in planning?
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么在规划中使用神经网络（NN）？
- en: 'Let’s look at the problem from three different perspectives: in the existing
    modular pipeline, as an end-to-end (e2e) NN planner, or as an e2e autonomous driving
    system.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从三个不同的角度来看待这个问题：在现有的模块化管道中，作为端到端（e2e）神经网络（NN）规划器，或者作为端到端的自动驾驶系统。
- en: Going back to the drawing board, let’s review the problem formulation of a planning
    system in autonomous driving. The goal is to obtain a trajectory that ensures
    safety, comfort, and efficiency in a highly uncertain and interactive environment,
    all while adhering to real-time engineering constraints onboard the vehicle. These
    factors are summarized as goals, environments, and constraints in the chart below.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 回到最初的设计，我们来回顾一下自动驾驶中规划系统的问题定义。目标是获得一个能够确保安全、舒适和高效的轨迹，同时在一个高度不确定且交互密集的环境中运行，并遵循车载的实时工程约束。以下图表总结了这些因素：目标、环境和约束。
- en: '![](../Images/fcaa289038ec10ef1f6767f19c7e5de5.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fcaa289038ec10ef1f6767f19c7e5de5.png)'
- en: The potentials of NN in planning (chart made by author)
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络在规划中的潜力（图表由作者制作）
- en: Uncertainty in autonomous driving can refer to uncertainty in perception (observation)
    and predicting long-term agent behaviors into the future. Planning systems must
    also handle the uncertainty in future trajectory predictions of other agents.
    As discussed earlier, a principled decision-making system is an effective way
    to manage this.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶中的不确定性可以指感知（观察）中的不确定性以及预测未来长期代理行为的不确定性。规划系统还必须处理其他代理未来轨迹预测中的不确定性。正如前面讨论的，采用有原则的决策系统是有效管理这些不确定性的方法。
- en: Additionally, a typically overlooked aspect is that planning must tolerate uncertain,
    imperfect, and sometimes incomplete perception results, especially in the current
    age of vision-centric and HD map-less driving. Having a Standard Definition (SD)
    map onboard as a prior helps alleviate this uncertainty, but it still poses significant
    challenges to a heavily handcrafted planner system. This perception uncertainty
    was considered a solved problem by Level 4 (L4) autonomous driving companies through
    the heavy use of Lidar and HD maps. However, it has resurfaced as the industry
    moves toward mass production autonomous driving solutions without these two crutches.
    An NN planner is more robust and can handle largely imperfect and incomplete perception
    results, which is key to mass production vision-centric and HD-mapless Advanced
    Driver Assistance Systems (ADAS).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，一个通常被忽视的方面是，规划必须容忍不确定的、不完美的，有时是不完整的感知结果，特别是在当前以视觉为中心且没有高清地图的驾驶时代。车载标准定义（SD）地图作为先验有助于缓解这种不确定性，但它仍然对一个高度手工制作的规划系统构成重大挑战。这个感知不确定性曾被Level
    4（L4）自动驾驶公司认为是一个已解决的问题，尤其是通过大量使用激光雷达（Lidar）和高清地图。然而，随着行业朝着没有这两项“拐杖”的量产自动驾驶解决方案发展，这个问题再次浮出水面。神经网络规划器更为强健，能够处理大部分不完美和不完整的感知结果，这对于以视觉为中心且没有高清地图的量产高级驾驶辅助系统（ADAS）至关重要。
- en: Interaction should be treated with a principled decision-making system such
    as Monte Carlo Tree Search (MCTS) or a simplified version of MPDM. The main challenge
    is dealing with the curse of dimensionality (combinatorial explosion) by growing
    a balanced policy tree with smart pruning through domain knowledge of autonomous
    driving. MPDM and its variants, in both academia and industry (e.g., Tesla), provide
    good examples of how to grow this tree in a balanced way.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 交互应该通过一个有原则的决策系统来处理，例如蒙特卡洛树搜索（MCTS）或简化版的MPDM。主要的挑战是通过利用自动驾驶领域的知识进行智能剪枝，成长一个平衡的策略树，以应对维度灾难（组合爆炸）问题。MPDM及其变种在学术界和工业界（如特斯拉）提供了如何以平衡的方式成长这棵树的良好示例。
- en: NNs can also enhance the real-time performance of planners by speeding up motion
    planning optimization. This can shift the compute load from CPU to GPU, achieving
    orders of magnitude speedup. A tenfold increase in optimization speed can fundamentally
    impact high-level algorithm design, such as MCTS.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络（NN）还可以通过加速运动规划优化，增强规划器的实时性能。这可以将计算负载从CPU转移到GPU，实现数量级的加速。优化速度的十倍提升可以从根本上影响高层次算法设计，例如MCTS。
- en: Trajectories also need to be more human-like. Human likeness and takeover predictors
    can be trained with the vast amount of human driving data available. It is more
    scalable to increase the compute pool rather than maintain a growing army of engineering
    talents.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 轨迹也需要更具人类特征。可以利用大量的人类驾驶数据训练人类相似度和接管预测模型。相比维持一个不断壮大的工程团队，增加计算池的规模更具可扩展性。
- en: '![](../Images/20831d3ba51f5c5083e4292b758d01d3.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20831d3ba51f5c5083e4292b758d01d3.png)'
- en: The NN-based planning stack can leverage human-driving data more effectively
    (Chart created by author)
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 基于神经网络（NN）的规划堆栈可以更有效地利用人类驾驶数据（图表由作者制作）
- en: What about e2e NN planners?
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 那么，端到端（e2e）神经网络规划器呢？
- en: An end-to-end (e2e) neural network (NN) planner still constitutes a modular
    autonomous driving (AD) design, accepting structured perception results (and potentially
    latent features) as its input. This approach combines prediction, decision, and
    planning into a single network. Companies such as [DeepRoute (2022) and Huawei
    (2024)](https://medium.com/p/ede324d78717/edit#:~:text=https%3A//www.xchuxing.com/article/124221)
    claim to utilize this method. Note that relevant raw sensor inputs, such as navigation
    and ego vehicle information, are omitted here.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端（e2e）神经网络（NN）规划器仍然构成一个模块化的自动驾驶（AD）设计，接受结构化的感知结果（以及可能的潜在特征）作为输入。该方法将预测、决策和规划融合到一个单一网络中。诸如[DeepRoute（2022）和华为（2024）](https://medium.com/p/ede324d78717/edit#:~:text=https%3A//www.xchuxing.com/article/124221)等公司声称采用此方法。请注意，这里省略了相关的原始传感器输入，如导航信息和自车信息。
- en: '![](../Images/44644c42a483b21510a189f25742521b.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44644c42a483b21510a189f25742521b.png)'
- en: A full autonomous driving stack with an e2e planner (chart made by author)
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 配备端到端规划器的完整自动驾驶堆栈（图表由作者制作）
- en: This e2e planner can be further developed into an end-to-end autonomous driving
    system that combines both perception and planning. This is what [Wayve’s LINGO-2
    (2024)](https://medium.com/p/ede324d78717/edit#:~:text=https%3A//wayve.ai/thinking/lingo%2D2%2Ddriving%2Dwith%2Dlanguage/)
    and Tesla’s FSDv12 (2024) claim to achieve.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 这个端到端（e2e）规划器可以进一步发展成一个结合感知与规划的端到端自动驾驶系统。这正是[Wayve的LINGO-2（2024）](https://medium.com/p/ede324d78717/edit#:~:text=https%3A//wayve.ai/thinking/lingo%2D2%2Ddriving%2Dwith%2Dlanguage/)和特斯拉的FSDv12（2024）声称实现的目标。
- en: The benefits of this approach are twofold. First, it addresses perception issues.
    There are many aspects of driving that we cannot easily model explicitly with
    commonly used perception interfaces. For example, it is quite challenging to handcraft
    a driving system to [nudge around a puddle of water](https://medium.com/p/ede324d78717/edit#:~:text=https%3A//x.com/AIDRIVR/status/1760841783708418094)
    or [slow down for dips or potholes](https://medium.com/p/ede324d78717/edit#:~:text=https%3A//x.com/AIDRIVR/status/1759843256513564997).
    While passing intermediate perception features might help, it may not fundamentally
    resolve the issue.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的好处有两个方面。首先，它解决了感知问题。驾驶中有许多方面我们无法通过常用的感知接口轻松地显式建模。例如，手工打造一个驾驶系统来[绕过水坑](https://medium.com/p/ede324d78717/edit#:~:text=https%3A//x.com/AIDRIVR/status/1760841783708418094)或[减速通过坑洼或裂缝](https://medium.com/p/ede324d78717/edit#:~:text=https%3A//x.com/AIDRIVR/status/1759843256513564997)是非常具有挑战性的。虽然传递中间的感知特征可能有所帮助，但它可能无法从根本上解决问题。
- en: Additionally, emergent behavior will likely help resolve corner cases more systematically.
    The intelligent handling of edge cases, such as the examples above, may result
    from the emergent behavior of large models.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，涌现行为可能帮助更系统地解决极端情况。大模型的涌现行为可能导致智能处理边缘情况，例如上述的例子。
- en: '![](../Images/7a33be1918eb98c773763a5fc0ee3b0c.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a33be1918eb98c773763a5fc0ee3b0c.png)'
- en: A full autonomous driving stack with a one-model e2e driver (chart made by author)
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 一个完整的自动驾驶堆栈包括一个一体化模型（e2e）驾驶员（图表由作者制作）
- en: My speculation is that, in its ultimate form, the end-to-end (e2e) driver would
    be a large vision and action-native multimodal model enhanced by Monte Carlo Tree
    Search (MCTS), assuming no computational constraints.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我的猜测是，在其最终形态下，端到端（e2e）驾驶员将是一个大规模视觉与行动原生的多模态模型，并通过蒙特卡罗树搜索（MCTS）增强，假设没有计算限制。
- en: A world model in autonomous driving, as of 2024 consensus, is typically a multimodal
    model covering at least vision and action modes (or a VA model). While language
    can be beneficial for accelerating training, adding controllability, and providing
    explainability, it is not essential. In its fully developed form, a world model
    would be a VLA (vision-language-action) model.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 到2024年共识为止，自动驾驶中的世界模型通常是一个多模态模型，至少覆盖视觉和行动模式（或VA模型）。虽然语言可以加速训练、增强可控性并提供可解释性，但它不是必需的。在其完全发展形态下，世界模型将是一个VLA（视觉-语言-行动）模型。
- en: 'There are at least two approaches to developing a world model:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 开发世界模型至少有两种方法：
- en: '**Video-Native Model**: Train a model to predict future video frames, conditioned
    on or outputting accompanying actions, as demonstrated by models like GAIA-1.'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**视频原生模型**：训练一个模型预测未来的视频帧，基于或输出相应的行动，正如GAIA-1等模型所展示的。'
- en: '**Multimodality Adaptors**: Start with a pretrained Large Language Model (LLM)
    and add multimodality adaptors, as seen in models like Lingo-2, RT2, or ApolloFM.
    These multimodal LLMs are not native to vision or action but require significantly
    less training resources.'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**多模态适配器**：从一个预训练的大型语言模型（LLM）开始，并添加多模态适配器，如Lingo-2、RT2或ApolloFM等模型所示。这些多模态LLM并非天生适用于视觉或行动，但需要的训练资源明显较少。'
- en: A world model can produce a policy itself through the action output, allowing
    it to drive the vehicle directly. Alternatively, MCTS can query the world model
    and use its policy outputs to guide the search. This World Model-MCTS approach,
    while much more computationally intensive, could have a higher ceiling in handling
    corner cases due to its explicit reasoning logic.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 一个世界模型可以通过行动输出自身生成策略，从而直接控制车辆。或者，MCTS可以查询世界模型并利用其策略输出引导搜索。虽然这种世界模型-MCTS方法计算密集度更高，但由于其明确的推理逻辑，它可能在处理极端情况时具有更高的上限。
- en: Can we do without prediction?
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们能做到不进行预测吗？
- en: Most current motion prediction modules represent the future trajectories of
    agents other than the ego vehicle as one or multiple discrete trajectories. It
    remains a question whether this prediction-planning interface is sufficient or
    necessary.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 当前大多数运动预测模块将自车之外的代理未来轨迹表示为一个或多个离散轨迹。是否这种预测-规划接口足够或必要，仍然是一个问题。
- en: In a classical modular pipeline, prediction is still needed. However, a predict-then-plan
    pipeline definitely caps the upper limit of autonomous driving systems, as discussed
    in the decision-making session. A more critical question is how to integrate this
    prediction module more effectively into the overall autonomous driving stack.
    Prediction should aid decision-making, and a queryable prediction module within
    an overall decision-making framework, such as MPDM and its variants, is preferred.
    There are no severe issues with concrete trajectory predictions as long as they
    are integrated correctly, such as through policy tree rollouts.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在经典的模块化管道中，预测仍然是必需的。然而，预测-再规划的管道确实限制了自动驾驶系统的上限，如在决策制定部分讨论的那样。一个更关键的问题是如何将这一预测模块更有效地整合进整个自动驾驶堆栈中。预测应该辅助决策制定，并且在整体决策框架中使用可查询的预测模块，如MPDM及其变种，是优选方案。只要正确整合，例如通过策略树展开，具体的轨迹预测没有严重问题。
- en: Another issue with prediction is that open-loop Key Performance Indicators (KPIs),
    such as Average Displacement Error (ADE) and Final Displacement Error (FDE), are
    not effective metrics as they fail to reflect the impact on planning. Instead,
    metrics like recall and precision at the intent level should be considered.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 预测的另一个问题是，开放式关键绩效指标（KPIs），例如平均位移误差（ADE）和最终位移误差（FDE），并不是有效的指标，因为它们无法反映对规划的影响。相反，应该考虑像意图层次的召回率和精确度等指标。
- en: In an end-to-end system, an explicit prediction module may not be necessary,
    but implicit supervision — along with other domain knowledge from a classical
    stack — can definitely help or at least boost the data efficiency of the learning
    system. Evaluating the prediction behavior, whether explicit or implicit, will
    also be helpful in debugging such an e2e system.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在端到端系统中，可能不需要显式预测模块，但隐式监督——以及来自经典堆栈的其他领域知识——绝对可以帮助，或者至少能提高学习系统的数据效率。评估预测行为，无论是显式的还是隐式的，也有助于调试这样的端到端系统。
- en: Can we do with just nets but no trees?
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们能仅依靠神经网络而没有树形结构吗？
- en: Conclusions First. For an assistant, neural networks (nets) can achieve very
    high, even superhuman performance. For agents, I believe that using a tree structure
    is still beneficial (though not necessarily a must).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 结论优先。对于助手，神经网络（nets）能够实现非常高的，甚至是超人类的表现。对于智能体，我认为使用树形结构仍然是有益的（尽管不一定是必须的）。
- en: First of all, trees can boost nets. Trees enhance the performance of a given
    network, whether it’s NN-based or not. In AlphaGo, even with a policy network
    trained via supervised learning and reinforcement learning, the overall performance
    was still inferior to the MCTS-based AlphaGo, which integrates the policy network
    as one component.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，树形结构可以增强神经网络。树形结构能够提升给定网络的性能，无论其是否基于神经网络。在AlphaGo中，即使使用通过监督学习和强化学习训练的策略网络，整体表现仍然不如基于蒙特卡罗树搜索的AlphaGo，后者将策略网络作为一个组件进行集成。
- en: Second, nets can distill trees. In AlphaGo, MCTS used both a value network and
    the reward from a fast rollout policy network to evaluate a node (state or board
    position) in the tree. The AlphaGo paper also mentioned that while a value function
    alone could be used, combining the results of the two yielded the best results.
    The value network essentially distilled the knowledge from the policy rollout
    by directly learning the state-value pair. This is akin to how humans distill
    the logical thinking of the slow System 2 into the fast, intuitive responses of
    System 1\. Daniel Kahneman, in his book “[Thinking, Fast and Slow](https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow),”
    describes how a chess master can quickly recognize patterns and make rapid decisions
    after years of practice, whereas a novice would require significant effort to
    achieve similar results. Similarly, the value network in AlphaGo was trained to
    provide a fast evaluation of a given board position.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，神经网络可以提炼树形结构。在AlphaGo中，蒙特卡罗树搜索（MCTS）使用了价值网络和来自快速回滚策略网络的奖励来评估树中的节点（状态或棋盘位置）。AlphaGo论文还提到，尽管单独使用价值函数也可以，但结合两者的结果会得到最佳效果。价值网络本质上是通过直接学习状态-价值对来提炼策略回滚的知识。这类似于人类如何将缓慢的系统2的逻辑思维提炼成快速、直觉性的系统1反应。丹尼尔·卡尼曼在他的书《[思考，快与慢](https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow)》中描述了一个国际象棋大师如何在多年练习后快速识别模式并做出迅速决策，而新手则需要付出相当大的努力才能达到类似的结果。类似地，AlphaGo中的价值网络被训练来提供对给定棋盘位置的快速评估。
- en: '![](../Images/94f5b0839feca2e4bcd4310d65598774.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94f5b0839feca2e4bcd4310d65598774.png)'
- en: 'Grandmaster-Level Chess Without Search (source: [DeepMind, 2024](https://arxiv.org/abs/2402.04494))'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 无需搜索的国际象棋大师级水平（来源：[DeepMind, 2024](https://arxiv.org/abs/2402.04494)）
- en: Recent papers explore the upper limits of this fast system with neural networks.
    The “[chess without search](https://arxiv.org/abs/2402.04494)” paper demonstrates
    that with sufficient data (prepared through tree search using a conventional algorithm),
    it is possible to achieve grandmaster-level proficiency. There is a clear “scaling
    law” related to data size and model size, indicating that as the amount of data
    and the complexity of the model increase, so does the proficiency of the system.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的论文探讨了神经网络在这一快速系统中的上限。《[无搜索的国际象棋](https://arxiv.org/abs/2402.04494)》一文证明，在有足够数据的情况下（这些数据通过传统算法的树搜索准备），可以达到大师级水平。数据量和模型规模与系统熟练度之间存在明显的“扩展法则”，表明随着数据量和模型复杂度的增加，系统的熟练度也会提高。
- en: 'So here we are with a power duo: trees boost nets, and nets distill trees.
    This positive feedback loop is essentially what AlphaZero uses to bootstrap itself
    to reach superhuman performance in multiple games.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们这里有一对强力组合：树提升网络，网络提炼树。这种正反馈循环本质上就是AlphaZero用来自我启动，达到在多种游戏中超越人类表现的机制。
- en: The same principles apply to the development of large language models (LLMs).
    For games, since we have clearly defined rewards as wins or losses, we can use
    forward rollout to determine the value of a certain action or state. For LLMs,
    the rewards are not as clear-cut as in the game of Go, so we rely on human preferences
    to rate the models via reinforcement learning with human feedback (RLHF). However,
    with models like ChatGPT already trained, we can use supervised fine-tuning (SFT),
    which is essentially imitation learning, to distill smaller yet still powerful
    models without RLHF.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的原则也适用于大规模语言模型（LLMs）的开发。对于游戏来说，由于我们有明确的奖励——胜利或失败，我们可以使用前向展开来确定某个动作或状态的价值。对于LLMs，奖励不像围棋那样明确，因此我们依赖人类偏好，通过人类反馈的强化学习（RLHF）来评估模型。然而，像ChatGPT这样的模型已经经过训练，我们可以使用监督微调（SFT），这本质上是模仿学习，用以提炼出更小但仍然强大的模型，而无需RLHF。
- en: Returning to the original question, nets can achieve extremely high performance
    with large quantities of high-quality data. This could be good enough for an assistant,
    depending on the tolerance for errors, but it may not be sufficient for an autonomous
    agent. For systems targeting driving assistance (ADAS), nets via imitation learning
    may be adequate.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 回到最初的问题，网络可以通过大量高质量的数据实现极高的性能。这对于一个助手可能已经足够，具体取决于对错误的容忍度，但对于一个自主智能体来说，可能还不够。对于面向驾驶辅助（ADAS）的系统，通过模仿学习的网络可能是足够的。
- en: Trees can significantly boost the performance of nets with an explicit reasoning
    loop, making them perhaps more suitable for fully autonomous agents. The extent
    of the tree or reasoning loop depends on the return on investment of engineering
    resources. For example, even one order of interaction can provide substantial
    benefits, as demonstrated in [TuSimple AI Day](https://www.bilibili.com/video/BV1Em4y1u7P7/?vd_source=73e03d3e246aa3ec284f028c7dcf0fa7).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 树可以通过显式的推理循环显著提升网络的性能，这使得它们可能更适合完全自主的智能体。树或推理循环的程度取决于工程资源的投入回报。例如，哪怕只有一个层次的交互，也能带来显著的好处，正如在[TuSimple
    AI Day](https://www.bilibili.com/video/BV1Em4y1u7P7/?vd_source=73e03d3e246aa3ec284f028c7dcf0fa7)中所展示的那样。
- en: Can we use LLMs to make decisions?
  id: totrans-295
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们可以使用LLMs做决策吗？
- en: From the summary below of the hottest representatives of AI systems, we can
    see that LLMs are not designed to perform decision-making. In essence, LLMs are
    trained to complete documents, and even SFT-aligned LLM assistants treat dialogues
    as a special type of document (completing a dialogue record).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 从下面关于当前最热AI系统代表的总结中，我们可以看到，LLMs并不是为了执行决策制定而设计的。本质上，LLMs是被训练来完成文档的，甚至是经过SFT对齐的LLM助手也将对话视为一种特殊类型的文档（完成一个对话记录）。
- en: '![](../Images/e0e938cc3c27089bf9c0be33a0a5ef29.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e0e938cc3c27089bf9c0be33a0a5ef29.png)'
- en: Representative AI products as of 2024 (chart made by author)
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 2024年代表性的AI产品（图表由作者制作）
- en: I do not fully agree with recent claims that LLMs are slow systems (System 2).
    They are unnecessarily slow in inference due to hardware constraints, but in their
    vanilla form, LLMs are fast systems as they cannot perform counterfactual checks.
    Prompting techniques such as Chain of Thought (CoT) or Tree of Thoughts (ToT)
    are actually simplified forms of MCTS, making LLMs function more like slower systems.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我不完全同意最近关于LLMs是慢速系统（系统2）的说法。由于硬件限制，它们在推理时确实不必要地慢，但在它们的原始形式中，LLMs是快速系统，因为它们无法进行反事实检查。诸如“思维链”（CoT）或“思维树”（ToT）之类的提示技术，实际上是简化版的蒙特卡洛树搜索（MCTS），使得LLMs的功能更像是较慢的系统。
- en: There is extensive research trying to integrate full-blown MCTS with LLMs. Specifically,
    [LLM-MCTS](https://arxiv.org/abs/2305.14078) (NeurIPS 2023) treats the LLM as
    a commonsense “world model” and uses LLM-induced policy actions as a heuristic
    to guide the search. LLM-MCTS outperforms both MCTS alone and policies induced
    by LLMs by a wide margin for complex, novel tasks. The [highly speculated Q-star
    from OpenAI](https://www.lesswrong.com/posts/JnM3EHegiBePeKkLc/possible-openai-s-q-breakthrough-and-deepmind-s-alphago-type)
    seems to follow the same approach of boosting LLMs with MCTS, as the name suggests.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 有大量研究尝试将完整的 MCTS 与大规模语言模型（LLMs）结合。具体来说，[LLM-MCTS](https://arxiv.org/abs/2305.14078)（NeurIPS
    2023）将 LLM 视为常识性的“世界模型”，并使用 LLM 引导的策略动作作为启发式方法来指导搜索。在复杂的新任务中，LLM-MCTS 在性能上远远超过了单独的
    MCTS 和 LLM 引导的策略。来自 [OpenAI 的高度推测 Q-star](https://www.lesswrong.com/posts/JnM3EHegiBePeKkLc/possible-openai-s-q-breakthrough-and-deepmind-s-alphago-type)
    看起来似乎采用了类似的方式，通过 MCTS 增强 LLM，就如其名称所示。
- en: The trend of evolution
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 演变趋势
- en: Below is a rough evolution of the planning stack in autonomous driving. It is
    rough as the listed solutions are not necessarily more advanced than the ones
    above, and their debut may not follow the exact chronological order. Nonetheless,
    we can observe general trends. Note that the listed representative solutions from
    the industry are based on my interpretation of various press releases and could
    be subject to error.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是自动驾驶规划栈的粗略演变。之所以说粗略，是因为列出的解决方案不一定比上面的更加先进，而且它们的首次出现可能并不按照严格的时间顺序排列。尽管如此，我们仍然可以观察到一些总体趋势。请注意，行业中列出的代表性解决方案基于我对各种新闻稿的解读，可能存在误差。
- en: One trend is the movement towards a more end-to-end design with more modules
    consolidated into one. We see the stack evolve from path-speed decoupled planning
    to joint spatiotemporal planning, and from a predict-then-plan system to a joint
    prediction and planning system. Another trend is the increasing incorporation
    of machine learning-based components, especially in the last three stages. These
    two trends converge towards an end-to-end NN planner (without perception) or even
    an end-to-end NN driver (with perception).
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 一种趋势是朝着更端到端的设计发展，将更多的模块整合成一个系统。我们看到规划栈从路径-速度解耦规划发展到联合时空规划，从预测-然后-规划系统发展到联合预测与规划系统。另一个趋势是越来越多地将基于机器学习的组件纳入其中，特别是在最后三个阶段。这两种趋势汇聚成一个端到端的神经网络规划器（没有感知）甚至是一个端到端的神经网络驾驶员（有感知）。
- en: '![](../Images/3e753016647b4fc5cd09a94aa0263393.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e753016647b4fc5cd09a94aa0263393.png)'
- en: A rough history of evolution of planning (Chart made by author)
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 规划演变的粗略历史（图表由作者制作）
- en: Takeaways
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重点
- en: '**ML as a Tool**: Machine learning is a tool, not a standalone solution. It
    can assist with planning even in current modular designs.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习作为工具**：机器学习是一种工具，而不是独立的解决方案。即使在当前的模块化设计中，它也能帮助进行规划。'
- en: '**Full Formulation**: Start with a full problem formulation, then make reasonable
    assumptions to balance performance and resources. This helps create a clear direction
    for a future-proof system design and allows for improvements as resources increase.
    Recall the transition from POMDP’s formulation to engineering solutions like AlphaGo’s
    MCTS and MPDM.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完整的公式化**：从完整的问题公式化开始，然后做出合理的假设，以平衡性能和资源。这有助于为未来的系统设计创造清晰的方向，并随着资源的增加进行改进。回想一下从
    POMDP 的公式化到像 AlphaGo 的 MCTS 和 MPDM 这样的工程解决方案的过渡。'
- en: '**Adapting Algorithms**: Theoretically beautiful algorithms (e.g., Dijkstra
    and Value Iteration) are great for understanding concepts but need adaptation
    for practical engineering (Value Iteration to MCTS as Dijkstra’s algorithm to
    Hybrid A-star).'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**算法的适应性**：理论上优美的算法（例如，Dijkstra 和价值迭代）在理解概念上非常有用，但在实际工程中需要适应（将价值迭代转化为 MCTS，就像将
    Dijkstra 算法转化为混合 A-star 一样）。'
- en: '**Deterministic vs. Stochastic**: Planning excels in resolving deterministic
    (not necessarily static) scenes. Decision-making in stochastic scenes is the most
    challenging task toward full autonomy.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**确定性与随机性**：规划在解决确定性（不一定是静态）场景方面表现出色。随机场景中的决策是朝着完全自主迈进的最具挑战性的任务。'
- en: '**Contingency Planning**: This can help merge multiple futures into a common
    action. It’s beneficial to be aggressive to the degree that you can always resort
    to a backup plan.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**应急规划**：这有助于将多个未来情景合并为一个共同的行动。采取积极进取的态度是有益的，这样你总能依靠备份计划。'
- en: '**End-to-end Models**: Whether an end-to-end model can solve full autonomy
    remains unclear. It may still need classical methods like MCTS. Neural networks
    can handle assistants, while trees can manage agents.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**端到端模型**：端到端模型是否能解决完全自动化问题尚不明确。它可能仍然需要经典方法，如MCTS。神经网络可以处理辅助任务，而树结构则能管理代理。'
- en: Acknowledgments
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢
- en: This blog post is heavily inspired by [Dr. Wenchao Ding](https://wenchaoding.github.io/personal/index.html)’s
    course on planning on [Shenlan Xueyuan (深蓝学院)](https://www.shenlanxueyuan.com/course/671).
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这篇博客文章深受[邓文超博士](https://wenchaoding.github.io/personal/index.html)在[深蓝学院](https://www.shenlanxueyuan.com/course/671)的规划课程启发。
- en: Thanks for the heavy and inspiring discussion with [Naiyan Wang](https://winsty.net/)
    and Jingwei Zhao. Thanks to [论文推土机](https://www.zhihu.com/people/george-reagan),
    [Invictus](https://www.zhihu.com/people/REX-X-96-9), [XXF](https://zhuanlan.zhihu.com/p/704847315),
    PYZ and JCL for giving critical feedback to the initial draft. Thanks for the
    insightful discussion with [Professor Wei Zhan](https://zhanwei.site/) from Berkeley
    on trends in academia.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 感谢[Naiyan Wang](https://winsty.net/)和Jingwei Zhao的深入和启发性讨论。感谢[论文推土机](https://www.zhihu.com/people/george-reagan)、[Invictus](https://www.zhihu.com/people/REX-X-96-9)、[XXF](https://zhuanlan.zhihu.com/p/704847315)、PYZ和JCL在初稿中提供的关键反馈。感谢与[伯克利大学的赵伟教授](https://zhanwei.site/)就学术界趋势进行的深刻讨论。
- en: Reference
  id: totrans-316
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[UniAD: Planning-oriented Autonomous Driving](https://arxiv.org/abs/2212.10156),
    CVPR 2023 best paper'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[UniAD: 面向规划的自动驾驶](https://arxiv.org/abs/2212.10156)，CVPR 2023 最佳论文'
- en: '[End-To-End Planning of Autonomous Driving in Industry and Academia: 2022–2023](https://arxiv.org/abs/2401.08658),
    Arxiv 2024'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自动驾驶行业与学术界端到端规划回顾：2022–2023](https://arxiv.org/abs/2401.08658)，Arxiv 2024'
- en: '[BEVGPT: Generative Pre-trained Large Model for Autonomous Driving Prediction,
    Decision-Making, and Planning](https://arxiv.org/abs/2310.10357), AAAI 2024'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[BEVGPT: 用于自动驾驶预测、决策和规划的生成预训练大模型](https://arxiv.org/abs/2310.10357)，AAAI 2024'
- en: '[Towards A General-Purpose Motion Planning for Autonomous Vehicles Using Fluid
    Dynamics](https://arxiv.org/abs/2406.05708), Arxiv 2024'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[基于流体动力学的自动驾驶通用运动规划](https://arxiv.org/abs/2406.05708)，Arxiv 2024'
- en: '[Tusimple AI day](https://www.bilibili.com/video/BV1Em4y1u7P7/?vd_source=73e03d3e246aa3ec284f028c7dcf0fa7),
    in Chinese with English subtitle on Bilibili, 2023/07'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Tusimple AI Day](https://www.bilibili.com/video/BV1Em4y1u7P7/?vd_source=73e03d3e246aa3ec284f028c7dcf0fa7)，中文带英文字幕在哔哩哔哩，2023年07月'
- en: '[Tech blog on joint spatiotemporal planning by Qcraft](https://zhuanlan.zhihu.com/p/551381336),
    in Chinese on Zhihu, 2022/08'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Qcraft 关于联合时空规划的技术博客](https://zhuanlan.zhihu.com/p/551381336)，中文在知乎，2022年08月'
- en: '[A review of the entire autonomous driving stack](https://zhuanlan.zhihu.com/p/53495492),
    in Chinese on Zhihu, 2018/12'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自动驾驶全栈回顾](https://zhuanlan.zhihu.com/p/53495492)，中文在知乎，2018年12月'
- en: '[Tesla AI Day Planning](https://zhuanlan.zhihu.com/p/572950979), in Chinese
    on Zhihu, 2022/10'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Tesla AI Day 规划](https://zhuanlan.zhihu.com/p/572950979)，中文在知乎，2022年10月'
- en: '[Technical blog on ApolloFM](https://mp.weixin.qq.com/s/8d1qXTm5v4H94HxAibp1dA),
    in Chinese by Tsinghua AIR, 2024'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[关于ApolloFM的技术博客](https://mp.weixin.qq.com/s/8d1qXTm5v4H94HxAibp1dA)，中文清华AIR发布，2024年'
- en: '[Optimal Trajectory Generation for Dynamic Street Scenarios in a Frenet Frame](https://www.semanticscholar.org/paper/Optimal-trajectory-generation-for-dynamic-street-in-Werling-Ziegler/6bda8fc13bda8cffb3bb426a73ce5c12cc0a1760),
    ICRA 2010'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在Frenet坐标系中为动态街道场景生成最优轨迹](https://www.semanticscholar.org/paper/Optimal-trajectory-generation-for-dynamic-street-in-Werling-Ziegler/6bda8fc13bda8cffb3bb426a73ce5c12cc0a1760)，ICRA
    2010'
- en: '[MP3: A Unified Model to Map, Perceive, Predict and Plan](https://arxiv.org/abs/2101.06806),
    CVPR 2021'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MP3: 一个统一的模型来映射、感知、预测和规划](https://arxiv.org/abs/2101.06806)，CVPR 2021'
- en: '[NMP: End-to-end Interpretable Neural Motion Planner](http://www.cs.toronto.edu/~wenjie/papers/cvpr19/nmp.pdf),
    CVPR 2019 oral'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NMP: 端到端可解释的神经运动规划器](http://www.cs.toronto.edu/~wenjie/papers/cvpr19/nmp.pdf)，CVPR
    2019 口头报告'
- en: '[Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly
    Unprojecting to 3D](https://arxiv.org/abs/2008.05711), ECCV 2020'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Lift, Splat, Shoot: 通过隐式反投影到 3D 编码任意相机装置的图像](https://arxiv.org/abs/2008.05711)，ECCV
    2020'
- en: '[CoverNet: Multimodal Behavior Prediction using Trajectory Sets](https://arxiv.org/abs/1911.10298),
    CVPR 2020'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CoverNet: 使用轨迹集进行多模态行为预测](https://arxiv.org/abs/1911.10298)，CVPR 2020'
- en: '[Baidu Apollo EM Motion Planner](https://arxiv.org/abs/1807.08048), Baidu,
    2018'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[百度 Apollo EM 动作规划器](https://arxiv.org/abs/1807.08048)，百度，2018年'
- en: '[AlphaGo: Mastering the game of Go with deep neural networks and tree search](https://www.nature.com/articles/nature16961),
    Nature 2016'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AlphaGo: 利用深度神经网络和树搜索掌握围棋](https://www.nature.com/articles/nature16961)，《自然》2016年'
- en: '[AlphaZero: A general reinforcement learning algorithm that masters chess,
    shogi, and Go through self-play](https://www.science.org/doi/full/10.1126/science.aar6404),
    Science 2017'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AlphaZero: 一种通用强化学习算法，通过自我对弈掌握国际象棋、将棋和围棋](https://www.science.org/doi/full/10.1126/science.aar6404)，Science
    2017'
- en: '[MuZero: Mastering Atari, Go, chess and shogi by planning with a learned model](https://www.nature.com/articles/s41586-020-03051-4),
    Nature 2020'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MuZero: 通过规划与学习模型掌握Atari、围棋、国际象棋和将棋](https://www.nature.com/articles/s41586-020-03051-4)，Nature
    2020'
- en: '[ToT: Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601),
    NeurIPS 2023 Oral'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ToT: 思维树：使用大语言模型进行深思熟虑的问题解决](https://arxiv.org/abs/2305.10601)，NeurIPS 2023
    Oral'
- en: '[CoT: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903),
    NeurIPS 2022'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CoT: 思维链提示引发大语言模型中的推理](https://arxiv.org/abs/2201.11903)，NeurIPS 2022'
- en: '[LLM-MCTS: Large Language Models as Commonsense Knowledge for Large-Scale Task
    Planning](https://arxiv.org/abs/2305.14078), NeurIPS 2023'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LLM-MCTS: 大语言模型作为大规模任务规划中的常识知识](https://arxiv.org/abs/2305.14078)，NeurIPS
    2023'
- en: '[MPDM: Multipolicy decision-making in dynamic, uncertain environments for autonomous
    driving](https://ieeexplore.ieee.org/document/7139412), ICRA 2015'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MPDM: 动态不确定环境下的多策略决策制定用于自动驾驶](https://ieeexplore.ieee.org/document/7139412)，ICRA
    2015'
- en: '[MPDM2: Multipolicy Decision-Making for Autonomous Driving via Changepoint-based
    Behavior Prediction](https://www.roboticsproceedings.org/rss11/p43.pdf), RSS 2015'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MPDM2: 基于变点行为预测的自动驾驶多策略决策制定](https://www.roboticsproceedings.org/rss11/p43.pdf)，RSS
    2015'
- en: '[MPDM3: Multipolicy decision-making for autonomous driving via changepoint-based
    behavior prediction: Theory and experiment](https://link.springer.com/article/10.1007/s10514-017-9619-z),
    RSS 2017'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MPDM3: 基于变点行为预测的自动驾驶多策略决策制定：理论与实验](https://link.springer.com/article/10.1007/s10514-017-9619-z)，RSS
    2017'
- en: '[EUDM: Efficient Uncertainty-aware Decision-making for Automated Driving Using
    Guided Branching](https://arxiv.org/abs/2003.02746), ICRA 2020'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[EUDM: 使用引导分支的高效不确定性意识决策制定用于自动驾驶](https://arxiv.org/abs/2003.02746)，ICRA 2020'
- en: '[MARC: Multipolicy and Risk-aware Contingency Planning for Autonomous Driving](https://arxiv.org/abs/2308.12021),
    RAL 2023'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MARC: 自动驾驶的多策略和风险意识应急规划](https://arxiv.org/abs/2308.12021)，RAL 2023'
- en: '[EPSILON: An Efficient Planning System for Automated Vehicles in Highly Interactive
    Environments](https://arxiv.org/abs/2108.07993), TRO 2021'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[EPSILON: 高度互动环境下自动驾驶车辆的高效规划系统](https://arxiv.org/abs/2108.07993)，TRO 2021'
