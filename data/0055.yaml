- en: Leverage KeyBERT, HDBSCAN and Zephyr-7B-Beta to Build a Knowledge Graph
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用 KeyBERT、HDBSCAN 和 Zephyr-7B-Beta 构建知识图谱
- en: 原文：[https://towardsdatascience.com/leverage-keybert-hdbscan-and-zephyr-7b-beta-to-build-a-knowledge-graph-33d7534ee01b?source=collection_archive---------0-----------------------#2024-01-07](https://towardsdatascience.com/leverage-keybert-hdbscan-and-zephyr-7b-beta-to-build-a-knowledge-graph-33d7534ee01b?source=collection_archive---------0-----------------------#2024-01-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/leverage-keybert-hdbscan-and-zephyr-7b-beta-to-build-a-knowledge-graph-33d7534ee01b?source=collection_archive---------0-----------------------#2024-01-07](https://towardsdatascience.com/leverage-keybert-hdbscan-and-zephyr-7b-beta-to-build-a-knowledge-graph-33d7534ee01b?source=collection_archive---------0-----------------------#2024-01-07)
- en: '*LLM-enhanced natural language processing and traditional machine learning
    techniques are used to extract structure and to build a knowledge graph from unstructured
    corpus.*'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*增强型大语言模型自然语言处理与传统机器学习技术结合，用于从非结构化语料库中提取结构并构建知识图谱。*'
- en: '[](https://medium.com/@silviaonofrei?source=post_page---byline--33d7534ee01b--------------------------------)[![Silvia
    Onofrei](../Images/198b04b2063b4269eaff52402dc5f8d5.png)](https://medium.com/@silviaonofrei?source=post_page---byline--33d7534ee01b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--33d7534ee01b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--33d7534ee01b--------------------------------)
    [Silvia Onofrei](https://medium.com/@silviaonofrei?source=post_page---byline--33d7534ee01b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@silviaonofrei?source=post_page---byline--33d7534ee01b--------------------------------)[![Silvia
    Onofrei](../Images/198b04b2063b4269eaff52402dc5f8d5.png)](https://medium.com/@silviaonofrei?source=post_page---byline--33d7534ee01b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--33d7534ee01b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--33d7534ee01b--------------------------------)
    [Silvia Onofrei](https://medium.com/@silviaonofrei?source=post_page---byline--33d7534ee01b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--33d7534ee01b--------------------------------)
    ·19 min read·Jan 7, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--33d7534ee01b--------------------------------)
    ·19 分钟阅读·2024年1月7日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f81dc37979040a1df7f80c88c9bfba77.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f81dc37979040a1df7f80c88c9bfba77.png)'
- en: '[Designed by Freepik](https://www.freepik.com/)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[设计者：Freepik](https://www.freepik.com/)'
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: While the Large Language Models (LLMs) are useful and skilled tools, relying
    entirely on their output is not always advisable as they often require verification
    and grounding. However, merging traditional NLP methods with the capabilities
    of generative AI typically yields satisfactory results. An excellent example of
    this synergy is the enhancement of KeyBERT with KeyLLM for keyword extraction.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大型语言模型（LLMs）是有用且高效的工具，但完全依赖它们的输出并不总是明智的，因为它们通常需要验证和基础支持。然而，将传统的自然语言处理方法与生成型人工智能的能力相结合，通常能取得令人满意的结果。一个很好的例子是，将
    KeyBERT 与 KeyLLM 相结合以进行关键词提取。
- en: In this blog, I intend to explore the efficacy of combining traditional NLP
    and machine learning techniques with the versatility of LLMs. This exploration
    includes integrating simple keyword extraction using KeyBERT, sentence embeddings
    with BERT, and employing UMAP for dimensionality reduction coupled with HDBSCAN
    for clustering. All these are used in conjunction with Zephyr-7B-Beta, a highly
    performant LLM. The findings are uploaded into a knowledge graph for enhanced
    analysis and discovery.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客中，我打算探讨将传统的自然语言处理和机器学习技术与大型语言模型的多功能性相结合的有效性。这一探索包括使用 KeyBERT 进行简单的关键词提取，利用
    BERT 进行句子嵌入，以及使用 UMAP 进行降维，结合 HDBSCAN 进行聚类。所有这些技术与高性能的 Zephyr-7B-Beta 一起使用，最终将结果上传到知识图谱中，以便进行更深入的分析和发现。
- en: My goal is to develop structure on a corpus of unstructured arXiv article titles
    in computer science. I selected these articles based on abstract length, not expecting
    inherent topics clusters. Indeed, a preliminary community analysis revealed nearly
    as many clusters as articles. Consequently, I’m exploring a different approach
    to linking these titles. Despite lacking clear communities, the titles often share
    common words. By extracting and clustering these keywords, I aim to uncover underlying
    connections between the titles, offering a versatile strategy for structuring
    the dataset.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我的目标是开发一个针对计算机科学领域非结构化 arXiv 文章标题的结构化方法。我根据摘要的长度选择了这些文章，而并非期待内在的主题聚类。事实上，初步的社区分析揭示了几乎与文章数量相等的聚类。因此，我正在探索另一种将这些标题联系起来的方法。尽管缺乏明显的社区，这些标题通常共享相同的词汇。通过提取并聚类这些关键词，我旨在揭示标题之间的潜在联系，为数据集结构化提供一种多功能的策略。
- en: 'To simplify and enhance data exploration, I upload my results in a Neo4j knowledge
    graph. Here’s a snapshot of the output:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化并增强数据探索，我将结果上传至 Neo4j 知识图谱。以下是输出的快照：
- en: '![](../Images/96e2d0b9ac4a38c4edcf1392d6108b9f.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96e2d0b9ac4a38c4edcf1392d6108b9f.png)'
- en: 'The two purple nodes represent the titles: “Cores of Countably Categorical
    Structures” (left) and “Transforming Structures by Set Interpretations” (right).
    They are linked by the common theme of “Mathematical Logic” (tan node) via the
    keyword “structures”. — Image by author —'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个紫色节点代表标题：“可数范畴结构的核心”（左）和“通过集合解释转化结构”（右）。它们通过“数学逻辑”（浅褐色节点）这一共同主题，通过关键词“结构”连接在一起。—
    图像由作者提供 —
- en: 'Outlined below are the project’s steps:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是项目的步骤：
- en: '![](../Images/c16d1bc3d9daa4b48d465d17360fc24e.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c16d1bc3d9daa4b48d465d17360fc24e.png)'
- en: — Diagram by author —
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: — 图示由作者提供 —
- en: Collect and parse the dataset, focusing on titles while retaining the abstracts
    for context.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集并解析数据集，重点关注标题，同时保留摘要以提供上下文。
- en: Employ KeyBERT to extract candidate keywords, which are then refined using KeyLLM,
    based on Zephyr-7B-Beta, to generate a list of enhanced keywords and keyphrases.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 KeyBERT 提取候选关键词，然后基于 Zephyr-7B-Beta，通过 KeyLLM 对其进行优化，生成增强后的关键词和关键短语列表。
- en: Gather all extracted keywords and keyphrases and cluster them using HDBSCAN.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集所有提取的关键词和关键短语，并使用 HDBSCAN 对它们进行聚类。
- en: Use Zephyr-7B-Beta again, to derive labels and descriptions for each cluster.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 再次使用 Zephyr-7B-Beta，为每个聚类推导标签和描述。
- en: Combine these elements in a knowledge graph whith nodes representing Articles,
    Keywords and (cluster) Topics.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将这些元素结合到知识图谱中，节点代表文章、关键词和（聚类）主题。
- en: It’s important to note that each step in this process offers the flexibility
    to experiment with alternative methods, algorithms, or models.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，过程中的每一步都提供了实验替代方法、算法或模型的灵活性。
- en: The work is done in a Google Colab Pro with a V100 GPU and High RAM setting
    for the steps involving LLM. The notebook is divided into self-contained sections,
    most of which can be executed independently, minimizing dependency on previous
    steps. Data is saved after each section, allowing continuation in a new session
    if needed. Additionally, the parsed dataset and the Python modules, are readily
    available in this [Github repository.](https://github.com/SolanaO/Blogs_Content/tree/master/keyllm_neo4j)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 工作在 Google Colab Pro 上进行，配备 V100 GPU 和高内存设置，用于涉及 LLM 的步骤。笔记本被划分为自包含的部分，其中大多数部分可以独立执行，最小化对先前步骤的依赖。每个部分执行完毕后都会保存数据，以便在需要时能够在新会话中继续。此外，解析后的数据集和
    Python 模块可以在这个 [Github 仓库](https://github.com/SolanaO/Blogs_Content/tree/master/keyllm_neo4j)
    中找到。
- en: Data Preparation
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: I use a subset of the [arXiv Dataset](https://www.kaggle.com/datasets/Cornell-University/arxiv)
    that is openly available on the Kaggle platform and primarly maintained by Cornell
    University. In a machine readable format, it contains a repository of 1.7 million
    scholarly papers across STEM, with relevant features such as article titles, authors,
    categories, abstracts, full text PDFs, and more. It is updated regularly.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用的是一个来自 [arXiv 数据集](https://www.kaggle.com/datasets/Cornell-University/arxiv)
    的子集，该数据集公开可用并主要由康奈尔大学维护。以机器可读格式提供，包含170万篇跨学科的学术论文，涵盖STEM领域，并提供文章标题、作者、类别、摘要、完整文本PDF等相关特征。该数据集定期更新。
- en: 'The dataset is clean and in an easy to use format, so we can focus on our task,
    without spending too much time on data preprocessing. To further simplify the
    data preparation process, I built a Python module that performs the relevant steps.
    It can be found at `[utils/arxiv_parser.py](https://github.com/SolanaO/Blogs_Content/blob/master/keyllm_neo4j/utils/arxiv_parser.py)`
    if you want to take a peek at the code, otherwise follow along the Google Colab:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集已清理完毕，并且格式简洁易用，因此我们可以集中精力处理任务，而无需花费过多时间进行数据预处理。为了进一步简化数据准备过程，我构建了一个Python模块，执行相关步骤。如果你想查看代码，可以在`[utils/arxiv_parser.py](https://github.com/SolanaO/Blogs_Content/blob/master/keyllm_neo4j/utils/arxiv_parser.py)`找到，或者继续使用Google
    Colab：
- en: download the zipped arXiv file (1.2 GB) in the directory of your choice which
    is labelled `data_path`,
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载压缩的arXiv文件（1.2 GB），并选择一个目录进行存储，该目录标记为`data_path`，
- en: download the `arxiv_parser.py` in the directory `utils`,
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载`arxiv_parser.py`到`utils`目录，
- en: import and initialize the module in your Google Colab notebook,
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的Google Colab笔记本中导入并初始化模块，
- en: 'unzip the file, this will extract a 3.7 GB file: `archive-metadata-oai-snapshot.json`,'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解压文件，这将提取出一个3.7 GB的文件：`archive-metadata-oai-snapshot.json`，
- en: specify a general topic (I work with `cs` which stands for computer science),
    so you’ll have a more maneagable size data,
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指定一个一般主题（我使用`cs`，即计算机科学），这样你将拥有一个更易于管理的数据集，
- en: choose the features to keep (there are 14 features in the downloaded dataset),
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择要保留的特征（下载的数据集中有14个特征），
- en: the abstracts can vary in length quite a bit, so I added the option of selecting
    entries for which the number of tokens in the abstract is in a given interval
    and used this feature to downsize the dataset,
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摘要的长度可能差异较大，因此我添加了一个选项，可以选择摘要中的token数量在给定区间内的条目，并利用此功能来缩小数据集的规模，
- en: although I choose to work with the `title` feature, there is an option to take
    the more common approach of concatenating the title and the abstact in a single
    feature denoted `corpus` .
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管我选择使用`title`特征，但也有一个选项可以采用更常见的方法，即将标题和摘要合并为一个单一特征，称为`corpus`。
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: With the options above I extract a dataset of 983 computer science articles.
    We are ready to move to the next step.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述选项，我提取了一个包含983篇计算机科学文章的数据集。我们准备好进入下一步。
- en: If you want to skip the data processing steps, you may use the `cs` dataset,
    available in the Github repository.
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你想跳过数据处理步骤，可以使用`cs`数据集，该数据集可以在Github仓库中找到。
- en: Keyword Extraction with KeyBERT and KeyLLM
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用KeyBERT和KeyLLM进行关键词提取
- en: The Method
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法
- en: '[KeyBERT](https://maartengr.github.io/KeyBERT/guides/quickstart.html) is a
    method that extracts keywords or keyphrases from text. It uses document and word
    embeddings to find the sub-phrases that are most similar to the document, via
    cosine similarity. KeyLLM is another minimal method for keyword extraction but
    it is based on LLMs. Both methods are developed and maintained by Maarten Grootendorst.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[KeyBERT](https://maartengr.github.io/KeyBERT/guides/quickstart.html)是一种从文本中提取关键词或关键短语的方法。它利用文档和单词的嵌入，通过余弦相似度找到与文档最相似的子短语。KeyLLM是另一种最小化的关键词提取方法，但它基于LLM。两种方法都由Maarten
    Grootendorst开发和维护。'
- en: The two methods can be combined for enhanced results. Keywords extracted with
    KeyBERT are fine-tuned through KeyLLM. Conversely, candidate keywords identified
    through traditional NLP techniques help grounding the LLM, minimizing the generation
    of undesired outputs.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法可以结合使用以获得更好的结果。通过KeyBERT提取的关键词可以通过KeyLLM进行微调。相反，通过传统NLP技术识别的候选关键词有助于为LLM提供基础，从而最小化不希望产生的输出。
- en: For details on different ways of using KeyLLM see [Maarten Grootendorst, Introducing
    KeyLLM — Keyword Extraction with LLMs](/introducing-keyllm-keyword-extraction-with-llms-39924b504813).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 有关使用KeyLLM的不同方式，请参见[Maarten Grootendorst, 介绍KeyLLM — 使用LLM进行关键词提取](/introducing-keyllm-keyword-extraction-with-llms-39924b504813)。
- en: '![](../Images/f59f1b8d7f5ad0fadf9cd40a15394682.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f59f1b8d7f5ad0fadf9cd40a15394682.png)'
- en: — Diagram by author —
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: — 作者制作的图表 —
- en: 'Use KeyBERT [[source](https://github.com/MaartenGr/KeyBERT/blob/master/keybert/_model.py)]
    to extract keywords from each document — these are the candidate keywords provided
    to LLM to fine-tune:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 使用KeyBERT [[source](https://github.com/MaartenGr/KeyBERT/blob/master/keybert/_model.py)]
    从每篇文档中提取关键词——这些是提供给LLM进行微调的候选关键词：
- en: documents are embedded using Sentence Transformers to build a document level
    representation,
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Sentence Transformers对文档进行嵌入，以构建文档级别的表示。
- en: word embeddings are extracted for N-grams words/phrases,
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为N-gram单词/短语提取词嵌入，
- en: cosine similarity is used to find the words or phrases that are most similar
    to each document.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用余弦相似度来查找与每个文档最相似的单词或短语。
- en: 'Use KeyLLM [[source](https://github.com/MaartenGr/KeyBERT/blob/master/keybert/_llm.py)]
    to finetune the kewords extracted by KeyBERT via text generation with transformers
    [[source](https://github.com/MaartenGr/KeyBERT/blob/master/keybert/llm/_textgeneration.py)]:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用KeyLLM[[source](https://github.com/MaartenGr/KeyBERT/blob/master/keybert/_llm.py)]微调通过KeyBERT提取的关键词，利用[transformers进行文本生成](https://github.com/MaartenGr/KeyBERT/blob/master/keybert/llm/_textgeneration.py)[[source](https://github.com/MaartenGr/KeyBERT/blob/master/keybert/llm/_textgeneration.py)]：
- en: the community detection method in Sentence Transformers [[source](https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/util.py)]
    groups the similar documents, so we will extract keywords only from one document
    in each group,
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sentence Transformers中的社区检测方法[[source](https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/util.py)]将相似文档分组，因此我们只会从每个组中的一个文档中提取关键词，
- en: the candidate keywords are provided the LLM which fine-tunes the keywords for
    each cluster.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 候选关键词由LLM提供，LLM为每个聚类微调关键词。
- en: Besides Sentence Transformers, KeyBERT supports other embedding models, see
    [[here](https://maartengr.github.io/KeyBERT/guides/embeddings.html)].
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 除了Sentence Transformers，KeyBERT还支持其他嵌入模型，见[[此处](https://maartengr.github.io/KeyBERT/guides/embeddings.html)]。
- en: ''
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Sentence Transformers facilitate community detection by using a specified threshold.
    When documents lack inherent clusters, clear groupings may not emerge. In my case,
    out of 983 titles, approximately 800 distinct communities were identified. More
    naturally clustered data tends to yield better-defined communities.
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Sentence Transformers通过使用指定的阈值来促进社区检测。当文档缺乏固有的聚类时，可能不会出现明确的分组。在我的案例中，从983个标题中，大约识别出了800个不同的社区。更加自然聚类的数据往往能产生定义更明确的社区。
- en: The Large Language Model
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大型语言模型
- en: 'After experimting with various smaller LLMs, I choose [Zephyr-7B-Beta](https://arxiv.org/pdf/2310.16944.pdf)
    for this project. This model is based on [Mistral-7B](https://mistral.ai/news/announcing-mistral-7b/),
    and it is one of the first models fine-tuned with Direct Preference Optimization
    (DPO). It not only outperforms other models in its class but also surpasses Llama2–70B
    on some benchmarks. For more insights on this LLM take a look at [Benjamin Marie,
    Zephyr 7B Beta: A Good Teacher is All You Need](/zephyr-7b-beta-a-good-teacher-is-all-you-need-c931fcd0bfe7).
    Although it’s feasible to use the model directly on a Google Colab Pro, I opted
    to work with a GPTQ quantized version prepared by [TheBloke](https://huggingface.co/TheBloke).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '在对各种较小的LLM进行实验后，我选择了[Zephyr-7B-Beta](https://arxiv.org/pdf/2310.16944.pdf)用于本项目。该模型基于[Mistral-7B](https://mistral.ai/news/announcing-mistral-7b/)，并且是首批使用直接偏好优化（DPO）进行微调的模型之一。它不仅在同类模型中表现优异，而且在一些基准测试中超过了Llama2–70B。欲了解有关此LLM的更多见解，请查看[Benjamin
    Marie, Zephyr 7B Beta: 一个好老师就是你所需要的一切](/zephyr-7b-beta-a-good-teacher-is-all-you-need-c931fcd0bfe7)。虽然可以直接在Google
    Colab Pro上使用该模型，但我选择了使用由[TheBloke](https://huggingface.co/TheBloke)准备的GPTQ量化版本。'
- en: Start by downloading the model and its tokenizer following the instructions
    provided in the [model card:](https://huggingface.co/TheBloke/zephyr-7B-beta-GPTQ)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 首先按照[模型卡片](https://huggingface.co/TheBloke/zephyr-7B-beta-GPTQ)中的说明下载模型及其分词器
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Additionally, build the text generation pipeline:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，构建文本生成管道：
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The Keyword Extraction Prompt
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键词提取提示
- en: 'Experimentation is key in this step. Finding the optimal prompt requires some
    trial and error, and the performance depends on the chosen model. Let’s not forget
    that LLMs are probabilistic, so it is not guaranteed that they will return the
    same output every time. To develop the prompt below, I relied on both experimentation
    and the following considerations:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程中的实验至关重要。寻找最佳提示需要一些试验和错误，性能取决于所选择的模型。不要忘记，LLM是基于概率的，因此无法保证每次都返回相同的输出。为了开发下面的提示，我依赖了实验以及以下考虑因素：
- en: 'the prompt template provided in the [model card](https://huggingface.co/TheBloke/zephyr-7B-beta-GPTQ):'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[模型卡片](https://huggingface.co/TheBloke/zephyr-7B-beta-GPTQ)中提供的提示模板：'
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: the suggestions from the [KeyLLM blogpost](https://medium.com/towards-data-science/introducing-keyllm-keyword-extraction-with-llms-39924b504813)
    and from the [documentation](https://maartengr.github.io/KeyBERT/guides/keyllm.html#2-extract-keywords-with-keyllm),
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自[KeyLLM博客文章](https://medium.com/towards-data-science/introducing-keyllm-keyword-extraction-with-llms-39924b504813)和[文档](https://maartengr.github.io/KeyBERT/guides/keyllm.html#2-extract-keywords-with-keyllm)中的建议，
- en: some experimentation with ChatGPT and KeyBERT to build an example,
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对ChatGPT和KeyBERT进行了一些实验，以构建示例，
- en: the [code for text_generation wrapper for KeyLLM](https://github.com/MaartenGr/KeyBERT/blob/master/keybert/llm/_textgeneration.py).
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KeyLLM文本生成封装的代码](https://github.com/MaartenGr/KeyBERT/blob/master/keybert/llm/_textgeneration.py)。'
- en: 'And here is the prompt I use to fine-tune the keywords extracted with KeyBERT:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我用来微调通过KeyBERT提取的关键词的提示：
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Keyword Extraction and Parsing
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键词提取与解析
- en: We now have everything needed to proceed with the keyword extraction. Let me
    remind you, that I work with the titles, so the input documents are short, staying
    well within the token limits for the BERT embeddings.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们拥有了进行关键词提取所需的一切。让我提醒你，我处理的是标题，因此输入文档较短，完全在BERT嵌入的令牌限制范围内。
- en: Start with creating a [TextGeneration pipeline wrapper](https://github.com/MaartenGr/KeyBERT/blob/master/keybert/llm/_textgeneration.py)
    for the LLM and instantiate [KeyBERT](https://github.com/MaartenGr/KeyBERT/blob/master/keybert/_model.py).
    Choose the embedding model. If no embedding model is specified, the default model
    is `all-MiniLM-L6-v2`. In this case, I select the highest-performant pretrained
    model for sentence embeddings, see [here](https://www.sbert.net/docs/pretrained_models.html)
    for a complete list.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 从创建[TextGeneration管道封装器](https://github.com/MaartenGr/KeyBERT/blob/master/keybert/llm/_textgeneration.py)开始，为LLM实例化[KeyBERT](https://github.com/MaartenGr/KeyBERT/blob/master/keybert/_model.py)。选择嵌入模型。如果未指定嵌入模型，默认模型为`all-MiniLM-L6-v2`。在这种情况下，我选择了句子嵌入的最高性能预训练模型，完整列表请参见[这里](https://www.sbert.net/docs/pretrained_models.html)。
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Recall that the dataset was prepared and saved as a pandas dataframe `df`.
    To process the titles, just call the `extract_keywords` method:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，数据集已经准备并保存为pandas数据框`df`。要处理标题，只需调用`extract_keywords`方法：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `threshold` parameter determines the minimum similarity required for documents
    to be grouped into the same community. A higher value will group nearly identical
    documents, while a lower value will cluster documents covering similar topics.
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`threshold` 参数决定了将文档分组到同一社区所需的最小相似度。较高的值会将几乎相同的文档分组，而较低的值则会将涵盖相似主题的文档聚类在一起。'
- en: ''
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The choice of embeddings significantly influences the appropriate threshold,
    so it’s advisable to consult the model card for guidance. I’m grateful to Maarten
    Grootendorst for highlighting this aspect, as can be seen [here](https://github.com/MaartenGr/KeyBERT/issues/190).
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 嵌入模型的选择显著影响合适的阈值，因此建议查阅模型卡片以获取指导。感谢Maarten Grootendorst强调这一点，正如[这里](https://github.com/MaartenGr/KeyBERT/issues/190)所示。
- en: ''
  id: totrans-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It’s important to note that my observations apply exclusively to sentence transformers,
    as I haven’t experimented with other types of embeddings.
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 需要注意的是，我的观察仅适用于句子变换器，因为我尚未尝试其他类型的嵌入模型。
- en: 'Let’s take a look at some outputs:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一些输出：
- en: '![](../Images/c770ad00ee29463c4e22dd7621eb95fa.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c770ad00ee29463c4e22dd7621eb95fa.png)'
- en: '**Comments**:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**评论**：'
- en: In the second example provided here, we observe keywords or keyphrases not present
    in the original text. If this poses a problem in your case, consider enabling
    `check_vocab=True` as done [[here](https://maartengr.github.io/KeyBERT/guides/keyllm.html#2-extract-keywords-with-keyllm)].
    However, it's important to remember that these results are highly influenced by
    the LLM choice, with quantization having a minor effect, as well as the construction
    of the prompt.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这里提供的第二个示例中，我们观察到原始文本中没有出现的关键词或关键短语。如果这对你构成问题，可以考虑启用`check_vocab=True`，如[[这里](https://maartengr.github.io/KeyBERT/guides/keyllm.html#2-extract-keywords-with-keyllm)]所做。然而，需要记住的是，这些结果受LLM选择的影响很大，量化的影响较小，提示的构造也有一定影响。
- en: With longer input documents, I noticed more deviations from the required output.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在处理较长的输入文档时，我注意到输出与预期结果的偏差增多。
- en: 'One consistent observation is that the number of keywords extracted often deviates
    from five. It’s common to encounter titles with fewer extracted keywords, especially
    when the input is brief. Conversely, some titles yield as many as 10 extracted
    keywords. Let’s examine the distribution of keyword counts for this run:'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个一致的观察是，提取的关键词数量通常偏离五个。尤其在输入较简短时，常会遇到提取的关键词较少的标题。相反，一些标题会提取出多达10个关键词。让我们来看一下此次运行的关键词数量分布：
- en: '![](../Images/77b2a23d02c46e836763d01129a71041.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77b2a23d02c46e836763d01129a71041.png)'
- en: 'These variations complicate the subsequent parsing steps. There are a few options
    for addressing this: we could investigate these cases in detail, request the model
    to revise and either trim or reiterate the keywords, or simply overlook these
    instances and focus solely on titles with exactly five keywords, as I’ve decided
    to do for this project.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这些变化使得后续的解析步骤变得复杂。对此，有几种解决方案：我们可以详细调查这些情况，要求模型修正并修剪或重新整理关键词，或者简单地忽略这些情况，只专注于包含正好五个关键词的标题，正如我为这个项目决定的那样。
- en: Clustering Keywords with HDBSCAN
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 HDBSCAN 进行关键词聚类
- en: 'The following step is to cluster the keywords and keyphrases to reveal common
    topics across articles. To accomplish this I use two algorithms: UMAP for dimensionality
    reduction and HDBSCAN for clustering.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的步骤是对关键词和关键短语进行聚类，以揭示文章中的共同话题。为此，我使用了两种算法：UMAP 用于降维，HDBSCAN 用于聚类。
- en: 'The Algorithms: HDBSCAN and UMAP'
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 算法：HDBSCAN 和 UMAP
- en: '[Hierarchical Density-Based Spatial Clustering of Applications with Noise](https://hdbscan.readthedocs.io/en/latest/basic_hdbscan.html#)
    or **HDBSCAN**, is a highly performant unsupervised algorithm designed to find
    patterns in the data. It finds the optimal clusters based on their density and
    proximity. This is especially useful in cases where the number and shape of the
    clusters may be unknown or difficult to determine.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[基于密度的层次空间聚类与噪声](https://hdbscan.readthedocs.io/en/latest/basic_hdbscan.html#)
    或 **HDBSCAN**，是一种高效的无监督算法，旨在发现数据中的模式。它根据聚类的密度和接近度找到最优的聚类。这在聚类的数量和形状可能未知或难以确定的情况下尤其有用。'
- en: The results of HDBSCAN clustering algorithm can vary if you run the algorithm
    multiple times with the same hyperparameters. This is because HDBSCAN is a stochastic
    algorithm, which means that it involves some degree of randomness in the clustering
    process. Specifically, HDBSCAN uses a random initialization of the cluster hierarchy,
    which can result in different cluster assignments each time the algorithm is run.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用相同的超参数多次运行 HDBSCAN 聚类算法，其结果可能会有所不同。这是因为 HDBSCAN 是一个随机算法，这意味着聚类过程涉及一定程度的随机性。具体来说，HDBSCAN
    使用随机初始化聚类层次结构，这可能导致每次运行算法时产生不同的聚类分配。
- en: However, the degree of variation between different runs of the algorithm can
    depend on several factors, such as the dataset, the hyperparameters, and the seed
    value used for the random number generator. In some cases, the variation may be
    minimal, while in other cases it can be significant.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，算法在不同运行之间的变化程度可能取决于多个因素，例如数据集、超参数以及用于随机数生成器的种子值。在某些情况下，变化可能很小，而在其他情况下则可能较大。
- en: There are two clustering options with HDBSCAN.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: HDBSCAN 提供了两种聚类选项。
- en: The primary clustering algorithm, denoted `hard_clustering` assigns each data
    point to a cluster or labels it as noise. This is a hard assignment; there are
    no mixed memberships. This approach might result in one large cluster categorized
    as noise (cluster labelled -1) and numerous smaller clusters. Fine-tuning the
    hyperparameters is crucial [[see here](https://hdbscan.readthedocs.io/en/latest/faq.html)],
    as it is selecting an embedding model specifically tailored for the domain. Take
    a look at the associated [Google Colab](https://github.com/SolanaO/Blogs_Content/blob/master/keyllm_neo4j/KeyBERT_LLM_Neo4j.ipynb)
    for the results of hard clustering on the project’s dataset.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主要的聚类算法，标记为 `hard_clustering`，将每个数据点分配到一个聚类或标记为噪声。这是硬性分配；没有混合的隶属关系。这种方法可能导致一个大的聚类被归类为噪声（聚类标签为
    -1），而其他则为许多较小的聚类。微调超参数至关重要 [[参见这里](https://hdbscan.readthedocs.io/en/latest/faq.html)]，因为它选择了一个专门为该领域量身定制的嵌入模型。请查看相关的
    [Google Colab](https://github.com/SolanaO/Blogs_Content/blob/master/keyllm_neo4j/KeyBERT_LLM_Neo4j.ipynb)，查看该项目数据集上硬聚类的结果。
- en: '`Soft clustering` on the other side is a newer feature of the HDBSCAN library.
    In this approach points are not assigned cluster labels, but instead they are
    assigned a vector of probabilities. The length of the vector is equal to the number
    of clusters found. The probability value at the entry of the vector is the probability
    the point is a member of the the cluster. This allows points to potentially be
    a mix of clusters. If you want to better understand how soft clustering works
    please refer to [How Soft Clustering for HDBSCAN Works](https://hdbscan.readthedocs.io/en/latest/soft_clustering_explanation.html).
    This approach is better suited for the present project, as it generates a larger
    set of rather similar sizes clusters.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`软聚类`是HDBSCAN库的一个新特性。在这种方法中，点不会被分配聚类标签，而是被分配一个概率向量。该向量的长度等于找到的聚类数。向量中每个位置的概率值表示该点是该聚类成员的概率。这使得点可能是多个聚类的混合。如果你想更好地理解软聚类是如何工作的，请参阅[HDBSCAN的软聚类工作原理](https://hdbscan.readthedocs.io/en/latest/soft_clustering_explanation.html)。这种方法更适合当前项目，因为它生成了一个较大的、相对相似大小的聚类集。'
- en: While HDBSCAN can perform well on low to medium dimensional data, the performance
    tends to decrease significantly as dimension increases. In general HDBSCAN performs
    best on up to around 50 dimensional data, [[see here](https://hdbscan.readthedocs.io/en/latest/faq.html)].
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然HDBSCAN在低到中维度数据上表现良好，但随着维度的增加，性能通常会显著下降。一般而言，HDBSCAN在最多大约50维的数据上表现最佳，[[请参阅这里](https://hdbscan.readthedocs.io/en/latest/faq.html)]。
- en: Documents for clustering are typically embedded using an efficient transformer
    from the BERT family, resulting in a several hundred dimensions data set.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类用的文档通常使用BERT家族中的高效变换器进行嵌入，得到的是一个几百维的数据集。
- en: To reduce the dimension of the embeddings vectors we use **UMAP** ([Uniform
    Manifold Approximation and Projection)](https://umap-learn.readthedocs.io/en/latest/basic_usage.html),
    a non-linear dimension reduction algorithm and the best performing in its class.
    It seeks to learn the manifold structure of the data and to find a low dimensional
    embedding that preserves the essential topological structure of that manifold.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少嵌入向量的维度，我们使用**UMAP**（[统一流形近似与投影](https://umap-learn.readthedocs.io/en/latest/basic_usage.html)），这是一种非线性降维算法，并且是同类中表现最好的算法。它旨在学习数据的流形结构，并找到一个低维嵌入，从而保留该流形的基本拓扑结构。
- en: UMAP has been shown to be highly effective at preserving the overall structure
    of high-dimensional data in lower dimensions, while also providing superior performance
    to other popular algorithms like t-SNE and PCA.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 研究表明，UMAP在将高维数据的整体结构保留到低维时非常有效，同时在性能上优于其他流行算法，如t-SNE和PCA。
- en: Keyword Clustering
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键词聚类
- en: Install and import the required packages and libraries.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装并导入所需的包和库。
- en: '[PRE7]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Prepare the dataset by aggregating all keywords and keyphrases from each title’s
    individual quintet into a single list of unique keywords and save it as a pandas
    dataframe.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备数据集，将每个标题的单独五元组中的所有关键词和关键短语聚合成一个独特关键词的单一列表，并将其保存为一个pandas数据框。
- en: '[PRE8]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'I obtain almost 3000 unique keywords and keyphrases from the 884 processed
    titles. Here is a sample: n-colorable graphs, experiments, constraints, tree structure,
    complexity, etc.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我从884个处理过的标题中获得了将近3000个独特的关键词和关键短语。以下是一个示例：n-可染图、实验、约束、树结构、复杂性等。
- en: Generate 768-dimensional embeddings with Sentence Transformers.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Sentence Transformers生成768维的嵌入。
- en: '[PRE9]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Perform dimensionality reduction with UMAP.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用UMAP进行降维。
- en: '[PRE10]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Cluster the 10-dimensional vectors with HDBSCAN. To keep this blog succinct,
    I will omit descriptions of the parameters that pertain more to hard clustering.
    For detailed information on each parameter, please refer to [[Parameter Selection
    for HDBSCAN*](https://hdbscan.readthedocs.io/en/latest/parameter_selection.html)].
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用HDBSCAN对10维向量进行聚类。为了保持这篇博客简洁，我将省略与硬聚类相关的参数描述。有关每个参数的详细信息，请参阅[[HDBSCAN参数选择](https://hdbscan.readthedocs.io/en/latest/parameter_selection.html)]。
- en: '[PRE11]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Below is the distribution of keywords across clusters. Examination of the spread
    of keywords and keyphrases into soft clusters reveals a total of 60 clusters,
    with a fairly even distribution of elements per cluster, varying from about 20
    to nearly 100.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是关键词在各个聚类中的分布情况。通过检查关键词和关键短语在软聚类中的分布，发现总共有60个聚类，每个聚类中的元素分布较为均匀，数量从大约20个到近100个不等。
- en: '![](../Images/07b5e9852a75b59e177d37efc1149ad4.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07b5e9852a75b59e177d37efc1149ad4.png)'
- en: Extract Cluster Descriptions and Labels
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取聚类描述和标签
- en: Having clustered the keywords, we are now ready to employ GenAI once more to
    enhance and refine our findings. At this step, we will use a LLM to analyze each
    cluster, summarize the keywords and keyphrases while assigning a brief label to
    the cluster.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在对关键词进行聚类后，我们现在准备再次使用GenAI来增强和优化我们的发现。在这一步，我们将使用大型语言模型（LLM）分析每个聚类，总结关键词和关键短语，并为每个聚类分配一个简短的标签。
- en: While it’s not necessary, I choose to continue with the same LLM, Zephyr-7B-Beta.
    Should you require downloading the model, please consult the relevant section.
    Notably, I will adjust the prompt to suit the distinct nature of this task.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这不是必要的，我选择继续使用相同的LLM——Zephyr-7B-Beta。如果您需要下载该模型，请参考相关章节。值得注意的是，我会调整提示，以适应此任务的不同特点。
- en: The following function is designed to extract a label and a description for
    a cluster, parse the output and integrate it into a pandas dataframe.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数旨在为每个聚类提取标签和描述，解析输出并将其集成到pandas数据框中。
- en: '[PRE12]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now we can apply the above function to each cluster and collect the results:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将上述函数应用于每个聚类并收集结果：
- en: '[PRE13]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Let’s take a look at a sample of outputs. For complete list of outputs please
    refer to the [Google Colab](https://github.com/SolanaO/Blogs_Content/blob/master/keyllm_neo4j/KeyBERT_LLM_Neo4j.ipynb).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个输出示例。完整的输出列表请参见[Google Colab](https://github.com/SolanaO/Blogs_Content/blob/master/keyllm_neo4j/KeyBERT_LLM_Neo4j.ipynb)。
- en: '![](../Images/bd5b0a43c213efe4df24d1d6b122cb5f.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd5b0a43c213efe4df24d1d6b122cb5f.png)'
- en: 'We must remember that LLMs, with their inherent probabilistic nature, can be
    unpredictable. While they generally adhere to instructions, their compliance is
    not absolute. Even slight alterations in the prompt or the input text can lead
    to substantial differences in the output. In the `extract_description()` function,
    I''ve incorporated a feature to log the *response* in both *label* and *description*
    columns in those cases where the *Label: Description* format is not followed,
    as illustrated by the irregular output for cluster 7 above. The outputs for the
    entire set of 60 clusters are available in the accompanying [Google Colab](https://github.com/SolanaO/Blogs_Content/blob/master/keyllm_neo4j/KeyBERT_LLM_Neo4j.ipynb)
    notebook.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '我们必须记住，LLM由于其固有的概率性，可能会表现出不可预测的行为。虽然它们通常遵循指令，但它们的遵从性并非绝对。即使是对提示或输入文本的微小修改，也可能导致输出结果的重大差异。在`extract_description()`函数中，我添加了一个功能，在*标签*和*描述*列中记录*响应*，以应对那些没有遵循*Label:
    Description*格式的情况，如上面第7个聚类的异常输出所示。所有60个聚类的输出可以在附带的[Google Colab](https://github.com/SolanaO/Blogs_Content/blob/master/keyllm_neo4j/KeyBERT_LLM_Neo4j.ipynb)笔记本中查看。'
- en: A second observation, is that each cluster is parsed independently by the LLM
    and it is possible to get repeated labels. Additionally, there may be instances
    of recurring keywords extracted from the input list.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个观察是，每个聚类都由LLM独立解析，并且可能会得到重复的标签。此外，可能会出现从输入列表中提取的重复关键词。
- en: The effectiveness of the process is highly reliant on the choice of the LLM
    and issues are minimal with a highly performant LLM. The output also depends on
    the quality of the keyword clustering and the presence of an inherent topic within
    the cluster.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程的有效性在很大程度上依赖于LLM的选择，使用高性能的LLM时，问题较少。输出结果还依赖于关键词聚类的质量以及聚类中是否存在固有的主题。
- en: 'Strategies to mitigate these challenges depend on the cluster count, dataset
    characteristics and the required accuracy for the project. Here are two options:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 缓解这些挑战的策略取决于聚类的数量、数据集的特征以及项目所需的准确性。以下是两种选择：
- en: Manually rectify each issue, as I did in this project. With only 60 clusters
    and merely three erroneous outputs, manual adjustments were made to correct the
    faulty outputs and to ensure unique labels for each cluster.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动修正每个问题，正如我在这个项目中所做的那样。只有60个聚类和三个错误的输出，手动调整以修正错误的输出，并确保每个聚类都有唯一的标签。
- en: Employ an LLM to make the corrections, although this method does not guarantee
    flawless results.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LLM进行修正，尽管这种方法无法保证完全无误。
- en: Build the Knowledge Graph
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建知识图谱
- en: Data to Upload into the Graph
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上传到图谱的数据
- en: There are two csv files (or pandas dataframes if working in a single session)
    to extract the data from.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个csv文件（或如果在单个会话中工作，则是pandas数据框）可以从中提取数据。
- en: '`articles` - it contains unique `id` for each article, `title` , `abstract`
    and `titles_keys` which is the list of five extracted keywords or keyphrases;'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`articles` - 它包含每篇文章的唯一`id`、`title`、`abstract`和`titles_keys`，后者是提取的五个关键词或关键短语的列表；'
- en: '`keywords` - with columns `key` , `cluster` , `description` and `label` , where
    `key` contains a complete list of unique keywords or keyphrases, and the remaining
    features describe the cluster the keyword belongs to.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`keywords` - 包含 `key`、`cluster`、`description` 和 `label` 等列，其中 `key` 包含一个完整的唯一关键词或关键短语列表，其他特征则描述关键词所属的集群。'
- en: Neo4j Connection
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Neo4j 连接
- en: To build a knowledge graph, we start with setting up a Neo4j instance, choosing
    from options like Sandbox, AuraDB, or Neo4j Desktop. For this project, I’m using
    AuraDB’s free version. It is straightforward to launch a blank instance and download
    its credentials.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建知识图谱，我们首先需要设置一个 Neo4j 实例，可以选择如 Sandbox、AuraDB 或 Neo4j Desktop 等选项。对于这个项目，我使用的是
    AuraDB 的免费版本。启动一个空白实例并下载其凭证非常简单。
- en: Next, establish a connection to Neo4j. For convenience, I use a custom Python
    module, which can be found at `[utils/neo4j_conn.py](<https://github.com/SolanaO/Blogs_Content/blob/master/keyllm_neo4j/utils/neo4j_conn.py>)`
    . This module contains methods for connecting and interacting with the graph database.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，建立与 Neo4j 的连接。为了方便，我使用了一个自定义的 Python 模块，您可以在 `[utils/neo4j_conn.py](<https://github.com/SolanaO/Blogs_Content/blob/master/keyllm_neo4j/utils/neo4j_conn.py>)`
    中找到。这个模块包含了连接和与图数据库交互的方法。
- en: '[PRE14]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The graph we are about to build has a simple schema consisting of three nodes
    and two relationships:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们即将构建的图有一个简单的架构，由三个节点和两个关系组成：
- en: '![](../Images/71d425143be8fb32291f099340ec6c1e.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/71d425143be8fb32291f099340ec6c1e.png)'
- en: — Image by author —
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: — 作者图片 —
- en: 'Building the graph now is straightforward with just two Cypher queries:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在构建图表非常简单，只需两个 Cypher 查询：
- en: '[PRE15]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Query the Graph
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查询图谱
- en: 'Let’s check the distribution of the nodes and relationships on types:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看节点和关系按类型的分布：
- en: '![](../Images/01877831cc27975b9ded0373be022c78.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01877831cc27975b9ded0373be022c78.png)'
- en: 'We can find what individual topics (or clusters) are the most popular among
    our collection of articles, by counting the cumulative number of articles associated
    to the keywords they are connected to:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过计算与它们连接的关键词关联的文章数量，找出我们文章集合中最受欢迎的主题（或集群）：
- en: '![](../Images/cb582327dfffc75475762e64f9ecad96.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb582327dfffc75475762e64f9ecad96.png)'
- en: 'Here is a snapshot of the node `Semantics` that corresponds to cluster 58 and
    its connected keywords:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这是与集群 58 对应的 `Semantics` 节点的快照及其相关的关键词：
- en: '![](../Images/42523efd9df95b35eb5d9ef5c4743273.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/42523efd9df95b35eb5d9ef5c4743273.png)'
- en: — Image by author —
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: — 作者图片 —
- en: 'We can also identify commonly occurring works in titles, using the query below:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过以下查询来识别标题中常见的词汇：
- en: '![](../Images/0fe0b27744cbcc18c6df860cf2221184.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0fe0b27744cbcc18c6df860cf2221184.png)'
- en: Conclusion
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: We saw how we can structure and enrich a collection of semingly unrelated short
    text entries. Using traditional NLP and machine learning, we first extract keywords
    and then we cluster them. These results guide and ground the refinement process
    performed by Zephyr-7B-Beta. While some oversight of the LLM is still neccessary,
    the initial output is significantly enriched. A knowledge graph is used to reveal
    the newly discovered connections in the corpus.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，如何通过结构化和丰富一组看似无关的短文本条目。使用传统的 NLP 和机器学习，我们首先提取关键词，然后进行聚类。这些结果为 Zephyr-7B-Beta
    进行的精炼过程提供了指导和基础。尽管仍然需要对 LLM 进行一些监督，但初步输出已经得到了显著的增强。知识图谱用于揭示语料库中新发现的连接。
- en: Our key takeaway is that no single method is perfect. However, by strategically
    combining different techniques, acknowledging their strenghts and weaknesses,
    we can achieve superior results.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的关键收获是，没有任何单一方法是完美的。然而，通过战略性地结合不同的技巧，认识到它们的优缺点，我们可以取得更优的结果。
- en: References
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '**Google Colab Notebook and Code**'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '**Google Colab 笔记本和代码**'
- en: '[Link to associated Github Repo.](https://github.com/SolanaO/Blogs_Content/tree/master/keyllm_neo4j)'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[关联的 Github 仓库链接。](https://github.com/SolanaO/Blogs_Content/tree/master/keyllm_neo4j)'
- en: '**Data**'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据**'
- en: 'Repository of scholary articles: [arXiv Dataset](https://www.kaggle.com/datasets/Cornell-University/arxiv)
    that has [CC0: Public Domain](https://creativecommons.org/publicdomain/zero/1.0/)
    license.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '学术文章仓库：[arXiv 数据集](https://www.kaggle.com/datasets/Cornell-University/arxiv)，该数据集具有
    [CC0: 公共领域](https://creativecommons.org/publicdomain/zero/1.0/) 许可。'
- en: '**Technical Documentation**'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '**技术文档**'
- en: '[KeyBERT and KeyLLM](https://maartengr.github.io/KeyBERT/guides/quickstart.html)
    — repository pages.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KeyBERT 和 KeyLLM](https://maartengr.github.io/KeyBERT/guides/quickstart.html)
    — 仓库页面。'
- en: '[HDBSCAN](https://hdbscan.readthedocs.io/en/latest/basic_hdbscan.html#) — documentation.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[HDBSCAN](https://hdbscan.readthedocs.io/en/latest/basic_hdbscan.html#) — 文档。'
- en: '[UMAP](https://umap-learn.readthedocs.io/en/latest/) — documentation.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[UMAP](https://umap-learn.readthedocs.io/en/latest/) —— 文档。'
- en: '**Blogs and Articles**'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**博客与文章**'
- en: '[Maarten Grootendorst, Introducing KeyLLM — Keyword Extraction with LLMs](/introducing-keyllm-keyword-extraction-with-llms-39924b504813),
    Towards Data Science, Oct 5, 2023.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Maarten Grootendorst，介绍KeyLLM——使用LLM进行关键词提取](/introducing-keyllm-keyword-extraction-with-llms-39924b504813)，数据科学前沿，2023年10月5日。'
- en: '[Benjamin Marie, Zephyr 7B Beta: A Good Teacher Is All You Need](/zephyr-7b-beta-a-good-teacher-is-all-you-need-c931fcd0bfe7),
    Towards Data Science, Nov 10, 2023.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Benjamin Marie，Zephyr 7B Beta：一个好老师就是你所需要的一切](/zephyr-7b-beta-a-good-teacher-is-all-you-need-c931fcd0bfe7)，数据科学前沿，2023年11月10日。'
- en: 'The H4 Team, Zephyr: Direct Distillation of LM Alignment, Technical Report,
    [arXiv: 2310.16944](https://arxiv.org/pdf/2310.16944.pdf), Oct 25, 2023.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'H4团队，Zephyr：LM对齐的直接蒸馏，技术报告，[arXiv: 2310.16944](https://arxiv.org/pdf/2310.16944.pdf)，2023年10月25日。'
