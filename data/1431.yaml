- en: What Is a Good Imputation for Missing Values?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是缺失值的良好填补方法？
- en: 原文：[https://towardsdatascience.com/what-is-a-good-imputation-for-missing-values-e9256d45851b?source=collection_archive---------5-----------------------#2024-06-08](https://towardsdatascience.com/what-is-a-good-imputation-for-missing-values-e9256d45851b?source=collection_archive---------5-----------------------#2024-06-08)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/what-is-a-good-imputation-for-missing-values-e9256d45851b?source=collection_archive---------5-----------------------#2024-06-08](https://towardsdatascience.com/what-is-a-good-imputation-for-missing-values-e9256d45851b?source=collection_archive---------5-----------------------#2024-06-08)
- en: My current take on what imputation should be
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我目前对缺失值填补的理解
- en: '[](https://medium.com/@jeffrey_85949?source=post_page---byline--e9256d45851b--------------------------------)[![Jeffrey
    Näf](../Images/0ce6db85501192cdebeeb910eb81a688.png)](https://medium.com/@jeffrey_85949?source=post_page---byline--e9256d45851b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e9256d45851b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e9256d45851b--------------------------------)
    [Jeffrey Näf](https://medium.com/@jeffrey_85949?source=post_page---byline--e9256d45851b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jeffrey_85949?source=post_page---byline--e9256d45851b--------------------------------)[![Jeffrey
    Näf](../Images/0ce6db85501192cdebeeb910eb81a688.png)](https://medium.com/@jeffrey_85949?source=post_page---byline--e9256d45851b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e9256d45851b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e9256d45851b--------------------------------)
    [Jeffrey Näf](https://medium.com/@jeffrey_85949?source=post_page---byline--e9256d45851b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e9256d45851b--------------------------------)
    ·18 min read·Jun 8, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e9256d45851b--------------------------------)
    ·18分钟阅读·2024年6月8日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: This article is a first article summarizing and discussing my most recent [paper](https://hal.science/hal-04521894).
    We study general-purpose imputation of tabular datasets. That is, the imputation
    should be done in a way that works for many different tasks in a second step (sometimes
    referred to as “broad imputation”).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本文是我最新的[论文](https://hal.science/hal-04521894)的总结与讨论。我们研究的是表格数据集的通用填补方法。即，填补方法应以一种适用于许多不同任务的方式进行（有时称为“广泛填补”）。
- en: In this article, I will write 3 lessons that I learned working on this problem
    over the last years. I am very excited about this paper in particular, but also
    cautious, as the problem of missing values has many aspects and it can be difficult
    to not miss something. So I invite you to judge for yourself if my lessons make
    sense to you.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将分享我在过去几年处理该问题时学到的三个经验教训。我对这篇论文特别感到兴奋，但也保持谨慎，因为缺失值问题有很多方面，且很难保证不会遗漏某些内容。所以，我邀请你自行判断我的经验教训是否对你有意义。
- en: If you do not want to get into great discussions about missing values, I will
    summarize my recommendations at the end of the article.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不想深入讨论缺失值的问题，我将在文章最后总结我的建议。
- en: '***Disclaimer:*** *The goal of this article is to use imputation to recreate
    the original data distribution. While I feel this is what most researchers and
    practitioners actually want, this is a difficult goal that might not be necessary
    in all applications. For instance, when performing (conditional mean) prediction,
    there are several recent papers showing that even simple imputation methods are
    sufficient for large sample sizes.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '***免责声明：*** *本文的目标是通过填补来重建原始数据分布。虽然我认为这是大多数研究者和实践者真正想要的目标，但这是一个困难的目标，并非在所有应用中都必要。例如，在进行（条件均值）预测时，有几篇近期的论文表明，即使是简单的填补方法对于大样本量来说也是足够的。*'
- en: All images in this article were created by the author.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中的所有图片均由作者创建。
- en: Preliminaries
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前言
- en: Before continuing we need to discuss how I think about missing values in this
    article.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们需要讨论一下我在本文中如何看待缺失值。
- en: 'We assume there is an underlying distribution *P** from which observations
    *X** are drawn. In addition, there is a vector of 0/1s of the same dimension as
    *X** that is drawn, let’s call this vector *M*. The actual observed data vector
    *X* is then *X** masked by *M*. Thus, we observe *n* independently and identically
    distributed (i.i.d.) copies of the joint vector *(X,M)*. If we write this up in
    a data matrix, this might look like this:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设有一个潜在的分布*P**，从中抽取观察值*X**。此外，还有一个与*X**相同维度的0/1向量，它被抽取，我们称这个向量为*M*。实际观察到的数据向量*X*就是被*M*遮蔽后的*X**。因此，我们观察到n个独立同分布（i.i.d.）的联合向量*(X,
    M)*副本。如果我们将其写成数据矩阵，可能看起来是这样的：
- en: '![](../Images/94912a4e4605beac18aa07a39621e32a.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94912a4e4605beac18aa07a39621e32a.png)'
- en: 'The data generating process: X* and M are drawn, then we observe n i.id. copies
    of (X,M), where X is X* but masked by M.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 数据生成过程：X*和M被抽取，然后我们观察到n个独立同分布的(X, M)副本，其中X是X*，但被M遮蔽。
- en: As usual small values *x, m* means “observed”, while large values refer to random
    quantities. The missingness mechanisms everyone talks about are then assumptions
    about the relationship or joint distribution of *(X*,M):*
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通常小值*x, m*表示“已观察”，而大值则指随机变量。大家讨论的缺失机制就是对*(X*, M)*的关系或联合分布的假设：
- en: '**Missing Completely at Random (MCAR):** The probability of a value being missing
    is a coin flip, independent of any variable in the dataset. Here missing values
    are but a nuisance. You could ignore them and just focus on the fully observed
    part of your dataset and there would be no bias. In math for all *m* and *x*:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**完全随机缺失（MCAR）：** 值缺失的概率是抛硬币的结果，独立于数据集中的任何变量。在这种情况下，缺失值仅仅是一个麻烦。你可以忽略它们，只关注数据集中完全观察到的部分，这样就不会产生偏差。在数学中，对于所有的*m*和*x*：'
- en: '![](../Images/1c04f98834fcb6936aad2af377e81393.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c04f98834fcb6936aad2af377e81393.png)'
- en: '**Missing at Random (MAR):** The probability of missingness can now depend
    on the *observed* variables in your dataset. A typical example would be two variables,
    say income and age, whereby age is always observed, but income might be missing
    for certain values of age. This is the example we study below. This may sound
    reasonable, but here it can get complicated. In math, for all *m* and *x*:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机缺失（MAR）：** 缺失的概率现在可以依赖于数据集中*观察到*的变量。一个典型的例子是两个变量，比如收入和年龄，其中年龄始终是观察到的，但收入可能在某些年龄值下缺失。这就是我们下面要研究的例子。这听起来很合理，但在这里可能会变得复杂。在数学中，对于所有的*m*和*x*：'
- en: '![](../Images/c38d327ba903dd79498b41377fbb2ffd.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c38d327ba903dd79498b41377fbb2ffd.png)'
- en: '**Missing Not at Random (MNAR):** Everything is possible here, and we cannot
    say anything about anything in general.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**非随机缺失（MNAR）：** 在这里一切皆有可能，我们不能就一般情况做出任何假设。'
- en: The key is that for imputation, we need to learn the conditional distribution
    of missing values given observed values in one pattern *m’* to impute in another
    pattern *m*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于，对于插补，我们需要学习在一个模式*m’*中，给定观察到的值，缺失值的条件分布，然后将其插补到另一个模式*m*中。
- en: 'A well-known method of achieving this is the Multiple Imputation by Chained
    Equations (**MICE**) method: Initially fill the values with a simple imputation,
    such as mean imputation. Then for each iteration *t*, for each variable *j* regress
    the observed *X_j* on all other variables (which are imputed). Then plug in the
    values of these variables into the learned imputer for all *X_j* that are not
    observed. This is explained in detail in [this article](https://medium.com/@ofirdi/mice-is-nice-but-why-should-you-care-e66698f245a3),
    with an amazing illustration that will make things immediately clear. In R this
    is conveniently implemented in the [mice R package](https://cran.r-project.org/web/packages/mice/index.html).
    As I will outline below, I am a huge fan of this method, based on the performance
    I have seen. In fact, the ability to recreate the underlying distribution of certain
    instances of MICE, such as mice-cart, is uncanny. In this article, we focus on
    a very simple example with only one variable missing, and so we can code by hand
    what MICE would usually do iteratively, to better illustrate what is happening.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这一目标的一个著名方法是通过链式方程进行多重插补（**MICE**）方法：首先用简单的插补方法填充数据，例如均值插补。然后对于每一次迭代*t*，对于每个变量*j*，将观察到的*X_j*回归到所有其他变量上（这些变量是插补的）。接着将这些变量的值输入到学习到的插补器中，插补所有未观察到的*X_j*。这一过程在[这篇文章](https://medium.com/@ofirdi/mice-is-nice-but-why-should-you-care-e66698f245a3)中有详细解释，并有一个很棒的插图，能让事情立刻变得清晰。在R语言中，这一过程方便地通过[mice
    R包](https://cran.r-project.org/web/packages/mice/index.html)实现。正如我在下面将概述的那样，基于我看到的表现，我非常喜欢这种方法。实际上，能够重建某些MICE实例的底层分布（如mice-cart）的能力非常精准。在本文中，我们关注一个只有一个变量缺失的非常简单的例子，因此我们可以手动编写代码，模拟MICE通常会做的迭代过程，以便更好地说明发生了什么。
- en: A first mini-lesson is that MICE is a host of methods; whatever method you choose
    to regress *X_j* on the other variables gives you a different imputation method.
    As such, there are countless variants in the mice R package, such as mice-cart,
    mice-rf, mice-pmm, mice-norm.nob, mice-norm.predict and so on. These methods will
    perform widely differently as we will see below. Despite this, at least some papers
    (in top conferences such as NeurIPS) confidently proclaim that they compare their
    methods to “MICE”, without any detail on what exactly they are using.
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 第一个小课程是，MICE是一系列方法；无论你选择哪种方法将*X_j*回归到其他变量上，都能得到不同的插补方法。因此，在mice R包中有无数的变种，例如mice-cart、mice-rf、mice-pmm、mice-norm.nob、mice-norm.predict等。这些方法的表现差异很大，下面我们会看到这一点。尽管如此，至少有一些论文（例如NeurIPS等顶级会议）自信地宣称他们将方法与“MICE”进行比较，却没有详细说明他们到底使用了什么方法。
- en: The Example
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例
- en: 'We will look at a very simple but illustrative example: Consider a data set
    with two jointly normal variables, *X_1, X_2*. We assume both variables have variance
    of 1 and a positive correlation of 0.5\. To give some context, we can imagine
    *X_1* to be (the logarithm of) income and *X_2* to be age. (This is just for illustration,
    obviously no one is between -3 and 3 years old). Moreover, assume a missing mechanism
    for the income *X_1*, whereby *X_1* tends to be missing whenever age is “high”.
    That is we set:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看一个非常简单但具有说明性的例子：考虑一个包含两个联合正态变量的数据集，*X_1, X_2*。我们假设两个变量的方差都是1，且它们之间的正相关系数为0.5。为了提供一些背景，我们可以假设*X_1*是（收入的对数），*X_2*是年龄。（这仅用于说明，显然没有人会介于-3岁到3岁之间）。此外，假设收入*X_1*的缺失机制是，当年龄“较高”时，*X_1*往往缺失。也就是说，我们设置：
- en: '![](../Images/b492606e06c00d26ddb636b76c91d888.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b492606e06c00d26ddb636b76c91d888.png)'
- en: 'So *X_1* (income) is missing with probability 0.8 whenever *X_2* (age) is “large”
    (i.e., larger zero). As we assume *X_2* is always observed, this is a textbook
    MAR example with two patterns, one where all variables are fully observed (*m1*)
    and a second (*m2*), wherein *X_1* is missing. Despite the simplicity of this
    example, if we assume that higher age is related to higher income, there is a
    *clear shift in the distribution of income and age when moving from one pattern
    to the other*. In pattern *m2*, where income is missing, values of both the observed
    age and the (unobserved) income tend to be higher. Let’s look at this in code:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当*X_2*（年龄）“较大”（即大于零）时，*X_1*（收入）以0.8的概率会缺失。由于我们假设*X_2*总是可观察的，这是一个典型的MAR（缺失不完全随机）例子，包含两个模式，一个是所有变量都完全观察到的模式（*m1*），另一个是*X_1*缺失的模式（*m2*）。尽管这个例子很简单，但如果我们假设较高的年龄与较高的收入相关联，那么在从一个模式到另一个模式的转换中，*收入和年龄的分布会发生明显变化*。在模式*m2*中，收入缺失时，观察到的年龄和（未观察到的）收入的值通常较高。我们来看一下代码实现：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/468ab5a6d59712941c96279dab0cf9aa.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/468ab5a6d59712941c96279dab0cf9aa.png)'
- en: 'Top: Distribution of (X_1,X_2) in the pattern where X_1 is observed, Bottom:
    Distribution of (X_1,X_2) in the pattern where X_1 is missing.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 上图：在X_1已观察到的情况下，(X_1, X_2)的分布；下图：在X_1缺失的情况下，(X_1, X_2)的分布。
- en: 'Lesson 1: Imputation is a distributional prediction problem'
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1课：插补是一个分布预测问题
- en: 'In my view, the goal of (general purpose) imputation should be to replicate
    the underlying data distribution as well as possible. To illustrate this, consider
    again the first example with *p=0*, such that only *X_1* has missing values. We
    will now try to impute this example, using the famous [MICE](https://medium.com/@ofirdi/mice-is-nice-but-why-should-you-care-e66698f245a3)
    approach. Since only *X_1* is missing, we can implement this by hand. We start
    with the *mean imputation*, which simply calculates the mean of *X_1* in the pattern
    where it is observed, and plugs this mean in the place of NA. We also use the
    *regression imputation* which is a bit more sophisticated: We regress *X_1* onto
    *X_2* in the pattern where *X_1* is observed and then for each missing observation
    of *X_1* we plug in the prediction of the regression. Thus here we impute the
    conditional mean of *X_1* given *X_2*. Finally, for the *Gaussian imputation*,
    we start with the same regression of *X_1* onto *X_2*, but then impute each missing
    value of *X_1* by drawing from a Gaussian distribution. *In other words, instead
    of imputing the conditional expectation (i.e. just the center of the conditional
    distribution), we draw from this distribution.* This leads to a random imputation,
    which may be a bit counterintuitive at first, but will actually lead to the best
    result:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在我看来，（通用）插补的目标应该是尽可能地复制基础数据分布。为了说明这一点，我们再次考虑第一个例子，其中*p=0*，即只有*X_1*缺失。我们现在将尝试使用著名的[MICE](https://medium.com/@ofirdi/mice-is-nice-but-why-should-you-care-e66698f245a3)方法来插补这个例子。由于只有*X_1*缺失，我们可以手动实现这个过程。我们从*均值插补*开始，它仅仅是计算*X_1*在观察到的模式中的均值，然后将该均值填补到NA的位置。我们还使用*回归插补*，它稍微复杂一些：我们将*X_1*回归到*X_2*，在*X_1*被观察到的模式下，然后对于每一个缺失的*X_1*观测值，我们填入回归的预测值。因此，在这里，我们插补了给定*X_2*的条件均值。最后，对于*高斯插补*，我们从相同的*X_1*到*X_2*的回归开始，但接着通过从高斯分布中抽取值来插补每个缺失的*X_1*值。*换句话说，我们不是插补条件期望值（即只是条件分布的中心），而是从这个分布中抽样。*这导致了一种随机插补，起初可能有点反直觉，但实际上将会得到最佳结果：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/e08bd7255512d697e271005c69023417.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e08bd7255512d697e271005c69023417.png)'
- en: The distribution of (X_1, X_2) plotted for different imputation methods. Different
    Imputation methods (red are the imputed points).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 针对不同插补方法绘制的(X_1, X_2)的分布图。不同的插补方法（红点为插补的值）。
- en: Studying this plot immediately reveals that the mean and regression imputations
    might not be ideal, as they completely fail at recreating the original data distribution.
    In contrast, the Gaussian imputation looks pretty good, in fact, I’d argue it
    would be hard to differentiate it from the truth. This might just seem like a
    technical notion, but this has consequences. Imagine you were given any of those
    imputed data sets and now you would like to find the regression coefficient when
    regressing *X_2* onto *X_1* (the opposite of what we did for imputation). The
    truth in this case is given by *beta=cov(X_1,X_2)/var(X_1)=0.7*.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 研究这张图表立即揭示出均值插补和回归插补可能不是理想的，因为它们完全无法重建原始数据分布。相比之下，高斯插补看起来相当不错，实际上，我认为它很难与真实数据区分开。这看起来可能只是一个技术性的概念，但它有着深远的影响。想象一下，如果你得到这些插补后的数据集，并且现在你想找到回归系数，当回归*X_2*与*X_1*时（这是我们在插补中做的事情的反向操作）。在这种情况下，真实的回归系数是*beta=cov(X_1,
    X_2)/var(X_1)=0.7*。
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The Gaussian imputation is pretty close to 0.7 (0.71), and importantly, it is
    very close to the estimate using the full (unobserved) data! On the other hand,
    the mean imputation underestimates *beta*, while *the regression imputation overestimates
    beta*. The latter is natural, as the conditional mean imputation artificially
    inflates the relationship between variables. This effect is particularly important,
    as this will result in effects that are overestimated in science and (data science)
    practice!!
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯插补接近0.7（0.71），更重要的是，它非常接近使用完整（未观察到的）数据估算的结果！另一方面，均值插补低估了*beta*，而*回归插补则高估了beta*。后者是自然的，因为条件均值插补人为地膨胀了变量之间的关系。这个效应特别重要，因为它会导致科学和（数据科学）实践中的效应被高估！！
- en: The regression imputation might seem overly simplistic. However, the key is
    that very commonly used imputation methods in machine learning and other fields
    work exactly like this. For instance, knn imputation and random forest imputation
    (i.e., [missForest](https://academic.oup.com/bioinformatics/article/28/1/112/219101)).
    Especially the latter has been praised and recommended in several benchmarking
    papers and appears very widely used. However, missForest fits a Random Forest
    on the observed data and then simply imputes by the conditional mean. So, using
    it in this example the result would look very similar to the regression imputation,
    thus resulting in an artificial strengthening of relations between variable and
    biased estimates!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 回归插补看起来可能过于简单。然而，关键是，机器学习和其他领域中常用的插补方法正是这样工作的。例如，knn 插补和随机森林插补（即，[missForest](https://academic.oup.com/bioinformatics/article/28/1/112/219101)）。尤其是后者，在几篇基准论文中得到了好评并被广泛推荐，且应用非常广泛。然而，missForest会在观测数据上拟合一个随机森林，然后通过条件均值简单插补。因此，在这个例子中使用它的结果看起来与回归插补非常相似，从而导致变量之间关系的人工增强和偏差估计！
- en: A lot of commonly used imputation methods, such as mean imputation, knn imputation,
    and missForest fail at replicating the distribution. What they estimate and approximate
    is the (conditional) mean, and so the imputation will look like that of the regression
    imputation (or even worse for the mean imputation). Instead, we should try to
    impute by drawing from estimated (conditional) distributions.
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 许多常用的插补方法，如均值插补、knn 插补和 missForest，在复制分布方面失败。它们估计和近似的是（条件）均值，因此插补结果看起来像回归插补的结果（甚至对于均值插补来说更差）。相反，我们应该尝试通过从估计的（条件）分布中抽取样本来进行插补。
- en: 'Lesson 2: Imputation should be evaluated as a distributional prediction problem'
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二课：插补应该作为一个分布预测问题进行评估
- en: There is a dual problem connected to the discussion of the first lesson. How
    should imputation methods be evaluated?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 与第一课的讨论相关，有一个双重问题。插补方法应该如何评估？
- en: Imagine we developed a new imputation method and now want to benchmark this
    against methods that exist already such as missForest, MICE, or [GAIN](https://arxiv.org/abs/1806.02920).
    In this setting, we artificially induce the missing values and so we have the
    actual data set just as above. We now want to compare this true dataset to our
    imputations. For the sake of the example, let us assume the regression imputation
    above is our new method, and we would like to compare it to mean and Gaussian
    imputation.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下我们开发了一种新的插补方法，现在想将其与已有的如 missForest、MICE 或 [GAIN](https://arxiv.org/abs/1806.02920)
    等方法进行基准测试。在这种情况下，我们人为地引入了缺失值，因此我们拥有的实际数据集与上面的一样。现在，我们想将这个真实的数据集与我们的插补结果进行比较。为了举例说明，假设上面的回归插补是我们的新方法，我们希望将其与均值插补和高斯插补进行比较。
- en: 'Even in the most prestigious conferences, this is done by calculating the root
    mean squared error (RMSE):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在最有声望的会议中，这也是通过计算均方根误差（RMSE）来完成的：
- en: '![](../Images/e09af5aad14d3f1fb12bc30d5e878641.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e09af5aad14d3f1fb12bc30d5e878641.png)'
- en: 'This is implemented here:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这里实现了这个：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This discussion is related to the discussion on how to correctly score predictions.
    [In this article](https://medium.com/towards-data-science/how-to-evaluate-your-predictions-cef80d8f6a69),
    I discussed that (R)MSE is the right score to evaluate (conditional) mean predictions.
    It turns out the exact same logic applies here; using RMSE like this to evaluate
    our imputation, will favor methods that impute the conditional mean, such as the
    regression imputation, knn imputation, and missForest.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这段讨论与如何正确评估预测的讨论相关。[在这篇文章中](https://medium.com/towards-data-science/how-to-evaluate-your-predictions-cef80d8f6a69)，我讨论了(R)MSE是评估（条件）均值预测的正确分数。事实证明，这个相同的逻辑同样适用；像这样使用RMSE来评估我们的插补方法，会偏向于那些插补条件均值的方法，如回归插补、knn
    插补和 missForest。
- en: 'Instead, imputation should be *evaluated* as a distributional prediction problem.
    I suggest using the *energy distance between the distribution of the fully observed
    data and the imputation “distribution”*. Details can be found in the paper, but
    in R it is easily coded using the nice “energy” R package:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，插补应该作为一个分布预测问题进行*评估*。我建议使用*完全观察数据与插补“分布”之间的能量距离*。详细信息可以在论文中找到，但在R中可以通过“energy”
    R包轻松实现：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We now apply the two scores to our imaginary research project and try to figure
    out whether our regression imputation is better than the other two:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将这两个分数应用于我们虚构的研究项目，试图弄清楚我们的回归插补是否比其他两个方法更好：
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/ea0401b75e1292ebc9b18887aea7cd7c.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea0401b75e1292ebc9b18887aea7cd7c.png)'
- en: If we look at RMSE, then our regression imputation appears great! It beats both
    mean and Gaussian imputation. However this clashes with the analysis from above,
    and choosing the regression imputation can and likely will lead to highly biased
    results. On the other hand, the (scaled) energy distance correctly identifies
    that the Gaussian imputation is the best method, agreeing with both visual intuition
    and better parameter estimates.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们看RMSE，那么回归插补看起来非常好！它优于均值插补和高斯插补。然而，这与上述分析冲突，选择回归插补方法可能会导致高度偏倚的结果。另一方面，（缩放后的）能量距离正确地识别出高斯插补是最佳方法，与视觉直觉和更好的参数估计结果一致。
- en: When evaluating imputation methods (when the true data are available) measures
    such as RMSE and MAE should be avoided. Instead, the problem should be treated
    and evaluated as a distributional prediction problem, and distributional metrics
    such as the energy distance should be used. The overuse of RMSE as an evaluation
    tool has some serious implications for research in this area.
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在评估插补方法时（当真实数据可用时），应避免使用RMSE和MAE等度量方法。相反，应将问题视为一个分布预测问题来评估，使用分布度量方法，如能量距离。RMSE作为评估工具的过度使用，对该领域的研究有一些严重的影响。
- en: Again this is not surprising, identifying the best mean prediction is what RMSE
    does. What is surprising, is how consistently it is used in research to evaluate
    imputation methods. In my view, this throws into question at least some recommendations
    of recent papers, about what imputation methods to use. Moreover, as new imputation
    methods get developed they are compared to other methods in terms of RMSE and
    are thus likely not replicating the distribution correctly. One thus has to question
    the usefulness of at least some of the myriad of imputation methods developed
    in recent years.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不令人惊讶，识别最佳均值预测是RMSE的作用。令人惊讶的是，它在研究中被如此一致地用于评估插补方法。以我个人的观点来看，这至少对一些近期论文的建议提出了疑问，关于使用哪些插补方法。此外，随着新插补方法的开发，它们通常会与其他方法在RMSE上进行比较，因此可能无法正确地重现分布。因此，我们必须质疑近年来开发的众多插补方法中，至少一些方法的实际价值。
- en: The question of evaluation gets much harder, **when the underlying observations
    are not available.** In the paper we develope a score that allows to rank imputation
    methods, even in this case! (a refinement of the idea presented in [this article](/i-scores-how-to-choose-the-best-method-to-fill-in-nas-in-your-data-set-43f3f0df971f)).
    The details are reserved for another medium post, but we can try it for this example.
    The “Iscore.R” function can be found on [Github](https://github.com/JeffNaef/MARimputation/tree/c1f5a1e48e8a60db95c727876086db5b7305f614/Useable)
    or at the end of this article.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当**基础观察数据不可得时**，评估问题变得更加困难。在本文中，我们开发了一种评分方法，可以在这种情况下对插补方法进行排名！（这是[这篇文章](/i-scores-how-to-choose-the-best-method-to-fill-in-nas-in-your-data-set-43f3f0df971f)中提出的思路的改进）。详细内容将在另一篇
    Medium 文章中说明，但我们可以尝试在这个示例中使用它。`Iscore.R` 函数可以在[Github](https://github.com/JeffNaef/MARimputation/tree/c1f5a1e48e8a60db95c727876086db5b7305f614/Useable)上找到，或者在本文末尾查看。
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Thus *without every seeing the values of the missing data*, our score is able
    to identify that norm.nob is the best method! This comes in handy, especially
    when the data has more than two dimensions. I will give more details on how to
    use the score and how it works in a next article.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，*即使从未看到缺失数据的值*，我们的评分方法也能识别出 norm.nob 是最佳方法！这在数据维度超过两个时尤其有用。我将在下一篇文章中详细说明如何使用该评分方法以及它是如何工作的。
- en: 'Lesson 3: MAR is weirder than you think'
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3课：MAR比你想象的更奇怪
- en: When reading the literature on missing value imputation, it is easy to get a
    sense that MAR is a solved case, and all the problems arise from whether it can
    be assumed or not. While this might be true under standard procedures such as
    maximum likelihood, if one wants to find a good (nonparametric) imputation, this
    is not the case.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读关于缺失值插补的文献时，人们很容易产生一种感觉：MAR已经是一个解决过的问题，所有的问题都来自于是否能够假设它成立。虽然在标准程序下，如最大似然估计，这可能是对的，但如果想要找到一个好的（非参数）插补方法，情况并非如此。
- en: Our paper discusses how complex distribution shifts are possible under MAR when
    changing from say the fully observed pattern to a pattern one wants to impute.
    We will focus here on the shift in distribution that can occur in the observed
    variables. For this, we turn to the example above, where we took *X_1* to be income
    and *X_2* to be age. As we have seen in the first figure the distribution looks
    quite different. However, the conditional distribution of *X_1 | X_2* remains
    the same! This allows to identify the right imputation distribution in principle.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的论文讨论了在MAR（缺失完全随机）情况下，如何在从完全观察的模式转变为想要插补的模式时，可能发生复杂的分布变化。在这里，我们将重点讨论观察变量中可能发生的分布变化。为此，我们回到上面的例子，其中我们将*X_1*视为收入，*X_2*视为年龄。正如我们在第一幅图中所看到的，分布看起来差异很大。然而，*X_1
    | X_2*的条件分布保持不变！这从理论上使我们能够识别出正确的插补分布。
- en: '![](../Images/ae052c33d825988f15194972c9256b78.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae052c33d825988f15194972c9256b78.png)'
- en: 'Distribution of X_2 in the pattern where X_1 is observed, Bottom: Distribution
    of (X_1,X_2) in the pattern where X_1 is missing.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在*X_1*已观察的模式下，*X_2*的分布。底部：在*X_1*缺失的模式下，*(X_1, X_2)*的分布。
- en: The problem is that even if we can nonparametrically estimate the conditional
    distribution in the pattern where *X_1* is missing, we need to extrapolate this
    to the distribution of *X_2* where *X_1* is missing. To illustrate this I will
    now introduce two very important nonparametric mice methods. One old (*mice-cart*)
    and one new (*mice-DRF*). The former uses one tree to regress *X_j* on all the
    other variables and then imputes by drawing samples from that tree. Thus instead
    of using the conditional expectation prediction of a tree/forest, as missForest
    does, it draws from the leaves to approximate sampling from the conditional distribution.
    In contrast, mice-DRF uses the [Distributional Random Forest](https://medium.com/towards-data-science/drf-a-random-forest-for-almost-everything-625fa5c3bcb8),
    a forest method designed to estimate distributions and samples from those predictions.
    Both work exceedingly well, as I will lay out below!
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，即使我们能够非参数估计*X_1*缺失模式下的条件分布，我们仍然需要将其外推到*X_2*缺失模式下的分布。为了说明这一点，我将介绍两种非常重要的非参数MICE方法。一种是旧的（*mice-cart*），一种是新的（*mice-DRF*）。前者使用一棵树来回归*X_j*与其他所有变量，然后通过从该树中抽样来进行插补。因此，与missForest使用树/森林的条件期望预测不同，它是通过从叶节点抽样来近似从条件分布中抽样。相比之下，mice-DRF使用[分布随机森林](https://medium.com/towards-data-science/drf-a-random-forest-for-almost-everything-625fa5c3bcb8)，这是一种设计用来估计分布并从这些预测中抽样的森林方法。这两种方法都非常有效，正如我下面所阐述的！
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/e991ed2be1c9f70f0c552c5361ef0877.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e991ed2be1c9f70f0c552c5361ef0877.png)'
- en: Though both mice-cart and mice-DRF do a good job, they are still not quite as
    good as the Gaussian imputation. This is not surprising per se, as the Gaussian
    imputation is the ideal imputation in this case (because *(X_1, X_2)* are indeed
    Gaussian). Nonetheless the distribution shift in *X_2* likely plays a role in
    the difficulty of mice-cart and mice-DRF to recover the distribution even for
    3000 observations (these methods are usually really really good). Note that this
    kind of extrapolation is not a problem for the Gaussian imputation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管mice-cart和mice-DRF都表现得很好，但它们仍然不如高斯插补效果好。这并不令人惊讶，因为高斯插补在这种情况下是理想的插补方法（因为*(X_1,
    X_2)*确实是高斯分布的）。然而，*X_2*中的分布变化可能是导致mice-cart和mice-DRF即使对于3000个观测值也难以恢复分布的原因（这些方法通常非常非常有效）。请注意，这种外推问题对于高斯插补来说并不存在。
- en: 'The paper also discusses a similar, but more extreme example with two variables
    *(X_1, X_2)*. In this example, the distribution shift is much more pronounced,
    and the forest-based methods struggle accordingly:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 论文还讨论了一个相似的，但更极端的例子，其中包含两个变量*(X_1, X_2)*。在这个例子中，分布变化更加明显，基于森林的方法因此也表现得较差：
- en: '![](../Images/7a8ab1ef4047e142f30d01eb755c79d8.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a8ab1ef4047e142f30d01eb755c79d8.png)'
- en: More extreme example of distribution shift in the paper. While the Gausisan
    imputation is near perfect, mice-RF and mice-DRF are not able to extrapolate correctly.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 论文中的一个更极端的分布变化示例。虽然高斯插补几乎完美，但mice-RF和mice-DRF未能正确外推。
- en: The problem is that these kinds of extreme distribution shifts are possible
    under MAR and forest-based methods have a hard time extrapolating outside of the
    data set (so do neural nets btw). Indeed, can you think of a method that can (1)
    learn a distribution nonparametrically and (2) extrapolate from *X_2* coming from
    the upper distribution to *X_2* drawn from the lower distribution reliably? For
    now, I cannot.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，这类极端分布偏移在 MAR 下是可能的，而基于森林的方法在数据集外推时很困难（神经网络也是如此）。实际上，你能想到一种方法吗，能够（1）非参数化地学习分布，并且（2）从上分布的*X_2*
    推断出从下分布抽取的*X_2* 吗？目前，我无法想到。
- en: Imputation is hard, even if MAR can be assumed, and the search for reliable
    imputation methods is not over.
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 插补是一个复杂的问题，即使可以假设为 MAR，寻找可靠的插补方法仍然没有结束。
- en: 'Conclusion: My current recommendations'
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论：我目前的建议
- en: Missing values are a hairy problem. Indeed, the best way to deal with missing
    values is to not have them. Accordingly, Lesson 3 shows that the search for imputation
    methods is not yet concluded, even if one only considers MAR. We still lack a
    method that can do (1) nonparametric distributional prediction and (2) adapt to
    distribution shifts that are possible under MAR. That said, I also sometimes feel
    people make the problem more complicated than it is; some MICE methods perform
    extremely well and might be enough for many missing value problems already.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失值是一个棘手的问题。事实上，处理缺失值的最佳方法就是根本不出现缺失值。因此，第3课表明，即使仅考虑 MAR，插补方法的探索也尚未结束。我们仍然缺乏一种方法，能够（1）进行非参数分布预测，并且（2）适应
    MAR 下可能出现的分布偏移。话虽如此，我有时也觉得人们把问题弄得比实际更复杂；一些 MICE 方法表现得非常好，可能已经足够应对许多缺失值问题了。
- en: 'I first want to mention that that there are very fancy machine learning methods
    like [GAIN](https://arxiv.org/abs/1806.02920) and variants, that try to impute
    data using neural nets. I like these methods because they follow the right idea:
    Impute the conditional distributions of missing given observed. However, after
    using them a bit, I am somewhat disappointed by their performance, especially
    compared to MICE.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我首先想提到一些非常先进的机器学习方法，如[GAIN](https://arxiv.org/abs/1806.02920)及其变种，这些方法尝试通过神经网络来插补数据。我喜欢这些方法，因为它们遵循了正确的思路：插补缺失值的条件分布，给定已观察到的数据。然而，在使用这些方法后，我对它们的表现感到有些失望，尤其是与
    MICE 方法相比。
- en: Thus, if I had a missing value problem the first thing I’d try is *mice-cart*
    (implemented in the mice R package) or the new *mice-DRF* (code on [Github](https://github.com/JeffNaef/MARimputation/tree/c1f5a1e48e8a60db95c727876086db5b7305f614/Useable))
    we developed in the paper. I have tried those two on quite a few examples and
    their ability to recreate the data is uncanny. However note that these observations
    of mine are not based on a large, systematic benchmarking and should be taken
    with a grain of salt. Moreover, this requires at least an intermediate sample
    size of say above 200 or 300\. Imputation is not easy and completely nonparametric
    methods will suffer if the sample size is too low. In the case of less than 200
    observations, I would go with simpler methods such as Gaussian imputation (*mice-norm.nob*
    in the R package). If you would then like to find the best out of these methods
    I recommend trying our score developed in the paper,as done in Lesson 2 (though
    the implementation might not be the best).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我遇到缺失值问题，首先会尝试的是*mice-cart*（在 mice R 包中实现）或我们在论文中开发的新方法*mice-DRF*（代码见[Github](https://github.com/JeffNaef/MARimputation/tree/c1f5a1e48e8a60db95c727876086db5b7305f614/Useable)）。我已经在不少实例中尝试了这两种方法，它们重建数据的能力非常出色。然而请注意，我的这些观察并非基于大规模、系统性的基准测试，应该谨慎看待。此外，这至少需要一个中等样本量，比如200或300以上。插补并不容易，如果样本量太小，完全非参数化的方法会受到影响。如果样本量少于200个观测值，我会选择更简单的方法，比如高斯插补（R
    包中的*mice-norm.nob*）。如果你想在这些方法中找出最佳的，我建议尝试我们在论文中开发的评分方法，正如第2课中所做的那样（尽管实现可能并非最佳）。
- en: Finally, note that none of these methods are able to effectively deal with **imputation
    uncertainty**! In a sense, we only discussed single imputation in this article.
    (Proper) multiple imputation would require that the uncertainty of the imputation
    method itself is taken into account, which is usually done using Bayesian methods.
    For frequentist method like we looked at here, this appears to be an open problem.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意这些方法都无法有效地处理**插补不确定性**！从某种意义上讲，我们在本文中只讨论了单次插补。（正确的）多次插补需要考虑插补方法本身的不确定性，这通常是通过贝叶斯方法来实现的。对于我们在这里讨论的频率学派方法，这似乎仍然是一个未解决的问题。
- en: 'Appendix 1: m-I-Score'
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 1：m-I-Score
- en: The File “Iscore.R”, which can also be found on [Github](https://github.com/JeffNaef/MARimputation/tree/c1f5a1e48e8a60db95c727876086db5b7305f614/Useable).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 文件“Iscore.R”，也可以在[Github](https://github.com/JeffNaef/MARimputation/tree/c1f5a1e48e8a60db95c727876086db5b7305f614/Useable)上找到。
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
