- en: Fine-tune Llama 3 with ORPO
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调 Llama 3 与 ORPO
- en: 原文：[https://towardsdatascience.com/fine-tune-llama-3-with-orpo-56cfab2f9ada?source=collection_archive---------3-----------------------#2024-04-19](https://towardsdatascience.com/fine-tune-llama-3-with-orpo-56cfab2f9ada?source=collection_archive---------3-----------------------#2024-04-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/fine-tune-llama-3-with-orpo-56cfab2f9ada?source=collection_archive---------3-----------------------#2024-04-19](https://towardsdatascience.com/fine-tune-llama-3-with-orpo-56cfab2f9ada?source=collection_archive---------3-----------------------#2024-04-19)
- en: '*A cheaper and faster unified fine-tuning technique*'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*一种更便宜、更快速的统一微调技术*'
- en: '[](https://medium.com/@mlabonne?source=post_page---byline--56cfab2f9ada--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page---byline--56cfab2f9ada--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--56cfab2f9ada--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--56cfab2f9ada--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page---byline--56cfab2f9ada--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne?source=post_page---byline--56cfab2f9ada--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page---byline--56cfab2f9ada--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--56cfab2f9ada--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--56cfab2f9ada--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page---byline--56cfab2f9ada--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--56cfab2f9ada--------------------------------)
    ·8 min read·Apr 19, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--56cfab2f9ada--------------------------------)
    ·阅读时长 8 分钟 ·2024年4月19日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/0024fbce8d1d7e5a96348655f9f40d35.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0024fbce8d1d7e5a96348655f9f40d35.png)'
- en: Image generated with DALL-E 3 by author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 DALL-E 3 生成，由作者提供
- en: ORPO is a **new exciting fine-tuning technique** that combines the traditional
    supervised fine-tuning and preference alignment stages into a single process.
    This reduces the computational resources and time required for training. Moreover,
    empirical results demonstrate that ORPO outperforms other alignment methods on
    various model sizes and benchmarks.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ORPO 是一种**新兴的令人兴奋的微调技术**，它将传统的监督微调和偏好对齐阶段合并为一个单一过程。这减少了训练所需的计算资源和时间。此外，实证结果表明，ORPO
    在多种模型规模和基准测试中超越了其他对齐方法。
- en: In this article, we will fine-tune the new Llama 3 8B model using ORPO with
    the TRL library. The code is available on [Google Colab](https://colab.research.google.com/drive/1eHNWg9gnaXErdAa8_mcvjMupbSS6rDvi?usp=sharing)
    and in the [LLM Course](https://github.com/mlabonne/llm-course) on GitHub.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将使用 ORPO 和 TRL 库微调新的 Llama 3 8B 模型。代码可以在 [Google Colab](https://colab.research.google.com/drive/1eHNWg9gnaXErdAa8_mcvjMupbSS6rDvi?usp=sharing)
    和 GitHub 上的 [LLM 课程](https://github.com/mlabonne/llm-course) 中找到。
- en: ⚖️ ORPO
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ⚖️ ORPO
- en: 'Instruction tuning and preference alignment are essential techniques for adapting
    Large Language Models (LLMs) to specific tasks. Traditionally, this involves a
    multi-stage process: 1/ **Supervised Fine-Tuning** (SFT) on instructions to adapt
    the model to the target domain, followed by 2/ **preference alignment methods**
    like Reinforcement Learning with Human Feedback (RLHF) or Direct Preference Optimization
    (DPO) to increase the likelihood of generating preferred responses over rejected
    ones.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 指令微调和偏好对齐是将大型语言模型（LLMs）适应特定任务的关键技术。传统上，这涉及到一个多阶段的过程：1/ **监督微调**（SFT），以使模型适应目标领域，然后是
    2/ **偏好对齐方法**，如人类反馈强化学习（RLHF）或直接偏好优化（DPO），以提高生成首选响应而非被拒绝响应的概率。
- en: '![](../Images/29cac2fc31790a8a74230a95bd7e7b4b.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29cac2fc31790a8a74230a95bd7e7b4b.png)'
- en: Image by author
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: However, researchers have identified a limitation in this approach. While SFT
    effectively adapts the model to the desired domain, it inadvertently **increases
    the probability of generating undesirable answers** alongside preferred ones.
    This is why the preference alignment stage is necessary to widen the gap between
    the likelihoods of preferred and rejected outputs.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，研究人员发现这种方法存在局限性。虽然 SFT 可以有效地将模型适应所需领域，但它无意中**增加了生成不良答案的概率**，而这些答案与首选答案一起出现。这就是为什么偏好对齐阶段是必要的，它能拉大首选输出和被拒绝输出之间的概率差距。
- en: '![](../Images/d27c0588ee55984b02754a279bb715f4.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d27c0588ee55984b02754a279bb715f4.png)'
- en: Note how the probability of rejected responses increases during supervised fine-tuning
    (image from the ORPO paper).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，经过监督微调后，被拒绝的响应概率是如何增加的（图片来自ORPO论文）。
- en: Introduced by [Hong and Lee (2024)](https://arxiv.org/abs/2403.07691), ORPO
    offers an elegant solution to this problem by combining instruction tuning and
    preference alignment into a single, monolithic training process. ORPO modifies
    the standard language modeling objective, combining the negative log-likelihood
    loss with an odds ratio (OR) term. This OR loss weakly penalizes rejected responses
    while strongly rewarding preferred ones, allowing the model to simultaneously
    learn the target task and align with human preferences.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由[Hong and Lee (2024)](https://arxiv.org/abs/2403.07691)提出，ORPO通过将指令微调和偏好对齐结合到一个单一的训练过程中，提供了一个优雅的解决方案。ORPO修改了标准的语言建模目标，将负对数似然损失与赔率比（OR）项结合起来。这个OR损失在惩罚被拒绝的响应时相对较弱，而在奖励偏好的响应时则较强，从而使模型能够同时学习目标任务并与人类偏好对齐。
- en: '![](../Images/246e9df3fef5d97400c82a00263c7f5d.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/246e9df3fef5d97400c82a00263c7f5d.png)'
- en: ORPO has been implemented in the major fine-tuning libraries, like [TRL](https://github.com/huggingface/trl),
    [Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl), and [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory).
    In the next section, we will see how to use with TRL.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ORPO已经在主要的微调库中实现，例如[TRL](https://github.com/huggingface/trl)、[Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)和[LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)。在下一部分中，我们将看到如何与TRL一起使用。
- en: 💻 Fine-tuning Llama 3 with ORPO
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 💻 使用ORPO微调Llama 3
- en: '[Llama 3](https://github.com/meta-llama/llama3/tree/main) is the latest family
    of LLMs developed by Meta. The models were trained on an extensive dataset of
    **15 trillion tokens** (compared to 2T tokens for Llama 2). Two model sizes have
    been released: a 70 billion parameter model and a smaller 8 billion parameter
    model. The 70B model has already demonstrated impressive performance, scoring
    82 on the MMLU benchmark and 81.7 on the HumanEval benchmark.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[Llama 3](https://github.com/meta-llama/llama3/tree/main)是Meta开发的最新LLM系列。这些模型在一个庞大的数据集上进行了训练，共**15万亿个标记**（相比之下，Llama
    2为2万亿标记）。已经发布了两种模型大小：一个70亿参数模型和一个较小的80亿参数模型。70B模型已经展示了出色的性能，在MMLU基准测试中得分为82，在HumanEval基准测试中得分为81.7。'
- en: Llama 3 models also increased the context length up to 8,192 tokens (4,096 tokens
    for Llama 2), and potentially scale up to 32k with RoPE. Additionally, the models
    use a new tokenizer with a 128K-token vocabulary, reducing the number of tokens
    required to encode text by 15%. This vocabulary also explains the bump from 7B
    to 8B parameters.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 3模型还将上下文长度增加到8,192个标记（Llama 2为4,096个标记），并可能通过RoPE扩展到32k。此外，模型使用了一个新的分词器，具有128K标记的词汇表，将编码文本所需的标记数减少了15%。这个词汇表也解释了从7B到8B参数的提升。
- en: '![](../Images/1fb24c5aea241733ae340216214e0bb8.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1fb24c5aea241733ae340216214e0bb8.png)'
- en: '*Samples from ORPO-DPO-mix-40k (image by author).*'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*来自ORPO-DPO-mix-40k的样本（图片由作者提供）。*'
- en: 'ORPO requires a preference dataset, including a prompt, a chosen answer, and
    a rejected answer. In this example, we will use `[mlabonne/orpo-dpo-mix-40k](https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k)`,
    a combination of the following high-quality DPO datasets:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ORPO需要一个偏好数据集，包括一个提示语、一个选择的答案和一个被拒绝的答案。在这个示例中，我们将使用`[mlabonne/orpo-dpo-mix-40k](https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k)`，它是以下高质量DPO数据集的组合：
- en: '`[argilla/distilabel-capybara-dpo-7k-binarized](https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized)`:
    highly scored chosen answers >=5 (2,882 samples)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[argilla/distilabel-capybara-dpo-7k-binarized](https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized)`：得分较高的选择答案>=5（2,882个样本）'
- en: '`[argilla/distilabel-intel-orca-dpo-pairs](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs)`:
    highly scored chosen answers >=9, not in GSM8K (2,299 samples)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[argilla/distilabel-intel-orca-dpo-pairs](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs)`：得分较高的选择答案>=9，但不在GSM8K中（2,299个样本）'
- en: '`[argilla/ultrafeedback-binarized-preferences-cleaned](https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned)`:
    highly scored chosen answers >=5 (22,799 samples)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[argilla/ultrafeedback-binarized-preferences-cleaned](https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned)`：得分较高的选择答案>=5（22,799个样本）'
- en: '`[argilla/distilabel-math-preference-dpo](https://huggingface.co/datasets/argilla/distilabel-math-preference-dpo)`:
    highly scored chosen answers >=9 (2,181 samples)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[argilla/distilabel-math-preference-dpo](https://huggingface.co/datasets/argilla/distilabel-math-preference-dpo)`：得分较高的选择答案>=9（2,181个样本）'
- en: '`[unalignment/toxic-dpo-v0.2](https://huggingface.co/datasets/unalignment/toxic-dpo-v0.2)`
    (541 samples)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[unalignment/toxic-dpo-v0.2](https://huggingface.co/datasets/unalignment/toxic-dpo-v0.2)`（541个样本）'
- en: '`[M4-ai/prm_dpo_pairs_cleaned](https://huggingface.co/datasets/M4-ai/prm_dpo_pairs_cleaned)`
    (7,958 samples)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[M4-ai/prm_dpo_pairs_cleaned](https://huggingface.co/datasets/M4-ai/prm_dpo_pairs_cleaned)`（7,958个样本）'
- en: '`[jondurbin/truthy-dpo-v0.1](https://huggingface.co/datasets/jondurbin/truthy-dpo-v0.1)`
    (1,016 samples)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[jondurbin/truthy-dpo-v0.1](https://huggingface.co/datasets/jondurbin/truthy-dpo-v0.1)`（1,016个样本）'
- en: Thanks to [argilla](https://huggingface.co/argilla), [unalignment](https://huggingface.co/unalignment),
    [M4-ai](https://huggingface.co/M4-ai), and [jondurbin](https://huggingface.co/jondurbin)
    for providing the source datasets.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢[argilla](https://huggingface.co/argilla)、[unalignment](https://huggingface.co/unalignment)、[M4-ai](https://huggingface.co/M4-ai)和[jondurbin](https://huggingface.co/jondurbin)提供源数据集。
- en: 'As per usual, let’s start by installing the required libraries:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如常，我们从安装所需的库开始：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once it’s installed, we can import the necessary libraries and log in to W&B
    (optional):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦安装完成，我们就可以导入必要的库并登录W&B（可选）：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you have a recent GPU, you should also be able to use the [Flash Attention
    library](https://github.com/Dao-AILab/flash-attention) to replace the default
    eager attention implementation with a more efficient one.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一块较新的GPU，你应该也能够使用[Flash Attention库](https://github.com/Dao-AILab/flash-attention)，将默认的急切注意力实现替换为更高效的实现。
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the following, we will load the Llama 3 8B model in 4-bit precision thanks
    to [bitsandbytes](https://github.com/TimDettmers/bitsandbytes). We then set the
    LoRA configuration using [PEFT](https://github.com/huggingface/peft) for QLoRA.
    I’m also using the convenient `setup_chat_format()` function to modify the model
    and tokenizer for [ChatML](https://huggingface.co/docs/transformers/en/chat_templating#what-template-should-i-use)
    support. It automatically applies this chat template, adds special tokens, and
    resizes the model's embedding layer to match the new vocabulary size.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将通过[bitsandbytes](https://github.com/TimDettmers/bitsandbytes)以4位精度加载Llama
    3 8B模型。然后，我们使用[PEFT](https://github.com/huggingface/peft)为QLoRA设置LoRA配置。我还使用了方便的`setup_chat_format()`函数，来修改模型和分词器以支持[ChatML](https://huggingface.co/docs/transformers/en/chat_templating#what-template-should-i-use)。它会自动应用这个聊天模板，添加特殊标记，并调整模型的嵌入层大小以匹配新的词汇表大小。
- en: Note that you need to submit a request to access [meta-llama/Meta-Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B)
    and be logged in to your Hugging Face account. Alternatively, you can load ungated
    copies of the model, like [NousResearch/Meta-Llama-3-8B](https://huggingface.co/NousResearch/Meta-Llama-3-8B).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，你需要提交请求以访问[meta-llama/Meta-Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B)，并且登录你的Hugging
    Face账户。或者，你可以加载没有权限限制的模型副本，如[NousResearch/Meta-Llama-3-8B](https://huggingface.co/NousResearch/Meta-Llama-3-8B)。
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now that the model is ready for training, we can take care of the dataset. We
    load `[mlabonne/orpo-dpo-mix-40k](https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k)`
    and use the `apply_chat_template()` function to convert the "chosen" and "rejected"
    columns into the ChatML format. Note that I'm only using 1,000 samples and not
    the entire dataset, as it would take too long to run.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型已经准备好进行训练，我们可以开始处理数据集。我们加载`[mlabonne/orpo-dpo-mix-40k](https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k)`，并使用`apply_chat_template()`函数将“选择的”和“拒绝的”列转换为ChatML格式。请注意，我只使用了1,000个样本，而不是整个数据集，因为运行整个数据集需要的时间太长。
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'First, we need to set a few hyperparameters:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要设置一些超参数：
- en: '`learning_rate`: ORPO uses very low learning rates compared to traditional
    SFT or even DPO. This value of 8e-6 comes from the original paper, and roughly
    corresponds to an SFT learning rate of 1e-5 and a DPO learning rate of 5e-6\.
    I would recommend increasing it around 1e-6 for a real fine-tune.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate`：ORPO使用的学习率比传统的SFT或DPO要低得多。这个8e-6的值来自原论文，粗略对应SFT学习率的1e-5和DPO学习率的5e-6。我建议将其增加到1e-6左右，以进行真正的微调。'
- en: '`beta`: It is the $\lambda$ parameter in the paper, with a default value of
    0.1\. An appendix from the original paper shows how it''s been selected with an
    ablation study.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beta`：它是论文中的$\lambda$参数，默认值为0.1。原论文的附录展示了如何通过消融实验来选择这个值。'
- en: Other parameters, like `max_length` and batch size are set to use as much VRAM
    as available (~20 GB in this configuration). Ideally, we would train the model
    for 3-5 epochs, but we'll stick to 1 here.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他参数，如`max_length`和批处理大小，设置为尽可能使用所有的VRAM（在此配置下约为20GB）。理想情况下，我们会将模型训练3-5个周期，但在这里我们将只训练1个周期。
- en: Finally, we can train the model using the ORPOTrainer, which acts as a wrapper.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用ORPOTrainer进行模型训练，它充当了一个封装器。
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Training the model on these 1,000 samples took about 2 hours on an L4 GPU.
    Let’s check the W&B plots:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些1,000个样本上训练模型花费了大约2小时，使用的是L4 GPU。让我们检查一下W&B的图表：
- en: '![](../Images/4ea31a21aa2d957cab4982835b4be4e9.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ea31a21aa2d957cab4982835b4be4e9.png)'
- en: 'While the loss goes down, the difference between the chosen and rejects answers
    is not clear: the average margin and accuracy are only slightly above zero and
    0.5, respectively.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然损失下降，但选择答案和拒绝答案之间的差异并不明显：平均边际和准确率分别仅略高于零和0.5。
- en: In the original paper, the authors trained models on the `[Anthropic/hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf)`
    dataset (161k samples) for 10 epochs, which is a lot longer than our quick run.
    They also experimented with Llama 3 and kindly [shared their logs](https://huggingface.co/orpo-explorers/hf-llama3-8b-orpo-v0.0/tensorboard)
    with me (thanks [Jiwoo Hong](https://twitter.com/jiwoohong98)).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始论文中，作者在`[Anthropic/hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf)`数据集（161k样本）上训练模型，并进行了10个epochs的训练，训练时间远长于我们的快速运行。他们还对Llama
    3进行了实验，并友好地[分享了他们的日志](https://huggingface.co/orpo-explorers/hf-llama3-8b-orpo-v0.0/tensorboard)给我（感谢[Jiwoo
    Hong](https://twitter.com/jiwoohong98)）。
- en: To end this tutorial, let’s merge the QLoRA adapter with the base model and
    push it to the Hugging Face Hub.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程的结尾，让我们将QLoRA适配器与基础模型合并，并将其推送到Hugging Face Hub。
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Congrats, we finished this quick fine-tune of Llama 3: [mlabonne/OrpoLlama-3–8B](https://huggingface.co/mlabonne/OrpoLlama-3-8B).
    You can play with it using this [Hugging Face Space](https://huggingface.co/spaces/mlabonne/OrpoLlama-3-8B)
    (here’s a [notebook](https://colab.research.google.com/drive/1LcVUW5wsJTO2NGmozjji5CkC--646LgC?usp=sharing)
    to make your own). Although the model is undertrained, as highlighted by the W&B
    curves, I ran some evaluations on Nous’ benchmark suite using [LLM AutoEval](https://github.com/mlabonne/llm-autoeval).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜，我们完成了Llama 3的快速微调：[mlabonne/OrpoLlama-3–8B](https://huggingface.co/mlabonne/OrpoLlama-3-8B)。你可以通过这个[Hugging
    Face Space](https://huggingface.co/spaces/mlabonne/OrpoLlama-3-8B)来玩这个模型（这里有一个[notebook](https://colab.research.google.com/drive/1LcVUW5wsJTO2NGmozjji5CkC--646LgC?usp=sharing)来创建你自己的模型）。尽管如W&B曲线所示，模型还未经过充分训练，我在Nous的基准套件上使用[LLM
    AutoEval](https://github.com/mlabonne/llm-autoeval)进行了一些评估。
- en: '![](../Images/127c1583eb4512d807a080e6d4bc4223.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/127c1583eb4512d807a080e6d4bc4223.png)'
- en: Our ORPO fine-tune is actually pretty decent and improves the base model’s performance
    on every benchmark. This is encouraging and likely means that a fine-tune on the
    entire 40k samples would yield great results.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的ORPO微调实际上相当不错，且在每个基准测试中都提升了基础模型的表现。这是令人鼓舞的，且很可能意味着对整个40k样本进行微调将取得很好的结果。
- en: This is an exciting time for the open-source community, with more and more high-quality
    open-weight models being released. The gap between closed-source and open-weight
    models is slowly closing, and fine-tuning is an essential tool to get the best
    performance for your use cases.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是开源社区的激动人心时刻，越来越多高质量的开源权重模型被发布。闭源模型和开源权重模型之间的差距正在慢慢缩小，微调是获取最佳性能的必备工具。
- en: '![](../Images/885096bd13d294fa53f7589664afdd0b.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/885096bd13d294fa53f7589664afdd0b.png)'
- en: Image by author
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: Conclusion
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we introduced the ORPO algorithm and explained how it unifies
    the SFT and preference alignment stages into a single process. Then, we used TRL
    to fine-tune a Llama 3 8B model on a custom preference dataset. The final model
    shows encouraging results and highlights ORPO’s potential as a new fine-tuning
    paradigm.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了ORPO算法，并解释了它如何将SFT和偏好对齐阶段统一为一个过程。然后，我们使用TRL对Llama 3 8B模型进行微调，基于自定义的偏好数据集。最终的模型显示出令人鼓舞的结果，突出了ORPO作为一种新的微调范式的潜力。
- en: I hope it was useful, and I recommend running the [Colab notebook](https://colab.research.google.com/drive/1eHNWg9gnaXErdAa8_mcvjMupbSS6rDvi?usp=sharing)
    to fine-tune your own Llama 3 models. In future articles, we will see how to create
    high-quality datasets — a point that is often overlooked. If you liked this article,
    please follow me on [Hugging Face](https://huggingface.co/mlabonne/) and Twitter
    [@maximelabonne](https://twitter.com/maximelabonne).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这对你有帮助，我建议你运行[Colab notebook](https://colab.research.google.com/drive/1eHNWg9gnaXErdAa8_mcvjMupbSS6rDvi?usp=sharing)来微调你自己的Llama
    3模型。在未来的文章中，我们将讨论如何创建高质量的数据集——这是一个经常被忽视的重点。如果你喜欢这篇文章，请在[Hugging Face](https://huggingface.co/mlabonne/)和Twitter上关注我[@maximelabonne](https://twitter.com/maximelabonne)。
- en: References
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'J. Hong, N. Lee, and J. Thorne, [ORPO: Monolithic Preference Optimization without
    Reference Model](https://arxiv.org/abs/2403.07691). 2024.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'J. Hong, N. Lee 和 J. Thorne，[ORPO: 无需参考模型的单体偏好优化](https://arxiv.org/abs/2403.07691)。2024年。'
- en: 'L. von Werra et al., TRL: Transformer Reinforcement Learning. GitHub, 2020\.
    [Online]. Available: [https://github.com/huggingface/trl](https://github.com/huggingface/trl)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'L. von Werra 等人, TRL: Transformer 强化学习. GitHub, 2020\. [在线]. 可用链接: [https://github.com/huggingface/trl](https://github.com/huggingface/trl)'
- en: Bartolome, A., Martin, G., & Vila, D. (2023). Notus. In GitHub Repository. GitHub.
    [https://github.com/argilla-io/notus](https://github.com/argilla-io/notus)
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bartolome, A., Martin, G., & Vila, D. (2023). Notus. 在 GitHub 仓库中。GitHub. [https://github.com/argilla-io/notus](https://github.com/argilla-io/notus)
- en: AI at Meta, [Introducing Meta Llama 3](https://ai.meta.com/blog/meta-llama-3/),
    2024.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meta 的人工智能，[介绍 Meta Llama 3](https://ai.meta.com/blog/meta-llama-3/)，2024年。
