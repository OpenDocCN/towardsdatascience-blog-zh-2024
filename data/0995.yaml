- en: Fine-tune Llama 3 with ORPO
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¾®è°ƒ Llama 3 ä¸ ORPO
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/fine-tune-llama-3-with-orpo-56cfab2f9ada?source=collection_archive---------3-----------------------#2024-04-19](https://towardsdatascience.com/fine-tune-llama-3-with-orpo-56cfab2f9ada?source=collection_archive---------3-----------------------#2024-04-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/fine-tune-llama-3-with-orpo-56cfab2f9ada?source=collection_archive---------3-----------------------#2024-04-19](https://towardsdatascience.com/fine-tune-llama-3-with-orpo-56cfab2f9ada?source=collection_archive---------3-----------------------#2024-04-19)
- en: '*A cheaper and faster unified fine-tuning technique*'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*ä¸€ç§æ›´ä¾¿å®œã€æ›´å¿«é€Ÿçš„ç»Ÿä¸€å¾®è°ƒæŠ€æœ¯*'
- en: '[](https://medium.com/@mlabonne?source=post_page---byline--56cfab2f9ada--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page---byline--56cfab2f9ada--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--56cfab2f9ada--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--56cfab2f9ada--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page---byline--56cfab2f9ada--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne?source=post_page---byline--56cfab2f9ada--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page---byline--56cfab2f9ada--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--56cfab2f9ada--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--56cfab2f9ada--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page---byline--56cfab2f9ada--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--56cfab2f9ada--------------------------------)
    Â·8 min readÂ·Apr 19, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--56cfab2f9ada--------------------------------)
    Â·é˜…è¯»æ—¶é•¿ 8 åˆ†é’Ÿ Â·2024å¹´4æœˆ19æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/0024fbce8d1d7e5a96348655f9f40d35.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0024fbce8d1d7e5a96348655f9f40d35.png)'
- en: Image generated with DALL-E 3 by author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± DALL-E 3 ç”Ÿæˆï¼Œç”±ä½œè€…æä¾›
- en: ORPO is a **new exciting fine-tuning technique** that combines the traditional
    supervised fine-tuning and preference alignment stages into a single process.
    This reduces the computational resources and time required for training. Moreover,
    empirical results demonstrate that ORPO outperforms other alignment methods on
    various model sizes and benchmarks.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ORPO æ˜¯ä¸€ç§**æ–°å…´çš„ä»¤äººå…´å¥‹çš„å¾®è°ƒæŠ€æœ¯**ï¼Œå®ƒå°†ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒå’Œåå¥½å¯¹é½é˜¶æ®µåˆå¹¶ä¸ºä¸€ä¸ªå•ä¸€è¿‡ç¨‹ã€‚è¿™å‡å°‘äº†è®­ç»ƒæ‰€éœ€çš„è®¡ç®—èµ„æºå’Œæ—¶é—´ã€‚æ­¤å¤–ï¼Œå®è¯ç»“æœè¡¨æ˜ï¼ŒORPO
    åœ¨å¤šç§æ¨¡å‹è§„æ¨¡å’ŒåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†å…¶ä»–å¯¹é½æ–¹æ³•ã€‚
- en: In this article, we will fine-tune the new Llama 3 8B model using ORPO with
    the TRL library. The code is available on [Google Colab](https://colab.research.google.com/drive/1eHNWg9gnaXErdAa8_mcvjMupbSS6rDvi?usp=sharing)
    and in the [LLM Course](https://github.com/mlabonne/llm-course) on GitHub.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ ORPO å’Œ TRL åº“å¾®è°ƒæ–°çš„ Llama 3 8B æ¨¡å‹ã€‚ä»£ç å¯ä»¥åœ¨ [Google Colab](https://colab.research.google.com/drive/1eHNWg9gnaXErdAa8_mcvjMupbSS6rDvi?usp=sharing)
    å’Œ GitHub ä¸Šçš„ [LLM è¯¾ç¨‹](https://github.com/mlabonne/llm-course) ä¸­æ‰¾åˆ°ã€‚
- en: âš–ï¸ ORPO
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: âš–ï¸ ORPO
- en: 'Instruction tuning and preference alignment are essential techniques for adapting
    Large Language Models (LLMs) to specific tasks. Traditionally, this involves a
    multi-stage process: 1/ **Supervised Fine-Tuning** (SFT) on instructions to adapt
    the model to the target domain, followed by 2/ **preference alignment methods**
    like Reinforcement Learning with Human Feedback (RLHF) or Direct Preference Optimization
    (DPO) to increase the likelihood of generating preferred responses over rejected
    ones.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æŒ‡ä»¤å¾®è°ƒå’Œåå¥½å¯¹é½æ˜¯å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€‚åº”ç‰¹å®šä»»åŠ¡çš„å…³é”®æŠ€æœ¯ã€‚ä¼ ç»Ÿä¸Šï¼Œè¿™æ¶‰åŠåˆ°ä¸€ä¸ªå¤šé˜¶æ®µçš„è¿‡ç¨‹ï¼š1/ **ç›‘ç£å¾®è°ƒ**ï¼ˆSFTï¼‰ï¼Œä»¥ä½¿æ¨¡å‹é€‚åº”ç›®æ ‡é¢†åŸŸï¼Œç„¶åæ˜¯
    2/ **åå¥½å¯¹é½æ–¹æ³•**ï¼Œå¦‚äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æˆ–ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼Œä»¥æé«˜ç”Ÿæˆé¦–é€‰å“åº”è€Œéè¢«æ‹’ç»å“åº”çš„æ¦‚ç‡ã€‚
- en: '![](../Images/29cac2fc31790a8a74230a95bd7e7b4b.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29cac2fc31790a8a74230a95bd7e7b4b.png)'
- en: Image by author
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: However, researchers have identified a limitation in this approach. While SFT
    effectively adapts the model to the desired domain, it inadvertently **increases
    the probability of generating undesirable answers** alongside preferred ones.
    This is why the preference alignment stage is necessary to widen the gap between
    the likelihoods of preferred and rejected outputs.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œç ”ç©¶äººå‘˜å‘ç°è¿™ç§æ–¹æ³•å­˜åœ¨å±€é™æ€§ã€‚è™½ç„¶ SFT å¯ä»¥æœ‰æ•ˆåœ°å°†æ¨¡å‹é€‚åº”æ‰€éœ€é¢†åŸŸï¼Œä½†å®ƒæ— æ„ä¸­**å¢åŠ äº†ç”Ÿæˆä¸è‰¯ç­”æ¡ˆçš„æ¦‚ç‡**ï¼Œè€Œè¿™äº›ç­”æ¡ˆä¸é¦–é€‰ç­”æ¡ˆä¸€èµ·å‡ºç°ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåå¥½å¯¹é½é˜¶æ®µæ˜¯å¿…è¦çš„ï¼Œå®ƒèƒ½æ‹‰å¤§é¦–é€‰è¾“å‡ºå’Œè¢«æ‹’ç»è¾“å‡ºä¹‹é—´çš„æ¦‚ç‡å·®è·ã€‚
- en: '![](../Images/d27c0588ee55984b02754a279bb715f4.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d27c0588ee55984b02754a279bb715f4.png)'
- en: Note how the probability of rejected responses increases during supervised fine-tuning
    (image from the ORPO paper).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œç»è¿‡ç›‘ç£å¾®è°ƒåï¼Œè¢«æ‹’ç»çš„å“åº”æ¦‚ç‡æ˜¯å¦‚ä½•å¢åŠ çš„ï¼ˆå›¾ç‰‡æ¥è‡ªORPOè®ºæ–‡ï¼‰ã€‚
- en: Introduced by [Hong and Lee (2024)](https://arxiv.org/abs/2403.07691), ORPO
    offers an elegant solution to this problem by combining instruction tuning and
    preference alignment into a single, monolithic training process. ORPO modifies
    the standard language modeling objective, combining the negative log-likelihood
    loss with an odds ratio (OR) term. This OR loss weakly penalizes rejected responses
    while strongly rewarding preferred ones, allowing the model to simultaneously
    learn the target task and align with human preferences.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±[Hong and Lee (2024)](https://arxiv.org/abs/2403.07691)æå‡ºï¼ŒORPOé€šè¿‡å°†æŒ‡ä»¤å¾®è°ƒå’Œåå¥½å¯¹é½ç»“åˆåˆ°ä¸€ä¸ªå•ä¸€çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæä¾›äº†ä¸€ä¸ªä¼˜é›…çš„è§£å†³æ–¹æ¡ˆã€‚ORPOä¿®æ”¹äº†æ ‡å‡†çš„è¯­è¨€å»ºæ¨¡ç›®æ ‡ï¼Œå°†è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±ä¸èµ”ç‡æ¯”ï¼ˆORï¼‰é¡¹ç»“åˆèµ·æ¥ã€‚è¿™ä¸ªORæŸå¤±åœ¨æƒ©ç½šè¢«æ‹’ç»çš„å“åº”æ—¶ç›¸å¯¹è¾ƒå¼±ï¼Œè€Œåœ¨å¥–åŠ±åå¥½çš„å“åº”æ—¶åˆ™è¾ƒå¼ºï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤ŸåŒæ—¶å­¦ä¹ ç›®æ ‡ä»»åŠ¡å¹¶ä¸äººç±»åå¥½å¯¹é½ã€‚
- en: '![](../Images/246e9df3fef5d97400c82a00263c7f5d.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/246e9df3fef5d97400c82a00263c7f5d.png)'
- en: ORPO has been implemented in the major fine-tuning libraries, like [TRL](https://github.com/huggingface/trl),
    [Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl), and [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory).
    In the next section, we will see how to use with TRL.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ORPOå·²ç»åœ¨ä¸»è¦çš„å¾®è°ƒåº“ä¸­å®ç°ï¼Œä¾‹å¦‚[TRL](https://github.com/huggingface/trl)ã€[Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)å’Œ[LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)ã€‚åœ¨ä¸‹ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•ä¸TRLä¸€èµ·ä½¿ç”¨ã€‚
- en: ğŸ’» Fine-tuning Llama 3 with ORPO
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ’» ä½¿ç”¨ORPOå¾®è°ƒLlama 3
- en: '[Llama 3](https://github.com/meta-llama/llama3/tree/main) is the latest family
    of LLMs developed by Meta. The models were trained on an extensive dataset of
    **15 trillion tokens** (compared to 2T tokens for Llama 2). Two model sizes have
    been released: a 70 billion parameter model and a smaller 8 billion parameter
    model. The 70B model has already demonstrated impressive performance, scoring
    82 on the MMLU benchmark and 81.7 on the HumanEval benchmark.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[Llama 3](https://github.com/meta-llama/llama3/tree/main)æ˜¯Metaå¼€å‘çš„æœ€æ–°LLMç³»åˆ—ã€‚è¿™äº›æ¨¡å‹åœ¨ä¸€ä¸ªåºå¤§çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œå…±**15ä¸‡äº¿ä¸ªæ ‡è®°**ï¼ˆç›¸æ¯”ä¹‹ä¸‹ï¼ŒLlama
    2ä¸º2ä¸‡äº¿æ ‡è®°ï¼‰ã€‚å·²ç»å‘å¸ƒäº†ä¸¤ç§æ¨¡å‹å¤§å°ï¼šä¸€ä¸ª70äº¿å‚æ•°æ¨¡å‹å’Œä¸€ä¸ªè¾ƒå°çš„80äº¿å‚æ•°æ¨¡å‹ã€‚70Bæ¨¡å‹å·²ç»å±•ç¤ºäº†å‡ºè‰²çš„æ€§èƒ½ï¼Œåœ¨MMLUåŸºå‡†æµ‹è¯•ä¸­å¾—åˆ†ä¸º82ï¼Œåœ¨HumanEvalåŸºå‡†æµ‹è¯•ä¸­å¾—åˆ†ä¸º81.7ã€‚'
- en: Llama 3 models also increased the context length up to 8,192 tokens (4,096 tokens
    for Llama 2), and potentially scale up to 32k with RoPE. Additionally, the models
    use a new tokenizer with a 128K-token vocabulary, reducing the number of tokens
    required to encode text by 15%. This vocabulary also explains the bump from 7B
    to 8B parameters.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 3æ¨¡å‹è¿˜å°†ä¸Šä¸‹æ–‡é•¿åº¦å¢åŠ åˆ°8,192ä¸ªæ ‡è®°ï¼ˆLlama 2ä¸º4,096ä¸ªæ ‡è®°ï¼‰ï¼Œå¹¶å¯èƒ½é€šè¿‡RoPEæ‰©å±•åˆ°32kã€‚æ­¤å¤–ï¼Œæ¨¡å‹ä½¿ç”¨äº†ä¸€ä¸ªæ–°çš„åˆ†è¯å™¨ï¼Œå…·æœ‰128Kæ ‡è®°çš„è¯æ±‡è¡¨ï¼Œå°†ç¼–ç æ–‡æœ¬æ‰€éœ€çš„æ ‡è®°æ•°å‡å°‘äº†15%ã€‚è¿™ä¸ªè¯æ±‡è¡¨ä¹Ÿè§£é‡Šäº†ä»7Båˆ°8Bå‚æ•°çš„æå‡ã€‚
- en: '![](../Images/1fb24c5aea241733ae340216214e0bb8.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1fb24c5aea241733ae340216214e0bb8.png)'
- en: '*Samples from ORPO-DPO-mix-40k (image by author).*'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ¥è‡ªORPO-DPO-mix-40kçš„æ ·æœ¬ï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰ã€‚*'
- en: 'ORPO requires a preference dataset, including a prompt, a chosen answer, and
    a rejected answer. In this example, we will use `[mlabonne/orpo-dpo-mix-40k](https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k)`,
    a combination of the following high-quality DPO datasets:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ORPOéœ€è¦ä¸€ä¸ªåå¥½æ•°æ®é›†ï¼ŒåŒ…æ‹¬ä¸€ä¸ªæç¤ºè¯­ã€ä¸€ä¸ªé€‰æ‹©çš„ç­”æ¡ˆå’Œä¸€ä¸ªè¢«æ‹’ç»çš„ç­”æ¡ˆã€‚åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`[mlabonne/orpo-dpo-mix-40k](https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k)`ï¼Œå®ƒæ˜¯ä»¥ä¸‹é«˜è´¨é‡DPOæ•°æ®é›†çš„ç»„åˆï¼š
- en: '`[argilla/distilabel-capybara-dpo-7k-binarized](https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized)`:
    highly scored chosen answers >=5 (2,882 samples)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[argilla/distilabel-capybara-dpo-7k-binarized](https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized)`ï¼šå¾—åˆ†è¾ƒé«˜çš„é€‰æ‹©ç­”æ¡ˆ>=5ï¼ˆ2,882ä¸ªæ ·æœ¬ï¼‰'
- en: '`[argilla/distilabel-intel-orca-dpo-pairs](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs)`:
    highly scored chosen answers >=9, not in GSM8K (2,299 samples)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[argilla/distilabel-intel-orca-dpo-pairs](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs)`ï¼šå¾—åˆ†è¾ƒé«˜çš„é€‰æ‹©ç­”æ¡ˆ>=9ï¼Œä½†ä¸åœ¨GSM8Kä¸­ï¼ˆ2,299ä¸ªæ ·æœ¬ï¼‰'
- en: '`[argilla/ultrafeedback-binarized-preferences-cleaned](https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned)`:
    highly scored chosen answers >=5 (22,799 samples)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[argilla/ultrafeedback-binarized-preferences-cleaned](https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned)`ï¼šå¾—åˆ†è¾ƒé«˜çš„é€‰æ‹©ç­”æ¡ˆ>=5ï¼ˆ22,799ä¸ªæ ·æœ¬ï¼‰'
- en: '`[argilla/distilabel-math-preference-dpo](https://huggingface.co/datasets/argilla/distilabel-math-preference-dpo)`:
    highly scored chosen answers >=9 (2,181 samples)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[argilla/distilabel-math-preference-dpo](https://huggingface.co/datasets/argilla/distilabel-math-preference-dpo)`ï¼šå¾—åˆ†è¾ƒé«˜çš„é€‰æ‹©ç­”æ¡ˆ>=9ï¼ˆ2,181ä¸ªæ ·æœ¬ï¼‰'
- en: '`[unalignment/toxic-dpo-v0.2](https://huggingface.co/datasets/unalignment/toxic-dpo-v0.2)`
    (541 samples)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[unalignment/toxic-dpo-v0.2](https://huggingface.co/datasets/unalignment/toxic-dpo-v0.2)`ï¼ˆ541ä¸ªæ ·æœ¬ï¼‰'
- en: '`[M4-ai/prm_dpo_pairs_cleaned](https://huggingface.co/datasets/M4-ai/prm_dpo_pairs_cleaned)`
    (7,958 samples)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[M4-ai/prm_dpo_pairs_cleaned](https://huggingface.co/datasets/M4-ai/prm_dpo_pairs_cleaned)`ï¼ˆ7,958ä¸ªæ ·æœ¬ï¼‰'
- en: '`[jondurbin/truthy-dpo-v0.1](https://huggingface.co/datasets/jondurbin/truthy-dpo-v0.1)`
    (1,016 samples)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[jondurbin/truthy-dpo-v0.1](https://huggingface.co/datasets/jondurbin/truthy-dpo-v0.1)`ï¼ˆ1,016ä¸ªæ ·æœ¬ï¼‰'
- en: Thanks to [argilla](https://huggingface.co/argilla), [unalignment](https://huggingface.co/unalignment),
    [M4-ai](https://huggingface.co/M4-ai), and [jondurbin](https://huggingface.co/jondurbin)
    for providing the source datasets.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢[argilla](https://huggingface.co/argilla)ã€[unalignment](https://huggingface.co/unalignment)ã€[M4-ai](https://huggingface.co/M4-ai)å’Œ[jondurbin](https://huggingface.co/jondurbin)æä¾›æºæ•°æ®é›†ã€‚
- en: 'As per usual, letâ€™s start by installing the required libraries:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å¸¸ï¼Œæˆ‘ä»¬ä»å®‰è£…æ‰€éœ€çš„åº“å¼€å§‹ï¼š
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once itâ€™s installed, we can import the necessary libraries and log in to W&B
    (optional):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦å®‰è£…å®Œæˆï¼Œæˆ‘ä»¬å°±å¯ä»¥å¯¼å…¥å¿…è¦çš„åº“å¹¶ç™»å½•W&Bï¼ˆå¯é€‰ï¼‰ï¼š
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you have a recent GPU, you should also be able to use the [Flash Attention
    library](https://github.com/Dao-AILab/flash-attention) to replace the default
    eager attention implementation with a more efficient one.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æœ‰ä¸€å—è¾ƒæ–°çš„GPUï¼Œä½ åº”è¯¥ä¹Ÿèƒ½å¤Ÿä½¿ç”¨[Flash Attentionåº“](https://github.com/Dao-AILab/flash-attention)ï¼Œå°†é»˜è®¤çš„æ€¥åˆ‡æ³¨æ„åŠ›å®ç°æ›¿æ¢ä¸ºæ›´é«˜æ•ˆçš„å®ç°ã€‚
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the following, we will load the Llama 3 8B model in 4-bit precision thanks
    to [bitsandbytes](https://github.com/TimDettmers/bitsandbytes). We then set the
    LoRA configuration using [PEFT](https://github.com/huggingface/peft) for QLoRA.
    Iâ€™m also using the convenient `setup_chat_format()` function to modify the model
    and tokenizer for [ChatML](https://huggingface.co/docs/transformers/en/chat_templating#what-template-should-i-use)
    support. It automatically applies this chat template, adds special tokens, and
    resizes the model's embedding layer to match the new vocabulary size.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„æ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬å°†é€šè¿‡[bitsandbytes](https://github.com/TimDettmers/bitsandbytes)ä»¥4ä½ç²¾åº¦åŠ è½½Llama
    3 8Bæ¨¡å‹ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨[PEFT](https://github.com/huggingface/peft)ä¸ºQLoRAè®¾ç½®LoRAé…ç½®ã€‚æˆ‘è¿˜ä½¿ç”¨äº†æ–¹ä¾¿çš„`setup_chat_format()`å‡½æ•°ï¼Œæ¥ä¿®æ”¹æ¨¡å‹å’Œåˆ†è¯å™¨ä»¥æ”¯æŒ[ChatML](https://huggingface.co/docs/transformers/en/chat_templating#what-template-should-i-use)ã€‚å®ƒä¼šè‡ªåŠ¨åº”ç”¨è¿™ä¸ªèŠå¤©æ¨¡æ¿ï¼Œæ·»åŠ ç‰¹æ®Šæ ‡è®°ï¼Œå¹¶è°ƒæ•´æ¨¡å‹çš„åµŒå…¥å±‚å¤§å°ä»¥åŒ¹é…æ–°çš„è¯æ±‡è¡¨å¤§å°ã€‚
- en: Note that you need to submit a request to access [meta-llama/Meta-Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B)
    and be logged in to your Hugging Face account. Alternatively, you can load ungated
    copies of the model, like [NousResearch/Meta-Llama-3-8B](https://huggingface.co/NousResearch/Meta-Llama-3-8B).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œä½ éœ€è¦æäº¤è¯·æ±‚ä»¥è®¿é—®[meta-llama/Meta-Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B)ï¼Œå¹¶ä¸”ç™»å½•ä½ çš„Hugging
    Faceè´¦æˆ·ã€‚æˆ–è€…ï¼Œä½ å¯ä»¥åŠ è½½æ²¡æœ‰æƒé™é™åˆ¶çš„æ¨¡å‹å‰¯æœ¬ï¼Œå¦‚[NousResearch/Meta-Llama-3-8B](https://huggingface.co/NousResearch/Meta-Llama-3-8B)ã€‚
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now that the model is ready for training, we can take care of the dataset. We
    load `[mlabonne/orpo-dpo-mix-40k](https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k)`
    and use the `apply_chat_template()` function to convert the "chosen" and "rejected"
    columns into the ChatML format. Note that I'm only using 1,000 samples and not
    the entire dataset, as it would take too long to run.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ¨¡å‹å·²ç»å‡†å¤‡å¥½è¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹å¤„ç†æ•°æ®é›†ã€‚æˆ‘ä»¬åŠ è½½`[mlabonne/orpo-dpo-mix-40k](https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k)`ï¼Œå¹¶ä½¿ç”¨`apply_chat_template()`å‡½æ•°å°†â€œé€‰æ‹©çš„â€å’Œâ€œæ‹’ç»çš„â€åˆ—è½¬æ¢ä¸ºChatMLæ ¼å¼ã€‚è¯·æ³¨æ„ï¼Œæˆ‘åªä½¿ç”¨äº†1,000ä¸ªæ ·æœ¬ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ•°æ®é›†ï¼Œå› ä¸ºè¿è¡Œæ•´ä¸ªæ•°æ®é›†éœ€è¦çš„æ—¶é—´å¤ªé•¿ã€‚
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'First, we need to set a few hyperparameters:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦è®¾ç½®ä¸€äº›è¶…å‚æ•°ï¼š
- en: '`learning_rate`: ORPO uses very low learning rates compared to traditional
    SFT or even DPO. This value of 8e-6 comes from the original paper, and roughly
    corresponds to an SFT learning rate of 1e-5 and a DPO learning rate of 5e-6\.
    I would recommend increasing it around 1e-6 for a real fine-tune.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate`ï¼šORPOä½¿ç”¨çš„å­¦ä¹ ç‡æ¯”ä¼ ç»Ÿçš„SFTæˆ–DPOè¦ä½å¾—å¤šã€‚è¿™ä¸ª8e-6çš„å€¼æ¥è‡ªåŸè®ºæ–‡ï¼Œç²—ç•¥å¯¹åº”SFTå­¦ä¹ ç‡çš„1e-5å’ŒDPOå­¦ä¹ ç‡çš„5e-6ã€‚æˆ‘å»ºè®®å°†å…¶å¢åŠ åˆ°1e-6å·¦å³ï¼Œä»¥è¿›è¡ŒçœŸæ­£çš„å¾®è°ƒã€‚'
- en: '`beta`: It is the $\lambda$ parameter in the paper, with a default value of
    0.1\. An appendix from the original paper shows how it''s been selected with an
    ablation study.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beta`ï¼šå®ƒæ˜¯è®ºæ–‡ä¸­çš„$\lambda$å‚æ•°ï¼Œé»˜è®¤å€¼ä¸º0.1ã€‚åŸè®ºæ–‡çš„é™„å½•å±•ç¤ºäº†å¦‚ä½•é€šè¿‡æ¶ˆèå®éªŒæ¥é€‰æ‹©è¿™ä¸ªå€¼ã€‚'
- en: Other parameters, like `max_length` and batch size are set to use as much VRAM
    as available (~20 GB in this configuration). Ideally, we would train the model
    for 3-5 epochs, but we'll stick to 1 here.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…¶ä»–å‚æ•°ï¼Œå¦‚`max_length`å’Œæ‰¹å¤„ç†å¤§å°ï¼Œè®¾ç½®ä¸ºå°½å¯èƒ½ä½¿ç”¨æ‰€æœ‰çš„VRAMï¼ˆåœ¨æ­¤é…ç½®ä¸‹çº¦ä¸º20GBï¼‰ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¼šå°†æ¨¡å‹è®­ç»ƒ3-5ä¸ªå‘¨æœŸï¼Œä½†åœ¨è¿™é‡Œæˆ‘ä»¬å°†åªè®­ç»ƒ1ä¸ªå‘¨æœŸã€‚
- en: Finally, we can train the model using the ORPOTrainer, which acts as a wrapper.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ORPOTrainerè¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œå®ƒå……å½“äº†ä¸€ä¸ªå°è£…å™¨ã€‚
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Training the model on these 1,000 samples took about 2 hours on an L4 GPU.
    Letâ€™s check the W&B plots:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™äº›1,000ä¸ªæ ·æœ¬ä¸Šè®­ç»ƒæ¨¡å‹èŠ±è´¹äº†å¤§çº¦2å°æ—¶ï¼Œä½¿ç”¨çš„æ˜¯L4 GPUã€‚è®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹W&Bçš„å›¾è¡¨ï¼š
- en: '![](../Images/4ea31a21aa2d957cab4982835b4be4e9.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ea31a21aa2d957cab4982835b4be4e9.png)'
- en: 'While the loss goes down, the difference between the chosen and rejects answers
    is not clear: the average margin and accuracy are only slightly above zero and
    0.5, respectively.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æŸå¤±ä¸‹é™ï¼Œä½†é€‰æ‹©ç­”æ¡ˆå’Œæ‹’ç»ç­”æ¡ˆä¹‹é—´çš„å·®å¼‚å¹¶ä¸æ˜æ˜¾ï¼šå¹³å‡è¾¹é™…å’Œå‡†ç¡®ç‡åˆ†åˆ«ä»…ç•¥é«˜äºé›¶å’Œ0.5ã€‚
- en: In the original paper, the authors trained models on the `[Anthropic/hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf)`
    dataset (161k samples) for 10 epochs, which is a lot longer than our quick run.
    They also experimented with Llama 3 and kindly [shared their logs](https://huggingface.co/orpo-explorers/hf-llama3-8b-orpo-v0.0/tensorboard)
    with me (thanks [Jiwoo Hong](https://twitter.com/jiwoohong98)).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŸå§‹è®ºæ–‡ä¸­ï¼Œä½œè€…åœ¨`[Anthropic/hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf)`æ•°æ®é›†ï¼ˆ161kæ ·æœ¬ï¼‰ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œå¹¶è¿›è¡Œäº†10ä¸ªepochsçš„è®­ç»ƒï¼Œè®­ç»ƒæ—¶é—´è¿œé•¿äºæˆ‘ä»¬çš„å¿«é€Ÿè¿è¡Œã€‚ä»–ä»¬è¿˜å¯¹Llama
    3è¿›è¡Œäº†å®éªŒï¼Œå¹¶å‹å¥½åœ°[åˆ†äº«äº†ä»–ä»¬çš„æ—¥å¿—](https://huggingface.co/orpo-explorers/hf-llama3-8b-orpo-v0.0/tensorboard)ç»™æˆ‘ï¼ˆæ„Ÿè°¢[Jiwoo
    Hong](https://twitter.com/jiwoohong98)ï¼‰ã€‚
- en: To end this tutorial, letâ€™s merge the QLoRA adapter with the base model and
    push it to the Hugging Face Hub.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ•™ç¨‹çš„ç»“å°¾ï¼Œè®©æˆ‘ä»¬å°†QLoRAé€‚é…å™¨ä¸åŸºç¡€æ¨¡å‹åˆå¹¶ï¼Œå¹¶å°†å…¶æ¨é€åˆ°Hugging Face Hubã€‚
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Congrats, we finished this quick fine-tune of Llama 3: [mlabonne/OrpoLlama-3â€“8B](https://huggingface.co/mlabonne/OrpoLlama-3-8B).
    You can play with it using this [Hugging Face Space](https://huggingface.co/spaces/mlabonne/OrpoLlama-3-8B)
    (hereâ€™s a [notebook](https://colab.research.google.com/drive/1LcVUW5wsJTO2NGmozjji5CkC--646LgC?usp=sharing)
    to make your own). Although the model is undertrained, as highlighted by the W&B
    curves, I ran some evaluations on Nousâ€™ benchmark suite using [LLM AutoEval](https://github.com/mlabonne/llm-autoeval).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œï¼Œæˆ‘ä»¬å®Œæˆäº†Llama 3çš„å¿«é€Ÿå¾®è°ƒï¼š[mlabonne/OrpoLlama-3â€“8B](https://huggingface.co/mlabonne/OrpoLlama-3-8B)ã€‚ä½ å¯ä»¥é€šè¿‡è¿™ä¸ª[Hugging
    Face Space](https://huggingface.co/spaces/mlabonne/OrpoLlama-3-8B)æ¥ç©è¿™ä¸ªæ¨¡å‹ï¼ˆè¿™é‡Œæœ‰ä¸€ä¸ª[notebook](https://colab.research.google.com/drive/1LcVUW5wsJTO2NGmozjji5CkC--646LgC?usp=sharing)æ¥åˆ›å»ºä½ è‡ªå·±çš„æ¨¡å‹ï¼‰ã€‚å°½ç®¡å¦‚W&Bæ›²çº¿æ‰€ç¤ºï¼Œæ¨¡å‹è¿˜æœªç»è¿‡å……åˆ†è®­ç»ƒï¼Œæˆ‘åœ¨Nousçš„åŸºå‡†å¥—ä»¶ä¸Šä½¿ç”¨[LLM
    AutoEval](https://github.com/mlabonne/llm-autoeval)è¿›è¡Œäº†ä¸€äº›è¯„ä¼°ã€‚
- en: '![](../Images/127c1583eb4512d807a080e6d4bc4223.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/127c1583eb4512d807a080e6d4bc4223.png)'
- en: Our ORPO fine-tune is actually pretty decent and improves the base modelâ€™s performance
    on every benchmark. This is encouraging and likely means that a fine-tune on the
    entire 40k samples would yield great results.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ORPOå¾®è°ƒå®é™…ä¸Šç›¸å½“ä¸é”™ï¼Œä¸”åœ¨æ¯ä¸ªåŸºå‡†æµ‹è¯•ä¸­éƒ½æå‡äº†åŸºç¡€æ¨¡å‹çš„è¡¨ç°ã€‚è¿™æ˜¯ä»¤äººé¼“èˆçš„ï¼Œä¸”å¾ˆå¯èƒ½æ„å‘³ç€å¯¹æ•´ä¸ª40kæ ·æœ¬è¿›è¡Œå¾®è°ƒå°†å–å¾—å¾ˆå¥½çš„ç»“æœã€‚
- en: This is an exciting time for the open-source community, with more and more high-quality
    open-weight models being released. The gap between closed-source and open-weight
    models is slowly closing, and fine-tuning is an essential tool to get the best
    performance for your use cases.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å¼€æºç¤¾åŒºçš„æ¿€åŠ¨äººå¿ƒæ—¶åˆ»ï¼Œè¶Šæ¥è¶Šå¤šé«˜è´¨é‡çš„å¼€æºæƒé‡æ¨¡å‹è¢«å‘å¸ƒã€‚é—­æºæ¨¡å‹å’Œå¼€æºæƒé‡æ¨¡å‹ä¹‹é—´çš„å·®è·æ­£åœ¨æ…¢æ…¢ç¼©å°ï¼Œå¾®è°ƒæ˜¯è·å–æœ€ä½³æ€§èƒ½çš„å¿…å¤‡å·¥å…·ã€‚
- en: '![](../Images/885096bd13d294fa53f7589664afdd0b.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/885096bd13d294fa53f7589664afdd0b.png)'
- en: Image by author
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼šä½œè€…
- en: Conclusion
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: In this article, we introduced the ORPO algorithm and explained how it unifies
    the SFT and preference alignment stages into a single process. Then, we used TRL
    to fine-tune a Llama 3 8B model on a custom preference dataset. The final model
    shows encouraging results and highlights ORPOâ€™s potential as a new fine-tuning
    paradigm.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ORPOç®—æ³•ï¼Œå¹¶è§£é‡Šäº†å®ƒå¦‚ä½•å°†SFTå’Œåå¥½å¯¹é½é˜¶æ®µç»Ÿä¸€ä¸ºä¸€ä¸ªè¿‡ç¨‹ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨TRLå¯¹Llama 3 8Bæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼ŒåŸºäºè‡ªå®šä¹‰çš„åå¥½æ•°æ®é›†ã€‚æœ€ç»ˆçš„æ¨¡å‹æ˜¾ç¤ºå‡ºä»¤äººé¼“èˆçš„ç»“æœï¼Œçªå‡ºäº†ORPOä½œä¸ºä¸€ç§æ–°çš„å¾®è°ƒèŒƒå¼çš„æ½œåŠ›ã€‚
- en: I hope it was useful, and I recommend running the [Colab notebook](https://colab.research.google.com/drive/1eHNWg9gnaXErdAa8_mcvjMupbSS6rDvi?usp=sharing)
    to fine-tune your own Llama 3 models. In future articles, we will see how to create
    high-quality datasets â€” a point that is often overlooked. If you liked this article,
    please follow me on [Hugging Face](https://huggingface.co/mlabonne/) and Twitter
    [@maximelabonne](https://twitter.com/maximelabonne).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å¸Œæœ›è¿™å¯¹ä½ æœ‰å¸®åŠ©ï¼Œæˆ‘å»ºè®®ä½ è¿è¡Œ[Colab notebook](https://colab.research.google.com/drive/1eHNWg9gnaXErdAa8_mcvjMupbSS6rDvi?usp=sharing)æ¥å¾®è°ƒä½ è‡ªå·±çš„Llama
    3æ¨¡å‹ã€‚åœ¨æœªæ¥çš„æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºå¦‚ä½•åˆ›å»ºé«˜è´¨é‡çš„æ•°æ®é›†â€”â€”è¿™æ˜¯ä¸€ä¸ªç»å¸¸è¢«å¿½è§†çš„é‡ç‚¹ã€‚å¦‚æœä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œè¯·åœ¨[Hugging Face](https://huggingface.co/mlabonne/)å’ŒTwitterä¸Šå…³æ³¨æˆ‘[@maximelabonne](https://twitter.com/maximelabonne)ã€‚
- en: References
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: 'J. Hong, N. Lee, and J. Thorne, [ORPO: Monolithic Preference Optimization without
    Reference Model](https://arxiv.org/abs/2403.07691). 2024.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'J. Hong, N. Lee å’Œ J. Thorneï¼Œ[ORPO: æ— éœ€å‚è€ƒæ¨¡å‹çš„å•ä½“åå¥½ä¼˜åŒ–](https://arxiv.org/abs/2403.07691)ã€‚2024å¹´ã€‚'
- en: 'L. von Werra et al., TRL: Transformer Reinforcement Learning. GitHub, 2020\.
    [Online]. Available: [https://github.com/huggingface/trl](https://github.com/huggingface/trl)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'L. von Werra ç­‰äºº, TRL: Transformer å¼ºåŒ–å­¦ä¹ . GitHub, 2020\. [åœ¨çº¿]. å¯ç”¨é“¾æ¥: [https://github.com/huggingface/trl](https://github.com/huggingface/trl)'
- en: Bartolome, A., Martin, G., & Vila, D. (2023). Notus. In GitHub Repository. GitHub.
    [https://github.com/argilla-io/notus](https://github.com/argilla-io/notus)
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bartolome, A., Martin, G., & Vila, D. (2023). Notus. åœ¨ GitHub ä»“åº“ä¸­ã€‚GitHub. [https://github.com/argilla-io/notus](https://github.com/argilla-io/notus)
- en: AI at Meta, [Introducing Meta Llama 3](https://ai.meta.com/blog/meta-llama-3/),
    2024.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meta çš„äººå·¥æ™ºèƒ½ï¼Œ[ä»‹ç» Meta Llama 3](https://ai.meta.com/blog/meta-llama-3/)ï¼Œ2024å¹´ã€‚
