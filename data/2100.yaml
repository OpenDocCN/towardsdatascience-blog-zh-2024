- en: A visual explanation of LLM hyperparameters
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM 超参数的视觉解释
- en: 原文：[https://towardsdatascience.com/a-visual-explanation-of-llm-hyperparameters-daf61d3b006e?source=collection_archive---------3-----------------------#2024-08-29](https://towardsdatascience.com/a-visual-explanation-of-llm-hyperparameters-daf61d3b006e?source=collection_archive---------3-----------------------#2024-08-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-visual-explanation-of-llm-hyperparameters-daf61d3b006e?source=collection_archive---------3-----------------------#2024-08-29](https://towardsdatascience.com/a-visual-explanation-of-llm-hyperparameters-daf61d3b006e?source=collection_archive---------3-----------------------#2024-08-29)
- en: Understand temperature, Top-k, Top-p, Frequency & Precense Penalty once and
    for all.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 彻底理解温度、Top-k、Top-p、频率和存在惩罚。
- en: '[](https://medium.com/@jenn-j-dev?source=post_page---byline--daf61d3b006e--------------------------------)[![Jenn
    J.](../Images/d2ef3b8f454d4f7a974edd5a965a80e8.png)](https://medium.com/@jenn-j-dev?source=post_page---byline--daf61d3b006e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--daf61d3b006e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--daf61d3b006e--------------------------------)
    [Jenn J.](https://medium.com/@jenn-j-dev?source=post_page---byline--daf61d3b006e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jenn-j-dev?source=post_page---byline--daf61d3b006e--------------------------------)[![Jenn
    J.](../Images/d2ef3b8f454d4f7a974edd5a965a80e8.png)](https://medium.com/@jenn-j-dev?source=post_page---byline--daf61d3b006e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--daf61d3b006e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--daf61d3b006e--------------------------------)
    [Jenn J.](https://medium.com/@jenn-j-dev?source=post_page---byline--daf61d3b006e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--daf61d3b006e--------------------------------)
    ·7 min read·Aug 29, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--daf61d3b006e--------------------------------)
    ·阅读时间 7 分钟 ·2024年8月29日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Getting a handle on temperature, Top-k, Top-p, frequency, and presence penalties
    can be a bit of a challenge, especially when you’re just starting out with LLM
    hyperparameters. Terms like “Top-k” and “presence penalty” can feel a bit overwhelming
    at first.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 理解温度、Top-k、Top-p、频率和存在惩罚可能会有点挑战，尤其是在你刚开始接触 LLM 超参数的时候。“Top-k”和“存在惩罚”之类的术语一开始可能会让人感到有些不知所措。
- en: 'When you look up “Top-k,” you might find a definition like: “Top-k sampling
    limits the model’s selection of the next word to only the top-k most probable
    options, based on their predicted probabilities.” That’s a lot to take in! But
    how does this actually help when you’re working on prompt engineering?'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当你查找“Top-k”时，你可能会看到类似这样的定义：“Top-k 采样将模型选择下一个单词的范围限制为仅选择前 k 个最有可能的选项，基于它们的预测概率。”这听起来很多！但当你在进行提示工程时，这究竟如何帮助你呢？
- en: If you’re anything like me and learn best with visuals, let’s break these down
    together and make these concepts easy to understand once and for all.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你像我一样，最喜欢通过视觉学习，那么让我们一起拆解这些内容，彻底理解这些概念，让它们变得容易理解。
- en: LLMs under the hood
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs 的内部工作原理
- en: Before we dive into LLM hyperparameters, let’s do a quick thought experiment.
    Imagine hearing the phrase “A cup of …”. Most of us would expect the next word
    to be something like “coffee” (or “tea” if you’re a tea person!) You probably
    wouldn’t think of “stars” or “courage” right away.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨 LLM 超参数之前，让我们做一个简单的思维实验。试想听到“一个杯子……”这个短语。大多数人会期望下一个单词是“咖啡”（如果你是茶爱好者，可能是“茶”！）你可能不会立刻想到“星星”或“勇气”。
- en: What’s happening here is that we’re instinctively *predicting* the most likely
    words to follow “A cup of …”, with “coffee” being a much higher likelihood than
    “stars”.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生的事情是，我们本能地*预测*接下来的单词是最可能的那个，显然，“咖啡”比“星星”更有可能。
- en: This is similar to how LLMs work — they calculate the probabilities of possible
    next words and choose one based on those probabilities.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这类似于 LLMs（大语言模型）的工作方式——它们计算可能的下一个单词的概率，并根据这些概率选择一个单词。
- en: So on a high level, the hyperparameters are ways to tune *how we select the
    next probable words*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，超参数是调整*我们如何选择下一个可能单词*的方式。
- en: 'Let’s start with the most common hyperparameter:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从最常见的超参数开始：
- en: Temperature
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 温度
- en: Temperature controls the randomness of the models’ output. A lower temperature
    makes the output more deterministic, favoring more likely words, while a higher
    temperature allows for more creativity by considering less likely words.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 温度控制模型输出的随机性。较低的温度使输出更加确定性，偏向选择更有可能的单词，而较高的温度则允许通过考虑不太可能的单词来产生更多的创造性。
- en: In our “A cup of…” example, setting the temperature to 0 makes the model favor
    the most likely word, which is “coffee”.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的“杯中的…”示例中，将温度设置为 0 会使模型倾向于选择最可能的单词，即“咖啡”。
- en: '![](../Images/869825d95b5bfdc9c79b14e9d1b13356.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/869825d95b5bfdc9c79b14e9d1b13356.png)'
- en: Image provided by the author
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: As temperature increases, the sampling probabilities between different words
    start to even out, prompting the model to generate highly unusal or unexpected
    outputs.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 随着温度的升高，不同单词之间的采样概率开始趋于平衡，促使模型生成高度不寻常或意外的输出。
- en: Note, that setting the temperature to 0 still doesn’t make the model completely
    deterministic, though it gets very close.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，将温度设置为 0 并不能使模型完全确定性，尽管它非常接近。
- en: '**Use cases**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用场景**'
- en: 'Low temperature (e.g. 0.2): Ideal for tasks requiring precise and predictable
    results, such as technical writing or formal documentation.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低温度（例如，0.2）：适用于需要精确和可预测结果的任务，例如技术写作或正式文档编写。
- en: 'High temperature (e.g. 0.8 or above): Useful for creative tasks like storytelling,
    poetry, or brainstorming'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高温度（例如，0.8 或以上）：适用于创意任务，如讲故事、诗歌创作或头脑风暴。
- en: Max Tokens
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大令牌数
- en: Max tokens define the maximum number of tokens (which can be words or parts
    of words) the model can generate in its responses. Tokens are the smallest units
    of text that a model processes.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最大令牌数定义了模型在响应中可以生成的最大令牌数（令牌可以是单词或单词的一部分）。令牌是模型处理的最小文本单位。
- en: '**Relationship between tokens and words:**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**令牌与单词之间的关系：**'
- en: '1 word = 1~2 tokens: In English, a typical word is usually split into 1 to
    2 tokens. For example, simple words like “cat” might be a single token, while
    more complex words like “unbelievable” might be split into multiple tokens.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 个单词 = 1~2 个令牌：在英语中，典型的单词通常会被拆分为 1 到 2 个令牌。例如，“cat”这样的简单单词可能是一个令牌，而“unbelievable”这样的复杂单词可能会被拆分成多个令牌。
- en: 'The general rule of thumb: You can roughly estimate the number of words by
    dividing tokens by 1.5 (as a rough average).'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一般经验法则：通过将令牌数除以 1.5（大致平均值），可以粗略估计单词数。
- en: '**Use cases**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用场景**'
- en: 'Low max tokens (e.g. 50): Ideal for tasks requiring brief responses, such as
    headlines, short summaries, or concise answers. (Be careful that the model might
    cut off the output response)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低最大令牌数（例如，50）：适用于需要简短响应的任务，例如标题、简短总结或简洁的回答。（注意，模型可能会截断输出响应）
- en: 'High max tokens (e.g. 500): Useful for generating longer content like articles,
    stories, or detailed explanations.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高最大令牌数（例如，500）：适用于生成较长的内容，如文章、故事或详细解释。
- en: Top-k
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Top-k
- en: Top-k sampling restricts the model from selecting from the top *k* most likely
    next words. By narrowing the choices, it helps reduce the chances of generating
    irrelevant or nonsensical outputs.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Top-k 采样限制模型从最可能的*k*个下一个单词中进行选择。通过缩小选择范围，它有助于减少生成无关或无意义输出的几率。
- en: In the diagram below, if we set *k* to 2, the model will only consider the two
    most likely next words — in this case, ‘coffee’ and ‘courage.’ These two words
    are then resampled, with their probabilities adjusted to sum to 1, ensuring one
    of them is chosen.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，如果我们将*k*设置为 2，模型只会考虑两个最可能的下一个单词——在这种情况下，“咖啡”和“勇气”。然后，这两个单词会被重新抽样，调整它们的概率使其总和为
    1，从而确保选择其中一个。
- en: '![](../Images/72bad6505f466125adcde5e07054ea70.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/72bad6505f466125adcde5e07054ea70.png)'
- en: Image provided by the author.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: '**Use cases:**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用场景：**'
- en: 'Low k (e.g., k=10): Best for structured tasks where you want to maintain focus
    and coherence, such as summarization or coding.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低k（例如，k=10）：最适合结构化任务，在这些任务中你希望保持焦点和连贯性，例如总结或编码。
- en: 'High k (e.g., k=50): Suitable for creative or exploratory tasks where you want
    to introduce more variability without losing coherence.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高k（例如，k=50）：适用于创造性或探索性任务，在这些任务中你希望引入更多变化而不失去连贯性。
- en: Top-p
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Top-p
- en: Top-p sampling selects the smallest set of words whose combined probability
    exceeds a threshold *p* (e.g., 0.9), allowing for a more context-sensitive choice
    of words.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Top-p 采样选择那些其联合概率超过阈值*p*（例如，0.9）的最小单词集，从而允许根据上下文进行更敏感的词语选择。
- en: In the diagram below, we start with the most probable word, ‘coffee,’ which
    has a probability of 0.6\. Since this is less than our threshold of *p* = 0.9,
    we add the next word, ‘courage,’ with a probability of 0.2\. Together, these give
    us a total probability of 0.8, which is still below 0.9\. Finally, we consider
    the word ‘dreams’ with a probability of 0.13, bringing the total to 0.93, which
    exceeds 0.9\. At this point, we stop, having selected the first two most probable
    words.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图表中，我们从最可能的单词“coffee”开始，它的概率是0.6。由于这个概率低于我们的阈值*p* = 0.9，我们加入下一个单词“courage”，它的概率是0.2。加起来，这两个词的总概率是0.8，仍然低于0.9。最后，我们考虑单词“dreams”，它的概率是0.13，总概率达到0.93，超过了0.9。此时，我们停止选择，选择了前两个最可能的单词。
- en: '![](../Images/9444be711d7c56d878173c9fa83df048.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9444be711d7c56d878173c9fa83df048.png)'
- en: Image provided by the author.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: '**Use cases:**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**用例：**'
- en: 'Low p (e.g., p=0.5): Effective for tasks that require concise and to-the-point
    outputs, like news headlines or instructional text.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低p值（例如，p=0.5）：适用于需要简洁且直截了当的输出的任务，如新闻标题或说明性文本。
- en: 'High p (e.g., p=0.95): Useful for more open-ended tasks, such as dialogue generation
    or creative content, where a wider variety of responses is desirable.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高p值（例如，p=0.95）：适用于更开放的任务，如对话生成或创意内容生成，这类任务需要更多样化的回应。
- en: Frequency Penalty
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 频率惩罚
- en: A frequency penalty reduces the likelihood of the model repeating the same word
    within the text, promoting diversity and minimizing redundancy in the output.
    By applying this penalty, the model is encouraged to introduce new words instead
    of reusing ones that have already appeared.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 频率惩罚减少了模型在文本中重复相同单词的可能性，促进了多样性并减少了输出中的冗余。通过应用此惩罚，模型被鼓励使用新单词，而不是重复已经出现过的单词。
- en: 'The frequency penalty is calculated using the formula:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 频率惩罚是通过以下公式计算的：
- en: '**Adjusted probability = initial probability / (1 + frequency penalty * count
    of appearance)**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**调整后的概率 = 初始概率 / (1 + 频率惩罚 * 出现次数)**'
- en: 'For example, let’s say that the word “sun” has a probability of 0.5, and it
    has already appeared twice in the text. If we set the frequency penalty to 1,
    the adjusted probability for “sun” would be:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设“sun”这个词的概率为0.5，并且它在文本中已经出现了两次。如果我们将频率惩罚设置为1，那么“sun”的调整概率将是：
- en: Adjusted probability = 0.5 / (1 + 1 * 2) = 0.5 / 3 = 0.16
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 调整后的概率 = 0.5 / (1 + 1 * 2) = 0.5 / 3 = 0.16
- en: '![](../Images/416077203419aadf31c7749d6fa2f540.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/416077203419aadf31c7749d6fa2f540.png)'
- en: Image provided by the author.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: '**Use cases:**'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**用例：**'
- en: 'High Penalty (e.g., 1.0): Ideal for generating content where repetition would
    be distracting or undesirable, such as essays or research papers.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高惩罚（例如，1.0）：适合生成内容时，重复会分散注意力或不受欢迎的情况，例如写作论文或研究报告。
- en: 'Low Penalty (e.g., 0.0): Useful when repetition might be necessary or beneficial,
    such as in poetry, mantras, or certain marketing slogans.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低惩罚（例如，0.0）：当重复可能是必要或有益的情况时有用，例如在诗歌、咒语或某些营销口号中。
- en: Presence Penalty
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存在惩罚
- en: 'The presence penalty is similar to the frequency penalty but with one key difference:
    it penalizes the model for reusing any word or phrase that has already been mentioned,
    *regardless of how often it appears*.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 存在惩罚与频率惩罚类似，但有一个关键区别：它惩罚模型重复使用任何已经出现的单词或短语，*不管它出现了多少次*。
- en: In other words, repeating the word 2 times is as bad as repeating it 20 times.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，重复2次单词和重复20次单词的后果是一样的。
- en: 'The formula for adjusting the probability with a presence penalty is:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 调整概率的公式为：
- en: '**Adjusted probability = initial probability / (1 + presence penalty * presence)**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**调整后的概率 = 初始概率 / (1 + 存在惩罚 * 存在次数)**'
- en: Let’s revisit the earlier example with the word “sun”. Instead of multiplying
    the penalty by the frequency of how many times “sun” has appeared, we simply check
    whether it has appeared at all — in this case, it has, so we count it as 1.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾之前使用“sun”这个词的例子。我们不是将惩罚乘以“sun”出现的频率，而是简单地检查它是否已经出现过——在这种情况下，它已经出现过，因此我们将其计为1。
- en: 'If we set the presence penalty to 1, the adjusted probability would be:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将存在惩罚设置为1，那么调整后的概率将是：
- en: Adjusted probability = 0.5 / (1 + 1 * 1) = 0.5 / 2 = 0.25
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 调整后的概率 = 0.5 / (1 + 1 * 1) = 0.5 / 2 = 0.25
- en: This reduction makes it less likely for the model to choose “sun” again, encouraging
    the use of new words or phrases, even if “sun” has only appeared once in the text.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这种减少使得模型不太可能再次选择“sun”，即使“sun”只在文本中出现过一次，也会鼓励使用新的单词或短语。
- en: '![](../Images/ebaf6807ca9661e8deec2354ea0a7683.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ebaf6807ca9661e8deec2354ea0a7683.png)'
- en: Image provided by author.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: '**Use cases:**'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用场景：**'
- en: 'High Penalty (e.g., 1.0): Great for exploratory or brainstorming sessions where
    you want the model to keep introducing new ideas or topics.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高惩罚（例如，1.0）：非常适合探索性或头脑风暴会议，在这些会议中，你希望模型不断引入新思想或新话题。
- en: 'Low Penalty (e.g., 0.0): Suitable for tasks where reinforcement of key terms
    or ideas is important, such as technical documentation or instructional material.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低惩罚（例如，0.0）：适用于强调关键术语或思想的重要任务，如技术文档或教学材料。
- en: Frequency and Presence Penalties often go hand-in-hand
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 频率惩罚和出现惩罚通常是一起使用的
- en: Now that we’ve gone over the basics, let’s dive into how frequency and presence
    penalties are often used together. Just a heads-up, though — they’re powerful
    tools, but it’s important to use them with a bit of caution to get the best results.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了基础知识，接下来让我们深入探讨频率和出现惩罚是如何一起使用的。不过提前提醒一下——它们是强大的工具，但重要的是要小心使用，以获得最佳效果。
- en: '**When to use them:**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**何时使用它们：**'
- en: Content Generation
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内容生成
- en: Preventing Redundancy
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 防止冗余
- en: '**When to not use them**'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**何时不使用它们**'
- en: 'Technical Writing: In technical documentation or specific instructions where
    consistent terminology is crucial, using these penalties might be counterproductive.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 技术写作：在技术文档或特定指令中，如果一致的术语至关重要，使用这些惩罚可能适得其反。
- en: 'Brand messaging: If you’re generating content that relies heavily on a specific
    brand tone or key phrases, reducing repetition might dilute the brand’s voice.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 品牌信息传递：如果你正在生成的内容高度依赖于特定品牌的语气或关键词，减少重复可能会削弱品牌的声音。
- en: By now, you should have a clearer picture of how temperature, Top-k, Top-p,
    frequency, and presence penalties work together to shape the output of your language
    model. And if it still feels a bit tricky, that’s totally okay — these concepts
    can take some time to fully click. Just keep experimenting and exploring, and
    you’ll get the hang of it before you know it.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你应该对温度、Top-k、Top-p、频率和出现惩罚是如何协同作用，塑造语言模型的输出有了更清晰的理解。如果你仍然觉得有点复杂，没关系——这些概念需要一些时间才能完全理解。只要不断实验和探索，你会很快掌握的。
- en: If you find visual content like this helpful and want more, we’d love to see
    you in our [Discord community.](https://discord.gg/ETGYYsswTb) It’s a space where
    we share ideas, help each other out, and learn together.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你觉得像这样的视觉内容有帮助，并希望看到更多，我们很高兴在我们的[Discord社区](https://discord.gg/ETGYYsswTb)见到你。这里是一个我们分享想法、互相帮助并一起学习的空间。
