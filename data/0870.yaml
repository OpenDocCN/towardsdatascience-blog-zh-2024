- en: 'Deploying Large Language Models: vLLM and Quantization'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署大语言模型：vLLM与量化
- en: 原文：[https://towardsdatascience.com/deploying-large-language-models-vllm-and-quantizationstep-by-step-guide-on-how-to-accelerate-becfe17396a2?source=collection_archive---------1-----------------------#2024-04-05](https://towardsdatascience.com/deploying-large-language-models-vllm-and-quantizationstep-by-step-guide-on-how-to-accelerate-becfe17396a2?source=collection_archive---------1-----------------------#2024-04-05)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/deploying-large-language-models-vllm-and-quantizationstep-by-step-guide-on-how-to-accelerate-becfe17396a2?source=collection_archive---------1-----------------------#2024-04-05](https://towardsdatascience.com/deploying-large-language-models-vllm-and-quantizationstep-by-step-guide-on-how-to-accelerate-becfe17396a2?source=collection_archive---------1-----------------------#2024-04-05)
- en: Step-by-step guide on how to accelerate large language models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大语言模型加速的逐步指南
- en: '[](https://olafenwaayoola.medium.com/?source=post_page---byline--becfe17396a2--------------------------------)[![Ayoola
    Olafenwa](../Images/86914a74cbf83e711887fab896e318a9.png)](https://olafenwaayoola.medium.com/?source=post_page---byline--becfe17396a2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--becfe17396a2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--becfe17396a2--------------------------------)
    [Ayoola Olafenwa](https://olafenwaayoola.medium.com/?source=post_page---byline--becfe17396a2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://olafenwaayoola.medium.com/?source=post_page---byline--becfe17396a2--------------------------------)[![Ayoola
    Olafenwa](../Images/86914a74cbf83e711887fab896e318a9.png)](https://olafenwaayoola.medium.com/?source=post_page---byline--becfe17396a2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--becfe17396a2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--becfe17396a2--------------------------------)
    [Ayoola Olafenwa](https://olafenwaayoola.medium.com/?source=post_page---byline--becfe17396a2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--becfe17396a2--------------------------------)
    ·9 min read·Apr 5, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--becfe17396a2--------------------------------)
    ·9分钟阅读·2024年4月5日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/399881e6f19fe28227d9982dd335ab20.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/399881e6f19fe28227d9982dd335ab20.png)'
- en: '[source](https://unsplash.com/photos/a-computer-chip-with-the-letter-a-on-top-of-it-eGGFZ5X2LnA)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://unsplash.com/photos/a-computer-chip-with-the-letter-a-on-top-of-it-eGGFZ5X2LnA)'
- en: Deployment of Large Language Models (LLMs)
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大语言模型（LLMs）的部署
- en: We live in an amazing time of Large Language Models like ChatGPT, GPT-4, and
    Claude that can perform multiple amazing tasks. In practically every field, ranging
    from education, healthcare to arts and business, Large Language Models are being
    used to facilitate efficiency in delivering services. Over the past year, many
    brilliant open-source Large Language Models, such as Llama, Mistral, Falcon, and
    Gemma, have been released. These open-source LLMs are available for everyone to
    use, but deploying them can be very challenging as they can be very slow and require
    a lot of GPU compute power to run for real-time deployment. Different tools and
    approaches have been created to simplify the deployment of Large Language Models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生活在一个令人惊叹的时代，大语言模型（如ChatGPT、GPT-4和Claude）可以执行多个惊人的任务。在教育、医疗、艺术和商业等几乎所有领域，大语言模型都被用来提高服务的效率。在过去的一年中，许多杰出的开源大语言模型（如Llama、Mistral、Falcon和Gemma）相继发布。这些开源LLM可以供所有人使用，但部署它们可能非常具有挑战性，因为它们可能非常慢，并且需要大量的GPU计算能力来实现实时部署。为简化大语言模型的部署，已创建了不同的工具和方法。
- en: Many deployment tools have been created for serving LLMs with faster inference,
    such as vLLM, c2translate, TensorRT-LLM, and llama.cpp. Quantization techniques
    are also used to optimize GPUs for loading very large Language Models. In this
    article, I will explain how to deploy Large Language Models with vLLM and quantization.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供更快的推理速度，许多部署工具已被创建来服务大语言模型（LLMs），如vLLM、c2translate、TensorRT-LLM和llama.cpp。同时，量化技术也被用来优化GPU，以便加载非常大的语言模型。本文将解释如何通过vLLM和量化技术来部署大语言模型。
- en: '**Latency and Throughput**'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**延迟与吞吐量**'
- en: Some of the major factors that affect the speed performance of a Large Language
    Model are GPU hardware requirements and model size. The larger the size of the
    model, the more GPU compute power is required to run it. Common benchmark metrics
    used in measuring the speed performance of a Large Language Model are ***Latency***
    and ***Throughput***.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 影响大型语言模型速度性能的主要因素包括GPU硬件要求和模型大小。模型越大，运行所需的GPU计算能力就越强。常用的基准度量标准来衡量大型语言模型的速度性能包括***延迟***和***吞吐量***。
- en: '**Latency:** This is the time required for a Large Language Model to generate
    a response. It is usually measured in seconds or milliseconds.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**延迟：** 这是大型语言模型生成响应所需的时间，通常以秒或毫秒为单位进行测量。'
- en: '**Throughput:** This is the number of tokens generated per second or millisecond
    from a Large Language Model.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**吞吐量：** 这是指每秒或每毫秒由大型语言模型生成的token数量。'
- en: Install Required Packages
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装所需的包
- en: 'Below are the two required packages for running a Large Language Model: Hugging
    Face ***transformers*** and ***accelerate***.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是运行大型语言模型所需的两个包：Hugging Face ***transformers***和***accelerate***。
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: What is Phi-2?
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是Phi-2？
- en: '***Phi-2*** is a state-of-the-art foundation model from Microsoft with 2.7
    billion parameters. It was pre-trained with a variety of data sources, ranging
    from code to textbooks. Learn more about ***Phi-2*** from [here](https://huggingface.co/microsoft/phi-2).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '***Phi-2***是微软推出的一个最先进的基础模型，拥有27亿个参数。它通过多种数据源进行预训练，包括代码和教科书等内容。了解更多关于***Phi-2***的信息，点击[这里](https://huggingface.co/microsoft/phi-2)。'
- en: Benchmarking LLM Latency and Throughput with Hugging Face Transformers
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Hugging Face Transformers基准测试LLM的延迟和吞吐量
- en: Generated Output
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成的输出
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Step By Step Code Breakdown**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**逐步代码解析**'
- en: '**Line 6–10:** Loaded ***Phi-2*** model and tokenized the prompt “***Generate
    a python code that accepts a list of numbers and returns the sum.***”'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**第6–10行：** 加载了***Phi-2***模型并对提示“***生成一个接受数字列表并返回其和的Python代码***”进行了分词。'
- en: '**Line 12- 18:** Generated a response from the model and obtained the ***latency***
    by calculating the time required to generate the response.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**第12-18行：** 从模型生成了一个响应，并通过计算生成响应所需的时间获得了***延迟***。'
- en: '**Line 21–23:** Obtained the total length of tokens in the response generated,
    divided it by the ***latency*** and calculated the ***throughput***.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**第21-23行：** 获取了生成响应的token总长度，将其除以***延迟***并计算出***吞吐量***。'
- en: This model was run on an A1000 (16GB GPU), and it achieves a ***latency*** of
    ***2.7 seconds*** and a throughput of ***32 tokens/second.***
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在A1000（16GB GPU）上运行，达到了***延迟***为***2.7秒***，吞吐量为***32 tokens/秒***。
- en: Deployment of A Large Language Model with vLLM
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用vLLM部署大型语言模型
- en: vLLM is an open source LLM library for serving Large Language Models at low
    ***latency*** and high ***throughput***.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: vLLM是一个开源的LLM库，用于以低***延迟***和高***吞吐量***提供大型语言模型服务。
- en: How vLLM works
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: vLLM的工作原理
- en: The transformer is the building block of Large Language Models. The transformer
    network uses a mechanism called the ***attention mechanism***, which is used by
    the network to study and understand the context of words. The ***attention mechanism***
    is made up of a bunch of mathematical calculations of matrices known as attention
    keys and values. The memory used by the interaction of these attention keys and
    values affects the speed of the model. vLLM introduced a new attention mechanism
    called ***PagedAttention*** that efficiently manages the allocation of memory
    for the transformer’s attention keys and values during the generation of tokens.
    The memory efficiency of vLLM has proven very useful in running Large Language
    Models at low latency and high throughput.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer是大型语言模型的构建模块。Transformer网络使用一种叫做***注意力机制***的机制，网络通过它来研究和理解单词的上下文。***注意力机制***由一系列数学计算矩阵组成，这些矩阵被称为注意力键和值。注意力键和值的交互所使用的内存影响着模型的速度。vLLM引入了一种新的注意力机制——***分页注意力(PagedAttention)***，它在生成token的过程中高效地管理了Transformer的注意力键和值的内存分配。vLLM的内存效率在低延迟和高吞吐量下运行大型语言模型时证明非常有用。
- en: This is a high-level explanation of how vLLM works. To learn more in-depth technical
    details, visit the vLLM documentation.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这是vLLM工作原理的高层次解释。要了解更多深入的技术细节，请访问vLLM文档。
- en: '[](https://blog.vllm.ai/2023/06/20/vllm.html?source=post_page-----becfe17396a2--------------------------------)
    [## vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://blog.vllm.ai/2023/06/20/vllm.html?source=post_page-----becfe17396a2--------------------------------)
    [## vLLM：使用分页注意力机制，轻松、快速且廉价地提供大型语言模型服务'
- en: GitHub | Documentation | Paper
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GitHub | 文档 | 论文
- en: blog.vllm.ai](https://blog.vllm.ai/2023/06/20/vllm.html?source=post_page-----becfe17396a2--------------------------------)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[blog.vllm.ai](https://blog.vllm.ai/2023/06/20/vllm.html?source=post_page-----becfe17396a2--------------------------------)'
- en: '**Install vLLM**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**安装vLLM**'
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Run Phi-2 with vLLM**'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**运行Phi-2与vLLM**'
- en: Generated Output
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成输出
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Step By Step Code Breakdown**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**逐步代码解析**'
- en: '**Line 1–3:** Imported required packages from vLLM for running ***Phi-2***.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**第1–3行：** 从vLLM导入了运行***Phi-2***所需的包。'
- en: '**Line 5–8:** Loaded ***Phi-2*** with vLLM, defined the prompt and set important
    parameters for running the model.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**第5–8行：** 使用vLLM加载了***Phi-2***，定义了提示词并设置了运行模型的重要参数。'
- en: '**Line 10–16:** Generated the model’s response using ***llm.generate*** and
    computed the ***latency***.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**第10–16行：** 使用***llm.generate***生成模型的响应，并计算***延迟***。'
- en: '**Line 19–21:** Obtained the length of total tokens generated from the response,
    divided the length of tokens by the latency to get the ***throughput***.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**第19–21行：** 获取从响应中生成的总token长度，将token的长度除以延迟以获得***吞吐量***。'
- en: '**Line 23–24:** Obtained the generated text.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**第23–24行：** 获取生成的文本。'
- en: I ran ***Phi-2*** with vLLM on the same prompt, ***“Generate a python code that
    accepts a list of numbers and returns the sum.”*** On the same GPU, an A1000 (16GB
    GPU), vLLM produces a ***latency*** of ***1.2 seconds*** and a ***throughput***
    of ***63 tokens/second***, compared to Hugging Face transformers’ ***latency***
    of ***2.85 seconds*** and a ***throughput*** of ***32 tokens/second.*** Running
    a Large Language Model with vLLM produces the same accurate result as using Hugging
    Face, with much lower latency and higher throughput.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我在同一个提示词下使用vLLM运行了***Phi-2***，***“生成一个接受数字列表并返回其和的Python代码。”*** 在相同的GPU（A1000，16GB
    GPU）上，vLLM的***延迟***为***1.2秒***，***吞吐量***为***63个token/秒***，而Hugging Face transformers的***延迟***为***2.85秒***，***吞吐量***为***32个token/秒***。使用vLLM运行大语言模型与使用Hugging
    Face得出的结果相同，但具有更低的延迟和更高的吞吐量。
- en: '**Note:** The metrics (latency and throughput) I obtained for vLLM are estimated
    benchmarks for vLLM performance. The model generation speed depends on many factors,
    such as the length of the input prompt and the size of the GPU. According to the
    official vLLM report, running an LLM model on a powerful GPU like the A100 in
    a production setting with vLLM achieves **24x higher throughput** than Hugging
    Face Transformers.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 我为vLLM获得的度量指标（延迟和吞吐量）是vLLM性能的估算基准。模型生成速度受多种因素影响，例如输入提示词的长度和GPU的大小。根据官方vLLM报告，在生产环境中使用像A100这样强大的GPU运行LLM模型，vLLM的**吞吐量比Hugging
    Face Transformers高出24倍**。'
- en: Benchmarking Latency and Throughput in Real Time
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实时延迟和吞吐量基准测试
- en: The way I calculated the latency and throughput for running Phi-2 is experimental,
    and I did this to explain how vLLM accelerates a Large Language Model’s performance.
    In the real-world use case of LLMs, such as a chat-based system where the model
    outputs a token as it is generated, measuring the latency and throughput is more
    complex.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我计算Phi-2的延迟和吞吐量的方式是实验性的，我这样做是为了说明vLLM如何加速大语言模型的性能。在LLM的实际使用案例中，例如一个基于聊天的系统，模型在生成时会输出token，测量延迟和吞吐量要复杂得多。
- en: A chat-based system is based on streaming output tokens. Some of the major factors
    that affect the LLM metrics are ***Time to First Token*** (the time required for
    a model to generate the first token), ***Time Per Output Token*** (the time spent
    per output token generated), ***the input sequence length, the expected output,
    the total expected output tokens***, and ***the model size***. In a chat-based
    system, the latency is usually a combination of ***Time to First Token*** and
    ***Time Per Output Token*** multiplied by the total expected output tokens.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 基于聊天的系统依赖于流式输出token。一些主要的因素影响LLM的度量标准，如***首次token生成时间***（模型生成第一个token所需的时间）、***每个输出token的时间***（生成每个输出token所花费的时间）、***输入序列长度、预期输出、预期的总输出token数***以及***模型大小***。在基于聊天的系统中，延迟通常是***首次token生成时间***与***每个输出token的时间***乘以预期的总输出token数的组合。
- en: The longer the input sequence length passed into a model, the slower the response.
    Some of the approaches used in running LLMs in real-time involve batching users’
    input requests or prompts to perform inference on the requests concurrently, which
    helps in improving the throughput. Generally, using a powerful GPU and serving
    LLMs with efficient tools like vLLM improves both the latency and throughput in
    real-time.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 输入序列长度越长，模型的响应速度越慢。一些实时运行 LLM 的方法包括将用户的输入请求或提示批处理，从而并发执行推理，这有助于提高吞吐量。一般来说，使用强大的
    GPU 和高效的工具（如 vLLM）来提供 LLM 服务，可以提高实时的延迟和吞吐量。
- en: '**Run the vLLM deployment on Google Colab**'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**在 Google Colab 上运行 vLLM 部署**'
- en: '[](https://colab.research.google.com/drive/171tVs8nndleyYHoFr1PVm6YRvYg6kBCx?usp=sharing&source=post_page-----becfe17396a2--------------------------------)
    [## Google Colaboratory'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://colab.research.google.com/drive/171tVs8nndleyYHoFr1PVm6YRvYg6kBCx?usp=sharing&source=post_page-----becfe17396a2--------------------------------)
    [## Google Colaboratory'
- en: Edit description
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编辑描述
- en: colab.research.google.com](https://colab.research.google.com/drive/171tVs8nndleyYHoFr1PVm6YRvYg6kBCx?usp=sharing&source=post_page-----becfe17396a2--------------------------------)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[colab.research.google.com](https://colab.research.google.com/drive/171tVs8nndleyYHoFr1PVm6YRvYg6kBCx?usp=sharing&source=post_page-----becfe17396a2--------------------------------)'
- en: Quantization of Large Language Models
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型的量化
- en: Quantization is the conversion of a machine learning model from a higher precision
    to a lower precision by shrinking the model’s weights into smaller bits, usually
    ***8-bit*** or ***4-bit***. Deployment tools like vLLM are very useful for inference
    serving of Large Language Models at very low latency and high throughput. We are
    able to run ***Phi-2*** with Hugging Face and vLLM conveniently on the T4 GPU
    on Google Colab because it is a smaller LLM with ***2.7 billion parameters***.
    For example, a 7-billion-parameter model like ***Mistral 7B*** cannot be run on
    Colab with either Hugging Face or vLLM. Quantization is best for managing GPU
    hardware requirements for Large Language Models. When GPU availability is limited
    and we need to run a very large Language Model, quantization is the best approach
    to load LLMs on constrained devices.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是将机器学习模型从高精度转换为低精度的过程，通过将模型的权重压缩成更小的位数，通常是 ***8-bit*** 或 ***4-bit***。像 vLLM
    这样的部署工具对于在低延迟和高吞吐量下提供大型语言模型推理服务非常有用。由于 ***Phi-2*** 是一个参数较小的 LLM，具有 ***27 亿参数***，因此我们能够方便地在
    Google Colab 的 T4 GPU 上使用 Hugging Face 和 vLLM 运行它。然而，像 ***Mistral 7B*** 这样的 70
    亿参数的模型无法在 Colab 上通过 Hugging Face 或 vLLM 运行。量化最适合管理大型语言模型的 GPU 硬件需求。当 GPU 资源有限且需要运行非常大的语言模型时，量化是将
    LLM 加载到受限设备上的最佳方法。
- en: BitsandBytes
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BitsandBytes
- en: It is a python library built with custom quantization functions for shrinking
    model’s weights into lower bits(***8-bit*** and ***4-bit***).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个 Python 库，内置了自定义量化函数，用于将模型的权重缩小为更低的位数（***8-bit*** 和 ***4-bit***）。
- en: '**Install BitsandBytes**'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**安装 BitsandBytes**'
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Quantization of Mistral 7B Model
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '***Mistral 7B*** 模型的量化'
- en: '***Mistral 7B***, a 7-billion-parameter model from MistralAI, is one of the
    best state-of-the-art open-source Large Language Models. I will go through a step-by-step
    process of running ***Mistral 7B*** with different quantization techniques that
    can be run on the T4 GPU on Google Colab.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '***Mistral 7B*** 是 MistralAI 发布的一个拥有 70 亿参数的模型，是最先进的开源大型语言模型之一。我将逐步讲解如何使用不同的量化技术在
    Google Colab 上的 T4 GPU 上运行 ***Mistral 7B***。'
- en: '**Quantization with 8bit Precision**: This is the conversion of a machine learning
    model’s weight into 8-bit precision. ***BitsandBytes*** has been integrated with
    Hugging Face transformers to load a language model using the same Hugging Face
    code, but with minor modifications for quantization.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用 8-bit 精度进行量化**：这是将机器学习模型的权重转换为 8-bit 精度的过程。***BitsandBytes*** 已与 Hugging
    Face transformers 集成，以使用相同的 Hugging Face 代码加载语言模型，但进行了少量修改以实现量化。'
- en: '**Line 1:** Imported the needed packages for running model, including the ***BitsandBytesConfig***
    library.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 1 行：** 导入了运行模型所需的包，包括 ***BitsandBytesConfig*** 库。'
- en: '**Line 3–4:** Defined the quantization config and set the parameter ***load_in_8bit***
    to true for loading the model’s weights in ***8-bit*** precision.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 3-4 行：** 定义了量化配置，并将参数 ***load_in_8bit*** 设置为 true，以便以 ***8-bit*** 精度加载模型的权重。'
- en: '**Line 7–9:** Passed the quantization config into the function for loading
    the model, set the parameter ***device_map*** for ***bitsandbytes*** to automatically
    allocate appropriate GPU memory for loading the model. Finally loaded the tokenizer
    weights.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**第7–9行：** 将量化配置传入加载模型的函数，设置参数***device_map***为***bitsandbytes***，以自动分配适当的GPU内存来加载模型。最后加载了分词器权重。'
- en: '**Quantization with 4bit Precision**: This is the conversion of a machine learning
    model’s weight into ***4-bi***t precision.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**4位精度量化**：这是将机器学习模型的权重转换为***4位***精度。'
- en: 'The code for loading ***Mistral 7B*** in 4-bit precision is similar to that
    of ***8-bit*** precision except for a few changes:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以4位精度加载***Mistral 7B***的代码与***8位***精度的代码类似，除了少数几处更改：
- en: changed ***load_in_8bit*** to ***load_in_4bit***.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将***load_in_8bit***改为***load_in_4bit***。
- en: A new parameter ***bnb_4bit_compute_dtype*** is introduced into the ***BitsandBytesConfig***
    to perform the model’s computation in ***bfloat16***. ***bfloat16*** is computation
    data type for loading model’s weights for faster inference. It can be used with
    both **4-bit** and ***8-bit*** precisions***.*** If it is in ***8-bit*** you just
    need to change the parameter from ***bnb_4bit_compute_dtype*** to ***bnb_8bit_compute_dtype.***
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新增了一个参数***bnb_4bit_compute_dtype***，它被引入到***BitsandBytesConfig***中，用于以***bfloat16***执行模型的计算。***bfloat16***是用于加载模型权重以加速推理的计算数据类型。它可以同时与**4位**和***8位***精度一起使用。如果是***8位***精度，您只需将参数从***bnb_4bit_compute_dtype***更改为***bnb_8bit_compute_dtype***。
- en: NF4(4-bit Normal Float) and **Double Quantization**
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NF4（4位标准浮动）和**双重量化**
- en: '**NF4 (4-bit Normal Float)** from QLoRA is an optimal quantization approach
    that yields better results than the standard 4-bit quantization. It is integrated
    with double quantization, where quantization occurs twice; quantized weights from
    the first stage of quantization are passed into the next stage of quantization,
    yielding optimal float range values for the model’s weights. According to the
    report from the QLoRA paper, ***NF4 with double quantization*** does not suffer
    from a drop in accuracy performance. Read more in-depth technical details about
    NF4 and Double Quantization from the QLoRA paper:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 来自QLoRA的**NF4（4位标准浮动）**是一种最优的量化方法，比标准的4位量化产生更好的结果。它与双重量化相结合，量化过程发生两次；第一次量化的量化权重传递到下一阶段的量化，产生模型权重的最佳浮动范围值。根据QLoRA论文的报告，***NF4与双重量化***不会导致准确率下降。阅读更多关于NF4和双重量化的深入技术细节，请参考QLoRA论文：
- en: '[](https://arxiv.org/abs/2305.14314?source=post_page-----becfe17396a2--------------------------------)
    [## QLoRA: Efficient Finetuning of Quantized LLMs'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://arxiv.org/abs/2305.14314?source=post_page-----becfe17396a2--------------------------------)
    [## QLoRA：高效的量化LLM微调'
- en: We present QLoRA, an efficient finetuning approach that reduces memory usage
    enough to finetune a 65B parameter model…
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们提出了QLoRA，一种高效的微调方法，能够显著减少内存使用，从而能够微调一个65B参数的模型……
- en: arxiv.org](https://arxiv.org/abs/2305.14314?source=post_page-----becfe17396a2--------------------------------)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[arxiv.org](https://arxiv.org/abs/2305.14314?source=post_page-----becfe17396a2--------------------------------)'
- en: '**Line 4–9:** Extra parameters were set the ***BitsandBytesConfig:***'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**第4–9行：** 设置了额外的参数，***BitsandBytesConfig：***'
- en: '***load_4bit:*** loading model in 4-bit precision is set to true.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***load_4bit：*** 以4位精度加载模型的设置为真。'
- en: '***bnb_4bit_quant_type:*** The type of quantization is set to nf4.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***bnb_4bit_quant_type：*** 量化类型设置为nf4。'
- en: '***bnb_4bit_use_double_quant:*** Double quantization is set to True.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***bnb_4bit_use_double_quant：*** 双重量化设置为True。'
- en: '***bnb_4_bit_compute_dtype:*** ***bfloat16*** computation data type is used
    for faster inference.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***bnb_4_bit_compute_dtype：*** 使用***bfloat16***计算数据类型，以加速推理。'
- en: '**Line 11–13:** Loaded the model’s weights and tokenizer.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**第11–13行：** 加载了模型的权重和分词器。'
- en: '**Full Code for Model Quantization**'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型量化的完整代码**'
- en: Generated Output
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成的输出
- en: '[PRE5]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Quantization is a very good approach for optimizing the running of very Large
    Language Models on smaller GPUs and can be applied to any model, such as Llama
    70B, Falcon 40B, and mpt-30b. According to reports from the [LLM.int8 paper](https://arxiv.org/abs/2208.07339),
    very Large Language Models suffer less from accuracy drops when quantized compared
    to smaller ones. Quantization is best applied to very Large Language Models and
    does not work well for smaller models because of the loss in accuracy performance.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是一种非常有效的方法，用于在小型GPU上优化大型语言模型的运行，并且可以应用于任何模型，如Llama 70B、Falcon 40B和mpt-30b。根据[LLM.int8论文](https://arxiv.org/abs/2208.07339)的报告，量化后，超大型语言模型在准确性下降方面的影响较小，远低于小型模型。量化最适用于超大型语言模型，对于小型模型而言，由于准确性的下降，效果并不好。
- en: '**Run Mixtral 7B Quantization on Google Colab**'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**在Google Colab上运行Mixtral 7B量化**'
- en: '[](https://colab.research.google.com/drive/1aYPlWaHC4iy6DLFjR291iEMxFvgeM1ia?usp=sharing&source=post_page-----becfe17396a2--------------------------------)
    [## Google Colaboratory'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://colab.research.google.com/drive/1aYPlWaHC4iy6DLFjR291iEMxFvgeM1ia?usp=sharing&source=post_page-----becfe17396a2--------------------------------)
    [## Google Colaboratory'
- en: Edit description
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编辑描述
- en: colab.research.google.com](https://colab.research.google.com/drive/1aYPlWaHC4iy6DLFjR291iEMxFvgeM1ia?usp=sharing&source=post_page-----becfe17396a2--------------------------------)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[colab.research.google.com](https://colab.research.google.com/drive/1aYPlWaHC4iy6DLFjR291iEMxFvgeM1ia?usp=sharing&source=post_page-----becfe17396a2--------------------------------)'
- en: Conclusion
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, I provided a step-by-step approach to measuring the speed performance
    of a Large Language Model, explained how vLLM works, and how it can be used to
    improve the latency and throughput of a Large Language Model. Finally, I explained
    quantization and how it is used to load Large Language Models on small-scale GPUs.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我提供了一个逐步的方法来衡量大型语言模型的速度性能，解释了vLLM是如何工作的，以及它如何被用来提升大型语言模型的延迟和吞吐量。最后，我解释了量化技术及其如何被用来在小规模GPU上加载大型语言模型。
- en: '**Reach to me via:**'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**通过以下方式联系我：**'
- en: 'Email: [olafenwaayoola@gmail.com](https://mail.google.com/mail/u/0/#inbox)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 邮箱：[olafenwaayoola@gmail.com](https://mail.google.com/mail/u/0/#inbox)
- en: 'Linkedin: [https://www.linkedin.com/in/ayoola-olafenwa-003b901a9/](https://www.linkedin.com/in/ayoola-olafenwa-003b901a9/)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 'LinkedIn: [https://www.linkedin.com/in/ayoola-olafenwa-003b901a9/](https://www.linkedin.com/in/ayoola-olafenwa-003b901a9/)'
- en: '**References**'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: '[](https://blog.vllm.ai/2023/06/20/vllm.html?source=post_page-----becfe17396a2--------------------------------)
    [## vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://blog.vllm.ai/2023/06/20/vllm.html?source=post_page-----becfe17396a2--------------------------------)
    [## vLLM：使用PagedAttention轻松、快速、低成本地提供LLM服务'
- en: GitHub | Documentation | Paper
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GitHub | 文档 | 论文
- en: blog.vllm.ai](https://blog.vllm.ai/2023/06/20/vllm.html?source=post_page-----becfe17396a2--------------------------------)
    [](https://huggingface.co/blog/4bit-transformers-bitsandbytes?source=post_page-----becfe17396a2--------------------------------)
    [## Making LLMs even more accessible with bitsandbytes, 4-bit quantization and
    QLoRA
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[blog.vllm.ai](https://blog.vllm.ai/2023/06/20/vllm.html?source=post_page-----becfe17396a2--------------------------------)
    [](https://huggingface.co/blog/4bit-transformers-bitsandbytes?source=post_page-----becfe17396a2--------------------------------)
    [## 使用bitsandbytes、4位量化和QLoRA使LLM更加易用'
- en: We're on a journey to advance and democratize artificial intelligence through
    open source and open science.
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们正在通过开源和开放科学的方式推进并普及人工智能。
- en: huggingface.co](https://huggingface.co/blog/4bit-transformers-bitsandbytes?source=post_page-----becfe17396a2--------------------------------)
    [](https://www.baseten.co/blog/understanding-performance-benchmarks-for-llm-inference/?source=post_page-----becfe17396a2--------------------------------)
    [## Understanding performance benchmarks for LLM inference
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[huggingface.co](https://huggingface.co/blog/4bit-transformers-bitsandbytes?source=post_page-----becfe17396a2--------------------------------)
    [](https://www.baseten.co/blog/understanding-performance-benchmarks-for-llm-inference/?source=post_page-----becfe17396a2--------------------------------)
    [## 理解LLM推理性能基准'
- en: This guide helps you interpret LLM performance metrics to make direct comparisons
    on latency, throughput, and cost.
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本指南帮助你解读LLM性能指标，直接对比延迟、吞吐量和成本。
- en: www.baseten.co](https://www.baseten.co/blog/understanding-performance-benchmarks-for-llm-inference/?source=post_page-----becfe17396a2--------------------------------)
    [](https://www.tensorops.ai/post/what-are-quantized-llms?source=post_page-----becfe17396a2--------------------------------)
    [## What are Quantized LLMs?
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[www.baseten.co](https://www.baseten.co/blog/understanding-performance-benchmarks-for-llm-inference/?source=post_page-----becfe17396a2--------------------------------)
    [](https://www.tensorops.ai/post/what-are-quantized-llms?source=post_page-----becfe17396a2--------------------------------)
    [## 什么是量化的LLM？'
- en: Discover the power of quantized LLMs! Learn how model quantization reduces size,
    enables efficient hardware usage, and…
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 发现量化LLMs的强大功能！了解模型量化如何减少模型大小，提升硬件使用效率，以及…
- en: www.tensorops.ai](https://www.tensorops.ai/post/what-are-quantized-llms?source=post_page-----becfe17396a2--------------------------------)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[www.tensorops.ai](https://www.tensorops.ai/post/what-are-quantized-llms?source=post_page-----becfe17396a2--------------------------------)'
