- en: 'Structure and Relationships: Graph Neural Networks and a Pytorch Implementation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构与关系：图神经网络及其在Pytorch中的实现
- en: 原文：[https://towardsdatascience.com/structure-and-relationships-graph-neural-networks-and-a-pytorch-implementation-c9d83b71c041?source=collection_archive---------1-----------------------#2024-03-05](https://towardsdatascience.com/structure-and-relationships-graph-neural-networks-and-a-pytorch-implementation-c9d83b71c041?source=collection_archive---------1-----------------------#2024-03-05)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/structure-and-relationships-graph-neural-networks-and-a-pytorch-implementation-c9d83b71c041?source=collection_archive---------1-----------------------#2024-03-05](https://towardsdatascience.com/structure-and-relationships-graph-neural-networks-and-a-pytorch-implementation-c9d83b71c041?source=collection_archive---------1-----------------------#2024-03-05)
- en: Understanding the mathematical background of graph neural networks and implementation
    for a regression problem in pytorch
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解图神经网络的数学背景及其在pytorch中回归问题的实现
- en: '[](https://medium.com/@ns650?source=post_page---byline--c9d83b71c041--------------------------------)[![Najib
    Sharifi, Ph.D.](../Images/d94932c5e3633e32247d98a3c221b181.png)](https://medium.com/@ns650?source=post_page---byline--c9d83b71c041--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c9d83b71c041--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c9d83b71c041--------------------------------)
    [Najib Sharifi, Ph.D.](https://medium.com/@ns650?source=post_page---byline--c9d83b71c041--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ns650?source=post_page---byline--c9d83b71c041--------------------------------)[![Najib
    Sharifi博士](../Images/d94932c5e3633e32247d98a3c221b181.png)](https://medium.com/@ns650?source=post_page---byline--c9d83b71c041--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c9d83b71c041--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c9d83b71c041--------------------------------)
    [Najib Sharifi博士](https://medium.com/@ns650?source=post_page---byline--c9d83b71c041--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c9d83b71c041--------------------------------)
    ·12 min read·Mar 5, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c9d83b71c041--------------------------------)
    ·12分钟阅读·2024年3月5日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '**Introduction**'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**简介**'
- en: Interconnected graphical data is all around us, ranging from molecular structures
    to social networks and design structures of cities. Graph Neural Networks (GNNs)
    are emerging as a powerful method of modelling and learning the spatial and graphical
    structure of such data. It has been applied to protein structures and other molecular
    applications such as drug discovery as well as modelling systems such as social
    networks. Recently the standard GNN has been combined with ideas from other ML
    models to develop exciting innovative applications. One such development is the
    integration of GNN with sequential models — Spatio-Temporal GNN that is able to
    capture both the temporal and spatial (hence the name) dependences of data, this
    alone could be applied to a number of challenges/problems in industry/research.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 相互连接的图形数据无处不在，从分子结构到社交网络以及城市的设计结构。图神经网络（GNN）正逐渐成为一种强大的方法，用于建模和学习此类数据的空间和图形结构。它已被应用于蛋白质结构及其他分子应用，如药物发现，并且还被用于建模如社交网络等系统。最近，标准的GNN与其他机器学习模型的思想相结合，开发出了一些令人兴奋的创新应用。其中一项发展是将GNN与序列模型结合
    —— 空间-时间图神经网络（Spatio-Temporal GNN），它能够捕捉数据的时间和空间（因此得名）依赖性，仅此一点就可以应用于行业/研究中的许多挑战/问题。
- en: Despite the exciting developments in GNN, there are very few resources on the
    topic which makes it inaccessible to many. In this short article, I want to provide
    a brief introduction to GNN covering both the mathematical description as well
    as a regression problem using the pytorch library. By unraveling the principles
    behind GNNs, we unlock a deeper comprehension of their capabilities and applications.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管图神经网络（GNN）取得了令人兴奋的进展，但相关资源仍然很少，这使得它对许多人来说难以接触。在这篇简短的文章中，我想提供一个图神经网络的简要介绍，涵盖数学描述以及使用pytorch库的回归问题。通过揭示GNN背后的原理，我们能够更深入地理解其能力和应用。
- en: '**Mathematical Description of GNNs**'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**图神经网络的数学描述**'
- en: A graph G can be defined as G = (V, E), where V is the set of nodes, and E are
    the edges between them. A graph is often represented by an adjacency matrix, A,
    which represents the presence or absence of edges between nodes i.e. aij takes
    values of 1 to indicate an edge (connection) between nodes i and j or 0 otherwise.
    If a graph has n nodes, A has a dimension of (n × n). The adjacency matrix is
    demonstrated in *Figure 1*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图 G 可以定义为 G = (V, E)，其中 V 是节点集合，E 是它们之间的边。图通常通过邻接矩阵 A 来表示，表示节点之间边的存在与否，即 aij
    的值为 1 表示节点 i 和节点 j 之间有边（连接），否则为 0。如果图有 n 个节点，则 A 的维度为 (n × n)。邻接矩阵在 *图 1* 中演示。
- en: '![](../Images/d1cd932311a015e28e70b76c55cf5f74.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1cd932311a015e28e70b76c55cf5f74.png)'
- en: Figure 1\. Adjacency matrix for three different graphs
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 三个不同图的邻接矩阵
- en: Each node (and edges! But we’ll come back to this later for simplicity) will
    have a set of features (e.g. if the node is a person, the features will be age,
    gender, height, job etc). If each node has f features, then the feature matrix
    X is (n × f). In some problems, each node may also have a target label which maybe
    a set of categorical labels or numerical values (shown in *Figure 2*).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点（以及边！但为了简化，我们稍后会回到边）将有一组特征（例如，如果节点是一个人，特征可能包括年龄、性别、身高、职业等）。如果每个节点有 f 个特征，则特征矩阵
    X 为 (n × f)。在某些问题中，每个节点可能还具有目标标签，该标签可能是一组分类标签或数值（如 *图 2* 所示）。
- en: '**Single Node Calculations**'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**单节点计算**'
- en: To learn the interdependence between any node and its neighbours, we need to
    consider the features of its neighbours. This is what enables GNNs to learn the
    structural representation of the data through a graph. Consider a node j with
    Nj neighbours, GNNs transform the features from each neighbour, aggregate them
    and then update node i’s feature space. Each of these steps are described as follows.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学习任何节点与其邻居之间的相互依赖关系，我们需要考虑其邻居的特征。这使得 GNN 能够通过图来学习数据的结构表示。考虑一个节点 j 及其 Nj 个邻居，GNN
    会转换每个邻居的特征，聚合这些特征，然后更新节点 i 的特征空间。以下是这些步骤的描述。
- en: '![](../Images/0858dd5d95be8a673a8737ac19200814.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0858dd5d95be8a673a8737ac19200814.png)'
- en: Figure 2\. A schematic of a node (j) with features xj and label yj and neighbour
    nodes (i, 2, 3), each with their on feature embeddings and corresponding label.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 一个节点（j）的示意图，具有特征 xj 和标签 yj，以及邻居节点（i、2、3），每个节点都有自己的特征嵌入和相应的标签。
- en: Neighbour feature transformation could be done a number of ways such as passing
    through an MLP network or by linear transformation such as
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 邻居特征转换可以通过多种方式进行，例如通过 MLP 网络或线性变换，例如
- en: '![](../Images/db48d782c7e6519c78c2b446c9387fb5.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db48d782c7e6519c78c2b446c9387fb5.png)'
- en: 'where w and b represent the weights and bias of the transformation. Information
    aggregation, the information from each neighboring nodes are then aggregated:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 w 和 b 表示变换的权重和偏置。信息聚合，即来自每个邻居节点的信息会被聚合：
- en: '![](../Images/e43f038327837089cafa9a3abafc5bf4.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e43f038327837089cafa9a3abafc5bf4.png)'
- en: 'The nature of the aggregation step could be a number of different methods such
    as summation, averaging, min/max pooling and concatenation:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合步骤的性质可以有多种不同的方法，例如求和、平均、最小/最大池化和拼接：
- en: '![](../Images/f59a108b5b916e5cbaef95e8541114e9.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f59a108b5b916e5cbaef95e8541114e9.png)'
- en: 'Following the aggregation step, the final step is to update node j:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚合步骤之后，最后一步是更新节点 j：
- en: '![](../Images/013599689b3480c1ca7ea900b40e5cbf.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/013599689b3480c1ca7ea900b40e5cbf.png)'
- en: This updated could be done using MLP with the concatenated node features and
    neighbour information aggregation (mj) or we could use linear transformation i.e.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 更新可以通过 MLP 使用拼接的节点特征和邻居信息聚合（mj）来完成，或者我们可以使用线性变换，即
- en: '![](../Images/3fa2a673ce468e1b1ea4a29c32af8aff.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3fa2a673ce468e1b1ea4a29c32af8aff.png)'
- en: Where U is a learnable weights matrix that combines the original node features
    (xj) with aggregated neighbour features (mj) through a non-linear activation function
    (ReLU in this case). This is it for the process of updating a single node in a
    single layer, the same process is applied to all other nodes in the graph, mathematically,
    this can be presented using the Adjacency matrix.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 U 是一个可学习的权重矩阵，它通过非线性激活函数（此处使用 ReLU）将原始节点特征（xj）与聚合的邻居特征（mj）结合起来。这就是在单层中更新单个节点的过程，相同的过程应用于图中的所有其他节点，数学上，这可以通过邻接矩阵来表示。
- en: '**Graph Level Calculation**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**图级计算**'
- en: 'For a graph with n nodes and each node has *f* features, we can concatenate
    all the features in a single matrix:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个包含n个节点的图，每个节点有*f*个特征，我们可以将所有特征连接成一个矩阵：
- en: '![](../Images/9498e7314928b00500195a4c89693cb8.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9498e7314928b00500195a4c89693cb8.png)'
- en: 'The neighbour feature transformation and aggregation steps can therefore be
    written as:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，邻居特征变换和聚合步骤可以写作：
- en: '![](../Images/630123572a8f0bc225a7721705bb0c4b.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/630123572a8f0bc225a7721705bb0c4b.png)'
- en: 'Where I is the identity matrix, this helps to include the each nodes own features
    too, otherwise, we are only considering the transformed features from the node
    j’s neighbours and not it’s own features. One final step is to normalise each
    node based on the number of connections i.e. for node j with Nj connections, the
    feature transformation can be done as:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其中I是单位矩阵，这有助于包括每个节点的自身特征，否则我们只考虑来自节点j邻居的变换特征，而不考虑其自身特征。最后一步是根据连接数对每个节点进行归一化，即对于具有Nj个连接的节点j，特征变换可以这样进行：
- en: '![](../Images/7c08e50dcb433ee0320f2e71594840e3.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c08e50dcb433ee0320f2e71594840e3.png)'
- en: 'The equation above can be adjusted as:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的方程可以调整为：
- en: '![](../Images/f92e5051e1e2e2d6438067a44dff65a1.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f92e5051e1e2e2d6438067a44dff65a1.png)'
- en: Where D is the degree matrix, a diagonal matrix of number of connections for
    each node. However, more commonly, this normalisation step is done as
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其中D是度矩阵，是每个节点连接数的对角矩阵。然而，更常见的是，这一步归一化是通过以下方式进行的：
- en: '![](../Images/39326a6cee6a5013846a077ad197452b.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/39326a6cee6a5013846a077ad197452b.png)'
- en: 'This is the graph convolution network (GCN) method that enables GNN to learn
    the structure and relationship between nodes. However, an issue with GCN is that
    the weight vector for neighbour feature transformation is shared across all neighbours
    i.e. all neighbours are considered equal, but this is usually not the case so
    not a good representative of real systems. To address is, graph attention network
    (GATs) can be used to compute the importance of a neighbour’s feature to the target
    node, allowing the different neighbours to contribute differently to the feature
    update of the target node based on their relevance. The attention coefficients
    are determined using a learnable matrix as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是图卷积网络（GCN）方法，它使得GNN能够学习节点之间的结构和关系。然而，GCN的一个问题是邻居特征变换的权重向量在所有邻居中是共享的，即所有邻居被认为是相等的，但通常并非如此，因此不能很好地代表真实系统。为了解决这个问题，可以使用图注意力网络（GAT）来计算邻居特征对目标节点的重要性，从而允许不同的邻居根据它们的相关性以不同的方式贡献于目标节点特征的更新。注意力系数是通过一个可学习的矩阵来确定的，如下所示：
- en: '![](../Images/d2f4ddfbeb15c5f5c453135d44fdda98.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2f4ddfbeb15c5f5c453135d44fdda98.png)'
- en: 'Where W is the shared learnable feature linear transformation, Wa is a learnable
    weight vector and eij is the raw attention score indicating importance of node
    i’s features to node j. The attention score is normalised using the SoftMax function:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中W是共享的可学习特征线性变换，Wa是一个可学习的权重向量，eij是原始的注意力分数，表示节点i的特征对节点j的重要性。注意力分数使用SoftMax函数进行归一化：
- en: '![](../Images/6af7ddab8bee47d14788676c8a01cf14.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6af7ddab8bee47d14788676c8a01cf14.png)'
- en: 'Now the feature aggregation step can be calculated using the attention coefficients:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，可以使用注意力系数来计算特征聚合步骤：
- en: '![](../Images/8a40d22ae79e2116e75022bfdf4fc1ad.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a40d22ae79e2116e75022bfdf4fc1ad.png)'
- en: This is it for a single layer, we can build multiple layers to increase the
    complexity of the model, this is demonstrated in *Figure 3*. Increasing the number
    of layers will allow the model to learn more global features and also capture
    more complex relationships, however, it is also likely to overfit so regularisation
    techniques should always be used to prevent this.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是单层的情况，我们可以构建多个层来增加模型的复杂性，这在*图3*中进行了演示。增加层数将允许模型学习更多的全局特征，并捕捉更复杂的关系，但也很容易导致过拟合，因此应始终使用正则化技术来防止过拟合。
- en: '![](../Images/781e51a407ec5c36ad5efd87597a4996.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/781e51a407ec5c36ad5efd87597a4996.png)'
- en: Figure 3\. An illustration of a multilayered GNN model
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. 一个多层GNN模型的示意图
- en: 'Finally, once final feature vectors for all nodes are obtained from the network,
    a feature matrix, H can be formed:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一旦从网络中获得所有节点的最终特征向量，就可以形成特征矩阵H：
- en: '![](../Images/849a73048c3e915b58d2294221aa16f7.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/849a73048c3e915b58d2294221aa16f7.png)'
- en: This feature matrix can be used to do a number of tasks e.g. node or graph classification.
    This brings us to the end of introduction into the mathematical description of
    GCN/GATs.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 该特征矩阵可以用于执行多种任务，例如节点或图的分类。这也将是GCN/GAT数学描述部分的结束。
- en: '**GCN Regression Example**'
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**GCN回归示例**'
- en: 'Let’s implement a regression example where the aim is to train a network to
    predict the value of a node given the value of all other nodes i.e. each node
    has a single feature (which is a scalar value). The aim of this example is to
    leverage the inherent relational information encoded in the graph to accurately
    predict numerical values for each node. The key thing to note is that we input
    the numerical value for all nodes except the target node (we mask the target node
    value with 0) then predict the target node’s value. For each data point, we repeat
    the process for all nodes. Perhaps this might come across as a bizarre task but
    lets see if we can predict the expected value of any node given the values of
    the other nodes. The data used is the corresponding simulation data to a series
    of sensors from industry and the graph structure I have chosen in the example
    below is based on the actual process structure. I have provided comments in the
    code to make it easy to follow. You can find a copy of the dataset [here](https://github.com/Nsharifi650/GNNRegression.git)
    (Note: this is my own data, generated from simulations).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现一个回归示例，目标是训练一个网络来预测一个节点的值，前提是已知其他所有节点的值，即每个节点有一个单一的特征（这是一个标量值）。本示例的目的是利用图中编码的固有关系信息，准确预测每个节点的数值。需要注意的是，我们输入所有节点的数值（除了目标节点，目标节点的值用0进行掩码），然后预测目标节点的值。对于每个数据点，我们对所有节点重复这一过程。也许这看起来像是一个奇怪的任务，但让我们看看是否能够根据其他节点的值预测任何节点的预期值。使用的数据是来自工业的传感器系列的相应仿真数据，下面示例中的图结构是基于实际过程结构的。我在代码中提供了注释，以便更容易理解。你可以在[这里](https://github.com/Nsharifi650/GNNRegression.git)找到数据集的副本（注意：这是我自己的数据，通过仿真生成）。
- en: This code and training procedure is far from being optimised but it’s aim is
    to illustrate the implementation of GNNs and get an intuition for how they work.
    An issue with the currently way I have done that should definitely not be done
    this way beyond learning purposes is the masking of node feature value and predicting
    it from the neighbours feature. Currently you’d have to loop over each node (not
    very efficient), a much better way to do is the stop the model from include it’s
    own features in the aggregation step and hence you wouldn’t need to do one node
    at a time but I thought it is easier to build intuition for the model with the
    current method:)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码和训练过程远未优化，但它的目的是展示GNN的实现，并让人对其工作原理有直观的理解。我目前的实现方式中有一个问题，除了用于学习目的外，这种方法绝对不应该继续使用，那就是将节点特征值进行掩码，并从邻居节点的特征中预测它。当前你需要对每个节点进行循环（效率不高），一种更好的方法是在聚合步骤中停止模型将自身特征包含进来，这样你就不需要一个节点一个节点地处理。但我认为通过当前的方法更容易为模型构建直观理解：)
- en: '**Preprocessing Data**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据预处理**'
- en: Importing the necessary libraries and Sensor data from CSV file. Normalise all
    data in the range of 0 to 1.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 导入必要的库和从CSV文件中读取传感器数据。将所有数据归一化到0到1的范围内。
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Defining the connectivity (edge index) between nodes in the graph using a PyTorch
    tensor — i.e. this provides the system’s graphical topology.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PyTorch张量定义图中节点之间的连接性（边缘索引）——即这提供了系统的图形拓扑。
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The Data imported from csv has a tabular structure but to use this in GNNs,
    it must be transformed to a graphical structure. Each row of data (one observation)
    is represented as one graph. Iterate through Each Row to Create Graphical representation
    of the data
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 从CSV导入的数据具有表格结构，但为了在GNN中使用，必须将其转换为图形结构。数据的每一行（一个观察值）表示一个图。迭代每一行以创建数据的图形表示。
- en: A mask is created for each node/sensor to indicate the presence (1) or absence
    (0) of data, allowing for flexibility in handling missing data. In most systems,
    there may be items with no data available hence the need for flexibility in handling
    missing data. Split the data into training and testing sets
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为每个节点/传感器创建一个掩码，以指示数据的存在（1）或缺失（0），从而提供处理缺失数据的灵活性。在大多数系统中，可能存在没有数据的项目，因此需要处理缺失数据的灵活性。将数据划分为训练集和测试集。
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Graph Visualisation**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**图形可视化**'
- en: The graph structure created above using the edge indices can be visualised using
    networkx.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述边缘索引创建的图结构可以通过networkx进行可视化。
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Model Definition**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型定义**'
- en: Let’s define the model. The model incorporates 2 GAT convolutional layers. The
    first layer transforms node features to an 8 dimensional space, and the second
    GAT layer further reduces this to an 8-dimensional representation.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义模型。该模型包含两个GAT卷积层，第一个层将节点特征转换到8维空间，第二个GAT层则将其进一步缩减为8维表示。
- en: GNNs are highly susceptible to overfitting, regularation (dropout) is applied
    after each GAT layer with a user defined probability to prevent over fitting.
    The dropout layer essentially randomly zeros some of the elements of the input
    tensor during training.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: GNN（图神经网络）对过拟合非常敏感，正则化（dropout）会在每个GAT层后应用，并使用用户定义的概率来防止过拟合。dropout层在训练过程中随机将输入张量的某些元素置零。
- en: The GAT convolution layer output results are passed through a fully connected
    (linear) layer to map the 8-dimensional output to the final node feature which
    in this case is a scalar value per node.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: GAT卷积层的输出结果通过一个全连接（线性）层，以将8维的输出映射到最终的节点特征，在本例中，每个节点对应一个标量值。
- en: Masking the value of the target Node; as mentioned earlier, the aim of this
    of task is to regress the value of the target node based on the value of it’s
    neighbours. This is the reason behind masking/replacing the target node’s value
    with zero.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对目标节点的值进行掩码处理；如前所述，本任务的目的是基于邻居节点的值回归目标节点的值。这也是将目标节点的值掩码或替换为零的原因。
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Training the model**'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练模型**'
- en: Initialising the model and defining the optimiser, loss function and the hyper
    parameters including learning rate, weight decay (for regularisation), batch_size
    and number of epochs.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化模型并定义优化器、损失函数以及包括学习率、权重衰减（用于正则化）、批处理大小和训练轮次在内的超参数。
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The training process is fairly standard, each graph (one data point) of data
    is passed through the forward pass of the model (iterating over each node and
    predicting the target node. The loss from the prediction is accumulated over the
    defined batch size before updating the GNN through backpropagation.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程相对标准，每个图（一个数据点）都会通过模型的前向传播（遍历每个节点并预测目标节点）。预测产生的损失会在定义的批处理大小上累积，然后通过反向传播更新GNN。
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Testing the trained model**'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**测试训练好的模型**'
- en: Using the test dataset, pass each graph through the forward pass of the trained
    model and predict each node’s value based on it’s neighbours value.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 使用测试数据集，将每个图像通过训练后的模型的前向传播，并根据每个节点的邻居节点的值预测其值。
- en: '[PRE7]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Visualising the test results**'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**可视化测试结果**'
- en: Using iplot we can visualise the predicted values of nodes against the ground
    truth values.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用iplot，我们可以将节点的预测值与真实值进行可视化对比。
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/cf42365780e85ca2cddbafe8d8993e00.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf42365780e85ca2cddbafe8d8993e00.png)'
- en: Despite a lack of fine tuning the model architecture or hyperparameters, it
    has done a decent job actually, we could tune the model further to get improved
    accuracy.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管没有对模型架构或超参数进行精细调优，实际上模型表现不错，我们可以进一步调优模型以提高准确性。
- en: This brings us to the end of this article. GNNs are relatively newer than other
    branches of machine learning, it will be very exciting to see the developments
    of this field but also it’s application to different problems. Finally, thank
    you for taking the time to read this article, I hope you found it useful in your
    understanding of GNNs or their mathematical background.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章到此为止。GNN相较于其他机器学习分支来说相对较新，看到这个领域的发展以及它应用于不同问题将是非常令人兴奋的。最后，感谢你花时间阅读这篇文章，希望它对你理解GNN或其数学背景有所帮助。
- en: '**Before you go**'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**在你离开之前**'
- en: Personally I really enjoy spending time to learn new concepts and apply those
    concepts to new problems and challenges and I am sure most of the people reading
    these kind of articles do too. I believe its a privilege to have the opportunity
    to do that, a privilege that everyone should have but not everyone does. We all
    have a responsibility to change that towards a brighter future for everyone. Please
    consider donating to UniArk ([UniArk.org)](http://uniark.org) to help talented
    students from a demography in the world so often overlooked by universities and
    countries — the persecuted minority (ethnic, religious or otherwise). UniArk searches
    the furthest and deepest to find talent and potential — the remote areas of developing
    countries. Your donation would be the beacon of hope for someone from an oppressive
    society. I hope you can help UniArk keep this beacon alight.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 个人而言，我非常喜欢花时间学习新概念，并将这些概念应用于新的问题和挑战，我相信大多数阅读这些文章的人也有同样的感受。我认为能够做这件事是一种特权，每个人都应该拥有这种特权，但并不是每个人都能拥有。我们每个人都有责任改变这一点，为每个人创造更加光明的未来。请考虑向UniArk捐款（[UniArk.org](http://uniark.org)），以帮助来自世界上常常被大学和国家忽视的群体——受迫害的少数群体（无论是种族、宗教还是其他方面）的才华横溢的学生。UniArk深入寻找最遥远、最贫困地区的人才与潜力——那些发展中国家的偏远地区。您的捐款将成为压迫社会中某个人的希望灯塔。我希望您能帮助UniArk保持这盏灯塔的光明。
- en: '*Unless otherwise noted, all images are by the author*'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有注明，所有图片均由作者提供*'
