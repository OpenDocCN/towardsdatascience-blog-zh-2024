- en: 'Understanding Positional Embeddings in Transformers: From Absolute to Rotary'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解变压器中的位置嵌入：从绝对到旋转
- en: 原文：[https://towardsdatascience.com/understanding-positional-embeddings-in-transformers-from-absolute-to-rotary-31c082e16b26?source=collection_archive---------2-----------------------#2024-07-20](https://towardsdatascience.com/understanding-positional-embeddings-in-transformers-from-absolute-to-rotary-31c082e16b26?source=collection_archive---------2-----------------------#2024-07-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/understanding-positional-embeddings-in-transformers-from-absolute-to-rotary-31c082e16b26?source=collection_archive---------2-----------------------#2024-07-20](https://towardsdatascience.com/understanding-positional-embeddings-in-transformers-from-absolute-to-rotary-31c082e16b26?source=collection_archive---------2-----------------------#2024-07-20)
- en: A deep dive into absolute, relative, and rotary positional embeddings with code
    examples
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入分析绝对位置、相对位置和旋转位置嵌入，并附有代码示例
- en: '[](https://medium.com/@mina.ghashami?source=post_page---byline--31c082e16b26--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page---byline--31c082e16b26--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--31c082e16b26--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--31c082e16b26--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page---byline--31c082e16b26--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mina.ghashami?source=post_page---byline--31c082e16b26--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page---byline--31c082e16b26--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--31c082e16b26--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--31c082e16b26--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page---byline--31c082e16b26--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--31c082e16b26--------------------------------)
    ·17 min read·Jul 20, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--31c082e16b26--------------------------------)
    ·阅读时间 17 分钟·2024年7月20日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d4cf43e5eac52b5568bc147832f80905.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d4cf43e5eac52b5568bc147832f80905.png)'
- en: Rotary position embedding — Image from [[6](https://arxiv.org/pdf/2104.09864)]
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 旋转位置嵌入 — 图片来自 [[6](https://arxiv.org/pdf/2104.09864)]
- en: 'One of the key components of transformers are **positional embeddings**. You
    may ask: why? Because the self-attention mechanism in transformers is permutation-invariant;
    that means it computes the amount of `attention` each token in the input receives
    from other tokens in the sequence, however it does not take the order of the tokens
    into account. In fact, *attention mechanism treats the sequence as a bag of tokens*.
    For this reason, we need to have another component called positional embedding
    which accounts for the order of tokens and it influences token embeddings. But
    what are the different types of positional embeddings and how are they implemented?'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器（transformers）的一个关键组件是**位置嵌入**。你可能会问：为什么？因为变压器中的自注意力机制是置换不变的；这意味着它计算每个输入标记从序列中其他标记接收到的`注意力`的量，但它并不考虑标记的顺序。事实上，*注意力机制将序列视为标记的集合*。因此，我们需要另一个组件，称为位置嵌入，用于表示标记的顺序，并影响标记的嵌入。但不同类型的位置嵌入是什么，它们是如何实现的呢？
- en: In this post, we take a look at three major types of positional embeddings and
    dive deep into their implementation.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将深入探讨三种主要的位置信息嵌入，并深入了解它们的实现方式。
- en: '**Here is the table of content for this post:**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**以下是本文的目录：**'
- en: 1\. Context and Background
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 背景与概述
- en: 2\. Absolute Positional Embedding
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 绝对位置嵌入
- en: 2.1 Learned Approach
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2.1 学习方法
- en: 2.2 Fixed Approach (Sinusoidal)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2.2 固定方法（正弦波）
- en: '2.3 Code Example: RoBERTa Implementation'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2.3 代码示例：RoBERTa 实现
