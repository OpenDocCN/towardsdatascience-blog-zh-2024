- en: Transforming Next-Token Prediction into Classification with LLMs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将下一个标记预测转化为LLM分类任务
- en: 原文：[https://towardsdatascience.com/transforming-next-token-prediction-into-classification-with-llms-fb4f33a02637?source=collection_archive---------3-----------------------#2024-06-20](https://towardsdatascience.com/transforming-next-token-prediction-into-classification-with-llms-fb4f33a02637?source=collection_archive---------3-----------------------#2024-06-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/transforming-next-token-prediction-into-classification-with-llms-fb4f33a02637?source=collection_archive---------3-----------------------#2024-06-20](https://towardsdatascience.com/transforming-next-token-prediction-into-classification-with-llms-fb4f33a02637?source=collection_archive---------3-----------------------#2024-06-20)
- en: 'From tokens to labels: Performing classification with large language models'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从标记到标签：使用大型语言模型进行分类
- en: '[](https://medium.com/@yuchengtsai84?source=post_page---byline--fb4f33a02637--------------------------------)[![Yu-Cheng
    Tsai](../Images/c0ec2d4b9fea512040c8e6e0250670fc.png)](https://medium.com/@yuchengtsai84?source=post_page---byline--fb4f33a02637--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fb4f33a02637--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fb4f33a02637--------------------------------)
    [Yu-Cheng Tsai](https://medium.com/@yuchengtsai84?source=post_page---byline--fb4f33a02637--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@yuchengtsai84?source=post_page---byline--fb4f33a02637--------------------------------)[![Yu-Cheng
    Tsai](../Images/c0ec2d4b9fea512040c8e6e0250670fc.png)](https://medium.com/@yuchengtsai84?source=post_page---byline--fb4f33a02637--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fb4f33a02637--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fb4f33a02637--------------------------------)
    [Yu-Cheng Tsai](https://medium.com/@yuchengtsai84?source=post_page---byline--fb4f33a02637--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fb4f33a02637--------------------------------)
    ·6 min read·Jun 20, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fb4f33a02637--------------------------------)
    ·阅读时间6分钟·2024年6月20日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/b7df3c282346bf90d1b8ab34a4d1fd47.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7df3c282346bf90d1b8ab34a4d1fd47.png)'
- en: Photo by [Myles Bloomfield](https://unsplash.com/@loomydoons?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Myles Bloomfield](https://unsplash.com/@loomydoons?utm_source=medium&utm_medium=referral)
    在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Free [link](https://medium.com/towards-data-science/transforming-next-token-prediction-into-classification-with-llms-fb4f33a02637?sk=c5693c58c60d47d6a1846cbafffd1b8a)!
    Thanks and please enjoy the read. :)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 免费[链接](https://medium.com/towards-data-science/transforming-next-token-prediction-into-classification-with-llms-fb4f33a02637?sk=c5693c58c60d47d6a1846cbafffd1b8a)!
    感谢，祝您阅读愉快。:)
- en: Large Language Models (LLMs), trained on vast amounts of internet data, are
    versatile and can perform a wide range of natural language tasks. One common application
    is classification, a supervised learning task that categorizes subjects into pre-defined
    labels. [Zero-shot](https://huggingface.co/tasks/zero-shot-classification) and
    f[ew-shots classification](https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api)
    have become popular techniques, enabling LLMs to perform classification tasks
    with no training data or a few examples. However, for better accuracy, it is demonstrated
    that instruction fine-tuning can improve the performance by tuning LLMs with curated
    datasets.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）通过在大量互联网数据上进行训练，展现出多功能性，可以执行各种自然语言任务。其中一个常见应用是分类，这是一个将主题分为预定义标签的监督学习任务。[零-shot](https://huggingface.co/tasks/zero-shot-classification)和[少-shot分类](https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api)已经成为流行技术，使得LLMs能够在没有训练数据或仅有少量示例的情况下执行分类任务。然而，研究表明，为了提高准确性，指令微调能够通过使用精心挑选的数据集对LLMs进行调优，从而提高其性能。
- en: Instruction Fine-Tuning LLMs
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指令微调大型语言模型（LLMs）
- en: The common practice for instruction fine-tuning involves constructing a dataset
    consisting of question-and-answer pairs. Pre-trained LLMs are then further fine-tuned
    using these pairs in a supervised manner.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的指令微调方法是构建一个包含问答对的数据集。然后，使用这些数据对预训练的LLM进行监督式的进一步微调。
- en: You can check my previous post for this approach.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以查看我之前的文章了解这种方法。
