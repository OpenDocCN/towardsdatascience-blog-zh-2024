- en: 'Audio Diffusion: Generative Musicâ€™s Secret Sauce'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: éŸ³é¢‘æ‰©æ•£ï¼šç”ŸæˆéŸ³ä¹çš„ç§˜å¯†æ­¦å™¨
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/audio-diffusion-generative-musics-secret-sauce-f625d0aca800?source=collection_archive---------0-----------------------#2024-01-22](https://towardsdatascience.com/audio-diffusion-generative-musics-secret-sauce-f625d0aca800?source=collection_archive---------0-----------------------#2024-01-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/audio-diffusion-generative-musics-secret-sauce-f625d0aca800?source=collection_archive---------0-----------------------#2024-01-22](https://towardsdatascience.com/audio-diffusion-generative-musics-secret-sauce-f625d0aca800?source=collection_archive---------0-----------------------#2024-01-22)
- en: '![](../Images/1bd508ba6fc0a161a09a10ef07843c01.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1bd508ba6fc0a161a09a10ef07843c01.png)'
- en: Image generated with DALLÂ·E
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒç”± DALLÂ·E ç”Ÿæˆ
- en: Exploring the principles behind diffusion technology and how it is being used
    to create groundbreaking AI tools for artists and producers.
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¢è®¨æ‰©æ•£æŠ€æœ¯èƒŒåçš„åŸç†åŠå…¶å¦‚ä½•è¢«åº”ç”¨äºä¸ºè‰ºæœ¯å®¶å’Œåˆ¶ä½œäººåˆ›é€ çªç ´æ€§äººå·¥æ™ºèƒ½å·¥å…·ã€‚
- en: '[](https://medium.com/@crlandschoot?source=post_page---byline--f625d0aca800--------------------------------)[![Christopher
    Landschoot](../Images/99a2569f5a6a3a99fd1f72553aa3d634.png)](https://medium.com/@crlandschoot?source=post_page---byline--f625d0aca800--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f625d0aca800--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f625d0aca800--------------------------------)
    [Christopher Landschoot](https://medium.com/@crlandschoot?source=post_page---byline--f625d0aca800--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@crlandschoot?source=post_page---byline--f625d0aca800--------------------------------)[![Christopher
    Landschoot](../Images/99a2569f5a6a3a99fd1f72553aa3d634.png)](https://medium.com/@crlandschoot?source=post_page---byline--f625d0aca800--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f625d0aca800--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f625d0aca800--------------------------------)
    [Christopher Landschoot](https://medium.com/@crlandschoot?source=post_page---byline--f625d0aca800--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f625d0aca800--------------------------------)
    Â·14 min readÂ·Jan 22, 2024
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f625d0aca800--------------------------------)
    Â·é˜…è¯»æ—¶é—´ï¼š14åˆ†é’ŸÂ·2024å¹´1æœˆ22æ—¥
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'Much has been made of the hype of recent generative music AI algorithms. Some
    see it as the future of creativity and others see it as the death of music. While
    I tend to lean towards the former camp, as an engineer and researcher, I generally
    attempt to view these advancements through a more objective lens. With this in
    mind, I wanted to offer an introduction to one of the core technologies fueling
    the world of generative audio and music: [*Diffusion*](https://en.wikipedia.org/wiki/Diffusion_model).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘å…³äºç”ŸæˆéŸ³ä¹äººå·¥æ™ºèƒ½ç®—æ³•çš„ç‚’ä½œå¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚æœ‰äº›äººè®¤ä¸ºå®ƒæ˜¯åˆ›é€ åŠ›çš„æœªæ¥ï¼Œè€Œå¦ä¸€äº›äººåˆ™è®¤ä¸ºå®ƒæ˜¯éŸ³ä¹çš„ç»ˆç»“ã€‚è™½ç„¶æˆ‘å€¾å‘äºæ”¯æŒå‰è€…ï¼Œä½†ä½œä¸ºä¸€åå·¥ç¨‹å¸ˆå’Œç ”ç©¶å‘˜ï¼Œæˆ‘é€šå¸¸å°è¯•ä»æ›´åŠ å®¢è§‚çš„è§’åº¦æ¥çœ‹å¾…è¿™äº›è¿›å±•ã€‚é‰´äºæ­¤ï¼Œæˆ‘æƒ³ä»‹ç»ä¸€ä¸‹é©±åŠ¨ç”ŸæˆéŸ³é¢‘å’ŒéŸ³ä¹ä¸–ç•Œçš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ï¼š[*æ‰©æ•£*](https://en.wikipedia.org/wiki/Diffusion_model)ã€‚
- en: My goal is not to sell or diminish the hype, but rather shed some light on what
    is happening under the hood so that musicians, producers, hobbyists, and creators,
    can better understand these new seemingly magic music-making black boxes. I will
    answer what it means by the claim that these AI algorithms are â€œcreating something
    completely newâ€ and how that differs from human originality. I hope that a clearer
    picture will lower the collective temperature and provide insight into how these
    powerful technologies can be leveraged to the benefit of the creator.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çš„ç›®æ ‡ä¸æ˜¯æ¨é”€æˆ–è´¬ä½è¿™äº›ç‚’ä½œï¼Œè€Œæ˜¯æ­ç¤ºè¿™äº›æŠ€æœ¯èƒŒåå‘ç”Ÿçš„äº‹æƒ…ï¼Œè®©éŸ³ä¹å®¶ã€åˆ¶ä½œäººã€çˆ±å¥½è€…å’Œåˆ›ä½œè€…èƒ½æ›´å¥½åœ°ç†è§£è¿™äº›çœ‹ä¼¼ç¥å¥‡çš„éŸ³ä¹åˆ›ä½œé»‘ç›’ã€‚æˆ‘å°†å›ç­”è¿™äº›äººå·¥æ™ºèƒ½ç®—æ³•â€œåˆ›é€ å‡ºå®Œå…¨æ–°çš„ä¸œè¥¿â€è¿™ä¸€è¯´æ³•çš„å«ä¹‰ï¼Œä»¥åŠå®ƒä¸äººç±»åŸåˆ›æ€§æœ‰ä½•ä¸åŒã€‚æˆ‘å¸Œæœ›é€šè¿‡æ›´æ¸…æ™°çš„è§£é‡Šï¼Œé™ä½é›†ä½“çš„ç„¦è™‘ï¼Œå¹¶æä¾›æ´è§ï¼Œå¸®åŠ©åˆ›ä½œè€…æ›´å¥½åœ°åˆ©ç”¨è¿™äº›å¼ºå¤§çš„æŠ€æœ¯ã€‚
- en: This piece will touch on technical topics, but you do not need an engineering
    background to follow along. Letâ€™s begin with some context and definitions.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡å°†æ¶‰åŠä¸€äº›æŠ€æœ¯è¯é¢˜ï¼Œä½†ä½ ä¸éœ€è¦å…·å¤‡å·¥ç¨‹èƒŒæ™¯å°±èƒ½ç†è§£ã€‚è®©æˆ‘ä»¬å…ˆä»ä¸€äº›èƒŒæ™¯ä¿¡æ¯å’Œå®šä¹‰å¼€å§‹ã€‚
- en: Background
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: èƒŒæ™¯
- en: The term â€œAI-generatedâ€ has become pervasive across the music industry, but
    what qualifies as â€œAI-generatedâ€ is actually quite clouded. Eager to jump on the
    hype of the buzzword, this claim is casually tossed around whether AI is used
    to emulate an effect, automatically mix or master, separate [stems](https://en.wikipedia.org/wiki/Stem_(audio)),
    or augment timbre. As long as the final audio has been touched in some way by
    AI, the term gets slapped on the entire piece. However, the vast majority of music
    presently released continues to be primarily generated through human production
    (yes, even ghostwriterâ€™s â€œ[Heart On My Sleeve](https://youtu.be/7HZ2ie2ErFI?si=Ie3954lyyj7l-tEG)â€
    ğŸ‘»).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: â€œAIç”Ÿæˆâ€è¿™ä¸ªæœ¯è¯­å·²ç»åœ¨éŸ³ä¹è¡Œä¸šä¸­å˜å¾—éå¸¸æ™®åŠï¼Œä½†ä»€ä¹ˆæ‰ç®—æ˜¯â€œAIç”Ÿæˆâ€çš„å†…å®¹å®é™…ä¸Šæ˜¯ç›¸å½“æ¨¡ç³Šçš„ã€‚ä¸ºäº†èµ¶ä¸Šè¿™ä¸ªæµè¡Œè¯çš„çƒ­æ½®ï¼Œè¿™ä¸ªè¯´æ³•è¢«éšæ„åœ°ä½¿ç”¨ï¼Œæ— è®ºæ˜¯AIç”¨æ¥æ¨¡ä»¿æŸç§æ•ˆæœã€è‡ªåŠ¨æ··éŸ³æˆ–æ¯å¸¦å¤„ç†ã€åˆ†ç¦»[éŸ³è½¨](https://en.wikipedia.org/wiki/Stem_(audio))ï¼Œè¿˜æ˜¯å¢å¼ºéŸ³è‰²ã€‚åªè¦æœ€ç»ˆçš„éŸ³é¢‘åœ¨æŸç§ç¨‹åº¦ä¸Šå—åˆ°AIçš„å¤„ç†ï¼Œè¿™ä¸ªæœ¯è¯­å°±ä¼šè¢«è´´ä¸Šæ•´ä¸ªä½œå“ã€‚ç„¶è€Œï¼Œç›®å‰å‘å¸ƒçš„å¤§å¤šæ•°éŸ³ä¹ä»ç„¶ä¸»è¦é€šè¿‡äººå·¥åˆ¶ä½œç”Ÿæˆï¼ˆæ˜¯çš„ï¼Œå³ä½¿æ˜¯é¬¼æ‰ä½œæ›²è€…çš„â€œ[Heart
    On My Sleeve](https://youtu.be/7HZ2ie2ErFI?si=Ie3954lyyj7l-tEG)â€ ğŸ‘»ï¼‰ã€‚
- en: Even though this â€œAI-generatedâ€ term is becoming hackneyed for clicks, an appropriate
    use is when new sounds truly are being created by a computer, i.e. *Generative
    Audio*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡â€œAIç”Ÿæˆâ€è¿™ä¸ªæœ¯è¯­ä¸ºäº†ç‚¹å‡»ç‡è€Œå˜å¾—è€ç”Ÿå¸¸è°ˆï¼Œä½†å®ƒçš„æ°å½“ä½¿ç”¨æ˜¯å½“æ–°å£°éŸ³çš„ç¡®æ˜¯ç”±è®¡ç®—æœºç”Ÿæˆæ—¶ï¼Œå³*ç”ŸæˆéŸ³é¢‘*ã€‚
- en: Audio generation can encompass the creation of sound effects samples, melodies,
    vocals, and even full songs. The two main ways that this is achieved are via *MIDI
    Generation* and A*udio Waveform Generation*. [MIDI (Musical Instrument Digital
    Interface)](https://blog.landr.com/what-is-midi/) generation has a much lower
    computational cost and can provide high-quality outputs, as the generated MIDI
    data is then run through an existing virtual instrument to produce sounds. This
    is the same concept as a producer programming MIDI on a [piano roll](https://blog.landr.com/piano-roll/)
    and playing it through a [VST](https://en.wikipedia.org/wiki/Virtual_Studio_Technology)
    plugin such as [Serum](https://xferrecords.com/products/serum/).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: éŸ³é¢‘ç”Ÿæˆå¯ä»¥æ¶µç›–éŸ³æ•ˆæ ·æœ¬ã€æ—‹å¾‹ã€äººå£°ï¼Œç”šè‡³å®Œæ•´æ­Œæ›²çš„åˆ›ä½œã€‚å®ç°è¿™ä¸€ç‚¹çš„ä¸¤ç§ä¸»è¦æ–¹å¼æ˜¯é€šè¿‡*MIDIç”Ÿæˆ*å’Œ*éŸ³é¢‘æ³¢å½¢ç”Ÿæˆ*ã€‚[MIDIï¼ˆæ•°å­—ä¹å™¨æ¥å£ï¼‰](https://blog.landr.com/what-is-midi/)ç”Ÿæˆè®¡ç®—æˆæœ¬è¾ƒä½ï¼Œå¹¶ä¸”èƒ½å¤Ÿæä¾›é«˜è´¨é‡çš„è¾“å‡ºï¼Œå› ä¸ºç”Ÿæˆçš„MIDIæ•°æ®ä¼šé€šè¿‡ç°æœ‰çš„è™šæ‹Ÿä¹å™¨æ¥äº§ç”Ÿå£°éŸ³ã€‚è¿™ä¸åˆ¶ä½œäººé€šè¿‡[piano
    roll](https://blog.landr.com/piano-roll/)ç¼–ç¨‹MIDIå¹¶é€šè¿‡[VST](https://en.wikipedia.org/wiki/Virtual_Studio_Technology)æ’ä»¶ï¼ˆå¦‚[Serum](https://xferrecords.com/products/serum/)ï¼‰æ’­æ”¾çš„æ¦‚å¿µç›¸åŒã€‚
- en: '![](../Images/2825dd04aa8dd49f0cb8d0cd21b1550c.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2825dd04aa8dd49f0cb8d0cd21b1550c.png)'
- en: MIDI Piano Roll in Pro Tools
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Pro Toolsä¸­çš„MIDIé’¢ç´å·è½´
- en: While this is compelling, it is only partially generative, as no audio is actually
    produced by AI, just as humans cannot synthesize instrument sounds from thin air.
    The creative capabilities are further limited by whatever virtual instruments
    the algorithm has access to. Even with these limitations, products implementing
    this technique, such as [AIVA](https://www.aiva.ai/) and [Seeds by Lemonaide](https://www.lemonaide.ai/),
    can generate quite compelling outputs.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶è¿™ä¸€ç‚¹å¾ˆæœ‰å¸å¼•åŠ›ï¼Œä½†å®ƒä»…éƒ¨åˆ†æ˜¯ç”Ÿæˆçš„ï¼Œå› ä¸ºå®é™…ä¸Šæ²¡æœ‰éŸ³é¢‘æ˜¯ç”±AIç”Ÿæˆçš„ï¼Œå°±åƒäººç±»ä¸èƒ½å‡­ç©ºåˆæˆä¹å™¨çš„å£°éŸ³ä¸€æ ·ã€‚åˆ›ä½œèƒ½åŠ›è¿˜å—åˆ°ç®—æ³•èƒ½å¤Ÿè®¿é—®çš„è™šæ‹Ÿä¹å™¨çš„é™åˆ¶ã€‚å³ä¾¿æœ‰è¿™äº›é™åˆ¶ï¼Œé‡‡ç”¨è¿™ç§æŠ€æœ¯çš„äº§å“ï¼Œå¦‚[AIVA](https://www.aiva.ai/)å’Œ[Seeds
    by Lemonaide](https://www.lemonaide.ai/)ï¼Œä¹Ÿèƒ½å¤Ÿç”Ÿæˆç›¸å½“å¼•äººæ³¨ç›®çš„è¾“å‡ºã€‚
- en: Audio waveform generation is a much more complicated task, as it is an end-to-end
    system that does not rely on any external technology. In other words, it produces
    sounds from scratch. This process most precisely aligns with the true definition
    of â€œAI-generatedâ€ audio.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: éŸ³é¢‘æ³¢å½¢ç”Ÿæˆæ˜¯ä¸€é¡¹æ›´ä¸ºå¤æ‚çš„ä»»åŠ¡ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„ç³»ç»Ÿï¼Œä¸ä¾èµ–ä»»ä½•å¤–éƒ¨æŠ€æœ¯ã€‚æ¢å¥è¯è¯´ï¼Œå®ƒä»é›¶å¼€å§‹ç”Ÿæˆå£°éŸ³ã€‚è¿™ä¸ªè¿‡ç¨‹æœ€ç²¾ç¡®åœ°ç¬¦åˆâ€œAIç”Ÿæˆâ€éŸ³é¢‘çš„çœŸæ­£å®šä¹‰ã€‚
- en: Audio waveform generation can be accomplished using various approaches and yield
    different outcomes. It can produce single samples, such as [Audialabâ€™s ED 2](https://audialab.com/features/)
    and [Humanize](https://audialab.com/humanize/) or my previous work with [Tiny
    Audio Diffusion](/tiny-audio-diffusion-ddc19e90af9b), all the way up to full songs,
    with models such as [AudioLM](https://google-research.github.io/seanet/audiolm/examples/),
    [MoÃ»sai](https://flavioschneider.notion.site/flavioschneider/Audio-Generation-with-Diffusion-c4f29f39048d4f03a23da13078a44cdb),
    [Riffusion](https://www.riffusion.com/), [MusicGen](https://ai.honu.io/papers/musicgen/),
    and [Stable Audio](https://www.stableaudio.com/). Among these state-of-the-art
    models, many leverage some form of *Diffusion* to generate sounds. You have likely
    at least tangentially heard of diffusion from [Stable Diffusion](https://stability.ai/stable-diffusion)
    or any of the other top-performing image generation models that took the world
    by storm. This generative method can be applied to audio as well. But what does
    any of it actually mean?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: éŸ³é¢‘æ³¢å½¢ç”Ÿæˆå¯ä»¥é€šè¿‡å¤šç§æ–¹æ³•å®ç°ï¼Œå¹¶äº§ç”Ÿä¸åŒçš„ç»“æœã€‚å®ƒå¯ä»¥ç”Ÿæˆå•ä¸ªæ ·æœ¬ï¼Œæ¯”å¦‚[Audialabçš„ED 2](https://audialab.com/features/)å’Œ[Humanize](https://audialab.com/humanize/)ï¼Œæˆ–æ˜¯æˆ‘ä¹‹å‰çš„ä½œå“[Tiny
    Audio Diffusion](/tiny-audio-diffusion-ddc19e90af9b)ï¼Œä¹Ÿå¯ä»¥ç”Ÿæˆå®Œæ•´çš„æ­Œæ›²ï¼Œå¦‚[AudioLM](https://google-research.github.io/seanet/audiolm/examples/)ã€[MoÃ»sai](https://flavioschneider.notion.site/flavioschneider/Audio-Generation-with-Diffusion-c4f29f39048d4f03a23da13078a44cdb)ã€[Riffusion](https://www.riffusion.com/)ã€[MusicGen](https://ai.honu.io/papers/musicgen/)å’Œ[Stable
    Audio](https://www.stableaudio.com/)ã€‚åœ¨è¿™äº›æœ€å…ˆè¿›çš„æ¨¡å‹ä¸­ï¼Œè®¸å¤šéƒ½åˆ©ç”¨äº†æŸç§å½¢å¼çš„*æ‰©æ•£*æ¥ç”Ÿæˆå£°éŸ³ã€‚ä½ å¯èƒ½è‡³å°‘åœ¨æŸç§ç¨‹åº¦ä¸Šå¬è¯´è¿‡æ‰©æ•£ï¼Œå¯èƒ½æ˜¯é€šè¿‡[ç¨³å®šæ‰©æ•£](https://stability.ai/stable-diffusion)æˆ–å…¶ä»–ä¸€äº›æ›¾å¸­å·å…¨çƒçš„é¡¶å°–å›¾åƒç”Ÿæˆæ¨¡å‹ã€‚è¿™ç§ç”Ÿæˆæ–¹æ³•åŒæ ·å¯ä»¥åº”ç”¨äºéŸ³é¢‘ã€‚é‚£ä¹ˆè¿™ä¸€åˆ‡åˆ°åº•æ˜¯ä»€ä¹ˆæ„æ€å‘¢ï¼Ÿ
- en: What is Diffusion?
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯æ‰©æ•£ï¼Ÿ
- en: The Basics
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŸºç¡€çŸ¥è¯†
- en: In the context of AI, diffusion simply refers to the process of adding or removing
    noise from a signal (like static from an old TV). *Forward Diffusion* adds noise
    to a signal (*Noising*) and *Reverse Diffusion* removes noise (*Denoising*). At
    a conceptual level, diffusion models take white noise and step through the denoising
    processes until the audio resembles something recognizable, such as a sample or
    a song. This process of denoising a signal is the secret sauce in the creativity
    of many generative audio models.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨äººå·¥æ™ºèƒ½çš„èƒŒæ™¯ä¸‹ï¼Œæ‰©æ•£æŒ‡çš„åªæ˜¯ç»™ä¿¡å·æ·»åŠ æˆ–ç§»é™¤å™ªå£°çš„è¿‡ç¨‹ï¼ˆå°±åƒè€ç”µè§†æœºä¸­çš„é™ç”µå™ªå£°ï¼‰ã€‚*å‰å‘æ‰©æ•£*å‘ä¿¡å·ä¸­æ·»åŠ å™ªå£°ï¼ˆ*å™ªå£°åŒ–*ï¼‰ï¼Œè€Œ*åå‘æ‰©æ•£*åˆ™ç§»é™¤å™ªå£°ï¼ˆ*å»å™ª*ï¼‰ã€‚ä»æ¦‚å¿µå±‚é¢æ¥çœ‹ï¼Œæ‰©æ•£æ¨¡å‹å°†ç™½å™ªå£°é€æ­¥é€šè¿‡å»å™ªè¿‡ç¨‹ï¼Œç›´åˆ°éŸ³é¢‘ç±»ä¼¼äºæŸä¸ªå¯è¯†åˆ«çš„å£°éŸ³ï¼Œæ¯”å¦‚ä¸€ä¸ªæ ·æœ¬æˆ–ä¸€é¦–æ­Œã€‚è¿™ä¸ªå»å™ªè¿‡ç¨‹æ˜¯è®¸å¤šç”ŸæˆéŸ³é¢‘æ¨¡å‹åˆ›é€ åŠ›çš„ç§˜å¯†æ­¦å™¨ã€‚
- en: '![](../Images/a50d1ee92ac55dd644ea459cf0b23f44.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a50d1ee92ac55dd644ea459cf0b23f44.png)'
- en: 'Audio Waveform Diffusion (Source: [CRASH: Raw Audio Score-based Generative
    Modeling for Controllable High-resolution Drum Sound Synthesis (Rouard, Hadjeres)](https://github.com/simonrouard/CRASH))'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: éŸ³é¢‘æ³¢å½¢æ‰©æ•£ï¼ˆæ¥æºï¼š[CRASHï¼šåŸºäºåŸå§‹éŸ³é¢‘è¯„åˆ†çš„å¯æ§é«˜åˆ†è¾¨ç‡é¼“å£°åˆæˆç”Ÿæˆæ¨¡å‹ï¼ˆRouard, Hadjeresï¼‰](https://github.com/simonrouard/CRASH)ï¼‰
- en: This process was originally developed for images. Looking at how the noise resolves
    into an image (for instance, a puppy sitting next to a tennis ball) provides an
    even clearer example of how these models work.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¿‡ç¨‹æœ€åˆæ˜¯ä¸ºå›¾åƒå¼€å‘çš„ã€‚è§‚å¯Ÿå™ªå£°å¦‚ä½•è§£ææˆä¸€å¹…å›¾åƒï¼ˆä¾‹å¦‚ï¼Œä¸€åªå°ç‹—ååœ¨ç½‘çƒæ—è¾¹ï¼‰èƒ½æ›´æ¸…æ¥šåœ°å±•ç¤ºè¿™äº›æ¨¡å‹æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚
- en: '![](../Images/35afb6da06976e35eca7951128b52ec0.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/35afb6da06976e35eca7951128b52ec0.png)'
- en: Image Diffusion (Image generated with Stable Diffusion)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒæ‰©æ•£ï¼ˆå›¾åƒç”Ÿæˆä½¿ç”¨ç¨³å®šæ‰©æ•£ï¼‰
- en: With a conceptual understanding, letâ€™s take a look under the hood at the key
    components of the architecture of an audio diffusion model. While this will veer
    into the technical side of things, stick with me, as a deeper understanding of
    how these algorithms work will better illustrate why and how they produce the
    results that they do (if not, you can always just ask [ChatGPT](https://chat.openai.com/)
    for the Cliff Notes).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æ¦‚å¿µæ€§çš„ç†è§£ï¼Œè®©æˆ‘ä»¬æ·±å…¥æ¢è®¨éŸ³é¢‘æ‰©æ•£æ¨¡å‹æ¶æ„çš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚è™½ç„¶è¿™ä¼šæ¶‰åŠä¸€äº›æŠ€æœ¯æ€§å†…å®¹ï¼Œä½†è¯·è·Ÿç€æˆ‘ï¼Œå› ä¸ºå¯¹è¿™äº›ç®—æ³•å¦‚ä½•å·¥ä½œçš„æ·±å…¥ç†è§£å°†æ›´å¥½åœ°è¯´æ˜å®ƒä»¬æ˜¯å¦‚ä½•äº§ç”Ÿç»“æœçš„ï¼ˆå¦‚æœæ²¡æœ‰ï¼Œæ‚¨éšæ—¶å¯ä»¥å‘[ChatGPT](https://chat.openai.com/)è¯·æ±‚ç®€æ˜æ‰¼è¦çš„è§£é‡Šï¼‰ã€‚
- en: The U-Net Model Architecture, Compression, and Reconstruction
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: U-Netæ¨¡å‹æ¶æ„ã€å‹ç¼©ä¸é‡å»º
- en: At the core of an audio diffusion model is the [*U-Net*](https://arxiv.org/abs/1505.04597).
    Originally developed for medical image segmentation and aptly named for its resemblance
    to a U, the U-Net has been adapted to generative audio due to its powerful ability
    to capture both local and global features in data. The original U-Net was a 2-dimensional
    [*convolutional neural network*](https://en.wikipedia.org/wiki/Convolutional_neural_network)
    (CNN) used for images, but can be adapted to 1-dimensional convolution to work
    with audio waveform data. See a visual representation of the original U-Net architecture
    (for images) below.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨éŸ³é¢‘æ‰©æ•£æ¨¡å‹çš„æ ¸å¿ƒæ˜¯[*U-Net*](https://arxiv.org/abs/1505.04597)ã€‚U-Netæœ€åˆæ˜¯ä¸ºäº†åŒ»å­¦å›¾åƒåˆ†å‰²è€Œå¼€å‘çš„ï¼Œå› å…¶å¤–å½¢åƒå­—æ¯Uè€Œå¾—åï¼Œåæ¥ç”±äºå…¶å¼ºå¤§çš„èƒ½åŠ›èƒ½å¤Ÿæ•æ‰æ•°æ®ä¸­çš„å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ï¼Œè¢«é€‚åº”ç”¨äºç”ŸæˆéŸ³é¢‘ã€‚åŸå§‹çš„U-Netæ˜¯ä¸€ä¸ªäºŒç»´[*å·ç§¯ç¥ç»ç½‘ç»œ*](https://en.wikipedia.org/wiki/Convolutional_neural_network)ï¼ˆCNNï¼‰ï¼Œç”¨äºå›¾åƒå¤„ç†ï¼Œä½†ä¹Ÿå¯ä»¥é€‚é…ä¸ºä¸€ç»´å·ç§¯ï¼Œä»¥å¤„ç†éŸ³é¢‘æ³¢å½¢æ•°æ®ã€‚è¯·å‚è§ä¸‹é¢çš„åŸå§‹U-Netæ¶æ„ï¼ˆç”¨äºå›¾åƒï¼‰çš„è§†è§‰è¡¨ç¤ºã€‚
- en: '![](../Images/7bbc0b12b588c6e781f128256fdcbb55.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7bbc0b12b588c6e781f128256fdcbb55.png)'
- en: 'U-Net (Source: [U-Net: Convolutional Networks for Biomedical Image Segmentation
    (Ronneberger, et. al)](https://arxiv.org/abs/1505.04597v1))'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 'U-Netï¼ˆæ¥æºï¼š[U-Net: å·ç§¯ç½‘ç»œç”¨äºç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆRonneberger ç­‰ï¼‰](https://arxiv.org/abs/1505.04597v1)ï¼‰'
- en: Similar to a [Variational Autoencoder (VAE)](/understanding-variational-autoencoders-vaes-f70510919f73),
    A U-Net consists of an *encoder* (the left side of the U) and a *decoder* (the
    right side of the U), connected by a *bottleneck* (the bottom layer of the U).
    Unlike a VAE, however, a U-Net hosts *skip connections* (shown by the horizontal
    gray arrows) that link the encoder to the decoder which is the key piece to producing
    high-resolution outputs. The encoder is responsible for capturing the [*features*](https://en.wikipedia.org/wiki/Feature_(machine_learning)),
    or characteristics, of the input audio signal while the decoder is responsible
    for the reconstruction of the signal.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼äº[å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰](/understanding-variational-autoencoders-vaes-f70510919f73)ï¼ŒU-Netç”±*ç¼–ç å™¨*ï¼ˆUçš„å·¦ä¾§ï¼‰å’Œ*è§£ç å™¨*ï¼ˆUçš„å³ä¾§ï¼‰ç»„æˆï¼Œé€šè¿‡*ç“¶é¢ˆ*ï¼ˆUçš„åº•éƒ¨å±‚ï¼‰ç›¸è¿æ¥ã€‚ç„¶è€Œï¼Œä¸VAEä¸åŒï¼ŒU-Netå…·æœ‰*è·³è·ƒè¿æ¥*ï¼ˆç”±æ°´å¹³ç°è‰²ç®­å¤´è¡¨ç¤ºï¼‰ï¼Œè¿™äº›è¿æ¥å°†ç¼–ç å™¨ä¸è§£ç å™¨è¿æ¥èµ·æ¥ï¼Œè¿™æ˜¯ç”Ÿæˆé«˜åˆ†è¾¨ç‡è¾“å‡ºçš„å…³é”®éƒ¨åˆ†ã€‚ç¼–ç å™¨è´Ÿè´£æ•æ‰è¾“å…¥éŸ³é¢‘ä¿¡å·çš„[*ç‰¹å¾*](https://en.wikipedia.org/wiki/Feature_(machine_learning))ï¼Œæˆ–ç‰¹æ€§ï¼Œè€Œè§£ç å™¨è´Ÿè´£ä¿¡å·çš„é‡å»ºã€‚
- en: To aid the visualization, picture the audio data entering the top left side
    of the U, following the red and blue arrows down the encoder to the bottleneck
    at the bottom, and then back up the decoder following the blue and green arrows
    to the top right of the U. Each blue rectangle represents a model *layer*. At
    each level of the encoder, the input audio signal is compressed further and further
    until it reaches a highly condensed representation of the sound at the base of
    the U (the bottleneck). The decoder then takes this compressed signal and effectively
    reverses the process to reconstruct the signal. Each layer (blue rectangle) the
    data passes through has a series of adjustable weights associated with it that
    can be thought of as millions of tiny knobs that can be turned to tweak this compression/reconstruction
    process. Having layers at different levels of compression allows the model to
    learn a range of features from the data, from large-scale features (e.g. melody
    and rhythm) to fine-grained details (e.g. high-frequency timbral characteristics).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å¸®åŠ©å¯è§†åŒ–ï¼Œå¯ä»¥æƒ³è±¡éŸ³é¢‘æ•°æ®ä»Uçš„å·¦ä¸Šæ–¹è¿›å…¥ï¼Œæ²¿ç€çº¢è‰²å’Œè“è‰²ç®­å¤´é€šè¿‡ç¼–ç å™¨å‘ä¸‹åˆ°è¾¾Uçš„åº•éƒ¨ç“¶é¢ˆå±‚ï¼Œç„¶åå†æ²¿ç€è“è‰²å’Œç»¿è‰²ç®­å¤´é€šè¿‡è§£ç å™¨å›åˆ°Uçš„å³ä¸Šæ–¹ã€‚æ¯ä¸ªè“è‰²çŸ©å½¢ä»£è¡¨ä¸€ä¸ªæ¨¡å‹*å±‚*ã€‚åœ¨ç¼–ç å™¨çš„æ¯ä¸€å±‚ä¸­ï¼Œè¾“å…¥éŸ³é¢‘ä¿¡å·ä¼šé€æ¸è¢«å‹ç¼©ï¼Œç›´åˆ°å®ƒåœ¨Uçš„åº•éƒ¨ï¼ˆç“¶é¢ˆå¤„ï¼‰è¾¾åˆ°é«˜åº¦æµ“ç¼©çš„å£°éŸ³è¡¨ç¤ºã€‚ç„¶åï¼Œè§£ç å™¨æ¥æ”¶è¿™ä¸ªå‹ç¼©ä¿¡å·ï¼Œå¹¶æœ‰æ•ˆåœ°é€†è½¬è¿™ä¸€è¿‡ç¨‹ä»¥é‡å»ºä¿¡å·ã€‚æ•°æ®é€šè¿‡çš„æ¯ä¸€å±‚ï¼ˆè“è‰²çŸ©å½¢ï¼‰éƒ½æœ‰ä¸€ç³»åˆ—å¯è°ƒçš„æƒé‡ï¼Œå¯ä»¥çœ‹ä½œæ˜¯æˆåƒä¸Šä¸‡çš„å¾®å°æ—‹é’®ï¼Œç”¨æˆ·å¯ä»¥æ—‹è½¬è¿™äº›æ—‹é’®æ¥è°ƒæ•´å‹ç¼©/é‡å»ºè¿‡ç¨‹ã€‚å…·æœ‰ä¸åŒå‹ç¼©çº§åˆ«çš„å±‚å…è®¸æ¨¡å‹ä»æ•°æ®ä¸­å­¦ä¹ å„ç§ç‰¹å¾ï¼Œä»å¤§å°ºåº¦çš„ç‰¹å¾ï¼ˆä¾‹å¦‚æ—‹å¾‹å’ŒèŠ‚å¥ï¼‰åˆ°ç»†ç²’åº¦çš„ç»†èŠ‚ï¼ˆä¾‹å¦‚é«˜é¢‘éŸ³è‰²ç‰¹å¾ï¼‰ã€‚
- en: Using an analogy, you can think of this full system as the process required
    to create an [MP3](https://en.wikipedia.org/wiki/MP3) audio file and then listen
    to that MP3 on a playback device. At its core, an MP3 is a compressed version
    of an audio signal. Imagine that the encoderâ€™s job is to create a new type of
    compressed audio format, just like an MP3, in order to consistently condense an
    audio signal as much as possible without losing fidelity. Then the decoderâ€™s job
    is to act like your iPhone (or any playback device) and reconstruct the MP3 into
    a high-fidelity audio representation that can be played through your headphones.
    The bottleneck can be thought of as this newly created MP3-type format, itself.
    **The U-Net represents the process of compression and reconstruction, not the
    audio data**. The model can then be trained with the goal of being able to accurately
    compress and reconstruct a wide range of audio signals.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ç±»æ¯”ï¼Œä½ å¯ä»¥å°†æ•´ä¸ªç³»ç»Ÿæƒ³è±¡æˆåˆ›å»ºä¸€ä¸ª[MP3](https://en.wikipedia.org/wiki/MP3)éŸ³é¢‘æ–‡ä»¶å¹¶åœ¨æ’­æ”¾è®¾å¤‡ä¸Šæ”¶å¬è¯¥MP3çš„è¿‡ç¨‹ã€‚ä»æœ¬è´¨ä¸Šè®²ï¼ŒMP3æ˜¯éŸ³é¢‘ä¿¡å·çš„å‹ç¼©ç‰ˆæœ¬ã€‚å‡è®¾ç¼–ç å™¨çš„å·¥ä½œæ˜¯åˆ›å»ºä¸€ç§æ–°çš„å‹ç¼©éŸ³é¢‘æ ¼å¼ï¼Œç±»ä¼¼äºMP3ï¼Œä»¥å°½å¯èƒ½åœ°å‹ç¼©éŸ³é¢‘ä¿¡å·è€Œä¸æŸå¤±ä¿çœŸåº¦ã€‚ç„¶åï¼Œè§£ç å™¨çš„å·¥ä½œå°±åƒä½ çš„iPhoneï¼ˆæˆ–ä»»ä½•æ’­æ”¾è®¾å¤‡ï¼‰ï¼Œå°†MP3è§£ç æˆå¯ä»¥é€šè¿‡è€³æœºæ’­æ”¾çš„é«˜ä¿çœŸéŸ³é¢‘è¡¨ç°å½¢å¼ã€‚ç“¶é¢ˆå¯ä»¥çœ‹ä½œæ˜¯è¿™ä¸ªæ–°åˆ›å»ºçš„MP3ç±»å‹æ ¼å¼æœ¬èº«ã€‚**U-Netä»£è¡¨çš„æ˜¯å‹ç¼©å’Œé‡å»ºçš„è¿‡ç¨‹ï¼Œè€Œä¸æ˜¯éŸ³é¢‘æ•°æ®**ã€‚ç„¶åï¼Œå¯ä»¥ä»¥å‡†ç¡®å‹ç¼©å’Œé‡å»ºå„ç§éŸ³é¢‘ä¿¡å·ä¸ºç›®æ ‡è®­ç»ƒè¿™ä¸ªæ¨¡å‹ã€‚
- en: This is all well and good, but we havenâ€™t generated anything yet. Weâ€™ve only
    constructed a way to compress and reconstruct an audio signal. However, this is
    the base process required to begin generating new audio, and it only requires
    a slight adjustment to do so.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€åˆ‡éƒ½å¾ˆå¥½ï¼Œä½†æˆ‘ä»¬è¿˜æ²¡æœ‰ç”Ÿæˆä»»ä½•å†…å®¹ã€‚æˆ‘ä»¬åªæ„å»ºäº†å‹ç¼©å’Œé‡å»ºéŸ³é¢‘ä¿¡å·çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™ä¸ªè¿‡ç¨‹æ˜¯ç”Ÿæˆæ–°éŸ³é¢‘æ‰€å¿…éœ€çš„åŸºæœ¬è¿‡ç¨‹ï¼Œè€Œä¸”åªéœ€ç¨å¾®è°ƒæ•´ä¸€ä¸‹å°±èƒ½å®ç°ã€‚
- en: Noising and Denoising
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å™ªå£°ä¸å»å™ª
- en: Let's revisit the idea of *noising* and *denoising* that we touched on earlier.
    Theoretically, we had some magic model that could be taught to take some white
    noise and â€œdenoiseâ€ it into recognizable audio, perhaps a beautiful concerto.
    A critical requirement of this magic model is that it must be able to reconstruct
    the input audio signal at high fidelity. Luckily, the U-Net architecture is designed
    to do exactly that. So the next piece of the puzzle is to modify the U-Net to
    perform this denoising process.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹æˆ‘ä»¬ä¹‹å‰æåˆ°çš„*å™ªå£°*å’Œ*å»å™ª*çš„æ¦‚å¿µã€‚ä»ç†è®ºä¸Šè®²ï¼Œæˆ‘ä»¬æ›¾è®¾æƒ³è¿‡ä¸€ä¸ªé­”æ³•æ¨¡å‹ï¼Œå®ƒå¯ä»¥è¢«è®­ç»ƒæ¥å°†ä¸€äº›ç™½å™ªå£°â€œå»å™ªâ€æˆå¯è¯†åˆ«çš„éŸ³é¢‘ï¼Œå¯èƒ½æ˜¯ä¸€é¦–ç¾ä¸½çš„åå¥æ›²ã€‚è¿™ä¸ªé­”æ³•æ¨¡å‹çš„ä¸€ä¸ªå…³é”®è¦æ±‚æ˜¯ï¼Œå®ƒå¿…é¡»èƒ½å¤Ÿä»¥é«˜ä¿çœŸåº¦é‡å»ºè¾“å…¥çš„éŸ³é¢‘ä¿¡å·ã€‚å¹¸è¿çš„æ˜¯ï¼ŒU-Netæ¶æ„çš„è®¾è®¡æ­£æ˜¯ä¸ºäº†å®Œæˆè¿™ä¸€ä»»åŠ¡ã€‚å› æ­¤ï¼Œæ¥ä¸‹æ¥è¦è§£å†³çš„éš¾é¢˜æ˜¯ä¿®æ”¹U-Netä»¥æ‰§è¡Œè¿™ä¸ªå»å™ªè¿‡ç¨‹ã€‚
- en: Counter-intuitively, to teach a model to denoise an audio signal, it is first
    taught how to add noise to a signal. Once it has learned this process, it inherently
    knows how to perform the inverse in order to denoise a signal.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: è¿åç›´è§‰çš„æ˜¯ï¼Œä¸ºäº†æ•™ä¼šä¸€ä¸ªæ¨¡å‹å»å™ªéŸ³é¢‘ä¿¡å·ï¼Œé¦–å…ˆè¦æ•™å®ƒå¦‚ä½•ç»™ä¿¡å·æ·»åŠ å™ªå£°ã€‚ä¸€æ—¦å®ƒå­¦ä¼šäº†è¿™ä¸ªè¿‡ç¨‹ï¼Œå®ƒå°±è‡ªç„¶çŸ¥é“å¦‚ä½•æ‰§è¡Œé€†æ“ä½œï¼Œä»¥å»é™¤å™ªå£°ã€‚
- en: Recall in the previous section that we detailed how the U-Net can learn to compress
    and reconstruct an audio signal. The noising process follows nearly the same formula,
    but instead of reconstructing the exact input audio signal, the U-Net is directed
    to reconstruct the input audio signal with a small amount of noise added to it.
    This can be visualized as reversing the steps taken in the earlier series of images
    of the puppy.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å›æƒ³ä¸€ä¸‹å‰ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬è¯¦ç»†æè¿°äº†U-Netå¦‚ä½•å­¦ä¹ å‹ç¼©å’Œé‡å»ºéŸ³é¢‘ä¿¡å·ã€‚å™ªå£°å¤„ç†è¿‡ç¨‹å‡ ä¹éµå¾ªç›¸åŒçš„å…¬å¼ï¼Œä½†ä¸åŒçš„æ˜¯ï¼ŒU-Netå¹¶ä¸æ˜¯é‡å»ºå®Œå…¨ç›¸åŒçš„è¾“å…¥éŸ³é¢‘ä¿¡å·ï¼Œè€Œæ˜¯è¢«æŒ‡å¯¼é‡å»ºåŠ å…¥å°‘é‡å™ªå£°çš„è¾“å…¥éŸ³é¢‘ä¿¡å·ã€‚è¿™å¯ä»¥é€šè¿‡åè½¬ä¹‹å‰å°ç‹—å›¾åƒåºåˆ—ä¸­çš„æ­¥éª¤æ¥å¯è§†åŒ–ã€‚
- en: '![](../Images/9e223a528f9d9406258a26a766f85b89.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e223a528f9d9406258a26a766f85b89.png)'
- en: Diffusion Noising Steps (Image generated with Stable Diffusion)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰©æ•£å™ªå£°æ­¥éª¤ï¼ˆå›¾åƒç”±Stable Diffusionç”Ÿæˆï¼‰
- en: The process of adding noise to a signal must be probabilistic (i.e. predictable).
    The model is shown an audio signal and then instructed to predict the same signal
    with a small amount of [Gaussian noise](https://en.wikipedia.org/wiki/Gaussian_noise#:~:text=In%20signal%20processing%20theory%2C%20Gaussian,can%20take%20are%20Gaussian%2Ddistributed.)
    added to it. Because of its properties, Gaussian noise is most commonly used but
    it is not required. The noise must be defined by probabilistic distribution, meaning
    that it follows a specific pattern that is consistently predictable. This process
    of instructing the model to add small amounts of predictable noise to the audio
    signal is repeated for a number of *steps* until the signal has effectively become
    just noise.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å‘ä¿¡å·æ·»åŠ å™ªå£°çš„è¿‡ç¨‹å¿…é¡»æ˜¯æ¦‚ç‡æ€§çš„ï¼ˆå³å¯é¢„æµ‹çš„ï¼‰ã€‚æ¨¡å‹é¦–å…ˆå±•ç¤ºç»™ä¸€ä¸ªéŸ³é¢‘ä¿¡å·ï¼Œç„¶åè¢«æŒ‡ç¤ºé¢„æµ‹æ·»åŠ å°‘é‡[é«˜æ–¯å™ªå£°](https://en.wikipedia.org/wiki/Gaussian_noise#:~:text=In%20signal%20processing%20theory%2C%20Gaussian,can%20take%20are%20Gaussian%2Ddistributed.)åçš„ä¿¡å·ã€‚ç”±äºå…¶ç‰¹æ€§ï¼Œé«˜æ–¯å™ªå£°æœ€ä¸ºå¸¸è§ï¼Œä½†å¹¶éå¿…é¡»ä½¿ç”¨ã€‚å™ªå£°å¿…é¡»ç”±æ¦‚ç‡åˆ†å¸ƒå®šä¹‰ï¼Œæ„å‘³ç€å®ƒéµå¾ªä¸€ä¸ªç‰¹å®šçš„æ¨¡å¼ï¼Œä¸”è¿™ä¸ªæ¨¡å¼æ˜¯å¯ä»¥ä¸€è‡´é¢„æµ‹çš„ã€‚è¿™ä¸ªè¿‡ç¨‹ä¼šåœ¨å¤šä¸ª*æ­¥éª¤*ä¸­é‡å¤ï¼Œç›´åˆ°ä¿¡å·æœ€ç»ˆå˜æˆåªæœ‰å™ªå£°ã€‚
- en: '![](../Images/c04fe7144cc50b5a0c8ba8149e95d410.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c04fe7144cc50b5a0c8ba8149e95d410.png)'
- en: 'Noising a Snare Sample (Source: [CRASH: Raw Audio Score-based Generative Modeling
    for Controllable High-resolution Drum Sound Synthesis (Rouard, Hadjeres)](https://github.com/simonrouard/CRASH))'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ·»åŠ å™ªå£°åˆ°å‡»é¼“æ ·æœ¬ï¼ˆæ¥æºï¼š[CRASH: åŸºäºéŸ³é¢‘åˆ†æ•°çš„å¯æ§é«˜åˆ†è¾¨ç‡é¼“å£°åˆæˆçš„åŸå§‹éŸ³é¢‘ç”Ÿæˆæ¨¡å‹ (Rouard, Hadjeres)](https://github.com/simonrouard/CRASH)ï¼‰'
- en: For example, letâ€™s take a one-shot sample of a snare drum. The U-Net is provided
    this snare sample and it is asked to reconstruct that snare sound, but with a
    little noise added making it sound a little less clean. Then this slightly noisy
    snare sample is provided to the model, and it is again instructed to reconstruct
    this snare sample with even more noise. This cycle is repeated until it sounds
    as if the snare sample no longer exists, rather only white noise remains. The
    model is then taught how to do this for a wide range of sounds. Once it becomes
    an expert at predicting how to add noise to an input audio signal, because the
    process is probabilistic, it can simply be reversed so that at each step a little
    noise is removed. This is how the model can generate a snare sample when provided
    with white noise.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬ä»¥ä¸€ä¸ªå‡»é¼“æ ·æœ¬ä¸ºä¾‹ã€‚U-Net æ¥æ”¶åˆ°è¿™ä¸ªå‡»é¼“æ ·æœ¬ï¼Œå¹¶è¢«è¦æ±‚é‡å»ºè¿™ä¸ªå‡»é¼“å£°éŸ³ï¼Œä½†åŠ å…¥ä¸€äº›å™ªå£°ï¼Œä½¿å…¶å¬èµ·æ¥ä¸é‚£ä¹ˆå¹²å‡€ã€‚ç„¶åï¼Œè¿™ä¸ªç•¥å¸¦å™ªå£°çš„å‡»é¼“æ ·æœ¬è¢«å†æ¬¡æä¾›ç»™æ¨¡å‹ï¼Œå¹¶å†æ¬¡è¦æ±‚é‡å»ºè¿™ä¸ªå‡»é¼“æ ·æœ¬ï¼ŒåŒæ—¶å¢åŠ æ›´å¤šå™ªå£°ã€‚è¿™ä¸ªå¾ªç¯ä¼šé‡å¤è¿›è¡Œï¼Œç›´åˆ°å¬èµ·æ¥åƒæ˜¯å‡»é¼“æ ·æœ¬å·²ç»ä¸å­˜åœ¨ï¼Œåªå‰©ä¸‹ç™½å™ªå£°ã€‚æ¥ç€ï¼Œæ¨¡å‹è¢«æ•™ä¼šå¦‚ä½•åœ¨å¹¿æ³›çš„å£°éŸ³ä¸­æ‰§è¡Œè¿™ç§æ“ä½œã€‚ä¸€æ—¦å®ƒæˆä¸ºé¢„æµ‹å¦‚ä½•å‘è¾“å…¥éŸ³é¢‘ä¿¡å·æ·»åŠ å™ªå£°çš„ä¸“å®¶ï¼Œå› ä¸ºè¿™ä¸ªè¿‡ç¨‹æ˜¯æ¦‚ç‡æ€§çš„ï¼Œå®ƒå°±å¯ä»¥ç®€å•åœ°åè½¬ï¼Œä½¿å¾—åœ¨æ¯ä¸€æ­¥ç§»é™¤ä¸€äº›å™ªå£°ã€‚è¿™å°±æ˜¯æ¨¡å‹åœ¨æä¾›ç™½å™ªå£°æ—¶èƒ½å¤Ÿç”Ÿæˆå‡»é¼“æ ·æœ¬çš„æ–¹å¼ã€‚
- en: '**Because of the probabilistic nature of this process, some incredible capabilities
    arise, specifically the ability to simulate creativity.**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç”±äºè¿™ä¸ªè¿‡ç¨‹çš„æ¦‚ç‡æ€§ç‰¹å¾ï¼Œä¸€äº›ä»¤äººéš¾ä»¥ç½®ä¿¡çš„èƒ½åŠ›å‡ºç°äº†ï¼Œç‰¹åˆ«æ˜¯æ¨¡æ‹Ÿåˆ›é€ åŠ›çš„èƒ½åŠ›ã€‚**'
- en: Letâ€™s continue with our snare example. Imagine the model was trained on thousands
    of one-shot snare samples. You would imagine that it could take some white noise
    and then turn it into any one of these snare samples. However, that is not exactly
    how the model learns. Because it is shown such a wide range of sounds, it instead
    learns to create sounds that are generally similar to any of the snares that it
    has been trained on, but not exactly. This is how brand new sounds are created
    and these models appear to exhibit a spark of creativity.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç»§ç»­è®¨è®ºå‡»é¼“çš„ä¾‹å­ã€‚å‡è®¾æ¨¡å‹å·²ç»åœ¨æˆåƒä¸Šä¸‡ä¸ªå•æ¬¡å‡»é¼“æ ·æœ¬ä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚ä½ å¯èƒ½ä¼šè®¤ä¸ºå®ƒå¯ä»¥æ‹¿ä¸€äº›ç™½å™ªå£°ï¼Œç„¶åå°†å…¶è½¬åŒ–ä¸ºä»»ä½•ä¸€ä¸ªè¿™äº›å‡»é¼“æ ·æœ¬ã€‚ç„¶è€Œï¼Œæ¨¡å‹çš„å­¦ä¹ æ–¹å¼å¹¶ä¸å®Œå…¨æ˜¯è¿™æ ·ã€‚ç”±äºå®ƒè¢«å±•ç¤ºäº†å¦‚æ­¤å¹¿æ³›çš„å£°éŸ³èŒƒå›´ï¼Œå®ƒåè€Œå­¦ä¼šäº†åˆ›å»ºé‚£äº›ä¸å®ƒè®­ç»ƒè¿‡çš„å‡»é¼“æ ·æœ¬å¤§è‡´ç›¸ä¼¼çš„å£°éŸ³ï¼Œä½†å¹¶ä¸å®Œå…¨ç›¸åŒã€‚è¿™å°±æ˜¯å¦‚ä½•åˆ›é€ å…¨æ–°å£°éŸ³çš„è¿‡ç¨‹ï¼Œè¿™äº›æ¨¡å‹çœ‹èµ·æ¥å±•ç°å‡ºäº†æŸç§åˆ›é€ åŠ›çš„ç«èŠ±ã€‚
- en: To illustrate this, letâ€™s use the following sketch.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¯´æ˜è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹è‰å›¾ã€‚
- en: '![](../Images/b4ad00a6c897be31a523510cd5e747c1.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4ad00a6c897be31a523510cd5e747c1.png)'
- en: Pretend that all possible sounds, from guitar strums to dog barks to white noise,
    can be plotted on a 2-dimensional plane represented by the black rectangle in
    the image above. Within this space, there is a region where snare hits exist.
    They are somewhat grouped together because of their similar timbral and transient
    characteristics. This is shown by the blue blob and each blue dot is representative
    of a single snare sample that we trained our model on. The red dots represent
    the fully noised versions of the snares the model was trained on and correspond
    to their un-noised blue dot counterparts.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æ‰€æœ‰å¯èƒ½çš„å£°éŸ³ï¼Œä»å‰ä»–å¼¹å¥åˆ°ç‹—å å£°ï¼Œå†åˆ°ç™½å™ªå£°ï¼Œéƒ½å¯ä»¥ç»˜åˆ¶åœ¨ä¸€ä¸ªäºŒç»´å¹³é¢ä¸Šï¼Œå¹³é¢ç”±ä¸Šå›¾ä¸­çš„é»‘è‰²çŸ©å½¢è¡¨ç¤ºã€‚åœ¨è¿™ä¸ªç©ºé—´ä¸­ï¼Œæœ‰ä¸€ä¸ªåŒºåŸŸæ˜¯å°å†›é¼“å‡»æ‰“å£°æ‰€åœ¨çš„ä½ç½®ã€‚ç”±äºå®ƒä»¬åœ¨éŸ³è‰²å’Œç¬æ€ç‰¹æ€§ä¸Šçš„ç›¸ä¼¼æ€§ï¼Œå®ƒä»¬è¢«ç¨å¾®èšé›†åœ¨ä¸€èµ·ã€‚è¿™ç”±è“è‰²çš„æ–‘ç‚¹æ˜¾ç¤ºï¼Œæ¯ä¸€ä¸ªè“è‰²çš„ç‚¹ä»£è¡¨æˆ‘ä»¬ç”¨æ¥è®­ç»ƒæ¨¡å‹çš„ä¸€ä¸ªå°å†›é¼“æ ·æœ¬ã€‚çº¢è‰²çš„ç‚¹ä»£è¡¨æ¨¡å‹è®­ç»ƒæ—¶ä½¿ç”¨çš„å·²ç»åŠ å…¥å™ªå£°çš„å°å†›é¼“ç‰ˆæœ¬ï¼Œå¹¶ä¸å®ƒä»¬æœªåŠ å…¥å™ªå£°çš„è“è‰²ç‚¹æ ·æœ¬ç›¸å¯¹åº”ã€‚
- en: In essence, our model learned to take dots from the â€œnot snareâ€ region and bring
    them into the â€œsnareâ€ region. So if we take a new green dot in the â€œnot snareâ€
    region (e.g. random noise) that does not correspond to any blue dot, and ask our
    model to bring it into the â€œsnareâ€ region, it will bring it to a new location
    within that â€œsnareâ€ region. This is the model generating a â€œnewâ€ snare sample
    that contains some similarities to all other snares it was trained on in the snare
    region, but also some new unknown characteristics.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬è´¨ä¸Šï¼Œæˆ‘ä»¬çš„æ¨¡å‹å­¦ä¼šäº†å°†â€œéå°å†›é¼“â€åŒºåŸŸçš„ç‚¹å¸¦å…¥â€œå†›é¼“â€åŒºåŸŸã€‚æ‰€ä»¥ï¼Œå¦‚æœæˆ‘ä»¬ä»â€œéå°å†›é¼“â€åŒºåŸŸï¼ˆä¾‹å¦‚éšæœºå™ªå£°ï¼‰é€‰å–ä¸€ä¸ªæ–°çš„ç»¿è‰²ç‚¹ï¼Œå®ƒä¸ä»»ä½•è“è‰²ç‚¹éƒ½ä¸å¯¹åº”ï¼Œå¹¶è¦æ±‚æˆ‘ä»¬çš„æ¨¡å‹å°†å…¶å¸¦å…¥â€œå†›é¼“â€åŒºåŸŸï¼Œæ¨¡å‹å°†æŠŠå®ƒå¸¦åˆ°â€œå†›é¼“â€åŒºåŸŸå†…çš„ä¸€ä¸ªæ–°ä½ç½®ã€‚è¿™å°±æ˜¯æ¨¡å‹ç”Ÿæˆä¸€ä¸ªâ€œæ–°çš„â€å†›é¼“æ ·æœ¬ï¼Œè™½ç„¶å®ƒä¸æ¨¡å‹è®­ç»ƒæ—¶çš„æ‰€æœ‰å†›é¼“æ ·æœ¬æœ‰ç›¸ä¼¼ä¹‹å¤„ï¼Œä½†ä¹ŸåŒ…å«ä¸€äº›æ–°çš„ã€æœªçŸ¥çš„ç‰¹å¾ã€‚
- en: This concept can be applied to any type of sound, including full songs. This
    is an incredible innovation that can lead to numerous new ways to create. It is
    important to understand that these models will not create something outside of
    the bounds of how they are trained, however. As shown in the previous illustration,
    while our conceptual model can take in any type of sound, it can only produce
    snare samples similar to those it was trained on. This holds true for any of these
    audio diffusion models. Because of this, it is critical to train models on extensive
    datasets so the known regions (like the snare region) are sufficiently diverse
    and large enough to not simply copy the training data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€æ¦‚å¿µå¯ä»¥åº”ç”¨äºä»»ä½•ç±»å‹çš„å£°éŸ³ï¼ŒåŒ…æ‹¬å®Œæ•´çš„æ­Œæ›²ã€‚è¿™æ˜¯ä¸€ä¸ªä»¤äººæƒŠå¹çš„åˆ›æ–°ï¼Œèƒ½å¤Ÿå¼•é¢†åˆ›ä½œæ–¹å¼çš„å¤šç§å˜åŒ–ã€‚ç„¶è€Œï¼Œé‡è¦çš„æ˜¯è¦ç†è§£ï¼Œè¿™äº›æ¨¡å‹ä¸ä¼šç”Ÿæˆè¶…å‡ºå®ƒä»¬è®­ç»ƒèŒƒå›´çš„å†…å®¹ã€‚å¦‚å‰å›¾æ‰€ç¤ºï¼Œå°½ç®¡æˆ‘ä»¬çš„æ¦‚å¿µæ¨¡å‹å¯ä»¥å¤„ç†ä»»ä½•ç±»å‹çš„å£°éŸ³ï¼Œä½†å®ƒåªèƒ½ç”Ÿæˆç±»ä¼¼äºè®­ç»ƒæ ·æœ¬çš„å°å†›é¼“æ ·æœ¬ã€‚æ‰€æœ‰è¿™äº›éŸ³é¢‘æ‰©æ•£æ¨¡å‹éƒ½éµå¾ªè¿™ä¸€åŸåˆ™ã€‚å› æ­¤ï¼Œè®­ç»ƒæ¨¡å‹æ—¶ä½¿ç”¨å¹¿æ³›çš„æ•°æ®é›†è‡³å…³é‡è¦ï¼Œä»¥ç¡®ä¿å·²çŸ¥åŒºåŸŸï¼ˆå¦‚å°å†›é¼“åŒºåŸŸï¼‰è¶³å¤Ÿå¤šæ ·åŒ–ä¸”è§„æ¨¡è¶³å¤Ÿå¤§ï¼Œä»è€Œé¿å…ä»…ä»…å¤åˆ¶è®­ç»ƒæ•°æ®ã€‚
- en: '**All of this means that no model can replicate human creativity, just simulate
    variations of it.**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¿™ä¸€åˆ‡æ„å‘³ç€æ²¡æœ‰ä»»ä½•æ¨¡å‹èƒ½å¤Ÿå¤åˆ¶äººç±»çš„åˆ›é€ åŠ›ï¼Œåªèƒ½æ¨¡æ‹Ÿå®ƒçš„å˜ä½“ã€‚**'
- en: Applications of Diffusion Models
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‰©æ•£æ¨¡å‹çš„åº”ç”¨
- en: 'These models will not magically generate new genres or explore unknown sonic
    landscapes as humans do. With this understanding, these generative models should
    not be viewed as a replacement for human creativity, but rather as tools that
    can enhance creativity. Below are just a few ways that this technology can be
    leveraged for creative means:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ¨¡å‹å¹¶ä¸ä¼šåƒäººç±»é‚£æ ·ç¥å¥‡åœ°ç”Ÿæˆæ–°çš„éŸ³ä¹é£æ ¼æˆ–æ¢ç´¢æœªçŸ¥çš„å£°éŸ³æ™¯è§‚ã€‚ç†è§£è¿™ä¸€ç‚¹åï¼Œæˆ‘ä»¬åº”è¯¥å°†è¿™äº›ç”Ÿæˆæ¨¡å‹è§†ä¸ºå¢å¼ºåˆ›æ„çš„å·¥å…·ï¼Œè€Œä¸æ˜¯æ›¿ä»£äººç±»åˆ›æ„çš„æ›¿ä»£å“ã€‚ä»¥ä¸‹æ˜¯è¿™é¡¹æŠ€æœ¯åœ¨åˆ›ä½œä¸­åº”ç”¨çš„å‡ ç§æ–¹å¼ï¼š
- en: '**Creativity Through Curation**: Searching through sample packs to find a desired
    sound is a common practice in production. These models can effectively be used
    as a version of an â€œunlimited sample packâ€, enhancing an artistâ€™s creativity through
    the curation of sounds.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é€šè¿‡ç­–å±•æ¿€å‘åˆ›é€ åŠ›**ï¼šåœ¨é‡‡æ ·åŒ…ä¸­æœç´¢ä»¥æ‰¾åˆ°æ‰€éœ€çš„å£°éŸ³æ˜¯åˆ¶ä½œè¿‡ç¨‹ä¸­å¸¸è§çš„åšæ³•ã€‚è¿™äº›æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°ä½œä¸ºâ€œæ— é™é‡‡æ ·åŒ…â€çš„ä¸€ç§å½¢å¼ï¼Œé€šè¿‡å£°éŸ³çš„ç­–å±•æ¥å¢å¼ºè‰ºæœ¯å®¶çš„åˆ›é€ åŠ›ã€‚'
- en: '**Voice Transfer**: Just like how diffusion models can take random noise and
    change it into recognizable audio, they can also be fed other sounds and â€œtransferâ€
    them to another type of sound. If we take our previous snare model, for example,
    and feed it a kick drum sample instead of white noise, it will take the kick sample
    and begin to morph it into a snare sound. This allows for very unique creations,
    being able to combine the characteristics of multiple different sounds.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å£°éŸ³è½¬ç§»ï¼š** å°±åƒæ‰©æ•£æ¨¡å‹å¯ä»¥å°†éšæœºå™ªå£°è½¬åŒ–ä¸ºå¯è¯†åˆ«çš„éŸ³é¢‘ä¸€æ ·ï¼Œå®ƒä»¬ä¹Ÿå¯ä»¥æ¥æ”¶å…¶ä»–å£°éŸ³å¹¶å°†å…¶â€œè½¬ç§»â€åˆ°å¦ä¸€ç§ç±»å‹çš„å£°éŸ³ä¸Šã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨ä¹‹å‰çš„å†›é¼“æ¨¡å‹ï¼Œå¹¶è¾“å…¥ä¸€ä¸ªè¸¢é¼“æ ·æœ¬è€Œä¸æ˜¯ç™½å™ªå£°ï¼Œå®ƒä¼šå°†è¸¢é¼“æ ·æœ¬è½¬å˜æˆå†›é¼“å£°éŸ³ã€‚è¿™ä½¿å¾—éå¸¸ç‹¬ç‰¹çš„åˆ›ä½œæˆä¸ºå¯èƒ½ï¼Œèƒ½å¤Ÿç»“åˆå¤šç§ä¸åŒå£°éŸ³çš„ç‰¹å¾ã€‚'
- en: '**Sound Variability (Humanization)**: When humans play a live instrument, such
    as a hi-hat on a drum set, there is always inherent variability in each hit. Various
    virtual instruments have attempted to simulate this via a number of different
    methods, but can still sound artificial and lack character. Audio diffusion allows
    for the unlimited variation of a single sound, which can add a human element to
    an audio sample. For example, if you program a drum kit, audio diffusion can be
    leveraged so that each hit is slightly different in timbre, velocity, attack,
    etc. to humanize what might sound like a stale performance.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å£°éŸ³å˜å¼‚æ€§ï¼ˆäººæ€§åŒ–ï¼‰ï¼š** å½“äººç±»æ¼”å¥ç°åœºä¹å™¨æ—¶ï¼Œä¾‹å¦‚é¼“ç»„ä¸­çš„é«˜å¸½ï¼Œæ¯ä¸€æ¬¡å‡»æ‰“éƒ½ä¼šæœ‰å›ºæœ‰çš„å˜å¼‚æ€§ã€‚å„ç§è™šæ‹Ÿä¹å™¨å°è¯•é€šè¿‡ä¸åŒçš„æ–¹æ³•æ¨¡æ‹Ÿè¿™ä¸€ç°è±¡ï¼Œä½†ä»ç„¶å¯èƒ½å¬èµ·æ¥ä¸è‡ªç„¶ï¼Œç¼ºä¹ä¸ªæ€§ã€‚éŸ³é¢‘æ‰©æ•£å¯ä»¥å®ç°å•ä¸€å£°éŸ³çš„æ— é™å˜åŒ–ï¼Œä»è€Œä¸ºéŸ³é¢‘æ ·æœ¬æ·»åŠ äººæ€§åŒ–å…ƒç´ ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ ç¼–ç¨‹ä¸€ä¸ªé¼“ç»„ï¼ŒéŸ³é¢‘æ‰©æ•£å¯ä»¥ç”¨æ¥è®©æ¯æ¬¡å‡»æ‰“åœ¨éŸ³è‰²ã€åŠ›åº¦ã€èµ·éŸ³ç­‰æ–¹é¢ç•¥æœ‰ä¸åŒï¼Œä»è€Œä½¿åŸæœ¬å¯èƒ½æ˜¾å¾—å‘†æ¿çš„æ¼”å¥æ›´å…·äººæ€§åŒ–ã€‚'
- en: '**Sound Design Adjustments**: Similar to the human variability potential, this
    concept can also be applied to sound design to create slight changes to a sound.
    Perhaps you mostly like the sound of a door slam sample, but you wish that it
    had more body or crunch. A diffusion model can take this sample and slightly change
    it to maintain most of its characteristics while taking on a few new ones. This
    can add, remove, or change the spectral content of a sound at a more fundamental
    level than applying an EQ or filter.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å£°éŸ³è®¾è®¡è°ƒæ•´ï¼š** ç±»ä¼¼äºäººç±»çš„å˜å¼‚æ€§æ½œåŠ›ï¼Œè¿™ä¸€æ¦‚å¿µä¹Ÿå¯ä»¥åº”ç”¨äºå£°éŸ³è®¾è®¡ï¼Œåˆ›é€ å¯¹å£°éŸ³çš„è½»å¾®å˜åŒ–ã€‚ä¹Ÿè®¸ä½ å¤§å¤šå–œæ¬¢é—¨ç °çš„ä¸€å£°æ ·æœ¬ï¼Œä½†å¸Œæœ›å®ƒæœ‰æ›´å¤šçš„è´¨æ„Ÿæˆ–è„†å“ã€‚æ‰©æ•£æ¨¡å‹å¯ä»¥åˆ©ç”¨è¿™ä¸ªæ ·æœ¬å¹¶å¯¹å…¶è¿›è¡Œå¾®è°ƒï¼Œä¿æŒå¤§éƒ¨åˆ†ç‰¹å¾ï¼ŒåŒæ—¶åŠ å…¥ä¸€äº›æ–°çš„ç‰¹å¾ã€‚è¿™å¯ä»¥åœ¨æ¯”ä½¿ç”¨å‡è¡¡å™¨æˆ–æ»¤æ³¢å™¨æ›´åŸºç¡€çš„å±‚é¢ä¸Šï¼Œæ·»åŠ ã€å»é™¤æˆ–æ”¹å˜å£°éŸ³çš„é¢‘è°±å†…å®¹ã€‚'
- en: '**Melody Generation:** Similar to surfing through sample packs, audio diffusion
    models can generate melodies that can spark ideas to build on.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ—‹å¾‹ç”Ÿæˆï¼š** ç±»ä¼¼äºæµè§ˆæ ·æœ¬åŒ…ï¼ŒéŸ³é¢‘æ‰©æ•£æ¨¡å‹å¯ä»¥ç”Ÿæˆæ—‹å¾‹ï¼Œæ¿€å‘å‡ºå¯ä¾›è¿›ä¸€æ­¥åˆ›ä½œçš„çµæ„Ÿã€‚'
- en: '**Stereo Effect**: There are several different mixing tricks to add stereo
    width to a single-channel (mono) sound. However, they can often add undesired
    coloration, delay, or phase shifts. Audio diffusion can be leveraged to generate
    a sound nearly identical to the mono sound, but different enough in its content
    to expand the stereo width while avoiding many of the unwanted phenomena.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç«‹ä½“å£°æ•ˆæœï¼š** æœ‰å¤šç§ä¸åŒçš„æ··éŸ³æŠ€å·§å¯ä»¥ä¸ºå•å£°é“ï¼ˆå•å£°é“ï¼‰å£°éŸ³æ·»åŠ ç«‹ä½“å£°å®½åº¦ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾€å¾€ä¼šå¸¦æ¥ä¸å¿…è¦çš„è‰²å½©ã€å»¶è¿Ÿæˆ–ç›¸ä½åç§»ã€‚éŸ³é¢‘æ‰©æ•£å¯ä»¥ç”¨æ¥ç”Ÿæˆå‡ ä¹ä¸å•å£°é“å£°éŸ³ç›¸åŒçš„å£°éŸ³ï¼Œä½†å…¶å†…å®¹è¶³å¤Ÿä¸åŒï¼Œä»¥æ‰©å±•ç«‹ä½“å£°å®½åº¦ï¼ŒåŒæ—¶é¿å…è®¸å¤šä¸å¸Œæœ›å‡ºç°çš„ç°è±¡ã€‚'
- en: '**Super Resolution:** Audio diffusion models can enhance the resolution and
    quality of audio recordings, making them clearer and more detailed. This can be
    particularly useful in audio restoration or when working with low-quality recordings.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è¶…åˆ†è¾¨ç‡ï¼š** éŸ³é¢‘æ‰©æ•£æ¨¡å‹å¯ä»¥å¢å¼ºéŸ³é¢‘å½•éŸ³çš„åˆ†è¾¨ç‡å’Œè´¨é‡ï¼Œä½¿å…¶æ›´åŠ æ¸…æ™°å’Œè¯¦ç»†ã€‚è¿™åœ¨éŸ³é¢‘ä¿®å¤æˆ–å¤„ç†ä½è´¨é‡å½•éŸ³æ—¶å°¤ä¸ºæœ‰ç”¨ã€‚'
- en: '**Inpainting:** Diffusion models can be leveraged to fill in missing or corrupted
    parts of audio signals, restoring them to their original or improved state. This
    is valuable for repairing damaged audio recordings, completing sections of audio
    that may be missing, or adding transitions between audio clips.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å›¾åƒä¿®å¤ï¼š** æ‰©æ•£æ¨¡å‹å¯ä»¥ç”¨æ¥å¡«è¡¥éŸ³é¢‘ä¿¡å·ä¸­ç¼ºå¤±æˆ–æŸåçš„éƒ¨åˆ†ï¼Œå°†å…¶æ¢å¤åˆ°åŸå§‹æˆ–æ”¹è¿›åçš„çŠ¶æ€ã€‚è¿™å¯¹äºä¿®å¤æŸåçš„éŸ³é¢‘å½•éŸ³ã€å®Œæˆå¯èƒ½ç¼ºå¤±çš„éŸ³é¢‘ç‰‡æ®µæˆ–åœ¨éŸ³é¢‘å‰ªè¾‘ä¹‹é—´æ·»åŠ è¿‡æ¸¡éå¸¸æœ‰ä»·å€¼ã€‚'
- en: Conclusion
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: There is no doubt that these new generative AI models are incredible technological
    advancements, independent of whether they are viewed in a positive or negative
    light. There are many more aspects to diffusion models that can optimize their
    performance regarding speed, diversity, and quality, but we have discussed the
    base principles that govern the functionality of these models. This knowledge
    provides a deeper context into what it really means when these models are generating
    â€œnew soundsâ€.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯«æ— ç–‘é—®ï¼Œè¿™äº›æ–°çš„ç”Ÿæˆå‹äººå·¥æ™ºèƒ½æ¨¡å‹æ˜¯ä»¤äººæƒŠå¹çš„æŠ€æœ¯è¿›æ­¥ï¼Œä¸è®ºå®ƒä»¬è¢«è§†ä¸ºç§¯æè¿˜æ˜¯æ¶ˆæçš„ã€‚å…³äºæ‰©æ•£æ¨¡å‹çš„ä¼˜åŒ–ç©ºé—´éå¸¸å¹¿æ³›ï¼Œå¯ä»¥åœ¨é€Ÿåº¦ã€å¤šæ ·æ€§å’Œè´¨é‡ç­‰æ–¹é¢æå‡å®ƒä»¬çš„æ€§èƒ½ï¼Œä½†æˆ‘ä»¬å·²ç»è®¨è®ºäº†è¿™äº›æ¨¡å‹åŠŸèƒ½çš„åŸºæœ¬åŸç†ã€‚è¿™äº›çŸ¥è¯†ä¸ºæˆ‘ä»¬æä¾›äº†æ›´æ·±åˆ»çš„èƒŒæ™¯ï¼Œè®©æˆ‘ä»¬ç†è§£å½“è¿™äº›æ¨¡å‹ç”Ÿæˆâ€œæ–°å£°éŸ³â€æ—¶ï¼ŒçœŸæ­£æ„å‘³ç€ä»€ä¹ˆã€‚
- en: On a broader level, it is not only the music, itself, that people care about
    â€” it is the human element in the creation of that music. Ask yourself, if you
    were to hear a recording of a virtuosic lightning-fast guitar solo, would you
    be impressed? It all depends. If it was artificially generated by a virtual MIDI
    instrument programmed by a producer, you will likely be unphased and may not even
    like how it sounds. However, if you know an actual guitarist played the solo on
    a real guitar, or even saw him or her do it, you will be completely enamored by
    their expertise and precision. We are drawn to the deftness in a performance,
    the thoughts and emotions behind lyrics, and the considerations that go into each
    decision when crafting a song.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ›´å¹¿æ³›çš„å±‚é¢æ¥çœ‹ï¼Œäººä»¬å…³å¿ƒçš„å¹¶ä¸ä»…ä»…æ˜¯éŸ³ä¹æœ¬èº«â€”â€”æ›´é‡è¦çš„æ˜¯éŸ³ä¹åˆ›ä½œä¸­çš„äººç±»å…ƒç´ ã€‚é—®é—®è‡ªå·±ï¼Œå¦‚æœä½ å¬åˆ°ä¸€æ®µé«˜è¶…ä¸”è¿…é€Ÿçš„å‰ä»–ç‹¬å¥å½•éŸ³ï¼Œä½ ä¼šæ„Ÿåˆ°å°è±¡æ·±åˆ»å—ï¼Ÿè¿™å–å†³äºæƒ…å†µã€‚å¦‚æœè¿™æ®µç‹¬å¥æ˜¯ç”±ä¸€ä¸ªåˆ¶ä½œäººç¼–ç¨‹çš„è™šæ‹ŸMIDIä¹å™¨äººå·¥ç”Ÿæˆçš„ï¼Œä½ å¯èƒ½ä¼šæ¯«ä¸åŠ¨å®¹ï¼Œç”šè‡³ä¸å–œæ¬¢å®ƒçš„å£°éŸ³ã€‚ç„¶è€Œï¼Œå¦‚æœä½ çŸ¥é“è¿™æ®µç‹¬å¥æ˜¯ç”±ä¸€ä½å‰ä»–æ‰‹ç”¨çœŸå®å‰ä»–æ¼”å¥çš„ï¼Œæˆ–è€…ç”šè‡³äº²çœ¼çœ‹åˆ°ä»–æˆ–å¥¹æ¼”å¥ï¼Œä½ å°†å®Œå…¨è¢«ä»–ä»¬çš„ä¸“ä¸šæŠ€å·§å’Œç²¾å‡†åº¦æ‰€å¸å¼•ã€‚æˆ‘ä»¬è¢«æ¼”å¥ä¸­çš„çµå·§ã€æ­Œè¯èƒŒåçš„æ€æƒ³å’Œæƒ…æ„Ÿï¼Œä»¥åŠåˆ›ä½œæ­Œæ›²æ—¶æ¯ä¸ªå†³å®šèƒŒåçš„è€ƒè™‘æ‰€å¸å¼•ã€‚
- en: While these incredible advancements have led to some existential dread for artists
    and producers, AI can never take that human element away from the sounds and music
    that we create. So we should approach these new advancements with the intent that
    they are tools for enhancing artistsâ€™ creativity rather than replacing it.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡è¿™äº›ä»¤äººéš¾ä»¥ç½®ä¿¡çš„è¿›æ­¥è®©è‰ºæœ¯å®¶å’Œåˆ¶ä½œäººæ„Ÿåˆ°ä¸€äº›ç”Ÿå­˜ç„¦è™‘ï¼Œä½†äººå·¥æ™ºèƒ½æ°¸è¿œæ— æ³•å‰¥å¤ºæˆ‘ä»¬åˆ›ä½œçš„å£°éŸ³å’ŒéŸ³ä¹ä¸­é‚£ä»½äººç±»å…ƒç´ ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åº”è¯¥ä»¥ä¸€ç§å·¥å…·çš„å¿ƒæ€æ¥çœ‹å¾…è¿™äº›æ–°çš„è¿›å±•ï¼Œè®¤ä¸ºå®ƒä»¬æ˜¯ä¸ºäº†å¢å¼ºè‰ºæœ¯å®¶çš„åˆ›é€ åŠ›ï¼Œè€Œä¸æ˜¯å–è€Œä»£ä¹‹ã€‚
- en: '*All images, unless otherwise noted, are by the author.*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*é™¤éå¦æœ‰è¯´æ˜ï¼Œæ‰€æœ‰å›¾ç‰‡å‡ç”±ä½œè€…æä¾›ã€‚*'
- en: I am an audio machine learning engineer and researcher as well as a lifelong
    musician. If you are interested in more audio AI applications, see my previously
    published articles on [Tiny Audio Diffusion](https://medium.com/towards-data-science/tiny-audio-diffusion-ddc19e90af9b)
    and [Music Demixing](https://medium.com/rock-nheavy/the-music-demixing-ai-revolution-9d1528c6ef7c).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ˜¯ä¸€åéŸ³é¢‘æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆå’Œç ”ç©¶å‘˜ï¼ŒåŒæ—¶ä¹Ÿæ˜¯ä¸€åç»ˆèº«éŸ³ä¹äººã€‚å¦‚æœä½ å¯¹æ›´å¤šéŸ³é¢‘äººå·¥æ™ºèƒ½åº”ç”¨æ„Ÿå…´è¶£ï¼Œå¯ä»¥é˜…è¯»æˆ‘ä¹‹å‰å‘å¸ƒçš„æ–‡ç« ï¼š[Tiny Audio Diffusion](https://medium.com/towards-data-science/tiny-audio-diffusion-ddc19e90af9b)
    å’Œ [éŸ³ä¹åˆ†ç¦»](https://medium.com/rock-nheavy/the-music-demixing-ai-revolution-9d1528c6ef7c)ã€‚
- en: 'Find me on [LinkedIn](https://www.linkedin.com/in/christopher-landschoot/)
    & [GitHub](https://github.com/crlandsc) and keep up to date with my current work
    and research here: [www.chrislandschoot.com](https://www.chrislandschoot.com/)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ [LinkedIn](https://www.linkedin.com/in/christopher-landschoot/) å’Œ [GitHub](https://github.com/crlandsc)
    ä¸Šæ‰¾åˆ°æˆ‘ï¼Œäº†è§£æˆ‘å½“å‰çš„å·¥ä½œå’Œç ”ç©¶è¿›å±•ï¼Œè®¿é—®æˆ‘çš„ç½‘ç«™ï¼š[www.chrislandschoot.com](https://www.chrislandschoot.com/)ã€‚
- en: Find my music on [Spotify](https://open.spotify.com/artist/2i6noWJnJQPXPsudoiJuMS?si=fvLOxUPqTAWKB894WXMz5Q),
    [Apple Music](https://music.apple.com/us/artist/after-august/259370281), [YouTube](https://www.youtube.com/AfterAugust),
    [SoundCloud](https://soundcloud.com/after-august), and other streaming platforms
    as [After August](https://www.instagram.com/the_after_august/).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ [Spotify](https://open.spotify.com/artist/2i6noWJnJQPXPsudoiJuMS?si=fvLOxUPqTAWKB894WXMz5Q),
    [Apple Music](https://music.apple.com/us/artist/after-august/259370281), [YouTube](https://www.youtube.com/AfterAugust),
    [SoundCloud](https://soundcloud.com/after-august) å’Œå…¶ä»–æµåª’ä½“å¹³å°ä¸Šæ‰¾åˆ°æˆ‘çš„éŸ³ä¹ï¼Œè‰ºåä¸º [After August](https://www.instagram.com/the_after_august/)ã€‚
