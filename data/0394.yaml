- en: LingoNaut Language Assistant
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LingoNaut语言助手
- en: 原文：[https://towardsdatascience.com/lingonaut-language-assistant-6abe3e8b045c?source=collection_archive---------3-----------------------#2024-02-11](https://towardsdatascience.com/lingonaut-language-assistant-6abe3e8b045c?source=collection_archive---------3-----------------------#2024-02-11)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/lingonaut-language-assistant-6abe3e8b045c?source=collection_archive---------3-----------------------#2024-02-11](https://towardsdatascience.com/lingonaut-language-assistant-6abe3e8b045c?source=collection_archive---------3-----------------------#2024-02-11)
- en: Multilingual Learning with an Ollama-Python Walkie-Talkie
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Ollama-Python对讲机进行多语言学习
- en: '[](https://natecibik.medium.com/?source=post_page---byline--6abe3e8b045c--------------------------------)[![Nate
    Cibik](../Images/008c22b715ddf4f1d0f9970142edc09f.png)](https://natecibik.medium.com/?source=post_page---byline--6abe3e8b045c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6abe3e8b045c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6abe3e8b045c--------------------------------)
    [Nate Cibik](https://natecibik.medium.com/?source=post_page---byline--6abe3e8b045c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://natecibik.medium.com/?source=post_page---byline--6abe3e8b045c--------------------------------)[![Nate
    Cibik](../Images/008c22b715ddf4f1d0f9970142edc09f.png)](https://natecibik.medium.com/?source=post_page---byline--6abe3e8b045c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6abe3e8b045c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6abe3e8b045c--------------------------------)
    [Nate Cibik](https://natecibik.medium.com/?source=post_page---byline--6abe3e8b045c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6abe3e8b045c--------------------------------)
    ·11 min read·Feb 11, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6abe3e8b045c--------------------------------)
    ·阅读时长11分钟·2024年2月11日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f1da3ce8bd54bd19ae6421399d27b428.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1da3ce8bd54bd19ae6421399d27b428.png)'
- en: Image by author using DALL-E 3.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由作者使用 DALL-E 3 创建。
- en: We live in an era where the world is truly placed at our fingertips, if we know
    where to look. Today’s open-source large language models (LLMs) are potent and
    compact enough to place reasonably complete collections of human knowledge on
    standard consumer hardware, available for countless hours of ad-free, in-depth
    discussion on innumerable subjects without requiring an internet connection. Thanks
    to the dedicated efforts of the open-source community, tools like [Ollama](https://ollama.ai/)
    now allow us to serve high-quality quantized versions of today’s top models locally
    and interact with them using streamlined APIs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生活在一个世界真正触手可及的时代，只要我们知道该往哪里看。今天的开源大型语言模型（LLMs）足够强大和紧凑，可以将人类知识的完整集合放置在标准消费级硬件上，提供数小时无广告的深入讨论，涉及无数主题，且无需互联网连接。感谢开源社区的努力，像
    [Ollama](https://ollama.ai/) 这样的工具使我们能够在本地提供今天顶级模型的高质量量化版本，并通过简化的API与之互动。
- en: This ease of development means we can spend less time thinking about *how* we
    might build LLM applications, and focus more on *what* we would like to build.
    Personally, I have always wanted to learn multiple languages, but never had the
    conditions to properly practice such a thing, as my life experience has not yet
    included a lot of time in multilingual environments, and attempts at touristic
    language learning can be embarrassing when we don’t have close friends to practice
    with, since we are forced to experiment on strangers.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这种开发的便利性意味着我们可以减少花费在思考*如何*构建LLM应用上的时间，更多地关注*我们*想要构建的内容。就个人而言，我一直想学习多种语言，但由于我的生活经历中没有足够的多语言环境，且在旅游语言学习时，因缺乏亲密的朋友可供练习，这种学习往往会显得尴尬，因为我们不得不在陌生人身上进行试探。
- en: This a promising opportunity for employing the open-ended dialogue capabilities
    of LLM-based chat bots. Since locally serving quantized open-source LLMs on consumer
    hardware is now a well-oiled machine, the only other ingredient we need to make
    this vision into a reality is to augment the interaction into a speech-to-speech
    format. Again, the bounties of open-source research avail themselves to us. High-quality
    speech-to-text and text-to-speech models are on the shelf, wrapped in their own
    intuitive APIs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个有前景的机会，旨在利用基于大型语言模型（LLM）的聊天机器人开放式对话能力。由于如今在消费级硬件上本地化运行量化的开源LLM已经成为一个高效的流程，我们所需要做的唯一一件事就是将互动方式扩展为语音到语音的格式。同样，开源研究的成果为我们提供了便利。高质量的语音转文本和文本转语音模型已经准备就绪，并配有直观的API接口。
- en: 'A perfect demonstration of how these tools can make exciting concepts come
    to life with ease is LingoNaut: a multilingual language assistant that runs from
    a single Python script containing just 300 lines of code. Using a combination
    of OpenAI’s [Whisper](https://github.com/openai/whisper) speech-to-text model,
    a local Ollama server, and the Coqui.ai [TTS](https://github.com/coqui-ai/TTS)
    text-to-speech library, we can construct a user-friendly walkie-talkie interface
    with a wide selection of LLMs. From there, it is just a matter of system prompt
    engineering (easily done with [ollama-python](https://github.com/ollama/ollama-python))
    to beckon our desired behavior from the LLM, in this case creating a helpful multilingual
    language tutor. Indeed, this means the LingoNaut code can easily be adapted to
    create a wide range of AI assistants by just adjusting the LLM and system prompt
    being used.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一个完美展示这些工具如何轻松地将激动人心的概念变为现实的示例就是 LingoNaut：一个多语言语言助手，它运行在一个仅包含 300 行代码的单一 Python
    脚本中。通过结合 OpenAI 的 [Whisper](https://github.com/openai/whisper) 语音转文本模型、本地 Ollama
    服务器和 Coqui.ai 的 [TTS](https://github.com/coqui-ai/TTS) 文本转语音库，我们可以构建一个用户友好的对讲机界面，并提供多种
    LLM 选择。从这里开始，只需要进行系统提示工程（通过 [ollama-python](https://github.com/ollama/ollama-python)
    容易完成），就可以从 LLM 中召唤出我们所需的行为，在这个案例中，创造一个有用的多语言语言导师。实际上，这意味着 LingoNaut 的代码可以轻松适应，通过调整使用的
    LLM 和系统提示，就能创建各种各样的 AI 助手。
- en: Demonstration of LingoNaut app.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: LingoNaut 应用演示。
- en: Code for running LingoNaut is available in the [GitHub repo](https://github.com/FoamoftheSea/lingonaut-python)
    with easy instructions for installation. LingoNaut is an open-source project,
    and contribution is welcomed. For example, future work could involve wrapping
    the backend in a more sophisticated web UI to allow remote hosting of walkie-talkie
    LLM apps, which could lead to supporting mobile devices. I hope that LingoNaut
    is a fun and helpful resource for others on their learning journeys, and that
    the code is useful as a lightweight skeleton for engineers sandboxing new ideas
    for LLM- or LMM-based applications.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 LingoNaut 的代码可以在 [GitHub 仓库](https://github.com/FoamoftheSea/lingonaut-python)中找到，提供了简单的安装说明。LingoNaut
    是一个开源项目，欢迎贡献。例如，未来的工作可能涉及将后端包装成更复杂的网页 UI，以支持远程托管对讲机 LLM 应用，从而支持移动设备。我希望 LingoNaut
    能成为一个有趣且有用的资源，帮助其他人在他们的学习旅程中，同时也希望这段代码能作为一个轻量级的框架，供工程师们在沙盒中实验新的 LLM 或 LMM 基于的应用想法。
- en: The rest of this article provides an overview of the Python code that runs LingoNaut,
    the toolbox of open-source components that makes the ready assembly of a tool
    like LingoNaut possible, and promising directions for future work. With some clever
    Python coding tricks, brilliant speech-to-text and text-to-speech models, and
    the local deployment of quantized LLMs on consumer machines, we can easily construct
    a speech-to-speech pipeline to unlock use cases that are less conducive to the
    confines of text, such as language learning. The ingenious contributions of the
    research community combine to provide us this extraordinary set of possibilities.
    Let’s walk through each one in more detail to understand the roles they play in
    bringing the LingoNaut app to life.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分概述了运行 LingoNaut 的 Python 代码，LingoNaut 是一个开源组件工具箱，使得像 LingoNaut 这样的工具能够轻松组合成型，并且为未来的工作提供了有前景的方向。通过一些巧妙的
    Python 编程技巧、出色的语音转文本和文本转语音模型，以及在消费者机器上本地部署量化的 LLM（大语言模型），我们可以轻松构建一个语音对语音的管道，来解锁那些不太适合文本形式的应用场景，例如语言学习。研究界的聪明贡献汇聚在一起，为我们提供了这一系列非凡的可能性。让我们更详细地走一遍每一个步骤，理解它们在让
    LingoNaut 应用成为现实过程中所扮演的角色。
- en: LingoNaut Code
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LingoNaut 代码
- en: '![](../Images/bcee7b55af34c1960546d4f1a9850421.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bcee7b55af34c1960546d4f1a9850421.png)'
- en: Image by author using DALL-E 3.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用 DALL-E 3 制作。
- en: Walkie-Talkie Interface
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对讲机界面
- en: The code in LingoNaut creates a handy terminal-based speech-to-speech app for
    use with Ollama which can be easily adapted for new use cases. Using a package
    called [pynput](https://pypi.org/project/pynput/), we can create a keyboard listener
    object that runs in a background thread and reacts to keystrokes from the user.
    This opens up a wide array of options for apps that run in continuous loops, most
    importantly in this case providing a control for triggering and terminating the
    user audio recording without the need for a graphic user interface with buttons.
    This way, an interactive app can be run directly from the terminal, easing many
    engineering concerns.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: LingoNaut中的代码创建了一个方便的基于终端的语音到语音应用，可以与Ollama一起使用，并且可以轻松适应新的使用场景。通过使用一个名为[pynput](https://pypi.org/project/pynput/)的包，我们可以创建一个在后台线程中运行并响应用户按键的键盘监听器对象。这为运行在连续循环中的应用提供了广泛的选项，最重要的是在这种情况下提供一个控制，用于触发和终止用户音频录制，而无需图形用户界面和按钮。这样，互动式应用程序可以直接从终端运行，从而简化了许多工程问题。
- en: In LingoNaut, different speech-to-text models can be deployed by pressing different
    keys to record audio. While lightweight Whisper models excel at quickly processing
    English audio, a larger and slower model must be used for accurate on-the-fly
    multilingual transcription. Thus, the user may choose to hold the **Ctrl** key
    to use a lightweight base model for asking questions in English, or hold the **Shift**
    key to speak in other languages.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在LingoNaut中，可以通过按不同的键来录制音频，从而部署不同的语音转文本模型。轻量级的Whisper模型在快速处理英语音频方面表现优异，而更大且较慢的模型则必须用于准确的多语言实时转录。因此，用户可以选择按住**Ctrl**键使用轻量级基础模型来提问英语问题，或者按住**Shift**键来用其他语言进行对话。
- en: Other useful LingoNaut features enabled by the keyboard listener are the ability
    to interrupt the model response with the **End** key when you’ve given it a bad
    input or are not satisfied with the direction of the response. This prevents getting
    stuck waiting for irrelevant text and audio to finish coming through so that the
    user can stay more engaged. The user can also lock the keyboard inputs using the
    **F2** key so they can leave a session open for later without worrying about triggering
    the audio recording accidentally.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由键盘监听器启用的其他有用的LingoNaut功能包括，当你输入错误或对响应方向不满意时，可以通过按**End**键中断模型响应。这防止了因等待无关的文本和音频完成播放而被卡住，从而使用户能够保持更高的参与度。用户还可以通过按**F2**键锁定键盘输入，这样他们就可以在不担心意外触发音频录制的情况下，保持会话打开以供以后使用。
- en: Concurrency
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并发
- en: 'While the packages used in LingoNaut provide streamlined APIs for interfacing
    with the three models used to create the speech-to-speech pipeline, naively waiting
    for the LLM to generate text, transcribing it to audio, and then playing it to
    the user in series would be a very slow experience. The streaming of text chunks
    from the LLM, the transcription of text chunks into audio files, and the playback
    of previously transcribed audio files can all happen concurrently, so LingoNaut
    uses a separate thread for each of these tasks. By using ThreadPoolExecutor objects
    with max_workers set to 1, we can easily open new threads for task submissions
    while guaranteeing those tasks will be executed in order, allowing us to outsource
    work from the main thread without having overlapping or shuffled returns. A basic
    code outline for this arrangement is shown below:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LingoNaut中使用的软件包为与创建语音到语音管道的三个模型接口提供了简化的API，但单纯等待大型语言模型（LLM）生成文本、将其转录为音频，然后将音频串行播放给用户，将是一个非常慢的体验。LLM文本块的流式传输、文本块转录成音频文件以及播放先前转录的音频文件都可以同时发生，因此LingoNaut为每个任务使用了独立的线程。通过使用`ThreadPoolExecutor`对象并将`max_workers`设置为1，我们可以轻松地为任务提交打开新的线程，同时确保任务按顺序执行，从而使我们能够将工作从主线程外包，而不会出现重叠或顺序错乱的返回。以下是该安排的基本代码框架：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Model Customization
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型定制
- en: The [ollama-python](https://github.com/ollama/ollama-python) package has an
    easy tool for creating custom tagged model configurations using “Modelfiles” to
    guide the LLM behavior. In the case of LingoNaut, the 4-bit quantized [Mistral
    7B](https://mistral.ai/news/announcing-mistral-7b/) model in the Ollama library
    was customized with an explicit system prompt to guide the desired behavior as
    a language learning assistant. The prompt can be found in the repository in the
    [create_lingonaut_ollama.py](https://github.com/FoamoftheSea/lingonaut-python/blob/main/create_lingonaut_ollama.py)
    file, and it should be noted that this is the only file that customizes the model
    selection and behavior in the repo, meaning that this repo can be instantly converted
    into any other walkie-talkie LLM application of your choosing by creating a tagged
    model using a different Modelfile. The LLM being used can also easily be swapped
    for larger or smaller models based on available resources.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[ollama-python](https://github.com/ollama/ollama-python) 包提供了一个简单的工具，可以使用“Modelfiles”创建自定义标签的模型配置，以引导LLM的行为。在
    LingoNaut 的情况下，Ollama 库中的 4 位量化 [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/)
    模型通过显式的系统提示进行了自定义，以引导其作为语言学习助手的预期行为。该提示可以在仓库中的 [create_lingonaut_ollama.py](https://github.com/FoamoftheSea/lingonaut-python/blob/main/create_lingonaut_ollama.py)
    文件中找到，需要注意的是，这是唯一一个自定义模型选择和行为的文件，这意味着通过创建一个带有不同 Modelfile 的标签模型，您可以立即将这个仓库转换成任何其他的对讲机
    LLM 应用程序。根据可用资源，所使用的 LLM 也可以轻松地替换为更大或更小的模型。'
- en: Speech-to-Speech Toolbox
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语音到语音工具箱
- en: '![](../Images/52b9aa9851a7f168490da2c147be7c1c.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52b9aa9851a7f168490da2c147be7c1c.png)'
- en: Image by author using DALL-E 3.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用 DALL-E 3 制作的图像。
- en: Whisper
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Whisper
- en: '[Whisper](https://openai.com/research/whisper) is an open-source speech-to-text
    model provided by OpenAI. There are five model sizes available in both English-focused
    and multilingual varieties to choose from, depending on the complexity of the
    application and desired accuracy-efficiency tradeoff. Whisper is an end-to-end
    speech-to-text framework that uses an encoder-decoder transformer architecture
    operating on input audio split into 30-second chunks and converted into a log-Mel
    spectrogram. The network is trained on multiple speech processing tasks, including
    multilingual speech recognition, speech translation, spoken language identification,
    and voice activity detection.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[Whisper](https://openai.com/research/whisper) 是由 OpenAI 提供的开源语音转文本模型。根据应用的复杂性和所需的准确性与效率的权衡，有五种不同大小的模型可供选择，包括英语专注和多语言版本。Whisper
    是一个端到端的语音转文本框架，采用编码器-解码器变压器架构，处理输入音频，将其分割成 30 秒的块并转换为对数 Mel 频谱图。该网络在多个语音处理任务上进行了训练，包括多语言语音识别、语音翻译、口语语言识别和语音活动检测。'
- en: '![](../Images/b563e9edc7a8d82fd8b1229c45e5b42d.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b563e9edc7a8d82fd8b1229c45e5b42d.png)'
- en: Diagram of Whisper architecture from the [research paper](https://cdn.openai.com/papers/whisper.pdf).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 [研究论文](https://cdn.openai.com/papers/whisper.pdf) 的 Whisper 架构图。
- en: 'For this project, two walkie-talkie buttons are available to the user: one
    which sends their general English-language questions to the bot through the lighter,
    faster “base” model, and a second which deploys the larger “medium” multilingual
    model that can distinguish between dozens of languages and accurately transcribe
    correctly pronounced statements. In the context of language learning, this leads
    the user to focus very intently on their pronunciation, accelerating the learning
    process. A chart of the available Whisper models is shown below:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目，用户可以使用两个对讲机按钮：一个将他们的普通英语问题通过更轻量、更快速的“基础”模型发送到机器人，另一个则部署更大的“中型”多语言模型，能够区分数十种语言并准确转录正确发音的语句。在语言学习的背景下，这促使用户更加专注于自己的发音，从而加速学习过程。下面是可用
    Whisper 模型的图表：
- en: '![](../Images/da85d960fe89c2ae302f7564b1d955d5.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da85d960fe89c2ae302f7564b1d955d5.png)'
- en: Chart from [https://github.com/openai/whisper](https://github.com/openai/whisper)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 [https://github.com/openai/whisper](https://github.com/openai/whisper) 的图表
- en: Ollama
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ollama
- en: 'There exists a variety of highly useful open-source language model interfaces,
    all catering to different use cases with varying levels of complexity for setup
    and use. Among the most widely known are the [oobabooga text-gen webui](https://github.com/oobabooga/text-generation-webui),
    with arguably the most flexibility and under-the-hood control, [llama.cpp](https://github.com/ggerganov/llama.cpp),
    which originally focused on optimized deployment of quantized models on smaller
    CPU-only devices but has since expanded to serving other hardware types, and the
    streamlined interface chosen for this project (built on top of llama.cpp): [Ollama](https://ollama.ai/).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 存在各种非常有用的开源语言模型接口，它们针对不同的使用场景提供不同复杂度的设置和使用方式。最广为人知的包括 [oobabooga text-gen webui](https://github.com/oobabooga/text-generation-webui)，它无疑提供了最多的灵活性和底层控制，[llama.cpp](https://github.com/ggerganov/llama.cpp)，最初专注于在小型仅支持CPU的设备上优化量化模型的部署，但后来扩展到支持其他硬件类型，以及本项目所选的简化界面（基于
    llama.cpp）：[Ollama](https://ollama.ai/)。
- en: Ollama focuses on simplicity and efficiency, running in the background and capable
    of serving multiple models simultaneously on small hardware, quickly shifting
    models in and out of memory as needed to serve their requests. Instead of focusing
    on lower-level tools like fine-tuning, Ollama excels at [simple installation](https://ollama.com/download/linux),
    efficient runtime, a great [spread of ready-to-use models](https://ollama.com/library),
    and [tools for importing pretrained model weights](https://github.com/ollama/ollama/blob/main/docs/import.md).
    The focus on efficiency and simplicity makes Ollama the natural choice for LLM
    interface in a project like LingoNaut, since the user does not need to remember
    to close their session to free up resources, as Ollama will automatically manage
    this in the background when the app is not in use. Further, the ready access to
    performant, quantized models in the library is perfect for frictionless development
    of LLM applications like LingoNaut.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Ollama 侧重于简洁性和高效性，能够在后台运行，并能够同时在小型硬件上服务多个模型，根据需要快速将模型载入和移出内存以响应请求。Ollama 的重点不是低级工具，如微调，而是擅长
    [简单安装](https://ollama.com/download/linux)、高效的运行时、丰富的 [现成可用模型](https://ollama.com/library)，以及
    [导入预训练模型权重的工具](https://github.com/ollama/ollama/blob/main/docs/import.md)。对效率和简洁性的关注使得
    Ollama 成为像 LingoNaut 这样的项目中 LLM 接口的自然选择，因为用户无需记得关闭会话以释放资源，因为 Ollama 会在应用程序不使用时自动在后台进行管理。此外，库中对高效、量化模型的便捷访问非常适合无摩擦地开发像
    LingoNaut 这样的 LLM 应用。
- en: While Ollama is not technically built for Windows, it is easy for Windows users
    to install it on Windows Subsystem for Linux ([WSL](https://learn.microsoft.com/en-us/windows/wsl/install)),
    then communicate with the server from their Windows applications. With WSL installed,
    open a Linux terminal and enter the one-liner Ollama [installation command](https://ollama.ai/download/linux).
    Once the installation finishes, simply run “ollama serve” in the Linux terminal,
    and you can then communicate with your Ollama server from any Python script on
    your Windows machine.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Ollama 技术上并未专为 Windows 构建，但 Windows 用户可以轻松地通过 Windows 子系统（[WSL](https://learn.microsoft.com/en-us/windows/wsl/install)）在
    Windows 上安装它，并通过 Windows 应用程序与服务器进行通信。安装 WSL 后，打开 Linux 终端并输入一行 Ollama [安装命令](https://ollama.ai/download/linux)。安装完成后，只需在
    Linux 终端中运行“ollama serve”，然后就可以通过任何 Windows 机器上的 Python 脚本与 Ollama 服务器进行通信。
- en: Coqui.ai 🐸 TTS
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Coqui.ai 🐸 TTS
- en: '[TTS](https://github.com/coqui-ai/TTS) is a fully-loaded text-to-speech library
    available for non-commercial use, with paid commercial licenses available. The
    library has experienced notable popularity, with 3k forks and 26.6k stars on GitHub
    as of the time of this writing, and it’s clear why: the library works like the
    Ollama of the text-to-speech space, providing a unified interface for accessing
    a diverse array of performant models which cover a variety of use cases (for example:
    providing a multi-speaker, multilingual model for this project), exciting features
    such as voice cloning, and controls over the speed and emotional tone of transcriptions.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[TTS](https://github.com/coqui-ai/TTS) 是一个功能齐全的文本转语音库，供非商业用途使用，同时也提供商业许可证。该库非常受欢迎，截至目前在
    GitHub 上已有 3k 次分叉和 26.6k 个星标，这也不难理解：该库就像文本转语音领域的 Ollama，提供了一个统一的接口来访问各种高效的模型，涵盖了多种使用场景（例如：为本项目提供一个多说话人、多语言模型）、令人兴奋的功能，如声音克隆，以及对转录速度和情感语调的控制。'
- en: The TTS library provides an extensive selection of text-to-speech models, including
    the illustrious Fairseq models from Facebook research’s Massively Multilingual
    Speech ([MMS](https://research.facebook.com/publications/scaling-speech-technology-to-1000-languages/))
    project. For LingoNaut, the Coqui.ai team’s own [XTTS](https://coqui.ai/blog/tts/open_xtts)
    model turned out to be the correct choice, as it generates high-quality speech
    in multiple languages seamlessly. Although the model does have a “language” input
    parameter, I found that even leaving this set to “en” for English and simply passing
    text in other languages still results in faithful multilingual generation with
    mostly correct pronunciations.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: TTS库提供了丰富的文本转语音模型选择，包括Facebook研究的“大规模多语言语音”（[MMS](https://research.facebook.com/publications/scaling-speech-technology-to-1000-languages/)）项目中的著名Fairseq模型。对于LingoNaut，Coqui.ai团队的[XTTS](https://coqui.ai/blog/tts/open_xtts)模型最终被证明是正确的选择，因为它能够无缝地生成多语言的高质量语音。虽然该模型确实有一个“语言”输入参数，但我发现即便将其设置为“en”表示英语，并且仅传递其他语言的文本，依然能生成准确的多语言语音，且发音大多数情况下正确。
- en: Conclusion
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: '![](../Images/4616ce07e69915d1d0e25eef8f897048.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4616ce07e69915d1d0e25eef8f897048.png)'
- en: Image by author using DALL-E 3.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由作者使用DALL-E 3生成。
- en: In this article, I’ve introduced a new speech-to-speech multilingual language
    learning assistant called LingoNaut. The app runs through the terminal using a
    light and easily adaptable Python script with a walkie-talkie keyboard interface.
    This completely free and locally hosted app allows the user to practice a large
    variety of languages using AI, becoming confident in new languages without having
    to practice on strangers before they are ready. The code is available on [GitHub](https://github.com/FoamoftheSea/lingonaut-python)
    with quick setup instructions, and is easily extensible to new use cases. I hope
    that the community finds the app helpful in their language learning endeavors,
    and that the code serves as a handy lightweight framework for future proof-of-concepts.
    LingoNaut is open-source, and contributions are welcomed.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我介绍了一款新的语音到语音多语言语言学习助手——LingoNaut。该应用通过终端运行，使用轻量且易于适配的Python脚本，并配有对讲机键盘界面。这个完全免费的本地托管应用允许用户利用人工智能练习多种语言，在不需要在准备好之前与陌生人练习的情况下，增强对新语言的信心。代码已在[GitHub](https://github.com/FoamoftheSea/lingonaut-python)上公开，并提供了快速设置说明，同时也容易扩展到新的应用场景。希望社区能在语言学习中从这款应用中受益，并且希望这段代码能作为未来概念验证的便捷轻量框架。LingoNaut是开源的，欢迎贡献。
- en: Future Work
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 未来工作
- en: This work built a speech-to-speech pipeline by combining the text-based conversational
    ability of a LLM with separate speech-to-text and text-to-speech models on the
    input and output sides, respectively. Such a design is clunky and prone to cascading
    error, so it is therefore inferior to using a truly multimodal language model
    that could understand and generate both audio and text tokens from a unified representation
    space. When we encode audio to text before passing it to the model, we remove
    all of the tonal information contained in the audio, including pronunciation and
    emotional delivery, which significantly limits how advanced our language assistant
    can be. By instead using an LMM that operates on a joint multimodal representation
    space, we would retain the nuanced tonal information in the user inputs. Similarly,
    encoding text-to-speech on the output side is another significant information
    bottleneck, and the interaction will not be as natural.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目通过将大型语言模型（LLM）的基于文本的对话能力与输入和输出端分别独立的语音转文本和文本转语音模型相结合，构建了一个语音到语音的流程。这种设计比较笨重，容易产生级联错误，因此比使用真正的多模态语言模型更为逊色，后者能够从统一的表示空间中理解和生成音频与文本标记。当我们在将音频传递给模型之前将其编码为文本时，我们丧失了音频中的所有音调信息，包括发音和情感表达，这显著限制了我们的语言助手的先进性。通过使用在联合多模态表示空间上操作的LLM，我们可以保留用户输入中的微妙音调信息。同样，在输出端进行文本转语音编码也是一个重要的信息瓶颈，导致交互不够自然。
- en: The authors of [NeXT-GPT](https://next-gpt.github.io/) provide a promising framework
    for using pretrained LLMs to create large multimodal models (LMMs) that can operate
    in a unified multimodal representation space, and this is a promising direction
    for speech-to-speech apps. With some effort, it is likely that the released NeXT-GPT
    weights could be imported into Ollama for experimentation. Their experiment used
    a similarly sized Vicuna 7B LLM, establishing that lightweight LLMs can work on
    multimodal spaces. While the Vicuna model is not advertised as a multilingual
    model, neither is the Mistral 7B model used in this LingoNaut experiment, though
    it still seems to work quite well for the purpose. Ideally, a fine-tuned multilingual
    instruction-tuned model would be the best choice for LingoNaut. To that end, a
    well-chosen dataset and low-rank adaptation ([LoRA](https://arxiv.org/abs/2106.09685))
    would lead to likely success. Further, the parameter efficient multimodal alignment
    with lightweight adapters demonstrated by [LaVIN](https://arxiv.org/abs/2305.15023)
    offers to make NeXT-GPT-style LMM development more attainable with limited resources.
    A first step would be to investigate aligning the representations of a high-quality
    audio encoder with the LLM using LaVIN’s “cheap and quick” Mixture-of-Modality
    Adaptation (MMA) training strategy, relieving the speech-to-text bottleneck on
    the input side. Then the next step would be to investigate enabling multimodal
    output using the NeXT-GPT-style modality-switching instruction tuning (MoSIT).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[NeXT-GPT](https://next-gpt.github.io/)的作者提供了一个有前景的框架，用于利用预训练的LLM（大语言模型）创建大型的多模态模型（LMM），这些模型可以在统一的多模态表示空间中运行，这对于语音到语音的应用程序来说是一个有前景的方向。经过一些努力，发布的NeXT-GPT权重可能会被导入到Ollama中进行实验。他们的实验使用了一个大小相似的Vicuna
    7B LLM，证明了轻量级LLM可以在多模态空间中工作。虽然Vicuna模型并未宣传为多语言模型，但LingoNaut实验中使用的Mistral 7B模型也没有宣传为多语言模型，尽管它似乎在此目的下表现得相当不错。理想情况下，经过微调的多语言指令调优模型将是LingoNaut的最佳选择。为此，选择合适的数据集和低秩适配（[LoRA](https://arxiv.org/abs/2106.09685)）可能会导致成功。此外，[LaVIN](https://arxiv.org/abs/2305.15023)展示的轻量级适配器与参数高效的多模态对齐提供了使NeXT-GPT风格的LMM开发在有限资源下更具可达性的可能。第一步将是调查如何使用LaVIN的“廉价且快速”的模态混合适应（MMA）训练策略，将高质量音频编码器的表示与LLM对齐，从而缓解输入端的语音转文本瓶颈。接下来的步骤是调查如何使用NeXT-GPT风格的模态切换指令调优（MoSIT）来实现多模态输出。'
- en: Finally, as mentioned in the introduction, building a web UI frontend which
    could communicate with a remote backend would expand the fun of LingoNaut considerably,
    as larger models could be deployed on rented cloud GPUs, and communicated with
    via https requests from laptops and mobile devices. This would allow the community
    to build any walkie-talkie LLM app of their imagination that could be used their
    by friends and family anywhere an internet connection is available, and turn the
    vision of a universally accessible language learning assistant into a reality.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如引言中所提到的，构建一个可以与远程后端通信的Web UI前端，将大大扩展LingoNaut的乐趣，因为更大的模型可以部署在租用的云GPU上，并通过笔记本和移动设备的https请求进行通信。这将允许社区构建任何他们想象中的对讲机LLM应用程序，让朋友和家人在任何有网络连接的地方都能使用，从而将普遍可访问的语言学习助手的愿景变为现实。
