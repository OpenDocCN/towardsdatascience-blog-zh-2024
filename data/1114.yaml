- en: Turn Llama 3 into an Embedding Model with LLM2Vec
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LLM2Vec将Llama 3转化为嵌入模型
- en: 原文：[https://towardsdatascience.com/turn-llama-3-into-an-embedding-model-with-llm2vec-8448005f99aa?source=collection_archive---------1-----------------------#2024-05-03](https://towardsdatascience.com/turn-llama-3-into-an-embedding-model-with-llm2vec-8448005f99aa?source=collection_archive---------1-----------------------#2024-05-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/turn-llama-3-into-an-embedding-model-with-llm2vec-8448005f99aa?source=collection_archive---------1-----------------------#2024-05-03](https://towardsdatascience.com/turn-llama-3-into-an-embedding-model-with-llm2vec-8448005f99aa?source=collection_archive---------1-----------------------#2024-05-03)
- en: RAG with Llama 3 for the generation and the retrieval
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Llama 3进行生成和检索的RAG
- en: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--8448005f99aa--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--8448005f99aa--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8448005f99aa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8448005f99aa--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--8448005f99aa--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--8448005f99aa--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--8448005f99aa--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8448005f99aa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8448005f99aa--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--8448005f99aa--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8448005f99aa--------------------------------)
    ·7 min read·May 3, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8448005f99aa--------------------------------)
    ·7分钟阅读·2024年5月3日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/aa3e5f5e3c663da28873ad33454491ca.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa3e5f5e3c663da28873ad33454491ca.png)'
- en: Generated with DALL-E
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DALL-E生成
- en: The embedding model is a critical component of retrieval-augmented generation
    (RAG) for large language models (LLMs). They encode the knowledge base and the
    query written by the user.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入模型是大语言模型（LLMs）中检索增强生成（RAG）的关键组成部分。它们对用户编写的知识库和查询进行编码。
- en: Using an embedding model trained or fine-tuned for the same domain as the LLM
    can significantly improve a RAG system. However, finding or training such an embedding
    model is often a difficult task as in-domain data are usually scarce.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 使用为与LLM相同领域训练或微调的嵌入模型，可以显著提高RAG系统的效果。然而，找到或训练这样的嵌入模型通常是一个困难的任务，因为领域内的数据通常稀缺。
- en: In this article, I show how to turn an LLM into a text embedding model using
    LLM2Vec. We will see how to do it with Llama 3 to create a RAG system that doesn’t
    need any other models than Llama 3.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将展示如何使用LLM2Vec将LLM转化为文本嵌入模型。我们将看到如何用Llama 3来创建一个RAG系统，该系统只需Llama 3即可，不需要任何其他模型。
- en: 'LLM2Vec: Your LLM is Also an Embedding Model'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM2Vec：你的LLM也是一个嵌入模型
- en: 'LLM2Vec is presented in this paper:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了LLM2Vec：
- en: '[LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders](https://arxiv.org/pdf/2404.05961.pdf)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[LLM2Vec: 大型语言模型秘密强大的文本编码器](https://arxiv.org/pdf/2404.05961.pdf)'
- en: 'The official implementation is available on GitHub:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 官方实现已在GitHub上发布：
- en: '[McGill-NLP/llm2vec](https://github.com/McGill-NLP/llm2vec) (MIT license)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[McGill-NLP/llm2vec](https://github.com/McGill-NLP/llm2vec)（MIT许可）'
- en: '*How does LLM2Vec work?*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*LLM2Vec是如何工作的？*'
- en: 'LLMs are trained with a causal language modeling loss. They are trained to
    predict the next token given a sequence of tokens. Since the training examples
    are entire sequences of tokens, a causal attention mask is applied to the sequence
    so that when the model learns to predict a token, all the following tokens in
    the sequence are masked and don’t influence the attention computation. For instance,
    if we have the following sequence:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs是通过因果语言建模损失进行训练的。它们被训练用来预测给定一序列token后的下一个token。由于训练示例是完整的token序列，因此在序列中应用因果注意力掩码，这样当模型学习预测某个token时，序列中所有后续的token都会被掩盖，不会影响注意力计算。例如，如果我们有以下序列：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'During training the following attention masks will be successively applied
    (under the mask, I only show the tokens that are not masked):'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，将依次应用以下注意力掩码（在掩码下，我只展示未被掩盖的token）：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The zeroes indicate that the token will not be considered for the attention
    computation.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 零值表示该token不会被用于注意力计算。
- en: On the other hand, text embedding models are trained to encode entire sequence
    tokens and are bidirectional, i.e., they encode a sequence from left to right
    and right to left.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，文本嵌入模型被训练用来编码整个序列的标记，并且是双向的，即它们从左到右和从右到左编码一个序列。
- en: The initial step in the LLM2Vec method to convert an LLM into an embedding model
    involves substituting the causal attention mask used in decoder-only LLMs with
    a matrix of all-ones. This alteration allows each token in the sequence to interact
    with all other tokens, effectively transforming it into a bidirectional LLM. However,
    decoder-only LLMs were not trained to encode future tokens, and thus, this straightforward
    modification degrades the quality of the representations. Nonetheless, it is possible
    to train the model to effectively use its new bidirectional attention capabilities.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: LLM2Vec方法将LLM转换为嵌入模型的初步步骤包括将解码器-only LLMs中使用的因果注意力掩码替换为全1矩阵。这一改变使得序列中的每个标记都可以与所有其他标记交互，实质上将其转变为一个双向LLM。然而，解码器-only
    LLMs并没有被训练来编码未来的标记，因此，这一简单的修改会降低表示的质量。尽管如此，仍然可以训练模型有效利用其新的双向注意力能力。
- en: They suggest employing a new masked next token prediction (MNTP) objective.
    MNTP merges the next token prediction objective with the masked language modeling
    used by BERT. To implement this, we take an arbitrary sequence x = (x1, x2, .
    . . , xN), mask a subset of the input tokens, and then train the model to predict
    these masked tokens using both past and future contexts. Importantly, when predicting
    a token that has been masked at position i, the loss is calculated using the logits
    from the token representation at the preceding position i − 1, rather than from
    the masked position itself as we will do for a BERT model.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 他们建议采用一种新的掩码下一个标记预测（MNTP）目标。MNTP将下一个标记预测目标与BERT使用的掩码语言模型结合在一起。为了实现这一点，我们取一个任意的序列x
    = (x1, x2, . . . , xN)，掩码掉输入标记的一个子集，然后训练模型使用过去和未来的上下文来预测这些被掩码的标记。重要的是，当预测一个在位置i被掩码的标记时，损失是根据前一个位置i
    - 1的标记表示的logits计算的，而不是像BERT模型那样根据被掩码的位置本身计算。
- en: Converting the LLM into a bidirectional model followed by MNTP training can
    adapt any decoder-only LLM into an encoder. However, these steps may not adequately
    address the need for sequence representations. Indeed, bidirectional encoders
    like BERT usually also incorporate a next-sentence prediction task in their pre-training
    while LLMs were not trained for this task. LLM2Vec addresses this missing capability
    by processing each input sequence twice through the model, each time with different
    randomly selected dropout masks, producing two distinct representations of the
    same sequence. The training objective is to improve the similarity between these
    two representations while reducing their similarity to representations of different
    sequences within the same training batch. They call this last step “Unsupervised
    Contrastive Learning” (SimCSE).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 将LLM转换为双向模型并随后进行MNTP训练，可以将任何解码器-only LLM转换为编码器。然而，这些步骤可能不足以解决序列表示的问题。实际上，像BERT这样的双向编码器通常还会在预训练中加入下一个句子预测任务，而LLM并未经过此类训练。LLM2Vec通过让模型对每个输入序列进行两次处理，每次使用不同随机选择的丢弃掩码，生成该序列的两个不同表示，来解决这一缺失的能力。训练目标是提高这两个表示之间的相似性，同时减少它们与同一训练批次中不同序列表示之间的相似性。他们将这最后一步称为“无监督对比学习”（SimCSE）。
- en: 'The full process behind LLM2Vec is illustrated by this figure in the paper:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: LLM2Vec的完整过程通过论文中的这张图示说明：
- en: '![](../Images/8f30670da68e07a279a68d6d8c488f4d.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f30670da68e07a279a68d6d8c488f4d.png)'
- en: '[source](https://arxiv.org/abs/2404.05961) (CC-BY)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[source](https://arxiv.org/abs/2404.05961) (CC-BY)'
- en: They evaluated the models produced by LLM2Vec in various tasks and showed that
    they can outperform standard text embedding models. You will find the results
    in the sections 3 and 4 of the paper.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 他们评估了LLM2Vec生成的模型在各类任务中的表现，并显示它们可以超越标准的文本嵌入模型。你可以在论文的第3节和第4节找到相关结果。
- en: Turning Llama 3 into a Text Embedding Model with LLM2Vec
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LLM2Vec将Llama 3转换为文本嵌入模型
- en: 'My notebook showing how to convert Llama 3 into an embedding model is available
    here:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我的笔记本展示了如何将Llama 3转换为嵌入模型，可以在此查看：
- en: '[Get the notebook (#65)](https://newsletter.kaitchup.com/p/notebooks)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[获取笔记本 (#65)](https://newsletter.kaitchup.com/p/notebooks)'
- en: Converting an LLM to a text embedding model with LLM2Vec is fairly simple.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 将LLM转换为文本嵌入模型使用LLM2Vec是相当简单的。
- en: 'First, install the following packages:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，安装以下软件包：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The llm2vec package will convert the LLM to an embedding model. flash-attn is
    the package for FlashAttention. It is not required but speeds up the training
    with MNTP. It works only with recent GPUs from the Ampere generation (RTX 3xxx/4xxx,
    A100, H100, etc.).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: llm2vec包将会把LLM转换为嵌入模型。flash-attn是FlashAttention的包。虽然不是必需的，但它可以加速MNTP训练。它仅适用于安培架构（RTX
    3xxx/4xxx、A100、H100等）的最新GPU。
- en: 'Then, the conversion itself is performed by the following code:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，转换本身通过以下代码执行：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: “torch_dtype=torch.bfloat16” is necessary to be able to run the conversion on
    a 24 GB GPU. If you don’t set it the model will be larger than the original model
    with float32 parameters.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: “torch_dtype=torch.bfloat16”是必要的，以便能在24GB的GPU上运行转换。如果不设置此参数，模型将比原始的float32参数模型大。
- en: 'I pushed this model to the hub in case you don’t want to do this conversion
    by yourself:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我已将该模型推送到中心，以防你不想自己进行此转换：
- en: '[kaitchup/Llama-3-8B-llm2vec-Emb](https://huggingface.co/kaitchup/Llama-3-8B-llm2vec-Emb)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[kaitchup/Llama-3-8B-llm2vec-Emb](https://huggingface.co/kaitchup/Llama-3-8B-llm2vec-Emb)'
- en: This model is ready to be used. You can plug it into a RAG pipeline. However,
    it won’t perform as well as a standard embedding model (in most cases).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型已经可以使用了。你可以将其插入RAG流水线中。不过，它的性能通常不如标准嵌入模型（在大多数情况下）。
- en: 'We need to train Llama 3 8B with the MNTP objective. The authors also provide
    a script to do it:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要用MNTP目标训练Llama 3 8B。作者也提供了一个脚本来完成此操作：
- en: '[experiments/run_mntp.py](https://github.com/McGill-NLP/llm2vec/blob/main/experiments/run_mntp.py)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[experiments/run_mntp.py](https://github.com/McGill-NLP/llm2vec/blob/main/experiments/run_mntp.py)'
- en: It currently supports models with the Llama and Mistral architectures.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 它目前支持Llama和Mistral架构的模型。
- en: 'To use it, clone the repository locally:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用它，请在本地克隆该仓库：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The script expects one argument which is a configuration file in the JSON format.
    They propose several examples here:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本期望传入一个参数，即一个JSON格式的配置文件。它们在此处提供了几个示例：
- en: '[train_configs/mntp](https://github.com/McGill-NLP/llm2vec/tree/main/train_configs/mntp)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[train_configs/mntp](https://github.com/McGill-NLP/llm2vec/tree/main/train_configs/mntp)'
- en: 'For Llama 3 8B, I made the following configuration:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Llama 3 8B，我做了以下配置：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The script loads the model with bfloat16 parameters. I set the batch sizes per
    device to one so that the training can fit on a 24 GB GPU.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本会以bfloat16参数加载模型。我将每个设备的批次大小设置为1，以便训练可以适应24GB的GPU。
- en: 'Then, we can start MNTP training:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以开始MNTP训练：
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This should take 4 days using a 24 GB GPU, such as the L4 of Google Colab, for
    three epochs. One epoch could be enough if you can’t wait that long.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用24GB的GPU（例如Google Colab的L4）进行三轮训练需要4天时间。如果你等不及这么久，可能一轮就足够了。
- en: After MNTP training, the model should yield much better results, especially
    for retrieval tasks.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在MNTP训练后，模型应能产生更好的结果，特别是在检索任务中。
- en: For the last step which is SimCSE training, the authors didn’t release their
    code yet but mentioned that they will.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于最后一步，即SimCSE训练，作者尚未发布代码，但提到他们会发布。
- en: Setting up Llama 3 Text Embedding Model for RAG
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置Llama 3文本嵌入模型用于RAG
- en: The embedding model created in the previous section is ready to be used in a
    RAG pipeline. For instance, you can load it with [sentence-transformers](https://github.com/UKPLab/sentence-transformers)
    (Apache 2.0 license).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一部分创建的嵌入模型已准备好用于RAG流水线。例如，你可以通过[sentence-transformers](https://github.com/UKPLab/sentence-transformers)（Apache
    2.0许可证）加载它。
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'If you use [LlamaIndex](https://github.com/run-llama/llama_index) (MIT license),
    you can set the HuggingFaceEmbedding model:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用[LlamaIndex](https://github.com/run-llama/llama_index)（MIT许可证），可以设置HuggingFaceEmbedding模型：
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: I set device=’cpu’ but using the CPU makes the RAG system slower. You can remove
    this argument to run it on the GPU. However, note that the model is loaded in
    full precision. It doesn’t fit on a consumer GPU.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我设置了device='cpu'，但使用CPU会使RAG系统运行变慢。你可以移除此参数以在GPU上运行。然而，请注意，模型是以全精度加载的，这使得它无法适应普通消费者的GPU。
- en: 'I explain in detail how to set up a RAG system with LlamaIndex in this tutorial:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我详细解释了如何使用LlamaIndex设置RAG系统：
- en: '[](https://newsletter.kaitchup.com/p/rag-for-mistral-7b-instruct-with?source=post_page-----8448005f99aa--------------------------------)
    [## RAG for Mistral 7B Instruct with LlamaIndex and Transformers'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://newsletter.kaitchup.com/p/rag-for-mistral-7b-instruct-with?source=post_page-----8448005f99aa--------------------------------)
    [## RAG for Mistral 7B Instruct with LlamaIndex and Transformers'
- en: RAG on budget
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预算有限的RAG
- en: newsletter.kaitchup.com](https://newsletter.kaitchup.com/p/rag-for-mistral-7b-instruct-with?source=post_page-----8448005f99aa--------------------------------)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: newsletter.kaitchup.com](https://newsletter.kaitchup.com/p/rag-for-mistral-7b-instruct-with?source=post_page-----8448005f99aa--------------------------------)
- en: Conclusion
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: With LLM2Vec, we can now use an LLM as a text embedding model. The conversion
    is simple and fast. Using one single model for both the generation and the retrieval
    in a RAG system is appealing as we don’t need to search for an additional embedding
    model.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LLM2Vec，我们现在可以将LLM用作文本嵌入模型。转换过程简单且快速。使用同一个模型同时进行生成和检索在RAG系统中非常有吸引力，因为我们不需要寻找额外的嵌入模型。
- en: However, embedding models simply extracted from LLMs tend to underperform regular
    embedding models. The authors of LLM2Vec propose new training objectives, MNTP
    and SimCSE, to train the embedding model extracted from LLMs. I found this training
    to be costly but can yield significantly better embedding models according to
    the authors.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从LLM中提取的嵌入模型往往表现不如常规嵌入模型。LLM2Vec的作者提出了新的训练目标，MNTP和SimCSE，用于训练从LLM中提取的嵌入模型。根据作者的说法，尽管这种训练成本较高，但能显著提升嵌入模型的表现。
- en: 'To support my work, consider subscribing to my newsletter for more articles/tutorials
    on recent advances in AI:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持我的工作，考虑订阅我的新闻通讯，以获取更多关于AI最新进展的文章/教程：
- en: '[](https://newsletter.kaitchup.com/?source=post_page-----8448005f99aa--------------------------------)
    [## The Kaitchup - AI on a Budget | Benjamin Marie | Substack'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://newsletter.kaitchup.com/?source=post_page-----8448005f99aa--------------------------------)
    [## Kaitchup - 预算内的AI | 本杰明·马里 | Substack'
- en: Weekly tutorials, tips, and news on fine-tuning, running, and serving large
    language models on your computer. The…
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每周教程、技巧和新闻，涉及在您的计算机上微调、运行和服务大型语言模型的内容。…
- en: newsletter.kaitchup.com](https://newsletter.kaitchup.com/?source=post_page-----8448005f99aa--------------------------------)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: newsletter.kaitchup.com](https://newsletter.kaitchup.com/?source=post_page-----8448005f99aa--------------------------------)
