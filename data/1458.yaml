- en: Multi-Head Attention — Formally Explained and Defined
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多头注意力 — 形式化解释与定义
- en: 原文：[https://towardsdatascience.com/multi-head-attention-formally-explained-and-defined-89dc70ce84bd?source=collection_archive---------9-----------------------#2024-06-11](https://towardsdatascience.com/multi-head-attention-formally-explained-and-defined-89dc70ce84bd?source=collection_archive---------9-----------------------#2024-06-11)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/multi-head-attention-formally-explained-and-defined-89dc70ce84bd?source=collection_archive---------9-----------------------#2024-06-11](https://towardsdatascience.com/multi-head-attention-formally-explained-and-defined-89dc70ce84bd?source=collection_archive---------9-----------------------#2024-06-11)
- en: A comprehensive and detailed formalisation of multi-head attention
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多头注意力的全面且详细的形式化
- en: '[](https://medium.com/@jmpion?source=post_page---byline--89dc70ce84bd--------------------------------)[![Jean
    Meunier-Pion](../Images/2d97f6d450ad143cbcb75a701204cc72.png)](https://medium.com/@jmpion?source=post_page---byline--89dc70ce84bd--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--89dc70ce84bd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--89dc70ce84bd--------------------------------)
    [Jean Meunier-Pion](https://medium.com/@jmpion?source=post_page---byline--89dc70ce84bd--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jmpion?source=post_page---byline--89dc70ce84bd--------------------------------)[![Jean
    Meunier-Pion](../Images/2d97f6d450ad143cbcb75a701204cc72.png)](https://medium.com/@jmpion?source=post_page---byline--89dc70ce84bd--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--89dc70ce84bd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--89dc70ce84bd--------------------------------)
    [Jean Meunier-Pion](https://medium.com/@jmpion?source=post_page---byline--89dc70ce84bd--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--89dc70ce84bd--------------------------------)
    ·9 min read·Jun 11, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--89dc70ce84bd--------------------------------)
    ·阅读时长9分钟·2024年6月11日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/06dd5a838c54bd29e7e3bcf3512bced8.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06dd5a838c54bd29e7e3bcf3512bced8.png)'
- en: Robot with multiple heads, paying attention — Image by author (AI-generated,
    Microsoft Copilot)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 带有多个头部的机器人，正在集中注意力 — 作者提供的图像（由AI生成，Microsoft Copilot）
- en: Multi-head attention plays a crucial role in transformers, which have revolutionized
    Natural Language Processing (NLP). Understanding this mechanism is a necessary
    step to getting a clearer picture of current state-of-the-art language models.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力在变换器（Transformers）中扮演着至关重要的角色，而变换器已经彻底改变了自然语言处理（NLP）。理解这个机制是了解当前最先进语言模型的必要步骤。
- en: Even though this concept was introduced several years ago and has been widely
    used and discussed since then, ambiguous notations along with a lack of formal
    definitions have prevented newcomers from quickly demystifying the multi-head
    attention mechanism.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个概念几年前已经提出，并且此后被广泛使用和讨论，但模糊的符号表示和缺乏正式定义，导致新手难以迅速揭开多头注意力机制的谜团。
- en: In this article, the goal is to offer a comprehensive and unambiguous formalization
    of multi-head attention, to make this mechanism easily understandable.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的目标是提供一个全面且清晰的多头注意力形式化定义，使这一机制更容易理解。
- en: Since, to better understand new concepts, it is crucial to use them by yourself,
    this article comes with several exercises/questions *(along with solutions)* to
    precisely get what happens with multi-head attention.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 由于要更好地理解新概念，关键是要自己动手实践，本文附带了几个练习/问题（*以及解答*），以帮助你准确理解多头注意力机制的作用。
- en: '**Disclaimer:** Before starting with the definitions and explanations of multi-head
    attention, note that the lack of LaTeX support forced me to turn the equations
    into images to display the different mathematical objects.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**免责声明：** 在开始多头注意力的定义和解释之前，请注意，由于缺乏对LaTeX的支持，我不得不将方程转化为图像，以显示不同的数学对象。'
- en: Input
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入
