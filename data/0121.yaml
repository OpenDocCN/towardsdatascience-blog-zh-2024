- en: Pytorch Introduction — Enter NonLinear Functions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch入门 — 进入非线性函数
- en: 原文：[https://towardsdatascience.com/pytorch-introduction-enter-nonlinear-functions-4dd893845592?source=collection_archive---------7-----------------------#2024-01-12](https://towardsdatascience.com/pytorch-introduction-enter-nonlinear-functions-4dd893845592?source=collection_archive---------7-----------------------#2024-01-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/pytorch-introduction-enter-nonlinear-functions-4dd893845592?source=collection_archive---------7-----------------------#2024-01-12](https://towardsdatascience.com/pytorch-introduction-enter-nonlinear-functions-4dd893845592?source=collection_archive---------7-----------------------#2024-01-12)
- en: Continuing the Pytorch series, in this post we’ll learn about how non-linearities
    help solve complex problems in the context of neural networks
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 继续PyTorch系列，在本篇文章中，我们将学习非线性如何帮助解决神经网络中的复杂问题。
- en: '[](https://ivopbernardo.medium.com/?source=post_page---byline--4dd893845592--------------------------------)[![Ivo
    Bernardo](../Images/39887b6f3e63a67c0545e87962ad5df0.png)](https://ivopbernardo.medium.com/?source=post_page---byline--4dd893845592--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4dd893845592--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4dd893845592--------------------------------)
    [Ivo Bernardo](https://ivopbernardo.medium.com/?source=post_page---byline--4dd893845592--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ivopbernardo.medium.com/?source=post_page---byline--4dd893845592--------------------------------)[![Ivo
    Bernardo](../Images/39887b6f3e63a67c0545e87962ad5df0.png)](https://ivopbernardo.medium.com/?source=post_page---byline--4dd893845592--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4dd893845592--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4dd893845592--------------------------------)
    [Ivo Bernardo](https://ivopbernardo.medium.com/?source=post_page---byline--4dd893845592--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4dd893845592--------------------------------)
    ·8 min read·Jan 12, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4dd893845592--------------------------------)
    ·阅读时长8分钟·2024年1月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ee5af60923412013e898de28da50645c.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee5af60923412013e898de28da50645c.png)'
- en: Neural Networks are Powerful Architectures able to Solve Complex Problems —
    Image generated by AI
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是能够解决复杂问题的强大架构 — 由AI生成的图像
- en: In the last blog posts of the PyTorch Introduction series, we spoke about [introduction
    to tensor objects](/pytorch-introduction-tensors-and-tensor-calculations-412ff818bd5b?sk=2cf4d44549664fc647baa3455e9d78e8)
    and building a [simple linear model using PyTorch](/pytorch-introduction-building-your-first-linear-model-d868a8681a41?sk=152e552c4338cd1f06c64aedde5838fa).
    The first two blog posts of the series were the start of a larger objective where
    we understand deep learning at a deeper level (pun intended). To do that, we are
    using one of the most famous libraries in the machine learning world, PyTorch.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch入门系列的上一篇博客中，我们讨论了[张量对象介绍](/pytorch-introduction-tensors-and-tensor-calculations-412ff818bd5b?sk=2cf4d44549664fc647baa3455e9d78e8)以及如何使用PyTorch构建[简单线性模型](/pytorch-introduction-building-your-first-linear-model-d868a8681a41?sk=152e552c4338cd1f06c64aedde5838fa)。该系列的前两篇博客是一个更大目标的起点，我们希望从更深的层次理解深度学习（双关语）。为了实现这一目标，我们正在使用机器学习领域最著名的库之一——PyTorch。
- en: When building our simple linear model, we’ve understood that PyTorch is able
    to solve simple regression problems — but it wouldn’t be a deep learning library
    if these would be the only problems that it could solve, right? In this blog post,
    we are going to go a bit deeper into the complexities of Neural Networks and learn
    a bit about how to implement a neural network that deals with non-linear patterns
    and solve complex problems by introducing the concept of activation functions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建我们的简单线性模型时，我们已经理解到PyTorch能够解决简单的回归问题——但如果它只能解决这些问题，它就不配被称为深度学习库了，对吧？在这篇博客中，我们将深入探讨神经网络的复杂性，了解如何通过引入激活函数的概念来实现处理非线性模式的神经网络，进而解决复杂问题。
- en: This blog post (and series) is loosely based on the structure of [https://www.learnpytorch.io/](https://www.learnpytorch.io/),
    an excellent resource to learn PyTorch that I recommend you to check out!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本博客（及系列）在结构上大致借鉴了[https://www.learnpytorch.io/](https://www.learnpytorch.io/)，这是一个学习PyTorch的优秀资源，强烈推荐你去看看！
- en: 'In a nutshell, in this blog post, we will:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在这篇博客中，我们将：
- en: Understand how activation functions in PyTorch work.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解PyTorch中的激活函数是如何工作的。
- en: Explore how we can solve a non-linear problem using Neural Networks.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索我们如何利用神经网络解决非线性问题。
- en: Let’s start!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 开始吧！
- en: Setting up our Data
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置我们的数据
- en: In this blog post, we’ll use the Heart Failure prediction dataset available
    at [Kaggle](https://www.kaggle.com/datasets/andrewmvd/heart-failure-clinical-data/data).
    The dataset contains data from 299 patients with heart failure and specifies different
    variables about their health status. The goal is to predict if the patients died
    (column named DEATH_EVENT) and understand if there’s any signal in the patient’s
    Age, Anaemia level, ejection fraction or other health data that can predict the
    death outcome.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博文中，我们将使用[Kaggle](https://www.kaggle.com/datasets/andrewmvd/heart-failure-clinical-data/data)提供的心脏衰竭预测数据集。该数据集包含来自299名心脏衰竭患者的数据，并指定了他们健康状态的不同变量。目标是预测患者是否死亡（名为DEATH_EVENT的列），并了解是否有任何信号（例如患者的年龄、贫血水平、射血分数或其他健康数据）可以预测死亡结果。
- en: 'Let’s start by loading our data using `pandas` :'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先使用`pandas`加载数据：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s see the `head` of our DataFrame:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看数据框（DataFrame）的`head`：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/32ee9e510675e85bbb1586d824350020.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/32ee9e510675e85bbb1586d824350020.png)'
- en: Head of the heart_failure_data — Image by Author
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: heart_failure_data的前几行 — 图源：作者
- en: 'Our goal is to predict the `DEATH_EVENT` binary column, available at the end
    of the DataFrame:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是预测数据框最后一列的`DEATH_EVENT`二进制列：
- en: '![](../Images/d3cf4c22ebc09f761de3d755612c93f9.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d3cf4c22ebc09f761de3d755612c93f9.png)'
- en: Head of the heart_failure_data, extra columns — Image by Author
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: heart_failure_data的前几行，附加列 — 图源：作者
- en: 'First, let’s standardize our data using `StandardScaler` — although not as
    important as in distance algorithms, standardizing the data will be extremely
    helpful to improve the gradient descent algorithm we’ll use during the training
    process. We’ll want to scale all but the last column (the target):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们使用`StandardScaler`对数据进行标准化——虽然在距离算法中标准化不如重要，但标准化数据对于提升我们在训练过程中使用的梯度下降算法将极为有用。我们希望对除了最后一列（目标列）之外的所有列进行缩放：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, we can perform a simple train-test split. We’ll use `sklearn` to do that
    and leave 20% of our dataset for testing purposes:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以执行一个简单的训练-测试拆分。我们将使用`sklearn`来完成这项工作，并将20%的数据集用于测试目的：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We also need to transform our data into `torch.tensor` :'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要将数据转换为`torch.tensor`：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Having our data ready, time to fit our Neural Network!
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备好了，现在是时候训练我们的神经网络了！
- en: Training a Vanilla Linear Neural Network
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练一个基本的线性神经网络
- en: 'With our data in-place, it’s time to train our first Neural Network. We’ll
    use a similar architecture to what we’ve done in the last blog post of the series,
    using a Linear version of our Neural Network with the ability to handle [linear
    patterns](/pytorch-introduction-building-your-first-linear-model-d868a8681a41?sk=152e552c4338cd1f06c64aedde5838fa):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备好后，到了训练我们的第一个神经网络的时刻。我们将使用与上篇系列博文中相似的架构，使用一个线性版本的神经网络来处理[线性模式](/pytorch-introduction-building-your-first-linear-model-d868a8681a41?sk=152e552c4338cd1f06c64aedde5838fa)：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This neural network uses the `nn.Linear`module from `pytorch` to create a Neural
    Network with 1 deep layer (one input layer, a deep layer and an output layers).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这个神经网络使用`pytorch`中的`nn.Linear`模块来创建一个具有1个深度层（一个输入层，一个深度层和一个输出层）的神经网络。
- en: 'Although we can create our own class inheriting from `nn.Module` , we can also
    use (more elegantly) the `nn.Sequential` constructor to do the same:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可以创建自己的类继承自`nn.Module`，但我们也可以使用（更优雅的方式）`nn.Sequential`构造函数来实现相同的功能：
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/fd150d50d7f417cd8b3d8cfafc4c0811.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd150d50d7f417cd8b3d8cfafc4c0811.png)'
- en: model_0 Neural Network Architecture — Image by Author
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: model_0神经网络架构 — 图源：作者
- en: Cool! So our Neural Network contains a single inner layer with 5 neurons (this
    can be seen by the `out_features=5` on the first layer).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 酷！我们的神经网络包含一个具有5个神经元的单一内部层（这一点可以通过第一层的`out_features=5`看出）。
- en: This inner layer receives the same number of connections from each input neuron.
    The 12 in `in_features` in the first layer reflects the number of features and
    the 1 in `out_features` of the second layer reflects the output (a single value
    raging from 0 to 1).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这个内部层从每个输入神经元接收相同数量的连接。第一层的`in_features`中的12反映了特征的数量，而第二层的`out_features`中的1则反映了输出（一个范围从0到1的单一值）。
- en: To train our Neural Network, we’ll define a loss function and an optimizer.
    We’ll define `BCEWithLogitsLoss` ([PyTorch 2.1 documentation](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html))
    as this loss function (torch implementation of Binary Cross-Entropy, appropriate
    for classification problems) and Stochastic Gradient Descent as the optimizer
    (using `torch.optim.SGD` ).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练我们的神经网络，我们将定义一个损失函数和一个优化器。我们将使用`BCEWithLogitsLoss`（[PyTorch 2.1 文档](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)）作为损失函数（适用于分类问题的二元交叉熵的PyTorch实现），并使用随机梯度下降作为优化器（使用`torch.optim.SGD`）。
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, as I’ll also want to calculate the accuracy for every epoch of training
    process, we’ll design a function to calculate that:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，由于我还想计算每个训练轮次的准确率，我们将设计一个函数来计算准确率：
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Time to train our model! Let’s train our model for 1000 epochs and see how
    a simple linear network is able to deal with this data:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候训练我们的模型了！让我们训练模型1000个轮次，看看一个简单的线性网络如何处理这些数据：
- en: '[PRE9]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Unfortunately the neural network we’ve just built is not good enough to solve
    this problem. Let’s see the evolution of training and test accuracy:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们刚刚构建的神经网络还不足以解决这个问题。让我们看看训练和测试准确率的变化：
- en: '![](../Images/4aab993292ac6a0ee659bab76fbf33e0.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4aab993292ac6a0ee659bab76fbf33e0.png)'
- en: Train and Test Accuracy through the Epochs — Image by Author
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和测试准确率随训练轮次的变化 — 图片来自作者
- en: '*(I’m plotting accuracy instead of loss as it is easier to interpret in this
    problem)*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*(我选择绘制准确率而非损失，因为在这个问题中准确率更容易解读)*'
- en: Interestingly, our Neural Network isn’t able improve much of the test set accuracy.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，我们的神经网络并未能显著提高测试集的准确率。
- en: 'With the knowledge have from previous blog posts, we can try to add more layers
    and neurons to our neural network. Let’s try to do both and see the outcome:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 通过之前博客中获得的知识，我们可以尝试为神经网络添加更多层和神经元。让我们试试看，看看结果如何：
- en: '[PRE10]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/de168d8143ebbf2f0d3a4fa97a7200fe.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de168d8143ebbf2f0d3a4fa97a7200fe.png)'
- en: deeper_model Neural Network Architecture — Image by Author
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: deeper_model 神经网络架构 — 图片来自作者
- en: 'Although our deeper model is a bit more complex with an extra layer and more
    neurons, that doesn’t translate into more performance in the network:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的更深层模型在增加了额外层和更多神经元后变得更加复杂，但这并没有转化为网络性能的提升：
- en: '![](../Images/1dbe817cbe76b1f309b8e83ecab499f3.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1dbe817cbe76b1f309b8e83ecab499f3.png)'
- en: Train and Test Accuracy through the Epochs for deeper model— Image by Author
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 更深层模型的训练和测试准确率随训练轮次的变化 — 图片来自作者
- en: Even though our model is more complex, that doesn’t really bring more accuracy
    to our classification problem.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的模型更复杂，但这并没有为我们的分类问题带来更多准确性。
- en: To be able to achieve more performance, we need to unlock a new feature of Neural
    Networks — activation functions!
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够获得更好的性能，我们需要解锁神经网络的一个新特性 — 激活函数！
- en: Enter NonLinearities!
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进入非线性！
- en: If making our model wider and larger didn’t bring much improvement, there must
    be something else that we can do with Neural Networks that will be able to improve
    its performance, right?
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果单纯通过扩展模型的宽度和深度没有带来显著的改进，那么一定还有其他方法可以改善神经网络的性能，对吧？
- en: 'That’s where activation functions can be used! In our example, we’ll return
    to our simpler model, but this time with a twist:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是激活函数可以发挥作用的地方！在我们的例子中，我们将回到一个更简单的模型，但这次会加入一些变化：
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'What’s the difference between this model and the first one? The difference
    is that we added a new block to our neural network — `nn.ReLU` . The [rectified
    linear unit](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) is an
    activation function that will change the calculation in each of the weights of
    the Neural Network:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型与第一个模型有什么不同？不同之处在于我们在神经网络中加入了一个新的模块 — `nn.ReLU`。 [修正线性单元](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))
    是一种激活函数，它将改变神经网络中每个权重的计算：
- en: '![](../Images/16289c0bd3c32ce0f22f2dc2cc5c1dab.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/16289c0bd3c32ce0f22f2dc2cc5c1dab.png)'
- en: ReLU Illustrative Example — Image by Author
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU示例 — 图片来自作者
- en: Every value that goes through our weights in the Neural Network will be computed
    against this function. If the value of the feature times the weight is negative,
    the value is set to 0, otherwise the calculated value is assumed. Just this small
    change adds a lot of power to a Neural Network architecture — in `torch` we have
    different activation functions we can use such as `nn.ReLU` , `nn.Tanh` or `nn.ELU`
    . For an overview of all activation functions, check this [link](https://pytorch.org/docs/stable/nn.html#non-linear-activations-other).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 每个通过我们神经网络权重的值都会根据这个函数进行计算。如果特征的值与权重相乘后是负数，值将被设为0，否则假定计算值为其本身。仅仅这一小的改变就为神经网络架构增添了很大的能力——在`torch`中，我们可以使用不同的激活函数，如`nn.ReLU`、`nn.Tanh`或`nn.ELU`。想了解所有激活函数的概览，请查看这个[链接](https://pytorch.org/docs/stable/nn.html#non-linear-activations-other)。
- en: 'Our neural network architecture contains a small twist, at the moment:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的神经网络架构目前包含一个小小的变化：
- en: '![](../Images/a74076b4998dd59c65d9b8f984198f6d.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a74076b4998dd59c65d9b8f984198f6d.png)'
- en: Neural Network Architecture — ReLU — Image by Author
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络架构 — ReLU — 图像来源：作者
- en: With this small twist in the Neural Network, every value coming from the first
    layer (represented by `nn.Linear(in_features=12, out_features=5)` ) will have
    to go through the “ReLU” test.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在神经网络中做出这个小小的变化，来自第一层的每个值（由`nn.Linear(in_features=12, out_features=5)`表示）都必须经过“ReLU”测试。
- en: 'Let’s see the impact of fitting this architecture on our data:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看将这个架构应用到数据上的影响：
- en: '![](../Images/7ceae486f686af1f4c222a79cf48aea0.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ceae486f686af1f4c222a79cf48aea0.png)'
- en: Train and Test Accuracy through the Epochs for non-linear model — Image by Author
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性模型的训练和测试准确度随epoch变化 — 图像来源：作者
- en: Cool! Although we see some of the performance degrading after 800 epochs, this
    model doesn’t exhibit overfitting as the previous ones. Keep in mind that our
    dataset is very small, so there’s a chance that our results are better just by
    randomness. Nevertheless, adding activation functions to your `torch` models definitely
    has a huge impact in terms of performance, training and generalization, particularly
    when you have a lot of data to train on.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 很棒！尽管我们看到在800个epoch之后性能有所下降，但这个模型并没有像之前的模型那样出现过拟合。请记住，我们的数据集非常小，因此我们的结果有可能只是随机性带来的更好表现。尽管如此，将激活函数添加到`torch`模型中确实在性能、训练和泛化能力方面有着巨大的影响，尤其是在你有大量数据进行训练时。
- en: 'Now that you know the power of non-linear activation functions, it’s also relevant
    to know:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了非线性激活函数的强大功能，接下来了解以下内容也很重要：
- en: You can add activation functions to every layer of the Neural Network.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以将激活函数添加到神经网络的每一层。
- en: Different activation functions have [different effects on your performance and
    training process](https://www.v7labs.com/blog/neural-networks-activation-functions).
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的激活函数对你的性能和训练过程有[不同的影响](https://www.v7labs.com/blog/neural-networks-activation-functions)。
- en: '`torch` elegantly gives you the ability to add activation functions in-between
    layers by leveraging the `nn` module.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch`优雅地让你可以通过利用`nn`模块，在层与层之间添加激活函数。'
- en: Conclusion
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Thank you for taking the time to read this post! In this blog post, we’ve checked
    how to incorporate activation functions inside `torch` Neural Network paradigm.
    **Another important concept that we’ve understood is that larger and wider networks
    are not a synonym of better performance.**
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你抽出时间阅读这篇文章！在这篇博客中，我们探讨了如何将激活函数应用到`torch`神经网络架构中。**我们还理解了另一个重要概念，那就是更大、更宽的网络并不意味着更好的性能。**
- en: Activation functions help us deal with problems that are solved with more complex
    architectures (again, more complex is different than larger/wider). They help
    with generalization power and help us converge our solution faster, being one
    of the major features of neural network models.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数帮助我们解决通过更复杂架构才能解决的问题（再说一遍，更复杂并不等于更大或更宽）。它们有助于提高泛化能力，帮助我们更快地收敛解决方案，是神经网络模型的主要特性之一。
- en: And because of their widespread use on a variety of neural models, `torch` has
    got our back with its cool implementation of different functions inside the `nn.Sequential`
    modules!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它们在各种神经模型中被广泛使用，`torch`通过其在`nn.Sequential`模块中的不同函数实现，给我们提供了很大的帮助！
- en: Hope you’ve enjoyed and see you on the next PyTorch post! You can check the
    first PyTorch blog posts [here](/pytorch-introduction-tensors-and-tensor-calculations-412ff818bd5b?sk=2cf4d44549664fc647baa3455e9d78e8)
    and [here](/pytorch-introduction-building-your-first-linear-model-d868a8681a41).
    I also recommend that you visit [PyTorch Zero to Mastery Course](https://www.learnpytorch.io/01_pytorch_workflow/),
    an amazing free resource that inspired the methodology behind this post.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你喜欢这篇文章，下一篇 PyTorch 文章见！你可以查看第一篇 PyTorch 博客文章 [这里](/pytorch-introduction-tensors-and-tensor-calculations-412ff818bd5b?sk=2cf4d44549664fc647baa3455e9d78e8)
    和 [这里](/pytorch-introduction-building-your-first-linear-model-d868a8681a41)。我还推荐你访问
    [PyTorch 零基础到精通课程](https://www.learnpytorch.io/01_pytorch_workflow/)，这是一个令人惊叹的免费资源，启发了这篇文章的写作方法。
- en: Also, I would love to see you on my newly created YouTube Channel — the [Data
    Journey](https://www.youtube.com/@TheDataJourney42) where I’ll be adding content
    on Data Science and Machine Learning.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我很高兴能在我新创建的 YouTube 频道——[数据之旅](https://www.youtube.com/@TheDataJourney42)上见到你，在那里我将添加有关数据科学和机器学习的内容。
- en: '*[The dataset used in this blog post is under licence Creative Commons* [*https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1023-5#Sec2*](https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1023-5#Sec2)*]*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*[本博客文章中使用的数据集属于创作共用许可协议* [*https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1023-5#Sec2*](https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1023-5#Sec2)*]*'
