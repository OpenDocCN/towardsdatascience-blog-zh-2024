- en: 'Paper Walkthrough: Vision Transformer (ViT)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 论文解析：Vision Transformer（ViT）
- en: 原文：[https://towardsdatascience.com/paper-walkthrough-vision-transformer-vit-c5dcf76f1a7a?source=collection_archive---------2-----------------------#2024-08-13](https://towardsdatascience.com/paper-walkthrough-vision-transformer-vit-c5dcf76f1a7a?source=collection_archive---------2-----------------------#2024-08-13)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/paper-walkthrough-vision-transformer-vit-c5dcf76f1a7a?source=collection_archive---------2-----------------------#2024-08-13](https://towardsdatascience.com/paper-walkthrough-vision-transformer-vit-c5dcf76f1a7a?source=collection_archive---------2-----------------------#2024-08-13)
- en: Exploring Vision Transformer (ViT) through PyTorch Implementation from Scratch.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过从零开始的 PyTorch 实现探索 Vision Transformer（ViT）。
- en: '[](https://medium.com/@muhammad_ardi?source=post_page---byline--c5dcf76f1a7a--------------------------------)[![Muhammad
    Ardi](../Images/b59b3752bc33df0166eea834bbdb122f.png)](https://medium.com/@muhammad_ardi?source=post_page---byline--c5dcf76f1a7a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c5dcf76f1a7a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c5dcf76f1a7a--------------------------------)
    [Muhammad Ardi](https://medium.com/@muhammad_ardi?source=post_page---byline--c5dcf76f1a7a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@muhammad_ardi?source=post_page---byline--c5dcf76f1a7a--------------------------------)[![Muhammad
    Ardi](../Images/b59b3752bc33df0166eea834bbdb122f.png)](https://medium.com/@muhammad_ardi?source=post_page---byline--c5dcf76f1a7a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c5dcf76f1a7a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c5dcf76f1a7a--------------------------------)
    [Muhammad Ardi](https://medium.com/@muhammad_ardi?source=post_page---byline--c5dcf76f1a7a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c5dcf76f1a7a--------------------------------)
    ·19 min read·Aug 13, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c5dcf76f1a7a--------------------------------)
    ·阅读时长 19 分钟 ·2024年8月13日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/00cee3371ad23d4932c8e123ee815157.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/00cee3371ad23d4932c8e123ee815157.png)'
- en: Photo by [v2osk](https://unsplash.com/@v2osk?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[v2osk](https://unsplash.com/@v2osk?utm_source=medium&utm_medium=referral)
    via [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '**Introduction**'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**简介**'
- en: 'Vision Transformer — or commonly abbreviated as ViT — can be perceived as a
    breakthrough in the field of computer vision. When it comes to vision-related
    tasks, it is commonly addressed using CNN-based models which so far always perform
    better than any other type of neural networks. It wasn’t until 2020, when a paper
    titled “*An Image is Worth 16×16 Words: Transformers for Image Recognition at
    Scale*” written by Dosovitskiy et al. [1] was published, which offers better capability
    than CNN.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Vision Transformer —— 通常缩写为 ViT —— 可以被视为计算机视觉领域的一个突破。在与视觉相关的任务中，通常使用基于 CNN 的模型，而这些模型迄今为止总是比任何其他类型的神经网络表现得更好。直到2020年，一篇名为“*一张图片值16×16个单词：用于大规模图像识别的Transformer*”的论文由
    Dosovitskiy 等人 [1] 发布，才提供了比 CNN 更强的能力。
- en: A single convolution layer in CNN works by extracting features using kernels.
    Since the size of a kernel is relatively small as compared to the input image,
    hence it can only capture information contained within that small region. In other
    words, we can simply say that it focuses on extracting local features. To understand
    the global context of an image, a stack of multiple convolution layers is required.
    This problem is addressed by ViT as it directly captures global information from
    the initial layer. Thus, stacking multiple layers in ViT results in even more
    comprehensive information extraction.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CNN 中，单个卷积层通过使用卷积核提取特征。由于卷积核的大小相对于输入图像较小，因此它只能捕捉到该小区域内的信息。换句话说，我们可以简单地说它侧重于提取局部特征。为了理解图像的全局上下文，需要堆叠多个卷积层。ViT
    通过直接从初始层捕获全局信息来解决这个问题。因此，在 ViT 中堆叠多个层可以实现更加全面的信息提取。
