- en: Evaluations with Chat Formats
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨èŠå¤©æ ¼å¼çš„è¯„ä¼°
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/evaluations-with-chat-formats-7604067023c9?source=collection_archive---------6-----------------------#2024-02-21](https://towardsdatascience.com/evaluations-with-chat-formats-7604067023c9?source=collection_archive---------6-----------------------#2024-02-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/evaluations-with-chat-formats-7604067023c9?source=collection_archive---------6-----------------------#2024-02-21](https://towardsdatascience.com/evaluations-with-chat-formats-7604067023c9?source=collection_archive---------6-----------------------#2024-02-21)
- en: Applying chat templates to generative LM evaluation tests
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å°†èŠå¤©æ¨¡æ¿åº”ç”¨äºç”Ÿæˆå¼è¯­è¨€æ¨¡å‹çš„è¯„ä¼°æµ‹è¯•
- en: '[](https://medium.com/@daniel_furman?source=post_page---byline--7604067023c9--------------------------------)[![Daniel
    Furman](../Images/f7a1b4c6239ede8bb01e50f167931719.png)](https://medium.com/@daniel_furman?source=post_page---byline--7604067023c9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7604067023c9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7604067023c9--------------------------------)
    [Daniel Furman](https://medium.com/@daniel_furman?source=post_page---byline--7604067023c9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@daniel_furman?source=post_page---byline--7604067023c9--------------------------------)[![Daniel
    Furman](../Images/f7a1b4c6239ede8bb01e50f167931719.png)](https://medium.com/@daniel_furman?source=post_page---byline--7604067023c9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7604067023c9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7604067023c9--------------------------------)
    [Daniel Furman](https://medium.com/@daniel_furman?source=post_page---byline--7604067023c9--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7604067023c9--------------------------------)
    Â·7 min readÂ·Feb 21, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7604067023c9--------------------------------)
    Â·7åˆ†é’Ÿé˜…è¯»Â·2024å¹´2æœˆ21æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a8e714bc5984165c0816edc5f138bd2f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8e714bc5984165c0816edc5f138bd2f.png)'
- en: Photo by [Google DeepMind](https://unsplash.com/@googledeepmind) on [Unsplash](https://unsplash.com/photos/a-close-up-of-a-metal-structure-made-of-wood-and-metal-pyET8SQTc0A)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± [Google DeepMind](https://unsplash.com/@googledeepmind) æä¾›ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/photos/a-close-up-of-a-metal-structure-made-of-wood-and-metal-pyET8SQTc0A)
- en: â€œ**Building solid evals should be the starting point** for any LLM-based system
    or product (as well as conventional machine learning systems).â€ â€” Eugene Yan,
    [link](https://eugeneyan.com/writing/llm-patterns/#how-to-apply-evals)
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œ**æ„å»ºæ‰å®çš„è¯„ä¼°åº”è¯¥æ˜¯ä»»ä½•åŸºäº LLM çš„ç³»ç»Ÿæˆ–äº§å“çš„èµ·ç‚¹**ï¼ˆä»¥åŠä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ ç³»ç»Ÿï¼‰ã€‚â€ â€” Eugene Yan, [é“¾æ¥](https://eugeneyan.com/writing/llm-patterns/#how-to-apply-evals)
- en: '**tl;dr**'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ç®€è¦æ€»ç»“**'
- en: Chat models are typically fine-tuned on datasets formatted with a prompt template.
    These chat templates are programmed recipes that convert a chat conversation into
    a single string. At prediction time, itâ€™s standard to match an LLMâ€™s expected
    chat format â€” not doing so is oft-noted as causing performance degradations [1].
    However, do we in fact see these degradations on evaluation benchmarks?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: èŠå¤©æ¨¡å‹é€šå¸¸åœ¨ä½¿ç”¨æç¤ºæ¨¡æ¿æ ¼å¼çš„æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒã€‚è¿™äº›èŠå¤©æ¨¡æ¿æ˜¯ç¼–ç¨‹å¥½çš„â€œé£Ÿè°±â€ï¼Œèƒ½å¤Ÿå°†ä¸€æ¬¡èŠå¤©å¯¹è¯è½¬åŒ–ä¸ºä¸€ä¸ªå­—ç¬¦ä¸²ã€‚åœ¨é¢„æµ‹æ—¶ï¼Œé€šå¸¸éœ€è¦åŒ¹é…å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœŸæœ›çš„èŠå¤©æ ¼å¼â€”â€”å¦‚æœä¸è¿™ä¹ˆåšï¼Œé€šå¸¸ä¼šè¢«æŒ‡å‡ºä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™
    [1]ã€‚ä½†æ˜¯ï¼Œå®é™…ä¸Šæˆ‘ä»¬æ˜¯å¦åœ¨è¯„ä¼°åŸºå‡†ä¸Šçœ‹åˆ°äº†è¿™äº›æ€§èƒ½ä¸‹é™ï¼Ÿ
- en: '**NB**: This blog post is intended for readers with basic familiarity with
    Python programming and neural language modeling.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„**ï¼šæœ¬åšå®¢é€‚åˆå…·æœ‰ Python ç¼–ç¨‹å’Œç¥ç»è¯­è¨€å»ºæ¨¡åŸºç¡€çŸ¥è¯†çš„è¯»è€…ã€‚'
- en: Introduction
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: 'If youâ€™ve built on top of OpenAIâ€™s chat API, the following code will be recognizable.
    Under the hood, this input is transformed into one tokenizable string via the
    [ChatML](https://github.com/MicrosoftDocs/azure-docs/blob/main/articles/ai-services/openai/includes/chat-markup-language.md#working-with-chat-markup-language-chatml)
    format:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å·²ç»åœ¨ OpenAI çš„èŠå¤© API ä¸Šæ„å»ºè¿‡åº”ç”¨ï¼Œä¸‹é¢çš„ä»£ç å°†æ˜¯ä½ ç†Ÿæ‚‰çš„ã€‚åº•å±‚ï¼Œè¿™äº›è¾“å…¥ä¼šé€šè¿‡ [ChatML](https://github.com/MicrosoftDocs/azure-docs/blob/main/articles/ai-services/openai/includes/chat-markup-language.md#working-with-chat-markup-language-chatml)
    æ ¼å¼è½¬æ¢æˆä¸€ä¸ªå¯åˆ†è¯çš„å­—ç¬¦ä¸²ï¼š
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'It turns out thereâ€™s a wide variety of chat templates across the LLM research
    community. Take an open-source model like `Mixtral-8x7B-Instruct-v0.1`*.* Itâ€™s
    format looks wildly different from `gpt-3.5-turbo` above:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: äº‹å®è¯æ˜ï¼ŒLLM ç ”ç©¶ç¤¾åŒºä¸­æœ‰å„ç§å„æ ·çš„èŠå¤©æ¨¡æ¿ã€‚ä»¥å¼€æºæ¨¡å‹ `Mixtral-8x7B-Instruct-v0.1`*.* ä¸ºä¾‹ï¼Œå®ƒçš„æ ¼å¼ä¸ä¸Šé¢æåˆ°çš„
    `gpt-3.5-turbo` çœ‹èµ·æ¥æˆªç„¶ä¸åŒï¼š
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Why bother with chat templates? Well, itâ€™s strongly advised to match the expected
    chat template at prediction time (for instance, see the info on â€œInstruction formatâ€
    at the [repo](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) for
    `Mixtral-8x7B-Instruct-v0.1`). And, with proprietary chat models like `gpt-3.5-turbo`,
    chat templates are often applied behind the scenes of an endpoint whether you
    like it or not!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆè¦ä½¿ç”¨èŠå¤©æ¨¡æ¿å‘¢ï¼Ÿå…¶å®ï¼Œå¼ºçƒˆå»ºè®®åœ¨é¢„æµ‹æ—¶åŒ¹é…æœŸæœ›çš„èŠå¤©æ¨¡æ¿ï¼ˆä¾‹å¦‚ï¼Œå‚è§[ä»“åº“](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)ä¸­çš„â€œæŒ‡ä»¤æ ¼å¼â€ä¿¡æ¯ï¼Œé’ˆå¯¹`Mixtral-8x7B-Instruct-v0.1`ï¼‰ã€‚è€Œä¸”ï¼Œå¯¹äºåƒ`gpt-3.5-turbo`è¿™æ ·çš„ä¸“æœ‰èŠå¤©æ¨¡å‹ï¼ŒèŠå¤©æ¨¡æ¿é€šå¸¸åœ¨ç«¯ç‚¹èƒŒåè‡ªåŠ¨åº”ç”¨ï¼Œæ— è®ºä½ æ˜¯å¦å–œæ¬¢ï¼
- en: But how do we know whether chat formatting is indeed improving our performance?
    Enter LM evals.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯æˆ‘ä»¬æ€ä¹ˆçŸ¥é“èŠå¤©æ ¼å¼æ˜¯å¦çœŸçš„åœ¨æé«˜æˆ‘ä»¬çš„æ€§èƒ½å‘¢ï¼Ÿè¿™å°±æ˜¯è¯­è¨€æ¨¡å‹è¯„ä¼°çš„ä½œç”¨ã€‚
- en: LM evals
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¯­è¨€æ¨¡å‹è¯„ä¼°
- en: 'Evaluations are used to measure an AI/ML modelâ€™s performance, and they can
    take many shapes and sizes. Evals include two core components: a dataset curated
    for a specific task and associated metric(s) measuring the modeling performance.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è¯„ä¼°ç”¨äºè¡¡é‡AI/MLæ¨¡å‹çš„æ€§èƒ½ï¼Œå®ƒä»¬å¯ä»¥æœ‰è®¸å¤šä¸åŒçš„å½¢å¼å’Œå¤§å°ã€‚è¯„ä¼°åŒ…æ‹¬ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šé’ˆå¯¹ç‰¹å®šä»»åŠ¡ç­–åˆ’çš„æ•°æ®é›†å’Œä¸ä¹‹ç›¸å…³çš„è¡¡é‡æ¨¡å‹æ€§èƒ½çš„æŒ‡æ ‡ã€‚
- en: Generative LM evals carry some additional nuances. For example, different frameworks
    measure text generation performance in different ways â€” even varying for the same
    eval ([reference](https://huggingface.co/blog/evaluating-mmlu-leaderboard)). When
    comparing scores across studies, itâ€™s therefore very important to confirm that
    the results were computed with the same code and config to avoid any errant analysis.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆæ€§è¯­è¨€æ¨¡å‹è¯„ä¼°åŒ…å«ä¸€äº›é¢å¤–çš„ç»†èŠ‚ã€‚ä¾‹å¦‚ï¼Œä¸åŒçš„æ¡†æ¶ä»¥ä¸åŒæ–¹å¼è¡¡é‡æ–‡æœ¬ç”Ÿæˆæ€§èƒ½â€”â€”å³ä½¿æ˜¯ç›¸åŒçš„è¯„ä¼°ä¹Ÿä¼šæœ‰æ‰€ä¸åŒï¼ˆ[å‚è€ƒ](https://huggingface.co/blog/evaluating-mmlu-leaderboard)ï¼‰ã€‚å› æ­¤ï¼Œåœ¨è·¨ç ”ç©¶æ¯”è¾ƒåˆ†æ•°æ—¶ï¼Œéå¸¸é‡è¦çš„ä¸€ç‚¹æ˜¯è¦ç¡®è®¤ç»“æœæ˜¯ä½¿ç”¨ç›¸åŒçš„ä»£ç å’Œé…ç½®è®¡ç®—çš„ï¼Œä»¥é¿å…é”™è¯¯åˆ†æã€‚
- en: 'The superb Instruction-Following Evaluation ([IFEval](https://arxiv.org/abs/2311.07911))
    [2] is used for our testing here. This eval includes 541 prompts that measures
    a language modelâ€™s ability to follow verifiable natural language instructions.
    Examples of these verifiable instructions include:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è¶…çº§çš„æŒ‡ä»¤éµå¾ªè¯„ä¼°ï¼ˆ[IFEval](https://arxiv.org/abs/2311.07911)ï¼‰[2]åœ¨è¿™é‡Œç”¨äºæˆ‘ä»¬çš„æµ‹è¯•ã€‚è¯¥è¯„ä¼°åŒ…æ‹¬541ä¸ªæç¤ºï¼Œç”¨æ¥è¡¡é‡è¯­è¨€æ¨¡å‹éµå¾ªå¯éªŒè¯è‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„èƒ½åŠ›ã€‚è¿™äº›å¯éªŒè¯æŒ‡ä»¤çš„ç¤ºä¾‹åŒ…æ‹¬ï¼š
- en: â€œWrite 450 to 500 wordsâ€, â€œyour entire output should be in JSON outputâ€, â€œinclude
    a title, and put it into two square brackets such as [[ title ]]â€
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œå†™450åˆ°500å­—â€ï¼Œâ€œä½ çš„æ‰€æœ‰è¾“å‡ºåº”è¯¥æ˜¯JSONæ ¼å¼â€ï¼Œâ€œåŒ…æ‹¬ä¸€ä¸ªæ ‡é¢˜ï¼Œå¹¶å°†å…¶æ”¾å…¥ä¸¤ä¸ªæ–¹æ‹¬å·ä¸­ï¼Œä¾‹å¦‚[[ title ]]â€
- en: 'For a given response and a verifiable instruction, we examine whether the instruction
    has been followed or not with the following four metrics:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºç»™å®šçš„å“åº”å’Œå¯éªŒè¯æŒ‡ä»¤ï¼Œæˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹å››ä¸ªæŒ‡æ ‡æ¥æ£€æŸ¥è¯¥æŒ‡ä»¤æ˜¯å¦å·²è¢«éµå¾ªï¼š
- en: '1\. **Prompt-level strict-accuracy**: The percentage of prompts that all verifiable
    instructions in each prompt are followed.'
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 1\. **æç¤ºçº§ä¸¥æ ¼å‡†ç¡®åº¦**ï¼šæ¯ä¸ªæç¤ºä¸­æ‰€æœ‰å¯éªŒè¯æŒ‡ä»¤éƒ½è¢«éµå¾ªçš„ç™¾åˆ†æ¯”ã€‚
- en: ''
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '2\. **Inst-level strict-accuracy**: The percentage of verifiable instructions
    that are followed.'
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 2\. **æŒ‡ä»¤çº§ä¸¥æ ¼å‡†ç¡®åº¦**ï¼šå¯éªŒè¯çš„æŒ‡ä»¤ä¸­è¢«éµå¾ªçš„ç™¾åˆ†æ¯”ã€‚
- en: ''
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '3\. **Prompt-level loose-accuracy**: Prompt-level accuracy computed with the
    loose criterion.'
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 3\. **æç¤ºçº§å®½æ¾å‡†ç¡®åº¦**ï¼šä½¿ç”¨å®½æ¾æ ‡å‡†è®¡ç®—çš„æç¤ºçº§å‡†ç¡®åº¦ã€‚
- en: ''
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '4\. **Inst-level loose-accuracy**: Instruction-level accuracy computed with
    a loose criterion.'
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 4\. **æŒ‡ä»¤çº§å®½æ¾å‡†ç¡®åº¦**ï¼šä½¿ç”¨å®½æ¾æ ‡å‡†è®¡ç®—çš„æŒ‡ä»¤çº§å‡†ç¡®åº¦ã€‚
- en: The average of these four metrics was computed here (Table 1), primarily to
    use a single metric that captures the most diverse signal available.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å››ä¸ªæŒ‡æ ‡çš„å¹³å‡å€¼åœ¨æ­¤è®¡ç®—ï¼ˆè¡¨æ ¼1ï¼‰ï¼Œä¸»è¦ç›®çš„æ˜¯ä½¿ç”¨ä¸€ä¸ªæ•æ‰æœ€å¹¿æ³›ä¿¡å·çš„å•ä¸€æŒ‡æ ‡ã€‚
- en: IFEval is an ideal test for exploring the impacts of chat templates, since the
    test is specifically designed to measure instruction-following capabilities on
    chat data. Another interesting line of questioning is whether chat templating
    positively impacts evals that arenâ€™t as well suited for chat data â€” a topic left
    for future research.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: IFEvalæ˜¯æ¢ç´¢èŠå¤©æ¨¡æ¿å½±å“çš„ç†æƒ³æµ‹è¯•ï¼Œå› ä¸ºè¯¥æµ‹è¯•ä¸“é—¨è®¾è®¡ç”¨æ¥è¡¡é‡åœ¨èŠå¤©æ•°æ®ä¸Šçš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚å¦ä¸€ä¸ªæœ‰è¶£çš„é—®é¢˜æ˜¯ï¼ŒèŠå¤©æ¨¡æ¿æ˜¯å¦å¯¹é‚£äº›ä¸å¤ªé€‚åˆèŠå¤©æ•°æ®çš„è¯„ä¼°äº§ç”Ÿç§¯æå½±å“â€”â€”è¿™æ˜¯ä¸€ä¸ªç•™å¾…æœªæ¥ç ”ç©¶çš„è¯é¢˜ã€‚
- en: Chat templates for IFEval
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: IFEvalçš„èŠå¤©æ¨¡æ¿
- en: 'Eleuther.AIâ€™s [lm-eval](https://github.com/EleutherAI/lm-evaluation-harness)
    is the de facto open-source package for LM evaluation. Since chat templating for
    more models is an oft-requested addition to the library, it was easy to sync up
    with other developers wanting to work on this feature in the ğŸ¤— model class specifically.
    At present, development is underway at the `add-chat-templating` branch ([link](https://github.com/EleutherAI/lm-evaluation-harness/tree/add-chat-templating)),
    spurred by issues #1098 ([link](https://github.com/EleutherAI/lm-evaluation-harness/issues/1098#issuecomment-1947116099))
    and #1209 ([link](https://github.com/EleutherAI/lm-evaluation-harness/issues/1209#issuecomment-1879966071)).
    When using this branch, we can apply chat formats to an eval as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Eleuther.AIçš„[lm-eval](https://github.com/EleutherAI/lm-evaluation-harness)æ˜¯äº‹å®ä¸Šçš„å¼€æºè¯­è¨€æ¨¡å‹è¯„ä¼°å·¥å…·åŒ…ã€‚ç”±äºæ›´å¤šæ¨¡å‹çš„èŠå¤©æ¨¡æ¿åŠŸèƒ½æ˜¯ç”¨æˆ·å¸¸è¯·æ±‚çš„æ–°å¢åŠŸèƒ½ï¼Œå› æ­¤æˆ‘ä»¬å¾ˆå®¹æ˜“ä¸å…¶ä»–å¼€å‘è€…åä½œï¼Œä¸“æ³¨äºåœ¨ğŸ¤—æ¨¡å‹ç±»ä¸­å®ç°è¿™ä¸€åŠŸèƒ½ã€‚ç›®å‰ï¼Œå¼€å‘å·¥ä½œæ­£åœ¨`add-chat-templating`åˆ†æ”¯ä¸­è¿›è¡Œï¼ˆ[é“¾æ¥](https://github.com/EleutherAI/lm-evaluation-harness/tree/add-chat-templating)ï¼‰ï¼Œç”±é—®é¢˜#1098ï¼ˆ[é“¾æ¥](https://github.com/EleutherAI/lm-evaluation-harness/issues/1098#issuecomment-1947116099)ï¼‰å’Œ#1209ï¼ˆ[é“¾æ¥](https://github.com/EleutherAI/lm-evaluation-harness/issues/1209#issuecomment-1879966071)ï¼‰æ¨åŠ¨ã€‚å½“ä½¿ç”¨æ­¤åˆ†æ”¯æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰å¦‚ä¸‹æ–¹å¼å°†èŠå¤©æ ¼å¼åº”ç”¨äºè¯„ä¼°ï¼š
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The newly introduced triggers `use_chat_template` and `system_prompt` appear
    to the right of `model_args` and control how the chat template is applied. In
    the branchâ€™s current experimental form, the code prints the first prompt before
    and after applying the chat template. Hereâ€™s what that looks like for the above
    code block:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æ–°å¼•å…¥çš„è§¦å‘å™¨`use_chat_template`å’Œ`system_prompt`å‡ºç°åœ¨`model_args`çš„å³ä¾§ï¼Œç”¨äºæ§åˆ¶èŠå¤©æ¨¡æ¿çš„åº”ç”¨æ–¹å¼ã€‚åœ¨å½“å‰åˆ†æ”¯çš„å®éªŒæ€§ç‰ˆæœ¬ä¸­ï¼Œä»£ç åœ¨åº”ç”¨èŠå¤©æ¨¡æ¿å‰åæ‰“å°ç¬¬ä¸€ä¸ªæç¤ºã€‚è¿™æ˜¯ä¸Šè¿°ä»£ç å—çš„æ•ˆæœï¼š
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The output has taken on the desired chat template!
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºå·²é‡‡ç”¨æ‰€éœ€çš„èŠå¤©æ¨¡æ¿ï¼
- en: 'We are now ready to A/B test the influence of chat templates on the IFEval.
    A handful of popular LLMs were selected for our experimentâ€” each with its own
    unique chat template. On the larger end we have the 70B parameter `Llama-2â€“70b-chat`,
    two variants of the same 47B parameter model, `Mixtral-8x7B-Instruct-v0.1` and
    `Nous-Hermes-2-Mixtral-8x7B-DPO`, as well as the 34B parameter `Nous-Hermes-2-Yi-34B`.
    On the smaller end we have three 7B parameter models: `Mistral-Instruct-7B-v0.2`,
    `Zephyr-7b-beta`, and `Starling-LM-7B-alpha`. As for the system prompt, a simple
    â€œYou are a helpful assistant.â€ was used for compatible models. More details about
    each of these seven models are included below [3].'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å‡†å¤‡è¿›è¡ŒA/Bæµ‹è¯•ï¼Œè¯„ä¼°èŠå¤©æ¨¡æ¿å¯¹IFEvalçš„å½±å“ã€‚æˆ‘ä»¬ä¸ºå®éªŒé€‰æ‹©äº†ä¸€äº›æµè¡Œçš„LLMï¼Œæ¯ä¸ªæ¨¡å‹éƒ½æœ‰è‡ªå·±ç‹¬ç‰¹çš„èŠå¤©æ¨¡æ¿ã€‚åœ¨è¾ƒå¤§çš„æ¨¡å‹æ–¹é¢ï¼Œæˆ‘ä»¬é€‰æ‹©äº†70Bå‚æ•°çš„`Llama-2â€“70b-chat`ï¼Œä¸¤ç§47Bå‚æ•°æ¨¡å‹çš„å˜ä½“ï¼Œ`Mixtral-8x7B-Instruct-v0.1`å’Œ`Nous-Hermes-2-Mixtral-8x7B-DPO`ï¼Œä»¥åŠ34Bå‚æ•°çš„`Nous-Hermes-2-Yi-34B`ã€‚åœ¨è¾ƒå°çš„æ¨¡å‹æ–¹é¢ï¼Œæˆ‘ä»¬æœ‰ä¸‰ä¸ª7Bå‚æ•°çš„æ¨¡å‹ï¼š`Mistral-Instruct-7B-v0.2`ã€`Zephyr-7b-beta`å’Œ`Starling-LM-7B-alpha`ã€‚è‡³äºç³»ç»Ÿæç¤ºï¼Œå…¼å®¹æ¨¡å‹ä½¿ç”¨äº†ç®€å•çš„æç¤ºâ€œä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚â€æ›´å¤šå…³äºè¿™ä¸ƒä¸ªæ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯è¯·å‚è§ä¸‹æ–‡[3]ã€‚
- en: 'And, without further delay, our results:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæ¯«ä¸æ‹–å»¶ï¼Œæˆ‘ä»¬çš„ç»“æœï¼š
- en: '![](../Images/89bcb4f71233c0c5252237b46374b882.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89bcb4f71233c0c5252237b46374b882.png)'
- en: '**Table 1**: Results from the A/B test on IFEval, sorted by model size descending
    ([link](https://docs.google.com/spreadsheets/d/1Tawz9IHH2B-_XWj-JjeVGmu-og60lgSSpMywrGxcj6Q/edit?usp=sharing)).
    See the â€œAdditional Notesâ€ section below for more details, such as links to the
    run logs. As per reproducibility, the experiments were executed with models in
    half precision bfloat16, a workstation equipped with 2x H100 80 GB SXM5 chips,
    and a fork of the lm-eval package at hash [0c0c314c0df4c10f35bf7c17dc80f745f8027e9b](https://github.com/EleutherAI/lm-evaluation-harness/tree/0c0c314c0df4c10f35bf7c17dc80f745f8027e9b).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¡¨1**ï¼šæ¥è‡ªIFEvalçš„A/Bæµ‹è¯•ç»“æœï¼ŒæŒ‰æ¨¡å‹å¤§å°é™åºæ’åˆ—ï¼ˆ[é“¾æ¥](https://docs.google.com/spreadsheets/d/1Tawz9IHH2B-_XWj-JjeVGmu-og60lgSSpMywrGxcj6Q/edit?usp=sharing)ï¼‰ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ä¸‹é¢çš„â€œé™„åŠ è¯´æ˜â€éƒ¨åˆ†ï¼Œä¾‹å¦‚è¿è¡Œæ—¥å¿—çš„é“¾æ¥ã€‚ä¸ºäº†ä¿è¯å¯é‡å¤æ€§ï¼Œå®éªŒåœ¨åŠç²¾åº¦bfloat16æ¨¡å‹ä¸Šæ‰§è¡Œï¼Œå·¥ä½œç«™é…ç½®äº†2ä¸ªH100
    80GB SXM5èŠ¯ç‰‡ï¼Œå¹¶ä½¿ç”¨äº†`lm-eval`åŒ…çš„åˆ†æ”¯ï¼Œå“ˆå¸Œå€¼ä¸º[0c0c314c0df4c10f35bf7c17dc80f745f8027e9b](https://github.com/EleutherAI/lm-evaluation-harness/tree/0c0c314c0df4c10f35bf7c17dc80f745f8027e9b)ã€‚'
- en: ğŸ”¥ Chat templates caused serious shakeup to IFEval scoring! `Nous-Hermes-2-Mixtral-8x7B-DPO`
    clocked in as the most performant model tested here, with an average score of
    ~63%. In contrast, `Zephyr-7b-beta` was the worst performing model yet had the
    largest boost from chat templating â€” a whopping +39%! As a reference, the IFEval
    paper reported `gpt-4` (Nov 2023) at an average score of ~81% and `PaLM 2S`(Aug
    2023) at ~51% [2].
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”¥ èŠå¤©æ¨¡æ¿å¯¹ IFEval è¯„åˆ†äº§ç”Ÿäº†é‡å¤§å½±å“ï¼`Nous-Hermes-2-Mixtral-8x7B-DPO` ä½œä¸ºæµ‹è¯•ä¸­è¡¨ç°æœ€å¥½çš„æ¨¡å‹ï¼Œå¹³å‡å¾—åˆ†çº¦ä¸º
    63%ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œ`Zephyr-7b-beta` æ˜¯è¡¨ç°æœ€å·®çš„æ¨¡å‹ï¼Œä½†å´ä»èŠå¤©æ¨¡æ¿ä¸­è·å¾—äº†æœ€å¤§çš„æå‡â€”â€”æƒŠäººçš„ +39%ï¼ä½œä¸ºå‚è€ƒï¼ŒIFEval è®ºæ–‡ä¸­æŠ¥å‘Šçš„
    `gpt-4`ï¼ˆ2023å¹´11æœˆï¼‰å¹³å‡å¾—åˆ†çº¦ä¸º 81%ï¼Œ`PaLM 2S`ï¼ˆ2023å¹´8æœˆï¼‰ä¸ºçº¦ 51% [2]ã€‚
- en: 'In sum, these results point to a couple key insights:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“æ¥è¯´ï¼Œè¿™äº›ç»“æœæ­ç¤ºäº†å‡ ä¸ªå…³é”®çš„æ´å¯Ÿï¼š
- en: Chat templating has a positive impact on instruction-following for open-source
    LLMs, the extent to which varies by model.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: èŠå¤©æ¨¡æ¿å¯¹å¼€æº LLM çš„æŒ‡ä»¤è·Ÿéšæœ‰ç§¯æå½±å“ï¼Œå…¶å½±å“ç¨‹åº¦å› æ¨¡å‹è€Œå¼‚ã€‚
- en: Open-source LLMs are less equipped at following natural language instructions
    than SOA proprietary models like `gpt-4`.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¼€æº LLM åœ¨éµå¾ªè‡ªç„¶è¯­è¨€æŒ‡ä»¤æ–¹é¢ä¸å¦‚ SOA ä¸“æœ‰æ¨¡å‹ï¼Œå¦‚ `gpt-4`ã€‚
- en: Conclusion
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: 'Chat templates caused a significant uplift in IFEval scores across the board
    in our experiment, as proven over a variety of formats and models. However, I
    donâ€™t necessarily expect these effects to generalize to all LM evals. To further
    explore the impacts of chat templating on benchmarks, next steps include experimentation
    with:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: èŠå¤©æ¨¡æ¿åœ¨æˆ‘ä»¬çš„å®éªŒä¸­æ˜¾è‘—æå‡äº† IFEval çš„è¯„åˆ†ï¼Œè¿™åœ¨å„ç§æ ¼å¼å’Œæ¨¡å‹ä¸­éƒ½å¾—åˆ°äº†è¯æ˜ã€‚ç„¶è€Œï¼Œæˆ‘å¹¶ä¸ä¸€å®šæœŸå¾…è¿™äº›æ•ˆæœèƒ½æ™®éé€‚ç”¨äºæ‰€æœ‰ LM è¯„ä¼°ã€‚ä¸ºäº†è¿›ä¸€æ­¥æ¢è®¨èŠå¤©æ¨¡æ¿å¯¹åŸºå‡†çš„å½±å“ï¼Œä¸‹ä¸€æ­¥åŒ…æ‹¬è¿›è¡Œä»¥ä¸‹å®éªŒï¼š
- en: more instruction-following evals similar to IFEval
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ›´å¤šç±»ä¼¼ IFEval çš„æŒ‡ä»¤è·Ÿéšè¯„ä¼°
- en: general-purpose evals such as those in ğŸ¤—â€™ [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€èˆ¬ç”¨é€”è¯„ä¼°ï¼Œä¾‹å¦‚ ğŸ¤— çš„[å¼€æ”¾LLMæ’è¡Œæ¦œ](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
- en: in-context retrieval evals like â€œ[Needle in a Haystack](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)â€
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸Šä¸‹æ–‡æ£€ç´¢è¯„ä¼°ï¼Œä¾‹å¦‚â€œ[Needle in a Haystack](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)â€
- en: and much, much more!
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿˜æœ‰æ›´å¤šï¼Œæ›´å¤šå†…å®¹ï¼
- en: Zooming out to a thirty thousand foot level, itâ€™s a great time to research LM
    evals â€” for one, because stronger LLMs require a new generation of tests to effectively
    evaluate them. Whether you create your own or build on top of existing ones, researching
    evals is an impactful way to contribute to the open science community.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä¸‰ä¸‡è‹±å°ºçš„é«˜åº¦æ¥çœ‹ï¼Œç°åœ¨æ˜¯è¿›è¡Œ LM è¯„ä¼°ç ”ç©¶çš„å¥½æ—¶æœºâ€”â€”é¦–å…ˆï¼Œå› ä¸ºæ›´å¼ºå¤§çš„ LLM éœ€è¦æ–°ä¸€ä»£æµ‹è¯•æ¥æœ‰æ•ˆè¯„ä¼°å®ƒä»¬ã€‚æ— è®ºæ˜¯åˆ›å»ºè‡ªå·±çš„è¯„ä¼°æ–¹æ³•ï¼Œè¿˜æ˜¯åœ¨ç°æœ‰æ–¹æ³•çš„åŸºç¡€ä¸Šè¿›è¡Œæ”¹è¿›ï¼Œç ”ç©¶è¯„ä¼°æ˜¯ä¸ºå¼€æ”¾ç§‘å­¦ç¤¾åŒºåšå‡ºè´¡çŒ®çš„ä¸€ç§é‡è¦æ–¹å¼ã€‚
- en: Citations
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¼•ç”¨
- en: '[1] Matthew Carrigan (2023), [Chat Templates: An End to the Silent Performance
    Killer](https://huggingface.co/blog/chat-templates), Hugging Face.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Matthew Carriganï¼ˆ2023ï¼‰ï¼Œ[èŠå¤©æ¨¡æ¿ï¼šç»ˆç»“æ²‰é»˜çš„æ€§èƒ½æ€æ‰‹](https://huggingface.co/blog/chat-templates)ï¼ŒHugging
    Faceã€‚'
- en: '[2] Zhou et al. (2023), [Instruction-Following Evaluation for Large Language
    Models](https://arxiv.org/pdf/2311.07911.pdf), arXiv.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Zhou ç­‰äººï¼ˆ2023ï¼‰ï¼Œ[å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤è·Ÿéšè¯„ä¼°](https://arxiv.org/pdf/2311.07911.pdf)ï¼ŒarXivã€‚'
- en: '**Dataset licensing**: The IFEval dataset used herein is publicly available
    to all without restriction ([Apache-2.0 license](https://github.com/google-research/google-research/tree/master#Apache-2.0-1-ov-file)).'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ•°æ®é›†è®¸å¯**ï¼šæ­¤å¤„ä½¿ç”¨çš„ IFEval æ•°æ®é›†å¯¹æ‰€æœ‰äººå…¬å¼€ï¼Œæ— é™åˆ¶ä½¿ç”¨ï¼ˆ[Apache-2.0 è®¸å¯è¯](https://github.com/google-research/google-research/tree/master#Apache-2.0-1-ov-file)ï¼‰ã€‚'
- en: '[3] Models used here, from largest to smallest (all permissively licensed for
    research use).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] æ­¤å¤„ä½¿ç”¨çš„æ¨¡å‹ï¼ŒæŒ‰å¤§å°æ’åˆ—ï¼ˆæ‰€æœ‰æ¨¡å‹å‡å·²è·å¾—ç ”ç©¶ä½¿ç”¨çš„å®½æ¾è®¸å¯ï¼‰ã€‚'
- en: '`Llama-2â€“70b-chat` ([link](http://meta-llama/Llama-2-70b-chat-hf)) â€” Meta'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Llama-2â€“70b-chat`ï¼ˆ[é“¾æ¥](http://meta-llama/Llama-2-70b-chat-hf)ï¼‰â€” Meta'
- en: '`Mixtral-8x7B-Instruct-v0.1` ([link](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1))
    â€” Mistral.AI'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Mixtral-8x7B-Instruct-v0.1`ï¼ˆ[é“¾æ¥](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)ï¼‰â€”
    Mistral.AI'
- en: '`Nous-Hermes-2-Mixtral-8x7B-DPO` ([link](https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO))
    â€” Nous-Research'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Nous-Hermes-2-Mixtral-8x7B-DPO`ï¼ˆ[é“¾æ¥](https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO)ï¼‰â€”
    Nous-Research'
- en: '`Nous-Hermes-2-Yi-34B` ([link](http://NousResearch/Nous-Hermes-2-Yi-34B)) â€”
    Nous-Research'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Nous-Hermes-2-Yi-34B`ï¼ˆ[é“¾æ¥](http://NousResearch/Nous-Hermes-2-Yi-34B)ï¼‰â€” Nous-Research'
- en: '`Starling-LM-7B-alpha` ([link](https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha))
    â€” Berkeley NEST'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Starling-LM-7B-alpha`ï¼ˆ[é“¾æ¥](https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha)ï¼‰â€”
    Berkeley NEST'
- en: '`Zephyr-7B-beta` ([link](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta))
    â€” Hugging Face'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Zephyr-7B-beta`ï¼ˆ[é“¾æ¥](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)ï¼‰â€”
    Hugging Face'
- en: '`Mistral-7B-Instruct-v0.2` ([link](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2))
    â€” Mistral.AI'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Mistral-7B-Instruct-v0.2` ([é“¾æ¥](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2))
    â€” Mistral.AI'
- en: Additional Notes
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å…¶ä»–æ³¨æ„äº‹é¡¹
- en: See the notebooks [here](https://github.com/daniel-furman/evals-with-chat-formats)
    for the code used to run the experiments.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹ç”¨äºè¿è¡Œå®éªŒçš„ä»£ç ï¼Œå¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/daniel-furman/evals-with-chat-formats)æ‰¾åˆ°ã€‚
- en: To audit the results, see outputs for each run [here](https://github.com/daniel-furman/evals-with-chat-formats/tree/main/assets/IFEval_results)
    as well as Zeno logs [here](https://hub.zenoml.com/project/79b4684d-0f4e-48f4-b739-ba4e0bd63ee8/IFEval-chat-templating-experiments-run-1)
    and [here](https://hub.zenoml.com/project/548fcb7a-52cf-4c60-aaf7-13d6b03343fd/IFEval-chat-templating-experiments-run-2)
    (models were ran in 2 total batches). Note that the Zeno logs donâ€™t yet capture
    the application of chat templates to the prompts â€” this is a â€œto doâ€ item in development
    backlog.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¦å®¡è®¡ç»“æœï¼Œè¯·æŸ¥çœ‹æ¯æ¬¡è¿è¡Œçš„è¾“å‡ºï¼Œ[è¿™é‡Œ](https://github.com/daniel-furman/evals-with-chat-formats/tree/main/assets/IFEval_results)ï¼Œä»¥åŠZenoæ—¥å¿—ï¼Œ[è¿™é‡Œ](https://hub.zenoml.com/project/79b4684d-0f4e-48f4-b739-ba4e0bd63ee8/IFEval-chat-templating-experiments-run-1)å’Œ[è¿™é‡Œ](https://hub.zenoml.com/project/548fcb7a-52cf-4c60-aaf7-13d6b03343fd/IFEval-chat-templating-experiments-run-2)ï¼ˆæ¨¡å‹åœ¨2ä¸ªæ‰¹æ¬¡ä¸­è¿è¡Œï¼‰ã€‚è¯·æ³¨æ„ï¼ŒZenoæ—¥å¿—å°šæœªæ•æ‰åˆ°èŠå¤©æ¨¡æ¿åº”ç”¨äºæç¤ºçš„è¿‡ç¨‹â€”â€”è¿™æ˜¯å¼€å‘å¾…åŠäº‹é¡¹ä¸­çš„ä¸€é¡¹å†…å®¹ã€‚
- en: For compute, RunPod ([link](https://www.runpod.io/)) was used for access to
    workstations with Nvidia GPU chips â€” in particular, a cluster with 2x H100 80
    GB SXM5 chips. In total, the experiment included 14 runs of the IFEval, which
    accumulated ~6 hrs of cluster uptime.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨è®¡ç®—æ–¹é¢ï¼Œä½¿ç”¨äº†RunPod ([é“¾æ¥](https://www.runpod.io/)) è®¿é—®å¸¦æœ‰Nvidia GPUèŠ¯ç‰‡çš„å·¥ä½œç«™â€”â€”ç‰¹åˆ«æ˜¯ä¸€ä¸ªæ‹¥æœ‰2ä¸ªH100
    80 GB SXM5èŠ¯ç‰‡çš„é›†ç¾¤ã€‚æ€»çš„æ¥è¯´ï¼Œå®éªŒåŒ…æ‹¬äº†14æ¬¡IFEvalçš„è¿è¡Œï¼Œæ€»å…±ç§¯ç´¯äº†çº¦6å°æ—¶çš„é›†ç¾¤è¿è¡Œæ—¶é—´ã€‚
- en: Confidence intervals were taken to estimate statistical uncertainty in our results
    (the bootstrap resampling method was used). These 95% confidence intervals ranged
    from roughly +/- 2.75% to 4.25% â€” small relative to the measured effects of chat
    templating.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡ç½®ä¿¡åŒºé—´ä¼°è®¡æˆ‘ä»¬çš„ç»“æœä¸­çš„ç»Ÿè®¡ä¸ç¡®å®šæ€§ï¼ˆä½¿ç”¨äº†è‡ªåŠ©æ³•é‡æŠ½æ ·æ–¹æ³•ï¼‰ã€‚è¿™äº›95%çš„ç½®ä¿¡åŒºé—´å¤§çº¦åœ¨+/- 2.75%åˆ°4.25%ä¹‹é—´â€”â€”ç›¸å¯¹äºèŠå¤©æ¨¡æ¿åº”ç”¨çš„æµ‹é‡æ•ˆæœæ¥è¯´ï¼Œè¿™ä¸ªèŒƒå›´è¾ƒå°ã€‚
