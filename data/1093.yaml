- en: My First Billion (of Rows) in DuckDB
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨ DuckDB ä¸­çš„ç¬¬ä¸€äº¿æ¡æ•°æ®ï¼ˆè¡Œï¼‰
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/my-first-billion-of-rows-in-duckdb-11873e5edbb5?source=collection_archive---------0-----------------------#2024-05-01](https://towardsdatascience.com/my-first-billion-of-rows-in-duckdb-11873e5edbb5?source=collection_archive---------0-----------------------#2024-05-01)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/my-first-billion-of-rows-in-duckdb-11873e5edbb5?source=collection_archive---------0-----------------------#2024-05-01](https://towardsdatascience.com/my-first-billion-of-rows-in-duckdb-11873e5edbb5?source=collection_archive---------0-----------------------#2024-05-01)
- en: First Impressions of DuckDB handling 450Gb in a real project
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DuckDB å¤„ç† 450Gb æ•°æ®çš„åˆæ­¥å°è±¡ï¼Œåœ¨å®é™…é¡¹ç›®ä¸­çš„åº”ç”¨
- en: '[](https://joaopedro214.medium.com/?source=post_page---byline--11873e5edbb5--------------------------------)[![JoÃ£o
    Pedro](../Images/64a0e14527be213e5fde0a02439fbfa7.png)](https://joaopedro214.medium.com/?source=post_page---byline--11873e5edbb5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--11873e5edbb5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--11873e5edbb5--------------------------------)
    [JoÃ£o Pedro](https://joaopedro214.medium.com/?source=post_page---byline--11873e5edbb5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://joaopedro214.medium.com/?source=post_page---byline--11873e5edbb5--------------------------------)[![JoÃ£o
    Pedro](../Images/64a0e14527be213e5fde0a02439fbfa7.png)](https://joaopedro214.medium.com/?source=post_page---byline--11873e5edbb5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--11873e5edbb5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--11873e5edbb5--------------------------------)
    [JoÃ£o Pedro](https://joaopedro214.medium.com/?source=post_page---byline--11873e5edbb5--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--11873e5edbb5--------------------------------)
    Â·12 min readÂ·May 1, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--11873e5edbb5--------------------------------)
    Â·é˜…è¯»æ—¶é—´12åˆ†é’ŸÂ·2024å¹´5æœˆ1æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f57054e6187d4d4d8a6e084b95284c6f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f57054e6187d4d4d8a6e084b95284c6f.png)'
- en: Duck blueprint. Generated by Copilot Designer.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Duck è“å›¾ã€‚ç”± Copilot Designer ç”Ÿæˆã€‚
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: The fields of AI, Data Science, and Data Engineering are progressing at full
    steam. Every day new tools, new paradigms, and new architectures are created,
    always trying to solve the problems of the previous ones. In this sea of new opportunities,
    itâ€™s interesting to know a little about the available tools to solve problems
    efficiently. And Iâ€™m not talking only about the technicalities, but the scope
    of use, advantages, disadvantages, challenges, and opportunities, something acquired
    with practice.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: äººå·¥æ™ºèƒ½ã€æ•°æ®ç§‘å­¦å’Œæ•°æ®å·¥ç¨‹é¢†åŸŸæ­£åœ¨å…¨é€Ÿå‘å±•ã€‚æ¯å¤©éƒ½æœ‰æ–°çš„å·¥å…·ã€æ–°çš„èŒƒå¼å’Œæ–°çš„æ¶æ„è¢«åˆ›é€ å‡ºæ¥ï¼Œå§‹ç»ˆè¯•å›¾è§£å†³å‰ä¸€ä¸ªé—®é¢˜ã€‚åœ¨è¿™ç‰‡å……æ»¡æ–°æœºé‡çš„æµ·æ´‹ä¸­ï¼Œäº†è§£ä¸€äº›ç°æœ‰å·¥å…·æ¥é«˜æ•ˆè§£å†³é—®é¢˜æ˜¯å¾ˆæœ‰è¶£çš„ã€‚è€Œæˆ‘è¯´çš„ä¸ä»…ä»…æ˜¯æŠ€æœ¯ç»†èŠ‚ï¼Œè¿˜æœ‰ä½¿ç”¨èŒƒå›´ã€ä¼˜ç¼ºç‚¹ã€æŒ‘æˆ˜å’Œæœºä¼šï¼Œè¿™äº›éƒ½æ˜¯é€šè¿‡å®è·µè·å¾—çš„ã€‚
- en: In this post, Iâ€™ll describe my first experience in DuckDB (the new hyped database
    for processing huge amounts of data locally on your computer) revisiting an old
    problem that I faced previously â€” The processing of Logs of Brazilian Electronic
    Ballot Boxes to calculate vote-time metrics. As youâ€™ll see through this post,
    this is a challenging problem that serves as a good benchmark for both performance
    and user experience assessments.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†æè¿°æˆ‘åœ¨ DuckDBï¼ˆä¸€ä¸ªç”¨äºåœ¨æœ¬åœ°è®¡ç®—æœºä¸Šå¤„ç†å¤§é‡æ•°æ®çš„æ–°å…´æ•°æ®åº“ï¼‰ä¸­çš„ç¬¬ä¸€æ¬¡ä½“éªŒï¼Œå¹¶é‡æ–°å®¡è§†æˆ‘ä»¥å‰é‡åˆ°çš„ä¸€ä¸ªè€é—®é¢˜â€”â€”å·´è¥¿ç”µå­æŠ•ç¥¨ç®±æ—¥å¿—çš„å¤„ç†ï¼Œä»¥è®¡ç®—æŠ•ç¥¨æ—¶é—´åº¦é‡ã€‚æ­£å¦‚ä½ å°†é€šè¿‡è¿™ç¯‡æ–‡ç« çœ‹åˆ°çš„ï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œæ˜¯å¯¹æ€§èƒ½å’Œç”¨æˆ·ä½“éªŒè¯„ä¼°çš„ä¸€ä¸ªè‰¯å¥½åŸºå‡†ã€‚
- en: The idea is that this post can serve as input for you, who want to know a little
    more about DuckDB, as I will cover both technical aspects, running the problem,
    and calculating the database performance, and more â€˜softâ€™ aspects, like programming
    experience and usability.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ–‡ç« çš„ç›®çš„æ˜¯ä¸ºä½ æä¾›ä¸€äº›å‚è€ƒï¼Œç‰¹åˆ«æ˜¯ä½ æƒ³äº†è§£æ›´å¤šå…³äº DuckDB çš„å†…å®¹ï¼Œæˆ‘å°†æ¶µç›–æŠ€æœ¯æ–¹é¢çš„å†…å®¹ï¼Œæ¯”å¦‚è¿è¡Œé—®é¢˜å’Œè®¡ç®—æ•°æ®åº“æ€§èƒ½ï¼Œä¹Ÿä¼šè°ˆåŠä¸€äº›â€œè½¯â€æ–¹é¢çš„å†…å®¹ï¼Œæ¯”å¦‚ç¼–ç¨‹ä½“éªŒå’Œå¯ç”¨æ€§ã€‚
- en: '*DuckDB is an Open Source Project [*[*OSD*](https://opensource.org/osd)*],
    the author has no affiliation with DuckDB/DuckDB Labs. The data used is available
    in the* [*ODbL*](https://opendatacommons.org/licenses/odbl/) *License. This project
    is completely free to carry out. It does not require payment for any services,
    data access, or other expenses.*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*DuckDB æ˜¯ä¸€ä¸ªå¼€æºé¡¹ç›® [*[*OSD*](https://opensource.org/osd)*]ï¼Œä½œè€…ä¸ DuckDB/DuckDB Labs
    æ²¡æœ‰ä»»ä½•å…³ç³»ã€‚æ‰€ä½¿ç”¨çš„æ•°æ®ç¬¦åˆ* [*ODbL*](https://opendatacommons.org/licenses/odbl/) *è®¸å¯è¯ã€‚è¿™æ˜¯ä¸€ä¸ªå®Œå…¨å…è´¹çš„é¡¹ç›®ï¼Œä¸éœ€è¦æ”¯ä»˜ä»»ä½•æœåŠ¡è´¹ã€æ•°æ®è®¿é—®è´¹æˆ–å…¶ä»–è´¹ç”¨ã€‚*'
- en: The Problem
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é—®é¢˜
- en: 'The problem consists of processing records from Electronic Ballot Boxesâ€™ Logs
    to obtain statistical metrics about the voting time of Brazilian voters. For example,
    calculate the average time citizens use to vote, collect fingerprints for identification,
    and so on. These metrics should be aggregated in several granularity levels: at
    the country level, state, electoral zone, and electoral section.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªé—®é¢˜åŒ…æ‹¬å¤„ç†ç”µå­æŠ•ç¥¨ç®±æ—¥å¿—ä¸­çš„è®°å½•ï¼Œä»¥è·å–å…³äºå·´è¥¿é€‰æ°‘æŠ•ç¥¨æ—¶é—´çš„ç»Ÿè®¡æŒ‡æ ‡ã€‚ä¾‹å¦‚ï¼Œè®¡ç®—å…¬æ°‘æŠ•ç¥¨æ‰€éœ€çš„å¹³å‡æ—¶é—´ã€æ”¶é›†æŒ‡çº¹ç”¨äºèº«ä»½è¯†åˆ«ç­‰ç­‰ã€‚è¿™äº›æŒ‡æ ‡åº”åœ¨ä¸åŒçš„ç²’åº¦å±‚æ¬¡è¿›è¡Œèšåˆï¼šä»å›½å®¶çº§ã€å·çº§ã€é€‰ä¸¾åŒºçº§åˆ°é€‰ä¸¾åˆ†åŒºçº§ã€‚
- en: In case you donâ€™t know, Brazil has a 100% electronic voting system, where all
    the 100+ million citizens vote on a single day and the electionâ€™s result is computed
    and released near real-time. Votes are collected by thousands of electronic ballot
    boxes spread all over the country.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ä¸çŸ¥é“ï¼Œå·´è¥¿æ‹¥æœ‰100%ç”µå­æŠ•ç¥¨ç³»ç»Ÿï¼Œæ‰€æœ‰è¶…è¿‡ä¸€äº¿çš„å…¬æ°‘éƒ½åœ¨åŒä¸€å¤©æŠ•ç¥¨ï¼Œé€‰ä¸¾ç»“æœå‡ ä¹å®æ—¶è®¡ç®—å¹¶å…¬å¸ƒã€‚æŠ•ç¥¨ç”±æˆåƒä¸Šä¸‡çš„ç”µå­æŠ•ç¥¨ç®±æ”¶é›†ï¼Œè¿™äº›æŠ•ç¥¨ç®±åˆ†å¸ƒåœ¨å…¨å›½å„åœ°ã€‚
- en: '![](../Images/c93e2da9a47c8ee0f7722ab4d5e391ed.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c93e2da9a47c8ee0f7722ab4d5e391ed.png)'
- en: Electronic ballot box. [Image from the Brazillian Superior Electoral Court](https://www.tre-rn.jus.br/comunicacao/noticias/2021/Maio/urna-eletronica-25-anos-100-brasileira-e-admirada-pelo-mundo).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ç”µå­æŠ•ç¥¨ç®±ã€‚[æ¥è‡ªå·´è¥¿æœ€é«˜é€‰ä¸¾æ³•é™¢çš„å›¾ç‰‡](https://www.tre-rn.jus.br/comunicacao/noticias/2021/Maio/urna-eletronica-25-anos-100-brasileira-e-admirada-pelo-mundo)ã€‚
- en: 'An electronic ballot box is a microcomputer for specific use for election**s**,
    with the following characteristics: resistant, small, light, with energy autonomy,
    and with security features [[4](https://international.tse.jus.br/en/electronic-ballot-box/presentation)].
    Each can hold up to 500 voters, a number chosen to avoid big queues in the voting
    locations.'
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ç”µå­æŠ•ç¥¨ç®±æ˜¯ä¸€ç§ä¸“ç”¨çš„å¾®å‹è®¡ç®—æœºï¼Œç”¨äºé€‰ä¸¾**å·¥ä½œ**ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼šåšå›ºã€å°å·§ã€è½»ä¾¿ã€å…·æœ‰èƒ½æºè‡ªç»™èƒ½åŠ›ï¼Œå¹¶å…·å¤‡å®‰å…¨åŠŸèƒ½ [[4](https://international.tse.jus.br/en/electronic-ballot-box/presentation)]ã€‚æ¯å°æŠ•ç¥¨ç®±æœ€å¤šå¯ä»¥å®¹çº³500åé€‰æ°‘ï¼Œè¿™æ˜¯ä¸ºäº†é¿å…æŠ•ç¥¨åœ°ç‚¹å‡ºç°é•¿æ—¶é—´æ’é˜Ÿçš„æƒ…å†µã€‚
- en: The system is administered by the TSE (Supreme Electoral Court), which shares
    data about the process in its [open data portal](https://dadosabertos.tse.jus.br/)
    [[ODbL](https://opendatacommons.org/licenses/odbl/) License]. The logs are text
    files with an exhaustive list of all events in the ballot box.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥ç³»ç»Ÿç”±TSEï¼ˆæœ€é«˜é€‰ä¸¾æ³•é™¢ï¼‰ç®¡ç†ï¼ŒTSEé€šè¿‡å…¶[å¼€æ”¾æ•°æ®é—¨æˆ·](https://dadosabertos.tse.jus.br/)åˆ†äº«æœ‰å…³é€‰ä¸¾è¿‡ç¨‹çš„æ•°æ®
    [[ODbL](https://opendatacommons.org/licenses/odbl/)]è®¸å¯è¯ã€‚æ—¥å¿—æ˜¯æ–‡æœ¬æ–‡ä»¶ï¼ŒåŒ…å«æŠ•ç¥¨ç®±ä¸­æ‰€æœ‰äº‹ä»¶çš„è¯¦ç»†åˆ—è¡¨ã€‚
- en: And thatâ€™s where the challenge begins. As the logs register absolutely every
    single event, itâ€™s possible to calculate an enormous amount of metrics from them;
    itâ€™s a vibrant information fountain. But what makes them rich, also makes them
    extremely hard to handle, as the totality of all the countryâ€™s records reaches
    the milestone of 450Gb in TSV files with + 4 billion lines.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯æŒ‘æˆ˜çš„å¼€å§‹ã€‚ç”±äºæ—¥å¿—è®°å½•äº†æ¯ä¸€ä¸ªäº‹ä»¶ï¼Œå› æ­¤å¯ä»¥ä»ä¸­è®¡ç®—å‡ºå¤§é‡çš„æŒ‡æ ‡ï¼›å®ƒæ˜¯ä¸€ä¸ªå……æ»¡æ´»åŠ›çš„ä¿¡æ¯æºæ³‰ã€‚ä½†æ­£æ˜¯è¿™ç§ä¸°å¯Œæ€§ï¼Œä½¿å¾—å®ƒä»¬å˜å¾—æä¸ºéš¾ä»¥å¤„ç†ï¼Œå› ä¸ºæ•´ä¸ªå›½å®¶çš„è®°å½•æ€»é‡å·²ç»è¾¾åˆ°äº†450GBï¼ŒTSVæ–‡ä»¶ä¸­æœ‰è¶…è¿‡40äº¿è¡Œã€‚
- en: Besides the volume, another thing that makes this work a good benchmark, in
    my opinion, is that the needed transformations to reach our final goal are from
    all sorts of complexities, from simple (where, group by, order by) to complex
    SQL operations (like windows functions).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†æ•°æ®é‡ï¼Œå¦ä¸€æ–¹é¢ä½¿è¿™é¡¹å·¥ä½œæˆä¸ºä¸€ä¸ªè‰¯å¥½åŸºå‡†çš„åŸå› æ˜¯ï¼Œè¾¾æˆæˆ‘ä»¬æœ€ç»ˆç›®æ ‡æ‰€éœ€çš„è½¬æ¢æ¶‰åŠäº†å„ç§å¤æ‚åº¦çš„æ“ä½œï¼Œä»ç®€å•çš„ï¼ˆå¦‚ `where`ã€`group by`ã€`order
    by`ï¼‰åˆ°å¤æ‚çš„ SQL æ“ä½œï¼ˆå¦‚çª—å£å‡½æ•°ï¼‰ã€‚
- en: DuckDB
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DuckDB
- en: With this relatively high volume of data, one can be willing to evoke traditional
    Big Data tools, like Apache Spark, and process this data in a cluster with many
    workers, several gigabytes of RAM, and a dozen CPUs.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¦‚æ­¤é«˜çš„æ•°æ®é‡ä¸‹ï¼Œäººä»¬å¯èƒ½ä¼šæ„¿æ„è°ƒç”¨ä¼ ç»Ÿçš„å¤§æ•°æ®å·¥å…·ï¼Œå¦‚ Apache Sparkï¼Œå¹¶åœ¨é›†ç¾¤ä¸­ä½¿ç”¨å¤šä¸ªå·¥ä½œèŠ‚ç‚¹ã€å‡ GBçš„RAMå’Œåå¤šä¸ªCPUæ¥å¤„ç†è¿™äº›æ•°æ®ã€‚
- en: DuckDB was created to challenge this *status quo*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: DuckDBçš„åˆ›å»ºå°±æ˜¯ä¸ºäº†æŒ‘æˆ˜è¿™ä¸€*ç°çŠ¶*ã€‚
- en: As its creator defends ([in this video](https://youtu.be/GaHWuQ_cBhA)), it is
    a database thought to empower single machines with the ability to process large
    volumes of data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚å…¶åˆ›å§‹äººåœ¨[è¿™ä¸ªè§†é¢‘](https://youtu.be/GaHWuQ_cBhA)ä¸­æ‰€è¾©æŠ¤çš„é‚£æ ·ï¼Œè¿™æ˜¯ä¸€ç§æ•°æ®åº“è®¾è®¡ï¼Œæ—¨åœ¨èµ‹äºˆå•ä¸€æœºå™¨å¤„ç†å¤§é‡æ•°æ®çš„èƒ½åŠ›ã€‚
- en: I.e., instead of looking for complex industry solutions â€” like PySpark â€” or
    cloud-based solutions â€” like Google BigQuery â€” one will use a local in-process
    database with standard SQL to realize the needed transformations.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿå°±æ˜¯è¯´ï¼Œæ”¾å¼ƒå¯»æ±‚å¤æ‚çš„è¡Œä¸šè§£å†³æ–¹æ¡ˆâ€”â€”å¦‚ PySparkâ€”â€”æˆ–åŸºäºäº‘çš„è§£å†³æ–¹æ¡ˆâ€”â€”å¦‚ Google BigQueryâ€”â€”è€Œæ˜¯ä½¿ç”¨æœ¬åœ°è¿›ç¨‹æ•°æ®åº“ï¼Œåˆ©ç”¨æ ‡å‡†
    SQL å®ç°æ‰€éœ€çš„è½¬æ¢ã€‚
- en: So, in a nutshell, DuckDB is an in-process (that runs in the program itself,
    it has no independent process, resembling SQLite), OLAP (adjusted to analytical
    loads), that handles data in traditional formats (CSV, parquet), optimized to
    handle large volumes of data using the power of a single machine (that doesnâ€™t
    need to be very powerful).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»è€Œè¨€ä¹‹ï¼ŒDuckDBæ˜¯ä¸€ä¸ªå†…åµŒå¼ï¼ˆè¿è¡Œåœ¨ç¨‹åºå†…éƒ¨ï¼Œæ²¡æœ‰ç‹¬ç«‹è¿›ç¨‹ï¼Œç±»ä¼¼äºSQLiteï¼‰ã€OLAPï¼ˆé’ˆå¯¹åˆ†æè´Ÿè½½è¿›è¡Œä¼˜åŒ–ï¼‰ã€èƒ½å¤Ÿå¤„ç†ä¼ ç»Ÿæ ¼å¼æ•°æ®ï¼ˆCSVã€Parquetï¼‰çš„æ•°æ®åº“ï¼Œä¼˜åŒ–äº†åœ¨å•å°æœºå™¨ä¸Šå¤„ç†å¤§é‡æ•°æ®çš„èƒ½åŠ›ï¼ˆä¸éœ€è¦ç‰¹åˆ«å¼ºå¤§çš„æœºå™¨ï¼‰ã€‚
- en: The Data
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ•°æ®
- en: A ballot boxâ€™s log is a single TSV file with a standardized name â€” XXXXXYYYYZZZZ.csv,
    composed of the boxâ€™s location metadata, with the 5 first digits being the city
    code, the next 4 the electoral zone (a geographical stateâ€™s subdivision), and
    the last 4 the electoral section (the ballot box itself).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæŠ•ç¥¨ç®±çš„æ—¥å¿—æ˜¯ä¸€ä¸ªæ ‡å‡†åŒ–åç§°çš„å•ä¸€TSVæ–‡ä»¶ â€” XXXXXYYYYZZZZ.csvï¼ŒåŒ…å«æŠ•ç¥¨ç®±ä½ç½®çš„å…ƒæ•°æ®ï¼Œå…¶ä¸­å‰5ä½æ•°å­—æ˜¯åŸå¸‚ä»£ç ï¼Œæ¥ä¸‹æ¥çš„4ä½æ˜¯é€‰ä¸¾åŒºï¼ˆä¸€ä¸ªåœ°ç†å·çš„å­åŒºåŸŸï¼‰ï¼Œæœ€å4ä½æ˜¯é€‰ä¸¾åŒºæ®µï¼ˆå³æŠ•ç¥¨ç®±æœ¬èº«ï¼‰ã€‚
- en: 'There are almost 500,000 ballot boxes in Brazil, so, almost 500.000 files.
    The fileâ€™s size depends on the number of voters in the section, which varies from
    1 to 500\. This is what the logs look like:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å·´è¥¿å‡ ä¹æœ‰50ä¸‡ä¸ªæŠ•ç¥¨ç®±ï¼Œå› æ­¤å‡ ä¹æœ‰50ä¸‡ä¸ªæ–‡ä»¶ã€‚æ–‡ä»¶çš„å¤§å°å–å†³äºè¯¥é€‰åŒºé€‰æ°‘çš„æ•°é‡ï¼ŒèŒƒå›´ä»1åˆ°500ã€‚è¿™å°±æ˜¯æ—¥å¿—çš„æ ·å­ï¼š
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Weâ€™re interested in transforming this raw information into statistical metrics
    about voting time(How much time each voter takes to vote? How many votes are computed
    each minute?) in several granularity levels (country, state, city) and, to achieve
    that, weâ€™re going to create an [OLAP Cube](https://en.wikipedia.org/wiki/OLAP_cube)
    like that:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å°†è¿™äº›åŸå§‹ä¿¡æ¯è½¬åŒ–ä¸ºæœ‰å…³æŠ•ç¥¨æ—¶é—´çš„ç»Ÿè®¡æŒ‡æ ‡ï¼ˆæ¯ä¸ªé€‰æ°‘æŠ•ç¥¨æ‰€éœ€çš„æ—¶é—´æ˜¯å¤šå°‘ï¼Ÿæ¯åˆ†é’Ÿè®¡ç®—å¤šå°‘ç¥¨ï¼Ÿï¼‰ï¼Œå¹¶åœ¨ä¸åŒçš„ç²’åº¦å±‚çº§ï¼ˆå›½å®¶ã€å·ã€åŸå¸‚ï¼‰ä¸Šå®ç°è¿™ä¸€ç›®æ ‡ï¼Œä¸ºæ­¤æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ª[OLAPç«‹æ–¹ä½“](https://en.wikipedia.org/wiki/OLAP_cube)ï¼Œå¦‚å›¾æ‰€ç¤ºï¼š
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The Implementation
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®ç°
- en: Setup the environment
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®¾ç½®ç¯å¢ƒ
- en: All thatâ€™s needed to run this project is a Python environment with the [DuckDB
    package installed](https://duckdb.org/docs/guides/python/install.html).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è¿è¡Œæ­¤é¡¹ç›®æ‰€éœ€çš„ä»…ä»…æ˜¯ä¸€ä¸ªå®‰è£…äº†[DuckDBåŒ…çš„Pythonç¯å¢ƒ](https://duckdb.org/docs/guides/python/install.html)ã€‚
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Transforming the data
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è½¬æ¢æ•°æ®
- en: In the following sections, Iâ€™ll describe each transformation, its objectives,
    how DuckDB can perform each one, the advantages, challenges, results, and conclusions.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘å°†æè¿°æ¯ä¸ªè½¬æ¢çš„ç›®æ ‡ã€DuckDBå¦‚ä½•æ‰§è¡Œæ¯ä¸ªè½¬æ¢ã€ä¼˜ç‚¹ã€æŒ‘æˆ˜ã€ç»“æœå’Œç»“è®ºã€‚
- en: 'The processing is divided into 4 steps: Convert TSV files to Parquet; Filter
    and Clear; Isolate votes and their attributes; and Compute metrics to the OLAP
    Cube.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å¤„ç†è¿‡ç¨‹åˆ†ä¸º4ä¸ªæ­¥éª¤ï¼šå°†TSVæ–‡ä»¶è½¬æ¢ä¸ºParquetï¼›è¿‡æ»¤å’Œæ¸…ç†ï¼›éš”ç¦»é€‰ç¥¨åŠå…¶å±æ€§ï¼›å¹¶è®¡ç®—OLAPç«‹æ–¹ä½“çš„æŒ‡æ ‡ã€‚
- en: '![](../Images/441568ddc6e145b3636bcef42c853242.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/441568ddc6e145b3636bcef42c853242.png)'
- en: Processing Steps. Image by Author.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å¤„ç†æ­¥éª¤ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: Unfortunately, to avoid making this post enormous, Iâ€™ll not explain each transformation
    in detail. But all the code is available on the [GitHub repository](https://github.com/jaumpedro214/urna-logs-data-tseng).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å¹¸çš„æ˜¯ï¼Œä¸ºäº†é¿å…ä½¿è¿™ç¯‡æ–‡ç« è¿‡äºåºå¤§ï¼Œæˆ‘ä¸ä¼šè¯¦ç»†è§£é‡Šæ¯ä¸ªè½¬æ¢ã€‚ä½†æ‰€æœ‰ä»£ç éƒ½å¯ä»¥åœ¨[GitHubä»“åº“](https://github.com/jaumpedro214/urna-logs-data-tseng)ä¸­æ‰¾åˆ°ã€‚
- en: '**Converting TSV files to Parquet**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**å°†TSVæ–‡ä»¶è½¬æ¢ä¸ºParquet**'
- en: A simple and indispensable step for anyone who wants to work with large volumes
    of data. Doing this on DuckDB is straightforward.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä»»ä½•æƒ³è¦å¤„ç†å¤§é‡æ•°æ®çš„äººæ¥è¯´ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•è€Œä¸å¯æˆ–ç¼ºçš„æ­¥éª¤ã€‚åœ¨DuckDBä¸­è¿›è¡Œæ­¤æ“ä½œéå¸¸ç›´æ¥ã€‚
- en: 'First, create a DuckDB session:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œåˆ›å»ºä¸€ä¸ªDuckDBä¼šè¯ï¼š
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this example, we instantiate the database connector with an empty string.
    This is done to indicate that DuckDB should not create its own database file;
    rather, it should only interact with system files. As mentioned earlier, DuckDB
    is a database, so it has the functionalities to create tables, views, and so on,
    which we wonâ€™t explore here. Weâ€™ll focus solely on using it as a transformation
    engine.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ç”¨ä¸€ä¸ªç©ºå­—ç¬¦ä¸²å®ä¾‹åŒ–æ•°æ®åº“è¿æ¥å™¨ã€‚è¿™æ ·åšæ˜¯ä¸ºäº†è¡¨æ˜DuckDBä¸åº”è¯¥åˆ›å»ºè‡ªå·±çš„æ•°æ®åº“æ–‡ä»¶ï¼Œè€Œæ˜¯ä»…ä¸ç³»ç»Ÿæ–‡ä»¶äº¤äº’ã€‚æ­£å¦‚ä¹‹å‰æ‰€æåˆ°çš„ï¼ŒDuckDBæ˜¯ä¸€ä¸ªæ•°æ®åº“ï¼Œå› æ­¤å®ƒå…·æœ‰åˆ›å»ºè¡¨ã€è§†å›¾ç­‰åŠŸèƒ½ï¼Œä½†æˆ‘ä»¬åœ¨è¿™é‡Œä¸ä¼šæ¢è®¨è¿™äº›åŠŸèƒ½ã€‚æˆ‘ä»¬å°†ä¸“æ³¨äºå°†å…¶ç”¨ä½œè½¬æ¢å¼•æ“ã€‚
- en: 'And define the following query:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶å®šä¹‰ä»¥ä¸‹æŸ¥è¯¢ï¼š
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: And thatâ€™s all!
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å°±è¿™æ ·ï¼
- en: 'Letâ€™s detail the query:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è¯¦ç»†çœ‹çœ‹è¿™ä¸ªæŸ¥è¯¢ï¼š
- en: The inner expression is just a standard *SELECT * FROM table* query, the only
    difference is that, instead of referencing a table, DuckDB can reference files
    directly.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å†…éƒ¨è¡¨è¾¾å¼åªæ˜¯ä¸€ä¸ªæ ‡å‡†çš„*SELECT * FROM table* æŸ¥è¯¢ï¼Œå”¯ä¸€çš„åŒºåˆ«æ˜¯ï¼ŒDuckDBå¯ä»¥ç›´æ¥å¼•ç”¨æ–‡ä»¶ï¼Œè€Œä¸æ˜¯å¼•ç”¨è¡¨ã€‚
- en: 'The result of this query could be imported to a pandas dataframe for further
    expression, just like this:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæŸ¥è¯¢çš„ç»“æœå¯ä»¥å¯¼å…¥åˆ°pandasæ•°æ®æ¡†ä¸­è¿›è¡Œè¿›ä¸€æ­¥çš„è¡¨è¾¾ï¼Œå°±åƒè¿™æ ·ï¼š
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Which allows seamless integration between DuckDB and pandas.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä½¿å¾— DuckDB ä¸ pandas ä¹‹é—´çš„æ— ç¼é›†æˆæˆä¸ºå¯èƒ½ã€‚
- en: The outer expression is a simple *COPY â€¦ TO â€¦* , which writes the inner queryâ€™s
    result as a file.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å¤–éƒ¨è¡¨è¾¾å¼æ˜¯ä¸€ä¸ªç®€å•çš„ *COPY â€¦ TO â€¦*ï¼Œå®ƒå°†å†…éƒ¨æŸ¥è¯¢çš„ç»“æœå†™å…¥æ–‡ä»¶ã€‚
- en: In this first transformation, we can start to see one of the strengths of DuckDBâ€”
    the ability to interact with files using plain old SQL, without needing to configure
    anything else. The above query is not different at all from day-to-day operations
    that we make in standard SGBDs, like PostgreSQL and MySQL, with the only difference
    being that, instead of manipulating tables, weâ€™re interacting with files.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªç¬¬ä¸€æ¬¡è½¬æ¢ä¸­ï¼Œæˆ‘ä»¬å¼€å§‹çœ‹åˆ° DuckDB çš„ä¸€ä¸ªä¼˜åŠ¿â€”â€”èƒ½å¤Ÿä½¿ç”¨çº¯ SQL ä¸æ–‡ä»¶è¿›è¡Œäº¤äº’ï¼Œè€Œæ— éœ€é…ç½®ä»»ä½•å…¶ä»–å†…å®¹ã€‚ä¸Šè¿°æŸ¥è¯¢ä¸æˆ‘ä»¬åœ¨æ ‡å‡† SGBDï¼ˆå¦‚
    PostgreSQL å’Œ MySQLï¼‰ä¸­æ‰§è¡Œçš„æ—¥å¸¸æ“ä½œå®Œå…¨ç›¸åŒï¼Œå”¯ä¸€çš„åŒºåˆ«æ˜¯ï¼Œæˆ‘ä»¬ä¸å†æ“ä½œè¡¨æ ¼ï¼Œè€Œæ˜¯ä¸æ–‡ä»¶è¿›è¡Œäº¤äº’ã€‚
- en: Originally, we had **450Gb** of TSV files and, after ~**30min**, we ended up
    with **97Gb** of Parquet.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: åŸæœ¬æˆ‘ä»¬æœ‰ **450GB** çš„ TSV æ–‡ä»¶ï¼Œçº¦ **30 åˆ†é’Ÿ** åï¼Œæˆ‘ä»¬å‰©ä¸‹äº† **97GB** çš„ Parquet æ–‡ä»¶ã€‚
- en: Filter and Clear
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¿‡æ»¤å¹¶æ¸…é™¤
- en: As mentioned earlier, the Logs store every event that happens on a ballot box.
    This first step aims to filter only vote-related events, like â€˜*The voter voted
    for PRESIDENT*â€™, â€˜*The Voter had fingerprints collected*â€™, and â€˜*The vote was
    computed*â€™ that happened on the election days (thatâ€™s important, as the logs also
    store training sections and other administrative procedures realized).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼Œæ—¥å¿—å­˜å‚¨äº†é€‰ç¥¨ç®±ä¸Šå‘ç”Ÿçš„æ¯ä¸ªäº‹ä»¶ã€‚ç¬¬ä¸€æ­¥æ—¨åœ¨è¿‡æ»¤å‡ºä»…ä¸æŠ•ç¥¨ç›¸å…³çš„äº‹ä»¶ï¼Œå¦‚â€œ*é€‰æ°‘æŠ•ç¥¨é€‰ä¸¾äº†æ€»ç»Ÿ*â€ã€â€œ*é€‰æ°‘é‡‡é›†äº†æŒ‡çº¹*â€ä»¥åŠâ€œ*æŠ•ç¥¨å·²è®¡ç®—*â€ï¼Œè¿™äº›äº‹ä»¶å‘ç”Ÿåœ¨é€‰ä¸¾æ—¥ï¼ˆè¿™å¾ˆé‡è¦ï¼Œå› ä¸ºæ—¥å¿—è¿˜è®°å½•äº†åŸ¹è®­éƒ¨åˆ†å’Œå…¶ä»–è¡Œæ”¿ç¨‹åºï¼‰ã€‚
- en: 'A simple query, but with a lot of text and date manipulations:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªç®€å•çš„æŸ¥è¯¢ï¼Œä½†åŒ…å«å¤§é‡æ–‡æœ¬å’Œæ—¥æœŸå¤„ç†ï¼š
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In this query, another advantage of DuckDB is highlighted, the ability to read
    and write partitioned data. Table partitioning is very relevant in the context
    of Big Data, but is still even more significant in the single-machine paradigm,
    given that weâ€™re operating the same disk for input and output, i.e., it suffers
    twice, and every optimization is welcome.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªæŸ¥è¯¢ä¸­ï¼Œçªå‡ºäº† DuckDB çš„å¦ä¸€ä¸ªä¼˜åŠ¿ï¼šèƒ½å¤Ÿè¯»å–å’Œå†™å…¥åˆ†åŒºæ•°æ®ã€‚è¡¨åˆ†åŒºåœ¨å¤§æ•°æ®çš„èƒŒæ™¯ä¸‹éå¸¸é‡è¦ï¼Œä½†åœ¨å•æœºæ¶æ„ä¸­å°¤ä¸ºå…³é”®ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨è¿›è¡Œè¾“å…¥å’Œè¾“å‡ºæ“ä½œæ—¶ä½¿ç”¨çš„æ˜¯åŒä¸€ç£ç›˜ï¼Œå³å®ƒè¦æ‰¿å—ä¸¤æ¬¡è´Ÿæ‹…ï¼Œæ¯ä¸€ä¸ªä¼˜åŒ–éƒ½éå¸¸æ¬¢è¿ã€‚
- en: Originally, we had 97Gb, but after ~30min, we were left with 63Gb of Parquet.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: åŸæœ¬æˆ‘ä»¬æœ‰ 97GBï¼Œä½†çº¦ 30 åˆ†é’Ÿåï¼Œæˆ‘ä»¬å‰©ä¸‹äº† 63GB çš„ Parquet æ–‡ä»¶ã€‚
- en: Isolate votes and their attributes
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: éš”ç¦»æŠ•ç¥¨åŠå…¶å±æ€§
- en: As each vote is composed of several lines, we need to condense all the information
    in a unique record, to ease the calculations. Here things get complicated, as
    the query gets complex and, unfortunately, DuckDB could not process all the data
    in one go.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæ¯ä¸ªæŠ•ç¥¨ç”±å¤šè¡Œç»„æˆï¼Œæˆ‘ä»¬éœ€è¦å°†æ‰€æœ‰ä¿¡æ¯æµ“ç¼©æˆä¸€ä¸ªå”¯ä¸€çš„è®°å½•ï¼Œä»¥ä¾¿ç®€åŒ–è®¡ç®—ã€‚è¿™é‡Œæƒ…å†µå˜å¾—å¤æ‚ï¼Œå› ä¸ºæŸ¥è¯¢å˜å¾—å¤æ‚ï¼Œè€Œä¸”ä¸å¹¸çš„æ˜¯ï¼ŒDuckDB æ— æ³•ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰æ•°æ®ã€‚
- en: 'To overcome this issue, I did a loop to process the data incrementally in slices:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å…‹æœè¿™ä¸ªé—®é¢˜ï¼Œæˆ‘åšäº†ä¸€ä¸ªå¾ªç¯ï¼Œä»¥å¢é‡çš„æ–¹å¼å¤„ç†æ•°æ®åˆ‡ç‰‡ï¼š
- en: '[PRE8]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The implementation details donâ€™t matter, the interesting part is that we donâ€™t
    need to change the code too much to build this final table incrementally. As each
    â€˜sliceâ€™ processed represents a partition, by setting the parameter OVERWRITE_OR_IGNORE
    to 1, DuckDB will automatically overwrite any existing data for that partition
    or ignore it if it already exists.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å®ç°ç»†èŠ‚å¹¶ä¸é‡è¦ï¼Œå…³é”®æ˜¯æˆ‘ä»¬ä¸éœ€è¦å¯¹ä»£ç åšå¤ªå¤šæ”¹åŠ¨ï¼Œå°±èƒ½é€æ­¥æ„å»ºè¿™ä¸ªæœ€ç»ˆè¡¨æ ¼ã€‚ç”±äºæ¯ä¸ªå¤„ç†çš„â€œåˆ‡ç‰‡â€ä»£è¡¨ä¸€ä¸ªåˆ†åŒºï¼Œé€šè¿‡å°†å‚æ•° OVERWRITE_OR_IGNORE
    è®¾ç½®ä¸º 1ï¼ŒDuckDB ä¼šè‡ªåŠ¨è¦†ç›–è¯¥åˆ†åŒºçš„ä»»ä½•ç°æœ‰æ•°æ®ï¼Œæˆ–è€…å¦‚æœæ•°æ®å·²å­˜åœ¨ï¼Œåˆ™å¿½ç•¥å®ƒã€‚
- en: Originally, we had 63GB, after ~1 hour and 20 minutes, we ended up with 15GB
    of Parquet.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: åŸæœ¬æˆ‘ä»¬æœ‰ 63GBï¼Œçº¦ 1 å°æ—¶ 20 åˆ†é’Ÿåï¼Œæœ€ç»ˆå¾—åˆ°äº† 15GB çš„ Parquet æ–‡ä»¶ã€‚
- en: '![](../Images/21c5aa0e745157d524083a297e1ab2c0.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21c5aa0e745157d524083a297e1ab2c0.png)'
- en: Compute metrics and build the OLAP Cube
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®¡ç®—æŒ‡æ ‡å¹¶æ„å»º OLAP Cube
- en: This is a simple step. Now, with each vote represented by a record, all needed
    is to compute the metrics.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªç®€å•çš„æ­¥éª¤ã€‚ç°åœ¨ï¼Œæ¯ä¸ªæŠ•ç¥¨éƒ½ç”±ä¸€ä¸ªè®°å½•è¡¨ç¤ºï¼Œæ‰€éœ€çš„åªæ˜¯è®¡ç®—æŒ‡æ ‡ã€‚
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As we need to compute the metrics in many levels of granularity, the ideal way
    to do this is with a GROUP BY + ROLLUP.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæˆ‘ä»¬éœ€è¦è®¡ç®—å¤šä¸ªç²’åº¦çº§åˆ«çš„æŒ‡æ ‡ï¼Œç†æƒ³çš„åšæ³•æ˜¯ä½¿ç”¨ GROUP BY + ROLLUPã€‚
- en: 'In this case, DuckDB stood out significantly: we started with 15 GB and, after
    36 seconds, the file was reduced to 88 MB!'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªæ¡ˆä¾‹ä¸­ï¼ŒDuckDB è¡¨ç°å¾—å°¤ä¸ºçªå‡ºï¼šæˆ‘ä»¬ä» 15GB å¼€å§‹ï¼Œ36 ç§’åï¼Œæ–‡ä»¶å¤§å°ç¼©å‡åˆ°äº† 88MBï¼
- en: This is a blazing fast performance, it grouped more than 200 million rows in
    4 different levels of granularity, where the highest level has cardinality=2 and,
    the lowest, cardinality=~200,000 in less than a minute!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªæƒŠäººçš„æ€§èƒ½ï¼Œå®ƒåœ¨ä¸åˆ°ä¸€åˆ†é’Ÿçš„æ—¶é—´é‡Œï¼Œå°†è¶…è¿‡ 2 äº¿è¡Œæ•°æ®æŒ‰ 4 ä¸ªä¸åŒçš„ç²’åº¦çº§åˆ«è¿›è¡Œäº†åˆ†ç»„ï¼Œå…¶ä¸­æœ€é«˜çº§åˆ«çš„åŸºæ•°ä¸º 2ï¼Œæœ€ä½çº§åˆ«çš„åŸºæ•°çº¦ä¸º 200,000ï¼
- en: Results
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“æœ
- en: 'The table below summarizes the results:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹è¡¨æ€»ç»“äº†ç»“æœï¼š
- en: '![](../Images/7425a6bf2d7c1611cba5e2da4e673e60.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7425a6bf2d7c1611cba5e2da4e673e60.png)'
- en: 'The total pipelineâ€™s execution time was ~2h30min, executed on WSL with the
    following specs: ~16GB of DDR4 RAM, an Intel 12th generation Core i7 processor,
    and a 1TB NVMe SSD.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: æ•´ä¸ªç®¡é“çš„æ‰§è¡Œæ—¶é—´çº¦ä¸º 2å°æ—¶30åˆ†é’Ÿï¼Œè¿è¡Œåœ¨ WSL ä¸Šï¼Œé…ç½®å¦‚ä¸‹ï¼šçº¦ 16GB çš„ DDR4 å†…å­˜ï¼Œä¸€é¢— Intel ç¬¬ 12 ä»£ Core i7
    å¤„ç†å™¨ï¼Œå’Œ 1TB NVMe SSDã€‚
- en: 'During the process, I noticed that memory usage was a bottleneck, as DuckDB
    constantly created temporary files in the disk in a .temp/ directory. Also, I
    had plenty of problems in running queries with Windows functions: they not only
    took more time than expected to execute, but also the program randomly crashed
    several times.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘æ³¨æ„åˆ°å†…å­˜ä½¿ç”¨æˆä¸ºäº†ç“¶é¢ˆï¼Œå› ä¸º DuckDB ä¸æ–­åœ¨ç£ç›˜çš„ .temp/ ç›®å½•ä¸­åˆ›å»ºä¸´æ—¶æ–‡ä»¶ã€‚å¦å¤–ï¼Œæˆ‘åœ¨è¿è¡Œå¸¦æœ‰ Windows å‡½æ•°çš„æŸ¥è¯¢æ—¶é‡åˆ°äº†å¾ˆå¤šé—®é¢˜ï¼šè¿™äº›æŸ¥è¯¢ä¸ä»…æ‰§è¡Œæ—¶é—´è¶…å‡ºäº†é¢„æœŸï¼Œè¿˜å‡ºç°äº†ç¨‹åºéšæœºå´©æºƒçš„æƒ…å†µã€‚
- en: Despite that, I believe that the performance reached was satisfactory, after
    all, weâ€™re talking about 1/2Tb of data being processed with complex queries by
    just one single machine (thatâ€™s not so strong, compared with clusters of computers).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å¦‚æ­¤ï¼Œæˆ‘è®¤ä¸ºæœ€ç»ˆçš„æ€§èƒ½æ˜¯ä»¤äººæ»¡æ„çš„ï¼Œæ¯•ç«Ÿæˆ‘ä»¬è®¨è®ºçš„æ˜¯ä»…é€šè¿‡ä¸€å°è®¡ç®—æœºï¼ˆç›¸å¯¹äºè®¡ç®—æœºé›†ç¾¤è€Œè¨€ï¼Œæ€§èƒ½ä¸ç®—ç‰¹åˆ«å¼ºå¤§ï¼‰å¤„ç† 1/2TB çš„æ•°æ®ï¼Œå¹¶æ‰§è¡Œå¤æ‚çš„æŸ¥è¯¢ã€‚
- en: Conclusion
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: The fact is that processing data is, sometimes, like refining uranium. We start
    with an enormous mass of raw material and, through a hard, time-consuming, and
    costly process (that, sometimes, puts lives at risk), we extract a small portion
    of the relevant refined information.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: äº‹å®ä¸Šï¼Œå¤„ç†æ•°æ®æœ‰æ—¶å°±åƒæç‚¼é“€çŸ¿ã€‚æˆ‘ä»¬ä»ä¸€å¤§å †åŸææ–™å¼€å§‹ï¼Œé€šè¿‡ä¸€ä¸ªè‰°éš¾ã€è€—æ—¶ä¸”æ˜‚è´µçš„è¿‡ç¨‹ï¼ˆæœ‰æ—¶è¿˜å¯èƒ½ä¼šå±åŠç”Ÿå‘½ï¼‰ï¼Œæå–å‡ºä¸€å°éƒ¨åˆ†æœ‰ç”¨çš„ç²¾ç‚¼ä¿¡æ¯ã€‚
- en: Jokes aside, in my posts, Iâ€™ve explored many ways to perform data processing,
    talking about tools, techniques, data architecturesâ€¦ always looking for the best
    way of doing things. This kind of knowledge is important, as it helps us choose
    the right tool for the right job. The goal of this post was exactly to know what
    kind of job DuckDB solves, and what experience it serves.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: è¯´æ­£ç»çš„ï¼Œåœ¨æˆ‘çš„å¸–å­ä¸­ï¼Œæˆ‘æ¢ç´¢äº†å¾ˆå¤šæ•°æ®å¤„ç†çš„æ–¹å¼ï¼Œè®¨è®ºäº†å·¥å…·ã€æŠ€æœ¯ã€æ•°æ®æ¶æ„â€¦â€¦å§‹ç»ˆåœ¨å¯»æ‰¾æœ€å¥½çš„åšäº‹æ–¹æ³•ã€‚è¿™ç±»çŸ¥è¯†éå¸¸é‡è¦ï¼Œå› ä¸ºå®ƒå¸®åŠ©æˆ‘ä»¬é€‰æ‹©åˆé€‚çš„å·¥å…·æ¥åšåˆé€‚çš„äº‹ã€‚æœ¬æ–‡çš„ç›®æ ‡æ­£æ˜¯è¦äº†è§£
    DuckDB èƒ½è§£å†³ä»€ä¹ˆæ ·çš„å·¥ä½œï¼Œä»¥åŠå®ƒé€‚åˆä»€ä¹ˆæ ·çš„ä½¿ç”¨ä½“éªŒã€‚
- en: And, in general terms, it was a good experience.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä½“æ¥è¯´ï¼Œè¿™æ˜¯ä¸€æ¬¡ä¸é”™çš„ä½“éªŒã€‚
- en: Working with this database was very smooth, I didnâ€™t have to configure practically
    anything, just imported and manipulated the data with plain-old SQL statements.
    In other words, the tool has an almost zero initial entry barrier for those who
    already know SQL and a little bit of Python. In my opinion, this was DuckDBâ€™s
    big victory. It not only empowered my machine with the ability to process 450Gb
    of data but this was achieved with a low adaptation cost for the environment (and
    the programmer).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¿™ä¸ªæ•°æ®åº“çš„åˆä½œéå¸¸é¡ºåˆ©ï¼Œæˆ‘å‡ ä¹ä¸éœ€è¦é…ç½®ä»»ä½•ä¸œè¥¿ï¼Œåªéœ€å¯¼å…¥æ•°æ®å¹¶ä½¿ç”¨æ™®é€šçš„ SQL è¯­å¥è¿›è¡Œæ“ä½œã€‚æ¢å¥è¯è¯´ï¼Œå¯¹äºé‚£äº›å·²ç»æ‡‚ SQL å’Œä¸€ç‚¹ Python
    çš„äººæ¥è¯´ï¼Œè¿™ä¸ªå·¥å…·å‡ ä¹æ²¡æœ‰ä»€ä¹ˆåˆå§‹ä½¿ç”¨é—¨æ§›ã€‚åœ¨æˆ‘çœ‹æ¥ï¼Œè¿™æ­£æ˜¯ DuckDB çš„å·¨å¤§èƒœåˆ©ã€‚å®ƒä¸ä»…èµ‹äºˆæˆ‘çš„æœºå™¨å¤„ç† 450GB æ•°æ®çš„èƒ½åŠ›ï¼Œè€Œä¸”åœ¨ç¯å¢ƒï¼ˆä»¥åŠç¨‹åºå‘˜ï¼‰é€‚åº”æˆæœ¬ä½çš„æƒ…å†µä¸‹è¾¾æˆäº†è¿™ä¸€ç›®æ ‡ã€‚
- en: In terms of processing speed, considering the complexity of the project, the
    volume of 450Gb, and the fact that I didnâ€™t optimize the database parameters,
    2h30m was a good result. Especially thinking that, without this tool, it would
    be impossible, or extremely complex, to realize this task on my computer.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å°±å¤„ç†é€Ÿåº¦è€Œè¨€ï¼Œè€ƒè™‘åˆ°é¡¹ç›®çš„å¤æ‚æ€§ã€450GB çš„æ•°æ®é‡ä»¥åŠæˆ‘æ²¡æœ‰ä¼˜åŒ–æ•°æ®åº“å‚æ•°ï¼Œ2å°æ—¶30åˆ†é’Ÿæ˜¯ä¸€ä¸ªä¸é”™çš„ç»“æœã€‚å°¤å…¶æ˜¯è€ƒè™‘åˆ°ï¼Œå¦‚æœæ²¡æœ‰è¿™ä¸ªå·¥å…·ï¼Œæƒ³è¦åœ¨æˆ‘çš„ç”µè„‘ä¸Šå®Œæˆè¿™ä¸ªä»»åŠ¡å‡ ä¹æ˜¯ä¸å¯èƒ½çš„ï¼Œæˆ–è€…è¯´æ˜¯æå…¶å¤æ‚çš„ã€‚
- en: DuckDB is somewhat between Pandas and Spark. For small volumes of data, Pandas
    can be more attractive in terms of usability, especially for folks with some background
    in programming, as the package has many built-in transformations that could be
    tricky to implement in SQL. It also has seamless integration with many other Python
    packages, including DuckDB. For enormous volumes of data, Spark will probably
    be a better alternative, with the parallelism, clusters, and all that stuff. So,
    DuckDB fills a blind spot of medium-to-not-so-large projects, where using pandas
    would be impossible and Spark, overkill.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: DuckDBåœ¨Pandaså’ŒSparkä¹‹é—´æŸç§ç¨‹åº¦ä¸Šæ˜¯ä¸€ä¸ªæŠ˜è¡·æ–¹æ¡ˆã€‚å¯¹äºå°è§„æ¨¡æ•°æ®ï¼ŒPandasåœ¨å¯ç”¨æ€§æ–¹é¢å¯èƒ½æ›´å…·å¸å¼•åŠ›ï¼Œå°¤å…¶æ˜¯å¯¹äºé‚£äº›æœ‰ä¸€å®šç¼–ç¨‹èƒŒæ™¯çš„äººï¼Œå› ä¸ºè¿™ä¸ªåŒ…æœ‰è®¸å¤šå†…ç½®çš„è½¬æ¢ï¼Œè¿™äº›åœ¨SQLä¸­å®ç°èµ·æ¥å¯èƒ½ä¼šå¾ˆæ£˜æ‰‹ã€‚å®ƒè¿˜ä¸è®¸å¤šå…¶ä»–PythonåŒ…ï¼ŒåŒ…æ‹¬DuckDBï¼Œå…·æœ‰æ— ç¼é›†æˆã€‚å¯¹äºæå¤§è§„æ¨¡çš„æ•°æ®ï¼ŒSparkå¯èƒ½æ˜¯æ›´å¥½çš„é€‰æ‹©ï¼Œå…·å¤‡å¹¶è¡Œå¤„ç†ã€é›†ç¾¤ç­‰ç‰¹æ€§ã€‚å› æ­¤ï¼ŒDuckDBå¡«è¡¥äº†ä¸­å‹åˆ°ä¸ç®—å¤ªå¤§çš„é¡¹ç›®çš„ç©ºç™½ï¼Œåœ¨è¿™äº›é¡¹ç›®ä¸­ä½¿ç”¨Pandasæ˜¯ä¸å¯è¡Œçš„ï¼Œè€Œä½¿ç”¨Sparkåˆ™è¿‡äºå¤æ‚ã€‚
- en: DuckDB extends the limits that a single machine can reach and expands the projects
    that can be developed locally, bringing speed to the analysis/manipulation of
    large volumes of data. Without a doubt, it is a powerful tool that I will proudly
    add to my toolbox.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: DuckDBæ‰©å±•äº†å•å°æœºå™¨èƒ½å¤Ÿè¾¾åˆ°çš„æé™ï¼Œå¹¶æ‰©å¤§äº†å¯ä»¥åœ¨æœ¬åœ°å¼€å‘çš„é¡¹ç›®èŒƒå›´ï¼Œä¸ºå¤§è§„æ¨¡æ•°æ®åˆ†æ/æ“ä½œå¸¦æ¥äº†é€Ÿåº¦ã€‚æ¯«æ— ç–‘é—®ï¼Œå®ƒæ˜¯ä¸€ä¸ªå¼ºå¤§çš„å·¥å…·ï¼Œæˆ‘ä¼šè‡ªè±ªåœ°å°†å®ƒåŠ å…¥åˆ°æˆ‘çš„å·¥å…·ç®±ä¸­ã€‚
- en: Furthermore, I hope this post helped you get a better view of DuckDB. As always,
    Iâ€™m not an expert in any of the subjects addressed in this post, and I strongly
    recommend further reading, my references are listed below and the code is available
    on GitHub.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæˆ‘å¸Œæœ›è¿™ç¯‡æ–‡ç« èƒ½å¸®åŠ©ä½ æ›´å¥½åœ°äº†è§£DuckDBã€‚å’Œå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘å¹¶ä¸æ˜¯æœ¬æ–‡æ‰€æ¶‰åŠä¸»é¢˜çš„ä¸“å®¶ï¼Œæˆ‘å¼ºçƒˆå»ºè®®è¿›ä¸€æ­¥é˜…è¯»ï¼Œä»¥ä¸‹æ˜¯æˆ‘çš„å‚è€ƒæ–‡çŒ®ï¼Œä»£ç ä¹Ÿå¯ä»¥åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚
- en: Thank you for reading! ;)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢é˜…è¯»ï¼ ;)
- en: References
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '*All the code is available in* [*this GitHub repository*](https://github.com/jaumpedro214/urna-logs-data-eng)*.'
  id: totrans-97
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*æ‰€æœ‰ä»£ç å¯åœ¨* [*æ­¤GitHubä»“åº“*](https://github.com/jaumpedro214/urna-logs-data-eng)*ä¸­æ‰¾åˆ°ã€‚'
- en: Interested in more works like this one? Visit my* [*posts repository*](https://github.com/jaumpedro214/posts)*.*
  id: totrans-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æƒ³äº†è§£æ›´å¤šç±»ä¼¼çš„ä½œå“å—ï¼Ÿè®¿é—®æˆ‘çš„* [*æ–‡ç« ä»“åº“*](https://github.com/jaumpedro214/posts)*ã€‚
- en: '[1] *2022 Results â€”Files transmitted for totalizationâ€” TSE Open Data Portal*.
    [Link](https://dadosabertos.tse.jus.br/dataset/resultados-2022-arquivos-transmitidos-para-totalizacao).
    [[ODbL](https://opendatacommons.org/licenses/odbl/)]'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] *2022å¹´ç»“æœ â€” ä¼ è¾“æ–‡ä»¶ç”¨äºæ±‡æ€» â€” TSEå¼€æ”¾æ•°æ®é—¨æˆ·*ã€‚[é“¾æ¥](https://dadosabertos.tse.jus.br/dataset/resultados-2022-arquivos-transmitidos-para-totalizacao)ã€‚[[ODbL](https://opendatacommons.org/licenses/odbl/)]'
- en: '[2] Databricks. (2023, June 29). [*Data + AI Summit Keynote, Thursday Part
    5 â€” DuckDB*](https://www.youtube.com/watch?v=GaHWuQ_cBhA). YouTube.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Databricksã€‚ï¼ˆ2023å¹´6æœˆ29æ—¥ï¼‰ã€‚[*æ•°æ® + AIå³°ä¼šä¸»æ—¨æ¼”è®²ï¼Œæ˜ŸæœŸå››ç¬¬5éƒ¨åˆ† â€” DuckDB*](https://www.youtube.com/watch?v=GaHWuQ_cBhA)ã€‚YouTubeã€‚'
- en: '[3][*DuckDB Official* *Documentation*](https://duckdb.org/docs/). DuckDB.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[3][*DuckDBå®˜æ–¹* *æ–‡æ¡£*](https://duckdb.org/docs/)ã€‚DuckDBã€‚'
- en: '[4][The electronic ballot box](https://international.tse.jus.br/en/electronic-ballot-box/presentation).
    *Superior Electoral Court*.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[4][ç”µå­æŠ•ç¥¨ç®±](https://international.tse.jus.br/en/electronic-ballot-box/presentation)ã€‚*æœ€é«˜é€‰ä¸¾æ³•é™¢*ã€‚'
- en: '[5] Wikipedia contributors. (2023, July 25). [*OLAP cube*](https://en.wikipedia.org/wiki/OLAP_cube).
    Wikipedia.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] ç»´åŸºç™¾ç§‘è´¡çŒ®è€…ã€‚ï¼ˆ2023å¹´7æœˆ25æ—¥ï¼‰ã€‚[*OLAPç«‹æ–¹ä½“*](https://en.wikipedia.org/wiki/OLAP_cube)ã€‚ç»´åŸºç™¾ç§‘ã€‚'
- en: '[6] Duckdb â€” GitHub. [*window performance Â· Issue #7809 Â· duckdb/duckdb*](https://github.com/duckdb/duckdb/issues/7809).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Duckdb â€” GitHubã€‚[*çª—å£æ€§èƒ½ Â· é—®é¢˜ #7809 Â· duckdb/duckdb*](https://github.com/duckdb/duckdb/issues/7809)ã€‚'
- en: '[7] Gunnarmorling. *GitHub â€” gunnarmorling/1brc:* [*1ï¸âƒ£ğŸğŸï¸ The One Billion
    Row Challenge*](https://github.com/gunnarmorling/1brc) *â€” A fun exploration of
    how quickly 1B rows from a text file can be aggregated with Java*.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Gunnarmorlingã€‚*GitHub â€” gunnarmorling/1brc:* [*1ï¸âƒ£ğŸğŸï¸ åäº¿è¡ŒæŒ‘æˆ˜*](https://github.com/gunnarmorling/1brc)
    *â€” ä¸€æ¬¡æœ‰è¶£çš„æ¢ç´¢ï¼Œäº†è§£å¦‚ä½•å¿«é€Ÿæ±‡æ€»æ¥è‡ªæ–‡æœ¬æ–‡ä»¶çš„10äº¿è¡Œæ•°æ®ï¼Œä½¿ç”¨Java*ã€‚'
