- en: My First Billion (of Rows) in DuckDB
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我在 DuckDB 中的第一亿条数据（行）
- en: 原文：[https://towardsdatascience.com/my-first-billion-of-rows-in-duckdb-11873e5edbb5?source=collection_archive---------0-----------------------#2024-05-01](https://towardsdatascience.com/my-first-billion-of-rows-in-duckdb-11873e5edbb5?source=collection_archive---------0-----------------------#2024-05-01)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/my-first-billion-of-rows-in-duckdb-11873e5edbb5?source=collection_archive---------0-----------------------#2024-05-01](https://towardsdatascience.com/my-first-billion-of-rows-in-duckdb-11873e5edbb5?source=collection_archive---------0-----------------------#2024-05-01)
- en: First Impressions of DuckDB handling 450Gb in a real project
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DuckDB 处理 450Gb 数据的初步印象，在实际项目中的应用
- en: '[](https://joaopedro214.medium.com/?source=post_page---byline--11873e5edbb5--------------------------------)[![João
    Pedro](../Images/64a0e14527be213e5fde0a02439fbfa7.png)](https://joaopedro214.medium.com/?source=post_page---byline--11873e5edbb5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--11873e5edbb5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--11873e5edbb5--------------------------------)
    [João Pedro](https://joaopedro214.medium.com/?source=post_page---byline--11873e5edbb5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://joaopedro214.medium.com/?source=post_page---byline--11873e5edbb5--------------------------------)[![João
    Pedro](../Images/64a0e14527be213e5fde0a02439fbfa7.png)](https://joaopedro214.medium.com/?source=post_page---byline--11873e5edbb5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--11873e5edbb5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--11873e5edbb5--------------------------------)
    [João Pedro](https://joaopedro214.medium.com/?source=post_page---byline--11873e5edbb5--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--11873e5edbb5--------------------------------)
    ·12 min read·May 1, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--11873e5edbb5--------------------------------)
    ·阅读时间12分钟·2024年5月1日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f57054e6187d4d4d8a6e084b95284c6f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f57054e6187d4d4d8a6e084b95284c6f.png)'
- en: Duck blueprint. Generated by Copilot Designer.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Duck 蓝图。由 Copilot Designer 生成。
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: The fields of AI, Data Science, and Data Engineering are progressing at full
    steam. Every day new tools, new paradigms, and new architectures are created,
    always trying to solve the problems of the previous ones. In this sea of new opportunities,
    it’s interesting to know a little about the available tools to solve problems
    efficiently. And I’m not talking only about the technicalities, but the scope
    of use, advantages, disadvantages, challenges, and opportunities, something acquired
    with practice.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能、数据科学和数据工程领域正在全速发展。每天都有新的工具、新的范式和新的架构被创造出来，始终试图解决前一个问题。在这片充满新机遇的海洋中，了解一些现有工具来高效解决问题是很有趣的。而我说的不仅仅是技术细节，还有使用范围、优缺点、挑战和机会，这些都是通过实践获得的。
- en: In this post, I’ll describe my first experience in DuckDB (the new hyped database
    for processing huge amounts of data locally on your computer) revisiting an old
    problem that I faced previously — The processing of Logs of Brazilian Electronic
    Ballot Boxes to calculate vote-time metrics. As you’ll see through this post,
    this is a challenging problem that serves as a good benchmark for both performance
    and user experience assessments.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将描述我在 DuckDB（一个用于在本地计算机上处理大量数据的新兴数据库）中的第一次体验，并重新审视我以前遇到的一个老问题——巴西电子投票箱日志的处理，以计算投票时间度量。正如你将通过这篇文章看到的，这是一个具有挑战性的问题，是对性能和用户体验评估的一个良好基准。
- en: The idea is that this post can serve as input for you, who want to know a little
    more about DuckDB, as I will cover both technical aspects, running the problem,
    and calculating the database performance, and more ‘soft’ aspects, like programming
    experience and usability.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这个文章的目的是为你提供一些参考，特别是你想了解更多关于 DuckDB 的内容，我将涵盖技术方面的内容，比如运行问题和计算数据库性能，也会谈及一些“软”方面的内容，比如编程体验和可用性。
- en: '*DuckDB is an Open Source Project [*[*OSD*](https://opensource.org/osd)*],
    the author has no affiliation with DuckDB/DuckDB Labs. The data used is available
    in the* [*ODbL*](https://opendatacommons.org/licenses/odbl/) *License. This project
    is completely free to carry out. It does not require payment for any services,
    data access, or other expenses.*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*DuckDB 是一个开源项目 [*[*OSD*](https://opensource.org/osd)*]，作者与 DuckDB/DuckDB Labs
    没有任何关系。所使用的数据符合* [*ODbL*](https://opendatacommons.org/licenses/odbl/) *许可证。这是一个完全免费的项目，不需要支付任何服务费、数据访问费或其他费用。*'
- en: The Problem
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'The problem consists of processing records from Electronic Ballot Boxes’ Logs
    to obtain statistical metrics about the voting time of Brazilian voters. For example,
    calculate the average time citizens use to vote, collect fingerprints for identification,
    and so on. These metrics should be aggregated in several granularity levels: at
    the country level, state, electoral zone, and electoral section.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题包括处理电子投票箱日志中的记录，以获取关于巴西选民投票时间的统计指标。例如，计算公民投票所需的平均时间、收集指纹用于身份识别等等。这些指标应在不同的粒度层次进行聚合：从国家级、州级、选举区级到选举分区级。
- en: In case you don’t know, Brazil has a 100% electronic voting system, where all
    the 100+ million citizens vote on a single day and the election’s result is computed
    and released near real-time. Votes are collected by thousands of electronic ballot
    boxes spread all over the country.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不知道，巴西拥有100%电子投票系统，所有超过一亿的公民都在同一天投票，选举结果几乎实时计算并公布。投票由成千上万的电子投票箱收集，这些投票箱分布在全国各地。
- en: '![](../Images/c93e2da9a47c8ee0f7722ab4d5e391ed.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c93e2da9a47c8ee0f7722ab4d5e391ed.png)'
- en: Electronic ballot box. [Image from the Brazillian Superior Electoral Court](https://www.tre-rn.jus.br/comunicacao/noticias/2021/Maio/urna-eletronica-25-anos-100-brasileira-e-admirada-pelo-mundo).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 电子投票箱。[来自巴西最高选举法院的图片](https://www.tre-rn.jus.br/comunicacao/noticias/2021/Maio/urna-eletronica-25-anos-100-brasileira-e-admirada-pelo-mundo)。
- en: 'An electronic ballot box is a microcomputer for specific use for election**s**,
    with the following characteristics: resistant, small, light, with energy autonomy,
    and with security features [[4](https://international.tse.jus.br/en/electronic-ballot-box/presentation)].
    Each can hold up to 500 voters, a number chosen to avoid big queues in the voting
    locations.'
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 电子投票箱是一种专用的微型计算机，用于选举**工作**，具有以下特点：坚固、小巧、轻便、具有能源自给能力，并具备安全功能 [[4](https://international.tse.jus.br/en/electronic-ballot-box/presentation)]。每台投票箱最多可以容纳500名选民，这是为了避免投票地点出现长时间排队的情况。
- en: The system is administered by the TSE (Supreme Electoral Court), which shares
    data about the process in its [open data portal](https://dadosabertos.tse.jus.br/)
    [[ODbL](https://opendatacommons.org/licenses/odbl/) License]. The logs are text
    files with an exhaustive list of all events in the ballot box.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 该系统由TSE（最高选举法院）管理，TSE通过其[开放数据门户](https://dadosabertos.tse.jus.br/)分享有关选举过程的数据
    [[ODbL](https://opendatacommons.org/licenses/odbl/)]许可证。日志是文本文件，包含投票箱中所有事件的详细列表。
- en: And that’s where the challenge begins. As the logs register absolutely every
    single event, it’s possible to calculate an enormous amount of metrics from them;
    it’s a vibrant information fountain. But what makes them rich, also makes them
    extremely hard to handle, as the totality of all the country’s records reaches
    the milestone of 450Gb in TSV files with + 4 billion lines.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是挑战的开始。由于日志记录了每一个事件，因此可以从中计算出大量的指标；它是一个充满活力的信息源泉。但正是这种丰富性，使得它们变得极为难以处理，因为整个国家的记录总量已经达到了450GB，TSV文件中有超过40亿行。
- en: Besides the volume, another thing that makes this work a good benchmark, in
    my opinion, is that the needed transformations to reach our final goal are from
    all sorts of complexities, from simple (where, group by, order by) to complex
    SQL operations (like windows functions).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数据量，另一方面使这项工作成为一个良好基准的原因是，达成我们最终目标所需的转换涉及了各种复杂度的操作，从简单的（如 `where`、`group by`、`order
    by`）到复杂的 SQL 操作（如窗口函数）。
- en: DuckDB
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DuckDB
- en: With this relatively high volume of data, one can be willing to evoke traditional
    Big Data tools, like Apache Spark, and process this data in a cluster with many
    workers, several gigabytes of RAM, and a dozen CPUs.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在如此高的数据量下，人们可能会愿意调用传统的大数据工具，如 Apache Spark，并在集群中使用多个工作节点、几GB的RAM和十多个CPU来处理这些数据。
- en: DuckDB was created to challenge this *status quo*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: DuckDB的创建就是为了挑战这一*现状*。
- en: As its creator defends ([in this video](https://youtu.be/GaHWuQ_cBhA)), it is
    a database thought to empower single machines with the ability to process large
    volumes of data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其创始人在[这个视频](https://youtu.be/GaHWuQ_cBhA)中所辩护的那样，这是一种数据库设计，旨在赋予单一机器处理大量数据的能力。
- en: I.e., instead of looking for complex industry solutions — like PySpark — or
    cloud-based solutions — like Google BigQuery — one will use a local in-process
    database with standard SQL to realize the needed transformations.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，放弃寻求复杂的行业解决方案——如 PySpark——或基于云的解决方案——如 Google BigQuery——而是使用本地进程数据库，利用标准
    SQL 实现所需的转换。
- en: So, in a nutshell, DuckDB is an in-process (that runs in the program itself,
    it has no independent process, resembling SQLite), OLAP (adjusted to analytical
    loads), that handles data in traditional formats (CSV, parquet), optimized to
    handle large volumes of data using the power of a single machine (that doesn’t
    need to be very powerful).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 总而言之，DuckDB是一个内嵌式（运行在程序内部，没有独立进程，类似于SQLite）、OLAP（针对分析负载进行优化）、能够处理传统格式数据（CSV、Parquet）的数据库，优化了在单台机器上处理大量数据的能力（不需要特别强大的机器）。
- en: The Data
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据
- en: A ballot box’s log is a single TSV file with a standardized name — XXXXXYYYYZZZZ.csv,
    composed of the box’s location metadata, with the 5 first digits being the city
    code, the next 4 the electoral zone (a geographical state’s subdivision), and
    the last 4 the electoral section (the ballot box itself).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一个投票箱的日志是一个标准化名称的单一TSV文件 — XXXXXYYYYZZZZ.csv，包含投票箱位置的元数据，其中前5位数字是城市代码，接下来的4位是选举区（一个地理州的子区域），最后4位是选举区段（即投票箱本身）。
- en: 'There are almost 500,000 ballot boxes in Brazil, so, almost 500.000 files.
    The file’s size depends on the number of voters in the section, which varies from
    1 to 500\. This is what the logs look like:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 巴西几乎有50万个投票箱，因此几乎有50万个文件。文件的大小取决于该选区选民的数量，范围从1到500。这就是日志的样子：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We’re interested in transforming this raw information into statistical metrics
    about voting time(How much time each voter takes to vote? How many votes are computed
    each minute?) in several granularity levels (country, state, city) and, to achieve
    that, we’re going to create an [OLAP Cube](https://en.wikipedia.org/wiki/OLAP_cube)
    like that:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是将这些原始信息转化为有关投票时间的统计指标（每个选民投票所需的时间是多少？每分钟计算多少票？），并在不同的粒度层级（国家、州、城市）上实现这一目标，为此我们将创建一个[OLAP立方体](https://en.wikipedia.org/wiki/OLAP_cube)，如图所示：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The Implementation
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现
- en: Setup the environment
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置环境
- en: All that’s needed to run this project is a Python environment with the [DuckDB
    package installed](https://duckdb.org/docs/guides/python/install.html).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此项目所需的仅仅是一个安装了[DuckDB包的Python环境](https://duckdb.org/docs/guides/python/install.html)。
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Transforming the data
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换数据
- en: In the following sections, I’ll describe each transformation, its objectives,
    how DuckDB can perform each one, the advantages, challenges, results, and conclusions.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我将描述每个转换的目标、DuckDB如何执行每个转换、优点、挑战、结果和结论。
- en: 'The processing is divided into 4 steps: Convert TSV files to Parquet; Filter
    and Clear; Isolate votes and their attributes; and Compute metrics to the OLAP
    Cube.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 处理过程分为4个步骤：将TSV文件转换为Parquet；过滤和清理；隔离选票及其属性；并计算OLAP立方体的指标。
- en: '![](../Images/441568ddc6e145b3636bcef42c853242.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/441568ddc6e145b3636bcef42c853242.png)'
- en: Processing Steps. Image by Author.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 处理步骤。图片由作者提供。
- en: Unfortunately, to avoid making this post enormous, I’ll not explain each transformation
    in detail. But all the code is available on the [GitHub repository](https://github.com/jaumpedro214/urna-logs-data-tseng).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，为了避免使这篇文章过于庞大，我不会详细解释每个转换。但所有代码都可以在[GitHub仓库](https://github.com/jaumpedro214/urna-logs-data-tseng)中找到。
- en: '**Converting TSV files to Parquet**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**将TSV文件转换为Parquet**'
- en: A simple and indispensable step for anyone who wants to work with large volumes
    of data. Doing this on DuckDB is straightforward.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何想要处理大量数据的人来说，这是一个简单而不可或缺的步骤。在DuckDB中进行此操作非常直接。
- en: 'First, create a DuckDB session:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，创建一个DuckDB会话：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this example, we instantiate the database connector with an empty string.
    This is done to indicate that DuckDB should not create its own database file;
    rather, it should only interact with system files. As mentioned earlier, DuckDB
    is a database, so it has the functionalities to create tables, views, and so on,
    which we won’t explore here. We’ll focus solely on using it as a transformation
    engine.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们用一个空字符串实例化数据库连接器。这样做是为了表明DuckDB不应该创建自己的数据库文件，而是仅与系统文件交互。正如之前所提到的，DuckDB是一个数据库，因此它具有创建表、视图等功能，但我们在这里不会探讨这些功能。我们将专注于将其用作转换引擎。
- en: 'And define the following query:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 并定义以下查询：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: And that’s all!
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！
- en: 'Let’s detail the query:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看这个查询：
- en: The inner expression is just a standard *SELECT * FROM table* query, the only
    difference is that, instead of referencing a table, DuckDB can reference files
    directly.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 内部表达式只是一个标准的*SELECT * FROM table* 查询，唯一的区别是，DuckDB可以直接引用文件，而不是引用表。
- en: 'The result of this query could be imported to a pandas dataframe for further
    expression, just like this:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这个查询的结果可以导入到pandas数据框中进行进一步的表达，就像这样：
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Which allows seamless integration between DuckDB and pandas.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得 DuckDB 与 pandas 之间的无缝集成成为可能。
- en: The outer expression is a simple *COPY … TO …* , which writes the inner query’s
    result as a file.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 外部表达式是一个简单的 *COPY … TO …*，它将内部查询的结果写入文件。
- en: In this first transformation, we can start to see one of the strengths of DuckDB—
    the ability to interact with files using plain old SQL, without needing to configure
    anything else. The above query is not different at all from day-to-day operations
    that we make in standard SGBDs, like PostgreSQL and MySQL, with the only difference
    being that, instead of manipulating tables, we’re interacting with files.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个第一次转换中，我们开始看到 DuckDB 的一个优势——能够使用纯 SQL 与文件进行交互，而无需配置任何其他内容。上述查询与我们在标准 SGBD（如
    PostgreSQL 和 MySQL）中执行的日常操作完全相同，唯一的区别是，我们不再操作表格，而是与文件进行交互。
- en: Originally, we had **450Gb** of TSV files and, after ~**30min**, we ended up
    with **97Gb** of Parquet.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 原本我们有 **450GB** 的 TSV 文件，约 **30 分钟** 后，我们剩下了 **97GB** 的 Parquet 文件。
- en: Filter and Clear
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过滤并清除
- en: As mentioned earlier, the Logs store every event that happens on a ballot box.
    This first step aims to filter only vote-related events, like ‘*The voter voted
    for PRESIDENT*’, ‘*The Voter had fingerprints collected*’, and ‘*The vote was
    computed*’ that happened on the election days (that’s important, as the logs also
    store training sections and other administrative procedures realized).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，日志存储了选票箱上发生的每个事件。第一步旨在过滤出仅与投票相关的事件，如“*选民投票选举了总统*”、“*选民采集了指纹*”以及“*投票已计算*”，这些事件发生在选举日（这很重要，因为日志还记录了培训部分和其他行政程序）。
- en: 'A simple query, but with a lot of text and date manipulations:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的查询，但包含大量文本和日期处理：
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In this query, another advantage of DuckDB is highlighted, the ability to read
    and write partitioned data. Table partitioning is very relevant in the context
    of Big Data, but is still even more significant in the single-machine paradigm,
    given that we’re operating the same disk for input and output, i.e., it suffers
    twice, and every optimization is welcome.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个查询中，突出了 DuckDB 的另一个优势：能够读取和写入分区数据。表分区在大数据的背景下非常重要，但在单机架构中尤为关键，因为我们在进行输入和输出操作时使用的是同一磁盘，即它要承受两次负担，每一个优化都非常欢迎。
- en: Originally, we had 97Gb, but after ~30min, we were left with 63Gb of Parquet.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 原本我们有 97GB，但约 30 分钟后，我们剩下了 63GB 的 Parquet 文件。
- en: Isolate votes and their attributes
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 隔离投票及其属性
- en: As each vote is composed of several lines, we need to condense all the information
    in a unique record, to ease the calculations. Here things get complicated, as
    the query gets complex and, unfortunately, DuckDB could not process all the data
    in one go.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个投票由多行组成，我们需要将所有信息浓缩成一个唯一的记录，以便简化计算。这里情况变得复杂，因为查询变得复杂，而且不幸的是，DuckDB 无法一次性处理所有数据。
- en: 'To overcome this issue, I did a loop to process the data incrementally in slices:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这个问题，我做了一个循环，以增量的方式处理数据切片：
- en: '[PRE8]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The implementation details don’t matter, the interesting part is that we don’t
    need to change the code too much to build this final table incrementally. As each
    ‘slice’ processed represents a partition, by setting the parameter OVERWRITE_OR_IGNORE
    to 1, DuckDB will automatically overwrite any existing data for that partition
    or ignore it if it already exists.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 实现细节并不重要，关键是我们不需要对代码做太多改动，就能逐步构建这个最终表格。由于每个处理的“切片”代表一个分区，通过将参数 OVERWRITE_OR_IGNORE
    设置为 1，DuckDB 会自动覆盖该分区的任何现有数据，或者如果数据已存在，则忽略它。
- en: Originally, we had 63GB, after ~1 hour and 20 minutes, we ended up with 15GB
    of Parquet.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 原本我们有 63GB，约 1 小时 20 分钟后，最终得到了 15GB 的 Parquet 文件。
- en: '![](../Images/21c5aa0e745157d524083a297e1ab2c0.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21c5aa0e745157d524083a297e1ab2c0.png)'
- en: Compute metrics and build the OLAP Cube
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算指标并构建 OLAP Cube
- en: This is a simple step. Now, with each vote represented by a record, all needed
    is to compute the metrics.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的步骤。现在，每个投票都由一个记录表示，所需的只是计算指标。
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As we need to compute the metrics in many levels of granularity, the ideal way
    to do this is with a GROUP BY + ROLLUP.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们需要计算多个粒度级别的指标，理想的做法是使用 GROUP BY + ROLLUP。
- en: 'In this case, DuckDB stood out significantly: we started with 15 GB and, after
    36 seconds, the file was reduced to 88 MB!'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例中，DuckDB 表现得尤为突出：我们从 15GB 开始，36 秒后，文件大小缩减到了 88MB！
- en: This is a blazing fast performance, it grouped more than 200 million rows in
    4 different levels of granularity, where the highest level has cardinality=2 and,
    the lowest, cardinality=~200,000 in less than a minute!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个惊人的性能，它在不到一分钟的时间里，将超过 2 亿行数据按 4 个不同的粒度级别进行了分组，其中最高级别的基数为 2，最低级别的基数约为 200,000！
- en: Results
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: 'The table below summarizes the results:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 下表总结了结果：
- en: '![](../Images/7425a6bf2d7c1611cba5e2da4e673e60.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7425a6bf2d7c1611cba5e2da4e673e60.png)'
- en: 'The total pipeline’s execution time was ~2h30min, executed on WSL with the
    following specs: ~16GB of DDR4 RAM, an Intel 12th generation Core i7 processor,
    and a 1TB NVMe SSD.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 整个管道的执行时间约为 2小时30分钟，运行在 WSL 上，配置如下：约 16GB 的 DDR4 内存，一颗 Intel 第 12 代 Core i7
    处理器，和 1TB NVMe SSD。
- en: 'During the process, I noticed that memory usage was a bottleneck, as DuckDB
    constantly created temporary files in the disk in a .temp/ directory. Also, I
    had plenty of problems in running queries with Windows functions: they not only
    took more time than expected to execute, but also the program randomly crashed
    several times.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，我注意到内存使用成为了瓶颈，因为 DuckDB 不断在磁盘的 .temp/ 目录中创建临时文件。另外，我在运行带有 Windows 函数的查询时遇到了很多问题：这些查询不仅执行时间超出了预期，还出现了程序随机崩溃的情况。
- en: Despite that, I believe that the performance reached was satisfactory, after
    all, we’re talking about 1/2Tb of data being processed with complex queries by
    just one single machine (that’s not so strong, compared with clusters of computers).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，我认为最终的性能是令人满意的，毕竟我们讨论的是仅通过一台计算机（相对于计算机集群而言，性能不算特别强大）处理 1/2TB 的数据，并执行复杂的查询。
- en: Conclusion
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The fact is that processing data is, sometimes, like refining uranium. We start
    with an enormous mass of raw material and, through a hard, time-consuming, and
    costly process (that, sometimes, puts lives at risk), we extract a small portion
    of the relevant refined information.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，处理数据有时就像提炼铀矿。我们从一大堆原材料开始，通过一个艰难、耗时且昂贵的过程（有时还可能会危及生命），提取出一小部分有用的精炼信息。
- en: Jokes aside, in my posts, I’ve explored many ways to perform data processing,
    talking about tools, techniques, data architectures… always looking for the best
    way of doing things. This kind of knowledge is important, as it helps us choose
    the right tool for the right job. The goal of this post was exactly to know what
    kind of job DuckDB solves, and what experience it serves.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 说正经的，在我的帖子中，我探索了很多数据处理的方式，讨论了工具、技术、数据架构……始终在寻找最好的做事方法。这类知识非常重要，因为它帮助我们选择合适的工具来做合适的事。本文的目标正是要了解
    DuckDB 能解决什么样的工作，以及它适合什么样的使用体验。
- en: And, in general terms, it was a good experience.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，这是一次不错的体验。
- en: Working with this database was very smooth, I didn’t have to configure practically
    anything, just imported and manipulated the data with plain-old SQL statements.
    In other words, the tool has an almost zero initial entry barrier for those who
    already know SQL and a little bit of Python. In my opinion, this was DuckDB’s
    big victory. It not only empowered my machine with the ability to process 450Gb
    of data but this was achieved with a low adaptation cost for the environment (and
    the programmer).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 与这个数据库的合作非常顺利，我几乎不需要配置任何东西，只需导入数据并使用普通的 SQL 语句进行操作。换句话说，对于那些已经懂 SQL 和一点 Python
    的人来说，这个工具几乎没有什么初始使用门槛。在我看来，这正是 DuckDB 的巨大胜利。它不仅赋予我的机器处理 450GB 数据的能力，而且在环境（以及程序员）适应成本低的情况下达成了这一目标。
- en: In terms of processing speed, considering the complexity of the project, the
    volume of 450Gb, and the fact that I didn’t optimize the database parameters,
    2h30m was a good result. Especially thinking that, without this tool, it would
    be impossible, or extremely complex, to realize this task on my computer.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 就处理速度而言，考虑到项目的复杂性、450GB 的数据量以及我没有优化数据库参数，2小时30分钟是一个不错的结果。尤其是考虑到，如果没有这个工具，想要在我的电脑上完成这个任务几乎是不可能的，或者说是极其复杂的。
- en: DuckDB is somewhat between Pandas and Spark. For small volumes of data, Pandas
    can be more attractive in terms of usability, especially for folks with some background
    in programming, as the package has many built-in transformations that could be
    tricky to implement in SQL. It also has seamless integration with many other Python
    packages, including DuckDB. For enormous volumes of data, Spark will probably
    be a better alternative, with the parallelism, clusters, and all that stuff. So,
    DuckDB fills a blind spot of medium-to-not-so-large projects, where using pandas
    would be impossible and Spark, overkill.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: DuckDB在Pandas和Spark之间某种程度上是一个折衷方案。对于小规模数据，Pandas在可用性方面可能更具吸引力，尤其是对于那些有一定编程背景的人，因为这个包有许多内置的转换，这些在SQL中实现起来可能会很棘手。它还与许多其他Python包，包括DuckDB，具有无缝集成。对于极大规模的数据，Spark可能是更好的选择，具备并行处理、集群等特性。因此，DuckDB填补了中型到不算太大的项目的空白，在这些项目中使用Pandas是不可行的，而使用Spark则过于复杂。
- en: DuckDB extends the limits that a single machine can reach and expands the projects
    that can be developed locally, bringing speed to the analysis/manipulation of
    large volumes of data. Without a doubt, it is a powerful tool that I will proudly
    add to my toolbox.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: DuckDB扩展了单台机器能够达到的极限，并扩大了可以在本地开发的项目范围，为大规模数据分析/操作带来了速度。毫无疑问，它是一个强大的工具，我会自豪地将它加入到我的工具箱中。
- en: Furthermore, I hope this post helped you get a better view of DuckDB. As always,
    I’m not an expert in any of the subjects addressed in this post, and I strongly
    recommend further reading, my references are listed below and the code is available
    on GitHub.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我希望这篇文章能帮助你更好地了解DuckDB。和往常一样，我并不是本文所涉及主题的专家，我强烈建议进一步阅读，以下是我的参考文献，代码也可以在GitHub上找到。
- en: Thank you for reading! ;)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！ ;)
- en: References
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*All the code is available in* [*this GitHub repository*](https://github.com/jaumpedro214/urna-logs-data-eng)*.'
  id: totrans-97
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*所有代码可在* [*此GitHub仓库*](https://github.com/jaumpedro214/urna-logs-data-eng)*中找到。'
- en: Interested in more works like this one? Visit my* [*posts repository*](https://github.com/jaumpedro214/posts)*.*
  id: totrans-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 想了解更多类似的作品吗？访问我的* [*文章仓库*](https://github.com/jaumpedro214/posts)*。
- en: '[1] *2022 Results —Files transmitted for totalization— TSE Open Data Portal*.
    [Link](https://dadosabertos.tse.jus.br/dataset/resultados-2022-arquivos-transmitidos-para-totalizacao).
    [[ODbL](https://opendatacommons.org/licenses/odbl/)]'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] *2022年结果 — 传输文件用于汇总 — TSE开放数据门户*。[链接](https://dadosabertos.tse.jus.br/dataset/resultados-2022-arquivos-transmitidos-para-totalizacao)。[[ODbL](https://opendatacommons.org/licenses/odbl/)]'
- en: '[2] Databricks. (2023, June 29). [*Data + AI Summit Keynote, Thursday Part
    5 — DuckDB*](https://www.youtube.com/watch?v=GaHWuQ_cBhA). YouTube.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Databricks。（2023年6月29日）。[*数据 + AI峰会主旨演讲，星期四第5部分 — DuckDB*](https://www.youtube.com/watch?v=GaHWuQ_cBhA)。YouTube。'
- en: '[3][*DuckDB Official* *Documentation*](https://duckdb.org/docs/). DuckDB.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[3][*DuckDB官方* *文档*](https://duckdb.org/docs/)。DuckDB。'
- en: '[4][The electronic ballot box](https://international.tse.jus.br/en/electronic-ballot-box/presentation).
    *Superior Electoral Court*.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[4][电子投票箱](https://international.tse.jus.br/en/electronic-ballot-box/presentation)。*最高选举法院*。'
- en: '[5] Wikipedia contributors. (2023, July 25). [*OLAP cube*](https://en.wikipedia.org/wiki/OLAP_cube).
    Wikipedia.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] 维基百科贡献者。（2023年7月25日）。[*OLAP立方体*](https://en.wikipedia.org/wiki/OLAP_cube)。维基百科。'
- en: '[6] Duckdb — GitHub. [*window performance · Issue #7809 · duckdb/duckdb*](https://github.com/duckdb/duckdb/issues/7809).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Duckdb — GitHub。[*窗口性能 · 问题 #7809 · duckdb/duckdb*](https://github.com/duckdb/duckdb/issues/7809)。'
- en: '[7] Gunnarmorling. *GitHub — gunnarmorling/1brc:* [*1️⃣🐝🏎️ The One Billion
    Row Challenge*](https://github.com/gunnarmorling/1brc) *— A fun exploration of
    how quickly 1B rows from a text file can be aggregated with Java*.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Gunnarmorling。*GitHub — gunnarmorling/1brc:* [*1️⃣🐝🏎️ 十亿行挑战*](https://github.com/gunnarmorling/1brc)
    *— 一次有趣的探索，了解如何快速汇总来自文本文件的10亿行数据，使用Java*。'
