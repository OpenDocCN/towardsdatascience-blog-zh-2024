- en: Exploring the Strategic Capabilities of LLMs in a Risk Game Setting
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索大语言模型在风险博弈环境中的战略能力
- en: 原文：[https://towardsdatascience.com/exploring-the-strategic-capabilities-of-llms-in-a-risk-game-setting-43c868d83c3b?source=collection_archive---------1-----------------------#2024-08-27](https://towardsdatascience.com/exploring-the-strategic-capabilities-of-llms-in-a-risk-game-setting-43c868d83c3b?source=collection_archive---------1-----------------------#2024-08-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/exploring-the-strategic-capabilities-of-llms-in-a-risk-game-setting-43c868d83c3b?source=collection_archive---------1-----------------------#2024-08-27](https://towardsdatascience.com/exploring-the-strategic-capabilities-of-llms-in-a-risk-game-setting-43c868d83c3b?source=collection_archive---------1-----------------------#2024-08-27)
- en: '[STRATEGIC AI](https://medium.com/@hc.ekne/list/strategic-ai-72a460668137)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[战略人工智能](https://medium.com/@hc.ekne/list/strategic-ai-72a460668137)'
- en: In a simulated Risk environment, large language models from Anthropic, OpenAI,
    and Meta showcase distinct strategic behaviors, with Claude Sonnet 3.5 edging
    out a narrow lead
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在一个模拟的《风险》环境中，Anthropic、OpenAI和Meta的巨大语言模型展示了截然不同的战略行为，其中Claude Sonnet 3.5稍微领先。
- en: '[](https://medium.com/@hc.ekne?source=post_page---byline--43c868d83c3b--------------------------------)[![Hans
    Christian Ekne](../Images/c85483d8b5dd89584b996b321b7f4a45.png)](https://medium.com/@hc.ekne?source=post_page---byline--43c868d83c3b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--43c868d83c3b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--43c868d83c3b--------------------------------)
    [Hans Christian Ekne](https://medium.com/@hc.ekne?source=post_page---byline--43c868d83c3b--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@hc.ekne?source=post_page---byline--43c868d83c3b--------------------------------)[![Hans
    Christian Ekne](../Images/c85483d8b5dd89584b996b321b7f4a45.png)](https://medium.com/@hc.ekne?source=post_page---byline--43c868d83c3b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--43c868d83c3b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--43c868d83c3b--------------------------------)
    [汉斯·克里斯蒂安·埃克内](https://medium.com/@hc.ekne?source=post_page---byline--43c868d83c3b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--43c868d83c3b--------------------------------)
    ·32 min read·Aug 27, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--43c868d83c3b--------------------------------)
    ·阅读时间32分钟·2024年8月27日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/6985aa99f03d8aafc79c081e0372f271.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6985aa99f03d8aafc79c081e0372f271.png)'
- en: Image generated by the author using DALL-E
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者使用DALL-E生成的图像
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: In recent years, large language models (LLMs) have rapidly become a part of
    our everyday lives. Since OpenAI blew our minds with GPT-3 we have witnessed a
    profound increase in the capabilities of the models. They are excelling in a myriad
    of different tests, in anything from language comprehension to reasoning and problem-solving
    tasks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，大型语言模型（LLMs）已迅速成为我们日常生活的一部分。自从OpenAI用GPT-3震撼了我们的思维以来，我们见证了模型能力的深刻提升。它们在众多不同的测试中表现出色，从语言理解到推理和解决问题的任务，涵盖了方方面面。
- en: One topic that I find particularly compelling — and perhaps under-explored —
    is the ability of LLMs to reason strategically. That is, how the models will act
    if you insert them into a situation where the outcome of their decisions depends
    not only on their own actions but also on the actions of others, who are also
    making decisions based on their own goals. The LLMs’ ability to think and act
    strategically is increasingly important as we weave them into our products and
    services, and especially considering the emerging risks associated with powerful
    AIs.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为特别引人注目——也许是尚未充分探索——的一个话题是大语言模型（LLMs）进行战略推理的能力。也就是说，如果你将它们置于一个决策结果不仅取决于它们自身行为，还取决于其他同样根据自身目标做出决策的个体的行为的情境中，它们将如何行动。随着我们将这些模型更加紧密地融入到我们的产品和服务中，特别是考虑到与强大人工智能相关的潜在风险，LLMs的战略思考和行动能力变得越来越重要。
- en: A decade ago, philosopher and author Nick Bostrom brought AI risk into the spotlight
    with his influential book *Superintelligence*. He started a global conversation
    about AI, and it brought AI as an existential risk into the popular debate. Although
    the LLMs are still far from Bostrom’s superintelligence, it’s important to keep
    an eye on their strategic capabilities as we integrate them tighter into our daily
    lives.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 十年前，哲学家兼作家尼克·博斯特罗姆通过其影响力巨大的著作《超级智能》将AI风险推到了聚光灯下。他启动了一场关于人工智能的全球讨论，将人工智能作为存在性风险引入了大众辩论。尽管LLMs距离博斯特罗姆所说的超级智能还远，但随着我们将它们更加紧密地融入到日常生活中，关注它们的战略能力仍然至关重要。
- en: When I was a child, I used to love playing board games, and Risk was one of
    my favorites. The game requires a great deal of strategy and if you don’t think
    through your moves, you will likely be decimated by your opponents. Risk serves
    as a good proxy for evaluating strategic behavior, because making strategic decisions
    often involves weighing potential gains against uncertain outcomes, and while
    for small troop sizes, luck clearly plays an big part, given enough time and larger
    army sizes, the luck component becomes less pronounced and the most skillful players
    emerge. So, what better arena to test the LLMs’ strategic behavior than Risk!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当我还是个孩子的时候，我非常喜欢玩桌面游戏，而《Risk》是我最喜欢的游戏之一。该游戏需要极高的战略性，如果你没有深思熟虑地进行决策，你很可能会被对手摧毁。《Risk》是评估战略行为的一个很好的代理，因为做出战略决策通常涉及权衡潜在收益与不确定结果，尽管在小规模军队的情况下，运气显然起着很大的作用，但在足够的时间和更大规模的军队下，运气因素会变得不那么突出，最熟练的玩家会脱颖而出。那么，哪里更适合测试
    LLM 的战略行为呢？当然是《Risk》！
- en: In this article I explore two main topics related to LLMs and strategy. Firstly,
    which of the top LLM models is the most strategic Risk player and how strategic
    is the best model in its actions? Secondly, how have the strategic capabilities
    of the models developed through the model iterations?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我探讨了与 LLM 和策略相关的两个主要话题。首先，哪些顶级 LLM 模型是最具战略性的《Risk》玩家，最佳模型在其行动中的战略性有多强？其次，随着模型迭代的发展，这些模型的战略能力如何变化？
- en: To answer these questions, I built a virtual Risk game engine and let the LLMs
    battle it out. The first part of this article will explore some of the details
    of the game implementation before we move on to analyzing the results. We then
    discuss how the LLMs approached the game and their strategic abilities and shortcomings,
    before we end with a section on what these results mean and what we can expect
    from future model generations.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这些问题，我构建了一个虚拟的《Risk》游戏引擎，让 LLM 们进行对战。本文的第一部分将探讨游戏实现的一些细节，之后我们将分析实验结果。接着，我们讨论
    LLM 如何处理游戏，它们的战略能力与不足，最后我们将以对这些结果的意义的讨论以及对未来模型代际的期望作为结尾。
- en: Setting the Stage
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置舞台
- en: '![](../Images/a36d9b074821175cb83aa682b4ab4319.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a36d9b074821175cb83aa682b4ab4319.png)'
- en: Image generated by author using DALL-E
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者使用 DALL-E 生成的图像
- en: '**Why Risk?**'
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**为什么选择《Risk》？**'
- en: My own experience playing Risk obviously played a part in choosing this game
    as a testbed for the LLMs. The game requires players to understand how their territories
    are linked, balance offense with defense and all while planning long-term strategies.
    Elements of uncertainty are also introduced through dice rolls and unpredictable
    opponent behavior, challenging AI models to manage risk and adapt to changing
    conditions.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我自己玩《Risk》的经历显然在选择这个游戏作为 LLM 测试平台时起了作用。这个游戏要求玩家理解自己的领土是如何相互关联的，平衡进攻与防守，同时还要规划长期战略。游戏中还通过掷骰子和不可预测的对手行为引入了不确定性，挑战
    AI 模型管理风险并适应变化的条件。
- en: Risk simulates real-world strategic challenges, such as resource allocation,
    adaptability, and pursuing long-term goals amid immediate obstacles, making it
    a valuable proxy for evaluating AI’s strategic capabilities. By placing LLMs in
    this environment, we can observe how well they handle these complexities compared
    to human players.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 《Risk》模拟了现实世界中的战略挑战，如资源分配、适应能力以及在面临即时障碍时追求长期目标，这使得它成为评估 AI 战略能力的宝贵代理。通过将 LLM
    放入这样的环境中，我们可以观察它们与人类玩家相比，如何应对这些复杂性。
- en: '**The Modelling Environment**'
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**建模环境**'
- en: To conduct the experiments, I created a small python package creatively named
    `risk_game`. (See the appendix for how to get started running this on your own
    machine.) The package is a Risk game engine, and it allows for the simulation
    of games played by LLMs. (The non-technical reader can safely skip this part and
    continue to the section “The Flow of the Game”.)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行实验，我创建了一个名为`risk_game`的小型 Python 包。（有关如何在您自己的机器上运行该包的说明，请参见附录。）该包是一个《Risk》游戏引擎，允许模拟由大型语言模型（LLM）进行的游戏。（非技术读者可以跳过这一部分，继续阅读“游戏流程”部分。）
- en: To make it easier to conceptually keep track of the moving parts, I followed
    an object-oriented approach to the package development, where I developed a few
    different key classes to run the simulation. This includes a game master class
    to control the flow of the game, a player class to control prompts sent to the
    LLMs and a game state class to control the state of the game, including which
    player controls which territories and how many troops they hold at any given time.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更容易地在概念上追踪各个部分的运作，我在包的开发中采用了面向对象的方法，开发了几个关键的类来运行模拟。这包括一个游戏主持人类，用来控制游戏流程，一个玩家类，用来控制发送给
    LLM 的提示信息，以及一个游戏状态类，用来控制游戏的状态，包括哪个玩家控制哪个领土，以及他们在任何时刻拥有多少部队。
- en: 'I tried to make it a flexible and extensible solution for AI-driven strategy
    simulations, and the package could potentially be modified to study strategic
    behavior of LLMs in other settings as well. See below for a full overview of the
    package structure:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我尝试使其成为一个灵活且可扩展的 AI 驱动策略模拟解决方案，并且该包有可能被修改以研究 LLM 在其他环境中的战略行为。以下是该包结构的完整概述：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: To run an experiment, I would first instantiate a `GameConfig` object. This
    config objects holds all the related game configuration settings, like whether
    we played with progressive cards, whether or not capitals mode was active, and
    how high percent of the territories needed to be controlled to win, in addition
    to multiple other game settings. I would then used that to create an instance
    of the `Experiment` class and call the `run_experiment` method.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行一个实验，我会首先实例化一个`GameConfig`对象。这个配置对象包含了所有相关的游戏配置设置，比如是否启用了渐进式卡片，是否启用了首都模式，以及赢得游戏所需控制的领土百分比等其他多个游戏设置。然后我会用它创建一个`Experiment`类的实例，并调用`run_experiment`方法。
- en: Diving deeper behind the scenes we can see how the `Experiment`class is set
    up.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 更深入地了解背后的实现，我们可以看到`Experiment`类是如何设置的。
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: From the code above, we see that the `run_experiment()` method will run the
    number of games that are specified in the initialization of the `Experiment` object.
    The first thing that happens is to initialize a game, and the first thing we need
    to do is to create the rules and instantiate at game with the `GameMaster` class.
    Subsequently, the chosen mix of LLM player agents are added to the game. This
    concludes the necessary pre-game set-up and we use the games’ `play_game()`method
    to start playing a game.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的代码中我们看到，`run_experiment()`方法将运行在`Experiment`对象初始化时指定的游戏数量。首先发生的事情是初始化一个游戏，接着我们需要做的第一件事是创建规则并用`GameMaster`类实例化一个游戏。随后，选定的
    LLM 玩家代理会被添加到游戏中。这就完成了游戏开始前的必要设置，我们使用游戏的`play_game()`方法来开始游戏。
- en: 'To avoid becoming too technical I will skip over most of the code details for
    now, and rather refer the interested reader to the Github repo below. Check out
    the `README` to get started:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免过于技术化，我暂时跳过大部分代码细节，转而将感兴趣的读者引导到下面的 GitHub 仓库。查看`README`以开始使用：
- en: '[](https://github.com/hcekne/risk-game.git?source=post_page-----43c868d83c3b--------------------------------)
    [## GitHub — hcekne/risk-game'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/hcekne/risk-game.git?source=post_page-----43c868d83c3b--------------------------------)
    [## GitHub — hcekne/risk-game'
- en: Contribute to hcekne/risk-game development by creating an account on GitHub.
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过在 GitHub 上创建账户来参与 hcekne/risk-game 的开发。
- en: github.com](https://github.com/hcekne/risk-game.git?source=post_page-----43c868d83c3b--------------------------------)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/hcekne/risk-game.git?source=post_page-----43c868d83c3b--------------------------------)'
- en: The Flow of the Game
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 游戏流程
- en: Once the game begins, the LLM player agents are prompted to do initial troop
    placement. The agents take turns placing their troops on their territories until
    all their initial troops have been exhausted.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦游戏开始，LLM 玩家代理将被提示进行初始部队部署。代理们轮流在他们的领土上放置部队，直到所有初始部队都被部署完。
- en: 'After initial troop placement, the first player starts its turn. In Risk a
    turn is comprised of the 3 following phases:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 初始部队部署后，第一位玩家开始他们的回合。在《风险》游戏中，一回合包括以下三个阶段：
- en: '**Phase 1: Card trading and troop placement.** If a player agent wins an attack
    during its turn, it gains a card. Once it has three cards, it can trade those
    in for troops if has the correct combination of infantry, cavalry, artillery or
    wildcard. The player also receives troops as a function of how many territories
    it controls and also if controls any continents.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**阶段 1：卡片交易和部队部署。** 如果玩家代理在其回合中赢得了一次攻击，它会获得一张卡片。当它有三张卡片时，如果有正确的步兵、骑兵、炮兵或万能卡的组合，就可以将这些卡片兑换为部队。玩家还会根据其控制的领土数量和是否控制任何大洲来获得额外的部队。'
- en: '**Phase 2: Attack.** In this phase the player agent can attack other players
    and take over their territories. It is a good idea to attack because that allows
    the player to gain a card for that turn and also gain more territories. The player
    agent can attack as many times as it wishes during a turn.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**阶段 2：攻击。** 在这个阶段，玩家代理可以攻击其他玩家并占领他们的领土。攻击是一个好主意，因为这可以让玩家在这一回合获得一张卡片，并且还能获得更多的领土。玩家代理在一个回合中可以随意攻击多次。'
- en: '**Phase 3: Fortify.** The last phase is the fortify phase, and now the player
    is allowed to move troops from one of its territories to another. However, the
    territories must be connected by territories the player controls. The player is
    only allowed one such fortify move. After this the is finished, the next player
    starts his turn.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**阶段 3：加固。** 最后一个阶段是加固阶段，此时玩家可以将部队从一个领土移至另一个领土。不过，这些领土必须通过玩家控制的其他领土相连。玩家只允许进行一次这样的加固移动。完成后，回合结束，下一位玩家开始其回合。'
- en: At the beginning of each turn, the LLM agents receive dynamically generated
    prompts to formulate their strategy. This strategy-setting prompt provides the
    agent with the current game rules, the state of the board, and possible attack
    vectors. The agent’s response to this prompt guides its decisions throughout the
    turn, ensuring that its actions align with an overall strategic plan.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一回合开始时，LLM代理会收到动态生成的提示，用以制定其策略。这个策略设定提示为代理提供了当前的游戏规则、棋盘状态以及可能的攻击路线。代理对这一提示的回应将在整个回合中指导其决策，确保其行动与整体战略计划保持一致。
- en: 'The request for strategy prompt is given below:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 策略提示的请求如下：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As you can see from the prompt above, there are multiple dynamically generated
    elements that help the player agent better understand the game context and make
    more informed strategic decisions.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，动态生成的多个元素帮助玩家代理更好地理解游戏背景，并做出更有信息支持的战略决策。
- en: 'These dynamically produced elements include:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这些动态生成的元素包括：
- en: '**Rules:** The rules of the game such as whether capitals mode is activated,
    how many percent of the territories are needed to secure a win, etc.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规则：** 游戏的规则，例如是否启用了首都模式，获胜所需占领的领土百分比等。'
- en: '**Current game state**: This is presented to the agent as the different continents
    and the'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**当前游戏状态：** 这一信息展示给代理，包括不同的大洲以及'
- en: '**Formatted Attack Vectors:** These are a collection of the possible territories
    the agent can launch an attack from, to which territories it can attack and the
    maximum number of troops the agent can attack with.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**格式化的攻击路线：** 这是一个可能的领土集合，代理可以从这些领土发动攻击，攻击的目标以及代理可以动用的最大兵力。'
- en: '**The extra territories needed to win the game:** This represents the remaining
    territories the agent needs to capture to win the game. For example, if the total
    territories required to win the game are 28 and the agent holds 25 territories,
    this number would be 3 and would maybe encourage the agent to develop a more aggressive
    strategy for that turn.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**获胜所需额外领土：** 这是代理需要占领的剩余领土数，才能赢得游戏。例如，如果赢得游戏所需的总领土数是28，而代理当前占领了25个领土，那么这个数字就是3，可能会促使代理为这一回合制定更具攻击性的策略。'
- en: For each specific action during the turn — whether it’s placing troops, attacking,
    or fortifying — the agent is given tailored prompts that reflect the current game
    situation. Thankfully, Risk’s gameplay can be simplified because it adheres to
    the Markov property, meaning that optimal moves depend only on the current game
    state, not on the history of moves. This allows for streamlined prompts that focus
    on the present conditions
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在回合中的每个具体行动——无论是部署部队、攻击还是加固——代理都会根据当前的游戏情况收到量身定制的提示。幸运的是，风险游戏的玩法可以简化，因为它符合马尔科夫性质，即最优的行动只依赖于当前的游戏状态，而不依赖于历史的行动。这使得提示能够简化，专注于当前的条件。
- en: The Experimental Setup
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验设置
- en: '![](../Images/dc054b42a45908a69df85bf2d0b5a9b2.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc054b42a45908a69df85bf2d0b5a9b2.png)'
- en: 'To explore the strategic capabilities of LLMs, I designed two main experiments.
    These experiments were crafted to address two key questions:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索LLMs的战略能力，我设计了两个主要实验。这些实验的目的是回答两个关键问题：
- en: '*What is the top performing LLM, and how strategic is it in its actions?*'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*哪个是表现最好的LLM，它在行动中有多具战略性？*'
- en: '*Is there a progression in the strategic capabilities of the LLMs through model
    iterations?*'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*LLMs的战略能力是否在模型迭代中有所进步？*'
- en: Both of these questions can be answered by running two different experiments,
    with a slightly different mix of AI agents.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个问题可以通过运行两个不同的实验来回答，每个实验使用略微不同的AI代理组合。
- en: '**Experiment-1: Evaluating the Top Models**'
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**实验-1：评估顶级模型**'
- en: 'For the first question, I created an experiment using the following top LLM
    models as players:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个问题，我设计了一个实验，使用以下顶级LLM模型作为玩家：
- en: OpenAI’s GPT-4o running off the OpenAI API endpoint
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI的GPT-4o通过OpenAI API端点运行
- en: Anthropic’s claude-3–5-sonnet-20240620 running off the Anthropic API endpoint
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic的claude-3–5-sonnet-20240620通过Anthropic API端点运行
- en: Meta’s llama-3.1–70b-versatile running of the Groq API endpoint
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meta的llama-3.1–70b-versatile通过Groq API端点运行
- en: I obviously wanted to try Meta’s meta.llama3–1–405b-instruct-v1:0 and configured
    it to run off AWS Bedrock, however the response time was painfully slow and made
    simulating games take forever. This is why we run Meta’s 70b model on Groq. It’s
    much faster than AWS bedrock. (If anyone knows how to speed up llama3.1 405b on
    AWS please let me know!)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我显然想尝试Meta的meta.llama3–1–405b-instruct-v1:0，并配置它通过AWS Bedrock运行，然而响应时间极慢，导致模拟游戏耗时过长。这就是为什么我们在Groq上运行Meta的70b模型的原因。它比AWS
    Bedrock要快得多。（如果有人知道如何加速AWS上的llama3.1 405b，请告诉我！）
- en: 'And we formulate our null and alternative hypotheses as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的零假设和备择假设如下：
- en: '**Experiment-1, H0 :** There is no difference in performance among the models;
    each model has an equal probability of winning.'
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**实验-1, H0：** 模型之间没有性能差异；每个模型有相等的获胜概率。'
- en: ''
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Experiment-1, H1​**: At least one model performs better (or worse) than the
    others, indicating that the models do not have equal performance.'
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**实验-1, H1​**：至少有一个模型的表现优于（或劣于）其他模型，表明这些模型的性能不相等。'
- en: '**Experiment-2: Analyzing the Model Generations**'
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**实验-2：分析模型代**'
- en: 'The second experiment aimed to evaluate how strategic capabilities have progressed
    through different iterations of OpenAI’s models. For this, I selected three models:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个实验的目的是评估OpenAI模型在不同迭代中战略能力的进展。为此，我选择了三个模型：
- en: GPT-4o
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-4o
- en: GPT-4o-mini
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-4o-mini
- en: GPT-3.5-turbo-0125
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-3.5-turbo-0125
- en: '**Experiment-2** allows us to see how the strategic capabilities of the models
    have developed across model generations, and also allows us to analyze the difference
    between different size models in the same model generation (GPT-4o vs GPT-4o-mini).
    I chose OpenAI’s solutions because they didn’t have the same restrictive rate
    limits as the other providers.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**实验-2**让我们能够观察模型的战略能力如何在不同模型代之间发展，也使我们能够分析同一模型代中不同规模模型之间的差异（GPT-4o与GPT-4o-mini）。我选择了OpenAI的解决方案，因为它们没有其他提供商那样严格的速率限制。'
- en: 'Similarly, as for **Experiment-1**, for this experiment we can formulate our
    null and alternative hypotheses:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，对于**实验-1**，对于这个实验我们也可以制定零假设和备择假设：
- en: '**Experiment-2, H0:** There is no difference in performance among the models;
    each model has an equal probability of winning'
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**实验-2, H0：** 模型之间没有性能差异；每个模型有相等的获胜概率。'
- en: ''
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Experiment-2, H1A​**: GPT-4o is better than GPT-4o-mini'
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**实验-2, H1A​**：GPT-4o优于GPT-4o-mini'
- en: ''
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Experiment-2, H1B:** GPT-4o and GPT-4o-mini are better than GPT-3.5-turbo'
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**实验-2, H1B：** GPT-4o和GPT-4o-mini优于GPT-3.5-turbo'
- en: Game Setup, Victory Conditions & Card Bonuses
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 游戏设置、胜利条件和卡片奖励
- en: 'Both experiments involved 10 games, each with the same victory conditions.
    There are multiple different victory conditions in Risk, and typical victory conditions
    that players can agree upon are:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个实验都包含10局游戏，每局都有相同的胜利条件。风险游戏中有多种不同的胜利条件，玩家可以达成共识的典型胜利条件是：
- en: Number of controlled territories required for the winner. “World domination”
    is subset of this where one player needs to control all the territories. Other
    typical territory conditions are 70% territory control.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 胜利者所需控制的领土数量。“世界统治”是其中的一个子集，指的是一个玩家需要控制所有领土。其他典型的领土条件是控制70%的领土。
- en: Number of controlled continent(s) required for the winner
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 胜利者所需控制的大陆数量
- en: Control / possession of key areas required for the winner
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 胜利者所需控制/拥有的关键区域
- en: 'Preset time / turn count: whoever controls the most territories after x hours
    or x turns wins.'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预设时间/回合数：在x小时或x回合后，控制最多领土的玩家获胜。
- en: 'In the end I settled for a more pragmatic approach which was a a combination
    of victory conditions that would be easier to fulfill and progressive cards. The
    victory conditions for the games in the experiments were finally chosen to be:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我选择了一个更为务实的方案，结合了易于实现的胜利条件和进阶卡片。实验中的游戏胜利条件最终选择为：
- en: First agent to reach 65% territory dominance or
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个达到65%领土控制或
- en: The agent with the most territories after 17 game rounds of play (Making the
    full game be concluded after at most 51 turns distributed across the three players.)
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在经过17轮游戏（使得整个游戏在最多51回合内由三位玩家完成）后，拥有最多领土的代理人获胜。
- en: For those of you unfamiliar with Risk, progressive cards means that the value
    of the traded cards increase progressively as the game goes on, which is contrasted
    by fixed cards, where the troop value of traded cards are the same throughout
    the game. (4,6,8,10 for the different combinations.) Progressive is generally
    accepted to be a faster game mode.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不熟悉《风险》游戏的玩家，进阶卡片意味着随着游戏的进行，交换的卡片的价值会逐步增加，而固定卡片则是整个游戏过程中交换的卡片的部队值保持不变（不同组合的卡片分别为4、6、8、10）。进阶卡片通常被认为是一种更快的游戏模式。
- en: The Results — Who Conquered the World?
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果——谁征服了世界？
- en: '![](../Images/026bdd04bf3a9d830e8e4c046b982a41.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/026bdd04bf3a9d830e8e4c046b982a41.png)'
- en: Image generated by the author using DALL-E
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者使用DALL-E生成的图像
- en: 'Experiment-1: The Top Models'
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验-1：顶级模型
- en: The results were actually quite astounding — for both experiments. Starting
    with the first, below we show the distribution of wins amongst the three agents.
    Anthropic’s Claude is the winner with 5 wins in total, second place goes to OpenAI’s
    GPT-4o with 3 wins and last place to Meta’s llama3.1 with 2 wins.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 结果实际上非常令人吃惊——对于这两个实验来说，第一实验中，以下是三位代理人之间的胜利分布。Anthropic的Claude获得了5次胜利，排名第二的是OpenAI的GPT-4o，获得了3次胜利，最后是Meta的llama3.1，获得了2次胜利。
- en: '![](../Images/6788b3e54ad8d81ba78591808ecfa9c9.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6788b3e54ad8d81ba78591808ecfa9c9.png)'
- en: Figure 3\. Experiment-1 wins by player, grouped by victory condition / image
    by author
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图3．实验-1按玩家胜利分组的图表，按胜利条件分组 / 图片来源：作者
- en: Because of their long history and early success with GPT-3 I was expecting OpenAI's
    model to be the winner, but it ended up being Anthropic’s Claude which took a
    lead in overall games. I guess if we take a look at how Claude is performing on
    [benchmark tests](https://www.anthropic.com/news/claude-3-5-sonnet), it shouldn’t
    be too unexpected that they come out ahead.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 由于OpenAI在GPT-3的长期历史和早期成功，我本以为OpenAI的模型会是胜者，但最终领先的是Anthropic的Claude。根据[基准测试](https://www.anthropic.com/news/claude-3-5-sonnet)的表现来看，Claude领先也并不令人意外。
- en: Territory Control and Game Flow
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 领土控制与游戏流程
- en: 'If we dive a little deeper in the overall flow of the game and evaluate the
    distribution of territories throughout the game, we find the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们深入分析整个游戏流程，并评估游戏中领土的分布，我们会发现以下情况：
- en: '![](../Images/aa1eddf080dd03ff5fde4148032d1d50.png)![](../Images/6f94cec739c08a2a3b1883dc0577085c.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa1eddf080dd03ff5fde4148032d1d50.png)![](../Images/6f94cec739c08a2a3b1883dc0577085c.png)'
- en: Figure 5\. Experiment-1 territory control per turn / image by author
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图5．实验-1每回合的领土控制情况 / 图片来源：作者
- en: When we examine the distribution of territories throughout the games, a clearer
    picture emerges. On average, Claude managed to gain a lead in territory control
    midway through most games and maintained that lead until the end. Interestingly,
    there was only one instance where a player was eliminated from the game entirely
    — this happened in Game 8, where Llama 3.1 was knocked out around turn 27.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们检查整个游戏中领土的分布时，一个更清晰的画面浮现出来。平均而言，Claude在大多数游戏中能够在中途获得领土控制的领先，并保持这种优势直到游戏结束。有趣的是，游戏中仅有一次玩家被完全淘汰——发生在第8局，Llama
    3.1在第27回合左右被淘汰。
- en: In our analysis, a “turn” refers to the full set of moves made by one player
    during their turn. Since we had three agents participating, each game round typically
    involved three turns, one for each player. As players were eliminated, the number
    of turns per round naturally decreased.
  id: totrans-102
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在我们的分析中，“回合”指的是每个玩家在其回合内进行的一整套操作。由于有三位代理人参与，每一轮游戏通常包含三回合，每个玩家一回合。随着玩家被淘汰，每轮的回合数自然减少。
- en: 'Looking at the evolution of troop strength and territory control we find the
    following:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 通过观察军队力量和领土控制的演变，我们发现以下几点：
- en: '![](../Images/5574ac0390ea9bc95173bd127e2eb94b.png)![](../Images/72b526b75082936ab08dead7e440d04d.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5574ac0390ea9bc95173bd127e2eb94b.png)![](../Images/72b526b75082936ab08dead7e440d04d.png)'
- en: Figure 6\. Experiment-1 change in troop strength throughout the game / image
    by author
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图6\. 实验-1 游戏过程中部队力量的变化 / 图片由作者提供
- en: The troop strength seems to be relatively even, on average, for all the models,
    so that is clearly not the reason why Claude is able to pull off the most wins.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 各模型的部队力量似乎大致相同，因此这显然不是Claude能够取得最多胜利的原因。
- en: 'Statistical Analysis: Is Claude Really the Best?'
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 统计分析：Claude真的是最强的吗？
- en: In this experiment, I aimed to determine whether any of the three models demonstrated
    significantly better performance than the others based on the number of wins.
    Given that the outcome of interest was the frequency of wins across multiple categories
    (the three models), the chi-square goodness-of-fit test is a good statistical
    tool to use.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次实验中，我的目标是确定是否有任何一个模型在获胜次数上显著优于其他模型。鉴于研究的重点是跨多个类别（这三种模型）中的获胜频率，卡方拟合优度检验是一个很好的统计工具。
- en: The test is often used to compare observed frequencies against expected frequencies
    under the null hypothesis, which in this case was that all models would have an
    equal probability of winning. By applying the chi-square test, I could assess
    whether the distribution of wins across the models deviated significantly from
    the expected distribution, thereby helping to identify if any model performed
    substantially better.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 该检验通常用于比较观察到的频率与原假设下的预期频率。在本例中，原假设是所有模型的获胜概率相同。通过应用卡方检验，我可以评估各模型的获胜分布是否与预期分布存在显著偏差，从而帮助确定是否有模型的表现显著更好。
- en: '[PRE3]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The chi-square goodness-of-fit test was conducted based on the observed wins
    for the three models: 5 wins for Claude, 3 wins for GPT-4o, and 2 wins for llama3.1\.
    Under the null hypothesis:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 基于三种模型的观察到的获胜次数（Claude获胜5次，GPT-4o获胜3次，Llama3.1获胜2次），进行了卡方拟合优度检验。根据原假设：
- en: '**Experiment-1, H0 :** There is no difference in performance among the models;
    each model has an equal probability of winning.'
  id: totrans-112
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**实验-1, H0 :** 各模型之间没有性能差异；每个模型的获胜概率相同。'
- en: each model was expected to win approximately 3.33 games out of the 10 trials.
    The chi-square test yielded a statistic of 1.4 with a corresponding p-value of
    0.497\. Since this p-value is much larger than the conventional significance level
    of 0.05, we can’t really say with any statistical rigor that Claude is better
    than the others.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型预计将在10次试验中获胜约3.33次。卡方检验的统计量为1.4，对应的p值为0.497。由于这个p值远大于常规的显著性水平0.05，我们不能用任何统计学上的严谨性来断言Claude比其他模型更优秀。
- en: We can interpret the p-value such that there is a 49.7% chance that we would
    observe an outcome as extreme as (5,3,2) under the null hypothesis, which assumes
    each model has the same probability of winning. So this is actually quite a likely
    scenario to observe.
  id: totrans-114
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们可以这样解读p值：在原假设下，即假设每个模型获胜的概率相同，观察到像(5,3,2)这样的极端结果的概率是49.7%。因此，这实际上是一个相当可能发生的情况。
- en: To make a definitive conclusion, we would need to run more experiments with
    a larger sample size. Unfortunately, rate limits — particularly with Llama 3.1
    hosted on Groq — made this impractical. I invite the eager reader to follow up
    and test themselves. See the appendix for how to run the experiments on your own
    machine.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得出明确的结论，我们需要进行更多实验并增加样本量。不幸的是，速率限制——特别是在Groq上托管的Llama 3.1——使得这变得不切实际。我邀请有兴趣的读者自行跟进并进行测试。有关如何在自己的计算机上运行实验，请参阅附录。
- en: 'Experiment-2: Model Generations'
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验-2：模型生成
- en: The results of Experiment-2 were equally surprising. Contrary to expectations,
    GPT-4o-mini outperformed both GPT-4o and GPT-3.5-turbo. GPT-4o-mini secured 7
    wins, while GPT-4o managed 3 wins, and GPT-3.5-turbo failed to win any games.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 实验-2的结果同样令人惊讶。与预期相反，GPT-4o-mini的表现超越了GPT-4o和GPT-3.5-turbo。GPT-4o-mini赢得了7场比赛，而GPT-4o赢得了3场，GPT-3.5-turbo则未能获胜。
- en: '![](../Images/1b907976ce49a154ef432f34e1646c86.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b907976ce49a154ef432f34e1646c86.png)'
- en: Figure 8\. Number of wins by player and victory condition / image by author
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图8\. 玩家获胜次数与胜利条件 / 图片由作者提供
- en: GPT-4o-mini actually went off with the overall victory. This was also rather
    substantial, with 7 wins of GPT-4o’s 3 and GPT-3.5 turbo’s 0 wins. While GPT-4o
    on average had more troops GPT-4o-mini won most of the games.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4o-mini实际上获得了整体胜利。这一胜利相当显著，GPT-4o取得了3次胜利，而GPT-3.5-turbo则未能获胜，GPT-4o-mini赢得了7局。尽管GPT-4o平均上拥有更多部队，但GPT-4o-mini还是赢得了大部分游戏。
- en: Territory Control and Troop Strength
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 领土控制与部队力量
- en: 'Again, diving deeper and looking at performance in individual games, we find
    the following:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 再次深入分析，查看各个游戏的表现，我们得出以下结论：
- en: '![](../Images/652e25d45a39e3b1f0977b557ad2dea5.png)![](../Images/c6b8388c2167e89e5993f197a2a45306.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/652e25d45a39e3b1f0977b557ad2dea5.png)![](../Images/c6b8388c2167e89e5993f197a2a45306.png)'
- en: Figure 9\. Experiment-2 Average territory control per turn, for all games /
    image by author
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图9. 实验2，每回合的平均领土控制，所有游戏 / 图像由作者提供
- en: The charts above show territory control per turn, on average, as well as for
    all the games. These plots show a confirmation of what we saw in the overall win
    statistics, namely that GPT-4o-mini is on average coming out with the lead in
    territory control by the end of the games. GPT-4o-mini is beating its big brother
    when it actually counts, close to the end of the game!
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表显示了每回合的领土控制情况，平均来看，以及所有游戏的情况。这些图表确认了我们在整体胜利统计中看到的情况，即GPT-4o-mini在游戏结束时，平均上在领土控制上处于领先地位。GPT-4o-mini在关键时刻，也就是接近游戏结束时，超越了其“大哥”GPT-4o！
- en: 'Turning around and examining troop strength, a slightly different picture emerges:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 转过头来，检查部队力量，我们看到了一幅稍微不同的图景：
- en: '![](../Images/6d6a45f00d2db026a2cc801099fc23a5.png)![](../Images/525b3f22fd081aa7e8afc38c2e3a07ff.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d6a45f00d2db026a2cc801099fc23a5.png)![](../Images/525b3f22fd081aa7e8afc38c2e3a07ff.png)'
- en: Figure 10\. Experiment-2 Average total troop strength per turn, for all games
    / image by author
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图10. 实验2，每回合的平均总部队力量，所有游戏 / 图像由作者提供
- en: The above chart shows that on average, the assumed strongest player, GPT-4o
    manages to keep the highest troop strength throughout most of the games. Surprisingly
    it fails to use this troop strength to its advantage! Also, there is a clear trend
    between troop strength and model size and model generation.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示，平均而言，被假定为最强玩家的GPT-4o在大多数游戏中都能够维持最高的部队力量。令人惊讶的是，它未能将这一部队力量转化为优势！此外，部队力量与模型大小及模型代际之间有明显的趋势。
- en: To get some more insights we can also evaluate a few games more in detail and
    look at the heatmap of controlled territories across the turns.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更多见解，我们还可以更详细地评估一些游戏，并查看每回合控制领土的热力图。
- en: '![](../Images/043d2693f7293e2f52cc8cb6cfbc7703.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/043d2693f7293e2f52cc8cb6cfbc7703.png)'
- en: Figure 11\. Experiment 2, heatmap of territory control, game 2 / image by author
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图11. 实验2，领土控制热力图，第2局 / 图像由作者提供
- en: '![](../Images/9888ffe2054a2e82bc5e55e6aea482b4.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9888ffe2054a2e82bc5e55e6aea482b4.png)'
- en: Figure 12\. Experiment 2, heatmap of territory control, game 7 / image by author
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图12. 实验2，领土控制热力图，第7局 / 图像由作者提供
- en: From the heatmaps we see how the models trade blows and grab territories from
    another. Here we have selected two games which seemed reasonably representative
    for the 10 games in the experiment.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 从热力图中可以看到，各模型之间是如何你来我往、争夺领土的。这里我们选取了两局游戏，这两局游戏在实验中的10局中似乎具有较高的代表性。
- en: Regarding specific territory ownership, a trend we saw play out frequently was
    GPT-4o trying to hold North America while GPT-4o-mini often tried to get Asia.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 关于具体的领土所有权，我们频繁看到的趋势是，GPT-4o倾向于控制北美，而GPT-4o-mini则常常争夺亚洲。
- en: 'Statistical Analysis: Generational Differences'
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 统计分析：代际差异
- en: 'With the above results, let’s again revisit our initial hypotheses:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述结果，我们再一次回顾最初的假设：
- en: '**Experiment-2, H0 :** There is no difference in performance among the models;
    each model has an equal probability of winning.'
  id: totrans-139
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**实验2，H0：** 各模型在性能上没有差异；每个模型的胜率相等。'
- en: ''
  id: totrans-140
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Experiment-2, H1A​**: GPT-4o is better than GPT-4o-mini'
  id: totrans-141
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**实验2，H1A​**：GPT-4o优于GPT-4o-mini'
- en: ''
  id: totrans-142
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Experiment-2, H1B:** GPT-4o and GPT-4o-mini are better than GPT-3.5-turbo'
  id: totrans-143
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**实验2，H1B：** GPT-4o和GPT-4o-mini优于GPT-3.5-turbo'
- en: Let’s start with the easy one, **H1B,** namely that GPT-4o and GPT-4o-mini are
    better than GPT-3.5-turbo. This is quite easy to see, and we can do a chi-squared
    test again, based on equal probabilities of winning for each model.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从简单的假设开始，**H1B**，即GPT-4o和GPT-4o-mini优于GPT-3.5-turbo。这一点很容易看出，我们可以再次进行卡方检验，假设每个模型的胜率相等。
- en: '[PRE4]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This suggests that the observed distribution of wins is unlikely to have occurred
    if every model had the same probability of winning, 33.3%. In fact, a case as
    extreme as this could only be expected to have occurred in 2.5% of cases.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明，如果每个模型的获胜概率都是33.3%，那么观察到的胜利分布不太可能发生。事实上，像这样的极端情况只有在2.5%的情况下才可能出现。
- en: 'To then evaluate our **H1A** hypothesis we should first update our null hypothesis
    adjusting for unequal probabilities of winning. For example, we can now assume
    that:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们的**H1A**假设，我们首先应该更新我们的零假设，调整获胜的概率不均等。例如，我们现在可以假设：
- en: 'GPT-4o-mini: Higher probability'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'GPT-4o-mini: 更高的获胜概率'
- en: 'GPT-4o: Higher probability'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'GPT-4o: 更高的获胜概率'
- en: 'GPT-3.5-turbo: Lower probability'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'GPT-3.5-turbo: 更低的获胜概率'
- en: 'Putting some numbers on these, and given the results we just observed, let’s
    assume GPT-4o-mini:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些数字，并结合我们刚刚观察到的结果，假设GPT-4o-mini的情况如下：
- en: 'GPT-40-mini: 45% chance of winning each game'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'GPT-40-mini: 每局游戏45%的获胜概率'
- en: 'GPT-4o: 45% chance of winning each game'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'GPT-4o: 每局游戏45%的获胜概率'
- en: 'GPT-3.5-turbo: 10% chance of winning each game'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'GPT-3.5-turbo: 每局游戏10%的获胜概率'
- en: 'Then, for 10 games, the expected wins would be:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，对于10局游戏，预期的获胜次数是：
- en: 'GPT-4o-mini: 0.45×10=4.5'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'GPT-4o-mini: 0.45 × 10 = 4.5'
- en: 'GPT-4o: 0.45 ×10=4.5'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'GPT-4o: 0.45 × 10 = 4.5'
- en: 'GPT-3.5-turbo: 0.1×10=10 → .1×10=1'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'GPT-3.5-turbo: 0.1 × 10 = 10 → 0.1 × 10 = 1'
- en: 'In addition, given the fact that GPT-4o-mini won 7 out of the 10 games, we
    also revise our alternative hypothesis:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，鉴于GPT-4o-mini在10局游戏中赢了7局，我们也修正了我们的备择假设：
- en: '**Experiment-2 Revised Hypothesis, H1AR**: GPT-4o-mini is better than GPT-4o.'
  id: totrans-160
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**实验-2 修正假设 H1AR**: GPT-4o-mini优于GPT-4o。'
- en: 'Using python to calculate the chi-squared test, we get:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Python计算卡方检验，我们得到：
- en: '[PRE5]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: With our updated probabilities, we see from the code above that seeing a result
    as extreme as we did, (7,3,0) is in fact not very unlikely under our new updated
    expected probabilities. Interpreting the p-value tells us that a results at least
    as extreme as what we observed would be expected 23% of the time. So, we cannot
    conclude with any statistical significance that there is a difference between
    GPT-4o-mini and GPT-4o and we reject the revised alternative hypothesis, **H1AR.**
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们更新后的概率，从上面的代码中可以看到，得到像我们观察到的那样极端的结果（7,3,0）在我们的新更新的预期概率下实际上并不是非常不可能。解释p值告诉我们，至少像我们观察到的这样极端的结果，在23%的情况下是可以预期的。因此，我们不能得出统计学上有显著性差异的结论，因此我们拒绝修正后的备择假设，**H1AR**。
- en: '**Key Takeaways**'
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**关键结论**'
- en: Although there is only limited evidence to suggest Claude is the more strategic
    model, we can with reasonably high confidence state that there is a difference
    in performance across model generations. GPT-3.5-turbo is significantly less strategic
    than its newer iterations. Obviously this implication works in reverse, which
    means we are seeing an increase in the strategic abilities of the models as they
    improve through the generations, and this is likely to profoundly impact how these
    models will be used in the future.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管目前只有有限的证据表明Claude是更具战略性的模型，但我们可以相对有信心地说，不同模型世代之间的性能存在差异。GPT-3.5-turbo显著不如其更新版本具有战略性。显然，这个结论是反向成立的，这意味着我们看到随着模型世代的进步，其战略能力不断增强，这很可能会深刻影响这些模型未来的使用方式。
- en: Analyzing the Strategic Behavior of LLMs
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析LLM的战略行为
- en: '![](../Images/58a82733ec071f0d0f915de1aab5cfd4.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/58a82733ec071f0d0f915de1aab5cfd4.png)'
- en: Image generated by the author using DALL-E
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用DALL-E生成
- en: One of the first things I noticed after running some initial tests were how
    different the LLMs play than humans. The LLM games seem to have more turns than
    human games, even after I prompted them to be more aggressive and try to go after
    weak opponents.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我在进行一些初步测试后，注意到的第一件事是，LLM（大语言模型）与人类玩游戏的方式差异很大。LLM的游戏回合数似乎比人类游戏更多，即使在我提示它们更具攻击性并尝试攻击较弱的对手后也是如此。
- en: 'While many of the observations about player strategy can be made just from
    looking at plots of territory control and troop strength; some of the more detailed
    observations below first became clear as I watched the LLMs play turn-by-turn.
    This is slightly hard to replicate in an article format, however all the data
    from both experiments are stored in .csv files in the Github repo and loaded into
    pandas dataframes in the Jupyter notebooks used for analysis. The interested reader
    can find them in the repo here: `/game_analysis/experiment1_analysis_notebook.ipynb`.
    The dataframe `experiment1_game_data_df`holds all relevant game data for Experiment-1\.
    By looking at territory ownership and troop control turn-by-turn more details
    about the playstyles emerge.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管许多关于玩家策略的观察可以仅通过查看领土控制和军队力量的图表来进行，但一些更为细致的观察，直到我逐步观看LLMs逐回合游戏时才变得更加明显。这在文章格式中有些难以复现，但实验中的所有数据都存储在Github仓库中的.csv文件中，并加载到用于分析的Jupyter笔记本中的pandas数据框中。感兴趣的读者可以在该仓库中找到它们：`/game_analysis/experiment1_analysis_notebook.ipynb`。数据框`experiment1_game_data_df`包含了实验1的所有相关游戏数据。通过逐回合查看领土所有权和军队控制，可以得出更多关于游戏风格的细节。
- en: Distinctive Winning Play Styles
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 独特的制胜游戏风格
- en: What seemed to distinguish Anthropic’s model was its ability to claim a lot
    of territory in one move. This can be seen in some of the plots of territory control,
    when you look at individual games. But even though Claude had the most wins, how
    strategic was it really? Based on what I observed in the experiments, it seems
    that the LLMs are still rather immature when it comes to strategy. Below we discuss
    some of the typical behavior observed through the games.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 看似区分Anthropic模型的特点在于它能够通过一次行动占领大量领土。这在某些领土控制图中可以看到，当你查看单个游戏时就会明白。不过，尽管Claude获得了最多的胜利，但它的策略性到底有多强？根据我在实验中观察到的情况，似乎大型语言模型（LLMs）在策略上仍然相当不成熟。下面我们将讨论一些通过游戏观察到的典型行为。
- en: Poor Fortifying Strategies
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 薄弱的防御策略
- en: A common issue across all models was a failure to adequately fortify their borders.
    Quite frequently, the times the agents were also stuck with a lot of troops inside
    their internal territory instead of guarding their borders. This made it easier
    for neighbors to attack their territories and steal continent bonuses. In addition,
    it made it more difficult for the player agents to actually do a larger land-grab
    since often their territories with large troop strengths were surrounded by other
    territories it controlled.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 所有模型普遍存在的一个问题是未能充分巩固边境防御。经常出现的情况是，代理们把大量军队集中在内部领土中，而没有守护好边界。这使得邻国可以更容易地攻击它们的领土并窃取大陆加成。此外，这也让玩家代理们更难进行大规模的领土扩张，因为它们的强大军力常常被其他它们控制的领土所包围。
- en: Failure to See Winning Moves
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 未能识别制胜之策
- en: Another noticeable shortcoming was the models’ failure to recognize winning
    moves. They don’t seem to realize that they can win in a turn if they play correctly.
    Less so with the stronger models but still present.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个显著的不足是模型未能识别制胜之策。它们似乎没有意识到，如果正确出招，就能在一回合内获胜。较强的模型表现得不那么明显，但问题仍然存在。
- en: For example, for all the games in the simulations we played with 65% territory
    control to win. This means you just need to acquire 28 territories. In one instance
    during Experiment-2 Game 2, OpenAI’s GPT-4o had 24 territories and 19 troops in
    Greenland. It could easily just have taken Europe which has several territories
    with just 1 troop, however, it fails to see the move. This is a move that even
    a relatively inexperienced human player would likely recognize.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在我们进行的所有模拟游戏中，获胜所需的领土控制率是65%。这意味着你只需要占领28个领土。在实验2的第2场游戏中，OpenAI的GPT-4o在格林兰拥有24个领土和19个军队。它本可以轻松占领欧洲，那里的几个领土只有1个军队，但它未能看到这一行动。即便是一个相对缺乏经验的人类玩家也很可能会认出这一动作。
- en: Failure to Eliminate Other Players
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 未能消除其他玩家
- en: The models also frequently failed to eliminate opponents with only a few troops
    remaining, even when it would have been strategically advantageous. More specifically,
    they fail to remove players with only a few troops left and more than 2 cards.
    This would be considered an easy move for most human players, especially when
    playing with progressive cards. The card bonuses quickly escalate, and if an opponent
    only has 10 troops left, but 3 or more cards, taking him down for the cards is
    almost always the right move.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型经常在敌人只剩下少量部队时，仍然未能消灭对手，即便这么做在战略上是有利的。更具体来说，它们未能消除只剩下少数部队且拥有两张以上卡片的玩家。对于大多数人类玩家来说，这被认为是一个简单的操作，尤其是在使用进阶卡片的情况下。卡片奖励迅速增加，如果一个对手只剩下10个部队，但拥有3张或更多的卡片，拿下他来换卡几乎总是正确的选择。
- en: GPT-4o Likes North America
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-4o 喜欢北美
- en: One of the very typical strategies that I saw GPT-4o pursue was to get early
    control over North America. Because of the strong continent bonus and the fact
    that it only requires to be guarded in 3 places means that is a strategically
    good starting point. I suspect the reason that GPT-4o does is because it has read
    as part of its training data that it is a strategically good location.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我看到GPT-4o采取的一个非常典型的策略是尽早控制北美。因为北美有强大的大陆加成，并且只需要在三个地方防守，这意味着它是一个战略上非常好的起点。我怀疑GPT-4o这么做的原因是它在训练数据中读到北美是一个战略上很好的位置。
- en: Top Models Finish More Games
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 顶级模型完成更多游戏
- en: Overall, there is a trend amongst the top models to finish more of the games,
    and achieve the victory conditions than weaker models. Of the games played with
    the top models, only 2 games went to the max game limit, while this happened 6
    times for the weaker models.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，顶级模型在完成更多游戏并实现胜利条件方面表现出了趋势，相比之下较弱的模型则不如。顶级模型所玩的游戏中，只有2场比赛达到了最大游戏时间限制，而较弱模型则有6场达到了这一限制。
- en: '**Limitations of Pre-Trained Knowledge**'
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**预训练知识的局限性**'
- en: A limitation of classic Risk is of course that the LLMs have read about strategies
    for playing Risk online, and then the top models are simply the best ones at executing
    on this. I think the tendency to quickly try to dominate North America highlights
    this. This limitation could be mitigated if we played with randomly generated
    maps instead. This would increase the difficulty level and would provide a higher
    bar for the models. However, given their performance on the current maps I don’t
    think we need to increase the difficulty for the current model generations.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 经典《风险》游戏的一个局限性是，大型语言模型已经读过关于玩《风险》的策略，并且顶级模型只是最擅长执行这些策略的模型。我认为，快速尝试控制北美的倾向突显了这一点。如果我们改为使用随机生成的地图，这一局限性可能会得到缓解。这将增加难度，并为模型提供更高的挑战。然而，鉴于它们在当前地图上的表现，我认为目前的模型代际并不需要提高难度。
- en: General observations
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一般观察
- en: Even the strongest LLMs are still far from mastering strategic gameplay. None
    of the models exhibited behavior that could challenge even an average human player.
    I think we have to wait at least one or two model generations before we can start
    to see a substantial increase in strategic behavior.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是最强大的大型语言模型（LLM），也仍然远未掌握战略游戏的玩法。这些模型没有展示出能够挑战普通人类玩家的行为。我认为我们至少需要等上一代或两代模型，才能开始看到战略行为的显著提升。
- en: That said, dynamically adjusting prompts to handle specific scenarios — such
    as eliminating weak opponents for card bonuses — could improve performance. With
    different and more enhanced prompting the models might be able to put up more
    of a fight. To get that to work though, you would need to manually program in
    a range of possible scenarios that typically occur and offer specialized prompts
    for each scenario.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，动态调整提示以应对特定情境——例如消除弱小对手以获取卡片奖励——可能会改善模型的表现。通过不同且更精细的提示，模型或许能够发挥更强的对抗能力。然而，要实现这一点，您需要手动编程出一系列通常会发生的情境，并为每种情境提供专门的提示。
- en: 'Consider a concrete example where we could see this come into play: Player
    B is weak and only has 4 territories with 10 troops, however player B has 3 Risk
    cards, and you are playing progressive cards and the reward for trading in cards
    is currently 20 troops.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个具体的例子，看看这一点如何发挥作用：B玩家很弱，只有4个领土和10个部队，但B玩家有3张《风险》卡，而你正在玩进阶卡，并且目前交易卡片的奖励是20个部队。
- en: For the sake of this experiment, I didn’t want to make the prompts too specialized,
    because the goal wasn’t to optimize agent behavior in Risk, but rather to test
    their ability to do that themselves, given the game state.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 出于这个实验的考虑，我不想让提示过于专业化，因为目标不是优化在《风险》游戏中的代理行为，而是测试它们在给定游戏状态下自行做到这一点的能力。
- en: What These Results Mean for the Future of AI and Strategy
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这些结果对未来人工智能与战略的意义
- en: '![](../Images/c4820641aa83f44cfde7286079673697.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4820641aa83f44cfde7286079673697.png)'
- en: Image generated by the author using DALL-E
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者使用DALL-E生成的图像
- en: The results from these experiments highlight a few key considerations for the
    future of AI and its strategic applications. While LLMs have shown remarkable
    improvements in language comprehension and problem-solving, their ability to reason
    and act strategically is still in its early stages.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实验的结果突出了未来人工智能及其战略应用的一些关键考虑因素。虽然LLM在语言理解和问题解决方面已经取得了显著进展，但它们在推理和战略行动方面的能力仍处于初步阶段。
- en: '**Strategic Awareness and AI Evolution**'
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**战略意识与人工智能进化**'
- en: As seen in the simulations, the current generation of LLMs struggles with basic
    strategic concepts like fortification and recognizing winning moves. This indicates
    that even though AI models have improved in many areas, the sophistication required
    for high-level strategic thinking remains underdeveloped.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如模拟中所示，目前一代的大型语言模型（LLM）在基本的战略概念上存在困难，比如防御和识别制胜之招。这表明，尽管人工智能模型在许多领域有所进展，但进行高级战略思维所需的复杂性仍然没有得到充分发展。
- en: However, as we clearly saw in Experiment-2, there is a trend towards improved
    strategic thinking, and if this trend keeps going for future generations we probably
    don’t have to wait too long until the models are much more capable. There are
    people claiming the LLMs have already plateaued, however I would be very careful
    assuming that.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如我们在实验-2中清楚地看到的那样，战略思维有了改善的趋势，如果这一趋势继续发展，未来几代的模型可能不会太久就变得更加高效。有些人声称LLM已经达到了瓶颈，但我会非常谨慎地作出这种假设。
- en: '**Implications for Real-World Applications**'
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**现实世界应用的意义**'
- en: The real-world applications of a strategically aware and capable AI agent are
    obviously enormous and cannot be understated. They could be used in anything from
    business strategy to military planning and complex human interaction. A strategic
    AI that can anticipate and react to the actions of others could be incredibly
    valuable — and of course also very dangerous. Below we present three possible
    applications.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 具有战略意识和能力的人工智能代理的现实世界应用显然是巨大的，无法低估。这些代理可以应用于从商业战略到军事规划、复杂人际互动等各个领域。能够预测和反应他人行为的战略性AI可能极具价值——当然也非常危险。以下我们提出三种可能的应用场景。
- en: If we consider a more positive application first, we could imagine everyone
    having a helpful strategic agent guiding them through their daily lives, helping
    make important decisions. This agent could help with anything from financial planning,
    planning daily tasks, to optimizing social interactions and behavior that involves
    the actions of other humans. It could act on your behalf and be goal oriented
    to optimize your interests and well-being.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们首先考虑一个更积极的应用场景，我们可以想象每个人都有一个有帮助的战略代理，指导他们度过日常生活，帮助做出重要决策。这个代理可以在从财务规划、日常任务安排到优化社交互动和涉及其他人行为的行动方面提供帮助。它可以代表你行事，并以目标为导向，优化你的利益和福祉。
- en: 'There are obviously many insidious application areas as well. Think: Autonomous
    fighter drones with on-board strategic capabilities. This might not be too far-fetched
    especially when we consider the relative strength of the smaller models compared
    to their big-brother counterparts, for example GPT-4o vs GPT-4o-mini. Smaller
    models are much easier to deploy on edge devices like drones, and when we see
    how [popular](https://www.atlanticcouncil.org/blogs/ukrainealert/fpv-drones-in-ukraine-are-changing-modern-warfare/)
    drones have become the Russian-Ukraine war, taking the step from first person
    view (FPV) drone to unmanned AI-driven drone might be considered feasible. Perhaps
    even as a back-up option if the drone operator lost contact with the drone.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，也有很多潜在的应用领域。想想看：具备战略能力的自主战斗无人机。这并非完全牵强，尤其是考虑到较小模型与它们的大型兄弟模型（例如GPT-4o与GPT-4o-mini）相比的相对优势。较小的模型更容易部署到像无人机这样的边缘设备上，而我们看到[流行的](https://www.atlanticcouncil.org/blogs/ukrainealert/fpv-drones-in-ukraine-are-changing-modern-warfare/)无人机在俄乌战争中的应用，若从第一人称视角（FPV）无人机发展到无人AI驱动无人机，或许是可行的。如果无人机操作员失去与无人机的联系，甚至可以作为备份选项。
- en: Detailed simulation of social interaction is a third way to use strategically
    aware agents. We could for example create simulations to model specific economic
    or other social phenomena, blending classic agent-based methods with LLMs. Agent
    based modelling (ABM) as a field of research and toolkit for understanding complex
    adaptive systems has existed for decades — I used in my Masters' thesis back in
    2012 — but coupled with much smarter and strategic agents this could potentially
    be game changing.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 详细的社会互动模拟是另一种使用具有战略意识的智能体的方式。我们可以举例来说，创建模拟来建模特定的经济或其他社会现象，将经典的基于智能体的方法与LLM结合起来。基于智能体的建模（ABM）作为理解复杂适应性系统的研究领域和工具箱已经存在了几十年——我在2012年硕士论文中就曾使用过——但如果将其与更智能和具有战略思维的智能体结合，这可能会改变游戏规则。
- en: '**The Importance of Dynamic Prompting**'
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**动态提示的重要性**'
- en: Detailed dynamic prompting is probably one of the best ways to use and interact
    with LLMs in the near future — and perhaps also for the next few model generations
    (GPT-5, Claude 4, etc.). By providing more dynamic scenario-specific prompts and
    letting LLM agents execute specific plans, we might see more sophisticated strategic
    behavior in the next generation of models.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 详细的动态提示可能是未来一段时间内与LLM互动和使用的最佳方式——也许对未来几个模型版本（如GPT-5、Claude 4等）也是如此。通过提供更多动态的情景特定提示，让LLM智能体执行特定的计划，我们可能会看到下一代模型展现出更复杂的战略行为。
- en: This type of “handholding” requires a lot more work from human programmers —
    than just prompting agents directly — however it could be a crucial stepping stone
    until the models become more capable of independent strategic thinking.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这种“手把手指导”的方式需要人类程序员投入更多的工作——而不仅仅是直接提示智能体——但它可能是一个至关重要的过渡阶段，直到这些模型变得更能独立进行战略思考。
- en: One could of course argue that if we provide too detailed and specific prompts
    we are working against the generalized nature of these models, and at that point
    we might as well introduce a different type of optimization algorithm, however
    I think there are many problems where the more open-ended problem-solving abilities
    of the LLMs could be paired with some form of dynamic prompting.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 当然可以有人辩称，如果我们提供过于详细和具体的提示，我们实际上是在违背这些模型的通用性特征，这时我们或许可以引入不同类型的优化算法。然而，我认为有很多问题可以将LLM（大语言模型）更开放式的问题解决能力与某种形式的动态提示结合起来。
- en: '**The Need for New Benchmarks**'
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**对新基准的需求**'
- en: As the LLMs continue to improve, it will also be necessary to develop new benchmarks
    to study them. The traditional benchmarks and tests are well suited to study problem-solving
    in isolated environments, but moving forward we might want to introduce more strategic
    tests, that allow us to understand how the agents behave in situations where they
    need to consider how their actions influence others over time. Games like Risk
    provide a reasonable starting point because of their strategic nature and elements
    of uncertainty.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 随着LLM的不断进步，我们也需要开发新的基准来研究它们。传统的基准和测试非常适合研究在孤立环境中的问题解决，但未来我们可能需要引入更具战略性的测试，帮助我们理解智能体在需要考虑他们的行动如何随着时间推移影响他人的情况下的表现。像《风险》这样的游戏提供了一个合理的起点，因为它们具有战略性特点和不确定性元素。
- en: '**Future Considerations**'
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**未来的考虑**'
- en: Looking ahead, as AI models continue to evolve, it will be important to monitor
    their strategic capabilities closely. We need to ensure that as these models become
    more powerful, they are aligned with human values and ethical considerations.
    The risks associated with strategic AI — such as unintended consequences in high-stakes
    environments — must be carefully managed.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 展望未来，随着人工智能模型的不断发展，密切监测其战略能力将变得至关重要。我们需要确保这些模型在变得更强大的同时，与人类的价值观和伦理考量保持一致。与战略人工智能相关的风险——例如在高风险环境中可能出现的意外后果——必须得到仔细管理。
- en: As smaller models like GPT-4o-mini have shown competitive performance in strategic
    tasks, there is potential for deploying highly capable AI on edge devices, such
    as drones or autonomous systems. This opens up new possibilities for decentralized
    AI applications that require real-time decision-making in dynamic environments.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 由于像GPT-4o-mini这样的较小模型在战略任务中展现出了竞争力，因此将高度有能力的人工智能部署到边缘设备上，如无人机或自主系统，具有潜力。这为需要在动态环境中进行实时决策的去中心化人工智能应用开辟了新的可能性。
- en: Conclusion
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: I think it’s safe to say that that while the strategic capabilities of LLMs
    are improving with each new generation, they still have a long way to go before
    they can rival even a moderately skilled human player. Models like Claude and
    GPT-4o are beginning to show some level of strategic thinking, but their shortcomings
    in areas such as fortification and recognizing winning moves highlight the current
    limitations of AI in complex, multi-agent environments. Nevertheless, the trend
    toward better performance across newer models shows promise for future advancements
    in AI strategy.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为可以肯定地说，虽然大语言模型的战略能力随着每一代的更新有所提升，但它们距离能够与一名中等水平的人工玩家竞争还需要很长一段路要走。像Claude和GPT-4o这样的模型开始显示出一定程度的战略思维，但它们在诸如加固和识别胜利步骤等方面的不足，突显了人工智能在复杂的多智能体环境中的局限性。尽管如此，随着新模型性能的不断提升，人工智能战略的未来发展依然充满希望。
- en: As we continue to integrate AI into more aspects of life, from business to military
    strategy, understanding and refining the strategic capabilities of these systems
    will become increasingly important. While we’re not there yet, the potential for
    AI to handle complex decision-making processes in dynamic environments is incredible.
    It will be super interesting to see how the capabilities of the LLMs evolve over
    time, and if our results that show the improvements of LLMs across model generations
    continue through to GPT-5, GPT-6, Claude 4, Claude 5 etc. I think we are in for
    a wild ride!
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们继续将人工智能融入生活的更多方面，从商业到军事战略，理解和完善这些系统的战略能力将变得越来越重要。虽然我们还没有达到那个程度，但人工智能在动态环境中处理复杂决策过程的潜力是令人难以置信的。看到大语言模型的能力随时间发展，尤其是我们展示的跨模型代际的进步，是否能延续到GPT-5、GPT-6、Claude
    4、Claude 5等模型，将是非常有趣的。我认为我们正迎接一次激动人心的旅程！
- en: If you are interested in developing your own AI driven tools, feel free to reach
    out! I am always happy to explore collaborative opportunities!
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣开发自己的人工智能驱动工具，随时与我联系！我总是乐于探索合作的机会！
- en: Appendix
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录
- en: Here I aim to provide some extra details that while interesting for the more
    technically inclined reader, might not be necessary for the full flow of the article.
    The first topic we touch on is rate limit issues. Then we describe more detailed
    analysis of errors, accumulated turn time used by the agents and parsing of responses
    from the LLMs. In addition, I provide the reader with a short description of how
    to test the code base out by cloning the Github repo and getting started with
    the docker setup.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我的目标是提供一些额外的细节，虽然这些细节对于技术导向的读者来说非常有趣，但对于文章的整体流程可能并非必需。我们首先讨论的是速率限制问题。然后，我们会描述关于错误、代理所用的累计轮次时间以及来自大语言模型（LLM）响应的解析的更详细分析。此外，我还向读者简要介绍如何通过克隆Github仓库并开始使用docker设置来测试代码库。
- en: Rate Limit Issues
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 速率限制问题
- en: There are many actions that needs to be considered every turn, and this leads
    to quite a lot of back-and-forth interaction between the program and the LLM providers.
    One issues that turned out to be slightly problematic for running longer experiments
    was rate limiting.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 每次决策时都有许多行动需要考虑，这导致了程序与大语言模型提供商之间频繁的交互。一个在进行更长时间实验时稍显问题的是速率限制。
- en: Rate limits are something the LLM providers set in place to protect against
    spamming an other potentially disrupting behavior, so even though you have funds
    in the accounts, the providers still limit the amount of tokens you can query.
    For example, Anthropic does a rate limit of 1M token / per day for their best
    model.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 速率限制是LLM提供商为了防止垃圾信息和其他可能扰乱行为而设立的限制，因此即使账户中有资金，提供商仍然限制你可以查询的令牌数量。例如，Anthropic对其最佳模型设定了每天1M令牌的速率限制。
- en: '![](../Images/75e0be47ff4140dbf85276862452a009.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75e0be47ff4140dbf85276862452a009.png)'
- en: Rate limits for Anthropic models, taken from Anthropic console
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: Anthropic模型的速率限制，来自Anthropic控制台
- en: And when you hit your rate limit, your LLM queries are answered with
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 当你达到速率限制时，你的LLM查询将会得到以下回复
- en: '[PRE6]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: For many application areas this might not be a problem, however the simulation
    queries each of the providers multiple times per turn (for strategy evaluation,
    card choice, troop placement, multiple attacks and fortification) so this quickly
    adds up, especially in long games that go over many rounds. I was initially planning
    on doing 10 experiments on with victory condition set to World Domination (this
    means the winner would need to control all 42 territories in the game to win),
    but because of how the LLMs play the game this wasn’t feasible in my time frame.
    The victory conditions had to be adjusted so a winner could be determined at an
    earlier stage.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多应用领域来说，这可能不是问题，但模拟每回合会查询每个提供商多次（用于战略评估、卡片选择、部队部署、多次攻击和防御），因此这会迅速累积，尤其是在游戏持续多回合的情况下。我最初计划做10个实验，胜利条件设定为世界统治（即获胜者需控制游戏中所有42个领土），但由于LLM在游戏中的表现，这在我的时间框架内是不可行的。胜利条件必须做出调整，以便在早期阶段就能决定胜者。
- en: Tracking Errors
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 错误追踪
- en: 'Some of the LLMs in the experiments were also struggling with a large number
    of errors when prompted for moves, this could be anything from trying to place
    troops in territories they didn’t control to fortifying to territories that were
    not connected. I implemented a few variables to track these errors. This was way
    more common with the weaker models as the plots below suggests:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验中，一些LLM在被提示进行操作时也遇到了大量错误，这些错误可能包括尝试在自己不控制的领土上部署部队，或加强不相连领土的防御。我实现了几个变量来追踪这些错误。较弱的模型中这种情况更为常见，正如下面的图表所示：
- en: '![](../Images/b5681a68a04ca5da46fe2bc155d153f0.png)![](../Images/b2a5e49a82cee4a6f18f039b4e426397.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5681a68a04ca5da46fe2bc155d153f0.png)![](../Images/b2a5e49a82cee4a6f18f039b4e426397.png)'
- en: Experiment-1 attack and fortify errors / image by author
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 实验-1 攻击与防御错误 / 图片来源：作者
- en: '![](../Images/1a73141944e7d89c7ab68119d32f9690.png)![](../Images/d2839d2ecda849e5ae8c087df31b4f31.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a73141944e7d89c7ab68119d32f9690.png)![](../Images/d2839d2ecda849e5ae8c087df31b4f31.png)'
- en: Experiment-2 attack and fortify errors / image by author
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 实验-2 攻击与防御错误 / 图片来源：作者
- en: Accumulated Turn Time
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 累积回合时间
- en: The last thing I tracked during the experiments was how much time each of the
    LLMs used on their actions. As expected, the largest and more complex models used
    the most time.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验中，我追踪的最后一项数据是每个LLM在其操作上花费的时间。正如预期的那样，越大、越复杂的模型花费的时间越多。
- en: '![](../Images/c0ecf143559382f80f1e3446b9b56e4c.png)![](../Images/050feda024ff8bef9cfc13075d75518e.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c0ecf143559382f80f1e3446b9b56e4c.png)![](../Images/050feda024ff8bef9cfc13075d75518e.png)'
- en: Accumulated turn time by player / image by author
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 按玩家划分的累积回合时间 / 图片来源：作者
- en: What is clear is that Claude seems to be really taking it’s time. For experiment-1
    GPT-4 is coming out better than llama3.1 70b running on Groq but this is likely
    due to the fact that there were more issues with internal server response errors
    etc., in addition to errors in the returned answers, which lead to the turn time
    going up. For pure inference, when it provides the correct response, Groq was
    marginally faster than OpenAI.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，Claude似乎真的花了更多时间。对于实验-1，GPT-4的表现比在Groq上运行的Llama3.1 70b要好，但这很可能是因为在返回答案时出现了更多的内部服务器响应错误等问题，导致回合时间增加。就纯推理而言，当提供正确答案时，Groq的速度略快于OpenAI。
- en: Trending Towards Less Mistakes and More Robust Output
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 趋向减少错误和更加稳健的输出
- en: As we could see from the model improved model generations, the new LLMs are
    generating far less erroneous output than the older models. This is important
    as we continue to build data products with the models and integrate them into
    pipelines. There will likely still be the need for some post-prompt error handling
    but less than before.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们从改进后的模型生成中看到的那样，新的大语言模型（LLMs）生成的错误输出明显少于旧模型。这一点非常重要，因为我们在继续使用这些模型构建数据产品并将其集成到管道中时，可能仍然需要进行一些后处理错误处理，但比以前少了。
- en: Parsing Responses
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解析响应
- en: A key issue in interacting with the LLMs is to parse the output that they produce.
    OpenAI recently [revealed](https://openai.com/index/introducing-structured-outputs-in-the-api/)
    that GPT-4o, can *“now reliably adhere to developer-supplied JSON Schemas.”* So,
    this is of course amazing news, but many of the other models, such as llama 3.1
    70B still struggled to consistently return JSON output in the right format.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 与LLMs交互的一个关键问题是解析它们产生的输出。OpenAI最近[披露](https://openai.com/index/introducing-structured-outputs-in-the-api/)，GPT-4o*“现在可以可靠地遵守开发者提供的JSON模式。”*
    这当然是个好消息，但许多其他模型，比如llama 3.1 70B，仍然难以持续返回正确格式的JSON输出。
- en: 'The solution to the parsing problem ended up packing the output into special
    text strings such as `||| output 1 |||`, `+++ output 2+++` and then using regex
    to parse those output strings. I simply prompt the LLM to format the output using
    the special text strings, and also provide examples of correctly formatted output.
    I guess because of how the LLMs are inherently sequence based this type of formatting
    is easier out of the box than for example asking it to return a complex JSON object.
    For a concrete example, see below:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 解决解析问题的方法是将输出打包成特殊的文本字符串，例如`||| output 1 |||`，`+++ output 2 +++`，然后使用正则表达式解析这些输出字符串。我只需提示LLM使用特殊的文本字符串格式化输出，并提供正确格式化输出的示例。我猜测，由于LLM本质上是基于序列的，这种格式化比要求它返回复杂的JSON对象更容易实现。具体示例如下：
- en: '[PRE7]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Trying Out the Code and Running Your Own Experiments
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 尝试运行代码并进行自己的实验
- en: I developed the package for the risk_game engine in addition to the modules
    and Jupyter notebook inside a docker container, and everything is self contained.
    So for anyone interested in trying out the simulator and run your own experiments
    all the code is available and should be very easy to run from the Github repo.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我为risk_game引擎开发了这个包，以及容器内的模块和Jupyter笔记本，所有内容都是自包含的。因此，对于任何有兴趣尝试模拟器并运行自己实验的人，所有代码都可用，应该非常容易从GitHub仓库运行。
- en: '[](https://github.com/hcekne/risk-game.git?source=post_page-----43c868d83c3b--------------------------------)
    [## GitHub — hcekne/risk-game'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/hcekne/risk-game.git?source=post_page-----43c868d83c3b--------------------------------)
    [## GitHub — hcekne/risk-game'
- en: Contribute to hcekne/risk-game development by creating an account on GitHub.
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过在GitHub上创建一个帐户，参与hcekne/risk-game的开发。
- en: github.com](https://github.com/hcekne/risk-game.git?source=post_page-----43c868d83c3b--------------------------------)
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/hcekne/risk-game.git?source=post_page-----43c868d83c3b--------------------------------)'
- en: Clone the repo and follow the instructions in the `README.md` file. It should
    be pretty straightforward. The only thing you need to change to get everything
    running on your own machine is the `.env_example` file. You need to put in your
    own API keys for the relevant LLM providers and change the name of the file to
    `.env`.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 克隆仓库并按照`README.md`文件中的说明操作。这应该非常直接。你唯一需要更改的地方是`.env_example`文件。你需要为相关的LLM提供商输入你自己的API密钥，并将文件名更改为`.env`。
- en: Then run the `start_container.sh` script. This is just a bash script that initializes
    some environment variables and runs a docker compose .yml file. This file configures
    the appropriate settings for the docker container, and everything should start
    up by itself. (The reason we feed these environment variables into the docker
    container is because when doing in-container development you can run into an issue
    with file permissions on the files that are created in the container. This is
    fixed if we change the container user to your user, then the files created by
    the container will have the same owner as the user on the machine running the
    container.)
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 然后运行`start_container.sh`脚本。这只是一个bash脚本，用于初始化一些环境变量并运行一个docker compose .yml文件。该文件配置了docker容器的适当设置，所有内容应该会自动启动。（我们将这些环境变量传入docker容器的原因是，在容器内开发时，你可能会遇到关于容器内创建的文件的文件权限问题。如果我们将容器用户改为你的用户，那么容器创建的文件将与运行容器的机器上的用户拥有相同的所有权，从而解决此问题。）
- en: '*If you enjoyed reading this article and would like to access more content
    from me please feel free to connect with me on LinkedIn at* [*https://www.linkedin.com/in/hans-christian-ekne-1760a259/*](https://www.linkedin.com/in/hans-christian-ekne-1760a259/)
    *or visit my webpage at* [*https://www.ekneconsulting.com/*](https://www.ekneconsulting.com/)
    *to explore some of the services I offer. Don’t hesitate to reach out via email
    at hce@ekneconsulting.com*'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你喜欢阅读这篇文章，并希望访问更多我的内容，请随时通过 LinkedIn 与我联系，链接是* [*https://www.linkedin.com/in/hans-christian-ekne-1760a259/*](https://www.linkedin.com/in/hans-christian-ekne-1760a259/)
    *，或者访问我的网站* [*https://www.ekneconsulting.com/*](https://www.ekneconsulting.com/)
    *，了解我提供的一些服务。不要犹豫，通过电子邮件 hce@ekneconsulting.com 与我联系*'
