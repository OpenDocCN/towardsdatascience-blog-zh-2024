- en: How Did Open Food Facts Fix OCR-Extracted Ingredients Using Open-Source LLMs?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Open Food Facts如何利用开源LLM修复OCR提取的成分？
- en: 原文：[https://towardsdatascience.com/how-did-open-food-facts-use-open-source-llms-to-enhance-ingredients-extraction-d74dfe02e0e4?source=collection_archive---------4-----------------------#2024-10-06](https://towardsdatascience.com/how-did-open-food-facts-use-open-source-llms-to-enhance-ingredients-extraction-d74dfe02e0e4?source=collection_archive---------4-----------------------#2024-10-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-did-open-food-facts-use-open-source-llms-to-enhance-ingredients-extraction-d74dfe02e0e4?source=collection_archive---------4-----------------------#2024-10-06](https://towardsdatascience.com/how-did-open-food-facts-use-open-source-llms-to-enhance-ingredients-extraction-d74dfe02e0e4?source=collection_archive---------4-----------------------#2024-10-06)
- en: Delve into an end-to-end Machine Learning project to improve the quality of
    the Open Food Facts database
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入了解一个端到端的机器学习项目，旨在提升Open Food Facts数据库的质量。
- en: '[](https://medium.com/@jeremyarancio?source=post_page---byline--d74dfe02e0e4--------------------------------)[![Jeremy
    Arancio](../Images/37c4c41e71eb91cfffc7e4ff2bb4394a.png)](https://medium.com/@jeremyarancio?source=post_page---byline--d74dfe02e0e4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d74dfe02e0e4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d74dfe02e0e4--------------------------------)
    [Jeremy Arancio](https://medium.com/@jeremyarancio?source=post_page---byline--d74dfe02e0e4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jeremyarancio?source=post_page---byline--d74dfe02e0e4--------------------------------)[![Jeremy
    Arancio](../Images/37c4c41e71eb91cfffc7e4ff2bb4394a.png)](https://medium.com/@jeremyarancio?source=post_page---byline--d74dfe02e0e4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d74dfe02e0e4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d74dfe02e0e4--------------------------------)
    [Jeremy Arancio](https://medium.com/@jeremyarancio?source=post_page---byline--d74dfe02e0e4--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d74dfe02e0e4--------------------------------)
    ·13 min read·Oct 6, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d74dfe02e0e4--------------------------------)
    ·阅读时间：13分钟·2024年10月6日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/36288a16e720c9435e278bbc7b280c5a.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36288a16e720c9435e278bbc7b280c5a.png)'
- en: Image generated with Flux1
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Flux1生成的图像
- en: Open Food Facts’ purpose is to create the largest **open-source food database
    in the world**. To this day, it has collected over 3 millions products and their
    information thanks to its contributors.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Open Food Facts的目标是创建全球最大的**开源食品数据库**。至今，它已经收录了超过300万种产品及其相关信息，得益于社区贡献者的帮助。
- en: Nutritional value, eco-score, product origins,… Various data that define each
    product and give consumers and researchers insights about what they put in their
    plates.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 营养价值、生态评分、产品来源……各种定义每个产品的数据，为消费者和研究人员提供关于他们所放置在餐盘上的食品的深刻见解。
- en: This information is provided by the community of users and contributors, who
    actively add products data, take pictures, and fill any missing data into the
    database through the [mobile app](https://play.google.com/store/apps/details?id=org.openfoodfacts.scanner&hl=en_US&pli=1).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这些信息由用户和贡献者社区提供，他们通过[移动应用](https://play.google.com/store/apps/details?id=org.openfoodfacts.scanner&hl=en_US&pli=1)积极添加产品数据、拍照，并填写数据库中任何缺失的数据。
- en: Using the product picture, Open Food Facts extracts the ingredients list, typically
    located on the back of the packaging, through Optical Character Recognition (OCR).
    The product composition is then parsed and added to the database.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 使用产品图片，Open Food Facts通过光学字符识别（OCR）技术提取通常位于包装背面的成分列表。然后对产品成分进行解析并将其添加到数据库中。
- en: '![](../Images/7c922462d16ef15165355663e856bf04.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c922462d16ef15165355663e856bf04.png)'
- en: List of ingredients on the product packaging
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 产品包装上的成分列表
- en: However, it often appears that the text extraction doesn’t go well…
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，文本提取过程往往并不顺利……
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: These typos may seem minimal, but when the list is parsed to extract individual
    ingredients, **such errors create unrecognized ingredients,** which harm the quality
    of the database. Light reflections, folded packaging, low-quality pictures, and
    other factors all complicate the ingredient parsing process.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这些拼写错误看起来可能微不足道，但当成分列表被解析以提取单个成分时，**此类错误会导致无法识别的成分**，从而影响数据库的质量。光线反射、包装折叠、低质量图片等因素都使得成分解析过程变得更加复杂。
- en: '![](../Images/dd0d3821089e8b68007112d12a755161.png)![](../Images/6074f6e9c6d559cc94eab9cbd049ab7f.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd0d3821089e8b68007112d12a755161.png)![](../Images/6074f6e9c6d559cc94eab9cbd049ab7f.png)'
- en: Examples of packaging pictures where the OCR fails (from the Open Food Facts
    database)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: OCR失败的包装图片示例（来自Open Food Facts数据库）
- en: '**Open Food Facts has tried to solve this issue for years using Regular Expressions
    and existing solutions such as Elasticsearch’s corrector, without success. Until
    recently.**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**Open Food Facts多年来一直尝试使用正则表达式和现有解决方案（如Elasticsearch的校正器）来解决这个问题，但未成功。直到最近。**'
- en: Thanks to the latest advancements in artificial intelligence, we now have access
    to powerful **Large Language Models**, also called **LLMs**.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢人工智能的最新进展，我们现在可以使用强大的**大语言模型**（Large Language Models），也称为**LLMs**。
- en: By training our own model, we created the **Ingredients Spellcheck** and managed
    to not only outperform proprietary LLMs such as **GPT-4o** or **Claude 3.5 Sonnet**
    on this task, but also to reduce the number of unrecognized ingredients in the
    database by **11%**.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 通过训练我们自己的模型，我们创建了**成分拼写检查**，不仅在这个任务上超越了如**GPT-4o**或**Claude 3.5 Sonnet**等专有LLM，而且还将数据库中未识别的成分数量减少了**11%**。
- en: '**This article walks you through the different stages of the project and shows
    you how we managed to improve the quality of the database using Machine Learning.**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**本文将带您了解项目的不同阶段，并展示我们如何利用机器学习提高数据库质量。**'
- en: Enjoy the reading!
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 享受阅读！
- en: Define the problem
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义问题
- en: When a product is added by a contributor, its pictures go through a series of
    processes to extract all relevant information. One crucial step is the extraction
    of the **list of ingredients**.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个产品由贡献者添加时，它的图片会经过一系列过程以提取所有相关信息。其中一个关键步骤是提取**成分列表**。
- en: When a word is identified as an ingredient, it is cross-referenced with a **taxonomy**
    that contains a predefined list of recognized ingredients. If the word matches
    an entry in the taxonomy, it is tagged as an ingredient and added to the product’s
    information.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个词被识别为成分时，它会与包含预定义成分列表的**分类法**进行交叉引用。如果该词与分类法中的某个条目匹配，它就会被标记为成分并添加到产品信息中。
- en: This tagging process ensures that ingredients are standardized and easily searchable,
    providing accurate data for consumers and analysis tools.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这个标签化过程确保了成分的标准化，且易于搜索，为消费者和分析工具提供准确的数据。
- en: '**But if an ingredient is not recognized, the process fails.**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**但是，如果某个成分未被识别，过程就会失败。**'
- en: '![](../Images/f91a23509ebc0f515c471094f2bf15bb.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f91a23509ebc0f515c471094f2bf15bb.png)'
- en: The ingredient “Jambon do porc” (Pork ham) was not recognized by the parser
    (from the Product Edition page)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 成分“Jambon do porc”（猪肉火腿）未被解析器识别（来自产品编辑页面）
- en: 'For this reason, we introduced an additional layer to the process: the **Ingredients
    Spellcheck**, designed to correct ingredient lists before they are processed by
    the ingredient parser.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们在过程中过引入了一个额外的层次：**成分拼写检查**，旨在在成分解析器处理之前修正成分列表。
- en: A simpler approach would be the [Peter Norvig algorithm](https://norvig.com/spell-correct.html),
    which processes each word by applying a series of character deletions, additions,
    and replacements to identify potential corrections.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更简单的方法是[Peter Norvig算法](https://norvig.com/spell-correct.html)，它通过应用一系列字符删除、添加和替换操作来处理每个单词，从而识别潜在的拼写纠正。
- en: 'However, this method proved to be insufficient for our use case, for several
    reasons:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于几个原因，这种方法证明对我们的用例不够充分：
- en: '**Special Characters and Formatting**: Elements like commas, brackets, and
    percentage signs hold critical importance in ingredient lists, influencing product
    composition and allergen labeling *(e.g., “salt (1.2%)”).*'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特殊字符和格式**：诸如逗号、括号和百分号等元素在成分列表中具有重要作用，影响产品组成和过敏原标签*(例如，“盐（1.2%）”)。*'
- en: '**Multilingual Challenges**: the database contains products from all over the
    word with a wide variety of languages. This further complicates a basic character-based
    approach like Norvig’s, which is language-agnostic.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多语言挑战**：数据库包含来自世界各地的产品，语言种类繁多。这进一步使得像诺尔维格的字符基础方法那样的语言无关的方法变得复杂。'
- en: Instead, we turned to the latest advancements in Machine Learning, particularly
    **Large Language Models (LLMs)**, which excel in a wide variety of **Natural Language
    Processing (NLP)** tasks, including spelling correction.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们转向了机器学习的最新进展，特别是**大语言模型（LLMs）**，它们在包括拼写纠正在内的各种**自然语言处理（NLP）**任务中表现优异。
- en: This is the path we decided to take.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们决定采取的路径。
- en: Evaluate
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估
- en: You can’t improve what you don’t measure.
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你无法改善你无法衡量的东西。
- en: '**What is a good correction? And how to measure the performance of the corrector,
    LLM or non-LLM?**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**什么是好的更正？如何衡量更正者的性能，LLM 或非 LLM？**'
- en: Our first step is to understand and catalog the diversity of errors the Ingredient
    Parser encounters.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一步是理解并分类成分解析器遇到的各种错误。
- en: 'Additionally, it’s essential to assess whether an error should even be corrected
    in the first place. Sometimes, trying to correct mistakes could do more harm than
    good:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，评估错误是否应该被更正也是至关重要的。有时，试图更正错误可能会适得其反：
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For these reasons, we created the **Spellcheck Guidelines**, a set of rules
    that limits the corrections. *These guidelines will serve us in many ways throughout
    the project, from the dataset generation to the model evaluation.*
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些原因，我们创建了**拼写检查指南**，这是一套限制更正范围的规则。*这些指南将在项目的各个阶段为我们提供帮助，从数据集生成到模型评估。*
- en: The guidelines was notably used to create the [**Spellcheck Benchmark**](https://huggingface.co/datasets/openfoodfacts/spellcheck-benchmark),
    a curated dataset containing approximately 300 lists of ingredients manually corrected.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 该指南特别用于创建[**拼写检查基准**](https://huggingface.co/datasets/openfoodfacts/spellcheck-benchmark)，这是一个经过精心策划的数据集，包含大约300个手动更正的成分列表。
- en: This benchmark is the **cornerstone of the project**. It enables us to evaluate
    any solution, Machine Learning or simple heuristic, on our use case.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这个基准是**项目的基石**。它使我们能够在我们的用例中评估任何解决方案，无论是机器学习还是简单的启发式方法。
- en: It goes along the **Evaluation algorithm**, a custom solution we developed that
    transform a set of corrections into measurable metrics.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 它与**评估算法**一起使用，这是我们开发的自定义解决方案，能够将一组更正转化为可度量的指标。
- en: The Evaluation Algorithm
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估算法
- en: Most of the existing metrics and evaluation algorithms for text-relative tasks
    compute the similarity between a reference and a prediction, such as [BLEU](https://en.wikipedia.org/wiki/BLEU)
    or [ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)) scores for language translation
    or summarization.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现有的文本相关任务评估算法通过计算参考与预测之间的相似性来评估性能，例如用于语言翻译或总结的[BLEU](https://en.wikipedia.org/wiki/BLEU)或[ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric))分数。
- en: However, in our case, these metrics fail short.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们的案例中，这些指标并不完全适用。
- en: 'We want to evaluate how well the Spellcheck algorithm recognizes and fixes
    the right words in a list of ingredients. Therefore, we adapt the **Precision**
    and **Recall** metrics for our task:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望评估拼写检查算法如何识别和修正成分列表中的正确单词。因此，我们将**精确度**和**召回率**指标适配到我们的任务中：
- en: '**Precision** = Right corrections by the model / ​Total corrections made by
    the model'
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**精确度** = 模型正确更正的单词数 / 模型所做的总更正数'
- en: ''
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Recall** = Right corrections by the model / ​Total number of errors'
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**召回率** = 模型正确更正的单词数 / 错误总数'
- en: 'However, we don’t have the fine-grained view of which words were supposed to
    be corrected… We only have access to:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们没有细粒度的视角来查看哪些单词应该被更正……我们只能访问：
- en: '**The *original*: the list of ingredients as present in the database;**'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**原始数据：数据库中现有的成分列表；**'
- en: '**The *reference*: how we expect this list to be corrected;**'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参考数据：我们期望这个列表被更正的方式；**'
- en: '**The *prediction*: the correction from the model.**'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测：模型的更正。**'
- en: Is there any way to calculate the number of errors that were correctly corrected,
    the ones that were missed by the Spellcheck, and finally the errors that were
    wrongly corrected?
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 是否有方法计算被正确更正的错误数、拼写检查未能更正的错误，以及最终被错误更正的错误数？
- en: The answer is yes!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是肯定的！
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'With the example above, we can easily spot which words were supposed to be
    corrected: `The` , `is` and `fridge` ; and which words were wrongly corrected:
    `on` into `in`. Finally, we see that an additional word was added: `big` .'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述示例，我们可以轻松地找出应该被更正的单词：`The`、`is` 和 `fridge`；以及哪些单词被错误更正：`on` 被更正为 `in`。最后，我们看到添加了一个额外的单词：`big`。
- en: If we align these 3 sequences in pairs, `original-reference` and `original-prediction`
    , we can detect which words were supposed to be corrected, and those that weren’t.
    This alignment problem is typical in bio-informatic, called [Sequence Alignment](https://en.wikipedia.org/wiki/Sequence_alignment),
    whose purpose is to identify regions of similarity.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将这三组序列配对对齐：`original-reference` 和 `original-prediction`，我们可以检测出哪些单词应该被更正，哪些没有被更正。这种对齐问题在生物信息学中是典型的，称为[序列对齐](https://en.wikipedia.org/wiki/Sequence_alignment)，其目的是识别相似性区域。
- en: This is a perfect analogy for our spellcheck evaluation task.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们拼写检查评估任务的完美类比。
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: By labeling each pair with a `0` or `1` whether the word changed or not, we
    can calculate how often the model correctly fixes mistakes **(True Positives —
    TP)**, incorrectly changes correct words **(False Positives — FP)**, and misses
    errors that should have been corrected **(False Negatives — FN).**
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 通过为每一对标注`0`或`1`，表示单词是否发生了变化，我们可以计算模型正确修正错误的次数**（真正例 — TP）**、错误改变了正确单词的次数**（假正例
    — FP）**，以及漏掉应该修正的错误的次数**（假负例 — FN）**。
- en: In other words, we can calculate the **Precision** and **Recall** of the Spellcheck!
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们可以计算拼写检查的**精确度**和**召回率**！
- en: We now have a robust algorithm that is capable of evaluating any Spellcheck
    solution!
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在拥有一个强大的算法，能够评估任何拼写检查解决方案！
- en: You can find the algorithm in the [project repository](https://github.com/openfoodfacts/openfoodfacts-ai/blob/develop/spellcheck/src/spellcheck/evaluation/evaluator.py).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[项目仓库](https://github.com/openfoodfacts/openfoodfacts-ai/blob/develop/spellcheck/src/spellcheck/evaluation/evaluator.py)中找到该算法。
- en: Large Language Models
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型
- en: Large Language Models (LLMs) have proved being great help in tackling Natural
    Language task in various industries.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已被证明在各个行业中处理自然语言任务时提供了极大的帮助。
- en: They constitute a path we have to explore for our use case.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 它们构成了我们必须为使用案例探索的路径。
- en: Many LLM providers brag about the performance of their model on leaderboards,
    but how do they perform on correcting error in lists of ingredients? Thus, we
    evaluated them!
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 许多LLM供应商在排行榜上炫耀他们模型的表现，但它们在修正配料清单中的错误表现如何呢？因此，我们对它们进行了评估！
- en: We evaluated **GPT-3.5** and **GPT-4o** from **OpenAI**, **Claude-Sonnet-3.5**
    from **Anthropic**, and **Gemini-1.5-Flash** from **Google** using our custom
    benchmark and evaluation algorithm.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用我们的自定义基准测试和评估算法，评估了**OpenAI**的**GPT-3.5**和**GPT-4o**、**Anthropic**的**Claude-Sonnet-3.5**以及**Google**的**Gemini-1.5-Flash**。
- en: We prompted detailed instructions to orient the corrections towards our custom
    guidelines.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了详细的指令，以便将修正方向定向到我们的自定义指南。
- en: '![](../Images/83eadd9bceeee0e567b618410e3800c1.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/83eadd9bceeee0e567b618410e3800c1.png)'
- en: LLMs evaluation on our benchmark (image from author)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: LLM在我们的基准测试上的评估（图来自作者）
- en: '**GPT-3.5-Turbo** delivered the best performance compared to other models,
    both in terms of metrics and manual review. Special mention goes to **Claude-Sonnet-3.5**,
    which showed impressive error corrections (high Recall), but often provided additional
    irrelevant explanations, lowering its Precision.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPT-3.5-Turbo**在指标和人工评审方面都提供了最好的性能，优于其他模型。特别提到的是**Claude-Sonnet-3.5**，它在修正错误方面表现出色（高召回率），但常常提供额外的无关解释，降低了其精确度。'
- en: Great! We have an LLM that works! Time to create the feature in the app!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！我们有了一个有效的LLM！是时候在应用程序中创建这个功能了！
- en: '***Well,*** not so fast…'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '***好吧，*** 别急…'
- en: 'Using private LLMs reveals many challenges:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用私有LLM会带来许多挑战：
- en: '**Lack of Ownership**: We become dependent on the providers and their models.
    New model versions are released frequently, altering the model’s behavior. This
    instability, primarily because the model is designed for general purposes rather
    than our specific task, complicates long-term maintenance.'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**缺乏所有权**：我们变得依赖于供应商及其模型。新版本的模型频繁发布，改变了模型的行为。这种不稳定性，主要是因为该模型设计是为了通用目的，而非针对我们特定的任务，从而使长期维护变得复杂。'
- en: '**Model Deletion Risk**: We have no safeguards against providers removing older
    models. For instance, GPT-3.5 is slowly being replace by more performant models,
    despite being the best model for this task!'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型删除风险**：我们没有防范措施来应对供应商删除旧模型的情况。例如，尽管GPT-3.5是完成此任务的最佳模型，但它正逐渐被更高效的模型替代！'
- en: '**Performance Limitations**: The performance of a private LLM is constrained
    by its prompts. In other words, our only way of improving outputs is through better
    prompts since we cannot modify the core weights of the model by training it on
    our own data.'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**性能限制**：私有LLM的性能受限于其提示。换句话说，我们改进输出的唯一方式是通过更好的提示，因为我们无法通过在我们自己的数据上训练来修改模型的核心权重。'
- en: '***For these reasons, we chose to focus our efforts on open-source solutions
    that would provide us with complete control and outperform general LLMs.***'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '***基于这些原因，我们选择将精力集中在开源解决方案上，这将使我们能够完全控制并超越通用LLM。***'
- en: Train our own model
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练我们自己的模型
- en: '![](../Images/bb3dbfab4d007c69b8ac90063f64f12f.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bb3dbfab4d007c69b8ac90063f64f12f.png)'
- en: 'The model training workflow: from dataset extraction to model training (image
    from author)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练工作流程：从数据集提取到模型训练（图来自作者）
- en: Any machine learning solution starts with data. In our case, data is the corrected
    lists of ingredients.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 任何机器学习解决方案都始于数据。在我们的案例中，数据就是经过修正的配料列表。
- en: However, not all lists of ingredients are equal. Some are free of unrecognized
    ingredients, some are just so unreadable they would be no point correcting them.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，并非所有的配料列表都是一样的。有些列表没有未识别的配料，而有些则难以阅读，甚至没有必要纠正它们。
- en: Therefore, we find a perfect balance by choosing lists of ingredients having
    between **10 and 40 percent of unrecognized ingredients**. We also ensured there’s
    no duplicate within the dataset, but also with the benchmark to prevent any data
    leakage during the evaluation stage.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们通过选择包含**10%到40%未识别配料**的配料列表找到了一个完美的平衡。我们还确保数据集内没有重复项，并且与基准数据也没有重复，以防在评估阶段出现数据泄漏。
- en: We extracted 6000 uncorrected lists from the Open Food Facts database using
    [DuckDB](https://duckdb.org/), a fast in-process SQL tool capable of processing
    millions of rows under the second.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用[DuckDB](https://duckdb.org/)从Open Food Facts数据库中提取了6000个未修正的列表，DuckDB是一个快速的进程内SQL工具，能够在一秒钟内处理数百万行数据。
- en: However, those extracted lists are not corrected yet, and manually annotating
    them would take too much time and resources…
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些提取的列表还没有被纠正，手动标注它们将需要大量时间和资源……
- en: '**However, we have access to LLMs we already evaluated on the exact task. Therefore,
    we prompted GPT-3.5-Turbo, the best model on our benchmark, to correct every list
    in respect of our guidelines.**'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**然而，我们可以访问已经在相同任务上进行评估的LLMs。因此，我们提示GPT-3.5-Turbo，这是我们基准上的最佳模型，按照我们的指南修正每一个列表。**'
- en: The process took less than an hour and cost nearly **2$**.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程用了不到一个小时，费用几乎为**2$**。
- en: We then manually reviewed the dataset using [Argilla](https://argilla.io/),
    an open-source annotation tool specialized in Natural Language Processing tasks.
    This process ensures the dataset is of sufficient quality to train a reliable
    model.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用[Argilla](https://argilla.io/)手动审查数据集，Argilla是一个开源标注工具，专门用于自然语言处理任务。这个过程确保数据集质量足够高，以训练出可靠的模型。
- en: '**We now have at our disposal a** [**training dataset**](https://huggingface.co/datasets/openfoodfacts/spellcheck-dataset)
    **and an** [**evaluation benchmark**](https://huggingface.co/datasets/openfoodfacts/spellcheck-benchmark)
    **to train our own model on the Spellcheck task.**'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**现在我们手头有一个** [**训练数据集**](https://huggingface.co/datasets/openfoodfacts/spellcheck-dataset)
    **和一个** [**评估基准**](https://huggingface.co/datasets/openfoodfacts/spellcheck-benchmark)
    **，可以用来训练我们自己的拼写检查模型。**'
- en: Training
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: For this stage, we decided to go with **Sequence-to-Sequence Language Models**.
    In other words, these models take a text as input and returns a text as output,
    which suits the spellcheck process.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这一阶段，我们决定使用**序列到序列语言模型**。换句话说，这些模型以文本作为输入，并返回文本作为输出，适合拼写检查的过程。
- en: Several models fit this role, such as the **T5 family** developed by Google
    in 2020, or the current open-source LLMs such as **Llama** or **Mistral**, which
    are designed for text generation and following instructions.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种模型适合这个角色，例如**T5系列**，由谷歌于2020年开发，或当前的开源LLMs，如**Llama**和**Mistral**，它们专为文本生成和执行指令而设计。
- en: The model training consists in a succession of steps, each one requiring different
    resources allocations, such as cloud GPUs, data validation and logging. For this
    reason, we decided to orchestrate the training using [Metaflow](https://metaflow.org/),
    a pipeline orchestrator designed for Data science and Machine Learning projects.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练包括一系列步骤，每个步骤都需要不同的资源分配，例如云GPU、数据验证和日志记录。出于这个原因，我们决定使用[Metaflow](https://metaflow.org/)来协调训练，它是一个专为数据科学和机器学习项目设计的管道编排工具。
- en: 'The training pipeline is composed as follow:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 训练管道的组成如下：
- en: Configurations and hyperparameters are imported to the pipeline from config
    yaml files;
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置和超参数是从配置的 YAML 文件导入到管道中的；
- en: The training job is launched in the cloud using [AWS Sagemaker](https://aws.amazon.com/sagemaker/),
    along the set of model hyperparameters and the custom modules such as the evaluation
    algorithm. Once the job is done, the model artifact is stored in an AWS S3 bucket.
    All training details are tracked using [Comet ML](https://www.comet.com/site/);
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练任务通过[AWS Sagemaker](https://aws.amazon.com/sagemaker/)在云端启动，使用一组模型超参数和自定义模块（如评估算法）。一旦任务完成，模型工件将存储在AWS
    S3桶中。所有训练细节都通过[Comet ML](https://www.comet.com/site/)进行跟踪；
- en: The fine-tuned model is then evaluated on the **benchmark** using the evaluation
    algorithm. Depending on the model sizem this process can be extremely long. Therefore,
    we used [vLLM](https://github.com/vllm-project/vllm), a Python library designed
    to accelerates LLM inferences;
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后使用评估算法在**基准**上评估微调后的模型。根据模型的大小，这个过程可能非常漫长。因此，我们使用了[vLLM](https://github.com/vllm-project/vllm)，一个旨在加速LLM推理的Python库；
- en: The predictions against the benchmark, also stored in AWS S3, are sent to [Argilla](https://argilla.io/)
    for human-evaluation.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对基准的预测结果，存储在AWS S3中，发送到[Argilla](https://argilla.io/)进行人工评估。
- en: After iterating over and over between refining the data and the model training,
    we achieved performance **comparable to proprietary LLMs** on the Spellcheck task,
    scoring an F1-Score of **0.65**.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在多次迭代数据精炼和模型训练之后，我们在拼写检查任务上达到了**与专有LLM相媲美**的性能，F1分数为**0.65**。
- en: '![](../Images/7fdb68e96e8e460f5871149e4f6a61df.png)![](../Images/407f381cc9c76a371fa2ee8a89f453ca.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7fdb68e96e8e460f5871149e4f6a61df.png)![](../Images/407f381cc9c76a371fa2ee8a89f453ca.png)'
- en: LLMs evaluation on our benchmark (image from author)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs在我们基准上的评估（图像来源：作者）
- en: The model, a fine-tuned [Mistral-7B-Base-v0.3](https://huggingface.co/mistralai/Mistral-7B-v0.3),
    is available on the Hugging Face platform and is publicly available, along its
    [dataset](https://huggingface.co/datasets/openfoodfacts/spellcheck-dataset) and
    evaluation [benchmark](https://huggingface.co/datasets/openfoodfacts/spellcheck-benchmark).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型是微调后的[Mistral-7B-Base-v0.3](https://huggingface.co/mistralai/Mistral-7B-v0.3)，可在Hugging
    Face平台上获取，并且是公开可用的，连同其[数据集](https://huggingface.co/datasets/openfoodfacts/spellcheck-dataset)和评估[基准](https://huggingface.co/datasets/openfoodfacts/spellcheck-benchmark)。
- en: '[](https://huggingface.co/openfoodfacts/spellcheck-mistral-7b?source=post_page-----d74dfe02e0e4--------------------------------)
    [## openfoodfacts/spellcheck-mistral-7b · Hugging Face'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://huggingface.co/openfoodfacts/spellcheck-mistral-7b?source=post_page-----d74dfe02e0e4--------------------------------)
    [## openfoodfacts/spellcheck-mistral-7b · Hugging Face'
- en: We're on a journey to advance and democratize artificial intelligence through
    open source and open science.
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们正在努力通过开源和开放科学推动并普及人工智能。
- en: huggingface.co](https://huggingface.co/openfoodfacts/spellcheck-mistral-7b?source=post_page-----d74dfe02e0e4--------------------------------)
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: huggingface.co](https://huggingface.co/openfoodfacts/spellcheck-mistral-7b?source=post_page-----d74dfe02e0e4--------------------------------)
- en: Furthermore, **we estimated the Spellcheck reduced the number of unrecognized
    ingredients by 11%, which is promising!**
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，**我们估计拼写检查减少了11%的未识别成分，这很有前景！**
- en: 'Now comes the final phase of the project: integrating the model into Open Food
    Facts.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在进入项目的最后阶段：将模型集成到Open Food Facts中。
- en: Deployment & Integration
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署与集成
- en: '**Our model is big!**'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们的模型很大！**'
- en: 7 billions parameters, which means **14 GB** of memory required to run it in
    **float16**, without considering the **20% overhead factor.**
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 70亿个参数，这意味着在**float16**格式下运行时需要**14 GB**内存，还不包括**20%的开销因子**。
- en: Additionally, large models often mean **low throughput during inference**, which
    can make them inappropriate for real-time serving. We need GPUs with large memory
    to run this model in production, such as the [Nvidia L4](https://www.nvidia.com/en-us/data-center/l4/),
    which is equipped with 24GB of VRAM.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，大型模型通常意味着**推理期间吞吐量低**，这可能使其不适合实时服务。我们需要配备大内存的GPU来在生产环境中运行该模型，例如[Nvidia L4](https://www.nvidia.com/en-us/data-center/l4/)，其配备了24GB的显存。
- en: '**But the price of running these instances in the cloud is quite expensive…**'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**但在云端运行这些实例的成本相当昂贵……**'
- en: However, a possibility to provide a real-time experience for our users, without
    requiring GPU instances running 24/7, is **batch inference**.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，提供实时体验的可能性，而不需要24/7运行GPU实例，就是**批处理推理**。
- en: Lists of ingredients are processed in batches by the model on a regular basis,
    then stored in the database. **This way, we pay only for the resources used during
    the batch processing!**
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 成分列表定期通过模型进行批处理处理，然后存储在数据库中。**这样，我们只需为批处理过程中使用的资源付费！**
- en: Batch Job
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批处理作业
- en: We developed a batch processing system to handle large-scale text processing
    using LLMs efficiently with [Google Batch Job](https://cloud.google.com/batch/docs/get-started).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开发了一个批处理系统，通过[Google Batch Job](https://cloud.google.com/batch/docs/get-started)高效地处理大规模文本处理任务。
- en: '![](../Images/e0c7d92fc5854b223efc5af25ed260ec.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e0c7d92fc5854b223efc5af25ed260ec.png)'
- en: Batch processing system (image from author)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理系统（图像来源：作者）
- en: The process begins by extracting data from the Open Food Facts database using
    DuckDB, which processes **43 GB of data in under 2 minutes!**
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程首先通过使用DuckDB从Open Food Facts数据库提取数据，**在不到2分钟的时间内处理43GB的数据！**
- en: The extracted data is then sent to a Google Bucket, triggering a Google Batch
    Job.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 提取的数据随后发送到Google Bucket，触发Google批处理作业。
- en: This job uses a pre-prepared Docker image containing all necessary dependencies
    and algorithms. To optimize the resource-intensive LLM processing, we reuse vLLM
    achieving impressive performances, **correcting 10,000 lists of ingredients in
    20 minutes only with a GPU L4**!
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 该作业使用一个预先准备的Docker镜像，包含所有必要的依赖项和算法。为了优化资源密集型的LLM处理，我们重用了vLLM，取得了令人印象深刻的表现，**仅用一块GPU
    L4就能在20分钟内修正10,000个食材列表！**
- en: After successful processing, the corrected data is saved in a intermediate database
    containing the predictions of all models in Open Food Facts, served by [Robotoff](https://github.com/openfoodfacts/robotoff).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 经过成功处理后，修正后的数据保存在一个中间数据库中，包含Open Food Facts中所有模型的预测，由[Robotoff](https://github.com/openfoodfacts/robotoff)提供服务。
- en: When a contributor modifies a product details, they’re presented with the spellcheck
    corrections, **ensuring users remain the key decision-makers in Open Food Facts’
    data quality process.**
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 当贡献者修改产品详情时，他们将看到拼写检查的修正建议，**确保用户仍然是Open Food Facts数据质量过程中的关键决策者。**
- en: This system allows Open Food Facts to leverage advanced AI capabilities for
    improving data quality while preserving its community-driven approach.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 该系统使Open Food Facts能够利用先进的AI能力来提高数据质量，同时保持其社区驱动的方法。
- en: '![](../Images/d73441d66e04c9ab650600565b6475d7.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d73441d66e04c9ab650600565b6475d7.png)'
- en: Batch jobs in GCP (image from author)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: GCP中的批处理作业（图片来自作者）
- en: Conlusion
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we walked you through the development and the integration of
    the **Ingredients Spellcheck**, an LLM-powered up feature to correct OCR-extracted
    lists of ingredients.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们带您了解了**食材拼写检查**的开发和集成，这是一个由LLM驱动的功能，用于修正OCR提取的食材列表。
- en: We first developed a set of rules, the **Spellcheck Guidelines**, to restrict
    the corrections . We created a benchmark of corrected lists of ingredients that,
    along a custom evaluation algorithm, to evaluate any solution to the problem.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先制定了一套规则，**拼写检查指南**，来限制修正范围。我们创建了一个经过修正的食材列表基准，并通过自定义评估算法来评估任何解决方案。
- en: With this setup, we evaluated various private LLMs and determined that **GPT-3.5-Turbo**
    was the most suitable model for our specific use case. However, we also demonstrated
    that relying on a private LLM imposes significant limitations, including lack
    of ownership and restricted opportunities to improve or fine-tune such a large
    model (175 billion parameters).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种设置下，我们评估了各种私有LLM，并确定**GPT-3.5-Turbo**是最适合我们特定用例的模型。然而，我们也展示了依赖私有LLM带来的显著限制，包括缺乏所有权和对如此大型模型（1750亿个参数）进行改进或微调的机会受限。
- en: To address these challenges, we decided to **develop our own model**, fine-tuning
    it on synthetically corrected texts extracted from the database. After several
    iterations and experiments, we successfully achieved good performances with an
    **open-source model**. Not only did we match private LLMs performance, it also
    solved the ownership problem we were facing, giving us full control over our model.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些挑战，我们决定**开发我们自己的模型**，并在从数据库提取的合成修正文本上进行微调。经过几轮迭代和实验，我们成功地通过一个**开源模型**达到了良好的表现。我们不仅匹配了私有LLM的性能，还解决了我们面临的所有权问题，使我们对模型拥有完全控制权。
- en: We then **integrated this model into the Open Food Facts** using **batch inference
    deployment,** enabling us to process thousands of lists on a regular basis. The
    predictions are stored in Robotoff database as **Insights** before being validated
    by contributors, **leaving OFF data quality ownership to contributors**.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后**将该模型集成到Open Food Facts**中，使用**批处理推理部署**，使我们能够定期处理成千上万的食材列表。预测结果存储在Robotoff数据库中作为**洞察**，然后由贡献者验证，**将OFF的数据质量所有权留给贡献者**。
- en: Next step
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下一步
- en: '**The Spellcheck integration is still a work in progress**. We are working
    on designing the user interface to propose ML generated corrections and let contributors
    accept, deny, or modify corrections. **We expect fully integrating the feature
    by the end of the year.**'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**拼写检查集成仍在进行中**。我们正在设计用户界面，以提供机器学习生成的修正建议，并让贡献者接受、拒绝或修改这些修正。**我们预计将在年底前完成该功能的完全集成。**'
- en: '![](../Images/4873fa37ebe6d27a18074cf083f3e87d.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4873fa37ebe6d27a18074cf083f3e87d.png)'
- en: Spellcheck corrections validated by users built in [Hugging Face Space](https://huggingface.co/spaces/openfoodfacts/ingredients-spellcheck-annotate)
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 用户验证的拼写检查已内置于[Hugging Face Space](https://huggingface.co/spaces/openfoodfacts/ingredients-spellcheck-annotate)
- en: Additionally, we plan to continue refining the model through iterative improvements.
    Its performance can be significantly enhanced by improving the quality of the
    training data and incorporating user feedback. This approach will allow us to
    fine-tune the model continuously, ensuring it remains highly effective and aligned
    with real-world use cases.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们计划通过持续的迭代改进来不断完善模型。通过提高训练数据的质量并结合用户反馈，可以显著提升其性能。这种方法将使我们能够持续微调模型，确保其始终保持高效，并与实际应用场景高度契合。
- en: The model, along its datasets, can be find in the official [Hugging Face repository](https://huggingface.co/openfoodfacts).
    The code used to developped this model is available in the [OpenFoodFacts-ai/spellcheck](https://github.com/openfoodfacts/openfoodfacts-ai/tree/develop/spellcheck)
    Github repository.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型及其数据集可以在官方[Hugging Face仓库](https://huggingface.co/openfoodfacts)中找到。用于开发该模型的代码可以在[OpenFoodFacts-ai/spellcheck](https://github.com/openfoodfacts/openfoodfacts-ai/tree/develop/spellcheck)的Github仓库中找到。
- en: Thank you for reading that far! We hope you enjoyed the reading.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你阅读到这里！我们希望你喜欢这篇文章。
- en: 'If you too, you want to contribute to Open Food Facts, you can:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你也想为Open Food Facts做出贡献，你可以：
- en: 'Contribute to the Open Food Facts [GitHub](https://github.com/openfoodfacts):
    explore open issues that align with your skills,'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为Open Food Facts的[GitHub](https://github.com/openfoodfacts)做贡献：探索与您技能相匹配的开放问题，
- en: 'Download the Open Food Facts [mobile app](https://world.openfoodfacts.org/open-food-facts-mobile-app?utm_source=off&utf_medium=web&utm_campaign=search_and_links_promo_en):
    add new products to the database or improve existing ones by simply scanning their
    barcodes,'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载Open Food Facts的[移动应用](https://world.openfoodfacts.org/open-food-facts-mobile-app?utm_source=off&utf_medium=web&utm_campaign=search_and_links_promo_en)：通过扫描条形码，轻松添加新产品或改进现有产品。
- en: Join the Open Food Facts [Slack](https://slack.openfoodfacts.org/) and start
    discussing with other contributors in the OFF community.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加入Open Food Facts的[Slack](https://slack.openfoodfacts.org/)，并与其他贡献者在OFF社区中开始讨论。
- en: We can’t wait to see you join the community!
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迫不及待地想看到你加入社区！
- en: 'Don’t hesitate to check our other articles:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 不要犹豫，查看我们的其他文章：
- en: '[](https://medium.com/@jeremyarancio/duckdb-open-food-facts-the-largest-open-food-database-in-the-palm-of-your-hand-0d4ab30d0701?source=post_page-----d74dfe02e0e4--------------------------------)
    [## DuckDB & Open Food Facts: the largest open food database in the palm of your
    hand 🦆🍊'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jeremyarancio/duckdb-open-food-facts-the-largest-open-food-database-in-the-palm-of-your-hand-0d4ab30d0701?source=post_page-----d74dfe02e0e4--------------------------------)
    [## DuckDB与Open Food Facts：掌中最大的开放食品数据库 🦆🍊'
- en: Exploit the power of DuckDB to explore the largest open database in the food
    market.
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 利用DuckDB的强大功能，探索食品市场中最大的开放数据库。
- en: medium.com](https://medium.com/@jeremyarancio/duckdb-open-food-facts-the-largest-open-food-database-in-the-palm-of-your-hand-0d4ab30d0701?source=post_page-----d74dfe02e0e4--------------------------------)
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@jeremyarancio/duckdb-open-food-facts-the-largest-open-food-database-in-the-palm-of-your-hand-0d4ab30d0701?source=post_page-----d74dfe02e0e4--------------------------------)
