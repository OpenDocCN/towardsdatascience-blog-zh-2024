- en: Diving into Word Embeddings with EDA
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用EDA深入探索词向量
- en: 原文：[https://towardsdatascience.com/eda-for-word-embeddings-224c524b5769?source=collection_archive---------1-----------------------#2024-07-12](https://towardsdatascience.com/eda-for-word-embeddings-224c524b5769?source=collection_archive---------1-----------------------#2024-07-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/eda-for-word-embeddings-224c524b5769?source=collection_archive---------1-----------------------#2024-07-12](https://towardsdatascience.com/eda-for-word-embeddings-224c524b5769?source=collection_archive---------1-----------------------#2024-07-12)
- en: Visualizing unexpected insights in text data
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化文本数据中的意外见解
- en: '[](https://medium.com/@crackalamoo?source=post_page---byline--224c524b5769--------------------------------)[![Harys
    Dalvi](../Images/cf7fa3865063408efd1fd4c0b4b603db.png)](https://medium.com/@crackalamoo?source=post_page---byline--224c524b5769--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--224c524b5769--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--224c524b5769--------------------------------)
    [Harys Dalvi](https://medium.com/@crackalamoo?source=post_page---byline--224c524b5769--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@crackalamoo?source=post_page---byline--224c524b5769--------------------------------)[![Harys
    Dalvi](../Images/cf7fa3865063408efd1fd4c0b4b603db.png)](https://medium.com/@crackalamoo?source=post_page---byline--224c524b5769--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--224c524b5769--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--224c524b5769--------------------------------)
    [Harys Dalvi](https://medium.com/@crackalamoo?source=post_page---byline--224c524b5769--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--224c524b5769--------------------------------)
    ·14 min read·Jul 12, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--224c524b5769--------------------------------)
    ·阅读时间：14分钟·2024年7月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: When starting work with a new dataset, it’s always a good idea to start with
    some **exploratory data analysis** (EDA). Taking the time to understand your data
    before training any fancy models can help you understand the structure of the
    dataset, identify any obvious issues, and apply domain-specific knowledge.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始使用新数据集时，通常从一些**探索性数据分析**（EDA）入手是个不错的选择。在训练任何复杂模型之前，花时间了解你的数据有助于你理解数据集的结构，识别任何明显的问题，并运用领域特定的知识。
- en: 'You see EDA in various forms with everything from [house prices](https://www.kaggle.com/code/spscientist/a-simple-tutorial-on-exploratory-data-analysis)
    to advanced applications in the data science industry. But I still haven’t seen
    it for the hottest new dataset: **word embeddings**, the basis of our best large
    language models. So why not try it?'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在各种形式中看到EDA，从[房价](https://www.kaggle.com/code/spscientist/a-simple-tutorial-on-exploratory-data-analysis)到数据科学行业中的高级应用。但我仍然没有看到它应用于当前最火的新数据集：**词向量**，这也是我们最优秀的大型语言模型的基础。那么，为什么不尝试一下呢？
- en: In this article, we’ll **apply EDA to GloVe word embeddings**, using techniques
    like **covariance matrices, clustering, PCA, and vector math**. This will help
    us understand the structure of word embeddings, giving us a useful starting point
    for building more powerful models with this data. As we discover this structure,
    we’ll find that it’s not always what it seems, and some surprising biases are
    hidden in the corpus.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将**将EDA应用于GloVe词向量**，使用**协方差矩阵、聚类、PCA和向量数学**等技术。这将帮助我们理解词向量的结构，为我们在此数据基础上构建更强大的模型提供一个有用的起点。当我们探索这个结构时，我们会发现它并不总是表面看起来的那样，某些意想不到的偏差隐藏在语料库中。
- en: 'You will need:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你将需要：
- en: Basic understanding of linear algebra, statistics, and vector mathematics
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本的线性代数、统计学和向量数学知识
- en: 'Python packages: `numpy`, `sklearn`, and `matplotlib`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 包：`numpy`、`sklearn` 和 `matplotlib`
- en: About 3 GB of spare disk space
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 约3 GB的空闲磁盘空间
- en: The Dataset
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集
- en: To get started, download the dataset at [huggingface.co/stanfordnlp/glove/resolve/main/glove.6B.zip](https://huggingface.co/stanfordnlp/glove/resolve/main/glove.6B.zip)[1].
    This contains three text files, each containing a list of words along with their
    vector representations. We will use the **300-dimensional representations** (glove.6B.300d.txt).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，请下载数据集，链接为：[huggingface.co/stanfordnlp/glove/resolve/main/glove.6B.zip](https://huggingface.co/stanfordnlp/glove/resolve/main/glove.6B.zip)[1]。此文件包含三个文本文件，每个文件包含一组单词及其向量表示。我们将使用**300维的表示**（glove.6B.300d.txt）。
- en: 'A quick note on where this dataset comes from: essentially, this is a list
    of word embeddings derived from 6 billion tokens’ worth of **co-occurrence data**
    from Wikipedia and various news sources. A useful side effect of using co-occurrence
    is that **words that mean similar things tend to be close together**. For example,
    since “the *red* bird” and “the *blue* bird” are both valid sentences, we might
    expect the vectors for “red” and “blue” to be close to each other. For more technical
    information, you can check [the original GloVe paper](https://nlp.stanford.edu/pubs/glove.pdf)[1].'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 简单说明一下这个数据集的来源：基本上，这是从维基百科和各种新闻来源中获取的60亿个标记的**共现数据**生成的词嵌入列表。使用共现的一个有用副作用是，**意思相近的词往往会聚集在一起**。例如，由于“the
    *red* bird”和“the *blue* bird”都是有效句子，我们可能会预期“red”和“blue”的向量会彼此接近。更多技术信息，可以参考[原始的GloVe论文](https://nlp.stanford.edu/pubs/glove.pdf)[1]。
- en: To be clear, these are **not** word embeddings trained for the purpose of large
    language models. They are a fully unsupervised technique based on a large corpus.
    But they display a lot of similar properties to language model embeddings, and
    are interesting in their own right.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 需要明确的是，这些**不是**为大型语言模型训练的词嵌入。它们是一种完全无监督的技术，基于大量语料库。然而，它们展示了许多与语言模型嵌入相似的特性，且本身也很有趣。
- en: Each line of this text file consists of a word, followed by all 300 vector components
    of the associated embedding separated by spaces. We can load that in with Python.
    (To reduce noise and speed things up, I’m using the top 10% of the full dataset
    here with the `//10`, but you can change that if you’d like.)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这个文本文件的每一行由一个单词和该单词对应的300个向量分量组成，分量之间以空格分开。我们可以用Python将其加载进来。（为了减少噪音并加速处理，我这里使用了完整数据集的前10%
    `//10`，但如果你愿意的话，可以调整。）
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: That leaves us with 40,000 embeddings loaded in.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们现在加载了40,000个嵌入向量。
- en: Similarity Metrics
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相似度度量
- en: 'One natural question we might ask is: **are vectors generally close to other
    vectors with similar meaning?** And as a follow-up question, how do we quantify
    this?'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会问的一个自然问题是：**向量通常是否与意义相似的其他向量接近？** 作为后续问题，我们如何量化这个？
- en: 'There are two main ways we will quantify similarity between vectors: one is
    **Euclidean distance**, which is simply the natural Pythagorean theorem distance
    we are familiar with. The other is **cosine similarity**, which measures the cosine
    of the *angle* between two vectors. A vector has a cosine similarity of 1 with
    itself, -1 with an opposite vector, and 0 with an orthogonal vector.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将量化向量之间相似度的主要方式有两种：一种是**欧几里得距离**，这就是我们熟悉的自然的毕达哥拉斯定理距离。另一种是**余弦相似度**，它衡量两个向量之间的*角度*的余弦值。一个向量与自身的余弦相似度为1，与相反的向量为-1，与正交向量为0。
- en: 'Let’s implement these in NumPy:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在NumPy中实现这些：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now we can find all the closest vectors to a given word or embedding vector!
    We’ll do this in increasing order.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以找到与给定单词或嵌入向量最接近的所有向量！我们将按升序进行操作。
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now we can write a function to display the 10 most similar words. It will be
    useful to include a reverse option as well, so we can display the *least* similar
    words.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以写一个函数来显示最相似的10个词。最好能加上一个反向选项，这样我们就能显示*最不*相似的词。
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Finally, we can test it!
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以进行测试了！
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Looks like the Boston Red Sox made an unexpected appearance here. But other
    than that, this is about what we would expect.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来波士顿红袜队在这里意外亮相了。不过除此之外，这大致符合我们的预期。
- en: Maybe we can try some verbs, and not just nouns and adjectives? How about a
    nice and kind verb like “share”?
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 也许我们可以尝试一些动词，而不仅仅是名词和形容词？怎么样，试试像“share”这样一个友好且温暖的动词？
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: I guess “share” isn’t often used as a verb in this dataset. Oh well.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我猜“share”在这个数据集中作为动词使用的情况不多。唉，没关系。
- en: 'We can try some more conventional examples as well:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以尝试一些更传统的例子：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Reasoning by Analogy
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 类比推理
- en: One of the fascinating properties about word embeddings is that **analogy is
    built in** using vector math. The example from the GloVe paper is *king - queen
    = man - woman.* In other words, rearranging the equation, we expect *king = man
    - woman + queen*. Is this true?
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入的一个迷人特性是，**类比是通过向量数学内建的**。GloVe论文中的例子是*king - queen = man - woman*。换句话说，重新排列这个方程，我们预期会得到*king
    = man - woman + queen*。这是真的吗？
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Not quite: the closest vector to *man - woman + queen* turns out to be *queen*
    (cosine similarity 0.78), followed somewhat distantly by *king* (cosine similarity
    0.66). Inspired by this excellent [3Blue1Brown video](https://www.youtube.com/watch?v=wjZofJX0v4M),
    we might try *aunt* and *uncle* instead:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 并不完全正确：与*man - woman + queen*最接近的向量实际上是*queen*（余弦相似度 0.78），接下来是*king*（余弦相似度
    0.66），不过差距有点远。受到这段精彩的[3Blue1Brown 视频](https://www.youtube.com/watch?v=wjZofJX0v4M)的启发，我们可以试试用*aunt*和*uncle*代替：
- en: '[PRE8]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This is better (cosine similarity 0.7348 vs 0.7344), but still doesn’t work
    perfectly. But we can try switching to Euclidean distance. Now we need to set
    `reverse=True`, because a *higher* Euclidean distance is actually a *lower* similarity.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果更好（余弦相似度 0.7348 对比 0.7344），但仍然不完美。不过我们可以尝试改用欧几里得距离。现在我们需要设置`reverse=True`，因为*较高*的欧几里得距离实际上代表了*较低*的相似度。
- en: '[PRE9]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now we got it. But it seems like the analogy math might not be as perfect as
    we hoped, at least in the naïve way that we are doing it here.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们明白了。但似乎类比数学可能没有我们希望的那样完美，至少在我们目前采用的这种简单方法中。
- en: Magnitude
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 幅度
- en: Cosine similarity is all about the *angles* between vectors. But is the **magnitude**
    of a vector also important?
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度完全与向量之间的*角度*有关。那么，向量的**幅度**是否也很重要呢？
- en: 'We can reuse our existing code by expressing magnitude as the Euclidean distance
    from the zero vector. Let’s see which words have the largest and smallest magnitudes:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将幅度表示为与零向量的欧几里得距离来重用现有的代码。让我们看看哪些单词具有最大和最小的幅度：
- en: '[PRE10]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: It doesn’t look like there’s much of a pattern to the meaning of the large magnitude
    vectors, but they all seem to have very specific (and sometimes confusing) meanings.
    On the other hand, the smallest magnitude vectors tend to be very common words
    that can be found in a variety of contexts.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来大幅度向量的意义似乎没有什么规律，但它们似乎都有非常具体（有时甚至令人困惑）的含义。另一方面，最小幅度的向量往往是一些非常常见的单词，可以在各种语境中找到。
- en: 'There’s a **huge range between magnitudes**: from about 2.6 for the smallest
    vector all the way to about 17 for the largest. What does this distribution look
    like? We can plot a **histogram** to get a better picture of this.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**幅度之间有巨大的差异**：从最小的向量大约 2.6 到最大的向量大约 17。这个分布看起来是怎样的呢？我们可以绘制一个**直方图**来更好地展示这一点。'
- en: '[PRE11]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/1cc1402864a59a660414c0cefbb9d77f.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1cc1402864a59a660414c0cefbb9d77f.png)'
- en: A histogram of magnitudes of our word embeddings
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的词嵌入的幅度直方图
- en: This distribution looks approximately normal. If we wanted to test this further,
    we could use a [Q-Q plot](https://medium.com/analytics-vidhya/what-are-qq-plots-4beb00670d81).
    But for our purposes right now, this is fine.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分布看起来大致呈正态分布。如果我们想进一步测试这一点，可以使用[Q-Q 图](https://medium.com/analytics-vidhya/what-are-qq-plots-4beb00670d81)。但就目前而言，这样已经足够了。
- en: Dataset Bias
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集偏差
- en: It turns out that directions and subspaces in vector embeddings can encode various
    kinds of concepts, often in biased ways. [This paper](https://arxiv.org/pdf/1607.06520)[2]
    studied how this works for **gender bias**.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，向量嵌入中的方向和子空间可以编码多种不同的概念，通常是有偏的。[这篇论文](https://arxiv.org/pdf/1607.06520)[2]研究了这种情况如何与**性别偏见**相关。
- en: 'We can replicate this concept in our GloVe embeddings, too. First, let’s find
    the direction of the concept of “masculinity”. We can accomplish this by taking
    the average of **differences between vectors** like *he* and *she*, *man* and
    *woman*, and so on:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以在 GloVe 嵌入中复制这个概念。首先，让我们找到“男性气质”概念的方向。我们可以通过计算**向量之间的差异**来完成这个任务，比如*he*
    和 *she*、*man* 和 *woman*，等等：
- en: '[PRE12]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now we can find the “most masculine” and “most feminine” vectors, as judged
    by the embedding space.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以找到“最具男性气质”和“最具女性气质”的向量，这些都是根据嵌入空间的判断来确定的。
- en: '[PRE13]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, we can run an easy test to detect bias in the dataset: compute the similarity
    between *nurse* and each of *man* and *woman*. Theoretically, these should be
    about equal: nurse is not a gendered word. Is this true?'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以进行一个简单的测试，检测数据集中的偏见：计算*nurse*与*man*和*woman*之间的相似度。从理论上讲，这两个值应该大致相等：nurse
    不是一个性别化的词汇。这是真的吗？
- en: '[PRE14]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: That’s a pretty big difference! (Remember cosine similarity runs from -1 to
    1, with positive associations in the range 0 to 1.) For reference, 0.45 is also
    close to the cosine similarity between *cat* and *leopard*.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当大的差异！(请记住，余弦相似度的范围是 -1 到 1，其中正相关值在 0 到 1 之间。)作为参考，0.45 也接近于*cat* 和 *leopard*
    之间的余弦相似度。
- en: Clustering
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类
- en: 'Let’s see if we can **cluster words with similar meaning** using ***k*-means
    clustering**. This is easy to do with the package `scikit-learn`. We are going
    to use 300 clusters, which sounds like a lot, but trust me: almost all of the
    clusters are so interesting, you could write an entire article just interpreting
    them!'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看能否使用***k*-均值聚类**来**聚类具有相似含义的词语**。使用`scikit-learn`包可以轻松实现这一点。我们将使用300个聚类，虽然听起来很多，但相信我：几乎所有聚类都非常有趣，你可以仅通过解释它们写一篇完整的文章！
- en: '[PRE15]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: There’s a lot to look at here. We have clusters for things as diverse as New
    York City (`manhattan, n.y., brooklyn, hudson, borough`), molecular biology (`protein,
    proteins, enzyme, beta, molecules`), and Indian names (`singh, ram, gandhi, kumar,
    rao`).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有很多内容可以查看。我们有涉及不同主题的聚类，如纽约市（`manhattan, n.y., brooklyn, hudson, borough`）、分子生物学（`protein,
    proteins, enzyme, beta, molecules`）和印度名字（`singh, ram, gandhi, kumar, rao`）。
- en: But **sometimes these clusters are not what they seem**. Let’s write code to
    display all words of a cluster containing a given word, along with the nearest
    and farthest cluster.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 但是**有时这些聚类并不像看起来那样简单**。让我们编写代码来显示包含给定词语的聚类中的所有词汇，以及最近和最远的聚类。
- en: '[PRE16]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now let’s try out this code.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试这段代码。
- en: '[PRE17]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You might not get exactly this result every time: the clustering algorithm
    is non-deterministic. But much of the time, “birds” is associated with disease
    words rather than animal words. It seems the original dataset tends to use the
    word “bird” in the context of disease vectors.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能不会每次都得到完全相同的结果：聚类算法是非确定性的。但大多数情况下，“鸟类”会与疾病相关词汇而非动物词汇关联。似乎原始数据集倾向于在疾病传播媒介的语境中使用“鸟”这个词。
- en: There are literally hundreds more clusters for you to explore the contents of.
    Some other clusters I found interesting are “Illinois” and “Genghis”.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有成百上千个聚类等待你探索它们的内容。我发现一些有趣的聚类包括“伊利诺伊州”和“成吉思汗”。
- en: Principal Component Analysis
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主成分分析
- en: '**Principal Component Analysis (PCA)** is a tool we can use to find the directions
    in vector space associated with the most variance in our dataset. Let’s try it.
    Like clustering, `sklearn` makes this easy.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**主成分分析（PCA）**是我们用来找出数据集在向量空间中方差最大的方向的工具。让我们试试它。和聚类一样，`sklearn`使得这一过程非常简单。'
- en: '[PRE18]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Like our *k*-means experiment, a lot of these PCA vectors are really interesting.
    For example, let’s take a look at principal component 9:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们的*k*-均值实验一样，这些PCA向量中有很多非常有趣的内容。例如，让我们来看看主成分9：
- en: '[PRE19]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: It looks like positive values for component 9 are associated with Middle Eastern,
    South Asian and Southeast Asian terms, while negative values are associated with
    North American and British terms.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来，组件9的正值与中东、南亚和东南亚的术语相关，而负值则与北美和英国的术语相关。
- en: Another interesting one is component 3\. All the positive terms are decimal
    numbers, apparently quite a salient feature for this model. Component 8 also shows
    a similar pattern.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的成分是成分3。所有的正值都是十进制数，显然这是这个模型中的一个显著特征。成分8也展示了类似的模式。
- en: '[PRE20]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Dimensionality Reduction
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 降维
- en: One of the main benefits of PCA is that it allows us to **take a very high-dimensional
    dataset** (300-dimensional in this case) and **plot it in just two or three dimensions**
    by projecting onto the first components. Let’s try a two-dimensional plot and
    see if there is any information we can gather from it. We’ll also include color-coding
    by cluster using *k*-means.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: PCA的一个主要优点是，它允许我们**将一个高维数据集**（在此案例中为300维）**通过投影到前几个组件，仅用两维或三维展示**。让我们尝试做一个二维图，看是否能从中提取出任何信息。我们还会使用*k*-均值进行按聚类的颜色编码。
- en: '[PRE21]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](../Images/8754000d85b058dbbdcf1ea2133c5029.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8754000d85b058dbbdcf1ea2133c5029.png)'
- en: A plot of the first (X) and second (Y) principal components for our embeddings
    dataset
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的嵌入数据集的第一（X轴）和第二（Y轴）主成分的图表
- en: Unfortunately, this plot is a total mess! It’s difficult to learn much from
    it. It looks like just two dimensions in isolation are not very easy to interpret
    among 300 total dimensions, at least in the case of this dataset.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这个图表完全乱了！从中学到的信息非常有限。看起来仅凭两个维度来解释300个维度的数据集，至少在这个数据集的情况下，并不是很容易。
- en: There are two exceptions. First, we see that names tend to cluster near the
    top of this graph. Second, there is a little section that sticks out like a sore
    thumb at the bottom left. This area appears to be associated with numbers, particularly
    decimal numbers.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个例外。首先，我们看到名字通常会聚集在图表的顶部。其次，在左下角有一个突出的小部分，像个伤疤一样。这一区域似乎与数字相关，尤其是十进制数字。
- en: Covariance
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 协方差
- en: It is often helpful to get an idea of the **covariance between input features**.
    In this case, our input features are just abstract vector directions that are
    difficult to interpret. Still, a covariance matrix can tell us how much of this
    information is actually being used. If we see high covariance, it means some dimensions
    are strongly correlated, and maybe we could get away with reducing the dimensionality
    a little bit.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 了解**输入特征之间的协方差**往往很有帮助。在这种情况下，我们的输入特征只是难以解释的抽象向量方向。然而，协方差矩阵可以告诉我们这些信息实际上被利用了多少。如果我们看到较高的协方差，意味着某些维度之间高度相关，或许我们可以稍微减少维度。
- en: '[PRE22]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](../Images/bdf321419db9850f8aaabea3567d3ad2.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bdf321419db9850f8aaabea3567d3ad2.png)'
- en: A covariance matrix for all 300 vector components in our dataset
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据集中所有300个向量组件的协方差矩阵
- en: Of course, there’s a big line down the major diagonal, representing that each
    component is strongly correlated with itself. Other than that, this isn’t a very
    interesting graph. Everything looks mostly blank, which is a good sign.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，主对角线有一条明显的线，表示每个组件与自身有很强的相关性。除此之外，这个图并没有什么特别有趣的地方。大部分区域看起来几乎是空白的，这其实是一个好兆头。
- en: 'If you look closely, there’s one exception: components 9 and 276 seem somewhat
    strongly related (covariance of 0.308).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果仔细观察，你会发现有一个例外：组件9和276似乎有较强的相关性（协方差为0.308）。
- en: '![](../Images/bd08c04d3fad3c5c45538546d37a43e3.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd08c04d3fad3c5c45538546d37a43e3.png)'
- en: The covariance matrix zoomed in on components 9 and 276\. Observe the somewhat
    bright red dot here, along with strange behavior along the row and column.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 聚焦于组件9和276的协方差矩阵。观察这里有一个稍微明亮的红点，以及沿着行和列的奇怪行为。
- en: Let’s investigate this further by printing the vectors that are most associated
    with components 9 and 276\. This is equivalent to cosine similarity to a basis
    vector of all zeros, except for a one in the relevant component.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过打印与组件9和276最相关的向量来进一步调查。这个操作相当于计算与一个全零基向量的余弦相似度，除了相关组件的位置为1。
- en: '[PRE23]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: These results are strange, and not very informative.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果很奇怪，并且信息量不大。
- en: 'But wait: we can also have a positive covariance in these components if words
    with a very *negative* value in one tend to also be very negative in the other.
    Let’s try reversing the direction of similarity.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 等一下：如果一个向量组件中具有非常*负*值的词汇在另一个组件中也倾向于具有非常负的值，那么这些组件之间也可能存在正协方差。让我们试着反转相似性的方向。
- en: '[PRE24]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: It looks like both of these components are associated with basic function words
    and numbers that can be found in many different contexts. This helps explain the
    covariance between them, at least more so than the positive case did.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来这两个组件都与基本功能词和数字相关，这些词可以在许多不同的语境中找到。这有助于解释它们之间的协方差，至少比正协方差的情况更能解释这一点。
- en: Conclusion
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article we applied a variety of **exploratory data analysis (EDA)**
    techniques to a 300-dimensional dataset of **GloVe word embeddings**. We used
    cosine similarity to measure the similarity between the meaning of words, clustering
    to group words into related groups, and principal component analysis (PCA) to
    identify the directions in vector space that are most important to the embedding
    model.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们对一个包含300维的**GloVe词向量**数据集应用了各种**探索性数据分析（EDA）**技术。我们使用余弦相似度来衡量词汇意义之间的相似性，使用聚类将词汇分组为相关群体，并通过主成分分析（PCA）来识别对词向量模型最重要的向量空间方向。
- en: We visually observed overall minimal covariance between the input features using
    principal component analysis. We tried using PCA to plot all of our 300-dimensional
    data in just two dimensions, but this was still a little messy.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过主成分分析（PCA）在视觉上观察到输入特征之间的协方差非常小。我们尝试使用PCA将所有300维的数据投影到二维空间中，但结果还是有点混乱。
- en: We also tested assumptions and biases in our dataset. We identified gender bias
    in our dataset by comparing the cosine similarity of *nurse* with each of *man*
    and *woman*. We tried using vector math to represent analogies (like “king” is
    to “queen” as “man” is to “woman”), with some success. By subtracting various
    examples of vectors referring to males and females, we were able to discover a
    vector direction associated with gender, and display the “most masculine” and
    “most feminine” vectors in the dataset.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还测试了数据集中的假设和偏差。通过比较*nurse*与*man*和*woman*的余弦相似度，我们识别出了数据集中的性别偏见。我们尝试使用向量数学表示类比（例如，“king”与“queen”的关系就像“man”与“woman”），并取得了一定的成功。通过减去指代男性和女性的向量示例，我们能够发现与性别相关的向量方向，并展示数据集中“最男性化”和“最女性化”的向量。
- en: There’s a lot more EDA you could try on a dataset of word embeddings, but I
    hope this was a good starting point to understand both some techniques of EDA
    in general and the structure of word embeddings in particular. If you want to
    see the full code associated with this article, plus some additional examples,
    you can check out my GitHub at [crackalamoo/glove-embeddings-eda](https://github.com/crackalamoo/glove-embeddings-eda).
    Thank you for reading!
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在词向量数据集上尝试更多的EDA，但我希望这能作为一个良好的起点，帮助你理解一般的EDA技术以及词向量的结构。如果你想查看与本文相关的完整代码及一些额外的示例，可以访问我的GitHub：[crackalamoo/glove-embeddings-eda](https://github.com/crackalamoo/glove-embeddings-eda)。感谢阅读！
- en: References
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] J. Pennington, R. Socher and C.Manning, [GloVe: Global Vectors for Word
    Representation](https://nlp.stanford.edu/projects/glove/) (2014), Stanford NLP
    (Public Domain Dataset)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] J. Pennington, R. Socher 和 C. Manning，[GloVe：用于词表示的全局向量](https://nlp.stanford.edu/projects/glove/)（2014年），斯坦福大学自然语言处理（公开领域数据集）'
- en: '[2] T. Bolukbasi, K. Chang, J. Zou, V. Saligrama and A. Kalai, [Man is to Computer
    Programmer as Woman is to Homemaker? Debiasing Word Embeddings](https://arxiv.org/pdf/1607.06520)
    (2016), Microsoft Research New England'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] T. Bolukbasi, K. Chang, J. Zou, V. Saligrama 和 A. Kalai，[人类和计算机程序员的关系，就像女人和家庭主妇的关系？去偏见词向量嵌入](https://arxiv.org/pdf/1607.06520)（2016年），微软研究院新英格兰分院'
- en: '*All images created by the author using Matplotlib.*'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*所有图片均由作者使用Matplotlib制作。*'
