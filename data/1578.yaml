- en: 'PySpark Explained: Dealing with Invalid Records When Reading CSV and JSON Files'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark解析：处理读取CSV和JSON文件时的无效记录
- en: 原文：[https://towardsdatascience.com/pyspark-explained-dealing-with-invalid-records-when-reading-csv-and-json-files-4c671feb4d8e?source=collection_archive---------8-----------------------#2024-06-25](https://towardsdatascience.com/pyspark-explained-dealing-with-invalid-records-when-reading-csv-and-json-files-4c671feb4d8e?source=collection_archive---------8-----------------------#2024-06-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/pyspark-explained-dealing-with-invalid-records-when-reading-csv-and-json-files-4c671feb4d8e?source=collection_archive---------8-----------------------#2024-06-25](https://towardsdatascience.com/pyspark-explained-dealing-with-invalid-records-when-reading-csv-and-json-files-4c671feb4d8e?source=collection_archive---------8-----------------------#2024-06-25)
- en: '![](../Images/773bc225167a3d85a50cc3f7756207c5.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/773bc225167a3d85a50cc3f7756207c5.png)'
- en: Image by AI (Ideogram)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：AI（Ideogram）
- en: Effective techniques for identifying and handling data errors
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有效的技术来识别和处理数据错误
- en: '[](https://medium.com/@thomas_reid?source=post_page---byline--4c671feb4d8e--------------------------------)[![Thomas
    Reid](../Images/c1b4e5f577272633ba07e5dbfd21c02d.png)](https://medium.com/@thomas_reid?source=post_page---byline--4c671feb4d8e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4c671feb4d8e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4c671feb4d8e--------------------------------)
    [Thomas Reid](https://medium.com/@thomas_reid?source=post_page---byline--4c671feb4d8e--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@thomas_reid?source=post_page---byline--4c671feb4d8e--------------------------------)[![Thomas
    Reid](../Images/c1b4e5f577272633ba07e5dbfd21c02d.png)](https://medium.com/@thomas_reid?source=post_page---byline--4c671feb4d8e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4c671feb4d8e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4c671feb4d8e--------------------------------)
    [Thomas Reid](https://medium.com/@thomas_reid?source=post_page---byline--4c671feb4d8e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4c671feb4d8e--------------------------------)
    ·8 min read·Jun 25, 2024
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4c671feb4d8e--------------------------------)
    ·8分钟阅读·2024年6月25日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: If you are a frequent user of PySpark, one of the most common operations you’ll
    do is reading CSV or JSON data from external files into DataFrames. If your input
    data has a user-specified schema definition associated with it, you may find that
    not all the records you are processing will meet the schema specification.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是PySpark的常用用户，那么你最常做的操作之一就是将CSV或JSON数据从外部文件读取到DataFrame中。如果你的输入数据有一个用户指定的模式定义，你可能会发现，并非所有处理的记录都符合该模式的规范。
- en: In other words, there may be invalid records present. Perhaps some fields will
    be missing, there will be extra unaccounted-for fields, or some will contain data
    that are not of the type specified in the schema. For small files, tracking down
    these records isn’t a problem, but for the huge mega files of the big data world,
    these can be a real headache to sort out. The question is, what can we do in these
    scenarios?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，可能存在无效记录。也许一些字段缺失，或者存在一些未被考虑的多余字段，或者某些字段的数据类型与模式中指定的不符。对于小文件来说，追踪这些记录并不困难，但对于大数据世界中的庞大文件，这些问题可能会成为一个真正的头痛问题。问题是，在这些情况下我们该怎么办？
- en: As you’ll find out shortly, one of the answers to this question is to use the
    various PySpark `parse`options available when you read CSV or JSON files into
    a DataFrame.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你很快会发现的那样，解决这个问题的一个方法是使用在将CSV或JSON文件读取到DataFrame时，PySpark提供的各种`parse`选项。
- en: Accessing a FREE PySpark development environment
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 访问免费的PySpark开发环境
- en: Before we continue, if you want to follow along with the code in this article,
    you’ll need access to a PySpark development environment.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，如果你想跟随本文中的代码进行操作，你需要访问一个PySpark开发环境。
