- en: Speeding Up the Vision Transformer with BatchNorm
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用BatchNorm加速视觉变换器
- en: 原文：[https://towardsdatascience.com/speeding-up-the-vision-transformer-with-batch-normalization-d37f13f20ae7?source=collection_archive---------7-----------------------#2024-08-06](https://towardsdatascience.com/speeding-up-the-vision-transformer-with-batch-normalization-d37f13f20ae7?source=collection_archive---------7-----------------------#2024-08-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/speeding-up-the-vision-transformer-with-batch-normalization-d37f13f20ae7?source=collection_archive---------7-----------------------#2024-08-06](https://towardsdatascience.com/speeding-up-the-vision-transformer-with-batch-normalization-d37f13f20ae7?source=collection_archive---------7-----------------------#2024-08-06)
- en: How integrating Batch Normalization in an encoder-only Transformer architecture
    can lead to reduced training time and inference time.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍了如何在仅包含编码器的变换器架构中集成批量归一化，从而减少训练时间和推理时间。
- en: '[](https://medium.com/@anindya.hepth?source=post_page---byline--d37f13f20ae7--------------------------------)[![Anindya
    Dey, PhD](../Images/5045d6826256d80721b2615ae701d4b1.png)](https://medium.com/@anindya.hepth?source=post_page---byline--d37f13f20ae7--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d37f13f20ae7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d37f13f20ae7--------------------------------)
    [Anindya Dey, PhD](https://medium.com/@anindya.hepth?source=post_page---byline--d37f13f20ae7--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@anindya.hepth?source=post_page---byline--d37f13f20ae7--------------------------------)[![Anindya
    Dey, PhD](../Images/5045d6826256d80721b2615ae701d4b1.png)](https://medium.com/@anindya.hepth?source=post_page---byline--d37f13f20ae7--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d37f13f20ae7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d37f13f20ae7--------------------------------)
    [Anindya Dey博士](https://medium.com/@anindya.hepth?source=post_page---byline--d37f13f20ae7--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d37f13f20ae7--------------------------------)
    ·23 min read·Aug 6, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[数据科学前沿](https://towardsdatascience.com/?source=post_page---byline--d37f13f20ae7--------------------------------)·阅读时间23分钟·2024年8月6日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/00399a6314badbf4b8ee98261035de52.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/00399a6314badbf4b8ee98261035de52.png)'
- en: 'Image courtesy: Jr Korpa on [Unsplash](https://unsplash.com/)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：Jr Korpa，来自[Unsplash](https://unsplash.com/)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: The introduction of transformer-based architectures, pioneered by the discovery
    of the [Vision Transformer](https://arxiv.org/abs/2010.11929) (ViT), has ushered
    in a revolution in the field of Computer Vision. For a wide range of applications,
    ViT and its various cousins have effectively challenged the status of Convolutional
    Neural Networks (CNNs) as the state-of-the-art architecture (see [this](https://arxiv.org/abs/2108.05305)
    paper for a nice comparative study). Yet, in spite of this success, ViTs are known
    to require significantly longer training times and have slower inference speed
    for smaller-to-medium input data sizes. It is therefore an important issue to
    study modifications of the Vision Transformer which may lead to faster training
    and inference speeds.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 基于变换器架构的引入，源自[视觉变换器](https://arxiv.org/abs/2010.11929)（ViT）的发现，已经在计算机视觉领域掀起了一场革命。对于广泛的应用，ViT及其各种变种有效地挑战了卷积神经网络（CNN）作为最先进架构的地位（有关详细的比较研究，请参见[这篇](https://arxiv.org/abs/2108.05305)论文）。然而，尽管取得了这一成功，ViT被认为需要显著更长的训练时间，并且在较小到中等输入数据规模下推理速度较慢。因此，研究可能导致更快训练和推理速度的视觉变换器修改方法是一个重要课题。
- en: '**In the first of a series of articles**, I explore in detail one such modification
    of the ViT, which will involve replacing Layer Normalization (LayerNorm) — the
    default normalization technique for transformers — with Batch Normalization (BatchNorm).
    More specifically, I will discuss two versions of such a model. As I will review
    in a minute, the ViT has an encoder-only architecture with the transformer encoder
    consisting of two distinct modules — the multi-headed self-attention (MHSA) and
    the feedforward network (FFN). The first model will involve implementing a BatchNorm
    layer *only* in the feedforward network — this will be referred to as **ViTBNFFN
    (**Vision Transformer with BatchNorm in the feedforward network**)**. The second
    model will involve replacing the LayerNorm with BatchNorm *everywhere* in the
    Vision Transformer — I refer to this model as **ViTBN (**Vision Transformer with
    BatchNorm**).** Therefore, the model ViTBNFFNwillinvolve both LayerNorm (in the
    MHSA) and BatchNorm (in the FFN), while ViTBN will involve BatchNorm only.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**在一系列文章的第一篇中**，我详细探讨了 ViT 的一种修改，这种修改将涉及用 Batch Normalization（BatchNorm）替换
    Layer Normalization（LayerNorm）——即 transformers 默认的归一化技术。更具体地说，我将讨论这种模型的两个版本。正如我稍后将回顾的，ViT
    是一个仅包含编码器的架构，其中 transformer 编码器由两个不同的模块组成——多头自注意力（MHSA）和前馈网络（FFN）。第一个模型将仅在前馈网络中实现
    BatchNorm 层——这将被称为 **ViTBNFFN**（带有前馈网络中 BatchNorm 的视觉 Transformer）。第二个模型将用 BatchNorm
    替换视觉 Transformer 中的 LayerNorm *所有*地方——我将此模型称为 **ViTBN**（带有 BatchNorm 的视觉 Transformer）。因此，模型
    ViTBNFFN 将同时涉及 LayerNorm（在 MHSA 中）和 BatchNorm（在 FFN 中），而 ViTBN 仅涉及 BatchNorm。'
- en: I will compare the performances of the three models — ViTBNFFN, ViTBN and the
    standard ViT — on the MNIST dataset of handwritten digits. To be more specific,
    I will compare the following metrics— training time per epoch, testing/inference
    time per epoch, training loss and test accuracy for the models in two distinct
    experimental set-ups. In the first set-up, the models are compared at a fixed
    choice of learning rate and batch size. The exercise is then repeated at different
    values of the learning rate keeping the batch size unchanged. In the second set-up,
    one first finds the best choices of learning rate and batch size for each model
    that maximizes the accuracy using a **Bayesian Optimization** procedure. The performances
    of these optimized models are then compared in terms of the metrics mentioned
    above. For a reasonable choice of architectures that we detail below, the models
    ViTBNFFN and ViTBN lead to more than 60% gain in the average training time per
    epoch as well as the average inference time per epoch while giving a comparable
    (or superior) accuracy compared to the standard ViT. In addition, the models with
    BatchNorm allow for a larger learning rate compared to ViT without compromising
    the stability of the models. The latter finding is consistent with the general
    intuition of BatchNorm deployed in CNNs, as pointed out in the original paper
    of [Ioffe and Szegedy](https://arxiv.org/abs/1502.03167).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我将比较三个模型的性能——ViTBNFFN、ViTBN 和标准 ViT——在手写数字的 MNIST 数据集上的表现。更具体地说，我将比较以下指标——每个
    epoch 的训练时间、每个 epoch 的测试/推理时间、训练损失和两个不同实验设置下模型的测试准确度。在第一个设置中，模型在固定的学习率和批量大小下进行比较。然后，在保持批量大小不变的情况下，使用不同的学习率值重复该实验。在第二个设置中，首先为每个模型找到最大化准确度的最佳学习率和批量大小，使用
    **贝叶斯优化** 过程。然后，比较这些优化后的模型在上述指标上的表现。对于我们在下面详细介绍的合理架构选择，ViTBNFFN 和 ViTBN 模型在每个
    epoch 的平均训练时间和平均推理时间上都提高了超过 60%，同时提供了与标准 ViT 相当（或更好的）准确度。此外，BatchNorm 模型相比 ViT
    允许使用更大的学习率，而不会妥协模型的稳定性。这个发现与原论文中提到的 CNN 中使用 BatchNorm 的直觉一致，正如 [Ioffe 和 Szegedy](https://arxiv.org/abs/1502.03167)
    所指出的。
- en: You can fork the code used in these articles at the github [repo](https://github.com/anindyahepth/BatchNorm_in_Transformers_CV)
    and play around with it. Let me know what you think!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 github 上的 [repo](https://github.com/anindyahepth/BatchNorm_in_Transformers_CV)
    中获取这些文章中使用的代码，并进行尝试。告诉我你的想法！
- en: Contents
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目录
- en: I begin with a gentle introduction to BatchNorm and its PyTorch implementation
    followed by a brief review of the Vision Transformer. Readers familiar with these
    topics can skip to the next section, where we describe the implementation of the
    ViTBNFFN and the ViTBN models using PyTorch. Next, I set up the simple numerical
    experiments using the tracking feature of **MLFlow** to train and test these models
    on the MNIST dataset (without any image augmentation), and compare the results
    with those of the standard ViT. The Bayesian optimization is performed using the
    BoTorch optimization engine available on the[**Ax**](https://ax.dev/) **platform.**
    I end with a brief summary of the results and a few concluding remarks.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我从对 BatchNorm 及其 PyTorch 实现的简单介绍开始，然后简要回顾了 Vision Transformer。熟悉这些主题的读者可以跳到下一节，在那里我们描述了如何使用
    PyTorch 实现 ViTBNFFN 和 ViTBN 模型。接下来，我使用 **MLFlow** 的跟踪功能设置简单的数值实验，训练并测试这些模型在 MNIST
    数据集上的表现（没有任何图像增强），并将结果与标准 ViT 模型进行比较。贝叶斯优化是通过 [**Ax**](https://ax.dev/) **平台**
    上的 BoTorch 优化引擎执行的。最后，我简要总结了结果并给出了一些结语。
- en: 'Batch Normalization : Definition and PyTorch Implementation'
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量归一化：定义与 PyTorch 实现
- en: 'Let us briefly review the basic concept of BatchNorm in a deep neural network.
    The idea was first introduced in a paper by [Ioffe and Szegedy](https://arxiv.org/abs/1502.03167)
    as a method to speed up training in Convolutional Neural Networks. Suppose zᵃᵢ
    denote the input for a given layer of a deep neural network, where a is the batch
    index which runs from a=1,…, Nₛ and i is the feature index running from i=1,…,
    C. Here Nₛ is the number of samples in a batch and C is the dimension of the layer
    that generates zᵃᵢ. The BatchNorm operation then involves the following steps:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要回顾一下深度神经网络中 BatchNorm 的基本概念。这个想法最早在 [Ioffe 和 Szegedy](https://arxiv.org/abs/1502.03167)
    的论文中提出，作为一种加速卷积神经网络训练的方法。假设 zᵃᵢ 表示深度神经网络中某一层的输入，其中 a 是批量索引，范围为 a=1,…, Nₛ，i 是特征索引，范围为
    i=1,…, C。这里，Nₛ 是批量中的样本数量，C 是生成 zᵃᵢ 的层的维度。BatchNorm 操作通常包括以下步骤：
- en: For a given feature i, compute the mean and the variance over the batch of size
    Nₛ i.e.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于给定的特征 i，计算批量大小为 Nₛ 的均值和方差，即：
- en: '![](../Images/c8e995359adbf70880eab38a3fc8b5f3.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c8e995359adbf70880eab38a3fc8b5f3.png)'
- en: '2\. For a given feature i, normalize the input using the mean and variance
    computed above, i.e. define ( for a fixed small positive number ϵ):'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 对于给定的特征 i，使用上述计算的均值和方差对输入进行标准化，即定义（对于一个固定的小正数 ϵ）：
- en: '![](../Images/8c60de6a01e73c93d4ed9a2f7aaa2507.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c60de6a01e73c93d4ed9a2f7aaa2507.png)'
- en: '3\. Finally, shift and rescale the normalized input for every feature i:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 最后，对于每个特征 i，偏移并重新缩放标准化后的输入：
- en: '![](../Images/559d70c5248bde1dd65e705a2f097f9a.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/559d70c5248bde1dd65e705a2f097f9a.png)'
- en: where there is no summation over the indices a or i, and the parameters (γᵃᵢ,
    βᵃᵢ) are trainable.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 其中没有对索引 a 或 i 进行求和，并且参数（γᵃᵢ, βᵃᵢ）是可训练的。
- en: The layer normalization (LayerNorm) on the other hand involves computing the
    mean and the variance over the feature index for a fixed batch index a, followed
    by analogous normalization and shift-rescaling operations.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，层归一化（LayerNorm）涉及对固定批量索引 a 计算特征索引的均值和方差，然后进行类似的标准化和缩放操作。
- en: 'PyTorch has an in-built class BatchNorm1d which performs batch normalization
    for a 2d or a 3d input with the following specifications:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 内置了一个 BatchNorm1d 类，用于对 2d 或 3d 输入执行批量归一化，具有以下规格：
- en: Code Block 1\. The BatchNorm1d class in PyTorch.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块 1\. PyTorch 中的 BatchNorm1d 类。
- en: In a generic image processing task, an image is usually divided into a number
    of smaller patches. The input z then has an index α (in addition to the indices
    a and i) which labels the specific patch in a sequence of patches that constitutes
    an image. The BatchNorm1d class treats the first index of the input as the batch
    index and the second as the feature index, where num_features = C. It is therefore
    important that the input is a 3d tensor of the shape Nₛ × C × N where N is the
    number of patches. The output tensor has the same shape as the input. PyTorch
    also has a class BatchNorm2d that can handle a 4d input. For our purposes it will
    be sufficient to make use of the BatchNorm1d class.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在一般的图像处理任务中，图像通常被分割成若干个较小的图块。输入z随后会有一个索引α（除了a和i的索引外），该索引标记序列中构成图像的特定图块。BatchNorm1d类将输入的第一个索引视为批次索引，第二个索引作为特征索引，其中num_features
    = C。因此，输入必须是一个形状为Nₛ × C × N的三维张量，其中N为图块数量。输出张量与输入的形状相同。PyTorch还提供了一个BatchNorm2d类，可以处理四维输入。对于我们的任务，使用BatchNorm1d类就足够了。
- en: The BatchNorm1d class in PyTorch has an additional feature that we need to discuss.
    If one sets track_running_stats = True (which is the default setting), the BatchNorm
    layer keeps running estimates of its computed mean and variance during training
    (see [here](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html)
    for more details), which are then used for normalization during testing. If one
    sets the option track_running_stats = False, the BatchNorm layer does not keep
    running estimates and instead uses the batch statistics for normalization during
    testing as well. For a generic dataset, the default setting might lead to the
    training and the testing accuracies being significantly different, at least for
    the first few epochs. However, for the datasets that I work with, one can explicitly
    check that this is not the case. I therefore simply keep the default setting while
    using the BatchNorm1d class.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch中的BatchNorm1d类有一个额外的特性，我们需要讨论。如果设置track_running_stats = True（这是默认设置），BatchNorm层会在训练过程中保持其计算的均值和方差的运行估计（更多细节请见[这里](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html)），这些估算值将在测试时用于归一化。如果设置track_running_stats
    = False，则BatchNorm层不会保持运行估算值，而是使用批次统计量在测试时进行归一化。对于一般数据集，默认设置可能导致训练和测试的准确率显著不同，至少在前几个epoch内是这样的。然而，对于我使用的数据集，可以明确检查发现并非如此。因此，在使用BatchNorm1d类时，我保持默认设置。
- en: 'The Standard Vision Transformer : A Brief Review'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标准Vision Transformer：简要回顾
- en: 'The Vision Transformer (**ViT**) was introduced in the paper [***An Image is
    worth 16 × 16 words***](https://arxiv.org/abs/2010.11929) for image classification
    tasks. Let us begin with a brief review of the model (see [here](https://github.com/lucidrains/vit-pytorch)
    for a PyTorch implementation). The details of the architecture for this encoder-only
    transformer model is shown in Figure 1 below, and consists of three main parts:
    **the** **embedding layers**, **a transformer encoder**, and **an MLP head**.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Vision Transformer（**ViT**）在论文[***一张图胜过16 × 16个词***](https://arxiv.org/abs/2010.11929)中被提出，主要用于图像分类任务。让我们从模型的简要回顾开始（有关PyTorch实现的详细信息，请见[这里](https://github.com/lucidrains/vit-pytorch)）。该编码器-仅Transformer模型的架构细节如图1所示，主要由三部分组成：**嵌入层**、**Transformer编码器**和**MLP头部**。
- en: '![](../Images/40a4b0358def22157dec2f8da52054c3.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40a4b0358def22157dec2f8da52054c3.png)'
- en: 'Figure 1\. The architecture of a Vision Transformer. Image courtesy: An Image
    is Worth 16x16 words .'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. Vision Transformer架构。图片来源：一张图胜过16×16个词。
- en: The embedding layers break up an image into a number of patches and maps each
    patch to a vector. The embedding layers are organized as follows. One can think
    of a 2d image as a real 3d tensor of shape H× W × c with H,W, and c being the
    height, width (in pixels) and the number of color channels of the image respectively.
    In the first step, such an image is reshaped into a 2d tensor of shape N × dₚ
    using patches of size p, where N= (H/p) × (W/p) is the number of patches and dₚ
    = p² × c is the patch dimension. As a concrete example, consider a 28 × 28 grey-scale
    image. In this case, H=W=28 while c=1\. If we choose a patch size p=7, then the
    image is divided into a sequence of N=4 × 4 = 16 patches with patch dimension
    dₚ = 49.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层将图像分解成多个patch，并将每个patch映射为一个向量。嵌入层的组织方式如下。可以将2d图像看作一个实际的3d张量，形状为H × W × c，其中H、W和c分别是图像的高度、宽度（单位为像素）和颜色通道数。在第一步中，图像被重新形状为一个形状为N
    × dₚ的2d张量，使用大小为p的patch，其中N = (H/p) × (W/p)是patch的数量，dₚ = p² × c是patch的维度。举一个具体的例子，考虑一个28
    × 28的灰度图像。在这种情况下，H = W = 28，c = 1。如果选择patch大小p = 7，则图像将被划分为N = 4 × 4 = 16个patch，patch维度为dₚ
    = 49。
- en: In the next step, a linear layer maps the tensor of shape N × dₚ to a tensor
    of shape N × dₑ , where dₑ is known as the embedding dimension. The tensor of
    shape N × dₑ is then promoted to a tensor **y** of shape(N+1) × dₑ by prepending
    the former with a learnable dₑ-dimensional vector **y₀**. The vector **y₀** represents
    the embedding of **CLS tokens** in the context of image classification as we will
    explain below. To the tensor **y** one then adds another tensor **yₑ** of shape(N+1)
    × dₑ — this tensor encodes the **positional embedding** information for the image.
    One can either choose a learnable **yₑ** or use a fixed 1d sinusoidal representation
    (see the [paper](https://arxiv.org/abs/2010.11929) for more details). The tensor
    **z** = **y** + **yₑ** of shape(N+1) × dₑ is then fed to the transformer encoder.
    Generically, the image will also be labelled by a batch index. The output of the
    embedding layer is therefore a 3d tensor of shape Nₛ × (N+1) × dₑ.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，一个线性层将形状为N × dₚ的张量映射到形状为N × dₑ的张量，其中dₑ被称为嵌入维度。然后，形状为N × dₑ的张量通过在前面加上一个可学习的dₑ维向量**y₀**，被提升为形状为(N+1)
    × dₑ的张量**y**。向量**y₀**表示**CLS tokens**在图像分类中的嵌入，下面我们将详细解释。接着，另一个形状为(N+1) × dₑ的张量**yₑ**被加到张量**y**中——该张量编码了图像的**位置嵌入**信息。可以选择使用一个可学习的**yₑ**，或者使用固定的1d正弦表示（更多细节请参见[论文](https://arxiv.org/abs/2010.11929)）。然后，形状为(N+1)
    × dₑ的张量**z** = **y** + **yₑ**被送入Transformer编码器。一般而言，图像还会被标记为一个批次索引。因此，嵌入层的输出是一个形状为Nₛ
    × (N+1) × dₑ的3d张量。
- en: The transformer encoder, which is shown in Figure 2 below, takes a 3d tensor
    **zᵢ** of shape Nₛ × (N+1) × dₑ as input and outputs a tensor **zₒ** ofthe same
    shape. This tensor **zₒ** is in turnfed to the MLP head for the final classification
    in the following fashion. Let **z⁰ₒ** be the tensor of shapeNₛ × dₑ corresponding
    to the first component of **zₒ** along the seconddimension. This tensor is the
    “final state” of the learnable tensor **y₀** that prepended the input tensor to
    the encoder, as I described earlier. If one chooses to use CLS tokens for the
    classification, the MLP head isolates **z⁰ₒ** from theoutput **zₒ** of thetransformer
    encoderand maps the former to an Nₛ × n tensor where n is the number of classes
    in the problem. Alternatively, one may also choose perform a global pooling whereby
    one computes the average of the output tensor **zₒ** over the (N+1) patchesfor
    a given feature which results in a tensor **zᵐₒ** of shape Nₛ × dₑ. The MLP head
    then maps **zᵐₒ** to a 2d tensor of shape Nₛ × n as before.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图2所示，Transformer编码器将形状为Nₛ × (N+1) × dₑ的3d张量**zᵢ**作为输入，并输出一个相同形状的张量**zₒ**。然后，这个张量**zₒ**被送入MLP头进行最终的分类处理。令**z⁰ₒ**为形状为Nₛ
    × dₑ的张量，对应于**zₒ**在第二维度上的第一个组件。这个张量是可学习张量**y₀**的“最终状态”，它在之前的步骤中将输入张量加到编码器之前。如果选择使用CLS
    tokens进行分类，MLP头会从Transformer编码器的输出**zₒ**中提取**z⁰ₒ**，并将其映射到一个形状为Nₛ × n的张量，其中n是问题中的类别数。或者，也可以选择进行全局池化，通过计算给定特征的输出张量**zₒ**在(N+1)个patch上的平均值，从而得到一个形状为Nₛ
    × dₑ的张量**zᵐₒ**。然后，MLP头会将**zᵐₒ**映射到一个形状为Nₛ × n的2d张量。
- en: '![](../Images/ad359302af86723ea594bb93875c5994.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad359302af86723ea594bb93875c5994.png)'
- en: 'Figure 2\. The structure of the transformer encoder inside the Vision Transformer.
    Image courtesy: An Image is Worth 16x16 words .'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图2. Vision Transformer中Transformer编码器的结构。图片来源：An Image is Worth 16x16 words。
- en: Let us now discuss the constituents of the transformer encoder in more detail.
    As shown in Figure 2, it consists of L transformer blocks, where the number L
    is often referred to as the *depth* of the model. Each transformer block in turn
    consists of a multi-headed self attention (MHSA) module and an MLP module (also
    referred to as a feedforward network) with residual connections as shown in the
    figure. The MLP module consists of two hidden layers with a GELU activation layer
    in the middle. The first hidden layer is also preceded by a LayerNorm operation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们更详细地讨论变换器编码器的组成部分。如图 2 所示，它由 L 个变换器块组成，其中 L 通常被称为模型的 *深度*。每个变换器块又由一个多头自注意力（MHSA）模块和一个带有残差连接的
    MLP 模块（也称为前馈网络）组成，如图所示。MLP 模块由两个隐藏层和一个位于中间的 GELU 激活层组成。第一个隐藏层前也有一个 LayerNorm 操作。
- en: We are now prepared to discuss the models ViTBNFFN and ViTBN.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备讨论模型 ViTBNFFN 和 ViTBN。
- en: 'Vision Transformer with BatchNorm : ViTBNFFN and ViTBN'
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 带有批量归一化的视觉变换器：ViTBNFFN 和 ViTBN
- en: 'To implement BatchNorm in the ViT architecture, I first introduce a new BatchNorm
    class tailored to our task:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 ViT 架构中实现批量归一化，我首先介绍一个专门针对我们任务的新 BatchNorm 类：
- en: Code Block 2\. The Batch_Norm class which implements the batch normalization
    operation in ViTBNFFN and ViTBN.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块 2\. 实现批量归一化操作的 Batch_Norm 类，适用于 ViTBNFFN 和 ViTBN。
- en: This new class *Batch_Norm* uses the BatchNorm1d (line 10) class which I reviewed
    above. The important modification appears in the lines 13–15\. Recall that the
    input tensor to the transformer encoder has the shape Nₛ × (N+1) × dₑ. At a generic
    layer inside the encoder, the input is a 3d tensor with the shape Nₛ × (N+1) ×
    D, where D is the number of features at that layer. For using the BatchNorm1d
    class, one has to reshape this tensor to Nₛ × D × (N+1), as we explained earlier.
    After implementing the BatchNorm, one needs to reshape the tensor back to the
    shape Nₛ × (N+1) × D, so that the rest of the architecture can be left untouched.
    Both reshaping operations are done using the function *rearrange* which is part
    of the *einops* package.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新类 *Batch_Norm* 使用了我之前讲解过的 BatchNorm1d（第 10 行）类。重要的修改出现在第 13-15 行。回想一下，输入到变换器编码器的张量形状是
    Nₛ × (N+1) × dₑ。在编码器的某一层，输入是形状为 Nₛ × (N+1) × D 的 3D 张量，其中 D 是该层的特征数量。为了使用 BatchNorm1d
    类，必须将这个张量重塑为 Nₛ × D × (N+1)，正如我们之前所解释的那样。实现批量归一化后，需要将张量重新塑形回 Nₛ × (N+1) × D 的形状，以便其余架构保持不变。所有的重塑操作都是通过
    *einops* 包中的 *rearrange* 函数来完成的。
- en: 'One can now describe the models with BatchNorm in the following fashion. First,
    one may modify the feedforward network in the transformer encoder of the ViT by
    removing the LayerNorm operation that precedes the first hidden layer and introducing
    a BatchNorm layer. I will choose to insert the BatchNorm layer between the first
    hidden layer and the GELU activation layer. This gives the model **ViTBNFFN**.
    The PyTorch implementation of the new feedforward network is given as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以以以下方式描述带有批量归一化的模型。首先，可以通过去除先前的 LayerNorm 操作并引入一个批量归一化层来修改 ViT 的变换器编码器中的前馈网络。我选择将批量归一化层插入到第一个隐藏层和
    GELU 激活层之间。这就得到了模型 **ViTBNFFN**。新前馈网络的 PyTorch 实现如下所示：
- en: Code Block 3\. The FeedForward (MLP) module of the transformer encoder with
    Batch Normalization.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块 3\. 带有批量归一化的变换器编码器的 FeedForward（MLP）模块。
- en: The constructor of the FeedForward class, given by the code in the lines 7–11,
    is self-evident. The BatchNorm layer is being implemented by the Batch_Norm class
    in line 8\. The input tensor to the feedforward network has the shape Nₛ × (N+1)
    × dₑ. The first linear layer transforms this to a tensor of shape Nₛ × (N+1) ×
    D, where D= *hidden_dim* (which is also called the *mlp_dimension*) in the code.
    The appropriate feature dimension for the Batch_Norm class is therefore D.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: FeedForward 类的构造函数，代码在第 7-11 行，显而易见。批量归一化层由第 8 行中的 Batch_Norm 类实现。输入到前馈网络的张量形状为
    Nₛ × (N+1) × dₑ。第一个线性层将其转换为形状为 Nₛ × (N+1) × D 的张量，其中 D= *hidden_dim*（代码中也称为 *mlp_dimension*）。因此，Batch_Norm
    类的适当特征维度为 D。
- en: 'Next, one can replace all the LayerNorm operations in the model **ViTBNFFN**
    with BatchNorm operations implemented by the class Batch_Norm. This gives the
    **ViTBN** model. We make a couple of additional tweaks in ViTBNFFN/ViTBN compared
    to the standard ViT. Firstly, we incorporate the option of having either a learnable
    positional encoding or a fixed sinusoidal one by introducing an additional model
    parameter. Similar to the standard ViT, one can choose a method involving either
    CLS tokens or global pooling for the final classification. In addition, we replace
    the MLP head by a simpler linear head. With these changes, the ViTBN class assumes
    the following form (the ViTBNFFN class has an analogous form):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，可以将模型**ViTBNFFN**中的所有LayerNorm操作替换为由Batch_Norm类实现的BatchNorm操作。这就得到了**ViTBN**模型。与标准的ViT相比，我们在ViTBNFFN/ViTBN中进行了一些额外的调整。首先，我们通过引入一个额外的模型参数，加入了可学习的位置编码或固定的正弦位置编码的选项。类似于标准的ViT，用户可以选择使用CLS
    token或全局池化的方式来进行最终分类。此外，我们将MLP头部替换为一个更简单的线性头部。通过这些更改，ViTBN类的形式如下（ViTBNFFN类也具有类似的形式）：
- en: Code Block 4\. The ViTBN class.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块 4\. ViTBN类。
- en: Most of the above code is self-explanatory and closely resembles the standard
    ViT class. Firstly, note that in the lines 23–28, we have replaced LayerNorm with
    BatchNorm in the embedding layers. Similar replacements have been performed inside
    the *Transformer* class representing the transformer encoder that ViTBN uses (see
    line 44). Next, we have added a new hyperparameter “pos_emb”whichtakes as values
    the string ‘pe1d’ or ‘learn’. In the first case, one uses the fixed 1d sinusoidal
    positional embedding while in the second case one uses learnable positional embedding.
    In the forward function, the first option is implemented in the lines 62–66 while
    the second is implemented in the lines 68–72\. The hyperparameter “pool” takes
    as values the strings ‘cls’ or ‘mean’ which correspond to a CLS token or a global
    pooling for the final classification respectively. The ViTBNFFN class can be written
    down in an analogous fashion.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 上述大部分代码不言自明，并且与标准的ViT类非常相似。首先，注意在第23到28行，我们已经在嵌入层中将LayerNorm替换为BatchNorm。在*Transformer*类（表示ViTBN使用的transformer编码器）内部也进行了类似的替换（见第44行）。接下来，我们添加了一个新的超参数“pos_emb”，其取值为字符串‘pe1d’或‘learn’。在第一个情况下，使用固定的1D正弦位置嵌入，而在第二个情况下，使用可学习的位置嵌入。在前向传播函数中，第一个选项在第62到66行实现，而第二个选项在第68到72行实现。超参数“pool”的取值为字符串‘cls’或‘mean’，分别对应于最终分类时使用CLS
    token或全局池化。ViTBNFFN类可以以类似的方式写出。
- en: 'The model ViTBN (analogously ViTBNFFN) can be used as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 模型ViTBN（类似地，ViTBNFFN）可以按如下方式使用：
- en: Code Block 5\. Usage of ViTBN for a 28 × 28 image.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块 5\. 使用ViTBN处理28 × 28图像。
- en: In this specific case, we have the input dimension *image_size* = 28 which implies
    H = W = 28\. The *patch_size* = p =7 means that the number of patches are N= 16\.
    With the number of color channels being 1, the patch dimension is dₚ =p²= 49\.
    The number of classes in the classification problem is given by *num_classes.*
    The parameter *dim=* 64in the model is the embedding dimension dₑ . The number
    of transformer blocks in the encoder is given by the *depth =* L =6*.* Theparameters
    *heads* and *dim_head* correspond tothe number of self-attention heads and the
    (common) dimension of each head in the MHSA module of the encoder. The parameter
    *mlp_dim* is the hidden dimension of the MLP or feedforward module. The parameter
    *dropout* is the single dropout parameter for the transformer encoder appearing
    both in the MHSA as well as in the MLP module, while *emb_dropout* is the dropout
    parameter associated with theembedding layers.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定的情况下，输入维度*image_size* = 28，这意味着H = W = 28。*patch_size* = p = 7，意味着补丁的数量为N
    = 16。由于颜色通道数量为1，补丁的维度是dₚ = p² = 49。分类问题中的类别数由*num_classes*给出。模型中的参数*dim* = 64表示嵌入维度dₑ。编码器中transformer块的数量由*depth*
    = L = 6给出。参数*heads*和*dim_head*分别对应自注意力头的数量以及编码器中MHSA模块中每个头的（共同）维度。参数*mlp_dim*是MLP或前馈模块的隐藏维度。参数*dropout*是transformer编码器的单个丢弃参数，出现在MHSA和MLP模块中，而*emb_dropout*是与嵌入层相关的丢弃参数。
- en: 'Experiment 1: Comparing Models at Fixed Hyperparameters'
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验 1：在固定超参数下比较模型
- en: Having introduced the models with BatchNorm, I will now set up the first numerical
    experiment. It is well known that BatchNorm makes deep neural networks converge
    faster and thereby speeds up training and inference. It also allows one to train
    CNNs with a relatively large learning rate without bringing in instabilities.
    In addition, it is expected to act as a regularizer eliminating the need for dropout.
    The main motivation of this experiment is to understand how some of these statements
    translate to the Vision Transformer with BatchNorm.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍了包含BatchNorm的模型之后，我将开始第一个数值实验。众所周知，BatchNorm能够加速深度神经网络的收敛，从而加速训练和推理过程。它还允许使用相对较大的学习率训练卷积神经网络，而不会引入不稳定性。此外，BatchNorm预计能够作为正则化器，从而消除使用dropout的需求。本实验的主要动机是了解这些说法如何转化为具有BatchNorm的视觉变换器（Vision
    Transformer）。
- en: For this experiment, I will use the hyperparameters for ViT as given in Code
    Block 5 — in particular, the number of transformer layers in the encoder is chosen
    to be 6 i.e. *‘depth =6’.* We will use CLS tokens for classification which corresponds
    to setting *pool = ‘cls’* , and learnable positional embedding which corresponds
    to setting *pos_emb = ‘learn’.* For ViTBNFFN and ViTBN, I will use a single transformer
    layer so that we have *‘depth =1’* while all the other hyperprameters remain the
    same as that of the ViT.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本实验，我将使用Code Block 5中给出的ViT超参数，特别是选择6层的transformer编码器，即*‘depth =6’*。我们将使用CLS标记进行分类，这对应于设置*pool
    = ‘cls’*，并使用可学习的位置信息嵌入，这对应于设置*pos_emb = ‘learn’*。对于ViTBNFFN和ViTBN，我将使用单层transformer，因此我们有*‘depth
    =1’*，同时所有其他超参数保持与ViT相同。
- en: 'The experiment involves the following steps :'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 实验涉及以下步骤：
- en: For a given learning rate, I will train the models ViT, ViTBNFFN and ViTBN on
    the MNIST dataset of handwritten images, for a total of 30 epochs. At this stage,
    I do not use any image augmentation. I will test the model once on the validation
    data after each epoch of training.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于给定的学习率，我将在MNIST手写图像数据集上训练ViT、ViTBNFFN和ViTBN模型，共进行30个周期。在这个阶段，我不会使用任何图像增强。每经过一个训练周期后，我将在验证数据上测试一次模型。
- en: 'For a given model and a given learning rate, I will measure the following quantities
    in a given epoch: the training time, the training loss, the testing time, and
    the testing accuracy. For a fixed learning rate, this will generate four graphs,
    where each graph plots one of these four quantities as a function of epochs for
    the three models. These graphs can then be used to compare the performance of
    the models. In particular, I want to compare the training and the testing times
    of the standard ViT with that of the models with BatchNorm to check if there is
    any significant speeding up in either case.'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于给定的模型和学习率，我将在每个周期中测量以下几个量：训练时间、训练损失、测试时间和测试准确度。对于固定的学习率，这将生成四个图表，每个图表绘制了这些量随周期变化的曲线，比较了三种模型的表现。这些图表可以用来比较模型的性能。特别地，我想比较标准ViT模型与包含BatchNorm的模型的训练和测试时间，以检查是否在某些情况下有显著的加速。
- en: I will perform the operations in Step 1 and Step 2 for three representative
    learning rates l = 0.0005, 0.005 and 0.01, holding all the other hyperparameters
    fixed.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我将对三个代表性的学习率l = 0.0005、0.005和0.01执行步骤1和步骤2的操作，保持所有其他超参数不变。
- en: Throughout the analysis, I will use **CrossEntropyLoss()** as the loss function
    and the **Adam** optimizer, with the training and testing batch sizes being fixed
    at 100 and 5000 respectively for all the epochs. Iwill set all the dropout parameters
    to zero for this experiment. I will also not consider any learning rate decay
    to keep things simple.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个分析过程中，我将使用**CrossEntropyLoss()**作为损失函数，使用**Adam**优化器，并且在所有训练周期中，训练和测试的批量大小分别固定为100和5000。我将把所有dropout参数设置为零，以进行本实验。为了简化问题，我也不考虑学习率衰减。
- en: The experiment has been conducted using the tracking feature of **MLFlow.**
    For all the runs in this experiment, I have used the **NVIDIA L4 Tensor Core GPU**
    available at **Google** **Colab.**
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 本实验已经通过**MLFlow**的跟踪功能进行。所有实验中的运行，我都使用了**Google Colab**上的**NVIDIA L4 Tensor
    Core GPU**。
- en: 'Let us begin by discussing the important ingredients of the MLFlow module which
    we execute for a given run in the experiment. The first of these is the function
    *train_model* which will be usedfor training and testing the models for a given
    choice of hyperparameters:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先讨论执行给定实验运行时使用的MLFlow模块的重要组成部分。第一个组成部分是*train_model*函数，用于根据给定的超参数选择进行模型的训练和测试：
- en: Code Block 6\. Training and testing module for the numerical experiment.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块6：数值实验的训练和测试模块。
- en: The function *train_model* returns four quantities for every epoch — the training
    loss (*cost_list*), test accuracy (*accuracy_list*), training time in seconds
    (*dur_list_train*) and testing time in seconds (*dur_list_val*). The lines of
    code 19–32 give the training module of the function, while the lines 35–45 give
    the testing module. Note that the function allows for testing the model once after
    every epoch of training. In the Git version of our code, you will also find accuracies
    by class, but I will skip that here for the sake of brevity.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 函数*train_model*返回每个epoch的四个量——训练损失(*cost_list*)，测试准确率(*accuracy_list*)，训练时间（秒）(*dur_list_train*)和测试时间（秒）(*dur_list_val*)。第19到32行给出了该函数的训练模块，第35到45行给出了测试模块。请注意，该函数允许在每个训练epoch后进行一次模型测试。在我们代码的Git版本中，你还可以找到按类别划分的准确率，但为了简洁起见，这里我将跳过这一部分。
- en: 'Next, one needs to define a function that will download the MNIST data, split
    it into the training dataset and the validation dataset, and transform the images
    to torch tensors (without any augmentation):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，需要定义一个函数来下载MNIST数据集，将其拆分为训练集和验证集，并将图像转换为torch张量（不进行任何增强）：
- en: Code Block 7\. Getting the MNIST dataset.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块7：获取MNIST数据集。
- en: 'We are now prepared to write down the MLFlow module which has the following
    form:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备编写MLFlow模块，模块的形式如下：
- en: Code Block 8\. MLFlow module to be executed for the experiment.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块8：要执行的MLFlow实验模块。
- en: Let us explain some of the important parts of the code.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解释一些代码中的重要部分。
- en: The lines 11–13 specify the learning rate, the number of epochs and the loss
    function respectively.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第11到13行分别指定了学习率、epoch数量和损失函数。
- en: The lines 16–33 specify the various details of the training and testing. The
    function *get_datesets() of* Code Block 7 downloads the training and validation
    datasets for the MNIST digits, while the function *get_model()* defined in Code
    Block 5 specifies the model. For the latter, we set *pool = ‘cls’* , and *pos_emb
    = ‘learn’.* On line 20, the optimizer is defined, and we specify the training
    and validation data loaders including the respective batch sizes on lines 21–24\.
    Line 25–26 specifies the output of the function *train_model* that we have in
    Code Block 6*—* four lists each with *n_epoch* entries. Lines 16–24 specify the
    various arguments of the function *train_model.*
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第16到33行指定了训练和测试的各种细节。代码块7中的*get_datesets()*函数下载了MNIST数字的训练集和验证集，而在代码块5中定义的*get_model()*函数指定了模型。对于后者，我们设置了*pool
    = ‘cls’*和*pos_emb = ‘learn’*。在第20行，定义了优化器，我们在第21到24行指定了训练和验证数据加载器以及各自的批大小。第25到26行指定了函数*train_model*的输出，我们在代码块6中定义了该函数——四个列表，每个列表包含*n_epoch*个条目。第16到24行指定了函数*train_model*的各种参数。
- en: On lines 37–40, one specifies the parameters that will be logged for a given
    run of the experiment, which for our experiment are the learning parameter and
    the number of epochs.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第37到40行，指定了对于给定实验运行将记录的参数，在我们的实验中是学习参数和epoch数量。
- en: Lines 44–52 constitute the most important part of the code where one specifies
    the metrics to be logged i.e. the four lists mentioned above. It turns out that
    by default the function *mlflow.log_metrics()* does not log a list. In other words,
    if we simply use *mlflow.log_metrics({generic_list}),* then the experiment will
    only log the output for the last epoch. As a workaround, we call the function
    multiple times using a for loop as shown.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第44到52行构成了代码中最重要的部分，在这里指定了需要记录的指标，也就是上面提到的四个列表。事实证明，默认情况下，函数*mlflow.log_metrics()*不会记录一个列表。换句话说，如果我们直接使用*mlflow.log_metrics({generic_list})*，那么实验将只记录最后一个epoch的输出。作为解决方法，我们使用for循环多次调用该函数，如下所示。
- en: 'Let us now take a deep dive into the results of the experiment, which are essentially
    summarized in the three sets of graphs of Figures 3–5 below. Each figure presents
    a set of four graphs corresponding to the training time per epoch (top left),
    testing time per epoch (top right), training loss (bottom left) and test accuracy
    (bottom right) for a fixed learning rate for the three models. Figures 3, 4 and
    5 correspond to the learning rates l=0.0005, l=0.005 and l=0.01 respectively.
    It will be convenient to define a pair of ratios :'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入分析实验结果，这些结果本质上总结在下面图3到图5的三组图表中。每个图表展示了一个包含四个图的集合，分别对应于每个时期的训练时间（左上角）、测试时间（右上角）、训练损失（左下角）和测试准确度（右下角），对于三种模型而言，这些图表都是在固定学习率下得到的。图3、图4和图5分别对应于学习率
    l=0.0005、l=0.005 和 l=0.01。为方便起见，我们定义一对比率：
- en: '![](../Images/6bde3dfe3ce5c4e92066b907b995a903.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6bde3dfe3ce5c4e92066b907b995a903.png)'
- en: 'where T(model|train) and T(model|test) are the average training and testing
    times per epoch for given a model in our experiment. These ratios give a rough
    measure of the speeding up of the Vision Transformer due to the integration of
    BatchNorm. We will always train and test the models for the same number of epochs
    — one can therefore define the percentage gains for the average training and testing
    times per epoch in terms of the above ratios respectively as:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 T(model|train) 和 T(model|test) 是我们实验中给定模型每个时期的平均训练时间和测试时间。这些比率为我们提供了一个粗略的度量，衡量了在集成
    BatchNorm 后，Vision Transformer 加速的效果。我们将始终训练和测试模型相同的时期数——因此，可以分别通过上述比率定义每个时期的平均训练时间和测试时间的百分比提升：
- en: '![](../Images/13ab31745911253c450cd8ba21e8c955.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13ab31745911253c450cd8ba21e8c955.png)'
- en: 'Let us begin with the smallest learning rate l=0.0005 which corresponds to
    Figure 3\. In this case, the standard ViT converges in a fewer number of epochs
    compared to the other models. After 30 epochs, the standard ViT has lower training
    loss and marginally higher accuracy (~ 98.2 %) compared to both ViTBNFFN (~ 97.8
    %) and ViTBN (~ 97.1 %) — see the bottom right graph. However, the training time
    and the testing time are higher for ViT compared to ViTBNFFN/ViTBN by a factor
    greater than 2\. From the graphs, one can read off the ratios rₜ and rᵥ : rₜ (ViTBNFFN)
    = 2.7 , rᵥ (ViTBNFFN)= 2.6, rₜ (ViTBNFFN) = 2.5, and rᵥ (ViTBN)= 2.5 , where rₜ
    , rᵥ have been defined above. Therefore, for the given learning rate, the gain
    in speed due to BatchNorm is significant for both training and inference — it
    is roughly of the order of 60%. The precise percentage gains are listed in Table
    1.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从最小的学习率 l=0.0005 开始，该学习率对应于图3。此时，标准ViT在较少的时期内就会收敛，相比其他模型，训练损失较低，准确度略高（~98.2%），见右下角图表，优于
    ViTBNFFN (~97.8%) 和 ViTBN (~97.1%)。然而，ViT 的训练时间和测试时间相比 ViTBNFFN/ViTBN 高出超过2倍。从图表中可以读取出比率
    rₜ 和 rᵥ：rₜ (ViTBNFFN) = 2.7, rᵥ (ViTBNFFN) = 2.6, rₜ (ViTBNFFN) = 2.5, rᵥ (ViTBN)
    = 2.5，其中 rₜ 和 rᵥ 如上所定义。因此，对于给定的学习率，BatchNorm 带来的速度提升在训练和推理中都非常显著——大约是60%的提升。精确的百分比提升列在表1中。
- en: '![](../Images/70852554f4be025b7f896cb8920127cf.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/70852554f4be025b7f896cb8920127cf.png)'
- en: Figure 3\. The graphs for the learning rate l = 0.0005.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. 学习率 l = 0.0005 的图表。
- en: In the next step, we increase the learning rate to l=0.005 and repeat the experiment,
    which yields the set of graphs in Figure 4.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们将学习率增加到 l=0.005 并重复实验，得到图4中的一组图表。
- en: '![](../Images/2d629584ba4f054331a5b31904d2dc7c.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d629584ba4f054331a5b31904d2dc7c.png)'
- en: Figure 4\. The graphs for the learning rate l=0.005.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. 学习率 l=0.005 的图表。
- en: 'For a learning rate l=0.005, the standard ViT does not seem to have any advantage
    in terms of faster convergence. However, the training time and the testing time
    are again higher for ViT compared to ViTBNFFN/ViTBN. A visual comparison of the
    top left graphs in Figure 3 and Figure 4 indicates that the training time for
    ViT increases significantly while those for ViTBNFFN and ViTBN roughly remain
    the same. This implies that there is a more significant gain in training time
    in this case. On the other hand, comparing the top right graphs in Figure 3 and
    Figure 4, one can see that gain in testing speed is roughly the same. The ratios
    rₜ and rᵥ can again be read off from the top row of graphs in Figure 4 : rₜ (ViTBNFFN)
    = 3.6 , rᵥ (ViTBNFFN)=2.5, rₜ (ViTBN) = 3.5 and rᵥ (ViTBN)= 2.5\. Evidently, the
    ratios rₜ are larger here compared to the case with smaller learning rate, while
    the ratios rᵥ remain about the same. This leads to a higher percentage gain (~70%)
    in training time, with the gain for inference time (~60%) remaining roughly the
    same.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于学习率l=0.005，标准ViT在更快收敛方面似乎没有任何优势。然而，与ViTBNFFN/ViTBN相比，ViT的训练时间和测试时间再次较长。从图3和图4左上方的图形可视化比较来看，ViT的训练时间显著增加，而ViTBNFFN和ViTBN的训练时间基本保持不变。这表明，在这种情况下训练时间的提升更为显著。另一方面，从图3和图4右上方的图形进行比较，可以看到测试速度的提升大致相同。rₜ和rᵥ的比值可以从图4顶部的图形中读取：rₜ（ViTBNFFN）=
    3.6，rᵥ（ViTBNFFN）=2.5，rₜ（ViTBN）= 3.5，rᵥ（ViTBN）= 2.5。显然，与较小学习率的情况相比，这里的rₜ比值更大，而rᵥ比值保持基本不变。这导致训练时间的提升百分比更高（约70%），而推理时间的提升（约60%）保持大致不变。
- en: Finally, let us increase the learning rate even further to l=0.01 and repeat
    the experiment, which yields the set of graphs in Figure 5.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们将学习率进一步提高到l=0.01，并重复实验，得到图5中的一组图形。
- en: '![](../Images/33c59f4a3a541b4f081ec62aedd1ebb7.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/33c59f4a3a541b4f081ec62aedd1ebb7.png)'
- en: Figure 5\. The graphs for the learning rate l=0.01.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图5。学习率l=0.01的图形。
- en: In this case, ViT becomes unstable after a few epochs as one can see from the
    training_loss graph in Figure 5, which shows a non-converging behavior starting
    in the vicinity of epoch 15\. This is also corroborated by the test_accuracy graph
    where the accuracy of ViT can be seen to plummet around epoch 15\. However, the
    models ViTBNFFN and ViTBN remain stable and reach accuracies higher than 97% at
    the end of 30 epochs of training. The training time for ViT is even higher in
    this case and fluctuates wildly. For ViTBNFFN, there is an appreciable increase
    in the training time, while it remains roughly the same for ViTBN — see the top
    left graph. In terms of the training ratios rₜ, we have rₜ (ViTBNFFN) = 2.7 and
    rₜ(ViTBN)=4.3\. The first ratio is lower than what we found in the previous case.
    This is an artifact of the higher training time for ViTBNFFN, which offsets the
    increase in the training time for ViT. The second ratio is significantly higher
    since the training time for ViTBN roughly remains unchanged. The test ratios rᵥ
    in this case — rᵥ (ViTBNFFN)=2.6 and rᵥ (ViTBN)= 2.7 — show a tiny increase.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，ViT在经过几个epoch后变得不稳定，正如图5中的训练损失图所示，训练损失从第15个epoch附近开始出现不收敛的行为。这一点也可以从测试精度图中得到验证，ViT的精度在第15个epoch左右急剧下降。然而，ViTBNFFN和ViTBN模型保持稳定，并且在30个epoch的训练结束时精度超过97%。在这种情况下，ViT的训练时间甚至更高，并且波动剧烈。对于ViTBNFFN，训练时间有明显的增加，而对于ViTBN，训练时间大致保持不变——请参见左上图。在训练比率rₜ方面，我们得到rₜ（ViTBNFFN）=
    2.7和rₜ（ViTBN）= 4.3。第一个比率低于我们在之前的情况下得到的结果，这是由于ViTBNFFN的训练时间较长，抵消了ViT的训练时间增加。第二个比率显著更高，因为ViTBN的训练时间几乎没有变化。在这种情况下，测试比率rᵥ——rᵥ（ViTBNFFN）=
    2.6和rᵥ（ViTBN）= 2.7——显示出微小的增加。
- en: The gains in training and inference times — gₜ and gᵥ are summarized for different
    learning rates in Table 1.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 训练时间和推理时间的提升——gₜ和gᵥ在不同学习率下的总结见表1。
- en: '![](../Images/18ef72028a94372566e7ae26e390207c.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18ef72028a94372566e7ae26e390207c.png)'
- en: Table 1\. Percentage gains in training and testing times per epoch for ViTBNFFN
    and ViTBN with respect to ViT.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 表1。ViTBNFFN和ViTBN相对于ViT在每个epoch的训练和测试时间的百分比提升。
- en: It is also interesting to visualize more explicitly how the training time for
    each model changes with the learning rate. This is shown in the set of three graphs
    in Figure 6 for ViT, ViTBNFFN and ViTBN. The subscripts i=1,2,3 in model_i corresponds
    to the three learning rates l= 0.0005, 0.005 and 0.01 respectively for a given
    model.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，直观地展示每个模型的训练时间如何随学习率变化也是很有趣的。这在图6中的三张图中得以展示，分别表示ViT、ViTBNFFN和ViTBN。模型_i中的下标i=1,2,3分别对应给定模型的三种学习率l=
    0.0005、0.005和0.01。
- en: '![](../Images/0831c53d0e59ae3b42740ca94a6c485f.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0831c53d0e59ae3b42740ca94a6c485f.png)'
- en: Figure 6\. Graphs showing how the training time per epoch changes with the learning
    rate for a given model. The subscripts 1,2,3 correspond to l=0.0005, 0.005 and
    0.01 respectively.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图6\. 显示给定模型的每个周期训练时间如何随学习率变化的图形。下标1、2、3分别对应l=0.0005、0.005和0.01。
- en: It is evident that the variation of the training time with learning rate is
    most significant for ViT (top figure). On the other hand, the training time for
    ViTBN remains roughly the same as we vary the learning rate (bottom figure). For
    ViTBNFFN, the variation becomes significant only at a relatively large value (~0.01)
    of the learning rate (middle figure).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，训练时间随学习率变化的差异对ViT（上图）最为显著。另一方面，对于ViTBN，在改变学习率时，训练时间大致保持不变（下图）。对于ViTBNFFN，只有当学习率达到较大的值（~0.01）时，训练时间的变化才变得显著（中图）。
- en: '**Experiment 2: Comparing the Optimized Models**'
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**实验2：比较优化后的模型**'
- en: 'Let us now set up the experiment where we compare the performance of the optimized
    models. This will involve the following steps:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们设置一个实验，比较优化后的模型的性能。该过程将包括以下步骤：
- en: First perform a Bayesian optimization to determine the best set of hyperparameters
    — learning parameter and batch size — for each model.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先执行贝叶斯优化，以确定每个模型的最佳超参数——学习率和批量大小。
- en: Given the three optimized models, train and test each of them for 30 epochs
    and compare the metrics using MLFlow as before — in particular, the training and
    testing/inference times per epoch.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定三个优化模型，训练并测试每个模型30个周期，并使用MLFlow进行指标比较，如前所述——特别是每个周期的训练和测试/推理时间。
- en: Let us begin with the first step. We use the BoTorch optimization engine available
    on the [**Ax**](https://ax.dev/) platform. For details on the optimization procedure
    using BoTorch, we refer the reader to [this](https://ax.dev/docs/bayesopt.html)
    documentation on Ax. We use accuracy as the optimization metric and limit the
    optimization procedure to 20 iterations. We also need to specify the ranges of
    hyperparameters over which the search will be performed in each case. Our previous
    experiments give us some insight into what the appropriate ranges should be. The
    learning parameter range is [1e-5, 1e-3] for ViT, while that for ViTBNFFN and
    ViTBN is [1e-5, 1e-2]. For all three models, the batch size range is [20, 120].
    The depths of the models are chosen to be the same as in Experiment 1\. The complete
    code for the optimization procedure can be found in the module *hypopt_train.py*
    in the *optimization* folder of the github repo.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从第一步开始。我们使用[**Ax**](https://ax.dev/)平台上可用的BoTorch优化引擎。有关使用BoTorch进行优化程序的详细信息，请参阅[此](https://ax.dev/docs/bayesopt.html)Ax文档。我们使用准确度作为优化指标，并将优化过程限制为20次迭代。我们还需要指定每种情况中进行搜索的超参数范围。我们之前的实验为我们提供了一些关于适当范围的见解。ViT的学习率范围为[1e-5,
    1e-3]，而ViTBNFFN和ViTBN的学习率范围为[1e-5, 1e-2]。对于所有三个模型，批量大小范围为[20, 120]。模型的深度与实验1中的设置相同。优化过程的完整代码可以在GitHub仓库的*optimization*文件夹中的模块*hypopt_train.py*中找到。
- en: The upshot of the procedure is a set of optimized hyperparameters for each model.
    We summarize them in Table 2.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程的结果是每个模型的一组优化超参数。我们在表2中总结了它们。
- en: '![](../Images/5b684fb5b236e3eaede192c20ef7fc34.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b684fb5b236e3eaede192c20ef7fc34.png)'
- en: Table 2\. The optimized hyperparameters for each model from Bayesian Optimization
    using BoTorch.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表2\. 使用BoTorch进行贝叶斯优化得到的每个模型的优化超参数。
- en: For each model, one can plot how the accuracy converges as a function of the
    iterations. As an illustrative example, we show the convergence plot for ViTBNFFN
    in Figure 7.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个模型，可以绘制准确率如何随迭代次数变化的图形。作为说明示例，我们在图7中展示了ViTBNFFN的收敛图。
- en: '![](../Images/b94a544b0f3708864de1b468b20a844c.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b94a544b0f3708864de1b468b20a844c.png)'
- en: Figure 7\. Convergence plot for ViTBNFFN.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图7\. ViTBNFFN的收敛图。
- en: One can now embark on step 2 — we train and test each model with the optimized
    hyperparameters for 30 epochs. The comparison of the metrics for the models for
    30 epochs of training and testing is summarized in the set of four graphs in Figure
    8.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以进入步骤2——我们使用优化的超参数训练和测试每个模型30个epoch。30个epoch的训练和测试指标比较总结在图8的四个图表中。
- en: '![](../Images/2281d9ae6e04b7606e8e69527fd71a29.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2281d9ae6e04b7606e8e69527fd71a29.png)'
- en: Figure 8\. Comparing the metrics of the optimized models trained and tested
    for 30 epochs on the MNIST data.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图8。比较优化模型在MNIST数据集上训练和测试30个epoch的指标。
- en: At the end of 30 epochs, the models — ViT, ViTBNFFN and ViTBN — achieve 98.1%,
    97.6 % and 97.8% accuracies respectively. ViT converges in a fewer number of epochs
    compared to ViTBNFFN and ViTBN.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在30个epoch结束时，模型——ViT、ViTBNFFN和ViTBN——分别达到了98.1%、97.6%和97.8%的准确率。与ViTBNFFN和ViTBN相比，ViT的收敛所需的epoch数较少。
- en: 'From the two graphs on the top row of Figure 8, one can readily see that the
    models with BatchNorm are significantly faster in training as well as in inference
    per epoch. For ViTBNFFN, the ratios rₜ and rᵥ can be computed from the above data
    : rₜ (ViTBNFFN) = 3.9 and rᵥ(ViTBNFFN)= 2.6, while for ViTBN, we have rₜ (ViTBN)
    = 3.5 and rᵥ(ViTBN)= 2.5\. The resulting gains in average training time per epoch
    and average inference time per epoch (gₜ and gᵥ respectively) are summarized in
    Table 3.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 从图8顶部的两幅图中可以明显看出，带有BatchNorm的模型在每个epoch的训练和推理速度上都有显著提升。对于ViTBNFFN，rₜ和rᵥ的比值可以从上述数据中计算得出：rₜ（ViTBNFFN）=
    3.9，rᵥ（ViTBNFFN）= 2.6，而对于ViTBN，则是rₜ（ViTBN）= 3.5，rᵥ（ViTBN）= 2.5。表3总结了每个epoch的平均训练时间增益（gₜ）和平均推理时间增益（gᵥ）。
- en: '![](../Images/48afc50cf3ad5e140f196714a82964f2.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/48afc50cf3ad5e140f196714a82964f2.png)'
- en: Table 3\. The gain in training time per epoch and inference time per epoch for
    ViTBNFFN and ViTBN with respect to the standard ViT.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 表3。ViTBNFFN和ViTBN相对于标准ViT在每个epoch的训练时间和推理时间的增益。
- en: A Brief Summary of the Results
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果简要总结
- en: 'Let us now present a quick summary of our investigation :'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们简要总结一下我们的研究：
- en: '**Gain in Training and Testing Speed at Fixed Learning Rate:** The average
    training time per epoch speeds up significantly for both ViTBNFFN and ViTBN with
    respect to ViT. The gain gₜ is >~ 60% throughout the range of learning rates probed
    here, but may vary significantly depending on the learning rate and the model
    as evident from Table 1\. For the average testing time per epoch, there is also
    a significant gain (~60%) but this remains roughly the same over the entire range
    of learning rates for both models.'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**在固定学习率下的训练和测试速度提升：** 相较于ViT，ViTBNFFN和ViTBN的每个epoch的平均训练时间显著加快。无论在什么学习率范围内，增益gₜ都大于~60%，但根据学习率和模型的不同，这一增益可能会有显著差异，具体请见表1。对于每个epoch的平均测试时间，也有显著的提升（~60%），但对于这两个模型而言，随着学习率的变化，这一提升大致保持不变。'
- en: '**Gain in Training and Testing Speed for Optimized Models:** The gain in average
    training time per epoch is above 70% for both ViTBNFFN and ViTBN while the gain
    in the inference time is a little above 60% — the precise values for gₜ and gᵥ
    are summarized in Table 3\. The optimized ViT model converges faster than the
    models with BatchNorm.'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**优化模型的训练和测试速度提升：** 对于ViTBNFFN和ViTBN来说，每个epoch的平均训练时间的增益都超过70%，而推理时间的增益则略高于60%——gₜ和gᵥ的具体值已总结在表3中。优化后的ViT模型比带有BatchNorm的模型收敛更快。'
- en: '**BatchNorm and Higher Learning Rate :** For smallerlearning rate (~ 0.0005),
    all three models are stable with ViT converging faster compared to ViTBNFFN/ViTBN.
    For the intermediate learning rate (~ 0.005), the three models have very similar
    convergences. For higher learning rate (~ 0.01), ViT becomes unstable while the
    models ViTBNFFN/ViTBN remain stable with an accuracy comparable to the case of
    the intermediate learning rate. Our findings, therefore, confirm the general expectation
    that integrating BatchNorm in the architecture allows one to use higher learning
    rates.'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**BatchNorm与较高学习率：** 对于较小的学习率（~ 0.0005），所有三个模型都表现稳定，且ViT的收敛速度相比于ViTBNFFN/ViTBN更快。对于中等的学习率（~
    0.005），三个模型的收敛速度非常相似。对于较高的学习率（~ 0.01），ViT变得不稳定，而ViTBNFFN/ViTBN模型保持稳定，并且其准确性与中等学习率的情况相当。因此，我们的研究结果验证了这样一个普遍预期：将BatchNorm集成到架构中，可以使用更高的学习率。'
- en: '**Variation of Training Time with Learning Rate :** For ViT, there is a large
    increase in the average training time per epoch as one dials up the learning rate,
    while for ViTBNFFN this increase is much smaller. On the other hand, for ViTBN
    the training time varies the least. In other words, the training time is most
    stable with respect to variation in the learning rate for ViTBN.'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练时间与学习率的变化：** 对于ViT，当增加学习率时，每个周期的平均训练时间会大幅增加，而对于ViTBNFFN，这一增加要小得多。另一方面，对于ViTBN，训练时间变化最小。换句话说，ViTBN对学习率变化的训练时间最为稳定。'
- en: Concluding Remarks
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结语
- en: In this article, I have introduced two models which integrate BatchNorm in a
    ViT-type architecture — one of them deploys BatchNorm in the feedforward network
    (ViTBNFFN) while the other replaces LayerNorm with BatchNorm everywhere (ViTBN).
    There are two main lessons that we learn from the numerical experiments discussed
    above. Firstly, models with BatchNorm allows one to reach the same (or superior)
    level of accuracy compared to the standard ViT while using fewer number of transformer
    layers. This in turn speeds up the training time and the inference time. For the
    MNIST dataset, the training and testing times per epoch speed up by at least 60%
    in the range of learning rates I consider. Secondly, models with BatchNorm allow
    one to use a larger learning rate during training without rendering the model
    unstable.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我介绍了两种在ViT类型架构中集成BatchNorm的模型——其中一个在前馈网络中部署BatchNorm（ViTBNFFN），而另一个则在各处用BatchNorm替代LayerNorm（ViTBN）。从上述数值实验中，我们得出了两个主要结论。首先，使用BatchNorm的模型可以在使用较少的变换器层的情况下，达到与标准ViT相同（或更高）的准确性。这反过来加快了训练时间和推理时间。对于MNIST数据集，在我考虑的学习率范围内，每个周期的训练和测试时间至少加快了60%。其次，使用BatchNorm的模型允许在训练过程中使用更大的学习率，而不会使模型不稳定。
- en: Also, in this article, I have focused my attention exclusively on the standard
    ViT architecture. However, one can obviously extend the discussion to other transformer-based
    architectures for computer vision. The integration of BatchNorm in transformer
    architecture has been addressed for the DeiT (Data efficient image Transformer)
    and the Swin Transformer by Yao et al. I refer the reader to [this](https://openaccess.thecvf.com/content/ICCV2021W/NeurArch/papers/Yao_Leveraging_Batch_Normalization_for_Vision_Transformers_ICCVW_2021_paper.pdf)
    paper for details.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在本文中，我专注于标准的ViT架构。然而，显然可以将讨论扩展到其他基于变换器的计算机视觉架构。Yao等人已经研究了在DeiT（数据高效图像变换器）和Swin
    Transformer中集成BatchNorm。我建议读者参考[这篇](https://openaccess.thecvf.com/content/ICCV2021W/NeurArch/papers/Yao_Leveraging_Batch_Normalization_for_Vision_Transformers_ICCVW_2021_paper.pdf)论文以获取更多细节。
- en: Thanks for reading! If you have made it to the end of the article, please do
    not forget to leave a comment! Unless otherwise stated, all images and graphs
    used in this article were generated by the author.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！如果你已读完本文，请不要忘记留下评论！除非另有说明，本文中使用的所有图片和图表均由作者生成。
