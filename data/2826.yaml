- en: 'Cluster While Predict: Iterative Methods for Regression and Classification'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类与预测：回归和分类的迭代方法
- en: 原文：[https://towardsdatascience.com/cluster-while-predict-iterative-methods-for-regression-and-classification-ec2acff22e46?source=collection_archive---------5-----------------------#2024-11-21](https://towardsdatascience.com/cluster-while-predict-iterative-methods-for-regression-and-classification-ec2acff22e46?source=collection_archive---------5-----------------------#2024-11-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/cluster-while-predict-iterative-methods-for-regression-and-classification-ec2acff22e46?source=collection_archive---------5-----------------------#2024-11-21](https://towardsdatascience.com/cluster-while-predict-iterative-methods-for-regression-and-classification-ec2acff22e46?source=collection_archive---------5-----------------------#2024-11-21)
- en: Predictive and prescriptive analytics to bridge the gap between segmentation
    and prediction for real-world applications
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测性和处方性分析：弥合分割与预测之间的鸿沟，以实现实际应用
- en: '[](https://medium.com/@h.fellahi?source=post_page---byline--ec2acff22e46--------------------------------)[![Hussein
    Fellahi](../Images/b49c8620d8a490ab078b5d4dfe8d017a.png)](https://medium.com/@h.fellahi?source=post_page---byline--ec2acff22e46--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ec2acff22e46--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ec2acff22e46--------------------------------)
    [Hussein Fellahi](https://medium.com/@h.fellahi?source=post_page---byline--ec2acff22e46--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@h.fellahi?source=post_page---byline--ec2acff22e46--------------------------------)[![Hussein
    Fellahi](../Images/b49c8620d8a490ab078b5d4dfe8d017a.png)](https://medium.com/@h.fellahi?source=post_page---byline--ec2acff22e46--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ec2acff22e46--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ec2acff22e46--------------------------------)
    [Hussein Fellahi](https://medium.com/@h.fellahi?source=post_page---byline--ec2acff22e46--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ec2acff22e46--------------------------------)
    ·11 min read·Nov 21, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ec2acff22e46--------------------------------)
    ·11分钟阅读·2024年11月21日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/517f3c30df83ab5b1261d110797b17b3.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/517f3c30df83ab5b1261d110797b17b3.png)'
- en: Photo by [NASA Hubble Space Telescope](https://unsplash.com/@hubblespacetelescope?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [NASA 哈勃太空望远镜](https://unsplash.com/@hubblespacetelescope?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'Introduction:'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言：
- en: In many real-world machine learning tasks, the population being studied is often
    **diverse and heterogeneous**. This variability presents unique challenges, particularly
    in regression and classification tasks where a single, generalized model may fail
    to capture important **nuances** within the data. For example, segmenting customers
    in marketing campaigns, estimating the sales of a new product using data from
    comparable products, or diagnosing a patient with limited medical history based
    on similar cases all highlight the need for models that can adapt to different
    subpopulations.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多实际的机器学习任务中，研究的群体通常是**多样化且异质的**。这种变异性带来了独特的挑战，尤其是在回归和分类任务中，当一个单一的、广义的模型无法捕捉数据中的重要**细微差异**时。例如，营销活动中客户的细分、通过类似产品的数据估算新产品的销售情况，或者根据类似病例诊断一个有有限病史的患者，都凸显了需要能够适应不同子群体的模型。
- en: This concept of segmentation is not new. Models like k-Nearest Neighbors or
    Decision Trees already implicitly leverage the idea of **dividing the input space**
    into regions that share somewhat similar properties. However, these approaches
    are often heuristic and do not explicitly optimize for both clustering and prediction
    simultaneously.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分割的概念并不新鲜。像 k-最近邻（k-Nearest Neighbors）或决策树（Decision Trees）这样的模型，已经在隐性地利用**将输入空间划分**为具有某些相似属性的区域。然而，这些方法通常是启发式的，并没有显式地同时优化聚类和预测。
- en: In this article, we approach this challenge from an optimization perspective,
    following the literature on **Predictive and Prescriptive Analytics** ([8]). Specifically,
    we focus on the task of **joint clustering and prediction**, which seeks to segment
    the data into clusters while simultaneously fitting a predictive model within
    each cluster. This approach has gained attention for its ability to bridge the
    gap between data-driven decision-making and actionable insights and **extracting
    more information from data** than other traditional methods (see [2] for instance).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们从优化的角度来解决这个挑战，参考了**预测性与指示性分析**的相关文献（[8]）。具体而言，我们关注的是**联合聚类与预测**任务，旨在将数据分割成聚类，同时在每个聚类内拟合一个预测模型。这种方法因其能够弥合数据驱动决策与可操作见解之间的差距，并且**从数据中提取更多信息**，相比其他传统方法（例如见[2]）而受到关注。
- en: After presenting some theoretical insights on Clustering and Regression from
    recent literature, we introduce a novel Classification method (Cluster While Classify)
    and show its superior performance in low data environments.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在展示了一些来自最近文献的聚类与回归的理论见解之后，我们介绍了一种新的分类方法（边聚类边分类），并展示了其在低数据环境中的优越表现。
- en: 1\. Joint Clustering and Regression
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 联合聚类与回归
- en: 1.1 Original optimization problem
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 原始优化问题
- en: 'We first start with formulating the problem of optimal clustering and regression
    — jointly — to achieve the best fitting and prediction performance. Some formal
    notations and assumptions:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过联合地表述优化聚类和回归问题，来实现最佳拟合和预测性能。一些正式的符号和假设：
- en: Data has the form (X, Y), where X = (xᵢ) are the features and Y is the target.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据的形式为(X, Y)，其中X = (xᵢ)是特征，Y是目标。
- en: We assume a clustering with k clusters — k can be defined later — and introduce
    the **binary variables** zᵢⱼ equal to 1 if the *i-th* data point is assigned to
    cluster j, 0 otherwise.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们假设一个有k个聚类的聚类——k可以稍后定义——并引入**二元变量**zᵢⱼ，如果*第i个*数据点被分配到聚类j，则zᵢⱼ为1，否则为0。
- en: We assume a class of regression models (fⱼ) (e.g. linear models), parametrized
    by (θⱼ) and their loss function L. Note that each θⱼ is **specific** to regression
    model fⱼ.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们假设一类回归模型（fⱼ）（例如线性模型），由(θⱼ)参数化，并且具有其损失函数L。注意，每个θⱼ是**特定**于回归模型fⱼ的。
- en: 'As ultimately a regression problem, the goal of the task is to **find the set
    of parameters** (i.e. parameters for each regression model θⱼ as well as the additional
    cluster assignment variables zᵢⱼ) **minimizing the loss function** L:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个回归问题，任务的目标是**找到一组参数**（即每个回归模型的参数θⱼ，以及附加的聚类分配变量zᵢⱼ）**最小化损失函数**L：
- en: '![](../Images/a4c053d9b58b49ac074a12cced376ddb.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4c053d9b58b49ac074a12cced376ddb.png)'
- en: '1.2 Suboptimality of Cluster Then Regress:'
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 先聚类再回归的次优性：
- en: 'One of the most natural approaches — and used in numerous practical application
    of clustering and regression analyses — is the naive **Cluster Then Regress**
    (CTR) approach — i.e. first running clustering then run a regression model on
    the static result of this clustering. It is known to be **suboptimal**: namely,
    **error propagates** from the clustering step to the regression step, and erroneous
    assignments can have significant consequences on the performance.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 最自然的一种方法——并且在许多聚类和回归分析的实际应用中使用——是天真的**先聚类再回归**（CTR）方法——即首先进行聚类，然后在该聚类的静态结果上运行回归模型。已知这种方法是**次优的**：也就是说，**误差从聚类步骤传播到回归步骤**，而错误的分配可能对性能产生重大影响。
- en: 'We will mathematically show this suboptimality. When running a CTR approach,
    we first assign the clusters, and then fit the *k* regression models with cluster
    assignments as static. This translates to the following **nested** optimization:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数学上展示这种次优性。当运行CTR方法时，我们首先分配聚类，然后将*k*个回归模型与静态的聚类分配一起拟合。这转化为以下**嵌套**优化问题：
- en: '![](../Images/07bf88895a59b68a6412b45e427c6633.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07bf88895a59b68a6412b45e427c6633.png)'
- en: With TIC a measure of Total Intra Cluster Variance. Given that Z is included
    in ({0, 1})ⁿ, we see that the CTR approach solves a problem that is actually more
    constrained the the original one (i.e. further constraining the (zᵢⱼ) to be in
    Z rather than free in ({0, 1})ⁿ). Consequently, this yields a suboptimal result
    for the original optimization.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: TIC是总聚类内方差的度量。由于Z包含在({0, 1})ⁿ中，我们可以看出，CTR方法解决了一个比原始问题更受约束的问题（即进一步将(zᵢⱼ)约束为在Z中，而不是自由地位于({0,
    1})ⁿ中）。因此，这会导致原始优化问题的次优解。
- en: '1.3 Cluster While Regress: an approximation solution to the original optimization'
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 Cluster While Regress：原始优化问题的近似解
- en: 'Unfortunately, attempting to directly solve the original optimization presented
    in section 1.1 can be **intractable** in practice, (Mixed integer optimization
    problem, with potential non-linearity coming from the choice of regression models).
    [1] presents a fast and easy — but **approximate** — solution to **jointly learn
    the optimal cluster assignment and regression** models: doing it iteratively.
    In practice, the Cluster While Regress (CWR) is:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，尝试直接求解第1.1节中提出的原始优化问题在实践中可能是**不可解的**（混合整数优化问题，可能由于回归模型选择而产生非线性）。[1] 提出了一个快速且简单——但**近似的**——解决方案，用于**联合学习最优聚类分配和回归**模型：通过迭代进行。在实践中，Cluster
    While Regress (CWR) 是：
- en: At iteration *i*, consider cluster assignments as static and calibrate the *k*
    regression models
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在迭代 *i* 时，将聚类分配视为静态，并校准 *k* 个回归模型
- en: Then consider the regression models as static and choose the cluster assignments
    that would minimize the total loss
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后将回归模型视为静态，选择能够最小化总损失的聚类分配
- en: Redo the previous two steps until cluster assignments do not change
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重做前两步，直到聚类分配不再改变
- en: 'Besides the iterative nature of this method, it presents a key difference with
    the CTR approach: **clustering and regression optimize for the same objective
    function**.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 除了该方法的迭代特性外，它与CTR方法的一个关键区别在于：**聚类和回归优化的是同一个目标函数**。
- en: 2\. Joint Clustering and Classification
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2. 联合聚类与分类
- en: 'Applying the previous reasoning to classification, we have 2 different routes:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 将之前的推理应用于分类，我们有两条不同的路线：
- en: Rewriting a new model from scratch, i.e. Cluster While Classify
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从头开始重写一个新模型，即 Cluster While Classify
- en: Using CWR on the log odds for a Logistic Regression approach — see appendix
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 CWR 对逻辑回归方法的对数几率进行处理——详见附录
- en: '2.1 Formulating Cluster While Classify:'
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 聚类与分类的公式化：
- en: A few modifications are to be done to the objective problem, namely the loss
    function L which becomes a classification loss. For simplicity, we will focus
    on binary classification, but this formulation can easily be extended.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对目标问题做一些修改，特别是损失函数 L，变成了分类损失。为了简化起见，我们将专注于二分类，但这个公式化可以轻松扩展。
- en: 'A popular loss function when doing binary classification is the **binary cross-entropy
    loss**:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行二分类时，常用的损失函数是**二元交叉熵损失**：
- en: '![](../Images/95bf47bfabf91650beb19bcb8c7a8ed2.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/95bf47bfabf91650beb19bcb8c7a8ed2.png)'
- en: Where *p* is the **prediction** of the classification model parametrized by
    *θ* in terms of **probability** of being in the class 1.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *p* 是**分类模型的预测**，由*θ* 参数化，以**概率**表示属于类别 1 的可能性。
- en: 'Introducing the clustering into this loss yields the following optimization
    model:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 将聚类引入这个损失函数中，得到以下优化模型：
- en: '![](../Images/a34a11b7edf0f823cdf5291d785d3f41.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a34a11b7edf0f823cdf5291d785d3f41.png)'
- en: Similarly to CWR, we can find an approximate solution to this problem through
    the same algorithm, i.e. iteratively fitting the clustering and classification
    steps until convergence.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 与 CWR 类似，我们可以通过相同的算法找到该问题的近似解，即通过迭代拟合聚类和分类步骤直到收敛。
- en: '2.2\. Application for Logistic Regression:'
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2. 逻辑回归的应用：
- en: 'In this specific case, the probabilities are of the form:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种特定情况下，概率的形式为：
- en: '![](../Images/f6947cc73b2de8c241c74685fca9f3ef.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6947cc73b2de8c241c74685fca9f3ef.png)'
- en: 'Injecting this formula in the objective function of the optimization problem
    gives:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个公式代入优化问题的目标函数中，得到：
- en: '![](../Images/dc78219d95f26c1dc877f886a212c532.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc78219d95f26c1dc877f886a212c532.png)'
- en: '2.3 Model inference:'
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 模型推理：
- en: 'Inference with both CWR and CWC models can be done with the following process,
    described in details in [1]:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 CWR 和 CWC 模型的推理可以通过以下过程完成，详细描述见 [1]：
- en: '**Infer cluster assignment:** fit a multiclass classification model on the
    data points with labels being the final cluster assignments. Use this classification
    model to assign probabilities of being in a cluster.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推断聚类分配：** 在数据点上拟合一个多分类模型，将标签视为最终的聚类分配。使用这个分类模型来分配属于某个聚类的概率。'
- en: '**Prediction:** for a given data point, the probability of being in a given
    class becomes the **weighted sum of probabilities** given by each fitted model.
    This comes from the law of total probability:'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测：** 对于给定的数据点，属于某一类的概率是由每个拟合模型给出的**加权概率和**。这来自全概率法则：'
- en: '![](../Images/03cb7cd9926086cfd7713f41c6452b4f.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03cb7cd9926086cfd7713f41c6452b4f.png)'
- en: Where *P(Yᵢ = 1| Xᵢ, i ∈ Clusterⱼ)* is given by *j-th* classification model
    fitted and *P(i ∈ Clusterⱼ)* comes from the cluster assignment classifier.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *P(Yᵢ = 1| Xᵢ, i ∈ Clusterⱼ)* 由 *j-th* 分类模型给出，*P(i ∈ Clusterⱼ)* 来自集群分配分类器。
- en: 3\. Generalization to non-integer weights
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 广义化到非整数权重
- en: Generalization to non-integer weights relaxes the integer constraint on the
    z variables. This corresponds to the case of an algorithm allowing for (probabilistic)
    assignment to multiple clusters, e.g. Soft K-Means — in this case assignments
    become weights between 0 and 1.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 广义化到非整数权重放宽了 z 变量上的整数约束。这对应于允许对多个集群进行（概率）分配的算法，例如软 K-Means——在这种情况下，分配变成了介于 0
    和 1 之间的权重。
- en: 'The fitting and inference processes are very similar to previously, with the
    sole differences being during the **fitting** phase: calibrating the regression
    / classification models on each cluster is replaced with calibrated the weighted
    regressions (e.g. Weighted Least Squares) or weighted classifications (e.g. Weighted
    Logistic Regression — see [4] for an example), with weight matrices *Wⱼ = Diag(zᵢⱼ)*
    with i corresponding to all the indices such that *zᵢⱼ > 0*. Note that unlike
    methods such as Weighted Least Squares, **weights here are given** when fitting
    the regression.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合和推断过程与之前非常相似，唯一的区别是在**拟合**阶段：在每个集群上校准回归/分类模型被替换为校准加权回归（例如加权最小二乘法）或加权分类（例如加权逻辑回归——见
    [4] 以获取示例），权重矩阵 *Wⱼ = Diag(zᵢⱼ)*，其中 i 对应于所有使得 *zᵢⱼ > 0* 的索引。注意，与加权最小二乘法等方法不同，**在拟合回归时，权重是已给定的**。
- en: 'This generalization has 2 direct implications on the problem at hand:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这一广义化有两个直接影响：
- en: Being a less constrained optimization problem, it will naturally yield a better
    solution, i.e. a lower loss ***in-sample*** than the integer-constrained version
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为一个约束较少的优化问题，它自然会得出更好的解决方案，即比整数约束版本具有更低的***样本内损失***。
- en: It will be more prone to **overfitting**, therefore bringing the need for **increased
    regularization**
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它更容易**过拟合**，因此需要**增加正则化**。
- en: '[1] already included a regularization term for the regression coefficients,
    which corresponds to having regularized *fⱼ* models: in the case of a Linear Regression,
    this would means for instance that *fⱼ* is a LASSO or a Ridge rather than a simple
    OLS.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 已经包括了回归系数的正则化项，这对应于对 *fⱼ* 模型的正则化：例如在线性回归的情况下，这意味着 *fⱼ* 是 LASSO 或 Ridge，而不是简单的
    OLS。'
- en: 'Yet, the proposal here is different, as we suggest additional regularization,
    this time **penalizing the non-zero *zᵢⱼ***: the rationale is that we want to
    **limit the number of models** implicated in the fitting / inference of a given
    data point to reduce noise and degrees of freedom to prevent overfitting.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这里提出的方案有所不同，因为我们建议额外的正则化，这次是**惩罚非零的 *zᵢⱼ***：其背后的逻辑是我们希望**限制涉及拟合/推断给定数据点的模型数量**，以减少噪声和自由度，从而防止过拟合。
- en: 'In practice, we add a new set of binary variables *(bᵢⱼ)* equal to 1 if *zᵢⱼ*
    > 0 and 0 otherwise. We can write it as linear constraints using the big M method:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们添加了一组新的二元变量 *(bᵢⱼ)*，当 *zᵢⱼ* > 0 时为 1，否则为 0。我们可以使用大 M 方法将其写为线性约束：
- en: '![](../Images/b668161a41ccbea99edcc1ac428875d3.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b668161a41ccbea99edcc1ac428875d3.png)'
- en: 'All in, we have the two optimization models:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们有两个优化模型：
- en: 'Generalized Cluster While Regress:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 广义集群回归：
- en: '![](../Images/9328ee7d9f4a464fa8c214fbf383533c.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9328ee7d9f4a464fa8c214fbf383533c.png)'
- en: 'Generalized Cluster While Classify:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 广义集群分类：
- en: '![](../Images/24586d7f3ceb4241df204c8fd7e9cffb.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/24586d7f3ceb4241df204c8fd7e9cffb.png)'
- en: These problems can be efficiently solved with First Order methods or Cutting
    Planes — see [3] for details.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题可以通过一阶方法或切割平面方法高效求解——详情请见 [3]。
- en: '4\. Evaluation:'
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 评估：
- en: 'We evaluate these methods on 3 different benchmark datasets to illustrate 3
    key aspects of their behavior and performance:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在三个不同的基准数据集上评估这些方法，以说明它们在行为和性能方面的三个关键方面：
- en: Propension to **overfit**
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过拟合**的倾向。'
- en: Better performance when data is **imbalanced or asymmetric** — i.e. bigger consequences
    in cases of false positive or false negative
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据**不平衡或不对称**的情况下表现更好——即在假阳性或假阴性的情况下，后果更严重。
- en: Better performance in **low data settings**
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**低数据设置**下表现更好。
- en: 'Some implementation details:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一些实现细节：
- en: Given that all the methods presented are agnostic to the type of classification
    model used, we will assume the **same classifier across the board** to ensure
    fair comparison. For simplicity, we choose Logistic Regression with L2 regularization
    (which is the base setting in Scikit-Learn).
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于所呈现的所有方法都与使用的分类模型类型无关，我们假设**使用相同的分类器**以确保公平比较。为简化起见，我们选择带有L2正则化的逻辑回归（这是Scikit-Learn中的基础设置）。
- en: For Cluster Then Classify (CTC), we use the K-Means clustering algorithm. We
    choose the number of clusters that **maximizes the silhouette score** of the clustering.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于聚类然后分类（CTC），我们使用K-Means聚类算法。我们选择**最大化聚类轮廓系数**的聚类数量。
- en: For Cluster While Classify (CWC), we choose the **number of clusters by cross-validation**,
    i.e. the number of clusters that maximizes the AUC of the ROC curve on a validation
    dataset. We then re-fit the chosen model on both train and validation datasets.
    If the optimal number of clusters is 2, we opt for the CWC with integer weights
    for parsimony.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于聚类同时分类（CWC），我们通过交叉验证选择**聚类数量**，即选择使ROC曲线在验证数据集上的AUC最大化的聚类数量。然后我们重新拟合选定的模型，使用训练集和验证集。如果最优的聚类数是2，我们选择具有整数权重的CWC，以确保简洁性。
- en: Inference for CTC and CWC is done with using process Model Inference presented
    earlier, i.e. a **weighted sum of the probabilities** **predicted** by each sub-model.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CTC和CWC的推理是使用前面介绍的过程模型推理进行的，即**每个子模型预测的概率的加权总和**。
- en: 4.1 UCI Diabetes 130 dataset
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 UCI 糖尿病130数据集
- en: The Diabetes 130-US Hospitals dataset (1999–2008) ([5]) contains information
    about diabetes patients admitted to 130 hospitals across the United States over
    a 9-year period. The goal of the classification task is to predict whether a given
    diabetes patient will be readmitted. We will simplify the classes into 2 classes
    — readmitted or not — instead of 3 (readmitted after less than 30 days, readmitted
    after more than 30 days, not readmitted). We will also consider a subset of 20,000
    data points instead of the full 100,000 instances for faster training.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 糖尿病130-美国医院数据集（1999–2008）（[5]）包含了有关在9年期间住院的糖尿病患者的信息，这些患者来自美国130家医院。分类任务的目标是预测某个糖尿病患者是否会被重新住院。我们将类简化为2类——是否重新住院——而不是3类（在30天内重新住院，30天后重新住院，不重新住院）。为了加快训练，我们还将考虑从20,000个数据点的子集进行训练，而不是使用全部100,000个实例。
- en: '![](../Images/44879767d705b2a7552c5408ae6e9119.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44879767d705b2a7552c5408ae6e9119.png)'
- en: 4.2 UCI MAGIC Gamma Telescope dataset
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 UCI MAGIC Gamma Telescope 数据集
- en: 'The MAGIC Gamma Telescope dataset ([6]) contains data from an observatory aimed
    at classifying high-energy cosmic ray events as either gamma rays (signal) or
    hadrons (background). A specificity of this dataset is the non-symmetric nature
    of errors: given the higher cost of false positives (misclassifying hadrons as
    gamma rays), accuracy is not suitable. Instead, performance is evaluated using
    the ROC curve and AUC, with a focus on maintaining a false positive rate (FPR)
    below 20% **—** as explained in [6].'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: MAGIC Gamma Telescope 数据集（[6]）包含来自一个天文台的数据，目的是将高能宇宙射线事件分类为伽马射线（信号）或强子（背景）。该数据集的一个特点是错误的非对称性：由于假阳性（将强子误分类为伽马射线）的成本较高，因此准确率不适用。相反，性能是通过ROC曲线和AUC进行评估的，重点是将假阳性率（FPR）保持在20%以下**——**如[6]中所解释的那样。
- en: '![](../Images/fa74579212ff5edf4e12880add5beeb2.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa74579212ff5edf4e12880add5beeb2.png)'
- en: 4.3 UCI Parkinsons dataset
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 UCI Parkinson's 数据集
- en: The Parkinson’s dataset ([7]) contains data collected from voice recordings
    of 195 individuals, including both those with Parkinson’s disease and healthy
    controls. The dataset is used for classifying the presence or absence of Parkinson’s
    based on features extracted from speech signals. A key challenge of this dataset
    is the low number of datapoints, which makes generalization with traditional ML
    methods difficult. We can diagnose this generalization challenge and overfitting
    by comparing the performance numbers on train vs test sets.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Parkinson's 数据集（[7]）包含了来自195个个体的语音记录数据，包括患有帕金森病的人和健康对照者。该数据集用于基于从语音信号中提取的特征来分类帕金森病的有无。这个数据集的一个关键挑战是数据点数量较少，这使得使用传统机器学习方法进行泛化变得困难。我们可以通过比较训练集和测试集上的性能数字来诊断这种泛化挑战和过拟合问题。
- en: '![](../Images/9bd484f12a91e486e60ed616b3e4ebd7.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9bd484f12a91e486e60ed616b3e4ebd7.png)'
- en: Conclusion
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The study of baseline and joint clustering and classification approaches demonstrates
    that choice of method depends significantly on the characteristics of the data
    and the problem setting — in short, ther**e is no one-size-fits-all model.**
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对基准和联合聚类与分类方法的研究表明，方法的选择在很大程度上取决于数据和问题设置的特点——简言之，**没有一种适合所有情况的模型**。
- en: 'Our findings highlight key distinctions between the approaches studied across
    various **scenarios**:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究结果突出了在不同**场景**下研究的各种方法之间的关键区别：
- en: '**In traditional settings**, i.e. large datasets, numerous features and balanced
    outcomes, conventional machine learning models generally perform well. The added
    step of clustering offers minor benefits, but the potential for **overfitting**
    with methods like CWC may lead to worse performance on unseen data.'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**在传统设置中**，即大数据集、众多特征和平衡的结果下，传统的机器学习模型通常表现良好。加入聚类步骤带来一些微小的好处，但像CWC这样的模型可能会导致**过拟合**，从而在未见过的数据上表现更差。'
- en: '**In non-traditional settings with asymmetric error consequences**, where false
    positives or false negatives carry unequal costs, methods like CWC provide some
    advantage. By **tailoring** predictions to cluster-specific dynamics, CWC seems
    to better aligns with the loss function’s priorities.'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**在具有不对称错误后果的非传统环境中**，当假阳性或假阴性带来不平等的成本时，像CWC这样的算法提供了一些优势。通过**根据聚类特定的动态调整**预测，CWC似乎更好地与损失函数的优先级对齐。'
- en: '**In low-data environments**, the benefits of joint clustering and prediction
    become **particularly pronounced**. While traditional models and CTC approaches
    often struggle with **overfitting** due to insufficient data, CWC outperforms
    by **extracting more information** **from** **what is available**. Its iterative
    optimization framework enables better generalization and robustness in these challenging
    scenarios.'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**在数据较少的环境中**，联合聚类和预测的优势变得**尤为显著**。传统模型和CTC方法通常由于数据不足而面临**过拟合**的挑战，而CWC通过**从现有数据中提取更多信息**，表现得更好。其迭代优化框架在这些挑战性场景中能实现更好的泛化和鲁棒性。'
- en: 'Appendix:'
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录：
- en: CWR on the log odds for Logistic Regression
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CWR在逻辑回归对数几率上的应用
- en: 'Starting with the log odds of Logistic Regression in the CWR form:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 从CWR形式的逻辑回归对数几率开始：
- en: '![](../Images/55e9d9a18667c51fc46b5be59c82946d.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55e9d9a18667c51fc46b5be59c82946d.png)'
- en: 'This yields the probabilities:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生概率：
- en: '![](../Images/15576a1039382c1527bba246f50d0913.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15576a1039382c1527bba246f50d0913.png)'
- en: 'Reinjecting these expressions in the likelihood function of Logistic Regression:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些表达式重新注入到逻辑回归的似然函数中：
- en: '![](../Images/3641fe1e3a6a9d3a13c3b152176378cf.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3641fe1e3a6a9d3a13c3b152176378cf.png)'
- en: 'And the log-likelihood:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 以及对数似然：
- en: '![](../Images/a4d1c3d04881fb7a896e81fc3df88ec7.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4d1c3d04881fb7a896e81fc3df88ec7.png)'
- en: This yields the same objective function as CWC when constraining the *zᵢⱼ* to
    be binary variables.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生与CWC相同的目标函数，前提是将*zᵢⱼ*限制为二进制变量。
- en: 'References:'
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献：
- en: '[1] L. Baardman, I. Levin, G. Perakis, D. Singhvi, [Leveraging Comparables
    for New Product Sales Forecasting](https://onlinelibrary.wiley.com/doi/10.1111/poms.12963)
    (2018), Wiley'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] L. Baardman, I. Levin, G. Perakis, D. Singhvi, [利用可比数据进行新产品销售预测](https://onlinelibrary.wiley.com/doi/10.1111/poms.12963)（2018），Wiley'
- en: '[2] L. Baardman, R. Cristian, G. Perakis, D. Singhvi, O. Skali Lami, L. Thayaparan,
    [The role of optimization in some recent advances in data-driven decision-making](https://link.springer.com/article/10.1007/s10107-022-01874-9)
    (2023), Springer Nature'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] L. Baardman, R. Cristian, G. Perakis, D. Singhvi, O. Skali Lami, L. Thayaparan,
    [优化在一些数据驱动决策进展中的作用](https://link.springer.com/article/10.1007/s10107-022-01874-9)（2023），Springer
    Nature'
- en: '[3] D. Bertsimas, J. Dunn, [Machine Learning Under a Modern Optimization Lens](https://www.dynamic-ideas.com/books/machine-learning-under-a-modern-optimization-lens)
    (2021), Dynamic Ideas'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] D. Bertsimas, J. Dunn, [现代优化视角下的机器学习](https://www.dynamic-ideas.com/books/machine-learning-under-a-modern-optimization-lens)（2021），Dynamic
    Ideas'
- en: '[4] G. Zeng, [A comprehensive study of coefficient signs in weighted logistic
    regression](https://www.sciencedirect.com/science/article/pii/S2405844024110717)
    (2024), Helyion'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] G. Zeng, [加权逻辑回归中系数符号的全面研究](https://www.sciencedirect.com/science/article/pii/S2405844024110717)（2024），Helyion'
- en: '[5] J. Clore, K. Cios, J. DeShazo, B. Strack, [Diabetes 130-US Hospitals for
    Years 1999–2008 [Dataset]](https://archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008)
    (2014), UCI Machine Learning Repository (CC BY 4.0)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] J. Clore, K. Cios, J. DeShazo, B. Strack, [糖尿病130个美国医院1999–2008年[数据集]](https://archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008)（2014），UCI机器学习库（CC
    BY 4.0）'
- en: '[6] R. Bock, [MAGIC Gamma Telescope [Dataset]](https://doi.org/10.24432/C52C8B)
    (2004), UCI Machine Learning Repository (CC BY 4.0)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] R. Bock, [MAGIC伽马射线望远镜数据集](https://doi.org/10.24432/C52C8B) (2004), UCI机器学习库
    (CC BY 4.0)'
- en: '[7] M. Little, [Parkinsons [Dataset]](https://doi.org/10.24432/C59C74) (2007).
    UCI Machine Learning Repository (CC BY 4.0)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] M. Little, [帕金森病数据集](https://doi.org/10.24432/C59C74) (2007). UCI机器学习库
    (CC BY 4.0)'
- en: '[8] D. Bertsimas, N. Kallus, [From Predictive to Prescriptive Analytics](https://pubsonline.informs.org/doi/10.1287/mnsc.2018.3253)
    (2019), INFORMS'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] D. Bertsimas, N. Kallus, [从预测分析到规范性分析](https://pubsonline.informs.org/doi/10.1287/mnsc.2018.3253)
    (2019), INFORMS'
