- en: Editing Text in Images with AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用AI编辑图片中的文字
- en: 原文：[https://towardsdatascience.com/editing-text-in-images-with-ai-03dee75d8b9c?source=collection_archive---------5-----------------------#2024-02-18](https://towardsdatascience.com/editing-text-in-images-with-ai-03dee75d8b9c?source=collection_archive---------5-----------------------#2024-02-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/editing-text-in-images-with-ai-03dee75d8b9c?source=collection_archive---------5-----------------------#2024-02-18](https://towardsdatascience.com/editing-text-in-images-with-ai-03dee75d8b9c?source=collection_archive---------5-----------------------#2024-02-18)
- en: 'Research Review for Scene Text Editing: STEFANN, SRNet, TextDiffuser, AnyText
    and more.'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 场景文本编辑的研究综述：STEFANN、SRNet、TextDiffuser、AnyText等。
- en: '[](https://medium.com/@turc.raluca?source=post_page---byline--03dee75d8b9c--------------------------------)[![Julia
    Turc](../Images/1ca27d7db36799dec53b8daf4099f5cb.png)](https://medium.com/@turc.raluca?source=post_page---byline--03dee75d8b9c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--03dee75d8b9c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--03dee75d8b9c--------------------------------)
    [Julia Turc](https://medium.com/@turc.raluca?source=post_page---byline--03dee75d8b9c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@turc.raluca?source=post_page---byline--03dee75d8b9c--------------------------------)[![Julia
    Turc](../Images/1ca27d7db36799dec53b8daf4099f5cb.png)](https://medium.com/@turc.raluca?source=post_page---byline--03dee75d8b9c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--03dee75d8b9c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--03dee75d8b9c--------------------------------)
    [Julia Turc](https://medium.com/@turc.raluca?source=post_page---byline--03dee75d8b9c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--03dee75d8b9c--------------------------------)
    ·13 min read·Feb 18, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--03dee75d8b9c--------------------------------)
    ·13分钟阅读·2024年2月18日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: If you ever tried to change the text in an image, you know it’s not trivial.
    Preserving the background, textures, and shadows takes a Photoshop license and
    hard-earned designer skills. In the video below, a Photoshop expert takes 13 minutes
    to fix a few misspelled characters in a poster that is not even stylistically
    complex. The good news is — in our relentless pursuit of AGI, humanity is also
    building AI models that are actually useful in real life. Like the ones that allow
    us to edit text in images with minimal effort.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾尝试过更改图片中的文字，你就会知道这并不简单。保持背景、纹理和阴影需要一份Photoshop许可证和辛苦获得的设计师技能。在下面的视频中，一位Photoshop专家花了13分钟时间修正海报中几个拼写错误，而这张海报的设计风格也并不复杂。好消息是——在人类不断追求AGI的过程中，我们也在构建一些实际生活中有用的AI模型。例如那些让我们能够以最小的努力编辑图片中文字的模型。
- en: Photoshop expert manually editing an AI-generated image to correctly spell “The
    Midnight City”, taking them more than 13 minutes.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Photoshop专家手动编辑一张AI生成的图片，将“午夜之城”正确拼写出来，花费了超过13分钟的时间。
- en: The task of automatically updating the text in an image is formally known as
    *Scene Text Editing (STE)*. This article describes how STE model architectures
    have evolved over time and the capabilities they have unlocked. We will also talk
    about their limitations and the work that remains to be done. Prior familiarity
    with [GANs](https://arxiv.org/abs/1406.2661) and [Diffusion models](https://jalammar.github.io/illustrated-stable-diffusion/)
    will be helpful, but not strictly necessary.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 自动更新图片中文字的任务正式被称为*场景文本编辑（STE）*。本文将描述STE模型架构是如何随着时间的发展而演变的，以及它们所解锁的能力。我们还将讨论它们的局限性和仍需完成的工作。对[GANs](https://arxiv.org/abs/1406.2661)和[扩散模型](https://jalammar.github.io/illustrated-stable-diffusion/)的先前了解会有所帮助，但不是严格必要的。
- en: '*Disclaimer: I am the cofounder of* [*Storia AI*](https://storia.ai/?utm_source=article&utm_medium=medium.com&utm_campaign=editing-text-in-images-with-ai)*,
    building an AI copilot for visual editing. This literature review was done as
    part of developing* [*Textify*](https://storia.ai/textify?utm_source=article&utm_medium=medium.com&utm_campaign=editing-text-in-images-with-ai)*,
    a feature that allows users to seamlessly change text in images. While Textify
    is closed-source, we open-sourced a related library,* [*Detextify*](https://github.com/iuliaturc/detextify)*,
    which automatically removes text from a corpus of images.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*免责声明：我是* [*Storia AI*](https://storia.ai/?utm_source=article&utm_medium=medium.com&utm_campaign=editing-text-in-images-with-ai)*的共同创始人，正在构建一个用于视觉编辑的AI助手。此文献综述是开发*
    [*Textify*](https://storia.ai/textify?utm_source=article&utm_medium=medium.com&utm_campaign=editing-text-in-images-with-ai)*功能的一部分，该功能允许用户无缝地修改图像中的文本。虽然Textify是闭源的，但我们开源了一个相关的库，*
    [*Detextify*](https://github.com/iuliaturc/detextify)*，它可以自动从图像集合中移除文本。*'
- en: '![](../Images/5de7cdf4ca0f5f49118fc1da30bd6b14.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5de7cdf4ca0f5f49118fc1da30bd6b14.png)'
- en: Example of Scene Text Editing (STE). The original image (left) was generated
    via [Midjourney](https://midjourney.com). We used [Textify](https://storia.ai/textify?utm_source=article&utm_medium=medium.com&utm_campaign=editing-text-in-images-with-ai)
    to annotate the image (center) and automatically fix the misspelling (right).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 场景文本编辑（STE）示例。原始图像（左）是通过[Midjourney](https://midjourney.com)生成的。我们使用[Textify](https://storia.ai/textify?utm_source=article&utm_medium=medium.com&utm_campaign=editing-text-in-images-with-ai)对图像进行了注释（中），并自动修正了拼写错误（右）。
- en: The Task of Scene Text Editing (STE)
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 场景文本编辑（STE）任务
- en: Definition
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义
- en: Scene Text Editing (STE) is the task of automatically modifying text in images
    that capture a visual scene (as opposed to images that mainly contain text, such
    as scanned documents). The goal is to change the text while preserving the original
    aesthetics (typography, calligraphy, background etc.) without the inevitably expensive
    human labor.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 场景文本编辑（STE）是指自动修改捕捉到视觉场景的图像中的文本（与主要包含文本的图像，如扫描文档不同）。目标是在不需要昂贵人工劳动的情况下，改变文本内容，同时保持原始的美学效果（如排版、书法、背景等）。
- en: Use Cases
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用案例
- en: 'Scene Text Editing might seem like a contrived task, but it actually has multiple
    practical uses cases:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 场景文本编辑可能看起来像是一个人为的任务，但实际上它有多个实际应用场景：
- en: '**(1) Synthetic data generation for Scene Text Recognition (STR)**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**(1) 场景文本识别（STR）的合成数据生成**'
- en: '![](../Images/a498baa12f1714aff00ab77313720739.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a498baa12f1714aff00ab77313720739.png)'
- en: Synthetic image (right) obtained by editing text in the original image (left,
    from [Unsplash](https://unsplash.com/photos/no-trespassing-bn-rr-sign-i5O8-90L2P8)).
    This technique can be used to augment the training set of STR (Scene Text Recognition)
    models.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 通过编辑原始图像（左，来自[Unsplash](https://unsplash.com/photos/no-trespassing-bn-rr-sign-i5O8-90L2P8)）中的文本获得的合成图像（右）。该技术可用于增强场景文本识别（STR）模型的训练集。
- en: When I started researching this task, I was surprised to discover that [Alibaba](https://www.alibaba.com/)
    (an e-commerce platform) and [Baidu](https://www.baidu.com/) (a search engine)
    are consistently publishing research on STE.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当我开始研究这个任务时，我惊讶地发现[阿里巴巴](https://www.alibaba.com/)（一个电子商务平台）和[百度](https://www.baidu.com/)（一个搜索引擎）一直在持续发布关于场景文本编辑（STE）的研究。
- en: At least in Alibaba’s case, it is likely their research is in support of [AMAP](https://www.alibabagroup.com/en-US/about-alibaba-businesses-1496655358913937408),
    their alternative to Google Maps [[source](https://www.alibabacloud.com/blog/evolution-of-text-recognition-in-amap-data-production_596817)].
    In order to map the world, you need a robust text recognition system that can
    read traffic and street signs in a variety of fonts, under various real-world
    conditions like occlusions or geometric distortions, potentially in multiple languages.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 至少在阿里巴巴的案例中，他们的研究可能是为了支持[AMAP](https://www.alibabagroup.com/en-US/about-alibaba-businesses-1496655358913937408)，这是他们的谷歌地图替代品[[source](https://www.alibabacloud.com/blog/evolution-of-text-recognition-in-amap-data-production_596817)]。为了绘制世界地图，你需要一个强大的文本识别系统，它能够在各种字体下读取交通和街道标志，并能在现实世界的各种条件下，如遮挡或几何失真，甚至多语言环境中识别。
- en: In order to build a training set for Scene Text Recognition, one could collect
    real-world data and have it annotated by humans. But this approach is bottlenecked
    by human labor, and might not guarantee enough data variety. Instead, synthetic
    data generation provides a virtually unlimited source of diverse data, with automatic
    labels.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建一个场景文本识别的训练集，可以收集现实世界的数据并由人工进行标注。但这种方法受到人工劳动的瓶颈，且可能无法保证数据的多样性。相反，合成数据生成提供了几乎无限的多样化数据源，并带有自动标签。
- en: '**(2) Control over AI-generated images**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**(2) 对AI生成图像的控制**'
- en: '![](../Images/c08c09ea6d644cc87485b3a9578120d9.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c08c09ea6d644cc87485b3a9578120d9.png)'
- en: AI-generated image via Midjourney (left) and corrected via Scene Text Editing.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 通过Midjourney生成的AI图像（左）并通过场景文本编辑进行修正。
- en: AI image generators like [Midjourney](http://midjourney.com), [Stability](http://stability.ai)
    and [Leonardo](http://leonardo.ai) have democratized visual asset creation. Small
    business owners and social media marketers can now create images without the help
    of an artist or a designer by simply typing a text prompt. However, the text-to-image
    paradigm lacks the controllability needed for practical assets that go beyond
    concept art — event posters, advertisements, or social media posts.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于[Midjourney](http://midjourney.com)、[Stability](http://stability.ai)和[Leonardo](http://leonardo.ai)的AI图像生成器已经使视觉资产的创建民主化。小企业主和社交媒体营销人员现在可以通过简单地输入文本提示来创建图像，而无需艺术家或设计师的帮助。然而，文本到图像的模式缺乏在概念艺术之外的实际资产所需的可控性——如活动海报、广告或社交媒体帖子。
- en: Such assets often need to include textual information (a date and time, contact
    details, or the name of the company). Spelling correctly has been historically
    difficult for text-to-image models, though there has been recent process — [DeepFloyd
    IF](https://github.com/deep-floyd/IF?tab=readme-ov-file), [Midjourney v6](https://www.reddit.com/r/midjourney/comments/18p1jwp/midjourney_v6_can_now_do_text/).
    But even when these models do eventually learn to spell perfectly, the UX constraints
    of the text-to-image interface remain. It is tedious to describe in words where
    and how to place a piece of text.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这些资产通常需要包含文本信息（如日期和时间、联系方式或公司名称）。拼写正确一直是文本到图像模型的一个难点，尽管最近有了进展——[DeepFloyd IF](https://github.com/deep-floyd/IF?tab=readme-ov-file)、[Midjourney
    v6](https://www.reddit.com/r/midjourney/comments/18p1jwp/midjourney_v6_can_now_do_text/)。但即使这些模型最终学会了完美拼写，文本到图像界面的用户体验约束依然存在。用文字描述文本放置的位置和方式仍然很繁琐。
- en: '**(3) Automatic localization of visual media**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**(3) 视觉媒体的自动本地化**'
- en: Movies and games are often localized for various geographies. Sometimes this
    might entail [switching a broccoli for a green pepper](https://www.slashfilm.com/727914/why-inside-out-changed-so-many-scenes-for-its-overseas-release/),
    but most times it requires translating the text that is visible on screen. With
    other aspects of the film and gaming industries getting automated (like [dubbing
    and lip sync](https://flawlessai.com)), there is no reason for visual text editing
    to remain manual.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 电影和游戏常常需要进行不同地区的本地化。这有时可能意味着[将西兰花换成青椒](https://www.slashfilm.com/727914/why-inside-out-changed-so-many-scenes-for-its-overseas-release/)，但大多数时候则需要翻译屏幕上可见的文本。随着电影和游戏行业的其他方面开始实现自动化（如[配音和口型同步](https://flawlessai.com)），没有理由让视觉文本编辑仍然保持手动操作。
- en: 'Timeline of Architectures: from GANs to Diffusion'
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构时间线：从GAN到扩散模型
- en: The training techniques and model architectures used for Scene Text Editing
    largely follow the trends of the larger task of image generation.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 用于场景文本编辑的训练技术和模型架构在很大程度上跟随了图像生成这一更大任务的趋势。
- en: The GAN Era (2019–2021)
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GAN时代（2019–2021）
- en: '[GANs](https://arxiv.org/abs/1406.2661) (Generative Adversarial Networks) dominated
    the mid-2010s for image generation tasks. GAN refers to a particular training
    framework (rather than prescribing a model architecture) that is adversarial in
    nature. A *generator* model is trained to capture the data distribution (and thus
    has the capability to *generate* new data), while a *discriminator* is trained
    to distinguish the output of the generator from real data. The training process
    is finalized when the discriminator’s guess is as good as a random coin toss.
    During inference, the discriminator is discarded.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[GANs](https://arxiv.org/abs/1406.2661)（生成对抗网络）在2010年代中期主导了图像生成任务。GAN指的是一种特定的训练框架（而不是规定模型架构），其本质上是对抗性的。一个*生成器*模型被训练来捕捉数据分布（从而具备*生成*新数据的能力），而一个*判别器*则被训练来区分生成器输出与真实数据。在训练过程中，当判别器的猜测接近于随机抛硬币时，训练过程即完成。在推理阶段，判别器被丢弃。'
- en: GANs are particularly suited for image generation because they can perform unsupervised
    learning — that is, learn the data distribution without requiring labeled data.
    Following the general trend of image generation, the initial Scene Text Editing
    models also leveraged GANs.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: GAN特别适合用于图像生成，因为它们可以执行无监督学习——也就是说，学习数据分布而无需标记数据。沿着图像生成的总体趋势，最初的场景文本编辑模型也采用了GAN。
- en: 'GAN Epoch #1: Character-Level Editing — STEFANN'
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GAN第1轮：字符级编辑——STEFANN
- en: '[STEFANN](https://arxiv.org/pdf/1903.01192.pdf), recognized as the first work
    to modify text in scene images, operates at a character level. The character editing
    problem is broken into two: font adaptation and color adaptation.'
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[STEFANN](https://arxiv.org/pdf/1903.01192.pdf)，被认为是首个修改场景图像中文本的工作，操作的是字符级别。字符编辑问题被分为两部分：字体适应和颜色适应。'
- en: '![](../Images/4f250c074811c2ea4cc9d4ce62a04875.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f250c074811c2ea4cc9d4ce62a04875.png)'
- en: 'The [**STEFANN**](https://arxiv.org/pdf/1903.01192.pdf) model architecture
    ([source](https://prasunroy.github.io/stefann/)). The character editing task is
    broken into two: FANnet (Font Adaptation Network) generates a black-and-white
    target character in the desired shape, and Colornet fills in the appropriate color.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[**STEFANN**](https://arxiv.org/pdf/1903.01192.pdf)模型架构（[来源](https://prasunroy.github.io/stefann/)）。字符编辑任务被分为两部分：FANnet（字体适应网络）生成所需形状的黑白目标字符，Colornet则填充合适的颜色。'
- en: '[STEFANN](https://arxiv.org/pdf/1903.01192.pdf) is recognized as the first
    work to modify text in scene images. It builds on prior work in the space of *font
    synthesis* (the task of creating new fonts or text styles that closely resemble
    the ones observed in input data), and adds the constraint that the output needs
    to blend seamlessly back into the original image. Compared to previous work, STEFANN
    takes a pure machine learning approach (as opposed to e.g. explicit geometrical
    modeling) and does not depend on character recognition to label the source character.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[STEFANN](https://arxiv.org/pdf/1903.01192.pdf)被认为是首个修改场景图像中文本的工作。它基于先前的*字体合成*（即创建与输入数据中观察到的字体或文本风格相似的新字体或文本样式）工作，并增加了一个约束，即输出需要无缝地融合回原始图像。与之前的工作相比，STEFANN采用纯机器学习方法（而非例如显式的几何建模），且不依赖于字符识别来标注源字符。'
- en: The STEFANN model architecture is based on [CNN](https://en.wikipedia.org/wiki/Convolutional_neural_network)s
    (Convolutional Neural Networks) and decomposes the problem into (1) *font adaptation*
    via FANnet — turning a binarized version of the source character into a binarized
    target character, (2) *color adaptation* via Colornet — colorizing the output
    of FANnet to match the rest of the text in the image, and (3) *character placement*
    — blending the target character back into the original image using previously-established
    techniques like [inpainting](https://www.olivier-augereau.com/docs/2004JGraphToolsTelea.pdf)
    and [seam carving](https://perso.crans.org/frenoy/matlab2012/seamcarving.pdf).
    The first two modules are trained with a GAN objective.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: STEFANN模型架构基于[CNN](https://en.wikipedia.org/wiki/Convolutional_neural_network)（卷积神经网络），并将问题分解为（1）通过FANnet进行*字体适应*——将源字符的二值化版本转换为目标字符的二值化版本，（2）通过Colornet进行*颜色适应*——将FANnet的输出着色，以匹配图像中其余文本的颜色，以及（3）*字符放置*——使用已建立的技术，如[修补](https://www.olivier-augereau.com/docs/2004JGraphToolsTelea.pdf)和[接缝雕刻](https://perso.crans.org/frenoy/matlab2012/seamcarving.pdf)，将目标字符融合回原始图像。前两个模块是通过GAN目标训练的。
- en: Official STEFANN demo made by its authors.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 官方STEFANN演示由其作者制作。
- en: While STEFANN paved the way for Scene Text Editing, it has multiple limitations
    that restrict its use in practice. It can only operate on one character at a time;
    changing an entire word requires multiple calls (one per letter) and constrains
    the target word to have the same length as the source word. Also, the character
    placement algorithm in step (3) assumes that the characters are non-overlapping.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管STEFANN为场景文本编辑开辟了道路，但它在实际应用中存在多项限制。它一次只能操作一个字符；修改整个单词需要多次调用（每个字母一次），并且要求目标单词与源单词具有相同的长度。此外，步骤（3）中的字符放置算法假设字符之间不重叠。
- en: 'GAN Epoch #2: Word-Level Editing — SRNet and 3-Module Networks'
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GAN第2轮：词级编辑——SRNet和3模块网络
- en: '[**SRNet**](https://arxiv.org/abs/1908.03047) was the first model to perform
    scene text editing **at the word level**. SRNet decomposed the STE task into three
    (jointly-trained) modules: text conversion, background inpainting and fusion.'
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[**SRNet**](https://arxiv.org/abs/1908.03047)是首个在**词级别**进行场景文本编辑的模型。SRNet将STE任务分解为三个（联合训练的）模块：文本转换、背景修复和融合。'
- en: '![](../Images/1d9862f8ca14adb12030871024577166.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d9862f8ca14adb12030871024577166.png)'
- en: The [**SRNet**](https://arxiv.org/abs/1908.03047) model architecture. The three
    modules decompose the STE problem into smaller building blocks (text conversion,
    background inpainting and fusion), while being jointly trained. This architecture
    was largely adopted by follow-up work in the field.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[**SRNet**](https://arxiv.org/abs/1908.03047)模型架构。三个模块将STE问题分解成更小的构建块（文本转换、背景修复和融合），并在联合训练的过程中进行优化。这个架构被该领域的后续工作广泛采用。'
- en: '[SRNet](https://arxiv.org/abs/1908.03047) was the first model to perform scene
    text editing at the word level. SRNet decomposed the STE task into three (jointly-trained)
    modules:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[SRNet](https://arxiv.org/abs/1908.03047)是第一个在词级别上执行场景文本编辑的模型。SRNet将STE任务分解为三个（联合训练的）模块：'
- en: '**The text conversion module** (in blue) takes a programatic rendering of the
    target text (“barbarous” in the figure above) and aims to render it in the same
    typeface as the input word (“introduce”) on a plain background.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**文本转换模块**（蓝色部分）接收目标文本的程序化渲染（如上图中的“barbarous”），并旨在将其以与输入单词（“introduce”）相同的字体在纯背景上渲染出来。'
- en: '**The background inpainting module** (in green) erases the text from the input
    image and fills in the gaps to reconstruct the original background.'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**背景修复模块**（绿色部分）从输入图像中去除文本，并填补空白，重建原始背景。'
- en: '**The fusion module** (in orange) pastes the rendered target text onto the
    background.'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**融合模块**（橙色部分）将渲染的目标文本粘贴到背景上。'
- en: '**SRNet architecture.** All three modules are flavors of Fully Convolutional
    Networks (FCNs), with the background inpainting module in particular resembling
    [U-Net](https://arxiv.org/abs/1505.04597) (an FCN with the specific property that
    encoder layers are skip-connected to decoder layers of the same size).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**SRNet架构。** 三个模块都是全卷积网络（FCNs）的变体，其中背景修复模块特别类似于[U-Net](https://arxiv.org/abs/1505.04597)（一种具有特定属性的FCN，编码器层与解码器层之间有跳跃连接，且大小相同）。'
- en: '**SRNet training.** Each module has its own loss, and the network is jointly
    trained on the sum of losses (*LT + LB + LF*), where the latter two are trained
    via GAN. While this modularization is conceptually elegant, it comes with the
    drawback of requiring paired training data, with supervision for each intermediate
    step. Realistically, this can only be achieved with artificial data. For each
    data point, one chooses a random image (from a dataset like [COCO](https://cocodataset.org/#home)),
    selects two arbitrary words from a dictionary, and renders them with an arbitrary
    typeface to simulate the “before” and “after” images. As a consequence, the training
    set doesn’t include any photorealistic examples (though it can somewhat generalize
    beyond rendered fonts).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**SRNet训练。** 每个模块都有自己的损失函数，网络是通过损失总和(*LT + LB + LF*)进行联合训练的，其中后两个损失通过GAN进行训练。虽然这种模块化概念上非常优雅，但也带来了需要配对训练数据的缺点，每个中间步骤都需要监督。实际上，这只能通过人工数据来实现。对于每个数据点，随机选择一张图像（来自像[COCO](https://cocodataset.org/#home)这样的数据集），从字典中选择两个任意单词，并用任意字体渲染它们，以模拟“前后”图像。因此，训练集不包含任何照片级真实的示例（尽管它可以在一定程度上超越渲染字体进行泛化）。'
- en: '**Honorable mentions.** [SwapText](https://arxiv.org/abs/2003.08152) followed
    the same GAN-based 3-module network approach to Scene Text Editing and proposed
    improvements to the text conversion module.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**荣誉提及。** [SwapText](https://arxiv.org/abs/2003.08152)采用了相同的基于GAN的三模块网络方法进行场景文本编辑，并对文本转换模块提出了改进。'
- en: 'GAN Epoch #3: Self-supervised and Hybrid Networks'
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'GAN Epoch #3: 自监督和混合网络'
- en: '**Leap to unsupervised learning.** The next leap in STE research was to adopt
    a self-supervised training approach, where models are trained on unpaired data
    (i.e., a mere repository of images containing text). To achieve this, one had
    to remove the label-dependent intermediate losses LT and LB. And due to the design
    of GANs, the remaining final loss does not require a label either; the model is
    simply trained on the *discriminator*’s ability to distinguish between real images
    and the ones produced by the *generator*. [TextStyleBrush](https://arxiv.org/abs/2106.08385)
    pioneered self-supervised training for STE, while [RewriteNet](https://arxiv.org/pdf/2107.11041.pdf)
    and [MOSTEL](https://arxiv.org/abs/2212.01982) made the best of both worlds by
    training in two stages: one supervised (advantage: abundance of synthetic labeled
    data) and one self-supervised (advantage: realism of natural unlabeled data).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**跃迁到自监督学习。** STE 研究的下一个跃迁是采用自监督训练方法，在这种方法中，模型在未配对的数据上进行训练（即仅包含文本的图像库）。为了实现这一点，需要去除依赖标签的中间损失
    LT 和 LB。由于 GANs 的设计，剩下的最终损失也不需要标签；模型只是根据*鉴别器*区分真实图像和由*生成器*产生的图像的能力进行训练。[TextStyleBrush](https://arxiv.org/abs/2106.08385)
    在 STE 的自监督训练方面开创了先河，而 [RewriteNet](https://arxiv.org/pdf/2107.11041.pdf) 和 [MOSTEL](https://arxiv.org/abs/2212.01982)
    通过两阶段训练充分发挥了两者的优势：一阶段为监督学习（优势：合成标签数据的丰富性），另一阶段为自监督学习（优势：自然无标签数据的真实性）。'
- en: '**Disentangling text content & style.** To remove the intermediate losses,
    [TextStyleBrush](https://arxiv.org/abs/2106.08385) and [RewriteNet](https://arxiv.org/pdf/2107.11041.pdf)
    reframe the problem into disentangling *text content* from *text style.* To reiterate,
    the inputs to an STE system are (a) an image with original text, and (b) the desired
    text — more specifically, a programatic rendering of the desired text on a white
    or gray background, with a fixed font like Arial. The goal is to combine the *style*
    from (a) with the *content* from (b). In other words, we complementarily aim to
    discard the *content* from (a) and the *style* of (b). This is why it’s necessary
    to disentangle the text *content* from the *style* in a given image.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**解耦文本内容与风格。** 为了去除中间损失，[TextStyleBrush](https://arxiv.org/abs/2106.08385)
    和 [RewriteNet](https://arxiv.org/pdf/2107.11041.pdf) 将问题重新定义为解耦*文本内容*与*文本风格*。再强调一次，STE
    系统的输入包括（a）包含原始文本的图像和（b）所需的文本——更具体地说，是在白色或灰色背景上使用固定字体（如 Arial）渲染的所需文本。目标是将（a）中的*风格*与（b）中的*内容*结合起来。换句话说，我们互补地旨在丢弃（a）中的*内容*和（b）中的*风格*。这就是为什么在给定图像中需要将文本*内容*与*风格*解耦的原因。'
- en: '![](../Images/fd703fb961b78b3b5369e2ed6cb4e196.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd703fb961b78b3b5369e2ed6cb4e196.png)'
- en: Inference architecture of [**RewriteNet**](https://arxiv.org/pdf/2107.11041.pdf).
    The encoder E disentangles text style (circle) from text content (triangle). The
    style embedding from the original image and content embedding from the text rendering
    are fed into a generator, which fuses the two into an output image.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[**RewriteNet**](https://arxiv.org/pdf/2107.11041.pdf) 的推理架构。编码器 E 将文本风格（圆形）和文本内容（三角形）解耦。来自原始图像的风格嵌入和来自文本渲染的内容嵌入被送入生成器，生成器将二者融合成输出图像。'
- en: '**TextStyleBrush and why GANs went out of fashion**. While the idea of disentangling
    text content from style is straightforward, achieving it in practice required
    complicated architectures. [TextStyleBrush](https://arxiv.org/abs/2106.08385),
    the most prominent paper in this category, used no less than *seven* jointly-trained
    subnetworks, a pre-trained typeface classifier, a pre-trained OCR model and multiple
    losses. Designing such a system must have been expensive, since all of these components
    require ablation studies to determine their effect. This, coupled with the fact
    that GANs are [notoriously difficult to train](https://machinelearningmastery.com/how-to-train-stable-generative-adversarial-networks/)
    (in theory, the generator and discriminator need to reach Nash equilibrium), made
    STE researchers eager to switch to diffusion models once they proved so apt for
    image generation.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**TextStyleBrush 以及为什么 GANs 逐渐不再流行**。虽然将文本内容与风格解耦的想法很简单，但在实践中实现这一点需要复杂的架构。[TextStyleBrush](https://arxiv.org/abs/2106.08385)，该领域最著名的论文，使用了不少于*七个*联合训练的子网络，一个预训练的字体分类器，一个预训练的
    OCR 模型和多个损失函数。设计这样的系统一定非常昂贵，因为所有这些组件都需要进行消融研究来确定它们的效果。再加上 GANs [众所周知难以训练](https://machinelearningmastery.com/how-to-train-stable-generative-adversarial-networks/)（理论上，生成器和鉴别器需要达到纳什均衡），这使得
    STE 研究人员在扩散模型证明非常适合图像生成后，迫不及待地希望转向扩散模型。'
- en: The Diffusion Era (2022 — present)
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩散时代（2022 — 至今）
- en: At the beginning of 2022, the image generation world shifted away from GANs
    towards [Latent Diffusion Models](https://arxiv.org/abs/2112.10752) (LDM). A comprehensive
    explanation of LDMs is out of scope here, but you can refer to [The Illustrated
    Stable Diffusion](https://jalammar.github.io/illustrated-stable-diffusion/) for
    an excellent tutorial. Here I will focus on the parts of the LDM architecture
    that are most relevant to the Scene Text Editing task.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 2022年初，图像生成领域从GANs转向了[潜在扩散模型](https://arxiv.org/abs/2112.10752)（LDM）。这里不涉及LDM的详细解释，但你可以参考[The
    Illustrated Stable Diffusion](https://jalammar.github.io/illustrated-stable-diffusion/)进行出色的教程。在这里，我将重点介绍与场景文本编辑任务最相关的LDM架构部分。
- en: '![](../Images/0d1c9a8a512ff1ac45126a9d4427c3a9.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0d1c9a8a512ff1ac45126a9d4427c3a9.png)'
- en: Diffusion-based Scene Text Editing. In addition to the text embedding passed
    to the actual diffusion module in a standard text-to-image-model, STE architectures
    also create embeddings that reflect desired properties of the target text (position,
    shape, style etc.). Illustration by the author.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 基于扩散的场景文本编辑。除了在标准文本到图像模型中传递给实际扩散模块的文本嵌入，STE架构还创建反映目标文本（位置、形状、样式等）所需属性的嵌入。作者插图。
- en: 'As illustrated above, an LDM-based text-to-image model has three main components:
    (1) a text encoder — typically [CLIP](http://CLIP), (2) the actual diffusion module
    — which converts the text embedding into an image embedding in latent space, and
    (3) an image decoder — which upscales the latent image into a fully-sized image.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，基于LDM的文本生成图像模型有三个主要组件：(1) 一个文本编码器——通常是[CLIP](http://CLIP)，(2) 实际的扩散模块——将文本嵌入转换为潜在空间中的图像嵌入，(3)
    一个图像解码器——将潜在图像放大为完整尺寸的图像。
- en: Scene Text Editing as a Diffusion Inpainting Task
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 场景文本编辑作为扩散图像修补任务
- en: Text-to-image is not the only paradigm supported by diffusion models. After
    all, [CLIP](http://CLIP) is equally a text *and* image encoder, so the embedding
    passed to the *image information creator* module can also encode an image. In
    fact, it can encode any modality, or a concatenation of multiple inputs.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 文本到图像并不是扩散模型支持的唯一范式。毕竟，[CLIP](http://CLIP)同样是一个文本*和*图像编码器，因此传递给*图像信息创作者*模块的嵌入也可以编码图像。实际上，它可以编码任何模态，或者多个输入的拼接。
- en: This is the principle behind **inpainting**, the task of modifying only a subregion
    of an input image based on given instructions, in a way that looks coherent with
    the rest of the image. The *image information creator* ingests an encoding that
    captures the input image, the mask of the region to be inpainted, and a textual
    instruction.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是**修补**的原理，即根据给定的指令，只修改输入图像的一个子区域，并使其与图像的其他部分看起来协调一致。*图像信息创作者*摄取一个编码，该编码包含输入图像、需要修补区域的掩模和文本指令。
- en: 'Scene Text Editing can be regarded as a specialized form of inpainting. Most
    of the STE research reduces to the following question: *How can we augment the
    text embedding with additional information about the task (i.e., the original
    image, the desired text and its positioning, etc.)?* Formally, this is known as
    **conditional guidance.**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 场景文本编辑可以看作是修补的一个专门化形式。大多数STE研究都归结为以下问题：*我们如何用有关任务的附加信息（即原始图像、期望文本及其位置等）来增强文本嵌入？*
    正式来说，这被称为**条件性引导**。
- en: The research papers that fall into this bucket ([TextDiffuser](https://arxiv.org/abs/2305.10855),
    [TextDiffuser 2](https://arxiv.org/abs/2311.16465), [GlyphDraw](https://arxiv.org/abs/2303.17870),
    [AnyText](https://arxiv.org/abs/2311.03054), etc.) propose various forms of conditional
    guidance.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 归入这一类别的研究论文（[TextDiffuser](https://arxiv.org/abs/2305.10855)，[TextDiffuser 2](https://arxiv.org/abs/2311.16465)，[GlyphDraw](https://arxiv.org/abs/2303.17870)，[AnyText](https://arxiv.org/abs/2311.03054)等）提出了各种形式的条件性引导。
- en: Positional guidance
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 位置信息引导
- en: Evidently, there needs to be a way of specifying *where* to make changes to
    the original image. This can be a text instruction (e.g. “Change the title at
    the bottom”), a granular indication of the text line, or more fine-grained positional
    information for each target character.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，需要一种方法来指定*在哪里*对原始图像进行更改。这可以是文本指令（例如“更改底部的标题”）、文本行的细粒度指示，或者每个目标字符的更精细的位置信息。
- en: '**Positional guidance via image masks.** One way of indicating the desired
    text position is via grayscale mask images, which can then be encoded into latent
    space via CLIP or an alternative image encoder. For instance, the [DiffUTE](https://arxiv.org/pdf/2305.10825.pdf)
    model simply uses a black image with a white strip indicating the desired text
    location.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**通过图像掩码的位置信息**。指示所需文本位置的一种方式是通过灰度掩码图像，然后可以通过CLIP或其他图像编码器将其编码到潜在空间中。例如，[DiffUTE](https://arxiv.org/pdf/2305.10825.pdf)模型仅使用一张黑色图像，其中有一条白色条带指示所需的文本位置。'
- en: '![](../Images/7213f885e9cb32a9d73ee11b9313bc37.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7213f885e9cb32a9d73ee11b9313bc37.png)'
- en: Input to the [DiffUTE](https://arxiv.org/pdf/2305.10825.pdf) model. Positional
    guidance is achieved via the mask m and the masked input xm. These are deterministically
    rendered based on user input.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 输入到[DiffUTE](https://arxiv.org/pdf/2305.10825.pdf)模型。位置指导通过掩码m和被掩盖的输入xm实现。这些是基于用户输入以确定的方式渲染的。
- en: '[TextDiffuser](https://arxiv.org/pdf/2305.10855.pdf) produces character-level
    segmentation masks: first, it roughly renders the desired text in the right position
    (black text in Arial font on a white image), then passes this rendering through
    a segmenter to obtain a grayscale image with individual bounding boxes for each
    character. The segmenter is a [U-Net](https://arxiv.org/abs/1505.04597) model
    trained separately from the main network on 4M of synthetic instances.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[TextDiffuser](https://arxiv.org/pdf/2305.10855.pdf)生成字符级分割掩码：首先，它大致渲染所需文本的位置（白色背景上的黑色文本，字体为Arial），然后将此渲染结果传递给分割器，以获得一个灰度图像，其中包含每个字符的单独边界框。该分割器是一个[U-Net](https://arxiv.org/abs/1505.04597)模型，单独于主网络进行训练，使用了400万的合成实例。'
- en: '![](../Images/3866a0aa4caa4da3fe8b5c9bc2cc6f84.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3866a0aa4caa4da3fe8b5c9bc2cc6f84.png)'
- en: Character-level segmentation mask used by [TextDiffuser](https://arxiv.org/pdf/2305.10855.pdf).
    The target word (“WORK”) is rendered with a standard font on a white background,
    then passed through a segmenter (U-Net) to obtain the grayscale mask.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[TextDiffuser](https://arxiv.org/pdf/2305.10855.pdf)使用的字符级分割掩码。目标词（“WORK”）以标准字体在白色背景上渲染，然后通过分割器（U-Net）得到灰度掩码。'
- en: '**Positional guidance via language modeling**. In [A Unified Sequence Inference
    for Vision Tasks](https://arxiv.org/abs/2206.07669), the authors show that large
    language models (LLMs) can act as effective descriptors of object positions within
    an image by simply generating numerical tokens. Arguably, this was an unintuitive
    discovery. Since LLMs learn language based on statistical frequency (i.e., by
    observing how often tokens occur in the same context), it feels unrealistic to
    expect them to generate the right numerical tokens. But the massive scale of current
    LLMs often defies our expectations nowadays.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**通过语言建模的位置信息**。在[A Unified Sequence Inference for Vision Tasks](https://arxiv.org/abs/2206.07669)中，作者展示了大型语言模型（LLM）通过简单地生成数值令牌，可以有效地描述图像中物体的位置。可以说，这是一个反直觉的发现。由于LLM是基于统计频率学习语言的（即通过观察令牌在相同上下文中出现的频率），人们可能认为它们生成正确的数值令牌是不现实的。但是，当前LLM的巨大规模往往超出了我们的预期。'
- en: '[TextDiffuser 2](https://arxiv.org/pdf/2311.16465.pdf) leverage this discovery
    in an interesting way. They fine-tune an LLM on a synthetic corpus of <text, OCR
    detection> pairs, teaching it to generate the top-left and bottom-right coordinates
    of text bounding boxes, as show in the figure below. Notably, they decide to generate
    bounding boxes for text *lines* (as opposed to *characters*), giving the image
    generator more flexibility. They also run an interesting ablation study that uses
    a single point to encode text position (either top-left or center of the box),
    but observe poorer spelling performance — the model often hallucinates additional
    characters when not explicitly told where the text should end.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[TextDiffuser 2](https://arxiv.org/pdf/2311.16465.pdf)以一种有趣的方式利用了这一发现。它们在一个由<文本，OCR检测>对组成的合成语料库上微调LLM，教它生成文本边界框的左上角和右下角坐标，如下图所示。值得注意的是，它们决定为文本*行*生成边界框（而不是*字符*），从而为图像生成器提供更多灵活性。它们还进行了一个有趣的消融实验，使用单个点来编码文本位置（无论是左上角还是框的中心），但观察到拼写表现较差——当模型没有明确告知文本应如何结束时，它经常会生成多余的字符。'
- en: '![](../Images/af4fe1ce054d35c906c275c657241da2.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af4fe1ce054d35c906c275c657241da2.png)'
- en: Architecture of [**TextDiffuser 2**](https://arxiv.org/pdf/2311.16465.pdf).
    The language model M1 takes the target text from the user, then splits it into
    lines and predicts their positions as [x1] [y1] [x2] [y2] tokens. The language
    model M2 is a fine-tuned version of CLIP that encodes the modified prompt (which
    includes text lines and their positions) into latent space.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[**TextDiffuser 2**](https://arxiv.org/pdf/2311.16465.pdf)的架构。语言模型M1从用户获取目标文本，然后将其拆分成多行，并预测它们的位置，作为[x1]
    [y1] [x2] [y2]令牌。语言模型M2是CLIP的一个微调版本，它将修改后的提示（包括文本行及其位置）编码到潜在空间中。'
- en: Glyph guidance
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 字形引导
- en: In addition to position, another piece of information that can be fed into the
    image generator is *the shape* of the characters. One could argue that shape information
    is redundant. After all, when we prompt a text-to-image model to generate a flamingo,
    we generally don’t need to pass any additional information about its long legs
    or the color of its feathers — the model has presumably learnt these details from
    the training data. However, in practice, the trainings sets (such as Stable Diffusion’s
    [LAION-5B](https://laion.ai/blog/laion-5b/)) are dominated by natural pictures,
    in which text is underrepresented (and non-Latin scripts even more so).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 除了位置，另一个可以输入图像生成器的信息是*字符的形状*。有人可能会认为形状信息是多余的。毕竟，当我们提示文本到图像模型生成火烈鸟时，通常不需要传递关于它长腿或羽毛颜色的任何额外信息——模型应该已经从训练数据中学到了这些细节。然而，实际上，训练集（如Stable
    Diffusion的[LAION-5B](https://laion.ai/blog/laion-5b/)）主要由自然图像组成，其中文本的比例较低（非拉丁文字的比例更低）。
- en: Multiple studies ([DiffUTE](https://arxiv.org/pdf/2305.10825.pdf), [GlyphControl](https://arxiv.org/abs/2305.18259),
    [GlyphDraw](http://GlyphDraw), [GlyphDiffusion](https://arxiv.org/pdf/2304.12519.pdf),
    [AnyText](https://arxiv.org/pdf/2311.03054.pdf) etc.) attempt to make up for this
    imbalance via explicit **glyph guidance** — effectively rendering the glyphs programmatically
    with a standard font, and then passing an encoding of the rendering to the image
    generator. Some simply place the glyphs in the center of the additional image,
    some close to the target positions (reminiscent of [ControlNet](https://arxiv.org/abs/2302.05543)).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 多项研究（[DiffUTE](https://arxiv.org/pdf/2305.10825.pdf)，[GlyphControl](https://arxiv.org/abs/2305.18259)，[GlyphDraw](http://GlyphDraw)，[GlyphDiffusion](https://arxiv.org/pdf/2304.12519.pdf)，[AnyText](https://arxiv.org/pdf/2311.03054.pdf)等）尝试通过显式的**字形引导**来弥补这种不平衡——有效地使用标准字体程序化呈现字形，然后将渲染的编码传递给图像生成器。有些方法只是将字形放置在附加图像的中央，有些则将字形放置在接近目标位置（让人想起[ControlNet](https://arxiv.org/abs/2302.05543)）。
- en: STE via Diffusion is (Still) Complicated
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过扩散进行STE仍然很复杂
- en: While the training process for diffusion models is more stable than GANs, the
    diffusion architectures for STE in particular are still quite complicated. The
    figure below shows the [AnyText](https://arxiv.org/pdf/2311.03054.pdf) architecture,
    which includes (1) an auxiliary latent module (including the positional and glyph
    guidance discussed above), (2) a text embedding module that, among other components,
    requires a pre-trained OCR module, and (3) the standard diffusion pipeline for
    image generation. It is hard to argue this is conceptually much simpler than the
    GAN-based [TextStyleBrush](https://arxiv.org/abs/2106.08385).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管扩散模型的训练过程比GAN更稳定，但特别是STE的扩散架构仍然相当复杂。下图展示了[AnyText](https://arxiv.org/pdf/2311.03054.pdf)的架构，其中包括（1）一个辅助潜在模块（包括上述讨论的位置和字形引导），（2）一个文本嵌入模块，其中包括需要预训练的OCR模块等组件，和（3）生成图像的标准扩散管道。很难说这在概念上比基于GAN的[TextStyleBrush](https://arxiv.org/abs/2106.08385)简单得多。
- en: '![](../Images/f9e6d8990fefceee87e09e11b7c48702.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f9e6d8990fefceee87e09e11b7c48702.png)'
- en: The (complex) architecture of [**AnyText**](https://arxiv.org/pdf/2311.03054.pdf)**.**
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[**AnyText**](https://arxiv.org/pdf/2311.03054.pdf)的（复杂）架构**。**'
- en: The Future of Scene Text Editing
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 场景文本编辑的未来
- en: 'When the status quo is too complicated, we have a natural tendency to keep
    working on it until it converges to a clean solution. In a way, this is what happened
    to the natural language processing field: computational linguistics theories,
    grammars, dependency parsing — all collapsed under Transformers, which make a
    very simple statement: *the meaning of a token depends on all others around it.*
    Evidently, Scene Text Editing is miles away from this clarity. Architectures contain
    many jointly-trained subnetworks, pre-trained components, and require specific
    training data.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当现状过于复杂时，我们有一种天然的倾向，即继续工作，直到它收敛到一个清晰的解决方案。从某种意义上说，这正是自然语言处理领域发生的事情：计算语言学理论、语法、依存句法分析——所有这些都在
    Transformer 面前崩溃，Transformer 提出了一个非常简单的陈述：*一个符号的意义依赖于其周围所有其他符号的意义。* 显然，场景文本编辑离这种清晰度还相差甚远。其架构包含许多共同训练的子网络、预训练组件，并且需要特定的训练数据。
- en: Text-to-image models will inevitably become better at certain aspects of text
    generation (spelling, typeface diversity, and how crisp the characters look),
    with the right amount and quality of training data. But controllability will remain
    a problem for a much longer time. And even when models do eventually learn to
    follow your instructions to the t, the text-to-image paradigm might still be a
    subpar user experience — would you rather describe the position, look and feel
    of a piece of text in excruciating detail, or would you rather just draw an approximate
    box and choose an inspiration color from a color picker?
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 文本到图像模型在特定方面（如拼写、字体多样性、字符的清晰度）会随着适当数量和质量的训练数据变得更好。但可控性问题将会存在很长时间。即使模型最终学会完全按照你的指示执行，文本到图像的范式可能仍然是一个不尽人意的用户体验——你是宁愿详细描述文本的位置、外观和感觉，还是宁愿仅仅画一个大致的框并从调色板中选择一个灵感颜色？
- en: 'Epilogue: Preventing Abuse'
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结语：防止滥用
- en: Generative AI has brought to light many ethical questions, from authorship /
    copyright / licensing to authenticity and misinformation. While all these loom
    large in our common psyche and manifest in various abstract ways, the misuses
    of Scene Text Editing are down-to-earth and obvious — people faking documents.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式 AI 揭示了许多伦理问题，从著作权/版权/许可到真实性和虚假信息。虽然这些问题在我们的集体意识中显得十分重要，并以各种抽象的方式体现出来，但场景文本编辑的滥用则是切实而明显的——人们伪造文档。
- en: While building [*Textify*](https://storia.ai/textify?utm_source=article&utm_medium=medium.com&utm_campaign=editing-text-in-images-with-ai),
    we’ve seen it all. Some people bump up their follower count in Instagram screenshots.
    Others increase their running speed in Strava screenshots. And yes, some attempt
    to fake IDs, credit cards and diplomas. The temporary solution is to build classifiers
    for certain types of documents and simply refuse to edit them, but, long-term
    the generative AI community needs to invest in automated ways of determining document
    authenticity, be it a text snippet, an image or a video.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建 [*Textify*](https://storia.ai/textify?utm_source=article&utm_medium=medium.com&utm_campaign=editing-text-in-images-with-ai)
    时，我们见识过各种情况。有些人在 Instagram 截图中提高他们的粉丝数，有些人在 Strava 截图中提高他们的跑步速度。是的，有些人甚至试图伪造身份证、信用卡和文凭。临时的解决方法是为某些类型的文档构建分类器，并直接拒绝编辑这些文档，但从长远来看，生成式
    AI 社区需要投资于自动化方式来确定文档的真实性，无论是文本片段、图像还是视频。
