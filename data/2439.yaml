- en: Scaling RAG from POC to Production
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将RAG从概念验证（POC）扩展到生产
- en: 原文：[https://towardsdatascience.com/scaling-rag-from-poc-to-production-31bd45d195c8?source=collection_archive---------0-----------------------#2024-10-07](https://towardsdatascience.com/scaling-rag-from-poc-to-production-31bd45d195c8?source=collection_archive---------0-----------------------#2024-10-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/scaling-rag-from-poc-to-production-31bd45d195c8?source=collection_archive---------0-----------------------#2024-10-07](https://towardsdatascience.com/scaling-rag-from-poc-to-production-31bd45d195c8?source=collection_archive---------0-----------------------#2024-10-07)
- en: Common challenges and architectural components to enable scaling
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启动和扩展的常见挑战与架构组件
- en: '[](https://medium.com/@bhagatanurag03?source=post_page---byline--31bd45d195c8--------------------------------)[![Anurag
    Bhagat](../Images/3711a1fce6e2a45d649e534f08c3d0ca.png)](https://medium.com/@bhagatanurag03?source=post_page---byline--31bd45d195c8--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--31bd45d195c8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--31bd45d195c8--------------------------------)
    [Anurag Bhagat](https://medium.com/@bhagatanurag03?source=post_page---byline--31bd45d195c8--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bhagatanurag03?source=post_page---byline--31bd45d195c8--------------------------------)[![Anurag
    Bhagat](../Images/3711a1fce6e2a45d649e534f08c3d0ca.png)](https://medium.com/@bhagatanurag03?source=post_page---byline--31bd45d195c8--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--31bd45d195c8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--31bd45d195c8--------------------------------)
    [Anurag Bhagat](https://medium.com/@bhagatanurag03?source=post_page---byline--31bd45d195c8--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--31bd45d195c8--------------------------------)
    ·7 min read·Oct 7, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--31bd45d195c8--------------------------------)
    ·阅读时长7分钟·2024年10月7日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ee14c78dc3f1d708f7e587a83fe8d0db.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee14c78dc3f1d708f7e587a83fe8d0db.png)'
- en: '*Source: Generated with the help of AI (OpenAI’s Dall-E model)*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*来源：在AI（OpenAI的Dall-E模型）的帮助下生成*'
- en: 1\. Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: 1.1\. Overview of RAG
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1\. RAG概述
- en: Those of you who have been immersed in generative AI and its large-scale applications
    outside of personal productivity apps have likely come across the notion of Retrieval
    Augmented Generation or RAG. The RAG architecture consists of two key components—the
    retrieval component which uses vector databases to do an index based search on
    a large corpus of documents. This is then sent over to a large language model
    (LLM) to generate a grounded response based on the richer context in the prompt.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 那些沉浸于生成性AI及其在个人生产力应用之外的大规模应用的朋友们，可能已经接触过检索增强生成（RAG）的概念。RAG架构由两个关键组件组成——检索组件，它使用向量数据库对大量文档进行基于索引的搜索，然后将其传送给大语言模型（LLM）以生成基于提示中更丰富上下文的有依据的响应。
- en: Whether you are building customer-facing chatbots to answer repetitive questions
    and reduce workload from customer service agents, or building a co-pilot for engineers
    to help them navigate complex user manuals step-by-step, RAG has become a key
    archetype of the application of LLMs. This has enabled LLMs to provide a contextually
    relevant response based on ground truth of hundreds or millions of documents,
    reducing hallucinations and improving the reliability of LLM-based applications.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你是在构建面向客户的聊天机器人以回答重复性问题并减少客户服务代表的工作负担，还是在为工程师构建一款协同助手，帮助他们一步步浏览复杂的用户手册，RAG已经成为大语言模型（LLMs）应用的关键原型。这使得LLM能够根据成千上万的文档的事实基础，提供上下文相关的响应，从而减少幻觉现象并提高基于LLM的应用的可靠性。
- en: 1.2\. Why scale from Proof of Concept(POC) to production
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2\. 为什么要从概念验证（POC）扩展到生产
- en: If you are asking this question, I might challenge you to answer why are you
    even building a POC if there is no intent of getting it to production. Pilot purgatory
    is a common risk with organisations that start to experiment, but then get stuck
    in experimentation mode. Remember that POCs are expensive, and true value realisation
    only happens once you go into production and do things at scale- either freeing
    up resources, making them more efficient, or creating additional revenue streams.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在问这个问题，我可能会挑战你回答：如果没有打算将其投入生产，为什么要构建一个POC（概念验证）？“试点地狱”是组织在开始实验时常见的风险，之后却陷入了实验模式。请记住，POC是昂贵的，真正的价值实现只有在进入生产并按规模执行时才会发生——无论是释放资源、提高效率，还是创造额外的收入流。
- en: 2\. Key challenges in scaling RAG
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 扩展RAG的关键挑战
- en: 2.1\. Performance
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1. 性能
- en: Performance challenges in RAGs come in various flavours. The speed of retrieval
    is generally not the primary challenge unless your knowledge corpus has millions
    of documents, and even then it can be solved by setting up the right infrastructure-
    of course, we are limited by inference times. The second performance problem we
    encounter is around getting the “right” chunks to be fed to the LLMs for generation,
    with a high level of precision and recall. The poorer the retrieval process is,
    the less contextually relevant the LLM response will be.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: RAG中的性能挑战有很多种形式。检索速度通常不是主要挑战，除非你的知识库有数百万份文档，即便如此，这个问题也可以通过搭建合适的基础设施来解决——当然，我们也受到推理时间的限制。我们遇到的第二个性能问题是如何获取“正确”的文段，以便传递给大语言模型（LLM）进行生成，且要保证高精度和高召回率。检索过程越差，LLM的回应就越缺乏上下文相关性。
- en: 2.2\. Data Management
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2. 数据管理
- en: We have all heard the age-old saying “garbage in garbage out (GIGO)”. RAG is
    nothing but a set of tools we have at our disposal, but the real value comes from
    the actual data. As RAG systems work with unstructured data, it comes with its
    own set of challenges including but not limited to- version control of documents,
    and format conversion (e.g. pdf to text), among others.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都听过那句古老的谚语“垃圾进，垃圾出（GIGO）”。RAG不过是我们手头的一组工具，真正的价值来自于实际数据。由于RAG系统处理的是非结构化数据，因此它有自己的一系列挑战，包括但不限于——文档版本控制和格式转换（例如从PDF到文本），等等。
- en: 2.3\. Risk
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3. 风险
- en: One of the biggest reasons corporations hesitate to move from testing the waters
    to jumping in is the possible risks that come with using AI based systems. Hallucinations
    are definitely lowered with the use of RAG, but are still non-zero. There are
    other associated risks including risks for bias, toxicity, regulatory risks etc.
    which could have long term implications.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 企业从试水到全面投入的最大犹豫之一，是使用基于AI的系统可能带来的风险。尽管使用RAG可以显著减少幻觉现象，但它们仍然存在非零的可能性。还有其他相关的风险，包括偏见、毒性、合规性风险等，这些风险可能带来长期的影响。
- en: 2.4\. Integration into existing workflows
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4. 集成到现有工作流程中
- en: Building an offline solution is easier, but bringing in the end users’ perspective
    is crucial to make sure the solution does not feel like a burden. No users want
    to go to another screen to use the “new AI feature”- users want the AI features
    built into their existing workflows so the technology is assistive, and not disruptive
    to the day-to-day.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 构建离线解决方案比较容易，但引入最终用户的视角至关重要，以确保解决方案不会让用户觉得负担重。没有用户愿意为了使用“新AI功能”而切换到另一个屏幕——用户希望AI功能能够内嵌到现有工作流程中，这样技术就能成为一种辅助工具，而不是日常工作中的干扰。
- en: 2.5\. Cost
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5. 成本
- en: Well, this one seems sort of obvious, doesn’t it? Organisations are implementing
    GenAI use cases so that they can create business impact. If the benefits are lower
    than we planned, or there are cost overruns, the impact would be severely diminished,
    or also completely negated.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，这一点似乎很显而易见，对吧？组织正在实施生成式人工智能（GenAI）用例，以便创造业务影响。如果实际收益低于我们的预期，或者出现成本超支，影响将大大减少，甚至可能完全抵消。
- en: 3\. Architectural components needed for Scaling
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3. 扩展所需的架构组件
- en: It would be unfair to only talk about challenges if we don’t talk about the
    “so what do we do”. There are a few essential components you can add to your architecture
    stack to overcome/diminish some of the problems we outlined above.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只谈论挑战而不谈论“那么我们该怎么做”，那就不公平了。你可以在架构堆栈中加入一些必要的组件，以克服或减少我们上面提到的一些问题。
- en: 3.1\. Scalable vector databases
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1. 可扩展的向量数据库
- en: A lot of teams, rightfully, start with open-source vector databases like [ChromaDB](https://www.trychroma.com/),
    which are great for POCs as they are easy to use and customise. However, it may
    face challenges with large-scale deployments. This is where scalable vector databases
    come in (such as [Pinecone](https://www.pinecone.io/), [Weaviate](https://weaviate.io/platform),
    [Milvus](https://milvus.io/), etc.) which are optimised for high-dimensional vector
    searches, enabling fast (sub-millisecond), accurate retrieval even as the dataset
    size increases into the millions or billions of vectors as they use [Approximate
    Nearest Neighbour](https://www.mongodb.com/resources/basics/ann-search) search
    techniques. These vector databases have APIs, plugins, and SDKs that allow for
    easier workflow integration and they are also horizontally scalable. Depending
    on the platform one is working on- it might make sense to explore vector databases
    offered by [Databricks](https://www.databricks.com/product/machine-learning/vector-search)
    or [AWS](https://aws.amazon.com/opensearch-service/).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 许多团队通常会首先使用开源向量数据库，如[ChromaDB](https://www.trychroma.com/)，因为它们易于使用和定制，非常适合概念验证（POC）。然而，它在大规模部署中可能会面临挑战。这时，可扩展的向量数据库就发挥了作用（例如，[Pinecone](https://www.pinecone.io/)，[Weaviate](https://weaviate.io/platform)，[Milvus](https://milvus.io/)等），这些数据库针对高维向量搜索进行了优化，能够实现快速（亚毫秒级）、准确的检索，即使数据集的大小增加到百万或十亿向量，它们仍然能够高效运作，因为它们使用[近似最近邻](https://www.mongodb.com/resources/basics/ann-search)搜索技术。这些向量数据库提供API、插件和SDK，方便与现有工作流集成，同时支持横向扩展。根据平台的不同，可能需要探索[Databricks](https://www.databricks.com/product/machine-learning/vector-search)或[AWS](https://aws.amazon.com/opensearch-service/)提供的向量数据库。
- en: '![](../Images/6f8562c4e112308a47a06f348db3715d.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f8562c4e112308a47a06f348db3715d.png)'
- en: '*Source: Generated with the help of AI (OpenAI’s Dall-E model)*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*来源：借助AI（OpenAI的Dall-E模型）生成*'
- en: 3.2\. Caching Mechanisms
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2\. 缓存机制
- en: The concept of caching has been around almost as long as the internet, [dating
    back to](https://cacm.acm.org/opinion/ibms-single-processor-supercomputer-efforts/#:~:text=a.,computer%20system%20to%20use%20cache)
    the 1960’s. The same concept applies to GenerativeAI as well—If there are a large
    number of queries, maybe in the millions (very common in the customer service
    function), it is likely that many queries are the same or extremely similar. Caching
    allows one to avoid sending a request to the LLM if we can instead return a response
    from a recent cached response. This serves two purposes- reduced costs, as well
    as better response times for common queries.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存的概念几乎与互联网的诞生一样久远，[可以追溯到](https://cacm.acm.org/opinion/ibms-single-processor-supercomputer-efforts/#:~:text=a.,computer%20system%20to%20use%20cache)20世纪60年代。这个概念同样适用于生成型AI——如果有大量查询，可能达到数百万（在客户服务功能中非常常见），很可能许多查询是相同的或极为相似的。缓存技术可以避免向LLM发送请求，如果可以从最近的缓存响应中返回结果，这样不仅能减少成本，还能提高常见查询的响应速度。
- en: This can be implemented as a memory Cache (in-memory caches like [Redis](https://redis.io/)
    or [Memcached](https://memcached.org/)), Disk Cache for less frequent queries
    or distributed Cache (Redis Cluster). Some model providers like Anthropic offer
    [prompt caching](https://www.anthropic.com/news/prompt-caching) as part of their
    APIs.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过内存缓存（如[Redis](https://redis.io/)或[Memcached](https://memcached.org/)）来实现，针对较少的查询使用磁盘缓存，或使用分布式缓存（如Redis集群）。一些模型提供商（如Anthropic）提供[提示缓存](https://www.anthropic.com/news/prompt-caching)作为其API的一部分。
- en: '![](../Images/eb99db871ae334a898f3493fc951fc01.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb99db871ae334a898f3493fc951fc01.png)'
- en: '*Source: Generated with the help of AI (OpenAI’s Dall-E model)*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*来源：借助AI（OpenAI的Dall-E模型）生成*'
- en: 3.3\. Advanced Search Techniques
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3.3\. 高级搜索技术
- en: 'While not as crisply an architecture component, multiple techniques can help
    elevate the search to enhance both efficiency and accuracy. Some of these include:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然并非一个明确的架构组件，但多种技术可以帮助提升搜索效率和准确性。以下是其中一些方法：
- en: '**Hybrid Search: I**nstead of relying only on semantic search(using vector
    databases), or keyword search, use a combination to boost your search.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混合搜索**：与仅依赖语义搜索（使用向量数据库）或关键词搜索不同，采用两者结合的方法来提升搜索效果。'
- en: '**Re-ranking:** Use a LLM or SLM to calculate a relevancy score for the query
    with each search result, and re-rank them to extract and share only the highly
    relevant ones. This is particularly useful for complex domains, or domains where
    one may have many documents being returned. One example of this is [Cohere’s Rerank](https://aws.amazon.com/marketplace/pp/prodview-pf7d2umihcseq).'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重新排序：** 使用 LLM 或 SLM 计算查询与每个搜索结果的相关性得分，并重新排序，从而仅提取并分享最相关的结果。这对于复杂领域或有许多文档返回的领域尤其有用。一个例子是
    [Cohere’s Rerank](https://aws.amazon.com/marketplace/pp/prodview-pf7d2umihcseq)。'
- en: '![](../Images/10211b4b3b6985b8800c7a4427794dc8.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10211b4b3b6985b8800c7a4427794dc8.png)'
- en: '*Source: Generated with the help of AI (OpenAI’s Dall-E model)*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*来源：通过 AI（OpenAI 的 Dall-E 模型）生成*'
- en: 3.4\. Responsible AI layer
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3.4\. 负责任的人工智能层
- en: 'Your Responsible AI modules have to be designed to mitigate bias, ensure transparency,
    align with your organisation’s ethical values, continuously monitor for user feedback
    and track compliance to regulation among other things, relevant to your industry/function.
    There are many ways to go about it, but fundamentally this has to be enabled programmatically,
    with human oversight. A few ways it can be done that can be done:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 您的负责任人工智能模块必须设计用来减轻偏见，确保透明度，与您组织的伦理价值观对齐，持续监控用户反馈，并跟踪符合相关行业/功能的法规要求。方法有很多，但从根本上来说，必须通过编程启用，并由人工监督。可以采用几种方式来实现：
- en: '**Pre-processing:** Filter user queries before they are ever sent over to the
    foundational model. This may include things like checking for bias, toxicity,
    un-intended use etc.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预处理：** 在用户查询发送到基础模型之前进行过滤。这可能包括检查偏见、有害内容、非预期用途等。'
- en: '**Post-processing:** Apply another set of checks after the results come back
    from the FMs, before exposing them to the end users.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**后处理：** 在结果从基础模型返回后，向最终用户展示之前应用另一组检查。'
- en: These checks can be enabled as small reusable modules you buy from an external
    provider, or build/customise for your own needs. One common way organisations
    have approached this is to use carefully engineered prompts and foundational models
    to orchestrate a workflow and prevent a result reaching the end user till it passes
    all checks.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这些检查可以作为小型可重用模块启用，您可以从外部提供商购买，或者根据自己的需求进行构建/定制。组织通常采用的一种方式是使用精心设计的提示和基础模型来编排工作流程，确保结果在通过所有检查之前不会呈现给最终用户。
- en: '![](../Images/184aa917e7c7eebc06ae752d9ef78de8.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/184aa917e7c7eebc06ae752d9ef78de8.png)'
- en: '*Source: Generated with the help of AI (OpenAI’s Dall-E model)*'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*来源：通过 AI（OpenAI 的 Dall-E 模型）生成*'
- en: 3.5\. API Gateway
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5\. API 网关
- en: 'An API Gateway can serve multiple purposes helping manage costs, and various
    aspects of Responsible AI:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: API 网关可以发挥多种作用，帮助管理成本以及负责任的人工智能的各个方面：
- en: Provide a unified interface to interact with foundational models, experiment
    with them
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供统一的接口与基础模型互动，进行实验。
- en: Help develop a fine-grained view into costs and usage by team/use case/cost
    centre — including rate-limiting, speed throttling, quota management
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帮助开发精细化的成本和使用情况视图，按团队/用例/成本中心进行划分 — 包括速率限制、速度控制、配额管理
- en: Serve as a responsible AI layer, filtering out in-intended requests/data before
    they ever hit the models
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为负责任的人工智能层，在请求/数据达到模型之前，过滤掉不适当的请求/数据。
- en: Enable audit trails and access control
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启用审计追踪和访问控制
- en: '![](../Images/cb937658eacc1f107b854540ff8edb4e.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb937658eacc1f107b854540ff8edb4e.png)'
- en: '*Source: Generated with the help of AI (OpenAI’s Dall-E model)*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*来源：通过 AI（OpenAI 的 Dall-E 模型）生成*'
- en: 4\. Is this enough, or do we need more?
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 这样就够了吗，还是我们需要更多？
- en: 'Of course not. There are a few other things that also need to be kept in mind,
    including but not limited to:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当然不是。还有一些其他因素也需要考虑，包括但不限于：
- en: Does the use case occupy a strategic place in your roadmap of use cases? This
    enables you to have leadership backing, and right investments to support the development
    and maintenance.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该用例是否在您的用例路线图中占据战略地位？这使您能够获得领导支持，并获得适当的投资以支持开发和维护。
- en: A clear evaluation criterion to measure the performance of the application,
    against dimensions of accuracy, cost, latency and responsible AI
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个明确的评估标准，用于衡量应用程序在准确性、成本、延迟和负责任的人工智能等维度上的表现。
- en: Improve business processes to keep knowledge up to date, maintain version control
    etc.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改善业务流程，保持知识的最新性，维护版本控制等。
- en: Architect the RAG system so that it only accesses documents based on the end
    user permission levels, to prevent unauthorised access.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计 RAG 系统时，确保它仅在最终用户权限级别的基础上访问文档，以防止未经授权的访问。
- en: Use design thinking to integrate the application into the workflow of the end
    user e.g. if you are building a bot to answer technical questions over Confluence
    as the knowledge base, should you build a separate UI, or integrate this with
    Teams/Slack/other applications users already use?
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用设计思维将应用程序集成到最终用户的工作流程中。例如，如果你正在构建一个通过 Confluence 作为知识库回答技术问题的机器人，你是否应该构建一个独立的用户界面，还是将其与
    Teams/Slack/其他用户已经使用的应用程序集成？
- en: 5\. Conclusion
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5. 结论
- en: RAGs are a prominent use case archetype, and one of the first few ones that
    organisations try to implement. Scaling RAG from POC to production comes with
    its challenges, but with careful planning and execution, many of these can be
    overcome. Some of these can be solved by tactical investment in the architecture
    and technology, some require better strategic direction and tactful planning.
    As LLM inference costs continue to drop, either owing to reduced inference costs
    or heavier adoption of open-source models, cost barriers may not be a concern
    for many new use cases.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 是一个典型的使用案例原型，也是组织首次尝试实施的几个用例之一。从POC到生产环境的 RAG 扩展面临诸多挑战，但通过精心的规划和执行，许多挑战是可以克服的。其中一些可以通过在架构和技术上的战术性投资来解决，而另一些则需要更好的战略方向和巧妙的规划。随着大语言模型（LLM）推理成本的不断下降，无论是由于推理成本的降低，还是开源模型的广泛采用，许多新用例的成本壁垒可能不再是问题。
- en: '*All views in this article are the Author’s and don’t represent an endorsement
    of any products or services.*'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*本文中的所有观点仅代表作者个人意见，并不代表对任何产品或服务的认可。*'
