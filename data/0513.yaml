- en: Designing and Deploying a Machine Learning Python Application (Part 2)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计与部署机器学习 Python 应用程序（第二部分）
- en: 原文：[https://towardsdatascience.com/designing-and-deploying-a-machine-learning-python-application-part-2-99eb37787b2b?source=collection_archive---------4-----------------------#2024-02-24](https://towardsdatascience.com/designing-and-deploying-a-machine-learning-python-application-part-2-99eb37787b2b?source=collection_archive---------4-----------------------#2024-02-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/designing-and-deploying-a-machine-learning-python-application-part-2-99eb37787b2b?source=collection_archive---------4-----------------------#2024-02-24](https://towardsdatascience.com/designing-and-deploying-a-machine-learning-python-application-part-2-99eb37787b2b?source=collection_archive---------4-----------------------#2024-02-24)
- en: You don’t have to be Atlas to get your model into the cloud
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你不需要是 Atlas 就能将你的模型部署到云端
- en: '[](https://medium.com/@noahhaglund?source=post_page---byline--99eb37787b2b--------------------------------)[![Noah
    Haglund](../Images/edfcc90677444ebced16549a1524d7fe.png)](https://medium.com/@noahhaglund?source=post_page---byline--99eb37787b2b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--99eb37787b2b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--99eb37787b2b--------------------------------)
    [Noah Haglund](https://medium.com/@noahhaglund?source=post_page---byline--99eb37787b2b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@noahhaglund?source=post_page---byline--99eb37787b2b--------------------------------)[![Noah
    Haglund](../Images/edfcc90677444ebced16549a1524d7fe.png)](https://medium.com/@noahhaglund?source=post_page---byline--99eb37787b2b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--99eb37787b2b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--99eb37787b2b--------------------------------)
    [Noah Haglund](https://medium.com/@noahhaglund?source=post_page---byline--99eb37787b2b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--99eb37787b2b--------------------------------)
    ·16 min read·Feb 24, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--99eb37787b2b--------------------------------)
    ·阅读时间 16 分钟 ·2024年2月24日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ea381d092591530df711ecba890d8217.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea381d092591530df711ecba890d8217.png)'
- en: Image by Midjourney
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来自 Midjourney
- en: Now that we have our trained Detectron2 model ([see Part 1](https://medium.com/towards-data-science/training-and-deploying-a-custom-detectron2-model-for-object-detection-using-pdf-documents-part-1-c724f61d8b4b)),
    let’s deploy it as a part of an application to provide its inferencing abilities
    to others.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了训练好的 Detectron2 模型（[请参见第一部分](https://medium.com/towards-data-science/training-and-deploying-a-custom-detectron2-model-for-object-detection-using-pdf-documents-part-1-c724f61d8b4b)），让我们将其作为应用程序的一部分进行部署，以便向他人提供推理能力。
- en: Even though Part 1 and 2 of this series use Detectron2 for Object Detection,
    no matter the machine learning library you are using (*Detectron, Yolo, PyTorch,
    Tensorflow, etc*) and no matter your use case (*Computer Vision, Natural Language
    Processing, Deep Learning, etc*), various topics discussed here concerning model
    deployment will be useful for all those developing ML processes.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 即使本系列的第一部分和第二部分使用 Detectron2 进行物体检测，无论你使用的是哪种机器学习库（*Detectron、Yolo、PyTorch、Tensorflow
    等*），无论你的使用场景是（*计算机视觉、自然语言处理、深度学习 等*），这里讨论的关于模型部署的各种话题对于所有开发机器学习过程的人都将有所帮助。
- en: Although the fields of Data Science and Computer Science overlap in many ways,
    training and deploying an ML model combines the two, as those concerned with developing
    an efficient and accurate model are not typically the ones trying to deploy it
    and vice versa. On the other hand, someone more CS oriented may not have the understanding
    of ML or its associated libraries to determine whether application bottlenecks
    could be fixed with configurations to the ML process or rather the backend and
    hosting service/s.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管数据科学和计算机科学在许多方面有所重叠，但训练和部署机器学习模型将两者结合起来，因为那些致力于开发高效准确模型的人通常并不是负责部署模型的人，反之亦然。另一方面，偏向计算机科学的人可能对机器学习或相关库的了解不足，无法判断是否可以通过对机器学习过程的配置来解决应用瓶颈，还是需要通过后端和托管服务来解决。
- en: 'In order to aid you in your quest to deploy an application that utilizes ML,
    this article will begin by discussing: (1) high level CS design concepts that
    can help DS folks makes decisions in order to balance load and mitigate bottlenecks
    and (2) low level design by walking through deploying a Detectron2 inferencing
    process using the Python web framework Django, an API using Django Rest Framework,
    the distributed task queue Celery, Docker, Heroku, and AWS S3.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助你部署一个利用 ML 的应用，本文将首先讨论：（1）可以帮助数据科学人员做决策的高层计算机科学设计概念，以平衡负载并缓解瓶颈；（2）低层设计，讲解如何通过使用
    Python Web 框架 Django、Django Rest Framework API、分布式任务队列 Celery、Docker、Heroku 和
    AWS S3 来部署 Detectron2 推理过程。
- en: 'For following along with this article, it will be helpful to have in advance:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 跟随本文的学习，提前具备以下知识将会有所帮助：
- en: Strong Python Knowledge
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扎实的 Python 知识
- en: Understanding of Django, Django Rest Framework, Docker, Celery, and AWS
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对 Django、Django Rest Framework、Docker、Celery 和 AWS 的理解
- en: Familiarity with Heroku
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熟悉 Heroku
- en: High Level Design
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高层设计
- en: In order to dig into the high level design, let’s discuss a couple key problems
    and potential solutions.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了深入探讨高层设计，让我们讨论一些关键问题和潜在解决方案。
- en: 'Problem 1: Memory'
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题 1：内存
- en: The saved ML model from [Part 1](https://medium.com/towards-data-science/training-and-deploying-a-custom-detectron2-model-for-object-detection-using-pdf-documents-part-1-c724f61d8b4b),
    titled model_final.pth, will start off at ~325MB. Additionally, an application
    based on (1) a Python runtime, (2) Detectron2, (3) large dependencies such as
    Torch, and (4) a Django web framework will utilize ~150MB of memory on deployment.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从[第 1 部分](https://medium.com/towards-data-science/training-and-deploying-a-custom-detectron2-model-for-object-detection-using-pdf-documents-part-1-c724f61d8b4b)保存的机器学习模型，命名为
    model_final.pth，起始大小为大约 325MB。此外，基于（1）Python 运行时、（2）Detectron2、（3）大型依赖项如 Torch
    和（4）Django Web 框架的应用程序在部署时将使用约 150MB 的内存。
- en: So at minimum, we are looking at ~475MB of memory utilized right off the bat.
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 因此，至少我们一开始就需要大约 475MB 的内存。
- en: We could load the Detectron2 model only when the ML process needs to run, but
    this would still mean that our application would eat up ~475MB eventually. If
    you have a tight budget and are unable to vertically scale your application, memory
    now becomes a substantial limitation on many hosting platforms. For example, Heroku
    offers containers to run applications, termed “dynos”, that started with 512MB
    RAM for base payment plans, will begin writing to disk beyond the 512MB threshold,
    and will crash and restart the dyno at 250% utilization (1280MB).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以仅在机器学习（ML）过程需要运行时加载 Detectron2 模型，但这仍然意味着我们的应用最终会占用约475MB的内存。如果你的预算有限，并且无法垂直扩展应用程序，那么内存就会成为许多托管平台上的一个重要限制。例如，Heroku
    提供了名为“dynos”的容器来运行应用程序，基础支付计划的 dynos 起始内存为 512MB，当内存超过 512MB 时，会开始写入磁盘，并且当内存使用达到
    250%（1280MB）时，dyno 会崩溃并重启。
- en: On the topic of memory, Detectron2 inferencing will cause spikes in memory usage
    depending on the amount of objects detected in an image, so it is important to
    ensure memory is available during this process.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 关于内存，Detectron2 推理将根据图像中检测到的物体数量引发内存使用的波动，因此确保在此过程中内存可用非常重要。
- en: 'For those of you trying to speed up inferencing, but are cautious of memory
    constraints, batch inferencing will be of no help here either. [As noted by one
    of the contributors to the Detectron2 repo](https://github.com/facebookresearch/detectron2/issues/1539),
    with batch inferencing:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些想加速推理，但又担心内存限制的人，批量推理在这里也无济于事。[正如 Detectron2 仓库中的一位贡献者所指出的](https://github.com/facebookresearch/detectron2/issues/1539)，使用批量推理时：
- en: N images use N times more memory than 1 image…You can predict on N images one
    by one in a loop instead.
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: N 张图像所需的内存是 1 张图像的 N 倍……你可以改为在循环中逐张预测 N 张图像。
- en: 'Overall, this summarizes **problem #1**:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '总的来说，这总结了**问题 #1**：'
- en: running a long ML processes as a part of an application will most likely be
    memory intensive, due to the size of the model, ML dependencies, and inferencing
    process.
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 将长时间运行的 ML 过程作为应用的一部分，通常会非常占用内存，因为模型的大小、ML 依赖项以及推理过程都需要大量内存。
- en: 'Problem 2: Time'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题 2：时间
- en: A deployed application that incorporates ML will likely need to be designed
    to manage a long-running process.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 集成 ML 的部署应用程序可能需要设计成能够管理长期运行的过程。
- en: Using the example of an application that uses Detectron2, the model would be
    sent an image as input and output inference coordinates. With one image, inference
    may only take a few seconds, but say for instance we are processing a long PDF
    document with one image per page (as per the training data in [Part 1](https://medium.com/towards-data-science/training-and-deploying-a-custom-detectron2-model-for-object-detection-using-pdf-documents-part-1-c724f61d8b4b)),
    this could take a while.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以使用 Detectron2 的应用程序为例，模型会接收一张图像作为输入并输出推理坐标。对于一张图像，推理可能只需几秒钟，但假设我们正在处理一个包含每页一张图像的长
    PDF 文档（如[第一部分](https://medium.com/towards-data-science/training-and-deploying-a-custom-detectron2-model-for-object-detection-using-pdf-documents-part-1-c724f61d8b4b)中的训练数据），这可能会需要一些时间。
- en: 'During this process, Detectron2 inferencing would be either CPU or GPU bound,
    depending on your configurations. See the below Python code block to change this
    (CPU is entirely fine for inferencing, however, GPU/Cuda is necessary for training
    as mentioned in [Part 1](https://medium.com/towards-data-science/training-and-deploying-a-custom-detectron2-model-for-object-detection-using-pdf-documents-part-1-c724f61d8b4b)):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，Detectron2 推理将是 CPU 或 GPU 限制的，具体取决于你的配置。请查看下面的 Python 代码块以进行更改（推理完全可以使用
    CPU，但正如[第一部分](https://medium.com/towards-data-science/training-and-deploying-a-custom-detectron2-model-for-object-detection-using-pdf-documents-part-1-c724f61d8b4b)中提到的，训练需要
    GPU/Cuda）：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Additionally, saving images after inferencing, say to AWS S3 for example, would
    introduce I/O bound processes. Altogether, this could serve to clog up the backend,
    which introduces **problem #2**:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，推理后保存图像，比如保存到 AWS S3，将引入 I/O 限制的进程。总体而言，这可能会堵塞后端，进而引发**问题 #2**：'
- en: single-threaded Python applications will not process additional HTTP requests,
    concurrently or otherwise, while running a process.
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 单线程的 Python 应用程序在运行某个进程时，不会同时处理额外的 HTTP 请求，无论是并发请求还是其他类型的请求。
- en: 'Problem 3: Scale'
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题 3：扩展性
- en: When considering the horizontal scalability of a Python application, it is important
    to note that Python (assuming it is compiled/interpreted by CPython) suffers from
    the limitations of the Global Interpreter Lock (GIL), which [allows only one thread
    to hold the control of the Python interpreter](https://realpython.com/python-gil/).
    Thus, the paradigm of multithreading doesn’t correctly apply to Python, as applications
    can still implement multithreading, using web servers such as Gunicorn, but will
    do so concurrently, meaning that the threads aren’t running in parallel.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑 Python 应用程序的水平可扩展性时，必须注意，Python（假设它是通过 CPython 编译/解释的）受限于全局解释器锁（GIL），这意味着它[只允许一个线程持有
    Python 解释器的控制权](https://realpython.com/python-gil/)。因此，Python 不完全适用多线程范式，虽然应用程序可以实现多线程，使用如
    Gunicorn 等 Web 服务器，但它们是并发执行的，这意味着线程并不是并行运行的。
- en: I know all of this sounds fairly abstract, perhaps especially for the Data Science
    folks, so let me provide an example to illustrate this problem.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道这一切听起来相当抽象，尤其是对于数据科学领域的朋友们，所以下面我会提供一个示例来说明这个问题。
- en: You are your application and right now your hardware, brain, is processing two
    requests, cleaning the counter and texting on your phone. With two arms to do
    this, you are now a multithreaded Python application, doing both simultaneously.
    But you’re not actually thinking about both at the same *exact* time, you start
    your hand in a cleaning motion, then switch your attention to your phone to look
    at what you are typing, then look back at the counter to make sure you didn’t
    miss a spot.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 你就是你的应用程序，现在你的硬件——大脑——正在处理两个请求，清理柜台和在手机上发短信。你有两只手来完成这两项任务，现在你就像一个多线程的 Python
    应用程序，正在同时做这两件事。但实际上，你并不是在*确切*的同一时刻考虑两件事，而是先开始清理动作，然后把注意力转移到手机上看你正在输入的内容，再看回柜台，确保没有漏掉任何地方。
- en: In actuality, you are processing these tasks concurrently.
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 实际上，你正在并发地处理这些任务。
- en: The GIL functions in the same way, processing one thread at a time but switching
    between them for concurrency. This means that multithreading a Python application
    is still useful for running background or I/O bound-oriented tasks, such as downloading
    a file, while the main execution’s thread is still running. To take the analogy
    this far, your background task of cleaning the counter (i.e. downloading a file)
    continues to happen while you are thinking about texting, but you still need to
    change your focus back to your cleaning hand in order to process the next step.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: GIL 的工作原理是一样的，每次处理一个线程，但通过在它们之间切换来实现并发。这意味着多线程处理 Python 应用程序仍然对运行后台任务或 I/O 密集型任务（例如下载文件）有用，而主执行线程仍在运行。用前面的类比来说，清理台面的后台任务（即下载文件）仍在进行，而你在考虑发短信时，但你仍然需要将焦点转回到清洁的手上，以便处理下一步。
- en: This “change in focus” may not seem like a big deal when concurrently processing
    multiple requests, but when you need to handle hundreds of requests simultaneously,
    suddenly this becomes a limiting factor for large scale applications that need
    to be adequately responsive to end users.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这种“焦点的变化”在同时处理多个请求时可能看起来并不是什么大问题，但当你需要同时处理数百个请求时，突然之间，这就成为了大规模应用程序的一个限制因素，因为它们需要对终端用户保持足够的响应能力。
- en: 'Thus, we have problem #3:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，我们有问题 #3：'
- en: the GIL prevents multithreading from being a good scalability solution for Python
    applications.
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: GIL 阻止了多线程成为 Python 应用程序的良好扩展解决方案。
- en: Solutions
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Now that we have identified key problems, let’s discuss a few potential solutions.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经识别出了关键问题，让我们来讨论一些潜在的解决方案。
- en: 'The aforementioned problems are ordered in terms of importance, as we need
    to manage memory first and foremost (problem #1) to ensure the application doesn’t
    crash, then leave room for the app to process more than one request at a time
    (problem #2) while still ensuring our means of simultaneous request handling is
    effective at scale (problem #3).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '前面提到的问题按重要性排序，因为我们首先需要管理内存（问题 #1），确保应用程序不会崩溃，然后为应用程序留出空间以便一次处理多个请求（问题 #2），同时仍确保我们在大规模下处理多个请求的方式是有效的（问题
    #3）。'
- en: 'So, let’s jump right into addressing problem #1.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '那么，让我们直接开始解决问题 #1。'
- en: 'Depending on the hosting platform, we will need to be fully aware of the configurations
    available in order to scale. As we will be using Heroku, feel free to check out
    the guidance on [dyno scaling](https://devcenter.heroku.com/articles/scaling).
    Without having to vertically scale up your dyno, *we can scale out by adding another*
    [*process*](https://devcenter.heroku.com/articles/process-model). For instance,
    with the [Basic dyno type](https://devcenter.heroku.com/articles/dyno-types),
    a developer is able to deploy both a web process and a worker process on the same
    dyno. A few reasons this is useful:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 根据托管平台的不同，我们需要充分了解可用的配置，以便进行扩展。由于我们将使用 Heroku，欢迎查看关于 [dyno 扩展](https://devcenter.heroku.com/articles/scaling)的指南。在不需要垂直扩展
    dyno 的情况下，*我们可以通过添加另一个* [*进程*](https://devcenter.heroku.com/articles/process-model)
    来进行扩展。例如，在 [Basic dyno 类型](https://devcenter.heroku.com/articles/dyno-types)中，开发者可以在同一个
    dyno 上部署 web 进程和 worker 进程。这种做法有几个好处：
- en: This enables a means of multiprocessing.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这使得多进程处理成为可能。
- en: The dyno resources are now duplicated, meaning each process has a 512MB RAM
    threshold.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，dyno 资源被复制了，这意味着每个进程都有一个 512MB 的内存阈值。
- en: Cost wise, we are looking at $7 per month per process (so $14 a month with both
    a web and worker process). Much cheaper than vertically scaling the dyno to get
    more RAM, with $50 a month per dyno if you want to increase the 512MB allocation
    to 1024MB.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在成本方面，每个进程每月费用为 7 美元（所以如果有一个 web 进程和一个 worker 进程，则每月为 14 美元）。比起垂直扩展 dyno 来增加内存要便宜得多，如果你想将
    512MB 的内存分配扩展到 1024MB，单个 dyno 每月需要 50 美元。
- en: Hopping back to the previous analogy of cleaning the counter and texting on
    your phone, instead of threading yourself further by adding additional arms to
    your body, we can now have two people (multiprocessing in parallel) to perform
    the separate tasks. We are scaling out by increasing workload diversity as opposed
    to scaling up, in turn saving us money.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 回到前面关于清洁台面和发短信的类比，与其通过为身体添加额外的手臂来让自己更复杂，不如让两个人（并行多进程）来执行这些独立的任务。我们通过增加工作负载的多样性来扩展，而不是通过扩展单一的进程，从而为我们节省了开支。
- en: Okay, but with two separate processes, what’s the difference?
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，但是有两个独立的进程，那它们之间有什么区别呢？
- en: 'Using Django, our web process will be initialized with:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Django 时，我们的 web 进程将会初始化为：
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'And using a distributed task queue, such as Celery, the worker will be initialized
    with:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 并且使用分布式任务队列（如 Celery），worker 将使用以下方式初始化：
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[As intended by Heroku](https://devcenter.heroku.com/articles/process-model#mapping-the-unix-process-model-to-web-apps),
    the web process is the server for our core web framework and the worker process
    is intended for queuing libraries, cron jobs, or other work performed in the background.
    Both represent an instance of the deployed application, so will be running at
    ~150MB given the core dependencies and runtime. However, we can ensure that the
    worker is the only process that runs the ML tasks, saving the web process from
    using ~325MB+ in RAM. This has multiple benefits:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[Heroku 的设计理念](https://devcenter.heroku.com/articles/process-model#mapping-the-unix-process-model-to-web-apps)中，web
    进程是我们核心 web 框架的服务器，而 worker 进程则用于处理队列库、定时任务或其他在后台执行的工作。两者都代表已部署应用的实例，因此，考虑到核心依赖和运行时，内存占用大约为
    ~150MB。然而，我们可以确保只有 worker 进程运行机器学习任务，从而避免 web 进程占用 ~325MB 以上的内存。这有多个好处：'
- en: 'Memory usage, although still high for the worker, will be distributed to a
    node outside of the system, ensuring any problems encountered during the execution
    of an ML task can be handled and monitored separately from the web process. This
    helps to mitigate problem #1.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '内存使用量，尽管对于 worker 进程仍然较高，但将分配到系统外的节点，从而确保在执行机器学习任务时遇到的任何问题可以被单独处理和监控，避免影响 web
    进程。这有助于缓解问题 #1。'
- en: 'The newly found means of parallelism ensures that the web process can still
    respond to requests during a long-running ML task, helping to address problem
    #2.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '新发现的并行处理方法确保了 web 进程在执行长时间运行的机器学习任务时仍能响应请求，帮助解决问题 #2。'
- en: 'We are preparing for scale by implementing a means of multiprocessing, helping
    to address problem #3.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '我们通过实现多进程的方式来为扩展做准备，帮助解决问题 #3。'
- en: 'As we haven’t quite solved the key problems, let’s dig in just a bit further
    before getting into the low-level nitty-gritty. [As stated by Heroku](https://devcenter.heroku.com/articles/python-gunicorn):'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们还没有完全解决关键问题，让我们再深入探讨一下，在进入低层次的细节之前。[Heroku 如是说](https://devcenter.heroku.com/articles/python-gunicorn)：
- en: Web applications that process incoming HTTP requests concurrently make much
    more efficient use of dyno resources than web applications that only process one
    request at a time. Because of this, we recommend using web servers that support
    concurrent request processing whenever developing and running production services.
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 同时处理传入 HTTP 请求的 web 应用比一次只处理一个请求的 web 应用更加高效地利用 dyno 资源。因此，我们建议在开发和运行生产服务时使用支持并发请求处理的
    web 服务器。
- en: ''
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Django and Flask web frameworks feature convenient built-in web servers,
    but these blocking servers only process a single request at a time. If you deploy
    with one of these servers on Heroku, your dyno resources will be underutilized
    and your application will feel unresponsive.
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Django 和 Flask web 框架内置了便捷的 web 服务器，但这些阻塞型服务器每次只能处理一个请求。如果你在 Heroku 上使用这些服务器部署，dyno
    资源将得不到充分利用，应用程序也会显得不响应。
- en: 'We are already ahead of the game by utilizing worker multiprocessing for the
    ML task, but can take this a step further by using Gunicorn:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过利用 worker 的多进程来处理机器学习任务，已经领先一步，但可以进一步通过使用 Gunicorn 来优化：
- en: '[Gunicorn](https://gunicorn.org/) is a pure-Python HTTP server for WSGI applications.
    It allows you to run any Python application concurrently by running multiple Python
    processes within a single dyno. It provides a perfect balance of performance,
    flexibility, and configuration simplicity.'
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[Gunicorn](https://gunicorn.org/) 是一个纯 Python 的 HTTP 服务器，专为 WSGI 应用设计。它允许你通过在单个
    dyno 中运行多个 Python 进程并发地运行任何 Python 应用。它提供了性能、灵活性和配置简便性之间的完美平衡。'
- en: 'Okay, awesome, now we can utilize even more processes, but there’s a catch:
    each new worker Gunicorn worker process will represent a copy of the application,
    meaning that they too will utilize the base ~150MB RAM *in addition* to the Heroku
    process. So, say we pip install gunicorn and now initialize the Heroku web process
    with the following command:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，很棒，现在我们可以利用更多的进程，但有一个问题：每一个新的 Gunicorn worker 进程都将代表一个应用副本，这意味着它们将使用基础 ~150MB
    的内存*，此外还会使用 Heroku 进程的内存。比如说，如果我们通过 `pip install gunicorn` 安装并使用以下命令初始化 Heroku
    web 进程：
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The base ~150MB RAM in the web process turns into ~300MB RAM (base memory usage
    multipled by # gunicorn workers).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Web 进程的基本 ~150MB 内存将变成 ~300MB 内存（基础内存使用量乘以 Gunicorn worker 数量）。
- en: 'While being cautious of the limitations to multithreading a Python application,
    we can add threads to workers as well using:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在谨慎考虑 Python 应用程序多线程的限制下，我们也可以通过以下方式为工作线程添加线程：
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Even with problem #3, we can still find a use for threads, as we want to ensure
    our web process is capable of processing more than one request at a time while
    being careful of the application’s memory footprint. Here, our threads could process
    miniscule requests while ensuring the ML task is distributed elsewhere.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '即使是在问题 #3 中，我们仍然可以找到使用线程的方式，因为我们希望确保我们的网络进程能够同时处理多个请求，同时注意应用程序的内存占用。在这里，我们的线程可以处理微小的请求，同时确保机器学习任务在其他地方进行分发。'
- en: 'Either way, by utilizing gunicorn workers, threads, or both, we are setting
    our Python application up to process more than one request at a time. We’ve more
    or less solved problem #2 by incorporating various ways to implement concurrency
    and/or parallel task handling while ensuring our application’s critical ML task
    doesn’t rely on potential pitfalls, such as multithreading, setting us up for
    scale and getting to the root of problem #3.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '无论是哪种方式，通过利用 Gunicorn 工作进程、线程或两者结合，我们都在为我们的 Python 应用程序配置处理多个请求的能力。我们通过采用多种方式实现并发和/或并行任务处理，基本上解决了问题
    #2，同时确保应用程序的关键机器学习任务不依赖于潜在的陷阱，比如多线程，从而为扩展做好准备，并深入解决问题 #3。'
- en: 'Okay so what about that tricky problem #1\. At the end of the day, ML processes
    will typically end up taxing the hardware in one way or another, whether that
    would be memory, CPU, and/or GPU. *However*, by using a distributed system, our
    ML task is integrally linked to the main web process yet handled in parallel via
    a Celery worker. We can track the start and end of the ML task via the chosen
    Celery [broker](https://docs.celeryq.dev/en/3.1/getting-started/brokers/index.html),
    as well as review metrics in a more isolated manner. Here, curtailing Celery and
    Heroku worker process configurations are up to you, but it is an excellent starting
    point for integrating a long-running, memory-intensive ML process into your application.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '好的，那么那个棘手的問題 #1 呢？归根结底，机器学习过程通常会以某种方式消耗硬件资源，无论是内存、CPU 还是 GPU。*然而*，通过使用分布式系统，我们的机器学习任务与主要的
    web 进程紧密相连，但通过 Celery 工作线程并行处理。我们可以通过选择的 Celery [代理](https://docs.celeryq.dev/en/3.1/getting-started/brokers/index.html)
    跟踪机器学习任务的开始和结束，并以更加隔离的方式查看度量数据。在这里，Celery 和 Heroku 工作进程的配置由你决定，但这是将一个长时间运行、内存密集型的机器学习任务集成到应用程序中的一个非常好的起点。'
- en: Low Level Design and Setup
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 低级设计和设置
- en: Now that we’ve had a chance to really dig in and get a high level picture of
    the system we are building, let’s put it together and focus on the specifics.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有机会深入了解并大致了解我们正在构建的系统，接下来让我们把它整合起来，聚焦于具体细节。
- en: For your convenience, [here is the repo](https://github.com/nzh2534/mltutorial/tree/main)
    I will be mentioning in this section.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便你，[这里是我将在本节中提到的代码库](https://github.com/nzh2534/mltutorial/tree/main)。
- en: First we will begin by setting up Django and Django Rest Framework, with installation
    guides [here](https://docs.djangoproject.com/en/5.0/intro/install/) and [here](https://www.django-rest-framework.org/#installation)
    respectively. All requirements for this app can be found in the repo’s requirements.txt
    file (and Detectron2 and Torch will be built from Python wheels specified in the
    Dockerfile, in order to keep the Docker image size small).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将从设置 Django 和 Django Rest Framework 开始，安装指南分别可以在 [这里](https://docs.djangoproject.com/en/5.0/intro/install/)
    和 [这里](https://www.django-rest-framework.org/#installation) 找到。该应用程序的所有需求可以在代码库的
    requirements.txt 文件中找到（同时，Detectron2 和 Torch 将从 Dockerfile 中指定的 Python wheel 构建，以保持
    Docker 镜像的体积较小）。
- en: The next part will be setting up the Django app, configuring the backend to
    save to AWS S3, and exposing an endpoint using DRF, so if you are already comfortable
    doing this, feel free to skip ahead and go straight to the *ML Task Setup and
    Deployment* section.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的部分将是设置 Django 应用程序，配置后端以保存到 AWS S3，并使用 DRF 暴露端点，所以如果你已经熟悉这部分内容，可以跳过并直接进入
    *机器学习任务设置与部署* 部分。
- en: Django Setup
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Django 设置
- en: Go ahead and create a folder for the Django project and cd into it. Activate
    the virtual/conda env you are using, ensure Detectron2 is installed as per the
    installation instructions in [Part 1](https://medium.com/towards-data-science/training-and-deploying-a-custom-detectron2-model-for-object-detection-using-pdf-documents-part-1-c724f61d8b4b),
    and install the requirements as well.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 继续创建一个文件夹用于 Django 项目并进入该目录。激活您正在使用的虚拟环境/conda 环境，确保按照 [第 1 部分](https://medium.com/towards-data-science/training-and-deploying-a-custom-detectron2-model-for-object-detection-using-pdf-documents-part-1-c724f61d8b4b)
    中的安装说明安装 Detectron2，并安装相关的要求。
- en: 'Issue the following command in a terminal:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在终端中执行以下命令：
- en: '[PRE5]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This will create a Django project root directory titled “mltutorial”. Go ahead
    and cd into it to find a manage.py file and a mltutorial sub directory (which
    is the actual Python package for your project).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个名为“mltutorial”的 Django 项目根目录。继续进入该目录，您可以找到 manage.py 文件和一个名为 mltutorial
    的子目录（这是您项目的实际 Python 包）。
- en: '[PRE6]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Open settings.py and add ‘rest_framework’, ‘celery’, and ‘storages’ (needed
    for boto3/AWS) in the INSTALLED_APPS list to register those packages with the
    Django project.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 打开 settings.py，并将‘rest_framework’，‘celery’和‘storages’（需要用于 boto3/AWS）添加到 INSTALLED_APPS
    列表中，以将这些包注册到 Django 项目中。
- en: 'In the root dir, let’s create an app which will house the core functionality
    of our backend. Issue another terminal command:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在根目录中，让我们创建一个应用程序，用于存放后端的核心功能。执行另一个终端命令：
- en: '[PRE7]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This will create an app in the root dir called docreader.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在根目录中创建一个名为 docreader 的应用程序。
- en: 'Let’s also create a file in docreader titled mltask.py. In it, define a simple
    function for testing our setup that takes in a variable, file_path, and prints
    it out:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将在 docreader 中创建一个名为 mltask.py 的文件。在其中，定义一个简单的函数来测试我们的设置，该函数接收一个变量 file_path
    并打印出来：
- en: '[PRE8]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now getting to structure, Django apps use the [Model View Controller](https://www.geeksforgeeks.org/mvc-design-pattern/)
    (MVC) design pattern, defining the Model in [models.py](https://docs.djangoproject.com/en/5.0/topics/db/models/),
    View in [views.py](https://docs.djangoproject.com/en/5.0/topics/http/views/),
    and Controller in Django [Templates](https://docs.djangoproject.com/en/5.0/topics/templates/)
    and [urls.py](https://docs.djangoproject.com/en/5.0/topics/http/urls/). Using
    Django Rest Framework, we will include serialization in this pipeline, which provide
    a way of serializing and deserializing native Python data structures into representations
    such as json. Thus, the application logic for exposing an endpoint is as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在进入结构部分，Django 应用程序使用 [模型-视图-控制器](https://www.geeksforgeeks.org/mvc-design-pattern/)（MVC）设计模式，定义了在
    [models.py](https://docs.djangoproject.com/en/5.0/topics/db/models/) 中的模型， [views.py](https://docs.djangoproject.com/en/5.0/topics/http/views/)
    中的视图，以及在 Django [模板](https://docs.djangoproject.com/en/5.0/topics/templates/)
    和 [urls.py](https://docs.djangoproject.com/en/5.0/topics/http/urls/) 中的控制器。使用
    Django Rest Framework，我们将在此管道中包含序列化，它提供了一种将本地 Python 数据结构序列化和反序列化为表示形式（如 json）的方法。因此，公开端点的应用程序逻辑如下：
- en: Database ← → models.py ← → serializers.py ← → views.py ← → urls.py
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数据库 ← → models.py ← → serializers.py ← → views.py ← → urls.py
- en: 'In docreader/models.py, write the following:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在 docreader/models.py 中，写入以下内容：
- en: '[PRE9]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This sets up a model Document that will require a title and file for each entry
    saved in the database. Once saved, the @receiver decorator listens for a post
    save signal, meaning that the specified model, Document, was saved in the database.
    Once saved, user_created_handler() takes the saved instance’s file field and passes
    it to, what will become, our Machine Learning function.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这将设置一个模型 Document，该模型要求每个条目在数据库中保存时都必须有一个标题和文件。一旦保存，@receiver 装饰器会监听一个保存后的信号，意味着指定的模型
    Document 已经保存到数据库。一旦保存，user_created_handler() 将获取保存实例的文件字段，并将其传递给我们的机器学习功能。
- en: 'Anytime changes are made to models.py, you will need to run the following two
    commands:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 每当对 models.py 进行更改时，您需要运行以下两个命令：
- en: '[PRE10]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Moving forward, create a serializers.py file in docreader, allowing for the
    serialization and deserialization of the Document’s title and file fields. Write
    in it:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在 docreader 中创建一个 serializers.py 文件，允许序列化和反序列化 Document 的标题和文件字段。在其中写入：
- en: '[PRE11]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next in views.py, where we can define our CRUD operations, let’s define the
    ability to create, as well as list, Document entries using [generic views](https://www.django-rest-framework.org/api-guide/generic-views/)
    (which essentially allows you to quickly write views using an abstraction of common
    view patterns):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在 views.py 中，我们可以定义我们的 CRUD 操作，让我们定义创建和列出文档条目的功能，使用 [通用视图](https://www.django-rest-framework.org/api-guide/generic-views/)（它本质上允许您使用常见视图模式的抽象快速编写视图）：
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, update urls.py in mltutorial:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，更新 mltutorial 中的 urls.py：
- en: '[PRE13]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'And create urls.py in docreader app dir and write:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在docreader应用目录中创建urls.py并写入：
- en: '[PRE14]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now we are all setup to save a Document entry, with title and field fields,
    at the /api/create/ endpoint, which will call mltask() post save! So, let’s test
    this out.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好在/api/create/端点保存一个包含标题和字段的文档条目，保存后将调用mltask()！所以，让我们测试一下。
- en: To help visualize testing, let’s register our Document model with the Django
    [admin interface](https://docs.djangoproject.com/en/5.0/ref/contrib/admin/), so
    we can see when a new entry has been created.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助可视化测试，让我们将Document模型注册到Django的[管理员界面](https://docs.djangoproject.com/en/5.0/ref/contrib/admin/)，这样我们就可以看到新条目何时被创建。
- en: 'In docreader/admin.py write:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在docreader/admin.py中编写：
- en: '[PRE15]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Create a user that can login to the Django admin interface using:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个可以登录Django管理员界面的用户，使用：
- en: '[PRE16]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now, let’s test the endpoint we exposed.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们测试我们暴露的端点。
- en: 'To do this without a frontend, run the Django server and go to Postman. Send
    the following POST request with a PDF file attached:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有前端，可以运行Django服务器并打开Postman。发送以下POST请求，附带PDF文件：
- en: '![](../Images/b9a6e26e977b4778e759baa5aca0f5fa.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b9a6e26e977b4778e759baa5aca0f5fa.png)'
- en: If we check our Django logs, we should see the file path printed out, as specified
    in the post save mltask() function call.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们检查Django日志，应该会看到文件路径被打印出来，如在保存后调用mltask()函数中所指定的。
- en: AWS Setup
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS设置
- en: You will notice that the PDF was saved to the project’s root dir. Let’s ensure
    any media is instead saved to AWS S3, getting our app ready for deployment.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到PDF已保存到项目的根目录。让我们确保将媒体文件保存到AWS S3中，为部署做好准备。
- en: Go to the [S3 console](https://s3.console.aws.amazon.com/) (and create an account
    and get our your account’s [Access and Secret keys](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html)
    if you haven’t already). Create a new bucket, here we will be titling it ‘djangomltest’.
    Update the permissions to ensure the bucket is public for testing (and revert
    back, as needed, for production).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 转到[S3控制台](https://s3.console.aws.amazon.com/)（如果你还没有账号，请创建一个并获取你的账号[访问密钥和秘密密钥](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html)）。创建一个新的存储桶，这里我们将命名为“djangomltest”。更新权限，确保存储桶对测试是公开的（并根据需要恢复，以便用于生产环境）。
- en: Now, let’s configure Django to work with AWS.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们配置Django以与AWS配合使用。
- en: 'Add your model_final.pth, trained in [Part 1](https://medium.com/towards-data-science/training-and-deploying-a-custom-detectron2-model-for-object-detection-using-pdf-documents-part-1-c724f61d8b4b),
    into the docreader dir. Create a .env file in the root dir and write the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 将你在[第1部分](https://medium.com/towards-data-science/training-and-deploying-a-custom-detectron2-model-for-object-detection-using-pdf-documents-part-1-c724f61d8b4b)中训练好的model_final.pth文件放入docreader目录。在根目录创建一个.env文件，并写入以下内容：
- en: '[PRE17]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Update settings.py to include AWS configurations:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 更新settings.py以包括AWS配置：
- en: '[PRE18]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Optionally, with AWS serving our static and media files, you will want to run
    the following command in order to serve static assets to the admin interface using
    S3:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，如果AWS为我们的静态和媒体文件提供服务，你将需要运行以下命令，以通过S3将静态资源提供给管理员界面：
- en: '[PRE19]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: If we run the server again, our admin should appear the same as how it would
    with our static files served locally.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们再次运行服务器，我们的管理员界面应该与本地提供静态文件时一样显示。
- en: Once again, let’s run the Django server and test the endpoint to make sure the
    file is now saved to S3.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 再次让我们运行Django服务器，并测试端点，确保文件现在已保存到S3。
- en: ML Task Setup and Deployment
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ML任务设置与部署
- en: With Django and AWS properly configured, let’s set up our ML process in mltask.py.
    As the file is long, see the repo [here](https://github.com/nzh2534/mltutorial/blob/main/docreader/mltask.py)
    for reference (with comments added in to help with understanding the various code
    blocks).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在Django和AWS配置正确后，让我们在mltask.py中设置我们的机器学习过程。由于文件较长，请参考此[repo](https://github.com/nzh2534/mltutorial/blob/main/docreader/mltask.py)（已添加注释以帮助理解各个代码块）。
- en: What’s important to see is that Detectron2 is imported and the model is loaded
    only when the function is called. Here, we will call the function only through
    a Celery task, ensuring the memory used during inferencing will be isolated to
    the Heroku worker process.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要看到，Detectron2仅在函数被调用时才会导入，并且模型在调用时才会加载。在这里，我们将仅通过Celery任务调用该函数，确保推理过程中使用的内存将仅限于Heroku工作进程。
- en: So finally, let’s setup Celery and then deploy to Heroku.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们设置Celery并将其部署到Heroku。
- en: 'In mltutorial/_init__.py write:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在mltutorial/_init__.py中编写：
- en: '[PRE20]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Create celery.py in the mltutorial dir and write:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在mltutorial目录中创建celery.py并编写：
- en: '[PRE21]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Lastly, make a tasks.py in docreader and write:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在docreader中创建tasks.py并编写：
- en: '[PRE22]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This Celery task, ml_celery_task(), should now be imported into models.py and
    used with the post save signal instead of the mltask function pulled directly
    from mltask.py. Update the post_save signal block to the following:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这个Celery任务ml_celery_task()应该被导入到models.py中，并与post save信号一起使用，而不是直接从mltask.py中拉取的mltask函数。将post_save信号块更新为以下内容：
- en: '[PRE23]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: And to test Celery, let’s deploy!
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试Celery，让我们进行部署！
- en: In the root project dir, include a Dockerfile and heroku.yml file, both specified
    in the [repo](https://github.com/nzh2534/mltutorial/tree/main). Most importantly,
    editing the heroku.yml *commands* will allow you to configure the gunicorn web
    process and the Celery worker process, which can aid in further mitigating potential
    problems.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在项目根目录下，包含一个Dockerfile和heroku.yml文件，二者在[仓库](https://github.com/nzh2534/mltutorial/tree/main)中有指定。最重要的是，编辑heroku.yml中的*commands*，可以让你配置gunicorn
    web进程和Celery工作进程，这将有助于进一步减少潜在问题。
- en: 'Make a Heroku account and create a new app called “mlapp” and gitignore the
    .env file. Then initialize git in the projects root dir and change the Heroku
    app’s stack to container (in order to deploy using Docker):'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个Heroku账户，并创建一个名为“mlapp”的新应用，同时将.env文件加入.gitignore。然后，在项目的根目录初始化git，并将Heroku应用的堆栈更改为容器（以便使用Docker进行部署）：
- en: '[PRE24]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Once pushed, we just need to add our env variables into the Heroku app.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦推送完成，我们只需要将环境变量添加到Heroku应用程序中。
- en: Go to settings in the online interface, scroll down to Config Vars, click Reveal
    Config Vars, and add each line listed in the .env file.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 进入在线界面的设置，滚动到配置变量（Config Vars）部分，点击“显示配置变量”（Reveal Config Vars），并添加.env文件中列出的每一行。
- en: '![](../Images/0cb721c6c2ac8c4e9fbadf33d1aa882f.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0cb721c6c2ac8c4e9fbadf33d1aa882f.png)'
- en: You may have noticed there was a CLOUDAMQP_URL variable specified in celery.py.
    We need to provision a Celery Broker on Heroku, for which there are a variety
    of options. I will be using [CloudAMQP](https://elements.heroku.com/addons/cloudamqp)
    which has a free tier. Go ahead and add this to your app. Once added, the CLOUDAMQP_URL
    environment variable will be included automatically in the Config Vars.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到在celery.py中指定了一个CLOUDAMQP_URL变量。我们需要在Heroku上配置一个Celery Broker，针对这个需求有多种选择。我将使用[CloudAMQP](https://elements.heroku.com/addons/cloudamqp)，它提供了一个免费的层级。请继续并将其添加到你的应用程序中。添加后，CLOUDAMQP_URL环境变量将会自动包含在配置变量中。
- en: Finally, let’s test the final product.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们测试最终产品。
- en: 'To monitor requests, run:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 要监控请求，请运行：
- en: '[PRE25]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Issue another Postman POST request to the Heroku app’s url at the /api/create/
    endpoint. You will see the POST request come through, Celery receive the task,
    load the model, and start running pages:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 对Heroku应用的URL发送另一个Postman POST请求，使用/api/create/端点。你会看到POST请求传输过来，Celery接收任务，加载模型并开始运行页面：
- en: '![](../Images/96ab69c4cda3ae0833d35f08f528b544.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96ab69c4cda3ae0833d35f08f528b544.png)'
- en: We will continue to see the “Running for page…” until the end of the process
    and you can check the AWS S3 bucket as it runs.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在整个过程结束之前继续看到“Running for page...”，你可以在此过程中检查AWS S3桶的状态。
- en: Congrats! You’ve now deployed and ran a Python backend using Machine Learning
    as a part of a distributed task queue running in parallel to the main web process!
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你现在已经成功部署并运行了一个使用机器学习的Python后端，该后端作为分布式任务队列的一部分，与主Web进程并行运行！
- en: As mentioned, you will want to adjust the heroku.yml *commands* to incorporate
    gunicorn threads and/or worker processes and fine tune celery. For further learning,
    here’s a [great article](https://medium.com/building-the-system/gunicorn-3-means-of-concurrency-efbb547674b7)
    on configuring gunicorn to meet your app’s needs, one for digging into [Celery
    for production](https://progressstory.com/tech/python/production-ready-celery-configuration/),
    and another for exploring Celery [worker pools](https://celery.school/celery-worker-pools),
    in order to help with properly managing your resources.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，你需要调整heroku.yml中的*commands*以包含gunicorn线程和/或工作进程，并对celery进行微调。如果需要进一步学习，这里有一篇[优秀文章](https://medium.com/building-the-system/gunicorn-3-means-of-concurrency-efbb547674b7)，讲解如何配置gunicorn以满足你的应用需求；还有一篇关于[生产环境中使用Celery](https://progressstory.com/tech/python/production-ready-celery-configuration/)的文章，以及另一篇关于探索Celery[工作池](https://celery.school/celery-worker-pools)的文章，可以帮助你更好地管理资源。
- en: Happy coding!
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 编程愉快！
- en: '*Unless otherwise noted, all images used in this article are by the author*'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，本文中使用的所有图片均为作者提供*'
