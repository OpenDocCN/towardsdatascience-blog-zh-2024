- en: 'Line-By-Line, Let’s Reproduce GPT-2: Section 1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逐行复现GPT-2：第一部分
- en: 原文：[https://towardsdatascience.com/line-by-line-lets-reproduce-gpt-2-section-1-b26684f98492?source=collection_archive---------8-----------------------#2024-07-23](https://towardsdatascience.com/line-by-line-lets-reproduce-gpt-2-section-1-b26684f98492?source=collection_archive---------8-----------------------#2024-07-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/line-by-line-lets-reproduce-gpt-2-section-1-b26684f98492?source=collection_archive---------8-----------------------#2024-07-23](https://towardsdatascience.com/line-by-line-lets-reproduce-gpt-2-section-1-b26684f98492?source=collection_archive---------8-----------------------#2024-07-23)
- en: This blog post will go line-by-line through the code in Section 1 of Andrej
    Karpathy’s “Let’s reproduce GPT-2 (124M)”
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本文将逐行分析Andrej Karpathy的“让我们复现GPT-2（124M）”第一部分的代码。
- en: '[](https://medium.com/@mgunton7?source=post_page---byline--b26684f98492--------------------------------)[![Matthew
    Gunton](../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png)](https://medium.com/@mgunton7?source=post_page---byline--b26684f98492--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b26684f98492--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b26684f98492--------------------------------)
    [Matthew Gunton](https://medium.com/@mgunton7?source=post_page---byline--b26684f98492--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mgunton7?source=post_page---byline--b26684f98492--------------------------------)[![Matthew
    Gunton](../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png)](https://medium.com/@mgunton7?source=post_page---byline--b26684f98492--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b26684f98492--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b26684f98492--------------------------------)
    [Matthew Gunton](https://medium.com/@mgunton7?source=post_page---byline--b26684f98492--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b26684f98492--------------------------------)
    ·21 min read·Jul 23, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b26684f98492--------------------------------)
    ·21分钟阅读·2024年7月23日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/6eaaa728ea0ae6b47ebb70bbe5b558fc.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6eaaa728ea0ae6b47ebb70bbe5b558fc.png)'
- en: Image by Author — SDXL
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像 — SDXL
- en: Andrej Karpathy is one of the foremost Artificial Intelligence (AI) researchers
    out there. He is a founding member of OpenAI, previously led AI at Tesla, and
    continues to be at the forefront of the AI community. He recently released an
    incredible [4 hour video walking through how to build a high-quality LLM model
    from scratch](https://www.youtube.com/watch?v=l8pRSuU81PU).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Andrej Karpathy是人工智能（AI）领域的领军人物之一。他是OpenAI的创始成员之一，曾领导Tesla的AI团队，并继续活跃在AI社区的前沿。他最近发布了一段精彩的[4小时视频，讲解如何从零开始构建一个高质量的大型语言模型（LLM）](https://www.youtube.com/watch?v=l8pRSuU81PU)。
- en: In that video, we go through all of the major parts of training an LLM, from
    coding the architecture to speeding up its training time to adjusting the hyperparameters
    for better results. There’s an incredible amount of knowledge there, so I wanted
    to expand upon it by going line-by-line through the code Karpathy creates and
    explaining how it is working. This blog post will be part of a series I do covering
    each section of Karpathy’s video.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在那个视频中，我们逐一讲解了训练大型语言模型（LLM）的所有主要部分，从编写架构代码到加速训练时间，再到调整超参数以获得更好的结果。这里包含了大量的知识，因此我希望通过逐行解析Karpathy创建的代码，进一步展开讲解其工作原理。本文将是我系列文章的一部分，内容将涵盖Karpathy视频的每个部分。
- en: In section one, we focus on implementing the architecture of GPT-2\. While GPT-2
    was open-sourced by OpenAI in 2018, it was written in Tensor Flow, which is a
    harder framework to debug than PyTorch. Consequently, we are going to recreate
    GPT-2 using more commonly used tools. Using only the code we are going to create
    today, you can create a LLM of your own!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一部分中，我们专注于实现GPT-2的架构。尽管GPT-2在2018年由OpenAI开源，但它是用TensorFlow编写的，这比PyTorch更难调试。因此，我们将使用更常用的工具重新构建GPT-2。仅凭我们今天要创建的代码，你就能创建自己的大型语言模型（LLM）！
- en: Let’s dive in!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: High Level Vocabulary
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级词汇
- en: Before we begin, let’s get on the same page about some terminology. While there
    may be some naming collisions with other sources, I’ll try to be consistent within
    these blog posts.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，先来统一一下术语。虽然可能与其他来源的命名有所冲突，但我会尽量在这些博客文章中保持一致。
- en: '*Block Size* — tells us how many positions in the input length our Transformer
    can process. Once you go over this limit, performance degrades as you have to
    wrap around (y[ou can learn more about how we expand this without training a new
    model from scratch in my Long RoPE Blog](/understanding-long-rope-in-llms-29337dc7e4a9))'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*块大小* — 告诉我们Transformer可以处理输入长度中多少个位置。一旦超过这个限制，性能会下降，因为你必须进行循环（[你可以在我的《长RoPE博客》](/understanding-long-rope-in-llms-29337dc7e4a9)中了解如何在不从头训练新模型的情况下扩展这一点）。'
- en: '*Vocabulary Size* — tells us how many unique tokens the model will be able
    to understand and use. In general, researchers have found that larger vocabulary
    sizes allow models to be more precise with their language and to capture more
    nuances in their responses.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*词汇表大小* — 告诉我们模型能够理解和使用多少个独特的标记。通常，研究人员发现，较大的词汇表大小使模型能够更精确地使用语言，并捕捉到更多细微的回应。'
- en: '*Layer —* part of the hidden layers of our neural network. Specifically here
    we refer to how many times we repeat the calculations shown in the grey box below:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*层* — 我们神经网络中的隐藏层的一部分。具体来说，这里指的是我们在下面的灰色框中重复进行计算的次数：'
- en: '![](../Images/eaf2574408a9f1c397a50330b3e663ab.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eaf2574408a9f1c397a50330b3e663ab.png)'
- en: A layer in our model from [“Attention is All You Need”](https://arxiv.org/pdf/1706.03762)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型中的一层来自[《Attention is All You Need》](https://arxiv.org/pdf/1706.03762)
- en: '*Embedding* — a vector representation of data we pass to the model.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*嵌入* — 我们传递给模型的数据的向量表示。'
- en: '*Multi-Head Attention* — rather than running attention once, we run it n-times
    and then concatenate all of the results together to get the final result.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*多头注意力* — 我们不是只运行一次注意力，而是运行n次，然后将所有结果连接起来，得到最终结果。'
- en: Let’s go into the code!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进入代码吧！
- en: GPT Class & Its Parameters
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT 类及其参数
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: To begin, we are setting 5 hyper-parameters in the GPTConfig class. `block_size`
    appears to be somewhat arbitrary along with `n_layer`and `n_head`. Put differently,
    these values were chosen empirically based on what the researchers saw had the
    best performance. Moreover, we choose 786 for `n_embd` as this is the value chosen
    for the GPT-2 paper, which we’ve decided to emulate.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们在GPTConfig类中设置了5个超参数。`block_size`与`n_layer`和`n_head`一样，看起来有些随意。换句话说，这些值是根据研究人员所观察到的最佳性能经验选择的。此外，我们为`n_embd`选择了786，这是GPT-2论文中选定的值，我们决定进行模拟。
- en: However, `vocab_size` is set based off the `tiktoken` gpt-2 tokenizer that we
    will use. The GPT-2 tokenizer was created by using the *Byte-Pair Encoding* algorithm
    ([read more here](https://www.geeksforgeeks.org/byte-pair-encoding-bpe-in-nlp/)).
    This starts off with an initial set of vocab (in our case 256) and then goes through
    the training data creating new vocab based on the frequency it sees the new vocabulary
    appearing in the training set. It keeps doing this until it has hit a limit (in
    our case 50,000). Finally, we have vocab set aside for internal use (in our case
    the end token character). Adding these up we get 50,257.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，`vocab_size`是基于我们将使用的`tiktoken` GPT-2分词器设置的。GPT-2分词器是通过使用*字节对编码*算法创建的（[在这里阅读更多](https://www.geeksforgeeks.org/byte-pair-encoding-bpe-in-nlp/)）。它从初始的词汇表（在我们的案例中为256）开始，然后遍历训练数据，根据新词汇在训练集中的出现频率创建新的词汇。它不断重复这一过程，直到达到限制（在我们的案例中为50,000）。最后，我们为内部使用预留了词汇（在我们的案例中为结束标记字符）。将这些值相加，我们得到50,257。
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: With our configs set, we create a GPT class which is an instance of the torch
    `nn.Module` class. This is the base class for all PyTorch neural networks, and
    so by using this we get access to all of the optimizations that PyTorch has for
    these types of models. Each `nn.Module` will have a `forward` function that defines
    what happens during a forward pass of the model (more on these in a moment).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 设置好我们的配置后，我们创建了一个GPT类，它是torch `nn.Module`类的一个实例。这个类是所有PyTorch神经网络的基类，因此通过使用这个类，我们可以访问PyTorch为这些类型的模型提供的所有优化。每个`nn.Module`都有一个`forward`函数，定义了模型在前向传播过程中发生的事情（稍后会详细讲解）。
- en: We begin by running the super constructor in the base class and then create
    a `transformer` object as a `ModuleDict`. This was created because it allows us
    to index into `transformer` like an object, which will come in handy both when
    we want to load in weights from HuggingFace and when we want to debug and quickly
    go through our model.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先运行基类中的超构造函数，然后将`transformer`对象创建为`ModuleDict`。这样做是因为它允许我们像操作对象一样索引`transformer`，这在我们想从HuggingFace加载权重时以及调试并快速浏览模型时都会非常有用。
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Our `transformer` here has 4 major pieces we are going to load in: the weights
    of the token embeddings (`wte`), the weights of the positional encodings (`wpe`),
    the hidden layers (`h`), and the layer normalization (`ln_f`). This setup is following
    *mostly* the decoder part of the Transformer architecture from [“Attention is
    All You Need”](https://arxiv.org/pdf/1706.03762) (output embeddings ~ `wte`, positional
    encoding ~ `wte`, hidden layers ~`h` ). One key difference is that we have an
    additional normalization layer `ln_f` done after all of the hidden layers have
    finished in our architecture.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`transformer`在这里有四个主要部分要加载：标记嵌入的权重（`wte`）、位置编码的权重（`wpe`）、隐藏层（`h`）和层归一化（`ln_f`）。这个设置*大致*遵循了[《Attention
    is All You Need》](https://arxiv.org/pdf/1706.03762)中Transformer架构的解码器部分（输出嵌入 ~
    `wte`，位置编码 ~ `wte`，隐藏层 ~ `h`）。一个关键的区别是，我们的架构中在所有隐藏层完成后增加了一个额外的归一化层`ln_f`。
- en: '![](../Images/a919d91bb480f5f3fede3f15e3f10fc5.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a919d91bb480f5f3fede3f15e3f10fc5.png)'
- en: Decoder Half of the Architecture shown in [“Attention is All You Need”](https://arxiv.org/pdf/1706.03762)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器架构部分，见[《Attention is All You Need》](https://arxiv.org/pdf/1706.03762)
- en: The `wte` and the `wpe` are both embeddings so naturally we use the `nn.Embedding`
    class to represent them. Our hidden layers are where we will have most of the
    logic for the Transformer, so I will go into this more later. For now, just note
    that we are creating a loop of the object `Block` so that we have `n.layer`‘s
    of them. Finally, we use the built-in `nn.LayerNorm` for `ln_f` , which will normalize
    our output based on the equation below (where x and y are input and output, E[x]
    is the mean value, and γ and β are learnable weights).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`wte`和`wpe`都是嵌入，因此我们自然使用`nn.Embedding`类来表示它们。我们的隐藏层是Transformer中大部分逻辑所在的地方，所以我稍后会详细讲解。现在只需注意，我们正在创建一个`Block`对象的循环，以便拥有`n.layer`个它们。最后，我们使用内建的`nn.LayerNorm`来处理`ln_f`，它将根据下面的公式对我们的输出进行归一化（其中x和y分别是输入和输出，E[x]是均值，γ和β是可学习的权重）。'
- en: '![](../Images/5a98c158cb71d90f31987dcb5e82662e.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a98c158cb71d90f31987dcb5e82662e.png)'
- en: Equation for [Layer Normalization in PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[PyTorch中层归一化的公式](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)'
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Next, we setup the final linear layer of our network which will generate the
    logits of the model. Here we are projecting from the embedding dimension of our
    model (768) to the vocabulary size of our model (50,257). The idea here is that
    we have taken the hidden state and expanded it to map onto our vocabulary so that
    our decoder head can use the values on each vocab to figure out what the next
    token should be.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们设置网络的最后一层线性层，用于生成模型的logits。在这里，我们将从模型的嵌入维度（768）映射到模型的词汇表大小（50,257）。这里的想法是，我们将隐藏状态扩展到映射到我们的词汇表，以便我们的解码器头可以使用每个词汇上的值来确定下一个标记应该是什么。
- en: Finally in our constructor, we have an interesting optimization where we tell
    the model to make the tokenizer weights the same as the linear layer weights.
    This is done because we want the linear layer and the tokenizer to have the same
    understanding of the tokens (if two tokens are similar when being input into the
    model, the same two tokens should be similar when being output by the model).
    Finally, we initialize the weights for the model so we can start training.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在我们的构造函数中，我们进行了一个有趣的优化，其中我们告诉模型将分词器的权重与线性层的权重设为相同。这样做的原因是，我们希望线性层和分词器对标记有相同的理解（如果两个标记在输入模型时相似，那么这两个标记在模型输出时也应该是相似的）。最后，我们初始化模型的权重，以便可以开始训练。
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Our forward function is where we lay out exactly how our model will behave during
    a forward pass. We start off by verifying that our sequence length is not greater
    than our configured max value (`block_size`). Once that’s true, we create a tensor
    with values of 0 to T-1 (for example if T = 4, we’d have tensor([0, 1, 2, 3])
    and run them through our positional embedding weights. Once that’s complete, we
    run the input tensor through the token embedding weights.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的前向函数是我们详细描述模型在前向传播过程中行为的地方。我们首先验证我们的序列长度是否不大于我们配置的最大值（`block_size`）。一旦这点成立，我们创建一个值为0到T-1的张量（例如，如果T=4，我们将得到tensor([0,
    1, 2, 3]），并通过位置嵌入权重运行它们。完成之后，我们将输入张量通过标记嵌入权重。
- en: We combine both the token and the positional embeddings into `x`, requiring
    a broadcast to combine them. As the `tok_emb` are bigger than the `pos_emb` (in
    our example 50257 vs 1024), x will have the dimensions of `tok_emb` . `x` is now
    our hidden state, which we will pass through the hidden layers via the for loop.
    We are careful to update `x` after each time through a Block.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将**token**和位置嵌入结合成`x`，需要广播操作来将它们合并。由于`tok_emb`比`pos_emb`大（在我们的示例中是50257对1024），因此`x`的维度将与`tok_emb`相同。`x`现在是我们的隐藏状态，我们将在隐藏层中通过for循环传递它。我们小心地在每次通过Block后更新`x`。
- en: Next, we normalize `x` via our LayerNormalization `ln_f` and then do our linear
    projection to get the logits necessary to predict the next token. If we are training
    the model (which we signal via the `targets` parameter), we will then compute
    cross entropy between the logits we have just produced and the ground truth values
    held in our `targets` variable. We accomplish this via our `cross_entropy` loss
    function. To do this right, we need to convert our `logits` and `target` to the
    right shape via `.view()`. We ask pytorch to infer the correct size when we pass
    through -1.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过LayerNormalization `ln_f`对`x`进行归一化，然后进行线性投影以获取预测下一个token所需的logits。如果我们正在训练模型（通过`targets`参数来指示），我们将计算我们刚刚生成的logits与存储在`targets`变量中的真实标签之间的交叉熵。我们通过`cross_entropy`损失函数来实现这一点。为了正确执行，我们需要通过`.view()`将`logits`和`target`转换为正确的形状。当我们通过-1传递时，我们让pytorch推断出正确的大小。
- en: There’s one more function in this class, the initialization function, but we’ll
    get to the initialization logic a little later. For now, let’s dive into the Block
    logic that will help us implement our multi-head attention and MLPs.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类还有一个函数是初始化函数，不过我们稍后会讲到初始化逻辑。现在，让我们深入了解Block逻辑，它将帮助我们实现多头注意力和MLP。
- en: Block Class
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Block 类
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Block is instantiated as a `nn.Module` , so we also call the super constructor
    at the beginning for its optimizations. Next, we setup the same calculations as
    set out in the [“Attention is All You Need”](https://arxiv.org/pdf/1706.03762)
    paper — 2 layer normalizations, an attention calculation, and a feed forward layer
    via MLPs.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Block被实例化为`nn.Module`，因此我们还在开始时调用了父类构造函数进行优化。接下来，我们按照[《Attention is All You
    Need》](https://arxiv.org/pdf/1706.03762)论文中的计算设置相同的操作——2个层归一化、一个注意力计算和通过MLPs的前馈层。
- en: '![](../Images/eaf2574408a9f1c397a50330b3e663ab.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eaf2574408a9f1c397a50330b3e663ab.png)'
- en: A Hidden Layer from [“Attention is All You Need”](https://arxiv.org/pdf/1706.03762)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[《Attention is All You Need》](https://arxiv.org/pdf/1706.03762)中的一个隐藏层'
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We then define our `forward` function which PyTorch will call for every forward
    pass of the model. Note that this is where we do something different than Attention
    is All You Need. We setup the layer normalizations to happen before attention
    and the feedforward respectively. This is part of the insights from GPT-2 paper,
    and you can see how making little changes like this can make a big difference.
    Note the addition to the original tensor remains in the corresponding same position.
    These 2 additions will be important when we setup our weight initialization function.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义了`forward`函数，PyTorch将在每次模型前向传递时调用它。请注意，这里我们做了与《Attention is All You Need》不同的事情。我们设置了层归一化，分别发生在注意力和前馈层之前。这是GPT-2论文中的一个重要见解，您可以看到，像这样做一些小的改变，能够带来很大的不同。注意，原始张量的加法仍然保留在相应的位置。这两个加法将在我们设置权重初始化函数时变得非常重要。
- en: This class is a nice abstraction, as it lets us swap out implementations of
    attention or choose another type of feed forward function other than MLP without
    having to majorly refactor the code.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类是一个很好的抽象，它允许我们替换注意力机制的实现或选择另一种类型的前馈函数，而不需要大规模重构代码。
- en: CausalSelfAttention Class
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CausalSelfAttention 类
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Attention is an important part of our model, so naturally there are a number
    of configurations here. We have the assert statement as a debugging tool to make
    sure that the configuration dimensions we pass through are compatible. Then we
    create some helper functions that will assist us when we do our self-attention.
    First, we have our `c_attn` and `c_proj` which are linear projections that convert
    our hidden state into new dimensions needed for the attention calculation. The
    `c_proj.NANOGPT_SCALE_INIT` is a flag we set here and in the MLP that will help
    us with the weight initialization later (in truth this could be named anything).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力是我们模型的一个重要部分，因此这里自然有很多配置项。我们使用断言语句作为调试工具，确保我们传递的配置维度是兼容的。接着，我们创建了一些辅助函数，帮助我们在进行自注意力时使用。首先，我们有`c_attn`和`c_proj`，它们是线性投影，将我们的隐藏状态转换为注意力计算所需的新维度。`c_proj.NANOGPT_SCALE_INIT`是我们在这里和MLP中设置的一个标志，帮助我们稍后的权重初始化（事实上，这个名字可以取任何名字）。
- en: Finally, we tell torch to create a buffer that will not be updated during training
    called `bias`. Bias will be a lower triangular matrix of dimensions `block_size`
    x `block_size` that we will then turn into a 4D tensor with dimensions 1 x 1 x
    `block_size` x `block_size` . The 1 x 1 is done so that we can compute these in
    a batch in a single channel. This buffer will be used to apply a mask on our multi-headed
    attention.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们告诉torch创建一个在训练过程中不会更新的缓冲区，称为`bias`。偏置将是一个尺寸为`block_size` x `block_size`的下三角矩阵，接着我们将其转换为尺寸为1
    x 1 x `block_size` x `block_size`的4D张量。1 x 1的维度是为了能够在单个通道中批量计算这些数据。这个缓冲区将用于在我们的多头注意力中应用掩码。
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now comes the implementation of attention, with a focus on making this performant
    in torch. Going line by line, we begin by finding the batch size, sequence length,
    and channels in our input tensor x. We then will call our `c_attn` from before
    to project our hidden state into the dimensions we’ll need. We then split that
    result into 3 tensors of (B, T, C) shape (specifically one for query, one for
    key, and one for value).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是注意力的实现，重点是使其在torch中具有高性能。逐行分析，我们首先找到输入张量x中的批量大小、序列长度和通道数。接着我们会调用之前的`c_attn`，将隐藏状态投影到所需的维度。然后，我们将结果分割成3个形状为(B,
    T, C)的张量（具体来说，一个用于查询，一个用于键，一个用于值）。
- en: We then adjust the dimensions of q, k, and v so that we can do multi-head attention
    on these performantly. By changing the dimensions from (B, T, C) to (B, T, self.n_head,
    C // self.n_head), we are dividing up the data so that each head gets its own
    unique data to operate on. We transpose our view so that we can make `T` the third
    dimension and `self.n_head` the second dimension, allowing us to more easily concatenate
    the heads.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们调整q、k和v的维度，以便能够高效地进行多头注意力。通过将维度从(B, T, C)更改为(B, T, self.n_head, C // self.n_head)，我们将数据划分开来，让每个头（head）获得自己的独特数据进行操作。我们转置视图，以便将T变为第三维，将`self.n_head`变为第二维，从而更方便地将头部拼接起来。
- en: '![](../Images/bb9433415775d70b3ab32f88ec34a6e3.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bb9433415775d70b3ab32f88ec34a6e3.png)'
- en: Attention equation from “[Attention is All You Need](https://youtu.be/l8pRSuU81PU?feature=shared)”
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 来自“[Attention is All You Need](https://youtu.be/l8pRSuU81PU?feature=shared)”的注意力公式
- en: Now that we have our values, we can start to calculate. We perform a matrix
    multiplication between query and key (making sure to transpose key so that it
    is in the proper direction), then divide by the square root of the size of k.
    After this calculation, we then apply the bias from our register so that the attention
    data from tokens in the future cannot impact tokens in the present (hence why
    we apply the mask only for tokens greater than T for the time and channel dimension).
    Once that is complete, we apply the softmax to only pass through certain information
    through.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了值，可以开始计算了。我们对查询（query）和键（key）进行矩阵乘法（确保将键转置，使其处于正确的方向），然后除以k的大小的平方根。完成此计算后，我们再应用来自寄存器的偏置，以确保未来的标记（tokens）的注意力数据不会影响当前的标记（因此我们只对时间和通道维度中大于T的标记应用掩码）。完成后，我们应用softmax，仅通过某些信息进行传递。
- en: Once the mask is on, we multiply the values by v, and then transpose our values
    back to (B, T, self.n_head, C // self.n_head) setup. We call `.contiguous()` to
    ensure that in memory all of the data is laid out next to each other, and finally
    convert our tensor back to the (B, T, C) dimensions it came in with (thus, concatenating
    our attention heads in this step).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦掩码被应用，我们将值乘以v，然后将我们的值转置回（B，T，self.n_head，C // self.n_head）的设置。我们调用`.contiguous()`以确保在内存中所有数据都按顺序排列，最后将张量转换回（B，T，C）维度，这样就完成了在此步骤中拼接注意力头的操作。
- en: Finally, we use our linear projection `c_proj` to convert back to the original
    dimensions of the hidden state.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用线性投影`c_proj`将其转换回隐藏状态的原始维度。
- en: MLP Class
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLP类
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Like all the classes before, MLP inherits from `nn.Module`. We begin by setting
    some helper functions — specifically the `c_fc` and `c_proj` linear projection
    layers, expanding from our embedding to 4 times the size and then back again respectively.
    Next, we have GELU. Karpathy makes a point to say that the approximate parameter
    here is only set so that we can closely match the GPT-2 paper. While at the time,
    the approximation of GELU was necessary, now a days we no longer need to approximate
    — we can calculate precisely.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前所有的类一样，MLP继承自`nn.Module`。我们首先设置一些辅助函数——特别是`c_fc`和`c_proj`线性投影层，分别将我们的嵌入扩展到原来的4倍大小，再缩回原来大小。接下来，我们使用GELU。Karpathy特别指出，近似参数这里只是为了与GPT-2论文中的结果尽可能接近。虽然当时，GELU的近似是必要的，但现在我们不再需要近似——我们可以精确计算。
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Our forward pass then is relatively straight forward. We call each function
    on our input tensor and return the final result.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的前向传播过程相对简单。我们对输入张量调用每个函数并返回最终结果。
- en: Hugging Face Connection Code
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hugging Face连接代码
- en: Because GPT-2 is open-source, it is available on Hugging Face. While our goal
    here is to train our own model, it is nice to be able to compare what our results
    will be with the ones OpenAI found in their training. To allow us to do so, we
    have the below function that pulls in the weights and populates them into our
    `GPT` class.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 因为GPT-2是开源的，所以它可以在Hugging Face上找到。虽然我们的目标是训练自己的模型，但能够将我们的结果与OpenAI在训练中得到的结果进行对比是很有意义的。为了实现这一点，我们有以下函数来提取权重并将它们填充到我们的`GPT`类中。
- en: This code also allows us to reuse this code to pull in foundation models from
    Hugging Face and fine-tune them (*with some modifications as right now it’s optimized
    only for gpt-2*).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码还允许我们重用这些代码，从Hugging Face拉取基础模型并进行微调（*经过一些修改，因为现在它仅对gpt-2进行了优化*）。
- en: '[PRE11]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Starting from the top, we bring in HuggingFace’s `transformers` library and
    setup the hyperparameters that vary between different variants of the GPT-2 model.
    As the `vocab_size` and `block_size` don’t change, you can see we hard-code them
    in. We then pass these variables into the `GPTConfig` class from before, and then
    instantiate the model object (`GPT`). Finally, we remove all keys from the model
    that end with `.attn.bias` , as these are not weights, but rather the register
    we setup to help with our attention function before.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 从顶部开始，我们引入HuggingFace的`transformers`库，并设置在不同版本的GPT-2模型之间变化的超参数。由于`vocab_size`和`block_size`不变，您可以看到我们将它们硬编码了进去。然后，我们将这些变量传递给之前的`GPTConfig`类，然后实例化模型对象（`GPT`）。最后，我们从模型中删除所有以`.attn.bias`结尾的键，因为这些不是权重，而是我们之前为帮助注意力功能设置的寄存器。
- en: '[PRE12]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Next, we load in the model from the HuggingFace class `GPT2LMHeadModel`. We
    take the keys out from this model and likewise ignore the `attn.masked_bias` and
    `attn.bias` keys. We then have an assert to make sure that we have the same number
    of keys in the hugging face model as we do in our model.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从HuggingFace类`GPT2LMHeadModel`加载模型。我们从这个模型中提取键，并同样忽略`attn.masked_bias`和`attn.bias`键。然后我们使用断言来确保Hugging
    Face模型中的键数量与我们模型中的键数量相同。
- en: '[PRE13]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: To round out the function, we loop through every key in the Hugging Face model
    and add its weights to the corresponding key in our model. There are certain keys
    that need to be manipulated so that they fit the data structure we’re using. We
    run the function `.t()` to transpose the hugging face matrix into the dimensions
    we need. For the rest, we copy them over directly. You’ll notice we are using
    `torch.no_grad()` . This is telling torch that it doesn’t need to cache the values
    for a backward propagation of the model, another optimization to make this run
    faster.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完善这个函数，我们遍历Hugging Face模型中的每个键，并将其权重添加到我们模型中对应的键上。有些键需要进行处理，以便适应我们正在使用的数据结构。我们使用函数`.t()`来转置hugging
    face矩阵，使其符合我们需要的维度。对于其余的键，我们直接复制过去。你会注意到我们使用了`torch.no_grad()`。这告诉torch它不需要缓存值用于模型的反向传播，这是另一个优化措施，使得运行速度更快。
- en: Generating Our First Predictions (Sampling Loop)
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成我们的第一次预测（采样循环）
- en: With the classes we have now, we can run the model and have it give us output
    tokens (*just make sure if you’re following this sequentially that you comment
    out the _init_weights call in the GPT constructor*). The below code shows how
    we would do that.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们现在的类，我们可以运行模型并让它给出输出的token（*只需确保如果你是按顺序跟随的，记得注释掉GPT构造函数中的_init_weights调用*）。下面的代码展示了我们如何做到这一点。
- en: '[PRE14]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We start off by determining what devices we have access to. Cuda is NVIDIA’s
    platform that runs extremely fast GPU calculations, so if we have access to chips
    that use CUDA we will use them. If we don’t have access but we’re on Apple Silicon,
    then we will use that. Finally, if we have neither, then we fall back to CPU (this
    will be the slowest, but every computer has one so we know we can still train
    on it).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先确定可以访问的设备。Cuda是NVIDIA的平台，可以进行非常快速的GPU计算，因此如果我们有访问使用CUDA的芯片，我们将使用它们。如果我们没有CUDA访问权限，但使用的是Apple
    Silicon，那么我们将使用它。最后，如果两者都没有，我们就退回到CPU（这将是最慢的，但每台计算机都有CPU，所以我们知道仍然可以在上面训练）。
- en: 'Then, we instantiate our model using the default configurations, and put the
    model into ‘`eval`’ mode — (this does a number of things, like disabling dropout,
    but from a high level it makes sure that our model is more consistent during inferencing).
    Once set, we move the model onto our device. Note that if we wanted to use the
    HuggingFace weights instead of our training weights, we would modify the third-to-last-line
    to read: `model = GPT.from_pretrained(‘gpt2’)`'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用默认配置实例化模型，并将模型置于'`eval`'模式——（这会做很多事情，比如禁用dropout，但从宏观上看，它确保我们的模型在推理过程中更加一致）。设置完成后，我们将模型移到我们的设备上。注意，如果我们想使用HuggingFace的预训练权重而不是我们的训练权重，我们需要修改倒数第三行，使其变为：`model
    = GPT.from_pretrained(‘gpt2’)`
- en: '[PRE15]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We now bring in `tiktoken` using the gpt2 encodings and have it tokenize our
    prompt. We take these tokens and put them into a tensor, which we then convert
    to batches in the below line. `unsqueeze()` will add a new first dimension of
    size 1 to the tensor, and `repeat` will repeat the entire tensor `num_return_sequences`
    times within the first dimension and once within the second dimension. What we’ve
    done here is formatted our data to fit the batched schema our model is expecting.
    Specifically we now match the (B, T) format: `num_return_sequences` x encoded
    length of prompt. Once we pass through the input tensor into the beginning of
    the model, our `wte` and `wpe` will create the C dimension.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在使用`gpt2`编码方式引入`tiktoken`并让它对我们的提示进行分词。我们将这些token放入一个tensor中，然后在下面一行中将其转换为批次。`unsqueeze()`会为tensor添加一个新的尺寸为1的第一维，`repeat`会在第一维重复整个tensor
    `num_return_sequences`次，并在第二维重复一次。我们所做的是将数据格式化为模型预期的批处理结构。具体来说，我们现在匹配了(B, T)格式：`num_return_sequences`
    x 提示的编码长度。一旦我们将输入tensor传入模型的开始位置，我们的`wte`和`wpe`就会生成C维度。
- en: '[PRE16]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now that they’re ready, we send them to the device and begin our sampling loop.
    The loop will be exclusively a forward pass, so we wrap it in the `torch.no_grad`
    to stop it from caching for any backward propagation. Our logits come out with
    shape (batch_size, seq_len, vocab_size) — (B,T,C) with C coming after a forward
    pass of the model.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在它们已经准备好，我们将它们发送到设备上并开始我们的采样循环。这个循环将仅仅是前向传播，因此我们将其包装在`torch.no_grad`中，以防它为任何反向传播缓存值。我们的logits的形状为(batch_size,
    seq_len, vocab_size) — (B,T,C)，其中C是在模型前向传播后得到的。
- en: We only need the last item in the sequence to predict the next token, so we
    pull out `[:, -1, :]` We then take those logits and run it through a `softmax`
    to get the token probabilities. Taking the top 50, we then choose a random index
    of the top 50 and pick that one as our predicted token. We then get the information
    about that and add it to our tensor `x`. By concatenating `xcol` to `x`, we set
    ourselves up to go into the next token given what we just predicted. This is how
    we code up autoregression.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要序列中的最后一个项来预测下一个token，因此我们提取`[:, -1, :]`。然后我们将这些logits通过`softmax`处理，得到token的概率。取前50个概率最大的token，我们随机选择一个索引，从中选出作为我们的预测token。然后我们获取关于该token的信息并将其添加到我们的张量`x`中。通过将`xcol`拼接到`x`上，我们为根据刚刚预测的内容进入下一个token做好了准备。这就是我们实现自回归的方式。
- en: '[PRE17]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: After the sampling loop is done, we can go through each of the selected tokens
    and decode them, showing the response to the user. We grab data from the `i`-th
    in our batch and decode it to get the next token.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 采样循环完成后，我们可以遍历每个选定的token并对其进行解码，展示给用户响应。我们从批次中的第`i`个获取数据，并解码以得到下一个token。
- en: If you run the sampling loop on our initial model, you will notice that the
    output leaves a lot to be desired. This is because we haven’t trained any of the
    weights. The next few classes show how we can begin a naive training of the model.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在我们初始模型上运行采样循环，你会发现输出结果差强人意。这是因为我们还没有训练任何权重。接下来的几节将展示如何开始对模型进行初步训练。
- en: DataLoaderLite
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DataLoaderLite
- en: All training requires high quality data. For Karpathy’s videos, he likes to
    use public domain Shakespeare text ([find it here](https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt)).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 所有训练都需要高质量的数据。对于Karpathy的视频，他喜欢使用公共领域的莎士比亚文本（[可以在这里找到](https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt)）。
- en: '[PRE18]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We begin by simply opening the file and reading in the text. This data source
    is ASCII only, so we don’t need to worry about any unexpected binary characters.
    We use `tiktoken` to get the encodings for the body, and then convert these tokens
    into a tensor. We then create a variable called `current_position`, which will
    let us know where in the token tensor we are currently training from (naturally,
    this is initialized to the beginning). Note, this class is not inheriting from
    `nn.Module`, mainly because we have no need for the `forward` function here. Just
    as with the prompt part of the sampling loop, our DataLoaderLite class only needs
    to generate tensors of shape (B, T).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过简单地打开文件并读取文本来开始。此数据源仅包含ASCII字符，因此我们不需要担心任何意外的二进制字符。我们使用`tiktoken`获取正文的编码，然后将这些tokens转换为张量。接着，我们创建一个名为`current_position`的变量，它将告诉我们当前正在从token张量的哪个位置进行训练（通常初始化为开始位置）。注意，这个类没有继承自`nn.Module`，主要是因为我们在这里不需要`forward`函数。就像在采样循环的提示部分一样，我们的DataLoaderLite类只需要生成形状为(B,
    T)的张量。
- en: '[PRE19]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In the above we define the function `next_batch` to help with training. To make
    programs run faster, we like to run the calculations in batches. We use the B
    and T fields to determine the batch size (B) and sequence length (T) we’ll be
    training on. Using these variables, we create a buffer that holds the tokens we
    are going to train with, setting the dimensions to be of rows B and columns T.
    Note that we read from `current_position` to `current_position + (B*T + 1)` ,
    where the +1 is to make sure we have all of the ground truth values for our `B*T`
    batch.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，我们定义了`next_batch`函数来帮助训练。为了加速程序运行，我们喜欢将计算分批执行。我们使用B和T字段来确定我们训练的批量大小（B）和序列长度（T）。使用这些变量，我们创建一个缓冲区来保存我们将用来训练的tokens，将维度设置为B行T列。注意，我们从`current_position`读取到`current_position
    + (B*T + 1)`，其中+1是为了确保我们有所有的真实值，适用于我们`B*T`的批次。
- en: We then setup our model input (`x`) and our expected output (`y`) along the
    same lines. `x` is the entire buffer except for the last character, and `y` is
    the entire buffer except for the first. The basic idea is that given the first
    value in token buffer, we expect to get back the second token in the token buffer
    from our model.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们设置我们的模型输入(`x`)和期望输出(`y`)。`x`是除了最后一个字符外的整个缓冲区，而`y`是除了第一个字符外的整个缓冲区。基本的想法是，给定token缓冲区中的第一个值，我们期望从模型中返回token缓冲区中的第二个token。
- en: Finally, we update the `current_position` and return `x` and `y`.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们更新`current_position`并返回`x`和`y`。
- en: Weight Initialization
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 权重初始化
- en: As we are dealing with probabilities, we’d like to pick initial values for our
    weights that are likely to require fewer epochs to get right. Our `_init_weights`
    function helps us do so, by initializing the weights with either zeroes or with
    a normal distribution.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们处理的是概率，我们希望为权重选择初始值，以便更少的训练轮次就能得到正确的结果。我们的`_init_weights`函数帮助我们实现这一点，它通过用零或正态分布来初始化权重。
- en: '[PRE20]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: If you remember from before, we’re passing in every field of the GPT class into
    `_init_weights`, so we’re processing `nn.Module`s. We are using the Xavier method
    to initialize our weights, which means we set the standard deviation of our sampling
    distribution equal to `1 / sqrt(hidden_layers)` . You will notice that in the
    code, we are often using the hardcoded 0.02 as the standard deviation. While this
    might seem arbitrary, from the below table you can see that as the hidden dimensions
    GPT-2 uses are all roughly 0.02, this is a fine-approximation.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前记得，我们将`GPT`类的每个字段都传递给了`_init_weights`，所以我们正在处理`nn.Module`。我们使用Xavier方法初始化权重，这意味着我们将采样分布的标准差设置为`1
    / sqrt(hidden_layers)`。你会注意到，在代码中，我们经常使用硬编码的0.02作为标准差。尽管这看起来是任意的，但从下面的表格可以看到，由于GPT-2使用的隐藏维度都大致为0.02，这就是一个很好的近似值。
- en: Going through the code, we start off by checking which subtype of `nn.Module`
    the module we’re operating on is.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 浏览代码时，我们首先检查我们操作的模块是`nn.Module`的哪种子类。
- en: 'If the module is Linear, then we will check if it is one of our projections
    from `MLP` or `CasualSelfAttention` classes (by checking if it has the `NANO_GPT_INIT`
    flag set). If it is, then our 0.02 approximation won’t work because the number
    of hidden layers in these modules is increasing (this is a function of our addition
    of the tensors in the `Block` class). Consequently, the GPT-2 paper uses a scaling
    function to account for this: `1/sqrt(2 * self.config.n_layer)`. The `2*` is because
    our `Block` has 2 places where we are adding the tensors.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果该模块是线性模块，我们会检查它是否是我们从`MLP`或`CasualSelfAttention`类中的投影之一（通过检查它是否设置了`NANO_GPT_INIT`标志）。如果是，那么我们0.02的近似值将不起作用，因为这些模块中的隐藏层数在增加（这是我们在`Block`类中添加张量的结果）。因此，GPT-2论文使用了一个缩放函数来解决这个问题：`1/sqrt(2
    * self.config.n_layer)`。`2*`的原因是我们的`Block`有两个地方在添加张量。
- en: If we have a bias in the Linear module, we will start by initializing these
    all to zero.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果线性模块中有偏置，我们将首先将其初始化为零。
- en: If we have an `Embedding` Module (like the Token or Positional Encoding pieces),
    we will initialize this with the same normal distribution with standard deviation
    of 0.02.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个`Embedding`模块（如Token或位置编码部分），我们将使用标准差为0.02的相同正态分布来初始化它。
- en: 'If you remember, we have another subtype of module that is in our model: `nn.LayerNorm`
    . This class already is initialized with a normal distribution and so we decide
    that this is good enough to not need any changes.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你记得，我们的模型中还有另一种子类：`nn.LayerNorm`。这个类已经通过正态分布进行了初始化，因此我们决定不需要做任何更改。
- en: Training Loop
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练循环
- en: Now that we have the training fundamentals setup, let’s put together a quick
    training loop to train our model.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置好了训练的基本要素，让我们快速组建一个训练循环来训练我们的模型。
- en: '[PRE21]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: You can see that we repeat our device calculations to get optimal performance.
    We then set our data loader to use batch sizes of 4 and sequence lengths of 32
    (set arbitrarily, although powers of 2 are best for memory efficiency).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，我们重复进行设备计算以获得最佳性能。然后，我们设置数据加载器，使用批量大小为4和序列长度为32（这个设置是任意的，尽管2的幂在内存效率上最佳）。
- en: '[PRE22]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Now we have the optimizer, which will help us train our model. The optimizer
    is a PyTorch class that takes in the parameters it should be training (in our
    case the ones given from the `GPT` class) and then the learning rate which is
    a hyperparameter during training determining how quickly we should be adjusting
    parameters — a higher learning rate means more drastic changes to the weights
    after each run. We chose our value based off of Karpathy’s recommendation.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了优化器，它将帮助我们训练模型。优化器是一个PyTorch类，它接受应训练的参数（在我们的情况下是从`GPT`类传入的参数），以及学习率，这是训练过程中的超参数，决定了我们调整参数的速度——较高的学习率意味着每次运行后权重的变化更为剧烈。我们根据Karpathy的推荐选择了我们的值。
- en: We then use 50 training steps to train the model. We start by getting the training
    batch and moving them onto our device. We set the optimizer’s gradients to zero
    (gradients in pytorch are sums, so if we don’t zero it out we will be carrying
    information over from the last batch). We calculate the logits and loss from our
    model, and then run backwards propagation to figure out what the new weight models
    should be. Finally, we run `optimizer.step()` to update all of our model parameters.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用50步训练来训练模型。我们首先获取训练批次并将其移到我们的设备上。我们将优化器的梯度设置为零（在pytorch中，梯度是累加的，所以如果不将其清零，我们将从上一批次携带信息）。我们计算模型的logits和损失，然后进行反向传播，以找出新的权重模型应该是什么。最后，我们运行`optimizer.step()`来更新所有模型参数。
- en: Sanity Check
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 合理性检查
- en: To see how all of the above code runs, you can [check out my Google Colab where
    I combine all of it and run it on the NVIDIA T4 GPU](https://colab.research.google.com/drive/1tJjzDP2wTq6R5LF0AWESlNheTn1YEYzI).
    Running our training loop, we see that the loss starts off at ~11\. To sanity
    test this, we expect that at the beginning the odds of predicting the right token
    is (1/`vocab_size`). Taking this through a simplified loss function of `-ln`,
    we get ~10.88, which is just about where we begin!
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 若要查看上述所有代码的执行情况，您可以[查看我的Google Colab，其中我将所有内容结合并在NVIDIA T4 GPU上运行](https://colab.research.google.com/drive/1tJjzDP2wTq6R5LF0AWESlNheTn1YEYzI)。运行我们的训练循环时，我们看到损失从大约~11开始。为了验证这一点，我们预计在开始时预测正确的标记的概率为（1/`vocab_size`）。通过一个简化的损失函数`-ln`，我们得到大约10.88，这正是我们开始的地方！
- en: '![](../Images/7a46cc240486e0eb1f38cf0bc67d2947.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a46cc240486e0eb1f38cf0bc67d2947.png)'
- en: Image by Author
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Closing
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结束语
- en: Thanks for reading through to the end!
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你读到最后！
- en: I tried to include as much detail as I could in this blog post, but naturally
    there were somethings I had to leave out. If you enjoyed the blog post or see
    anything you think should be modified / expanded upon, please let me know!
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我尽力在这篇博客文章中包含了尽可能多的细节，但自然也有一些内容我不得不省略。如果你喜欢这篇博客文章或者发现有任何你认为应该修改/扩展的地方，请告诉我！
- en: It’s an exciting time to be building!
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是构建的激动人心时刻！
- en: '[1] Karpathy, A., [“Let’s reproduce GPT-2 (124M)”](https://youtu.be/l8pRSuU81PU?feature=shared)
    (2024), YouTube'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Karpathy, A.，[《让我们重现 GPT-2（124M）》](https://youtu.be/l8pRSuU81PU?feature=shared)（2024），YouTube'
- en: '[2] Radford, A., et al., [“Language Models are Unsupervised Multitask Learners”](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)
    (2018), Papers With Code'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Radford, A. 等人，[《语言模型是无监督多任务学习者》](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)（2018），《带代码的论文》'
- en: '[3] Vaswani, A., et al., “[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)”
    (2017), arXiv'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Vaswani, A. 等人，"[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)"（2017），arXiv'
