- en: Fine-Tune Llama 3.1 Ultra-Efficiently with Unsloth
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Unslothè¶…é«˜æ•ˆå¾®è°ƒLlama 3.1
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/fine-tune-llama-3-1-ultra-efficiently-with-unsloth-7196c7165bab?source=collection_archive---------0-----------------------#2024-07-29](https://towardsdatascience.com/fine-tune-llama-3-1-ultra-efficiently-with-unsloth-7196c7165bab?source=collection_archive---------0-----------------------#2024-07-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/fine-tune-llama-3-1-ultra-efficiently-with-unsloth-7196c7165bab?source=collection_archive---------0-----------------------#2024-07-29](https://towardsdatascience.com/fine-tune-llama-3-1-ultra-efficiently-with-unsloth-7196c7165bab?source=collection_archive---------0-----------------------#2024-07-29)
- en: '*A beginnerâ€™s guide to state-of-the-art supervised fine-tuning*'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*é€‚åˆåˆå­¦è€…çš„å…ˆè¿›ç›‘ç£å¾®è°ƒæŒ‡å—*'
- en: '[](https://medium.com/@mlabonne?source=post_page---byline--7196c7165bab--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page---byline--7196c7165bab--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7196c7165bab--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7196c7165bab--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page---byline--7196c7165bab--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne?source=post_page---byline--7196c7165bab--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page---byline--7196c7165bab--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7196c7165bab--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7196c7165bab--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page---byline--7196c7165bab--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7196c7165bab--------------------------------)
    Â·12 min readÂ·Jul 29, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7196c7165bab--------------------------------)
    Â·é˜…è¯»æ—¶é—´ï¼š12åˆ†é’ŸÂ·2024å¹´7æœˆ29æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/442eac2ef94d8b6584ef18418dcbdb93.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/442eac2ef94d8b6584ef18418dcbdb93.png)'
- en: Image generated with DALL-E 3 by author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±ä½œè€…ä½¿ç”¨DALL-E 3ç”Ÿæˆçš„å›¾ç‰‡
- en: The recent release of Llama 3.1 offers models with an incredible level of performance,
    closing the gap between closed-source and open-weight models. Instead of using
    frozen, general-purpose LLMs like GPT-4o and Claude 3.5, you can fine-tune Llama
    3.1 for your specific use cases to achieve better performance and customizability
    at a lower cost.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘å‘å¸ƒçš„Llama 3.1æä¾›äº†å…·æœ‰ä»¤äººéš¾ä»¥ç½®ä¿¡çš„æ€§èƒ½æ°´å¹³çš„æ¨¡å‹ï¼Œç¼©å°äº†å°é—­æºæ¨¡å‹ä¸å¼€æ”¾æƒé‡æ¨¡å‹ä¹‹é—´çš„å·®è·ã€‚ä¸ä½¿ç”¨å†»ç»“çš„ã€é€šç”¨çš„LLMï¼ˆå¦‚GPT-4oå’ŒClaude
    3.5ï¼‰ä¸åŒï¼Œæ‚¨å¯ä»¥æ ¹æ®ç‰¹å®šçš„ä½¿ç”¨åœºæ™¯å¯¹Llama 3.1è¿›è¡Œå¾®è°ƒï¼Œä»è€Œåœ¨æ›´ä½çš„æˆæœ¬ä¸‹å®ç°æ›´å¥½çš„æ€§èƒ½å’Œå®šåˆ¶åŒ–ã€‚
- en: '![](../Images/e2bfb879bf9d5078c60062fe6e5f8c59.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2bfb879bf9d5078c60062fe6e5f8c59.png)'
- en: Image by author
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼šä½œè€…
- en: In this article, we will provide a comprehensive overview of supervised fine-tuning.
    We will compare it to prompt engineering to understand when it makes sense to
    use it, detail the main techniques with their pros and cons, and introduce major
    concepts, such as LoRA hyperparameters, storage formats, and chat templates. Finally,
    we will implement it in practice by fine-tuning Llama 3.1 8B in Google Colab with
    state-of-the-art optimization using Unsloth.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†å…¨é¢æ¦‚è¿°ç›‘ç£å¾®è°ƒã€‚æˆ‘ä»¬å°†å…¶ä¸æç¤ºå·¥ç¨‹è¿›è¡Œæ¯”è¾ƒï¼Œä»¥äº†è§£ä½•æ—¶ä½¿ç”¨å®ƒæœ€ä¸ºåˆé€‚ï¼Œè¯¦ç»†ä»‹ç»ä¸»è¦çš„æŠ€æœ¯åŠå…¶ä¼˜ç¼ºç‚¹ï¼Œå¹¶ä»‹ç»ä¸€äº›é‡è¦æ¦‚å¿µï¼Œå¦‚LoRAè¶…å‚æ•°ã€å­˜å‚¨æ ¼å¼å’ŒèŠå¤©æ¨¡æ¿ã€‚æœ€åï¼Œæˆ‘ä»¬å°†åœ¨Google
    Colabä¸­å®è·µè¿™ä¸€æŠ€æœ¯ï¼Œä½¿ç”¨Unslothè¿›è¡ŒLlama 3.1 8Bçš„å¾®è°ƒï¼Œå¹¶åº”ç”¨æœ€å…ˆè¿›çš„ä¼˜åŒ–æ–¹æ³•ã€‚
- en: All the code used in this article is available on [Google Colab](https://colab.research.google.com/drive/164cg_O7SV7G8kZr_JXqLd6VC7pd86-1Z?usp=sharing)
    and in the [LLM Course](https://github.com/mlabonne/llm-course).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡ä¸­ä½¿ç”¨çš„æ‰€æœ‰ä»£ç éƒ½å¯ä»¥åœ¨[Google Colab](https://colab.research.google.com/drive/164cg_O7SV7G8kZr_JXqLd6VC7pd86-1Z?usp=sharing)å’Œ[LLMè¯¾ç¨‹](https://github.com/mlabonne/llm-course)ä¸­æ‰¾åˆ°ã€‚
- en: ğŸ”§ Supervised Fine-Tuning
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ”§ ç›‘ç£å¾®è°ƒ
- en: '![](../Images/721214d6a9b43ef4eaa195651d4a8119.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/721214d6a9b43ef4eaa195651d4a8119.png)'
- en: Image by author
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼šä½œè€…
- en: Supervised Fine-Tuning (SFT) is a method to **improve and customize** pre-trained
    LLMs. It involves retraining base models on a smaller dataset of instructions
    and answers. The main goal is to transform a basic model that predicts text into
    an assistant that can follow instructions and answer questions. SFT can also enhance
    the modelâ€™s overall performance, add new knowledge, or adapt it to specific tasks
    and domains. Fine-tuned models can then go through an optional preference alignment
    stage (see [my article about DPO](https://mlabonne.github.io/blog/posts/Fine_tune_Mistral_7b_with_DPO.html))
    to remove unwanted responses, modify their style, and more.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ˜¯ä¸€ç§**æ”¹è¿›å’Œå®šåˆ¶**é¢„è®­ç»ƒ LLM çš„æ–¹æ³•ã€‚å®ƒæ¶‰åŠåœ¨ä¸€ä¸ªè¾ƒå°çš„æŒ‡ä»¤å’Œç­”æ¡ˆæ•°æ®é›†ä¸Šå¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œé‡æ–°è®­ç»ƒã€‚å…¶ä¸»è¦ç›®æ ‡æ˜¯å°†ä¸€ä¸ªé¢„æµ‹æ–‡æœ¬çš„åŸºæœ¬æ¨¡å‹è½¬å˜ä¸ºä¸€ä¸ªèƒ½å¤Ÿéµå¾ªæŒ‡ä»¤å¹¶å›ç­”é—®é¢˜çš„åŠ©æ‰‹ã€‚SFT
    è¿˜å¯ä»¥æé«˜æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ï¼Œæ·»åŠ æ–°çš„çŸ¥è¯†ï¼Œæˆ–å°†å…¶é€‚åº”ç‰¹å®šçš„ä»»åŠ¡å’Œé¢†åŸŸã€‚ç»è¿‡å¾®è°ƒçš„æ¨¡å‹å¯ä»¥é€šè¿‡ä¸€ä¸ªå¯é€‰çš„åå¥½å¯¹é½é˜¶æ®µï¼ˆå‚è§[æˆ‘å…³äº DPO çš„æ–‡ç« ](https://mlabonne.github.io/blog/posts/Fine_tune_Mistral_7b_with_DPO.html)ï¼‰æ¥å»é™¤ä¸éœ€è¦çš„å›ç­”ï¼Œä¿®æ”¹å…¶é£æ ¼ç­‰ã€‚
- en: The following figure shows an instruction sample. It includes a system prompt
    to steer the model, a user prompt to provide a task, and the output the model
    is expected to generate. You can find a list of high-quality open-source instruction
    datasets in the [ğŸ’¾ LLM Datasets](https://github.com/mlabonne/llm-datasets) GitHub
    repo.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹å›¾å±•ç¤ºäº†ä¸€ä¸ªæŒ‡ä»¤æ ·æœ¬ã€‚å®ƒåŒ…æ‹¬ä¸€ä¸ªç³»ç»Ÿæç¤ºæ¥å¼•å¯¼æ¨¡å‹ï¼Œä¸€ä¸ªç”¨æˆ·æç¤ºæ¥æä¾›ä»»åŠ¡ï¼Œä»¥åŠæ¨¡å‹é¢„æœŸç”Ÿæˆçš„è¾“å‡ºã€‚ä½ å¯ä»¥åœ¨[ğŸ’¾ LLM æ•°æ®é›†](https://github.com/mlabonne/llm-datasets)çš„
    GitHub ä»“åº“ä¸­æ‰¾åˆ°ä¸€ä»½é«˜è´¨é‡çš„å¼€æºæŒ‡ä»¤æ•°æ®é›†åˆ—è¡¨ã€‚
- en: '![](../Images/4f1f2c16c4190c178e4fa8be8bccb8c9.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f1f2c16c4190c178e4fa8be8bccb8c9.png)'
- en: Image by author
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: Before considering SFT, I recommend trying prompt engineering techniques like
    **few-shot prompting** or **retrieval augmented generation** (RAG). In practice,
    these methods can solve many problems without the need for fine-tuning, using
    either closed-source or open-weight models (e.g., Llama 3.1 Instruct). If this
    approach doesnâ€™t meet your objectives (in terms of quality, cost, latency, etc.),
    then SFT becomes a viable option when instruction data is available. Note that
    SFT also offers benefits like additional control and customizability to create
    personalized LLMs.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è€ƒè™‘ SFT ä¹‹å‰ï¼Œæˆ‘å»ºè®®å°è¯•åƒ**å°‘é‡ç¤ºä¾‹æç¤º**æˆ–**æ£€ç´¢å¢å¼ºç”Ÿæˆ**ï¼ˆRAGï¼‰è¿™æ ·çš„æç¤ºå·¥ç¨‹æŠ€æœ¯ã€‚å®é™…ä¸Šï¼Œè¿™äº›æ–¹æ³•å¯ä»¥åœ¨æ— éœ€å¾®è°ƒçš„æƒ…å†µä¸‹è§£å†³è®¸å¤šé—®é¢˜ï¼Œé€‚ç”¨äºå°é—­æºä»£ç æˆ–å¼€æ”¾æƒé‡çš„æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼ŒLlama
    3.1 Instructï¼‰ã€‚å¦‚æœè¿™ç§æ–¹æ³•æ— æ³•æ»¡è¶³ä½ çš„ç›®æ ‡ï¼ˆå¦‚è´¨é‡ã€æˆæœ¬ã€å»¶è¿Ÿç­‰æ–¹é¢ï¼‰ï¼Œé‚£ä¹ˆå½“æœ‰æŒ‡ä»¤æ•°æ®å¯ç”¨æ—¶ï¼ŒSFT å°±æˆä¸ºä¸€ä¸ªå¯è¡Œçš„é€‰æ‹©ã€‚è¯·æ³¨æ„ï¼ŒSFT
    è¿˜æä¾›äº†è¯¸å¦‚é¢å¤–çš„æ§åˆ¶å’Œå®šåˆ¶åŒ–ç­‰å¥½å¤„ï¼Œå¯ä»¥åˆ›å»ºä¸ªæ€§åŒ–çš„ LLMã€‚
- en: However, SFT has limitations. It works best when leveraging knowledge already
    present in the base model. Learning completely new information like an unknown
    language can be challenging and lead to more frequent hallucinations. For new
    domains unknown to the base model, it is recommended to continuously pre-train
    it on a raw dataset first.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼ŒSFT ä¹Ÿæœ‰å±€é™æ€§ã€‚å®ƒåœ¨åˆ©ç”¨å·²ç»å­˜åœ¨äºåŸºç¡€æ¨¡å‹ä¸­çš„çŸ¥è¯†æ—¶æ•ˆæœæœ€ä½³ã€‚å­¦ä¹ å®Œå…¨æ–°çš„ä¿¡æ¯ï¼Œæ¯”å¦‚ä¸€ç§æœªçŸ¥çš„è¯­è¨€ï¼Œå¯èƒ½ä¼šé¢ä¸´æŒ‘æˆ˜ï¼Œå¹¶ä¸”æ›´å®¹æ˜“å¯¼è‡´é¢‘ç¹çš„å¹»è§‰ã€‚å¯¹äºåŸºç¡€æ¨¡å‹æœªçŸ¥çš„æ–°é¢†åŸŸï¼Œå»ºè®®é¦–å…ˆåœ¨åŸå§‹æ•°æ®é›†ä¸ŠæŒç»­è¿›è¡Œé¢„è®­ç»ƒã€‚
- en: On the opposite end of the spectrum, instruct models (i.e., already fine-tuned
    models) can already be very close to your requirements. For example, a model might
    perform very well but state that it was trained by OpenAI or Meta instead of you.
    In this case, you might want to slightly steer the instruct modelâ€™s behavior using
    preference alignment. By providing chosen and rejected samples for a small set
    of instructions (between 100 and 1000 samples), you can force the LLM to say that
    you trained it instead of OpenAI.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€ç³»åˆ—çš„å¦ä¸€ç«¯ï¼ŒæŒ‡ä»¤æ¨¡å‹ï¼ˆå³å·²ç»å¾®è°ƒçš„æ¨¡å‹ï¼‰å¯èƒ½å·²ç»éå¸¸æ¥è¿‘ä½ çš„éœ€æ±‚ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªæ¨¡å‹å¯èƒ½è¡¨ç°å¾—éå¸¸å¥½ï¼Œä½†å£°æ˜å®ƒæ˜¯ç”± OpenAI æˆ– Meta è€Œä¸æ˜¯ä½ è®­ç»ƒçš„ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ å¯èƒ½å¸Œæœ›é€šè¿‡åå¥½å¯¹é½ç¨å¾®è°ƒæ•´æŒ‡ä»¤æ¨¡å‹çš„è¡Œä¸ºã€‚é€šè¿‡ä¸ºå°‘é‡æŒ‡ä»¤ï¼ˆ100
    åˆ° 1000 ä¸ªæ ·æœ¬ï¼‰æä¾›é€‰æ‹©å’Œæ‹’ç»æ ·æœ¬ï¼Œä½ å¯ä»¥è¿«ä½¿ LLM å£°æ˜æ˜¯ç”±ä½ è€Œä¸æ˜¯ OpenAI è®­ç»ƒçš„ã€‚
- en: âš–ï¸ SFT Techniques
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: âš–ï¸ SFT æŠ€æœ¯
- en: The three most popular SFT techniques are full fine-tuning, LoRA, and QLoRA.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‰ç§æœ€å—æ¬¢è¿çš„ SFT æŠ€æœ¯æ˜¯å®Œå…¨å¾®è°ƒã€LoRA å’Œ QLoRAã€‚
- en: '![](../Images/a95694ba5600c25c2a3027f40c66b853.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a95694ba5600c25c2a3027f40c66b853.png)'
- en: Image by author
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: '**Full fine-tuning** is the most straightforward SFT technique. It involves
    retraining all parameters of a pre-trained model on an instruction dataset. This
    method often provides the best results but requires significant computational
    resources (several high-end GPUs are required to fine-tune a 8B model). Because
    it modifies the entire model, it is also the most destructive method and can lead
    to the catastrophic forgetting of previous skills and knowledge.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**å…¨é‡å¾®è°ƒ**æ˜¯æœ€ç›´æ¥çš„SFTæŠ€æœ¯ã€‚å®ƒæ¶‰åŠåœ¨æŒ‡ä»¤æ•°æ®é›†ä¸Šé‡æ–°è®­ç»ƒé¢„è®­ç»ƒæ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚æ­¤æ–¹æ³•é€šå¸¸æä¾›æœ€ä½³ç»“æœï¼Œä½†éœ€è¦å¤§é‡è®¡ç®—èµ„æºï¼ˆå¯¹ä¸€ä¸ª8Bæ¨¡å‹è¿›è¡Œå¾®è°ƒéœ€è¦å‡ å¼ é«˜ç«¯GPUï¼‰ã€‚ç”±äºå®ƒä¿®æ”¹äº†æ•´ä¸ªæ¨¡å‹ï¼Œå› æ­¤ä¹Ÿæ˜¯æœ€å…·ç ´åæ€§çš„æ–¹æ³•ï¼Œå¯èƒ½å¯¼è‡´ä¹‹å‰çš„æŠ€èƒ½å’ŒçŸ¥è¯†å‡ºç°ç¾éš¾æ€§é—å¿˜ã€‚'
- en: '**Low-Rank Adaptation (LoRA)** is a popular parameter-efficient fine-tuning
    technique. Instead of retraining the entire model, it freezes the weights and
    introduces small adapters (low-rank matrices) at each targeted layer. This allows
    LoRA to train a number of parameters that is drastically lower than full fine-tuning
    (less than 1%), reducing both memory usage and training time. This method is non-destructive
    since the original parameters are frozen, and adapters can then be switched or
    combined at will.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰**æ˜¯ä¸€ç§æµè¡Œçš„å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ã€‚å®ƒä¸æ˜¯é‡æ–°è®­ç»ƒæ•´ä¸ªæ¨¡å‹ï¼Œè€Œæ˜¯å†»ç»“æƒé‡ï¼Œå¹¶åœ¨æ¯ä¸ªç›®æ ‡å±‚å¼•å…¥å°å‹é€‚é…å™¨ï¼ˆä½ç§©çŸ©é˜µï¼‰ã€‚è¿™ä½¿å¾—LoRAè®­ç»ƒçš„å‚æ•°æ•°é‡è¿œä½äºå…¨é‡å¾®è°ƒï¼ˆä¸åˆ°1%ï¼‰ï¼Œä»è€Œå‡å°‘äº†å†…å­˜ä½¿ç”¨å’Œè®­ç»ƒæ—¶é—´ã€‚æ­¤æ–¹æ³•æ˜¯éç ´åæ€§çš„ï¼Œå› ä¸ºåŸå§‹å‚æ•°è¢«å†»ç»“ï¼Œé€‚é…å™¨å¯ä»¥éšæ—¶æ›´æ¢æˆ–ç»„åˆã€‚'
- en: '**QLoRA (Quantization-aware Low-Rank Adaptation)** is an extension of LoRA
    that offers even greater memory savings. It provides up to 33% additional memory
    reduction compared to standard LoRA, making it particularly useful when GPU memory
    is constrained. This increased efficiency comes at the cost of longer training
    times, with QLoRA typically taking about 39% more time to train than regular LoRA.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**QLoRAï¼ˆé‡åŒ–æ„ŸçŸ¥ä½ç§©é€‚åº”ï¼‰**æ˜¯LoRAçš„æ‰©å±•ï¼Œæä¾›äº†æ›´å¤§çš„å†…å­˜èŠ‚çœã€‚ä¸æ ‡å‡†LoRAç›¸æ¯”ï¼Œå®ƒå¯é¢å¤–èŠ‚çœæœ€å¤š33%çš„å†…å­˜ï¼Œä½¿å…¶åœ¨GPUå†…å­˜å—é™çš„æƒ…å†µä¸‹å°¤ä¸ºæœ‰ç”¨ã€‚å°½ç®¡æ•ˆç‡æé«˜äº†ï¼Œä½†å®ƒçš„è®­ç»ƒæ—¶é—´æ›´é•¿ï¼ŒQLoRAçš„è®­ç»ƒæ—¶é—´é€šå¸¸æ¯”æ™®é€šLoRAå¤šèŠ±è´¹çº¦39%çš„æ—¶é—´ã€‚'
- en: While QLoRA requires more training time, its substantial memory savings can
    make it the only viable option in scenarios where GPU memory is limited. For this
    reason, this is the technique we will use in the next section to fine-tune a Llama
    3.1 8B model on Google Colab.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶QLoRAéœ€è¦æ›´é•¿çš„è®­ç»ƒæ—¶é—´ï¼Œä½†å…¶æ˜¾è‘—çš„å†…å­˜èŠ‚çœä½¿å¾—åœ¨GPUå†…å­˜å—é™çš„æƒ…å†µä¸‹ï¼ŒQLoRAæˆä¸ºå”¯ä¸€å¯è¡Œçš„é€‰æ‹©ã€‚å› æ­¤ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†åœ¨Google Colabä¸­ä½¿ç”¨æ­¤æŠ€æœ¯å¯¹Llama
    3.1 8Bæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚
- en: ğŸ¦™ Fine-Tune Llama 3.1 8B
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ¦™ å¾®è°ƒLlama 3.1 8B
- en: To efficiently fine-tune a [Llama 3.1 8B](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B)
    model, weâ€™ll use the [Unsloth](https://github.com/unslothai/unsloth) library by
    Daniel and Michael Han. Thanks to its custom kernels, Unsloth provides 2x faster
    training and 60% memory use compared to other options, making it ideal in a constrained
    environment like Colab. Unfortunately, Unsloth only supports single-GPU settings
    at the moment. For multi-GPU settings, I recommend popular alternatives like [TRL](https://huggingface.co/docs/trl/en/index)
    and [Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl) (both also
    include Unsloth as a backend).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†é«˜æ•ˆåœ°å¾®è°ƒ[Llama 3.1 8B](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B)æ¨¡å‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨Danielå’ŒMichael
    Hanå¼€å‘çš„[Unsloth](https://github.com/unslothai/unsloth)åº“ã€‚å¾—ç›Šäºå…¶å®šåˆ¶çš„å†…æ ¸ï¼ŒUnslothç›¸æ¯”å…¶ä»–é€‰é¡¹æä¾›äº†2å€æ›´å¿«çš„è®­ç»ƒé€Ÿåº¦å’Œ60%çš„å†…å­˜ä½¿ç”¨ç‡ï¼Œä½¿å…¶åœ¨åƒColabè¿™æ ·çš„å—é™ç¯å¢ƒä¸­è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚ä¸å¹¸çš„æ˜¯ï¼ŒUnslothç›®å‰ä»…æ”¯æŒå•GPUè®¾ç½®ã€‚å¯¹äºå¤šGPUè®¾ç½®ï¼Œæˆ‘æ¨èæµè¡Œçš„æ›¿ä»£æ–¹æ¡ˆï¼Œå¦‚[TRL](https://huggingface.co/docs/trl/en/index)å’Œ[Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)ï¼ˆè¿™ä¸¤è€…ä¹Ÿéƒ½å°†Unslothä½œä¸ºåç«¯ï¼‰ã€‚
- en: In this example, we will QLoRA fine-tune it on the [mlabonne/FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k)
    dataset. Itâ€™s a subset of [arcee-ai/The-Tome](https://huggingface.co/datasets/arcee-ai/The-Tome)
    (without [arcee-ai/qwen2â€“72b-magpie-en](https://huggingface.co/datasets/arcee-ai/qwen2-72b-magpie-en))
    that I re-filtered using [HuggingFaceFW/fineweb-edu-classifier](https://huggingface.co/HuggingFaceFW/fineweb-edu-classifier).
    Note that this classifier wasnâ€™t designed for instruction data quality evaluation,
    but we can use it as a rough proxy. The resulting FineTome is an ultra-high quality
    dataset that includes conversations, reasoning problems, function calling, and
    more.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨[mlabonne/FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k)æ•°æ®é›†ä¸Šè¿›è¡ŒQLoRAå¾®è°ƒã€‚è¿™æ˜¯[arcee-ai/The-Tome](https://huggingface.co/datasets/arcee-ai/The-Tome)çš„ä¸€ä¸ªå­é›†ï¼ˆæ²¡æœ‰[arcee-ai/qwen2â€“72b-magpie-en](https://huggingface.co/datasets/arcee-ai/qwen2-72b-magpie-en)ï¼‰ï¼Œæˆ‘ä½¿ç”¨[HuggingFaceFW/fineweb-edu-classifier](https://huggingface.co/HuggingFaceFW/fineweb-edu-classifier)é‡æ–°è¿‡æ»¤è¿‡ã€‚è¯·æ³¨æ„ï¼Œè¿™ä¸ªåˆ†ç±»å™¨å¹¶ä¸æ˜¯ä¸ºè¯„ä¼°æŒ‡ä»¤æ•°æ®è´¨é‡è€Œè®¾è®¡çš„ï¼Œä½†æˆ‘ä»¬å¯ä»¥å°†å…¶ä½œä¸ºç²—ç•¥çš„ä»£ç†ã€‚ç”Ÿæˆçš„FineTomeæ˜¯ä¸€ä¸ªè¶…é«˜è´¨é‡çš„æ•°æ®é›†ï¼ŒåŒ…æ‹¬å¯¹è¯ã€æ¨ç†é—®é¢˜ã€å‡½æ•°è°ƒç”¨ç­‰å†…å®¹ã€‚
- en: Letâ€™s start by installing all the required libraries.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é¦–å…ˆå®‰è£…æ‰€æœ‰å¿…éœ€çš„åº“ã€‚
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Once installed, we can import them as follows.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å®‰è£…å®Œæˆåï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰å¦‚ä¸‹æ–¹å¼å¯¼å…¥å®ƒä»¬ã€‚
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Letâ€™s now load the model. Since we want to use QLoRA, I chose the pre-quantized
    [unsloth/Meta-Llama-3.1â€“8B-bnb-4bit](https://huggingface.co/unsloth/Meta-Llama-3.1-8B-bnb-4bit).
    This 4-bit precision version of [meta-llama/Meta-Llama-3.1â€“8B](https://markdown-to-medium.surge.sh/meta-llama/Meta-Llama-3.1-8B)
    is significantly smaller (5.4 GB) and faster to download compared to the original
    16-bit precision model (16 GB). We load in NF4 format using the bitsandbytes library.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬åŠ è½½æ¨¡å‹ã€‚ç”±äºæˆ‘ä»¬è¦ä½¿ç”¨QLoRAï¼Œæˆ‘é€‰æ‹©äº†é¢„é‡åŒ–çš„[unsloth/Meta-Llama-3.1â€“8B-bnb-4bit](https://huggingface.co/unsloth/Meta-Llama-3.1-8B-bnb-4bit)ã€‚è¿™ä¸ª4ä½ç²¾åº¦ç‰ˆæœ¬çš„[meta-llama/Meta-Llama-3.1â€“8B](https://markdown-to-medium.surge.sh/meta-llama/Meta-Llama-3.1-8B)æ¯”åŸå§‹16ä½ç²¾åº¦æ¨¡å‹ï¼ˆ16
    GBï¼‰è¦å°å¾—å¤šï¼ˆ5.4 GBï¼‰ï¼Œä¸”ä¸‹è½½é€Ÿåº¦æ›´å¿«ã€‚æˆ‘ä»¬ä½¿ç”¨bitsandbytesåº“ä»¥NF4æ ¼å¼åŠ è½½ã€‚
- en: When loading the model, we must specify a maximum sequence length, which restricts
    its context window. Llama 3.1 supports up to 128k context length, but we will
    set it to 2,048 in this example since it consumes more compute and VRAM. Finally,
    the `dtype` parameter automatically detects if your GPU supports the [BF16 format](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html#background-on-floating-point-representation)
    for more stability during training (this feature is restricted to Ampere and more
    recent GPUs).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŠ è½½æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬å¿…é¡»æŒ‡å®šæœ€å¤§åºåˆ—é•¿åº¦ï¼Œè¿™é™åˆ¶äº†å…¶ä¸Šä¸‹æ–‡çª—å£ã€‚Llama 3.1æ”¯æŒæœ€å¤§128kçš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œä½†ç”±äºå®ƒæ¶ˆè€—æ›´å¤šè®¡ç®—èµ„æºå’Œæ˜¾å­˜ï¼Œåœ¨è¿™ä¸ªç¤ºä¾‹ä¸­æˆ‘ä»¬å°†å…¶è®¾ç½®ä¸º2,048ã€‚æœ€åï¼Œ`dtype`å‚æ•°ä¼šè‡ªåŠ¨æ£€æµ‹ä½ çš„GPUæ˜¯å¦æ”¯æŒ[BF16æ ¼å¼](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html#background-on-floating-point-representation)ï¼Œä»¥ä¾¿åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æä¾›æ›´å¤šçš„ç¨³å®šæ€§ï¼ˆæ­¤åŠŸèƒ½ä»…é€‚ç”¨äºAmpereåŠæ›´æ–°çš„GPUï¼‰ã€‚
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now that our model is loaded in 4-bit precision, we want to prepare it for
    parameter-efficient fine-tuning with LoRA adapters. LoRA has three important parameters:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬çš„æ¨¡å‹å·²ç»åŠ è½½ä¸º4ä½ç²¾åº¦ï¼Œæˆ‘ä»¬å¸Œæœ›é€šè¿‡LoRAé€‚é…å™¨ä¸ºå…¶å‡†å¤‡å¥½å‚æ•°é«˜æ•ˆçš„å¾®è°ƒã€‚LoRAæœ‰ä¸‰ä¸ªé‡è¦çš„å‚æ•°ï¼š
- en: '**Rank** (r), which determines LoRA matrix size. Rank typically starts at 8
    but can go up to 256\. Higher ranks can store more information but increase the
    computational and memory cost of LoRA. We set it to 16 here.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç§©**ï¼ˆrï¼‰ï¼Œå†³å®šLoRAçŸ©é˜µçš„å¤§å°ã€‚ç§©é€šå¸¸ä»8å¼€å§‹ï¼Œä½†å¯ä»¥è¾¾åˆ°256ã€‚æ›´é«˜çš„ç§©å¯ä»¥å­˜å‚¨æ›´å¤šçš„ä¿¡æ¯ï¼Œä½†ä¼šå¢åŠ LoRAçš„è®¡ç®—å’Œå†…å­˜å¼€é”€ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œå°†å…¶è®¾ç½®ä¸º16ã€‚'
- en: '**Alpha** (Î±), a scaling factor for updates. Alpha directly impacts the adaptersâ€™
    contribution and is often set to 1x or 2x the rank value.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Alpha**ï¼ˆÎ±ï¼‰ï¼Œæ›´æ–°çš„ç¼©æ”¾å› å­ã€‚Alphaç›´æ¥å½±å“é€‚é…å™¨çš„è´¡çŒ®ï¼Œé€šå¸¸è®¾ç½®ä¸ºç§©å€¼çš„1å€æˆ–2å€ã€‚'
- en: '**Target modules**: LoRA can be applied to various model components, including
    attention mechanisms (Q, K, V matrices), output projections, feed-forward blocks,
    and linear output layers. While initially focused on attention mechanisms, extending
    LoRA to other components has shown benefits. However, adapting more modules increases
    the number of trainable parameters and memory needs.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç›®æ ‡æ¨¡å—**ï¼šLoRAå¯ä»¥åº”ç”¨äºå„ç§æ¨¡å‹ç»„ä»¶ï¼ŒåŒ…æ‹¬æ³¨æ„åŠ›æœºåˆ¶ï¼ˆQã€Kã€VçŸ©é˜µï¼‰ã€è¾“å‡ºæŠ•å½±ã€å‰é¦ˆå—å’Œçº¿æ€§è¾“å‡ºå±‚ã€‚å°½ç®¡æœ€åˆä¸»è¦é›†ä¸­äºæ³¨æ„åŠ›æœºåˆ¶ï¼Œä½†å°†LoRAæ‰©å±•åˆ°å…¶ä»–ç»„ä»¶ä¹Ÿå·²è¯æ˜å…·æœ‰ç›Šå¤„ã€‚ç„¶è€Œï¼Œé€‚é…æ›´å¤šæ¨¡å—ä¼šå¢åŠ å¯è®­ç»ƒå‚æ•°çš„æ•°é‡å’Œå†…å­˜éœ€æ±‚ã€‚'
- en: Here, we set r=16, Î±=16, and target every linear module to maximize quality.
    We donâ€™t use dropout and biases for faster training.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†r=16ï¼ŒÎ±=16ï¼Œå¹¶å°†æ¯ä¸ªçº¿æ€§æ¨¡å—ä½œä¸ºç›®æ ‡ä»¥æœ€å¤§åŒ–è´¨é‡ã€‚æˆ‘ä»¬ä¸ä½¿ç”¨dropoutå’Œåç½®ä»¥åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚
- en: In addition, we will use [Rank-Stabilized LoRA](https://arxiv.org/abs/2312.03732)
    (rsLoRA), which modifies the scaling factor of LoRA adapters to be proportional
    to 1/âˆšr instead of 1/r. This stabilizes learning (especially for higher adapter
    ranks) and allows for improved fine-tuning performance as rank increases. Gradient
    checkpointing is handled by Unsloth to offload input and output embeddings to
    disk and save VRAM.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨[Rank-Stabilized LoRA](https://arxiv.org/abs/2312.03732)ï¼ˆrsLoRAï¼‰ï¼Œå®ƒé€šè¿‡å°†LoRAé€‚é…å™¨çš„ç¼©æ”¾å› å­ä¿®æ”¹ä¸ºä¸1/âˆšræˆæ¯”ä¾‹ï¼Œè€Œä¸æ˜¯ä¸1/ræˆæ¯”ä¾‹ï¼Œæ¥ç¨³å®šå­¦ä¹ ï¼ˆç‰¹åˆ«æ˜¯å¯¹äºæ›´é«˜çš„é€‚é…å™¨ç§©ï¼‰ï¼Œå¹¶å…è®¸éšç€ç§©çš„å¢åŠ æé«˜å¾®è°ƒæ€§èƒ½ã€‚æ¢¯åº¦æ£€æŸ¥ç‚¹ç”±Unslothå¤„ç†ï¼Œå°†è¾“å…¥å’Œè¾“å‡ºåµŒå…¥ä¿å­˜åˆ°ç£ç›˜ï¼Œä»¥èŠ‚çœæ˜¾å­˜ï¼ˆVRAMï¼‰ã€‚
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: With this LoRA configuration, weâ€™ll only train 42 million out of 8 billion parameters
    (0.5196%). This shows how much more efficient LoRA is compared to full fine-tuning.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¿™ä¸ªLoRAé…ç½®ï¼Œæˆ‘ä»¬åªè®­ç»ƒ42ç™¾ä¸‡ä¸ªå‚æ•°ï¼ˆåœ¨80äº¿ä¸ªå‚æ•°ä¸­å 0.5196%ï¼‰ã€‚è¿™å±•ç¤ºäº†LoRAç›¸æ¯”å®Œå…¨å¾®è°ƒçš„é«˜æ•ˆæ€§ã€‚
- en: 'Letâ€™s now load and prepare our dataset. Instruction datasets are stored in
    a **particular format**: it can be Alpaca, ShareGPT, OpenAI, etc. First, we want
    to parse this format to retrieve our instructions and answers. Our [mlabonne/FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k)
    dataset uses the ShareGPT format with a unique â€œconversationsâ€ column containing
    messages in JSONL. Unlike simpler formats like Alpaca, ShareGPT is ideal for storing
    multi-turn conversations, which is closer to how users interact with LLMs.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬åŠ è½½å¹¶å‡†å¤‡æ•°æ®é›†ã€‚æŒ‡ä»¤æ•°æ®é›†ä»¥**ç‰¹å®šæ ¼å¼**å­˜å‚¨ï¼šå¯ä»¥æ˜¯Alpacaã€ShareGPTã€OpenAIç­‰ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦è§£æè¿™ç§æ ¼å¼ä»¥æå–æˆ‘ä»¬çš„æŒ‡ä»¤å’Œç­”æ¡ˆã€‚æˆ‘ä»¬çš„[mlabonne/FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k)æ•°æ®é›†ä½¿ç”¨ShareGPTæ ¼å¼ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªç‹¬ç‰¹çš„â€œconversationsâ€åˆ—ï¼Œå­˜å‚¨ä»¥JSONLæ ¼å¼çš„æ¶ˆæ¯ã€‚ä¸åƒAlpacaè¿™æ ·æ›´ç®€å•çš„æ ¼å¼ä¸åŒï¼ŒShareGPTéå¸¸é€‚åˆå­˜å‚¨å¤šè½®å¯¹è¯ï¼Œè¿™æ›´æ¥è¿‘ç”¨æˆ·ä¸LLMï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰çš„äº¤äº’æ–¹å¼ã€‚
- en: 'Once our instruction-answer pairs are parsed, we want to reformat them to follow
    a **chat template**. Chat templates are a way to structure conversations between
    users and models. They typically include special tokens to identify the beginning
    and the end of a message, whoâ€™s speaking, etc. Base models donâ€™t have chat templates
    so we can choose any: ChatML, Llama3, Mistral, etc. In the open-source community,
    the ChatML template (originally from OpenAI) is a popular option. It simply adds
    two special tokens (`<|im_start|>` and `<|im_end|>`) to indicate who''s speaking.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æˆ‘ä»¬çš„æŒ‡ä»¤-ç­”æ¡ˆå¯¹è¢«è§£æï¼Œæˆ‘ä»¬éœ€è¦é‡æ–°æ ¼å¼åŒ–å®ƒä»¬ï¼Œä»¥éµå¾ª**èŠå¤©æ¨¡æ¿**ã€‚èŠå¤©æ¨¡æ¿æ˜¯ä¸€ç§æ„å»ºç”¨æˆ·å’Œæ¨¡å‹ä¹‹é—´å¯¹è¯çš„æ–¹å¼ã€‚å®ƒä»¬é€šå¸¸åŒ…å«ç‰¹æ®Šçš„æ ‡è®°ï¼Œç”¨äºæ ‡è¯†æ¶ˆæ¯çš„å¼€å§‹å’Œç»“æŸï¼Œè°åœ¨è¯´è¯ç­‰ç­‰ã€‚åŸºç¡€æ¨¡å‹æ²¡æœ‰èŠå¤©æ¨¡æ¿ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥é€‰æ‹©ä»»ä½•ä¸€ç§ï¼šChatMLã€Llama3ã€Mistralç­‰ã€‚åœ¨å¼€æºç¤¾åŒºä¸­ï¼ŒChatMLæ¨¡æ¿ï¼ˆæœ€åˆæ¥è‡ªOpenAIï¼‰æ˜¯ä¸€ä¸ªæµè¡Œçš„é€‰æ‹©ã€‚å®ƒä»…æ·»åŠ äº†ä¸¤ä¸ªç‰¹æ®Šæ ‡è®°ï¼ˆ`<|im_start|>`å’Œ`<|im_end|>`ï¼‰æ¥æŒ‡ç¤ºè¯´è¯è€…ã€‚
- en: 'If we apply this template to the previous instruction sample, hereâ€™s what we
    get:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å°†æ­¤æ¨¡æ¿åº”ç”¨äºå‰é¢çš„æŒ‡ä»¤ç¤ºä¾‹ï¼Œå¾—åˆ°çš„ç»“æœå¦‚ä¸‹ï¼š
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the following code block, we parse our ShareGPT dataset with the `mapping`
    parameter and include the ChatML template. We then load and process the entire
    dataset to apply the chat template to every conversation.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»¥ä¸‹ä»£ç å—ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡`mapping`å‚æ•°è§£æShareGPTæ•°æ®é›†ï¼Œå¹¶åŒ…æ‹¬ChatMLæ¨¡æ¿ã€‚ç„¶åï¼Œæˆ‘ä»¬åŠ è½½å¹¶å¤„ç†æ•´ä¸ªæ•°æ®é›†ï¼Œå°†èŠå¤©æ¨¡æ¿åº”ç”¨äºæ¯ä¸€å¯¹è¯ã€‚
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Weâ€™re now ready to specify the training parameters for our run. I want to briefly
    introduce the most important hyperparameters:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å‡†å¤‡å¥½ä¸ºæˆ‘ä»¬çš„è®­ç»ƒè¿è¡ŒæŒ‡å®šè®­ç»ƒå‚æ•°ã€‚æˆ‘æƒ³ç®€è¦ä»‹ç»ä¸€ä¸‹æœ€é‡è¦çš„è¶…å‚æ•°ï¼š
- en: '**Learning rate**: It controls how strongly the model updates its parameters.
    Too low, and training will be slow and may get stuck in local minima. Too high,
    and training may become unstable or diverge, which degrades performance.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å­¦ä¹ ç‡**ï¼šå®ƒæ§åˆ¶æ¨¡å‹æ›´æ–°å‚æ•°çš„å¼ºåº¦ã€‚è¿‡ä½ä¼šå¯¼è‡´è®­ç»ƒç¼“æ…¢å¹¶å¯èƒ½é™·å…¥å±€éƒ¨æœ€å°å€¼ã€‚è¿‡é«˜åˆ™å¯èƒ½ä½¿è®­ç»ƒå˜å¾—ä¸ç¨³å®šæˆ–å‘æ•£ï¼Œè¿›è€Œé™ä½æ€§èƒ½ã€‚'
- en: '**LR scheduler**: It adjusts the learning rate (LR) during training, starting
    with a higher LR for rapid initial progress and then decreasing it in later stages.
    Linear and cosine schedulers are the two most common options.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆLR schedulerï¼‰**ï¼šå®ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è°ƒæ•´å­¦ä¹ ç‡ï¼ˆLRï¼‰ï¼ŒåˆæœŸä½¿ç”¨è¾ƒé«˜çš„å­¦ä¹ ç‡ä»¥åŠ å¿«è¿›åº¦ï¼Œéšååœ¨è®­ç»ƒåæœŸé€æ¸é™ä½å­¦ä¹ ç‡ã€‚çº¿æ€§è°ƒåº¦å™¨å’Œä½™å¼¦è°ƒåº¦å™¨æ˜¯æœ€å¸¸ç”¨çš„ä¸¤ç§é€‰æ‹©ã€‚'
- en: '**Batch size**: Number of samples processed before the weights are updated.
    Larger batch sizes generally lead to more stable gradient estimates and can improve
    training speed, but they also require more memory. Gradient accumulation allows
    for effectively larger batch sizes by accumulating gradients over multiple forward/backward
    passes before updating the model.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ‰¹é‡å¤§å°**ï¼šåœ¨æ›´æ–°æƒé‡ä¹‹å‰å¤„ç†çš„æ ·æœ¬æ•°é‡ã€‚è¾ƒå¤§çš„æ‰¹é‡å¤§å°é€šå¸¸ä¼šå¯¼è‡´æ›´ç¨³å®šçš„æ¢¯åº¦ä¼°è®¡ï¼Œå¹¶ä¸”å¯ä»¥æé«˜è®­ç»ƒé€Ÿåº¦ï¼Œä½†å®ƒä»¬ä¹Ÿéœ€è¦æ›´å¤šçš„å†…å­˜ã€‚æ¢¯åº¦ç´¯ç§¯é€šè¿‡åœ¨æ›´æ–°æ¨¡å‹ä¹‹å‰å¯¹å¤šä¸ªå‰å‘/åå‘ä¼ é€’çš„æ¢¯åº¦è¿›è¡Œç´¯ç§¯ï¼Œå®ç°åœ¨æ•ˆæœä¸Šä½¿ç”¨æ›´å¤§çš„æ‰¹é‡å¤§å°ã€‚'
- en: '**Num epochs**: The number of complete passes through the training dataset.
    More epochs allow the model to see the data more times, potentially leading to
    better performance. However, too many epochs can cause overfitting.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è®­ç»ƒè½®æ•°**ï¼šå®Œæˆå¯¹è®­ç»ƒæ•°æ®é›†çš„æ‰€æœ‰éå†æ¬¡æ•°ã€‚æ›´å¤šçš„è®­ç»ƒè½®æ•°ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå¤šæ¬¡çœ‹åˆ°æ•°æ®ï¼Œå¯èƒ½ä¼šæé«˜æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿‡å¤šçš„è®­ç»ƒè½®æ•°å¯èƒ½ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆã€‚'
- en: '**Optimizer**: Algorithm used to adjust the parameters of a model to minimize
    the loss function. In practice, AdamW 8-bit is strongly recommended: it performs
    as well as the 32-bit version while using less GPU memory. The paged version of
    AdamW is only interesting in distributed settings.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¼˜åŒ–å™¨**ï¼šç”¨æ¥è°ƒæ•´æ¨¡å‹å‚æ•°ä»¥æœ€å°åŒ–æŸå¤±å‡½æ•°çš„ç®—æ³•ã€‚åœ¨å®è·µä¸­ï¼Œå¼ºçƒˆæ¨èä½¿ç”¨AdamW 8ä½ç‰ˆæœ¬ï¼šå®ƒçš„è¡¨ç°ä¸32ä½ç‰ˆæœ¬ç›¸å½“ï¼Œä½†ä½¿ç”¨æ›´å°‘çš„GPUå†…å­˜ã€‚AdamWçš„åˆ†é¡µç‰ˆæœ¬ä»…åœ¨åˆ†å¸ƒå¼è®¾ç½®ä¸­æ‰æœ‰æ„ä¹‰ã€‚'
- en: '**Weight decay**: A regularization technique that adds a penalty for large
    weights to the loss function. It helps prevent overfitting by encouraging the
    model to learn simpler, more generalizable features. However, too much weight
    decay can impede learning.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æƒé‡è¡°å‡**ï¼šä¸€ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œé€šè¿‡åœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ å¯¹å¤§æƒé‡çš„æƒ©ç½šæ¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œé¼“åŠ±æ¨¡å‹å­¦ä¹ æ›´ç®€å•ã€æ›´å…·å¯æ³›åŒ–æ€§çš„ç‰¹å¾ã€‚ç„¶è€Œï¼Œè¿‡å¼ºçš„æƒé‡è¡°å‡å¯èƒ½ä¼šé˜»ç¢å­¦ä¹ ã€‚'
- en: '**Warmup steps**: A period at the beginning of training where the learning
    rate is gradually increased from a small value to the initial learning rate. Warmup
    can help stabilize early training, especially with large learning rates or batch
    sizes, by allowing the model to adjust to the data distribution before making
    large updates.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é¢„çƒ­æ­¥éª¤**ï¼šè®­ç»ƒå¼€å§‹æ—¶çš„ä¸€æ®µæ—¶é—´ï¼Œå­¦ä¹ ç‡ä»ä¸€ä¸ªè¾ƒå°çš„å€¼é€æ¸å¢åŠ åˆ°åˆå§‹å­¦ä¹ ç‡ã€‚é¢„çƒ­å¯ä»¥å¸®åŠ©ç¨³å®šæ—©æœŸè®­ç»ƒï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨è¾ƒå¤§å­¦ä¹ ç‡æˆ–æ‰¹é‡å¤§å°æ—¶ï¼Œé€šè¿‡è®©æ¨¡å‹åœ¨è¿›è¡Œå¤§å¹…æ›´æ–°ä¹‹å‰é€‚åº”æ•°æ®åˆ†å¸ƒã€‚'
- en: '**Packing**: Batches have a pre-defined sequence length. Instead of assigning
    one batch per sample, we can combine multiple small samples in one batch, increasing
    efficiency.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ‰“åŒ…**ï¼šæ‰¹æ¬¡æœ‰ä¸€ä¸ªé¢„å®šä¹‰çš„åºåˆ—é•¿åº¦ã€‚æˆ‘ä»¬å¯ä»¥å°†å¤šä¸ªå°æ ·æœ¬åˆå¹¶æˆä¸€ä¸ªæ‰¹æ¬¡ï¼Œè€Œä¸æ˜¯ä¸ºæ¯ä¸ªæ ·æœ¬åˆ†é…ä¸€ä¸ªæ‰¹æ¬¡ï¼Œä»è€Œæé«˜æ•ˆç‡ã€‚'
- en: I trained the model on the entire dataset (100k samples) using an A100 GPU (40
    GB of VRAM) on Google Colab. The training took 4 hours and 45 minutes. Of course,
    you can use smaller GPUs with less VRAM and a smaller batch size, but theyâ€™re
    not nearly as fast. For example, it takes roughly 19 hours and 40 minutes on an
    L4 and a whopping 47 hours on a free T4.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨æ•´ä¸ªæ•°æ®é›†ï¼ˆ100kä¸ªæ ·æœ¬ï¼‰ä¸Šä½¿ç”¨Google Colabä¸Šçš„A100 GPUï¼ˆ40 GB VRAMï¼‰è¿›è¡Œäº†æ¨¡å‹è®­ç»ƒã€‚è®­ç»ƒç”¨äº†4å°æ—¶45åˆ†é’Ÿã€‚å½“ç„¶ï¼Œä½ å¯ä»¥ä½¿ç”¨å…·æœ‰æ›´å°‘VRAMå’Œè¾ƒå°æ‰¹é‡å¤§å°çš„è¾ƒå°GPUï¼Œä½†å®ƒä»¬çš„é€Ÿåº¦è¿œä¸å¦‚A100ã€‚ä¾‹å¦‚ï¼Œåœ¨L4ä¸Šå¤§çº¦éœ€è¦19å°æ—¶40åˆ†é’Ÿï¼Œåœ¨å…è´¹çš„T4ä¸Šéœ€è¦æ•´æ•´47å°æ—¶ã€‚
- en: In this case, I recommend only loading a subset of the dataset to speed up training.
    You can do it by modifying the previous code block, like `dataset = load_dataset("mlabonne/FineTome-100k",
    split="train[:10000]")` to only load 10k samples. Alternatively, you can use cheaper
    cloud GPU providers like Paperspace, RunPod, or Lambda Labs.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘å»ºè®®åªåŠ è½½æ•°æ®é›†çš„ä¸€ä¸ªå­é›†æ¥åŠ å¿«è®­ç»ƒã€‚ä½ å¯ä»¥é€šè¿‡ä¿®æ”¹ä¹‹å‰çš„ä»£ç å—æ¥å®ç°ï¼Œä¾‹å¦‚å°†`dataset = load_dataset("mlabonne/FineTome-100k",
    split="train[:10000]")`ä¿®æ”¹ä¸ºä»…åŠ è½½10kä¸ªæ ·æœ¬ã€‚æˆ–è€…ï¼Œä½ å¯ä»¥ä½¿ç”¨åƒPaperspaceã€RunPodæˆ–Lambda Labsè¿™æ ·çš„æ›´ä¾¿å®œçš„äº‘GPUæä¾›å•†ã€‚
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now that the model is trained, letâ€™s test it with a simple prompt. This is not
    a rigorous evaluation but just a quick check to detect potential issues. We use
    `FastLanguageModel.for_inference()` to get 2x faster inference.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ¨¡å‹å·²ç»è®­ç»ƒå®Œæˆï¼Œè®©æˆ‘ä»¬ç”¨ä¸€ä¸ªç®€å•çš„æç¤ºæ¥æµ‹è¯•å®ƒã€‚è¿™ä¸æ˜¯ä¸¥æ ¼çš„è¯„ä¼°ï¼Œè€Œåªæ˜¯ä¸€ä¸ªå¿«é€Ÿæ£€æŸ¥ï¼Œç”¨æ¥å‘ç°æ½œåœ¨çš„é—®é¢˜ã€‚æˆ‘ä»¬ä½¿ç”¨`FastLanguageModel.for_inference()`æ¥å®ç°2å€æ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚
- en: '[PRE7]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The modelâ€™s response is â€œ9.9â€, which is correct!
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹çš„å“åº”æ˜¯â€œ9.9â€ï¼Œè¿™æ˜¯æ­£ç¡®çš„ï¼
- en: 'Letâ€™s now save our trained model. If you remember the part about LoRA and QLoRA,
    what we trained is not the model itself but a set of adapters. There are three
    save methods in Unsloth: `lora` to only save the adapters, and `merged_16bit`/`merged_4bit`
    to merge the adapters with the model in 16-bit/ 4-bit precision.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚å¦‚æœä½ è¿˜è®°å¾—LoRAå’ŒQLoRAçš„éƒ¨åˆ†ï¼Œæˆ‘ä»¬è®­ç»ƒçš„ä¸æ˜¯æ¨¡å‹æœ¬èº«ï¼Œè€Œæ˜¯ä¸€ç»„é€‚é…å™¨ã€‚Unslothä¸­æœ‰ä¸‰ç§ä¿å­˜æ–¹æ³•ï¼š`lora`ä»…ä¿å­˜é€‚é…å™¨ï¼Œ`merged_16bit`/`merged_4bit`åˆ™æ˜¯å°†é€‚é…å™¨ä¸æ¨¡å‹åˆå¹¶ä¸º16ä½/4ä½ç²¾åº¦ã€‚
- en: In the following, we merge them in 16-bit precision to maximize the quality.
    We first save it locally in the â€œmodelâ€ directory and then upload it to the Hugging
    Face Hub. You can find the trained model on [mlabonne/FineLlama-3.1â€“8B](https://huggingface.co/mlabonne/FineLlama-3.1-8B).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„æ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬å°†å®ƒä»¬åˆå¹¶ä¸º16ä½ç²¾åº¦ï¼Œä»¥æœ€å¤§åŒ–è´¨é‡ã€‚æˆ‘ä»¬é¦–å…ˆå°†å…¶ä¿å­˜åœ¨â€œmodelâ€ç›®å½•ä¸‹ï¼Œç„¶åä¸Šä¼ åˆ° Hugging Face Hubã€‚ä½ å¯ä»¥åœ¨[mlabonne/FineLlama-3.1â€“8B](https://huggingface.co/mlabonne/FineLlama-3.1-8B)ä¸Šæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Unsloth also allows you to directly convert your model into GGUF format. This
    is a quantization format created for llama.cpp and compatible with most inference
    engines, like [LM Studio](https://lmstudio.ai/), [Ollama](https://ollama.com/),
    and oobaboogaâ€™s [text-generation-webui](https://github.com/oobabooga/text-generation-webui).
    Since you can specify different precisions (see [my article about GGUF and llama.cpp](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html)),
    weâ€™ll loop over a list to quantize it in `q2_k`, `q3_k_m`, `q4_k_m`, `q5_k_m`,
    `q6_k`, `q8_0` and upload these quants on Hugging Face. The [mlabonne/FineLlama-3.1-8B-GGUF](https://huggingface.co/mlabonne/FineLlama-3.1-8B-GGUF)
    contains all our GGUFs.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Unslothè¿˜å…è®¸ä½ ç›´æ¥å°†æ¨¡å‹è½¬æ¢ä¸ºGGUFæ ¼å¼ã€‚è¿™æ˜¯ä¸€ä¸ªä¸ºllama.cppåˆ›å»ºçš„é‡åŒ–æ ¼å¼ï¼Œå¹¶ä¸å¤§å¤šæ•°æ¨ç†å¼•æ“å…¼å®¹ï¼Œå¦‚[LM Studio](https://lmstudio.ai/)ã€[Ollama](https://ollama.com/)å’Œoobaboogaçš„[text-generation-webui](https://github.com/oobabooga/text-generation-webui)ã€‚ç”±äºä½ å¯ä»¥æŒ‡å®šä¸åŒçš„ç²¾åº¦ï¼ˆè¯·å‚é˜…[æˆ‘çš„å…³äºGGUFå’Œllama.cppçš„æ–‡ç« ](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html)ï¼‰ï¼Œæˆ‘ä»¬å°†éå†ä¸€ä¸ªåˆ—è¡¨ï¼Œä½¿ç”¨`q2_k`ã€`q3_k_m`ã€`q4_k_m`ã€`q5_k_m`ã€`q6_k`ã€`q8_0`è¿›è¡Œé‡åŒ–ï¼Œå¹¶å°†è¿™äº›é‡åŒ–æ–‡ä»¶ä¸Šä¼ è‡³Hugging
    Faceã€‚ [mlabonne/FineLlama-3.1-8B-GGUF](https://huggingface.co/mlabonne/FineLlama-3.1-8B-GGUF)åŒ…å«äº†æˆ‘ä»¬æ‰€æœ‰çš„GGUFæ–‡ä»¶ã€‚
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Congratulations, we fine-tuned a model from scratch and uploaded quants you
    can now use in your favorite inference engine. Feel free to try the final model
    available on [mlabonne/FineLlama-3.1â€“8B-GGUF](https://huggingface.co/mlabonne/FineLlama-3.1-8B-GGUF).
    What to do now? Here are some ideas on how to use your model:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œä½ ï¼Œæˆ‘ä»¬ä»é›¶å¼€å§‹å¾®è°ƒäº†ä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶ä¸Šä¼ äº†ä½ ç°åœ¨å¯ä»¥åœ¨ä½ æœ€å–œæ¬¢çš„æ¨ç†å¼•æ“ä¸­ä½¿ç”¨çš„é‡åŒ–æ–‡ä»¶ã€‚éšæ—¶å¯ä»¥å°è¯•åœ¨[mlabonne/FineLlama-3.1â€“8B-GGUF](https://huggingface.co/mlabonne/FineLlama-3.1-8B-GGUF)ä¸Šä½¿ç”¨æœ€ç»ˆæ¨¡å‹ã€‚æ¥ä¸‹æ¥è¯¥åšä»€ä¹ˆå‘¢ï¼Ÿä»¥ä¸‹æ˜¯ä¸€äº›ä½¿ç”¨ä½ æ¨¡å‹çš„å»ºè®®ï¼š
- en: '**Evaluate** it on the [Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)
    (you can submit it for free) or using other evals like in [LLM AutoEval](https://github.com/mlabonne/llm-autoeval).'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è¯„ä¼°**å®ƒåœ¨[Open LLMæ’è¡Œæ¦œ](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)ä¸Šï¼ˆä½ å¯ä»¥å…è´¹æäº¤ï¼‰æˆ–è€…ä½¿ç”¨å…¶ä»–è¯„ä¼°å·¥å…·ï¼Œå¦‚[LLM
    AutoEval](https://github.com/mlabonne/llm-autoeval)ã€‚'
- en: '**Align** it with Direct Preference Optimization using a preference dataset
    like [mlabonne/orpo-dpo-mix-40k](https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k)
    to boost performance.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¯¹é½**å®ƒå¹¶ä½¿ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDirect Preference Optimizationï¼‰ä¸åå¥½æ•°æ®é›†ï¼Œå¦‚[mlabonne/orpo-dpo-mix-40k](https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k)æ¥æå‡æ€§èƒ½ã€‚'
- en: '**Quantize** it in other formats like EXL2, AWQ, GPTQ, or HQQ for faster inference
    or lower precision using [AutoQuant](https://colab.research.google.com/drive/1b6nqC7UZVt8bx4MksX7s656GXPM-eWw4?usp=sharing).'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é‡åŒ–**å®ƒä¸ºå…¶ä»–æ ¼å¼ï¼Œå¦‚EXL2ã€AWQã€GPTQæˆ–HQQï¼Œä»¥ä¾¿æ›´å¿«çš„æ¨ç†æˆ–é™ä½ç²¾åº¦ï¼Œä½¿ç”¨[AutoQuant](https://colab.research.google.com/drive/1b6nqC7UZVt8bx4MksX7s656GXPM-eWw4?usp=sharing)ã€‚'
- en: '**Deploy** it on a Hugging Face Space with [ZeroChat](https://colab.research.google.com/drive/1LcVUW5wsJTO2NGmozjji5CkC--646LgC)
    for models that have been sufficiently trained to follow a chat template (~20k
    samples).'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**éƒ¨ç½²**å®ƒåˆ°Hugging Face Spaceï¼Œå¹¶ä½¿ç”¨[ZeroChat](https://colab.research.google.com/drive/1LcVUW5wsJTO2NGmozjji5CkC--646LgC)è¿›è¡ŒèŠå¤©æ¨¡æ¿çš„è®­ç»ƒï¼ˆçº¦20kæ ·æœ¬çš„è®­ç»ƒï¼‰ã€‚'
- en: Conclusion
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: This article provided a comprehensive overview of supervised fine-tuning and
    how to apply it in practice to a Llama 3.1 8B model. By leveraging QLoRAâ€™s efficient
    memory usage, we managed to fine-tune an 8B LLM on a super high-quality dataset
    with limited GPU resources. We also provided more efficient alternatives for bigger
    runs and suggestions for further steps, including evaluation, preference alignment,
    quantization, and deployment.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡æä¾›äº†ä¸€ä¸ªå…³äºç›‘ç£å¾®è°ƒçš„å…¨é¢æ¦‚è¿°ï¼Œå¹¶è¯´æ˜äº†å¦‚ä½•åœ¨å®è·µä¸­åº”ç”¨åˆ°Llama 3.1 8Bæ¨¡å‹ã€‚é€šè¿‡åˆ©ç”¨QLoRAçš„é«˜æ•ˆå†…å­˜ä½¿ç”¨ï¼Œæˆ‘ä»¬æˆåŠŸåœ°åœ¨æœ‰é™çš„GPUèµ„æºä¸‹å¯¹8Bå¤§æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œå¹¶ä½¿ç”¨äº†ä¸€ä¸ªé«˜è´¨é‡çš„æ•°æ®é›†ã€‚æˆ‘ä»¬è¿˜æä¾›äº†æ›´é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œä»¥åº”å¯¹æ›´å¤§çš„è¿è¡Œä»»åŠ¡ï¼Œå¹¶æä¾›äº†è¿›ä¸€æ­¥æ­¥éª¤çš„å»ºè®®ï¼ŒåŒ…æ‹¬è¯„ä¼°ã€åå¥½å¯¹é½ã€é‡åŒ–å’Œéƒ¨ç½²ã€‚
- en: I hope this guide was useful. If youâ€™re interested in learning more about LLMs,
    I recommend checking the [LLM Course](https://github.com/mlabonne/llm-course).
    If you enjoyed this article, follow me on X [@maximelabonne](https://x.com/maximelabonne)
    and on Hugging Face [@mlabonne](https://huggingface.co/mlabonne). Good luck fine-tuning
    models!
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å¸Œæœ›è¿™ç¯‡æŒ‡å—å¯¹ä½ æœ‰æ‰€å¸®åŠ©ã€‚å¦‚æœä½ æœ‰å…´è¶£äº†è§£æ›´å¤šå…³äºLLMçš„å†…å®¹ï¼Œæˆ‘æ¨èä½ æŸ¥çœ‹[LLMè¯¾ç¨‹](https://github.com/mlabonne/llm-course)ã€‚å¦‚æœä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œæ¬¢è¿åœ¨Xå¹³å°ä¸Šå…³æ³¨æˆ‘[@maximelabonne](https://x.com/maximelabonne)å’Œåœ¨Hugging
    Faceä¸Šå…³æ³¨[@mlabonne](https://huggingface.co/mlabonne)ã€‚ç¥ä½ å¾®è°ƒæ¨¡å‹é¡ºåˆ©ï¼
