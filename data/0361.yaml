- en: '3 Key Encoding Techniques for Machine Learning: A Beginner-Friendly Guide with
    Pros, Cons, and Python Code Examples'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习的3种关键编码技术：适合初学者的指南，包含优缺点和Python代码示例
- en: 原文：[https://towardsdatascience.com/3-key-encoding-techniques-for-machine-learning-a-beginner-friendly-guide-aff8a01a7b6a?source=collection_archive---------1-----------------------#2024-02-07](https://towardsdatascience.com/3-key-encoding-techniques-for-machine-learning-a-beginner-friendly-guide-aff8a01a7b6a?source=collection_archive---------1-----------------------#2024-02-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/3-key-encoding-techniques-for-machine-learning-a-beginner-friendly-guide-aff8a01a7b6a?source=collection_archive---------1-----------------------#2024-02-07](https://towardsdatascience.com/3-key-encoding-techniques-for-machine-learning-a-beginner-friendly-guide-aff8a01a7b6a?source=collection_archive---------1-----------------------#2024-02-07)
- en: How should we choose between label, one-hot, and target encoding?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们应该如何在标签编码、独热编码和目标编码之间做选择呢？
- en: '[](https://medium.com/@ryuryu09030903?source=post_page---byline--aff8a01a7b6a--------------------------------)[![Ryu
    Sonoda](../Images/52445252872ed381dd86d3ada5665e1b.png)](https://medium.com/@ryuryu09030903?source=post_page---byline--aff8a01a7b6a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--aff8a01a7b6a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--aff8a01a7b6a--------------------------------)
    [Ryu Sonoda](https://medium.com/@ryuryu09030903?source=post_page---byline--aff8a01a7b6a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ryuryu09030903?source=post_page---byline--aff8a01a7b6a--------------------------------)[![Ryu
    Sonoda](../Images/52445252872ed381dd86d3ada5665e1b.png)](https://medium.com/@ryuryu09030903?source=post_page---byline--aff8a01a7b6a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--aff8a01a7b6a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--aff8a01a7b6a--------------------------------)
    [Ryu Sonoda](https://medium.com/@ryuryu09030903?source=post_page---byline--aff8a01a7b6a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--aff8a01a7b6a--------------------------------)
    ·15 min read·Feb 7, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--aff8a01a7b6a--------------------------------)
    ·15分钟阅读·2024年2月7日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '**Why Do We Need Encoding?**'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么我们需要编码？**'
- en: In the realm of machine learning, most algorithms demand inputs in numeric form,
    especially in many popular Python frameworks. For instance, in scikit-learn, linear
    regression, and neural networks require numerical variables. This means we need
    to transform categorical variables into numeric ones for these models to understand
    them. However, this step isn’t always necessary for models like tree-based ones.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习领域，大多数算法要求输入为数字形式，特别是在许多流行的Python框架中。例如，在scikit-learn中，线性回归和神经网络要求数字变量。这意味着我们需要将分类变量转换为数值变量，以便这些模型能够理解它们。然而，对于像基于树的模型，通常不需要这一过程。
- en: Today, I’m thrilled to introduce three fundamental encoding techniques that
    are essential for every budding data scientist! Plus, I’ve included a practical
    tip to help you see these techniques in action at the end! Unless stated, all
    the codes and pictures are created by the author.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，我很高兴为大家介绍三种基本的编码技术，它们是每个初学数据科学家的必备技能！另外，我在最后还附上了一个实用的小贴士，帮助你更好地理解这些技术的实际应用！除非特别说明，所有代码和图片均由作者创作。
- en: '**Label Encoding / Ordinal Encoding**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**标签编码 / 顺序编码**'
- en: Both label encoding and ordinal encoding involve assigning integers to different
    classes. The distinction lies in whether the categorical variable inherently has
    an order. For example, responses like ‘strongly agree,’ ‘agree,’ ‘neutral,’ ‘disagree,’
    and ‘strongly disagree’ are ordinal as they follow a specific sequence. When a
    variable doesn’t have such an order, we use label encoding.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 标签编码和顺序编码都涉及将整数分配给不同的类别。区别在于类别变量是否本身具有顺序。例如，“强烈同意”，“同意”，“中立”，“不同意”和“强烈不同意”是有序的，因为它们遵循特定的顺序。当一个变量没有这样的顺序时，我们使用标签编码。
- en: Let’s delve into **label encoding**.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨**标签编码**。
- en: I’ve prepared a synthetic dataset with math test scores and students’ favorite
    subjects. This dataset is designed to reflect higher scores for students who prefer
    STEM subjects. The following code shows how it is synthesized.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经准备了一个合成数据集，包含数学考试成绩和学生最喜欢的科目。这个数据集旨在反映偏爱STEM科目的学生获得更高的分数。以下代码展示了数据集的合成过程。
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/425df2f19fb2c7f82ab651ebec06fa83.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/425df2f19fb2c7f82ab651ebec06fa83.png)'
- en: You’ll be amazed at how simple it is to encode your data — it takes just a single
    line of code! You can pass a dictionary that maps between the subject name and
    number to the default method of the pandas dataframe like the following.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你会惊讶于数据编码是多么简单——只需一行代码！你可以传递一个字典，将主题名称和数字映射到 pandas 数据框的默认方法，如下所示。
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/669fe3aa0c74e4e450d749624c259f84.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/669fe3aa0c74e4e450d749624c259f84.png)'
- en: Encoded manually
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 手动编码
- en: But what if you’re dealing with a vast array of classes, or perhaps you’re looking
    for a more straightforward approach? That’s where the **scikit-learn** library’s
    `**LabelEncoder**` function comes in handy. It automatically encodes your classes
    based on their alphabetical order. For the best experience, I recommend using
    version 1.4.0, which supports all the encoders we’re discussing.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果你处理的是大量的类别，或者你希望找到一种更直接的方法怎么办？这时，**scikit-learn** 库中的`**LabelEncoder**`函数就派上了用场。它根据类别的字母顺序自动编码。为了获得最佳体验，我建议使用
    1.4.0 版本，它支持我们正在讨论的所有编码器。
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/c834b34b76ab0d0baaf65601dac39920.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c834b34b76ab0d0baaf65601dac39920.png)'
- en: Encoded using scikit-learn library
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 库进行编码
- en: 'However, there’s a catch. Consider this: our dataset doesn’t imply an ordinal
    relationship between favorite subjects. For instance, ‘History’ is encoded as
    0, but that doesn’t mean it’s ‘inferior’ to ‘Math,’ which is encoded as 3\. Similarly,
    the numerical gap between ‘English’ and ‘Science’ is smaller than that between
    ‘English’ and ‘History,’ but this doesn’t necessarily reflect their relative similarity.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有一个问题。考虑一下：我们的数据集并没有暗示喜欢的科目之间存在有序关系。例如，“历史”被编码为 0，但这并不意味着它比编码为 3 的“数学”更“低级”。类似地，“英语”和“科学”之间的数值差距小于“英语”和“历史”之间的差距，但这不一定反映它们之间的相对相似性。
- en: This encoding approach also affects interpretability in some algorithms. For
    example, in linear regression, each coefficient indicates the expected change
    in the outcome variable for a one-unit change in a predictor. But how do we interpret
    a ‘unit change’ in a subject that’s been numerically encoded? Let’s put this into
    perspective with a linear regression on our dataset.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这种编码方法也会影响某些算法的可解释性。例如，在线性回归中，每个系数表示预测变量变化 1 单位时，结果变量的预期变化。但是，当一个主题被数值编码时，我们该如何解释“单位变化”？让我们通过对我们的数据集进行线性回归来进行理解。
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/283806185bd22927dfab305bad5004c1.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/283806185bd22927dfab305bad5004c1.png)'
- en: How can we interpret the coefficient 8.26 here? The naive way would be when
    the label changes by 1 unit, the test score changes by 8\. However, it is not
    really true from Science (encoded as 1) to History (encoded as 2) since I synthesized
    in a way that the mean score would be 80 and 70 respectively. So, we should not
    interpret the coefficient when there is no meaning in the way we label each class!
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何解释这里的系数 8.26 呢？最直观的方式是，当标签变化 1 单位时，测试分数变化 8。但这对于从科学（编码为 1）到历史（编码为 2）并不完全正确，因为我合成的方式使得平均分分别为
    80 和 70。所以，当我们标记每个类别的方式没有实际意义时，我们不应解释该系数！
- en: 'Now, moving on to **ordinal encoding**, let’s apply it to another synthetic
    dataset, this time focusing on height and school categories. I’ve tailored this
    dataset to reflect average heights for different school levels: 110 cm for kindergarten,
    140 cm for elementary school, and so on. Let’s see how this plays out.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，转到**有序编码**，我们将其应用于另一个合成数据集，这次专注于身高和学校类别。我已经调整这个数据集，以反映不同学校级别的平均身高：幼儿园 110
    厘米，小学 140 厘米，依此类推。让我们看看结果如何。
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/d812fbef79a68c55496d48fc51d143cb.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d812fbef79a68c55496d48fc51d143cb.png)'
- en: A part of the synthesized shcool height data
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一部分合成的学校身高数据
- en: The `**OrdinalEncoder**` from scikit-learn’s preprocessing toolkit is a real
    gem for handling ordinal variables. It’s intuitive, automatically determining
    the ordinal structure and encoding it accordingly. If you look at encoder.categories_,
    you can check how the variable was encoded.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 的预处理工具包中的`**OrdinalEncoder**`是处理有序变量的一个真正的宝藏。它非常直观，自动确定有序结构并相应地进行编码。如果你查看
    encoder.categories_，你可以检查变量是如何被编码的。
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/6bfbcd0470e04147051ae7fb70ebdc82.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6bfbcd0470e04147051ae7fb70ebdc82.png)'
- en: After encoded
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 编码完成后
- en: When it comes to ordinal categorical variables, interpreting linear regression
    models becomes more straightforward. The encoding reflects the degree of education
    in a numerical order — the higher the education level, the higher its corresponding
    value.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于序数类别变量，解释线性回归模型变得更加简单。编码反映了教育程度的数值顺序——教育程度越高，其对应的数值越大。
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/14d3d127f4382cfa23145eabdf63eb02.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14d3d127f4382cfa23145eabdf63eb02.png)'
- en: 'The model reveals something quite intuitive: a one-unit change in school type
    corresponds to a 17.5 cm increase in height. This makes perfect sense given our
    dataset!'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 模型揭示了一些直观的内容：学校类型的单位变化对应身高增加17.5厘米。考虑到我们的数据集，这完全合情合理！
- en: 'So, let’s wrap up with a quick summary of **label/ordinal** encoding:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们用一个快速的总结来结束**标签/序数**编码：
- en: 'Pros:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 优点：
- en: '- Simplicity: It’s user-friendly and easy to implement.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '- 简单性：用户友好，易于实现。'
- en: '- Efficiency: This method is light on computational resources and memory, creating
    just one new numerical feature.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '- 效率：这种方法对计算资源和内存的要求较低，仅生成一个新的数值特征。'
- en: '- Ideal for Ordinal Categories: It shines when dealing with categorical variables
    that have a natural order.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '- 适用于有序类别：在处理具有自然顺序的类别变量时，独热编码表现得尤为出色。'
- en: 'Cons:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: '- Implied Order: One potential downside is that it can introduce a sense of
    order where none exists, potentially leading to misinterpretation (like assuming
    a category labeled ‘3’ is superior to one labeled ‘2’).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '- 隐含顺序：一个潜在的缺点是，它可能会引入一种本不存在的顺序，导致误解（比如认为标记为‘3’的类别优于标记为‘2’的类别）。'
- en: '- Not Always Suitable: Certain algorithms, such as linear or logistic regression,
    might incorrectly interpret the encoded numerical values as having ordinal significance.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '- 并非总是适用：某些算法，如线性回归或逻辑回归，可能会错误地将编码后的数值解释为具有序数意义。'
- en: '**One-hot encoding**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**独热编码**'
- en: 'Next up, let’s dive into another encoding technique that addresses the interpretability
    issue: **One-hot encoding**.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们深入探讨另一种解决可解释性问题的编码技术：**独热编码**。
- en: The core issue with label encoding is that it imposes an ordinal structure on
    variables that don’t inherently have one, by replacing categories with numerical
    values. **One-hot encoding tackles this by creating a separate column for each
    class. Each of these columns contains binary values, indicating whether the row
    belongs to that class.** It’s like pivoting the data to a wider format, for those
    who are familiar with that concept. To make this clearer, let’s see an example
    using the math_score and subject data. The `**OneHotEncoder**` from sklearn.preprocessing
    is perfect for this task.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 标签编码的核心问题是，它为那些本身没有顺序的变量强加了一个序数结构，通过将类别替换为数值。**独热编码通过为每个类别创建一个单独的列来解决这个问题。每个列包含二进制值，表示该行是否属于该类别。**
    这就像是将数据转换为更宽格式，对于熟悉这种概念的人来说。为了更清楚地说明这一点，我们来看一个使用`math_score`和`subject`数据的例子。`**OneHotEncoder**`来自sklearn.preprocessing，非常适合这个任务。
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/ff50f995059ddc727f541bdec6365237.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff50f995059ddc727f541bdec6365237.png)'
- en: Encoded by one-hot encoding
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通过独热编码进行编码
- en: Now, instead of having a single ‘Subject’ column, our dataset features individual
    columns for each subject. This effectively eliminates any unintended ordinal structure!
    However, the process here is a bit more involved, so let me explain.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的数据集不再只有一个‘学科’列，而是为每个学科单独设置了列。这有效地消除了任何不必要的序数结构！不过，这个过程稍微复杂一些，我来解释一下。
- en: 'Like with label/ordinal encoding, you first need to define your encoder. But
    the output of one-hot encoding differs: while label/ordinal encoding returns a
    numpy array, one-hot encoding typically produces a `scipy.sparse._csr.csr_matrix`.
    To integrate this with a pandas dataframe, you’ll need to convert it into an array.
    Then, create a new dataframe with this array and assign column names, which you
    can get from the encoder’s `get_feature_names_out()` method. Alternatively, you
    can get numpy array directly by setting `sparse_output=False` when defining the
    encoder.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 与标签/序数编码一样，你首先需要定义编码器。但独热编码的输出有所不同：标签/序数编码返回一个numpy数组，而独热编码通常生成一个`scipy.sparse._csr.csr_matrix`。为了将其与pandas数据框架结合使用，你需要将其转换为数组。然后，创建一个包含这个数组的新数据框，并为其指定列名，这些列名可以通过编码器的`get_feature_names_out()`方法获取。或者，你也可以通过在定义编码器时设置`sparse_output=False`来直接获得numpy数组。
- en: However, in practical applications, you don’t need to go through all these steps.
    I’ll show you a more streamlined approach using `**make_column_transformer**`
    towards the end of our discussion!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实际应用中，你不需要经历所有这些步骤。我将在讨论的最后向你展示使用`**make_column_transformer**`的更简化方法！
- en: Now, let’s proceed with running a linear regression on our one-hot encoded data.
    This should make the interpretation much easier, right?
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续对我们的独热编码数据进行线性回归。这应该使得解释变得更容易，对吧？
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/a4ff2048605cc43f2b11c754ec38397a.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4ff2048605cc43f2b11c754ec38397a.png)'
- en: Intercept and coefficients for each column
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 每列的截距和系数
- en: But wait, why are the coefficients so tiny, and the intercept so large? What’s
    going wrong here? This conundrum is a specific issue in linear regression known
    as perfect multicollinearity. Perfect multicollinearity occurs when when one variable
    in a linear regression model can be perfectly predicted from the others, which
    in the case of one-hot encoding happens because one class can be inferred if all
    other classes are zero. To sidestep this problem, we can drop one of the classes
    by setting `OneHotEncoder(drop=”first”)`. Let’s check out the impact of this adjustment.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 等等，为什么系数这么小，截距又这么大？到底出了什么问题？这个困惑是线性回归中的一个特定问题，叫做完美多重共线性。完美多重共线性发生在当线性回归模型中的一个变量可以通过其他变量完美预测时，独热编码中也会出现这种情况，因为如果其他类别都为零，一个类别就可以被推断出来。为了避免这个问题，我们可以通过设置`OneHotEncoder(drop=”first”)`来去除一个类别。让我们看看这一调整的影响。
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/0900f84041b8739332f51a45eb66a95f.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0900f84041b8739332f51a45eb66a95f.png)'
- en: Intercept and coeffcients for each column with dropping one column
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 每列的截距和系数（去除一列）
- en: Here, the column for English has been dropped, and now the coefficients seem
    much more reasonable! Plus, they’re easier to interpret. When all the one-hot
    encoded columns are zero (indicating English as the favorite subject), we predict
    the test score to be around 71 (aligned with our defined average score for English).
    For History, it would be 71 minus 11 equals 60, for Math, 71 plus 19, and so on.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，英语列已经被去除，现在系数看起来更加合理了！而且，它们也更容易解释。当所有的独热编码列都为零（表示英语是最喜欢的科目）时，我们预测考试成绩大约为71（与我们为英语定义的平均分数一致）。对于历史科目，成绩是71减去11等于60，对于数学，成绩是71加上19，以此类推。
- en: 'However, there’s a significant caveat with one-hot encoding: it can lead to
    high-dimensional datasets, especially when the variable has a large number of
    classes. Let’s consider a dataset that includes 1000 rows, each representing a
    unique product with various features, including a category that spans 100 different
    types.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，独热编码有一个显著的警告：它可能导致高维数据集，特别是当变量具有大量类别时。让我们考虑一个包含1000行数据集的情况，每一行代表一个具有各种特征的独特产品，其中一个类别跨越了100种不同类型。
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/1ac7b8734b85d8db3f69932a87037dd7.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ac7b8734b85d8db3f69932a87037dd7.png)'
- en: The synthesized dataset for products
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 产品的合成数据集
- en: Note that the dataset’s dimensions are 1000 rows by 5 columns. Now, let’s observe
    the changes after applying a one-hot encoder.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，数据集的维度是1000行和5列。现在，让我们观察应用独热编码后的变化。
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/925f370c09ba99ca0a025fc32e89fdbf.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/925f370c09ba99ca0a025fc32e89fdbf.png)'
- en: The dimension increased significantly!
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 维度显著增加了！
- en: After applying one-hot encoding, our dataset’s dimension balloons to 1000x201
    — a whopping 40 times larger than before. This increase is a concern, as it demands
    more memory. Moreover, you’ll notice that most of the values in the newly created
    columns are zeros, resulting in what we call a sparse dataset. Certain models,
    especially tree-based ones, struggle with sparse data. Furthermore, other challenges
    arise when dealing with high-dimensional data often referred to as the ‘curse
    of dimensionality.’ Also, since one-hot encoding treats each class as an individual
    column, we lose any ordinal information. Therefore, if the classes in your variable
    inherently have a hierarchical order, one-hot encoding might not be your best
    choice.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用独热编码后，我们的数据集维度膨胀到了1000x201——比之前大了整整40倍。这一增长是个问题，因为它需要更多的内存。而且，你会注意到新创建的列中的大多数值都是零，这就导致了我们所谓的稀疏数据集。某些模型，特别是基于树的模型，处理稀疏数据时会遇到困难。此外，处理高维数据时会出现其他挑战，通常被称为“维度灾难”。另外，由于独热编码将每个类别当作一个独立的列，我们会失去任何顺序信息。因此，如果你的变量中的类别本身具有层次顺序，独热编码可能不是最佳选择。
- en: How do we tackle these disadvantages? One approach is to use a different encoding
    method. Alternatively, you can limit the number of classes in the variable. Often,
    even with a large number of classes, the majority of values for a variable are
    concentrated in just a few classes. In such cases, treating these minority classes
    as ‘others’ can be effective. This can be achieved by setting parameters like
    `**min_frequency**` or `**max_categories**` in OneHotEncoder. Another strategy
    for dealing with sparse data involves techniques like feature hashing, which essentially
    simplifies the representation by mapping multiple categories to a lower-dimensional
    space using a hash function, or dimension reduction techniques like PCA.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何应对这些缺点？一种方法是使用不同的编码方式。或者，你可以限制变量的类别数量。通常，即使类别数量很多，变量的大多数值也集中在少数几个类别中。在这种情况下，将这些少数类别视为“其他”类别可能会很有效。这可以通过设置如`**min_frequency**`或`**max_categories**`之类的参数，在OneHotEncoder中实现。另一种应对稀疏数据的策略是使用特征哈希技术，本质上是通过哈希函数将多个类别映射到一个低维空间，或者使用PCA等降维技术。
- en: 'Here’s a quick summary of **One-hot encoding**:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这是**独热编码**的简要总结：
- en: 'Pros:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 优点：
- en: '- Prevents Misleading Interpretations: It avoids the risk of models misinterpreting
    the data as having some sort of order, an issue prevalent in label/target encoding.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '- 防止误导性解释：避免模型将数据误解为有某种顺序，这是标签/目标编码中常见的问题。'
- en: '- Suitable for Non-Ordinal Features: Ideal for categorical data without an
    ordinal relationship.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '- 适用于非序数特征：非常适合没有序数关系的类别数据。'
- en: 'Cons:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: '- Dimensionality Increase: Leads to a significant increase in the dataset’s
    dimensionality, which can be problematic, especially for variables with many categories.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '- 维度增加：会显著增加数据集的维度，这在变量类别较多时尤为成问题。'
- en: '- Sparse Matrix: Results in many columns filled with zeros, creating sparse
    data.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '- 稀疏矩阵：会导致许多列填充零，生成稀疏数据。'
- en: '- Not Efficient with High Cardinality Features: Less effective for variables
    with a large number of categories.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '- 高基数特征效率低：对于类别数较多的变量效果较差。'
- en: '**Target Encoding**'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**目标编码**'
- en: Let’s now explore target encoding, a technique particularly effective with high-cardinality
    data and in models like tree-based algorithms.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们探索目标编码，这是一种在高基数数据和树模型等算法中特别有效的技术。
- en: The essence of target encoding is to leverage the information from the value
    of the dependent variable. Its implementation varies depending on the task. In
    regression, we encode the target variable by the mean of the dependent variable
    for each class. For binary classification, it’s done by encoding the target variable
    with the probability of being in one class (calculated as the number of rows in
    that class where the outcome is 1, divided by the total number of rows in the
    class). In multiclass classification, the categorical variable is encoded based
    on the probability of belonging to each class, resulting in as many new columns
    as there are classes in the dependent variable. To clarify, let’s use the same
    product dataset we employed for one-hot encoding.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 目标编码的本质是利用因变量的值信息。其实现方式根据任务的不同而有所变化。在回归任务中，我们通过每个类别的因变量均值来编码目标变量。在二分类任务中，编码是通过目标变量所属类别的概率来进行的（该概率通过该类别中结果为1的行数与该类别总行数之比计算得到）。在多分类任务中，类别变量是基于所属每个类别的概率进行编码的，结果会产生与因变量中的类别数量相等的新列。为了更清楚地说明这一点，让我们使用与独热编码相同的商品数据集。
- en: Let’s begin with target encoding for a regression task. Imagine we want to predict
    the price of goods and aim to encode the product type. Similar to other encodings,
    we use **TargetEncoder** from sklearn.preprocessing!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从回归任务中的目标编码开始。假设我们想要预测商品价格，并希望对商品类型进行编码。与其他编码方式类似，我们使用来自sklearn.preprocessing的**TargetEncoder**！
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/2bb9fff8787574918ac3354c70c58f01.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2bb9fff8787574918ac3354c70c58f01.png)'
- en: Target encoding for regression
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 回归任务中的目标编码
- en: After the encoding, you’ll notice that, despite the variable having many classes,
    the dataset’s dimension remains unchanged (1000 x 5). You can also observe how
    each class is encoded. Although I mentioned that the encoding for each class is
    based on the mean of the target variable for that class, you’ll find that the
    actual mean differs slightly from the encoding using the default settings. This
    discrepancy arises because, by default, the function automatically selects a smoothing
    parameter. This parameter blends the local category mean with the overall global
    mean, which is particularly useful to prevent overfitting in categories with limited
    samples. If we set `smooth=0`, the encoded values align precisely with the actual
    means.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 编码后，你会注意到，尽管变量有许多类别，数据集的维度保持不变（1000 x 5）。你还可以观察每个类别是如何编码的。尽管我提到每个类别的编码是基于该类别的目标变量的均值，但你会发现，使用默认设置进行编码时，实际均值与编码结果略有不同。这种差异产生的原因是，默认情况下，函数会自动选择一个平滑参数。该参数将本地类别的均值与全局均值相结合，这对于防止样本量较少的类别过拟合非常有用。如果我们设置`smooth=0`，编码值将与实际均值完全一致。
- en: Now, let’s consider binary classification. Imagine our goal is to classify whether
    the quality of a product is satisfactory. In this scenario, the encoded value
    represents the probability of a category being ‘satisfactory.’
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑二分类问题。假设我们的目标是判断产品的质量是否令人满意。在这种情况下，编码值表示该类别是“满意”的概率。
- en: '[PRE13]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/3f79e8ec250bc34257fd730c1849880c.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f79e8ec250bc34257fd730c1849880c.png)'
- en: Target encoding for binary classification
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 二分类的目标编码
- en: You can indeed see that the encoded_category represent the probability being
    “Satisfied” (float value between 0 and 1). To see how each class is encoded, you
    can check the `classes_` attribute of the encoder. For binary classification,
    the first value in the list is typically dropped, meaning that the column here
    indicates the probability of being satisfied. Conveniently, the encoder automatically
    detects the type of task, so there’s no need to specify that it’s a binary classification.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你确实可以看到，`encoded_category`表示的是“满意”的概率（介于0和1之间的浮动值）。要查看每个类别是如何编码的，你可以查看编码器的`classes_`属性。对于二分类问题，列表中的第一个值通常会被丢弃，这意味着此列表示的是满意的概率。方便的是，编码器会自动检测任务类型，因此无需指定它是一个二分类任务。
- en: Lastly, let’s see multi-class classification example. Suppose we’re predicting
    which manufacturer produced a product.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们来看一个多类分类的例子。假设我们正在预测哪家制造商生产了某个产品。
- en: '[PRE14]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](../Images/2bdc98734f9d85e95b1efc30c5c275a5.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2bdc98734f9d85e95b1efc30c5c275a5.png)'
- en: Target encoding for multi-class classification
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 多类分类的目标编码
- en: After encoding, you’ll see that we now have columns for each manufacturer. These
    columns indicate the probability of a product belonging to a certain category
    being produced by that manufacturer. Although our dataset has expanded slightly,
    the number of classes for the dependent variable is usually much smaller, so it’s
    unlikely to cause issues.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 编码后，你会看到现在我们有了每个制造商的列。这些列表示一个产品属于某个类别并由该制造商生产的概率。尽管我们的数据集略有扩展，但因变量的类别数通常较少，因此不太可能引起问题。
- en: Target encoding is particularly advantageous for tree-based models. These models
    make splits based on feature values that most effectively separate the target
    variable. By directly incorporating the mean of the target variable, target encoding
    provides a clear and efficient means for the model to make these splits, often
    more so than other encoding methods.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 目标编码对于基于树的模型尤其有利。这些模型根据特征值进行划分，目的是最有效地分隔目标变量。通过直接结合目标变量的均值，目标编码为模型提供了一种清晰且高效的划分方式，通常比其他编码方法更有效。
- en: However, caution is needed with target encoding. If there are only a few observations
    for a class, and these don’t represent the true mean for that class, there’s a
    risk of overfitting.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用目标编码时需要谨慎。如果某个类别的观测样本很少，而且这些样本不能代表该类别的真实均值，就有可能导致过拟合。
- en: 'This leads to another crucial point: it’s vital to perform target encoding
    after splitting your data into training and testing sets. Doing it beforehand
    can lead to data leakage, as the encoding would be influenced by the outcomes
    in the test dataset. This could result in the model performing exceptionally well
    on the training dataset, giving you a false impression of its efficacy. Therefore,
    to accurately assess your model’s performance, ensure target encoding is done
    post train-test split.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了另一个关键点：在将数据拆分为训练集和测试集之后执行目标编码是至关重要的。如果提前进行编码，可能会导致数据泄漏，因为编码会受到测试数据集中的结果的影响。这可能导致模型在训练数据集上表现得非常好，给您一个虚假的效果评价。因此，为了准确评估模型的性能，确保在训练集和测试集拆分后再进行目标编码。
- en: 'Here’s a quick summary of **target encoding**:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是关于**目标编码**的简要总结：
- en: 'Pros:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 优点：
- en: '- Keeps Cardinality in Check: It’s highly effective for high cardinality features
    as it doesn’t increase the feature space.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '- 控制基数：对于高基数特征非常有效，因为它不会增加特征空间。'
- en: '- Can Capture Information Within Labels: By incorporating target data, it often
    enhances predictive performance.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '- 能够捕获标签中的信息：通过引入目标数据，它通常能够提高预测性能。'
- en: '- Useful for Tree-Based Models: Particularly advantageous for complex models
    such as random forests or gradient boosting machines.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '- 对树模型有用：特别适用于复杂模型，如随机森林或梯度提升机。'
- en: 'Cons:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: '- Risk of Overfitting: There’s a heightened risk of overfitting, especially
    when categories have a limited number of observations.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '- 过拟合风险：特别是当类别的观察数据量有限时，过拟合的风险会增加。'
- en: '- Target Leakage: It may inadvertently introduce future information into the
    model, i.e., details from the target variable that wouldn’t be accessible during
    actual predictions.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '- 目标泄漏：它可能无意中将未来的信息引入模型，即目标变量中的细节，这些细节在实际预测中是无法访问的。'
- en: '- Less Interpretable: Since the transformations are based on the target, they
    can be more challenging to interpret compared to methods like one-hot or label
    encoding.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '- 可解释性较差：由于转换是基于目标的，与独热编码或标签编码等方法相比，它们可能更难解释。'
- en: '**Final tip**'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**最后的提示**'
- en: To wrap up, I’d like to offer some practical tips. Throughout this discussion,
    we’ve looked at different encoding techniques, but in reality, you might want
    to apply various encodings to different variables within a dataset. This is where
    `**make_column_transformer**` from sklearn.compose comes in handy. For example,
    suppose you’re predicting product prices and decide to use target encoding for
    the ‘Category’ due to its high cardinality, while applying one-hot encoding for
    ‘Manufacturer’ and ‘Quality’. To do this, you would define arrays containing the
    names of the variables for each encoding type and apply the function as shown
    below. This approach allows you to handle the transformed data seamlessly, leading
    you to an efficiently encoded dataset ready for your analyses!
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我想提供一些实用的建议。在整个讨论中，我们已经探讨了不同的编码技术，但实际上，您可能希望对数据集中的不同变量应用不同的编码。这时候，`**make_column_transformer**`来自sklearn.compose就非常有用了。例如，假设您正在预测产品价格，并决定对“类别”使用目标编码，因为其基数较高，而对“制造商”和“质量”使用独热编码。为此，您可以定义包含每种编码类型变量名称的数组，并像下面这样应用该函数。这个方法使得您能够无缝地处理转换后的数据，从而得到一个高效编码的数据集，准备好进行分析！
- en: '[PRE15]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](../Images/3cb2e44c1f3d49351b860bced3a3f104.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3cb2e44c1f3d49351b860bced3a3f104.png)'
- en: Combination of target and one-hot encoding using make_column_faster
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 使用make_column_faster结合目标编码和独热编码
- en: Thank you so much for taking the time to read through this! When I first embarked
    on my machine learning journey, choosing the right encoding techniques and understanding
    their implementation was quite a maze for me. I genuinely hope this article has
    shed some light for you and made your path a bit clearer!
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢您抽出时间阅读这篇文章！当我第一次开始我的机器学习旅程时，选择正确的编码技术并理解它们的实现对我来说是一件相当复杂的事。我真诚地希望这篇文章能为您提供一些启示，让您的路径变得更加清晰！
- en: '**Source:**'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**来源：**'
- en: 'Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825–2830,
    2011.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 'Scikit-learn: Python中的机器学习，Pedregosa等人，JMLR 12，页2825–2830，2011。'
- en: 'Documentation of Scikit-learn:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn文档：
- en: 'Ordinal encoder: [https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '有序编码器: [https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder)'
- en: 'Target encoder: [https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html#sklearn.preprocessing.TargetEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html#sklearn.preprocessing.TargetEncoder)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '目标编码器: [https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html#sklearn.preprocessing.TargetEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html#sklearn.preprocessing.TargetEncoder)'
- en: One-hot encoder [https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder)
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 独热编码器 [https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder)
