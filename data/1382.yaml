- en: A Deep Dive into Fine-Tuning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入探讨微调
- en: 原文：[https://towardsdatascience.com/stepping-out-of-the-comfort-zone-through-domain-adaptation-a-deep-dive-into-dynamic-prompting-4860c6d16224?source=collection_archive---------0-----------------------#2024-06-03](https://towardsdatascience.com/stepping-out-of-the-comfort-zone-through-domain-adaptation-a-deep-dive-into-dynamic-prompting-4860c6d16224?source=collection_archive---------0-----------------------#2024-06-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/stepping-out-of-the-comfort-zone-through-domain-adaptation-a-deep-dive-into-dynamic-prompting-4860c6d16224?source=collection_archive---------0-----------------------#2024-06-03](https://towardsdatascience.com/stepping-out-of-the-comfort-zone-through-domain-adaptation-a-deep-dive-into-dynamic-prompting-4860c6d16224?source=collection_archive---------0-----------------------#2024-06-03)
- en: Stepping out of the “comfort zone” — part 3/3 of a deep-dive into domain adaptation
    approaches for LLMs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 脱离“舒适区”——大型语言模型领域适应方法深度解析系列的第三部分/共三部分
- en: '[](https://medium.com/@aris.tsakpinis?source=post_page---byline--4860c6d16224--------------------------------)[![Aris
    Tsakpinis](../Images/2cc1101aed68e1f71a0026bfdec28f58.png)](https://medium.com/@aris.tsakpinis?source=post_page---byline--4860c6d16224--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4860c6d16224--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4860c6d16224--------------------------------)
    [Aris Tsakpinis](https://medium.com/@aris.tsakpinis?source=post_page---byline--4860c6d16224--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@aris.tsakpinis?source=post_page---byline--4860c6d16224--------------------------------)[![Aris
    Tsakpinis](../Images/2cc1101aed68e1f71a0026bfdec28f58.png)](https://medium.com/@aris.tsakpinis?source=post_page---byline--4860c6d16224--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4860c6d16224--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4860c6d16224--------------------------------)
    [Aris Tsakpinis](https://medium.com/@aris.tsakpinis?source=post_page---byline--4860c6d16224--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4860c6d16224--------------------------------)
    ·24 min read·Jun 3, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[数据科学前沿](https://towardsdatascience.com/?source=post_page---byline--4860c6d16224--------------------------------)
    ·阅读时间24分钟·2024年6月3日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/0f95d6e4633f3120af41fb5091422cde.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f95d6e4633f3120af41fb5091422cde.png)'
- en: Photo by StableDiffusionXL on Amazon Web Services
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：StableDiffusionXL（来自Amazon Web Services）
- en: Exploring domain adapting large language models (LLMs) to your specific domain
    or use case? This **3-part blog post series** explains the motivation for domain
    adaptation and dives deep into various options to do so. Further, a detailed guide
    for mastering the entire domain adaptation journey covering popular tradeoffs
    is being provided.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 想要将领域适应技术应用到大型语言模型（LLMs），以适应你的特定领域或使用案例吗？这篇**三部分系列博客文章**阐述了领域适应的动机，并深入探讨了实现这一目标的多种选项。此外，还提供了一个详细的指南，帮助你掌握整个领域适应过程，涵盖了流行的权衡选择。
- en: '[*Part 1: Introduction into domain adaptation — motivation, options, tradeoffs*](/stepping-out-of-the-comfort-zone-through-domain-adaptation-a-deep-dive-into-dynamic-prompting-47a865b16740)[*Part
    2: A deep dive into in-context learning*](/stepping-out-of-the-comfort-zone-through-domain-adaptation-a-deep-dive-into-dynamic-prompting-9ee1d71d1e35)
    *Part 3: A deep dive into fine-tuning* ***— You’re here!***'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第1部分：领域适应简介 — 动机、选项与权衡*](/stepping-out-of-the-comfort-zone-through-domain-adaptation-a-deep-dive-into-dynamic-prompting-47a865b16740)[*第2部分：深入探讨上下文学习*](/stepping-out-of-the-comfort-zone-through-domain-adaptation-a-deep-dive-into-dynamic-prompting-9ee1d71d1e35)
    *第3部分：深入探讨微调* ***— 你正在阅读此部分！***'
- en: 'Note: All images, unless otherwise noted, are by the author.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 注：所有图片，除非另有说明，均为作者提供。
- en: Recap
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回顾
- en: 'In the previous part of this blog post series, we explored the concept of in-context
    learning as a powerful approach to overcome the “comfort zone” limitations of
    large language models (LLMs). We discussed how these techniques can be used to
    transform tasks and move them back into the models’ areas of expertise, leading
    to improved performance and alignment with the key design principles of Helpfulness,
    Honesty, and Harmlessness. In this third part, we will shift our focus to the
    second domain adaptation approach: fine-tuning. We will dive into the details
    of fine-tuning, exploring how it can be leveraged to expand the models’ “comfort
    zones” and hence uplift performance by adapting them to specific domains and tasks.
    We will discuss the trade-offs between prompt engineering and fine-tuning, and
    provide guidance on when to choose one approach over the other based on factors
    such as data velocity, task ambiguity, and other considerations.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本系列博客文章的前一部分中，我们探讨了上下文学习的概念，作为一种强有力的方法来克服大规模语言模型（LLMs）“舒适区”限制。我们讨论了如何利用这些技术将任务转化并使其回到模型的专业领域，从而提升性能，并与有用性、诚实性和无害性等关键设计原则保持一致。在第三部分中，我们将重点转向第二种领域适应方法：微调。我们将深入探讨微调的细节，探索如何利用它来扩展模型的“舒适区”，从而通过将模型适应特定领域和任务来提升性能。我们还将讨论提示工程与微调之间的权衡，并根据数据流速、任务模糊性等因素提供选择合适方法的指导。
- en: Transformers 101
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变换器基础
- en: Most state-of-the-art LLMs are powered by the transformer architecture, a family
    of deep neural network architectures which has disrupted the field of NLP after
    being proposed by [Vaswani et al in 2017](https://arxiv.org/abs/1706.03762), breaking
    all common benchmarks across the domain. The core differentiator of this architecture
    family is a concept called “attention” which excels in capturing the semantic
    meaning of words or larger pieces of natural language based on the context they
    are used in.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数最先进的LLM都采用变换器架构，这是一类深度神经网络架构，自从[Vaswani等人在2017年提出](https://arxiv.org/abs/1706.03762)以来，已经颠覆了自然语言处理（NLP）领域，打破了该领域的所有常规基准。这个架构家族的核心区分点是一个叫做“注意力”（attention）的概念，它在根据上下文捕捉词语或更大语义单元的含义方面表现出色。
- en: The transformer architecture consists of two fundamentally different building
    blocks. On the one side, the “encoder” block focuses on translating the semantics
    of natural language into so-called contextualized embeddings, which are mathematical
    representations in the vector space. This makes encoder models particularly useful
    in use cases utilizing these vector representations for downstream deterministic
    or probabilistic tasks like classification problems, NER, or semantic search.
    On the other side, the decoder block is trained on next-token prediction and hence
    capable of generatively producing text if used in a recursive manner. They can
    be used for all tasks relying on the generation of text. These building blocks
    can be used independently of each other, but also in combination. Most of the
    models referred to within the field of generative AI today are decoder-only models.
    This is why this blog post will focus on this type of model.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器架构由两个基本上不同的构建模块组成。一方面，“编码器”模块专注于将自然语言的语义转换为所谓的上下文化嵌入（contextualized embeddings），这些嵌入是向量空间中的数学表示。这使得编码器模型在使用这些向量表示进行下游确定性或概率性任务（如分类问题、命名实体识别或语义搜索）时特别有用。另一方面，解码器模块则以下一个标记预测为训练目标，因此在递归使用时能够生成文本。它们可以用于所有依赖文本生成的任务。这些构建模块可以独立使用，也可以结合使用。目前，生成式人工智能领域大多数提到的模型都是仅使用解码器的模型。这也是本文将重点讨论这一类型模型的原因。
- en: '![](../Images/169c0e20afc902b109b03c67b20db3f6.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/169c0e20afc902b109b03c67b20db3f6.png)'
- en: 'Figure 1: The transformer architecture (adapted from Vaswani et al, 2017)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：变换器架构（改编自Vaswani等人，2017年）
- en: E2E fine-tuning pipeline
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: E2E 微调流程
- en: 'Fine-tuning leverages transfer learning to efficiently inject niche expertise
    into a foundation model like LLaMA2\. The process involves updating the model’s
    weights through training on domain-specific data, while keeping the overall network
    architecture unchanged. Unlike full pre-training which requires massive datasets
    and compute, fine-tuning is highly sample and compute efficient. On a high level,
    the end-to-end process can be broken down into the following phases:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 微调利用迁移学习高效地将特定领域的专业知识注入到像LLaMA2这样的基础模型中。该过程包括通过在特定领域的数据上训练来更新模型的权重，同时保持整体网络架构不变。与需要大量数据集和计算资源的全预训练不同，微调在样本和计算方面都非常高效。从高层次来看，端到端的过程可以分为以下几个阶段：
- en: '![](../Images/48caf5fb29411dde0f0a062bbf783067.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/48caf5fb29411dde0f0a062bbf783067.png)'
- en: 'Figure 2: E2E fine-tuning pipeline'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：端到端微调流程
- en: '**Data collection and selection:** The set of proprietary data to be ingested
    into the model needs to be carefully selected. On top of that, for specific fine-tuning
    purposes data might not be available yet and has to be purposely collected. Depending
    on the data available and task to be achieved through fine-tuning, data of different
    quantitative or qualitative characteristics might be selected (e.g. labeled, un-labeled,
    preference data — see below) Besides the data quality aspect, dimensions like
    data source, confidentiality and IP, licensing, copyright, PII and more need to
    be considered.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据收集与选择：** 要输入模型的专有数据集需要经过仔细挑选。除此之外，针对特定微调目的，数据可能尚不可用，必须有目的地收集。根据可用数据和通过微调实现的任务，可能会选择具有不同定量或定性特征的数据（例如，标记数据、未标记数据、偏好数据——见下文）。除了数据质量方面，还需要考虑数据来源、保密性与知识产权、许可、版权、个人身份信息（PII）等维度。'
- en: LLM pre-training usually leverages a mix of web scrapes and curated corpora,
    the nature of fine-tuning as a domain adaptation approach implies that the datasets
    used are mostly curated corpora of labeled or unlabelled data specific to an organizational,
    knowledge, or task-specific domain.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: LLM预训练通常利用网络抓取和精心挑选的语料库，作为一种领域适应方法的微调特性意味着使用的数据集大多是针对特定组织、知识或任务领域的标记或未标记的精心挑选的语料库。
- en: '![](../Images/ff6d46928f4776dc9590d9ce6f876f41.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff6d46928f4776dc9590d9ce6f876f41.png)'
- en: 'Figure 3: Pre-training vs. fine-tuning: data composition and selection criteria'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：预训练与微调：数据构成与选择标准
- en: While this data can be sourced differently (document repositories, human-created
    content, etc.), this underlines that for fine-tuning, it is important to carefully
    select the data with respect to quality, but as mentioned above, also consider
    topics like confidentiality and IP, licensing, copyright, PII, and others.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些数据可以通过不同方式获取（文档库、人类创建的内容等），但这强调了对于微调而言，必须谨慎地根据质量选择数据，但如前所述，还要考虑保密性和知识产权、许可、版权、个人身份信息（PII）等问题。
- en: '![](../Images/a4742eeaea7bfc6447c2a812fa29cd9e.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4742eeaea7bfc6447c2a812fa29cd9e.png)'
- en: 'Figure 4: Data requirements per fine-tuning approach'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：每种微调方法的数据需求
- en: In addition to this, an important dimension is the categorization of the training
    dataset into unlabelled and labeled (including preference) data. Domain adaptation
    fine-tuning requires unlabelled textual data (as opposed to other fine-tuning
    approaches, see the figure 4). In other words, we can simply use any full-text
    documents in natural language that we consider to be of relevant content and sufficient
    quality. This could be user manuals, internal documentation, or even legal contracts,
    depending on the actual use case.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一个重要的维度是将训练数据集分类为未标记数据和标记数据（包括偏好数据）。领域适应微调需要未标记的文本数据（与其他微调方法不同，见图4）。换句话说，我们可以简单地使用任何我们认为相关且足够质量的自然语言全文档。这可以是用户手册、内部文档，甚至是法律合同，具体取决于实际的应用场景。
- en: On the other hand, labeled datasets like an instruction-context-response dataset
    can be used for supervised fine-tuning approaches. Lately, reinforcement learning
    approaches for aligning models to actual user feedback have shown great results,
    leveraging human- or machine-created preference data, e.g., binary human feedback
    (thumbs up/down) or multi-response ranking.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，像指令-上下文-响应数据集这样的标记数据集可以用于监督微调方法。最近，通过强化学习方法将模型与实际用户反馈对齐，取得了显著成果，利用了人类或机器创建的偏好数据，例如二元人类反馈（好/差）或多项响应排序。
- en: As opposed to unlabeled data, labeled datasets are more difficult and expensive
    to collect, especially at scale and with sufficient domain expertise. Open-source
    data hubs like [HuggingFace Datasets](https://huggingface.co/docs/hub/en/datasets-overview)
    can be good sources for labeled datasets, especially in areas where the broader
    part of a relevant human population group agrees (e.g., a toxicity dataset for
    red-teaming), and using an open-source dataset as a proxy for the model’s real
    users’ preferences is sufficient.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 与未标注数据不同，标注数据集的收集更为困难和昂贵，特别是在大规模收集且具备足够领域专业知识时。像[HuggingFace Datasets](https://huggingface.co/docs/hub/en/datasets-overview)这样的开源数据中心是标注数据集的良好来源，尤其是在那些相关人群广泛达成共识的领域（例如，红队测试的毒性数据集），并且使用开源数据集作为模型真实用户偏好的代理已足够。
- en: Still, many use cases are more specific and open-source proxy datasets are not
    sufficient. This is when datasets labeled by real humans, potentially with significant
    domain expertise, are required. Tools like [Amazon SageMaker Ground Truth](https://aws.amazon.com/sagemaker/groundtruth/)
    can help with collecting the data, be it by providing fully managed user interfaces
    and workflows or the entire workforce.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，许多使用场景更为具体，开源的代理数据集不足以满足需求。这时就需要由真正的人类标注的数据集，尤其是那些具有显著领域专业知识的人类标注。像[Amazon
    SageMaker Ground Truth](https://aws.amazon.com/sagemaker/groundtruth/)这样的工具可以帮助收集数据，无论是通过提供完全托管的用户界面和工作流，还是提供完整的劳动力。
- en: Recently, synthetic data collection has become more and more a topic in the
    space of fine-tuning. This is the practice of using powerful LLMs to synthetically
    create labeled datasets, be it for SFT or preference alignment. Even though this
    approach has already shown promising results, it is currently still subject to
    further research and has to prove itself to be useful at scale in practice.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，合成数据收集已成为微调领域越来越多讨论的话题。这是使用强大的大型语言模型（LLMs）合成创建标注数据集的实践，无论是用于监督微调（SFT）还是偏好对齐。尽管这种方法已经展示了有希望的结果，但目前仍处于进一步研究阶段，必须证明它在实际中能够在大规模上发挥作用。
- en: '**Data pre-processing:** The selected data needs to be pre-processed to make
    it “well digestible” for the downstream training algorithm. Popular pre-processing
    steps are the following:'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据预处理：** 选定的数据需要进行预处理，以使其对下游训练算法“易于消化”。常见的预处理步骤包括：'
- en: '**Quality-related pre-processing**, e.g. formatting, deduplication, PII filtering'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**质量相关的预处理，** 如格式化、去重、PII（个人身份信息）过滤。'
- en: '**Fine-tuning approach related pre-processing**: e.g. rendering into prompt
    templates for supervised fine-tuning'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与微调方法相关的预处理：** 如将数据呈现为用于监督微调的提示模板。'
- en: '**NLP-related pre-processing**, e.g. tokenisation, embedding, chunking (according
    to context window)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与NLP相关的预处理，** 如分词、嵌入、分块（根据上下文窗口）。'
- en: '**Model training:** training of the deep neural network according to selected
    fine-tuning approach. Popular fine-tuning approaches we will discuss in detail
    further below are:'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型训练：** 根据选定的微调方法训练深度神经网络。我们将在下面进一步详细讨论的几种流行微调方法包括：'
- en: '**Continued pre-training aka domain-adaptation fine-tuning:** training on full-text
    data, alignment tied to a next-token-prediction task'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持续预训练即领域适应微调：** 在完整文本数据上进行训练，对齐任务与下一个令牌预测任务相关。'
- en: '**Supervised fine-tuning:** fine-tuning approach leveraging labeled data, alignment
    tied towards the target label'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监督微调：** 利用标注数据的微调方法，对齐与目标标签相关。'
- en: '**Preference-alignment approaches:** fine-tuning approach leveraging preference
    data, aligning to a desired behaviour defined by the actual users of a model /
    system'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏好对齐方法：** 利用偏好数据的微调方法，依据模型/系统的实际用户定义的期望行为进行对齐。'
- en: Subsequently, we will dive deeper into the single phases, starting with an introduction
    to the training approach and different fine-tuning approaches before we move over
    to the dataset and data processing requirements.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们将深入探讨每个阶段，首先介绍训练方法和不同的微调方法，然后再讨论数据集和数据处理要求。
- en: Training
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练
- en: In this section we will explore the approach for training decoder transformer
    models. This applies for pre-training as well as fine-tuning.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将探讨解码器变换器模型的训练方法。该方法适用于预训练和微调。
- en: As opposed to traditional ML training approaches like unsupervised learning
    with unlabeled data or supervised learning with labeled data, training of transformer
    models utilizes a hybrid approach referred to as self-supervised learning. This
    is because although being fed with unlabeled textual data, the algorithm is actually
    intrinsically supervising itself by masking specific input tokens. Given the below
    input sequence of tokens “Berlin is the capital of Germany.”, this natively leads
    into a supervised sample with y being the masked token and X being the rest.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的机器学习训练方法（如使用未标注数据的无监督学习或使用标注数据的有监督学习）不同，变压器模型的训练采用了一种称为自监督学习的混合方法。这是因为，尽管输入的是未标注的文本数据，算法实际上通过掩盖特定的输入词元，自我监督地进行学习。以以下输入序列“Berlin
    is the capital of Germany.”为例，它自然地变成了一个监督样本，其中y是被掩盖的词元，而X是其余部分。
- en: '![](../Images/e2badc9e497d5745f328e429526b6008.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2badc9e497d5745f328e429526b6008.png)'
- en: 'Figure 5: Self-supervised training of language models'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：语言模型的自监督训练
- en: The above-mentioned self-supervised training approach is optimizing the model
    weights towards a language modeling (LM) specific loss function. While encoder
    model training is utilizing Masked Language Modeling (MLM) to leverage a bi-directional
    context by randomly masking tokens, decoder-only models are tied towards a Causal
    Language Modeling (CLM) approach with a uni-directional context by always masking
    the rightmost token of a sequence. In simple words, this means that they are trained
    towards predicting the subsequent token in an auto-regressive manner based on
    the previous ones as semantic context. Beyond this, other LM approaches like Permutation
    Language Modelling (PLM) exist, where a model is conditioned towards bringing
    a sequence of randomly shuffled tokens back into sorted order.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 上述自监督训练方法通过优化模型权重，朝着特定的语言模型（LM）损失函数进行调整。而在编码器模型的训练中，利用掩码语言建模（MLM）通过随机掩盖词元来利用双向上下文，而仅解码器模型则依赖因果语言建模（CLM）方法，通过始终掩盖序列中的最右侧词元来使用单向上下文。简而言之，这意味着它们是以自回归方式，基于前面的词元作为语义上下文来预测下一个词元。除此之外，还有其他语言模型方法，例如排列语言建模（PLM），其通过条件化模型将一系列随机打乱的词元重新排列成正确顺序。
- en: '![](../Images/b208927803b712df5c02e54aa2929d48.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b208927803b712df5c02e54aa2929d48.png)'
- en: 'Figure 6: Language modeling variations and loss functions'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：语言建模变体及损失函数
- en: By using the CLM task as a proxy, a prediction and ground truth are created
    which can be utilized to calculate the prediction loss. Therefore, the predicted
    probability distribution over all tokens of a model’s vocabulary is compared to
    the ground truth, a sparse vector with a probability of 1.0 for the token representing
    the ground truth. The actual loss function used depends on the specific model
    architecture, but loss functions like cross-entropy or perplexity loss, which
    perform well in categorical problem spaces like token prediction, are commonly
    used. The loss function is leveraged to gradually minimize the loss and hence
    optimize the model weights towards our training goal with every iteration through
    performing gradient descent in the deep neural network backpropagation.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用CLM任务作为代理，创建了预测值和真实标签，这些可以用来计算预测损失。因此，模型词汇表中所有词元的预测概率分布将与真实标签进行比较，真实标签是一个稀疏向量，其中表示真实标签的词元的概率为1.0。实际使用的损失函数取决于具体的模型架构，但像交叉熵损失或困惑度损失这样的损失函数，在像词元预测这样的分类问题中表现良好，因此被广泛使用。损失函数被用来逐步最小化损失，从而通过在深度神经网络反向传播中执行梯度下降来优化模型权重，朝着我们的训练目标进行每次迭代。
- en: Fine-tuning variations — scenario
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调变体 — 场景
- en: Enough of theory, let’s move into practice. Let’s assume you are an organization
    from the BioTech domain, aiming to leverage an LLM, let’s say LLaMA2, as a foundation
    model for various NLP use cases around COVID-19 vaccine research. Unfortunately,
    there are quite a few dimensions in which this domain is not part of the “comfort
    zone” of general-purpose off-the-shelf pre-trained LLMs, leading to performance
    being below your expected bar. In the next sections, we will discuss different
    fine-tuning approaches and how they can help elevate LLaMA2’s performance above
    the bar in various dimensions in our fictitious scenario.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 够了，理论部分讲解完毕，我们进入实践部分。假设你是来自生物技术领域的一个组织，旨在利用LLM，比如LLaMA2，作为基础模型，处理关于COVID-19疫苗研究的各种自然语言处理任务。不幸的是，在许多维度上，这个领域并不在通用现成预训练LLM的“舒适区”内，导致其性能低于你的预期标准。在接下来的章节中，我们将讨论不同的微调方法，以及它们如何帮助提升LLaMA2在我们假设的场景中在多个维度上的表现。
- en: Fine-tuning variations — continued pre-training aka. domain-adaptation fine-tuning
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调变体——持续预训练，也称为领域适应微调
- en: As the headline indicates, while the field starts to converge into the term
    “continued pre-training” a definite term for the fine-tuning approach discussed
    in this sections has yet to be agreed on by community. But what is this fine-tuning
    approach really about?
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 正如标题所示，尽管该领域开始趋同于“持续预训练”这一术语，但关于本节讨论的微调方法，社区尚未达成一致的明确术语。那么，这种微调方法究竟是关于什么的呢？
- en: Research papers in the BioTech domain are quite peculiar in writing style, full
    of domain-specific knowledge and industry- or even organisation-specific acronyms
    (e.g. [Polack et al, 2020](https://pubmed.ncbi.nlm.nih.gov/33301246/); see Figure
    7).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 生物技术领域的研究论文在写作风格上颇为独特，充满了领域特定的知识以及行业或甚至组织特有的缩写（例如，[Polack et al, 2020](https://pubmed.ncbi.nlm.nih.gov/33301246/);
    见图 7）。
- en: '![](../Images/d9f716433347a11a756ed13b8de5adf4.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d9f716433347a11a756ed13b8de5adf4.png)'
- en: 'Figure 7: Domain specifics of research papers illustrated using the example
    of Polack et al (2020)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：通过Polack等人（2020年）的例子说明研究论文中的领域特性
- en: On the other hand, a detailed look into the pre-training dataset mixtures of
    the Meta LLaMA models (Touvron et al., 2023; Figure 8) and the TII Falcon model
    family (Almazrouei et al., 2023; Figure 9) indicates that with 2.5% and 2%, general-purpose
    LLMs contain only a very little portion of data from the research or even BioTech
    domain (pre-training data mixture of LLaMA 3 family not public at the time of
    blog publication).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，详细分析Meta LLaMA模型（Touvron等，2023年；图 8）和TII Falcon模型家族（Almazrouei等，2023年；图
    9）的预训练数据集混合物表明，通用大型语言模型（LLM）中仅有2.5%和2%的数据来自研究领域甚至生物技术领域（LLaMA 3家族的预训练数据混合物在博客发布时尚未公开）。
- en: '![](../Images/cd3b31356155e1029a6f9c1be0b4bc94.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cd3b31356155e1029a6f9c1be0b4bc94.png)'
- en: 'Figure 8: Pre-training dataset mixture of Meta LLaMA models — Source: Touvron
    et al. (2023)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：Meta LLaMA模型的预训练数据集混合物——来源：Touvron等（2023年）
- en: '![](../Images/1fcba7426ab5fa6a8d103e40f4cba06a.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1fcba7426ab5fa6a8d103e40f4cba06a.png)'
- en: 'Figure 9: Pre-training dataset mixture of TII Falcon models — Source: Almazrouei
    et al. (2023)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：TII Falcon模型的预训练数据集混合物——来源：Almazrouei等（2023年）
- en: Hence, we need to bridge this gap by utilizing fine-tuning to expand the model’s
    “comfort zone” for better performance on the specific tasks to carry out. Continued
    pre-training excels at exactly the above-mentioned dimensions. It involves the
    process of adjusting a pre-trained LLM on a specific dataset consisting of plain
    textual data. This technique is helpful for infusing domain-specific information
    like linguistic patterns (domain-specific language, acronyms, etc.) or information
    implicitly contained in raw full-text into the model’s parametric knowledge to
    align the model’s responses to fit this specific language or knowledge domain.
    For this approach, pre-trained decoder models are fine-tuned on next-token prediction
    using unlabeled textual data. This makes continued pre-training the most similar
    fine-tuning approach to pre-training.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要通过利用微调来弥合这一差距，从而扩大模型的“舒适区”，以便在特定任务中取得更好的表现。持续预训练正是擅长上述提到的维度。它包括在特定数据集上调整预训练LLM的过程，该数据集由纯文本数据组成。这种技术有助于将领域特定的信息（如语言模式、领域特定语言、缩写等）或隐含在原始全文中的信息注入到模型的参数知识中，使模型的回答更符合这一特定语言或知识领域。对于这种方法，预训练的解码器模型通过无标注文本数据进行下一个词预测的微调。这使得持续预训练成为与预训练最相似的微调方法。
- en: In our example, we could use the content of the mentioned paper together with
    related literature from a similar field and convert it into a concatenated textual
    file. Depending on the tuning goal and other requirements, data curation steps
    like removal of unnecessary content (e.g., authors, tables of content, etc.),
    deduplication, or PII reduction can be applied. Finally, the dataset undergoes
    some NLP-specific preprocessing (e.g., tokenization, chunking according to the
    context window, etc. — see above), before it is used for training the model. The
    training itself is a classic CLM-based training as discussed in the previous section.
    After having adapted LLaMA2 with continued pre-training on a set of research publications
    from the BioTech domain, we can now utilize it in this specific domain as a text-completion
    model “BioLLaMA2.”
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们可以将提到的论文内容与相关领域的文献结合，并将其转换为一个连接的文本文件。根据调整目标和其他需求，可以应用数据整理步骤，如去除不必要的内容（例如，作者、目录等）、去重或减少个人身份信息（PII）。最后，数据集会进行一些特定于自然语言处理（NLP）的预处理（例如，分词、根据上下文窗口进行切分等——见上文），然后用于训练模型。训练本身是基于经典的CLM训练，如前一节所讨论的那样。在对LLaMA2进行了继续预训练，使用一组来自生物技术（BioTech）领域的研究出版物之后，我们现在可以将其用于这个特定领域，作为文本生成模型“BioLLaMA2”。
- en: Fine-tuning variations — supervised fine-tuning (SFT)
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调变体——监督微调（SFT）
- en: Unfortunately, we humans don’t like to frame the problems we want to get solved
    in a pure text-completion/token-prediction form. Instead, we are a conversational
    species with a tendency towards chatty or instructive behavior, especially when
    we are aiming to get things done.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们人类并不喜欢将我们希望解决的问题以纯粹的文本补全/标记预测的形式来框定。相反，我们是一个会话性物种，尤其是在我们旨在完成任务时，更倾向于表现出聊天或指令性行为。
- en: Hence, we require some sophistication beyond simple next-token prediction in
    the model’s behavior. This is where supervised fine-tuning approaches come into
    the game. Supervised fine-tuning (SFT) involves the process of aligning a pre-trained
    LLM on a specific dataset with labeled examples. This technique is essential for
    tailoring the model’s responses to fit particular domains or tasks, e.g., the
    above-mentioned conversational nature or instruction following. By training on
    a dataset that closely represents the target application, SFT allows the LLM to
    develop a deeper understanding and produce more accurate outputs in line with
    the specialized requirements and behaviour.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要模型行为中超越简单下一个标记预测的复杂性。这就是监督微调方法发挥作用的地方。监督微调（SFT）是指将一个预训练的语言模型（LLM）对准特定数据集，并使用带标签的示例进行训练的过程。这种技术对于定制模型的响应，以适应特定领域或任务至关重要，例如上述提到的对话性或遵循指令的行为。通过在一个密切代表目标应用的数据集上进行训练，SFT使得LLM能够发展更深的理解，并在符合专门要求和行为的情况下产生更准确的输出。
- en: Beyond the above-mentioned ones, good examples of SFT can be the training of
    the model for Q&A, a data extraction task such as entity recognition, or red-teaming
    to prevent harmful responses.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述提到的应用，SFT的良好示例还包括将模型训练用于问答、数据抽取任务（例如实体识别）或红队测试（以防止有害响应）。
- en: '![](../Images/9cfd021b9de4dbaf6b90552b69daa6ad.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9cfd021b9de4dbaf6b90552b69daa6ad.png)'
- en: 'Figure 10: E2E supervised fine-tuning pipeline'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：E2E监督微调流程
- en: As we understood above, SFT requires a labeled dataset. There are plenty of
    general-purpose labeled datasets in open-source, however, to tailor the model
    best to your specific use case, industry, or knowledge domain, it can make sense
    to manually craft a custom one. Recently, the approach of using powerful LLMs
    like Claude 3 or GPT-4 for crafting such datasets has evolved as a resource- and
    time-effective alternative to human labelling.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，SFT需要一个带标签的数据集。虽然开源中有许多通用的带标签数据集，但为了将模型最优化以适应你的特定用例、行业或知识领域，手动制作一个定制的数据集可能更有意义。最近，使用像Claude
    3或GPT-4这样的强大LLM来制作此类数据集，已经成为一种资源和时间高效的替代人工标注的方法。
- en: 'The “dolly-15k” dataset is a popular general-purpose open-source instruct fine-tuning
    dataset manually crafted by Databricks’ employees. It consists of roughly 15k
    examples of an instruction and a context labeled with a desired response. This
    dataset could be used to align our BioLLaMA2 model towards following instructions,
    e.g. for a closed Q&A task. For SFT towards instruction following, we would proceed
    and convert every single item of the dataset into a full-text prompt, embedded
    into a prompt structure representing the task we want to align the model towards.
    This could look as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: “dolly-15k”数据集是一个流行的通用开放源代码指令微调数据集，由Databricks的员工手动创建。它包含大约15k条指令和上下文示例，并附有期望的响应。该数据集可以用于将我们的BioLLaMA2模型对齐，以便遵循指令，例如用于封闭式问答任务。对于面向指令跟随的SFT，我们将继续将数据集中的每一项转化为完整的文本提示，并嵌入到一个表示我们希望对齐模型的任务的提示结构中。其结构可能如下所示：
- en: '[PRE0]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The prompt template can vary depending on the model family, as some models prefer
    HTML tags or other special characters over hashtags. This procedure is being applied
    for every item of the dataset before all of them are concatenated into a large
    piece of text. Finally, after the above-explained NLP-specific preprocessing,
    this file can be trained into the model by utilizing next-token prediction and
    a CLM-based training objective. Since it is consistently being exposed to this
    specific prompt structure, the model will learn to stick to it and act in a respective
    manner — in our case, instruction following. After aligning our BioLLaMA2 to the
    dolly-15k dataset, our BioLLaMA2-instruct model will thoroughly follow instructions
    submitted through the prompt.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 提示模板可以根据模型家族有所不同，因为某些模型更偏好HTML标签或其他特殊字符而非井号（#）。在所有数据项被合并成一大块文本之前，该程序将应用于数据集中的每个条目。最后，在上述NLP特定预处理之后，该文件可以通过利用下一个令牌预测和基于CLM的训练目标来训练模型。由于模型持续暴露于这一特定提示结构，它将学会坚持这一结构并以相应的方式行动——在我们的案例中，就是指令跟随。在将我们的BioLLaMA2与dolly-15k数据集对齐之后，我们的BioLLaMA2-instruct模型将严格按照通过提示提交的指令执行。
- en: Fine-tuning variations — human preference alignment techniques (RLHF/PPO, DPO,
    KTO, ORPO)
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调变体 — 人类偏好对齐技术（RLHF/PPO, DPO, KTO, ORPO）
- en: 'With BioLLaMA2 we have a model adapted to the BioTech research domain, following
    our instructions conveniently to what our users expect. But wait — is the model
    really aligned with our actual users? This highlights a core problem with the
    fine-tuning approaches discussed so far. The datasets we have used are proxies
    for what we think our users like or need: the content, language, acronyms from
    the selected research papers, as well as the desired instruct-behavior of a handful
    of Databricks employees crafting dolly-15k. This contrasts with the concept of
    user-centric product development, one of the core and well-established principles
    of agile product development. Iteratively looping in feedback from actual target
    users has proven to be highly successful when developing great products. In fact,
    this is definetly something we want to do if we are aiming to build a great experience
    for your users!'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 使用BioLLaMA2，我们拥有一款适应生物技术研究领域的模型，能够方便地根据我们的指令满足用户的预期。但等等——这个模型真的与我们的实际用户对齐吗？这突显了迄今为止讨论的微调方法的一个核心问题。我们所使用的数据集只是我们认为用户喜欢或需要的内容的代理：包括所选研究论文中的内容、语言、缩略语，以及少数Databricks员工在制作dolly-15k时设定的期望行为。这与以用户为中心的产品开发概念相对立，而后者是敏捷产品开发的核心原则之一。通过反复循环地融入实际目标用户的反馈，在开发优秀产品时被证明非常成功。事实上，如果我们旨在为用户构建一个优秀的体验，这正是我们想要做的！
- en: '![](../Images/a4eff0ffb3203b3f5ee164fa703c27c1.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4eff0ffb3203b3f5ee164fa703c27c1.png)'
- en: 'Figure 11: Reinforcement learning framework'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：强化学习框架
- en: With this in mind, researchers have put quite some effort into finding ways
    to incorporate human feedback into improving the performance of LLMs. On the path
    towards that, they realized a significant overlap with (deep) reinforcement learning
    (RL), which deals with autonomous agents performing actions in an action space
    within an environment, producing a next state, which is always coupled to a reward.
    The agents are acting based on a policy or a value-map, which has been gradually
    optimized towards maximizing the reward during the training phase.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，研究人员在将人类反馈纳入到大语言模型性能提升中的方法上投入了大量精力。在这条道路上，他们意识到与（深度）强化学习（RL）有显著的重叠，强化学习涉及的是在环境中执行动作的自主代理，这些动作会产生下一个状态，并总是与奖励相关联。这些代理基于策略或价值图进行行动，而这一策略在训练阶段逐步优化，以最大化奖励。
- en: '![](../Images/a7ccf68164b7ab714f3cc6655c7c96b6.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a7ccf68164b7ab714f3cc6655c7c96b6.png)'
- en: 'Figure 12: Adapted reinforcement learning framework for language modeling'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：适应性强化学习框架用于语言建模
- en: This concept — projected into the world of LLMs — comes down to the LLM itself
    acting as the agent. During inference, with every step of its auto-regressive
    token-prediction nature, it performs an action, where the action space is the
    model’s vocabulary, and the environment is all possible token combinations. With
    every new inference cycle, a new state is established, which is honored with a
    reward that is ideally correlated with some human feedback.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这一概念——在大语言模型（LLMs）的世界中——归结为大语言模型本身充当代理。在推理过程中，凭借其自回归的 token 预测特性，每一步都会执行一个动作，其中动作空间是模型的词汇表，环境则是所有可能的
    token 组合。每次新的推理周期开始时，都会建立一个新的状态，并根据人类反馈给予奖励，理想情况下这个奖励与人类反馈相关。
- en: 'Based on this idea, several human preference alignment approaches have been
    proposed and tested. In what follows, we will walk through some of the most important
    ones:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这一思想，已经提出并测试了几种人类偏好对齐方法。接下来，我们将逐一介绍其中一些最重要的方法：
- en: Reinforcement Learning from Human Feedback (RLHF) with Proximal Policy Optimization
    (PPO)
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 来自人类反馈的强化学习（RLHF）与近端策略优化（PPO）
- en: '![](../Images/b0a8bad7a89e5fd657ae7a358524d8b0.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b0a8bad7a89e5fd657ae7a358524d8b0.png)'
- en: 'Figure 13: Reward model training for RLHF'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：RLHF 的奖励模型训练
- en: Reinforcement learning from human feedback was one of the major hidden technical
    backbones of the early Generative AI hype, giving the breakthrough achieved with
    great large decoder models like Anthropic Claude or GPT-3.5 an additional boost
    into the direction of user alignment.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 来自人类反馈的强化学习是早期生成式 AI 热潮的主要技术支撑之一，推动了像 Anthropic Claude 或 GPT-3.5 等大型解码器模型的突破，并进一步促进了用户对齐的方向发展。
- en: 'RLHF works in a two-step process and is illustrated in Figures 13 and 14:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF 以两步过程工作，具体见图 13 和图 14：
- en: 'Step 1 (Figure 13): First, a reward model needs to be trained for later usage
    in the actual RL-powered training approach. Therefore, a prompt dataset aligned
    with the objective (in the case of our BioLLaMA2-instruct model, this would be
    pairs of an instruction and a context) to optimize is being fed to the model to
    be fine-tuned, while requesting not only one but two or more inference results.
    These results will be presented to human labelers for scoring (1st, 2nd, 3rd,
    …) based on the optimization objective. There are also a few open-sourced preference
    ranking datasets, among them [“Anthropic/hh-rlhf”](https://huggingface.co/datasets/Anthropic/hh-rlhf)which
    is tailored towards red-teaming and the objectives of honesty and harmlessness.
    After a normalization step as well as a translation into reward values, a reward
    model is being trained based on the single sample-reward pairs, where the sample
    is a single model response. The reward model architecture is usually similar to
    the model to be fine-tuned, adapted with a small head eventually projecting the
    latent space into a reward value instead of a probability distribution over tokens.
    However, the ideal sizing of this model in parameters is still subject to research,
    and different approaches have been chosen by model providers in the past.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步（图13）：首先，需要训练一个奖励模型，以便在实际的基于强化学习（RL）的训练方法中使用。因此，需要提供一个与目标对齐的提示数据集（以我们的BioLLaMA2-instruct模型为例，这将是由指令和上下文组成的配对）供模型优化，并要求不仅生成一个推理结果，而是两个或更多的推理结果。这些结果将提交给人工标注员进行评分（第一、第二、第三等），根据优化目标进行排序。此外，还有一些开源的偏好排序数据集，其中[“Anthropic/hh-rlhf”](https://huggingface.co/datasets/Anthropic/hh-rlhf)是专门为红队测试以及诚实和无害性目标量身定制的。在经过标准化步骤并将其转化为奖励值之后，基于单一的样本-奖励对训练奖励模型，其中样本是单个模型响应。奖励模型的架构通常与待微调的模型类似，通过添加一个小的头部，将潜在空间投影到奖励值上，而不是令牌的概率分布。然而，这个模型的理想参数大小仍然是一个研究课题，过去模型提供者采取了不同的方法。
- en: '![](../Images/dc984cb826448732166f22fbc86030b1.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc984cb826448732166f22fbc86030b1.png)'
- en: 'Figure 14: Reinforcement learning based model tuning with PPO for RLHF'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：基于PPO的强化学习模型调优用于RLHF
- en: 'Step 2 (Figure 14): Our new reward model is now used for training the actual
    model. Therefore, another set of prompts is fed through the model to be tuned
    (grey box in illustration), resulting in one response each. Subsequently, these
    responses are fed into the reward model for retrieval of the individual reward.
    Then, Proximal Policy Optimization (PPO), a policy-based RL algorithm, is used
    to gradually adjust the model’s weights in order to maximize the reward allocated
    to the model’s answers. As opposed to CLM, instead of gradient descent, this approach
    leverages gradient ascent (or gradient descent over *1 — reward*) since we are
    now trying to maximize an objective (reward). For increased algorithmic stability
    to prevent too heavy drifts in model behavior during training, which can be caused
    by RL-based approaches like PPO, a prediction shift penalty is being added to
    the reward term, penalizing answers diverging too much from the initial language
    model’s predicted probability distribution on the same input prompt.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步（图14）：我们的新奖励模型现在用于训练实际模型。因此，另一组提示被输入待调整的模型（图中的灰色框），每次生成一个响应。随后，这些响应被输入奖励模型以获取各自的奖励值。接着，使用基于策略的强化学习算法——近端策略优化（PPO）来逐步调整模型的权重，以最大化分配给模型回答的奖励。与CLM不同，这种方法不是使用梯度下降，而是利用梯度上升（或者对*1
    - 奖励*使用梯度下降），因为我们现在试图最大化一个目标（奖励）。为了增强算法的稳定性，防止在训练过程中由于像PPO这样的基于强化学习的方法引起模型行为的剧烈漂移，我们在奖励项中加入了预测偏移惩罚，惩罚那些在相同输入提示上偏离初始语言模型预测概率分布过多的回答。
- en: Beyond RLHF with PPO, which currently is the most widely adopted and proven
    approach to preference alignment several other approaches have been developed.
    In the next couple of sections we will dive deep into some of these approaches
    on an advanced level. This is for advanced readers only, so depending on your
    level of experience with deep learning and reinforcement learning you might want
    to skip directly to the next section “**Decision flow chart — which model to choose,
    which fine-tuning path to pick**”.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用PPO进行的RLHF（目前是最广泛采用且已证明有效的偏好对齐方法），还开发了其他几种方法。在接下来的几个部分中，我们将深入探讨这些方法中的一些高级内容。这部分仅供高级读者阅读，因此根据你在深度学习和强化学习方面的经验水平，你可能想跳过直接阅读下一部分“**决策流程图——选择哪个模型，选择哪条微调路径**”。
- en: Direct Policy Optimization (DPO)
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 直接策略优化（DPO）
- en: 'Direct Policy Optimization (DPO) is a preference alignment approach deducted
    from RLHF, tackling two major downsides of it:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 直接策略优化（DPO）是一种从RLHF中推导出来的偏好对齐方法，解决了RLHF的两个主要缺点：
- en: Training a reward model first is additional resource investment and can be significant
    depending on the reward model size
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先训练奖励模型需要额外的资源投入，并且根据奖励模型的大小，可能会是一个较大的开销。
- en: The training phase of RLHF with PPO requires massive compute clusters since
    three replicas of the model (initial LM, tuned LM, reward model) need to be hosted
    and orchestrated simultaneously in a low latency setup
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PPO的RLHF训练阶段需要大量的计算集群，因为三个模型副本（初始语言模型、调整后的语言模型、奖励模型）需要在低延迟的设置中同时托管和协调。
- en: RLHF can be an unstable procedure (→ prediction shift penalty tries to mitigate
    this)
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RLHF可能是一个不稳定的过程（→预测偏移惩罚尝试缓解这一问题）
- en: '![](../Images/9840b077f94d719ae8ae706fb73e3d3d.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9840b077f94d719ae8ae706fb73e3d3d.png)'
- en: 'Figure 15: RLHF vs. DPO (Rafailov et al., 2023)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：RLHF与DPO对比（Rafailov等，2023年）
- en: DPO is an alternative preference alignment approach and was proposed by Rafailov
    et al. in 2023\. The core idea of DPO is to skip the reward model training and
    tune the final preference-aligned LLM directly on the preference data. This is
    being achieved by applying some mathematical tweaks to transform the parameterization
    of the reward model (reward term) into a loss function (figure 16) while replacing
    the actual reward values with probability values over the preference data.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: DPO是一种替代的偏好对齐方法，由Rafailov等人于2023年提出。DPO的核心思想是跳过奖励模型的训练，直接在偏好数据上调整最终的偏好对齐语言模型。通过应用一些数学调整，将奖励模型的参数化（奖励项）转化为损失函数（图16），同时用偏好数据上的概率值替代实际的奖励值，从而实现这一目标。
- en: '![](../Images/f4ac00726f1ca1456a19a66955bbab90.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4ac00726f1ca1456a19a66955bbab90.png)'
- en: 'Figure 16: Loss function for DPO (Rafailov et al., 2023)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：DPO的损失函数（Rafailov等，2023年）
- en: This saves computational as well as algorithmic complexity on the way towards
    a preference-aligned model. While the paper is also showing performance increases
    as compared to RLHF, this approach is fairly recent and hence the results are
    subject to practical proof.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这节省了朝着偏好对齐模型前进过程中计算和算法的复杂性。尽管论文也显示了与RLHF相比的性能提升，但这种方法相对较新，因此其结果仍需经过实际验证。
- en: Kahneman-Tversky Optimization (KTO)
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卡尼曼-特沃斯基优化（KTO）
- en: Existing methods for aligning language models with human feedback, such as RLHF
    and DPO, require preference data — pairs of outputs where one is preferred over
    the other for a given input. However, collecting high-quality preference data
    at scale is challenging and expensive in the real world. Preference data often
    suffers from noise, inconsistencies, and intransitivities, as different human
    raters may have conflicting views on which output is better. KTO was [proposed](https://arxiv.org/pdf/2402.01306)
    by Ethayarajh et al. (2024) as an alternative approach that can work with a simpler,
    more abundant signal — just whether a given output is desirable or undesirable
    for an input, without needing to know the relative preference between outputs.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的将语言模型与人类反馈对齐的方法，如RLHF和DPO，需要偏好数据——即一对对输出，其中一个在给定输入下被认为优于另一个。然而，在现实世界中，大规模收集高质量的偏好数据是具有挑战性且昂贵的。偏好数据常常受到噪声、不一致性和不传递性问题的困扰，因为不同的人类评估者可能对哪个输出更好存在冲突的看法。KTO由Ethayarajh等人（2024年）[提出](https://arxiv.org/pdf/2402.01306)，作为一种可以使用更简单、更丰富的信号的替代方法——即仅仅知道给定输出对于输入是否是可取的，而无需知道输出之间的相对偏好。
- en: '![](../Images/17c251b83d01948689c4d3b94848ad8c.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17c251b83d01948689c4d3b94848ad8c.png)'
- en: 'Figure 17: Implied human utility of desicions according to Kahneman and Tversky’s
    prospect theory (Ethayarajh et al., 2024)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：根据卡尼曼和特沃斯基的前景理论推导的决策的隐含人类效用（Ethayarajh等，2024年）
- en: At a high level, KTO works by defining a reward function that captures the relative
    “goodness” of a generation, and then optimizing the model to maximize the expected
    value of this reward under a Kahneman-Tversky value function. Kahneman and Tversky’s
    prospect theory explains how humans make decisions about uncertain outcomes in
    a biased but well-defined manner. The theory posits that human utility depends
    on a value function that is concave in gains and convex in losses, with a reference
    point that separates gains from losses (see figure 17). KTO directly optimizes
    this notion of human utility, rather than just maximizing the likelihood of preferences.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，KTO的工作原理是定义一个奖励函数，用于捕捉生成结果的相对“优越性”，然后优化模型，以最大化在卡尼曼-特沃斯基价值函数下该奖励的期望值。卡尼曼和特沃斯基的前景理论解释了人类如何以一种有偏但明确的方式对不确定的结果做出决策。该理论认为，人类效用依赖于一个在收益上是凹的、在损失上是凸的价值函数，并且有一个分离收益与损失的参考点（见图17）。KTO直接优化这一人类效用的概念，而不仅仅是最大化偏好的可能性。
- en: '![](../Images/3f9262b1cd548e14214003bd150d1f41.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f9262b1cd548e14214003bd150d1f41.png)'
- en: 'Figure 18: RLHF vs. DPO vs. KTO (Ethayarajh et al., 2024)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：RLHF与DPO与KTO（Ethayarajh等，2024）
- en: The key innovation is that KTO only requires a binary signal of whether an output
    is desirable or undesirable, rather than full preference pairs. This allows KTO
    to be more data-efficient than preference-based methods, as the binary feedback
    signal is much more abundant and cheaper to collect. (see figure 18)
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 关键创新在于KTO只需要一个二元信号来表示输出是期望的还是不期望的，而不是完整的偏好对。这使得KTO在数据效率上优于基于偏好的方法，因为二元反馈信号更加丰富且更容易收集（见图18）。
- en: KTO is particularly useful in scenarios where preference data is scarce or expensive
    to collect, but you have access to a larger volume of binary feedback on the quality
    of model outputs. According to the paper, it can match or even exceed the performance
    of preference-based methods like DPO, especially at larger model scales. However,
    this needs to be validated at scale in practice. KTO may be preferable when the
    goal is to directly optimize for human utility rather than just preference likelihood.
    However, if the preference data is very high-quality with little noise or intransitivity,
    then preference-based methods could still be the better choice. KTO also has theoretical
    advantages in handling extreme data imbalances and avoiding the need for supervised
    fine-tuning in some cases.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: KTO特别适用于偏好数据稀缺或收集成本高昂的场景，但你可以获得更多关于模型输出质量的二元反馈信号。根据论文，KTO的表现可以与基于偏好的方法（如DPO）匹敌，甚至在较大规模的模型下超过它们。然而，这还需要在实践中进行大规模验证。当目标是直接优化人类效用而不仅仅是偏好可能性时，KTO可能是更好的选择。然而，如果偏好数据非常高质量，噪声或不传递性很少，那么基于偏好的方法可能仍然是更好的选择。KTO在处理极端数据不平衡和避免某些情况下需要监督微调方面也具有理论优势。
- en: Odds Ration Preference Optimization (ORPO)
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比值比偏好优化（ORPO）
- en: The key motivation behind ORPO is to address the limitations of existing preference
    alignment methods, such as RLHF and DPO, which often require a separate supervised
    fine-tuning (SFT) stage, a reference model, or a reward model. The paper by Hong
    et al. (2024) argues that SFT alone can inadvertently increase the likelihood
    of generating tokens in undesirable styles, as the cross-entropy loss does not
    provide a direct penalty for the disfavored responses. At the same time, they
    claim that SFT is vital for converging into powerful preference alignment models.
    This leads to a two-stage alignment process heavily incurring resources. By combining
    these stages into one, ORPO aims to preserve the domain adaptation benefits of
    SFT while concurrently discerning and mitigating unwanted generation styles as
    aimed towards by preference-alignment approaches. (see figure 19)
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ORPO的关键动机是解决现有偏好对齐方法（如RLHF和DPO）的局限性，这些方法通常需要单独的监督微调（SFT）阶段、参考模型或奖励模型。Hong等（2024）的论文认为，单独使用SFT可能会无意中增加生成不期望风格的令牌的可能性，因为交叉熵损失并没有为不受欢迎的响应提供直接的惩罚。同时，他们认为SFT对于收敛到强大的偏好对齐模型至关重要。这导致了一个资源密集型的两阶段对齐过程。通过将这些阶段合并为一个，ORPO旨在保留SFT的领域适应优势，同时识别并缓解偏好对齐方法所期望的、不希望的生成风格（见图19）。
- en: '![](../Images/1e7f778a63d29db342a3b57f72642818.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e7f778a63d29db342a3b57f72642818.png)'
- en: 'Figure 19: RLHF vs. DPO vs. ORPO (Hong et al., 2024)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图19：RLHF与DPO与ORPO（Hong等，2024）
- en: 'ORPO introduces a novel preference alignment algorithm that incorporates an
    odds ratio-based penalty to the conventional causal language modeling tied loss
    (e.g., cross-entropy loss). The objective function of ORPO consists of two components:
    the SFT loss and the relative ratio loss (LOR). The LOR term maximizes the odds
    ratio between the likelihood of generating the favored response and the disfavored
    response, effectively penalizing the model for assigning high probabilities to
    the rejected responses.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ORPO 引入了一种新颖的偏好对齐算法，通过引入基于赔率比的惩罚到传统的因果语言建模绑定损失（例如交叉熵损失）中。ORPO 的目标函数由两个部分组成：SFT
    损失和相对比率损失（LOR）。LOR 项通过最大化生成偏好响应和不偏好响应的可能性之间的赔率比，有效地惩罚模型对被拒绝的响应分配高概率。
- en: '![](../Images/29a50143618fbc9873643aaa6c24d053.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29a50143618fbc9873643aaa6c24d053.png)'
- en: 'Figure 20: ORPO loss function incorporating both SFT loss and preference odds
    ratio into a single loss term'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20：ORPO 损失函数将 SFT 损失和偏好赔率比结合到一个单一的损失项中
- en: ORPO is particularly useful when you want to fine-tune a pre-trained language
    model to adapt to a specific domain or task while ensuring that the model’s outputs
    align with human preferences. It can be applied in scenarios where you have access
    to a pairwise preference dataset (yw = favored, yl = disfavored, such as the [UltraFeedback](https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned)
    or [HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf) datasets. With
    this in mind, ORPO is designed to be a more efficient and effective alternative
    to RLHF and DPO, as it does not require a separate reference model, reward model
    or a two-step fine-tuning approach.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ORPO 在你想要微调一个预训练语言模型以适应特定领域或任务时尤其有用，同时确保模型的输出与人类偏好一致。它可以应用于你拥有成对偏好数据集的场景（yw
    = 喜好，yl = 不喜好，例如 [UltraFeedback](https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned)
    或 [HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf) 数据集）。考虑到这一点，ORPO
    被设计成比 RLHF 和 DPO 更高效、有效的替代方案，因为它不需要单独的参考模型、奖励模型或两步微调方法。
- en: Decision flow chart — which model to choose, which fine-tuning path to pick
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策流程图——选择哪种模型，选择哪条微调路径
- en: 'After diving deep into plenty of fine-tuning approaches, the obvious question
    arises as to which model to start with and which approach to pick best based on
    specific requirements. The approach for picking the right model for fine-tuning
    purposes is a two-step approach. The first step is very similar to picking a base
    model without any fine-tuning intentions, including considerations alongside the
    following dimensions (not exhaustive):'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究了大量微调方法之后，显然的问题是应该选择哪个模型开始，以及根据特定要求选择哪种方法。选择合适的微调模型的方法是一个两步过程。第一步与选择一个没有微调意图的基础模型非常相似，包括在以下维度（不完全列举）下的考虑因素：
- en: '**Platform to be used:** Every platform comes with a set of models accessible
    through it. This needs to be taken into consideration. Please note that region-specific
    differences in model availability can apply. Please check the respective platform’s
    documentation for more information on this.'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**使用的平台**：每个平台都提供一组可以访问的模型。需要考虑到这一点。请注意，模型的可用性可能存在区域性的差异。请查阅相关平台的文档以获取更多信息。'
- en: '**Performance:** Organizations should aim to use the leanest model for a specific
    task. While no generic guidance on this can be given and fine-tuning can significantly
    uplift a model’s performance (smaller fine-tuned models can outperform larger
    general-purpose models), leveraging evaluation results of base models can be helpful
    as an indicator.'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**性能**：组织应致力于为特定任务使用最简洁的模型。虽然无法给出通用的指导，并且微调可以显著提升模型性能（较小的微调模型可以超越较大的通用模型），但利用基础模型的评估结果作为指标会有所帮助。'
- en: '**Budget (TCO):** In general, larger models require more compute and potentially
    multi-GPU instances for training and serving across multiple accelerators. This
    has a direct impact on factors like training and inference cost, complexity of
    training and inference, resources and skills required, etc., as part of TCO along
    a model’s entire lifecycle. This needs to be aligned with the short- and long-term
    budget allocated.'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预算（TCO）**：通常，较大的模型需要更多的计算资源，并可能需要多 GPU 实例来进行训练和服务，跨多个加速器进行处理。这直接影响到训练和推理成本、训练和推理的复杂性、所需资源和技能等因素，作为模型整个生命周期中总拥有成本（TCO）的一部分。需要与分配的短期和长期预算保持一致。'
- en: '**Licensing model:** Models, wether proprietary or open-source ones come with
    licensing constraints depending on the domain of usage and commercial model to
    be used. This needs to be taken into account.'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**许可模型：** 模型，无论是专有的还是开源的，都有许可限制，具体取决于使用领域和商业模式。这一点需要考虑。'
- en: '**Governance, Ethics, Responsible AI:** Every organisation has compliance guidelines
    alongside these dimensions. This needs to be considered in the model’s choice.'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**治理、伦理、负责任的AI：** 每个组织都有与这些维度相关的合规指南。这需要在选择模型时予以考虑。'
- en: '*Example: An organisation might decide to consider LLaMA 2 models and rule
    out the usage of proprietary models like Anthropic Claude or AI21Labs Jurassic
    based on evaluation results of the base models. Further, they decide to only use
    the 7B-parameter version of this model to be able to train and serve them on single
    GPU instances.*'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例：一个组织可能决定考虑LLaMA 2模型，并基于基础模型的评估结果排除使用像Anthropic Claude或AI21Labs Jurassic这样的专有模型。此外，他们决定只使用7B参数版本的模型，以便能够在单GPU实例上进行训练和服务。*'
- en: The second step is concerned with narrowing down the initial selection of models
    to 1-few models to be taken into consideration for the experimenting phase. The
    final decision on which specific approach to choose is dependent on the desired
    entry point into the fine-tuning lifecycle of language models illustrated in the
    below figure.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是将初步选择的模型缩小到1到几个模型，以供实验阶段使用。最终选择哪种具体方法取决于期望进入语言模型微调生命周期的起始点，具体见下图。
- en: '![](../Images/c1306adc65c5bbb291a06e167f44c76c.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c1306adc65c5bbb291a06e167f44c76c.png)'
- en: 'Figure 21: Decision flow chart for domain adaptation through fine-tuning'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21：通过微调进行领域适应的决策流程图
- en: 'Thereby, the following dimensions need to be taken into consideration:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，以下维度需要考虑：
- en: '**Task to be performed:** Different use cases require specific model behaviour.
    While for some use cases a simple text-completion model (next-token-prediction)
    might be sufficient, most use cases require task-specific behaviour like chattiness,
    instruction-following or other task-specific behaviour. To meet this requirement,
    we can take a working backwards approach from the desired task to be performed.
    This means we need to define our specific fine-tuning journey to end at a model
    aligned to this specific task. With regards to the illustration this implies that
    the model must — aligned with the desired model behaviour — end in the blue, orange
    or green circle while the fine-tuning journey is defined alongside the possible
    paths of the flow diagram.'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**要执行的任务：** 不同的使用案例要求特定的模型行为。对于某些使用案例，简单的文本补全模型（下一个词预测）可能就足够了，但大多数使用案例需要特定的任务行为，如健谈性、遵循指令或其他任务特定的行为。为了满足这一需求，我们可以从期望的任务开始，采用逆向工作的方法。这意味着我们需要定义特定的微调路径，最终使模型与特定任务对齐。就示意图而言，这意味着模型必须——与期望的模型行为对齐——最终进入蓝色、橙色或绿色圆圈，同时微调路径沿着流程图的可能路径定义。'
- en: '**Choose the right starting point (as long as reasonable):** While we should
    be very clear on where our fine-tuning journey should end, we can start anywhere
    in the flow diagram by picking a respective base model. This however needs to
    be reasonable — in times of model hubs with millions of published models, it can
    make sense to check if the fine-tuning step has not already been performed by
    someone else who shared the resulting model, especially when considering popular
    models in combination with open-source datasets.'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择合适的起点（只要合理）：** 虽然我们应该非常明确微调过程的终点，但我们可以通过选择相应的基础模型，在流程图中的任何位置开始。不过，这一选择必须是合理的——在模型库中拥有数百万个已发布模型的时代，检查是否有人已经执行过微调步骤并分享了结果模型是有意义的，特别是在考虑流行模型与开源数据集的结合时。'
- en: '**Fine-tuning is an iterative, potentially recursive process:** It is possible
    to perform multiple subsequent fine-tuning jobs on the way to our desired model.
    However, please note that catastrophic forgetting is something we need to keep
    in mind as models can’t encode an infinite amount of information in their weights.
    To mitigate this, you can leverage parameter-efficient fine-tuning approaches
    like LoRA as shown in this [paper](https://arxiv.org/abs/2405.09673) and [blog](https://medium.com/towards-data-science/leveraging-qlora-for-fine-tuning-of-task-fine-tuned-models-without-catastrophic-forgetting-d9bcd594cff4).'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**微调是一个迭代的、潜在的递归过程：** 在实现我们期望的模型的过程中，可以进行多次后续的微调工作。然而，请注意，灾难性遗忘是我们需要关注的问题，因为模型无法在其权重中编码无限量的信息。为缓解这一问题，可以利用如LoRA等参数高效的微调方法，正如这篇[论文](https://arxiv.org/abs/2405.09673)和[博客](https://medium.com/towards-data-science/leveraging-qlora-for-fine-tuning-of-task-fine-tuned-models-without-catastrophic-forgetting-d9bcd594cff4)所示。'
- en: '**Task-specific performance uplift targeted:** Fine-tuning is performed to
    uplift a model’s performance in a specific task. If we are looking for performance
    uplift in linguistic patterns (domain-specific language, acronyms, etc.) or information
    implicitly contained in your training data, continued pre-training is the right
    choice. If we want to uplift performance towards a specific task, supervised fine-tuning
    should be chosen. If we want to align your model behaviour towards our actual
    users, human preference alignment is the right choice.'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**特定任务性能提升目标：** 微调是为了提升模型在特定任务中的表现。如果我们希望在语言模式（领域特定语言、缩略语等）或训练数据中隐含的信息上提升表现，持续预训练是正确的选择。如果我们希望在特定任务上提升性能，则应选择监督微调。如果我们希望将模型行为与实际用户对齐，则应选择人类偏好对齐。'
- en: '**Data availability:** Training data will also influence which path we choose.
    In general, organisations hold larger amounts of unlabelled textual data is as
    opposed to labelled data, and acquiring labelled data can be an expensive task.
    This dimension needs to be taken into consideration when navigating through the
    flow chart.'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据可用性：** 训练数据也会影响我们选择的路径。通常，组织持有大量未标注的文本数据，而非标注数据，而获取标注数据可能是一个昂贵的任务。在穿越流程图时，这一维度需要被考虑。'
- en: With this working backwards approach alongside the above flow chart we can identify
    the model to start with and the path to take while traversing the fine-tuning
    flow diagram.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种从后向前的方法，并结合上述流程图，我们可以确定开始的模型以及在穿越微调流程图时需要采取的路径。
- en: 'To make this a bit more obvious we are providing two examples:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这一点更加明显，我们提供了两个示例：
- en: '![](../Images/ad42bd3807216e2d9142c08e6fff9a86.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad42bd3807216e2d9142c08e6fff9a86.png)'
- en: 'Figure 22: Decision flow chart for example 1'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 22：示例 1 的决策流程图
- en: '*Example 1: Following the example illustrated in the fine-tuning section above,
    we could constitute the desire of having an instruct model for our specific use
    case, aligned to our actual user’s preferences. However, we want to uplift performance
    in the BioTech domain. Unlabelled data in the form of research papers are available.
    We choose the LLaMA-2–7b model family as the desired starting point. Since Meta
    has not published an LLaMA-2–7b instruct model, we start from the text completion
    model LLaMA-2–7b-base. Then we perform continued pre-training on the corpus of
    research papers, followed by supervised fine-tuning on an open-source instruct
    dataset like the dolly-15k dataset. This results in an instruct-fine-tuned BioTech
    version of LLaMA-2–7B-base, which we call BioLLaMA-2–7b-instruct. In the next
    step, we want to align the model to our actual users’ preferences. We collect
    a preference dataset, train a reward model, and use RLHF with PPO to preference-align
    our model.*'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例 1：根据上述微调部分中举的例子，我们可以构建一个用于我们特定用例的指令模型，符合我们实际用户的偏好。然而，我们希望在生物技术领域提升性能。未标注的数据以研究论文的形式可用。我们选择LLaMA-2–7b模型家族作为期望的起点。由于Meta没有发布LLaMA-2–7b指令模型，我们从文本补全模型LLaMA-2–7b-base开始。然后，我们对研究论文语料库进行持续的预训练，接着在像dolly-15k这样的开源指令数据集上进行监督微调。这将得到一个经过指令微调的生物技术版LLaMA-2–7B-base，我们称之为BioLLaMA-2–7b-instruct。在下一步中，我们希望将模型与实际用户的偏好对齐。我们收集偏好数据集，训练奖励模型，并使用带有PPO的RLHF对我们的模型进行偏好对齐。*'
- en: '![](../Images/edea3768ebb2322e1beaa20475944b90.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/edea3768ebb2322e1beaa20475944b90.png)'
- en: 'Figure 23: Decision flow chart for example 2'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 23：示例 2 的决策流程图
- en: '*Example 2: In this example we are aiming to use a chat model, however aligned
    to our actual user’s preferences. We choose the LLaMA-2–7b model family as the
    desired starting point. We figure out that Meta is providing an off-the-shelf
    chat-fine-tuned model LLaMA-2–7b-chat, which we can use as a starting point. In
    the next step, we want to align the model to our actual user’s preferences. We
    collect a preference dataset from our users, train a reward model and use RLHF
    with PPO to preference-align our model.*'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例 2：在这个例子中，我们旨在使用一个聊天模型，但需要使其与实际用户的偏好对齐。我们选择LLaMA-2–7b模型家族作为理想的起点。我们发现Meta提供了一个现成的聊天微调模型LLaMA-2–7b-chat，我们可以作为起点使用。在下一步中，我们希望将模型与我们实际用户的偏好对齐。我们收集用户的偏好数据集，训练一个奖励模型，并使用RLHF和PPO进行偏好对齐。*'
- en: Conclusion
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Generative AI has many exciting use cases for businesses and organizations.
    However, these applications are usually much more complex than individual consumer
    uses like generating recipes or speeches. For companies, the AI needs to understand
    the organization’s specific domain knowledge, processes, and data. It must integrate
    with existing enterprise systems and applications. And it needs to provide a highly
    customized experience for different employees and roles while acting in a harmless
    way. To successfully implement generative AI in an enterprise setting, the technology
    must be carefully designed and tailored to the unique needs of the organization.
    Simply using a generic, publicly-trained model won’t be sufficient.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 生成性人工智能在企业和组织中有许多令人兴奋的应用场景。然而，这些应用通常比个人消费者用途（如生成食谱或演讲）要复杂得多。对于公司来说，AI需要理解组织的特定领域知识、流程和数据。它必须与现有的企业系统和应用程序集成，并为不同的员工和角色提供高度定制化的体验，同时以无害的方式进行操作。为了在企业环境中成功实施生成性AI，技术必须经过精心设计，并根据组织的独特需求量身定制。仅仅使用一个通用的、公开训练的模型是不够的。
- en: In this blog post we discussed how domain adaptation can help bridging this
    gap by overcoming situations where a model is confronted with tasks outside of
    its “comfort zone”. With in-context learning and fine-tuning we dived deep into
    two powerful approaches for domain adaptation. Finally, we discussed tradeoffs
    to take when deciding between these approaches.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我们讨论了域适应如何通过克服模型面临超出其“舒适区”任务的情况来帮助弥合这一差距。通过上下文学习和微调，我们深入探讨了两种强大的域适应方法。最后，我们讨论了在决定这两种方法时需要权衡的因素。
- en: Successfully bridging this gap between powerful AI capabilities and real-world
    business requirements is key for unlocking the full potential of generative AI
    for companies.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 成功弥合强大AI能力与现实世界商业需求之间的差距是释放生成性AI在公司中全部潜力的关键。
