- en: 'Continual Learning: The Three Common Scenarios'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持续学习：三种常见场景
- en: 原文：[https://towardsdatascience.com/continual-learning-the-three-common-scenarios-e6c3260fe0cb?source=collection_archive---------11-----------------------#2024-10-29](https://towardsdatascience.com/continual-learning-the-three-common-scenarios-e6c3260fe0cb?source=collection_archive---------11-----------------------#2024-10-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/continual-learning-the-three-common-scenarios-e6c3260fe0cb?source=collection_archive---------11-----------------------#2024-10-29](https://towardsdatascience.com/continual-learning-the-three-common-scenarios-e6c3260fe0cb?source=collection_archive---------11-----------------------#2024-10-29)
- en: Plus paper recommendations
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另外，推荐论文
- en: '[](https://pascaljanetzky.medium.com/?source=post_page---byline--e6c3260fe0cb--------------------------------)[![Pascal
    Janetzky](../Images/43d68509b63c5f9b3fc9cef3cbfc1a88.png)](https://pascaljanetzky.medium.com/?source=post_page---byline--e6c3260fe0cb--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e6c3260fe0cb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e6c3260fe0cb--------------------------------)
    [Pascal Janetzky](https://pascaljanetzky.medium.com/?source=post_page---byline--e6c3260fe0cb--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pascaljanetzky.medium.com/?source=post_page---byline--e6c3260fe0cb--------------------------------)[![Pascal
    Janetzky](../Images/43d68509b63c5f9b3fc9cef3cbfc1a88.png)](https://pascaljanetzky.medium.com/?source=post_page---byline--e6c3260fe0cb--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e6c3260fe0cb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e6c3260fe0cb--------------------------------)
    [Pascal Janetzky](https://pascaljanetzky.medium.com/?source=post_page---byline--e6c3260fe0cb--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e6c3260fe0cb--------------------------------)
    ·6 min read·Oct 29, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e6c3260fe0cb--------------------------------)
    ·阅读时间：6分钟·2024年10月29日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: As the training costs of machine learning models rise [1], continual learning
    (CL) emerges as a useful countermeasure. In CL, a machine learning model (*e.g.*,
    a LLM such as GPT), is trained on a continually arriving stream of data (*e.g.*,
    text data). Crucially, in CL the data cannot be stored, and thus only the most
    recent data is available for training. The main challenge is then to train on
    the current data (often called *task*) while not forgetting the knowledge learned
    from the old tasks. Not forgetting old knowledge is critical because at test-time,
    the model is evaluated on the test-data of all seen tasks. That challenge is often
    described as catastrophic forgetting in the literature, and is part of the stability-plasticity
    tradeoff.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习模型的训练成本上升[1]，持续学习（CL）作为一种有效的应对措施应运而生。在CL中，机器学习模型（*例如*，像GPT这样的LLM）在不断到来的数据流（*例如*，文本数据）上进行训练。关键在于，在CL中数据不能被存储，因此只有最新的数据可用于训练。主要挑战是如何在当前数据（通常称为*任务*）上进行训练，同时又不忘记从旧任务中学到的知识。避免遗忘旧知识至关重要，因为在测试时，模型会在所有已见任务的测试数据上进行评估。这个挑战通常在文献中被称为灾难性遗忘，且它是稳定性-可塑性权衡的一部分。
- en: One the one hand, the stability-plasticity tradeoff refers to keeping network
    parameters (*e.g.*, layer weights) stable to not forget (stability). On the other
    hand, it means to allow parameter changes in order to learn from novel tasks (plasticity).
    CL methods approach this tradeoff from multiple directions, which I have written
    about in [a previous article](https://medium.com/towards-data-science/continual-learning-a-primer-e328ed1d072f).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，稳定性-可塑性权衡指的是保持网络参数（*例如*，层权重）稳定，以避免遗忘（稳定性）。另一方面，它意味着允许参数发生变化，以便从新任务中学习（可塑性）。CL方法从多个角度处理这一权衡，我在[上一篇文章](https://medium.com/towards-data-science/continual-learning-a-primer-e328ed1d072f)中曾提到过。
- en: '![](../Images/67d4ce51f17fed060c52239abcd559c4.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67d4ce51f17fed060c52239abcd559c4.png)'
- en: Photo by [lionel mermoz](https://unsplash.com/@mermoz?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[lionel mermoz](https://unsplash.com/@mermoz?utm_source=medium&utm_medium=referral)
    于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'The focus of today’s article is on the fundamental scenarios that repeatedly
    appear in CL research: class-incremental, domain-incremental, and task-incremental
    learning. The choice of…'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 今天文章的重点是CL研究中反复出现的基本场景：类别增量学习、领域增量学习和任务增量学习。
