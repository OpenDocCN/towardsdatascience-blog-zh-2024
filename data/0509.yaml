- en: OpenAI vs Open-Source Multilingual Embedding Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI ä¸å¼€æºå¤šè¯­è¨€åµŒå…¥æ¨¡å‹
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05?source=collection_archive---------0-----------------------#2024-02-24](https://towardsdatascience.com/openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05?source=collection_archive---------0-----------------------#2024-02-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05?source=collection_archive---------0-----------------------#2024-02-24](https://towardsdatascience.com/openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05?source=collection_archive---------0-----------------------#2024-02-24)
- en: Choosing the model that works best for your data
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é€‰æ‹©æœ€é€‚åˆä½ æ•°æ®çš„æ¨¡å‹
- en: '[](https://ya-lb.medium.com/?source=post_page---byline--e5ccb7c90f05--------------------------------)[![Yann-AÃ«l
    Le Borgne](../Images/acc1c8b32373d7f345064b89b51869fd.png)](https://ya-lb.medium.com/?source=post_page---byline--e5ccb7c90f05--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e5ccb7c90f05--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e5ccb7c90f05--------------------------------)
    [Yann-AÃ«l Le Borgne](https://ya-lb.medium.com/?source=post_page---byline--e5ccb7c90f05--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ya-lb.medium.com/?source=post_page---byline--e5ccb7c90f05--------------------------------)[![Yann-AÃ«l
    Le Borgne](../Images/acc1c8b32373d7f345064b89b51869fd.png)](https://ya-lb.medium.com/?source=post_page---byline--e5ccb7c90f05--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e5ccb7c90f05--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e5ccb7c90f05--------------------------------)
    [Yann-AÃ«l Le Borgne](https://ya-lb.medium.com/?source=post_page---byline--e5ccb7c90f05--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e5ccb7c90f05--------------------------------)
    Â·12 min readÂ·Feb 24, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e5ccb7c90f05--------------------------------)
    Â·é˜…è¯»æ—¶é—´12åˆ†é’ŸÂ·2024å¹´2æœˆ24æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d4106437a23e5edaba980ce8486937d9.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d4106437a23e5edaba980ce8486937d9.png)'
- en: Weâ€™ll use the EU AI act as the data corpus for our embedding model comparison.
    Image by Dall-E 3.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨æ¬§ç›Ÿäººå·¥æ™ºèƒ½æ³•æ¡ˆä½œä¸ºæˆ‘ä»¬çš„åµŒå…¥æ¨¡å‹æ¯”è¾ƒçš„æ•°æ®è¯­æ–™åº“ã€‚å›¾åƒç”±Dall-E 3ç”Ÿæˆã€‚
- en: 'OpenAI recently released their new generation of embedding models, called *embedding
    v3*, which they [describe](https://openai.com/blog/new-embedding-models-and-api-updates)
    as their most performant embedding models, with higher multilingual performances.
    The models come in two classes: a smaller one called `text-embedding-3-small`,
    and a larger and more powerful one called `text-embedding-3-large`.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAIæœ€è¿‘å‘å¸ƒäº†ä»–ä»¬çš„æ–°ä¸€ä»£åµŒå…¥æ¨¡å‹ï¼Œç§°ä¸º*embedding v3*ï¼Œä»–ä»¬[æè¿°](https://openai.com/blog/new-embedding-models-and-api-updates)è¿™äº›æ¨¡å‹æ˜¯æ€§èƒ½æœ€å¼ºçš„åµŒå…¥æ¨¡å‹ï¼Œå…·æœ‰æ›´é«˜çš„å¤šè¯­è¨€æ€§èƒ½ã€‚è¯¥æ¨¡å‹æœ‰ä¸¤ç§ç±»å‹ï¼šä¸€ç§è¾ƒå°çš„ï¼Œç§°ä¸º`text-embedding-3-small`ï¼Œå¦ä¸€ç§è¾ƒå¤§ä¸”æ›´å¼ºå¤§çš„ï¼Œç§°ä¸º`text-embedding-3-large`ã€‚
- en: Very little information was disclosed concerning the way these models were designed
    and trained. As their previous embedding model release (December 2022 with the
    ada-002 model class), OpenAI again chooses a closed-source approach where the
    models may only be accessed through a paid API.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºè¿™äº›æ¨¡å‹çš„è®¾è®¡å’Œè®­ç»ƒæ–¹å¼ï¼Œå…¬å¼€çš„ä¿¡æ¯éå¸¸å°‘ã€‚ä¸ä»–ä»¬ä¹‹å‰çš„åµŒå…¥æ¨¡å‹å‘å¸ƒï¼ˆ2022å¹´12æœˆï¼Œada-002æ¨¡å‹ç±»ï¼‰ä¸€æ ·ï¼ŒOpenAIå†æ¬¡é€‰æ‹©äº†ä¸€ç§é—­æºçš„æ–¹æ³•ï¼Œæ¨¡å‹åªèƒ½é€šè¿‡ä»˜è´¹APIè®¿é—®ã€‚
- en: But are the performances so good that they make it worth paying?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œæ€§èƒ½çœŸçš„é‚£ä¹ˆå¥½å—ï¼Œå€¼å¾—ä»˜è´¹å—ï¼Ÿ
- en: '**The motivation for this post is to empirically compare the performances of
    these new models with their open-source counterparts**. Weâ€™ll rely on a data retrieval
    workflow, where the most relevant documents in a corpus have to be found given
    a user query.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¿™ç¯‡æ–‡ç« çš„åŠ¨æœºæ˜¯é€šè¿‡å®è¯æ¯”è¾ƒè¿™äº›æ–°æ¨¡å‹ä¸å®ƒä»¬çš„å¼€æºå¯¹æ‰‹çš„è¡¨ç°**ã€‚æˆ‘ä»¬å°†ä¾èµ–ä¸€ä¸ªæ•°æ®æ£€ç´¢å·¥ä½œæµï¼Œåœ¨è¯¥å·¥ä½œæµä¸­ï¼Œå¿…é¡»æ ¹æ®ç”¨æˆ·æŸ¥è¯¢æ‰¾åˆ°è¯­æ–™åº“ä¸­æœ€ç›¸å…³çš„æ–‡æ¡£ã€‚'
- en: Our corpus will be the [European AI Act](https://artificialintelligenceact.eu/),
    which is currently in its final stages of validation. An interesting characteristic
    of this corpus, besides being the first-ever legal framework on AI worldwide,
    is its availability in 24 languages. This makes it possible to compare the accuracy
    of data retrieval **across different families of languages**.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„è¯­æ–™åº“å°†æ˜¯[æ¬§æ´²äººå·¥æ™ºèƒ½æ³•æ¡ˆ](https://artificialintelligenceact.eu/)ï¼Œè¯¥æ³•æ¡ˆç›®å‰æ­£å¤„äºæœ€ç»ˆéªŒè¯é˜¶æ®µã€‚è¿™ä¸ªè¯­æ–™åº“çš„ä¸€ä¸ªæœ‰è¶£ç‰¹ç‚¹æ˜¯ï¼Œé™¤äº†å®ƒæ˜¯å…¨çƒé¦–ä¸ªå…³äºäººå·¥æ™ºèƒ½çš„æ³•å¾‹æ¡†æ¶å¤–ï¼Œå®ƒè¿˜æä¾›24ç§è¯­è¨€ç‰ˆæœ¬ã€‚è¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿæ¯”è¾ƒ**ä¸åŒè¯­è¨€å®¶æ—ä¹‹é—´çš„æ•°æ®æ£€ç´¢å‡†ç¡®æ€§**ã€‚
- en: 'The post will go through the two main following steps:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡å°†æ¶µç›–ä»¥ä¸‹ä¸¤ä¸ªä¸»è¦æ­¥éª¤ï¼š
- en: Generate a custom synthetic question/answer dataset from a multilingual text
    corpus
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»å¤šè¯­è¨€æ–‡æœ¬è¯­æ–™åº“ä¸­ç”Ÿæˆä¸€ä¸ªå®šåˆ¶çš„åˆæˆé—®ç­”æ•°æ®é›†
- en: Compare the accuracy of OpenAI and state-of-the-art open-source embedding models
    on this custom dataset.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯”è¾ƒOpenAIä¸æœ€å…ˆè¿›çš„å¼€æºåµŒå…¥æ¨¡å‹åœ¨æ­¤è‡ªå®šä¹‰æ•°æ®é›†ä¸Šçš„å‡†ç¡®æ€§ã€‚
- en: The code and data to reproduce the results presented in this post are made available
    in [this Github repository](https://github.com/Yannael/multilingual-embeddings).
    Note that the EU AI Act is used as an example, and the methodology followed in
    this post can be adapted to other data corpus.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†é‡ç°æœ¬æ–‡ä¸­å±•ç¤ºçš„ç»“æœï¼Œä»£ç å’Œæ•°æ®å·²ç»å…¬å¼€åœ¨[è¿™ä¸ªGithubä»“åº“](https://github.com/Yannael/multilingual-embeddings)ä¸­ã€‚è¯·æ³¨æ„ï¼Œã€Šæ¬§ç›Ÿäººå·¥æ™ºèƒ½æ³•æ¡ˆã€‹ä½œä¸ºç¤ºä¾‹ä½¿ç”¨ï¼Œæœ¬æ–‡ä¸­éµå¾ªçš„æ–¹æ³•å¯ä»¥é€‚åº”å…¶ä»–æ•°æ®è¯­æ–™åº“ã€‚
- en: Generate a custom Q/A dataset
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç”Ÿæˆè‡ªå®šä¹‰çš„é—®ç­”æ•°æ®é›†
- en: Let us first start by generating a dataset of questions and answers (Q/A) on
    custom data, which will be used to assess the performance of different embedding
    models. The benefits of generating a custom Q/A dataset are twofold. First, it
    avoids biases by ensuring that the dataset has not been part of the training of
    an embedding model, which may happen on reference benchmarks such as [MTEB](https://huggingface.co/spaces/mteb/leaderboard).
    Second, it allows to tailor the assessment to a specific corpus of data, which
    can be relevant in the case of retrieval augmented applications (RAG) for example.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é¦–å…ˆå¼€å§‹ç”Ÿæˆè‡ªå®šä¹‰æ•°æ®ä¸Šçš„é—®ç­”æ•°æ®é›†ï¼ˆQ/Aï¼‰ï¼Œç”¨äºè¯„ä¼°ä¸åŒåµŒå…¥æ¨¡å‹çš„æ€§èƒ½ã€‚ç”Ÿæˆè‡ªå®šä¹‰é—®ç­”æ•°æ®é›†æœ‰ä¸¤ä¸ªå¥½å¤„ã€‚é¦–å…ˆï¼Œå®ƒé¿å…äº†åè§ï¼Œç¡®ä¿æ•°æ®é›†æœªå‚ä¸åµŒå…¥æ¨¡å‹çš„è®­ç»ƒï¼Œè¿™ç§æƒ…å†µå¯èƒ½å‘ç”Ÿåœ¨å‚è€ƒåŸºå‡†ä¸Šï¼Œå¦‚[MTEB](https://huggingface.co/spaces/mteb/leaderboard)ã€‚å…¶æ¬¡ï¼Œå®ƒä½¿å¾—è¯„ä¼°å¯ä»¥æ ¹æ®ç‰¹å®šæ•°æ®è¯­æ–™åº“è¿›è¡Œå®šåˆ¶ï¼Œè¿™åœ¨æ£€ç´¢å¢å¼ºåº”ç”¨ï¼ˆRAGï¼‰ç­‰æƒ…å†µä¸‹å°¤ä¸ºé‡è¦ã€‚
- en: 'We will follow the simple process suggested by [Llama Index in their documentation](https://blog.llamaindex.ai/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971).
    The corpus is first split into a set of chunks. Then, for each chunk, a set of
    synthetic questions are generated by means of a large language model (LLM), such
    that the answer lies in the corresponding chunk. The process is illustrated below:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†éµå¾ª[Llama Indexæ–‡æ¡£ä¸­](https://blog.llamaindex.ai/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971)å»ºè®®çš„ç®€å•æµç¨‹ã€‚é¦–å…ˆå°†è¯­æ–™åº“æ‹†åˆ†æˆå¤šä¸ªå—ã€‚ç„¶åï¼Œå¯¹äºæ¯ä¸ªå—ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆä¸€ç»„åˆæˆé—®é¢˜ï¼Œä½¿å¾—é—®é¢˜çš„ç­”æ¡ˆä½äºç›¸åº”çš„å—ä¸­ã€‚è¯¥è¿‡ç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
- en: '![](../Images/debcd01ca6179cf42bb71ec7c684627d.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/debcd01ca6179cf42bb71ec7c684627d.png)'
- en: Generating a question/answer dataset for your data, methodology from [Llama
    Index](https://blog.llamaindex.ai/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä½ çš„æ•°æ®ç”Ÿæˆé—®ç­”æ•°æ®é›†ï¼Œæ–¹æ³•å‚è€ƒ[Llama Index](https://blog.llamaindex.ai/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971)
- en: Implementing this strategy is straightforward with a data framework for LLM
    such as Llama Index. The loading of the corpus and splitting of text can be conveniently
    carried out using high-level functions, as illustrated with the following code.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å®ç°è¿™ä¸ªç­–ç•¥åœ¨ä½¿ç”¨åƒLlama Indexè¿™æ ·çš„LLMæ•°æ®æ¡†æ¶æ—¶éå¸¸ç›´æ¥ã€‚è¯­æ–™åº“çš„åŠ è½½å’Œæ–‡æœ¬çš„æ‹†åˆ†å¯ä»¥é€šè¿‡é«˜é˜¶å‡½æ•°æ–¹ä¾¿åœ°å®Œæˆï¼Œå¦‚ä¸‹é¢çš„ä»£ç æ‰€ç¤ºã€‚
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this example, the corpus is the EU AI Act in English, taken directly from
    the Web using this [official URL](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206).
    We use the draft version from April 2021, as the final version is not yet available
    for all European languages. In this version, English language can be replaced
    in the URL by any of the 23 other EU official languages to retrieve the text in
    a different language (BG for Bulgarian, ES for Spanish, CS for Czech, and so forth).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œè¯­æ–™åº“æ˜¯ã€Šæ¬§ç›Ÿäººå·¥æ™ºèƒ½æ³•æ¡ˆã€‹çš„è‹±æ–‡ç‰ˆï¼Œç›´æ¥ä»ç½‘ç»œä¸Šè·å–ï¼Œä½¿ç”¨çš„æ˜¯è¿™ä¸ª[å®˜æ–¹ç½‘å€](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206)ã€‚æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯2021å¹´4æœˆçš„è‰æ¡ˆç‰ˆæœ¬ï¼Œå› ä¸ºæœ€ç»ˆç‰ˆæœ¬å°šæœªæä¾›æ‰€æœ‰æ¬§ç›Ÿè¯­è¨€ã€‚åœ¨è¿™ä¸ªç‰ˆæœ¬ä¸­ï¼Œç½‘å€ä¸­çš„è‹±æ–‡å¯ä»¥æ›¿æ¢ä¸ºå…¶ä»–23ç§æ¬§ç›Ÿå®˜æ–¹è¯­è¨€ä¸­çš„ä»»ä½•ä¸€ç§ï¼Œä»¥è·å–ä¸åŒè¯­è¨€çš„æ–‡æœ¬ï¼ˆä¾‹å¦‚ï¼ŒBGä»£è¡¨ä¿åŠ åˆ©äºšè¯­ï¼ŒESä»£è¡¨è¥¿ç­ç‰™è¯­ï¼ŒCSä»£è¡¨æ·å…‹è¯­ï¼Œç­‰ç­‰ï¼‰ã€‚
- en: '![](../Images/b228519bd3b36c7cec689ffd01b3226c.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b228519bd3b36c7cec689ffd01b3226c.png)'
- en: Download links to the EU AI Act for the 24 official EU languages (from [EU official
    website](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206))
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹è½½ã€Šæ¬§ç›Ÿäººå·¥æ™ºèƒ½æ³•æ¡ˆã€‹åœ¨24ç§å®˜æ–¹æ¬§ç›Ÿè¯­è¨€ä¸­çš„é“¾æ¥ï¼ˆæ¥è‡ª[æ¬§ç›Ÿå®˜ç½‘](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206)ï¼‰
- en: We use the SentenceSplitter object to split the document in chunks of 1000 tokens.
    For English, this results in about 100 chunks.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨SentenceSplitterå¯¹è±¡å°†æ–‡æ¡£æ‹†åˆ†æˆ1000ä¸ªæ ‡è®°çš„å—ã€‚å¯¹äºè‹±æ–‡æ–‡æœ¬ï¼Œè¿™å¤§çº¦ä¼šç”Ÿæˆ100ä¸ªå—ã€‚
- en: 'Each chunk is then provided as context to the following prompt ([the default
    prompt suggested in the Llama Index library](https://github.com/run-llama/llama_index/blob/c058f2531ea86ee74822cb1421ceaeee7098a99f/llama_index/finetuning/embeddings/common.py#L51)):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæ¯ä¸ªæ–‡æ¡£å—ä½œä¸ºä¸Šä¸‹æ–‡æä¾›ç»™ä»¥ä¸‹æç¤ºï¼ˆ[Llama Indexåº“ä¸­å»ºè®®çš„é»˜è®¤æç¤º](https://github.com/run-llama/llama_index/blob/c058f2531ea86ee74822cb1421ceaeee7098a99f/llama_index/finetuning/embeddings/common.py#L51)ï¼‰ï¼š
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The prompt aims at generating questions about the document chunk, as if a teacher
    were preparing an upcoming quiz. The number of questions to generate for each
    chunk is passed as the parameter â€˜num_questions_per_chunkâ€™, which we set to two.
    Questions can then be generated by calling the generate_qa_embedding_pairs from
    the Llama Index library:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æç¤ºçš„ç›®çš„æ˜¯ç”Ÿæˆå…³äºæ–‡æ¡£å—çš„é—®é¢˜ï¼Œå°±åƒè€å¸ˆåœ¨å‡†å¤‡å³å°†åˆ°æ¥çš„å°æµ‹éªŒä¸€æ ·ã€‚æ¯ä¸ªæ–‡æ¡£å—è¦ç”Ÿæˆçš„é—®é¢˜æ•°é‡é€šè¿‡å‚æ•°â€˜num_questions_per_chunkâ€™ä¼ é€’ï¼Œæˆ‘ä»¬å°†å…¶è®¾ç½®ä¸ºä¸¤ä¸ªã€‚ç„¶åï¼Œå¯ä»¥é€šè¿‡è°ƒç”¨Llama
    Indexåº“ä¸­çš„generate_qa_embedding_pairsæ¥ç”Ÿæˆé—®é¢˜ï¼š
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We rely for this task on the GPT-3.5-turbo-0125 mode from OpenAI, which is according
    to OpenAI the flagship model of this family, supporting a 16K context window and
    optimized for dialog ([https://platform.openai.com/docs/models/gpt-3-5-turbo](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fplatform.openai.com%2Fdocs%2Fmodels%2Fgpt-3-5-turbo)).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨æ­¤ä»»åŠ¡ä¸­ä¾èµ–äºOpenAIçš„GPT-3.5-turbo-0125æ¨¡å‹ï¼Œæ ¹æ®OpenAIçš„è¯´æ³•ï¼Œè¿™æ˜¯è¯¥ç³»åˆ—çš„æ——èˆ°æ¨¡å‹ï¼Œæ”¯æŒ16Kçš„ä¸Šä¸‹æ–‡çª—å£ï¼Œå¹¶é’ˆå¯¹å¯¹è¯è¿›è¡Œäº†ä¼˜åŒ–ï¼ˆ[https://platform.openai.com/docs/models/gpt-3-5-turbo](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fplatform.openai.com%2Fdocs%2Fmodels%2Fgpt-3-5-turbo)ï¼‰ã€‚
- en: 'The resulting objet â€˜qa_datasetâ€™ contains the questions and answers (chunks)
    pairs. As an example of generated questions, here is the result for the first
    two questions (for which the â€˜answerâ€™ is the first chunk of text):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆçš„å¯¹è±¡â€˜qa_datasetâ€™åŒ…å«é—®é¢˜å’Œç­”æ¡ˆï¼ˆæ–‡æ¡£å—ï¼‰å¯¹ã€‚ä½œä¸ºç”Ÿæˆé—®é¢˜çš„ç¤ºä¾‹ï¼Œä»¥ä¸‹æ˜¯å‰ä¸¤ä¸ªé—®é¢˜çš„ç»“æœï¼ˆå…¶ä¸­â€˜ç­”æ¡ˆâ€™æ˜¯ç¬¬ä¸€ä¸ªæ–‡æ¡£å—çš„æ–‡æœ¬ï¼‰ï¼š
- en: 1) What are the main objectives of the proposal for a Regulation laying down
    harmonised rules on artificial intelligence (Artificial Intelligence Act) according
    to the explanatory memorandum?
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 1) æ ¹æ®è¯´æ˜æ€§å¤‡å¿˜å½•ï¼Œå…³äºäººå·¥æ™ºèƒ½çš„ç»Ÿä¸€è§„åˆ™ææ¡ˆï¼ˆäººå·¥æ™ºèƒ½æ³•æ¡ˆï¼‰çš„ä¸»è¦ç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ
- en: 2) How does the proposal for a Regulation on artificial intelligence aim to
    address the risks associated with the use of AI while promoting the uptake of
    AI in the European Union, as outlined in the context information?
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 2) æ ¹æ®ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå…³äºäººå·¥æ™ºèƒ½çš„ç»Ÿä¸€è§„åˆ™ææ¡ˆå¦‚ä½•åœ¨ä¿ƒè¿›äººå·¥æ™ºèƒ½åœ¨æ¬§ç›Ÿçš„åº”ç”¨çš„åŒæ—¶ï¼Œè§£å†³ä¸ä½¿ç”¨äººå·¥æ™ºèƒ½ç›¸å…³çš„é£é™©ï¼Ÿ
- en: The number of chunks and questions depends on the language, ranging from around
    100 chunks and 200 questions for English, to 200 chunks and 400 questions for
    Hungarian.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡æ¡£å—å’Œé—®é¢˜çš„æ•°é‡å–å†³äºè¯­è¨€ï¼Œä»è‹±è¯­çš„çº¦100ä¸ªæ–‡æ¡£å—å’Œ200ä¸ªé—®é¢˜ï¼Œåˆ°åŒˆç‰™åˆ©è¯­çš„200ä¸ªæ–‡æ¡£å—å’Œ400ä¸ªé—®é¢˜ä¸ç­‰ã€‚
- en: Evaluation of OpenAI embedding models
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAIåµŒå…¥æ¨¡å‹çš„è¯„ä¼°
- en: Our evaluation function follows the [Llama Index documentation](https://docs.llamaindex.ai/en/stable/examples/finetuning/embeddings/finetune_embedding.html)
    and consists in two main steps. First, the embeddings for all answers (document
    chunks) are stored in a VectorStoreIndex for efficient retrieval. Then, the evaluation
    function loops over all queries, retrieves the top k most similar documents, and
    the accuracy of the retrieval in assessed in terms of MRR ([Mean Reciprocal Rank](https://en.wikipedia.org/wiki/Mean_reciprocal_rank)).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„è¯„ä¼°å‡½æ•°éµå¾ª[Llama Indexæ–‡æ¡£](https://docs.llamaindex.ai/en/stable/examples/finetuning/embeddings/finetune_embedding.html)ï¼Œç”±ä¸¤ä¸ªä¸»è¦æ­¥éª¤ç»„æˆã€‚é¦–å…ˆï¼Œæ‰€æœ‰ç­”æ¡ˆï¼ˆæ–‡æ¡£å—ï¼‰çš„åµŒå…¥å­˜å‚¨åœ¨ä¸€ä¸ªVectorStoreIndexä¸­ï¼Œä»¥ä¾¿é«˜æ•ˆæ£€ç´¢ã€‚ç„¶åï¼Œè¯„ä¼°å‡½æ•°å¾ªç¯éå†æ‰€æœ‰æŸ¥è¯¢ï¼Œæ£€ç´¢æœ€ç›¸ä¼¼çš„å‰kä¸ªæ–‡æ¡£ï¼Œå¹¶é€šè¿‡MRRï¼ˆ[å¹³å‡å€’æ•°æ’å](https://en.wikipedia.org/wiki/Mean_reciprocal_rank)ï¼‰æ¥è¯„ä¼°æ£€ç´¢çš„å‡†ç¡®æ€§ã€‚
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The embedding model is passed to the evaluation function by means of the `embed_model`
    argument, which for OpenAI models is an OpenAIEmbedding object initialised with
    the name of the model, and the model dimension.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: åµŒå…¥æ¨¡å‹é€šè¿‡`embed_model`å‚æ•°ä¼ é€’ç»™è¯„ä¼°å‡½æ•°ï¼Œå¯¹äºOpenAIæ¨¡å‹æ¥è¯´ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆå§‹åŒ–äº†æ¨¡å‹åç§°å’Œæ¨¡å‹ç»´åº¦çš„OpenAIEmbeddingå¯¹è±¡ã€‚
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `dimensions` API parameter can shorten embeddings (i.e. remove some numbers
    from the end of the sequence) without the embedding losing its concept-representing
    properties. OpenAI for example suggests [in their annoucement](https://openai.com/blog/new-embedding-models-and-api-updates)
    that on the MTEB benchmark, an embedding can be shortened to a size of 256 while
    still outperforming an unshortened `text-embedding-ada-002` embedding with a size
    of 1536.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`dimensions` APIå‚æ•°å¯ä»¥ç¼©çŸ­åµŒå…¥ï¼ˆå³ç§»é™¤åºåˆ—æœ«å°¾çš„ä¸€äº›æ•°å­—ï¼‰ï¼Œè€Œä¸ä¼šå¤±å»å…¶è¡¨ç¤ºæ¦‚å¿µçš„ç‰¹æ€§ã€‚OpenAIä¾‹å¦‚åœ¨å®ƒä»¬çš„[å…¬å‘Š](https://openai.com/blog/new-embedding-models-and-api-updates)ä¸­å»ºè®®ï¼Œåœ¨MTEBåŸºå‡†æµ‹è¯•ä¸­ï¼ŒåµŒå…¥å¯ä»¥ç¼©çŸ­åˆ°256çš„å¤§å°ï¼ŒåŒæ—¶ä»ç„¶ä¼˜äºæœªç¼©çŸ­çš„`text-embedding-ada-002`åµŒå…¥ï¼ˆå…¶å¤§å°ä¸º1536ï¼‰ã€‚'
- en: 'We ran the evaluation function on four different OpenAI embedding models:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨å››ä¸ªä¸åŒçš„OpenAIåµŒå…¥æ¨¡å‹ä¸Šè¿è¡Œäº†è¯„ä¼°å‡½æ•°ï¼š
- en: 'two versions of `text-embedding-3-large` : one with the lowest possible dimension
    (256), and the other one with the highest possible dimension (3072). These are
    called â€˜OAI-large-256â€™ and â€˜OAI-large-3072â€™.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸¤ä¸ªç‰ˆæœ¬çš„`text-embedding-3-large`ï¼šä¸€ä¸ªæ˜¯æœ€ä½ç»´åº¦ï¼ˆ256ï¼‰ï¼Œå¦ä¸€ä¸ªæ˜¯æœ€é«˜ç»´åº¦ï¼ˆ3072ï¼‰ã€‚å®ƒä»¬åˆ†åˆ«ç§°ä¸ºâ€˜OAI-large-256â€™å’Œâ€˜OAI-large-3072â€™ã€‚
- en: 'OAI-small: The `text-embedding-3-small` embedding model, with a dimension of
    1536.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OAI-smallï¼š`text-embedding-3-small`åµŒå…¥æ¨¡å‹ï¼Œç»´åº¦ä¸º1536ã€‚
- en: 'OAI-ada-002: The legacy `text-embedding-ada-002` model, with a dimension of
    1536.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OAI-ada-002ï¼šä¼ ç»Ÿçš„`text-embedding-ada-002`æ¨¡å‹ï¼Œç»´åº¦ä¸º1536ã€‚
- en: 'Each model was evaluated on four different languages: English (EN), French
    (FR), Czech (CS) and Hungarian (HU), covering examples of Germanic, Romance, Slavic
    and Uralic language, respectively.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªæ¨¡å‹éƒ½åœ¨å››ç§ä¸åŒçš„è¯­è¨€ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼šè‹±è¯­ï¼ˆENï¼‰ã€æ³•è¯­ï¼ˆFRï¼‰ã€æ·å…‹è¯­ï¼ˆCSï¼‰å’ŒåŒˆç‰™åˆ©è¯­ï¼ˆHUï¼‰ï¼Œåˆ†åˆ«æ¶µç›–äº†æ—¥è€³æ›¼è¯­ç³»ã€ç½—æ›¼è¯­ç³»ã€æ–¯æ‹‰å¤«è¯­ç³»å’Œä¹Œæ‹‰å°”è¯­ç³»çš„ç¤ºä¾‹ã€‚
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The resulting accuracy in terms of MRR is reported below:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœä¸­çš„å‡†ç¡®åº¦ï¼ˆä»¥MRRè¡¡é‡ï¼‰å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](../Images/e85932db410223b301977602a81afcd7.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e85932db410223b301977602a81afcd7.png)'
- en: Summary of performances for the OpenAI models
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAIæ¨¡å‹è¡¨ç°çš„æ€»ç»“
- en: As expected, for the large model, better performances are observed with the
    larger embedding size of 3072\. Compared with the small and legacy Ada models,
    the large model is however smaller than we would have expected. For comparison,
    we also report below the performances obtained by the OpenAI models on the MTEB
    benchmark.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œå¯¹äºå¤§å‹æ¨¡å‹ï¼Œéšç€åµŒå…¥å¤§å°å¢å¤§åˆ°3072ï¼Œè¡¨ç°æœ‰æ‰€æå‡ã€‚ä¸å°å‹å’Œä¼ ç»ŸAdaæ¨¡å‹ç›¸æ¯”ï¼Œå¤§å‹æ¨¡å‹çš„è¡¨ç°è™½æœ‰æ‰€æå‡ï¼Œä½†ä¾ç„¶æ¯”æˆ‘ä»¬é¢„æœŸçš„è¦å°ã€‚ä¸ºè¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬è¿˜åœ¨ä¸‹æ–¹æŠ¥å‘Šäº†OpenAIæ¨¡å‹åœ¨MTEBåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ã€‚
- en: '![](../Images/d7f95f6dd6fc1fa806136b1f0a8236bd.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d7f95f6dd6fc1fa806136b1f0a8236bd.png)'
- en: Performances of OpenAI embedding models, as reported in their [official announcement](https://openai.com/blog/new-embedding-models-and-api-updates)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAIåµŒå…¥æ¨¡å‹çš„è¡¨ç°ï¼Œè¯¦è§å®ƒä»¬çš„[å®˜æ–¹å…¬å‘Š](https://openai.com/blog/new-embedding-models-and-api-updates)ã€‚
- en: It is interesting to note that the differences in performances between the large,
    small and Ada models are much less pronounced in our assessment than in the MTEB
    benchmark, reflecting the fact that the average performances observed in large
    benchmarks do not necessarily reflect those obtained on custom datasets.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨æˆ‘ä»¬çš„è¯„ä¼°ä¸­ï¼Œå¤§å‹ã€å°å‹å’ŒAdaæ¨¡å‹ä¹‹é—´çš„è¡¨ç°å·®å¼‚ï¼Œæ¯”MTEBåŸºå‡†æµ‹è¯•ä¸­è§‚å¯Ÿåˆ°çš„å·®å¼‚è¦å°ï¼Œè¿™åæ˜ äº†ä¸€ä¸ªäº‹å®ï¼Œå³åœ¨å¤§å‹åŸºå‡†æµ‹è¯•ä¸­è§‚å¯Ÿåˆ°çš„å¹³å‡è¡¨ç°ï¼Œå¹¶ä¸ä¸€å®šèƒ½åæ˜ åœ¨å®šåˆ¶æ•°æ®é›†ä¸Šè·å¾—çš„ç»“æœã€‚
- en: Evaluation of open-source embedding models
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¼€æºåµŒå…¥æ¨¡å‹çš„è¯„ä¼°
- en: The open-source research around embeddings is quite active, and new models are
    regularly published. A good place to keep updated about the latest published models
    is the [Hugging Face ğŸ˜Š MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºåµŒå…¥çš„å¼€æºç ”ç©¶éå¸¸æ´»è·ƒï¼Œæ–°çš„æ¨¡å‹å®šæœŸå‘å¸ƒã€‚ä¸€ä¸ªä¸é”™çš„ä¿æŒæœ€æ–°å‘å¸ƒæ¨¡å‹çš„åœ°æ–¹æ˜¯[Hugging Face ğŸ˜Š MTEBæ’è¡Œæ¦œ](https://huggingface.co/spaces/mteb/leaderboard)ã€‚
- en: For the comparison in this article, we selected a set of four embedding models
    recently published (2024). The criteria for selection were their average score
    on the MTEB leaderboard and their ability to deal with multilingual data. A summary
    of the main characteristics of the selected models are reported below.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡ä¸­çš„æ¯”è¾ƒï¼Œæˆ‘ä»¬é€‰æ‹©äº†ä¸€ç»„æœ€è¿‘å‘å¸ƒçš„å››ç§åµŒå…¥æ¨¡å‹ï¼ˆ2024å¹´ï¼‰ã€‚é€‰æ‹©æ ‡å‡†æ˜¯å®ƒä»¬åœ¨MTEBæ’è¡Œæ¦œä¸Šçš„å¹³å‡å¾—åˆ†ä»¥åŠå¤„ç†å¤šè¯­è¨€æ•°æ®çš„èƒ½åŠ›ã€‚ä»¥ä¸‹æ˜¯æ‰€é€‰æ¨¡å‹çš„ä¸»è¦ç‰¹å¾æ€»ç»“ã€‚
- en: Selected open-source embedding models
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€é€‰çš„å¼€æºåµŒå…¥æ¨¡å‹
- en: '[***E5-Mistral-7B-instruct***](https://huggingface.co/intfloat/e5-mistral-7b-instruct)(E5-mistral-7b):
    This E5 embedding model by Microsoft is initialized from [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)
    and fine-tuned on a mixture of multilingual datasets. The model performs best
    on the MTEB leaderboard, but is also by far the biggest one (14GB).'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[***E5-Mistral-7B-instruct***](https://huggingface.co/intfloat/e5-mistral-7b-instruct)(E5-mistral-7b)ï¼šå¾®è½¯çš„è¿™æ¬¾E5åµŒå…¥æ¨¡å‹æ˜¯ä»[Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)åˆå§‹åŒ–çš„ï¼Œå¹¶åœ¨å¤šè¯­è¨€æ•°æ®é›†çš„æ··åˆä¸Šè¿›è¡Œå¾®è°ƒã€‚è¯¥æ¨¡å‹åœ¨MTEBæ’è¡Œæ¦œä¸Šè¡¨ç°æœ€ä½³ï¼Œä½†ä¹Ÿæ˜¯æœ€å¤§çš„æ¨¡å‹ï¼ˆ14GBï¼‰ã€‚'
- en: '[***multilingual-e5-large-instruct***](https://huggingface.co/intfloat/multilingual-e5-large-instruct)(ML-E5-large):
    Another E5 model from Microsoft, meant to better handle multilingual data. It
    is initialized from [xlm-roberta-large](https://huggingface.co/xlm-roberta-large)
    and trained on a mixture of multilingual datasets. It is much smaller (10 times)
    than E5-Mistral, but also has a much lower context size (514).'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[***multilingual-e5-large-instruct***](https://huggingface.co/intfloat/multilingual-e5-large-instruct)(ML-E5-large)ï¼šå¾®è½¯çš„å¦ä¸€æ¬¾E5æ¨¡å‹ï¼Œæ—¨åœ¨æ›´å¥½åœ°å¤„ç†å¤šè¯­è¨€æ•°æ®ã€‚å®ƒæ˜¯ä»[xlm-roberta-large](https://huggingface.co/xlm-roberta-large)åˆå§‹åŒ–çš„ï¼Œå¹¶åœ¨å¤šè¯­è¨€æ•°æ®é›†çš„æ··åˆä¸Šè¿›è¡Œè®­ç»ƒã€‚å®ƒæ¯”E5-Mistralå°å¾—å¤šï¼ˆå°10å€ï¼‰ï¼Œä½†ä¸Šä¸‹æ–‡å¤§å°ä¹Ÿæ›´å°ï¼ˆ514ï¼‰ã€‚'
- en: '[***BGE-M3***](https://huggingface.co/BAAI/bge-m3): The model was designed
    by the Beijing Academy of Artificial Intelligence, and is their state-of-the-art
    embedding model for multilingual data, supporting more than 100 working languages.
    It was not yet benchmarked on the MTEB leaderboard as of 22/02/2024.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[***BGE-M3***](https://huggingface.co/BAAI/bge-m3)ï¼šè¯¥æ¨¡å‹ç”±åŒ—äº¬äººå·¥æ™ºèƒ½ç ”ç©¶é™¢è®¾è®¡ï¼Œæ˜¯å…¶é’ˆå¯¹å¤šè¯­è¨€æ•°æ®çš„æœ€å…ˆè¿›åµŒå…¥æ¨¡å‹ï¼Œæ”¯æŒ100å¤šç§å·¥ä½œè¯­è¨€ã€‚æˆªè‡³2024å¹´2æœˆ22æ—¥ï¼Œå®ƒå°šæœªåœ¨MTEBæ’è¡Œæ¦œä¸Šè¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚'
- en: '[***nomic-embed-text-v1***](https://huggingface.co/nomic-ai/nomic-embed-text-v1)(Nomic-Embed):
    The model was designed by [Nomic](https://home.nomic.ai/), and claims better performances
    than OpenAI Ada-002 and text-embedding-3-small while being only 0.55GB in size.
    Interestingly, the model is the first to be fully reproducible and auditable (open
    data and open-source training code).'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[***nomic-embed-text-v1***](https://huggingface.co/nomic-ai/nomic-embed-text-v1)(Nomic-Embed)ï¼šè¯¥æ¨¡å‹ç”±[Nomic](https://home.nomic.ai/)è®¾è®¡ï¼Œå£°ç§°åœ¨æ€§èƒ½ä¸Šä¼˜äºOpenAI
    Ada-002å’Œtext-embedding-3-smallï¼ŒåŒæ—¶ä»…ä¸º0.55GBå¤§å°ã€‚æœ‰è¶£çš„æ˜¯ï¼Œè¯¥æ¨¡å‹æ˜¯ç¬¬ä¸€ä¸ªå®Œå…¨å¯å¤ç°å’Œå¯å®¡è®¡çš„æ¨¡å‹ï¼ˆå¼€æ”¾æ•°æ®å’Œå¼€æºè®­ç»ƒä»£ç ï¼‰ã€‚'
- en: 'The code for evaluating these open-source models is similar to the code used
    for OpenAI models. The main change lies in the model specifications, where additional
    details such as maximum context length and pooling types have to be specified.
    We then evaluate each model for each of the four languages:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: è¯„ä¼°è¿™äº›å¼€æºæ¨¡å‹çš„ä»£ç ä¸ç”¨äºOpenAIæ¨¡å‹çš„ä»£ç ç±»ä¼¼ã€‚ä¸»è¦çš„å˜åŒ–åœ¨äºæ¨¡å‹è§„æ ¼ï¼Œå¿…é¡»æŒ‡å®šé™„åŠ çš„ç»†èŠ‚ï¼Œå¦‚æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦å’Œæ± åŒ–ç±»å‹ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹å››ç§è¯­è¨€ä¸­çš„æ¯ç§æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼š
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The resulting accuracies in terms of MRR are reported below.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æŠ¥å‘Šäº†ä»¥MRRä¸ºæ ‡å‡†çš„å‡†ç¡®åº¦ç»“æœã€‚
- en: '![](../Images/f77ab24c4d15f7334aa730420a285de6.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f77ab24c4d15f7334aa730420a285de6.png)'
- en: Summary of performances for the open-source models
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å¼€æºæ¨¡å‹çš„æ€§èƒ½æ€»ç»“
- en: BGE-M3 turns out to provide the best performances, followed on average by ML-E5-Large,
    E5-mistral-7b and Nomic-Embed. BGE-M3 model is not yet benchmarked on the MTEB
    leaderboard, and our results indicate that it could rank higher than other models.
    It is also interesting to note that while BGE-M3 is optimized for multilingual
    data, it also performs better for English than the other models.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: BGE-M3è¡¨ç°æœ€ä½³ï¼Œå¹³å‡è¡¨ç°ç´§éšå…¶åçš„æ˜¯ML-E5-Largeã€E5-mistral-7bå’ŒNomic-Embedã€‚BGE-M3æ¨¡å‹å°šæœªåœ¨MTEBæ’è¡Œæ¦œä¸Šè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜å®ƒå¯èƒ½ä¼šæ’åœ¨å…¶ä»–æ¨¡å‹ä¹‹å‰ã€‚æœ‰è¶£çš„æ˜¯ï¼Œè™½ç„¶BGE-M3æ˜¯é’ˆå¯¹å¤šè¯­è¨€æ•°æ®è¿›è¡Œä¼˜åŒ–çš„ï¼Œä½†å®ƒåœ¨è‹±è¯­ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚
- en: We additionally report the processing times for each embedding model below.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜æŠ¥å‘Šäº†æ¯ä¸ªåµŒå…¥æ¨¡å‹çš„å¤„ç†æ—¶é—´ã€‚
- en: '![](../Images/026e18ce209b4a9150cc60b529e46e89.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/026e18ce209b4a9150cc60b529e46e89.png)'
- en: Processing times in seconds for going throught the English Q/A dataset
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è‹±æ–‡é—®ç­”æ•°æ®é›†çš„å¤„ç†æ—¶é—´ï¼ˆä»¥ç§’ä¸ºå•ä½ï¼‰
- en: The E5-mistral-7b, which is more than 10 times larger than the other models,
    is without surprise by far the slowest model.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: E5-mistral-7bæ¯”å…¶ä»–æ¨¡å‹å¤§10å€ä»¥ä¸Šï¼Œæ¯«ä¸æ„å¤–åœ°æ˜¯æœ€æ…¢çš„æ¨¡å‹ã€‚
- en: Conclusion
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: Let us put side-by-side of the performance of the eight tested models in a single
    figure.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å°†è¿™å…«ä¸ªæµ‹è¯•æ¨¡å‹çš„æ€§èƒ½å¹¶æ’å±•ç¤ºåœ¨ä¸€ä¸ªå›¾ä¸­ã€‚
- en: '![](../Images/2098db260ef495cac46c32a79518bcc3.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2098db260ef495cac46c32a79518bcc3.png)'
- en: Summary of performances for the eight tested models
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å…«ä¸ªæµ‹è¯•æ¨¡å‹çš„æ€§èƒ½æ€»ç»“
- en: 'The key observations from these results are:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¿™äº›ç»“æœä¸­å¯ä»¥å¾—å‡ºä»¥ä¸‹å…³é”®è§‚å¯Ÿï¼š
- en: '**Best performances were obtained by open-source models**. The [BGE-M3](https://huggingface.co/BAAI/bge-m3)
    model, developed by the Beijing Academy of Artificial Intelligence, emerged as
    the top performer. The model has the same context length as OpenAI models (8K),
    for a size of 2.2GB.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¼€æºæ¨¡å‹çš„è¡¨ç°æœ€å¥½**ã€‚ç”±åŒ—äº¬äººå·¥æ™ºèƒ½å­¦ä¼šå¼€å‘çš„ [BGE-M3](https://huggingface.co/BAAI/bge-m3) æ¨¡å‹è¡¨ç°æœ€ä½³ã€‚è¯¥æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦ä¸
    OpenAI æ¨¡å‹ç›¸åŒï¼ˆ8Kï¼‰ï¼Œå¤§å°ä¸º 2.2GBã€‚'
- en: '**Consistency Across OpenAIâ€™s Range**. The performances of the large (3072),
    small and legacy OpenAI models were very similar. Reducing the embedding size
    of the large model (256) however led to a degradation of performances.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenAI æ¨¡å‹èŒƒå›´çš„ä¸€è‡´æ€§**ã€‚å¤§ï¼ˆ3072ï¼‰ã€å°å‹å’Œé—ç•™ç‰ˆ OpenAI æ¨¡å‹çš„è¡¨ç°éå¸¸ç›¸ä¼¼ã€‚ç„¶è€Œï¼Œå‡å°‘å¤§æ¨¡å‹ï¼ˆ256ï¼‰çš„åµŒå…¥å¤§å°ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚'
- en: '**Language Sensitivity.** Almost all models (except ML-E5-large) performed
    best on English. Significant variations in performances were observed in languages
    like Czech and Hungarian.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è¯­è¨€æ•æ„Ÿæ€§**ã€‚å‡ ä¹æ‰€æœ‰æ¨¡å‹ï¼ˆé™¤äº† ML-E5-largeï¼‰åœ¨è‹±è¯­ä¸Šçš„è¡¨ç°æœ€å¥½ã€‚åœ¨æ·å…‹è¯­å’ŒåŒˆç‰™åˆ©è¯­ç­‰è¯­è¨€ä¸­ï¼Œæ€§èƒ½å·®å¼‚è¾ƒå¤§ã€‚'
- en: Should you therefore go for a paid OpenAI subscription, or for hosting an open-source
    embedding model?
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œä½ æ˜¯åº”è¯¥é€‰æ‹©ä»˜è´¹çš„ OpenAI è®¢é˜…ï¼Œè¿˜æ˜¯æ‰˜ç®¡ä¸€ä¸ªå¼€æºåµŒå…¥æ¨¡å‹ï¼Ÿ
- en: OpenAIâ€™s [recent price revision](https://openai.com/pricing) has made access
    to their API significantly more affordable, with the cost now standing at $0.13
    per million tokens. Dealing with one million queries per month (and assuming that
    each query involves around 1K token) would therefore cost on the order of $130\.
    Depending on your use case, it may therefore not be cost-effective to rent and
    maintain your own embedding server.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI çš„ [æœ€è¿‘ä»·æ ¼è°ƒæ•´](https://openai.com/pricing)ä½¿å¾—è®¿é—®å…¶ API æ›´åŠ å®æƒ ï¼Œç°åœ¨æ¯ç™¾ä¸‡ä¸ª token çš„è´¹ç”¨ä¸º
    $0.13ã€‚å¤„ç†æ¯æœˆç™¾ä¸‡æ¬¡æŸ¥è¯¢ï¼ˆå‡è®¾æ¯æ¬¡æŸ¥è¯¢å¤§çº¦æ¶‰åŠ 1K tokenï¼‰å¤§çº¦éœ€è¦ $130ã€‚æ ¹æ®ä½ çš„ä½¿ç”¨åœºæ™¯ï¼Œç§Ÿç”¨å’Œç»´æŠ¤è‡ªå·±çš„åµŒå…¥æœåŠ¡å™¨å¯èƒ½ä¸å…·æœ‰æˆæœ¬æ•ˆç›Šã€‚
- en: Cost-effectiveness is however not the sole consideration. Other factors such
    as latency, privacy, and control over data processing workflows may also need
    to be considered. Open-source models offer the advantage of complete data control,
    enhancing privacy and customization. On the other hand, latency issues have been
    observed with OpenAIâ€™s API, sometimes resulting in extended response times.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæˆæœ¬æ•ˆç›Šå¹¶éå”¯ä¸€çš„è€ƒè™‘å› ç´ ã€‚å…¶ä»–å› ç´ å¦‚å»¶è¿Ÿã€éšç§å’Œæ•°æ®å¤„ç†å·¥ä½œæµçš„æ§åˆ¶ä¹Ÿå¯èƒ½éœ€è¦è¢«è€ƒè™‘ã€‚å¼€æºæ¨¡å‹æä¾›äº†å®Œå…¨æ§åˆ¶æ•°æ®çš„ä¼˜åŠ¿ï¼Œä»è€Œå¢å¼ºäº†éšç§æ€§å’Œå®šåˆ¶æ€§ã€‚å¦ä¸€æ–¹é¢ï¼ŒOpenAI
    çš„ API å­˜åœ¨å»¶è¿Ÿé—®é¢˜ï¼Œæœ‰æ—¶ä¼šå¯¼è‡´å“åº”æ—¶é—´å»¶é•¿ã€‚
- en: In conclusion, the choice between open-source models and proprietary solutions
    like OpenAIâ€™s does not lend itself to a straightforward answer. Open-source embeddings
    present a compelling option, combining performance with greater control over data.
    Conversely, OpenAIâ€™s offerings may still appeal to those prioritizing convenience,
    especially if privacy concerns are secondary.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“æ¥è¯´ï¼Œé€‰æ‹©å¼€æºæ¨¡å‹è¿˜æ˜¯åƒ OpenAI è¿™æ ·çš„ä¸“æœ‰è§£å†³æ–¹æ¡ˆå¹¶ä¸æ˜¯ä¸€ä¸ªç®€å•çš„ç­”æ¡ˆã€‚å¼€æºåµŒå…¥æ¨¡å‹æä¾›äº†ä»¤äººä¿¡æœçš„é€‰æ‹©ï¼Œç»“åˆäº†æ€§èƒ½å’Œå¯¹æ•°æ®çš„æ›´å¤§æ§åˆ¶ã€‚ç›¸åï¼ŒOpenAI
    çš„äº§å“å¯èƒ½ä»ç„¶å¸å¼•é‚£äº›ä¼˜å…ˆè€ƒè™‘ä¾¿åˆ©æ€§çš„ç”¨æˆ·ï¼Œå°¤å…¶æ˜¯å½“éšç§é—®é¢˜ä¸æ˜¯ä¸»è¦å…³æ³¨ç‚¹æ—¶ã€‚
- en: Useful links
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœ‰ç”¨çš„é“¾æ¥
- en: 'Companion Github repository: [https://github.com/Yannael/multilingual-embeddings](https://github.com/Yannael/multilingual-embeddings)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é…å¥—çš„ Github ä»“åº“ï¼š[https://github.com/Yannael/multilingual-embeddings](https://github.com/Yannael/multilingual-embeddings)
- en: '[Everything you wanted to know about sentence embeddings (and maybe a bit more)](https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings/)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ä½ æƒ³çŸ¥é“çš„å…³äºå¥å­åµŒå…¥çš„ä¸€åˆ‡ï¼ˆä¹Ÿè®¸è¿˜æœ‰æ›´å¤šï¼‰](https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings/)'
- en: '[OpenAI blog announcement: New embedding models and API updates](https://openai.com/blog/new-embedding-models-and-api-updates)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenAI åšå®¢å…¬å‘Šï¼šæ–°çš„åµŒå…¥æ¨¡å‹å’Œ API æ›´æ–°](https://openai.com/blog/new-embedding-models-and-api-updates)'
- en: '[Embeddings: OpenAI guide](https://platform.openai.com/docs/guides/embeddings/embedding-models)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[åµŒå…¥ï¼šOpenAI æŒ‡å—](https://platform.openai.com/docs/guides/embeddings/embedding-models)'
- en: '[MTEB: Massive Text Embedding Benchmark](https://arxiv.org/abs/2210.07316)
    and [Hugging Face MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MTEB: å¤§è§„æ¨¡æ–‡æœ¬åµŒå…¥åŸºå‡†](https://arxiv.org/abs/2210.07316) å’Œ [Hugging Face MTEB æ’è¡Œæ¦œ](https://huggingface.co/spaces/mteb/leaderboard)'
- en: '[Text Embeddings: Comprehensive Guide](/text-embeddings-comprehensive-guide-afd97fce8fb5)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ–‡æœ¬åµŒå…¥ï¼šå…¨é¢æŒ‡å—](/text-embeddings-comprehensive-guide-afd97fce8fb5)'
- en: '[A Practitioners Guide to Retrieval Augmented Generation (RAG)](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å®è·µè€…æŒ‡å—ï¼šæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval)'
- en: '[How to Find the Best Multilingual Embedding Model for Your RAG](/how-to-find-the-best-multilingual-embedding-model-for-your-rag-40325c308ebb)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å¦‚ä½•ä¸ºä½ çš„ RAG æ‰¾åˆ°æœ€ä½³çš„å¤šè¯­è¨€åµŒå…¥æ¨¡å‹](/how-to-find-the-best-multilingual-embedding-model-for-your-rag-40325c308ebb)'
- en: 'Notes:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼š
- en: Unless otherwise noted, all images are by the author
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é™¤éå¦æœ‰è¯´æ˜ï¼Œæ‰€æœ‰å›¾ç‰‡å‡ç”±ä½œè€…æä¾›
- en: The EU AI act draft is published under the [Commissionâ€™s document reuse policy](https://eur-lex.europa.eu/content/legal-notice/legal-notice.html)
    based on [Decision 2011/833/EU](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A32011D0833),
    and can be re-used for commercial or non-commercial purposes.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ã€Šæ¬§ç›Ÿäººå·¥æ™ºèƒ½æ³•æ¡ˆè‰æ¡ˆã€‹æ ¹æ®[å§”å‘˜ä¼šçš„æ–‡ä»¶é‡ç”¨æ”¿ç­–](https://eur-lex.europa.eu/content/legal-notice/legal-notice.html)å‘å¸ƒï¼ŒåŸºäº[å†³å®š
    2011/833/EU](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A32011D0833)ï¼Œå¯ç”¨äºå•†ä¸šæˆ–éå•†ä¸šç›®çš„ã€‚
- en: '*Enjoyed this post? Share your thoughts, give it claps, or* [*connect with
    me on LinkedIn*](https://www.linkedin.com/in/yannaelb/)*.*'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*å–œæ¬¢è¿™ç¯‡æ–‡ç« å—ï¼Ÿåˆ†äº«ä½ çš„æƒ³æ³•ï¼Œç»™å®ƒç‚¹ä¸ªèµï¼Œæˆ–* [*åœ¨ LinkedIn ä¸Šä¸æˆ‘è”ç³»*](https://www.linkedin.com/in/yannaelb/)*ã€‚*'
