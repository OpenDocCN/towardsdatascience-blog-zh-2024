- en: OpenAI vs Open-Source Multilingual Embedding Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI 与开源多语言嵌入模型
- en: 原文：[https://towardsdatascience.com/openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05?source=collection_archive---------0-----------------------#2024-02-24](https://towardsdatascience.com/openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05?source=collection_archive---------0-----------------------#2024-02-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05?source=collection_archive---------0-----------------------#2024-02-24](https://towardsdatascience.com/openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05?source=collection_archive---------0-----------------------#2024-02-24)
- en: Choosing the model that works best for your data
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择最适合你数据的模型
- en: '[](https://ya-lb.medium.com/?source=post_page---byline--e5ccb7c90f05--------------------------------)[![Yann-Aël
    Le Borgne](../Images/acc1c8b32373d7f345064b89b51869fd.png)](https://ya-lb.medium.com/?source=post_page---byline--e5ccb7c90f05--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e5ccb7c90f05--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e5ccb7c90f05--------------------------------)
    [Yann-Aël Le Borgne](https://ya-lb.medium.com/?source=post_page---byline--e5ccb7c90f05--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ya-lb.medium.com/?source=post_page---byline--e5ccb7c90f05--------------------------------)[![Yann-Aël
    Le Borgne](../Images/acc1c8b32373d7f345064b89b51869fd.png)](https://ya-lb.medium.com/?source=post_page---byline--e5ccb7c90f05--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e5ccb7c90f05--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e5ccb7c90f05--------------------------------)
    [Yann-Aël Le Borgne](https://ya-lb.medium.com/?source=post_page---byline--e5ccb7c90f05--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e5ccb7c90f05--------------------------------)
    ·12 min read·Feb 24, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e5ccb7c90f05--------------------------------)
    ·阅读时间12分钟·2024年2月24日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d4106437a23e5edaba980ce8486937d9.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d4106437a23e5edaba980ce8486937d9.png)'
- en: We’ll use the EU AI act as the data corpus for our embedding model comparison.
    Image by Dall-E 3.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用欧盟人工智能法案作为我们的嵌入模型比较的数据语料库。图像由Dall-E 3生成。
- en: 'OpenAI recently released their new generation of embedding models, called *embedding
    v3*, which they [describe](https://openai.com/blog/new-embedding-models-and-api-updates)
    as their most performant embedding models, with higher multilingual performances.
    The models come in two classes: a smaller one called `text-embedding-3-small`,
    and a larger and more powerful one called `text-embedding-3-large`.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI最近发布了他们的新一代嵌入模型，称为*embedding v3*，他们[描述](https://openai.com/blog/new-embedding-models-and-api-updates)这些模型是性能最强的嵌入模型，具有更高的多语言性能。该模型有两种类型：一种较小的，称为`text-embedding-3-small`，另一种较大且更强大的，称为`text-embedding-3-large`。
- en: Very little information was disclosed concerning the way these models were designed
    and trained. As their previous embedding model release (December 2022 with the
    ada-002 model class), OpenAI again chooses a closed-source approach where the
    models may only be accessed through a paid API.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这些模型的设计和训练方式，公开的信息非常少。与他们之前的嵌入模型发布（2022年12月，ada-002模型类）一样，OpenAI再次选择了一种闭源的方法，模型只能通过付费API访问。
- en: But are the performances so good that they make it worth paying?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，性能真的那么好吗，值得付费吗？
- en: '**The motivation for this post is to empirically compare the performances of
    these new models with their open-source counterparts**. We’ll rely on a data retrieval
    workflow, where the most relevant documents in a corpus have to be found given
    a user query.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**这篇文章的动机是通过实证比较这些新模型与它们的开源对手的表现**。我们将依赖一个数据检索工作流，在该工作流中，必须根据用户查询找到语料库中最相关的文档。'
- en: Our corpus will be the [European AI Act](https://artificialintelligenceact.eu/),
    which is currently in its final stages of validation. An interesting characteristic
    of this corpus, besides being the first-ever legal framework on AI worldwide,
    is its availability in 24 languages. This makes it possible to compare the accuracy
    of data retrieval **across different families of languages**.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的语料库将是[欧洲人工智能法案](https://artificialintelligenceact.eu/)，该法案目前正处于最终验证阶段。这个语料库的一个有趣特点是，除了它是全球首个关于人工智能的法律框架外，它还提供24种语言版本。这使得我们能够比较**不同语言家族之间的数据检索准确性**。
- en: 'The post will go through the two main following steps:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将涵盖以下两个主要步骤：
- en: Generate a custom synthetic question/answer dataset from a multilingual text
    corpus
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从多语言文本语料库中生成一个定制的合成问答数据集
- en: Compare the accuracy of OpenAI and state-of-the-art open-source embedding models
    on this custom dataset.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较OpenAI与最先进的开源嵌入模型在此自定义数据集上的准确性。
- en: The code and data to reproduce the results presented in this post are made available
    in [this Github repository](https://github.com/Yannael/multilingual-embeddings).
    Note that the EU AI Act is used as an example, and the methodology followed in
    this post can be adapted to other data corpus.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了重现本文中展示的结果，代码和数据已经公开在[这个Github仓库](https://github.com/Yannael/multilingual-embeddings)中。请注意，《欧盟人工智能法案》作为示例使用，本文中遵循的方法可以适应其他数据语料库。
- en: Generate a custom Q/A dataset
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成自定义的问答数据集
- en: Let us first start by generating a dataset of questions and answers (Q/A) on
    custom data, which will be used to assess the performance of different embedding
    models. The benefits of generating a custom Q/A dataset are twofold. First, it
    avoids biases by ensuring that the dataset has not been part of the training of
    an embedding model, which may happen on reference benchmarks such as [MTEB](https://huggingface.co/spaces/mteb/leaderboard).
    Second, it allows to tailor the assessment to a specific corpus of data, which
    can be relevant in the case of retrieval augmented applications (RAG) for example.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先开始生成自定义数据上的问答数据集（Q/A），用于评估不同嵌入模型的性能。生成自定义问答数据集有两个好处。首先，它避免了偏见，确保数据集未参与嵌入模型的训练，这种情况可能发生在参考基准上，如[MTEB](https://huggingface.co/spaces/mteb/leaderboard)。其次，它使得评估可以根据特定数据语料库进行定制，这在检索增强应用（RAG）等情况下尤为重要。
- en: 'We will follow the simple process suggested by [Llama Index in their documentation](https://blog.llamaindex.ai/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971).
    The corpus is first split into a set of chunks. Then, for each chunk, a set of
    synthetic questions are generated by means of a large language model (LLM), such
    that the answer lies in the corresponding chunk. The process is illustrated below:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遵循[Llama Index文档中](https://blog.llamaindex.ai/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971)建议的简单流程。首先将语料库拆分成多个块。然后，对于每个块，使用大型语言模型（LLM）生成一组合成问题，使得问题的答案位于相应的块中。该过程如下图所示：
- en: '![](../Images/debcd01ca6179cf42bb71ec7c684627d.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/debcd01ca6179cf42bb71ec7c684627d.png)'
- en: Generating a question/answer dataset for your data, methodology from [Llama
    Index](https://blog.llamaindex.ai/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为你的数据生成问答数据集，方法参考[Llama Index](https://blog.llamaindex.ai/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971)
- en: Implementing this strategy is straightforward with a data framework for LLM
    such as Llama Index. The loading of the corpus and splitting of text can be conveniently
    carried out using high-level functions, as illustrated with the following code.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这个策略在使用像Llama Index这样的LLM数据框架时非常直接。语料库的加载和文本的拆分可以通过高阶函数方便地完成，如下面的代码所示。
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this example, the corpus is the EU AI Act in English, taken directly from
    the Web using this [official URL](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206).
    We use the draft version from April 2021, as the final version is not yet available
    for all European languages. In this version, English language can be replaced
    in the URL by any of the 23 other EU official languages to retrieve the text in
    a different language (BG for Bulgarian, ES for Spanish, CS for Czech, and so forth).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，语料库是《欧盟人工智能法案》的英文版，直接从网络上获取，使用的是这个[官方网址](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206)。我们使用的是2021年4月的草案版本，因为最终版本尚未提供所有欧盟语言。在这个版本中，网址中的英文可以替换为其他23种欧盟官方语言中的任何一种，以获取不同语言的文本（例如，BG代表保加利亚语，ES代表西班牙语，CS代表捷克语，等等）。
- en: '![](../Images/b228519bd3b36c7cec689ffd01b3226c.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b228519bd3b36c7cec689ffd01b3226c.png)'
- en: Download links to the EU AI Act for the 24 official EU languages (from [EU official
    website](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206))
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 下载《欧盟人工智能法案》在24种官方欧盟语言中的链接（来自[欧盟官网](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206)）
- en: We use the SentenceSplitter object to split the document in chunks of 1000 tokens.
    For English, this results in about 100 chunks.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用SentenceSplitter对象将文档拆分成1000个标记的块。对于英文文本，这大约会生成100个块。
- en: 'Each chunk is then provided as context to the following prompt ([the default
    prompt suggested in the Llama Index library](https://github.com/run-llama/llama_index/blob/c058f2531ea86ee74822cb1421ceaeee7098a99f/llama_index/finetuning/embeddings/common.py#L51)):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，每个文档块作为上下文提供给以下提示（[Llama Index库中建议的默认提示](https://github.com/run-llama/llama_index/blob/c058f2531ea86ee74822cb1421ceaeee7098a99f/llama_index/finetuning/embeddings/common.py#L51)）：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The prompt aims at generating questions about the document chunk, as if a teacher
    were preparing an upcoming quiz. The number of questions to generate for each
    chunk is passed as the parameter ‘num_questions_per_chunk’, which we set to two.
    Questions can then be generated by calling the generate_qa_embedding_pairs from
    the Llama Index library:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 该提示的目的是生成关于文档块的问题，就像老师在准备即将到来的小测验一样。每个文档块要生成的问题数量通过参数‘num_questions_per_chunk’传递，我们将其设置为两个。然后，可以通过调用Llama
    Index库中的generate_qa_embedding_pairs来生成问题：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We rely for this task on the GPT-3.5-turbo-0125 mode from OpenAI, which is according
    to OpenAI the flagship model of this family, supporting a 16K context window and
    optimized for dialog ([https://platform.openai.com/docs/models/gpt-3-5-turbo](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fplatform.openai.com%2Fdocs%2Fmodels%2Fgpt-3-5-turbo)).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在此任务中依赖于OpenAI的GPT-3.5-turbo-0125模型，根据OpenAI的说法，这是该系列的旗舰模型，支持16K的上下文窗口，并针对对话进行了优化（[https://platform.openai.com/docs/models/gpt-3-5-turbo](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fplatform.openai.com%2Fdocs%2Fmodels%2Fgpt-3-5-turbo)）。
- en: 'The resulting objet ‘qa_dataset’ contains the questions and answers (chunks)
    pairs. As an example of generated questions, here is the result for the first
    two questions (for which the ‘answer’ is the first chunk of text):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的对象‘qa_dataset’包含问题和答案（文档块）对。作为生成问题的示例，以下是前两个问题的结果（其中‘答案’是第一个文档块的文本）：
- en: 1) What are the main objectives of the proposal for a Regulation laying down
    harmonised rules on artificial intelligence (Artificial Intelligence Act) according
    to the explanatory memorandum?
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 1) 根据说明性备忘录，关于人工智能的统一规则提案（人工智能法案）的主要目标是什么？
- en: 2) How does the proposal for a Regulation on artificial intelligence aim to
    address the risks associated with the use of AI while promoting the uptake of
    AI in the European Union, as outlined in the context information?
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 2) 根据上下文信息，关于人工智能的统一规则提案如何在促进人工智能在欧盟的应用的同时，解决与使用人工智能相关的风险？
- en: The number of chunks and questions depends on the language, ranging from around
    100 chunks and 200 questions for English, to 200 chunks and 400 questions for
    Hungarian.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 文档块和问题的数量取决于语言，从英语的约100个文档块和200个问题，到匈牙利语的200个文档块和400个问题不等。
- en: Evaluation of OpenAI embedding models
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI嵌入模型的评估
- en: Our evaluation function follows the [Llama Index documentation](https://docs.llamaindex.ai/en/stable/examples/finetuning/embeddings/finetune_embedding.html)
    and consists in two main steps. First, the embeddings for all answers (document
    chunks) are stored in a VectorStoreIndex for efficient retrieval. Then, the evaluation
    function loops over all queries, retrieves the top k most similar documents, and
    the accuracy of the retrieval in assessed in terms of MRR ([Mean Reciprocal Rank](https://en.wikipedia.org/wiki/Mean_reciprocal_rank)).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的评估函数遵循[Llama Index文档](https://docs.llamaindex.ai/en/stable/examples/finetuning/embeddings/finetune_embedding.html)，由两个主要步骤组成。首先，所有答案（文档块）的嵌入存储在一个VectorStoreIndex中，以便高效检索。然后，评估函数循环遍历所有查询，检索最相似的前k个文档，并通过MRR（[平均倒数排名](https://en.wikipedia.org/wiki/Mean_reciprocal_rank)）来评估检索的准确性。
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The embedding model is passed to the evaluation function by means of the `embed_model`
    argument, which for OpenAI models is an OpenAIEmbedding object initialised with
    the name of the model, and the model dimension.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入模型通过`embed_model`参数传递给评估函数，对于OpenAI模型来说，这是一个初始化了模型名称和模型维度的OpenAIEmbedding对象。
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `dimensions` API parameter can shorten embeddings (i.e. remove some numbers
    from the end of the sequence) without the embedding losing its concept-representing
    properties. OpenAI for example suggests [in their annoucement](https://openai.com/blog/new-embedding-models-and-api-updates)
    that on the MTEB benchmark, an embedding can be shortened to a size of 256 while
    still outperforming an unshortened `text-embedding-ada-002` embedding with a size
    of 1536.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`dimensions` API参数可以缩短嵌入（即移除序列末尾的一些数字），而不会失去其表示概念的特性。OpenAI例如在它们的[公告](https://openai.com/blog/new-embedding-models-and-api-updates)中建议，在MTEB基准测试中，嵌入可以缩短到256的大小，同时仍然优于未缩短的`text-embedding-ada-002`嵌入（其大小为1536）。'
- en: 'We ran the evaluation function on four different OpenAI embedding models:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在四个不同的OpenAI嵌入模型上运行了评估函数：
- en: 'two versions of `text-embedding-3-large` : one with the lowest possible dimension
    (256), and the other one with the highest possible dimension (3072). These are
    called ‘OAI-large-256’ and ‘OAI-large-3072’.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个版本的`text-embedding-3-large`：一个是最低维度（256），另一个是最高维度（3072）。它们分别称为‘OAI-large-256’和‘OAI-large-3072’。
- en: 'OAI-small: The `text-embedding-3-small` embedding model, with a dimension of
    1536.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OAI-small：`text-embedding-3-small`嵌入模型，维度为1536。
- en: 'OAI-ada-002: The legacy `text-embedding-ada-002` model, with a dimension of
    1536.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OAI-ada-002：传统的`text-embedding-ada-002`模型，维度为1536。
- en: 'Each model was evaluated on four different languages: English (EN), French
    (FR), Czech (CS) and Hungarian (HU), covering examples of Germanic, Romance, Slavic
    and Uralic language, respectively.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型都在四种不同的语言上进行了评估：英语（EN）、法语（FR）、捷克语（CS）和匈牙利语（HU），分别涵盖了日耳曼语系、罗曼语系、斯拉夫语系和乌拉尔语系的示例。
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The resulting accuracy in terms of MRR is reported below:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 结果中的准确度（以MRR衡量）如下所示：
- en: '![](../Images/e85932db410223b301977602a81afcd7.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e85932db410223b301977602a81afcd7.png)'
- en: Summary of performances for the OpenAI models
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI模型表现的总结
- en: As expected, for the large model, better performances are observed with the
    larger embedding size of 3072\. Compared with the small and legacy Ada models,
    the large model is however smaller than we would have expected. For comparison,
    we also report below the performances obtained by the OpenAI models on the MTEB
    benchmark.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，对于大型模型，随着嵌入大小增大到3072，表现有所提升。与小型和传统Ada模型相比，大型模型的表现虽有所提升，但依然比我们预期的要小。为进行比较，我们还在下方报告了OpenAI模型在MTEB基准测试中的表现。
- en: '![](../Images/d7f95f6dd6fc1fa806136b1f0a8236bd.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d7f95f6dd6fc1fa806136b1f0a8236bd.png)'
- en: Performances of OpenAI embedding models, as reported in their [official announcement](https://openai.com/blog/new-embedding-models-and-api-updates)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI嵌入模型的表现，详见它们的[官方公告](https://openai.com/blog/new-embedding-models-and-api-updates)。
- en: It is interesting to note that the differences in performances between the large,
    small and Ada models are much less pronounced in our assessment than in the MTEB
    benchmark, reflecting the fact that the average performances observed in large
    benchmarks do not necessarily reflect those obtained on custom datasets.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，在我们的评估中，大型、小型和Ada模型之间的表现差异，比MTEB基准测试中观察到的差异要小，这反映了一个事实，即在大型基准测试中观察到的平均表现，并不一定能反映在定制数据集上获得的结果。
- en: Evaluation of open-source embedding models
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开源嵌入模型的评估
- en: The open-source research around embeddings is quite active, and new models are
    regularly published. A good place to keep updated about the latest published models
    is the [Hugging Face 😊 MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 关于嵌入的开源研究非常活跃，新的模型定期发布。一个不错的保持最新发布模型的地方是[Hugging Face 😊 MTEB排行榜](https://huggingface.co/spaces/mteb/leaderboard)。
- en: For the comparison in this article, we selected a set of four embedding models
    recently published (2024). The criteria for selection were their average score
    on the MTEB leaderboard and their ability to deal with multilingual data. A summary
    of the main characteristics of the selected models are reported below.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中的比较，我们选择了一组最近发布的四种嵌入模型（2024年）。选择标准是它们在MTEB排行榜上的平均得分以及处理多语言数据的能力。以下是所选模型的主要特征总结。
- en: Selected open-source embedding models
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 所选的开源嵌入模型
- en: '[***E5-Mistral-7B-instruct***](https://huggingface.co/intfloat/e5-mistral-7b-instruct)(E5-mistral-7b):
    This E5 embedding model by Microsoft is initialized from [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)
    and fine-tuned on a mixture of multilingual datasets. The model performs best
    on the MTEB leaderboard, but is also by far the biggest one (14GB).'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[***E5-Mistral-7B-instruct***](https://huggingface.co/intfloat/e5-mistral-7b-instruct)(E5-mistral-7b)：微软的这款E5嵌入模型是从[Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)初始化的，并在多语言数据集的混合上进行微调。该模型在MTEB排行榜上表现最佳，但也是最大的模型（14GB）。'
- en: '[***multilingual-e5-large-instruct***](https://huggingface.co/intfloat/multilingual-e5-large-instruct)(ML-E5-large):
    Another E5 model from Microsoft, meant to better handle multilingual data. It
    is initialized from [xlm-roberta-large](https://huggingface.co/xlm-roberta-large)
    and trained on a mixture of multilingual datasets. It is much smaller (10 times)
    than E5-Mistral, but also has a much lower context size (514).'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[***multilingual-e5-large-instruct***](https://huggingface.co/intfloat/multilingual-e5-large-instruct)(ML-E5-large)：微软的另一款E5模型，旨在更好地处理多语言数据。它是从[xlm-roberta-large](https://huggingface.co/xlm-roberta-large)初始化的，并在多语言数据集的混合上进行训练。它比E5-Mistral小得多（小10倍），但上下文大小也更小（514）。'
- en: '[***BGE-M3***](https://huggingface.co/BAAI/bge-m3): The model was designed
    by the Beijing Academy of Artificial Intelligence, and is their state-of-the-art
    embedding model for multilingual data, supporting more than 100 working languages.
    It was not yet benchmarked on the MTEB leaderboard as of 22/02/2024.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[***BGE-M3***](https://huggingface.co/BAAI/bge-m3)：该模型由北京人工智能研究院设计，是其针对多语言数据的最先进嵌入模型，支持100多种工作语言。截至2024年2月22日，它尚未在MTEB排行榜上进行基准测试。'
- en: '[***nomic-embed-text-v1***](https://huggingface.co/nomic-ai/nomic-embed-text-v1)(Nomic-Embed):
    The model was designed by [Nomic](https://home.nomic.ai/), and claims better performances
    than OpenAI Ada-002 and text-embedding-3-small while being only 0.55GB in size.
    Interestingly, the model is the first to be fully reproducible and auditable (open
    data and open-source training code).'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[***nomic-embed-text-v1***](https://huggingface.co/nomic-ai/nomic-embed-text-v1)(Nomic-Embed)：该模型由[Nomic](https://home.nomic.ai/)设计，声称在性能上优于OpenAI
    Ada-002和text-embedding-3-small，同时仅为0.55GB大小。有趣的是，该模型是第一个完全可复现和可审计的模型（开放数据和开源训练代码）。'
- en: 'The code for evaluating these open-source models is similar to the code used
    for OpenAI models. The main change lies in the model specifications, where additional
    details such as maximum context length and pooling types have to be specified.
    We then evaluate each model for each of the four languages:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 评估这些开源模型的代码与用于OpenAI模型的代码类似。主要的变化在于模型规格，必须指定附加的细节，如最大上下文长度和池化类型。然后，我们对四种语言中的每种模型进行了评估：
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The resulting accuracies in terms of MRR are reported below.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 下面报告了以MRR为标准的准确度结果。
- en: '![](../Images/f77ab24c4d15f7334aa730420a285de6.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f77ab24c4d15f7334aa730420a285de6.png)'
- en: Summary of performances for the open-source models
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 开源模型的性能总结
- en: BGE-M3 turns out to provide the best performances, followed on average by ML-E5-Large,
    E5-mistral-7b and Nomic-Embed. BGE-M3 model is not yet benchmarked on the MTEB
    leaderboard, and our results indicate that it could rank higher than other models.
    It is also interesting to note that while BGE-M3 is optimized for multilingual
    data, it also performs better for English than the other models.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: BGE-M3表现最佳，平均表现紧随其后的是ML-E5-Large、E5-mistral-7b和Nomic-Embed。BGE-M3模型尚未在MTEB排行榜上进行基准测试，我们的结果表明它可能会排在其他模型之前。有趣的是，虽然BGE-M3是针对多语言数据进行优化的，但它在英语上的表现优于其他模型。
- en: We additionally report the processing times for each embedding model below.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还报告了每个嵌入模型的处理时间。
- en: '![](../Images/026e18ce209b4a9150cc60b529e46e89.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/026e18ce209b4a9150cc60b529e46e89.png)'
- en: Processing times in seconds for going throught the English Q/A dataset
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通过英文问答数据集的处理时间（以秒为单位）
- en: The E5-mistral-7b, which is more than 10 times larger than the other models,
    is without surprise by far the slowest model.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: E5-mistral-7b比其他模型大10倍以上，毫不意外地是最慢的模型。
- en: Conclusion
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Let us put side-by-side of the performance of the eight tested models in a single
    figure.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这八个测试模型的性能并排展示在一个图中。
- en: '![](../Images/2098db260ef495cac46c32a79518bcc3.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2098db260ef495cac46c32a79518bcc3.png)'
- en: Summary of performances for the eight tested models
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 八个测试模型的性能总结
- en: 'The key observations from these results are:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些结果中可以得出以下关键观察：
- en: '**Best performances were obtained by open-source models**. The [BGE-M3](https://huggingface.co/BAAI/bge-m3)
    model, developed by the Beijing Academy of Artificial Intelligence, emerged as
    the top performer. The model has the same context length as OpenAI models (8K),
    for a size of 2.2GB.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开源模型的表现最好**。由北京人工智能学会开发的 [BGE-M3](https://huggingface.co/BAAI/bge-m3) 模型表现最佳。该模型的上下文长度与
    OpenAI 模型相同（8K），大小为 2.2GB。'
- en: '**Consistency Across OpenAI’s Range**. The performances of the large (3072),
    small and legacy OpenAI models were very similar. Reducing the embedding size
    of the large model (256) however led to a degradation of performances.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenAI 模型范围的一致性**。大（3072）、小型和遗留版 OpenAI 模型的表现非常相似。然而，减少大模型（256）的嵌入大小会导致性能下降。'
- en: '**Language Sensitivity.** Almost all models (except ML-E5-large) performed
    best on English. Significant variations in performances were observed in languages
    like Czech and Hungarian.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言敏感性**。几乎所有模型（除了 ML-E5-large）在英语上的表现最好。在捷克语和匈牙利语等语言中，性能差异较大。'
- en: Should you therefore go for a paid OpenAI subscription, or for hosting an open-source
    embedding model?
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你是应该选择付费的 OpenAI 订阅，还是托管一个开源嵌入模型？
- en: OpenAI’s [recent price revision](https://openai.com/pricing) has made access
    to their API significantly more affordable, with the cost now standing at $0.13
    per million tokens. Dealing with one million queries per month (and assuming that
    each query involves around 1K token) would therefore cost on the order of $130\.
    Depending on your use case, it may therefore not be cost-effective to rent and
    maintain your own embedding server.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 的 [最近价格调整](https://openai.com/pricing)使得访问其 API 更加实惠，现在每百万个 token 的费用为
    $0.13。处理每月百万次查询（假设每次查询大约涉及 1K token）大约需要 $130。根据你的使用场景，租用和维护自己的嵌入服务器可能不具有成本效益。
- en: Cost-effectiveness is however not the sole consideration. Other factors such
    as latency, privacy, and control over data processing workflows may also need
    to be considered. Open-source models offer the advantage of complete data control,
    enhancing privacy and customization. On the other hand, latency issues have been
    observed with OpenAI’s API, sometimes resulting in extended response times.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，成本效益并非唯一的考虑因素。其他因素如延迟、隐私和数据处理工作流的控制也可能需要被考虑。开源模型提供了完全控制数据的优势，从而增强了隐私性和定制性。另一方面，OpenAI
    的 API 存在延迟问题，有时会导致响应时间延长。
- en: In conclusion, the choice between open-source models and proprietary solutions
    like OpenAI’s does not lend itself to a straightforward answer. Open-source embeddings
    present a compelling option, combining performance with greater control over data.
    Conversely, OpenAI’s offerings may still appeal to those prioritizing convenience,
    especially if privacy concerns are secondary.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，选择开源模型还是像 OpenAI 这样的专有解决方案并不是一个简单的答案。开源嵌入模型提供了令人信服的选择，结合了性能和对数据的更大控制。相反，OpenAI
    的产品可能仍然吸引那些优先考虑便利性的用户，尤其是当隐私问题不是主要关注点时。
- en: Useful links
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有用的链接
- en: 'Companion Github repository: [https://github.com/Yannael/multilingual-embeddings](https://github.com/Yannael/multilingual-embeddings)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配套的 Github 仓库：[https://github.com/Yannael/multilingual-embeddings](https://github.com/Yannael/multilingual-embeddings)
- en: '[Everything you wanted to know about sentence embeddings (and maybe a bit more)](https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings/)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[你想知道的关于句子嵌入的一切（也许还有更多）](https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings/)'
- en: '[OpenAI blog announcement: New embedding models and API updates](https://openai.com/blog/new-embedding-models-and-api-updates)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenAI 博客公告：新的嵌入模型和 API 更新](https://openai.com/blog/new-embedding-models-and-api-updates)'
- en: '[Embeddings: OpenAI guide](https://platform.openai.com/docs/guides/embeddings/embedding-models)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[嵌入：OpenAI 指南](https://platform.openai.com/docs/guides/embeddings/embedding-models)'
- en: '[MTEB: Massive Text Embedding Benchmark](https://arxiv.org/abs/2210.07316)
    and [Hugging Face MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MTEB: 大规模文本嵌入基准](https://arxiv.org/abs/2210.07316) 和 [Hugging Face MTEB 排行榜](https://huggingface.co/spaces/mteb/leaderboard)'
- en: '[Text Embeddings: Comprehensive Guide](/text-embeddings-comprehensive-guide-afd97fce8fb5)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文本嵌入：全面指南](/text-embeddings-comprehensive-guide-afd97fce8fb5)'
- en: '[A Practitioners Guide to Retrieval Augmented Generation (RAG)](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[实践者指南：检索增强生成（RAG）](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval)'
- en: '[How to Find the Best Multilingual Embedding Model for Your RAG](/how-to-find-the-best-multilingual-embedding-model-for-your-rag-40325c308ebb)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何为你的 RAG 找到最佳的多语言嵌入模型](/how-to-find-the-best-multilingual-embedding-model-for-your-rag-40325c308ebb)'
- en: 'Notes:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：
- en: Unless otherwise noted, all images are by the author
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图片均由作者提供
- en: The EU AI act draft is published under the [Commission’s document reuse policy](https://eur-lex.europa.eu/content/legal-notice/legal-notice.html)
    based on [Decision 2011/833/EU](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A32011D0833),
    and can be re-used for commercial or non-commercial purposes.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《欧盟人工智能法案草案》根据[委员会的文件重用政策](https://eur-lex.europa.eu/content/legal-notice/legal-notice.html)发布，基于[决定
    2011/833/EU](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A32011D0833)，可用于商业或非商业目的。
- en: '*Enjoyed this post? Share your thoughts, give it claps, or* [*connect with
    me on LinkedIn*](https://www.linkedin.com/in/yannaelb/)*.*'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*喜欢这篇文章吗？分享你的想法，给它点个赞，或* [*在 LinkedIn 上与我联系*](https://www.linkedin.com/in/yannaelb/)*。*'
