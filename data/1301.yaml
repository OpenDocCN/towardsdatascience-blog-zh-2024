- en: Additive Decision Trees
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加性决策树
- en: 原文：[https://towardsdatascience.com/additive-decision-trees-85f2feda2223?source=collection_archive---------4-----------------------#2024-05-24](https://towardsdatascience.com/additive-decision-trees-85f2feda2223?source=collection_archive---------4-----------------------#2024-05-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/additive-decision-trees-85f2feda2223?source=collection_archive---------4-----------------------#2024-05-24](https://towardsdatascience.com/additive-decision-trees-85f2feda2223?source=collection_archive---------4-----------------------#2024-05-24)
- en: An interpretable classification and regression model
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可解释的分类和回归模型
- en: '[](https://medium.com/@wkennedy934?source=post_page---byline--85f2feda2223--------------------------------)[![W
    Brett Kennedy](../Images/b3ce55ffd028167326c117d47c64c467.png)](https://medium.com/@wkennedy934?source=post_page---byline--85f2feda2223--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--85f2feda2223--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--85f2feda2223--------------------------------)
    [W Brett Kennedy](https://medium.com/@wkennedy934?source=post_page---byline--85f2feda2223--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@wkennedy934?source=post_page---byline--85f2feda2223--------------------------------)[![W
    Brett Kennedy](../Images/b3ce55ffd028167326c117d47c64c467.png)](https://medium.com/@wkennedy934?source=post_page---byline--85f2feda2223--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--85f2feda2223--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--85f2feda2223--------------------------------)
    [W Brett Kennedy](https://medium.com/@wkennedy934?source=post_page---byline--85f2feda2223--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--85f2feda2223--------------------------------)
    ·20 min read·May 24, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--85f2feda2223--------------------------------)
    ·阅读时间 20 分钟·2024年5月24日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: This article is part of a series related to interpretable predictive models,
    in this case covering a model type called Additive Decision Trees. The previous
    described [ikNN](/interpretable-knn-iknn-33d38402b8fc), an interpretable variation
    of kNN models, based on ensembles of 2d kNNs.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本文是与可解释预测模型相关的系列文章的一部分，这篇文章介绍了名为加性决策树的模型类型。前一篇文章描述了[ikNN](/interpretable-knn-iknn-33d38402b8fc)，它是kNN模型的一种可解释变体，基于二维kNN的集成方法。
- en: Additive Decision Trees are a variation of standard decision trees, constructed
    in similar manner, but in a way that can often allow them to be more accurate,
    more interpretable, or both. They include some nodes that are somewhat more complex
    than standard decision tree nodes (though usually just slightly), but can be constructed
    with, often, far fewer nodes, allowing for more comprehensible trees overall.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 加性决策树是标准决策树的一种变体，构造方式类似，但通常可以使它们更准确、更易解释，或者两者兼具。它们包含一些比标准决策树节点稍微复杂的节点（尽管通常只是略微复杂），但往往可以通过更少的节点构造，从而使整体树更加易于理解。
- en: 'The main project is: [https://github.com/Brett-Kennedy/AdditiveDecisionTree](https://github.com/Brett-Kennedy/AdditiveDecisionTree).
    Both AdditiveDecitionTreeClassifier and AdditiveDecisionTreeRegressor classes
    are provided.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 主要项目是：[https://github.com/Brett-Kennedy/AdditiveDecisionTree](https://github.com/Brett-Kennedy/AdditiveDecisionTree)。提供了加性决策树分类器（AdditiveDecitionTreeClassifier）和加性决策树回归器（AdditiveDecisionTreeRegressor）类。
- en: Additive Decision Trees were motivated by the lack of options for interpretable
    classification and regression models available. Interpretable models are desirable
    in a number of scenarios, including high-stakes environments, audited environments
    (where we must understand well how the models behave), cases where we must ensure
    the models are not biased against protected classes (for example, discriminating
    based on race or gender), among other places.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 加性决策树的提出源于缺乏可解释的分类和回归模型。可解释模型在许多场景中都很重要，包括高风险环境、审计环境（我们必须清楚地理解模型如何行为）、以及确保模型不对保护类群体存在偏见的情况（例如，基于种族或性别进行歧视）等。
- en: As covered in the article on [ikNN](/interpretable-knn-iknn-33d38402b8fc), there
    are some options available for interpretable classifiers and regression models
    (such as standard decision trees, rule lists, rule sets, linear/logistic regression
    and a small number of others), but far fewer than one may wish.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如在关于[ikNN](/interpretable-knn-iknn-33d38402b8fc)的文章中所述，目前有一些可解释的分类器和回归模型（如标准决策树、规则列表、规则集、线性/逻辑回归等少数几种），但远少于理想情况下的数量。
- en: Standard Decision Trees
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标准决策树
- en: One of the most commonly-used interpretable models is the decision tree. It
    often works well, but not in all cases. They may not always achieve a sufficient
    level of accuracy, and where they do, they may not always be reasonably considered
    interpretable.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的可解释模型之一是决策树。它通常表现良好，但并非在所有情况下都如此。决策树可能无法始终达到足够的准确度，且在能够达到时，也可能无法合理地认为其是可解释的。
- en: Decision trees can often achieve high accuracy only when grown to large sizes,
    which eliminates any interpretability. A decision tree with five or six leaf nodes
    will be quite interpretable; a decision tree with a hundred leaf nodes is close
    to a black-box. Though arguably more interpretable than a neural network or boosted
    model, it becomes very difficult to fully make sense of the predictions of decision
    trees with very large numbers of leaf nodes, especially as each may be associated
    with quite long decision paths. This is the primary issue Additive Decision Trees
    were designed to address.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树通常只有在被构建为较大规模时才能达到较高的准确性，这会剥夺其可解释性。一个具有五六个叶节点的决策树是相当可解释的；但一个具有一百个叶节点的决策树几乎变成了黑箱。尽管与神经网络或提升模型相比，决策树可能更具可解释性，但当叶节点数量非常多时，完全理解决策树的预测就变得非常困难，尤其是每个叶节点可能都与相当长的决策路径相关联。这正是加法决策树设计的主要问题所在。
- en: Additive Decision Tree also addresses some other well-known limitations of decision
    trees, in particular their limited stability (small differences in the training
    data can result in quite different trees), their necessity to split based on fewer
    and fewer samples lower in the trees, repeated sub-trees, and their tendency to
    overfit if not restricted or pruned.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 加法决策树还解决了决策树的其他一些著名局限性，特别是它们的稳定性较差（训练数据中的小差异可能导致截然不同的树），它们在树的低层次基于越来越少的样本进行划分，需要重复子树，以及如果不加以限制或修剪，容易过拟合的问题。
- en: 'To consider closer the issue where splits are based on fewer and few samples
    lower in the trees: this is due to the nature of the splitting process used by
    decision trees; the dataspace is divided into separate regions at each split.
    The root node covers every record in the training data and each child node covers
    a portion of this. Each of their child nodes a portion of that and so on. Given
    this, splits lower in the tree become progressively less reliable.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步考虑在树的低层次上基于越来越少的样本进行划分的问题：这是由于决策树使用的划分过程的性质；数据空间在每次划分时被分为不同的区域。根节点涵盖了训练数据中的每一条记录，而每个子节点涵盖其中的一部分。它们的每个子节点又涵盖其中的一部分，依此类推。基于此，树的低层次划分变得越来越不可靠。
- en: These limitations are typically addressed by ensembling decision trees, either
    through bagging (as with Random Forests) or boosting (as with CatBoost, XGBoost,
    and LGBM). Ensembling results in uninterpretable, though generally more accurate,
    models. Other techniques to make decision trees more stable and accurate include
    constructing oblivious trees (this is done, for example, within CatBoost) and
    oblique decision trees (trees where the splits may be at oblique angles through
    the dataspace, as opposed to the axis-parallel splits that are normally used with
    decision trees).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这些局限性通常通过集成决策树来解决，可以通过袋装（例如随机森林）或提升（例如CatBoost、XGBoost和LGBM）实现。集成方法会产生不可解释的模型，尽管通常更准确。其他提高决策树稳定性和准确性的技术包括构建盲树（例如，CatBoost中就是这样做的）和斜决策树（即在数据空间中以斜角进行划分，而非决策树通常使用的轴平行划分）。
- en: As decision trees are likely the most, or among the most, commonly used models
    where interpretability is required, our comparisons, both in terms of accuracy
    and interpretability, are made with respect to standard decision trees.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于决策树可能是最常用的可解释模型，或者至少是最常用的模型之一，我们在准确性和可解释性方面的比较都是相对于标准决策树进行的。
- en: Introduction to Additive Decision Trees
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加法决策树简介
- en: Additive Decision Trees will not always perform preferably to decision trees,
    but will quite often, and are usually worth testing where an interpretable model
    is useful. In some cases, they may provide higher accuracy, in some cased improved
    interpretability, and in many cases, both. Testing to date suggests this is more
    true for classification than regression.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 加法决策树并不总是优于决策树，但它通常能表现得更好，且在需要可解释模型的情况下通常值得进行测试。在某些情况下，它们可能提供更高的准确性，在某些情况下提高了解释性，在许多情况下则两者兼具。迄今为止的测试表明，这对于分类任务比回归任务更为成立。
- en: Additive Decision Trees are not intended to be competitive with approaches such
    as boosting or neural networks in terms of accuracy, but are simply a tool to
    generate interpretable models. Their appeal is that they can often produce models
    comparable in accuracy to deeper standard decision trees, while having a lower
    overall complexity compared to these, very often considerably lower.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 加性决策树并不旨在与提升方法（boosting）或神经网络在准确度上竞争，而只是作为一种生成可解释模型的工具。它们的吸引力在于，它们通常能生成与更深层次的标准决策树相当的模型，同时相较于这些树，整体复杂度较低，通常大大较低。
- en: Intuition Behind Additive Decision Trees
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加性决策树背后的直觉
- en: The intuition behind Additive Decision Trees is that often the true function,
    *f(x)*, mapping the input x to the target y, is based on logical conditions (with
    IF-ELSE logic, or can be approximated with IF-ELSE logic); and in other cases
    it is simply a probabilistic function where each input feature may be considered
    somewhat independently (as with the Naive Bayes assumption).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 加性决策树背后的直觉是，通常真实的函数，*f(x)*，将输入x映射到目标y，是基于逻辑条件的（具有IF-ELSE逻辑，或者可以用IF-ELSE逻辑近似）；而在其他情况下，它只是一个概率函数，其中每个输入特征可能被相对独立地考虑（就像朴素贝叶斯假设一样）。
- en: 'The true f(x) can have different types of feature interactions: cases where
    the value of one feature affects how other features relate to the target. And
    these may be stronger or weaker in different datasets.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 真实的f(x)可能具有不同类型的特征交互：即一个特征的值影响其他特征如何与目标相关。这些交互在不同的数据集中可能更强或更弱。
- en: 'For example, the true f(x) may include something to the effect:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，真实的f(x)可能包含如下内容：
- en: '**True f(x) Example 1**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**真实的f(x)示例 1**'
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This is an example of the first case, where the true f(x) is composed of logical
    conditions and may be accurately (and in a simple manner) represented as a series
    of rules, such as in a Decision Tree (as below), Rule List, or Rule Set.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这是第一个案例的一个例子，其中真实的f(x)由逻辑条件组成，并且可以准确（且简单地）表示为一系列规则，例如决策树（如下所示）、规则列表或规则集。
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, a simple tree can be created to represent the rules related to features
    A and B.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，可以创建一个简单的树来表示与特征A和B相关的规则。
- en: But the rule related to C*D will generate a very large sub-tree, as the tree
    may only split based on either C or D at each step. For example, for values of
    C over 1.0, values of D over 44 will result in class Y. For values of C over 1.1,
    values of D over 40 will result in class Y. For values of C over 1.11, values
    over 39.64 will results in class Y. This must be calculated for all combinations
    of C and D to as fine a level of granularity as is possible given the size of
    the training data. The sub-tree may be accurate, but will be large, and will be
    close to incomprehensible.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，涉及到C*D的规则将生成一个非常大的子树，因为每次分裂时，树可能仅根据C或D之一进行分裂。例如，对于C值大于1.0时，D值大于44将导致类别为Y；对于C值大于1.1时，D值大于40将导致类别为Y；对于C值大于1.11时，D值大于39.64将导致类别为Y。必须为C和D的所有组合计算这些值，直到训练数据的规模允许的最细粒度为止。子树可能是准确的，但会非常庞大，几乎接近无法理解。
- en: 'On the other hand, the true f(x) may be a set of patterns related to probabilities,
    more of the form:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，真实的f(x)可能是与概率相关的一组模式，更像是以下的形式：
- en: '**True f(x) Example 2**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**真实的f(x)示例 2**'
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, the classes are predicted entirely based on probabilities related to each
    feature, with no feature interactions. In this form of function, for each instance,
    the feature values each contribute some probability to the target value and these
    probabilities are summed to determine the overall probability distribution.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，类别的预测完全基于与每个特征相关的概率，且没有特征之间的交互。在这种函数形式中，对于每个实例，每个特征值都会对目标值贡献一些概率，这些概率会被加总以确定整体的概率分布。
- en: 'Here, there is no simple tree that could be created. There are three target
    classes (X, Y, and Z). If f(x) were simpler, containing only a rule related to
    feature A:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，无法创建一个简单的树。这里有三个目标类别（X、Y和Z）。如果f(x)更简单，仅包含与特征A相关的规则：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We could create a small tree based on the split points in A where each of the
    three classes become most likely. This may require only a small number of nodes:
    the tree would likely first split A at roughly its midpoint, then each child node
    would split A in roughly half again and so on, until we have a tree where the
    nodes each indicate either X, Y, or Z as the most likely class.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以基于 A 中的分裂点创建一棵小树，其中每个类别的概率最大。这可能只需要少量的节点：这棵树可能首先在 A 的大约中点分裂，然后每个子节点会再次将
    A 分裂成大约一半，依此类推，直到我们得到一棵树，其中每个节点指示 X、Y 或 Z 是最可能的类别。
- en: But, given there are three such rules, it’s not clear which would be represented
    by splits first. If we, for example, split for feature B first, we need to handle
    the logic related to features A and C in each subtree (repeating the logic related
    to these multiple times in the trees). If we split first based on feature B, then
    feature A, then feature C, then when we determine the split points for feature
    C, we may have few enough records covered by the nodes that the split points are
    selected at sub-optimal points.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，由于有三条这样的规则，尚不清楚哪一条会首先通过分裂表示。如果我们首先按特征 B 进行分裂，我们需要在每个子树中处理与特征 A 和特征 C 相关的逻辑（在树中多次重复这些逻辑）。如果我们首先按特征
    B，然后按特征 A，再按特征 C 分裂，那么当我们确定特征 C 的分裂点时，可能会有足够少的记录被节点覆盖，以至于分裂点会选择在次优的点上。
- en: Example 2 could likely (with enough training data) be represented by a decision
    tree with reasonably high accuracy, but the tree would be quite large, and the
    splits would not likely be intelligible. Lower and lower in the tree, the split
    points become less and less comprehensible, as they’re simply the split points
    in one of the three relevant features that best split the data given the progressively
    less training data in each lower node.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 2 可能（在足够的训练数据下）可以通过决策树以相当高的准确率表示，但这棵树会相当大，且分裂点可能不太容易理解。树的下层，分裂点变得越来越难以理解，因为它们仅仅是在三个相关特征中最能划分数据的分裂点，而每个下层节点的训练数据越来越少。
- en: 'In Example 3, we have a similar f(x), but with some feature interactions in
    the form of conditions and multiplication:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例 3 中，我们有一个类似的 f(x)，但其中包含一些特征交互，形式为条件和乘法：
- en: '**True f(x) Example 3**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**真实的 f(x) 示例 3**'
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This is a combination of the ideas in Example 1 and Example 2\. Here we have
    both conditions (based on the value of feature B) and cases where the feature
    are independently related to the probability of each target class.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这是示例 1 和示例 2 中思想的结合。在这里，我们既有基于特征 B 值的条件，又有特征与每个目标类的概率独立相关的情况。
- en: While there are other means to taxonify functions, this system is useful, and
    many true functions may be viewed as some combination of these, somewhere between
    Example 1 and Example 2.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然还有其他方法可以对函数进行分类，但该系统是有用的，许多真实函数可能被视为这些方法的某种组合，介于示例 1 和示例 2 之间。
- en: Standard decision trees do not explicitly assume the true function is similar
    to Example 1 and can accurately (often through the use of very large trees) capture
    non-conditional relationships such as those based on probabilities (cases more
    like Examples 2 or 3). They do, however, model the functions as conditions, which
    can limit their expressive power and lower their interpretability.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 标准决策树并不明确假设真实的函数类似于示例 1，并且能够准确地（通常通过使用非常大的树）捕捉到非条件性关系，例如基于概率的关系（更像示例 2 或示例 3
    的情况）。然而，它们将函数建模为条件，这可能会限制它们的表达能力并降低其可解释性。
- en: Additive Decision Trees remove the assumption in standard decision trees that
    f(x) may be best modeled as a set of conditions, but does support conditions where
    the data suggests they exist. The central idea is that the true f(x) may be based
    on logical conditions, probabilities (additive, independent rules), or some combination
    of these.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 加法决策树去除了标准决策树中假设 f(x) 可能最好通过一组条件建模的假设，但支持在数据表明存在时的条件。核心思想是，真实的 f(x) 可能基于逻辑条件、概率（加法的、独立的规则）或这些的某种组合。
- en: In general, standard decision trees may perform very well (in terms of interpretability)
    where the true f(x) is similar to Example 1.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，标准决策树在真实的 f(x) 类似于示例 1 时，可能会表现得非常好（在可解释性方面）。
- en: Where the true f(x) is similar to Example 2, we may be better to use a linear
    or logistic regression, Naive Bayes models, or GAM (Generalized Additive Model),
    or other models that simply predict based on a weighted sum of each independent
    feature. However, these models can struggle with functions similar to Example
    1.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当真实的 f(x) 类似于示例 2 时，我们可能更适合使用线性回归或逻辑回归、朴素贝叶斯模型、广义加法模型（GAM）或其他基于每个独立特征的加权总和进行预测的模型。然而，这些模型在处理类似示例
    1 的函数时可能表现不佳。
- en: Additive Decision Trees can adapt to both cases, though may perform best where
    the true f(x) is somewhere between, as with Example 3.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 加法决策树能够适应这两种情况，尽管在真实的 f(x) 介于两者之间时，如示例 3，可能表现最佳。
- en: Constructing Additive Decision Trees
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建加法决策树
- en: We describe here how Additive Decision Trees are constructed. The process is
    simpler to present for classification problems, and so the examples relate to
    this, but the ideas apply equally to regression.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里描述了加法决策树的构建过程。对于分类问题，这个过程更容易呈现，因此示例与此相关，但这些思想同样适用于回归问题。
- en: The approach taken by Additive Decision Trees is to use two types of split.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 加法决策树所采用的方法是使用两种类型的划分。
- en: First, where appropriate, it may split the dataspace in the same way as standard
    decision trees. As with standard decision trees, most nodes in an Additive Decision
    Tree represent a region of the full space, with the root representing the full
    space. Each node splits this region in two, based on a split point in one feature.
    This results in two child nodes, each covering a portion of the space covered
    by the parent node. For example, in Example 1, we may have a node (the root node)
    that splits the data based on Feature A at 10\. The rows where A is less than
    or equal to 10 would go to one child node and the rows where A is greater than
    10 would go to the other child node.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在适当的情况下，它可能会像标准决策树一样对数据空间进行划分。与标准决策树一样，加法决策树中的大多数节点表示完整空间的一个区域，根节点表示整个空间。每个节点根据一个特征的划分点将该区域分为两部分。这样就会产生两个子节点，每个子节点覆盖父节点所覆盖的空间的一部分。例如，在示例
    1 中，我们可能有一个节点（根节点），根据特征 A 在 10 处将数据划分。A 小于或等于 10 的行会进入一个子节点，而 A 大于 10 的行则进入另一个子节点。
- en: Second, in Additive Decision Trees, the splits may be based on an aggregate
    decision based on numerous potential splits (each are standard splits for a single
    feature and split point). That is, in some cases, we do not rely on a single split,
    but assume there could be numerous features that are valid to split at a given
    node, and take the average of splitting in each of these ways. When splitting
    in this way, there are no other nodes below, so these become leaf nodes, known
    as *Additive Nodes*.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，在加法决策树中，划分可能基于多种潜在的划分（每个都是针对单一特征和划分点的标准划分）的聚合决策。也就是说，在某些情况下，我们并不依赖单一的划分，而是假设在给定节点处可能有多个有效的特征可以进行划分，并取这些划分方式的平均值。在这种划分方式下，没有其他节点在下方，因此这些节点成为叶节点，称为*加法节点*。
- en: Constructing Additive Decision Trees is done such that the first type of splits
    (standard decision tree nodes, based on a single feature) appear higher in the
    tree, where there are larger numbers of samples to base the splits on and they
    may be found in a more reliable manner. In these cases, it’s more reasonable to
    rely on a single split on a single feature.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 构建加法决策树的方式是，第一种类型的划分（基于单一特征的标准决策树节点）出现在树的较高位置，在这里有更多的样本可以用于基于划分，并且这些划分可以更可靠地找到。在这些情况下，依赖单一特征的单一划分更为合理。
- en: The second type (additive nodes, based on aggregations of many splits) appear
    lower in the tree, where there are less samples to rely on.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种类型（基于多个划分聚合的加法节点）出现在树的较低位置，此时可依赖的样本较少。
- en: 'An example, creating a tree to represent Example 3, may produce an Additive
    Decision Tree such as:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一个示例是，创建一个树来表示示例 3，可能会产生如下的加法决策树：
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this example, we have a normal node at the root, which is split on feature
    B at 100\. Beneath that we have two additive nodes (which are always leaf nodes).
    During training, we may determine that splitting this node based on features A,
    B, C, and D are all productive; while picking one may appear to work slightly
    better than the others, it is somewhat arbitrary which is selected. When training
    standard decision trees, it’s very often a factor of minor variations in the training
    data which is selected.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们在根节点有一个正常节点，它在100处分裂了特征B。在它下面有两个加法节点（这些节点始终是叶节点）。在训练过程中，我们可能会确定，基于特征A、B、C和D进行分裂都是有效的；虽然选择其中一个可能看起来比其他选择稍微有效一些，但究竟选择哪一个是有些任意的。在训练标准决策树时，通常是根据训练数据中的细微变化来选择分裂点。
- en: 'To compare this to a standard decision tree: a decision tree would pick one
    of the four possible splits in the first node and also one of the four possible
    splits in the second node. In the first node, if it selected, say, Feature A (split
    at 50), then this would split this node into two child nodes, which can then be
    further split into more child nodes and so on. This can work well, but the splits
    would be determined based on fewer and fewer rows. And it may not be necessary
    to split the data into finer spaces: the true f(x) may not have conditional logic.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 将此与标准决策树进行比较：标准决策树会选择第一个节点中的四个可能分裂点中的一个，并且也会选择第二个节点中的四个可能分裂点中的一个。在第一个节点中，如果它选择了例如特征A（在50处分裂），那么这个节点将被分裂成两个子节点，这些子节点可以继续被分裂成更多的子节点，依此类推。这种方式可以很好地工作，但分裂是基于越来越少的数据行进行的。而且，可能不需要将数据分割成更细的空间：真正的f(x)可能没有条件逻辑。
- en: In this case, the Additive Decision tree examined the four possible splits and
    decided to take all four. The predictions for these nodes would be based on *adding*
    the predictions of each.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，加法决策树检查了四个可能的分裂点，并决定采用所有四个分裂点。这些节点的预测将基于*加总*每个分裂点的预测结果。
- en: 'One major advantage of this is: each of the four splits is based on the full
    data available in this node; each are as accurate as is possible given the training
    data in this node. We also avoid a potentially very large sub-tree underneath
    this.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个主要优势是：四个分裂点中的每一个都是基于该节点中可用的完整数据；每个分裂点的准确性尽可能高，基于该节点中的训练数据。我们还避免了在其下可能出现的非常大的子树。
- en: 'Reaching these nodes during prediction, we would add the predictions together.
    For example if a record has values for A, B, C, and D of : [60, 120, 80, 120],
    then when it hits the first node, we compare the value of B (120) to the split
    point 100\. B is over 100, so we go to the first node. Now, instead of another
    split, there are four splits. We split based on the values in A, in B, in C, *and*
    in D. That is, we calculate the prediction based on all four splits. In each case,
    we get a set of probabilities for class X, Y, and Z. We add these together to
    get the final probabilities of each class.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测时到达这些节点时，我们会将预测结果相加。例如，如果一个记录的A、B、C和D的值分别为：[60, 120, 80, 120]，那么当它到达第一个节点时，我们将B（120）的值与分裂点100进行比较。B大于100，因此我们进入第一个节点。现在，代替另一个分裂点，这里有四个分裂点。我们根据A、B、C、*以及*D的值进行分裂。也就是说，我们基于所有四个分裂点计算预测结果。在每一种情况下，我们都会得到一个关于X、Y和Z类的概率集合。我们将这些概率加在一起，得到每个类的最终概率。
- en: The first split is based on A at split point 50\. The row has value 60, so there
    are a set of probabilities for each target class (X, Y, and Z) associated with
    this split. The second split is based on B at split point 150\. B has value 120,
    so there are another set of probabilities for each target class associated with
    this split. Similar for the other two splits inside this additive node. We find
    the predictions for each of these four splits and add them for the final prediction
    for this record.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个分裂点是基于A，在50处分裂。该行的值为60，因此与这个分裂相关的每个目标类别（X、Y和Z）都有一组概率。第二个分裂点是基于B，在150处分裂。B的值为120，因此与此分裂相关的每个目标类别又有一组概率。对于加法节点中的其他两个分裂也是类似的。我们找到每个分裂的预测结果，并将它们加在一起，得到该记录的最终预测。
- en: 'This provides, then, a simple form of ensembling *within* a decision tree.
    We receive the normal benefits of ensembling: more accurate and stable predictions,
    while actually increasing interpretability.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这就提供了一种简单的集成方法*在*决策树内部。我们获得了集成的正常好处：更准确和稳定的预测，同时实际提高了可解释性。
- en: 'This may appear to create a more complex tree, and in a sense it does: the
    additive nodes are more complex than standard nodes. But, the additive nodes tend
    to aggregate relatively few splits (usually about two to five). And, they also
    remove the need for a very large number of nodes below them. The net reduction
    in complexity is often quite significant.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来可能会创造一个更复杂的树，从某种意义上讲，它确实是这样：加法节点比标准节点更复杂。但加法节点通常聚合相对较少的分裂（通常是两到五个）。而且，它们也消除了在其下方存在大量节点的需求。复杂度的整体减少通常是相当显著的。
- en: Interpretability with Standard Decision Trees
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标准决策树的可解释性
- en: 'In standard decision trees, global explanations (explanations of the model
    itself) are presented as the tree: we simply render in some way (such as scikit-learn’s
    plot_tree() or export_text() methods). This allows us to understand the predictions
    that will be produced for any unseen data.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准决策树中，全局解释（对模型本身的解释）以树的形式呈现：我们只需以某种方式呈现（例如scikit-learn的plot_tree()或export_text()方法）。这使我们能够理解对于任何未见过的数据将产生的预测。
- en: 'Local explanations (explanations of the prediction for a single instance) are
    presented as the decision path: the path from the root to the leaf node where
    the instance ends, with each split point on the path leading to this final decision.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 局部解释（对单个实例的预测解释）以决策路径的形式呈现：从根节点到叶节点的路径，实例最终落在叶节点上，路径中的每个分裂点都指向这个最终的决策。
- en: The decision paths can be difficult to interpret. The decision paths can be
    very long, can include nodes that are not relevant to the current prediction,
    and that are somewhat arbitrary (where one split was selected by the decision
    tree during training, there may be multiple others that are equally valid).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 决策路径可能难以解释。决策路径可能非常长，可能包括与当前预测无关的节点，而且有些节点是相对任意的（在训练过程中，决策树选择了某个分裂，但可能还有多个其他同样有效的分裂）。
- en: Interpretability of Additive Decision Trees
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加法决策树的可解释性
- en: Additive decision trees are interpreted in the mostly same way as standard decision
    trees. The one difference is additive nodes, where there are multiple splits as
    opposed to one.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 加法决策树的解释方式与标准决策树基本相同。唯一的区别是加法节点，在这些节点中有多个分裂，而不是一个。
- en: The maximum number of splits aggregated together is configurable, but 4 or 5
    is typically sufficient. In most cases, as well, all splits agree, and only one
    needs to be presented to the user. And in fact, even where the splits disagree,
    the majority prediction may be presented as a single split. Therefore, the explanations
    are usually similar as those for standard decision trees, but with shorter decision
    paths.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合在一起的最大分裂数量是可配置的，但通常4或5个就足够了。在大多数情况下，所有分裂一致，且只需要向用户展示其中一个。事实上，即使分裂不一致，最多数的预测也可能作为单一分裂呈现。因此，解释通常与标准决策树相似，但决策路径较短。
- en: This, then, produces a model where there are a small number of standard (single)
    splits, ideally representing the true conditions, if any, in the model, followed
    by *additive nodes*, which are leaf nodes that average the predictions of multiple
    splits, providing more robust predictions. This reduces the need to split the
    data into progressively smaller subsets, each with less statistical significance.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，就产生了一个模型，其中有少数标准（单一）分裂，理想情况下表示模型中的真实条件（如果有的话），然后是*加法节点*，这些是叶节点，负责平均多个分裂的预测，提供更稳健的预测。这减少了将数据拆分为越来越小的子集的需求，每个子集的统计意义较小。
- en: Pruning Algorithm
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修剪算法
- en: 'Additive decision trees first construct standard decision trees. They then
    run a pruning algorithm to try to reduce the number of nodes: by combining many
    standard nodes into a single node (an Additive Node) that aggregates predictions.
    The ideas is: where there are many nodes in a tree, or a sub-tree within a tree,
    this may be due the the tree attempting to narrow in on a prediction, while balancing
    the influence of many features.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 加法决策树首先构建标准决策树。然后，它们运行修剪算法，试图减少节点的数量：通过将许多标准节点合并为一个节点（加法节点），该节点聚合预测。其理念是：当树中的某个节点，或树中的子树有许多节点时，这可能是因为树试图专注于一个预测，同时平衡许多特征的影响。
- en: The algorithm behaves similarly to most pruning algorithms, starting at the
    bottom, at the leaves, and working towards the root node. At each node, a decision
    is made to either leave the node as is, or convert it to an additive node; that
    is, a node combining multiple data splits.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法的行为类似于大多数修剪算法，从底部开始，从叶节点开始，向根节点方向工作。在每个节点上，做出决策，要么保持节点不变，要么将其转换为加法节点；即，合并多个数据分裂的节点。
- en: At each node, the accuracy of the tree is evaluated on the training data given
    the current split, then again treating this node as an additive node. If the accuracy
    is higher with this node set as an additive node, it is set as such, and all nodes
    below it removed. This node itself may be later removed, if a node above it is
    converted to an additive node. Testing indicates a very significant proportion
    of sub-trees benefit from being aggregated in this way.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个节点上，评估树在当前分裂下的训练数据准确性，然后再次将该节点视为加法节点。如果将该节点设为加法节点时准确性更高，则将其设置为加法节点，并移除其下方的所有节点。如果一个上层节点被转换为加法节点，这个节点本身也可能会被移除。测试表明，相当大比例的子树通过这种方式进行聚合后，能够获得显著的提升。
- en: Evaluation Tests
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估测试
- en: To evaluate the effectiveness of the tool we considered both accuracy (macro
    f1-score for classification; and normalized root mean squared error (NRMSE) for
    regression) and interpretability, measured by the size of the tree. Details regarding
    the complexity metric are included below. Further details about the evaluation
    tests are provided on the github page.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估工具的有效性，我们考虑了准确性（分类任务的宏F1分数；回归任务的标准化均方根误差（NRMSE））和可解释性，后者通过树的大小来衡量。关于复杂度度量的详细信息在下文中提供。更多评估测试的细节请参见GitHub页面。
- en: To evaluate, we compared to standard decision trees, comparing where both models
    used default hyperparameters, and again where both models used a grid search to
    estimate the best parameters. 100 datasets selected randomly from OpenML were
    used.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行评估，我们将加法决策树与标准决策树进行了比较，分别在两种情况下进行比较：一种是两种模型使用默认超参数，另一种是两种模型都使用网格搜索来估算最佳参数。我们使用了从OpenML随机选择的100个数据集。
- en: This used a tool called [DatasetsEvaluator](https://github.com/Brett-Kennedy/DatasetsEvaluator),
    though the experiments can be reproduced easily enough without this. DatasetsEvaluator
    is simply a convenient tool to simplify such testing and to remove any bias selecting
    the test datasets.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作使用了一个名为[DatasetsEvaluator](https://github.com/Brett-Kennedy/DatasetsEvaluator)的工具，尽管没有这个工具，实验也能轻松重现。DatasetsEvaluator只是一个方便的工具，用来简化这种测试并消除选择测试数据集时的任何偏差。
- en: '**Results for classification on 100 datasets**'
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**在100个数据集上的分类结果**'
- en: '![](../Images/076d655748ed85bc226e81167840c658.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/076d655748ed85bc226e81167840c658.png)'
- en: Here ‘DT’ refers to scikit-learn decision trees and ‘ADT’ refers to Additive
    Decision Trees. The Train-Test Gap was found subtracting the F1 macro score on
    test set from that on the train set, and is used to estimate overfitting. ADT
    models suffered considerably less from over-fitting.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的“DT”指的是scikit-learn的决策树，“ADT”指的是加法决策树。训练-测试差距是通过从测试集的F1宏分数中减去训练集的F1宏分数来计算的，用于估计过拟合程度。加法决策树模型的过拟合程度明显较低。
- en: Additive Decision Trees did very similar to standard decision trees with respect
    to accuracy. There are many cases where standard Decision Trees do better, where
    Additive Decision Trees do better, and where they do about the same. The time
    required for ADT is longer than for DT, but still very small, averaging about
    4 seconds.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 加法决策树在准确性方面与标准决策树非常相似。有很多情况下，标准决策树表现更好，也有许多情况下加法决策树表现更好，另外还有一些情况两者表现差不多。加法决策树所需的时间比标准决策树长，但仍然非常短，平均约为4秒。
- en: The major difference is in the complexity of the generated trees.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的区别在于生成树的复杂性。
- en: The following plots compare the accuracy (top pane) and complexity (bottom pane),
    over the 100 datasets, ordered from lowest to highest accuracy with a standard
    decision tree.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表比较了100个数据集上的准确性（上面一栏）和复杂性（下面一栏），这些数据集按标准决策树的准确性从低到高排列。
- en: '![](../Images/b44e1d36bb0a5a88db27348ba8ca9487.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b44e1d36bb0a5a88db27348ba8ca9487.png)'
- en: The top plot tracks the 100 datasets on the x-axis, with F1 score (macro) on
    y-axis. Higher is better. We can see, towards the right, where both models are
    quite accurate. To the left, we see several cases where DT fairs poorly, but ADT
    much better in terms of accuracy. We can also see, there are several cases where,
    in terms of accuracy, it is clearly preferable to use standard decision trees
    and several cases where it is clearly preferable to use Additive Decision Trees.
    In most cases, it may be best to try both (as well as other model types).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 上方的图表跟踪了 x 轴上的 100 个数据集，y 轴为 F1 分数（宏平均）。分数越高越好。我们可以看到，在右侧，两种模型都非常准确。左侧则有一些情况，决策树（DT）表现不佳，但加性决策树（ADT）在准确性方面表现更好。我们还可以看到，在一些情况下，从准确性的角度来看，使用标准决策树显然更合适，而在另一些情况下，使用加性决策树显然更合适。在大多数情况下，最好尝试两者（以及其他模型类型）。
- en: The second plot tracks the same 100 datasets on the x-axis, and model complexity
    on the y-axis. Lower is better. In this case, ADT is consistently more interpretable
    than DT, at least using the current complexity metric used here. In all 100 cases,
    the trees produced are simpler, and frequently much simpler.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 第二张图表跟踪了相同的 100 个数据集，x 轴为模型复杂度，y 轴为模型复杂度。分数越低越好。在这种情况下，ADT 一直比 DT 更具可解释性，至少在这里使用的当前复杂度度量中如此。在所有
    100 个案例中，产生的树更加简单，且经常更为简单。
- en: Example
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例
- en: Additive Decision Trees follow the standard sklearn fit-predict API framework.
    We typically, as in this example, create an instance, call fit() and call predict().
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 加性决策树遵循标准的 sklearn fit-predict API 框架。通常，如本例所示，我们创建一个实例，调用 fit() 方法，然后调用 predict()
    方法。
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The github page also provides example notebooks covering basic usage and evaluation
    of the model.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Github 页面还提供了示例笔记本，涵盖了模型的基本使用和评估。
- en: 'Additive Decision Trees provide two additional APIs to make interpretability
    greater: output_tree() and get_explanations(). output_tree() provides a view of
    a decision tree similar to in scikit-learn using export_text(), though provides
    somewhat more information.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 加性决策树提供了两个额外的 API 以提高可解释性：output_tree() 和 get_explanations()。output_tree() 提供类似于
    scikit-learn 中使用 export_text() 的决策树视图，但提供了更多信息。
- en: get_explanations provides the local explanations (in the form of the decision
    paths) for a specified set of rows. Here we get the explanations for the first
    five rows.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: get_explanations 提供了针对指定行集的本地解释（以决策路径的形式）。在这里，我们获取前五行的解释。
- en: '[PRE7]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The explanation for the first row is:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行的解释是：
- en: '[PRE8]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: From the first line we see there are two classes (0 and 1) and there are 159
    instances of class 0 in the training data and 267 of class 1.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 从第一行我们可以看到有两个类别（0 和 1），在训练数据中类别 0 有 159 个实例，类别 1 有 267 个实例。
- en: The root node is always node 0\. This row is taken through nodes 0, 2, and 6,
    based on its values for ‘mean concave points’ and ‘worst area’. Information about
    these nodes can be found calling output_tree(). In this case, all nodes on the
    path are standard decision tree nodes (none are additive nodes).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 根节点始终是节点 0。该行通过节点 0、2 和 6，基于‘平均凹点数’和‘最差区域’的值。关于这些节点的信息可以通过调用 output_tree() 获得。在这个例子中，路径上的所有节点都是标准决策树节点（没有加性节点）。
- en: At each stage, we see the counts for both classes. After the first split, we
    are in a region where class 0 is most likely (146 to 20). After another split,
    class 0 is even more likely (133 to 3).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个阶段，我们看到两类的计数。在第一次切分后，我们进入一个类别 0 更可能出现的区域（146 对 20）。再经过一次切分后，类别 0 更为可能（133
    对 3）。
- en: The next example shows an example of a prediction for a row that goes through
    an additive node (node 3).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个示例展示了一个预测示例，其中一行通过了一个加性节点（节点 3）。
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The last node is an additive node, based on two splits. In both splits, the
    prediction is strongly for class 1 (1 to 209 and 4 to 243). Accordingly, the final
    prediction is class 1.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一节点是一个加性节点，基于两个切分。在这两个切分中，预测强烈偏向类别 1（从 1 到 209 和从 4 到 243）。因此，最终的预测结果是类别 1。
- en: Interpretability Metric
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可解释性指标
- en: The evaluation above is based on the global complexity of the models, which
    is the overall size of the trees, combined with the complexity of each node.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 上述评估基于模型的全局复杂度，即树的整体大小，加上每个节点的复杂度。
- en: 'It’s also valid to look at the average local complexity (complexity of each
    decision path: the length of the paths combined with the complexity of the nodes
    on the decision paths). Using the average local complexity is also a valid metric,
    and ADT does well in this regard as well. But, for simplicity, we look here the
    global complexity of the models.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以查看平均局部复杂度（每个决策路径的复杂度：路径的长度加上决策路径上节点的复杂度）。使用平均局部复杂度也是一个有效的度量标准，加性决策树在这方面表现良好。但为了简化，我们在这里关注模型的全局复杂度。
- en: For standard decision trees, the evaluation simply uses the number of nodes
    (a common metric for decision tree complexity, though others are commonly used,
    for example number of leaf nodes). For additive trees, we do this as well, but
    for each additive node, count it as many times as there are splits aggregated
    together at this node.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对于标准决策树，评估仅使用节点数量（这是衡量决策树复杂度的常见指标，尽管通常也使用其他指标，例如叶节点数量）。对于加性树，我们也这样做，但对于每个加性节点，我们会根据该节点上聚合在一起的分裂次数来计数。
- en: We, therefore, measure the total number of comparisons of feature values to
    thresholds (the number of splits) regardless if these are in multiple nodes or
    a single node. Future work will consider additional metrics.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们衡量特征值与阈值的总比较次数（即分裂次数），无论这些比较发生在多个节点还是单个节点中。未来的工作将考虑额外的度量标准。
- en: For example, in a standard node we may have a split such as Feature C > 0.01\.
    That counts as one. In an additive node, we may have multiple splits, such as
    Feature C > 0.01, Feature E > 3.22, Feature G > 990\. That counts as three. This
    appears to be a sensible metric, though it is notoriously difficult and subjective
    to try to quantify the cognitive load of different forms of model.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在标准节点中，我们可能有像 `Feature C > 0.01` 这样的分裂，这算作一次。在加性节点中，我们可能有多个分裂，例如 `Feature
    C > 0.01`、`Feature E > 3.22`、`Feature G > 990`，这算作三次。这似乎是一个合理的度量标准，尽管尝试量化不同模型形式的认知负担是出了名的困难且主观。
- en: Additive Decision Trees for XAI
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加性决策树在 XAI 中的应用
- en: As well as being used as interpretable model, Additive Decision Trees may also
    be considered a useful XAI (Explainable AI) tool — Additive Decision Trees may
    be used as proxy models, and so provide explanations of black-box models. This
    is a common technique in XAI, where an interpretable model is trained to predict
    the output of a black-box model. Doing this the proxy models can provide comprehensible,
    though only approximate, explanations of the predictions produced by black-box
    models. Typically, the same models that are appropriate to use as interpretable
    models may also be used as proxy models.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 除了作为可解释模型使用外，加性决策树还可以被视为一个有用的 XAI（可解释人工智能）工具——加性决策树可以作为代理模型，从而提供对黑箱模型的解释。这是
    XAI 中的常见技术，其中训练一个可解释的模型来预测黑箱模型的输出。通过这种方式，代理模型可以提供易于理解的解释，尽管只是近似的，且解释黑箱模型产生的预测。通常，适合作为可解释模型使用的模型也可以用作代理模型。
- en: For example, if an XGBoost model is trained to predict a certain target (eg
    stock prices, weather forecasts, customer churn, etc.), the model may be accurate,
    but we may not know *why* the model is making the predictions it is. We can then
    train an interpretable model (including standard decision tree, Additive Decision
    Tree, ikNN, GAM, and so on) to predict (in an interpretable way) the predictions
    of the XGBoost. This won’t work perfectly, but where the proxy model is able to
    predict the behavior of the XGBoost model reasonably accurately, it provides explanations
    that are usually approximately correct.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果训练了一个 XGBoost 模型来预测某个目标（例如股价、天气预报、客户流失等），该模型可能是准确的，但我们可能不知道*为什么*模型会做出这些预测。我们可以训练一个可解释的模型（包括标准决策树、加性决策树、ikNN、GAM
    等），以可解释的方式预测 XGBoost 的预测结果。这不会完美，但当代理模型能够合理准确地预测 XGBoost 模型的行为时，它提供的解释通常是大致正确的。
- en: Installation
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装
- en: The source code is provided in a single .py file, [AdditiveDecisionTree.py](https://github.com/Brett-Kennedy/AdditiveDecisionTree/blob/main/AdditiveDecisionTree/AdditiveDecisionTree.py),
    which may be included in any project. It uses no non-standard libraries.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 源代码提供在一个单独的 .py 文件中，[AdditiveDecisionTree.py](https://github.com/Brett-Kennedy/AdditiveDecisionTree/blob/main/AdditiveDecisionTree/AdditiveDecisionTree.py)，可以包含在任何项目中。它没有使用任何非标准库。
- en: Conclusions
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Though the final trees may be somewhat more complex than an standard decision
    tree of equal depth, Additive Decision Trees are more accurate than standard decision
    trees of equal depth, and simpler than standard decision trees of equal accuracy.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管最终的树可能比深度相同的标准决策树更复杂，但加法决策树在精度上优于深度相同的标准决策树，并且比精度相同的标准决策树更简单。
- en: As with all interpretable models, Additive Decision Trees are not intended to
    be competitive in terms of accuracy with state of the art models for tabular data
    such as boosted models. Additive Decision Trees are, though, competitive with
    most other interpretable models, both in terms of accuracy and interpretability.
    While no one tool will be best, where interpretability is important, is is usually
    worth trying several tools, including Additive Decision Trees.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有可解释模型一样，加法决策树并不旨在与当前最先进的表格数据模型（如提升模型）在精度上竞争。然而，加法决策树在精度和可解释性方面与大多数其他可解释模型具有竞争力。虽然没有一种工具是最佳的，但在可解释性重要的场合，通常值得尝试几种工具，包括加法决策树。
- en: All images are by author.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 所有图片均由作者提供。
