- en: Mastering K-Means Clustering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精通 K-Means 聚类
- en: 原文：[https://towardsdatascience.com/mastering-k-means-clustering-065bc42637e4?source=collection_archive---------0-----------------------#2024-05-22](https://towardsdatascience.com/mastering-k-means-clustering-065bc42637e4?source=collection_archive---------0-----------------------#2024-05-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/mastering-k-means-clustering-065bc42637e4?source=collection_archive---------0-----------------------#2024-05-22](https://towardsdatascience.com/mastering-k-means-clustering-065bc42637e4?source=collection_archive---------0-----------------------#2024-05-22)
- en: Implement the K-Means algorithm from scratch with this step-by-step Python tutorial
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过这个逐步 Python 教程，从头开始实现 K-Means 算法
- en: '[](https://marcusmvls-vinicius.medium.com/?source=post_page---byline--065bc42637e4--------------------------------)[![Marcus
    Sena](../Images/ff594ec7029e6259f0be6dc031d8a6cd.png)](https://marcusmvls-vinicius.medium.com/?source=post_page---byline--065bc42637e4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--065bc42637e4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--065bc42637e4--------------------------------)
    [Marcus Sena](https://marcusmvls-vinicius.medium.com/?source=post_page---byline--065bc42637e4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://marcusmvls-vinicius.medium.com/?source=post_page---byline--065bc42637e4--------------------------------)[![Marcus
    Sena](../Images/ff594ec7029e6259f0be6dc031d8a6cd.png)](https://marcusmvls-vinicius.medium.com/?source=post_page---byline--065bc42637e4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--065bc42637e4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--065bc42637e4--------------------------------)
    [Marcus Sena](https://marcusmvls-vinicius.medium.com/?source=post_page---byline--065bc42637e4--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--065bc42637e4--------------------------------)
    ·8 min read·May 22, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--065bc42637e4--------------------------------)
    ·阅读时间 8 分钟·2024年5月22日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/5c7adde7e4e88ee31689443db5e31e44.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5c7adde7e4e88ee31689443db5e31e44.png)'
- en: Image by the author using DALL-E.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用 DALL-E 创建。
- en: In this article, I show how I’d learn the K-Means algorithm if I’d started today.
    We’ll start with the fundamental concepts and implement a Python class that performs
    clustering tasks using nothing more than the Numpy package.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将展示如果今天开始学习 K-Means 算法，我会如何学习。我们将从基础概念开始，并实现一个使用 Numpy 包执行聚类任务的 Python
    类。
- en: Whether you are a machine learning beginner trying to build a solid understanding
    of the concepts or a practitioner interested in creating custom machine learning
    applications and needs to understand how the algorithms work under the hood, that
    article is for you.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你是一个试图建立坚实概念理解的机器学习初学者，还是一个有兴趣创建自定义机器学习应用并需要了解算法底层实现的从业者，本文都适合你。
- en: Table of contents
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 目录
- en: ''
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[1\. Introduction](#4970)'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[1\. 介绍](#4970)'
- en: '[2\. What Does the K-Means algorithm do?](#7e98)'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[2\. K-Means 算法做什么？](#7e98)'
- en: '[3\. Implementation in Python](#60a5)'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[3\. Python 实现](#60a5)'
- en: '[4\. Evaluation and Interpretation](#2e89)'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[4\. 评估与解释](#2e89)'
- en: '[5\. Conclusions and Next Steps](#655b)'
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[5\. 结论与下一步](#655b)'
- en: 1\. Introduction
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 介绍
- en: Most of the machine learning algorithms widely used, such as Linear Regression,
    Logistic Regression, Decision Trees, and others are useful for making predictions
    from labeled data, that is, each input comprises feature values with a label value
    associated. That is what is called **Supervised Learning**.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数广泛使用的机器学习算法，如线性回归、逻辑回归、决策树等，都是用来从标记数据中进行预测的，即每个输入包含特征值并与一个标签值相关联。这就是所谓的**监督学习**。
- en: However, often we have to deal with large sets of data with no label associated.
    Imagine a business that needs to understand the different groups of customers
    based on purchasing behavior, demographics, address, and other information, thus
    it can offer better services, products, and promotions.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，通常我们需要处理没有标签的大数据集。想象一下，一个企业需要根据购买行为、人口统计学、地址等信息了解不同的客户群体，从而能够提供更好的服务、产品和促销活动。
- en: These types of problems can be addressed with the use of **Unsupervised Learning**
    techniques. The K-Means algorithm is a widely used unsupervised learning algorithm
    in Machine Learning. Its simple and elegant approach makes it possible to separate
    a dataset into a desired number of K distinct clusters, thus allowing one to learn
    patterns from unlabelled data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这类问题可以通过使用 **无监督学习** 技术来解决。K-Means 算法是机器学习中广泛使用的无监督学习算法。它简洁而优雅的方法使得将数据集分成 K
    个不同的簇成为可能，从而可以从未标记的数据中学习模式。
- en: 2\. What Does the K-Means algorithm do?
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. K-Means 算法做什么？
- en: As said earlier, the K-Means algorithm seeks to partition data points into a
    given number of clusters. The points within each cluster are similar, while points
    in different clusters have considerable differences.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，K-Means 算法旨在将数据点划分为给定数量的簇。每个簇中的点是相似的，而不同簇中的点有显著的差异。
- en: 'Having said that, one question arises: how do we define similarity or difference?
    In K-Means clustering, the Euclidean distance is the most common metric for measuring
    similarity.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，一个问题随之而来：我们如何定义相似性或差异？在 K-Means 聚类中，欧几里得距离是最常用的度量相似性的指标。
- en: In the figure below, we can clearly see 3 different groups. Hence, we could
    determine the centers of each group and each point would be associated with the
    closest center.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们可以清楚地看到 3 个不同的组。因此，我们可以确定每个组的中心，并且每个点将与最接近的中心关联。
- en: '![](../Images/0ec2067871637ccf83430558df44cfbf.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ec2067871637ccf83430558df44cfbf.png)'
- en: Simulated dataset with 200 observations (image by the author).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 带有 200 个观测值的模拟数据集（图片由作者提供）。
- en: By doing that, mathematically speaking, the idea is to minimize the *within-cluster
    variance*, the measurement of similarity between each point and its closest center.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样做，从数学角度来说，目标是最小化 *簇内方差*，即每个点与其最接近的中心之间的相似性度量。
- en: Performing the task in the example above was straightforward because the data
    was two-dimensional and the groups were clearly distinct. However, as the number
    of dimensions increases and different values of K are considered, we need an algorithm
    to handle the complexity.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 执行上述示例中的任务很简单，因为数据是二维的且各组之间差异明显。然而，随着维度的增加和考虑不同的 K 值，我们需要一种算法来处理复杂性。
- en: 'Step 1: Pick the initial centers (randomly)'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 1 步：选择初始中心（随机选择）
- en: We need to seed the algorithm with initial center vectors that can be chosen
    randomly from the data or generate random vectors with the same dimensions as
    the original data. See the white diamonds in the image below.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为算法提供初始中心向量，这些向量可以从数据中随机选择，或者生成与原始数据相同维度的随机向量。请参见下图中的白色菱形。
- en: '![](../Images/33dd2f443181dc5507406b8ec38c9077.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/33dd2f443181dc5507406b8ec38c9077.png)'
- en: Initial centers are randomly picked (image by the author).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 初始中心是随机选择的（图片由作者提供）。
- en: 'Step 2: Find the distances of each point to the centers'
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 2 步：找到每个点到中心的距离
- en: Now, we'll calculate the distance of each data point to the K centers. Then
    we associate each point with the center closest to that point.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将计算每个数据点到 K 个中心的距离。然后将每个点与离它最近的中心关联。
- en: 'Given a dataset with *N* entries and *M* features, the distances to the centers
    *c* can be given by the following equation:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个包含 *N* 条数据和 *M* 个特征的数据集，可以通过以下公式计算到中心 *c* 的距离：
- en: '![](../Images/01b720e1312e5eea301af0f6dfa65c36.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01b720e1312e5eea301af0f6dfa65c36.png)'
- en: Euclidean distance (image generated using codecogs.com).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离（图像通过 codecogs.com 生成）。
- en: 'where:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '*k* varies from 1 to *K*;'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*k* 从 1 到 *K* 变化；'
- en: '*D* is the distance of a point n to the *k* center;'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*D* 是点 n 到 *k* 中心的距离；'
- en: '*x* is the point vector;'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*x* 是点向量；'
- en: '*c* is the center vector.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*c* 是中心向量。'
- en: 'Hence, for each data point *n* we''ll have K distances, then we have to label
    the vector to the center with the smallest distance:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于每个数据点 *n*，我们将得到 K 个距离，然后我们需要将该向量标记为与最小距离的中心关联：
- en: '![](../Images/6c1fb0e5d6b3deeba9e459fef5ca022c.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c1fb0e5d6b3deeba9e459fef5ca022c.png)'
- en: (image generated using codecogs.com)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: （图像通过 codecogs.com 生成）
- en: Where *D* is a vector with *K* distances.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *D* 是一个包含 *K* 个距离的向量。
- en: 'Step 3: Find the *K* centroids and iterate'
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 3 步：找到 *K* 个质心并进行迭代
- en: For each of the *K* clusters, recalculate the centroid. The new centroid is
    the mean of all data points assigned to that cluster. Then update the positions
    of the centroids to the newly calculated.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个 *K* 个簇，重新计算质心。新的质心是分配给该簇的所有数据点的均值。然后更新质心的位置，使用新计算出的质心。
- en: Check if the centroids have changed significantly from the previous iteration.
    This can be done by comparing the positions of the centroids in the current iteration
    with those in the last iteration.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 检查质心是否发生了显著变化。这可以通过将当前迭代中的质心位置与上一迭代中的质心位置进行比较来完成。
- en: If the centroids have changed significantly, go back to Step 2\. If not, the
    algorithm has converged and the process stops. See the image below.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果质心发生了显著变化，请返回第2步。如果没有，算法已收敛，过程停止。见下图。
- en: '![](../Images/0c157b01ce3cb851db9481f3b53f5786.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c157b01ce3cb851db9481f3b53f5786.png)'
- en: Convergence of the centroids (image by the author).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 质心的收敛（作者提供的图片）。
- en: 3\. Implementation in Python
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 在Python中的实现
- en: Now that we know the fundamental concepts of the K-Means algorithm, it's time
    to implement a Python class. The packages used were Numpy for mathematical calculations,
    Matplotlib for visualization, and the Make_blobs package from Sklearn for simulated
    data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了K-Means算法的基本概念，是时候实现一个Python类了。所使用的包包括Numpy进行数学计算、Matplotlib进行可视化，以及Sklearn中的Make_blobs包用于生成模拟数据。
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The class will have the following methods:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 该类将具有以下方法：
- en: '**Init method**'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**初始化方法**'
- en: 'A constructor method to initialize the basic parameters of the algorithm: the
    value *k* of clusters, the maximum number of iterations *max_iter,* and the tolerance
    *tol* value to interrupt the optimization when there is no significant improvement.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一个构造方法，用于初始化算法的基本参数：聚类的值*k*，最大迭代次数*max_iter*，以及当没有显著改善时中止优化的容忍度*tol*。
- en: '**Helper functions**'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**辅助函数**'
- en: These methods aim to assist the optimization process during training, such as
    calculating the Euclidean distance, randomly choosing the initial centroids, assigning
    the closest centroid to each point, updating the centroids’ values, and verifying
    whether the optimization converged.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法旨在协助训练过程中的优化，如计算欧几里得距离、随机选择初始质心、将每个点分配给最近的质心、更新质心的值以及验证优化是否收敛。
- en: '**Fit and predict method**'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**拟合与预测方法**'
- en: As mentioned earlier, the K-Means algorithm is an unsupervised learning technique,
    meaning it does not require labeled data during the training process. That way,
    it's necessary a single method to fit the data and predict to which cluster each
    data point belongs.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，K-Means算法是一种无监督学习技术，这意味着它在训练过程中不需要标签数据。这样，需要一种单一的方法来拟合数据并预测每个数据点所属的聚类。
- en: '**Total error method**'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**总误差方法**'
- en: A method to evaluate the quality of the optimization by calculating the *total
    squared error* of the optimization. That will be explored in the next section.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 评估优化质量的方法，通过计算优化的*总平方误差*。这将在下一节中详细探讨。
- en: 'Here it goes the full code:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是完整的代码：
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 4\. Evaluation and Interpretation
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 评估与解释
- en: Now we'll use the K-Means class to cluster simulated data. To do that, the make_blobs
    package from the Sklearn library will be used. The data consists of 500 two-dimensional
    points with 4 fixed centers.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用K-Means类对模拟数据进行聚类。为此，将使用Sklearn库中的make_blobs包。数据由500个二维点组成，具有4个固定的中心。
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/0c6ff01bc9de6ffa2c0a4b553b15d86e.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c6ff01bc9de6ffa2c0a4b553b15d86e.png)'
- en: Simulated data (image by the author).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟数据（作者提供的图片）。
- en: After performing the training using four clusters, we achieve the following
    result.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用四个聚类进行训练后，我们得到了以下结果。
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/2beea78e39fb0e4730512b9c829df0ad.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2beea78e39fb0e4730512b9c829df0ad.png)'
- en: Clustering for k=4 (image by the author).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: k=4的聚类（作者提供的图片）。
- en: In that case, the algorithm was capable of calculating the clusters successfully
    with 18 iterations. However, we must keep in mind that we already know the optimal
    number of clusters from the simulated data. In real-world applications, we often
    don't know that value.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，算法成功地通过18次迭代计算出了聚类。然而，我们必须记住，模拟数据已经知道了最优的聚类数。在实际应用中，我们通常不知道这个值。
- en: 'As said earlier, the K-Means algorithm aims to make the *within-cluster variance*
    as small as possible. The metric used to calculate that variance is the *total
    squared Euclidean distance* given by:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，K-Means算法的目标是尽量减小*类内方差*。用于计算该方差的度量是*总平方欧几里得距离*，计算公式为：
- en: '![](../Images/72661e266df6e85c3b4301407fe58c8c.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/72661e266df6e85c3b4301407fe58c8c.png)'
- en: Total squared Euclidean distance formula (image by the author using codecogs.com).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 总平方欧几里得距离公式（作者通过codecogs.com提供的图片）。
- en: 'where:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: p is the number of data points in a cluster;
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: p 是一个簇中的数据点数量；
- en: c_i is the centroid vector of a cluster;
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: c_i 是一个簇的质心向量；
- en: K is the number of clusters.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: K 是簇的数量。
- en: In words, the formula above adds up the distances of the data points to the
    nearest centroid. The error decreases as the number K increases.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，上面的公式将数据点到最近质心的距离相加。随着K值的增加，误差会减小。
- en: In the extreme case of K =N, you have one cluster for each data point and this
    error will be zero.
  id: totrans-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在极端情况下，当K = N时，你会为每个数据点创建一个簇，此时误差将为零。
- en: ''
  id: totrans-86
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Willmott, Paul (2019).
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Willmott, Paul (2019)。
- en: If we plot the error against the number of clusters and look at where the graph
    "bends", we'll be able to find the optimal number of clusters.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将误差与簇的数量绘制出来，并观察图形“弯曲”的位置，我们就能找到最佳簇数。
- en: '![](../Images/cb9e6cb88d7dfb252a6602ef6d826078.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb9e6cb88d7dfb252a6602ef6d826078.png)'
- en: Scree plot (image by the author).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 屏幕图（作者提供的图像）。
- en: As we can see, the plot has an "elbow shape" and it bends at K = 4, meaning
    that for greater values of K, the decrease in the total error will be less significant.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，图形呈现“肘部形状”，并在K = 4时弯曲，这意味着对于更大的K值，误差的减少将不再显著。
- en: 5\. Conclusions and Next Steps
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 结论和下一步
- en: In this article, we covered the fundamental concepts behind the K-Means algorithm,
    its uses, and applications. Also, using these concepts, we were able to implement
    a Python class from scratch that performed the clustering of simulated data and
    how to find the optimal value for K using a scree plot.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了K-Means算法背后的基本概念、用途和应用。此外，通过这些概念，我们能够从零开始实现一个Python类，执行模拟数据的聚类，并展示如何通过屏幕图找到K的最佳值。
- en: However, since we are dealing with an unsupervised technique, there is one additional
    step. The algorithm can successfully assign a label to the clusters, but the meaning
    of each label is a task that the data scientist or machine learning engineer will
    have to do by analyzing the data of each cluster.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于我们使用的是无监督技术，还有一个额外的步骤。算法可以成功地为簇分配标签，但每个标签的含义是数据科学家或机器学习工程师通过分析每个簇的数据来完成的任务。
- en: 'In addition, I''ll leave some points for further exploration:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我还将留下一些供进一步探索的要点：
- en: Our simulated data used two-dimensional points. Try to use the algorithm for
    other datasets and find the optimal values for K.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的模拟数据使用了二维点。尝试将该算法应用于其他数据集，并找到K的最佳值。
- en: There are other unsupervised learning algorithms widely used such as *Hierarchical
    Clustering*.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还有其他广泛使用的无监督学习算法，如*层次聚类*。
- en: Depending on the domain of the problem, it may be necessary to use other error
    metrics such as Manhattan distance and cosine similarity. Try to investigate them.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据问题领域，可能需要使用其他误差度量，如曼哈顿距离和余弦相似度。尝试研究它们。
- en: 'Complete code available [here](https://github.com/Marcussena/ML-and-Ai-from-scratch/tree/main/K-Means):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 完整代码可在[此处](https://github.com/Marcussena/ML-and-Ai-from-scratch/tree/main/K-Means)获取：
- en: '[](https://github.com/Marcussena/ML-and-Ai-from-scratch/tree/main/K-Means?source=post_page-----065bc42637e4--------------------------------)
    [## ML-and-Ai-from-scratch/K-Means at main · Marcussena/ML-and-Ai-from-scratch'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/Marcussena/ML-and-Ai-from-scratch/tree/main/K-Means?source=post_page-----065bc42637e4--------------------------------)
    [## ML-and-Ai-from-scratch/K-Means 在 main · Marcussena/ML-and-Ai-from-scratch'
- en: Python implementation of machine learning and AI algorithms from scratch - ML-and-Ai-from-scratch/K-Means
    at main ·…
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从零开始实现的机器学习和人工智能算法 - ML-and-Ai-from-scratch/K-Means 在 main ·…
- en: github.com](https://github.com/Marcussena/ML-and-Ai-from-scratch/tree/main/K-Means?source=post_page-----065bc42637e4--------------------------------)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/Marcussena/ML-and-Ai-from-scratch/tree/main/K-Means?source=post_page-----065bc42637e4--------------------------------)
- en: Please feel free to use and improve the code, comment, make suggestions, and
    connect with me on [LinkedIn](https://www.linkedin.com/in/marcus-sena-660198150/),
    [X](https://twitter.com/MarcusMVLS), and [Github](https://github.com/Marcussena/ML-and-Ai-from-scratch).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 请随时使用和改进代码、评论、提出建议，并通过[LinkedIn](https://www.linkedin.com/in/marcus-sena-660198150/)、[X](https://twitter.com/MarcusMVLS)和[Github](https://github.com/Marcussena/ML-and-Ai-from-scratch)与我联系。
- en: References
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Sebastian Raschka (2015), Python Machine Learning.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Sebastian Raschka (2015), 《Python机器学习》。'
- en: '[2] Willmott, Paul. (2019). *Machine Learning: An Applied Mathematics Introduction*.
    Panda Ohana Publishing.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Willmott, Paul. (2019). *机器学习：应用数学导论*。Panda Ohana 出版社。'
- en: '[3] Géron, A. (2017). *Hands-On Machine Learning*. O’Reilly Media Inc.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Géron, A. (2017). *动手学机器学习*。O’Reilly Media Inc.'
- en: '[4] Grus, Joel. (2015). *Data Science from Scratch*. O’Reilly Media Inc.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Grus, Joel. (2015). *从零开始学数据科学*. O''Reilly Media Inc.'
