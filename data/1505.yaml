- en: Benchmarking LLM Inference Backends
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基准测试 LLM 推理后端
- en: 原文：[https://towardsdatascience.com/benchmarking-llm-inference-backends-6c8ae46e72e4?source=collection_archive---------0-----------------------#2024-06-17](https://towardsdatascience.com/benchmarking-llm-inference-backends-6c8ae46e72e4?source=collection_archive---------0-----------------------#2024-06-17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/benchmarking-llm-inference-backends-6c8ae46e72e4?source=collection_archive---------0-----------------------#2024-06-17](https://towardsdatascience.com/benchmarking-llm-inference-backends-6c8ae46e72e4?source=collection_archive---------0-----------------------#2024-06-17)
- en: Comparing Llama 3 serving performance on vLLM, LMDeploy, MLC-LLM, TensorRT-LLM,
    and TGI
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较 Llama 3 在 vLLM、LMDeploy、MLC-LLM、TensorRT-LLM 和 TGI 上的服务性能
- en: '[](https://medium.com/@ssheng?source=post_page---byline--6c8ae46e72e4--------------------------------)[![Sean
    Sheng](../Images/ae58cf760ce5c482e7a6614995b8b8e1.png)](https://medium.com/@ssheng?source=post_page---byline--6c8ae46e72e4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6c8ae46e72e4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6c8ae46e72e4--------------------------------)
    [Sean Sheng](https://medium.com/@ssheng?source=post_page---byline--6c8ae46e72e4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ssheng?source=post_page---byline--6c8ae46e72e4--------------------------------)[![Sean
    Sheng](../Images/ae58cf760ce5c482e7a6614995b8b8e1.png)](https://medium.com/@ssheng?source=post_page---byline--6c8ae46e72e4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6c8ae46e72e4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6c8ae46e72e4--------------------------------)
    [Sean Sheng](https://medium.com/@ssheng?source=post_page---byline--6c8ae46e72e4--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6c8ae46e72e4--------------------------------)
    ·10 min read·Jun 17, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6c8ae46e72e4--------------------------------)
    ·阅读时间 10 分钟 ·2024年6月17日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Choosing the right inference backend for serving large language models (LLMs)
    is crucial. It not only ensures an optimal user experience with fast generation
    speed but also improves cost efficiency through a high token generation rate and
    resource utilization. Today, developers have a variety of choices for inference
    backends created by reputable research and industry teams. However, selecting
    the best backend for a specific use case can be challenging.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 为大语言模型（LLM）选择合适的推理后端至关重要。它不仅能确保通过快速生成速度提供最佳的用户体验，还能通过高令牌生成率和资源利用率提高成本效益。如今，开发人员有多种选择，可以选择由知名研究和行业团队创建的推理后端。然而，选择适合特定用例的最佳后端可能是一个挑战。
- en: 'To help developers make informed decisions, the [BentoML](https://bentoml.com/)
    engineering team conducted a comprehensive benchmark study on the Llama 3 serving
    performance with [vLLM](https://github.com/vllm-project/vllm), [LMDeploy](https://github.com/InternLM/lmdeploy),
    [MLC-LLM](https://github.com/mlc-ai/mlc-llm), [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM),
    and [Hugging Face TGI](https://github.com/huggingface/text-generation-inference)
    on [BentoCloud](https://cloud.bentoml.com/). These inference backends were evaluated
    using two key metrics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助开发人员做出明智的决策，[BentoML](https://bentoml.com/) 工程团队对 Llama 3 的服务性能进行了全面的基准测试，涉及的推理后端包括
    [vLLM](https://github.com/vllm-project/vllm)、[LMDeploy](https://github.com/InternLM/lmdeploy)、[MLC-LLM](https://github.com/mlc-ai/mlc-llm)、[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)
    和 [Hugging Face TGI](https://github.com/huggingface/text-generation-inference)，测试是在
    [BentoCloud](https://cloud.bentoml.com/) 上进行的。这些推理后端通过两个关键指标进行了评估：
- en: '**Time to First Token (TTFT)**: Measures the time from when a request is sent
    to when the first token is generated, recorded in milliseconds. TTFT is important
    for applications requiring immediate feedback, such as interactive chatbots. Lower
    latency improves perceived performance and user satisfaction.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**首次令牌生成时间 (TTFT)**：衡量从发送请求到生成第一个令牌所需的时间，单位为毫秒。TTFT 对于需要即时反馈的应用程序至关重要，例如互动聊天机器人。更低的延迟可以提高感知性能和用户满意度。'
- en: '**Token Generation Rate**: Assesses how many tokens the model generates per
    second during decoding, measured in tokens per second. The token generation rate
    is an indicator of the model’s capacity to handle high loads. A higher rate suggests
    that the model can efficiently manage multiple requests and generate responses
    quickly, making it suitable for high-concurrency environments.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**令牌生成率**：评估模型在解码过程中每秒生成多少令牌，单位为令牌每秒。令牌生成率是模型处理高负载能力的指标。更高的生成率表明模型能够高效地处理多个请求并迅速生成响应，适用于高并发环境。'
- en: Key benchmark findings
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主要基准结果
- en: 'We conducted the benchmark study with the Llama 3 8B and 70B 4-bit quantization
    models on an A100 80GB GPU instance (`gpu.a100.1x80`) on BentoCloud across three
    levels of inference loads (10, 50, and 100 concurrent users). Here are some of
    our key findings:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在BentoCloud上的A100 80GB GPU实例（`gpu.a100.1x80`）上，使用Llama 3 8B和70B 4位量化模型进行基准测试，测试了三个不同的推理负载级别（10、50和100个并发用户）。以下是我们的一些关键发现：
- en: Llama 3 8B
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Llama 3 8B
- en: '![](../Images/52a24358b3da84dea6157b08563068d1.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52a24358b3da84dea6157b08563068d1.png)'
- en: 'Llama 3 8B: Time to First Token (TTFT) of Different Backends'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 3 8B：不同后端的首个令牌时间（TTFT）
- en: '![](../Images/7505a27c1f9720916db37986a19d329b.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7505a27c1f9720916db37986a19d329b.png)'
- en: 'Llama 3 8B: Token Generation Rate of Different Backends'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 3 8B：不同后端的令牌生成速率
- en: '**LMDeploy**: Delivered the best decoding performance in terms of token generation
    rate, with up to 4000 tokens per second for 100 users. Achieved best-in-class
    TTFT with 10 users. Although TTFT gradually increases with more users, it remains
    low and consistently ranks among the best.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LMDeploy**：在令牌生成速率方面提供了最佳的解码性能，对于100个用户，每秒最多可生成4000个令牌。对于10个用户，达到了业内最佳的TTFT。尽管随着用户数量的增加，TTFT逐渐上升，但它仍然保持在较低水平，并始终位居最佳行列。'
- en: '**MLC-LLM:** Delivered similar decoding performance to LMDeploy with 10 users.
    Achieved best-in-class TTFT with 10 and 50 users. However, it struggles to maintain
    that efficiency under very high loads. When concurrency increases to 100 users,
    the decoding speed and TFTT does not keep up with LMDeploy.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLC-LLM**：在10个用户下提供了与LMDeploy相似的解码性能。对于10个和50个用户，达到了业内最佳的TTFT。然而，在非常高的负载下，它难以保持这种效率。当并发用户数增加到100时，解码速度和TTFT未能跟上LMDeploy的表现。'
- en: '**vLLM:** Achieved best-in-class TTFT across all levels of concurrent users.
    But decoding performance is less optimal compared to LMDeploy and MLC-LLM, with
    2300–2500 tokens per second similar to TGI and TRT-LLM.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**vLLM**：在所有并发用户级别上实现了业内最佳的TTFT。但与LMDeploy和MLC-LLM相比，解码性能较差，令牌生成速率为每秒2300-2500个令牌，类似于TGI和TRT-LLM。'
- en: Llama 3 70B with 4-bit quantization
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Llama 3 70B 4位量化
- en: '![](../Images/efb3a288fc02e5f49173f29b47439a12.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/efb3a288fc02e5f49173f29b47439a12.png)'
- en: 'Llama 3 70B Q4: Time to First Token (TTFT) of Different Backends'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 3 70B Q4：不同后端的首个令牌时间（TTFT）
- en: '![](../Images/7ec6d6cce672cf861f2a9c728f7d3604.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ec6d6cce672cf861f2a9c728f7d3604.png)'
- en: 'Llama 3 70B Q4: Token Generate Rate for Different Backends'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 3 70B Q4：不同后端的令牌生成速率
- en: '**LMDeploy:** Delivered the best token generation rate with up to 700 tokens
    when serving 100 users while keeping the lowest TTFT across all levels of concurrent
    users.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LMDeploy**：在服务100个用户时，提供了最佳的令牌生成速率，每秒最多可生成700个令牌，同时在所有并发用户级别中保持了最低的TTFT。'
- en: '**TensorRT-LLM:** Exhibited similar performance to LMDeploy in terms of token
    generation rate and maintained low TTFT at a low concurrent user count. However,
    TTFT increased significantly to over 6 seconds when concurrent users reach 100.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorRT-LLM**：在令牌生成速率方面展现出与LMDeploy相似的表现，并且在低并发用户数下维持了较低的TTFT。然而，当并发用户数达到100时，TTFT显著增加，超过了6秒。'
- en: '**vLLM:** Demonstrated consistently low TTFT across all levels of concurrent
    users, similar to what we observed with the 8B model. Exhibited a lower token
    generation rate compared to LMDeploy and TensorRT-LLM, likely due to a lack of
    inference optimization for quantized models.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**vLLM**：在所有并发用户级别下展现了一贯的低TTFT，类似于我们在8B模型中观察到的情况。与LMDeploy和TensorRT-LLM相比，vLLM的令牌生成速率较低，这可能是由于缺乏针对量化模型的推理优化。'
- en: We discovered that the token generation rate is strongly correlated with the
    GPU utilization achieved by an inference backend. Backends capable of maintaining
    a high token generation rate also exhibited GPU utilization rates approaching
    100%. Conversely, backends with lower GPU utilization rates appeared to be bottlenecked
    by the Python process.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现令牌生成速率与推理后端实现的GPU利用率密切相关。能够维持高令牌生成速率的后端也表现出接近100%的GPU利用率。相反，GPU利用率较低的后端似乎在Python进程中存在瓶颈。
- en: Beyond performance
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 除了性能之外
- en: When choosing an inference backend for serving LLMs, considerations beyond just
    performance also play an important role in the decision. The following list highlights
    key dimensions that we believe are important to consider when selecting the ideal
    inference backend.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择推理后端为LLM提供服务时，除了性能之外，其他因素也在决策中扮演着重要角色。以下列表突出了我们认为在选择理想推理后端时应考虑的关键维度。
- en: Quantization
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 量化
- en: Quantization trades off precision for performance by representing weights with
    lower-bit integers. This technique, combined with optimizations from inference
    backends, enables faster inference and a smaller memory footprint. As a result,
    we were able to load the weights of the 70B parameter Llama 3 model on a single
    A100 80GB GPU, whereas multiple GPUs would otherwise be necessary.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 量化通过使用较低位数的整数表示权重，在性能和精度之间进行权衡。这种技术结合推理后端的优化，使得推理速度更快，并且占用更少的内存。因此，我们能够在单个 A100
    80GB GPU 上加载 70B 参数的 Llama 3 模型的权重，而在没有量化的情况下，通常需要多个 GPU。
- en: '**LMDeploy**: Supports 4-bit AWQ, 8-bit quantization, and 4-bit KV quantization.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LMDeploy**: 支持 4 位 AWQ、8 位量化和 4 位 KV 量化。'
- en: '**vLLM**: Not fully supported as of now. Users need to quantize the model through
    AutoAWQ or find pre-quantized models on Hugging Face. Performance is under-optimized.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**vLLM**: 目前尚不完全支持。用户需要通过 AutoAWQ 对模型进行量化，或者在 Hugging Face 上找到预量化的模型。性能尚未优化。'
- en: '**TensorRT-LLM**: [Supports quantization via modelopt](https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/quantization/README.md#ptq-post-training-quantization),
    and note that quantized data types are not implemented for all the models.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorRT-LLM**: [通过 modelopt 支持量化](https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/quantization/README.md#ptq-post-training-quantization)，请注意并非所有模型都实现了量化数据类型。'
- en: '**TGI**: Supports AWQ, GPTQ and bits-and-bytes quantization'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TGI**: 支持 AWQ、GPTQ 和 bits-and-bytes 量化'
- en: '**MLC-LLM**: Supports 3-bit and 4-bit group quantization. AWQ quantization
    support is still experimental.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLC-LLM**: 支持 3 位和 4 位分组量化。AWQ 量化支持仍处于实验阶段。'
- en: Model architectures
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型架构
- en: Being able to leverage the same inference backend for different model architectures
    offers agility for engineering teams. It allows them to switch between various
    large language models as new improvements emerge, without needing to migrate the
    underlying inference infrastructure.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 能够在不同的模型架构之间使用相同的推理后端，为工程团队提供了灵活性。这使得他们可以随着新改进的出现，在各种大型语言模型之间切换，而无需迁移底层推理基础设施。
- en: '**LMDeploy**: [About 20 models supported](https://github.com/InternLM/lmdeploy/blob/main/docs/en/supported_models/supported_models.md)
    by TurboMind engine. Models that require sliding window attention, e.g. Mistral,
    Qwen 1.5, are not fully supported as of now.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LMDeploy**: [TurboMind 引擎支持的 20 多种模型](https://github.com/InternLM/lmdeploy/blob/main/docs/en/supported_models/supported_models.md)。目前，像
    Mistral、Qwen 1.5 这样的需要滑动窗口注意力的模型尚不完全支持。'
- en: '**vLLM**: [30+ models supported](https://docs.vllm.ai/en/latest/models/supported_models.html)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**vLLM**: [支持 30 多种模型](https://docs.vllm.ai/en/latest/models/supported_models.html)'
- en: '**TensorRT-LLM**: [30+ models supported](https://nvidia.github.io/TensorRT-LLM/reference/support-matrix.html)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorRT-LLM**: [支持 30 多种模型](https://nvidia.github.io/TensorRT-LLM/reference/support-matrix.html)'
- en: '**TGI**: [20+ models supported](https://huggingface.co/docs/text-generation-inference/en/supported_models)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TGI**: [支持 20 多种模型](https://huggingface.co/docs/text-generation-inference/en/supported_models)'
- en: '**MLC-LLM**: [20+ models supported](https://github.com/mlc-ai/mlc-llm/tree/main/python/mlc_llm/model)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLC-LLM**: [支持 20 多种模型](https://github.com/mlc-ai/mlc-llm/tree/main/python/mlc_llm/model)'
- en: Hardware limitations
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 硬件限制
- en: Having the ability to run on different hardware provides cost savings and the
    flexibility to select the appropriate hardware based on inference requirements.
    It also offers alternatives during the current GPU shortage, helping to navigate
    supply constraints effectively.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 能够在不同的硬件上运行提供了成本节约，并且能够根据推理需求选择合适的硬件。同时，在当前 GPU 短缺的情况下，它也提供了替代方案，有效地帮助解决供应瓶颈问题。
- en: '**LMDeploy**: Only optimized for Nvidia CUDA'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LMDeploy**: 仅针对 Nvidia CUDA 进行了优化'
- en: '**vLLM**: Nvidia CUDA, AMD ROCm, AWS Neuron, CPU'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**vLLM**: 支持 Nvidia CUDA，AMD ROCm，AWS Neuron，CPU'
- en: '**TensorRT-LLM**: Only supports Nvidia CUDA'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorRT-LLM**: 仅支持 Nvidia CUDA'
- en: '**TGI**: Nvidia CUDA, AMD ROCm, Intel Gaudi, AWS Inferentia'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TGI**: Nvidia CUDA，AMD ROCm，Intel Gaudi，AWS Inferentia'
- en: '**MLC-LLM**: Nvidia CUDA, AMD ROCm, Metal, Android, IOS, WebGPU'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLC-LLM**: 支持 Nvidia CUDA，AMD ROCm，Metal，Android，IOS，WebGPU'
- en: Developer experience
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发者体验
- en: An inference backend designed for production environments should provide stable
    releases and facilitate simple workflows for continuous deployment. Additionally,
    a developer-friendly backend should feature well-defined interfaces that support
    rapid development and high code maintainability, essential for building AI applications
    powered by LLMs.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为生产环境设计的推理后端应提供稳定的版本发布，并便于简化的持续部署工作流。此外，开发者友好的后端应具有明确定义的接口，支持快速开发和高代码可维护性，这对于构建由大型语言模型（LLMs）驱动的
    AI 应用至关重要。
- en: '**Stable releases**: LMDeploy, TensorRT-LLM, vLLM, and TGI all offer stable
    releases. MLC-LLM does not currently have stable tagged releases, with only nightly
    builds; one possible solution is to build from source.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稳定版本**：LMDeploy、TensorRT-LLM、vLLM和TGI都提供了稳定版本。MLC-LLM目前没有稳定的标记版本，只有夜间构建版本；一种可能的解决方案是从源代码构建。'
- en: '**Model compilation**: TensorRT-LLM and MLC-LLM require an explicit model compilation
    step before the inference backend is ready. This step could potentially introduce
    additional cold-start delays during deployment.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型编译**：TensorRT-LLM和MLC-LLM在推理后端准备就绪之前，需要进行显式的模型编译步骤。这个步骤可能在部署时引入额外的冷启动延迟。'
- en: '**Documentation**: LMDeploy, vLLM, and TGI were all easy to learn with their
    comprehensive documentation and examples. MLC-LLM presented a moderate learning
    curve, primarily due to the necessity of understanding the model compilation steps.
    TensorRT-LLM was the most challenging to set up in our benchmark test. Without
    enough quality examples, we had to read through the documentation of TensorRT-LLM,
    *tensorrtllm_backend* and Triton Inference Server, convert the checkpoints, build
    the TRT engine, and write a lot of configurations.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档**：LMDeploy、vLLM和TGI都具有易于学习的文档和示例。MLC-LLM的学习曲线适中，主要是因为需要理解模型编译步骤。TensorRT-LLM是在我们的基准测试中设置最具挑战性的。由于缺乏足够的优质示例，我们不得不阅读TensorRT-LLM、*tensorrtllm_backend*和Triton
    Inference Server的文档，转换检查点，构建TRT引擎，并编写大量配置。'
- en: Concepts
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概念
- en: Llama 3
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Llama 3
- en: '[Llama 3](https://ai.meta.com/blog/meta-llama-3/) is the latest iteration in
    the Llama LLM series, available in various configurations. We used the following
    model sizes in our benchmark tests.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[Llama 3](https://ai.meta.com/blog/meta-llama-3/)是Llama LLM系列的最新迭代，提供多种配置。我们在基准测试中使用了以下模型大小。'
- en: '**8B**: This model has 8 billion parameters, making it powerful yet manageable
    in terms of computational resources. Using FP16, it requires about 16GB of RAM
    (excluding KV cache and other overheads), fitting on a single A100–80G GPU instance.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**8B**：该模型具有80亿个参数，使其在计算资源方面既强大又易于管理。使用FP16时，它大约需要16GB的内存（不包括KV缓存和其他开销），可以在单个A100–80G
    GPU实例上运行。'
- en: '**70B 4-bit Quantization**: This 70 billion parameter model, when quantized
    to 4 bits, significantly reduces its memory footprint. Quantization compresses
    the model by reducing the bits per parameter, providing faster inference and lowering
    memory usage with minimal performance loss. With 4-bit AWQ quantization, it requires
    approximately 37GB of RAM for loading model weights, fitting on a single A100–80G
    instance. Serving quantized weights on a single GPU device typically achieves
    the best throughput of a model compared to serving on multiple devices.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**70B 4位量化**：该70亿参数模型经过4位量化后，显著减少了其内存占用。量化通过减少每个参数的位数来压缩模型，提供更快的推理速度，并在性能损失最小的情况下降低内存使用。使用4位AWQ量化时，加载模型权重大约需要37GB内存，可以在单个A100–80G实例上运行。在单个GPU设备上提供量化权重通常能够实现模型的最佳吞吐量，相较于在多个设备上提供。'
- en: Inference platform
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理平台
- en: We ensured that the inference backends served with [BentoML](https://github.com/bentoml/BentoML)
    added only minimal performance overhead compared to serving natively in Python.
    The overhead is due to the provision of functionality for scaling, observability,
    and IO serialization. Using BentoML and [BentoCloud](https://bentoml.com/) gave
    us a consistent RESTful API for the different inference backends, simplifying
    benchmark setup and operations.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确保使用[BentoML](https://github.com/bentoml/BentoML)提供的推理后端相比原生Python推理仅增加了最小的性能开销。这个开销源于提供了用于扩展、可观察性和IO序列化的功能。使用BentoML和[BentoCloud](https://bentoml.com/)为不同的推理后端提供了统一的RESTful
    API，简化了基准测试的设置和操作。
- en: Inference backends
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理后端
- en: Different backends provide various ways to serve LLMs, each with unique features
    and optimization techniques. All of the inference backends we tested are under
    Apache 2.0 License.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的后端提供了多种服务LLM的方式，每种方式都有其独特的功能和优化技术。我们测试的所有推理后端都遵循Apache 2.0许可证。
- en: '[LMDeploy](https://github.com/InternLM/lmdeploy): An inference backend focusing
    on delivering high decoding speed and efficient handling of concurrent requests.
    It supports various quantization techniques, making it suitable for deploying
    large models with reduced memory requirements.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LMDeploy](https://github.com/InternLM/lmdeploy)：一个推理后端，专注于提供高解码速度和高效处理并发请求。它支持各种量化技术，适合部署具有较低内存要求的大型模型。'
- en: '[vLLM](https://github.com/vllm-project/vllm): A high-performance inference
    engine optimized for serving LLMs. It is known for its efficient use of GPU resources
    and fast decoding capabilities.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[vLLM](https://github.com/vllm-project/vllm)：一个高性能的推理引擎，专门优化用于服务LLM。它因高效利用GPU资源和快速解码能力而闻名。'
- en: '[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM): An inference backend
    that leverages NVIDIA’s TensorRT, a high-performance deep learning inference library.
    It is optimized for running large models on NVIDIA GPUs, providing fast inference
    and support for advanced optimizations like quantization.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)：一个推理后端，利用NVIDIA的TensorRT，一个高性能的深度学习推理库。它优化了在NVIDIA
    GPU上运行大模型，提供快速推理并支持诸如量化等高级优化。'
- en: '[Hugging Face Text Generation Inference (TGI)](https://github.com/huggingface/text-generation-inference):
    A toolkit for deploying and serving LLMs. It is used in production at Hugging
    Face to power Hugging Chat, the Inference API and Inference Endpoint.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hugging Face文本生成推理（TGI）](https://github.com/huggingface/text-generation-inference)：一个用于部署和服务LLM的工具包。它在Hugging
    Face的生产环境中用于驱动Hugging Chat、推理API和推理端点。'
- en: '[MLC-LLM](https://github.com/mlc-ai/mlc-llm): An ML compiler and high-performance
    deployment engine for LLMs. It is built on top of Apache TVM and requires compilation
    and weight conversion before serving models.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MLC-LLM](https://github.com/mlc-ai/mlc-llm)：一个为LLM量身定制的ML编译器和高性能部署引擎。它建立在Apache
    TVM之上，在服务模型之前需要进行编译和权重转换。'
- en: Integrating BentoML with various inference backends to self-host LLMs is straightforward.
    The BentoML community provides the following example projects on GitHub to guide
    you through the process.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 将BentoML与各种推理后端集成以自托管LLM是非常简单的。BentoML社区在GitHub上提供了以下示例项目，帮助你完成整个过程。
- en: 'vLLM: [https://github.com/bentoml/BentoVLLM](https://github.com/bentoml/BentoVLLM)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'vLLM: [https://github.com/bentoml/BentoVLLM](https://github.com/bentoml/BentoVLLM)'
- en: 'MLC-LLM: [https://github.com/bentoml/BentoMLCLLM](https://github.com/bentoml/BentoMLCLLM)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'MLC-LLM: [https://github.com/bentoml/BentoMLCLLM](https://github.com/bentoml/BentoMLCLLM)'
- en: 'LMDeploy: [https://github.com/bentoml/BentoLMDeploy](https://github.com/bentoml/BentoLMDeploy)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LMDeploy: [https://github.com/bentoml/BentoLMDeploy](https://github.com/bentoml/BentoLMDeploy)'
- en: 'TRT-LLM: [https://github.com/bentoml/BentoTRTLLM](https://github.com/bentoml/BentoTRTLLM)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'TRT-LLM: [https://github.com/bentoml/BentoTRTLLM](https://github.com/bentoml/BentoTRTLLM)'
- en: 'TGI: [https://github.com/bentoml/BentoTGI](https://github.com/bentoml/BentoTGI)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'TGI: [https://github.com/bentoml/BentoTGI](https://github.com/bentoml/BentoTGI)'
- en: Benchmark setup
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基准测试设置
- en: Models
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型
- en: We tested both the Meta-Llama-3–8B-Instruct and Meta-Llama-3–70B-Instruct 4-bit
    quantization models. For the 70B model, we performed 4-bit quantization so that
    it could run on a single A100–80G GPU. If the inference backend supports native
    quantization, we used the inference backend-provided quantization method. For
    example, for MLC-LLM, we used the `q4f16_1` quantization scheme. Otherwise, we
    used the AWQ-quantized `casperhansen/llama-3-70b-instruct-awq` model from Hugging
    Face.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们测试了Meta-Llama-3–8B-Instruct和Meta-Llama-3–70B-Instruct的4位量化模型。对于70B模型，我们进行了4位量化，使其能够在单个A100–80G
    GPU上运行。如果推理后端支持原生量化，我们使用推理后端提供的量化方法。例如，对于MLC-LLM，我们使用了`q4f16_1`量化方案。否则，我们使用了来自Hugging
    Face的AWQ量化`casperhansen/llama-3-70b-instruct-awq`模型。
- en: Note that other than enabling common inference optimization techniques, such
    as continuous batching, flash attention, and prefix caching, we did not fine-tune
    the inference configurations (GPU memory utilization, max number of sequences,
    paged KV cache block size, etc.) for each individual backend. This is because
    this approach is not scalable as the number of LLMs we serve gets larger. Providing
    an optimal set of inference parameters is an implicit measure of performance and
    ease-of-use of the backends.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，除了启用常见的推理优化技术，如连续批处理、闪存注意力和前缀缓存外，我们没有为每个独立的后端微调推理配置（GPU内存使用、最大序列数、分页KV缓存块大小等）。这是因为随着我们服务的LLM数量的增加，这种方法不可扩展。提供一组最优的推理参数是后端性能和易用性的隐性衡量标准。
- en: Benchmark client
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基准客户端
- en: To accurately assess the performance of different LLM backends, we created a
    custom benchmark script. This script simulates real-world scenarios by varying
    user loads and sending generation requests under different levels of concurrency.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准确评估不同LLM后端的性能，我们创建了一个自定义基准测试脚本。该脚本通过变化用户负载并在不同的并发级别下发送生成请求，模拟了实际场景。
- en: Our benchmark client can spawn up to the target number of users within 20 seconds,
    after which it stress tests the LLM backend by sending concurrent generation requests
    with randomly selected prompts. We tested with 10, 50, and 100 concurrent users
    to evaluate the system under varying loads.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的基准测试客户端可以在20秒内启动目标用户数量，之后通过发送带有随机选择提示词的并发生成请求来对LLM后端进行压力测试。我们测试了10、50和100个并发用户，以评估系统在不同负载下的表现。
- en: Each stress test ran for 5 minutes, during which time we collected inference
    metrics every 5 seconds. This duration was sufficient to observe potential performance
    degradation, resource utilization bottlenecks, or other issues that might not
    be evident in shorter tests.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 每次压力测试持续5分钟，在此期间我们每隔5秒收集一次推理指标。这个持续时间足以观察到潜在的性能下降、资源利用瓶颈或其他在短时间测试中可能未能显现的问题。
- en: For more information, see [the source code of our benchmark client](https://github.com/bentoml/llm-bench).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息，请参见[我们的基准测试客户端的源代码](https://github.com/bentoml/llm-bench)。
- en: Prompt dataset
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示词数据集
- en: The prompts for our tests were derived from the [databricks-dolly-15k dataset](https://github.com/bentoml/openllm-bench/blob/main/common.py#L294).
    For each test session, we randomly selected prompts from this dataset. We also
    tested text generation with and without system prompts. Some backends might have
    additional optimizations regarding common system prompt scenarios by enabling
    prefix caching.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们测试的提示词来自[databricks-dolly-15k数据集](https://github.com/bentoml/openllm-bench/blob/main/common.py#L294)。在每次测试会话中，我们从该数据集中随机选择提示词。我们还测试了有无系统提示词的文本生成。一些后端可能通过启用前缀缓存来优化常见的系统提示词场景。
- en: Library versions
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 库版本
- en: '**BentoML**: 1.2.16'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BentoML**: 1.2.16'
- en: '**vLLM**: 0.4.2'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**vLLM**: 0.4.2'
- en: '**MLC-LLM**: mlc-llm-nightly-cu121 0.1.dev1251 (No stable release yet)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLC-LLM**: mlc-llm-nightly-cu121 0.1.dev1251（尚无稳定版）'
- en: '**LMDeploy**: 0.4.0'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LMDeploy**: 0.4.0'
- en: '**TensorRT-LLM**: 0.9.0 (with Triton v24.04)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorRT-LLM**: 0.9.0（与Triton v24.04一起使用）'
- en: '**TGI**: 2.0.4'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TGI**: 2.0.4'
- en: Recommendations
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推荐
- en: The field of LLM inference optimization is rapidly evolving and heavily researched.
    The best inference backend available today might quickly be surpassed by newcomers.
    Based on our benchmarks and usability studies conducted at the time of writing,
    we have the following recommendations for selecting the most suitable backend
    for Llama 3 models under various scenarios.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: LLM推理优化领域正在迅速发展并且受到广泛研究。今天可用的最佳推理后端可能很快会被新兴技术所超越。根据我们在撰写时进行的基准测试和可用性研究，我们有以下建议，帮助选择在各种场景下最适合Llama
    3模型的后端。
- en: Llama 3 8B
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Llama 3 8B
- en: For the Llama 3 8B model, **LMDeploy** consistently delivers low TTFT and the
    highest decoding speed across all user loads. Its ease of use is another significant
    advantage, as it can convert the model into TurboMind engine format on the fly,
    simplifying the deployment process. At the time of writing, LMDeploy offers limited
    support for models that utilize sliding window attention mechanisms, such as Mistral
    and Qwen 1.5.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Llama 3 8B模型，**LMDeploy**在所有用户负载下始终提供低TTFT和最高的解码速度。其易用性也是一个显著优势，因为它可以即时将模型转换为TurboMind引擎格式，从而简化部署过程。在撰写时，LMDeploy对使用滑动窗口注意力机制的模型（如Mistral和Qwen
    1.5）支持有限。
- en: '**vLLM** consistently maintains a low TTFT, even as user loads increase, making
    it suitable for scenarios where maintaining low latency is crucial. vLLM offers
    easy integration, extensive model support, and broad hardware compatibility, all
    backed by a robust open-source community.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**vLLM**即使在用户负载增加的情况下，也始终保持较低的TTFT，适合需要保持低延迟的场景。vLLM提供了易于集成、广泛的模型支持和广泛的硬件兼容性，所有这些都得到强大的开源社区的支持。'
- en: '**MLC-LLM** offers the lowest TTFT and maintains high decoding speeds at lower
    concurrent users. However, under very high user loads, MLC-LLM struggles to maintain
    top-tier decoding performance. Despite these challenges, MLC-LLM shows significant
    potential with its machine learning compilation technology. Addressing these performance
    issues and implementing a stable release could greatly enhance its effectiveness.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**MLC-LLM**提供了最低的TTFT，并在较低的并发用户情况下维持较高的解码速度。然而，在极高用户负载下，MLC-LLM在维持顶级解码性能方面表现较差。尽管面临这些挑战，MLC-LLM凭借其机器学习编译技术展现出巨大的潜力。如果能够解决这些性能问题并实施稳定的版本发布，将极大提升其效能。'
- en: Llama 3 70B 4-bit quantization
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Llama 3 70B 4位量化
- en: For the Llama 3 70B Q4 model, **LMDeploy** demonstrates impressive performance
    with the lowest TTFT across all user loads. It also maintains a high decoding
    speed, making it ideal for applications where both low latency and high throughput
    are essential. LMDeploy also stands out for its ease of use, as it can quickly
    convert models without the need for extensive setup or compilation, making it
    ideal for rapid deployment scenarios.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Llama 3 70B Q4模型，**LMDeploy**在所有用户负载下展现了卓越的性能，具有最低的TTFT。它还保持了较高的解码速度，适用于低延迟和高吞吐量都至关重要的应用场景。LMDeploy还以其易用性脱颖而出，因为它能够快速转换模型，无需大量的设置或编译，非常适合快速部署场景。
- en: '**TensorRT-LLM** matches LMDeploy in throughput, yet it exhibits less optimal
    latency for TTFT under high user load scenarios. Backed by Nvidia, we anticipate
    these gaps will be quickly addressed. However, its inherent requirement for model
    compilation and reliance on Nvidia CUDA GPUs are intentional design choices that
    may pose limitations during deployment.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**TensorRT-LLM**在吞吐量上与LMDeploy相匹配，但在高用户负载场景下，其TTFT延迟表现不如预期。得益于Nvidia的支持，我们预计这些差距将会得到迅速解决。然而，它对模型编译的固有需求以及对Nvidia
    CUDA GPU的依赖是有意的设计选择，这在部署过程中可能会带来一些限制。'
- en: '**vLLM** manages to maintain a low TTFT even as user loads increase, and its
    ease of use can be a significant advantage for many users. However, at the time
    of writing, the backend’s lack of optimization for AWQ quantization leads to less
    than optimal decoding performance for quantized models.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**vLLM**即使在用户负载增加的情况下，也能保持较低的TTFT，其易用性对许多用户来说是一个显著的优势。然而，在撰写本文时，后端缺乏对AWQ量化的优化，导致量化模型的解码性能不尽如人意。'
- en: Acknowledgements
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢
- en: The article and accompanying benchmarks were collaboratively with my esteemed
    colleagues, Rick Zhou, Larme Zhao, and Bo Jiang. All images presented in this
    article were created by the authors.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 本文及相关基准测试是与我尊敬的同事Rick Zhou、Larme Zhao和Bo Jiang共同合作完成的。本文中展示的所有图片均由作者创作。
