- en: 'DPO Full Training vs. LoRA: How Good is LoRA for DPO Training?'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DPO全量训练与LoRA：LoRA对DPO训练的效果如何？
- en: 原文：[https://towardsdatascience.com/dpo-full-training-vs-lora-how-good-is-lora-for-dpo-training-a1dd8e088d9d?source=collection_archive---------8-----------------------#2024-11-20](https://towardsdatascience.com/dpo-full-training-vs-lora-how-good-is-lora-for-dpo-training-a1dd8e088d9d?source=collection_archive---------8-----------------------#2024-11-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/dpo-full-training-vs-lora-how-good-is-lora-for-dpo-training-a1dd8e088d9d?source=collection_archive---------8-----------------------#2024-11-20](https://towardsdatascience.com/dpo-full-training-vs-lora-how-good-is-lora-for-dpo-training-a1dd8e088d9d?source=collection_archive---------8-----------------------#2024-11-20)
- en: One model, two adapters
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一种模型，两个适配器
- en: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--a1dd8e088d9d--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--a1dd8e088d9d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a1dd8e088d9d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a1dd8e088d9d--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--a1dd8e088d9d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--a1dd8e088d9d--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--a1dd8e088d9d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a1dd8e088d9d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a1dd8e088d9d--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--a1dd8e088d9d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a1dd8e088d9d--------------------------------)
    ·8 min read·Nov 20, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a1dd8e088d9d--------------------------------)
    ·8分钟阅读·2024年11月20日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/c4b983a03d2d4b6fdc348a6556fd8dcd.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4b983a03d2d4b6fdc348a6556fd8dcd.png)'
- en: Generated with Grok
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Grok生成
- en: There are various methods to align LLMs with human preferences. Beyond reinforcement
    learning with human feedback (RLHF), often seen as too resource-intensive for
    consistent application on newly fine-tuned models, Direct Preference Optimization
    (DPO) is one of the most popular alternatives for LLM alignment.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以使大型语言模型（LLM）与人类偏好对齐。除了人类反馈强化学习（RLHF），它通常被认为对于新调整过的模型应用过于资源密集，直接偏好优化（DPO）是LLM对齐中最受欢迎的替代方案之一。
- en: Although DPO is significantly more cost-effective than RLHF, it still requires
    a reference model in addition to the “policy” model (i.e., the model being actively
    trained). This means both models must be loaded into GPU memory simultaneously,
    which can be challenging for single-GPU configurations, especially with large
    models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管DPO比RLHF显著更具成本效益，但它仍然需要一个参考模型，除了“策略”模型（即正在积极训练的模型）。这意味着两个模型必须同时加载到GPU内存中，这对于单GPU配置来说可能具有挑战性，尤其是在大模型的情况下。
- en: A more memory-efficient approach would be to use LoRA for DPO training. Instead
    of training the entire model, we freeze its parameters and train a small adapter.
    This method becomes even more efficient if both the policy and reference models
    share the same base model; in that case, we load the base model once, then load
    a frozen adapter for the reference model and a trainable adapter for the policy
    model, significantly reducing memory requirements.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更节省内存的方式是使用LoRA进行DPO训练。我们冻结模型的参数并训练一个小的适配器，而不是训练整个模型。如果策略模型和参考模型共享相同的基础模型，那么这种方法会更加高效；在这种情况下，我们只需要加载一次基础模型，然后加载冻结的参考模型适配器和可训练的策略模型适配器，从而显著减少内存需求。
- en: However, the effect of LoRA on DPO’s performance is still understudied in my
    opinion. While LoRA can closely approximate full training, its performance…
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，个人认为LoRA对DPO性能的影响仍然没有得到充分研究。尽管LoRA可以较为接近全量训练，但它的表现…
