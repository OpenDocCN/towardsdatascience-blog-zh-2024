- en: Multimodal Models — LLMs That Can See and Hear
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多模态模型 — 可以“看”和“听”的LLM
- en: 原文：[https://towardsdatascience.com/multimodal-models-llms-that-can-see-and-hear-5c6737c981d3?source=collection_archive---------7-----------------------#2024-11-19](https://towardsdatascience.com/multimodal-models-llms-that-can-see-and-hear-5c6737c981d3?source=collection_archive---------7-----------------------#2024-11-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/multimodal-models-llms-that-can-see-and-hear-5c6737c981d3?source=collection_archive---------7-----------------------#2024-11-19](https://towardsdatascience.com/multimodal-models-llms-that-can-see-and-hear-5c6737c981d3?source=collection_archive---------7-----------------------#2024-11-19)
- en: An introduction with example Python code
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 包含示例Python代码的介绍
- en: '[](https://shawhin.medium.com/?source=post_page---byline--5c6737c981d3--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page---byline--5c6737c981d3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5c6737c981d3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5c6737c981d3--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page---byline--5c6737c981d3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://shawhin.medium.com/?source=post_page---byline--5c6737c981d3--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page---byline--5c6737c981d3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5c6737c981d3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5c6737c981d3--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page---byline--5c6737c981d3--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5c6737c981d3--------------------------------)
    ·9 min read·Nov 19, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5c6737c981d3--------------------------------)
    ·阅读时长9分钟·2024年11月19日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: This is the first post in a larger series on [Multimodal AI](https://shawhin.medium.com/list/multimodal-ai-fe9521d0e77a).
    A **Multimodal Model (MM)** is an **AI system capable of processing or generating
    multiple data modalities** (e.g., text, image, audio, video). In this article,
    I will discuss a particular type of MM that builds on top of a large language
    model (LLM). I’ll start with a high-level overview of such models and then share
    example code for using LLaMA 3.2 Vision to perform various image-to-text tasks.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这是关于[多模态AI](https://shawhin.medium.com/list/multimodal-ai-fe9521d0e77a)系列文章的第一篇。**多模态模型
    (MM)** 是一种**能够处理或生成多种数据模态的AI系统**（例如文本、图像、音频、视频）。在本文中，我将讨论一种特殊类型的MM，它建立在大型语言模型
    (LLM) 的基础上。我将从高层次概述此类模型开始，然后分享使用LLaMA 3.2 Vision执行各种图像到文本任务的示例代码。
- en: '![](../Images/27ab3af36cfa4d6886cc5244b94147db.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/27ab3af36cfa4d6886cc5244b94147db.png)'
- en: Photo by [Sincerely Media](https://unsplash.com/@sincerelymedia?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Sincerely Media](https://unsplash.com/@sincerelymedia?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '[Large language models (LLMs)](/a-practical-introduction-to-llms-65194dda1148)
    have marked a fundamental shift in AI research and development. However, despite
    their broader impacts, they are still **fundamentally limited**.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[大型语言模型 (LLMs)](/a-practical-introduction-to-llms-65194dda1148)在人工智能的研究与开发中标志着一次根本性的转变。然而，尽管它们的影响广泛，仍然**存在根本的局限性**。'
- en: Namely, LLMs can only process and generate text, making them *blind* to other
    modalities such as images, video, audio, and more. This is a major limitation
    since **some tasks rely on non-text data,** e.g., analyzing engineering blueprints,
    reading body language or speech tonality, and interpreting plots and infographics.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，LLMs 只能处理和生成文本，这使得它们对图像、视频、音频等其他模态*视而不见*。这是一个主要的限制，因为**一些任务依赖于非文本数据**，例如分析工程图纸、解读肢体语言或语音语调，以及解释图表和信息图。
