- en: 'The AI Developer’s Dilemma: Proprietary AI vs. Open Source Ecosystem'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI 开发者的困境：专有 AI 与开源生态系统
- en: 原文：[https://towardsdatascience.com/the-ai-developers-dilemma-proprietary-ai-vs-open-source-ecosystem-453ac735b760?source=collection_archive---------6-----------------------#2024-09-30](https://towardsdatascience.com/the-ai-developers-dilemma-proprietary-ai-vs-open-source-ecosystem-453ac735b760?source=collection_archive---------6-----------------------#2024-09-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-ai-developers-dilemma-proprietary-ai-vs-open-source-ecosystem-453ac735b760?source=collection_archive---------6-----------------------#2024-09-30](https://towardsdatascience.com/the-ai-developers-dilemma-proprietary-ai-vs-open-source-ecosystem-453ac735b760?source=collection_archive---------6-----------------------#2024-09-30)
- en: '![](../Images/f2eb22a33aaa38f31c7baa594e4409d1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f2eb22a33aaa38f31c7baa594e4409d1.png)'
- en: '*Image credit: Adobe Stock.*'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*图片来源：Adobe Stock。*'
- en: '***Fundamental choices impacting integration and deployment at scale of GenAI
    into businesses***'
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '***影响生成式人工智能（GenAI）大规模集成和部署的基本选择***'
- en: '[](https://gadi-singer.medium.com/?source=post_page---byline--453ac735b760--------------------------------)[![Gadi
    Singer](../Images/293941f11306a6e2100c2375ccb1a85a.png)](https://gadi-singer.medium.com/?source=post_page---byline--453ac735b760--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--453ac735b760--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--453ac735b760--------------------------------)
    [Gadi Singer](https://gadi-singer.medium.com/?source=post_page---byline--453ac735b760--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://gadi-singer.medium.com/?source=post_page---byline--453ac735b760--------------------------------)[![Gadi
    Singer](../Images/293941f11306a6e2100c2375ccb1a85a.png)](https://gadi-singer.medium.com/?source=post_page---byline--453ac735b760--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--453ac735b760--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--453ac735b760--------------------------------)
    [Gadi Singer](https://gadi-singer.medium.com/?source=post_page---byline--453ac735b760--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--453ac735b760--------------------------------)
    ·17 min read·Sep 30, 2024
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--453ac735b760--------------------------------)
    ·阅读时间 17 分钟·2024年9月30日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'Before a company or a developer adopts generative artificial intelligence (GenAI),
    they often wonder how to get business value from the integration of AI into their
    business. With this in mind, a fundamental question arises: Which approach will
    deliver the best value on investment — a large all-encompassing proprietary model
    or an open source AI model that can be molded and fine-tuned for a company’s needs?
    AI adoption strategies fall within a wide spectrum, from accessing a cloud service
    from a large proprietary frontier model like [OpenAI’s GPT-4o](https://openai.com/index/hello-gpt-4o/)
    to building an internal solution in the company’s compute environment with an
    open source small model using indexed company data for a targeted set of tasks.
    Current AI solutions go well beyond the model itself, with a whole ecosystem of
    retrieval systems, agents, and other functional components such as AI accelerators,
    which are beneficial for both large and small models. Emergence of cross-industry
    collaborations like the [Open Platform for Enterprise AI (OPEA)](https://opea.dev/)
    further the promise of streamlining the access and structuring of end-to-end open
    source solutions.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在公司或开发者采用生成式人工智能（GenAI）之前，他们常常会思考如何从 AI 集成到业务中获得商业价值。考虑到这一点，一个基本问题随之而来：哪种方式能提供最佳的投资回报——是采用一个大型、包罗万象的专有模型，还是使用一个可以根据公司需求进行塑造和微调的开源
    AI 模型？AI 采用策略范围广泛，从访问像 [OpenAI 的 GPT-4o](https://openai.com/index/hello-gpt-4o/)
    这样的专有前沿大模型的云服务，到在公司的计算环境中构建一个使用公司数据索引的小型开源模型来执行一组特定任务的内部解决方案。当前的 AI 解决方案远远超出了模型本身，还包括一个完整的生态系统，其中包含检索系统、代理以及其他功能组件，如
    AI 加速器，这些对于大模型和小模型都具有重要意义。跨行业合作的出现，如 [Open Platform for Enterprise AI (OPEA)](https://opea.dev/)，进一步推动了简化访问和构建端到端开源解决方案的承诺。
- en: This basic choice between the open source ecosystem and a proprietary setting
    impacts countless business and technical decisions, making it “the AI developer’s
    dilemma.” I believe that for most enterprise and other business deployments, it
    makes sense to initially use proprietary models to learn about AI’s potential
    and minimize early capital expenditure (CapEx). However, for broad sustained deployment,
    in many cases companies would use ecosystem-based open source targeted solutions,
    which allows for a cost-effective, adaptable strategy that aligns with evolving
    business needs and industry trends.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这种在开源生态系统和专有设置之间的基本选择会影响无数的商业和技术决策，使其成为“AI 开发者的困境”。我认为，对于大多数企业和其他商业部署，最初使用专有模型来了解
    AI 的潜力并最小化早期资本支出（CapEx）是合理的。然而，对于广泛的持续部署，在许多情况下，企业将使用基于生态系统的开源定向解决方案，这提供了一种成本效益高、适应性强的战略，能够与不断发展的商业需求和行业趋势保持一致。
- en: '**GenAI Transition from Consumer to Business Deployment**'
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**GenAI 从消费者向商业部署的过渡**'
- en: When GenAI burst onto the scene in late 2022 with Open AI’s GPT-3 and ChatGPT
    3.5, it mainly garnered consumer interest. As businesses began investigating GenAI,
    two approaches to deploying GenAI quickly emerged in 2023 — using giant frontier
    models like ChatGPT vs. the newly introduced small, open source models originally
    inspired by Meta’s LLaMa model. By early 2024, two basic approaches have solidified,
    as shown in the columns in Figure 1\. With the proprietary AI approach, the company
    relies on a large closed model to provide all the needed technology value. For
    example, taking GPT-4o as a proxy for the left column, AI developers would use
    OpenAI technology for the model, data, security, and compute. With the open source
    ecosystem AI approach, the company or developer may opt for the right-sized open
    source model, using corporate or private data, customized functionality, and the
    necessary compute and security.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当 GenAI 于 2022 年末凭借 Open AI 的 GPT-3 和 ChatGPT 3.5 闯入市场时，主要吸引了消费者的兴趣。随着企业开始研究
    GenAI，两种部署 GenAI 的方法在 2023 年迅速出现——使用像 ChatGPT 这样的巨大前沿模型，还是使用由 Meta 的 LLaMa 模型启发而来的新推出的小型开源模型。到
    2024 年初，两种基本方法已逐渐固定，如图 1 中的列所示。采用专有 AI 方法的公司依赖于一个大型封闭模型来提供所需的所有技术价值。例如，以 GPT-4o
    作为左侧列的代理，AI 开发者将使用 OpenAI 的技术来提供模型、数据、安全性和计算资源。而采用开源生态系统 AI 方法的公司或开发者，可能会选择合适大小的开源模型，使用公司或私有数据、定制功能以及必要的计算和安全性。
- en: Both directions are valid and have advantages and disadvantages. It is not an
    absolute partition and developers can choose components from either approach,
    but taking either a proprietary or ecosystem-based open source AI path provides
    the company with a strategy with high internal consistency. While it is expected
    that both approaches will be broadly deployed, I believe that after an initial
    learning and transition period, most companies will follow the open source approach.
    Depending on the usage and setting, open source internal AI may provide significant
    benefits, including the ability to fine-tune the model and drive deployment using
    the company’s current infrastructure to run the model at the edge, on the client,
    in the data center, or as a dedicated service. With new AI fine-tuning tools,
    deep expertise is less of a barrier.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方向都是有效的，并且各有优缺点。这不是一个绝对的划分，开发者可以从任一方法中选择组件，但选择专有的或基于生态系统的开源 AI 路径为公司提供了一个高度一致的战略。虽然预计两种方法都会广泛部署，但我认为，在初步的学习和过渡期后，大多数公司将会采用开源方法。根据使用情况和设置，开源内部
    AI 可能带来显著的好处，包括能够微调模型，并利用公司当前的基础设施推动部署，将模型运行在边缘、客户端、数据中心或作为专用服务。随着新的 AI 微调工具的出现，深厚的专业知识已不再是障碍。
- en: '![](../Images/94deab0e4f8037a024f2d918e341385c.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94deab0e4f8037a024f2d918e341385c.png)'
- en: '*Figure 1\. Base approaches to the AI developer’s dilemma. Image credit: Intel
    Labs.*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 1\. AI 开发者困境的基础方法。图片来源：英特尔实验室。*'
- en: Across all industries, AI developers are using GenAI for a variety of applications.
    An October 2023 [poll by Gartner](https://www.gartner.com/en/newsroom/press-releases/2023-10-03-gartner-poll-finds-55-percent-of-organizations-are-in-piloting-or-production-mode-with-generative-ai)
    found that 55% of organizations reported increasing investment in GenAI since
    early 2023, and many companies are in pilot or production mode for the growing
    technology. As of the time of the survey, companies were mainly investing in using
    GenAI for software development, followed closely by marketing and customer service
    functions. Clearly, the range of AI applications is growing rapidly.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在各行各业中，AI 开发者正在使用生成式人工智能（GenAI）进行各种应用。2023年10月，[Gartner的调查](https://www.gartner.com/en/newsroom/press-releases/2023-10-03-gartner-poll-finds-55-percent-of-organizations-are-in-piloting-or-production-mode-with-generative-ai)显示，55%的组织报告称自2023年初以来增加了对生成式人工智能的投资，许多公司正在进行生成式人工智能的试点或生产模式。根据调查时的数据，企业主要投资于将生成式人工智能用于软件开发，其次是市场营销和客户服务功能。显然，人工智能应用的范围正在迅速增长。
- en: '**Large Proprietary Models vs. Small and Large Open Source Models**'
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**大型专有模型与小型和大型开源模型**'
- en: '![](../Images/aae966aaa55c7c687bb93089d425b2fb.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aae966aaa55c7c687bb93089d425b2fb.png)'
- en: '*Figure 2: Advantages of large proprietary models, and small and large open
    source models. For business considerations, see Figure 7 for CapEx and OpEx aspects.
    Image credit: Intel Labs.*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2：大型专有模型与小型和大型开源模型的优点。有关商业考虑，请参见图7中的资本支出和运营支出方面。图片来源：英特尔实验室。*'
- en: 'In my blog [Survival of the Fittest: Compact Generative AI Models Are the Future
    for Cost-Effective AI at Scale](/survival-of-the-fittest-compact-generative-ai-models-are-the-future-for-cost-effective-ai-at-scale-6bbdc138f618),
    I provide a detailed evaluation of large models vs. small models. In essence,
    following the introduction of [Meta’s LLaMa open source model in February 2023](https://ai.meta.com/blog/large-language-model-llama-meta-ai/),
    there has been a virtuous cycle of innovation and rapid improvement where the
    academia and broad-base ecosystem are creating highly effective models that are
    10x to 100x smaller than the large frontier models. A crop of small models, which
    in 2024 were mostly less than 30 billion parameters, could [closely match](https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/)
    the capabilities of ChatGPT-style large models containing well over 100B parameters,
    especially when targeted for particular domains. While GenAI is already being
    deployed throughout industries for a wide range of business usages, the use of
    compact models is rising.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的博客[《适者生存：紧凑型生成式AI模型是大规模成本效益AI的未来》](/survival-of-the-fittest-compact-generative-ai-models-are-the-future-for-cost-effective-ai-at-scale-6bbdc138f618)中，我对大型模型与小型模型进行了详细评估。简而言之，自从[Meta于2023年2月发布的LLaMa开源模型](https://ai.meta.com/blog/large-language-model-llama-meta-ai/)以来，学术界和广泛的生态系统进入了一个创新和快速改进的良性循环，创造出比大型前沿模型小10倍到100倍的高效模型。到2024年，许多小型模型的参数数量通常不到300亿，它们可以[接近匹敌](https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/)拥有超过1000亿参数的ChatGPT风格大型模型的能力，尤其是在针对特定领域时。虽然生成式人工智能已经在各行业中广泛部署，用于各种商业用途，但紧凑型模型的使用正在上升。
- en: In addition, open source models are mostly lagging [only six to 12 months behind](https://ark-invest.com/newsletter_item/1-openais-improved-chatgpt-should-delight-both-expert-and-novice-developers)
    the performance of proprietary models. Using the broad language benchmark MMLU,
    the improvement pace of the open source models is faster and the gap seems to
    be closing with proprietary models. For example, OpenAI’s [GPT-4o](https://openai.com/index/hello-gpt-4o/)
    came out this year on May 13 with major multimodal features while Microsoft’s
    small open source [Phi-3-vision](https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/)
    was introduced just a week later on May 21\. In [rudimentary comparisons](https://youtu.be/PZaNL6igONU?si=jCvhwvWBoZFnRG5X)
    done on visual recognition and understanding, the models showed some similar competencies,
    with several tests even favoring the Phi-3-vision model. [Initial evaluations
    of Meta’s Llama 3.2 open source release](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/)
    suggest that its “vision models are competitive with leading foundation models,
    Claude 3 Haiku and GPT4o-mini on image recognition and a range of visual understanding
    tasks.”
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，开源模型通常落后于专有模型[仅6到12个月](https://ark-invest.com/newsletter_item/1-openais-improved-chatgpt-should-delight-both-expert-and-novice-developers)。使用广泛的语言基准MMLU，开源模型的改进速度更快，差距似乎正在缩小。例如，OpenAI的[GPT-4o](https://openai.com/index/hello-gpt-4o/)于今年5月13日发布，带来了重要的多模态特性，而微软的小型开源[Phi-3-vision](https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/)则在5月21日发布，仅晚了一周。在[初步比较](https://youtu.be/PZaNL6igONU?si=jCvhwvWBoZFnRG5X)中，视觉识别和理解方面，这些模型展示了相似的能力，几项测试甚至偏向Phi-3-vision模型。[Meta的Llama
    3.2开源发布的初步评估](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/)表明，其“视觉模型在图像识别和一系列视觉理解任务上与领先的基础模型、Claude
    3 Haiku和GPT4o-mini具有竞争力”。
- en: 'Large models have incredible all-in-one versatility. Developers can choose
    from a variety of large commercially available proprietary GenAI models, including
    OpenAI’s GPT-4o multimodal model. Google’s [Gemini 1.5](https://deepmind.google/technologies/gemini/#introduction)
    natively multimodal model is available in four sizes: Nano for mobile device app
    development, Flash small model for specific tasks, Pro for a wide range of tasks,
    and Ultra for highly complex tasks. And Anthropic’s [Claude 3 Opus](https://www.anthropic.com/news/claude-3-family),
    rumored to have [approximately 2 trillion parameters](https://lifearchitect.substack.com/p/the-memo-special-edition-claude-3),
    has a 200K token context window, allowing users to upload large amounts of information.
    There’s also another category of out-of-the-box large GenAI models that businesses
    can use for employee productivity and creative development. [Microsoft 365 Copilot](https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/)
    integrates the Microsoft 365 Apps suite, Microsoft Graph (content and context
    from emails, files, meetings, chats, calendars, and contacts), and GPT-4.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 大型模型具有令人难以置信的多功能性。开发者可以选择各种大型商业化的专有GenAI模型，包括OpenAI的GPT-4o多模态模型。谷歌的[Gemini 1.5](https://deepmind.google/technologies/gemini/#introduction)原生多模态模型有四种尺寸：用于移动设备应用开发的Nano，小型的Flash模型用于特定任务，Pro用于广泛的任务，Ultra用于高度复杂的任务。此外，Anthropic的[Claude
    3 Opus](https://www.anthropic.com/news/claude-3-family)，据说拥有[大约2万亿个参数](https://lifearchitect.substack.com/p/the-memo-special-edition-claude-3)，具有200K的上下文窗口，允许用户上传大量信息。还有一类即开即用的大型GenAI模型，企业可以用来提升员工的生产力和创造性发展。[Microsoft
    365 Copilot](https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/)集成了Microsoft
    365应用套件、Microsoft Graph（来自电子邮件、文件、会议、聊天、日历和联系人等的内容与上下文），以及GPT-4。
- en: Most large and small open source models are often more transparent about application
    frameworks, tool ecosystem, training data, and evaluation platforms. Model architecture,
    hyperparameters, response quality, input modalities, context window size, and
    inference cost are partially or fully disclosed. These models often provide information
    on the dataset so that developers can determine if it meets copyright or quality
    expectations. This transparency allows developers to easily interchange models
    for future versions. Among the growing number of small commercially available
    open source models, Meta’s [Llama 3 and 3.1](https://ai.meta.com/blog/meta-llama-3-1/)
    are based on transformer architecture and available in 8B, 70B, and 405B parameters.
    Llama 3.2 multimodal model has 11B and 90B, with smaller versions at 1B and 3B
    parameters. Built in collaboration with NVIDIA, Mistral AI’s [Mistral NeMo](https://mistral.ai/news/mistral-nemo/)
    is a 12B model that features a large 128k context window while Microsoft’s [Phi-3](https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/)
    (3.8B, 7B, and 14B) offers Transformer models for reasoning and language understanding
    tasks. Microsoft highlights Phi models as an example of “[the surprising power
    of small language models](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/)”
    while investing heavily in OpenAI’s very large models. Microsoft’s diverse interest
    in GenAI indicates that it’s not a one-size-fits-all market.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数大型和小型开源模型通常对应用框架、工具生态系统、训练数据和评估平台更加透明。模型架构、超参数、响应质量、输入模式、上下文窗口大小和推理成本部分或完全公开。这些模型通常会提供数据集的信息，以便开发者判断是否符合版权或质量要求。这种透明度使得开发者可以轻松地交换模型，以便适应未来的版本。在越来越多的小型商业化开源模型中，Meta的[Llama
    3和3.1](https://ai.meta.com/blog/meta-llama-3-1/)基于Transformer架构，提供8B、70B和405B参数版本。Llama
    3.2多模态模型有11B和90B版本，较小的版本为1B和3B参数。与NVIDIA合作开发的Mistral AI的[Mistral NeMo](https://mistral.ai/news/mistral-nemo/)是一个12B模型，具有128k的大上下文窗口，而微软的[Phi-3](https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/)（3.8B、7B和14B）提供用于推理和语言理解任务的Transformer模型。微软将Phi模型作为“[小型语言模型的惊人力量](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/)”的例子，同时也在对OpenAI的大型模型进行大量投资。微软在生成式AI（GenAI）领域的多样化兴趣表明，这不是一个一刀切的市场。
- en: '**Model-Incorporated Data (with RAG) vs. Retrieval-Centric Generation (RCG)**'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**模型融合数据（带有RAG）与检索为中心的生成（RCG）**'
- en: The next key question that AI developers need to address is where to find the
    data used during inference — within the model parametric memory or outside the
    model (accessible by retrieval). It might be hard to believe, but the first ChatGPT
    launched in November 2022 did not have any access to data outside the model. It
    was trained on September 21, 2022 and notoriously had no inclination of events
    and data past its training date. This major oversight was addressed in 2023 when
    retrieval plug-ins where added. Today, most models are coupled with a retrieval
    front-end with exceptions in cases where there is no expectation of accessing
    large or continuously updating information, such as dedicated programming models.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: AI开发者需要解决的下一个关键问题是在哪里找到推理过程中使用的数据——是在模型的参数化记忆中，还是在模型之外（通过检索可访问）。这可能难以置信，但2022年11月推出的第一版ChatGPT并未访问模型之外的数据。它是在2022年9月21日训练的，且明显无法了解其训练日期之后的事件和数据。这一重大疏忽在2023年得到了修正，检索插件被添加进来。如今，大多数模型都与检索前端结合使用，只有在不期望访问大规模或持续更新信息的情况下，如专用编程模型，才会有所例外。
- en: Current models have made significant progress on this issue by enhancing the
    solution platforms with a retrieval-augmented generation (RAG) front-end to allow
    for extracting information external to the model. An efficient and secure RAG
    is a requirement in enterprise GenAI deployment, as shown by Microsoft’s introduction
    of [GPT-RAG](https://github.com/Azure/GPT-RAG/) in late 2023\. Furthermore, in
    the blog [Knowledge Retrieval Takes Center Stage](/knowledge-retrieval-takes-center-stage-183be733c6e8),
    I cover how in the transition from consumer to business deployment for GenAI,
    solutions should be built primarily around information external to the model using
    retrieval-centric generation (RCG).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的模型通过增强解决方案平台，结合检索增强生成（RAG）前端，使得可以提取模型外部的信息，从而在这一问题上取得了显著进展。高效且安全的RAG是企业级GenAI部署的要求，微软于2023年底推出的[GPT-RAG](https://github.com/Azure/GPT-RAG/)便是一个例证。此外，在博客[知识检索成为核心](https://example.org/knowledge-retrieval-takes-center-stage-183be733c6e8)中，我探讨了如何在GenAI从消费级部署过渡到商业级部署时，解决方案应主要围绕模型外部的信息构建，采用以检索为核心的生成（RCG）方法。
- en: '![](../Images/0be3876b1e8c780ab5400ce962cc804f.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0be3876b1e8c780ab5400ce962cc804f.png)'
- en: 'Figure 3\. Advantage of RAG vs. RCG. Image credit: Intel Labs.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图3. RAG与RCG的优势对比。图片来源：英特尔实验室。
- en: RCG models can be defined as a special case of RAG GenAI solutions designed
    for systems where the vast majority of data resides outside the model parametric
    memory and is mostly not seen in pre-training or fine-tuning. With RCG, the primary
    role of the GenAI model is to interpret rich retrieved information from a company’s
    indexed data corpus or other curated content. Rather than memorizing data, the
    model focuses on fine-tuning for targeted constructs, relationships, and functionality.
    The quality of data in generated output is expected to approach 100% accuracy
    and timeliness.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: RCG模型可以定义为RAG GenAI解决方案的一个特例，专为那些绝大多数数据存在于模型的参数记忆之外，且在预训练或微调过程中大多没有涉及的系统设计。通过RCG，GenAI模型的主要角色是解释从公司已索引数据库或其他策划内容中检索到的丰富信息。与其说是记忆数据，不如说是聚焦于针对特定构造、关系和功能的微调。生成输出中的数据质量预计将接近100%的准确性和时效性。
- en: '![](../Images/591e4dc8864d1884a1e060c397f19d3a.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/591e4dc8864d1884a1e060c397f19d3a.png)'
- en: '*Figure 4\. How retrieval works in GenAI platforms. Image credit: Intel Labs.*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4. GenAI平台中检索的工作原理。图片来源：英特尔实验室。*'
- en: '[OPEA](https://www.intel.com/content/www/us/en/developer/articles/news/introducing-the-open-platform-for-enterprise-ai.html)
    is a cross-ecosystem effort to ease the adoption and tuning of GenAI systems.
    Using this composable framework, developers can create and evaluate “open, multi-provider,
    robust, and composable GenAI solutions that harness the best innovation across
    the ecosystem.” OPEA is expected to simplify the implementation of enterprise-grade
    composite GenAI solutions, including RAG, agents, and memory systems.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[OPEA](https://www.intel.com/content/www/us/en/developer/articles/news/introducing-the-open-platform-for-enterprise-ai.html)是一个跨生态系统的努力，旨在简化GenAI系统的采纳和调优。使用这一可组合的框架，开发者可以创建并评估“开放的、多供应商的、强大的且可组合的GenAI解决方案，充分利用生态系统中的最佳创新。”OPEA有望简化企业级复合GenAI解决方案的实施，包括RAG、代理和记忆系统。'
- en: '![](../Images/51f3053a21ba51004ef80aa91d0c16d4.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51f3053a21ba51004ef80aa91d0c16d4.png)'
- en: '*Figure 5\. OPEA core principles for GenAI implementation.* *Image credit:
    OPEA.*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5. GenAI实施的OPEA核心原则。* *图片来源：OPEA。*'
- en: '**All-in-One General Purpose vs. Targeted Customized Models**'
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**通用多功能模型与定制化目标模型的对比**'
- en: Models like GPT-4o, Claude 3, and Gemini 1.5 are general purpose all-in-one
    foundation models. They are designed to perform a broad range of GenAI from coding
    to chat to summarization. The latest models have rapidly expanded to perform vision/image
    tasks, changing their function from just large language models to large multimodal
    models or vision language models (VLMs). Open source foundation models are headed
    in the same direction as integrated multimodalities.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 像GPT-4o、Claude 3和Gemini 1.5这样的模型是通用型的全能基础模型。它们被设计用于执行广泛的GenAI任务，包括编码、聊天和总结。最新的模型已经迅速扩展到执行视觉/图像任务，将其功能从单一的大型语言模型扩展到大型多模态模型或视觉语言模型（VLMs）。开源基础模型也朝着集成多模态的方向发展。
- en: '![](../Images/2016c69e41178f63411396e9836e74cf.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2016c69e41178f63411396e9836e74cf.png)'
- en: '*Figure 6\. Advantages of general purpose vs. targeted customized models. Image
    credit: Intel Labs.*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6. 通用模型与定制化目标模型的优势对比。图片来源：英特尔实验室。*'
- en: However, rather than adopting the first wave of consumer-oriented GenAI models
    in this general-purpose form, most businesses are electing to use some form of
    specialization. When a healthcare company deploys GenAI technology, they would
    not use one general model for managing the supply chain, coding in the IT department,
    and deep medical analytics for managing patient care. Businesses deploy more specialized
    versions of the technology for each use case. There are several different ways
    that companies can build specialized GenAI solutions, including domain-specific
    models, targeted models, customized models, and optimized models.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，大多数企业选择采用某种形式的专业化，而不是直接使用第一波面向消费者的通用型生成式人工智能（GenAI）模型。当一家医疗保健公司部署GenAI技术时，他们不会使用一个通用模型来管理供应链、进行IT部门的编码工作以及进行深度医疗分析来管理患者护理。企业会根据不同的使用场景，部署该技术的更多专业化版本。企业可以通过多种方式构建专业化的GenAI解决方案，包括领域特定模型、针对性模型、定制化模型和优化模型。
- en: '*Domain-specific models* are specialized for a particular field of business
    or an area of interest. There are both proprietary and open source domain-specific
    models. For example, BloombergGPT, a 50B parameter proprietary large language
    model specialized for finance, [beats the larger GPT-3 175B parameter model](https://arxiv.org/pdf/2303.17564.pdf)
    on various financial benchmarks. However, small open source domain-specific models
    can provide an excellent alternative, as demonstrated by [FinGPT](https://arxiv.org/pdf/2306.06031.pdf),
    which provides accessible and transparent resources to develop FinLLMs. FinGPT
    3.3 uses Llama 2 13B as a base model targeted for the financial sector. [In recent
    benchmarks](https://github.com/AI4Finance-Foundation/FinGPT), FinGPT surpassed
    BloombergGPT on a variety of tasks and beat GPT-4 handily on financial benchmark
    tasks like FPB, FiQA-SA, and TFNS. To understand the tremendous potential of this
    small open source model, it should be noted that FinGPT can be fine-tuned to incorporate
    new data for less than $300 per fine-tuning.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*领域特定模型* 专注于特定的业务领域或兴趣领域。领域特定模型有专有的和开源的两种类型。例如，BloombergGPT是一款专门为金融领域设计的50B参数专有大语言模型，[在多个金融基准测试中超越了更大的GPT-3
    175B参数模型](https://arxiv.org/pdf/2303.17564.pdf)。然而，小型的开源领域特定模型也可以提供出色的替代方案，正如[FinGPT](https://arxiv.org/pdf/2306.06031.pdf)所展示的，它提供了开发金融语言模型（FinLLMs）的可获取和透明的资源。FinGPT
    3.3以Llama 2 13B为基础模型，专为金融领域设计。[在最近的基准测试中](https://github.com/AI4Finance-Foundation/FinGPT)，FinGPT在多个任务上超过了BloombergGPT，并在金融基准任务（如FPB、FiQA-SA和TFNS）上轻松战胜了GPT-4。为了理解这个小型开源模型的巨大潜力，值得注意的是，FinGPT可以以不到300美元的成本进行微调，以融入新的数据。'
- en: '*Targeted models* specialize in a family of tasks or functions, such as separate
    targeted models for [coding](https://huggingface.co/docs/transformers/v4.39.0/en/model_doc/starcoder2),
    image generation, question answering, or sentiment analysis. A recent example
    of a targeted model is [SetFit](https://huggingface.co/blog/setfit) from Intel
    Labs, Hugging Face, and the UKP Lab. This few-shot text classification approach
    for fine-tuning Sentence Transformers is faster at inference and training, achieving
    high accuracy with a small number of labeled training data, such as only eight
    labeled examples per class on the Customer Reviews (CR) sentiment dataset. This
    small 355M parameter model can best the GPT-3 175B parameter model on the diverse
    RAFT benchmark.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*针对性模型* 专注于一类任务或功能，例如针对[编码](https://huggingface.co/docs/transformers/v4.39.0/en/model_doc/starcoder2)、图像生成、问答或情感分析的单独针对性模型。最近的一个针对性模型例子是英特尔实验室、Hugging
    Face和UKP实验室联合推出的[SetFit](https://huggingface.co/blog/setfit)。这种针对Sentence Transformers进行微调的少量样本文本分类方法，在推理和训练时更为高效，能够在少量标注的训练数据下实现高精度，例如仅用每个类别八个标注样本的客户评价（CR）情感数据集。这个仅有355M参数的小型模型可以在多样化的RAFT基准测试中超过GPT-3
    175B参数模型的表现。'
- en: It’s important to note that targeted models are independent from domain-specific
    models. For example, a sentiment analysis solution like [SetFitABSA](https://huggingface.co/blog/setfit-absa)
    has targeted functionality and can be applied to various domains like industrial,
    entertainment, or hospitality. However, models that are both targeted and domain
    specialized can be more effective.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，针对性模型与领域特定模型是独立的。例如，像[SetFitABSA](https://huggingface.co/blog/setfit-absa)这样的情感分析解决方案具有针对性的功能，可以应用于工业、娱乐或酒店等多个领域。然而，既具有针对性又具有领域专门化的模型可能会更有效。
- en: '*Customized models* are further fine-tuned and refined to meet particular needs
    and preferences of companies, organizations, or individuals. By indexing particular
    content for retrieval, the resulting system becomes highly specific and effective
    on tasks related to this data (private or public). The open source field offers
    an array of options to customize the model. For example, Intel Labs used direct
    preference optimization (DPO) to improve on a Mistral 7B model to create the open
    source [Intel NeuralChat](https://huggingface.co/Intel/neural-chat-7b-v3-1). Developers
    also can fine-tune and customize models by using low-rank adaptation of large
    language ([LoRA](https://arxiv.org/abs/2106.09685)) models and its more memory-efficient
    version, [QLoRA](https://arxiv.org/abs/2305.14314).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*定制化模型*进一步进行微调和优化，以满足公司、组织或个人的特定需求和偏好。通过对特定内容进行索引以供检索，生成的系统在处理与这些数据（无论是私有还是公开）相关的任务时，变得高度具体且高效。开源领域提供了多种定制模型的选项。例如，Intel
    Labs使用直接偏好优化（DPO）对Mistral 7B模型进行改进，从而创建了开源的[Intel NeuralChat](https://huggingface.co/Intel/neural-chat-7b-v3-1)。开发人员还可以通过使用大型语言模型的低秩适应（[LoRA](https://arxiv.org/abs/2106.09685)）以及其更加节省内存的版本[QLoRA](https://arxiv.org/abs/2305.14314)，来微调和定制模型。'
- en: '*Optimization capabilities* are available for open source models. The objective
    of optimization is to retain the functionality and accuracy of a model while substantially
    reducing its execution footprint, which can significantly improve cost, latency,
    and optimal execution of an intended platform. Some techniques used for model
    optimization include distillation, pruning, compression, and quantization (to
    8-bit and even 4-bit). Some methods like mixture of experts (MoE) and [speculative
    decoding](https://arxiv.org/pdf/2211.17192.pdf) can be considered as forms of
    execution optimization. For example, [GPT-4 is reportedly comprised](https://the-decoder.com/gpt-4-has-a-trillion-parameters/)
    of eight smaller MoE models with 220B parameters. The execution only activates
    parts of the model, allowing for much more economical inference.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*优化能力*可用于开源模型。优化的目标是在保持模型功能和准确性的同时，显著减少其执行负担，从而显著提高成本、延迟和预期平台的最佳执行效率。用于模型优化的一些技术包括蒸馏、剪枝、压缩和量化（至8位甚至4位）。一些方法，如专家混合（MoE）和[推测解码](https://arxiv.org/pdf/2211.17192.pdf)，可以视为执行优化的形式。例如，[据报道GPT-4由](https://the-decoder.com/gpt-4-has-a-trillion-parameters/)八个较小的MoE模型组成，每个模型有220B个参数。执行仅激活模型的部分，从而使推理更加经济。'
- en: '**Generative-as-a-Service Cloud Execution vs. Managed Execution Environment
    for Inference**'
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**生成即服务云执行与托管执行环境的推理对比**'
- en: '![](../Images/8292f287c7dbbac24afcde2430ef2789.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8292f287c7dbbac24afcde2430ef2789.png)'
- en: '*Figure 7\. Advantages of GaaS vs. managed execution. Image credit: Intel Labs.*'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7. GaaS与托管执行的优势对比。图片来源：Intel Labs。*'
- en: Another key choice for developers to consider is the execution environment.
    If the company chooses a proprietary model direction, inference execution is done
    through API or query calls to an abstracted and obscured image of the model running
    in the cloud. The size of the model and other implementation details are insignificant,
    except when translated to availability and the cost charged by some key (per token,
    per query, or unlimited compute license). This approach, sometimes referred to
    as a [generative-as-a-service (GaaS)](https://www.forbes.com/sites/steveandriole/2023/07/26/llama-chatgpt-bard-co-pilot--all-the-rest--how-large-language-models-will-become-huge-cloud-services-with-massive-ecosystems/?sh=78764e1175b7)
    cloud offering, is the principle way for companies to consume very large proprietary
    models like GPT-4o, Gemini Ultra, and Claude 3\. However, GaaS can also be offered
    for smaller models like Llama 3.2.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 开发人员需要考虑的另一个关键选择是执行环境。如果公司选择专有模型的方向，推理执行将通过API或查询调用来完成，这些调用连接到云中运行的模型的抽象化和隐蔽版本。模型的大小及其他实现细节并不重要，除非它们会影响可用性或一些关键费用（按令牌、查询次数，或无限计算许可证收费）。这种方法有时被称为[生成即服务（GaaS）](https://www.forbes.com/sites/steveandriole/2023/07/26/llama-chatgpt-bard-co-pilot--all-the-rest--how-large-language-models-will-become-huge-cloud-services-with-massive-ecosystems/?sh=78764e1175b7)云服务，是公司消费非常大的专有模型（如GPT-4o、Gemini
    Ultra和Claude 3）的一种主要方式。然而，GaaS也可以用于提供像Llama 3.2这样的小型模型。
- en: There are clear positive aspects to using GaaS for the outsourced intelligence
    approach. For example, the access is usually instantaneous and easy to use out-of-the-box,
    alleviating in-house development efforts. There is also the implied promise that
    when the models or their environment get upgraded, the AI solution developers
    have access to the latest updates without substantial effort or changes to their
    setup. Also, the costs are almost entirely operational expenditures (OpEx), which
    is preferred if the workload is initial or limited. For early-stage adoption and
    intermittent use, GaaS offers more support.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GaaS进行外包智能方法有明确的积极方面。例如，访问通常是即时的，并且开箱即用，减轻了内部开发的工作量。还有一个隐含的承诺，即当模型或其环境升级时，AI解决方案开发者可以在不做大量努力或更改设置的情况下，获取最新的更新。此外，费用几乎完全是运营支出（OpEx），如果工作负载是初步的或有限的，这种方式更为优选。对于早期采用和间歇性使用，GaaS提供了更多支持。
- en: In contrast, when companies choose an internal intelligence approach, the model
    inference cycle is incorporated and managed within the compute environment and
    the existing business software setting. This is a viable solution for relatively
    small models (approximately 30B parameters or less in 2024) and potentially even
    medium models (50B to 70B parameters in 2024) on a client device, network, on-prem
    data center, or on-cloud cycles in an environment set with a service provider
    such as a virtual private cloud (VPC).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，当公司选择内部智能方法时，模型推理周期被纳入并在计算环境和现有的业务软件设置中进行管理。这对于相对较小的模型（2024年约30B参数或更少）以及可能甚至是中等规模的模型（2024年50B到70B参数）在客户端设备、网络、本地数据中心或云环境中，使用如虚拟私有云（VPC）等服务提供商的设置，是一个可行的解决方案。
- en: Models like Llama 3.1 8B or similar can run on the [developer’s local machine](https://www.forbes.com/sites/steveandriole/2023/07/26/llama-chatgpt-bard-co-pilot--all-the-rest--how-large-language-models-will-become-huge-cloud-services-with-massive-ecosystems/?sh=78764e1175b7)
    (Mac or PC). Using optimization techniques like [quantization](https://www.intel.com/content/www/us/en/developer/articles/case-study/q8-chat-efficient-generative-ai-experience-xeon.html#gs.36q4lk),
    the needed user experience can be achieved while operating within the local setting.
    Using a tool and framework like [Ollama](https://ollama.ai/), developers can manage
    inference execution locally. Inference cycles can be run on legacy GPUs, [Intel
    Xeon](https://www.intel.com/content/www/us/en/products/docs/processors/xeon-accelerated/ai-accelerators-product-brief.html),
    or [Intel Gaudi AI accelerators](https://www.intel.com/content/www/us/en/products/details/processors/ai-accelerators/gaudi-overview.html)
    in the company’s data center. If inference is run on the model at a service provider,
    it will be billed as infrastructure-as-a-service (IaaS), using the company’s own
    setting and execution choices.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 像Llama 3.1 8B这样的模型或类似模型可以在[开发者的本地机器](https://www.forbes.com/sites/steveandriole/2023/07/26/llama-chatgpt-bard-co-pilot--all-the-rest--how-large-language-models-will-become-huge-cloud-services-with-massive-ecosystems/?sh=78764e1175b7)（Mac或PC）上运行。通过使用像[量化](https://www.intel.com/content/www/us/en/developer/articles/case-study/q8-chat-efficient-generative-ai-experience-xeon.html#gs.36q4lk)这样的优化技术，可以在本地环境中实现所需的用户体验。使用像[Ollama](https://ollama.ai/)这样的工具和框架，开发者可以本地管理推理执行。推理周期可以在公司的数据中心通过传统的GPU、[Intel
    Xeon](https://www.intel.com/content/www/us/en/products/docs/processors/xeon-accelerated/ai-accelerators-product-brief.html)或[Intel
    Gaudi AI加速器](https://www.intel.com/content/www/us/en/products/details/processors/ai-accelerators/gaudi-overview.html)上运行。如果推理是在服务提供商处运行，则会按基础设施即服务（IaaS）计费，使用公司自己的设置和执行选项。
- en: When inference execution is done in the company compute environment (client,
    edge, on-prem, or IaaS), there is a higher requirement for CapEx for ownership
    of the computer equipment if it goes beyond adding a workload to existing hardware.
    While the comparison of OpEx vs. CapEx is complex and depends on many variables,
    CapEx is preferable when deployment requires broad, continuous, stable usage.
    This is especially true as smaller models and optimization technologies allow
    for running advanced open source models on mainstream devices and processors and
    even local notebooks/desktops.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当推理执行在公司计算环境中（客户端、边缘、本地或IaaS）进行时，如果超出仅在现有硬件上添加工作负载的范围，则对计算设备的资本支出（CapEx）要求更高。虽然OpEx与CapEx的比较复杂且依赖于许多变量，但当部署需要广泛、持续、稳定的使用时，CapEx更为可取。尤其是随着较小模型和优化技术的出现，允许在主流设备和处理器上运行先进的开源模型，甚至在本地笔记本/台式机上运行时，这一点尤为真实。
- en: Running inference in the company compute environment allows for tighter control
    over aspects of security and privacy. Reducing data movement and exposure can
    be valuable in preserving privacy. Furthermore, a retrieval-based AI solution
    run in a local setting can be supported with fine controls to address potential
    privacy concerns by giving user-controlled access to information. Security is
    frequently mentioned as one of the top concerns of companies deploying GenAI and
    [confidential computing](https://www.intel.com/content/dam/www/public/us/en/documents/solution-briefs/intro-to-confidential-computing-solution-brief.pdf)
    is a primary ask. Confidential computing protects data in use by computing in
    an attested hardware-based [Trusted Execution Environment (TEE)](https://www.intel.com/content/www/us/en/content-details/788130/what-is-a-trusted-execution-environment.html).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在公司计算环境中运行推理任务可以更好地控制安全性和隐私性。减少数据的移动和暴露在保护隐私方面非常有价值。此外，在本地环境中运行基于检索的AI解决方案时，可以通过细致的控制来应对潜在的隐私问题，允许用户控制对信息的访问。安全性通常被提及为公司部署GenAI时的首要关注点，[机密计算](https://www.intel.com/content/dam/www/public/us/en/documents/solution-briefs/intro-to-confidential-computing-solution-brief.pdf)是一个主要需求。机密计算通过在受信硬件基础上计算，保护数据在使用中的安全，使用的是[受信执行环境（TEE）](https://www.intel.com/content/www/us/en/content-details/788130/what-is-a-trusted-execution-environment.html)。
- en: Smaller, open source models can run within a company’s most secure application
    setting. For example, a model running on Xeon can be fully executed within a TEE
    with limited overhead. As shown in Figure 8, encrypted data remains protected
    while not in compute. The model is checked for provenance and integrity to protect
    against tampering. The actual execution is protected from any breach, including
    by the operating system or other applications, preventing viewing or alteration
    by untrusted entities.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 较小的开源模型可以在公司的最安全应用环境中运行。例如，运行在Xeon上的模型可以在TEE（受信执行环境）中完全执行，且仅带有有限的开销。如图 8所示，加密数据在计算过程中得到保护。该模型会检查其来源和完整性，以防篡改。实际执行过程中，模型免受任何破坏，包括操作系统或其他应用程序的干扰，从而防止被不受信任的实体查看或篡改。
- en: '![](../Images/918a31fbc46ec6dc9d1ca7a229f2ca46.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/918a31fbc46ec6dc9d1ca7a229f2ca46.png)'
- en: 'Figure 8\. Security requirements for GenAI. Image credit: Intel Labs.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8. GenAI的安全要求。图片来源：英特尔实验室。
- en: '**Summary**'
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**总结**'
- en: Generative AI is a transformative technology now under evaluation or active
    adoption by most companies across all industries and sectors. As AI developers
    consider their options for the best solution, one of the most important questions
    they need to address is whether to use external proprietary models or rely on
    the open source ecosystem. One path is to rely on a large proprietary black-box
    GaaS solution using RAG, such as GPT-4o or Gemini Ultra. The other path uses a
    more adaptive and integrative approach — small, selected, and exchanged as needed
    from a large open source model pool, mainly utilizing company information, customized
    and optimized based on particular needs, and executed within the existing infrastructure
    of the company. As mentioned, there could be a combination of choices within these
    two base strategies.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 生成型AI是一项正在评估或被大多数各行各业公司积极采用的变革性技术。在AI开发者考虑最佳解决方案时，他们需要面对的一个重要问题是，是否选择使用外部专有模型，还是依赖开源生态系统。一条路径是依赖于一个大型的专有黑盒GaaS解决方案，使用RAG，例如GPT-4o或Gemini
    Ultra。另一条路径则采取一种更加适应性强且具整合性的方式——从一个庞大的开源模型池中挑选小型模型，并根据需要交换，主要利用公司内部信息，定制并优化以满足特定需求，并在公司现有的基础设施中执行。如前所述，这两种基本策略之间可能会有组合选择。
- en: I believe that as numerous AI solution developers face this essential dilemma,
    most will eventually (after a learning period) choose to embed open source GenAI
    models in their internal compute environment, data, and business setting. They
    will ride the incredible advancement of the open source and broad ecosystem virtuous
    cycle of AI innovation, while maintaining control over their costs and destiny.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我相信，随着众多AI解决方案开发者面临这一核心难题，大多数人最终（经过一段学习期）会选择将开源生成型AI（GenAI）模型嵌入到他们的内部计算环境、数据和业务环境中。这样，他们将能够利用开源及其广泛生态系统的良性循环推动AI创新的巨大进步，同时保持对成本和命运的控制。
- en: Let’s give AI the final word in solving the AI developer’s dilemma. In a [staged
    AI debate](https://pub.aimind.so/gpt-4-debates-open-orca-2-13b-with-surprising-results-b4ada53845ba),
    OpenAI’s GPT-4 argued with Microsoft’s open source Orca 2 13B on the merits of
    using proprietary vs. open source GenAI for future development. Using GPT-4 Turbo
    as the judge, open source GenAI won the debate. The [winning argument](https://youtu.be/JuwJLeVlB-w?t=774)?
    Orca 2 called for a “more distributed, open, collaborative future of AI development
    that leverages worldwide talent and aims for collective advancements. This model
    promises to accelerate innovation and democratize access to AI, and ensure ethical
    and transparent practices through community governance.”
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让AI在解决AI开发者困境中拥有最终发言权。在一次[ staged AI辩论](https://pub.aimind.so/gpt-4-debates-open-orca-2-13b-with-surprising-results-b4ada53845ba)，OpenAI的GPT-4与微软的开源Orca
    2 13B就使用专有与开源生成性AI在未来开发中的优劣进行了辩论。以GPT-4 Turbo作为裁判，开源生成性AI赢得了辩论。[获胜的论点](https://youtu.be/JuwJLeVlB-w?t=774)?
    Orca 2呼吁“更分散、开放、协作的AI开发未来，利用全球人才，旨在实现集体进步。该模型有望加速创新，推动AI普及，并通过社区治理确保道德和透明的做法。”
- en: '**Learn More: GenAI Series**'
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**了解更多：生成式AI系列**'
- en: '[Knowledge Retrieval Takes Center Stage: GenAI Architecture Shifting from RAG
    Toward Interpretive Retrieval-Centric Generation (RCG) Models](/knowledge-retrieval-takes-center-stage-183be733c6e8)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[知识检索登上舞台：生成性AI架构从RAG向解释性检索中心生成（RCG）模型转变](/knowledge-retrieval-takes-center-stage-183be733c6e8)'
- en: '[Survival of the Fittest: Compact Generative AI Models Are the Future for Cost-Effective
    AI at Scale](/survival-of-the-fittest-compact-generative-ai-models-are-the-future-for-cost-effective-ai-at-scale-6bbdc138f618)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[适者生存：紧凑型生成性AI模型是大规模、成本效益高的AI未来](/survival-of-the-fittest-compact-generative-ai-models-are-the-future-for-cost-effective-ai-at-scale-6bbdc138f618)'
- en: '[Have Machines Just Made an Evolutionary Leap to Speak in Human Language?](/have-machines-just-made-an-evolutionary-leap-to-speak-in-human-language-319237593aa4)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[机器是否已经实现了进化性飞跃，可以用人类语言交流？](/have-machines-just-made-an-evolutionary-leap-to-speak-in-human-language-319237593aa4)'
- en: '**References**'
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: Hello GPT-4o. (2024, May 13). [https://openai.com/index/hello-gpt-4o/](https://openai.com/index/hello-gpt-4o/)
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你好，GPT-4o。（2024年5月13日）。[https://openai.com/index/hello-gpt-4o/](https://openai.com/index/hello-gpt-4o/)
- en: Open platform for enterprise AI. (n.d.). Open Platform for Enterprise AI (OPEA).
    [https://opea.dev/](https://opea.dev/)
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 企业人工智能开放平台。（无日期）。企业人工智能开放平台（OPEA）。[https://opea.dev/](https://opea.dev/)
- en: Gartner Poll Finds 55% of Organizations are in Piloting or Production. (2023,
    October 3). Gartner. [https://www.gartner.com/en/newsroom/press-releases/2023-10-03-gartner-poll-finds-55-percent-of-organizations-are-in-piloting-or-production-mode-with-generative-ai](https://www.gartner.com/en/newsroom/press-releases/2023-10-03-gartner-poll-finds-55-percent-of-organizations-are-in-piloting-or-production-mode-with-generative-ai)
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gartner调查发现55%的组织正在进行试点或生产中。（2023年10月3日）。Gartner。[https://www.gartner.com/en/newsroom/press-releases/2023-10-03-gartner-poll-finds-55-percent-of-organizations-are-in-piloting-or-production-mode-with-generative-ai](https://www.gartner.com/en/newsroom/press-releases/2023-10-03-gartner-poll-finds-55-percent-of-organizations-are-in-piloting-or-production-mode-with-generative-ai)
- en: 'Singer, G. (2023, July 28). Survival of the fittest: Compact generative AI
    models are the future for Cost-Effective AI at scale. *Medium*. [https://towardsdatascience.com/survival-of-the-fittest-compact-generative-ai-models-are-the-future-for-cost-effective-ai-at-scale-6bbdc138f618](/survival-of-the-fittest-compact-generative-ai-models-are-the-future-for-cost-effective-ai-at-scale-6bbdc138f618)'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Singer, G.（2023年7月28日）。适者生存：紧凑型生成性AI模型是大规模、成本效益高的AI未来。*Medium*。[https://towardsdatascience.com/survival-of-the-fittest-compact-generative-ai-models-are-the-future-for-cost-effective-ai-at-scale-6bbdc138f618](/survival-of-the-fittest-compact-generative-ai-models-are-the-future-for-cost-effective-ai-at-scale-6bbdc138f618)
- en: 'Introducing LLaMA: A foundational, 65-billion-parameter language model. (n.d.).
    [https://ai.meta.com/blog/large-language-model-llama-meta-ai/](https://ai.meta.com/blog/large-language-model-llama-meta-ai/)'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 介绍LLaMA：一款基础性的65亿参数语言模型。（无日期）。[https://ai.meta.com/blog/large-language-model-llama-meta-ai/](https://ai.meta.com/blog/large-language-model-llama-meta-ai/)
- en: '#392: OpenAI’s improved ChatGPT should delight both expert and novice developers,
    & more — ARK Invest. (n.d.). Ark Invest. [https://ark-invest.com/newsletter_item/1-openais-improved-chatgpt-should-delight-both-expert-and-novice-developers](https://ark-invest.com/newsletter_item/1-openais-improved-chatgpt-should-delight-both-expert-and-novice-developers)'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '#392：OpenAI改进版的ChatGPT应让专家和初学者开发者都感到兴奋，&更多内容—ARK投资。（无日期）。Ark Invest。[https://ark-invest.com/newsletter_item/1-openais-improved-chatgpt-should-delight-both-expert-and-novice-developers](https://ark-invest.com/newsletter_item/1-openais-improved-chatgpt-should-delight-both-expert-and-novice-developers)'
- en: Bilenko, M. (2024, May 22). New models added to the Phi-3 family, available
    on Microsoft Azure. Microsoft Azure Blog. [https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/](https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/)
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bilenko, M.（2024年5月22日）。Phi-3 系列新增模型，可在 Microsoft Azure 上使用。微软 Azure 博客。[https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/](https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/)
- en: Matthew Berman. (2024, June 2). Open-Source Vision AI — Surprising Results!
    (Phi3 Vision vs LLaMA 3 Vision vs GPT4o) [Video]. YouTube. [https://www.youtube.com/watch?v=PZaNL6igONU](https://www.youtube.com/watch?v=PZaNL6igONU)
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Matthew Berman。（2024年6月2日）。开源视觉AI — 令人惊讶的结果！（Phi3 Vision 对比 LLaMA 3 Vision 对比
    GPT4o）[视频]。YouTube。[https://www.youtube.com/watch?v=PZaNL6igONU](https://www.youtube.com/watch?v=PZaNL6igONU)
- en: 'Llama 3.2: Revolutionizing edge AI and vision with open, customizable models.
    (n.d.). [https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/)'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Llama 3.2：通过开放、可定制的模型，革新边缘AI和视觉技术。[https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/)
- en: Gemini — Google DeepMind. (n.d.). [https://deepmind.google/technologies/gemini/#introduction](https://deepmind.google/technologies/gemini/#introduction)
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gemini — Google DeepMind。（无日期）。[https://deepmind.google/technologies/gemini/#introduction](https://deepmind.google/technologies/gemini/#introduction)
- en: Introducing the next generation of Claude \ Anthropic. (n.d.). [https://www.anthropic.com/news/claude-3-family](https://www.anthropic.com/news/claude-3-family)
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 介绍下一代 Claude \ Anthropic。（无日期）。[https://www.anthropic.com/news/claude-3-family](https://www.anthropic.com/news/claude-3-family)
- en: 'Thompson, A. D. (2024, March 4). The Memo — Special edition: Claude 3 Opus.
    The Memo by LifeArchitect.ai. [https://lifearchitect.substack.com/p/the-memo-special-edition-claude-3](https://lifearchitect.substack.com/p/the-memo-special-edition-claude-3)'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Thompson, A. D.（2024年3月4日）。The Memo — 特别版：Claude 3 Opus。《The Memo》 by LifeArchitect.ai。[https://lifearchitect.substack.com/p/the-memo-special-edition-claude-3](https://lifearchitect.substack.com/p/the-memo-special-edition-claude-3)
- en: Spataro, J. (2023, May 16). Introducing Microsoft 365 Copilot — your copilot
    for work — The Official Microsoft Blog. The Official Microsoft Blog. [https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/](https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/)
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spataro, J.（2023年5月16日）。介绍 Microsoft 365 Copilot — 你的工作副驾驶 — 官方微软博客。官方微软博客。[https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/](https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/)
- en: 'Introducing Llama 3.1: Our most capable models to date. (n.d.). [https://ai.meta.com/blog/meta-llama-3-1/](https://ai.meta.com/blog/meta-llama-3-1/)'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 介绍 Llama 3.1：迄今为止我们最强大的模型。（无日期）。[https://ai.meta.com/blog/meta-llama-3-1/](https://ai.meta.com/blog/meta-llama-3-1/)
- en: Mistral AI. (2024, March 4). Mistral Nemo. Mistral AI | Frontier AI in Your
    Hands. [https://mistral.ai/news/mistral-nemo/](https://mistral.ai/news/mistral-nemo/)
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Mistral AI。（2024年3月4日）。Mistral Nemo。Mistral AI | 让前沿人工智能触手可得。[https://mistral.ai/news/mistral-nemo/](https://mistral.ai/news/mistral-nemo/)
- en: 'Beatty, S. (2024, April 29). Tiny but mighty: The Phi-3 small language models
    with big potential. Microsoft Research. [https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/](https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/)'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Beatty, S.（2024年4月29日）。虽小却强大：Phi-3 小型语言模型的巨大潜力。微软研究院。[https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/](https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/)
- en: 'Hughes, A. (2023, December 16). Phi-2: The surprising power of small language
    models. Microsoft Research. [https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/)'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hughes, A.（2023年12月16日）。Phi-2：小型语言模型的惊人力量。微软研究院。[https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/)
- en: Azure. (n.d.). GitHub — Azure/GPT-RAG. GitHub. [https://github.com/Azure/GPT-RAG/](https://github.com/Azure/GPT-RAG/)
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Azure。（无日期）。GitHub — Azure/GPT-RAG。GitHub。[https://github.com/Azure/GPT-RAG/](https://github.com/Azure/GPT-RAG/)
- en: Singer, G. (2023, November 16). Knowledge Retrieval Takes Center Stage — Towards
    Data Science. Medium. [https://towardsdatascience.com/knowledge-retrieval-takes-center-stage-183be733c6e8](/knowledge-retrieval-takes-center-stage-183be733c6e8)
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Singer, G. (2023年11月16日). 知识检索成为焦点 — 数据科学前沿. Medium. [https://towardsdatascience.com/knowledge-retrieval-takes-center-stage-183be733c6e8](/knowledge-retrieval-takes-center-stage-183be733c6e8)
- en: Introducing the open platform for enterprise AI. (n.d.). Intel. [https://www.intel.com/content/www/us/en/developer/articles/news/introducing-the-open-platform-for-enterprise-ai.html](https://www.intel.com/content/www/us/en/developer/articles/news/introducing-the-open-platform-for-enterprise-ai.html)
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 介绍企业AI开放平台. (无日期). Intel. [https://www.intel.com/content/www/us/en/developer/articles/news/introducing-the-open-platform-for-enterprise-ai.html](https://www.intel.com/content/www/us/en/developer/articles/news/introducing-the-open-platform-for-enterprise-ai.html)
- en: 'Wu, S., Irsoy, O., Lu, S., Dabravolski, V., Dredze, M., Gehrmann, S., Kambadur,
    P., Rosenberg, D., & Mann, G. (2023, March 30). BloombergGPT: A large language
    model for finance. arXiv.org. [https://arxiv.org/abs/2303.17564](https://arxiv.org/abs/2303.17564)'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Wu, S., Irsoy, O., Lu, S., Dabravolski, V., Dredze, M., Gehrmann, S., Kambadur,
    P., Rosenberg, D., & Mann, G. (2023年3月30日). BloombergGPT: 一种面向金融的大型语言模型. arXiv.org.
    [https://arxiv.org/abs/2303.17564](https://arxiv.org/abs/2303.17564)'
- en: 'Yang, H., Liu, X., & Wang, C. D. (2023, June 9). FINGPT: Open-Source Financial
    Large Language Models. arXiv.org. [https://arxiv.org/abs/2306.06031](https://arxiv.org/abs/2306.06031)'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Yang, H., Liu, X., & Wang, C. D. (2023年6月9日). FINGPT: 开源金融大型语言模型. arXiv.org.
    [https://arxiv.org/abs/2306.06031](https://arxiv.org/abs/2306.06031)'
- en: AI4Finance-Foundation. (n.d.). FinGPT. GitHub. [https://github.com/AI4Finance-Foundation/FinGPT](https://github.com/AI4Finance-Foundation/FinGPT)
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AI4Finance-Foundation. (无日期). FinGPT. GitHub. [https://github.com/AI4Finance-Foundation/FinGPT](https://github.com/AI4Finance-Foundation/FinGPT)
- en: Starcoder2\. (n.d.). GitHub. [https://huggingface.co/docs/transformers/v4.39.0/en/model_doc/starcoder2](https://huggingface.co/docs/transformers/v4.39.0/en/model_doc/starcoder2)
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Starcoder2\. (无日期). GitHub. [https://huggingface.co/docs/transformers/v4.39.0/en/model_doc/starcoder2](https://huggingface.co/docs/transformers/v4.39.0/en/model_doc/starcoder2)
- en: 'SetFit: Efficient Few-Shot Learning Without Prompts. (n.d.). [https://huggingface.co/blog/setfit](https://huggingface.co/blog/setfit)'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'SetFit: 无需提示的高效少量样本学习. (无日期). [https://huggingface.co/blog/setfit](https://huggingface.co/blog/setfit)'
- en: 'SetFitABSA: Few-Shot Aspect Based Sentiment Analysis Using SetFit. (n.d.).
    [https://huggingface.co/blog/setfit-absa](https://huggingface.co/blog/setfit-absa)'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'SetFitABSA: 使用SetFit进行少量样本的基于方面的情感分析. (无日期). [https://huggingface.co/blog/setfit-absa](https://huggingface.co/blog/setfit-absa)'
- en: Intel/neural-chat-7b-v3–1\. Hugging Face. (2023, October 12). [https://huggingface.co/Intel/neural-chat-7b-v3-1](https://huggingface.co/Intel/neural-chat-7b-v3-1)
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Intel/neural-chat-7b-v3–1\. Hugging Face. (2023年10月12日). [https://huggingface.co/Intel/neural-chat-7b-v3-1](https://huggingface.co/Intel/neural-chat-7b-v3-1)
- en: 'Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L.,
    & Chen, W. (2021, June 17). LORA: Low-Rank adaptation of Large Language Models.
    arXiv.org. [https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L.,
    & Chen, W. (2021年6月17日). LORA: 大型语言模型的低秩适应. arXiv.org. [https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)'
- en: 'Dettmers, T., Pagnoni, A., Holtzman, A., & Zettlemoyer, L. (2023, May 23).
    QLORA: Efficient Finetuning of Quantized LLMS. arXiv.org. [https://arxiv.org/abs/2305.14314](https://arxiv.org/abs/2305.14314)'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Dettmers, T., Pagnoni, A., Holtzman, A., & Zettlemoyer, L. (2023年5月23日). QLORA:
    量化LLMS的高效微调. arXiv.org. [https://arxiv.org/abs/2305.14314](https://arxiv.org/abs/2305.14314)'
- en: Leviathan, Y., Kalman, M., & Matias, Y. (2022, November 30). Fast Inference
    from Transformers via Speculative Decoding. arXiv.org. [https://arxiv.org/abs/2211.17192](https://arxiv.org/abs/2211.17192)
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Leviathan, Y., Kalman, M., & Matias, Y. (2022年11月30日). 通过推测解码实现快速推理. arXiv.org.
    [https://arxiv.org/abs/2211.17192](https://arxiv.org/abs/2211.17192)
- en: Bastian, M. (2023, July 3). GPT-4 has more than a trillion parameters — Report.
    THE DECODER. [https://the-decoder.com/gpt-4-has-a-trillion-parameters/](https://the-decoder.com/gpt-4-has-a-trillion-parameters/)
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bastian, M. (2023年7月3日). GPT-4拥有超过一万亿个参数 — 报告. THE DECODER. [https://the-decoder.com/gpt-4-has-a-trillion-parameters/](https://the-decoder.com/gpt-4-has-a-trillion-parameters/)
- en: Andriole, S. (2023, September 12). LLAMA, ChatGPT, Bard, Co-Pilot & all the
    rest. How large language models will become huge cloud services with massive ecosystems.
    Forbes. [https://www.forbes.com/sites/steveandriole/2023/07/26/llama-chatgpt-bard-co-pilot--all-the-rest--how-large-language-models-will-become-huge-cloud-services-with-massive-ecosystems/?sh=78764e1175b7](https://www.forbes.com/sites/steveandriole/2023/07/26/llama-chatgpt-bard-co-pilot--all-the-rest--how-large-language-models-will-become-huge-cloud-services-with-massive-ecosystems/?sh=78764e1175b7)
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Andriole, S. (2023年9月12日). LLAMA、ChatGPT、Bard、Co-Pilot 及其他。大规模语言模型如何成为庞大的云服务，并拥有巨大的生态系统。
    Forbes. [https://www.forbes.com/sites/steveandriole/2023/07/26/llama-chatgpt-bard-co-pilot--all-the-rest--how-large-language-models-will-become-huge-cloud-services-with-massive-ecosystems/?sh=78764e1175b7](https://www.forbes.com/sites/steveandriole/2023/07/26/llama-chatgpt-bard-co-pilot--all-the-rest--how-large-language-models-will-become-huge-cloud-services-with-massive-ecosystems/?sh=78764e1175b7)
- en: 'Q8-Chat LLM: An efficient generative AI experience on Intel® CPUs. (n.d.).
    Intel. [https://www.intel.com/content/www/us/en/developer/articles/case-study/q8-chat-efficient-generative-ai-experience-xeon.html#gs.36q4lk](https://www.intel.com/content/www/us/en/developer/articles/case-study/q8-chat-efficient-generative-ai-experience-xeon.html#gs.36q4lk)'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Q8-Chat LLM：在 Intel® CPU 上高效的生成式 AI 体验. (无日期). Intel. [https://www.intel.com/content/www/us/en/developer/articles/case-study/q8-chat-efficient-generative-ai-experience-xeon.html#gs.36q4lk](https://www.intel.com/content/www/us/en/developer/articles/case-study/q8-chat-efficient-generative-ai-experience-xeon.html#gs.36q4lk)
- en: Ollama. (n.d.). Ollama. [https://ollama.com/](https://ollama.com/)
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ollama. (无日期). Ollama. [https://ollama.com/](https://ollama.com/)
- en: AI Accelerated Intel® Xeon® Scalable Processors Product Brief. (n.d.). Intel.
    [https://www.intel.com/content/www/us/en/products/docs/processors/xeon-accelerated/ai-accelerators-product-brief.html](https://www.intel.com/content/www/us/en/products/docs/processors/xeon-accelerated/ai-accelerators-product-brief.html)
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AI 加速 Intel® Xeon® 可扩展处理器产品简介. (无日期). Intel. [https://www.intel.com/content/www/us/en/products/docs/processors/xeon-accelerated/ai-accelerators-product-brief.html](https://www.intel.com/content/www/us/en/products/docs/processors/xeon-accelerated/ai-accelerators-product-brief.html)
- en: Intel® Gaudi® AI Accelerator products. (n.d.). Intel. [https://www.intel.com/content/www/us/en/products/details/processors/ai-accelerators/gaudi-overview.html](https://www.intel.com/content/www/us/en/products/details/processors/ai-accelerators/gaudi-overview.html)
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Intel® Gaudi® AI 加速器产品. (无日期). Intel. [https://www.intel.com/content/www/us/en/products/details/processors/ai-accelerators/gaudi-overview.html](https://www.intel.com/content/www/us/en/products/details/processors/ai-accelerators/gaudi-overview.html)
- en: Confidential Computing Solutions — Intel. (n.d.). Intel. [https://www.intel.com/content/www/us/en/security/confidential-computing.html](https://www.intel.com/content/www/us/en/security/confidential-computing.html)
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保密计算解决方案 — Intel. (无日期). Intel. [https://www.intel.com/content/www/us/en/security/confidential-computing.html](https://www.intel.com/content/www/us/en/security/confidential-computing.html)
- en: What is a Trusted Execution Environment? (n.d.). Intel. [https://www.intel.com/content/www/us/en/content-details/788130/what-is-a-trusted-execution-environment.html](https://www.intel.com/content/www/us/en/content-details/788130/what-is-a-trusted-execution-environment.html)
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是受信执行环境？ (无日期). Intel. [https://www.intel.com/content/www/us/en/content-details/788130/what-is-a-trusted-execution-environment.html](https://www.intel.com/content/www/us/en/content-details/788130/what-is-a-trusted-execution-environment.html)
- en: Adeojo, J. (2023, December 3). GPT-4 Debates Open Orca-2–13B with Surprising
    Results! Medium. [https://pub.aimind.so/gpt-4-debates-open-orca-2-13b-with-surprising-results-b4ada53845ba](https://pub.aimind.so/gpt-4-debates-open-orca-2-13b-with-surprising-results-b4ada53845ba)
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Adeojo, J. (2023年12月3日). GPT-4 与 Open Orca-2–13B 辩论，结果令人惊讶！ Medium. [https://pub.aimind.so/gpt-4-debates-open-orca-2-13b-with-surprising-results-b4ada53845ba](https://pub.aimind.so/gpt-4-debates-open-orca-2-13b-with-surprising-results-b4ada53845ba)
- en: 'Data Centric. (2023, November 30). Surprising Debate Showdown: GPT-4 Turbo
    vs. Orca-2–13B — Programmed with AutoGen! [Video]. YouTube. [https://www.youtube.com/watch?v=JuwJLeVlB-w](https://www.youtube.com/watch?v=JuwJLeVlB-w)'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Data Centric. (2023年11月30日). 惊人的辩论对决：GPT-4 Turbo 对决 Orca-2–13B — 使用 AutoGen
    编程！ [视频]. YouTube. [https://www.youtube.com/watch?v=JuwJLeVlB-w](https://www.youtube.com/watch?v=JuwJLeVlB-w)
