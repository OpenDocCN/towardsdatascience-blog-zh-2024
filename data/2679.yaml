- en: Dynamic Execution
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态执行
- en: 原文：[https://towardsdatascience.com/dynamic-execution-3d9f5380a7a8?source=collection_archive---------4-----------------------#2024-11-03](https://towardsdatascience.com/dynamic-execution-3d9f5380a7a8?source=collection_archive---------4-----------------------#2024-11-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/dynamic-execution-3d9f5380a7a8?source=collection_archive---------4-----------------------#2024-11-03](https://towardsdatascience.com/dynamic-execution-3d9f5380a7a8?source=collection_archive---------4-----------------------#2024-11-03)
- en: '**Getting your AI task to distinguish between Hard and Easy problems**'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**让你的AI任务区分难题与易题**'
- en: '[](https://medium.com/@haim_69762?source=post_page---byline--3d9f5380a7a8--------------------------------)[![Haim
    Barad](../Images/4cf6a9f80e9f0ad7543857a95f1e64bb.png)](https://medium.com/@haim_69762?source=post_page---byline--3d9f5380a7a8--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3d9f5380a7a8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3d9f5380a7a8--------------------------------)
    [Haim Barad](https://medium.com/@haim_69762?source=post_page---byline--3d9f5380a7a8--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@haim_69762?source=post_page---byline--3d9f5380a7a8--------------------------------)[![Haim
    Barad](../Images/4cf6a9f80e9f0ad7543857a95f1e64bb.png)](https://medium.com/@haim_69762?source=post_page---byline--3d9f5380a7a8--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3d9f5380a7a8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3d9f5380a7a8--------------------------------)
    [Haim Barad](https://medium.com/@haim_69762?source=post_page---byline--3d9f5380a7a8--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3d9f5380a7a8--------------------------------)
    ·10 min read·Nov 3, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3d9f5380a7a8--------------------------------)
    ·10分钟阅读·2024年11月3日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: In this position paper, I discuss the premise that a lot of potential performance
    enhancement is left on the table because we don’t often address the potential
    of dynamic execution.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇立场论文中，我讨论了这样一个前提：大量的潜在性能提升机会被忽视了，因为我们往往没有关注动态执行的潜力。
- en: I guess I need to first define what is *dynamic execution* in this context.
    As many of you are no doubt aware of, we often address performance optimizations
    by taking a good look at the model itself and what can be done to make processing
    of this model more efficient (which can be measured in terms of lower latency,
    higher throughput and/or energy savings).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我想我需要首先定义在这个语境下什么是*动态执行*。正如你们很多人无疑已经意识到的那样，我们常常通过仔细审视模型本身，来优化性能，看看有什么方法能使得该模型的处理更加高效（这可以通过降低延迟、提高吞吐量和/或节省能源来衡量）。
- en: These methods often address the size of the model, so we look for ways to compress
    the model. If the model is smaller, then memory footprint and bandwidth requirements
    are improved. Some methods also address sparsity within the model, thus avoiding
    inconsequential calculations.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法通常关注模型的大小，因此我们寻找压缩模型的方法。如果模型更小，那么内存占用和带宽需求就会得到改善。一些方法还针对模型内部的稀疏性进行优化，从而避免无关的计算。
- en: Still… we are only looking at the model itself.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但是…我们现在仅仅是在看模型本身。
- en: This is definitely something we want to do, but are there additional opportunities
    we can leverage to boost performance even more? Often, we overlook the most human-intuitive
    methods that don’t focus on the model size.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这绝对是我们想做的事情，但是否有其他的机会可以利用，进一步提升性能？我们经常忽视那些不专注于模型大小的、最具人类直觉的方法。
- en: '![](../Images/61fa370e56583f7a770dbd7bf0e4ddfd.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61fa370e56583f7a770dbd7bf0e4ddfd.png)'
- en: Figure 1\. Intuition of hard vs easy decisions
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图1. 难题与易题的直觉区别
- en: '**Hard vs Easy**'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**难题与易题**'
- en: In Figure 1, there’s a simple example (perhaps a bit simplistic) regarding how
    to classify between red and blue data points. It would be really useful to be
    able to draw a decision boundary so that we know the red and blue points are on
    opposite sides of the boundary as much as possible. One method is to do a linear
    regression whereby we fit a straight line as best as we can to separate the data
    points as much as possible. The bold black line in Figure 1 represents one potential
    boundary. Focusing only on the bold black line, you can see that there is a substantial
    number of points that fall on the wrong side of the boundary, but it does a decent
    job most of the time.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在图1中，有一个简单的例子（可能有点过于简化），展示了如何对红色和蓝色数据点进行分类。能够绘制决策边界会非常有用，这样我们就能尽可能地让红色和蓝色数据点位于边界的两侧。一种方法是进行线性回归，在尽可能的情况下拟合一条直线，以便尽可能地将数据点分开。图1中的粗黑线代表一个潜在的边界。仅关注粗黑线，你可以看到有相当数量的点落在边界的错误一侧，但大多数时候它的表现还是相当不错的。
- en: If we focus on the curved line, this does a much better job, but it’s also more
    difficult to compute as it’s no longer a simple, linear equation. If we want more
    accuracy, clearly the curve is a much better decision boundary than the black
    line.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们关注曲线，这样的表现要好得多，但计算上也更困难，因为它不再是一个简单的线性方程。如果我们想要更高的精确度，显然曲线比黑线更适合作为决策边界。
- en: But let’s not just throw out the black line just yet. Now let’s look at the
    green parallel lines on each side of the black boundary. Note that the linear
    decision boundary is very accurate for points outside of the green line. Let’s
    call these points “Easy”.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们暂时不要完全舍弃黑线。现在让我们看看黑色边界两侧的绿色平行线。请注意，线性决策边界对于绿色线外的点非常准确。我们把这些点称为“简单”。
- en: In fact, it is 100% as accurate as the curved boundary for Easy points. Points
    that lie inside the green lines are “Hard” and there is a clear advantage to using
    the more complex decision boundary for these points.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，对于“简单”点，它的准确性和曲线边界是100%一致的。位于绿色线内的点是“困难”的，使用更复杂的决策边界处理这些点显然有优势。
- en: So… if we can tell if the input data is hard or easy, we can apply different
    methods to solving the problem with no loss of accuracy and a clear savings of
    computations for the easy points.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 所以……如果我们能够判断输入数据是简单还是复杂，我们可以采用不同的方法来解决问题，在不失去准确度的前提下，对简单的数据点节省计算资源。
- en: This is very intuitive as this is exactly how humans address problems. If we
    perceive a problem as easy, we often don’t think too hard about it and give an
    answer quickly. If we perceive a problem as being hard, we think more about it
    and often it takes more time to get to the answer.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这是非常直观的，因为这正是人类解决问题的方式。如果我们认为一个问题简单，我们通常不会想太多，迅速给出答案；而如果我们认为一个问题很困难，我们会思考得更多，通常也需要更多的时间来得出答案。
- en: So, can we apply a similar approach to AI?
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们能否将类似的方法应用到人工智能中呢？
- en: '**Dynamic Execution Methods**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**动态执行方法**'
- en: In the dynamic execution scenario, we employ a set of specialized techniques
    designed to scrutinize the specific query at hand. These techniques involve a
    thorough examination of the query’s structure, content, and context with the aim
    of discerning whether the problem it represents can be addressed in a more straightforward
    manner.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在动态执行场景中，我们采用一套专门的技术，旨在仔细审查当前的查询。这些技术涉及对查询的结构、内容和上下文进行彻底检查，目的是判断它所代表的问题是否可以通过更简单的方式来解决。
- en: This approach mirrors the way humans tackle problem-solving. Just as we, as
    humans, are often able to identify problems that are ’easy’ or ’simple’ and solve
    them with less effort compared to ’hard’ or ’complex’ problems, these techniques
    strive to do the same. They are designed to recognize simpler problems and solve
    them more efficiently, thereby saving computational resources and time.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法类似于人类解决问题的方式。就像我们人类经常能够识别出“简单”或“易懂”的问题，并比“困难”或“复杂”的问题用更少的精力解决它们一样，这些技术也力图做到这一点。它们旨在识别出更简单的问题，并更高效地解决，从而节省计算资源和时间。
- en: This is why we refer to these techniques as Dynamic Execution. The term ’dynamic’
    signifies the adaptability and flexibility of this approach. Unlike static methods
    that rigidly adhere to a predetermined path regardless of the problem’s nature,
    Dynamic Execution adjusts its strategy based on the specific problem it encounters,
    that is, the opportunity is data dependent.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们将这些技术称为动态执行的原因。术语“动态”表示这种方法的适应性和灵活性。与静态方法不同，静态方法无论问题的性质如何，都会严格遵循预定的路径，而动态执行则根据它遇到的具体问题调整策略，也就是说，机会是数据依赖的。
- en: The goal of Dynamic Execution is not to optimize the model itself, but to optimize
    the compute flow. In other words, it seeks to streamline the process through which
    the model interacts with the data. By tailoring the compute flow to the data presented
    to the model, Dynamic Execution ensures that the model’s computational resources
    are utilized in the most efficient manner possible.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 动态执行的目标不是优化模型本身，而是优化计算流程。换句话说，它旨在通过调整模型与数据的交互过程来简化流程。通过将计算流程调整为适应提供给模型的数据，动态执行确保模型的计算资源以最有效的方式得到利用。
- en: In essence, Dynamic Execution is about making the problem-solving process as
    efficient and effective as possible by adapting the strategy to the problem at
    hand, much like how humans approach problem-solving. It is about working smarter,
    not harder. This approach not only saves computational resources but also improves
    the speed and accuracy of the problem-solving process.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，动态执行旨在通过将策略适应于当前问题，使问题解决过程尽可能高效和有效，类似于人类解决问题的方式。它关注的是更聪明地工作，而不是更努力地工作。这种方法不仅节省了计算资源，还提高了问题解决过程的速度和准确性。
- en: '**Early Exit**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**早期退出**'
- en: This technique involves adding exits at various stages in a deep neural network
    (DNN). The idea is to allow the network to terminate the inference process earlier
    for simpler tasks, thus saving computational resources. It takes advantage of
    the observation that some test examples can be easier to predict than others [1],
    [2].
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 该技术涉及在深度神经网络（DNN）的各个阶段添加退出点。其想法是让网络在处理简单任务时能够更早终止推理过程，从而节省计算资源。它利用了这样的观察：某些测试示例比其他示例更容易预测[1]，[2]。
- en: Below is an example of the Early Exit strategy in several encoder models, including
    BERT, ROBERTA, and ALBERT.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是早期退出策略在多个编码器模型中的应用示例，包括BERT、ROBERTA和ALBERT。
- en: We measured the speed-ups on glue scores for various entropy thresholds. Figure
    2 shows a plot of these scores and how they drop with respect to the entropy threshold.
    The scores show the percentage of the baseline score (that is, without Early Exit).
    Note that we can get 2x to 4X speed-up without sacrificing much quality.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在不同的熵阈值下测量了粘合分数的加速情况。图2显示了这些分数的变化以及它们如何随着熵阈值的变化而下降。分数表示基准分数的百分比（即没有使用早期退出的情况）。注意，我们可以在不牺牲太多质量的情况下实现2倍到4倍的加速。
- en: '![](../Images/9612a457b5601693dc674529b94a44bc.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9612a457b5601693dc674529b94a44bc.png)'
- en: 'Figure 2\. Early Exit: SST-2'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图2. 早期退出：SST-2
- en: '**Speculative Sampling**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**推测采样**'
- en: This method aims to speed up the inference process by computing several candidate
    tokens from a smaller draft model. These candidate tokens are then evaluated in
    parallel in the full target model [3], [4].
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法通过从一个较小的草稿模型中计算多个候选令牌来加速推理过程。这些候选令牌随后在完整的目标模型中并行评估[3]，[4]。
- en: Speculative sampling is a technique designed to accelerate the decoding process
    of large language models [5], [6]. The concept behind speculative sampling is
    based on the observation that the latency of parallel scoring of short continuations,
    generated by a faster but less powerful draft model, is comparable to that of
    sampling a single token from the larger target model. This approach allows multiple
    tokens to be generated from each transformer call, increasing the speed of the
    decoding process.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 推测采样是一种旨在加速大规模语言模型解码过程的技术[5]，[6]。推测采样背后的概念基于这样的观察：通过一个更快但更不强大的草稿模型生成的短文本续写的并行评分延迟，和从更大的目标模型中采样单个令牌的延迟是相当的。这种方法允许从每次变换器调用中生成多个令牌，从而加速了解码过程。
- en: 'The process of speculative sampling involves two models: a smaller, faster
    draft model and a larger, slower target model. The draft model speculates what
    the output is several steps into the future, while the target model determines
    how many of those tokens we should accept. The draft model decodes several tokens
    in a regular autoregressive fashion, and the probability outputs of the target
    and the draft models on the new predicted sequence are compared. Based on some
    rejection criteria, it is determined how many of the speculated tokens we want
    to keep. If a token is rejected, it is resampled using a combination of the two
    distributions, and no more tokens are accepted. If all speculated tokens are accepted,
    an additional final token can be sampled from the target model probability output.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 推测采样过程涉及两个模型：一个较小、较快的草稿模型和一个较大、较慢的目标模型。草稿模型推测输出结果的未来若干步，而目标模型则确定我们应接受多少个这些推测的标记。草稿模型以常规自回归的方式解码若干个标记，并比较目标模型和草稿模型在新预测序列上的概率输出。根据一些拒绝标准，决定我们要保留多少个推测的标记。如果某个标记被拒绝，则通过两种分布的组合重新采样该标记，且不再接受更多标记。如果所有推测的标记都被接受，则可以从目标模型的概率输出中额外采样一个最终标记。
- en: In terms of performance boost, speculative sampling has shown significant improvements.
    For instance, it was benchmarked with Chinchilla, a 70 billion parameter language
    model, achieving a 2–2.5x decoding speedup in a distributed setup, without compromising
    the sample quality or making modifications to the model itself. Another example
    is the application of speculative decoding to Whisper, a general purpose speech
    transcription model, which resulted in a 2x speed-up in inference throughput [7],
    [8]. Note that speculative sampling can be used to boost CPU inference performance,
    but the boost will likely be less (typically around 1.5x).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在性能提升方面，推测采样显示出了显著的改进。例如，它与一个70亿参数的语言模型Chinchilla进行基准测试，在分布式设置中实现了2–2.5倍的解码加速，而不影响样本质量，也不对模型本身进行修改。另一个例子是推测解码应用于Whisper，一个通用语音转录模型，结果使推理吞吐量提高了2倍[7]，[8]。需要注意的是，推测采样可以用来提升CPU推理性能，但这种提升通常较小（通常在1.5倍左右）。
- en: In conclusion, speculative sampling is a promising technique that leverages
    the strengths of both a draft and a target model to accelerate the decoding process
    of large language models. It offers a significant performance boost, making it
    a valuable tool in the field of natural language processing. However, it is important
    to note that the actual performance boost can vary depending on the specific models
    and setup used.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，推测采样是一种有前景的技术，它利用草稿模型和目标模型的优势，加速大规模语言模型的解码过程。它提供了显著的性能提升，使其成为自然语言处理领域中的一项有价值的工具。然而，需要注意的是，实际的性能提升可能会根据使用的具体模型和设置而有所不同。
- en: '**StepSaver**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**StepSaver**'
- en: This is a method that could also be called Early Stopping for Diffusion Generation,
    using an innovative NLP model specifically fine-tuned to determine the minimal
    number of denoising steps required for any given text prompt. This advanced model
    serves as a real-time tool that recommends the ideal number of denoising steps
    for generating high-quality images efficiently. It is designed to work seamlessly
    with the Diffusion model, ensuring that images are produced with superior quality
    in the shortest possible time. [9]
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种也可以称为“扩散生成的早期停止”方法，使用一个创新的NLP模型，特别针对确定任何给定文本提示所需的最小去噪步数进行微调。该高级模型作为一个实时工具，推荐生成高质量图像所需的理想去噪步数，从而高效地生成图像。它与扩散模型无缝配合，确保在最短的时间内生成优质图像[9]。
- en: Diffusion models iteratively enhance a random noise signal until it closely
    resembles the target data distribution [10]. When generating visual content such
    as images or videos, diffusion models have demonstrated significant realism [11].
    For example, video diffusion models and SinFusion represent instances of diffusion
    models utilized in video synthesis [12][13]. More recently, there has been growing
    attention towards models like OpenAI’s Sora; however, this model is currently
    not publicly available due to its proprietary nature.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型通过迭代增强随机噪声信号，直到它与目标数据分布高度相似[10]。在生成视觉内容（如图像或视频）时，扩散模型展现出了显著的逼真性[11]。例如，视频扩散模型和SinFusion代表了扩散模型在视频合成中的应用实例[12][13]。最近，像OpenAI的Sora这样的模型也引起了越来越多的关注；然而，由于其专有性质，该模型目前尚未公开。
- en: Performance in diffusion models involves a large number of iterations to recover
    images or videos from Gaussian noise [14]. This process is called denoising and
    is trained on a specific number of iterations of denoising. The number of iterations
    in this sampling procedure is a key factor in the quality of the generated data,
    as measured by metrics, such as FID.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型中的性能涉及大量迭代，以从高斯噪声中恢复图像或视频[14]。这个过程称为去噪，并且是在特定数量的去噪迭代下进行训练的。该采样过程中的迭代次数是生成数据质量的一个关键因素，通过诸如
    FID 等指标来衡量。
- en: Latent space diffusion inference uses iterations in feature space, and performance
    suffers from the expense of many iterations required for quality output. Various
    techniques, such as patching transformation and transformer-based diffusion models
    [15], improve the efficiency of each iteration.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在空间扩散推理使用特征空间中的迭代，且由于输出质量所需的众多迭代，其性能会受到影响。各种技术，如补丁变换和基于变换器的扩散模型[15]，提高了每次迭代的效率。
- en: StepSaver dynamically recommends significantly lower denoising steps, which
    is critical to address the slow sampling issue of stable diffusion models during
    image generation [9]. The recommended steps also ensure better image quality.
    Figure 3 shows that images generated using dynamic steps result in a 3X throughput
    improvement and a similar image quality compared to static 100 steps.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: StepSaver 动态推荐显著较低的去噪步骤，这对于解决稳定扩散模型在图像生成过程中出现的慢采样问题至关重要[9]。推荐的步骤还确保了更好的图像质量。图
    3 显示，使用动态步骤生成的图像相比于静态 100 步骤，带来了 3 倍的吞吐量提升，并且图像质量相似。
- en: '![](../Images/30e3b7d5727a748240d92947d8db9ed1.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30e3b7d5727a748240d92947d8db9ed1.png)'
- en: Figure 3\. StepSaver Performance
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. StepSaver 性能
- en: '**LLM Routing**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**LLM 路由**'
- en: Dynamic Execution isn’t limited to just optimizing a specific task (e.g. generating
    a sequence of text). We can take a step above the LLM and look at the entire pipeline.
    Suppose we are running a huge LLM in our data center (or we’re being billed by
    OpenAI for token generation via their API), can we optimize the calls to LLM so
    that we select the best LLM for the job (and “best” could be a function of token
    generation cost). Complicated prompts might require a more expensive LLM, but
    many prompts can be handled with much lower cost on a simpler LLM (or even locally
    on your notebook). So if we can route our prompt to the appropriate destination,
    then we can optimize our tasks based on several criteria.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 动态执行不仅限于优化特定任务（例如生成文本序列）。我们可以超越 LLM 层级，查看整个管道。假设我们在数据中心运行一个巨大的 LLM（或通过 OpenAI
    的 API 为代币生成付费），我们能否优化对 LLM 的调用，以便选择最适合的 LLM（“最佳”可能是代币生成成本的函数）。复杂的提示词可能需要更昂贵的 LLM，但许多提示词可以在更简单的
    LLM 上处理，甚至可以在你的笔记本上本地处理。因此，如果我们能够将提示词路由到合适的目标位置，那么我们就可以基于多个标准优化我们的任务。
- en: Routing is a form of classification in which the prompt is used to determine
    the best model. The prompt is then routed to this model. By best, we can use different
    criteria to determine the most effective model in terms of cost and accuracy.
    In many ways, routing is a form of dynamic execution done at the pipeline level
    where many of the other optimizations we are focusing on in this paper is done
    to make each LLM more efficient. For example, RouteLLM is an open-source framework
    for serving LLM routers and provides several mechanisms for reference, such as
    matrix factorization. [16] In this study, the researchers at LMSys were able to
    save 85% of costs while still keeping 95% accuracy.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 路由是一种分类形式，其中提示词用于确定最佳模型。然后，提示词被路由到该模型。所谓的“最佳”可以通过不同的标准来确定在成本和准确性方面最有效的模型。在许多方面，路由是一种在管道级别执行的动态执行方式，其中我们在本文中关注的许多优化工作正是为了提高每个
    LLM 的效率。例如，RouteLLM 是一个开源框架，用于服务 LLM 路由器，并提供了多个机制供参考，例如矩阵分解[16]。在这项研究中，LMSys 的研究人员能够在保持
    95% 准确率的同时节省 85% 的成本。
- en: '**Conclusion**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**结论**'
- en: This certainly was not meant to be an exhaustive study of all dynamic execution
    methods, but it should provide data scientists and engineers with the motivation
    to find additional performance boosts and cost savings from the characteristics
    of the data and not solely focus on model-based methods. Dynamic Execution provides
    this opportunity and does not interfere with or hamper traditional model-based
    optimization efforts.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这显然不是对所有动态执行方法的详尽研究，但它应该为数据科学家和工程师提供动机，从数据的特性中寻找额外的性能提升和成本节约，而不仅仅是专注于基于模型的方法。动态执行提供了这种机会，并且不会干扰或妨碍传统基于模型的优化工作。
- en: '*Unless otherwise noted, all images are by the author.*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有注明，所有图片均由作者提供。*'
- en: '[1] *K. Liao, Y. Zhang, X. Ren, Q. Su, X. Sun, and B. He, “A Global Past-Future
    Early Exit Method for Accelerating Inference of Pre-trained Language Models,”
    in Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, pp. 2013–2023, Association for Computational
    Linguistics (ACL), June 2021.*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] *K. Liao, Y. Zhang, X. Ren, Q. Su, X. Sun, 和 B. He, “一种用于加速预训练语言模型推理的全球性过去-未来早期退出方法，”
    载于《北美计算语言学学会会议：人类语言技术》，第2013–2023页，计算语言学协会（ACL），2021年6月。*'
- en: '[2] *F. Ilhan, K.-H. Chow, S. Hu, T. Huang, S. Tekin, W. Wei, Y. Wu, M. Lee,
    R. Kompella, H. Latapie, G. Liu, and L. Liu, “Adaptive Deep Neural Network Inference
    Optimization with EENet,” Dec. 2023\. arXiv:2301.07099 [cs].*'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] *F. Ilhan, K.-H. Chow, S. Hu, T. Huang, S. Tekin, W. Wei, Y. Wu, M. Lee,
    R. Kompella, H. Latapie, G. Liu, 和 L. Liu, “基于EENet的自适应深度神经网络推理优化，” 2023年12月。arXiv:2301.07099
    [cs]。*'
- en: '[3] *Y. Leviathan, M. Kalman, and Y. Matias, “Fast Inference from Transformers
    via Speculative Decoding,” May 2023\. arXiv:2211.17192 [cs].*'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] *Y. Leviathan, M. Kalman, 和 Y. Matias, “通过投机解码实现Transformer快速推理，” 2023年5月。arXiv:2211.17192
    [cs]。*'
- en: '[4] *H. Barad, E. Aidova, and Y. Gorbachev, “Leveraging Speculative Sampling
    and KV-Cache Optimizations Together for Generative AI using OpenVINO,” Nov. 2023\.
    arXiv:2311.04951 [cs].*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] *H. Barad, E. Aidova, 和 Y. Gorbachev, “结合投机采样和KV-缓存优化共同推动基于OpenVINO的生成式AI应用，”
    2023年11月。arXiv:2311.04951 [cs]。*'
- en: '[5] *C. Chen, S. Borgeaud, G. Irving, J.-B. Lespiau, L. Sifre, and J. Jumper,
    “Accelerating Large Language Model Decoding with Speculative Sampling,” Feb. 2023\.
    arXiv:2302.01318 [cs] version: 1.*'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] *C. Chen, S. Borgeaud, G. Irving, J.-B. Lespiau, L. Sifre, 和 J. Jumper,
    “通过投机采样加速大规模语言模型解码，” 2023年2月。arXiv:2302.01318 [cs] 版本：1。*'
- en: '[6] *J. Mody, “Speculative Sampling,” Feb. 2023.*'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] *J. Mody, “投机采样，” 2023年2月。*'
- en: '[7] *J. Gante, “Assisted Generation: a new direction toward low-latency text
    generation,” May 2023.*'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] *J. Gante, “辅助生成：迈向低延迟文本生成的新方向，” 2023年5月。*'
- en: '[8] *S. Gandhi, “Speculative Decoding for 2x Faster Whisper Inference.”*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] *S. Gandhi, “用于2倍加速Whisper推理的投机解码。”*'
- en: '[9] *J. Yu and H. Barad, “Step Saver: Predicting Minimum Denoising Steps for
    Diffusion Model Image Generation,” Aug. 2024\. arXiv:2408.02054 [cs].*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] *J. Yu 和 H. Barad, “Step Saver：预测扩散模型图像生成的最小去噪步骤，” 2024年8月。arXiv:2408.02054
    [cs]。*'
- en: '[10] *Notomoro, “Diffusion Model: A Comprehensive Guide With Example,” Feb.
    2024\. Section: Artificial Intelligence.*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] *Notomoro, “扩散模型：带示例的全面指南，” 2024年2月。章节：人工智能。*'
- en: '[11] *T. H¨oppe, A. Mehrjou, S. Bauer, D. Nielsen, and A. Dittadi, “Diffusion
    Models for Video Prediction and Infilling,” Nov. 2022\. arXiv:2206.07696 [cs,
    stat].*'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] *T. H¨oppe, A. Mehrjou, S. Bauer, D. Nielsen, 和 A. Dittadi, “用于视频预测和填充的扩散模型，”
    2022年11月。arXiv:2206.07696 [cs, stat]。*'
- en: '[12] *J. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and D. J. Fleet,
    “Video Diffusion Models,” June 2022\. arXiv:2204.03458 [cs].*'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] *J. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, 和 D. J. Fleet,
    “视频扩散模型，” 2022年6月。arXiv:2204.03458 [cs]。*'
- en: '[13] *Y. Nikankin, N. Haim, and M. Irani, “SinFusion: Training Diffusion Models
    on a Single Image or Video,” June 2023\. arXiv:2211.11743 [cs].*'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] *Y. Nikankin, N. Haim, 和 M. Irani, “SinFusion：在单一图像或视频上训练扩散模型，” 2023年6月。arXiv:2211.11743
    [cs]。*'
- en: '[14] *Z. Chen, Y. Zhang, D. Liu, B. Xia, J. Gu, L. Kong, and X. Yuan, “Hierarchical
    Integration Diffusion Model for Realistic Image Deblurring,” Sept. 2023\. arXiv:2305.12966
    [cs]*'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] *Z. Chen, Y. Zhang, D. Liu, B. Xia, J. Gu, L. Kong, 和 X. Yuan, “用于真实图像去模糊的层次集成扩散模型，”
    2023年9月。arXiv:2305.12966 [cs]。*'
- en: '[15] *W. Peebles and S. Xie, “Scalable Diffusion Models with Transformers,”
    Mar. 2023\. arXiv:2212.09748 [cs].*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] *W. Peebles 和 S. Xie, “基于Transformer的可扩展扩散模型，” 2023年3月。arXiv:2212.09748
    [cs]。*'
- en: '[16] *I. Ong, A. Almahairi, V. Wu, W.-L. Chiang, T. Wu, J. E. Gonzalez, M.
    W. Kadous, and I. Stoica, “RouteLLM: Learning to Route LLMs with Preference Data,”
    July 2024\. arXiv:2406.18665 [cs].*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] *I. Ong, A. Almahairi, V. Wu, W.-L. Chiang, T. Wu, J. E. Gonzalez, M.
    W. Kadous, 和 I. Stoica, “RouteLLM：通过偏好数据学习路由LLM，” 2024年7月。arXiv:2406.18665 [cs]。*'
