- en: The Evolution of Text to Video Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本到视频模型的演变
- en: 原文：[https://towardsdatascience.com/the-evolution-of-text-to-video-models-1577878043bd?source=collection_archive---------3-----------------------#2024-09-19](https://towardsdatascience.com/the-evolution-of-text-to-video-models-1577878043bd?source=collection_archive---------3-----------------------#2024-09-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-evolution-of-text-to-video-models-1577878043bd?source=collection_archive---------3-----------------------#2024-09-19](https://towardsdatascience.com/the-evolution-of-text-to-video-models-1577878043bd?source=collection_archive---------3-----------------------#2024-09-19)
- en: Simplifying the neural nets behind Generative Video Diffusion
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简化生成视频扩散背后的神经网络
- en: '[](https://medium.com/@neural.avb?source=post_page---byline--1577878043bd--------------------------------)[![Avishek
    Biswas](../Images/6feb591069f354aa096f6108f1a70ea7.png)](https://medium.com/@neural.avb?source=post_page---byline--1577878043bd--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1577878043bd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1577878043bd--------------------------------)
    [Avishek Biswas](https://medium.com/@neural.avb?source=post_page---byline--1577878043bd--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@neural.avb?source=post_page---byline--1577878043bd--------------------------------)[![Avishek
    Biswas](../Images/6feb591069f354aa096f6108f1a70ea7.png)](https://medium.com/@neural.avb?source=post_page---byline--1577878043bd--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1577878043bd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1577878043bd--------------------------------)
    [Avishek Biswas](https://medium.com/@neural.avb?source=post_page---byline--1577878043bd--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1577878043bd--------------------------------)
    ·9 min read·Sep 19, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[数据科学前沿](https://towardsdatascience.com/?source=post_page---byline--1577878043bd--------------------------------)
    ·阅读时间9分钟·2024年9月19日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d4e0a8ffc80f7a943eba83fbb819d59d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d4e0a8ffc80f7a943eba83fbb819d59d.png)'
- en: Let’s learn Video Diffusion!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来学习视频扩散！
- en: We’ve witnessed remarkable strides in AI image generation. But what happens
    when we add the dimension of time? Videos are moving images, after all.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经见证了AI图像生成的显著进步。但当我们加入时间这一维度时会发生什么呢？毕竟，视频是动态图像。
- en: '*Text-to-video generation is a complex task that requires AI to understand
    not just what things look like, but how they move and interact over time. It is
    an order of magnitude more complex than text-to-image.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*文本到视频生成是一项复杂的任务，它要求AI不仅要理解物体的外观，还要理解它们如何随着时间的推移运动和互动。这比文本到图像要复杂一个数量级。*'
- en: 'To produce a coherent video, a neural network must:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 要制作一个连贯的视频，一个神经网络必须：
- en: 1\. Comprehend the input prompt
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 理解输入提示
- en: 2\. Understand how the world works
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 理解世界是如何运作的
- en: 3\. Know how objects move and how physics applies
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 了解物体如何运动以及物理学如何应用
- en: 4\. Generate a sequence of frames that make sense spatially, temporally, and
    logically
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 生成在空间、时间和逻辑上都合理的帧序列
- en: Despite these challenges, today’s diffusion neural networks are making impressive
    progress in this field. In this article, we will cover the main ideas behind video
    diffusion models — main challenges, approaches, and the seminal papers in the
    field.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管面临这些挑战，今天的扩散神经网络在这一领域取得了令人瞩目的进展。在本文中，我们将介绍视频扩散模型背后的主要思想——主要挑战、方法以及该领域的开创性论文。
- en: '[Also, this article is based on this larger YouTube video I made. If you enjoy
    this read, you will enjoy watching the video too](https://youtu.be/KRTEOkYftUY).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[此外，本文基于我制作的更大规模YouTube视频。如果你喜欢这篇文章，你也会喜欢观看该视频](https://youtu.be/KRTEOkYftUY)。'
- en: '**Text to Image Overview**'
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**文本到图像概述**'
- en: 'To understand text-to-video generation, we need to start with its predecessor:
    text-to-image diffusion models. These models have a singular goal — to transform
    random noise and a text prompt into a coherent image. In general, all generative
    image models do this — Variational Autoencoders (VAE), Generative Adversarial
    Neural Nets (GANs), and yes, Diffusion too.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解文本到视频生成，我们需要从它的前身开始：文本到图像的扩散模型。这些模型有一个单一的目标——将随机噪声和文本提示转换为连贯的图像。一般来说，所有生成图像的模型都是这样做的——变分自编码器（VAE）、生成对抗神经网络（GAN）以及扩散模型。
- en: '![](../Images/fa3ee732372254ed0b342013d31012df.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa3ee732372254ed0b342013d31012df.png)'
- en: The basic goal of all image generation models is to convert random noise into
    an image, often conditioned on additional conditioning prompts (like text). [Image
    by Author]
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 所有图像生成模型的基本目标是将随机噪声转换为图像，通常是基于附加的条件提示（例如文本）。[图像由作者提供]
- en: Diffusion, in particular, relies on a gradual denoising process to generate
    images.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散，特别依赖于逐渐去噪的过程来生成图像。
- en: 1\. Start with a randomly generated noisy image
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 从一个随机生成的噪声图像开始
- en: 2\. Use a neural network to progressively remove noise
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 使用神经网络逐步去除噪声
- en: 3\. Condition the denoising process on text input
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 使去噪过程依赖于文本输入
- en: 4\. Repeat until a clear image emerges
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 重复直到生成清晰图像
- en: '![](../Images/addc4ac84df4107765f976727e1648e5.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/addc4ac84df4107765f976727e1648e5.png)'
- en: How Diffusion Models generate images — A neural network progressively removes
    noise from a pure noise image conditioned on a text prompt, eventually revealing
    a clear image. [Illustration by Author] (Image generated by a neural network)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型是如何生成图像的——神经网络逐步从纯噪声图像中去除噪声，并根据文本提示进行条件处理，最终显示清晰图像。 [插图：作者]（图像由神经网络生成）
- en: But how are these denoising neural networks trained?
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 但这些去噪神经网络是如何训练的呢？
- en: During training, we start with real images and progressively add noise to it
    in small steps — this is called forward diffusion. This generates a lot of samples
    of clear image and their slightly noisier versions. The neural network is then
    trained to reverse this process by inputting the noisy image and predicting how
    much noise to remove to retrieve the clearer version. In text-conditional models,
    we train attention layers to attend to the inputted prompt for guided denoising.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们从真实图像开始，逐步向其中添加噪声——这叫做正向扩散。这样会生成大量清晰图像及其稍微有噪声的版本。然后，神经网络被训练来通过输入噪声图像，预测去除多少噪声才能恢复更清晰的版本。在文本条件模型中，我们训练注意力层以关注输入的提示进行引导去噪。
- en: '![](../Images/53b828311a6b3c5f2220b6612e562002.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/53b828311a6b3c5f2220b6612e562002.png)'
- en: During training, we add noise to clear images (left) — this is called Forward
    Diffusion. The neural network is trained to reverse this noise addition process
    — a process known as Reverse Diffusion. Images generated using a neural network.
    [Image by Author]
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们向清晰图像（左）添加噪声——这叫做正向扩散。神经网络被训练来逆转这一噪声添加过程——这个过程被称为反向扩散。图像由神经网络生成。 [图片来源：作者]
- en: This iterative approach allows for the generation of highly detailed and diverse
    images. You can watch the following YouTube video where I explain text to image
    in much more detail — concepts like Forward and Reverse Diffusion, U-Net, CLIP
    models, and how I implemented them in Python and Pytorch from scratch.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这种迭代方法能够生成高度详细且多样化的图像。你可以观看以下的YouTube视频，里面我会详细讲解文本到图像的过程——比如正向和反向扩散、U-Net、CLIP模型，以及我如何从零开始在Python和Pytorch中实现这些概念。
- en: If you are comfortable with the core concepts of Text-to-Image Conditional Diffusion,
    let’s move to videos next.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经掌握了文本到图像条件扩散的核心概念，那么我们接下来可以讨论视频。
- en: 'The Temporal Dimension: A New Frontier'
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间维度：新的前沿
- en: 'In theory, we could follow the same conditioned noise-removal idea to do text-to-video
    diffusion. However, adding time into the equation introduces several new challenges:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，我们可以遵循相同的条件噪声去除思路来进行文本到视频的扩散。然而，加入时间这一因素后，带来了几个新的挑战：
- en: 1\. **Temporal Consistency:** Ensuring objects, backgrounds, and motions remain
    coherent across frames.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. **时间一致性：** 确保物体、背景和运动在各帧之间保持一致。
- en: '2\. **Computational Demands**: Generating multiple frames per second instead
    of a single image.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. **计算需求：** 生成每秒多个帧，而不是单个图像。
- en: '3\. **Data Scarcity**: While large image-text datasets are readily available,
    high-quality video-text datasets are scarce.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. **数据稀缺：** 尽管大型图像-文本数据集很容易获得，但高质量的视频-文本数据集却很稀缺。
- en: '![](../Images/f38f959fbc03992030d59eee24f1c063.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f38f959fbc03992030d59eee24f1c063.png)'
- en: Some commonly used video-text datasets [Image by Author]
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一些常用的视频-文本数据集 [图片来源：作者]
- en: The Evolution of Video Diffusion
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视频扩散的演变
- en: Because of the lack of high quality datasets, text-to-video cannot rely just
    on supervised training. And that is why people usually also combine two more data
    sources to train video diffusion models — **one — paired image-text data**, which
    is much more readily available, and **two — unlabelled video data**, which are
    super-abundant and contains lots of information about how the world works. Several
    groundbreaking models have emerged to tackle these challenges. Let’s discuss some
    of the important milestone papers one by one.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 由于缺乏高质量的数据集，文本到视频不能仅依赖监督训练。因此，通常人们还会结合另外两种数据源来训练视频扩散模型——**第一种 — 配对的图像-文本数据**，这种数据更加容易获得，和**第二种
    — 未标注的视频数据**，这种数据极为丰富，包含了大量关于世界如何运作的信息。为了解决这些挑战，出现了几种突破性的模型。我们将逐一讨论一些重要的里程碑论文。
- en: '***We are about to get into the technical nitty gritty!*** *If you find the
    material ahead difficult, feel free to watch this companion video as a visual
    side-by-side guide while reading the next section.*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '***我们即将进入技术细节！*** *如果你觉得接下来的内容比较难，随时可以在阅读下一部分时观看这段辅助视频，它会为你提供一个并排的视觉指导。*'
- en: Video Diffusion Model (VDM) — 2022
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视频扩散模型（VDM）— 2022
- en: VDM Uses a 3D U-Net architecture with factorized spatio-temporal convolution
    layers. Each term is explained in the picture below.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: VDM使用带有因式分解时空卷积层的3D U-Net架构。每个术语在下图中进行了说明。
- en: '![](../Images/4c1cbf25a60ae8c5d23f85a354b91817.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c1cbf25a60ae8c5d23f85a354b91817.png)'
- en: What each of the terms mean (Image by Author)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 每个术语的含义（图像来源：作者）
- en: VDM is jointly trained on both image and video data. VDM replaces the 2D UNets
    from Image Diffusion models with 3D UNet models. The video is input into the model
    as a time sequence of 2D frames. The term Factorized basically means that the
    spatial and temporal layers are decoupled and processed separately from each other.
    This makes the computations much more efficient.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: VDM是在图像和视频数据上共同训练的。VDM用3D UNet模型替代了图像扩散模型中的2D UNet。视频被作为一系列2D帧的时间序列输入到模型中。术语“因式分解”基本上意味着空间和时间层被解耦并分别处理。这使得计算更加高效。
- en: '**What is a 3D-UNet?**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**什么是3D-UNet？**'
- en: 3D U-Net is a unique computer vision neural network that first downsamples the
    video through a series of these factorized spatio-temporal convolutional layers,
    basically extracting video features at different resolutions. Then, there is an
    upsampling path that expands the low-dimensional features back to the shape of
    the original video. While upsampling, skip connections are used to reuse the generated
    features during the downsampling path.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 3D U-Net是一种独特的计算机视觉神经网络，首先通过一系列因式分解的时空卷积层对视频进行下采样，基本上是在不同分辨率下提取视频特征。然后，网络有一个上采样路径，将低维特征扩展回原始视频的形状。在上采样过程中，使用跳跃连接来重用在下采样路径中生成的特征。
- en: '![](../Images/f4ab5461e5c125ead2ea3d4fc57f1d67.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4ab5461e5c125ead2ea3d4fc57f1d67.png)'
- en: The 3D Factorized UNet Architecture [Image by Author]
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 3D因式分解UNet架构 [图像来源：作者]
- en: Remember in any convolutional neural network, the earlier layers always capture
    detailed information about local sections of the image, while latter layers pick
    up global level pattern by accessing larger sections — so by using skip connections,
    U-Net combines local details with global features to be a super-awesome network
    for feature learning and denoising.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在任何卷积神经网络中，较早的层总是捕捉图像局部区域的详细信息，而后面的层则通过访问更大的区域来捕捉全局级别的模式——因此，通过使用跳跃连接，U-Net将局部细节与全局特征结合起来，成为一个超棒的特征学习和去噪网络。
- en: VDM is jointly trained on paired image-text and video-text datasets. While it’s
    a great proof of concept, VDM generates quite low-resolution videos for today’s
    standards.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: VDM是在配对的图像-文本和视频-文本数据集上共同训练的。虽然它是一个很好的概念验证，但对于今天的标准而言，VDM生成的视频分辨率相当低。
- en: '[You can read more about VDM here.](https://arxiv.org/abs/2204.03458)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[你可以在这里阅读更多关于VDM的信息。](https://arxiv.org/abs/2204.03458)'
- en: Make-A-Video (Meta AI) — 2022
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Make-A-Video（Meta AI）— 2022
- en: Make-A-Video by Meta AI takes the bold approach of claiming that **we don’t
    necessarily need labeled-video data to train video diffusion models**. WHHAAA?!
    Yes, you read that right.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Meta AI的Make-A-Video大胆地提出了一个观点：**我们不一定需要标注过的视频数据来训练视频扩散模型**。什么？没错，你没看错。
- en: Adding temporal layers to Image Diffusion
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向图像扩散模型添加时间层
- en: Make A Video first trains a regular text-to-image diffusion model, just like
    Dall-E or Stable Diffusion with paired image-text data. Next, unsupervised learning
    is done on unlabelled video data to teach the model temporal relationships. The
    additional layers of the network are trained using a technique called masked spatio-temporal
    decoding, where the network learns to generate missing frames by processing the
    visible frames. Note that no labelled video data is needed in this pipeline (although
    further video-text fine-tuning is possible as an additional third step), because
    the model learns spatio-temporal relationships with paired text-image and raw
    unlabelled video data.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Make A Video首先训练一个常规的文本到图像扩散模型，就像Dall-E或Stable Diffusion那样，使用配对的图像-文本数据。接下来，使用无监督学习对未标注的视频数据进行训练，以教授模型时间关系。网络的附加层使用一种称为遮掩时空解码的技术进行训练，在该技术中，网络通过处理可见帧来生成缺失的帧。请注意，这个流程中不需要标注的视频数据（尽管进一步的视频-文本微调可以作为附加的第三步），因为模型通过配对的文本-图像数据和原始未标注的视频数据来学习时空关系。
- en: '![](../Images/e21bb7d54a232d244773c170f6a86265.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e21bb7d54a232d244773c170f6a86265.png)'
- en: Make-A-Video in a nutshell [Image by Author]
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Make-A-Video概述 [作者图示]
- en: The video outputted by the above model is 64x64 with 16 frames. This video is
    then upsampled along the time and pixel axis using separate neural networks called
    Temporal Super Resolution or TSR (insert new frames between existing frames to
    increase frames-per-second (fps)), and Spatial Super Resolution or SSR (super-scale
    the individual frames of the video to be higher resolution). After these steps,
    Make-A-Video outputs 256x256 videos with 76 frames.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 上述模型输出的视频是64x64，包含16帧。然后，通过使用称为时间超分辨率（TSR）和空间超分辨率（SSR）单独的神经网络，在时间和像素轴上对视频进行上采样（在现有帧之间插入新帧以增加每秒帧数（fps）），以及对视频的单帧进行超分辨率处理，提升分辨率。经过这些步骤后，Make-A-Video输出的是256x256分辨率、包含76帧的视频。
- en: '[You can learn more about Make-A-Video right here.](https://makeavideo.studio/)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[你可以在这里了解更多关于Make-A-Video的信息。](https://makeavideo.studio/)'
- en: Imagen Video (Google) — 2022
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Imagen Video（谷歌）— 2022
- en: Imagen video employs a cascade of seven models for video generation and enhancement.
    The process starts with a base video generation model that creates low-resolution
    video clips. This is followed by a series of super-resolution models — three SSR
    (Spatial Super Resolution) models for spatial upscaling and three TSR (Temporal
    Super Resolution) models for temporal upscaling. This cascaded approach allows
    Imagen Video to generate high-quality, high-resolution videos with impressive
    temporal consistency. Generates high-quality, high-resolution videos with impressive
    temporal consistency
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Imagen视频采用七个模型的级联来进行视频生成和增强。该过程从一个基础视频生成模型开始，创建低分辨率的视频片段。接着是一系列超分辨率模型——三个SSR（空间超分辨率）模型用于空间放大，三个TSR（时间超分辨率）模型用于时间放大。这种级联方法使得Imagen
    Video能够生成高质量、高分辨率的视频，并且具有令人印象深刻的时间一致性。
- en: '![](../Images/c0093e958c4063d6ceef8a65b91cf14d.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c0093e958c4063d6ceef8a65b91cf14d.png)'
- en: 'The Imagen workflow [Source: Imagen paper: [https://imagen.research.google/video/paper.pdf](https://imagen.research.google/video/paper.pdf)]'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Imagen工作流程 [来源：Imagen论文：[https://imagen.research.google/video/paper.pdf](https://imagen.research.google/video/paper.pdf)]
- en: VideoLDM (NVIDIA) — 2023
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: VideoLDM（NVIDIA）— 2023
- en: Models like Nvidia’s VideoLDM tries to address the temporal consistency issue
    by using latent diffusion modelling. First they train a latent diffusion image
    generator. The basic idea is to train a Variational Autoencoder or VAE. The VAE
    consists of an encoder network that can compress input frames into a low dimensional
    latent space and another decoder network that can reconstruct it back to the original
    images. The diffusion process is done entirely on this low dimensional space instead
    of the full pixel-space, making it much more computationally efficient and semantically
    powerful.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于Nvidia的VideoLDM这样的模型尝试通过使用潜在扩散建模来解决时间一致性问题。首先，他们训练一个潜在扩散图像生成器。基本思路是训练一个变分自编码器（VAE）。VAE由一个编码器网络组成，可以将输入帧压缩成低维潜在空间，另一个解码器网络则可以将其重建回原始图像。扩散过程完全在这个低维空间中进行，而不是在完整的像素空间中，从而使得计算效率更高，并且在语义上更强大。
- en: '![](../Images/c1abd5b3867bb017921333a2f112b69b.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c1abd5b3867bb017921333a2f112b69b.png)'
- en: A typical Autoencoder. The input frames are individually downsampled into a
    low dimensional compressed latent space. A Decoder network then learns to reconstruct
    the image back from this low resolution space. [Image by Author]
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的自编码器。输入帧被单独下采样到低维压缩潜在空间。然后，解码器网络学习从这个低分辨率空间中重建图像。[图片由作者提供]
- en: What are Latent Diffusion Models?
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是潜在扩散模型？
- en: The diffusion model is trained entirely in the low dimensional latent space,
    i.e. the diffusion model learns to denoise the low dimensional latent space images
    instead of the full resolution frames. This is why we call it **Latent Diffusion
    Models.** The resulting latent space outputs is then pass through the VAE decoder
    to convert it back to pixel-space.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型完全在低维潜在空间中进行训练，也就是说，扩散模型学习去噪的是低维潜在空间中的图像，而不是全分辨率的帧。这就是为什么我们称其为**潜在扩散模型**。最终的潜在空间输出将通过VAE解码器转换回像素空间。
- en: The decoder of the VAE is enhanced by adding new temporal layers in between
    it’s spatial layers. These temporal layers are fine-tuned on video data, making
    the VAE produce temporally consistent and flicker-free videos from the latents
    generated by the image diffusion model. This is done by freezing the spatial layers
    of the decoder and adding new trainable temporal layers that are conditioned on
    previously generated frames.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: VAE的解码器通过在其空间层之间添加新的时间层进行增强。这些时间层是在视频数据上微调的，使得VAE能够从由图像扩散模型生成的潜在向量中生成时间一致且无闪烁的视频。通过冻结解码器的空间层，并添加新的可训练时间层，这些时间层基于先前生成的帧进行条件化。
- en: '![](../Images/70f142e0ee2a11c20ea619134925aac1.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/70f142e0ee2a11c20ea619134925aac1.png)'
- en: 'The VAE Decoder is finetuned with temporal information so that it can produce
    consistent videos from the latents generated by the Latent Diffusion Model (LDM)
    [Source: Video LDM Paper [https://arxiv.org/abs/2304.08818](https://arxiv.org/abs/2304.08818)]'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 'VAE解码器通过加入时间信息进行微调，以便从由潜在扩散模型（LDM）生成的潜在向量中生成一致的视频[来源: 视频LDM论文 [https://arxiv.org/abs/2304.08818](https://arxiv.org/abs/2304.08818)]'
- en: '[You can learn more about Video LDMs here.](https://research.nvidia.com/labs/toronto-ai/VideoLDM/index.html)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[你可以在这里了解更多关于视频LDM的信息。](https://research.nvidia.com/labs/toronto-ai/VideoLDM/index.html)'
- en: '**SORA (OpenAI)** — 2024'
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**SORA（OpenAI）** — 2024'
- en: While Video LDM compresses individual frames of the video to train an LDM, SORA
    compresses video both spatially and temporally. Recent papers like [CogVideoX
    have demonstrated that 3D Causal VAEs are great at compressing videos making diffusion
    training computationally efficient](https://arxiv.org/abs/2408.06072), and able
    to generate flicker-free consistent videos.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然视频LDM将视频的单独帧压缩以训练LDM，SORA则在空间和时间上同时压缩视频。最近的论文如[CogVideoX已证明，3D因果变分自编码器（VAE）在压缩视频方面表现出色，使得扩散训练在计算上更加高效](https://arxiv.org/abs/2408.06072)，并能够生成无闪烁且一致的视频。
- en: '![](../Images/b452aef4a5df2bd864c12e1f7d45ee90.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b452aef4a5df2bd864c12e1f7d45ee90.png)'
- en: 3D VAEs compress videos spatio-temporally to generate compressed 4D representations
    of video data [Image by Author]
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 3D VAE通过空间和时间压缩视频，以生成视频数据的压缩4D表示。[图片由作者提供]
- en: Transformers for Diffusion
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于扩散的Transformers
- en: A transformer model is used as the diffusion network instead of the more traditional
    UNEt model. Of course, transformers need the input data to be presented as a sequence
    of tokens. That’s why the compressed video encodings are flattened into a sequence
    of patches. Observe that each patch and its location in the sequence represents
    a spatio-temporal feature of the original video.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散网络使用的是Transformer模型，而不是更传统的UNet模型。当然，transformer需要输入数据以序列的形式呈现。这就是为什么压缩的视频编码被展平为一个补丁序列。请注意，每个补丁及其在序列中的位置代表了原始视频的一个时空特征。
- en: '![](../Images/1d3a48f0370c3333302727a6b1917995.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d3a48f0370c3333302727a6b1917995.png)'
- en: 'OpenAI SORA Video Preprocessing [Source: OpenAI (https://openai.com/index/sora/)]
    (License: Free)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 'OpenAI SORA 视频预处理 [来源: OpenAI (https://openai.com/index/sora/)] （许可: 免费）'
- en: It is speculated that OpenAI has collected a rather large annotation dataset
    of video-text data which they are using to train conditional video generation
    models.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 有人推测OpenAI已经收集了一个相当大的视频-文本标注数据集，他们正在利用这些数据集训练条件性视频生成模型。
- en: Combining all the strengths listed below, plus more tricks that the ironically-named
    OpenAI may never disclose, SORA promises to be a giant leap in video generation
    AI models.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 结合下面列出的所有优势，再加上一些OpenAI可能永远不会透露的巧妙技巧，SORA有望成为视频生成AI模型的一大飞跃。
- en: Massive video-text annotated dataset + pretraining techniques with image-text
    data and unlabelled data
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大规模视频-文本标注数据集 + 图像-文本数据和未标注数据的预训练技术
- en: General architectures of Transformers
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变换器的通用架构
- en: Huge compute investment (thanks Microsoft)
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 巨大的计算投资（感谢微软）
- en: The representation power of Latent Diffusion Modeling.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 潜在扩散建模（Latent Diffusion Modeling）的表征能力。
- en: What’s next
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接下来会发生什么
- en: The future of AI is easy to predict. **In 2024, Data + Compute = Intelligence**.
    Large corporations will invest computing resources to train large diffusion transformers.
    They will hire annotators to label high-quality video-text data. Large-scale text-video
    datasets probably already exist in the closed-source domain (looking at you OpenAI),
    and they may become open-source within the next 2–3 years, especially with recent
    advancements in AI video understanding. It remains to be seen if the upcoming
    huge computing and financial investments could on their own solve video generation.
    Or will further architectural and algorithmic advancements be needed from the
    research community?
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能的未来是容易预测的。**在2024年，数据 + 计算 = 智能**。大型企业将投入计算资源来训练大型扩散变换器（diffusion transformers）。它们将雇佣注释员标注高质量的视频-文本数据。大规模的视频-文本数据集可能已经存在于封闭源领域（看着你，OpenAI），并且它们可能会在接下来的2-3年内成为开源，特别是随着近期AI视频理解的进展。尚待观察的是，未来巨大的计算和财务投资是否能够单独解决视频生成问题，或者是否需要研究界进一步的架构和算法进展？
- en: Links
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 链接
- en: 'Author’s Youtube Channel: [https://www.youtube.com/@avb_fj](https://www.youtube.com/@avb_fj)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '作者的YouTube频道: [https://www.youtube.com/@avb_fj](https://www.youtube.com/@avb_fj)'
- en: 'Video on this topic: [https://youtu.be/KRTEOkYftUY](https://youtu.be/KRTEOkYftUY)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '关于这个话题的视频: [https://youtu.be/KRTEOkYftUY](https://youtu.be/KRTEOkYftUY)'
- en: '15-step Zero-to-Hero on Conditional Image Diffusion: [https://youtu.be/w8YQc](https://youtu.be/w8YQcEd77)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '15步从零到英雄：条件图像扩散教程: [https://youtu.be/w8YQc](https://youtu.be/w8YQcEd77)'
- en: Papers and Articles
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 论文与文章
- en: 'Video Diffusion Models: [https://arxiv.org/abs/2204.03458](https://arxiv.org/abs/2204.03458)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '视频扩散模型: [https://arxiv.org/abs/2204.03458](https://arxiv.org/abs/2204.03458)'
- en: 'Imagen: [https://imagen.research.google/video/](https://imagen.research.google/video/)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 'Imagen: [https://imagen.research.google/video/](https://imagen.research.google/video/)'
- en: 'Make A Video: [https://makeavideo.studio/](https://makeavideo.studio/)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '制作视频: [https://makeavideo.studio/](https://makeavideo.studio/)'
- en: 'Video LDM: [https://research.nvidia.com/labs/toronto-ai/VideoLDM/index.html](https://research.nvidia.com/labs/toronto-ai/VideoLDM/index.html)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '视频LDM: [https://research.nvidia.com/labs/toronto-ai/VideoLDM/index.html](https://research.nvidia.com/labs/toronto-ai/VideoLDM/index.html)'
- en: 'CogVideoX: [https://arxiv.org/abs/2408.06072](https://arxiv.org/abs/2408.06072)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 'CogVideoX: [https://arxiv.org/abs/2408.06072](https://arxiv.org/abs/2408.06072)'
- en: 'OpenAI SORA article: [https://openai.com/index/sora/](https://openai.com/index/sora/)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 'OpenAI SORA 文章: [https://openai.com/index/sora/](https://openai.com/index/sora/)'
- en: 'Diffusion Transformers: [https://arxiv.org/abs/2212.09748](https://arxiv.org/abs/2212.09748)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '扩散变换器: [https://arxiv.org/abs/2212.09748](https://arxiv.org/abs/2212.09748)'
- en: 'Useful article: [https://lilianweng.github.io/posts/2024-04-12-diffusion-video/](https://lilianweng.github.io/posts/2024-04-12-diffusion-video/)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '有用的文章: [https://lilianweng.github.io/posts/2024-04-12-diffusion-video/](https://lilianweng.github.io/posts/2024-04-12-diffusion-video/)'
