- en: How I Dockerized Apache Flink, Kafka, and PostgreSQL for Real-Time Data Streaming
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•å°† Apache Flinkã€Kafka å’Œ PostgreSQL Docker åŒ–ï¼Œå®ç°å®æ—¶æ•°æ®æµå¤„ç†
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/how-i-dockerized-apache-flink-kafka-and-postgresql-for-real-time-data-streaming-c4ce38598336?source=collection_archive---------1-----------------------#2024-06-19](https://towardsdatascience.com/how-i-dockerized-apache-flink-kafka-and-postgresql-for-real-time-data-streaming-c4ce38598336?source=collection_archive---------1-----------------------#2024-06-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/how-i-dockerized-apache-flink-kafka-and-postgresql-for-real-time-data-streaming-c4ce38598336?source=collection_archive---------1-----------------------#2024-06-19](https://towardsdatascience.com/how-i-dockerized-apache-flink-kafka-and-postgresql-for-real-time-data-streaming-c4ce38598336?source=collection_archive---------1-----------------------#2024-06-19)
- en: Integrating pyFlink, Kafka, and PostgreSQL using Docker
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Docker é›†æˆ pyFlinkã€Kafka å’Œ PostgreSQL
- en: '[](https://adenevreze.medium.com/?source=post_page---byline--c4ce38598336--------------------------------)[![Augusto
    de NevrezÃ©](../Images/bd7d6509149ddb447dd7e5af9f09e4b1.png)](https://adenevreze.medium.com/?source=post_page---byline--c4ce38598336--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c4ce38598336--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c4ce38598336--------------------------------)
    [Augusto de NevrezÃ©](https://adenevreze.medium.com/?source=post_page---byline--c4ce38598336--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://adenevreze.medium.com/?source=post_page---byline--c4ce38598336--------------------------------)[![Augusto
    de NevrezÃ©](../Images/bd7d6509149ddb447dd7e5af9f09e4b1.png)](https://adenevreze.medium.com/?source=post_page---byline--c4ce38598336--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c4ce38598336--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c4ce38598336--------------------------------)
    [Augusto de NevrezÃ©](https://adenevreze.medium.com/?source=post_page---byline--c4ce38598336--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c4ce38598336--------------------------------)
    Â·10 min readÂ·Jun 19, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c4ce38598336--------------------------------)
    Â·é˜…è¯»æ—¶é—´ï¼š10åˆ†é’ŸÂ·2024å¹´6æœˆ19æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/11d218990f84e7153933c512352bd704.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11d218990f84e7153933c512352bd704.png)'
- en: Get your pyFlink applications ready using docker â€” author generated image using
    [https://www.dall-efree.com/](https://www.dall-efree.com/)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Docker å‡†å¤‡ä½ çš„ pyFlink åº”ç”¨ç¨‹åº â€” ä½œè€…ä½¿ç”¨ [https://www.dall-efree.com/](https://www.dall-efree.com/)
    ç”Ÿæˆçš„å›¾ç‰‡
- en: Why Read This?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆè¦é˜…è¯»è¿™ç¯‡æ–‡ç« ï¼Ÿ
- en: '**Real-World Insights**: Get practical tips from my personal journey of overcoming
    integration hurdles.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**çœŸå®ä¸–ç•Œçš„æ´å¯Ÿ**ï¼šä»æˆ‘ä¸ªäººå…‹æœé›†æˆéš¾é¢˜çš„ç»å†ä¸­è·å–å®ç”¨çš„å»ºè®®ã€‚'
- en: '**Complete Setup**: Learn how to integrate Flink, Kafka, and PostgreSQL seamlessly
    using Docker-Compose.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å®Œæ•´è®¾ç½®**ï¼šå­¦ä¹ å¦‚ä½•ä½¿ç”¨ Docker-Compose æ— ç¼é›†æˆ Flinkã€Kafka å’Œ PostgreSQLã€‚'
- en: '**Step-by-Step Guide**: Perfect for both beginners and experienced developers
    looking to streamline their data streaming stack.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é€æ­¥æŒ‡å—**ï¼šéå¸¸é€‚åˆåˆå­¦è€…å’Œæœ‰ç»éªŒçš„å¼€å‘è€…ï¼Œå¸®åŠ©ä½ ç®€åŒ–æ•°æ®æµå¤„ç†æ¶æ„ã€‚'
- en: Setting Up the Scene
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®¾ç½®åœºæ™¯
- en: I embarked on a mission to integrate Apache Flink with Kafka and PostgreSQL
    using Docker. What makes this endeavor particularly exciting is the use of pyFlink
    â€” the Python flavor of Flink â€” which is both powerful and relatively rare. This
    setup aims to handle real-time data processing and storage efficiently. In the
    following sections, Iâ€™ll demonstrate how I achieved this, discussing the challenges
    encountered and how I overcame them. Iâ€™ll conclude with a step-by-step guide so
    you can build and experiment with this streaming pipeline yourself.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 'æˆ‘å¼€å§‹äº†ä¸€ä¸ªä»»åŠ¡ï¼Œæ—¨åœ¨ä½¿ç”¨ Docker é›†æˆ Apache Flinkã€Kafka å’Œ PostgreSQLã€‚è¿™é¡¹å·¥ä½œå°¤å…¶ä»¤äººå…´å¥‹ï¼Œå› ä¸ºæˆ‘ä½¿ç”¨äº† pyFlink
    â€”â€” Flink çš„ Python ç‰ˆæœ¬ â€”â€” å®ƒæ—¢å¼ºå¤§åˆç›¸å¯¹ç½•è§ã€‚è¿™ä¸ªè®¾ç½®çš„ç›®æ ‡æ˜¯é«˜æ•ˆåœ°å¤„ç†å’Œå­˜å‚¨å®æ—¶æ•°æ®ã€‚åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ï¼Œæˆ‘å°†å±•ç¤ºæˆ‘æ˜¯å¦‚ä½•å®ç°è¿™ä¸€ç›®æ ‡çš„ï¼Œè®¨è®ºæˆ‘é‡åˆ°çš„æŒ‘æˆ˜ä»¥åŠå¦‚ä½•å…‹æœå®ƒä»¬ã€‚æœ€åï¼Œæˆ‘å°†æä¾›ä¸€ä¸ªé€æ­¥æŒ‡å—ï¼Œå¸®åŠ©ä½ è‡ªå·±æ­å»ºå¹¶å®éªŒè¿™ä¸ªæ•°æ®æµç®¡é“ã€‚  '
- en: The infrastructure weâ€™ll build is illustrated below. Externally, thereâ€™s a publisher
    module that simulates IoT sensor messages, similar to what was discussed in a
    [previous post](https://medium.com/dev-genius/detecting-iot-alerts-with-apache-flink-7a2be19ad9dd).
    Inside the Docker container, we will create two Kafka topics. The first topic,
    *sensors*, will store incoming messages from IoT devices in real-time. A Flink
    application will then consume messages from this topic, filter those with temperatures
    above 30Â°C, and publish them to a second topic, *alerts*. Additionally, the Flink
    application will insert the consumed messages into a PostgreSQL table created
    specifically for this purpose. This setup allows us to persist sensor data in
    a structured, tabular format, providing opportunities for further transformation
    and analysis. Visualization tools like Tableau or Power BI can be connected to
    this data for real-time plotting and dashboards.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è¦æ„å»ºçš„åŸºç¡€è®¾æ–½å¦‚ä¸‹æ‰€ç¤ºã€‚ä»å¤–éƒ¨æ¥çœ‹ï¼Œæœ‰ä¸€ä¸ªå‘å¸ƒè€…æ¨¡å—ï¼Œç”¨äºæ¨¡æ‹Ÿç‰©è”ç½‘ä¼ æ„Ÿå™¨æ¶ˆæ¯ï¼Œç±»ä¼¼äº[ä¹‹å‰çš„æ–‡ç« ](https://medium.com/dev-genius/detecting-iot-alerts-with-apache-flink-7a2be19ad9dd)ä¸­è®¨è®ºçš„å†…å®¹ã€‚åœ¨
    Docker å®¹å™¨å†…éƒ¨ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸¤ä¸ª Kafka ä¸»é¢˜ã€‚ç¬¬ä¸€ä¸ªä¸»é¢˜ *sensors* ç”¨äºå®æ—¶å­˜å‚¨æ¥è‡ªç‰©è”ç½‘è®¾å¤‡çš„æ¶ˆæ¯ã€‚Flink åº”ç”¨ç¨‹åºå°†ä»è¯¥ä¸»é¢˜æ¶ˆè´¹æ¶ˆæ¯ï¼Œè¿‡æ»¤å‡ºæ¸©åº¦é«˜äº30Â°Cçš„æ¶ˆæ¯ï¼Œå¹¶å°†å…¶å‘å¸ƒåˆ°ç¬¬äºŒä¸ªä¸»é¢˜
    *alerts*ã€‚æ­¤å¤–ï¼ŒFlink åº”ç”¨ç¨‹åºè¿˜å°†æŠŠæ¶ˆè´¹çš„æ¶ˆæ¯æ’å…¥åˆ°ä¸“é—¨ä¸ºæ­¤ç›®çš„åˆ›å»ºçš„ PostgreSQL è¡¨ä¸­ã€‚è¿™ä¸ªè®¾ç½®ä½¿æˆ‘ä»¬èƒ½å¤Ÿä»¥ç»“æ„åŒ–çš„è¡¨æ ¼æ ¼å¼æŒä¹…åŒ–ä¼ æ„Ÿå™¨æ•°æ®ï¼Œæä¾›è¿›ä¸€æ­¥è½¬åŒ–å’Œåˆ†æçš„æœºä¼šã€‚åƒ
    Tableau æˆ– Power BI è¿™æ ·çš„å¯è§†åŒ–å·¥å…·å¯ä»¥è¿æ¥åˆ°è¿™äº›æ•°æ®ï¼Œç”¨äºå®æ—¶ç»˜å›¾å’Œä»ªè¡¨ç›˜å±•ç¤ºã€‚
- en: Moreover, the alerts topic can be consumed by other clients to initiate actions
    based on the messages it holds, such as activating air conditioning systems or
    triggering fire safety protocols.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œalerts ä¸»é¢˜å¯ä»¥è¢«å…¶ä»–å®¢æˆ·ç«¯æ¶ˆè´¹ï¼Œä»¥æ ¹æ®å…¶åŒ…å«çš„æ¶ˆæ¯å¯åŠ¨ç›¸åº”çš„åŠ¨ä½œï¼Œä¾‹å¦‚å¯ç”¨ç©ºè°ƒç³»ç»Ÿæˆ–è§¦å‘æ¶ˆé˜²å®‰å…¨åè®®ã€‚
- en: '![](../Images/a1ad055bc5105bf1c170c1d526b9702c.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1ad055bc5105bf1c170c1d526b9702c.png)'
- en: Services included in the docker container â€” image by author
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: docker å®¹å™¨ä¸­åŒ…å«çš„æœåŠ¡ â€”â€” å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: In order to follow up the tutorial, you can clone the following [repo](https://github.com/augustodn/pyflink-docker).
    A docker-compose.yml is placed in the root of the project so you can initialize
    the multi-container application. Furthermore, you can find detailed instructions
    in the README file.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è·Ÿéšæœ¬æ•™ç¨‹ï¼Œä½ å¯ä»¥å…‹éš†ä»¥ä¸‹[ä»“åº“](https://github.com/augustodn/pyflink-docker)ã€‚é¡¹ç›®æ ¹ç›®å½•ä¸­åŒ…å«ä¸€ä¸ª
    docker-compose.yml æ–‡ä»¶ï¼Œä¾¿äºä½ åˆå§‹åŒ–å¤šå®¹å™¨åº”ç”¨ç¨‹åºã€‚æ­¤å¤–ï¼Œä½ å¯ä»¥åœ¨ README æ–‡ä»¶ä¸­æ‰¾åˆ°è¯¦ç»†çš„è¯´æ˜ã€‚
- en: Issues With Kafka Ports in docker-compose.yml
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: docker-compose.yml ä¸­ Kafka ç«¯å£çš„é—®é¢˜
- en: Initially, I encountered problems with Kafkaâ€™s port configuration when using
    the confluentinc Kafka Docker image, a popular choice for such setups. This issue
    became apparent through the logs, emphasizing the importance of not running docker-compose
    up in detached mode (-d) during initial setup and troubleshooting phases.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åˆï¼Œåœ¨ä½¿ç”¨ confluentinc Kafka Docker é•œåƒæ—¶ï¼Œæˆ‘é‡åˆ°äº† Kafka ç«¯å£é…ç½®çš„é—®é¢˜ï¼Œè¿™æ˜¯è¿™ç§è®¾ç½®çš„å¸¸è§é€‰æ‹©ã€‚é€šè¿‡æ—¥å¿—å¯ä»¥æ˜æ˜¾çœ‹åˆ°è¿™ä¸ªé—®é¢˜ï¼Œçªæ˜¾å‡ºåœ¨åˆå§‹è®¾ç½®å’Œæ•…éšœæ’é™¤é˜¶æ®µï¼Œä¸åº”è¯¥ä»¥åˆ†ç¦»æ¨¡å¼ï¼ˆ-dï¼‰è¿è¡Œ
    docker-compose up çš„é‡è¦æ€§ã€‚
- en: The reason for the failure was that the internal and external hosts were using
    the same port, which led to connectivity problems. I fixed this by changing the
    internal port to 19092\. Iâ€™ve found [this](https://www.confluent.io/blog/kafka-client-cannot-connect-to-broker-on-aws-on-docker-etc/)
    blog post pretty clarifying.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å¤±è´¥çš„åŸå› æ˜¯å†…éƒ¨å’Œå¤–éƒ¨ä¸»æœºä½¿ç”¨äº†ç›¸åŒçš„ç«¯å£ï¼Œå¯¼è‡´äº†è¿æ¥é—®é¢˜ã€‚æˆ‘é€šè¿‡å°†å†…éƒ¨ç«¯å£æ›´æ”¹ä¸º19092æ¥è§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚æˆ‘å‘ç°[è¿™ç¯‡](https://www.confluent.io/blog/kafka-client-cannot-connect-to-broker-on-aws-on-docker-etc/)åšå®¢æ–‡ç« å¯¹é—®é¢˜çš„è§£é‡Šç›¸å½“æ¸…æ™°ã€‚
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Configuring Flink in Session Mode
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é…ç½® Flink ä¼šè¯æ¨¡å¼
- en: To run Flink in [session mode](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/overview/#session-mode)
    (allowing multiple jobs in a single cluster), Iâ€™m using the following directives
    in the docker-compose.yml.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨[ä¼šè¯æ¨¡å¼](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/overview/#session-mode)ä¸‹è¿è¡Œ
    Flinkï¼ˆå…è®¸åœ¨ä¸€ä¸ªé›†ç¾¤ä¸­è¿è¡Œå¤šä¸ªä½œä¸šï¼‰ï¼Œæˆ‘åœ¨ docker-compose.yml æ–‡ä»¶ä¸­ä½¿ç”¨äº†ä»¥ä¸‹æŒ‡ä»¤ã€‚
- en: Custom Docker Image for PyFlink
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyFlink çš„è‡ªå®šä¹‰ Docker é•œåƒ
- en: Given the limitations of the default Apache Flink Docker image, which doesnâ€™t
    include Python support, I created a [custom Docker image](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/standalone/docker/#using-flink-python-on-docker)
    for pyFlink. This custom image ensures that Flink can run Python jobs and includes
    the necessary dependencies for integration with Kafka and PostgreSQL. The Dockerfile
    used for this is located in the pyflink subdirectory.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: é‰´äºé»˜è®¤çš„ Apache Flink Docker é•œåƒä¸åŒ…æ‹¬ Python æ”¯æŒï¼Œæˆ‘ä¸º pyFlink åˆ›å»ºäº†ä¸€ä¸ª[è‡ªå®šä¹‰ Docker é•œåƒ](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/standalone/docker/#using-flink-python-on-docker)ã€‚è¿™ä¸ªè‡ªå®šä¹‰é•œåƒç¡®ä¿
    Flink å¯ä»¥è¿è¡Œ Python ä½œä¸šï¼Œå¹¶åŒ…å«ä¸ Kafka å’Œ PostgreSQL é›†æˆæ‰€éœ€çš„ä¾èµ–é¡¹ã€‚ç”¨äºåˆ›å»ºæ­¤é•œåƒçš„ Dockerfile ä½äº pyflink
    å­ç›®å½•ä¸­ã€‚
- en: '**Base Image**: We start with the official Flink image.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åŸºç¡€é•œåƒ**ï¼šæˆ‘ä»¬ä»å®˜æ–¹ Flink é•œåƒå¼€å§‹ã€‚'
- en: '**Python Installation**: Python and pip are installed, upgrading pip to the
    latest version.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Python å®‰è£…**ï¼šå®‰è£…äº† Python å’Œ pipï¼Œå¹¶å°† pip å‡çº§åˆ°æœ€æ–°ç‰ˆæœ¬ã€‚'
- en: '**Dependency Management**: Dependencies are installed via requirements.txt.
    Alternatively, lines are commented to demonstrate how to manually install dependencies
    from local files, useful for deployment in environments without internet access.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä¾èµ–ç®¡ç†**ï¼šé€šè¿‡ requirements.txt å®‰è£…ä¾èµ–é¡¹ã€‚æˆ–è€…ï¼Œå¯ä»¥å°†æŸäº›è¡Œæ³¨é‡Šæ‰ï¼Œæ¼”ç¤ºå¦‚ä½•ä»æœ¬åœ°æ–‡ä»¶æ‰‹åŠ¨å®‰è£…ä¾èµ–é¡¹ï¼Œè¿™å¯¹äºåœ¨æ²¡æœ‰äº’è”ç½‘è®¿é—®çš„ç¯å¢ƒä¸­éƒ¨ç½²å¾ˆæœ‰ç”¨ã€‚'
- en: '**Connector Libraries**: Connectors for Kafka and PostgreSQL are downloaded
    directly into the Flink lib directory. This enables Flink to interact with Kafka
    and PostgreSQL during job execution.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è¿æ¥å™¨åº“**ï¼šKafka å’Œ PostgreSQL çš„è¿æ¥å™¨ä¼šç›´æ¥ä¸‹è½½åˆ° Flink çš„ lib ç›®å½•ä¸­ã€‚è¿™ä½¿å¾— Flink åœ¨ä½œä¸šæ‰§è¡Œè¿‡ç¨‹ä¸­èƒ½å¤Ÿä¸
    Kafka å’Œ PostgreSQL è¿›è¡Œäº¤äº’ã€‚'
- en: '**Script Copying**: Scripts from the repository are copied into the /opt/flink
    directory to be executed by the Flink task manager.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è„šæœ¬å¤åˆ¶**ï¼šä»ä»£ç åº“ä¸­çš„è„šæœ¬ä¼šå¤åˆ¶åˆ° /opt/flink ç›®å½•ï¼Œç”± Flink ä»»åŠ¡ç®¡ç†å™¨æ‰§è¡Œã€‚'
- en: With this custom Docker image, we ensure pyFlink can run properly within the
    Docker container, equipped with the necessary libraries to interact with Kafka
    and PostgreSQL seamlessly. This approach provides flexibility and is suitable
    for both development and production environments.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¿™ä¸ªè‡ªå®šä¹‰ Docker é•œåƒï¼Œæˆ‘ä»¬ç¡®ä¿ pyFlink å¯ä»¥åœ¨ Docker å®¹å™¨ä¸­æ­£å¸¸è¿è¡Œï¼Œå¹¶é…å¤‡äº†ä¸ Kafka å’Œ PostgreSQL æ— ç¼äº¤äº’æ‰€éœ€çš„åº“ã€‚è¿™ç§æ–¹æ³•æä¾›äº†çµæ´»æ€§ï¼Œé€‚ç”¨äºå¼€å‘å’Œç”Ÿäº§ç¯å¢ƒã€‚
- en: '**Note:** Ensure that any network or security considerations for downloading
    connectors and other dependencies are addressed according to your deployment environmentâ€™s
    policies.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** è¯·ç¡®ä¿æ ¹æ®æ‚¨çš„éƒ¨ç½²ç¯å¢ƒçš„æ”¿ç­–ï¼Œå¤„ç†ä¸‹è½½è¿æ¥å™¨å’Œå…¶ä»–ä¾èµ–é¡¹æ—¶çš„ç½‘ç»œæˆ–å®‰å…¨æ€§é—®é¢˜ã€‚'
- en: Integrating PostgreSQL
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é›†æˆ PostgreSQL
- en: To connect Apache Flink to the PostgreSQL database, a proper JDBC connector
    is required. The custom Docker image for pyFlink downloads the JDBC connector
    for PostgreSQL, which is compatible with PostgreSQL 16.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å°† Apache Flink è¿æ¥åˆ° PostgreSQL æ•°æ®åº“ï¼Œå¿…é¡»ä½¿ç”¨é€‚å½“çš„ JDBC è¿æ¥å™¨ã€‚pyFlink çš„è‡ªå®šä¹‰ Docker é•œåƒä¼šä¸‹è½½ä¸
    PostgreSQL 16 å…¼å®¹çš„ PostgreSQL JDBC è¿æ¥å™¨ã€‚
- en: To simplify this process, a download_libs.sh script is included in the repository,
    mirroring the actions performed in the Flink Docker container. This script automates
    the download of the necessary libraries, ensuring consistency between the Docker
    and local environments.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç®€åŒ–è¿™ä¸€è¿‡ç¨‹ï¼Œä»£ç åº“ä¸­åŒ…å«ä¸€ä¸ª download_libs.sh è„šæœ¬ï¼Œæ¨¡æ‹Ÿäº†åœ¨ Flink Docker å®¹å™¨ä¸­æ‰§è¡Œçš„æ“ä½œã€‚è¯¥è„šæœ¬è‡ªåŠ¨ä¸‹è½½æ‰€éœ€çš„åº“ï¼Œç¡®ä¿
    Docker å’Œæœ¬åœ°ç¯å¢ƒä¹‹é—´çš„ä¸€è‡´æ€§ã€‚
- en: '**Note:** Connectors usually have two versions. In this particular case, since
    Iâ€™m using Flink 1.18, the latest stable version available, Iâ€™ve downloaded 3.1.2â€“1.18\.
    My guess is that the first version tracks JDBC implementation for several databases.
    Theyâ€™re available in the [maven directory](https://mvnrepository.com/artifact/org.apache.flink/flink-connector-jdbc/3.1.2-1.18).'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** è¿æ¥å™¨é€šå¸¸æœ‰ä¸¤ä¸ªç‰ˆæœ¬ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç”±äºæˆ‘ä½¿ç”¨çš„æ˜¯æœ€æ–°çš„ç¨³å®šç‰ˆæœ¬ Flink 1.18ï¼Œæˆ‘ä¸‹è½½äº† 3.1.2â€“1.18 ç‰ˆæœ¬ã€‚æˆ‘çš„çŒœæµ‹æ˜¯ï¼Œç¬¬ä¸€ä¸ªç‰ˆæœ¬è·Ÿè¸ªäº†å¤šä¸ªæ•°æ®åº“çš„
    JDBC å®ç°ã€‚å®ƒä»¬å¯ä»¥åœ¨ [maven ç›®å½•](https://mvnrepository.com/artifact/org.apache.flink/flink-connector-jdbc/3.1.2-1.18)ä¸­æ‰¾åˆ°ã€‚'
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Defining JDBC Sink**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**å®šä¹‰ JDBC Sink**'
- en: In our Flink task, thereâ€™s a crucial function named configure_postgre_sink located
    in the usr_jobs/postgres_sink.py file. This function is responsible for configuring
    a generic PostgreSQL sink. To use it effectively, you need to provide the SQL
    Data Manipulation Language (DML) statement and the corresponding value types.
    The types used in the streaming data are defined as TYPE_INFO â€¦ it took me a while
    to come up with the correct declaration ğŸ˜….
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„ Flink ä»»åŠ¡ä¸­ï¼Œæœ‰ä¸€ä¸ªè‡³å…³é‡è¦çš„å‡½æ•°ï¼Œåä¸º `configure_postgre_sink`ï¼Œä½äº `usr_jobs/postgres_sink.py`
    æ–‡ä»¶ä¸­ã€‚è¿™ä¸ªå‡½æ•°è´Ÿè´£é…ç½®ä¸€ä¸ªé€šç”¨çš„ PostgreSQL æ¥æ”¶å™¨ã€‚ä¸ºäº†æœ‰æ•ˆä½¿ç”¨å®ƒï¼Œä½ éœ€è¦æä¾› SQL æ•°æ®æ“ä½œè¯­è¨€ï¼ˆDMLï¼‰è¯­å¥å’Œç›¸åº”çš„å€¼ç±»å‹ã€‚ç”¨äºæµæ•°æ®çš„ç±»å‹å®šä¹‰ä¸º
    `TYPE_INFO`... æˆ‘èŠ±äº†äº›æ—¶é—´æ‰æ‰¾å‡ºæ­£ç¡®çš„å£°æ˜æ–¹å¼ ğŸ˜…ã€‚
- en: Notice also that the JdbcSink has an optional parameter to define the ExecutionOptions.
    For this particular case, Iâ€™ll use an update interval of 1 second and limit the
    amount of rows to 200\. You can find more information in the [official documentation](https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/jdbc/#jdbc-execution-options).
    Yes, you guessed it, since Iâ€™m defining an interval, this can be considered a
    micro-batch ETL. However, due to Flink parallelism you can handle multiple streams
    at once in a simple script which is at the same time, easy to follow.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒJdbcSink æœ‰ä¸€ä¸ªå¯é€‰å‚æ•°ï¼Œç”¨äºå®šä¹‰ `ExecutionOptions`ã€‚åœ¨è¿™ä¸ªç‰¹å®šçš„ä¾‹å­ä¸­ï¼Œæˆ‘å°†ä½¿ç”¨ 1 ç§’çš„æ›´æ–°é—´éš”ï¼Œå¹¶å°†è¡Œæ•°é™åˆ¶ä¸º
    200ã€‚ä½ å¯ä»¥åœ¨[å®˜æ–¹æ–‡æ¡£](https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/jdbc/#jdbc-execution-options)ä¸­æ‰¾åˆ°æ›´å¤šä¿¡æ¯ã€‚æ˜¯çš„ï¼Œä½ çŒœå¯¹äº†ï¼Œç”±äºæˆ‘å®šä¹‰äº†ä¸€ä¸ªé—´éš”ï¼Œè¿™å¯ä»¥è§†ä¸ºä¸€ä¸ªå¾®æ‰¹æ¬¡
    ETLã€‚ç„¶è€Œï¼Œç”±äº Flink çš„å¹¶è¡Œæ€§ï¼Œä½ å¯ä»¥åœ¨ä¸€ä¸ªç®€å•çš„è„šæœ¬ä¸­åŒæ—¶å¤„ç†å¤šä¸ªæ•°æ®æµï¼Œè€Œä¸”å®ƒéå¸¸æ˜“äºç†è§£ã€‚
- en: '**Note:** Donâ€™t forget to create the raw_sensors_data table in Postgres, where
    raw data coming from the IoT sensors will be received. This is covered in the
    step-by-step guide in the sections below.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** åˆ«å¿˜äº†åœ¨ Postgres ä¸­åˆ›å»º `raw_sensors_data` è¡¨ï¼Œè¿™æ˜¯æ¥æ”¶æ¥è‡ª IoT ä¼ æ„Ÿå™¨çš„åŸå§‹æ•°æ®çš„åœ°æ–¹ã€‚è¿™ä¸ªæ­¥éª¤åœ¨ä¸‹é¢çš„é€æ­¥æŒ‡å—ä¸­å·²ç»æ¶µç›–ã€‚'
- en: Sinking Data to Kafka
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å°†æ•°æ®ä¼ è¾“åˆ° Kafka
- en: Iâ€™ve covered how to consume data from a Kafka topic in [a previous discussion](https://medium.com/dev-genius/detecting-iot-alerts-with-apache-flink-7a2be19ad9dd).
    However, I havenâ€™t configured a sink yet and thatâ€™s what weâ€™ll do. The configuration
    has some intricacies and itâ€™s defined in a function, similarly to the Postgres
    sink. Additionally, you have to define the type for the data stream before sinking
    it to Kafka. Notice that the alarms_data stream is properly casted as a string
    with output_type=Types.STRING() before sinking it to Kafka, since Iâ€™ve declared
    the serializer as SimpleStringSchema().
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨[ä¹‹å‰çš„è®¨è®º](https://medium.com/dev-genius/detecting-iot-alerts-with-apache-flink-7a2be19ad9dd)ä¸­ä»‹ç»äº†å¦‚ä½•ä»
    Kafka ä¸»é¢˜æ¶ˆè´¹æ•°æ®ã€‚ç„¶è€Œï¼Œæˆ‘è¿˜æ²¡æœ‰é…ç½®æ¥æ”¶å™¨ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°±ä¼šé…ç½®ã€‚è¿™ä¸ªé…ç½®æœ‰ä¸€äº›å¤æ‚ä¹‹å¤„ï¼Œå®ƒæ˜¯å®šä¹‰åœ¨ä¸€ä¸ªå‡½æ•°ä¸­çš„ï¼Œç±»ä¼¼äº Postgres æ¥æ”¶å™¨ã€‚æ­¤å¤–ï¼Œåœ¨å°†æ•°æ®æµä¼ è¾“åˆ°
    Kafka ä¹‹å‰ï¼Œä½ å¿…é¡»å®šä¹‰æ•°æ®æµçš„ç±»å‹ã€‚è¯·æ³¨æ„ï¼Œ`alarms_data` æµåœ¨ä¼ è¾“åˆ° Kafka ä¹‹å‰å·²ç»æ­£ç¡®åœ°è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œ`output_type=Types.STRING()`ï¼Œå› ä¸ºæˆ‘å·²ç»å°†åºåˆ—åŒ–å™¨å£°æ˜ä¸º
    `SimpleStringSchema()`ã€‚
- en: Iâ€™ll show you how to fetch data from the alerts topic in the following steps.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥çš„æ­¥éª¤æˆ‘å°†å±•ç¤ºå¦‚ä½•ä» `alerts` ä¸»é¢˜è·å–æ•°æ®ã€‚
- en: Local or Containerized configuration
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æœ¬åœ°æˆ–å®¹å™¨åŒ–é…ç½®
- en: One of the greatest things about this docker configuration is that you can run
    Flink from local or inside the container as a managed task. The local Flink setup
    is depicted in the following figure, where you can see our Flink application detached
    from the docker container. This may help to troubleshoot Flink, which doesnâ€™t
    have a good suite of native observability tools. Actually, we would like to give
    a try to [datorios](https://datorios.com/) tools for Flink, they are very promising
    for monitoring purposes.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ª Docker é…ç½®çš„ä¸€ä¸ªæœ€å¤§ä¼˜ç‚¹æ˜¯ï¼Œä½ å¯ä»¥åœ¨æœ¬åœ°æˆ–å®¹å™¨å†…éƒ¨ä»¥æ‰˜ç®¡ä»»åŠ¡çš„æ–¹å¼è¿è¡Œ Flinkã€‚ä¸‹å›¾å±•ç¤ºäº†æœ¬åœ° Flink è®¾ç½®ï¼Œä½ å¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„ Flink
    åº”ç”¨ç¨‹åºä¸ Docker å®¹å™¨æ˜¯åˆ†ç¦»çš„ã€‚è¿™æœ‰åŠ©äºæ’æŸ¥ Flink çš„é—®é¢˜ï¼Œå› ä¸º Flink æœ¬èº«å¹¶æ²¡æœ‰å¾ˆå¥½çš„æœ¬åœ°å¯è§‚å¯Ÿå·¥å…·ã€‚å®é™…ä¸Šï¼Œæˆ‘ä»¬æƒ³å°è¯•ä¸€ä¸‹[datorios](https://datorios.com/)æä¾›çš„
    Flink å·¥å…·ï¼Œå®ƒä»¬åœ¨ç›‘æ§æ–¹é¢éå¸¸æœ‰å‰æ™¯ã€‚
- en: '![](../Images/6d8993906980fe9862badc9ecdcad799.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d8993906980fe9862badc9ecdcad799.png)'
- en: Runing Flink applications in local with other services running inside the container
    â€” image by author
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬åœ°è¿è¡Œ Flink åº”ç”¨ç¨‹åºï¼Œå¹¶ä¸”å®¹å™¨å†…éƒ¨è¿˜è¿è¡Œç€å…¶ä»–æœåŠ¡ â€” å›¾åƒæ¥è‡ªä½œè€…
- en: 'If you want to try the Flink application locally, you have to correctly define
    the hosts and ports used by the script which actually are two constants in the
    usr_jobs/postgres_sink.py file:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³åœ¨æœ¬åœ°å°è¯• Flink åº”ç”¨ç¨‹åºï¼Œä½ éœ€è¦æ­£ç¡®åœ°å®šä¹‰è„šæœ¬æ‰€ä½¿ç”¨çš„ä¸»æœºå’Œç«¯å£ï¼Œè¿™å®é™…ä¸Šæ˜¯ `usr_jobs/postgres_sink.py` æ–‡ä»¶ä¸­çš„ä¸¤ä¸ªå¸¸é‡ï¼š
- en: 'For container run, use:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å®¹å™¨è¿è¡Œæ—¶ï¼Œä½¿ç”¨ï¼š
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'For local run, use:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬åœ°è¿è¡Œæ—¶ï¼Œä½¿ç”¨ï¼š
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: By default the repo sets up the Flink application to run inside the container.
    You can monitor the jobs running using the web UI, accessing from [http://localhost:8081](http://localhost:8081).
    You wonâ€™t be able to see it if you choose to run the job locally.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤æƒ…å†µä¸‹ï¼Œä»“åº“è®¾ç½®Flinkåº”ç”¨ç¨‹åºåœ¨å®¹å™¨å†…è¿è¡Œã€‚ä½ å¯ä»¥é€šè¿‡Web UIç›‘æ§æ­£åœ¨è¿è¡Œçš„ä½œä¸šï¼Œè®¿é—®[http://localhost:8081](http://localhost:8081)ã€‚å¦‚æœä½ é€‰æ‹©åœ¨æœ¬åœ°è¿è¡Œä½œä¸šï¼Œåˆ™æ— æ³•æŸ¥çœ‹ã€‚
- en: '![](../Images/add4733c451e9d50906584d638693caf.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/add4733c451e9d50906584d638693caf.png)'
- en: Screenshot of the Flink web UI with the running job â€” image by author
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¾ç¤ºFlink Web UIä¸­è¿è¡Œä½œä¸šçš„æˆªå›¾â€”â€”ä½œè€…æä¾›çš„å›¾ç‰‡
- en: '**Note**: If you run the job locally, you need to install the Flink dependencies
    located in the requirements.txt. Also a pyproject.toml file is provided if you
    like to set up the environment with poetry.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„**ï¼šå¦‚æœåœ¨æœ¬åœ°è¿è¡Œä½œä¸šï¼Œåˆ™éœ€è¦å®‰è£…ä½äºrequirements.txtä¸­çš„Flinkä¾èµ–é¡¹ã€‚å¦‚æœä½ æƒ³ä½¿ç”¨poetryè®¾ç½®ç¯å¢ƒï¼Œè¿˜æä¾›äº†ä¸€ä¸ªpyproject.tomlæ–‡ä»¶ã€‚'
- en: Step-by-Step Guide to Run the Streaming Pipeline
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆ†æ­¥æŒ‡å—ï¼šè¿è¡Œæµå¤„ç†ç®¡é“
- en: 'Step 1: Launch the multi-container application'
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬1æ­¥ï¼šå¯åŠ¨å¤šå®¹å™¨åº”ç”¨ç¨‹åº
- en: Launch the containers by running docker-compose. I preferred to do it without
    detached mode to see the logs while the containers are spinning up and then running.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è¿è¡Œdocker-composeå¯åŠ¨å®¹å™¨ã€‚æˆ‘æ›´å€¾å‘äºä¸ä½¿ç”¨åˆ†ç¦»æ¨¡å¼ï¼Œä»¥ä¾¿åœ¨å®¹å™¨å¯åŠ¨å¹¶è¿è¡Œæ—¶æŸ¥çœ‹æ—¥å¿—ã€‚
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Check for the logs to see if the services are running properly.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€æŸ¥æ—¥å¿—ä»¥æŸ¥çœ‹æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œã€‚
- en: 'Step 2: Create the Kafka topics'
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬2æ­¥ï¼šåˆ›å»ºKafkaä¸»é¢˜
- en: Next, weâ€™re going to create the topics to receive data from the IoT sensors
    and store the alerts filtered by the Flink application.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸»é¢˜ä»¥æ¥æ”¶æ¥è‡ªIoTä¼ æ„Ÿå™¨çš„æ•°æ®ï¼Œå¹¶å­˜å‚¨Flinkåº”ç”¨ç¨‹åºè¿‡æ»¤åçš„è­¦æŠ¥ã€‚
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: To check if the topics were created correctly you can execute the following
    command
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æ£€æŸ¥ä¸»é¢˜æ˜¯å¦æ­£ç¡®åˆ›å»ºï¼Œå¯ä»¥æ‰§è¡Œä»¥ä¸‹å‘½ä»¤
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Step 3: Create Postgres table'
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬3æ­¥ï¼šåˆ›å»ºPostgresè¡¨
- en: Login to the postgres console
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ç™»å½•åˆ°Postgresæ§åˆ¶å°
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Enter the password flinkpassword to log into the postgres console, remember
    this is a local configuration so default access has been configured in the docker-compose.yml.
    Then create the table
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥å¯†ç flinkpasswordç™»å½•åˆ°Postgresæ§åˆ¶å°ï¼Œè¯·è®°ä½è¿™æ˜¯æœ¬åœ°é…ç½®ï¼Œå› æ­¤é»˜è®¤è®¿é—®æƒé™å·²åœ¨docker-compose.ymlä¸­é…ç½®ã€‚ç„¶ååˆ›å»ºè¡¨æ ¼
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: You can check if the table is properly created by doing the following
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼æ£€æŸ¥è¡¨æ˜¯å¦æ­£ç¡®åˆ›å»º
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This will show you a result similar to the following one:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†æ˜¾ç¤ºç±»ä¼¼ä»¥ä¸‹çš„ç»“æœï¼š
- en: '![](../Images/d60419e34e6e3d6595fb7dabb57d16ff.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d60419e34e6e3d6595fb7dabb57d16ff.png)'
- en: 'Step 4: Launching the Kafka producer'
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬4æ­¥ï¼šå¯åŠ¨Kafkaç”Ÿäº§è€…
- en: 'Create a local environment with conda or poetry and install python kafka package:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨condaæˆ–poetryåˆ›å»ºä¸€ä¸ªæœ¬åœ°ç¯å¢ƒå¹¶å®‰è£…python kafkaåŒ…ï¼š
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Then execute the kafka producer, which mimics IoT sensor messages and publishes
    messages to the sensors topic.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæ‰§è¡ŒKafkaç”Ÿäº§è€…ï¼Œå®ƒæ¨¡æ‹ŸIoTä¼ æ„Ÿå™¨æ¶ˆæ¯å¹¶å°†æ¶ˆæ¯å‘å¸ƒåˆ°ä¼ æ„Ÿå™¨ä¸»é¢˜ã€‚
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Leave it running for the rest of the tutorial.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: è®©å®ƒåœ¨æ¥ä¸‹æ¥çš„æ•™ç¨‹ä¸­ä¸€ç›´è¿è¡Œã€‚
- en: 'Step 5: Initializing the Flink task'
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬5æ­¥ï¼šåˆå§‹åŒ–Flinkä»»åŠ¡
- en: 'Weâ€™re going to launch the Flink application from within the container, so you
    can monitor it from the web UI through localhost:8081\. Run the following command
    from the repository root:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä»å®¹å™¨å†…å¯åŠ¨Flinkåº”ç”¨ç¨‹åºï¼Œå› æ­¤ä½ å¯ä»¥é€šè¿‡Web UIé€šè¿‡localhost:8081ç›‘æ§å®ƒã€‚ä»ä»“åº“æ ¹ç›®å½•è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Youâ€™ll see some logging information, additionally alerts will also be displayed
    in the flink-jobmanager container logs. Also, you can check if the job is running
    from the Flink web UI [http://localhost:8081/#/job/running](http://localhost:8081/#/job/running).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å°†çœ‹åˆ°ä¸€äº›æ—¥å¿—ä¿¡æ¯ï¼Œæ­¤å¤–ï¼Œè­¦æŠ¥ä¹Ÿä¼šæ˜¾ç¤ºåœ¨flink-jobmanagerå®¹å™¨çš„æ—¥å¿—ä¸­ã€‚åŒæ—¶ï¼Œä½ å¯ä»¥ä»Flink Web UI [http://localhost:8081/#/job/running](http://localhost:8081/#/job/running)æ£€æŸ¥ä½œä¸šæ˜¯å¦æ­£åœ¨è¿è¡Œã€‚
- en: '![](../Images/61e46067d510dfa78e078de834b3ad0a.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61e46067d510dfa78e078de834b3ad0a.png)'
- en: Details of running job â€” image by author
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: è¿è¡Œä½œä¸šçš„è¯¦ç»†ä¿¡æ¯â€”â€”ä½œè€…æä¾›çš„å›¾ç‰‡
- en: Apparently the monitoring tells that there are no messages going through the
    Flink job, which is not true, since alerts can be seen in the docker log.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¾ç„¶ï¼Œç›‘æ§æ˜¾ç¤ºFlinkä½œä¸šä¸­æ²¡æœ‰æ¶ˆæ¯æµåŠ¨ï¼Œä½†è¿™ä¸æ˜¯çœŸçš„ï¼Œå› ä¸ºå¯ä»¥åœ¨dockeræ—¥å¿—ä¸­çœ‹åˆ°è­¦æŠ¥ã€‚
- en: '![](../Images/a76525be2f6b2ec01ef7e1d6d89b2518.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a76525be2f6b2ec01ef7e1d6d89b2518.png)'
- en: Weâ€™ll check the messages using the Postgres table and read the alerts topic,
    which were created for this purpose.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†é€šè¿‡Postgresè¡¨æ£€æŸ¥æ¶ˆæ¯ï¼Œå¹¶è¯»å–ä¸ºæ­¤ç›®çš„åˆ›å»ºçš„è­¦æŠ¥ä¸»é¢˜ã€‚
- en: 'Step 6: Read Alerts in Kafka Topic'
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬6æ­¥ï¼šè¯»å–Kafkaä¸»é¢˜ä¸­çš„è­¦æŠ¥
- en: 'To read data in the alerts topic, you can execute the following command:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è¯»å–è­¦æŠ¥ä¸»é¢˜ä¸­çš„æ•°æ®ï¼Œå¯ä»¥æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: That will bring all the messages that the topic has received so far.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†å±•ç¤ºåˆ°ç›®å‰ä¸ºæ­¢è¯¥ä¸»é¢˜æ¥æ”¶åˆ°çš„æ‰€æœ‰æ¶ˆæ¯ã€‚
- en: 'Step 7: Read raw data from Postgres table'
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬7æ­¥ï¼šä»Postgresè¡¨è¯»å–åŸå§‹æ•°æ®
- en: 'Additionally you can query the raw messages from the IoT sensor and even parse
    the JSON data in PostgreSQL:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œä½ è¿˜å¯ä»¥æŸ¥è¯¢ IoT ä¼ æ„Ÿå™¨çš„åŸå§‹æ¶ˆæ¯ï¼Œç”šè‡³åœ¨ PostgreSQL ä¸­è§£æ JSON æ•°æ®ï¼š
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Step 8: Stopping Services'
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬ 8 æ­¥ï¼šåœæ­¢æœåŠ¡
- en: 'You can easily stop everything by doing ctrl-c on the docker terminal. If you
    prefer, to make proper shutdown, proceed with the following steps:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥é€šè¿‡åœ¨ Docker ç»ˆç«¯æŒ‰ Ctrl-c æ¥è½»æ¾åœæ­¢æ‰€æœ‰æœåŠ¡ã€‚å¦‚æœä½ æ›´å€¾å‘äºæ­£ç¡®åœ°å…³é—­æœåŠ¡ï¼Œå¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œæ“ä½œï¼š
- en: Cancel the Flink job by clicking in the top right corner of job details in the
    web UI.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨ Web UI çš„ä½œä¸šè¯¦æƒ…é¡µå³ä¸Šè§’ç‚¹å‡»ä»¥å–æ¶ˆ Flink ä½œä¸šã€‚
- en: Stop the kafka_producer.py script which was running locally.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœæ­¢æœ¬åœ°è¿è¡Œçš„ kafka_producer.py è„šæœ¬ã€‚
- en: Ctrl-c on the docker terminal to stop the services
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨ Docker ç»ˆç«¯æŒ‰ Ctrl-c åœæ­¢æœåŠ¡
- en: The information exchanged in the session, while the services were running, is
    permanently stored. So in the case you want to query the Postgres table or the
    Kafka topics, the data is going to be there.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼šè¯ä¸­äº¤æ¢çš„ä¿¡æ¯ï¼Œåœ¨æœåŠ¡è¿è¡ŒæœŸé—´ä¼šè¢«æ°¸ä¹…å­˜å‚¨ã€‚å› æ­¤ï¼Œå¦‚æœä½ æƒ³æŸ¥è¯¢ Postgres è¡¨æˆ– Kafka ä¸»é¢˜ï¼Œæ•°æ®å°†ä¼šåœ¨å…¶ä¸­ã€‚
- en: Insights on Using Multiple Sinks in a PyFlink Job
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å¤šä¸ª Sink åœ¨ PyFlink ä½œä¸šä¸­çš„è§è§£
- en: 'In the Flink job used for demonstration, Iâ€™m managing 2 data streams simultaneously,
    in the same task. The one that writes raw data coming from the sensors topic (IoT
    devices) and the filtered alerts which are set to another topic. This has some
    advantages and drawbacks, as a simple summary, here are the pros and cons:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç”¨äºæ¼”ç¤ºçš„ Flink ä½œä¸šä¸­ï¼Œæˆ‘åŒæ—¶ç®¡ç† 2 ä¸ªæ•°æ®æµï¼Œå®ƒä»¬åœ¨åŒä¸€ä¸ªä»»åŠ¡ä¸­è¿è¡Œã€‚ä¸€ä¸ªæ˜¯å†™å…¥æ¥è‡ªä¼ æ„Ÿå™¨ä¸»é¢˜ï¼ˆIoT è®¾å¤‡ï¼‰çš„åŸå§‹æ•°æ®ï¼Œå¦ä¸€ä¸ªæ˜¯è¿‡æ»¤åçš„è­¦æŠ¥æ•°æ®ï¼Œå†™å…¥å¦ä¸€ä¸ªä¸»é¢˜ã€‚è¿™ç§æ–¹å¼æœ‰ä¸€äº›ä¼˜ç‚¹å’Œç¼ºç‚¹ï¼Œç®€å•æ€»ç»“å¦‚ä¸‹ï¼š
- en: '**Pros of Single Job with Multiple Sinks:**'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**å•ä¸€ä½œä¸šä½¿ç”¨å¤šä¸ª Sink çš„ä¼˜ç‚¹ï¼š**'
- en: '- Simplicity in resource management.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '- èµ„æºç®¡ç†çš„ç®€ä¾¿æ€§ã€‚'
- en: '- Consistency in data flow.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '- æ•°æ®æµçš„ä¸€è‡´æ€§ã€‚'
- en: '**Cons of Single Job:**'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**å•ä¸€ä½œä¸šçš„ç¼ºç‚¹ï¼š**'
- en: '- Can become complex as logic grows.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '- éšç€é€»è¾‘çš„å¢é•¿ï¼Œå¯èƒ½å˜å¾—å¤æ‚ã€‚'
- en: '- Scalability might be an issue.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '- å¯æ‰©å±•æ€§å¯èƒ½æ˜¯ä¸ªé—®é¢˜ã€‚'
- en: '**Pros of Multiple Jobs:**'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¤šä¸ªä½œä¸šçš„ä¼˜ç‚¹ï¼š**'
- en: '- Better fault isolation.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '- æ›´å¥½çš„æ•…éšœéš”ç¦»ã€‚'
- en: '- Focused optimization.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '- èšç„¦ä¼˜åŒ–ã€‚'
- en: '**Cons of Multiple Jobs:**'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¤šä¸ªä½œä¸šçš„ç¼ºç‚¹ï¼š**'
- en: '- Resource overhead.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '- èµ„æºå¼€é”€ã€‚'
- en: '- Coordination complexity.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '- åè°ƒå¤æ‚æ€§ã€‚'
- en: Conclusion
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: This setup offers a robust solution for real-time data streaming and processing,
    integrating Flink, Kafka, and PostgreSQL effectively. The main purpose of using
    Postgres in the loop is to check the raw messages coming from the IoT devices
    without relying on queries to the topic itself. It also helped to demonstrate
    how to sink data using a JDBC connector, which might be pretty standard. The message
    transformations were done using the DataStream API. I would like to dive further
    into the SQL API which introduces a friendlier interface. Finally, regarding how
    to manage data streams, choose between single or multiple jobs based on your specific
    requirements ensuring scalability and maintainability.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥è®¾ç½®ä¸ºå®æ—¶æ•°æ®æµå’Œå¤„ç†æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„è§£å†³æ–¹æ¡ˆï¼Œæœ‰æ•ˆåœ°é›†æˆäº† Flinkã€Kafka å’Œ PostgreSQLã€‚ä½¿ç”¨ Postgres çš„ä¸»è¦ç›®çš„æ˜¯æ£€æŸ¥æ¥è‡ª
    IoT è®¾å¤‡çš„åŸå§‹æ¶ˆæ¯ï¼Œè€Œä¸æ˜¯ä¾èµ–äºç›´æ¥æŸ¥è¯¢ä¸»é¢˜æœ¬èº«ã€‚å®ƒè¿˜å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ JDBC è¿æ¥å™¨è¿›è¡Œæ•°æ®è¾“å‡ºï¼Œè¿™å¯èƒ½æ˜¯ä¸€ä¸ªç›¸å¯¹æ ‡å‡†çš„åšæ³•ã€‚æ¶ˆæ¯è½¬æ¢é€šè¿‡ DataStream
    API å®Œæˆã€‚æˆ‘å¸Œæœ›è¿›ä¸€æ­¥æ·±å…¥å­¦ä¹  SQL APIï¼Œå®ƒæä¾›äº†ä¸€ä¸ªæ›´åŠ å‹å¥½çš„æ¥å£ã€‚æœ€åï¼Œå…³äºå¦‚ä½•ç®¡ç†æ•°æ®æµï¼Œå»ºè®®æ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©å•ä¸€ä½œä¸šæˆ–å¤šä¸ªä½œä¸šï¼Œç¡®ä¿å¯æ‰©å±•æ€§å’Œå¯ç»´æŠ¤æ€§ã€‚
- en: Next Steps
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥
- en: 1\. Use SQL API to make transformations.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. ä½¿ç”¨ SQL API è¿›è¡Œæ•°æ®è½¬æ¢ã€‚
- en: 2\. Optimize resource usage based on job complexity.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. æ ¹æ®ä½œä¸šå¤æ‚æ€§ä¼˜åŒ–èµ„æºä½¿ç”¨ã€‚
- en: 3\. Explore advanced Flink features for complex data processing tasks.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. æ¢ç´¢ Flink çš„é«˜çº§åŠŸèƒ½ä»¥åº”å¯¹å¤æ‚çš„æ•°æ®å¤„ç†ä»»åŠ¡ã€‚
- en: Happy streaming! ğŸš€
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: å¿«ä¹æµå¼å¤„ç†ï¼ğŸš€
- en: '**Stay tuned for more tutorials on integrating and scaling data engineering
    solutions with Docker!**'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ•¬è¯·æœŸå¾…æ›´å¤šå…³äºå¦‚ä½•é€šè¿‡ Docker é›†æˆå’Œæ‰©å±•æ•°æ®å·¥ç¨‹è§£å†³æ–¹æ¡ˆçš„æ•™ç¨‹ï¼**'
- en: '*Feel free to reach out for any questions or suggestions in the comments below!*'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¦‚æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·éšæ—¶åœ¨ä¸‹æ–¹è¯„è®ºåŒºä¸æˆ‘ä»¬è”ç³»ï¼*'
- en: Ready to Optimize Your Streaming Data Applications?
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‡†å¤‡å¥½ä¼˜åŒ–ä½ çš„æµæ•°æ®åº”ç”¨äº†å—ï¼Ÿ
- en: Unlock the full potential of your data with our [expert consulting services](https://www.squadralabs.com/),
    tailored for streaming data applications. Whether youâ€™re looking to enhance real-time
    analytics, streamline data pipelines, or optimize performance, weâ€™re here to help.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: è§£é”æ•°æ®çš„å…¨éƒ¨æ½œåŠ›ï¼Œä½¿ç”¨æˆ‘ä»¬çš„[ä¸“å®¶å’¨è¯¢æœåŠ¡](https://www.squadralabs.com)ï¼Œä¸ºæµæ•°æ®åº”ç”¨é‡èº«å®šåˆ¶ã€‚æ— è®ºä½ æ˜¯æƒ³å¢å¼ºå®æ—¶åˆ†æã€ä¼˜åŒ–æ•°æ®ç®¡é“ï¼Œè¿˜æ˜¯æå‡æ€§èƒ½ï¼Œæˆ‘ä»¬éƒ½èƒ½ä¸ºä½ æä¾›å¸®åŠ©ã€‚
- en: References
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒèµ„æ–™
- en: '[https://www.confluent.io/blog/kafka-client-cannot-connect-to-broker-on-aws-on-docker-etc/](https://www.confluent.io/blog/kafka-client-cannot-connect-to-broker-on-aws-on-docker-etc/)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.confluent.io/blog/kafka-client-cannot-connect-to-broker-on-aws-on-docker-etc/](https://www.confluent.io/blog/kafka-client-cannot-connect-to-broker-on-aws-on-docker-etc/)'
- en: '[https://mvnrepository.com/](https://mvnrepository.com/)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://mvnrepository.com/](https://mvnrepository.com/)'
- en: '[https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/jdbc/](https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/jdbc/)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/jdbc/](https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/jdbc/)'
- en: '[https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/overview/#session-mode](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/overview/#session-mode)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/overview/#session-mode](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/overview/#session-mode)'
- en: '[https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/standalone/docker/#using-flink-python-on-docker](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/standalone/docker/#using-flink-python-on-docker)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/standalone/docker/#using-flink-python-on-docker](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/standalone/docker/#using-flink-python-on-docker)'
- en: '[https://medium.com/@sant1/flink-docker-kafka-faee9c0f1580](https://medium.com/@sant1/flink-docker-kafka-faee9c0f1580)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://medium.com/@sant1/flink-docker-kafka-faee9c0f1580](https://medium.com/@sant1/flink-docker-kafka-faee9c0f1580)'
