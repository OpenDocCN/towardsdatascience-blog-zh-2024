- en: Perform Outlier Detection More Effectively Using Subsets of Features
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用特征子集更有效地执行异常值检测
- en: 原文：[https://towardsdatascience.com/perform-outlier-detection-more-effectively-using-subsets-of-features-d984bde99981?source=collection_archive---------1-----------------------#2024-11-24](https://towardsdatascience.com/perform-outlier-detection-more-effectively-using-subsets-of-features-d984bde99981?source=collection_archive---------1-----------------------#2024-11-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/perform-outlier-detection-more-effectively-using-subsets-of-features-d984bde99981?source=collection_archive---------1-----------------------#2024-11-24](https://towardsdatascience.com/perform-outlier-detection-more-effectively-using-subsets-of-features-d984bde99981?source=collection_archive---------1-----------------------#2024-11-24)
- en: 'Identify relevant subspaces: subsets of features that allow you to most effectively
    perform outlier detection on tabular data'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 确定相关子空间：特征的子集，它们可以让你在表格数据上更有效地进行异常值检测
- en: '[](https://medium.com/@wkennedy934?source=post_page---byline--d984bde99981--------------------------------)[![W
    Brett Kennedy](../Images/b3ce55ffd028167326c117d47c64c467.png)](https://medium.com/@wkennedy934?source=post_page---byline--d984bde99981--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d984bde99981--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d984bde99981--------------------------------)
    [W Brett Kennedy](https://medium.com/@wkennedy934?source=post_page---byline--d984bde99981--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@wkennedy934?source=post_page---byline--d984bde99981--------------------------------)[![W
    Brett Kennedy](../Images/b3ce55ffd028167326c117d47c64c467.png)](https://medium.com/@wkennedy934?source=post_page---byline--d984bde99981--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d984bde99981--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d984bde99981--------------------------------)
    [W Brett Kennedy](https://medium.com/@wkennedy934?source=post_page---byline--d984bde99981--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d984bde99981--------------------------------)
    ·28 min read·Nov 24, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d984bde99981--------------------------------)
    ·阅读时长28分钟·2024年11月24日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: This article is part of a series related to the challenges, and the techniques
    that may be used, to best identify outliers in data, including articles related
    to using [PCA](/using-pca-for-outlier-detection-afecab4d2b78), [Distance Metric
    Learning](/distance-metric-learning-for-outlier-detection-5b4840d01246), [Shared
    Nearest Neighbors](/shared-nearest-neighbors-a-more-robust-distance-metric-064d7f99ffb7),
    [Frequent Patterns Outlier Factor](/interpretable-outlier-detection-frequent-patterns-outlier-factor-fpof-0d9cbf51b17a),
    [Counts Outlier Detector](/counts-outlier-detector-interpretable-outlier-detection-ead0d469557a)
    (a multi-dimensional histogram-based method), and [doping](https://medium.com/towards-data-science/doping-a-technique-to-test-outlier-detectors-3f6b847ab8d4).
    This article also contains an excerpt from my book, [Outlier Detection in Python](https://www.manning.com/books/outlier-detection-in-python).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本文是系列文章的一部分，涉及在数据中识别异常值的挑战和可以使用的技术，包括使用[PCA](/using-pca-for-outlier-detection-afecab4d2b78)、[距离度量学习](/distance-metric-learning-for-outlier-detection-5b4840d01246)、[共享最近邻](/shared-nearest-neighbors-a-more-robust-distance-metric-064d7f99ffb7)、[频繁模式异常因子](/interpretable-outlier-detection-frequent-patterns-outlier-factor-fpof-0d9cbf51b17a)、[计数异常值检测器](/counts-outlier-detector-interpretable-outlier-detection-ead0d469557a)（一种基于多维直方图的方法），以及[doping](https://medium.com/towards-data-science/doping-a-technique-to-test-outlier-detectors-3f6b847ab8d4)技术。本文还包含了我书中的一段摘录，[Python中的异常值检测](https://www.manning.com/books/outlier-detection-in-python)。
- en: We look here at techniques to create, instead of a single outlier detector examining
    all features within a dataset, a series of smaller outlier detectors, each working
    with a subset of the features (referred to as *subspaces*).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们探讨了一种技术，旨在创建一系列较小的异常值检测器，而不是单一的异常值检测器来检查数据集中的所有特征，每个检测器都只处理特征的子集（称为*子空间*）。
- en: Challenges with outlier detection
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异常值检测中的挑战
- en: When performing outlier detection on tabular data, we’re looking for the records
    in the data that are the most unusual — either relative to the other records in
    the same dataset, or relative to previous data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在对表格数据进行异常值检测时，我们关注的是数据中最不寻常的记录——这些记录要么与同一数据集中的其他记录相比最为特殊，要么与之前的数据相比最为特殊。
- en: There are a number of challenges associated with finding the most meaningful
    outliers, particularly that there is no definition of *statistically unusual*
    that definitively specifies which anomalies in the data should be considered the
    strongest. As well, the outliers that are most *relevant* (and not necessarily
    the most statistically unusual) for your purposes will be specific to your project,
    and may evolve over time.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找最有意义的异常值存在许多挑战，尤其是没有明确的*统计异常*定义来确定哪些数据中的异常值应被视为最强的异常值。此外，最*相关*的异常值（而不一定是最统计异常的）将取决于你的项目，并可能随着时间的推移而变化。
- en: There are also a number of technical challenges that appear in outlier detection.
    Among these are the difficulties that occur where data has many features. As covered
    in previous articles related to [Counts Outlier Detector](/counts-outlier-detector-interpretable-outlier-detection-ead0d469557a)
    and [Shared Nearest Neighbors](/shared-nearest-neighbors-a-more-robust-distance-metric-064d7f99ffb7),
    where we have many features, we often face an issue known as the *curse of dimensionality*.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值检测中还存在许多技术挑战，其中之一是数据中存在许多特征时所出现的困难。如在之前与[Counts Outlier Detector](/counts-outlier-detector-interpretable-outlier-detection-ead0d469557a)和[Shared
    Nearest Neighbors](/shared-nearest-neighbors-a-more-robust-distance-metric-064d7f99ffb7)相关的文章中所述，当特征很多时，我们通常会面临一个称为*维度灾难*的问题。
- en: This has a number of implications for outlier detection, including that it makes
    distance metrics unreliable. Many outlier detection algorithms rely on calculating
    the distances between records — in order to identify as outliers the records that
    are similar to unusually few other records, and that are unusually different from
    most other records — that is, records that are *close to* few other records and
    *far from* most other records.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这对异常值检测有多个影响，包括使得距离度量变得不可靠。许多异常值检测算法依赖于计算记录之间的距离——为了识别作为异常值的记录，这些记录与其他记录相比非常相似，且与大多数其他记录有显著不同——也就是说，记录与其他记录的距离是*近*少数记录，*远*多数记录。
- en: For example, if we have a table with 40 features, each record in the data may
    be viewed as a point in 40-dimensional space, and its outlierness can be evaluated
    by the distances from it to the other points in this space. This, then, requires
    a way to measure the distance between records. A variety of measures are used,
    with Euclidean distances being quite common (assuming the data is numeric, or
    is converted to numeric values). So, the outlierness of each record is often measured
    based on the Euclidean distance between it and the other records in the dataset.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们有一个包含40个特征的表格，数据中的每条记录可以被视为40维空间中的一个点，并且其异常性可以通过该点与该空间中其他点之间的距离来评估。因此，这就需要一种衡量记录之间距离的方法。使用了多种度量方法，其中欧几里得距离相当常见（假设数据是数值型的，或已转换为数值）。因此，每条记录的异常性通常是根据它与数据集中其他记录之间的欧几里得距离来衡量的。
- en: These distance calculations can, though, break down where we are working with
    many features and, in fact, issues with distance metrics may appear even with
    only ten or twenty features, and very often with about thirty or forty or more.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当我们处理许多特征时，这些距离计算可能会出现问题，实际上，即使只有十个或二十个特征，距离度量的问题也可能出现，尤其是当特征数量达到三十、四十个甚至更多时。
- en: We should note though, issues dealing with large numbers of features do not
    appear with all outlier detectors. For example, they do not tend to be significant
    when working with univariate tests (tests such as z-score or interquartile range
    tests, that consider each feature one at a time, independently of the other features
    — described in more detail in [A Simple Example Using PCA for Outlier Detection](https://medium.com/towards-data-science/a-simple-example-using-pca-for-outlier-detection-ab2773b98e4a))
    or when using categorical outlier detectors such as [FPOF](https://medium.com/towards-data-science/interpretable-outlier-detection-frequent-patterns-outlier-factor-fpof-0d9cbf51b17a).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，处理大量特征的问题并非所有异常值检测器都会遇到。例如，在使用单变量测试（例如z-score或四分位数范围测试，这些测试逐个考虑每个特征，独立于其他特征——在[A
    Simple Example Using PCA for Outlier Detection](https://medium.com/towards-data-science/a-simple-example-using-pca-for-outlier-detection-ab2773b98e4a)中有更详细的描述）时，它们通常不会显得很重要，或者在使用像[FPOF](https://medium.com/towards-data-science/interpretable-outlier-detection-frequent-patterns-outlier-factor-fpof-0d9cbf51b17a)这样的类别型异常值检测器时。
- en: However, the majority of outlier detectors commonly used are numeric multi-variate
    outlier detectors — detectors that assume all features are numeric, and that generally
    work on all features at once. For example, LOF (Local Outlier Factor) and KNN
    (k-Nearest Neighbors) are two the the most widely-used detectors and these both
    evaluate the outlierness of each record based on their distances (in the high-dimensional
    spaces the data points live in) to the other records.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，常用的大多数异常值检测器都是数值型多变量异常值检测器——这些检测器假设所有特征都是数值型的，并且通常会同时处理所有特征。例如，LOF（局部异常因子）和KNN（k-近邻）是最广泛使用的两种检测器，它们都基于记录与其他记录在高维空间中的距离来评估每个记录的异常性。
- en: An example of outliers based on their distances to other datapoints
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于记录与其他数据点的距离判断异常值的示例
- en: Consider the plots below. This presents a dataset with six features, shown in
    three 2d scatter plots. This includes two points that can reasonably be considered
    outliers, P1 and P2.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 请看下面的图。这展示了一个包含六个特征的数据集，并通过三个二维散点图表示。图中包括了两个可以合理认为是异常值的点，P1和P2。
- en: Looking, for now, at P1, it is far from the other points, at least in feature
    A. That is, considering just feature A, P1 can easily be flagged as an outlier.
    However, most detectors will consider the distance of each point to the other
    points using all six dimensions, which, unfortunately, means P1 may not necessarily
    stand out as an outlier, due to the nature of distance calculations in high-dimensional
    spaces. P1 is fairly typical in the other five features, and so it’s distance
    to the other points, in 6d space, may be fairly normal.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在来看P1，它至少在特征A上与其他点相距较远。也就是说，仅考虑特征A时，P1很容易被标记为异常值。然而，大多数检测器会考虑每个点到其他点的距离，使用所有六个维度，这不幸的是意味着P1未必会因为高维空间中的距离计算方式而显得突出。P1在其他五个特征上是相当典型的，因此它在六维空间中的距离可能是相对正常的。
- en: 'Nevertheless, we can see that this general approach to outlier detection —
    where we examine the distances from each record to the other records — is quite
    reasonable: P1 and P2 are outliers because they are far (at least in some dimensions)
    from the other points.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们可以看到，这种通用的异常值检测方法——通过检查每个记录到其他记录的距离——是相当合理的：P1和P2是异常值，因为它们与其他点的距离较远（至少在某些维度上）。
- en: '![](../Images/61f3d772fbd9a9a7ef17de2d324cf2bb.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61f3d772fbd9a9a7ef17de2d324cf2bb.png)'
- en: KNN and LOF algorithms
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KNN和LOF算法
- en: As KNN and LOF are very commonly used detectors, we’ll look at them a little
    closer here, and then look specifically at using subspaces with these algorithms.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 由于KNN和LOF是非常常用的异常值检测器，我们将在这里仔细研究它们，然后特别探讨如何在这些算法中使用子空间。
- en: With the KNN outlier detector, we pick a value for k, which determines how many
    neighbors each record is compared to. Let’s say we pick 10 (in practice, this
    would be a fairly typical value).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用KNN异常值检测器时，我们选择一个k值，决定每个记录与多少个邻居进行比较。假设我们选择10（在实际应用中，这通常是一个相对典型的值）。
- en: For each record, we then measure the distance to its 10 nearest neighbors, which
    provides a good sense of how isolated and remote each point is. We then need to
    create a single outlier score (i.e., a single number) for each record based on
    these 10 distances. For this, we generally then take either the mean or the maximum
    of these distances.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个记录，我们测量它与其10个最近邻的距离，这有助于我们了解每个点的孤立性和远离程度。然后，我们需要根据这10个距离为每个记录创建一个单一的异常值评分（即一个数字）。通常，我们会取这些距离的均值或最大值。
- en: Let’s assume we take the maximum (using the mean, median, or other function
    works similarly, though each have their nuances). If a record has an unusually
    large distance to its 10th nearest neighbor, this means there are at most 9 records
    that are reasonably close to it (and possibly less), and that it is otherwise
    unusually far from most other points, so can be considered an outlier.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们选择最大值（使用均值、中位数或其他函数也类似，尽管每个方法有其细微差别）。如果某个记录与其第10近邻的距离异常大，这意味着最多有9个记录与其相对较近（可能更少），而它与大多数其他点的距离则异常远，因此可以认为它是异常值。
- en: With the LOF outlier detector, we use a similar approach, though it works a
    bit differently. We also look at the distance of each point to its k nearest neighbors,
    but then compare this to the distances of these k neighbors to *their* k nearest
    neighbors. So LOF measures the outlierness of each point relative to the other
    points in their neighborhoods.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LOF离群点检测器时，我们采用类似的方法，尽管其工作方式有所不同。我们同样查看每个点到其k个最近邻的距离，然后将其与这些k个邻居到*它们*的k个最近邻的距离进行比较。因此，LOF通过衡量每个点相对于其邻域内其他点的离群程度来评估离群点。
- en: That is, while KNN uses a global standard to determine what are unusually large
    distances to their neighbors, LOF uses a local standard to determine what are
    unusually large distances.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，虽然KNN使用全局标准来判断与邻居之间的距离是否异常大，但LOF则使用局部标准来判断这些距离是否异常大。
- en: The details of the LOF algorithm are actually a bit more involved, and the implications
    of the specific differences in these two algorithms (and the many variations of
    these algorithms) are covered in more detail in [Outlier Detection in Python](https://www.manning.com/books/outlier-detection-in-python).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: LOF算法的细节实际上要复杂一些，这两个算法（以及这些算法的多种变体）在具体差异上的含义在[《Python中的离群点检测》](https://www.manning.com/books/outlier-detection-in-python)中有更详细的介绍。
- en: These are interesting considerations in themselves, but the main point for here
    is that KNN and LOF both evaluate records based on their distances to their closest
    neighbors. And that these distance metrics can work sub-optimally (or even completely
    breakdown) if using many features at once, which is reduced greatly by working
    with small numbers of features (subspaces) at a time.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这些本身就是有趣的考虑因素，但此处的主要观点是，KNN和LOF都根据记录与其最近邻的距离来评估记录。而且，如果同时使用大量特征，这些距离度量可能会表现不佳（甚至完全失效），而通过一次只处理少量特征（子空间）可以大大减少这种情况。
- en: The idea of using subspaces is useful even where the detector used does not
    use distance metrics, but where detectors based on distance calculations are used,
    some of the benefits of using subspaces can be a bit more clear. And, using distances
    in ways similar to KNN and LOF is quite common among detectors. As well as KNN
    and LOF, for example, Radius, ODIN, INFLO, and LoOP detectors, as well as detectors
    based on sampling, and detectors based on clustering, all use distances.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用子空间的思想在检测器不使用距离度量的情况下仍然有用，但在使用基于距离计算的检测器时，使用子空间的一些好处可能会更加明显。而且，像KNN和LOF这样的距离使用方法在检测器中是相当常见的。除了KNN和LOF之外，例如，Radius、ODIN、INFLO、LoOP检测器，以及基于采样和聚类的检测器，都使用距离。
- en: However, issues with the curse of dimensionality can occur with other detectors
    as well. For example, ABOD (Angle-based Outlier Detector) uses the angles between
    records to evaluate the outlierness of each record, as opposed to the distances.
    But, the idea is similar, and using subspaces can also be helpful when working
    with ABOD.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，维度灾难的问题也可能出现在其他检测器中。例如，ABOD（基于角度的离群点检测器）使用记录之间的角度来评估每条记录的离群程度，而不是使用距离。但其思路相似，使用子空间在使用ABOD时也同样有效。
- en: 'As well, other benefits of subspaces I’ll go through below apply equally to
    many detectors, whether using distance calculations or not. Still, the curse of
    dimensionality is a serious concern in outlier detection: where detectors use
    distance calculations (or similar measures, such as angle calculations), and there
    are many features, these distance calculations can break down. In the plots above,
    P1 and P2 may be detected well considering only six dimensions, and quite possibly
    if using 10 or 20 features, but if there were, say, 100 dimensions, the distances
    between all points would actually end up about the same, and P1 and P2 would not
    stand out at all as unusual.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我将在下面介绍的其他子空间的好处同样适用于许多检测器，无论是否使用距离计算。不过，维度灾难在离群点检测中是一个严重的问题：当检测器使用距离计算（或类似的度量，如角度计算），且特征数量很多时，这些距离计算可能会失效。在上面的图示中，P1和P2在仅考虑六个维度的情况下可能被良好检测到，甚至在使用10或20个特征时也可能如此，但如果有100个维度的话，所有点之间的距离最终可能会非常相似，P1和P2就不再显得异常了。
- en: Issues with moderate numbers of features
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征数适中时的问题
- en: Outside of the issues related to working with very large numbers of features,
    our attempts to identify the most unusual records in a dataset can be undermined
    even when working with fairly small numbers of features.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 除了与处理大量特征相关的问题外，即便是在特征数量相对较少的情况下，我们在试图识别数据集中最不寻常的记录时也可能会遇到困难。
- en: While very large numbers of features can make the distances calculated between
    records meaningless, even moderate numbers of features can make records that are
    unusual in just one or two features more difficult to identify.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大量的特征可能会使记录之间计算出的距离变得毫无意义，但即使是适度数量的特征，也可能使仅在一两个特征上表现异常的记录更难被识别出来。
- en: Consider again the scatter plot shown earlier, repeated here. Point P1 is an
    outlier in feature A (thought not in the other five features). Point P2 is unusual
    in features C and D, but not in the other four features). However, when considering
    the Euclidean distances of these points to the other points in 6-dimensional space,
    they may not reliably stand out as outliers. The same would be true using Manhattan,
    and most other distance metrics as well.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 再次考虑之前展示的散点图，并在这里重复展示。点 P1 在特征 A 上是异常值（但在其他五个特征上并非如此）。点 P2 在特征 C 和 D 上表现异常，但在其他四个特征上并非如此。然而，当考虑这些点到其他点在
    6 维空间中的欧几里得距离时，它们可能并不能可靠地突出表现为异常值。使用曼哈顿距离以及大多数其他距离度量时也是如此。
- en: '![](../Images/61f3d772fbd9a9a7ef17de2d324cf2bb.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61f3d772fbd9a9a7ef17de2d324cf2bb.png)'
- en: The left pane shows point P1 in a 2D dataspace. The point is unusual considering
    feature A, but less so if using Euclidean distances in the full 6D dataspace,
    or even the 2D dataspace shown in this plot. This is an example where using additional
    features can be counterproductive. In the middle pane, we see another point, point
    P2, which is an outlier in the C–D subspace but not in the A-B or E–F subspaces.
    We need only features C and D to identify this outlier, and again including other
    features will simply make P2 more difficult to identify.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧窗格展示了点 P1 在 2D 数据空间中的位置。考虑到特征 A，这个点是异常的，但如果使用 6D 数据空间中的欧几里得距离，甚至在此图所示的 2D
    数据空间中，它的异常性就较小了。这是一个使用额外特征可能适得其反的例子。在中间窗格中，我们看到另一个点——点 P2，它在 C-D 子空间中是异常值，但在 A-B
    或 E-F 子空间中并不是。我们只需要特征 C 和 D 来识别这个异常值，再次强调，包含其他特征只会让 P2 更难被识别出来。
- en: 'P1, for example, even in the 2d space shown in the left-most plot, is not unusually
    far from most other points. It’s unusual that there are no other points near it
    (which KNN and LOF will detect), but the distance from P1 to the other points
    in this 2d space is not unusual: it’s similar to the distances between most other
    pairs of points.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，即使在最左侧图表中显示的二维空间中，P1 与大多数其他点的距离也并不异常远。它的异常之处在于周围没有其他点（KNN 和 LOF 会检测到这一点），但是
    P1 到该二维空间中其他点的距离并不异常：它与大多数其他点对之间的距离相似。
- en: Using a KNN algorithm, we would likely be able to detect this, at least if k
    is set fairly low, for example, to 5 or 10 — most records have their 5th (and
    their 10th) nearest neighbors much closer than P1 does. Though, when including
    all six features in the calculations, this is much less clear than when viewing
    just feature A, or just the left-most plot, with just features A and B.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 KNN 算法，我们很可能能够检测到这一点，至少当 k 设置得相对较低时，例如设为 5 或 10——大多数记录的第 5 个（以及第 10 个）最近邻距离比
    P1 更近。不过，当将所有六个特征纳入计算时，这一点就不如仅查看特征 A 或仅查看最左侧的图表（仅包含特征 A 和 B）时那么明显。
- en: Point P2 stands out well as an outlier when considering just features C and
    D. Using a KNN detector with a k value of, say, 5, we can identify its 5 nearest
    neighbors, and the distances to these would be larger than is typical for points
    in this dataset.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当仅考虑特征 C 和 D 时，点 P2 很明显是异常值。使用 KNN 检测器，假设 k 值为 5，我们可以识别它的 5 个最近邻，并且这些点到 P2 的距离会比该数据集中的典型点距离要大。
- en: Using an LOF detector, again with a k value of, say, 5, we can compare the distances
    to P1’s or P2’s 5 nearest neighbors to the distances to their 5 nearest neighbors
    and here as well, the distance from P1 or P2 to their 5 nearest neighbors would
    be found to be unusually large.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LOF 检测器，同样设置 k 值为 5，我们可以比较 P1 或 P2 与其 5 个最近邻的距离与它们的 5 个最近邻之间的距离，在这种情况下，P1
    或 P2 到它们的 5 个最近邻的距离会被发现异常大。
- en: At least this is straightforward when considering only Features A and B, or
    Features C and D, but again, when considering the full 6-d space, they become
    more difficult to identify as outliers.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 至少当只考虑特征 A 和 B，或特征 C 和 D 时，这一点比较直接，但当考虑完整的 6 维空间时，它们变得更难以识别为异常值。
- en: While many outlier detectors may still be able to identify P1 and P2 even with
    six, or a small number more, dimensions, it is clearly easier and more reliable
    to use fewer features. To detect P1, we really only need to consider feature A;
    and to identify P2, we really only need to consider features C and D. Including
    other features in the process simply makes this more difficult.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管许多离群点检测器可能仍然能够在六个或稍多维度的情况下识别P1和P2，但显然使用较少的特征会更容易且更可靠。要检测P1，我们实际上只需要考虑特征A；而要识别P2，我们实际上只需要考虑特征C和D。将其他特征包含进来反而会增加难度。
- en: 'This is actually a common theme with outlier detection. We often have many
    features in the datasets we work with, and each can be useful. For example, if
    we have a table with 50 features, it may be that all 50 features are relevant:
    either a rare value in any of these features would be interesting, or a rare combination
    of values in two or more features, for each of these 50 features, would be interesting.
    It would be, then, worth keeping all 50 features for analysis.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是离群点检测中的一个常见主题。我们通常处理的数据集有许多特征，每个特征都可能有用。例如，如果我们有一个包含50个特征的表格，可能所有50个特征都是相关的：无论是这些特征中的任何一个出现稀有值都可能是有趣的，还是这些50个特征中的两个或多个特征出现稀有值组合都可能是有趣的。那么，对于分析来说，保留所有50个特征是值得的。
- en: But, to identify any one anomaly, we generally need only a small number of features.
    In fact, it’s very rare for a record to be unusual in all features. And it’s very
    rare for a record to have a anomaly based on a rare combination of many features
    (see [Counts Outlier Detector](https://medium.com/towards-data-science/counts-outlier-detector-interpretable-outlier-detection-ead0d469557a)
    for more explanation of this).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，要识别任何一个异常，通常只需要少量特征。事实上，记录在所有特征中异常的情况非常罕见。基于许多特征的稀有组合来判断记录是否异常也非常罕见（请参阅[Counts
    Outlier Detector](https://medium.com/towards-data-science/counts-outlier-detector-interpretable-outlier-detection-ead0d469557a)以获取更多解释）。
- en: Any given outlier will likely have a rare value in one or two features, or a
    rare combination of values in a pair, or a set of perhaps three or four features.
    Only these features are necessary to identify the anomalies in that row, even
    though the other features may be necessary to detect the anomalies in other rows.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 任何给定的离群点可能在一两个特征上具有罕见的值，或在一对特征中具有罕见的值组合，或者在三个或四个特征的组合中具有罕见的值。即使其他特征可能对于检测其他行的异常很重要，识别该行的异常时，只有这些特征是必要的。
- en: Subspaces
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 子空间
- en: 'To address these issues, an important technique in outlier detection is using
    subspaces. The term *subspaces* simply refers to subsets of the features. In the
    example above, if we use the subspaces: A-B, C-D, E-F, A-E, B-C, B-D-F, and A-B-E,
    then we have seven subspaces (five 2d subspaces and two 3d subspaces). Creating
    these, we would run one (or more) detectors on each subspace, so would run at
    least seven detectors on each record.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，离群点检测中的一个重要技术是使用子空间。术语*子空间*仅指特征的子集。在上面的例子中，如果我们使用以下子空间：A-B、C-D、E-F、A-E、B-C、B-D-F，以及A-B-E，那么我们有七个子空间（五个二维子空间和两个三维子空间）。创建这些子空间后，我们会在每个子空间上运行一个（或多个）检测器，因此每条记录至少会运行七个检测器。
- en: Realistically, subspaces become more useful where we have many more features
    that six, and generally even the the subspaces themselves will have more than
    six features, and not just two or three, but viewing this simple case, for now,
    with a small number of small subspaces is fairly easy to understand.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 从现实角度看，当特征数超过六个时，子空间变得更加有用，通常即使是子空间本身也会有超过六个特征，而不仅仅是二三个特征，但目前来看，使用特征数量较少的小子空间的简单情况相对容易理解。
- en: Using these subspaces, we can more reliably find P1 and P2 as outliers. P1 would
    likely be scored high by the detector running on features A-B, the detector running
    on features A-E, and the detector running on features A-B-E. P2 would likely be
    detected by the detector running on features C-D, and possibly the detector running
    on B-C.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些子空间，我们可以更可靠地将P1和P2识别为离群点。P1很可能会被运行在特征A-B、A-E和A-B-E上的检测器打分较高。P2很可能会被运行在特征C-D上的检测器识别出来，可能还会被运行在特征B-C上的检测器检测到。
- en: 'However, we have to be careful: using only these seven subspaces, as opposed
    to a single 6d space covering all features, would miss any rare combinations of,
    for example, A and D, or C and E. These may or may not be detected using a detector
    covering all six features, but definitely could not be detected using a suite
    of detectors that simply never examine these combinations of features.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们必须小心：仅使用这七个子空间，而不是覆盖所有特征的单一6维空间，将会错过任何罕见的特征组合，例如A和D，或C和E。这些组合可能会被覆盖所有六个特征的检测器检测到，也可能不会被检测到，但绝对无法通过一组检测器来检测到，因为这些检测器根本不会检查这些特征组合。
- en: Using subspaces does have some large benefits, but does have some risk of missing
    relevant outliers. We’ll cover some techniques to generate subspaces below that
    mitigate this issue, but it can be useful to still run one or more outlier detectors
    on the full dataspace as well. In general, with outlier detection, we’re rarely
    able to find the full set of outliers we’re interested in unless we apply many
    techniques. As important as the use of subspaces can be, it is still often useful
    to use a variety of techniques, which may include running some detectors on the
    full data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 使用子空间确实有一些显著的好处，但也存在错过相关离群点的风险。我们将在下面介绍一些生成子空间的技术，以缓解这个问题，但仍然可以考虑在完整的数据空间上运行一个或多个离群点检测器。通常来说，使用离群点检测时，除非我们应用多种技术，否则很难找到我们感兴趣的完整离群点集。尽管子空间的使用非常重要，但仍然通常有必要使用多种技术，这可能包括在完整数据上运行一些检测器。
- en: Similarly, with each subspace, we may execute multiple detectors. For example,
    we may use both a KNN and LOF detector, as well as Radius, ABOD, and possibly
    a number of other detectors — again, using multiple techniques allows us to better
    cover the range of outliers we wish to detect.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在每个子空间中，我们可能会执行多个检测器。例如，我们可能会同时使用KNN和LOF检测器，以及Radius、ABOD，甚至可能是其他一些检测器——再次强调，使用多种技术可以帮助我们更好地覆盖希望检测的异常范围。
- en: Further Motivations for Subspaces
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 子空间的进一步动机
- en: 'We’ve seen, then, a couple motivations for working with subspaces: we can mitigate
    the curse of dimensionality, and we can reduce where anomalies are not identified
    reliably where they are based on small numbers of features that are lost among
    many features.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经看到了一些使用子空间的动机：我们可以缓解维度灾难，并且可以减少那些基于小数量特征且在众多特征中丢失的异常情况，从而无法可靠识别的情况。
- en: 'As well as handling situations like this, there are a number of other advantages
    to using subspaces with outlier detection. These include:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 除了处理类似的情况外，使用子空间进行离群点检测还有许多其他优势。包括：
- en: '**Accuracy due to the effects of using ensembles** — Using multiple subspaces
    allows us to create ensembles (collections of outlier detectors), which allows
    us to combine the results of many detectors. In general, using ensembles of detectors
    provides greater accuracy than using a single detector. This is similar (though
    with some real differences too) to the way ensembles of predictors tend to be
    stronger for classification and regression problems than a single predictor. Here,
    using subspaces, each record is examined multiple times, which provides a more
    stable evaluation of each record than any single detector would.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**由于使用集成方法的精度提升** — 使用多个子空间使我们能够创建集成（离群点检测器集合），这使我们能够结合多个检测器的结果。通常，使用检测器的集成方法比使用单一检测器提供更高的精度。这与集成预测器在分类和回归问题中通常比单一预测器更强的方式类似（尽管也存在一些实际差异）。在这里，使用子空间时，每个记录会被多次检查，这比任何单一检测器提供了更稳定的评估。'
- en: '**Interpretability** — The results can be more interpretable, and interpretability
    is often a key concern in outlier detection. Very often in outlier detection,
    we’re flagging unusual records with the idea that they may be a concern, or a
    point of interest, in some way, and often they will be manually examined. Knowing
    why they are unusual is necessary to be able to do this efficiently and effectively.
    Manually assessing outliers that are flagged by detectors that examined many features
    can be especially difficult; on the other hand, outliers flagged by detectors
    using only a small number of features can be much more manageable to asses.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可解释性** — 结果可以更具可解释性，而可解释性通常是离群点检测中的一个关键问题。在离群点检测中，我们经常标记异常记录，认为它们可能是某种程度上的问题或关注点，通常这些记录会被手动检查。了解为什么它们不寻常是有效且高效地进行此操作所必需的。通过检测器标记的异常点，尤其是那些检查了许多特征的检测器，手动评估起来可能尤其困难；另一方面，通过只使用少数特征的检测器标记的异常点则更易于评估。'
- en: '**Faster systems** — Using fewer features allows us to create faster (and less
    memory-intensive) detectors. This can speed up both fitting and inference, particularly
    when working with detectors whose execution time is non-linear in the number of
    features (many detectors are, for example, quadratic in execution time based on
    the number of features). Depending on the detectors, using, say, 20 detectors,
    each covering 8 features, may actually execute faster than a single detector covering
    100 features.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更快的系统** — 使用更少的特征可以帮助我们创建更快（且占用内存更少）的检测器。这可以加速拟合和推理过程，尤其是在处理执行时间与特征数量呈非线性关系的检测器时（例如，许多检测器的执行时间与特征数量的平方成正比）。根据检测器的不同，使用例如20个检测器，每个覆盖8个特征，可能比使用一个覆盖100个特征的单一检测器执行得更快。'
- en: '**Execution in parallel** — Given that we use many small detectors instead
    of one large detector, it’s possible to execute both the fitting and the predicting
    steps in parallel, allowing for faster execution where there are the hardware
    resources to support this.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并行执行** — 由于我们使用许多小检测器而不是一个大型检测器，因此可以在硬件资源支持的情况下，将拟合和预测步骤并行执行，从而加快执行速度。'
- en: '**Ease of tuning over time** — Using many simple detectors creates a system
    that’s easier to tune over time. Very often with outlier detection, we’re simply
    evaluating a single dataset and wish just to identify the outliers in this. But
    it’s also very common to execute outlier detection systems on a long-running basis,
    for example, monitoring industrial processes, website activity, financial transactions,
    the data being input to machine learning systems or other software applications,
    the output of these systems, and so on. In these cases, we generally wish to improve
    the outlier detection system over time, allowing us to focus better on the more
    relevant outliers. Having a suite of simple detectors, each based on a small number
    of features, makes this much more manageable. It allows us to, over time, increase
    the weight of the more useful detectors and decrease the weight of the less useful
    detectors.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随时间调节的简易性** — 使用许多简单的检测器可以创建一个更易于随时间调节的系统。在离群点检测中，我们通常只是在评估一个单一的数据集，并希望识别其中的离群点。但通常也会定期执行离群点检测系统，例如监控工业过程、网站活动、金融交易、输入到机器学习系统或其他软件应用的数据、这些系统的输出等。在这些情况下，我们通常希望随着时间的推移改进离群点检测系统，使我们能够更好地关注更相关的离群点。拥有一组简单的检测器，每个检测器基于少量特征，可以使这项工作更具可管理性。这使得我们能够随着时间的推移，增加更有用的检测器的权重，减少不太有用的检测器的权重。'
- en: Choosing the subspaces
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择子空间
- en: As indicated, we will need, for each dataset evaluated, to determine the appropriate
    subspaces. It can, though, be difficult to find the relevant set of subspaces,
    or at least to find the optimal set of subspaces. That is, assuming we are interested
    in finding any unusual combinations of values, it can be difficult to know which
    sets of features will contain the most relevant of the unusual combinations.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们需要针对每个被评估的数据集，确定适当的子空间。然而，找到相关的子空间集合可能很困难，或者至少难以找到最优的子空间集合。也就是说，假设我们有兴趣找到任何异常的值组合，确定哪些特征集包含最相关的异常组合可能是困难的。
- en: As an example, if a dataset has 100 features, we may train 10 models, each covering
    10 features. We may use, say, the first 10 features for the first detector, the
    second set of 10 features for the second, and so on, If the first two features
    have some rows with anomalous combinations of values, we will detect this. But
    if there are anomalous combinations related to the first feature and any of the
    90 features not covered by the same model, we will miss these.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果一个数据集有100个特征，我们可以训练10个模型，每个模型覆盖10个特征。我们可以使用例如前10个特征作为第一个检测器，接下来的10个特征作为第二个检测器，依此类推。如果前两个特征存在一些具有异常值组合的行，我们将检测到这一点。但如果存在与第一个特征相关的异常值组合，而这些异常值组合与其余90个未包含在同一模型中的特征相关，我们将无法检测到这些。
- en: We can improve the odds of putting relevant features together by using many
    more subspaces, but it can be difficult to ensure all sets of features that should
    be together are actually together at least once, particularly where there are
    relevant outliers in the data that are based on three, four, or more features
    — which must appear together in at least one subspace to be detected. For example,
    in a table of staff expenses, you may wish to identify expenses for rare combinations
    of Department, Expense Type, and Amount. If so, these three features must appear
    together in at least one subspace.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用更多的子空间来提高将相关特征组合在一起的可能性，但要确保所有应该在一起的特征集至少出现一次是很困难的，特别是在数据中存在基于三个、四个或更多特征的相关异常值时——这些特征必须至少在一个子空间中一起出现才能被检测到。例如，在一个员工费用表格中，你可能希望识别部门、费用类型和金额的稀有组合。如果是这样，这三个特征必须至少在一个子空间中一起出现。
- en: So, we have the questions of how many features should be in each subspace, which
    features should go together, and how many subspaces to create.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们有以下几个问题：每个子空间中应该包含多少个特征，哪些特征应该放在一起，应该创建多少个子空间。
- en: There are a very large number of combinations to consider. If there are 20 features,
    there are ²²⁰ possible subspaces, which is just over a million. If there are 30
    features, there over a billion. If we decide ahead of time how many features will
    be in each subspace, the numbers of combinations decreases, but is still very
    large. If there are 20 features and we wish to use subspaces with 8 features each,
    there are 20 chose 8, or 125,970 combinations. If there are 30 features and we
    wish for subspaces with 7 features each, there are 30 chose 7, or 2,035,800 combinations.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 需要考虑的组合非常多。如果有20个特征，那么可能的子空间有²²⁰个，略多于一百万个。如果有30个特征，则有超过十亿个。如果我们提前决定每个子空间中包含多少个特征，组合的数量会减少，但仍然非常庞大。如果有20个特征，我们希望每个子空间有8个特征，则有20选8，即125,970个组合。如果有30个特征，我们希望每个子空间有7个特征，则有30选7，即2,035,800个组合。
- en: One approach we may wish to take is to keep the subspaces small, which allows
    for greater interpretability. The most interpretable option, using two features
    per subspace, also allows for simple visualization. However, if we have d features,
    we will need d*(d-1)/2 models to cover all combinations, which can be intractable.
    With 100 features, we would require 4,950 detectors. We usually need to use at
    least several features per detector, though not necessarily a large number.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能希望采取的一种方法是保持子空间较小，这样有助于提高可解释性。最具可解释性的选项是每个子空间使用两个特征，它还允许简单的可视化。然而，如果我们有d个特征，我们将需要d*(d-1)/2个模型来覆盖所有组合，这可能是不可行的。如果有100个特征，我们将需要4,950个检测器。我们通常需要每个检测器使用至少几个特征，但不一定是大量特征。
- en: We wish to use enough detectors, and enough features per detector, that each
    pair of features appears together ideally at least once, and few enough features
    per detector that the detectors have largely different features from each other.
    For example, if each detector used 90 out of the 100 features, we’d cover all
    combinations of features well, but the subspaces would still be quite large (undoing
    much of the benefit of using subspaces), and all the subspaces will be quite similar
    to each other (undoing much of the benefit of creating ensembles).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望使用足够的检测器，每个检测器使用足够的特征，使得每对特征理想情况下至少出现一次，并且每个检测器中的特征足够少，以便这些检测器之间具有显著不同的特征。例如，如果每个检测器使用100个特征中的90个，我们将很好地覆盖所有特征组合，但子空间仍然会相当大（从而抵消使用子空间的很多好处），而且所有子空间也会彼此非常相似（从而抵消创建集成方法的很多好处）。
- en: 'While the number of features per subspace requires balancing these concerns,
    the number of subspaces created is a bit more straightforward: in terms of accuracy,
    using more subspaces is strictly better, but is computationally more expensive.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然每个子空间中使用的特征数量需要平衡这些问题，但创建的子空间数量则稍微直观一些：从准确性角度看，使用更多子空间是绝对更好的，但计算开销更大。
- en: There are a few broad approaches to finding useful subspaces. I list these here
    quickly, then look at some in more detail below.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找有用子空间的方法有几种广泛的策略。我在这里快速列出这些方法，然后在下面详细讨论一些。
- en: '**Based on domain knowledge** — Here we consider which sets of features could
    potentially have combinations of values we would consider noteworthy.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于领域知识**——在这里我们考虑哪些特征组合可能具有我们认为值得注意的值组合。'
- en: '**Based on associations** — Unusual combinations of values are only possible
    if a set of features are associated in some way. In prediction problems, we often
    wish to minimize the correlations between features, but with outlier detection,
    these are the features that are most useful to consider together. The features
    with the strongest associations will have the most meaningful outliers if there
    are exceptions to the normal patterns.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于关联** — 只有当一组特征以某种方式关联时，才可能出现值的异常组合。在预测问题中，我们通常希望最小化特征之间的相关性，但在异常值检测中，这些特征是最有用的，应该一起考虑。如果有异常情况出现，具有最强关联的特征将包含最有意义的异常值。'
- en: '**Based on finding very sparse regions** — Records are typically considered
    as outliers if they are unlike most other records in the data, which implies they
    are located in sparse regions of the data. Therefore, useful subspaces can be
    found as those that contain large, nearly-empty regions.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于找到非常稀疏的区域** — 如果记录与数据中的大多数其他记录不同，通常会被视为异常值，这意味着它们位于数据的稀疏区域。因此，可以将包含大且几乎为空的区域的子空间视为有用的子空间。'
- en: '**Randomly** — This is the method used by a technique shown later called *FeatureBagging*
    and, while it can be suboptimal, it avoids the expensive searches for associations
    and sparse regions, and can work reasonably well where many subspaces are used.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机** — 这是稍后将介绍的一种技术，称为*FeatureBagging*，虽然它可能不是最优的，但它避免了昂贵的关联搜索和稀疏区域搜索，并且在使用多个子空间的情况下，能够较好地工作。'
- en: '**Exhaustive searches** — This is the method employed by [Counts Outlier Detector](https://medium.com/towards-data-science/counts-outlier-detector-interpretable-outlier-detection-ead0d469557a).
    This is limited to subspaces with small numbers of features, but the results are
    highly interpretable. It also avoids any computation, or biases, associated with
    selecting only a subset of the possible subspaces.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**穷举搜索** — 这是[Counts Outlier Detector](https://medium.com/towards-data-science/counts-outlier-detector-interpretable-outlier-detection-ead0d469557a)使用的方法。这仅限于具有少量特征的子空间，但结果是高度可解释的。它还避免了与选择可能子空间的子集相关的任何计算或偏差。'
- en: '**Using the features related to any known outliers** — If we have a set of
    known outliers, can identify why they are outliers (the relevant features), and
    are in a situation where we do not wish to identify unknown outliers (only these
    specific outliers), then we can take advantage of this and identify the sets of
    features relevant for each known outlier, and construct models for the various
    sets of features required.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用与已知异常值相关的特征** — 如果我们有一组已知的异常值，并且能够确定它们为何是异常值（相关特征），并且我们不希望识别未知的异常值（仅识别这些特定的异常值），那么我们可以利用这一点，确定与每个已知异常值相关的特征集，并为所需的各个特征集构建模型。'
- en: We’ll look at a few of these next in a little more detail.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将更详细地探讨其中的一些内容。
- en: '**Domain knowledge**'
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**领域知识**'
- en: Let’s take the example of a dataset, specifically an expenses table, shown below.
    If examining this table, we may be able to determine the types of outliers we
    would and would not be interested in. Unusual combinations of Account and Amount,
    as well as unusual combinations of Department and Account, may be of interest;
    whereas Date of Expense and Time would likely not be a useful combination. We
    can continue in this way, creating a small number of subspaces, each with likely
    two, three, or four features, which can allow for very efficient and interpretable
    outlier detection, flagging the most relevant outliers.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以一个数据集为例，特别是下面展示的开销表。如果我们查看此表格，我们可能能够确定我们感兴趣或不感兴趣的异常值类型。帐户和金额的异常组合，以及部门和帐户的异常组合，可能会引起我们的兴趣；而费用日期和时间的组合可能不会是有用的组合。我们可以继续以这种方式进行，创建少数几个子空间，每个子空间通常包含两个、三个或四个特征，这可以实现非常高效且可解释的异常值检测，标记出最相关的异常值。
- en: '![](../Images/b937a9fb973ad691e3cc5eec0b4e1cfd.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b937a9fb973ad691e3cc5eec0b4e1cfd.png)'
- en: Expenses table
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 开销表
- en: This can miss cases where we have an association in the data, though the association
    is not obvious. So, as well as taking advantage of domain knowledge, it may be
    worth searching the data for associations. We can discover relationships among
    the features, for example, testing where features can be predicted accurately
    from the other features using simple predictive models. Where we find such associations,
    these can be worth investigating.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可能会遗漏数据中存在某些关联的情况，尽管这种关联并不显而易见。因此，除了利用领域知识外，搜索数据中的关联也许是值得的。例如，我们可以发现特征之间的关系，例如，测试是否可以通过其他特征使用简单的预测模型准确预测某些特征。如果发现了这样的关联，它们值得进一步调查。
- en: Discovering these associations, though, may be useful for some purposes, but
    may or may not be useful for the outlier detection process. If there is, for example,
    a relationship between accounts and the time of the day, this may simply be due
    to the process people happen to typically use to submit their expenses, and it
    may be that deviations from this are of interest, but more likely they are not.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，发现这些关联可能对某些目的有用，但可能对异常值检测过程有用也可能无用。例如，如果账户和时间之间存在关系，这可能仅仅是因为人们通常使用的报销流程，而这种偏差可能是值得关注的，但更可能并不重要。
- en: Random feature subspaces
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机特征子空间
- en: Creating subspaces randomly can be effective if there is no domain knowledge
    to draw on. This is fast and can create a set of subspaces that will tend to catch
    the strongest outliers, though it can miss some important outliers too.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有领域知识可以借鉴，随机创建子空间可能是有效的。这种方法快速且能够创建一组子空间，这些子空间往往能够捕捉到最强的异常值，尽管它也可能遗漏一些重要的异常值。
- en: The code below provides an example of one method to create a set of random subspaces.
    This example uses a set of eight features, named A through H, and creates a set
    of subspaces of these.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码提供了创建一组随机子空间的示例。这个示例使用了一组八个特征，命名为A到H，并基于这些特征创建子空间。
- en: Each subspace starts by selecting the feature that is so far the least-used
    (if there is a tie, one is selected randomly). It uses a variable called *ft_used_counts*
    to track this. It then adds features to this subspace one at a time, each step
    selecting the feature that has appeared in other subspaces the least often with
    the features so far in the subspace. It uses a feature called *ft_pair_mtx* to
    track how many subspaces each pair of features have appeared in together so far.
    Doing this, we create a set of subspaces that matches each pair of features roughly
    equally often.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 每个子空间首先选择目前使用最少的特征（如果有平局，则随机选择一个）。它使用一个名为*ft_used_counts*的变量来跟踪这一点。接着，它会逐一添加特征到这个子空间，每一步选择在当前子空间中与其他子空间中出现次数最少的特征。它使用一个名为*ft_pair_mtx*的特征来跟踪每对特征已经一起出现在多少个子空间中。通过这种方式，我们创建了一组子空间，使得每对特征大致相同的频率出现在子空间中。
- en: '[PRE0]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Normally we would create many more base detectors (each subspace often corresponds
    to one base detector, though we can also run multiple base detectors on each subspace)
    than we do in this example, but this uses just four to keep things simple. This
    will output the following subspaces:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们会创建更多的基础检测器（每个子空间通常对应一个基础检测器，尽管我们也可以在每个子空间上运行多个基础检测器），但是为了简化，本示例仅使用了四个。这将输出以下子空间：
- en: '[PRE1]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The code here will create the subspaces such that all have the same number of
    features. There is also an advantage in having the subspaces cover different numbers
    of features, as this can introduce some more diversity (which is important when
    creating ensembles), but there is strong diversity in any case from using different
    features (so long as each uses a relatively small number of features, such that
    the subspaces are largely different features).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的代码将创建具有相同特征数量的子空间。让子空间覆盖不同数量的特征也有一个好处，因为这可以引入更多的多样性（这在创建集成模型时非常重要），但无论如何，通过使用不同的特征已经能够提供强大的多样性（只要每个子空间使用相对较少的特征，从而使得子空间大体上具有不同的特征）。
- en: Having the same number of features has a couple benefits. It simplifies tuning
    the models, as many parameters used by outlier detectors depend on the number
    of features. If all subspaces have the same number of features, they can also
    use the same parameters.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有相同数量的特征有几个好处。它简化了模型的调优，因为许多异常值检测器使用的参数依赖于特征的数量。如果所有子空间都有相同数量的特征，它们也可以使用相同的参数。
- en: It also simplifies combining the scores, as the detectors will be more comparable
    to each other. If using different numbers of features, this can produce scores
    that are on different scales, and not easily comparable. For example, with k-Nearest
    Neighbors (KNN), we expect greater distances between neighbors if there are more
    features.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 它还简化了分数组合，因为检测器之间的可比性更强。如果使用不同数量的特征，可能会生成在不同尺度上的分数，难以进行比较。例如，使用 k-最近邻（KNN）时，如果特征更多，我们预期邻居之间的距离会更大。
- en: Feature subspaces based on correlations
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于相关性的特征子空间
- en: Everything else equal, in creating the subspaces, it’s useful to keep associated
    features together as much as possible. In the code below, we provide an example
    of code to select subspaces based on correlations.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建子空间时，其他条件相同的情况下，尽可能将关联特征放在一起是非常有用的。在下面的代码中，我们提供了一个示例，展示如何根据相关性选择子空间。
- en: There are several ways to test for associations. We can create predictive models
    to attempt to predict each feature from each other single feature (this will capture
    even relatively complex relationships between features). With numeric features,
    the simplest method is likely to check for Spearman correlations, which will miss
    nonmonotonic relationships, but will detect most strong relationships. This is
    what is used in the code example below.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 测试关联性有几种方法。我们可以创建预测模型，尝试从每个特征中预测其他单一特征（这将捕捉到特征之间甚至是相对复杂的关系）。对于数值特征，最简单的方法可能是检查斯皮尔曼相关性，这种方法虽然无法检测到非单调关系，但能发现大多数强关联关系。以下代码示例使用的就是这种方法。
- en: To execute the code, we first specify the number of subspaces desired and the
    number of features in each.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行代码，我们首先指定所需的子空间数量以及每个子空间中的特征数量。
- en: This executes by first finding all pairwise correlations between the features
    and storing this in a matrix. We then create the first subspace, starting by finding
    the largest correlation in the correlation matrix (this adds two features to this
    subspace) and then looping over the number of other features to be added to this
    subspace. For each, we take the largest correlation in the correlation matrix
    for any pair of features, such that one feature is currently in the subspace and
    one is not. Once this subspace has a sufficient number of features, we create
    the next subspace, taking the largest correlation remaining in the correlation
    matrix, and so on.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法首先通过寻找特征之间的所有配对相关性并将其存储在一个矩阵中来执行。然后我们创建第一个子空间，方法是首先找到相关矩阵中的最大相关性（这会将两个特征添加到该子空间），然后遍历待添加到该子空间的其他特征数量。对于每个特征，我们从相关矩阵中选择一对特征，要求其中一个特征已经在子空间内，而另一个特征还未在子空间内。一旦该子空间包含足够的特征，我们就创建下一个子空间，继续选择相关矩阵中剩余的最大相关性，以此类推。
- en: For this example, we use a real dataset, the [baseball](https://www.openml.org/search?type=data&sort=version&status=any&order=asc&exact_name=baseball&id=185)
    dataset from OpenML (available with a public license). The dataset turns out to
    contain some large correlations. The correlation, for example, between At bats
    and Runs is 0.94, indicating that any values that deviate significantly from this
    pattern would likely be outliers.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用一个真实数据集——[baseball](https://www.openml.org/search?type=data&sort=version&status=any&order=asc&exact_name=baseball&id=185)数据集，来自OpenML（该数据集具有公开许可）。这个数据集包含了一些较强的相关性。例如，击球次数（At
    bats）和得分（Runs）之间的相关性为0.94，这意味着任何显著偏离这一模式的值很可能是异常值。
- en: '[PRE2]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This produces:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生：
- en: '[PRE3]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: PyOD
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyOD
- en: '[PyOD](https://github.com/yzhao062/pyod) is likely the most comprehensive and
    well-used tool for outlier detection on numeric tabular data available in Python
    today. It includes a large number of detectors, ranging from very simple to very
    complex — including several deep learning-based methods.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[PyOD](https://github.com/yzhao062/pyod)可能是目前Python中最全面、最常用的数值表格数据异常值检测工具。它包括大量的检测器，涵盖从非常简单到非常复杂的多种方法——其中包括几种基于深度学习的方法。'
- en: Now that we have an idea of how subspaces work with outlier detection, we’ll
    look at two tools provided by PyOD that work with subspaces, called SOD and FeatureBagging.
    Both of these tools identify a set of subspaces, execute a detector on each subspace,
    and combine the results for a single score for each record.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了子空间在异常值检测中的作用，接下来我们将看看PyOD提供的两个处理子空间的工具：SOD和FeatureBagging。两个工具都会识别一组子空间，对每个子空间执行检测器，并将结果合并为每条记录的单一分数。
- en: Whether using subspaces or not, it’s necessary to determine what base detectors
    to use. If not using subspaces, we would select one or more detectors and run
    these on the full dataset. And, if we are using subspaces, we again select one
    or more detectors, here running these on each subspace. As indicated above, LOF
    and KNN can be reasonable choices, but PyOD provides a number of others as well
    that can work well if executed on each subspace, including, for example, Angle-based
    Outlier Detector (ABOD), models based on Gaussian Mixture Models (GMMs), Kernel
    Density Estimations (KDE), and several others. Other detectors, provided outside
    PyOD can work very effectively as well.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是否使用子空间，都必须确定使用什么基础检测器。如果不使用子空间，我们将选择一个或多个检测器，并在整个数据集上运行它们。如果使用子空间，我们同样选择一个或多个检测器，并在每个子空间上运行这些检测器。如上所述，LOF和KNN可以是合理的选择，但PyOD还提供了许多其他的选择，这些选择在每个子空间上执行时也能很好地工作，例如基于角度的离群点检测器（ABOD）、基于高斯混合模型（GMM）的模型、核密度估计（KDE）等。PyOD之外的其他检测器也能非常有效地工作。
- en: SOD (Subspace Outlier Detection)
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SOD（子空间离群点检测）
- en: SOD was designed specifically to handle situations such as shown in the scatter
    plots above. SOD works, similar to KNN and LOF, by identifying a neighborhood
    of k neighbors for each point, known as the *reference set*. The reference set
    is found in a different way, though, using a method called shared nearest neighbors
    (SNN).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: SOD（子空间离群点检测）专门设计来处理如上所示的情况。SOD的工作方式类似于KNN和LOF，通过为每个点识别k个邻居的邻域，这些邻居被称为*参考集*。不过，参考集是通过不同的方式找到的，使用的是一种叫做共享最近邻（SNN）的方法。
- en: 'Shared nearest neighbors are described thoroughly in [this article](/shared-nearest-neighbors-a-more-robust-distance-metric-064d7f99ffb7),
    but the general idea is that if two points are generated by the same mechanism,
    they will tend to not only be close, but also to have many of the same neighbors.
    And so, the similarity of any two records can be measured by the number of shared
    neighbors they have. Given this, neighborhoods can be identified by using not
    only the sets of points with the smallest Euclidean distances between them (as
    KNN and LOF do), but the points with the most shared neighbors. This tends to
    be robust even in high dimensions and even where there are many irrelevant features:
    the rank order of neighbors tends to remain meaningful even in these cases, and
    so the set of nearest neighbors can be reliably found even where specific distances
    cannot.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 共享最近邻在[这篇文章](/shared-nearest-neighbors-a-more-robust-distance-metric-064d7f99ffb7)中有详细描述，但一般来说，如果两个点是由相同机制生成的，它们不仅会接近，而且往往会有许多相同的邻居。因此，任何两个记录的相似度可以通过它们共享的邻居数量来衡量。基于这一点，邻域可以通过使用不仅是欧几里得距离最小的点集（如KNN和LOF方法所做的），还可以使用共享最多邻居的点来识别。这种方法即使在高维空间中，甚至在有许多无关特征的情况下，也能保持鲁棒性：即使在这些情况下，邻居的排名顺序仍然保持有意义，因此即使在无法计算具体距离的情况下，最近邻集仍然能够可靠地找到。
- en: Once we have the reference set, we use this to determine the subspace, which
    here is the set of features that explain the greatest amount of variance for the
    reference set. Once we identify these subspaces, SOD examines the distances of
    each point to the data center.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们得到了参考集，就可以用它来确定子空间，这里指的是解释参考集方差最大的一组特征。一旦识别出这些子空间，SOD就会检查每个点到数据中心的距离。
- en: 'I provide a quick example using SOD below. This assumes pyod has been installed,
    which requires running:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我提供了一个使用SOD的快速示例。假设已经安装了pyod，安装步骤如下：
- en: '[PRE4]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We’ll use, as an example, a synthetic dataset, which allows us to experiment
    with the data and model hyperparameters to get a better sense of the strengths
    and limitations of each detector. The code here provides an example of working
    with 35 features, where two features (features 8 and 9) are correlated and the
    other features are irrelevant. A single outlier is created as an unusual combination
    of the two correlated features.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个合成数据集作为示例，这使我们可以通过实验数据和模型超参数，更好地理解每个检测器的优缺点。这里的代码提供了一个处理35个特征的示例，其中两个特征（特征8和9）是相关的，其他特征是无关的。一个单独的离群点是通过这两个相关特征的不寻常组合创建的。
- en: SOD is able to identify the one known outlier as the top outlier. I set the
    contamination rate to 0.01 to specify to return (given there are 100 records)
    only a single outlier. Testing this beyond 35 features, though, SOD scores this
    point much lower. This example specifies the size of the reference set to be 3;
    different results may be seen with different values.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: SOD 能够将已知的一个异常点识别为最显著的异常点。我将污染率设置为 0.01，以指定返回（假设有 100 条记录）仅一个异常点。然而，当测试超过 35
    个特征时，SOD 将此点的得分大大降低。此示例将参考集的大小指定为 3；使用不同的值可能会得到不同的结果。
- en: '[PRE5]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We display four scatterplots below, showing four pairs of the 35 features. The
    known outlier is shown as a star in each of these. We can see features 8 and 9
    (the two relevant features) in the second pane, and we can see the point is a
    clear outlier, though it is typical in all other dimensions.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们下面展示了四个散点图，显示了 35 个特征中的四对特征。在每个图中，已知的异常点被标记为星号。我们可以在第二个面板中看到特征 8 和 9（这两个相关特征），并且可以看到该点明显是一个异常点，尽管它在所有其他维度中是典型的。
- en: '![](../Images/5a3159315c07d883e72c50cda17ea3e0.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a3159315c07d883e72c50cda17ea3e0.png)'
- en: Testing SOD with 35-dimensional data. One outlier was inserted into the data
    and can be seen clearly in the second pane for features 8 and 9\. Although the
    point is typical otherwise, it is flagged as the top outlier by SOD. The third
    pane also includes feature 9, and we can see the point is somewhat unusual here,
    though no more so than many other points in other dimensions. The relationship
    in features 8 and 9 is the most relevant, and SOD appears to detect this
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 测试 SOD 在 35 维数据上的表现。数据中插入了一个异常点，并且可以在第二个面板中清楚地看到特征 8 和 9。尽管该点在其他维度中是典型的，它还是被
    SOD 标记为最显著的异常点。第三个面板也包含特征 9，我们可以看到该点在这里有些不同寻常，尽管与其他维度中的许多点相比并无太大区别。特征 8 和 9 之间的关系最为相关，而
    SOD 似乎能够检测到这一点。
- en: FeatureBagging
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FeatureBagging
- en: FeatureBagging was designed to solve the same problem as SOD, though takes a
    different approach to determining the subspaces. It creates the subspaces completely
    randomly (so slightly differently than the example above, which keeps a record
    of how often each pair of features are placed in a subspace together and attempts
    to balance this). It also subsamples the rows for each base detector, which provides
    a little more diversity between the detectors.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: FeatureBagging 的设计目的是解决与 SOD 相同的问题，尽管它采用了不同的方法来确定子空间。它完全随机地创建子空间（与上面的示例略有不同，后者会记录每对特征一起被放入子空间的频率，并尝试平衡这一点）。它还会对每个基本检测器进行行采样，从而在检测器之间提供更多的多样性。
- en: 'A specified number of base detectors are used (10 by default, though it is
    preferable to use more), each of which selects a random set of rows and features.
    For each, the maximum number of features that may be selected is specified as
    a parameter, defaulting to all. So, for each base detector, FeatureBagging:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 使用指定数量的基本检测器（默认值为 10，尽管使用更多的检测器会更好），每个检测器都会选择一组随机的行和特征。对于每个检测器，可以选择的最大特征数被指定为一个参数，默认为所有特征。因此，对于每个基本检测器，FeatureBagging
    会：
- en: Determines the number of features to use, up to the specified maximum.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定要使用的特征数量，直到达到指定的最大值。
- en: Chooses this many features randomly.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机选择如此多的特征。
- en: Chooses a set of rows randomly. This is a bootstrap sample of the same size
    as the number of rows.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机选择一组行。这是一个与行数相同大小的自助样本。
- en: Creates an LOF detector (by default; other base detectors may be used) to evaluate
    the subspace.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个 LOF 检测器（默认为此；也可以使用其他基本检测器）来评估子空间。
- en: 'Once this is complete, each row will have been scored by each base detector
    and the scores must then be combined into a single, final score for each row.
    PyOD’s FeatureBagging provides two options for this: using the maximum score and
    using the mean score.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成，每一行将通过每个基本检测器进行评分，然后必须将这些评分合并为每一行的单一最终评分。PyOD 的 FeatureBagging 提供了两种合并评分的选项：使用最大评分和使用平均评分。
- en: As we saw in the scatter plots above, points can be strong outliers in some
    subspaces and not in others, and averaging in their scores from the subspaces
    where they are typical can water down their scores and defeat the benefit of using
    subspaces. In other forms of ensembling with outlier detection, using the mean
    can work well, but when working with multiple subspaces, using the maximum will
    typically be the better of the two options. Doing that, we give each record a
    score based on the subspace where it was most unusual. This isn’t perfect either,
    and there can be better options, but using the maximum is simple and is almost
    always preferable to the mean.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上面的散点图中所看到的，某些子空间中的点可能是强异常值，而在其他子空间中则不是，从这些子空间中取平均分数，可能会稀释它们的得分，进而削弱使用子空间的好处。在其他异常值检测的集成方法中，使用均值通常有效，但在处理多个子空间时，使用最大值通常是两个选项中更好的。这样，我们根据记录在最异常的子空间中的得分来给每个记录打分。这也不是完美的，可能还有更好的选择，但使用最大值既简单又几乎总是比均值更可取。
- en: Any detector can be used within the subspaces. PyOD uses LOF by default, as
    did the original paper describing FeatureBagging. LOF is a strong detector and
    a sensible choice, though you may find better results with other base detectors.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 任何检测器都可以在子空间中使用。PyOD默认使用LOF，就像最初描述FeatureBagging的论文一样。LOF是一个强大的检测器，是一个合理的选择，尽管你可能会发现使用其他基础检测器能得到更好的结果。
- en: In the original paper, subspaces are created randomly, each using between d/2
    and d — 1 features, where d is the total number of features. Some researchers
    have pointed out that the number of features used in the original paper is likely
    much larger than is appropriate.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始论文中，子空间是随机创建的，每个子空间使用d/2到d-1之间的特征，其中d是特征的总数。一些研究人员指出，原始论文中使用的特征数量可能远大于适当的数量。
- en: If the full number of features is large, using over half the features at once
    will allow the curse of dimensionality to take effect. And using many features
    in each detector will result in the detectors being correlated with each other
    (for example, if all base detectors use 90% of the features, they will use roughly
    the same features and tend to score each record roughly the same), which can also
    remove much of the benefit of creating ensembles.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果特征的总数较大，同时使用超过一半的特征将使维度灾难生效。而且，在每个检测器中使用许多特征将导致检测器之间的相关性（例如，如果所有基础检测器都使用90%的特征，它们将使用大致相同的特征，并且倾向于为每条记录打上相似的分数），这也可能会消除创建集成模型的许多好处。
- en: PyOD allows setting the number of features used in each subspace, and it should
    be typically set fairly low, with a large number of base estimators created.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: PyOD允许设置每个子空间中使用的特征数量，通常应该设置得比较低，并创建大量的基础估计器。
- en: Using other detectors
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用其他检测器
- en: In this article we’ve looked at subspaces as a way to improve outlier detection
    in a number of ways, including reducing the curse of dimensionality, increasing
    interpretability, allowing parallel execution, allowing easier tuning over time,
    and so on. Each of these are important considerations, and using subspaces is
    often very helpful.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们探讨了子空间作为改善异常值检测的一种方式，包括减少维度灾难、提高可解释性、支持并行执行、简化时间上的调优等。每个因素都是重要的考虑因素，使用子空间通常非常有帮助。
- en: There are, though, often other approaches as well that can be used for these
    purposes, sometimes as alternatives, and sometimes in combination of with the
    use of subspaces. For example, to improve interpretability, its important to,
    as much as possible, select model types that are inherently interpretable (for
    example univariate tests such as z-score tests, Counts Outlier Detector, or a
    detector provided by PyOD called ECOD).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，通常还有其他方法可以用于这些目的，有时作为替代方案，有时与使用子空间结合使用。例如，为了提高可解释性，重要的是尽可能选择本身具有可解释性的模型类型（例如，单变量测试，如z分数测试、计数异常值检测器，或PyOD提供的一个名为ECOD的检测器）。
- en: Where the main interest is in reducing the curse of dimensionality, here again,
    it can be useful to look at model types that scale to many features well, for
    instance Isolation Forest or Counts Outlier Detector. It can also be useful to
    look at executing univariate tests, or applying [PCA](https://medium.com/towards-data-science/a-simple-example-using-pca-for-outlier-detection-ab2773b98e4a).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 当主要关注点是减少维度灾难时，在这里再次，查看那些在许多特征上表现良好的模型类型可能会很有用，例如隔离森林（Isolation Forest）或计数异常值检测器（Counts
    Outlier Detector）。此外，执行单变量测试或应用[PCA](https://medium.com/towards-data-science/a-simple-example-using-pca-for-outlier-detection-ab2773b98e4a)也是有益的。
- en: Ongoing outlier detection projects
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正在进行的异常值检测项目
- en: One thing to be aware of when constructing subspaces, if they are formed based
    on correlations, or on sparse regions, is that the relevant subspaces may change
    over time as the data changes. New associations may emerge between features and
    new sparse regions may form that will be useful for identifying outliers, though
    these will be missed if the subspaces are not recalculated from time to time.
    Finding the relevant subspaces in these ways can be quite effective, but they
    may need to to be updated on some schedule, or where the data is known to have
    changed.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建子空间时需要注意的一点是，如果子空间是基于相关性或稀疏区域形成的，那么随着数据的变化，相关的子空间可能会发生变化。随着新特征之间的关联出现或新的稀疏区域形成，这些区域可能对识别离群点有用，但如果不定期重新计算子空间，可能会错过这些信息。通过这种方式找到相关的子空间可以非常有效，但可能需要在某些时间表上进行更新，或者当已知数据发生变化时进行更新。
- en: Conclusions
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: It’s common with outlier detection projects on tabular data for it to be worth
    looking at using subspaces, particularly where we have many features. Using subspaces
    is a relatively straightforward technique with a number of noteworthy advantages.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在表格数据的离群点检测项目中，通常值得考虑使用子空间，特别是在我们有许多特征的情况下。使用子空间是一种相对简单的技术，并具有许多显著的优势。
- en: Where you face issues related to large data volumes, execution times, or memory
    limits, using [PCA](https://medium.com/towards-data-science/a-simple-example-using-pca-for-outlier-detection-ab2773b98e4a)
    may also be a useful technique, and may work better in some cases than creating
    sub-spaces, though working with sub-spaces (and so, working with the original
    features, and not the components created by PCA) can be substantially more interpretable,
    and interpretability is often quite important with outlier detection.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在面临大数据量、执行时间或内存限制等问题时，使用[PCA](https://medium.com/towards-data-science/a-simple-example-using-pca-for-outlier-detection-ab2773b98e4a)也可能是一种有用的技术，并且在某些情况下可能比创建子空间更有效，尽管使用子空间（因此，使用原始特征而不是PCA生成的组件）通常更具可解释性，而可解释性在离群点检测中往往非常重要。
- en: 'Subspaces can be used in combination with other techniques to improve outlier
    detection. As an example, using subspaces can be combined with other ways to create
    ensembles: it’s possible to create larger ensembles using both subspaces (where
    different detectors in the ensemble use different features) as well as different
    model types, different training rows, different pre-processing, and so on. This
    can provide some further benefits, though with some increase in computation as
    well.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 子空间可以与其他技术结合使用，以提高离群点检测的效果。例如，使用子空间可以与其他方法结合来创建集成：通过子空间（在集成中的不同检测器使用不同的特征）以及不同的模型类型、不同的训练数据、不同的预处理等，能够创建更大的集成。这可以带来一些额外的好处，但也会增加计算量。
- en: All images by author
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 所有图片均由作者提供
