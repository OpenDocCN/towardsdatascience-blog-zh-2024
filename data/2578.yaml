- en: 'Why Scaling Works: Inductive Biases vs The Bitter Lesson'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么规模化有效：归纳偏差与痛苦的教训
- en: 原文：[https://towardsdatascience.com/why-scaling-works-inductive-biases-vs-the-bitter-lesson-9c2782f99b18?source=collection_archive---------8-----------------------#2024-10-22](https://towardsdatascience.com/why-scaling-works-inductive-biases-vs-the-bitter-lesson-9c2782f99b18?source=collection_archive---------8-----------------------#2024-10-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/why-scaling-works-inductive-biases-vs-the-bitter-lesson-9c2782f99b18?source=collection_archive---------8-----------------------#2024-10-22](https://towardsdatascience.com/why-scaling-works-inductive-biases-vs-the-bitter-lesson-9c2782f99b18?source=collection_archive---------8-----------------------#2024-10-22)
- en: Building deep insights with a toy problem
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用一个玩具问题构建深刻的洞见
- en: '[](https://medium.com/@TarikDzekman?source=post_page---byline--9c2782f99b18--------------------------------)[![Tarik
    Dzekman](../Images/0c66b22ecbdbbce79b2516e555c67432.png)](https://medium.com/@TarikDzekman?source=post_page---byline--9c2782f99b18--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9c2782f99b18--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9c2782f99b18--------------------------------)
    [Tarik Dzekman](https://medium.com/@TarikDzekman?source=post_page---byline--9c2782f99b18--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@TarikDzekman?source=post_page---byline--9c2782f99b18--------------------------------)[![Tarik
    Dzekman](../Images/0c66b22ecbdbbce79b2516e555c67432.png)](https://medium.com/@TarikDzekman?source=post_page---byline--9c2782f99b18--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9c2782f99b18--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9c2782f99b18--------------------------------)
    [Tarik Dzekman](https://medium.com/@TarikDzekman?source=post_page---byline--9c2782f99b18--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9c2782f99b18--------------------------------)
    ·10 min read·Oct 22, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9c2782f99b18--------------------------------)
    ·10分钟阅读·2024年10月22日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/0183bdb92e1fdfde53b87a0823b4b64a.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0183bdb92e1fdfde53b87a0823b4b64a.png)'
- en: 'Source: All images by the author'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：所有图片由作者提供
- en: Over the past decade we’ve witnessed the power of scaling deep learning models.
    Larger models, trained on heaps of data, consistently outperform previous methods
    in language modelling, image generation, playing games, and even protein folding.
    To understand why scaling works, let’s look at a toy problem.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的十年里，我们见证了深度学习模型规模化的强大力量。大型模型通过在海量数据上训练，持续在语言建模、图像生成、游戏对弈甚至蛋白质折叠等任务中超越了以往的方法。为了理解为什么规模化有效，我们来看看一个玩具问题。
- en: Introducing a Toy Problem
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入一个玩具问题
- en: 'We start with a 1D manifold weaving its way through the 2D plane and forming
    a spiral:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个一维流形开始，它穿越二维平面并形成一个螺旋：
- en: '![](../Images/51d4cee4cf66501ec6a0feaeb883bc77.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51d4cee4cf66501ec6a0feaeb883bc77.png)'
- en: 'Now we add a heatmap which represents the probability density of sampling a
    particular 2D point. Notably, this probability density is *independent* of the
    shape of the manifold:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们添加一个热图，表示采样特定二维点的概率密度。值得注意的是，这个概率密度是*独立*于流形的形状的：
- en: '![](../Images/9b3c3785c731f0e4ca60a264159b21fa.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b3c3785c731f0e4ca60a264159b21fa.png)'
- en: 'Let’s assume that the data on either side of the manifold is always completely
    separable (i.e. there is no noise). Datapoints on the outside of the manifold
    are blue and those on the inside are orange. If we draw a sample of N=1000 points
    it may look like this:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 假设流形两侧的数据总是完全可分离的（即没有噪声）。流形外部的数据点为蓝色，内部的数据点为橙色。如果我们抽取N=1000个数据点，它可能看起来是这样的：
- en: '![](../Images/542fc4ff7a0dabbe6f44e7f2e8e6b28c.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/542fc4ff7a0dabbe6f44e7f2e8e6b28c.png)'
- en: 'Toy problem: How do we build a model which predicts the colour of a point based
    on its 2D coordinates?'
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 玩具问题：我们如何构建一个基于二维坐标预测点的颜色的模型？
- en: In the real world we often can’t sample uniformly from all parts of the feature
    space. For example, in image classification it’s easy to find images of trees
    in general but less easy to find many examples of specific trees. As a result,
    it may be harder for a model to learn the difference between species there aren’t
    many examples of. Similarly, in our toy problem, different parts of the space
    will become difficult to predict simply because they are harder to sample.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，我们通常不能从特征空间的所有部分均匀地采样。例如，在图像分类中，找到树的图像相对容易，但要找到许多特定树种的例子则较为困难。因此，模型可能会更难学会区分那些没有很多例子的物种。类似地，在我们的玩具问题中，空间的不同部分将变得难以预测，仅仅因为它们更难采样。
- en: Solving the Toy Problem
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决玩具问题
- en: 'First, we build a simple neural network with 3 layers, running for 1000 epochs.
    The neural network’s predictions are heavily influenced by the particulars of
    the sample. As a result, the trained model has difficulty inferring the shape
    of the manifold just because of sampling sparsity:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们构建一个简单的三层神经网络，训练1000个epoch。神经网络的预测受到样本特性的强烈影响。因此，训练后的模型很难推断出流形的形状，仅仅因为采样的稀疏性：
- en: '![](../Images/1d1f434d9dde81d62fe6b51f6268f37c.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d1f434d9dde81d62fe6b51f6268f37c.png)'
- en: Even knowing that the points are completely separable, there are infinitely
    many ways to draw a boundary around the sampled points. Based on the sample data,
    why should any one boundary be considered superior to another?
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 即使知道这些点是完全可分的，也有无数种方式可以围绕采样点绘制边界。基于样本数据，为什么应该认为某一边界比其他边界更优？
- en: With regularisation techniques we could encourage the model to produce a smoother
    boundary rather than curving tightly around predicted points. That helps to an
    extent but it won’t solve our problem in regions of sparsity.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 使用正则化技术，我们可以鼓励模型生成更平滑的边界，而不是紧密围绕预测点弯曲。这在一定程度上有所帮助，但在稀疏区域无法解决我们的問題。
- en: Since we already know the manifold is a spiral, can we encourage the model to
    make spiral-like predictions?
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 既然我们已经知道流形是一个螺旋形，我们能否鼓励模型做出螺旋形的预测？
- en: 'We can add what’s called an “[inductive prior](https://en.wikipedia.org/wiki/Inductive_bias)”:
    something we put in the model architecture or the training process which contains
    information about the problem space. In this toy problem we can do some [feature
    engineering](https://en.wikipedia.org/wiki/Feature_engineering) and adjust the
    way we present inputs to the model. Instead of 2D (x, y) coordinates, we transform
    the input into [polar coordinates](https://en.wikipedia.org/wiki/Polar_coordinate_system)
    (r, θ).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以添加一个叫做“[归纳先验](https://en.wikipedia.org/wiki/Inductive_bias)”的东西：我们在模型架构或训练过程中加入的，包含有关问题空间的信息。在这个玩具问题中，我们可以做一些[特征工程](https://en.wikipedia.org/wiki/Feature_engineering)，调整我们呈现给模型的输入方式。我们将输入从二维（x，y）坐标转换为[极坐标](https://en.wikipedia.org/wiki/Polar_coordinate_system)（r，θ）。
- en: 'Now the neural network can make predictions based on the distance and angle
    from the origin. This biases the model towards producing decision boundaries which
    are more curved. Here is how the newly trained model predicts the decision boundary:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，神经网络可以基于与原点的距离和角度做出预测。这使得模型倾向于产生更加弯曲的决策边界。以下是新训练的模型预测的决策边界：
- en: '![](../Images/86f45d8f83378a2d8c9c06d364b6e0e3.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/86f45d8f83378a2d8c9c06d364b6e0e3.png)'
- en: Notice how much better the model performs in parts of the input space where
    there are no samples. The features of those missing points remain similar to features
    of observed points and so the model can predict an effective boundary without
    seeing additional data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在输入空间的无样本部分，模型表现得有多好。那些缺失点的特征仍然与观察到的点的特征相似，因此模型可以在没有额外数据的情况下预测有效的边界。
- en: '*Obviously, inductive priors are useful.*'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*显然，归纳先验是有用的。*'
- en: 'Most architecture decisions will induce an inductive prior. Let’s try some
    enhancements and try to think about what kind of inductive prior they introduce:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数架构决策都会引入归纳先验。让我们尝试一些增强方法，并思考它们引入了什么样的归纳先验：
- en: Focal Loss — increases the loss on data points the model finds hard to predict.
    This might improve accuracy at the cost of increasing the model complexity around
    those points (as we would expect from the bias-variance trade-off). To reduce
    the impact of increased variance we can add some regularisation.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Focal Loss — 增加模型在难以预测的数据点上的损失。这可能会提高准确性，但代价是增加这些点周围的模型复杂度（正如我们从偏差-方差权衡中所期望的那样）。为了减少方差增加的影响，我们可以添加一些正则化。
- en: Weight Decay — L2 norm on the size of the weights prevents the model from learning
    features weighted too strongly to any one sample.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Weight Decay — 对权重大小施加L2范数，防止模型对任何单一样本学习出过于强烈的特征。
- en: Layer Norm — has a lot of subtle effects, one of which could be that the model
    focuses more on the relationships between points rather than their magnitude,
    which might help offset the increased variance from using Focal Loss.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Layer Norm — 有许多微妙的效果，其中之一可能是模型更多地关注点与点之间的关系，而不是它们的大小，这可能有助于抵消使用Focal Loss所增加的方差。
- en: After making all of these improvements, how much better does our predicted manifold
    look?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在做出所有这些改进后，我们预测的流形效果有多好？
- en: '![](../Images/75acb7830492f71044a8ae525f9a5599.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75acb7830492f71044a8ae525f9a5599.png)'
- en: '**Not much better at all**. In fact, it’s introduced an artefact near the centre
    of the spiral. And it’s still failed to predict anything at the end of the spiral
    (in the upper-left quadrant) where there is no data. That said, it has managed
    to capture more of the curve near the origin which is a plus.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**一点都不更好**。事实上，它在螺旋的中心附近引入了一个伪影。并且它仍然没有预测到螺旋末端（左上象限）没有数据的地方。话虽如此，它成功地捕捉到了接近原点的曲线，这也是一个进步。'
- en: The Bitter Lesson
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 苦涩的教训
- en: Now suppose that another research team has no idea that there’s a hard boundary
    in the shape of a single continuous spiral. For all they know there could be pockets
    inside pockets with fuzzy probabilistic boundaries.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设另一个研究团队完全不知道单一连续螺旋形状中有一个硬边界。对于他们来说，形状内部可能存在多个带有模糊概率边界的口袋。
- en: However, this team is able to collect a sample of 10,000 instead of 1,000\.
    For their model they just use a k-Nearest Neighbour (kNN) approach with k=5.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个团队能够收集到10,000个样本，而不是1,000个。对于他们的模型，他们仅使用k近邻(kNN)方法，k=5。
- en: '*Side note: k=5 is a poor inductive prior here. For this problem k=1 is generally
    better. C****hallenge:*** *can you figure out why? Add a comment to this article
    with your answer.*'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*附注：这里k=5是一个不好的归纳先验。对于这个问题，通常k=1会更好。C****hallenge:*** *你能弄明白为什么吗？在文章中留下你的答案。*'
- en: 'Now, kNN is not a particularly powerful algorithm compared to a neural network.
    However, even with a bad inductive prior here is how the kNN solution scales with
    10x more data:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，kNN与神经网络相比并不是一个特别强大的算法。然而，即使有一个不好的归纳先验，下面是kNN在使用10倍更多数据时的表现：
- en: '![](../Images/8aef06ef199cae51f7879b2258635d85.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8aef06ef199cae51f7879b2258635d85.png)'
- en: With 10x more data the kNN approach is performing closer to the neural network.
    In particular it’s better at predicting the shape at the tails of the spiral,
    although it’s still missing that hard to sample upper-left quadrant. It’s also
    making some mistakes, often producing a fuzzier border.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用10倍更多数据后，kNN方法的表现接近于神经网络。特别是在螺旋形状的尾部预测上，它表现得更好，尽管它仍然无法捕捉到那个难以采样的左上象限。它也会犯一些错误，通常会产生一个模糊的边界。
- en: 'What if we added 100x or 1000x more data? Let’s see how both the kNN vs Neural
    Network approaches compare as we scale the amount of data used:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们增加100倍或1000倍的数据会怎样？让我们看看当我们扩大使用的数据量时，kNN与神经网络方法的表现如何对比：
- en: '![](../Images/1b143968f78d15c4666d0119bbd22d6b.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b143968f78d15c4666d0119bbd22d6b.png)'
- en: As we increase the size of the training data it largely doesn’t matter which
    model we use. What’s more, given enough data, the lowly kNN actually starts to
    perform better than our carefully crafted neural network with well thought out
    inductive priors.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们增加训练数据的大小时，使用哪种模型基本上不再重要。而且，考虑到足够的数据，低级的kNN实际上开始表现得比我们精心设计的、拥有深思熟虑的归纳先验的神经网络更好。
- en: This is a big lesson. As a field, we still have not thoroughly learned it, as
    we are continuing to make the same kind of mistakes. To see this, and to effectively
    resist it, we have to understand the appeal of these mistakes. We have to learn
    the bitter lesson that building in how we think we think does not work in the
    long run. The bitter lesson is based on the historical observations that 1) AI
    researchers have often tried to build knowledge into their agents, 2) this always
    helps in the short term, and is personally satisfying to the researcher, but 3)
    in the long run it plateaus and even inhibits further progress, and 4) breakthrough
    progress eventually arrives by an opposing approach based on scaling computation
    by search and learning. The eventual success is tinged with bitterness, and often
    incompletely digested, because it is success over a favored, human-centric approach.
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这是一个重要的教训。作为一个领域，我们至今仍没有完全领会这一点，因为我们仍在犯同样的错误。为了看清这一点，并有效抵制它，我们必须理解这些错误的诱惑。我们必须学习到那个苦涩的教训：在长期来看，构建我们认为的思维方式是行不通的。这个苦涩的教训基于历史观察：1)
    AI研究人员通常尝试将知识内置到他们的智能体中；2) 这在短期内总是有效，并且会让研究人员感到个人满足；但3) 从长远来看，这种方法会进入瓶颈，甚至会抑制进一步的进展；4)
    突破性进展最终是通过基于计算扩展的搜索和学习的对立方法实现的。最终的成功带有一丝苦涩，且往往未被完全消化，因为它是战胜了一种偏向人类中心的做法。
- en: ''
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: From Rich Sutton’s essay “[The Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)”
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来自Rich Sutton的文章“[The Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)”
- en: Superior inductive priors are no match for just using more compute to solve
    the problem. In this case, “more compute” just involves storing a larger sample
    of data in memory and using kNN to match to the nearest neighbours. We’ve seen
    this play out with transformer-based Large Language Models (LLMs). They continue
    to overpower other Natural Language Processing techniques simply by training larger
    and larger models, with more and more GPUs, on more and more text data.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 优越的归纳先验无法与单纯使用更多计算来解决问题相提并论。在这种情况下，“更多计算”仅仅涉及在内存中存储更多的数据样本，并使用kNN匹配最近邻。我们已经在基于变换器的大型语言模型（LLM）中看到这种情况的发生。它们通过不断训练更大、更强的模型，配备更多的GPU，并使用越来越多的文本数据，持续超越其他自然语言处理技术。
- en: But Surely…?
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 但是，肯定…？
- en: 'This toy example has a subtle issue we’ve seen pop up with both models: failing
    to predict that sparse section of the spiral in the upper-left quadrant. This
    is particularly relevant to Large Language Models, training reasoning capabilities,
    and our quest towards “Artificial General Intelligence” (AGI). To see what I mean
    let’s zoom in on that unusual shaped tail in the upper-left.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个玩具示例有一个微妙的问题，我们在这两种模型中都曾见过：未能预测到左上象限中螺旋的稀疏部分。这与大型语言模型、训练推理能力以及我们追求“人工通用智能”（AGI）息息相关。为了更好地理解我的意思，让我们放大看一下左上角那个形状独特的尾部。
- en: '![](../Images/fbbdcca1634e9a1ac90ec1566e3ee6c1.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fbbdcca1634e9a1ac90ec1566e3ee6c1.png)'
- en: 'This region has a particularly low sampling density and the boundary is quite
    different to the rest of the manifold. Suppose this area is something we care
    a lot about, for example: generating “reasoning” from a Large Language Model (LLM).
    Not only is such data rare (if randomly sampled) but it is sufficiently different
    to the rest of the data, which means features from other parts of the space are
    not useful in making predictions here. Additionally, notice how sharp and specific
    the boundary is — points sampled near the tip could very easily fall on the outside.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这个区域的采样密度特别低，边界与其余的流形有很大的不同。假设这个区域是我们非常关心的，例如：从大型语言模型（LLM）生成“推理”。这样的数据不仅稀缺（如果是随机抽样的话），而且与其余数据有足够的不同，这意味着来自空间其他部分的特征在此处进行预测时并不有用。此外，注意到边界是多么尖锐和具体——在尖端附近采样的点很容易落在外部。
- en: 'Let’s see how this compares to a simplified view of training an LLM on text-based
    reasoning:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这与基于文本推理训练大型语言模型（LLM）的简化视图有何不同：
- en: Reasoning is complicated and we probably won’t find a solution by fitting a
    “smooth” line that averages out a few samples. To solve a reasoning problem it’s
    not enough to follow an apparent pattern but it’s necessary to really understand
    the problem. Training a model to reason will likely need a lot of data.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 推理是复杂的，我们可能无法通过拟合一条“平滑”的线来平均几个样本，从而找到解决方案。要解决推理问题，单纯遵循一个显而易见的模式是不够的，而是必须真正理解问题。训练一个能够进行推理的模型可能需要大量的数据。
- en: Randomly sampling data from the internet doesn’t give us many samples where
    humans explain intricate mental reasoning steps required to get to an answer.
    Paying people to explicitly generate reasoning data may help increase the density.
    But it’s a slow process and the amount of data needed is actually quite high.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从互联网上随机抽取数据并不能为我们提供很多人类解释复杂思维推理步骤的样本。支付人们显式生成推理数据可能有助于提高数据的密度。但这是一个缓慢的过程，而且所需的数据量实际上相当庞大。
- en: We care a lot about getting it right because reasoning abilities would open
    up a lot more use cases for AI.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们非常重视正确的推理能力，因为它将为人工智能开辟更多的应用场景。
- en: Of course reasoning is more complex than predicting the tip of this spiral.
    There are usually multiple ways to get to a correct answer, there may be many
    correct answers, and sometimes the boundary can be fuzzy. However, we are also
    not without inductive priors in deep learning architectures, including techniques
    using reinforcement learning.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，推理比预测螺旋的尖端要复杂得多。通常有多种方法可以得出正确答案，可能有多个正确答案，而且有时边界可能模糊。然而，在深度学习架构中，我们也并非没有归纳先验，包括使用强化学习的技术。
- en: In our toy problem there is regularity in the shape of the boundary and so we
    used an inductive prior to encourage the model to learn that shape. When modelling
    reasoning, if we could construct a [manifold in a higher dimensional space](https://en.wikipedia.org/wiki/Manifold_hypothesis)
    representing concepts and ideas, there would be some regularity to its shape that
    could be exploited for an inductive prior. If *The Bitter Lesson* continues to
    hold then we would assume the search for such an inductive prior is not the path
    forward. We just need to scale compute. And so far the best way to do that is
    to collect more data and throw it at larger models.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的玩具问题中，边界形状具有规律性，因此我们使用了归纳先验来鼓励模型学习这种形状。在建模推理时，如果我们能够构建一个[更高维空间中的流形](https://en.wikipedia.org/wiki/Manifold_hypothesis)来表示概念和思想，那么它的形状会有某种规律性，这可以用作归纳先验。如果*《苦涩的教训》*继续成立，那么我们将假设寻找这样的归纳先验不是前进的道路。我们只需要扩展计算能力。到目前为止，做到这一点的最佳方法就是收集更多的数据，并将其投入更大的模型中。
- en: But surely, I hear you say, transformers were so successful because the *attention
    mechanism* introduced a strong inductive prior into language modelling? The paper
    “[Were RNNs all we needed](https://arxiv.org/pdf/2410.01201)” suggests that a
    simplified Recurrent Neural Network (RNN) can also perform well if scaled up.
    It’s not because of an inductive prior. It’s because the paper improved the speed
    with which we can train an RNN on large amounts of data. And that’s why transformers
    are so effective — parallelism allowed us to leverage much more compute. It’s
    an architecture straight from the heart of *The Bitter Lesson*.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 但你肯定会说，transformer之所以如此成功，是因为*注意力机制*为语言建模引入了强大的归纳先验？论文“[我们只需要RNN吗](https://arxiv.org/pdf/2410.01201)”表明，如果规模足够大，简化版的循环神经网络（RNN）也能表现得很好。这不是归纳先验的作用。它是因为论文提高了我们在大数据上训练RNN的速度。这就是transformer如此有效的原因——并行性使我们能够利用更多的计算资源。这是一种直接来源于*《苦涩的教训》*的架构。
- en: Running Out of Data?
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据耗尽了吗？
- en: There’s always more data. Synthetic data or reinforcement learning techniques
    like self-play can generate infinite data. Although without connection to the
    real world the validity of that data can get fuzzy. That’s why techniques like
    RLHF have hand crafted data as a base — so that the model of human preferences
    can be as accurate as possible. Also, given that reasoning is often mathematical,
    it may be easy to generate such data using automated methods.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 数据永远不够。合成数据或强化学习技术（如自我对弈）可以生成无限的数据。尽管如果与现实世界没有关联，这些数据的有效性可能会变得模糊。这就是为什么像RLHF这样的技术以手工制作的数据为基础——以便尽可能准确地建模人类偏好。此外，鉴于推理通常是数学化的，因此可能很容易通过自动化方法生成这些数据。
- en: 'Now the question is: given the current inductive priors we have, how much data
    would it take to train models with true reasoning capability?'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是：考虑到我们目前拥有的归纳先验，需要多少数据才能训练出具有真正推理能力的模型？
- en: 'If *The Bitter Lesson* continues to apply the answer is: it doesn’t matter,
    finding better ways to leverage more compute will continue to give better gains
    than trying to find superior inductive priors^. This means that the search for
    ever more powerful AI is firmly in the domain of the companies with the biggest
    budgets.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*《苦涩的教训》*继续适用，答案是：没关系，找到更好的方法来利用更多的计算资源，所能带来的收益将继续大于试图寻找更优的归纳先验^。这意味着，寻找更强大的AI将坚决掌握在预算最大的公司手中。
- en: And after writing all of this… I still hope that’s not true.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 写完这一切后……我仍然希望这不是真的。
- en: About me
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于我
- en: I’m the Lead AI Engineer @ [Affinda](https://www.affinda.com/). Check out our
    [AI Document Automation Case Studies](https://www.affinda.com/case-studies) to
    learn more.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我是[Affinda](https://www.affinda.com/)的首席AI工程师。请查看我们的[AI文档自动化案例研究](https://www.affinda.com/case-studies)了解更多信息。
- en: '**Some of my long reads:**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**一些我的长文：**'
- en: '[What do Large Language Models “Understand”?](https://medium.com/towards-data-science/what-do-large-language-models-understand-befdb4411b77)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[大语言模型“理解”什么？](https://medium.com/towards-data-science/what-do-large-language-models-understand-befdb4411b77)'
- en: '[Exploring the AI Alignment Problem with GridWorlds](https://medium.com/towards-data-science/exploring-the-ai-alignment-problem-with-gridworlds-2683f2f5af38)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过网格世界探索AI对齐问题](https://medium.com/towards-data-science/exploring-the-ai-alignment-problem-with-gridworlds-2683f2f5af38)'
- en: '**More practical reads:**'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**更多实用阅读：**'
- en: '[5 Hidden Risks in Deploying Generative AI](https://medium.com/management-matters/managing-risks-in-deploying-generative-ai-393254259497)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[部署生成性AI的五大隐性风险](https://medium.com/management-matters/managing-risks-in-deploying-generative-ai-393254259497)'
- en: '[How I Deal with Hallucinations at an AI Startup](https://medium.com/towards-data-science/how-i-deal-with-hallucinations-at-an-ai-startup-9fc4121295cc)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[我在人工智能初创公司如何应对幻觉](https://medium.com/towards-data-science/how-i-deal-with-hallucinations-at-an-ai-startup-9fc4121295cc)'
- en: Appendix
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录
- en: ^ It’s important to note that the essay “The Bitter Lesson” isn’t explicitly
    about inductive biases vs collecting more data. Throwing more data at bigger models
    is one way to leverage more compute. And in deep learning that usually means finding
    better ways to increase parallelism in training. Lately it’s also about leveraging
    more inference time compute (e.g. o1-preview). There may yet be other ways. The
    topic is slightly more nuanced than I’ve presented here in this short article.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ^ 值得注意的是，文章《苦涩的教训》并非明确讨论归纳偏差与收集更多数据的问题。将更多数据投入更大的模型是利用更多计算资源的一种方式。而在深度学习中，这通常意味着找到更好的方式来提高训练中的并行性。最近，这也涉及到利用更多推理时间的计算（例如
    o1-preview）。可能还有其他方法。这个话题比我在这篇短文中呈现的要更为微妙。
