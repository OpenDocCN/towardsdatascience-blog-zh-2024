- en: Constrained Sentence Generation Using Gibbs Sampling and BERT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 受限句子生成使用吉布斯采样和BERT
- en: 原文：[https://towardsdatascience.com/constrained-sentence-generation-using-gibbs-sampling-and-bert-8d326b9027b1?source=collection_archive---------6-----------------------#2024-07-19](https://towardsdatascience.com/constrained-sentence-generation-using-gibbs-sampling-and-bert-8d326b9027b1?source=collection_archive---------6-----------------------#2024-07-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/constrained-sentence-generation-using-gibbs-sampling-and-bert-8d326b9027b1?source=collection_archive---------6-----------------------#2024-07-19](https://towardsdatascience.com/constrained-sentence-generation-using-gibbs-sampling-and-bert-8d326b9027b1?source=collection_archive---------6-----------------------#2024-07-19)
- en: A fast and effective approach to generating fluent sentences from given keywords
    using public pre-trained models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一种快速有效的方法，通过使用公共预训练模型，从给定的关键词生成流畅的句子
- en: '[](https://medium.com/@sergeyvilov?source=post_page---byline--8d326b9027b1--------------------------------)[![Sergey
    Vilov](../Images/42efe223e2aa575250e050cf3660cf20.png)](https://medium.com/@sergeyvilov?source=post_page---byline--8d326b9027b1--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8d326b9027b1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8d326b9027b1--------------------------------)
    [Sergey Vilov](https://medium.com/@sergeyvilov?source=post_page---byline--8d326b9027b1--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@sergeyvilov?source=post_page---byline--8d326b9027b1--------------------------------)[![Sergey
    Vilov](../Images/42efe223e2aa575250e050cf3660cf20.png)](https://medium.com/@sergeyvilov?source=post_page---byline--8d326b9027b1--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8d326b9027b1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8d326b9027b1--------------------------------)
    [Sergey Vilov](https://medium.com/@sergeyvilov?source=post_page---byline--8d326b9027b1--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8d326b9027b1--------------------------------)
    ·10 min read·Jul 19, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8d326b9027b1--------------------------------)
    ·阅读时间 10 分钟·2024年7月19日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/9c688d373215ed1941eb26ed02e66cb5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c688d373215ed1941eb26ed02e66cb5.png)'
- en: Photo by [Brett Jordan](https://unsplash.com/@brett_jordan?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [Brett Jordan](https://unsplash.com/@brett_jordan?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '[Large language models](https://en.wikipedia.org/wiki/Large_language_model),
    like [GPT](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer),
    have achieved unprecedented results in *free-form* text generation. They’re widely
    used for writing e-mails, copyrighting, or storytelling. However, their success
    in *constrained* text generation remains limited [1].'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[大型语言模型](https://en.wikipedia.org/wiki/Large_language_model)，如 [GPT](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer)，在*自由形式*文本生成方面取得了前所未有的成果。它们广泛用于撰写电子邮件、广告文案或讲故事。然而，它们在*受限*文本生成方面的成功仍然有限[1]。'
- en: Constrained text generation involves producing sentences with specific attributes
    like sentiment, tense, template, or style. We will consider one specific kind
    of constrained text generation, namely keyword-based generation. In this task,
    it is required that the model generate sentences that include given keywords.
    Depending on the application, these sentences should (a) contain all the keywords
    (i.e. assure high coverage) (b) be grammatically correct (c) respect common sense
    (d) exhibit lexical and grammatical diversity.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 受限文本生成是指生成具有特定属性的句子，例如情感、时态、模板或风格。我们将考虑一种特定的受限文本生成方式，即基于关键词的生成。在这个任务中，要求模型生成包含给定关键词的句子。根据应用的不同，这些句子应该（a）包含所有关键词（即保证高覆盖率）（b）语法正确（c）符合常识（d）展示词汇和语法的多样性。
- en: For auto-regressive forward generation models, like GPT, constrained generation
    is particularly challenging. These models yield tokens sequentially from left
    to right, one at a time. By design, they lack precise control over the generated
    sequence and struggle to support constraints at arbitrary positions in the output
    or constraints involving multiple keywords. As a result, these models usually
    exhibit poor coverage (a) and diversity (d), while providing fluent sentences
    (b,c). Although some sampling strategies, like dynamic beam allocation [2], were
    specifically designed to improve constrained text generation with forward models,
    they demonstrated inferior results in independent testing [3].
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自回归的前向生成模型，如GPT，受限生成尤其具有挑战性。这些模型一次一个地从左到右顺序生成标记。由于设计原因，它们缺乏对生成序列的精确控制，并且难以支持在输出中的任意位置上的约束或涉及多个关键词的约束。因此，这些模型通常表现出较差的覆盖度（a）和多样性（d），但能够生成流畅的句子（b,c）。尽管一些采样策略，如动态束分配[2]，专门设计来改善前向模型的受限文本生成，但在独立测试中，它们的结果表现较差[3]。
- en: An alternative approach [4], known as CGMH, consists in constructing the sentence
    iteratively by executing elementary operations on the existing sequence, such
    as word deletion, insertion, or replacement. The initial sequence is usually an
    ordered sequence of given keywords. Because of the vast search space, such methods
    often struggle to produce a meaningful sentence within a reasonable time frame.
    Therefore, although these models may ensure good coverage (a) and diversity (d),
    they might fail to satisfy fluency requirements (b,c). To overcome these problems,
    it was suggested to restrict the search space by including a differentiable loss
    function [5] or a pre-trained neural network [6] to guide the sampler. However,
    these adjustments did not lead to any practically significant improvement compared
    to CGMH.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法[4]，被称为CGMH，包含通过对现有序列执行基本操作（如单词删除、插入或替换）来迭代地构建句子。初始序列通常是给定关键词的有序序列。由于搜索空间极其庞大，这类方法往往难以在合理的时间内生成有意义的句子。因此，尽管这些模型可能确保良好的覆盖度（a）和多样性（d），它们可能未能满足流畅性要求（b,c）。为了解决这些问题，曾建议通过引入可微分的损失函数[5]或预训练神经网络[6]来引导采样器，从而限制搜索空间。然而，与CGMH相比，这些调整并未带来任何在实践中显著的改进。
- en: In the following, we will propose a new approach to generating sentences with
    given keywords. The idea is to limit the search space by starting from a correct
    sentence and reducing the set of possible operations. It turns out that when only
    the replacement operation is considered, the BERT model provides a convenient
    way to generate desired sentences via Gibbs sampling.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下内容中，我们将提出一种基于给定关键词生成句子的全新方法。该方法的核心思想是通过从一个正确的句子出发，限制搜索空间并减少可能的操作集合。事实证明，当仅考虑替换操作时，BERT模型通过吉布斯采样提供了一种便捷的方式来生成期望的句子。
- en: Gibbs sampling from BERT
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从BERT进行吉布斯采样
- en: Sampling sentences via [Gibbs sampling](https://en.wikipedia.org/wiki/Gibbs_sampling)
    from [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)) was first proposed
    in [7]. Here, we adapt this idea for constrained sentence generation.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 通过从[BERT](https://en.wikipedia.org/wiki/BERT_(language_model))进行[吉布斯采样](https://en.wikipedia.org/wiki/Gibbs_sampling)来采样句子，最早是在[7]中提出的。在这里，我们将这一思路应用于受限句子的生成。
- en: To simplify theoretical introduction, we will start by explaining the grounds
    of the CGMH approach [4], which uses the [Metropolis-Hastings algorithm](https://en.wikipedia.org/wiki/Metropolis–Hastings_algorithm)
    to sample from a sentence distribution satisfying the given constraints.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化理论介绍，我们将从解释CGMH方法的基础开始[4]，该方法使用[Metropolis-Hastings算法](https://en.wikipedia.org/wiki/Metropolis–Hastings_algorithm)从满足给定约束条件的句子分布中进行采样。
- en: 'The sampler starts from a given sequence of keywords. At each step, a random
    position in the current sentence is selected and one of the three possible actions
    (chosen with probability *p*=1/3) is executed: insertion, deletion, or word replacement.
    After that, a candidate sentence is sampled from the corresponding proposal distribution.
    In particular, the proposal distribution for replacement takes up the form:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 采样器从给定的关键词序列开始。在每一步中，选择当前句子中的一个随机位置，并执行三种可能操作之一（按概率*p*=1/3选择）：插入、删除或替换单词。之后，从相应的提议分布中采样候选句子。特别地，替换的提议分布具有以下形式：
- en: '![](../Images/463f34fa318c22f5a9d63318be308e7d.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/463f34fa318c22f5a9d63318be308e7d.png)'
- en: (image by the author)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: （图片来源：作者）
- en: 'where *x* is the current sentence, *x’* is a candidate sentence, *w_1*…*w_n*
    are the words in the sentence, *w^c* is the proposed word, *V* is the dictionary
    size, and π is the sampled distribution. The candidate sentence can then be either
    accepted or rejected using the acceptance rate:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*x*是当前句子，*x’*是候选句子，*w_1*…*w_n*是句子中的单词，*w^c*是提议的单词，*V*是词典大小，π是采样分布。然后，可以使用接受率来决定是否接受或拒绝候选句子：
- en: '![](../Images/7c3776efb5b375b1a1f94b2ee4cab722.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c3776efb5b375b1a1f94b2ee4cab722.png)'
- en: (image by the author)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: （图像来自作者）
- en: 'To get a sentence probability, the authors propose to use a simple seq2seq
    LSTM-based network:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得句子的概率，作者建议使用一个简单的基于seq2seq LSTM的网络：
- en: '![](../Images/d2180dc7f1fd9c9400ad22d542596b12.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2180dc7f1fd9c9400ad22d542596b12.png)'
- en: (Image by the author)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: （图像来自作者）
- en: where *p_LM(x)* is the sentence probability given by a language model and *χ(x)*
    is an indicator function, which is 1 when all of the keyword words are included
    in the sentence and 0 otherwise.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*p_LM(x)*是语言模型给出的句子概率，*χ(x)*是一个指示函数，当所有关键字都包含在句子中时值为1，否则为0。
- en: When keyword constraints are imposed, the generation starts from a given sequence
    of keywords. These words are then excluded from deletion and replacement operations.
    After a certain time ([the burn-in period](https://en.wikipedia.org/wiki/Gibbs_sampling)),
    generation converges to a stationary distribution.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当施加关键字约束时，生成从给定的关键词序列开始。这些词然后被排除在删除和替换操作之外。在一段时间后（[预热期](https://en.wikipedia.org/wiki/Gibbs_sampling)），生成过程会收敛到一个平稳分布。
- en: As noted above, a weak point of such methods is the large search space that
    prevents them from generating meaningful sentences within a reasonable time. We
    will now reduce the search space by completely eliminating insertions and deletions
    from sentence generation.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，这类方法的一个弱点是庞大的搜索空间，这使得它们无法在合理的时间内生成有意义的句子。我们现在将通过完全消除插入和删除操作来缩小搜索空间。
- en: Ok, but what does this have to do with Gibbs sampling and BERT?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这和吉布斯采样以及BERT有什么关系呢？
- en: '[Citing Wikipedia](https://en.wikipedia.org/wiki/Gibbs_sampling), Gibbs sampling
    is used when the joint distribution is not known explicitly or is difficult to
    sample from directly, but the conditional distribution of each variable is known
    and is easy (or at least, easier) to sample from.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[引用 Wikipedia](https://en.wikipedia.org/wiki/Gibbs_sampling)，吉布斯采样在联合分布未知或难以直接从中采样时使用，但每个变量的条件分布是已知的，并且易于（或者至少，更容易）从中采样。'
- en: 'BERT is a transformer-based model designed to pre-train deep bidirectional
    representations by jointly conditioning on both left and right context, enabling
    it to understand the context of a word based on its surroundings. For us, it is
    particularly important that BERT is trained in a masked language model fashion,
    i.e. it predicts masked words (tokens) given all other words (tokens) in the sentence.
    If only a single word is masked, then the model directly provides the conditional
    probability *p(w_c|w_1,…,w_{m-1},w_{m+1},…,w_n)*. Note that it is only possible
    due to the bidirectional nature of BERT, since it provides access to tokens on
    the left as well as on the right from the masked word. On the other hand, the
    joint probability *p(w_1,…w_n)* is not readily available from the BERT output.
    Looks like a Gibbs sampling use case, doesn’t it? Rewriting *g(x’|x)*, one obtains:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: BERT是一个基于变换器的模型，旨在通过同时考虑左右上下文来进行深度双向表示的预训练，使其能够根据周围的上下文理解一个单词的意义。对我们来说，特别重要的是BERT是以掩码语言模型的方式进行训练的，即给定句子中的所有其他单词（标记），它预测掩码单词（标记）。如果仅掩盖一个单词，那么模型直接提供条件概率*p(w_c|w_1,…,w_{m-1},w_{m+1},…,w_n)*。请注意，这只有在BERT的双向特性下才能实现，因为它能够访问掩码单词左右两侧的标记。另一方面，联合概率*p(w_1,…w_n)*并不直接从BERT输出中获得。看起来像是一个吉布斯采样的应用案例，对吧？重新编写*g(x’|x)*，可以得到：
- en: '![](../Images/0854eb2db1eb0f84989372d7a7ab27b6.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0854eb2db1eb0f84989372d7a7ab27b6.png)'
- en: (image by the author)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: （图像来自作者）
- en: 'Note that as far as only the replacement action is considered, the acceptance
    rate is always 1:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，只要考虑替换操作，接受率总是1：
- en: '![](../Images/1392aaaf6018fb4b4216bc21cfda36dd.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1392aaaf6018fb4b4216bc21cfda36dd.png)'
- en: (image by the author)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: （图像来自作者）
- en: So, replacement is, in fact, a Gibbs sampling step, with the proposal distribution
    directly provided by the BERT model!
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，替换实际上是一个吉布斯采样步骤，其中提议分布由BERT模型直接提供！
- en: Experiment
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验
- en: To illustrate the method, we will use a pre-trained BERT model from [Hugging
    Face](https://huggingface.co). To have an independent assessment of sentence fluency,
    we will also compute sentence [perplexity](https://en.wikipedia.org/wiki/Perplexity)
    using the [GPT2](https://huggingface.co/openai-community/gpt2) model.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这个方法，我们将使用来自[Hugging Face](https://huggingface.co)的一个预训练BERT模型。为了独立评估句子的流畅度，我们还将使用[GPT2](https://huggingface.co/openai-community/gpt2)模型计算句子的[困惑度](https://en.wikipedia.org/wiki/Perplexity)。
- en: 'Let us start by loading all the required modules and models into memory:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从加载所有必要的模块和模型到内存开始：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We then need to define some important constants:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要定义一些重要的常量：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Since we will use only the replacement action, we need to select an initial
    sentences containing the desired keywords. Let it be
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们只会使用替换操作，我们需要选择一个包含所需关键词的初始句子。假设是：
- en: '*I often dream about a spacious villa by the sea*.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*我常常梦想着有一座位于海边的宽敞别墅*。'
- en: Everybody must have dreamt about this at some time… As keywords we will fix,
    quite arbitrary, *dream* and *sea*.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 每个人在某个时候肯定都曾梦想过这个……我们将随意选择*梦想*和*海*作为关键词。
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now we are ready to sample:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好进行采样：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let’s now have a look at the perplexity plot:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看困惑度图：
- en: '![](../Images/e35e69a4a91a0732947a87e3e45c1d85.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e35e69a4a91a0732947a87e3e45c1d85.png)'
- en: GPT2 perplexity for the sampled sentences (image by the author).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于采样句子的GPT2困惑度（图片由作者提供）。
- en: There are two things to note here. First, the perplexity starts from a relatively
    small value (*perplexity*=147). This is just because we initialized the sampler
    with a valid sentence that doesn’t look awkward to GPT2\. Basically, the sentences
    whose perplexity does not exceed the starting value (dashed red line) can be considered
    passing the external check. Second, subsequent samples are correlated. This is
    a [known property](https://en.wikipedia.org/wiki/Gibbs_sampling) of the Gibbs
    sampler and the reason why it is often recommended to take every *k*th sample.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两点需要注意。首先，困惑度从一个相对较小的值开始（*困惑度*=147）。这是因为我们用一个对GPT2来说不显得尴尬的有效句子初始化了采样器。基本上，困惑度不超过起始值（虚线红线）的句子可以视为通过了外部检查。其次，后续的样本是相关的。这是[Gibbs采样](https://en.wikipedia.org/wiki/Gibbs_sampling)的一个[已知特性](https://en.wikipedia.org/wiki/Gibbs_sampling)，也是为什么通常建议每取第*k*个样本的原因。
- en: 'In fact, out of 2000 generated sentences we got 822 unique. Their perplexity
    ranges from 60 to 1261 with 341 samples having perplexity below that of the initial
    sentence:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，在2000个生成的句子中，我们得到了822个独特句子。它们的困惑度从60到1261不等，341个样本的困惑度低于初始句子的困惑度：
- en: '![](../Images/dd5c75df96c63cba4241ac6768dfb5cf.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd5c75df96c63cba4241ac6768dfb5cf.png)'
- en: GPT2 perplexity distribution across unique sentences (image by the author).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: GPT2困惑度在独特句子中的分布（图片由作者提供）。
- en: 'How do these sentences look like? Let’s take a random subset:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这些句子看起来怎么样？让我们随机挑选一部分：
- en: '![](../Images/013a1614f64dceeb9eafd2650bf0a2cd.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/013a1614f64dceeb9eafd2650bf0a2cd.png)'
- en: A random subset of generated sentences with perplexity below the starting value
    (image by the author).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的困惑度低于起始值的句子随机子集（图片由作者提供）。
- en: These sentences look indeed quite fluent. Note that the chosen keywords (*dream*
    and *sea*) appear in each sentence.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这些句子看起来确实相当流畅。请注意，所选的关键词（*梦想*和*海*）在每个句子中都出现了。
- en: 'It is also tempting to see what happens if we don’t set any keywords. Let’s
    take a random subset of sentences generated with an empty keywords set:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不设置任何关键词，看看会发生什么也是很有趣的。让我们随机选取一些使用空关键词集生成的句子：
- en: '![](../Images/acf141780fcff6e0c5b418ea748a3808.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/acf141780fcff6e0c5b418ea748a3808.png)'
- en: A random subset of sentences generated without fixing keywords (image by the
    author).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用未固定关键词生成的句子的随机子集（图片由作者提供）。
- en: So, these sentence also look quite fluent and diverse! In fact, using an empty
    keyword set simply turns BERT into a random sentence generator. Note, however,
    that all these sentences have 10 words, as the initial sentence. The reason is
    that the BERT model can’t change the sentence length arbitrary.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这些句子也看起来相当流畅和多样化！事实上，使用一个空的关键词集实际上会将BERT变成一个随机句子生成器。不过需要注意的是，所有这些句子都有10个单词，就像初始句子一样。原因是BERT模型不能随意改变句子的长度。
- en: 'Now, why do we need to run the sampler N_GIBBS_RUNS=4 times, wouldn’t just
    a single run be enough? In fact, running several times is necessary since a Gibbs
    sampler can get stuck in a local minimum [7]. To illustrate this case, we computed
    the accumulated vocabulary size (number of distinct words used so far in the generated
    sentences) when running the sampler once for 2000 iterations and when re-initializing
    the sampler with the initial sentence every 500 iterations:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们需要运行采样器 N_GIBBS_RUNS=4 次，单次运行不足以吗？实际上，运行多次是必要的，因为 Gibbs 采样器可能会陷入局部最小值[7]。为了说明这一情况，我们计算了在单次运行采样器
    2000 次迭代时和在每 500 次迭代时重新初始化采样器的情况下，累积词汇量（生成的句子中迄今使用的不同单词数量）：
- en: '![](../Images/f14bcdd41d6bdc70405c95a3a23ca9ac.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f14bcdd41d6bdc70405c95a3a23ca9ac.png)'
- en: Accumulated vocabulary size when running Gibbs samplig for 2000 iterations in
    a single run and in 4 runs, 500 iterations each (image by the author)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在单次运行中运行 Gibbs 采样 2000 次迭代和在 4 次运行中每次 500 次迭代时的累积词汇量（作者提供的图片）
- en: It can be clearly seen that a single run gets stuck at about 1500 iterations
    and the sampler is not able to generate sentences with new words after this point.
    In contrast, re-initializing the sampler every 500 iterations helps to get out
    of this local minimum and improves lexically diversity of the generated sentences.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 可以清楚地看到，单次运行在约 1500 次迭代时陷入困境，采样器无法在此点之后生成具有新单词的句子。相反，每 500 次迭代重新初始化采样器有助于摆脱这一局部最小值，并提高生成句子的词汇多样性。
- en: Conclusion
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In sum, the proposed method generates realistic sentences starting from a sentence
    containing given keywords. The resulting sentences ensure 100% coverage (a), sound
    grammatically correct (b), respect common sense (c), and provide lexical diversity
    (d). Additionally, the method is incredibly simple and can be used with publicly
    available pre-trained models. The main weaknesses of the method, is, of course,
    its dependence of a starting sentence satisfying the given constraints. First,
    the starting sentence should be somehow provided from an expert or any other external
    source. Second, while ensuring grammatically correct sentence generation, it also
    limits the grammatical diversity of the output. A possible solution would be to
    provide several input sentences by mining a reliable sentence database.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，所提出的方法从包含给定关键词的句子开始生成逼真的句子。生成的句子确保 100% 覆盖（a），语法正确（b），符合常识（c），并提供词汇多样性（d）。此外，该方法非常简单，可以与公开可用的预训练模型一起使用。该方法的主要弱点当然是其依赖于满足给定约束的起始句子。首先，起始句子应该由专家或其他外部来源提供。其次，虽然确保生成语法正确的句子，但也限制了输出的语法多样性。一个可能的解决方案是通过挖掘可靠的句子数据库提供几个输入句子。
- en: References
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Garbacea, Cristina, and Qiaozhu Mei. “Why is constrained neural language
    generation particularly challenging?.” *arXiv preprint arXiv:2206.05395* (2022).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Garbacea, Cristina, and Qiaozhu Mei. “为什么受限神经语言生成特别具有挑战性？.” *arXiv 预印本
    arXiv:2206.05395* (2022).'
- en: '[2] Post, Matt, and David Vilar. “Fast lexically constrained decoding with
    dynamic beam allocation for neural machine translation.” *arXiv preprint arXiv:1804.06609*
    (2018).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Post, Matt, and David Vilar. “使用动态束分配进行快速词汇受限解码的神经机器翻译.” *arXiv 预印本 arXiv:1804.06609*
    (2018).'
- en: '[3] Lin, Bill Yuchen, et al. “CommonGen: A constrained text generation challenge
    for generative commonsense reasoning.” *arXiv preprint arXiv:1911.03705* (2019).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Lin, Bill Yuchen, 等. “CommonGen: 用于生成常识推理的受限文本生成挑战.” *arXiv 预印本 arXiv:1911.03705*
    (2019).'
- en: '[4] Miao, Ning, et al. “Cgmh: Constrained sentence generation by metropolis-hastings
    sampling.” *Proceedings of the AAAI Conference on Artificial Intelligence*. Vol.
    33\. №01\. 2019.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Miao, Ning, 等. “Cgmh: 通过 Metropolis-Hastings 采样进行受限句子生成.” *人工智能AAAI会议论文集*.
    Vol. 33\. №01\. 2019.'
- en: '[5] Sha, Lei. “Gradient-guided unsupervised lexically constrained text generation.”
    *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing
    (EMNLP)*. 2020.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Sha, Lei. “梯度引导的无监督词汇受限文本生成.” *2020年自然语言处理实证方法会议论文集（EMNLP）*. 2020.'
- en: '[6] He, Xingwei, and Victor OK Li. “Show me how to revise: Improving lexically
    constrained sentence generation with xlnet.” *Proceedings of the AAAI Conference
    on Artificial Intelligence*. Vol. 35\. №14\. 2021.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] He, Xingwei, and Victor OK Li. “告诉我如何修改：通过 xlnet 改进词汇受限句子生成.” *人工智能AAAI会议论文集*.
    Vol. 35\. №14\. 2021.'
- en: '[7] Wang, Alex, and Kyunghyun Cho. “BERT has a mouth, and it must speak: BERT
    as a Markov random field language model.” *arXiv preprint arXiv:1902.04094* (2019).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] 王艾利克斯，赵庆贤. “BERT 有嘴巴，它必须说话：BERT 作为马尔可夫随机场语言模型.” *arXiv 预印本 arXiv:1902.04094*
    (2019).'
