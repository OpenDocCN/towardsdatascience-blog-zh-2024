- en: 'From Unimodals to Multimodality: DIY Techniques for Building Foundational Models'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从单模态到多模态：构建基础模型的DIY技术
- en: 原文：[https://towardsdatascience.com/from-open-source-unimodal-to-multimodal-diy-techniques-for-building-foundational-models-e1df92276379?source=collection_archive---------10-----------------------#2024-06-25](https://towardsdatascience.com/from-open-source-unimodal-to-multimodal-diy-techniques-for-building-foundational-models-e1df92276379?source=collection_archive---------10-----------------------#2024-06-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/from-open-source-unimodal-to-multimodal-diy-techniques-for-building-foundational-models-e1df92276379?source=collection_archive---------10-----------------------#2024-06-25](https://towardsdatascience.com/from-open-source-unimodal-to-multimodal-diy-techniques-for-building-foundational-models-e1df92276379?source=collection_archive---------10-----------------------#2024-06-25)
- en: 'A comprehensive tutorial: Using advanced techniques like prompt adaptation
    and adapters to transform open-source unimodal models into multimodal ones, including
    all variants of LLaMA-Adapters, LLaVa, MiniGPT-4, and more.'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一篇全面的教程：使用像提示适配和适配器这样的高级技术，将开源的单模态模型转化为多模态模型，包括所有LLaMA-Adapters、LLaVa、MiniGPT-4等变种。
- en: '[](https://medium.com/@InfiniteLearningLoop?source=post_page---byline--e1df92276379--------------------------------)[![Elahe
    Aghapour](../Images/47a2023c566d50d8ecfcafdb69bb9bb7.png)](https://medium.com/@InfiniteLearningLoop?source=post_page---byline--e1df92276379--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e1df92276379--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e1df92276379--------------------------------)
    [Elahe Aghapour](https://medium.com/@InfiniteLearningLoop?source=post_page---byline--e1df92276379--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@InfiniteLearningLoop?source=post_page---byline--e1df92276379--------------------------------)[![Elahe
    Aghapour](../Images/47a2023c566d50d8ecfcafdb69bb9bb7.png)](https://medium.com/@InfiniteLearningLoop?source=post_page---byline--e1df92276379--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e1df92276379--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e1df92276379--------------------------------)
    [Elahe Aghapour](https://medium.com/@InfiniteLearningLoop?source=post_page---byline--e1df92276379--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e1df92276379--------------------------------)
    ·15 min read·Jun 25, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e1df92276379--------------------------------)
    ·15分钟阅读·2024年6月25日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '**Authors:** [Elahe Aghapour](https://medium.com/u/75214fb27311?source=post_page---user_mention--e1df92276379--------------------------------),
    [Salar Rahili](https://medium.com/u/6dff1eb2cc9f?source=post_page---user_mention--e1df92276379--------------------------------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：** [Elahe Aghapour](https://medium.com/u/75214fb27311?source=post_page---user_mention--e1df92276379--------------------------------)，[Salar
    Rahili](https://medium.com/u/6dff1eb2cc9f?source=post_page---user_mention--e1df92276379--------------------------------)'
- en: '***INTRODUCTION***'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '***引言***'
- en: With recent advancements in large language models (LLMs), AI has become the
    spotlight of technology. We’re now more eager than ever to reach AGI-level intelligence.
    Yet, achieving a human-like understanding of our surroundings involves much more
    than just mastering language and text comprehension. Humans use their five senses
    to interact with the world and act based on these interactions to achieve goals.
    This highlights that the next step for us is to develop large models that incorporate
    multimodal inputs and outputs, bringing us closer to human-like capabilities.
    However, we face two main obstacles. First, we need a multimodal labeled dataset,
    which is not as accessible as text data. Second, we are already pushing the limits
    of compute capacity for training models with textual data. Increasing this capacity
    to include other modalities, especially high-dimensional ones like images and
    videos, is incredibly challenging.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型语言模型（LLMs）的最新进展，人工智能已成为技术的焦点。我们比以往任何时候都更渴望达到AGI级别的智能。然而，想要实现对周围环境的类人理解，涉及的远不止是掌握语言和文本理解。人类通过五感与世界互动，并基于这些互动采取行动以实现目标。这突显了我们的下一个步骤是开发能够融合多模态输入和输出的大型模型，从而使我们更接近类人能力。然而，我们面临着两个主要障碍。首先，我们需要一个多模态标注数据集，而这并不像文本数据那样易于获取。其次，我们已经在推升训练文本数据模型的计算能力上到达极限。将这一计算能力扩展到包括其他模态，尤其是高维度的图像和视频，是一个极具挑战性的任务。
- en: These limitations have been a barrier for many AI researchers aiming to create
    capable multimodal models. So far, only a few well-established companies like
    Google, Meta, and OpenAI have managed to train such models. However, none of these
    prominent models are open source, and only a few APIs are available for public
    use. This has forced researchers, especially in academia, to find ways to build
    multimodal models without massive compute capabilities, relying instead on open-sourced
    pre-trained models, which are mostly single modal.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这些限制一直是许多AI研究人员创建强大多模态模型的障碍。到目前为止，只有少数几家知名公司，如Google、Meta和OpenAI，成功地训练了此类模型。然而，这些知名模型都不是开源的，仅提供少数API供公众使用。这迫使研究人员，尤其是学术界的研究人员，寻找在没有强大计算能力的情况下构建多模态模型的方法，而是依赖于开源的预训练模型，这些模型大多是单模态的。
- en: In this blog, we focus on successful, low-effort approaches to creating multi-modal
    models. Our criteria are centered on projects where the compute costs remain a
    few thousand dollars, assuming this is within the budget a typical lab can afford.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客中，我们重点讨论了成功且低成本的多模态模型创建方法。我们的标准集中在那些计算成本保持在几千美元以内的项目，假设这是一个典型实验室可以承担的预算。
- en: '***1- Parameter-Efficient Fine-Tuning (PEFT)***'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '***1- 参数高效微调（PEFT）***'
- en: Before we dive into the proposed approaches for integrating and aligning two
    pre-trained models, we need to discuss the mechanics of fine-tuning a large model
    with limited compute power. Therefore, we’ll start by exploring Parameter-Efficient
    Fine-Tuning (PEFT) and then describe how these methods can be further used to
    align pre-trained models and build open-source multimodal models.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨整合和对齐两个预训练模型的方法之前，我们需要讨论如何在有限计算能力下微调大模型的机制。因此，我们将从探索参数高效微调（PEFT）开始，然后描述这些方法如何进一步用于对齐预训练模型并构建开源的多模态模型。
- en: '![](../Images/f23534ce80c10c51bc3988dae122296c.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f23534ce80c10c51bc3988dae122296c.png)'
- en: Fig. 1\. Different PEFT methods (image from [paper](https://arxiv.org/pdf/2302.08106)
    ).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 不同的PEFT方法（图片来自[论文](https://arxiv.org/pdf/2302.08106)）。
- en: 'As model sizes continue to grow, the need for efficient fine-tuning methods
    becomes more critical. Fine-tuning all parameters in a large-scale pre-trained
    model is often impractical due to the substantial computational resources and
    time required. Parameter-efficient fine-tuning (PEFT) addresses this challenge
    by freezing the model’s parameters and only training the injected modules with
    a small number of parameters. Hence, only one copy of the large Transformer is
    stored with learned task specific lightweight PEFT modules, yielding a very small
    overhead for each additional task. This approach not only reduces resource demands
    but also accelerates the adaptation of models to new tasks, making it a practical
    and effective strategy in the era of ever-expanding models. PEFT approaches are
    very commonly used in LLMs and giant vision models and can be mainly divided into
    three categories as shown in Fig. 1: Among several methods that have been proposed,
    three have gotten significant attention from the community.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 随着模型规模的持续增长，对于高效微调方法的需求变得更加关键。由于大规模预训练模型微调所有参数通常不切实际，所需的计算资源和时间非常庞大，参数高效微调（PEFT）通过冻结模型的参数，仅训练注入的具有少量参数的模块，解决了这一挑战。因此，只存储一个大规模Transformer副本，并搭载经过学习的特定任务轻量级PEFT模块，从而为每个附加任务带来非常小的开销。这种方法不仅减少了资源需求，还加速了模型向新任务的适应，使其成为在模型不断扩展的时代中一种切实可行且有效的策略。PEFT方法在大语言模型（LLMs）和大型视觉模型中非常常见，主要可以分为三类，如图1所示：在已提出的几种方法中，有三种受到了社区的显著关注。
- en: '1- adapters: An adapter is essentially a small module, typically consisting
    of a downsample layer, nonlinearity, and an upsample layer with a skip connection
    to preserve the original input. This module is inserted into a pretrained model,
    with only the adapters being trained during fine-tuning.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 1- 适配器：适配器本质上是一个小模块，通常包括一个下采样层、非线性层和一个带有跳跃连接的上采样层，以保留原始输入。该模块被插入到预训练模型中，在微调过程中仅训练适配器。
- en: '2- LoRA injects trainable low-rank decomposition matrices into the model to
    approximate weight updates, significantly reducing the number of trainable parameters
    for downstream tasks. For a pre-trained weight matrix W of dimensions d×k, LoRA
    represents its update with a low-rank decomposition: W+ΔW=W+DU'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 2- LoRA通过将可训练的低秩分解矩阵注入模型来近似权重更新，从而显著减少下游任务的可训练参数数量。对于一个维度为d×k的预训练权重矩阵W，LoRA通过低秩分解表示其更新：W+ΔW=W+DU
- en: where D​ has dimensions d×r and U has dimensions r×k. These matrices D and U
    are the tunable parameters. LoRA can be applied to the attention matrices and/or
    the feedforward module for efficient finetuning.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，D的维度是d×r，U的维度是r×k。这些矩阵D和U是可调参数。LoRA可以应用于注意力矩阵和/或前馈模块，以实现高效的微调。
- en: 3- P*-tuning (prefix-tuning, prompt tuning) typically prepend a set of learnable
    prefix vectors or tokens to the input embedding, and only these so-called “soft
    prompts” are trained when fine-tuning on downstream tasks. The philosophy behind
    this approach is to assist the pre-trained models in understanding downstream
    tasks with the guidance of a sequence of extra “virtual tokens” information. Soft
    prompts are sequences of vectors that do not correspond to actual tokens in the
    vocabulary. Instead, they serve as intermediary representations that guide the
    model’s behavior to accomplish specific tasks, despite having no direct linguistic
    connection to the task itself.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 3- **P*-调优**（前缀调优，提示调优）通常将一组可学习的前缀向量或令牌附加到输入嵌入中，并且在下游任务的微调过程中，仅训练这些所谓的“软提示”。这种方法背后的理念是，通过一系列额外的“虚拟令牌”信息的指导，帮助预训练模型理解下游任务。软提示是与词汇表中的实际令牌无关的向量序列。相反，它们充当中介表示，指导模型的行为以完成特定任务，尽管它们与任务本身没有直接的语言学联系。
- en: '***Evaluating PEFT Techniques: Strengths and Limitations:***'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '***评估PEFT技术：优点与局限：***'
- en: '**Adapters** add a small number of parameters (3–4% of the total parameters)
    which makes them more efficient than full fine-tuning but less than prompt tuning
    or LoRA. However, they are capable of capturing complex task-specific information
    effectively due to the additional neural network layers and often achieve high
    performance on specific tasks by learning detailed task-specific features. On
    the downside, this approach makes the model deeper, which can complicate the optimization
    process and lead to longer training times.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**适配器**增加少量参数（占总参数的3-4%），使其比完全微调更高效，但低于提示调优或LoRA。然而，由于额外的神经网络层，它们能够有效地捕捉复杂的任务特定信息，并通过学习详细的任务特征，通常能够在特定任务上取得高性能。缺点是，这种方法使模型更深，这可能会使优化过程变得复杂，并导致更长的训练时间。'
- en: '**LoRa** adds only a small fraction of parameters (0.1% to 3%), making it highly
    efficient and scalable with very large models, making it suitable for adapting
    state-of-the-art LLMs and VLMs. However, LoRA’s adaptation is constrained to what
    can be expressed within the low-rank structure. While efficient, LoRA might be
    less flexible compared to adapters in capturing certain types of task-specific
    information.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**LoRA**仅添加少量参数（0.1%至3%），使其在处理非常大的模型时具有高度的效率和可扩展性，适用于调整最先进的LLM和VLM。然而，LoRA的适应性受到低秩结构的限制。尽管LoRA高效，但与适配器相比，它在捕捉某些类型的任务特定信息时可能灵活性较差。'
- en: '**P*- tuning** is extremely parameter-efficient (often requiring less than
    0.1%), as it only requires learning additional prompt tokens while keeping the
    original model parameters unchanged, thereby preserving the model’s generalization
    capabilities. However, it may not be able to capture complex task-specific information
    as effectively as other methods.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**P*-调优**在参数效率方面极为高效（通常只需要不到0.1%），因为它仅需要学习额外的提示令牌，而保持原始模型参数不变，从而保留模型的泛化能力。然而，它可能无法像其他方法那样有效地捕捉复杂的任务特定信息。'
- en: So far, we’ve reviewed new methods to fine-tune a large model with minimal compute
    power. This capability opens the door for us to combine two large models, each
    with billions of parameters, and fine-tune only a few million parameters to make
    them work together properly. This alignment allows one or both models to generate
    embeddings that are understandable by the other. Next, we’ll discuss three main
    approaches that demonstrate successful implementations of such a training regime.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经回顾了用最少计算资源微调大模型的新方法。这种能力为我们提供了将两个拥有数十亿参数的大模型结合起来的机会，并且只需微调几百万个参数就能使它们协调工作。这种对齐使得一个或两个模型能够生成对另一个模型可理解的嵌入。接下来，我们将讨论三种主要方法，展示了成功实现这种训练模式的例子。
- en: '***2.1 Prompt adaptation:***'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '***2.1 提示适应：***'
- en: '![](../Images/8747e0b6294956f4a19d280589019020.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8747e0b6294956f4a19d280589019020.png)'
- en: Fig. 2\. Early fusion of visual prompts and late fusion of adaptation prompts
    (image from [paper](https://arxiv.org/pdf/2304.15010))
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. 视觉提示的早期融合与适应提示的后期融合（图像来自[论文](https://arxiv.org/pdf/2304.15010)）
- en: '[**LLaMA-Adapter**](https://arxiv.org/pdf/2303.16199) presents a lightweight
    adaptation method to efficiently fine-tune the [LLaMA](https://arxiv.org/pdf/2302.13971)
    model into an instruction-following model. This is achieved by freezing the pre-trained
    [LLaMA](https://arxiv.org/pdf/2302.13971) 7B model and introducing a set of learnable
    adaptation prompts (1.2M parameters) into the topmost transformer layers. To avoid
    the initial instability and effectiveness issues caused by randomly initialized
    prompts, the adaptation prompts are zero-initialized. Additionally, a learnable
    zero-initialized gating factor is introduced to adaptively control the importance
    of the adaptation prompts.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[**LLaMA-Adapter**](https://arxiv.org/pdf/2303.16199)提出了一种轻量级的适应性方法，可以有效地将[LLaMA](https://arxiv.org/pdf/2302.13971)模型微调为一个跟随指令的模型。这是通过冻结预训练的[LLaMA](https://arxiv.org/pdf/2302.13971)
    7B模型，并在最上层的变换器层中引入一组可学习的适应性提示（1.2M参数）实现的。为了避免由于随机初始化提示而导致的初始不稳定性和效果问题，适应性提示被零初始化。此外，还引入了一个可学习的零初始化门控因子，用于自适应地控制适应性提示的重要性。'
- en: Furthermore, [**LLaMA-Adapter**](https://arxiv.org/pdf/2303.16199) extends to
    multi-modal tasks by integrating visual information using a pre-trained visual
    encoder such as CLIP. Given an image as visual context, the global visual features
    are acquired through multi-scale feature aggregation and then projected into the
    dimension of the LLM’s adaptation prompt via a learnable projection network. The
    resulting overall image token is repeated K times, and element-wisely added to
    the K-length adaptation prompts at all L inserted transformer layers. Fine-tuning
    with [**LLaMA-Adapter**](https://arxiv.org/pdf/2303.16199) takes less than one
    hour on 8 A100 GPUs. A similar approach is used in [**RobustGER**](https://arxiv.org/pdf/2401.10446),
    where LLMs are fine-tuned to perform denoising for generative error correction
    (GER) in automatic speech recognition. This process takes 1.5–4.5 hours of training
    on a single NVIDIA A40 GPU.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，[**LLaMA-Adapter**](https://arxiv.org/pdf/2303.16199)通过使用预训练的视觉编码器（如CLIP）集成视觉信息，扩展到多模态任务。在给定图像作为视觉上下文的情况下，通过多尺度特征聚合获取全局视觉特征，然后通过一个可学习的投影网络将这些特征投影到LLM的适应性提示的维度。最终得到的整体图像标记会被重复K次，并在所有插入的K长度适应性提示的变换器层中逐元素相加。使用[**LLaMA-Adapter**](https://arxiv.org/pdf/2303.16199)进行微调时，在8个A100
    GPU上耗时不到一小时。在[**RobustGER**](https://arxiv.org/pdf/2401.10446)中也使用了类似的方法，LLM被微调用于执行自动语音识别中的生成错误修正（GER）去噪。这个过程在单个NVIDIA
    A40 GPU上训练需要1.5到4.5小时。
- en: '[**LLaMA-Adapter V2**](https://arxiv.org/pdf/2304.15010) focuses on instruction-following
    vision models that can also generalize well on open-ended visual instructions.
    To achieve this goal, three key improvements are presented over the original [LLaMA-Adapter](https://arxiv.org/pdf/2303.16199).
    First, it introduces more learnable parameters (14M) by unfreezing all the normalization
    layers in [LLaMA](https://arxiv.org/pdf/2302.13971) and adding a learnable bias
    and scale factor to all linear layers in the transformer, which distributes the
    instruction-following capability across the entire model. Second, visual tokens
    are fed into the early layers of the language model, while the adaptation prompts
    are added to the top layers. This improves the integration of visual knowledge
    without disrupting the model’s instruction-following abilities (see Fig. 2). Third,
    a joint training paradigm for both image-text captioning data and language-only
    instruction data is employed. The visual projection layers are trained for image-text
    captioning data while the late adaptation prompts and the unfrozen norms are trained
    from the instruction-following data. Additionally, expert models like captioning
    and OCR systems are integrated during inference, enhancing image understanding
    without additional training costs. We weren’t able to find specific details on
    GPU requirements and the time needed for training. However, based on information
    from GitHub, it takes approximately 100 hours on a single A100 GPU.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[**LLaMA-Adapter V2**](https://arxiv.org/pdf/2304.15010)专注于能够在开放式视觉指令上良好泛化的指令跟随视觉模型。为实现这一目标，相较于原始的[**LLaMA-Adapter**](https://arxiv.org/pdf/2303.16199)，提出了三项关键改进。首先，通过解冻[**LLaMA**](https://arxiv.org/pdf/2302.13971)中的所有归一化层，并为转换器中的所有线性层添加可学习的偏置和缩放因子，引入了更多可学习参数（14M），这将指令跟随能力分布到整个模型。其次，视觉标记被输入到语言模型的早期层，而适应性提示被添加到顶部层。这在不破坏模型指令跟随能力的情况下改善了视觉知识的整合（见图2）。第三，采用了图像-文本标注数据和仅语言指令数据的联合训练范式。视觉投影层通过图像-文本标注数据进行训练，而晚期适应性提示和解冻的归一化层则通过指令跟随数据进行训练。此外，在推理过程中集成了专家模型，如标注和OCR系统，增强了图像理解而无需额外的训练成本。我们无法找到有关GPU要求和训练所需时间的具体细节。然而，根据GitHub上的信息，在单个A100
    GPU上大约需要100小时。'
- en: '***2.2 Intermediate Module Training:***'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '***2.2 中间模块训练：***'
- en: '![](../Images/45e1e925a345f38d407df0488f1ce2df.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/45e1e925a345f38d407df0488f1ce2df.png)'
- en: Fig. 3\. Overview of how intermediate module training is working (image by authors)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图3. 中间模块训练工作原理概览（图片来自作者）
- en: 'To create a multi-modal model, two or more unimodal foundation models can be
    connected through a learnable projection module. This module maps features from
    one modality to another, enabling the integration of different data types. For
    instance, a vision encoder can be connected to a large language model (LLM) via
    a projection module. Hence, as illustrated in Fig. 3, the LLM’s input consists
    of a sequence of projected image features and text. The training process typically
    involves two stages:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建一个多模态模型，可以通过一个可学习的投影模块将两个或更多单模态基础模型连接起来。该模块将一个模态的特征映射到另一个模态，从而实现不同数据类型的集成。例如，可以通过投影模块将视觉编码器连接到大型语言模型（LLM）。因此，如图3所示，LLM的输入由一系列投影的图像特征和文本组成。训练过程通常包括两个阶段：
- en: '**Pretraining**: The projection module is pretrained on a large dataset of
    paired examples to achieve cross-modality alignment.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预训练**：投影模块在一大规模配对数据集上进行预训练，以实现跨模态对齐。'
- en: '**Fine-tuning**: The projection module (with one or more unimodal models) is
    fine-tuned for specific downstream tasks, such as instruction-following tasks.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**微调**：投影模块（具有一个或多个单模态模型）针对特定的下游任务进行微调，例如指令跟随任务。'
- en: '[**MiniGPT-4**](https://arxiv.org/pdf/2304.10592), aligns a frozen visual encoder,
    [ViT-G/14](https://openaccess.thecvf.com/content/CVPR2023/papers/Fang_EVA_Exploring_the_Limits_of_Masked_Visual_Representation_Learning_at_CVPR_2023_paper.pdf),
    with a frozen LLM, [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/), using
    one projection layer. For visual encoder, the same pretrained visual perception
    component of [BLIP-2](https://arxiv.org/pdf/2301.12597) is utilized which consists
    of a [ViT-G/14](https://openaccess.thecvf.com/content/CVPR2023/papers/Fang_EVA_Exploring_the_Limits_of_Masked_Visual_Representation_Learning_at_CVPR_2023_paper.pdf)
    and Q-former network. [**MiniGPT-4**](https://arxiv.org/pdf/2304.10592) adds a
    single learnable projection layer where its output is considered as a soft prompt
    for the LLM in the following format:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[**MiniGPT-4**](https://arxiv.org/pdf/2304.10592)将冻结的视觉编码器[ViT-G/14](https://openaccess.thecvf.com/content/CVPR2023/papers/Fang_EVA_Exploring_the_Limits_of_Masked_Visual_Representation_Learning_at_CVPR_2023_paper.pdf)与冻结的LLM
    [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/)对齐，使用一个投影层。对于视觉编码器，采用与[BLIP-2](https://arxiv.org/pdf/2301.12597)相同的预训练视觉感知组件，该组件包括[ViT-G/14](https://openaccess.thecvf.com/content/CVPR2023/papers/Fang_EVA_Exploring_the_Limits_of_Masked_Visual_Representation_Learning_at_CVPR_2023_paper.pdf)和Q-former网络。[**MiniGPT-4**](https://arxiv.org/pdf/2304.10592)添加了一个可学习的投影层，其输出被视为LLM的软提示，格式如下：'
- en: '“###Human: <Img><ImageFeatureFromProjectionLayer></Img> TextTokens. ###Assistant:”.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '“###Human: <Img><ImageFeatureFromProjectionLayer></Img> TextTokens. ###Assistant:”.'
- en: Training the projection layer involves two stages. First, pretrain the projection
    layer on a large dataset of aligned image-text pairs to acquire vision-language
    knowledge. Then, fine-tune the linear projection layer with a smaller, high-quality
    dataset. In both stages, all other parameters are frozen. As a result, [**MiniGPT-4**](https://arxiv.org/pdf/2304.10592)
    is capable of producing more natural and reliable language outputs. [**MiniGPT-4**](https://arxiv.org/pdf/2304.10592)
    requires training approximately 10 hours on 4 A100 GPUs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 训练投影层涉及两个阶段。首先，在一个大型的图像-文本配对数据集上对投影层进行预训练，以获得视觉-语言知识。然后，使用一个较小且高质量的数据集对线性投影层进行微调。在这两个阶段中，所有其他参数保持冻结。因此，[**MiniGPT-4**](https://arxiv.org/pdf/2304.10592)能够生成更自然、更可靠的语言输出。[**MiniGPT-4**](https://arxiv.org/pdf/2304.10592)的训练需要大约在4个A100
    GPU上训练10小时。
- en: Tuning the LLMs to follow instructions using machine-generated instruction-following
    data has been shown to improve zero-shot capabilities on new tasks. To explore
    this idea in the multimodality field, [**LLaVA**](https://proceedings.neurips.cc/paper_files/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf)
    connects LLM [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/), with a vision
    encoder, [ViT-L/14](https://arxiv.org/pdf/2103.00020), using a single linear layer
    for vision-language instruction following tasks. In the first stage, the projection
    layer is trained on a large image-text pairs dataset while the visual encoder
    and LLM weights are kept frozen. This stage creates a compatible visual tokenizer
    for the frozen LLM. In the second stage, the pre-trained projection layer and
    LLM weights are fine-tuned using a high-quality generated dataset of language-image
    instruction-following data. This stage enhances the model’s ability to follow
    multimodal instructions and perform specific tasks. [**LLaVA**](https://proceedings.neurips.cc/paper_files/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf)
    uses 8× A100s GPUs. The pretraining takes 4 hours, and the fine-tuning takes 4–8
    hours depending on the specific task dataset. It showcases commendable proficiency
    in visual reasoning capabilities, although it falls short on academic benchmarks
    requiring short-form answers.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 使用机器生成的指令跟随数据对LLM进行微调，已被证明能够提高其在新任务上的零-shot能力。为了在多模态领域探索这一思路，[**LLaVA**](https://proceedings.neurips.cc/paper_files/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf)将LLM
    [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/)与视觉编码器[ViT-L/14](https://arxiv.org/pdf/2103.00020)连接，通过一个单独的线性层来处理视觉-语言指令跟随任务。在第一阶段，投影层在一个大型图像-文本配对数据集上进行训练，同时冻结视觉编码器和LLM的权重。此阶段创建了一个兼容冻结LLM的视觉标记器。在第二阶段，预训练的投影层和LLM权重使用一个高质量生成的数据集进行微调，这个数据集包含语言-图像指令跟随数据。此阶段增强了模型处理多模态指令并执行特定任务的能力。[**LLaVA**](https://proceedings.neurips.cc/paper_files/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf)使用了8×
    A100 GPU。预训练需要4小时，微调需要4到8小时，具体取决于任务数据集。它在视觉推理能力上展示了值得称赞的熟练度，尽管在需要简短答案的学术基准测试中表现不足。
- en: 'To improve the performance of [**LLaVA**](https://proceedings.neurips.cc/paper_files/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf),
    in [**LLaVa-1.5**](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Improved_Baselines_with_Visual_Instruction_Tuning_CVPR_2024_paper.pdf):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高 [**LLaVA**](https://proceedings.neurips.cc/paper_files/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf)
    的性能，在 [**LLaVa-1.5**](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Improved_Baselines_with_Visual_Instruction_Tuning_CVPR_2024_paper.pdf)
    中：
- en: A two-layer MLP is added to connect the LLM to the vision encoder.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加了一个两层的 MLP 来将 LLM 与视觉编码器连接起来。
- en: The input image resolution has been scaled up using CLIP-ViT-L-336px, allowing
    for better detail perception.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入图像分辨率通过使用 CLIP-ViT-L-336px 被放大，从而提高了细节感知能力。
- en: The [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/) model has been scaled
    up to 13B parameters.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/) 模型已经扩展到 13B 参数。'
- en: Academic-task-oriented VQA data has been added, along with specific response
    formatting prompts to indicate the desired output format. When prompting for short-form
    answers, the prompt “Answer the question using a single word or phrase.” is appended
    to the VQA question.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加了面向学术任务的 VQA 数据，并包含了特定的响应格式提示，用于指示期望的输出格式。当提示简短的答案时，VQA 问题会附加上提示语：“用一个单词或短语回答问题。”
- en: The training finishes in ∼1 day on a single 8-A100 GPU and achieves state-of-the-art
    results on a wide range of benchmarks.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程大约在 1 天内完成，使用单个 8-A100 GPU，并在多个基准测试中取得了最先进的结果。
- en: '![](../Images/5d98bc0d070476023387b41945ac6d9d.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d98bc0d070476023387b41945ac6d9d.png)'
- en: Fig. 4\. Architecture of Video-ChatGPT (image from [paper](https://arxiv.org/pdf/2306.05424))
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. 视频-ChatGPT 的架构（图片来自 [论文](https://arxiv.org/pdf/2306.05424)）
- en: '[**Video-ChatGPT**](https://arxiv.org/pdf/2306.05424) focuses on creating a
    video-based conversational agent. Given the limited availability of video-caption
    pairs and the substantial resources required for training from scratch, it uses
    the pretrained image-based visual encoder, CLIP [ViT-L/14](https://arxiv.org/pdf/2103.00020)
    for video tasks and connect it with pretrained LLM [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/)
    through a learnable linear projection model. [ViT-L/14](https://arxiv.org/pdf/2103.00020)
    encodes images, so for a given video sample with T frames, it generates T frame-level
    embeddings with dimensions h*w*D. As illustrated in Fig. 4\. The process of obtaining
    video-level features involves two key steps:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[**Video-ChatGPT**](https://arxiv.org/pdf/2306.05424) 旨在创建基于视频的对话代理。鉴于视频-字幕对的稀缺性以及从零开始训练所需的巨大资源，它采用了预训练的基于图像的视觉编码器
    CLIP [ViT-L/14](https://arxiv.org/pdf/2103.00020) 来处理视频任务，并通过可学习的线性投影模型将其与预训练的
    LLM [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/) 连接。由于 [ViT-L/14](https://arxiv.org/pdf/2103.00020)
    编码的是图像，因此对于一个包含 T 帧的视频样本，它会生成 T 个帧级别的嵌入，维度为 h*w*D。正如图 4 所示，获得视频级特征的过程包括两个关键步骤：'
- en: '**Spatial Video Features**: These are obtained by average-pooling frame-level
    features across the temporal dimension to achieve an *h*w*D* dimension.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**空间视频特征**：通过在时间维度上进行平均池化帧级特征，获得 *h*w*D* 维度的特征。'
- en: '**Temporal Video Features**: These are obtained by average-pooling across the
    spatial dimension to achieve *T*D* dimensions.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间视频特征**：通过在空间维度上进行平均池化，获得 *T*D* 维度的特征。'
- en: These temporal and spatial features are concatenated to form video-level features,
    which are then projected into the textual embedding space by a learnable linear
    layer. The model is trained on their curated, high-quality dataset of video-text
    pairs, and the training of the linear projection layer takes around 3 hours on
    8 A100 40GB GPUs. This approach allows [**Video-ChatGPT**](https://arxiv.org/pdf/2306.05424)
    to generate detailed and coherent conversations about video content.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这些时间和空间特征被连接在一起，形成视频级特征，然后通过一个可学习的线性层投影到文本嵌入空间。该模型在经过精心策划的高质量视频-文本对数据集上训练，线性投影层的训练大约需要
    3 小时，使用 8 个 A100 40GB GPU。该方法使得 [**Video-ChatGPT**](https://arxiv.org/pdf/2306.05424)
    能够生成关于视频内容的详细且连贯的对话。
- en: '![](../Images/c9d5e22d243e4c0ea39f2e47e0d76609.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c9d5e22d243e4c0ea39f2e47e0d76609.png)'
- en: Fig. 5\. llustration of PandaGPT (image from [paper](https://arxiv.org/pdf/2305.16355))
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. PandaGPT 的示意图（图片来自 [论文](https://arxiv.org/pdf/2305.16355)）
- en: '[**PandaGPT**](https://arxiv.org/pdf/2305.16355), while not connecting unimodal
    models, introduces the first general-purpose model capable of instruction-following
    by integrating the pretrained LLM [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/)
    with the multimodal encoder [ImageBind](https://arxiv.org/pdf/2305.05665) through
    a linear projection layer (see Fig. 5). The linear projection layer is trained,
    and [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/)’s attention modules are
    fine-tuned using LoRA on 8×A100 40G GPUs for 7 hours, leveraging only image-language
    (multi-turn conversation) instruction-following data. Despite being trained exclusively
    on image-text pairs, [PandaGPT](https://arxiv.org/pdf/2305.16355) exhibits emergent,
    zero-shot, cross-modal capabilities across multiple modalities by leveraging the
    binding property across six modalities (image/video, text, audio, depth, thermal,
    and IMU) inherited from the frozen [ImageBind](https://arxiv.org/pdf/2305.05665)
    encoders. This enables PandaGPT to excel in tasks such as image/video-grounded
    question answering, image/video-inspired creative writing, visual and auditory
    reasoning, and more.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[**PandaGPT**](https://arxiv.org/pdf/2305.16355)虽然没有连接单模态模型，但通过将预训练的大型语言模型[Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/)与多模态编码器[ImageBind](https://arxiv.org/pdf/2305.05665)通过线性投影层集成，首次推出了能够进行指令跟随的通用模型（见图5）。线性投影层是经过训练的，[Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/)的注意力模块使用LoRA在8×A100
    40G GPUs上进行微调，训练时间为7小时，仅使用图像-语言（多轮对话）指令跟随数据。尽管只在图像-文本对上进行训练，[PandaGPT](https://arxiv.org/pdf/2305.16355)通过利用来自冻结的[ImageBind](https://arxiv.org/pdf/2305.05665)编码器的六种模态（图像/视频、文本、音频、深度、热图和IMU）之间的绑定特性，表现出了新兴的零样本跨模态能力。这使得PandaGPT在图像/视频基础的问答、图像/视频启发的创意写作、视觉和听觉推理等任务中表现出色。'
- en: '***2.3 Adapter Mixture:***'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '***2.3 适配器混合:***'
- en: '![](../Images/9b16151683a668fa33079bfa3b6cdb3f.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b16151683a668fa33079bfa3b6cdb3f.png)'
- en: Fig. 6\. The overview of the Mixture-of-Modality Adaptation (MMA) (image by
    authors)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图6\. 多模态适配适应（MMA）概述（图片由作者提供）
- en: '[**Cheap&Quick**](https://proceedings.neurips.cc/paper_files/paper/2023/file/5e84e4413268b713f0d4a1b23a9dae57-Paper-Conference.pdf)
    adopts lightweight adapters to integrate large language models (LLMs) and vision
    models for vision-language tasks. The paper proposes a Mixture-of-Modality Adapters
    (MMA), designed to facilitate switching between single- and multi-modal instructions
    without compromising performance. A learnable token *t* is proposed as the modality
    selector token. This token indicates the input features’ modality (i.e., unimodal
    or multimodal input) and informs the router module on how to combine the output
    of the learned adapters, as illustrated in Fig 6\. The adapter is formulated as
    :'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[**Cheap&Quick**](https://proceedings.neurips.cc/paper_files/paper/2023/file/5e84e4413268b713f0d4a1b23a9dae57-Paper-Conference.pdf)采用轻量级适配器来集成大型语言模型（LLM）和视觉模型，处理视觉-语言任务。该论文提出了一种多模态适配器混合（MMA），旨在促进单模态和多模态指令之间的切换，同时不影响性能。提出了一种可学习的标记*t*作为模态选择器标记。这个标记指示输入特征的模态（即单模态或多模态输入），并向路由模块提供如何结合学习到的适配器输出的信息，如图6所示。适配器的公式为：'
- en: Z′=Z+s⋅router(f(Z),g(Z); t)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Z′=Z+s⋅router(f(Z),g(Z); t)
- en: where Z is the input features, either unimodal or concatenated multimodal (image-text)
    features. Modules *f* and *g* share a common [unimodal adapter](https://arxiv.org/pdf/2302.08106)
    architecture. s is a scaling factor, and the router(⋅) function determines the
    routing path based on the modality token *t*.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 其中Z是输入特征，可以是单模态或连接的多模态（图像-文本）特征。模块*f*和*g*共享一个通用的[单模态适配器](https://arxiv.org/pdf/2302.08106)架构。s是一个缩放因子，路由器(⋅)函数根据模态标记*t*确定路由路径。
- en: To demonstrate the effectiveness of MMA, the authors connected [LLaMA](https://arxiv.org/pdf/2302.13971)
    and [CLIP-ViT](https://arxiv.org/pdf/2103.00020) with a single linear layer and
    inserted MMA into both ViT and [LLaMA](https://arxiv.org/pdf/2302.13971) before
    the multi-head attention modules. The adapters and projection layer (only 3.8M
    parameters) were trained with a mixture of text-only and text-image data on 8
    A100 GPUs for 1.4 hours. This approach showed a significant reduction in training
    costs while maintaining high performance on vision-language tasks.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证MMA的有效性，作者通过一个线性层将[LLaMA](https://arxiv.org/pdf/2302.13971)和[CLIP-ViT](https://arxiv.org/pdf/2103.00020)连接，并将MMA插入到ViT和[LLaMA](https://arxiv.org/pdf/2302.13971)的多头注意力模块之前。适配器和投影层（仅3.8M参数）使用文本-图像数据和纯文本数据的混合，在8个A100
    GPUs上训练了1.4小时。该方法显著降低了训练成本，同时在视觉-语言任务中保持了高性能。
- en: '***2.4 A Modality as Grounding Without Training*** Up to this point, we have
    discussed papers that connect unimodal models to create a multimodal model. However,
    advancing toward AGI requires a multimodal model capable of handling data from
    different modalities for diverse tasks, ranging from calculus to generating images
    based on descriptions.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '***2.4 作为基础的模态，无需训练*** 到目前为止，我们讨论了将单一模态模型连接起来创建多模态模型的论文。然而，迈向通用人工智能（AGI）需要一个能够处理来自不同模态数据的多模态模型，以应对各种任务，从微积分到基于描述生成图像。'
- en: Recently, many papers have explored integrating pre-trained multi-modal models
    via language prompting into a unified model capable of handling various tasks
    across different modalities without additional training. In this approach, language
    serves as an intermediary for models to exchange information. Through prompt engineering
    (e.g., [**Visual ChatGPT**](https://arxiv.org/pdf/2303.04671),[**MM-REACT**](https://arxiv.org/pdf/2303.11381))
    or fine-tuning (e.g., [**Toolformer**](https://proceedings.neurips.cc/paper_files/paper/2023/file/d842425e4bf79ba039352da0f658a906-Paper-Conference.pdf),[**GPT4Tools**](https://proceedings.neurips.cc/paper_files/paper/2023/file/e393677793767624f2821cec8bdd02f1-Paper-Conference.pdf)),
    LLMs can invoke specialized foundation models to handle various modality-specific
    tasks. While this topic is beyond the scope of our current blog post, you can
    refer to these papers for more detailed information.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，许多论文探讨了通过语言提示将预训练的多模态模型集成到一个统一模型中的方法，该模型能够在不进行额外训练的情况下处理跨不同模态的各种任务。在这种方法中，语言作为模型之间交换信息的中介。通过提示工程（例如，[**Visual
    ChatGPT**](https://arxiv.org/pdf/2303.04671)，[**MM-REACT**](https://arxiv.org/pdf/2303.11381)）或微调（例如，[**Toolformer**](https://proceedings.neurips.cc/paper_files/paper/2023/file/d842425e4bf79ba039352da0f658a906-Paper-Conference.pdf)，[**GPT4Tools**](https://proceedings.neurips.cc/paper_files/paper/2023/file/e393677793767624f2821cec8bdd02f1-Paper-Conference.pdf)），大型语言模型（LLMs）可以调用专门的基础模型来处理各种模态特定的任务。虽然这个话题超出了我们当前博客文章的范围，但你可以参考这些论文以获取更详细的信息。
- en: In another similar work, [**MAGIC**](https://arxiv.org/pdf/2205.02655) proposes
    a novel, training-free, plug-and-play framework and uses image embedding, through
    pre-trained CLIP, as the grounding foundation. This framework connects GPT-2 with
    CLIP to perform image-grounded text generation (e.g., image captioning) in a zero-shot
    manner. By incorporating the similarity between the image embeddings from CLIP
    and the top-k generated tokens from a pre-trained LLM at each time step into the
    decoding inference, the model effectively leverages visual information to guide
    text generation. Without any additional training, this approach demonstrates the
    capability to generate visually grounded stories given both an image and a text
    prompt.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一项类似的工作中，[**MAGIC**](https://arxiv.org/pdf/2205.02655)提出了一种新颖的、无需训练的即插即用框架，并通过预训练的CLIP将图像嵌入作为基础来进行支撑。该框架将GPT-2与CLIP连接起来，以零样本方式执行基于图像的文本生成（例如，图像标题生成）。通过将CLIP图像嵌入与预训练的大型语言模型（LLM）在每个时间步生成的top-k词汇的相似度融入解码推理中，模型能够有效地利用视觉信息来引导文本生成。无需任何额外的训练，这种方法展示了在给定图像和文本提示的情况下生成视觉支撑故事的能力。
- en: '***3\. High quality Curated data:***'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '***3. 高质量策划数据：***'
- en: We have discussed various methods of aligning different modalities up to this
    point; however, it is important to remember that having curated, high-quality
    data in such training regimes is equally crucial. For instance, detailed and accurate
    instructions and responses significantly enhance the zero-shot performance of
    large language models on interactive natural language tasks. In the field of interactive
    vision-language tasks, the availability of high-quality data is often limited,
    prompting researchers to develop innovative methods for generating such data.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们到目前为止讨论了对齐不同模态的各种方法；然而，重要的是要记住，在这种训练机制中拥有精心策划的高质量数据同样至关重要。例如，详细且准确的指令和回应能够显著提升大型语言模型在互动自然语言任务中的零样本表现。在互动视觉语言任务领域，高质量数据的可用性往往有限，这促使研究人员开发创新的方法来生成此类数据。
- en: '[**MiniGPT-4**](https://arxiv.org/pdf/2304.10592) proposes a two-stage method
    to curate a detailed, instruction-following image description dataset:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[**MiniGPT-4**](https://arxiv.org/pdf/2304.10592)提出了一种两阶段的方法，用于策划一个详细的、能够遵循指令的图像描述数据集：'
- en: '1-**Data Generation**: The pre-trained model from the first stage of training
    is used to generate detailed descriptions. For a given image, a carefully crafted
    prompt is used to enable the pre-trained model to produce detailed and informative
    image descriptions in multiple steps,'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 1-**数据生成**：第一阶段训练中的预训练模型用于生成详细的描述。对于给定的图像，使用精心设计的提示词，使预训练模型能够分步生成详细且富有信息的图像描述。
- en: '2- **Post-Processing and Filtering**: The generated image descriptions contain
    noisy or incoherent descriptions. In order to fix these issues, ChatGPT is employed
    to refine the generated descriptions according to specific post-processing requirements
    and standards, guided by a designed prompt. The refined dataset is then manually
    verified to ensure the correctness and quality of the image-text pairs.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 2- **后处理与过滤**：生成的图像描述可能包含噪声或不连贯的描述。为了修正这些问题，采用ChatGPT根据特定的后处理要求和标准来优化生成的描述，并通过设计的提示词进行引导。然后，经过优化的数据集将手动验证，以确保图像-文本对的正确性和质量。
- en: '[**LLaVA**](https://proceedings.neurips.cc/paper_files/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf)
    proposes a method to generate multimodal instruction-following data by querying
    ChatGPT/GPT-4 based on widely available image-text pair data. They designed a
    prompt that consists of an image caption, bounding boxes to localize objects in
    the scene, and a few examples for in-context learning. This method leverages the
    existing rich dataset and the capabilities of ChatGPT/GPT-4 to produce highly
    detailed and accurate multimodal data.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[**LLaVA**](https://proceedings.neurips.cc/paper_files/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf)提出了一种通过基于广泛可用的图像-文本配对数据查询ChatGPT/GPT-4生成多模态指令跟随数据的方法。他们设计了一种提示词，包含图像字幕、定位场景中物体的边界框以及几个用于上下文学习的示例。这种方法利用了现有的丰富数据集和ChatGPT/GPT-4的能力，生成高度详细和准确的多模态数据。'
- en: '[**Video-ChatGPT**](https://arxiv.org/pdf/2306.05424) utilized two approaches
    for generating high-quality video instruction data.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[**Video-ChatGPT**](https://arxiv.org/pdf/2306.05424)采用了两种方法来生成高质量的视频指令数据。'
- en: '1- **Human-Assisted Annotation**: Expert annotators enrich given video-caption
    pairs by adding comprehensive details to the captions'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 1- **人工辅助注释**：专家注释员通过向字幕添加全面的细节来丰富给定的视频-字幕对。
- en: '2- **Semi-Automatic Annotation**: This involves a multi-step process leveraging
    several pretrained models:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 2- **半自动注释**：这一过程涉及利用多个预训练模型的多步骤过程：
- en: Pretrained [BLIP-2](https://arxiv.org/pdf/2301.12597) and [GRiT](https://arxiv.org/pdf/2212.00280)
    models are used for analyzing key frames in the videos. [BLIP-2](https://arxiv.org/pdf/2301.12597)
    generates frame-level captions, while [GRiT](https://arxiv.org/pdf/2212.00280)
    provides detailed descriptions of scene objects. Additionally, the pretrained
    [Tag2Text](https://arxiv.org/pdf/2303.05657) model generates tags for each key
    frame of the video.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预训练的[BLIP-2](https://arxiv.org/pdf/2301.12597)和[GRiT](https://arxiv.org/pdf/2212.00280)模型用于分析视频中的关键帧。[BLIP-2](https://arxiv.org/pdf/2301.12597)生成帧级别的字幕，而[GRiT](https://arxiv.org/pdf/2212.00280)提供场景对象的详细描述。此外，预训练的[Tag2Text](https://arxiv.org/pdf/2303.05657)模型为视频的每个关键帧生成标签。
- en: A specialized filtering mechanism is employed to remove any captions from [BLIP-2](https://arxiv.org/pdf/2301.12597)
    or [GRiT](https://arxiv.org/pdf/2212.00280) that do not match the [Tag2Text](https://arxiv.org/pdf/2303.05657)
    frame-level tags.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用专门的过滤机制来移除任何来自[BLIP-2](https://arxiv.org/pdf/2301.12597)或[GRiT](https://arxiv.org/pdf/2212.00280)的、不符合[Tag2Text](https://arxiv.org/pdf/2303.05657)帧级标签的字幕。
- en: The GPT-3.5 model is used to merge the filtered captions and generate a singular,
    coherent video-level caption.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GPT-3.5模型用于合并过滤后的字幕并生成一个统一且连贯的视频级字幕。
- en: '[**MIMIC-IT**](https://arxiv.org/pdf/2306.05425) generated a dataset of 2.8
    million multimodal instruction-response pairs, aimed at enhancing Vision-Language
    Models (VLMs) in perception, reasoning, and planning. To demonstrate the importance
    of high-quality data, they fine-tuned [OpenFlamingo](https://arxiv.org/pdf/2308.01390)
    using the MIMIC-IT dataset on 8 A100 GPUs over 3 epochs in one day. The resulting
    model outperforms [OpenFlamingo](https://arxiv.org/pdf/2308.01390), demonstrating
    superior in-context and zero-shot learning capabilities.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[**MIMIC-IT**](https://arxiv.org/pdf/2306.05425)生成了一个包含280万多模态指令-响应对的数据集，旨在增强视觉语言模型（VLMs）在感知、推理和规划方面的能力。为了展示高质量数据的重要性，他们使用MIMIC-IT数据集，在一天内使用8个A100
    GPU，经过3个周期的微调[OpenFlamingo](https://arxiv.org/pdf/2308.01390)。最终模型优于[OpenFlamingo](https://arxiv.org/pdf/2308.01390)，展现了更强的上下文学习和零-shot学习能力。'
- en: The opinions expressed in this blog post are solely our own and do not reflect
    those of our employer.
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本文所表达的观点仅代表我们个人意见，并不反映我们雇主的立场。
- en: '***References:***'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '***参考文献:***'
- en: '[1] [**LLaMA-Adapter**](https://arxiv.org/pdf/2303.16199): Zhang, Renrui, et
    al. “Llama-adapter: Efficient fine-tuning of language models with zero-init attention.”
    (2023).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [**LLaMA-Adapter**](https://arxiv.org/pdf/2303.16199): Zhang, Renrui, 等.
    “Llama-adapter: Efficient fine-tuning of language models with zero-init attention.”
    (2023).'
- en: '[2] [**LLaMA-Adapter V2**](https://arxiv.org/pdf/2304.15010): Gao, Peng, et
    al. “Llama-adapter v2: Parameter-efficient visual instruction model.” (2023).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [**LLaMA-Adapter V2**](https://arxiv.org/pdf/2304.15010): Gao, Peng, 等.
    “Llama-adapter v2: Parameter-efficient visual instruction model.” (2023).'
- en: '[3] [**MiniGPT-4**](https://arxiv.org/pdf/2304.10592): Zhu, Deyao, et al. “Minigpt-4:
    Enhancing vision-language understanding with advanced large language models.”
    (2023).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [**MiniGPT-4**](https://arxiv.org/pdf/2304.10592): Zhu, Deyao, 等. “Minigpt-4:
    Enhancing vision-language understanding with advanced large language models.”
    (2023).'
- en: '[4] [**LLaVA**](https://proceedings.neurips.cc/paper_files/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf):
    Liu, Haotian, et al. “Visual instruction tuning.” (2024).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [**LLaVA**](https://proceedings.neurips.cc/paper_files/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf):
    Liu, Haotian, 等. “Visual instruction tuning.” (2024).'
- en: '[5] [**LLaVa-1.5**](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Improved_Baselines_with_Visual_Instruction_Tuning_CVPR_2024_paper.pdf):
    Liu, Haotian, et al. “Improved baselines with visual instruction tuning.” (2024).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [**LLaVa-1.5**](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Improved_Baselines_with_Visual_Instruction_Tuning_CVPR_2024_paper.pdf):
    Liu, Haotian, 等. “Improved baselines with visual instruction tuning.” (2024).'
- en: '[6] [**Video-ChatGPT**](https://arxiv.org/pdf/2306.05424): Maaz, Muhammad,
    et al. “Video-chatgpt: Towards detailed video understanding via large vision and
    language models.” (2023).'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] [**Video-ChatGPT**](https://arxiv.org/pdf/2306.05424): Maaz, Muhammad,
    等. “Video-chatgpt: Towards detailed video understanding via large vision and language
    models.” (2023).'
- en: '[7] [**PandaGPT**](https://arxiv.org/pdf/2305.16355): Su, Yixuan, et al. “Pandagpt:
    One model to instruction-follow them all.” (2023).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] [**PandaGPT**](https://arxiv.org/pdf/2305.16355): Su, Yixuan, 等. “Pandagpt:
    One model to instruction-follow them all.” (2023).'
- en: '[8] [**Cheap&Quick**](https://proceedings.neurips.cc/paper_files/paper/2023/file/5e84e4413268b713f0d4a1b23a9dae57-Paper-Conference.pdf):
    Luo, Gen, et al. “Cheap and quick: Efficient vision-language instruction tuning
    for large language models.” (2024).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] [**Cheap&Quick**](https://proceedings.neurips.cc/paper_files/paper/2023/file/5e84e4413268b713f0d4a1b23a9dae57-Paper-Conference.pdf):
    Luo, Gen, 等. “Cheap and quick: Efficient vision-language instruction tuning for
    large language models.” (2024).'
- en: '[9] [**RobustGER**](https://arxiv.org/pdf/2401.10446): Hu, Yuchen, et al. “Large
    Language Models are Efficient Learners of Noise-Robust Speech Recognition.” (2024).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] [**RobustGER**](https://arxiv.org/pdf/2401.10446): Hu, Yuchen, 等. “Large
    Language Models are Efficient Learners of Noise-Robust Speech Recognition.” (2024).'
- en: '[10] [**MAGIC**](https://arxiv.org/pdf/2205.02655): Su, Yixuan, et al. “Language
    models can see: Plugging visual controls in text generation.” (2022).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] [**MAGIC**](https://arxiv.org/pdf/2205.02655): Su, Yixuan, 等. “Language
    models can see: Plugging visual controls in text generation.” (2022).'
- en: '[11] [**Visual ChatGPT**](https://arxiv.org/pdf/2303.04671): Wu, Chenfei, et
    al. “Visual chatgpt: Talking, drawing and editing with visual foundation models.”
    (2023).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] [**Visual ChatGPT**](https://arxiv.org/pdf/2303.04671): Wu, Chenfei, 等.
    “Visual chatgpt: Talking, drawing and editing with visual foundation models.”
    (2023).'
- en: '[12] [**MM-REACT**](https://arxiv.org/pdf/2303.11381): Yang, Zhengyuan, et
    al. “Mm-react: Prompting chatgpt for multimodal reasoning and action.” (2023).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] [**MM-REACT**](https://arxiv.org/pdf/2303.11381): Yang, Zhengyuan, 等.
    “Mm-react: Prompting chatgpt for multimodal reasoning and action.” (2023).'
- en: '[13] [**Toolformer**](https://proceedings.neurips.cc/paper_files/paper/2023/file/d842425e4bf79ba039352da0f658a906-Paper-Conference.pdf):
    Schick, Timo, et al. “Toolformer: Language models can teach themselves to use
    tools.” (2024).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] [**Toolformer**](https://proceedings.neurips.cc/paper_files/paper/2023/file/d842425e4bf79ba039352da0f658a906-Paper-Conference.pdf):
    Schick, Timo, 等. “Toolformer: Language models can teach themselves to use tools.”
    (2024).'
- en: '[14] [**GPT4Tools**](https://proceedings.neurips.cc/paper_files/paper/2023/file/e393677793767624f2821cec8bdd02f1-Paper-Conference.pdf):
    Yang, Rui, et al. “Gpt4tools: Teaching large language model to use tools via self-instruction.”
    (2024).'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] [**GPT4Tools**](https://proceedings.neurips.cc/paper_files/paper/2023/file/e393677793767624f2821cec8bdd02f1-Paper-Conference.pdf):
    Yang, Rui, 等. “Gpt4tools: Teaching large language model to use tools via self-instruction.”
    (2024).'
- en: '[15] [**MIMIC-IT**](https://arxiv.org/pdf/2306.05425): Li, Bo, et al. “Mimic-it:
    Multi-modal in-context instruction tuning.” (2023).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] [**MIMIC-IT**](https://arxiv.org/pdf/2306.05425): Li, Bo, 等. “Mimic-it:
    Multi-modal in-context instruction tuning.” (2023).'
- en: '[16] He, Junxian, et al. “Towards a unified view of parameter-efficient transfer
    learning.” (2021).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] He, Junxian, 等. “Towards a unified view of parameter-efficient transfer
    learning.” (2021).'
