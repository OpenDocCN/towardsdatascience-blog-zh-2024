- en: 'The New Frontiers of LLMs: Challenges, Solutions, and Tools'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs的新前沿：挑战、解决方案与工具
- en: 原文：[https://towardsdatascience.com/the-new-frontiers-of-llms-challenges-solutions-and-tools-b1d48c34cf8e?source=collection_archive---------2-----------------------#2024-01-25](https://towardsdatascience.com/the-new-frontiers-of-llms-challenges-solutions-and-tools-b1d48c34cf8e?source=collection_archive---------2-----------------------#2024-01-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-new-frontiers-of-llms-challenges-solutions-and-tools-b1d48c34cf8e?source=collection_archive---------2-----------------------#2024-01-25](https://towardsdatascience.com/the-new-frontiers-of-llms-challenges-solutions-and-tools-b1d48c34cf8e?source=collection_archive---------2-----------------------#2024-01-25)
- en: '[](https://towardsdatascience.medium.com/?source=post_page---byline--b1d48c34cf8e--------------------------------)[![TDS
    Editors](../Images/4b2d1beaf4f6dcf024ffa6535de3b794.png)](https://towardsdatascience.medium.com/?source=post_page---byline--b1d48c34cf8e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b1d48c34cf8e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b1d48c34cf8e--------------------------------)
    [TDS Editors](https://towardsdatascience.medium.com/?source=post_page---byline--b1d48c34cf8e--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://towardsdatascience.medium.com/?source=post_page---byline--b1d48c34cf8e--------------------------------)[![TDS
    Editors](../Images/4b2d1beaf4f6dcf024ffa6535de3b794.png)](https://towardsdatascience.medium.com/?source=post_page---byline--b1d48c34cf8e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b1d48c34cf8e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b1d48c34cf8e--------------------------------)
    [TDS 编辑](https://towardsdatascience.medium.com/?source=post_page---byline--b1d48c34cf8e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b1d48c34cf8e--------------------------------)
    ·Sent as a [Newsletter](/newsletter?source=post_page---byline--b1d48c34cf8e--------------------------------)
    ·3 min read·Jan 25, 2024
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b1d48c34cf8e--------------------------------)
    ·通过 [Newsletter](/newsletter?source=post_page---byline--b1d48c34cf8e--------------------------------)
    发送 ·3分钟阅读·2024年1月25日
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Large language models have been around for several years, but it wasn’t until
    2023 that their presence became truly ubiquitous both within and outside machine
    learning communities. Previously opaque concepts like fine-tuning and RAG have
    gone mainstream, and companies big and small have been either building or integrating
    LLM-powered tools into their workflows.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型已经存在多年，但直到2023年，它们在机器学习社区内外的存在才变得真正无处不在。以前晦涩的概念，如微调和RAG，已经成为主流，大小公司要么在构建，要么在将LLM驱动的工具整合到它们的工作流程中。
- en: As we look ahead at what 2024 might bring, it seems all but certain that these
    models’ footprint is poised to grow further, and that alongside exciting innovations,
    they’ll also generate new challenges for practitioners. The standout posts we’re
    highlighting this week point at some of these emerging aspects of working with
    LLMs; whether you’re relatively new to the topic or have already experimented
    extensively with these models, you’re bound to find something here to pique your
    curiosity.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 展望2024年，似乎几乎可以确定这些模型的影响力将进一步扩大，并且随着令人兴奋的创新，它们也将为从业者带来新的挑战。本周我们强调的一些突出文章指向了与LLMs相关的一些新兴方面；无论你是对这个话题相对陌生，还是已经广泛尝试过这些模型，你都能在这里找到一些能激发你好奇心的内容。
- en: '[**Democratizing LLMs: 4-bit Quantization for Optimal LLM Inference**](/democratizing-llms-4-bit-quantization-for-optimal-llm-inference-be30cf4e0e34)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**民主化LLMs：4位量化以优化LLM推理**](/democratizing-llms-4-bit-quantization-for-optimal-llm-inference-be30cf4e0e34)'
- en: Quantization is one of the main approaches for making the power of massive models
    accessible to a wider user base of ML professionals, many of whom might not have
    access to limitless memory and compute. [Wenqi Glantz](https://medium.com/u/ce7cd5b8b74a?source=post_page---user_mention--b1d48c34cf8e--------------------------------)
    walks us through the process of quantizing the Mistral-7B-Instruct-v0.2 model,
    and explains this method’s inherent tradeoffs between efficiency and performance.
  id: totrans-8
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 量化是使大规模模型的强大能力能够为更广泛的机器学习专业人士群体所用的主要方法之一，这些专业人士中的许多人可能无法获得无限的内存和计算能力。[Wenqi
    Glantz](https://medium.com/u/ce7cd5b8b74a?source=post_page---user_mention--b1d48c34cf8e--------------------------------)带我们了解了量化Mistral-7B-Instruct-v0.2模型的过程，并解释了这种方法在效率和性能之间固有的权衡。
- en: '[**Navigating the World of LLM Agents: A Beginner’s Guide**](/navigating-the-world-of-llm-agents-a-beginners-guide-3b8d499db7a9)How
    can we get LLMs “to the point where they can solve more complex questions on their
    own?” [Dominik Polzer](https://medium.com/u/3ab8d3143e32?source=post_page---user_mention--b1d48c34cf8e--------------------------------)’s
    accessible primer shows how to build LLM agents that can leverage disparate tools
    and functionalities to automate complex workflows with minimal human intervention.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**导航LLM代理的世界：初学者指南**](/navigating-the-world-of-llm-agents-a-beginners-guide-3b8d499db7a9)我们如何让LLM“达到能够独立解决更复杂问题的水平”？[Dominik
    Polzer](https://medium.com/u/3ab8d3143e32?source=post_page---user_mention--b1d48c34cf8e--------------------------------)的易懂入门指南展示了如何构建能够利用不同工具和功能、以最少人类干预自动化复杂工作流的LLM代理。'
- en: '![](../Images/41f848543855fdc6762d8c38848f6b66.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41f848543855fdc6762d8c38848f6b66.png)'
- en: Photo by [Beth Macdonald](https://unsplash.com/@elsbethcat?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Beth Macdonald](https://unsplash.com/@elsbethcat?utm_source=medium&utm_medium=referral)拍摄，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '[**Leverage KeyBERT, HDBSCAN and Zephyr-7B-Beta to Build a Knowledge Graph**](/leverage-keybert-hdbscan-and-zephyr-7b-beta-to-build-a-knowledge-graph-33d7534ee01b)LLMs
    are very powerful on their own, of course, but their potential becomes even more
    striking when combined with other approaches and tools. [Silvia Onofrei](https://medium.com/u/ab562e798558?source=post_page---user_mention--b1d48c34cf8e--------------------------------)’s
    recent guide on building a knowledge graph with the aid of the Zephyr-7B-Beta
    model is a case in point; it demonstrates how bringing together LLMs and traditional
    NLP methods can produce impressive results.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**利用KeyBERT、HDBSCAN和Zephyr-7B-Beta构建知识图谱**](/leverage-keybert-hdbscan-and-zephyr-7b-beta-to-build-a-knowledge-graph-33d7534ee01b)LLM本身非常强大，当然，但当与其他方法和工具结合时，其潜力更为显著。[Silvia
    Onofrei](https://medium.com/u/ab562e798558?source=post_page---user_mention--b1d48c34cf8e--------------------------------)最近的指南通过Zephyr-7B-Beta模型的帮助，展示了如何构建一个知识图谱，这正是一个例证；它演示了如何将LLM与传统的NLP方法结合，产生令人印象深刻的结果。'
- en: '[**Merge Large Language Models with mergekit**](/merge-large-language-models-with-mergekit-2118fb392b54)As
    unlikely as it may sound, sometimes a single LLM might not be enough for your
    project’s specific needs. As [Maxime Labonne](https://medium.com/u/dc89da634938?source=post_page---user_mention--b1d48c34cf8e--------------------------------)
    shows in his latest tutorial, model merging, a “relatively new and experimental
    method to create new models for cheap,” might just be the solution for those moments
    when you need to mix-and-match elements from multiple models.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**将大型语言模型与mergekit合并**](/merge-large-language-models-with-mergekit-2118fb392b54)尽管听起来不太可能，但有时单一的LLM可能无法满足项目的特定需求。正如[Maxime
    Labonne](https://medium.com/u/dc89da634938?source=post_page---user_mention--b1d48c34cf8e--------------------------------)在他最新的教程中所展示的，模型合并是一种“相对较新且实验性的低成本创建新模型的方法”，它可能正是你需要从多个模型中混合搭配元素时的解决方案。'
- en: '[**Does Using an LLM During the Hiring Process Make You a Fraud as a Candidate?**](/does-using-an-llm-during-the-hiring-process-make-you-a-fraud-as-a-candidate-99a05678536b)The
    types of questions LLMs raise go beyond the technical—they also touch on ethical
    and social issues that can get quite thorny. [Christine Egan](https://medium.com/u/8e9b4d1cb38?source=post_page---user_mention--b1d48c34cf8e--------------------------------)
    focuses on the stakes for job candidates who take advantage of LLMs and tools
    like ChatGPT as part of the job search, and explores the sometimes blurry line
    between using and misusing technology to streamline tedious tasks.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**在招聘过程中使用LLM会让你作为候选人变成欺诈者吗？**](/does-using-an-llm-during-the-hiring-process-make-you-a-fraud-as-a-candidate-99a05678536b)LLM提出的问题不仅仅是技术性的——它们还涉及到伦理和社会问题，这些问题可能变得相当棘手。[Christine
    Egan](https://medium.com/u/8e9b4d1cb38?source=post_page---user_mention--b1d48c34cf8e--------------------------------)聚焦于那些在求职过程中利用LLM和像ChatGPT这样的工具的求职者所面临的风险，并探讨了利用和滥用技术来简化繁琐任务之间有时模糊的界限。'
- en: 'As always, the range and depth of topics our authors covered in recent weeks
    is staggering—here’s a representative sample of must-reads:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 正如往常一样，我们作者在最近几周所涵盖的主题范围和深度令人震撼——这里是一些必读的代表性样本：
- en: Going beyond LLMs, [Nate Cibik](https://medium.com/u/82bf2304955e?source=post_page---user_mention--b1d48c34cf8e--------------------------------)
    dives deep into the emerging world of large *multimodal* models (LMMs) and [how
    they’re shaping the autonomous robotics](/navigating-the-future-62ea60f27046)
    field.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超越LLM，[Nate Cibik](https://medium.com/u/82bf2304955e?source=post_page---user_mention--b1d48c34cf8e--------------------------------)
    深入探索了大规模*多模态*模型（LMMs）和[它们如何塑造自主机器人领域](/navigating-the-future-62ea60f27046)。
- en: In a new project walkthrough, [Christabelle Pabalan](https://medium.com/u/4200eb8e8b26?source=post_page---user_mention--b1d48c34cf8e--------------------------------)
    combines [NLP techniques, feature engineering, and visualization](/evaluating-cinematic-dialogue-which-syntactic-and-semantic-features-are-predictive-of-genre-2c69a71af6e2)
    to assess the links between the semantic attributes of movie dialogue and genre.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个新的项目介绍中，[Christabelle Pabalan](https://medium.com/u/4200eb8e8b26?source=post_page---user_mention--b1d48c34cf8e--------------------------------)
    结合了[NLP技术、特征工程和可视化](/evaluating-cinematic-dialogue-which-syntactic-and-semantic-features-are-predictive-of-genre-2c69a71af6e2)来评估电影对白的语义属性与类型之间的联系。
- en: '[Where is temporal graph learning headed this year](/temporal-graph-learning-in-2024-feaa9371b8e2)?
    [Shenyang(Andy) Huang](https://medium.com/u/8aa224c5cedd?source=post_page---user_mention--b1d48c34cf8e--------------------------------)
    and coauthors Emanuele Rossi, [Michael Galkin](https://medium.com/u/4d4f8ddd1e68?source=post_page---user_mention--b1d48c34cf8e--------------------------------),
    Andrea Cini, and Ingo Scholtesoffer a panoramic overview of the field’s trajectory.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[今年时序图学习的发展方向在哪里](/temporal-graph-learning-in-2024-feaa9371b8e2)？[Shenyang(Andy)
    Huang](https://medium.com/u/8aa224c5cedd?source=post_page---user_mention--b1d48c34cf8e--------------------------------)
    与合著者 Emanuele Rossi，[Michael Galkin](https://medium.com/u/4d4f8ddd1e68?source=post_page---user_mention--b1d48c34cf8e--------------------------------)，Andrea
    Cini 和 Ingo Scholtes 提供了该领域发展轨迹的全景概览。'
- en: Causal inference is an essential concept for all data professionals, but its
    meaning can vary depending on your role. [Zijing Zhu, PhD](https://medium.com/u/7d83c09fb5d4?source=post_page---user_mention--b1d48c34cf8e--------------------------------)
    zooms in on [how these differences play out in academia and in industry](/how-is-causal-inference-different-in-academia-and-industry-fb5afd12e2e7).
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因果推断是所有数据专业人士必须掌握的基本概念，但其含义可能会因角色而异。[Zijing Zhu, PhD](https://medium.com/u/7d83c09fb5d4?source=post_page---user_mention--b1d48c34cf8e--------------------------------)
    聚焦于[这些差异在学术界和行业中的表现](/how-is-causal-inference-different-in-academia-and-industry-fb5afd12e2e7)。
- en: Diffusion has made a splash in recent years in the context of AI-generated images,
    but as [Christopher Landschoot](https://medium.com/u/b64548f914a5?source=post_page---user_mention--b1d48c34cf8e--------------------------------)
    shows, [its potential extends to the world of music and audio generation](/audio-diffusion-generative-musics-secret-sauce-f625d0aca800)
    as well.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩散在近年来的AI生成图像领域引起了广泛关注，但正如[Christopher Landschoot](https://medium.com/u/b64548f914a5?source=post_page---user_mention--b1d48c34cf8e--------------------------------)
    所展示的，[其潜力同样延伸到了音乐和音频生成的世界](/audio-diffusion-generative-musics-secret-sauce-f625d0aca800)。
- en: Thank you for supporting the work of our authors! If you’re feeling inspired
    to join their ranks, why not [write your first post? We’d love to read it](http://bit.ly/write-for-tds).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你支持我们作者的工作！如果你感到受到启发并想加入他们的行列，为什么不[写下你的第一篇文章？我们期待阅读它](http://bit.ly/write-for-tds)。
- en: Until the next Variable,
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 直到下一个Variable，
- en: TDS Team
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: TDS团队
