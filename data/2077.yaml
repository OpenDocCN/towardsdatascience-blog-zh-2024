- en: 'Advanced Retrieval Techniques in a World of 2M Token Context Windows: Part
    2 on Re-rankers'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2百万令牌上下文窗口中的高级检索技术：关于重新排序器的第二部分
- en: 原文：[https://towardsdatascience.com/advanced-retrieval-techniques-in-a-world-of-2m-token-context-windows-part-2-on-re-rankers-a0dfa03ba325?source=collection_archive---------7-----------------------#2024-08-26](https://towardsdatascience.com/advanced-retrieval-techniques-in-a-world-of-2m-token-context-windows-part-2-on-re-rankers-a0dfa03ba325?source=collection_archive---------7-----------------------#2024-08-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/advanced-retrieval-techniques-in-a-world-of-2m-token-context-windows-part-2-on-re-rankers-a0dfa03ba325?source=collection_archive---------7-----------------------#2024-08-26](https://towardsdatascience.com/advanced-retrieval-techniques-in-a-world-of-2m-token-context-windows-part-2-on-re-rankers-a0dfa03ba325?source=collection_archive---------7-----------------------#2024-08-26)
- en: '![](../Images/d563caf69cb0e3c037f66913efe18f14.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d563caf69cb0e3c037f66913efe18f14.png)'
- en: Visualising AI project launched by Google DeepMind. From [Unsplash](https://unsplash.com/photos/a-close-up-of-a-group-of-different-colored-objects-_aU_AxlS04E)
    image.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Google DeepMind推出的可视化AI项目。图像来自[Unsplash](https://unsplash.com/photos/a-close-up-of-a-group-of-different-colored-objects-_aU_AxlS04E)。
- en: Exploring RAG techniques to improve retrieval accuracy
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索RAG技术以提高检索准确性
- en: '[](https://medium.com/@meghanheintz?source=post_page---byline--a0dfa03ba325--------------------------------)[![Meghan
    Heintz](../Images/9eaae6d3d8168086d83ff7100329c51f.png)](https://medium.com/@meghanheintz?source=post_page---byline--a0dfa03ba325--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a0dfa03ba325--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a0dfa03ba325--------------------------------)
    [Meghan Heintz](https://medium.com/@meghanheintz?source=post_page---byline--a0dfa03ba325--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@meghanheintz?source=post_page---byline--a0dfa03ba325--------------------------------)[![Meghan
    Heintz](../Images/9eaae6d3d8168086d83ff7100329c51f.png)](https://medium.com/@meghanheintz?source=post_page---byline--a0dfa03ba325--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a0dfa03ba325--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a0dfa03ba325--------------------------------)
    [Meghan Heintz](https://medium.com/@meghanheintz?source=post_page---byline--a0dfa03ba325--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a0dfa03ba325--------------------------------)
    ·7 min read·Aug 26, 2024
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a0dfa03ba325--------------------------------)
    ·7分钟阅读·2024年8月26日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: In [Part 1](https://medium.com/p/2edc0266aabe), we dove into improving RAG (retrieval
    augmented generation) outcomes by re-writing queries before performing retrieval.
    This time we will learn about how re-ranking results from vector database retrievals
    helps performance.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第一部分](https://medium.com/p/2edc0266aabe)中，我们探讨了通过在执行检索之前重写查询来改善RAG（增强检索生成）结果。本次，我们将学习如何通过重新排序来自向量数据库检索的结果来提高性能。
- en: While I highly recommend experimenting with promising proprietary options like
    Cohere’s [Re-Rank 3](https://cohere.com/blog/rerank-3), we’ll focus mainly on
    understanding what researchers have shared on the topic.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我强烈建议尝试一些有前景的专有选项，如Cohere的 [Re-Rank 3](https://cohere.com/blog/rerank-3)，但我们将主要关注了解研究人员在这一主题上分享的内容。
- en: Re-Ranking, what’s the point?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新排序，意义何在？
- en: First of all, why rerank at all? Results from vector databases return “similarity”
    scores based on the embeddings of the query and document. These scores can already
    be used to sort the results and since this is already a semantic similarity scoring
    of the document and query, why would we need another step?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，为什么要进行重新排序呢？向量数据库返回的结果是基于查询和文档的嵌入计算出的“相似度”分数。这些分数已经可以用来对结果进行排序，既然这已经是对文档和查询的语义相似度评分，为什么我们还需要另一步骤呢？
- en: 'There are a few reasons why we would take this approach:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采取这种方法有几个原因：
- en: Document embeddings are “lossy”. Documents are compressed in vector format before
    seeing the query, which means the document vector is not tailored to the query
    vector. Re-ranking allows us to better understand the **document’s meaning specific
    to the query**.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档嵌入是“有损”的。文档在看到查询之前就已被压缩为向量格式，这意味着文档向量并未针对查询向量进行定制。重新排序使我们能够更好地理解**文档与查询特定相关的含义**。
- en: Two-stage systems have become standard in traditional search and recommender
    systems. They offer improvements in scalability, flexibility, and accuracy. **Retrieval
    models are very fast, whereas ranking models are slow.** By building a hybrid
    system, we can balance the speed and accuracy trade-offs between each stage.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两阶段系统已成为传统搜索和推荐系统的标准。它们在可扩展性、灵活性和准确性方面提供了改进。**检索模型非常快速，而排名模型较慢。**通过构建混合系统，我们可以平衡每个阶段之间的速度和准确性的权衡。
- en: Re-ranking allows us to reduce the number of documents we stuff into the context
    window which **a) reduces costs** and **b) reduces the chances of relevant data
    being “lost in the haystack”.**
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新排序允许我们减少将文档放入上下文窗口的数量，这**a) 降低了成本**并且**b) 减少了相关数据被“埋没在大海捞针”中的机会**。
- en: Traditional Methods of Re-Ranking
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 传统的重新排序方法
- en: Informational retrieval is not a new field. Before LLMs employed RAG to improve
    generation, search engines used re-ranking methods to improve search results.
    Two popular methodologies are TF-IDF (term frequency–inverse document frequency)
    and BM25 (Best Match 25).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 信息检索并不是一个新领域。在 LLMs 使用 RAG 改善生成之前，搜索引擎已经使用重新排序方法来改善搜索结果。两种流行的方法是 TF-IDF（词频-逆文档频率）和
    BM25（最佳匹配 25）。
- en: '[Karen Spärck Jones](https://en.wikipedia.org/wiki/Karen_Sp%C3%A4rck_Jones)
    conceived of the concept of IDF (of TF-IDF), inverse document frequency, as a
    statistical interpretation of term-specificity in the 1970s. The general concept
    is that the specificity of a term can be quantified as an inverse function of
    the number of documents in which it occurs. A toy example is the frequency of
    terms in Shakespearean plays. Because the term “Romeo” only appears in one play,
    we believe it is more informative to the subject of the play than the word “sweet”
    because that term occurs in all plays.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[Karen Spärck Jones](https://en.wikipedia.org/wiki/Karen_Sp%C3%A4rck_Jones)
    在 1970 年代提出了 IDF（TF-IDF 的逆文档频率）概念，作为术语特异性的统计解释。其一般概念是，术语的特异性可以作为该术语出现的文档数量的倒数来量化。一个简单的例子是莎士比亚戏剧中的术语频率。因为“Romeo”一词仅出现在一部戏剧中，我们认为它对于该戏剧的主题比“sweet”一词更具信息性，因为“sweet”在所有戏剧中都有出现。'
- en: 'BM25 or Okapi BM25 was developed by both Karen Spärck Jones and [Stephen Robertson](https://en.wikipedia.org/wiki/Stephen_Robertson_(computer_scientist))
    as an improvement to TF-IDF. BM25 is a “bag-of-words” retrieval function that
    ranks a set of documents based on the query terms appearing in each document,
    regardless of their proximity within the document. This method expands on TF-IDF
    in a few important ways:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: BM25 或 Okapi BM25 是由 Karen Spärck Jones 和 [Stephen Robertson](https://en.wikipedia.org/wiki/Stephen_Robertson_(computer_scientist))
    共同开发的，作为 TF-IDF 的改进。BM25 是一种“词袋”检索函数，根据查询词在每个文档中出现的频率对一组文档进行排名，而不考虑这些词在文档中的相对位置。该方法在几个重要方面扩展了
    TF-IDF：
- en: 'BM25 uses a saturation function where the importance of a term increases with
    frequency but with diminishing returns. (Side note: This was important for protecting
    accuracy when search engine optimization (SEO) became higher stakes. You can’t
    just spam your keyword with this improvement.)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BM25 使用一个饱和函数，其中一个词的频率越高，其重要性也随之增加，但回报是递减的。（旁注：这对于在搜索引擎优化（SEO）变得更加重要时保护准确性至关重要。你不能仅仅通过增加关键词的频率来作弊。）
- en: BM25 includes document length normalization to ensure that longer documents
    are not unfairly advantaged. (Another improvement to thwart would-be SEO gamers.)
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BM25 包括文档长度归一化，确保较长的文档不会被不公平地偏袒。（这是另一个阻止潜在 SEO 游戏者的改进。）
- en: Both of these methods can be used to re-rank results from vector databases before
    documents are used in the context of generation. This would be called “feature”
    based re-ranking.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法都可以在文档用于生成上下文之前，重新对向量数据库中的结果进行排序。这被称为基于“特征”的重新排序。
- en: Neural Re-Ranking Models
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经重新排序模型
- en: Something you should notice about the traditional methods is that they focus
    on exact term matches. These methods will struggle when documents use semantically
    similar but different terms. Neural re-ranking methods like SBERT ([Sentence Transformers](https://arxiv.org/pdf/1908.10084))
    seek to overcome this limitation.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该注意到传统方法的一个特点是，它们专注于精确的术语匹配。当文档使用语义相似但不同的术语时，这些方法将会遇到困难。像 SBERT（[Sentence
    Transformers](https://arxiv.org/pdf/1908.10084)）这样的神经重新排序方法试图克服这一局限性。
- en: SBERT is a fine-tuned BERT (Bidirectional Encoder Representations from Transformers)
    model with a siamese / triplet network architecture which greatly improves the
    computation efficiency and latency for calculating sentence similarity. Transformers
    like [SBERT](https://arxiv.org/pdf/1908.10084) (Sentence-BERT) use the context
    in which terms are used, allowing the model to handle synonyms and words with
    multiple meanings.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: SBERT是一个经过微调的BERT（Bidirectional Encoder Representations from Transformers）模型，采用了孪生/三元组网络架构，极大地提高了计算效率和计算句子相似性的延迟。像[SBERT](https://arxiv.org/pdf/1908.10084)（Sentence-BERT）这样的变换器（Transformers）利用术语使用的上下文，使模型能够处理同义词和多义词。
- en: '![](../Images/d654e0e6d8d456ab557a95ef07dc4080.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d654e0e6d8d456ab557a95ef07dc4080.png)'
- en: 'SBERT architecture at inference, for example, to compute similarity scores.
    This architecture is also used with the regression objective function. From [Sentence-BERT:
    Sentence Embeddings using Siamese BERT-Networks by Nils Reimers and Iryna Gurevych](https://arxiv.org/pdf/1908.10084)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 'SBERT推理时的架构，例如计算相似度分数。该架构也用于回归目标函数。从[Nils Reimers 和 Iryna Gurevych的《Sentence-BERT:
    Sentence Embeddings using Siamese BERT-Networks》](https://arxiv.org/pdf/1908.10084)'
- en: SBERT tends to perform better for semantic similarity ranking due to its specialization.
    However, using SBERT comes with the downside that you will need to manage the
    models locally versus calling an API, such as with OpenAI’s embedding models.
    Pick your poison wisely!
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 由于SBERT的专业化，它在语义相似度排名中通常表现更好。然而，使用SBERT的缺点是，你需要在本地管理模型，而不是调用API，例如使用OpenAI的嵌入模型。明智地选择你所需的工具！
- en: Cross-Encoder Re-Ranking
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉编码器重新排序
- en: The top K results from a vector database search are the most similar document
    vectors compared to the query vector. Another way of describing this ranking method
    is to say it is a “bi-encoder” ranking. Vectors are calculated up front and approximate
    nearest neighbors algorithms (ANNs) select the most similar documents making this
    a highly efficient ranking method. But that efficiency comes at the expense of
    some accuracy.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库搜索中的前K个结果是与查询向量最相似的文档向量。另一种描述这种排名方法的方式是称其为“二重编码器”排名。向量在前期计算，并且近邻算法（ANNs）选择最相似的文档，这使得该方法成为一种高效的排名方式。但这种高效性是以牺牲一些准确性为代价的。
- en: In contrast, cross-encoders use a classification mechanism on data pairs to
    calculate similarity. This means you need a pair for each document and query.
    This approach can yield much more accurate results but it’s highly inefficient.
    That is why cross-encoders are best implemented through a hybrid approach where
    the number of documents is first pruned using a “bi-encoder” top K result before
    ranking with a cross-encoder. You can read more about using bi-encoders and cross-encoders
    together in the [SBERT documentation](https://www.sbert.net/examples/applications/retrieve_rerank/README.html).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，交叉编码器通过对数据对进行分类机制来计算相似性。这意味着你需要为每个文档和查询提供一对数据。这种方法能够提供更加准确的结果，但效率极低。这也是为什么交叉编码器最好通过一种混合方法来实现，其中首先使用“二重编码器”前K个结果进行文档数量的修剪，然后再使用交叉编码器进行排名。你可以在[SBERT文档](https://www.sbert.net/examples/applications/retrieve_rerank/README.html)中了解更多关于如何一起使用二重编码器和交叉编码器的信息。
- en: '![](../Images/e87d3746696f5d9c5fd2179ff3fb8756.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e87d3746696f5d9c5fd2179ff3fb8756.png)'
- en: Information Retrieval / Question Answering Retrieval Diagram explaining how
    to use Bi-Encoders and Cross-Encoders together from the [SBERT documentation](https://www.sbert.net/examples/applications/retrieve_rerank/README.html),
    full citation below.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 信息检索/问题回答检索图表，解释如何结合使用二重编码器和交叉编码器，来自[SBERT文档](https://www.sbert.net/examples/applications/retrieve_rerank/README.html)，完整引用见下文。
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Prompt-based Re-Ranking (PBR)
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于提示的重新排序（PBR）
- en: Until now, we have focused on using vectors or other numeric methods to rerank
    our RAG results. But does that mean we are underleveraging the LLM? Feeding the
    document and the query back to the LLM for scoring can be an effective way to
    score the document; there is approximately no information loss when you take this
    approach. If the LLM is prompted to return only a single token (the score), the
    latency incurred is often acceptable (although this is one of the more expensive
    approaches to scale). This is considered to be “zero-shot” re-ranking and research
    is still limited on this topic but we know it must be sensitive to the quality
    of the prompt.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直专注于使用向量或其他数值方法对我们的RAG结果进行重新排序。但这是否意味着我们没有充分利用LLM？将文档和查询重新输入LLM进行评分，可能是一种有效的评分方式；采用这种方法时几乎没有信息损失。如果LLM被提示只返回一个单一的标记（即评分），则通常可以接受所产生的延迟（尽管这是扩展的较为昂贵的方式之一）。这被视为“零-shot”重新排序，关于这一主题的研究仍然有限，但我们知道它必须对提示的质量非常敏感。
- en: Another version of (PBR) is the [**DSLR Framework**](https://ar5iv.labs.arxiv.org/html/2407.03627v4)
    (Document Refinement with Sentence-Level Re-ranking and Reconstruction). DSLR
    proposes an unsupervised method that decomposes retrieved documents into sentences,
    re-ranks them based on relevance, and reconstructs them into coherent passages
    before passing them to the LLM. This approach contrasts with traditional methods
    that rely on fixed-size passages, which may include redundant or irrelevant information.
    Pruning non-relevant sentences before generating a response can reduce hallucinations
    and improve overall accuracy. Below you can see an example of how DSLR refinement
    improves the LLMs response.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: (PBR)的另一个版本是[**DSLR框架**](https://ar5iv.labs.arxiv.org/html/2407.03627v4)（基于句子级重新排序和重构的文档优化）。DSLR提出了一种无监督方法，将检索到的文档分解成句子，基于相关性对其重新排序，然后在传递给LLM之前将其重构为连贯的段落。这种方法与传统依赖于固定大小段落的方式不同，后者可能包含冗余或无关的信息。在生成响应之前修剪掉无关的句子，可以减少幻觉现象并提高整体准确性。下面您可以看到DSLR优化如何提升LLM的响应。
- en: '![](../Images/f1b8b5cb97f9535858ebec79376c3e32.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1b8b5cb97f9535858ebec79376c3e32.png)'
- en: 'Example DSLR refinement from [DSLR: Document Refinement with Sentence-Level
    Re-ranking and Reconstruction to Enhance Retrieval-Augmented Generation by Taeho
    Hwang, Soyeong Jeong, Sukmin Cho, SeungYoon Han, Jong C. Park at the School of
    Computing Korea Advanced Institute of Science and Technology](https://ar5iv.labs.arxiv.org/html/2407.03627v4)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '来自[DSLR: 基于句子级重新排序和重构的文档优化，以增强检索增强生成（由Taeho Hwang, Soyeong Jeong, Sukmin Cho,
    SeungYoon Han, Jong C. Park 在韩国高等科学技术学院计算机学院提供）](https://ar5iv.labs.arxiv.org/html/2407.03627v4)的示例DSLR优化'
- en: Graph-based Reranking
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于图的重新排序
- en: Sometimes the answer is not going to fit cleanly inside a single document chunk.
    Books and papers are written with the expectation that they’ll be read linearly
    or at least the reader will be able to easily refer back to earlier passages.
    For example, you could be asked to refer back to an earlier chapter on BM25 when
    reading about SBERT. In a basic RAG application, this would be impossible because
    your retrieved document would have no connections to the previous chapters.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，答案并不适合完全包含在一个单一的文档块中。书籍和论文的编写假设它们会被线性阅读，或者至少读者能够轻松地回溯到早期的段落。例如，当阅读关于SBERT的内容时，可能会要求回顾早先关于BM25的章节。在基本的RAG应用中，这是不可能的，因为您检索到的文档与之前的章节没有任何连接。
- en: '[G-RAG](https://arxiv.org/pdf/2405.18414), an approach proposed by researchers
    at Google and UCLA, hopes to alleviate this issue. G-RAG is a re-ranker that leverages
    graph neural networks (GNNs) to consider connections between retrieved documents.
    Documents are represented as nodes and edges are shared concepts between documents.
    These graphs are generated as Abstract Meaning Representation (AMR) Graphs which
    can be created with tools like [https://github.com/goodbai-nlp/AMRBART](https://github.com/goodbai-nlp/AMRBART)
    (MIT License).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[G-RAG](https://arxiv.org/pdf/2405.18414)，是由谷歌和UCLA的研究人员提出的一种方法，旨在缓解这一问题。G-RAG是一种重新排序器，利用图神经网络（GNNs）来考虑检索到的文档之间的连接。文档被表示为节点，边表示文档之间共享的概念。这些图表被生成作为抽象意义表示（AMR）图，可以通过如[https://github.com/goodbai-nlp/AMRBART](https://github.com/goodbai-nlp/AMRBART)（MIT许可）等工具创建。'
- en: Experiments with [Natural Question (NQ)](https://aclanthology.org/Q19-1026/)
    and [TriviaQA (TQA) datasets](https://paperswithcode.com/dataset/triviaqa) showed
    this approach made improvements to Mean Tied Reciprocal Ranking (MTRR) and Tied
    Mean Hits@10 (TMHits@10) over other state-of-the-art approaches.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对[自然问题(NQ)](https://aclanthology.org/Q19-1026/)和[TriviaQA(TQA)数据集](https://paperswithcode.com/dataset/triviaqa)的实验表明，这种方法在与其他最先进方法相比时，改进了平均联结倒数排名(MTRR)和联结平均命中率@10(TMHits@10)。
- en: '![](../Images/1e26c1544a15d9add1ba755fd03236a3.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e26c1544a15d9add1ba755fd03236a3.png)'
- en: '[From UCLA and Google researchers](https://arxiv.org/pdf/2405.18414): G-RAG
    uses two graphs for re-ranking documents: The Abstract Meaning Representation
    (AMR) graph is used as feature for the document-level graph. Document graph is
    then used for document reranking'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[来自UCLA和谷歌研究人员](https://arxiv.org/pdf/2405.18414)：G-RAG使用两个图来重新排序文档：抽象意义表示（AMR）图被用作文档级图的特征，文档图随后用于文档重排序。'
- en: Conclusion
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: I hope you’ve enjoyed this overview of techniques you can use to improve the
    performance of your RAG applications. I look forward to continued advancements
    in this field. I know there will be many considering the blistering pace of research
    at the moment.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你喜欢这个关于提高RAG应用性能的技术概述。我期待着该领域的持续进展，考虑到目前研究的飞速发展，我知道会有很多新的突破。
- en: Let me know in the comments section if you have favorite re-ranking methods
    not covered in this article.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有任何在本文中没有涵盖的最爱的重排序方法，请在评论区告诉我。
