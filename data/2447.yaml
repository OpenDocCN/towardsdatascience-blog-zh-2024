- en: Implementing Sequential Algorithms on TPU
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 TPU 上实现顺序算法
- en: 原文：[https://towardsdatascience.com/implementing-sequential-algorithms-on-tpu-41d75c6aaa95?source=collection_archive---------8-----------------------#2024-10-07](https://towardsdatascience.com/implementing-sequential-algorithms-on-tpu-41d75c6aaa95?source=collection_archive---------8-----------------------#2024-10-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/implementing-sequential-algorithms-on-tpu-41d75c6aaa95?source=collection_archive---------8-----------------------#2024-10-07](https://towardsdatascience.com/implementing-sequential-algorithms-on-tpu-41d75c6aaa95?source=collection_archive---------8-----------------------#2024-10-07)
- en: Accelerating AI/ML Model Training with Custom Operators — Part 3.A
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加速 AI/ML 模型训练与自定义运算符 — 第 3.A 部分
- en: '[](https://chaimrand.medium.com/?source=post_page---byline--41d75c6aaa95--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page---byline--41d75c6aaa95--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--41d75c6aaa95--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--41d75c6aaa95--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page---byline--41d75c6aaa95--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://chaimrand.medium.com/?source=post_page---byline--41d75c6aaa95--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page---byline--41d75c6aaa95--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--41d75c6aaa95--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--41d75c6aaa95--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page---byline--41d75c6aaa95--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--41d75c6aaa95--------------------------------)
    ·11 min read·Oct 7, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--41d75c6aaa95--------------------------------)
    ·阅读时间 11 分钟·2024 年 10 月 7 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a2daa59de127264e6b8d93713b38f15d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2daa59de127264e6b8d93713b38f15d.png)'
- en: Photo by [Bernd Dittrich](https://unsplash.com/@hdbernd?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Bernd Dittrich](https://unsplash.com/@hdbernd?utm_source=medium&utm_medium=referral)
    于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: This is a direct sequel to a [previous post](https://chaimrand.medium.com/the-rise-of-pallas-unlocking-tpu-potential-with-custom-kernels-67be10ab846a)
    on the topic of implementing custom TPU operations with [Pallas](https://jax.readthedocs.io/en/latest/pallas/index.html).
    Of particular interest are custom kernels that leverage the unique properties
    of the TPU architecture in a manner that optimizes runtime performance. In this
    post, we will attempt to demonstrate this opportunity by applying the power of
    Pallas to the challenge of running sequential algorithms that are interspersed
    within a predominantly parallelizable deep learning (DL) workload.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这是关于使用 [Pallas](https://jax.readthedocs.io/en/latest/pallas/index.html) 实现自定义
    TPU 运算的主题的直接续篇，前文可见 [上一篇文章](https://chaimrand.medium.com/the-rise-of-pallas-unlocking-tpu-potential-with-custom-kernels-67be10ab846a)。特别感兴趣的是那些利用
    TPU 架构独特特性，优化运行时性能的自定义内核。本文将尝试通过应用 Pallas 的强大功能，展示如何在主要可并行化的深度学习 (DL) 工作负载中，处理穿插其中的顺序算法问题。
- en: We will focus on [Non Maximum Suppression (NMS)](https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch/)
    of bounding-box proposals as a representative algorithm, and explore ways to optimize
    its implementation. An important component of computer vision (CV) [object detection](https://en.wikipedia.org/wiki/Object_detection)
    solutions (e.g., [Mask RCNN](https://arxiv.org/abs/1703.06870)), NMS is commonly
    used to filter out overlapping bounding boxes, keeping only the “best” ones. NMS
    receives a list of bounding box proposals, an associated list of scores, and an
    [IOU](https://en.wikipedia.org/wiki/IOU) threshold, and proceeds to *greedily*
    and *iteratively* choose the remaining box with the highest score and disqualify
    all other boxes with which it has an IOU that exceeds the given threshold. The
    fact that the box chosen at the *n-th* iteration depends on the preceding *n-1*
    steps of the algorithm dictates the sequential nature of its implementation. Please
    see [here](https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch/)
    and/or [here](https://medium.com/analytics-vidhya/non-max-suppression-nms-6623e6572536)
    for more on the rationale behind NMS and its implementation. Although we have
    chosen to focus on one specific algorithm, most of our discussion should carry
    over to other sequential algorithms.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以[非最大抑制 (NMS)](https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch/)的边界框提议作为代表性算法，并探讨如何优化其实现。NMS是计算机视觉（CV）[目标检测](https://en.wikipedia.org/wiki/Object_detection)解决方案中的一个重要组成部分（例如，[Mask
    RCNN](https://arxiv.org/abs/1703.06870)），通常用于筛选重叠的边界框，只保留“最佳”框。NMS接收一组边界框提议、一组关联的得分列表，以及一个[IOU](https://en.wikipedia.org/wiki/IOU)阈值，然后*贪婪地*和*迭代地*选择剩余得分最高的框，并排除所有与其IOU超过给定阈值的其他框。选择的框在第*n*次迭代中依赖于前*n-1*次算法步骤，这决定了其实现的顺序性。有关NMS及其实现的更多信息，请参见[此处](https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch/)和/或[此处](https://medium.com/analytics-vidhya/non-max-suppression-nms-6623e6572536)。尽管我们选择专注于一个特定的算法，但我们的大部分讨论应适用于其他顺序算法。
- en: Offloading Sequential Algorithms to CPU
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将顺序算法卸载到CPU
- en: The presence of a sequential algorithm within a predominantly parallelizable
    ML model (e.g., Mask R-CNN) presents an interesting challenge. While GPUs, commonly
    used for such workloads, excel at executing parallel operations like matrix multiplication,
    they can significantly underperform compared to CPUs when handling sequential
    algorithms. This often leads to computation graphs that include crossovers between
    the GPU and CPU, where the GPU handles the parallel operations and the CPU handles
    the sequential ones. NMS is a prime example of a sequential algorithm that is
    commonly offloaded onto the CPU. In fact, a close analysis of [torchvision](https://pytorch.org/vision/stable/index.html)’s
    “CUDA” implementation of [NMS](https://github.com/pytorch/vision/blob/v0.19.1/torchvision/csrc/ops/cuda/nms_kernel.cu),
    reveals that even it runs a significant portion of the algorithm on [CPU](https://github.com/pytorch/vision/blob/v0.19.1/torchvision/csrc/ops/cuda/nms_kernel.cu#L136C33-L136C41).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在主要可并行化的机器学习模型（例如，Mask R-CNN）中存在顺序算法，提出了一个有趣的挑战。尽管GPU通常用于这种工作负载，擅长执行并行操作（如矩阵乘法），但在处理顺序算法时，它们的表现往往远不如CPU。这通常导致计算图中存在GPU和CPU之间的交叉点，其中GPU处理并行操作，而CPU处理顺序操作。NMS就是一个常被卸载到CPU的顺序算法的典型例子。事实上，仔细分析[torchvision](https://pytorch.org/vision/stable/index.html)的“CUDA”实现中的[NMS](https://github.com/pytorch/vision/blob/v0.19.1/torchvision/csrc/ops/cuda/nms_kernel.cu)会发现，它甚至将算法的一个重要部分运行在[CPU](https://github.com/pytorch/vision/blob/v0.19.1/torchvision/csrc/ops/cuda/nms_kernel.cu#L136C33-L136C41)上。
- en: 'Although offloading sequential operations to the CPU may lead to improved runtime
    performance, there are several potential drawbacks to consider:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管将顺序操作卸载到CPU可能会提高运行时性能，但也有几个潜在的缺点需要考虑：
- en: Cross-device execution between the CPU and GPU usually requires multiple points
    of synchronization between the devices which commonly results in idle time on
    the GPU while it waits for the CPU to complete its tasks. Given that the GPU is
    typically the most expensive component of the training platform our goal is to
    minimize such idle time.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CPU和GPU之间的跨设备执行通常需要多个同步点，这通常导致GPU在等待CPU完成任务时出现空闲时间。考虑到GPU通常是训练平台中最昂贵的组件，我们的目标是最小化这种空闲时间。
- en: In standard ML workflows, the CPU is responsible for preparing and feeding data
    to the model, which resides on the GPU. If the data input pipeline involves compute-intensive
    processing, this can strain the CPU, leading to “input starvation” on the GPU.
    In such scenarios, offloading portions of the model’s computation to the CPU could
    further exacerbate this issue.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在标准的 ML 工作流程中，CPU 负责准备并将数据传递给模型，而模型则位于 GPU 上。如果数据输入管道涉及计算密集型处理，这可能会给 CPU 带来压力，导致
    GPU 出现“输入饥饿”现象。在这种情况下，将模型的一部分计算卸载到 CPU 上可能会进一步加剧这个问题。
- en: To avoid these drawbacks you could consider alternative approaches, such as
    replacing the sequential algorithm with a comparable alternative (e.g., the one
    suggested [here](https://hertasecurity.com/wp-content/uploads/work-efficient-parallel-non-maximum-suppression.pdf)),
    settling for a slow/suboptimal GPU implementation of the sequential algorithm,
    or running the workload on CPU — each of which come with their own potential trade-offs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这些缺点，您可以考虑替代方法，比如用一个可比较的替代方法（例如，[这里](https://hertasecurity.com/wp-content/uploads/work-efficient-parallel-non-maximum-suppression.pdf)
    提出的方案）替换顺序算法，或者选择一个较慢/次优的 GPU 实现顺序算法，或者将工作负载运行在 CPU 上——每种方法都有其潜在的折衷。
- en: Sequential Algorithms on TPU
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TPU 上的顺序算法
- en: This is where the unique architecture of the TPU could present an opportunity.
    Contrary to GPUs, TPUs are sequential processors. While their ability to run highly
    vectorized operations makes them competitive with GPUs when running parallelizable
    operations such as matrix multiplication, their sequential nature could make them
    uniquely suited for running ML workloads that include a mix of both sequential
    and parallel components. Armed with the [Pallas extension](https://jax.readthedocs.io/en/latest/pallas/index.html)
    to JAX, our [newfound TPU kernel creation](https://chaimrand.medium.com/the-rise-of-pallas-unlocking-tpu-potential-with-custom-kernels-67be10ab846a)
    tool, we will evaluate this opportunity by implementing and evaluating a custom
    implementation of NMS for TPU.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是 TPU 独特架构可能带来机会的地方。与 GPU 相反，TPU 是顺序处理器。尽管它们在执行高度矢量化的操作时，由于其能够进行矩阵乘法等并行操作，因此在运行可以并行化的操作时与
    GPU 竞争，但它们的顺序特性使其在运行包含顺序和并行组件混合的 ML 工作负载时，可能具有独特的优势。借助 [Pallas 扩展](https://jax.readthedocs.io/en/latest/pallas/index.html)
    和我们的 [新型 TPU 核心创建工具](https://chaimrand.medium.com/the-rise-of-pallas-unlocking-tpu-potential-with-custom-kernels-67be10ab846a)，我们将通过实现和评估一个自定义的
    TPU 上的 NMS 实现来评估这一机会。
- en: Disclaimers
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 免责声明
- en: The NMS implementations we will share below are intended for demonstrative purposes
    only. We have not made any significant effort to optimize them or to verify their
    robustness, durability, or accuracy. Please keep in mind that, as of the time
    of this writing, Pallas is an *experimental* feature — still under active development.
    The code we share (based on [JAX](https://pypi.org/project/jax/) version 0.4.32)
    may become outdated by the time you read this. Be sure to refer to the most up-to-date
    APIs and resources available for your Pallas development. Please do not view our
    mention of any algorithm, library, or API as an endorsement for their use.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下我们分享的 NMS 实现仅供演示使用。我们并没有做出任何显著努力来优化它们，或者验证它们的鲁棒性、耐用性或准确性。请记住，截至本文写作时，Pallas
    仍然是一个 *实验性* 功能——仍在积极开发中。我们分享的代码（基于 [JAX](https://pypi.org/project/jax/) 版本 0.4.32）可能在您阅读时已经过时。请务必参考最新的
    API 和资源，供您进行 Pallas 开发使用。请不要将我们提到的任何算法、库或 API 视为对其使用的推荐。
- en: NMS on CPU
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CPU 上的 NMS
- en: 'We begin with a simple implementation of NMS in [numpy](https://numpy.org/)
    that will serve as a baseline for performance comparison:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个简单的 [numpy](https://numpy.org/) 实现的 NMS 开始，它将作为性能比较的基准：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To evaluate the performance of our NMS function, we generate a batch of random
    boxes and scores (as JAX tensors) and run the script on a [Google Cloud TPU v5e](https://cloud.google.com/tpu/docs/v5e)
    system using the same environment and same benchmarking utility as in our [previous
    post](https://chaimrand.medium.com/the-rise-of-pallas-unlocking-tpu-potential-with-custom-kernels-67be10ab846a).
    For this experiment, we specify the CPU as the [JAX default device](https://jax.readthedocs.io/en/latest/_autosummary/jax.default_device.html):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们的 NMS 功能的性能，我们生成了一批随机框和分数（作为 JAX 张量），并在 [Google Cloud TPU v5e](https://cloud.google.com/tpu/docs/v5e)
    系统上运行该脚本，使用与我们 [之前的帖子](https://chaimrand.medium.com/the-rise-of-pallas-unlocking-tpu-potential-with-custom-kernels-67be10ab846a)
    中相同的环境和基准测试工具。对于此实验，我们将 CPU 指定为 [JAX 默认设备](https://jax.readthedocs.io/en/latest/_autosummary/jax.default_device.html)：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The resultant average runtime is 2.99 milliseconds. Note the assumption that
    the input and output tensors reside on the CPU. If they are on the TPU, then the
    time to copy them between the devices should also be taken into consideration.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的平均运行时间为2.99毫秒。请注意，这里假设输入和输出张量位于CPU上。如果它们位于TPU上，则还应考虑它们在设备之间复制的时间。
- en: NMS on TPU
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TPU上的NMS
- en: If our NMS function is a component within a larger computation graph running
    on the TPU, we might prefer a TPU-compatible implementation to avoid the drawbacks
    of cross-device execution. The code block below contains a JAX implementation
    of NMS specifically designed to enable acceleration via JIT compilation. Denoting
    the number of boxes by *N*, we begin by calculating the IOU between each of the
    *N(N-1)* pairs of boxes and preparing an *N*x*N* boolean tensor (*mask_threshold*)where
    the (*i,j*)-th entry indicates whether the IOU between boxes *i* and *j* exceed
    the predefined threshold.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的NMS函数是一个在TPU上运行的大型计算图中的组件，我们可能更倾向于选择一个TPU兼容的实现，以避免跨设备执行的缺点。下面的代码块包含了一个JAX实现的NMS，专门设计用来通过JIT编译加速。假设框的数量为*N*，我们首先计算*N(N-1)*对框之间的IOU，并准备一个*N*x*N*的布尔张量（*mask_threshold*），其中(*i,j*)位置的值表示框*i*和框*j*之间的IOU是否超过了预定的阈值。
- en: 'To simplify the iterative selection of boxes, we create a copy of the mask
    tensor (*mask_threshold2*) where the diagonal elements are zeroed to prevent a
    box from suppressing itself. We further define two score-tracking tensors: *out_scores*,
    which retains the scores of the chosen boxes (and zeros the scores of the eliminated
    ones), and *remaining_scores*, which maintains the scores of the boxes still being
    considered. We then use the [jax.lax.while_loop](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.while_loop.html)
    function to iteratively choose boxes while updating the *out_scores* and *remaining_scores*
    tensors. Note that the format of the output of this function differs from the
    previous function and may need to be adjusted to fit into subsequent steps of
    the computation graph.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化框的迭代选择，我们创建了一个掩码张量（*mask_threshold2*）的副本，其中对角线元素被置零，以防止框抑制自身。我们进一步定义了两个评分跟踪张量：*out_scores*，它保留已选择框的评分（并将被淘汰框的评分置零），以及*remaining_scores*，它保持仍在考虑中的框的评分。接着，我们使用[jax.lax.while_loop](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.while_loop.html)函数来迭代地选择框，同时更新*out_scores*和*remaining_scores*张量。请注意，这个函数的输出格式与之前的函数不同，可能需要调整以适应计算图的后续步骤。
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The runtimes of this implementation of NMS are 1.231 and 0.416 milliseconds
    on CPU and TPU, respectively.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 该NMS实现的运行时间在CPU和TPU上分别为1.231毫秒和0.416毫秒。
- en: Custom NMS Pallas Kernel
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义NMS Pallas内核
- en: We now present a custom implementation of NMS in which we explicitly leverage
    the fact that on TPUs Pallas kernels are executed in a [sequential manner](https://jax.readthedocs.io/en/latest/pallas/grid_blockspec.html#grid-a-k-a-kernels-in-a-loop).
    Our implementation uses two boolean matrix masks and two score-keeping tensors,
    similar to the approach in our previous function.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在介绍一个自定义的NMS实现，在该实现中，我们显式地利用了在TPU上Pallas内核是以[顺序方式](https://jax.readthedocs.io/en/latest/pallas/grid_blockspec.html#grid-a-k-a-kernels-in-a-loop)执行的这一事实。我们的实现使用了两个布尔矩阵掩码和两个评分保持张量，类似于我们前一个函数中的方法。
- en: We define a kernel function, *choose_box*, responsible for selecting the next
    box and updating the score-keeping tensors, which are maintained in scratch memory.
    We invoke the kernel across a one-dimensional grid where the number of steps (i.e.,
    the grid-size) is determined by the *max_output_size* parameter.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个内核函数，*choose_box*，负责选择下一个框并更新评分保持张量，这些张量存储在临时内存中。我们在一维网格上调用该内核，其中步数（即网格大小）由*max_output_size*参数确定。
- en: Note that due to some [limitations](https://jax.readthedocs.io/en/latest/pallas/tpu/details.html#supported-operations)
    (as of the time of this writing) on the operations supported by Pallas, some acrobatics
    are required to implement both the “argmax” function and the validity check for
    the selected boxes. For the sake of brevity, we omit the technical details and
    refer the interested reader to the comments in the code below.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于Pallas支持的操作存在一些[限制](https://jax.readthedocs.io/en/latest/pallas/tpu/details.html#supported-operations)（截至本文撰写时），为了实现“argmax”函数和对选中框的有效性检查，需要进行一些技巧性操作。为了简洁起见，我们省略了技术细节，并将感兴趣的读者指向下面代码中的注释部分。
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The average runtime of our custom NMS operator is 0.139 milliseconds, making
    it roughly three times faster than our JAX-native implementation. This result
    highlights the potential of tailoring the implementation of sequential algorithms
    to the unique properties of the TPU architecture.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的自定义NMS操作符的平均运行时间为0.139毫秒，约为我们JAX原生实现的三倍速度。这一结果突显了根据TPU架构的独特特性定制顺序算法实现的潜力。
- en: Note that in our Pallas kernel implementation, we load the full input tensors
    into [TPU VMEM memory](https://jax.readthedocs.io/en/latest/pallas/tpu/pipelining.html#constraints-of-using-vmem-smem).
    Given the limited capacity of VMEM, scaling up the input size (i.e., increase
    the number of bounding boxes) will likely lead to memory issues. Typically, such
    limitations can be addressed by [chunking the inputs](https://jax.readthedocs.io/en/latest/pallas/grid_blockspec.html#blockspec-a-k-a-how-to-chunk-up-inputs)
    with BlockSpecs. Unfortunately, applying this approach would break the current
    NMS implementation. Implementing NMS across input chunks would require a different
    design, which is beyond the scope of this post.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在我们的Pallas内核实现中，我们将完整的输入张量加载到[TPU VMEM内存](https://jax.readthedocs.io/en/latest/pallas/tpu/pipelining.html#constraints-of-using-vmem-smem)中。鉴于VMEM的容量有限，扩大输入大小（即增加边界框数量）可能会导致内存问题。通常，这些限制可以通过使用BlockSpecs对输入进行[分块](https://jax.readthedocs.io/en/latest/pallas/grid_blockspec.html#blockspec-a-k-a-how-to-chunk-up-inputs)来解决。不幸的是，应用这种方法会破坏当前的NMS实现。实现跨输入块的NMS需要不同的设计，这超出了本文的范围。
- en: Results
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: 'The results of our experiments are summarized in the table below:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实验的结果总结在下面的表格中：
- en: '![](../Images/3f06e6fa7106deb64169f9bfcf4f9b70.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f06e6fa7106deb64169f9bfcf4f9b70.png)'
- en: Results of NMS experiments (lower is better) — by Author
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: NMS实验结果（越低越好） — 作者
- en: These results demonstrate the potential for running full ML computation graphs
    on TPU, even when they include sequential components. The performance improvement
    demonstrated by our Pallas NMS operator, in particular, highlights the opportunity
    of customizing kernels in a way that leverages the TPUs strengths.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果展示了在TPU上运行完整的机器学习计算图的潜力，即使它们包含顺序组件。特别是我们的Pallas NMS操作符所展示的性能提升，突显了通过定制内核来利用TPU优势的机会。
- en: Summary
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: In our [previous post](https://chaimrand.medium.com/the-rise-of-pallas-unlocking-tpu-potential-with-custom-kernels-67be10ab846a)
    we learned of the opportunity for building custom TPU operators using the Pallas
    extension for JAX. Maximizing this opportunity requires tailoring the kernel implementations
    to the specific properties of the TPU architecture. In this post, we focused on
    the sequential nature of the TPU processor and its use in optimizing a custom
    NMS kernel. While scaling the solution to support an unrestricted number of bounding
    boxes would require further work, the core principles we have discussed remain
    applicable.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的[上一篇文章](https://chaimrand.medium.com/the-rise-of-pallas-unlocking-tpu-potential-with-custom-kernels-67be10ab846a)中，我们了解了使用Pallas扩展为JAX构建自定义TPU操作符的机会。最大化这一机会需要根据TPU架构的特性量身定制内核实现。在本文中，我们专注于TPU处理器的顺序特性及其在优化自定义NMS内核中的应用。尽管将该解决方案扩展以支持无限数量的边界框需要进一步的工作，但我们讨论的核心原理依然适用。
- en: Still in the experimental phase of its development, there remain some limitations
    in Pallas that may require creative workarounds. But the strength and potential
    are clearly evident and we anticipate that they will only increase as the framework
    matures.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Pallas仍处于开发的实验阶段，存在一些限制，可能需要创造性的解决方法。但其强大性和潜力是显而易见的，我们预计随着框架的成熟，这些优势将会进一步增强。
