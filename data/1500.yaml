- en: Gated Recurrent Units (GRU) — Improving RNNs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 门控循环单元（GRU）—— 改进的RNN
- en: 原文：[https://towardsdatascience.com/gated-recurrent-units-gru-improving-rnns-4467b66c5150?source=collection_archive---------7-----------------------#2024-06-15](https://towardsdatascience.com/gated-recurrent-units-gru-improving-rnns-4467b66c5150?source=collection_archive---------7-----------------------#2024-06-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/gated-recurrent-units-gru-improving-rnns-4467b66c5150?source=collection_archive---------7-----------------------#2024-06-15](https://towardsdatascience.com/gated-recurrent-units-gru-improving-rnns-4467b66c5150?source=collection_archive---------7-----------------------#2024-06-15)
- en: Explaining how Gated Recurrent Neural Networks work
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释门控循环神经网络（GRU）是如何工作的
- en: '[](https://medium.com/@egorhowell?source=post_page---byline--4467b66c5150--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page---byline--4467b66c5150--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4467b66c5150--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4467b66c5150--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page---byline--4467b66c5150--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@egorhowell?source=post_page---byline--4467b66c5150--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page---byline--4467b66c5150--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4467b66c5150--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4467b66c5150--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page---byline--4467b66c5150--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4467b66c5150--------------------------------)
    ·10 min read·Jun 15, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4467b66c5150--------------------------------)
    ·10分钟阅读·2024年6月15日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/c751c90c4f7c68dff3f32198562febf8.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c751c90c4f7c68dff3f32198562febf8.png)'
- en: ”[https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network)"
    title=”neural network icons”>Neural network icons created by juicy_fish — Flaticon.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '"[https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network)"
    title="神经网络图标">神经网络图标由juicy_fish创建 — Flaticon。'
- en: 'In this article, I will explore a standard implementation of recurrent neural
    networks (RNNs): gated recurrent units (GRUs).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将探讨循环神经网络（RNNs）的标准实现方式：门控循环单元（GRUs）。
- en: GRUs were introduced in 2014 by [Kyunghyun Cho et al.](https://arxiv.org/abs/1406.1078)
    and are an improvement from vanilla RNNs as they suffer less from the vanishing
    gradient problem, allowing them to have more extended memory.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: GRU于2014年由[Kyunghyun Cho等人](https://arxiv.org/abs/1406.1078)提出，是对传统RNN的改进，因为它们较少受到梯度消失问题的影响，从而拥有更长的记忆能力。
- en: They are similar to [Long-Short-Term Memory (LSTM)](/long-short-term-memory-lstm-improving-rnns-40323d1c05f8?sk=03c1ce36eba874e0e0bc0d0267eb5314)
    networks but have fewer operations, making them more memory efficient.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 它们与[长短期记忆（LSTM）](/long-short-term-memory-lstm-improving-rnns-40323d1c05f8?sk=03c1ce36eba874e0e0bc0d0267eb5314)网络类似，但操作较少，使它们在内存使用上更加高效。
- en: 'What we will cover:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖的内容：
- en: Overview of RNNs
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN概述
- en: Vanishing and Exploding Gradient Problem
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度消失与梯度爆炸问题
- en: Overview of LSTMs
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM概述
- en: How GRUs Work
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GRU是如何工作的
- en: Recurrent Neural Network
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: Recurrent neural networks are a type of neural network that is especially adept
    at sequence-based data, such as natural language and time series.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络是一种特别擅长处理基于序列的数据类型的神经网络，例如自然语言和时间序列数据。
- en: They achieve this by adding a “recurrent” neuron that allows information to
    be fed through from past inputs and outputs to the next step.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 它们通过添加一个“循环”神经元来实现这一点，允许信息从过去的输入和输出传递到下一步。
