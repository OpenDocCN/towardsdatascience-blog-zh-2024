- en: BERT — Intuitively and Exhaustively Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BERT — 直观且详尽的解释
- en: 原文：[https://towardsdatascience.com/bert-intuitively-and-exhaustively-explained-48a24ecc1c8a?source=collection_archive---------2-----------------------#2024-08-23](https://towardsdatascience.com/bert-intuitively-and-exhaustively-explained-48a24ecc1c8a?source=collection_archive---------2-----------------------#2024-08-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/bert-intuitively-and-exhaustively-explained-48a24ecc1c8a?source=collection_archive---------2-----------------------#2024-08-23](https://towardsdatascience.com/bert-intuitively-and-exhaustively-explained-48a24ecc1c8a?source=collection_archive---------2-----------------------#2024-08-23)
- en: Baking General Understanding into Language Models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将通用理解融入语言模型
- en: '[](https://medium.com/@danielwarfield1?source=post_page---byline--48a24ecc1c8a--------------------------------)[![Daniel
    Warfield](../Images/c1c8b4dd514f6813e08e401401324bca.png)](https://medium.com/@danielwarfield1?source=post_page---byline--48a24ecc1c8a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--48a24ecc1c8a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--48a24ecc1c8a--------------------------------)
    [Daniel Warfield](https://medium.com/@danielwarfield1?source=post_page---byline--48a24ecc1c8a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@danielwarfield1?source=post_page---byline--48a24ecc1c8a--------------------------------)[![Daniel
    Warfield](../Images/c1c8b4dd514f6813e08e401401324bca.png)](https://medium.com/@danielwarfield1?source=post_page---byline--48a24ecc1c8a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--48a24ecc1c8a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--48a24ecc1c8a--------------------------------)
    [Daniel Warfield](https://medium.com/@danielwarfield1?source=post_page---byline--48a24ecc1c8a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--48a24ecc1c8a--------------------------------)
    ·46 min read·Aug 23, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--48a24ecc1c8a--------------------------------)
    ·阅读时间：46分钟·2024年8月23日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a936f980e2bca9d843e27da4b08d8e3f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a936f980e2bca9d843e27da4b08d8e3f.png)'
- en: “Baking” by Daniel Warfield using MidJourney. All images by the author unless
    otherwise specified. Article originally made available on [Intuitively and Exhaustively
    Explained](https://iaee.substack.com/).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: “Baking”由Daniel Warfield使用MidJourney创作。所有图片除非另有说明，均为作者提供。文章最初发布于[直观且详尽的解释](https://iaee.substack.com/)。
- en: In this article we’ll discuss “Bidirectional Encoder Representations from Transformers”
    (BERT), a model designed to understand language. While BERT is similar to models
    like GPT, the focus of BERT is to understand text rather than generate it. This
    is useful in a variety of tasks like ranking how positive a review of a product
    is, or predicting if an answer to a question is correct.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将讨论“来自变换器的双向编码器表示”（BERT），这是一种旨在理解语言的模型。虽然BERT与像GPT这样的模型相似，但BERT的重点是理解文本，而不是生成文本。这在各种任务中都很有用，比如对产品评论的正面性进行排序，或者预测问题的答案是否正确。
- en: Before we get into BERT we’ll briefly discuss the transformer architecture,
    which is the direct inspiration of BERT. Using that understanding we’ll dive into
    BERT and discuss how it’s built and trained to solve problems by leveraging a
    general understanding of language. Finally, we’ll create a BERT model ourselves
    from scratch and use it to predict if product reviews are positive or negative.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨BERT之前，我们将简要讨论变换器架构，它是BERT的直接灵感来源。通过理解这一点，我们将深入了解BERT，并讨论它是如何构建和训练的，以通过利用语言的一般理解来解决问题。最后，我们将从零开始创建一个BERT模型，并使用它来预测产品评论是正面的还是负面的。
- en: '**Who is this useful for?** Anyone who wants to form a complete understanding
    of the state of the art of AI.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**这个对谁有用？** 任何希望全面了解人工智能前沿状态的人。'
- en: '**How advanced is this post?** Early parts of this article are accessible to
    readers of all levels, while later sections concerning the from-scratch implementation
    are fairly advanced. Supplemental resources are provided as necessary.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**这篇文章的难度如何？** 本文的前半部分适合各个水平的读者，而后半部分涉及从零开始的实现则相对较为高级。根据需要提供了补充资源。'
- en: '**Pre-requisites:** I would highly recommend understanding fundamental ideas
    about…'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**前提条件：** 我强烈建议理解关于…的基本概念。'
