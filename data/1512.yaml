- en: Foundation Models in Graph & Geometric Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å›¾ç»“æ„ä¸å‡ ä½•æ·±åº¦å­¦ä¹ ä¸­çš„åŸºç¡€æ¨¡å‹
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/foundation-models-in-graph-geometric-deep-learning-f363e2576f58?source=collection_archive---------0-----------------------#2024-06-18](https://towardsdatascience.com/foundation-models-in-graph-geometric-deep-learning-f363e2576f58?source=collection_archive---------0-----------------------#2024-06-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/foundation-models-in-graph-geometric-deep-learning-f363e2576f58?source=collection_archive---------0-----------------------#2024-06-18](https://towardsdatascience.com/foundation-models-in-graph-geometric-deep-learning-f363e2576f58?source=collection_archive---------0-----------------------#2024-06-18)
- en: '[](https://mgalkin.medium.com/?source=post_page---byline--f363e2576f58--------------------------------)[![Michael
    Galkin](../Images/c5eb13334712ca0462d8a5df4a268ad0.png)](https://mgalkin.medium.com/?source=post_page---byline--f363e2576f58--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f363e2576f58--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f363e2576f58--------------------------------)
    [Michael Galkin](https://mgalkin.medium.com/?source=post_page---byline--f363e2576f58--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mgalkin.medium.com/?source=post_page---byline--f363e2576f58--------------------------------)[![Michael
    Galkin](../Images/c5eb13334712ca0462d8a5df4a268ad0.png)](https://mgalkin.medium.com/?source=post_page---byline--f363e2576f58--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f363e2576f58--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f363e2576f58--------------------------------)
    [Michael Galkin](https://mgalkin.medium.com/?source=post_page---byline--f363e2576f58--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f363e2576f58--------------------------------)
    Â·20 min readÂ·Jun 18, 2024
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f363e2576f58--------------------------------)
    Â·é˜…è¯»æ—¶é—´20åˆ†é’ŸÂ·2024å¹´6æœˆ18æ—¥
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Foundation Models in language, vision, and audio have been among the primary
    research topics in Machine Learning in 2024 whereas FMs for graph-structured data
    have somewhat lagged behind. In this post, we argue that the era of Graph FMs
    has already begun and provide a few examples of how one can use them already today.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è¯­è¨€ã€è§†è§‰å’ŒéŸ³é¢‘ä¸­çš„åŸºç¡€æ¨¡å‹å·²æˆä¸º2024å¹´æœºå™¨å­¦ä¹ çš„ä¸»è¦ç ”ç©¶è¯¾é¢˜ï¼Œè€Œå›¾ç»“æ„æ•°æ®çš„åŸºç¡€æ¨¡å‹åˆ™ç›¸å¯¹æ»åã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬è®¤ä¸ºå›¾åŸºç¡€æ¨¡å‹çš„æ—¶ä»£å·²ç»å¼€å§‹ï¼Œå¹¶æä¾›äº†ä¸€äº›å¦‚ä½•ä»Šå¤©å°±èƒ½ä½¿ç”¨å®ƒä»¬çš„ç¤ºä¾‹ã€‚
- en: '*This post was written and edited by* [*Michael Galkin*](https://twitter.com/michael_galkin)
    *and* [*Michael Bronstein*](https://twitter.com/mmbronstein) *with significant
    contributions from* [*Jianan Zhao*](https://twitter.com/AndyJiananZhao)*,* [*Haitao
    Mao*](https://twitter.com/haitao_mao_)*,* [*Zhaocheng Zhu*](https://twitter.com/zhu_zhaocheng)*.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*æœ¬æ–‡ç”±* [*Michael Galkin*](https://twitter.com/michael_galkin) *å’Œ* [*Michael
    Bronstein*](https://twitter.com/mmbronstein) *ç¼–å†™å’Œç¼–è¾‘ï¼Œå¹¶å¾—åˆ°äº†* [*Jianan Zhao*](https://twitter.com/AndyJiananZhao)
    *ã€* [*Haitao Mao*](https://twitter.com/haitao_mao_) *ã€* [*Zhaocheng Zhu*](https://twitter.com/zhu_zhaocheng)
    *çš„é‡å¤§è´¡çŒ®ã€‚*'
- en: '![](../Images/499b977cbe07cc686ff86f6c35480d28.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/499b977cbe07cc686ff86f6c35480d28.png)'
- en: The timeline of emerging foundation models in graph- and geometric deep learning.
    Image by Authors.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾å½¢å’Œå‡ ä½•æ·±åº¦å­¦ä¹ ä¸­åŸºç¡€æ¨¡å‹çš„æ—¶é—´çº¿ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: '**Table of Contents**'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ç›®å½•**'
- en: '[What are Graph Foundation Models and how to build them?](#6f0a)'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯å›¾åŸºç¡€æ¨¡å‹ï¼Œå¦‚ä½•æ„å»ºå®ƒä»¬ï¼Ÿ](#6f0a)'
- en: '[Node Classification: GraphAny](#b4b7)'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[èŠ‚ç‚¹åˆ†ç±»ï¼šGraphAny](#b4b7)'
- en: '[Link Prediction: Not yet](#89a1)'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[é“¾æ¥é¢„æµ‹ï¼šå°šæœª](#89a1)'
- en: '[Knowledge Graph Reasoning: ULTRA and UltraQuery](#bda1)'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[çŸ¥è¯†å›¾æ¨ç†ï¼šULTRAä¸UltraQuery](#bda1)'
- en: '[Algorithmic Reasoning: Generalist Algorithmic Learner](#11c3)'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ç®—æ³•æ¨ç†ï¼šé€šç”¨ç®—æ³•å­¦ä¹ è€…](#11c3)'
- en: '[Geometric and AI4Science Foundation Models](#65b0)a. [ML Potentials: JMP-1,
    DPA-2 for molecules, MACE-MP-0 and MatterSim for inorganic crystals](#4d3e)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[å‡ ä½•å­¦ä¸AI4ScienceåŸºç¡€æ¨¡å‹](#65b0)a. [æœºå™¨å­¦ä¹ æ½œåŠ›ï¼šJMP-1ï¼ŒDPA-2ç”¨äºåˆ†å­ï¼ŒMACE-MP-0å’ŒMatterSimç”¨äºæ— æœºæ™¶ä½“](#4d3e)'
- en: 'b. [Protein LMs: ESM-2](#b2cd)'
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. [è›‹ç™½è´¨è¯­è¨€æ¨¡å‹ï¼šESM-2](#b2cd)
- en: 'c. [2D Molecules: MiniMol and MolGPS](#cc8c)'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c. [äºŒç»´åˆ†å­ï¼šMiniMolå’ŒMolGPS](#cc8c)
- en: '[Expressivity & Scaling Laws: Do Graph FMs scale?](#1443)'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[è¡¨è¾¾æ€§ä¸æ‰©å±•è§„å¾‹ï¼šå›¾åŸºç¡€æ¨¡å‹èƒ½æ‰©å±•å—ï¼Ÿ](#1443)'
- en: '[The Data Question: What should be scaled? Is there enough graph data to train
    Graph FMs?](#40c5)'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[æ•°æ®é—®é¢˜ï¼šåº”è¯¥æ‰©å±•ä»€ä¹ˆï¼Ÿæ˜¯å¦æœ‰è¶³å¤Ÿçš„å›¾æ•°æ®æ¥è®­ç»ƒå›¾åŸºç¡€æ¨¡å‹ï¼Ÿ](#40c5)'
- en: '[ğŸ‘‰ Key Takeaways ğŸ‘ˆ](#2add)'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ğŸ‘‰ å…³é”®è¦ç‚¹ ğŸ‘ˆ](#2add)'
- en: What are Graph Foundation Models and how to build them?
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯å›¾åŸºç¡€æ¨¡å‹ï¼Œå¦‚ä½•æ„å»ºå®ƒä»¬ï¼Ÿ
- en: 'Since there is a certain degree of ambiguity in what counts as a â€œfoundationalâ€
    model, it would be appropriate to start with a definition to establish a common
    ground:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºâ€œåŸºç¡€â€æ¨¡å‹çš„å®šä¹‰å­˜åœ¨ä¸€å®šçš„æ¨¡ç³Šæ€§ï¼Œæœ€å¥½ä»ä¸€ä¸ªå®šä¹‰å¼€å§‹ï¼Œä»¥å»ºç«‹å…±è¯†ï¼š
- en: â€œA Graph Foundation Model is a single (neural) model that learns transferable
    graph representations that can generalize to any new, previously unseen graphâ€
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œå›¾åŸºç¡€æ¨¡å‹æ˜¯ä¸€ä¸ªå•ä¸€çš„ï¼ˆç¥ç»ï¼‰æ¨¡å‹ï¼Œå®ƒå­¦ä¹ å¯è¿ç§»çš„å›¾è¡¨ç¤ºï¼Œå¯ä»¥æ³›åŒ–åˆ°ä»»ä½•æ–°çš„ã€ä»¥å‰æœªè§è¿‡çš„å›¾â€
- en: One of the challenges is that graphs come in all forms and shapes and their
    connectivity and feature structure can be very different. Standard Graph Neural
    Networks (GNNs) are not â€œfoundationalâ€ because they can work in the best case
    only on graphs with the same type and dimension of features. Graph heuristics
    like [Label Propagation](https://en.wikipedia.org/wiki/Label_propagation_algorithm)
    or [Personalized PageRank](https://en.wikipedia.org/wiki/PageRank) that can run
    on any graph can neither be considered Graph FMs because they do not involve any
    learning. As much as we love Large Language Models, it is still unclear whether
    parsing graphs into sequences that can be then passed to an LLM (like in [GraphText](https://arxiv.org/abs/2310.01089)
    or [Talk Like A Graph](https://openreview.net/forum?id=IuXR1CCrSi)) is a suitable
    approach for retaining graph symmetries and scaling to anything larger than toy-sized
    datasets (we leave LLMs + Graphs to a separate post).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æŒ‘æˆ˜ä¹‹ä¸€æ˜¯å›¾çš„å½¢å¼å’Œå½¢çŠ¶å„å¼‚ï¼Œå®ƒä»¬çš„è¿é€šæ€§å’Œç‰¹å¾ç»“æ„å¯èƒ½å¤§ä¸ç›¸åŒã€‚æ ‡å‡†çš„å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰å¹¶ä¸æ˜¯â€œåŸºç¡€æ€§â€çš„ï¼Œå› ä¸ºå®ƒä»¬åœ¨æœ€ä½³æƒ…å†µä¸‹åªèƒ½åœ¨å…·æœ‰ç›¸åŒç±»å‹å’Œç»´åº¦ç‰¹å¾çš„å›¾ä¸Šå·¥ä½œã€‚åƒ[æ ‡ç­¾ä¼ æ’­](https://en.wikipedia.org/wiki/Label_propagation_algorithm)æˆ–[ä¸ªæ€§åŒ–PageRank](https://en.wikipedia.org/wiki/PageRank)è¿™æ ·çš„å›¾å¯å‘å¼æ–¹æ³•å¯ä»¥åœ¨ä»»ä½•å›¾ä¸Šè¿è¡Œï¼Œä½†ä¸èƒ½è¢«è§†ä¸ºå›¾FMsï¼Œå› ä¸ºå®ƒä»¬ä¸æ¶‰åŠä»»ä½•å­¦ä¹ ã€‚å°½ç®¡æˆ‘ä»¬å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æƒ…æœ‰ç‹¬é’Ÿï¼Œä½†ä»ä¸æ¸…æ¥šå°†å›¾è§£æä¸ºåºåˆ—å¹¶å°†å…¶ä¼ é€’ç»™LLMï¼ˆä¾‹å¦‚åœ¨[GraphText](https://arxiv.org/abs/2310.01089)æˆ–[Talk
    Like A Graph](https://openreview.net/forum?id=IuXR1CCrSi)ä¸­é‚£æ ·ï¼‰æ˜¯å¦æ˜¯ä¿æŒå›¾çš„å¯¹ç§°æ€§å¹¶æ‰©å±•åˆ°å¤§äºç©å…·çº§æ•°æ®é›†çš„åˆé€‚æ–¹æ³•ï¼ˆæˆ‘ä»¬å°†LLMsä¸å›¾çš„ç»“åˆç•™å¾…å•ç‹¬çš„æ–‡ç« è®¨è®ºï¼‰ã€‚
- en: 'Perhaps the most important question for designing Graph FMs is transferable
    graph representations. LLMs, as suggested in the recent [ICML 2024 position paper
    by Mao, Chen et al](https://arxiv.org/abs/2402.02216)., can squash any text in
    any language into tokens from a fixed-size vocabulary. Video-Language FMs resort
    to patches that can always be extracted from an image (one always has RGB channels
    in any image or video). It is not immediately clear what could a universal featurization
    (Ã  la tokenization) scheme be for graphs, which might have very diverse characteristics,
    e.g.:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿè®¸è®¾è®¡å›¾FMsæ—¶æœ€é‡è¦çš„é—®é¢˜æ˜¯å¯è¿ç§»çš„å›¾è¡¨ç¤ºã€‚æ­£å¦‚æœ€è¿‘[ICML 2024å¹´ç”±Maoã€Chenç­‰äººæå‡ºçš„å®šä½è®ºæ–‡](https://arxiv.org/abs/2402.02216)ä¸­æ‰€å»ºè®®çš„ï¼ŒLLMså¯ä»¥å°†ä»»ä½•è¯­è¨€çš„æ–‡æœ¬å‹ç¼©ä¸ºå›ºå®šå¤§å°è¯æ±‡è¡¨ä¸­çš„æ ‡è®°ã€‚è§†é¢‘-è¯­è¨€FMsä¾èµ–äºå¯ä»¥ä»å›¾åƒä¸­æå–çš„è¡¥ä¸ï¼ˆä»»ä½•å›¾åƒæˆ–è§†é¢‘ä¸­æ€»æ˜¯æœ‰RGBé€šé“ï¼‰ã€‚ç›®å‰è¿˜ä¸æ¸…æ¥šå›¾çš„æ™®é€‚ç‰¹å¾åŒ–ï¼ˆç±»ä¼¼äºæ ‡è®°åŒ–ï¼‰æ–¹æ¡ˆæ˜¯ä»€ä¹ˆï¼Œå› ä¸ºå›¾å¯èƒ½å…·æœ‰éå¸¸å¤šæ ·çš„ç‰¹å¾ï¼Œä¾‹å¦‚ï¼š
- en: One large graph with node features and some given node labels (typical for node
    classification tasks)
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå¤§å‹å›¾ï¼Œå¸¦æœ‰èŠ‚ç‚¹ç‰¹å¾å’Œä¸€äº›ç»™å®šçš„èŠ‚ç‚¹æ ‡ç­¾ï¼ˆå…¸å‹çš„èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ï¼‰
- en: One large graph without node features and classes, but with meaningful edge
    types (typical for link prediction and KG reasoning)
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå¤§å‹å›¾ï¼Œæ²¡æœ‰èŠ‚ç‚¹ç‰¹å¾å’Œç±»åˆ«ï¼Œä½†æœ‰æœ‰æ„ä¹‰çš„è¾¹ç±»å‹ï¼ˆå…¸å‹çš„é“¾æ¥é¢„æµ‹å’ŒçŸ¥è¯†å›¾è°±æ¨ç†ä»»åŠ¡ï¼‰
- en: Many small graphs with/without node/edge features, with graph-level labels (typical
    for graph classification and regression)
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¸å¤šå°å›¾ï¼Œå¸¦/ä¸å¸¦èŠ‚ç‚¹/è¾¹ç‰¹å¾ï¼Œå¹¶ä¸”æœ‰å›¾çº§æ ‡ç­¾ï¼ˆå…¸å‹çš„å›¾åˆ†ç±»å’Œå›å½’ä»»åŠ¡ï¼‰
- en: '![](../Images/af6091af7bbb5362938fa26f9fca562c.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af6091af7bbb5362938fa26f9fca562c.png)'
- en: ğŸ¦„ An ideal graph foundation model that takes any graph with any node/edge/graph
    features and performs any node- / edge- / graph-level task. Such Graph FMs do
    not exist in pure form as of mid-2024\. Image by Authors
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¦„ ä¸€ä¸ªç†æƒ³çš„å›¾åŸºç¡€æ¨¡å‹ï¼Œå®ƒèƒ½å¤Ÿå¤„ç†ä»»ä½•å¸¦æœ‰ä»»æ„èŠ‚ç‚¹/è¾¹/å›¾ç‰¹å¾çš„å›¾ï¼Œå¹¶æ‰§è¡Œä»»ä½•èŠ‚ç‚¹/è¾¹/å›¾çº§ä»»åŠ¡ã€‚è¿™æ ·çš„å›¾FMsåœ¨2024å¹´ä¸­æœŸä¹‹å‰å¹¶ä¸å­˜åœ¨ã€‚å›¾åƒæ¥æºï¼šä½œè€…
- en: 'So far, there is a handful of open research questions for the graph learning
    community when designing Graph FMs:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œåœ¨è®¾è®¡å›¾FMsæ—¶ï¼Œå›¾å­¦ä¹ ç¤¾åŒºæœ‰ä¸€äº›å¼€æ”¾çš„ç ”ç©¶é—®é¢˜ï¼š
- en: '**1ï¸âƒ£ How to generalize across graphs with heterogeneous node/edge/graph features?**
    For example, the popular [Cora](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.Planetoid.html#torch_geometric.datasets.Planetoid)
    dataset for node classification is one graph with node features of dimension 1,433,
    whereas the Citeseer dataset has 3,703-dimensional features. How can one define
    a single representation space for such diverse graphs?'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**1ï¸âƒ£ å¦‚ä½•åœ¨å…·æœ‰å¼‚æ„èŠ‚ç‚¹/è¾¹/å›¾ç‰¹å¾çš„å›¾ä¹‹é—´è¿›è¡Œæ³›åŒ–ï¼Ÿ** ä¾‹å¦‚ï¼Œç”¨äºèŠ‚ç‚¹åˆ†ç±»çš„æµè¡Œæ•°æ®é›†[Cora](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.Planetoid.html#torch_geometric.datasets.Planetoid)æ˜¯ä¸€ä¸ªå›¾ï¼ŒèŠ‚ç‚¹ç‰¹å¾çš„ç»´åº¦ä¸º1,433ï¼Œè€ŒCiteseeræ•°æ®é›†çš„ç‰¹å¾ç»´åº¦ä¸º3,703ã€‚å¦‚ä½•ä¸ºå¦‚æ­¤å¤šæ ·åŒ–çš„å›¾å®šä¹‰ä¸€ä¸ªå•ä¸€çš„è¡¨ç¤ºç©ºé—´ï¼Ÿ'
- en: '**2ï¸âƒ£ How to generalize across prediction tasks?** Node classification tasks
    may have a different number of node classes (e.g., Cora has 7 classes and Citeseer
    6). Even further, can a node classification model perform well in link prediction?'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**2ï¸âƒ£ å¦‚ä½•åœ¨é¢„æµ‹ä»»åŠ¡ä¸­è¿›è¡Œæ³›åŒ–ï¼Ÿ** èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡å¯èƒ½æœ‰ä¸åŒæ•°é‡çš„èŠ‚ç‚¹ç±»åˆ«ï¼ˆä¾‹å¦‚ï¼ŒCoraæœ‰7ä¸ªç±»åˆ«ï¼ŒCiteseeræœ‰6ä¸ªç±»åˆ«ï¼‰ã€‚æ›´è¿›ä¸€æ­¥ï¼ŒèŠ‚ç‚¹åˆ†ç±»æ¨¡å‹èƒ½å¦åœ¨é“¾æ¥é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Ÿ'
- en: '**3ï¸âƒ£ What should the foundational model expressivity be?** Much research has
    been done on the expressive power of GNNs, typically resorting to the analogy
    with Weisfeiler-Lehman isomorphism tests. Since graph foundational models should
    ideally handle a broad spectrum of problems, the right expressive power is elusive.
    For instance, in node classification tasks, node features are important along
    with graph homophily or heterophily. In link prediction, structural patterns and
    breaking automorphisms are more important (node features often donâ€™t give a huge
    performance boost). In graph-level tasks, graph isomorphism starts to play a crucial
    role. In 3D geometric tasks like molecule generation, there is an additional complexity
    of continuous symmetries to take care of (see the [Hitchhikerâ€™s Guide to Geometric
    GNNs](https://arxiv.org/abs/2312.07511)).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**3ï¸âƒ£ åŸºç¡€æ¨¡å‹çš„è¡¨ç°åŠ›åº”è¯¥æ˜¯å¤šå°‘ï¼Ÿ** è®¸å¤šå…³äºGNNè¡¨ç°åŠ›çš„ç ”ç©¶å·²ç»å®Œæˆï¼Œé€šå¸¸å€Ÿç”¨Weisfeiler-LehmanåŒæ„æ€§æµ‹è¯•çš„ç±»æ¯”ã€‚ç”±äºå›¾å½¢åŸºç¡€æ¨¡å‹ç†æƒ³æƒ…å†µä¸‹åº”è¯¥å¤„ç†å¹¿æ³›çš„é—®é¢˜ï¼Œå› æ­¤åˆé€‚çš„è¡¨ç°åŠ›ä»ç„¶éš¾ä»¥æ‰æ‘¸ã€‚ä¾‹å¦‚ï¼Œåœ¨èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒèŠ‚ç‚¹ç‰¹å¾ä¸å›¾å½¢çš„åŒè´¨æ€§æˆ–å¼‚è´¨æ€§éƒ½å¾ˆé‡è¦ã€‚åœ¨é“¾æ¥é¢„æµ‹ä¸­ï¼Œç»“æ„æ¨¡å¼å’Œæ‰“ç ´è‡ªåŒæ„æ€§æ›´åŠ é‡è¦ï¼ˆèŠ‚ç‚¹ç‰¹å¾é€šå¸¸ä¸ä¼šå¸¦æ¥å·¨å¤§çš„æ€§èƒ½æå‡ï¼‰ã€‚åœ¨å›¾å½¢çº§ä»»åŠ¡ä¸­ï¼Œå›¾å½¢åŒæ„æ€§å¼€å§‹å‘æŒ¥é‡è¦ä½œç”¨ã€‚åœ¨åƒåˆ†å­ç”Ÿæˆè¿™æ ·çš„ä¸‰ç»´å‡ ä½•ä»»åŠ¡ä¸­ï¼Œè¿˜éœ€è¦è€ƒè™‘è¿ç»­å¯¹ç§°æ€§å¸¦æ¥çš„é¢å¤–å¤æ‚æ€§ï¼ˆå‚è§[å‡ ä½•GNNçš„æ­ä¾¿è½¦æŒ‡å—](https://arxiv.org/abs/2312.07511)ï¼‰ã€‚'
- en: In the following sections, we will show that at least in some tasks and domains,
    Graph FMs are already available. We will highlight their design choices when it
    comes to transferable features and practical benefits when it comes to inductive
    inference on new unseen graphs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†å±•ç¤ºï¼Œè‡³å°‘åœ¨æŸäº›ä»»åŠ¡å’Œé¢†åŸŸä¸­ï¼Œå›¾å½¢åŸºç¡€æ¨¡å‹ï¼ˆGraph FMsï¼‰å·²ç»å¯ç”¨ã€‚æˆ‘ä»¬å°†é‡ç‚¹ä»‹ç»å®ƒä»¬åœ¨å¯è¿ç§»ç‰¹å¾æ–¹é¢çš„è®¾è®¡é€‰æ‹©ï¼Œä»¥åŠåœ¨æ–°æœªè§å›¾å½¢ä¸Šçš„å½’çº³æ¨ç†æ—¶å¸¦æ¥çš„å®é™…å¥½å¤„ã€‚
- en: '**ğŸ“šRead more in references [1][2] and** [**Github Repo**](https://github.com/CurryTang/Awesome_Graph_Foundation_Models)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸ“šæ›´å¤šå†…å®¹è¯·å‚è§å‚è€ƒæ–‡çŒ®[1][2]å’Œ** [**Githubä»“åº“**](https://github.com/CurryTang/Awesome_Graph_Foundation_Models)'
- en: 'Node Classification: GraphAny'
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: èŠ‚ç‚¹åˆ†ç±»ï¼šGraphAny
- en: 'For years, GNN-based node classifiers have been limited to a single graph dataset.
    That is, given e.g. the Cora graph with 2.7K nodes, 1433-dimensional features,
    and 7 classes, one has to train a GNN specifically on the Cora graph with its
    labels and run inference on the same graph. Applying a trained model on another
    graph, e.g. Citeseer with 3703-dimensional features and 6 classes would run into
    an unsurmountable difficulty: how would one model generalize to different input
    feature dimensions and a different number of classes? Usually, prediction heads
    are hardcoded to a fixed number of classes.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šå¹´æ¥ï¼ŒåŸºäºGNNçš„èŠ‚ç‚¹åˆ†ç±»å™¨ä»…é™äºå•ä¸€å›¾å½¢æ•°æ®é›†ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œä¾‹å¦‚ç»™å®šä¸€ä¸ªå…·æœ‰2.7Kä¸ªèŠ‚ç‚¹ã€1433ç»´ç‰¹å¾å’Œ7ä¸ªç±»åˆ«çš„Coraå›¾å½¢ï¼Œå¿…é¡»ä¸“é—¨åœ¨Coraå›¾å½¢ä¸Šè®­ç»ƒGNNï¼Œå¹¶ä½¿ç”¨è¯¥å›¾å½¢çš„æ ‡ç­¾è¿›è¡Œæ¨ç†ã€‚å¦‚æœå°†è®­ç»ƒå¥½çš„æ¨¡å‹åº”ç”¨äºå¦ä¸€ä¸ªå›¾å½¢ï¼Œä¾‹å¦‚å…·æœ‰3703ç»´ç‰¹å¾å’Œ6ä¸ªç±»åˆ«çš„Citeseerå›¾å½¢ï¼Œå°†é‡åˆ°ä¸€ä¸ªéš¾ä»¥å…‹æœçš„éš¾é¢˜ï¼šå¦‚ä½•è®©ä¸€ä¸ªæ¨¡å‹åœ¨ä¸åŒçš„è¾“å…¥ç‰¹å¾ç»´åº¦å’Œä¸åŒç±»åˆ«æ•°é‡ä¸‹è¿›è¡Œæ³›åŒ–ï¼Ÿé€šå¸¸ï¼Œé¢„æµ‹å¤´éƒ¨æ˜¯ç¡¬ç¼–ç ä¸ºå›ºå®šçš„ç±»åˆ«æ•°ã€‚
- en: '[**GraphAny**](https://arxiv.org/abs/2405.20445) is, to the best of our knowledge,
    the first Graph FM where a single pre-trained model can perform node classification
    on any graph with any feature dimension and any number of classes. A single GraphAny
    model pre-trained on 120 nodes of the standard [Wisconsin](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.WebKB.html#torch_geometric.datasets.WebKB)
    dataset successfully generalizes to 30+ other graphs of different sizes and features
    and, on average, outperforms GCN and GAT graph neural network architectures trained
    from scratch on each of those graphs.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[**GraphAny**](https://arxiv.org/abs/2405.20445)æ˜¯æˆ‘ä»¬æ‰€çŸ¥çš„ç¬¬ä¸€ä¸ªå›¾å½¢åŸºç¡€æ¨¡å‹ï¼ˆGraph FMï¼‰ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿé€šè¿‡å•ä¸€çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œåœ¨ä»»ä½•å›¾å½¢ä¸Šæ‰§è¡ŒèŠ‚ç‚¹åˆ†ç±»ï¼Œä¸”æ”¯æŒä»»æ„ç‰¹å¾ç»´åº¦å’Œç±»åˆ«æ•°é‡ã€‚ä¸€ä¸ªåœ¨æ ‡å‡†[å¨æ–¯åº·è¾›](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.WebKB.html#torch_geometric.datasets.WebKB)æ•°æ®é›†çš„120ä¸ªèŠ‚ç‚¹ä¸Šé¢„è®­ç»ƒçš„GraphAnyæ¨¡å‹ï¼Œèƒ½å¤ŸæˆåŠŸåœ°æ¨å¹¿åˆ°30å¤šä¸ªä¸åŒå¤§å°å’Œç‰¹å¾çš„å›¾å½¢ï¼Œå¹¶ä¸”å¹³å‡æ€§èƒ½è¶…è¶Šäº†ä»å¤´å¼€å§‹åœ¨æ¯ä¸ªå›¾å½¢ä¸Šè®­ç»ƒçš„GCNå’ŒGATå›¾ç¥ç»ç½‘ç»œæ¶æ„ã€‚'
- en: '![](../Images/e92e81c9cefb5768acd1e515dc448c7d.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e92e81c9cefb5768acd1e515dc448c7d.png)'
- en: 'Overview of GraphAny: LinearGNNs are used to perform non-parametric predictions
    and derive the entropy-normalized distance features. The final prediction is generated
    by fusing multiple LinearGNN predictions on each node with attention learned based
    on the distance features. Source: [Zhao et al](https://arxiv.org/abs/2405.20445).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: GraphAnyæ¦‚è¿°ï¼šLinearGNNsç”¨äºæ‰§è¡Œéå‚æ•°é¢„æµ‹ï¼Œå¹¶æ¨å¯¼ç†µå½’ä¸€åŒ–çš„è·ç¦»ç‰¹å¾ã€‚æœ€ç»ˆé¢„æµ‹é€šè¿‡èåˆæ¯ä¸ªèŠ‚ç‚¹ä¸Šå¤šä¸ªLinearGNNé¢„æµ‹çš„ç»“æœï¼Œå¹¶æ ¹æ®è·ç¦»ç‰¹å¾å­¦ä¹ çš„æ³¨æ„åŠ›ç”Ÿæˆã€‚æ¥æºï¼š[Zhao
    et al](https://arxiv.org/abs/2405.20445)ã€‚
- en: '**Setup:** Semi-supervised node classification: given a graph G, node features
    X, and a few labeled nodes from C classes, predict labels of target nodes (binary
    or multi-class classification). The dimension of node features and the number
    of unique classes are not fixed and are graph-dependent.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®¾ç½®ï¼š** åŠç›‘ç£èŠ‚ç‚¹åˆ†ç±»ï¼šç»™å®šä¸€ä¸ªå›¾Gã€èŠ‚ç‚¹ç‰¹å¾Xä»¥åŠæ¥è‡ªCä¸ªç±»çš„å°‘é‡æ ‡è®°èŠ‚ç‚¹ï¼Œé¢„æµ‹ç›®æ ‡èŠ‚ç‚¹çš„æ ‡ç­¾ï¼ˆäºŒåˆ†ç±»æˆ–å¤šåˆ†ç±»ï¼‰ã€‚èŠ‚ç‚¹ç‰¹å¾çš„ç»´åº¦å’Œç±»åˆ«çš„æ•°é‡ä¸æ˜¯å›ºå®šçš„ï¼Œå–å†³äºå›¾ã€‚'
- en: '**What is transferable:** Instead of modeling a universal latent space for
    all possible graphs (which is quite cumbersome or maybe even practically impossible),
    GraphAny bypasses this problem and focuses on the *interactions between predictions
    of spectral filters*. Given a collection of high-pass and low-pass filters akin
    to [Simplified Graph Convolutions](https://arxiv.org/abs/1902.07153) (for instance,
    operations of the form AX and (I-A)X, dubbed â€œLinearGNNsâ€ in the paper) and known
    node labels:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»€ä¹ˆæ˜¯å¯è¿ç§»çš„ï¼š** GraphAnyä¸æ˜¯ä¸ºæ‰€æœ‰å¯èƒ½çš„å›¾å»ºæ¨¡ä¸€ä¸ªé€šç”¨çš„æ½œåœ¨ç©ºé—´ï¼ˆè¿™ç›¸å½“ç¹çï¼Œç”šè‡³å¯èƒ½åœ¨å®é™…ä¸­ä¸å¯è¡Œï¼‰ï¼Œè€Œæ˜¯ç»•è¿‡äº†è¿™ä¸ªé—®é¢˜ï¼Œä¸“æ³¨äº*é¢„æµ‹çš„è°±æ»¤æ³¢å™¨ä¹‹é—´çš„ç›¸äº’ä½œç”¨*ã€‚ç»™å®šä¸€ç»„é«˜é€šå’Œä½é€šæ»¤æ³¢å™¨ï¼Œç±»ä¼¼äº[ç®€åŒ–å›¾å·ç§¯](https://arxiv.org/abs/1902.07153)ï¼ˆä¾‹å¦‚ï¼Œå½¢å¦‚AXå’Œ(I-A)Xçš„æ“ä½œï¼Œåœ¨è®ºæ–‡ä¸­è¢«ç§°ä¸ºâ€œLinearGNNsâ€ï¼‰ä»¥åŠå·²çŸ¥çš„èŠ‚ç‚¹æ ‡ç­¾ï¼š'
- en: 0ï¸âƒ£ GraphAny applies filters to all nodes;
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 0ï¸âƒ£ GraphAnyå°†æ»¤æ³¢å™¨åº”ç”¨äºæ‰€æœ‰èŠ‚ç‚¹ï¼›
- en: 1ï¸âƒ£ GraphAny obtains optimal weights for each predictor from nodes with known
    labels by solving a least squares optimization problem in closed form (optimal
    weights are expressed as a pseudoinverse);
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ GraphAnyé€šè¿‡æ±‚è§£ä¸€ä¸ªæœ€å°äºŒä¹˜ä¼˜åŒ–é—®é¢˜ï¼Œè·å¾—æ¥è‡ªå…·æœ‰å·²çŸ¥æ ‡ç­¾èŠ‚ç‚¹çš„æ¯ä¸ªé¢„æµ‹å™¨çš„æœ€ä½³æƒé‡ï¼ˆæœ€ä½³æƒé‡è¡¨ç¤ºä¸ºä¼ªé€†ï¼‰ï¼›
- en: 2ï¸âƒ£ Applies the optimal weights to unknown nodes to get tentative prediction
    logits;
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ å°†æœ€ä½³æƒé‡åº”ç”¨äºæœªçŸ¥èŠ‚ç‚¹ï¼Œä»¥è·å¾—åˆæ­¥é¢„æµ‹å¯¹æ•°å€¼ï¼›
- en: 3ï¸âƒ£ Computes pair-wise distances between those logits and applies entropy regularization
    (such that different graph- and feature sizes will not affect the distribution).
    For example, for 5 LinearGNNs, this would result in 5 x 4 = 20 combinations of
    logit scores;
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 3ï¸âƒ£ è®¡ç®—è¿™äº›å¯¹æ•°å€¼ä¹‹é—´çš„æˆå¯¹è·ç¦»ï¼Œå¹¶åº”ç”¨ç†µæ­£åˆ™åŒ–ï¼ˆä½¿å¾—ä¸åŒçš„å›¾å’Œç‰¹å¾å¤§å°ä¸ä¼šå½±å“åˆ†å¸ƒï¼‰ã€‚ä¾‹å¦‚ï¼Œå¯¹äº5ä¸ªLinearGNNsï¼Œè¿™å°†äº§ç”Ÿ5 x 4 =
    20ç§å¯¹æ•°åˆ†æ•°çš„ç»„åˆï¼›
- en: 4ï¸âƒ£ Learns the inductive attention matrix over those logits to weight the predictions
    most effectively (e.g., putting more attention to high-pass filters for heterophilic
    graphs).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 4ï¸âƒ£ å­¦ä¹ è¿™äº›å¯¹æ•°å€¼çš„å½’çº³æ³¨æ„åŠ›çŸ©é˜µï¼Œä»¥æœ€æœ‰æ•ˆåœ°åŠ æƒé¢„æµ‹ï¼ˆä¾‹å¦‚ï¼Œç»™å¼‚è´¨å›¾çš„é«˜é€šæ»¤æ³¢å™¨æ›´å¤šçš„å…³æ³¨ï¼‰ã€‚
- en: In the end, the only learnable component in the model is the parameterization
    of attention (via MLP), which *does not depend* on the target number of unique
    classes, but only on the number of LinearGNNs used. In the same vein, all LinearGNN
    predictors are non-parametric, their updated node features and optimal weights
    can be pre-computed beforehand for faster inference.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆï¼Œæ¨¡å‹ä¸­å”¯ä¸€éœ€è¦å­¦ä¹ çš„éƒ¨åˆ†æ˜¯æ³¨æ„åŠ›çš„å‚æ•°åŒ–ï¼ˆé€šè¿‡MLPï¼‰ï¼Œå®ƒ*ä¸ä¾èµ–äº*ç›®æ ‡ç±»åˆ«çš„æ•°é‡ï¼Œè€Œä»…ä»…ä¾èµ–äºä½¿ç”¨çš„LinearGNNsçš„æ•°é‡ã€‚ç±»ä¼¼åœ°ï¼Œæ‰€æœ‰LinearGNNé¢„æµ‹å™¨éƒ½æ˜¯éå‚æ•°çš„ï¼Œå®ƒä»¬æ›´æ–°åçš„èŠ‚ç‚¹ç‰¹å¾å’Œæœ€ä½³æƒé‡å¯ä»¥é¢„å…ˆè®¡ç®—ï¼Œä»¥åŠ é€Ÿæ¨ç†ã€‚
- en: '**ğŸ“šRead more in references [3]**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸ“šè¯¦ç»†ä¿¡æ¯è§å‚è€ƒæ–‡çŒ®[3]**'
- en: 'Link Prediction: Not yet'
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é“¾æ¥é¢„æµ‹ï¼šå°šæœªå®ç°
- en: '**Setup**: given a graph G, with or without node features, predict whether
    a link exists between a pair of nodes (v1, v2)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®¾ç½®**ï¼šç»™å®šä¸€ä¸ªå›¾Gï¼Œæ˜¯å¦æœ‰èŠ‚ç‚¹ç‰¹å¾ï¼Œé¢„æµ‹ä¸€å¯¹èŠ‚ç‚¹ï¼ˆv1, v2ï¼‰ä¹‹é—´æ˜¯å¦å­˜åœ¨é“¾æ¥'
- en: ğŸ˜¢ For graphs with node features, we are not aware of any single transferable
    model for link prediction.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜¢ å¯¹äºå¸¦æœ‰èŠ‚ç‚¹ç‰¹å¾çš„å›¾ï¼Œæˆ‘ä»¬å°šæœªå‘ç°ä»»ä½•é€‚ç”¨äºé“¾æ¥é¢„æµ‹çš„é€šç”¨å¯è¿ç§»æ¨¡å‹ã€‚
- en: For non-featurized graphs (or when you decide to omit node features deliberately),
    there is more to say â€” basically, all GNNs with a labeling trick *potentially*
    can transfer to new graphs thanks to the uniform node featurization strategy.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ²¡æœ‰ç‰¹å¾çš„å›¾ï¼ˆæˆ–å½“ä½ å†³å®šæ•…æ„çœç•¥èŠ‚ç‚¹ç‰¹å¾æ—¶ï¼‰ï¼Œæœ‰æ›´å¤šè¦è¯´çš„â€”â€”åŸºæœ¬ä¸Šï¼Œæ‰€æœ‰å¸¦æ ‡ç­¾æŠ€å·§çš„GNN *æœ‰å¯èƒ½* å¯ä»¥é€šè¿‡ç»Ÿä¸€çš„èŠ‚ç‚¹ç‰¹å¾åŒ–ç­–ç•¥è¿ç§»åˆ°æ–°å›¾ã€‚
- en: It is known that in link prediction, the biggest hurdle is the presence of automorphic
    nodes (nodes that have the same structural roles) â€” vanilla GNNs assign them the
    same feature making two links (v1, v2) and (v1, v3) in the image below ğŸ‘‡ indistinguishable.
    [Labeling tricks](https://arxiv.org/abs/2010.16103) like [Double Radius Node Labeling](https://proceedings.neurips.cc/paper/2018/hash/53f0d7c537d99b3824f0f99d62ea2428-Abstract.html)
    or [Distance Encoding](https://proceedings.neurips.cc/paper_files/paper/2020/hash/2f73168bf3656f697507752ec592c437-Abstract.html)
    are such node featurization strategies that break automorphism symmetries.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼—æ‰€å‘¨çŸ¥ï¼Œåœ¨é“¾æ¥é¢„æµ‹ä¸­ï¼Œæœ€å¤§çš„éšœç¢æ˜¯è‡ªåŒæ„èŠ‚ç‚¹çš„å­˜åœ¨ï¼ˆå…·æœ‰ç›¸åŒç»“æ„è§’è‰²çš„èŠ‚ç‚¹ï¼‰â€”â€”æ™®é€šçš„GNNä¼šå°†å®ƒä»¬åˆ†é…ç›¸åŒçš„ç‰¹å¾ï¼Œä»è€Œä½¿å¾—ä¸‹å›¾ä¸­çš„ä¸¤ä¸ªé“¾æ¥(v1,
    v2)å’Œ(v1, v3)å˜å¾—ä¸å¯åŒºåˆ†ã€‚[æ ‡ç­¾æŠ€å·§](https://arxiv.org/abs/2010.16103)å¦‚[åŒåŠå¾„èŠ‚ç‚¹æ ‡ç­¾åŒ–](https://proceedings.neurips.cc/paper/2018/hash/53f0d7c537d99b3824f0f99d62ea2428-Abstract.html)æˆ–[è·ç¦»ç¼–ç ](https://proceedings.neurips.cc/paper_files/paper/2020/hash/2f73168bf3656f697507752ec592c437-Abstract.html)æ˜¯æ‰“ç ´è‡ªåŒæ„å¯¹ç§°æ€§çš„èŠ‚ç‚¹ç‰¹å¾åŒ–ç­–ç•¥ã€‚
- en: '![](../Images/0c043afe3e726a494270ae3dab284874.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c043afe3e726a494270ae3dab284874.png)'
- en: 'V2 and v3 are automorphic nodes and standard GNNs score (v1,v2) and (v1,v3)
    equally. When we predict (v1, v2), we will label these two nodes differently from
    the rest, so that a GNN is aware of the target link when learning v1 and v2â€™s
    representations. Similarly, when predicting (v1, v3), nodes v1 and v3 will be
    labeled differently. This way, the representation of v2 in the left graph will
    be different from that of v3 in the right graph, enabling GNNs to distinguish
    the non-isomorphic links (v1, v2) and (v1, v3). Source: [Zhang et al](https://arxiv.org/abs/2010.16103).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: V2å’Œv3æ˜¯è‡ªåŒæ„èŠ‚ç‚¹ï¼Œæ ‡å‡†çš„GNNä¼šå¯¹(v1,v2)å’Œ(v1,v3)æ‰“ç›¸åŒçš„åˆ†æ•°ã€‚å½“æˆ‘ä»¬é¢„æµ‹(v1, v2)æ—¶ï¼Œæˆ‘ä»¬ä¼šå°†è¿™ä¸¤ä¸ªèŠ‚ç‚¹ä¸å…¶ä»–èŠ‚ç‚¹åŒºåˆ†å¼€æ¥ï¼Œä½¿å¾—GNNåœ¨å­¦ä¹ v1å’Œv2çš„è¡¨ç¤ºæ—¶èƒ½è¯†åˆ«å‡ºç›®æ ‡é“¾æ¥ã€‚ç±»ä¼¼åœ°ï¼Œå½“é¢„æµ‹(v1,
    v3)æ—¶ï¼ŒèŠ‚ç‚¹v1å’Œv3ä¼šè¢«ä¸åŒåœ°æ ‡è®°ã€‚è¿™æ ·ï¼Œå·¦å›¾ä¸­çš„v2çš„è¡¨ç¤ºä¼šä¸å³å›¾ä¸­v3çš„è¡¨ç¤ºä¸åŒï¼Œä»è€Œä½¿å¾—GNNèƒ½å¤ŸåŒºåˆ†éåŒæ„çš„é“¾æ¥(v1, v2)å’Œ(v1, v3)ã€‚æ¥æºï¼š[Zhang
    et al](https://arxiv.org/abs/2010.16103).
- en: Perhaps the only approach with a labeling trick (for non-featurized graphs)
    that was evaluated on link prediction on unseen graphs is [UniLP](https://arxiv.org/abs/2402.07738).
    UniLP is an in-context, contrastive learning model that requires a set of positive
    and negative samples for each target link to be predicted. Practically, UniLP
    uses [SEAL](https://proceedings.neurips.cc/paper/2018/hash/53f0d7c537d99b3824f0f99d62ea2428-Abstract.html)
    as a backbone GNN and learns an attention over a fixed number of positive and
    negative samples. On the other hand, SEAL is notoriously slow, so the first step
    towards making UniLP scale to large graphs is to replace subgraph mining with
    more efficient approaches like [ELPH](https://arxiv.org/abs/2209.15486) and [BUDDY](https://arxiv.org/abs/2209.15486).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿè®¸å”¯ä¸€åœ¨æœªè§å›¾ä¸Šçš„é“¾æ¥é¢„æµ‹ä¸­è¯„ä¼°è¿‡çš„å¸¦æ ‡ç­¾æŠ€å·§ï¼ˆé€‚ç”¨äºæœªç‰¹å¾åŒ–å›¾ï¼‰çš„æ–¹æ¡ˆæ˜¯[UniLP](https://arxiv.org/abs/2402.07738)ã€‚UniLPæ˜¯ä¸€ç§ä¸Šä¸‹æ–‡å¯¹æ¯”å­¦ä¹ æ¨¡å‹ï¼Œéœ€è¦ä¸ºæ¯ä¸ªç›®æ ‡é“¾æ¥é¢„æµ‹æä¾›ä¸€ç»„æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬ã€‚å®é™…ä¸Šï¼ŒUniLPä½¿ç”¨[SEAL](https://proceedings.neurips.cc/paper/2018/hash/53f0d7c537d99b3824f0f99d62ea2428-Abstract.html)ä½œä¸ºä¸»å¹²GNNï¼Œå¹¶å¯¹å›ºå®šæ•°é‡çš„æ­£è´Ÿæ ·æœ¬è¿›è¡Œå­¦ä¹ æ³¨æ„åŠ›ã€‚å¦ä¸€æ–¹é¢ï¼ŒSEALä»¥å…¶æ…¢é€Ÿè‘—ç§°ï¼Œå› æ­¤ä½¿UniLPèƒ½å¤Ÿæ‰©å±•åˆ°å¤§è§„æ¨¡å›¾çš„ç¬¬ä¸€æ­¥æ˜¯ç”¨æ›´é«˜æ•ˆçš„æ–¹æ³•æ›¿ä»£å­å›¾æŒ–æ˜ï¼Œå¦‚[ELPH](https://arxiv.org/abs/2209.15486)å’Œ[BUDDY](https://arxiv.org/abs/2209.15486)ã€‚
- en: '![](../Images/d3b18da48a991a5b346bb00ba7c3706b.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d3b18da48a991a5b346bb00ba7c3706b.png)'
- en: 'Overview of the Universal Link Predictor framework. (a) For predicting a query
    link ğ‘, we initially sample positive (ğ‘ +) and negative (ğ‘ -) in-context links from
    the target graph. Both the query link and these in-context links are independently
    processed through a shared subgraph GNN encoder. An attention mechanism then calculates
    scores based on the similarity between the query link and the in-context links.
    (b) The final representation of the query link, contextualized by the target graph,
    is obtained through a weighted summation, which combines the representations of
    the in-context links with their respective labels. Source: [Dong et al.](https://arxiv.org/abs/2402.07738)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: é€šç”¨é“¾æ¥é¢„æµ‹æ¡†æ¶æ¦‚è¿°ã€‚ï¼ˆaï¼‰ä¸ºäº†é¢„æµ‹æŸ¥è¯¢é“¾æ¥ğ‘ï¼Œæˆ‘ä»¬é¦–å…ˆä»ç›®æ ‡å›¾ä¸­é‡‡æ ·æ­£ä¾‹ï¼ˆğ‘ +ï¼‰å’Œè´Ÿä¾‹ï¼ˆğ‘ -ï¼‰é“¾æ¥ã€‚è¿™äº›æŸ¥è¯¢é“¾æ¥å’Œä¸Šä¸‹æ–‡é“¾æ¥éƒ½ä¼šé€šè¿‡å…±äº«çš„å­å›¾GNNç¼–ç å™¨è¿›è¡Œç‹¬ç«‹å¤„ç†ã€‚æ¥ç€ï¼Œæ³¨æ„åŠ›æœºåˆ¶ä¼šåŸºäºæŸ¥è¯¢é“¾æ¥ä¸ä¸Šä¸‹æ–‡é“¾æ¥ä¹‹é—´çš„ç›¸ä¼¼æ€§è®¡ç®—å¾—åˆ†ã€‚ï¼ˆbï¼‰æŸ¥è¯¢é“¾æ¥çš„æœ€ç»ˆè¡¨ç¤ºï¼Œç”±ç›®æ ‡å›¾ä¸Šä¸‹æ–‡åŒ–åï¼Œé€šè¿‡åŠ æƒæ±‚å’Œè·å¾—ï¼Œè¯¥åŠ æƒæ±‚å’Œå°†ä¸Šä¸‹æ–‡é“¾æ¥çš„è¡¨ç¤ºä¸å…¶ç›¸åº”çš„æ ‡ç­¾ç»“åˆèµ·æ¥ã€‚æ¥æºï¼š[Dong
    et al.](https://arxiv.org/abs/2402.07738)
- en: '**What is transferable:** structural patterns learned by labeling trick GNNs
    â€” it is proven that methods like [Neural Bellman-Ford](https://arxiv.org/abs/2106.06935)
    capture metrics over node pairs, eg, Personalized PageRank or Katz index (often
    used for link prediction).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¯è½¬ç§»çš„å†…å®¹ï¼š** é€šè¿‡æ ‡æ³¨æŠ€å·§ GNN å­¦åˆ°çš„ç»“æ„æ¨¡å¼â€”â€”å·²æœ‰ç ”ç©¶è¯æ˜ï¼Œåƒ [Neural Bellman-Ford](https://arxiv.org/abs/2106.06935)
    è¿™æ ·çš„ç®—æ³•å¯ä»¥æ•æ‰èŠ‚ç‚¹å¯¹çš„åº¦é‡ï¼Œä¾‹å¦‚ä¸ªæ€§åŒ–çš„ PageRank æˆ– Katz æŒ‡æ•°ï¼ˆé€šå¸¸ç”¨äºé“¾æ¥é¢„æµ‹ï¼‰ã€‚'
- en: Now, as we know how to deal with automorphisms, the only step towards a single
    graph FM for link prediction would be to add a support for heterogeneous node
    features â€” perhaps GraphAny-style approaches might be an inspiration?
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæ—¢ç„¶æˆ‘ä»¬çŸ¥é“å¦‚ä½•å¤„ç†è‡ªåŒæ„ï¼Œæœç€å•ä¸€å›¾ FM è¿›è¡Œé“¾æ¥é¢„æµ‹çš„å”¯ä¸€æ­¥éª¤å°±æ˜¯æ·»åŠ å¯¹å¼‚æ„èŠ‚ç‚¹ç‰¹å¾çš„æ”¯æŒâ€”â€”æˆ–è®¸ç±»ä¼¼ GraphAny çš„æ–¹æ³•å¯ä»¥ä½œä¸ºå¯å‘ï¼Ÿ
- en: '**ğŸ“šRead more in references [4][5][6][7]**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸ“šæ›´å¤šå†…å®¹è¯·å‚è§å‚è€ƒæ–‡çŒ® [4][5][6][7]**'
- en: 'Knowledge Graph Reasoning: ULTRA and UltraQuery'
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: çŸ¥è¯†å›¾è°±æ¨ç†ï¼šULTRA å’Œ UltraQuery
- en: Knowledge graphs have graph-specific sets of entities and relations, e.g. common
    encyclopedia facts from Wikipedia / Wikidata or biomedical facts in Hetionet,
    those relations have different semantics and are not directly mappable to each
    other. For years, KG reasoning models were hardcoded to a given vocabulary of
    relations and could not transfer to new, unseen KGs with completely new entities
    and relations.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: çŸ¥è¯†å›¾è°±å…·æœ‰ç‰¹å®šäºå›¾çš„å®ä½“å’Œå…³ç³»é›†åˆï¼Œä¾‹å¦‚æ¥è‡ª Wikipedia / Wikidata çš„å¸¸è§ç™¾ç§‘äº‹å®ï¼Œæˆ–æ¥è‡ª Hetionet çš„ç”Ÿç‰©åŒ»å­¦äº‹å®ï¼Œè¿™äº›å…³ç³»å…·æœ‰ä¸åŒçš„è¯­ä¹‰ï¼Œä¸èƒ½ç›´æ¥æ˜ å°„åˆ°å½¼æ­¤ä¹‹é—´ã€‚å¤šå¹´æ¥ï¼ŒKG
    æ¨ç†æ¨¡å‹éƒ½æ˜¯ç¡¬ç¼–ç åˆ°ç»™å®šçš„å…³ç³»è¯æ±‡ä¸­ï¼Œæ— æ³•è¿ç§»åˆ°å…·æœ‰å…¨æ–°å®ä½“å’Œå…³ç³»çš„å…¨æ–°çŸ¥è¯†å›¾è°±ã€‚
- en: '[ULTRA](https://openreview.net/forum?id=jVEoydFOl9) is the first foundation
    model for KG reasoning that transfers to any KG at inference time in the zero-shot
    manner. That is, a single pre-trained model can run inference on any multi-relational
    graph with any size and entity/relation vocabulary. Averaged over 57 graphs, ULTRA
    significantly outperforms baselines trained specifically on each graph. Recently,
    ULTRA was extended to [UltraQuery](https://arxiv.org/abs/2404.07198) to support
    even more complex logical queries on graphs involving conjunctions, disjunctions,
    and negation operators. UltraQuery transfers to unseen graphs and 10+ complex
    query patterns on those unseen graphs outperforming much larger baselines trained
    from scratch.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[ULTRA](https://openreview.net/forum?id=jVEoydFOl9) æ˜¯ç¬¬ä¸€ä¸ªç”¨äºçŸ¥è¯†å›¾è°±æ¨ç†çš„åŸºç¡€æ¨¡å‹ï¼Œå®ƒå¯ä»¥åœ¨æ¨ç†æ—¶ä»¥é›¶æ ·æœ¬æ–¹å¼è¿ç§»åˆ°ä»»ä½•çŸ¥è¯†å›¾è°±ä¸Šã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹å¯ä»¥å¯¹ä»»ä½•å¤šå…³ç³»å›¾è¿›è¡Œæ¨ç†ï¼Œæ— è®ºå…¶å¤§å°å’Œå®ä½“/å…³ç³»è¯æ±‡å¦‚ä½•ã€‚åœ¨
    57 ä¸ªå›¾ä¸Šçš„å¹³å‡è¡¨ç°è¡¨æ˜ï¼ŒULTRA æ˜¾è‘—ä¼˜äºä¸“é—¨ä¸ºæ¯ä¸ªå›¾è®­ç»ƒçš„åŸºå‡†æ¨¡å‹ã€‚æœ€è¿‘ï¼ŒULTRA è¢«æ‰©å±•ä¸º [UltraQuery](https://arxiv.org/abs/2404.07198)ï¼Œä»¥æ”¯æŒå›¾ä¸­æ¶‰åŠç»“åˆã€æå–å’Œå¦å®šè¿ç®—ç¬¦çš„æ›´å¤æ‚é€»è¾‘æŸ¥è¯¢ã€‚UltraQuery
    å¯ä»¥è¿ç§»åˆ°æœªè§è¿‡çš„å›¾ï¼Œå¹¶ä¸”åœ¨è¿™äº›æœªè§è¿‡çš„å›¾ä¸Šï¼Œè¶…è¿‡ 10 ç§å¤æ‚æŸ¥è¯¢æ¨¡å¼çš„è¡¨ç°ä¼˜äºä»å¤´è®­ç»ƒçš„æ›´å¤§åŸºå‡†æ¨¡å‹ã€‚'
- en: '![](../Images/816385f060a3fa6b50d11575547cdb80.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/816385f060a3fa6b50d11575547cdb80.png)'
- en: 'Given a query (Michael Jackson, genre, ?), ULTRA builds a graph of relations
    (edge types) to capture their interactions in the original graph conditioned on
    the query relation (genre) and derives relational representations from this smaller
    graph. Those features are then used as edge type features in the original bigger
    graph to answer the query. Source: [Galkin et al](https://openreview.net/forum?id=jVEoydFOl9).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šæŸ¥è¯¢ï¼ˆMichael Jacksonï¼Œgenreï¼Œ?ï¼‰ï¼ŒULTRA æ„å»ºäº†ä¸€ä¸ªå…³ç³»å›¾ï¼ˆè¾¹ç±»å‹ï¼‰ï¼Œä»¥æ•æ‰åœ¨åŸå§‹å›¾ä¸­åŸºäºæŸ¥è¯¢å…³ç³»ï¼ˆgenreï¼‰çš„äº¤äº’ï¼Œå¹¶ä»è¿™ä¸ªè¾ƒå°çš„å›¾ä¸­æ¨å¯¼å‡ºå…³ç³»è¡¨ç¤ºã€‚è¿™äº›ç‰¹å¾éšåä½œä¸ºè¾¹ç±»å‹ç‰¹å¾ï¼Œåœ¨åŸå§‹çš„å¤§å›¾ä¸­ç”¨äºå›ç­”æŸ¥è¯¢ã€‚æ¥æºï¼š[Galkin
    ç­‰äºº](https://openreview.net/forum?id=jVEoydFOl9)ã€‚
- en: '**Setup:** Given a multi-relational graph G with |E| nodes and |R| edge types,
    no node features, answer simple KG completion queries *(head, relation, ?)* or
    complex queries involving logical operators by returning a probability distribution
    over all nodes in the given graph. The set of nodes and relation types depends
    on the graph and can vary.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®¾ç½®ï¼š** ç»™å®šä¸€ä¸ªå¤šå…³ç³»å›¾ Gï¼Œå…¶ä¸­æœ‰ |E| ä¸ªèŠ‚ç‚¹å’Œ |R| ç§è¾¹ç±»å‹ï¼Œä¸”æ²¡æœ‰èŠ‚ç‚¹ç‰¹å¾ï¼Œå›ç­”ç®€å•çš„ KG å®ŒæˆæŸ¥è¯¢ï¼ˆ*(head, relation,
    ?)*ï¼‰æˆ–æ¶‰åŠé€»è¾‘è¿ç®—ç¬¦çš„å¤æ‚æŸ¥è¯¢ï¼Œé€šè¿‡è¿”å›ç»™å®šå›¾ä¸­æ‰€æœ‰èŠ‚ç‚¹çš„æ¦‚ç‡åˆ†å¸ƒã€‚èŠ‚ç‚¹å’Œå…³ç³»ç±»å‹çš„é›†åˆå–å†³äºå›¾ï¼Œå¹¶ä¸”å¯èƒ½æœ‰æ‰€ä¸åŒã€‚'
- en: '**What is transferable:** ULTRA relies on modeling relational interactions.
    Forgetting about relation identities and target graph domain for a second, if
    we see that relations â€œauthoredâ€ and â€œcollaboratedâ€ can share the same starting
    node, and relations â€œstudentâ€ and â€œcoauthorâ€ in another graph can share a starting
    node, then the relative, structural representations of those two pairs of relations
    might be similar. This holds for any multi-relational graph in any domain, be
    it encyclopedia or biomedical KGs. ULTRA goes further and captures 4 such â€œfundamentalâ€
    interactions between relations. Those fundamental interactions are transferable
    to any KG (together with learned GNN weights) â€” this way, one single pre-trained
    model is ready for inference on any unseen graph and simple or complex reasoning
    query.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»€ä¹ˆæ˜¯å¯è½¬ç§»çš„ï¼š** ULTRAä¾èµ–äºå¯¹å…³ç³»äº’åŠ¨çš„å»ºæ¨¡ã€‚æš‚æ—¶ä¸è€ƒè™‘å…³ç³»æ ‡è¯†å’Œç›®æ ‡å›¾è°±é¢†åŸŸï¼Œå¦‚æœæˆ‘ä»¬çœ‹åˆ°â€œä½œè€…â€å’Œâ€œåˆä½œâ€è¿™ä¸¤ä¸ªå…³ç³»å¯ä»¥å…±äº«ç›¸åŒçš„èµ·å§‹èŠ‚ç‚¹ï¼Œä¸”å¦ä¸€ä¸ªå›¾è°±ä¸­çš„â€œå­¦ç”Ÿâ€å’Œâ€œå…±åŒä½œè€…â€ä¹Ÿèƒ½å…±äº«ä¸€ä¸ªèµ·å§‹èŠ‚ç‚¹ï¼Œé‚£ä¹ˆè¿™ä¸¤ä¸ªå…³ç³»å¯¹çš„ç›¸å¯¹ç»“æ„è¡¨ç¤ºå¯èƒ½æ˜¯ç›¸ä¼¼çš„ã€‚è¿™é€‚ç”¨äºä»»ä½•é¢†åŸŸçš„å¤šå…³ç³»å›¾è°±ï¼Œæ— è®ºæ˜¯ç™¾ç§‘å…¨ä¹¦è¿˜æ˜¯ç”Ÿç‰©åŒ»å­¦çŸ¥è¯†å›¾è°±ã€‚ULTRAè¿›ä¸€æ­¥æ•æ‰äº†4ç§è¿™ç§â€œåŸºç¡€â€å…³ç³»äº’åŠ¨ã€‚è¿™äº›åŸºç¡€å…³ç³»äº’åŠ¨æ˜¯å¯è½¬ç§»çš„ï¼Œé€‚ç”¨äºä»»ä½•çŸ¥è¯†å›¾è°±ï¼ˆè¿åŒå­¦ä¹ åˆ°çš„GNNæƒé‡ï¼‰â€”â€”è¿™æ ·ï¼Œä¸€ä¸ªç»è¿‡é¢„è®­ç»ƒçš„æ¨¡å‹å°±å¯ä»¥åœ¨ä»»ä½•æœªè§è¿‡çš„å›¾è°±ä¸Šè¿›è¡Œæ¨ç†ï¼Œå¹¶å¤„ç†ç®€å•æˆ–å¤æ‚çš„æ¨ç†æŸ¥è¯¢ã€‚'
- en: 'Read more in the dedicated Medium post:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸“é—¨çš„Mediumå¸–å­ä¸­é˜…è¯»æ›´å¤šå†…å®¹ï¼š
- en: '[](/ultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09?source=post_page-----f363e2576f58--------------------------------)
    [## ULTRA: Foundation Models for Knowledge Graph Reasoning'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/ultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09?source=post_page-----f363e2576f58--------------------------------)
    [## ULTRAï¼šçŸ¥è¯†å›¾è°±æ¨ç†çš„åŸºç¡€æ¨¡å‹'
- en: One model to rule them all
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ¨¡å‹ï¼Œç»Ÿæ²»ä¸€åˆ‡
- en: towardsdatascience.com](/ultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09?source=post_page-----f363e2576f58--------------------------------)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/ultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09?source=post_page-----f363e2576f58--------------------------------)'
- en: '**ğŸ“šRead more in references [8][9]**'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸ“šåœ¨å‚è€ƒæ–‡çŒ®[8][9]ä¸­é˜…è¯»æ›´å¤šå†…å®¹**'
- en: 'Algorithmic Reasoning: Generalist Algorithmic Learner'
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç®—æ³•æ¨ç†ï¼šé€šç”¨ç®—æ³•å­¦ä¹ å™¨
- en: '![](../Images/d719d30af605e44657fe13140067cb22.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d719d30af605e44657fe13140067cb22.png)'
- en: 'A generalist neural algorithmic learner is a single processor GNN P, with a
    single set of weights, capable of solving several algorithmic tasks in a shared
    latent space (each of which is attached to P with simple encoders/decoders f and
    g). Among others, the processor network is capable of sorting (top), shortest
    path-finding (middle), and convex hull finding (bottom). Source: [Ibarz et al.](https://proceedings.mlr.press/v198/ibarz22a/ibarz22a.pdf)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§é€šç”¨ç¥ç»ç®—æ³•å­¦ä¹ å™¨æ˜¯ä¸€ä¸ªå•å¤„ç†å™¨GNN Pï¼Œå…·æœ‰ä¸€ç»„æƒé‡ï¼Œèƒ½å¤Ÿåœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­è§£å†³å¤šç§ç®—æ³•ä»»åŠ¡ï¼ˆæ¯ä¸ªä»»åŠ¡é€šè¿‡ç®€å•çš„ç¼–ç å™¨/è§£ç å™¨få’Œgé™„åŠ åˆ°Pï¼‰ã€‚å…¶ä¸­ï¼Œè¯¥å¤„ç†å™¨ç½‘ç»œèƒ½å¤Ÿè¿›è¡Œæ’åºï¼ˆä¸Šï¼‰ã€æœ€çŸ­è·¯å¾„æŸ¥æ‰¾ï¼ˆä¸­ï¼‰å’Œå‡¸åŒ…æŸ¥æ‰¾ï¼ˆä¸‹ï¼‰ã€‚æ¥æºï¼š[Ibarz
    et al.](https://proceedings.mlr.press/v198/ibarz22a/ibarz22a.pdf)
- en: '**Setup:** [Neural algorithmic reasoning](https://arxiv.org/abs/2105.02761)
    (NAR) studies the execution of standard algorithms (eg, sorting, searching, dynamic
    programming) in the latent space and generalization to the inputs of arbitrary
    size. A lot of such algorithms can be represented with a graph input and pointers.
    Given a graph G with node and edge features, the task is to simulate the algorithm
    and produce the correct output. Optionally, you can get access to hints â€” time
    series of intermediate states of the algorithm which can act as the intermediate
    supervised signal. Obviously, different algorithms require a different number
    of steps to execute, so length is not fixed here.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®¾ç½®ï¼š** [ç¥ç»ç®—æ³•æ¨ç†](https://arxiv.org/abs/2105.02761)ï¼ˆNARï¼‰ç ”ç©¶åœ¨æ½œåœ¨ç©ºé—´ä¸­æ‰§è¡Œæ ‡å‡†ç®—æ³•ï¼ˆå¦‚æ’åºã€æœç´¢ã€åŠ¨æ€è§„åˆ’ï¼‰å¹¶æ¨å¹¿åˆ°ä»»æ„å¤§å°è¾“å…¥çš„è¿‡ç¨‹ã€‚è®¸å¤šæ­¤ç±»ç®—æ³•å¯ä»¥é€šè¿‡å›¾è°±è¾“å…¥å’ŒæŒ‡é’ˆæ¥è¡¨ç¤ºã€‚ç»™å®šä¸€ä¸ªåŒ…å«èŠ‚ç‚¹å’Œè¾¹ç‰¹å¾çš„å›¾Gï¼Œä»»åŠ¡æ˜¯æ¨¡æ‹Ÿç®—æ³•å¹¶äº§ç”Ÿæ­£ç¡®çš„è¾“å‡ºã€‚å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥è®¿é—®æç¤ºâ€”â€”ç®—æ³•çš„ä¸­é—´çŠ¶æ€æ—¶é—´åºåˆ—ï¼Œè¿™äº›å¯ä»¥ä½œä¸ºä¸­é—´ç›‘ç£ä¿¡å·ã€‚æ˜¾ç„¶ï¼Œä¸åŒçš„ç®—æ³•éœ€è¦ä¸åŒçš„æ­¥éª¤æ¥æ‰§è¡Œï¼Œå› æ­¤æ­¥éª¤æ•°åœ¨è¿™é‡Œä¸æ˜¯å›ºå®šçš„ã€‚'
- en: '**What is transferable:** Homogeneous feature space and similar control flow
    for similar algorithms. For instance, Primâ€™s and Dijkstraâ€™s algorithms share a
    similar structure, differing only in the choice of key function and edge relaxation
    subroutine. Besides, there are [several](https://arxiv.org/abs/1905.13211) [proofs](https://arxiv.org/abs/2203.15544)
    of a direct alignment between message passing and dynamic programming. This is
    the main motivation behind one â€œprocessorâ€ neural network that updates latent
    states for all considered algorithms ([30 classic algos](https://github.com/google-deepmind/clrs)
    from the CLRS book).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¯è½¬ç§»çš„å†…å®¹**ï¼šåŒè´¨çš„ç‰¹å¾ç©ºé—´å’Œç›¸ä¼¼çš„æ§åˆ¶æµç”¨äºç›¸ä¼¼çš„ç®—æ³•ã€‚ä¾‹å¦‚ï¼ŒPrimç®—æ³•å’ŒDijkstraç®—æ³•å…±äº«ç›¸ä¼¼çš„ç»“æ„ï¼Œä»…åœ¨å…³é”®å‡½æ•°çš„é€‰æ‹©å’Œè¾¹ç¼˜æ¾å¼›å­ç¨‹åºä¸Šæœ‰æ‰€ä¸åŒã€‚æ­¤å¤–ï¼Œå·²æœ‰[å‡ ç¯‡](https://arxiv.org/abs/1905.13211)
    [è¯æ˜](https://arxiv.org/abs/2203.15544)è¡¨æ˜æ¶ˆæ¯ä¼ é€’ä¸åŠ¨æ€è§„åˆ’ä¹‹é—´å­˜åœ¨ç›´æ¥çš„å¯¹é½å…³ç³»ã€‚è¿™æ­£æ˜¯â€œå¤„ç†å™¨â€ç¥ç»ç½‘ç»œçš„ä¸»è¦åŠ¨æœºï¼Œå®ƒæ›´æ–°æ‰€æœ‰è€ƒè™‘ä¸­çš„ç®—æ³•çš„æ½œåœ¨çŠ¶æ€ï¼ˆ[CLRSä¹¦ä¸­çš„30ä¸ªç»å…¸ç®—æ³•](https://github.com/google-deepmind/clrs)ï¼‰ã€‚'
- en: '[Triplet-GMPNN](https://proceedings.mlr.press/v198/ibarz22a/ibarz22a.pdf) was
    the first such universal processor neural net (by 2024 it became rather standard
    in the NAR literature) â€” it is a GNN that operates on triples of nodes and their
    features (akin to [Edge Transformers](https://arxiv.org/abs/2112.00578) and triangular
    attention in AlphaFold). The model is trained in the multi-task mode on all algorithmic
    tasks in the benchmark with a handful of optimization and tricks. A single model
    bumps the average performance on 30 tasks by over 20% (in absolute numbers) compared
    to single-task specialist models.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[Triplet-GMPNN](https://proceedings.mlr.press/v198/ibarz22a/ibarz22a.pdf) æ˜¯ç¬¬ä¸€ä¸ªæ­¤ç±»é€šç”¨å¤„ç†å™¨ç¥ç»ç½‘ç»œï¼ˆåˆ°2024å¹´ï¼Œå®ƒåœ¨NARæ–‡çŒ®ä¸­å·²ç»æˆä¸ºç›¸å½“æ ‡å‡†çš„åšæ³•ï¼‰â€”â€”å®ƒæ˜¯ä¸€ä¸ªæ“ä½œèŠ‚ç‚¹ä¸‰å…ƒç»„åŠå…¶ç‰¹å¾çš„å›¾ç¥ç»ç½‘ç»œï¼ˆç±»ä¼¼äº[è¾¹å˜æ¢å™¨](https://arxiv.org/abs/2112.00578)å’ŒAlphaFoldä¸­çš„ä¸‰è§’æ³¨æ„åŠ›ï¼‰ã€‚è¯¥æ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•çš„æ‰€æœ‰ç®—æ³•ä»»åŠ¡ä¸Šä»¥å¤šä»»åŠ¡æ¨¡å¼è¿›è¡Œè®­ç»ƒï¼Œå¹¶é‡‡ç”¨äº†ä¸€äº›ä¼˜åŒ–å’ŒæŠ€å·§ã€‚ä¸å•ä»»åŠ¡ä¸“ç”¨æ¨¡å‹ç›¸æ¯”ï¼Œå•ä¸€æ¨¡å‹ä½¿å¾—30ä¸ªä»»åŠ¡çš„å¹³å‡è¡¨ç°æé«˜äº†20%ä»¥ä¸Šï¼ˆç»å¯¹æ•°å€¼ï¼‰ã€‚'
- en: Still, encoders and decoders are parameterized specifically for each task â€”
    one of the ways to unify the input and output formats might as well be text with
    LLM processors as done in the recent [text version of CLRS](https://arxiv.org/abs/2406.04229).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å¦‚æ­¤ï¼Œç¼–ç å™¨å’Œè§£ç å™¨ä¾ç„¶æ˜¯é’ˆå¯¹æ¯ä¸ªä»»åŠ¡ç‰¹å®šå‚æ•°åŒ–çš„â€”â€”ç»Ÿä¸€è¾“å…¥å’Œè¾“å‡ºæ ¼å¼çš„ä¸€ç§æ–¹å¼å¯èƒ½æ˜¯ä½¿ç”¨æ–‡æœ¬ä¸LLMå¤„ç†å™¨ï¼Œå¦‚æœ€è¿‘[CLRSæ–‡æœ¬ç‰ˆ](https://arxiv.org/abs/2406.04229)æ‰€åšçš„é‚£æ ·ã€‚
- en: '![](../Images/35f10005a7020cbd88a744051d639e8e.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/35f10005a7020cbd88a744051d639e8e.png)'
- en: '**Top**: The graph algorithmic trace of insertion sorting a list *[5, 2, 4,
    3, 1]* in graph form. **Bottom**: The same algorithmic trace, represented textually,
    by using the CLRS-Text generator. The model receives as input (depicted in green)
    the input array (key) and the initial value of the sorting trace (initial_trace),
    using which it is prompted to predict the trace (depicted in blue) of gradually
    sorting the list, by inserting one element at a time into a partially sorted list,
    from left to right. At the end, the model needs to output the final sorted array
    (depicted in red), and it is evaluated on whether this array is predicted correctly.
    Source: [Markeeva, McLeish, Ibarz, et al.](https://arxiv.org/abs/2406.04229)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**é¡¶éƒ¨**ï¼šæ’å…¥æ’åºä¸€ä¸ªåˆ—è¡¨ *[5, 2, 4, 3, 1]* çš„å›¾å½¢ç®—æ³•è¿‡ç¨‹ã€‚**åº•éƒ¨**ï¼šç›¸åŒç®—æ³•è¿‡ç¨‹ï¼Œä»¥æ–‡æœ¬æ–¹å¼è¡¨ç¤ºï¼Œé€šè¿‡ä½¿ç”¨CLRS-Textç”Ÿæˆå™¨ã€‚æ¨¡å‹æ¥æ”¶çš„è¾“å…¥ï¼ˆä»¥ç»¿è‰²è¡¨ç¤ºï¼‰æ˜¯è¾“å…¥æ•°ç»„ï¼ˆé”®ï¼‰å’Œæ’åºè¿‡ç¨‹çš„åˆå§‹å€¼ï¼ˆinitial_traceï¼‰ï¼Œå¹¶ä»¥æ­¤ä¸ºæç¤ºé¢„æµ‹è¿‡ç¨‹ï¼ˆä»¥è“è‰²è¡¨ç¤ºï¼‰ï¼Œå³é€šè¿‡å°†æ¯æ¬¡ä¸€ä¸ªå…ƒç´ æ’å…¥åˆ°éƒ¨åˆ†æ’åºçš„åˆ—è¡¨ä¸­ï¼Œé€æ­¥å¯¹åˆ—è¡¨è¿›è¡Œæ’åºï¼Œä»å·¦åˆ°å³ã€‚æœ€åï¼Œæ¨¡å‹éœ€è¦è¾“å‡ºæœ€ç»ˆæ’åºå¥½çš„æ•°ç»„ï¼ˆä»¥çº¢è‰²è¡¨ç¤ºï¼‰ï¼Œå¹¶è¯„ä¼°æ˜¯å¦æ­£ç¡®é¢„æµ‹äº†è¯¥æ•°ç»„ã€‚æ¥æºï¼š[Markeeva,
    McLeish, Ibarz ç­‰](https://arxiv.org/abs/2406.04229)'
- en: 'Perhaps the most interesting question of 2024 and 2025 in NAR is:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 2024å¹´å’Œ2025å¹´NARä¸­æˆ–è®¸æœ€æœ‰è¶£çš„é—®é¢˜æ˜¯ï¼š
- en: '*Can algorithmic reasoning ideas for OOD generalization be the key to generalizable
    LLM reasoning?*'
  id: totrans-86
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*ç®—æ³•æ¨ç†æ€æƒ³èƒ½å¦æˆä¸ºOODæ³›åŒ–ä¸­LLMæ¨ç†çš„å…³é”®ï¼Ÿ*'
- en: LLMs notoriously struggle with complex reasoning problems, dozens of papers
    appear on arxiv every month trying a new prompting method to bump benchmarking
    performance another percentage-or-two, but most of them do not transfer across
    tasks of similar graph structures (see the example below). There is a need for
    more principled approaches and NAR has the potential to fill this gap!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: LLMåœ¨å¤„ç†å¤æ‚æ¨ç†é—®é¢˜æ—¶é€šå¸¸è¡¨ç°ä¸ä½³ï¼Œæ¯ä¸ªæœˆarxivä¸Šéƒ½ä¼šå‡ºç°æ•°åç¯‡è®ºæ–‡ï¼Œå°è¯•é€šè¿‡ä¸€ç§æ–°çš„æç¤ºæ–¹æ³•æé«˜åŸºå‡†æ€§èƒ½ç™¾åˆ†ä¹‹ä¸€æˆ–ä¸¤ï¼Œä½†å¤§å¤šæ•°æ–¹æ³•åœ¨å¤„ç†ç›¸ä¼¼å›¾ç»“æ„çš„ä»»åŠ¡æ—¶å¹¶æ²¡æœ‰è½¬ç§»æˆåŠŸï¼ˆè§ä¸‹ä¾‹ï¼‰ã€‚è¿«åˆ‡éœ€è¦æ›´æœ‰åŸåˆ™æ€§çš„æ–¹æ³•ï¼Œè€ŒNARæœ‰å¯èƒ½å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼
- en: '![](../Images/f8df0df88abfbd23a7ca070c14f81293.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8df0df88abfbd23a7ca070c14f81293.png)'
- en: Failure of LLMs on reasoning problems with similar graph structures. Image by
    Authors.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: LLMåœ¨å¤„ç†å…·æœ‰ç›¸ä¼¼å›¾ç»“æ„çš„æ¨ç†é—®é¢˜æ—¶å¤±è´¥ã€‚å›¾ç‰‡æ¥æºï¼šä½œè€…ã€‚
- en: '**ğŸ“šRead more in references [10][11]**'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸ“šæ›´å¤šå†…å®¹è§å‚è€ƒæ–‡çŒ®[10][11]**'
- en: Geometric and AI4Science Foundation Models
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‡ ä½•å­¦å’ŒAI4ScienceåŸºç¡€æ¨¡å‹
- en: In the world of Geometric Deep Learning and scientific applications, foundation
    models are becoming prevalent as universal ML potentials, protein language models,
    and universal molecular property predictors. Although the universal vocabulary
    exists in most such cases (e.g., atom types in small molecules or amino acids
    in proteins) and we do not have to think about universal featurization, the main
    complexity lies in the real-world physical nature of atomistic objects â€” they
    have pronounced 3D structure and properties (like energy), which have theoretical
    justifications rooted in chemistry, physics, and quantum mechanics.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‡ ä½•æ·±åº¦å­¦ä¹ å’Œç§‘å­¦åº”ç”¨é¢†åŸŸï¼ŒåŸºç¡€æ¨¡å‹æ­£é€æ¸æˆä¸ºé€šç”¨MLåŠ¿èƒ½ã€è›‹ç™½è´¨è¯­è¨€æ¨¡å‹å’Œé€šç”¨åˆ†å­æ€§è´¨é¢„æµ‹å™¨ã€‚è™½ç„¶åœ¨å¤§å¤šæ•°è¿™ç§æƒ…å†µä¸‹ï¼Œé€šç”¨è¯æ±‡å·²ç»å­˜åœ¨ï¼ˆä¾‹å¦‚ï¼Œå°åˆ†å­ä¸­çš„åŸå­ç±»å‹æˆ–è›‹ç™½è´¨ä¸­çš„æ°¨åŸºé…¸ï¼‰ï¼Œä¸”æˆ‘ä»¬ä¸éœ€è¦æ€è€ƒé€šç”¨ç‰¹å¾åŒ–ï¼Œä½†ä¸»è¦çš„å¤æ‚æ€§åœ¨äºåŸå­å¯¹è±¡çš„çœŸå®ç‰©ç†ç‰¹æ€§â€”â€”å®ƒä»¬å…·æœ‰æ˜æ˜¾çš„ä¸‰ç»´ç»“æ„å’Œæ€§è´¨ï¼ˆå¦‚èƒ½é‡ï¼‰ï¼Œè¿™äº›æ€§è´¨æœ‰ç†è®ºä¾æ®ï¼Œæ ¹æ¤äºåŒ–å­¦ã€ç‰©ç†å­¦å’Œé‡å­åŠ›å­¦ã€‚
- en: 'ML Potentials: JMP-1, DPA-2 for molecules, MACE-MP-0 and MatterSim for inorganic
    crystals'
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLåŠ¿èƒ½ï¼šJMP-1ã€DPA-2ç”¨äºåˆ†å­ï¼ŒMACE-MP-0å’ŒMatterSimç”¨äºæ— æœºæ™¶ä½“
- en: '**Setup**: given a 3D structure, predict the energy of the structure and per-atom
    forces;'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®¾ç½®**ï¼šç»™å®šä¸€ä¸ªä¸‰ç»´ç»“æ„ï¼Œé¢„æµ‹è¯¥ç»“æ„çš„èƒ½é‡å’Œæ¯ä¸ªåŸå­çš„åŠ›ï¼›'
- en: '**What is transferable**: a vocabulary of atoms from the periodic table.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¯è½¬ç§»çš„å†…å®¹**ï¼šæ¥è‡ªå…ƒç´ å‘¨æœŸè¡¨çš„åŸå­è¯æ±‡ã€‚'
- en: ML potentials estimate the potential energy of a chemical compound â€” like molecules
    or periodic crystals â€” given their 3D coordinates and optional input (like periodic
    boundary conditions for crystals). For any atomistic model, the vocabulary of
    possible atoms is always bound by the [Periodic Table](https://en.wikipedia.org/wiki/Periodic_table)
    which currently includes 118 elements. The â€œfoundationalâ€ aspect in ML potentials
    is to generalize to any atomistic structure (there can be combinatorially many)
    and be stable enough to be used in molecular dynamics (MD), drug- and materials
    discovery pipelines.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: MLåŠ¿èƒ½é€šè¿‡ç»™å®šåŒ–å­¦åŒ–åˆç‰©çš„ä¸‰ç»´åæ ‡å’Œå¯é€‰è¾“å…¥ï¼ˆä¾‹å¦‚æ™¶ä½“çš„å‘¨æœŸæ€§è¾¹ç•Œæ¡ä»¶ï¼‰æ¥ä¼°ç®—å…¶æ½œåœ¨èƒ½é‡ï¼ŒåŒ–å­¦åŒ–åˆç‰©å¯ä»¥æ˜¯åˆ†å­æˆ–å‘¨æœŸæ€§æ™¶ä½“ã€‚å¯¹äºä»»ä½•åŸå­æ¨¡å‹ï¼Œå¯èƒ½çš„åŸå­ç§ç±»çš„è¯æ±‡æ€»æ˜¯å—åˆ°[å…ƒç´ å‘¨æœŸè¡¨](https://en.wikipedia.org/wiki/Periodic_table)çš„çº¦æŸï¼Œç›®å‰è¯¥å‘¨æœŸè¡¨åŒ…å«118ç§å…ƒç´ ã€‚MLåŠ¿èƒ½çš„â€œåŸºç¡€æ€§â€æ–¹é¢æ˜¯å°†å…¶æ¨å¹¿åˆ°ä»»ä½•åŸå­ç»“æ„ï¼ˆå¯èƒ½æœ‰ç»„åˆå½¢å¼å¤šæ ·çš„ç»“æ„ï¼‰ï¼Œå¹¶ä¸”è¶³å¤Ÿç¨³å®šï¼Œä»¥ä¾¿åœ¨åˆ†å­åŠ¨åŠ›å­¦ï¼ˆMDï¼‰ã€è¯ç‰©å‘ç°å’Œææ–™å‘ç°æµç¨‹ä¸­ä½¿ç”¨ã€‚
- en: '[JMP-1](https://arxiv.org/abs/2310.16802) and [DPA-2](https://arxiv.org/abs/2312.15492)
    released around the same time aim to be such universal ML potential models â€” they
    are trained on a sheer variety of structures â€” from organic molecules to crystals
    to MD trajectories. For example, a single pre-trained JMP-1 excels at QM9, rMD17
    for small molecules, MatBench and QMOF on crystals, and MD22, SPICE on large molecules
    being on-par or better than specialized per-dataset models. Similarly, [MACE-MP-0](https://arxiv.org/abs/2401.00096)
    and [MatterSim](https://arxiv.org/abs/2405.04967) are the most advanced FMs for
    inorganic crystals (MACE-MP-0 is already available with weights) evaluated on
    20+ crystal tasks ranging from multicomponent alloys to combustion and molten
    salts. Equivariant GNNs are at the heart of those systems helping to process equivariant
    features (Cartesian coordinates) and invariant features (like atom types).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[JMP-1](https://arxiv.org/abs/2310.16802)å’Œ[DPA-2](https://arxiv.org/abs/2312.15492)å¤§è‡´åŒæ—¶å‘å¸ƒï¼Œæ—¨åœ¨æˆä¸ºè¿™æ ·çš„é€šç”¨MLåŠ¿èƒ½æ¨¡å‹â€”â€”å®ƒä»¬åœ¨å¤šç§ç»“æ„ä¸Šè¿›è¡Œè®­ç»ƒâ€”â€”ä»æœ‰æœºåˆ†å­åˆ°æ™¶ä½“ï¼Œå†åˆ°MDè½¨è¿¹ã€‚ä¾‹å¦‚ï¼Œå•ä¸ªé¢„è®­ç»ƒçš„JMP-1åœ¨QM9ã€rMD17ï¼ˆå°åˆ†å­ï¼‰ã€MatBenchå’ŒQMOFï¼ˆæ™¶ä½“ï¼‰ã€ä»¥åŠMD22ã€SPICEï¼ˆå¤§åˆ†å­ï¼‰ç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä¸”ä¸ä¸“é—¨é’ˆå¯¹ç‰¹å®šæ•°æ®é›†çš„æ¨¡å‹ç›¸å½“æˆ–æ›´ä¼˜ã€‚ç±»ä¼¼åœ°ï¼Œ[MACE-MP-0](https://arxiv.org/abs/2401.00096)å’Œ[MatterSim](https://arxiv.org/abs/2405.04967)æ˜¯æ— æœºæ™¶ä½“é¢†åŸŸæœ€å…ˆè¿›çš„åŸºç¡€æ¨¡å‹ï¼ˆMACE-MP-0å·²ç»æœ‰æƒé‡å¯ç”¨ï¼‰ï¼Œåœ¨20å¤šä¸ªæ™¶ä½“ä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œä»»åŠ¡æ¶µç›–äº†ä»å¤šç»„åˆ†åˆé‡‘åˆ°ç‡ƒçƒ§å’Œç†”èç›çš„å„ç±»ä»»åŠ¡ã€‚ç­‰å˜å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰æ˜¯è¿™äº›ç³»ç»Ÿçš„æ ¸å¿ƒï¼Œå¸®åŠ©å¤„ç†ç­‰å˜ç‰¹å¾ï¼ˆå¦‚ç¬›å¡å°”åæ ‡ï¼‰å’Œä¸å˜ç‰¹å¾ï¼ˆå¦‚åŸå­ç±»å‹ï¼‰ã€‚'
- en: '![](../Images/fa55ad6dff257dc117eefc32cb637976.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa55ad6dff257dc117eefc32cb637976.png)'
- en: 'Sources: (1) Pre-training and fine-tuning of **JMP-1** for molecules and crystals,
    [Shoghi et al](https://arxiv.org/abs/2310.16802) (2) **MACE-MP-0** is trained
    only on the Materials Project data and transfers to molecular dynamics simulation
    across a wide variety of chemistries in the solid, liquid and gaseous phases,
    [Batatia, Benner, Chiang, Elena, KovÃ¡cs, Riebesell et al](https://arxiv.org/abs/2401.00096).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼šï¼ˆ1ï¼‰å¯¹**JMP-1**è¿›è¡Œåˆ†å­å’Œæ™¶ä½“çš„é¢„è®­ç»ƒä¸å¾®è°ƒï¼Œ[Shoghi et al](https://arxiv.org/abs/2310.16802)ï¼ˆ2ï¼‰**MACE-MP-0**ä»…åœ¨ææ–™é¡¹ç›®æ•°æ®ä¸Šè®­ç»ƒï¼Œå¹¶åœ¨å›ºæ€ã€æ¶²æ€å’Œæ°”æ€ä¸‹çš„å¤šç§åŒ–å­¦ä½“ç³»ä¸­è¿›è¡Œåˆ†å­åŠ¨åŠ›å­¦æ¨¡æ‹Ÿï¼Œ[Batatia,
    Benner, Chiang, Elena, KovÃ¡cs, Riebesell et al](https://arxiv.org/abs/2401.00096)ã€‚
- en: The next frontier seems to be ML-accelerated molecular dynamics simulations
    â€” traditional computational methods work at the femtosecond scale (10â€“15) and
    require millions and billions of steps to simulate a molecule, crystal, or protein.
    Speeding up such computations would have an immense scientific impact.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€ä¸ªå‰æ²¿ä¼¼ä¹æ˜¯æœºå™¨å­¦ä¹ åŠ é€Ÿçš„åˆ†å­åŠ¨åŠ›å­¦æ¨¡æ‹Ÿâ€”â€”ä¼ ç»Ÿçš„è®¡ç®—æ–¹æ³•åœ¨é£ç§’å°ºåº¦ï¼ˆ10^-15ç§’ï¼‰ä¸‹å·¥ä½œï¼Œå¹¶ä¸”éœ€è¦æ•°ç™¾ä¸‡åˆ°æ•°åäº¿æ­¥æ‰èƒ½æ¨¡æ‹Ÿä¸€ä¸ªåˆ†å­ã€æ™¶ä½“æˆ–è›‹ç™½è´¨ã€‚åŠ é€Ÿè¿™äº›è®¡ç®—å°†å¯¹ç§‘å­¦äº§ç”Ÿå·¨å¤§çš„å½±å“ã€‚
- en: '**ğŸ“šRead more in references [12][13][14][15]**'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸ“šæ›´å¤šå†…å®¹è¯·å‚è€ƒæ–‡çŒ® [12][13][14][15]**'
- en: 'Protein LMs: ESM-2'
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è›‹ç™½è´¨è¯­è¨€æ¨¡å‹ï¼šESM-2
- en: '**Setup**: given a protein sequence, predict the masked tokens akin to masked
    language modeling;'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®¾ç½®**ï¼šç»™å®šä¸€ä¸ªè›‹ç™½è´¨åºåˆ—ï¼Œé¢„æµ‹è¢«æ©ç çš„æ ‡è®°ï¼Œç±»ä¼¼äºæ©ç è¯­è¨€å»ºæ¨¡ï¼›'
- en: '**What is transferable**: a vocabulary of 20 (22) amino acids.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¯è½¬ç§»çš„å†…å®¹**ï¼šä¸€ä¸ªåŒ…å«20ï¼ˆ22ï¼‰ç§æ°¨åŸºé…¸çš„è¯æ±‡è¡¨ã€‚'
- en: Protein sequences resemble natural language with amino acids as tokens, and
    Transformers excel at encoding sequence data. Although the vocabulary of amino
    acids is relatively small, the space of possible proteins is enormous, so training
    on large volumes of known proteins might hint at the properties of unseen combinations.
    [ESM-2](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1) is perhaps
    the most popular protein LM thanks to the pre-training data size, a variety of
    available checkpoints, and informative features.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: è›‹ç™½è´¨åºåˆ—ç±»ä¼¼äºè‡ªç„¶è¯­è¨€ï¼Œå…¶ä¸­æ°¨åŸºé…¸æ˜¯æ ‡è®°ï¼Œè€ŒTransformeråœ¨ç¼–ç åºåˆ—æ•°æ®æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚å°½ç®¡æ°¨åŸºé…¸çš„è¯æ±‡è¡¨ç›¸å¯¹è¾ƒå°ï¼Œä½†å¯èƒ½çš„è›‹ç™½è´¨ç©ºé—´å´æä¸ºå¹¿é˜”ï¼Œå› æ­¤åœ¨å¤§é‡å·²çŸ¥è›‹ç™½è´¨ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¯èƒ½ä¼šæç¤ºçœ‹ä¸è§çš„ç»„åˆçš„ç‰¹æ€§ã€‚[ESM-2](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1)ä¹Ÿè®¸æ˜¯æœ€æµè¡Œçš„è›‹ç™½è´¨è¯­è¨€æ¨¡å‹ï¼Œè¿™å¾—ç›Šäºé¢„è®­ç»ƒæ•°æ®çš„è§„æ¨¡ã€å„ç§å¯ç”¨çš„æ£€æŸ¥ç‚¹å’Œä¿¡æ¯ä¸°å¯Œçš„ç‰¹å¾ã€‚
- en: '![](../Images/3796a202aa30508022eba58d8a368413.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3796a202aa30508022eba58d8a368413.png)'
- en: 'ESM2 as a masked LM and ESMFold for protein structure prediction. Source: [Lin,
    Akin, Rao, Hie, et al.](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ESM2ä½œä¸ºä¸€ä¸ªæ©ç è¯­è¨€æ¨¡å‹å’ŒESMFoldç”¨äºè›‹ç™½è´¨ç»“æ„é¢„æµ‹ã€‚æ¥æºï¼š[Lin, Akin, Rao, Hie, et al.](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1)
- en: ESM features are used in countless applications from predicting 3D structure
    (in [ESMFold](https://github.com/facebookresearch/esm)) to protein-ligand binding
    ([DiffDock](https://arxiv.org/abs/2210.01776) and its descendants) to protein
    structure generative models (like a recent [FoldFlow 2](https://www.dreamfold.ai/blog/foldflow-2)).
    Bigger transformers and more data are likely to increase protein LMsâ€™ performance
    even further â€” at this scale, however, the data question becomes more prevalent
    (we also discuss the interplay between architecture and data in the dedicated
    section), eg, the [ESM Metagenomic Atlas](https://esmatlas.com/) already encodes
    700M+ structures including those seen outside humans in the soil, oceans, or hydrothermal
    vents. Is there a way to trillions of tokens as in common LLM training datasets?
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ESMç‰¹å¾è¢«åº”ç”¨äºæ— æ•°é¢†åŸŸï¼Œä»é¢„æµ‹3Dç»“æ„ï¼ˆåœ¨[ESMFold](https://github.com/facebookresearch/esm)ä¸­ï¼‰åˆ°è›‹ç™½è´¨-é…ä½“ç»“åˆï¼ˆ[DiffDock](https://arxiv.org/abs/2210.01776)åŠå…¶åç»­æ¨¡å‹ï¼‰ï¼Œå†åˆ°è›‹ç™½è´¨ç»“æ„ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚æœ€è¿‘çš„[FoldFlow
    2](https://www.dreamfold.ai/blog/foldflow-2)ï¼‰ã€‚æ›´å¤§çš„Transformerå’Œæ›´å¤šçš„æ•°æ®å¯èƒ½ä¼šè¿›ä¸€æ­¥æé«˜è›‹ç™½è´¨è¯­è¨€æ¨¡å‹çš„æ€§èƒ½â€”â€”ç„¶è€Œï¼Œåœ¨è¿™ä¸ªè§„æ¨¡ä¸‹ï¼Œæ•°æ®é—®é¢˜å˜å¾—æ›´åŠ çªå‡ºï¼ˆæˆ‘ä»¬ä¹Ÿåœ¨ä¸“é—¨çš„éƒ¨åˆ†è®¨è®ºäº†æ¶æ„ä¸æ•°æ®ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼‰ï¼Œä¾‹å¦‚ï¼Œ[ESMå®åŸºå› ç»„å›¾è°±](https://esmatlas.com/)å·²ç»ç¼–ç äº†è¶…è¿‡7äº¿ä¸ªç»“æ„ï¼ŒåŒ…æ‹¬åœ¨äººç±»ä¹‹å¤–çš„åœŸå£¤ã€æµ·æ´‹æˆ–çƒ­æ³‰ä¸­è§åˆ°çš„ç»“æ„ã€‚æ˜¯å¦æœ‰åŠæ³•åƒå¸¸è§çš„LLMè®­ç»ƒæ•°æ®é›†é‚£æ ·ï¼Œå¤„ç†æ•°ä¸‡äº¿ä¸ªæ ‡è®°ï¼Ÿ
- en: '**ğŸ“šRead more in references [16][17]**'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸ“šæ›´å¤šå†…å®¹è¯·å‚è€ƒæ–‡çŒ® [16][17]**'
- en: '2D Molecules: MiniMol and MolGPS'
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2Dåˆ†å­ï¼šMiniMolå’ŒMolGPS
- en: '**Setup**: given a 2D graph structure with atom types and bond types, predict
    molecular properties'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®¾ç½®**ï¼šç»™å®šä¸€ä¸ªåŒ…å«åŸå­ç±»å‹å’Œé”®ç±»å‹çš„2Då›¾ç»“æ„ï¼Œé¢„æµ‹åˆ†å­ç‰¹æ€§'
- en: '**What is transferable**: a vocabulary of atoms from the periodic table and
    bond types'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¯è½¬ç§»çš„å†…å®¹**ï¼šå‘¨æœŸè¡¨ä¸­çš„åŸå­è¯æ±‡å’Œé”®ç±»å‹'
- en: With 2D graphs (without 3D atom coordinates) universal encoding and transferability
    come from a fixed vocabulary of atom and bond types which you can send to any
    GNN or Transformer encoder. Although molecular fingerprints have been used since
    1960s ([Morgan fingerprints](https://pubs.acs.org/doi/abs/10.1021/c160017a018)
    [18]), their primary goal was to evaluate similarity, not to model a latent space.
    The task of a single (large) neural encoder is to learn useful representations
    that might hint at certain physical molecular properties.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨2Då›¾å½¢ï¼ˆæ²¡æœ‰3DåŸå­åæ ‡ï¼‰æ—¶ï¼Œé€šç”¨ç¼–ç å’Œå¯è½¬ç§»æ€§æ¥è‡ªäºä¸€ä¸ªå›ºå®šçš„åŸå­å’Œé”®ç±»å‹è¯æ±‡è¡¨ï¼Œä½ å¯ä»¥å°†å…¶å‘é€åˆ°ä»»ä½•GNNæˆ–Transformerç¼–ç å™¨ä¸­ã€‚å°½ç®¡åˆ†å­æŒ‡çº¹è‡ª1960å¹´ä»£ä»¥æ¥å°±å·²è¢«ä½¿ç”¨ï¼ˆ[æ‘©æ ¹æŒ‡çº¹](https://pubs.acs.org/doi/abs/10.1021/c160017a018)
    [18]ï¼‰ï¼Œå®ƒä»¬çš„ä¸»è¦ç›®æ ‡æ˜¯è¯„ä¼°ç›¸ä¼¼æ€§ï¼Œè€Œä¸æ˜¯å»ºæ¨¡æ½œåœ¨ç©ºé—´ã€‚å•ä¸ªï¼ˆå¤§å‹ï¼‰ç¥ç»ç¼–ç å™¨çš„ä»»åŠ¡æ˜¯å­¦ä¹ æœ‰ç”¨çš„è¡¨ç¤ºï¼Œè¿™äº›è¡¨ç¤ºå¯èƒ½æš—ç¤ºæŸäº›ç‰©ç†åˆ†å­ç‰¹æ€§ã€‚
- en: Recent examples of generalist models for learning molecular representations
    are [MiniMol](https://arxiv.org/pdf/2404.14986) and [MolGPS](https://arxiv.org/abs/2404.11568v1)
    which have been trained on a large corpus of molecular graphs and probed on dozens
    of downstream tasks. That said, you still need to fine-tune a separate task-specific
    decoder / predictor given the modelsâ€™ representations â€” in that sense, one single
    pre-trained model will not be able to run zero-shot inference on all possible
    unseen tasks, rather on those for which decoders have been trained. Fine-tuning
    is still a good cheap option though since those models are orders of magnitude
    smaller than LLMs.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: è¿‘æœŸå…³äºå­¦ä¹ åˆ†å­è¡¨ç¤ºçš„é€šç”¨æ¨¡å‹ç¤ºä¾‹åŒ…æ‹¬[MiniMol](https://arxiv.org/pdf/2404.14986)å’Œ[MolGPS](https://arxiv.org/abs/2404.11568v1)ï¼Œè¿™äº›æ¨¡å‹å·²ç»åœ¨å¤§é‡åˆ†å­å›¾ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œå¹¶åœ¨æ•°åä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­è¿›è¡Œäº†æµ‹è¯•ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå°½ç®¡å¦‚æ­¤ï¼Œä½ ä»ç„¶éœ€è¦æ ¹æ®æ¨¡å‹çš„è¡¨ç¤ºæ¥å¾®è°ƒä¸€ä¸ªå•ç‹¬çš„ä»»åŠ¡ç‰¹å®šè§£ç å™¨/é¢„æµ‹å™¨â€”â€”ä»è¿™ä¸ªæ„ä¹‰ä¸Šè®²ï¼Œä¸€ä¸ªå•ä¸€çš„é¢„è®­ç»ƒæ¨¡å‹ä¸èƒ½å¯¹æ‰€æœ‰å¯èƒ½çš„æœªè§ä»»åŠ¡è¿›è¡Œé›¶-shot
    æ¨ç†ï¼Œè€Œåªèƒ½é’ˆå¯¹å·²ç»è®­ç»ƒäº†è§£ç å™¨çš„ä»»åŠ¡è¿›è¡Œæ¨ç†ã€‚ä¸è¿‡ï¼Œå¾®è°ƒä»ç„¶æ˜¯ä¸€ä¸ªä¾¿å®œä¸”æœ‰æ•ˆçš„é€‰æ‹©ï¼Œå› ä¸ºè¿™äº›æ¨¡å‹çš„è§„æ¨¡æ¯”å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å°å‡ ä¸ªæ•°é‡çº§ã€‚
- en: '![](../Images/4ecb1c3dab87648d36a6710602ab0524.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ecb1c3dab87648d36a6710602ab0524.png)'
- en: 'Source: (1) Workflow overview of the [MiniMol](https://arxiv.org/pdf/2404.14986)
    pre-training and downstream task evaluation. (2) Criteria of the scaling study
    of [MolGPS](https://arxiv.org/abs/2404.11568v1)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼šï¼ˆ1ï¼‰[MiniMol](https://arxiv.org/pdf/2404.14986)çš„é¢„è®­ç»ƒå’Œä¸‹æ¸¸ä»»åŠ¡è¯„ä¼°å·¥ä½œæµæ¦‚è¿°ã€‚ï¼ˆ2ï¼‰[MolGPS](https://arxiv.org/abs/2404.11568v1)æ‰©å±•æ€§ç ”ç©¶çš„æ ‡å‡†ã€‚
- en: '**ğŸ“šRead more in references [19][20]**'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸ“šæŸ¥çœ‹æ›´å¤šå‚è€ƒæ–‡çŒ® [19][20]**'
- en: 'Expressivity & Scaling Laws: Do Graph FMs scale?'
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¡¨è¾¾æ€§ä¸æ‰©å±•æ€§å®šå¾‹ï¼šå›¾å½¢ FM æ˜¯å¦å…·å¤‡æ‰©å±•æ€§ï¼Ÿ
- en: Transformers in LLMs and multi-modal frontier models are rather standard and
    we know some basic scaling principles for them. Do transformers (as an architecture,
    not LLMs) work equally well on graphs? What are the general challenges when designing
    a backbone for Graph FMs?
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€å‰æ²¿æ¨¡å‹ä¸­ï¼Œå˜æ¢å™¨æ˜¯ç›¸å¯¹æ ‡å‡†çš„ï¼Œå¹¶ä¸”æˆ‘ä»¬å·²ç»äº†è§£äº†å®ƒä»¬çš„ä¸€äº›åŸºæœ¬æ‰©å±•åŸç†ã€‚é‚£ä¹ˆï¼Œå˜æ¢å™¨ï¼ˆä½œä¸ºä¸€ç§æ¶æ„ï¼Œè€Œä¸æ˜¯ LLMsï¼‰åœ¨å›¾ä¸Šèƒ½å¦åŒæ ·æœ‰æ•ˆï¼Ÿåœ¨è®¾è®¡å›¾å½¢
    FM çš„ä¸»å¹²æ—¶ï¼Œé€šå¸¸é¢ä¸´å“ªäº›æŒ‘æˆ˜ï¼Ÿ
- en: 'If you categorize the models highlighted in the previous sections, only 2 areas
    feature transformers â€” protein LMs (ESM) with a natural sequential bias and small
    molecules (MolGPS). The rest are GNNs. There are several reasons for that:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å¯¹å‰é¢å‡ èŠ‚ä¸­æåˆ°çš„æ¨¡å‹è¿›è¡Œåˆ†ç±»ï¼Œåªæœ‰ä¸¤ä¸ªé¢†åŸŸä½¿ç”¨äº†å˜æ¢å™¨â€”â€”è›‹ç™½è´¨è¯­è¨€æ¨¡å‹ï¼ˆESMï¼‰å…·æœ‰è‡ªç„¶çš„åºåˆ—åå‘ï¼Œå’Œå°åˆ†å­ï¼ˆMolGPSï¼‰ã€‚å…¶ä½™çš„éƒ½æ˜¯å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰ã€‚ä¹‹æ‰€ä»¥å¦‚æ­¤ï¼Œæœ‰å‡ ä¸ªåŸå› ï¼š
- en: Vanilla transformers do not scale to any reasonably large graph larger than
    a standard context length (>4â€“10k nodes). Anything above that range requires tricks
    like feeding only subgraphs (losing the whole graph structure and long-range dependencies)
    or linear attention (that might not have good scaling properties). In contrast,
    GNNs are linear in the number of edges, and, in the case of sparse graphs (V ~
    E), are linear in the number of nodes.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ™®é€šå˜æ¢å™¨æ— æ³•æ‰©å±•åˆ°ä»»ä½•å¤§äºæ ‡å‡†ä¸Šä¸‹æ–‡é•¿åº¦çš„åˆç†å¤§å›¾ï¼ˆ>4â€“10k ä¸ªèŠ‚ç‚¹ï¼‰ã€‚è¶…è¿‡è¿™ä¸ªèŒƒå›´å°±éœ€è¦ä¸€äº›æŠ€å·§ï¼Œä¾‹å¦‚åªè¾“å…¥å­å›¾ï¼ˆå¤±å»æ•´ä¸ªå›¾çš„ç»“æ„å’Œé•¿è·ç¦»ä¾èµ–ï¼‰æˆ–çº¿æ€§æ³¨æ„åŠ›ï¼ˆå¯èƒ½æ²¡æœ‰è‰¯å¥½çš„æ‰©å±•æ€§ï¼‰ã€‚ç›¸åï¼ŒGNNs
    åœ¨è¾¹çš„æ•°é‡ä¸Šæ˜¯çº¿æ€§çš„ï¼Œå¹¶ä¸”åœ¨ç¨€ç–å›¾ï¼ˆV ~ Eï¼‰çš„æƒ…å†µä¸‹ï¼ŒèŠ‚ç‚¹çš„æ•°é‡ä¹Ÿæ˜¯çº¿æ€§çš„ã€‚
- en: Vanilla transformers without positional encodings are [less expressive than
    GNNs](https://arxiv.org/abs/2302.04181). Mining positional encodings like Laplacian
    PEs on a graph with V nodes is O(VÂ³).
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ²¡æœ‰ä½ç½®ç¼–ç çš„æ™®é€šå˜æ¢å™¨[æ¯” GNNs æ›´ä¸å…·å¤‡è¡¨è¾¾æ€§](https://arxiv.org/abs/2302.04181)ã€‚åœ¨åŒ…å« V ä¸ªèŠ‚ç‚¹çš„å›¾ä¸ŠæŒ–æ˜ä½ç½®ç¼–ç ï¼ˆå¦‚æ‹‰æ™®æ‹‰æ–¯ä½ç½®ç¼–ç ï¼‰æ˜¯
    O(VÂ³)ã€‚
- en: What should be a â€œtokenâ€ when encoding graphs via transformers? There is no
    clear winner in the literature, e.g., [nodes](https://arxiv.org/abs/2106.05234),
    [nodes + edges](https://arxiv.org/abs/2406.03148), or [subgraphs](https://arxiv.org/abs/2212.13350)
    are all viable option
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨é€šè¿‡å˜æ¢å™¨ç¼–ç å›¾æ—¶ï¼Œä»€ä¹ˆåº”å½“æˆä¸ºâ€œtokenâ€ï¼Ÿæ–‡çŒ®ä¸­å¹¶æ²¡æœ‰æ˜ç¡®çš„ç»“è®ºï¼Œä¾‹å¦‚ï¼Œ[èŠ‚ç‚¹](https://arxiv.org/abs/2106.05234)ã€[èŠ‚ç‚¹
    + è¾¹](https://arxiv.org/abs/2406.03148)æˆ–[å­å›¾](https://arxiv.org/abs/2212.13350)éƒ½æ˜¯å¯è¡Œçš„é€‰æ‹©ã€‚
- en: '**â¡ï¸** Touching upon **expressivity**, different graph tasks need to deal with
    different symmetries, e.g., automorphic nodes in link prediction lead to indistinguishable
    representations, whereas in graph classification/regression going beyond 1-WL
    is necessary for distinguishing molecules which otherwise might look isomorphic
    to vanilla GNNs.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**â¡ï¸** è¯´åˆ°**è¡¨è¾¾æ€§**ï¼Œä¸åŒçš„å›¾ä»»åŠ¡éœ€è¦å¤„ç†ä¸åŒçš„å¯¹ç§°æ€§ï¼Œä¾‹å¦‚ï¼Œé“¾æ¥é¢„æµ‹ä¸­çš„è‡ªåŒæ„èŠ‚ç‚¹ä¼šå¯¼è‡´æ— æ³•åŒºåˆ†çš„è¡¨ç¤ºï¼Œè€Œåœ¨å›¾åˆ†ç±»/å›å½’ä¸­ï¼Œè¶…è¶Š 1-WL
    æ˜¯åŒºåˆ†åˆ†å­æ‰€å¿…éœ€çš„ï¼Œå¦åˆ™è¿™äº›åˆ†å­å¯èƒ½çœ‹èµ·æ¥ä¸æ™®é€šçš„å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰ç›¸åŒã€‚'
- en: '![](../Images/90096ce16fd35189294dadd4da0ca661.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90096ce16fd35189294dadd4da0ca661.png)'
- en: 'Different tasks need to deal with different symmetries. Image by Authors. Sources
    of graphs: (1) [Zhang et al](https://arxiv.org/abs/2010.16103), (2) [Morris et
    al](https://arxiv.org/abs/2112.09992)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸åŒçš„ä»»åŠ¡éœ€è¦å¤„ç†ä¸åŒçš„å¯¹ç§°æ€§ã€‚å›¾åƒä½œè€…æä¾›ã€‚å›¾çš„æ¥æºï¼šï¼ˆ1ï¼‰[Zhangç­‰äºº](https://arxiv.org/abs/2010.16103)ï¼Œï¼ˆ2ï¼‰[Morrisç­‰äºº](https://arxiv.org/abs/2112.09992)
- en: 'This fact begs two questions:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€äº‹å®å¼•å‡ºäº†ä¸¤ä¸ªé—®é¢˜ï¼š
- en: '*How expressive should GFMs be? What is the trade-off between expressivity
    and scalability?*'
  id: totrans-128
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*GFMåœ¨è¡¨è¾¾ä¸Šåº”è¯¥æœ‰å¤šè¡¨è¾¾æ€§ï¼Ÿè¡¨è¾¾æ€§å’Œå¯æ‰©å±•æ€§ä¹‹é—´çš„æƒè¡¡åœ¨å“ªé‡Œï¼Ÿ*'
- en: Ideally, we want a single model to resolve all those symmetries equally well.
    However, more expressive models would lead to more computationally expensive architectures
    both in training and inference. We agree with the recent [ICMLâ€™24 position paper
    on the future directions in Graph ML theory](https://arxiv.org/abs/2402.02287)
    that the community should seek the balance between expressivity, generalization,
    and optimization.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ç†æƒ³æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›å•ä¸€æ¨¡å‹èƒ½å¤ŸåŒæ ·æœ‰æ•ˆåœ°è§£å†³æ‰€æœ‰è¿™äº›å¯¹ç§°æ€§ã€‚ç„¶è€Œï¼Œæ›´å…·è¡¨ç°åŠ›çš„æ¨¡å‹å°†å¯¼è‡´åœ¨è®­ç»ƒå’Œæ¨ç†ä¸­æ›´æ˜‚è´µçš„æ¶æ„ã€‚æˆ‘ä»¬åŒæ„æœ€è¿‘çš„[ICML'24å›¾æœºå™¨å­¦ä¹ ç†è®ºæœªæ¥æ–¹å‘çš„ç«‹åœºè®ºæ–‡](https://arxiv.org/abs/2402.02287)ï¼Œå³ç¤¾åŒºåº”è¯¥åœ¨è¡¨è¾¾æ€§ã€æ³›åŒ–æ€§å’Œä¼˜åŒ–ä¹‹é—´å¯»æ±‚å¹³è¡¡ã€‚
- en: Still, it is worth noting that with the growing availability of training data,
    it might be a computationally cheaper idea to defer learning complex symmetries
    and invariances directly from the data (instead of baking them into a model).
    A few recent good examples of this thesis are [AlphaFold 3](https://www.nature.com/articles/s41586-024-07487-w)
    and [Molecular Conformer Fields](https://arxiv.org/abs/2311.17932) that reach
    SOTA in many generative applications *without* expensive equivariant geometric
    encoders.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œéšç€è®­ç»ƒæ•°æ®çš„æ—¥ç›Šå¢å¤šï¼Œæ¨è¿Ÿç›´æ¥ä»æ•°æ®ä¸­å­¦ä¹ å¤æ‚çš„å¯¹ç§°æ€§å’Œä¸å˜æ€§å¯èƒ½æ˜¯ä¸€ä¸ªè®¡ç®—ä¸Šæ›´ä¾¿å®œçš„æƒ³æ³•ï¼ˆè€Œä¸æ˜¯å°†å®ƒä»¬æ•´åˆåˆ°æ¨¡å‹ä¸­ï¼‰ã€‚è¿™ä¸ªè®ºç‚¹çš„ä¸€äº›æœ€æ–°è‰¯å¥½ä¾‹å­æ˜¯[AlphaFold
    3](https://www.nature.com/articles/s41586-024-07487-w)å’Œ[Molecular Conformer Fields](https://arxiv.org/abs/2311.17932)ï¼Œåœ¨è®¸å¤šç”Ÿæˆåº”ç”¨ä¸­è¾¾åˆ°äº†SOTAï¼Œ*è€Œæ— éœ€*æ˜‚è´µçš„ç­‰å˜å‡ ä½•ç¼–ç å™¨ã€‚
- en: '**ğŸ“šRead more in references [21]**'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸ“š åœ¨å‚è€ƒæ–‡çŒ® [21] ä¸­é˜…è¯»æ›´å¤š**'
- en: '**â¡ï¸** When it comes to **scaling**, both model and data should be scaled up.
    However:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**â¡ï¸** åœ¨æ¶‰åŠ**æ‰©å±•**æ—¶ï¼Œæ¨¡å‹å’Œæ•°æ®éƒ½åº”è¯¥æ‰©å±•ã€‚ç„¶è€Œï¼š'
- en: 'âŒ Non-geometric graphs: There is no principled study on scaling GNNs or Transformers
    to large graphs and common tasks like node classification and link prediction.
    2-layer GraphSAGE is often not very far away from huge 16-layer graph transformers.
    In a similar trend, in the KG reasoning domain, a single ULTRA model (discussed
    above) with <200k parameters outperforms million-sized shallow embedding models
    on 50+ graphs. Why is it happening? Weâ€™d hypothesize the crux is in 1ï¸âƒ£ task nature
    â€” most of non-geometric graphs are noisy similarity graphs that are not bounded
    to a concrete physical phenomenon like molecules 2ï¸âƒ£ Given rich node and edge
    features, models have to learn *representations of graph structures* (common for
    link prediction) or just *functions over given features* (a good example is [node
    classification in OGB](https://ogb.stanford.edu/docs/leader_nodeprop/) where most
    gains are achieved by adding an LLM feature encoder).'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: âŒ éå‡ ä½•å›¾ï¼šæ²¡æœ‰å…³äºå°†GNNæˆ–å˜å‹å™¨æ‰©å±•åˆ°å¤§å›¾å’Œå¸¸è§ä»»åŠ¡ï¼ˆå¦‚èŠ‚ç‚¹åˆ†ç±»å’Œé“¾æ¥é¢„æµ‹ï¼‰çš„åŸåˆ™æ€§ç ”ç©¶ã€‚2å±‚GraphSAGEé€šå¸¸ä¸åºå¤§çš„16å±‚å›¾å˜æ¢å™¨ç›¸è·ä¸è¿œã€‚åœ¨KGæ¨ç†é¢†åŸŸä¸­ï¼Œä¸€ä¸ªå•ä¸€çš„ULTRAæ¨¡å‹ï¼ˆå¦‚ä¸Šæ‰€è¿°ï¼‰ä»¥å°äº20ä¸‡ä¸ªå‚æ•°ä¼˜äº50å¤šä¸ªå›¾ä¸Šç™¾ä¸‡å¤§å°çš„æµ…åµŒå…¥æ¨¡å‹ã€‚ä¸ºä»€ä¹ˆä¼šå‘ç”Ÿè¿™ç§æƒ…å†µï¼Ÿæˆ‘ä»¬æ¨æµ‹å…³é”®åœ¨äº1ï¸âƒ£ä»»åŠ¡æ€§è´¨
    â€” å¤§å¤šæ•°éå‡ ä½•å›¾æ˜¯å˜ˆæ‚çš„ç›¸ä¼¼æ€§å›¾ï¼Œå¹¶ä¸å±€é™äºå…·ä½“çš„ç‰©ç†ç°è±¡å¦‚åˆ†å­ï¼›2ï¸âƒ£é‰´äºä¸°å¯Œçš„èŠ‚ç‚¹å’Œè¾¹ç¼˜ç‰¹å¾ï¼Œæ¨¡å‹å¿…é¡»å­¦ä¹ *å›¾ç»“æ„çš„è¡¨ç¤º*ï¼ˆç”¨äºé“¾æ¥é¢„æµ‹çš„å¸¸è§ç‰¹å¾ï¼‰æˆ–ä»…*åœ¨ç»™å®šç‰¹å¾ä¸Šæ‰§è¡ŒåŠŸèƒ½*ï¼ˆä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­æ˜¯[OGBä¸­çš„èŠ‚ç‚¹åˆ†ç±»](https://ogb.stanford.edu/docs/leader_nodeprop/)ï¼Œå¤§éƒ¨åˆ†æ”¶ç›Šæ˜¯é€šè¿‡æ·»åŠ LLMç‰¹å¾ç¼–ç å™¨å®ç°çš„ï¼‰ã€‚
- en: 'âœ… Geometric graphs: There are several recent works focusing on molecular graphs:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: âœ… å‡ ä½•å›¾ï¼šæœ‰å‡ ç¯‡è¿‘æœŸçš„ä½œå“ä¸“æ³¨äºåˆ†å­å›¾ï¼š
- en: '[Frey et al](https://www.nature.com/articles/s42256-023-00740-3) (2023) study
    scaling of geometric GNNs for ML potentials;'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Freyç­‰äºº](https://www.nature.com/articles/s42256-023-00740-3)ï¼ˆ2023å¹´ï¼‰ç ”ç©¶äº†ç”¨äºMLæ½œåŠ›çš„å‡ ä½•GNNçš„æ‰©å±•ï¼›'
- en: '[Sypetkowski, Wenkel et al](https://arxiv.org/abs/2404.11568v1) (2024) introduce
    MolGPS and study scaling MPNNs and Graph Transformers up to 1B parameters on the
    large dataset of 5M molecules'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Sypetkowski, Wenkelç­‰äºº](https://arxiv.org/abs/2404.11568v1)ï¼ˆ2024å¹´ï¼‰ä»‹ç»äº†MolGPSï¼Œå¹¶ç ”ç©¶äº†åœ¨åŒ…å«500ä¸‡åˆ†å­çš„å¤§æ•°æ®é›†ä¸Šå°†MPNNså’Œå›¾å˜æ¢å™¨æ‰©å±•åˆ°äº†10äº¿å‚æ•°çš„é—®é¢˜ã€‚'
- en: '[Liu et al](https://arxiv.org/abs/2402.02054) (2024) probe GCN, GIN, and GraphGPS
    up to 100M parameters on molecular datasets up to 4M molecules.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Liuç­‰äºº](https://arxiv.org/abs/2402.02054)ï¼ˆ2024å¹´ï¼‰æ¢ç´¢äº†åœ¨åŒ…å«4ç™¾ä¸‡åˆ†å­çš„åˆ†å­æ•°æ®é›†ä¸Šå°†GCNã€GINå’ŒGraphGPSæ‰©å±•åˆ°1äº¿å‚æ•°çš„é—®é¢˜ã€‚'
- en: '![](../Images/73d40bef726833d6ae93a9e9c4bff79b.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/73d40bef726833d6ae93a9e9c4bff79b.png)'
- en: 'Scaling molecular GNNs and GTs. Sources: (1) [Sypetkowski, Wenkel et al](https://arxiv.org/abs/2404.11568v1),
    (2) [Liu et al](https://arxiv.org/abs/2402.02054)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰©å±•åˆ†å­GNNå’ŒGTã€‚æ¥æºï¼š(1) [Sypetkowski, Wenkel ç­‰äºº](https://arxiv.org/abs/2404.11568v1)ï¼Œ(2)
    [Liu ç­‰äºº](https://arxiv.org/abs/2402.02054)
- en: 'The Data Question: What should be scaled? Is there enough graph data to train
    Graph FMs?'
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ•°æ®é—®é¢˜ï¼šåº”è¯¥æ‰©å±•ä»€ä¹ˆï¼Ÿæ˜¯å¦æœ‰è¶³å¤Ÿçš„å›¾æ•°æ®æ¥è®­ç»ƒå›¾å½¢FMï¼Ÿ
- en: 1ï¸âƒ£ **What should be scaled in graph data?** Nodes? Edges? The number of graphs?
    Something else?
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ **å›¾æ•°æ®ä¸­åº”è¯¥æ‰©å±•ä»€ä¹ˆï¼Ÿ** èŠ‚ç‚¹ï¼Ÿè¾¹ï¼Ÿå›¾çš„æ•°é‡ï¼Ÿè¿˜æ˜¯å…¶ä»–ä¸œè¥¿ï¼Ÿ
- en: There is no clear winner in the literature, we would rather gravitate towards
    a broader term ***diversity***, that is, a diversity of patterns in the graph
    data. For example, in node classification on large product graphs, it likely would
    not matter much if you train on a graph with 100M nodes or 10B nodes since itâ€™s
    the same nature of a user-item graph. However, showing examples with homophily
    and heterophily on different scales and sparsities might be quite beneficial.
    In **GraphAny**, showing examples of such graphs allowed to build a robust node
    classifier that generalizes to different graph distributions,
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡çŒ®ä¸­æ²¡æœ‰æ˜ç¡®çš„èµ¢å®¶ï¼Œæˆ‘ä»¬æ›´å€¾å‘äºé‡‡ç”¨ä¸€ä¸ªæ›´å¹¿æ³›çš„æœ¯è¯­***å¤šæ ·æ€§***ï¼Œå³å›¾æ•°æ®ä¸­æ¨¡å¼çš„å¤šæ ·æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨å¤§å‹äº§å“å›¾ä¸Šçš„èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œå¦‚æœä½ åœ¨ä¸€ä¸ªæœ‰1äº¿ä¸ªèŠ‚ç‚¹çš„å›¾ä¸Šè®­ç»ƒï¼Œè¿˜æ˜¯åœ¨ä¸€ä¸ªæœ‰100äº¿ä¸ªèŠ‚ç‚¹çš„å›¾ä¸Šè®­ç»ƒï¼Œå¯èƒ½æ²¡æœ‰å¤ªå¤§åŒºåˆ«ï¼Œå› ä¸ºå®ƒä»¬æœ¬è´¨ä¸Šéƒ½æ˜¯ç”¨æˆ·-ç‰©å“å›¾ã€‚ç„¶è€Œï¼Œå±•ç¤ºåœ¨ä¸åŒå°ºåº¦å’Œç¨€ç–åº¦ä¸‹çš„åŒç±»æ€§å’Œå¼‚ç±»æ€§çš„ä¾‹å­å¯èƒ½ä¼šéå¸¸æœ‰ç›Šã€‚åœ¨**GraphAny**ä¸­ï¼Œå±•ç¤ºè¿™ç§å›¾å½¢çš„ä¾‹å­æœ‰åŠ©äºæ„å»ºä¸€ä¸ªèƒ½å¤Ÿæ³›åŒ–åˆ°ä¸åŒå›¾åˆ†å¸ƒçš„ç¨³å¥èŠ‚ç‚¹åˆ†ç±»å™¨ã€‚
- en: In KG reasoning with **ULTRA**, it was found that the ***diversity of relational
    patterns*** in pre-training plays the biggest role in inductive generalization,
    e.g., one large dense graph is worse than a collection of smaller but sparse,
    dense, few-relational, and many-relational graphs.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½¿ç”¨**ULTRA**è¿›è¡ŒçŸ¥è¯†å›¾è°±æ¨ç†æ—¶ï¼Œå‘ç°é¢„è®­ç»ƒä¸­çš„***å…³ç³»æ¨¡å¼å¤šæ ·æ€§***åœ¨å½’çº³æ³›åŒ–ä¸­èµ·åˆ°äº†æœ€å¤§ä½œç”¨ï¼Œä¾‹å¦‚ï¼Œä¸€ä¸ªå¤§çš„å¯†é›†å›¾æ¯”ä¸€ç»„è¾ƒå°ä½†ç¨€ç–ã€å¯†é›†ã€å°‘å…³ç³»å’Œå¤šå…³ç³»çš„å›¾å·®ã€‚
- en: In molecular graph-level tasks, e.g., in **MolGPS**, scaling the number of unique
    molecules with different physical properties helps a lot (as shown in the charts
    above ğŸ‘†).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åˆ†å­å›¾çº§åˆ«çš„ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚åœ¨**MolGPS**ä¸­ï¼Œå¢åŠ å…·æœ‰ä¸åŒç‰©ç†å±æ€§çš„ç‹¬ç‰¹åˆ†å­çš„æ•°é‡æœ‰å¾ˆå¤§å¸®åŠ©ï¼ˆå¦‚ä¸Šå›¾æ‰€ç¤º ğŸ‘†ï¼‰ã€‚
- en: Besides, [UniAug](https://arxiv.org/abs/2406.01899) finds that increased coverage
    of the structural patterns in pre-training data adds to the performance across
    different downstream tasks from various domains.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œ[UniAug](https://arxiv.org/abs/2406.01899)å‘ç°ï¼Œåœ¨é¢„è®­ç»ƒæ•°æ®ä¸­å¢åŠ ç»“æ„æ¨¡å¼çš„è¦†ç›–ç‡å¯ä»¥æé«˜ä¸åŒä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ï¼Œè¿™äº›ä»»åŠ¡æ¥è‡ªä¸åŒé¢†åŸŸã€‚
- en: '**2ï¸âƒ£ Is there enough data to train Graph FMs?**'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**2ï¸âƒ£ æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®æ¥è®­ç»ƒå›¾å½¢FMï¼Ÿ**'
- en: Openly available graph data is orders of magnitudes smaller than natural language
    tokens or images or videos, and it is fine. This very article includes thousands
    of language and image tokens and no explicit graphs (unless you try to parse this
    text to a graph like an [abstract meaning representation](https://en.wikipedia.org/wiki/Abstract_Meaning_Representation)
    graph). The number of â€˜goodâ€™ proteins with known structures in PDB is small, the
    number of known â€˜goodâ€™ molecules for drugs is small.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: å¼€æ”¾å¯ç”¨çš„å›¾æ•°æ®çš„è§„æ¨¡æ¯”è‡ªç„¶è¯­è¨€æ ‡è®°ã€å›¾åƒæˆ–è§†é¢‘å°å‡ ä¸ªæ•°é‡çº§ï¼Œè¿™å®Œå…¨æ²¡é—®é¢˜ã€‚æœ¬æ–‡å°±åŒ…å«äº†æˆåƒä¸Šä¸‡çš„è¯­è¨€å’Œå›¾åƒæ ‡è®°ï¼Œä½†æ²¡æœ‰æ˜ç¡®çš„å›¾å½¢ï¼ˆé™¤éä½ è¯•å›¾å°†è¿™æ®µæ–‡æœ¬è§£ææˆåƒ[æŠ½è±¡æ„ä¹‰è¡¨ç¤º](https://en.wikipedia.org/wiki/Abstract_Meaning_Representation)è¿™æ ·çš„å›¾ï¼‰ã€‚åœ¨PDBä¸­ï¼Œå·²çŸ¥ç»“æ„çš„â€œå¥½â€è›‹ç™½è´¨æ•°é‡å¾ˆå°‘ï¼Œå·²çŸ¥ç”¨äºè¯ç‰©çš„â€œå¥½â€åˆ†å­æ•°é‡ä¹Ÿå¾ˆå°‘ã€‚
- en: Are Graph FMs doomed because of data scarcity?
  id: totrans-148
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å›¾å½¢FMæ˜¯å¦æ³¨å®šå› ä¸ºæ•°æ®ç¨€ç¼ºè€Œå¤±è´¥ï¼Ÿ
- en: 'Well, not really. The two open avenues are: (1) more sample-efficient architectures;
    (2) using more black-box and synthetic data.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ï¼Œå®é™…ä¸Šå¹¶æ²¡æœ‰ã€‚ä¸¤ä¸ªå¼€æ”¾çš„æ–¹å‘æ˜¯ï¼š(1) æ›´å…·æ ·æœ¬æ•ˆç‡çš„æ¶æ„ï¼›(2) ä½¿ç”¨æ›´å¤šçš„é»‘ç®±æ•°æ®å’Œåˆæˆæ•°æ®ã€‚
- en: Synthetic benchmarks like [GraphWorld](https://arxiv.org/abs/2203.00112) might
    be of use to increase the diversity of training data and improve generalization
    to real-world datasets. Black-box data obtained from scientific experiments, in
    turn, is likely to become the key factor in building successful foundation models
    in AI 4 Science â€” those who master it will prevail on the market.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: åˆæˆåŸºå‡†å¦‚[GraphWorld](https://arxiv.org/abs/2203.00112)å¯èƒ½å¯¹å¢åŠ è®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§å’Œæé«˜å¯¹çœŸå®ä¸–ç•Œæ•°æ®é›†çš„æ³›åŒ–æœ‰å¸®åŠ©ã€‚è€Œä»ç§‘å­¦å®éªŒä¸­è·å¾—çš„é»‘ç®±æ•°æ®ï¼Œåè¿‡æ¥å¯èƒ½æˆä¸ºæ„å»ºAI
    4 ScienceåŸºç¡€æ¨¡å‹çš„å…³é”®å› ç´ â€”â€”æŒæ¡å®ƒçš„äººå°†ä¼šåœ¨å¸‚åœºä¸­å æ®ä¸»å¯¼åœ°ä½ã€‚
- en: '[](/the-road-to-biology-2-0-will-pass-through-black-box-data-bbd00fabf959?source=post_page-----f363e2576f58--------------------------------)
    [## The Road to Biology 2.0 Will Pass Through Black-Box Data'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/the-road-to-biology-2-0-will-pass-through-black-box-data-bbd00fabf959?source=post_page-----f363e2576f58--------------------------------)
    [## é€šå‘ç”Ÿç‰©å­¦2.0çš„é“è·¯å°†é€šè¿‡é»‘ç®±æ•°æ®'
- en: Future bio-AI breakthroughs will arise from novel high-throughput low-cost AI-specific
    â€œblack-boxâ€ data modalities.
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æœªæ¥çš„ç”Ÿç‰©AIçªç ´å°†æ¥è‡ªæ–°å‹çš„é«˜é€šé‡ä½æˆæœ¬AIç‰¹å®šçš„â€œé»‘ç®±â€æ•°æ®æ¨¡æ€ã€‚
- en: towardsdatascience.com](/the-road-to-biology-2-0-will-pass-through-black-box-data-bbd00fabf959?source=post_page-----f363e2576f58--------------------------------)
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/the-road-to-biology-2-0-will-pass-through-black-box-data-bbd00fabf959?source=post_page-----f363e2576f58--------------------------------)
- en: '**ğŸ“šRead more in references [20][22][23]**'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸ“šåœ¨å‚è€ƒæ–‡çŒ®[20][22][23]ä¸­æŸ¥çœ‹æ›´å¤šå†…å®¹**'
- en: ğŸ‘‰ Key Takeaways ğŸ‘ˆ
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ‘‰ å…³é”®è¦ç‚¹ ğŸ‘ˆ
- en: '**â¡ï¸ How to generalize across graphs with heterogeneous node/edge/graph features?**'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**â¡ï¸ å¦‚ä½•åœ¨å…·æœ‰å¼‚è´¨èŠ‚ç‚¹/è¾¹/å›¾ç‰¹å¾çš„å›¾ä¸Šè¿›è¡Œæ³›åŒ–ï¼Ÿ**'
- en: 'Non-geometric graphs: Relative information transfers (such as prediction differences
    in *GraphAny* or relational interactions in *Ultra*), absolute information does
    not.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: éå‡ ä½•å›¾ï¼šç›¸å¯¹ä¿¡æ¯å¯ä»¥è½¬ç§»ï¼ˆå¦‚*GraphAny*ä¸­çš„é¢„æµ‹å·®å¼‚æˆ–*Ultra*ä¸­çš„å…³ç³»äº’åŠ¨ï¼‰ï¼Œä½†ç»å¯¹ä¿¡æ¯æ— æ³•è½¬ç§»ã€‚
- en: 'Geometric graphs: transfer is easier thanks to the fixed set of atoms, but
    models have to learn some notion of physics to be reliable'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‡ ä½•å›¾ï¼šç”±äºå›ºå®šçš„åŸå­é›†åˆï¼Œè¿ç§»å­¦ä¹ æ›´å®¹æ˜“ï¼Œä½†æ¨¡å‹å¿…é¡»å­¦ä¹ ä¸€äº›ç‰©ç†æ¦‚å¿µæ‰èƒ½å¯é ã€‚
- en: '**â¡ï¸ How to generalize across prediction tasks?**'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**â¡ï¸ å¦‚ä½•åœ¨ä¸åŒçš„é¢„æµ‹ä»»åŠ¡ä¹‹é—´è¿›è¡Œæ³›åŒ–ï¼Ÿ**'
- en: To date, there is no single model (among non-geometric GNNs) that would be able
    to perform node classification, link prediction, and graph classification in the
    zero-shot inference mode.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿„ä»Šä¸ºæ­¢ï¼Œæ²¡æœ‰ä¸€ä¸ªæ¨¡å‹ï¼ˆåœ¨éå‡ ä½•å›¾ç¥ç»ç½‘ç»œä¸­ï¼‰èƒ½å¤Ÿåœ¨é›¶-shotæ¨ç†æ¨¡å¼ä¸‹æ‰§è¡ŒèŠ‚ç‚¹åˆ†ç±»ã€é“¾æ¥é¢„æµ‹å’Œå›¾åˆ†ç±»ã€‚
- en: Framing all tasks through the lens of one might help, eg, node classification
    can be framed as link prediction.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡æŸä¸€ä¸ªè§†è§’æ¡†æ¶æ‰€æœ‰ä»»åŠ¡å¯èƒ½ä¼šæœ‰æ‰€å¸®åŠ©ï¼Œä¾‹å¦‚ï¼ŒèŠ‚ç‚¹åˆ†ç±»å¯ä»¥æ¡†å®šä¸ºé“¾æ¥é¢„æµ‹ä»»åŠ¡ã€‚
- en: '**â¡ï¸ What is the optimal model expressivity?**'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**â¡ï¸ æœ€ä¼˜æ¨¡å‹è¡¨è¾¾èƒ½åŠ›æ˜¯ä»€ä¹ˆï¼Ÿ**'
- en: Node classification, link prediction, and graph classification leverage different
    symmetries.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: èŠ‚ç‚¹åˆ†ç±»ã€é“¾æ¥é¢„æµ‹å’Œå›¾åˆ†ç±»åˆ©ç”¨äº†ä¸åŒçš„å¯¹ç§°æ€§ã€‚
- en: Blunt application of maximally expressive models quickly leads to exponential
    runtime complexity or enormous memory costs â€” need to maintain the *expressivity
    vs efficiency* balance.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹æœ€å¤§è¡¨è¾¾èƒ½åŠ›æ¨¡å‹çš„ç›´æ¥åº”ç”¨ä¼šè¿…é€Ÿå¯¼è‡´æŒ‡æ•°çº§çš„è¿è¡Œæ—¶å¤æ‚åº¦æˆ–å·¨å¤§çš„å†…å­˜å¼€é”€â€”â€”éœ€è¦ä¿æŒ*è¡¨è¾¾èƒ½åŠ›ä¸æ•ˆç‡*çš„å¹³è¡¡ã€‚
- en: The link between expressivity, sample complexity (how much training data you
    need), and inductive generalization is still unknown.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¡¨è¾¾èƒ½åŠ›ã€æ ·æœ¬å¤æ‚åº¦ï¼ˆä½ éœ€è¦å¤šå°‘è®­ç»ƒæ•°æ®ï¼‰å’Œå½’çº³æ³›åŒ–ä¹‹é—´çš„å…³ç³»ä»ç„¶æœªçŸ¥ã€‚
- en: '**â¡ï¸ Data**'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**â¡ï¸ æ•°æ®**'
- en: Openly available graph data is orders of magnitude smaller than text/vision
    data, models have to be sample-efficient.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¼€æ”¾å¯ç”¨çš„å›¾æ•°æ®çš„è§„æ¨¡æ¯”æ–‡æœ¬/è§†è§‰æ•°æ®å°å‡ ä¸ªæ•°é‡çº§ï¼Œå› æ­¤æ¨¡å‹å¿…é¡»å…·å¤‡æ ·æœ¬é«˜æ•ˆæ€§ã€‚
- en: Scaling laws are at the emerging stage, it is still unclear what to scale â€”
    number of nodes? Edges? Motifs? What is the notion of a token in graphs?
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰©å±•æ³•åˆ™ä»å¤„äºæ–°å…´é˜¶æ®µï¼Œå°šä¸æ¸…æ¥šåº”è¯¥æ‰©å±•ä»€ä¹ˆâ€”â€”èŠ‚ç‚¹æ•°ï¼Ÿè¾¹æ•°ï¼Ÿå›¾æ¡ˆï¼Ÿå›¾ä¸­çš„tokenæ¦‚å¿µæ˜¯ä»€ä¹ˆï¼Ÿ
- en: 'Geometric GNNs: there is much more experimental data available that makes little
    sense to domain experts but might be of value to neural nets.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‡ ä½•å›¾ç¥ç»ç½‘ç»œï¼šæœ‰å¤§é‡å®éªŒæ•°æ®ï¼Œè™½ç„¶è¿™äº›æ•°æ®å¯¹é¢†åŸŸä¸“å®¶æ¥è¯´æ„ä¹‰ä¸å¤§ï¼Œä½†å¯èƒ½å¯¹ç¥ç»ç½‘ç»œæœ‰ä»·å€¼ã€‚
- en: Mao, Chen, et al. [Graph Foundation Models Are Already Here](https://arxiv.org/abs/2402.02216).
    ICML 2024
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Maoã€Chenç­‰äºº[å›¾åŸºç¡€æ¨¡å‹å·²ç»åˆ°æ¥](https://arxiv.org/abs/2402.02216)ã€‚ICML 2024
- en: Morris et al. [Future Directions in Foundations of Graph Machine Learning](https://arxiv.org/abs/2402.02287).
    ICML 2024
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Morrisç­‰äºº[å›¾æœºå™¨å­¦ä¹ åŸºç¡€çš„æœªæ¥æ–¹å‘](https://arxiv.org/abs/2402.02287)ã€‚ICML 2024
- en: 'Zhao et al. [GraphAny: A Foundation Model for Node Classification on Any Graph](https://arxiv.org/abs/2405.20445).
    Arxiv 2024\. [Code on Github](https://github.com/DeepGraphLearning/GraphAny)'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zhaoç­‰äºº[GraphAnyï¼šä¸€ä¸ªç”¨äºä»»ä½•å›¾ä¸ŠèŠ‚ç‚¹åˆ†ç±»çš„åŸºç¡€æ¨¡å‹](https://arxiv.org/abs/2405.20445)ã€‚Arxiv 2024ã€‚[Githubä¸Šçš„ä»£ç ](https://github.com/DeepGraphLearning/GraphAny)
- en: Dong et al. [Universal Link Predictor By In-Context Learning on Graphs](https://arxiv.org/abs/2402.07738),
    arxiv 2024
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Dongç­‰äºº[é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ è¿›è¡Œå›¾çš„é€šç”¨é“¾æ¥é¢„æµ‹](https://arxiv.org/abs/2402.07738)ï¼Œarxiv 2024
- en: 'Zhang et al. [Labeling Trick: A Theory of Using Graph Neural Networks for Multi-Node
    Representation Learning](https://arxiv.org/abs/2010.16103). NeurIPS 2021'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zhangç­‰äºº[æ ‡ç­¾æŠ€å·§ï¼šä½¿ç”¨å›¾ç¥ç»ç½‘ç»œè¿›è¡Œå¤šèŠ‚ç‚¹è¡¨ç¤ºå­¦ä¹ çš„ç†è®º](https://arxiv.org/abs/2010.16103)ã€‚NeurIPS
    2021
- en: Chamberlain, Shirobokov, et al. [Graph Neural Networks for Link Prediction with
    Subgraph Sketching](https://arxiv.org/abs/2209.15486). ICLR 2023
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Chamberlainã€Shirobokovç­‰äºº[é€šè¿‡å­å›¾è‰å›¾è¿›è¡Œé“¾æ¥é¢„æµ‹çš„å›¾ç¥ç»ç½‘ç»œ](https://arxiv.org/abs/2209.15486)ã€‚ICLR
    2023
- en: 'Zhu et al. [Neural Bellman-Ford Networks: A General Graph Neural Network Framework
    for Link Prediction](https://arxiv.org/abs/2106.06935). NeurIPS 2021'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zhuç­‰äºº[ç¥ç»è´å°”æ›¼-ç¦ç‰¹ç½‘ç»œï¼šç”¨äºé“¾æ¥é¢„æµ‹çš„é€šç”¨å›¾ç¥ç»ç½‘ç»œæ¡†æ¶](https://arxiv.org/abs/2106.06935)ã€‚NeurIPS
    2021
- en: Galkin et al. [Towards Foundation Models for Knowledge Graph Reasoning](https://openreview.net/forum?id=jVEoydFOl9).
    ICLR 2024
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Galkin ç­‰äºº [é¢å‘çŸ¥è¯†å›¾è°±æ¨ç†çš„åŸºç¡€æ¨¡å‹](https://openreview.net/forum?id=jVEoydFOl9)ã€‚ICLR 2024
- en: Galkin et al. [Zero-shot Logical Query Reasoning on any Knowledge Graph](https://arxiv.org/abs/2404.07198).
    arxiv 2024\. [Code on Github](https://github.com/DeepGraphLearning/ULTRA)
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Galkin ç­‰äºº [åœ¨ä»»ä½•çŸ¥è¯†å›¾è°±ä¸Šçš„é›¶-shoté€»è¾‘æŸ¥è¯¢æ¨ç†](https://arxiv.org/abs/2404.07198)ã€‚arxiv 2024\.
    [GitHubä¸Šçš„ä»£ç ](https://github.com/DeepGraphLearning/ULTRA)
- en: Ibarz et al. [A Generalist Neural Algorithmic Learner](https://proceedings.mlr.press/v198/ibarz22a/ibarz22a.pdf)
    LoG 2022
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ibarz ç­‰äºº [é€šç”¨ç¥ç»ç®—æ³•å­¦ä¹ è€…](https://proceedings.mlr.press/v198/ibarz22a/ibarz22a.pdf)
    LoG 2022
- en: Markeeva, McLeish, Ibarz, et al. [The CLRS-Text Algorithmic Reasoning Language
    Benchmar](https://arxiv.org/abs/2406.04229)k. arxiv 2024
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Markeeva, McLeish, Ibarz ç­‰äºº [CLRS-Text ç®—æ³•æ¨ç†è¯­è¨€åŸºå‡†](https://arxiv.org/abs/2406.04229)ï¼Œarxiv
    2024
- en: 'Shoghi et al. [From Molecules to Materials: Pre-training Large Generalizable
    Models for Atomic Property Prediction](https://arxiv.org/abs/2310.16802). ICLR
    2024'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Shoghi ç­‰äºº [ä»åˆ†å­åˆ°ææ–™ï¼šé¢„è®­ç»ƒçš„å¤§è§„æ¨¡å¯æ³›åŒ–æ¨¡å‹ç”¨äºåŸå­å±æ€§é¢„æµ‹](https://arxiv.org/abs/2310.16802)ã€‚ICLR
    2024
- en: 'Zhang, Liu et al. [DPA-2: Towards a universal large atomic model for molecular
    and material simulation](https://arxiv.org/abs/2312.15492), arxiv 2023'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zhang, Liu ç­‰äºº [DPA-2ï¼šé¢å‘åˆ†å­å’Œææ–™æ¨¡æ‹Ÿçš„é€šç”¨å¤§è§„æ¨¡åŸå­æ¨¡å‹](https://arxiv.org/abs/2312.15492)ï¼Œarxiv
    2023
- en: Batatia et al. [A foundation model for atomistic materials chemistry](https://arxiv.org/abs/2401.00096),
    arxiv 2024
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Batatia ç­‰äºº [åŸå­ææ–™åŒ–å­¦çš„åŸºç¡€æ¨¡å‹](https://arxiv.org/abs/2401.00096)ï¼Œarxiv 2024
- en: 'Yang et al. [MatterSim: A Deep Learning Atomistic Model Across Elements, Temperatures
    and Pressures](https://arxiv.org/abs/2405.04967), arxiv 2024'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Yang ç­‰äºº [MatterSimï¼šè·¨å…ƒç´ ã€æ¸©åº¦å’Œå‹åŠ›çš„æ·±åº¦å­¦ä¹ åŸå­æ¨¡å‹](https://arxiv.org/abs/2405.04967)ï¼Œarxiv
    2024
- en: Rives et al. [Biological Structure and Function Emerge from Scaling Unsupervised
    Learning to 250 Million Protein Sequences](https://www.pnas.org/doi/full/10.1073/pnas.2016239118).
    PNAS 2021
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Rives ç­‰äºº [é€šè¿‡å°†æ— ç›‘ç£å­¦ä¹ æ‰©å±•åˆ°2.5äº¿ä¸ªè›‹ç™½è´¨åºåˆ—ï¼Œç”Ÿç‰©ç»“æ„å’ŒåŠŸèƒ½å¾—ä»¥æ˜¾ç°](https://www.pnas.org/doi/full/10.1073/pnas.2016239118)ã€‚PNAS
    2021
- en: Lin, Akin, Rao, Hie, et al. [Language models of protein sequences at the scale
    of evolution enable accurate structure prediction](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1).
    Science 2023\. [Code](https://github.com/facebookresearch/esm)
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Lin, Akin, Rao, Hie ç­‰äºº [åœ¨è¿›åŒ–è§„æ¨¡ä¸Šï¼Œè›‹ç™½è´¨åºåˆ—çš„è¯­è¨€æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®é¢„æµ‹ç»“æ„](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1)ã€‚Science
    2023\. [ä»£ç ](https://github.com/facebookresearch/esm)
- en: Morgan HL (1965) [The generation of a unique machine description for chemical
    structures â€” a technique developed at chemical abstracts service](https://pubs.acs.org/doi/abs/10.1021/c160017a018).
    J Chem Doc 5:107â€“113.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Morgan HL (1965) [åŒ–å­¦ç»“æ„çš„å”¯ä¸€æœºå™¨æè¿°ç”Ÿæˆâ€”â€”åŒ–å­¦æ–‡æ‘˜æœåŠ¡å¼€å‘çš„ä¸€é¡¹æŠ€æœ¯](https://pubs.acs.org/doi/abs/10.1021/c160017a018)ã€‚J
    Chem Doc 5:107â€“113.
- en: 'KlÃ¤ser, Banaszewski, et al. [MiniMol: A Parameter Efficient Foundation Model
    for Molecular Learning](https://arxiv.org/pdf/2404.14986), arxiv 2024'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: KlÃ¤ser, Banaszewski ç­‰äºº [MiniMolï¼šä¸€ç§é«˜æ•ˆçš„åˆ†å­å­¦ä¹ åŸºç¡€æ¨¡å‹](https://arxiv.org/pdf/2404.14986)ï¼Œarxiv
    2024
- en: Sypetkowski, Wenkel et al. [On the Scalability of GNNs for Molecular Graphs](https://arxiv.org/abs/2404.11568v1),
    arxiv 2024
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sypetkowski, Wenkel ç­‰äºº [GNNs åœ¨åˆ†å­å›¾ä¸Šçš„å¯æ‰©å±•æ€§](https://arxiv.org/abs/2404.11568v1)ï¼Œarxiv
    2024
- en: Morris et al. [Future Directions in Foundations of Graph Machine Learning](https://arxiv.org/abs/2402.02287).
    ICML 2024
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Morris ç­‰äºº [å›¾å½¢æœºå™¨å­¦ä¹ åŸºç¡€çš„æœªæ¥æ–¹å‘](https://arxiv.org/abs/2402.02287)ã€‚ICML 2024
- en: Liu et al. [Neural Scaling Laws on Graphs](https://arxiv.org/abs/2402.02054),
    arxiv 2024
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Liu ç­‰äºº [å›¾ä¸Šçš„ç¥ç»ç¼©æ”¾æ³•åˆ™](https://arxiv.org/abs/2402.02054)ï¼Œarxiv 2024
- en: Frey et al. [Neural scaling of deep chemical models](https://www.nature.com/articles/s42256-023-00740-3),
    Nature Machine Intelligence 2023
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Frey ç­‰äºº [æ·±åº¦åŒ–å­¦æ¨¡å‹çš„ç¥ç»ç¼©æ”¾](https://www.nature.com/articles/s42256-023-00740-3)ï¼ŒNature
    Machine Intelligence 2023
