- en: Foundation Models in Graph & Geometric Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图结构与几何深度学习中的基础模型
- en: 原文：[https://towardsdatascience.com/foundation-models-in-graph-geometric-deep-learning-f363e2576f58?source=collection_archive---------0-----------------------#2024-06-18](https://towardsdatascience.com/foundation-models-in-graph-geometric-deep-learning-f363e2576f58?source=collection_archive---------0-----------------------#2024-06-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/foundation-models-in-graph-geometric-deep-learning-f363e2576f58?source=collection_archive---------0-----------------------#2024-06-18](https://towardsdatascience.com/foundation-models-in-graph-geometric-deep-learning-f363e2576f58?source=collection_archive---------0-----------------------#2024-06-18)
- en: '[](https://mgalkin.medium.com/?source=post_page---byline--f363e2576f58--------------------------------)[![Michael
    Galkin](../Images/c5eb13334712ca0462d8a5df4a268ad0.png)](https://mgalkin.medium.com/?source=post_page---byline--f363e2576f58--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f363e2576f58--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f363e2576f58--------------------------------)
    [Michael Galkin](https://mgalkin.medium.com/?source=post_page---byline--f363e2576f58--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mgalkin.medium.com/?source=post_page---byline--f363e2576f58--------------------------------)[![Michael
    Galkin](../Images/c5eb13334712ca0462d8a5df4a268ad0.png)](https://mgalkin.medium.com/?source=post_page---byline--f363e2576f58--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f363e2576f58--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f363e2576f58--------------------------------)
    [Michael Galkin](https://mgalkin.medium.com/?source=post_page---byline--f363e2576f58--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f363e2576f58--------------------------------)
    ·20 min read·Jun 18, 2024
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f363e2576f58--------------------------------)
    ·阅读时间20分钟·2024年6月18日
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Foundation Models in language, vision, and audio have been among the primary
    research topics in Machine Learning in 2024 whereas FMs for graph-structured data
    have somewhat lagged behind. In this post, we argue that the era of Graph FMs
    has already begun and provide a few examples of how one can use them already today.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 语言、视觉和音频中的基础模型已成为2024年机器学习的主要研究课题，而图结构数据的基础模型则相对滞后。在这篇文章中，我们认为图基础模型的时代已经开始，并提供了一些如何今天就能使用它们的示例。
- en: '*This post was written and edited by* [*Michael Galkin*](https://twitter.com/michael_galkin)
    *and* [*Michael Bronstein*](https://twitter.com/mmbronstein) *with significant
    contributions from* [*Jianan Zhao*](https://twitter.com/AndyJiananZhao)*,* [*Haitao
    Mao*](https://twitter.com/haitao_mao_)*,* [*Zhaocheng Zhu*](https://twitter.com/zhu_zhaocheng)*.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*本文由* [*Michael Galkin*](https://twitter.com/michael_galkin) *和* [*Michael
    Bronstein*](https://twitter.com/mmbronstein) *编写和编辑，并得到了* [*Jianan Zhao*](https://twitter.com/AndyJiananZhao)
    *、* [*Haitao Mao*](https://twitter.com/haitao_mao_) *、* [*Zhaocheng Zhu*](https://twitter.com/zhu_zhaocheng)
    *的重大贡献。*'
- en: '![](../Images/499b977cbe07cc686ff86f6c35480d28.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/499b977cbe07cc686ff86f6c35480d28.png)'
- en: The timeline of emerging foundation models in graph- and geometric deep learning.
    Image by Authors.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图形和几何深度学习中基础模型的时间线。图像由作者提供。
- en: '**Table of Contents**'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**目录**'
- en: '[What are Graph Foundation Models and how to build them?](#6f0a)'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[什么是图基础模型，如何构建它们？](#6f0a)'
- en: '[Node Classification: GraphAny](#b4b7)'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[节点分类：GraphAny](#b4b7)'
- en: '[Link Prediction: Not yet](#89a1)'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[链接预测：尚未](#89a1)'
- en: '[Knowledge Graph Reasoning: ULTRA and UltraQuery](#bda1)'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[知识图推理：ULTRA与UltraQuery](#bda1)'
- en: '[Algorithmic Reasoning: Generalist Algorithmic Learner](#11c3)'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[算法推理：通用算法学习者](#11c3)'
- en: '[Geometric and AI4Science Foundation Models](#65b0)a. [ML Potentials: JMP-1,
    DPA-2 for molecules, MACE-MP-0 and MatterSim for inorganic crystals](#4d3e)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[几何学与AI4Science基础模型](#65b0)a. [机器学习潜力：JMP-1，DPA-2用于分子，MACE-MP-0和MatterSim用于无机晶体](#4d3e)'
- en: 'b. [Protein LMs: ESM-2](#b2cd)'
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. [蛋白质语言模型：ESM-2](#b2cd)
- en: 'c. [2D Molecules: MiniMol and MolGPS](#cc8c)'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c. [二维分子：MiniMol和MolGPS](#cc8c)
- en: '[Expressivity & Scaling Laws: Do Graph FMs scale?](#1443)'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[表达性与扩展规律：图基础模型能扩展吗？](#1443)'
- en: '[The Data Question: What should be scaled? Is there enough graph data to train
    Graph FMs?](#40c5)'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[数据问题：应该扩展什么？是否有足够的图数据来训练图基础模型？](#40c5)'
- en: '[👉 Key Takeaways 👈](#2add)'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[👉 关键要点 👈](#2add)'
- en: What are Graph Foundation Models and how to build them?
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是图基础模型，如何构建它们？
- en: 'Since there is a certain degree of ambiguity in what counts as a “foundational”
    model, it would be appropriate to start with a definition to establish a common
    ground:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由于“基础”模型的定义存在一定的模糊性，最好从一个定义开始，以建立共识：
- en: “A Graph Foundation Model is a single (neural) model that learns transferable
    graph representations that can generalize to any new, previously unseen graph”
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “图基础模型是一个单一的（神经）模型，它学习可迁移的图表示，可以泛化到任何新的、以前未见过的图”
- en: One of the challenges is that graphs come in all forms and shapes and their
    connectivity and feature structure can be very different. Standard Graph Neural
    Networks (GNNs) are not “foundational” because they can work in the best case
    only on graphs with the same type and dimension of features. Graph heuristics
    like [Label Propagation](https://en.wikipedia.org/wiki/Label_propagation_algorithm)
    or [Personalized PageRank](https://en.wikipedia.org/wiki/PageRank) that can run
    on any graph can neither be considered Graph FMs because they do not involve any
    learning. As much as we love Large Language Models, it is still unclear whether
    parsing graphs into sequences that can be then passed to an LLM (like in [GraphText](https://arxiv.org/abs/2310.01089)
    or [Talk Like A Graph](https://openreview.net/forum?id=IuXR1CCrSi)) is a suitable
    approach for retaining graph symmetries and scaling to anything larger than toy-sized
    datasets (we leave LLMs + Graphs to a separate post).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 挑战之一是图的形式和形状各异，它们的连通性和特征结构可能大不相同。标准的图神经网络（GNNs）并不是“基础性”的，因为它们在最佳情况下只能在具有相同类型和维度特征的图上工作。像[标签传播](https://en.wikipedia.org/wiki/Label_propagation_algorithm)或[个性化PageRank](https://en.wikipedia.org/wiki/PageRank)这样的图启发式方法可以在任何图上运行，但不能被视为图FMs，因为它们不涉及任何学习。尽管我们对大型语言模型（LLMs）情有独钟，但仍不清楚将图解析为序列并将其传递给LLM（例如在[GraphText](https://arxiv.org/abs/2310.01089)或[Talk
    Like A Graph](https://openreview.net/forum?id=IuXR1CCrSi)中那样）是否是保持图的对称性并扩展到大于玩具级数据集的合适方法（我们将LLMs与图的结合留待单独的文章讨论）。
- en: 'Perhaps the most important question for designing Graph FMs is transferable
    graph representations. LLMs, as suggested in the recent [ICML 2024 position paper
    by Mao, Chen et al](https://arxiv.org/abs/2402.02216)., can squash any text in
    any language into tokens from a fixed-size vocabulary. Video-Language FMs resort
    to patches that can always be extracted from an image (one always has RGB channels
    in any image or video). It is not immediately clear what could a universal featurization
    (à la tokenization) scheme be for graphs, which might have very diverse characteristics,
    e.g.:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 也许设计图FMs时最重要的问题是可迁移的图表示。正如最近[ICML 2024年由Mao、Chen等人提出的定位论文](https://arxiv.org/abs/2402.02216)中所建议的，LLMs可以将任何语言的文本压缩为固定大小词汇表中的标记。视频-语言FMs依赖于可以从图像中提取的补丁（任何图像或视频中总是有RGB通道）。目前还不清楚图的普适特征化（类似于标记化）方案是什么，因为图可能具有非常多样的特征，例如：
- en: One large graph with node features and some given node labels (typical for node
    classification tasks)
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个大型图，带有节点特征和一些给定的节点标签（典型的节点分类任务）
- en: One large graph without node features and classes, but with meaningful edge
    types (typical for link prediction and KG reasoning)
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个大型图，没有节点特征和类别，但有有意义的边类型（典型的链接预测和知识图谱推理任务）
- en: Many small graphs with/without node/edge features, with graph-level labels (typical
    for graph classification and regression)
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多小图，带/不带节点/边特征，并且有图级标签（典型的图分类和回归任务）
- en: '![](../Images/af6091af7bbb5362938fa26f9fca562c.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af6091af7bbb5362938fa26f9fca562c.png)'
- en: 🦄 An ideal graph foundation model that takes any graph with any node/edge/graph
    features and performs any node- / edge- / graph-level task. Such Graph FMs do
    not exist in pure form as of mid-2024\. Image by Authors
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 🦄 一个理想的图基础模型，它能够处理任何带有任意节点/边/图特征的图，并执行任何节点/边/图级任务。这样的图FMs在2024年中期之前并不存在。图像来源：作者
- en: 'So far, there is a handful of open research questions for the graph learning
    community when designing Graph FMs:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在设计图FMs时，图学习社区有一些开放的研究问题：
- en: '**1️⃣ How to generalize across graphs with heterogeneous node/edge/graph features?**
    For example, the popular [Cora](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.Planetoid.html#torch_geometric.datasets.Planetoid)
    dataset for node classification is one graph with node features of dimension 1,433,
    whereas the Citeseer dataset has 3,703-dimensional features. How can one define
    a single representation space for such diverse graphs?'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**1️⃣ 如何在具有异构节点/边/图特征的图之间进行泛化？** 例如，用于节点分类的流行数据集[Cora](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.Planetoid.html#torch_geometric.datasets.Planetoid)是一个图，节点特征的维度为1,433，而Citeseer数据集的特征维度为3,703。如何为如此多样化的图定义一个单一的表示空间？'
- en: '**2️⃣ How to generalize across prediction tasks?** Node classification tasks
    may have a different number of node classes (e.g., Cora has 7 classes and Citeseer
    6). Even further, can a node classification model perform well in link prediction?'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**2️⃣ 如何在预测任务中进行泛化？** 节点分类任务可能有不同数量的节点类别（例如，Cora有7个类别，Citeseer有6个类别）。更进一步，节点分类模型能否在链接预测任务中表现良好？'
- en: '**3️⃣ What should the foundational model expressivity be?** Much research has
    been done on the expressive power of GNNs, typically resorting to the analogy
    with Weisfeiler-Lehman isomorphism tests. Since graph foundational models should
    ideally handle a broad spectrum of problems, the right expressive power is elusive.
    For instance, in node classification tasks, node features are important along
    with graph homophily or heterophily. In link prediction, structural patterns and
    breaking automorphisms are more important (node features often don’t give a huge
    performance boost). In graph-level tasks, graph isomorphism starts to play a crucial
    role. In 3D geometric tasks like molecule generation, there is an additional complexity
    of continuous symmetries to take care of (see the [Hitchhiker’s Guide to Geometric
    GNNs](https://arxiv.org/abs/2312.07511)).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**3️⃣ 基础模型的表现力应该是多少？** 许多关于GNN表现力的研究已经完成，通常借用Weisfeiler-Lehman同构性测试的类比。由于图形基础模型理想情况下应该处理广泛的问题，因此合适的表现力仍然难以捉摸。例如，在节点分类任务中，节点特征与图形的同质性或异质性都很重要。在链接预测中，结构模式和打破自同构性更加重要（节点特征通常不会带来巨大的性能提升）。在图形级任务中，图形同构性开始发挥重要作用。在像分子生成这样的三维几何任务中，还需要考虑连续对称性带来的额外复杂性（参见[几何GNN的搭便车指南](https://arxiv.org/abs/2312.07511)）。'
- en: In the following sections, we will show that at least in some tasks and domains,
    Graph FMs are already available. We will highlight their design choices when it
    comes to transferable features and practical benefits when it comes to inductive
    inference on new unseen graphs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将展示，至少在某些任务和领域中，图形基础模型（Graph FMs）已经可用。我们将重点介绍它们在可迁移特征方面的设计选择，以及在新未见图形上的归纳推理时带来的实际好处。
- en: '**📚Read more in references [1][2] and** [**Github Repo**](https://github.com/CurryTang/Awesome_Graph_Foundation_Models)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**📚更多内容请参见参考文献[1][2]和** [**Github仓库**](https://github.com/CurryTang/Awesome_Graph_Foundation_Models)'
- en: 'Node Classification: GraphAny'
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 节点分类：GraphAny
- en: 'For years, GNN-based node classifiers have been limited to a single graph dataset.
    That is, given e.g. the Cora graph with 2.7K nodes, 1433-dimensional features,
    and 7 classes, one has to train a GNN specifically on the Cora graph with its
    labels and run inference on the same graph. Applying a trained model on another
    graph, e.g. Citeseer with 3703-dimensional features and 6 classes would run into
    an unsurmountable difficulty: how would one model generalize to different input
    feature dimensions and a different number of classes? Usually, prediction heads
    are hardcoded to a fixed number of classes.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，基于GNN的节点分类器仅限于单一图形数据集。也就是说，例如给定一个具有2.7K个节点、1433维特征和7个类别的Cora图形，必须专门在Cora图形上训练GNN，并使用该图形的标签进行推理。如果将训练好的模型应用于另一个图形，例如具有3703维特征和6个类别的Citeseer图形，将遇到一个难以克服的难题：如何让一个模型在不同的输入特征维度和不同类别数量下进行泛化？通常，预测头部是硬编码为固定的类别数。
- en: '[**GraphAny**](https://arxiv.org/abs/2405.20445) is, to the best of our knowledge,
    the first Graph FM where a single pre-trained model can perform node classification
    on any graph with any feature dimension and any number of classes. A single GraphAny
    model pre-trained on 120 nodes of the standard [Wisconsin](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.WebKB.html#torch_geometric.datasets.WebKB)
    dataset successfully generalizes to 30+ other graphs of different sizes and features
    and, on average, outperforms GCN and GAT graph neural network architectures trained
    from scratch on each of those graphs.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[**GraphAny**](https://arxiv.org/abs/2405.20445)是我们所知的第一个图形基础模型（Graph FM），该模型能够通过单一的预训练模型，在任何图形上执行节点分类，且支持任意特征维度和类别数量。一个在标准[威斯康辛](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.WebKB.html#torch_geometric.datasets.WebKB)数据集的120个节点上预训练的GraphAny模型，能够成功地推广到30多个不同大小和特征的图形，并且平均性能超越了从头开始在每个图形上训练的GCN和GAT图神经网络架构。'
- en: '![](../Images/e92e81c9cefb5768acd1e515dc448c7d.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e92e81c9cefb5768acd1e515dc448c7d.png)'
- en: 'Overview of GraphAny: LinearGNNs are used to perform non-parametric predictions
    and derive the entropy-normalized distance features. The final prediction is generated
    by fusing multiple LinearGNN predictions on each node with attention learned based
    on the distance features. Source: [Zhao et al](https://arxiv.org/abs/2405.20445).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: GraphAny概述：LinearGNNs用于执行非参数预测，并推导熵归一化的距离特征。最终预测通过融合每个节点上多个LinearGNN预测的结果，并根据距离特征学习的注意力生成。来源：[Zhao
    et al](https://arxiv.org/abs/2405.20445)。
- en: '**Setup:** Semi-supervised node classification: given a graph G, node features
    X, and a few labeled nodes from C classes, predict labels of target nodes (binary
    or multi-class classification). The dimension of node features and the number
    of unique classes are not fixed and are graph-dependent.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**设置：** 半监督节点分类：给定一个图G、节点特征X以及来自C个类的少量标记节点，预测目标节点的标签（二分类或多分类）。节点特征的维度和类别的数量不是固定的，取决于图。'
- en: '**What is transferable:** Instead of modeling a universal latent space for
    all possible graphs (which is quite cumbersome or maybe even practically impossible),
    GraphAny bypasses this problem and focuses on the *interactions between predictions
    of spectral filters*. Given a collection of high-pass and low-pass filters akin
    to [Simplified Graph Convolutions](https://arxiv.org/abs/1902.07153) (for instance,
    operations of the form AX and (I-A)X, dubbed “LinearGNNs” in the paper) and known
    node labels:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**什么是可迁移的：** GraphAny不是为所有可能的图建模一个通用的潜在空间（这相当繁琐，甚至可能在实际中不可行），而是绕过了这个问题，专注于*预测的谱滤波器之间的相互作用*。给定一组高通和低通滤波器，类似于[简化图卷积](https://arxiv.org/abs/1902.07153)（例如，形如AX和(I-A)X的操作，在论文中被称为“LinearGNNs”）以及已知的节点标签：'
- en: 0️⃣ GraphAny applies filters to all nodes;
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 0️⃣ GraphAny将滤波器应用于所有节点；
- en: 1️⃣ GraphAny obtains optimal weights for each predictor from nodes with known
    labels by solving a least squares optimization problem in closed form (optimal
    weights are expressed as a pseudoinverse);
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ GraphAny通过求解一个最小二乘优化问题，获得来自具有已知标签节点的每个预测器的最佳权重（最佳权重表示为伪逆）；
- en: 2️⃣ Applies the optimal weights to unknown nodes to get tentative prediction
    logits;
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 将最佳权重应用于未知节点，以获得初步预测对数值；
- en: 3️⃣ Computes pair-wise distances between those logits and applies entropy regularization
    (such that different graph- and feature sizes will not affect the distribution).
    For example, for 5 LinearGNNs, this would result in 5 x 4 = 20 combinations of
    logit scores;
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 3️⃣ 计算这些对数值之间的成对距离，并应用熵正则化（使得不同的图和特征大小不会影响分布）。例如，对于5个LinearGNNs，这将产生5 x 4 =
    20种对数分数的组合；
- en: 4️⃣ Learns the inductive attention matrix over those logits to weight the predictions
    most effectively (e.g., putting more attention to high-pass filters for heterophilic
    graphs).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 4️⃣ 学习这些对数值的归纳注意力矩阵，以最有效地加权预测（例如，给异质图的高通滤波器更多的关注）。
- en: In the end, the only learnable component in the model is the parameterization
    of attention (via MLP), which *does not depend* on the target number of unique
    classes, but only on the number of LinearGNNs used. In the same vein, all LinearGNN
    predictors are non-parametric, their updated node features and optimal weights
    can be pre-computed beforehand for faster inference.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，模型中唯一需要学习的部分是注意力的参数化（通过MLP），它*不依赖于*目标类别的数量，而仅仅依赖于使用的LinearGNNs的数量。类似地，所有LinearGNN预测器都是非参数的，它们更新后的节点特征和最佳权重可以预先计算，以加速推理。
- en: '**📚Read more in references [3]**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**📚详细信息见参考文献[3]**'
- en: 'Link Prediction: Not yet'
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 链接预测：尚未实现
- en: '**Setup**: given a graph G, with or without node features, predict whether
    a link exists between a pair of nodes (v1, v2)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**设置**：给定一个图G，是否有节点特征，预测一对节点（v1, v2）之间是否存在链接'
- en: 😢 For graphs with node features, we are not aware of any single transferable
    model for link prediction.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 😢 对于带有节点特征的图，我们尚未发现任何适用于链接预测的通用可迁移模型。
- en: For non-featurized graphs (or when you decide to omit node features deliberately),
    there is more to say — basically, all GNNs with a labeling trick *potentially*
    can transfer to new graphs thanks to the uniform node featurization strategy.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于没有特征的图（或当你决定故意省略节点特征时），有更多要说的——基本上，所有带标签技巧的GNN *有可能* 可以通过统一的节点特征化策略迁移到新图。
- en: It is known that in link prediction, the biggest hurdle is the presence of automorphic
    nodes (nodes that have the same structural roles) — vanilla GNNs assign them the
    same feature making two links (v1, v2) and (v1, v3) in the image below 👇 indistinguishable.
    [Labeling tricks](https://arxiv.org/abs/2010.16103) like [Double Radius Node Labeling](https://proceedings.neurips.cc/paper/2018/hash/53f0d7c537d99b3824f0f99d62ea2428-Abstract.html)
    or [Distance Encoding](https://proceedings.neurips.cc/paper_files/paper/2020/hash/2f73168bf3656f697507752ec592c437-Abstract.html)
    are such node featurization strategies that break automorphism symmetries.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，在链接预测中，最大的障碍是自同构节点的存在（具有相同结构角色的节点）——普通的GNN会将它们分配相同的特征，从而使得下图中的两个链接(v1,
    v2)和(v1, v3)变得不可区分。[标签技巧](https://arxiv.org/abs/2010.16103)如[双半径节点标签化](https://proceedings.neurips.cc/paper/2018/hash/53f0d7c537d99b3824f0f99d62ea2428-Abstract.html)或[距离编码](https://proceedings.neurips.cc/paper_files/paper/2020/hash/2f73168bf3656f697507752ec592c437-Abstract.html)是打破自同构对称性的节点特征化策略。
- en: '![](../Images/0c043afe3e726a494270ae3dab284874.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c043afe3e726a494270ae3dab284874.png)'
- en: 'V2 and v3 are automorphic nodes and standard GNNs score (v1,v2) and (v1,v3)
    equally. When we predict (v1, v2), we will label these two nodes differently from
    the rest, so that a GNN is aware of the target link when learning v1 and v2’s
    representations. Similarly, when predicting (v1, v3), nodes v1 and v3 will be
    labeled differently. This way, the representation of v2 in the left graph will
    be different from that of v3 in the right graph, enabling GNNs to distinguish
    the non-isomorphic links (v1, v2) and (v1, v3). Source: [Zhang et al](https://arxiv.org/abs/2010.16103).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: V2和v3是自同构节点，标准的GNN会对(v1,v2)和(v1,v3)打相同的分数。当我们预测(v1, v2)时，我们会将这两个节点与其他节点区分开来，使得GNN在学习v1和v2的表示时能识别出目标链接。类似地，当预测(v1,
    v3)时，节点v1和v3会被不同地标记。这样，左图中的v2的表示会与右图中v3的表示不同，从而使得GNN能够区分非同构的链接(v1, v2)和(v1, v3)。来源：[Zhang
    et al](https://arxiv.org/abs/2010.16103).
- en: Perhaps the only approach with a labeling trick (for non-featurized graphs)
    that was evaluated on link prediction on unseen graphs is [UniLP](https://arxiv.org/abs/2402.07738).
    UniLP is an in-context, contrastive learning model that requires a set of positive
    and negative samples for each target link to be predicted. Practically, UniLP
    uses [SEAL](https://proceedings.neurips.cc/paper/2018/hash/53f0d7c537d99b3824f0f99d62ea2428-Abstract.html)
    as a backbone GNN and learns an attention over a fixed number of positive and
    negative samples. On the other hand, SEAL is notoriously slow, so the first step
    towards making UniLP scale to large graphs is to replace subgraph mining with
    more efficient approaches like [ELPH](https://arxiv.org/abs/2209.15486) and [BUDDY](https://arxiv.org/abs/2209.15486).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 也许唯一在未见图上的链接预测中评估过的带标签技巧（适用于未特征化图）的方案是[UniLP](https://arxiv.org/abs/2402.07738)。UniLP是一种上下文对比学习模型，需要为每个目标链接预测提供一组正样本和负样本。实际上，UniLP使用[SEAL](https://proceedings.neurips.cc/paper/2018/hash/53f0d7c537d99b3824f0f99d62ea2428-Abstract.html)作为主干GNN，并对固定数量的正负样本进行学习注意力。另一方面，SEAL以其慢速著称，因此使UniLP能够扩展到大规模图的第一步是用更高效的方法替代子图挖掘，如[ELPH](https://arxiv.org/abs/2209.15486)和[BUDDY](https://arxiv.org/abs/2209.15486)。
- en: '![](../Images/d3b18da48a991a5b346bb00ba7c3706b.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d3b18da48a991a5b346bb00ba7c3706b.png)'
- en: 'Overview of the Universal Link Predictor framework. (a) For predicting a query
    link 𝑞, we initially sample positive (𝑠+) and negative (𝑠-) in-context links from
    the target graph. Both the query link and these in-context links are independently
    processed through a shared subgraph GNN encoder. An attention mechanism then calculates
    scores based on the similarity between the query link and the in-context links.
    (b) The final representation of the query link, contextualized by the target graph,
    is obtained through a weighted summation, which combines the representations of
    the in-context links with their respective labels. Source: [Dong et al.](https://arxiv.org/abs/2402.07738)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 通用链接预测框架概述。（a）为了预测查询链接𝑞，我们首先从目标图中采样正例（𝑠+）和负例（𝑠-）链接。这些查询链接和上下文链接都会通过共享的子图GNN编码器进行独立处理。接着，注意力机制会基于查询链接与上下文链接之间的相似性计算得分。（b）查询链接的最终表示，由目标图上下文化后，通过加权求和获得，该加权求和将上下文链接的表示与其相应的标签结合起来。来源：[Dong
    et al.](https://arxiv.org/abs/2402.07738)
- en: '**What is transferable:** structural patterns learned by labeling trick GNNs
    — it is proven that methods like [Neural Bellman-Ford](https://arxiv.org/abs/2106.06935)
    capture metrics over node pairs, eg, Personalized PageRank or Katz index (often
    used for link prediction).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**可转移的内容：** 通过标注技巧 GNN 学到的结构模式——已有研究证明，像 [Neural Bellman-Ford](https://arxiv.org/abs/2106.06935)
    这样的算法可以捕捉节点对的度量，例如个性化的 PageRank 或 Katz 指数（通常用于链接预测）。'
- en: Now, as we know how to deal with automorphisms, the only step towards a single
    graph FM for link prediction would be to add a support for heterogeneous node
    features — perhaps GraphAny-style approaches might be an inspiration?
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，既然我们知道如何处理自同构，朝着单一图 FM 进行链接预测的唯一步骤就是添加对异构节点特征的支持——或许类似 GraphAny 的方法可以作为启发？
- en: '**📚Read more in references [4][5][6][7]**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**📚更多内容请参见参考文献 [4][5][6][7]**'
- en: 'Knowledge Graph Reasoning: ULTRA and UltraQuery'
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 知识图谱推理：ULTRA 和 UltraQuery
- en: Knowledge graphs have graph-specific sets of entities and relations, e.g. common
    encyclopedia facts from Wikipedia / Wikidata or biomedical facts in Hetionet,
    those relations have different semantics and are not directly mappable to each
    other. For years, KG reasoning models were hardcoded to a given vocabulary of
    relations and could not transfer to new, unseen KGs with completely new entities
    and relations.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱具有特定于图的实体和关系集合，例如来自 Wikipedia / Wikidata 的常见百科事实，或来自 Hetionet 的生物医学事实，这些关系具有不同的语义，不能直接映射到彼此之间。多年来，KG
    推理模型都是硬编码到给定的关系词汇中，无法迁移到具有全新实体和关系的全新知识图谱。
- en: '[ULTRA](https://openreview.net/forum?id=jVEoydFOl9) is the first foundation
    model for KG reasoning that transfers to any KG at inference time in the zero-shot
    manner. That is, a single pre-trained model can run inference on any multi-relational
    graph with any size and entity/relation vocabulary. Averaged over 57 graphs, ULTRA
    significantly outperforms baselines trained specifically on each graph. Recently,
    ULTRA was extended to [UltraQuery](https://arxiv.org/abs/2404.07198) to support
    even more complex logical queries on graphs involving conjunctions, disjunctions,
    and negation operators. UltraQuery transfers to unseen graphs and 10+ complex
    query patterns on those unseen graphs outperforming much larger baselines trained
    from scratch.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[ULTRA](https://openreview.net/forum?id=jVEoydFOl9) 是第一个用于知识图谱推理的基础模型，它可以在推理时以零样本方式迁移到任何知识图谱上。也就是说，一个预训练模型可以对任何多关系图进行推理，无论其大小和实体/关系词汇如何。在
    57 个图上的平均表现表明，ULTRA 显著优于专门为每个图训练的基准模型。最近，ULTRA 被扩展为 [UltraQuery](https://arxiv.org/abs/2404.07198)，以支持图中涉及结合、析取和否定运算符的更复杂逻辑查询。UltraQuery
    可以迁移到未见过的图，并且在这些未见过的图上，超过 10 种复杂查询模式的表现优于从头训练的更大基准模型。'
- en: '![](../Images/816385f060a3fa6b50d11575547cdb80.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/816385f060a3fa6b50d11575547cdb80.png)'
- en: 'Given a query (Michael Jackson, genre, ?), ULTRA builds a graph of relations
    (edge types) to capture their interactions in the original graph conditioned on
    the query relation (genre) and derives relational representations from this smaller
    graph. Those features are then used as edge type features in the original bigger
    graph to answer the query. Source: [Galkin et al](https://openreview.net/forum?id=jVEoydFOl9).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 给定查询（Michael Jackson，genre，?），ULTRA 构建了一个关系图（边类型），以捕捉在原始图中基于查询关系（genre）的交互，并从这个较小的图中推导出关系表示。这些特征随后作为边类型特征，在原始的大图中用于回答查询。来源：[Galkin
    等人](https://openreview.net/forum?id=jVEoydFOl9)。
- en: '**Setup:** Given a multi-relational graph G with |E| nodes and |R| edge types,
    no node features, answer simple KG completion queries *(head, relation, ?)* or
    complex queries involving logical operators by returning a probability distribution
    over all nodes in the given graph. The set of nodes and relation types depends
    on the graph and can vary.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**设置：** 给定一个多关系图 G，其中有 |E| 个节点和 |R| 种边类型，且没有节点特征，回答简单的 KG 完成查询（*(head, relation,
    ?)*）或涉及逻辑运算符的复杂查询，通过返回给定图中所有节点的概率分布。节点和关系类型的集合取决于图，并且可能有所不同。'
- en: '**What is transferable:** ULTRA relies on modeling relational interactions.
    Forgetting about relation identities and target graph domain for a second, if
    we see that relations “authored” and “collaborated” can share the same starting
    node, and relations “student” and “coauthor” in another graph can share a starting
    node, then the relative, structural representations of those two pairs of relations
    might be similar. This holds for any multi-relational graph in any domain, be
    it encyclopedia or biomedical KGs. ULTRA goes further and captures 4 such “fundamental”
    interactions between relations. Those fundamental interactions are transferable
    to any KG (together with learned GNN weights) — this way, one single pre-trained
    model is ready for inference on any unseen graph and simple or complex reasoning
    query.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**什么是可转移的：** ULTRA依赖于对关系互动的建模。暂时不考虑关系标识和目标图谱领域，如果我们看到“作者”和“合作”这两个关系可以共享相同的起始节点，且另一个图谱中的“学生”和“共同作者”也能共享一个起始节点，那么这两个关系对的相对结构表示可能是相似的。这适用于任何领域的多关系图谱，无论是百科全书还是生物医学知识图谱。ULTRA进一步捕捉了4种这种“基础”关系互动。这些基础关系互动是可转移的，适用于任何知识图谱（连同学习到的GNN权重）——这样，一个经过预训练的模型就可以在任何未见过的图谱上进行推理，并处理简单或复杂的推理查询。'
- en: 'Read more in the dedicated Medium post:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在专门的Medium帖子中阅读更多内容：
- en: '[](/ultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09?source=post_page-----f363e2576f58--------------------------------)
    [## ULTRA: Foundation Models for Knowledge Graph Reasoning'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/ultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09?source=post_page-----f363e2576f58--------------------------------)
    [## ULTRA：知识图谱推理的基础模型'
- en: One model to rule them all
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个模型，统治一切
- en: towardsdatascience.com](/ultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09?source=post_page-----f363e2576f58--------------------------------)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/ultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09?source=post_page-----f363e2576f58--------------------------------)'
- en: '**📚Read more in references [8][9]**'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**📚在参考文献[8][9]中阅读更多内容**'
- en: 'Algorithmic Reasoning: Generalist Algorithmic Learner'
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法推理：通用算法学习器
- en: '![](../Images/d719d30af605e44657fe13140067cb22.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d719d30af605e44657fe13140067cb22.png)'
- en: 'A generalist neural algorithmic learner is a single processor GNN P, with a
    single set of weights, capable of solving several algorithmic tasks in a shared
    latent space (each of which is attached to P with simple encoders/decoders f and
    g). Among others, the processor network is capable of sorting (top), shortest
    path-finding (middle), and convex hull finding (bottom). Source: [Ibarz et al.](https://proceedings.mlr.press/v198/ibarz22a/ibarz22a.pdf)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一种通用神经算法学习器是一个单处理器GNN P，具有一组权重，能够在共享潜在空间中解决多种算法任务（每个任务通过简单的编码器/解码器f和g附加到P）。其中，该处理器网络能够进行排序（上）、最短路径查找（中）和凸包查找（下）。来源：[Ibarz
    et al.](https://proceedings.mlr.press/v198/ibarz22a/ibarz22a.pdf)
- en: '**Setup:** [Neural algorithmic reasoning](https://arxiv.org/abs/2105.02761)
    (NAR) studies the execution of standard algorithms (eg, sorting, searching, dynamic
    programming) in the latent space and generalization to the inputs of arbitrary
    size. A lot of such algorithms can be represented with a graph input and pointers.
    Given a graph G with node and edge features, the task is to simulate the algorithm
    and produce the correct output. Optionally, you can get access to hints — time
    series of intermediate states of the algorithm which can act as the intermediate
    supervised signal. Obviously, different algorithms require a different number
    of steps to execute, so length is not fixed here.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**设置：** [神经算法推理](https://arxiv.org/abs/2105.02761)（NAR）研究在潜在空间中执行标准算法（如排序、搜索、动态规划）并推广到任意大小输入的过程。许多此类算法可以通过图谱输入和指针来表示。给定一个包含节点和边特征的图G，任务是模拟算法并产生正确的输出。可选地，您可以访问提示——算法的中间状态时间序列，这些可以作为中间监督信号。显然，不同的算法需要不同的步骤来执行，因此步骤数在这里不是固定的。'
- en: '**What is transferable:** Homogeneous feature space and similar control flow
    for similar algorithms. For instance, Prim’s and Dijkstra’s algorithms share a
    similar structure, differing only in the choice of key function and edge relaxation
    subroutine. Besides, there are [several](https://arxiv.org/abs/1905.13211) [proofs](https://arxiv.org/abs/2203.15544)
    of a direct alignment between message passing and dynamic programming. This is
    the main motivation behind one “processor” neural network that updates latent
    states for all considered algorithms ([30 classic algos](https://github.com/google-deepmind/clrs)
    from the CLRS book).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**可转移的内容**：同质的特征空间和相似的控制流用于相似的算法。例如，Prim算法和Dijkstra算法共享相似的结构，仅在关键函数的选择和边缘松弛子程序上有所不同。此外，已有[几篇](https://arxiv.org/abs/1905.13211)
    [证明](https://arxiv.org/abs/2203.15544)表明消息传递与动态规划之间存在直接的对齐关系。这正是“处理器”神经网络的主要动机，它更新所有考虑中的算法的潜在状态（[CLRS书中的30个经典算法](https://github.com/google-deepmind/clrs)）。'
- en: '[Triplet-GMPNN](https://proceedings.mlr.press/v198/ibarz22a/ibarz22a.pdf) was
    the first such universal processor neural net (by 2024 it became rather standard
    in the NAR literature) — it is a GNN that operates on triples of nodes and their
    features (akin to [Edge Transformers](https://arxiv.org/abs/2112.00578) and triangular
    attention in AlphaFold). The model is trained in the multi-task mode on all algorithmic
    tasks in the benchmark with a handful of optimization and tricks. A single model
    bumps the average performance on 30 tasks by over 20% (in absolute numbers) compared
    to single-task specialist models.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[Triplet-GMPNN](https://proceedings.mlr.press/v198/ibarz22a/ibarz22a.pdf) 是第一个此类通用处理器神经网络（到2024年，它在NAR文献中已经成为相当标准的做法）——它是一个操作节点三元组及其特征的图神经网络（类似于[边变换器](https://arxiv.org/abs/2112.00578)和AlphaFold中的三角注意力）。该模型在基准测试的所有算法任务上以多任务模式进行训练，并采用了一些优化和技巧。与单任务专用模型相比，单一模型使得30个任务的平均表现提高了20%以上（绝对数值）。'
- en: Still, encoders and decoders are parameterized specifically for each task —
    one of the ways to unify the input and output formats might as well be text with
    LLM processors as done in the recent [text version of CLRS](https://arxiv.org/abs/2406.04229).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，编码器和解码器依然是针对每个任务特定参数化的——统一输入和输出格式的一种方式可能是使用文本与LLM处理器，如最近[CLRS文本版](https://arxiv.org/abs/2406.04229)所做的那样。
- en: '![](../Images/35f10005a7020cbd88a744051d639e8e.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/35f10005a7020cbd88a744051d639e8e.png)'
- en: '**Top**: The graph algorithmic trace of insertion sorting a list *[5, 2, 4,
    3, 1]* in graph form. **Bottom**: The same algorithmic trace, represented textually,
    by using the CLRS-Text generator. The model receives as input (depicted in green)
    the input array (key) and the initial value of the sorting trace (initial_trace),
    using which it is prompted to predict the trace (depicted in blue) of gradually
    sorting the list, by inserting one element at a time into a partially sorted list,
    from left to right. At the end, the model needs to output the final sorted array
    (depicted in red), and it is evaluated on whether this array is predicted correctly.
    Source: [Markeeva, McLeish, Ibarz, et al.](https://arxiv.org/abs/2406.04229)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**顶部**：插入排序一个列表 *[5, 2, 4, 3, 1]* 的图形算法过程。**底部**：相同算法过程，以文本方式表示，通过使用CLRS-Text生成器。模型接收的输入（以绿色表示）是输入数组（键）和排序过程的初始值（initial_trace），并以此为提示预测过程（以蓝色表示），即通过将每次一个元素插入到部分排序的列表中，逐步对列表进行排序，从左到右。最后，模型需要输出最终排序好的数组（以红色表示），并评估是否正确预测了该数组。来源：[Markeeva,
    McLeish, Ibarz 等](https://arxiv.org/abs/2406.04229)'
- en: 'Perhaps the most interesting question of 2024 and 2025 in NAR is:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 2024年和2025年NAR中或许最有趣的问题是：
- en: '*Can algorithmic reasoning ideas for OOD generalization be the key to generalizable
    LLM reasoning?*'
  id: totrans-86
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*算法推理思想能否成为OOD泛化中LLM推理的关键？*'
- en: LLMs notoriously struggle with complex reasoning problems, dozens of papers
    appear on arxiv every month trying a new prompting method to bump benchmarking
    performance another percentage-or-two, but most of them do not transfer across
    tasks of similar graph structures (see the example below). There is a need for
    more principled approaches and NAR has the potential to fill this gap!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: LLM在处理复杂推理问题时通常表现不佳，每个月arxiv上都会出现数十篇论文，尝试通过一种新的提示方法提高基准性能百分之一或两，但大多数方法在处理相似图结构的任务时并没有转移成功（见下例）。迫切需要更有原则性的方法，而NAR有可能填补这一空白！
- en: '![](../Images/f8df0df88abfbd23a7ca070c14f81293.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8df0df88abfbd23a7ca070c14f81293.png)'
- en: Failure of LLMs on reasoning problems with similar graph structures. Image by
    Authors.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: LLM在处理具有相似图结构的推理问题时失败。图片来源：作者。
- en: '**📚Read more in references [10][11]**'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**📚更多内容见参考文献[10][11]**'
- en: Geometric and AI4Science Foundation Models
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 几何学和AI4Science基础模型
- en: In the world of Geometric Deep Learning and scientific applications, foundation
    models are becoming prevalent as universal ML potentials, protein language models,
    and universal molecular property predictors. Although the universal vocabulary
    exists in most such cases (e.g., atom types in small molecules or amino acids
    in proteins) and we do not have to think about universal featurization, the main
    complexity lies in the real-world physical nature of atomistic objects — they
    have pronounced 3D structure and properties (like energy), which have theoretical
    justifications rooted in chemistry, physics, and quantum mechanics.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在几何深度学习和科学应用领域，基础模型正逐渐成为通用ML势能、蛋白质语言模型和通用分子性质预测器。虽然在大多数这种情况下，通用词汇已经存在（例如，小分子中的原子类型或蛋白质中的氨基酸），且我们不需要思考通用特征化，但主要的复杂性在于原子对象的真实物理特性——它们具有明显的三维结构和性质（如能量），这些性质有理论依据，根植于化学、物理学和量子力学。
- en: 'ML Potentials: JMP-1, DPA-2 for molecules, MACE-MP-0 and MatterSim for inorganic
    crystals'
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ML势能：JMP-1、DPA-2用于分子，MACE-MP-0和MatterSim用于无机晶体
- en: '**Setup**: given a 3D structure, predict the energy of the structure and per-atom
    forces;'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**设置**：给定一个三维结构，预测该结构的能量和每个原子的力；'
- en: '**What is transferable**: a vocabulary of atoms from the periodic table.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**可转移的内容**：来自元素周期表的原子词汇。'
- en: ML potentials estimate the potential energy of a chemical compound — like molecules
    or periodic crystals — given their 3D coordinates and optional input (like periodic
    boundary conditions for crystals). For any atomistic model, the vocabulary of
    possible atoms is always bound by the [Periodic Table](https://en.wikipedia.org/wiki/Periodic_table)
    which currently includes 118 elements. The “foundational” aspect in ML potentials
    is to generalize to any atomistic structure (there can be combinatorially many)
    and be stable enough to be used in molecular dynamics (MD), drug- and materials
    discovery pipelines.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ML势能通过给定化学化合物的三维坐标和可选输入（例如晶体的周期性边界条件）来估算其潜在能量，化学化合物可以是分子或周期性晶体。对于任何原子模型，可能的原子种类的词汇总是受到[元素周期表](https://en.wikipedia.org/wiki/Periodic_table)的约束，目前该周期表包含118种元素。ML势能的“基础性”方面是将其推广到任何原子结构（可能有组合形式多样的结构），并且足够稳定，以便在分子动力学（MD）、药物发现和材料发现流程中使用。
- en: '[JMP-1](https://arxiv.org/abs/2310.16802) and [DPA-2](https://arxiv.org/abs/2312.15492)
    released around the same time aim to be such universal ML potential models — they
    are trained on a sheer variety of structures — from organic molecules to crystals
    to MD trajectories. For example, a single pre-trained JMP-1 excels at QM9, rMD17
    for small molecules, MatBench and QMOF on crystals, and MD22, SPICE on large molecules
    being on-par or better than specialized per-dataset models. Similarly, [MACE-MP-0](https://arxiv.org/abs/2401.00096)
    and [MatterSim](https://arxiv.org/abs/2405.04967) are the most advanced FMs for
    inorganic crystals (MACE-MP-0 is already available with weights) evaluated on
    20+ crystal tasks ranging from multicomponent alloys to combustion and molten
    salts. Equivariant GNNs are at the heart of those systems helping to process equivariant
    features (Cartesian coordinates) and invariant features (like atom types).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[JMP-1](https://arxiv.org/abs/2310.16802)和[DPA-2](https://arxiv.org/abs/2312.15492)大致同时发布，旨在成为这样的通用ML势能模型——它们在多种结构上进行训练——从有机分子到晶体，再到MD轨迹。例如，单个预训练的JMP-1在QM9、rMD17（小分子）、MatBench和QMOF（晶体）、以及MD22、SPICE（大分子）等任务上表现出色，且与专门针对特定数据集的模型相当或更优。类似地，[MACE-MP-0](https://arxiv.org/abs/2401.00096)和[MatterSim](https://arxiv.org/abs/2405.04967)是无机晶体领域最先进的基础模型（MACE-MP-0已经有权重可用），在20多个晶体任务上进行了评估，任务涵盖了从多组分合金到燃烧和熔融盐的各类任务。等变图神经网络（GNNs）是这些系统的核心，帮助处理等变特征（如笛卡尔坐标）和不变特征（如原子类型）。'
- en: '![](../Images/fa55ad6dff257dc117eefc32cb637976.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa55ad6dff257dc117eefc32cb637976.png)'
- en: 'Sources: (1) Pre-training and fine-tuning of **JMP-1** for molecules and crystals,
    [Shoghi et al](https://arxiv.org/abs/2310.16802) (2) **MACE-MP-0** is trained
    only on the Materials Project data and transfers to molecular dynamics simulation
    across a wide variety of chemistries in the solid, liquid and gaseous phases,
    [Batatia, Benner, Chiang, Elena, Kovács, Riebesell et al](https://arxiv.org/abs/2401.00096).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：（1）对**JMP-1**进行分子和晶体的预训练与微调，[Shoghi et al](https://arxiv.org/abs/2310.16802)（2）**MACE-MP-0**仅在材料项目数据上训练，并在固态、液态和气态下的多种化学体系中进行分子动力学模拟，[Batatia,
    Benner, Chiang, Elena, Kovács, Riebesell et al](https://arxiv.org/abs/2401.00096)。
- en: The next frontier seems to be ML-accelerated molecular dynamics simulations
    — traditional computational methods work at the femtosecond scale (10–15) and
    require millions and billions of steps to simulate a molecule, crystal, or protein.
    Speeding up such computations would have an immense scientific impact.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个前沿似乎是机器学习加速的分子动力学模拟——传统的计算方法在飞秒尺度（10^-15秒）下工作，并且需要数百万到数十亿步才能模拟一个分子、晶体或蛋白质。加速这些计算将对科学产生巨大的影响。
- en: '**📚Read more in references [12][13][14][15]**'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**📚更多内容请参考文献 [12][13][14][15]**'
- en: 'Protein LMs: ESM-2'
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蛋白质语言模型：ESM-2
- en: '**Setup**: given a protein sequence, predict the masked tokens akin to masked
    language modeling;'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**设置**：给定一个蛋白质序列，预测被掩码的标记，类似于掩码语言建模；'
- en: '**What is transferable**: a vocabulary of 20 (22) amino acids.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**可转移的内容**：一个包含20（22）种氨基酸的词汇表。'
- en: Protein sequences resemble natural language with amino acids as tokens, and
    Transformers excel at encoding sequence data. Although the vocabulary of amino
    acids is relatively small, the space of possible proteins is enormous, so training
    on large volumes of known proteins might hint at the properties of unseen combinations.
    [ESM-2](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1) is perhaps
    the most popular protein LM thanks to the pre-training data size, a variety of
    available checkpoints, and informative features.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 蛋白质序列类似于自然语言，其中氨基酸是标记，而Transformer在编码序列数据方面表现优异。尽管氨基酸的词汇表相对较小，但可能的蛋白质空间却极为广阔，因此在大量已知蛋白质上进行训练，可能会提示看不见的组合的特性。[ESM-2](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1)也许是最流行的蛋白质语言模型，这得益于预训练数据的规模、各种可用的检查点和信息丰富的特征。
- en: '![](../Images/3796a202aa30508022eba58d8a368413.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3796a202aa30508022eba58d8a368413.png)'
- en: 'ESM2 as a masked LM and ESMFold for protein structure prediction. Source: [Lin,
    Akin, Rao, Hie, et al.](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ESM2作为一个掩码语言模型和ESMFold用于蛋白质结构预测。来源：[Lin, Akin, Rao, Hie, et al.](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1)
- en: ESM features are used in countless applications from predicting 3D structure
    (in [ESMFold](https://github.com/facebookresearch/esm)) to protein-ligand binding
    ([DiffDock](https://arxiv.org/abs/2210.01776) and its descendants) to protein
    structure generative models (like a recent [FoldFlow 2](https://www.dreamfold.ai/blog/foldflow-2)).
    Bigger transformers and more data are likely to increase protein LMs’ performance
    even further — at this scale, however, the data question becomes more prevalent
    (we also discuss the interplay between architecture and data in the dedicated
    section), eg, the [ESM Metagenomic Atlas](https://esmatlas.com/) already encodes
    700M+ structures including those seen outside humans in the soil, oceans, or hydrothermal
    vents. Is there a way to trillions of tokens as in common LLM training datasets?
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ESM特征被应用于无数领域，从预测3D结构（在[ESMFold](https://github.com/facebookresearch/esm)中）到蛋白质-配体结合（[DiffDock](https://arxiv.org/abs/2210.01776)及其后续模型），再到蛋白质结构生成模型（如最近的[FoldFlow
    2](https://www.dreamfold.ai/blog/foldflow-2)）。更大的Transformer和更多的数据可能会进一步提高蛋白质语言模型的性能——然而，在这个规模下，数据问题变得更加突出（我们也在专门的部分讨论了架构与数据之间的相互作用），例如，[ESM宏基因组图谱](https://esmatlas.com/)已经编码了超过7亿个结构，包括在人类之外的土壤、海洋或热泉中见到的结构。是否有办法像常见的LLM训练数据集那样，处理数万亿个标记？
- en: '**📚Read more in references [16][17]**'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**📚更多内容请参考文献 [16][17]**'
- en: '2D Molecules: MiniMol and MolGPS'
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2D分子：MiniMol和MolGPS
- en: '**Setup**: given a 2D graph structure with atom types and bond types, predict
    molecular properties'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**设置**：给定一个包含原子类型和键类型的2D图结构，预测分子特性'
- en: '**What is transferable**: a vocabulary of atoms from the periodic table and
    bond types'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**可转移的内容**：周期表中的原子词汇和键类型'
- en: With 2D graphs (without 3D atom coordinates) universal encoding and transferability
    come from a fixed vocabulary of atom and bond types which you can send to any
    GNN or Transformer encoder. Although molecular fingerprints have been used since
    1960s ([Morgan fingerprints](https://pubs.acs.org/doi/abs/10.1021/c160017a018)
    [18]), their primary goal was to evaluate similarity, not to model a latent space.
    The task of a single (large) neural encoder is to learn useful representations
    that might hint at certain physical molecular properties.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用2D图形（没有3D原子坐标）时，通用编码和可转移性来自于一个固定的原子和键类型词汇表，你可以将其发送到任何GNN或Transformer编码器中。尽管分子指纹自1960年代以来就已被使用（[摩根指纹](https://pubs.acs.org/doi/abs/10.1021/c160017a018)
    [18]），它们的主要目标是评估相似性，而不是建模潜在空间。单个（大型）神经编码器的任务是学习有用的表示，这些表示可能暗示某些物理分子特性。
- en: Recent examples of generalist models for learning molecular representations
    are [MiniMol](https://arxiv.org/pdf/2404.14986) and [MolGPS](https://arxiv.org/abs/2404.11568v1)
    which have been trained on a large corpus of molecular graphs and probed on dozens
    of downstream tasks. That said, you still need to fine-tune a separate task-specific
    decoder / predictor given the models’ representations — in that sense, one single
    pre-trained model will not be able to run zero-shot inference on all possible
    unseen tasks, rather on those for which decoders have been trained. Fine-tuning
    is still a good cheap option though since those models are orders of magnitude
    smaller than LLMs.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 近期关于学习分子表示的通用模型示例包括[MiniMol](https://arxiv.org/pdf/2404.14986)和[MolGPS](https://arxiv.org/abs/2404.11568v1)，这些模型已经在大量分子图上进行了训练，并在数十个下游任务中进行了测试。也就是说，尽管如此，你仍然需要根据模型的表示来微调一个单独的任务特定解码器/预测器——从这个意义上讲，一个单一的预训练模型不能对所有可能的未见任务进行零-shot
    推理，而只能针对已经训练了解码器的任务进行推理。不过，微调仍然是一个便宜且有效的选择，因为这些模型的规模比大语言模型（LLMs）小几个数量级。
- en: '![](../Images/4ecb1c3dab87648d36a6710602ab0524.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ecb1c3dab87648d36a6710602ab0524.png)'
- en: 'Source: (1) Workflow overview of the [MiniMol](https://arxiv.org/pdf/2404.14986)
    pre-training and downstream task evaluation. (2) Criteria of the scaling study
    of [MolGPS](https://arxiv.org/abs/2404.11568v1)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：（1）[MiniMol](https://arxiv.org/pdf/2404.14986)的预训练和下游任务评估工作流概述。（2）[MolGPS](https://arxiv.org/abs/2404.11568v1)扩展性研究的标准。
- en: '**📚Read more in references [19][20]**'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**📚查看更多参考文献 [19][20]**'
- en: 'Expressivity & Scaling Laws: Do Graph FMs scale?'
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 表达性与扩展性定律：图形 FM 是否具备扩展性？
- en: Transformers in LLMs and multi-modal frontier models are rather standard and
    we know some basic scaling principles for them. Do transformers (as an architecture,
    not LLMs) work equally well on graphs? What are the general challenges when designing
    a backbone for Graph FMs?
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在大语言模型（LLMs）和多模态前沿模型中，变换器是相对标准的，并且我们已经了解了它们的一些基本扩展原理。那么，变换器（作为一种架构，而不是 LLMs）在图上能否同样有效？在设计图形
    FM 的主干时，通常面临哪些挑战？
- en: 'If you categorize the models highlighted in the previous sections, only 2 areas
    feature transformers — protein LMs (ESM) with a natural sequential bias and small
    molecules (MolGPS). The rest are GNNs. There are several reasons for that:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对前面几节中提到的模型进行分类，只有两个领域使用了变换器——蛋白质语言模型（ESM）具有自然的序列偏向，和小分子（MolGPS）。其余的都是图神经网络（GNNs）。之所以如此，有几个原因：
- en: Vanilla transformers do not scale to any reasonably large graph larger than
    a standard context length (>4–10k nodes). Anything above that range requires tricks
    like feeding only subgraphs (losing the whole graph structure and long-range dependencies)
    or linear attention (that might not have good scaling properties). In contrast,
    GNNs are linear in the number of edges, and, in the case of sparse graphs (V ~
    E), are linear in the number of nodes.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 普通变换器无法扩展到任何大于标准上下文长度的合理大图（>4–10k 个节点）。超过这个范围就需要一些技巧，例如只输入子图（失去整个图的结构和长距离依赖）或线性注意力（可能没有良好的扩展性）。相反，GNNs
    在边的数量上是线性的，并且在稀疏图（V ~ E）的情况下，节点的数量也是线性的。
- en: Vanilla transformers without positional encodings are [less expressive than
    GNNs](https://arxiv.org/abs/2302.04181). Mining positional encodings like Laplacian
    PEs on a graph with V nodes is O(V³).
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有位置编码的普通变换器[比 GNNs 更不具备表达性](https://arxiv.org/abs/2302.04181)。在包含 V 个节点的图上挖掘位置编码（如拉普拉斯位置编码）是
    O(V³)。
- en: What should be a “token” when encoding graphs via transformers? There is no
    clear winner in the literature, e.g., [nodes](https://arxiv.org/abs/2106.05234),
    [nodes + edges](https://arxiv.org/abs/2406.03148), or [subgraphs](https://arxiv.org/abs/2212.13350)
    are all viable option
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在通过变换器编码图时，什么应当成为“token”？文献中并没有明确的结论，例如，[节点](https://arxiv.org/abs/2106.05234)、[节点
    + 边](https://arxiv.org/abs/2406.03148)或[子图](https://arxiv.org/abs/2212.13350)都是可行的选择。
- en: '**➡️** Touching upon **expressivity**, different graph tasks need to deal with
    different symmetries, e.g., automorphic nodes in link prediction lead to indistinguishable
    representations, whereas in graph classification/regression going beyond 1-WL
    is necessary for distinguishing molecules which otherwise might look isomorphic
    to vanilla GNNs.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**➡️** 说到**表达性**，不同的图任务需要处理不同的对称性，例如，链接预测中的自同构节点会导致无法区分的表示，而在图分类/回归中，超越 1-WL
    是区分分子所必需的，否则这些分子可能看起来与普通的图神经网络（GNNs）相同。'
- en: '![](../Images/90096ce16fd35189294dadd4da0ca661.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90096ce16fd35189294dadd4da0ca661.png)'
- en: 'Different tasks need to deal with different symmetries. Image by Authors. Sources
    of graphs: (1) [Zhang et al](https://arxiv.org/abs/2010.16103), (2) [Morris et
    al](https://arxiv.org/abs/2112.09992)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的任务需要处理不同的对称性。图像作者提供。图的来源：（1）[Zhang等人](https://arxiv.org/abs/2010.16103)，（2）[Morris等人](https://arxiv.org/abs/2112.09992)
- en: 'This fact begs two questions:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这一事实引出了两个问题：
- en: '*How expressive should GFMs be? What is the trade-off between expressivity
    and scalability?*'
  id: totrans-128
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*GFM在表达上应该有多表达性？表达性和可扩展性之间的权衡在哪里？*'
- en: Ideally, we want a single model to resolve all those symmetries equally well.
    However, more expressive models would lead to more computationally expensive architectures
    both in training and inference. We agree with the recent [ICML’24 position paper
    on the future directions in Graph ML theory](https://arxiv.org/abs/2402.02287)
    that the community should seek the balance between expressivity, generalization,
    and optimization.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们希望单一模型能够同样有效地解决所有这些对称性。然而，更具表现力的模型将导致在训练和推理中更昂贵的架构。我们同意最近的[ICML'24图机器学习理论未来方向的立场论文](https://arxiv.org/abs/2402.02287)，即社区应该在表达性、泛化性和优化之间寻求平衡。
- en: Still, it is worth noting that with the growing availability of training data,
    it might be a computationally cheaper idea to defer learning complex symmetries
    and invariances directly from the data (instead of baking them into a model).
    A few recent good examples of this thesis are [AlphaFold 3](https://www.nature.com/articles/s41586-024-07487-w)
    and [Molecular Conformer Fields](https://arxiv.org/abs/2311.17932) that reach
    SOTA in many generative applications *without* expensive equivariant geometric
    encoders.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，值得注意的是，随着训练数据的日益增多，推迟直接从数据中学习复杂的对称性和不变性可能是一个计算上更便宜的想法（而不是将它们整合到模型中）。这个论点的一些最新良好例子是[AlphaFold
    3](https://www.nature.com/articles/s41586-024-07487-w)和[Molecular Conformer Fields](https://arxiv.org/abs/2311.17932)，在许多生成应用中达到了SOTA，*而无需*昂贵的等变几何编码器。
- en: '**📚Read more in references [21]**'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**📚 在参考文献 [21] 中阅读更多**'
- en: '**➡️** When it comes to **scaling**, both model and data should be scaled up.
    However:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**➡️** 在涉及**扩展**时，模型和数据都应该扩展。然而：'
- en: '❌ Non-geometric graphs: There is no principled study on scaling GNNs or Transformers
    to large graphs and common tasks like node classification and link prediction.
    2-layer GraphSAGE is often not very far away from huge 16-layer graph transformers.
    In a similar trend, in the KG reasoning domain, a single ULTRA model (discussed
    above) with <200k parameters outperforms million-sized shallow embedding models
    on 50+ graphs. Why is it happening? We’d hypothesize the crux is in 1️⃣ task nature
    — most of non-geometric graphs are noisy similarity graphs that are not bounded
    to a concrete physical phenomenon like molecules 2️⃣ Given rich node and edge
    features, models have to learn *representations of graph structures* (common for
    link prediction) or just *functions over given features* (a good example is [node
    classification in OGB](https://ogb.stanford.edu/docs/leader_nodeprop/) where most
    gains are achieved by adding an LLM feature encoder).'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❌ 非几何图：没有关于将GNN或变压器扩展到大图和常见任务（如节点分类和链接预测）的原则性研究。2层GraphSAGE通常与庞大的16层图变换器相距不远。在KG推理领域中，一个单一的ULTRA模型（如上所述）以小于20万个参数优于50多个图上百万大小的浅嵌入模型。为什么会发生这种情况？我们推测关键在于1️⃣任务性质
    — 大多数非几何图是嘈杂的相似性图，并不局限于具体的物理现象如分子；2️⃣鉴于丰富的节点和边缘特征，模型必须学习*图结构的表示*（用于链接预测的常见特征）或仅*在给定特征上执行功能*（一个很好的例子是[OGB中的节点分类](https://ogb.stanford.edu/docs/leader_nodeprop/)，大部分收益是通过添加LLM特征编码器实现的）。
- en: '✅ Geometric graphs: There are several recent works focusing on molecular graphs:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ✅ 几何图：有几篇近期的作品专注于分子图：
- en: '[Frey et al](https://www.nature.com/articles/s42256-023-00740-3) (2023) study
    scaling of geometric GNNs for ML potentials;'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Frey等人](https://www.nature.com/articles/s42256-023-00740-3)（2023年）研究了用于ML潜力的几何GNN的扩展；'
- en: '[Sypetkowski, Wenkel et al](https://arxiv.org/abs/2404.11568v1) (2024) introduce
    MolGPS and study scaling MPNNs and Graph Transformers up to 1B parameters on the
    large dataset of 5M molecules'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Sypetkowski, Wenkel等人](https://arxiv.org/abs/2404.11568v1)（2024年）介绍了MolGPS，并研究了在包含500万分子的大数据集上将MPNNs和图变换器扩展到了10亿参数的问题。'
- en: '[Liu et al](https://arxiv.org/abs/2402.02054) (2024) probe GCN, GIN, and GraphGPS
    up to 100M parameters on molecular datasets up to 4M molecules.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Liu等人](https://arxiv.org/abs/2402.02054)（2024年）探索了在包含4百万分子的分子数据集上将GCN、GIN和GraphGPS扩展到1亿参数的问题。'
- en: '![](../Images/73d40bef726833d6ae93a9e9c4bff79b.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/73d40bef726833d6ae93a9e9c4bff79b.png)'
- en: 'Scaling molecular GNNs and GTs. Sources: (1) [Sypetkowski, Wenkel et al](https://arxiv.org/abs/2404.11568v1),
    (2) [Liu et al](https://arxiv.org/abs/2402.02054)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展分子GNN和GT。来源：(1) [Sypetkowski, Wenkel 等人](https://arxiv.org/abs/2404.11568v1)，(2)
    [Liu 等人](https://arxiv.org/abs/2402.02054)
- en: 'The Data Question: What should be scaled? Is there enough graph data to train
    Graph FMs?'
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据问题：应该扩展什么？是否有足够的图数据来训练图形FM？
- en: 1️⃣ **What should be scaled in graph data?** Nodes? Edges? The number of graphs?
    Something else?
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ **图数据中应该扩展什么？** 节点？边？图的数量？还是其他东西？
- en: There is no clear winner in the literature, we would rather gravitate towards
    a broader term ***diversity***, that is, a diversity of patterns in the graph
    data. For example, in node classification on large product graphs, it likely would
    not matter much if you train on a graph with 100M nodes or 10B nodes since it’s
    the same nature of a user-item graph. However, showing examples with homophily
    and heterophily on different scales and sparsities might be quite beneficial.
    In **GraphAny**, showing examples of such graphs allowed to build a robust node
    classifier that generalizes to different graph distributions,
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中没有明确的赢家，我们更倾向于采用一个更广泛的术语***多样性***，即图数据中模式的多样性。例如，在大型产品图上的节点分类任务中，如果你在一个有1亿个节点的图上训练，还是在一个有100亿个节点的图上训练，可能没有太大区别，因为它们本质上都是用户-物品图。然而，展示在不同尺度和稀疏度下的同类性和异类性的例子可能会非常有益。在**GraphAny**中，展示这种图形的例子有助于构建一个能够泛化到不同图分布的稳健节点分类器。
- en: In KG reasoning with **ULTRA**, it was found that the ***diversity of relational
    patterns*** in pre-training plays the biggest role in inductive generalization,
    e.g., one large dense graph is worse than a collection of smaller but sparse,
    dense, few-relational, and many-relational graphs.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用**ULTRA**进行知识图谱推理时，发现预训练中的***关系模式多样性***在归纳泛化中起到了最大作用，例如，一个大的密集图比一组较小但稀疏、密集、少关系和多关系的图差。
- en: In molecular graph-level tasks, e.g., in **MolGPS**, scaling the number of unique
    molecules with different physical properties helps a lot (as shown in the charts
    above 👆).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在分子图级别的任务中，例如在**MolGPS**中，增加具有不同物理属性的独特分子的数量有很大帮助（如上图所示 👆）。
- en: Besides, [UniAug](https://arxiv.org/abs/2406.01899) finds that increased coverage
    of the structural patterns in pre-training data adds to the performance across
    different downstream tasks from various domains.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，[UniAug](https://arxiv.org/abs/2406.01899)发现，在预训练数据中增加结构模式的覆盖率可以提高不同下游任务的性能，这些任务来自不同领域。
- en: '**2️⃣ Is there enough data to train Graph FMs?**'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**2️⃣ 是否有足够的数据来训练图形FM？**'
- en: Openly available graph data is orders of magnitudes smaller than natural language
    tokens or images or videos, and it is fine. This very article includes thousands
    of language and image tokens and no explicit graphs (unless you try to parse this
    text to a graph like an [abstract meaning representation](https://en.wikipedia.org/wiki/Abstract_Meaning_Representation)
    graph). The number of ‘good’ proteins with known structures in PDB is small, the
    number of known ‘good’ molecules for drugs is small.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 开放可用的图数据的规模比自然语言标记、图像或视频小几个数量级，这完全没问题。本文就包含了成千上万的语言和图像标记，但没有明确的图形（除非你试图将这段文本解析成像[抽象意义表示](https://en.wikipedia.org/wiki/Abstract_Meaning_Representation)这样的图）。在PDB中，已知结构的“好”蛋白质数量很少，已知用于药物的“好”分子数量也很少。
- en: Are Graph FMs doomed because of data scarcity?
  id: totrans-148
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 图形FM是否注定因为数据稀缺而失败？
- en: 'Well, not really. The two open avenues are: (1) more sample-efficient architectures;
    (2) using more black-box and synthetic data.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，实际上并没有。两个开放的方向是：(1) 更具样本效率的架构；(2) 使用更多的黑箱数据和合成数据。
- en: Synthetic benchmarks like [GraphWorld](https://arxiv.org/abs/2203.00112) might
    be of use to increase the diversity of training data and improve generalization
    to real-world datasets. Black-box data obtained from scientific experiments, in
    turn, is likely to become the key factor in building successful foundation models
    in AI 4 Science — those who master it will prevail on the market.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 合成基准如[GraphWorld](https://arxiv.org/abs/2203.00112)可能对增加训练数据的多样性和提高对真实世界数据集的泛化有帮助。而从科学实验中获得的黑箱数据，反过来可能成为构建AI
    4 Science基础模型的关键因素——掌握它的人将会在市场中占据主导地位。
- en: '[](/the-road-to-biology-2-0-will-pass-through-black-box-data-bbd00fabf959?source=post_page-----f363e2576f58--------------------------------)
    [## The Road to Biology 2.0 Will Pass Through Black-Box Data'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/the-road-to-biology-2-0-will-pass-through-black-box-data-bbd00fabf959?source=post_page-----f363e2576f58--------------------------------)
    [## 通向生物学2.0的道路将通过黑箱数据'
- en: Future bio-AI breakthroughs will arise from novel high-throughput low-cost AI-specific
    “black-box” data modalities.
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 未来的生物AI突破将来自新型的高通量低成本AI特定的“黑箱”数据模态。
- en: towardsdatascience.com](/the-road-to-biology-2-0-will-pass-through-black-box-data-bbd00fabf959?source=post_page-----f363e2576f58--------------------------------)
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/the-road-to-biology-2-0-will-pass-through-black-box-data-bbd00fabf959?source=post_page-----f363e2576f58--------------------------------)
- en: '**📚Read more in references [20][22][23]**'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**📚在参考文献[20][22][23]中查看更多内容**'
- en: 👉 Key Takeaways 👈
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 👉 关键要点 👈
- en: '**➡️ How to generalize across graphs with heterogeneous node/edge/graph features?**'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**➡️ 如何在具有异质节点/边/图特征的图上进行泛化？**'
- en: 'Non-geometric graphs: Relative information transfers (such as prediction differences
    in *GraphAny* or relational interactions in *Ultra*), absolute information does
    not.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非几何图：相对信息可以转移（如*GraphAny*中的预测差异或*Ultra*中的关系互动），但绝对信息无法转移。
- en: 'Geometric graphs: transfer is easier thanks to the fixed set of atoms, but
    models have to learn some notion of physics to be reliable'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几何图：由于固定的原子集合，迁移学习更容易，但模型必须学习一些物理概念才能可靠。
- en: '**➡️ How to generalize across prediction tasks?**'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**➡️ 如何在不同的预测任务之间进行泛化？**'
- en: To date, there is no single model (among non-geometric GNNs) that would be able
    to perform node classification, link prediction, and graph classification in the
    zero-shot inference mode.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迄今为止，没有一个模型（在非几何图神经网络中）能够在零-shot推理模式下执行节点分类、链接预测和图分类。
- en: Framing all tasks through the lens of one might help, eg, node classification
    can be framed as link prediction.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过某一个视角框架所有任务可能会有所帮助，例如，节点分类可以框定为链接预测任务。
- en: '**➡️ What is the optimal model expressivity?**'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**➡️ 最优模型表达能力是什么？**'
- en: Node classification, link prediction, and graph classification leverage different
    symmetries.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点分类、链接预测和图分类利用了不同的对称性。
- en: Blunt application of maximally expressive models quickly leads to exponential
    runtime complexity or enormous memory costs — need to maintain the *expressivity
    vs efficiency* balance.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对最大表达能力模型的直接应用会迅速导致指数级的运行时复杂度或巨大的内存开销——需要保持*表达能力与效率*的平衡。
- en: The link between expressivity, sample complexity (how much training data you
    need), and inductive generalization is still unknown.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表达能力、样本复杂度（你需要多少训练数据）和归纳泛化之间的关系仍然未知。
- en: '**➡️ Data**'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**➡️ 数据**'
- en: Openly available graph data is orders of magnitude smaller than text/vision
    data, models have to be sample-efficient.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开放可用的图数据的规模比文本/视觉数据小几个数量级，因此模型必须具备样本高效性。
- en: Scaling laws are at the emerging stage, it is still unclear what to scale —
    number of nodes? Edges? Motifs? What is the notion of a token in graphs?
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展法则仍处于新兴阶段，尚不清楚应该扩展什么——节点数？边数？图案？图中的token概念是什么？
- en: 'Geometric GNNs: there is much more experimental data available that makes little
    sense to domain experts but might be of value to neural nets.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几何图神经网络：有大量实验数据，虽然这些数据对领域专家来说意义不大，但可能对神经网络有价值。
- en: Mao, Chen, et al. [Graph Foundation Models Are Already Here](https://arxiv.org/abs/2402.02216).
    ICML 2024
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Mao、Chen等人[图基础模型已经到来](https://arxiv.org/abs/2402.02216)。ICML 2024
- en: Morris et al. [Future Directions in Foundations of Graph Machine Learning](https://arxiv.org/abs/2402.02287).
    ICML 2024
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Morris等人[图机器学习基础的未来方向](https://arxiv.org/abs/2402.02287)。ICML 2024
- en: 'Zhao et al. [GraphAny: A Foundation Model for Node Classification on Any Graph](https://arxiv.org/abs/2405.20445).
    Arxiv 2024\. [Code on Github](https://github.com/DeepGraphLearning/GraphAny)'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zhao等人[GraphAny：一个用于任何图上节点分类的基础模型](https://arxiv.org/abs/2405.20445)。Arxiv 2024。[Github上的代码](https://github.com/DeepGraphLearning/GraphAny)
- en: Dong et al. [Universal Link Predictor By In-Context Learning on Graphs](https://arxiv.org/abs/2402.07738),
    arxiv 2024
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Dong等人[通过上下文学习进行图的通用链接预测](https://arxiv.org/abs/2402.07738)，arxiv 2024
- en: 'Zhang et al. [Labeling Trick: A Theory of Using Graph Neural Networks for Multi-Node
    Representation Learning](https://arxiv.org/abs/2010.16103). NeurIPS 2021'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zhang等人[标签技巧：使用图神经网络进行多节点表示学习的理论](https://arxiv.org/abs/2010.16103)。NeurIPS
    2021
- en: Chamberlain, Shirobokov, et al. [Graph Neural Networks for Link Prediction with
    Subgraph Sketching](https://arxiv.org/abs/2209.15486). ICLR 2023
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Chamberlain、Shirobokov等人[通过子图草图进行链接预测的图神经网络](https://arxiv.org/abs/2209.15486)。ICLR
    2023
- en: 'Zhu et al. [Neural Bellman-Ford Networks: A General Graph Neural Network Framework
    for Link Prediction](https://arxiv.org/abs/2106.06935). NeurIPS 2021'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zhu等人[神经贝尔曼-福特网络：用于链接预测的通用图神经网络框架](https://arxiv.org/abs/2106.06935)。NeurIPS
    2021
- en: Galkin et al. [Towards Foundation Models for Knowledge Graph Reasoning](https://openreview.net/forum?id=jVEoydFOl9).
    ICLR 2024
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Galkin 等人 [面向知识图谱推理的基础模型](https://openreview.net/forum?id=jVEoydFOl9)。ICLR 2024
- en: Galkin et al. [Zero-shot Logical Query Reasoning on any Knowledge Graph](https://arxiv.org/abs/2404.07198).
    arxiv 2024\. [Code on Github](https://github.com/DeepGraphLearning/ULTRA)
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Galkin 等人 [在任何知识图谱上的零-shot逻辑查询推理](https://arxiv.org/abs/2404.07198)。arxiv 2024\.
    [GitHub上的代码](https://github.com/DeepGraphLearning/ULTRA)
- en: Ibarz et al. [A Generalist Neural Algorithmic Learner](https://proceedings.mlr.press/v198/ibarz22a/ibarz22a.pdf)
    LoG 2022
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ibarz 等人 [通用神经算法学习者](https://proceedings.mlr.press/v198/ibarz22a/ibarz22a.pdf)
    LoG 2022
- en: Markeeva, McLeish, Ibarz, et al. [The CLRS-Text Algorithmic Reasoning Language
    Benchmar](https://arxiv.org/abs/2406.04229)k. arxiv 2024
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Markeeva, McLeish, Ibarz 等人 [CLRS-Text 算法推理语言基准](https://arxiv.org/abs/2406.04229)，arxiv
    2024
- en: 'Shoghi et al. [From Molecules to Materials: Pre-training Large Generalizable
    Models for Atomic Property Prediction](https://arxiv.org/abs/2310.16802). ICLR
    2024'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Shoghi 等人 [从分子到材料：预训练的大规模可泛化模型用于原子属性预测](https://arxiv.org/abs/2310.16802)。ICLR
    2024
- en: 'Zhang, Liu et al. [DPA-2: Towards a universal large atomic model for molecular
    and material simulation](https://arxiv.org/abs/2312.15492), arxiv 2023'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zhang, Liu 等人 [DPA-2：面向分子和材料模拟的通用大规模原子模型](https://arxiv.org/abs/2312.15492)，arxiv
    2023
- en: Batatia et al. [A foundation model for atomistic materials chemistry](https://arxiv.org/abs/2401.00096),
    arxiv 2024
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Batatia 等人 [原子材料化学的基础模型](https://arxiv.org/abs/2401.00096)，arxiv 2024
- en: 'Yang et al. [MatterSim: A Deep Learning Atomistic Model Across Elements, Temperatures
    and Pressures](https://arxiv.org/abs/2405.04967), arxiv 2024'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Yang 等人 [MatterSim：跨元素、温度和压力的深度学习原子模型](https://arxiv.org/abs/2405.04967)，arxiv
    2024
- en: Rives et al. [Biological Structure and Function Emerge from Scaling Unsupervised
    Learning to 250 Million Protein Sequences](https://www.pnas.org/doi/full/10.1073/pnas.2016239118).
    PNAS 2021
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Rives 等人 [通过将无监督学习扩展到2.5亿个蛋白质序列，生物结构和功能得以显现](https://www.pnas.org/doi/full/10.1073/pnas.2016239118)。PNAS
    2021
- en: Lin, Akin, Rao, Hie, et al. [Language models of protein sequences at the scale
    of evolution enable accurate structure prediction](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1).
    Science 2023\. [Code](https://github.com/facebookresearch/esm)
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Lin, Akin, Rao, Hie 等人 [在进化规模上，蛋白质序列的语言模型能够准确预测结构](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1)。Science
    2023\. [代码](https://github.com/facebookresearch/esm)
- en: Morgan HL (1965) [The generation of a unique machine description for chemical
    structures — a technique developed at chemical abstracts service](https://pubs.acs.org/doi/abs/10.1021/c160017a018).
    J Chem Doc 5:107–113.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Morgan HL (1965) [化学结构的唯一机器描述生成——化学文摘服务开发的一项技术](https://pubs.acs.org/doi/abs/10.1021/c160017a018)。J
    Chem Doc 5:107–113.
- en: 'Kläser, Banaszewski, et al. [MiniMol: A Parameter Efficient Foundation Model
    for Molecular Learning](https://arxiv.org/pdf/2404.14986), arxiv 2024'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kläser, Banaszewski 等人 [MiniMol：一种高效的分子学习基础模型](https://arxiv.org/pdf/2404.14986)，arxiv
    2024
- en: Sypetkowski, Wenkel et al. [On the Scalability of GNNs for Molecular Graphs](https://arxiv.org/abs/2404.11568v1),
    arxiv 2024
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sypetkowski, Wenkel 等人 [GNNs 在分子图上的可扩展性](https://arxiv.org/abs/2404.11568v1)，arxiv
    2024
- en: Morris et al. [Future Directions in Foundations of Graph Machine Learning](https://arxiv.org/abs/2402.02287).
    ICML 2024
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Morris 等人 [图形机器学习基础的未来方向](https://arxiv.org/abs/2402.02287)。ICML 2024
- en: Liu et al. [Neural Scaling Laws on Graphs](https://arxiv.org/abs/2402.02054),
    arxiv 2024
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Liu 等人 [图上的神经缩放法则](https://arxiv.org/abs/2402.02054)，arxiv 2024
- en: Frey et al. [Neural scaling of deep chemical models](https://www.nature.com/articles/s42256-023-00740-3),
    Nature Machine Intelligence 2023
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Frey 等人 [深度化学模型的神经缩放](https://www.nature.com/articles/s42256-023-00740-3)，Nature
    Machine Intelligence 2023
