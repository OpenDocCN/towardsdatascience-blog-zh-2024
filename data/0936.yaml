- en: Quantifying the Complexity and Learnability of Strategic Classification Problems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é‡åŒ–æˆ˜ç•¥åˆ†ç±»é—®é¢˜çš„å¤æ‚æ€§å’Œå¯å­¦ä¹ æ€§
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/quantifying-the-complexity-and-learnability-of-strategic-classification-problems-fd04cbfdd4b9?source=collection_archive---------8-----------------------#2024-04-12](https://towardsdatascience.com/quantifying-the-complexity-and-learnability-of-strategic-classification-problems-fd04cbfdd4b9?source=collection_archive---------8-----------------------#2024-04-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/quantifying-the-complexity-and-learnability-of-strategic-classification-problems-fd04cbfdd4b9?source=collection_archive---------8-----------------------#2024-04-12](https://towardsdatascience.com/quantifying-the-complexity-and-learnability-of-strategic-classification-problems-fd04cbfdd4b9?source=collection_archive---------8-----------------------#2024-04-12)
- en: How generalizing the notion of VC dimension to a strategic setting can help
    us understand whether or not a problem is learnable
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å°†VCç»´åº¦çš„æ¦‚å¿µæ¨å¹¿åˆ°æˆ˜ç•¥è®¾ç½®ä¸­ï¼Œå¯ä»¥å¸®åŠ©æˆ‘ä»¬ç†è§£ä¸€ä¸ªé—®é¢˜æ˜¯å¦å¯å­¦ä¹ 
- en: '[](https://jhyahav.medium.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------)[![Jonathan
    Yahav](../Images/30c3293a94be9258a65c38afd58bb521.png)](https://jhyahav.medium.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------)
    [Jonathan Yahav](https://jhyahav.medium.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://jhyahav.medium.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------)[![Jonathan
    Yahav](../Images/30c3293a94be9258a65c38afd58bb521.png)](https://jhyahav.medium.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------)
    [Jonathan Yahav](https://jhyahav.medium.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------)
    Â·8 min readÂ·Apr 12, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------)
    Â·é˜…è¯»æ—¶é•¿8åˆ†é’ŸÂ·2024å¹´4æœˆ12æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/7bfd19338bed666221665670b5f48bb6.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7bfd19338bed666221665670b5f48bb6.png)'
- en: Image generated by the author using DALL-E 3.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…ä½¿ç”¨DALL-E 3ç”Ÿæˆã€‚
- en: In [the first article in this series](/extending-pac-learning-to-a-strategic-classification-setting-6c374935dde2),
    we formally defined the ***strategic classification problem***, denoted Sá´›Ê€á´€á´„âŸ¨*H*,
    *R*, *c*âŸ©***,*** as a generalization of canonical binary classification. We did
    so based on the paper [***PAC-Learning for Strategic Classification***](https://arxiv.org/abs/2012.03310)
    (Sundaram, Vullikanti, Xu, & Yao, 2021). Along the way, we explored why we should
    care about considering the various preferences of rational agents during classification
    and how we can do so (subject to certain assumptions). We will rely heavily on
    the concepts introduced in the previous article, so I encourage you to read it
    if you havenâ€™t already.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[æœ¬ç³»åˆ—çš„ç¬¬ä¸€ç¯‡æ–‡ç« ](/extending-pac-learning-to-a-strategic-classification-setting-6c374935dde2)ï¼Œæˆ‘ä»¬æ­£å¼å®šä¹‰äº†***æˆ˜ç•¥åˆ†ç±»é—®é¢˜***ï¼Œè®°ä½œSá´›Ê€á´€á´„âŸ¨*H*,
    *R*, *c*âŸ©***ï¼Œ***å®ƒæ˜¯ç»å…¸äºŒå…ƒåˆ†ç±»çš„ä¸€ä¸ªæ¨å¹¿ã€‚æˆ‘ä»¬åŸºäºè®ºæ–‡[***PAC-Learning for Strategic Classification***](https://arxiv.org/abs/2012.03310)ï¼ˆSundaram,
    Vullikanti, Xu, & Yao, 2021ï¼‰æ¥è¿›è¡Œæ­¤å®šä¹‰ã€‚åœ¨è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†ä¸ºä»€ä¹ˆåœ¨åˆ†ç±»æ—¶éœ€è¦è€ƒè™‘ç†æ€§ä¸»ä½“çš„ä¸åŒåå¥½ï¼Œä»¥åŠå¦‚ä½•åœ¨æŸäº›å‡è®¾æ¡ä»¶ä¸‹åšåˆ°è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬å°†åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–å‰ä¸€ç¯‡æ–‡ç« ä¸­ä»‹ç»çš„æ¦‚å¿µï¼Œå› æ­¤å¦‚æœä½ è¿˜æ²¡é˜…è¯»è¿‡ï¼Œå¼ºçƒˆå»ºè®®ä½ å…ˆé˜…è¯»å®ƒã€‚
- en: '[](/extending-pac-learning-to-a-strategic-classification-setting-6c374935dde2?source=post_page-----fd04cbfdd4b9--------------------------------)
    [## Extending PAC Learning to a Strategic Classification Setting'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/extending-pac-learning-to-a-strategic-classification-setting-6c374935dde2?source=post_page-----fd04cbfdd4b9--------------------------------)
    [## å°†PACå­¦ä¹ æ‰©å±•åˆ°æˆ˜ç•¥åˆ†ç±»è®¾ç½®'
- en: A case study of the meeting point between game theory and fundamental concepts
    in machine learning
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åšå¼ˆè®ºä¸æœºå™¨å­¦ä¹ åŸºç¡€æ¦‚å¿µäº¤æ±‡çš„æ¡ˆä¾‹ç ”ç©¶
- en: towardsdatascience.com](/extending-pac-learning-to-a-strategic-classification-setting-6c374935dde2?source=post_page-----fd04cbfdd4b9--------------------------------)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/extending-pac-learning-to-a-strategic-classification-setting-6c374935dde2?source=post_page-----fd04cbfdd4b9--------------------------------)
- en: Weâ€™ll pick up where we left off, **using the definition of Sá´›Ê€á´€á´„âŸ¨*H*, *R*, *c*âŸ©
    as a jumping-off point for the useful concept of strategic VC dimension (SVC).**
    Once we make sense of SVC, what I call *the* *Fundamental Theorem of Strategic
    Learning* will follow rather naturally.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä»æˆ‘ä»¬åœä¸‹çš„åœ°æ–¹ç»§ç»­ï¼Œ**ä»¥ Sá´›Ê€á´€á´„âŸ¨*H*, *R*, *c*âŸ© çš„å®šä¹‰ä¸ºèµ·ç‚¹ï¼Œæ¥å¼•å…¥æœ‰ç”¨çš„æˆ˜ç•¥ VC ç»´åº¦ï¼ˆSVCï¼‰æ¦‚å¿µã€‚** ä¸€æ—¦æˆ‘ä»¬å¼„æ¸…æ¥šäº†
    SVCï¼Œæ¥ä¸‹æ¥æˆ‘æ‰€ç§°çš„*æˆ˜ç•¥å­¦ä¹ çš„åŸºæœ¬å®šç†*å°†è‡ªç„¶è€Œç„¶åœ°è·Ÿéšå…¶åã€‚
- en: While helpful, **prior familiarity** with shattering coefficients, the canonical
    VC dimension, and the [Fundamental Theorem of Statistical Learning](https://www.cs.princeton.edu/courses/archive/spring16/cos511/lec17.pdf)
    **will not be necessary for you to follow along.** However, thereâ€™s far more depth
    to each of them than I could ever hope to cover as part of this series, let alone
    in a single article. The curious reader is referred to [Andrew Rothman](https://medium.com/u/4688574fc42a?source=post_page---user_mention--fd04cbfdd4b9--------------------------------)â€™s
    wonderful and very thorough articles on the (canonical) shattering coefficient
    and VC dimension.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æœ‰å¸®åŠ©ï¼Œ**å¯¹æ‰©å±•ç³»æ•°ã€ç»å…¸ VC ç»´åº¦ä»¥åŠ[ç»Ÿè®¡å­¦ä¹ åŸºæœ¬å®šç†](https://www.cs.princeton.edu/courses/archive/spring16/cos511/lec17.pdf)çš„å…ˆå‰äº†è§£**
    **å¹¶ä¸æ˜¯è·Ÿå¾—ä¸Šæœ¬ç³»åˆ—å†…å®¹çš„å¿…è¦æ¡ä»¶ã€‚** ç„¶è€Œï¼Œå®ƒä»¬æ¯ä¸ªéƒ½æ¯”æˆ‘åœ¨æœ¬ç³»åˆ—ä¸­æ‰€èƒ½æ¶‰åŠçš„å†…å®¹è¦æ·±å¥¥å¾—å¤šï¼Œæ›´ä¸ç”¨è¯´åœ¨ä¸€ç¯‡æ–‡ç« ä¸­æ¶µç›–äº†ã€‚å¯¹äºæ„Ÿå…´è¶£çš„è¯»è€…ï¼Œå»ºè®®é˜…è¯»[Andrew
    Rothman](https://medium.com/u/4688574fc42a?source=post_page---user_mention--fd04cbfdd4b9--------------------------------)å…³äºï¼ˆç»å…¸ï¼‰æ‰©å±•ç³»æ•°å’Œ
    VC ç»´åº¦çš„ç²¾å½©ä¸”éå¸¸è¯¦å°½çš„æ–‡ç« ã€‚
- en: '[](https://anr248.medium.com/statistical-learning-theory-part-5-shattering-coefficient-9fbce2bd98c2?source=post_page-----fd04cbfdd4b9--------------------------------)
    [## Statistical Learning Theory Part 5: Shattering Coefficient'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://anr248.medium.com/statistical-learning-theory-part-5-shattering-coefficient-9fbce2bd98c2?source=post_page-----fd04cbfdd4b9--------------------------------)
    [## ç»Ÿè®¡å­¦ä¹ ç†è®º ç¬¬ 5 éƒ¨åˆ†ï¼šæ‰©å±•ç³»æ•°'
- en: Proof of Consistency, Rates, and Generalization Bounds for ML Estimators over
    Infinite Function Classes leveraging theâ€¦
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åˆ©ç”¨æ— é™å‡½æ•°ç±»çš„ ML ä¼°è®¡å™¨çš„ä¸€è‡´æ€§ã€é€Ÿç‡å’Œæ³›åŒ–ç•Œé™è¯æ˜â€¦â€¦
- en: 'anr248.medium.com](https://anr248.medium.com/statistical-learning-theory-part-5-shattering-coefficient-9fbce2bd98c2?source=post_page-----fd04cbfdd4b9--------------------------------)
    [](https://anr248.medium.com/statistical-learning-theory-part-6-vapnik-chervonenkis-vc-dimension-47848a38b6e7?source=post_page-----fd04cbfdd4b9--------------------------------)
    [## Statistical Learning Theory Part 6: Vapnikâ€“Chervonenkis (VC) Dimension'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: anr248.medium.com](https://anr248.medium.com/statistical-learning-theory-part-5-shattering-coefficient-9fbce2bd98c2?source=post_page-----fd04cbfdd4b9--------------------------------)
    [](https://anr248.medium.com/statistical-learning-theory-part-6-vapnik-chervonenkis-vc-dimension-47848a38b6e7?source=post_page-----fd04cbfdd4b9--------------------------------)
    [## ç»Ÿè®¡å­¦ä¹ ç†è®º ç¬¬ 6 éƒ¨åˆ†ï¼šVapnikâ€“Chervonenkisï¼ˆVCï¼‰ç»´åº¦
- en: Proof of Consistency, Rates, and Generalization Bounds for ML Estimators over
    Infinite Function Classes leveraging theâ€¦
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åˆ©ç”¨æ— é™å‡½æ•°ç±»çš„ ML ä¼°è®¡å™¨çš„ä¸€è‡´æ€§ã€é€Ÿç‡å’Œæ³›åŒ–ç•Œé™è¯æ˜â€¦â€¦
- en: anr248.medium.com](https://anr248.medium.com/statistical-learning-theory-part-6-vapnik-chervonenkis-vc-dimension-47848a38b6e7?source=post_page-----fd04cbfdd4b9--------------------------------)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[anr248.medium.com](https://anr248.medium.com/statistical-learning-theory-part-6-vapnik-chervonenkis-vc-dimension-47848a38b6e7?source=post_page-----fd04cbfdd4b9--------------------------------)'
- en: As weâ€™ll see, **strategic shattering coefficients and SVC are fairly natural
    generalizations of their canonical (i.e., non-strategic) counterparts.** We will
    therefore begin with a brief rundown of each of those counterparts before explaining
    how they can be modified to work in a strategic setting.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬å°†çœ‹åˆ°çš„ï¼Œ**æˆ˜ç•¥æ‰©å±•ç³»æ•°å’Œ SVC æ˜¯å®ƒä»¬ç»å…¸ï¼ˆå³éæˆ˜ç•¥ï¼‰å¯¹åº”ç‰©çš„è‡ªç„¶æ¨å¹¿ã€‚** å› æ­¤ï¼Œæˆ‘ä»¬å°†é¦–å…ˆç®€è¦ä»‹ç»è¿™äº›å¯¹åº”ç‰©ï¼Œç„¶åè§£é‡Šå¦‚ä½•å°†å®ƒä»¬ä¿®æ”¹ä»¥é€‚åº”æˆ˜ç•¥ç¯å¢ƒã€‚
- en: 'Counting Achievable Labelings: Canonical Shattering Coefficients'
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯å®ç°æ ‡è®°çš„è®¡æ•°ï¼šç»å…¸æ‰©å±•ç³»æ•°
- en: 'Verbally defining shattering coefficients seems straightforward at first glance:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å£å¤´å®šä¹‰æ‰©å±•ç³»æ•°ä¹ä¸€çœ‹ä¼¼ä¹æ˜¯ç›´æ¥çš„ï¼š
- en: '*Given a hypothesis class* H*,* ***its* n*áµ—Ê° shattering coefficient, denoted*
    Sâ‚™*(*H*),*** *represents the* ***largest number of labelings achievable by classifiers
    in* H *on a sample of* n *feature vectors.***'
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*ç»™å®šä¸€ä¸ªå‡è®¾ç±»* H*ï¼Œ* ***å®ƒçš„ç¬¬ n*áµ—Ê° æ‰©å±•ç³»æ•°ï¼Œè®°ä½œ* Sâ‚™*(*H*),*** *è¡¨ç¤ºåˆ†ç±»å™¨åœ¨* H *ä¸­èƒ½å¤Ÿå¯¹* n *ä¸ªç‰¹å¾å‘é‡çš„æ ·æœ¬è¿›è¡Œæ ‡è®°çš„æœ€å¤§æ•°é‡ã€‚*'
- en: But what is a â€œ*labeling*â€? And what makes it â€œ*achievable*â€? Answering those
    questions will help us lay some groundwork in pursuit of a more formal definition.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œä»€ä¹ˆæ˜¯â€œ*æ ‡è®°*â€ï¼Ÿä»€ä¹ˆæ‰ç®—æ˜¯â€œ*å¯å®ç°çš„*â€ï¼Ÿå›ç­”è¿™äº›é—®é¢˜å°†å¸®åŠ©æˆ‘ä»¬ä¸ºæ›´æ­£å¼çš„å®šä¹‰æ‰“ä¸‹åŸºç¡€ã€‚
- en: In the context of binary classification, a **labeling** of a sample of feature
    vectors is simply any one of the ways we can assign values from the set { -1,
    1 } to those vectors. As a very simple example, consider two one-dimensional feature
    vectors (i.e., points on a number line), *x*â‚ = 1 and *x*â‚‚ = 2.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨äºŒåˆ†ç±»çš„èƒŒæ™¯ä¸‹ï¼Œå¯¹ç‰¹å¾å‘é‡æ ·æœ¬çš„**æ ‡æ³¨**ä»…ä»…æ˜¯å°†é›†åˆ { -1, 1 } ä¸­çš„å€¼åˆ†é…ç»™è¿™äº›å‘é‡çš„ä»»æ„ä¸€ç§æ–¹å¼ã€‚ä½œä¸ºä¸€ä¸ªéå¸¸ç®€å•çš„ä¾‹å­ï¼Œè€ƒè™‘ä¸¤ä¸ªä¸€ç»´ç‰¹å¾å‘é‡ï¼ˆå³æ•°è½´ä¸Šçš„ç‚¹ï¼‰ï¼Œ*x*â‚
    = 1 å’Œ *x*â‚‚ = 2ã€‚
- en: '![](../Images/44bc76860772a5a6089a5c11a9bce341.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44bc76860772a5a6089a5c11a9bce341.png)'
- en: A visualization of the four possible labelings of the sample *x*â‚ = 1, *x*â‚‚
    = 2\. Red points are negatively classified, blue ones are positively classified.
    Image by the author.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æ ·æœ¬ *x*â‚ = 1, *x*â‚‚ = 2 çš„å››ç§å¯èƒ½æ ‡æ³¨çš„å¯è§†åŒ–ã€‚çº¢è‰²ç‚¹ä¸ºè´Ÿç±»ï¼Œè“è‰²ç‚¹ä¸ºæ­£ç±»ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: The possible labelings are any combination of the classification values we can
    assign the individual feature vectors independent of one another. We can represent
    each labeling as a vector, where the first and second coordinate represent the
    values assigned to *x*â‚ and *x*â‚‚, respectively. The set of possible labelings
    is thus { (-1, -1), (-1, 1), (1, -1), (1, 1) }. Note that a sample of size 2 yields
    2Â² = 4 possible labelings â€” weâ€™ll see how this generalizes to arbitrarily-sized
    samples soon.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å¯èƒ½çš„æ ‡æ³¨æ˜¯æˆ‘ä»¬å¯ä»¥ä¸ºæ¯ä¸ªç‰¹å¾å‘é‡ç‹¬ç«‹åˆ†é…åˆ†ç±»å€¼çš„ä»»æ„ç»„åˆã€‚æˆ‘ä»¬å¯ä»¥å°†æ¯ä¸ªæ ‡æ³¨è¡¨ç¤ºä¸ºä¸€ä¸ªå‘é‡ï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ªåæ ‡åˆ†åˆ«è¡¨ç¤ºåˆ†é…ç»™ *x*â‚ å’Œ *x*â‚‚
    çš„å€¼ã€‚å› æ­¤ï¼Œå¯èƒ½çš„æ ‡æ³¨é›†åˆä¸º { (-1, -1), (-1, 1), (1, -1), (1, 1) }ã€‚è¯·æ³¨æ„ï¼Œå¤§å°ä¸º 2 çš„æ ·æœ¬ä¼šäº§ç”Ÿ 2Â² = 4
    ç§å¯èƒ½çš„æ ‡æ³¨â€”â€”æˆ‘ä»¬å°†å¾ˆå¿«çœ‹åˆ°è¿™å¦‚ä½•æ¨å¹¿åˆ°ä»»æ„å¤§å°çš„æ ·æœ¬ã€‚
- en: We say that **a labeling is *achievable* by a hypothesis class *H*** if there
    exists a classifier *h* âˆˆ *H* from which that labeling can result. Continuing
    with our simple example, suppose we are limited to classifiers of the form *x*
    â‰¥ *k*, kâˆˆ â„, that is, one-dimensional thresholds such that anything to the right
    of the threshold is positively classified. The labeling (1, -1) is not achievable
    by this hypothesis class. *x*â‚‚ being greater than *x*â‚ implies that any threshold
    that classifies *x*â‚ positively must do the same for *x*â‚‚. The set of achievable
    labelings is therefore { (-1, -1), (-1, 1), (1, 1) }.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¯´**ä¸€ä¸ªæ ‡æ³¨æ˜¯ç”±å‡è®¾ç±» *H* *å¯å®ç°çš„***ï¼Œå¦‚æœå­˜åœ¨ä¸€ä¸ªåˆ†ç±»å™¨ *h* âˆˆ *H*ï¼Œè¯¥åˆ†ç±»å™¨èƒ½å¾—åˆ°è¯¥æ ‡æ³¨ã€‚ç»§ç»­æˆ‘ä»¬çš„ç®€å•ç¤ºä¾‹ï¼Œå‡è®¾æˆ‘ä»¬ä»…é™äºå½¢å¼ä¸º
    *x* â‰¥ *k*, kâˆˆ â„ çš„åˆ†ç±»å™¨ï¼Œå³ä¸€ç»´é˜ˆå€¼åˆ†ç±»å™¨ï¼Œä»»ä½•ä½äºé˜ˆå€¼å³ä¾§çš„ç‚¹éƒ½ä¼šè¢«æ­£ç±»åŒ–ã€‚æ ‡æ³¨ (1, -1) æ˜¯æ— æ³•é€šè¿‡æ­¤å‡è®¾ç±»å®ç°çš„ã€‚ç”±äº *x*â‚‚
    å¤§äº *x*â‚ï¼Œä»»ä½•å°† *x*â‚ åˆ†ç±»ä¸ºæ­£ç±»çš„é˜ˆå€¼ä¹Ÿå¿…å®šå°† *x*â‚‚ åˆ†ç±»ä¸ºæ­£ç±»ã€‚å› æ­¤ï¼Œå¯å®ç°çš„æ ‡æ³¨é›†åˆä¸º { (-1, -1), (-1, 1), (1,
    1) }ã€‚
- en: '![](../Images/1e2bc966296645a690f64a4b3d1418fb.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e2bc966296645a690f64a4b3d1418fb.png)'
- en: Examples of one-dimensional threshold classifiers that can be used to achieve
    all but one of the possible labelings of the sample *x*â‚ = 1, *x*â‚‚ = 2\. Image
    by the author.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç»´é˜ˆå€¼åˆ†ç±»å™¨çš„ç¤ºä¾‹ï¼Œå¯ä»¥ç”¨æ¥å®ç°é™¤ä¸€ä¸ªå¤–çš„æ‰€æœ‰å¯èƒ½çš„æ ‡æ³¨æ ·æœ¬ *x*â‚ = 1, *x*â‚‚ = 2ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: Having understood the basic terminology, we can start to develop some notation
    to formally express elements of the verbal definition with which we started.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç†è§£äº†åŸºæœ¬æœ¯è¯­åï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹å‘å±•ä¸€äº›ç¬¦å·æ¥æ­£å¼è¡¨è¾¾æˆ‘ä»¬å¼€å§‹æ—¶æ‰€æåˆ°çš„å£å¤´å®šä¹‰ä¸­çš„å…ƒç´ ã€‚
- en: '![](../Images/f90d9d885a53417968f0fa5e367aec9e.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f90d9d885a53417968f0fa5e367aec9e.png)'
- en: We stick to representing labelings as vectors as we did in our simple example,
    with each coordinate representing the classification value assigned to the corresponding
    feature vector. **There are 2*â¿* possible labelings in total:** there are two
    possible choices for each feature vector, and we can think of a labeling as a
    collection of *n* such choices, each made independently of the rest. **If a hypothesis
    class *H* can achieve all possible labelings of a sample ğ’*â‚™*,** i.e., if the
    number of *achievable* labelings of ğ’*â‚™* equals 2*â¿*, **we say that *H shatters*
    ğ’*â‚™.***
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¿æŒå°†æ ‡æ³¨è¡¨ç¤ºä¸ºå‘é‡ï¼Œå°±åƒåœ¨æˆ‘ä»¬çš„ç®€å•ç¤ºä¾‹ä¸­ä¸€æ ·ï¼Œæ¯ä¸ªåæ ‡ä»£è¡¨åˆ†é…ç»™ç›¸åº”ç‰¹å¾å‘é‡çš„åˆ†ç±»å€¼ã€‚**æ€»å…±æœ‰ 2*â¿* ç§å¯èƒ½çš„æ ‡æ³¨ï¼š**æ¯ä¸ªç‰¹å¾å‘é‡éƒ½æœ‰ä¸¤ç§å¯èƒ½çš„é€‰æ‹©ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ ‡æ³¨çœ‹ä½œæ˜¯
    *n* ä¸ªæ­¤ç±»é€‰æ‹©çš„é›†åˆï¼Œæ¯ä¸ªé€‰æ‹©éƒ½ç‹¬ç«‹äºå…¶ä»–é€‰æ‹©ã€‚**å¦‚æœå‡è®¾ç±» *H* èƒ½å¤Ÿå®ç°æ ·æœ¬ ğ’*â‚™* çš„æ‰€æœ‰å¯èƒ½æ ‡æ³¨ï¼Œ**å³ï¼Œå¦‚æœ ğ’*â‚™* çš„ *å¯å®ç°*
    æ ‡æ³¨çš„æ•°é‡ç­‰äº 2*â¿*ï¼Œ**æˆ‘ä»¬è¯´ *H* â€œæ‰“ç ´â€äº† ğ’*â‚™*ã€‚**
- en: 'Finally, using the notation from above, we converge on a more rigorous definition
    of *Sâ‚™*(*H*):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œä½¿ç”¨ä¸Šé¢çš„ç¬¦å·ï¼Œæˆ‘ä»¬å¾—å‡ºæ›´ä¸¥æ ¼çš„ *Sâ‚™*(*H*) å®šä¹‰ï¼š
- en: '![](../Images/510674c0db78fe8a4edc6c7d2ea05a2e.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/510674c0db78fe8a4edc6c7d2ea05a2e.png)'
- en: In keeping with our explanation of shattering, ***Sâ‚™*(*H*) equalling 2*â¿* implies
    that there exists a sample of size *n* that is shattered by *H*.**
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“åˆæˆ‘ä»¬å¯¹æ‰“ç ´çš„è§£é‡Šï¼Œ***Sâ‚™*(*H*) ç­‰äº 2*â¿* æ„å‘³ç€å­˜åœ¨ä¸€ä¸ªå¤§å°ä¸º *n* çš„æ ·æœ¬è¢« *H* æ‰“ç ´ã€‚**
- en: 'Estimating Hypothesis Class Expressiveness: Canonical VC Dimension'
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¼°è®¡å‡è®¾ç±»çš„è¡¨è¾¾èƒ½åŠ›ï¼šç»å…¸ VC ç»´åº¦
- en: '**The Vapnikâ€“Chervonenkis (VC) dimension is a way to gauge the expressive power
    of a hypothesis class.** Itâ€™s based on the idea of shattering we just defined,
    and it plays an important role in helping us determine which hypothesis classes
    are PAC learnable and which arenâ€™t.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**Vapnikâ€“Chervonenkis (VC) ç»´åº¦æ˜¯è¡¡é‡å‡è®¾ç±»è¡¨è¾¾èƒ½åŠ›çš„ä¸€ç§æ–¹å¼ã€‚** å®ƒåŸºäºæˆ‘ä»¬åˆšåˆšå®šä¹‰çš„ç ´ç¢æ¦‚å¿µï¼Œå¹¶åœ¨å¸®åŠ©æˆ‘ä»¬ç¡®å®šå“ªäº›å‡è®¾ç±»æ˜¯
    PAC å¯å­¦ä¹ çš„ã€å“ªäº›ä¸æ˜¯æ–¹é¢èµ·ç€é‡è¦ä½œç”¨ã€‚'
- en: 'Letâ€™s begin by attempting to intuitively define the canonical VC dimension:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»ç›´è§‚åœ°å®šä¹‰ç»å…¸çš„ VC ç»´åº¦å¼€å§‹ï¼š
- en: '*Given a hypothesis class* H*, its VC dimension, denoted VCdim(*H*), is defined
    to be the greatest natural number* n *for which there exists a sample of size*
    n *that is* ***shattered*** *by* H*.*'
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*ç»™å®šä¸€ä¸ªå‡è®¾ç±»* Hï¼Œ*å…¶ VC ç»´åº¦ï¼Œè®°ä½œ VCdim(*H*), è¢«å®šä¹‰ä¸ºå­˜åœ¨ä¸€ä¸ªå¤§å°ä¸º* n *çš„æ ·æœ¬ï¼Œå…¶ä¸­* n *æ˜¯æœ€å¤§çš„è‡ªç„¶æ•°ï¼Œä½¿å¾—è¯¥æ ·æœ¬è¢«*
    H* ***ç ´ç¢***ã€‚*'
- en: 'Using *Sâ‚™*(*H*) enables us to express this much more cleanly and succinctly:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ *Sâ‚™*(*H*) ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ›´åŠ ç®€æ´å’Œæ¸…æ™°åœ°è¡¨è¾¾è¿™ä¸€ç‚¹ï¼š
- en: '*VCdim(*H*)* = *max{* n *âˆˆ â„• :* Sâ‚™*(*H*) = 2*â¿ *}*'
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*VCdim(*H*)* = *max{* n *âˆˆ â„• :* Sâ‚™*(*H*) = 2*â¿ *}*'
- en: 'However, this definition isnâ€™t precise. Note that the set of numbers for which
    the shattering coefficient equals 2*â¿* may be infinite. (Consequently, it is possible
    that VCdim(*H*) = âˆ.) If thatâ€™s the case, the set has no well-defined maximum.
    We address this by taking the supremum instead:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œè¿™ä¸€å®šä¹‰å¹¶ä¸ç²¾ç¡®ã€‚è¯·æ³¨æ„ï¼Œå¯¹äºç ´ç¢ç³»æ•°ç­‰äº 2*â¿* çš„æ•°å­—é›†åˆå¯èƒ½æ˜¯æ— é™çš„ã€‚ï¼ˆå› æ­¤ï¼ŒVCdim(*H*) = âˆ æ˜¯æœ‰å¯èƒ½çš„ã€‚ï¼‰å¦‚æœæ˜¯è¿™ç§æƒ…å†µï¼Œé›†åˆå°±æ²¡æœ‰æ˜ç¡®çš„æœ€å¤§å€¼ã€‚æˆ‘ä»¬é€šè¿‡å–ä¸Šç¡®ç•Œæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼š
- en: '***VCdim(*H*)* = *sup{* n *âˆˆ â„• :* Sâ‚™*(*H*) = 2*â¿ *}***'
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***VCdim(*H*)* = *sup{* n *âˆˆ â„• :* Sâ‚™*(*H*) = 2*â¿ *}***'
- en: This rigorous and concise definition is the one weâ€™ll use moving forward.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªä¸¥æ ¼è€Œç®€æ´çš„å®šä¹‰å°†æ˜¯æˆ‘ä»¬æ¥ä¸‹æ¥ä½¿ç”¨çš„å®šä¹‰ã€‚
- en: 'Adding Preferences to the Mix: Strategic Shattering Coefficients'
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‘æ··åˆä¸­åŠ å…¥åå¥½ï¼šæˆ˜ç•¥æ€§ç ´ç¢ç³»æ•°
- en: Generalizing the canonical notions we just went over so that they work in a
    strategic setup is fairly straightforward. Redefining shattering coefficients
    in terms of the *data point best response* we defined in [the previous article](/extending-pac-learning-to-a-strategic-classification-setting-6c374935dde2)
    is practically all weâ€™ll have to do.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æˆ‘ä»¬åˆšåˆšè®¨è®ºçš„ç»å…¸æ¦‚å¿µæ¨å¹¿åˆ°æˆ˜ç•¥æ€§è®¾ç½®ä¸­æ˜¯ç›¸å½“ç®€å•çš„ã€‚åªéœ€é€šè¿‡é‡æ–°å®šä¹‰ç ´ç¢ç³»æ•°ä¸ºæˆ‘ä»¬åœ¨[ä¸Šä¸€ç¯‡æ–‡ç« ](/extending-pac-learning-to-a-strategic-classification-setting-6c374935dde2)ä¸­å®šä¹‰çš„æ•°æ®ç‚¹æœ€ä½³å“åº”ï¼Œå‡ ä¹å°±èƒ½å®Œæˆæ‰€æœ‰å·¥ä½œã€‚
- en: '*Given a hypothesis class* H, *a preference set* R*, and a cost function* c*,*
    ***the* n*áµ—Ê° shattering coefficient of Sá´›Ê€á´€á´„âŸ¨*H*,* R*,* c*âŸ©, denoted Ïƒ*â‚™*(*H,
    R, c*),*** *represents* ***the largest number of labelings achievable by classifiers
    in* H** *on a set of* n *potentially-manipulated feature vectors, i.e.,***n *data
    point best responses.***'
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*ç»™å®šä¸€ä¸ªå‡è®¾ç±»* Hï¼Œ*ä¸€ä¸ªåå¥½é›†åˆ* Rï¼Œ*å’Œä¸€ä¸ªæˆæœ¬å‡½æ•°* cï¼Œ* ***Sá´›Ê€á´€á´„âŸ¨*H*,* R*,* câŸ©çš„ç¬¬*n*æ¬¡ç ´ç¢ç³»æ•°ï¼Œè®°ä½œ Ïƒ*â‚™*(*H,
    R, c*),*** *è¡¨ç¤º* ***ç”±* H* ä¸­çš„åˆ†ç±»å™¨åœ¨ä¸€ç»„* n *ä¸ªå¯èƒ½è¢«æ“æ§çš„ç‰¹å¾å‘é‡ä¸Šæ‰€èƒ½å®ç°çš„æœ€å¤§æ ‡æ³¨æ•°ï¼Œå³* n *ä¸ªæ•°æ®ç‚¹æœ€ä½³å“åº”ã€‚***'
- en: 'As a reminder, this is how we defined the data point best response:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æé†’ä¸€ä¸‹ï¼Œè¿™æ˜¯æˆ‘ä»¬å®šä¹‰æ•°æ®ç‚¹æœ€ä½³å“åº”çš„æ–¹å¼ï¼š
- en: '![](../Images/069b6c5d55f22bdb773f696053f0c01b.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/069b6c5d55f22bdb773f696053f0c01b.png)'
- en: 'We can tweak the notation we used in our discussion of canonical shattering
    coefficients to further formalize this:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è°ƒæ•´åœ¨è®¨è®ºç»å…¸ç ´ç¢ç³»æ•°æ—¶ä½¿ç”¨çš„ç¬¦å·ï¼Œä»¥è¿›ä¸€æ­¥å½¢å¼åŒ–è¿™ä¸€ç‚¹ï¼š
- en: '![](../Images/39102a2417d9cf94e79a49dfd64f6df7.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/39102a2417d9cf94e79a49dfd64f6df7.png)'
- en: The main difference is that each *x* in the sample has to have a corresponding
    *r*. Other than that, putting the data point best response where we had x in the
    canonical case works smoothly.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸»è¦çš„åŒºåˆ«åœ¨äºæ ·æœ¬ä¸­çš„æ¯ä¸ª *x* å¿…é¡»æœ‰ä¸€ä¸ªå¯¹åº”çš„ *r*ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œåœ¨ç»å…¸æƒ…å†µä¸‹ï¼Œå°†æ•°æ®ç‚¹æœ€ä½³å“åº”ç½®äº x å¤„å°±å¯ä»¥é¡ºåˆ©å·¥ä½œã€‚
- en: '**As a quick sanity check, letâ€™s consider what happens if *R* = { 0 }.** The
    realized reward term ğ•€(*h*(*z*) = 1) â‹… *r* will be 0 across all the data points.
    **Maximizing utility thus becomes synonymous with minimizing cost**. The best
    way to minimize the cost incurred by a data point is trivial: **never manipulating
    its feature vector.**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä½œä¸ºä¸€ä¸ªç®€å•çš„ç†æ€§æ£€æŸ¥ï¼Œè®©æˆ‘ä»¬è€ƒè™‘å¦‚æœ *R* = { 0 } ä¼šå‘ç”Ÿä»€ä¹ˆã€‚** å®é™…çš„å¥–åŠ±é¡¹ ğ•€(*h*(*z*) = 1) â‹… *r* åœ¨æ‰€æœ‰æ•°æ®ç‚¹ä¸Šéƒ½å°†ä¸º
    0ã€‚**å› æ­¤ï¼Œæœ€å¤§åŒ–æ•ˆç”¨å°±ç­‰åŒäºæœ€å°åŒ–æˆæœ¬**ã€‚æœ€ç®€å•çš„å‡å°‘æ•°æ®ç‚¹æˆæœ¬çš„æ–¹å¼å°±æ˜¯ï¼š**æ°¸è¿œä¸è¦æ“æ§å…¶ç‰¹å¾å‘é‡ã€‚**'
- en: '**Î”(*x*, *r*; *h*) ends up always just being *x*,** placing us firmly within
    the territory of canonical classification. **It follows that Ïƒ*â‚™*(*H*, { 0 },
    *c*) *= Sâ‚™*(*H*) for all *H, c*.** This is consistent with our observation that
    the impartial preference class represented by *R* = { 0 } is equivalent to canonical
    binary classification.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**Î”(*x*, *r*; *h*) æœ€ç»ˆæ€»æ˜¯ä»…ä»…æ˜¯ *x*ï¼Œ** è¿™å°†æˆ‘ä»¬ç‰¢ç‰¢åœ°ç½®äºç»å…¸åˆ†ç±»çš„é¢†åŸŸå†…ã€‚ **å› æ­¤ï¼ŒÏƒ*â‚™*(*H*, { 0 },
    *c*) *= Sâ‚™*(*H*) å¯¹æ‰€æœ‰ *H, c* éƒ½æˆç«‹ã€‚** è¿™ä¸€ç‚¹ä¸æˆ‘ä»¬è§‚å¯Ÿåˆ°çš„æ— ååå¥½ç±»ï¼ˆç”± *R* = { 0 } è¡¨ç¤ºï¼‰ç­‰åŒäºç»å…¸äºŒåˆ†ç±»çš„è§‚ç‚¹æ˜¯ä¸€è‡´çš„ã€‚'
- en: 'Expressiveness with Preferences: Strategic VC Dimension (SVC)'
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…·æœ‰åå¥½æ€§çš„è¡¨è¾¾èƒ½åŠ›ï¼šæˆ˜ç•¥VCç»´åº¦ï¼ˆSVCï¼‰
- en: Having defined the *n*áµ—Ê°strategic shattering coefficient, **we can simply swap
    out the *Sâ‚™*(*H*)in the canonical definition of the VC dimension for Ïƒ*â‚™*(*H*,
    *R*, *c*).**
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®šä¹‰äº†*n*áµ—Ê°æˆ˜ç•¥ç¢ç‰‡ç³»æ•°åï¼Œ**æˆ‘ä»¬åªéœ€å°†æ ‡å‡†VCç»´åº¦å®šä¹‰ä¸­çš„*Sâ‚™*(*H*)æ›¿æ¢ä¸ºÏƒ*â‚™*(*H*, *R*, *c*)ã€‚**
- en: '***SVC(*H, R, c*)* = *sup{* n *âˆˆ â„• : Ïƒ*â‚™*(*H, R, c*) = 2*â¿ *}***'
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***SVC(*H, R, c*)* = *sup{* n *âˆˆ â„• : Ïƒ*â‚™*(*H, R, c*) = 2*â¿ *}***'
- en: Based on the example we considered above, we find that SVC(*H*, { 0 }, *c*)
    = VCdim(*H*) for any *H*, *c*. Indeed, **SVC is to VCdim as the strategic shattering
    coefficient is to its canonical equivalent:** both are elegant generalizations
    of non-strategic concepts.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæˆ‘ä»¬ä¸Šè¿°è€ƒè™‘çš„ä¾‹å­ï¼Œæˆ‘ä»¬å‘ç°SVC(*H*, { 0 }, *c*) = VCdim(*H*) å¯¹ä»»ä½• *H*, *c* éƒ½æˆç«‹ã€‚å®é™…ä¸Šï¼Œ**SVCä¸VCdimçš„å…³ç³»å°±åƒæˆ˜ç•¥ç¢ç‰‡ç³»æ•°ä¸å…¶æ ‡å‡†ç‰ˆæœ¬çš„å…³ç³»ä¸€æ ·ï¼š**ä¸¤è€…éƒ½æ˜¯éæˆ˜ç•¥æ¦‚å¿µçš„ä¼˜é›…æ¨å¹¿ã€‚
- en: 'From SVC to Strategic PAC Learnability: The Fundamental Theorem of Strategic
    Learning'
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»SVCåˆ°æˆ˜ç•¥PACå¯å­¦ä¹ æ€§ï¼šæˆ˜ç•¥å­¦ä¹ çš„åŸºæœ¬å®šç†
- en: '**We can now use SVC to state the Fundamental Theorem of Strategic Learning,**
    which relates the complexity of a strategic classification problem to its (agnostic)
    PAC learnability.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**æˆ‘ä»¬ç°åœ¨å¯ä»¥ä½¿ç”¨SVCæ¥é™ˆè¿°æˆ˜ç•¥å­¦ä¹ çš„åŸºæœ¬å®šç†ï¼Œ** è¯¥å®šç†å°†æˆ˜ç•¥åˆ†ç±»é—®é¢˜çš„å¤æ‚æ€§ä¸å…¶ï¼ˆæ— å…³çš„ï¼‰PACå¯å­¦ä¹ æ€§ç›¸å…³è”ã€‚'
- en: '***A strategic classification instance Sá´›Ê€á´€á´„âŸ¨*H*,* R*,* c*âŸ© is agnostic PAC
    learnable if and only if SVC(*H, R, c*) is finite.*** *The* [*sample complexity*](https://en.wikipedia.org/wiki/Sample_complexity)
    *for strategic agnostic PAC learning is* **m*(*Î´*,* Îµ*) â‰¤* CÎµ *â»Â² â‹…* *(SVC(*H,
    R, c*) + logâ¡(1/*Î´*))****, with* C *being a constant.*'
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***ä¸€ä¸ªæˆ˜ç•¥åˆ†ç±»å®ä¾‹Sá´›Ê€á´€á´„âŸ¨*H*,* R*,* c*âŸ©æ˜¯æ— å…³PACå¯å­¦ä¹ çš„ï¼Œå½“ä¸”ä»…å½“SVC(*H, R, c*)æ˜¯æœ‰é™çš„ã€‚*** *æˆ˜ç•¥æ— å…³PACå­¦ä¹ çš„*
    [*æ ·æœ¬å¤æ‚åº¦*](https://en.wikipedia.org/wiki/Sample_complexity) *æ˜¯* **m*(*Î´*,* Îµ*)
    â‰¤* CÎµ *â»Â² â‹…* *(SVC(*H, R, c*) + logâ¡(1/*Î´*))****ï¼Œå…¶ä¸­*C*æ˜¯å¸¸æ•°ã€‚**'
- en: We wonâ€™t elaborate too much on how this can be proven. Suffice it to say that
    it boils down to a clever reduction to the (well-documented) [Fundamental Theorem
    of *Statistical* Learning](https://www.cs.princeton.edu/courses/archive/spring16/cos511/lec17.pdf),
    which is essentially the non-strategic version of the theorem. If youâ€™re mathematically
    inclined and interested in the nuts and bolts of the proof, you can find it in
    [Appendix B of the paper](https://arxiv.org/pdf/2012.03310.pdf#page=12).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸ä¼šè¯¦ç»†é˜è¿°è¿™ä¸€ç‚¹å¦‚ä½•è¢«è¯æ˜ã€‚åªéœ€è¯´å®ƒå½’ç»“ä¸ºä¸€ä¸ªå·§å¦™çš„ç®€åŒ–ï¼Œè½¬åŒ–ä¸ºï¼ˆæœ‰å……åˆ†æ–‡çŒ®è®°è½½çš„ï¼‰[*ç»Ÿè®¡*å­¦ä¹ çš„åŸºæœ¬å®šç†](https://www.cs.princeton.edu/courses/archive/spring16/cos511/lec17.pdf)ï¼Œè¯¥å®šç†æœ¬è´¨ä¸Šæ˜¯è¯¥å®šç†çš„éæˆ˜ç•¥ç‰ˆæœ¬ã€‚å¦‚æœä½ å¯¹æ•°å­¦æœ‰å…´è¶£å¹¶ä¸”æƒ³äº†è§£è¯æ˜çš„ç»†èŠ‚ï¼Œå¯ä»¥åœ¨[è®ºæ–‡çš„é™„å½•B](https://arxiv.org/pdf/2012.03310.pdf#page=12)ä¸­æ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚
- en: '**This theorem essentially completes our generalization of classic PAC learning
    to a strategic classification setting.** It shows that the way we defined SVC
    actually doesnâ€™t just make sense in our heads; it actually works as a generalization
    of VCdim where it matters most. Armed with the Fundamental Theorem, we are well-equipped
    to analyze strategic classification problems much as we would any old binary classification
    problem. In my opinion, having the ability to determine whether a strategic problem
    is theoretically learnable or not is pretty incredible!'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¿™ä¸ªå®šç†åŸºæœ¬ä¸Šå®Œæˆäº†æˆ‘ä»¬å°†ç»å…¸PACå­¦ä¹ æ¨å¹¿åˆ°æˆ˜ç•¥åˆ†ç±»ç¯å¢ƒçš„å·¥ä½œã€‚** å®ƒè¡¨æ˜ï¼Œæˆ‘ä»¬å®šä¹‰çš„SVCä¸ä»…åœ¨æˆ‘ä»¬çš„è„‘æµ·ä¸­æ˜¯åˆç†çš„ï¼›å®ƒå®é™…ä¸Šä½œä¸ºVCdimçš„ä¸€ä¸ªæ¨å¹¿åœ¨æœ€å…³é”®çš„åœ°æ–¹æ˜¯æœ‰æ•ˆçš„ã€‚å‡­å€ŸåŸºæœ¬å®šç†çš„æ”¯æŒï¼Œæˆ‘ä»¬å®Œå…¨å¯ä»¥åƒåˆ†æä»»ä½•ä¼ ç»Ÿçš„äºŒåˆ†ç±»é—®é¢˜ä¸€æ ·åˆ†ææˆ˜ç•¥åˆ†ç±»é—®é¢˜ã€‚åœ¨æˆ‘çœ‹æ¥ï¼Œèƒ½å¤Ÿåˆ¤æ–­ä¸€ä¸ªæˆ˜ç•¥é—®é¢˜æ˜¯å¦å¯ä»¥ç†è®ºä¸Šå­¦ä¹ ï¼Œå®åœ¨æ˜¯éå¸¸äº†ä¸èµ·ï¼'
- en: Conclusion
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: We began by introducing the **canonical shattering coefficient and VC dimension,**
    which are central to PAC learning (and theFundamental Theorem of Statistical Learning).
    Next, we leveraged the *data point best response* to **generalize the aforementioned
    concepts so that they would work in our strategic setup.** We defined the **strategic
    VC dimension (SVC)** and showed that when faced with the impartial preference
    class, it degenerates back into the canonical VC dimension. Finally, **we demonstrated
    how SVC relates to strategic PAC learnability by means of the Fundamental Theorem
    of Strategic Learning.**
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆä»‹ç»äº†**æ ‡å‡†åŒ–çš„ç¢ç‰‡ç³»æ•°å’ŒVCç»´åº¦ï¼Œ** è¿™ä¸¤ä¸ªæ¦‚å¿µæ˜¯PACå­¦ä¹ ï¼ˆä»¥åŠç»Ÿè®¡å­¦ä¹ åŸºæœ¬å®šç†ï¼‰çš„æ ¸å¿ƒã€‚æ¥ç€ï¼Œæˆ‘ä»¬åˆ©ç”¨*æ•°æ®ç‚¹æœ€ä½³ååº”*æ¥**å°†ä¸Šè¿°æ¦‚å¿µæ¨å¹¿ï¼Œä½¿å…¶é€‚ç”¨äºæˆ‘ä»¬çš„æˆ˜ç•¥è®¾ç½®ã€‚**æˆ‘ä»¬å®šä¹‰äº†**æˆ˜ç•¥VCç»´åº¦ï¼ˆSVCï¼‰**å¹¶å±•ç¤ºäº†å½“é¢å¯¹å…¬æ­£åå¥½ç±»æ—¶ï¼Œå®ƒä¼šé€€åŒ–ä¸ºæ ‡å‡†çš„VCç»´åº¦ã€‚æœ€åï¼Œ**æˆ‘ä»¬é€šè¿‡æˆ˜ç•¥å­¦ä¹ åŸºæœ¬å®šç†å±•ç¤ºäº†SVCå¦‚ä½•ä¸æˆ˜ç•¥PACå¯å­¦ä¹ æ€§ç›¸å…³ã€‚**
- en: '**In the final article in this series, weâ€™ll build on the concepts Iâ€™ve introduced
    here to break down my favorite proof in the paper,** which I think is a beautiful
    illustration of strategic classification and SVC in action.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**åœ¨æœ¬ç³»åˆ—çš„æœ€åä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†åŸºäºæˆ‘åœ¨è¿™é‡Œä»‹ç»çš„æ¦‚å¿µï¼Œæ·±å…¥å‰–æè®ºæ–‡ä¸­æˆ‘æœ€å–œæ¬¢çš„è¯æ˜ï¼Œ**æˆ‘è®¤ä¸ºè¿™æ˜¯æˆ˜ç•¥åˆ†ç±»å’ŒSVCåº”ç”¨çš„ä¸€ä¸ªç¾ä¸½ä¾‹è¯ã€‚'
- en: References
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] R. Sundaram, A. Vullikanti, H. Xu, F. Yao. [PAC-Learning for Strategic
    Classification](https://arxiv.org/abs/2012.03310) (2021), International Conference
    on Machine Learning.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] R. Sundaram, A. Vullikanti, H. Xu, F. Yao. [PAC-å­¦ä¹ ä¸æˆ˜ç•¥åˆ†ç±»](https://arxiv.org/abs/2012.03310)
    (2021)ï¼Œå›½é™…æœºå™¨å­¦ä¹ ä¼šè®®ã€‚'
