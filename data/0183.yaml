- en: Evaluating LLMs in Cypher Statement Generation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Cypher语句生成中评估LLMs
- en: 原文：[https://towardsdatascience.com/evaluating-llms-in-cypher-statement-generation-c570884089b3?source=collection_archive---------2-----------------------#2024-01-19](https://towardsdatascience.com/evaluating-llms-in-cypher-statement-generation-c570884089b3?source=collection_archive---------2-----------------------#2024-01-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/evaluating-llms-in-cypher-statement-generation-c570884089b3?source=collection_archive---------2-----------------------#2024-01-19](https://towardsdatascience.com/evaluating-llms-in-cypher-statement-generation-c570884089b3?source=collection_archive---------2-----------------------#2024-01-19)
- en: Step-by-step tutorial for assessing the accuracy of generated Cypher Statements
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估生成的Cypher语句准确性的分步教程
- en: '[](https://bratanic-tomaz.medium.com/?source=post_page---byline--c570884089b3--------------------------------)[![Tomaz
    Bratanic](../Images/d5821aa70918fcb3fc1ff0013497b3d5.png)](https://bratanic-tomaz.medium.com/?source=post_page---byline--c570884089b3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c570884089b3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c570884089b3--------------------------------)
    [Tomaz Bratanic](https://bratanic-tomaz.medium.com/?source=post_page---byline--c570884089b3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://bratanic-tomaz.medium.com/?source=post_page---byline--c570884089b3--------------------------------)[![Tomaz
    Bratanic](../Images/d5821aa70918fcb3fc1ff0013497b3d5.png)](https://bratanic-tomaz.medium.com/?source=post_page---byline--c570884089b3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c570884089b3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c570884089b3--------------------------------)
    [Tomaz Bratanic](https://bratanic-tomaz.medium.com/?source=post_page---byline--c570884089b3--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c570884089b3--------------------------------)
    ·9 min read·Jan 19, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c570884089b3--------------------------------)
    ·阅读时间9分钟·2024年1月19日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/88372b2e2c33a680583b4fc6098d1e1a.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/88372b2e2c33a680583b4fc6098d1e1a.png)'
- en: Image generated by DALL-E
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由DALL-E生成的图像
- en: Shortly after large language models (LLMs) became popular, we realized they
    were decent at translating natural language to database queries such as SQL and
    Cypher. To enable the LLM to create tailored queries for your particular database,
    you must provide its schema and, optionally, a few example queries. With this
    information, the LLM can generate database queries based on natural language input.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）开始流行后不久，我们意识到它们在将自然语言转换为SQL和Cypher等数据库查询方面相当不错。为了使LLM能够为您的特定数据库生成定制的查询，您必须提供其架构，并可选地提供一些示例查询。有了这些信息，LLM可以根据自然语言输入生成数据库查询。
- en: While LLMs show great potential at translating natural language to database
    queries, they are far from perfect. Therefore, it is essential to understand how
    well they perform using an evaluation process. Luckily, the process of generating
    SQL statements has been researched by academia in studies like the [Spider](https://arxiv.org/abs/1809.08887).
    We will use the following metrics to evaluate the Cypher generation ability of
    LLMs.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LLMs在将自然语言转换为数据库查询方面展示了巨大的潜力，但它们仍然远非完美。因此，理解它们的表现如何，通过评估过程来评估其性能是至关重要的。幸运的是，生成SQL语句的过程已经在学术界得到了研究，例如[Spider](https://arxiv.org/abs/1809.08887)等研究。我们将使用以下指标来评估LLMs的Cypher生成能力。
- en: '**Jaro-Winkler**: This is a text similarity metric based on edit distance.
    We compare the produced Cypher query to a correct Cypher query, and measure how
    different the strings are, by how much one would have to edit one query to be
    the same as the other query'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Jaro-Winkler：** 这是一种基于编辑距离的文本相似度度量。我们将生成的Cypher查询与正确的Cypher查询进行比较，并通过编辑查询所需的差异，衡量两个字符串之间的不同程度。'
- en: '**Pass@1:** This score is 1.0 if the produced query returns the same results
    from the database as the correct Cypher would, and 0.0 otherwise.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pass@1：** 如果生成的查询从数据库中返回的结果与正确的Cypher语句相同，则得分为1.0，否则为0.0。'
- en: '**Pass@3:** Similar to Pass@1, but instead of generating 1 query, we generate
    3 queries. If any of them produce the same results as the correct query the score
    is 1.0, otherwise 0.0'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pass@3：** 与Pass@1类似，不同之处在于我们生成3个查询，而不是1个查询。如果其中任何一个查询的结果与正确的查询相同，则得分为1.0，否则为0.0。'
- en: '**Jaccard similarity:** It measures the Jaccard similarity between the response
    returned by the produced Cypher and the correct Cypher’s response. This metric
    is used in mind to capture examples where the model may return almost correct
    results.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Jaccard相似度：**它衡量由生成的Cypher语句返回的响应与正确的Cypher响应之间的Jaccard相似度。这个度量标准旨在捕捉模型可能返回几乎正确的结果的示例。'
- en: Code is available on [GitHub](https://github.com/tomasonjo/blogs/blob/master/llm/evaluating_cypher.ipynb)
    as was developed in collaboration with [Adam Schill Collberg](https://medium.com/u/8b125c49d121?source=post_page---user_mention--c570884089b3--------------------------------).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 代码可以在[GitHub](https://github.com/tomasonjo/blogs/blob/master/llm/evaluating_cypher.ipynb)上找到，这是与[Adam
    Schill Collberg](https://medium.com/u/8b125c49d121?source=post_page---user_mention--c570884089b3--------------------------------)合作开发的。
- en: As you can observe, the focus is on evaluating responses from the database and
    not the actual Cypher statement itself. One reason is that a Cypher statement
    can be written multiple ways to retrieve identical information. We don’t care
    which syntax the LLM prefers; we only care that it produces correct responses.
    Additionally, we don’t have a strong preference for how the LLM names the column
    in responses, and therefore, we don’t want to evaluate its column naming abilities,
    etc…
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所观察到的，重点是评估来自数据库的响应，而不是实际的Cypher语句本身。一个原因是，Cypher语句可以有多种写法来获取相同的信息。我们不关心LLM偏好哪种语法；我们只关心它是否能生成正确的响应。此外，我们对于LLM在响应中如何命名列没有强烈偏好，因此我们不希望评估它的列命名能力等……
- en: Test dataset
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试数据集
- en: The test dataset consists of question and relevant Cypher statement pairs.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 测试数据集由问题和相关的Cypher语句对组成。
- en: You can use an LLM to generate suggestions for the testing dataset. However,
    you need to manually validate the examples as the LLMs make mistakes and aren’t
    100% reliable. If they were, we wouldn’t need to test them anyway. As we are evaluating
    based on database results and not the Cypher statements themselves, we need to
    have a running database with relevant information that we can use. In this blog
    post, we will use the [recommendations project in Neo4j Sandbox](http://ndbox.neo4j.com/?usecase=recommendations).
    The recommendations project uses the [MovieLens dataset](https://grouplens.org/datasets/movielens/),
    which contains movies, actors, ratings, and more information. The recommendations
    project is also available as readonly access on the demo server, which means you
    don’t have to create a new database instance if you don’t want to.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用LLM来生成测试数据集的建议。然而，你需要手动验证这些示例，因为LLM可能会出错，并且并不是100%可靠。如果它们完全可靠，我们就不需要测试它们了。由于我们是基于数据库结果而不是Cypher语句本身来进行评估的，因此我们需要一个运行中的数据库，并且必须包含我们可以使用的相关信息。在这篇博客文章中，我们将使用[Neo4j
    Sandbox中的推荐项目](http://ndbox.neo4j.com/?usecase=recommendations)。该推荐项目使用了[MovieLens数据集](https://grouplens.org/datasets/movielens/)，该数据集包含了电影、演员、评分等信息。推荐项目也可以在演示服务器上以只读访问的形式使用，这意味着如果你不想创建新的数据库实例，完全不需要这么做。
- en: '![](../Images/62d91638c35854e38d31dbd85606726e.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62d91638c35854e38d31dbd85606726e.png)'
- en: Recommendations project graph schema. Image by the author.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐项目图谱模式。图像由作者提供。
- en: In this example, I used GPT-4 to come up with suggestions for the training dataset,
    and then went through them and corrected them where needed. We will use only 27
    testing pairs. In practice, you probably want to use at least a couple hundred
    examples.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我使用了GPT-4来为训练数据集生成建议，然后逐一检查并在需要的地方进行修正。我们将仅使用27个测试对。在实际应用中，你可能需要使用至少几百个示例。
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Generating Cypher statements
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成Cypher语句
- en: We will be using [LangChain](https://www.langchain.com/) to generate Cypher
    statements. The [Neo4jGraph](https://python.langchain.com/docs/use_cases/graph/graph_cypher_qa)
    object in LangChain establishes the connection to Neo4j and retrieves its schema
    information.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用[LangChain](https://www.langchain.com/)来生成Cypher语句。[Neo4jGraph](https://python.langchain.com/docs/use_cases/graph/graph_cypher_qa)对象在LangChain中建立与Neo4j的连接，并检索其模式信息。
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The schema contains node labels, their properties, and the corresponding relationships.
    Next, we will use the LangChain expression language to define a prompt sent to
    the LLM with instructions to translate the natural language to a Cypher statement
    that retrieves relevant information to answer the question. Visit the [official
    documentation](https://python.langchain.com/docs/expression_language/) for more
    details on the LangChain expression language.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 架构包含节点标签、它们的属性以及相应的关系。接下来，我们将使用LangChain表达式语言定义一个提示，发送给LLM，指示它将自然语言翻译为Cypher语句，以检索相关信息来回答问题。欲了解有关LangChain表达式语言的更多详情，请访问[官方文档](https://python.langchain.com/docs/expression_language/)。
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If you are familiar with conversational LLMs, you can spot the **system** and
    **human** message definitions. As you can observe, we put both the graph schema
    as well as user question into the human message. The exact prompt engineering
    instructions to generate Cypher statements is not a solved problem, which means
    that there could be some improvements made here. Using an evaluation process,
    you could see what works best for the particular LLM. In this example, we are
    using **gpt-4-turbo**.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉对话型LLM，你可以识别出**系统**和**用户**消息定义。正如你所观察到的，我们将图形架构和用户问题都放入了用户消息中。生成Cypher语句的具体提示工程指令仍未解决，这意味着这里可能还有改进空间。通过评估过程，你可以看到哪种方法对特定的LLM效果最佳。在这个例子中，我们使用的是**gpt-4-turbo**。
- en: 'We can test the Cypher generation with the following example:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下示例来测试Cypher生成：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We can observe that gpt-4-turbo is somewhat decent at translating natural language
    to Cypher statements. Let’s now define the evaluation process.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到，gpt-4-turbo在将自然语言翻译为Cypher语句方面表现得相当不错。现在让我们定义评估过程。
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*Running this code took about 5 minutes as we need to generate 81 responses
    to calculate the pass@3 metric.*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*运行此代码大约花费了5分钟，因为我们需要生成81个响应来计算pass@3度量。*'
- en: The code is slightly lengthy. However, the gist is quite simple to understand.
    We iterate over all the rows in the data frame that stores the testing examples.
    Next, we generate three Cypher queries for each training example and retrieve
    corresponding data from the database. What follows is then calculating the relevant
    metrics and storing them in lists so that we can evaluate and visualize them.
    We didn’t include the helper functions in the blog post because, in my opinion,
    reviewing each metric code implementation isn’t relevant. All of these functions,
    however, are included in the [notebook](https://github.com/tomasonjo/blogs/blob/master/llm/evaluating_cypher.ipynb).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 代码稍长，但其核心概念相当简单。我们遍历存储测试示例的数据框中的所有行。接下来，我们为每个训练示例生成三个Cypher查询，并从数据库中检索相应的数据。接下来就是计算相关的度量标准并将其存储在列表中，以便我们可以评估并可视化它们。我们没有在博客文章中包括辅助函数，因为我认为逐个审查每个度量标准的代码实现没有必要。然而，所有这些函数都包含在[笔记本](https://github.com/tomasonjo/blogs/blob/master/llm/evaluating_cypher.ipynb)中。
- en: Let’s now evaluate the results.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们评估结果。
- en: '![](../Images/8aaa45fbcae5ebb94cecbd739f9b8a76.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8aaa45fbcae5ebb94cecbd739f9b8a76.png)'
- en: Evaluation of LLM-generated Cypher statements. Image by the author.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: LLM生成的Cypher语句评估。图片由作者提供。
- en: 'The evaluation is based on four different metrics:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 评估基于四种不同的度量标准：
- en: '**Jaro-Winkler**: This metric shows a high average of 0.89, indicating that
    the LLMs generated Cypher queries that are very similar to the correct Cypher
    queries on a string level.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Jaro-Winkler**：该度量显示了0.89的高平均值，表明LLM生成的Cypher查询在字符串层面上与正确的Cypher查询非常相似。'
- en: '**Pass@1**: The average score here is 0.48, suggesting that nearly half of
    the generated Cypher queries returned the exact same results as the correct query
    when each query is evaluated independently.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Pass@1**：此处的平均得分为0.48，表明当每个查询独立评估时，几乎有一半生成的Cypher查询返回与正确查询完全相同的结果。'
- en: '**Pass@3**: With an average of 0.63, this metric indicates an improvement over
    Pass@1\. This suggests that while the LLM may not always get the query right on
    the first attempt, it often comes up with a correct version within three tries.'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Pass@3**：该度量的平均值为0.63，表明相较于Pass@1有所提高。这意味着虽然LLM在第一次尝试时可能不会生成正确的查询，但通常会在三次尝试内生成一个正确版本。'
- en: '**Jaccard Similarity**: The average score of 0.53 is the lowest among the metrics
    but still indicates that more than half of the time, the sets of results from
    the LLM-generated Cypher queries share more than half of their elements with the
    sets from the correct queries.'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Jaccard相似度**：平均得分为0.53，是所有指标中最低的，但仍然表明大多数情况下，LLM生成的Cypher查询的结果集与正确查询的结果集共享超过一半的元素。'
- en: 'Overall, these metrics suggest that LLMs are decent at generating Cypher queries
    that are similar to the correct ones and often produce functionally equivalent
    results, especially when given multiple attempts. However, there is still room
    for improvement, particularly in generating the correct query on the first attempt.
    Additionally there is also room for improvement in the evaluation process. Let’s
    take a look at one example:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这些指标表明LLM在生成与正确查询相似的Cypher查询方面表现不错，并且通常能够生成功能等效的结果，尤其是在多次尝试后。然而，仍然有改进的空间，特别是在第一次尝试时生成正确查询的能力方面。此外，评估过程本身也有改进的余地。让我们来看一个例子：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: For the question of which directors never had a movie with a rating below 6.0,
    the LLM did a decent job of getting the results. It used a different approach
    than in the test dataset, but that’s not a problem, as it should get the same
    results. However, we returned both the title and the movie’s rating in the testing
    data. On the other hand, the LLM produced only the titles and not the ratings.
    We can’t blame it, as it simply followed the instructions. Still, you must know
    that the pass@1 score is 0 in this example, while the Jaccard similarity is only
    0.5\. Therefore, you have to very careful how you construct the testing dataset,
    both how you define the prompts as well as the corresponding Cypher statements.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于问题“哪些导演从未拍过评分低于6.0的电影”，LLM在获取结果方面表现不错。它使用了与测试数据集不同的方法，但这不是问题，因为它应该得到相同的结果。然而，我们在测试数据中返回了电影的标题和评分。另一方面，LLM只返回了标题，而没有评分。我们不能责怪它，因为它只是按照指令执行的。然而，你必须知道，在这个例子中，Pass@1得分为0，而Jaccard相似度仅为0.5。因此，在构建测试数据集时，你必须非常小心，既要注意如何定义提示，也要注意相应的Cypher语句。
- en: Another characteristic of LLMs is that they are non-deterministic, meaning you
    can get different results on every run. Let’s run the evaluation three times in
    sequence now. This evaluation takes around 15 minutes.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的另一个特点是它们是非确定性的，这意味着每次运行可能会得到不同的结果。现在我们将连续运行三次评估。这项评估大约需要15分钟。
- en: '![](../Images/88a9e606be5ed7980e371f818c34ad89.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/88a9e606be5ed7980e371f818c34ad89.png)'
- en: Evaluation of LLM-generated Cypher statements. Image by the author.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对LLM生成的Cypher语句的评估。图片来自作者。
- en: The bar chart highlights the non-deterministic nature of LLMs. The Jaro-Winkler
    scores are consistently high across all runs, showing minor fluctuations between
    0.88 and 0.89, which indicates stable string similarity of the generated queries.
    However, for Pass@1, there’s a notable variation, with the first run scoring 0.52,
    and subsequent runs showing scores of 0.59 and 0.48\. Pass@3 scores exhibit less
    variance, hovering around 0.56 to 0.63, suggesting that multiple attempts yield
    more consistent correct results.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 条形图突出显示了LLM的非确定性特性。Jaro-Winkler分数在所有运行中始终较高，波动幅度在0.88和0.89之间，表明生成的查询在字符串相似度上保持稳定。然而，对于Pass@1，有明显的变化，第一次运行得分为0.52，后续运行得分分别为0.59和0.48。Pass@3得分变化较小，约为0.56到0.63之间，表明多次尝试能够获得更一致的正确结果。
- en: Conclusion
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: Through this blog post, we’ve learned that LLMs like GPT-4 have a promising
    capacity for generating Cypher queries, yet the technology isn’t foolproof. The
    evaluation framework presented offers a detailed, quantitative evaluation of LLM
    performance, allowing you to continuously experiment and update prompt engineering
    and other steps needed to generate valid and accurate Cypher statements. Additionally,
    it shows how the non-deterministic nature of LLMs affects the performance from
    one evaluation to another. Therefore, you can expect similar non-deterministic
    behavior in production.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这篇博客文章，我们了解到像GPT-4这样的LLM在生成Cypher查询方面有着很有前景的能力，但该技术并非万无一失。所展示的评估框架提供了LLM性能的详细定量评估，允许你不断地进行实验，并更新提示工程及生成有效准确的Cypher语句所需的其他步骤。此外，它还展示了LLM的非确定性特性如何影响不同评估之间的表现。因此，你可以预期在生产环境中会遇到类似的非确定性行为。
- en: The code is available on [GitHub](https://github.com/tomasonjo/blogs/blob/master/llm/evaluating_cypher.ipynb).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 代码可在[GitHub](https://github.com/tomasonjo/blogs/blob/master/llm/evaluating_cypher.ipynb)上找到。
- en: Dataset
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: 'F. Maxwell Harper and Joseph A. Konstan. 2015\. The MovieLens Datasets: History
    and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4:
    19:1–19:19\. [https://doi.org/10.1145/2827872](https://doi.org/10.1145/2827872)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: F. Maxwell Harper 和 Joseph A. Konstan. 2015年. 《MovieLens 数据集：历史与背景》. 《ACM互动智能系统学报》（TiiS）5卷，4期：19:1–19:19.
    [https://doi.org/10.1145/2827872](https://doi.org/10.1145/2827872)
