- en: Increasing Transformer Model Efficiency Through Attention Layer Optimization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提高Transformer模型效率：通过优化注意力层
- en: 原文：[https://towardsdatascience.com/increasing-transformer-model-efficiency-through-attention-layer-optimization-fefa6f87b1d6?source=collection_archive---------2-----------------------#2024-11-18](https://towardsdatascience.com/increasing-transformer-model-efficiency-through-attention-layer-optimization-fefa6f87b1d6?source=collection_archive---------2-----------------------#2024-11-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/increasing-transformer-model-efficiency-through-attention-layer-optimization-fefa6f87b1d6?source=collection_archive---------2-----------------------#2024-11-18](https://towardsdatascience.com/increasing-transformer-model-efficiency-through-attention-layer-optimization-fefa6f87b1d6?source=collection_archive---------2-----------------------#2024-11-18)
- en: How paying “better” attention can drive ML cost savings
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何通过“更好”地关注来推动机器学习成本节省
- en: '[](https://chaimrand.medium.com/?source=post_page---byline--fefa6f87b1d6--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page---byline--fefa6f87b1d6--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fefa6f87b1d6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fefa6f87b1d6--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page---byline--fefa6f87b1d6--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://chaimrand.medium.com/?source=post_page---byline--fefa6f87b1d6--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page---byline--fefa6f87b1d6--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fefa6f87b1d6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fefa6f87b1d6--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page---byline--fefa6f87b1d6--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fefa6f87b1d6--------------------------------)
    ·13 min read·Nov 18, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fefa6f87b1d6--------------------------------)
    ·13分钟阅读·2024年11月18日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/392d04054c201f7813635598c7dd5502.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/392d04054c201f7813635598c7dd5502.png)'
- en: Photo by [Andrew Seaman](https://unsplash.com/@amseaman?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Andrew Seaman](https://unsplash.com/@amseaman?utm_source=medium&utm_medium=referral)拍摄，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduced in the landmark 2017 paper [*“Attention Is All You Need”*](https://arxiv.org/abs/1706.03762)
    (Vaswani et al., 2017), the Transformer architecture is widely regarded as one
    of the most influential scientific breakthroughs of the past decade. At the core
    of the Transformer is the attention mechanism, a novel approach that enables AI
    models to comprehend complex structures by focusing on different parts of input
    sequences based on the task at hand. Originally demonstrated in the world of natural
    language processing, the success of the Transformer architecture has quickly spread
    to many other domains, including speech recognition, scene understanding, reinforcement
    learning, protein structure prediction, and more. However, attention layers are
    highly resource-intensive, and as these layers become the standard across increasingly
    large models, the costs associated with their training and deployment have surged.
    This has created an urgent need for strategies that reduce the computational cost
    of this core layer so as to increase the efficiency and scalability of Transformer-based
    AI models.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在2017年具有里程碑意义的论文[*“Attention Is All You Need”*](https://arxiv.org/abs/1706.03762)（Vaswani等人，2017）中首次提出，Transformer架构被广泛认为是过去十年最具影响力的科学突破之一。Transformer的核心是注意力机制，这是一种新颖的方法，使得人工智能模型能够通过根据手头任务集中关注输入序列的不同部分，来理解复杂的结构。最初在自然语言处理领域展示成功后，Transformer架构的成功迅速扩展到许多其他领域，包括语音识别、场景理解、强化学习、蛋白质结构预测等。然而，注意力层是高度资源密集型的，随着这些层成为越来越大模型的标准，其训练和部署的成本也急剧上升。这催生了减少这一核心层计算成本的迫切需求，以提高基于Transformer的人工智能模型的效率和可扩展性。
- en: In this post, we will explore several tools for optimizing attention in [PyTorch](https://pytorch.org/).
    Our focus will be on methods that maintain the accuracy of the attention layer.
    These will include [PyTorch SDPA](https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html),
    [FlashAttention](https://pytorch.org/blog/flashattention-3/), [TransformerEngine](https://github.com/NVIDIA/TransformerEngine)
    Attention, [FlexAttention](https://pytorch.org/blog/flexattention/), and [xFormer](https://github.com/facebookresearch/xformers)
    attention. Other methods that reduce the computational cost via approximation
    of the attention calculation (e.g., [DeepSpeed’s Sparse Attention](https://www.deepspeed.ai/tutorials/sparse-attention/),
    [Longformer](https://github.com/allenai/longformer), [Linformer](https://arxiv.org/abs/2006.04768),
    and more) will not be considered. Additionally, we will not discuss general optimization
    techniques that, while beneficial to attention performance, are not specific to
    the attention computation itself (e.g., [FP8 training](/accelerating-pytorch-training-workloads-with-fp8-5a5123aec7d7),
    [model sharding](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/),
    and [more](/pytorch-model-performance-analysis-and-optimization-10c3c5822869)).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将探索几个用于优化[PyTorch](https://pytorch.org/)中注意力的工具。我们将重点介绍那些保持注意力层准确性的方法。包括[PyTorch
    SDPA](https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html)、[FlashAttention](https://pytorch.org/blog/flashattention-3/)、[TransformerEngine](https://github.com/NVIDIA/TransformerEngine)注意力、[FlexAttention](https://pytorch.org/blog/flexattention/)和[xFormer](https://github.com/facebookresearch/xformers)注意力。其他通过近似注意力计算来减少计算成本的方法（例如，[DeepSpeed的稀疏注意力](https://www.deepspeed.ai/tutorials/sparse-attention/)、[Longformer](https://github.com/allenai/longformer)、[Linformer](https://arxiv.org/abs/2006.04768)等）将不予考虑。此外，我们也不会讨论那些对注意力性能有益，但与注意力计算本身无关的通用优化技术（例如，[FP8训练](/accelerating-pytorch-training-workloads-with-fp8-5a5123aec7d7)、[模型分片](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)和[更多](/pytorch-model-performance-analysis-and-optimization-10c3c5822869)）。
- en: Importantly, attention optimization is an active area of research with new methods
    coming out on a pretty regular basis. Our goal is to increase your awareness of
    some of the existing solutions and provide you with a foundation for further exploration
    and experimentation. The code we will share below is intended for demonstrative
    purposes only — we make no claims regarding its accuracy, optimality, or robustness.
    Please do not interpret our mention of any platforms, libraries, or optimization
    techniques as an endorsement for their use. The best options for you will depend
    greatly on the specifics of your own use-case.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，注意力优化是一个活跃的研究领域，新方法几乎定期发布。我们的目标是提高你对一些现有解决方案的认知，并为进一步探索和实验提供基础。我们下面分享的代码仅用于演示目的——我们并不对其准确性、最优性或健壮性做出任何声明。请不要将我们提到的任何平台、库或优化技术视为对其使用的认可。最适合你的选项将很大程度上依赖于你自己用例的具体情况。
- en: Many thanks to [Yitzhak Levi](https://www.linkedin.com/in/yitzhak-levi-49a217201/)
    for his contributions to this post.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢[伊兹哈克·莱维](https://www.linkedin.com/in/yitzhak-levi-49a217201/)对本文的贡献。
- en: Toy Model
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 玩具模型
- en: To facilitate our discussion, we build a [Vision Transformer](https://en.wikipedia.org/wiki/Vision_transformer)
    (ViT)-backed classification model using the popular [timm](https://pypi.org/project/timm/)
    Python package (version 0.9.7). We will use this model to illustrate the performance
    impact of various attention kernels.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于讨论，我们使用流行的[timm](https://pypi.org/project/timm/) Python包（版本0.9.7）构建了一个基于[视觉Transformer](https://en.wikipedia.org/wiki/Vision_transformer)（ViT）的分类模型。我们将使用这个模型来说明各种注意力内核对性能的影响。
- en: We start by defining a simplified Transformer block that allows for programming
    the attention function by passing it into its constructor. Since attention implementations
    assume specific input tensor formats, we also include an option for controlling
    the format, ensuring compatibility with the attention kernel of our choosing.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义了一个简化的Transformer块，通过将注意力函数传递给其构造函数来进行编程。由于注意力实现假设特定的输入张量格式，我们还提供了一个选项来控制格式，确保与我们选择的注意力内核兼容。
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We define a randomly generated dataset which we will use to feed to our model
    during training.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个随机生成的数据集，我们将在训练过程中将其输入到我们的模型中。
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, we define our ViT training function. While our example focuses on demonstrating
    a *training* workload, it is crucial to emphasize that optimizing the attention
    layer is equally, if not more, important during model *inference*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义我们的ViT训练函数。尽管我们的示例主要演示了*训练*工作负载，但必须强调的是，在模型*推理*过程中优化注意力层同样重要，甚至更为重要。
- en: The training function we define accepts the customized Transformer block and
    a flag that controls the use of [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义的训练函数接受定制的Transformer块和一个控制是否使用[torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)的标志。
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In the code block below we define a PyTorch-native attention function and use
    it to train our ViT model:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码块中，我们定义了一个PyTorch原生的注意力函数，并用它来训练我们的ViT模型：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We ran this on an [NVIDIA H100](https://www.nvidia.com/en-eu/data-center/h100/)
    with [CUDA 12.4](https://developer.nvidia.com/cuda-toolkit) and [PyTorch](https://pytorch.org/)
    2.5.1\. The uncompiled variant resulted in an average step time of 370 milliseconds
    (ms), while the compiled variant improved to 242 ms. We will use these results
    as a baseline for comparison as we consider alternative solutions for performing
    the attention computation.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在一台配备[NVIDIA H100](https://www.nvidia.com/en-eu/data-center/h100/)的机器上运行此测试，使用[CUDA
    12.4](https://developer.nvidia.com/cuda-toolkit)和[PyTorch](https://pytorch.org/)
    2.5.1。未经编译的变体平均步骤时间为370毫秒（ms），而编译后的变体提高到242毫秒。我们将使用这些结果作为基准，在考虑其他执行注意力计算的解决方案时进行对比。
- en: PyTorch SDPA
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch SDPA
- en: 'One of the easiest ways to boost the performance of our attention layers in
    PyTorch is to use the [scaled_dot_product_attention](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)
    (SDPA) function. Currently in beta, PyTorch SDPA consolidates multiple kernel-level
    optimizations and dynamically selects the most efficient one based on the input''s
    properties. Supported backends (as of now) include: [FlashAttention-2](https://arxiv.org/abs/2307.08691),
    [Memory-Efficient Attention](https://github.com/facebookresearch/xformers), a
    C++-based Math Attention, and [CuDNN](https://pytorch.org/blog/pytorch2-5/#beta-cudnn-backend-for-sdpa).
    These backends fuse together high-level operations while employing GPU-level optimizations
    for increasing compute efficiency and memory utilization.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 提升PyTorch中注意力层性能的最简单方法之一是使用[scaled_dot_product_attention](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)（SDPA）函数。目前在beta阶段，PyTorch
    SDPA整合了多个内核级优化，并根据输入的特性动态选择最有效的优化方法。支持的后端（截至目前）包括：[FlashAttention-2](https://arxiv.org/abs/2307.08691)，[Memory-Efficient
    Attention](https://github.com/facebookresearch/xformers)，一个基于C++的数学注意力，和[CuDNN](https://pytorch.org/blog/pytorch2-5/#beta-cudnn-backend-for-sdpa)。这些后端将高级操作融合在一起，同时利用GPU级优化来提高计算效率和内存利用率。
- en: SDPA is continuously evolving, with new and improved backend implementations
    being introduced regularly. Staying up to date with the latest PyTorch releases
    is key to leveraging the most recent performance improvements. For example, [PyTorch
    2.5](https://pytorch.org/blog/pytorch2-5/) introduced an updated [CuDNN backend](https://pytorch.org/blog/pytorch2-5/#beta-cudnn-backend-for-sdpa)
    featuring a specialized [SDPA primitive](https://developer.nvidia.com/blog/accelerating-transformers-with-nvidia-cudnn-9/)
    specifically tailored for training on [NVIDIA Hopper architecture](https://www.nvidia.com/en-us/data-center/technologies/hopper-architecture/)
    GPUs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: SDPA正在不断发展，新的和改进的后端实现定期推出。保持跟进最新的PyTorch发布版本是利用最新性能改进的关键。例如，[PyTorch 2.5](https://pytorch.org/blog/pytorch2-5/)引入了更新的[CuDNN后端](https://pytorch.org/blog/pytorch2-5/#beta-cudnn-backend-for-sdpa)，其中包含一个专门为[NVIDIA
    Hopper架构](https://www.nvidia.com/en-us/data-center/technologies/hopper-architecture/)
    GPU训练量身定制的[SDPA原语](https://developer.nvidia.com/blog/accelerating-transformers-with-nvidia-cudnn-9/)。
- en: 'In the code block below, we iterate through the list of supported backends
    and assess the runtime performance of training with each one. We use a helper
    function, *set_sdpa_backend*, for programming the SDPA backend:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码块中，我们遍历支持的后端列表，并评估每个后端的训练运行时性能。我们使用一个辅助函数，*set_sdpa_backend*，来编程SDPA后端：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We summarize our interim results in the table below
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在下面的表格中总结了我们的阶段性结果
- en: '![](../Images/7f98ef38f1ec30988ed4b79706748b20.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7f98ef38f1ec30988ed4b79706748b20.png)'
- en: Step times for various attention functions (lower is better) — by Author
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 各种注意力函数的步骤时间（越低越好）— 按作者分类
- en: While the choice of SDPA backend has a noticeable impact on performance when
    running in eager mode, the optimizations performed by [model compilation](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)
    appear to overshadow the differences between the attention kernels. Once again,
    we caution against deriving any conclusions from these results as the performance
    impact of different attention functions can vary significantly depending on the
    specific model and use case.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在 Eager 模式下选择 SDPA 后端会对性能产生显著影响，但通过 [模型编译](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)
    进行的优化似乎掩盖了不同注意力内核之间的差异。再次提醒，我们不应仅从这些结果得出结论，因为不同注意力函数的性能影响可能会根据特定的模型和使用场景而有所不同。
- en: Third-Party Attention Kernels
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三方注意力内核
- en: While PyTorch SDPA is a great place to start, using third-party attention kernels
    can help accelerate your ML workloads further. These alternatives often come with
    added flexibility, offering a wider range of configuration options for attention.
    Some may also include optimizations tailored for specific hardware accelerators
    or newer GPU architectures.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 PyTorch SDPA 是一个很好的起点，但使用第三方注意力内核可以进一步加速你的 ML 工作负载。这些替代方案通常提供更多的灵活性，提供更广泛的注意力配置选项。某些方案还可能包括针对特定硬件加速器或新一代
    GPU 架构的优化。
- en: In this section, we will explore some of the third-party attention kernels available
    and evaluate their potential impact on runtime performance.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨一些可用的第三方注意力核，并评估它们对运行时性能的潜在影响。
- en: FlashAttention-3
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlashAttention-3
- en: While Pytorch SDPA supports a [FlashAttention](https://arxiv.org/abs/2307.08691)
    backend, more advanced FlashAttention implementations can be found in the [flash-attn](https://pypi.org/project/flash-attn/)
    library. Here we will explore the [FlashAttention-3](https://pytorch.org/blog/flashattention-3/)
    beta release which boasts a speed of up to 2x compared to FlashAttention-2\. Given
    the early stage in its development, FlashAttention-3 can only be installed directly
    from the [GitHub repository](https://github.com/HazyResearch/flash-attention)
    and its use is limited to certain head dimensions. Additionally, it does not yet
    support model compilation. In the following code block, we configure our transformer
    block to use flash-attn-3 while setting the attention input format to “bshd” (batch,
    sequence, head, depth) to meet the expectations of the library.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Pytorch SDPA 支持 [FlashAttention](https://arxiv.org/abs/2307.08691) 后端，但更先进的
    FlashAttention 实现可以在 [flash-attn](https://pypi.org/project/flash-attn/) 库中找到。在这里，我们将探讨
    [FlashAttention-3](https://pytorch.org/blog/flashattention-3/) 的测试版发布，它的速度比 FlashAttention-2
    快 2 倍。鉴于其开发处于早期阶段，FlashAttention-3 只能直接从 [GitHub 仓库](https://github.com/HazyResearch/flash-attention)
    安装，并且其使用仅限于特定的头维度。此外，它尚不支持模型编译。在下面的代码块中，我们将配置我们的 Transformer 块以使用 flash-attn-3，并将注意力输入格式设置为
    "bshd"（batch，sequence，head，depth），以符合该库的要求。
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The resultant step time was 240 ms, making it 5% faster than the SDPA flash-attn.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的步骤时间为 240 毫秒，比 SDPA flash-attn 快 5%。
- en: Transformer Engine
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer Engine
- en: '[Transformer Engine](https://github.com/NVIDIA/TransformerEngine) (TE) is a
    specialized library designed to accelerate Transformer models on NVIDIA GPUs.
    TE is updated regularly with optimizations that leverage the capabilities of the
    latest NVIDIA hardware and software offerings, giving users access to specialized
    kernels long before they are integrated into general-purpose frameworks such as
    PyTorch.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[Transformer Engine](https://github.com/NVIDIA/TransformerEngine)（TE）是一个专门设计的库，用于加速
    NVIDIA GPU 上的 Transformer 模型。TE 定期更新，采用优化技术，充分利用最新 NVIDIA 硬件和软件的能力，使用户能够在这些功能被集成到通用框架如
    PyTorch 之前，便可以访问到专门的内核。'
- en: In the code block below we use [DotProductAttention](https://github.com/NVIDIA/TransformerEngine/blob/main/transformer_engine/pytorch/attention.py#L7271)
    from [TE version 1.11.0](https://pypi.org/project/transformer-engine/). Similar
    to PyTorch SDPA, TE supports a number of backends which are controlled via environment
    variables. Here we demonstrate the use of the *NVTE_FUSED_ATTN* backend.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码块中，我们使用了来自 [TE 版本 1.11.0](https://pypi.org/project/transformer-engine/)
    的 [DotProductAttention](https://github.com/NVIDIA/TransformerEngine/blob/main/transformer_engine/pytorch/attention.py#L7271)。与
    PyTorch SDPA 类似，TE 支持多种后端，这些后端通过环境变量控制。在此我们演示了 *NVTE_FUSED_ATTN* 后端的使用。
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: TE attention resulted in average step times of 243 ms and 204 ms for the eager
    and compiled model variants, correspondingly.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: TE 注意力在 Eager 和编译模型变体中的平均步骤时间分别为 243 毫秒和 204 毫秒。
- en: XFormer Attention
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XFormer 注意力
- en: Underlying the memory-efficient backend of PyTorch SDPA is an attention kernel
    provided by the [xFormers](https://github.com/facebookresearch/xformers/tree/main)
    library. Once again, we can go to the source to benefit from the latest kernel
    optimizations and from the full set of API capabilities. In the following code
    block we use the [memory_efficient_attention](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)
    operator from [xFormers version 0.0.28](https://pypi.org/project/xformers/).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch SDPA 的内存高效后端基础是由 [xFormers](https://github.com/facebookresearch/xformers/tree/main)
    库提供的注意力内核。我们可以再次访问源代码，以便受益于最新的内核优化和完整的 API 功能。在以下代码块中，我们使用了来自 [xFormers 版本 0.0.28](https://pypi.org/project/xformers/)
    的 [memory_efficient_attention](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)
    操作符。
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This eager model variant resulted in an average step time of 246 ms, making
    it 10.5% faster than the SDPA memory efficient kernel. The compiled variant resulted
    in a step time of 203 ms.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这种急切模型变体的平均步长时间为 246 毫秒，比 SDPA 内存高效内核快了 10.5%。编译版的步长时间为 203 毫秒。
- en: Results
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: 'The table below summarizes our experiments:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格总结了我们的实验：
- en: '![](../Images/8e3051e670033c5e99bcdf855549632c.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8e3051e670033c5e99bcdf855549632c.png)'
- en: Step times for various attention functions (lower is better) — by Author
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 各种注意力函数的步长时间（较低的值更好）— 作者
- en: The winner for the eager model was flash-attn-3 with an average step time that
    is 54% faster than our baseline model. This translates to a similar 54% reduction
    in training costs. In compiled mode, the performance across the optimized kernels
    was more or less equal, with the fastest implementations achieving 202 ms, representing
    a 20% improvement compared to the baseline experiment.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于急切模型，获胜者是 flash-attn-3，其平均步长时间比我们的基准模型快了 54%。这相当于训练成本减少了 54%。在编译模式下，优化内核的性能大致相等，最快的实现达到了
    202 毫秒，比基准实验提升了 20%。
- en: As mentioned above, the precise impact savings is greatly dependent on the model
    definition. To assess this variability, we reran the experiments using modified
    settings that increased the attention sequence length to 3136 tokens.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，节省的精确影响在很大程度上取决于模型定义。为了评估这种变化性，我们使用修改后的设置重新进行了实验，将注意力序列长度增加到 3136 个标记。
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The results are summarized in the table below:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 结果总结在下面的表格中：
- en: '![](../Images/54ce4ff619bd4657bb51114e22be1772.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/54ce4ff619bd4657bb51114e22be1772.png)'
- en: Results for large seqlen (lower is better) — by Author
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模序列长度的结果（较低的值更好）— 作者
- en: Our immediate observation is that when the sequence length is greater the performance
    impact of the attention kernels is far more pronounced. Once again, flash-attn-3
    came out in front for the eager execution mode — this time with a ~5x increase
    in performance compared to the PyTorch-native function. For the compiled model
    we see that the TE kernel broke away from the pack with an overall best step-time
    of 53 ms.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的初步观察是，当序列长度较大时，注意力内核的性能影响更为明显。再次，flash-attn-3 在急切执行模式下领先，这一次相比于 PyTorch 本地函数性能提升约
    5 倍。对于编译后的模型，我们看到 TE 内核在整体最优步长时间 53 毫秒的表现下脱颖而出。
- en: Customizing Attention with FlexAttention
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 FlexAttention 自定义注意力机制
- en: 'Thus far, we’ve focused on the standard attention function. However, sometimes
    we may want to use a variant of the typical attention computation in which we
    either mask out some of the values of intermediate tensors or apply some operation
    on them. These types of changes may interfere with our ability to use the optimized
    attention blocks we covered above. In this section we discuss some of the ways
    to address this:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们主要关注了标准的注意力函数。然而，有时我们可能希望使用典型注意力计算的变体，其中我们要么屏蔽掉部分中间张量的值，要么对它们应用某些操作。这类变化可能会干扰我们使用前面介绍的优化注意力块的能力。在本节中，我们将讨论一些解决方法：
- en: '**Leverage Advanced Kernel APIs**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**利用高级内核 API**'
- en: Many optimized attention kernels provide extensive APIs with controls for customizing
    the attention computation. Before implementing a new solution, explore these APIs
    to determine if they already support your required functionality.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 许多优化过的注意力内核提供了广泛的 API，允许定制注意力计算。在实现新方案之前，请先探索这些 API，看看它们是否已经支持所需的功能。
- en: '**Implement a custom kernel:** If the existing APIs do not meet your needs,
    you could consider creating your own custom attention implementation. In previous
    posts (e.g., [here](/unleashing-the-power-of-triton-mastering-gpu-kernel-optimization-in-python-160a3f52701e))
    we discussed some of the pros and cons of custom kernel development. Achieving
    optimal performance can be extremely difficult. If you do go down this path, one
    approach might be to start with an existing (optimal) kernel and apply minimal
    changes to integrate the desired change.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**实现自定义内核：** 如果现有的 API 无法满足你的需求，可以考虑创建自己的自定义注意力实现。在之前的文章中（例如，[这里](/unleashing-the-power-of-triton-mastering-gpu-kernel-optimization-in-python-160a3f52701e)）我们讨论了自定义内核开发的一些优缺点。实现最佳性能可能非常困难。如果你选择这条路径，一种方法可能是从现有的（最佳）内核开始，进行最小的修改以集成所需的变化。'
- en: '**Use FlexAttention:** A recent addition to PyTorch, [FlexAttention](https://pytorch.org/blog/flexattention/)
    empowers users to implement a wide variety of attention variants without needing
    to compromise on performance. Denoting the result of the dot product of the query
    and key tokens by *score*, [flex_attention](https://github.com/pytorch/pytorch/blob/v2.5.1/torch/nn/attention/flex_attention.py#L927)
    allows for programming either a [*score_mod*](https://pytorch.org/blog/flexattention/#score-mod-examples)
    function or a [*block_mask*](https://pytorch.org/blog/flexattention/#mask-mods)
    mask that is automatically applied to the *score* tensor. See the [documentation](https://pytorch.org/blog/flexattention/)
    as well as the accompanying [attention-gym](https://github.com/pytorch-labs/attention-gym)
    repository for examples of the types of operations that the API enables.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用 FlexAttention：** PyTorch 最近新增了 [FlexAttention](https://pytorch.org/blog/flexattention/)，它使用户能够实现各种各样的注意力变体，而无需在性能上做出妥协。用
    *score* 表示查询和键令牌的点积结果，[flex_attention](https://github.com/pytorch/pytorch/blob/v2.5.1/torch/nn/attention/flex_attention.py#L927)
    允许编程一个 [*score_mod*](https://pytorch.org/blog/flexattention/#score-mod-examples)
    函数或一个自动应用于 *score* 张量的 [*block_mask*](https://pytorch.org/blog/flexattention/#mask-mods)
    屏蔽。请参见 [文档](https://pytorch.org/blog/flexattention/) 和附带的 [attention-gym](https://github.com/pytorch-labs/attention-gym)
    仓库，了解 API 支持的操作类型示例。'
- en: '[FlexAttention](https://pytorch.org/blog/flexattention/) works by [compiling](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)
    the *score_mod* operator into the attention operator, thereby creating a single
    fused kernel. It also leverages the sparsity of *block_masks* to avoid unnecessary
    computations. The [benchmarks](https://pytorch.org/blog/flexattention/#performance)
    reported in the [FlexAttention](https://pytorch.org/blog/flexattention/) documentation
    show considerable performance gains for a variety of use cases.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[FlexAttention](https://pytorch.org/blog/flexattention/) 通过将 *score_mod* 操作符
    [编译](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) 到注意力操作符中，从而创建一个单一的融合内核。它还利用
    *block_masks* 的稀疏性来避免不必要的计算。[FlexAttention](https://pytorch.org/blog/flexattention/)
    文档中报告的 [基准测试](https://pytorch.org/blog/flexattention/#performance) 显示了在多种使用案例中取得了显著的性能提升。'
- en: Let’s see both the *score_mod* and *block_mask* in action.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下 *score_mod* 和 *block_mask* 的实际应用。
- en: Score Mod Example — Soft-Capping with Tanh
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Score Mod 示例 — 使用 Tanh 进行软限制
- en: 'Soft-capping is a common technique used to control the logit sizes (e.g., see
    [here](https://arxiv.org/pdf/1611.09940)). The following code block extends our
    PyTorch-native attention kernel with soft-capping:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Soft-capping 是一种常用的技术，用于控制 logit 大小（例如，见 [这里](https://arxiv.org/pdf/1611.09940)）。以下代码块扩展了我们的
    PyTorch 原生注意力内核，添加了软限制：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In the code block below we train our model, first with our PyTorch-native kernel,
    and then with the optimized Flex Attention API. These experiments were run with
    the 3136-length sequence settings.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码块中，我们先用我们的 PyTorch 原生内核训练模型，然后使用优化过的 Flex Attention API。这些实验是在 3136 长度的序列设置下运行的。
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The results of the experiments are captured in the table below:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果记录在下表中：
- en: '![](../Images/de916d94926beddd350137bca752fa5b.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de916d94926beddd350137bca752fa5b.png)'
- en: Soft-cap step time results (lower is better) — by Author
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Soft-cap 步骤时间结果（越低越好） — 作者
- en: The impact of the Flash Attention kernel is clearly evident, delivering performance
    boosts of approximately 3.5x in eager mode and 1.5x in compiled mode.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Flash Attention 核心的影响显而易见，在贪婪模式下带来了约 3.5 倍的性能提升，在编译模式下提升了 1.5 倍。
- en: Mask Mod Example — Neighborhood Masking
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Mask Mod 示例 — 邻域屏蔽
- en: We assess the *mask_mod* functionality by applying a sparse mask to our attention
    *score*. Recall that each token in our sequence represents a patch in our 2D input
    image. We modify our kernel so that each token attends only to other tokens that
    are within a 5x5 window in the corresponding 2-D token array.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过对注意力*得分*应用稀疏掩码来评估*mask_mod*功能。回想一下，我们序列中的每个标记都代表我们二维输入图像中的一个补丁。我们修改了核函数，使得每个标记仅关注位于相应二维标记数组中5x5窗口内的其他标记。
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As a baseline for our experiment, we use PyTorch SDPA which includes support
    for passing in an attention mask. The following block includes the masked SDPA
    experiment followed by the Flex Attention implementation:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 作为我们实验的基准，我们使用支持传递注意力掩码的PyTorch SDPA。以下代码块包含了掩码SDPA实验以及Flex Attention实现：
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The results of the experiments are captured below:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果如下所示：
- en: '![](../Images/3e73e0cff7eb528d567982bc709f8bb4.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e73e0cff7eb528d567982bc709f8bb4.png)'
- en: Masked attention step time results (lower is better) — by Author
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码注意力步骤时间结果（时间越短越好）——由作者提供
- en: Once again, Flex Attention offers a considerable performance boost, amounting
    to 2.19x in eager mode and 2.59x in compiled mode.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 再次证明，Flex Attention提供了显著的性能提升，急切模式下提升了2.19倍，编译模式下提升了2.59倍。
- en: Flex Attention Limitations
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Flex Attention的局限性
- en: 'Although we have succeeded in demonstrating the power and potential of Flex
    Attention, there are a few limitations that should be noted:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们成功展示了Flex Attention的强大潜力，但仍有一些局限性需要注意：
- en: '**Limited Scope of Modifications**: With Flex Attention you can (as of the
    time of this writing) only modify the attention score (the result of the dot product
    between the query and key tokens). It does not support changes at other stages
    of the attention computation.'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**修改范围有限**：使用Flex Attention（截至本文写作时）只能修改注意力得分（即查询和键标记之间的点积结果）。它不支持在注意力计算的其他阶段进行修改。'
- en: '**Dependency on torch.compile:** Given the reliance on torch.compile, care
    must be taken to avoid excessive recompilations which could greatly degrade runtime
    performance. For instance, while the support for [Document Masking](https://pytorch.org/blog/flexattention/#document-maskingjagged-sequences)
    is very compelling, it will perform as expected only if the sum of the lengths
    of all of the documents remains fixed.'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**依赖于torch.compile：** 鉴于对torch.compile的依赖，必须小心避免过多的重新编译，因为这可能会显著降低运行时性能。例如，虽然[文档掩码](https://pytorch.org/blog/flexattention/#document-maskingjagged-sequences)的支持非常有吸引力，但只有当所有文档的长度之和保持固定时，才能按预期执行。'
- en: '**No Support for Trainable Parameters in *score_mod*:** At the time of this
    writing, Flex Attention does not support a *score_mod* implementation that includes
    *trainable* parameters. For example, while the documentation highlights support
    for [relative position encodings](https://pytorch.org/blog/flexattention/#relative-position-encodings),
    these are commonly implemented with *trainable* parameters (rather than fixed
    values) which cannot currently be accommodated.'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**不支持*score_mod*中的可训练参数：** 在撰写本文时，Flex Attention不支持包含*可训练*参数的*score_mod*实现。例如，尽管文档中提到支持[相对位置编码](https://pytorch.org/blog/flexattention/#relative-position-encodings)，但这些通常是通过*可训练*参数（而非固定值）实现的，而目前无法支持这一点。'
- en: In the face of these limitations, we can return to one of the other optimization
    opportunities discussed above.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 面对这些限制，我们可以回到前面讨论的其他优化机会之一。
- en: Summary
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: As the reliance on transformer architectures and attention layers in ML models
    increases, so does the need for tools and techniques for optimizing these components.
    In this post, we have explored a number of attention kernel variants, each with
    its own unique properties, capabilities, and limitations. Importantly, one size
    does not fit all — different models and use cases will warrant the use of different
    kernels and different optimization strategies. This underscores the importance
    of having a wide variety of tools and techniques for optimizing attention layers.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习模型中对变换器架构和注意力层的依赖增加，优化这些组件的工具和技术的需求也随之增加。在这篇文章中，我们探讨了多种注意力核函数变体，每种变体都有其独特的属性、能力和局限性。重要的是，一种方法并不适用于所有情况——不同的模型和应用场景需要使用不同的核函数和不同的优化策略。这强调了拥有各种各样的工具和技术以优化注意力层的重要性。
- en: In a [sequel to this post](https://chaimrand.medium.com/optimizing-transformer-models-for-variable-length-input-sequences-19fb88fddf71),
    we will further explore attention layer optimization by focusing on applying some
    of the tools we discussed to tackle the challenge of handling variable-sized input
    sequences. Stay tuned…
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇[续篇](https://chaimrand.medium.com/optimizing-transformer-models-for-variable-length-input-sequences-19fb88fddf71)中，我们将进一步探讨注意力层优化，重点应用我们讨论过的一些工具来应对处理可变长度输入序列的挑战。敬请期待…
