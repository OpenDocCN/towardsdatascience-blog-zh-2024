- en: 'Decision Tree Classifier, Explained: A Visual Guide with Code Examples for
    Beginners'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树分类器，解释：适合初学者的可视化指南与代码示例
- en: 原文：[https://towardsdatascience.com/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=collection_archive---------2-----------------------#2024-08-30](https://towardsdatascience.com/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=collection_archive---------2-----------------------#2024-08-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=collection_archive---------2-----------------------#2024-08-30](https://towardsdatascience.com/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=collection_archive---------2-----------------------#2024-08-30)
- en: CLASSIFICATION ALGORITHM
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类算法
- en: A fresh look on our favorite upside-down tree
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们最喜爱的倒立树的新视角
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--7c863f06a71e--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--7c863f06a71e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7c863f06a71e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7c863f06a71e--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--7c863f06a71e--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@samybaladram?source=post_page---byline--7c863f06a71e--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--7c863f06a71e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7c863f06a71e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7c863f06a71e--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--7c863f06a71e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7c863f06a71e--------------------------------)
    ·10 min read·Aug 30, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7c863f06a71e--------------------------------)
    ·阅读时间：10分钟·2024年8月30日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/8f3c249fffe8ea9445912bdf52eaa4d4.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f3c249fffe8ea9445912bdf52eaa4d4.png)'
- en: '`⛳️ More [CLASSIFICATION ALGORITHM](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c),
    explained: · [Dummy Classifier](/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e)
    · [K Nearest Neighbor Classifier](/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1)
    · [Bernoulli Naive Bayes](/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6)
    · [Gaussian Naive Bayes](/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c)
    ▶ [Decision Tree Classifier](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e)
    · [Logistic Regression](/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505)
    · [Support Vector Classifier](/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9)
    · [Multilayer Perceptron](/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c)`'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '`⛳️ 更多[分类算法](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c)，解释如下：
    · [虚拟分类器](/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e)
    · [K近邻分类器](/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1)
    · [伯努利朴素贝叶斯](/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6)
    · [高斯朴素贝叶斯](/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c)
    ▶ [决策树分类器](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e)
    · [逻辑回归](/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505)
    · [支持向量机分类器](/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9)
    · [多层感知器](/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c)`'
- en: Decision Trees are everywhere in machine learning, beloved for their intuitive
    output. Who doesn’t love a simple “if-then” flowchart? Despite their popularity,
    it’s surprising how challenging it is to find a clear, step-by-step explanation
    of how Decision Trees work. (I’m actually embarrassed by how long it took me to
    actually understand how the algorithm works.)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树在机器学习中无处不在，因其直观的输出而深受喜爱。谁不喜欢一个简单的“如果-那么”流程图呢？尽管它们很受欢迎，但令人惊讶的是，找到一个清晰的、逐步解释决策树工作原理的教程竟然如此困难。（实际上，我都为自己花了那么长时间才真正理解这个算法感到有些不好意思。）
- en: So, in this breakdown, I’ll be focusing on the essentials of tree construction.
    We’ll unpack exactly what’s happening in each node and why, from root to final
    leaves (with visuals of course).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇分析中，我将重点讲解树构建的核心要点。我们将逐一解析每个节点发生了什么以及为什么发生，从根节点到最终的叶节点（当然会配有视觉示例）。
- en: '![](../Images/f2c085640e48ec94b595d5b97b972bf3.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f2c085640e48ec94b595d5b97b972bf3.png)'
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 所有视觉效果：作者使用Canva Pro创建，已优化为移动端显示；在桌面端可能显示过大。
- en: Definition
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义
- en: A Decision Tree classifier creates an upside-down tree to make predictions,
    starting at the top with a question about an important feature in your data, then
    branches out based on the answers. As you follow these branches down, each stop
    asks another question, narrowing down the possibilities. This question-and-answer
    game continues until you reach the bottom — a leaf node — where you get your final
    prediction or classification.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树分类器创建一个倒置的树来进行预测，从顶部开始，提出一个关于数据中重要特征的问题，然后根据答案分支。沿着这些分支往下走，每一个停靠点都会问另一个问题，逐步缩小可能性。这个问答游戏会一直持续，直到到达最底部——一个叶节点——在这里你将得到最终的预测或分类。
- en: '![](../Images/a30bdfba9eb934452be2e590c8911700.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a30bdfba9eb934452be2e590c8911700.png)'
- en: Decision Tree is one of the most important machine learning algorithms — it’s
    a series of yes or no question.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是最重要的机器学习算法之一——它是一个一系列的“是”或“否”问题。
- en: Dataset Used
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用的数据集
- en: Throughout this article, we’ll use this artificial golf dataset (inspired by
    [1]) as an example. This dataset predicts whether a person will play golf based
    on weather conditions.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将使用这个人工高尔夫数据集（灵感来自[1]）作为示例。这个数据集预测一个人在特定天气条件下是否会打高尔夫。
- en: '![](../Images/85150f3b513774d624ecd9a76f212074.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/85150f3b513774d624ecd9a76f212074.png)'
- en: 'Columns: ‘Outlook’ (already one-hot encoded to sunny, overcast, rainy), ‘Temperature’
    (in Fahrenheit), ‘Humidity’ (in %), ‘Wind’ (yes/no), and ‘Play’ (target feature)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 列：‘Outlook’（已经进行独热编码为晴天、多云、雨天），‘Temperature’（华氏温度），‘Humidity’（湿度%），‘Wind’（是否有风），‘Play’（目标特征）
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Main Mechanism
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主要机制
- en: 'The Decision Tree classifier operates by recursively splitting the data based
    on the most informative features. Here’s how it works:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树分类器通过递归地根据最具信息量的特征来划分数据。以下是它的工作原理：
- en: Start with the entire dataset at the root node.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从根节点开始，包含整个数据集。
- en: Select the best feature to split the data (based on measures like Gini impurity).
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择最佳特征来划分数据（基于如基尼杂质等衡量标准）。
- en: Create child nodes for each possible value of the selected feature.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为所选特征的每个可能值创建子节点。
- en: Repeat steps 2–3 for each child node until a stopping criterion is met (e.g.,
    maximum depth reached, minimum samples per leaf, or pure leaf nodes).
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个子节点重复步骤2-3，直到满足停止标准（例如，最大深度达到、每叶最小样本数、或纯叶节点）。
- en: Assign the majority class to each leaf node.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将多数类别分配给每个叶节点。
- en: '![](../Images/9727aa4eaceaeb71756092b0b93f7062.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9727aa4eaceaeb71756092b0b93f7062.png)'
- en: Training Steps
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练步骤
- en: 'In scikit-learn, the decision tree algorithm is called CART (Classification
    and Regression Trees). It builds binary trees and typically follows these steps:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中，决策树算法称为 CART（分类与回归树）。它构建二叉树，通常遵循以下步骤：
- en: Start with all training samples in the root node.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从根节点开始，包含所有训练样本。
- en: '![](../Images/23c28dcf9e69f808c8112d8d29a8585e.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23c28dcf9e69f808c8112d8d29a8585e.png)'
- en: Starting with the root node containing all 14 training samples, we will figure
    out the best way feature and the best point to split the data to start building
    the tree.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从根节点开始，根节点包含所有14个训练样本，我们将找出最佳特征及最佳切分点，开始构建树。
- en: '2.For each feature:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 对每个特征：
- en: a. Sort the feature values.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: a. 对特征值进行排序。
- en: b. Consider all possible thresholds between adjacent values as potential split
    points.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: b. 考虑相邻值之间的所有可能阈值作为潜在的切分点。
- en: '![](../Images/77b58f9197fc4b59c466b4f469f585b5.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77b58f9197fc4b59c466b4f469f585b5.png)'
- en: In this root node, there are 23 split points to check. Binary columns only has
    one split point.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个根节点中，有23个切分点需要检查。二元列仅有一个切分点。
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '3\. For each potential split point:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 对于每个潜在的切分点：
- en: a. Calculate the impurity (e.g, Gini impurity) of the current node.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: a. 计算当前节点的杂质（例如，基尼杂质）。
- en: b. Calculate the weighted average of impurities.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: b. 计算杂质的加权平均值。
- en: '![](../Images/e626abeb8db669d3fc9053dd00f2e145.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e626abeb8db669d3fc9053dd00f2e145.png)'
- en: For example, for feature “sunny” with split point 0.5, the impurity (like “Gini
    Impurity”) is calculated for both part of the dataset.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于特征“sunny”及分裂点 0.5，计算数据集两部分的杂质（如“基尼杂质”）。
- en: '![](../Images/4c7e072954b82fac5523fc7bf3627529.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c7e072954b82fac5523fc7bf3627529.png)'
- en: Another example, same process can be done to continuous features like “Temperature”
    as well.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子，类似的过程也可以应用于像“Temperature”这样的连续特征。
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 4\. After calculating all impurity for all features and split points, choose
    the lowest one.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 在计算所有特征和分裂点的杂质后，选择最低的杂质值。
- en: '![](../Images/9fefd6c986dec5130d330bfd461b2da0.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9fefd6c986dec5130d330bfd461b2da0.png)'
- en: The feature “overcast” with split point 0.5 gives the lowest impurity. This
    means the split will be the purest out of all the other split points!
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 特征“overcast”在分裂点 0.5 时具有最低的杂质值。这意味着这个分裂点将是所有其他分裂点中最纯净的！
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '5\. Create two child nodes based on the chosen feature and split point:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 根据选择的特征和分裂点创建两个子节点：
- en: '- Left child: samples with feature value <= split point'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '- 左子节点：特征值 <= 分裂点的样本'
- en: '- Right child: samples with feature value > split point'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '- 右子节点：特征值 > 分裂点的样本'
- en: '![](../Images/eed9578805510d407a3629dcc00dce22.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eed9578805510d407a3629dcc00dce22.png)'
- en: The selected split point split the data into two parts. As one part already
    pure (the right side! That’s why it’s impurity is low!), we only need to continue
    the tree on the left node.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 选定的分裂点将数据分为两部分。由于一部分已经是纯净的（右侧！这就是为什么它的杂质值较低！），我们只需要在左侧节点继续构建决策树。
- en: 6\. Recursively repeat steps 2–5 for each child node. You can also stop until
    a stopping criterion is met (e.g., maximum depth reached, minimum number of samples
    per leaf node, or minimum impurity decrease).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 6\. 递归地重复步骤 2–5，直到达到每个子节点。你也可以在满足停止条件时停止（例如，达到最大深度、每个叶节点的最小样本数或最小杂质减少）。
- en: '![](../Images/e7f196e4783fa584cb603ce79caa6f9e.png)![](../Images/cb8c1ac4bfcabef262be2dd59a867daa.png)![](../Images/7da77a6fdf7b810a7b1eb32a3a4d8621.png)![](../Images/bd92ae0789299100e892058b2d6f326b.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7f196e4783fa584cb603ce79caa6f9e.png)![](../Images/cb8c1ac4bfcabef262be2dd59a867daa.png)![](../Images/7da77a6fdf7b810a7b1eb32a3a4d8621.png)![](../Images/bd92ae0789299100e892058b2d6f326b.png)'
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Final Complete Tree
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最终完整的树
- en: The class label of a leaf node is the majority class of the training samples
    that reached that node.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 叶节点的类别标签是到达该节点的训练样本的多数类。
- en: '![](../Images/56a86845131b5b272b092ae61ae5d5fa.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56a86845131b5b272b092ae61ae5d5fa.png)'
- en: The right one is the final tree that will be used for classification. We do
    not need the samples anymore at this point.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧的树是最终将用于分类的决策树。在这一点上，我们不再需要样本。
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/33880bf12113be224d7d40b0e6b57454.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/33880bf12113be224d7d40b0e6b57454.png)'
- en: In this scikit-learn output, the information of the non-leaf node is also stored
    such as number of samples and number of each class in the node (value).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个 scikit-learn 输出中，还存储了非叶节点的信息，例如该节点的样本数和每个类别的样本数（值）。
- en: Classification Step
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类步骤
- en: 'Here’s how the prediction process works once the decision tree has been trained:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 训练好决策树后，预测过程是这样进行的：
- en: Start at the root node of the trained decision tree.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从训练好的决策树的根节点开始。
- en: Evaluate the feature and split condition at the current node.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估当前节点的特征和分裂条件。
- en: Repeat step 2 at each subsequent node until reaching a leaf node.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个后续节点重复步骤 2，直到达到叶节点。
- en: The class label of the leaf node becomes the prediction for the new instance.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 叶节点的类别标签成为新实例的预测结果。
- en: '![](../Images/2de5f2c47ede7f1862589e8dfa8c45b5.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2de5f2c47ede7f1862589e8dfa8c45b5.png)'
- en: We only need the columns that is asked by the tree. Other than “overcast” and
    “Temperature”, other values does not matter in making the prediction.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要决策树所要求的列。除了“overcast”和“Temperature”之外，其他值在做出预测时并不重要。
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Evaluation Step
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估步骤
- en: '![](../Images/a145e243167d5b7ce42710a2b37688e1.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a145e243167d5b7ce42710a2b37688e1.png)'
- en: The decision tree gives an adequate accuracy. As our tree only checks two features,
    it might not capture the test set characteristic well.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树提供了足够的准确性。由于我们的树只检查了两个特征，它可能无法很好地捕捉到测试集的特征。
- en: '[PRE8]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Key Parameters
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键参数
- en: 'Decision Trees have several important parameters that control their growth
    and complexity:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树有几个重要参数，控制它们的生长和复杂度：
- en: '1 . **Max Depth**: This sets the maximum depth of the tree, which can be a
    valuable tool in preventing overfitting.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 1 . **最大深度**：设置树的最大深度，这可以有效地防止过拟合。
- en: '**👍 Helpful Tip:** Consider starting with a shallow tree (perhaps 3–5 levels
    deep) and gradually increasing the depth.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**👍 有用提示**：考虑从一个浅层树开始（可能是3到5层深），然后逐渐增加深度。'
- en: '![](../Images/e50d0935722b988f9cd0a4dd8244cf79.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e50d0935722b988f9cd0a4dd8244cf79.png)'
- en: Start with a shallow tree (e.g., depth of 3–5) and gradually increase until
    you find the optimal balance between model complexity and performance on validation
    data.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个浅层树开始（例如，深度为3–5），逐步增加，直到找到模型复杂性和验证数据性能之间的最佳平衡。
- en: '2\. **Min Samples Split**: This parameter determines the minimum number of
    samples needed to split an internal node.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 2. **最小样本分割**：此参数决定了拆分内部节点所需的最小样本数。
- en: '**👍 Helpful Tip**: Setting this to a higher value (around 5–10% of your training
    data) can help prevent the tree from creating too many small, specific splits
    that might not generalize well to new data.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**👍 有用提示**：将此值设置为较高（大约训练数据的5%–10%）可以帮助防止决策树创建过多的小而具体的划分，这些划分可能不能很好地推广到新数据。'
- en: '![](../Images/80874a10794455846f06bafa6d22e8c1.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80874a10794455846f06bafa6d22e8c1.png)'
- en: '3\. **Min Samples Leaf**: This specifies the minimum number of samples required
    at a leaf node.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 3. **最小样本叶节点**：此参数指定叶节点所需的最小样本数。
- en: '**👍 Helpful Tip**: Choose a value that ensures each leaf represents a meaningful
    subset of your data (approximately 1–5% of your training data). This can help
    avoid overly specific predictions.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**👍 有用提示**：选择一个值，确保每个叶子节点表示一个有意义的数据子集（大约占训练数据的1%–5%）。这可以帮助避免过于具体的预测。'
- en: '![](../Images/49e5788bad70aa27458e469489d909a9.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/49e5788bad70aa27458e469489d909a9.png)'
- en: '4\. **Criterion**: The function used to measure the quality of a split (usually
    “gini” for Gini impurity or “entropy” for information gain).'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 4. **标准**：用于衡量划分质量的函数（通常使用“gini”表示Gini不纯度，或“entropy”表示信息增益）。
- en: '**👍 Helpful Tip**: While Gini is generally simpler and faster to compute, entropy
    often performs better for multi-class problems. That said, they frequently give
    similar results.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**👍 有用提示**：虽然 Gini 一般更简单且计算速度更快，但熵在多分类问题中通常表现更好。不过，它们通常会给出相似的结果。'
- en: '![](../Images/fa616fea61719ae9827571dc8815d6e4.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa616fea61719ae9827571dc8815d6e4.png)'
- en: Example of Entropy calculation for ‘sunny’ with split point 0.5.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 计算“晴天”熵的示例，分割点为0.5。
- en: Pros & Cons
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优缺点
- en: Like any algorithm in machine learning, Decision Trees have their strengths
    and limitations.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 像任何机器学习算法一样，决策树也有其优点和局限性。
- en: 'Pros:'
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优点：
- en: '**Interpretability**: Easy to understand and visualize the decision-making
    process.'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**可解释性**：容易理解和可视化决策过程。'
- en: '**No Feature Scaling**: Can handle both numerical and categorical data without
    normalization.'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**无特征缩放**：可以处理数值型和类别型数据，无需归一化。'
- en: '**Handles Non-linear Relationships**: Can capture complex patterns in the data.'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**处理非线性关系**：能够捕捉数据中的复杂模式。'
- en: '**Feature Importance**: Provides a clear indication of which features are most
    important for prediction.'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**特征重要性**：提供了一个清晰的指示，表明哪些特征对于预测最为重要。'
- en: 'Cons:'
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺点：
- en: '**Overfitting**: Prone to creating overly complex trees that don’t generalize
    well, especially with small datasets.'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**过拟合**：容易生成过于复杂的树，导致泛化能力差，尤其是在数据集较小时。'
- en: '**Instability**: Small changes in the data can result in a completely different
    tree being generated.'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**不稳定性**：数据的微小变化可能会导致生成完全不同的决策树。'
- en: '**Biased with Imbalanced Datasets**: Can be biased towards dominant classes.'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**偏向不平衡的数据集**：可能会偏向占主导地位的类别。'
- en: '**Inability to Extrapolate**: Cannot make predictions beyond the range of the
    training data.'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**无法外推**：无法对训练数据范围之外的情况做出预测。'
- en: In our golf example, a Decision Tree might create very accurate and interpretable
    rules for deciding whether to play golf based on weather conditions. However,
    it might overfit to specific combinations of conditions if not properly pruned
    or if the dataset is small.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的高尔夫例子中，决策树可能会基于天气条件创建非常准确且可解释的规则来决定是否打高尔夫。然而，如果没有适当修剪，或者数据集较小，它可能会过拟合特定条件的组合。
- en: Final Remarks
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终总结
- en: Decision Tree Classifiers are a great tool for solving many types of problems
    in machine learning. They’re easy to understand, can handle complex data, and
    show us how they make decisions. This makes them useful in many areas, from business
    to medicine. While Decision Trees are powerful and interpretable, they’re often
    used as building blocks for more advanced ensemble methods like Random Forests
    or Gradient Boosting Machines.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树分类器是解决机器学习中许多问题的绝佳工具。它们易于理解，能够处理复杂数据，并能展示它们如何做出决策。这使得它们在多个领域都非常有用，从商业到医学。虽然决策树强大且可解释，但它们通常作为更高级集成方法的构建模块，如随机森林或梯度提升机。
- en: 🌟 Decision Tree Classifier Simplified
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🌟 决策树分类器简化版
- en: '[PRE9]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Further Reading
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: For a detailed explanation of the [Decision Tree Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)
    and its implementation in scikit-learn, readers can refer to the official documentation,
    which provides comprehensive information on its usage and parameters.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 若想详细了解[决策树分类器](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)及其在
    scikit-learn 中的实现，读者可参考官方文档，文档中提供了关于其使用和参数的全面信息。
- en: Technical Environment
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术环境
- en: This article uses Python 3.7 and scikit-learn 1.5\. While the concepts discussed
    are generally applicable, specific code implementations may vary slightly with
    different versions.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 本文使用 Python 3.7 和 scikit-learn 1.5。虽然讨论的概念一般适用，但具体的代码实现可能会因版本不同而略有差异。
- en: About the Illustrations
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于插图
- en: Unless otherwise noted, all images are created by the author, incorporating
    licensed design elements from Canva Pro.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图片均由作者创作，并融合了来自 Canva Pro 的授权设计元素。
- en: '![](../Images/d9c95539a3bc257d368c105ebcd446c9.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d9c95539a3bc257d368c105ebcd446c9.png)'
- en: For a concise visual summary of Decision Tree Classifier, check out [the companion
    Instagram post.](https://www.instagram.com/p/C_SZq1BSYIw/)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 若想查看决策树分类器的简明视觉总结，请访问[配套的 Instagram 帖子](https://www.instagram.com/p/C_SZq1BSYIw/)。
- en: Reference
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] T. M. Mitchell, [Machine Learning](https://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html)
    (1997), McGraw-Hill Science/Engineering/Math, pp. 59'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] T. M. Mitchell, [机器学习](https://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html)（1997），McGraw-Hill
    Science/Engineering/Math，第59页'
- en: '𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝘾𝙡𝙖𝙨𝙨𝙞𝙛𝙞𝙘𝙖𝙩𝙞𝙤𝙣 𝘼𝙡𝙜𝙤𝙧𝙞𝙩𝙝𝙢𝙨 𝙝𝙚𝙧𝙚:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝘾𝙡𝙖𝙨𝙨𝙞𝙛𝙞𝙘𝙖𝙩𝙞𝙤𝙣 𝘼𝙡𝙜𝙤𝙧𝙞𝙩𝙝𝙢𝙨 𝙝𝙚𝙧𝙚:'
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----7c863f06a71e--------------------------------)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----7c863f06a71e--------------------------------)'
- en: Classification Algorithms
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类算法
- en: '[View list](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----7c863f06a71e--------------------------------)8
    stories![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----7c863f06a71e--------------------------------)8
    个故事![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)'
- en: '𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:'
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----7c863f06a71e--------------------------------)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----7c863f06a71e--------------------------------)'
- en: Regression Algorithms
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归算法
- en: '[View list](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----7c863f06a71e--------------------------------)5
    stories![A cartoon doll with pigtails and a pink hat. This “dummy” doll, with
    its basic design and heart-adorned shirt, visually represents the concept of a
    dummy regressor in machine. Just as this toy-like figure is a simplified, static
    representation of a person, a dummy regressor is a basic models serve as baselines
    for more sophisticated analyses.](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)![Samy
    Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----7c863f06a71e--------------------------------)5
    个故事![一个戴着粉红色帽子和辫子的卡通娃娃。这个“假人”娃娃，凭借其简单的设计和装饰有爱心的衬衫，形象地展示了机器中“假回归器”概念。就像这个玩具般的形象是一个简化、静态的人物展示，假回归器是作为基线模型的基本模型，用于更复杂的分析。](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)![Samy
    Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----7c863f06a71e--------------------------------)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----7c863f06a71e--------------------------------)'
- en: Ensemble Learning
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成学习
- en: '[View list](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----7c863f06a71e--------------------------------)4
    stories![](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----7c863f06a71e--------------------------------)4
    个故事![](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)'
