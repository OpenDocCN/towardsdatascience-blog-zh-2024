- en: How to Choose the Architecture for Your GenAI Application
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何为你的 GenAI 应用选择架构
- en: 原文：[https://towardsdatascience.com/how-to-choose-the-architecture-for-your-genai-application-6053e862c457?source=collection_archive---------1-----------------------#2024-10-03](https://towardsdatascience.com/how-to-choose-the-architecture-for-your-genai-application-6053e862c457?source=collection_archive---------1-----------------------#2024-10-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-choose-the-architecture-for-your-genai-application-6053e862c457?source=collection_archive---------1-----------------------#2024-10-03](https://towardsdatascience.com/how-to-choose-the-architecture-for-your-genai-application-6053e862c457?source=collection_archive---------1-----------------------#2024-10-03)
- en: A framework to select the simplest, fastest, cheapest architecture that will
    balance LLMs’ creativity and risk
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个框架，用于选择最简单、最快、最便宜的架构，以平衡 LLM 的创造力和风险。
- en: '[](https://lakshmanok.medium.com/?source=post_page---byline--6053e862c457--------------------------------)[![Lak
    Lakshmanan](../Images/9faaaf72d600f592cbaf3e9089cbb913.png)](https://lakshmanok.medium.com/?source=post_page---byline--6053e862c457--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6053e862c457--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6053e862c457--------------------------------)
    [Lak Lakshmanan](https://lakshmanok.medium.com/?source=post_page---byline--6053e862c457--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://lakshmanok.medium.com/?source=post_page---byline--6053e862c457--------------------------------)[![Lak
    Lakshmanan](../Images/9faaaf72d600f592cbaf3e9089cbb913.png)](https://lakshmanok.medium.com/?source=post_page---byline--6053e862c457--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6053e862c457--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6053e862c457--------------------------------)
    [Lak Lakshmanan](https://lakshmanok.medium.com/?source=post_page---byline--6053e862c457--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6053e862c457--------------------------------)
    ·16 min read·Oct 3, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6053e862c457--------------------------------)
    ·阅读时间：16 分钟 ·2024年10月3日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'Look at any LLM tutorial and the suggested usage involves invoking the API,
    sending it a prompt, and using the response. Suppose you want the LLM to generate
    a thank-you note, you could do:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 看任何 LLM 教程时，建议的使用方法是调用 API，发送提示，并使用其响应。假设你希望 LLM 生成一封感谢信，你可以这样做：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: While this is fine for PoCs, rolling to production with an architecture that
    treats an LLM as just another text-to-text (or text-to-image/audio/video) API
    results in an application that is under-engineered in terms of risk, cost, and
    latency.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种方式适用于概念验证（PoC），但如果直接将这种架构投入生产，将 LLM 仅仅当作另一个文本到文本（或文本到图像/音频/视频）的 API 使用，最终会导致一个在风险、成本和延迟方面都设计不足的应用。
- en: The solution is not to go to the other extreme and over-engineer your application
    by fine-tuning the LLM and adding guardrails, etc. every time. The goal, as with
    any engineering project, is to find the right balance of complexity, fit-for-purpose,
    risk, cost, and latency for the specifics of each use case. In this article, I’ll
    describe a framework that will help you strike this balance.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案不是走向另一个极端，通过微调 LLM 和每次添加保护措施等方式过度设计你的应用。目标，和任何工程项目一样，是找到合适的平衡点，兼顾复杂性、适用性、风险、成本和延迟，以适应每个用例的具体需求。在本文中，我将描述一个框架，帮助你找到这个平衡点。
- en: The framework of LLM application architectures
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM 应用架构的框架
- en: Here’s a framework that I suggest you use to decide on the architecture for
    your GenAI application or agent. I’ll cover each of the eight alternatives shown
    in the Figure below in the sections that follow.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我建议你用来决定你的 GenAI 应用或代理架构的框架。我将在接下来的部分中详细讲解图中展示的八种替代方案。
- en: '![](../Images/1425ffc872beb43bfaeddb529c895e9a.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1425ffc872beb43bfaeddb529c895e9a.png)'
- en: Choosing the right application architecture for your GenAI application. Diagram
    by author.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为你的 GenAI 应用选择合适的应用架构。图示来自作者。
- en: The axes here (i.e., the decision criteria) are risk and creativity. For each
    use case where you are going to employ an LLM, start by identifying the creativity
    you need from the LLM and the amount of risk that the use case carries. This helps
    you narrow down the choice that strikes the right balance for you.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的轴（即决策标准）是风险和创造力。对于每个将要使用 LLM 的用例，首先要识别你需要的 LLM 创造力以及该用例所承载的风险。这有助于你缩小选择范围，找到最适合你的平衡点。
- en: Note that whether or not to use Agentic Systems is a completely orthogonal decision
    to this — employ agentic systems when the task is too complex to be done by a
    single LLM call or if the task requires non-LLM capabilities. In such a situation,
    you’d break down the complex task into simpler tasks and orchestrate them in an
    agent framework. This article shows you how to build a GenAI application (or an
    agent) to perform one of those simple tasks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，是否使用代理系统与此完全是正交的决策——当任务过于复杂，单个 LLM 调用无法完成，或者任务需要非 LLM 能力时，可以使用代理系统。在这种情况下，你需要将复杂任务拆解成更简单的任务，并在代理框架中协调它们。本文将向你展示如何构建一个
    GenAI 应用（或代理），以执行这些简单任务之一。
- en: Why the 1st decision criterion is creativity
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么第一个决策标准是创造力
- en: Why are creativity and risk the axes? LLMs are a non-deterministic technology
    and are more trouble than they are worth if you don’t really need all that much
    uniqueness in the content being created.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么创造力和风险是轴线？LLM 是一种非确定性技术，如果你不需要在创建的内容中有太多独特性，它带来的麻烦远大于其价值。
- en: For example, if you are generating a bunch of product catalog pages, how different
    do they really have to be? Your customers want accurate information on the products
    and may not really care that all SLR camera pages explain the benefits of SLR
    technology in the same way — in fact, some amount of standardization may be quite
    preferable for easy comparisons. This is a case where your creativity requirement
    on the LLM is quite low.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你正在生成一堆产品目录页面，它们到底需要有多大的不同？你的客户希望获得准确的产品信息，可能并不关心所有 SLR 相机页面是否都以相同的方式解释
    SLR 技术的优势——实际上，一定程度的标准化可能更有利于便于比较。这是一个 LLM 的创造力要求非常低的例子。
- en: It turns out that architectures that reduce the non-determinism also reduce
    the total number of calls to the LLM, and so also have the side-effect of reducing
    the overall cost of using the LLM. Since LLM calls are slower than the typical
    web service, this also has the nice side-effect of reducing the latency. That’s
    why the y-axis is creativity, and why we have cost and latency also on that axis.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，减少非确定性的方法也减少了 LLM 调用的总次数，因此也有减少使用 LLM 总成本的副作用。由于 LLM 调用比典型的 Web 服务要慢，这也有减少延迟的良好副作用。这就是为什么
    y 轴代表创造力，并且我们在该轴上还展示了成本和延迟的原因。
- en: '![](../Images/6cc4282e8145257ebf5242641daed464.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6cc4282e8145257ebf5242641daed464.png)'
- en: 'Illustrative: use cases ordered by creativity. Diagram by author'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 说明性：按创造力排序的使用案例。图表由作者提供
- en: You could look at the illustrative use cases listed in the diagram above and
    argue whether they require low creativity or high. It really depends on your business
    problem. If you are a magazine or ad agency, even your informative content web
    pages (unlike the product catalog pages) may need to be creative.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以查看上图中列出的说明性使用案例，并辩论它们是否需要低创造力或高创造力。这实际上取决于你的业务问题。如果你是一个杂志或广告公司，即使是你的资讯内容网页（与产品目录页不同）可能也需要有创造力。
- en: Why the 2nd decision criterion is risk
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么第二个决策标准是风险
- en: LLMs have a tendency to hallucinate details and to reflect biases and toxicity
    in their training data. Given this, there are risks associated with directly sending
    LLM-generated content to end-users. Solving for this problem adds a lot of engineering
    complexity — you might have to introduce a human-in-the-loop to review content,
    or add guardrails to your application to validate that the generated content doesn’t
    violate policy.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 有时会产生幻觉，反映其训练数据中的偏见和毒性。鉴于此，直接将 LLM 生成的内容发送给最终用户存在风险。解决这个问题需要增加大量的工程复杂性——你可能需要引入人类审查环节，来审核内容，或者为应用添加防护措施，以验证生成的内容是否违反政策。
- en: If your use case allows end-users to send prompts to the model and the application
    takes actions on the backend (a common situation in many SaaS products) to generate
    a user-facing response, the risk associated with errors, hallucination, and toxicity
    is quite high.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的使用案例允许最终用户向模型发送提示，并且应用程序在后台采取行动（这是许多 SaaS 产品中的常见情况）生成面向用户的响应，那么与错误、幻觉和毒性相关的风险相当高。
- en: The same use case (art generation) could carry different levels and kinds of
    risk depending on the context as shown in the figure below. For example, if you
    are generating background instrumental music to a movie, the risk associated might
    involve mistakenly reproducing copyrighted notes, whereas if you are generating
    ad images or videos broadcast to millions of users, you may be worried about toxicity.
    These different types of risk are associated with different levels of risk. As
    another example, if you are building an enterprise search application that returns
    document snippets from your corporate document store or technology documentation,
    the LLM-associated risks might be quite low. If your document store consists of
    medical textbooks, the risk associated with out-of-context content returned by
    a search application might be high.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的用例（艺术生成）可能根据上下文承担不同级别和种类的风险，如下图所示。例如，如果你正在为电影生成背景器乐音乐，可能会面临不小心重现版权音符的风险，而如果你正在生成广告图片或视频并向数百万用户广播，你可能会担心有害内容。这些不同类型的风险与不同程度的风险相关。另一个例子是，如果你正在构建一个企业搜索应用程序，该应用程序从公司的文档库或技术文档中返回文档片段，LLM相关的风险可能会很低。如果你的文档库包含医学教科书，搜索应用程序返回的上下文不符的内容的风险可能会很高。
- en: '![](../Images/44c9c98ea17f42c4ebd891ad054e9d4d.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44c9c98ea17f42c4ebd891ad054e9d4d.png)'
- en: 'Illustrative: use cases ordered by risk. Diagram by author'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：按风险排序的用例。图表由作者提供
- en: As with the list of use cases ordered by creativity, you can quibble with the
    ordering of use cases by risk. But once you identify the risk associated with
    the use case and the creativity it requires, the suggested architecture is worth
    considering as a starting point. Then, if you understand the “why” behind each
    of these architectural patterns, you can select an architecture that balances
    your needs.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 就像按创造力排序的用例列表一样，你可以对按风险排序的用例排序提出异议。但一旦你识别出与用例相关的风险及其所需的创造力，建议的架构可以作为起点考虑。然后，如果你理解了这些架构模式背后的“为什么”，你就能选择一个平衡需求的架构。
- en: 'In the rest of this article, I’ll describe the architectures, starting from
    #1 in the diagram.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文的其余部分，我将描述从图中#1开始的架构。
- en: 1\. Generate each time (for High Creativity, Low Risk tasks)
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 每次生成（适用于高创造力、低风险任务）
- en: This is the architectural pattern that serves as the default — invoke the API
    of the deployed LLM each time you want generated content. It’s the simplest, but
    it also involves making an LLM call each time.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这是作为默认架构使用的模式——每次需要生成内容时，调用已部署的大型语言模型（LLM）API。这是最简单的方式，但每次都需要进行LLM调用。
- en: Typically, you’ll use a PromptTemplate and templatize the prompt that you send
    to the LLM based on run-time parameters. It’s a good idea to use a framework that
    allows you to swap out the LLM.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你会使用PromptTemplate，并根据运行时参数将你发送给LLM的提示进行模板化。使用一个允许更换LLM的框架是个不错的主意。
- en: 'For our example of sending an email based on the prompt, we could use langchain:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们根据提示发送电子邮件的示例中，我们可以使用langchain：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Because you are calling the LLM each time, it’s appropriate only for tasks that
    require extremely high creativity (e.g., you want a different thank you note each
    time) and where you are not worried about the risk (e.g, if the end-user gets
    to read and edit the note before hitting “send”).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 因为每次都在调用LLM，它只适用于需要极高创造力的任务（例如，你希望每次都收到不同的感谢信）并且你不担心风险的情况（例如，如果最终用户可以在点击“发送”之前阅读并编辑信件）。
- en: A common situation where this pattern is employed is for interactive applications
    (so it needs to respond to all kinds of prompts) meant for internal users (so
    low risk).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式常用于交互式应用程序（因此需要响应各种提示），这些应用程序是为内部用户设计的（因此风险较低）。
- en: 2\. Response/Prompt caching (for Medium Creativity, Low Risk tasks)
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 响应/提示缓存（适用于中等创造力、低风险任务）
- en: You probably don’t want to send the same thank you note again to the same person.
    You want it to be different each time.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能不希望向同一个人再次发送相同的感谢信。你希望每次都不一样。
- en: But what if you are building a search engine on your past tickets, such as to
    assist internal customer support teams? In such cases, you do want repeat questions
    to generate the same answer each time.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果你正在构建一个基于过去工单的搜索引擎，例如为了帮助内部客户支持团队，该怎么办？在这种情况下，你确实希望重复的问题每次都生成相同的答案。
- en: 'A way to drastically reduce cost and latency is to cache past prompts and responses.
    You can do such caching on the client side using langchain:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 大幅降低成本和延迟的一种方法是缓存过去的提示和响应。你可以使用langchain在客户端进行这样的缓存：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: When I tried it, the cached response took 1/1000th of the time and avoided the
    LLM call completely.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当我尝试时，缓存的回应只用了1/1000的时间，完全避免了LLM调用。
- en: Caching is useful beyond client-side caching of exact text inputs and the corresponding
    responses (see Figure below). Anthropic supports “[prompt caching](https://www.anthropic.com/news/prompt-caching)”
    whereby you can ask the model to cache part of a prompt (typically the system
    prompt and repetitive context) server-side, while continuing to send it new instructions
    in each subsequent query. Using prompt caching reduces cost and latency per query
    while not affecting the creativity. It is particularly helpful in RAG, document
    extraction, and few-shot prompting when the examples get large.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存不仅限于客户端缓存精确的文本输入和相应的回应（见下图）。Anthropic支持“[提示缓存](https://www.anthropic.com/news/prompt-caching)”，通过这种方式，您可以要求模型在服务器端缓存部分提示（通常是系统提示和重复的上下文），同时在每次后续查询中继续发送新的指令。使用提示缓存可以减少每次查询的成本和延迟，同时不影响创意性。在RAG（检索增强生成）、文档提取和少量示例提示中，当示例变得庞大时，这尤其有帮助。
- en: '![](../Images/daf299024bebc58df4684ce2e14e59cd.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/daf299024bebc58df4684ce2e14e59cd.png)'
- en: Response caching reduces the number of LLM calls; context caching reduces the
    number of tokens processed in each individual call. Together, they reduce the
    overall number of tokens and therefore the cost and latency. Diagram by author
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 响应缓存减少了LLM调用的次数；上下文缓存减少了每次调用中处理的令牌数量。两者结合起来，减少了总体令牌数量，从而降低了成本和延迟。图示由作者提供。
- en: Gemini separates out this functionality into [context caching](https://ai.google.dev/gemini-api/docs/caching?lang=python)
    (which reduces the cost and latency) and [system instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instructions)
    (which don’t reduce the token count, but do reduce latency). OpenAI recently announced
    support for prompt caching, with its implementation automatically caching the
    [longest prefix of a prompt](https://openai.com/index/api-prompt-caching/) that
    was previously sent to the API, as long as the prompt is longer than 1024 tokens.
    Server-side caches like these do not reduce the capability of the model, only
    the latency and/or cost, as you will continue to potentially get different results
    to the same text prompt.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Gemini将此功能分为[上下文缓存](https://ai.google.dev/gemini-api/docs/caching?lang=python)（减少成本和延迟）和[系统指令](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instructions)（虽然不减少令牌数量，但能减少延迟）。OpenAI最近宣布支持提示缓存，其实现会自动缓存先前发送到API的[最长前缀](https://openai.com/index/api-prompt-caching/)，只要提示超过1024个令牌。这类服务器端缓存不会降低模型的能力，仅会减少延迟和/或成本，因为即便是相同的文本提示，您仍然可能获得不同的结果。
- en: The built-in caching methods require exact text match. However, it is possible
    to implement caching in a way that takes advantage of the nuances of your case.
    For example, you could rewrite prompts to canonical forms to increase the chances
    of a cache hit. Another common trick is to store the hundred most frequent questions,
    for any question that is close enough, you could rewrite the prompt to ask the
    stored question instead. In a multi-turn chatbot, you could get user confirmation
    on such semantic similarity. Semantic caching techniques like this will reduce
    the capability of the model somewhat, since you will get the same responses to
    even similar prompts.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 内置的缓存方法需要精确的文本匹配。然而，可以通过一种利用具体情况微妙差异的方式来实现缓存。例如，您可以将提示重写为规范化形式，以增加缓存命中的机会。另一个常见的技巧是存储最常见的100个问题，对于任何相似的问题，您可以将提示重写为询问存储的问题。在多轮对话的聊天机器人中，您可以通过用户确认来验证这种语义相似性。像这样的语义缓存技术会稍微降低模型的能力，因为即便是相似的提示也会得到相同的回应。
- en: 3\. Pregenerated templates (for Medium Creativity, Low-Medium Risk tasks)
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. 预生成模板（适用于中等创意性、低中风险任务）
- en: Sometimes, you don’t really mind the same thank you note being generated to
    everyone in the same situation. Perhaps you are writing the thank you note to
    a customer who bought a product, and you don’t mind the same thank you note being
    generated to any customer who bought that product.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，您并不介意对每个处于相同情况的人生成相同的感谢信。也许您正在写感谢信给购买了某个产品的客户，您并不介意对任何购买了该产品的客户生成相同的感谢信。
- en: At the same time, there is a higher risk associated with this use case because
    these communications are going out to end-users and there is no internal staff
    person able to edit each generated letter before sending it out.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，这种使用场景也存在较高的风险，因为这些通信将直接发送给最终用户，且没有内部工作人员能够在发送之前编辑每封生成的信件。
- en: In such cases, it can be helpful to pregenerate templated responses. For example,
    suppose you are a tour company and you offer 5 different packages. All you need
    is one thank you message for each of these packages. Maybe you want different
    messages for solo travelers vs. families vs. groups. You still need only 3x as
    many messages as you have packages.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，预生成模板化的响应可能会很有帮助。例如，假设你是一家旅行社，并提供5种不同的套餐。你所需要的，只是为每种套餐准备一条感谢信息。也许你希望为独自旅行者、家庭旅行者和团队旅行者提供不同的信息。但你所需要的消息数量仍然是套餐数量的三倍。
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The result is messages like this for a given group-type and tour-destination:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是针对特定团体类型和旅游目的地的消息，如下所示：
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can generate these messages, have a human vet them, and store them in your
    database.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以生成这些消息，进行人工审核，然后将它们存储在数据库中。
- en: As you can see, we asked the LLM to insert placeholders in the message that
    we can replace dynamically. Whenever you need to send out a response, retrieve
    the message from the database and replace the placeholders with actual data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们要求LLM在消息中插入占位符，这样我们就可以动态地进行替换。每当需要发送响应时，只需从数据库中检索消息，并将占位符替换为实际数据。
- en: Using pregenerated templates turns a problem that would have required vetting
    hundreds of messages per day into one that requires vetting a few messages only
    when a new tour is added.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预生成模板将一个原本需要每天审核数百条消息的问题，转变为只需在新增旅游产品时审核少量消息的问题。
- en: 4\. Small Language Models (Low Risk, Low Creativity)
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 小型语言模型（低风险，低创意）
- en: Recent research shows that it is [impossible to eliminate hallucination](https://arxiv.org/abs/2401.11817)
    in LLMs because it arises from a tension between learning all the computable functions
    we desire. A smaller LLM for a more targeted task has less risk of hallucinating than one
    that’s too large for the desired task. You might be using a frontier LLM for tasks
    that don’t require the power and world-knowledge that it brings.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究表明，**[LLM中的幻觉现象无法完全消除](https://arxiv.org/abs/2401.11817)**，因为幻觉源于学习我们希望的所有可计算函数之间的矛盾。对于更有针对性的任务，一个较小的LLM比一个过于庞大的模型更不容易产生幻觉。你可能会使用一个前沿的LLM来处理一些不需要其强大功能和世界知识的任务。
- en: In use cases where you have a very simple task that doesn’t require much creativity
    and very low risk tolerance, you have the option of using a small language model
    (SLM). This does trade off accuracy — in a [June 2024 study, a Microsoft researcher](https://techcommunity.microsoft.com/t5/azure-for-isv-and-startups/evaluating-the-quality-of-ai-document-data-extraction-with-small/ba-p/4157719)
    found that for extracting structured data from unstructured text corresponding
    to an invoice, their smaller text-based model (Phi-3 Mini 128K) could get 93%
    accuracy as compared to the 99% accuracy achievable by GPT-4o.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些使用场景中，如果你有一个非常简单的任务，且该任务不需要太多创造力，并且对风险的容忍度非常低，你可以选择使用小型语言模型（SLM）。这确实会牺牲准确性——在一项**2024年6月的研究中，微软的研究人员**发现，对于从非结构化文本中提取发票相关的结构化数据，他们的小型文本模型（Phi-3
    Mini 128K）能够达到93%的准确率，而GPT-4o可以达到99%的准确率。
- en: The team at LLMWare [evaluates a wide range of SLMs](https://medium.com/@darrenoberst/best-small-language-models-for-accuracy-and-enterprise-use-cases-benchmark-results-cf71964759c8).
    At the time of writing (2024), they found that Phi-3 was the best, but that over
    time, smaller and smaller models were achieving this performance.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: LLMWare团队[评估了各种SLM](https://medium.com/@darrenoberst/best-small-language-models-for-accuracy-and-enterprise-use-cases-benchmark-results-cf71964759c8)。在写作时（2024年），他们发现Phi-3是最好的，但随着时间推移，越来越小的模型也在实现这一性能。
- en: Representing these two studies pictorially, SLMs are increasingly achieving
    their accuracy with smaller and smaller sizes (so less and less hallucination)
    while LLMs have been focused on increasing task ability (so more and more hallucination).
    The difference in accuracy between these approaches for tasks like document extraction
    has stabilized (see Figure).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 通过图示表示这两项研究，SLM（小型语言模型）正越来越以更小的尺寸实现更高的准确性（因此幻觉现象越来越少），而LLM（大型语言模型）则专注于提高任务能力（因此幻觉现象越来越多）。在像文档提取这样的任务中，这两种方法的准确性差异已经趋于稳定（见图）。
- en: '![](../Images/3405095403aa19ecb80e6296e2b1022a.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3405095403aa19ecb80e6296e2b1022a.png)'
- en: The trend is for SLMs to get the same accuracy with smaller and smaller models,
    and for LLMs to focus on more capabilities with larger and larger models. The
    accuracy differential on simple tasks has stabilized. Diagram by author.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这一趋势是SLMs通过越来越小的模型获得相同的准确性，而LLMs则通过越来越大的模型专注于更多的能力。在简单任务上的准确性差距已经稳定。图表由作者提供。
- en: If this trend holds up, expect to be using SLMs and non-frontier LLMs for more
    and more enterprise tasks that require only low creativity and have a low tolerance
    for risk. Creating embeddings from documents, such as for knowledge retrieval
    and topic modeling, are use cases that tend to fit this profile. Use small language
    models for these tasks.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这一趋势持续下去，预计您将在越来越多的企业任务中使用SLMs和非前沿LLMs，这些任务只需要较低的创造力，并且对风险的容忍度较低。从文档创建嵌入，例如用于知识检索和主题建模，是适合这种情况的用例。对于这些任务，使用小型语言模型。
- en: 5\. Assembled Reformat (Medium Risk, Low Creativity)
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 组装重新格式化（中等风险，低创造力）
- en: The underlying idea behind Assembled Reformat is to use pre-generation to reduce
    the risk on dynamic content, and use LLMs only for extraction and summarization,
    tasks that introduce only a low-level of risk even though they are done “live”.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 组装重新格式化的基本思路是通过预生成来降低动态内容的风险，只使用LLM进行提取和总结，这些任务虽然是“实时”完成的，但仅引入较低级别的风险。
- en: Suppose you are a manufacturer of machine parts and need to create a web page
    for each item in your product catalog. You are obviously concerned about accuracy.
    You don’t want to claim some item is heat-resistant when it’s not. You don’t want
    the LLM to hallucinate the tools required to install the part.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您是一家机械部件制造商，需要为产品目录中的每个项目创建一个网页。显然，您非常关心准确性。您不想错误地声称某个物品是耐热的，但它实际上并不是。您也不希望LLM产生安装该部件所需的工具。
- en: 'You probably have a database that describes the attributes of each part. A
    simple approach is to employ an LLM to generate content for each of the attributes.
    As with pre-generated templates (Pattern #3 above), make sure to have a human
    review them before storing the content in your content management system.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能有一个数据库，描述了每个部件的属性。一种简单的方法是使用LLM为每个属性生成内容。与预生成的模板（上面的模式#3）一样，请确保在将内容存储到内容管理系统之前，经过人工审查。
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'However, simply appending all the text generated will result in something that’s
    not very pleasing to read. You could, instead, assemble all of this content into
    the context of the prompt, and ask the LLM to reformat the content into the desired
    website layout:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仅仅将所有生成的文本附加起来会导致读起来不太愉快。相反，您可以将所有这些内容组装到提示的上下文中，并要求LLM将内容重新格式化为所需的网站布局：
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If you need to summarize reviews, or trade articles about the item, you can
    have this be done in a batch processing pipeline, and feed the summary into the
    context as well.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要总结评论或与该商品相关的交易文章，您可以通过批处理处理流水线来完成这一任务，并将摘要也输入到上下文中。
- en: 6\. ML Selection of Template (Medium Creativity, Medium Risk)
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. ML模板选择（中等创造力，中等风险）
- en: The assembled reformat approach works for web pages where the content is quite
    static (as in product catalog pages). However, if you are an e-commerce retailer,
    and you want to create personalized recommendations, the content is much more
    dynamic. You need higher creativity out of the LLM. Your risk tolerance in terms
    of accuracy is still about the same.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 组装重新格式化的方法适用于内容相当静态的网页（例如产品目录页面）。然而，如果您是一家电子商务零售商，且希望创建个性化推荐，则内容将更加动态。您需要LLM有更高的创造力。就准确性而言，您的风险容忍度仍然差不多。
- en: What you can do in such cases is to continue to use pre-generated templates
    for each of your products, and then use machine learning to select which templates
    you will employ.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，您可以继续使用为每个产品预生成的模板，然后利用机器学习来选择您将使用的模板。
- en: For personalized recommendations, for example, you’d use a traditional recommendations
    engine to select which products will be shown to the user, and pull in the appropriate
    pre-generated content (images + text) for that product.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于个性化推荐，您可以使用传统的推荐引擎来选择展示给用户的产品，并拉取适当的预生成内容（图片+文字）来展示该产品。
- en: This approach of combining pregeneration + ML can also be used if you are customizing
    your website for different customer journeys. You’ll pregenerate the landing pages
    and use a propensity model to choose what the next best action is.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 结合预生成+机器学习的方法也可以用于根据不同客户旅程定制你的网站。你将预生成着陆页，并使用倾向模型来选择下一个最佳动作。
- en: '**7.Fine-tune (High Creativity, Medium Risk)**'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**7. 微调（高创意，中等风险）**'
- en: If your creativity needs are high, there is no way to avoid using LLMs to generate
    the content you need. But, generating the content every time means that you can
    not scale human review.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的创意需求较高，就无法避免使用大型语言模型（LLM）来生成所需的内容。但每次生成内容意味着你无法扩展人工审查。
- en: There are two ways to address this conundrum. The simpler one, from an engineering
    complexity standpoint, is to teach the LLM to produce the kind of content that
    you want and not generate the kinds of content you don’t. This can be done through
    fine-tuning.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方式可以解决这个难题。从工程复杂度的角度来看，更简单的方式是教会LLM生成你想要的内容，并避免生成你不希望的内容。这可以通过微调来实现。
- en: 'There are three methods to fine-tune a foundational model: adapter tuning,
    distillation, and human feedback. Each of these fine-tuning methods address different
    risks:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种方法可以微调基础模型：适配器调优、蒸馏和人类反馈。这些微调方法解决了不同的风险：
- en: Adapter tuning retains the full capability of the foundational model, but allows
    you to select for specific style (such as content that fits your company voice).
    The risk addressed here is brand risk.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适配器调优保留了基础模型的全部能力，但允许你选择特定风格（例如符合公司语气的内容）。这里解决的风险是品牌风险。
- en: Distillation approximates the capability of the foundational model, but on a
    limited set of tasks, and using a smaller model that can be deployed on premises
    or behind a firewall. The risk addressed here is of confidentiality.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蒸馏方法接近基础模型的能力，但仅适用于有限的任务，并使用一个可以在本地或防火墙后部署的较小模型。这里解决的风险是保密性问题。
- en: Human feedback either through RLHF or through DPO allows the model to start
    off with reasonable accuracy, but get better with human feedback. The risk addressed
    here is of fit-for-purpose.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过RLHF或DPO获取的人类反馈可以让模型一开始就具备合理的准确性，但通过人类反馈，模型会变得更好。这里解决的风险是适用性问题。
- en: Common use cases for fine-tuning include being able to create branded content,
    summaries of confidential information, and personalized content.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 微调的常见使用案例包括能够创建品牌内容、机密信息的摘要和个性化内容。
- en: 8\. Guardrails (High Creativity, High Risk)
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8\. 防护措施（高创意，高风险）
- en: What if you want the full spectrum of capabilities, and you have more than one
    type of risk to mitigate — perhaps you are worried about brand risk, leakage of
    confidential information, and/or interested in ongoing improvement through feedback?
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要全面的能力范围，并且有多种风险需要缓解——也许你担心品牌风险、机密信息泄漏，或者有兴趣通过反馈进行持续改进呢？
- en: At that point, there is no alternative but to go whole hog and build guardrails.
    Guardrails may involve preprocessing the information going into the model, post-processing
    the output of the model, or iterating on the prompt based on error conditions.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 到那时，唯一的选择就是全力以赴，构建防护措施。防护措施可能包括预处理输入到模型的信息、后处理模型输出的结果，或根据错误条件迭代提示。
- en: Pre-built guardrails (eg. Nvidia’s NeMo) exist for commonly needed functionality
    such as checking for jailbreak, masking sensitive data in the input, and self-check
    of facts.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 预构建的防护措施（例如Nvidia的NeMo）存在，用于检查越狱、掩码输入中的敏感数据和自查事实等常见功能。
- en: '![](../Images/07ef1f452e1c29a798262cd1c41af684.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07ef1f452e1c29a798262cd1c41af684.png)'
- en: Guardrails you may have to build. Diagram by author.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能需要自己构建的防护措施。图示由作者提供。
- en: However, it’s likely that you’ll have to implement some of the guardrails yourself
    (see Figure above). An application that needs to be deployed alongside programmable
    guardrails is the most complex way that you could choose to implement a GenAI
    application. Make sure that this complexity is warranted before going down this
    route.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你可能不得不自己实现一些防护措施（见上图）。需要与可编程防护措施一起部署的应用程序是实现GenAI应用的最复杂方式。在走这条路之前，确保这种复杂性是合理的。
- en: Summary
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: I suggest you use a framework that balances creativity and risk to decide on
    the architecture for your GenAI application or agent. Creativity refers to the
    level of uniqueness required in the generated content. Risk relates to the impact
    if the LLM generates inaccurate, biased, or toxic content. Addressing high-risk
    scenarios necessitates engineering complexity, such as human review or guardrails.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议您使用一种平衡创造性和风险的框架，以决定您的GenAI应用或代理的架构。创造性指的是生成内容中所需的独特性水平。风险则与LLM生成不准确、有偏见或有毒内容的影响相关。应对高风险场景需要更多的工程复杂性，例如人工审核或防护措施。
- en: 'The framework consists of eight architectural patterns that address different
    combination of creativity and risk:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 该框架包括八种架构模式，旨在应对不同的创造性与风险组合：
- en: 1\. **Generate Each Time:** Invokes the LLM API for every content generation
    request, offering maximum creativity but with higher cost and latency. Suitable
    for interactive applications that don’t have much risk, such as internal tools..
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. **每次生成**：每次内容生成请求都调用LLM API，提供最大的创造性，但成本和延迟较高。适用于风险较低的互动型应用程序，例如内部工具。
- en: '2\. **Response/Prompt Caching**: For medium creativity, low-risk tasks. Caches
    past prompts and responses to reduce cost and latency. Useful when consistent
    answers are desirable, such as internal customer support search engines. Techniques
    like prompt caching, semantic caching, and context caching enhance efficiency
    without sacrificing creativity.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. **响应/提示缓存**：适用于中等创造性、低风险的任务。缓存过去的提示和响应，以减少成本和延迟。对于需要一致答案的场景非常有用，例如内部客户支持搜索引擎。像提示缓存、语义缓存和上下文缓存等技术可以提高效率，同时不牺牲创造性。
- en: '3\. **Pregenerated Templates**: Employs pre-generated, vetted templates for
    repetitive tasks, reducing the need for constant human review. Suitable for medium
    creativity, low-medium risk situations where standardized yet personalized content
    is required, such as customer communication in a tour company.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. **预生成模板**：采用预生成、经过审查的模板来处理重复性任务，减少持续人工审查的需求。适用于中等创造性、低到中风险的情况，需要标准化但个性化的内容，如旅游公司中的客户沟通。
- en: '4\. **Small Language Models (SLMs**): Uses smaller models to reduce hallucination
    and cost as compared to larger LLMs. Ideal for low creativity, low-risk tasks
    like embedding creation for knowledge retrieval or topic modeling.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. **小型语言模型（SLM）**：使用较小的模型来减少幻觉现象和成本，相较于更大的LLM。这对于低创造性、低风险的任务（如知识检索嵌入创建或主题建模）非常理想。
- en: 5\. **Assembled Reformat:** Uses LLMs for reformatting and summarization, with
    pre-generated content to ensure accuracy. Suitable for content like product catalogs
    where accuracy is paramount on some parts of the content, while creative writing
    is required on others.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. **组装重格式化**：使用LLM进行重新格式化和总结，借助预生成的内容确保准确性。适用于如产品目录这类需要在部分内容上保证准确性，而在其他部分需要创意写作的内容。
- en: 6\. **ML Selection of Template:** Leverages machine learning to select appropriate
    pre-generated templates based on user context, balancing personalization with
    risk management. Suitable for personalized recommendations or dynamic website
    content.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 6\. **机器学习模板选择**：利用机器学习根据用户上下文选择适当的预生成模板，平衡个性化与风险管理。适用于个性化推荐或动态网站内容。
- en: '7\. **Fine-tune**: Involves fine-tuning the LLM to generate desired content
    while minimizing undesired outputs, addressing risks related to one of brand voice,
    confidentiality, or accuracy. Adapter Tuning focuses on stylistic adjustments,
    distillation on specific tasks, and human feedback for ongoing improvement.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 7\. **微调**：涉及对大型语言模型（LLM）进行微调，以生成期望的内容，同时最小化不希望出现的输出，处理与品牌声音、机密性或准确性相关的风险。适配器微调专注于风格调整，蒸馏专注于特定任务，而人类反馈则用于持续改进。
- en: '8\. **Guardrails**: High creativity, high-risk tasks require guardrails to
    mitigate multiple risks, including brand risk and confidentiality, through preprocessing,
    post-processing, and iterative prompting. Off-the-shelf guardrails address common
    concerns like jailbreaking and sensitive data masking while custom-built guardrails
    may be necessary for industry/application-specific requirements.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 8\. **防护措施**：高创造性、高风险的任务需要防护措施，通过预处理、后处理和迭代提示来减轻多种风险，包括品牌风险和机密性风险。现成的防护措施可以解决诸如破解和敏感数据掩蔽等常见问题，而针对特定行业/应用需求可能需要定制的防护措施。
- en: By using the above framework to architect GenAI applications, you will be able
    to balance complexity, fit-for-purpose, risk, cost, and latency for each use case.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述框架来构建生成式人工智能（GenAI）应用时，你将能够为每个使用案例平衡复杂性、适用性、风险、成本和延迟。
- en: '*(Periodic reminder: these posts are my personal views, not those of my employers,
    past or present.)*'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '*(周期性提醒：这些文章仅代表我个人观点，并非我过去或现在雇主的立场。)*'
