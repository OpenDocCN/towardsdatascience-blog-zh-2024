- en: Diffusion Model from Scratch in Pytorch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用PyTorch从零开始构建扩散模型
- en: 原文：[https://towardsdatascience.com/diffusion-model-from-scratch-in-pytorch-ddpm-9d9760528946?source=collection_archive---------0-----------------------#2024-07-04](https://towardsdatascience.com/diffusion-model-from-scratch-in-pytorch-ddpm-9d9760528946?source=collection_archive---------0-----------------------#2024-07-04)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/diffusion-model-from-scratch-in-pytorch-ddpm-9d9760528946?source=collection_archive---------0-----------------------#2024-07-04](https://towardsdatascience.com/diffusion-model-from-scratch-in-pytorch-ddpm-9d9760528946?source=collection_archive---------0-----------------------#2024-07-04)
- en: Implementation of Denoising Diffusion Probabilistic Models (DDPM)
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 去噪扩散概率模型（DDPM）的实现
- en: '[](https://medium.com/@nickd16718?source=post_page---byline--9d9760528946--------------------------------)[![Nicholas
    DiSalvo](../Images/481fbbf016523bfee37ac5b11d46de41.png)](https://medium.com/@nickd16718?source=post_page---byline--9d9760528946--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9d9760528946--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9d9760528946--------------------------------)
    [Nicholas DiSalvo](https://medium.com/@nickd16718?source=post_page---byline--9d9760528946--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@nickd16718?source=post_page---byline--9d9760528946--------------------------------)[![Nicholas
    DiSalvo](../Images/481fbbf016523bfee37ac5b11d46de41.png)](https://medium.com/@nickd16718?source=post_page---byline--9d9760528946--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9d9760528946--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9d9760528946--------------------------------)
    [Nicholas DiSalvo](https://medium.com/@nickd16718?source=post_page---byline--9d9760528946--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9d9760528946--------------------------------)
    ·13 min read·Jul 4, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9d9760528946--------------------------------)
    ·13分钟阅读·2024年7月4日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/0c8310a9e682e4d713961c2b785c6878.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c8310a9e682e4d713961c2b785c6878.png)'
- en: DDPM Example on MNIST — Image by the Author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: DDPM在MNIST上的例子 — 图片由作者提供
- en: '**Introduction**'
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**介绍**'
- en: 'A diffusion model in general terms is a type of generative deep learning model
    that creates data from a learned denoising process. There are many variations
    of diffusion models with the most popular ones usually being text conditional
    models that can generate a certain image based on a prompt. Some diffusion models
    (Control-Net) can even blend images with certain artistic styles. Here is an example
    below here:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型一般来说是一种生成式深度学习模型，通过学习去噪过程来生成数据。扩散模型有很多变体，其中最流行的通常是基于文本条件的模型，可以根据提示生成特定的图像。一些扩散模型（如Control-Net）甚至可以将图像与特定的艺术风格融合。下面是一个例子：
- en: '![](../Images/3d4c1a771b4f06566e9e6fdc589124bb.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d4c1a771b4f06566e9e6fdc589124bb.png)'
- en: Image by the Author using finetuned MonsterLabs’ QR Monster V2
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用微调的MonsterLabs’ QR Monster V2生成
- en: If you don’t know what's so special about the image, try moving farther away
    from the screen or squinting your eyes to see the secret hidden in the image.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不知道这张图片有什么特别之处，可以试着把眼睛离屏幕远一点，或者眯着眼睛看，看看图像中隐藏的秘密。
- en: There are many different applications and types of diffusion models, but in
    this tutorial we are going to build the foundational unconditional diffusion model,
    DDPM (Denoising Diffusion Probabilistic Models) [1]. We will start by looking
    into how the algorithm works intuitively under the hood, and then we will build
    it from scratch in PyTorch. Also, this tutorial will focus primarily on the intuitive
    idea behind the algorithm and the specific implementation details. For the mathematical
    derivations and background, this book [2] is a great reference.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型有许多不同的应用和类型，但在本教程中，我们将构建基础的无条件扩散模型——DDPM（去噪扩散概率模型）[1]。我们将从直观地理解算法的工作原理开始，然后从零开始用PyTorch构建它。此外，本教程将主要关注算法背后的直观思想和具体的实现细节。有关数学推导和背景内容，可以参考本书[2]。
- en: 'Last Notes: This implementation was built for workflows that contain a single
    GPU with CUDA compatibility. In addition, the complete code repository can be
    found here [https://github.com/nickd16/Diffusion-Models-from-Scratch](https://github.com/nickd16/Diffusion-Models-from-Scratch)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的备注：此实现是为包含单个支持CUDA的GPU的工作流而构建的。此外，完整的代码仓库可以在这里找到[https://github.com/nickd16/Diffusion-Models-from-Scratch](https://github.com/nickd16/Diffusion-Models-from-Scratch)
- en: How it Works -> The Forward and Reverse Process
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的 -> 正向和反向过程
- en: '![](../Images/a1d50f21f5b1addc7f55a20411b1ad15.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1d50f21f5b1addc7f55a20411b1ad15.png)'
- en: Image from [2] Understand Deep Learning by Simon J.D. Prince
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [2] Simon J.D. Prince 的《理解深度学习》
- en: The diffusion process includes a forward and a reverse process. The forward
    process is a predetermined Markov chain based on a noise schedule. The noise schedule
    is a set of variances B1, B2, … BT that govern the conditional normal distributions
    that make up the Markov chain.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散过程包括正向和反向过程。正向过程是基于噪声时间表的预定马尔可夫链。噪声时间表是一组方差 B1、B2、… BT，它们控制组成马尔可夫链的条件正态分布。
- en: '![](../Images/7a2db63a9761fb56a563371fb873ec51.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a2db63a9761fb56a563371fb873ec51.png)'
- en: The Forward Process Markov Chain — Image from [2]
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 正向过程马尔可夫链 — 图片来自 [2]
- en: This formula is the mathematical representation of the forward process, but
    intuitively we can understand it as a sequence where we gradually map our data
    examples X to pure noise. Our first term in the forward process is just our initial
    data example. At an intermediate time step t, we have a noised version of X, and
    at our final time step T, we arrive at pure noise that is approximately governed
    by a standard normal distribution. When we build a diffusion model, we choose
    our noise schedule. In DDPM for example, our noise schedule features 1000 time
    steps of linearly increasing variances starting at 1e-4 to 0.02\. It is also important
    to note that our forward process is static, meaning we choose our noise schedule
    as a hyperparameter to our diffusion model and we do not train the forward process
    as it is already defined explicitly.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式是正向过程的数学表示，但直观上我们可以理解它为一个序列，我们逐渐将我们的数据示例 X 映射到纯噪声。我们正向过程的第一个项只是我们的初始数据示例。在中间时间步
    t，我们有 X 的带噪声版本，在最终时间步 T，我们到达大致由标准正态分布控制的纯噪声。当我们构建扩散模型时，我们选择我们的噪声时间表。例如，在 DDPM
    中，我们的噪声时间表包含 1000 个时间步，线性增加的方差从 1e-4 到 0.02。还要注意的是，我们的正向过程是静态的，这意味着我们将我们的噪声时间表作为扩散模型的超参数，并且我们不训练正向过程，因为它已经明确定义。
- en: 'The final key detail we have to know about the forward process is that because
    the distributions are normal, we can mathematically derive a distribution known
    as the “Diffusion Kernel” which is the distribution of any intermediate value
    in our forward process given our initial data point. This allows us to bypass
    all of the intermediate steps of iteratively adding t-1 levels of noise in the
    forward process to get an image with t noise which will come in handy later when
    we train our model. This is mathematically represented as:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须了解关于正向过程的最后一个关键细节是，因为分布是正态的，我们可以数学推导出一个称为“扩散核”的分布，它是给定我们初始数据点的正向过程中任何中间值的分布。这使我们能够绕过在正向过程中迭代地添加
    t-1 个噪声级别的所有中间步骤，以获得具有 t 噪声的图像的分布，这在我们训练模型时会很有用。这在数学上表示为：
- en: '![](../Images/c6a311fdfd54b18fdd204519a6cc1f90.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c6a311fdfd54b18fdd204519a6cc1f90.png)'
- en: The Diffusion Kernel — Image from [2]
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散核 — 图片来自 [2]
- en: where alpha at time t is defined as the cumulative product (1-B) from our initial
    time step to our current time step.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间 t 时的 alpha 被定义为从我们的初始时间步到当前时间步的累积乘积 (1-B)。
- en: The reverse process is the key to a diffusion model. The reverse process is
    essentially the undoing of the forward process by gradually removing amounts of
    noise from a pure noisy image to generate new images. We do this by starting at
    purely noised data, and for each time step t we subtract the amount of noise that
    would have theoretically been added by the forward process for that time step.
    We keep removing noise until eventually we have something that resembles our original
    data distribution. The bulk of our work is training a model to carefully approximate
    the forward process in order to estimate a reverse process that can generate new
    samples.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 反向过程是扩散模型的关键。反向过程本质上是逐渐从纯噪声图像中去除噪声量以生成新图像的正向过程的撤销。我们通过从纯噪声数据开始，对于每个时间步 t，我们减去在该时间步上正向过程理论上会添加的噪声量。我们不断去除噪声，直到最终得到类似于我们原始数据分布的东西。我们的大部分工作是训练一个模型来精确逼近正向过程，以估计能够生成新样本的反向过程。
- en: The Algorithm and Training Objective
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 算法和训练目标
- en: 'To train such a model to estimate the reverse diffusion process, we can follow
    the algorithm in the image defined below:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练这样一个模型来估计反向扩散过程，我们可以按照下面定义的图像中的算法进行操作：
- en: Take a randomly sampled data point from our training dataset
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从我们的训练数据集中随机选择一个数据点
- en: Select a random timestep on our noise (variance) schedule
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从我们的噪声（方差）调度中选择一个随机的时间步
- en: Add the noise from that time step to our data, simulating the forward diffusion
    process through the “diffusion kernel”
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将该时间步的噪声添加到我们的数据中，通过“扩散核”模拟前向扩散过程
- en: Pass our defused image into our model to predict the noise we added
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将去噪后的图像传入我们的模型，预测我们添加的噪声
- en: Compute the mean squared error between the predicted noise and the actual noise
    and optimize our model’s parameters through that objective function
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算预测噪声与实际噪声之间的均方误差，并通过该目标函数优化我们模型的参数
- en: And repeat!
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后重复！
- en: '![](../Images/d35924b68d26c2d976d2a34f8aa5d1b5.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d35924b68d26c2d976d2a34f8aa5d1b5.png)'
- en: DDPM Training Algorithm — Image from [2]
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: DDPM训练算法 — 图片来自[2]
- en: Mathematically, the exact formula in the algorithm might look a little strange
    at first without seeing the full derivation, but intuitively its a reparameterization
    of the diffusion kernel based on the alpha values of our noise schedule and its
    simply the squared difference of predicted noise and the actual noise we added
    to an image.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学上，算法中的精确公式乍一看可能有些奇怪，尤其是没有看到完整的推导过程，但直观上它是基于我们噪声调度的alpha值对扩散核的重参数化，本质上是预测的噪声与我们实际添加到图像中的噪声的平方差。
- en: If our model can successfully predict the amount of noise based on a specific
    time step of our forward process, we can iteratively start from noise at time
    step T and gradually remove noise based on each time step until we recover data
    that resembles a generated sample from our original data distribution.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的模型能够成功预测基于前向过程特定时间步的噪声量，我们可以从时间步T的噪声开始，逐步去除噪声，基于每个时间步，直到我们恢复出类似于原始数据分布的生成样本。
- en: 'The sampling algorithm is summarized in the following:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 采样算法总结如下：
- en: Generate random noise from a standard normal distribution
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从标准正态分布生成随机噪声
- en: 'For each timestep starting from our last timestep and moving backwards:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个时间步，从我们最后的时间步开始，向后推进：
- en: 2\. Update Z by estimating the reverse process distribution with mean parameterized
    by Z from the previous step and variance parameterized by the noise our model
    estimates at that timestep
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 通过估计反向过程分布来更新Z，均值由前一步的Z参数化，方差由模型在该时间步估计的噪声参数化
- en: 3\. Add a small amount of the noise back for stability (explanation below)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 为了稳定性，添加少量噪声（以下解释）
- en: 4\. And repeat until we arrive at time step 0, our recovered image!
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 重复此过程，直到我们到达时间步0，即恢复的图像！
- en: '![](../Images/1bcc7f5c7ee37512339bb6c5504bf081.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1bcc7f5c7ee37512339bb6c5504bf081.png)'
- en: DDPM Sampling Algorithm — Image from [2]
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: DDPM采样算法 — 图片来自[2]
- en: The algorithm to then sample and generate images might look mathematically complicated
    but it intuitively boils down to an iterative process where we start with pure
    noise, estimate the noise that theoretically was added at time step t, and subtract
    it. We do this until we arrive at our generated sample. The only small detail
    we should be mindful of is after we subtract the estimated noise, we add back
    a small amount of it to keep the process stable. For example, estimating and subtracting
    the total amount of noise in the beginning of the iterative process all at once
    leads to very incoherent samples, so in practice adding a bit of the noise back
    and iterating through every time step has empirically been shown to generate better
    samples.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然后用于采样和生成图像的算法可能在数学上看起来复杂，但直观上它归结为一个迭代过程，我们从纯噪声开始，估计在时间步t时理论上添加的噪声，并将其减去。我们一直执行这个过程，直到得到生成的样本。唯一需要注意的小细节是，在减去估计的噪声后，我们会加回一小部分噪声，以保持过程的稳定性。例如，在迭代过程开始时一次性估计并减去所有噪声会导致非常不连贯的样本，因此，实际上将噪声稍微加回，并在每个时间步上进行迭代，已被实验证明能生成更好的样本。
- en: The UNET
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UNET
- en: The authors of the DDPM paper used the UNET architecture originally designed
    for medical image segmentation to build a model to predict the noise for the diffusion
    reverse process. The model we are going to use in this tutorial is meant for 32x32
    images perfect for datasets such as MNIST, but the model can be scaled to also
    handle data of much higher resolutions. There are many variations of the UNET,
    but the overview of the model architecture we will build is in the image below.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: DDPM论文的作者使用了最初为医学图像分割设计的UNET架构，构建了一个模型来预测扩散逆过程的噪声。我们在本教程中将使用的模型是为32x32图像设计的，适合MNIST等数据集，但该模型也可以扩展以处理更高分辨率的数据。UNET有许多变体，但我们将构建的模型架构概览如下图所示。
- en: '![](../Images/edfac73b9b54f51677bdb2ed90f537cf.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/edfac73b9b54f51677bdb2ed90f537cf.png)'
- en: UNET for Diffusion — Image by the Author
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 用于扩散的UNET — 作者提供的图片
- en: The UNET for DDPM is similar to the classic UNET because it contains both a
    down sampling stream and an up sampling stream that lightens the computational
    burden of the network, while also having skip connections between the two streams
    to merge the information from both the shallow and deep features of the model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 用于DDPM的UNET类似于经典的UNET，因为它包含了一个下采样流和一个上采样流，减轻了网络的计算负担，同时在两个流之间有跳跃连接，将模型的浅层和深层特征的信息合并起来。
- en: The main differences between the DDPM UNET and the classic UNET is that the
    DDPM UNET features attention in the 16x16 dimensional layers and sinusoidal transformer
    embeddings in every residual block. The meaning behind the sinusoidal embeddings
    is to tell the model which time step we are trying to predict the noise. This
    helps the model predict the noise at each time step by injecting positional information
    on where the model is on our noise schedule. For example, if we had a schedule
    of noise that had a lot of noise in certain time steps, the model understanding
    what time step it has to predict can help the model’s prediction on that noise
    for the corresponding time step. More general information on attention and embeddings
    can be found here [3] for those not already familiar with them from the transformer
    architecture.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: DDPM UNET和经典UNET之间的主要区别在于DDPM UNET在16x16维度层中具有注意力机制，并且在每个残差块中具有正弦变压器嵌入。正弦嵌入背后的含义是告诉模型我们正在尝试预测噪声的时间步长。这有助于模型通过在噪声时间表上注入位置信息来预测每个时间步长的噪声。例如，如果我们有一个在某些时间步长上有很多噪声的噪声时间表，模型理解它必须预测的时间步长可以帮助模型对相应时间步长的噪声进行预测。关于注意力和嵌入的更一般信息可以在这里找到[3]，供那些还不熟悉它们的人从变压器架构中了解。
- en: In our implementation of the model, we will start by defining our imports (possible
    pip install commands commented for reference) and coding our sinusoidal time step
    embeddings. Intuitively, the sinusoidal embeddings are different sin and cos frequencies
    that can be added directly to our inputs to give the model additional positional/sequential
    understanding. As you can see from the image below, each sinusoidal wave is unique
    which will give the model awareness on its location in our noise schedule.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的模型实现中，我们将从定义我们的导入开始（可能的pip安装命令已经注释以供参考），并编写我们的正弦时间步嵌入。直观地说，正弦嵌入是不同的sin和cos频率，可以直接添加到我们的输入中，为模型提供额外的位置/顺序理解。从下面的图片中可以看到，每个正弦波是独一无二的，这将使模型意识到它在我们的噪声时间表中的位置。
- en: '![](../Images/7b837ad03871bd70590fcc4b8a30a5ff.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b837ad03871bd70590fcc4b8a30a5ff.png)'
- en: Sinusoidal Embeddings — Image from [3]
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 正弦嵌入 — 来自[3]的图片
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The residual blocks in each layer of the UNET will be equivalent to the ones
    used in the original DDPM paper. Each residual block will have a sequence of group-norm,
    the ReLU activation, a 3x3 “same” convolution, dropout, and a skip-connection.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: UNET中每个层中的残差块将等同于原始DDPM论文中使用的残差块。每个残差块将包含一系列的group-norm、ReLU激活、3x3“same”卷积、dropout和一个跳跃连接。
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In DDPM, the authors used 2 residual blocks per layer (resolution scale) of
    the UNET and for the 16x16 dimension layers, we include the classic transformer
    attention mechanism between the two residual blocks. We will now implement the
    attention mechanism for the UNET:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在DDPM中，作者在每个UNET的层（分辨率尺度）中使用了2个残差块，对于16x16维度的层，我们在两个残差块之间包含了经典的变压器注意力机制。我们现在将为UNET实现注意力机制：
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The attention implementation is straight forward. We reshape our data such
    that the h*w dimensions are combined into a “sequence” dimension like the classic
    input for a transformer model and the channel dimension turns into the embedding
    feature dimension. In this implementation we utilize torch.nn.functional.scaled_dot_product_attention
    because this implementation contains flash attention, which is an optimized version
    of attention which is still mathematically equivalent to classic transformer attention.
    For more information on flash attention you can refer to these papers: [4], [5].'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制的实现很直接。我们重新塑造我们的数据，使得h*w维度合并成一个“序列”维度，就像传统的变压器模型的输入一样，通道维度变成了嵌入特征维度。在这个实现中，我们利用torch.nn.functional.scaled_dot_product_attention，因为这个实现包含了flash
    attention，这是一个经过优化的注意力版本，仍然在数学上等同于经典的变压器注意力。关于flash attention的更多信息，您可以参考这些论文：[4]，[5]。
- en: 'Finally at this point, we can define a complete layer of the UNET:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，在这一阶段，我们可以定义 UNET 的完整层：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Each layer in DDPM as previously discussed has 2 residual blocks and may contain
    an attention mechanism, and we additionally pass our embeddings into each residual
    block. Also, we return both the downsampled or upsampled value as well as the
    value prior which we will store and use for our residual concatenated skip connections.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，DDPM 中的每一层都有 2 个残差块，并可能包含一个注意力机制，我们还将嵌入向量传递到每个残差块中。此外，我们返回下采样或上采样的值，以及之前的值，我们会存储并用于残差拼接跳跃连接。
- en: 'Finally, we can finish the UNET Class:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以完成 UNET 类：
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The implementation is straight forward based on the classes we have already
    created. The only difference in this implementation is that our channels for the
    up-stream are slightly larger than the typical channels of the UNET. I found that
    this architecture trained more efficiently on a single GPU with 16GB of VRAM.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们已经创建的类，实施过程是直接的。此实现的唯一区别在于我们上游的通道略大于典型的 UNET 通道。我发现这种架构在 16GB VRAM 的单个 GPU
    上训练效率更高。
- en: The Scheduler
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调度器
- en: Coding the noise/variance scheduler for DDPM is also very straightforward. In
    DDPM, our schedule will start, as previously mentioned, at 1e-4 and end at 0.02
    and increase linearly.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为 DDPM 编写噪声/方差调度器也是非常简单的。在 DDPM 中，正如之前提到的，我们的调度器将从 1e-4 开始，到 0.02 结束，并线性增加。
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We return both the beta (variance) values and the alpha values since we the
    formulas for training and sampling use both based on their mathematical derivations.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们返回 beta（方差）值和 alpha 值，因为我们在训练和采样的公式中都会使用这两个值，基于它们的数学推导。
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Additionally (not required) this function defines a training seed. This means
    that if you want to reproduce a specific training instance you can use a set seed
    such that the random weight and optimizer initializations are the same each time
    you use the same seed.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 另外（不是必须的），此功能定义了一个训练种子。这意味着，如果你想重现特定的训练实例，可以使用一个设置的种子，这样每次使用相同的种子时，随机权重和优化器的初始化将保持一致。
- en: Training
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: For our implementation, we will create a model to generate MNIST data (hand
    written digits). Since these images are 28x28 by default in pytorch, we pad the
    images to 32x32 to follow the original paper trained on 32x32 images.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的实现，我们将创建一个模型来生成 MNIST 数据（手写数字）。由于这些图像在 pytorch 中默认是 28x28，我们将图像填充到 32x32
    以符合原始论文中基于 32x32 图像训练的设置。
- en: For optimization, we use Adam with initial learning rate of 2e-5\. We also use
    EMA (Exponential Moving Average) to aid in generation quality. EMA is a weighted
    average of the model’s parameters that in inference time can create smoother,
    less noisy samples. For this implementation I use the library timm’s EMAV3 out
    of the box implementation with weight 0.9999 as used in the DDPM paper.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于优化，我们使用 Adam 优化器，初始学习率为 2e-5。我们还使用 EMA（指数加权移动平均）来提高生成质量。EMA 是模型参数的加权平均值，在推理时可以生成更平滑、噪声更少的样本。对于这一实现，我使用了
    timm 库中的 EMAV3 默认实现，权重为 0.9999，正如 DDPM 论文中所使用的。
- en: To summarize our training, we simply follow the psuedo-code above. We pick random
    time steps for our batch, noise our data in the batch based on our schedule at
    those time steps, and we input that batch of noised images into the UNET along
    with the time steps themselves to guide the sinusoidal embeddings. We use the
    formulas in the pseudo-code based on the “diffusion kernel” to noise the images.
    We then take our model’s prediction of how much noise we added and compare to
    the actual noise we added and optimize the mean squared error of the noise. We
    also implemented basic checkpointing to pause and resume training on different
    epochs.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 总结我们的训练过程，我们只需按照上述伪代码执行。我们为每个批次随机选择时间步，根据这些时间步和调度器为批次中的数据添加噪声，并将带噪声的图像批次输入到
    UNET 中，同时提供时间步信息以引导正弦嵌入。我们根据伪代码中的“扩散核”公式为图像添加噪声。然后，我们将模型预测的噪声量与实际添加的噪声进行比较，并优化噪声的均方误差。我们还实现了基本的检查点功能，以便在不同的训练周期暂停和恢复训练。
- en: '[PRE7]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: For inference, we exactly follow again the other part of the pseudo code. Intuitively,
    we are just reversing the forward process. We are starting from pure noise, and
    our now trained model can predict the estimated noise at each time step and can
    then generate brand new samples iteratively. Each different starting point for
    the noise, we can generate a different unique sample that is similar to our original
    data distribution but unique. The formulas for inference were not derived in this
    article but the reference linked in the beginning can help guide readers who want
    a deeper understanding.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于推理，我们再次完全遵循伪代码的另一部分。直观地说，我们只是反转了前向过程。我们从纯噪声开始，现在训练好的模型可以预测每个时间步的估计噪声，然后可以逐步生成全新的样本。每个不同的噪声起点，我们都可以生成一个与原始数据分布相似但独特的不同样本。推理的公式在本文中并未推导，但开头提到的参考文献可以帮助读者深入理解。
- en: Also note, I included a helper function to view the diffused images so you can
    visualize how well the model learned the reverse process.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，请注意，我包括了一个辅助函数来查看扩散图像，这样你可以直观地看到模型学习反向过程的效果如何。
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'After training for 75 epochs with the experimental details listed above, we
    obtain these results:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述实验细节下训练 75 个 epoch 后，我们得到以下结果：
- en: '![](../Images/f5a91a14e2033f82cf00696c6a413e72.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f5a91a14e2033f82cf00696c6a413e72.png)'
- en: Image by the Author
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: At this point we have just coded DDPM from scratch in PyTorch!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经用 PyTorch 从零开始编码了 DDPM！
- en: Thanks for reading!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: References
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] [DDPM https://arxiv.org/abs/2006.11239](https://arxiv.org/abs/2006.11239)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [DDPM](https://arxiv.org/abs/2006.11239)'
- en: '[2] [Understanding Deep Learning https://udlbook.github.io/udlbook/](https://udlbook.github.io/udlbook/)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [理解深度学习](https://udlbook.github.io/udlbook/)'
- en: '[3] [Attention is All You Need https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [Attention is All You Need](https://arxiv.org/abs/1706.03762)'
- en: '[4] [Flash Attention https://arxiv.org/abs/2205.14135](https://arxiv.org/abs/2205.14135)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [Flash Attention](https://arxiv.org/abs/2205.14135)'
- en: '[5] [Flash Attention 2 https://arxiv.org/abs/2307.08691](https://arxiv.org/abs/2307.08691)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [Flash Attention 2](https://arxiv.org/abs/2307.08691)'
