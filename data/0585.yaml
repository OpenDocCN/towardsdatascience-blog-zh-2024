- en: A Definitive Guide to Using BigQuery Efficiently
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸€ä»½å…³äºå¦‚ä½•é«˜æ•ˆä½¿ç”¨BigQueryçš„ç»ˆææŒ‡å—
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/burn-data-rather-than-money-with-bigquery-the-definitive-guide-1b50a9fdf096?source=collection_archive---------2-----------------------#2024-03-03](https://towardsdatascience.com/burn-data-rather-than-money-with-bigquery-the-definitive-guide-1b50a9fdf096?source=collection_archive---------2-----------------------#2024-03-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/burn-data-rather-than-money-with-bigquery-the-definitive-guide-1b50a9fdf096?source=collection_archive---------2-----------------------#2024-03-03](https://towardsdatascience.com/burn-data-rather-than-money-with-bigquery-the-definitive-guide-1b50a9fdf096?source=collection_archive---------2-----------------------#2024-03-03)
- en: Make the most out of your BigQuery usage, burn data rather than money to create
    real value with some practical techniques.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å……åˆ†åˆ©ç”¨ä½ çš„BigQueryä½¿ç”¨ï¼Œçƒ§æ‰æ•°æ®è€Œä¸æ˜¯çƒ§æ‰é’±ï¼Œç”¨ä¸€äº›å®ç”¨æŠ€å·§åˆ›é€ çœŸæ­£çš„ä»·å€¼ã€‚
- en: '[](https://vojay.medium.com/?source=post_page---byline--1b50a9fdf096--------------------------------)[![Volker
    Janz](../Images/0825160d6d521f4152948f0187cf354b.png)](https://vojay.medium.com/?source=post_page---byline--1b50a9fdf096--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1b50a9fdf096--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1b50a9fdf096--------------------------------)
    [Volker Janz](https://vojay.medium.com/?source=post_page---byline--1b50a9fdf096--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://vojay.medium.com/?source=post_page---byline--1b50a9fdf096--------------------------------)[![Volker
    Janz](../Images/0825160d6d521f4152948f0187cf354b.png)](https://vojay.medium.com/?source=post_page---byline--1b50a9fdf096--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1b50a9fdf096--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1b50a9fdf096--------------------------------)
    [Volker Janz](https://vojay.medium.com/?source=post_page---byline--1b50a9fdf096--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1b50a9fdf096--------------------------------)
    Â·20 min readÂ·Mar 3, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â· å‘å¸ƒäº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1b50a9fdf096--------------------------------)
    Â· 20åˆ†é’Ÿé˜…è¯» Â· 2024å¹´3æœˆ3æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Â· [ğŸ“ Introduction](#b5f1)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Â· [ğŸ“ å¼•è¨€](#b5f1)
- en: Â· [ğŸ’ BigQuery basics and understanding costs](#ecac)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Â· [ğŸ’ BigQueryåŸºç¡€å’Œç†è§£æˆæœ¬](#ecac)
- en: âˆ˜ [Storage](#2451)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [å­˜å‚¨](#2451)
- en: âˆ˜ [Compute](#73e8)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [è®¡ç®—](#73e8)
- en: Â· [ğŸ“ Data modeling](#c6f3)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Â· [ğŸ“ æ•°æ®å»ºæ¨¡](#c6f3)
- en: âˆ˜ [Data types](#5d43)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [æ•°æ®ç±»å‹](#5d43)
- en: âˆ˜ [The shift towards de-normalization](#20ab)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [å‘å»è§„èŒƒåŒ–è½¬å˜](#20ab)
- en: âˆ˜ [Partitioning](#acf8)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [åˆ†åŒº](#acf8)
- en: âˆ˜ [Clustering](#8d1d)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [èšç±»](#8d1d)
- en: âˆ˜ [Nested repeated columns](#2c8a)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [åµŒå¥—é‡å¤åˆ—](#2c8a)
- en: âˆ˜ [Indexing](#7ed9)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [ç´¢å¼•](#7ed9)
- en: âˆ˜ [Physical Bytes Storage Billing](#507a)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [ç‰©ç†å­—èŠ‚å­˜å‚¨è®¡è´¹](#507a)
- en: âˆ˜ [Join optimizations with primary keys and foreign keys](#258d)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [ä½¿ç”¨ä¸»é”®å’Œå¤–é”®çš„è¿æ¥ä¼˜åŒ–](#258d)
- en: Â· [âš™ï¸ Data operations](#6bae)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Â· [âš™ï¸ æ•°æ®æ“ä½œ](#6bae)
- en: âˆ˜ [Copy data / tables](#28ad)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [å¤åˆ¶æ•°æ®/è¡¨](#28ad)
- en: âˆ˜ [Load data](#da01)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [åŠ è½½æ•°æ®](#da01)
- en: âˆ˜ [Delete partitions](#0ef6)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [åˆ é™¤åˆ†åŒº](#0ef6)
- en: âˆ˜ [Get distinct partitions for a table](#0c4d)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [è·å–è¡¨çš„ä¸åŒåˆ†åŒº](#0c4d)
- en: âˆ˜ [Do not persist calculated measures](#3ae7)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [ä¸è¦æŒä¹…åŒ–è®¡ç®—çš„åº¦é‡](#3ae7)
- en: Â· [ğŸ“š Summary](#59d4)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Â· [ğŸ“š æ‘˜è¦](#59d4)
- en: âˆ˜ [Embrace data modeling best practices](#165f)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [éµå¾ªæ•°æ®å»ºæ¨¡æœ€ä½³å®è·µ](#165f)
- en: âˆ˜ [Master data operations for cost-effectiveness](#8018)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [æŒæ¡æ•°æ®æ“ä½œä»¥å®ç°æˆæœ¬æ•ˆç›Š](#8018)
- en: âˆ˜ [Design for efficiency and avoid unnecessary data persistence](#af6f)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [ä¸ºäº†æ•ˆç‡è€Œè®¾è®¡ï¼Œé¿å…ä¸å¿…è¦çš„æ•°æ®æŒä¹…åŒ–](#af6f)
- en: '**Disclaimer**: BigQuery is a product which is constantly being developed,
    pricing might change at any time and this article is based on my own experience.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**å…è´£å£°æ˜**ï¼šBigQueryæ˜¯ä¸€ä¸ªä¸æ–­å‘å±•çš„äº§å“ï¼Œå®šä»·å¯èƒ½éšæ—¶å˜åŒ–ï¼Œæœ¬æ–‡åŸºäºæˆ‘ä¸ªäººçš„ç»éªŒã€‚'
- en: '![](../Images/ad3d101dc043e43fc1b8f1c82871e0d7.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad3d101dc043e43fc1b8f1c82871e0d7.png)'
- en: Photo by [Konstantin Evdokimov](https://unsplash.com/@constantinevdokimov?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±[Konstantin Evdokimov](https://unsplash.com/@constantinevdokimov?utm_source=medium&utm_medium=referral)æ‹æ‘„ï¼Œç…§ç‰‡æ¥æºäº[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: ğŸ“ Introduction
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ“ å¼•è¨€
- en: 'In the field of data warehousing, thereâ€™s a universal truth: managing data
    can be costly. Like a dragon guarding its treasure, each byte stored and each
    query executed demands its share of gold coins. But let me give you a magical
    spell to appease the dragon: burn data, not money!'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ•°æ®ä»“åº“é¢†åŸŸï¼Œæœ‰ä¸€ä¸ªæ™®éçš„çœŸç†ï¼šç®¡ç†æ•°æ®å¯èƒ½æ˜¯æ˜‚è´µçš„ã€‚å°±åƒä¸€æ¡å®ˆæŠ¤è´¢å®çš„é¾™ï¼Œæ¯ä¸ªå­˜å‚¨çš„å­—èŠ‚å’Œæ¯ä¸ªæ‰§è¡Œçš„æŸ¥è¯¢éƒ½éœ€è¦å®ƒçš„é‡‘å¸ä»½é¢ã€‚ä½†æ˜¯è®©æˆ‘ç»™ä½ ä¸€ä¸ªé­”æ³•å’’è¯­æ¥å®‰æŠšè¿™æ¡é¾™ï¼šçƒ§æ‰æ•°æ®ï¼Œè€Œä¸æ˜¯çƒ§æ‰é’±ï¼
- en: In this article, we will unravel the arts of BigQuery sorcery, to reduce costs
    while increasing efficiency, and beyond. Join as we journey through the depths
    of cost optimization, where every byte is a precious coin.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ­å¼€ BigQuery é­”æ³•çš„è‰ºæœ¯ï¼Œæ—¨åœ¨åœ¨æé«˜æ•ˆç‡çš„åŒæ—¶é™ä½æˆæœ¬ï¼Œç”šè‡³æ›´å¤šã€‚åŠ å…¥æˆ‘ä»¬ï¼Œä¸€åŒæ¢ç´¢æˆæœ¬ä¼˜åŒ–çš„æ·±åº¦ï¼Œåœ¨è¿™é‡Œï¼Œæ¯ä¸ªå­—èŠ‚éƒ½æ˜¯çè´µçš„ç¡¬å¸ã€‚
- en: '![](../Images/26edd2af9f5eda4849e2b289d311ab64.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26edd2af9f5eda4849e2b289d311ab64.png)'
- en: Photo by [Jonathan Kemper](https://unsplash.com/@jupp?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± [Jonathan Kemper](https://unsplash.com/@jupp?utm_source=medium&utm_medium=referral)
    æä¾›ï¼Œæ¥è‡ª [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: ğŸ’ BigQuery basics and understanding costs
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ’ BigQuery åŸºç¡€å’Œæˆæœ¬ç†è§£
- en: BigQuery is not just a tool but a package of scalable compute and storage technologies,
    with fast network, everything managed by Google. At its core, BigQuery is a serverless
    Data Warehouse for analytical purposes and built-in features like Machine Learning
    (*BigQuery ML*). BigQuery separates storage and compute with Googleâ€™s Jupiter
    network in-between to utilize 1 Petabit/sec of total bisection bandwidth. The
    storage system is using Capacitor, a proprietary columnar storage format by Google
    for semi-structured data and the file system underneath is Colossus, the distributed
    file system by Google. The compute engine is based on Dremel and it uses Borg
    for cluster management, running thousands of Dremel jobs across cluster(s).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: BigQuery ä¸ä»…ä»…æ˜¯ä¸€ä¸ªå·¥å…·ï¼Œè€Œæ˜¯ä¸€æ•´å¥—å¯æ‰©å±•çš„è®¡ç®—å’Œå­˜å‚¨æŠ€æœ¯åŒ…ï¼Œé…å¤‡å¿«é€Ÿç½‘ç»œï¼Œä¸€åˆ‡ç”±è°·æ­Œç®¡ç†ã€‚åœ¨å…¶æ ¸å¿ƒï¼ŒBigQuery æ˜¯ä¸€ä¸ªæ— æœåŠ¡å™¨çš„æ•°æ®ä»“åº“ï¼Œä¸“ä¸ºåˆ†æç›®çš„è€Œè®¾è®¡ï¼Œå†…ç½®ç‰¹æ€§å¦‚æœºå™¨å­¦ä¹ ï¼ˆ*BigQuery
    ML*ï¼‰ã€‚BigQuery å°†å­˜å‚¨å’Œè®¡ç®—åˆ†å¼€ï¼Œé€šè¿‡è°·æ­Œçš„ Jupiter ç½‘ç»œè¿›è¡Œè¿æ¥ï¼Œä»¥åˆ©ç”¨ 1 Petabit/ç§’çš„æ€»åŒå‘å¸¦å®½ã€‚å­˜å‚¨ç³»ç»Ÿä½¿ç”¨ Capacitorï¼Œè¿™æ˜¯è°·æ­Œä¸ºåŠç»“æ„åŒ–æ•°æ®æä¾›çš„ä¸“æœ‰åˆ—å¼å­˜å‚¨æ ¼å¼ï¼Œåº•å±‚çš„æ–‡ä»¶ç³»ç»Ÿæ˜¯è°·æ­Œçš„åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ
    Colossusã€‚è®¡ç®—å¼•æ“åŸºäº Dremelï¼Œå¹¶ä½¿ç”¨ Borg è¿›è¡Œé›†ç¾¤ç®¡ç†ï¼Œèƒ½å¤Ÿè·¨å¤šä¸ªé›†ç¾¤è¿è¡Œæˆåƒä¸Šä¸‡çš„ Dremel ä½œä¸šã€‚
- en: '***BigQuery is not just a tool but a package of scalable compute and storage
    technologies, with fast network, everything managed by Google***'
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***BigQuery ä¸ä»…ä»…æ˜¯ä¸€ä¸ªå·¥å…·ï¼Œè€Œæ˜¯ä¸€æ•´å¥—å¯æ‰©å±•çš„è®¡ç®—å’Œå­˜å‚¨æŠ€æœ¯åŒ…ï¼Œé…å¤‡å¿«é€Ÿç½‘ç»œï¼Œä¸€åˆ‡ç”±è°·æ­Œç®¡ç†***'
- en: 'The following illustration shows the basic architecture of how BigQuery is
    structured:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ’å›¾å±•ç¤ºäº† BigQuery æ¶æ„çš„åŸºæœ¬ç»“æ„ï¼š
- en: '![](../Images/348b1c767b85af6fbb00e012327b8993.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/348b1c767b85af6fbb00e012327b8993.png)'
- en: BigQuery architecture (by author)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: BigQuery æ¶æ„ï¼ˆä½œè€…ï¼‰
- en: Data can be stored in Colossus, however, it is also possible to create BigQuery
    tables on top of data stored in Google Cloud Storage. In that case, queries are
    still processed using the BigQuery compute infrastructure but read data from GCS
    instead. Such `external tables` come with some disadvantages but in some cases
    it can be more cost efficient to have the data stored in GCS. Also, sometimes
    it is not about Big Data but simply reading data from existing CSV files that
    are somehow ingested to GCS. For simplicity it can also be benificial to use these
    kind of tables.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®å¯ä»¥å­˜å‚¨åœ¨ Colossus ä¸­ï¼Œä½†ä¹Ÿå¯ä»¥åœ¨ Google Cloud Storage ä¸­åˆ›å»º BigQuery è¡¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒæŸ¥è¯¢ä»ç„¶é€šè¿‡ BigQuery
    è®¡ç®—åŸºç¡€è®¾æ–½å¤„ç†ï¼Œä½†è¯»å–çš„æ•°æ®æ¥è‡ª GCSã€‚æ­¤ç±» `å¤–éƒ¨è¡¨` æœ‰ä¸€äº›ç¼ºç‚¹ï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå°†æ•°æ®å­˜å‚¨åœ¨ GCS ä¸­å¯èƒ½æ›´å…·æˆæœ¬æ•ˆç›Šã€‚å¦å¤–ï¼Œæœ‰æ—¶å€™å¹¶ä¸æ˜¯å…³äºå¤§æ•°æ®ï¼Œè€Œä»…ä»…æ˜¯ä»ç°æœ‰çš„
    CSV æ–‡ä»¶ä¸­è¯»å–æ•°æ®ï¼Œè¿™äº›æ–‡ä»¶ä»¥æŸç§æ–¹å¼è¢«å¯¼å…¥åˆ° GCSã€‚ä¸ºäº†ç®€ä¾¿èµ·è§ï¼Œä½¿ç”¨è¿™ç§è¡¨æ ¼ä¹Ÿå¯èƒ½å¸¦æ¥å¥½å¤„ã€‚
- en: '![](../Images/f263f69736f23df70761a9222df7e3b2.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f263f69736f23df70761a9222df7e3b2.png)'
- en: BigQuery external tables (by author)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: BigQuery å¤–éƒ¨è¡¨ï¼ˆä½œè€…ï¼‰
- en: To utilize the full potential of BigQuery, the regular case is to store data
    in the BigQuery storage.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å……åˆ†å‘æŒ¥ BigQuery çš„æ½œåŠ›ï¼Œå¸¸è§çš„åšæ³•æ˜¯å°†æ•°æ®å­˜å‚¨åœ¨ BigQuery å­˜å‚¨ä¸­ã€‚
- en: The main drivers for costs are storage and compute, Google is not charging you
    for other parts, like the network transfer in between storage and compute.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æˆæœ¬çš„ä¸»è¦é©±åŠ¨å› ç´ æ˜¯å­˜å‚¨å’Œè®¡ç®—ï¼Œè°·æ­Œä¸ä¼šå¯¹å…¶ä»–éƒ¨åˆ†æ”¶è´¹ï¼Œæ¯”å¦‚å­˜å‚¨ä¸è®¡ç®—ä¹‹é—´çš„ç½‘ç»œä¼ è¾“ã€‚
- en: Storage
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å­˜å‚¨
- en: Storage costs you $0.02 per GB â€” $0.04 per GB for active and $0.01 per GB â€”
    $0.02 per GB for inactive data (*which means not modified in the last 90 days*).
    If you have a table or partition that is not modified for 90 consecutive days,
    it is considered long term storage, and the price of storage automatically drops
    by 50%. Discount is applied on a per-table, per-partition basis. Modification
    resets the 90-day counter.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å­˜å‚¨è´¹ç”¨ä¸ºæ¯ GB $0.02 â€” æ¯ GB $0.04 ä¸ºæ´»è·ƒæ•°æ®ï¼Œå’Œæ¯ GB $0.01 â€” æ¯ GB $0.02 ä¸ºéæ´»è·ƒæ•°æ®ï¼ˆ*å³åœ¨è¿‡å» 90
    å¤©å†…æœªè¢«ä¿®æ”¹çš„æ•°æ®*ï¼‰ã€‚å¦‚æœä½ çš„è¡¨æˆ–åˆ†åŒºåœ¨è¿ç»­ 90 å¤©å†…æ²¡æœ‰ä¿®æ”¹ï¼Œåˆ™è¢«è§†ä¸ºé•¿æœŸå­˜å‚¨ï¼Œå­˜å‚¨ä»·æ ¼å°†è‡ªåŠ¨ä¸‹é™ 50%ã€‚æŠ˜æ‰£æ˜¯åŸºäºæ¯ä¸ªè¡¨æˆ–æ¯ä¸ªåˆ†åŒºåº”ç”¨çš„ã€‚ä¿®æ”¹æ“ä½œä¼šé‡ç½®
    90 å¤©è®¡æ—¶å™¨ã€‚
- en: Compute
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®¡ç®—
- en: BigQuery charges for data scanned and not the runtime of the query, also transfer
    from storage to compute cluster is not charged. Compute costs depend on the location,
    the costs for `europe-west3` are $8.13 per TB for example.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: BigQuery æŒ‰æ‰«æçš„æ•°æ®é‡æ”¶è´¹ï¼Œè€Œä¸æ˜¯æŸ¥è¯¢çš„è¿è¡Œæ—¶ï¼Œä¹Ÿä¸ä¼šå¯¹ä»å­˜å‚¨åˆ°è®¡ç®—é›†ç¾¤çš„ä¼ è¾“æ”¶è´¹ã€‚è®¡ç®—æˆæœ¬å–å†³äºä½ç½®ï¼Œä¾‹å¦‚ `europe-west3` çš„è´¹ç”¨ä¸ºæ¯
    TB $8.13ã€‚
- en: 'This means:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€ï¼š
- en: '***We want to minimize the data to be scanned for each query!***'
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***æˆ‘ä»¬å¸Œæœ›æœ€å°åŒ–æ¯ä¸ªæŸ¥è¯¢è¦æ‰«æçš„æ•°æ®é‡ï¼***'
- en: '![](../Images/fb555e76a253c89bb09904ccc83e1e24.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fb555e76a253c89bb09904ccc83e1e24.png)'
- en: 'Left: [Jp Valery](https://unsplash.com/de/@jpvalery?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/de/fotos/zeitrafferfotografie-mehrerer-brennender-us-dollar-banknoten-blOLCO2K4M0?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash),
    right: [Gabriel Jimenez](https://unsplash.com/de/@gabrielj_photography?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/de/fotos/bokeh-fotografie-einer-person-die-erde-tragt-jin4W1HqgL4?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å·¦ï¼š [Jp Valery](https://unsplash.com/de/@jpvalery?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    åœ¨ [Unsplash](https://unsplash.com/de/fotos/zeitrafferfotografie-mehrerer-brennender-us-dollar-banknoten-blOLCO2K4M0?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)ï¼Œå³ï¼š
    [Gabriel Jimenez](https://unsplash.com/de/@gabrielj_photography?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    åœ¨ [Unsplash](https://unsplash.com/de/fotos/bokeh-fotografie-einer-person-die-erde-tragt-jin4W1HqgL4?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
- en: When executing a query, BigQuery is estimating the data to be processed. After
    entering your query in the BigQuery Studio query editor, you can see the estimate
    on the top right.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‰§è¡ŒæŸ¥è¯¢æ—¶ï¼ŒBigQuery ä¼šä¼°ç®—è¦å¤„ç†çš„æ•°æ®é‡ã€‚åœ¨ BigQuery Studio æŸ¥è¯¢ç¼–è¾‘å™¨ä¸­è¾“å…¥æŸ¥è¯¢åï¼Œä½ å¯ä»¥åœ¨å³ä¸Šè§’çœ‹åˆ°ä¼°ç®—å€¼ã€‚
- en: '![](../Images/51bf962d037887c304ce20cda8662211.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51bf962d037887c304ce20cda8662211.png)'
- en: BigQuery Studio
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: BigQuery Studio
- en: 'If it says 1.27 GB like in the screenshot above and the query is processed
    in the location `europe-west3`, the costs can be calculated like this:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœåƒä¸Šå›¾æ‰€ç¤ºæ˜¾ç¤ºä¸º 1.27 GBï¼Œå¹¶ä¸”æŸ¥è¯¢åœ¨ `europe-west3` ä½ç½®å¤„ç†ï¼Œåˆ™å¯ä»¥æŒ‰ä»¥ä¸‹æ–¹å¼è®¡ç®—è´¹ç”¨ï¼š
- en: '[PRE0]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The estimate is mostly a pessimistic calculation, often the optimizer is able
    to use cached results, materialized views or other techniques, so that the actual
    bytes billed are lower than the estimate. It is still a good practice to check
    this estimate in order to get a rough feeling of the impact of your work.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼°ç®—å€¼é€šå¸¸æ˜¯ä¸€ä¸ªæ‚²è§‚çš„è®¡ç®—ï¼Œä¼˜åŒ–å™¨é€šå¸¸èƒ½å¤Ÿåˆ©ç”¨ç¼“å­˜ç»“æœã€ç‰©åŒ–è§†å›¾æˆ–å…¶ä»–æŠ€æœ¯ï¼Œä»è€Œä½¿å®é™…è®¡è´¹å­—èŠ‚æ•°ä½äºä¼°ç®—å€¼ã€‚æ£€æŸ¥è¿™ä¸ªä¼°ç®—å€¼ä»ç„¶æ˜¯ä¸€ä¸ªå¥½çš„åšæ³•ï¼Œå¯ä»¥è®©ä½ å¤§è‡´äº†è§£å·¥ä½œå½±å“ã€‚
- en: It is also possible to set a maximum for the bytes billed for your query. If
    your query exceeds the limit it will fail and create no costs at all. The setting
    can be changed by navigating to More -> Query settings -> Advanced options ->
    Maximum bytes billed.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ä¹Ÿå¯ä»¥ä¸ºæŸ¥è¯¢è®¾ç½®æœ€å¤§è®¡è´¹å­—èŠ‚æ•°ã€‚å¦‚æœæŸ¥è¯¢è¶…è¿‡è¯¥é™åˆ¶ï¼Œå®ƒå°†å¤±è´¥ï¼Œå¹¶ä¸”ä¸ä¼šäº§ç”Ÿä»»ä½•è´¹ç”¨ã€‚è¿™ä¸ªè®¾ç½®å¯ä»¥é€šè¿‡å¯¼èˆªåˆ° æ›´å¤š -> æŸ¥è¯¢è®¾ç½® -> é«˜çº§é€‰é¡¹ ->
    æœ€å¤§è®¡è´¹å­—èŠ‚æ•° æ¥æ›´æ”¹ã€‚
- en: '![](../Images/31c6507a2b70acf6778a6ef827a030bd.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31c6507a2b70acf6778a6ef827a030bd.png)'
- en: BigQuery Query Settings
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: BigQuery æŸ¥è¯¢è®¾ç½®
- en: '![](../Images/cbe8c884ffa43f6cf07d42f442d90060.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cbe8c884ffa43f6cf07d42f442d90060.png)'
- en: BigQuery exceeded limit for bytes billed
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: BigQuery è¶…è¿‡äº†è®¡è´¹å­—èŠ‚æ•°çš„é™åˆ¶
- en: Unfortunately up until now, it is not possible to set a default value per query.
    It is only possible to limit the bytes billed for each day per user per project
    or for all bytes billed combined per day for a project.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å¹¸çš„æ˜¯ï¼Œç›´åˆ°ç°åœ¨ï¼Œæ— æ³•ä¸ºæ¯ä¸ªæŸ¥è¯¢è®¾ç½®é»˜è®¤å€¼ã€‚åªèƒ½é™åˆ¶æ¯ä¸ªç”¨æˆ·æ¯å¤©æ¯ä¸ªé¡¹ç›®çš„è®¡è´¹å­—èŠ‚æ•°ï¼Œæˆ–é™åˆ¶ä¸€ä¸ªé¡¹ç›®æ¯å¤©çš„æ‰€æœ‰å­—èŠ‚æ€»è®¡ã€‚
- en: When you start using BigQuery for the first projects, you will most likely stick
    with the on-demand compute pricing model. With on-demand pricing, you will generally
    have access to up to 2000 concurrent slots, shared among all queries in a single
    project, which is more than enough in most cases. A slot is like a virtual CPU
    working on a unit of work of your query DAG.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½ ç¬¬ä¸€æ¬¡å¼€å§‹ä½¿ç”¨ BigQuery è¿›è¡Œé¡¹ç›®æ—¶ï¼Œä½ å¾ˆå¯èƒ½ä¼šé€‰æ‹©æŒ‰éœ€è®¡ç®—å®šä»·æ¨¡å‹ã€‚åœ¨æŒ‰éœ€å®šä»·æ¨¡å‹ä¸‹ï¼Œä½ é€šå¸¸å¯ä»¥è®¿é—®æœ€å¤š 2000 ä¸ªå¹¶å‘æ§½ï¼Œè¿™äº›æ§½ä¼šåœ¨å•ä¸ªé¡¹ç›®çš„æ‰€æœ‰æŸ¥è¯¢ä¹‹é—´å…±äº«ï¼Œè¿™åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹æ˜¯è¶³å¤Ÿçš„ã€‚ä¸€ä¸ªæ§½ç±»ä¼¼äºä¸€ä¸ªè™šæ‹Ÿ
    CPUï¼Œå¤„ç†æŸ¥è¯¢ DAG çš„ä¸€å•ä½å·¥ä½œã€‚
- en: When reaching a certain spending per month, it is worth looking into the capacity
    pricing model, which gives you more predictable costs.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ¯æœˆæ”¯å‡ºè¾¾åˆ°ä¸€å®šé‡‘é¢æ—¶ï¼Œå€¼å¾—è€ƒè™‘å®¹é‡å®šä»·æ¨¡å‹ï¼Œè¿™æ ·å¯ä»¥è®©æˆæœ¬æ›´å…·å¯é¢„æµ‹æ€§ã€‚
- en: ğŸ“ Data modeling
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ“ æ•°æ®å»ºæ¨¡
- en: Data types
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°æ®ç±»å‹
- en: 'To reduce the costs for storage but also compute, it is very important to always
    use the smallest datatype possible for your columns. You can easily estimate the
    costs for a certain amount of rows following this overview:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å‡å°‘å­˜å‚¨å’Œè®¡ç®—æˆæœ¬ï¼Œå§‹ç»ˆä½¿ç”¨æœ€å°çš„æ•°æ®ç±»å‹å¯¹äºä½ çš„åˆ—éå¸¸é‡è¦ã€‚ä½ å¯ä»¥è½»æ¾åœ°é€šè¿‡ä»¥ä¸‹æ¦‚è§ˆä¼°ç®—ä¸€å®šæ•°é‡è¡Œæ•°æ®çš„æˆæœ¬ï¼š
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`*NULL*` *is calculated as 0 logical bytes*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`*NULL*` *è¢«è®¡ç®—ä¸º0é€»è¾‘å­—èŠ‚*'
- en: '**Example**:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¤ºä¾‹**ï¼š'
- en: '[PRE2]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'With this definition and the table of datatypes, it is possible to estimate
    the logical size of 100,000,000 rows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è¿™ä¸ªå®šä¹‰å’Œæ•°æ®ç±»å‹è¡¨ï¼Œèƒ½å¤Ÿä¼°ç®—100,000,000è¡Œæ•°æ®çš„é€»è¾‘å¤§å°ï¼š
- en: '[PRE3]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Assuming we are running a `SELECT *` on this table, it would cost us 5.78 GB
    / 1024 = 0.0056 TB * $8.13 = $0.05 in `europe-west3`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬åœ¨è¿™ä¸ªè¡¨ä¸Šæ‰§è¡Œ`SELECT *`ï¼Œå®ƒå°†èŠ±è´¹æˆ‘ä»¬5.78 GB / 1024 = 0.0056 TB * $8.13 = $0.05ï¼Œåœ¨`europe-west3`åŒºåŸŸã€‚
- en: It is a good idea to make these calculations before designing your data model,
    not only to optimize the datatype usage but also to get an estimate of the costs
    for the project that you are working on.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®¾è®¡æ•°æ®æ¨¡å‹ä¹‹å‰è¿›è¡Œè¿™äº›è®¡ç®—æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ï¼Œè¿™ä¸ä»…æœ‰åŠ©äºä¼˜åŒ–æ•°æ®ç±»å‹çš„ä½¿ç”¨ï¼Œè¿˜èƒ½ä¼°ç®—ä½ æ‰€ä»äº‹é¡¹ç›®çš„æˆæœ¬ã€‚
- en: The shift towards de-normalization
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‘åè§„èŒƒåŒ–è½¬å˜
- en: In the realm of database design and management, data normalization and de-normalization
    are fundamental concepts aimed at optimizing data structures for efficient storage,
    retrieval, and manipulation. Traditionally, normalization has been hailed as a
    best practice, emphasizing the reduction of redundancy and the preservation of
    data integrity. However, in the context of BigQuery and other modern data warehouses,
    the dynamics shift, and de-normalization often emerges as the preferred approach.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ•°æ®åº“è®¾è®¡å’Œç®¡ç†é¢†åŸŸï¼Œæ•°æ®è§„èŒƒåŒ–å’Œåè§„èŒƒåŒ–æ˜¯æ—¨åœ¨ä¼˜åŒ–æ•°æ®ç»“æ„ä»¥å®ç°é«˜æ•ˆå­˜å‚¨ã€æ£€ç´¢å’Œæ“ä½œçš„åŸºæœ¬æ¦‚å¿µã€‚ä¼ ç»Ÿä¸Šï¼Œè§„èŒƒåŒ–è¢«èª‰ä¸ºæœ€ä½³å®è·µï¼Œå¼ºè°ƒå‡å°‘å†—ä½™å¹¶ä¿æŒæ•°æ®å®Œæ•´æ€§ã€‚ç„¶è€Œï¼Œåœ¨BigQueryå’Œå…¶ä»–ç°ä»£æ•°æ®ä»“åº“çš„èƒŒæ™¯ä¸‹ï¼ŒåŠ¨æ€å‘ç”Ÿå˜åŒ–ï¼Œåè§„èŒƒåŒ–å¾€å¾€æˆä¸ºé¦–é€‰çš„æ–¹æ³•ã€‚
- en: In normalized databases, data is structured into multiple tables, each representing
    a distinct entity or concept, and linked through relationships such as one-to-one,
    one-to-many, or many-to-many. This approach adheres to the principles laid out
    by database normalization forms, such as the First Normal Form (1NF), Second Normal
    Form (2NF), and Third Normal Form (3NF), among others.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è§„èŒƒåŒ–æ•°æ®åº“ä¸­ï¼Œæ•°æ®è¢«ç»„ç»‡æˆå¤šä¸ªè¡¨ï¼Œæ¯ä¸ªè¡¨ä»£è¡¨ä¸€ä¸ªç‹¬ç«‹çš„å®ä½“æˆ–æ¦‚å¿µï¼Œå¹¶é€šè¿‡ä¸€å¯¹ä¸€ã€ä¸€å¯¹å¤šæˆ–å¤šå¯¹å¤šç­‰å…³ç³»è¿›è¡Œè¿æ¥ã€‚è¿™ç§æ–¹æ³•éµå¾ªæ•°æ®åº“è§„èŒƒåŒ–å½¢å¼çš„åŸåˆ™ï¼Œå¦‚ç¬¬ä¸€èŒƒå¼ï¼ˆ1NFï¼‰ã€ç¬¬äºŒèŒƒå¼ï¼ˆ2NFï¼‰å’Œç¬¬ä¸‰èŒƒå¼ï¼ˆ3NFï¼‰ç­‰ã€‚
- en: This comes with the advantages of reduction of redundancy, data integrity and
    consequently, less storage usage.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¸¦æ¥äº†å‡å°‘å†—ä½™ã€æ•°æ®å®Œæ•´æ€§ä»¥åŠå› æ­¤å‡å°‘å­˜å‚¨ä½¿ç”¨çš„ä¼˜åŠ¿ã€‚
- en: '![](../Images/da2e4c1933ff53547b874460d16bfabc.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da2e4c1933ff53547b874460d16bfabc.png)'
- en: Photo by [Shubham Dhage](https://unsplash.com/@theshubhamdhage?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±[Shubham Dhage](https://unsplash.com/@theshubhamdhage?utm_source=medium&utm_medium=referral)æä¾›ï¼Œæ¥æºäº[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: While data normalization holds merit in traditional relational databases, the
    paradigm shifts when dealing with modern analytics platforms like BigQuery. BigQuery
    is designed for handling massive volumes of data and performing complex analytical
    queries at scale. In this environment, the emphasis shifts from minimizing storage
    space to optimizing query performance.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æ•°æ®è§„èŒƒåŒ–åœ¨ä¼ ç»Ÿå…³ç³»å‹æ•°æ®åº“ä¸­å…·æœ‰ä¼˜åŠ¿ï¼Œä½†åœ¨å¤„ç†åƒBigQueryè¿™æ ·çš„ç°ä»£åˆ†æå¹³å°æ—¶ï¼ŒèŒƒå¼å‘ç”Ÿäº†è½¬å˜ã€‚BigQueryæ—¨åœ¨å¤„ç†æµ·é‡æ•°æ®å¹¶æ‰§è¡Œå¤§è§„æ¨¡çš„å¤æ‚åˆ†ææŸ¥è¯¢ã€‚åœ¨è¿™ç§ç¯å¢ƒä¸‹ï¼Œé‡ç‚¹ä»æœ€å°åŒ–å­˜å‚¨ç©ºé—´è½¬å‘ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½ã€‚
- en: 'In BigQuery, de-normalization emerges as a preferred strategy for several reasons:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨BigQueryä¸­ï¼Œåè§„èŒƒåŒ–æˆä¸ºé¦–é€‰ç­–ç•¥æœ‰å‡ ä¸ªåŸå› ï¼š
- en: '**Query Performance**: BigQueryâ€™s distributed architecture excels at scanning
    large volumes of data in parallel. De-normalized tables reduce the need for complex
    joins, resulting in faster query execution times.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æŸ¥è¯¢æ€§èƒ½**ï¼šBigQueryçš„åˆ†å¸ƒå¼æ¶æ„åœ¨å¹¶è¡Œæ‰«æå¤§é‡æ•°æ®æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚åè§„èŒƒåŒ–è¡¨å‡å°‘äº†å¤æ‚è¿æ¥çš„éœ€æ±‚ï¼Œä»è€Œç¼©çŸ­æŸ¥è¯¢æ‰§è¡Œæ—¶é—´ã€‚'
- en: '**Cost Efficiency**: By minimizing the computational resources required for
    query processing, de-normalization can lead to cost savings, as query costs in
    BigQuery are based on the amount of data processed.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æˆæœ¬æ•ˆç›Š**ï¼šé€šè¿‡å‡å°‘æŸ¥è¯¢å¤„ç†æ‰€éœ€çš„è®¡ç®—èµ„æºï¼Œåè§„èŒƒåŒ–å¯ä»¥å¸¦æ¥æˆæœ¬èŠ‚çœï¼Œå› ä¸ºBigQueryä¸­çš„æŸ¥è¯¢æˆæœ¬æ˜¯åŸºäºå¤„ç†çš„æ•°æ®é‡è®¡ç®—çš„ã€‚'
- en: '**Simplified Data Modeling**: De-normalized tables simplify the data modeling
    process, making it easier to design and maintain schemas for analytical purposes.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç®€åŒ–æ•°æ®å»ºæ¨¡**ï¼šéè§„èŒƒåŒ–è¡¨ç®€åŒ–äº†æ•°æ®å»ºæ¨¡è¿‡ç¨‹ï¼Œä½¿è®¾è®¡å’Œç»´æŠ¤åˆ†æç”¨é€”çš„æ¶æ„å˜å¾—æ›´åŠ å®¹æ˜“ã€‚'
- en: '**Optimized for Analytical Workloads**: De-normalized structures are well-suited
    for analytical workloads, where aggregations, transformations, and complex queries
    are common.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¼˜åŒ–åˆ†æå·¥ä½œè´Ÿè½½**ï¼šéè§„èŒƒåŒ–ç»“æ„éå¸¸é€‚åˆåˆ†æå·¥ä½œè´Ÿè½½ï¼Œåœ¨è¿™ç§è´Ÿè½½ä¸­ï¼Œèšåˆã€è½¬æ¢å’Œå¤æ‚æŸ¥è¯¢æ˜¯å¸¸è§çš„ã€‚'
- en: 'Also, storage is much cheaper than compute and that means:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œå­˜å‚¨æ¯”è®¡ç®—ä¾¿å®œå¾—å¤šï¼Œè¿™æ„å‘³ç€ï¼š
- en: '***With pre-joined datasets, you exchange compute for storage resources!***'
  id: totrans-94
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***å¯¹äºé¢„å…ˆè¿æ¥çš„æ•°æ®é›†ï¼Œä½ å¯ä»¥ç”¨å­˜å‚¨èµ„æºæ¢å–è®¡ç®—èµ„æºï¼***'
- en: '![](../Images/37e6cbd109fc824810306d381286db7c.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/37e6cbd109fc824810306d381286db7c.png)'
- en: De-normalization (by author)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: éè§„èŒƒåŒ–ï¼ˆä½œè€…è§‚ç‚¹ï¼‰
- en: While de-normalization is not a one-size-fits-all solution, it should be considered
    for cost and performance optimization. However, there are aspects that might lead
    to a different, cost-efficient design.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶éè§„èŒƒåŒ–å¹¶ä¸æ˜¯ä¸€ç§é€‚åˆæ‰€æœ‰æƒ…å†µçš„è§£å†³æ–¹æ¡ˆï¼Œä½†å®ƒåº”è¯¥è¢«è€ƒè™‘ç”¨äºæˆæœ¬å’Œæ€§èƒ½ä¼˜åŒ–ã€‚ç„¶è€Œï¼Œä¹Ÿæœ‰ä¸€äº›æ–¹é¢å¯èƒ½å¯¼è‡´ä¸åŒçš„ã€æˆæœ¬æ•ˆç›Šæ›´é«˜çš„è®¾è®¡ã€‚
- en: Especially when having small tables on the **right side** of the `JOIN`, BigQuery
    utilizes **Broadcast Joins** to broadcast the full dataset of the table to each
    slot which processes the larger table. That way, normalization has no negative
    impact on performance. Actually, the opposite is the case and due to reduced data
    redundancy.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹åˆ«æ˜¯å½“åœ¨`JOIN`çš„**å³ä¾§**æœ‰è¾ƒå°çš„è¡¨æ—¶ï¼ŒBigQuery åˆ©ç”¨**å¹¿æ’­è¿æ¥**å°†è¡¨çš„å®Œæ•´æ•°æ®é›†å¹¿æ’­åˆ°æ¯ä¸ªå¤„ç†è¾ƒå¤§è¡¨çš„æ§½ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œè§„èŒƒåŒ–ä¸ä¼šå¯¹æ€§èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ã€‚äº‹å®ä¸Šï¼Œæƒ…å†µæ°æ°ç›¸åï¼Œå› ä¸ºæ•°æ®å†—ä½™å‡å°‘äº†ã€‚
- en: When BigQuery is not using the Broadcast Join, it uses the **Hash Join** approach.
    In this case, BigQuery uses hash and shuffle operations so that matching keys
    are processed in the same slot in order to perform a local join. However, compared
    to a Broadcast Join, this can be a an expensive operation as data needs to be
    moved.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ BigQuery ä¸ä½¿ç”¨å¹¿æ’­è¿æ¥æ—¶ï¼Œå®ƒä¼šé‡‡ç”¨**å“ˆå¸Œè¿æ¥**æ–¹æ³•ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒBigQuery ä½¿ç”¨å“ˆå¸Œå’Œæ´—ç‰Œæ“ä½œï¼Œä»¥ä¾¿åŒ¹é…çš„é”®åœ¨åŒä¸€æ§½ä¸­å¤„ç†ï¼Œä»è€Œæ‰§è¡Œæœ¬åœ°è¿æ¥ã€‚ç„¶è€Œï¼Œä¸å¹¿æ’­è¿æ¥ç›¸æ¯”ï¼Œè¿™å¯èƒ½æ˜¯ä¸€ä¸ªä»£ä»·é«˜æ˜‚çš„æ“ä½œï¼Œå› ä¸ºéœ€è¦ç§»åŠ¨æ•°æ®ã€‚
- en: If you find yourself in a situation where Hash Joins are being used, there are
    still ways to potentially improve performance. At least aim for defining the join
    columns as cluster columns. This colocates data in the same columnar file, reducing
    the impact of shuffling.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å‘ç°è‡ªå·±å¤„äºå“ˆå¸Œè¿æ¥è¢«ä½¿ç”¨çš„æƒ…å†µï¼Œä»ç„¶æœ‰æ–¹æ³•å¯èƒ½æ”¹å–„æ€§èƒ½ã€‚è‡³å°‘å¯ä»¥å°†è¿æ¥åˆ—å®šä¹‰ä¸ºé›†ç¾¤åˆ—ã€‚è¿™æ ·å¯ä»¥å°†æ•°æ®æ”¾ç½®åœ¨åŒä¸€åˆ—å­˜å‚¨æ–‡ä»¶ä¸­ï¼Œå‡å°‘æ´—ç‰Œçš„å½±å“ã€‚
- en: 'Ultimately, the best approach depends on the specifics of your data model and
    the size of the normalized tables. If redundancy can be reduced with a normalized
    structure while keeping the size of the `JOIN` tables small, so that Broadcast
    Joins are used, this is the better solution than enforcing a de-normalized approach.
    For tables bigger than 10G however, this should be evaluated with concrete benchmarks,
    which leads to the golden rule:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆï¼Œæœ€ä½³æ–¹æ³•å–å†³äºæ•°æ®æ¨¡å‹çš„å…·ä½“æƒ…å†µä»¥åŠè§„èŒƒåŒ–è¡¨çš„å¤§å°ã€‚å¦‚æœé€šè¿‡è§„èŒƒåŒ–ç»“æ„å¯ä»¥å‡å°‘å†—ä½™ï¼ŒåŒæ—¶ä¿æŒ`JOIN`è¡¨çš„å¤§å°è¾ƒå°ï¼Œä»è€Œä½¿ç”¨å¹¿æ’­è¿æ¥ï¼Œé‚£ä¹ˆè¿™æ¯”å¼ºåˆ¶æ‰§è¡Œéè§„èŒƒåŒ–æ–¹æ³•æ›´ä¸ºä¼˜è¶Šã€‚ç„¶è€Œï¼Œå¯¹äºå¤§äº10GBçš„è¡¨ï¼Œåº”è¯¥é€šè¿‡å…·ä½“çš„åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°ï¼Œè¿™ä¹Ÿå¼•å‡ºäº†é»„é‡‘æ³•åˆ™ï¼š
- en: '**Benchmarking is key!** Donâ€™t rely solely on theory. Test different approaches
    (normalized, denormalized, nested/repeated) to find the most efficient solution
    for your specific use case.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**åŸºå‡†æµ‹è¯•æ˜¯å…³é”®ï¼** ä¸è¦ä»…ä»…ä¾èµ–ç†è®ºã€‚æµ‹è¯•ä¸åŒçš„æ–¹æ³•ï¼ˆè§„èŒƒåŒ–ã€éè§„èŒƒåŒ–ã€åµŒå¥—/é‡å¤ï¼‰ï¼Œä»¥æ‰¾åˆ°æœ€é€‚åˆä½ å…·ä½“ç”¨ä¾‹çš„é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚'
- en: Partitioning
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ†åŒº
- en: 'Partitions divide a table into segments based on **one** specific column. The
    partition column can use one of 3 approaches:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†åŒºå°†è¡¨åˆ’åˆ†ä¸ºåŸºäº**ä¸€ä¸ª**ç‰¹å®šåˆ—çš„å¤šä¸ªæ®µã€‚åˆ†åŒºåˆ—å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä¸‰ç§æ–¹æ³•ä¹‹ä¸€ï¼š
- en: 'ğŸ—‚ï¸ **Integer range partitioning**: Partition by integer column based on range
    with start, end and interval'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ—‚ï¸ **æ•´æ•°èŒƒå›´åˆ†åŒº**ï¼šæ ¹æ®æ•´æ•°åˆ—çš„èŒƒå›´è¿›è¡Œåˆ†åŒºï¼ŒèŒƒå›´åŒ…æ‹¬èµ·å§‹å€¼ã€ç»“æŸå€¼å’Œé—´éš”
- en: 'â° **Time-unit partitioning**: Partition by date, timestamp or datetime column
    in table with hourly, daily, monthly or yearly granularity'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: â° **æ—¶é—´å•ä½åˆ†åŒº**ï¼šæŒ‰æ—¥æœŸã€æ—¶é—´æˆ³æˆ–æ—¥æœŸæ—¶é—´åˆ—å¯¹è¡¨è¿›è¡Œåˆ†åŒºï¼Œç²’åº¦å¯ä»¥æ˜¯æ¯å°æ—¶ã€æ¯æ—¥ã€æ¯æœˆæˆ–æ¯å¹´
- en: 'â±ï¸ **Ingestion time partitioning**: Automatically assign partition when inserting
    data based on current time with a pseudocolumn named `_PARTITIONTIME`'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: â±ï¸ **æ‘„å–æ—¶é—´åˆ†åŒº**ï¼šæ ¹æ®å½“å‰æ—¶é—´ï¼Œä½¿ç”¨åä¸º`_PARTITIONTIME`çš„ä¼ªåˆ—åœ¨æ’å…¥æ•°æ®æ—¶è‡ªåŠ¨åˆ†é…åˆ†åŒº
- en: It is up to you to define the partition column but it is highly recommend to
    choose this wisely as it can eliminate a lot of bytes processed / billed.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±ä½ æ¥å®šä¹‰åˆ†åŒºåˆ—ï¼Œä½†å¼ºçƒˆå»ºè®®æ˜æ™ºåœ°é€‰æ‹©è¯¥åˆ—ï¼Œå› ä¸ºè¿™å¯ä»¥å‡å°‘å¤„ç†/è®¡è´¹çš„å­—èŠ‚æ•°ã€‚
- en: '![](../Images/ab874087ba6efbc653ba93c0707b36c0.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ab874087ba6efbc653ba93c0707b36c0.png)'
- en: Partitioning example (by author)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†åŒºç¤ºä¾‹ï¼ˆæŒ‰ä½œè€…ï¼‰
- en: '**Example:**'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¤ºä¾‹ï¼š**'
- en: '[PRE4]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the above example you can also see how to set the `partition_expiration_days`
    option, which will remove partitions older than X days.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œä½ è¿˜å¯ä»¥çœ‹åˆ°å¦‚ä½•è®¾ç½® `partition_expiration_days` é€‰é¡¹ï¼Œè¯¥é€‰é¡¹ä¼šåˆ é™¤è¶…è¿‡ X å¤©çš„åˆ†åŒºã€‚
- en: Clustering
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: èšç±»
- en: Clusters sort the data within each partition based on one ore more columns.
    When using cluster columns in your query filter, this technique will speed up
    the execution since BigQuery can determine which blocks to scan. This is especially
    recommended to use with high cardinality columns such as the `title` column in
    the following example.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: èšç±»æ ¹æ®ä¸€ä¸ªæˆ–å¤šä¸ªåˆ—åœ¨æ¯ä¸ªåˆ†åŒºå†…å¯¹æ•°æ®è¿›è¡Œæ’åºã€‚å½“åœ¨æŸ¥è¯¢ç­›é€‰ä¸­ä½¿ç”¨èšç±»åˆ—æ—¶ï¼Œè¿™é¡¹æŠ€æœ¯å°†åŠ é€Ÿæ‰§è¡Œï¼Œå› ä¸º BigQuery å¯ä»¥ç¡®å®šæ‰«æå“ªäº›æ•°æ®å—ã€‚ç‰¹åˆ«æ¨èåœ¨é«˜åŸºæ•°åˆ—ä¸­ä½¿ç”¨è¯¥æŠ€æœ¯ï¼Œä¾‹å¦‚ä»¥ä¸‹ç¤ºä¾‹ä¸­çš„
    `title` åˆ—ã€‚
- en: You can define up to **four** cluster columns.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥å®šä¹‰æœ€å¤š **å››ä¸ª** èšç±»åˆ—ã€‚
- en: '**Example:**'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¤ºä¾‹ï¼š**'
- en: '[PRE5]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Nested repeated columns
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åµŒå¥—é‡å¤åˆ—
- en: With data de-normalization often also duplication of information is introduced.
    This data redundancy adds additional storage and bytes to be processed in our
    queries. However, there is a way to have a de-normalized table design without
    redundancy using nested repeated columns.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®åè§„èŒƒåŒ–é€šå¸¸ä¹Ÿä¼šå¼•å…¥ä¿¡æ¯çš„é‡å¤ã€‚æ•°æ®å†—ä½™ä¼šå¢åŠ é¢å¤–çš„å­˜å‚¨ç©ºé—´å’ŒæŸ¥è¯¢æ—¶éœ€è¦å¤„ç†çš„å­—èŠ‚æ•°ã€‚ç„¶è€Œï¼Œæœ‰ä¸€ç§æ–¹æ³•å¯ä»¥åœ¨æ²¡æœ‰å†—ä½™çš„æƒ…å†µä¸‹ä½¿ç”¨åµŒå¥—é‡å¤åˆ—å®ç°åè§„èŒƒåŒ–çš„è¡¨è®¾è®¡ã€‚
- en: 'A **nested** column uses the type `struct` and combines certain attributes
    to one object. A nested **repeated** column is an array of `struct`s stored for
    a single row in the table. For example: if you have a table storing one row per
    login of a user, together with the user ID and the registration country of that
    user, you would have redundancy in form of the ID and country per login for each
    user.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**åµŒå¥—**åˆ—ä½¿ç”¨ `struct` ç±»å‹å¹¶å°†æŸäº›å±æ€§ç»„åˆæˆä¸€ä¸ªå¯¹è±¡ã€‚åµŒå¥—çš„ **é‡å¤** åˆ—æ˜¯ä¸€ä¸ª `struct` æ•°ç»„ï¼Œä¸ºè¡¨æ ¼ä¸­çš„å•è¡Œå­˜å‚¨ã€‚ä¾‹å¦‚ï¼šå¦‚æœä½ æœ‰ä¸€ä¸ªè¡¨æ ¼ï¼Œæ¯è¡Œå­˜å‚¨ä¸€ä¸ªç”¨æˆ·çš„ç™»å½•è®°å½•ï¼ŒåŒ…æ‹¬ç”¨æˆ·
    ID å’Œè¯¥ç”¨æˆ·çš„æ³¨å†Œå›½å®¶ï¼Œé‚£ä¹ˆä½ å°±ä¼šåœ¨æ¯ä¸ªç”¨æˆ·çš„æ¯æ¬¡ç™»å½•ä¸­å‡ºç° ID å’Œå›½å®¶çš„å†—ä½™ã€‚'
- en: 'Instead of storing one row per login, with a nested repeated column you can
    store one single row per user and in a column of type `ARRAY<STRUCT<...>>` you
    store an array of all logins of that user. The struct holds all attributes attached
    to the login, like the date and device. The following illustration visualizes
    this example:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å…¶ä¸ºæ¯æ¬¡ç™»å½•å­˜å‚¨ä¸€è¡Œæ•°æ®ï¼Œä½¿ç”¨åµŒå¥—é‡å¤åˆ—ï¼Œä½ å¯ä»¥ä¸ºæ¯ä¸ªç”¨æˆ·å­˜å‚¨ä¸€è¡Œæ•°æ®ï¼Œå¹¶åœ¨ä¸€ä¸ªç±»å‹ä¸º `ARRAY<STRUCT<...>>` çš„åˆ—ä¸­å­˜å‚¨è¯¥ç”¨æˆ·çš„æ‰€æœ‰ç™»å½•è®°å½•ã€‚è¯¥ç»“æ„ä½“åŒ…å«ä¸ç™»å½•ç›¸å…³çš„æ‰€æœ‰å±æ€§ï¼Œä¾‹å¦‚æ—¥æœŸå’Œè®¾å¤‡ã€‚ä»¥ä¸‹æ’å›¾å¯è§†åŒ–äº†è¿™ä¸ªç¤ºä¾‹ï¼š
- en: '![](../Images/f6562058c8182f92d6a5db94c78706eb.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6562058c8182f92d6a5db94c78706eb.png)'
- en: Nested repeated column example (by author)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: åµŒå¥—é‡å¤åˆ—ç¤ºä¾‹ï¼ˆæŒ‰ä½œè€…ï¼‰
- en: '**Example:**'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¤ºä¾‹ï¼š**'
- en: '[PRE6]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The above example also shows the utilization of the `require_partition_filter`
    which will prevent any queries without filtering on the partition column.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šé¢çš„ç¤ºä¾‹è¿˜å±•ç¤ºäº† `require_partition_filter` çš„ä½¿ç”¨ï¼Œè¯¥åŠŸèƒ½ä¼šé˜»æ­¢ä»»ä½•ä¸å¯¹åˆ†åŒºåˆ—è¿›è¡Œç­›é€‰çš„æŸ¥è¯¢ã€‚
- en: 'This data modelling technique can reduce the stored and processed bytes drastically.
    However, it is not the silver bullet for all de-normalization or data modeling
    cases. The major downside is: **you canâ€™t set cluster or partition columns on
    attributes of structs**.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ•°æ®å»ºæ¨¡æŠ€æœ¯å¯ä»¥æ˜¾è‘—å‡å°‘å­˜å‚¨å’Œå¤„ç†çš„å­—èŠ‚æ•°ã€‚ç„¶è€Œï¼Œå®ƒå¹¶ä¸æ˜¯æ‰€æœ‰åè§„èŒƒåŒ–æˆ–æ•°æ®å»ºæ¨¡åœºæ™¯çš„ä¸‡èƒ½è§£å†³æ–¹æ¡ˆã€‚ä¸»è¦çš„ç¼ºç‚¹æ˜¯ï¼š**ä½ ä¸èƒ½åœ¨ç»“æ„ä½“å±æ€§ä¸Šè®¾ç½®èšç±»æˆ–åˆ†åŒºåˆ—**ã€‚
- en: 'That means: in the example above, if a user would filter by `login_device`
    a full table scan is necessary and we do not have the option to optimize this
    with clustering. This can be an issue especially if your table is used as a data
    source for third party software like Excel or PowerBI. In such cases, you should
    carefully evaluate if the benefit of removing redundancy with nested repeated
    columns compensates the lack of optimizations via clustering.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€ï¼šåœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œå¦‚æœç”¨æˆ·æŒ‰ `login_device` è¿›è¡Œç­›é€‰ï¼Œåˆ™éœ€è¦è¿›è¡Œå…¨è¡¨æ‰«æï¼Œè€Œä¸”æˆ‘ä»¬æ²¡æœ‰é€šè¿‡èšç±»è¿›è¡Œä¼˜åŒ–çš„é€‰é¡¹ã€‚ç‰¹åˆ«æ˜¯å½“ä½ çš„è¡¨æ ¼è¢«ç”¨ä½œ
    Excel æˆ– PowerBI ç­‰ç¬¬ä¸‰æ–¹è½¯ä»¶çš„æ•°æ®æºæ—¶ï¼Œè¿™å¯èƒ½ä¼šæˆä¸ºä¸€ä¸ªé—®é¢˜ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ åº”ä»”ç»†è¯„ä¼°é€šè¿‡åµŒå¥—é‡å¤åˆ—å»é™¤å†—ä½™çš„å¥½å¤„æ˜¯å¦è¶³ä»¥å¼¥è¡¥æ— æ³•é€šè¿‡èšç±»è¿›è¡Œä¼˜åŒ–çš„ç¼ºç‚¹ã€‚
- en: Indexing
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç´¢å¼•
- en: By defining a search index on one or multiple columns, BigQuery can use this
    to speed up queries using the `SEARCH` function.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡åœ¨ä¸€ä¸ªæˆ–å¤šä¸ªåˆ—ä¸Šå®šä¹‰æœç´¢ç´¢å¼•ï¼ŒBigQuery å¯ä»¥åˆ©ç”¨æ­¤ç´¢å¼•åŠ é€Ÿä½¿ç”¨ `SEARCH` å‡½æ•°çš„æŸ¥è¯¢ã€‚
- en: 'A search index can be created with the `CREATE SEARCH INDEX` statement:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨`CREATE SEARCH INDEX`è¯­å¥åˆ›å»ºæœç´¢ç´¢å¼•ï¼š
- en: '[PRE7]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'With `ALL COLUMNS` the index is automatically created for all `STRING` and
    `JSON` columns. It is also possible to be more selective and add a list of column
    names instead. With the `SEARCH` function, the index can be utilized to search
    within all or specific columns:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`ALL COLUMNS`æ—¶ï¼Œç´¢å¼•ä¼šè‡ªåŠ¨ä¸ºæ‰€æœ‰`STRING`å’Œ`JSON`åˆ—åˆ›å»ºã€‚ä½ ä¹Ÿå¯ä»¥æ›´æœ‰é€‰æ‹©æ€§åœ°åªä¸ºç‰¹å®šåˆ—æ·»åŠ åˆ—ååˆ—è¡¨ã€‚ä½¿ç”¨`SEARCH`åŠŸèƒ½ï¼Œç´¢å¼•å¯ä»¥åœ¨æ‰€æœ‰æˆ–ç‰¹å®šåˆ—ä¸­è¿›è¡Œæœç´¢ï¼š
- en: '[PRE8]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: A new feature, which is in preview state by the time writing this article, is
    to also utilize the index for operators such as `=`, `IN`, `LIKE`, and `STARTS_WITH`.
    This can be very beneficial for data structures that are directly used by end
    users via third party tools like PowerBI or Excel to further increase speed and
    reduce costs for certain filter operations.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€é¡¹æ–°åŠŸèƒ½ï¼Œåœ¨æ’°å†™æœ¬æ–‡æ—¶å¤„äºé¢„è§ˆçŠ¶æ€ï¼Œå…è®¸å°†ç´¢å¼•ç”¨äºå¦‚`=`ã€`IN`ã€`LIKE`å’Œ`STARTS_WITH`ç­‰æ“ä½œç¬¦ã€‚è¿™å¯¹äºé‚£äº›é€šè¿‡åƒPowerBIæˆ–Excelè¿™æ ·çš„ç¬¬ä¸‰æ–¹å·¥å…·ç›´æ¥ä¾›æœ€ç»ˆç”¨æˆ·ä½¿ç”¨çš„æ•°æ®ç»“æ„éå¸¸æœ‰åˆ©ï¼Œå¯ä»¥è¿›ä¸€æ­¥æé«˜é€Ÿåº¦å¹¶é™ä½æŸäº›è¿‡æ»¤æ“ä½œçš„æˆæœ¬ã€‚
- en: More information about this can be found in the [official search index documentation](https://cloud.google.com/bigquery/docs/search-index).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºè¿™ä¸€ç‚¹çš„æ›´å¤šä¿¡æ¯å¯ä»¥åœ¨[å®˜æ–¹æœç´¢ç´¢å¼•æ–‡æ¡£](https://cloud.google.com/bigquery/docs/search-index)ä¸­æ‰¾åˆ°ã€‚
- en: Physical Bytes Storage Billing
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç‰©ç†å­—èŠ‚å­˜å‚¨è®¡è´¹
- en: 'BigQuery offers two billing models for storage: Standard and Physical Bytes
    Storage Billing. Choosing the right model depends on your data access patterns
    and compression capabilities.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: BigQueryæä¾›äº†ä¸¤ç§å­˜å‚¨è®¡è´¹æ¨¡å‹ï¼šæ ‡å‡†æ¨¡å‹å’Œç‰©ç†å­—èŠ‚å­˜å‚¨è®¡è´¹æ¨¡å‹ã€‚é€‰æ‹©åˆé€‚çš„æ¨¡å‹å–å†³äºä½ çš„æ•°æ®è®¿é—®æ¨¡å¼å’Œå‹ç¼©èƒ½åŠ›ã€‚
- en: The standard model is straightforward. You pay a set price per gigabyte of data,
    with a slight discount for data that hasnâ€™t been modified in 90 days. This is
    simple to use and doesnâ€™t require managing different storage categories. However,
    it can be more expensive if your data is highly compressed or if you donâ€™t access
    it very often.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡å‡†æ¨¡å‹éå¸¸ç›´æ¥ã€‚ä½ æŒ‰æ¯GBæ•°æ®æ”¯ä»˜å›ºå®šè´¹ç”¨ï¼Œå¦‚æœæ•°æ®åœ¨90å¤©å†…æœªä¿®æ”¹ï¼Œè¿˜ä¼šæœ‰è½»å¾®çš„æŠ˜æ‰£ã€‚è¿™ç§æ–¹å¼ç®€å•æ˜“ç”¨ï¼Œä¸éœ€è¦ç®¡ç†ä¸åŒçš„å­˜å‚¨ç±»åˆ«ã€‚ç„¶è€Œï¼Œå¦‚æœä½ çš„æ•°æ®é«˜åº¦å‹ç¼©ï¼Œæˆ–è€…ä½ ä¸ç»å¸¸è®¿é—®å®ƒï¼Œè¿™ç§æ¨¡å‹å¯èƒ½ä¼šæ›´æ˜‚è´µã€‚
- en: 'Physical Bytes Storage Billing takes a different approach. Instead of paying
    based on how much logical data you store, you pay based on the physical space
    it occupies on disk, regardless of how often you access it or how well itâ€™s compressed.
    This model can be **significantly cheaper** for highly compressed data or data
    you donâ€™t access frequently. However, it requires you to manage two separate storage
    classes: one for frequently accessed data and another for long-term storage, which
    can add complexity.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰©ç†å­—èŠ‚å­˜å‚¨è®¡è´¹é‡‡ç”¨ä¸åŒçš„æ–¹æ³•ã€‚ä½ æ”¯ä»˜çš„è´¹ç”¨æ˜¯åŸºäºæ•°æ®åœ¨ç£ç›˜ä¸Šå æ®çš„ç‰©ç†ç©ºé—´ï¼Œè€Œä¸æ˜¯åŸºäºå­˜å‚¨çš„é€»è¾‘æ•°æ®é‡ï¼Œæ— è®ºä½ è®¿é—®å®ƒçš„é¢‘ç‡å¦‚ä½•ï¼Œæˆ–è€…å®ƒå‹ç¼©å¾—æœ‰å¤šå¥½ã€‚å¯¹äºé«˜åº¦å‹ç¼©çš„æ•°æ®æˆ–ä¸ç»å¸¸è®¿é—®çš„æ•°æ®ï¼Œè¿™ç§æ¨¡å‹**å¯èƒ½ä¼šä¾¿å®œå¾—å¤š**ã€‚ç„¶è€Œï¼Œå®ƒè¦æ±‚ä½ ç®¡ç†ä¸¤ç§ç‹¬ç«‹çš„å­˜å‚¨ç±»åˆ«ï¼šä¸€ç§æ˜¯é¢‘ç¹è®¿é—®çš„æ•°æ®ï¼Œå¦ä¸€ç§æ˜¯é•¿æœŸå­˜å‚¨çš„æ•°æ®ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ å¤æ‚æ€§ã€‚
- en: 'So, which model should you choose? **Hereâ€™s a quick guide**:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œä½ åº”è¯¥é€‰æ‹©å“ªä¸ªæ¨¡å‹å‘¢ï¼Ÿ**ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€æ˜æŒ‡å—**ï¼š
- en: '**Choose the standard model if**:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¦‚æœé€‰æ‹©æ ‡å‡†æ¨¡å‹**ï¼š'
- en: Your data isnâ€™t highly compressed.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½ çš„æ•°æ®æ²¡æœ‰é«˜åº¦å‹ç¼©ã€‚
- en: You prefer a simple and easy-to-manage approach.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½ æ›´å€¾å‘äºé€‰æ‹©ç®€å•ä¸”æ˜“äºç®¡ç†çš„æ–¹æ³•ã€‚
- en: '**Choose PBSB if**:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¦‚æœé€‰æ‹©PBSBæ¨¡å‹**ï¼š'
- en: Your data is highly compressed.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½ çš„æ•°æ®é«˜åº¦å‹ç¼©ã€‚
- en: Youâ€™re comfortable managing different storage classes to optimize costs.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½ èƒ½å¤Ÿç®¡ç†ä¸åŒçš„å­˜å‚¨ç±»åˆ«æ¥ä¼˜åŒ–æˆæœ¬ã€‚
- en: You can change the billing model in the advanced option for your datasets. You
    can also check the logical vs. physical bytes in the table details view, which
    makes it easier to decide for a model.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨æ•°æ®é›†çš„é«˜çº§é€‰é¡¹ä¸­æ›´æ”¹è®¡è´¹æ¨¡å‹ã€‚ä½ è¿˜å¯ä»¥åœ¨è¡¨æ ¼è¯¦ç»†ä¿¡æ¯è§†å›¾ä¸­æ£€æŸ¥é€»è¾‘å­—èŠ‚ä¸ç‰©ç†å­—èŠ‚çš„å·®å¼‚ï¼Œè¿™ä½¿å¾—é€‰æ‹©æ¨¡å‹æ›´ä¸ºæ–¹ä¾¿ã€‚
- en: '![](../Images/4cb2180cfd099101aac3829db377d55c.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4cb2180cfd099101aac3829db377d55c.png)'
- en: Dataset advanced options for Storage Billing Model
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: å­˜å‚¨è®¡è´¹æ¨¡å‹çš„æ•°æ®é›†é«˜çº§é€‰é¡¹
- en: Join optimizations with primary keys and foreign keys
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸»é”®å’Œå¤–é”®çš„è¿æ¥ä¼˜åŒ–
- en: Since [July 2023, BigQuery introduced unenforced Primary Key and Foreign Key
    constraints](https://cloud.google.com/blog/products/data-analytics/join-optimizations-with-bigquery-primary-and-foreign-keys?hl=en).
    Keep in mind that BigQuery is not a classical RDBMS, even though defining a data
    model with this feature might give you the feeling that it is.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ª[2023å¹´7æœˆèµ·ï¼ŒBigQueryå¼•å…¥äº†éå¼ºåˆ¶æ€§çš„ä¸»é”®å’Œå¤–é”®çº¦æŸ](https://cloud.google.com/blog/products/data-analytics/join-optimizations-with-bigquery-primary-and-foreign-keys?hl=en)ã€‚è¯·è®°ä½ï¼ŒBigQueryå¹¶ä¸æ˜¯ä¸€ä¸ªç»å…¸çš„å…³ç³»æ•°æ®åº“ç®¡ç†ç³»ç»Ÿï¼Œå°½ç®¡ä½¿ç”¨æ­¤åŠŸèƒ½å®šä¹‰æ•°æ®æ¨¡å‹å¯èƒ½ä¼šè®©ä½ è§‰å¾—å®ƒæ˜¯ã€‚
- en: 'If the keys are not enforced and this is not a relational database as we know
    it, what is the point? The answer is: the query optimizer may use this information
    to better optimize queries, namely with the concepts of Inner Join Elimination,
    Outer Join Elimination and Join Reordering.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœè¿™äº›é”®æ²¡æœ‰å¼ºåˆ¶æ‰§è¡Œï¼Œå¹¶ä¸”è¿™ä¸æ˜¯æˆ‘ä»¬ç†Ÿæ‚‰çš„å…³ç³»æ•°æ®åº“ï¼Œé‚£ä¹ˆæ„ä¹‰ä½•åœ¨ï¼Ÿç­”æ¡ˆæ˜¯ï¼šæŸ¥è¯¢ä¼˜åŒ–å™¨å¯ä»¥åˆ©ç”¨è¿™äº›ä¿¡æ¯æ¥æ›´å¥½åœ°ä¼˜åŒ–æŸ¥è¯¢ï¼Œç‰¹åˆ«æ˜¯åœ¨å†…éƒ¨è¿æ¥æ¶ˆé™¤ã€å¤–éƒ¨è¿æ¥æ¶ˆé™¤å’Œè¿æ¥é‡æ’åºç­‰æ¦‚å¿µä¸Šã€‚
- en: 'Defining constraints is similar to other SQL dialects, just that you have to
    specify them as `NOT ENFORCED`:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: å®šä¹‰çº¦æŸç±»ä¼¼äºå…¶ä»– SQL æ–¹è¨€ï¼Œåªä¸è¿‡ä½ å¿…é¡»å°†å…¶æŒ‡å®šä¸º `NOT ENFORCED`ï¼š
- en: '[PRE9]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: âš™ï¸ Data operations
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: âš™ï¸ æ•°æ®æ“ä½œ
- en: Copy data / tables
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¤åˆ¶æ•°æ® / è¡¨
- en: 'Copying data from one place to another is a typical part of our daily business
    as Data Engineers. Letâ€™s assume the task is to copy data from a BigQuery dataset
    called `bronze` to another dataset called `silver` within a Google Cloud Platform
    project called `project_x`. The naive approach would be a simple SQL query like:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä¸€ä¸ªåœ°æ–¹å¤åˆ¶æ•°æ®åˆ°å¦ä¸€ä¸ªåœ°æ–¹æ˜¯æˆ‘ä»¬ä½œä¸ºæ•°æ®å·¥ç¨‹å¸ˆæ—¥å¸¸å·¥ä½œçš„ä¸€éƒ¨åˆ†ã€‚å‡è®¾ä»»åŠ¡æ˜¯å°†åä¸º `bronze` çš„ BigQuery æ•°æ®é›†ä¸­çš„æ•°æ®å¤åˆ¶åˆ°å¦ä¸€ä¸ªåä¸º
    `silver` çš„æ•°æ®é›†ï¼Œä¸”è¯¥æ•°æ®é›†ä½äºåä¸º `project_x` çš„ Google Cloud Platform é¡¹ç›®ä¸­ã€‚ç®€å•çš„æ–¹æ³•æ˜¯æ‰§è¡Œå¦‚ä¸‹ SQL
    æŸ¥è¯¢ï¼š
- en: '[PRE10]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Even though this allows for transformation, in many cases we simply want to
    copy data from one place to another. The bytes billed for the query above would
    essentially be the amount of data we have to read from the source. However, we
    can also get this **for free** with the following query:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡è¿™æ ·å¯ä»¥è¿›è¡Œè½¬æ¢ï¼Œä½†åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åªæ˜¯å¸Œæœ›å°†æ•°æ®ä»ä¸€ä¸ªåœ°æ–¹å¤åˆ¶åˆ°å¦ä¸€ä¸ªåœ°æ–¹ã€‚ä¸Šé¢æŸ¥è¯¢çš„è®¡è´¹å­—èŠ‚æ•°åŸºæœ¬ä¸Šæ˜¯æˆ‘ä»¬éœ€è¦ä»æºå¤´è¯»å–çš„æ•°æ®é‡ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡ä»¥ä¸‹æŸ¥è¯¢**å…è´¹**è·å¾—è¿™äº›æ•°æ®ï¼š
- en: '[PRE11]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Alternatively, the `bq` CLI tool can be used to achieve the same result:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ï¼Œå¯ä»¥ä½¿ç”¨ `bq` CLI å·¥å…·æ¥å®ç°ç›¸åŒçš„ç»“æœï¼š
- en: '[PRE12]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: That way, you can copy data for **0 costs**.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è¿™ç§æ–¹å¼ï¼Œä½ å¯ä»¥ä»¥**0è´¹ç”¨**å¤åˆ¶æ•°æ®ã€‚
- en: Load data
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ è½½æ•°æ®
- en: For data ingestion Google Cloud Storage is a pragmatic way to solve the task.
    No matter if it is a CSV file, ORC / Parquet files from a Hadoop ecosystem or
    any other source. Data can easily be uploaded and stored for low costs.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ•°æ®æ‘„å–ï¼ŒGoogle Cloud Storage æ˜¯è§£å†³è¯¥ä»»åŠ¡çš„åŠ¡å®æ–¹å¼ã€‚æ— è®ºæ˜¯ CSV æ–‡ä»¶ã€æ¥è‡ª Hadoop ç”Ÿæ€ç³»ç»Ÿçš„ ORC / Parquet
    æ–‡ä»¶ï¼Œè¿˜æ˜¯å…¶ä»–ä»»ä½•æ¥æºï¼Œéƒ½å¯ä»¥è½»æ¾ä¸Šä¼ å¹¶ä»¥ä½æˆæœ¬å­˜å‚¨æ•°æ®ã€‚
- en: It is also possible to create BigQuery tables on top of data stored in GCS.
    These **external** tables still utilize the compute infrastructure from BigQuery
    but do not offer some of the features and performance.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿå¯ä»¥åœ¨ GCS ä¸Šçš„æ•°æ®åŸºç¡€ä¸Šåˆ›å»º BigQuery è¡¨ã€‚è¿™äº›**å¤–éƒ¨**è¡¨ä»ç„¶åˆ©ç”¨ BigQuery çš„è®¡ç®—åŸºç¡€è®¾æ–½ï¼Œä½†ä¸æä¾›æŸäº›åŠŸèƒ½å’Œæ€§èƒ½ã€‚
- en: Letâ€™s assume we upload data from a partitioned Hive table using the ORC storage
    format. Uploading the data can be achieved using `distcp` or simply by getting
    the data from HDFS first and then uploading it to GCS using one of the available
    CLI tools to interact with Cloud Storage.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬ä»ä½¿ç”¨ ORC å­˜å‚¨æ ¼å¼çš„åˆ†åŒº Hive è¡¨ä¸Šä¼ æ•°æ®ã€‚ä¸Šä¼ æ•°æ®å¯ä»¥ä½¿ç”¨ `distcp` å®Œæˆï¼Œæˆ–è€…é€šè¿‡å…ˆä» HDFS è·å–æ•°æ®ï¼Œç„¶åä½¿ç”¨ä¸ Cloud
    Storage äº¤äº’çš„å¯ç”¨ CLI å·¥å…·å°†å…¶ä¸Šä¼ åˆ° GCSã€‚
- en: 'Assuming we have a partition structure including one partition called month,
    the files might look like the following:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªåŒ…å«åä¸º month çš„åˆ†åŒºç»“æ„ï¼Œé‚£ä¹ˆæ–‡ä»¶å¯èƒ½çœ‹èµ·æ¥å¦‚ä¸‹ï¼š
- en: '[PRE13]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'When we uploaded this data to GCS, an external table definition can be created
    like this:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬å°†è¿™äº›æ•°æ®ä¸Šä¼ åˆ° GCS æ—¶ï¼Œå¯ä»¥åƒè¿™æ ·åˆ›å»ºå¤–éƒ¨è¡¨å®šä¹‰ï¼š
- en: '[PRE14]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: It will derive the schema from the ORC files and even detect the partition column.
    The naive approach to move this data from GCS to BigQuery storage might now be,
    to create a table in BigQuery and then follow the pragmatic `INSERT INTO ... SELECT
    FROM` approach.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒå°†ä» ORC æ–‡ä»¶ä¸­æ¨å¯¼å‡ºæ¨¡å¼ï¼Œç”šè‡³æ£€æµ‹åˆ†åŒºåˆ—ã€‚å°†è¿™äº›æ•°æ®ä» GCS ç§»åŠ¨åˆ° BigQuery å­˜å‚¨çš„ç®€å•æ–¹æ³•ç°åœ¨å¯èƒ½æ˜¯ï¼Œåœ¨ BigQuery ä¸­åˆ›å»ºä¸€ä¸ªè¡¨ï¼Œç„¶åæŒ‰ç…§åŠ¡å®çš„
    `INSERT INTO ... SELECT FROM` æ–¹æ³•æ“ä½œã€‚
- en: However, similar to the previous example, the bytes billed would reflect the
    amount of data stored in `gs://project_x/ingest/some_orc_table`. There is another
    way, which will achieve the same result but again for **0 costs** using the `LOAD
    DATA` SQL statement.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œç±»ä¼¼äºå‰é¢çš„ä¾‹å­ï¼Œè®¡è´¹çš„å­—èŠ‚æ•°å°†åæ˜ å­˜å‚¨åœ¨ `gs://project_x/ingest/some_orc_table` ä¸­çš„æ•°æ®é‡ã€‚è¿˜æœ‰å¦ä¸€ç§æ–¹å¼ï¼Œå¯ä»¥é€šè¿‡ä½¿ç”¨
    `LOAD DATA` SQL è¯­å¥ä»¥**0è´¹ç”¨**å®ç°ç›¸åŒçš„ç»“æœã€‚
- en: '[PRE15]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Using this statement, we directly get a BigQuery table with the data ingested,
    **no need to create an external table first**! Also this query comes at **0 costs**.
    The `OVERWRITE` is optional, since data can also be appended instead of overwriting
    the table on every run.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ­¤è¯­å¥ï¼Œæˆ‘ä»¬ç›´æ¥è·å–åŒ…å«æ•°æ®çš„ BigQuery è¡¨ï¼Œ**æ— éœ€å…ˆåˆ›å»ºå¤–éƒ¨è¡¨**ï¼æ­¤å¤–ï¼Œæ­¤æŸ¥è¯¢**æ²¡æœ‰è´¹ç”¨**ã€‚`OVERWRITE` æ˜¯å¯é€‰çš„ï¼Œå› ä¸ºæ•°æ®ä¹Ÿå¯ä»¥è¿½åŠ ï¼Œè€Œä¸æ˜¯æ¯æ¬¡è¿è¡Œæ—¶éƒ½è¦†ç›–è¡¨ã€‚
- en: 'As you can see, also the partition columns can be specified. Even though no
    transformation can be applied, there is one major advantage: we can already define
    cluster columns. That way, we can create an efficient version of the target table
    for further downstream processing, for free!'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œåˆ†åŒºåˆ—ä¹Ÿå¯ä»¥è¢«æŒ‡å®šã€‚è™½ç„¶ä¸èƒ½åº”ç”¨ä»»ä½•è½¬æ¢ï¼Œä½†æœ‰ä¸€ä¸ªä¸»è¦ä¼˜ç‚¹ï¼šæˆ‘ä»¬å¯ä»¥æå‰å®šä¹‰é›†ç¾¤åˆ—ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªé«˜æ•ˆçš„ç›®æ ‡è¡¨ç‰ˆæœ¬ï¼Œç”¨äºåç»­çš„ä¸‹æ¸¸å¤„ç†ï¼Œä¸”**å…è´¹**ï¼
- en: Delete partitions
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ é™¤åˆ†åŒº
- en: In certain ETL or ELT scenarios, a typical workflow is to have a table partitioned
    by day and then replace specific partitions based on new data coming from a staging
    / ingestion table.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æŸäº›ETLæˆ–ELTåœºæ™¯ä¸­ï¼Œå…¸å‹çš„å·¥ä½œæµæ˜¯å°†è¡¨æŒ‰å¤©åˆ†åŒºï¼Œç„¶åæ ¹æ®æ¥è‡ªä¸´æ—¶/æ‘„å–è¡¨çš„æ–°æ•°æ®æ›¿æ¢ç‰¹å®šåˆ†åŒºã€‚
- en: '![](../Images/c0c962a5f17c681e88a437f318b19468.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c0c962a5f17c681e88a437f318b19468.png)'
- en: Ingest partition example (by author)
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†åŒºæ‘„å–ç¤ºä¾‹ï¼ˆä½œè€…ï¼‰
- en: BigQuery offers the `MERGE` statement but the naive approach is to first delete
    the affected partitions from the target table and then insert the data.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: BigQueryæä¾›äº†`MERGE`è¯­å¥ï¼Œä½†ç®€å•çš„æ–¹æ³•æ˜¯å…ˆä»ç›®æ ‡è¡¨ä¸­åˆ é™¤å—å½±å“çš„åˆ†åŒºï¼Œç„¶åæ’å…¥æ•°æ®ã€‚
- en: 'Deleting partitions in such a scenario can be achieved like this:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåˆ é™¤åˆ†åŒºå¯ä»¥è¿™æ ·å®ç°ï¼š
- en: '[PRE16]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Even if `day` is a partition column in both cases, this operation is connected
    to several costs. However, again there is an alternative solution that comes at
    **0 costs** again:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿`day`æ˜¯ä¸¤ç§æƒ…å†µä¸‹çš„åˆ†åŒºåˆ—ï¼Œè¯¥æ“ä½œä»ç„¶æ¶‰åŠè‹¥å¹²æˆæœ¬ã€‚ç„¶è€Œï¼Œå†ä¸€æ¬¡ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ª**é›¶æˆæœ¬**çš„æ›¿ä»£æ–¹æ¡ˆï¼š
- en: '[PRE17]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: With `DROP TABLE` you can actually also just drop one single partition by appending
    the suffix `$<partition_id>`.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`DROP TABLE`ï¼Œä½ å®é™…ä¸Šä¹Ÿå¯ä»¥é€šè¿‡é™„åŠ åç¼€`$<partition_id>`æ¥åˆ é™¤å•ä¸ªåˆ†åŒºã€‚
- en: Of course the above example is just dropping one partition. However, with the
    procedual language from BigQuery, we can easily execute the statement in a loop.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œä¸Šé¢çš„ç¤ºä¾‹åªæ˜¯åˆ é™¤ä¸€ä¸ªåˆ†åŒºã€‚ç„¶è€Œï¼Œä½¿ç”¨BigQueryçš„è¿‡ç¨‹è¯­è¨€ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾åœ°åœ¨å¾ªç¯ä¸­æ‰§è¡Œè¯¥è¯­å¥ã€‚
- en: '[PRE18]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Or alternatively use Airflow and/or dbt to first select the partitions and then
    run a certain templated query in a loop.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ï¼Œå¯ä»¥ä½¿ç”¨Airflowå’Œ/æˆ–dbtï¼Œå…ˆé€‰æ‹©åˆ†åŒºï¼Œç„¶ååœ¨å¾ªç¯ä¸­è¿è¡ŒæŸä¸ªæ¨¡æ¿åŒ–æŸ¥è¯¢ã€‚
- en: However, getting the distinct partitions for a partitioned table can be done
    like the in the examples above, but this will still cause some costs even if we
    only read a single column. But yet again, there is a way to get this almost for
    free, which we will explore in the next chapter.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œè·å–åˆ†åŒºè¡¨çš„ä¸åŒåˆ†åŒºå¯ä»¥åƒä¸Šè¿°ç¤ºä¾‹é‚£æ ·è¿›è¡Œï¼Œä½†å³ä½¿æˆ‘ä»¬åªè¯»å–å•ä¸€åˆ—ï¼Œè¿™ä»ç„¶ä¼šäº§ç”Ÿä¸€äº›æˆæœ¬ã€‚ä½†å†æ¬¡å¼ºè°ƒï¼Œæœ‰ä¸€ç§å‡ ä¹**å…è´¹**çš„æ–¹æ³•æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ç« ä¸­æ¢è®¨ã€‚
- en: Get distinct partitions for a table
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è·å–è¡¨çš„ä¸åŒåˆ†åŒº
- en: 'In the examples above, we used the following approach to get the distinct partitions
    of a partitioned BigQuery table:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä»¥ä¸‹æ–¹æ³•æ¥è·å–åˆ†åŒºçš„ä¸åŒåˆ†åŒºï¼š
- en: '[PRE19]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This is how much the query cost me in an example use-case I worked on:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘åœ¨ä¸€ä¸ªç¤ºä¾‹ç”¨ä¾‹ä¸­æŸ¥è¯¢çš„æˆæœ¬ï¼š
- en: '[PRE20]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'BigQuery maintains a lot of valuable metadata about tables, columns and partitions.
    This can be accessed via the `INFORMATION_SCHEMA`. We can achieve the very same
    result, by simply using this metadata:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: BigQueryç»´æŠ¤äº†å¤§é‡å…³äºè¡¨ã€åˆ—å’Œåˆ†åŒºçš„æœ‰ä»·å€¼å…ƒæ•°æ®ã€‚è¿™äº›ä¿¡æ¯å¯ä»¥é€šè¿‡`INFORMATION_SCHEMA`è®¿é—®ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ç®€å•åœ°ä½¿ç”¨è¿™äº›å…ƒæ•°æ®æ¥å®ç°å®Œå…¨ç›¸åŒçš„ç»“æœï¼š
- en: '[PRE21]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'And comparing it with the same use-case as I mentioned above, this is how much
    the query cost:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æˆ‘ä¹‹å‰æåˆ°çš„ç›¸åŒç”¨ä¾‹è¿›è¡Œæ¯”è¾ƒï¼Œè¿™å°±æ˜¯æŸ¥è¯¢çš„æˆæœ¬ï¼š
- en: '[PRE22]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As you can see, 149GB vs 10MB is a huge difference. With this method, you can
    get the distinct partitions even for huge tables at **almost 0 costs**.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œ149GBä¸10MBä¹‹é—´çš„å·®è·éå¸¸å¤§ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œå³ä½¿æ˜¯å·¨å¤§çš„è¡¨ï¼Œä½ ä¹Ÿå¯ä»¥ä»¥**å‡ ä¹é›¶æˆæœ¬**è·å–ä¸åŒçš„åˆ†åŒºã€‚
- en: Do not persist calculated measures
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸è¦æŒä¹…åŒ–è®¡ç®—çš„åº¦é‡
- en: When you start using BigQuery for the first projects, you will most likely stick
    with the on-demand compute pricing model. With on-demand pricing, you will generally
    have access to up to 2000 concurrent slots, shared among all queries in a single
    project. But even with capacity pricing, you will have a minimum of 100 slots.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½ å¼€å§‹ä½¿ç”¨BigQueryè¿›è¡Œç¬¬ä¸€ä¸ªé¡¹ç›®æ—¶ï¼Œä½ å¾ˆå¯èƒ½ä¼šé€‰æ‹©æŒ‰éœ€è®¡ç®—å®šä»·æ¨¡å¼ã€‚ä½¿ç”¨æŒ‰éœ€å®šä»·ï¼Œä½ é€šå¸¸å¯ä»¥è®¿é—®æœ€å¤š2000ä¸ªå¹¶å‘æ§½ä½ï¼Œæ‰€æœ‰æŸ¥è¯¢å…±äº«è¿™äº›æ§½ä½ã€‚ä½†å³ä½¿ä½¿ç”¨å®¹é‡å®šä»·ï¼Œä½ ä¹Ÿä¼šè‡³å°‘æ‹¥æœ‰100ä¸ªæ§½ä½ã€‚
- en: With a lot of the daily ETL / ELT workload, these slots are actually not the
    limitation of the performance. You can simply check this yourself by navigating
    to BigQuery -> Administration -> Monitoring, select the correct location and change
    the Chart to *Slot Usage* under *Chart Configuration*. In a lot of cases you will
    be surprised how little slots you are actually using.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå¤§éƒ¨åˆ†æ—¥å¸¸çš„ ETL/ELT å·¥ä½œè´Ÿè½½ï¼Œè¿™äº›æ’æ§½å®é™…ä¸Šå¹¶ä¸æ˜¯æ€§èƒ½çš„ç“¶é¢ˆã€‚ä½ å¯ä»¥é€šè¿‡è¿›å…¥ BigQuery -> ç®¡ç† -> ç›‘æ§ï¼Œé€‰æ‹©æ­£ç¡®çš„ä½ç½®ï¼Œå¹¶åœ¨â€œå›¾è¡¨é…ç½®â€ä¸‹å°†å›¾è¡¨æ”¹ä¸º
    *æ’æ§½ä½¿ç”¨æƒ…å†µ* æ¥è‡ªè¡Œæ£€æŸ¥ã€‚åœ¨å¾ˆå¤šæƒ…å†µä¸‹ï¼Œä½ ä¼šæƒŠè®¶äºå®é™…ä½¿ç”¨çš„æ’æ§½æ˜¯å¤šä¹ˆå°‘ã€‚
- en: '![](../Images/3b574cfdb6e528945373257885f71b59.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3b574cfdb6e528945373257885f71b59.png)'
- en: BigQuery Monitoring for slots
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: BigQuery æ’æ§½ç›‘æ§
- en: How does that relate to saving costs? Letâ€™s assume you have a classic fact table
    or some table in general, which delivers certain KPIs. This table is then used
    for analysis / reporting in Looker, Excel, PowerBI or other tools.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸èŠ‚çœæˆæœ¬æœ‰ä»€ä¹ˆå…³ç³»å‘¢ï¼Ÿå‡è®¾ä½ æœ‰ä¸€ä¸ªç»å…¸çš„äº‹å®è¡¨ï¼Œæˆ–è€…ä¸€èˆ¬çš„æŸä¸ªè¡¨ï¼Œå®ƒæä¾›äº†æŸäº›å…³é”®ç»©æ•ˆæŒ‡æ ‡ï¼ˆKPIï¼‰ã€‚è¿™ä¸ªè¡¨éšåè¢«ç”¨äº Lookerã€Excelã€PowerBI
    æˆ–å…¶ä»–å·¥å…·ä¸­çš„åˆ†æ/æŠ¥å‘Šã€‚
- en: Often these tools automatically generate queries to serve the report or dashboard
    with the necessary data. These generated queries might not be ideal, when it comes
    to applying BigQuery best practices. In other words, they might end up scanning
    more data than necessary which increases the bytes billed.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œè¿™äº›å·¥å…·ä¼šè‡ªåŠ¨ç”ŸæˆæŸ¥è¯¢æ¥ä¸ºæŠ¥å‘Šæˆ–ä»ªè¡¨ç›˜æä¾›æ‰€éœ€çš„æ•°æ®ã€‚è¿™äº›è‡ªåŠ¨ç”Ÿæˆçš„æŸ¥è¯¢å¯èƒ½åœ¨åº”ç”¨ BigQuery æœ€ä½³å®è·µæ—¶å¹¶ä¸ç†æƒ³ã€‚æ¢å¥è¯è¯´ï¼Œå®ƒä»¬å¯èƒ½ä¼šæ‰«ææ¯”å¿…è¦æ›´å¤šçš„æ•°æ®ï¼Œä»è€Œå¢åŠ è®¡è´¹çš„å­—èŠ‚æ•°ã€‚
- en: We can avoid this, by introducing a view layer on top of our fact tables. Serving
    tools with data from a view rather than the actual table is a very valuable best
    practice, as it gives you more flexibility when it comes to schema changes but
    it also gives the possibility to introduce calculated measures within the view
    without persisting the data.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡åœ¨äº‹å®è¡¨ä¸Šæ–¹å¼•å…¥è§†å›¾å±‚æ¥é¿å…è¿™ä¸€ç‚¹ã€‚é€šè¿‡è§†å›¾è€Œéå®é™…è¡¨æä¾›æ•°æ®ç»™æœåŠ¡å·¥å…·æ˜¯ä¸€ç§éå¸¸æœ‰ä»·å€¼çš„æœ€ä½³å®è·µï¼Œå› ä¸ºè¿™ä¸ºä½ åœ¨æ¨¡å¼å˜æ›´æ—¶æä¾›äº†æ›´å¤šçµæ´»æ€§ï¼ŒåŒæ—¶ä¹Ÿèƒ½åœ¨è§†å›¾ä¸­å¼•å…¥è®¡ç®—åº¦é‡è€Œä¸éœ€è¦æŒä¹…åŒ–æ•°æ®ã€‚
- en: Of course this might increase the CPU usage when these measures are used but
    on the other hand, it can drastically reduce the total size of the underlying
    table.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œå½“ä½¿ç”¨è¿™äº›åº¦é‡æ—¶ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ  CPU ä½¿ç”¨ç‡ï¼Œä½†å¦ä¸€æ–¹é¢ï¼Œå®ƒä¹Ÿèƒ½æ˜¾è‘—å‡å°‘åº•å±‚è¡¨çš„æ€»å¤§å°ã€‚
- en: 'To illustrate this principle, take the following fact table as a basis:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¯´æ˜è¿™ä¸€åŸåˆ™ï¼Œä»¥ä»¥ä¸‹äº‹å®è¡¨ä¸ºåŸºç¡€ï¼š
- en: '[PRE23]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The basic idea is to introduce a view for stakeholders accessing this data
    and extend it with calculated measures:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºæœ¬æ€è·¯æ˜¯ä¸ºè®¿é—®è¿™äº›æ•°æ®çš„åˆ©ç›Šç›¸å…³è€…å¼•å…¥è§†å›¾ï¼Œå¹¶é€šè¿‡è®¡ç®—åº¦é‡å¯¹å…¶è¿›è¡Œæ‰©å±•ï¼š
- en: '[PRE24]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In this example we were able to avoid persisting two `INT64` values. One of
    these uses 8 logical bytes. If our fact table has 1,000,000,000 rows this would
    mean we save:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬èƒ½å¤Ÿé¿å…æŒä¹…åŒ–ä¸¤ä¸ª `INT64` å€¼ã€‚æ¯ä¸€ä¸ªå€¼éƒ½ä½¿ç”¨ 8 ä¸ªé€»è¾‘å­—èŠ‚ã€‚å¦‚æœæˆ‘ä»¬çš„äº‹å®è¡¨æœ‰ 1,000,000,000 è¡Œï¼Œé‚£ä¹ˆè¿™æ„å‘³ç€æˆ‘ä»¬èŠ‚çœäº†ï¼š
- en: '[PRE25]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This is not a huge amount of data, but it can mean that BigQuery has to scan
    15 GB less data in certain situations. In practice, there can be calculated measures
    that might save you much more data to be scanned.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¹¶ä¸æ˜¯å¤§é‡æ•°æ®ï¼Œä½†å®ƒå¯èƒ½æ„å‘³ç€åœ¨æŸäº›æƒ…å†µä¸‹ BigQuery å¿…é¡»æ‰«æçš„ 15 GB æ•°æ®å‡å°‘äº†ã€‚å®é™…ä¸Šï¼Œå¯èƒ½å­˜åœ¨è®¡ç®—åº¦é‡ï¼Œå®ƒä»¬èƒ½å¤ŸèŠ‚çœæ›´å¤šçš„æ•°æ®æ‰«æé‡ã€‚
- en: ğŸ“š Summary
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ“š æ€»ç»“
- en: Forget hoarding every byte like a dragon guarding its treasure. Instead, learn
    to burn data through smart management and optimization ğŸ”¥. By embracing this fiery
    approach, youâ€™ll transform BigQuery from a cost center to a powerful engine for
    data exploration, allowing you to burn data, not money!
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¦åƒå®ˆæŠ¤å®è—çš„å·¨é¾™ä¸€æ ·å›¤ç§¯æ¯ä¸€ä¸ªå­—èŠ‚ã€‚ç›¸åï¼Œå­¦ä¼šé€šè¿‡æ™ºèƒ½ç®¡ç†å’Œä¼˜åŒ–æ¥â€œç‡ƒçƒ§â€æ•°æ® ğŸ”¥ã€‚é€šè¿‡é‡‡ç”¨è¿™ç§æ¿€çƒˆçš„æ–¹æ³•ï¼Œä½ å°†æŠŠ BigQuery ä»ä¸€ä¸ªæˆæœ¬ä¸­å¿ƒè½¬å˜ä¸ºå¼ºå¤§çš„æ•°æ®æ¢ç´¢å¼•æ“ï¼Œè®©ä½ ç‡ƒçƒ§çš„æ˜¯æ•°æ®ï¼Œè€Œä¸æ˜¯é‡‘é’±ï¼
- en: Embrace data modeling best practices
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é‡‡ç”¨æ•°æ®å»ºæ¨¡æœ€ä½³å®è·µ
- en: Utilize the smallest data types possible to minimize storage and processing
    costs.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ©ç”¨å°½å¯èƒ½å°çš„æ•°æ®ç±»å‹ä»¥æœ€å°åŒ–å­˜å‚¨å’Œå¤„ç†æˆæœ¬ã€‚
- en: Leverage de-normalization when appropriate to optimize query performance and
    reduce storage usage.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨é€‚å½“çš„æ—¶å€™åˆ©ç”¨å»è§„èŒƒåŒ–ï¼Œä»¥ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½å¹¶å‡å°‘å­˜å‚¨ä½¿ç”¨ã€‚
- en: Implement partitioning and clustering to enable BigQuery to efficiently scan
    only the relevant data for your queries.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®æ–½åˆ†åŒºå’Œèšç±»ï¼Œä½¿ BigQuery èƒ½å¤Ÿé«˜æ•ˆåœ°ä»…æ‰«ææŸ¥è¯¢æ‰€éœ€çš„ç›¸å…³æ•°æ®ã€‚
- en: Explore nested repeated columns as a way to eliminate redundancy while maintaining
    data integrity, but be mindful of limitations regarding clustering.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¢ç´¢åµŒå¥—é‡å¤åˆ—ä½œä¸ºæ¶ˆé™¤å†—ä½™å¹¶ä¿æŒæ•°æ®å®Œæ•´æ€§çš„ä¸€ç§æ–¹å¼ï¼Œä½†è¦æ³¨æ„èšç±»çš„é™åˆ¶ã€‚
- en: Master data operations for cost-effectiveness
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æŒæ¡æ•°æ®æ“ä½œä»¥æé«˜æˆæœ¬æ•ˆç›Š
- en: Employ `CREATE TABLE ... COPY` or `bq cp` commands to copy data between tables
    without incurring charges.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ `CREATE TABLE ... COPY` æˆ– `bq cp` å‘½ä»¤åœ¨è¡¨ä¹‹é—´å¤åˆ¶æ•°æ®ï¼Œæ— éœ€äº§ç”Ÿè´¹ç”¨ã€‚
- en: Utilize `LOAD DATA` statements to directly load data from Cloud Storage into
    BigQuery tables, again at no cost.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ `LOAD DATA` è¯­å¥ç›´æ¥ä» Cloud Storage åŠ è½½æ•°æ®åˆ° BigQuery è¡¨ä¸­ï¼Œä¸”æ— éœ€ä»˜è´¹ã€‚
- en: Leverage the power of `DROP TABLE` with partition suffixes to efficiently remove
    specific partitions.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ©ç”¨ `DROP TABLE` å’Œåˆ†åŒºåç¼€çš„åŠŸèƒ½é«˜æ•ˆåœ°åˆ é™¤ç‰¹å®šåˆ†åŒºã€‚
- en: Utilize `INFORMATION_SCHEMA` to retrieve table metadata like distinct partition
    values, significantly reducing costs compared to traditional queries.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ©ç”¨ `INFORMATION_SCHEMA` è·å–è¡¨çš„å…ƒæ•°æ®ï¼Œä¾‹å¦‚ä¸åŒçš„åˆ†åŒºå€¼ï¼Œç›¸æ¯”ä¼ ç»ŸæŸ¥è¯¢å¤§å¤§é™ä½æˆæœ¬ã€‚
- en: Design for efficiency and avoid unnecessary data persistence
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®¾è®¡æ—¶è¦æ³¨é‡æ•ˆç‡ï¼Œé¿å…ä¸å¿…è¦çš„æ•°æ®æŒä¹…åŒ–ã€‚
- en: Implement a view layer to serve data with calculated measures, preventing the
    storage of redundant data.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ç°è§†å›¾å±‚æ¥æä¾›è®¡ç®—åº¦é‡çš„æ•°æ®ï¼Œé¿å…å­˜å‚¨å†—ä½™æ•°æ®ã€‚
- en: Monitor your BigQuery slot usage to understand if slot limitations are a concern,
    allowing you to focus on optimizing query structures.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç›‘æ§ä½ çš„ BigQuery æ’æ§½ä½¿ç”¨æƒ…å†µï¼Œä»¥äº†è§£æ’æ§½é™åˆ¶æ˜¯å¦æ˜¯ä¸€ä¸ªé—®é¢˜ï¼Œä»è€Œè®©ä½ å¯ä»¥é›†ä¸­ç²¾åŠ›ä¼˜åŒ–æŸ¥è¯¢ç»“æ„ã€‚
- en: '***By adopting these strategies, you can unlock the true potential of BigQuery,
    transforming it into a cost-effective engine for data exploration and analysis.
    Remember, in the realm of BigQuery, itâ€™s all about burning data, not money!***'
  id: totrans-234
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***é€šè¿‡é‡‡ç”¨è¿™äº›ç­–ç•¥ï¼Œä½ å¯ä»¥é‡Šæ”¾ BigQuery çš„çœŸæ­£æ½œåŠ›ï¼Œå°†å…¶è½¬å˜ä¸ºä¸€ä¸ªé«˜æ•ˆä¸”å…·æˆæœ¬æ•ˆç›Šçš„æ•°æ®æ¢ç´¢å’Œåˆ†æå¼•æ“ã€‚è®°ä½ï¼Œåœ¨ BigQuery çš„ä¸–ç•Œé‡Œï¼Œå…³é”®æ˜¯æ¶ˆè€—æ•°æ®ï¼Œè€Œä¸æ˜¯é‡‘é’±ï¼***'
- en: '**Feel free to share your experiences in the comments!**'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ¬¢è¿åœ¨è¯„è®ºåŒºåˆ†äº«ä½ çš„ç»éªŒï¼**'
