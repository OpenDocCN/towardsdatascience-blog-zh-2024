- en: A Definitive Guide to Using BigQuery Efficiently
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一份关于如何高效使用BigQuery的终极指南
- en: 原文：[https://towardsdatascience.com/burn-data-rather-than-money-with-bigquery-the-definitive-guide-1b50a9fdf096?source=collection_archive---------2-----------------------#2024-03-03](https://towardsdatascience.com/burn-data-rather-than-money-with-bigquery-the-definitive-guide-1b50a9fdf096?source=collection_archive---------2-----------------------#2024-03-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/burn-data-rather-than-money-with-bigquery-the-definitive-guide-1b50a9fdf096?source=collection_archive---------2-----------------------#2024-03-03](https://towardsdatascience.com/burn-data-rather-than-money-with-bigquery-the-definitive-guide-1b50a9fdf096?source=collection_archive---------2-----------------------#2024-03-03)
- en: Make the most out of your BigQuery usage, burn data rather than money to create
    real value with some practical techniques.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 充分利用你的BigQuery使用，烧掉数据而不是烧掉钱，用一些实用技巧创造真正的价值。
- en: '[](https://vojay.medium.com/?source=post_page---byline--1b50a9fdf096--------------------------------)[![Volker
    Janz](../Images/0825160d6d521f4152948f0187cf354b.png)](https://vojay.medium.com/?source=post_page---byline--1b50a9fdf096--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1b50a9fdf096--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1b50a9fdf096--------------------------------)
    [Volker Janz](https://vojay.medium.com/?source=post_page---byline--1b50a9fdf096--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://vojay.medium.com/?source=post_page---byline--1b50a9fdf096--------------------------------)[![Volker
    Janz](../Images/0825160d6d521f4152948f0187cf354b.png)](https://vojay.medium.com/?source=post_page---byline--1b50a9fdf096--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1b50a9fdf096--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1b50a9fdf096--------------------------------)
    [Volker Janz](https://vojay.medium.com/?source=post_page---byline--1b50a9fdf096--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1b50a9fdf096--------------------------------)
    ·20 min read·Mar 3, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: · 发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1b50a9fdf096--------------------------------)
    · 20分钟阅读 · 2024年3月3日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: · [📝 Introduction](#b5f1)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: · [📝 引言](#b5f1)
- en: · [💎 BigQuery basics and understanding costs](#ecac)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: · [💎 BigQuery基础和理解成本](#ecac)
- en: ∘ [Storage](#2451)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [存储](#2451)
- en: ∘ [Compute](#73e8)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [计算](#73e8)
- en: · [📐 Data modeling](#c6f3)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: · [📐 数据建模](#c6f3)
- en: ∘ [Data types](#5d43)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [数据类型](#5d43)
- en: ∘ [The shift towards de-normalization](#20ab)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [向去规范化转变](#20ab)
- en: ∘ [Partitioning](#acf8)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [分区](#acf8)
- en: ∘ [Clustering](#8d1d)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [聚类](#8d1d)
- en: ∘ [Nested repeated columns](#2c8a)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [嵌套重复列](#2c8a)
- en: ∘ [Indexing](#7ed9)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [索引](#7ed9)
- en: ∘ [Physical Bytes Storage Billing](#507a)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [物理字节存储计费](#507a)
- en: ∘ [Join optimizations with primary keys and foreign keys](#258d)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [使用主键和外键的连接优化](#258d)
- en: · [⚙️ Data operations](#6bae)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: · [⚙️ 数据操作](#6bae)
- en: ∘ [Copy data / tables](#28ad)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [复制数据/表](#28ad)
- en: ∘ [Load data](#da01)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [加载数据](#da01)
- en: ∘ [Delete partitions](#0ef6)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [删除分区](#0ef6)
- en: ∘ [Get distinct partitions for a table](#0c4d)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [获取表的不同分区](#0c4d)
- en: ∘ [Do not persist calculated measures](#3ae7)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [不要持久化计算的度量](#3ae7)
- en: · [📚 Summary](#59d4)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: · [📚 摘要](#59d4)
- en: ∘ [Embrace data modeling best practices](#165f)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [遵循数据建模最佳实践](#165f)
- en: ∘ [Master data operations for cost-effectiveness](#8018)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [掌握数据操作以实现成本效益](#8018)
- en: ∘ [Design for efficiency and avoid unnecessary data persistence](#af6f)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [为了效率而设计，避免不必要的数据持久化](#af6f)
- en: '**Disclaimer**: BigQuery is a product which is constantly being developed,
    pricing might change at any time and this article is based on my own experience.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**免责声明**：BigQuery是一个不断发展的产品，定价可能随时变化，本文基于我个人的经验。'
- en: '![](../Images/ad3d101dc043e43fc1b8f1c82871e0d7.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad3d101dc043e43fc1b8f1c82871e0d7.png)'
- en: Photo by [Konstantin Evdokimov](https://unsplash.com/@constantinevdokimov?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由[Konstantin Evdokimov](https://unsplash.com/@constantinevdokimov?utm_source=medium&utm_medium=referral)拍摄，照片来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 📝 Introduction
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 📝 引言
- en: 'In the field of data warehousing, there’s a universal truth: managing data
    can be costly. Like a dragon guarding its treasure, each byte stored and each
    query executed demands its share of gold coins. But let me give you a magical
    spell to appease the dragon: burn data, not money!'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据仓库领域，有一个普遍的真理：管理数据可能是昂贵的。就像一条守护财宝的龙，每个存储的字节和每个执行的查询都需要它的金币份额。但是让我给你一个魔法咒语来安抚这条龙：烧掉数据，而不是烧掉钱！
- en: In this article, we will unravel the arts of BigQuery sorcery, to reduce costs
    while increasing efficiency, and beyond. Join as we journey through the depths
    of cost optimization, where every byte is a precious coin.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将揭开 BigQuery 魔法的艺术，旨在在提高效率的同时降低成本，甚至更多。加入我们，一同探索成本优化的深度，在这里，每个字节都是珍贵的硬币。
- en: '![](../Images/26edd2af9f5eda4849e2b289d311ab64.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26edd2af9f5eda4849e2b289d311ab64.png)'
- en: Photo by [Jonathan Kemper](https://unsplash.com/@jupp?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Jonathan Kemper](https://unsplash.com/@jupp?utm_source=medium&utm_medium=referral)
    提供，来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 💎 BigQuery basics and understanding costs
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 💎 BigQuery 基础和成本理解
- en: BigQuery is not just a tool but a package of scalable compute and storage technologies,
    with fast network, everything managed by Google. At its core, BigQuery is a serverless
    Data Warehouse for analytical purposes and built-in features like Machine Learning
    (*BigQuery ML*). BigQuery separates storage and compute with Google’s Jupiter
    network in-between to utilize 1 Petabit/sec of total bisection bandwidth. The
    storage system is using Capacitor, a proprietary columnar storage format by Google
    for semi-structured data and the file system underneath is Colossus, the distributed
    file system by Google. The compute engine is based on Dremel and it uses Borg
    for cluster management, running thousands of Dremel jobs across cluster(s).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: BigQuery 不仅仅是一个工具，而是一整套可扩展的计算和存储技术包，配备快速网络，一切由谷歌管理。在其核心，BigQuery 是一个无服务器的数据仓库，专为分析目的而设计，内置特性如机器学习（*BigQuery
    ML*）。BigQuery 将存储和计算分开，通过谷歌的 Jupiter 网络进行连接，以利用 1 Petabit/秒的总双向带宽。存储系统使用 Capacitor，这是谷歌为半结构化数据提供的专有列式存储格式，底层的文件系统是谷歌的分布式文件系统
    Colossus。计算引擎基于 Dremel，并使用 Borg 进行集群管理，能够跨多个集群运行成千上万的 Dremel 作业。
- en: '***BigQuery is not just a tool but a package of scalable compute and storage
    technologies, with fast network, everything managed by Google***'
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***BigQuery 不仅仅是一个工具，而是一整套可扩展的计算和存储技术包，配备快速网络，一切由谷歌管理***'
- en: 'The following illustration shows the basic architecture of how BigQuery is
    structured:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以下插图展示了 BigQuery 架构的基本结构：
- en: '![](../Images/348b1c767b85af6fbb00e012327b8993.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/348b1c767b85af6fbb00e012327b8993.png)'
- en: BigQuery architecture (by author)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: BigQuery 架构（作者）
- en: Data can be stored in Colossus, however, it is also possible to create BigQuery
    tables on top of data stored in Google Cloud Storage. In that case, queries are
    still processed using the BigQuery compute infrastructure but read data from GCS
    instead. Such `external tables` come with some disadvantages but in some cases
    it can be more cost efficient to have the data stored in GCS. Also, sometimes
    it is not about Big Data but simply reading data from existing CSV files that
    are somehow ingested to GCS. For simplicity it can also be benificial to use these
    kind of tables.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可以存储在 Colossus 中，但也可以在 Google Cloud Storage 中创建 BigQuery 表。在这种情况下，查询仍然通过 BigQuery
    计算基础设施处理，但读取的数据来自 GCS。此类 `外部表` 有一些缺点，但在某些情况下，将数据存储在 GCS 中可能更具成本效益。另外，有时候并不是关于大数据，而仅仅是从现有的
    CSV 文件中读取数据，这些文件以某种方式被导入到 GCS。为了简便起见，使用这种表格也可能带来好处。
- en: '![](../Images/f263f69736f23df70761a9222df7e3b2.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f263f69736f23df70761a9222df7e3b2.png)'
- en: BigQuery external tables (by author)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: BigQuery 外部表（作者）
- en: To utilize the full potential of BigQuery, the regular case is to store data
    in the BigQuery storage.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要充分发挥 BigQuery 的潜力，常见的做法是将数据存储在 BigQuery 存储中。
- en: The main drivers for costs are storage and compute, Google is not charging you
    for other parts, like the network transfer in between storage and compute.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 成本的主要驱动因素是存储和计算，谷歌不会对其他部分收费，比如存储与计算之间的网络传输。
- en: Storage
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 存储
- en: Storage costs you $0.02 per GB — $0.04 per GB for active and $0.01 per GB —
    $0.02 per GB for inactive data (*which means not modified in the last 90 days*).
    If you have a table or partition that is not modified for 90 consecutive days,
    it is considered long term storage, and the price of storage automatically drops
    by 50%. Discount is applied on a per-table, per-partition basis. Modification
    resets the 90-day counter.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 存储费用为每 GB $0.02 — 每 GB $0.04 为活跃数据，和每 GB $0.01 — 每 GB $0.02 为非活跃数据（*即在过去 90
    天内未被修改的数据*）。如果你的表或分区在连续 90 天内没有修改，则被视为长期存储，存储价格将自动下降 50%。折扣是基于每个表或每个分区应用的。修改操作会重置
    90 天计时器。
- en: Compute
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算
- en: BigQuery charges for data scanned and not the runtime of the query, also transfer
    from storage to compute cluster is not charged. Compute costs depend on the location,
    the costs for `europe-west3` are $8.13 per TB for example.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: BigQuery 按扫描的数据量收费，而不是查询的运行时，也不会对从存储到计算集群的传输收费。计算成本取决于位置，例如 `europe-west3` 的费用为每
    TB $8.13。
- en: 'This means:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着：
- en: '***We want to minimize the data to be scanned for each query!***'
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***我们希望最小化每个查询要扫描的数据量！***'
- en: '![](../Images/fb555e76a253c89bb09904ccc83e1e24.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fb555e76a253c89bb09904ccc83e1e24.png)'
- en: 'Left: [Jp Valery](https://unsplash.com/de/@jpvalery?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/de/fotos/zeitrafferfotografie-mehrerer-brennender-us-dollar-banknoten-blOLCO2K4M0?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash),
    right: [Gabriel Jimenez](https://unsplash.com/de/@gabrielj_photography?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/de/fotos/bokeh-fotografie-einer-person-die-erde-tragt-jin4W1HqgL4?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 左： [Jp Valery](https://unsplash.com/de/@jpvalery?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    在 [Unsplash](https://unsplash.com/de/fotos/zeitrafferfotografie-mehrerer-brennender-us-dollar-banknoten-blOLCO2K4M0?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)，右：
    [Gabriel Jimenez](https://unsplash.com/de/@gabrielj_photography?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    在 [Unsplash](https://unsplash.com/de/fotos/bokeh-fotografie-einer-person-die-erde-tragt-jin4W1HqgL4?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
- en: When executing a query, BigQuery is estimating the data to be processed. After
    entering your query in the BigQuery Studio query editor, you can see the estimate
    on the top right.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行查询时，BigQuery 会估算要处理的数据量。在 BigQuery Studio 查询编辑器中输入查询后，你可以在右上角看到估算值。
- en: '![](../Images/51bf962d037887c304ce20cda8662211.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51bf962d037887c304ce20cda8662211.png)'
- en: BigQuery Studio
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: BigQuery Studio
- en: 'If it says 1.27 GB like in the screenshot above and the query is processed
    in the location `europe-west3`, the costs can be calculated like this:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果像上图所示显示为 1.27 GB，并且查询在 `europe-west3` 位置处理，则可以按以下方式计算费用：
- en: '[PRE0]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The estimate is mostly a pessimistic calculation, often the optimizer is able
    to use cached results, materialized views or other techniques, so that the actual
    bytes billed are lower than the estimate. It is still a good practice to check
    this estimate in order to get a rough feeling of the impact of your work.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 估算值通常是一个悲观的计算，优化器通常能够利用缓存结果、物化视图或其他技术，从而使实际计费字节数低于估算值。检查这个估算值仍然是一个好的做法，可以让你大致了解工作影响。
- en: It is also possible to set a maximum for the bytes billed for your query. If
    your query exceeds the limit it will fail and create no costs at all. The setting
    can be changed by navigating to More -> Query settings -> Advanced options ->
    Maximum bytes billed.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以为查询设置最大计费字节数。如果查询超过该限制，它将失败，并且不会产生任何费用。这个设置可以通过导航到 更多 -> 查询设置 -> 高级选项 ->
    最大计费字节数 来更改。
- en: '![](../Images/31c6507a2b70acf6778a6ef827a030bd.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31c6507a2b70acf6778a6ef827a030bd.png)'
- en: BigQuery Query Settings
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: BigQuery 查询设置
- en: '![](../Images/cbe8c884ffa43f6cf07d42f442d90060.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cbe8c884ffa43f6cf07d42f442d90060.png)'
- en: BigQuery exceeded limit for bytes billed
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: BigQuery 超过了计费字节数的限制
- en: Unfortunately up until now, it is not possible to set a default value per query.
    It is only possible to limit the bytes billed for each day per user per project
    or for all bytes billed combined per day for a project.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，直到现在，无法为每个查询设置默认值。只能限制每个用户每天每个项目的计费字节数，或限制一个项目每天的所有字节总计。
- en: When you start using BigQuery for the first projects, you will most likely stick
    with the on-demand compute pricing model. With on-demand pricing, you will generally
    have access to up to 2000 concurrent slots, shared among all queries in a single
    project, which is more than enough in most cases. A slot is like a virtual CPU
    working on a unit of work of your query DAG.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当你第一次开始使用 BigQuery 进行项目时，你很可能会选择按需计算定价模型。在按需定价模型下，你通常可以访问最多 2000 个并发槽，这些槽会在单个项目的所有查询之间共享，这在大多数情况下是足够的。一个槽类似于一个虚拟
    CPU，处理查询 DAG 的一单位工作。
- en: When reaching a certain spending per month, it is worth looking into the capacity
    pricing model, which gives you more predictable costs.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当每月支出达到一定金额时，值得考虑容量定价模型，这样可以让成本更具可预测性。
- en: 📐 Data modeling
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 📐 数据建模
- en: Data types
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据类型
- en: 'To reduce the costs for storage but also compute, it is very important to always
    use the smallest datatype possible for your columns. You can easily estimate the
    costs for a certain amount of rows following this overview:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少存储和计算成本，始终使用最小的数据类型对于你的列非常重要。你可以轻松地通过以下概览估算一定数量行数据的成本：
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`*NULL*` *is calculated as 0 logical bytes*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`*NULL*` *被计算为0逻辑字节*'
- en: '**Example**:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例**：'
- en: '[PRE2]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'With this definition and the table of datatypes, it is possible to estimate
    the logical size of 100,000,000 rows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个定义和数据类型表，能够估算100,000,000行数据的逻辑大小：
- en: '[PRE3]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Assuming we are running a `SELECT *` on this table, it would cost us 5.78 GB
    / 1024 = 0.0056 TB * $8.13 = $0.05 in `europe-west3`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们在这个表上执行`SELECT *`，它将花费我们5.78 GB / 1024 = 0.0056 TB * $8.13 = $0.05，在`europe-west3`区域。
- en: It is a good idea to make these calculations before designing your data model,
    not only to optimize the datatype usage but also to get an estimate of the costs
    for the project that you are working on.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计数据模型之前进行这些计算是一个好主意，这不仅有助于优化数据类型的使用，还能估算你所从事项目的成本。
- en: The shift towards de-normalization
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向反规范化转变
- en: In the realm of database design and management, data normalization and de-normalization
    are fundamental concepts aimed at optimizing data structures for efficient storage,
    retrieval, and manipulation. Traditionally, normalization has been hailed as a
    best practice, emphasizing the reduction of redundancy and the preservation of
    data integrity. However, in the context of BigQuery and other modern data warehouses,
    the dynamics shift, and de-normalization often emerges as the preferred approach.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据库设计和管理领域，数据规范化和反规范化是旨在优化数据结构以实现高效存储、检索和操作的基本概念。传统上，规范化被誉为最佳实践，强调减少冗余并保持数据完整性。然而，在BigQuery和其他现代数据仓库的背景下，动态发生变化，反规范化往往成为首选的方法。
- en: In normalized databases, data is structured into multiple tables, each representing
    a distinct entity or concept, and linked through relationships such as one-to-one,
    one-to-many, or many-to-many. This approach adheres to the principles laid out
    by database normalization forms, such as the First Normal Form (1NF), Second Normal
    Form (2NF), and Third Normal Form (3NF), among others.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在规范化数据库中，数据被组织成多个表，每个表代表一个独立的实体或概念，并通过一对一、一对多或多对多等关系进行连接。这种方法遵循数据库规范化形式的原则，如第一范式（1NF）、第二范式（2NF）和第三范式（3NF）等。
- en: This comes with the advantages of reduction of redundancy, data integrity and
    consequently, less storage usage.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这带来了减少冗余、数据完整性以及因此减少存储使用的优势。
- en: '![](../Images/da2e4c1933ff53547b874460d16bfabc.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da2e4c1933ff53547b874460d16bfabc.png)'
- en: Photo by [Shubham Dhage](https://unsplash.com/@theshubhamdhage?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Shubham Dhage](https://unsplash.com/@theshubhamdhage?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: While data normalization holds merit in traditional relational databases, the
    paradigm shifts when dealing with modern analytics platforms like BigQuery. BigQuery
    is designed for handling massive volumes of data and performing complex analytical
    queries at scale. In this environment, the emphasis shifts from minimizing storage
    space to optimizing query performance.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管数据规范化在传统关系型数据库中具有优势，但在处理像BigQuery这样的现代分析平台时，范式发生了转变。BigQuery旨在处理海量数据并执行大规模的复杂分析查询。在这种环境下，重点从最小化存储空间转向优化查询性能。
- en: 'In BigQuery, de-normalization emerges as a preferred strategy for several reasons:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在BigQuery中，反规范化成为首选策略有几个原因：
- en: '**Query Performance**: BigQuery’s distributed architecture excels at scanning
    large volumes of data in parallel. De-normalized tables reduce the need for complex
    joins, resulting in faster query execution times.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询性能**：BigQuery的分布式架构在并行扫描大量数据方面表现出色。反规范化表减少了复杂连接的需求，从而缩短查询执行时间。'
- en: '**Cost Efficiency**: By minimizing the computational resources required for
    query processing, de-normalization can lead to cost savings, as query costs in
    BigQuery are based on the amount of data processed.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本效益**：通过减少查询处理所需的计算资源，反规范化可以带来成本节省，因为BigQuery中的查询成本是基于处理的数据量计算的。'
- en: '**Simplified Data Modeling**: De-normalized tables simplify the data modeling
    process, making it easier to design and maintain schemas for analytical purposes.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简化数据建模**：非规范化表简化了数据建模过程，使设计和维护分析用途的架构变得更加容易。'
- en: '**Optimized for Analytical Workloads**: De-normalized structures are well-suited
    for analytical workloads, where aggregations, transformations, and complex queries
    are common.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化分析工作负载**：非规范化结构非常适合分析工作负载，在这种负载中，聚合、转换和复杂查询是常见的。'
- en: 'Also, storage is much cheaper than compute and that means:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，存储比计算便宜得多，这意味着：
- en: '***With pre-joined datasets, you exchange compute for storage resources!***'
  id: totrans-94
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***对于预先连接的数据集，你可以用存储资源换取计算资源！***'
- en: '![](../Images/37e6cbd109fc824810306d381286db7c.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/37e6cbd109fc824810306d381286db7c.png)'
- en: De-normalization (by author)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 非规范化（作者观点）
- en: While de-normalization is not a one-size-fits-all solution, it should be considered
    for cost and performance optimization. However, there are aspects that might lead
    to a different, cost-efficient design.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然非规范化并不是一种适合所有情况的解决方案，但它应该被考虑用于成本和性能优化。然而，也有一些方面可能导致不同的、成本效益更高的设计。
- en: Especially when having small tables on the **right side** of the `JOIN`, BigQuery
    utilizes **Broadcast Joins** to broadcast the full dataset of the table to each
    slot which processes the larger table. That way, normalization has no negative
    impact on performance. Actually, the opposite is the case and due to reduced data
    redundancy.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是当在`JOIN`的**右侧**有较小的表时，BigQuery 利用**广播连接**将表的完整数据集广播到每个处理较大表的槽。通过这种方式，规范化不会对性能产生负面影响。事实上，情况恰恰相反，因为数据冗余减少了。
- en: When BigQuery is not using the Broadcast Join, it uses the **Hash Join** approach.
    In this case, BigQuery uses hash and shuffle operations so that matching keys
    are processed in the same slot in order to perform a local join. However, compared
    to a Broadcast Join, this can be a an expensive operation as data needs to be
    moved.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当 BigQuery 不使用广播连接时，它会采用**哈希连接**方法。在这种情况下，BigQuery 使用哈希和洗牌操作，以便匹配的键在同一槽中处理，从而执行本地连接。然而，与广播连接相比，这可能是一个代价高昂的操作，因为需要移动数据。
- en: If you find yourself in a situation where Hash Joins are being used, there are
    still ways to potentially improve performance. At least aim for defining the join
    columns as cluster columns. This colocates data in the same columnar file, reducing
    the impact of shuffling.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你发现自己处于哈希连接被使用的情况，仍然有方法可能改善性能。至少可以将连接列定义为集群列。这样可以将数据放置在同一列存储文件中，减少洗牌的影响。
- en: 'Ultimately, the best approach depends on the specifics of your data model and
    the size of the normalized tables. If redundancy can be reduced with a normalized
    structure while keeping the size of the `JOIN` tables small, so that Broadcast
    Joins are used, this is the better solution than enforcing a de-normalized approach.
    For tables bigger than 10G however, this should be evaluated with concrete benchmarks,
    which leads to the golden rule:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，最佳方法取决于数据模型的具体情况以及规范化表的大小。如果通过规范化结构可以减少冗余，同时保持`JOIN`表的大小较小，从而使用广播连接，那么这比强制执行非规范化方法更为优越。然而，对于大于10GB的表，应该通过具体的基准测试来评估，这也引出了黄金法则：
- en: '**Benchmarking is key!** Don’t rely solely on theory. Test different approaches
    (normalized, denormalized, nested/repeated) to find the most efficient solution
    for your specific use case.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**基准测试是关键！** 不要仅仅依赖理论。测试不同的方法（规范化、非规范化、嵌套/重复），以找到最适合你具体用例的高效解决方案。'
- en: Partitioning
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分区
- en: 'Partitions divide a table into segments based on **one** specific column. The
    partition column can use one of 3 approaches:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 分区将表划分为基于**一个**特定列的多个段。分区列可以使用以下三种方法之一：
- en: '🗂️ **Integer range partitioning**: Partition by integer column based on range
    with start, end and interval'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 🗂️ **整数范围分区**：根据整数列的范围进行分区，范围包括起始值、结束值和间隔
- en: '⏰ **Time-unit partitioning**: Partition by date, timestamp or datetime column
    in table with hourly, daily, monthly or yearly granularity'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ⏰ **时间单位分区**：按日期、时间戳或日期时间列对表进行分区，粒度可以是每小时、每日、每月或每年
- en: '⏱️ **Ingestion time partitioning**: Automatically assign partition when inserting
    data based on current time with a pseudocolumn named `_PARTITIONTIME`'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ⏱️ **摄取时间分区**：根据当前时间，使用名为`_PARTITIONTIME`的伪列在插入数据时自动分配分区
- en: It is up to you to define the partition column but it is highly recommend to
    choose this wisely as it can eliminate a lot of bytes processed / billed.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 由你来定义分区列，但强烈建议明智地选择该列，因为这可以减少处理/计费的字节数。
- en: '![](../Images/ab874087ba6efbc653ba93c0707b36c0.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ab874087ba6efbc653ba93c0707b36c0.png)'
- en: Partitioning example (by author)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 分区示例（按作者）
- en: '**Example:**'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：**'
- en: '[PRE4]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the above example you can also see how to set the `partition_expiration_days`
    option, which will remove partitions older than X days.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的示例中，你还可以看到如何设置 `partition_expiration_days` 选项，该选项会删除超过 X 天的分区。
- en: Clustering
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类
- en: Clusters sort the data within each partition based on one ore more columns.
    When using cluster columns in your query filter, this technique will speed up
    the execution since BigQuery can determine which blocks to scan. This is especially
    recommended to use with high cardinality columns such as the `title` column in
    the following example.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类根据一个或多个列在每个分区内对数据进行排序。当在查询筛选中使用聚类列时，这项技术将加速执行，因为 BigQuery 可以确定扫描哪些数据块。特别推荐在高基数列中使用该技术，例如以下示例中的
    `title` 列。
- en: You can define up to **four** cluster columns.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以定义最多 **四个** 聚类列。
- en: '**Example:**'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：**'
- en: '[PRE5]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Nested repeated columns
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌套重复列
- en: With data de-normalization often also duplication of information is introduced.
    This data redundancy adds additional storage and bytes to be processed in our
    queries. However, there is a way to have a de-normalized table design without
    redundancy using nested repeated columns.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 数据反规范化通常也会引入信息的重复。数据冗余会增加额外的存储空间和查询时需要处理的字节数。然而，有一种方法可以在没有冗余的情况下使用嵌套重复列实现反规范化的表设计。
- en: 'A **nested** column uses the type `struct` and combines certain attributes
    to one object. A nested **repeated** column is an array of `struct`s stored for
    a single row in the table. For example: if you have a table storing one row per
    login of a user, together with the user ID and the registration country of that
    user, you would have redundancy in form of the ID and country per login for each
    user.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**嵌套**列使用 `struct` 类型并将某些属性组合成一个对象。嵌套的 **重复** 列是一个 `struct` 数组，为表格中的单行存储。例如：如果你有一个表格，每行存储一个用户的登录记录，包括用户
    ID 和该用户的注册国家，那么你就会在每个用户的每次登录中出现 ID 和国家的冗余。'
- en: 'Instead of storing one row per login, with a nested repeated column you can
    store one single row per user and in a column of type `ARRAY<STRUCT<...>>` you
    store an array of all logins of that user. The struct holds all attributes attached
    to the login, like the date and device. The following illustration visualizes
    this example:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 与其为每次登录存储一行数据，使用嵌套重复列，你可以为每个用户存储一行数据，并在一个类型为 `ARRAY<STRUCT<...>>` 的列中存储该用户的所有登录记录。该结构体包含与登录相关的所有属性，例如日期和设备。以下插图可视化了这个示例：
- en: '![](../Images/f6562058c8182f92d6a5db94c78706eb.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6562058c8182f92d6a5db94c78706eb.png)'
- en: Nested repeated column example (by author)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌套重复列示例（按作者）
- en: '**Example:**'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：**'
- en: '[PRE6]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The above example also shows the utilization of the `require_partition_filter`
    which will prevent any queries without filtering on the partition column.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的示例还展示了 `require_partition_filter` 的使用，该功能会阻止任何不对分区列进行筛选的查询。
- en: 'This data modelling technique can reduce the stored and processed bytes drastically.
    However, it is not the silver bullet for all de-normalization or data modeling
    cases. The major downside is: **you can’t set cluster or partition columns on
    attributes of structs**.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这种数据建模技术可以显著减少存储和处理的字节数。然而，它并不是所有反规范化或数据建模场景的万能解决方案。主要的缺点是：**你不能在结构体属性上设置聚类或分区列**。
- en: 'That means: in the example above, if a user would filter by `login_device`
    a full table scan is necessary and we do not have the option to optimize this
    with clustering. This can be an issue especially if your table is used as a data
    source for third party software like Excel or PowerBI. In such cases, you should
    carefully evaluate if the benefit of removing redundancy with nested repeated
    columns compensates the lack of optimizations via clustering.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着：在上面的示例中，如果用户按 `login_device` 进行筛选，则需要进行全表扫描，而且我们没有通过聚类进行优化的选项。特别是当你的表格被用作
    Excel 或 PowerBI 等第三方软件的数据源时，这可能会成为一个问题。在这种情况下，你应仔细评估通过嵌套重复列去除冗余的好处是否足以弥补无法通过聚类进行优化的缺点。
- en: Indexing
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 索引
- en: By defining a search index on one or multiple columns, BigQuery can use this
    to speed up queries using the `SEARCH` function.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在一个或多个列上定义搜索索引，BigQuery 可以利用此索引加速使用 `SEARCH` 函数的查询。
- en: 'A search index can be created with the `CREATE SEARCH INDEX` statement:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`CREATE SEARCH INDEX`语句创建搜索索引：
- en: '[PRE7]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'With `ALL COLUMNS` the index is automatically created for all `STRING` and
    `JSON` columns. It is also possible to be more selective and add a list of column
    names instead. With the `SEARCH` function, the index can be utilized to search
    within all or specific columns:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`ALL COLUMNS`时，索引会自动为所有`STRING`和`JSON`列创建。你也可以更有选择性地只为特定列添加列名列表。使用`SEARCH`功能，索引可以在所有或特定列中进行搜索：
- en: '[PRE8]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: A new feature, which is in preview state by the time writing this article, is
    to also utilize the index for operators such as `=`, `IN`, `LIKE`, and `STARTS_WITH`.
    This can be very beneficial for data structures that are directly used by end
    users via third party tools like PowerBI or Excel to further increase speed and
    reduce costs for certain filter operations.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 一项新功能，在撰写本文时处于预览状态，允许将索引用于如`=`、`IN`、`LIKE`和`STARTS_WITH`等操作符。这对于那些通过像PowerBI或Excel这样的第三方工具直接供最终用户使用的数据结构非常有利，可以进一步提高速度并降低某些过滤操作的成本。
- en: More information about this can be found in the [official search index documentation](https://cloud.google.com/bigquery/docs/search-index).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这一点的更多信息可以在[官方搜索索引文档](https://cloud.google.com/bigquery/docs/search-index)中找到。
- en: Physical Bytes Storage Billing
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 物理字节存储计费
- en: 'BigQuery offers two billing models for storage: Standard and Physical Bytes
    Storage Billing. Choosing the right model depends on your data access patterns
    and compression capabilities.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: BigQuery提供了两种存储计费模型：标准模型和物理字节存储计费模型。选择合适的模型取决于你的数据访问模式和压缩能力。
- en: The standard model is straightforward. You pay a set price per gigabyte of data,
    with a slight discount for data that hasn’t been modified in 90 days. This is
    simple to use and doesn’t require managing different storage categories. However,
    it can be more expensive if your data is highly compressed or if you don’t access
    it very often.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 标准模型非常直接。你按每GB数据支付固定费用，如果数据在90天内未修改，还会有轻微的折扣。这种方式简单易用，不需要管理不同的存储类别。然而，如果你的数据高度压缩，或者你不经常访问它，这种模型可能会更昂贵。
- en: 'Physical Bytes Storage Billing takes a different approach. Instead of paying
    based on how much logical data you store, you pay based on the physical space
    it occupies on disk, regardless of how often you access it or how well it’s compressed.
    This model can be **significantly cheaper** for highly compressed data or data
    you don’t access frequently. However, it requires you to manage two separate storage
    classes: one for frequently accessed data and another for long-term storage, which
    can add complexity.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 物理字节存储计费采用不同的方法。你支付的费用是基于数据在磁盘上占据的物理空间，而不是基于存储的逻辑数据量，无论你访问它的频率如何，或者它压缩得有多好。对于高度压缩的数据或不经常访问的数据，这种模型**可能会便宜得多**。然而，它要求你管理两种独立的存储类别：一种是频繁访问的数据，另一种是长期存储的数据，这可能会增加复杂性。
- en: 'So, which model should you choose? **Here’s a quick guide**:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你应该选择哪个模型呢？**以下是一个简明指南**：
- en: '**Choose the standard model if**:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果选择标准模型**：'
- en: Your data isn’t highly compressed.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的数据没有高度压缩。
- en: You prefer a simple and easy-to-manage approach.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你更倾向于选择简单且易于管理的方法。
- en: '**Choose PBSB if**:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果选择PBSB模型**：'
- en: Your data is highly compressed.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的数据高度压缩。
- en: You’re comfortable managing different storage classes to optimize costs.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你能够管理不同的存储类别来优化成本。
- en: You can change the billing model in the advanced option for your datasets. You
    can also check the logical vs. physical bytes in the table details view, which
    makes it easier to decide for a model.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在数据集的高级选项中更改计费模型。你还可以在表格详细信息视图中检查逻辑字节与物理字节的差异，这使得选择模型更为方便。
- en: '![](../Images/4cb2180cfd099101aac3829db377d55c.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4cb2180cfd099101aac3829db377d55c.png)'
- en: Dataset advanced options for Storage Billing Model
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 存储计费模型的数据集高级选项
- en: Join optimizations with primary keys and foreign keys
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主键和外键的连接优化
- en: Since [July 2023, BigQuery introduced unenforced Primary Key and Foreign Key
    constraints](https://cloud.google.com/blog/products/data-analytics/join-optimizations-with-bigquery-primary-and-foreign-keys?hl=en).
    Keep in mind that BigQuery is not a classical RDBMS, even though defining a data
    model with this feature might give you the feeling that it is.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 自[2023年7月起，BigQuery引入了非强制性的主键和外键约束](https://cloud.google.com/blog/products/data-analytics/join-optimizations-with-bigquery-primary-and-foreign-keys?hl=en)。请记住，BigQuery并不是一个经典的关系数据库管理系统，尽管使用此功能定义数据模型可能会让你觉得它是。
- en: 'If the keys are not enforced and this is not a relational database as we know
    it, what is the point? The answer is: the query optimizer may use this information
    to better optimize queries, namely with the concepts of Inner Join Elimination,
    Outer Join Elimination and Join Reordering.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些键没有强制执行，并且这不是我们熟悉的关系数据库，那么意义何在？答案是：查询优化器可以利用这些信息来更好地优化查询，特别是在内部连接消除、外部连接消除和连接重排序等概念上。
- en: 'Defining constraints is similar to other SQL dialects, just that you have to
    specify them as `NOT ENFORCED`:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 定义约束类似于其他 SQL 方言，只不过你必须将其指定为 `NOT ENFORCED`：
- en: '[PRE9]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ⚙️ Data operations
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ⚙️ 数据操作
- en: Copy data / tables
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 复制数据 / 表
- en: 'Copying data from one place to another is a typical part of our daily business
    as Data Engineers. Let’s assume the task is to copy data from a BigQuery dataset
    called `bronze` to another dataset called `silver` within a Google Cloud Platform
    project called `project_x`. The naive approach would be a simple SQL query like:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个地方复制数据到另一个地方是我们作为数据工程师日常工作的一部分。假设任务是将名为 `bronze` 的 BigQuery 数据集中的数据复制到另一个名为
    `silver` 的数据集，且该数据集位于名为 `project_x` 的 Google Cloud Platform 项目中。简单的方法是执行如下 SQL
    查询：
- en: '[PRE10]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Even though this allows for transformation, in many cases we simply want to
    copy data from one place to another. The bytes billed for the query above would
    essentially be the amount of data we have to read from the source. However, we
    can also get this **for free** with the following query:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这样可以进行转换，但在许多情况下，我们只是希望将数据从一个地方复制到另一个地方。上面查询的计费字节数基本上是我们需要从源头读取的数据量。然而，我们也可以通过以下查询**免费**获得这些数据：
- en: '[PRE11]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Alternatively, the `bq` CLI tool can be used to achieve the same result:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，可以使用 `bq` CLI 工具来实现相同的结果：
- en: '[PRE12]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: That way, you can copy data for **0 costs**.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，你可以以**0费用**复制数据。
- en: Load data
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载数据
- en: For data ingestion Google Cloud Storage is a pragmatic way to solve the task.
    No matter if it is a CSV file, ORC / Parquet files from a Hadoop ecosystem or
    any other source. Data can easily be uploaded and stored for low costs.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据摄取，Google Cloud Storage 是解决该任务的务实方式。无论是 CSV 文件、来自 Hadoop 生态系统的 ORC / Parquet
    文件，还是其他任何来源，都可以轻松上传并以低成本存储数据。
- en: It is also possible to create BigQuery tables on top of data stored in GCS.
    These **external** tables still utilize the compute infrastructure from BigQuery
    but do not offer some of the features and performance.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以在 GCS 上的数据基础上创建 BigQuery 表。这些**外部**表仍然利用 BigQuery 的计算基础设施，但不提供某些功能和性能。
- en: Let’s assume we upload data from a partitioned Hive table using the ORC storage
    format. Uploading the data can be achieved using `distcp` or simply by getting
    the data from HDFS first and then uploading it to GCS using one of the available
    CLI tools to interact with Cloud Storage.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们从使用 ORC 存储格式的分区 Hive 表上传数据。上传数据可以使用 `distcp` 完成，或者通过先从 HDFS 获取数据，然后使用与 Cloud
    Storage 交互的可用 CLI 工具将其上传到 GCS。
- en: 'Assuming we have a partition structure including one partition called month,
    the files might look like the following:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个包含名为 month 的分区结构，那么文件可能看起来如下：
- en: '[PRE13]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'When we uploaded this data to GCS, an external table definition can be created
    like this:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将这些数据上传到 GCS 时，可以像这样创建外部表定义：
- en: '[PRE14]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: It will derive the schema from the ORC files and even detect the partition column.
    The naive approach to move this data from GCS to BigQuery storage might now be,
    to create a table in BigQuery and then follow the pragmatic `INSERT INTO ... SELECT
    FROM` approach.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 它将从 ORC 文件中推导出模式，甚至检测分区列。将这些数据从 GCS 移动到 BigQuery 存储的简单方法现在可能是，在 BigQuery 中创建一个表，然后按照务实的
    `INSERT INTO ... SELECT FROM` 方法操作。
- en: However, similar to the previous example, the bytes billed would reflect the
    amount of data stored in `gs://project_x/ingest/some_orc_table`. There is another
    way, which will achieve the same result but again for **0 costs** using the `LOAD
    DATA` SQL statement.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，类似于前面的例子，计费的字节数将反映存储在 `gs://project_x/ingest/some_orc_table` 中的数据量。还有另一种方式，可以通过使用
    `LOAD DATA` SQL 语句以**0费用**实现相同的结果。
- en: '[PRE15]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Using this statement, we directly get a BigQuery table with the data ingested,
    **no need to create an external table first**! Also this query comes at **0 costs**.
    The `OVERWRITE` is optional, since data can also be appended instead of overwriting
    the table on every run.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此语句，我们直接获取包含数据的 BigQuery 表，**无需先创建外部表**！此外，此查询**没有费用**。`OVERWRITE` 是可选的，因为数据也可以追加，而不是每次运行时都覆盖表。
- en: 'As you can see, also the partition columns can be specified. Even though no
    transformation can be applied, there is one major advantage: we can already define
    cluster columns. That way, we can create an efficient version of the target table
    for further downstream processing, for free!'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，分区列也可以被指定。虽然不能应用任何转换，但有一个主要优点：我们可以提前定义集群列。这样，我们可以创建一个高效的目标表版本，用于后续的下游处理，且**免费**！
- en: Delete partitions
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 删除分区
- en: In certain ETL or ELT scenarios, a typical workflow is to have a table partitioned
    by day and then replace specific partitions based on new data coming from a staging
    / ingestion table.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些ETL或ELT场景中，典型的工作流是将表按天分区，然后根据来自临时/摄取表的新数据替换特定分区。
- en: '![](../Images/c0c962a5f17c681e88a437f318b19468.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c0c962a5f17c681e88a437f318b19468.png)'
- en: Ingest partition example (by author)
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 分区摄取示例（作者）
- en: BigQuery offers the `MERGE` statement but the naive approach is to first delete
    the affected partitions from the target table and then insert the data.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: BigQuery提供了`MERGE`语句，但简单的方法是先从目标表中删除受影响的分区，然后插入数据。
- en: 'Deleting partitions in such a scenario can be achieved like this:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，删除分区可以这样实现：
- en: '[PRE16]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Even if `day` is a partition column in both cases, this operation is connected
    to several costs. However, again there is an alternative solution that comes at
    **0 costs** again:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 即使`day`是两种情况下的分区列，该操作仍然涉及若干成本。然而，再一次，这里有一个**零成本**的替代方案：
- en: '[PRE17]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: With `DROP TABLE` you can actually also just drop one single partition by appending
    the suffix `$<partition_id>`.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`DROP TABLE`，你实际上也可以通过附加后缀`$<partition_id>`来删除单个分区。
- en: Of course the above example is just dropping one partition. However, with the
    procedual language from BigQuery, we can easily execute the statement in a loop.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，上面的示例只是删除一个分区。然而，使用BigQuery的过程语言，我们可以轻松地在循环中执行该语句。
- en: '[PRE18]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Or alternatively use Airflow and/or dbt to first select the partitions and then
    run a certain templated query in a loop.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，可以使用Airflow和/或dbt，先选择分区，然后在循环中运行某个模板化查询。
- en: However, getting the distinct partitions for a partitioned table can be done
    like the in the examples above, but this will still cause some costs even if we
    only read a single column. But yet again, there is a way to get this almost for
    free, which we will explore in the next chapter.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，获取分区表的不同分区可以像上述示例那样进行，但即使我们只读取单一列，这仍然会产生一些成本。但再次强调，有一种几乎**免费**的方法来实现这一点，我们将在下一章中探讨。
- en: Get distinct partitions for a table
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取表的不同分区
- en: 'In the examples above, we used the following approach to get the distinct partitions
    of a partitioned BigQuery table:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的示例中，我们使用了以下方法来获取分区的不同分区：
- en: '[PRE19]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This is how much the query cost me in an example use-case I worked on:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我在一个示例用例中查询的成本：
- en: '[PRE20]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'BigQuery maintains a lot of valuable metadata about tables, columns and partitions.
    This can be accessed via the `INFORMATION_SCHEMA`. We can achieve the very same
    result, by simply using this metadata:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: BigQuery维护了大量关于表、列和分区的有价值元数据。这些信息可以通过`INFORMATION_SCHEMA`访问。我们可以通过简单地使用这些元数据来实现完全相同的结果：
- en: '[PRE21]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'And comparing it with the same use-case as I mentioned above, this is how much
    the query cost:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 与我之前提到的相同用例进行比较，这就是查询的成本：
- en: '[PRE22]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As you can see, 149GB vs 10MB is a huge difference. With this method, you can
    get the distinct partitions even for huge tables at **almost 0 costs**.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，149GB与10MB之间的差距非常大。通过这种方法，即使是巨大的表，你也可以以**几乎零成本**获取不同的分区。
- en: Do not persist calculated measures
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不要持久化计算的度量
- en: When you start using BigQuery for the first projects, you will most likely stick
    with the on-demand compute pricing model. With on-demand pricing, you will generally
    have access to up to 2000 concurrent slots, shared among all queries in a single
    project. But even with capacity pricing, you will have a minimum of 100 slots.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开始使用BigQuery进行第一个项目时，你很可能会选择按需计算定价模式。使用按需定价，你通常可以访问最多2000个并发槽位，所有查询共享这些槽位。但即使使用容量定价，你也会至少拥有100个槽位。
- en: With a lot of the daily ETL / ELT workload, these slots are actually not the
    limitation of the performance. You can simply check this yourself by navigating
    to BigQuery -> Administration -> Monitoring, select the correct location and change
    the Chart to *Slot Usage* under *Chart Configuration*. In a lot of cases you will
    be surprised how little slots you are actually using.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大部分日常的 ETL/ELT 工作负载，这些插槽实际上并不是性能的瓶颈。你可以通过进入 BigQuery -> 管理 -> 监控，选择正确的位置，并在“图表配置”下将图表改为
    *插槽使用情况* 来自行检查。在很多情况下，你会惊讶于实际使用的插槽是多么少。
- en: '![](../Images/3b574cfdb6e528945373257885f71b59.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3b574cfdb6e528945373257885f71b59.png)'
- en: BigQuery Monitoring for slots
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: BigQuery 插槽监控
- en: How does that relate to saving costs? Let’s assume you have a classic fact table
    or some table in general, which delivers certain KPIs. This table is then used
    for analysis / reporting in Looker, Excel, PowerBI or other tools.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这与节省成本有什么关系呢？假设你有一个经典的事实表，或者一般的某个表，它提供了某些关键绩效指标（KPI）。这个表随后被用于 Looker、Excel、PowerBI
    或其他工具中的分析/报告。
- en: Often these tools automatically generate queries to serve the report or dashboard
    with the necessary data. These generated queries might not be ideal, when it comes
    to applying BigQuery best practices. In other words, they might end up scanning
    more data than necessary which increases the bytes billed.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这些工具会自动生成查询来为报告或仪表盘提供所需的数据。这些自动生成的查询可能在应用 BigQuery 最佳实践时并不理想。换句话说，它们可能会扫描比必要更多的数据，从而增加计费的字节数。
- en: We can avoid this, by introducing a view layer on top of our fact tables. Serving
    tools with data from a view rather than the actual table is a very valuable best
    practice, as it gives you more flexibility when it comes to schema changes but
    it also gives the possibility to introduce calculated measures within the view
    without persisting the data.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在事实表上方引入视图层来避免这一点。通过视图而非实际表提供数据给服务工具是一种非常有价值的最佳实践，因为这为你在模式变更时提供了更多灵活性，同时也能在视图中引入计算度量而不需要持久化数据。
- en: Of course this might increase the CPU usage when these measures are used but
    on the other hand, it can drastically reduce the total size of the underlying
    table.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，当使用这些度量时，这可能会增加 CPU 使用率，但另一方面，它也能显著减少底层表的总大小。
- en: 'To illustrate this principle, take the following fact table as a basis:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一原则，以以下事实表为基础：
- en: '[PRE23]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The basic idea is to introduce a view for stakeholders accessing this data
    and extend it with calculated measures:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思路是为访问这些数据的利益相关者引入视图，并通过计算度量对其进行扩展：
- en: '[PRE24]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In this example we were able to avoid persisting two `INT64` values. One of
    these uses 8 logical bytes. If our fact table has 1,000,000,000 rows this would
    mean we save:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们能够避免持久化两个 `INT64` 值。每一个值都使用 8 个逻辑字节。如果我们的事实表有 1,000,000,000 行，那么这意味着我们节省了：
- en: '[PRE25]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This is not a huge amount of data, but it can mean that BigQuery has to scan
    15 GB less data in certain situations. In practice, there can be calculated measures
    that might save you much more data to be scanned.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是大量数据，但它可能意味着在某些情况下 BigQuery 必须扫描的 15 GB 数据减少了。实际上，可能存在计算度量，它们能够节省更多的数据扫描量。
- en: 📚 Summary
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 📚 总结
- en: Forget hoarding every byte like a dragon guarding its treasure. Instead, learn
    to burn data through smart management and optimization 🔥. By embracing this fiery
    approach, you’ll transform BigQuery from a cost center to a powerful engine for
    data exploration, allowing you to burn data, not money!
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 不要像守护宝藏的巨龙一样囤积每一个字节。相反，学会通过智能管理和优化来“燃烧”数据 🔥。通过采用这种激烈的方法，你将把 BigQuery 从一个成本中心转变为强大的数据探索引擎，让你燃烧的是数据，而不是金钱！
- en: Embrace data modeling best practices
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 采用数据建模最佳实践
- en: Utilize the smallest data types possible to minimize storage and processing
    costs.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用尽可能小的数据类型以最小化存储和处理成本。
- en: Leverage de-normalization when appropriate to optimize query performance and
    reduce storage usage.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在适当的时候利用去规范化，以优化查询性能并减少存储使用。
- en: Implement partitioning and clustering to enable BigQuery to efficiently scan
    only the relevant data for your queries.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施分区和聚类，使 BigQuery 能够高效地仅扫描查询所需的相关数据。
- en: Explore nested repeated columns as a way to eliminate redundancy while maintaining
    data integrity, but be mindful of limitations regarding clustering.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索嵌套重复列作为消除冗余并保持数据完整性的一种方式，但要注意聚类的限制。
- en: Master data operations for cost-effectiveness
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 掌握数据操作以提高成本效益
- en: Employ `CREATE TABLE ... COPY` or `bq cp` commands to copy data between tables
    without incurring charges.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `CREATE TABLE ... COPY` 或 `bq cp` 命令在表之间复制数据，无需产生费用。
- en: Utilize `LOAD DATA` statements to directly load data from Cloud Storage into
    BigQuery tables, again at no cost.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `LOAD DATA` 语句直接从 Cloud Storage 加载数据到 BigQuery 表中，且无需付费。
- en: Leverage the power of `DROP TABLE` with partition suffixes to efficiently remove
    specific partitions.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用 `DROP TABLE` 和分区后缀的功能高效地删除特定分区。
- en: Utilize `INFORMATION_SCHEMA` to retrieve table metadata like distinct partition
    values, significantly reducing costs compared to traditional queries.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用 `INFORMATION_SCHEMA` 获取表的元数据，例如不同的分区值，相比传统查询大大降低成本。
- en: Design for efficiency and avoid unnecessary data persistence
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计时要注重效率，避免不必要的数据持久化。
- en: Implement a view layer to serve data with calculated measures, preventing the
    storage of redundant data.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现视图层来提供计算度量的数据，避免存储冗余数据。
- en: Monitor your BigQuery slot usage to understand if slot limitations are a concern,
    allowing you to focus on optimizing query structures.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控你的 BigQuery 插槽使用情况，以了解插槽限制是否是一个问题，从而让你可以集中精力优化查询结构。
- en: '***By adopting these strategies, you can unlock the true potential of BigQuery,
    transforming it into a cost-effective engine for data exploration and analysis.
    Remember, in the realm of BigQuery, it’s all about burning data, not money!***'
  id: totrans-234
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***通过采用这些策略，你可以释放 BigQuery 的真正潜力，将其转变为一个高效且具成本效益的数据探索和分析引擎。记住，在 BigQuery 的世界里，关键是消耗数据，而不是金钱！***'
- en: '**Feel free to share your experiences in the comments!**'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**欢迎在评论区分享你的经验！**'
