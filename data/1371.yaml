- en: Orchestrating a Dynamic Time-Series Pipeline in Azure
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Azure中编排动态时间序列管道
- en: 原文：[https://towardsdatascience.com/orchestrating-a-dynamic-time-series-pipeline-with-azure-data-factory-and-databricks-810819608231?source=collection_archive---------9-----------------------#2024-05-31](https://towardsdatascience.com/orchestrating-a-dynamic-time-series-pipeline-with-azure-data-factory-and-databricks-810819608231?source=collection_archive---------9-----------------------#2024-05-31)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/orchestrating-a-dynamic-time-series-pipeline-with-azure-data-factory-and-databricks-810819608231?source=collection_archive---------9-----------------------#2024-05-31](https://towardsdatascience.com/orchestrating-a-dynamic-time-series-pipeline-with-azure-data-factory-and-databricks-810819608231?source=collection_archive---------9-----------------------#2024-05-31)
- en: Explore how to build, trigger, and parameterize a time-series data pipeline
    with Azure Data Factory (ADF) and Databricks, accompanied by a step-by-step tutorial
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索如何使用Azure Data Factory（ADF）和Databricks构建、触发和参数化一个时间序列数据管道，并附有逐步教程。
- en: '[](https://medium.com/@johnleungTJ?source=post_page---byline--810819608231--------------------------------)[![John
    Leung](../Images/ef45063e759e3450fa7f3c32b2f292c3.png)](https://medium.com/@johnleungTJ?source=post_page---byline--810819608231--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--810819608231--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--810819608231--------------------------------)
    [John Leung](https://medium.com/@johnleungTJ?source=post_page---byline--810819608231--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@johnleungTJ?source=post_page---byline--810819608231--------------------------------)[![John
    Leung](../Images/ef45063e759e3450fa7f3c32b2f292c3.png)](https://medium.com/@johnleungTJ?source=post_page---byline--810819608231--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--810819608231--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--810819608231--------------------------------)
    [John Leung](https://medium.com/@johnleungTJ?source=post_page---byline--810819608231--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--810819608231--------------------------------)
    ·8 min read·May 31, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--810819608231--------------------------------)
    ·阅读时间8分钟·2024年5月31日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: In the [previous story](https://medium.com/towards-data-science/feature-engineering-for-time-series-using-pyspark-on-databricks-02b97d62a287),
    we went through the potential of PySpark on Databricks for time-series data. I
    encourage you to catch up [here](https://medium.com/towards-data-science/feature-engineering-for-time-series-using-pyspark-on-databricks-02b97d62a287)
    to know more. Without configuring a standalone Spark instance, we can ingest static
    and streaming data, perform transformation, extract useful time-related features,
    and build visualization using PySpark on Databricks. Its scalability and performance
    are particularly advantageous when handling complex transformations of enterprise-level
    data, up to petabytes.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在[上一篇故事](https://medium.com/towards-data-science/feature-engineering-for-time-series-using-pyspark-on-databricks-02b97d62a287)中，我们回顾了PySpark在Databricks上处理时间序列数据的潜力。我鼓励你通过[这里](https://medium.com/towards-data-science/feature-engineering-for-time-series-using-pyspark-on-databricks-02b97d62a287)了解更多内容。在不配置独立Spark实例的情况下，我们可以通过Databricks上的PySpark摄取静态和流数据，执行数据转换，提取有用的时间相关特征，并构建可视化。当处理企业级数据的大规模复杂转换时，PySpark的可扩展性和性能特别具有优势，甚至可以处理PB级别的数据。
- en: All the feature engineering tasks were successfully performed within a single
    Databricks notebook. However, this is only a part of the data engineering story
    when building a data-centric system. The core part of the data pipeline lies in
    data orchestration.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 所有特征工程任务都成功地在一个Databricks笔记本中完成。然而，这只是构建数据中心系统时数据工程故事的一部分。数据管道的核心部分在于数据编排。
- en: Data orchestration generally refers to have the centralized control over data
    flows so that we can automate, manage, and monitor the entire data pipeline.
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数据编排通常指的是对数据流进行集中控制，以便我们可以自动化、管理和监控整个数据管道。
- en: '![](../Images/2a39ebad8db26cfd20e2a647df768903.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2a39ebad8db26cfd20e2a647df768903.png)'
- en: Photo by [Julio Rionaldo](https://unsplash.com/@juliorionaldo?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Julio Rionaldo](https://unsplash.com/@juliorionaldo?utm_source=medium&utm_medium=referral)提供，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Azure Data Factory (ADF) with Azure Databricks
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Azure Data Factory (ADF)与Azure Databricks
- en: To satisfy these needs, one of the most popular solutions in the industry is
    to run [Azure Databricks](https://azure.microsoft.com/en-gb/products/databricks/#content-card-list-oc803c)
    notebooks from an [ADF](https://azure.microsoft.com/en-us/products/data-factory#features)
    platform.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足这些需求，行业中最流行的解决方案之一是从[ADF](https://azure.microsoft.com/en-us/products/data-factory#features)平台运行[Azure
    Databricks](https://azure.microsoft.com/en-gb/products/databricks/#content-card-list-oc803c)笔记本。
- en: ADF is a cloud-based, serverless, and fully managed data integration service.
    Though [Databricks Workflow](https://docs.databricks.com/en/workflows/index.html)
    gives a good alternative that covers some ADF features, there are still several
    key benefits to choosing ADF. For example, ADF is a mature tool for integrating
    with diverse data stores using connectors, including SaaS applications like Salesforce,
    and Big Data sources like Amazon Redshift, and Google BigQuery. Therefore, it
    works well for ingestion and integration, especially if the current system has
    complex dependencies with data systems outside of Databricks. Besides, ADF simplifies
    and facilitates the quick building of basic pipelines using a drag-and-drop and
    low-code interface.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ADF 是一个基于云的、无服务器且完全托管的数据集成服务。尽管[Databricks Workflow](https://docs.databricks.com/en/workflows/index.html)提供了一个很好的替代方案，涵盖了部分
    ADF 的功能，但选择 ADF 仍然有若干关键优势。例如，ADF 是一个成熟的工具，能够使用连接器与各种数据存储进行集成，包括像 Salesforce 这样的
    SaaS 应用和像 Amazon Redshift、Google BigQuery 这样的大数据源。因此，它在数据摄取和集成方面表现良好，尤其是当当前系统与
    Databricks 以外的数据系统存在复杂依赖关系时。此外，ADF 简化并便于使用拖放和低代码界面快速构建基本管道。
- en: '**In this hands-on journey, we will dive deeper into the data engineering project
    and explore how ADF helps build a dynamic, skeletal data pipeline for time-series
    data.** I will demonstrate how to mount cloud storage on Azure Databricks, transform
    data by embedding Notebook on Azure Databricks, and dynamically orchestrate data
    through custom settings in ADF. Let’s get started!'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**在这个实践过程中，我们将深入探讨数据工程项目，并探索 ADF 如何帮助构建一个动态的、骨架型的数据管道，用于时间序列数据。** 我将展示如何在 Azure
    Databricks 上挂载云存储，通过嵌入的 Notebook 转换数据，并通过 ADF 中的自定义设置动态编排数据。让我们开始吧！'
- en: The initial setup
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始设置
- en: There are several cloud components and services in the first place.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 首先有几个云组件和服务。
- en: '**#1 Create an Azure resource group**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**#1 创建一个 Azure 资源组**'
- en: This container is used to hold and group the resources for an Azure solution.
    We will place our necessary cloud service components in this logical group for
    easier building or deployment.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个容器用于保存和分组 Azure 解决方案的资源。我们将把必要的云服务组件放入这个逻辑组中，以便更容易进行构建或部署。
- en: '![](../Images/0b3a948d5cc07b62727ff956da16e18d.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0b3a948d5cc07b62727ff956da16e18d.png)'
- en: Azure resource group (Image by author)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 资源组（作者提供的图片）
- en: '**#2 Create an Azure Data Lake Gen 2 storage account**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**#2 创建一个 Azure Data Lake Gen 2 存储账户**'
- en: You can choose a suitable storage account based on the requirements of performance
    and replication. In the advanced tab, we enable the Hierarchical Namespace to
    set up the [Data Lake Storage Gen 2](https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction).
    This allows for storing both structured and unstructured data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以根据性能和复制需求选择合适的存储账户。在高级选项卡中，我们启用了分层命名空间以设置[Data Lake Storage Gen 2](https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction)。这使得既可以存储结构化数据，也可以存储非结构化数据。
- en: '![](../Images/1a6a8f5c3ba036f8056555f2f1328079.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a6a8f5c3ba036f8056555f2f1328079.png)'
- en: Storage account (Image by author)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 存储账户（作者提供的图片）
- en: '**#3 Set up Azure Databricks service**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**#3 设置 Azure Databricks 服务**'
- en: 'If you have used Databricks before, Azure Databricks service is largely the
    same. Besides, it is natively integrated with other Azure services and provides
    a unified billing platform. There are two [tiers](https://azure.microsoft.com/en-us/pricing/details/databricks/):
    (1) Standard — sufficient for our proof-of-concept here; and (2) Premium — the
    features of the Standard tier, with additionally the [Unity Catalog](https://learn.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/)
    and the advanced networking features that may be necessary for a large enterprise
    with multiple Databricks workspaces.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前使用过 Databricks，Azure Databricks 服务大体相同。此外，它与其他 Azure 服务原生集成，并提供统一的计费平台。这里有两个[层级](https://azure.microsoft.com/en-us/pricing/details/databricks/)：(1)
    标准层——足以满足我们在此的概念验证需求；(2) 高级层——具有标准层的功能，额外提供[Unity Catalog](https://learn.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/)和可能对于拥有多个
    Databricks 工作区的大型企业所需的高级网络功能。
- en: '![](../Images/508e7c988c5787278492f29a7cb7c45b.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/508e7c988c5787278492f29a7cb7c45b.png)'
- en: Azure Databricks workspace (Image by author)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Azure Databricks 工作区（图片来源：作者）
- en: '**#4 Register an application**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**#4 注册应用程序**'
- en: This service will help mount Azure storage to Databricks, so make sure you note
    the application ID and tenant ID, and most importantly the app secret value, which
    cannot be viewed when you revisit it.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 该服务将帮助将 Azure 存储挂载到 Databricks，因此请确保记下应用 ID 和租户 ID，最重要的是应用的密钥值，在你重新访问时是无法查看的。
- en: '![](../Images/71347d79943224942a4c8b3849793827.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/71347d79943224942a4c8b3849793827.png)'
- en: App registration — Setting (Image by author)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 应用注册 — 设置（图片来源：作者）
- en: '![](../Images/899152e47dc2eb94104d5fbe94d58e6e.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/899152e47dc2eb94104d5fbe94d58e6e.png)'
- en: App registration — Info (Image by author)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 应用注册 — 信息（图片来源：作者）
- en: '![](../Images/39029473003e288a43c854dd3933028b.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/39029473003e288a43c854dd3933028b.png)'
- en: App registration — Client secret (Image by author)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 应用注册 — 客户端密钥（图片来源：作者）
- en: Afterward, grant the app service access rights to the app service. This is achieved
    by assigning the “Storage Blob Data Contributor” role to the app we just registered.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，授予应用服务对应用服务的访问权限。这是通过将“Storage Blob Data Contributor”角色分配给我们刚注册的应用来实现的。
- en: '![](../Images/4307ac970035d3de1b0ff44f080fcd4d.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4307ac970035d3de1b0ff44f080fcd4d.png)'
- en: Storage account — Grant access right (1/3) (Image by author)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 存储账户 — 授予访问权限（1/3）（图片来源：作者）
- en: '![](../Images/9e803eab9aa056940edad8db00b91982.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e803eab9aa056940edad8db00b91982.png)'
- en: Storage account — Grant access right (2/3) (Image by author)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 存储账户 — 授予访问权限（2/3）（图片来源：作者）
- en: '![](../Images/8a7941f7932f3f085535d7aa5b247c1c.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a7941f7932f3f085535d7aa5b247c1c.png)'
- en: Storage account — Grant access right (3/3) (Image by author)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 存储账户 — 授予访问权限（3/3）（图片来源：作者）
- en: '**#5 Create Azure SQL Database**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**#5 创建 Azure SQL 数据库**'
- en: To store a transformed data frame, we search for Azure SQL resources and pick
    a “Single database” as the resource type. There are choices of SQL database servers
    with different computing hardware, maximum data size, and more. You can instantly
    get the estimated cost summary while adjusting the server specifications.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了存储转换后的数据框，我们搜索 Azure SQL 资源并选择“单一数据库”作为资源类型。SQL 数据库服务器提供了不同的计算硬件、最大数据大小等选项。你可以在调整服务器配置时即时查看估算的费用摘要。
- en: '![](../Images/d1f6640b38d66f1ecf3bc1e31089c0c4.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1f6640b38d66f1ecf3bc1e31089c0c4.png)'
- en: Create SQL Database (1/2) (Image by author)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 SQL 数据库（1/2）（图片来源：作者）
- en: '![](../Images/bf6eaa445f2dbef9ff9a3aef44dd5ef3.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bf6eaa445f2dbef9ff9a3aef44dd5ef3.png)'
- en: Create SQL Database (2/2) (Image by author)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 SQL 数据库（2/2）（图片来源：作者）
- en: After all the initial setups, you are ready to explore how these services are
    linked together.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 完成所有初始设置后，你就可以探索这些服务是如何相互连接的。
- en: Prepare for data orchestrating pipeline
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备数据编排管道
- en: '**#1 Ingest data**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**#1 导入数据**'
- en: 'We first upload the electric power consumption data to Azure Data Lake Gen2\.
    This [dataset](https://www.kaggle.com/datasets/uciml/electric-power-consumption-data-set/data)
    [with license as [Database: Open Database, Contents: Database Contents](https://opendatacommons.org/licenses/dbcl/1-0/)],
    obtained from Kaggle, is sampled at a one-minute rate from December 2006 to November
    2010.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将电力消耗数据上传到 Azure Data Lake Gen2。这个[数据集](https://www.kaggle.com/datasets/uciml/electric-power-consumption-data-set/data)[许可证为[数据库：开放数据库，内容：数据库内容](https://opendatacommons.org/licenses/dbcl/1-0/)]，来自
    Kaggle，采样频率为每分钟一次，数据时间从 2006 年 12 月到 2010 年 11 月。
- en: '![](../Images/da8e38c40f958471e0fecc8f8f2c5ea3.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da8e38c40f958471e0fecc8f8f2c5ea3.png)'
- en: Upload input data (Image by author)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 上传输入数据（图片来源：作者）
- en: Next, we create a Notebook on the Azure Databricks workspace and mount storage
    by defining the parameters using previously stored ID values.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在 Azure Databricks 工作区创建一个 Notebook，并通过定义参数，使用之前存储的 ID 值来挂载存储。
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To verify file access, we can run the following command:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证文件访问，我们可以运行以下命令：
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**#2 Embed Notebook on Azure Databricks**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**#2 在 Azure Databricks 中嵌入 Notebook**'
- en: Most of the source codes in this section build upon my [previous story](/feature-engineering-for-time-series-using-pyspark-on-databricks-02b97d62a287).
    The idea is to perform data cleansing, transformation, and feature engineering
    (create time-related and moving averaging features). The transformed data is ultimately
    written to the Azure database table.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的大部分源代码基于我的[上一篇文章](/feature-engineering-for-time-series-using-pyspark-on-databricks-02b97d62a287)。其思路是进行数据清理、转换和特征工程（创建时间相关特征和移动平均特征）。转换后的数据最终写入
    Azure 数据库表中。
- en: You can check the complete code below to see the implementation.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以查看下面的完整代码，了解其实现过程。
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**#3 Build a basic pipeline in ADF**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**#3 在 ADF 中构建基本管道**'
- en: In ADF, we add a “Notebook” activity to the pipeline environment, then configure
    it to reference the desired Notebook in the Databricks folder. Set up the Databricks
    linked service, then validate and publish the entire activity pipeline in ADF.
    You can then run the pipeline in “Debug” mode.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在ADF中，我们将“Notebook”活动添加到管道环境中，然后配置它以引用Databricks文件夹中的所需Notebook。设置Databricks连接服务，然后在ADF中验证并发布整个活动管道。然后，您可以在“调试”模式下运行管道。
- en: '![](../Images/77f80f4bd8dac29b014997ad560cb4d7.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77f80f4bd8dac29b014997ad560cb4d7.png)'
- en: The success status of pipeline run (Image by author)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 管道运行的成功状态（图片由作者提供）
- en: The activity status shows “Succeeded”, meaning the data should be migrated and
    inserted into the Azure SQL Database table. We can view the results for verification
    using the query editor.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 活动状态显示为“已成功”，这意味着数据应该已迁移并插入到Azure SQL数据库表中。我们可以使用查询编辑器查看结果以进行验证。
- en: '![](../Images/f9a3cbd5c05477cfd6c63700ff13670f.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f9a3cbd5c05477cfd6c63700ff13670f.png)'
- en: Query result of the Azure SQL database (Image by author)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 查询Azure SQL数据库的结果（图片由作者提供）
- en: '**#4 Automate the pipeline**'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**#4 自动化管道**'
- en: 'ADF offers functionalities that are far beyond the above simple implementation.
    For example, we can automate the pipeline by creating a [storage-based event trigger](https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger?tabs=data-factory).
    Make sure that `Microsoft.EventGrid` is registered as one of the resource providers
    in your account subscription, then set up the trigger: Whenever a new dataset
    is uploaded to the storage account, the pipeline will automatically execute.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ADF提供的功能远超上述简单实现。例如，我们可以通过创建[基于存储的事件触发器](https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger?tabs=data-factory)来自动化管道。确保`Microsoft.EventGrid`已注册为您账户订阅中的资源提供者之一，然后设置触发器：每当新数据集上传到存储帐户时，管道将自动执行。
- en: '![](../Images/cd7dbcf9ffdcf90150c136a408b6bf58.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cd7dbcf9ffdcf90150c136a408b6bf58.png)'
- en: Set a new trigger in ADF (Image by author)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在ADF中设置新的触发器（图片由作者提供）
- en: This type of trigger has various use cases in the industries, such as monitoring
    the inventory level to replenish orders for the supply chain or tracking customer
    interactions for personalized recommendations in digital marketing.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的触发器在行业中有各种应用场景，例如监控库存水平以补充供应链订单，或追踪客户互动以实现数字营销中的个性化推荐。
- en: '**#5 Parameterize the Notebook variables**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**#5 参数化Notebook变量**'
- en: To take a further step to build a more dynamic data pipeline, we can make variables
    more parametric. For example, during feature engineering on time-series data,
    the window size of data features may not be optimized initially. The window sizes
    may need to be adjusted to capture seasonal patterns or based on downstream model
    fine-tuning. For this scenario, we can amend with the below settings.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步构建更具动态性的数据信息管道，我们可以使变量更加参数化。例如，在时间序列数据的特征工程中，数据特征的窗口大小最初可能并未优化。窗口大小可能需要根据季节性模式或下游模型微调进行调整。对于这种情况，我们可以通过以下设置进行修改。
- en: '![](../Images/3c3140b421b91d991eb48967cfdc610b.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c3140b421b91d991eb48967cfdc610b.png)'
- en: Set up parameters for pipeline run (Image by author)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 设置管道运行的参数（图片由作者提供）
- en: 'In the Notebook, add the code below to create a widget that can get the parameters
    input from the ADF pipeline:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在Notebook中，添加以下代码以创建一个小部件，可以从ADF管道获取参数输入：
- en: '[PRE3]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: After adjusting the settings and the Notebook codes, we can run the pipeline
    by providing the window size parameter values, such as 30 and 60.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在调整设置和Notebook代码后，我们可以通过提供窗口大小参数值，如30和60，来运行管道。
- en: '![](../Images/77ce6fbed57e233a919818d647fdfc8b.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77ce6fbed57e233a919818d647fdfc8b.png)'
- en: Input window size value for pipeline run (Image by author)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为管道运行输入窗口大小值（图片由作者提供）
- en: Finally, we can monitor the pipeline status again using ADF or Databricks workspace.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过ADF或Databricks工作区再次监控管道状态。
- en: Wrapping it up
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: 'In our hands-on exploration, we mainly used ADF with Azure Databricks to orchestrate
    a dynamic time-series data pipeline:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实践探索中，我们主要使用ADF与Azure Databricks来编排一个动态的时间序列数据管道：
- en: Setup the cloud resources for the compute, analytics, and storage
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置云资源用于计算、分析和存储。
- en: Build the skeleton of the data pipeline from data ingestion to storage
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数据摄取到存储，构建数据管道的骨架。
- en: Bring flexibilities to the pipeline by creating triggers and parameterizing
    variables
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过创建触发器和参数化变量，为管道带来灵活性。
- en: At the enterprise level, more [complex cloud architectures](https://medium.com/@johnleungTJ/how-to-design-the-ai-architectures-in-azure-for-the-new-era-9531229cfd33)
    may be implemented to satisfy evolving needs, such as streaming data, model monitoring,
    and multi-model pipelines. It thus becomes essential to strive for a delicate
    balance between performance, reliability, and cost-efficiency through team collaboration
    on governance policies and spending management.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在企业层面，可能会实施更多的[复杂云架构](https://medium.com/@johnleungTJ/how-to-design-the-ai-architectures-in-azure-for-the-new-era-9531229cfd33)，以满足不断变化的需求，如数据流、模型监控和多模型管道。因此，团队在治理政策和支出管理上的协作变得至关重要，以实现性能、可靠性和成本效益之间的精细平衡。
- en: Before you go
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在您离开之前
- en: If you enjoy this reading, I invite you tofollow my [Medium page](https://medium.com/@johnleungTJ)
    and [LinkedIn page](https://www.linkedin.com/in/john-leung-639800115/). By doing
    so, you can stay updated with exciting content related to data science side projects
    and Machine Learning Operations (MLOps) demonstration methodologies.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您喜欢这篇文章，我邀请您关注我的[Medium页面](https://medium.com/@johnleungTJ)和[LinkedIn页面](https://www.linkedin.com/in/john-leung-639800115/)。通过这样做，您可以随时了解与数据科学侧项目和机器学习运维（MLOps）演示方法相关的精彩内容。
- en: '[](/performing-customer-analytics-with-langchain-and-llms-0af4ea38f7b5?source=post_page-----810819608231--------------------------------)
    [## Performing Customer Analytics with LangChain and LLMs'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/performing-customer-analytics-with-langchain-and-llms-0af4ea38f7b5?source=post_page-----810819608231--------------------------------)
    [## 使用LangChain和LLMs进行客户分析'
- en: Discover the potentials and constraints of LangChain for customer analytics,
    accompanied by practical implementation…
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 发现LangChain在客户分析中的潜力与局限性，并附带实际的实施案例…
- en: towardsdatascience.com](/performing-customer-analytics-with-langchain-and-llms-0af4ea38f7b5?source=post_page-----810819608231--------------------------------)
    [](/managing-the-technical-debts-of-machine-learning-systems-5b85d420ab9d?source=post_page-----810819608231--------------------------------)
    [## Managing the Technical Debts of Machine Learning Systems
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/performing-customer-analytics-with-langchain-and-llms-0af4ea38f7b5?source=post_page-----810819608231--------------------------------)
    [](/managing-the-technical-debts-of-machine-learning-systems-5b85d420ab9d?source=post_page-----810819608231--------------------------------)
    [## 管理机器学习系统的技术债务
- en: Explore the practices with implementation codes for sustainably mitigating the
    cost of speedy delivery
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索通过实施代码持续降低快速交付成本的实践
- en: towardsdatascience.com](/managing-the-technical-debts-of-machine-learning-systems-5b85d420ab9d?source=post_page-----810819608231--------------------------------)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/managing-the-technical-debts-of-machine-learning-systems-5b85d420ab9d?source=post_page-----810819608231--------------------------------)
