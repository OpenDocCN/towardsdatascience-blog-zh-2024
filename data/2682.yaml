- en: 'When Machines Think Ahead: The Rise of Strategic AI'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 当机器开始思考未来：战略性人工智能的崛起
- en: 原文：[https://towardsdatascience.com/when-machines-think-ahead-the-rise-of-strategic-ai-91052e4c5da9?source=collection_archive---------2-----------------------#2024-11-04](https://towardsdatascience.com/when-machines-think-ahead-the-rise-of-strategic-ai-91052e4c5da9?source=collection_archive---------2-----------------------#2024-11-04)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/when-machines-think-ahead-the-rise-of-strategic-ai-91052e4c5da9?source=collection_archive---------2-----------------------#2024-11-04](https://towardsdatascience.com/when-machines-think-ahead-the-rise-of-strategic-ai-91052e4c5da9?source=collection_archive---------2-----------------------#2024-11-04)
- en: '[STRATEGIC AI](https://medium.com/@hc.ekne/list/strategic-ai-72a460668137)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[战略性人工智能](https://medium.com/@hc.ekne/list/strategic-ai-72a460668137)'
- en: Exploring the advancements in strategic AI and how large language models fit
    into the bigger picture
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探讨战略性人工智能的进展，以及大型语言模型在大局中的角色
- en: '[](https://medium.com/@hc.ekne?source=post_page---byline--91052e4c5da9--------------------------------)[![Hans
    Christian Ekne](../Images/c85483d8b5dd89584b996b321b7f4a45.png)](https://medium.com/@hc.ekne?source=post_page---byline--91052e4c5da9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--91052e4c5da9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--91052e4c5da9--------------------------------)
    [Hans Christian Ekne](https://medium.com/@hc.ekne?source=post_page---byline--91052e4c5da9--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@hc.ekne?source=post_page---byline--91052e4c5da9--------------------------------)[![Hans
    Christian Ekne](../Images/c85483d8b5dd89584b996b321b7f4a45.png)](https://medium.com/@hc.ekne?source=post_page---byline--91052e4c5da9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--91052e4c5da9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--91052e4c5da9--------------------------------)
    [Hans Christian Ekne](https://medium.com/@hc.ekne?source=post_page---byline--91052e4c5da9--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--91052e4c5da9--------------------------------)
    ·27 min read·Nov 4, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--91052e4c5da9--------------------------------)
    ·阅读时间27分钟·2024年11月4日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d5bf5664439826f7f4d5850d8ea8354a.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d5bf5664439826f7f4d5850d8ea8354a.png)'
- en: Image generated by the author using Canva Magic Studio
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 该图由作者使用Canva Magic Studio生成
- en: Prologue
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 序言
- en: 11\. May 1997, New York City.
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1997年5月11日，纽约市。
- en: It was a beautiful spring day in New York City. The skies were clear, and temperatures
    were climbing toward 20 degrees Celsius. The Yankees prepared to play the Kansas
    City Royals at Yankee Stadium, and the Rangers were facing off against the Devils
    at Madison Square Garden.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 那是一个美丽的春日，纽约市的天气格外宜人。天空湛蓝，气温逐渐上升，接近20摄氏度。洋基队准备在洋基体育场与堪萨斯城皇家队进行比赛，而纽约游骑兵队则在麦迪逊广场花园与魔鬼队展开对决。
- en: Nothing seemed out of the ordinary, yet the people gathering at the Equitable
    Center in Midtown Manhattan were about to experience something truly unique. They
    were about to witness the historic event when a computer, for the first time,
    would beat a reigning world champion in chess under standard tournament conditions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一切看似平常，但聚集在曼哈顿中城Equitable大厦的人们即将经历一场真正独特的事件。他们将亲眼见证历史性时刻——一台计算机将在标准比赛条件下首次战胜现任世界冠军。
- en: Representing humans was Gary Kasparov, widely recognized as the world’s top
    chess player at the time. And representing the machines, Deep Blue — a chess computer
    developed by IBM. Going into the final and 6th game of the match, both players
    had 2.5 points. It was today that the winner was to be decided.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 代表人类的是Gary Kasparov，当时被广泛认为是世界顶级棋手。而代表机器的是Deep Blue——一款由IBM开发的国际象棋计算机。在这场比赛的第六局决胜局之前，两位选手各自获得了2.5分。今天，胜负将最终揭晓。
- en: Gary started out as black, but made an early error and faced a strong, aggressive
    attack from Deep Blue. After just 19 moves it was all over. Kasparov, feeling
    demoralized and under pressure, resigned, believing his position was untenable.
    A symbolic, and by many hailed as one of the most important moments between man
    and machine was a fact. This landmark event marked a turning point in AI development,
    highlighting the potential — and challenges — of strategic AI.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Gary最初执黑，但很快犯了一个错误，遭遇了Deep Blue的强烈、攻击性打法。仅仅19步后，比赛便结束了。Kasparov感到气馁且承受着巨大压力，认为自己的位置已经无法维持，于是选择了认输。这一符号性时刻，被许多人视为人类与机器之间最重要的时刻之一。这个具有里程碑意义的事件标志着人工智能发展的转折点，突显了战略性人工智能的潜力和挑战。
- en: Introduction
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: '![](../Images/2cb43ef33c02ff9f1cd4b58d63a51ddb.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2cb43ef33c02ff9f1cd4b58d63a51ddb.png)'
- en: Image generated by the author using Canva Magic Studio
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用Canva Magic Studio生成
- en: Inspired by the recent advancements in generative AI — and my own experiments
    with large language models and their strategic capabilities — I have increasingly
    been thinking about strategic AI. How have we tried to approach this topic in
    the past? What are the challenges and what remains to be solved before we have
    a more generalist strategic AI agent?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 受最近生成式AI进展的启发——以及我自己对大型语言模型及其战略能力的实验——我越来越多地在思考战略性AI。我们过去是如何尝试接近这个话题的？有哪些挑战，又有哪些问题仍需解决，才能拥有一个更具通用性的战略性AI代理？
- en: As data scientists, we are increasingly implementing AI solutions for our clients
    and employers. For society at large, the ever-increasing interaction with AI makes
    it critical to understand the development of AI and especially strategic AI. Once
    we have autonomous agents with the ability to maneuver well in strategic contexts,
    this will have profound implications for everyone.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据科学家，我们越来越多地为客户和雇主实施AI解决方案。对于整个社会而言，随着与AI的互动日益增加，理解AI的发展，尤其是战略性AI，变得至关重要。一旦我们拥有能够在战略背景中灵活操作的自主代理，这将对每个人产生深远的影响。
- en: But what exactly do we mean when we say *strategic AI*? At its core, strategic
    AI involves machines making decisions that not only consider potential actions,
    but also anticipate and influence the responses of others. It’s about maximizing
    expected outcomes in complex, uncertain environments.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，当我们说*战略性AI*时，究竟是什么意思呢？从本质上讲，战略性AI涉及机器做出决策，不仅考虑潜在的行动，还要预测和影响他人的反应。这是关于在复杂、不确定的环境中最大化期望结果的。
- en: In this article, we’ll define strategic AI, explore what it is and how it has
    developed through the years since IBM’s Deep Blue beat Kasparov in 1997\. We will
    try to understand the general architecture of some of the models, and in addition
    also examine how large language models (LLMs) fit into the picture. By understanding
    these trends and developments, we can better prepare for a world where autonomous
    AI agents are integrated into society.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将定义战略性AI，探讨它是什么，以及自从1997年IBM的深蓝战胜卡斯帕罗夫以来它是如何发展的。我们将尝试理解一些模型的一般架构，并且还会考察大型语言模型（LLMs）如何融入其中。通过理解这些趋势和发展，我们可以更好地为一个自主AI代理融入社会的世界做好准备。
- en: Defining Strategic AI
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义战略性AI
- en: '![](../Images/cee4f4f55a836637e126ee9a59d317db.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cee4f4f55a836637e126ee9a59d317db.png)'
- en: Image generated by the author using Canva Magic Studio
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用Canva Magic Studio生成
- en: A deeper discussion around strategic AI starts with a well-formulated definition
    of the topic.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对战略性AI的深入讨论始于对这一话题的明确定义。
- en: 'When we consider strategy in a commercial setting, we often tend to associate
    it with topics like long-term thinking, resource allocation and optimization,
    a holistic understanding of inter-dependences in an organization, alignment of
    decisions with the purpose and mission of the company and so on. While these topics
    are useful to consider, I often prefer a more game theoretical definition of strategy
    when dealing with AI and autonomous agents. In this case we define being strategic
    as:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在商业环境中考虑战略时，我们通常会将其与长期思维、资源分配与优化、对组织中相互依赖关系的整体理解、决策与公司使命和目标的对齐等话题联系在一起。虽然这些话题值得考虑，但在涉及AI和自主代理时，我通常更喜欢使用博弈论的战略定义。在这种情况下，我们将“战略性”定义为：
- en: Choosing a course of action that maximizes your expected payoff by considering
    not just your own potential actions but also how others will respond to those
    actions and how your decisions impact the overall dynamics of the environment.
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 选择一个行动方案，最大化你的期望收益，不仅要考虑你自己的潜在行动，还要考虑他人如何回应这些行动，以及你的决策如何影响环境的整体动态。
- en: The critical part of this definition is that strategic choices are choices that
    do not occur in a vacuum, but rather in the context of other participants, be
    they humans, organizations or other AIs. These other entities can have similar
    or conflicting goals of their own and may also try to act strategically to further
    their own interests.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定义的关键部分是，战略选择是那些并非在真空中做出的选择，而是在其他参与者的背景下做出的，这些参与者可以是人类、组织或其他AI。这些其他实体可能有相似的或相互冲突的目标，并可能尝试采取战略行动以进一步推动自身利益。
- en: Also, strategic choices always seek to **maximize expected payoffs**, whether
    those payoffs are in terms of money, utility, or other measures of value. If we
    wanted to incorporate the more traditional “commercial” topics related to strategy
    we could imagine that we want to maximize the value of a company 10 years from
    now. In this case, to formulate a good strategy we would need to take a “long
    term” view, and might also consider the “purpose and mission” of the company as
    well, to ensure alignment with the strategy. However, pursuing these endeavors
    are merely a consequence of what it actually means to act strategically.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，战略选择总是力图**最大化期望回报**，无论这些回报是以金钱、效用还是其他价值衡量标准为基础。如果我们想要融入更多传统的与战略相关的“商业”话题，可以想象我们希望最大化一家公司的价值，目标是10年后的情况。在这种情况下，为了制定出好的战略，我们需要采取“长期”的视角，可能还会考虑公司的“使命和目的”，以确保与战略的一致性。然而，追求这些目标只是采取战略性行动的一个结果。
- en: 'The game-theoretic view of strategy captures the essence of strategic decision-making
    and consequently lets us clearly define what we mean by strategic AI. From the
    definition we see that if an AI system or agent is to act strategically, it needs
    to have a few core capabilities. Specifically, it will need to be able to:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏理论中的战略视角捕捉了战略决策的本质，因此使我们能够清晰地定义什么是战略人工智能。从定义中我们可以看出，如果一个人工智能系统或代理要进行战略性行动，它需要具备几个核心能力。具体而言，它需要能够：
- en: '**Model other agents** (using predictive techniques or probabilistic reasoning;
    those agents being anything from humans, AIs or organizations).'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模拟其他代理**（使用预测技术或概率推理；这些代理可以是人类、人工智能或组织）。'
- en: '**Optimize actions** based on expected utility.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于预期效用优化行动**。'
- en: '**Adapt dynamically** as they gather new information about other agents’ strategies.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动态适应**，随着收集到其他代理策略的新信息。'
- en: There is currently no well-known, or well published system, that is capable
    of all of these actions in an autonomous way in the real world. However, given
    the recent advances in AI systems and the rapid rise of LLMs that might be about
    to change!
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当前没有一个广为人知或已发布的系统，能够在现实世界中以自主方式执行这些所有操作。然而，考虑到人工智能系统的最新进展以及大型语言模型（LLMs）的快速崛起，这种情况可能即将发生变化！
- en: Other Important Concepts from Game Theory
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 游戏理论中的其他重要概念
- en: '![](../Images/7cb7baba9e66aec6f9861baf7b765e0a.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7cb7baba9e66aec6f9861baf7b765e0a.png)'
- en: Image generated by the author using Canva Magic Studio
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用Canva Magic Studio生成
- en: Before we proceed with further discussion into strategic AI, it might be useful
    to review some concepts and ideas from game theory. A lot of the work that has
    been done around strategic AI has a foundation in game theoretic concepts and
    using theorems from game theory can show the existence of certain properties that
    make some games and situations easier to deal with than others. It also helps
    to highlight some of the shortcomings of game theory when it comes to real world
    situations and highlights where we might be better off looking in other directions
    for inspiration.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在进一步讨论战略人工智能之前，回顾一些游戏理论中的概念和思想可能会有所帮助。许多关于战略人工智能的工作都基于游戏理论的概念，使用游戏理论中的定理可以揭示某些性质，使得某些游戏和情境比其他情境更容易处理。这也有助于突出游戏理论在应对现实世界情境时的局限性，并强调我们可能更适合从其他方向寻找灵感。
- en: What is a Game?
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是游戏？
- en: 'We define a game as a mathematical model comprising three key components:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将游戏定义为一个包含三个关键组件的数学模型：
- en: '**Players**: The individuals or entities making decisions.'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**玩家**：做出决策的个人或实体。'
- en: '**Strategies**: The possible actions or plans each player can adopt.'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**策略**：每个玩家可以采取的可能行动或计划。'
- en: '**Payoffs**: The rewards or outcomes each player receives based on the chosen
    strategies.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**回报**：每个玩家根据选择的策略所获得的奖励或结果。'
- en: This formal structure allows for the systematic study of strategic interactions
    and decision-making processes.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这种正式的结构使得战略互动和决策过程的系统性研究成为可能。
- en: Finite vs Infinite Games
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有限游戏与无限游戏
- en: When speaking on games it also makes sense to look at the distinction between
    finite and infinite games.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论游戏时，区分有限游戏和无限游戏也是有意义的。
- en: Finite games have a fixed set of players, defined rules, and a clear endpoint.
    The objective is to win, and examples include chess, go, checkers, and most traditional
    board games.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 有限游戏有一组固定的玩家，定义的规则和明确的终点。目标是获胜，例子包括象棋、围棋、跳棋和大多数传统棋盘游戏。
- en: Infinite games on the other hand have no predetermined endpoint, and the rules
    can evolve over time. The objective is not to win but to continue playing. Real-world
    scenarios like business competition or societal evolution can be viewed as infinite
    games. The Cold War can be viewed as an example of an infinite game. It was a
    prolonged geopolitical struggle between the United States and its allies (the
    West) and the Soviet Union and its allies (the East). The conflict had no fixed
    endpoint, and the strategies and “rules” evolved over time.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，无限游戏没有预定的结束点，规则可以随着时间的推移而演变。其目标不是赢得胜利，而是继续游戏。像商业竞争或社会演变这样的现实世界情境可以看作是无限游戏。冷战可以作为无限游戏的一个例子。它是美国及其盟国（西方）与苏联及其盟国（东方）之间长期的地缘政治斗争。这场冲突没有固定的终点，战略和“规则”随着时间变化。
- en: Subgames
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 子博弈
- en: 'Sometimes we might be able to find smaller games within a larger game context.
    Mathematically, subgames are self-contained games in their own right, and the
    need to satisfy a few different criteria:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们可能会在一个更大的游戏背景中找到较小的游戏。从数学角度来看，子博弈是一个自成体系的游戏，并且需要满足一些不同的标准：
- en: A subgame starts at a point where the player knows exactly where they are in
    the game.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 子博弈从玩家清楚自己在游戏中的位置的时刻开始。
- en: It includes every possible action and outcome that could follow from that point.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它包含了从那个时刻开始可能发生的所有行动和结果。
- en: It encompasses all the players’ knowledge and uncertainties relevant to those
    actions.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它包含了所有玩家的知识和与这些行动相关的不确定性。
- en: We can visualize a subgame if we imagine a large tree representing an entire
    game. A subgame is like selecting a branch of this tree starting from a certain
    point (node) and including everything that extends from it, while also ensuring
    that any uncertainties are fully represented within this branch.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过想象一棵代表整个游戏的大树来可视化一个子博弈。子博弈就像从某个特定点（节点）开始选择这棵树的一条分支，并包括从该节点延伸出来的所有内容，同时确保该分支中的任何不确定性都被完全呈现。
- en: The core idea behind a subgame makes it useful for our discussion around strategic
    AI. The reason is primarily that some infinite games between players might be
    very complex and difficult to model while if we choose to look at smaller games
    within that game, we can have more success applying game theoretical analysis.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 子博弈背后的核心理念使它在我们讨论战略人工智能时变得有用。原因主要在于，一些玩家之间的无限游戏可能非常复杂且难以建模，而如果我们选择在这个游戏中查看较小的游戏，就能更成功地应用博弈理论分析。
- en: 'Coming back to our example with the Cold War as an infinite game, we can recognize
    several subgames within that context. Some examples include:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们以冷战作为无限游戏的例子，我们可以在这一背景中识别出几个子博弈。一些例子包括：
- en: '**The Cuban Missile Crisis (1962):**'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**古巴导弹危机（1962年）**：'
- en: '**Players**: The United States and the Soviet Union.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**玩家**：美国和苏联。'
- en: '**Strategies**: The U.S. considered options ranging from diplomatic negotiations
    to military invasion, while the Soviet Union had to decide whether to remove the
    missiles or escalate the confrontation.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略**：美国考虑了从外交谈判到军事入侵的各种选项，而苏联则需要决定是否撤除导弹或升级对抗。'
- en: '**Payoffs**: Avoiding nuclear war, maintaining global image, and strategic
    military positioning.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回报**：避免核战争，维持全球形象，以及战略军事定位。'
- en: '**The Berlin Blockade and Airlift (1948–1949):**'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**柏林封锁与空运（1948–1949年）**：'
- en: '**Players**: The Western Allies and the Soviet Union.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**玩家**：西方盟国和苏联。'
- en: '**Strategies**: The Soviets blocked Berlin to push the Allies out, while the
    Allies had to decide between abandoning the city or supplying it via air.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略**：苏联封锁柏林以迫使盟国撤出，而盟国则必须决定是放弃这座城市还是通过空运进行供应。'
- en: '**Payoffs**: Control over Berlin, demonstrating political resolve, and influencing
    European alignment.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回报**：控制柏林，展示政治决心，影响欧洲的政治走向。'
- en: Although of course very difficult and complex to deal with, both “subgames”
    are easier to analyze and develop responses to than to the whole of the Cold War.
    They had a defined set of players, with a limited set of strategies and payoffs,
    and also a clearer time frame. This made them both more applicable for game theoretical
    analysis.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管当然非常困难和复杂，但这两个“子博弈”比整个冷战更容易分析和制定反应。它们有一组明确的玩家，有限的策略和回报，并且时间框架也更为清晰。这使得它们更适合进行博弈理论分析。
- en: In the context of strategic AI, analyzing these sub-games is crucial for developing
    intelligent systems capable of making optimal decisions in complex, dynamic environments.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在战略人工智能的背景下，分析这些子博弈对开发能够在复杂动态环境中做出最优决策的智能系统至关重要。
- en: Two Player Games
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**双人博弈**'
- en: Two player games are simply a game between two players. This could for example
    be a game between two chess players, or coming back to our Cold War example, the
    West vs the East. Having only two players in the game simplifies the analysis
    but still captures essential competitive or cooperative dynamics. Many of the
    results in game theory are based around two player games.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 双人博弈指的是两名玩家之间的游戏。例如，这可以是两名国际象棋玩家之间的对局，或者回到我们的冷战例子，西方与东方的对抗。只有两个玩家的博弈简化了分析，但仍能捕捉到基本的竞争或合作动态。博弈论中的许多结果都是围绕双人博弈展开的。
- en: Zero-Sum Games
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**零和博弈**'
- en: Zero-sum games are a subset of games where one player’s gain is another player’s
    loss. The total payoff remains constant, and the players are in direct competition.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 零和博弈是一类博弈，其中一个玩家的收益是另一个玩家的损失。总的支付保持不变，玩家之间是直接竞争的关系。
- en: Nash Equilibrium and Optimal Actions
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 纳什均衡与最优行动
- en: A Nash Equilibrium (NE) is a set of strategies where no player can gain additional
    benefit by unilaterally changing their own strategy, assuming the other players
    keep theirs unchanged. In this state, each player’s strategy is the best response
    to the strategies of the others, leading to a stable outcome where no player has
    an incentive to deviate.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 纳什均衡（NE）是指一组策略，在此状态下，没有玩家能够通过单方面改变自己的策略而获得额外的好处，前提是其他玩家保持不变。在这种状态下，每个玩家的策略都是对其他玩家策略的最佳回应，导致一个稳定的结果，其中没有玩家有动机偏离该策略。
- en: For example, in the game Rock-Paper-Scissor (RPS), the NE is the state where
    all players play rock, paper and scissors, randomly, each with equal probability.
    If you as a player choose to play the NE strategy, you ensure that no other player
    can exploit your play and in a two player zero-sum games it can be shown that
    you will not lose in expectation, and that the worst you can do is break even.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在剪刀石头布（RPS）游戏中，纳什均衡是所有玩家随机地以相等的概率选择石头、纸和剪刀的状态。如果你作为玩家选择纳什均衡策略，你可以确保没有其他玩家能够利用你的出招，并且在双人零和博弈中，可以证明你期望不会失败，最糟糕的结果是打平。
- en: However, playing a NE strategy might not always be the optimal strategy, especially
    if your opponent is playing in a predictably sub-optimal way. Consider a scenario
    with two players, A and B. If player B starts playing paper more, player A could
    recognize this and increase its frequency of playing scissors. However, this deviation
    from A could again be exploited by B again which could change and play more rock.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，选择纳什均衡策略可能并不总是最优策略，特别是当对手以可预测的次优方式进行游戏时。考虑一个有两个玩家A和B的场景。如果B玩家开始更多地选择“纸”，A玩家可以识别这一点并增加“剪刀”的出场频率。然而，A的这种偏离行为可能会再次被B利用，B可能会改变策略，更多地选择“石头”。
- en: Key Takeaways Regarding Strategic AI
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**关于战略人工智能的关键要点**'
- en: Reviewing the game theoretic concepts, it would seem the idea of a subgame is
    especially useful for strategic AI. The ability to find possible smaller and easier
    to analyze games within a larger context makes it easier to apply already know
    solutions and solvers.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾博弈论的概念，子博弈的思想对于战略人工智能尤其有用。在更大范围的上下文中找到可能的小型且更易于分析的博弈，使得应用已经知道的解决方案和解算器变得更加容易。
- en: For example, let’s say you are working on developing your career, something
    which could be classified as an infinite game and difficult to “solve”, but suddenly
    you get the opportunity to negotiate a new contract. This negotiation process
    presents an opportunity for a subgame within your career and would be much more
    approachable for a strategic AI using game theoretic concepts.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你正在发展自己的职业生涯，这可以被归类为一种无限博弈，并且很难“解决”，但突然间，你获得了谈判新合同的机会。这个谈判过程为你的职业生涯中提供了一个子博弈的机会，使用博弈论概念的战略人工智能会使这个过程更加易于接近。
- en: Indeed, humans have been creating subgames within our lives for thousands of
    years. About 1500 years ago in India, we created the origins of what is now known
    as chess. Chess turned out to be quite a challenge for AI to beat, but also allowed
    us to start developing more mature tools and techniques that could be used for
    even more complicated and difficult strategic situations.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，人类在我们的生活中已经创造了数千年的子游戏。大约1500年前，在印度，我们创造了如今所知的象棋的起源。象棋证明对人工智能而言是一个相当大的挑战，但也促使我们开始开发更加成熟的工具和技术，这些工具和技术可以应用于更加复杂和困难的战略性情境。
- en: A Short History of Strategic AI in Games
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 战略人工智能在游戏中的简短历史
- en: '![](../Images/b95e03d50c5f7748be10b0da6b218516.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b95e03d50c5f7748be10b0da6b218516.png)'
- en: Image generated by the author using Canva Magic Studio
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 该图像由作者使用Canva Magic Studio生成
- en: Games have provided an amazing proving ground for developing strategic AI. The
    closed nature of games makes it easier to train models and develop solution techniques
    than in open ended systems. Games are clearly defined; the players are known and
    so are the payoffs. One of the biggest and earliest milestones was Deep Blue,
    the machine that beat the world champion in chess.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏为开发战略性人工智能提供了一个极佳的试验场。游戏的封闭性使得训练模型和开发解决方案比开放系统更为容易。游戏的规则明确；玩家已知，回报也已知。一个重要且早期的里程碑是深蓝（Deep
    Blue），这台机器击败了世界象棋冠军。
- en: '**Early Milestones**: Deep Blue'
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**早期里程碑**：深蓝'
- en: Deep Blue was a chess-playing supercomputer developed by IBM in the 1990s. As
    stated in the prologue, it made history in May 1997 by defeating the reigning
    world chess champion, Garry Kasparov, in a six-game match. Deep Blue utilized
    specialized hardware and algorithms capable of evaluating 200 million chess positions
    per second. It combined brute-force search techniques with heuristic evaluation
    functions, enabling it to search deeper into potential move sequences than any
    previous system. What made Deep Blue special was its ability to process vast numbers
    of positions quickly, effectively handling the combinatorial complexity of chess
    and marking a significant milestone in artificial intelligence.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 深蓝是由IBM在1990年代开发的象棋超级计算机。如同序言中所述，1997年5月，它通过在六局比赛中击败当时的世界象棋冠军加里·卡斯帕罗夫（Garry
    Kasparov）创造了历史。深蓝采用了专门的硬件和算法，能够每秒评估2亿个象棋局面。它将暴力搜索技术与启发式评估函数相结合，使其能够比任何先前的系统深入搜索潜在的走法序列。深蓝的特别之处在于它能够迅速处理大量局面，成功应对象棋的组合复杂性，标志着人工智能的一大里程碑。
- en: However, as Gary Kasparov notes in his interview with Lex Fridman¹, Deep Blue
    was more of a brute force machine than anything else, so it’s perhaps hard to
    qualify it as any type of intelligence. The core of the search is basically just
    trial and error. And speaking of errors, it makes significantly less errors than
    humans, and according to Kasparov this is one of the features which made it hard
    to beat.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如加里·卡斯帕罗夫在与莱克斯·弗里德曼（Lex Fridman）¹的采访中所指出的那样，深蓝更像是一台暴力机器，而非智能机器，因此很难将其归类为某种类型的智能。其搜索的核心基本上只是反复试错。说到错误，它比人类犯的错误要少得多，而根据卡斯帕罗夫的说法，这也是它很难被击败的一个特点。
- en: '**Advancements in Complex Games**: AlphaGo'
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**复杂游戏的进展**：AlphaGo'
- en: 19 years after the Deep Blue victory in chess, a team from Google’s DeepMind
    produced another model that would contribute to a special moment in the history
    of AI. In 2016, AlphaGo became the first AI model to defeat a world champion go
    player, Lee Sedol.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在深蓝赢得象棋比赛的19年后，谷歌DeepMind团队开发出了另一款模型，这为人工智能历史上的一个特殊时刻做出了贡献。2016年，AlphaGo成为第一个击败世界围棋冠军李世石的人工智能模型。
- en: Go is a very old board game with origins in Asia, known for its deep complexity
    and vast number of possible positions, far exceeding those in chess. AlphaGo combined
    deep neural networks with Monte Carlo tree search, allowing it to evaluate positions
    and plan moves effectively. The more time AlphaGo was given at inference, the
    better it performs.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 围棋是一项源自亚洲的古老棋类游戏，以其深奥的复杂性和庞大的可能局面数而著称，远远超过象棋。AlphaGo将深度神经网络与蒙特卡罗树搜索相结合，使其能够有效地评估局面并规划走法。AlphaGo在推理时所给予的时间越多，它的表现就越好。
- en: The AI trained on a dataset of human expert games and improved further through
    self-play. What made AlphaGo special was its ability to handle the complexity
    of Go, utilizing advanced machine learning techniques to achieve superhuman performance
    in a domain previously thought to be resistant to AI mastery.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 AI 在人类专家的对局数据集上进行了训练，并通过自我对弈进一步提升。AlphaGo 的特别之处在于它能处理围棋的复杂性，利用先进的机器学习技术在这一被认为难以被
    AI 掌握的领域实现了超人类的表现。
- en: One could argue AlphaGo exhibits more intelligence than Deep Blue, given its
    exceptional ability to deeply evaluate board states and select moves. Move 37
    from its 2016 game against Lee Sedol is a classic example. For those acquainted
    with Go, it was a shoulder hit at the fifth line and initially baffled commentators,
    including Lee Sedol himself. But as would later become clear, the move was a brilliant
    play and showcased how AlphaGo would explore strategies that human players might
    overlook and disregard.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 有人可能会认为 AlphaGo 比 Deep Blue 更具智能，因为它具有深度评估棋盘状态并选择走法的卓越能力。2016 年它与李世石的对局中的第 37
    手就是一个经典例子。对于熟悉围棋的人来说，那是第五路的肩膀打击，最初让包括李世石本人在内的解说员都感到困惑。但后来证明，这一手是一次精彩的走法，展示了 AlphaGo
    如何探索人类玩家可能忽视和忽略的策略。
- en: 'Combining Chess and Go: AlphaZero'
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结合国际象棋与围棋：AlphaZero
- en: One year later, Google DeepMind made headlines again. This time, they took many
    of the learnings from AlphaGo and created AlphaZero, which was more of a general-purpose
    AI system that mastered chess, as well as Go and shogi. The researchers were able
    to build the AI solely through self-play and reinforcement learning without prior
    human knowledge or data. Unlike traditional chess engines that rely on handcrafted
    evaluation functions and extensive opening libraries, AlphaZero used deep neural
    networks and a novel algorithm combining Monte Carlo tree search with self-learning.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一年后，Google DeepMind 再次成为新闻焦点。这一次，他们从 AlphaGo 中汲取了许多经验，创造了 AlphaZero，这是一个更具通用性的
    AI 系统，掌握了国际象棋、围棋和将棋。研究人员能够通过自我对弈和强化学习来构建这个 AI，而无需依赖人类的先验知识或数据。与依赖手工制作的评估函数和广泛开局库的传统国际象棋引擎不同，AlphaZero
    使用了深度神经网络和一种结合蒙特卡洛树搜索与自我学习的新算法。
- en: The system started with only the basic rules and learned optimal strategies
    by playing millions of games against itself. What made AlphaZero special was its
    ability to discover creative and efficient strategies, showcasing a new paradigm
    in AI that leverages self-learning over human-engineered knowledge.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 该系统从仅具备基本规则开始，通过与自己对弈数百万局来学习最佳策略。AlphaZero 的特别之处在于它能够发现创造性且高效的策略，展示了一个新的 AI
    模式，利用自我学习而非人类工程化知识。
- en: 'Integrating Speed and Strategy: Star Craft II'
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结合速度与策略：《星际争霸 II》
- en: Continuing its domination in the AI space, the Google DeepMind team changed
    its focus to a highly popular computer game, StarCraft II. In 2019 they developed
    an AI called AlphaStar² which was able to achieve Grandmaster level play and rank
    higher than 99.8% of human players on the competitive leaderboard.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续主宰 AI 领域的过程中，Google DeepMind 团队将焦点转向了一个备受欢迎的电脑游戏——《星际争霸 II》。2019 年，他们开发了一个名为
    AlphaStar² 的 AI，该 AI 能够达到大师级水平，并在竞争排行榜上超过 99.8% 的人类玩家。
- en: StarCraft II is a real time strategy game that provided several novel challenges
    for the team at DeepMind. The goal of the game is to conquer the opposing player
    or players, by gathering resources, constructing buildings and amassing armies
    that can defeat the opponent. The main challenges in this game arise from the
    enormous action space that needs to be considered, the real-time decision making,
    partial observability due to fog of war and the need for long-term strategic planning,
    as some games can last for hours.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 《星际争霸 II》是一款实时战略游戏，给 DeepMind 团队带来了几个新颖的挑战。游戏的目标是通过收集资源、建造建筑物和积累能击败对手的军队，来征服对手玩家。游戏中的主要挑战来自于需要考虑的巨大动作空间、实时决策、由于战争迷雾而导致的部分可观察性，以及长期战略规划的需求，因为某些游戏可能会持续数小时。
- en: 'By building on some of the techniques developed for previous AIs, like reinforcement
    learning through self-play and deep neural networks, the team was able to make
    a unique game engine. Firstly, they trained a neural net using supervised learning
    and human play. Then, they used that to seed another algorithm that could play
    against itself in a multi-agent game framework. The DeepMind team created a virtual
    league where the agents could explore strategies against each other and where
    the dominant strategies would be rewarded. Ultimately, they combined the strategies
    from the league into a super strategy that could be effective against many different
    opponents and strategies. In their own words³:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 通过借鉴先前AI技术的发展，如通过自我对弈的强化学习和深度神经网络，团队成功开发出了一个独特的游戏引擎。首先，他们使用监督学习和人类游戏数据训练了一个神经网络。接着，利用该网络启动了一个能够在多代理游戏框架中自我对弈的算法。DeepMind团队创建了一个虚拟联赛，在这个联赛中，代理能够相互探索策略，成功的策略会得到奖励。最终，他们将联赛中的策略整合成一个超级策略，这个策略能够在面对不同对手和多种策略时保持有效。正如他们所言³：
- en: The final AlphaStar agent consists of the components of the [Nash distribution
    of the league](https://papers.nips.cc/paper/7588-re-evaluating-evaluation.pdf)
    — in other words, the most effective mixture of strategies that have been discovered
    — that run on a single desktop GPU.
  id: totrans-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 最终的AlphaStar代理由[联盟的纳什分布](https://papers.nips.cc/paper/7588-re-evaluating-evaluation.pdf)组成——换句话说，就是发现的最有效策略的混合体——并运行在一台桌面GPU上。
- en: '**Deep Dive into Pluribus and Poker**'
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**深入探讨 Pluribus 和扑克**'
- en: 'I love playing poker, and when I was living and studying in Trondheim, we used
    to have a weekly cash game which could get quite intense! One of the last milestones
    to be eclipsed by strategic AI was in the game of poker. Specifically, in one
    of the most popular forms of poker, 6-player no-limit Texas hold’em. In this game
    we use a regular deck of cards with 52 cards, and the play follows the following
    structure:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我非常喜欢玩扑克，曾在特隆赫姆（Trondheim）生活和学习时，我们每周都会举行一次现金牌局，有时气氛相当紧张！战略性人工智能克服的最后一个里程碑便是在扑克游戏中。特别是，在扑克中最流行的形式之一——6人无限注德州扑克。这个游戏使用的是一副包含52张牌的常规扑克牌，游戏遵循以下结构：
- en: '**The Preflop:** All players are given 2 cards (hole cards) which only they
    themselves know the value of.'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**前翻牌阶段（The Preflop）：** 所有玩家被发放2张手牌（口袋牌），这些牌只有玩家自己知道其点数。'
- en: '**The Flop:** 3 cards are drawn and laid face up so that all players can see
    them.'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**翻牌（The Flop）：** 三张牌被翻开，所有玩家都能看到这三张牌。'
- en: '**The Turn:** Another card is drawn and laid face up.'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**转牌（The Turn）：** 又一张牌被翻开，正面朝上。'
- en: '**The River:** A final 5th card is drawn and laid face up.'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**河牌（The River）：** 最后一张第五张牌被翻开，正面朝上。'
- en: The players can use the cards on the table and the two cards on their hand to
    assemble a 5-card poker hand. For each round of the game, the players take turns
    placing bets, and the game can end at any of the rounds if one player places a
    bet that no one else is willing to call.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 玩家可以利用桌面上的公共牌和自己手中的两张牌，组成一手5张牌的扑克组合。在每一轮游戏中，玩家轮流下注，游戏可以在任何一轮结束，如果有玩家下注而其他玩家不愿跟注。
- en: Though reasonably simple to learn, one only needs to know the hierarchy of the
    various poker hands, this game proved to be very difficult to solve with AI, despite
    ongoing efforts for several decades.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管学习起来相对简单，只需了解各种扑克牌型的排名，这个游戏通过人工智能的努力，依然证明是非常难以解决的，尽管已有数十年的尝试。
- en: There are multiple factors contributing to the difficulty of solving poker.
    Firstly, we have the issue of hidden information, because you don’t know which
    cards the other players have. Secondly, we have a multiplayer setup with many
    players, with each extra player increasing the number of possible interactions
    and strategies exponentially. Thirdly, we have the no-limit betting rules, which
    allow for a complex betting structure where one player can suddenly decide to
    bet his entire stack. Fourth, we have an enormous game tree complexity due to
    the combinations of hole cards, community cards, and betting sequences. In addition,
    we also have complexity due to the stochastic nature of the cards, the potential
    for bluffing and the opponent modelling!
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 解决扑克问题的难度由多个因素导致。首先，我们面临隐藏信息的问题，因为你并不知道其他玩家手中有哪几张牌。其次，我们有一个多人对战的局面，参与的玩家越多，可能的互动和策略就呈指数级增长。第三，扑克游戏有无限制的下注规则，这导致了一个复杂的下注结构，玩家可以突然决定将自己的所有筹码押上。第四，游戏树的复杂度巨大，这源于口袋牌、公共牌和下注顺序的不同组合。此外，还有因扑克牌的随机性、虚张声势的可能性和对手建模的复杂性带来的挑战！
- en: It was only in 2019 that a couple of researchers, Noam Brown and Tuomas Sandholm,
    finally cracked the code. In a paper published in Science, they describe a novel
    poker AI — Pluribus — that managed to beat the best players in the world in 6-player
    no-limit Texas hold’em.⁴ They conducted two different experiments, each consisting
    of a 10000 poker hands, and both experiments clearly showed the dominance of Pluribus.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 直到2019年，研究人员Noam Brown和Tuomas Sandholm才最终破解了这个难题。在《科学》期刊上发表的论文中，他们描述了一种新型的扑克人工智能——Pluribus——它成功地击败了世界上最顶尖的玩家，在6人无限注德州扑克中获胜。⁴他们进行了两次不同的实验，每个实验都包括10000局扑克，而这两个实验都清楚地显示了Pluribus的优势。
- en: In the first experiment, Pluribus played against 5 human opponents, achieving
    an average win rate of 48 mbb/game, with a standard deviation of 25 mbb/game.
    (mbb/game stands for milli big blind per game, how many big blinds is won per
    1000 games played.) 48 mbb/game is considered a very high win rate, especially
    among elite poker players, and implies that Pluribus is stronger than the human
    opponents.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次实验中，Pluribus与5位人类对手对战，取得了每局48 mbb（毫大盲注）的平均胜率，标准差为25 mbb/局。（mbb/局代表每1000局游戏中赢得的毫大盲注数量。）48
    mbb/局被认为是一个非常高的胜率，尤其是在精英扑克玩家中，这意味着Pluribus比人类对手更强。
- en: In the second experiment, the researchers had 5 versions of Pluribus play against
    1 human. They set up the experiment so that 2 different humans would each play
    5000 hands each against the 5 machines. Pluribus ended up beating the humans by
    an average of 32 mbb/game with a standard error of 15 mbb/game, again showing
    its strategic superiority.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二次实验中，研究人员让5个版本的Pluribus与1位人类对战。他们设置了实验，使得2个不同的人类各自与这5台机器对战5000局。最终，Pluribus以每局平均32
    mbb的优势战胜了人类，标准误差为15 mbb/局，再次展示了其战略上的优越性。
- en: 'The dominance of Pluribus is quite amazing, especially given all the complexities
    the researchers had to overcome. Brown and Sandholm came up with several smart
    strategies that helped Pluribus to become superhuman and computationally much
    more efficient than previous top poker AIs. Some of their techniques include:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Pluribus的优势非常惊人，特别是考虑到研究人员必须克服的所有复杂性。Brown和Sandholm提出了几种聪明的策略，帮助Pluribus变得超越人类，并在计算上比以往的顶级扑克AI更高效。他们的一些技术包括：
- en: The use of two different algorithms for evaluating moves. They would first use
    a so called “blueprint strategy” which was created by having the program play
    against itself using a method called Monte Carlo counterfactual regret minimization.
    This blueprint strategy would be used in the first round of betting, but in subsequent
    betting rounds, Pluribus conducts a real-time search to find a better more granular
    strategy.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用两种不同的算法来评估操作。他们首先使用所谓的“蓝图策略”，该策略是通过让程序与自己对战并采用一种称为蒙特卡罗反事实遗憾最小化的方法创建的。这个蓝图策略将用于第一轮投注，但在随后的投注回合中，Pluribus会进行实时搜索，以找到更好、更精细的策略。
- en: To make its real-time search algorithm be more computationally efficient, they
    would use a dept-limited search and evaluate 4 different possible strategies that
    the opponents might choose to play. Firstly, they would evaluate each strategy
    for 2 moves ahead. In addition, they would only evaluate four different strategies
    for the opponents, including the original blueprint strategy, a blueprint strategy
    biased towards folding, a blueprint strategy biased towards calling and a final
    blueprint strategy biased towards raising.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了使其实时搜索算法更加高效，他们采用了深度限制搜索，并评估对手可能选择的4种不同策略。首先，他们会评估每个策略2步之内的效果。此外，他们只评估对手的四种不同策略，包括原始的蓝图策略、一种偏向于弃牌的蓝图策略、一种偏向于跟注的蓝图策略和一种偏向于加注的最终蓝图策略。
- en: They also used various abstraction techniques to reduce the number of possible
    game states. For example, because a 9 high straight is fundamentally similar to
    a 8 high straight these can be viewed in a similar way.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 他们还使用了各种抽象技术来减少可能的游戏状态数量。例如，由于9高顺子本质上与8高顺子相似，因此可以将它们以类似的方式看待。
- en: Pluribus would discretize the continuous betting space into a limited set of
    buckets, making it easier to consider and evaluate various betting sizes.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Pluribus将连续的投注空间离散化成一组有限的“桶”，使得考虑和评估各种投注大小变得更加容易。
- en: In addition, Pluribus also balances its strategy in way that for any given hand
    it is playing, it would also consider other possible hands it could have in that
    situation and evaluate how it would play those hands, so that the final play would
    be balanced and thus harder to counter.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，Pluribus还以一种平衡的方式调整它的策略，在每一局它所玩的手牌中，它还会考虑到在该情形下可能出现的其他手牌，并评估如何玩这些手牌，从而确保最终的玩法是平衡的，进而更难被反制。
- en: There are quite a few interesting observations to draw from Pluribus, but perhaps
    the most interesting is that it doesn’t vary its play against different opponents,
    but instead has developed a robust strategy that is effective against a wide variety
    of players. Since a lot of poker players think they have to adjust their play
    to various situations and people, Pluribus shows us that this is not needed and
    probably not even optimal, given how it beat all the humans it played against.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 从Pluribus中可以得出一些有趣的观察，但或许最有趣的是，它在面对不同对手时并不改变自己的玩法，而是发展出了一种对多种玩家都有效的稳健策略。由于许多扑克玩家认为他们必须根据不同的情况和人群调整自己的玩法，Pluribus向我们展示了这一点是多余的，甚至可能不是最优策略，鉴于它击败了所有与之对战的玩家。
- en: In our short foray into game theory, we noted that if you play the NE strategy
    in two-player zero-sum games you are guaranteed not to lose in expectation. However,
    for a multiplayer game like 6-player poker there is no such guarantee. Noam Brown
    speculates⁵ that it is perhaps the adversarial nature of a game like poker which
    still makes it suitable to try to approach it with a NE strategy. Conversely,
    in a game like Risk where players can cooperate more, pursuing a NE strategy is
    not guaranteed to work, because, if you are playing a risk game with 6 people,
    there is nothing you can do if your 5 opponents decide to gang up on you and kill
    you.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对博弈论的简短探讨中，我们注意到，如果你在双人零和游戏中采用NE策略，那么在期望值上你可以保证不会输。然而，对于像6人扑克这样的多人游戏来说，并不存在这样的保证。Noam
    Brown推测⁵，或许正是扑克等游戏的对抗性特征使得它仍然适合尝试用NE策略来接近。相反，在像《风险》这样玩家可以更多合作的游戏中，追求NE策略并不一定有效，因为如果你在与6人玩的《风险》游戏中，若你的5个对手决定联合起来攻击你并将你淘汰，那么你无能为力。
- en: Evaluating the Trend in Strategic AI
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估战略AI的趋势
- en: Summarizing the history of strategic AI in games, we see a clear trend emerging.
    The games are slowly but surely becoming closer to the real-world strategic situations
    that humans find themselves in on an everyday basis.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 总结战略AI在游戏中的历史，我们看到一个明显的趋势正在形成。游戏正逐渐但稳步地接近人类日常生活中所面临的现实战略情境。
- en: Firstly, we are moving from a two-player to a multiplayer setting. This can
    be seen from the initial success in two-player games to multiplayer games like
    6-player poker. Secondly, we are seeing an increase in the mastery of games with
    hidden information. Thirdly we are also seeing an increase in mastery of games
    with more stochastic elements.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们正在从双人游戏过渡到多人游戏。这可以从最初在双人游戏中的成功到6人扑克等多人游戏的出现中看出。其次，我们看到在掌握含有隐藏信息的游戏方面的进展。第三，我们还看到在掌握更多随机元素的游戏方面的进步。
- en: Hidden information, multiplayer settings and stochastic events are the norm
    rather than the exception in strategic interactions among humans, so mastering
    these complexities is key in achieving a more general superhuman strategic AI
    that can navigate in the real world.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏信息、多玩家设置和随机事件在人类的战略互动中是常态，而非例外，因此，掌握这些复杂性对于实现能够在现实世界中导航的更为通用的超人类战略AI至关重要。
- en: Large Language Models and Strategic AI
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型与战略AI
- en: '![](../Images/f8b08e014ecbb186e08284200c885f58.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8b08e014ecbb186e08284200c885f58.png)'
- en: Image generated by the author using Canva Magic Studio
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用Canva Magic Studio生成
- en: I recently ran an experiment where I let LLMs play the boardgame Risk against
    each other. My objective with the experiment was to gauge how well the LLMs could
    perform in a strategic setting, more of less out of the box. Quite a lot of detailed
    prompting were given to the agents to provide the right context, however, and
    perhaps not surprisingly, the LLM performance was rather mediocre.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我最近进行了一项实验，让LLM相互对战桌面游戏《风险》。我的实验目标是评估LLM在战略环境中的表现，基本上是“开箱即用”的状态。当然，为了提供正确的上下文，给这些代理人做了大量详细的提示，但也许并不令人意外的是，LLM的表现相当平庸。
- en: 'You can find an article about the experiment here:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到关于这个实验的文章：
- en: '[](/exploring-the-strategic-capabilities-of-llms-in-a-risk-game-setting-43c868d83c3b?source=post_page-----91052e4c5da9--------------------------------)
    [## Exploring the Strategic Capabilities of LLMs in a Risk Game Setting'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/exploring-the-strategic-capabilities-of-llms-in-a-risk-game-setting-43c868d83c3b?source=post_page-----91052e4c5da9--------------------------------)
    [## 探索LLM在风险游戏环境中的战略能力'
- en: In a simulated Risk environment, large language models from Anthropic, OpenAI,
    and Meta showcase distinct strategic…
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在一个模拟的风险环境中，Anthropic、OpenAI 和 Meta 的大型语言模型展示了不同的战略能力……
- en: towardsdatascience.com](/exploring-the-strategic-capabilities-of-llms-in-a-risk-game-setting-43c868d83c3b?source=post_page-----91052e4c5da9--------------------------------)
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/exploring-the-strategic-capabilities-of-llms-in-a-risk-game-setting-43c868d83c3b?source=post_page-----91052e4c5da9--------------------------------)
- en: Summarizing some of the key findings from the experiment, the current generation
    of LLMs struggles with basic strategic concepts like fortification and recognizing
    winning moves. They also fail to eliminate other players when it would have been
    strategically beneficial for them to do so.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 总结实验中的一些关键发现，目前一代的大型语言模型在处理像防御和识别制胜之举等基本战略概念时存在困难。它们也未能在战略上有利时消除其他玩家。
- en: The above experiment indicates that even though we have seen a rapid improvement
    in the LLMs, they still lack the sophistication for strategic reasoning. Given
    their very general training data and how they have been constructed this shouldn’t
    come as a surprise.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 上述实验表明，尽管我们已看到LLM的快速进步，但它们仍然缺乏战略推理的复杂性。考虑到它们的训练数据非常通用，并且它们的构建方式，这一点并不令人惊讶。
- en: So how do they fit into the discussion around strategic AI? To understand that,
    we need to understand what the LLMs really excel at. Perhaps the most promising
    feature of the LLMs is their ability to digest and generate vast amounts of text.
    And now with multimodal models, video and audio too. In other words, LLMs are
    great for interacting with the real world, both in human and other contexts. Recently,
    an AI team at Meta was able to combine the general language capabilities of a
    language model with the strategic insights of a strategy engine.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，它们如何融入到战略AI的讨论中呢？为了理解这一点，我们需要了解LLM真正擅长的是什么。也许LLM最有前途的特性是它们能够处理和生成大量文本。现在，随着多模态模型的出现，它们还可以处理视频和音频。换句话说，LLM非常适合与现实世界互动，无论是在人类还是其他背景下。最近，Meta的一个AI团队成功将语言模型的通用语言能力与战略引擎的战略洞察结合起来。
- en: 'Case Study: Cicero and Diplomacy'
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 案例研究：Cicero与《外交》
- en: The game of Diplomacy is a 2 to 7-player strategy game, which Meta describes
    as a mix between Risk, Poker and the TV show Survivor. The players start out with
    a map of Europe ca. 1900, and the objective is to gain control over a majority
    of supply centers. Specifically, a player aims to control 18 out of 34 supply
    centers to achieve victory. By doing so, a player effectively dominates the map,
    representing their nation’s ascendancy over Europe in the period leading up to
    World War I.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 《外交》是一款2至7人玩的策略游戏，Meta将其描述为《风险》《扑克》和电视节目《幸存者》的混合体。玩家们从约1900年的欧洲地图开始，目标是控制大多数补给中心。具体来说，玩家需要控制34个补给中心中的18个才能获得胜利。通过这样做，玩家实际上支配了地图，代表了他们的国家在第一次世界大战前夕对欧洲的主导地位。
- en: What sets Diplomacy apart from many of the other games we have discussed so
    far is its reliance on negotiations between players. It’s a much more cooperative
    form of play than for example poker. Each player uses natural language to communicate
    with the other players before each turn, and they make plans to ally with each
    other. When the preparations are finished all players reveal their plans at the
    same time and the turn is executed. This type of game obviously resembles actual
    diplomacy and real-life negotiations closer than most other boardgames, however
    because of the natural language component, it has been very difficult for AI to
    master.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 《外交》与我们迄今讨论的许多其他游戏不同之处在于它依赖于玩家之间的谈判。这是一种比扑克等游戏更具合作性的玩法形式。每位玩家在每回合之前使用自然语言与其他玩家进行交流，并制定结盟计划。当所有准备工作完成后，所有玩家同时揭示他们的计划，并执行回合。这种类型的游戏显然比大多数其他棋盘游戏更接近实际的外交和现实中的谈判，然而由于自然语言的因素，AI很难掌握。
- en: This changed in 2022, when the AI team at Meta developed Cicero. Using the latest
    advancements in language modelling, combined with a strategic module, Cicero was
    a game engine that was able to achieve more than “double the average score of
    the human players and ranked in the top 10% of participants who played more than
    one game.”⁶ As Meta describes it, their model is able to produce a strategy-grounded
    dialogue and generate a dialogue aware-strategy.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况在2022年发生了变化，当时 Meta 的 AI 团队开发了 Cicero。利用最新的语言建模进展，并结合战略模块，Cicero 是一款能够“超越人类玩家平均得分两倍以上，并排名前10%的多局游戏参与者”的游戏引擎⁶。正如
    Meta 所描述的，他们的模型能够生成以策略为基础的对话，并且能够生成意识到策略的对话。
- en: Differences Between Cicero and Other Strategic AI Models
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Cicero 与其他战略 AI 模型的区别
- en: There are a few key differences between Diplomacy and some of the other games
    where we have had recent strategic AI advancements. Most notably is the cooperative
    nature of the game — compared to the adversarial nature of the other games — and
    the open-ended natural language format it uses. I would argue that these differences
    makes the game more like real human interaction, however it also places restrictions
    on how the researches could train the algorithms that power Cicero.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 《外交》与其他一些近期取得战略 AI 进展的游戏之间存在一些关键区别。最显著的是该游戏的合作性质——与其他游戏的对抗性质相比——以及它使用的开放式自然语言格式。我认为这些差异使得该游戏更像真实的人类互动，但它也对研究人员训练支撑
    Cicero 的算法提出了限制。
- en: 'Unlike Pluribus and AlphaZero, Cicero is not primarily trained through self-play
    and reinforcement learning. Instead, the Meta team used a data set with over 125,000
    games and 40,000,000 messages to help train the algorithm. They thought that given
    the negotiating, persuading and trust-building aspects of the game, they might
    see strange behavior if they let the AI negotiate with itself through self-play,
    and that it might not capture the essence of human interaction. Quoting their
    research article:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Pluribus 和 AlphaZero 不同，Cicero 并非主要通过自我对弈和强化学习进行训练。相反，Meta 团队使用了一个包含超过 125,000
    场游戏和 40,000,000 条消息的数据集来帮助训练算法。他们认为，考虑到游戏中的谈判、说服和建立信任等方面，如果让 AI 通过自我对弈进行谈判，可能会出现奇怪的行为，并且可能无法捕捉到人类互动的本质。引用他们的研究文章：
- en: “…we found that a self-play algorithm that achieved superhuman performance in
    2p0s versions of the game performed poorly in games with multiple human players
    owing to learning a policy inconsistent with the norms and expectations of potential
    human allies.”
  id: totrans-141
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “…我们发现，一个在 2p0s 版本的游戏中表现出超人类表现的自我对弈算法，在与多个真人玩家进行的游戏中表现较差，因为它学到的策略与潜在的人类盟友的规范和期望不一致。”
- en: However, reinforcement learning was used to train part of the strategy engine,
    specifically it was used to train Cicero’s value function — which it needs to
    predict the utility of its actions. The researchers used a modified version of
    behavioral cloning, piKL, which seeks to maximize the expected utility from an
    action and at the same time minimize the divergence from human behavior.⁶ Simply
    put, they wanted the model to be able to find strategically sound actions while
    at the same time staying close to human actions.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，强化学习确实被用来训练部分战略引擎，具体来说，它被用来训练 Cicero 的价值函数——即它预测行为效用所需要的函数。研究人员使用了修改版的行为克隆算法
    piKL，该算法旨在最大化某一行为的预期效用，同时最小化与人类行为的差异⁶。简而言之，他们希望模型能够找到战略上合理的行动，同时尽量贴近人类的行为。
- en: The above features of Diplomacy highlight some important issues related to creating
    a strategic AI that can operate in a real-world human setting, and need to be
    taken into consideration when we evaluate how strategic AI will evolve moving
    forward.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 上述《外交》游戏的特点突显了一些与在真实世界人类环境中操作的战略 AI 创建相关的重要问题，在评估战略 AI 未来发展时需要加以考虑。
- en: The Future of Strategic AI
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 战略 AI 的未来
- en: '![](../Images/ae1fff96e8fa5b3fa4de4d3395c3fabd.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae1fff96e8fa5b3fa4de4d3395c3fabd.png)'
- en: Image generated by the author using Canva Magic Studio
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用 Canva Magic Studio 生成
- en: Predicting the future is always tricky, however, one approach can be to use
    the current trends and extrapolate into future scenarios. Below, we investigate
    a few topics that closely relate to our previous discussion and evaluate how they
    can influence the future of strategic AI.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 预测未来总是充满挑战，但一种方法是利用当前的趋势并将其外推到未来情境。以下，我们探讨一些与之前讨论密切相关的主题，并评估它们如何影响战略 AI 的未来。
- en: '**General Symbolic Strategy Engines vs. Specialized Modules**'
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**通用符号战略引擎与专门化模块**'
- en: If we examine the trajectory of strategic AI engines so far, one thing that
    strikes us is how specialized each game engine is. Even though the architectures
    can be similar — like with AlphaZero learning how to play multiple different games
    — the AI still plays millions of games with itself for each specific game. For
    chess, AlphaZero played 44 million games and for Go 130 million games!⁷ A natural
    question to ask is whether we should try to build more general strategy engines
    or continue to focus on specialized modules for specific tasks?
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们回顾目前战略AI引擎的发展轨迹，一个显著的特点是每个游戏引擎的专门化程度。尽管这些架构可能相似——就像AlphaZero学习如何玩多种不同的游戏——但AI仍然会为每个特定游戏进行数百万次自我对弈。例如，对于国际象棋，AlphaZero进行了4400万局游戏，对于围棋则进行了1.3亿局游戏！⁷
    一个自然的问题是，我们是否应该尝试构建更通用的战略引擎，还是继续专注于为特定任务提供专门化模块？
- en: A general strategy engine would aim to understand and apply broad strategic
    principles across different situations. Perhaps by creating games that capture
    many aspects of human strategic interaction, AI could learn through play against
    itself and develop strategies that apply to real-world scenarios. This approach
    could help AI generalize its learning, making it useful in various contexts.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 一个通用战略引擎的目标是理解并应用跨不同情境的广泛战略原则。也许通过创造能够捕捉人类战略互动各个方面的游戏，AI可以通过与自己对弈来学习，并制定适用于现实世界场景的战略。这种方法可能有助于AI进行学习的泛化，使其在各种情境中都能发挥作用。
- en: On the other hand, specialized modules are AI systems designed for particular
    scenarios or tasks. We could envision that we could create a general strategic
    AI by combining multiple specialized agents. AI agents could be trained to excel
    in each specific area, providing deep expertise where it’s most needed. While
    this method might limit the AI’s ability to generalize, it ensures high performance
    in specific domains, which can lead to practical applications more quickly.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，专门化模块是为特定情境或任务设计的AI系统。我们可以设想，通过结合多个专门化代理，我们能够创建一个通用的战略AI。AI代理可以被训练在每个特定领域中表现卓越，在最需要的地方提供深度专业知识。尽管这种方法可能限制了AI的泛化能力，但它能确保在特定领域内的高性能，从而更快地推动实际应用。
- en: Given the issues with using AI for self-play in cooperative settings — as we
    observed with Diplomacy — and the current trend which seems to favor specialized
    modules for different strategic situations, it seems likely that for the near
    future we will have specialized strategic modules for different contexts. However,
    one could also envision a mixed system where we used general strategy engines
    to provide insights into broader topics, while specialized modules handle complex,
    specific challenges. This balance could allow AI systems to apply general strategic
    insights while adapting to the details of particular situations.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于在合作环境中使用AI进行自我对弈所面临的问题——正如我们在《外交》游戏中所观察到的——以及当前的趋势似乎更倾向于为不同的战略情境提供专门化模块，因此在短期内，我们很可能会拥有针对不同情境的专门化战略模块。然而，人们也可以设想一种混合系统，在这种系统中，我们使用通用战略引擎为更广泛的主题提供洞察，而专门化模块则处理复杂的具体挑战。通过这种平衡，AI系统可以在适应特定情境细节的同时，应用通用战略洞察。
- en: '**LLMs Bridging the Gap Between Strategic Modules and Real-World Applications**'
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**大型语言模型弥合战略模块与现实世界应用之间的差距**'
- en: Large language models have changed how AI interacts with human language, offering
    a powerful way to connect strategic AI modules with real-world use cases. LLMs
    are great at understanding and generating human-like text, making them ideal as
    an intermediary that can translate real-world situations into structured data
    that strategy engines can process. As seen with Meta’s Cicero, combining LLMs
    with strategic reasoning allowed the AI to understand human communication, negotiate,
    and plan actions in collaborative environments.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型改变了AI与人类语言的互动方式，提供了一种强大的方式，将战略AI模块与现实世界应用案例连接起来。LLMs擅长理解和生成类人文本，使它们成为理想的中介，可以将现实世界的情境转化为战略引擎能够处理的结构化数据。正如Meta的Cicero所展示的那样，将LLMs与战略推理结合，允许AI理解人类沟通、进行谈判并在协作环境中制定行动计划。
- en: Given the current trend towards more multimodal models, the LLMs are also increasingly
    able to translate not just text, but any real-world context into a machine readable
    syntax. This makes the models even more useful as intermediaries.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于目前向多模态模型发展的趋势，LLMs（大型语言模型）越来越能够将不仅仅是文本，而是任何现实世界的情境转化为机器可读的语法。这使得这些模型作为中介变得更加有用。
- en: If we build on the ideas developed for Cicero, we could also envision fine-tuning
    different language models for specific tasks — like diplomatic communication —
    perhaps by fine tuning the models on historic diplomatic correspondence and then
    training separate strategy engines to come up with optimal actions.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在Cicero开发的思想基础上继续发展，我们也许可以设想针对特定任务（如外交沟通）微调不同的语言模型——可能通过在历史外交信函上进行微调，然后训练独立的战略引擎来提出最佳行动方案。
- en: '**Human-AI Collaboration: The Centaur Model**'
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**人类-人工智能协作：半人马模型**'
- en: The future of strategic AI isn’t just about machines taking over decision-making;
    for a transition period it’s also about humans and AI working together effectively.
    This partnership is often called the “Centaur Model,” combining human intuition
    with AI’s computing power. In this model, humans bring creativity, ethical judgment,
    and flexibility, while AI systems offer powerful data processing and consistent
    application of strategic principles.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 战略性人工智能的未来不仅仅是机器接管决策过程；在过渡期内，它也涉及人类与人工智能的有效合作。这种合作伙伴关系通常被称为“半人马模型”，即将人类的直觉与人工智能的计算能力相结合。在这种模型中，人类带来创造力、伦理判断和灵活性，而人工智能系统则提供强大的数据处理能力和战略原则的一致应用。
- en: Real-world examples of this model include areas where human-AI teams outperform
    either humans or machines working alone. In chess, for example, Garry Kasparov
    promoted the idea of teaming up with AI, combining human strategic insight with
    AI’s precise calculations. The centaur model seemed to work well in chess until
    the programs started to become really good. At that point the human contribution
    wasn’t worth anything and was in the worst case detrimental.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这一模型的现实世界应用包括那些人类-人工智能团队表现优于单独工作的人类或机器的领域。例如，在国际象棋中，Garry Kasparov 提出了与人工智能合作的想法，将人类的战略洞察力与人工智能的精准计算相结合。半人马模型在国际象棋中似乎运作良好，直到程序变得非常强大。此时，人类的贡献变得毫无价值，最糟糕的情况下甚至可能是有害的。
- en: However, in other areas that are more open-ended and real-world-like than chess,
    the centaur model is probably a good bet going forward. Simply consider how human
    collaboration with modern LLMs has the potential to drastically improve productivity.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在其他更开放、更多元化、且更接近现实世界的领域，半人马模型可能是一个较好的选择。只需考虑人类与现代大型语言模型（LLM）合作的方式，这种合作有潜力显著提升生产力。
- en: This collaborative approach improves decision-making by combining human judgment
    with AI analysis, possibly leading to more informed and balanced outcomes. It
    allows for quick adaptation to new and unexpected situations, as humans can adjust
    strategies in real-time with AI support.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这种协作方法通过将人类判断与人工智能分析相结合，改进了决策过程，可能导致更有依据和平衡的结果。它使得在面对新情况和意外情境时能够迅速适应，因为人类可以在人工智能的支持下实时调整策略。
- en: '**Real-World Applications Beyond Games**'
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**超越游戏的现实世界应用**'
- en: Games have been a great testing ground for developing strategic AI, but the
    real impact comes from applying these advancements to real-world challenges. Below
    we highlight a few examples.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏一直是开发战略性人工智能的一个重要测试平台，但真正的影响来自于将这些进展应用于现实世界的挑战。以下是一些例子。
- en: One field that has seen tremendous development in the last few years is self-driving
    cars, and how they use strategic AI to navigate roads safely. They must predict
    and respond to the actions of other drivers, pedestrians, and cyclists. For example,
    an autonomous vehicle needs to anticipate if a pedestrian is about to cross the
    street or if another driver is about to change lanes unexpectedly.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，一个取得巨大进展的领域是自动驾驶汽车，它们如何利用战略性人工智能安全地导航道路。自动驾驶汽车必须预测并应对其他驾驶员、行人和骑行者的行动。例如，一辆自动驾驶汽车需要预测是否有行人即将过马路，或是其他驾驶员是否即将无预警地变道。
- en: 'Just this year, Waymo — a company that develops autonomous vehicles and ride-hailing
    services — started using fully autonomous taxis in three US cities: Phoenix, Arizona,
    and California’s Los Angeles and San Francisco. In the coming years we can probably
    expect to see a massive rise in fully autonomous vehicles due to the improvements
    in strategic AI.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 就在今年，Waymo——一家开发自动驾驶汽车和共享出行服务的公司——在美国的三个城市启动了完全自动驾驶的出租车服务：亚利桑那州的凤凰城，加利福尼亚州的洛杉矶和旧金山。在未来几年，我们可能会看到由于战略性人工智能的进步，完全自动驾驶车辆的大幅增长。
- en: In the financial markets, AI-driven trading systems analyze enormous amounts
    of data to make investment decisions. These systems consider the likely actions
    of other market participants, such as traders and institutions, to anticipate
    market movements. They use strategic reasoning to execute trades that maximize
    returns while minimizing risks, often in highly volatile environments.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在金融市场中，人工智能驱动的交易系统分析大量数据以做出投资决策。这些系统考虑到其他市场参与者（如交易员和机构）的可能行为，以预测市场波动。它们利用战略推理执行交易，以最大化回报并最小化风险，通常是在高度波动的环境中。
- en: AI systems also optimize supply chains by considering the actions of suppliers,
    competitors, and customers. They can strategically adjust production schedules,
    inventory levels, and logistics based on anticipated demand and competitor behavior.
    For example, if a competitor is expected to launch a new product, the AI can recommend
    increasing stock levels to meet potential increases in demand.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能系统还通过考虑供应商、竞争者和客户的行为来优化供应链。它们可以根据预期需求和竞争者的行为，战略性地调整生产计划、库存水平和物流。例如，如果预计竞争者将推出新产品，人工智能可以建议增加库存量，以应对潜在的需求增加。
- en: Strategic AI is also used to manage energy distribution efficiently. Smart grids
    employ AI to predict consumption patterns and adjust supply accordingly. They
    consider how consumers might change their usage in response to pricing signals
    or environmental factors. The AI strategically allocates resources to balance
    load, prevent outages, and integrate renewable energy sources.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 战略人工智能还被用于有效地管理能源分配。智能电网利用人工智能预测消费模式并相应调整供应。它们考虑消费者在价格信号或环境因素的影响下可能改变使用模式。人工智能战略性地分配资源，以平衡负载、预防停电并整合可再生能源。
- en: The examples above clearly show how strategic AI is being integrated into various
    industries and fields. By considering the actions of others, these AI systems
    make informed decisions that optimize outcomes, enhance efficiency, and often
    provide a competitive advantage. As strategic AI continues to improve so will
    these systems, and we will likely see their emergence in many other domains as
    well.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 以上例子清楚地展示了战略人工智能如何被整合到各个行业和领域中。通过考虑他人的行动，这些人工智能系统做出明智的决策，从而优化结果、提高效率，并常常提供竞争优势。随着战略人工智能的不断改进，这些系统也将不断进步，我们很可能会在许多其他领域看到它们的出现。
- en: Conclusion
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: '![](../Images/2e1d22b968c8f0fd29f072e36c9b70a6.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e1d22b968c8f0fd29f072e36c9b70a6.png)'
- en: Image generated by the author using Canva Magic Studio
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者使用Canva Magic Studio生成的图像
- en: Strategic AI has come a long way since Deep Blue’s victory over Garry Kasparov.
    From mastering complex board games to engaging in human-like negotiations, AI
    systems are increasingly exhibiting strategic reasoning abilities.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 自从深蓝战胜加里·卡斯帕罗夫以来，战略人工智能已经取得了长足进展。从掌握复杂的棋类游戏到进行类人化的谈判，人工智能系统越来越多地展现出战略推理能力。
- en: In this article we investigated the foundational concepts of strategic AI, emphasizing
    the importance of game theory and how some of the concepts from the field can
    be applied to strategic AI. We also looked at how specialized AI systems have
    achieved superhuman performance in specific games by focusing on narrow domains
    and extensive self-play. This raises the question of whether the future of strategic
    AI lies in developing general symbolic strategy engines capable of broader application
    or continuing with specialized modules tailored to specific tasks.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们研究了战略人工智能的基础概念，强调了博弈论的重要性，以及该领域的一些概念如何应用于战略人工智能。我们还探讨了专门化的人工智能系统如何通过专注于狭窄领域和广泛的自我对弈，在特定游戏中实现超人类的表现。这引发了一个问题：战略人工智能的未来是发展能够广泛应用的通用符号战略引擎，还是继续开发针对特定任务量身定制的专门模块。
- en: As we saw with Cicero, language models will also likely have a future in the
    space of strategic AI. The new models from providers like OpenAI, Anthropic and
    Meta make it easier than ever before to integrate these tools into autonomous
    agents that can use them to translate the real-world into structured data that
    AI systems can process.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在Cicero中看到的那样，语言模型也可能在战略人工智能领域有一席之地。来自OpenAI、Anthropic和Meta等提供商的新模型使得将这些工具集成到自主智能体中比以往任何时候都更加容易，这些智能体能够利用它们将现实世界转化为人工智能系统可以处理的结构化数据。
- en: However, the journey toward a general-purpose strategic AI that can navigate
    the complexities of the real world is just beginning. Challenges remain in developing
    systems that can generalize across domains, adapt to unforeseen situations, and
    integrate ethical considerations into their decision-making processes.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，朝着能够应对现实世界复杂性的通用战略AI的旅程才刚刚开始。开发能够跨领域泛化、适应不可预见情况，并将伦理考量融入决策过程的系统仍然面临挑战。
- en: Thanks for reading!
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: '**Want to be notified whenever I publish a new article? ➡️** [**Subscribe to
    my newsletter here**](https://ekneconsulting.com/register_subscriber/) **⬅️. It’s
    free & you can unsubscribe at any time!**'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**想要在我发布新文章时收到通知吗？ ➡️** [**点击这里订阅我的新闻通讯**](https://ekneconsulting.com/register_subscriber/)
    **⬅️。完全免费，您随时可以取消订阅！**'
- en: '*If you enjoyed reading this article and would like to access more content
    from me please feel free to connect with me on LinkedIn at* [*https://www.linkedin.com/in/hans-christian-ekne-1760a259/*](https://www.linkedin.com/in/hans-christian-ekne-1760a259/)
    *or visit my webpage at* [*https://www.ekneconsulting.com/*](https://www.ekneconsulting.com/)
    *to explore some of the services I offer. Don’t hesitate to reach out via email
    at hce@ekneconsulting.com*'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果您喜欢阅读这篇文章，并希望获取更多我的内容，欢迎在LinkedIn上与我联系* [*https://www.linkedin.com/in/hans-christian-ekne-1760a259/*](https://www.linkedin.com/in/hans-christian-ekne-1760a259/)
    *，或者访问我的网站* [*https://www.ekneconsulting.com/*](https://www.ekneconsulting.com/)
    *，了解我提供的部分服务。也欢迎通过电子邮件联系我，邮箱地址是hce@ekneconsulting.com*'
- en: References
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Lex Fridman. (2019, October 27). Garry Kasparov: Chess, Deep Blue, AI, and
    Putin | Lex Fridman Podcast #46 [Video File]. Youtube. [https://youtu.be/8RVa0THWUWw?si=1ErCnwlAn4myoK9W](https://youtu.be/8RVa0THWUWw?si=1ErCnwlAn4myoK9W)'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Lex Fridman. (2019年10月27日). Garry Kasparov: Chess, Deep Blue, AI, and Putin
    | Lex Fridman Podcast #46 [视频文件]. Youtube. [https://youtu.be/8RVa0THWUWw?si=1ErCnwlAn4myoK9W](https://youtu.be/8RVa0THWUWw?si=1ErCnwlAn4myoK9W)'
- en: Vinyals, O., Babuschkin, I., Czarnecki, W.M. *et al.* Grandmaster level in StarCraft
    II using multi-agent reinforcement learning. *Nature* 575, 350–354 (2019). [https://doi.org/10.1038/s41586-019-1724-z](https://doi.org/10.1038/s41586-019-1724-z)
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Vinyals, O., Babuschkin, I., Czarnecki, W.M. *等人.* 使用多智能体强化学习在《星际争霸II》中达到大师级水平。*Nature*
    575, 350–354 (2019). [https://doi.org/10.1038/s41586-019-1724-z](https://doi.org/10.1038/s41586-019-1724-z)
- en: '[https://deepmind.google/discover/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii/](https://deepmind.google/discover/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii/)'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://deepmind.google/discover/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii/](https://deepmind.google/discover/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii/)'
- en: Brown et al. (2019, August 30). Superhuman AI for multiplayer poker. *Science
    365, 885–890, (2019).* [https://www.science.org/doi/epdf/10.1126/science.aay2400](https://www.science.org/doi/epdf/10.1126/science.aay2400)
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Brown等人. (2019年8月30日). 超人类AI在多人扑克中的应用。*Science 365, 885–890, (2019).* [https://www.science.org/doi/epdf/10.1126/science.aay2400](https://www.science.org/doi/epdf/10.1126/science.aay2400)
- en: 'Lex Fridman. (2022, December 6). Noam Brown: AI vs Humans in Poker and Games
    of Strategic Negotiation | Lex Fridman Podcast #344 [Video File]. Youtube. [https://youtu.be/2oHH4aClJQs?si=AvE_Esb42GNGIPRG](https://youtu.be/2oHH4aClJQs?si=AvE_Esb42GNGIPRG)'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Lex Fridman. (2022年12月6日). Noam Brown: AI与人类在扑克和战略谈判游戏中的对抗 | Lex Fridman Podcast
    #344 [视频文件]. Youtube. [https://youtu.be/2oHH4aClJQs?si=AvE_Esb42GNGIPRG](https://youtu.be/2oHH4aClJQs?si=AvE_Esb42GNGIPRG)'
- en: '[Meta Fundamental AI Research Diplomacy Team (FAIR)†](https://www.science.org/action/doSearch?ContribAuthorRaw=Meta+Fundamental+AI+Research+Diplomacy+Team+FAIR)
    *et al.,* Human-level play in the game of *Diplomacy* by combining language models
    with strategic reasoning.*Science***378**,1067 1074(2022).DOI:[10.1126/science.ade9097](https://doi.org/10.1126/science.ade9097),
    [https://noambrown.github.io/papers/22-Science-Diplomacy-TR.pdf](https://noambrown.github.io/papers/22-Science-Diplomacy-TR.pdf)'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Meta Fundamental AI Research Diplomacy Team (FAIR)†](https://www.science.org/action/doSearch?ContribAuthorRaw=Meta+Fundamental+AI+Research+Diplomacy+Team+FAIR)
    *等人,* 通过结合语言模型与战略推理，在游戏*外交*中实现人类级别的玩法。*Science* ***378**,1067 1074(2022).DOI：[10.1126/science.ade9097](https://doi.org/10.1126/science.ade9097)，[https://noambrown.github.io/papers/22-Science-Diplomacy-TR.pdf](https://noambrown.github.io/papers/22-Science-Diplomacy-TR.pdf)'
- en: David Silver *et al. ,* A general reinforcement learning algorithm that masters
    chess, shogi, and Go through self-play.*Science***362**,1140–1144(2018).DOI:[10.1126/science.aar6404](https://doi.org/10.1126/science.aar6404)
    [https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphazero-shedding-new-light-on-chess-shogi-and-go/alphazero_preprint.pdf](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphazero-shedding-new-light-on-chess-shogi-and-go/alphazero_preprint.pdf)
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: David Silver *等人，* 一种通用强化学习算法，通过自我对弈掌握国际象棋、将棋和围棋。*科学杂志* **362**，1140–1144（2018）。DOI：[10.1126/science.aar6404](https://doi.org/10.1126/science.aar6404)
    [https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphazero-shedding-new-light-on-chess-shogi-and-go/alphazero_preprint.pdf](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphazero-shedding-new-light-on-chess-shogi-and-go/alphazero_preprint.pdf)
