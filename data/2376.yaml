- en: Is Less More? Do Deep Learning Forecasting Models Need Feature Reduction?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 少即是多？深度学习预测模型需要特征减少吗？
- en: 原文：[https://towardsdatascience.com/is-less-more-do-deep-learning-forecasting-models-need-feature-reduction-25d8968ac15c?source=collection_archive---------7-----------------------#2024-09-30](https://towardsdatascience.com/is-less-more-do-deep-learning-forecasting-models-need-feature-reduction-25d8968ac15c?source=collection_archive---------7-----------------------#2024-09-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/is-less-more-do-deep-learning-forecasting-models-need-feature-reduction-25d8968ac15c?source=collection_archive---------7-----------------------#2024-09-30](https://towardsdatascience.com/is-less-more-do-deep-learning-forecasting-models-need-feature-reduction-25d8968ac15c?source=collection_archive---------7-----------------------#2024-09-30)
- en: To curate, or not to curate, that is the question
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 是否需要筛选特征，这是一个问题
- en: '[](https://ostiguyphilippe.medium.com/?source=post_page---byline--25d8968ac15c--------------------------------)[![Philippe
    Ostiguy, M. Sc.](../Images/8b292bc1baa848a0c5de821dc9576534.png)](https://ostiguyphilippe.medium.com/?source=post_page---byline--25d8968ac15c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--25d8968ac15c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--25d8968ac15c--------------------------------)
    [Philippe Ostiguy, M. Sc.](https://ostiguyphilippe.medium.com/?source=post_page---byline--25d8968ac15c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ostiguyphilippe.medium.com/?source=post_page---byline--25d8968ac15c--------------------------------)[![Philippe
    Ostiguy, M. Sc.](../Images/8b292bc1baa848a0c5de821dc9576534.png)](https://ostiguyphilippe.medium.com/?source=post_page---byline--25d8968ac15c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--25d8968ac15c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--25d8968ac15c--------------------------------)
    [Philippe Ostiguy, M. Sc.](https://ostiguyphilippe.medium.com/?source=post_page---byline--25d8968ac15c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--25d8968ac15c--------------------------------)
    ·12 min read·Sep 30, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--25d8968ac15c--------------------------------)
    ·12 分钟阅读·2024年9月30日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d06437fd932d3128b235a26000f217c7.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d06437fd932d3128b235a26000f217c7.png)'
- en: AI image created on MidJourney V6.1 by the author.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: AI 图像由作者在 MidJourney V6.1 上创建。
- en: 'Time series forecasting is a powerful tool in data science, offering insights
    into future trends based on historical patterns. In our previous article, we explored
    [how making your time series stationary automatically](https://levelup.gitconnected.com/want-to-decrease-your-models-prediction-errors-by-20-follow-this-simple-trick-97354102098e)
    can significantly enhance model performance. But stationarity is just one piece
    of the puzzle. As we continue to refine our forecasting models, another crucial
    question arises: how do we handle the multitude of features our data may present?'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列预测是数据科学中的一项强大工具，通过历史模式提供未来趋势的洞察。在我们之前的文章中，我们探讨了 [如何使你的时间序列自动平稳化](https://levelup.gitconnected.com/want-to-decrease-your-models-prediction-errors-by-20-follow-this-simple-trick-97354102098e)，这能显著提高模型性能。但平稳性只是解决方案的一部分。随着我们继续优化预测模型，另一个关键问题出现了：如何处理数据可能呈现的众多特征？
- en: As you work with time series data, you’ll often find yourself with many potential
    features to include in your model. While it’s tempting to use all available data,
    adding more features isn’t always better. It can make your model more complex
    and slower to train, without necessarily improving its performance.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当你处理时间序列数据时，你经常会发现自己面临着许多可能的特征，需要将其包含在模型中。虽然使用所有可用数据很有诱惑力，但添加更多特征并不总是更好的选择。这可能会使你的模型变得更加复杂，训练速度变慢，而不一定会提高模型的性能。
- en: 'You might be wondering: is it important to simplify the feature set and what
    are the techniques available out there? That’s exactly what we’ll discuss in this
    article.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能在想：简化特征集是否重要，现有的技术有哪些？这正是我们将在本文中讨论的内容。
- en: 'Here’s a quick summary of what will be covered:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们将要讨论的内容的简要总结：
- en: '**Feature reduction in time series** — *We’ll explain the concept of feature
    reduction in time series analysis and why it matters.*'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间序列中的特征减少** — *我们将解释时间序列分析中特征减少的概念以及它的重要性。*'
- en: '**Practical implementation guide** — *Using Python, we’ll walk through evaluating
    and selecting features for your time series model, providing hands-on tools to
    optimize your approach. We’ll also assess whether trimming down features is necessary
    for our forecasting models.*'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实际实施指南** — *通过使用Python，我们将逐步讲解如何评估和选择时间序列模型的特征，提供优化你方法的实用工具。我们还将评估是否需要削减特征以优化我们的预测模型。*'
- en: Once you’re familiar with techniques like stationarity and feature reduction
    as explained in this article, and you’re looking to elevate your models even further?
    Check out this article on [using a custom validation loss](https://medium.com/@ostiguyphilippe/enhancing-deep-learning-model-evaluation-for-stock-market-forecasting-bea30b905b80)
    in your deep learning model to get better stock forecasts — it’s a great next
    step!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你熟悉了像平稳性和特征减少这样的技术，并且想进一步提升你的模型？可以查看这篇关于[在深度学习模型中使用自定义验证损失](https://medium.com/@ostiguyphilippe/enhancing-deep-learning-model-evaluation-for-stock-market-forecasting-bea30b905b80)的文章，以获得更好的股票预测——这是一个很好的下一步！
- en: 'Feature reduction in time series : a simple explanation'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列中的特征减少：一个简单的解释
- en: Feature reduction is like cleaning up your workspace to make it easier to find
    what you need. In time series analysis, it means cutting down the number of input
    variables (features) that your model uses to make predictions. The goal is to
    simplify the model while retaining its predictive power. This is important because
    too many and correlated features can make your model complicated, slow, and less
    accurate.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 特征减少就像是整理工作区，使得你更容易找到所需的东西。在时间序列分析中，这意味着减少模型用于预测的输入变量（特征）的数量。目标是简化模型，同时保留其预测能力。这一点非常重要，因为过多的特征和相关特征会使模型变得复杂、缓慢并降低准确性。
- en: 'Specifically, simplifying the feature set can :'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，简化特征集可以：
- en: '**Reduced complexity:** Fewer features means a simpler model, which is often
    faster to train and use.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降低复杂度**：特征较少意味着模型更简单，这通常使得训练和使用更快。'
- en: '**Improved generalization**: By removing noise, eliminating correlated features
    and focusing on key information, it helps the model to learn the true underlying
    patterns rather than memorizing redundant information. This enhances the model’s
    ability to generalize its predictions to different datasets.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提高泛化能力**：通过去除噪声、消除相关特征并聚焦于关键信息，它帮助模型学习真实的潜在模式，而不是记住冗余信息。这增强了模型将预测推广到不同数据集的能力。'
- en: '**Easier interpretation**: A model with fewer features is often easier for
    humans to understand and explain.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更易解释**：特征较少的模型通常更容易让人类理解和解释。'
- en: '**Computational efficiency**: Fewer features requires less memory and processing
    power, which can be crucial for large datasets or real-time applications.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算效率**：较少的特征需要更少的内存和处理能力，这对于大数据集或实时应用至关重要。'
- en: It’s also important to note that most time series packages in Python for forecasting
    don’t perform feature reduction automatically. This is a step you typically need
    to handle on your own before using these packages.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要注意的是，大多数用于时间序列预测的Python包并不会自动进行特征减少。这是你通常需要在使用这些包之前自行处理的一个步骤。
- en: 'To better understand these concepts, let’s walk through a practical example
    using real-world daily data from the Federal Reserve Economic Data (FRED) database.
    We’ll skip the data retrieval process here, as we’ve already covered how to get
    free and reliable data from the FRED API in a [previous article](https://levelup.gitconnected.com/get-free-and-reliable-financial-market-data-machine-learning-ready-246e59b00cea).
    You can get the data we’ll be using [this script](https://github.com/philippe-ostiguy/free-fin-data).
    Once you’ve fetched the data :'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这些概念，让我们通过一个实际的例子来使用来自美国联邦储备经济数据（FRED）数据库的真实世界日数据。我们在这里跳过数据获取的过程，因为我们已经在[上一篇文章](https://levelup.gitconnected.com/get-free-and-reliable-financial-market-data-machine-learning-ready-246e59b00cea)中详细介绍了如何通过FRED
    API获取免费的可靠数据。你可以通过[这个脚本](https://github.com/philippe-ostiguy/free-fin-data)获取我们将使用的数据。一旦你获取了数据：
- en: Create a`data` directory in your current directory
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在当前目录中创建一个`data`目录
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Copy the data in your directory
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据复制到你的目录中
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now that we have our data, let’s dive into our feature reduction example.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了数据，让我们深入探讨特征减少的示例。
- en: We’ve previously demonstrated how to clean the daily data fetched from the FRED
    API in [another article](https://levelup.gitconnected.com/get-free-and-reliable-financial-market-data-machine-learning-ready-246e59b00cea),
    so we’ll skip that process here and use the `processed_dataframes`(list of dataframes)
    that resulted from the steps outlined in that article.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前在[另一篇文章](https://levelup.gitconnected.com/get-free-and-reliable-financial-market-data-machine-learning-ready-246e59b00cea)中展示了如何清理从FRED
    API获取的每日数据，因此我们在这里跳过该过程，直接使用文章中步骤得到的`processed_dataframes`（数据框列表）。
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You might be wondering why we’ve divided our data into training and testing
    sets? The reason is to ensure that there is no data leakage before applying any
    transformation or reduction technique.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问，为什么我们将数据划分为训练集和测试集？原因是为了确保在应用任何转换或降维技术之前不会发生数据泄漏。
- en: '`initial_model_data` contains the S&P 500 daily prices (stored originally `processed_dataframes`),
    which will be the data we’ll be trying to forecast.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`initial_model_data`包含标准普尔500的每日价格（最初存储在`processed_dataframes`中），这将是我们尝试预测的数据。'
- en: Then, we need to ensure our data is stationary. For a detailed explanation on
    how to automatically make your data stationary and improve your model by 20%,
    refer to [this article](https://levelup.gitconnected.com/want-to-decrease-your-models-prediction-errors-by-20-follow-this-simple-trick-97354102098e).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要确保我们的数据是平稳的。关于如何自动使数据平稳并提高20%模型表现的详细解释，请参阅[这篇文章](https://levelup.gitconnected.com/want-to-decrease-your-models-prediction-errors-by-20-follow-this-simple-trick-97354102098e)。
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Then, we will count the number of variables that have at least a 95% correlation
    coefficient with another variable.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将统计至少与另一个变量有95%相关系数的变量数量。
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/3e88d562d5f155546e490822fecc2ca8.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e88d562d5f155546e490822fecc2ca8.png)'
- en: Number of variables highly correlated. Image by the author.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 高度相关的变量数量。图像由作者提供。
- en: 'Having 260 out of 438 variables that have a correlation of 95% or more with
    at least another variable can be an issue. It indicates significant multicollinearity
    in the dataset. This redundancy can lead to several issues:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果438个变量中有260个变量与至少另一个变量的相关性达到95%或更高，这可能是一个问题。这表明数据集中存在显著的多重共线性。这种冗余可能会导致以下几个问题：
- en: It complicates the model without adding substantial new information
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使模型变得复杂，但并未增加实质性的新信息
- en: Potentially causes instability in coefficient estimates
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能导致回归系数估计的不稳定
- en: Increases the risk of overfitting
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加过拟合的风险
- en: Makes interpretation of individual variable impacts challenging
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使得解释单个变量的影响变得困难
- en: Feature evaluation and selection
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征评估与选择
- en: We understand that feature reduction can be important, but how do we perform
    it? Which techniques should we use? These are the questions we’ll explore now.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们理解特征降维可能很重要，但我们该如何进行呢？我们应该使用哪些技术？这些是我们现在要探讨的问题。
- en: The first technique we’ll examine is Principal Component Analysis (PCA). It’s
    a common and an effective dimensionality reduction technique. PCA identifies linear
    relationships between features and retains the principal components that explain
    a predetermined percentage of the variance in the original dataset. In our use
    case, we’ve set the `EXPLAINED_VARIANCE` threshold to 90%.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要探讨的第一种技术是主成分分析（PCA）。这是一种常见且有效的降维技术。PCA识别特征之间的线性关系，并保留解释原始数据集中预定百分比方差的主成分。在我们的使用案例中，我们将`EXPLAINED_VARIANCE`阈值设置为90%。
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/5544416f6efe78886454deea283a782d.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5544416f6efe78886454deea283a782d.png)'
- en: Feature reduction using Principal Component Analysis. Image by the author.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 使用主成分分析进行特征降维。图像由作者提供。
- en: 'It’s an impressive: only 76 components out of 438 features remaining after
    the reduction while keeping 90% of the variance explained! Now let’s move to a
    non-linear reduction technique.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 令人印象深刻的是：在降维后，438个特征中只剩下76个成分，同时保持了90%的方差解释！接下来我们将转向一种非线性降维技术。
- en: The [Temporal Fusion Transformers (TFT)](https://arxiv.org/pdf/1912.09363.pdf)
    is an advanced model for time series forecasting. It includes the Variable Selection
    Network (VSN), which is a key component of the model. It’s specifically designed
    to automatically identify and focus on the most relevant features within a dataset.
    It achieves this by assigning learned weights to each input variable, effectively
    highlighting which features contribute most to the predictive task.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[时间序列融合变换器（TFT）](https://arxiv.org/pdf/1912.09363.pdf)是一种用于时间序列预测的先进模型。它包含变量选择网络（VSN），这是模型的一个关键组件。它专门设计用于自动识别和关注数据集中的最相关特征。通过为每个输入变量分配学习到的权重，它有效地突出哪些特征对预测任务的贡献最大。'
- en: This VSN-based approach will be our second reduction technique. We’ll implement
    it using [PyTorch Forecasting](https://pytorch-forecasting.readthedocs.io/en/stable/),
    which allows us to leverage the Variable Selection Network from the TFT model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 基于VSN的方法将是我们第二种降维技术。我们将使用[PyTorch Forecasting](https://pytorch-forecasting.readthedocs.io/en/stable/)来实现，它允许我们利用TFT模型中的变量选择网络。
- en: We’ll use a basic configuration. Our goal isn’t to create the highest-performing
    model possible, but rather to identify the most relevant features while using
    minimal resources.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个基础配置。我们的目标不是创建性能最强的模型，而是识别最相关的特征，同时尽量减少资源使用。
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `VARIABLES_IMPORTANCE` threshold is set to 0.8, which means we'll retain
    features in the top 80th percentile of importance as determined by the Variable
    Selection Network (VSN). For more information about the Temporal Fusion Transformers
    (TFT) and its parameters, please refer to the [documentation](https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.models.temporal_fusion_transformer.TemporalFusionTransformer.html#pytorch_forecasting.models.temporal_fusion_transformer.TemporalFusionTransformer).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`VARIABLES_IMPORTANCE`阈值设置为0.8，这意味着我们将保留由变量选择网络（VSN）确定的排名前80%的重要特征。有关时间序列融合变换器（TFT）及其参数的更多信息，请参考[文档](https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.models.temporal_fusion_transformer.TemporalFusionTransformer.html#pytorch_forecasting.models.temporal_fusion_transformer.TemporalFusionTransformer)。'
- en: Next, we’ll train the TFT model.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将训练TFT模型。
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We intentionally set `max_epochs=20` so the model doesn’t train too long. Additionally,
    we implemented an `early_stop_callback` that halts training if the model shows
    no improvement for 5 consecutive epochs (`patience=5`).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们故意设置了`max_epochs=20`，以避免模型训练时间过长。此外，我们还实现了`early_stop_callback`，如果模型在连续5个epoch中没有改善，将停止训练（`patience=5`）。
- en: Finally, using the best model obtained, we select the 80th percentile of the
    most important features as determined by the VSN.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用获得的最佳模型，我们选择由VSN确定的最重要特征的80百分位。
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/752a7a4f43f1726c1de6b2236a68e5c7.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/752a7a4f43f1726c1de6b2236a68e5c7.png)'
- en: Feature reduction using Variable Selection Network. Image by the author.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用变量选择网络进行特征降维。图片由作者提供。
- en: 'The original dataset contained 438 features, which were then reduced to 1 feature
    only after applying the VSN method! This drastic reduction suggests several possibilities:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据集包含438个特征，在应用VSN方法后，仅剩下1个特征！这种剧烈的降维暗示了几种可能性：
- en: Many of the original features may have been redundant.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 许多原始特征可能是冗余的。
- en: The feature selection process may have oversimplified the data.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征选择过程可能已过度简化数据。
- en: Using only the target variable’s historical values (autoregressive approach)
    might perform as well as, or possibly better than, models incorporating exogenous
    variables.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅使用目标变量的历史值（自回归方法）可能与包含外生变量的模型表现相当，甚至可能更好。
- en: Evaluating feature reduction techniques
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估特征降维技术
- en: In this final section, we compare out reduction techniques applied to our model.
    Each method is tested while maintaining identical model configurations, varying
    only the features subjected to reduction.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一节中，我们将比较应用于模型的降维技术。每种方法在保持相同模型配置的同时进行测试，唯一区别是所选特征的降维处理。
- en: We’ll use [TiDE](https://arxiv.org/pdf/2304.08424.pdf), a small state-of-the-art
    Transformer-based model. We’ll use the implementation provided by [NeuralForecast](https://nixtlaverse.nixtla.io/neuralforecast/models.tide.html).
    Any model from NeuralForecast [here](https://nixtlaverse.nixtla.io/neuralforecast/docs/capabilities/overview.html)
    would work as long as it allows exogenous historical variables.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用[TiDE](https://arxiv.org/pdf/2304.08424.pdf)，这是一种基于Transformer的小型最先进模型。我们将使用[NeuralForecast](https://nixtlaverse.nixtla.io/neuralforecast/models.tide.html)提供的实现。只要它允许外生历史变量，NeuralForecast的任何模型[这里](https://nixtlaverse.nixtla.io/neuralforecast/docs/capabilities/overview.html)都可以使用。
- en: 'We’ll train and test two models using daily SPY (S&P 500 ETF) data. Both models
    will have the same:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用每日SPY（标普500ETF）数据训练和测试两个模型。两个模型将具有相同的：
- en: Train-test split ratio
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练-测试分割比例
- en: Hyperparameters
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 超参数
- en: Single time series (SPY)
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单一时间序列（SPY）
- en: Forecasting horizon of 1 step ahead
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测范围为一步 ahead
- en: The only difference between the models will be the feature reduction technique.
    That’s it!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个模型之间唯一的区别是特征减少技术。就这样！
- en: 'First model: Original features (no feature reduction)'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个模型：原始特征（没有特征减少）
- en: 'Second model: Feature reduction using PCA'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二个模型：使用PCA进行特征减少
- en: 'Third model: Feature reduction using VSN'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第三个模型：使用VSN进行特征减少
- en: This setup allows us to isolate the impact of each feature reduction technique
    on model performance.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 该设置使我们能够隔离每种特征减少技术对模型性能的影响。
- en: First we train the 3 models with the same configuration except for the features.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用相同的配置训练三个模型，唯一的区别是特征。
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Then, we make the predictions.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们进行预测。
- en: '[PRE10]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We forecast the daily returns using the model, then convert these back to prices.
    This approach allows us to calculate prediction errors using prices and compare
    the actual prices to the forecasted prices in a plot.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用模型预测每日回报，然后将这些回报转换为价格。这种方法使我们能够通过价格计算预测误差，并将实际价格与预测价格进行比较，呈现在图表中。
- en: '![](../Images/269abd534c5abdfe8194df39136fd090.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/269abd534c5abdfe8194df39136fd090.png)'
- en: Comparison of prediction errors with different feature reduction techniques.
    Image by the author.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 使用不同特征减少技术的预测误差比较。图片由作者提供。
- en: 'The similar performance of the TiDE model across both original and reduced
    feature sets reveals a crucial insight: feature reduction did not lead to improved
    predictions as one might expect. This suggests potential key issues:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: TiDE模型在原始和减少特征集上表现相似，揭示了一个关键的见解：特征减少并没有像预期那样提高预测精度。这表明可能存在一些关键问题：
- en: 'Information loss: despite aiming to preserve essential data, dimensionality
    reduction techniques discarded information relevant to the prediction task, explaining
    the lack of improvement with fewer features.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信息损失：尽管目的是保留重要数据，但降维技术丢弃了与预测任务相关的信息，这解释了在减少特征时预测没有改进。
- en: 'Generalization struggles: consistent performance across feature sets indicates
    the model’s difficulty in capturing underlying patterns, regardless of feature
    count.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 泛化困难：不同特征集之间的一致表现表明模型在捕捉潜在模式方面存在困难，无论特征数量多少。
- en: 'Complexity overkill: similar results with fewer features suggest TiDE’s sophisticated
    architecture may be unnecessarily complex. A simpler model, like ARIMA, could
    potentially perform just as well.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复杂性过度：使用较少特征获得类似结果表明，TiDE的复杂架构可能过于复杂。一种更简单的模型，例如ARIMA，可能表现得同样好。
- en: Then, let’s examine the chart to see if we can observe any significant differences
    among the three forecasting methods and the actual prices.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，让我们查看图表，看看是否能观察到三种预测方法与实际价格之间有任何显著差异。
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/198959fd7771ea78751d923086b0820d.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/198959fd7771ea78751d923086b0820d.png)'
- en: SPY price forecast using all original features. Image created by the author.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 使用所有原始特征的SPY价格预测。图片由作者提供。
- en: '![](../Images/12c100de5f3352051765fbd2db5be6e0.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12c100de5f3352051765fbd2db5be6e0.png)'
- en: SPY price forecast using PCA. Image created by the author.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PCA的SPY价格预测。图片由作者提供。
- en: '![](../Images/ba26bb6bbb1727805ee38cd8788db4cd.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba26bb6bbb1727805ee38cd8788db4cd.png)'
- en: SPY price forecast using VSN. Image created by the author.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 使用VSN的SPY价格预测。图片由作者提供。
- en: The difference between true and predicted prices appears consistent across all
    three models, with no noticeable variation in performance between them.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 真实价格与预测价格之间的差异在三个模型中看起来一致，性能之间没有明显的差异。
- en: Conclusion
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'We did it! We explored the importance of feature reduction in time series analysis
    and provided a practical implementation guide:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做到了！我们探讨了特征减少在时间序列分析中的重要性，并提供了一个实用的实现指南：
- en: Feature reduction aims to simplify models while maintaining predictive power.
    Benefits include reduced complexity, improved generalization, easier interpretation,
    and computational efficiency.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征降维的目标是简化模型，同时保持预测能力。其好处包括减少复杂性、提高泛化能力、简化解释和计算效率。
- en: 'We demonstrated two reduction techniques using FRED data:'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用FRED数据演示了两种降维技术：
- en: Principal Component Analysis (PCA), a linear dimensionality reduction method,
    reduced features from 438 to 76 while retaining 90% of explained variance.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主成分分析（PCA），一种线性降维方法，将特征从438个减少到76个，同时保留了90%的解释方差。
- en: Variable Selection Network (VSN) from the Temporal Fusion Transformers, a non-linear
    approach, drastically reduced features to just 1 using an 80th percentile importance
    threshold.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 来自时序融合变换器（Temporal Fusion Transformers）的变量选择网络（VSN），一种非线性方法，通过设置80百分位重要性阈值，将特征大幅减少到仅剩1个。
- en: Evaluation using TiDE models showed similar performance across original and
    reduced feature sets, suggesting feature reduction may not always improve forecasting
    performance. This could be due to information loss during reduction, the model’s
    difficulty in capturing underlying patterns, or the possibility that a simpler
    model might be equally effective for this particular forecasting task.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TiDE模型进行的评估表明，原始特征集和降维后的特征集在性能上相似，这表明特征降维并不总是能提升预测性能。这可能是由于降维过程中信息丢失、模型难以捕捉潜在模式，或可能是对于这个特定的预测任务，更简单的模型同样有效。
- en: On a final note, we didn’t explore all feature reduction techniques, such as
    SHAP (SHapley Additive exPlanations), which provides a unified measure of feature
    importance across various model types. Even if we didn’t improve our model, it’s
    still better to perform feature curation and compare performance across different
    reduction methods. This approach helps ensure you’re not discarding valuable information
    while optimizing your model’s efficiency and interpretability.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一点，我们并没有探索所有的特征降维技术，例如SHAP（SHapley Additive exPlanations），它提供了跨多种模型类型的统一特征重要性度量。即使我们没有改进模型，进行特征筛选并比较不同降维方法的性能仍然是值得的。这种方法有助于确保在优化模型效率和可解释性的同时，不丢失有价值的信息。
- en: In future articles, we’ll apply these feature reduction techniques to more complex
    models, comparing their impact on performance and interpretability. Stay tuned!
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来的文章中，我们将把这些特征降维技术应用于更复杂的模型，比较它们对性能和可解释性的影响。敬请关注！
- en: Ready to put these concepts into action? You can find the complete code implementation
    [here](https://github.com/philippe-ostiguy/feature-reduction-ts).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 准备将这些概念付诸实践了吗？你可以在[这里](https://github.com/philippe-ostiguy/feature-reduction-ts)找到完整的代码实现。
- en: Liked this article? Show your support!
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 喜欢这篇文章吗？表示你的支持！
- en: 👏 Clap it up to 50 times
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 👏 鼓掌50次
- en: 🤝 Send me a [LinkedIn](https://www.linkedin.com/in/philippe-ostiguy/) connection
    request to stay in touch
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 🤝 通过[LinkedIn](https://www.linkedin.com/in/philippe-ostiguy/)向我发送连接请求，保持联系
- en: '*Your support means everything!* 🙏'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*你的支持意义重大！* 🙏'
