- en: Large Language Models, GPT-1 — Generative Pre-Trained Transformer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型，GPT-1 — 生成式预训练Transformer
- en: 原文：[https://towardsdatascience.com/large-language-models-gpt-1-generative-pre-trained-transformer-7b895f296d3b?source=collection_archive---------0-----------------------#2024-01-27](https://towardsdatascience.com/large-language-models-gpt-1-generative-pre-trained-transformer-7b895f296d3b?source=collection_archive---------0-----------------------#2024-01-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/large-language-models-gpt-1-generative-pre-trained-transformer-7b895f296d3b?source=collection_archive---------0-----------------------#2024-01-27](https://towardsdatascience.com/large-language-models-gpt-1-generative-pre-trained-transformer-7b895f296d3b?source=collection_archive---------0-----------------------#2024-01-27)
- en: Diving deeply into the working structure of the first version of gigantic GPT-models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入探讨第一个版本的巨大GPT模型的工作结构
- en: '[](https://medium.com/@slavahead?source=post_page---byline--7b895f296d3b--------------------------------)[![Vyacheslav
    Efimov](../Images/441e600862b2b93564c6cd81abb0092d.png)](https://medium.com/@slavahead?source=post_page---byline--7b895f296d3b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7b895f296d3b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7b895f296d3b--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page---byline--7b895f296d3b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@slavahead?source=post_page---byline--7b895f296d3b--------------------------------)[![Vyacheslav
    Efimov](../Images/441e600862b2b93564c6cd81abb0092d.png)](https://medium.com/@slavahead?source=post_page---byline--7b895f296d3b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7b895f296d3b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7b895f296d3b--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page---byline--7b895f296d3b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7b895f296d3b--------------------------------)
    ·10 min read·Jan 27, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7b895f296d3b--------------------------------)
    ·10分钟阅读·2024年1月27日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/693f9e9a838bc5b59082bdb32087f5b2.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/693f9e9a838bc5b59082bdb32087f5b2.png)'
- en: Introduction
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: '2017 was a historical year in machine learning. Researchers from the Google
    Brain team introduced **Transformer** which rapidly outperformed most of the existing
    approaches in deep learning. The famous **attention** mechanism became the key
    component in the future models derived from Transformer. The amazing fact about
    Transformer’s architecture is its vaste flexibility: it can be efficiently used
    for a variety of machine learning task types including NLP, image and video processing
    problems.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年是机器学习历史上具有里程碑意义的一年。来自Google Brain团队的研究人员提出了**Transformer**，它迅速超越了大多数现有的深度学习方法。著名的**注意力**机制成为未来基于Transformer的模型中的关键组成部分。Transformer架构的一个令人惊讶的事实是它的巨大灵活性：它可以高效地用于多种机器学习任务类型，包括自然语言处理、图像和视频处理问题。
- en: The original Transformer can be decomposed into two parts which are called **encoder**
    and **decoder**. As the name suggests, the goal of the encoder is to encode an
    input sequence in the form of a vector of numbers — a low-level format that is
    understood by machines. On the other hand, the decoder takes the encoded sequence
    and by applying a language modeling task, it generates a new sequence.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的Transformer可以分解为两个部分，分别称为**编码器**和**解码器**。顾名思义，编码器的目标是将输入序列编码为一个数字向量——这是机器可以理解的低级格式。另一方面，解码器接受编码后的序列，并通过应用语言建模任务，生成一个新的序列。
- en: Encoders and decoders can be used individually for specific tasks. The two most
    famous models deriving their parts from the original Transformer are called [**BERT**](https://medium.com/towards-data-science/bert-3d1bf880386a)
    (Bidirectional Encoder Representations from Transformer) consisting of…
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器和解码器可以单独用于特定任务。两种最著名的模型，它们的部分组件来自原始的Transformer，分别叫做[**BERT**](https://medium.com/towards-data-science/bert-3d1bf880386a)（来自Transformer的双向编码表示），包含…
