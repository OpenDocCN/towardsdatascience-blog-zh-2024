- en: Performance Insights from Sigma Rule Detections in Spark Streaming
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 流处理中 Sigma 规则检测的性能洞察
- en: 原文：[https://towardsdatascience.com/performance-insights-from-sigma-rule-detections-in-spark-streaming-fac8c67d37b8?source=collection_archive---------3-----------------------#2024-06-01](https://towardsdatascience.com/performance-insights-from-sigma-rule-detections-in-spark-streaming-fac8c67d37b8?source=collection_archive---------3-----------------------#2024-06-01)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/performance-insights-from-sigma-rule-detections-in-spark-streaming-fac8c67d37b8?source=collection_archive---------3-----------------------#2024-06-01](https://towardsdatascience.com/performance-insights-from-sigma-rule-detections-in-spark-streaming-fac8c67d37b8?source=collection_archive---------3-----------------------#2024-06-01)
- en: 'Utilizing Sigma rules for anomaly detection in cybersecurity logs: A study
    on performance optimization'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在网络安全日志中利用 Sigma 规则进行异常检测：关于性能优化的研究
- en: '[](https://medium.com/@jean-claude.cote?source=post_page---byline--fac8c67d37b8--------------------------------)[![Jean-Claude
    Cote](../Images/aea2df9c7b95fc85cc336f64d64b0a76.png)](https://medium.com/@jean-claude.cote?source=post_page---byline--fac8c67d37b8--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fac8c67d37b8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fac8c67d37b8--------------------------------)
    [Jean-Claude Cote](https://medium.com/@jean-claude.cote?source=post_page---byline--fac8c67d37b8--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jean-claude.cote?source=post_page---byline--fac8c67d37b8--------------------------------)[![Jean-Claude
    Cote](../Images/aea2df9c7b95fc85cc336f64d64b0a76.png)](https://medium.com/@jean-claude.cote?source=post_page---byline--fac8c67d37b8--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fac8c67d37b8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fac8c67d37b8--------------------------------)
    [Jean-Claude Cote](https://medium.com/@jean-claude.cote?source=post_page---byline--fac8c67d37b8--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fac8c67d37b8--------------------------------)
    ·14 min read·Jun 1, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fac8c67d37b8--------------------------------)
    ·14 分钟阅读·2024年6月1日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/37046fd8ffe78e40feab09bcdfb16198.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/37046fd8ffe78e40feab09bcdfb16198.png)'
- en: Photo by Ed Vazquez on Unsplash
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：Ed Vazquez，Unsplash
- en: One of the roles of the [Canadian Centre for Cyber Security](https://www.cyber.gc.ca/en)
    (CCCS) is to detect anomalies and issue mitigations as quickly as possible.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[加拿大网络安全中心](https://www.cyber.gc.ca/en)（CCCS）的职责之一是尽快检测异常并发布缓解措施。'
- en: 'While putting our Sigma rule detections into production, we made an interesting
    observation in our Spark streaming application. Running a single large SQL statement
    expressing 1000 Sigma detection rules was slower than running five separate queries,
    each applying 200 Sigma rules. This was surprising, as running five queries forces
    Spark to read the source data five times rather than once. For further details,
    please refer to our series of articles:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在将我们的 Sigma 规则检测投入生产时，我们在 Spark 流处理应用程序中发现了一个有趣的现象。运行一个包含 1000 条 Sigma 检测规则的大型
    SQL 语句的速度比运行五个单独的查询要慢，每个查询应用 200 条 Sigma 规则。这令人惊讶，因为运行五个查询会迫使 Spark 阅读源数据五次，而不是一次。有关更多细节，请参考我们的系列文章：
- en: '[](/anomaly-detection-using-sigma-rules-part-1-leveraging-spark-sql-streaming-246900e95457?source=post_page-----fac8c67d37b8--------------------------------)
    [## Anomaly Detection using Sigma Rules (Part 1): Leveraging Spark SQL Streaming'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/anomaly-detection-using-sigma-rules-part-1-leveraging-spark-sql-streaming-246900e95457?source=post_page-----fac8c67d37b8--------------------------------)
    [## 使用 Sigma 规则进行异常检测（第一部分）：利用 Spark SQL 流处理'
- en: Sigma rules are used to detect anomalies in cyber security logs. We use Spark
    structured streaming to evaluate Sigma…
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Sigma 规则用于检测网络安全日志中的异常。我们使用 Spark 结构化流处理来评估 Sigma…
- en: towardsdatascience.com](/anomaly-detection-using-sigma-rules-part-1-leveraging-spark-sql-streaming-246900e95457?source=post_page-----fac8c67d37b8--------------------------------)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/anomaly-detection-using-sigma-rules-part-1-leveraging-spark-sql-streaming-246900e95457?source=post_page-----fac8c67d37b8--------------------------------)
- en: Given the vast amount of telemetry data and detection rules we need to execute,
    every gain in performance yields significant cost savings. Therefore, we decided
    to investigate this peculiar observation, aiming to explain it and potentially
    discover additional opportunities to improve performance. We learned a few things
    along the way and wanted to share them with the broader community.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们需要执行的大量遥测数据和检测规则，每一点性能提升都能带来显著的成本节省。因此，我们决定调查这个奇特的观察结果，旨在解释它并可能发现额外的性能提升机会。在这个过程中我们学到了一些东西，并希望与更广泛的社区分享。
- en: '**Introduction**'
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**简介**'
- en: 'Our hunch was that we were reaching a limit in Spark’s code generation. So,
    a little background on this topic is required. In 2014, Spark introduced code
    generation to evaluate expressions of the form `(id > 1 and id > 2) and (id <
    1000 or (id + id) = 12)`. This article from Databricks explains it very well:
    [Exciting Performance Improvements on the Horizon for Spark SQL](https://www.databricks.com/blog/2014/06/02/exciting-performance-improvements-on-the-horizon-for-spark-sql.html)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的直觉是，我们达到了Spark代码生成的限制。因此，了解一下这个主题的背景是必要的。2014年，Spark引入了代码生成来评估形如`(id > 1
    and id > 2) and (id < 1000 or (id + id) = 12)`的表达式。Databricks的一篇文章对此做了很好的解释：[Spark
    SQL的激动人心的性能提升即将来临](https://www.databricks.com/blog/2014/06/02/exciting-performance-improvements-on-the-horizon-for-spark-sql.html)
- en: 'Two years later, Spark introduced Whole-Stage Code Generation. This optimization
    merges multiple operators together into a single Java function. Like expression
    code generation, Whole-Stage Code Generation eliminates virtual function calls
    and leverages CPU registers for intermediate data. However, rather than being
    at the expression level, it is applied at the operator level. Operators are the
    nodes in an execution plan. To find out more, read [Apache Spark as a Compiler:
    Joining a Billion Rows per Second on a Laptop](https://www.databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 两年后，Spark引入了全阶段代码生成。这项优化将多个操作符合并成一个单一的Java函数。像表达式代码生成一样，全阶段代码生成消除了虚拟函数调用，并利用CPU寄存器处理中间数据。然而，它与表达式级别的生成不同，它是应用于操作符级别的。操作符是执行计划中的节点。欲了解更多信息，请阅读[Apache
    Spark作为编译器：在笔记本电脑上每秒连接十亿行数据](https://www.databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html)
- en: 'To summarize these articles, let’s generate the plan for this simple query:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结这些文章，让我们生成这个简单查询的执行计划：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In this simple query, we are using two operators: Range to generate rows and
    Select to perform a projection. We see these operators in the query’s physical
    plan. Notice the asterisk (*) beside the nodes and their associated `[codegen
    id : 1]`. This indicates that these two operators were merged into a single Java
    function using Whole-Stage Code Generation.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '在这个简单的查询中，我们使用了两个操作符：Range用于生成行，Select用于执行投影。我们可以在查询的物理计划中看到这些操作符。注意节点旁边的星号（*）以及它们相关的`[codegen
    id : 1]`。这表示这两个操作符通过全阶段代码生成被合并为一个单一的Java函数。'
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The generated code clearly shows the two operators being merged.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的代码清楚地展示了这两个操作符是如何被合并的。
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `project_doConsume_0` function contains the code to evaluate `(id > 1 and
    id > 2) and (id < 1000 or (id + id) = 12)`. Notice how this code is generated
    to evaluate this specific expression. This is an illustration of expression code
    generation.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '`project_doConsume_0`函数包含了评估`(id > 1 and id > 2) and (id < 1000 or (id + id)
    = 12)`的代码。请注意，这段代码是如何被生成来评估这个特定表达式的。这是表达式代码生成的一个示例。'
- en: The whole class is an operator with a `processNext` method. This generated operator
    performs both the Projection and the Range operations. Inside the while loop at
    line 117, we see the code to produce rows and a specific call (not a virtual function)
    to `project_doConsume_0`. This illustrates what Whole-Stage Code Generation does.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 整个类是一个具有`processNext`方法的操作符。这个生成的操作符同时执行投影（Projection）和范围（Range）操作。在第117行的while循环中，我们可以看到生成行的代码和一个特定的调用（不是虚拟函数）`project_doConsume_0`。这展示了全阶段代码生成（Whole-Stage
    Code Generation）是如何工作的。
- en: '**Breaking Down the Performance**'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**性能分析**'
- en: 'Now that we have a better understanding of Spark’s code generation, let’s try
    to explain why breaking a query doing 1000 Sigma rules into smaller ones performs
    better. Let’s consider a SQL statement that evaluates two Sigma rules. These rules
    are straightforward: Rule1 matches events with an `Imagepath` ending in ‘schtask.exe’,
    and Rule2 matches an `Imagepath` starting with ‘d:’.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对 Spark 的代码生成有了更好的理解，让我们尝试解释为什么将一个包含 1000 个 Sigma 规则的查询拆分成较小的规则会表现得更好。我们来考虑一个评估两个
    Sigma 规则的 SQL 语句。这些规则非常简单：Rule1 匹配 `Imagepath` 以 ‘schtask.exe’ 结尾的事件，Rule2 匹配
    `Imagepath` 以 ‘d:’ 开头的事件。
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The select labeled #1 performs the detections and stores the results in new
    columns named rule1 and rule2\. Select #2 regroups these columns under a single
    `results_map`, and finally select #3 transforms the map into an array of matching
    rules. It uses `map_filter` to keep only the entries of rules that actually matched,
    and then `map_keys` is used to convert the map entries into a list of matching
    rule names.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '标记为 #1 的选择执行检测，并将结果存储在名为 rule1 和 rule2 的新列中。选择 #2 将这些列重新组合到一个名为 `results_map`
    的单一列中，最后选择 #3 将映射转换为一个匹配规则的数组。它使用 `map_filter` 仅保留实际匹配的规则条目，然后使用 `map_keys` 将映射条目转换为匹配规则名称的列表。'
- en: 'Let’s print out the Spark execution plan for this query:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印出这个查询的 Spark 执行计划：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Notice that node Project (4) is not code generated. Node 4 has a lambda function,
    does it prevent whole stage code generation? More on this later.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，节点 Project (4) 不是由代码生成的。节点 4 包含一个 lambda 函数，它是否阻止了整个阶段的代码生成？稍后会详细讨论这个问题。
- en: 'This query is not quite what we want. We would like to produce a table of events
    with a column indicating the rule that was matched. Something like this:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这个查询并不是我们想要的。我们希望生成一个事件表，并且有一列显示匹配的规则。如下所示：
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: That’s easy enough. We just need to explode the `matching_rules` column.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这很简单。我们只需要展开 `matching_rules` 列。
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This produces two additional operators: Generate (6) and Project (7). However,
    there is also a new Filter (3).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生两个额外的操作符：Generate (6) 和 Project (7)。然而，还有一个新的 Filter (3)。
- en: '[PRE7]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `explode` function generates rows for every element in the array. When the
    array is empty, `explode` does not produce any rows, effectively filtering out
    rows where the array is empty.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`explode` 函数会为数组中的每个元素生成一行。当数组为空时，`explode` 不会生成任何行，实际上过滤掉了那些数组为空的行。'
- en: 'Spark has an optimization rule that detects the explode function and produces
    this additional condition. The filter is an attempt by Spark to short-circuit
    processing as much as possible. The source code for this rule, named `org.apache.spark.sql.catalyst.optimizer.InferFiltersFromGenerate`,
    explains it like this:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 有一个优化规则，用于检测 `explode` 函数，并产生这个额外的条件。该过滤器是 Spark 尝试尽可能短路处理的做法。这个规则的源代码，名为
    `org.apache.spark.sql.catalyst.optimizer.InferFiltersFromGenerate`，是这样解释的：
- en: Infers filters from Generate, such that rows that would have been removed by
    this Generate can be removed earlier — before joins and in data sources.
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 从 Generate 推断过滤器，以便可以在连接和数据源之前，提前移除本该被此 Generate 移除的行。
- en: For more details on how Spark optimizes execution plans please refer to David
    Vrba’s article [Mastering Query Plans in Spark 3.0](/mastering-query-plans-in-spark-3-0-f4c334663aa4)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 Spark 如何优化执行计划的更多细节，请参阅 David Vrba 的文章 [Mastering Query Plans in Spark 3.0](/mastering-query-plans-in-spark-3-0-f4c334663aa4)。
- en: 'Another question arises: do we benefit from this additional filter? Notice
    this additional filter is not whole-stage code generated either, presumably because
    of the lambda function. Let’s try to express the same query but without using
    a lambda function.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题出现了：我们是否从这个额外的过滤器中受益？注意，这个额外的过滤器同样没有被整个阶段的代码生成，可能是因为 lambda 函数的原因。让我们尝试表达相同的查询，但不使用
    lambda 函数。
- en: Instead, we can put the rule results in a map, explode the map, and filter out
    the rows, thereby bypassing the need for `map_filter`.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我们可以将规则结果放入映射中，展开映射，并过滤掉不需要的行，从而绕过 `map_filter`。
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The select #3 operation explodes the map into two new columns. The `matched_rule`
    column will hold the key, representing the rule name, while the `matched_result`
    column will contain the result of the detection test. To filter the rows, we simply
    keep only those with a positive `matched_result`.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '选择 #3 操作将映射展开成两个新列。`matched_rule` 列将保存键，表示规则名称，而 `matched_result` 列将包含检测测试的结果。为了过滤行，我们只保留
    `matched_result` 为正的行。'
- en: The physical plan indicates that all nodes are whole-stage code generated into
    a single Java function, which is promising.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 物理计划表明，所有节点都被整个阶段的代码生成，合并成一个 Java 函数，这非常有前景。
- en: '[PRE9]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Let’s conduct some tests to compare the performance of the query using `map_filter`
    and the one using explode then filter.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行一些测试，以比较使用 `map_filter` 和使用 explode 然后 filter 的查询性能。
- en: We ran these tests on a machine with 4 CPUs. We generated 1 million rows, each
    with 100 rules, and each rule evaluating 5 expressions. These tests were run 5
    times.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在一台配备 4 个 CPU 的机器上运行了这些测试。我们生成了 100 万行数据，每行包含 100 条规则，每条规则评估 5 个表达式。这些测试共运行了
    5 次。
- en: On average
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 平均而言
- en: map_filter took 42.6 seconds
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: map_filter 耗时 42.6 秒
- en: explode_then_filter took 51.2 seconds
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: explode_then_filter 耗时 51.2 秒
- en: So, map_filter is slightly faster even though it’s not using whole-stage code
    generation.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，map_filter 略微更快，尽管它没有使用 WholeStageCodeGen。
- en: 'However, in our production query, we execute many more Sigma rules — a total
    of 1000 rules. This includes 29 regex expressions, 529 equals, 115 starts-with,
    2352 ends-with, and 5838 contains expressions. Let’s test our query again, but
    this time with a slight increase in the number of expressions, using 7 instead
    of 5 per rule. Upon doing this, we encountered an error in our logs:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们的生产查询中，我们执行了更多的 Sigma 规则——共计 1000 条规则。这包括 29 个正则表达式，529 个等号，115 个以“开始”为前缀，2352
    个以“结束”为后缀的表达式，以及 5838 个包含表达式。让我们再次测试查询，但这次我们将每条规则的表达式数量从 5 增加到 7。当这样做时，我们在日志中遇到了以下错误：
- en: '[PRE10]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We tried increasing `spark.sql.codegen.maxFields` and `spark.sql.codegen.hugeMethodLimit`,
    but fundamentally, Java classes have a function size limit of 64 KB. Additionally,
    the JVM JIT compiler limits itself to compiling functions smaller than 8 KB.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试增加了`spakr.sql.codegen.maxFields`和`spark.sql.codegen.hugeMethodLimit`，但从根本上讲，Java
    类的函数大小限制为64 KB。此外，JVM JIT 编译器限制其只能编译小于 8 KB 的函数。
- en: However, the query still runs fine because Spark falls back to the Volcano execution
    model for certain parts of the plan. WholeStageCodeGen is just an optimization
    after all.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，查询仍然能够正常运行，因为 Spark 会在某些执行计划的部分回退到火山执行模型（Volcano execution model）。毕竟，WholeStageCodeGen
    只是一个优化。
- en: Running the same test as before but with 7 expressions per rule rather than
    5, explode_then_filter is much faster than map_filter.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 运行与之前相同的测试，但每条规则使用 7 个表达式而不是 5 个时，explode_then_filter 比 map_filter 快得多。
- en: map_filter took 68.3 seconds
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: map_filter 耗时 68.3 秒
- en: explode_then_filter took 15.8 seconds
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: explode_then_filter 耗时 15.8 秒
- en: 'Increasing the number of expressions causes parts of the explode_then_filter
    to no longer be whole-stage code generated. In particular, the Filter operator
    introduced by the rule `org.apache.spark.sql.catalyst.optimizer.InferFiltersFromGenerate`
    is too big to be incorporated into whole-stage code generation. Let’s see what
    happens if we exclude the InferFiltersFromGenerate rule:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 增加表达式数量导致 explode_then_filter 的部分代码不再进行 WholeStageCodeGen 优化。特别是，由规则 `org.apache.spark.sql.catalyst.optimizer.InferFiltersFromGenerate`
    引入的 Filter 操作符过大，无法包含在 WholeStageCodeGen 中。让我们看看如果排除 InferFiltersFromGenerate
    规则会发生什么：
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As expected, the physical plan of both queries no longer has an additional Filter
    operator.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，两个查询的物理计划中都不再有额外的 Filter 操作符。
- en: '[PRE12]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Removing the rule indeed had a significant impact on performance:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 移除规则确实对性能产生了显著影响：
- en: map_filter took 22.49 seconds
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: map_filter 耗时 22.49 秒
- en: explode_then_filter took 4.08 seconds
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: explode_then_filter 耗时 4.08 秒
- en: 'Both queries benefited greatly from removing the rule. Given the improved performance,
    we decided to increase the number of Sigma rules to 500 and the complexity to
    21 expressions:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个查询在移除规则后都受益匪浅。鉴于性能的改善，我们决定将 Sigma 规则的数量增加到 500，并将复杂度提高到 21 个表达式：
- en: 'Results:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：
- en: map_filter took 195.0 seconds
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: map_filter 耗时 195.0 秒
- en: explode_then_filter took 25.09 seconds
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: explode_then_filter 耗时 25.09 秒
- en: Despite the increased complexity, both queries still deliver pretty good performance,
    with explode_then_filter significantly outperforming map_filter.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管复杂性增加了，但两个查询仍然能提供相当不错的性能，其中 explode_then_filter 显著优于 map_filter。
- en: It’s interesting to explore the different aspects of code generation employed
    by Spark. While we may not currently benefit from whole-stage code generation,
    we can still gain advantages from expression generation.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 探讨 Spark 所采用的不同代码生成方式是很有趣的。尽管我们目前可能没有从 WholeStageCodeGen 中获益，但我们仍然可以从表达式生成中获得优势。
- en: Expression generation doesn’t face the same limitations as whole-stage code
    generation. Very large expression trees can be broken into smaller ones, and Spark’s
    `spark.sql.codegen.methodSplitThreshold` controls how these are broken up. Although
    we experimented with this property, we didn’t observe significant improvements.
    The default setting seems satisfactory.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 表达式生成不受与整体代码生成相同的限制。非常大的表达式树可以被拆分成更小的树，Spark 的 `spark.sql.codegen.methodSplitThreshold`
    控制如何拆分这些树。虽然我们尝试了这一属性，但并没有观察到显著的改进。默认设置似乎已足够令人满意。
- en: 'Spark provides a debugging property named `spark.sql.codegen.factoryMode`,
    which can be set to FALLBACK, CODEGEN_ONLY, or NO_CODEGEN. We can turn off expression
    code generation by setting `spark.sql.codegen.factoryMode=NO_CODEGEN`, which results
    in a drastic performance degradation:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 提供了一个名为 `spark.sql.codegen.factoryMode` 的调试属性，可以设置为 FALLBACK、CODEGEN_ONLY
    或 NO_CODEGEN。我们可以通过设置 `spark.sql.codegen.factoryMode=NO_CODEGEN` 来关闭表达式代码生成，这会导致性能急剧下降：
- en: 'With 500 rules and 21 expressions:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 500 条规则和 21 个表达式：
- en: map_filter took 1581 seconds
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: map_filter 花费了 1581 秒
- en: explode_then_filter took 122.31 seconds.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: explode_then_filter 花费了 122.31 秒。
- en: Even though not all operators participate in whole-stage code generation, we
    still observe significant benefits from expression code generation.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 即使并非所有操作符都参与整体代码生成，我们仍然观察到表达式代码生成带来了显著的好处。
- en: The Results
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: '![](../Images/e6090af0bc9f5f696f7d7887ded95d2b.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e6090af0bc9f5f696f7d7887ded95d2b.png)'
- en: Image by author
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: With our best case of 25.1 seconds to evaluate 10,500 expressions on 1 million
    rows, we achieve a very respectable rate of 104 million expressions per second
    per CPU.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的最佳案例中，评估 10,500 个表达式所需的时间为 25.1 秒，处理 100 万行数据时，我们实现了每个 CPU 每秒处理 1.04 亿个表达式的非常可观的速度。
- en: The takeaway from this study is that when evaluating a large number of expressions,
    we benefit from converting our queries that use `map_filter` to ones using an
    explode then filter approach. Additionally, the `org.apache.spark.sql.catalyst.optimizer.InferFiltersFromGenerate`
    rule does not seem beneficial in our use case, so we should exclude that rule
    from our queries.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究的启示是，在评估大量表达式时，我们通过将使用 `map_filter` 的查询转换为使用先 explode 后 filter 的方法可以获得好处。此外，`org.apache.spark.sql.catalyst.optimizer.InferFiltersFromGenerate`
    规则在我们的使用案例中似乎并不有利，因此我们应该将该规则从查询中排除。
- en: Does it Explain our Initial Observations?
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它解释了我们最初的观察结果吗？
- en: Implementing these lessons learned in our production jobs yielded significant
    benefits. However, even after these optimizations, splitting the large query into
    multiple smaller ones continued to provide advantages. Upon further investigation,
    we discovered that this was not solely due to code generation but rather a simpler
    explanation.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产工作中实施这些经验教训带来了显著的好处。然而，即使在进行了这些优化后，将大型查询拆分为多个较小查询仍然提供了优势。经过进一步调查，我们发现这不仅仅是由于代码生成，实际上有一个更简单的解释。
- en: Spark streaming operates by running a micro-batch to completion and then checkpoints
    its progress before starting a new micro-batch.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 流处理通过运行微批次直到完成，然后在开始新的微批次之前检查点其进度来工作。
- en: During each micro-batch, Spark has to complete all its tasks, typically 200\.
    However, not all tasks are created equal. Spark employs a round-robin strategy
    to distribute rows among these tasks. So, on occasion, some tasks can contain
    events with large attributes, for example, a very large command line, causing
    certain tasks to finish quickly while others take much longer. For example here
    the distribution of a micro-batch task execution time. The median task time is
    14 seconds. However, the worst straggler is 1.6 minutes!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个微批次中，Spark 必须完成所有任务，通常是 200 个。然而，并非所有任务的难度相同。Spark 采用轮询策略将行分配给这些任务。因此，某些任务可能会包含大属性的事件，例如非常大的命令行，导致某些任务快速完成，而其他任务则需要更长时间。例如，这里展示了微批任务执行时间的分布。中位数任务时间为
    14 秒。然而，最慢的任务竟然需要 1.6 分钟！
- en: '![](../Images/8b9727dd12cf03571a89fcfbd3947a42.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b9727dd12cf03571a89fcfbd3947a42.png)'
- en: Image by author
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: This indeed sheds light on a different phenomenon. The fact that Spark waits
    on a few straggler tasks during each micro-batch leaves many CPUs idle, which
    explains why splitting the large query into multiple smaller ones resulted in
    faster overall performance.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这确实揭示了一个不同的现象。每个微批中，Spark 会等待一些滞后任务，这导致许多 CPU 空闲，这也解释了为什么将大型查询拆分为多个较小的查询会导致整体性能更快。
- en: This picture shows 5 smaller queries running in parallel inside the same Spark
    application. Batch3 is waiting on a straggler task while the other queries keep
    progressing.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图展示了5个较小的查询在同一个Spark应用中并行运行。Batch3在等待一个拖慢的任务，而其他查询继续进展。
- en: '![](../Images/a683d30843790b2d9984ffabbce854f5.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a683d30843790b2d9984ffabbce854f5.png)'
- en: Image by author
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: During these periods of waiting, Spark can utilize the idle CPUs to tackle other
    queries, thereby maximizing resource utilization and overall throughput.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些等待期间，Spark可以利用空闲的CPU来处理其他查询，从而最大化资源利用率和整体吞吐量。
- en: Conclusion
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we provided an overview of Spark’s code generation process
    and discussed how built-in optimizations may not always yield desirable results.
    Additionally, we demonstrated that refactoring a query from using lambda functions
    to one utilizing a simple explode operation resulted in performance improvements.
    Finally, we concluded that while splitting a large statement did lead to performance
    boosts, the primary factor driving these gains was the execution topology rather
    than the queries themselves.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们概述了Spark的代码生成过程，并讨论了内置优化可能并不总是产生理想的结果。此外，我们展示了将查询从使用lambda函数重构为使用简单的explode操作后，性能得到了提升。最后，我们得出结论，尽管拆分大查询确实提升了性能，但推动这些提升的主要因素是执行拓扑结构，而非查询本身。
