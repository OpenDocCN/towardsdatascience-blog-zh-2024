- en: 'Language Model Training and Inference: From Concept to Code'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言模型的训练与推理：从概念到代码
- en: 原文：[https://towardsdatascience.com/language-model-training-and-inference-from-concept-to-code-483cf9b305ef?source=collection_archive---------6-----------------------#2024-01-06](https://towardsdatascience.com/language-model-training-and-inference-from-concept-to-code-483cf9b305ef?source=collection_archive---------6-----------------------#2024-01-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/language-model-training-and-inference-from-concept-to-code-483cf9b305ef?source=collection_archive---------6-----------------------#2024-01-06](https://towardsdatascience.com/language-model-training-and-inference-from-concept-to-code-483cf9b305ef?source=collection_archive---------6-----------------------#2024-01-06)
- en: Learning and implementing next token prediction with a casual language model…
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习和实现下一个词预测与一种简单的语言模型...
- en: '[](https://wolfecameron.medium.com/?source=post_page---byline--483cf9b305ef--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page---byline--483cf9b305ef--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--483cf9b305ef--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--483cf9b305ef--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page---byline--483cf9b305ef--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page---byline--483cf9b305ef--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page---byline--483cf9b305ef--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--483cf9b305ef--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--483cf9b305ef--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page---byline--483cf9b305ef--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--483cf9b305ef--------------------------------)
    ·17 min read·Jan 6, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--483cf9b305ef--------------------------------)
    ·17分钟阅读·2024年1月6日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a8b71e93af85d8dfd9d788819ccea379.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8b71e93af85d8dfd9d788819ccea379.png)'
- en: (Photo by [Chris Ried](https://unsplash.com/@cdr6934?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/a-computer-screen-with-a-bunch-of-code-on-it-ieic5Tq8YMk?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: （图片来自[Chris Ried](https://unsplash.com/@cdr6934?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    在 [Unsplash](https://unsplash.com/photos/a-computer-screen-with-a-bunch-of-code-on-it-ieic5Tq8YMk?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)）
- en: Despite all that has been accomplished with large language models (LLMs), the
    underlying concept that powers all of these models is simple — *we just need to
    accurately predict the next token*! Though some may (reasonably) argue that recent
    research on LLMs goes beyond this basic idea, next token prediction still underlies
    the pre-training, fine-tuning (depending on the variant), and inference process
    of all causal language models, making it a fundamental and important concept for
    any LLM practitioner to understand.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大语言模型（LLMs）已经取得了诸多成就，但驱动这些模型的基本概念依然很简单——*我们只需要准确预测下一个词*! 尽管有些人可能（合理地）认为，近期关于
    LLM 的研究超越了这一基本思想，但下一个词预测依然是所有因果语言模型的预训练、微调（视变体而定）和推理过程的基础，因此它是任何 LLM 从业者必须理解的一个基本而重要的概念。
- en: “It is perhaps surprising that underlying all this progress is still the original
    autoregressive mechanism for generating text, which makes token-level decisions
    one by one and in a left-to-right fashion.” *— from [10]*
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “或许令人惊讶的是，所有这些进展的基础仍然是最初的自回归机制来生成文本，它按顺序一个一个地做出词级决策，且是从左到右的方式。” *— 引自 [10]*
- en: Within this overview, we will take a deep and practical dive into the concept
    of next token prediction to understand how it is used by language models both
    during training and inference. First, we will learn these ideas at a conceptual
    level. Then, we will walk through an actual implementation (in PyTorch) of the
    language model pretraining and inference processes to make the idea of next token
    prediction more concrete.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本概述中，我们将深入且实际地探讨“下一个词预测”这一概念，以理解它在语言模型训练和推理中的应用。首先，我们将在概念层面学习这些思想。然后，我们将通过一个实际实现（使用
    PyTorch）来走一遍语言模型的预训练和推理过程，以便使“下一个词预测”的概念更加具体。
