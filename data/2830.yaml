- en: Explaining LLMs for RAG and Summarization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释用于 RAG 和摘要的 LLMs
- en: 原文：[https://towardsdatascience.com/explaining-llms-for-rag-and-summarization-067e486020b4?source=collection_archive---------9-----------------------#2024-11-21](https://towardsdatascience.com/explaining-llms-for-rag-and-summarization-067e486020b4?source=collection_archive---------9-----------------------#2024-11-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/explaining-llms-for-rag-and-summarization-067e486020b4?source=collection_archive---------9-----------------------#2024-11-21](https://towardsdatascience.com/explaining-llms-for-rag-and-summarization-067e486020b4?source=collection_archive---------9-----------------------#2024-11-21)
- en: A fast and low-resource method using similarity-based attribution
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一种快速且低资源的基于相似度归因的方法
- en: '[](https://medium.com/@daniel-klitzke?source=post_page---byline--067e486020b4--------------------------------)[![Daniel
    Klitzke](../Images/4471634e1a0f0546402d582dcc36c7c4.png)](https://medium.com/@daniel-klitzke?source=post_page---byline--067e486020b4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--067e486020b4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--067e486020b4--------------------------------)
    [Daniel Klitzke](https://medium.com/@daniel-klitzke?source=post_page---byline--067e486020b4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@daniel-klitzke?source=post_page---byline--067e486020b4--------------------------------)[![Daniel
    Klitzke](../Images/4471634e1a0f0546402d582dcc36c7c4.png)](https://medium.com/@daniel-klitzke?source=post_page---byline--067e486020b4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--067e486020b4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--067e486020b4--------------------------------)
    [Daniel Klitzke](https://medium.com/@daniel-klitzke?source=post_page---byline--067e486020b4--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--067e486020b4--------------------------------)
    ·8 min read·Nov 21, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--067e486020b4--------------------------------)
    ·8分钟阅读·2024年11月21日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/56119ebe547d500ce80bfe7a215b9ec8.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56119ebe547d500ce80bfe7a215b9ec8.png)'
- en: Information flow between an input document and its summary as computed by the
    proposed explainability method. (image created by author)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 输入文档与其摘要之间的信息流，计算方式由提出的可解释性方法得出。（图像由作者创建）
- en: TL;DR
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TL;DR
- en: Explaining LLMs is very **slow** and **resource-intensive.**
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释 LLMs 是非常**缓慢**且**资源密集型**的。
- en: This article proposes a task-specific explanation technique or **RAG Q&A** and
    **Summarization.**
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本文提出了一种任务特定的解释技术，即**RAG 问答**和**摘要**。
- en: The approach is **model agnostic** and is **similarity-based.**
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该方法是**模型无关**的，并且是**基于相似度的**。
- en: The approach is **low-resource** and **low-latency**, so can run almost everywhere.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该方法是**低资源**和**低延迟**的，因此几乎可以在任何地方运行。
- en: I provided the **code** on [Github](https://github.com/Renumics/rag-explanation-blogpost),
    using the [Huggingface Transformers](https://github.com/huggingface/transformers)
    ecosystem.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我在[Github](https://github.com/Renumics/rag-explanation-blogpost)上提供了**代码**，使用了[Huggingface
    Transformers](https://github.com/huggingface/transformers)生态系统。
- en: Motivation
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动机
- en: There are a lot of good reasons to get explanations for your model outputs.
    For example, they could help you **find problems** with your model, or they just
    could be a way to provide more **transparency** to the user, thereby facilitating
    user trust. This is why, for models like XGBoost, I have regularly applied methods
    like [SHAP](https://github.com/shap/shap) to get more insights into my model’s
    behavior.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多充分的理由需要为你的模型输出提供解释。例如，它们可以帮助你**发现问题**，或者它们可能仅仅是为用户提供更多**透明度**的一种方式，从而促进用户信任。这就是为什么对于像
    XGBoost 这样的模型，我经常应用像[SHAP](https://github.com/shap/shap)这样的方法，以便更好地了解我的模型行为。
- en: 'Now, with myself more and more dealing with LLM-based ML systems, I wanted
    to **explore ways of explaining LLM models** the same way I did with more traditional
    ML approaches. However, I quickly found myself being stuck because:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，随着我越来越多地处理基于 LLM 的机器学习系统，我想以与传统机器学习方法相同的方式**探索解释 LLM 模型**的方法。然而，我很快发现自己被卡住了，因为：
- en: '**SHAP** does offer [examples for text-based models](https://shap.readthedocs.io/en/latest/text_examples.html),
    but for me, they failed with newer models, as SHAP did not support the embedding
    layers.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**SHAP**确实为基于文本的模型提供了[示例](https://shap.readthedocs.io/en/latest/text_examples.html)，但对我来说，它们在新模型上失败了，因为
    SHAP 不支持嵌入层。'
- en: '**Captum** also offers a tutorial for [LLM attribution](https://captum.ai/tutorials/Llama2_LLM_Attribution);
    however, both presented methods also had their very specific drawbacks. Concretely,
    the perturbation-based method was simply too slow, while the gradient-based method
    was letting my GPU memory explode and ultimately failed.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Captum**还提供了一个[LLM归因](https://captum.ai/tutorials/Llama2_LLM_Attribution)的教程；然而，所展示的两种方法也各自有非常具体的缺点。具体而言，基于扰动的方法速度太慢，而基于梯度的方法则导致我的GPU内存爆炸，最终失败。'
- en: After playing with quantization and even spinning up GPU cloud instances with
    still limited success I had enough I took a step back.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试了量化甚至启动GPU云实例并取得有限成功之后，我决定退后一步。
- en: A Similarity-based Approach
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一种基于相似度的方法
- en: For understanding the approach, let’s first briefly define what we want to achieve.
    Concretely, we want to **identify and highlight sections in our input text** (e.g.
    long text document or RAG context) that are highly relevant to our model output
    (e.g., a summary or RAG answer).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这种方法，让我们首先简要定义我们想要达成的目标。具体而言，我们想要**识别并突出显示输入文本中与模型输出高度相关的部分**（例如，长文本文件或RAG上下文），这些输出可能是摘要或RAG回答。
- en: '![](../Images/4a22728cf380398f046abd250ab10b39.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4a22728cf380398f046abd250ab10b39.png)'
- en: Typical flow of tasks our explainability method is applicable to. (image created
    by author)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的可解释性方法适用的典型任务流程。（图像由作者创建）
- en: In case of **summarization**, our method would have to highlight parts of the
    original input text that are highly reflected in the summary. In case of a **RAG
    system**, our approach would have to highlight document chunks from the RAG context
    that are showing up in the answer.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在**摘要**的情况下，我们的方法需要突出显示原始输入文本中在摘要中高度反映的部分。在**RAG系统**的情况下，我们的方法需要突出显示RAG上下文中在答案中出现的文档块。
- en: 'Since directly explaining the LLM itself has proven intractable for me, I instead
    propose to **model the relation between model inputs and outputs** via a separate
    text similarity model. Concretely, I implemented the following simple but effective
    approach:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于直接解释LLM本身对我来说已经证明是棘手的，因此我提议通过一个单独的文本相似度模型来**建模模型输入与输出之间的关系**。具体来说，我实现了以下简单但有效的方法：
- en: I **split the model inputs and outputs** into sentences.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我**将模型输入和输出拆分成句子**。
- en: I **calculate pairwise similarities** between all sentences.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我**计算了所有句子之间的成对相似度**。
- en: I then **normalize the similarity scores** using Softmax
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我使用Softmax**归一化相似度得分**。
- en: After that, I **visualize the similarities** between input and output sentences
    in a nice plot
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我**将输入和输出句子之间的相似度**可视化为一个漂亮的图表。
- en: In code, this is implemented as shown below. For running the code you need the
    [Huggingface Transformers](https://github.com/huggingface/transformers), [Sentence
    Transformers](https://github.com/UKPLab/sentence-transformers), and [NLTK](https://github.com/nltk/nltk)
    libraries.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，这如下面所示。要运行代码，您需要[Huggingface Transformers](https://github.com/huggingface/transformers)、[Sentence
    Transformers](https://github.com/UKPLab/sentence-transformers)和[NLTK](https://github.com/nltk/nltk)库。
- en: Please, also check out this [Github Repository](https://github.com/Renumics/rag-explanation-blogpost)
    for the full code accompanying this blog post.
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请查看这个[Github仓库](https://github.com/Renumics/rag-explanation-blogpost)，获取与此博客文章相关的完整代码。
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: So, as you can see, so far, that is pretty simple. Obviously, we don’t explain
    the model itself. However, we might be able to get a good sense of **relations
    between input and output sentences** for this specific type of tasks (summarization
    / RAG Q&A). But how does this actually perform and how to visualize the attribution
    results to make sense of the output?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，到目前为止，这非常简单。显然，我们并没有解释模型本身。然而，我们或许可以对这种特定类型的任务（如摘要/ RAG问答）**输入与输出句子之间的关系**有一个较好的理解。那么，这实际上如何表现，以及如何可视化归因结果以理解输出呢？
- en: Evaluation for RAG and Summarization
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAG和摘要的评估
- en: To visualize the outputs of this approach, I created **two visualizations**
    that are suitable for **showing the feature attributions or connections between
    input and output** of the LLM, respectively.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化这种方法的输出，我创建了**两种可视化**，分别适用于**展示LLM输入与输出之间的特征归因或连接**。
- en: 'These visualizations were generated for a **summary** of the LLM input that
    goes as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这些可视化是为LLM输入的**摘要**生成的，其内容如下：
- en: This section discusses the state of the art in semantic segmentation and instance
    segmentation, focusing on deep learning approaches. Early patch classification
    methods use superpixels, while more recent fully convolutional networks (FCNs)
    predict class probabilities for each pixel. FCNs are similar to CNNs but use transposed
    convolutions for upsampling. Standard architectures include U-Net and VGG-based
    FCNs, which are optimized for computational efficiency and feature size. For instance
    segmentation, proposal-based and instance embedding-based techniques are reviewed,
    including the use of proposals for instance segmentation and the concept of instance
    embeddings.
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本节讨论了语义分割和实例分割的最新技术进展，重点介绍了深度学习方法。早期的补丁分类方法使用超像素，而最近的全卷积网络（FCN）则为每个像素预测类别概率。FCN类似于CNN，但使用转置卷积进行上采样。标准架构包括U-Net和基于VGG的FCN，它们针对计算效率和特征大小进行了优化。在实例分割方面，回顾了基于提议和实例嵌入的方法，包括使用提议进行实例分割和实例嵌入的概念。
- en: Visualizing the Feature Attributions
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征归因的可视化
- en: For visualizing the **feature attributions**, my choice was to simply stick
    to the original representation of the input data as close as possible.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特征归因的可视化，我的选择是尽可能保持输入数据的原始表示。
- en: '![](../Images/1fcb363021a34a497d7e7c691eb1de3c.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1fcb363021a34a497d7e7c691eb1de3c.png)'
- en: Visualization of sentence-wise feature attribution scores based on color mapping.
    (image created by author)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 基于颜色映射的逐句特征归因分数的可视化。（图像由作者创建）
- en: Concretely, I simply plot the sentences, including their calculated attribution
    scores. Therefore, I **map the attribution scores to the colors** of the respective
    sentences.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我只是绘制了句子图，包括它们计算出的归因分数。因此，我**将归因分数映射到相应句子的颜色**。
- en: In this case, this shows us some dominant patterns in the summarization and
    the source sentences that the information might be stemming from. Concretely,
    the **dominance of mentions of FCNs** as an architecture variant mentioned in
    the text, as well as the **mention of proposal- and instance embedding-based instance
    segmentation methods,** are clearly highlighted.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，这向我们展示了一些总结和源句子中的主导模式，信息可能来自这些句子。具体来说，文中提到的**FCN（全卷积网络）架构变种的主导性提及**，以及**基于提议和实例嵌入的实例分割方法的提及**，都得到了清晰的突出显示。
- en: In general, this method turned out to work pretty well for easily capturing
    attributions on the input of a summarization task, as it is very **close to the
    original representation** and adds very **low clutter** to the data. I could imagine
    also providing such a visualization to the user of a RAG system on demand. Potentially,
    the outputs could also be further processed to threshold to certain especially
    relevant chunks; then, this could also be displayed to the user by default to
    **highlight relevant sources**.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，这种方法非常适合轻松捕捉总结任务输入中的归因，因为它非常**接近原始表示**，并且对数据的**干扰非常小**。我可以想象，也可以根据需要为RAG系统的用户提供这样的可视化。潜在地，输出还可以进一步处理，阈值化为某些特别相关的片段；然后，这也可以作为默认设置展示给用户，以**突出相关的来源**。
- en: Again, check out the [Github Repository](https://github.com/Renumics/rag-explanation-blogpost)
    to get the visualization code
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 再次查看[Github仓库](https://github.com/Renumics/rag-explanation-blogpost)以获取可视化代码
- en: Visualizing the Information Flow
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 信息流的可视化
- en: Another visualization technique focuses not on the feature attributions, but
    mostly on the **flow of information** between input text and summary.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种可视化技术侧重的不是特征归因，而主要是**信息流动**，即输入文本和摘要之间的信息流动。
- en: '![](../Images/f032fd38c1af76db7f413c88f23bb556.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f032fd38c1af76db7f413c88f23bb556.png)'
- en: Visualization of the information flow between sentences in Input text and summary
    as Sankey diagram. (image created by author)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 输入文本和摘要中句子之间信息流的可视化，以Sankey图表示。（图像由作者创建）
- en: Concretely, what I do here, is to first determine the **major connections between
    input and output** sentences based on the attribution scores. I then visualize
    those connections using a Sankey diagram. Here, the width of the flow connections
    is the **strength of the connection**, and the coloring is done based on the sentences
    in the summary for better **traceability**.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我在这里做的是首先根据归因分数确定**输入句子和输出句子之间的主要连接**。然后，我使用Sankey图可视化这些连接。这里，流动连接的宽度表示**连接的强度**，颜色则是基于摘要中的句子进行着色，以便更好的**可追溯性**。
- en: Here, it shows that the summary mostly follows the order of the text. However,
    there are few parts where the LLM might have **combined information** from the
    beginning and the end of the text, e.g., the summary mentions a focus on deep
    learning approaches in the first sentence. This is taken from the last sentence
    of the input text and is clearly shown in the flow chart.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，可以看到摘要大部分遵循文本的顺序。然而，也有一些部分，LLM可能**结合了**文本开头和结尾的信息，例如，摘要在第一句话中提到重点是深度学习方法。这是从输入文本的最后一句话提取的，并且在流程图中清楚地展示出来。
- en: In general, I found this to be useful, especially to get a sense on how much
    the LLM is aggregating information together **from different parts** of the input,
    rather than just copying or rephrasing certain parts. In my opinion, this can
    also be useful to estimate how much **potential for error** there is if an output
    is relying too much on the LLM for making connections between different bits of
    information.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我发现这种方法很有用，尤其是可以帮助我们了解LLM在多大程度上将信息**从输入的不同部分**进行聚合，而不仅仅是复制或改写某些部分。在我看来，这也有助于估算如果输出过度依赖LLM在不同信息片段之间建立联系时，可能存在的**错误潜力**。
- en: Possible Extensions and Adaptations
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可能的扩展与适配
- en: 'In the [code provided on Github](https://github.com/Renumics/rag-explanation-blogpost)
    I implemented certain extensions of the basic approach shown in the previous sections.
    Concretely I explored the following:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在[GitHub上提供的代码](https://github.com/Renumics/rag-explanation-blogpost)中，我实现了前面章节中展示的基本方法的某些扩展。具体而言，我探索了以下内容：
- en: Use of **different aggregations,** such as max, for the similarity score.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用**不同的聚合方式**，例如最大值，用于相似度评分。
- en: This can make sense as not necessarily the mean similarity to output sentences
    is relevant. Already one good hit could be relevant for out explanation.
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这样做是有道理的，因为输出句子的平均相似度并不是最相关的。即使只有一个好的匹配，也可能对我们的解释很有帮助。
- en: Use of **different window sizes**, e.g., taking chunks of three sentences to
    compute similarities.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用**不同的窗口大小**，例如，使用三句话的片段来计算相似度。
- en: This again makes sense if suspecting that one sentence alone is not enough content
    to truly capture relatedness of two sentences so a larger context is created.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果怀疑单一的句子不足以真正捕捉两个句子之间的相关性，进而创造更大的上下文，这也是有道理的。
- en: Use of **cross-encoding-based models,** such as rerankers.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用**基于交叉编码的模型**，如重排序器。
- en: This could be useful as rerankers are more rexplicitely modeling the relatedness
    of two input documents in one model, being way more sensitive to nuanced language
    in the two documents. See also my recent post on [Towards Data Science](https://medium.com/towards-data-science/reranking-using-huggingface-transformers-for-optimizing-retrieval-in-rag-pipelines-fbfc6288c91f).
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这可能很有用，因为重排序器更明确地在一个模型中建模两个输入文档的相关性，对这两个文档中的细微语言更加敏感。也请参阅我在[Towards Data Science](https://medium.com/towards-data-science/reranking-using-huggingface-transformers-for-optimizing-retrieval-in-rag-pipelines-fbfc6288c91f)上的最新文章。
- en: As said, all this is demoed in the provided Code so make sure to check that
    out as well.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，所有这些内容在提供的代码中都有演示，因此务必查看代码。
- en: Conclusion
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In general, I found it pretty challenging to find tutorials that truly demonstrate
    explainability techniques for non-toy scenarios in RAG and summarization. Especially
    techniques that are useful in “real-time” scenarios, and are thus providing **low-latency**
    seemed to be scarce. However, as shown in this post, simple solutions can already
    give quite nice results when it comes to **showing relations** between documents
    and answers in a RAG use case. I will definitely explore this further and see
    how I can probably use that in RAG production scenarios, as **providing traceable
    outputs** to the users has proven invaluable to me. If you are interested in the
    topic and want to get more content in this style, follow me here on [Medium](https://medium.com/@daniel-klitzke)
    and on [LinkedIn](https://www.linkedin.com/in/daniel-klitzke/).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我发现很难找到能够真正展示RAG和总结中可解释性技术的教程，尤其是在“实时”场景中有用的技术，能够提供**低延迟**的技术似乎稀缺。然而，正如这篇文章所展示的，简单的解决方案已经能够在RAG用例中提供相当不错的结果，尤其是在**展示文档与答案之间的关系**时。我肯定会进一步探索这个问题，看看如何将其应用于RAG生产场景，因为**提供可追溯的输出**对我来说已经证明是极为宝贵的。如果你对这个话题感兴趣，并希望获得更多此类内容，请在[Medium](https://medium.com/@daniel-klitzke)和[LinkedIn](https://www.linkedin.com/in/daniel-klitzke/)上关注我。
