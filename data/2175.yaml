- en: A Guide to Clustering Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类算法指南
- en: 原文：[https://towardsdatascience.com/a-guide-to-clustering-algorithms-e28af85da0b7?source=collection_archive---------4-----------------------#2024-09-06](https://towardsdatascience.com/a-guide-to-clustering-algorithms-e28af85da0b7?source=collection_archive---------4-----------------------#2024-09-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-guide-to-clustering-algorithms-e28af85da0b7?source=collection_archive---------4-----------------------#2024-09-06](https://towardsdatascience.com/a-guide-to-clustering-algorithms-e28af85da0b7?source=collection_archive---------4-----------------------#2024-09-06)
- en: '[](https://medium.com/@adavis08?source=post_page---byline--e28af85da0b7--------------------------------)[![Alex
    Davis](../Images/f773cce9438a68856cb8ba486ac8b051.png)](https://medium.com/@adavis08?source=post_page---byline--e28af85da0b7--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e28af85da0b7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e28af85da0b7--------------------------------)
    [Alex Davis](https://medium.com/@adavis08?source=post_page---byline--e28af85da0b7--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@adavis08?source=post_page---byline--e28af85da0b7--------------------------------)[![Alex
    Davis](../Images/f773cce9438a68856cb8ba486ac8b051.png)](https://medium.com/@adavis08?source=post_page---byline--e28af85da0b7--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e28af85da0b7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e28af85da0b7--------------------------------)
    [Alex Davis](https://medium.com/@adavis08?source=post_page---byline--e28af85da0b7--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e28af85da0b7--------------------------------)
    ·6 min read·Sep 6, 2024
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e28af85da0b7--------------------------------)
    ·阅读时间：6分钟·2024年9月6日
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '*Clustering is a must-have skill set for any data scientist due to its utility
    and flexibility to real-world problems. This article is an overview of clustering
    and the different types of clustering algorithms.*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*聚类是任何数据科学家必备的技能，因为它在解决实际问题时的实用性和灵活性。本文概述了聚类及不同类型的聚类算法。*'
- en: What is Clustering?
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是聚类？
- en: Clustering is a popular unsupervised learning technique that is designed to
    group objects or observations together based on their similarities. Clustering
    has a lot of useful applications such as market segmentation, recommendation systems,
    exploratory analysis, and more.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是一种流行的无监督学习技术，旨在根据对象或观测值之间的相似性将它们分组。聚类有许多有用的应用，如市场细分、推荐系统、探索性分析等。
- en: '![](../Images/3aa1508082c9b47231972b9af15633a8.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3aa1508082c9b47231972b9af15633a8.png)'
- en: Image by Author
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: While clustering is a well-known and widely used technique in the field of data
    science, some may not be aware of the different types of clustering algorithms.
    While there are just a few, it is important to understand these algorithms and
    how they work to get the best results for your use case.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然聚类是数据科学领域广为人知且被广泛使用的技术，但一些人可能不了解不同类型的聚类算法。尽管聚类算法种类不多，但理解这些算法及其工作原理对于根据你的使用场景获得最佳结果至关重要。
- en: Centroid-Based Clustering
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于中心点的聚类
- en: Centroid-based clustering is what most think of when it comes to clustering.
    It is the “traditional” way to cluster data by using a defined number of centroids
    (centers) to group data points based on their distance to each centroid. The centroid
    ultimately becomes the mean of it’s assigned data points. While centroid-based
    clustering is powerful, it is not robust against outliers, as outliers will need
    to be assigned to a cluster.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 基于中心点的聚类是大多数人提到聚类时的典型方式。它是通过使用预定义数量的中心点（中心）来根据数据点与每个中心点的距离将数据分组的“传统”聚类方法。最终，中心点成为其分配数据点的均值。尽管基于中心点的聚类非常强大，但它对离群值的鲁棒性较差，因为离群值仍需要被分配到某个聚类中。
- en: K-Means
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K-Means
- en: 'K-Means is the most widely used clustering algorithm, and is likely the first
    one you will learn as a data scientist. As explained above, the objective is to
    minimize the sum of distances between the data points and the cluster centroid
    to identify the correct group that each data point should belong to. Here’s how
    it works:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: K-Means 是最广泛使用的聚类算法，它可能是你作为数据科学家学习的第一个算法。如上所述，目标是最小化数据点与聚类中心之间的距离之和，从而确定每个数据点应该属于的正确组别。其工作原理如下：
- en: A defined number of centroids are randomly dropped into the vector space of
    the unlabeled data (initialization).
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个预定义数量的中心点被随机放置到无标签数据的向量空间中（初始化）。
- en: Each data point measures itself to each centroid (usually using Euclidean distance)
    and assigns itself to the closest one.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个数据点会与每个质心进行自我比较（通常使用欧几里得距离），并将自己分配给最近的一个质心。
- en: The centroids relocate to the mean of their assigned data points.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 质心会移动到它们所分配的数据点的均值位置。
- en: Steps 2–3 repeat until the ‘optimal’ clusters are produced.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步骤2-3会反复执行，直到生成“最佳”聚类。
- en: '![](../Images/228afd72b5c9d40421a55f06880992fe.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/228afd72b5c9d40421a55f06880992fe.png)'
- en: Image by Author
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: K-Means ++
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K-Means ++
- en: K-Means ++ is an improvement of the initialization step of K-Means. Since the
    centroids are randomly dropped in, there is a chance that more than one centroid
    might be initialized into the same cluster, resulting in poor results.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: K-Means ++是K-Means初始化步骤的改进版。由于质心是随机选择的，因此可能会有多个质心初始化到同一个聚类中，导致结果不理想。
- en: However K-Means ++ solves this by randomly assigning the first centroid that
    will eventually find the largest cluster. Then, the other centroids are placed
    a certain distance away from the initial cluster. The goal of K-Means ++ is to
    push the centroids as far as possible from one another. This results in high-quality
    clusters that are distinct and well-defined.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，K-Means ++通过随机分配第一个质心来解决这个问题，最终找到最大的聚类。然后，其他质心会被放置在与初始聚类一定距离的位置。K-Means ++的目标是使质心之间的距离尽可能远。这种方式可以得到高质量的聚类，且这些聚类是独立且明确定义的。
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Density-Based Clustering
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于密度的聚类
- en: Density-based algorithms are also a popular form of clustering. However, instead
    of measuring from randomly placed centroids, they create clusters by identifying
    high-density areas within the data. Density-based algorithms do not require a
    defined number of clusters, and therefore are less work to optimize.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 基于密度的算法也是一种流行的聚类形式。然而，它们并不是通过从随机位置的质心进行度量，而是通过识别数据中的高密度区域来创建聚类。基于密度的算法不需要预先定义聚类的数量，因此优化起来工作量较小。
- en: While centroid-based algorithms perform better with spherical clusters, density-based
    algorithms can take arbitrary shapes and are more flexible. They also do not include
    outliers in their clusters and therefore are robust. However, they can struggle
    with data of varying densities and high dimensions.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然基于质心的算法在处理球形聚类时表现更好，但基于密度的算法可以处理任意形状的聚类，并且更具灵活性。它们还不会将离群点包括在聚类中，因此具有较强的鲁棒性。然而，这些算法在面对具有不同密度和高维度的数据时可能会遇到困难。
- en: '![](../Images/900c97181437cb771ab90dc1c2a7240b.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/900c97181437cb771ab90dc1c2a7240b.png)'
- en: Image by Author
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: DBSCAN
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DBSCAN
- en: 'DBSCAN is the most popular density-based algorithm. DBSCAN works as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN是最流行的基于密度的算法。DBSCAN的工作原理如下：
- en: DBSCAN randomly selects a data point and checks if it has enough neighbors within
    a specified radius.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DBSCAN随机选择一个数据点，并检查它是否在指定的半径内有足够的邻居。
- en: If the point has enough neighbors, it is marked as part of a cluster.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果某个点有足够的邻居，它将被标记为聚类的一部分。
- en: DBSCAN recursively checks if the neighbors also have enough neighbors within
    the radius until all points in the cluster have been visited.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DBSCAN会递归检查邻居是否也在半径内有足够的邻居，直到聚类中的所有点都被访问过。
- en: Repeat steps 1–3 until the remaining data point do not have enough neighbors
    in the radius.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤1-3，直到剩余的数据点在半径内没有足够的邻居。
- en: Remaining data points are marked as outliers.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 剩余的数据点被标记为离群点。
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Hierarchical Clustering
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次聚类
- en: Next, we have hierarchical clustering. This method starts off by computing a
    distance matrix from the raw data. This distance matrix is best and often visualized
    by a dendrogram (see below). Data points are linked together one by one by finding
    the nearest neighbor to eventually form one giant cluster. Therefore, a cut-off
    point to identify the clusters by stopping all data points from linking together.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来讲解层次聚类。这种方法首先通过从原始数据计算一个距离矩阵开始。这个距离矩阵最好且通常通过树状图（见下文）进行可视化。数据点通过找到最近的邻居，一一连接，最终形成一个巨大的聚类。因此，可以通过设置一个截断点，停止所有数据点的连接，从而识别出不同的聚类。
- en: '![](../Images/e180fc95c23f18fec38023cc80147e63.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e180fc95c23f18fec38023cc80147e63.png)'
- en: Image by Author
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: By using this method, the data scientist can build a robust model by defining
    outliers and excluding them in the other clusters. This method works great against
    hierarchical data, such as taxonomies. The number of clusters depends on the depth
    parameter and can be anywhere from 1-n.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用这种方法，数据科学家可以通过定义离群点并将其排除在其他聚类之外，构建一个鲁棒的模型。这种方法在处理层次结构数据（如分类法）时效果极佳。聚类的数量取决于深度参数，范围可以是1到n。
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Distribution Based Clustering
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于分布的聚类
- en: Lastly, distribution-based clustering considers a metric other than distance
    and density, and that is probability. Distribution-based clustering assumes that
    the data is made up of probabilistic distributions, such as normal distributions.
    The algorithm creates ‘bands’ that represent confidence intervals. The further
    away a data point is from the center of a cluster, the less confident we are that
    the data point belongs to that cluster.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，基于分布的聚类考虑了除距离和密度之外的度量标准，即概率。基于分布的聚类假设数据由概率分布组成，例如正态分布。该算法创建了“带状”区域，表示置信区间。数据点距离聚类中心越远，我们就越不确定该数据点是否属于该聚类。
- en: '![](../Images/b9c66c47eb6422b7014a43d6bd01a57b.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b9c66c47eb6422b7014a43d6bd01a57b.png)'
- en: Image by Author
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Distribution-based clustering is very difficult to implement due to the assumptions
    it makes. It usually is not recommended unless rigorous analysis has been done
    to confirm its results. For example, using it to identify customer segments in
    a marketing dataset, and confirming these segments follow a distribution. This
    can also be a great method for exploratory analysis to see not only what the centers
    of clusters comprise of, but also the edges and outliers.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 基于分布的聚类由于其假设条件，实施起来非常困难。通常不推荐使用，除非经过严格分析确认其结果。例如，可以使用它来识别市场数据集中的客户群体，并确认这些群体符合某种分布。这个方法也非常适合探索性分析，不仅能了解聚类中心的组成，还能分析聚类的边缘和离群点。
- en: Conclusion
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Clustering is an unsupervised machine learning technique that has a growing
    utility in many fields. It can be used to support data analysis, segmentation
    projects, recommendation systems, and more. Above we have explored how they work,
    their pros and cons, code samples, and even some use cases. I would consider experience
    with clustering algorithms a must-have for data scientists due to their utility
    and flexibility.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是一种无监督机器学习技术，在许多领域的应用日益增多。它可以用于支持数据分析、细分项目、推荐系统等多个场景。上述内容中，我们探讨了它们的工作原理、优缺点、代码示例，甚至一些使用案例。我认为，数据科学家必须具备使用聚类算法的经验，因为它们具有广泛的实用性和灵活性。
- en: '*I hope you have enjoyed my article! Please feel free to comment, ask questions,
    or request other topics.*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*希望你喜欢我的文章！请随时发表评论、提问或请求其他话题。*'
