- en: 'AWS DeepRacer : A Practical Guide to Reducing The Sim2Real Gap — Part 2 ||
    Training Guide'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AWS DeepRacer：减少Sim2Real差距的实用指南——第二部分 || 训练指南
- en: 原文：[https://towardsdatascience.com/aws-deepracer-a-practical-guide-to-reducing-the-sim2real-gap-part-2-training-guide-e96805cd7141?source=collection_archive---------5-----------------------#2024-08-26](https://towardsdatascience.com/aws-deepracer-a-practical-guide-to-reducing-the-sim2real-gap-part-2-training-guide-e96805cd7141?source=collection_archive---------5-----------------------#2024-08-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/aws-deepracer-a-practical-guide-to-reducing-the-sim2real-gap-part-2-training-guide-e96805cd7141?source=collection_archive---------5-----------------------#2024-08-26](https://towardsdatascience.com/aws-deepracer-a-practical-guide-to-reducing-the-sim2real-gap-part-2-training-guide-e96805cd7141?source=collection_archive---------5-----------------------#2024-08-26)
- en: How to select action space, reward function, and training paradigm for different
    vehicle behaviors
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何为不同的车辆行为选择动作空间、奖励函数和训练范式
- en: '[](https://shrey-pareek.medium.com/?source=post_page---byline--e96805cd7141--------------------------------)[![Shrey
    Pareek, PhD](../Images/e1169ff2f538e8bc9f64c6f591bf1f80.png)](https://shrey-pareek.medium.com/?source=post_page---byline--e96805cd7141--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e96805cd7141--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e96805cd7141--------------------------------)
    [Shrey Pareek, PhD](https://shrey-pareek.medium.com/?source=post_page---byline--e96805cd7141--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://shrey-pareek.medium.com/?source=post_page---byline--e96805cd7141--------------------------------)[![Shrey
    Pareek, PhD](../Images/e1169ff2f538e8bc9f64c6f591bf1f80.png)](https://shrey-pareek.medium.com/?source=post_page---byline--e96805cd7141--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e96805cd7141--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e96805cd7141--------------------------------)
    [Shrey Pareek, PhD](https://shrey-pareek.medium.com/?source=post_page---byline--e96805cd7141--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e96805cd7141--------------------------------)
    ·11 min read·Aug 26, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e96805cd7141--------------------------------)
    ·11分钟阅读·2024年8月26日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'This article describes how to train the AWS DeepRacer to drive safely around
    a track without crashing. The goal is not to train the fastest car (although I
    will discuss that briefly), but to train a model that can learn to stay on the
    track and navigate turns. Video below shows the so called “safe” model:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本文描述了如何训练AWS DeepRacer在赛道上安全驾驶而不发生碰撞。目标并不是训练最快的赛车（尽管我会简要讨论这一点），而是训练一个可以学习保持在赛道上并顺利过弯的模型。下方的视频展示了所谓的“安全”模型：
- en: DeepRacer tries to stay on track by following the center line. Video by author.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: DeepRacer尝试通过跟随中心线保持在赛道上。视频由作者提供。
- en: '[**Part 1 (08/20/2024)**](/aws-deepracer-a-practical-guide-to-reducing-the-sim2real-gap-part-1-580fb1244229)
    : Track and surrounding environment setup.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**第一部分（2024年8月20日）**](/aws-deepracer-a-practical-guide-to-reducing-the-sim2real-gap-part-1-580fb1244229)：赛道和周围环境设置。'
- en: '[**Part 2 (08/26/2024)**](https://medium.com/towards-data-science/aws-deepracer-a-practical-guide-to-reducing-the-sim2real-gap-part-2-training-guide-e96805cd7141):
    Action space and reward function design along with training paradigm.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**第二部分（2024年8月26日）**](https://medium.com/towards-data-science/aws-deepracer-a-practical-guide-to-reducing-the-sim2real-gap-part-2-training-guide-e96805cd7141):
    动作空间和奖励函数设计，以及训练范式。'
- en: '**Link to GitRepo :** [**https://github.com/shreypareek1991/deepracer-sim2rea**](https://github.com/shreypareek1991/deepracer-sim2real/tree/main)**l**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**GitRepo链接：** [**https://github.com/shreypareek1991/deepracer-sim2rea**](https://github.com/shreypareek1991/deepracer-sim2real/tree/main)**l**'
- en: '[In Part 1](https://shrey-pareek.medium.com/aws-deepracer-a-practical-guide-to-reducing-the-sim2real-gap-part-1-580fb1244229),
    I described how to prepare the track and the surrounding environment to maximize
    chances of successfully completing multiple laps using the DeepRacer. If you haven’t
    read Part 1, I strongly urge you to read it as it forms the basis of understanding
    physical factors that affect the DeepRacer’s performance.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第一部分](https://shrey-pareek.medium.com/aws-deepracer-a-practical-guide-to-reducing-the-sim2real-gap-part-1-580fb1244229)中，我描述了如何准备赛道和周围环境，以最大化成功完成多圈驾驶DeepRacer的机会。如果你还没有阅读第一部分，我强烈建议你阅读它，因为它是理解影响DeepRacer性能的物理因素的基础。
- en: I initially used [this guide](https://medium.com/@marsmans/how-i-got-into-the-top-2-in-aws-deepracer-32127a364212)
    from [Sam Marsman](https://medium.com/u/c7e8170f7240?source=post_page---user_mention--e96805cd7141--------------------------------)
    as a starting point. It helped me train fast sim models, but they had a low success
    rate on the track. That being said, I would **highly recommend** reading their
    blog as it provides incredible advice on how to incrementally train your model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我最初使用了[这篇指南](https://medium.com/@marsmans/how-i-got-into-the-top-2-in-aws-deepracer-32127a364212)，来自[Sam
    Marsman](https://medium.com/u/c7e8170f7240?source=post_page---user_mention--e96805cd7141--------------------------------)作为起点。它帮助我快速训练模拟模型，但它们在赛道上的成功率较低。话虽如此，我**强烈推荐**阅读他们的博客，因为它提供了关于如何逐步训练模型的极佳建议。
- en: '**NOTE: We will first train a slow model, then increase speed later. The video
    at the top is a faster model that I will briefly explain towards the end.**'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：我们将首先训练一个慢速模型，然后稍后再提高速度。上方的视频是一个更快的模型，我将在最后简要解释。**'
- en: Part 1 Summary
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1部分总结
- en: 'In Part 1— we identified that the DeepRacer uses grey scale images from its
    front facing camera as input to understand and navigate its surroundings. Two
    key findings were highlighted:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1部分中，我们发现DeepRacer使用来自前置摄像头的灰度图像作为输入，以理解和导航其周围环境。我们强调了两个关键发现：
- en: 1\. DeepRacer cannot *recognize* objects, rather it learns to stay on and avoid
    certain pixel values. The car learns to stay on the *Black* track surface, avoid
    crossing *White* track boundaries, and avoid *Green* (or rather a shade of grey)
    sections of the track.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. DeepRacer无法*识别*物体，而是学会保持在某些像素值上并避免其他像素值。汽车学会保持在*黑色*赛道表面上，避免越过*白色*赛道边界，并避免进入*绿色*（或者说是一种灰色调）赛道区域。
- en: 2\. Camera is very sensitive to ambient light and background distractions.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 相机对环境光和背景干扰非常敏感。
- en: By reducing ambient lights and placing colorful barriers, we try and mitigate
    the above. Here is picture of my setup copied from Part 1.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 通过减少环境光和放置五光十色的障碍物，我们尝试缓解上述问题。下面是我从第1部分复制的设置图片。
- en: '![](../Images/b95ba4a6be0b17096e33390d316ca294.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b95ba4a6be0b17096e33390d316ca294.png)'
- en: Track and ambient setup described in Part 1\. Use of colorful barriers and reduction
    of ambient lighting are key here. Image by author.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 第1部分中描述的轨道和环境设置。使用五光十色的障碍物和减少环境光照是关键。图像由作者提供。
- en: Training
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练
- en: I won’t go into the details of Reinforcement Learning or the DeepRacer training
    environment in this article. There are numerous articles and [guides from AWS](https://docs.aws.amazon.com/deepracer/latest/developerguide/deepracer-get-started.html)
    that cover this.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我不会详细讨论强化学习或DeepRacer训练环境。已有许多文章和来自[AWS的指南](https://docs.aws.amazon.com/deepracer/latest/developerguide/deepracer-get-started.html)涵盖了这一内容。
- en: Very briefly, Reinforcement Learning is a technique where an autonomous agent
    seeks to learn an optimal policy that **maximizes a scalar reward**. In other
    words, the agent learns a set of situation-based actions that would maximize a
    reward. Actions that lead to *desirable outcomes* are (usually) awarded a *positive
    reward*. Conversely, *unfavorable actions* are either *penalized* (negative reward)
    or awarded a small positive reward.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，强化学习是一种技术，智能体试图学习一个**最大化标量奖励**的最优策略。换句话说，智能体学习一组基于情况的动作，以最大化奖励。那些导致*理想结果*的动作通常会得到*正向奖励*。相反，*不利的动作*则会被*惩罚*（负向奖励）或给予小额正向奖励。
- en: 'Instead, my goal is to focus on providing you a training strategy that will
    maximize the chances of your car navigating the track without crashing. I will
    look at five things:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我的目标是提供一种训练策略，最大化汽车不发生碰撞地完成赛道的机会。我将从五个方面进行探讨：
- en: Track — Clockwise and Counterclockwise orientation
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 赛道 — 顺时针和逆时针方向
- en: Hyperparameters — Reducing learning rates
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 超参数 — 降低学习率
- en: Action Space
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 动作空间
- en: Reward Function
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 奖励函数
- en: Training Paradigm/Cloning Models
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练范式/克隆模型
- en: Track
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 赛道
- en: Ideally you want to use the **same track** in the sim as in real life. I used
    the [A To Z Speedway](https://www.amazon.com/Speedway-Printed-Track-DeepRacer-Matte/dp/B0BT8CGKTP).
    Additionally, for the best performance, you want to iteratively train on a **clockwise
    and counter clockwise** orientation to *minimize* effects of over training.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，你希望在模拟环境中使用与现实中相同的赛道。我使用了[A To Z Speedway](https://www.amazon.com/Speedway-Printed-Track-DeepRacer-Matte/dp/B0BT8CGKTP)。此外，为了获得最佳性能，你需要反复在**顺时针和逆时针**方向上进行训练，以*最小化*过度训练的影响。
- en: Hyperparameters
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数
- en: I used the defaults from AWS to train the first few models. **Reduce learning
    rate by half every 2–3 iterations** so that you can fine tune a previous best
    model.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了 AWS 的默认设置来训练前几个模型。**每 2–3 次迭代将学习率减半**，这样你可以对之前的最佳模型进行微调。
- en: Action Space
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 行动空间
- en: This refers to a set of actions that DeepRacer can take to navigate an environment.
    Two actions are available — **steering angle (degrees) and throttle (m/s).**
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这指的是 DeepRacer 为了在环境中导航而可以采取的一组动作。可以选择两种动作 — **转向角度（度）和油门（米/秒）**。
- en: I would recommend using the **discrete action space** instead of continuous.
    Although the continuous action space leads to a smoother and faster behavior,
    it takes longer to train and training costs will add up quickly. Additionally,
    the discrete action space provides more control over executing a particular behavior.
    Eg. Slower speed on turns.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议使用**离散行动空间**而不是连续行动空间。虽然连续行动空间可以实现更平滑、更快速的行为，但它需要更长的训练时间，训练成本也会迅速增加。此外，离散行动空间提供了更多对特定行为执行的控制。例如，转弯时的较慢速度。
- en: Start with the following action space. The maximum forward speed of the DeepRacer
    is 4m/s, but we will **start off with much lower speeds.** You can increase this
    later (I will show how). **Remember, our first goal is to simply drive around
    the track.**
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从以下行动空间开始。DeepRacer 的最大前进速度为 4 米/秒，但我们将**从更低的速度开始**。你可以稍后增加这一速度（我将展示如何增加）。**记住，我们的第一个目标是仅仅绕着赛道行驶。**
- en: '**Slow and Steady Action Space**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**慢而稳的行动空间**'
- en: Slow and Steady model that requires nudges from human but stays on track. Video
    by author.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 慢而稳的模型，需要人类的轻推，但能够保持在轨道上。视频由作者提供。
- en: First, we will train a model that is very slow but goes around the track without
    leaving it. **Don’t worry** if the car keeps getting stuck. You may have to give
    it small pushes, but as long as it can do a lap — *you are on the right track*
    (pun intended). Ensure that **Advanced Configuration** is selected.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将训练一个非常慢的模型，但它可以绕着赛道行驶而不偏离轨道。**不用担心**如果汽车不断卡住。你可能需要给它一些小推力，但只要它能完成一圈 —
    *你就走在正确的道路上*（双关语）。确保选中了**高级配置**。
- en: '![](../Images/9c6f1d00d3279270e1d787ce8ff1de96.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c6f1d00d3279270e1d787ce8ff1de96.png)'
- en: Discrete Action space for a slow and steady model. Image by author.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 慢而稳模型的离散行动空间。图片由作者提供。
- en: Reward Function
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奖励函数
- en: The reward function is arguably the most crucial factor and accordingly — the
    most challenging aspect of reinforcement learning. It governs the behaviors your
    agent will learn and should be designed very carefully. Yes, the choice of your
    learning model, hyperparameters, etc. do affect the overall agent behavior — but
    they rely on your reward function.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励函数可以说是最关键的因素，因此也是强化学习中最具挑战性的部分。它决定了智能体将学习到的行为，因此必须非常小心地设计。是的，你选择的学习模型、超参数等确实会影响智能体的整体行为
    — 但它们都依赖于你的奖励函数。
- en: The key to designing a good reward function is to list out the behaviors you
    want your agent to execute and then think about how these behaviors would interact
    with each other and the environment. Of course, you cannot account for all possible
    behaviors or interactions, and even if you can — the agent might learn a completely
    different policy.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 设计一个好的奖励函数的关键是列出你希望智能体执行的行为，然后思考这些行为如何相互作用以及与环境的互动。当然，你无法涵盖所有可能的行为或互动，甚至即使能
    — 智能体可能会学到完全不同的策略。
- en: Now let’s list out the desired behaviors we want our car to execute and their
    corresponding reward function in Python. I will first provide reward functions
    for **each behavior individually** and then **Put it All Together** later**.**
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们列出我们希望汽车执行的期望行为以及它们对应的 Python 奖励函数。我将首先为**每个行为单独**提供奖励函数，然后稍后**将它们汇总在一起**。
- en: '**Behavior 1 — Drive On Track**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**行为 1 — 保持在轨道上行驶**'
- en: 'This one is easy. We want our car to stay on the track and avoid going outside
    the white lines. We achieve this using two sub-behaviors:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点很简单。我们希望汽车保持在赛道上，避免驶出白色线条。我们通过以下两个子行为来实现这一点：
- en: '**#1 Stay Close to Center Line:** Closer the car is to the center of the track,
    lower is the chance of a collision. To do this, we award a large positive reward
    when the car is close to the center and a smaller positive reward when it is further
    away. We award a small positive reward because being away from the center is not
    necessarily a bad thing as long as the car stays within the track.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**#1 靠近中心线行驶：** 汽车越靠近赛道中心，发生碰撞的几率就越小。为此，当汽车靠近中心时，我们会给予较大的正奖励，而当它远离时，则给予较小的正奖励。我们会给予一个较小的正奖励，因为只要汽车仍在赛道内，偏离中心不一定是坏事。'
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**#2 Keep All 4 Wheels on Track:** In racing, lap times are deleted if all
    four wheels of a car veer off track. To this end, we apply a **large negative
    penalty** if all four wheel are off track.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**#2 保持所有四个车轮在赛道上：** 在赛车中，如果汽车的四个车轮都偏离赛道，则圈速会被删除。为此，我们会对四个车轮都偏离赛道的情况施加**大幅负惩罚**。'
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Our hope here is that using a combination of the above sub-behaviors, our agent
    will learn that staying close to the center of the track is a desirable behavior
    while veering off leads to a penalty.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里的期望是，通过结合上述子行为，我们的代理会学会靠近赛道中心是一种期望的行为，而偏离中心会导致惩罚。
- en: '**Behavior 2 — Slow Down for Turns**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**行为2 — 转弯时减速**'
- en: 'As in real life, we want our vehicle to slow down while navigating turns. Additionally,
    sharper the turn, slower the desired speed. We do this by:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 就像现实生活中一样，我们希望车辆在转弯时减速。此外，转弯越急，期望的速度越慢。我们通过以下方式实现：
- en: Providing a large positive reward such that if the steering angle is high (i.e.
    sharp turn) speed is lower than a threshold.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供一个较大的正奖励，使得如果转向角度较大（即急转弯），速度低于某个阈值时，给予奖励。
- en: Providing a smaller positive reward is high steering angle is accompanied by
    a speed greater than a threshold.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供一个较小的正奖励，当转向角度较大且速度超过某个阈值时。
- en: '***Unintended Zigzagging Behavior:***Reward function design is a subtle balancing
    art. There is no free lunch. Attempting to train certain desired behavior may
    lead to unexpected and undesirable behaviors. In our case, by forcing the agent
    to stay close to the center line, our agent will learn a *zigzagging* policy.
    Anytime it veers away from the center, it will try to correct itself by steering
    in the opposite direction and the cycle will continue. We can reduce this by **penalizing
    extreme steering angles** by multiplying the final reward by 0.85 (i.e. a 15%
    reduction).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '***非故意的曲折行为：***奖励函数设计是一种微妙的平衡艺术。没有免费的午餐。尝试训练某些期望的行为可能会导致意想不到和不希望发生的行为。在我们的案例中，通过强制代理靠近中心线，我们的代理将学会一种*曲折行驶*的策略。每当它偏离中心时，它会尝试通过向相反方向转向来纠正自己，循环会继续。我们可以通过**惩罚极端的转向角度**来减少这种情况，将最终奖励乘以0.85（即减少15%）。'
- en: On a side note, this can also be achieved by tracking change in steering angle
    and penalizing large and sudden changes. I am not sure if DeepRacer API provides
    access to previous states to design such a reward function.
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 顺便提一下，这也可以通过跟踪转向角度的变化并惩罚大的突然变化来实现。我不确定DeepRacer API是否提供访问先前状态的功能，以设计这样的奖励函数。
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Putting it All Together**'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**把所有内容整合起来**'
- en: Next, we combine all the above to get our final reward function. [Sam Marsman](https://medium.com/u/c7e8170f7240?source=post_page---user_mention--e96805cd7141--------------------------------)’s
    guide recommends training additional behaviors incrementally by training a model
    to learn one reward and then adding others. You can try this approach. In my case,
    it did not make too much of a difference.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将以上所有内容结合起来，得到我们的最终奖励函数。[Sam Marsman](https://medium.com/u/c7e8170f7240?source=post_page---user_mention--e96805cd7141--------------------------------)的指南建议通过训练模型逐步学习一个奖励函数，然后再加入其他奖励，以逐步训练附加行为。你可以尝试这种方法。在我的情况下，这并没有带来太大变化。
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Training Paradigm/Model Cloning
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练范式/模型克隆
- en: 'The key to training a successful model is to iteratively clone and improve
    an existing model. In other words, instead of training one model for 10 hours,
    you want to:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 训练成功模型的关键在于迭代地克隆和改进现有模型。换句话说，与其训练一个模型10小时，不如：
- en: train an initial model for a **couple** of hours
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练一个初始模型**几个小时**
- en: clone the **best** model
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 克隆**最佳**模型
- en: train for an **hour** or so
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练**一个小时**左右
- en: clone **best** model
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 克隆**最佳**模型
- en: repeat till you get **reliable** 100 percent completion during validation
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复，直到在验证过程中获得**可靠的** 100% 完成度
- en: '**switch** between clockwise and counter clockwise track direction for every
    training iteration'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每次训练迭代中，**切换**时针方向和逆时针方向的赛道方向
- en: '**reduce** the learning rate by half every 2–3 iterations'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每2-3次迭代将学习率减少一半**'
- en: You are looking for a reward graph that looks something like this. It’s okay
    if you do not achieve 100% completion every time. Consistency is key here.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 你要寻找的奖励图形应该像这样。如果你每次都没有达到100%的完成度也是可以的。关键是保持一致性。
- en: '![](../Images/d0489c8d37955b5b8028b610102e75f0.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d0489c8d37955b5b8028b610102e75f0.png)'
- en: Desired reward and percent completion behavior. Image by author.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 想要的奖励和完成百分比行为。图片来源：作者。
- en: Test, Retrain, Test, Retrain, Repeat
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试、重新训练、测试、重新训练、重复
- en: Machine Learning and Robotics are all about iterations. There is no *one-size-fits-all*
    solution. So you will have to experiment.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习和机器人技术都讲究反复迭代。没有*一刀切*的解决方案。所以你需要进行实验。
- en: (Bonus) Training a Faster Model
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: （附加内容）训练一个更快的模型
- en: Once your car can navigate the track safely (even if it needs some pushes),
    you can increase the speed in the action space and the reward functions.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你的车能够安全地绕过赛道（即使它有时需要推一下），你就可以在动作空间和奖励函数中提高车速。
- en: The video at the top of this page was created using the following action space
    and reward function.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 本页面顶部的视频是使用以下动作空间和奖励函数创建的。
- en: '![](../Images/18f4ce0a5f4d3a92edeb6f6e4ec8cdd5.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18f4ce0a5f4d3a92edeb6f6e4ec8cdd5.png)'
- en: Action space for faster speeds around the track while maintaining safety. Image
    by author.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个在保持安全的情况下，可以让车速更快的动作空间。图片来源：作者。
- en: '[PRE4]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Fast but Crashy Model — Use at your Own Risk
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速但易撞的模型——使用时请自担风险
- en: '[The video showed in Part 1](https://youtu.be/DRz1IV_g-Mg) of this series was
    trained to prefer speed. No penalties were applied for going off track or crashing.
    Instead a very small positive reward was awared. This led to a fast model that
    was able to do a time of `10.337s` in the sim. In practice, it would crash a lot
    but when it managed to complete a lap, it was very satisfying.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[本系列第一部分中展示的视频](https://youtu.be/DRz1IV_g-Mg)经过训练后偏向于速度。没有对离开赛道或碰撞进行惩罚，反而给予了非常小的正向奖励。这导致了一个快速的模型，在模拟中完成了`10.337秒`的时间。在实际操作中，它会频繁碰撞，但当它成功完成一圈时，十分令人满足。'
- en: Here is the action space and reward in case you want to give it a try.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你可以尝试的动作空间和奖励。
- en: '![](../Images/eaaca296d6df0cd6660a37ea6bdce785.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eaaca296d6df0cd6660a37ea6bdce785.png)'
- en: Action space for fastest lap times I could manage. The car does crash a lot
    while using this. Image by author.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我能够管理的最快圈速的动作空间。使用这个空间时，车子会经常发生碰撞。图片来源：作者。
- en: '[PRE5]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Conclusion
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In conclusion, remember two things.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，记住两件事。
- en: Start by training a slow model that can successfully navigate the track, even
    if you need to push the car a bit at times. Once this is done, you can experiment
    with increasing the speed in your action space. As in real life, baby steps first.
    You can also gradually increase throttle percentage from **50 to 100%** using
    the DeepRacer control UI to manage speeds. **In my case 95% throttle worked best.**
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从训练一个能成功绕过赛道的慢速模型开始，即使你有时需要推一下车子。完成这个后，你可以尝试在动作空间中增加车速。就像现实生活中一样，先从小步走开始。你也可以通过DeepRacer控制界面逐渐增加油门百分比，从**50%到100%**，以管理车速。**在我的情况下，95%的油门效果最好。**
- en: Train your model incrementally. Start with a couple of hours of training, then
    switch track direction (clockwise/counter clockwise) and gradually reduce training
    times to one hour. You may also reduce the learning rate by half every 2–3 iteration
    to hone and improve a previous best model.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 逐步训练你的模型。从几个小时的训练开始，然后切换赛道方向（顺时针/逆时针），逐渐将训练时间减少到一个小时。你还可以每进行2-3次迭代，就将学习率减半，以精炼并改进以前的最佳模型。
- en: Finally, you will have to **reiterate multiple times** based on your physical
    setup. In my case I trained *100+ models*. Hopefully with this guide you can get
    similar results with *15–20* instead*.*
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你必须根据你的物理设置**多次重复**。在我的案例中，我训练了*100+个模型*。希望通过本指南，你能够通过*15-20个模型*得到类似的结果。*
- en: Thanks for reading.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读。
