- en: 'ML Engineering 101: A Thorough Explanation of The Error “DataLoader worker
    (pid(s) xxx) exited unexpectedly”'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习工程 101：对错误“DataLoader worker (pid(s) xxx) exited unexpectedly”的全面解释
- en: 原文：[https://towardsdatascience.com/ml-engineering-101-a-thorough-explanation-of-the-error-dataloader-worker-pid-s-xxx-exited-f3a6a983911e?source=collection_archive---------6-----------------------#2024-06-03](https://towardsdatascience.com/ml-engineering-101-a-thorough-explanation-of-the-error-dataloader-worker-pid-s-xxx-exited-f3a6a983911e?source=collection_archive---------6-----------------------#2024-06-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/ml-engineering-101-a-thorough-explanation-of-the-error-dataloader-worker-pid-s-xxx-exited-f3a6a983911e?source=collection_archive---------6-----------------------#2024-06-03](https://towardsdatascience.com/ml-engineering-101-a-thorough-explanation-of-the-error-dataloader-worker-pid-s-xxx-exited-f3a6a983911e?source=collection_archive---------6-----------------------#2024-06-03)
- en: A deep dive into PyTorch DataLoader with Multiprocessing
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入探讨 PyTorch DataLoader 与多进程
- en: '[](https://mengliuz.medium.com/?source=post_page---byline--f3a6a983911e--------------------------------)[![Mengliu
    Zhao](../Images/0b950a0785fa065db3319ed5be4a91de.png)](https://mengliuz.medium.com/?source=post_page---byline--f3a6a983911e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f3a6a983911e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f3a6a983911e--------------------------------)
    [Mengliu Zhao](https://mengliuz.medium.com/?source=post_page---byline--f3a6a983911e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mengliuz.medium.com/?source=post_page---byline--f3a6a983911e--------------------------------)[![赵梦柳](../Images/0b950a0785fa065db3319ed5be4a91de.png)](https://mengliuz.medium.com/?source=post_page---byline--f3a6a983911e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f3a6a983911e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f3a6a983911e--------------------------------)
    [赵梦柳](https://mengliuz.medium.com/?source=post_page---byline--f3a6a983911e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f3a6a983911e--------------------------------)
    ·6 min read·Jun 3, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f3a6a983911e--------------------------------)
    ·阅读时长 6 分钟·2024年6月3日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: As one of the many who use the PyTorch library on a day-to-day basis, I believe
    many ML engineer sooner or later encounters the problem “DataLoader worker (pid(s)
    xxx) exited unexpectedly” during training.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 作为日常使用 PyTorch 库的众多用户之一，我相信许多机器学习工程师在训练过程中迟早会遇到“DataLoader worker (pid(s) xxx)
    exited unexpectedly”这个问题。
- en: It’s frustrating.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这令人沮丧。
- en: This error is often triggered when calling the DataLoader with parameter *num_workers
    > 0*. Many online posts provide simple solutions like setting the num_workers=0,
    which makes the current issue go away but causes problems new in reality.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 *num_workers > 0* 调用 DataLoader 时，通常会触发这个错误。许多在线帖子提供了简单的解决方案，比如将 num_workers
    设置为 0，这样当前的问题就会消失，但实际上会引发新的问题。
- en: This post will show you some tricks that may help resolve the problem. I’m going
    to do a deeper dive into the Torch.multiprocessing module and show you some useful
    virtual memory monitoring and leakage-preventing techniques. In a really rare
    case, the asynchronous memory occupation and release of the torch.multiprocessing
    workers could still trigger the issue, even without leakage. The ultimate solution
    is to optimize the virtual memory usage and understand the torch.multiprocessing
    behaviour, and perform garbage collection in the __getitem_ method.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将向你展示一些可能有助于解决这个问题的技巧。我将深入探讨 Torch.multiprocessing 模块，并展示一些有用的虚拟内存监控和泄漏防止技术。在极少数情况下，即使没有内存泄漏，torch.multiprocessing
    工作进程的异步内存占用和释放仍然可能触发该问题。最终解决方案是优化虚拟内存的使用，理解 torch.multiprocessing 的行为，并在 __getitem__
    方法中进行垃圾回收。
- en: 'Note: the platform I worked on is Ubuntu 20.04\. To adapt to other platforms,
    many terminal commands need to be changed.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我使用的平台是 Ubuntu 20.04。为了适应其他平台，许多终端命令需要做相应的调整。
- en: '![](../Images/506f6a7810c2c2408d0d19a4c61dc831.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/506f6a7810c2c2408d0d19a4c61dc831.png)'
- en: 'Image source: [https://pxhere.com/en/photo/1379760#google_vignette](https://pxhere.com/en/photo/1379760#google_vignette)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[https://pxhere.com/en/photo/1379760#google_vignette](https://pxhere.com/en/photo/1379760#google_vignette)
- en: '**Brute-force Solution and the Cons**'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**暴力解决方案及其缺点**'
- en: If you search on the web, most people encountering the same issue will tell
    you the brute-force solution; just set num_workers=0 in the DataLoader, and the
    issue will be gone.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在网上搜索，大多数遇到相同问题的人都会告诉你暴力解决方案；只需将 DataLoader 中的 num_workers 设置为 0，问题就会消失。
- en: It will be the easiest solution if you have a small dataset and can tolerate
    the training time. However, the underlying issue is still there, and if you have
    a very large dataset, setting *num_workers=0* will result in a very slow performance,
    sometimes 10x slower. That’s why we must look into the issue further and seek
    alternative solutions.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的数据集较小，并且能容忍较长的训练时间，这将是最简单的解决方案。然而，根本问题依然存在，如果你有一个非常大的数据集，设置 *num_workers=0*
    将导致非常慢的性能，有时会慢 10 倍。因此，我们必须进一步研究问题并寻找替代解决方案。
- en: '**Monitor Your Virtual Memory Usage**'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**监控你的虚拟内存使用情况**'
- en: What exactly happens when the dataloader worker exits?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当 DataLoader 工作进程退出时，究竟发生了什么？
- en: To catch the last error log in the system, run the following command in the
    terminal, which will give you a more detailed error message.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 要捕捉系统中的最后一个错误日志，请在终端中运行以下命令，它将为你提供更详细的错误信息。
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Usually, you’ll see the real cause is “out of memory”. But why is there an out-of-memory
    issue? What specifically caused the extra memory consumption?
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你会看到真实的原因是“内存不足”。但是为什么会出现内存不足的问题呢？具体是什么导致了额外的内存消耗？
- en: When we set *num_workers =0* in the DataLoader, a single main process runs the
    training script. It will run properly as long as the data batch can fit into memory.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在 DataLoader 中设置 *num_workers=0* 时，只有一个主进程运行训练脚本。只要数据批次能够装入内存，它就会正常运行。
- en: However, when setting *num_workers > 0*, things become different. DataLoader
    will start child processes alongside preloading *prefetch_factor*num_workers*
    into the memory to speed things up. By default, *prefetch_factor = 2*. The prefetched
    data will consume the machine’s virtual memory (but the good news is that it doesn’t
    eat up GPUs, so you don’t need to shrink the batch size). So, the first thing
    we need to do is to monitor the system’s virtual memory usage.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当设置 *num_workers > 0* 时，情况就不一样了。DataLoader 会启动子进程，并将 *prefetch_factor*num_workers*
    的数据预加载到内存中以加速训练。默认情况下，*prefetch_factor = 2*。预加载的数据将消耗机器的虚拟内存（但好消息是它不会占用 GPU，因此你不需要缩小批次大小）。所以，我们需要做的第一件事是监控系统的虚拟内存使用情况。
- en: One of the easiest ways to monitor virtual memory usage is the *psutil* package,
    which will monitor the percentage of virtual memory being used
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 监控虚拟内存使用情况的最简单方法之一是使用 *psutil* 包，它将监控虚拟内存的使用百分比
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You can also use the tracemalloc package, which will give you more detailed
    information:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用 tracemalloc 包，它将为你提供更详细的信息：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'When the actual RAM is full, idle data will flow into the swap space (so it’s
    part of your virtual memory). To check the swap, use the command:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当实际的 RAM 满时，空闲的数据将流入交换空间（因此它是你虚拟内存的一部分）。要检查交换空间，使用以下命令：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'And to change your swap size temporarily during training (e.g., increase to
    16G) in the terminal:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间临时更改交换空间大小（例如，增加到 16G），在终端中执行以下命令：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*/dev/shm* (or, in certain cases, */run/shm* ) is another file system for storing
    temporary files, which should be monitored. Simply run the following, and you
    will see the list of drives in your file system:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*/dev/shm*（或者在某些情况下，*/run/shm*）是用于存储临时文件的另一种文件系统，应该进行监控。只需运行以下命令，你将看到文件系统中的驱动器列表：'
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To resize it temporarily (e.g., increase to 16GB), simply run:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要临时调整它的大小（例如，增加到 16GB），只需运行：
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Torch.multiprocessing Best Practices**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**Torch.multiprocessing 最佳实践**'
- en: However, virtual memory is only one side of the story. What if the issue doesn’t
    go away after adjusting the swap disk?
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，虚拟内存只是问题的一部分。如果在调整交换磁盘后问题仍然存在怎么办？
- en: 'The other side of the story is the underlying issues of the torch.multiprocessing
    module. There are a number of best practices recommendations on the official webpage:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 问题的另一面是 torch.multiprocessing 模块的底层问题。官方网页上有许多最佳实践建议：
- en: '[## Multiprocessing best practices - PyTorch 2.3 documentation'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 多进程最佳实践 - PyTorch 2.3 文档](https://pytorch.org/docs/stable/notes/multiprocessing.html?source=post_page-----f3a6a983911e--------------------------------#)'
- en: torch.multiprocessing is a drop in replacement for Python's module. It supports
    the exact same operations, but extends…
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: torch.multiprocessing 是 Python 模块的直接替代品。它支持完全相同的操作，但进行了扩展……
- en: pytorch.org](https://pytorch.org/docs/stable/notes/multiprocessing.html?source=post_page-----f3a6a983911e--------------------------------#)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[pytorch.org](https://pytorch.org/docs/stable/notes/multiprocessing.html?source=post_page-----f3a6a983911e--------------------------------#)'
- en: But besides these, three more approaches should be considered, especially regarding
    memory usage.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些，还应该考虑另外三种方法，特别是关于内存使用的方面。
- en: '**The first thing is shared memory leakage**. Leakage means that memory is
    not released properly after each run of the child worker, and you will observe
    this phenomenon when you monitor the virtual memory usage at runtime. Memory consumption
    will keep increasing and reach the point of being “out of memory.” This is a very
    typical memory leakage.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**第一个问题是共享内存泄漏**。泄漏意味着每次子进程运行后，内存没有正确释放，你可以通过监控运行时的虚拟内存使用情况来观察这个现象。内存消耗会不断增加，直到达到“内存不足”的程度。这是典型的内存泄漏现象。'
- en: So what will cause the leakage?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，是什么导致了内存泄漏呢？
- en: 'Let’s take a look at the DataLoader class itself:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看DataLoader类本身：
- en: '[https://github.com/pytorch/pytorch/blob/main/torch/utils/data/dataloader.py](https://github.com/pytorch/pytorch/blob/main/torch/utils/data/dataloader.py)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/pytorch/pytorch/blob/main/torch/utils/data/dataloader.py](https://github.com/pytorch/pytorch/blob/main/torch/utils/data/dataloader.py)'
- en: 'Looking under the hood of DataLoader, we’ll see that when nums_worker > 0,
    _MultiProcessingDataLoaderIter is called. Inside _MultiProcessingDataLoaderIter,
    Torch.multiprocessing creates the worker queue. Torch.multiprocessing uses two
    different strategies for memory sharing and caching: *file_descriptor* and *file_system.*
    While *file_system* requires no file descriptor caching, it is prone to shared
    memory leaks.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 查看DataLoader的内部结构时，我们会看到，当`nums_worker > 0`时，调用的是`_MultiProcessingDataLoaderIter`。在`_MultiProcessingDataLoaderIter`内部，Torch.multiprocessing创建了工作队列。Torch.multiprocessing使用两种不同的内存共享和缓存策略：*file_descriptor*和*file_system*。虽然*file_system*不需要文件描述符缓存，但它容易导致共享内存泄漏。
- en: 'To check what sharing strategy your machine is using, simply add in the script:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查你的机器使用的是哪种共享策略，只需在脚本中添加以下内容：
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'To get your system file descriptor limit (Linux), run the following command
    in the terminal:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取系统文件描述符限制（Linux），请在终端运行以下命令：
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To switch your sharing strategy to *file_descriptor*:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要将共享策略切换为*file_descriptor*：
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To count the number of opened file descriptors, run the following command:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 要统计已打开的文件描述符数量，请运行以下命令：
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As long as the system allows, the *file_descriptor* strategy is recommended.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 只要系统允许，建议使用*file_descriptor*策略。
- en: '**The second is the multiprocessing worker starting method.** Simply put, it’s
    the debate as to whether to use a fork or spawn as the worker-starting method.
    Fork is the default way to start multiprocessing in Linux and can avoid certain
    file copying, so it is much faster, but it might have issues handling CUDA tensors
    and third-party libraries like OpenCV in your DataLoader.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**第二个问题是多进程工作者启动方法**。简而言之，这是关于是否使用fork或spawn作为工作者启动方法的争论。Fork是Linux中启动多进程的默认方式，可以避免某些文件复制，因此速度更快，但在处理CUDA张量和第三方库（如OpenCV）时，可能会遇到问题。'
- en: To use the spawn method, you can simply pass the argument *multiprocessing_context=*
    *“spawn”*. to the DataLoader.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用spawn方法，你只需将参数`*multiprocessing_context=* “spawn”`传递给DataLoader。
- en: '**Three, make the Dataset Objects Pickable/Serializable**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**第三，确保数据集对象可序列化/可拾取**'
- en: 'There is a super nice post further discussing the “copy-on-read” effect for
    process folding: [https://ppwwyyxx.com/blog/2022/Demystify-RAM-Usage-in-Multiprocess-DataLoader/](https://ppwwyyxx.com/blog/2022/Demystify-RAM-Usage-in-Multiprocess-DataLoader/)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 有一篇非常不错的文章进一步讨论了进程折叠中的“按需复制”效应：[https://ppwwyyxx.com/blog/2022/Demystify-RAM-Usage-in-Multiprocess-DataLoader/](https://ppwwyyxx.com/blog/2022/Demystify-RAM-Usage-in-Multiprocess-DataLoader/)
- en: 'Simply put, it’s **no longer a good approach** to create a list of filenames
    and load them in the __getitem__ method. Create a numpy array or panda dataframe
    to store the list of filenames for serialization purposes. And if you’re familiar
    with HuggingFace, using a CSV/dataframe is the recommended way to load a local
    dataset: [https://huggingface.co/docs/datasets/v2.19.0/en/package_reference/loading_methods#datasets.load_dataset.example-2](https://huggingface.co/docs/datasets/v2.19.0/en/package_reference/loading_methods#datasets.load_dataset.example-2)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，**不再推荐**在`__getitem__`方法中创建文件名列表并加载它们。可以创建一个numpy数组或pandas数据框来存储文件名列表，以便进行序列化。如果你熟悉HuggingFace，使用CSV/数据框是加载本地数据集的推荐方法：[https://huggingface.co/docs/datasets/v2.19.0/en/package_reference/loading_methods#datasets.load_dataset.example-2](https://huggingface.co/docs/datasets/v2.19.0/en/package_reference/loading_methods#datasets.load_dataset.example-2)
- en: '**What If You Have a Really Slow Loader?**'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果你的数据加载器非常慢怎么办？**'
- en: Okay, now we have a better understanding of the multiprocessing module. But
    is it the end of the story?
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们对多进程模块有了更好的理解。但这就是故事的结局吗？
- en: It sounds really crazy. If you have a large and heavy dataset (e.g., each data
    point > 5 MB), there is a weird chance of encountering the above issues, and I’ll
    tell you why. The secret is the asynchronous memory release of the multiprocessing
    workers.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来真的很疯狂。如果你有一个大且重的数据集（例如，每个数据点 > 5 MB），就有可能遇到上述问题，我将告诉你为什么。秘密就在于多进程工作进程的异步内存释放。
- en: 'The trick is simple: hack into the torch library and add a *psutil.virtual_memory().percent*
    line before and after the data queue in the _MultiProcessingDataLoaderIter class:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 诀窍很简单：黑进 `torch` 库，在 _MultiProcessingDataLoaderIter 类的数据队列前后添加一行 *psutil.virtual_memory().percent*：
- en: '[](https://github.com/pytorch/pytorch/blob/70d8bc2da1da34915ce504614495c8cf19c85df2/torch/utils/data/dataloader.py?source=post_page-----f3a6a983911e--------------------------------#L1130)
    [## pytorch/torch/utils/data/dataloader.py at 70d8bc2da1da34915ce504614495c8cf19c85df2
    ·…'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/pytorch/pytorch/blob/70d8bc2da1da34915ce504614495c8cf19c85df2/torch/utils/data/dataloader.py?source=post_page-----f3a6a983911e--------------------------------#L1130)
    [## pytorch/torch/utils/data/dataloader.py 在 70d8bc2da1da34915ce504614495c8cf19c85df2
    ·…'
- en: Tensors and Dynamic neural networks in Python with strong GPU acceleration -
    pytorch/torch/utils/data/dataloader.py at…
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用强大的 GPU 加速的 Python 中的张量和动态神经网络 - pytorch/torch/utils/data/dataloader.py 在…
- en: github.com](https://github.com/pytorch/pytorch/blob/70d8bc2da1da34915ce504614495c8cf19c85df2/torch/utils/data/dataloader.py?source=post_page-----f3a6a983911e--------------------------------#L1130)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/pytorch/pytorch/blob/70d8bc2da1da34915ce504614495c8cf19c85df2/torch/utils/data/dataloader.py?source=post_page-----f3a6a983911e--------------------------------#L1130)
- en: Something like
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 类似这样的：
- en: '[PRE11]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In my case, I started my DataLoader with num_workers=8 and observed something
    like the following:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下，我将 DataLoader 的 `num_workers` 设置为 8，并观察到如下情况：
- en: '![](../Images/f36f97f1e7987c83092f6e9ea35452e5.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f36f97f1e7987c83092f6e9ea35452e5.png)'
- en: 'So the memory keeps flowing up — but is it memory leakage? Not really. It’s
    simply because the dataloader workers load faster than they release, creating
    8 jobs while releasing 2\. And that’s the root cause of the memory overflowing.
    The solution is simple: just add a garbage collector to the beginning of your
    *__getitem__* method*:*'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 所以内存不断上涨——但这算是内存泄漏吗？其实不算。问题的根本原因是 dataloader 的工作进程加载速度快于释放速度，创建了 8 个任务，却只释放了
    2 个。这就是内存溢出的根本原因。解决方案很简单：只需在你的 *__getitem__* 方法的开头添加一个垃圾回收器即可：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: And now you’re good!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经做好了准备！
- en: '![](../Images/3be6a5dc984f45eb7fb8957b5f146e25.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3be6a5dc984f45eb7fb8957b5f146e25.png)'
- en: '**References**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: '[https://pytorch.org/docs/stable/multiprocessing.html#sharing-strategies](https://pytorch.org/docs/stable/multiprocessing.html#sharing-strategies)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://pytorch.org/docs/stable/multiprocessing.html#sharing-strategies](https://pytorch.org/docs/stable/multiprocessing.html#sharing-strategies)'
- en: '[https://stackoverflow.com/questions/76491885/how-many-file-descriptors-are-open](https://stackoverflow.com/questions/76491885/how-many-file-descriptors-are-open)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://stackoverflow.com/questions/76491885/how-many-file-descriptors-are-open](https://stackoverflow.com/questions/76491885/how-many-file-descriptors-are-open)'
- en: '[https://psutil.readthedocs.io/en/latest/index.html#psutil.virtual_memory](https://psutil.readthedocs.io/en/latest/index.html#psutil.virtual_memory)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://psutil.readthedocs.io/en/latest/index.html#psutil.virtual_memory](https://psutil.readthedocs.io/en/latest/index.html#psutil.virtual_memory)'
- en: '[https://stackoverflow.com/questions/4970421/whats-the-difference-between-virtual-memory-and-swap-space](https://stackoverflow.com/questions/4970421/whats-the-difference-between-virtual-memory-and-swap-space)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://stackoverflow.com/questions/4970421/whats-the-difference-between-virtual-memory-and-swap-space](https://stackoverflow.com/questions/4970421/whats-the-difference-between-virtual-memory-and-swap-space)'
- en: '[https://ploi.io/documentation/server/change-swap-size-in-ubuntu](https://ploi.io/documentation/server/change-swap-size-in-ubuntu)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://ploi.io/documentation/server/change-swap-size-in-ubuntu](https://ploi.io/documentation/server/change-swap-size-in-ubuntu)'
- en: '[https://stackoverflow.com/questions/58804022/how-to-resize-dev-shm](https://stackoverflow.com/questions/58804022/how-to-resize-dev-shm)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://stackoverflow.com/questions/58804022/how-to-resize-dev-shm](https://stackoverflow.com/questions/58804022/how-to-resize-dev-shm)'
- en: '[https://pytorch.org/docs/stable/data.html](https://pytorch.org/docs/stable/data.html)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://pytorch.org/docs/stable/data.html](https://pytorch.org/docs/stable/data.html)'
- en: '[https://britishgeologicalsurvey.github.io/science/python-forking-vs-spawn/](https://britishgeologicalsurvey.github.io/science/python-forking-vs-spawn/)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://britishgeologicalsurvey.github.io/science/python-forking-vs-spawn/](https://britishgeologicalsurvey.github.io/science/python-forking-vs-spawn/)'
- en: '[https://stackoverflow.com/questions/64095876/multiprocessing-fork-vs-spawn](https://stackoverflow.com/questions/64095876/multiprocessing-fork-vs-spawn)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://stackoverflow.com/questions/64095876/multiprocessing-fork-vs-spawn](https://stackoverflow.com/questions/64095876/multiprocessing-fork-vs-spawn)'
