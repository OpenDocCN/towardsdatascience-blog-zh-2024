- en: TinyLlama —The Promising Generation of Powerful Smaller Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TinyLlama——强大的小型语言模型的有希望的未来
- en: 原文：[https://towardsdatascience.com/tiny-llama-a-performance-review-and-discussion-a68d68bc2826?source=collection_archive---------5-----------------------#2024-04-20](https://towardsdatascience.com/tiny-llama-a-performance-review-and-discussion-a68d68bc2826?source=collection_archive---------5-----------------------#2024-04-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/tiny-llama-a-performance-review-and-discussion-a68d68bc2826?source=collection_archive---------5-----------------------#2024-04-20](https://towardsdatascience.com/tiny-llama-a-performance-review-and-discussion-a68d68bc2826?source=collection_archive---------5-----------------------#2024-04-20)
- en: Learn about TinyLlama, a smaller language model capable of a variety of complex
    tasks with a small amount of compute
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解TinyLlama，一个小型语言模型，能够在较少的计算资源下完成多种复杂任务
- en: '[](https://oieivind.medium.com/?source=post_page---byline--a68d68bc2826--------------------------------)[![Eivind
    Kjosbakken](../Images/5f91b74428e1202fc4a176a3dd1cb1c7.png)](https://oieivind.medium.com/?source=post_page---byline--a68d68bc2826--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a68d68bc2826--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a68d68bc2826--------------------------------)
    [Eivind Kjosbakken](https://oieivind.medium.com/?source=post_page---byline--a68d68bc2826--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://oieivind.medium.com/?source=post_page---byline--a68d68bc2826--------------------------------)[![Eivind
    Kjosbakken](../Images/5f91b74428e1202fc4a176a3dd1cb1c7.png)](https://oieivind.medium.com/?source=post_page---byline--a68d68bc2826--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a68d68bc2826--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a68d68bc2826--------------------------------)
    [Eivind Kjosbakken](https://oieivind.medium.com/?source=post_page---byline--a68d68bc2826--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a68d68bc2826--------------------------------)
    ·10 min read·Apr 20, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a68d68bc2826--------------------------------)
    ·10分钟阅读·2024年4月20日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: TinyLlama is an open-source project that trains a small language model of around
    1.1B parameters. The project aims to have a language model capable of performing
    tasks a full LLM like Llama 2 can achieve but with less memory usage. This article
    will discuss how TinyLlama can be implemented and run locally on your computer.
    Furthermore, it will also discuss TinyLlama's current performance, along with
    its strengths and weaknesses.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: TinyLlama是一个开源项目，旨在训练一个约为11亿参数的小型语言模型。该项目的目标是创建一个能够完成类似Llama 2这类完整LLM所能实现任务的语言模型，但占用更少的内存。本文将讨论如何在本地计算机上实现并运行TinyLlama模型。此外，还将讨论TinyLlama当前的性能，以及其优缺点。
- en: '![](../Images/305119936432aa90313172421cf45caf.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/305119936432aa90313172421cf45caf.png)'
- en: ChatGPT’s imagination of a TinyLlama model. OpenAI. (2024). *ChatGPT* (4) [Large
    language model]. [https://chat.openai.com](https://chat.openai.com)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT对TinyLlama模型的构想。OpenAI。（2024年）。*ChatGPT*（4）[大型语言模型]。[https://chat.openai.com](https://chat.openai.com)
- en: Table of contents
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目录
- en: · [Table of contents](#a4a9)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: · [目录](#a4a9)
- en: · [Motivation](#aa74)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: · [动机](#aa74)
- en: · [Implementing the model locally](#051b)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: · [在本地实现模型](#051b)
- en: · [Testing the model](#d239)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: · [测试模型](#d239)
- en: ∘ [Fibonacci sequence](#dc58)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [斐波那契数列](#dc58)
- en: ∘ [RAG](#2509)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [RAG](#2509)
- en: ∘ [Generating dialog](#879b)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [生成对话](#879b)
- en: ∘ [Coding with TinyLlama](#b65c)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [使用TinyLlama编程](#b65c)
- en: · [My thoughts on the model](#da52)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: · [我对该模型的看法](#da52)
- en: · [Conclusion](#dc79)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: · [结论](#dc79)
- en: Motivation
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动机
- en: My motivation for writing this article is to keep up with the latest trends
    in machine learning. Though TinyLlama was released a few…
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我撰写这篇文章的动机是跟上机器学习的最新趋势。虽然TinyLlama发布已有一段时间……
