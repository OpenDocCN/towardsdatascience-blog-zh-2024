- en: What We Still Donâ€™t Understand About Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»ç„¶ä¸ç†è§£çš„æœºå™¨å­¦ä¹ é—®é¢˜
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/what-we-still-dont-understand-about-machine-learning-699e0002a057?source=collection_archive---------1-----------------------#2024-07-26](https://towardsdatascience.com/what-we-still-dont-understand-about-machine-learning-699e0002a057?source=collection_archive---------1-----------------------#2024-07-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/what-we-still-dont-understand-about-machine-learning-699e0002a057?source=collection_archive---------1-----------------------#2024-07-26](https://towardsdatascience.com/what-we-still-dont-understand-about-machine-learning-699e0002a057?source=collection_archive---------1-----------------------#2024-07-26)
- en: Machine Learning unknowns that researchers struggle to understand â€” from Batch
    Norm to what SGD hides
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æœºå™¨å­¦ä¹ ä¸­çš„æœªçŸ¥é—®é¢˜ï¼Œç ”ç©¶äººå‘˜éš¾ä»¥ç†è§£â€”â€”ä»æ‰¹é‡å½’ä¸€åŒ–åˆ°SGDéšè—çš„å¥¥ç§˜
- en: '[](https://medium.com/@itshesamsheikh?source=post_page---byline--699e0002a057--------------------------------)[![Hesam
    Sheikh](../Images/b8d5f4f285eef77634e4c1d4321580ed.png)](https://medium.com/@itshesamsheikh?source=post_page---byline--699e0002a057--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--699e0002a057--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--699e0002a057--------------------------------)
    [Hesam Sheikh](https://medium.com/@itshesamsheikh?source=post_page---byline--699e0002a057--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@itshesamsheikh?source=post_page---byline--699e0002a057--------------------------------)[![Hesam
    Sheikh](../Images/b8d5f4f285eef77634e4c1d4321580ed.png)](https://medium.com/@itshesamsheikh?source=post_page---byline--699e0002a057--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--699e0002a057--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--699e0002a057--------------------------------)
    [Hesam Sheikh](https://medium.com/@itshesamsheikh?source=post_page---byline--699e0002a057--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--699e0002a057--------------------------------)
    Â·12 min readÂ·Jul 26, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--699e0002a057--------------------------------)
    Â·é˜…è¯»æ—¶é—´12åˆ†é’ŸÂ·2024å¹´7æœˆ26æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/1a58ae922fc1a99ffd5a8d87485bad57.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a58ae922fc1a99ffd5a8d87485bad57.png)'
- en: What We Still Donâ€™t Understand About Machine Learning. (by author)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»ç„¶ä¸ç†è§£çš„æœºå™¨å­¦ä¹ é—®é¢˜ã€‚ï¼ˆä½œè€…ï¼‰
- en: If youâ€™re not a member, [read for free here](/what-we-still-dont-understand-about-machine-learning-699e0002a057?sk=e2498900bd4ada1e8abe37e056f6ebc9)
    ğŸ‘ˆ
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¦‚æœä½ è¿˜ä¸æ˜¯ä¼šå‘˜ï¼Œ[ç‚¹å‡»è¿™é‡Œå…è´¹é˜…è¯»](/what-we-still-dont-understand-about-machine-learning-699e0002a057?sk=e2498900bd4ada1e8abe37e056f6ebc9)
    ğŸ‘ˆ
- en: It is surprising how some of the basic subjects in machine learning are still
    unknown by researchers and despite being fundamental and common to use, seem to
    be mysterious. Itâ€™s a fun thing about machine learning that we build things that
    work and then figure out why they work at all!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæœºå™¨å­¦ä¹ ä¸­çš„ä¸€äº›åŸºç¡€ä¸»é¢˜è‡³ä»Šä»ç„¶æ˜¯ç ”ç©¶äººå‘˜æœªè§£ä¹‹è°œï¼Œå°½ç®¡å®ƒä»¬æ˜¯åŸºæœ¬çš„ä¸”å¸¸ç”¨çš„ï¼Œä½†ä¼¼ä¹ä»ç„¶ç¥ç§˜è«æµ‹ã€‚æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªæœ‰è¶£ä¹‹å¤„åœ¨äºï¼Œæˆ‘ä»¬æ„å»ºäº†æœ‰æ•ˆçš„ç³»ç»Ÿï¼Œç„¶åæ‰å»å¼„æ¸…æ¥šå®ƒä»¬ä¸ºä½•æœ‰æ•ˆï¼
- en: Here, I aim to investigate the unknown territory in some machine learning concepts
    in order to show while these ideas can seem basic, in reality, they are constructed
    by layers upon layers of abstraction. This helps us to practice questioning the
    **depth of our knowledge**.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘æ—¨åœ¨æ¢è®¨ä¸€äº›æœºå™¨å­¦ä¹ æ¦‚å¿µä¸­çš„æœªçŸ¥é¢†åŸŸï¼Œä»¥å±•ç¤ºå°½ç®¡è¿™äº›æƒ³æ³•çœ‹èµ·æ¥åŸºç¡€ï¼Œå®é™…ä¸Šå®ƒä»¬æ˜¯é€šè¿‡ä¸€å±‚å±‚æŠ½è±¡æ„å»ºè€Œæˆçš„ã€‚è¿™æœ‰åŠ©äºæˆ‘ä»¬ç»ƒä¹ è´¨ç–‘**æˆ‘ä»¬çŸ¥è¯†çš„æ·±åº¦**ã€‚
- en: In this article, we explore several key phenomena in deep learning that challenge
    our traditional understanding of neural networks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†æ·±åº¦å­¦ä¹ ä¸­çš„å‡ ä¸ªå…³é”®ç°è±¡ï¼Œè¿™äº›ç°è±¡æŒ‘æˆ˜äº†æˆ‘ä»¬å¯¹ç¥ç»ç½‘ç»œçš„ä¼ ç»Ÿç†è§£ã€‚
- en: We start with **Batch Normalization** and its underlying mechanisms that remain
    not fully understood.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»**æ‰¹é‡å½’ä¸€åŒ–**å¼€å§‹ï¼Œå¹¶è®¨è®ºå…¶å°šæœªå®Œå…¨ç†è§£çš„åŸºæœ¬æœºåˆ¶ã€‚
- en: We examine the counterintuitive observation that **overparameterized** models
    often **generalize better**, contradicting the classical machine learning theories.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è€ƒå¯Ÿäº†ä¸€ä¸ªåç›´è§‰çš„è§‚å¯Ÿç»“æœï¼Œå³**è¿‡å‚æ•°åŒ–**æ¨¡å‹å¾€å¾€**å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›**ï¼Œè¿™ä¸ç»å…¸çš„æœºå™¨å­¦ä¹ ç†è®ºç›¸çŸ›ç›¾ã€‚
- en: We explore the implicit regularization effects of **gradient descent**, which
    seem to naturally bias neural networks towards simpler, more generalizable solutions.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ¢ç´¢äº†**æ¢¯åº¦ä¸‹é™**çš„éšå¼æ­£åˆ™åŒ–æ•ˆåº”ï¼Œè¿™ä¼¼ä¹è‡ªç„¶åœ°ä½¿ç¥ç»ç½‘ç»œå€¾å‘äºæ›´ç®€å•ã€æ›´å…·æ™®é€‚æ€§çš„è§£å†³æ–¹æ¡ˆã€‚
- en: Finally, we touch on the Lottery Ticket Hypothesis, which proposes that large
    neural networks contain smaller subnetworks capable of achieving **comparable
    performance** when trained in isolation.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†å½©ç¥¨ç¥¨å‡è¯´ï¼ˆLottery Ticket Hypothesisï¼‰ï¼Œè¯¥å‡è¯´æå‡ºï¼Œå¤§å‹ç¥ç»ç½‘ç»œåŒ…å«è¾ƒå°çš„å­ç½‘ç»œï¼Œå½“å®ƒä»¬è¢«å•ç‹¬è®­ç»ƒæ—¶ï¼Œèƒ½å¤Ÿå®ç°**ç›¸å½“çš„æ€§èƒ½**ã€‚
- en: '*This article contains 4 parts, they are not connected so feel free to skip
    to the ones you like more.*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*æœ¬æ–‡åŒ…å«4ä¸ªéƒ¨åˆ†ï¼Œå®ƒä»¬ä¹‹é—´æ²¡æœ‰ç›´æ¥å…³è”ï¼Œæ‰€ä»¥å¯ä»¥éšæ„è·³åˆ°ä½ æ›´æ„Ÿå…´è¶£çš„éƒ¨åˆ†ã€‚*'
- en: Â· [1\. Batch Normalization](#de8b)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Â· [1\. æ‰¹é‡å½’ä¸€åŒ–](#de8b)
- en: âˆ˜ [What We Donâ€™t Understand](#95c8)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [æˆ‘ä»¬ä¸ç†è§£çš„ä¸œè¥¿](#95c8)
- en: âˆ˜ [Batch Norm Considerations](#ec44)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [æ‰¹é‡å½’ä¸€åŒ–çš„æ³¨æ„äº‹é¡¹](#ec44)
- en: Â· [2\. Over-Parameterization and Generalization](#82b3)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Â· [2\. è¿‡åº¦å‚æ•°åŒ–ä¸æ³›åŒ–](#82b3)
- en: âˆ˜ [Understanding Deep Learning Requires Rethinking Generalization](#6185)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [ç†è§£æ·±åº¦å­¦ä¹ éœ€è¦é‡æ–°æ€è€ƒæ³›åŒ–](#6185)
- en: Â· [3\. Implicit Regularization in Neural Networks](#f626)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Â· [3\. ç¥ç»ç½‘ç»œä¸­çš„éšå¼æ­£åˆ™åŒ–](#f626)
- en: âˆ˜ [Experiment 1](#2783)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [å®éªŒ 1](#2783)
- en: âˆ˜ [Experiment 2](#ff53)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [å®éªŒ 2](#ff53)
- en: âˆ˜ [Gradient Descent as a natural Reguralizer](#85f8)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [æ¢¯åº¦ä¸‹é™ä½œä¸ºä¸€ç§è‡ªç„¶æ­£åˆ™åŒ–å™¨](#85f8)
- en: Â· [4\. The Lottery Ticket Hypothesis](#b35e)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Â· [4\. å½©ç¥¨ç¥¨å‡è®¾](#b35e)
- en: Â· [Final Word.](#821e)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Â· [æœ€åçš„è¯ã€‚](#821e)
- en: âˆ˜ [Letâ€™s Connect!](#03ee)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [è®©æˆ‘ä»¬è¿æ¥ï¼](#03ee)
- en: Â· [Further Reads](#397f)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Â· [è¿›ä¸€æ­¥é˜…è¯»](#397f)
- en: Â· [References](#f3e3)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Â· [å‚è€ƒæ–‡çŒ®](#f3e3)
- en: 1\. Batch Normalization
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. æ‰¹é‡å½’ä¸€åŒ–
- en: Introduced by *Sergey Ioffe* and *Christian Szegedy* in 2015 [1], batch normalization
    is a method to train neural networks faster and more stable. It was previously
    known that transforming the input data to have their mean set to zero and variance
    to one would result in a faster convergence. The authors used this idea further
    and introduced **Batch Normalization** to transform the input of hidden layers
    to have a mean of zero and a variance of one.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±*è°¢å°”ç›–Â·çº¦å¤«*å’Œ*å…‹é‡Œæ–¯è’‚å®‰Â·è°¢æ ¼è¿ª*äº2015å¹´æå‡ºçš„[1]ï¼Œæ‰¹é‡å½’ä¸€åŒ–æ˜¯ä¸€ç§åŠ é€Ÿç¥ç»ç½‘ç»œè®­ç»ƒå¹¶æé«˜å…¶ç¨³å®šæ€§çš„æ–¹æ³•ã€‚ä¹‹å‰å·²çŸ¥ï¼Œå°†è¾“å…¥æ•°æ®çš„å‡å€¼è°ƒæ•´ä¸ºé›¶ã€æ–¹å·®è°ƒæ•´ä¸ºä¸€ï¼Œå¯ä»¥å®ç°æ›´å¿«çš„æ”¶æ•›ã€‚ä½œè€…è¿›ä¸€æ­¥ä½¿ç”¨è¿™ä¸€æ€æƒ³ï¼Œæå‡ºäº†**æ‰¹é‡å½’ä¸€åŒ–**ï¼Œä»¥ä½¿éšè—å±‚çš„è¾“å…¥å…·æœ‰é›¶å‡å€¼å’Œå•ä½æ–¹å·®ã€‚
- en: '![](../Images/3d64e4c77a25558d72ba330536db8ba1.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d64e4c77a25558d72ba330536db8ba1.png)'
- en: An outline of the Residual Block used in Resnet and the use of Batch Norm in
    it. (by author)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ¦‚è¿°ResNetä¸­ä½¿ç”¨çš„æ®‹å·®å—ä»¥åŠå…¶ä¸­æ‰¹é‡å½’ä¸€åŒ–çš„åº”ç”¨ã€‚ï¼ˆä½œè€…æä¾›ï¼‰
- en: Since its introduction, the batch norm has become common in neural networks.
    One such example, among many, is its use in the famous ResNet architecture. So
    we could confidently say we are certain of how effective it could be.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªå…¶æå‡ºä»¥æ¥ï¼Œæ‰¹é‡å½’ä¸€åŒ–åœ¨ç¥ç»ç½‘ç»œä¸­å˜å¾—ååˆ†å¸¸è§ã€‚å…¶ä¸­ä¸€ä¸ªä¾‹å­ï¼Œå°±æ˜¯å®ƒåœ¨è‘—åçš„ResNetæ¶æ„ä¸­çš„åº”ç”¨ã€‚å› æ­¤æˆ‘ä»¬å¯ä»¥è‡ªä¿¡åœ°è¯´ï¼Œå®ƒçš„æœ‰æ•ˆæ€§æ˜¯æ¯‹åº¸ç½®ç–‘çš„ã€‚
- en: An interesting research [2] on the effectiveness of Batch Norm showed that while
    training a full ResNet-866 network yielded **93** percent accuracy, freezing all
    the parameters and training **only** the parameters of the Batch Norm layers resulted
    in **83** percent accuracy â€” only a 10% difference.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€é¡¹æœ‰è¶£çš„ç ”ç©¶[2]è¡¨æ˜ï¼Œå°½ç®¡è®­ç»ƒå®Œæ•´çš„ResNet-866ç½‘ç»œè¾¾åˆ°äº†**93**%çš„å‡†ç¡®ç‡ï¼Œä½†å†»ç»“æ‰€æœ‰å‚æ•°ï¼Œä»…è®­ç»ƒæ‰¹é‡å½’ä¸€åŒ–å±‚çš„å‚æ•°ï¼Œç»“æœåªè¾¾åˆ°äº†**83**%çš„å‡†ç¡®ç‡â€”â€”ä»…æœ‰10%çš„å·®å¼‚ã€‚
- en: 'Batch Norm is beneficial in three ways:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰¹é‡å½’ä¸€åŒ–æœ‰ä¸‰ä¸ªæ–¹é¢çš„å¥½å¤„ï¼š
- en: By normalizing the inputs to each layer, it **accelerates** the training process.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡å¯¹æ¯ä¸€å±‚çš„è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–ï¼Œå®ƒ**åŠ é€Ÿ**äº†è®­ç»ƒè¿‡ç¨‹ã€‚
- en: It reduces the sensitivity to **initial conditions**, meaning we need less careful
    weight initialization.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ƒå‡å°‘äº†å¯¹**åˆå§‹æ¡ä»¶**çš„æ•æ„Ÿæ€§ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬ä¸éœ€è¦éå¸¸å°å¿ƒåœ°åˆå§‹åŒ–æƒé‡ã€‚
- en: Batch Norm also acts as a **regularizer**. It improves model generalization
    and in some cases, reduces the need for other regularization techniques.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰¹é‡å½’ä¸€åŒ–è¿˜å……å½“äº†**æ­£åˆ™åŒ–å™¨**ã€‚å®ƒæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå‡å°‘äº†å¯¹å…¶ä»–æ­£åˆ™åŒ–æŠ€æœ¯çš„éœ€æ±‚ã€‚
- en: What We Donâ€™t Understand
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸ç†è§£çš„ä¸œè¥¿
- en: While the positive effect of the Batch Norm is evident, nobody quite understands
    the reason behind its effectiveness. Originally, the authors of the Batch Normalization
    paper introduced it as a method to solve the problem of **internal covariate shift**.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æ‰¹é‡å½’ä¸€åŒ–çš„æ­£é¢æ•ˆæœæ˜¾è€Œæ˜“è§ï¼Œä½†æ²¡äººçœŸæ­£ç†è§£å…¶èƒŒåçš„æœ‰æ•ˆæ€§åŸå› ã€‚æœ€åˆï¼Œæ‰¹é‡å½’ä¸€åŒ–è®ºæ–‡çš„ä½œè€…å°†å…¶æå‡ºä½œä¸ºè§£å†³**å†…éƒ¨åå˜é‡åç§»**é—®é¢˜çš„æ–¹æ³•ã€‚
- en: '*A heads-up about internal covariate shift is that you will find various definitions
    of it that donâ€™t seem to be related at first.* ***Here is how I would like to
    define it.***'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*å…³äºå†…éƒ¨åå˜é‡åç§»çš„ä¸€ä¸ªæç¤ºæ˜¯ï¼Œä½ ä¼šå‘ç°å®ƒæœ‰å„ç§ä¸åŒçš„å®šä¹‰ï¼Œä¹ä¸€çœ‹ä¼¼ä¹æ²¡æœ‰å…³è”ã€‚* ***è¿™æ˜¯æˆ‘æƒ³è¦å®šä¹‰å®ƒçš„æ–¹å¼ã€‚***'
- en: The layers of a neural network are updated during the backpropagation from the
    finish (output layer) to the start (input layer). Internal covariate shift refers
    to the phenomenon where the distribution of inputs to a layer changes during training
    as the parameters of the previous layers are updated.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œçš„å±‚æ˜¯åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ä»è¾“å‡ºå±‚ï¼ˆç»“æŸï¼‰æ›´æ–°åˆ°è¾“å…¥å±‚ï¼ˆå¼€å§‹ï¼‰çš„ã€‚å†…éƒ¨åå˜é‡åç§»æŒ‡çš„æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç”±äºå‰ä¸€å±‚å‚æ•°çš„æ›´æ–°ï¼Œè¾“å…¥å±‚çš„åˆ†å¸ƒå‘ç”Ÿå˜åŒ–çš„ç°è±¡ã€‚
- en: As we change the parameters of the earlier layers, we also change the input
    distribution to the later layers that have been updated to better fit the older
    distribution.
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬æ”¹å˜å‰é¢å‡ å±‚çš„å‚æ•°æ—¶ï¼Œä¹Ÿä¼šæ”¹å˜åç»­å±‚çš„è¾“å…¥åˆ†å¸ƒï¼Œè€Œè¿™äº›åç»­å±‚å·²ç»æ›´æ–°ï¼Œä»¥æ›´å¥½åœ°é€‚åº”è¾ƒæ—§çš„åˆ†å¸ƒã€‚
- en: Internal covariate shift slows down the training and makes it harder for the
    network to converge, as each layer must continuously adapt to the changing distribution
    of its inputs introduced by the update of previous layers.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å†…éƒ¨åå˜é‡åç§»ï¼ˆinternal covariate shiftï¼‰ä¼šå‡ç¼“è®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶ä½¿ç½‘ç»œæ›´éš¾ä»¥æ”¶æ•›ï¼Œå› ä¸ºæ¯ä¸€å±‚å¿…é¡»ä¸æ–­é€‚åº”å‰ä¸€å±‚æ›´æ–°å¼•å…¥çš„è¾“å…¥åˆ†å¸ƒçš„å˜åŒ–ã€‚
- en: The authors of the original Batch Normalization paper believed that the reason
    behind its effectiveness is because of mitigating the problem of internal covariate
    shift. However, a later paper [3] argued that the success of Batch Norm has nothing
    to do with internal covariate shift, but is due to **smoothing the optimization
    landscape**.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹æ‰¹é‡å½’ä¸€åŒ–è®ºæ–‡çš„ä½œè€…è®¤ä¸ºï¼Œå…¶æœ‰æ•ˆæ€§çš„åŸå› åœ¨äºç¼“è§£äº†å†…éƒ¨åå˜é‡åç§»é—®é¢˜ã€‚ç„¶è€Œï¼Œåæ¥çš„ä¸€ç¯‡è®ºæ–‡[3]è®¤ä¸ºï¼Œæ‰¹é‡å½’ä¸€åŒ–çš„æˆåŠŸä¸å†…éƒ¨åå˜é‡åç§»æ— å…³ï¼Œè€Œæ˜¯ç”±äº**å¹³æ»‘ä¼˜åŒ–æ™¯è§‚**çš„ä½œç”¨ã€‚
- en: '![](../Images/2f88c5bb3a672aa66fcd7373c0c431a1.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f88c5bb3a672aa66fcd7373c0c431a1.png)'
- en: 'Comparison of two loss landscapes: a highly rough and sharp loss surface (left)
    vs. a smoother loss surface (right). ([Source](https://arxiv.org/abs/1712.09913))'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¤ç§æŸå¤±æ™¯è§‚çš„æ¯”è¾ƒï¼šä¸€ä¸ªæ˜¯é«˜åº¦ç²—ç³™å’Œé™¡å³­çš„æŸå¤±è¡¨é¢ï¼ˆå·¦ï¼‰ï¼Œå¦ä¸€ä¸ªæ˜¯è¾ƒä¸ºå¹³æ»‘çš„æŸå¤±è¡¨é¢ï¼ˆå³ï¼‰ã€‚([Source](https://arxiv.org/abs/1712.09913))
- en: The figure above is taken from [4], which is not actually about Batch Normalization,
    but is a good visualization of how a smooth loss landscape looks like. However,
    the theory that the Batch Norm is effective due to smoothing the loss landscape
    has challenges and questions of its own.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šå›¾æ¥è‡ª[4]ï¼Œè¯¥æ–‡å¹¶éå…³äºæ‰¹é‡å½’ä¸€åŒ–çš„å†…å®¹ï¼Œä½†å¾ˆå¥½åœ°å±•ç¤ºäº†å¹³æ»‘æŸå¤±æ™¯è§‚çš„æ ·å­ã€‚ç„¶è€Œï¼Œæ‰¹é‡å½’ä¸€åŒ–é€šè¿‡å¹³æ»‘æŸå¤±æ™¯è§‚è€Œæœ‰æ•ˆçš„ç†è®ºæœ¬èº«ä¹Ÿå­˜åœ¨æŒ‘æˆ˜å’Œç–‘é—®ã€‚
- en: Batch Norm Considerations
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‰¹é‡å½’ä¸€åŒ–çš„æ³¨æ„äº‹é¡¹
- en: 'Due to our limited understanding of how Batch Normalization works, here is
    what to consider when it comes to using them in your network:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæˆ‘ä»¬å¯¹æ‰¹é‡å½’ä¸€åŒ–å·¥ä½œåŸç†çš„ç†è§£æœ‰é™ï¼Œä»¥ä¸‹æ˜¯åœ¨ç½‘ç»œä¸­ä½¿ç”¨å®ƒæ—¶éœ€è¦è€ƒè™‘çš„äº‹é¡¹ï¼š
- en: There is a difference between **train** and **inference** modes when using the
    Batch Norm. Using the wrong mode can lead to unexpected behavior that is tricky
    to identify. [5]
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ‰¹é‡å½’ä¸€åŒ–æ—¶ï¼Œ**è®­ç»ƒï¼ˆtrainï¼‰**å’Œ**æ¨ç†ï¼ˆinferenceï¼‰**æ¨¡å¼æ˜¯ä¸åŒçš„ã€‚ä½¿ç”¨é”™è¯¯çš„æ¨¡å¼å¯èƒ½å¯¼è‡´éš¾ä»¥è¯†åˆ«çš„æ„å¤–è¡Œä¸ºã€‚[5]
- en: Batch Norm relies heavily on the size of your **minibatch**. So while it makes
    the need for a careful weight initialization less significant, choosing the right
    size of minibatch becomes more crucial. [5]
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰¹é‡å½’ä¸€åŒ–ï¼ˆBatch Normï¼‰åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äº**å°æ‰¹é‡ï¼ˆminibatchï¼‰**çš„å¤§å°ã€‚å› æ­¤ï¼Œè™½ç„¶å®ƒå‡å°‘äº†å¯¹è°¨æ…åˆå§‹åŒ–æƒé‡çš„éœ€æ±‚ï¼Œä½†é€‰æ‹©åˆé€‚çš„å°æ‰¹é‡å¤§å°å˜å¾—æ›´åŠ å…³é”®ã€‚[5]
- en: There is still an ongoing debate about whether to use Batch Norm before the
    activation function or after it. [6]
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…³äºåœ¨æ¿€æ´»å‡½æ•°ä¹‹å‰è¿˜æ˜¯ä¹‹åä½¿ç”¨æ‰¹é‡å½’ä¸€åŒ–ï¼Œç›®å‰ä»å­˜åœ¨äº‰è®ºã€‚[6]
- en: While Batch Norm has a **regularizer** effect, its interaction with other regularizations
    such as *dropout* or *weight decay* is not clearly known.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è™½ç„¶æ‰¹é‡å½’ä¸€åŒ–å…·æœ‰**æ­£åˆ™åŒ–ï¼ˆregularizerï¼‰**çš„æ•ˆæœï¼Œä½†å®ƒä¸å…¶ä»–æ­£åˆ™åŒ–æ–¹æ³•ï¼ˆå¦‚*ä¸¢å¼ƒæ³•ï¼ˆdropoutï¼‰*æˆ–*æƒé‡è¡°å‡ï¼ˆweight decayï¼‰*ï¼‰çš„ç›¸äº’ä½œç”¨å°šä¸å®Œå…¨æ˜ç¡®ã€‚
- en: There are many questions to answer when it comes to Batch Norms, but research
    is still ongoing to uncover how these layers affect a neural network.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºæ‰¹é‡å½’ä¸€åŒ–ä»æœ‰è®¸å¤šé—®é¢˜å¾…è§£ç­”ï¼Œä½†ç ”ç©¶ä»åœ¨è¿›è¡Œï¼Œä»¥æ­ç¤ºè¿™äº›å±‚å¦‚ä½•å½±å“ç¥ç»ç½‘ç»œã€‚
- en: 2\. Over-Parameterization and Generalization
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. è¿‡åº¦å‚æ•°åŒ–ä¸æ³›åŒ–
- en: '![](../Images/c4c6c3f08203f8d9473bd7ce0197c417.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4c6c3f08203f8d9473bd7ce0197c417.png)'
- en: Face recognition experiments showing that the optimal number of weights in a
    network can be much larger than the number of data points.. ([source](https://clgiles.ist.psu.edu/papers/UMD-CS-TR-3617.what.size.neural.net.to.use.pdf))
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: é¢éƒ¨è¯†åˆ«å®éªŒè¡¨æ˜ï¼Œç½‘ç»œä¸­æƒé‡çš„æœ€ä¼˜æ•°é‡å¯ä»¥è¿œå¤§äºæ•°æ®ç‚¹çš„æ•°é‡ã€‚([source](https://clgiles.ist.psu.edu/papers/UMD-CS-TR-3617.what.size.neural.net.to.use.pdf))
- en: Big networks have challenged our old beliefs of how neural networks work.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å‹ç½‘ç»œæŒ‘æˆ˜äº†æˆ‘ä»¬è¿‡å»å¯¹ç¥ç»ç½‘ç»œå·¥ä½œåŸç†çš„ç†è§£ã€‚
- en: It was traditionally believed using over-parametrized models would result in
    overfitting. Thus the solution would be either to limit the size of the network
    or to add regularization to prevent overfitting to the training data.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼ ç»Ÿä¸Šè®¤ä¸ºï¼Œä½¿ç”¨è¿‡åº¦å‚æ•°åŒ–çš„æ¨¡å‹ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆã€‚å› æ­¤ï¼Œè§£å†³æ–¹æ¡ˆé€šå¸¸æ˜¯é™åˆ¶ç½‘ç»œçš„å¤§å°ï¼Œæˆ–æ·»åŠ æ­£åˆ™åŒ–ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®ã€‚
- en: Surprisingly, in the case of neural networks, using bigger networks could improve
    the **generalization error** (|train error - test error|). In other words, bigger
    networks generalize better. [7] This is in contradiction to what traditional complexity
    metrics such as [VC dimension](https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension)
    â€” a metric to quantify the difficulty of learning from examples, have promised.
    [8]
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å‡ºäººæ„æ–™çš„æ˜¯ï¼Œåœ¨ç¥ç»ç½‘ç»œçš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨æ›´å¤§çš„ç½‘ç»œå¯èƒ½ä¼šæ”¹å–„**æ³›åŒ–è¯¯å·®**ï¼ˆ|è®­ç»ƒè¯¯å·® - æµ‹è¯•è¯¯å·®|ï¼‰ã€‚æ¢å¥è¯è¯´ï¼Œæ›´å¤§çš„ç½‘ç»œæœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚[7]
    è¿™ä¸ä¼ ç»Ÿçš„å¤æ‚åº¦åº¦é‡æ ‡å‡†ï¼ˆå¦‚[VCç»´åº¦](https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension)â€”â€”ä¸€ä¸ªé‡åŒ–ä»æ ·æœ¬ä¸­å­¦ä¹ éš¾åº¦çš„æŒ‡æ ‡ï¼‰æ‰€å®£ç§°çš„å†…å®¹ç›¸çŸ›ç›¾ã€‚[8]
- en: This theory also challenges a debate about whether or not deep neural networks
    (DNNs) achieve their performance by *memorizing* training data or by learning
    patterns. [9] If they memorize the data, how could they possibly generalize to
    predict the unseen data? And if they donâ€™t memorize the data but only learn the
    underlying pattern, how do they predict correct labels even when we introduce
    a certain amount of noise to the labels?
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€ç†è®ºè¿˜æŒ‘æˆ˜äº†ä¸€ä¸ªå…³äºæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰æ˜¯å¦é€šè¿‡*è®°å¿†*è®­ç»ƒæ•°æ®æ¥å®ç°å…¶æ€§èƒ½çš„è¾©è®ºã€‚[9] å¦‚æœå®ƒä»¬æ˜¯é€šè¿‡è®°å¿†æ•°æ®æ¥åšçš„ï¼Œé‚£å®ƒä»¬æ€ä¹ˆå¯èƒ½æ³›åŒ–åˆ°é¢„æµ‹æœªè§è¿‡çš„æ•°æ®å‘¢ï¼Ÿå¦‚æœå®ƒä»¬ä¸è®°å¿†æ•°æ®ï¼Œè€Œåªæ˜¯å­¦ä¹ æ•°æ®ä¸­çš„æ½œåœ¨æ¨¡å¼ï¼Œé‚£å®ƒä»¬åˆæ˜¯å¦‚ä½•é¢„æµ‹æ­£ç¡®çš„æ ‡ç­¾ï¼Œå³ä½¿æˆ‘ä»¬ç»™æ ‡ç­¾å¼•å…¥äº†ä¸€å®šçš„å™ªå£°ï¼Ÿ
- en: '![](../Images/d8dbde067c84e6356a774b2dcf646f1f.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d8dbde067c84e6356a774b2dcf646f1f.png)'
- en: overfitting of a classifier. (upscaled â€” [source](https://commons.wikimedia.org/wiki/File:Overfitting.svg))
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†ç±»å™¨çš„è¿‡æ‹Ÿåˆã€‚ï¼ˆæ”¾å¤§å›¾â€”â€”[æ¥æº](https://commons.wikimedia.org/wiki/File:Overfitting.svg)ï¼‰
- en: '*Understanding Deep Learning Requires Rethinking Generalization*'
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*ç†è§£æ·±åº¦å­¦ä¹ éœ€è¦é‡æ–°æ€è€ƒæ³›åŒ–èƒ½åŠ›*'
- en: An interesting paper on this subject is *Understanding deep learning requires
    rethinking generalization* [10]. The authors argue that traditional approaches
    fail to explain why larger networks generalize well and at the same time, these
    networks can fit even random data.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºè¿™ä¸€ä¸»é¢˜çš„æœ‰è¶£è®ºæ–‡æ˜¯*ç†è§£æ·±åº¦å­¦ä¹ éœ€è¦é‡æ–°æ€è€ƒæ³›åŒ–èƒ½åŠ›*[10]ã€‚ä½œè€…è®¤ä¸ºï¼Œä¼ ç»Ÿçš„æ–¹æ³•æ— æ³•è§£é‡Šä¸ºä»€ä¹ˆæ›´å¤§çš„ç½‘ç»œèƒ½å¤Ÿè‰¯å¥½æ³›åŒ–ï¼ŒåŒæ—¶è¿™äº›ç½‘ç»œä¹Ÿèƒ½æ‹Ÿåˆéšæœºæ•°æ®ã€‚
- en: 'A notable part of this paper explains the role of **explicit regularizations**
    such as weight decay, dropout, and data augmentation on the generalization error:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡çš„ä¸€éƒ¨åˆ†é‡ç‚¹è§£é‡Šäº†**æ˜¾å¼æ­£åˆ™åŒ–**ï¼ˆå¦‚æƒé‡è¡°å‡ã€ä¸¢å¼ƒæ³•å’Œæ•°æ®å¢å¼ºï¼‰åœ¨æ³›åŒ–è¯¯å·®ä¸­çš„ä½œç”¨ï¼š
- en: Explicit regularization may improve generalization performance, but is neither
    necessary nor by itself sufficient for controlling generalization error. L2-regularization
    (weight decay) sometimes even helps optimization, illustrating its poorly understood
    nature in deep learning. [10]
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ˜¾å¼æ­£åˆ™åŒ–å¯èƒ½ä¼šæ”¹å–„æ³›åŒ–æ€§èƒ½ï¼Œä½†æ—¢ä¸æ˜¯å¿…è¦çš„ï¼Œä¹Ÿä¸æ˜¯å•ç‹¬è¶³ä»¥æ§åˆ¶æ³›åŒ–è¯¯å·®ã€‚L2æ­£åˆ™åŒ–ï¼ˆæƒé‡è¡°å‡ï¼‰æœ‰æ—¶ç”šè‡³æœ‰åŠ©äºä¼˜åŒ–ï¼Œè¯´æ˜å®ƒåœ¨æ·±åº¦å­¦ä¹ ä¸­çš„ç†è§£ä»ç„¶ä¸å®Œå…¨ã€‚[10]
- en: Even with dropout and weight decay, InceptionV3 was still able to fit the **random**
    training set very well beyond expectation. This implication is not to devalue
    regularization, but more to emphasize that bigger gains could be achieved from
    changing the model **architecture**.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿ä½¿ç”¨äº†ä¸¢å¼ƒæ³•å’Œæƒé‡è¡°å‡ï¼ŒInceptionV3ä»ç„¶èƒ½å¤Ÿéå¸¸å¥½åœ°æ‹Ÿåˆ**éšæœº**è®­ç»ƒé›†ï¼Œè¶…å‡ºäº†é¢„æœŸã€‚è¿™ä¸€å«ä¹‰å¹¶ä¸æ˜¯è´¬ä½æ­£åˆ™åŒ–ï¼Œè€Œæ˜¯æ›´å¼ºè°ƒé€šè¿‡æ”¹å˜æ¨¡å‹**æ¶æ„**å¯ä»¥è·å¾—æ›´å¤§çš„æ”¶ç›Šã€‚
- en: '![](../Images/e203216819fb3accb40cefdf6557f216.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e203216819fb3accb40cefdf6557f216.png)'
- en: The effect of regularization on generalization. [10]
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£åˆ™åŒ–å¯¹æ³›åŒ–èƒ½åŠ›çš„å½±å“ã€‚[10]
- en: 'So what makes a neural network that generalizes well, different from those
    that generalize poorly? It seems like a rabbit hole. We yet need to rethink a
    few things:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œæ˜¯ä»€ä¹ˆè®©ä¸€ä¸ªèƒ½å¤Ÿè‰¯å¥½æ³›åŒ–çš„ç¥ç»ç½‘ç»œä¸é‚£äº›æ³›åŒ–ä¸è‰¯çš„ç½‘ç»œä¸åŒå‘¢ï¼Ÿè¿™ä¼¼ä¹æ˜¯ä¸€ä¸ªâ€œå…”å­æ´â€ã€‚æˆ‘ä»¬ä»ç„¶éœ€è¦é‡æ–°æ€è€ƒä¸€äº›é—®é¢˜ï¼š
- en: Our understanding of a modelâ€™s **effective capacity**.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯¹æ¨¡å‹**æœ‰æ•ˆå®¹é‡**çš„ç†è§£ã€‚
- en: Our measurement of a modelâ€™s complexityand size. Are model parameters or FLOPs
    simply good metrics? Obviously not.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯¹æ¨¡å‹å¤æ‚åº¦å’Œå¤§å°çš„è¡¡é‡ã€‚æ¨¡å‹å‚æ•°æˆ–FLOPæ˜¯å¦ä»…ä»…æ˜¯å¥½çš„åº¦é‡æ ‡å‡†ï¼Ÿæ˜¾ç„¶ä¸æ˜¯ã€‚
- en: The definition of generalization and how to measure it.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ³›åŒ–èƒ½åŠ›çš„å®šä¹‰ä»¥åŠå¦‚ä½•è¡¡é‡å®ƒã€‚
- en: '![](../Images/cd130cd6128053c768b899942736cb1f.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cd130cd6128053c768b899942736cb1f.png)'
- en: As the size of the networks (H) keeps increasing, train and test errors keep
    decreasing and overfitting does not happen. [11]
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€ç½‘ç»œè§„æ¨¡ï¼ˆHï¼‰çš„ä¸æ–­å¢å¤§ï¼Œè®­ç»ƒè¯¯å·®å’Œæµ‹è¯•è¯¯å·®æŒç»­ä¸‹é™ï¼Œå¹¶ä¸”æ²¡æœ‰å‘ç”Ÿè¿‡æ‹Ÿåˆã€‚[11]
- en: When it comes to big networks and the effect of parameter count on generalization
    you can find numerous papers and blog posts, some even contradicting others.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºå¤§è§„æ¨¡ç½‘ç»œå’Œå‚æ•°æ•°é‡å¯¹æ³›åŒ–èƒ½åŠ›çš„å½±å“ï¼Œå·²æœ‰å¤§é‡è®ºæ–‡å’Œåšå®¢æ–‡ç« ï¼Œå…¶ä¸­ä¸€äº›ç”šè‡³äº’ç›¸çŸ›ç›¾ã€‚
- en: Our current understanding could suggest that larger networks can generalize
    well despite their tendency to overfit. This could be due to their depth, allowing
    learning of more complex patterns when compared to shallow networks. This is mostly
    domain-dependant â€” certain data types may benefit from smaller models and by following
    Occamâ€™s razor principle (donâ€™t miss this post for a further readğŸ‘‡).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç›®å‰çš„ç†è§£å¯èƒ½è¡¨æ˜ï¼Œå°½ç®¡è¾ƒå¤§çš„ç½‘ç»œå€¾å‘äºè¿‡æ‹Ÿåˆï¼Œä½†å®ƒä»¬ä»èƒ½è¾ƒå¥½åœ°æ³›åŒ–ã€‚è¿™å¯èƒ½æ˜¯ç”±äºå®ƒä»¬çš„æ·±åº¦ï¼Œä½¿å¾—å®ƒä»¬èƒ½å¤Ÿå­¦ä¹ æ¯”æµ…å±‚ç½‘ç»œæ›´å¤æ‚çš„æ¨¡å¼ã€‚è¿™ä¸»è¦å–å†³äºé¢†åŸŸâ€”â€”æŸäº›æ•°æ®ç±»å‹å¯èƒ½æ›´é€‚åˆä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ï¼Œå¹¶éµå¾ªå¥¥å¡å§†å‰ƒåˆ€åŸç†ï¼ˆä¸è¦é”™è¿‡è¿™ç¯‡æ–‡ç« ä»¥è¿›ä¸€æ­¥é˜…è¯»ğŸ‘‡ï¼‰ã€‚
- en: 3\. Implicit Regularization in Neural Networks
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. ç¥ç»ç½‘ç»œä¸­çš„éšå¼æ­£åˆ™åŒ–
- en: At the heart of machine learning lies Gradient Descent â€” the steps we take to
    find the local minima in a loss landscape. Gradient Descent (GD), along with Stochastic
    Gradient Descent (SGD) is one of the first that is digested by anyone starting
    to learn machine learning.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: æœºå™¨å­¦ä¹ çš„æ ¸å¿ƒæ˜¯æ¢¯åº¦ä¸‹é™â€”â€”æˆ‘ä»¬ä¸ºäº†åœ¨æŸå¤±æ™¯è§‚ä¸­æ‰¾åˆ°å±€éƒ¨æœ€å°å€¼è€Œé‡‡å–çš„æ­¥éª¤ã€‚æ¢¯åº¦ä¸‹é™ï¼ˆGDï¼‰ä»¥åŠéšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰æ˜¯ä»»ä½•å¼€å§‹å­¦ä¹ æœºå™¨å­¦ä¹ çš„äººé¦–å…ˆç†è§£çš„ç®—æ³•ã€‚
- en: As the algorithm seems straightforward, one might expect that it does not have
    much depth. However, you can never find the button of the pool in machine learning.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ç®—æ³•çœ‹èµ·æ¥ç®€å•ï¼Œå¯èƒ½æœ‰äººè®¤ä¸ºå®ƒæ²¡æœ‰å¤ªå¤šæ·±åº¦ã€‚ç„¶è€Œï¼Œåœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œä½ æ°¸è¿œæ— æ³•æ‰¾åˆ°æ± å¡˜çš„åº•éƒ¨ã€‚
- en: Do neural networks benefit from an implicit regularization by Gradient Descent
    that pushes them to find *simpler* and *more general* solutions? Could this be
    the reason why over-parametrized networks generalize as shown in the previous
    part?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œæ˜¯å¦ä»æ¢¯åº¦ä¸‹é™çš„éšå¼æ­£åˆ™åŒ–ä¸­å—ç›Šï¼Œè¿™ç§æ­£åˆ™åŒ–æ¨åŠ¨å®ƒä»¬æ‰¾åˆ°*æ›´ç®€å•*å’Œ*æ›´é€šç”¨*çš„è§£å†³æ–¹æ¡ˆï¼Ÿè¿™æ˜¯å¦æ˜¯ä¹‹å‰æåˆ°çš„è¿‡å‚æ•°åŒ–ç½‘ç»œèƒ½å¤Ÿæ³›åŒ–çš„åŸå› ï¼Ÿ
- en: '![](../Images/304be222467e499f2e5c6c5d56844e50.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/304be222467e499f2e5c6c5d56844e50.png)'
- en: 'Gradient Descent in 2D. (Source: [Wikimedia Commons](https://en.wikipedia.org/wiki/File:Gradient_Descent_in_2D.webm))'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: äºŒç»´ä¸­çš„æ¢¯åº¦ä¸‹é™ã€‚ï¼ˆæ¥æºï¼š[Wikimedia Commons](https://en.wikipedia.org/wiki/File:Gradient_Descent_in_2D.webm)ï¼‰
- en: 'There are two experiments you need to pay close attention to:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸¤ä¸ªå®éªŒéœ€è¦ç‰¹åˆ«æ³¨æ„ï¼š
- en: Experiment 1
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®éªŒ1
- en: When the authors of [11] trained models on CIFAR-10 and MNIST datasets using
    SGD and no explicit regularization, they concluded that as the size of the network
    increases, the test and training errors keep decreasing. This goes against the
    belief that bigger networks have a higher test error because of overfitting. Even
    after adding more and more parameters to the network, the generalization error
    does not increase. Then they forced the network to overfit by adding random label
    noise. As shown in the figure below, even with 5% random labels, the test error
    decreases further and there are no significant signs of overfitting.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å½“[11]çš„ä½œè€…ä½¿ç”¨SGDå¹¶ä¸”æ²¡æœ‰æ˜¾å¼æ­£åˆ™åŒ–ï¼Œè®­ç»ƒCIFAR-10å’ŒMNISTæ•°æ®é›†çš„æ¨¡å‹æ—¶ï¼Œä»–ä»¬å¾—å‡ºç»“è®ºï¼šéšç€ç½‘ç»œå¤§å°çš„å¢åŠ ï¼Œæµ‹è¯•å’Œè®­ç»ƒè¯¯å·®ä¸æ–­å‡å°‘ã€‚è¿™ä¸è®¤ä¸ºè¾ƒå¤§çš„ç½‘ç»œç”±äºè¿‡æ‹Ÿåˆè€Œæœ‰æ›´é«˜æµ‹è¯•è¯¯å·®çš„è§‚ç‚¹ç›¸æ‚–ã€‚å³ä½¿åœ¨ç½‘ç»œä¸­æ·»åŠ æ›´å¤šçš„å‚æ•°ï¼Œæ³›åŒ–è¯¯å·®ä¹Ÿæ²¡æœ‰å¢åŠ ã€‚éšåï¼Œä»–ä»¬é€šè¿‡æ·»åŠ éšæœºæ ‡ç­¾å™ªå£°å¼ºåˆ¶ç½‘ç»œè¿‡æ‹Ÿåˆã€‚å¦‚ä¸‹é¢çš„å›¾æ‰€ç¤ºï¼Œå³ä½¿æ ‡ç­¾éšæœºå™ªå£°è¾¾åˆ°5%ï¼Œæµ‹è¯•è¯¯å·®ä»ç„¶è¿›ä¸€æ­¥é™ä½ï¼Œè€Œä¸”æ²¡æœ‰æ˜æ˜¾çš„è¿‡æ‹Ÿåˆè¿¹è±¡ã€‚
- en: '![](../Images/99f78afda3879ffd5d8b592f2199a524.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99f78afda3879ffd5d8b592f2199a524.png)'
- en: Test and Train error of a network with increasing size (H) and 5% noise to the
    labels. Left is MNIST and right is CIFAR-10\. [11]
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ç½‘ç»œéšç€å¤§å°ï¼ˆHï¼‰å¢åŠ å¹¶ä¸”æ ‡ç­¾å™ªå£°ä¸º5%çš„æµ‹è¯•å’Œè®­ç»ƒè¯¯å·®ã€‚å·¦ä¾§æ˜¯MNISTï¼Œå³ä¾§æ˜¯CIFAR-10ã€‚[11]
- en: Experiment 2
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®éªŒ2
- en: An important paper, *In Search of the Real Inductive Bias* [12], experiments
    by fitting a predictor using linearly separable datasets. The authors show how
    logistic regression, using gradient descent and no regularization, inherently
    biases the solution towards the maximum-margin separator (also known as hard margin
    SVM). This is an interesting and surprising behavior of gradient descent. Because
    even though the loss and the optimization **donâ€™t directly involve** any terms
    that encourage margin maximization (like those you find in Support Vector Machines),
    gradient descent inherently biases the solution towards a max-margin classifier.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç¯‡é‡è¦çš„è®ºæ–‡ï¼Œ*å¯»æ‰¾çœŸå®çš„å½’çº³åç½®* [12]ï¼Œé€šè¿‡æ‹Ÿåˆä¸€ä¸ªä½¿ç”¨çº¿æ€§å¯åˆ†æ•°æ®é›†çš„é¢„æµ‹å™¨è¿›è¡Œå®éªŒã€‚ä½œè€…å±•ç¤ºäº†å¦‚ä½•åœ¨æ²¡æœ‰æ­£åˆ™åŒ–çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨æ¢¯åº¦ä¸‹é™çš„é€»è¾‘å›å½’æœ¬èƒ½åœ°å°†è§£åå‘äºæœ€å¤§é—´éš”åˆ†ç¦»å™¨ï¼ˆä¹Ÿç§°ä¸ºç¡¬é—´éš”SVMï¼‰ã€‚è¿™æ˜¯æ¢¯åº¦ä¸‹é™çš„ä¸€ä¸ªæœ‰è¶£ä¸”ä»¤äººæƒŠè®¶çš„è¡Œä¸ºã€‚å› ä¸ºå³ä½¿æŸå¤±å’Œä¼˜åŒ–**å¹¶æœªç›´æ¥æ¶‰åŠ**ä»»ä½•é¼“åŠ±æœ€å¤§åŒ–é—´éš”çš„é¡¹ï¼ˆæ¯”å¦‚åœ¨æ”¯æŒå‘é‡æœºä¸­æ‰¾åˆ°çš„é‚£äº›é¡¹ï¼‰ï¼Œæ¢¯åº¦ä¸‹é™æœ¬èƒ½åœ°å°†è§£åå‘äºæœ€å¤§é—´éš”åˆ†ç±»å™¨ã€‚
- en: '![](../Images/67d2bde1fbdf0706ff73b0f3baeddee9.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67d2bde1fbdf0706ff73b0f3baeddee9.png)'
- en: 'H3 represents how a hard-margin SVM would classify the dataset. (Source: [Wikimedia
    Commons](https://commons.wikimedia.org/wiki/File:Svm_separating_hyperplanes_(SVG).svg))'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: H3è¡¨ç¤ºç¡¬é—´éš”æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰å¦‚ä½•å¯¹æ•°æ®é›†è¿›è¡Œåˆ†ç±»ã€‚ï¼ˆæ¥æºï¼š[Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Svm_separating_hyperplanes_(SVG).svg)ï¼‰
- en: Gradient Descent as a natural Reguralizer
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™ä½œä¸ºä¸€ç§è‡ªç„¶çš„æ­£åˆ™åŒ–æ–¹æ³•
- en: The experiments suggest an implicit regularization effect as if the optimization
    process favors simpler and more stable solutions. This implies that GD has a preference
    for simpler models, often converging to a special type of local minima referred
    to as â€œflatâ€ minima, which tends to have **lower generalization error** compared
    to sharper minima. This helps explain why deep learning models often perform well
    on real-world tasks beyond the training data. This suggests that the optimization
    process itself can be considered a form of implicit regularization, leading to
    models that are not only minimal in error on the training data, but also robust
    in their prediction of unseen data. A full theoretical explanation remains an
    active area of research.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: å®éªŒè¡¨æ˜å­˜åœ¨ä¸€ç§éšå¼æ­£åˆ™åŒ–æ•ˆåº”ï¼Œä¼˜åŒ–è¿‡ç¨‹ä¼¼ä¹åå‘æ›´ç®€å•å’Œæ›´ç¨³å®šçš„è§£å†³æ–¹æ¡ˆã€‚è¿™æ„å‘³ç€æ¢¯åº¦ä¸‹é™ï¼ˆGDï¼‰æ›´å€¾å‘äºç®€å•çš„æ¨¡å‹ï¼Œé€šå¸¸ä¼šæ”¶æ•›åˆ°ä¸€ç§ç‰¹æ®Šç±»å‹çš„å±€éƒ¨æœ€å°å€¼ï¼Œå³â€œå¹³å¦â€æœ€å°å€¼ï¼Œç›¸æ¯”äºæ›´å°–é”çš„æœ€å°å€¼ï¼Œå¹³å¦æœ€å°å€¼é€šå¸¸å…·æœ‰**è¾ƒä½çš„æ³›åŒ–è¯¯å·®**ã€‚è¿™æœ‰åŠ©äºè§£é‡Šä¸ºä½•æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å®é™…ä»»åŠ¡ä¸­å¾€å¾€è¡¨ç°è‰¯å¥½ï¼Œå³ä½¿åœ¨è¶…å‡ºè®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ã€‚è¿™è¡¨æ˜ä¼˜åŒ–è¿‡ç¨‹æœ¬èº«å¯ä»¥è§†ä¸ºä¸€ç§éšå¼æ­£åˆ™åŒ–ï¼Œå¯¼è‡´æ¨¡å‹ä¸ä»…åœ¨è®­ç»ƒæ•°æ®ä¸Šæœ€å°åŒ–è¯¯å·®ï¼Œè€Œä¸”åœ¨é¢„æµ‹æœªè§æ•°æ®æ—¶ä¹Ÿè¡¨ç°å‡ºé²æ£’æ€§ã€‚å¯¹æ­¤çš„å®Œæ•´ç†è®ºè§£é‡Šä»ç„¶æ˜¯ä¸€ä¸ªæ´»è·ƒçš„ç ”ç©¶é¢†åŸŸã€‚
- en: 'Perhaps this article could also be interesting to you, on how and why deep
    neural networks are converging into a unified representation of reality:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è®¸è¿™ç¯‡æ–‡ç« ä¹Ÿä¼šå¯¹ä½ æœ‰è¶£ï¼Œè®²è¿°äº†æ·±åº¦ç¥ç»ç½‘ç»œæ˜¯å¦‚ä½•ä»¥åŠä¸ºä½•è¶‹å‘äºç»Ÿä¸€çš„ç°å®è¡¨ç¤ºï¼š
- en: '[](/platonic-representation-hypothesis-c812813d7248?source=post_page-----699e0002a057--------------------------------)
    [## Platonic Representation: Are AI Deep Network Models Converging?'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/platonic-representation-hypothesis-c812813d7248?source=post_page-----699e0002a057--------------------------------)
    [## æŸæ‹‰å›¾è¡¨ç¤ºæ³•ï¼šAIæ·±åº¦ç½‘ç»œæ¨¡å‹æ˜¯å¦æ­£åœ¨è¶‹åŒï¼Ÿ'
- en: Are Artificial Intelligence models evolving towards a unified representation
    of reality? The Platonic Representationâ€¦
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: äººå·¥æ™ºèƒ½æ¨¡å‹æ˜¯å¦æ­£åœ¨æœå‘ç»Ÿä¸€çš„ç°å®è¡¨ç¤ºå‘å±•ï¼ŸæŸæ‹‰å›¾è¡¨ç¤ºæ³•â€¦â€¦
- en: towardsdatascience.com](/platonic-representation-hypothesis-c812813d7248?source=post_page-----699e0002a057--------------------------------)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/platonic-representation-hypothesis-c812813d7248?source=post_page-----699e0002a057--------------------------------)'
- en: 4\. The Lottery Ticket Hypothesis
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4. å½©ç¥¨ç¥¨æ®å‡è¯´
- en: Model pruning can reduce the parameters of a trained neural network by 90%.
    If done correctly, this could be achieved without dropping the accuracy. But you
    can only prune your model **after** it has been trained. If we could manage to
    remove the excess parameters **before** training, this would mean using much less
    time and resources.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å‰ªæå¯ä»¥é€šè¿‡å‡å°‘è®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œçš„å‚æ•°è¾¾åˆ°90%ã€‚å¦‚æœæ“ä½œå¾—å½“ï¼Œå¯ä»¥åœ¨ä¸é™ä½å‡†ç¡®åº¦çš„æƒ…å†µä¸‹å®ç°è¿™ä¸€ç‚¹ã€‚ä½†ä½ åªèƒ½åœ¨æ¨¡å‹è®­ç»ƒå®Œæˆå**è¿›è¡Œ**å‰ªæã€‚å¦‚æœæˆ‘ä»¬èƒ½å¤Ÿåœ¨è®­ç»ƒå‰ç§»é™¤å¤šä½™çš„å‚æ•°ï¼Œè¿™æ„å‘³ç€å¯ä»¥ä½¿ç”¨æ›´å°‘çš„æ—¶é—´å’Œèµ„æºã€‚
- en: The Lottery Ticket Hypothesis [13] argues that a neural network contains subnetworks
    that when trained in isolation, can reach test accuracy comparable to the original
    network. These subnetworks â€” *the winning tickets,* have the initial weights that
    make their training successful â€” *the lottery*.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å½©ç¥¨ç¥¨æ®å‡è¯´[13]è®¤ä¸ºï¼Œç¥ç»ç½‘ç»œåŒ…å«ä¸€äº›å­ç½‘ç»œï¼Œå½“å®ƒä»¬å•ç‹¬è®­ç»ƒæ—¶ï¼Œå¯ä»¥è¾¾åˆ°ä¸åŸå§‹ç½‘ç»œç›¸å½“çš„æµ‹è¯•å‡†ç¡®åº¦ã€‚è¿™äº›å­ç½‘ç»œâ€”â€”*ä¸­å¥–çš„ç¥¨æ®*ï¼Œæ‹¥æœ‰ä½¿å…¶è®­ç»ƒæˆåŠŸçš„åˆå§‹æƒé‡â€”â€”*å½©ç¥¨*ã€‚
- en: 'The authors find these subnetworks through an **iterative pruning** method:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…é€šè¿‡**è¿­ä»£å‰ªæ**æ–¹æ³•æ‰¾åˆ°äº†è¿™äº›å­ç½‘ç»œï¼š
- en: '**Training the Network**: First, they train the original unpruned network.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è®­ç»ƒç½‘ç»œ**ï¼šé¦–å…ˆï¼Œè®­ç»ƒåŸå§‹çš„æœªå‰ªæç½‘ç»œã€‚'
- en: '**Pruning**: After training, they prune **p%** of the weights.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å‰ªæ**ï¼šè®­ç»ƒåï¼Œå‰ªæ**p%**çš„æƒé‡ã€‚'
- en: '**Resetting Weights**: The remaining weights are set to their original values
    from the initial initialization.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é‡ç½®æƒé‡**ï¼šå‰©ä½™çš„æƒé‡è¢«é‡ç½®ä¸ºåˆå§‹åˆå§‹åŒ–æ—¶çš„åŸå§‹å€¼ã€‚'
- en: '**Retraining**: The pruned network is retrained to see if it can reach the
    same or higher performance than the previous network.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å†è®­ç»ƒ**ï¼šå‰ªæåçš„ç½‘ç»œè¢«é‡æ–°è®­ç»ƒï¼Œä»¥æŸ¥çœ‹å®ƒæ˜¯å¦èƒ½è¾¾åˆ°ä¸ä¹‹å‰çš„ç½‘ç»œç›¸åŒæˆ–æ›´é«˜çš„æ€§èƒ½ã€‚'
- en: '**Repeat**: Until a desired sparsity of the original network is achieved, or
    the pruned network can no longer match the performance of the unpruned network,
    this process is repeated.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é‡å¤**ï¼šç›´åˆ°è¾¾åˆ°åŸå§‹ç½‘ç»œæœŸæœ›çš„ç¨€ç–åº¦ï¼Œæˆ–è€…å‰ªæåçš„ç½‘ç»œæ— æ³•å†ä¸æœªå‰ªæç½‘ç»œåŒ¹æ•Œæ—¶ï¼Œæ‰ä¼šåœæ­¢æ­¤è¿‡ç¨‹ã€‚'
- en: '![](../Images/40897b86443a85999f980a18cdfaea41.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40897b86443a85999f980a18cdfaea41.png)'
- en: Iterative Pruning used in the Lottery Ticket Hypothesis paper. (by author)
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: è¿­ä»£å‰ªæåœ¨ã€Šå½©ç¥¨ç¥¨æ®å‡è¯´ã€‹è®ºæ–‡ä¸­çš„åº”ç”¨ã€‚ï¼ˆä½œè€…ï¼‰
- en: The proposed method of iterative training is computationally expensive, requiring
    training a network 15 times or more on multiple experiments.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: æå‡ºçš„è¿­ä»£è®­ç»ƒæ–¹æ³•åœ¨è®¡ç®—ä¸Šéå¸¸æ˜‚è´µï¼Œè¦æ±‚åœ¨å¤šä¸ªå®éªŒä¸­è®­ç»ƒä¸€ä¸ªç½‘ç»œ15æ¬¡æˆ–æ›´å¤šæ¬¡ã€‚
- en: It remains an area of research why we have such phenomena in neural networks.
    Could it be true that SGD only focuses on the winning tickets when training the
    network and not the full body of the network? Why do certain random initializations
    contain such highly effective sub-networks? If you want to dive deep into this
    theory, donâ€™t miss out on [13] and [14].
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆåœ¨ç¥ç»ç½‘ç»œä¸­ä¼šå‡ºç°è¿™ç§ç°è±¡ä»æ˜¯ä¸€ä¸ªç ”ç©¶é¢†åŸŸã€‚æ˜¯å¦æœ‰å¯èƒ½æ˜¯SGDåœ¨è®­ç»ƒç½‘ç»œæ—¶åªå…³æ³¨æˆåŠŸçš„ç½‘ç»œéƒ¨åˆ†ï¼Œè€Œä¸æ˜¯ç½‘ç»œçš„å…¨éƒ¨ï¼Ÿä¸ºä»€ä¹ˆæŸäº›éšæœºåˆå§‹åŒ–ä¼šåŒ…å«å¦‚æ­¤é«˜æ•ˆçš„å­ç½‘ç»œï¼Ÿå¦‚æœä½ æƒ³æ·±å…¥æ¢è®¨è¿™ä¸ªç†è®ºï¼Œä¸è¦é”™è¿‡[13]å’Œ[14]ã€‚
- en: Final Word.
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœ€åçš„è¯ã€‚
- en: '*Thank you for reading through the article!* I have tried my best to provide
    an accurate article, however, please share your opinions and suggestions if you
    think any modifications are required.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ„Ÿè°¢æ‚¨é˜…è¯»æœ¬æ–‡ï¼* æˆ‘å·²ç»å°½åŠ›æä¾›ä¸€ç¯‡å‡†ç¡®çš„æ–‡ç« ï¼Œä½†å¦‚æœæ‚¨è®¤ä¸ºéœ€è¦ä¿®æ”¹ï¼Œè¯·åˆ†äº«æ‚¨çš„æ„è§å’Œå»ºè®®ã€‚'
- en: Letâ€™s Connect!
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä¿æŒè”ç³»ï¼
- en: '*Subscribe for FREE to be notified of new articles! You can also find me on*
    [*LinkedIn*](https://www.linkedin.com/in/hesamsheikh/) *and* [*Twitter*](https://x.com/itsHesamSheikh)*.*'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '*å…è´¹è®¢é˜…ä»¥æ¥æ”¶æ–°æ–‡ç« é€šçŸ¥ï¼ä½ è¿˜å¯ä»¥åœ¨* [*LinkedIn*](https://www.linkedin.com/in/hesamsheikh/)
    *å’Œ* [*Twitter*](https://x.com/itsHesamSheikh)* ä¸Šæ‰¾åˆ°æˆ‘ã€‚*'
- en: Further Reads
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¿›ä¸€æ­¥é˜…è¯»
- en: 'If you have reached so far, you might also find these articles interesting:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å·²ç»çœ‹åˆ°è¿™é‡Œï¼Œä½ å¯èƒ½ä¹Ÿä¼šå¯¹ä»¥ä¸‹æ–‡ç« æ„Ÿå…´è¶£ï¼š
- en: '[](https://pub.towardsai.net/learn-anything-with-ai-and-the-feynman-technique-00a33f6a02bc?source=post_page-----699e0002a057--------------------------------)
    [## Learn Anything with AI and the Feynman Technique'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pub.towardsai.net/learn-anything-with-ai-and-the-feynman-technique-00a33f6a02bc?source=post_page-----699e0002a057--------------------------------)
    [## åˆ©ç”¨AIå’Œè´¹æ›¼æŠ€å·§å­¦ä¹ ä»»ä½•ä¸œè¥¿'
- en: study any concept in four easy steps, by applying AI and a Noble Prize winner
    approach
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é€šè¿‡åº”ç”¨AIå’Œè¯ºè´å°”å¥–å¾—ä¸»çš„æ–¹æ³•ï¼Œåˆ†å››ä¸ªç®€å•æ­¥éª¤å­¦ä¹ ä»»ä½•æ¦‚å¿µ
- en: pub.towardsai.net](https://pub.towardsai.net/learn-anything-with-ai-and-the-feynman-technique-00a33f6a02bc?source=post_page-----699e0002a057--------------------------------)
    [](/a-comprehensive-guide-to-collaborative-ai-agents-in-practice-1f4048947d9c?source=post_page-----699e0002a057--------------------------------)
    [## A Comprehensive Guide to Collaborative AI Agents in Practice
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[pub.towardsai.net](https://pub.towardsai.net/learn-anything-with-ai-and-the-feynman-technique-00a33f6a02bc?source=post_page-----699e0002a057--------------------------------)
    [](/a-comprehensive-guide-to-collaborative-ai-agents-in-practice-1f4048947d9c?source=post_page-----699e0002a057--------------------------------)
    [## å®è·µä¸­çš„åä½œAIä»£ç†å…¨é¢æŒ‡å—'
- en: the definition, and building a team of agents that refine your CV and Cover
    Letter for job applications
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å®šä¹‰ï¼Œå¹¶å»ºç«‹ä¸€ä¸ªä»£ç†å›¢é˜Ÿï¼Œç²¾ç‚¼ä½ çš„ç®€å†å’Œæ±‚èŒä¿¡
- en: towardsdatascience.com](/a-comprehensive-guide-to-collaborative-ai-agents-in-practice-1f4048947d9c?source=post_page-----699e0002a057--------------------------------)
    [](https://pub.towardsai.net/chatgpt-as-a-game-engine-to-play-flappy-bird-ee4adff46f48?source=post_page-----699e0002a057--------------------------------)
    [## I Played Flappy Bird in ChatGPT
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/a-comprehensive-guide-to-collaborative-ai-agents-in-practice-1f4048947d9c?source=post_page-----699e0002a057--------------------------------)
    [](https://pub.towardsai.net/chatgpt-as-a-game-engine-to-play-flappy-bird-ee4adff46f48?source=post_page-----699e0002a057--------------------------------)
    [## æˆ‘åœ¨ChatGPTä¸­ç©äº†Flappy Bird'
- en: GPT-4 is fantastic, but is it good enough to be a Game Engine? I tried this
    with Flappy Bird, using a simple LangChainâ€¦
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GPT-4éå¸¸æ£’ï¼Œä½†å®ƒè¶³å¤Ÿå¥½ï¼Œèƒ½å¤Ÿä½œä¸ºæ¸¸æˆå¼•æ“å—ï¼Ÿæˆ‘ç”¨ä¸€ä¸ªç®€å•çš„LangChainå°è¯•äº†è¿™ä¸ªï¼Œåšäº†ä¸€ä¸ªFlappy Birdçš„æ¸¸æˆâ€¦â€¦
- en: pub.towardsai.net](https://pub.towardsai.net/chatgpt-as-a-game-engine-to-play-flappy-bird-ee4adff46f48?source=post_page-----699e0002a057--------------------------------)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[pub.towardsai.net](https://pub.towardsai.net/chatgpt-as-a-game-engine-to-play-flappy-bird-ee4adff46f48?source=post_page-----699e0002a057--------------------------------)'
- en: References
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] Ioffe, S., & Szegedy, C. (2015). Batch Normalization: Accelerating Deep
    Network Training by Reducing Internal Covariate Shift. [arXiv](https://arxiv.org/abs/1502.03167).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Ioffe, S., & Szegedy, C. (2015)ã€‚ ã€Šæ‰¹é‡å½’ä¸€åŒ–ï¼šé€šè¿‡å‡å°‘å†…éƒ¨åæ–¹å·®åç§»åŠ é€Ÿæ·±åº¦ç½‘ç»œè®­ç»ƒã€‹ã€‚ [arXiv](https://arxiv.org/abs/1502.03167)ã€‚'
- en: '[2] [Outside the Norm](https://www.deeplearning.ai/the-batch/outside-the-norm/),
    DeepLearning.AI'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [è¶…å‡ºå¸¸è§„](https://www.deeplearning.ai/the-batch/outside-the-norm/)ï¼ŒDeepLearning.AI'
- en: '[3] Santurkar, Shibani; Tsipras, Dimitris; Ilyas, Andrew; Madry, Aleksander
    (29 May 2018). â€œHow Does Batch Normalization Help Optimization?â€. arXiv:[1805.11604](https://arxiv.org/abs/1805.11604)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Santurkar, Shibani; Tsipras, Dimitris; Ilyas, Andrew; Madry, Aleksander
    (2018å¹´5æœˆ29æ—¥)ã€‚ â€œæ‰¹é‡å½’ä¸€åŒ–å¦‚ä½•å¸®åŠ©ä¼˜åŒ–ï¼Ÿâ€ arXivï¼š[1805.11604](https://arxiv.org/abs/1805.11604)'
- en: '[4] Li, H., Xu, Z., Taylor, G., Studer, C., & Goldstein, T. (2018). Visualizing
    the Loss Landscape of Neural Nets. [arXiv](https://arxiv.org/abs/1712.09913)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Li, H., Xu, Z., Taylor, G., Studer, C., & Goldstein, T. (2018). å¯è§†åŒ–ç¥ç»ç½‘ç»œçš„æŸå¤±æ™¯è§‚ã€‚[arXiv](https://arxiv.org/abs/1712.09913)'
- en: '[5] [On The Perils of Batch Norm](https://www.alexirpan.com/2017/04/26/perils-batch-norm.html)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [æ‰¹å½’ä¸€åŒ–çš„å±é™©](https://www.alexirpan.com/2017/04/26/perils-batch-norm.html)'
- en: '[6] [https://x.com/svpino/status/1588501331316121601](https://x.com/svpino/status/1588501331316121601)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] [https://x.com/svpino/status/1588501331316121601](https://x.com/svpino/status/1588501331316121601)'
- en: '[7] Neyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y., & Srebro, N. (2018).
    Towards Understanding the Role of Over-Parametrization in Generalization of Neural
    Networks. [arXiv](https://arxiv.org/abs/1805.12076).'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Neyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y., & Srebro, N. (2018).
    æœç€ç†è§£è¿‡åº¦å‚æ•°åŒ–åœ¨ç¥ç»ç½‘ç»œæ³›åŒ–ä¸­çš„ä½œç”¨è¿ˆè¿›ã€‚[arXiv](https://arxiv.org/abs/1805.12076)'
- en: '[8] [Why is deep learning hyped despite bad VC dimension?](https://cs.stackexchange.com/questions/75327/why-is-deep-learning-hyped-despite-bad-vc-dimension)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] [ä¸ºä»€ä¹ˆæ·±åº¦å­¦ä¹ åœ¨ç³Ÿç³•çš„VCç»´åº¦ä¸‹ä»ç„¶è¢«ç‚’ä½œï¼Ÿ](https://cs.stackexchange.com/questions/75327/why-is-deep-learning-hyped-despite-bad-vc-dimension)'
- en: '[9] [DEEP NETS DONâ€™T LEARN VIA MEMORIZATION](https://openreview.net/pdf?id=rJv6ZgHYg)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] [æ·±åº¦ç½‘ç»œä¸æ˜¯é€šè¿‡è®°å¿†åŒ–å­¦ä¹ çš„](https://openreview.net/pdf?id=rJv6ZgHYg)'
- en: '[10] Zhang, C., Bengio, S., Hardt, M., Recht, B., & Vinyals, O. (2017). Understanding
    Deep Learning Requires Rethinking Generalization. [*arXiv:1611.03530*](https://arxiv.org/abs/1611.03530)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Zhang, C., Bengio, S., Hardt, M., Recht, B., & Vinyals, O. (2017). ç†è§£æ·±åº¦å­¦ä¹ éœ€è¦é‡æ–°æ€è€ƒæ³›åŒ–é—®é¢˜ã€‚[*arXiv:1611.03530*](https://arxiv.org/abs/1611.03530)'
- en: '[11] Neyshabur, B., Tomioka, R., & Srebro, N. (2015). In Search of the Real
    Inductive Bias: On the Role of Implicit Regularization in Deep Learning. *arXiv:1412.6614*'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Neyshabur, B., Tomioka, R., & Srebro, N. (2015). å¯»æ‰¾çœŸå®çš„å½’çº³åå·®ï¼šå…³äºéšå¼æ­£åˆ™åŒ–åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„ä½œç”¨ã€‚*arXiv:1412.6614*'
- en: '[12] Soudry, D., Hoffer, E., Nacson, M. S., Gunasekar, S., & Srebro, N. (2017).
    The Implicit Bias of Gradient Descent on Separable Data. *arXiv:1710.10345*'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Soudry, D., Hoffer, E., Nacson, M. S., Gunasekar, S., & Srebro, N. (2017).
    æ¢¯åº¦ä¸‹é™åœ¨å¯åˆ†æ•°æ®ä¸Šçš„éšå¼åå·®ã€‚*arXiv:1710.10345*'
- en: '[13] Frankle, J., & Carbin, M. (2019). The Lottery Ticket Hypothesis: Finding
    Sparse, Trainable Neural Networks. *arXiv:1803.03635*'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Frankle, J., & Carbin, M. (2019). å½©ç¥¨ç¥¨å‡è®¾ï¼šå¯»æ‰¾ç¨€ç–çš„ã€å¯è®­ç»ƒçš„ç¥ç»ç½‘ç»œã€‚*arXiv:1803.03635*'
- en: '[14] [https://www.lesswrong.com/posts/Z7R6jFjce3J2Ryj44/exploring-the-lottery-ticket-hypothesis](https://www.lesswrong.com/posts/Z7R6jFjce3J2Ryj44/exploring-the-lottery-ticket-hypothesis)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] [https://www.lesswrong.com/posts/Z7R6jFjce3J2Ryj44/exploring-the-lottery-ticket-hypothesis](https://www.lesswrong.com/posts/Z7R6jFjce3J2Ryj44/exploring-the-lottery-ticket-hypothesis)'
