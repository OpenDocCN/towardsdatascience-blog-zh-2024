- en: Feature Engineering for Time-Series Using PySpark on Databricks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PySpark在Databricks上进行时间序列特征工程
- en: 原文：[https://towardsdatascience.com/feature-engineering-for-time-series-using-pyspark-on-databricks-02b97d62a287?source=collection_archive---------3-----------------------#2024-05-15](https://towardsdatascience.com/feature-engineering-for-time-series-using-pyspark-on-databricks-02b97d62a287?source=collection_archive---------3-----------------------#2024-05-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/feature-engineering-for-time-series-using-pyspark-on-databricks-02b97d62a287?source=collection_archive---------3-----------------------#2024-05-15](https://towardsdatascience.com/feature-engineering-for-time-series-using-pyspark-on-databricks-02b97d62a287?source=collection_archive---------3-----------------------#2024-05-15)
- en: 'Discover the potentials of PySpark for time-series data: Ingest, extract, and
    visualize data, accompanied by practical implementation code'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索PySpark在时间序列数据中的潜力：获取、提取和可视化数据，并附有实际实施代码
- en: '[](https://medium.com/@johnleungTJ?source=post_page---byline--02b97d62a287--------------------------------)[![John
    Leung](../Images/ef45063e759e3450fa7f3c32b2f292c3.png)](https://medium.com/@johnleungTJ?source=post_page---byline--02b97d62a287--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--02b97d62a287--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--02b97d62a287--------------------------------)
    [John Leung](https://medium.com/@johnleungTJ?source=post_page---byline--02b97d62a287--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@johnleungTJ?source=post_page---byline--02b97d62a287--------------------------------)[![John
    Leung](../Images/ef45063e759e3450fa7f3c32b2f292c3.png)](https://medium.com/@johnleungTJ?source=post_page---byline--02b97d62a287--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--02b97d62a287--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--02b97d62a287--------------------------------)
    [John Leung](https://medium.com/@johnleungTJ?source=post_page---byline--02b97d62a287--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--02b97d62a287--------------------------------)
    ·9 min read·May 15, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--02b97d62a287--------------------------------)
    ·阅读时长：9分钟·2024年5月15日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: With the increasing demand for high-speed querying and analysis on large datasets,
    [Apache Spark](https://spark.apache.org/) has stood out as one of the most popular
    analytical engines in recent years. It is powerful in distributed data processing
    due to its [master-worker architecture](/a-beginners-guide-to-apache-spark-ff301cb4cd92).
    This includes a driver program that coordinates with the cluster manager (master)
    and controls the execution of distributing smaller tasks to worker nodes. Besides,
    designed as an in-memory data processing engine, Spark primarily uses RAM to store
    and manipulate data, rather than relying on disk storage. These coordinately facilitate
    faster execution of overall tasks.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对大规模数据集进行高速查询和分析需求的增加，[Apache Spark](https://spark.apache.org/)已经成为近年来最流行的分析引擎之一。由于其[主-从架构](/a-beginners-guide-to-apache-spark-ff301cb4cd92)，它在分布式数据处理方面非常强大。这包括一个与集群管理器（主节点）协调的驱动程序，并控制将较小任务分配给工作节点的执行。此外，作为一个内存数据处理引擎，Spark主要使用RAM来存储和处理数据，而不是依赖磁盘存储。这些协同作用加速了整体任务的执行。
- en: '![](../Images/f5ba1a26d9aa68b032e07ceb2446dd09.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f5ba1a26d9aa68b032e07ceb2446dd09.png)'
- en: Photo by [Dawid Zawiła](https://unsplash.com/@davealmine?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Dawid Zawiła](https://unsplash.com/@davealmine?utm_source=medium&utm_medium=referral)提供，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'Apache Spark: From low-level to high-level'
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache Spark：从低级到高级
- en: 'At the lower level, its architecture is designed based on two main abstractions:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在低级别上，它的架构基于两个主要抽象：
- en: Resilient Distributed Dataset ([RDD](https://spark.apache.org/docs/latest/rdd-programming-guide.html))
    — A low-level data abstraction in which each dataset can be divided into logical
    portions and executed on cluster worker nodes, thus aiding in parallel programming.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弹性分布式数据集（[RDD](https://spark.apache.org/docs/latest/rdd-programming-guide.html)）—
    一种低级数据抽象，其中每个数据集可以被划分为逻辑部分，并在集群工作节点上执行，从而有助于并行编程。
- en: Directed Acyclic Graph ([DAG](https://sparkbyexamples.com/spark/what-is-dag-in-spark/))
    — The representation that facilitates optimizing and scheduling the dependencies
    and sequences of tasks.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有向无环图（[DAG](https://sparkbyexamples.com/spark/what-is-dag-in-spark/)) — 一种有助于优化和调度任务依赖关系及执行顺序的表示方法。
- en: At the higher level, we can leverage a rich set of high-level tools using languages
    Scala, Python, or R. Examples of tools include [Spark SQL](https://spark.apache.org/sql/)
    for SQL and DataFrames, [Pandas API](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html)
    on Spark for Pandas workloads, and [Structured Streaming](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
    for stream processing.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在更高层次上，我们可以使用Scala、Python或R语言利用丰富的高级工具集。工具示例包括用于SQL和DataFrame的[Spark SQL](https://spark.apache.org/sql/)、用于Pandas工作负载的[Spark上的Pandas
    API](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html)，以及用于流处理的[结构化流式处理](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)。
- en: However, before enjoying these functionalities, we may need much effort to self-manage
    a Spark cluster with the setup of infrastructure and a bunch of complex tools,
    which could cause a headache.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在享受这些功能之前，我们可能需要花费大量精力来自行管理一个Spark集群，包括基础设施的设置和一堆复杂的工具，这可能会让人头疼。
- en: PySpark on Databricks
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PySpark在Databricks上的应用
- en: To address these challenges, [PySpark](https://spark.apache.org/docs/latest/api/python/index.html)
    on [Databricks](https://docs.databricks.com/en/introduction/index.html) is recently
    one of the high-level solutions in the industry. PySpark is the Python API for
    Spark, while Databricks is a full software platform built on top of Spark. It
    includes notebooks, infrastructure orchestration (auto-provisioning and scaling),
    process orchestration (job submission and scheduling), managed clusters, and even
    source control.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些挑战，[PySpark](https://spark.apache.org/docs/latest/api/python/index.html)在[Databricks](https://docs.databricks.com/en/introduction/index.html)上最近成为了行业中一种高级解决方案。PySpark是Spark的Python
    API，而Databricks是一个基于Spark构建的完整软件平台。它包括笔记本、基础设施编排（自动配置和扩展）、流程编排（作业提交和调度）、托管集群，甚至源代码控制。
- en: Using PySpark APIs in Databricks, we will demonstrate and perform a feature
    engineering project on time series data. In this hands-on journey, we will simulate
    how Pandas library generally behaves for data processing, with the extra benefits
    of scalability and parallelism.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在Databricks中使用PySpark API，我们将展示并执行一个时间序列数据的特征工程项目。在这个实践过程中，我们将模拟Pandas库在数据处理中的常规行为，同时享受可扩展性和并行处理的额外优势。
- en: '*Note: If you want to know further how to dynamically orchestrate this Databricks
    notebook written in PySpark APIs in Azure, you can click* [*here*](https://medium.com/towards-data-science/orchestrating-a-dynamic-time-series-pipeline-with-azure-data-factory-and-databricks-810819608231)*.*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：如果你想进一步了解如何在Azure中使用PySpark API动态编排这个Databricks笔记本，你可以点击* [*这里*](https://medium.com/towards-data-science/orchestrating-a-dynamic-time-series-pipeline-with-azure-data-factory-and-databricks-810819608231)*。*'
- en: '![](../Images/65a0654ef823dc1def1a7caf5abd5b30.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/65a0654ef823dc1def1a7caf5abd5b30.png)'
- en: Photo by [Alexandru Boicu](https://unsplash.com/@boiq?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Alexandru Boicu](https://unsplash.com/@boiq?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Consider a scenario in which you have the electric power consumption data for
    your household on hand, sampled at a one-minute rate from December 2006 to November
    2010\. Our objective is to ingest and manipulate data, extract features, and generate
    visualizations.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你手头有一份家庭电力消耗数据，数据是从2006年12月到2010年11月按一分钟的频率采样的。我们的目标是处理并操作数据，提取特征，并生成可视化结果。
- en: 'The [dataset](https://www.kaggle.com/datasets/uciml/electric-power-consumption-data-set/data)
    [with license as [Database: Open Database, Contents: Database Contents](https://opendatacommons.org/licenses/dbcl/1-0/)],
    obtained from Kaggle, includes various fields such as date, time, global power
    (active and reactive), voltage, global intensity, and submetering (1, 2 and 3).
    We can begin our analysis now.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这个[数据集](https://www.kaggle.com/datasets/uciml/electric-power-consumption-data-set/data)
    [根据许可证[数据库：开放数据库，内容：数据库内容](https://opendatacommons.org/licenses/dbcl/1-0/)]，来自Kaggle，包含多个字段，例如日期、时间、全局功率（有功和无功）、电压、全局电流和子计量（1、2和3）。我们现在可以开始分析。
- en: The initial setup
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始设置
- en: To begin, we need to create a user account for [Databricks Community Edition](https://www.databricks.com/product/faq/community-edition),
    which gives a suitable Databricks environment for our proof-of-concept purpose.
    Afterward, we can upload the input data file to the FileStore, a dedicated Databricks
    path. By clicking “Create Table in Notebook”, you are provided with the code template
    to initiate data ingestion.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要为[Databricks Community Edition](https://www.databricks.com/product/faq/community-edition)创建一个用户账户，该版本提供了适合我们概念验证目的的Databricks环境。之后，我们可以将输入数据文件上传到FileStore，这是Databricks的专用路径。点击“在笔记本中创建表”后，您将获得一个代码模板以启动数据导入。
- en: '![](../Images/c5e6d630d059bb13a169a9fdc1e5e9e6.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c5e6d630d059bb13a169a9fdc1e5e9e6.png)'
- en: Initial setup (1/2) — Create a user account (Image by author)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 初始设置 (1/2) — 创建用户账户（图源：作者）
- en: '![](../Images/f3f45f84fb426e8deba717b1e847a53c.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3f45f84fb426e8deba717b1e847a53c.png)'
- en: Initial setup (2/2) — Create a new table (Image by author)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 初始设置 (2/2) — 创建新表（图源：作者）
- en: Create a feature engineering project
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个特征工程项目
- en: '**#1 Ingest data**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**#1 导入数据**'
- en: Static data
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 静态数据
- en: We use the method `spark.read()` to read our data source and return a DataFrame,
    a relational table. It supports various data sources such as CSV, JSON, Parquet,
    and more. In this case, we read the power consumption data in CSV format with
    a defined schema, where the first row serves as the header, and “;” is as the
    delimiter.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用方法`spark.read()`读取数据源并返回一个数据框，这是一个关系表。它支持多种数据源，如CSV、JSON、Parquet等。在此示例中，我们以CSV格式读取电力消耗数据，并定义了模式，其中第一行作为表头，分隔符为“;”。
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output of the DataFrame with the first several rows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框输出的前几行：
- en: '![](../Images/0625e5a7006818e72ebba9b31738edb4.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0625e5a7006818e72ebba9b31738edb4.png)'
- en: Output of the DataFrame (Image by author)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框输出（图源：作者）
- en: Streaming data
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流数据
- en: In scenarios where data is continuously generated, we use stream processing
    techniques to read it incrementally. To demonstrate Spark’s behavior, I partitioned
    the original dataset into 10 subsets and stored them at the path “/FileStore/tables/stream/”
    beforehand. We then use another method `spark.readStream()`for streaming data.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据持续生成的场景中，我们使用流处理技术来逐步读取数据。为了演示Spark的行为，我将原始数据集划分为10个子集，并预先存储在路径“/FileStore/tables/stream/”下。然后我们使用另一个方法`spark.readStream()`来处理流数据。
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: It is worth mentioning that the `mode` setting as “dropMalformed” means that
    we discard the corrupted records, no matter whether the corruption is due to structural
    inconsistencies or other factors that make them unusable. Also, we choose to process
    only one file per trigger event.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，`mode`设置为“dropMalformed”意味着我们会丢弃损坏的记录，无论损坏是由于结构不一致还是其他使其无法使用的因素。此外，我们选择每次触发事件时仅处理一个文件。
- en: By starting to receive data and checking the record count every ten seconds,
    we can observe the continuous arrival of streaming data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通过开始接收数据并每十秒检查一次记录数，我们可以观察到流数据的持续到达。
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**#2 Manipulate and explore data**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**#2 操作和探索数据**'
- en: Data transformation
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据转换
- en: Since the number of rows with missing values is relatively negligible, we choose
    to drop them. Besides, we extract time-related features so that patterns can potentially
    be observed in higher dimensions later.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 由于缺失值的行数相对较少，我们选择删除这些行。此外，我们提取与时间相关的特征，以便稍后可以在更高维度中观察到潜在的模式。
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Data exploration
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据探索
- en: We can explore data with various basic [PySpark methods](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过各种基本的[PySpark方法](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html)来探索数据。
- en: (1) Select
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 选择
- en: The “‘select” method allows us to create a subset of the data frame column-wise.
    In this example, we select columns in descending order of global active power.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: “‘select”方法允许我们按列创建数据框的子集。在这个例子中，我们按全球有功功率的降序选择列。
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/63e2ed1f5139466002ac988a985dccd1.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63e2ed1f5139466002ac988a985dccd1.png)'
- en: Output of “select” method (Image by author)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: “select”方法的输出（图源：作者）
- en: (2) Filter
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 过滤
- en: 'This filters data points based on column values. In this example, we filter
    by two columns: “year” and “Global_intensity”.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这根据列值过滤数据点。在这个例子中，我们通过两个列进行过滤：“year”和“Global_intensity”。
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: (3) groupby
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: (3) groupby
- en: We can also perform some aggregations. In our dataset, we calculate the average
    of global active power and sub-meterings for different months.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以执行一些聚合操作。在我们的数据集中，我们计算了不同月份的全球有功功率和子计量的平均值。
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/0c23b506d6091e313838198cd696ef74.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c23b506d6091e313838198cd696ef74.png)'
- en: Output of “groupby” method (Image by author)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: “groupby”方法的输出（图像来自作者）
- en: '**#3 Extract features using Window functions**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**#3 使用窗口函数提取特征**'
- en: In addition to the above basic PySpark methods and functions, we can leverage
    [Window functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Window.html)
    to generate additional features to capture temporal dependencies and relationships
    in time series data. Assuming we have a transformed dataset (“df2”) where the
    total global active power is aggregated by day from one-minute rate samples. Let’s
    explore how we can obtain these features.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述基本的 PySpark 方法和函数外，我们还可以利用 [窗口函数](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Window.html)
    来生成额外的特征，以捕捉时间序列数据中的时间依赖性和关系。假设我们有一个经过转换的数据集（“df2”），该数据集中的全球有功功率按天聚合，来自每分钟的速率样本。让我们探索如何获取这些特征。
- en: (1) Lag features
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 滞后特征
- en: These represent the metrics’ values from previous days, which aids our models
    in learning from historical data and identifying trends.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这些表示前几天的度量值，有助于我们的模型从历史数据中学习并识别趋势。
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/434e8ccfb2bb2a4fb421b8df83620e29.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/434e8ccfb2bb2a4fb421b8df83620e29.png)'
- en: Output — Lag features (Image by author)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 — 滞后特征（图像来自作者）
- en: (2) Delta features
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: (2) Delta 特征
- en: This is to take a subsequent step to capture short-term changes or variations
    over time by calculating the difference between original data fields and the lag
    features.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过计算原始数据字段与滞后特征之间的差异，进一步捕捉短期变化或波动。
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/7a55306d33c5684e2d67047a8f5ece93.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a55306d33c5684e2d67047a8f5ece93.png)'
- en: Output — Delta features (Image by author)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 — Delta 特征（图像来自作者）
- en: (3) Window average features
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 窗口平均特征
- en: These features calculate the average value of our target data fields within
    a sliding window, enabling us to capture the smoothed patterns and relatively
    long-term trends. Here I pick the window sizes as 14 (2 weeks) and 30 (roughly
    1 month).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特征计算目标数据字段在滑动窗口中的平均值，使我们能够捕捉平滑的模式和相对长期的趋势。在这里，我选择了窗口大小为 14（2 周）和 30（大约 1 个月）。
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/6da0266dc8acdc4ecba82eea68778a0e.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6da0266dc8acdc4ecba82eea68778a0e.png)'
- en: Output — Window average features (Image by author)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 — 窗口平均特征（图像来自作者）
- en: (4) Exponentially weighted moving average (EWMA) features
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 指数加权移动平均（EWMA）特征
- en: 'EWMA features are the rectified version of window average features by assigning
    more weight and emphasis to recent data, and less to past data. A higher value
    of weight (alpha) means that the EWMA features track more closely to the original
    time series. Here I pick two separate weights: 0.2 and 0.8.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: EWMA 特征是通过赋予最近数据更多权重而修正后的窗口平均特征，过去的数据权重较小。权重（alpha）值越大，EWMA 特征与原始时间序列的匹配度越高。在这里，我选择了两个不同的权重值：0.2
    和 0.8。
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/6f8d723516896fea4200c35e0e10f34a.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f8d723516896fea4200c35e0e10f34a.png)'
- en: Output — EWMA features (Image by author)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 — EWMA 特征（图像来自作者）
- en: '**#4 Generate visualizations on Notebook**'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**#4 在 Notebook 中生成可视化**'
- en: After extracting time-related data and features using various PySpark functions
    and methods, we can use the built-in support from Databricks to create visualizations
    efficiently. This works by dragging and dropping data fields and configuring visual
    settings in the visualization editor. Some examples are shown below.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用各种 PySpark 函数和方法提取与时间相关的数据和特征后，我们可以利用 Databricks 提供的内建支持来高效地创建可视化。这是通过拖放数据字段并在可视化编辑器中配置可视化设置来实现的。以下是一些示例。
- en: 'Scatter plot: Relationship between global active power and global intensity'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 散点图：全球有功功率与全球强度之间的关系
- en: '*Interpretation: There is a highly positive correlation between the two fields.*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*解释：这两个字段之间有高度的正相关。*'
- en: '![](../Images/91d07a8535fb209f7e67989232457ed8.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91d07a8535fb209f7e67989232457ed8.png)'
- en: Scatter plot, using a visualization editor (Image by author)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 散点图，使用可视化编辑器（图像来自作者）
- en: 'Box plot: The distribution of global active power across hours'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 箱形图：全球有功功率在各个小时的分布
- en: '*Interpretation: There are relatively large variations of global active power
    from 7:00 to 21:00.*'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*解释：全球有功功率在7:00到21:00之间有较大的波动。*'
- en: '![](../Images/d737bb1829863d28a0311a0a48f2dafd.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d737bb1829863d28a0311a0a48f2dafd.png)'
- en: Box plot (Image by author)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 箱形图（图像来自作者）
- en: 'Line chart: The changes in total global active power, EWMA with alpha 0.2,
    and EWMA with alpha 0.8 from Jan 2008 to Mar 2008'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 折线图：2008年1月到2008年3月，全球有功功率的变化，EWMA（alpha = 0.2）和EWMA（alpha = 0.8）
- en: '*Interpretation: EWMA with alpha 0.8 sticks more closely to the original time
    series than EWMA with alpha 0.2.*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*解释：使用alpha为0.8的EWMA比使用alpha为0.2的EWMA更接近原始时间序列。*'
- en: '![](../Images/6505478ffa3793a4eb8a874d7cb56713.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6505478ffa3793a4eb8a874d7cb56713.png)'
- en: Line chart (Image by author)
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 折线图（图片来自作者）
- en: Besides, we can generate the default data profiles to display summary statistics
    such as count, % of missing values, and data distributions. This ensures data
    quality throughout the entire feature engineering process. These above visualizations
    can alternatively be created by the query output using Databricks SQL.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以生成默认数据概况，显示诸如计数、缺失值百分比和数据分布等汇总统计信息。这确保了整个特征工程过程中的数据质量。上述可视化也可以通过Databricks
    SQL查询输出生成。
- en: Wrapping it up
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: 'In our hands-on exploration, we used PySpark for feature engineering of time-series
    data using the Databricks platform:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实践探索中，我们使用PySpark进行时间序列数据的特征工程，使用的是Databricks平台：
- en: Ingesting static and streaming data, by using the `spark.read()` and `spark.readStream()`
    methods respectively.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过分别使用` spark.read()`和`spark.readStream()`方法来处理静态和流式数据。
- en: Manipulating and exploring data, by using a range of basic PySpark functions
    available in `pyspark.sql.functions`, and DataFrame methods.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用`pyspark.sql.functions`中的一系列基本PySpark函数和DataFrame方法来操作和探索数据。
- en: Extract trend-related features, by calculating the relationship within groups
    of data using `pyspark.sql.Window`.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过计算数据组之间的关系，使用`pyspark.sql.Window`提取趋势相关特征。
- en: Visualization, by using the built-in features of Databricks Notebook.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化，使用Databricks Notebook中的内置功能。
- en: When dealing with a massive dataset, PySpark is often preferred over Pandas
    due to its scalability and performance capabilities. PySpark’s support for lazy
    evaluation also means that computations are only performed when necessary, which
    reduces the overhead. However, Scala can sometimes be a better option, because
    we can closely catch up with the latest features as Spark itself is written in
    Scala. And more likely that systems which are less error-prone can be designed
    with the use of immutable objects. Different languages or libraries have their
    edges. Ultimately, the choice depends on the enterprise requirements, developers’
    learning curves, and integration with other systems.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理大规模数据集时，PySpark通常比Pandas更受青睐，因为它具有可扩展性和性能优势。PySpark支持懒评估，这意味着只有在必要时才会执行计算，从而减少了开销。然而，有时Scala可能是更好的选择，因为Spark本身是用Scala编写的，因此可以更紧密地跟进最新特性。而且，使用不可变对象的系统更不容易出错。因此，不同的语言或库各有其优势。最终的选择取决于企业的需求、开发者的学习曲线以及与其他系统的集成。
- en: Before you go
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在你离开之前
- en: If you enjoy this reading, I invite you tofollow my [Medium page](https://medium.com/@johnleungTJ)
    and [LinkedIn page](https://www.linkedin.com/in/john-leung-639800115/). By doing
    so, you can stay updated with exciting content related to data science side projects,
    and Machine Learning Operations (MLOps) demonstrations methodologies.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这篇文章，欢迎关注我的[Medium页面](https://medium.com/@johnleungTJ)和[LinkedIn页面](https://www.linkedin.com/in/john-leung-639800115/)。这样，你可以及时了解与数据科学副项目和机器学习运维（MLOps）演示方法相关的精彩内容。
- en: '[](/performing-customer-analytics-with-langchain-and-llms-0af4ea38f7b5?source=post_page-----02b97d62a287--------------------------------)
    [## Performing Customer Analytics with LangChain and LLMs'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/performing-customer-analytics-with-langchain-and-llms-0af4ea38f7b5?source=post_page-----02b97d62a287--------------------------------)
    [## 使用LangChain和LLMs进行客户分析'
- en: Discover the potentials and constraints of LangChain for customer analytics,
    accompanied by practical implementation…
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索LangChain在客户分析中的潜力与限制，并附有实际实现…
- en: towardsdatascience.com](/performing-customer-analytics-with-langchain-and-llms-0af4ea38f7b5?source=post_page-----02b97d62a287--------------------------------)
    [](/managing-the-technical-debts-of-machine-learning-systems-5b85d420ab9d?source=post_page-----02b97d62a287--------------------------------)
    [## Managing the Technical Debts of Machine Learning Systems
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/performing-customer-analytics-with-langchain-and-llms-0af4ea38f7b5?source=post_page-----02b97d62a287--------------------------------)
    [](/managing-the-technical-debts-of-machine-learning-systems-5b85d420ab9d?source=post_page-----02b97d62a287--------------------------------)
    [## 管理机器学习系统的技术债务
- en: Explore the practices with implementation codes for sustainably mitigating the
    cost of speedy delivery
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索通过实施代码可持续减轻快速交付成本的做法
- en: towardsdatascience.com](/managing-the-technical-debts-of-machine-learning-systems-5b85d420ab9d?source=post_page-----02b97d62a287--------------------------------)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/managing-the-technical-debts-of-machine-learning-systems-5b85d420ab9d?source=post_page-----02b97d62a287--------------------------------)'
