- en: 'Graph & Geometric ML in 2024: Where We Are and Whatâ€™s Next (Part I â€” Theory
    & Architectures)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2024å¹´å›¾å½¢ä¸å‡ ä½•æœºå™¨å­¦ä¹ ï¼šæˆ‘ä»¬ç›®å‰çš„çŠ¶å†µä¸æœªæ¥å±•æœ›ï¼ˆç¬¬ä¸€éƒ¨åˆ† â€” ç†è®ºä¸æ¶æ„ï¼‰
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-i-theory-architectures-3af5d38376e1?source=collection_archive---------1-----------------------#2024-01-16](https://towardsdatascience.com/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-i-theory-architectures-3af5d38376e1?source=collection_archive---------1-----------------------#2024-01-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-i-theory-architectures-3af5d38376e1?source=collection_archive---------1-----------------------#2024-01-16](https://towardsdatascience.com/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-i-theory-architectures-3af5d38376e1?source=collection_archive---------1-----------------------#2024-01-16)
- en: State-of-the-Art Digest
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æœ€å‰æ²¿æŠ€æœ¯æ‘˜è¦
- en: Following the tradition from previous years, we interviewed a cohort of distinguished
    and prolific academic and industrial experts in an attempt to summarise the highlights
    of the past year and predict what is in store for 2024\. Past 2023 was so ripe
    with results that we had to break this post into two parts. This is Part I focusing
    on theory & new architectures, see also [Part II](https://medium.com/towards-data-science/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-ii-applications-1ed786f7bf63)
    on applications.
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ ¹æ®å¾€å¹´ä¼ ç»Ÿï¼Œæˆ‘ä»¬é‡‡è®¿äº†ä¸€æ‰¹æ°å‡ºä¸”é«˜äº§çš„å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œä¸“å®¶ï¼Œæ—¨åœ¨æ€»ç»“è¿‡å»ä¸€å¹´çš„äº®ç‚¹å¹¶é¢„æµ‹2024å¹´çš„å‘å±•è¶‹åŠ¿ã€‚2023å¹´æˆæœä¸°å¯Œï¼Œæˆ‘ä»¬ä¸å¾—ä¸å°†è¿™ç¯‡æ–‡ç« åˆ†ä¸ºä¸¤éƒ¨åˆ†ã€‚è¿™æ˜¯ç¬¬ä¸€éƒ¨åˆ†ï¼Œé‡ç‚¹è®¨è®ºç†è®ºä¸æ–°æ¶æ„ï¼Œå¦è¯·å‚é˜…å…³äºåº”ç”¨çš„[ç¬¬äºŒéƒ¨åˆ†](https://medium.com/towards-data-science/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-ii-applications-1ed786f7bf63)ã€‚
- en: '[](https://mgalkin.medium.com/?source=post_page---byline--3af5d38376e1--------------------------------)[![Michael
    Galkin](../Images/c5eb13334712ca0462d8a5df4a268ad0.png)](https://mgalkin.medium.com/?source=post_page---byline--3af5d38376e1--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3af5d38376e1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3af5d38376e1--------------------------------)
    [Michael Galkin](https://mgalkin.medium.com/?source=post_page---byline--3af5d38376e1--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mgalkin.medium.com/?source=post_page---byline--3af5d38376e1--------------------------------)[![Michael
    Galkin](../Images/c5eb13334712ca0462d8a5df4a268ad0.png)](https://mgalkin.medium.com/?source=post_page---byline--3af5d38376e1--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3af5d38376e1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3af5d38376e1--------------------------------)
    [Michael Galkin](https://mgalkin.medium.com/?source=post_page---byline--3af5d38376e1--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3af5d38376e1--------------------------------)
    Â·30 min readÂ·Jan 16, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3af5d38376e1--------------------------------)
    Â·é˜…è¯»æ—¶é—´ï¼š30åˆ†é’ŸÂ·2024å¹´1æœˆ16æ—¥
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f888e845d4bdb9f34131d8b0544d71fc.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f888e845d4bdb9f34131d8b0544d71fc.png)'
- en: Image by Authors with some help from DALL-E 3.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡ï¼Œéƒ¨åˆ†æ¥è‡ªDALL-E 3ã€‚
- en: '*The post is written and edited by* [*Michael Galkin*](https://twitter.com/michael_galkin)
    *and* [*Michael Bronstein*](https://twitter.com/mmbronstein) *with significant
    contributions from* [*Johannes Brandstetter*](https://twitter.com/jo_brandstetter)*,*
    [*Ä°smail Ä°lkan Ceylan*](https://twitter.com/ismaililkanc/)*,* [*Francesco Di Giovanni*](https://twitter.com/Francesco_dgv)*,*
    [*Ben Finkelshtein*](https://twitter.com/benfinkelshtein)*,* [*Kexin Huang*](https://twitter.com/KexinHuang5)*,*
    [*Chaitanya Joshi*](https://twitter.com/chaitjo)*,* [*Chen Lin*](https://twitter.com/WillLin1028)*,*
    [*Christopher Morris*](https://twitter.com/chrsmrrs)*,* [*Mathilde Papillon*](https://twitter.com/mathildepapillo)*,*
    [*Liudmila Prokhorenkova*](https://twitter.com/LProkhorenkova)*,* [*Bastian Rieck*](https://twitter.com/Pseudomanifold)*,*
    [*David Ruhe*](https://twitter.com/djjruhe)*,* [*Hannes StÃ¤rk*](https://twitter.com/HannesStaerk)*,
    and* [*Petar VeliÄkoviÄ‡*](https://twitter.com/PetarV_93)*.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*æœ¬æ–‡ç”±* [*Michael Galkin*](https://twitter.com/michael_galkin) *å’Œ* [*Michael
    Bronstein*](https://twitter.com/mmbronstein) *æ’°å†™å’Œç¼–è¾‘ï¼Œå¹¶ç”±* [*Johannes Brandstetter*](https://twitter.com/jo_brandstetter)*,*
    [*Ä°smail Ä°lkan Ceylan*](https://twitter.com/ismaililkanc/)*,* [*Francesco Di Giovanni*](https://twitter.com/Francesco_dgv)*,*
    [*Ben Finkelshtein*](https://twitter.com/benfinkelshtein)*,* [*Kexin Huang*](https://twitter.com/KexinHuang5)*,*
    [*Chaitanya Joshi*](https://twitter.com/chaitjo)*,* [*Chen Lin*](https://twitter.com/WillLin1028)*,*
    [*Christopher Morris*](https://twitter.com/chrsmrrs)*,* [*Mathilde Papillon*](https://twitter.com/mathildepapillo)*,*
    [*Liudmila Prokhorenkova*](https://twitter.com/LProkhorenkova)*,* [*Bastian Rieck*](https://twitter.com/Pseudomanifold)*,*
    [*David Ruhe*](https://twitter.com/djjruhe)*,* [*Hannes StÃ¤rk*](https://twitter.com/HannesStaerk)*
    å’Œ* [*Petar VeliÄkoviÄ‡*](https://twitter.com/PetarV_93)*.*  '
- en: '[Theory of Graph Neural Networks](#79aa)'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[å›¾ç¥ç»ç½‘ç»œçš„ç†è®º](#79aa)'
- en: 1\. [Message passing neural networks and Graph Transformers](#5903)
  id: totrans-11
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 1. [ä¿¡æ¯ä¼ é€’ç¥ç»ç½‘ç»œä¸å›¾Transformer](#5903)
- en: 2\. [Graph components, biconnectivity & planarity](#a6d7)
  id: totrans-12
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2. [å›¾ç»„ä»¶ã€äºŒè¿é€šæ€§ä¸å¹³é¢æ€§](#a6d7)
- en: 3\. [Aggregation functions & uniform expressivity](#27e6)
  id: totrans-13
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 3. [èšåˆå‡½æ•°ä¸ç»Ÿä¸€è¡¨è¾¾èƒ½åŠ›](#27e6)
- en: 4\. [Convergence & zero-one laws of GNNs](#645f)
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 4. [GNNçš„æ”¶æ•›æ€§ä¸é›¶ä¸€æ³•åˆ™](#645f)
- en: 5. [Descriptive complexity of GNNs](#c8ac)
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 5. [GNNçš„æè¿°å¤æ‚åº¦](#c8ac)
- en: 6\. [Fine-grained expressivity of GNNs](#9b59)
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 6. [GNNçš„ç²¾ç»†åŒ–è¡¨è¾¾èƒ½åŠ›](#9b59)
- en: 7\. [Expressivity results for Subgraph GNNs](#06c2)
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 7. [å­å›¾GNNçš„è¡¨è¾¾èƒ½åŠ›ç»“æœ](#06c2)
- en: 8\. [Expressivity for Link Prediction and Knowledge Graphs](#ab19)
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 8. [é“¾è·¯é¢„æµ‹ä¸çŸ¥è¯†å›¾è°±çš„è¡¨è¾¾èƒ½åŠ›](#ab19)
- en: 9\. [Over-squashing & Expressivity](#c284)
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 9. [è¿‡åº¦å‹ç¼©ä¸è¡¨è¾¾èƒ½åŠ›](#c284)
- en: 10\. [Generalization and Extrapolation capabilities of GNNs](#4a32)
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 10. [GNNçš„æ³›åŒ–ä¸å¤–æ¨èƒ½åŠ›](#4a32)
- en: 11\. [Predictions time!](#4f30)
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 11. [é¢„æµ‹æ—¶é—´ï¼](#4f30)
- en: '[New and Exotic Message Passing](#b09f)'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[æ–°å‹ä¸å¼‚åŸŸä¿¡æ¯ä¼ é€’](#b09f)'
- en: '[Beyond Graphs](#9a3b)'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[è¶…è¶Šå›¾å½¢](#9a3b)'
- en: 1\. [Topology](#efa6)
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 1. [æ‹“æ‰‘](#efa6)
- en: 2\. [Geometric Algebras](#a368)
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2. [å‡ ä½•ä»£æ•°](#a368)
- en: 3\. [PDEs](#5b67)
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 3. [åå¾®åˆ†æ–¹ç¨‹](#5b67)
- en: '[Robustness & Explainability](#8171)'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[é²æ£’æ€§ä¸å¯è§£é‡Šæ€§](#8171)'
- en: '[Graph Transformers](#e7b4)'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[å›¾Transformer](#e7b4)'
- en: '[New Datasets & Benchmarks](#cf16)'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[æ–°æ•°æ®é›†ä¸åŸºå‡†æµ‹è¯•](#cf16)'
- en: '[Conferences, Courses & Community](#926c)'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ä¼šè®®ã€è¯¾ç¨‹ä¸ç¤¾åŒº](#926c)'
- en: '[Memes of 2023](#f1d3)'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2023å¹´çš„è¿·å› ](#f1d3)'
- en: 'The legend we will be using throughout the text:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†åœ¨æ•´ä¸ªæ–‡æœ¬ä¸­ä½¿ç”¨çš„å›¾ä¾‹ï¼š
- en: ğŸ’¡ - yearâ€™s highlight
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ - æœ¬å¹´åº¦äº®ç‚¹
- en: ğŸ‹ï¸ - challenges
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ‹ï¸ - æŒ‘æˆ˜
- en: â¡ï¸ - current/next developments
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ - å½“å‰/ä¸‹ä¸€æ­¥å‘å±•
- en: ğŸ”®- predictions/speculations
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”®- é¢„æµ‹/çŒœæµ‹
- en: Theory of Graph Neural Networks
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å›¾ç¥ç»ç½‘ç»œçš„ç†è®º
- en: '*Michael Bronstein (Oxford), Francesco Di Giovanni (Oxford), Ä°smail Ä°lkan Ceylan
    (Oxford), Chris Morris (RWTH Aachen)*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿ˆå…‹å°”Â·å¸ƒæœ—æ–¯å¦ï¼ˆç‰›æ´¥å¤§å­¦ï¼‰ã€å¼—æœ—åˆ‡æ–¯ç§‘Â·è¿ªÂ·ä¹”ç“¦å°¼ï¼ˆç‰›æ´¥å¤§å­¦ï¼‰ã€ä¼Šæ–¯æ¢…å°”Â·ä¼Šå°”åÂ·æ°å…°ï¼ˆç‰›æ´¥å¤§å­¦ï¼‰ã€å…‹é‡Œæ–¯Â·è«é‡Œæ–¯ï¼ˆäºšç›å·¥ä¸šå¤§å­¦ï¼‰*'
- en: '**Message Passing Neural Networks & Graph Transformers**'
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ä¿¡æ¯ä¼ é€’ç¥ç»ç½‘ç»œä¸å›¾Transformer**'
- en: Graph Transformers are a relatively recent trend in graph ML, trying to extend
    the successes of Transformers from sequences to graphs. As far as traditional
    expressivity results go, these architectures do not offer any particular advantages.
    In fact, it is arguable that most of their benefits in terms of expressivity (see
    e.g. [Kreuzer et al.](https://arxiv.org/abs/2106.03893)) come from powerful structural
    encodings rather than the architecture itself and such encodings can in principle
    be used with MPNNs.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾Transformeræ˜¯å›¾æœºå™¨å­¦ä¹ ä¸­ä¸€ä¸ªç›¸å¯¹è¾ƒæ–°çš„è¶‹åŠ¿ï¼Œè¯•å›¾å°†Transformeråœ¨åºåˆ—ä¸Šçš„æˆåŠŸæ‰©å±•åˆ°å›¾å½¢ä¸Šã€‚å°±ä¼ ç»Ÿçš„è¡¨è¾¾èƒ½åŠ›ç»“æœè€Œè¨€ï¼Œè¿™äº›æ¶æ„å¹¶æ²¡æœ‰æä¾›ä»»ä½•ç‰¹åˆ«çš„ä¼˜åŠ¿ã€‚äº‹å®ä¸Šï¼Œå¯ä»¥è¯´ï¼Œå®ƒä»¬åœ¨è¡¨è¾¾èƒ½åŠ›æ–¹é¢çš„å¤§å¤šæ•°ä¼˜åŠ¿ï¼ˆä¾‹å¦‚ï¼Œè§[Kreuzerç­‰äºº](https://arxiv.org/abs/2106.03893)ï¼‰æ¥è‡ªäºå¼ºå¤§çš„ç»“æ„ç¼–ç ï¼Œè€Œéæ¶æ„æœ¬èº«ï¼Œè€Œè¿™äº›ç¼–ç åŸåˆ™ä¸Šä¹Ÿå¯ä»¥ä¸MPNNä¸€èµ·ä½¿ç”¨ã€‚
- en: 'In a recent paper, [Cai et al.](https://arxiv.org/abs/2301.11956) investigate
    the connection between MPNNs and (graph) Transformers showing that an MPNN with
    a virtual node â€” an auxiliary node that is connected to all other nodes in a specific
    way â€” can simulate a (graph) Transformer. This architecture is *non-uniform*,
    i.e., the size and structure of the neural networks may depend on the size of
    the input graphs. Interestingly, once we restrict our attention to linear Transformers
    (e.g., Performer) then there is a *uniform* result: there exists a single MPNN
    using a virtual node that can approximate a linear transformer such as Performer
    on any input of any size.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ€è¿‘çš„ä¸€ç¯‡è®ºæ–‡ä¸­ï¼Œ[è”¡ç­‰äºº](https://arxiv.org/abs/2301.11956)æ¢è®¨äº†MPNNä¸ï¼ˆå›¾å½¢ï¼‰Transformerä¹‹é—´çš„è”ç³»ï¼Œè¡¨æ˜ä¸€ä¸ªå¸¦æœ‰è™šæ‹ŸèŠ‚ç‚¹çš„MPNNâ€”â€”ä¸€ä¸ªä»¥ç‰¹å®šæ–¹å¼ä¸æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹è¿æ¥çš„è¾…åŠ©èŠ‚ç‚¹â€”â€”å¯ä»¥æ¨¡æ‹Ÿä¸€ä¸ªï¼ˆå›¾å½¢ï¼‰Transformerã€‚è¿™ç§æ¶æ„æ˜¯*éç»Ÿä¸€*çš„ï¼Œå³ç¥ç»ç½‘ç»œçš„å¤§å°å’Œç»“æ„å¯èƒ½ä¾èµ–äºè¾“å…¥å›¾å½¢çš„å¤§å°ã€‚æœ‰è¶£çš„æ˜¯ï¼Œä¸€æ—¦æˆ‘ä»¬å°†æ³¨æ„åŠ›é™åˆ¶åˆ°çº¿æ€§Transformerï¼ˆä¾‹å¦‚ï¼ŒPerformerï¼‰ï¼Œå°±ä¼šæœ‰ä¸€ä¸ª*ç»Ÿä¸€*çš„ç»“æœï¼šå­˜åœ¨ä¸€ä¸ªä½¿ç”¨è™šæ‹ŸèŠ‚ç‚¹çš„å•ä¸€MPNNï¼Œå¯ä»¥åœ¨ä»»ä½•å¤§å°çš„è¾“å…¥ä¸Šé€¼è¿‘çº¿æ€§Transformerï¼Œå¦‚Performerã€‚
- en: '![](../Images/58777f0195680c0818525a67e58b2031.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/58777f0195680c0818525a67e58b2031.png)'
- en: 'Figure from [Cai et al.](https://arxiv.org/abs/2301.11956): (a) MPNN with a
    virtual node, (b) a Transformer.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[è”¡ç­‰äºº](https://arxiv.org/abs/2301.11956)çš„å›¾ï¼š (a) å¸¦è™šæ‹ŸèŠ‚ç‚¹çš„MPNNï¼Œ(b) ä¸€ä¸ªTransformerã€‚'
- en: This is related to the discussions on whether graph transformer architectures
    present advantages for capturing long-range dependencies when compared to MPNNs.
    Graph transformers are compared to MPNNs that include a global computation component
    through the use of virtual nodes, which is a common practice. [Cai et al.](https://arxiv.org/abs/2301.11956)
    empirically show that MPNNs with virtual nodes can surpass the performance of
    graph transformers on the Long-Range Graph Benchmark (LRGB, [Dwivedi et al.](https://arxiv.org/abs/2206.08164))
    Moreover, [TÃ¶nshoff et al.](https://arxiv.org/abs/2309.00367) re-evaluate MPNN
    baselines on the LRGB benchmark to find out that the earlier reported performance
    gap in favor of graph transformers was overestimated due to suboptimal hyperparameter
    choices, essentially closing the gap between MPNNs and graph Transformers.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸å…³äºå›¾å˜æ¢å™¨æ¶æ„æ˜¯å¦ç›¸è¾ƒäºMPNNåœ¨æ•æ‰é•¿ç¨‹ä¾èµ–æ–¹é¢å…·æœ‰ä¼˜åŠ¿çš„è®¨è®ºç›¸å…³ã€‚å›¾å˜æ¢å™¨ä¸MPNNè¿›è¡Œæ¯”è¾ƒï¼Œåè€…é€šè¿‡ä½¿ç”¨è™šæ‹ŸèŠ‚ç‚¹åŒ…å«äº†ä¸€ä¸ªå…¨å±€è®¡ç®—ç»„ä»¶ï¼Œè¿™æ˜¯å¸¸è§çš„åšæ³•ã€‚[Cai
    et al.](https://arxiv.org/abs/2301.11956) å®è¯è¡¨æ˜ï¼Œå¸¦æœ‰è™šæ‹ŸèŠ‚ç‚¹çš„MPNNèƒ½å¤Ÿåœ¨é•¿ç¨‹å›¾åŸºå‡†ï¼ˆLRGBï¼Œ[Dwivedi
    et al.](https://arxiv.org/abs/2206.08164)ï¼‰ä¸Šè¶…è¶Šå›¾å˜æ¢å™¨çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œ[TÃ¶nshoff et al.](https://arxiv.org/abs/2309.00367)
    åœ¨LRGBåŸºå‡†ä¸Šé‡æ–°è¯„ä¼°äº†MPNNçš„åŸºå‡†ï¼Œå‘ç°å…ˆå‰æŠ¥å‘Šçš„å›¾å˜æ¢å™¨æ€§èƒ½å·®è·è¢«é«˜ä¼°äº†ï¼Œè¿™æ˜¯ç”±äºäºšä¼˜çš„è¶…å‚æ•°é€‰æ‹©ï¼Œå®è´¨ä¸Šç¼©å°äº†MPNNå’Œå›¾å˜æ¢å™¨ä¹‹é—´çš„å·®è·ã€‚
- en: '![](../Images/e9fc784a08cfc83613d260e0224f4499.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e9fc784a08cfc83613d260e0224f4499.png)'
- en: 'Figure from [Lim et al.](https://arxiv.org/abs/2202.13013): SignNet pipeline.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[Lim et al.](https://arxiv.org/abs/2202.13013)ä¸­çš„å›¾ï¼šSignNetç®¡é“ã€‚'
- en: It is also well-known that common Laplacian positional encodings (e.g., LapPE),
    are not invariant to the changes of signs and basis of eigenvectors. The lack
    of invariance makes it easier to obtain (non-uniform) universality results, but
    these models do not compute graph invariants as a consequence. This has motivated
    a body of work this year, including the study of sign and basis invariant networks
    ([Lim et al., 2023a](https://arxiv.org/abs/2202.13013)) and sign equivariant networks
    ([Lim et al., 2023b](https://arxiv.org/abs/2312.02339)). These findings suggest
    that more research is necessary to theoretically ground the claims commonly found
    in the literature regarding the comparisons of MPNNs and graph transformers.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜å¹¿ä¸ºäººçŸ¥ï¼Œå¸¸è§çš„æ‹‰æ™®æ‹‰æ–¯ä½ç½®ç¼–ç ï¼ˆä¾‹å¦‚ï¼ŒLapPEï¼‰å¯¹ç‰¹å¾å‘é‡çš„ç¬¦å·å’ŒåŸºå˜æ¢ä¸å…·æœ‰ä¸å˜æ€§ã€‚ç¼ºä¹ä¸å˜æ€§ä½¿å¾—è·å¾—ï¼ˆéå‡åŒ€ï¼‰æ™®é€‚æ€§ç»“æœå˜å¾—æ›´å®¹æ˜“ï¼Œä½†è¿™äº›æ¨¡å‹å› æ­¤ä¸ä¼šè®¡ç®—å›¾çš„ä¸å˜æ€§ã€‚è¿™æ¿€å‘äº†ä»Šå¹´ä¸€ç³»åˆ—çš„ç ”ç©¶ï¼ŒåŒ…æ‹¬ç¬¦å·å’ŒåŸºä¸å˜ç½‘ç»œçš„ç ”ç©¶ï¼ˆ[Lim
    et al., 2023a](https://arxiv.org/abs/2202.13013)ï¼‰ä»¥åŠç¬¦å·ç­‰å˜ç½‘ç»œçš„ç ”ç©¶ï¼ˆ[Lim et al., 2023b](https://arxiv.org/abs/2312.02339)ï¼‰ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå…³äºMPNNå’Œå›¾å˜æ¢å™¨æ¯”è¾ƒçš„æ–‡çŒ®ä¸­å¸¸è§çš„ä¸»å¼ ï¼Œä»éœ€è¿›è¡Œæ›´å¤šçš„ç†è®ºç ”ç©¶ã€‚
- en: '**Graph components, biconnectivity, and planarity**'
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**å›¾ç»„ä»¶ã€åŒè¿é€šæ€§å’Œå¹³é¢æ€§**'
- en: '![](../Images/f38beda635e6b001c4b4b95828e6afbe.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f38beda635e6b001c4b4b95828e6afbe.png)'
- en: Figure originally by Zyqqh at [Wikipedia](https://commons.wikimedia.org/w/index.php?curid=19053091).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åˆç”±Zyqqhåœ¨[Wikipedia](https://commons.wikimedia.org/w/index.php?curid=19053091)ä¸Šå‘å¸ƒçš„å›¾ã€‚
- en: '[Zhang et al. (2023a)](https://arxiv.org/abs/2301.09505) brings the study of
    graph biconnectivity to the attention of graph ML community. There are many results
    presented by [Zhang et al. (2023a)](https://arxiv.org/abs/2301.09505) relative
    to different biconnectivity metrics. It has been shown that standard MPNNs cannot
    detect graph biconnectivity unlike many existing higher-order models (i.e., those
    that can match the power of 2-FWL). On the other hand, Graphormers with certain
    distance encodings and subgraph GNNs such as ESAN can detect graph biconnectivity.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[Zhang et al. (2023a)](https://arxiv.org/abs/2301.09505)å°†å›¾çš„åŒè¿é€šæ€§ç ”ç©¶å¸¦å…¥äº†å›¾æœºå™¨å­¦ä¹ é¢†åŸŸã€‚ç”±[Zhang
    et al. (2023a)](https://arxiv.org/abs/2301.09505)æå‡ºçš„è®¸å¤šç»“æœä¸ä¸åŒçš„åŒè¿é€šæ€§åº¦é‡ç›¸å…³ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ ‡å‡†çš„MPNNæ— æ³•æ£€æµ‹å›¾çš„åŒè¿é€šæ€§ï¼Œè€Œè®¸å¤šç°æœ‰çš„é«˜é˜¶æ¨¡å‹ï¼ˆå³èƒ½å¤ŸåŒ¹é…2-FWLèƒ½åŠ›çš„æ¨¡å‹ï¼‰åˆ™å¯ä»¥ã€‚å¦ä¸€æ–¹é¢ï¼Œå…·æœ‰æŸäº›è·ç¦»ç¼–ç å’Œå­å›¾GNNï¼ˆå¦‚ESANï¼‰çš„Graphormersèƒ½å¤Ÿæ£€æµ‹å›¾çš„åŒè¿é€šæ€§ã€‚'
- en: '![](../Images/73874a490cdd02918773c2f31c4c5701.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/73874a490cdd02918773c2f31c4c5701.png)'
- en: 'Figure from [Dimitrov et al. (2023)](https://arxiv.org/abs/2307.01180): LHS
    shows the graph decompositions (A-C) and RHS shows the associated encoders (D-F)
    and the update equation (G).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[Dimitrov et al. (2023)](https://arxiv.org/abs/2307.01180)ä¸­çš„å›¾ï¼šå·¦ä¾§æ˜¾ç¤ºäº†å›¾çš„åˆ†è§£ï¼ˆA-Cï¼‰ï¼Œå³ä¾§æ˜¾ç¤ºäº†ç›¸å…³çš„ç¼–ç å™¨ï¼ˆD-Fï¼‰å’Œæ›´æ–°æ–¹ç¨‹ï¼ˆGï¼‰ã€‚'
- en: '[Dimitrov et al. (2023)](https://arxiv.org/abs/2307.01180) rely on graph decompositions
    to develop dedicated architectures for learning with planar graphs. The idea is
    to align with a variation of the classical [Hopcroft & Tarjan](https://www.sciencedirect.com/science/article/pii/0020019071900196)
    algorithm for planar isomorphism testing. [Dimitrov et al. (2023)](https://arxiv.org/abs/2307.01180)
    first decompose the graph into its biconnected and triconnected components, and
    afterwards learn representations for nodes, cut nodes, biconnected components,
    and triconnected components. This is achieved using the classical structures of
    Block-Cut Trees and SPQR Trees which can be computed in linear time. The resulting
    framework is called [PlanE](https://arxiv.org/abs/2307.01180) and contains architectures
    such as [BasePlanE](https://arxiv.org/abs/2307.01180). BasePlanE computes *isomorphism-complete
    graph invariants* and hence it can distinguish any pair of planar graphs. The
    key contribution of this work is to design architectures for efficiently learning
    complete invariants of planar graphs while remaining practically scalable. It
    is worth noting that 3-FWL is known to be complete on planar graphs ([Kiefer et
    al., 2019](https://dl.acm.org/doi/10.1145/3333003)), but this algorithm is not
    scalable.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[Dimitrov ç­‰äºº (2023)](https://arxiv.org/abs/2307.01180)ä¾èµ–å›¾åˆ†è§£æ¥å¼€å‘ä¸“é—¨çš„æ¶æ„ï¼Œä»¥ä¾¿åœ¨å¹³é¢å›¾ä¸Šè¿›è¡Œå­¦ä¹ ã€‚å…¶æ€è·¯æ˜¯ä¸ç»å…¸çš„[Hopcroft
    & Tarjan](https://www.sciencedirect.com/science/article/pii/0020019071900196)å¹³é¢åŒæ„æµ‹è¯•ç®—æ³•çš„å˜ç§å¯¹é½ã€‚[Dimitrov
    ç­‰äºº (2023)](https://arxiv.org/abs/2307.01180)é¦–å…ˆå°†å›¾åˆ†è§£ä¸ºå…¶åŒè¿é€šå’Œä¸‰è¿é€šåˆ†é‡ï¼Œç„¶åå­¦ä¹ èŠ‚ç‚¹ã€å‰²èŠ‚ç‚¹ã€åŒè¿é€šåˆ†é‡å’Œä¸‰è¿é€šåˆ†é‡çš„è¡¨ç¤ºã€‚è¿™ä¸ªè¿‡ç¨‹é€šè¿‡ä½¿ç”¨å¯ä»¥åœ¨çº¿æ€§æ—¶é—´å†…è®¡ç®—çš„ç»å…¸ç»“æ„â€”â€”å—å‰²æ ‘ï¼ˆBlock-Cut
    Treesï¼‰å’ŒSPQRæ ‘ï¼ˆSPQR Treesï¼‰æ¥å®ç°ã€‚æœ€ç»ˆçš„æ¡†æ¶è¢«ç§°ä¸º[PlanE](https://arxiv.org/abs/2307.01180)ï¼ŒåŒ…å«å¦‚[BasePlanE](https://arxiv.org/abs/2307.01180)ç­‰æ¶æ„ã€‚BasePlanEè®¡ç®—*åŒæ„å®Œå…¨å›¾ä¸å˜é‡*ï¼Œå› æ­¤å®ƒèƒ½å¤ŸåŒºåˆ†ä»»æ„ä¸€å¯¹å¹³é¢å›¾ã€‚è¯¥å·¥ä½œçš„ä¸»è¦è´¡çŒ®æ˜¯è®¾è®¡äº†ä¸€ç§æ¶æ„ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°å­¦ä¹ å¹³é¢å›¾çš„å®Œæ•´ä¸å˜é‡ï¼ŒåŒæ—¶ä¿æŒåœ¨å®é™…åº”ç”¨ä¸­çš„å¯æ‰©å±•æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ3-FWLå·²çŸ¥åœ¨å¹³é¢å›¾ä¸Šæ˜¯å®Œå…¨çš„ï¼ˆ[Kiefer
    ç­‰äºº, 2019](https://dl.acm.org/doi/10.1145/3333003)ï¼‰ï¼Œä½†è¯¥ç®—æ³•å¹¶ä¸å¯æ‰©å±•ã€‚'
- en: '**Aggregation functions: A uniform expressiveness study**'
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**èšåˆå‡½æ•°ï¼šç»Ÿä¸€è¡¨è¾¾æ€§ç ”ç©¶**'
- en: It was broadly argued that different aggregation functions have their place,
    but this had not been rigorously proven. In fact, in the non-uniform setup, sum
    aggregation with MLPs yields an injective mapping and as a result subsumes other
    aggregation functions ([Xu et al., 2020](https://arxiv.org/abs/1810.00826)), which
    builds on earlier results ([Zaheer et al., 2017](https://arxiv.org/abs/1703.06114)).
    The situation is different in the uniform setup, where one fixed model is required
    to work on *all* graphs. [Rosenbluth et al. (2023)](https://arxiv.org/abs/2302.11603)
    show that sum aggregation does not always subsume other aggregations in the uniform
    setup. If, for example, we consider an unbounded feature domain, sum aggregation
    networks cannot even approximate mean aggregation networks. Interestingly, even
    for the positive results, where sum aggregation is shown to approximate other
    aggregations, the presented constructions generally require a large number of
    layers (growing with the inverse of the approximation error).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: äººä»¬å¹¿æ³›è®¤ä¸ºä¸åŒçš„èšåˆå‡½æ•°å„æœ‰å…¶é€‚ç”¨ä¹‹å¤„ï¼Œä½†è¿™å¹¶æ²¡æœ‰è¢«ä¸¥æ ¼è¯æ˜ã€‚äº‹å®ä¸Šï¼Œåœ¨éç»Ÿä¸€è®¾ç½®ä¸‹ï¼Œä½¿ç”¨MLPçš„æ±‚å’Œèšåˆä¼šäº§ç”Ÿä¸€ä¸ªå•å°„æ˜ å°„ï¼Œå› æ­¤å¯ä»¥æ¶µç›–å…¶ä»–èšåˆå‡½æ•°ï¼ˆ[Xu
    ç­‰äºº, 2020](https://arxiv.org/abs/1810.00826)ï¼‰ï¼Œè¿™ä¸€ç»“æœå»ºç«‹åœ¨æ—©æœŸçš„ç ”ç©¶åŸºç¡€ä¸Šï¼ˆ[Zaheer ç­‰äºº, 2017](https://arxiv.org/abs/1703.06114)ï¼‰ã€‚è€Œåœ¨ç»Ÿä¸€è®¾ç½®ä¸‹ï¼Œæƒ…å†µåˆ™ä¸åŒï¼Œè¿™è¦æ±‚ä¸€ä¸ªå›ºå®šçš„æ¨¡å‹èƒ½å¤Ÿåœ¨*æ‰€æœ‰*å›¾ä¸Šæœ‰æ•ˆå·¥ä½œã€‚[Rosenbluth
    ç­‰äºº (2023)](https://arxiv.org/abs/2302.11603)è¡¨æ˜ï¼Œåœ¨ç»Ÿä¸€è®¾ç½®ä¸­ï¼Œæ±‚å’Œèšåˆå¹¶ä¸æ€»æ˜¯èƒ½å¤Ÿæ¶µç›–å…¶ä»–èšåˆæ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªæ— é™çš„ç‰¹å¾åŸŸï¼Œæ±‚å’Œèšåˆç½‘ç»œç”šè‡³æ— æ³•è¿‘ä¼¼å‡å€¼èšåˆç½‘ç»œã€‚æœ‰è¶£çš„æ˜¯ï¼Œå³ä½¿æ˜¯åœ¨ä¸€äº›æ­£é¢çš„ç»“æœä¸­ï¼Œå…¶ä¸­æ±‚å’Œèšåˆè¢«è¯æ˜å¯ä»¥è¿‘ä¼¼å…¶ä»–èšåˆæ–¹æ³•ï¼Œæ‰€å‘ˆç°çš„æ„é€ é€šå¸¸ä¹Ÿéœ€è¦å¤§é‡çš„å±‚ï¼ˆéšç€è¿‘ä¼¼è¯¯å·®çš„å€’æ•°å¢é•¿ï¼‰ã€‚
- en: '**Convergence and zero-one laws of GNNs on random graphs**'
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**GNNåœ¨éšæœºå›¾ä¸Šçš„æ”¶æ•›æ€§å’Œé›¶ä¸€æ³•åˆ™**'
- en: GNNs can in principle be applied to graphs of any size following training. This
    makes an asymptotic analysis in the size of the input graphs very appealing. Previous
    studies of the asymptotic behaviour of GNNs have focused on convergence to theoretical
    limit networks ([Keriven et al., 2020](https://arxiv.org/abs/2006.01868)) and
    their stability under the perturbation of large graphs ([Levie et al., 2021](https://arxiv.org/abs/1907.12972)).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: GNNï¼ˆå›¾ç¥ç»ç½‘ç»œï¼‰ç†è®ºä¸Šå¯ä»¥åœ¨è®­ç»ƒååº”ç”¨äºä»»æ„å¤§å°çš„å›¾ã€‚è¿™ä½¿å¾—å¯¹è¾“å…¥å›¾çš„å¤§å°è¿›è¡Œæ¸è¿›åˆ†æå˜å¾—éå¸¸æœ‰å¸å¼•åŠ›ã€‚æ­¤å‰å¯¹GNNæ¸è¿›è¡Œä¸ºçš„ç ”ç©¶ï¼Œé›†ä¸­åœ¨å®ƒä»¬å¯¹ç†è®ºæé™ç½‘ç»œçš„æ”¶æ•›æ€§ï¼ˆ[Keriven
    ç­‰äºº, 2020](https://arxiv.org/abs/2006.01868)ï¼‰ä»¥åŠåœ¨å¤§å›¾æ‰°åŠ¨ä¸‹çš„ç¨³å®šæ€§ï¼ˆ[Levie ç­‰äºº, 2021](https://arxiv.org/abs/1907.12972)ï¼‰ä¸Šã€‚
- en: 'In a recent study, [Adam-Day et al. (2023](https://arxiv.org/abs/2301.13060))
    proved a *zero-one law* for binary GNN classifiers. The question being tackled
    is the following: How do binary GNN classifiers behave as we draw Erdos-RÃ©nyi
    graphs of increasing size with random node features? The main finding is that
    the probability that such graphs are mapped to a particular output by a class
    of GNN classifiers tends to either zero or to one. That is, the model eventually
    maps either *all* graphs to zero or *all* graphs to one. This result applies to
    GCNs as well as to GNNs with sum and mean aggregation.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ€è¿‘çš„ä¸€é¡¹ç ”ç©¶ä¸­ï¼Œ[Adam-Day ç­‰äºº (2023)](https://arxiv.org/abs/2301.13060) è¯æ˜äº†äºŒè¿›åˆ¶ GNN
    åˆ†ç±»å™¨çš„*é›¶ä¸€æ³•åˆ™*ã€‚è¦è§£å†³çš„é—®é¢˜æ˜¯ï¼šå½“æˆ‘ä»¬ç»˜åˆ¶å…·æœ‰éšæœºèŠ‚ç‚¹ç‰¹å¾çš„ Erdos-RÃ©nyi å›¾ï¼Œä¸”å›¾çš„å¤§å°é€æ¸å¢å¤§æ—¶ï¼ŒäºŒè¿›åˆ¶ GNN åˆ†ç±»å™¨çš„è¡¨ç°å¦‚ä½•ï¼Ÿä¸»è¦å‘ç°æ˜¯ï¼Œè¿™äº›å›¾é€šè¿‡ä¸€ç±»
    GNN åˆ†ç±»å™¨æ˜ å°„åˆ°ç‰¹å®šè¾“å‡ºçš„æ¦‚ç‡æœ€ç»ˆè¶‹å‘äºé›¶æˆ–ä¸€ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæ¨¡å‹æœ€ç»ˆå°†*æ‰€æœ‰*å›¾æ˜ å°„åˆ°é›¶æˆ–*æ‰€æœ‰*å›¾æ˜ å°„åˆ°ä¸€ã€‚è¿™ä¸ªç»“æœé€‚ç”¨äº GCN ä»¥åŠå…·æœ‰æ±‚å’Œå’Œå‡å€¼èšåˆçš„
    GNNã€‚
- en: 'The principal import of this result is that it establishes a novel *uniform*
    upper bound on the expressive power of GNNs: any property of graphs which can
    be uniformly expressed by these GNN architectures must obey a zero-one law. An
    example of a simple property which does not asymptotically tend to zero or one
    is that of having an even number of nodes.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€ç»“æœçš„ä¸»è¦æ„ä¹‰åœ¨äºå®ƒä¸º GNNs çš„è¡¨è¾¾èƒ½åŠ›å»ºç«‹äº†ä¸€ä¸ªæ–°çš„*ç»Ÿä¸€*ä¸Šç•Œï¼šä»»ä½•å¯ä»¥ç”±è¿™äº› GNN æ¶æ„ç»Ÿä¸€è¡¨è¾¾çš„å›¾çš„æ€§è´¨å¿…é¡»éµå®ˆé›¶ä¸€æ³•åˆ™ã€‚ä¸€ä¸ªç®€å•çš„æ€§è´¨ç¤ºä¾‹æ˜¯æ‹¥æœ‰å¶æ•°ä¸ªèŠ‚ç‚¹ï¼Œè¿™ä¸ªæ€§è´¨åœ¨æ¸è¿‘æ„ä¹‰ä¸Šæ—¢ä¸è¶‹å‘é›¶ä¹Ÿä¸è¶‹å‘ä¸€ã€‚
- en: '**The descriptive complexity of GNNs**'
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**GNNs çš„æè¿°æ€§å¤æ‚åº¦**'
- en: '[Grohe (2023)](https://arxiv.org/abs/2303.04613) recently analysed the descriptive
    complexity of GNNs in terms of Boolean circuit complexity. The specific circuit
    complexity class of interest is TC0\. This class contains all languages which
    are decided by Boolean circuits with constant depth and polynomial size, using
    only AND, OR, NOT, and threshold [](https://en.wikipedia.org/wiki/Majority_gate)
    (or, majority) gates. [Grohe (2023)](https://arxiv.org/abs/2303.04613) proves
    that the graph functions that can be computed by a class of polynomial-size bounded-depth
    family of GNNs lie in the circuit complexity class TC0\. Furthermore, if the class
    of GNNs are allowed to use random node initialization and global readout as in
    [Abboud el al. (2020)](https://arxiv.org/abs/2010.01179) then there is a matching
    lower bound in that they can compute exactly the same functions that can be expressed
    in TC0\. This establishes an upper bound on the power of GNNs with random node
    features, by requiring the class of models to be of bounded depth (fixed #layers)
    and of size polynomial. While this result is still non-uniform, it improves the
    result of [Abboud el al. (2020)](https://arxiv.org/abs/2010.01179) where the construction
    can be worst-case exponential.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[Grohe (2023)](https://arxiv.org/abs/2303.04613) æœ€è¿‘åˆ†æäº† GNNs åœ¨å¸ƒå°”ç”µè·¯å¤æ‚åº¦æ–¹é¢çš„æè¿°æ€§å¤æ‚åº¦ã€‚å…·ä½“çš„ç”µè·¯å¤æ‚åº¦ç±»åˆ«æ˜¯
    TC0ã€‚è¯¥ç±»åˆ«åŒ…å«æ‰€æœ‰é€šè¿‡å…·æœ‰æ’å®šæ·±åº¦å’Œå¤šé¡¹å¼å¤§å°çš„å¸ƒå°”ç”µè·¯å†³å®šçš„è¯­è¨€ï¼Œè¿™äº›ç”µè·¯ä»…ä½¿ç”¨ ANDã€ORã€NOT å’Œé˜ˆå€¼ [](https://en.wikipedia.org/wiki/Majority_gate)ï¼ˆæˆ–å¤šæ•°ï¼‰é—¨ã€‚[Grohe
    (2023)](https://arxiv.org/abs/2303.04613) è¯æ˜äº†å¯ä»¥ç”±ä¸€ç±»å¤šé¡¹å¼å¤§å°æœ‰ç•Œæ·±åº¦çš„ GNN å®¶æ—è®¡ç®—çš„å›¾å‡½æ•°å±äºç”µè·¯å¤æ‚åº¦ç±»åˆ«
    TC0ã€‚æ­¤å¤–ï¼Œå¦‚æœå…è®¸ GNN ç±»ä½¿ç”¨éšæœºèŠ‚ç‚¹åˆå§‹åŒ–å’Œå…¨å±€è¯»å–ï¼Œå¦‚ [Abboud el al. (2020)](https://arxiv.org/abs/2010.01179)
    ä¸­æ‰€è¿°ï¼Œåˆ™å­˜åœ¨ä¸€ä¸ªåŒ¹é…çš„ä¸‹ç•Œï¼Œå³å®ƒä»¬å¯ä»¥è®¡ç®—å‡ºæ°å¥½å¯ä»¥ç”¨ TC0 è¡¨è¾¾çš„ç›¸åŒå‡½æ•°ã€‚è¿™ä¸ºå…·æœ‰éšæœºèŠ‚ç‚¹ç‰¹å¾çš„ GNN çš„èƒ½åŠ›å»ºç«‹äº†ä¸Šç•Œï¼Œè¦æ±‚æ¨¡å‹ç±»åˆ«å…·æœ‰æœ‰ç•Œæ·±åº¦ï¼ˆå›ºå®šå±‚æ•°ï¼‰ä¸”å¤§å°ä¸ºå¤šé¡¹å¼ã€‚è™½ç„¶è¿™ä¸€ç»“æœä»ç„¶æ˜¯éç»Ÿä¸€çš„ï¼Œä½†å®ƒæ”¹å–„äº†
    [Abboud el al. (2020)](https://arxiv.org/abs/2010.01179) çš„ç»“æœï¼Œåœ¨è¯¥ç»“æœä¸­ï¼Œæ„é€ å¯èƒ½æ˜¯æœ€åæƒ…å†µä¸‹çš„æŒ‡æ•°çº§ã€‚'
- en: '**A fine-grained expressivity study of GNNs**'
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**GNNs çš„ç»†ç²’åº¦è¡¨è¾¾æ€§ç ”ç©¶**'
- en: Numerous recent works have analyzed the expressive power of MPNNs, primarily
    utilizing combinatorial techniques such as the 1-WL for the graph isomorphism
    problem. However, the graph isomorphism objective is inherently binary, not giving
    insights into the degree of similarity between two given graphs. [BÃ¶ker et al.
    (2023)](https://arxiv.org/abs/2306.03698) resolve this issue by deriving continuous
    extensions of both 1-WL and MPNNs to graphons. Concretely, they show that the
    continuous variant of 1-WL delivers an accurate topological characterization of
    the expressive power of MPNNs on graphons, revealing which graphs these networks
    can distinguish and the difficulty level in separating them. They provide a theoretical
    framework for graph and graphon similarity, combining various topological variants
    of classical characterizations of the 1-WL. In particular, they characterize the
    expressive power of MPNNs in terms of the tree distance, which is a graph distance
    based on the concept of fractional isomorphisms, and substructure counts via tree
    homomorphisms, showing that these concepts have the same expressive power as the
    1-WL and MPNNs on graphons. Interestingly, they also validated their theoretical
    findings by showing that randomly initialized MPNNs, without training, show competitive
    performance compared to their trained counterparts.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘çš„è®¸å¤šç ”ç©¶åˆ†æäº†MPNNsçš„è¡¨è¾¾åŠ›ï¼Œä¸»è¦åˆ©ç”¨ç»„åˆæŠ€æœ¯ï¼Œå¦‚ç”¨äºå›¾åŒæ„é—®é¢˜çš„1-WLã€‚ç„¶è€Œï¼Œå›¾åŒæ„ç›®æ ‡æœ¬è´¨ä¸Šæ˜¯äºŒå…ƒçš„ï¼Œæ— æ³•æ·±å…¥äº†è§£ä¸¤ä¸ªç»™å®šå›¾ä¹‹é—´çš„ç›¸ä¼¼åº¦ç¨‹åº¦ã€‚[BÃ¶ker
    et al. (2023)](https://arxiv.org/abs/2306.03698)é€šè¿‡æ¨å¯¼1-WLå’ŒMPNNsçš„è¿ç»­æ‰©å±•åˆ°å›¾è®ºå‡½æ•°ï¼Œè§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚å…·ä½“è€Œè¨€ï¼Œä»–ä»¬å±•ç¤ºäº†1-WLçš„è¿ç»­å˜ä½“èƒ½å¤Ÿå‡†ç¡®åœ°å¯¹MPNNsåœ¨å›¾è®ºå‡½æ•°ä¸Šçš„è¡¨è¾¾åŠ›è¿›è¡Œæ‹“æ‰‘ç‰¹å¾åŒ–ï¼Œæ­ç¤ºäº†è¿™äº›ç½‘ç»œèƒ½å¤ŸåŒºåˆ†å“ªäº›å›¾ï¼Œä»¥åŠåˆ†ç¦»å®ƒä»¬çš„éš¾åº¦çº§åˆ«ã€‚ä»–ä»¬æä¾›äº†ä¸€ä¸ªå›¾å’Œå›¾è®ºå‡½æ•°ç›¸ä¼¼åº¦çš„ç†è®ºæ¡†æ¶ï¼Œç»“åˆäº†1-WLç»å…¸ç‰¹å¾åŒ–çš„å„ç§æ‹“æ‰‘å˜ä½“ã€‚ç‰¹åˆ«æ˜¯ï¼Œä»–ä»¬é€šè¿‡æ ‘è·ç¦»æ¥è¡¨å¾MPNNsçš„è¡¨è¾¾åŠ›ï¼Œæ ‘è·ç¦»æ˜¯ä¸€ç§åŸºäºåˆ†æ•°åŒæ„æ¦‚å¿µçš„å›¾è·ç¦»ï¼Œä»¥åŠé€šè¿‡æ ‘åŒæ€çš„å­ç»“æ„è®¡æ•°ï¼Œè¡¨æ˜è¿™äº›æ¦‚å¿µä¸1-WLå’ŒMPNNsåœ¨å›¾è®ºå‡½æ•°ä¸Šçš„è¡¨è¾¾åŠ›ç›¸åŒã€‚æœ‰è¶£çš„æ˜¯ï¼Œä»–ä»¬è¿˜é€šè¿‡å±•ç¤ºéšæœºåˆå§‹åŒ–çš„MPNNsï¼ˆæœªç»è¿‡è®­ç»ƒï¼‰åœ¨ä¸ç»è¿‡è®­ç»ƒçš„å¯¹åº”æ¨¡å‹ç›¸æ¯”æ—¶å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œä»è€ŒéªŒè¯äº†ä»–ä»¬çš„ç†è®ºå‘ç°ã€‚
- en: '**Expressiveness results for Subgraph GNNs**'
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**å­å›¾GNNsçš„è¡¨è¾¾æ€§ç»“æœ**'
- en: Subgraph-based GNNs were already a big trend in 2022 ([Bevilacqua et al., 2022](https://arxiv.org/abs/2110.02910),
    [Qian et al., 2022](https://arxiv.org/abs/2206.11168)). This year, [Zhang et al.
    (2023b)](https://arxiv.org/abs/2302.07090) established more fine-grained expressivity
    results for such architectures. The paper investigates subgraph GNNs via the so-called
    Subgraph Weisfeiler-Leman Tests (SWL). Through this, they show a complete hierarchy
    of SWL with strictly growing expressivity. Concretely, they define equivalence
    classes for SWL-type algorithms and show that almost all existing subgraph GNNs
    fall in one of them. Moreover, the so-called SSWL achieves the maximal expressive
    power. Interestingly, they also relate SWL to several existing expressive GNNs
    architectures. For example, they show that SWL has the same expressivity as the
    local versions of 2-WL ([Morris et al., 2020](https://arxiv.org/abs/1904.01543)).
    In addition to theory, they also show that SWL-type architectures achieve good
    empirical results.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºå­å›¾çš„å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰åœ¨2022å¹´å·²ç»æˆä¸ºä¸€ä¸ªå¤§è¶‹åŠ¿ ([Bevilacqua et al., 2022](https://arxiv.org/abs/2110.02910),
    [Qian et al., 2022](https://arxiv.org/abs/2206.11168))ã€‚ä»Šå¹´ï¼Œ[Zhang et al. (2023b)](https://arxiv.org/abs/2302.07090)ä¸ºè¿™ç§æ¶æ„å»ºç«‹äº†æ›´ç²¾ç»†çš„è¡¨è¾¾åŠ›ç»“æœã€‚è®ºæ–‡é€šè¿‡æ‰€è°“çš„å­å›¾Weisfeiler-Lemanæµ‹è¯•ï¼ˆSWLï¼‰ç ”ç©¶äº†å­å›¾GNNsã€‚é€šè¿‡è¿™ä¸€æ–¹æ³•ï¼Œä»–ä»¬å±•ç¤ºäº†SWLçš„å®Œæ•´å±‚æ¬¡ç»“æ„ï¼Œå¹¶ä¸”è¡¨è¾¾åŠ›ä¸¥æ ¼å¢é•¿ã€‚å…·ä½“è€Œè¨€ï¼Œä»–ä»¬ä¸ºSWLç±»å‹çš„ç®—æ³•å®šä¹‰äº†ç­‰ä»·ç±»ï¼Œå¹¶è¡¨æ˜å‡ ä¹æ‰€æœ‰ç°æœ‰çš„å­å›¾GNNséƒ½å±äºå…¶ä¸­ä¹‹ä¸€ã€‚æ­¤å¤–ï¼Œæ‰€è°“çš„SSWLè¾¾åˆ°äº†æœ€å¤§çš„è¡¨è¾¾åŠ›ã€‚ä»¤äººæ„Ÿå…´è¶£çš„æ˜¯ï¼Œä»–ä»¬è¿˜å°†SWLä¸å‡ ä¸ªç°æœ‰çš„è¡¨è¾¾æ€§GNNæ¶æ„è”ç³»äº†èµ·æ¥ã€‚ä¾‹å¦‚ï¼Œä»–ä»¬å±•ç¤ºäº†SWLå…·æœ‰ä¸2-WLçš„å±€éƒ¨ç‰ˆæœ¬ç›¸åŒçš„è¡¨è¾¾åŠ›
    ([Morris et al., 2020](https://arxiv.org/abs/1904.01543))ã€‚é™¤äº†ç†è®ºä¹‹å¤–ï¼Œä»–ä»¬è¿˜è¡¨æ˜SWLç±»å‹çš„æ¶æ„åœ¨å®é™…åº”ç”¨ä¸­å–å¾—äº†è‰¯å¥½çš„å®è¯ç»“æœã€‚
- en: '**Expressive power of architectures for link prediction on KGs**'
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ç”¨äºçŸ¥è¯†å›¾è°±é“¾æ¥é¢„æµ‹çš„æ¶æ„è¡¨è¾¾åŠ›**'
- en: The expressive power of architectures such as RGCN and CompGCN for link prediction
    on knowledge graphs has been studied by [BarcelÃ³ et al. (2022)](https://arxiv.org/abs/2211.17113).
    This year, [Huang et al. (2023)](https://arxiv.org/abs/2302.02209) generalized
    these results to characterize the expressive power of various other model architectures.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºçŸ¥è¯†å›¾è°±é“¾æ¥é¢„æµ‹çš„æ¶æ„ï¼Œå¦‚RGCNå’ŒCompGCNçš„è¡¨è¾¾åŠ›å·²è¢«[BarcelÃ³ et al. (2022)](https://arxiv.org/abs/2211.17113)ç ”ç©¶ã€‚ä»Šå¹´ï¼Œ[Huang
    et al. (2023)](https://arxiv.org/abs/2302.02209)å°†è¿™äº›ç»“æœæ¨å¹¿åˆ°è¡¨å¾å„ç§å…¶ä»–æ¨¡å‹æ¶æ„çš„è¡¨è¾¾åŠ›ã€‚
- en: '![](../Images/aa233644e0a91a5ad4811e59bfe32f92.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa233644e0a91a5ad4811e59bfe32f92.png)'
- en: 'Figure from [Huang et al. (2023)](https://arxiv.org/abs/2302.02209): The figure
    compares the respective mode of operations in R-MPNNs and C-MPNNs.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥è‡ª [Huang et al. (2023)](https://arxiv.org/abs/2302.02209) çš„å›¾ï¼šè¯¥å›¾æ¯”è¾ƒäº† R-MPNNs
    å’Œ C-MPNNs åœ¨æ“ä½œæ¨¡å¼ä¸Šçš„ä¸åŒã€‚
- en: '[Huang et al. (2023)](https://arxiv.org/abs/2302.02209) introduced the framework
    of conditional message passing networks ([C-MPNNs](https://arxiv.org/abs/2302.02209))
    which includes architectures such as [NBFNets](https://arxiv.org/abs/2106.06935).
    Classical relational message passing networks (R-MPNNs) are unary encoders (i.e.,
    encoding graph nodes) and rely on a binary decoder for the task of link prediction
    ([Zhang, 2021](https://arxiv.org/abs/2010.16103)). On the other hand, C-MPNNs
    serve as binary encoders (i.e., encoding pairs of graph nodes) and as a result,
    are more suitable for the binary task of link prediction. C-MPNNs are shown to
    align with a relational Weisfeiler-Leman algorithm that can be seen as a local
    approximation of 2WL. These findings explain the superior performance of NBFNets
    and alike over, e.g., RGCNs. [Huang et al. (2023)](https://arxiv.org/abs/2302.02209)
    also present uniform expressiveness results in terms of precise logical characterizations
    for the class of binary functions captured by C-MPNNs.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[Huang et al. (2023)](https://arxiv.org/abs/2302.02209) æå‡ºäº†æ¡ä»¶æ¶ˆæ¯ä¼ é€’ç½‘ç»œï¼ˆ[C-MPNNs](https://arxiv.org/abs/2302.02209)ï¼‰çš„æ¡†æ¶ï¼Œå…¶ä¸­åŒ…æ‹¬å¦‚
    [NBFNets](https://arxiv.org/abs/2106.06935) è¿™æ ·çš„æ¶æ„ã€‚ç»å…¸çš„å…³ç³»æ¶ˆæ¯ä¼ é€’ç½‘ç»œï¼ˆR-MPNNsï¼‰æ˜¯å•ä¸€ç¼–ç å™¨ï¼ˆå³ï¼Œç¼–ç å›¾èŠ‚ç‚¹ï¼‰ï¼Œå¹¶ä¾èµ–äºäºŒå…ƒè§£ç å™¨è¿›è¡Œé“¾è·¯é¢„æµ‹ä»»åŠ¡ï¼ˆ[Zhang,
    2021](https://arxiv.org/abs/2010.16103)ï¼‰ã€‚å¦ä¸€æ–¹é¢ï¼ŒC-MPNNs ä½œä¸ºäºŒå…ƒç¼–ç å™¨ï¼ˆå³ï¼Œç¼–ç å›¾èŠ‚ç‚¹å¯¹ï¼‰ï¼Œå› æ­¤æ›´é€‚åˆäºé“¾è·¯é¢„æµ‹çš„äºŒå…ƒä»»åŠ¡ã€‚C-MPNNs
    è¢«è¯æ˜ä¸å…³ç³» Weisfeiler-Leman ç®—æ³•å¯¹é½ï¼Œå¯ä»¥çœ‹ä½œæ˜¯ 2WL çš„å±€éƒ¨è¿‘ä¼¼ã€‚è¿™äº›å‘ç°è§£é‡Šäº† NBFNets ç­‰åœ¨æ€§èƒ½ä¸Šä¼˜äºï¼Œä¾‹å¦‚ï¼ŒRGCNsã€‚[Huang
    et al. (2023)](https://arxiv.org/abs/2302.02209) è¿˜å±•ç¤ºäº† C-MPNNs æ•æ‰çš„äºŒå…ƒå‡½æ•°ç±»çš„ç²¾ç¡®é€»è¾‘ç‰¹å¾ï¼Œä»è€Œå¾—å‡ºäº†ç»Ÿä¸€çš„è¡¨è¾¾èƒ½åŠ›ç»“æœã€‚'
- en: '**Over-squashing and expressivity**'
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**è¿‡åº¦å‹ç¼©ä¸è¡¨è¾¾èƒ½åŠ›**'
- en: Over-squashing is a phenomenon originally described by [Alon & Yahav](https://arxiv.org/abs/2006.05205)
    in 2021 as the compression of exponentially-growing receptive fields into fixed-size
    vectors. Subsequent research ([Topping et al., 2022](https://arxiv.org/abs/2111.14522),
    [Di Giovanni et al., 2023](https://arxiv.org/abs/2302.02941), [Black et al., 2023](https://arxiv.org/abs/2302.06835),
    [Nguyen et al., 2023](https://arxiv.org/abs/2211.15779)) has characterised over-squashing
    through sensitivity analysis, proving that the dependence of the output features
    on hidden representations from earlier layers, is impaired by topological properties
    such as negative curvature or large commute time. Since the graph topology plays
    a crucial role in the formation of bottlenecks, *graph rewiring*, a paradigm shift
    elevating the graph connectivity to design factor in GNNs, has been proposed as
    a key strategy for alleviating over-squashing (if you are interested, see the
    Section on **Exotic Message Passing**below).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Over-squashing æ˜¯ [Alon & Yahav](https://arxiv.org/abs/2006.05205) åœ¨2021å¹´é¦–æ¬¡æè¿°çš„ä¸€ç§ç°è±¡ï¼ŒæŒ‡çš„æ˜¯æŒ‡æ•°å¢é•¿çš„æ„Ÿå—é‡è¢«å‹ç¼©ä¸ºå›ºå®šå¤§å°çš„å‘é‡ã€‚åç»­ç ”ç©¶ï¼ˆ[Topping
    et al., 2022](https://arxiv.org/abs/2111.14522), [Di Giovanni et al., 2023](https://arxiv.org/abs/2302.02941),
    [Black et al., 2023](https://arxiv.org/abs/2302.06835), [Nguyen et al., 2023](https://arxiv.org/abs/2211.15779)ï¼‰é€šè¿‡çµæ•åº¦åˆ†æå¯¹è¿‡åº¦å‹ç¼©è¿›è¡Œäº†è¡¨å¾ï¼Œè¯æ˜äº†è¾“å‡ºç‰¹å¾å¯¹æ—©æœŸå±‚çš„éšè—è¡¨ç¤ºçš„ä¾èµ–æ€§ä¼šå› æ‹“æ‰‘æ€§è´¨ï¼ˆå¦‚è´Ÿæ›²ç‡æˆ–é•¿æ—¶é—´é€šå‹¤ï¼‰è€Œå—åˆ°å½±å“ã€‚ç”±äºå›¾çš„æ‹“æ‰‘åœ¨ç“¶é¢ˆå½¢æˆä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œ*å›¾é‡è¿çº¿*ï¼ˆgraph
    rewiringï¼‰ä½œä¸ºä¸€ç§èŒƒå¼è½¬å˜ï¼Œæå‡äº†å›¾è¿æ¥æ€§åœ¨GNNè®¾è®¡ä¸­çš„ä½œç”¨ï¼Œè¢«æå‡ºä½œä¸ºç¼“è§£è¿‡åº¦å‹ç¼©çš„å…³é”®ç­–ç•¥ï¼ˆå¦‚æœä½ æ„Ÿå…´è¶£ï¼Œå‚è§ä¸‹é¢å…³äº**å¼‚æ„æ¶ˆæ¯ä¼ é€’**çš„éƒ¨åˆ†ï¼‰ã€‚
- en: '![](../Images/d769b18fc6f274c8abfbebf54baa83f2.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d769b18fc6f274c8abfbebf54baa83f2.png)'
- en: 'For the given graph, the MPNN learns stronger mixing (tight springs) for nodes
    (v, u) and (u, w) since their commute time is small, while nodes (u, q) and (u,
    z), with high commute-time, have weak mixing (loose springs). Source: [Di Giovanni
    et al., 2023](https://arxiv.org/abs/2306.03589)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºç»™å®šçš„å›¾ï¼ŒMPNN ä¸ºèŠ‚ç‚¹ï¼ˆvï¼Œuï¼‰å’Œï¼ˆuï¼Œwï¼‰å­¦ä¹ åˆ°æ›´å¼ºçš„æ··åˆï¼ˆç´§å¼¹ç°§ï¼‰ï¼Œå› ä¸ºå®ƒä»¬çš„é€šå‹¤æ—¶é—´è¾ƒçŸ­ï¼Œè€ŒèŠ‚ç‚¹ï¼ˆuï¼Œqï¼‰å’Œï¼ˆuï¼Œzï¼‰ç”±äºè¾ƒé•¿çš„é€šå‹¤æ—¶é—´ï¼Œå…·æœ‰è¾ƒå¼±çš„æ··åˆï¼ˆæ¾å¼›å¼¹ç°§ï¼‰ã€‚æ¥æºï¼š[Di
    Giovanni et al., 2023](https://arxiv.org/abs/2306.03589)
- en: 'Over-squashing is an obstruction to the expressive power, for it causes GNNs
    to falter in tasks with long-range interactions. To formally study this, [Di Giovanni
    et al., 2023](https://arxiv.org/abs/2306.03589) introduce a new metric of expressivity,
    referred to as â€œmixingâ€, which encodes the joint and nonlinear dependence of a
    graph function on pairs of nodesâ€™ features: for a GNN to approximate a function
    with large mixing, a necessary condition is allowing â€œstrongâ€ message exchange
    between the relevant nodes. Hence, they postulate to measure over-squashing through
    the mixing of a GNN prediction, and prove that the depth required by a GNN to
    induce enough mixing, *as required by the task*, grows with the commute time â€”
    typically much worse than the shortest-path distance. The results show how over-squashing
    hinders the expressivity of GNNs with â€œpracticalâ€ size, and validate that it arises
    from the misalignment between the task (requiring strong mixing between nodes
    i and j) and the topology (inducing large commute time between i and j).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: è¿‡åº¦æŒ¤å‹æ˜¯é™åˆ¶è¡¨è¾¾èƒ½åŠ›çš„éšœç¢ï¼Œå› ä¸ºå®ƒå¯¼è‡´GNNsåœ¨å¤„ç†å…·æœ‰é•¿ç¨‹äº¤äº’çš„ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†æ­£å¼ç ”ç©¶è¿™ä¸€é—®é¢˜ï¼Œ[Di Giovanniç­‰äººï¼Œ2023](https://arxiv.org/abs/2306.03589)æå‡ºäº†ä¸€ç§æ–°çš„è¡¨è¾¾èƒ½åŠ›åº¦é‡ï¼Œç§°ä¸ºâ€œæ··åˆâ€ï¼Œå®ƒç¼–ç äº†å›¾å‡½æ•°å¯¹èŠ‚ç‚¹ç‰¹å¾å¯¹çš„è”åˆå’Œéçº¿æ€§ä¾èµ–å…³ç³»ï¼šä¸ºäº†ä½¿GNNèƒ½å¤Ÿè¿‘ä¼¼å…·æœ‰è¾ƒå¤§æ··åˆçš„å‡½æ•°ï¼Œå¿…è¦çš„æ¡ä»¶æ˜¯å…è®¸ç›¸å…³èŠ‚ç‚¹ä¹‹é—´è¿›è¡Œâ€œå¼ºâ€çš„ä¿¡æ¯äº¤æ¢ã€‚å› æ­¤ï¼Œä»–ä»¬å‡è®¾é€šè¿‡GNNé¢„æµ‹çš„æ··åˆæ¥è¡¡é‡è¿‡åº¦æŒ¤å‹ï¼Œå¹¶è¯æ˜äº†GNNéœ€è¦çš„æ·±åº¦æ¥å¼•å…¥è¶³å¤Ÿçš„æ··åˆï¼Œ*å¦‚ä»»åŠ¡æ‰€éœ€*ï¼Œéšç€é€šå‹¤æ—¶é—´çš„å¢é•¿â€”â€”é€šå¸¸æ¯”æœ€çŸ­è·¯å¾„è·ç¦»è¦å·®å¾—å¤šã€‚ç»“æœæ˜¾ç¤ºï¼Œè¿‡åº¦æŒ¤å‹å¦‚ä½•å¦¨ç¢å…·æœ‰â€œå®é™…â€å¤§å°çš„GNNçš„è¡¨è¾¾èƒ½åŠ›ï¼Œå¹¶éªŒè¯äº†å®ƒæºäºä»»åŠ¡ï¼ˆè¦æ±‚èŠ‚ç‚¹iå’Œjä¹‹é—´çš„å¼ºæ··åˆï¼‰ä¸æ‹“æ‰‘ï¼ˆå¯¼è‡´iå’Œjä¹‹é—´çš„é•¿é€šå‹¤æ—¶é—´ï¼‰ä¹‹é—´çš„ä¸åŒ¹é…ã€‚
- en: The â€œmixingâ€ of a function pertains to the exchange of information between nodes,
    whatever this information is, and not to its capacity to separate node representations.
    In fact, these results [](https://arxiv.org/abs/2306.03589) also hold for GNNs
    more powerful than the 1-WL test. The analysis in [Di Giovanni et al., (2023)](https://arxiv.org/abs/2306.03589)
    offers an alternative approach for studying the expressivity of GNNs, which easily
    extends to equivariant GNNs in 3D space and their ability to model interactions
    between nodes.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å‡½æ•°çš„â€œæ··åˆâ€æ¶‰åŠèŠ‚ç‚¹ä¹‹é—´ä¿¡æ¯çš„äº¤æ¢ï¼Œæ— è®ºè¿™äº›ä¿¡æ¯æ˜¯ä»€ä¹ˆï¼Œè€Œä¸æ˜¯å…¶åˆ†ç¦»èŠ‚ç‚¹è¡¨ç¤ºçš„èƒ½åŠ›ã€‚äº‹å®ä¸Šï¼Œè¿™äº›ç»“æœ [](https://arxiv.org/abs/2306.03589)
    åŒæ ·é€‚ç”¨äºæ¯”1-WLæµ‹è¯•æ›´å¼ºå¤§çš„GNNsã€‚[Di Giovanniç­‰äººï¼ˆ2023ï¼‰](https://arxiv.org/abs/2306.03589)çš„åˆ†ææä¾›äº†ä¸€ç§æ›¿ä»£æ–¹æ³•æ¥ç ”ç©¶GNNsçš„è¡¨è¾¾èƒ½åŠ›ï¼Œè¯¥æ–¹æ³•å¯ä»¥è½»æ¾æ‰©å±•åˆ°3Dç©ºé—´ä¸­çš„ç­‰å˜GNNsä»¥åŠå®ƒä»¬å»ºæ¨¡èŠ‚ç‚¹ä¹‹é—´äº¤äº’çš„èƒ½åŠ›ã€‚
- en: '**Generalization and extrapolation capabilities of GNNs**'
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**GNNçš„æ³›åŒ–å’Œå¤–æ¨èƒ½åŠ›**'
- en: The expressive power of MPNNs has achieved a lot of attention in recent years
    through its connection to the WL test. While this connection has led to significant
    advances in understanding and enhancing MPNNsâ€™ expressive power ([Morris et al,
    2023a](https://arxiv.org/abs/2301.11039)), it does not provide insights into their
    generalization performance, i.e., their ability to make meaningful predictions
    beyond the training set. Surprisingly, only a few notable contributions study
    MPNNsâ€™ generalization behaviors, e.g., [Garg et al. (2020](https://arxiv.org/abs/2002.06157)),
    [Kriege et al. (2018)](https://www.ijcai.org/proceedings/2018/0325.pdf), [Liao
    et al. (2021)](https://arxiv.org/abs/2012.07690), [Maskey et al. (2022)](https://arxiv.org/abs/2202.00645),
    [Scarselli et al. (2018)](https://pubmed.ncbi.nlm.nih.gov/30219742/). However,
    these approaches express MPNNsâ€™ generalization ability using only classical graph
    parameters, e.g., maximum degree, number of vertices, or edges, which cannot fully
    capture the complex structure of real-world graphs. Further, most approaches study
    generalization in the non-uniform regime, i.e., assuming that the MPNNs operate
    on graphs of a pre-specified order.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: è¿‘å¹´æ¥ï¼ŒMPNNsï¼ˆæ¶ˆæ¯ä¼ é€’ç¥ç»ç½‘ç»œï¼‰çš„è¡¨è¾¾èƒ½åŠ›å› å…¶ä¸WLæµ‹è¯•çš„å…³è”è€Œå¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚å°½ç®¡è¿™ç§å…³è”ä¿ƒæˆäº†åœ¨ç†è§£å’Œæå‡MPNNsè¡¨è¾¾èƒ½åŠ›æ–¹é¢çš„æ˜¾è‘—è¿›å±•ï¼ˆ[Morrisç­‰ï¼Œ2023a](https://arxiv.org/abs/2301.11039)ï¼‰ï¼Œä½†å®ƒå¹¶æœªæä¾›å…³äºå…¶æ³›åŒ–æ€§èƒ½çš„è§è§£ï¼Œå³å®ƒä»¬åœ¨è®­ç»ƒé›†ä¹‹å¤–è¿›è¡Œæœ‰æ„ä¹‰é¢„æµ‹çš„èƒ½åŠ›ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œåªæœ‰å°‘æ•°å‡ é¡¹é‡è¦çš„ç ”ç©¶æ¢è®¨äº†MPNNsçš„æ³›åŒ–è¡Œä¸ºï¼Œä¾‹å¦‚ï¼Œ[Gargç­‰ï¼ˆ2020ï¼‰](https://arxiv.org/abs/2002.06157)ï¼Œ[Kriegeç­‰ï¼ˆ2018ï¼‰](https://www.ijcai.org/proceedings/2018/0325.pdf)ï¼Œ[Liaoç­‰ï¼ˆ2021ï¼‰](https://arxiv.org/abs/2012.07690)ï¼Œ[Maskeyç­‰ï¼ˆ2022ï¼‰](https://arxiv.org/abs/2202.00645)ï¼Œ[Scarselliç­‰ï¼ˆ2018ï¼‰](https://pubmed.ncbi.nlm.nih.gov/30219742/)ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä»…ä½¿ç”¨ç»å…¸çš„å›¾å‚æ•°æ¥è¡¨ç¤ºMPNNsçš„æ³›åŒ–èƒ½åŠ›ï¼Œä¾‹å¦‚æœ€å¤§åº¦æ•°ã€é¡¶ç‚¹æ•°æˆ–è¾¹æ•°ï¼Œè¿™äº›æ–¹æ³•æ— æ³•å®Œå…¨æ•æ‰ç°å®ä¸–ç•Œå›¾çš„å¤æ‚ç»“æ„ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°æ–¹æ³•åœ¨éå‡åŒ€åŒºåŸŸç ”ç©¶æ³›åŒ–ï¼Œå³å‡è®¾MPNNsåœ¨é¢„å®šé˜¶æ•°çš„å›¾ä¸Šè¿›è¡Œæ“ä½œã€‚
- en: '![](../Images/b5b9c2a9751ff78b44277f541166bbf8.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5b9c2a9751ff78b44277f541166bbf8.png)'
- en: 'Figure from [Morris et al. (2023b)](https://arxiv.org/abs/2301.11039): Overview
    of the generalization capabilities of MPNNs and their link to the 1-WL.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥è‡ª[Morrisç­‰äºº(2023b)](https://arxiv.org/abs/2301.11039)çš„å›¾ï¼šMPNNsçš„æ³›åŒ–èƒ½åŠ›æ¦‚è§ˆåŠå…¶ä¸1-WLçš„è”ç³»ã€‚
- en: Hence, [Morris et al. (2023b)](https://arxiv.org/abs/2301.11039) showed a tight
    connection between the expressive power of the 1-WL and generalization performance.
    They investigate the influence of graph structure and the parametersâ€™ encoding
    lengths on MPNNsâ€™ generalization by tightly connecting 1-WLâ€™s expressivity and
    MPNNsâ€™ Vapnikâ€“Chervonenkis (VC) dimension. To that, they show several results.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œ[è«é‡Œæ–¯ç­‰äºº (2023b)](https://arxiv.org/abs/2301.11039)å±•ç¤ºäº†1-WLçš„è¡¨è¾¾èƒ½åŠ›ä¸æ³›åŒ–æ€§èƒ½ä¹‹é—´çš„ç´§å¯†è”ç³»ã€‚ä»–ä»¬é€šè¿‡ç´§å¯†è¿æ¥1-WLçš„è¡¨è¾¾èƒ½åŠ›å’ŒMPNNsçš„Vapnikâ€“Chervonenkis
    (VC)ç»´åº¦ï¼Œç ”ç©¶äº†å›¾ç»“æ„å’Œå‚æ•°ç¼–ç é•¿åº¦å¯¹MPNNsæ³›åŒ–èƒ½åŠ›çš„å½±å“ã€‚ä¸ºæ­¤ï¼Œä»–ä»¬å±•ç¤ºäº†å‡ ä¸ªç»“æœã€‚
- en: 1ï¸âƒ£ First, in the non-uniform regime, they show that MPNNsâ€™ VC dimension depends
    tightly on the number of equivalence classes computed by the 1-WL over a set of
    graphs. In addition, their results easily extend to the k-WL and many recent expressive
    MPNN extensions.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ é¦–å…ˆï¼Œåœ¨éå‡åŒ€çŠ¶æ€ä¸‹ï¼Œä»–ä»¬å±•ç¤ºäº†MPNNsçš„VCç»´åº¦ä¸1-WLåœ¨ä¸€ç»„å›¾ä¸Šè®¡ç®—å‡ºçš„ç­‰ä»·ç±»æ•°é‡ç´§å¯†ç›¸å…³ã€‚æ­¤å¤–ï¼Œä»–ä»¬çš„ç»“æœå¯ä»¥è½»æ¾æ¨å¹¿åˆ°k-WLå’Œè®¸å¤šæœ€è¿‘çš„è¡¨è¾¾æ€§MPNNæ‰©å±•ã€‚
- en: 2ï¸âƒ£ In the uniform regime, i.e., when graphs can have arbitrary order, they
    show that MPNNsâ€™ VC dimension is lower and upper bounded by the largest bitlength
    of its weights. In both the uniform and non-uniform regimes, MPNNsâ€™ VC dimension
    depends logarithmically on the number of colors computed by the 1-WL and polynomially
    on the number of parameters. Moreover, they also empirically show that their theoretical
    findings hold in practice to some extent.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ åœ¨å‡åŒ€çŠ¶æ€ä¸‹ï¼Œå³å½“å›¾å¯ä»¥å…·æœ‰ä»»æ„é¡ºåºæ—¶ï¼Œä»–ä»¬å±•ç¤ºäº†MPNNsçš„VCç»´åº¦å—å…¶æƒé‡çš„æœ€å¤§æ¯”ç‰¹é•¿åº¦çš„ä¸Šä¸‹ç•Œé™åˆ¶ã€‚åœ¨å‡åŒ€å’Œéå‡åŒ€çŠ¶æ€ä¸‹ï¼ŒMPNNsçš„VCç»´åº¦å¯¹1-WLè®¡ç®—çš„é¢œè‰²æ•°å‘ˆå¯¹æ•°å…³ç³»ï¼Œå¹¶ä¸”å¯¹å‚æ•°çš„æ•°é‡å‘ˆå¤šé¡¹å¼å…³ç³»ã€‚æ­¤å¤–ï¼Œä»–ä»¬è¿˜é€šè¿‡å®è¯ç ”ç©¶è¡¨æ˜ï¼Œä»–ä»¬çš„ç†è®ºå‘ç°ä¸€å®šç¨‹åº¦ä¸Šåœ¨å®è·µä¸­æˆç«‹ã€‚
- en: ğŸ”® Predictions time!
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ğŸ”® é¢„æµ‹æ—¶é—´ï¼
- en: '***Christopher Morris (RWTH Aachen)***'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '***å…‹é‡Œæ–¯æ‰˜å¼—Â·è«é‡Œæ–¯ (äºšç›å·¥ä¸šå¤§å­¦)***'
- en: â€œI believe that there is a pressing need for a better and more practical theory
    of generalization of GNNs. â€ â€” **Christopher Morris** (RWTH Aachen)
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œæˆ‘ç›¸ä¿¡ï¼ŒGNNsæ³›åŒ–ç†è®ºäºŸéœ€æ›´å¥½ä¸”æ›´å®ç”¨çš„ç†è®ºã€‚â€ â€” **å…‹é‡Œæ–¯æ‰˜å¼—Â·è«é‡Œæ–¯** (äºšç›å·¥ä¸šå¤§å­¦)
- en: â¡ï¸ For example, we need to understand how graph structure and various architectural
    parameters influence generalization. Moreover, the dynamics of SGD for training
    GNNs are currently understudied and not well understood, and more works will study
    this.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ ä¾‹å¦‚ï¼Œæˆ‘ä»¬éœ€è¦ç†è§£å›¾ç»“æ„å’Œå„ç§æ¶æ„å‚æ•°å¦‚ä½•å½±å“æ³›åŒ–ã€‚æ­¤å¤–ï¼Œå½“å‰å¯¹äºè®­ç»ƒGNNsçš„SGDåŠ¨æ€ç ”ç©¶è¾ƒå°‘ï¼Œç†è§£ä¹Ÿä¸å¤Ÿé€å½»ï¼Œæ›´å¤šçš„ç ”ç©¶å°†ä¼šæ¢è®¨è¿™ä¸€é—®é¢˜ã€‚
- en: '***Ä°smail Ä°lkan Ceylan (Oxford)***'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '***ä¼Šæ–¯æ¢…å°”Â·ä¼Šå°”åÂ·æ°å…° (ç‰›æ´¥å¤§å­¦)***'
- en: â€œI hope to see more expressivity results in the uniform setting, where we fix
    the parameters of a neural network and examine its capabilities.â€ â€” **Ä°smail Ä°lkan
    Ceylan** (Oxford)
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œæˆ‘å¸Œæœ›åœ¨å‡åŒ€è®¾ç½®ä¸‹çœ‹åˆ°æ›´å¤šçš„è¡¨è¾¾èƒ½åŠ›ç ”ç©¶ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹æˆ‘ä»¬å›ºå®šç¥ç»ç½‘ç»œçš„å‚æ•°å¹¶æ£€æŸ¥å…¶èƒ½åŠ›ã€‚â€ â€” **ä¼Šæ–¯æ¢…å°”Â·ä¼Šå°”åÂ·æ°å…°** (ç‰›æ´¥å¤§å­¦)
- en: â¡ï¸ In this case, we can identify a better connection to generalization, because
    if a property cannot be expressed uniformly then the model cannot generalise to
    larger graph sizes.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥è¯†åˆ«å‡ºæ›´å¥½çš„æ³›åŒ–è”ç³»ï¼Œå› ä¸ºå¦‚æœæŸä¸ªå±æ€§æ— æ³•å‡åŒ€è¡¨è¾¾ï¼Œé‚£ä¹ˆæ¨¡å‹å°±æ— æ³•å¯¹æ›´å¤§çš„å›¾å¤§å°è¿›è¡Œæ³›åŒ–ã€‚
- en: â¡ï¸ This year, we may also see expressiveness studies that target graph regression
    or graph generation, which remain under-explored. There are good reasons to hope
    for learning algorithms which are isomorphism-complete on larger graph classes,
    strictly generalizing the results for planar graphs.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ ä»Šå¹´ï¼Œæˆ‘ä»¬ä¹Ÿå¯èƒ½ä¼šçœ‹åˆ°é’ˆå¯¹å›¾å›å½’æˆ–å›¾ç”Ÿæˆçš„è¡¨è¾¾èƒ½åŠ›ç ”ç©¶ï¼Œè¿™äº›é¢†åŸŸä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚äººä»¬æœ‰å……åˆ†çš„ç†ç”±æœŸæœ›å‡ºç°åœ¨æ›´å¤§çš„å›¾ç±»åˆ«ä¸Šå…·æœ‰åŒæ„å®Œå¤‡æ€§çš„å­¦ä¹ ç®—æ³•ï¼Œä»è€Œä¸¥æ ¼åœ°æ¨å¹¿å¹³é¢å›¾çš„ç»“æœã€‚
- en: â¡ï¸ It is also time to develop a theory for learning with fully relational data
    (i.e., knowledge hypergraphs), which will unlock applications in relational databases!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ ç°åœ¨ä¹Ÿæ˜¯æ—¶å€™ä¸ºå­¦ä¹ å®Œå…¨å…³è”æ•°æ®ï¼ˆå³çŸ¥è¯†è¶…å›¾ï¼‰å‘å±•ä¸€å¥—ç†è®ºäº†ï¼Œè¿™å°†è§£é”å…³ç³»æ•°æ®åº“ä¸­çš„åº”ç”¨ï¼
- en: '***Francesco Di Giovanni (Oxford)***'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '***å¼—æœ—åˆ‡æ–¯ç§‘Â·è¿ªÂ·ä¹”ç“¦å°¼ (ç‰›æ´¥å¤§å­¦)***'
- en: In terms of future theoretical developments of GNNs, I can see two directions
    that deserve attention.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨GNNsçš„æœªæ¥ç†è®ºå‘å±•æ–¹é¢ï¼Œæˆ‘çœ‹åˆ°ä¸¤ä¸ªå€¼å¾—å…³æ³¨çš„æ–¹å‘ã€‚
- en: â€œThere is very little understanding of the dynamics of the weights of a GNN
    under gradient flow (or SGD); assessing the impact of the graph topology on the
    evolution of the weights is key to addressing questions about generalisation and
    hardness of a task.â€ â€” Francesco Di Giovanni (Oxford)
  id: totrans-96
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œæˆ‘ä»¬å¯¹GNNåœ¨æ¢¯åº¦æµï¼ˆæˆ–SGDï¼‰ä¸‹çš„æƒé‡åŠ¨æ€å‡ ä¹æ²¡æœ‰ç†è§£ï¼›è¯„ä¼°å›¾æ‹“æ‰‘å¯¹æƒé‡æ¼”å˜çš„å½±å“æ˜¯è§£å†³æ³›åŒ–å’Œä»»åŠ¡éš¾åº¦é—®é¢˜çš„å…³é”®ã€‚â€ â€” å¼—æœ—åˆ‡æ–¯ç§‘Â·è¿ªÂ·ä¹”ç“¦å°¼ï¼ˆç‰›æ´¥å¤§å­¦ï¼‰
- en: â¡ï¸ Second, I believe it would be valuable to develop alternative paradigms of
    expressivity, which more directly focus on approximation power (of graph functions
    and their derivatives) and identify precisely the tasks which are hard to learn.
    The latter direction could also be particularly meaningful for characterising
    the power of equivariant GNNs in 3D space, where measurements of expressivity
    might need to be decoupled from the 2D case in order to be better aligned with
    tasks coming from the scientific domain.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ å…¶æ¬¡ï¼Œæˆ‘è®¤ä¸ºå‘å±•è¡¨ç°åŠ›çš„æ›¿ä»£èŒƒå¼ä¼šå¾ˆæœ‰ä»·å€¼ï¼Œè¿™äº›èŒƒå¼æ›´ç›´æ¥åœ°å…³æ³¨è¿‘ä¼¼èƒ½åŠ›ï¼ˆå›¾å‡½æ•°åŠå…¶å¯¼æ•°çš„è¿‘ä¼¼èƒ½åŠ›ï¼‰ï¼Œå¹¶ç²¾ç¡®åœ°è¯†åˆ«å‡ºå“ªäº›ä»»åŠ¡éš¾ä»¥å­¦ä¹ ã€‚åä¸€æ–¹å‘åœ¨è¡¨å¾ä¸‰ç»´ç©ºé—´ä¸­ç­‰å˜GNNçš„èƒ½åŠ›æ—¶å¯èƒ½ç‰¹åˆ«æœ‰æ„ä¹‰ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¡¨ç°åŠ›çš„åº¦é‡å¯èƒ½éœ€è¦ä¸äºŒç»´æƒ…å†µè§£è€¦ï¼Œä»¥ä¾¿æ›´å¥½åœ°ä¸æ¥è‡ªç§‘å­¦é¢†åŸŸçš„ä»»åŠ¡å¯¹æ¥ã€‚
- en: 'At the end: a fun fact about where WL went in 2023'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“å°¾ï¼šå…³äºWLåœ¨2023å¹´å»å‘çš„ä¸€ä¸ªæœ‰è¶£äº‹å®
- en: '![](../Images/c96ff616c959c6a3f3e2be22a280b3af.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c96ff616c959c6a3f3e2be22a280b3af.png)'
- en: 'Portraits: Ihor Gorsky'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: äººç‰©è‚–åƒï¼šä¼Šæˆˆå°”Â·æˆˆå°”æ–¯åŸº
- en: '**Predictions from the 2023 post**'
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2023å¹´é¢„æµ‹**'
- en: (1) More efforts on creating time- and memory-efficient subgraph GNNs.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: (1) åœ¨åˆ›å»ºæ—¶é—´å’Œå†…å­˜é«˜æ•ˆçš„å­å›¾GNNæ–¹é¢æŠ•å…¥æ›´å¤šåŠªåŠ›
- en: âŒ not really
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: âŒ å¹¶ä¸å®Œå…¨æ˜¯
- en: (2) Better understanding of generalization of GNNs
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: (2) æ›´å¥½åœ°ç†è§£GNNçš„æ³›åŒ–èƒ½åŠ›
- en: âœ… yes, see the subsections on oversquashing and generalization
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: âœ… æ˜¯çš„ï¼Œè§å…³äºè¿‡åº¦å‹ç¼©å’Œæ³›åŒ–çš„å­ç« èŠ‚
- en: (3) Weisfeiler and Leman visit 10 new places!
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: (3) è´¹æ–¯è´¹å°”å’Œåˆ©æ›¼è®¿é—®äº†10ä¸ªæ–°åœ°æ–¹ï¼
- en: âŒ (4 so far) [Grammatical](https://openreview.net/forum?id=eZneJ55mRO), [indifferent](https://arxiv.org/abs/2311.01205),
    [measurement modeling](https://arxiv.org/abs/2307.05775), [paths](https://arxiv.org/abs/2308.06838)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: âŒï¼ˆåˆ°ç›®å‰ä¸ºæ­¢ï¼Œ4ç¯‡ï¼‰[è¯­æ³•é—®é¢˜](https://openreview.net/forum?id=eZneJ55mRO)ï¼Œ[æ— æ‰€è°“](https://arxiv.org/abs/2311.01205)ï¼Œ[æµ‹é‡å»ºæ¨¡](https://arxiv.org/abs/2307.05775)ï¼Œ[è·¯å¾„](https://arxiv.org/abs/2308.06838)
- en: New and exotic message passing
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ–°é¢–ä¸”ç‹¬ç‰¹çš„æ¶ˆæ¯ä¼ é€’
- en: '*Ben Finkelshtein (Oxford), Francesco Di Giovanni (Oxford), Petar VeliÄkoviÄ‡
    (Google DeepMind)*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*æœ¬Â·èŠ¬å…‹å°”æ–½å¦ï¼ˆç‰›æ´¥å¤§å­¦ï¼‰ï¼Œå¼—æœ—åˆ‡æ–¯ç§‘Â·è¿ªÂ·ä¹”ç“¦å°¼ï¼ˆç‰›æ´¥å¤§å­¦ï¼‰ï¼Œä½©å¡”å°”Â·ç»´åˆ©å¥‡ç§‘ç»´å¥‡ï¼ˆè°·æ­Œ DeepMindï¼‰*'
- en: '***Petar VeliÄkoviÄ‡ (Google DeepMind)***'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '***ä½©å¡”å°”Â·ç»´åˆ©å¥‡ç§‘ç»´å¥‡ï¼ˆè°·æ­Œ DeepMindï¼‰***'
- en: 'Over the years, it has become part of common folklore that the development
    of message passing operators has saturated. What I find particularly exciting
    about the progress made in 2023 is that, from several independent research groups
    (including our own), a unified novel direction has emerged: letâ€™s start considering
    the impact of ***time*** in the GNN â³.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šå¹´æ¥ï¼Œå…³äºæ¶ˆæ¯ä¼ é€’ç®—å­çš„å¼€å‘å·²ç»é¥±å’Œï¼Œè¿™å·²ç»æˆä¸ºä¸€ç§å¸¸è§çš„æ°‘é—´ä¼ è¯´ã€‚æˆ‘è®¤ä¸º2023å¹´å–å¾—çš„è¿›å±•ç‰¹åˆ«ä»¤äººå…´å¥‹ï¼Œå› ä¸ºæ¥è‡ªå‡ ä¸ªç‹¬ç«‹ç ”ç©¶å°ç»„ï¼ˆåŒ…æ‹¬æˆ‘ä»¬è‡ªå·±ï¼‰çš„æˆæœè¡¨æ˜ï¼Œä¸€ä¸ªç»Ÿä¸€çš„å…¨æ–°æ–¹å‘å·²ç»å‡ºç°ï¼šè®©æˆ‘ä»¬å¼€å§‹è€ƒè™‘***æ—¶é—´***åœ¨GNNä¸­çš„å½±å“
    â³ã€‚
- en: â€œI forecast that, in 2024, time will assume a central role in the development
    of novel GNN architectures.â€ â€” Petar VeliÄkoviÄ‡ (Google DeepMind)
  id: totrans-112
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œæˆ‘é¢„æµ‹ï¼Œåœ¨2024å¹´ï¼Œæ—¶é—´å°†åœ¨æ–°å‹GNNæ¶æ„çš„å‘å±•ä¸­æ‰®æ¼”æ ¸å¿ƒè§’è‰²ã€‚â€ â€” ä½©å¡”å°”Â·ç»´åˆ©å¥‡ç§‘ç»´å¥‡ï¼ˆè°·æ­Œ DeepMindï¼‰
- en: ğŸ’¡ Time has already been leveraged in GNN design when it is explicitly provided
    in the input (in spatiotemporal or fully dynamic graphs). This year, it has started
    to feature in research of GNN operators on *static* graph inputs. Several works
    are dropping the assumption of a unified, synchronised clock â±ï¸ which forces all
    messages in a layer to be sent and received at once.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ åœ¨GNNè®¾è®¡ä¸­ï¼Œæ—¶é—´å·²ç»åœ¨æ˜¾å¼æä¾›çš„è¾“å…¥ä¸­å¾—åˆ°äº†åˆ©ç”¨ï¼ˆå¦‚æ—¶ç©ºå›¾æˆ–å®Œå…¨åŠ¨æ€å›¾ï¼‰ã€‚ä»Šå¹´ï¼Œå®ƒå¼€å§‹å‡ºç°åœ¨å…³äº*é™æ€*å›¾è¾“å…¥çš„GNNæ“ä½œç¬¦ç ”ç©¶ä¸­ã€‚ä¸€äº›ç ”ç©¶å¼€å§‹ä¸å†å‡è®¾ä¸€ä¸ªç»Ÿä¸€çš„ã€åŒæ­¥çš„æ—¶é’Ÿ
    â±ï¸ï¼Œè¯¥æ—¶é’Ÿè¿«ä½¿æ‰€æœ‰ä¿¡æ¯åœ¨åŒä¸€å±‚ä¸­åŒæ—¶å‘é€å’Œæ¥æ”¶ã€‚
- en: 1ï¸âƒ£ The first such work, [GwAC](https://openreview.net/forum?id=zffXH0sEJP)
    ğŸ¥‘, only played with rudimentary randomised message scheduling, but provided **proofs**
    for why such processing might yield significant improvements in expressive power.
    [Co-GNNs](https://arxiv.org/abs/2310.01267) ğŸ¤ carry the torch further, demonstrating
    a more elaborate and fine-tuned message scheduling mechanism which is node-centric,
    allowing each node to choose when to send ğŸ“¨ or receive ğŸ“¬ messages. Co-GNNs also
    provide a practical method for training such schedulers by gradient descent. While
    the development of such asynchronous GNN models is highly desirable, we must also
    acknowledge the associated scalability issues â€” our present frontier hardware
    is not designed to efficiently scale such sequential systems.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ ç¬¬ä¸€ç¯‡æ­¤ç±»å·¥ä½œï¼Œ[GwAC](https://openreview.net/forum?id=zffXH0sEJP) ğŸ¥‘ï¼Œä»…ä»…ç©å¼„äº†åŸºç¡€çš„éšæœºåŒ–æ¶ˆæ¯è°ƒåº¦ï¼Œä½†æä¾›äº†**è¯æ˜**ï¼Œè¯´æ˜ä¸ºä»€ä¹ˆè¿™ç§å¤„ç†æ–¹å¼å¯èƒ½åœ¨è¡¨è¾¾èƒ½åŠ›ä¸Šå¸¦æ¥æ˜¾è‘—æå‡ã€‚[Co-GNNs](https://arxiv.org/abs/2310.01267)
    ğŸ¤ è¿›ä¸€æ­¥æ¨åŠ¨äº†è¿™ä¸€è¿›ç¨‹ï¼Œå±•ç¤ºäº†ä¸€ç§æ›´åŠ ç²¾ç»†è°ƒæ•´çš„æ¶ˆæ¯è°ƒåº¦æœºåˆ¶ï¼Œå®ƒä»¥èŠ‚ç‚¹ä¸ºä¸­å¿ƒï¼Œå…è®¸æ¯ä¸ªèŠ‚ç‚¹é€‰æ‹©ä½•æ—¶å‘é€ ğŸ“¨ æˆ–æ¥æ”¶ ğŸ“¬ æ¶ˆæ¯ã€‚Co-GNNsè¿˜æä¾›äº†ä¸€ç§é€šè¿‡æ¢¯åº¦ä¸‹é™è®­ç»ƒè¿™ç§è°ƒåº¦å™¨çš„å®ç”¨æ–¹æ³•ã€‚è™½ç„¶å¼€å‘è¿™ç§å¼‚æ­¥GNNæ¨¡å‹æ˜¯éå¸¸æœŸæœ›çš„ï¼Œä½†æˆ‘ä»¬ä¹Ÿå¿…é¡»æ‰¿è®¤ä¸ä¹‹ç›¸å…³çš„å¯æ‰©å±•æ€§é—®é¢˜â€”â€”æˆ‘ä»¬å½“å‰çš„å‰æ²¿ç¡¬ä»¶å¹¶ä¸è®¾è®¡ä¸ºèƒ½å¤Ÿé«˜æ•ˆæ‰©å±•æ­¤ç±»é¡ºåºç³»ç»Ÿã€‚
- en: 2ï¸âƒ£ In our own work on [asynchronous algorithmic alignment](https://openreview.net/forum?id=ba4bbZ4KoF),
    we instead opt to design a *synchronous* GNN, but **constrain** its message, aggregation,
    and update functions such that the GNN would yield identical embeddings even if
    parts of its dataflow were made asynchronous. This led us to an exciting journey
    through monoids, 1-cocycles, and category theory, resulting in a scalable GNN
    model that achieves superior performance on many CLRS-30 tasks.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ åœ¨æˆ‘ä»¬å…³äº[å¼‚æ­¥ç®—æ³•å¯¹é½](https://openreview.net/forum?id=ba4bbZ4KoF)çš„ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é€‰æ‹©è®¾è®¡ä¸€ä¸ª*åŒæ­¥*
    GNNï¼Œä½†**é™åˆ¶**å…¶æ¶ˆæ¯ä¼ é€’ã€èšåˆå’Œæ›´æ–°å‡½æ•°ï¼Œä½¿å¾—å³ä½¿å…¶æ•°æ®æµçš„éƒ¨åˆ†å˜ä¸ºå¼‚æ­¥ï¼ŒGNNä¹Ÿèƒ½äº§ç”Ÿç›¸åŒçš„åµŒå…¥ã€‚è¿™å¸¦é¢†æˆ‘ä»¬è¿›å…¥äº†ä¸€ä¸ªæ¿€åŠ¨äººå¿ƒçš„æ—…ç¨‹ï¼Œæ¶‰åŠåˆ°å•ç¾¤ã€1-ä½™åˆ‡å’ŒèŒƒç•´ç†è®ºï¼Œæœ€ç»ˆå¾—å‡ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„GNNæ¨¡å‹ï¼Œåœ¨è®¸å¤šCLRS-30ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚
- en: '![](../Images/95b2b7c260ab4af9b85e305ef66c3ca4.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/95b2b7c260ab4af9b85e305ef66c3ca4.png)'
- en: 'A possible execution trace of an asynchronous GNN. While traditional GNNs send
    and receive all messages synchronously, under our framework, at any step the GNN
    may choose to execute any number of possible operations (depicted here with a
    collection on the right side of the graph). Source: [Dudzik et al.](https://openreview.net/forum?id=ba4bbZ4KoF)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå¼‚æ­¥GNNçš„å¯èƒ½æ‰§è¡Œè½¨è¿¹ã€‚ä¼ ç»Ÿçš„GNNåŒæ­¥å‘é€å’Œæ¥æ”¶æ‰€æœ‰æ¶ˆæ¯ï¼Œè€Œåœ¨æˆ‘ä»¬çš„æ¡†æ¶ä¸‹ï¼Œåœ¨ä»»ä½•æ­¥éª¤ï¼ŒGNNå¯ä»¥é€‰æ‹©æ‰§è¡Œä»»æ„æ•°é‡çš„å¯èƒ½æ“ä½œï¼ˆè¿™é‡Œé€šè¿‡å›¾çš„å³ä¾§é›†åˆæ¥è¡¨ç¤ºï¼‰ã€‚æ¥æºï¼š[Dudzik
    et al.](https://openreview.net/forum?id=ba4bbZ4KoF)
- en: â¡ï¸ Lastly, it is worth noting that for certain special choices of message scheduling,
    we do not need to make modifications to synchronous GNNsâ€™ architecture â€” and may
    instead resort to dynamic graph rewiring. [DREW](https://arxiv.org/abs/2305.08018)
    and [Half-Hop](https://openreview.net/forum?id=lXczFIwQkv) are two concurrently
    published papers at ICMLâ€™23 which embody the principle of using graph rewiring
    to *slow down* message passing ğŸŒ. In DREW, a message from each node is actually
    sent to every other node, but it takes *k* layers before a message will reach
    a neighbour that is *k* hops away! Half-Hop, on the other hand, takes a more lenient
    approach, and just randomly decides whether or not to introduce a â€œslow nodeâ€
    which extends the path between any two nodes connected by an edge. Both approaches
    naturally alleviate the oversmoothing problem, as messages travelling longer distances
    will oversmooth less.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ æœ€åï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¯¹äºæŸäº›ç‰¹å®šçš„æ¶ˆæ¯è°ƒåº¦é€‰æ‹©ï¼Œæˆ‘ä»¬ä¸éœ€è¦ä¿®æ”¹åŒæ­¥GNNçš„æ¶æ„â€”â€”è€Œå¯ä»¥è½¬è€Œä½¿ç”¨åŠ¨æ€å›¾é‡è¿æ¥ã€‚[DREW](https://arxiv.org/abs/2305.08018)
    å’Œ[Half-Hop](https://openreview.net/forum?id=lXczFIwQkv)æ˜¯ä¸¤ç¯‡åœ¨ICML'23ä¸ŠåŒæ—¶å‘è¡¨çš„è®ºæ–‡ï¼Œå®ƒä»¬ä½“ç°äº†ä½¿ç”¨å›¾é‡è¿æ¥æ¥*å‡ç¼“*æ¶ˆæ¯ä¼ é€’ğŸŒçš„åŸç†ã€‚åœ¨DREWä¸­ï¼Œæ¯ä¸ªèŠ‚ç‚¹çš„æ¶ˆæ¯å®é™…ä¸Šä¼šå‘é€åˆ°æ¯ä¸€ä¸ªå…¶ä»–èŠ‚ç‚¹ï¼Œä½†éœ€è¦*k*å±‚æ‰èƒ½åˆ°è¾¾ä¸€ä¸ªä¸å…¶ç›¸éš”*k*è·³çš„é‚»å±…ï¼è€ŒHalf-Hopåˆ™é‡‡å–æ›´å®½æ¾çš„æ–¹å¼ï¼Œéšæœºå†³å®šæ˜¯å¦å¼•å…¥ä¸€ä¸ªâ€œæ…¢èŠ‚ç‚¹â€ï¼Œä»è€Œå»¶é•¿ä»»ä½•é€šè¿‡è¾¹è¿æ¥çš„ä¸¤ä¸ªèŠ‚ç‚¹ä¹‹é—´çš„è·¯å¾„ã€‚ä¸¤ç§æ–¹æ³•éƒ½èƒ½è‡ªç„¶ç¼“è§£è¿‡å¹³æ»‘é—®é¢˜ï¼Œå› ä¸ºæ¶ˆæ¯ä¼ é€’çš„è·ç¦»è¶Šè¿œï¼Œè¿‡å¹³æ»‘çš„ç¨‹åº¦è¶Šå°ã€‚
- en: Whether it is used for message passing design, GNN dataflow or graph rewiring,
    in 2023 we have just started to grasp the importance of *time* â€” even when time
    variation is not explicitly present in our dataset.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: æ— è®ºæ˜¯ç”¨äºæ¶ˆæ¯ä¼ é€’è®¾è®¡ã€GNNæ•°æ®æµè¿˜æ˜¯å›¾é‡è¿æ¥ï¼Œåˆ°äº†2023å¹´ï¼Œæˆ‘ä»¬æ‰åˆšåˆšå¼€å§‹ç†è§£*æ—¶é—´*çš„é‡è¦æ€§â€”â€”å³ä¾¿æ—¶é—´å˜åŒ–åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸­å¹¶æœªæ˜¾å¼å­˜åœ¨ã€‚
- en: '***Ben Finkelshtein (Oxford)***'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '***Ben Finkelshtein (ç‰›æ´¥å¤§å­¦)***'
- en: The time-dependent message passing paradigm presented in [Co-GNNs](https://arxiv.org/abs/2310.01267)
    is a learnable generalisation of message passing, which allows each node to decide
    how to propagate information from or to its neighbours, thus enabling a more flexible
    flow of information. The nodes are regarded as players that can either broadcast
    to neighbors that listen *and* listen to neighbors that broadcast (like in classical
    message-passing), Broadcast to neighbors that listen, or Isolate (neither listen
    nor broadcast).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[Co-GNNs](https://arxiv.org/abs/2310.01267)ä¸­æå‡ºçš„åŸºäºæ—¶é—´çš„æ¶ˆæ¯ä¼ é€’èŒƒå¼æ˜¯ä¸€ç§å¯å­¦ä¹ çš„æ¶ˆæ¯ä¼ é€’æ³›åŒ–æ–¹å¼ï¼Œå®ƒå…è®¸æ¯ä¸ªèŠ‚ç‚¹å†³å®šå¦‚ä½•ä»å…¶é‚»å±…ä¼ é€’ä¿¡æ¯æˆ–å‘å…¶é‚»å±…ä¼ æ’­ä¿¡æ¯ï¼Œä»è€Œå®ç°ä¿¡æ¯æµåŠ¨çš„æ›´å¤§çµæ´»æ€§ã€‚è¿™äº›èŠ‚ç‚¹è¢«è§†ä¸ºå¯ä»¥è¿›è¡Œä»¥ä¸‹æ“ä½œçš„å‚ä¸è€…ï¼šå‘ç›‘å¬çš„é‚»å±…å¹¿æ’­*å¹¶ä¸”*ç›‘å¬å¹¿æ’­çš„é‚»å±…ï¼ˆç±»ä¼¼äºç»å…¸çš„æ¶ˆæ¯ä¼ é€’ï¼‰ï¼Œä»…å‘ç›‘å¬çš„é‚»å±…å¹¿æ’­ï¼Œæˆ–è€…éš”ç¦»ï¼ˆæ—¢ä¸ç›‘å¬ä¹Ÿä¸å¹¿æ’­ï¼‰ã€‚
- en: The interplay between these actions and the ability to change them locally and
    dynamically allows CoGNNs to determine a **task-specific** computational graph
    (which can be considered as a form of **dynamic** and **directed rewiring**, learn
    different action distribution for two nodes with different node features (both
    **feature-** and **structure-based)**.CoGNNs allow **asynchronous** updates across
    nodes and also yield unique node identifiers with high probability, which allows
    them to distinguish any pair of graphs (**more expressive than 1-WL**, at the
    expense of equivariance holding only in expectation).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ“ä½œä¹‹é—´çš„ç›¸äº’ä½œç”¨ä»¥åŠå±€éƒ¨å’ŒåŠ¨æ€åœ°æ”¹å˜å®ƒä»¬çš„èƒ½åŠ›ä½¿CoGNNèƒ½å¤Ÿç¡®å®šä¸€ä¸ª**ä»»åŠ¡ç‰¹å®š**çš„è®¡ç®—å›¾ï¼ˆå¯ä»¥è§†ä¸ºä¸€ç§**åŠ¨æ€**å’Œ**å®šå‘é‡è¿æ¥**çš„å½¢å¼ï¼‰ï¼Œä¸ºå…·æœ‰ä¸åŒèŠ‚ç‚¹ç‰¹å¾çš„ä¸¤ä¸ªèŠ‚ç‚¹å­¦ä¹ ä¸åŒçš„æ“ä½œåˆ†å¸ƒï¼ˆåŒ…æ‹¬**ç‰¹å¾**å’Œ**ç»“æ„**åŸºç¡€çš„ï¼‰ã€‚CoGNNè¿˜å…è®¸èŠ‚ç‚¹ä¹‹é—´è¿›è¡Œ**å¼‚æ­¥**æ›´æ–°ï¼Œå¹¶ä¸”ä»¥è¾ƒé«˜çš„æ¦‚ç‡ç”Ÿæˆå”¯ä¸€çš„èŠ‚ç‚¹æ ‡è¯†ç¬¦ï¼Œä»è€Œä½¿å®ƒä»¬èƒ½å¤ŸåŒºåˆ†ä»»ä½•ä¸€å¯¹å›¾ï¼ˆ**æ¯”1-WLæ›´å…·è¡¨ç°åŠ›**ï¼Œä½†ä»…åœ¨æœŸæœ›ä¸‹ä¿æŒç­‰å˜æ€§ï¼‰ã€‚
- en: '![](../Images/832de2447364cc0ce241d969ef16d4ff.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/832de2447364cc0ce241d969ef16d4ff.png)'
- en: 'Left to right: classical MPNNs (all nodes broadcast & listen), DeepSets (all
    nodes isolate), and generic CoGNNs. Figure from [blog post](/co-operative-graph-neural-networks-34c59bf6805e?gi=98ca39c38e41).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å·¦åˆ°å³ï¼šç»å…¸çš„MPNNï¼ˆæ‰€æœ‰èŠ‚ç‚¹éƒ½å¹¿æ’­å’Œç›‘å¬ï¼‰ã€DeepSetsï¼ˆæ‰€æœ‰èŠ‚ç‚¹éƒ½éš”ç¦»ï¼‰ä»¥åŠé€šç”¨çš„CoGNNã€‚å›¾æºè‡ª[åšå®¢æ–‡ç« ](/co-operative-graph-neural-networks-34c59bf6805e?gi=98ca39c38e41)ã€‚
- en: 'Check the Medium post for more details:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹Mediumæ–‡ç« äº†è§£æ›´å¤šè¯¦æƒ…ï¼š
- en: '[](/co-operative-graph-neural-networks-34c59bf6805e?source=post_page-----3af5d38376e1--------------------------------)
    [## Co-operative Graph Neural Networks'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/co-operative-graph-neural-networks-34c59bf6805e?source=post_page-----3af5d38376e1--------------------------------)
    [## åä½œå›¾ç¥ç»ç½‘ç»œ'
- en: A new message-passing paradigm where every node can choose to either â€˜listenâ€™,
    â€˜broadcastâ€™, â€˜listen & broadcastâ€™ orâ€¦
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¸€ç§æ–°çš„æ¶ˆæ¯ä¼ é€’èŒƒå¼ï¼Œå…¶ä¸­æ¯ä¸ªèŠ‚ç‚¹å¯ä»¥é€‰æ‹©â€œç›‘å¬â€ã€â€œå¹¿æ’­â€ã€â€œç›‘å¬å¹¶å¹¿æ’­â€æˆ–...
- en: towardsdatascience.com](/co-operative-graph-neural-networks-34c59bf6805e?source=post_page-----3af5d38376e1--------------------------------)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/co-operative-graph-neural-networks-34c59bf6805e?source=post_page-----3af5d38376e1--------------------------------)'
- en: '***Francesco Di Giovanni (Oxford)***'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '***Francesco Di Giovanniï¼ˆç‰›æ´¥å¤§å­¦ï¼‰***'
- en: â€œThe understanding of over-squashing, arising when the task depends on the interaction
    between nodes with large commute time, acted as a catalyst for the emergence of
    graph rewiring as a valid approach for designing new GNNs.â€ â€” **Francesco Di Giovanni**
    (Oxford)
  id: totrans-130
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œè¿‡åº¦å‹ç¼©ï¼ˆover-squashingï¼‰çš„ç†è§£ï¼Œå°¤å…¶æ˜¯å½“ä»»åŠ¡ä¾èµ–äºèŠ‚ç‚¹é—´é•¿æ—¶é—´äº¤äº’æ—¶ï¼Œæˆä¸ºäº†å›¾é‡è¿æ¥ä½œä¸ºä¸€ç§æœ‰æ•ˆæ–¹æ³•çš„å‚¬åŒ–å‰‚ï¼Œç”¨ä»¥è®¾è®¡æ–°çš„GNNã€‚â€
    â€” **Francesco Di Giovanni**ï¼ˆç‰›æ´¥å¤§å­¦ï¼‰
- en: ï¸ğŸ’¡ *Graph rewiring* broadly entails altering the connectivity of the input graph
    to facilitate the solution of the downstream task. Recently, this has often targeted
    bottlenecks in the graph, thereby adding (and removing) edges to improve the flow
    of information. While the emphasis has been on **where** messages are exchanged,
    recent works (discussed above) have shed light on the relevance of **when** messages
    should be exchanged as well. One rationale behind these approaches, albeit often
    implicit, is that the hidden representations built by the layers of a GNN, provide
    the graph with an (artificially) *dynamic* component, even though the graph and
    input features are static. This perspective can be leveraged in several ways.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ï¸ğŸ’¡ *å›¾ç»“æ„é‡è¿æ¥*å¹¿ä¹‰ä¸ŠæŒ‡çš„æ˜¯æ”¹å˜è¾“å…¥å›¾çš„è¿æ¥æ–¹å¼ï¼Œä»¥ä¿ƒè¿›ä¸‹æ¸¸ä»»åŠ¡çš„è§£å†³ã€‚æœ€è¿‘ï¼Œè¿™é€šå¸¸èšç„¦äºå›¾ä¸­çš„ç“¶é¢ˆï¼Œä»è€Œæ·»åŠ ï¼ˆæˆ–ç§»é™¤ï¼‰è¾¹ç¼˜ï¼Œä»¥æ”¹å–„ä¿¡æ¯æµåŠ¨ã€‚è™½ç„¶é‡ç‚¹é€šå¸¸æ”¾åœ¨**ä¿¡æ¯äº¤æ¢çš„åœ°ç‚¹**ï¼Œä½†æœ€è¿‘çš„ç ”ç©¶ï¼ˆå¦‚ä¸Šæ‰€è¿°ï¼‰ä¹Ÿæ­ç¤ºäº†**ä½•æ—¶**è¿›è¡Œä¿¡æ¯äº¤æ¢çš„é‡è¦æ€§ã€‚è¿™äº›æ–¹æ³•èƒŒåçš„ä¸€ä¸ªé€»è¾‘ï¼Œå°½ç®¡é€šå¸¸æ˜¯éšå«çš„ï¼Œå°±æ˜¯GNNå„å±‚æ„å»ºçš„éšè—è¡¨ç¤ºä¸ºå›¾æä¾›äº†ä¸€ä¸ªï¼ˆäººä¸ºçš„ï¼‰*åŠ¨æ€*ç»„ä»¶ï¼Œå³ä¾¿å›¾å’Œè¾“å…¥ç‰¹å¾æ˜¯é™æ€çš„ã€‚è¿™ä¸ªè§†è§’å¯ä»¥é€šè¿‡å¤šç§æ–¹å¼è¿›è¡Œåˆ©ç”¨ã€‚
- en: '![](../Images/5e8ada337159c1aa6df710ba9f895619.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e8ada337159c1aa6df710ba9f895619.png)'
- en: 'In the classicical MPNN setting, at every layer information only travels from
    a node to its immediate neighbours. In DRew, the graph changes based on the layer,
    with newly added edges connecting nodes at distance r from layer r âˆ’ 1 onward.
    Finally, in Î½DRew, we also introduce a delay mechanism equivalent to skip-connections
    between different nodes based on their mutual distance. Source: [Gutteridge et
    al.](https://arxiv.org/abs/2305.08018)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç»å…¸çš„MPNNè®¾ç½®ä¸­ï¼Œåœ¨æ¯ä¸€å±‚ï¼Œä¿¡æ¯ä»…åœ¨èŠ‚ç‚¹åŠå…¶ç›´æ¥é‚»å±…ä¹‹é—´ä¼ æ’­ã€‚åœ¨DRewä¸­ï¼Œå›¾ä¼šéšç€å±‚æ•°çš„å˜åŒ–è€Œå˜åŒ–ï¼Œæ–°æ·»åŠ çš„è¾¹ä»ç¬¬ *r* å±‚èµ·è¿æ¥è·ç¦» *r*
    çš„èŠ‚ç‚¹ã€‚æœ€åï¼Œåœ¨Î½DRewä¸­ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªå»¶è¿Ÿæœºåˆ¶ï¼Œç›¸å½“äºåŸºäºèŠ‚ç‚¹ä¹‹é—´çš„ç›¸äº’è·ç¦»çš„è·³è·ƒè¿æ¥ã€‚æ¥æºï¼š[Gutteridgeç­‰äºº](https://arxiv.org/abs/2305.08018)
- en: 'â¡ï¸ One framework that has particularly embraced such an angle is [**DRew**](https://arxiv.org/abs/2305.08018),
    which extends any message-passing model in two ways: (i) it connects nodes at
    distance *r* directly, but only from layer *r* onwards; (ii) when nodes are connected,
    a delay is applied to their message exchange, based on their mutual distance.
    As the figure above illustrates, (i) allows the network to better retain the inductive
    bias, as nodes that are closer, interact *earlier;* (ii) instead acts as *distance-aware*
    *skip connections,* thereby facilitating the propagation of gradients for the
    loss. Most likely, it is for this reason, and not prevention of over-smoothing
    (which hardly has an impact for graph-level tasks), that the framework significantly
    enhances the performance of standard GNNs at larger depths (more details can be
    found in this [blog post](/dynamically-rewired-delayed-message-passing-gnns-2d5ff18687c2)).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ ä¸€ä¸ªç‰¹åˆ«é‡‡çº³è¿™ä¸€è§’åº¦çš„æ¡†æ¶æ˜¯[**DRew**](https://arxiv.org/abs/2305.08018)ï¼Œå®ƒé€šè¿‡ä¸¤ç§æ–¹å¼æ‰©å±•äº†ä»»ä½•æ¶ˆæ¯ä¼ é€’æ¨¡å‹ï¼šï¼ˆiï¼‰å®ƒç›´æ¥è¿æ¥è·ç¦»
    *r* çš„èŠ‚ç‚¹ï¼Œä½†ä»…ä»ç¬¬ *r* å±‚å¼€å§‹ï¼›ï¼ˆiiï¼‰å½“èŠ‚ç‚¹è¿æ¥æ—¶ï¼ŒåŸºäºå®ƒä»¬ä¹‹é—´çš„ç›¸äº’è·ç¦»ï¼Œåº”ç”¨å»¶è¿Ÿåˆ°å®ƒä»¬çš„æ¶ˆæ¯äº¤æ¢ã€‚å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œï¼ˆiï¼‰ä½¿ç½‘ç»œæ›´å¥½åœ°ä¿ç•™å½’çº³åå·®ï¼Œå› ä¸ºè¾ƒè¿‘çš„èŠ‚ç‚¹ä¼š*æ›´æ—©*äº’åŠ¨ï¼›ï¼ˆiiï¼‰åˆ™å……å½“*è·ç¦»æ„ŸçŸ¥çš„*
    *è·³è·ƒè¿æ¥*ï¼Œä»è€Œæœ‰åŠ©äºæŸå¤±å‡½æ•°çš„æ¢¯åº¦ä¼ æ’­ã€‚å¾ˆå¯èƒ½æ­£æ˜¯ç”±äºè¿™ä¸ªåŸå› ï¼Œè€Œä¸æ˜¯é˜²æ­¢è¿‡åº¦å¹³æ»‘ï¼ˆå¯¹å›¾çº§ä»»åŠ¡å‡ ä¹æ²¡æœ‰å½±å“ï¼‰ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—å¢å¼ºäº†æ ‡å‡†GNNåœ¨æ›´å¤§æ·±åº¦ä¸‹çš„è¡¨ç°ï¼ˆæ›´å¤šç»†èŠ‚å¯ä»¥å‚è€ƒè¿™ç¯‡[åšå®¢æ–‡ç« ](/dynamically-rewired-delayed-message-passing-gnns-2d5ff18687c2)ï¼‰ã€‚
- en: '**ğŸ”® Predictions:** I believe that the deep implications of extending message-passing
    over the â€œtimeâ€ component would start to emerge in the coming year. Works like
    DRew have only scratched the surface of why rewiring over time (beyond space)
    might benefit the training of GNNs, drastically affecting their accuracy response
    across different depth regimes.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸ”® é¢„æµ‹ï¼š** æˆ‘ç›¸ä¿¡ï¼Œæ‰©å±•æ¶ˆæ¯ä¼ é€’åˆ°â€œæ—¶é—´â€ç»´åº¦çš„æ·±è¿œå½±å“å°†åœ¨æ¥å¹´å¼€å§‹æ˜¾ç°ã€‚åƒDRewè¿™æ ·çš„å·¥ä½œä»…ä»…è§¦åŠäº†æ—¶é—´ä¸Šé‡è¿ï¼ˆè¶…è¶Šç©ºé—´ï¼‰çš„åŸå› ï¼Œå®ƒå¯èƒ½æœ‰åŠ©äºGNNè®­ç»ƒï¼Œæ˜¾è‘—å½±å“å®ƒä»¬åœ¨ä¸åŒæ·±åº¦å±‚æ¬¡ä¸‹çš„å‡†ç¡®æ€§å“åº”ã€‚'
- en: â¡ï¸ More broadly, I hope that theoretical and practical developments of graph
    rewiring could be translated into scientific domains, where equivariant GNNs are
    often applied to 3D problems which either do not have a natural graph structure
    (making the question of â€œwhereâ€ messages should be exchanged ever more relevant)
    or (and) exhibit natural temporal (multi-scale) properties (making the question
    of â€œwhenâ€ messages should be exchanged likely to be key for reducing memory constraints
    and retaining the right inductive bias).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ æ›´å¹¿æ³›åœ°è¯´ï¼Œæˆ‘å¸Œæœ›å›¾é‡è¿çš„ç†è®ºå’Œå®è·µå‘å±•èƒ½å¤Ÿè¢«åº”ç”¨åˆ°ç§‘å­¦é¢†åŸŸï¼Œå…¶ä¸­ç­‰å˜å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰é€šå¸¸åº”ç”¨äºä¸‰ç»´é—®é¢˜ï¼Œè¿™äº›é—®é¢˜è¦ä¹ˆæ²¡æœ‰è‡ªç„¶çš„å›¾ç»“æ„ï¼ˆä½¿å¾—â€œåœ¨å“ªé‡Œâ€äº¤æ¢æ¶ˆæ¯å˜å¾—æ›´åŠ ç›¸å…³ï¼‰ï¼Œè¦ä¹ˆï¼ˆä¸”ï¼‰å±•ç°å‡ºè‡ªç„¶çš„æ—¶é—´ï¼ˆå¤šå°ºåº¦ï¼‰ç‰¹æ€§ï¼ˆä½¿å¾—â€œä½•æ—¶â€äº¤æ¢æ¶ˆæ¯å¯èƒ½æ˜¯å‡å°‘å†…å­˜é™åˆ¶å¹¶ä¿æŒæ­£ç¡®å½’çº³åå·®çš„å…³é”®ï¼‰ã€‚
- en: Geometry, Topology, Geometric Algebras & PDEs
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‡ ä½•å­¦ã€æ‹“æ‰‘å­¦ã€å‡ ä½•ä»£æ•°ä¸åå¾®åˆ†æ–¹ç¨‹
- en: '*Johannes Brandstetter (JKU Linz), Michael Galkin (Intel), Mathilde Papillon
    (UC Santa Barbara), Bastian Rieck (Helmholtz & TUM), and David Ruhe (U Amsterdam)*'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '*Johannes Brandstetterï¼ˆJKU Linzï¼‰ï¼ŒMichael Galkinï¼ˆIntelï¼‰ï¼ŒMathilde Papillonï¼ˆUC
    Santa Barbaraï¼‰ï¼ŒBastian Rieckï¼ˆHelmholtz & TUMï¼‰ï¼ŒDavid Ruheï¼ˆU Amsterdamï¼‰*'
- en: '2023 brought the most comprehensive introduction to (and a survey of) Geometric
    GNNs covering the most basic and necessary concepts with a handful of examples:
    **A Hitchhikerâ€™s Guide to Geometric GNNs for 3D Atomic Systems** ([Duval, Mathis,
    Joshi, Schmidt, et al.](https://arxiv.org/abs/2312.07511)). If you ever wanted
    to learn from scratch the core architectures powering recent breakthroughs of
    graph ML in protein design, material discovery, molecular simulations, and more
    â€” this is what you need!'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 2023å¹´å¸¦æ¥äº†æœ€å…¨é¢çš„å‡ ä½•å›¾ç¥ç»ç½‘ç»œï¼ˆGeometric GNNsï¼‰ä»‹ç»ï¼ˆåŠç»¼è¿°ï¼‰ï¼Œæ¶µç›–äº†æœ€åŸºç¡€å’Œå¿…è¦çš„æ¦‚å¿µï¼Œå¹¶æä¾›äº†ä¸€äº›ç¤ºä¾‹ï¼šã€Š**ä¸‰ç»´åŸå­ç³»ç»Ÿçš„å‡ ä½•å›¾ç¥ç»ç½‘ç»œæŒ‡å—**ã€‹([Duval,
    Mathis, Joshi, Schmidtç­‰](https://arxiv.org/abs/2312.07511))ã€‚å¦‚æœä½ æ›¾ç»æƒ³ä»é›¶å¼€å§‹å­¦ä¹ é©±åŠ¨æœ€è¿‘è›‹ç™½è´¨è®¾è®¡ã€ææ–™å‘ç°ã€åˆ†å­æ¨¡æ‹Ÿç­‰çªç ´çš„å›¾æœºå™¨å­¦ä¹ æ ¸å¿ƒæ¶æ„â€”â€”è¿™æ­£æ˜¯ä½ éœ€è¦çš„ï¼
- en: '![](../Images/93b169f6f151d0760b6c3005dcaf3a49.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93b169f6f151d0760b6c3005dcaf3a49.png)'
- en: 'Timeline of key Geometric GNNs for 3D atomic systems, characterised by the
    type of intermediate representations within layers. Source: [Duval, Mathis, Joshi,
    Schmidt, et al.](https://arxiv.org/abs/2312.07511)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 3DåŸå­ç³»ç»Ÿçš„å…³é”®å‡ ä½•GNNæ—¶é—´çº¿ï¼ŒæŒ‰å±‚å†…ä¸­é—´è¡¨ç¤ºçš„ç±»å‹è¿›è¡Œåˆ†ç±»ã€‚æ¥æºï¼š[Duval, Mathis, Joshi, Schmidt ç­‰äºº](https://arxiv.org/abs/2312.07511)
- en: '**Topology**'
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**æ‹“æ‰‘**'
- en: 'ğŸ’¡ Working with topological structures in 2023 has become much easier for both
    researchers and practitioners thanks to the amazing efforts of the [PyT team](https://github.com/pyt-team)
    and their suite of resources: **TopoNetX**, **TopoModelX**, and **TopoEmbedX**.
    [TopoNetX](https://github.com/pyt-team/TopoNetX) is pretty much the networkx for
    topological data. TopoNetX supports standard structures like cellular complexes,
    simplicial complexes, and combinatorial complexes. [TopoModelX](https://github.com/pyt-team/TopoModelX)
    is a PyG-like library for deep learning on topological data and implements famous
    models like [MPSN](https://arxiv.org/abs/2103.03212) and [CIN](https://arxiv.org/abs/2106.12575)
    with a neat unified interface (the original PyG implementations are quite tangled).
    [TopoEmbedX](https://github.com/pyt-team/TopoEmbedX) helps to train embedding
    models on topological data and supports core algorithms like [Cell2Vec](https://arxiv.org/abs/2010.00743).'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ å¾—ç›Šäº[PyTå›¢é˜Ÿ](https://github.com/pyt-team)çš„å“è¶ŠåŠªåŠ›å’Œä»–ä»¬çš„ä¸€ç³»åˆ—èµ„æºï¼Œ2023å¹´ç ”ç©¶äººå‘˜å’Œå®è·µè€…åœ¨å¤„ç†æ‹“æ‰‘ç»“æ„æ—¶å˜å¾—æ›´åŠ å®¹æ˜“ï¼š**TopoNetX**ã€**TopoModelX**
    å’Œ **TopoEmbedX**ã€‚[TopoNetX](https://github.com/pyt-team/TopoNetX)åŸºæœ¬ä¸Šæ˜¯æ‹“æ‰‘æ•°æ®çš„networkxã€‚TopoNetXæ”¯æŒæ ‡å‡†ç»“æ„ï¼Œå¦‚ç»†èƒå¤å½¢ã€å•çº¯å½¢å¤å½¢å’Œç»„åˆå¤å½¢ã€‚[TopoModelX](https://github.com/pyt-team/TopoModelX)æ˜¯ä¸€ä¸ªç±»ä¼¼PyGçš„åº“ï¼Œç”¨äºæ‹“æ‰‘æ•°æ®ä¸Šçš„æ·±åº¦å­¦ä¹ ï¼Œå¹¶å®ç°äº†è‘—åçš„æ¨¡å‹å¦‚[MPSN](https://arxiv.org/abs/2103.03212)å’Œ[CIN](https://arxiv.org/abs/2106.12575)ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªç®€æ´ç»Ÿä¸€çš„æ¥å£ï¼ˆåŸå§‹çš„PyGå®ç°æ¯”è¾ƒå¤æ‚ï¼‰ã€‚[TopoEmbedX](https://github.com/pyt-team/TopoEmbedX)æœ‰åŠ©äºåœ¨æ‹“æ‰‘æ•°æ®ä¸Šè®­ç»ƒåµŒå…¥æ¨¡å‹ï¼Œå¹¶æ”¯æŒåƒ[Cell2Vec](https://arxiv.org/abs/2010.00743)è¿™æ ·çš„æ ¸å¿ƒç®—æ³•ã€‚
- en: '![](../Images/39ecb150b3d75cdea28e11b8d544628d.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/39ecb150b3d75cdea28e11b8d544628d.png)'
- en: 'Domains: Nodes in blue, (hyper)edges in pink, and faces in dark red. Source:
    [TopoNetX](https://github.com/pyt-team/TopoNetX), [Papillon et al](https://arxiv.org/abs/2304.10031)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: é¢†åŸŸï¼šè“è‰²èŠ‚ç‚¹ï¼Œç²‰è‰²ï¼ˆè¶…ï¼‰è¾¹ï¼Œæ·±çº¢è‰²é¢ã€‚æ¥æºï¼š[TopoNetX](https://github.com/pyt-team/TopoNetX)ï¼Œ[Papillon
    ç­‰äºº](https://arxiv.org/abs/2304.10031)
- en: ğŸ’¡ A great headstart to the field and basic building blocks of those topological
    networks are the papers by [Hajij et al](https://arxiv.org/abs/2206.00606) and
    by [Papillon et al](https://arxiv.org/abs/2304.10031). A notable chunk of models
    was implemented by the members of the [Topology, Algebra, and Geometry in Data
    Science](https://www.tagds.com/home) (TAG) community that regularly organizes
    topological workshops at ML conferences.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ è¯¥é¢†åŸŸçš„ä¸€ä¸ªé‡è¦èµ·æ­¥å’ŒåŸºæœ¬æ„å»ºå—æ˜¯[Hajij ç­‰äºº](https://arxiv.org/abs/2206.00606)å’Œ[Papillon ç­‰äºº](https://arxiv.org/abs/2304.10031)çš„è®ºæ–‡ã€‚ä¸€ä¸ªæ˜¾è‘—çš„æ¨¡å‹éƒ¨åˆ†æ˜¯ç”±[æ•°æ®ç§‘å­¦ä¸­çš„æ‹“æ‰‘ã€ä»£æ•°ä¸å‡ ä½•](https://www.tagds.com/home)ï¼ˆTAGï¼‰ç¤¾åŒºçš„æˆå‘˜å®ç°çš„ï¼Œè¯¥ç¤¾åŒºå®šæœŸåœ¨æœºå™¨å­¦ä¹ ä¼šè®®ä¸Šç»„ç»‡æ‹“æ‰‘å­¦ç ”è®¨ä¼šã€‚
- en: '***Mathilde Papillon (UCSB)***'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '***Mathilde Papillon (UCSB)***'
- en: â€œUntil 2023, the field of topological deep learning featured a fractured landscape
    of enriched representations for relational data.â€ â€” Mathilde Papillon (UC Santa
    Barbara)
  id: totrans-148
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œç›´åˆ°2023å¹´ï¼Œæ‹“æ‰‘æ·±åº¦å­¦ä¹ é¢†åŸŸä»ç„¶å‘ˆç°å‡ºä¸€ä¸ªæ”¯ç¦»ç ´ç¢çš„å…³ç³»æ•°æ®å¢å¼ºè¡¨ç¤ºçš„æ ¼å±€ã€‚â€ â€”â€” Mathilde Papillonï¼ˆåŠ å·å¤§å­¦åœ£å¡”èŠ­èŠ­æ‹‰åˆ†æ ¡ï¼‰
- en: â¡ï¸ Message-passing models were only built upon and benchmarked against other
    models of the same domain, e.g., the simplicial complex community remained insular
    to the hypergraph community. To make matters worse, most models adopted a unique
    mathematical notation. Deciding which model would be best suited to a given application
    seemed like a monumental task. A unification theory proposed by [Hajij et al](https://arxiv.org/abs/2206.00606)
    offered a general scheme under which all models could be systematically described
    and classified. We applied this theory to the literature to produce a comprehensive
    yet concise [survey of message passing in topological deep learning](https://arxiv.org/abs/2304.10031)
    that also serves as an accessible introduction to the field. We additionally provide
    a [dictionary listing all the model architectures](https://github.com/awesome-tnns/awesome-tnns)
    in one unifying notation.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ æ¶ˆæ¯ä¼ é€’æ¨¡å‹ä»…åŸºäºç›¸åŒé¢†åŸŸçš„å…¶ä»–æ¨¡å‹è¿›è¡Œæ„å»ºå’ŒåŸºå‡†æµ‹è¯•ï¼Œä¾‹å¦‚ï¼Œå•çº¯å½¢å¤å½¢ç¤¾åŒºä¸è¶…å›¾ç¤¾åŒºä¹‹é—´ä¸€ç›´æ˜¯å°é—­çš„ã€‚æ›´ç³Ÿç³•çš„æ˜¯ï¼Œå¤§å¤šæ•°æ¨¡å‹é‡‡ç”¨äº†ç‹¬ç‰¹çš„æ•°å­¦ç¬¦å·ã€‚è¿™ä½¿å¾—é€‰æ‹©æœ€é€‚åˆç‰¹å®šåº”ç”¨çš„æ¨¡å‹å˜å¾—åƒä¸€é¡¹è‰°å·¨çš„ä»»åŠ¡ã€‚[Hajij
    ç­‰äºº](https://arxiv.org/abs/2206.00606)æå‡ºçš„ç»Ÿä¸€ç†è®ºæä¾›äº†ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œåœ¨è¿™ä¸ªæ¡†æ¶ä¸‹ï¼Œæ‰€æœ‰æ¨¡å‹éƒ½å¯ä»¥è¢«ç³»ç»Ÿåœ°æè¿°å’Œåˆ†ç±»ã€‚æˆ‘ä»¬å°†è¯¥ç†è®ºåº”ç”¨äºæ–‡çŒ®ä¸­ï¼Œåˆ¶ä½œäº†ä¸€ä»½å…¨é¢è€Œç®€æ˜çš„[æ‹“æ‰‘æ·±åº¦å­¦ä¹ ä¸­æ¶ˆæ¯ä¼ é€’çš„è°ƒæŸ¥](https://arxiv.org/abs/2304.10031)ï¼Œè¿™ä¹Ÿæ˜¯è¯¥é¢†åŸŸçš„æ˜“äºç†è§£çš„å…¥é—¨ä»‹ç»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ª[å­—å…¸ï¼Œåˆ—å‡ºäº†æ‰€æœ‰æ¨¡å‹æ¶æ„](https://github.com/awesome-tnns/awesome-tnns)ï¼Œå¹¶é‡‡ç”¨ç»Ÿä¸€çš„ç¬¦å·è¡¨ç¤ºã€‚
- en: â¡ï¸ To further unify the field, we organized the first [Topological Deep Learning
    Challenge](https://pyt-team.github.io/topomodelx/challenge/index.html), hosted
    at the [2023 ICML TAG workshop](https://www.tagds.com/events/conference-workshops/tag-ml23)
    and recorded via this white paper by [Papillon et al](https://proceedings.mlr.press/v221/papillon23a.html).
    The goal was to foster reproducible research by crowdsourcing the open-source
    implementation of neural networks on topological domains. As part of the challenge,
    participants from around the world contributed implementations of pre-existing
    topological deep learning models in [TopoModelX](https://github.com/pyt-team/TopoModelX).
    Each submission was rigorously unit-tested and included benchmark training on
    datasets loaded from [TopoNetX](https://github.com/pyt-team/TopoNetX). It is our
    hope that this one-stop-shop suite of consistently implemented models will help
    practitioners test-drive topological methods for new applications and developments
    in 2024.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ ä¸ºäº†è¿›ä¸€æ­¥ç»Ÿä¸€è¿™ä¸€é¢†åŸŸï¼Œæˆ‘ä»¬ç»„ç»‡äº†ç¬¬ä¸€æ¬¡[æ‹“æ‰‘æ·±åº¦å­¦ä¹ æŒ‘æˆ˜èµ›](https://pyt-team.github.io/topomodelx/challenge/index.html)ï¼Œè¯¥æ´»åŠ¨åœ¨[2023
    ICML TAG ç ”è®¨ä¼š](https://www.tagds.com/events/conference-workshops/tag-ml23)ä¸Šä¸¾åŠï¼Œå¹¶é€šè¿‡[Papillon
    ç­‰äºº](https://proceedings.mlr.press/v221/papillon23a.html)çš„ç™½çš®ä¹¦è¿›è¡Œäº†è®°å½•ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡ä¼—åŒ…çš„æ–¹å¼ä¿ƒè¿›å¯é‡å¤ç ”ç©¶ï¼Œå°¤å…¶æ˜¯æ¨åŠ¨åŸºäºæ‹“æ‰‘é¢†åŸŸçš„ç¥ç»ç½‘ç»œå¼€æºå®ç°ã€‚ä½œä¸ºæŒ‘æˆ˜çš„ä¸€éƒ¨åˆ†ï¼Œæ¥è‡ªä¸–ç•Œå„åœ°çš„å‚ä¸è€…è´¡çŒ®äº†åœ¨[TopoModelX](https://github.com/pyt-team/TopoModelX)ä¸­å®ç°çš„ç°æœ‰æ‹“æ‰‘æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚æ¯ä¸ªæäº¤éƒ½ç»è¿‡ä¸¥æ ¼çš„å•å…ƒæµ‹è¯•ï¼Œå¹¶åŒ…æ‹¬åœ¨[TopoNetX](https://github.com/pyt-team/TopoNetX)åŠ è½½çš„æ•°æ®é›†ä¸Šçš„åŸºå‡†è®­ç»ƒã€‚æˆ‘ä»¬å¸Œæœ›ï¼Œè¿™ä¸€å¥—å§‹ç»ˆå¦‚ä¸€åœ°å®ç°çš„æ¨¡å‹èƒ½å¤Ÿå¸®åŠ©ä»ä¸šè€…åœ¨2024å¹´æµ‹è¯•æ‹“æ‰‘æ–¹æ³•åœ¨æ–°åº”ç”¨å’Œå¼€å‘ä¸­çš„è¡¨ç°ã€‚
- en: '***Bastian Rieck (Helmholtz & TUM)***'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '***Bastian Rieckï¼ˆèµ«å°”å§†éœå…¹ç ”ç©¶æ‰€ & æ…•å°¼é»‘å·¥ä¸šå¤§å­¦ï¼‰***'
- en: 2023 was an exciting year for topology-driven machine learning methods. On the
    one hand, we saw more integrations with geometrical concepts like curvature, thus
    demonstrating the versatility of hybrid geometrical-topological models. For instance,
    in [â€˜Curvature Filtrations for Graph Generative Model Evaluation,â€™](https://arxiv.org/abs/2301.12906)
    we showed how to employ curvature as a way to select suitable graph generative
    models. Here, curvature serves as a â€˜lensâ€™ that we use to extract graph structure
    information, while we employ persistent homology, a topological method, to compare
    this information in a consistent fashion.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 2023å¹´å¯¹äºæ‹“æ‰‘é©±åŠ¨çš„æœºå™¨å­¦ä¹ æ–¹æ³•æ¥è¯´æ˜¯æ¿€åŠ¨äººå¿ƒçš„ä¸€å¹´ã€‚ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†æ›´å¤šä¸å‡ ä½•æ¦‚å¿µï¼ˆå¦‚æ›²ç‡ï¼‰ç›¸ç»“åˆçš„åº”ç”¨ï¼Œå±•ç¤ºäº†å‡ ä½•-æ‹“æ‰‘æ··åˆæ¨¡å‹çš„å¤šæ ·æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨[ã€Šç”¨äºå›¾ç”Ÿæˆæ¨¡å‹è¯„ä¼°çš„æ›²ç‡è¿‡æ»¤ã€‹](https://arxiv.org/abs/2301.12906)ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨æ›²ç‡æ¥é€‰æ‹©åˆé€‚çš„å›¾ç”Ÿæˆæ¨¡å‹ã€‚åœ¨è¿™é‡Œï¼Œæ›²ç‡ä½œä¸ºä¸€ç§â€œé€é•œâ€ï¼Œç”¨æ¥æå–å›¾ç»“æ„ä¿¡æ¯ï¼Œè€Œæˆ‘ä»¬åˆ™ä½¿ç”¨æŒç»­åŒè°ƒï¼ˆä¸€ç§æ‹“æ‰‘æ–¹æ³•ï¼‰ä»¥ä¸€è‡´çš„æ–¹å¼æ¯”è¾ƒè¿™äº›ä¿¡æ¯ã€‚
- en: '![](../Images/17bc4a07a06a2f13e04b43502677d736.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17bc4a07a06a2f13e04b43502677d736.png)'
- en: 'An overview of the pipeline for evaluating graph generative models using discrete
    curvature. The ordering on edges gives rise to a curvature filtration, followed
    by a corresponding persistence diagram and landscape. For graph generative models,
    we select a curvature, apply this framework element-wise, and evaluate the similarity
    of the generated and reference distributions by comparing their average landscapes.
    Source: [Southern, Wayland, Bronstein, and Rieck.](https://arxiv.org/abs/2301.12906)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ç¦»æ•£æ›²ç‡è¯„ä¼°å›¾ç”Ÿæˆæ¨¡å‹çš„ç®¡é“æ¦‚è¿°ã€‚è¾¹çš„æ’åºä¼šäº§ç”Ÿä¸€ä¸ªæ›²ç‡è¿‡æ»¤ï¼Œæ¥ç€æ˜¯ç›¸åº”çš„æŒä¹…æ€§å›¾å’Œåœ°å½¢å›¾ã€‚å¯¹äºå›¾ç”Ÿæˆæ¨¡å‹ï¼Œæˆ‘ä»¬é€‰æ‹©ä¸€ä¸ªæ›²ç‡ï¼Œé€å…ƒç´ åº”ç”¨è¿™ä¸ªæ¡†æ¶ï¼Œå¹¶é€šè¿‡æ¯”è¾ƒç”Ÿæˆåˆ†å¸ƒå’Œå‚è€ƒåˆ†å¸ƒçš„å¹³å‡åœ°å½¢å›¾æ¥è¯„ä¼°å®ƒä»¬çš„ç›¸ä¼¼æ€§ã€‚æ¥æºï¼š[Southern,
    Wayland, Bronstein, å’Œ Rieck.](https://arxiv.org/abs/2301.12906)
- en: â¡ï¸ Another direction that serves to underscore that topology-driven methods
    are becoming a staple in graph learning research uses topology to assess the expressivity
    of graph neural network models. Sometimes, as in a very fascinating work from
    NeurIPS 2023 by [Immonen et al.](https://openreview.net/pdf?id=27TdrEvqLD) this
    even leads to novel models that leverage both geometrical and topological aspects
    of graphs in tandem! My own research also aims to contribute to this facet by
    specifically analyzing the [expressivity of persistent homology in graph learning](https://arxiv.org/abs/2302.09826).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ å¦ä¸€ä¸ªæ–¹å‘å¼ºè°ƒæ‹“æ‰‘é©±åŠ¨æ–¹æ³•æ­£æˆä¸ºå›¾å­¦ä¹ ç ”ç©¶çš„ä¸»æµï¼Œåˆ©ç”¨æ‹“æ‰‘æ¥è¯„ä¼°å›¾ç¥ç»ç½‘ç»œæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚æœ‰æ—¶ï¼Œæ­£å¦‚2023å¹´NeurIPSä¸Š[Immonenç­‰äºº](https://openreview.net/pdf?id=27TdrEvqLD)çš„ä¸€ä¸ªéå¸¸æœ‰è¶£çš„å·¥ä½œæ‰€å±•ç¤ºçš„é‚£æ ·ï¼Œè¿™ç”šè‡³ä¼šå¯¼è‡´æ–°çš„æ¨¡å‹ï¼Œå®ƒä»¬åŒæ—¶åˆ©ç”¨å›¾çš„å‡ ä½•å’Œæ‹“æ‰‘ç‰¹æ€§ï¼æˆ‘çš„ç ”ç©¶ä¹Ÿæ—¨åœ¨é€šè¿‡ä¸“é—¨åˆ†æ[æŒä¹…åŒè°ƒåœ¨å›¾å­¦ä¹ ä¸­çš„è¡¨è¾¾èƒ½åŠ›](https://arxiv.org/abs/2302.09826)ä¸ºè¿™ä¸€é¢†åŸŸåšå‡ºè´¡çŒ®ã€‚
- en: â€œ2023 also was the cusp of moving away â€” or beyond â€” persistent homology. Despite
    being rightfully seen as the paradigmatic algorithm for topology-driven machine
    learning, algebraic topology and differential topology offer an even richer fabric
    that can be used to analyse data.â€ â€” Bastian Rieck (Helmholtz & TUM)
  id: totrans-156
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œ2023å¹´ä¹Ÿæ­£æ˜¯ä¸€ä¸ªè½¬æŠ˜ç‚¹ï¼Œæ ‡å¿—ç€æˆ‘ä»¬å³å°†æ‘†è„±â€”â€”æˆ–è€…è¯´è¶…è¶Šâ€”â€”æŒä¹…åŒè°ƒã€‚å°½ç®¡æŒä¹…åŒè°ƒè¢«å…¬è®¤ä¸ºæ‹“æ‰‘é©±åŠ¨çš„æœºå™¨å­¦ä¹ èŒƒå¼ç®—æ³•ï¼Œä½†ä»£æ•°æ‹“æ‰‘å’Œå¾®åˆ†æ‹“æ‰‘æä¾›äº†æ›´ä¸ºä¸°å¯Œçš„ç»“æ„ï¼Œå¯ä»¥ç”¨æ¥åˆ†ææ•°æ®ã€‚â€â€”â€”å·´æ–¯è’‚å®‰Â·é‡Œå…‹ï¼ˆHelmholtz
    & TUMï¼‰
- en: â¡ï¸ With my great collaborators, we started looking at some alternatives very
    recently and came up with the concept of [neural differential forms](https://arxiv.org/abs/2312.08515).
    Differential forms permit us to elegantly build a bridge between geometry and
    topology by means of the [de Rham cohomology](https://en.wikipedia.org/wiki/De_Rham_cohomology)
    â€” a way to link the integration of certain objects (differential forms), i.e.
    a fundamentally *geometric* operation, to topological characteristics of input
    data. With some additional constructions, the de Rham cohomology permits us to
    learn geometric descriptions of graphs (or higher-order combinatorial complexes)
    and solve learning tasks without having to rely on message passing. The upshot
    are models with fewer parameters that are potentially more effective at solving
    such tasks. Thereâ€™s more to come here, since we have just started scratching the
    surface!
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ ä¸æˆ‘çš„ä¼˜ç§€åˆä½œä¼™ä¼´ä»¬ï¼Œæˆ‘ä»¬æœ€è¿‘å¼€å§‹æ¢ç´¢ä¸€äº›æ›¿ä»£æ–¹æ³•ï¼Œå¹¶æå‡ºäº†[ç¥ç»å¾®åˆ†å½¢å¼](https://arxiv.org/abs/2312.08515)çš„æ¦‚å¿µã€‚å¾®åˆ†å½¢å¼å…è®¸æˆ‘ä»¬é€šè¿‡[å¾·æ‹‰å§†åŒè°ƒ](https://en.wikipedia.org/wiki/De_Rham_cohomology)ä¼˜é›…åœ°å»ºç«‹å‡ ä½•å’Œæ‹“æ‰‘ä¹‹é—´çš„æ¡¥æ¢â€”â€”å¾·æ‹‰å§†åŒè°ƒæ˜¯å°†æŸäº›å¯¹è±¡ï¼ˆå¾®åˆ†å½¢å¼ï¼‰çš„ç§¯åˆ†â€”â€”å³ä¸€ç§æœ¬è´¨ä¸Š*å‡ ä½•*çš„æ“ä½œâ€”â€”ä¸è¾“å…¥æ•°æ®çš„æ‹“æ‰‘ç‰¹å¾è”ç³»èµ·æ¥çš„æ–¹æ³•ã€‚é€šè¿‡ä¸€äº›é¢å¤–çš„æ„é€ ï¼Œå¾·æ‹‰å§†åŒè°ƒä½¿æˆ‘ä»¬èƒ½å¤Ÿå­¦ä¹ å›¾ï¼ˆæˆ–æ›´é«˜é˜¶çš„ç»„åˆå¤å½¢ï¼‰çš„å‡ ä½•æè¿°ï¼Œå¹¶åœ¨ä¸ä¾èµ–ä¿¡æ¯ä¼ é€’çš„æƒ…å†µä¸‹è§£å†³å­¦ä¹ ä»»åŠ¡ã€‚å…¶ç»“æœæ˜¯å‚æ•°æ›´å°‘çš„æ¨¡å‹ï¼Œå¯èƒ½åœ¨è§£å†³è¿™äº›ä»»åŠ¡æ—¶æ›´åŠ é«˜æ•ˆã€‚è¿™é‡Œè¿˜æœ‰æ›´å¤šå†…å®¹ï¼Œæ¯•ç«Ÿæˆ‘ä»¬æ‰åˆšåˆšå¼€å§‹æ¢ç´¢ï¼
- en: 'ğŸ”®My hopeful predictions for 2024 are that we will:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”® æˆ‘å¯¹2024å¹´çš„å¸Œæœ›é¢„æµ‹æ˜¯ï¼Œæˆ‘ä»¬å°†ï¼š
- en: 1ï¸âƒ£ see many more diverse tools from algebraic and differential topology applied
    to graphs and combinatorial complexes,
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ çœ‹åˆ°æ›´å¤šæ¥è‡ªä»£æ•°å’Œå¾®åˆ†æ‹“æ‰‘çš„å¤šæ ·åŒ–å·¥å…·åº”ç”¨äºå›¾å’Œç»„åˆå¤å½¢ï¼Œ
- en: 2ï¸âƒ£ better understand message passing on higher-order input data, and
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ æ›´å¥½åœ°ç†è§£é«˜é˜¶è¾“å…¥æ•°æ®ä¸Šçš„ä¿¡æ¯ä¼ é€’ï¼Œå¹¶ä¸”
- en: 3ï¸âƒ£ finally obtain better parallel algorithms for persistent homology to truly
    unleash its power in a deep learning setting. A [recent paper on spectral sequences](https://link.springer.com/article/10.1007/s00454-023-00549-2)
    by Torras-Casas reports some very exciting results that show the great prospects
    of this technique.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 3ï¸âƒ£ æœ€ç»ˆè·å¾—æ›´å¥½çš„å¹¶è¡Œç®—æ³•ï¼Œç”¨äºæŒä¹…åŒè°ƒï¼ŒçœŸæ­£é‡Šæ”¾å…¶åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„æ½œåŠ›ã€‚[Torras-Casasæœ€è¿‘å…³äºè°±åºåˆ—çš„è®ºæ–‡](https://link.springer.com/article/10.1007/s00454-023-00549-2)æŠ¥å‘Šäº†ä¸€äº›éå¸¸ä»¤äººå…´å¥‹çš„ç»“æœï¼Œå±•ç¤ºäº†è¯¥æŠ€æœ¯çš„å·¨å¤§å‰æ™¯ã€‚
- en: '**Geometric Algebras**'
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**å‡ ä½•ä»£æ•°**'
- en: '*Johannes Brandstetter (JKU Linz) and David Ruhe (U Amsterdam)*'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '*çº¦ç¿°å†…æ–¯Â·å¸ƒå…°å¾·æ–¯ç‰¹ç‰¹ï¼ˆJKU Linzï¼‰å’Œå¤§å«Â·é²èµ«ï¼ˆU Amsterdamï¼‰*'
- en: â€œIn 2023, we saw the subfield of deep learning on geometric algebras (also known
    as **Clifford algebras**) take off. Previously, neural network layers formulated
    as operations on Clifford algebra *multivectors* were introduced by [Brandstetter
    et al.](https://arxiv.org/abs/2209.04934) This year, the â€˜geometricâ€™ in â€˜geometric
    algebraâ€™ was clearly put into action.â€ â€” Johannes Brandstetter (JKU Linz) and
    David Ruhe (U Amsterdam)
  id: totrans-164
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œåœ¨2023å¹´ï¼Œæˆ‘ä»¬è§è¯äº†å‡ ä½•ä»£æ•°ï¼ˆä¹Ÿå«**Cliffordä»£æ•°**ï¼‰æ·±åº¦å­¦ä¹ å­é¢†åŸŸçš„è“¬å‹ƒå‘å±•ã€‚ä¹‹å‰ï¼Œç¥ç»ç½‘ç»œå±‚é€šè¿‡Cliffordä»£æ•°*å¤šå‘é‡*çš„è¿ç®—å½¢å¼è¢«æå‡ºï¼Œç”±[Brandstetter
    et al.](https://arxiv.org/abs/2209.04934)ä»‹ç»ã€‚ä»Šå¹´ï¼Œâ€˜å‡ ä½•â€™è¿™ä¸€æ¦‚å¿µåœ¨â€˜å‡ ä½•ä»£æ•°â€™ä¸­å¾—åˆ°äº†æ˜ç¡®çš„åº”ç”¨ã€‚â€â€”â€”Johannes
    Brandstetterï¼ˆJKU Linzï¼‰å’ŒDavid Ruheï¼ˆU Amsterdamï¼‰
- en: â¡ï¸ First, [Ruhe et al.](https://arxiv.org/abs/2302.06594) applied the quintessence
    of modern (plane-based) geometric algebra by introducing **Geometric Clifford
    Algebra Networks (GCAN)**, neural network templates that model symmetry transformations
    described by various geometric algebras. We saw an intriguing application thereof
    by [Pepe et al.](https://openaccess.thecvf.com/content/WACV2024/papers/Pepe_CGAPoseNetGCAN_A_Geometric_Clifford_Algebra_Network_for_Geometry-Aware_Camera_Pose_WACV_2024_paper.pdf)
    in **CGAPoseNet**, building a geometry-aware pipeline for camera pose regression.
    Next, [Ruhe et al.](https://arxiv.org/abs/2305.11141) introduced **Clifford Group
    Equivariant Neural Networks (CGENN)**, building steerable O(n)- and E(n)-equivariant
    (graph) neural networks of any dimension via the Clifford group. [Pepe et al.](https://openreview.net/forum?id=JNfpsiGS5E)
    apply CGENNs to a Protein Structure Prediction (PSP) pipeline, increasing prediction
    accuracies by up to 2.1%.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ é¦–å…ˆï¼Œ[Ruhe et al.](https://arxiv.org/abs/2302.06594)é€šè¿‡å¼•å…¥**å‡ ä½•Cliffordä»£æ•°ç½‘ç»œï¼ˆGCANï¼‰**ï¼Œåº”ç”¨äº†ç°ä»£ï¼ˆåŸºäºå¹³é¢çš„ï¼‰å‡ ä½•ä»£æ•°çš„ç²¾é«“ï¼Œè¿™äº›ç¥ç»ç½‘ç»œæ¨¡æ¿æ¨¡æ‹Ÿç”±å„ç§å‡ ä½•ä»£æ•°æè¿°çš„å¯¹ç§°å˜æ¢ã€‚æˆ‘ä»¬çœ‹åˆ°ç”±[Pepe
    et al.](https://openaccess.thecvf.com/content/WACV2024/papers/Pepe_CGAPoseNetGCAN_A_Geometric_Clifford_Algebra_Network_for_Geometry-Aware_Camera_Pose_WACV_2024_paper.pdf)åœ¨**CGAPoseNet**ä¸­åº”ç”¨è¿™ä¸€æ–¹æ³•ï¼Œæ„å»ºäº†ä¸€ä¸ªå‡ ä½•æ„ŸçŸ¥çš„ç›¸æœºå§¿æ€å›å½’ç®¡é“ã€‚æ¥ä¸‹æ¥ï¼Œ[Ruhe
    et al.](https://arxiv.org/abs/2305.11141)å¼•å…¥äº†**Cliffordç¾¤ç­‰å˜ç¥ç»ç½‘ç»œï¼ˆCGENNï¼‰**ï¼Œé€šè¿‡Cliffordç¾¤æ„å»ºå¯è°ƒçš„O(n)-å’ŒE(n)-ç­‰å˜ï¼ˆå›¾ï¼‰ç¥ç»ç½‘ç»œï¼Œé€‚ç”¨äºä»»æ„ç»´åº¦ã€‚[Pepe
    et al.](https://openreview.net/forum?id=JNfpsiGS5E)å°†CGENNsåº”ç”¨äºè›‹ç™½è´¨ç»“æ„é¢„æµ‹ï¼ˆPSPï¼‰ç®¡é“ï¼Œå°†é¢„æµ‹ç²¾åº¦æé«˜äº†æœ€å¤š2.1%ã€‚
- en: '![](../Images/7a7c1ca9882edc21bb8fa7dec09ecdd4.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a7c1ca9882edc21bb8fa7dec09ecdd4.png)'
- en: 'CGENNs (represented with Ï•) are able to operate on multivectors (elements of
    the Clifford algebra) in an O(n)- or E(n)-equivariant way. Specifically, when
    an action Ï(w) of the Clifford group, representing an orthogonal transformation
    such as a rotation, is applied to the data, the modelâ€™s representations corotate.
    Multivectors can be decomposed into scalar, vector, bivector, trivector, and even
    higher-order components. These elements can represent geometric quantities such
    as (oriented) areas or volumes. The action Ï(w) is designed to respect these structures
    when acting on them. Source: [Ruhe et al.](https://arxiv.org/abs/2305.11141)'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: CGENNsï¼ˆç”¨Ï•è¡¨ç¤ºï¼‰èƒ½å¤Ÿä»¥O(n)-æˆ–E(n)-ç­‰å˜æ–¹å¼ä½œç”¨äºå¤šå‘é‡ï¼ˆCliffordä»£æ•°çš„å…ƒç´ ï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œå½“Cliffordç¾¤çš„ä½œç”¨Ï(w)ï¼Œä»£è¡¨åƒæ—‹è½¬è¿™æ ·çš„æ­£äº¤å˜æ¢ï¼Œä½œç”¨äºæ•°æ®æ—¶ï¼Œæ¨¡å‹çš„è¡¨ç¤ºä¼šè¿›è¡Œå…±åŒæ—‹è½¬ã€‚å¤šå‘é‡å¯ä»¥åˆ†è§£ä¸ºæ ‡é‡ã€å‘é‡ã€åŒå‘é‡ã€ä¸‰å‘é‡ç”šè‡³æ›´é«˜é˜¶çš„åˆ†é‡ã€‚è¿™äº›å…ƒç´ å¯ä»¥è¡¨ç¤ºå‡ ä½•é‡ï¼Œå¦‚ï¼ˆå®šå‘çš„ï¼‰é¢ç§¯æˆ–ä½“ç§¯ã€‚ä½œç”¨Ï(w)çš„è®¾è®¡æ˜¯ä¸ºäº†åœ¨ä½œç”¨äºè¿™äº›ç»“æ„æ—¶ä¿æŒè¿™äº›ç»“æ„çš„å®Œæ•´æ€§ã€‚æ¥æºï¼š[Ruhe
    et al.](https://arxiv.org/abs/2305.11141)
- en: â¡ï¸ Coincidently, [Brehmer et al.](https://arxiv.org/abs/2305.18415) formulated
    **Geometric Algebra Transformer(GATr)**, a scalable Transformer architecture that
    harnesses the benefits of representations provided by the projective geometric
    algebra and the scalability of Transformers to build E(3)-equivariant architectures.
    The GATr architecture was extended to other algebras by [Haan et al.](https://arxiv.org/abs/2311.04744)
    who also examine which flavor of geometric algebra is best suited for your E(3)-equivariant
    machine learning problem.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ æ°å·§ï¼Œ[Brehmer et al.](https://arxiv.org/abs/2305.18415)æå‡ºäº†**å‡ ä½•ä»£æ•°å˜æ¢å™¨ï¼ˆGATrï¼‰**ï¼Œä¸€ç§å¯æ‰©å±•çš„Transformeræ¶æ„ï¼Œåˆ©ç”¨æŠ•å½±å‡ ä½•ä»£æ•°æä¾›çš„è¡¨ç¤ºä¼˜åŠ¿å’ŒTransformerçš„å¯æ‰©å±•æ€§ï¼Œæ„å»ºE(3)-ç­‰å˜æ¶æ„ã€‚GATræ¶æ„ç”±[Haan
    et al.](https://arxiv.org/abs/2311.04744)æ‰©å±•åˆ°å…¶ä»–ä»£æ•°ï¼Œå¹¶ä¸”è¿˜æ¢è®¨äº†å“ªç§å‡ ä½•ä»£æ•°æœ€é€‚åˆæ‚¨çš„E(3)-ç­‰å˜æœºå™¨å­¦ä¹ é—®é¢˜ã€‚
- en: '![](../Images/4c74ddc1a3b15b01636ed352b0b3b5b0.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c74ddc1a3b15b01636ed352b0b3b5b0.png)'
- en: 'Overview of the GATr architecture. Boxes with solid lines are learnable components,
    those with dashed lines are fixed. Source: [Brehmer et al.](https://arxiv.org/abs/2305.18415)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: GATræ¶æ„æ¦‚è¿°ã€‚å¸¦å®çº¿çš„æ¡†è¡¨ç¤ºå¯å­¦ä¹ ç»„ä»¶ï¼Œå¸¦è™šçº¿çš„æ¡†è¡¨ç¤ºå›ºå®šç»„ä»¶ã€‚æ¥æºï¼š[Brehmer et al.](https://arxiv.org/abs/2305.18415)
- en: ğŸ”® In 2024, we can expect exciting new applications from these advancements.
    Some examples include the following.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”® åœ¨2024å¹´ï¼Œæˆ‘ä»¬å¯ä»¥æœŸå¾…è¿™äº›è¿›å±•å¸¦æ¥ä»¤äººå…´å¥‹çš„æ–°åº”ç”¨ã€‚ä¸€äº›ä¾‹å­åŒ…æ‹¬ä»¥ä¸‹å†…å®¹ã€‚
- en: 1ï¸âƒ£ We can expect explorations of their applicability to molecular data, drug
    design, neural physics emulations, crystals, etc. Other geometry-aware applications
    include 3D rendering, pose estimations, and planning for, e.g., robot arms.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ æˆ‘ä»¬å¯ä»¥æœŸå¾…æ¢ç´¢å…¶åœ¨åˆ†å­æ•°æ®ã€è¯ç‰©è®¾è®¡ã€ç¥ç»ç‰©ç†ä»¿çœŸã€æ™¶ä½“ç­‰é¢†åŸŸçš„åº”ç”¨ã€‚å…¶ä»–å‡ ä½•æ„ŸçŸ¥çš„åº”ç”¨åŒ…æ‹¬3Dæ¸²æŸ“ã€å§¿æ€ä¼°è®¡ï¼Œä»¥åŠä¾‹å¦‚æœºå™¨äººæ‰‹è‡‚çš„è§„åˆ’ã€‚
- en: 2ï¸âƒ£ We can expect the extension of geometric algebra-based networks to other
    neural network architectures, such as convolutional neural networks.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ æˆ‘ä»¬å¯ä»¥æœŸå¾…å°†åŸºäºå‡ ä½•ä»£æ•°çš„ç½‘ç»œæ‰©å±•åˆ°å…¶ä»–ç¥ç»ç½‘ç»œæ¶æ„ä¸­ï¼Œå¦‚å·ç§¯ç¥ç»ç½‘ç»œã€‚
- en: 3ï¸âƒ£ Next, the generality of the CGENN allows for explorations in other dimensions,
    e.g., 2D, but also in settings where data of various dimensionalities should be
    processed together. Further, they enable non-Euclidean geometries, which have
    several use cases in relativistic physics.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 3ï¸âƒ£ æ¥ä¸‹æ¥ï¼ŒCGENNçš„é€šç”¨æ€§ä½¿å¾—å¯ä»¥åœ¨å…¶ä»–ç»´åº¦ä¸­è¿›è¡Œæ¢ç´¢ï¼Œä¾‹å¦‚2Dï¼Œä½†ä¹Ÿå¯ä»¥åœ¨éœ€è¦å¤„ç†å¤šç»´æ•°æ®çš„ç¯å¢ƒä¸­è¿›è¡Œæ¢ç´¢ã€‚æ­¤å¤–ï¼Œå®ƒä»¬è¿˜æ”¯æŒéæ¬§å‡ é‡Œå¾—å‡ ä½•ï¼Œè¿™åœ¨ç›¸å¯¹è®ºç‰©ç†å­¦ä¸­æœ‰å¤šä¸ªåº”ç”¨åœºæ™¯ã€‚
- en: 4ï¸âƒ£ Finally, GATr and CGENN can be extended and applied to projective, conformal,
    hyperbolic, or elliptic geometries.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 4ï¸âƒ£ æœ€åï¼ŒGATrå’ŒCGENNå¯ä»¥æ‰©å±•å¹¶åº”ç”¨äºæŠ•å½±å‡ ä½•ã€ä¿è§’å‡ ä½•ã€åŒæ›²å‡ ä½•æˆ–æ¤­åœ†å‡ ä½•ã€‚
- en: '**PDEs**'
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**åå¾®åˆ†æ–¹ç¨‹ï¼ˆPDEsï¼‰**'
- en: '*Johannes Brandstetter (JKU Linz)*'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '*Johannes Brandstetterï¼ˆJKU Linzï¼‰*'
- en: Concerning the landscape of neural PDE modelling, what topics have surfaced
    or gathered momentum through 2023?
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºç¥ç»åå¾®åˆ†æ–¹ç¨‹å»ºæ¨¡çš„ç°çŠ¶ï¼Œå“ªäº›è¯é¢˜åœ¨2023å¹´æœ‰æ‰€æµ®ç°æˆ–è·å¾—äº†æ›´å¤šå…³æ³¨ï¼Ÿ
- en: 1ï¸âƒ£ To begin, there is a noticeable trend towards modelling PDEs on and within
    intricate geometries, necessitating a mesh-based discretization of space. This
    aligns with the overarching goal to address increasingly realistic real world
    problems. For example, [Li et al](https://arxiv.org/abs/2309.00583). have introduced
    **Geometry-Informed Neural Operator (GINO)** for large-scale 3D PDEs.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ é¦–å…ˆï¼Œæœ‰ä¸€ä¸ªæ˜æ˜¾çš„è¶‹åŠ¿æ˜¯ï¼Œå¼€å§‹åœ¨å¤æ‚å‡ ä½•ä¸Šä»¥åŠå‡ ä½•å†…éƒ¨å»ºæ¨¡PDEsï¼Œè¿™éœ€è¦åŸºäºç½‘æ ¼çš„ç©ºé—´ç¦»æ•£åŒ–ã€‚è¿™ä¸è§£å†³è¶Šæ¥è¶Šé€¼çœŸçš„ç°å®ä¸–ç•Œé—®é¢˜çš„æ€»ä½“ç›®æ ‡ç›¸ä¸€è‡´ã€‚ä¾‹å¦‚ï¼Œ[Li
    et al](https://arxiv.org/abs/2309.00583)æå‡ºäº†ç”¨äºå¤§è§„æ¨¡3D PDEçš„**å‡ ä½•æ„ŸçŸ¥ç¥ç»ç®—å­ï¼ˆGINOï¼‰**ã€‚
- en: 2ï¸âƒ£ Secondly, the development of neural network surrogates for Lagrangian-based
    simulations is becoming increasingly intriguing. The Lagrangian discretization
    of space uses finite material points which are tracked as fluid parcels through
    space and time. The most prominent Lagrangian discretization scheme is called
    smoothed particle hydrodynamics (SPH), which is the numerical baseline in the
    **LagrangeBench** benchmark dataset provided by [Toshev et al.](https://arxiv.org/abs/2309.16342)
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ å…¶æ¬¡ï¼ŒåŸºäºæ‹‰æ ¼æœ—æ—¥æ¨¡æ‹Ÿçš„ç¥ç»ç½‘ç»œä»£ç†çš„å‘å±•å˜å¾—è¶Šæ¥è¶Šå¼•äººæ³¨ç›®ã€‚æ‹‰æ ¼æœ—æ—¥ç©ºé—´ç¦»æ•£åŒ–ä½¿ç”¨æœ‰é™çš„ç‰©è´¨ç‚¹ï¼Œè¿™äº›ç‰©è´¨ç‚¹ä½œä¸ºæµä½“å›¢å—åœ¨æ—¶ç©ºä¸­è¿›è¡Œè¿½è¸ªã€‚æœ€è‘—åçš„æ‹‰æ ¼æœ—æ—¥ç¦»æ•£åŒ–æ–¹æ¡ˆæ˜¯å¹³æ»‘ç²’å­æµä½“åŠ¨åŠ›å­¦ï¼ˆSPHï¼‰ï¼Œè¿™æ˜¯ç”±[Toshev
    et al.](https://arxiv.org/abs/2309.16342)æä¾›çš„**LagrangeBench**åŸºå‡†æ•°æ®é›†ä¸­ä½¿ç”¨çš„æ•°å€¼åŸºçº¿ã€‚
- en: '![](../Images/4c431332c56925e208b2421a66e4fadc.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c431332c56925e208b2421a66e4fadc.png)'
- en: 'Time snapshots of our datasets, at the initial time (top), 40% (middle), and
    95% (bottom) of the trajectory. Color temperature represents velocity magnitude.
    (a) Taylor Green vortex (2D and 3D), (b) Reverse Poiseuille flow (2D and 3D),
    Â© Lid-driven cavity (2D and 3D), (d) Dam break (2D). Source: LagrangeBench by
    [Toshev et al.](https://arxiv.org/abs/2309.16342)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ•°æ®é›†çš„æ—¶é—´å¿«ç…§ï¼Œåœ¨åˆå§‹æ—¶åˆ»ï¼ˆé¡¶éƒ¨ï¼‰ã€40%ï¼ˆä¸­éƒ¨ï¼‰å’Œ95%ï¼ˆåº•éƒ¨ï¼‰çš„è½¨è¿¹ä¸Šã€‚é¢œè‰²æ¸©åº¦è¡¨ç¤ºé€Ÿåº¦å¤§å°ã€‚ (a) æ³°å‹’-æ ¼æ—æ¶¡æ—‹ï¼ˆ2D å’Œ 3Dï¼‰ï¼Œ(b)
    åå‘æ³Šæ¾æµï¼ˆ2D å’Œ 3Dï¼‰ï¼Œ(c) é©±åŠ¨è…”ä½“ï¼ˆ2D å’Œ 3Dï¼‰ï¼Œ(d) æ°´åæºƒåï¼ˆ2Dï¼‰ã€‚æ¥æºï¼šLagrangeBenchï¼Œç”±[Toshev et al.](https://arxiv.org/abs/2309.16342)æä¾›
- en: 3ï¸âƒ£ Thirdly, diffusion-based modelling is also not stopping for PDEs. We roughly
    see two directions. The first direction recasts the iterative nature of the diffusion
    process into a refinement of a candidate state initialised from noise and conditioned
    on previous timesteps. This iterative refinement was introduced in **PDE-Refiner**
    ([Lippe et al.](https://arxiv.org/abs/2308.05732)) and a variant thereof was already
    applied in **GenCast** ([Price et al.](https://arxiv.org/abs/2312.15796)). The
    second direction exerts the probabilistic nature of diffusion models to model
    chaotic phenomena such as 3D turbulence. Examples of this can be found in **Turbulent
    Flow Simulation** ([Kohl et al.](https://arxiv.org/abs/2309.01745)) and in **From
    Zero To Turbulence** ([Lienen et al.](https://arxiv.org/abs/2306.01776)). Especially
    for 3D turbulence, there are a lot of interesting things that will happen in the
    near future.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 3ï¸âƒ£ å…¶æ¬¡ï¼ŒåŸºäºæ‰©æ•£çš„å»ºæ¨¡åœ¨åå¾®åˆ†æ–¹ç¨‹ï¼ˆPDEï¼‰é¢†åŸŸä¹Ÿæ²¡æœ‰åœä¸‹è„šæ­¥ã€‚æˆ‘ä»¬å¤§è‡´çœ‹åˆ°ä¸¤ä¸ªæ–¹å‘ã€‚ç¬¬ä¸€ä¸ªæ–¹å‘å°†æ‰©æ•£è¿‡ç¨‹çš„è¿­ä»£æ€§è´¨è½¬åŒ–ä¸ºå¯¹ä»å™ªå£°åˆå§‹åŒ–å¹¶æ ¹æ®å‰ä¸€æ­¥éª¤è¿›è¡Œæ¡ä»¶åŒ–çš„å€™é€‰çŠ¶æ€çš„ç²¾ç‚¼ã€‚è¿™ç§è¿­ä»£ç²¾ç‚¼åœ¨**PDE-Refiner**ï¼ˆ[Lippe
    ç­‰äºº](https://arxiv.org/abs/2308.05732)ï¼‰ä¸­æœ‰æ‰€ä»‹ç»ï¼Œè€Œå…¶å˜ä½“å·²ç»åœ¨**GenCast**ï¼ˆ[Price ç­‰äºº](https://arxiv.org/abs/2312.15796)ï¼‰ä¸­å¾—åˆ°åº”ç”¨ã€‚ç¬¬äºŒä¸ªæ–¹å‘åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ¦‚ç‡æ€§è´¨æ¥æ¨¡æ‹Ÿè¯¸å¦‚ä¸‰ç»´æ¹æµç­‰æ··æ²Œç°è±¡ã€‚å…³äºè¿™ä¸€ç‚¹çš„ä¾‹å­å¯ä»¥åœ¨**æ¹æµæµåŠ¨ä»¿çœŸ**ï¼ˆ[Kohl
    ç­‰äºº](https://arxiv.org/abs/2309.01745)ï¼‰å’Œ**ä»é›¶åˆ°æ¹æµ**ï¼ˆ[Lienen ç­‰äºº](https://arxiv.org/abs/2306.01776)ï¼‰ä¸­æ‰¾åˆ°ã€‚ç‰¹åˆ«æ˜¯å¯¹äºä¸‰ç»´æ¹æµï¼Œæœªæ¥æœ‰å¾ˆå¤šæœ‰è¶£çš„äº‹æƒ…å°†ä¼šå‘ç”Ÿã€‚
- en: â€œWeather modelling has become a great success story over the last months. There
    is potentially much more exciting stuff to come, especially regarding weather
    forecasting directly from observational data or when building weather foundation
    models.â€ â€” Johannes Brandstetter (JKU Linz)
  id: totrans-184
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œå¤©æ°”å»ºæ¨¡åœ¨è¿‡å»å‡ ä¸ªæœˆå–å¾—äº†å·¨å¤§æˆåŠŸã€‚æœªæ¥å¯èƒ½ä¼šæœ‰æ›´å¤šä»¤äººæ¿€åŠ¨çš„è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨ç›´æ¥ä»è§‚æµ‹æ•°æ®è¿›è¡Œå¤©æ°”é¢„æŠ¥æˆ–æ„å»ºå¤©æ°”åŸºç¡€æ¨¡å‹æ–¹é¢ã€‚â€ â€” Johannes
    Brandstetterï¼ˆJKU Linzï¼‰
- en: 'ğŸ”® **What to expect in 2024**:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”® **2024å¹´çš„é¢„æµ‹**ï¼š
- en: 1ï¸âƒ£ More work regarding 3D turbulence modelling.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ æ›´å¤šå…³äºä¸‰ç»´æ¹æµå»ºæ¨¡çš„ç ”ç©¶ã€‚
- en: 2ï¸âƒ£ Multi-modality aspects of PDEs might emerge. This could include combining
    different PDEs, different resolutions, or different discretization schemes. We
    are already seeing a glimpse thereof in e.g. [Multiple Physics Pretraining for
    Physical Surrogate Models](https://arxiv.org/abs/2310.02994) by McCabe et al.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ åå¾®åˆ†æ–¹ç¨‹çš„å¤šæ¨¡æ€æ–¹é¢å¯èƒ½ä¼šå‡ºç°ã€‚è¿™å¯èƒ½åŒ…æ‹¬ç»“åˆä¸åŒçš„åå¾®åˆ†æ–¹ç¨‹ã€ä¸åŒçš„åˆ†è¾¨ç‡æˆ–ä¸åŒçš„ç¦»æ•£åŒ–æ–¹æ¡ˆã€‚æˆ‘ä»¬å·²ç»åœ¨ä¾‹å¦‚[å¤šç‰©ç†é¢„è®­ç»ƒç‰©ç†ä»£ç†æ¨¡å‹](https://arxiv.org/abs/2310.02994)ä¸­çœ‹åˆ°äº†ä¸€äº›ç›¸å…³çš„è¿¹è±¡ï¼Œè¿™æ˜¯McCabeç­‰äººçš„ç ”ç©¶ã€‚
- en: '**Predictions from the 2023 post**'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '**2023å¹´é¢„æµ‹**'
- en: (1) Neural PDEs and their applications are likely to expand to more physics-related
    AI4Science subfields; computational fluid dynamics (CFD) will potentially be influenced
    by GNN.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: (1) ç¥ç»ç½‘ç»œåå¾®åˆ†æ–¹ç¨‹åŠå…¶åº”ç”¨å¯èƒ½ä¼šæ‰©å±•åˆ°æ›´å¤šä¸ç‰©ç†ç›¸å…³çš„AI4Scienceå­é¢†åŸŸï¼›è®¡ç®—æµä½“åŠ›å­¦ï¼ˆCFDï¼‰å¯èƒ½ä¼šå—åˆ°å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰çš„å½±å“ã€‚
- en: âœ… We are seeing 3D turbulence modelling, geometry-aware neural operators, particle-based
    neural surrogates, and a huge impact in e.g. weather forecasting.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: âœ… æˆ‘ä»¬å·²ç»çœ‹åˆ°äº†ä¸‰ç»´æ¹æµå»ºæ¨¡ã€å‡ ä½•æ„ŸçŸ¥ç¥ç»ç®—å­ã€åŸºäºç²’å­çš„ç¥ç»ä»£ç†æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨ä¾‹å¦‚å¤©æ°”é¢„æŠ¥ç­‰é¢†åŸŸäº§ç”Ÿäº†å·¨å¤§å½±å“ã€‚
- en: (2) GNN based surrogates might augment/replace traditional well-tried techniques.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: (2) åŸºäºå›¾ç¥ç»ç½‘ç»œçš„ä»£ç†æ¨¡å‹å¯èƒ½ä¼šå¢å¼ºæˆ–å–ä»£ä¼ ç»Ÿçš„æˆç†ŸæŠ€æœ¯ã€‚
- en: âœ… Weather forecasting has become a great success story. Neural network based
    weather forecasts overtake traditional forecasts (medium range+local forecasts),
    e.g., [GraphCast](https://www.science.org/doi/full/10.1126/science.adi2336) by
    Lam et al. and [MetNet-3](https://arxiv.org/abs/2306.06079) by Andrychowicz et
    al.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: âœ… å¤©æ°”é¢„æŠ¥å·²ç»æˆä¸ºä¸€ä¸ªå·¨å¤§çš„æˆåŠŸæ¡ˆä¾‹ã€‚åŸºäºç¥ç»ç½‘ç»œçš„å¤©æ°”é¢„æŠ¥è¶…è¿‡äº†ä¼ ç»Ÿçš„é¢„æŠ¥ï¼ˆä¸­æœŸ+å±€éƒ¨é¢„æŠ¥ï¼‰ï¼Œä¾‹å¦‚ï¼Œ[GraphCast](https://www.science.org/doi/full/10.1126/science.adi2336)ï¼ˆLam
    ç­‰äººï¼‰å’Œ[MetNet-3](https://arxiv.org/abs/2306.06079)ï¼ˆAndrychowicz ç­‰äººï¼‰ã€‚
- en: Robustness and Explainability
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¨³å¥æ€§å’Œå¯è§£é‡Šæ€§
- en: '*Kexin Huang (Stanford)*'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '*é»„å…‹æ¬£ï¼ˆæ–¯å¦ç¦å¤§å­¦ï¼‰*'
- en: â€œAs GNNs are getting deployed in various domains, their reliability and robustness
    have become increasingly important, especially in safety-critical applications
    (e.g. scientific discovery) where the cost of errors is significant.â€ â€” Kexin
    Huang (Stanford)
  id: totrans-195
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œéšç€å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰åœ¨å„ä¸ªé¢†åŸŸçš„éƒ¨ç½²ï¼Œå…¶å¯é æ€§å’Œç¨³å¥æ€§å˜å¾—è¶Šæ¥è¶Šé‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨å®‰å…¨å…³é”®çš„åº”ç”¨ä¸­ï¼ˆä¾‹å¦‚ç§‘å­¦å‘ç°ï¼‰ï¼Œå› ä¸ºé”™è¯¯çš„ä»£ä»·å¯èƒ½éå¸¸é«˜ã€‚â€ â€”
    é»„å…‹æ¬£ï¼ˆæ–¯å¦ç¦å¤§å­¦ï¼‰
- en: 1ï¸âƒ£ When discussing the reliability of GNNs, a key criterion is **uncertainty
    quantification** â€” quantifying how much the model knows about the prediction.
    There are numerous works on estimating and calibrating uncertainty, also designed
    specifically for GNNs (e.g. [GATS](https://proceedings.neurips.cc/paper_files/paper/2022/hash/5975754c7650dfee0682e06e1fec0522-Abstract-Conference.html)).
    However, they fall short of achieving pre-defined target coverage (i.e. % of points
    falling into the prediction set) both theoretically and empirically. I want to
    emphasize that this notion of having a coverage guarantee is **critical** especially
    in ML deployment for scientific discovery since practitioners often trust a model
    with statistical guarantees.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ åœ¨è®¨è®ºå›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰çš„å¯é æ€§æ—¶ï¼Œä¸€ä¸ªå…³é”®æ ‡å‡†æ˜¯**ä¸ç¡®å®šæ€§é‡åŒ–**â€”â€”é‡åŒ–æ¨¡å‹å¯¹é¢„æµ‹çš„äº†è§£ç¨‹åº¦ã€‚å…³äºä¼°è®¡å’Œæ ¡å‡†ä¸ç¡®å®šæ€§çš„ç ”ç©¶å¾ˆå¤šï¼Œå…¶ä¸­ä¹Ÿæœ‰ä¸“é—¨é’ˆå¯¹GNNçš„å·¥ä½œï¼ˆä¾‹å¦‚ï¼Œ[GATS](https://proceedings.neurips.cc/paper_files/paper/2022/hash/5975754c7650dfee0682e06e1fec0522-Abstract-Conference.html)ï¼‰ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨ç†è®ºå’Œå®è¯ä¸Šéƒ½æœªèƒ½è¾¾åˆ°é¢„å®šä¹‰çš„ç›®æ ‡è¦†ç›–ç‡ï¼ˆå³é¢„æµ‹é›†å†…çš„ç‚¹æ•°ç™¾åˆ†æ¯”ï¼‰ã€‚æˆ‘æƒ³å¼ºè°ƒçš„æ˜¯ï¼Œæ‹¥æœ‰è¦†ç›–ä¿è¯è¿™ä¸€æ¦‚å¿µ**è‡³å…³é‡è¦**ï¼Œå°¤å…¶æ˜¯åœ¨ç§‘å­¦å‘ç°çš„æœºå™¨å­¦ä¹ éƒ¨ç½²ä¸­ï¼Œå› ä¸ºå®è·µè€…é€šå¸¸ä¿¡ä»»å…·æœ‰ç»Ÿè®¡ä¿è¯çš„æ¨¡å‹ã€‚
- en: '**2ï¸âƒ£ Conformal prediction** is an exciting direction in statistics where it
    has finite sample coverage guarantees and has been applied in many domains such
    as [vision and NLP](https://arxiv.org/abs/2107.07511). But it is unclear if it
    can be used in graphs theoretically since it is not obvious if the exchangeability
    assumption holds for graph settings. In 2023, we see conformal prediction has
    been extended to graphs. Notably, [CF-GNN](https://arxiv.org/abs/2305.14535) and
    [DAPS](https://proceedings.mlr.press/v202/h-zargarbashi23a/h-zargarbashi23a.pdf)
    have derived theoretical conditions for conformal validity in transductive node-level
    prediction setting and also developed methods to reduce the prediction set size
    for efficient downstream usage. More recently, we have also seen conformal prediction
    extensions to [link prediction](https://arxiv.org/pdf/2306.14693v1.pdf), [non-uniform
    split](https://arxiv.org/abs/2306.07252), [edge exchangeability](https://openreview.net/forum?id=homn1jOKI5),
    and also adaptations for settings where exchangeability does not hold (such as
    [inductive setting](https://arxiv.org/abs/2211.14555)).'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '**2ï¸âƒ£ ç¬¦åˆæ€§é¢„æµ‹**æ˜¯ç»Ÿè®¡å­¦ä¸­çš„ä¸€ä¸ªä»¤äººå…´å¥‹çš„æ–¹å‘ï¼Œå®ƒå…·æœ‰æœ‰é™æ ·æœ¬è¦†ç›–ä¿è¯ï¼Œä¸”å·²åº”ç”¨äºè®¸å¤šé¢†åŸŸï¼Œå¦‚[è§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†](https://arxiv.org/abs/2107.07511)ã€‚ä½†å°šä¸æ¸…æ¥šå®ƒæ˜¯å¦å¯ä»¥åœ¨å›¾ä¸­ç†è®ºä¸Šåº”ç”¨ï¼Œå› ä¸ºåœ¨å›¾çš„è®¾ç½®ä¸‹ï¼Œå¯äº¤æ¢æ€§å‡è®¾æ˜¯å¦æˆç«‹å¹¶ä¸æ˜¾è€Œæ˜“è§ã€‚2023å¹´ï¼Œæˆ‘ä»¬çœ‹åˆ°ç¬¦åˆæ€§é¢„æµ‹å·²æ‰©å±•åˆ°å›¾ç»“æ„æ•°æ®ã€‚ç‰¹åˆ«æ˜¯ï¼Œ[CF-GNN](https://arxiv.org/abs/2305.14535)å’Œ[DAPS](https://proceedings.mlr.press/v202/h-zargarbashi23a/h-zargarbashi23a.pdf)å·²æ¨å¯¼å‡ºä¼ å¯¼èŠ‚ç‚¹çº§é¢„æµ‹è®¾ç½®ä¸­ç¬¦åˆæ€§æœ‰æ•ˆæ€§çš„ç†è®ºæ¡ä»¶ï¼Œå¹¶ä¸”è¿˜å¼€å‘äº†å‡å°‘é¢„æµ‹é›†å¤§å°çš„æ–¹æ³•ï¼Œä»¥ä¾¿é«˜æ•ˆåœ°è¿›è¡Œä¸‹æ¸¸ä½¿ç”¨ã€‚æœ€è¿‘ï¼Œæˆ‘ä»¬è¿˜çœ‹åˆ°ç¬¦åˆæ€§é¢„æµ‹æ‰©å±•åˆ°[é“¾æ¥é¢„æµ‹](https://arxiv.org/pdf/2306.14693v1.pdf)ã€[éå‡åŒ€åˆ’åˆ†](https://arxiv.org/abs/2306.07252)ã€[è¾¹å¯äº¤æ¢æ€§](https://openreview.net/forum?id=homn1jOKI5)ï¼Œä»¥åŠé’ˆå¯¹ä¸æ»¡è¶³å¯äº¤æ¢æ€§çš„è®¾ç½®ï¼ˆå¦‚[å½’çº³è®¾ç½®](https://arxiv.org/abs/2211.14555)ï¼‰çš„é€‚åº”ã€‚'
- en: '![](../Images/9c51e4ae0619793413247af551445751.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c51e4ae0619793413247af551445751.png)'
- en: 'Conformal prediction for graph-structured data. (1) A base GNN model (GNN)
    that produces prediction scores Âµ for node i. (2) Conformal correction. Since
    the training step is not aware of the conformal calibration step, the size/length
    of prediction sets/intervals (i.e. efficiency) are not optimized. We use a topology-aware
    correction model that takes Âµ as the input node feature and aggregates information
    from its local subgraph to produce an updated prediction ÂµËœ. (3) Conformal prediction.
    We prove that in a transductive random split setting, graph exchangeability holds
    given permutation invariance. Thus, standard CP can be used to produce a prediction
    set/interval based on ÂµËœ that includes true label with pre-specified coverage
    rate 1-Î±. Source: [Huang et al.](https://arxiv.org/abs/2305.14535)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: é’ˆå¯¹å›¾ç»“æ„æ•°æ®çš„ç¬¦åˆæ€§é¢„æµ‹ã€‚ (1) ä¸€ä¸ªåŸºç¡€çš„GNNæ¨¡å‹ï¼ˆGNNï¼‰ï¼Œå®ƒä¸ºèŠ‚ç‚¹iç”Ÿæˆé¢„æµ‹å¾—åˆ†Âµã€‚ (2) ç¬¦åˆæ€§æ ¡æ­£ã€‚ç”±äºè®­ç»ƒæ­¥éª¤å¹¶ä¸è€ƒè™‘ç¬¦åˆæ€§æ ¡æ­£æ­¥éª¤ï¼Œå› æ­¤é¢„æµ‹é›†/åŒºé—´çš„å¤§å°/é•¿åº¦ï¼ˆå³æ•ˆç‡ï¼‰æ²¡æœ‰å¾—åˆ°ä¼˜åŒ–ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªæ‹“æ‰‘æ„ŸçŸ¥çš„æ ¡æ­£æ¨¡å‹ï¼Œå°†Âµä½œä¸ºè¾“å…¥èŠ‚ç‚¹ç‰¹å¾ï¼Œå¹¶æ±‡èšå…¶å±€éƒ¨å­å›¾çš„ä¿¡æ¯ï¼Œä»¥ç”Ÿæˆæ›´æ–°åçš„é¢„æµ‹ÂµËœã€‚
    (3) ç¬¦åˆæ€§é¢„æµ‹ã€‚æˆ‘ä»¬è¯æ˜ï¼Œåœ¨ä¸€ä¸ªä¼ å¯¼éšæœºåˆ’åˆ†çš„è®¾ç½®ä¸‹ï¼Œç»™å®šç½®æ¢ä¸å˜æ€§ï¼Œå›¾çš„å¯äº¤æ¢æ€§æ˜¯æˆç«‹çš„ã€‚å› æ­¤ï¼Œæ ‡å‡†çš„ç¬¦åˆæ€§é¢„æµ‹ï¼ˆCPï¼‰å¯ä»¥åŸºäºÂµËœç”Ÿæˆé¢„æµ‹é›†/åŒºé—´ï¼Œä¸”è¯¥é¢„æµ‹é›†åŒ…å«å…·æœ‰é¢„è®¾è¦†ç›–ç‡1-Î±çš„çœŸå®æ ‡ç­¾ã€‚æ¥æºï¼š[é»„ç­‰](https://arxiv.org/abs/2305.14535)
- en: ğŸ”® Looking ahead, we expect more extensions to cover a wide range of GNN deployment
    use cases. Overall, I think having statistical guarantees for GNNs is very nice
    because it enables the trust of practitioners to use GNN predictions.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”® å±•æœ›æœªæ¥ï¼Œæˆ‘ä»¬é¢„è®¡ä¼šæœ‰æ›´å¤šæ‰©å±•ï¼Œè¦†ç›–æ›´å¹¿æ³›çš„GNNéƒ¨ç½²ç”¨ä¾‹ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘è®¤ä¸ºä¸ºGNNæä¾›ç»Ÿè®¡ä¿è¯éå¸¸å¥½ï¼Œå› ä¸ºå®ƒä½¿å¾—å®è·µè€…èƒ½å¤Ÿä¿¡ä»»GNNçš„é¢„æµ‹ç»“æœã€‚
- en: Graph Transformers
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å›¾ç¥ç»ç½‘ç»œå˜æ¢å™¨ï¼ˆGraph Transformersï¼‰
- en: '*Chen Lin (Oxford)*'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '*é™ˆç³ï¼ˆç‰›æ´¥å¤§å­¦ï¼‰*'
- en: ğŸ’¡ In 2023, we have seen the continuation of the rise of Graph Transformers.
    It has become the **common GNN design**, e.g., in [GATr](https://arxiv.org/abs/2305.18415),
    the authors attribute its popularity to its *â€œfavorable scaling properties, expressiveness,
    trainability, and versatilityâ€*.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ 2023å¹´ï¼Œæˆ‘ä»¬è§è¯äº†å›¾ç¥ç»ç½‘ç»œå˜æ¢å™¨çš„æŒç»­å´›èµ·ã€‚å®ƒå·²ç»æˆä¸º**å¸¸è§çš„GNNè®¾è®¡**ï¼Œä¾‹å¦‚åœ¨[GATr](https://arxiv.org/abs/2305.18415)ä¸­ï¼Œä½œè€…å°†å…¶æµè¡Œå½’å› äºå…¶*â€œè‰¯å¥½çš„æ‰©å±•æ€§ã€è¡¨è¾¾èƒ½åŠ›ã€å¯è®­ç»ƒæ€§å’Œå¤šåŠŸèƒ½æ€§â€*ã€‚
- en: 1ï¸âƒ£ **Expressiveness of GTs.** As mentioned in the GNN Theory section, recent
    work from [Cai et al. (2023)](https://arxiv.org/abs/2301.11956) shows the equivalence
    between MPNNs with a Virtural Node and GTs under a *non-uniform setting.* This
    poses a question on how powerful are GTs and what is the source of their representation
    ability. [Zhang et al. (2023)](https://arxiv.org/abs/2301.09505) successfully
    combine a new powerful positional embedding (PE) to improve the expressiveness
    of their GTs, achieving expressivity over the biconnectivity problem. This gives
    evidence of the importance of PEs to the expressiveness of GTs. A recent submission
    [GPNN](https://openreview.net/pdf?id=JfjduOxrTY) provides a clearer view on the
    central role of the positional encoding. It has been shown that one can generalize
    the proof in [Zhang et al. (2023)](https://arxiv.org/abs/2301.09505) to show how
    GTsâ€™ expressiveness is decided by various positional encodings.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ **GTçš„è¡¨è¾¾èƒ½åŠ›ã€‚**æ­£å¦‚åœ¨GNNç†è®ºéƒ¨åˆ†æåˆ°çš„ï¼Œæœ€è¿‘[Caiç­‰äººï¼ˆ2023ï¼‰](https://arxiv.org/abs/2301.11956)çš„å·¥ä½œæ˜¾ç¤ºï¼Œå¸¦æœ‰è™šæ‹ŸèŠ‚ç‚¹çš„MPNNå’ŒGTåœ¨*éå‡åŒ€è®¾ç½®*ä¸‹æ˜¯ç­‰ä»·çš„ã€‚è¿™æå‡ºäº†ä¸€ä¸ªé—®é¢˜ï¼šGTåˆ°åº•æœ‰å¤šå¼ºå¤§ï¼Œå®ƒä»¬çš„è¡¨è¾¾èƒ½åŠ›æ¥æºäºå“ªé‡Œï¼Ÿ[Zhangç­‰äººï¼ˆ2023ï¼‰](https://arxiv.org/abs/2301.09505)æˆåŠŸåœ°å°†ä¸€ç§æ–°çš„å¼ºå¤§ä½ç½®åµŒå…¥ï¼ˆPEï¼‰ç»“åˆèµ·æ¥ï¼Œä»¥æé«˜GTçš„è¡¨è¾¾èƒ½åŠ›ï¼Œåœ¨åŒè¿é€šæ€§é—®é¢˜ä¸Šå–å¾—äº†æ›´å¥½çš„è¡¨ç°ã€‚è¿™ä¸ºPEå¯¹GTè¡¨è¾¾èƒ½åŠ›çš„é‡è¦æ€§æä¾›äº†è¯æ®ã€‚æœ€è¿‘çš„æäº¤[GPNN](https://openreview.net/pdf?id=JfjduOxrTY)æ›´æ¸…æ¥šåœ°å±•ç¤ºäº†ä½ç½®ç¼–ç åœ¨å…¶ä¸­çš„æ ¸å¿ƒä½œç”¨ã€‚å·²ç»è¯æ˜ï¼Œäººä»¬å¯ä»¥æ¨å¹¿[Zhangç­‰äººï¼ˆ2023ï¼‰](https://arxiv.org/abs/2301.09505)ä¸­çš„è¯æ˜ï¼Œå±•ç¤ºGTçš„è¡¨è¾¾èƒ½åŠ›æ˜¯å¦‚ä½•ç”±å„ç§ä½ç½®ç¼–ç å†³å®šçš„ã€‚
- en: '**2ï¸âƒ£** **Positional (Structural) Encoding.** Given the importance of PE/SE
    to GTs, now we turn to the design of those expressive features usually derived
    from existing graph invariants. In 2022, [GraphGPS](https://arxiv.org/abs/2205.12454)
    observed a huge empirical success by combining GTs with various (or even multiple)
    PE/SEs. In 2023, more powerful PE/SE is available.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**2ï¸âƒ£** **ä½ç½®ï¼ˆç»“æ„ï¼‰ç¼–ç ã€‚**é‰´äºPE/SEå¯¹å›¾ç¥ç»ç½‘ç»œï¼ˆGTsï¼‰çš„é‡è¦æ€§ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†è®¨è®ºé‚£äº›é€šå¸¸æ¥æºäºç°æœ‰å›¾ä¸å˜é‡çš„å¯Œæœ‰è¡¨ç°åŠ›çš„ç‰¹å¾çš„è®¾è®¡ã€‚2022å¹´ï¼Œ[GraphGPS](https://arxiv.org/abs/2205.12454)é€šè¿‡å°†GTä¸å„ç§ï¼ˆç”šè‡³å¤šä¸ªï¼‰PE/SEç»“åˆï¼Œå–å¾—äº†å·¨å¤§çš„å®è¯æˆåŠŸã€‚åˆ°2023å¹´ï¼Œå‡ºç°äº†æ›´å¼ºå¤§çš„PE/SEã€‚'
- en: '**Relative Random Walk PE (RRWP)** proposed by [Ma et al](https://arxiv.org/abs/2305.17589)
    generalizes the random walk structural encoding with the relational part. Together
    with a new variant of attention mechanism, **GRIT** achieves a strong empirical
    performance compared with existing PE/SEs on property prediction benchmarks (SOTA
    on ZINC). Theoretically, RRWP can approximate the Shortest path distance, personalized
    PageRank, and heat kernel with a specific choice of parameters. With RRWP, GRIT
    is more expressive than SPD-WL.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç›¸å¯¹éšæœºæ¸¸èµ°ç»“æ„ç¼–ç ï¼ˆRRWPï¼‰**ç”±[Maç­‰äºº](https://arxiv.org/abs/2305.17589)æå‡ºï¼Œé€šè¿‡å¼•å…¥å…³ç³»éƒ¨åˆ†æ‰©å±•äº†éšæœºæ¸¸èµ°ç»“æ„ç¼–ç ã€‚ç»“åˆä¸€ç§æ–°çš„æ³¨æ„åŠ›æœºåˆ¶å˜ä½“ï¼Œ**GRIT**åœ¨å±æ€§é¢„æµ‹åŸºå‡†æµ‹è¯•ä¸­ç›¸æ¯”ç°æœ‰çš„PE/SEè¡¨ç°å‡ºäº†å¼ºå¤§çš„å®è¯æ€§èƒ½ï¼ˆåœ¨ZINCä¸Šè¾¾åˆ°äº†SOTAï¼‰ã€‚ç†è®ºä¸Šï¼ŒRRWPå¯ä»¥é€šè¿‡ç‰¹å®šçš„å‚æ•°é€‰æ‹©æ¥é€¼è¿‘æœ€çŸ­è·¯å¾„è·ç¦»ã€ä¸ªæ€§åŒ–çš„PageRankå’Œçƒ­æ ¸ã€‚ä½¿ç”¨RRWPåï¼ŒGRITæ¯”SPD-WLå…·æœ‰æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›ã€‚'
- en: '![](../Images/47e07c8f85dacc7c111f2918d3975ecc.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47e07c8f85dacc7c111f2918d3975ecc.png)'
- en: 'RRWP visualization for the fluorescein molecule, up to the 4th power. Thicker
    and darker edges indicate higher edge weight. Probabilities for longer random
    walks reveal higher-order structures (e.g., the cliques evident in 3-RW and the
    star patterns in 4-RW). Source: [Ma et al](https://arxiv.org/abs/2305.17589).'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: fluoresceinåˆ†å­çš„RRWPå¯è§†åŒ–ï¼Œæœ€é«˜åˆ°4æ¬¡æ–¹ã€‚è¾ƒç²—ä¸”è¾ƒæ·±çš„è¾¹è¡¨ç¤ºæ›´é«˜çš„è¾¹æƒé‡ã€‚è¾ƒé•¿çš„éšæœºæ¸¸èµ°çš„æ¦‚ç‡æ­ç¤ºäº†é«˜é˜¶ç»“æ„ï¼ˆä¾‹å¦‚ï¼Œåœ¨3-RWä¸­æ˜¾ç°çš„å›¢å’Œåœ¨4-RWä¸­å‡ºç°çš„æ˜Ÿå‹å›¾æ¡ˆï¼‰ã€‚æ¥æºï¼š[Maç­‰äºº](https://arxiv.org/abs/2305.17589)ã€‚
- en: '[Puny et al](https://arxiv.org/abs/2302.11556) proposed a new theoretical framework
    for expressivity based on **Equivariant Polynomials** where the expressivity of
    common GNNs can be improved by having the polynomial features, computed with tensor
    contractions based on the equivariant basis, as positional encodings. The empirical
    results are surprising: GatedGCNs is improved from a test MAE of 0.265 to 0.106
    with the d-expressive polynomials. It will be very interesting to see if someone
    combines this with GTs in the future.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[Punyç­‰äºº](https://arxiv.org/abs/2302.11556)æå‡ºäº†ä¸€ç§åŸºäº**ç­‰å˜å¤šé¡¹å¼**çš„æ–°è¡¨è¾¾åŠ›ç†è®ºæ¡†æ¶ï¼Œé€šè¿‡åœ¨ç­‰å˜åŸºä¸Šè®¡ç®—å¼ é‡æ”¶ç¼©å¾—åˆ°çš„å¤šé¡¹å¼ç‰¹å¾ä½œä¸ºä½ç½®ç¼–ç ï¼Œå¯ä»¥æé«˜å¸¸è§GNNçš„è¡¨è¾¾åŠ›ã€‚å®éªŒç»“æœä»¤äººæƒŠè®¶ï¼šGatedGCNsçš„æµ‹è¯•MAEä»0.265æé«˜åˆ°0.106ï¼Œé‡‡ç”¨äº†d-è¡¨è¾¾å¤šé¡¹å¼ã€‚æœªæ¥æœ‰äººå°†è¿™ä¸€æ–¹æ³•ä¸GTç»“åˆå°†ä¼šéå¸¸æœ‰è¶£ã€‚'
- en: '**3ï¸âƒ£ Efficient GTs.** It remains challenging for GTs to be applied to large
    graphs due to the O(NÂ²) complexity. In 2023, we saw more works trying to eliminate
    such difficulty by lowering the computation complexity of GTs. [Deac et al](https://arxiv.org/abs/2210.02997)
    used [expander graphs](https://en.wikipedia.org/wiki/Expander_graph) for the propagation,
    which is regularly connected with few edges.[Exphormer](https://arxiv.org/abs/2303.06147)
    extended this idea to GT by combining expander graphs with the local neighborhood
    aggregation and virtual node. Exphormer allows graph transformers to scale to
    larger graphs (as large as *ogbn-arxiv* with 169K nodes). It also achieved strong
    empirical results and ranked top on several [Long-Range Graph Benchmark](https://github.com/vijaydwivedi75/lrgb)
    tasks.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '**3ï¸âƒ£ é«˜æ•ˆçš„GTsã€‚** ç”±äºGTçš„O(NÂ²)å¤æ‚åº¦ï¼ŒGTåœ¨å¤§å›¾ä¸Šçš„åº”ç”¨ä¾ç„¶å……æ»¡æŒ‘æˆ˜ã€‚åœ¨2023å¹´ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†æ›´å¤šå°è¯•é€šè¿‡é™ä½GTè®¡ç®—å¤æ‚åº¦æ¥è§£å†³è¿™ä¸€é—®é¢˜çš„ç ”ç©¶ã€‚[Deacç­‰äºº](https://arxiv.org/abs/2210.02997)ä½¿ç”¨äº†[æ‰©å±•å›¾](https://en.wikipedia.org/wiki/Expander_graph)è¿›è¡Œä¼ æ’­ï¼Œè¯¥å›¾é€šå¸¸ç”¨è¾ƒå°‘çš„è¾¹è¿›è¡Œè¿æ¥ã€‚[Exphormer](https://arxiv.org/abs/2303.06147)å°†è¿™ä¸€æ€æƒ³æ‰©å±•åˆ°GTï¼Œé€šè¿‡å°†æ‰©å±•å›¾ä¸å±€éƒ¨é‚»åŸŸèšåˆå’Œè™šæ‹ŸèŠ‚ç‚¹ç»“åˆèµ·æ¥ã€‚Exphormerä½¿å›¾è½¬æ¢å™¨èƒ½å¤Ÿæ‰©å±•åˆ°æ›´å¤§çš„å›¾ï¼ˆä¾‹å¦‚ï¼Œå…·æœ‰169KèŠ‚ç‚¹çš„*ogbn-arxiv*ï¼‰ã€‚å®ƒè¿˜å–å¾—äº†å¼ºå¤§çš„å®éªŒç»“æœï¼Œå¹¶åœ¨å¤šä¸ª[é•¿è·ç¦»å›¾åŸºå‡†æµ‹è¯•](https://github.com/vijaydwivedi75/lrgb)ä»»åŠ¡ä¸­æ’åé å‰ã€‚'
- en: ğŸ”® **Moving forward to 2024:**
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”® **è¿ˆå‘2024å¹´ï¼š**
- en: A better understanding of self-attentionâ€™s benefits on abstract beyond expressiveness.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ›´å¥½åœ°ç†è§£è‡ªæ³¨æ„åŠ›åœ¨æŠ½è±¡å±‚é¢ä¸Šå¯¹è¡¨ç°åŠ›ä¹‹å¤–çš„å¥½å¤„ã€‚
- en: Big open-source pre-trained equivariant GT in 2024!
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2024å¹´ï¼Œå¼€æºå¤§è§„æ¨¡é¢„è®­ç»ƒçš„ç­‰å˜GTï¼
- en: More powerful positional encodings.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ›´å¼ºå¤§çš„ä½ç½®ç¼–ç ã€‚
- en: New Datasets & Benchmarks
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ–°çš„æ•°æ®é›†ä¸åŸºå‡†æµ‹è¯•
- en: '**Structural biology:** Pinder from VantAI, [PoseBusters](https://arxiv.org/abs/2308.05777)
    from Oxford, [PoseCheck](https://arxiv.org/abs/2308.07413) from The Other Place,
    [DockGen](https://openreview.net/forum?id=UfBIxpTK10), and LargeMix and UltraLarge
    datasets [from Valence Labs](https://arxiv.org/abs/2310.04292)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç»“æ„ç”Ÿç‰©å­¦ï¼š** æ¥è‡ªVantAIçš„Pinderï¼Œæ¥è‡ªç‰›æ´¥çš„[PoseBusters](https://arxiv.org/abs/2308.05777)ï¼Œæ¥è‡ªThe
    Other Placeçš„[PoseCheck](https://arxiv.org/abs/2308.07413)ï¼Œ[DockGen](https://openreview.net/forum?id=UfBIxpTK10)ï¼Œä»¥åŠæ¥è‡ª[Valence
    Labs](https://arxiv.org/abs/2310.04292)çš„LargeMixå’ŒUltraLargeæ•°æ®é›†'
- en: '[**Temporal Graph Benchmark**](http://tgb.mila.quebec/) (TGB): Until now, progress
    in temporal graph learning has been held back by the lack of large high-quality
    datasets, as well as the lack of proper evaluation thus leading to over-optimistic
    performance. TGB addresses this by introducing a collection of seven realistic,
    large-scale and diverse benchmarks for learning on temporal graphs, including
    both node-wise and link-wise tasks. Inspired by the success of OGB, TGB automates
    dataset downloading and processing as well as evaluation protocols, and allows
    users to compare model performance using a [leaderboard](https://tgb-website.pages.dev/docs/leader_linkprop/).
    Check out the [associated blog post](/temporal-graph-benchmark-bb5cc26fcf11) for
    more details.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '[**æ—¶åºå›¾åŸºå‡†æµ‹è¯•**](http://tgb.mila.quebec/)ï¼ˆTGBï¼‰ï¼šåˆ°ç›®å‰ä¸ºæ­¢ï¼Œæ—¶åºå›¾å­¦ä¹ çš„è¿›å±•ä¸€ç›´å—åˆ°ç¼ºä¹å¤§å‹é«˜è´¨é‡æ•°æ®é›†ä»¥åŠç¼ºä¹é€‚å½“è¯„ä¼°çš„é™åˆ¶ï¼Œè¿™å¯¼è‡´äº†è¿‡äºä¹è§‚çš„æ€§èƒ½è¯„ä¼°ã€‚TGBé€šè¿‡å¼•å…¥ä¸ƒä¸ªç°å®çš„å¤§è§„æ¨¡ä¸”å¤šæ ·åŒ–çš„åŸºå‡†æµ‹è¯•ï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ï¼Œè¿™äº›åŸºå‡†åŒ…æ‹¬èŠ‚ç‚¹çº§å’Œé“¾æ¥çº§ä»»åŠ¡ã€‚å—åˆ°OGBæˆåŠŸçš„å¯å‘ï¼ŒTGBè‡ªåŠ¨åŒ–äº†æ•°æ®é›†ä¸‹è½½å’Œå¤„ç†ï¼Œä»¥åŠè¯„ä¼°åè®®ï¼Œå¹¶å…è®¸ç”¨æˆ·ä½¿ç”¨[æ’è¡Œæ¦œ](https://tgb-website.pages.dev/docs/leader_linkprop/)æ¯”è¾ƒæ¨¡å‹æ€§èƒ½ã€‚æ›´å¤šè¯¦æƒ…è¯·æŸ¥çœ‹[ç›¸å…³åšå®¢æ–‡ç« ](/temporal-graph-benchmark-bb5cc26fcf11)ã€‚'
- en: '[**TpuGraphs**](https://github.com/google-research-datasets/tpu_graphs) from
    Google Research: the graph property prediction dataset of TPU computational graphs.
    The dataset provides 25x more graphs than the largest graph property prediction
    dataset (with comparable graph sizes), and 770x larger graphs on average compared
    to existing performance prediction datasets on machine learning programs. Google
    ran [Kaggle competition](https://www.kaggle.com/competitions/predict-ai-model-runtime)
    based off TpuGraphs!'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥è‡ªGoogle Researchçš„[**TpuGraphs**](https://github.com/google-research-datasets/tpu_graphs)ï¼šTPUè®¡ç®—å›¾çš„å›¾å±æ€§é¢„æµ‹æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†æä¾›çš„å›¾æ•°é‡æ˜¯ç°æœ‰æœ€å¤§å›¾å±æ€§é¢„æµ‹æ•°æ®é›†çš„25å€ï¼ˆå›¾å¤§å°ç›¸å½“ï¼‰ï¼Œä¸”ä¸ç°æœ‰æœºå™¨å­¦ä¹ ç¨‹åºæ€§èƒ½é¢„æµ‹æ•°æ®é›†ç›¸æ¯”ï¼Œå›¾çš„å¹³å‡è§„æ¨¡å¤§çº¦å¤§770å€ã€‚GoogleåŸºäºTpuGraphsä¸¾åŠäº†[Kaggleæ¯”èµ›](https://www.kaggle.com/competitions/predict-ai-model-runtime)ï¼
- en: '[**LagrangeBench**](https://github.com/tumaer/lagrangebench): A Lagrangian
    Fluid Mechanics Benchmarking Suite â€” where you can evaluate your favorite GNN-based
    simulator in a JAX-based environment (for JAX aficionados).'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[**LagrangeBench**](https://github.com/tumaer/lagrangebench)ï¼šæ‹‰æ ¼æœ—æ—¥æµä½“åŠ›å­¦åŸºå‡†æµ‹è¯•å¥—ä»¶â€”â€”åœ¨ä¸€ä¸ªåŸºäºJAXçš„ç¯å¢ƒä¸­è¯„ä¼°ä½ å–œçˆ±çš„åŸºäºGNNçš„æ¨¡æ‹Ÿå™¨ï¼ˆé€‚åˆJAXçˆ±å¥½è€…ï¼‰'
- en: '[**RelBench**](https://relbench.stanford.edu/): Relational Deep Learning Benchmark
    from Stanford and Kumo.AI: make time-based predictions over relational databases
    (which you can model as graphs or hypergraphs).'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[**RelBench**](https://relbench.stanford.edu/)ï¼šæ¥è‡ªæ–¯å¦ç¦å’ŒKumo.AIçš„å…³ç³»æ·±åº¦å­¦ä¹ åŸºå‡†ï¼šå¯¹å…³ç³»æ•°æ®åº“è¿›è¡ŒåŸºäºæ—¶é—´çš„é¢„æµ‹ï¼ˆä½ å¯ä»¥å°†å…¶å»ºæ¨¡ä¸ºå›¾æˆ–è¶…å›¾ï¼‰'
- en: '[**The GNoMe dataset**](https://github.com/google-deepmind/materials_discovery?tab=readme-ov-file#dataset)
    from Google DeepMind: 381k more novel stable materials for your materials discovery
    and ML potentials models!'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[**GNoMeæ•°æ®é›†**](https://github.com/google-deepmind/materials_discovery?tab=readme-ov-file#dataset)æ¥è‡ªGoogle
    DeepMindï¼š381kç§æ–°å‹ç¨³å®šææ–™ï¼Œç”¨äºææ–™å‘ç°å’Œæœºå™¨å­¦ä¹ æ½œåŠ›æ¨¡å‹ï¼'
- en: Conferences, Courses & Community
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¼šè®®ã€è¯¾ç¨‹ä¸ç¤¾åŒº
- en: 'The main events in the graph and geometric learning world (apart from big ML
    conferences) grow larger and more mature: [The Learning on Graphs Conference (LoG)](https://logconference.org/),
    [Molecular ML](https://www.moml.mit.edu/) (MoML), and the [Stanford Graph Learning
    Workshop](https://snap.stanford.edu/graphlearning-workshop-2023/). The LoG conference
    features a cool format with the remote-first conference and dozens of local meetups
    organized by community members spanning the whole globe from China to UK & Europe
    to the US West Coast ğŸŒğŸŒğŸŒ .'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾å½¢å’Œå‡ ä½•å­¦ä¹ é¢†åŸŸçš„ä¸»è¦äº‹ä»¶ï¼ˆé™¤äº†å¤§å‹MLä¼šè®®ï¼‰ä¸æ–­å£®å¤§å’Œæˆç†Ÿï¼š[å›¾å­¦ä¹ ä¼šè®®ï¼ˆLoGï¼‰](https://logconference.org/)ã€[åˆ†å­æœºå™¨å­¦ä¹ ](https://www.moml.mit.edu/)ï¼ˆMoMLï¼‰å’Œ[æ–¯å¦ç¦å›¾å­¦ä¹ å·¥ä½œåŠ](https://snap.stanford.edu/graphlearning-workshop-2023/)ã€‚LoGä¼šè®®é‡‡ç”¨è¿œç¨‹ä¼˜å…ˆçš„ç‹¬ç‰¹å½¢å¼ï¼Œå…¨çƒèŒƒå›´å†…çš„ç¤¾åŒºæˆå‘˜ç»„ç»‡äº†æ•°ååœºæœ¬åœ°èšä¼šï¼Œä»ä¸­å›½åˆ°è‹±å›½ã€æ¬§æ´²ï¼Œå†åˆ°ç¾å›½è¥¿æµ·å²¸
    ğŸŒğŸŒğŸŒã€‚
- en: '![](../Images/d717711a2b7d239dc03499441d93f3bc.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d717711a2b7d239dc03499441d93f3bc.png)'
- en: 'The LoG meetups in Amsterdam, Paris, TromsÃ¸, and Shanghai. Source: Slack of
    the LoG community'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: LoGåœ¨é˜¿å§†æ–¯ç‰¹ä¸¹ã€å·´é»ã€ç‰¹ç½—å§†ç‘Ÿå’Œä¸Šæµ·çš„èšä¼šã€‚æ¥æºï¼šLoGç¤¾åŒºçš„Slack
- en: Courses, books, and educational resources
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯¾ç¨‹ã€ä¹¦ç±å’Œæ•™è‚²èµ„æº
- en: '[Geometric GNN Dojo](https://github.com/chaitjo/geometric-gnn-dojo) â€” a pedagogical
    resource for beginners and experts to explore the design space of GNNs for geometric
    graphs (pairs best with the recent Hitchhikerâ€™s Guide to Geometric GNNs).'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å‡ ä½•GNNé“åœº](https://github.com/chaitjo/geometric-gnn-dojo)â€”â€”ä¸€ä¸ªé¢å‘åˆå­¦è€…å’Œä¸“å®¶çš„æ•™å­¦èµ„æºï¼Œå¸®åŠ©æ¢ç´¢å‡ ä½•å›¾å½¢çš„GNNè®¾è®¡ç©ºé—´ï¼ˆä¸æœ€è¿‘çš„ã€Šå‡ ä½•GNNçš„ä¾¿æ·æŒ‡å—ã€‹æœ€ä½³æ­é…ï¼‰'
- en: '[TorchCFM](https://github.com/atong01/conditional-flow-matching) â€” the main
    entrypoint to the world of flow matching.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TorchCFM](https://github.com/atong01/conditional-flow-matching)â€”â€”æµåŒ¹é…é¢†åŸŸçš„ä¸»è¦å…¥å£'
- en: The [PyT team](https://github.com/pyt-team) maintains TopoNetX, TopoModelX,
    and TopoEmbedX â€” the most hands-on libraries to jump into topological deep learning.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTå›¢é˜Ÿ](https://github.com/pyt-team)ç»´æŠ¤ç€TopoNetXã€TopoModelXå’ŒTopoEmbedXâ€”â€”æœ€å®ç”¨çš„æ‹“æ‰‘æ·±åº¦å­¦ä¹ åº“'
- en: 'The book on [Equivariant and Coordinate Independent Convolutional Networks:
    A Gauge Field Theory of Neural Networks](https://maurice-weiler.gitlab.io/#cnn_book)
    by Maurice Weiler, Patrick ForrÃ©, Erik Verlinde, and Max Welling â€” brings together
    the findings on the representation theory and differential geometry of equivariant
    CNNs'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”±Maurice Weilerã€Patrick ForrÃ©ã€Erik Verlindeå’ŒMax Wellingç¼–å†™çš„[ã€Šç­‰å˜å’Œåæ ‡ç‹¬ç«‹å·ç§¯ç½‘ç»œï¼šç¥ç»ç½‘ç»œçš„è§„èŒƒåœºç†è®ºã€‹](https://maurice-weiler.gitlab.io/#cnn_book)ä¸€ä¹¦â€”â€”æ±‡é›†äº†å…³äºç­‰å˜å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„è¡¨ç¤ºç†è®ºå’Œå¾®åˆ†å‡ ä½•çš„ç ”ç©¶æˆæœ
- en: Surveys
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è°ƒæŸ¥
- en: '**ML for Science in Quantum, Atomistic, and Continuum systems** by well over
    60 authors from 23 institutions ([Zhang, Wang, Helwig, Luo, Fu, Xie et al.](https://arxiv.org/abs/2307.08423))'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é‡å­ã€åŸå­ç³»ç»Ÿå’Œè¿ç»­ä»‹è´¨ä¸­çš„ç§‘å­¦æœºå™¨å­¦ä¹ **ï¼Œç”±æ¥è‡ª23ä¸ªæœºæ„çš„60å¤šä½ä½œè€…ï¼ˆ[å¼ ã€ç‹ã€Helwigã€ç½—ã€å‚…ã€è°¢ç­‰](https://arxiv.org/abs/2307.08423)ï¼‰æ’°å†™'
- en: '**Scientific discovery in the age of artificial intelligence** by [Wang et
    al](https://www.nature.com/articles/s41586-023-06221-2) published in Nature.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**äººå·¥æ™ºèƒ½æ—¶ä»£çš„ç§‘å­¦å‘ç°**ï¼Œç”±[ç‹ç­‰äºº](https://www.nature.com/articles/s41586-023-06221-2)å‘è¡¨äºã€Šè‡ªç„¶ã€‹æ‚å¿—ã€‚'
- en: Prominent seminar series
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: çŸ¥åç ”è®¨ä¼šç³»åˆ—
- en: '[Learning on Graphs & Geometry](https://portal.valencelabs.com/logg)'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å›¾å½¢ä¸å‡ ä½•å­¦ä¹ ](https://portal.valencelabs.com/logg)'
- en: '[Molecular Modeling and Drug Discovery (M2D2)](https://portal.valencelabs.com/m2d2)'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[åˆ†å­å»ºæ¨¡ä¸è¯ç‰©å‘ç°ï¼ˆM2D2ï¼‰](https://portal.valencelabs.com/m2d2)'
- en: '[VantAI reading group](https://www.youtube.com/@Vant_AI)'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[VantAIé˜…è¯»å°ç»„](https://www.youtube.com/@Vant_AI)'
- en: '[Oxford LoG2 seminar series](https://log-2.github.io/)'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ç‰›æ´¥LoG2ç ”è®¨ä¼šç³»åˆ—](https://log-2.github.io/)'
- en: Slack communities
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Slackç¤¾ç¾¤
- en: '[LoGaG](https://join.slack.com/t/logag/shared_invite/zt-22y7n3k7a-FHwX31gc85yZCa0uF8BU7w)'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LoGaG](https://join.slack.com/t/logag/shared_invite/zt-22y7n3k7a-FHwX31gc85yZCa0uF8BU7w)'
- en: '[LOG conference](https://join.slack.com/t/logconference/shared_invite/zt-27nv8ba1y-pXspnAzgLOMdDzfKgpOafg)'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LOGä¼šè®®](https://join.slack.com/t/logconference/shared_invite/zt-27nv8ba1y-pXspnAzgLOMdDzfKgpOafg)'
- en: '[PyG](https://data.pyg.org/slack.html)'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyG](https://data.pyg.org/slack.html)'
- en: Memes of 2023
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2023å¹´çš„è¿·å› 
- en: '![](../Images/51dd958953c300da8926b7e2758c3032.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51dd958953c300da8926b7e2758c3032.png)'
- en: 'Commemorating the successes of flow matching in 2023 in the meme and unique
    t-shirts brought to NeurIPSâ€™23\. Right: Hannes StÃ¤rk and Michael Galkin are making
    a statement at NeurIPSâ€™23\. Images by Michael Galkin'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨2023å¹´ï¼Œçºªå¿µæµåŒ¹é…çš„æˆåŠŸï¼Œå¸¦åˆ°äº†NeurIPSâ€™23çš„è¿·å› å’Œç‹¬ç‰¹Tæ¤ã€‚å³å›¾ï¼šHannes StÃ¤rkå’ŒMichael Galkinåœ¨NeurIPSâ€™23å‘è¡¨å£°æ˜ã€‚å›¾ç‰‡ç”±Michael
    Galkinæä¾›ã€‚
- en: '![](../Images/fdee1c1983f3c90e78d5b1711742cdee.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fdee1c1983f3c90e78d5b1711742cdee.png)'
- en: GNN aggregation functions are actually portals to category theory (Created by
    Petar VeliÄkoviÄ‡)
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: GNNèšåˆå‡½æ•°å®é™…ä¸Šæ˜¯èŒƒç•´ç†è®ºçš„é—¨æˆ·ï¼ˆç”±Petar VeliÄkoviÄ‡åˆ›å»ºï¼‰
- en: '![](../Images/7255c70c42c2ce3763a19bff623a6e22.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7255c70c42c2ce3763a19bff623a6e22.png)'
- en: 'Michael Bronstein continues to harass Google by demanding his [DeepMind chair](https://www.cs.ox.ac.uk/news/1996-full.html)
    at every ML conference, but so far, he has only been offered stools (photo credits:
    Jelani Nelson and Thomas Kipf).'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: Michael Bronsteinç»§ç»­é€šè¿‡è¦æ±‚ä»–çš„[DeepMindä¸»å¸­èŒä½](https://www.cs.ox.ac.uk/news/1996-full.html)æ¥éªšæ‰°è°·æ­Œï¼Œä½†åˆ°ç›®å‰ä¸ºæ­¢ï¼Œä»–åªè¢«æä¾›äº†å‡³å­ï¼ˆç…§ç‰‡æ¥æºï¼šJelani
    Nelsonå’ŒThomas Kipfï¼‰ã€‚
- en: '![](../Images/6d7fe205e735aeb0f578f2c421c23b3f.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d7fe205e735aeb0f578f2c421c23b3f.png)'
- en: 'The authors of this blog post congratulate you upon completing the long read.
    Michael Galkin and Michael Bronstein with the Meme of 2022 at ICML 2023 in Hawaii
    (Photo credit: Ben Finkelshtein)'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬åšå®¢çš„ä½œè€…ç¥è´ºä½ å®Œæˆäº†è¿™ç¯‡é•¿æ–‡ã€‚Michael Galkinå’ŒMichael Bronsteinåœ¨å¤å¨å¤·ICML 2023ä¸Šçš„2022å¹´è¿·å› ï¼ˆç…§ç‰‡æ¥æºï¼šBen
    Finkelshteinï¼‰
- en: '*For additional articles about geometric and graph deep learning, see* [*Michael
    Galkin*](https://medium.com/@mgalkin)*â€™s and* [*Michael Bronstein*](https://medium.com/@michael-bronstein)*â€™s
    Medium posts and follow the two Michaels (*[*Galkin*](https://twitter.com/michael_galkin)
    *and* [*Bronstein*](https://twitter.com/mmbronstein)*) on Twitter.*'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ¬²äº†è§£æ›´å¤šå‡ ä½•å­¦å’Œå›¾æ·±åº¦å­¦ä¹ çš„æ–‡ç« ï¼Œè¯·å‚é˜…* [*Michael Galkin*](https://medium.com/@mgalkin)*å’Œ*
    [*Michael Bronstein*](https://medium.com/@michael-bronstein)*çš„Mediumæ–‡ç« ï¼Œå¹¶åœ¨Twitterä¸Šå…³æ³¨è¿™ä¸¤ä½Michaelï¼ˆ*[*Galkin*](https://twitter.com/michael_galkin)
    *å’Œ* [*Bronstein*](https://twitter.com/mmbronstein)ï¼‰ã€‚*'
