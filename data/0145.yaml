- en: 'Graph & Geometric ML in 2024: Where We Are and What’s Next (Part I — Theory
    & Architectures)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2024年图形与几何机器学习：我们目前的状况与未来展望（第一部分 — 理论与架构）
- en: 原文：[https://towardsdatascience.com/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-i-theory-architectures-3af5d38376e1?source=collection_archive---------1-----------------------#2024-01-16](https://towardsdatascience.com/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-i-theory-architectures-3af5d38376e1?source=collection_archive---------1-----------------------#2024-01-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-i-theory-architectures-3af5d38376e1?source=collection_archive---------1-----------------------#2024-01-16](https://towardsdatascience.com/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-i-theory-architectures-3af5d38376e1?source=collection_archive---------1-----------------------#2024-01-16)
- en: State-of-the-Art Digest
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最前沿技术摘要
- en: Following the tradition from previous years, we interviewed a cohort of distinguished
    and prolific academic and industrial experts in an attempt to summarise the highlights
    of the past year and predict what is in store for 2024\. Past 2023 was so ripe
    with results that we had to break this post into two parts. This is Part I focusing
    on theory & new architectures, see also [Part II](https://medium.com/towards-data-science/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-ii-applications-1ed786f7bf63)
    on applications.
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 根据往年传统，我们采访了一批杰出且高产的学术界和工业界专家，旨在总结过去一年的亮点并预测2024年的发展趋势。2023年成果丰富，我们不得不将这篇文章分为两部分。这是第一部分，重点讨论理论与新架构，另请参阅关于应用的[第二部分](https://medium.com/towards-data-science/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-ii-applications-1ed786f7bf63)。
- en: '[](https://mgalkin.medium.com/?source=post_page---byline--3af5d38376e1--------------------------------)[![Michael
    Galkin](../Images/c5eb13334712ca0462d8a5df4a268ad0.png)](https://mgalkin.medium.com/?source=post_page---byline--3af5d38376e1--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3af5d38376e1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3af5d38376e1--------------------------------)
    [Michael Galkin](https://mgalkin.medium.com/?source=post_page---byline--3af5d38376e1--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mgalkin.medium.com/?source=post_page---byline--3af5d38376e1--------------------------------)[![Michael
    Galkin](../Images/c5eb13334712ca0462d8a5df4a268ad0.png)](https://mgalkin.medium.com/?source=post_page---byline--3af5d38376e1--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3af5d38376e1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3af5d38376e1--------------------------------)
    [Michael Galkin](https://mgalkin.medium.com/?source=post_page---byline--3af5d38376e1--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3af5d38376e1--------------------------------)
    ·30 min read·Jan 16, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3af5d38376e1--------------------------------)
    ·阅读时间：30分钟·2024年1月16日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f888e845d4bdb9f34131d8b0544d71fc.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f888e845d4bdb9f34131d8b0544d71fc.png)'
- en: Image by Authors with some help from DALL-E 3.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片，部分来自DALL-E 3。
- en: '*The post is written and edited by* [*Michael Galkin*](https://twitter.com/michael_galkin)
    *and* [*Michael Bronstein*](https://twitter.com/mmbronstein) *with significant
    contributions from* [*Johannes Brandstetter*](https://twitter.com/jo_brandstetter)*,*
    [*İsmail İlkan Ceylan*](https://twitter.com/ismaililkanc/)*,* [*Francesco Di Giovanni*](https://twitter.com/Francesco_dgv)*,*
    [*Ben Finkelshtein*](https://twitter.com/benfinkelshtein)*,* [*Kexin Huang*](https://twitter.com/KexinHuang5)*,*
    [*Chaitanya Joshi*](https://twitter.com/chaitjo)*,* [*Chen Lin*](https://twitter.com/WillLin1028)*,*
    [*Christopher Morris*](https://twitter.com/chrsmrrs)*,* [*Mathilde Papillon*](https://twitter.com/mathildepapillo)*,*
    [*Liudmila Prokhorenkova*](https://twitter.com/LProkhorenkova)*,* [*Bastian Rieck*](https://twitter.com/Pseudomanifold)*,*
    [*David Ruhe*](https://twitter.com/djjruhe)*,* [*Hannes Stärk*](https://twitter.com/HannesStaerk)*,
    and* [*Petar Veličković*](https://twitter.com/PetarV_93)*.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*本文由* [*Michael Galkin*](https://twitter.com/michael_galkin) *和* [*Michael
    Bronstein*](https://twitter.com/mmbronstein) *撰写和编辑，并由* [*Johannes Brandstetter*](https://twitter.com/jo_brandstetter)*,*
    [*İsmail İlkan Ceylan*](https://twitter.com/ismaililkanc/)*,* [*Francesco Di Giovanni*](https://twitter.com/Francesco_dgv)*,*
    [*Ben Finkelshtein*](https://twitter.com/benfinkelshtein)*,* [*Kexin Huang*](https://twitter.com/KexinHuang5)*,*
    [*Chaitanya Joshi*](https://twitter.com/chaitjo)*,* [*Chen Lin*](https://twitter.com/WillLin1028)*,*
    [*Christopher Morris*](https://twitter.com/chrsmrrs)*,* [*Mathilde Papillon*](https://twitter.com/mathildepapillo)*,*
    [*Liudmila Prokhorenkova*](https://twitter.com/LProkhorenkova)*,* [*Bastian Rieck*](https://twitter.com/Pseudomanifold)*,*
    [*David Ruhe*](https://twitter.com/djjruhe)*,* [*Hannes Stärk*](https://twitter.com/HannesStaerk)*
    和* [*Petar Veličković*](https://twitter.com/PetarV_93)*.*  '
- en: '[Theory of Graph Neural Networks](#79aa)'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[图神经网络的理论](#79aa)'
- en: 1\. [Message passing neural networks and Graph Transformers](#5903)
  id: totrans-11
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 1. [信息传递神经网络与图Transformer](#5903)
- en: 2\. [Graph components, biconnectivity & planarity](#a6d7)
  id: totrans-12
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2. [图组件、二连通性与平面性](#a6d7)
- en: 3\. [Aggregation functions & uniform expressivity](#27e6)
  id: totrans-13
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 3. [聚合函数与统一表达能力](#27e6)
- en: 4\. [Convergence & zero-one laws of GNNs](#645f)
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 4. [GNN的收敛性与零一法则](#645f)
- en: 5. [Descriptive complexity of GNNs](#c8ac)
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 5. [GNN的描述复杂度](#c8ac)
- en: 6\. [Fine-grained expressivity of GNNs](#9b59)
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 6. [GNN的精细化表达能力](#9b59)
- en: 7\. [Expressivity results for Subgraph GNNs](#06c2)
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 7. [子图GNN的表达能力结果](#06c2)
- en: 8\. [Expressivity for Link Prediction and Knowledge Graphs](#ab19)
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 8. [链路预测与知识图谱的表达能力](#ab19)
- en: 9\. [Over-squashing & Expressivity](#c284)
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 9. [过度压缩与表达能力](#c284)
- en: 10\. [Generalization and Extrapolation capabilities of GNNs](#4a32)
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 10. [GNN的泛化与外推能力](#4a32)
- en: 11\. [Predictions time!](#4f30)
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 11. [预测时间！](#4f30)
- en: '[New and Exotic Message Passing](#b09f)'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[新型与异域信息传递](#b09f)'
- en: '[Beyond Graphs](#9a3b)'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[超越图形](#9a3b)'
- en: 1\. [Topology](#efa6)
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 1. [拓扑](#efa6)
- en: 2\. [Geometric Algebras](#a368)
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2. [几何代数](#a368)
- en: 3\. [PDEs](#5b67)
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 3. [偏微分方程](#5b67)
- en: '[Robustness & Explainability](#8171)'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[鲁棒性与可解释性](#8171)'
- en: '[Graph Transformers](#e7b4)'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[图Transformer](#e7b4)'
- en: '[New Datasets & Benchmarks](#cf16)'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[新数据集与基准测试](#cf16)'
- en: '[Conferences, Courses & Community](#926c)'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[会议、课程与社区](#926c)'
- en: '[Memes of 2023](#f1d3)'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2023年的迷因](#f1d3)'
- en: 'The legend we will be using throughout the text:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在整个文本中使用的图例：
- en: 💡 - year’s highlight
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 💡 - 本年度亮点
- en: 🏋️ - challenges
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 🏋️ - 挑战
- en: ➡️ - current/next developments
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ - 当前/下一步发展
- en: 🔮- predictions/speculations
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 🔮- 预测/猜测
- en: Theory of Graph Neural Networks
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图神经网络的理论
- en: '*Michael Bronstein (Oxford), Francesco Di Giovanni (Oxford), İsmail İlkan Ceylan
    (Oxford), Chris Morris (RWTH Aachen)*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*迈克尔·布朗斯坦（牛津大学）、弗朗切斯科·迪·乔瓦尼（牛津大学）、伊斯梅尔·伊尔坎·杰兰（牛津大学）、克里斯·莫里斯（亚琛工业大学）*'
- en: '**Message Passing Neural Networks & Graph Transformers**'
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**信息传递神经网络与图Transformer**'
- en: Graph Transformers are a relatively recent trend in graph ML, trying to extend
    the successes of Transformers from sequences to graphs. As far as traditional
    expressivity results go, these architectures do not offer any particular advantages.
    In fact, it is arguable that most of their benefits in terms of expressivity (see
    e.g. [Kreuzer et al.](https://arxiv.org/abs/2106.03893)) come from powerful structural
    encodings rather than the architecture itself and such encodings can in principle
    be used with MPNNs.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图Transformer是图机器学习中一个相对较新的趋势，试图将Transformer在序列上的成功扩展到图形上。就传统的表达能力结果而言，这些架构并没有提供任何特别的优势。事实上，可以说，它们在表达能力方面的大多数优势（例如，见[Kreuzer等人](https://arxiv.org/abs/2106.03893)）来自于强大的结构编码，而非架构本身，而这些编码原则上也可以与MPNN一起使用。
- en: 'In a recent paper, [Cai et al.](https://arxiv.org/abs/2301.11956) investigate
    the connection between MPNNs and (graph) Transformers showing that an MPNN with
    a virtual node — an auxiliary node that is connected to all other nodes in a specific
    way — can simulate a (graph) Transformer. This architecture is *non-uniform*,
    i.e., the size and structure of the neural networks may depend on the size of
    the input graphs. Interestingly, once we restrict our attention to linear Transformers
    (e.g., Performer) then there is a *uniform* result: there exists a single MPNN
    using a virtual node that can approximate a linear transformer such as Performer
    on any input of any size.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在最近的一篇论文中，[蔡等人](https://arxiv.org/abs/2301.11956)探讨了MPNN与（图形）Transformer之间的联系，表明一个带有虚拟节点的MPNN——一个以特定方式与所有其他节点连接的辅助节点——可以模拟一个（图形）Transformer。这种架构是*非统一*的，即神经网络的大小和结构可能依赖于输入图形的大小。有趣的是，一旦我们将注意力限制到线性Transformer（例如，Performer），就会有一个*统一*的结果：存在一个使用虚拟节点的单一MPNN，可以在任何大小的输入上逼近线性Transformer，如Performer。
- en: '![](../Images/58777f0195680c0818525a67e58b2031.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/58777f0195680c0818525a67e58b2031.png)'
- en: 'Figure from [Cai et al.](https://arxiv.org/abs/2301.11956): (a) MPNN with a
    virtual node, (b) a Transformer.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[蔡等人](https://arxiv.org/abs/2301.11956)的图： (a) 带虚拟节点的MPNN，(b) 一个Transformer。'
- en: This is related to the discussions on whether graph transformer architectures
    present advantages for capturing long-range dependencies when compared to MPNNs.
    Graph transformers are compared to MPNNs that include a global computation component
    through the use of virtual nodes, which is a common practice. [Cai et al.](https://arxiv.org/abs/2301.11956)
    empirically show that MPNNs with virtual nodes can surpass the performance of
    graph transformers on the Long-Range Graph Benchmark (LRGB, [Dwivedi et al.](https://arxiv.org/abs/2206.08164))
    Moreover, [Tönshoff et al.](https://arxiv.org/abs/2309.00367) re-evaluate MPNN
    baselines on the LRGB benchmark to find out that the earlier reported performance
    gap in favor of graph transformers was overestimated due to suboptimal hyperparameter
    choices, essentially closing the gap between MPNNs and graph Transformers.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这与关于图变换器架构是否相较于MPNN在捕捉长程依赖方面具有优势的讨论相关。图变换器与MPNN进行比较，后者通过使用虚拟节点包含了一个全局计算组件，这是常见的做法。[Cai
    et al.](https://arxiv.org/abs/2301.11956) 实证表明，带有虚拟节点的MPNN能够在长程图基准（LRGB，[Dwivedi
    et al.](https://arxiv.org/abs/2206.08164)）上超越图变换器的表现。此外，[Tönshoff et al.](https://arxiv.org/abs/2309.00367)
    在LRGB基准上重新评估了MPNN的基准，发现先前报告的图变换器性能差距被高估了，这是由于亚优的超参数选择，实质上缩小了MPNN和图变换器之间的差距。
- en: '![](../Images/e9fc784a08cfc83613d260e0224f4499.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e9fc784a08cfc83613d260e0224f4499.png)'
- en: 'Figure from [Lim et al.](https://arxiv.org/abs/2202.13013): SignNet pipeline.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[Lim et al.](https://arxiv.org/abs/2202.13013)中的图：SignNet管道。'
- en: It is also well-known that common Laplacian positional encodings (e.g., LapPE),
    are not invariant to the changes of signs and basis of eigenvectors. The lack
    of invariance makes it easier to obtain (non-uniform) universality results, but
    these models do not compute graph invariants as a consequence. This has motivated
    a body of work this year, including the study of sign and basis invariant networks
    ([Lim et al., 2023a](https://arxiv.org/abs/2202.13013)) and sign equivariant networks
    ([Lim et al., 2023b](https://arxiv.org/abs/2312.02339)). These findings suggest
    that more research is necessary to theoretically ground the claims commonly found
    in the literature regarding the comparisons of MPNNs and graph transformers.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 还广为人知，常见的拉普拉斯位置编码（例如，LapPE）对特征向量的符号和基变换不具有不变性。缺乏不变性使得获得（非均匀）普适性结果变得更容易，但这些模型因此不会计算图的不变性。这激发了今年一系列的研究，包括符号和基不变网络的研究（[Lim
    et al., 2023a](https://arxiv.org/abs/2202.13013)）以及符号等变网络的研究（[Lim et al., 2023b](https://arxiv.org/abs/2312.02339)）。这些发现表明，关于MPNN和图变换器比较的文献中常见的主张，仍需进行更多的理论研究。
- en: '**Graph components, biconnectivity, and planarity**'
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**图组件、双连通性和平面性**'
- en: '![](../Images/f38beda635e6b001c4b4b95828e6afbe.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f38beda635e6b001c4b4b95828e6afbe.png)'
- en: Figure originally by Zyqqh at [Wikipedia](https://commons.wikimedia.org/w/index.php?curid=19053091).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 最初由Zyqqh在[Wikipedia](https://commons.wikimedia.org/w/index.php?curid=19053091)上发布的图。
- en: '[Zhang et al. (2023a)](https://arxiv.org/abs/2301.09505) brings the study of
    graph biconnectivity to the attention of graph ML community. There are many results
    presented by [Zhang et al. (2023a)](https://arxiv.org/abs/2301.09505) relative
    to different biconnectivity metrics. It has been shown that standard MPNNs cannot
    detect graph biconnectivity unlike many existing higher-order models (i.e., those
    that can match the power of 2-FWL). On the other hand, Graphormers with certain
    distance encodings and subgraph GNNs such as ESAN can detect graph biconnectivity.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[Zhang et al. (2023a)](https://arxiv.org/abs/2301.09505)将图的双连通性研究带入了图机器学习领域。由[Zhang
    et al. (2023a)](https://arxiv.org/abs/2301.09505)提出的许多结果与不同的双连通性度量相关。研究表明，标准的MPNN无法检测图的双连通性，而许多现有的高阶模型（即能够匹配2-FWL能力的模型）则可以。另一方面，具有某些距离编码和子图GNN（如ESAN）的Graphormers能够检测图的双连通性。'
- en: '![](../Images/73874a490cdd02918773c2f31c4c5701.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/73874a490cdd02918773c2f31c4c5701.png)'
- en: 'Figure from [Dimitrov et al. (2023)](https://arxiv.org/abs/2307.01180): LHS
    shows the graph decompositions (A-C) and RHS shows the associated encoders (D-F)
    and the update equation (G).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[Dimitrov et al. (2023)](https://arxiv.org/abs/2307.01180)中的图：左侧显示了图的分解（A-C），右侧显示了相关的编码器（D-F）和更新方程（G）。'
- en: '[Dimitrov et al. (2023)](https://arxiv.org/abs/2307.01180) rely on graph decompositions
    to develop dedicated architectures for learning with planar graphs. The idea is
    to align with a variation of the classical [Hopcroft & Tarjan](https://www.sciencedirect.com/science/article/pii/0020019071900196)
    algorithm for planar isomorphism testing. [Dimitrov et al. (2023)](https://arxiv.org/abs/2307.01180)
    first decompose the graph into its biconnected and triconnected components, and
    afterwards learn representations for nodes, cut nodes, biconnected components,
    and triconnected components. This is achieved using the classical structures of
    Block-Cut Trees and SPQR Trees which can be computed in linear time. The resulting
    framework is called [PlanE](https://arxiv.org/abs/2307.01180) and contains architectures
    such as [BasePlanE](https://arxiv.org/abs/2307.01180). BasePlanE computes *isomorphism-complete
    graph invariants* and hence it can distinguish any pair of planar graphs. The
    key contribution of this work is to design architectures for efficiently learning
    complete invariants of planar graphs while remaining practically scalable. It
    is worth noting that 3-FWL is known to be complete on planar graphs ([Kiefer et
    al., 2019](https://dl.acm.org/doi/10.1145/3333003)), but this algorithm is not
    scalable.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[Dimitrov 等人 (2023)](https://arxiv.org/abs/2307.01180)依赖图分解来开发专门的架构，以便在平面图上进行学习。其思路是与经典的[Hopcroft
    & Tarjan](https://www.sciencedirect.com/science/article/pii/0020019071900196)平面同构测试算法的变种对齐。[Dimitrov
    等人 (2023)](https://arxiv.org/abs/2307.01180)首先将图分解为其双连通和三连通分量，然后学习节点、割节点、双连通分量和三连通分量的表示。这个过程通过使用可以在线性时间内计算的经典结构——块割树（Block-Cut
    Trees）和SPQR树（SPQR Trees）来实现。最终的框架被称为[PlanE](https://arxiv.org/abs/2307.01180)，包含如[BasePlanE](https://arxiv.org/abs/2307.01180)等架构。BasePlanE计算*同构完全图不变量*，因此它能够区分任意一对平面图。该工作的主要贡献是设计了一种架构，能够高效地学习平面图的完整不变量，同时保持在实际应用中的可扩展性。值得注意的是，3-FWL已知在平面图上是完全的（[Kiefer
    等人, 2019](https://dl.acm.org/doi/10.1145/3333003)），但该算法并不可扩展。'
- en: '**Aggregation functions: A uniform expressiveness study**'
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**聚合函数：统一表达性研究**'
- en: It was broadly argued that different aggregation functions have their place,
    but this had not been rigorously proven. In fact, in the non-uniform setup, sum
    aggregation with MLPs yields an injective mapping and as a result subsumes other
    aggregation functions ([Xu et al., 2020](https://arxiv.org/abs/1810.00826)), which
    builds on earlier results ([Zaheer et al., 2017](https://arxiv.org/abs/1703.06114)).
    The situation is different in the uniform setup, where one fixed model is required
    to work on *all* graphs. [Rosenbluth et al. (2023)](https://arxiv.org/abs/2302.11603)
    show that sum aggregation does not always subsume other aggregations in the uniform
    setup. If, for example, we consider an unbounded feature domain, sum aggregation
    networks cannot even approximate mean aggregation networks. Interestingly, even
    for the positive results, where sum aggregation is shown to approximate other
    aggregations, the presented constructions generally require a large number of
    layers (growing with the inverse of the approximation error).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 人们广泛认为不同的聚合函数各有其适用之处，但这并没有被严格证明。事实上，在非统一设置下，使用MLP的求和聚合会产生一个单射映射，因此可以涵盖其他聚合函数（[Xu
    等人, 2020](https://arxiv.org/abs/1810.00826)），这一结果建立在早期的研究基础上（[Zaheer 等人, 2017](https://arxiv.org/abs/1703.06114)）。而在统一设置下，情况则不同，这要求一个固定的模型能够在*所有*图上有效工作。[Rosenbluth
    等人 (2023)](https://arxiv.org/abs/2302.11603)表明，在统一设置中，求和聚合并不总是能够涵盖其他聚合方法。例如，如果我们考虑一个无限的特征域，求和聚合网络甚至无法近似均值聚合网络。有趣的是，即使是在一些正面的结果中，其中求和聚合被证明可以近似其他聚合方法，所呈现的构造通常也需要大量的层（随着近似误差的倒数增长）。
- en: '**Convergence and zero-one laws of GNNs on random graphs**'
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**GNN在随机图上的收敛性和零一法则**'
- en: GNNs can in principle be applied to graphs of any size following training. This
    makes an asymptotic analysis in the size of the input graphs very appealing. Previous
    studies of the asymptotic behaviour of GNNs have focused on convergence to theoretical
    limit networks ([Keriven et al., 2020](https://arxiv.org/abs/2006.01868)) and
    their stability under the perturbation of large graphs ([Levie et al., 2021](https://arxiv.org/abs/1907.12972)).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: GNN（图神经网络）理论上可以在训练后应用于任意大小的图。这使得对输入图的大小进行渐进分析变得非常有吸引力。此前对GNN渐进行为的研究，集中在它们对理论极限网络的收敛性（[Keriven
    等人, 2020](https://arxiv.org/abs/2006.01868)）以及在大图扰动下的稳定性（[Levie 等人, 2021](https://arxiv.org/abs/1907.12972)）上。
- en: 'In a recent study, [Adam-Day et al. (2023](https://arxiv.org/abs/2301.13060))
    proved a *zero-one law* for binary GNN classifiers. The question being tackled
    is the following: How do binary GNN classifiers behave as we draw Erdos-Rényi
    graphs of increasing size with random node features? The main finding is that
    the probability that such graphs are mapped to a particular output by a class
    of GNN classifiers tends to either zero or to one. That is, the model eventually
    maps either *all* graphs to zero or *all* graphs to one. This result applies to
    GCNs as well as to GNNs with sum and mean aggregation.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在最近的一项研究中，[Adam-Day 等人 (2023)](https://arxiv.org/abs/2301.13060) 证明了二进制 GNN
    分类器的*零一法则*。要解决的问题是：当我们绘制具有随机节点特征的 Erdos-Rényi 图，且图的大小逐渐增大时，二进制 GNN 分类器的表现如何？主要发现是，这些图通过一类
    GNN 分类器映射到特定输出的概率最终趋向于零或一。也就是说，模型最终将*所有*图映射到零或*所有*图映射到一。这个结果适用于 GCN 以及具有求和和均值聚合的
    GNN。
- en: 'The principal import of this result is that it establishes a novel *uniform*
    upper bound on the expressive power of GNNs: any property of graphs which can
    be uniformly expressed by these GNN architectures must obey a zero-one law. An
    example of a simple property which does not asymptotically tend to zero or one
    is that of having an even number of nodes.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这一结果的主要意义在于它为 GNNs 的表达能力建立了一个新的*统一*上界：任何可以由这些 GNN 架构统一表达的图的性质必须遵守零一法则。一个简单的性质示例是拥有偶数个节点，这个性质在渐近意义上既不趋向零也不趋向一。
- en: '**The descriptive complexity of GNNs**'
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**GNNs 的描述性复杂度**'
- en: '[Grohe (2023)](https://arxiv.org/abs/2303.04613) recently analysed the descriptive
    complexity of GNNs in terms of Boolean circuit complexity. The specific circuit
    complexity class of interest is TC0\. This class contains all languages which
    are decided by Boolean circuits with constant depth and polynomial size, using
    only AND, OR, NOT, and threshold [](https://en.wikipedia.org/wiki/Majority_gate)
    (or, majority) gates. [Grohe (2023)](https://arxiv.org/abs/2303.04613) proves
    that the graph functions that can be computed by a class of polynomial-size bounded-depth
    family of GNNs lie in the circuit complexity class TC0\. Furthermore, if the class
    of GNNs are allowed to use random node initialization and global readout as in
    [Abboud el al. (2020)](https://arxiv.org/abs/2010.01179) then there is a matching
    lower bound in that they can compute exactly the same functions that can be expressed
    in TC0\. This establishes an upper bound on the power of GNNs with random node
    features, by requiring the class of models to be of bounded depth (fixed #layers)
    and of size polynomial. While this result is still non-uniform, it improves the
    result of [Abboud el al. (2020)](https://arxiv.org/abs/2010.01179) where the construction
    can be worst-case exponential.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[Grohe (2023)](https://arxiv.org/abs/2303.04613) 最近分析了 GNNs 在布尔电路复杂度方面的描述性复杂度。具体的电路复杂度类别是
    TC0。该类别包含所有通过具有恒定深度和多项式大小的布尔电路决定的语言，这些电路仅使用 AND、OR、NOT 和阈值 [](https://en.wikipedia.org/wiki/Majority_gate)（或多数）门。[Grohe
    (2023)](https://arxiv.org/abs/2303.04613) 证明了可以由一类多项式大小有界深度的 GNN 家族计算的图函数属于电路复杂度类别
    TC0。此外，如果允许 GNN 类使用随机节点初始化和全局读取，如 [Abboud el al. (2020)](https://arxiv.org/abs/2010.01179)
    中所述，则存在一个匹配的下界，即它们可以计算出恰好可以用 TC0 表达的相同函数。这为具有随机节点特征的 GNN 的能力建立了上界，要求模型类别具有有界深度（固定层数）且大小为多项式。虽然这一结果仍然是非统一的，但它改善了
    [Abboud el al. (2020)](https://arxiv.org/abs/2010.01179) 的结果，在该结果中，构造可能是最坏情况下的指数级。'
- en: '**A fine-grained expressivity study of GNNs**'
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**GNNs 的细粒度表达性研究**'
- en: Numerous recent works have analyzed the expressive power of MPNNs, primarily
    utilizing combinatorial techniques such as the 1-WL for the graph isomorphism
    problem. However, the graph isomorphism objective is inherently binary, not giving
    insights into the degree of similarity between two given graphs. [Böker et al.
    (2023)](https://arxiv.org/abs/2306.03698) resolve this issue by deriving continuous
    extensions of both 1-WL and MPNNs to graphons. Concretely, they show that the
    continuous variant of 1-WL delivers an accurate topological characterization of
    the expressive power of MPNNs on graphons, revealing which graphs these networks
    can distinguish and the difficulty level in separating them. They provide a theoretical
    framework for graph and graphon similarity, combining various topological variants
    of classical characterizations of the 1-WL. In particular, they characterize the
    expressive power of MPNNs in terms of the tree distance, which is a graph distance
    based on the concept of fractional isomorphisms, and substructure counts via tree
    homomorphisms, showing that these concepts have the same expressive power as the
    1-WL and MPNNs on graphons. Interestingly, they also validated their theoretical
    findings by showing that randomly initialized MPNNs, without training, show competitive
    performance compared to their trained counterparts.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的许多研究分析了MPNNs的表达力，主要利用组合技术，如用于图同构问题的1-WL。然而，图同构目标本质上是二元的，无法深入了解两个给定图之间的相似度程度。[Böker
    et al. (2023)](https://arxiv.org/abs/2306.03698)通过推导1-WL和MPNNs的连续扩展到图论函数，解决了这个问题。具体而言，他们展示了1-WL的连续变体能够准确地对MPNNs在图论函数上的表达力进行拓扑特征化，揭示了这些网络能够区分哪些图，以及分离它们的难度级别。他们提供了一个图和图论函数相似度的理论框架，结合了1-WL经典特征化的各种拓扑变体。特别是，他们通过树距离来表征MPNNs的表达力，树距离是一种基于分数同构概念的图距离，以及通过树同态的子结构计数，表明这些概念与1-WL和MPNNs在图论函数上的表达力相同。有趣的是，他们还通过展示随机初始化的MPNNs（未经过训练）在与经过训练的对应模型相比时具有竞争力的性能，从而验证了他们的理论发现。
- en: '**Expressiveness results for Subgraph GNNs**'
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**子图GNNs的表达性结果**'
- en: Subgraph-based GNNs were already a big trend in 2022 ([Bevilacqua et al., 2022](https://arxiv.org/abs/2110.02910),
    [Qian et al., 2022](https://arxiv.org/abs/2206.11168)). This year, [Zhang et al.
    (2023b)](https://arxiv.org/abs/2302.07090) established more fine-grained expressivity
    results for such architectures. The paper investigates subgraph GNNs via the so-called
    Subgraph Weisfeiler-Leman Tests (SWL). Through this, they show a complete hierarchy
    of SWL with strictly growing expressivity. Concretely, they define equivalence
    classes for SWL-type algorithms and show that almost all existing subgraph GNNs
    fall in one of them. Moreover, the so-called SSWL achieves the maximal expressive
    power. Interestingly, they also relate SWL to several existing expressive GNNs
    architectures. For example, they show that SWL has the same expressivity as the
    local versions of 2-WL ([Morris et al., 2020](https://arxiv.org/abs/1904.01543)).
    In addition to theory, they also show that SWL-type architectures achieve good
    empirical results.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 基于子图的图神经网络（GNNs）在2022年已经成为一个大趋势 ([Bevilacqua et al., 2022](https://arxiv.org/abs/2110.02910),
    [Qian et al., 2022](https://arxiv.org/abs/2206.11168))。今年，[Zhang et al. (2023b)](https://arxiv.org/abs/2302.07090)为这种架构建立了更精细的表达力结果。论文通过所谓的子图Weisfeiler-Leman测试（SWL）研究了子图GNNs。通过这一方法，他们展示了SWL的完整层次结构，并且表达力严格增长。具体而言，他们为SWL类型的算法定义了等价类，并表明几乎所有现有的子图GNNs都属于其中之一。此外，所谓的SSWL达到了最大的表达力。令人感兴趣的是，他们还将SWL与几个现有的表达性GNN架构联系了起来。例如，他们展示了SWL具有与2-WL的局部版本相同的表达力
    ([Morris et al., 2020](https://arxiv.org/abs/1904.01543))。除了理论之外，他们还表明SWL类型的架构在实际应用中取得了良好的实证结果。
- en: '**Expressive power of architectures for link prediction on KGs**'
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**用于知识图谱链接预测的架构表达力**'
- en: The expressive power of architectures such as RGCN and CompGCN for link prediction
    on knowledge graphs has been studied by [Barceló et al. (2022)](https://arxiv.org/abs/2211.17113).
    This year, [Huang et al. (2023)](https://arxiv.org/abs/2302.02209) generalized
    these results to characterize the expressive power of various other model architectures.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 用于知识图谱链接预测的架构，如RGCN和CompGCN的表达力已被[Barceló et al. (2022)](https://arxiv.org/abs/2211.17113)研究。今年，[Huang
    et al. (2023)](https://arxiv.org/abs/2302.02209)将这些结果推广到表征各种其他模型架构的表达力。
- en: '![](../Images/aa233644e0a91a5ad4811e59bfe32f92.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa233644e0a91a5ad4811e59bfe32f92.png)'
- en: 'Figure from [Huang et al. (2023)](https://arxiv.org/abs/2302.02209): The figure
    compares the respective mode of operations in R-MPNNs and C-MPNNs.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 [Huang et al. (2023)](https://arxiv.org/abs/2302.02209) 的图：该图比较了 R-MPNNs
    和 C-MPNNs 在操作模式上的不同。
- en: '[Huang et al. (2023)](https://arxiv.org/abs/2302.02209) introduced the framework
    of conditional message passing networks ([C-MPNNs](https://arxiv.org/abs/2302.02209))
    which includes architectures such as [NBFNets](https://arxiv.org/abs/2106.06935).
    Classical relational message passing networks (R-MPNNs) are unary encoders (i.e.,
    encoding graph nodes) and rely on a binary decoder for the task of link prediction
    ([Zhang, 2021](https://arxiv.org/abs/2010.16103)). On the other hand, C-MPNNs
    serve as binary encoders (i.e., encoding pairs of graph nodes) and as a result,
    are more suitable for the binary task of link prediction. C-MPNNs are shown to
    align with a relational Weisfeiler-Leman algorithm that can be seen as a local
    approximation of 2WL. These findings explain the superior performance of NBFNets
    and alike over, e.g., RGCNs. [Huang et al. (2023)](https://arxiv.org/abs/2302.02209)
    also present uniform expressiveness results in terms of precise logical characterizations
    for the class of binary functions captured by C-MPNNs.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[Huang et al. (2023)](https://arxiv.org/abs/2302.02209) 提出了条件消息传递网络（[C-MPNNs](https://arxiv.org/abs/2302.02209)）的框架，其中包括如
    [NBFNets](https://arxiv.org/abs/2106.06935) 这样的架构。经典的关系消息传递网络（R-MPNNs）是单一编码器（即，编码图节点），并依赖于二元解码器进行链路预测任务（[Zhang,
    2021](https://arxiv.org/abs/2010.16103)）。另一方面，C-MPNNs 作为二元编码器（即，编码图节点对），因此更适合于链路预测的二元任务。C-MPNNs
    被证明与关系 Weisfeiler-Leman 算法对齐，可以看作是 2WL 的局部近似。这些发现解释了 NBFNets 等在性能上优于，例如，RGCNs。[Huang
    et al. (2023)](https://arxiv.org/abs/2302.02209) 还展示了 C-MPNNs 捕捉的二元函数类的精确逻辑特征，从而得出了统一的表达能力结果。'
- en: '**Over-squashing and expressivity**'
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**过度压缩与表达能力**'
- en: Over-squashing is a phenomenon originally described by [Alon & Yahav](https://arxiv.org/abs/2006.05205)
    in 2021 as the compression of exponentially-growing receptive fields into fixed-size
    vectors. Subsequent research ([Topping et al., 2022](https://arxiv.org/abs/2111.14522),
    [Di Giovanni et al., 2023](https://arxiv.org/abs/2302.02941), [Black et al., 2023](https://arxiv.org/abs/2302.06835),
    [Nguyen et al., 2023](https://arxiv.org/abs/2211.15779)) has characterised over-squashing
    through sensitivity analysis, proving that the dependence of the output features
    on hidden representations from earlier layers, is impaired by topological properties
    such as negative curvature or large commute time. Since the graph topology plays
    a crucial role in the formation of bottlenecks, *graph rewiring*, a paradigm shift
    elevating the graph connectivity to design factor in GNNs, has been proposed as
    a key strategy for alleviating over-squashing (if you are interested, see the
    Section on **Exotic Message Passing**below).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Over-squashing 是 [Alon & Yahav](https://arxiv.org/abs/2006.05205) 在2021年首次描述的一种现象，指的是指数增长的感受野被压缩为固定大小的向量。后续研究（[Topping
    et al., 2022](https://arxiv.org/abs/2111.14522), [Di Giovanni et al., 2023](https://arxiv.org/abs/2302.02941),
    [Black et al., 2023](https://arxiv.org/abs/2302.06835), [Nguyen et al., 2023](https://arxiv.org/abs/2211.15779)）通过灵敏度分析对过度压缩进行了表征，证明了输出特征对早期层的隐藏表示的依赖性会因拓扑性质（如负曲率或长时间通勤）而受到影响。由于图的拓扑在瓶颈形成中起着关键作用，*图重连线*（graph
    rewiring）作为一种范式转变，提升了图连接性在GNN设计中的作用，被提出作为缓解过度压缩的关键策略（如果你感兴趣，参见下面关于**异构消息传递**的部分）。
- en: '![](../Images/d769b18fc6f274c8abfbebf54baa83f2.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d769b18fc6f274c8abfbebf54baa83f2.png)'
- en: 'For the given graph, the MPNN learns stronger mixing (tight springs) for nodes
    (v, u) and (u, w) since their commute time is small, while nodes (u, q) and (u,
    z), with high commute-time, have weak mixing (loose springs). Source: [Di Giovanni
    et al., 2023](https://arxiv.org/abs/2306.03589)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的图，MPNN 为节点（v，u）和（u，w）学习到更强的混合（紧弹簧），因为它们的通勤时间较短，而节点（u，q）和（u，z）由于较长的通勤时间，具有较弱的混合（松弛弹簧）。来源：[Di
    Giovanni et al., 2023](https://arxiv.org/abs/2306.03589)
- en: 'Over-squashing is an obstruction to the expressive power, for it causes GNNs
    to falter in tasks with long-range interactions. To formally study this, [Di Giovanni
    et al., 2023](https://arxiv.org/abs/2306.03589) introduce a new metric of expressivity,
    referred to as “mixing”, which encodes the joint and nonlinear dependence of a
    graph function on pairs of nodes’ features: for a GNN to approximate a function
    with large mixing, a necessary condition is allowing “strong” message exchange
    between the relevant nodes. Hence, they postulate to measure over-squashing through
    the mixing of a GNN prediction, and prove that the depth required by a GNN to
    induce enough mixing, *as required by the task*, grows with the commute time —
    typically much worse than the shortest-path distance. The results show how over-squashing
    hinders the expressivity of GNNs with “practical” size, and validate that it arises
    from the misalignment between the task (requiring strong mixing between nodes
    i and j) and the topology (inducing large commute time between i and j).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 过度挤压是限制表达能力的障碍，因为它导致GNNs在处理具有长程交互的任务时表现不佳。为了正式研究这一问题，[Di Giovanni等人，2023](https://arxiv.org/abs/2306.03589)提出了一种新的表达能力度量，称为“混合”，它编码了图函数对节点特征对的联合和非线性依赖关系：为了使GNN能够近似具有较大混合的函数，必要的条件是允许相关节点之间进行“强”的信息交换。因此，他们假设通过GNN预测的混合来衡量过度挤压，并证明了GNN需要的深度来引入足够的混合，*如任务所需*，随着通勤时间的增长——通常比最短路径距离要差得多。结果显示，过度挤压如何妨碍具有“实际”大小的GNN的表达能力，并验证了它源于任务（要求节点i和j之间的强混合）与拓扑（导致i和j之间的长通勤时间）之间的不匹配。
- en: The “mixing” of a function pertains to the exchange of information between nodes,
    whatever this information is, and not to its capacity to separate node representations.
    In fact, these results [](https://arxiv.org/abs/2306.03589) also hold for GNNs
    more powerful than the 1-WL test. The analysis in [Di Giovanni et al., (2023)](https://arxiv.org/abs/2306.03589)
    offers an alternative approach for studying the expressivity of GNNs, which easily
    extends to equivariant GNNs in 3D space and their ability to model interactions
    between nodes.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的“混合”涉及节点之间信息的交换，无论这些信息是什么，而不是其分离节点表示的能力。事实上，这些结果 [](https://arxiv.org/abs/2306.03589)
    同样适用于比1-WL测试更强大的GNNs。[Di Giovanni等人（2023）](https://arxiv.org/abs/2306.03589)的分析提供了一种替代方法来研究GNNs的表达能力，该方法可以轻松扩展到3D空间中的等变GNNs以及它们建模节点之间交互的能力。
- en: '**Generalization and extrapolation capabilities of GNNs**'
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**GNN的泛化和外推能力**'
- en: The expressive power of MPNNs has achieved a lot of attention in recent years
    through its connection to the WL test. While this connection has led to significant
    advances in understanding and enhancing MPNNs’ expressive power ([Morris et al,
    2023a](https://arxiv.org/abs/2301.11039)), it does not provide insights into their
    generalization performance, i.e., their ability to make meaningful predictions
    beyond the training set. Surprisingly, only a few notable contributions study
    MPNNs’ generalization behaviors, e.g., [Garg et al. (2020](https://arxiv.org/abs/2002.06157)),
    [Kriege et al. (2018)](https://www.ijcai.org/proceedings/2018/0325.pdf), [Liao
    et al. (2021)](https://arxiv.org/abs/2012.07690), [Maskey et al. (2022)](https://arxiv.org/abs/2202.00645),
    [Scarselli et al. (2018)](https://pubmed.ncbi.nlm.nih.gov/30219742/). However,
    these approaches express MPNNs’ generalization ability using only classical graph
    parameters, e.g., maximum degree, number of vertices, or edges, which cannot fully
    capture the complex structure of real-world graphs. Further, most approaches study
    generalization in the non-uniform regime, i.e., assuming that the MPNNs operate
    on graphs of a pre-specified order.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，MPNNs（消息传递神经网络）的表达能力因其与WL测试的关联而引起了广泛关注。尽管这种关联促成了在理解和提升MPNNs表达能力方面的显著进展（[Morris等，2023a](https://arxiv.org/abs/2301.11039)），但它并未提供关于其泛化性能的见解，即它们在训练集之外进行有意义预测的能力。令人惊讶的是，只有少数几项重要的研究探讨了MPNNs的泛化行为，例如，[Garg等（2020）](https://arxiv.org/abs/2002.06157)，[Kriege等（2018）](https://www.ijcai.org/proceedings/2018/0325.pdf)，[Liao等（2021）](https://arxiv.org/abs/2012.07690)，[Maskey等（2022）](https://arxiv.org/abs/2202.00645)，[Scarselli等（2018）](https://pubmed.ncbi.nlm.nih.gov/30219742/)。然而，这些方法仅使用经典的图参数来表示MPNNs的泛化能力，例如最大度数、顶点数或边数，这些方法无法完全捕捉现实世界图的复杂结构。此外，大多数方法在非均匀区域研究泛化，即假设MPNNs在预定阶数的图上进行操作。
- en: '![](../Images/b5b9c2a9751ff78b44277f541166bbf8.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5b9c2a9751ff78b44277f541166bbf8.png)'
- en: 'Figure from [Morris et al. (2023b)](https://arxiv.org/abs/2301.11039): Overview
    of the generalization capabilities of MPNNs and their link to the 1-WL.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[Morris等人(2023b)](https://arxiv.org/abs/2301.11039)的图：MPNNs的泛化能力概览及其与1-WL的联系。
- en: Hence, [Morris et al. (2023b)](https://arxiv.org/abs/2301.11039) showed a tight
    connection between the expressive power of the 1-WL and generalization performance.
    They investigate the influence of graph structure and the parameters’ encoding
    lengths on MPNNs’ generalization by tightly connecting 1-WL’s expressivity and
    MPNNs’ Vapnik–Chervonenkis (VC) dimension. To that, they show several results.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，[莫里斯等人 (2023b)](https://arxiv.org/abs/2301.11039)展示了1-WL的表达能力与泛化性能之间的紧密联系。他们通过紧密连接1-WL的表达能力和MPNNs的Vapnik–Chervonenkis
    (VC)维度，研究了图结构和参数编码长度对MPNNs泛化能力的影响。为此，他们展示了几个结果。
- en: 1️⃣ First, in the non-uniform regime, they show that MPNNs’ VC dimension depends
    tightly on the number of equivalence classes computed by the 1-WL over a set of
    graphs. In addition, their results easily extend to the k-WL and many recent expressive
    MPNN extensions.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ 首先，在非均匀状态下，他们展示了MPNNs的VC维度与1-WL在一组图上计算出的等价类数量紧密相关。此外，他们的结果可以轻松推广到k-WL和许多最近的表达性MPNN扩展。
- en: 2️⃣ In the uniform regime, i.e., when graphs can have arbitrary order, they
    show that MPNNs’ VC dimension is lower and upper bounded by the largest bitlength
    of its weights. In both the uniform and non-uniform regimes, MPNNs’ VC dimension
    depends logarithmically on the number of colors computed by the 1-WL and polynomially
    on the number of parameters. Moreover, they also empirically show that their theoretical
    findings hold in practice to some extent.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 在均匀状态下，即当图可以具有任意顺序时，他们展示了MPNNs的VC维度受其权重的最大比特长度的上下界限制。在均匀和非均匀状态下，MPNNs的VC维度对1-WL计算的颜色数呈对数关系，并且对参数的数量呈多项式关系。此外，他们还通过实证研究表明，他们的理论发现一定程度上在实践中成立。
- en: 🔮 Predictions time!
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 🔮 预测时间！
- en: '***Christopher Morris (RWTH Aachen)***'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '***克里斯托弗·莫里斯 (亚琛工业大学)***'
- en: “I believe that there is a pressing need for a better and more practical theory
    of generalization of GNNs. ” — **Christopher Morris** (RWTH Aachen)
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我相信，GNNs泛化理论亟需更好且更实用的理论。” — **克里斯托弗·莫里斯** (亚琛工业大学)
- en: ➡️ For example, we need to understand how graph structure and various architectural
    parameters influence generalization. Moreover, the dynamics of SGD for training
    GNNs are currently understudied and not well understood, and more works will study
    this.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 例如，我们需要理解图结构和各种架构参数如何影响泛化。此外，当前对于训练GNNs的SGD动态研究较少，理解也不够透彻，更多的研究将会探讨这一问题。
- en: '***İsmail İlkan Ceylan (Oxford)***'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '***伊斯梅尔·伊尔坎·杰兰 (牛津大学)***'
- en: “I hope to see more expressivity results in the uniform setting, where we fix
    the parameters of a neural network and examine its capabilities.” — **İsmail İlkan
    Ceylan** (Oxford)
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我希望在均匀设置下看到更多的表达能力研究，在这种情况下我们固定神经网络的参数并检查其能力。” — **伊斯梅尔·伊尔坎·杰兰** (牛津大学)
- en: ➡️ In this case, we can identify a better connection to generalization, because
    if a property cannot be expressed uniformly then the model cannot generalise to
    larger graph sizes.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 在这种情况下，我们可以识别出更好的泛化联系，因为如果某个属性无法均匀表达，那么模型就无法对更大的图大小进行泛化。
- en: ➡️ This year, we may also see expressiveness studies that target graph regression
    or graph generation, which remain under-explored. There are good reasons to hope
    for learning algorithms which are isomorphism-complete on larger graph classes,
    strictly generalizing the results for planar graphs.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 今年，我们也可能会看到针对图回归或图生成的表达能力研究，这些领域仍然未被充分探索。人们有充分的理由期望出现在更大的图类别上具有同构完备性的学习算法，从而严格地推广平面图的结果。
- en: ➡️ It is also time to develop a theory for learning with fully relational data
    (i.e., knowledge hypergraphs), which will unlock applications in relational databases!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 现在也是时候为学习完全关联数据（即知识超图）发展一套理论了，这将解锁关系数据库中的应用！
- en: '***Francesco Di Giovanni (Oxford)***'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '***弗朗切斯科·迪·乔瓦尼 (牛津大学)***'
- en: In terms of future theoretical developments of GNNs, I can see two directions
    that deserve attention.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在GNNs的未来理论发展方面，我看到两个值得关注的方向。
- en: “There is very little understanding of the dynamics of the weights of a GNN
    under gradient flow (or SGD); assessing the impact of the graph topology on the
    evolution of the weights is key to addressing questions about generalisation and
    hardness of a task.” — Francesco Di Giovanni (Oxford)
  id: totrans-96
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我们对GNN在梯度流（或SGD）下的权重动态几乎没有理解；评估图拓扑对权重演变的影响是解决泛化和任务难度问题的关键。” — 弗朗切斯科·迪·乔瓦尼（牛津大学）
- en: ➡️ Second, I believe it would be valuable to develop alternative paradigms of
    expressivity, which more directly focus on approximation power (of graph functions
    and their derivatives) and identify precisely the tasks which are hard to learn.
    The latter direction could also be particularly meaningful for characterising
    the power of equivariant GNNs in 3D space, where measurements of expressivity
    might need to be decoupled from the 2D case in order to be better aligned with
    tasks coming from the scientific domain.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 其次，我认为发展表现力的替代范式会很有价值，这些范式更直接地关注近似能力（图函数及其导数的近似能力），并精确地识别出哪些任务难以学习。后一方向在表征三维空间中等变GNN的能力时可能特别有意义，在这种情况下，表现力的度量可能需要与二维情况解耦，以便更好地与来自科学领域的任务对接。
- en: 'At the end: a fun fact about where WL went in 2023'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 结尾：关于WL在2023年去向的一个有趣事实
- en: '![](../Images/c96ff616c959c6a3f3e2be22a280b3af.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c96ff616c959c6a3f3e2be22a280b3af.png)'
- en: 'Portraits: Ihor Gorsky'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 人物肖像：伊戈尔·戈尔斯基
- en: '**Predictions from the 2023 post**'
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2023年预测**'
- en: (1) More efforts on creating time- and memory-efficient subgraph GNNs.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 在创建时间和内存高效的子图GNN方面投入更多努力
- en: ❌ not really
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❌ 并不完全是
- en: (2) Better understanding of generalization of GNNs
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 更好地理解GNN的泛化能力
- en: ✅ yes, see the subsections on oversquashing and generalization
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ✅ 是的，见关于过度压缩和泛化的子章节
- en: (3) Weisfeiler and Leman visit 10 new places!
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 费斯费尔和利曼访问了10个新地方！
- en: ❌ (4 so far) [Grammatical](https://openreview.net/forum?id=eZneJ55mRO), [indifferent](https://arxiv.org/abs/2311.01205),
    [measurement modeling](https://arxiv.org/abs/2307.05775), [paths](https://arxiv.org/abs/2308.06838)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ❌（到目前为止，4篇）[语法问题](https://openreview.net/forum?id=eZneJ55mRO)，[无所谓](https://arxiv.org/abs/2311.01205)，[测量建模](https://arxiv.org/abs/2307.05775)，[路径](https://arxiv.org/abs/2308.06838)
- en: New and exotic message passing
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 新颖且独特的消息传递
- en: '*Ben Finkelshtein (Oxford), Francesco Di Giovanni (Oxford), Petar Veličković
    (Google DeepMind)*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*本·芬克尔施坦（牛津大学），弗朗切斯科·迪·乔瓦尼（牛津大学），佩塔尔·维利奇科维奇（谷歌 DeepMind）*'
- en: '***Petar Veličković (Google DeepMind)***'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '***佩塔尔·维利奇科维奇（谷歌 DeepMind）***'
- en: 'Over the years, it has become part of common folklore that the development
    of message passing operators has saturated. What I find particularly exciting
    about the progress made in 2023 is that, from several independent research groups
    (including our own), a unified novel direction has emerged: let’s start considering
    the impact of ***time*** in the GNN ⏳.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，关于消息传递算子的开发已经饱和，这已经成为一种常见的民间传说。我认为2023年取得的进展特别令人兴奋，因为来自几个独立研究小组（包括我们自己）的成果表明，一个统一的全新方向已经出现：让我们开始考虑***时间***在GNN中的影响
    ⏳。
- en: “I forecast that, in 2024, time will assume a central role in the development
    of novel GNN architectures.” — Petar Veličković (Google DeepMind)
  id: totrans-112
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我预测，在2024年，时间将在新型GNN架构的发展中扮演核心角色。” — 佩塔尔·维利奇科维奇（谷歌 DeepMind）
- en: 💡 Time has already been leveraged in GNN design when it is explicitly provided
    in the input (in spatiotemporal or fully dynamic graphs). This year, it has started
    to feature in research of GNN operators on *static* graph inputs. Several works
    are dropping the assumption of a unified, synchronised clock ⏱️ which forces all
    messages in a layer to be sent and received at once.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 💡 在GNN设计中，时间已经在显式提供的输入中得到了利用（如时空图或完全动态图）。今年，它开始出现在关于*静态*图输入的GNN操作符研究中。一些研究开始不再假设一个统一的、同步的时钟
    ⏱️，该时钟迫使所有信息在同一层中同时发送和接收。
- en: 1️⃣ The first such work, [GwAC](https://openreview.net/forum?id=zffXH0sEJP)
    🥑, only played with rudimentary randomised message scheduling, but provided **proofs**
    for why such processing might yield significant improvements in expressive power.
    [Co-GNNs](https://arxiv.org/abs/2310.01267) 🤝 carry the torch further, demonstrating
    a more elaborate and fine-tuned message scheduling mechanism which is node-centric,
    allowing each node to choose when to send 📨 or receive 📬 messages. Co-GNNs also
    provide a practical method for training such schedulers by gradient descent. While
    the development of such asynchronous GNN models is highly desirable, we must also
    acknowledge the associated scalability issues — our present frontier hardware
    is not designed to efficiently scale such sequential systems.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ 第一篇此类工作，[GwAC](https://openreview.net/forum?id=zffXH0sEJP) 🥑，仅仅玩弄了基础的随机化消息调度，但提供了**证明**，说明为什么这种处理方式可能在表达能力上带来显著提升。[Co-GNNs](https://arxiv.org/abs/2310.01267)
    🤝 进一步推动了这一进程，展示了一种更加精细调整的消息调度机制，它以节点为中心，允许每个节点选择何时发送 📨 或接收 📬 消息。Co-GNNs还提供了一种通过梯度下降训练这种调度器的实用方法。虽然开发这种异步GNN模型是非常期望的，但我们也必须承认与之相关的可扩展性问题——我们当前的前沿硬件并不设计为能够高效扩展此类顺序系统。
- en: 2️⃣ In our own work on [asynchronous algorithmic alignment](https://openreview.net/forum?id=ba4bbZ4KoF),
    we instead opt to design a *synchronous* GNN, but **constrain** its message, aggregation,
    and update functions such that the GNN would yield identical embeddings even if
    parts of its dataflow were made asynchronous. This led us to an exciting journey
    through monoids, 1-cocycles, and category theory, resulting in a scalable GNN
    model that achieves superior performance on many CLRS-30 tasks.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 在我们关于[异步算法对齐](https://openreview.net/forum?id=ba4bbZ4KoF)的研究中，我们选择设计一个*同步*
    GNN，但**限制**其消息传递、聚合和更新函数，使得即使其数据流的部分变为异步，GNN也能产生相同的嵌入。这带领我们进入了一个激动人心的旅程，涉及到单群、1-余切和范畴理论，最终得出了一个可扩展的GNN模型，在许多CLRS-30任务上表现出色。
- en: '![](../Images/95b2b7c260ab4af9b85e305ef66c3ca4.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/95b2b7c260ab4af9b85e305ef66c3ca4.png)'
- en: 'A possible execution trace of an asynchronous GNN. While traditional GNNs send
    and receive all messages synchronously, under our framework, at any step the GNN
    may choose to execute any number of possible operations (depicted here with a
    collection on the right side of the graph). Source: [Dudzik et al.](https://openreview.net/forum?id=ba4bbZ4KoF)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一个异步GNN的可能执行轨迹。传统的GNN同步发送和接收所有消息，而在我们的框架下，在任何步骤，GNN可以选择执行任意数量的可能操作（这里通过图的右侧集合来表示）。来源：[Dudzik
    et al.](https://openreview.net/forum?id=ba4bbZ4KoF)
- en: ➡️ Lastly, it is worth noting that for certain special choices of message scheduling,
    we do not need to make modifications to synchronous GNNs’ architecture — and may
    instead resort to dynamic graph rewiring. [DREW](https://arxiv.org/abs/2305.08018)
    and [Half-Hop](https://openreview.net/forum?id=lXczFIwQkv) are two concurrently
    published papers at ICML’23 which embody the principle of using graph rewiring
    to *slow down* message passing 🐌. In DREW, a message from each node is actually
    sent to every other node, but it takes *k* layers before a message will reach
    a neighbour that is *k* hops away! Half-Hop, on the other hand, takes a more lenient
    approach, and just randomly decides whether or not to introduce a “slow node”
    which extends the path between any two nodes connected by an edge. Both approaches
    naturally alleviate the oversmoothing problem, as messages travelling longer distances
    will oversmooth less.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 最后，值得注意的是，对于某些特定的消息调度选择，我们不需要修改同步GNN的架构——而可以转而使用动态图重连接。[DREW](https://arxiv.org/abs/2305.08018)
    和[Half-Hop](https://openreview.net/forum?id=lXczFIwQkv)是两篇在ICML'23上同时发表的论文，它们体现了使用图重连接来*减缓*消息传递🐌的原理。在DREW中，每个节点的消息实际上会发送到每一个其他节点，但需要*k*层才能到达一个与其相隔*k*跳的邻居！而Half-Hop则采取更宽松的方式，随机决定是否引入一个“慢节点”，从而延长任何通过边连接的两个节点之间的路径。两种方法都能自然缓解过平滑问题，因为消息传递的距离越远，过平滑的程度越小。
- en: Whether it is used for message passing design, GNN dataflow or graph rewiring,
    in 2023 we have just started to grasp the importance of *time* — even when time
    variation is not explicitly present in our dataset.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是用于消息传递设计、GNN数据流还是图重连接，到了2023年，我们才刚刚开始理解*时间*的重要性——即便时间变化在我们的数据集中并未显式存在。
- en: '***Ben Finkelshtein (Oxford)***'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '***Ben Finkelshtein (牛津大学)***'
- en: The time-dependent message passing paradigm presented in [Co-GNNs](https://arxiv.org/abs/2310.01267)
    is a learnable generalisation of message passing, which allows each node to decide
    how to propagate information from or to its neighbours, thus enabling a more flexible
    flow of information. The nodes are regarded as players that can either broadcast
    to neighbors that listen *and* listen to neighbors that broadcast (like in classical
    message-passing), Broadcast to neighbors that listen, or Isolate (neither listen
    nor broadcast).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在[Co-GNNs](https://arxiv.org/abs/2310.01267)中提出的基于时间的消息传递范式是一种可学习的消息传递泛化方式，它允许每个节点决定如何从其邻居传递信息或向其邻居传播信息，从而实现信息流动的更大灵活性。这些节点被视为可以进行以下操作的参与者：向监听的邻居广播*并且*监听广播的邻居（类似于经典的消息传递），仅向监听的邻居广播，或者隔离（既不监听也不广播）。
- en: The interplay between these actions and the ability to change them locally and
    dynamically allows CoGNNs to determine a **task-specific** computational graph
    (which can be considered as a form of **dynamic** and **directed rewiring**, learn
    different action distribution for two nodes with different node features (both
    **feature-** and **structure-based)**.CoGNNs allow **asynchronous** updates across
    nodes and also yield unique node identifiers with high probability, which allows
    them to distinguish any pair of graphs (**more expressive than 1-WL**, at the
    expense of equivariance holding only in expectation).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作之间的相互作用以及局部和动态地改变它们的能力使CoGNN能够确定一个**任务特定**的计算图（可以视为一种**动态**和**定向重连接**的形式），为具有不同节点特征的两个节点学习不同的操作分布（包括**特征**和**结构**基础的）。CoGNN还允许节点之间进行**异步**更新，并且以较高的概率生成唯一的节点标识符，从而使它们能够区分任何一对图（**比1-WL更具表现力**，但仅在期望下保持等变性）。
- en: '![](../Images/832de2447364cc0ce241d969ef16d4ff.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/832de2447364cc0ce241d969ef16d4ff.png)'
- en: 'Left to right: classical MPNNs (all nodes broadcast & listen), DeepSets (all
    nodes isolate), and generic CoGNNs. Figure from [blog post](/co-operative-graph-neural-networks-34c59bf6805e?gi=98ca39c38e41).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 从左到右：经典的MPNN（所有节点都广播和监听）、DeepSets（所有节点都隔离）以及通用的CoGNN。图源自[博客文章](/co-operative-graph-neural-networks-34c59bf6805e?gi=98ca39c38e41)。
- en: 'Check the Medium post for more details:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 查看Medium文章了解更多详情：
- en: '[](/co-operative-graph-neural-networks-34c59bf6805e?source=post_page-----3af5d38376e1--------------------------------)
    [## Co-operative Graph Neural Networks'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/co-operative-graph-neural-networks-34c59bf6805e?source=post_page-----3af5d38376e1--------------------------------)
    [## 协作图神经网络'
- en: A new message-passing paradigm where every node can choose to either ‘listen’,
    ‘broadcast’, ‘listen & broadcast’ or…
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一种新的消息传递范式，其中每个节点可以选择“监听”、“广播”、“监听并广播”或...
- en: towardsdatascience.com](/co-operative-graph-neural-networks-34c59bf6805e?source=post_page-----3af5d38376e1--------------------------------)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/co-operative-graph-neural-networks-34c59bf6805e?source=post_page-----3af5d38376e1--------------------------------)'
- en: '***Francesco Di Giovanni (Oxford)***'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '***Francesco Di Giovanni（牛津大学）***'
- en: “The understanding of over-squashing, arising when the task depends on the interaction
    between nodes with large commute time, acted as a catalyst for the emergence of
    graph rewiring as a valid approach for designing new GNNs.” — **Francesco Di Giovanni**
    (Oxford)
  id: totrans-130
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “过度压缩（over-squashing）的理解，尤其是当任务依赖于节点间长时间交互时，成为了图重连接作为一种有效方法的催化剂，用以设计新的GNN。”
    — **Francesco Di Giovanni**（牛津大学）
- en: ️💡 *Graph rewiring* broadly entails altering the connectivity of the input graph
    to facilitate the solution of the downstream task. Recently, this has often targeted
    bottlenecks in the graph, thereby adding (and removing) edges to improve the flow
    of information. While the emphasis has been on **where** messages are exchanged,
    recent works (discussed above) have shed light on the relevance of **when** messages
    should be exchanged as well. One rationale behind these approaches, albeit often
    implicit, is that the hidden representations built by the layers of a GNN, provide
    the graph with an (artificially) *dynamic* component, even though the graph and
    input features are static. This perspective can be leveraged in several ways.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ️💡 *图结构重连接*广义上指的是改变输入图的连接方式，以促进下游任务的解决。最近，这通常聚焦于图中的瓶颈，从而添加（或移除）边缘，以改善信息流动。虽然重点通常放在**信息交换的地点**，但最近的研究（如上所述）也揭示了**何时**进行信息交换的重要性。这些方法背后的一个逻辑，尽管通常是隐含的，就是GNN各层构建的隐藏表示为图提供了一个（人为的）*动态*组件，即便图和输入特征是静态的。这个视角可以通过多种方式进行利用。
- en: '![](../Images/5e8ada337159c1aa6df710ba9f895619.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e8ada337159c1aa6df710ba9f895619.png)'
- en: 'In the classicical MPNN setting, at every layer information only travels from
    a node to its immediate neighbours. In DRew, the graph changes based on the layer,
    with newly added edges connecting nodes at distance r from layer r − 1 onward.
    Finally, in νDRew, we also introduce a delay mechanism equivalent to skip-connections
    between different nodes based on their mutual distance. Source: [Gutteridge et
    al.](https://arxiv.org/abs/2305.08018)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在经典的MPNN设置中，在每一层，信息仅在节点及其直接邻居之间传播。在DRew中，图会随着层数的变化而变化，新添加的边从第 *r* 层起连接距离 *r*
    的节点。最后，在νDRew中，我们还引入了一个延迟机制，相当于基于节点之间的相互距离的跳跃连接。来源：[Gutteridge等人](https://arxiv.org/abs/2305.08018)
- en: '➡️ One framework that has particularly embraced such an angle is [**DRew**](https://arxiv.org/abs/2305.08018),
    which extends any message-passing model in two ways: (i) it connects nodes at
    distance *r* directly, but only from layer *r* onwards; (ii) when nodes are connected,
    a delay is applied to their message exchange, based on their mutual distance.
    As the figure above illustrates, (i) allows the network to better retain the inductive
    bias, as nodes that are closer, interact *earlier;* (ii) instead acts as *distance-aware*
    *skip connections,* thereby facilitating the propagation of gradients for the
    loss. Most likely, it is for this reason, and not prevention of over-smoothing
    (which hardly has an impact for graph-level tasks), that the framework significantly
    enhances the performance of standard GNNs at larger depths (more details can be
    found in this [blog post](/dynamically-rewired-delayed-message-passing-gnns-2d5ff18687c2)).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 一个特别采纳这一角度的框架是[**DRew**](https://arxiv.org/abs/2305.08018)，它通过两种方式扩展了任何消息传递模型：（i）它直接连接距离
    *r* 的节点，但仅从第 *r* 层开始；（ii）当节点连接时，基于它们之间的相互距离，应用延迟到它们的消息交换。如上图所示，（i）使网络更好地保留归纳偏差，因为较近的节点会*更早*互动；（ii）则充当*距离感知的*
    *跳跃连接*，从而有助于损失函数的梯度传播。很可能正是由于这个原因，而不是防止过度平滑（对图级任务几乎没有影响），该框架显著增强了标准GNN在更大深度下的表现（更多细节可以参考这篇[博客文章](/dynamically-rewired-delayed-message-passing-gnns-2d5ff18687c2)）。
- en: '**🔮 Predictions:** I believe that the deep implications of extending message-passing
    over the “time” component would start to emerge in the coming year. Works like
    DRew have only scratched the surface of why rewiring over time (beyond space)
    might benefit the training of GNNs, drastically affecting their accuracy response
    across different depth regimes.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**🔮 预测：** 我相信，扩展消息传递到“时间”维度的深远影响将在来年开始显现。像DRew这样的工作仅仅触及了时间上重连（超越空间）的原因，它可能有助于GNN训练，显著影响它们在不同深度层次下的准确性响应。'
- en: ➡️ More broadly, I hope that theoretical and practical developments of graph
    rewiring could be translated into scientific domains, where equivariant GNNs are
    often applied to 3D problems which either do not have a natural graph structure
    (making the question of “where” messages should be exchanged ever more relevant)
    or (and) exhibit natural temporal (multi-scale) properties (making the question
    of “when” messages should be exchanged likely to be key for reducing memory constraints
    and retaining the right inductive bias).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 更广泛地说，我希望图重连的理论和实践发展能够被应用到科学领域，其中等变图神经网络（GNNs）通常应用于三维问题，这些问题要么没有自然的图结构（使得“在哪里”交换消息变得更加相关），要么（且）展现出自然的时间（多尺度）特性（使得“何时”交换消息可能是减少内存限制并保持正确归纳偏差的关键）。
- en: Geometry, Topology, Geometric Algebras & PDEs
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 几何学、拓扑学、几何代数与偏微分方程
- en: '*Johannes Brandstetter (JKU Linz), Michael Galkin (Intel), Mathilde Papillon
    (UC Santa Barbara), Bastian Rieck (Helmholtz & TUM), and David Ruhe (U Amsterdam)*'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '*Johannes Brandstetter（JKU Linz），Michael Galkin（Intel），Mathilde Papillon（UC
    Santa Barbara），Bastian Rieck（Helmholtz & TUM），David Ruhe（U Amsterdam）*'
- en: '2023 brought the most comprehensive introduction to (and a survey of) Geometric
    GNNs covering the most basic and necessary concepts with a handful of examples:
    **A Hitchhiker’s Guide to Geometric GNNs for 3D Atomic Systems** ([Duval, Mathis,
    Joshi, Schmidt, et al.](https://arxiv.org/abs/2312.07511)). If you ever wanted
    to learn from scratch the core architectures powering recent breakthroughs of
    graph ML in protein design, material discovery, molecular simulations, and more
    — this is what you need!'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年带来了最全面的几何图神经网络（Geometric GNNs）介绍（及综述），涵盖了最基础和必要的概念，并提供了一些示例：《**三维原子系统的几何图神经网络指南**》([Duval,
    Mathis, Joshi, Schmidt等](https://arxiv.org/abs/2312.07511))。如果你曾经想从零开始学习驱动最近蛋白质设计、材料发现、分子模拟等突破的图机器学习核心架构——这正是你需要的！
- en: '![](../Images/93b169f6f151d0760b6c3005dcaf3a49.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93b169f6f151d0760b6c3005dcaf3a49.png)'
- en: 'Timeline of key Geometric GNNs for 3D atomic systems, characterised by the
    type of intermediate representations within layers. Source: [Duval, Mathis, Joshi,
    Schmidt, et al.](https://arxiv.org/abs/2312.07511)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 3D原子系统的关键几何GNN时间线，按层内中间表示的类型进行分类。来源：[Duval, Mathis, Joshi, Schmidt 等人](https://arxiv.org/abs/2312.07511)
- en: '**Topology**'
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**拓扑**'
- en: '💡 Working with topological structures in 2023 has become much easier for both
    researchers and practitioners thanks to the amazing efforts of the [PyT team](https://github.com/pyt-team)
    and their suite of resources: **TopoNetX**, **TopoModelX**, and **TopoEmbedX**.
    [TopoNetX](https://github.com/pyt-team/TopoNetX) is pretty much the networkx for
    topological data. TopoNetX supports standard structures like cellular complexes,
    simplicial complexes, and combinatorial complexes. [TopoModelX](https://github.com/pyt-team/TopoModelX)
    is a PyG-like library for deep learning on topological data and implements famous
    models like [MPSN](https://arxiv.org/abs/2103.03212) and [CIN](https://arxiv.org/abs/2106.12575)
    with a neat unified interface (the original PyG implementations are quite tangled).
    [TopoEmbedX](https://github.com/pyt-team/TopoEmbedX) helps to train embedding
    models on topological data and supports core algorithms like [Cell2Vec](https://arxiv.org/abs/2010.00743).'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 💡 得益于[PyT团队](https://github.com/pyt-team)的卓越努力和他们的一系列资源，2023年研究人员和实践者在处理拓扑结构时变得更加容易：**TopoNetX**、**TopoModelX**
    和 **TopoEmbedX**。[TopoNetX](https://github.com/pyt-team/TopoNetX)基本上是拓扑数据的networkx。TopoNetX支持标准结构，如细胞复形、单纯形复形和组合复形。[TopoModelX](https://github.com/pyt-team/TopoModelX)是一个类似PyG的库，用于拓扑数据上的深度学习，并实现了著名的模型如[MPSN](https://arxiv.org/abs/2103.03212)和[CIN](https://arxiv.org/abs/2106.12575)，并提供了一个简洁统一的接口（原始的PyG实现比较复杂）。[TopoEmbedX](https://github.com/pyt-team/TopoEmbedX)有助于在拓扑数据上训练嵌入模型，并支持像[Cell2Vec](https://arxiv.org/abs/2010.00743)这样的核心算法。
- en: '![](../Images/39ecb150b3d75cdea28e11b8d544628d.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/39ecb150b3d75cdea28e11b8d544628d.png)'
- en: 'Domains: Nodes in blue, (hyper)edges in pink, and faces in dark red. Source:
    [TopoNetX](https://github.com/pyt-team/TopoNetX), [Papillon et al](https://arxiv.org/abs/2304.10031)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 领域：蓝色节点，粉色（超）边，深红色面。来源：[TopoNetX](https://github.com/pyt-team/TopoNetX)，[Papillon
    等人](https://arxiv.org/abs/2304.10031)
- en: 💡 A great headstart to the field and basic building blocks of those topological
    networks are the papers by [Hajij et al](https://arxiv.org/abs/2206.00606) and
    by [Papillon et al](https://arxiv.org/abs/2304.10031). A notable chunk of models
    was implemented by the members of the [Topology, Algebra, and Geometry in Data
    Science](https://www.tagds.com/home) (TAG) community that regularly organizes
    topological workshops at ML conferences.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 💡 该领域的一个重要起步和基本构建块是[Hajij 等人](https://arxiv.org/abs/2206.00606)和[Papillon 等人](https://arxiv.org/abs/2304.10031)的论文。一个显著的模型部分是由[数据科学中的拓扑、代数与几何](https://www.tagds.com/home)（TAG）社区的成员实现的，该社区定期在机器学习会议上组织拓扑学研讨会。
- en: '***Mathilde Papillon (UCSB)***'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '***Mathilde Papillon (UCSB)***'
- en: “Until 2023, the field of topological deep learning featured a fractured landscape
    of enriched representations for relational data.” — Mathilde Papillon (UC Santa
    Barbara)
  id: totrans-148
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “直到2023年，拓扑深度学习领域仍然呈现出一个支离破碎的关系数据增强表示的格局。” —— Mathilde Papillon（加州大学圣塔芭芭拉分校）
- en: ➡️ Message-passing models were only built upon and benchmarked against other
    models of the same domain, e.g., the simplicial complex community remained insular
    to the hypergraph community. To make matters worse, most models adopted a unique
    mathematical notation. Deciding which model would be best suited to a given application
    seemed like a monumental task. A unification theory proposed by [Hajij et al](https://arxiv.org/abs/2206.00606)
    offered a general scheme under which all models could be systematically described
    and classified. We applied this theory to the literature to produce a comprehensive
    yet concise [survey of message passing in topological deep learning](https://arxiv.org/abs/2304.10031)
    that also serves as an accessible introduction to the field. We additionally provide
    a [dictionary listing all the model architectures](https://github.com/awesome-tnns/awesome-tnns)
    in one unifying notation.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 消息传递模型仅基于相同领域的其他模型进行构建和基准测试，例如，单纯形复形社区与超图社区之间一直是封闭的。更糟糕的是，大多数模型采用了独特的数学符号。这使得选择最适合特定应用的模型变得像一项艰巨的任务。[Hajij
    等人](https://arxiv.org/abs/2206.00606)提出的统一理论提供了一个通用框架，在这个框架下，所有模型都可以被系统地描述和分类。我们将该理论应用于文献中，制作了一份全面而简明的[拓扑深度学习中消息传递的调查](https://arxiv.org/abs/2304.10031)，这也是该领域的易于理解的入门介绍。此外，我们还提供了一个[字典，列出了所有模型架构](https://github.com/awesome-tnns/awesome-tnns)，并采用统一的符号表示。
- en: ➡️ To further unify the field, we organized the first [Topological Deep Learning
    Challenge](https://pyt-team.github.io/topomodelx/challenge/index.html), hosted
    at the [2023 ICML TAG workshop](https://www.tagds.com/events/conference-workshops/tag-ml23)
    and recorded via this white paper by [Papillon et al](https://proceedings.mlr.press/v221/papillon23a.html).
    The goal was to foster reproducible research by crowdsourcing the open-source
    implementation of neural networks on topological domains. As part of the challenge,
    participants from around the world contributed implementations of pre-existing
    topological deep learning models in [TopoModelX](https://github.com/pyt-team/TopoModelX).
    Each submission was rigorously unit-tested and included benchmark training on
    datasets loaded from [TopoNetX](https://github.com/pyt-team/TopoNetX). It is our
    hope that this one-stop-shop suite of consistently implemented models will help
    practitioners test-drive topological methods for new applications and developments
    in 2024.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 为了进一步统一这一领域，我们组织了第一次[拓扑深度学习挑战赛](https://pyt-team.github.io/topomodelx/challenge/index.html)，该活动在[2023
    ICML TAG 研讨会](https://www.tagds.com/events/conference-workshops/tag-ml23)上举办，并通过[Papillon
    等人](https://proceedings.mlr.press/v221/papillon23a.html)的白皮书进行了记录。我们的目标是通过众包的方式促进可重复研究，尤其是推动基于拓扑领域的神经网络开源实现。作为挑战的一部分，来自世界各地的参与者贡献了在[TopoModelX](https://github.com/pyt-team/TopoModelX)中实现的现有拓扑深度学习模型。每个提交都经过严格的单元测试，并包括在[TopoNetX](https://github.com/pyt-team/TopoNetX)加载的数据集上的基准训练。我们希望，这一套始终如一地实现的模型能够帮助从业者在2024年测试拓扑方法在新应用和开发中的表现。
- en: '***Bastian Rieck (Helmholtz & TUM)***'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '***Bastian Rieck（赫尔姆霍兹研究所 & 慕尼黑工业大学）***'
- en: 2023 was an exciting year for topology-driven machine learning methods. On the
    one hand, we saw more integrations with geometrical concepts like curvature, thus
    demonstrating the versatility of hybrid geometrical-topological models. For instance,
    in [‘Curvature Filtrations for Graph Generative Model Evaluation,’](https://arxiv.org/abs/2301.12906)
    we showed how to employ curvature as a way to select suitable graph generative
    models. Here, curvature serves as a ‘lens’ that we use to extract graph structure
    information, while we employ persistent homology, a topological method, to compare
    this information in a consistent fashion.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年对于拓扑驱动的机器学习方法来说是激动人心的一年。一方面，我们看到了更多与几何概念（如曲率）相结合的应用，展示了几何-拓扑混合模型的多样性。例如，在[《用于图生成模型评估的曲率过滤》](https://arxiv.org/abs/2301.12906)中，我们展示了如何使用曲率来选择合适的图生成模型。在这里，曲率作为一种“透镜”，用来提取图结构信息，而我们则使用持续同调（一种拓扑方法）以一致的方式比较这些信息。
- en: '![](../Images/17bc4a07a06a2f13e04b43502677d736.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17bc4a07a06a2f13e04b43502677d736.png)'
- en: 'An overview of the pipeline for evaluating graph generative models using discrete
    curvature. The ordering on edges gives rise to a curvature filtration, followed
    by a corresponding persistence diagram and landscape. For graph generative models,
    we select a curvature, apply this framework element-wise, and evaluate the similarity
    of the generated and reference distributions by comparing their average landscapes.
    Source: [Southern, Wayland, Bronstein, and Rieck.](https://arxiv.org/abs/2301.12906)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 通过离散曲率评估图生成模型的管道概述。边的排序会产生一个曲率过滤，接着是相应的持久性图和地形图。对于图生成模型，我们选择一个曲率，逐元素应用这个框架，并通过比较生成分布和参考分布的平均地形图来评估它们的相似性。来源：[Southern,
    Wayland, Bronstein, 和 Rieck.](https://arxiv.org/abs/2301.12906)
- en: ➡️ Another direction that serves to underscore that topology-driven methods
    are becoming a staple in graph learning research uses topology to assess the expressivity
    of graph neural network models. Sometimes, as in a very fascinating work from
    NeurIPS 2023 by [Immonen et al.](https://openreview.net/pdf?id=27TdrEvqLD) this
    even leads to novel models that leverage both geometrical and topological aspects
    of graphs in tandem! My own research also aims to contribute to this facet by
    specifically analyzing the [expressivity of persistent homology in graph learning](https://arxiv.org/abs/2302.09826).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 另一个方向强调拓扑驱动方法正成为图学习研究的主流，利用拓扑来评估图神经网络模型的表达能力。有时，正如2023年NeurIPS上[Immonen等人](https://openreview.net/pdf?id=27TdrEvqLD)的一个非常有趣的工作所展示的那样，这甚至会导致新的模型，它们同时利用图的几何和拓扑特性！我的研究也旨在通过专门分析[持久同调在图学习中的表达能力](https://arxiv.org/abs/2302.09826)为这一领域做出贡献。
- en: “2023 also was the cusp of moving away — or beyond — persistent homology. Despite
    being rightfully seen as the paradigmatic algorithm for topology-driven machine
    learning, algebraic topology and differential topology offer an even richer fabric
    that can be used to analyse data.” — Bastian Rieck (Helmholtz & TUM)
  id: totrans-156
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “2023年也正是一个转折点，标志着我们即将摆脱——或者说超越——持久同调。尽管持久同调被公认为拓扑驱动的机器学习范式算法，但代数拓扑和微分拓扑提供了更为丰富的结构，可以用来分析数据。”——巴斯蒂安·里克（Helmholtz
    & TUM）
- en: ➡️ With my great collaborators, we started looking at some alternatives very
    recently and came up with the concept of [neural differential forms](https://arxiv.org/abs/2312.08515).
    Differential forms permit us to elegantly build a bridge between geometry and
    topology by means of the [de Rham cohomology](https://en.wikipedia.org/wiki/De_Rham_cohomology)
    — a way to link the integration of certain objects (differential forms), i.e.
    a fundamentally *geometric* operation, to topological characteristics of input
    data. With some additional constructions, the de Rham cohomology permits us to
    learn geometric descriptions of graphs (or higher-order combinatorial complexes)
    and solve learning tasks without having to rely on message passing. The upshot
    are models with fewer parameters that are potentially more effective at solving
    such tasks. There’s more to come here, since we have just started scratching the
    surface!
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 与我的优秀合作伙伴们，我们最近开始探索一些替代方法，并提出了[神经微分形式](https://arxiv.org/abs/2312.08515)的概念。微分形式允许我们通过[德拉姆同调](https://en.wikipedia.org/wiki/De_Rham_cohomology)优雅地建立几何和拓扑之间的桥梁——德拉姆同调是将某些对象（微分形式）的积分——即一种本质上*几何*的操作——与输入数据的拓扑特征联系起来的方法。通过一些额外的构造，德拉姆同调使我们能够学习图（或更高阶的组合复形）的几何描述，并在不依赖信息传递的情况下解决学习任务。其结果是参数更少的模型，可能在解决这些任务时更加高效。这里还有更多内容，毕竟我们才刚刚开始探索！
- en: '🔮My hopeful predictions for 2024 are that we will:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 🔮 我对2024年的希望预测是，我们将：
- en: 1️⃣ see many more diverse tools from algebraic and differential topology applied
    to graphs and combinatorial complexes,
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ 看到更多来自代数和微分拓扑的多样化工具应用于图和组合复形，
- en: 2️⃣ better understand message passing on higher-order input data, and
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 更好地理解高阶输入数据上的信息传递，并且
- en: 3️⃣ finally obtain better parallel algorithms for persistent homology to truly
    unleash its power in a deep learning setting. A [recent paper on spectral sequences](https://link.springer.com/article/10.1007/s00454-023-00549-2)
    by Torras-Casas reports some very exciting results that show the great prospects
    of this technique.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 3️⃣ 最终获得更好的并行算法，用于持久同调，真正释放其在深度学习中的潜力。[Torras-Casas最近关于谱序列的论文](https://link.springer.com/article/10.1007/s00454-023-00549-2)报告了一些非常令人兴奋的结果，展示了该技术的巨大前景。
- en: '**Geometric Algebras**'
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**几何代数**'
- en: '*Johannes Brandstetter (JKU Linz) and David Ruhe (U Amsterdam)*'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '*约翰内斯·布兰德斯特特（JKU Linz）和大卫·鲁赫（U Amsterdam）*'
- en: “In 2023, we saw the subfield of deep learning on geometric algebras (also known
    as **Clifford algebras**) take off. Previously, neural network layers formulated
    as operations on Clifford algebra *multivectors* were introduced by [Brandstetter
    et al.](https://arxiv.org/abs/2209.04934) This year, the ‘geometric’ in ‘geometric
    algebra’ was clearly put into action.” — Johannes Brandstetter (JKU Linz) and
    David Ruhe (U Amsterdam)
  id: totrans-164
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “在2023年，我们见证了几何代数（也叫**Clifford代数**）深度学习子领域的蓬勃发展。之前，神经网络层通过Clifford代数*多向量*的运算形式被提出，由[Brandstetter
    et al.](https://arxiv.org/abs/2209.04934)介绍。今年，‘几何’这一概念在‘几何代数’中得到了明确的应用。”——Johannes
    Brandstetter（JKU Linz）和David Ruhe（U Amsterdam）
- en: ➡️ First, [Ruhe et al.](https://arxiv.org/abs/2302.06594) applied the quintessence
    of modern (plane-based) geometric algebra by introducing **Geometric Clifford
    Algebra Networks (GCAN)**, neural network templates that model symmetry transformations
    described by various geometric algebras. We saw an intriguing application thereof
    by [Pepe et al.](https://openaccess.thecvf.com/content/WACV2024/papers/Pepe_CGAPoseNetGCAN_A_Geometric_Clifford_Algebra_Network_for_Geometry-Aware_Camera_Pose_WACV_2024_paper.pdf)
    in **CGAPoseNet**, building a geometry-aware pipeline for camera pose regression.
    Next, [Ruhe et al.](https://arxiv.org/abs/2305.11141) introduced **Clifford Group
    Equivariant Neural Networks (CGENN)**, building steerable O(n)- and E(n)-equivariant
    (graph) neural networks of any dimension via the Clifford group. [Pepe et al.](https://openreview.net/forum?id=JNfpsiGS5E)
    apply CGENNs to a Protein Structure Prediction (PSP) pipeline, increasing prediction
    accuracies by up to 2.1%.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 首先，[Ruhe et al.](https://arxiv.org/abs/2302.06594)通过引入**几何Clifford代数网络（GCAN）**，应用了现代（基于平面的）几何代数的精髓，这些神经网络模板模拟由各种几何代数描述的对称变换。我们看到由[Pepe
    et al.](https://openaccess.thecvf.com/content/WACV2024/papers/Pepe_CGAPoseNetGCAN_A_Geometric_Clifford_Algebra_Network_for_Geometry-Aware_Camera_Pose_WACV_2024_paper.pdf)在**CGAPoseNet**中应用这一方法，构建了一个几何感知的相机姿态回归管道。接下来，[Ruhe
    et al.](https://arxiv.org/abs/2305.11141)引入了**Clifford群等变神经网络（CGENN）**，通过Clifford群构建可调的O(n)-和E(n)-等变（图）神经网络，适用于任意维度。[Pepe
    et al.](https://openreview.net/forum?id=JNfpsiGS5E)将CGENNs应用于蛋白质结构预测（PSP）管道，将预测精度提高了最多2.1%。
- en: '![](../Images/7a7c1ca9882edc21bb8fa7dec09ecdd4.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a7c1ca9882edc21bb8fa7dec09ecdd4.png)'
- en: 'CGENNs (represented with ϕ) are able to operate on multivectors (elements of
    the Clifford algebra) in an O(n)- or E(n)-equivariant way. Specifically, when
    an action ρ(w) of the Clifford group, representing an orthogonal transformation
    such as a rotation, is applied to the data, the model’s representations corotate.
    Multivectors can be decomposed into scalar, vector, bivector, trivector, and even
    higher-order components. These elements can represent geometric quantities such
    as (oriented) areas or volumes. The action ρ(w) is designed to respect these structures
    when acting on them. Source: [Ruhe et al.](https://arxiv.org/abs/2305.11141)'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: CGENNs（用ϕ表示）能够以O(n)-或E(n)-等变方式作用于多向量（Clifford代数的元素）。具体来说，当Clifford群的作用ρ(w)，代表像旋转这样的正交变换，作用于数据时，模型的表示会进行共同旋转。多向量可以分解为标量、向量、双向量、三向量甚至更高阶的分量。这些元素可以表示几何量，如（定向的）面积或体积。作用ρ(w)的设计是为了在作用于这些结构时保持这些结构的完整性。来源：[Ruhe
    et al.](https://arxiv.org/abs/2305.11141)
- en: ➡️ Coincidently, [Brehmer et al.](https://arxiv.org/abs/2305.18415) formulated
    **Geometric Algebra Transformer(GATr)**, a scalable Transformer architecture that
    harnesses the benefits of representations provided by the projective geometric
    algebra and the scalability of Transformers to build E(3)-equivariant architectures.
    The GATr architecture was extended to other algebras by [Haan et al.](https://arxiv.org/abs/2311.04744)
    who also examine which flavor of geometric algebra is best suited for your E(3)-equivariant
    machine learning problem.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 恰巧，[Brehmer et al.](https://arxiv.org/abs/2305.18415)提出了**几何代数变换器（GATr）**，一种可扩展的Transformer架构，利用投影几何代数提供的表示优势和Transformer的可扩展性，构建E(3)-等变架构。GATr架构由[Haan
    et al.](https://arxiv.org/abs/2311.04744)扩展到其他代数，并且还探讨了哪种几何代数最适合您的E(3)-等变机器学习问题。
- en: '![](../Images/4c74ddc1a3b15b01636ed352b0b3b5b0.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c74ddc1a3b15b01636ed352b0b3b5b0.png)'
- en: 'Overview of the GATr architecture. Boxes with solid lines are learnable components,
    those with dashed lines are fixed. Source: [Brehmer et al.](https://arxiv.org/abs/2305.18415)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: GATr架构概述。带实线的框表示可学习组件，带虚线的框表示固定组件。来源：[Brehmer et al.](https://arxiv.org/abs/2305.18415)
- en: 🔮 In 2024, we can expect exciting new applications from these advancements.
    Some examples include the following.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 🔮 在2024年，我们可以期待这些进展带来令人兴奋的新应用。一些例子包括以下内容。
- en: 1️⃣ We can expect explorations of their applicability to molecular data, drug
    design, neural physics emulations, crystals, etc. Other geometry-aware applications
    include 3D rendering, pose estimations, and planning for, e.g., robot arms.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ 我们可以期待探索其在分子数据、药物设计、神经物理仿真、晶体等领域的应用。其他几何感知的应用包括3D渲染、姿态估计，以及例如机器人手臂的规划。
- en: 2️⃣ We can expect the extension of geometric algebra-based networks to other
    neural network architectures, such as convolutional neural networks.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 我们可以期待将基于几何代数的网络扩展到其他神经网络架构中，如卷积神经网络。
- en: 3️⃣ Next, the generality of the CGENN allows for explorations in other dimensions,
    e.g., 2D, but also in settings where data of various dimensionalities should be
    processed together. Further, they enable non-Euclidean geometries, which have
    several use cases in relativistic physics.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 3️⃣ 接下来，CGENN的通用性使得可以在其他维度中进行探索，例如2D，但也可以在需要处理多维数据的环境中进行探索。此外，它们还支持非欧几里得几何，这在相对论物理学中有多个应用场景。
- en: 4️⃣ Finally, GATr and CGENN can be extended and applied to projective, conformal,
    hyperbolic, or elliptic geometries.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 4️⃣ 最后，GATr和CGENN可以扩展并应用于投影几何、保角几何、双曲几何或椭圆几何。
- en: '**PDEs**'
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**偏微分方程（PDEs）**'
- en: '*Johannes Brandstetter (JKU Linz)*'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '*Johannes Brandstetter（JKU Linz）*'
- en: Concerning the landscape of neural PDE modelling, what topics have surfaced
    or gathered momentum through 2023?
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 关于神经偏微分方程建模的现状，哪些话题在2023年有所浮现或获得了更多关注？
- en: 1️⃣ To begin, there is a noticeable trend towards modelling PDEs on and within
    intricate geometries, necessitating a mesh-based discretization of space. This
    aligns with the overarching goal to address increasingly realistic real world
    problems. For example, [Li et al](https://arxiv.org/abs/2309.00583). have introduced
    **Geometry-Informed Neural Operator (GINO)** for large-scale 3D PDEs.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ 首先，有一个明显的趋势是，开始在复杂几何上以及几何内部建模PDEs，这需要基于网格的空间离散化。这与解决越来越逼真的现实世界问题的总体目标相一致。例如，[Li
    et al](https://arxiv.org/abs/2309.00583)提出了用于大规模3D PDE的**几何感知神经算子（GINO）**。
- en: 2️⃣ Secondly, the development of neural network surrogates for Lagrangian-based
    simulations is becoming increasingly intriguing. The Lagrangian discretization
    of space uses finite material points which are tracked as fluid parcels through
    space and time. The most prominent Lagrangian discretization scheme is called
    smoothed particle hydrodynamics (SPH), which is the numerical baseline in the
    **LagrangeBench** benchmark dataset provided by [Toshev et al.](https://arxiv.org/abs/2309.16342)
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 其次，基于拉格朗日模拟的神经网络代理的发展变得越来越引人注目。拉格朗日空间离散化使用有限的物质点，这些物质点作为流体团块在时空中进行追踪。最著名的拉格朗日离散化方案是平滑粒子流体动力学（SPH），这是由[Toshev
    et al.](https://arxiv.org/abs/2309.16342)提供的**LagrangeBench**基准数据集中使用的数值基线。
- en: '![](../Images/4c431332c56925e208b2421a66e4fadc.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c431332c56925e208b2421a66e4fadc.png)'
- en: 'Time snapshots of our datasets, at the initial time (top), 40% (middle), and
    95% (bottom) of the trajectory. Color temperature represents velocity magnitude.
    (a) Taylor Green vortex (2D and 3D), (b) Reverse Poiseuille flow (2D and 3D),
    © Lid-driven cavity (2D and 3D), (d) Dam break (2D). Source: LagrangeBench by
    [Toshev et al.](https://arxiv.org/abs/2309.16342)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据集的时间快照，在初始时刻（顶部）、40%（中部）和95%（底部）的轨迹上。颜色温度表示速度大小。 (a) 泰勒-格林涡旋（2D 和 3D），(b)
    反向泊松流（2D 和 3D），(c) 驱动腔体（2D 和 3D），(d) 水坝溃坝（2D）。来源：LagrangeBench，由[Toshev et al.](https://arxiv.org/abs/2309.16342)提供
- en: 3️⃣ Thirdly, diffusion-based modelling is also not stopping for PDEs. We roughly
    see two directions. The first direction recasts the iterative nature of the diffusion
    process into a refinement of a candidate state initialised from noise and conditioned
    on previous timesteps. This iterative refinement was introduced in **PDE-Refiner**
    ([Lippe et al.](https://arxiv.org/abs/2308.05732)) and a variant thereof was already
    applied in **GenCast** ([Price et al.](https://arxiv.org/abs/2312.15796)). The
    second direction exerts the probabilistic nature of diffusion models to model
    chaotic phenomena such as 3D turbulence. Examples of this can be found in **Turbulent
    Flow Simulation** ([Kohl et al.](https://arxiv.org/abs/2309.01745)) and in **From
    Zero To Turbulence** ([Lienen et al.](https://arxiv.org/abs/2306.01776)). Especially
    for 3D turbulence, there are a lot of interesting things that will happen in the
    near future.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 3️⃣ 其次，基于扩散的建模在偏微分方程（PDE）领域也没有停下脚步。我们大致看到两个方向。第一个方向将扩散过程的迭代性质转化为对从噪声初始化并根据前一步骤进行条件化的候选状态的精炼。这种迭代精炼在**PDE-Refiner**（[Lippe
    等人](https://arxiv.org/abs/2308.05732)）中有所介绍，而其变体已经在**GenCast**（[Price 等人](https://arxiv.org/abs/2312.15796)）中得到应用。第二个方向利用扩散模型的概率性质来模拟诸如三维湍流等混沌现象。关于这一点的例子可以在**湍流流动仿真**（[Kohl
    等人](https://arxiv.org/abs/2309.01745)）和**从零到湍流**（[Lienen 等人](https://arxiv.org/abs/2306.01776)）中找到。特别是对于三维湍流，未来有很多有趣的事情将会发生。
- en: “Weather modelling has become a great success story over the last months. There
    is potentially much more exciting stuff to come, especially regarding weather
    forecasting directly from observational data or when building weather foundation
    models.” — Johannes Brandstetter (JKU Linz)
  id: totrans-184
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “天气建模在过去几个月取得了巨大成功。未来可能会有更多令人激动的进展，特别是在直接从观测数据进行天气预报或构建天气基础模型方面。” — Johannes
    Brandstetter（JKU Linz）
- en: '🔮 **What to expect in 2024**:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 🔮 **2024年的预测**：
- en: 1️⃣ More work regarding 3D turbulence modelling.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ 更多关于三维湍流建模的研究。
- en: 2️⃣ Multi-modality aspects of PDEs might emerge. This could include combining
    different PDEs, different resolutions, or different discretization schemes. We
    are already seeing a glimpse thereof in e.g. [Multiple Physics Pretraining for
    Physical Surrogate Models](https://arxiv.org/abs/2310.02994) by McCabe et al.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 偏微分方程的多模态方面可能会出现。这可能包括结合不同的偏微分方程、不同的分辨率或不同的离散化方案。我们已经在例如[多物理预训练物理代理模型](https://arxiv.org/abs/2310.02994)中看到了一些相关的迹象，这是McCabe等人的研究。
- en: '**Predictions from the 2023 post**'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '**2023年预测**'
- en: (1) Neural PDEs and their applications are likely to expand to more physics-related
    AI4Science subfields; computational fluid dynamics (CFD) will potentially be influenced
    by GNN.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 神经网络偏微分方程及其应用可能会扩展到更多与物理相关的AI4Science子领域；计算流体力学（CFD）可能会受到图神经网络（GNN）的影响。
- en: ✅ We are seeing 3D turbulence modelling, geometry-aware neural operators, particle-based
    neural surrogates, and a huge impact in e.g. weather forecasting.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ✅ 我们已经看到了三维湍流建模、几何感知神经算子、基于粒子的神经代理模型，并且在例如天气预报等领域产生了巨大影响。
- en: (2) GNN based surrogates might augment/replace traditional well-tried techniques.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 基于图神经网络的代理模型可能会增强或取代传统的成熟技术。
- en: ✅ Weather forecasting has become a great success story. Neural network based
    weather forecasts overtake traditional forecasts (medium range+local forecasts),
    e.g., [GraphCast](https://www.science.org/doi/full/10.1126/science.adi2336) by
    Lam et al. and [MetNet-3](https://arxiv.org/abs/2306.06079) by Andrychowicz et
    al.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ✅ 天气预报已经成为一个巨大的成功案例。基于神经网络的天气预报超过了传统的预报（中期+局部预报），例如，[GraphCast](https://www.science.org/doi/full/10.1126/science.adi2336)（Lam
    等人）和[MetNet-3](https://arxiv.org/abs/2306.06079)（Andrychowicz 等人）。
- en: Robustness and Explainability
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稳健性和可解释性
- en: '*Kexin Huang (Stanford)*'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '*黄克欣（斯坦福大学）*'
- en: “As GNNs are getting deployed in various domains, their reliability and robustness
    have become increasingly important, especially in safety-critical applications
    (e.g. scientific discovery) where the cost of errors is significant.” — Kexin
    Huang (Stanford)
  id: totrans-195
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “随着图神经网络（GNN）在各个领域的部署，其可靠性和稳健性变得越来越重要，尤其是在安全关键的应用中（例如科学发现），因为错误的代价可能非常高。” —
    黄克欣（斯坦福大学）
- en: 1️⃣ When discussing the reliability of GNNs, a key criterion is **uncertainty
    quantification** — quantifying how much the model knows about the prediction.
    There are numerous works on estimating and calibrating uncertainty, also designed
    specifically for GNNs (e.g. [GATS](https://proceedings.neurips.cc/paper_files/paper/2022/hash/5975754c7650dfee0682e06e1fec0522-Abstract-Conference.html)).
    However, they fall short of achieving pre-defined target coverage (i.e. % of points
    falling into the prediction set) both theoretically and empirically. I want to
    emphasize that this notion of having a coverage guarantee is **critical** especially
    in ML deployment for scientific discovery since practitioners often trust a model
    with statistical guarantees.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ 在讨论图神经网络（GNN）的可靠性时，一个关键标准是**不确定性量化**——量化模型对预测的了解程度。关于估计和校准不确定性的研究很多，其中也有专门针对GNN的工作（例如，[GATS](https://proceedings.neurips.cc/paper_files/paper/2022/hash/5975754c7650dfee0682e06e1fec0522-Abstract-Conference.html)）。然而，这些方法在理论和实证上都未能达到预定义的目标覆盖率（即预测集内的点数百分比）。我想强调的是，拥有覆盖保证这一概念**至关重要**，尤其是在科学发现的机器学习部署中，因为实践者通常信任具有统计保证的模型。
- en: '**2️⃣ Conformal prediction** is an exciting direction in statistics where it
    has finite sample coverage guarantees and has been applied in many domains such
    as [vision and NLP](https://arxiv.org/abs/2107.07511). But it is unclear if it
    can be used in graphs theoretically since it is not obvious if the exchangeability
    assumption holds for graph settings. In 2023, we see conformal prediction has
    been extended to graphs. Notably, [CF-GNN](https://arxiv.org/abs/2305.14535) and
    [DAPS](https://proceedings.mlr.press/v202/h-zargarbashi23a/h-zargarbashi23a.pdf)
    have derived theoretical conditions for conformal validity in transductive node-level
    prediction setting and also developed methods to reduce the prediction set size
    for efficient downstream usage. More recently, we have also seen conformal prediction
    extensions to [link prediction](https://arxiv.org/pdf/2306.14693v1.pdf), [non-uniform
    split](https://arxiv.org/abs/2306.07252), [edge exchangeability](https://openreview.net/forum?id=homn1jOKI5),
    and also adaptations for settings where exchangeability does not hold (such as
    [inductive setting](https://arxiv.org/abs/2211.14555)).'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '**2️⃣ 符合性预测**是统计学中的一个令人兴奋的方向，它具有有限样本覆盖保证，且已应用于许多领域，如[视觉和自然语言处理](https://arxiv.org/abs/2107.07511)。但尚不清楚它是否可以在图中理论上应用，因为在图的设置下，可交换性假设是否成立并不显而易见。2023年，我们看到符合性预测已扩展到图结构数据。特别是，[CF-GNN](https://arxiv.org/abs/2305.14535)和[DAPS](https://proceedings.mlr.press/v202/h-zargarbashi23a/h-zargarbashi23a.pdf)已推导出传导节点级预测设置中符合性有效性的理论条件，并且还开发了减少预测集大小的方法，以便高效地进行下游使用。最近，我们还看到符合性预测扩展到[链接预测](https://arxiv.org/pdf/2306.14693v1.pdf)、[非均匀划分](https://arxiv.org/abs/2306.07252)、[边可交换性](https://openreview.net/forum?id=homn1jOKI5)，以及针对不满足可交换性的设置（如[归纳设置](https://arxiv.org/abs/2211.14555)）的适应。'
- en: '![](../Images/9c51e4ae0619793413247af551445751.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c51e4ae0619793413247af551445751.png)'
- en: 'Conformal prediction for graph-structured data. (1) A base GNN model (GNN)
    that produces prediction scores µ for node i. (2) Conformal correction. Since
    the training step is not aware of the conformal calibration step, the size/length
    of prediction sets/intervals (i.e. efficiency) are not optimized. We use a topology-aware
    correction model that takes µ as the input node feature and aggregates information
    from its local subgraph to produce an updated prediction µ˜. (3) Conformal prediction.
    We prove that in a transductive random split setting, graph exchangeability holds
    given permutation invariance. Thus, standard CP can be used to produce a prediction
    set/interval based on µ˜ that includes true label with pre-specified coverage
    rate 1-α. Source: [Huang et al.](https://arxiv.org/abs/2305.14535)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 针对图结构数据的符合性预测。 (1) 一个基础的GNN模型（GNN），它为节点i生成预测得分µ。 (2) 符合性校正。由于训练步骤并不考虑符合性校正步骤，因此预测集/区间的大小/长度（即效率）没有得到优化。我们使用一个拓扑感知的校正模型，将µ作为输入节点特征，并汇聚其局部子图的信息，以生成更新后的预测µ˜。
    (3) 符合性预测。我们证明，在一个传导随机划分的设置下，给定置换不变性，图的可交换性是成立的。因此，标准的符合性预测（CP）可以基于µ˜生成预测集/区间，且该预测集包含具有预设覆盖率1-α的真实标签。来源：[黄等](https://arxiv.org/abs/2305.14535)
- en: 🔮 Looking ahead, we expect more extensions to cover a wide range of GNN deployment
    use cases. Overall, I think having statistical guarantees for GNNs is very nice
    because it enables the trust of practitioners to use GNN predictions.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 🔮 展望未来，我们预计会有更多扩展，覆盖更广泛的GNN部署用例。总体而言，我认为为GNN提供统计保证非常好，因为它使得实践者能够信任GNN的预测结果。
- en: Graph Transformers
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图神经网络变换器（Graph Transformers）
- en: '*Chen Lin (Oxford)*'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '*陈琳（牛津大学）*'
- en: 💡 In 2023, we have seen the continuation of the rise of Graph Transformers.
    It has become the **common GNN design**, e.g., in [GATr](https://arxiv.org/abs/2305.18415),
    the authors attribute its popularity to its *“favorable scaling properties, expressiveness,
    trainability, and versatility”*.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 💡 2023年，我们见证了图神经网络变换器的持续崛起。它已经成为**常见的GNN设计**，例如在[GATr](https://arxiv.org/abs/2305.18415)中，作者将其流行归因于其*“良好的扩展性、表达能力、可训练性和多功能性”*。
- en: 1️⃣ **Expressiveness of GTs.** As mentioned in the GNN Theory section, recent
    work from [Cai et al. (2023)](https://arxiv.org/abs/2301.11956) shows the equivalence
    between MPNNs with a Virtural Node and GTs under a *non-uniform setting.* This
    poses a question on how powerful are GTs and what is the source of their representation
    ability. [Zhang et al. (2023)](https://arxiv.org/abs/2301.09505) successfully
    combine a new powerful positional embedding (PE) to improve the expressiveness
    of their GTs, achieving expressivity over the biconnectivity problem. This gives
    evidence of the importance of PEs to the expressiveness of GTs. A recent submission
    [GPNN](https://openreview.net/pdf?id=JfjduOxrTY) provides a clearer view on the
    central role of the positional encoding. It has been shown that one can generalize
    the proof in [Zhang et al. (2023)](https://arxiv.org/abs/2301.09505) to show how
    GTs’ expressiveness is decided by various positional encodings.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ **GT的表达能力。**正如在GNN理论部分提到的，最近[Cai等人（2023）](https://arxiv.org/abs/2301.11956)的工作显示，带有虚拟节点的MPNN和GT在*非均匀设置*下是等价的。这提出了一个问题：GT到底有多强大，它们的表达能力来源于哪里？[Zhang等人（2023）](https://arxiv.org/abs/2301.09505)成功地将一种新的强大位置嵌入（PE）结合起来，以提高GT的表达能力，在双连通性问题上取得了更好的表现。这为PE对GT表达能力的重要性提供了证据。最近的提交[GPNN](https://openreview.net/pdf?id=JfjduOxrTY)更清楚地展示了位置编码在其中的核心作用。已经证明，人们可以推广[Zhang等人（2023）](https://arxiv.org/abs/2301.09505)中的证明，展示GT的表达能力是如何由各种位置编码决定的。
- en: '**2️⃣** **Positional (Structural) Encoding.** Given the importance of PE/SE
    to GTs, now we turn to the design of those expressive features usually derived
    from existing graph invariants. In 2022, [GraphGPS](https://arxiv.org/abs/2205.12454)
    observed a huge empirical success by combining GTs with various (or even multiple)
    PE/SEs. In 2023, more powerful PE/SE is available.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**2️⃣** **位置（结构）编码。**鉴于PE/SE对图神经网络（GTs）的重要性，接下来我们将讨论那些通常来源于现有图不变量的富有表现力的特征的设计。2022年，[GraphGPS](https://arxiv.org/abs/2205.12454)通过将GT与各种（甚至多个）PE/SE结合，取得了巨大的实证成功。到2023年，出现了更强大的PE/SE。'
- en: '**Relative Random Walk PE (RRWP)** proposed by [Ma et al](https://arxiv.org/abs/2305.17589)
    generalizes the random walk structural encoding with the relational part. Together
    with a new variant of attention mechanism, **GRIT** achieves a strong empirical
    performance compared with existing PE/SEs on property prediction benchmarks (SOTA
    on ZINC). Theoretically, RRWP can approximate the Shortest path distance, personalized
    PageRank, and heat kernel with a specific choice of parameters. With RRWP, GRIT
    is more expressive than SPD-WL.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '**相对随机游走结构编码（RRWP）**由[Ma等人](https://arxiv.org/abs/2305.17589)提出，通过引入关系部分扩展了随机游走结构编码。结合一种新的注意力机制变体，**GRIT**在属性预测基准测试中相比现有的PE/SE表现出了强大的实证性能（在ZINC上达到了SOTA）。理论上，RRWP可以通过特定的参数选择来逼近最短路径距离、个性化的PageRank和热核。使用RRWP后，GRIT比SPD-WL具有更强的表达能力。'
- en: '![](../Images/47e07c8f85dacc7c111f2918d3975ecc.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47e07c8f85dacc7c111f2918d3975ecc.png)'
- en: 'RRWP visualization for the fluorescein molecule, up to the 4th power. Thicker
    and darker edges indicate higher edge weight. Probabilities for longer random
    walks reveal higher-order structures (e.g., the cliques evident in 3-RW and the
    star patterns in 4-RW). Source: [Ma et al](https://arxiv.org/abs/2305.17589).'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: fluorescein分子的RRWP可视化，最高到4次方。较粗且较深的边表示更高的边权重。较长的随机游走的概率揭示了高阶结构（例如，在3-RW中显现的团和在4-RW中出现的星型图案）。来源：[Ma等人](https://arxiv.org/abs/2305.17589)。
- en: '[Puny et al](https://arxiv.org/abs/2302.11556) proposed a new theoretical framework
    for expressivity based on **Equivariant Polynomials** where the expressivity of
    common GNNs can be improved by having the polynomial features, computed with tensor
    contractions based on the equivariant basis, as positional encodings. The empirical
    results are surprising: GatedGCNs is improved from a test MAE of 0.265 to 0.106
    with the d-expressive polynomials. It will be very interesting to see if someone
    combines this with GTs in the future.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[Puny等人](https://arxiv.org/abs/2302.11556)提出了一种基于**等变多项式**的新表达力理论框架，通过在等变基上计算张量收缩得到的多项式特征作为位置编码，可以提高常见GNN的表达力。实验结果令人惊讶：GatedGCNs的测试MAE从0.265提高到0.106，采用了d-表达多项式。未来有人将这一方法与GT结合将会非常有趣。'
- en: '**3️⃣ Efficient GTs.** It remains challenging for GTs to be applied to large
    graphs due to the O(N²) complexity. In 2023, we saw more works trying to eliminate
    such difficulty by lowering the computation complexity of GTs. [Deac et al](https://arxiv.org/abs/2210.02997)
    used [expander graphs](https://en.wikipedia.org/wiki/Expander_graph) for the propagation,
    which is regularly connected with few edges.[Exphormer](https://arxiv.org/abs/2303.06147)
    extended this idea to GT by combining expander graphs with the local neighborhood
    aggregation and virtual node. Exphormer allows graph transformers to scale to
    larger graphs (as large as *ogbn-arxiv* with 169K nodes). It also achieved strong
    empirical results and ranked top on several [Long-Range Graph Benchmark](https://github.com/vijaydwivedi75/lrgb)
    tasks.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '**3️⃣ 高效的GTs。** 由于GT的O(N²)复杂度，GT在大图上的应用依然充满挑战。在2023年，我们看到了更多尝试通过降低GT计算复杂度来解决这一问题的研究。[Deac等人](https://arxiv.org/abs/2210.02997)使用了[扩展图](https://en.wikipedia.org/wiki/Expander_graph)进行传播，该图通常用较少的边进行连接。[Exphormer](https://arxiv.org/abs/2303.06147)将这一思想扩展到GT，通过将扩展图与局部邻域聚合和虚拟节点结合起来。Exphormer使图转换器能够扩展到更大的图（例如，具有169K节点的*ogbn-arxiv*）。它还取得了强大的实验结果，并在多个[长距离图基准测试](https://github.com/vijaydwivedi75/lrgb)任务中排名靠前。'
- en: 🔮 **Moving forward to 2024:**
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 🔮 **迈向2024年：**
- en: A better understanding of self-attention’s benefits on abstract beyond expressiveness.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更好地理解自注意力在抽象层面上对表现力之外的好处。
- en: Big open-source pre-trained equivariant GT in 2024!
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2024年，开源大规模预训练的等变GT！
- en: More powerful positional encodings.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更强大的位置编码。
- en: New Datasets & Benchmarks
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 新的数据集与基准测试
- en: '**Structural biology:** Pinder from VantAI, [PoseBusters](https://arxiv.org/abs/2308.05777)
    from Oxford, [PoseCheck](https://arxiv.org/abs/2308.07413) from The Other Place,
    [DockGen](https://openreview.net/forum?id=UfBIxpTK10), and LargeMix and UltraLarge
    datasets [from Valence Labs](https://arxiv.org/abs/2310.04292)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '**结构生物学：** 来自VantAI的Pinder，来自牛津的[PoseBusters](https://arxiv.org/abs/2308.05777)，来自The
    Other Place的[PoseCheck](https://arxiv.org/abs/2308.07413)，[DockGen](https://openreview.net/forum?id=UfBIxpTK10)，以及来自[Valence
    Labs](https://arxiv.org/abs/2310.04292)的LargeMix和UltraLarge数据集'
- en: '[**Temporal Graph Benchmark**](http://tgb.mila.quebec/) (TGB): Until now, progress
    in temporal graph learning has been held back by the lack of large high-quality
    datasets, as well as the lack of proper evaluation thus leading to over-optimistic
    performance. TGB addresses this by introducing a collection of seven realistic,
    large-scale and diverse benchmarks for learning on temporal graphs, including
    both node-wise and link-wise tasks. Inspired by the success of OGB, TGB automates
    dataset downloading and processing as well as evaluation protocols, and allows
    users to compare model performance using a [leaderboard](https://tgb-website.pages.dev/docs/leader_linkprop/).
    Check out the [associated blog post](/temporal-graph-benchmark-bb5cc26fcf11) for
    more details.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '[**时序图基准测试**](http://tgb.mila.quebec/)（TGB）：到目前为止，时序图学习的进展一直受到缺乏大型高质量数据集以及缺乏适当评估的限制，这导致了过于乐观的性能评估。TGB通过引入七个现实的大规模且多样化的基准测试，解决了这一问题，这些基准包括节点级和链接级任务。受到OGB成功的启发，TGB自动化了数据集下载和处理，以及评估协议，并允许用户使用[排行榜](https://tgb-website.pages.dev/docs/leader_linkprop/)比较模型性能。更多详情请查看[相关博客文章](/temporal-graph-benchmark-bb5cc26fcf11)。'
- en: '[**TpuGraphs**](https://github.com/google-research-datasets/tpu_graphs) from
    Google Research: the graph property prediction dataset of TPU computational graphs.
    The dataset provides 25x more graphs than the largest graph property prediction
    dataset (with comparable graph sizes), and 770x larger graphs on average compared
    to existing performance prediction datasets on machine learning programs. Google
    ran [Kaggle competition](https://www.kaggle.com/competitions/predict-ai-model-runtime)
    based off TpuGraphs!'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 来自Google Research的[**TpuGraphs**](https://github.com/google-research-datasets/tpu_graphs)：TPU计算图的图属性预测数据集。该数据集提供的图数量是现有最大图属性预测数据集的25倍（图大小相当），且与现有机器学习程序性能预测数据集相比，图的平均规模大约大770倍。Google基于TpuGraphs举办了[Kaggle比赛](https://www.kaggle.com/competitions/predict-ai-model-runtime)！
- en: '[**LagrangeBench**](https://github.com/tumaer/lagrangebench): A Lagrangian
    Fluid Mechanics Benchmarking Suite — where you can evaluate your favorite GNN-based
    simulator in a JAX-based environment (for JAX aficionados).'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[**LagrangeBench**](https://github.com/tumaer/lagrangebench)：拉格朗日流体力学基准测试套件——在一个基于JAX的环境中评估你喜爱的基于GNN的模拟器（适合JAX爱好者）'
- en: '[**RelBench**](https://relbench.stanford.edu/): Relational Deep Learning Benchmark
    from Stanford and Kumo.AI: make time-based predictions over relational databases
    (which you can model as graphs or hypergraphs).'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[**RelBench**](https://relbench.stanford.edu/)：来自斯坦福和Kumo.AI的关系深度学习基准：对关系数据库进行基于时间的预测（你可以将其建模为图或超图）'
- en: '[**The GNoMe dataset**](https://github.com/google-deepmind/materials_discovery?tab=readme-ov-file#dataset)
    from Google DeepMind: 381k more novel stable materials for your materials discovery
    and ML potentials models!'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[**GNoMe数据集**](https://github.com/google-deepmind/materials_discovery?tab=readme-ov-file#dataset)来自Google
    DeepMind：381k种新型稳定材料，用于材料发现和机器学习潜力模型！'
- en: Conferences, Courses & Community
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 会议、课程与社区
- en: 'The main events in the graph and geometric learning world (apart from big ML
    conferences) grow larger and more mature: [The Learning on Graphs Conference (LoG)](https://logconference.org/),
    [Molecular ML](https://www.moml.mit.edu/) (MoML), and the [Stanford Graph Learning
    Workshop](https://snap.stanford.edu/graphlearning-workshop-2023/). The LoG conference
    features a cool format with the remote-first conference and dozens of local meetups
    organized by community members spanning the whole globe from China to UK & Europe
    to the US West Coast 🌏🌍🌎 .'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图形和几何学习领域的主要事件（除了大型ML会议）不断壮大和成熟：[图学习会议（LoG）](https://logconference.org/)、[分子机器学习](https://www.moml.mit.edu/)（MoML）和[斯坦福图学习工作坊](https://snap.stanford.edu/graphlearning-workshop-2023/)。LoG会议采用远程优先的独特形式，全球范围内的社区成员组织了数十场本地聚会，从中国到英国、欧洲，再到美国西海岸
    🌏🌍🌎。
- en: '![](../Images/d717711a2b7d239dc03499441d93f3bc.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d717711a2b7d239dc03499441d93f3bc.png)'
- en: 'The LoG meetups in Amsterdam, Paris, Tromsø, and Shanghai. Source: Slack of
    the LoG community'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: LoG在阿姆斯特丹、巴黎、特罗姆瑟和上海的聚会。来源：LoG社区的Slack
- en: Courses, books, and educational resources
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 课程、书籍和教育资源
- en: '[Geometric GNN Dojo](https://github.com/chaitjo/geometric-gnn-dojo) — a pedagogical
    resource for beginners and experts to explore the design space of GNNs for geometric
    graphs (pairs best with the recent Hitchhiker’s Guide to Geometric GNNs).'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[几何GNN道场](https://github.com/chaitjo/geometric-gnn-dojo)——一个面向初学者和专家的教学资源，帮助探索几何图形的GNN设计空间（与最近的《几何GNN的便捷指南》最佳搭配）'
- en: '[TorchCFM](https://github.com/atong01/conditional-flow-matching) — the main
    entrypoint to the world of flow matching.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TorchCFM](https://github.com/atong01/conditional-flow-matching)——流匹配领域的主要入口'
- en: The [PyT team](https://github.com/pyt-team) maintains TopoNetX, TopoModelX,
    and TopoEmbedX — the most hands-on libraries to jump into topological deep learning.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyT团队](https://github.com/pyt-team)维护着TopoNetX、TopoModelX和TopoEmbedX——最实用的拓扑深度学习库'
- en: 'The book on [Equivariant and Coordinate Independent Convolutional Networks:
    A Gauge Field Theory of Neural Networks](https://maurice-weiler.gitlab.io/#cnn_book)
    by Maurice Weiler, Patrick Forré, Erik Verlinde, and Max Welling — brings together
    the findings on the representation theory and differential geometry of equivariant
    CNNs'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由Maurice Weiler、Patrick Forré、Erik Verlinde和Max Welling编写的[《等变和坐标独立卷积网络：神经网络的规范场理论》](https://maurice-weiler.gitlab.io/#cnn_book)一书——汇集了关于等变卷积神经网络（CNN）的表示理论和微分几何的研究成果
- en: Surveys
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调查
- en: '**ML for Science in Quantum, Atomistic, and Continuum systems** by well over
    60 authors from 23 institutions ([Zhang, Wang, Helwig, Luo, Fu, Xie et al.](https://arxiv.org/abs/2307.08423))'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**量子、原子系统和连续介质中的科学机器学习**，由来自23个机构的60多位作者（[张、王、Helwig、罗、傅、谢等](https://arxiv.org/abs/2307.08423)）撰写'
- en: '**Scientific discovery in the age of artificial intelligence** by [Wang et
    al](https://www.nature.com/articles/s41586-023-06221-2) published in Nature.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人工智能时代的科学发现**，由[王等人](https://www.nature.com/articles/s41586-023-06221-2)发表于《自然》杂志。'
- en: Prominent seminar series
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 知名研讨会系列
- en: '[Learning on Graphs & Geometry](https://portal.valencelabs.com/logg)'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[图形与几何学习](https://portal.valencelabs.com/logg)'
- en: '[Molecular Modeling and Drug Discovery (M2D2)](https://portal.valencelabs.com/m2d2)'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分子建模与药物发现（M2D2）](https://portal.valencelabs.com/m2d2)'
- en: '[VantAI reading group](https://www.youtube.com/@Vant_AI)'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[VantAI阅读小组](https://www.youtube.com/@Vant_AI)'
- en: '[Oxford LoG2 seminar series](https://log-2.github.io/)'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[牛津LoG2研讨会系列](https://log-2.github.io/)'
- en: Slack communities
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Slack社群
- en: '[LoGaG](https://join.slack.com/t/logag/shared_invite/zt-22y7n3k7a-FHwX31gc85yZCa0uF8BU7w)'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LoGaG](https://join.slack.com/t/logag/shared_invite/zt-22y7n3k7a-FHwX31gc85yZCa0uF8BU7w)'
- en: '[LOG conference](https://join.slack.com/t/logconference/shared_invite/zt-27nv8ba1y-pXspnAzgLOMdDzfKgpOafg)'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LOG会议](https://join.slack.com/t/logconference/shared_invite/zt-27nv8ba1y-pXspnAzgLOMdDzfKgpOafg)'
- en: '[PyG](https://data.pyg.org/slack.html)'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyG](https://data.pyg.org/slack.html)'
- en: Memes of 2023
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2023年的迷因
- en: '![](../Images/51dd958953c300da8926b7e2758c3032.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51dd958953c300da8926b7e2758c3032.png)'
- en: 'Commemorating the successes of flow matching in 2023 in the meme and unique
    t-shirts brought to NeurIPS’23\. Right: Hannes Stärk and Michael Galkin are making
    a statement at NeurIPS’23\. Images by Michael Galkin'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在2023年，纪念流匹配的成功，带到了NeurIPS’23的迷因和独特T恤。右图：Hannes Stärk和Michael Galkin在NeurIPS’23发表声明。图片由Michael
    Galkin提供。
- en: '![](../Images/fdee1c1983f3c90e78d5b1711742cdee.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fdee1c1983f3c90e78d5b1711742cdee.png)'
- en: GNN aggregation functions are actually portals to category theory (Created by
    Petar Veličković)
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: GNN聚合函数实际上是范畴理论的门户（由Petar Veličković创建）
- en: '![](../Images/7255c70c42c2ce3763a19bff623a6e22.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7255c70c42c2ce3763a19bff623a6e22.png)'
- en: 'Michael Bronstein continues to harass Google by demanding his [DeepMind chair](https://www.cs.ox.ac.uk/news/1996-full.html)
    at every ML conference, but so far, he has only been offered stools (photo credits:
    Jelani Nelson and Thomas Kipf).'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: Michael Bronstein继续通过要求他的[DeepMind主席职位](https://www.cs.ox.ac.uk/news/1996-full.html)来骚扰谷歌，但到目前为止，他只被提供了凳子（照片来源：Jelani
    Nelson和Thomas Kipf）。
- en: '![](../Images/6d7fe205e735aeb0f578f2c421c23b3f.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d7fe205e735aeb0f578f2c421c23b3f.png)'
- en: 'The authors of this blog post congratulate you upon completing the long read.
    Michael Galkin and Michael Bronstein with the Meme of 2022 at ICML 2023 in Hawaii
    (Photo credit: Ben Finkelshtein)'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 本博客的作者祝贺你完成了这篇长文。Michael Galkin和Michael Bronstein在夏威夷ICML 2023上的2022年迷因（照片来源：Ben
    Finkelshtein）
- en: '*For additional articles about geometric and graph deep learning, see* [*Michael
    Galkin*](https://medium.com/@mgalkin)*’s and* [*Michael Bronstein*](https://medium.com/@michael-bronstein)*’s
    Medium posts and follow the two Michaels (*[*Galkin*](https://twitter.com/michael_galkin)
    *and* [*Bronstein*](https://twitter.com/mmbronstein)*) on Twitter.*'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '*欲了解更多几何学和图深度学习的文章，请参阅* [*Michael Galkin*](https://medium.com/@mgalkin)*和*
    [*Michael Bronstein*](https://medium.com/@michael-bronstein)*的Medium文章，并在Twitter上关注这两位Michael（*[*Galkin*](https://twitter.com/michael_galkin)
    *和* [*Bronstein*](https://twitter.com/mmbronstein)）。*'
