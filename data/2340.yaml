- en: 'The Art of Tokenization: Breaking Down Text for AI'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分词的艺术：为AI分解文本
- en: 原文：[https://towardsdatascience.com/the-art-of-tokenization-breaking-down-text-for-ai-43c7bccaed25?source=collection_archive---------0-----------------------#2024-09-26](https://towardsdatascience.com/the-art-of-tokenization-breaking-down-text-for-ai-43c7bccaed25?source=collection_archive---------0-----------------------#2024-09-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-art-of-tokenization-breaking-down-text-for-ai-43c7bccaed25?source=collection_archive---------0-----------------------#2024-09-26](https://towardsdatascience.com/the-art-of-tokenization-breaking-down-text-for-ai-43c7bccaed25?source=collection_archive---------0-----------------------#2024-09-26)
- en: 'Demystifying NLP: From Text to Embeddings'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 揭开NLP的神秘面纱：从文本到嵌入
- en: '[](https://medium.com/@murilogustineli?source=post_page---byline--43c7bccaed25--------------------------------)[![Murilo
    Gustineli](../Images/2a56c10e79b4810c7bf5e511300bfc34.png)](https://medium.com/@murilogustineli?source=post_page---byline--43c7bccaed25--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--43c7bccaed25--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--43c7bccaed25--------------------------------)
    [Murilo Gustineli](https://medium.com/@murilogustineli?source=post_page---byline--43c7bccaed25--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@murilogustineli?source=post_page---byline--43c7bccaed25--------------------------------)[![Murilo
    Gustineli](../Images/2a56c10e79b4810c7bf5e511300bfc34.png)](https://medium.com/@murilogustineli?source=post_page---byline--43c7bccaed25--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--43c7bccaed25--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--43c7bccaed25--------------------------------)
    [Murilo Gustineli](https://medium.com/@murilogustineli?source=post_page---byline--43c7bccaed25--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--43c7bccaed25--------------------------------)
    ·10 min read·Sep 26, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--43c7bccaed25--------------------------------)
    ·10分钟阅读·2024年9月26日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/10347347f8af7b1b60b09cc2d9a55932.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10347347f8af7b1b60b09cc2d9a55932.png)'
- en: Tokenization example generated by Llama-3-8B. Each colored subword represents
    a distinct token.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由Llama-3-8B生成的分词示例。每个不同颜色的子词代表一个独立的标记（token）。
- en: '**What is tokenization?**'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**什么是分词（Tokenization）？**'
- en: In computer science, we refer to human languages, like English and Mandarin,
    as “natural” languages. In contrast, languages designed to interact with computers,
    like Assembly and LISP, are called “machine” languages, following strict syntactic
    rules that leave little room for interpretation. While computers excel at processing
    their own highly structured languages, they struggle with the messiness of human
    language.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机科学中，我们将像英语和普通话这样的语言称为“自然”语言。相反，像汇编语言和LISP这样的语言，专为与计算机交互而设计，被称为“机器”语言，遵循严格的句法规则，几乎没有解释的余地。尽管计算机擅长处理自身高度结构化的语言，但它们在处理人类语言的混乱性方面却表现得相当挣扎。
- en: Language — especially text — makes up most of our communication and knowledge
    storage. For example, the internet is mostly text. Large language models like
    [ChatGPT](https://openai.com/chatgpt/), [Claude](https://www.anthropic.com/claude),
    and [Llama](https://www.llama.com/) are trained on enormous amounts of text —
    essentially all the text available online — using sophisticated computational
    techniques. However, computers operate on numbers, not words or sentences. So,
    how do we bridge the gap between human language and machine understanding?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 语言——尤其是文本——构成了我们大部分的沟通和知识存储。例如，互联网主要由文本组成。像[ChatGPT](https://openai.com/chatgpt/)、[Claude](https://www.anthropic.com/claude)和[Llama](https://www.llama.com/)这样的大型语言模型，都是通过使用复杂的计算技术，在海量的文本上进行训练——本质上是互联网上所有可用的文本。然而，计算机处理的是数字，而不是单词或句子。那么，我们如何弥合人类语言与机器理解之间的差距呢？
- en: This is where **Natural Language Processing (NLP)** comes into play. NLP is
    a field that combines linguistics, computer science, and artificial intelligence
    to enable computers to understand, interpret, and generate human language. Whether
    translating text from English to French, summarizing articles, or engaging in
    conversation, NLP allows machines to produce meaningful outputs from textual inputs.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是**自然语言处理（NLP）**的作用所在。NLP是一个结合语言学、计算机科学和人工智能的领域，旨在使计算机能够理解、解释和生成自然语言。无论是将文本从英语翻译成法语、总结文章，还是进行对话，NLP都能让机器从文本输入中生成有意义的输出。
- en: 'The first critical step in NLP is transforming raw text into a format that
    computers can work with effectively. This process is known as **tokenization**.
    Tokenization involves breaking down text into smaller, manageable units called
    ***tokens***, which can be words, subwords, or even individual characters. Here’s
    how the process typically works:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理的第一步是将原始文本转化为计算机能够有效处理的格式。这个过程称为**分词**。分词是将文本分解成更小、易于处理的单位，称为***词元***，这些词元可以是单词、子词甚至是单个字符。以下是该过程的典型工作方式：
- en: '**Standardization:** Before tokenizing, the text is standardized to ensure
    consistency. This may include converting all letters to lowercase, removing punctuation,
    and applying other normalization techniques.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准化：** 在进行分词之前，文本需要进行标准化，以确保一致性。这可能包括将所有字母转换为小写、去除标点符号，并应用其他规范化技术。'
- en: '**Tokenization:** The standardized text is then split into tokens. For example,
    the sentence `“The quick brown fox jumps over the lazy dog”` can be tokenized
    into words:'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分词：** 然后，将标准化后的文本拆分成词元。例如，句子`“The quick brown fox jumps over the lazy dog”`可以被分词为以下词语：'
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Numerical representation:** Since computers operate on numerical data, each
    token is converted into a numerical representation. This can be as simple as assigning
    a unique identifier to each token or as complex as creating multi-dimensional
    vectors that capture the token’s meaning and context.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数值表示：** 由于计算机处理的是数值数据，每个词元会被转换为数值表示。这可以是简单地为每个词元分配一个唯一标识符，也可以是创建多维向量来捕捉词元的意义和上下文。'
- en: '![](../Images/404ea4ba951265efc54d7a42d444724f.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/404ea4ba951265efc54d7a42d444724f.png)'
- en: '*Illustration inspired by “Figure 11.1 From text to vectors” from* [**Deep
    Learning with Python** *by François Chollet*](https://www.manning.com/books/deep-learning-with-python-second-edition)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*插图灵感来源于《**Python深度学习** *作者：François Chollet*](https://www.manning.com/books/deep-learning-with-python-second-edition)中的“图11.1
    从文本到向量”*'
- en: Tokenization is more than just splitting text; it’s about preparing language
    data in a way that preserves meaning and context for computational models. Different
    tokenization methods can significantly impact how well a model understands and
    processes language.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 分词不仅仅是拆分文本；它是以一种保留意义和上下文的方式准备语言数据，以便计算模型使用。不同的分词方法会显著影响模型理解和处理语言的效果。
- en: In this article, we focus on text standardization and tokenization, exploring
    a few techniques and implementations. We’ll lay the groundwork for converting
    text into numerical forms that machines can process — a crucial step toward advanced
    topics like word embeddings and language modeling that we’ll tackle in future
    articles.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将重点讨论文本标准化和分词，探讨几种技术和实现方法。我们将为将文本转换为机器可以处理的数值形式打下基础——这是迈向更高级主题（如词嵌入和语言建模）的一项关键步骤，未来的文章中我们将深入讨论这些内容。
- en: '**Text standardization**'
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**文本标准化**'
- en: 'Consider these two sentences:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 请看这两个句子：
- en: '*1\.* `“dusk fell, i was gazing at the Sao Paulo skyline. Isnt urban life vibrant??”`'
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*1\.* `“dusk fell, i was gazing at the Sao Paulo skyline. Isnt urban life vibrant??”`'
- en: ''
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*2\.* `“Dusk fell; I gazed at the São Paulo skyline. Isn’t urban life vibrant?”`'
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*2\.* `“Dusk fell; I gazed at the São Paulo skyline. Isn’t urban life vibrant?”`'
- en: 'At first glance, these sentences convey a similar meaning. However, when processed
    by a computer, especially during tasks like tokenization or encoding, they can
    appear vastly different due to subtle variations:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 初看这些句子，它们传达了类似的意思。然而，当计算机处理这些句子时，尤其是在分词或编码任务中，由于微小的变化，它们可能表现出截然不同的结果：
- en: '**Capitalization:** `“dusk”` vs. `“Dusk”`'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**大写化：** `“dusk”` 与 `“Dusk”`'
- en: '**Punctuation:** Comma vs. semicolon; presence of question marks'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标点符号：** 逗号与分号；问号的存在'
- en: '**Contractions:** `“Isnt”` vs. `“Isn’t”`'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缩写：** `“Isnt”` 与 `“Isn’t”`'
- en: '**Spelling and Special Characters:** `“Sao Paulo”` vs. `“São Paulo”`'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**拼写和特殊字符：** `“Sao Paulo”` 与 `“São Paulo”`'
- en: These differences can significantly impact how algorithms interpret the text.
    For example, `“Isnt”` without an apostrophe may not be recognized as the contraction
    of `“is not”`, and special characters like `“ã”` in `“São”` may be misinterpreted
    or cause encoding issues.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这些差异可能会显著影响算法如何解读文本。例如，`“Isnt”`没有撇号，可能不会被识别为`“is not”`的缩写，而像`“São”`中的特殊字符`“ã”`可能会被误解或导致编码问题。
- en: T**ext standardization** is a crucial preprocessing step in NLP that addresses
    these issues. By standardizing text, we reduce irrelevant variability and ensure
    that the data fed into models is consistent. This process is a form of feature
    engineering where we eliminate differences that are not meaningful for the task
    at hand.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**文本标准化**是自然语言处理中的一个关键预处理步骤，它解决了这些问题。通过标准化文本，我们减少了无关的变化，确保输入到模型中的数据是一致的。这个过程是一种特征工程方法，我们消除了对于当前任务没有意义的差异。'
- en: 'A simple method for text standardization includes:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的文本标准化方法包括：
- en: '**Converting to lowercase**: Reduces discrepancies due to capitalization.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转为小写**：减少因大小写不同而导致的差异。'
- en: '**Removing punctuation**: Simplifies the text by eliminating punctuation marks.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**去除标点符号**：通过去除标点符号简化文本。'
- en: '**Normalizing special characters**: Converts characters like `“ã”` to their
    standard forms (`“a”`).'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规范化特殊字符**：将像`“ã”`这样的字符转换为其标准形式（如`“a”`）。'
- en: 'Applying these steps to our sentences, we get:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些步骤应用到我们的句子中，我们得到：
- en: '*1\.* `“dusk fell i was gazing at the sao paulo skyline isnt urban life vibrant”`'
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*1\.* `“dusk fell i was gazing at the sao paulo skyline isnt urban life vibrant”`'
- en: ''
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*2\.* `“dusk fell i gazed at the sao paulo skyline isnt urban life vibrant”`'
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*2\.* `“dusk fell i gazed at the sao paulo skyline isnt urban life vibrant”`'
- en: Now, the sentences are more uniform, highlighting only the meaningful differences
    in word choice (e.g., `“was gazing at”` vs. `“gazed at”`).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，句子更加统一，突出了只有在单词选择上的有意义差异（例如`“was gazing at”`与`“gazed at”`的区别）。
- en: While there are more advanced standardization techniques like [**stemming**](https://en.wikipedia.org/wiki/Stemming)
    (reducing words to their root forms) and [**lemmatization**](https://en.wikipedia.org/wiki/Lemmatization)
    (reducing words to their dictionary form), this basic approach effectively minimizes
    superficial differences.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有更高级的标准化技术，如[**词干提取**](https://en.wikipedia.org/wiki/Stemming)（将单词还原为词根形式）和[**词形还原**](https://en.wikipedia.org/wiki/Lemmatization)（将单词还原为词典形式），但这种基本方法有效地最小化了表面上的差异。
- en: '**Python implementation of text standardization**'
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**文本标准化的 Python 实现**'
- en: 'Here’s how you can implement basic text standardization in Python:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何在 Python 中实现基本文本标准化的方法：
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Output:**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：**'
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: By standardizing the text, we’ve minimized differences that could confuse a
    computational model. The model can now focus on the variations between the sentences,
    such as the difference between `“was gazing at”` and `“gazed at”`, rather than
    discrepancies like punctuation or capitalization.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通过标准化文本，我们已经最小化了可能会混淆计算模型的差异。模型现在可以专注于句子之间的变化，比如`“was gazing at”`和`“gazed at”`之间的区别，而不是像标点符号或大小写等差异。
- en: Tokenization
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标记化
- en: After text standardization, the next critical step in natural language processing
    is **tokenization**. Tokenization involves breaking down the standardized text
    into smaller units called ***tokens***. These tokens are the building blocks that
    models use to understand and generate human language. Tokenization prepares the
    text for vectorization, where each token is converted into numerical representations
    that machines can process.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本标准化后，自然语言处理中的下一个关键步骤是**标记化**。标记化涉及将标准化后的文本拆分成更小的单元，称为***标记***。这些标记是模型用来理解和生成自然语言的基本构建块。标记化为向量化做准备，其中每个标记都被转换为机器可以处理的数值表示。
- en: 'We aim to convert sentences into a form that computers can efficiently and
    effectively handle. There are three common methods for tokenization:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是将句子转换成计算机可以高效处理的形式。标记化有三种常见方法：
- en: '**1\. Word-level tokenization**'
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**1\. 单词级标记化**'
- en: Splits text into individual words based on spaces and punctuation. It’s the
    most intuitive way to break down text.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 根据空格和标点符号将文本拆分成单独的单词。这是分解文本最直观的方式。
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Output:**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：**'
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**2\. Character-level tokenization**'
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2\. 字符级标记化**'
- en: Breaks text into individual characters, including letters and sometimes punctuation.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 将文本拆分成单独的字符，包括字母，有时也包括标点符号。
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Output:**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：**'
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**3\. Subword tokenization**'
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3\. 子词标记化**'
- en: 'Splits words into smaller, meaningful subword units. This method balances the
    granularity of character-level tokenization with the semantic richness of word-level
    tokenization. Algorithms like **Byte-Pair Encoding (BPE)** and **WordPiece** fall
    under this category. For instance, the [BertTokenizer](https://huggingface.co/docs/transformers/v4.44.2/en/model_doc/bert#transformers.BertTokenizer)
    tokenizes `“I have a new GPU!”` as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 将单词拆分成更小的、有意义的子词单元。这种方法在字符级别的分词粒度与词汇级别分词的语义丰富性之间取得了平衡。像 **字节对编码（BPE）** 和 **WordPiece**
    这样的算法属于这一类别。例如，[BertTokenizer](https://huggingface.co/docs/transformers/v4.44.2/en/model_doc/bert#transformers.BertTokenizer)
    将 `“I have a new GPU!”` 分词如下：
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Output:**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：**'
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Here, `“GPU”` is split into `“gp”` and `“##u”`, where `“##”` indicates that
    `“u”` is a continuation of the previous subword.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`“GPU”` 被拆分为 `“gp”` 和 `“##u”`，其中 `“##”` 表示 `“u”` 是前一个子词的延续。
- en: Subword tokenization offers a balanced approach between vocabulary size and
    semantic representation. By decomposing rare words into common subwords, it maintains
    a manageable vocabulary size without sacrificing meaning. Subwords carry semantic
    information that aids models in understanding context more effectively. This means
    models can process new or rare words by breaking them down into familiar subwords,
    increasing their ability to handle a wider range of language inputs.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 子词分词提供了一种在词汇大小和语义表示之间的平衡方法。通过将稀有词拆解为常见的子词，它保持了可管理的词汇大小，而不牺牲含义。子词携带着有助于模型更有效理解上下文的语义信息。这意味着，模型可以通过将新词或稀有词分解成熟悉的子词来处理它们，从而提高其处理更广泛语言输入的能力。
- en: For example, consider the word `“annoyingly”` which might be rare in a training
    corpus. It can be decomposed into the subwords `“annoying”` and `“ly”`. Both `“annoying”`
    and `“ly”` appear more frequently on their own, and their combined meanings retain
    the essence of `“annoyingly”`. This approach is especially beneficial in [agglutinative
    languages](https://en.wikipedia.org/wiki/Agglutinative_language) like Turkish,
    where words can become exceedingly long by stringing together subwords to convey
    complex meanings.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑单词 `“annoyingly”`，它在训练语料库中可能比较稀有。它可以被拆解为子词 `“annoying”` 和 `“ly”`。`“annoying”`
    和 `“ly”` 在它们各自的形式中更为常见，并且它们的组合含义保留了 `“annoyingly”` 的本质。这种方法在[粘着语](https://en.wikipedia.org/wiki/Agglutinative_language)（如土耳其语）中尤其有益，因为这些语言中的单词可以通过将子词组合在一起，形成极长的词来传达复杂的含义。
- en: 'Notice that the standardization step is often integrated into the tokenizer
    itself. Large language models use tokens as both inputs and outputs when processing
    text. Here’s a visual representation of tokens generated by Llama-3–8B on [Tiktokenizer](https://tiktokenizer.vercel.app/):'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，标准化步骤通常会集成到分词器本身。大型语言模型在处理文本时，使用标记作为输入和输出。以下是由 Llama-3–8B 在[Tiktokenizer](https://tiktokenizer.vercel.app/)上生成的标记的可视化表示：
- en: '![](../Images/9dc6a0f02ad1b56bfc550734d2ff6555.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9dc6a0f02ad1b56bfc550734d2ff6555.png)'
- en: '**Tiktokenizer** example using **Llama-3–8B**. Each token is represented by
    a different color.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**Tiktokenizer** 示例使用 **Llama-3–8B**。每个标记都用不同的颜色表示。'
- en: Additionally, Hugging Face provides an excellent [summary of the tokenizers](https://huggingface.co/docs/transformers/en/tokenizer_summary)
    guide, in which I use some of its examples in this article.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Hugging Face 提供了一份出色的[分词器总结](https://huggingface.co/docs/transformers/en/tokenizer_summary)指南，我在本文中使用了其中的一些示例。
- en: Let’s now explore how different subword tokenization algorithms work. Note that
    all of those tokenization algorithms rely on some form of training which is usually
    done on the corpus the corresponding model will be trained on.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们探索不同的子词分词算法是如何工作的。请注意，所有这些分词算法都依赖于某种形式的训练，通常是在与对应模型训练相关的语料库上进行的。
- en: Byte-Pair Encoding (BPE)
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 字节对编码（BPE）
- en: B**yte-Pair Encoding** is a subword tokenization method introduced in [Neural
    Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909)
    by Sennrich et al. in 2015\. BPE starts with a base vocabulary consisting of all
    unique characters in the training data and iteratively merges the most frequent
    pairs of symbols — which can be characters or sequences of characters — to form
    new subwords. This process continues until the vocabulary reaches a predefined
    size, which is a hyperparameter you choose before training.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: B**字节对编码**（Byte-Pair Encoding，BPE）是一种子词分词方法，最早由 Sennrich 等人在 2015 年的论文 [《使用子词单元的稀有词神经机器翻译》](https://arxiv.org/abs/1508.07909)
    中提出。BPE 从一个包含所有唯一字符的基础词汇开始，逐步合并最频繁的符号对——这些符号可以是字符或字符序列——以形成新的子词。这个过程会持续进行，直到词汇表达到预定义的大小，这是你在训练前选择的超参数。
- en: 'Suppose we have the following words with their frequencies:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有以下单词及其频率：
- en: '`“hug”` (10 occurrences)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`“hug”`（出现 10 次）'
- en: '`“pug”` (5 occurrences)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`“pug”`（出现 5 次）'
- en: '`“pun”` (12 occurrences)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`“pun”`（出现 12 次）'
- en: '`“bun”` (4 occurrences)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`“bun”`（出现 4 次）'
- en: '`“hugs”` (5 occurrences)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`“hugs”`（出现 5 次）'
- en: 'Our initial base vocabulary consists of the following characters: `[“h”, “u”,
    “g”, “p”, “n”, “b”, “s”]`.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的初始基础词汇包含以下字符：`[“h”, “u”, “g”, “p”, “n”, “b”, “s”]`。
- en: 'We split the words into individual characters:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将单词拆分为单个字符：
- en: '`“h” “u” “g”` (hug)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`“h” “u” “g”`（hug）'
- en: '`“p” “u” “g”` (pug)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`“p” “u” “g”`（pug）'
- en: '`“p” “u” “n”` (pun)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`“p” “u” “n”`（pun）'
- en: '`“b” “u” “n”` (bun)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`“b” “u” “n”`（bun）'
- en: '`“h” “u” “g” “s”` (hugs)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`“h” “u” “g” “s”`（hugs）'
- en: 'Next, we count the frequency of each symbol pair:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算每个符号对的频率：
- en: '`“h u”`: 15 times (from `“hug”` and `“hugs”`)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`“h u”`：出现 15 次（来自 `“hug”` 和 `“hugs”`）'
- en: '`“u g”`: 20 times (from `“hug”`, `“pug”`, `“hugs”`)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`“u g”`：出现 20 次（来自 `“hug”`，`“pug”`，`“hugs”`）'
- en: '`“p u”`: 17 times (from `“pug”`, `“pun”`)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`“p u”`：出现 17 次（来自 `“pug”`，`“pun”`）'
- en: '`“u n”`: 16 times (from `“pun”`, `“bun”`)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`“u n”`：出现 16 次（来自 `“pun”`，`“bun”`）'
- en: 'The most frequent pair is `“u g”` (20 times), so we merge `“u”` and `“g”` to
    form `“ug”` and update our words:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最频繁的字符对是 `“u g”`（出现 20 次），因此我们将 `“u”` 和 `“g”` 合并为 `“ug”` 并更新我们的单词：
- en: '`“h” “ug”` (hug)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`“h” “ug”`（hug）'
- en: '`“p” “ug”` (pug)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`“p” “ug”`（pug）'
- en: '`“p” “u” “n”` (pun)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`“p” “u” “n”`（pun）'
- en: '`“b” “u” “n”` (bun)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`“b” “u” “n”`（bun）'
- en: '`“h” “ug” “s”` (hugs)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`“h” “ug” “s”`（hugs）'
- en: We continue this process, merging the next most frequent pairs, such as `“u
    n”` into `“un”`, until we reach our desired vocabulary size.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续进行此过程，合并下一个最频繁的符号对，例如将 `“u n”` 合并为 `“un”`，直到达到我们期望的词汇大小。
- en: BPE controls the vocabulary size by specifying the number of merge operations.
    Frequent words remain intact, reducing the need for extensive memorization. And,
    rare or unseen words can be represented through combinations of known subwords.
    It’s used in models like [**GPT**](https://openai.com/index/language-unsupervised/)
    and [**RoBERTa**](https://arxiv.org/abs/1907.11692).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: BPE 通过指定合并操作的次数来控制词汇大小。频繁的单词保持不变，从而减少了大量记忆的需求。同时，罕见或未见过的单词可以通过已知子词的组合来表示。它被用于像
    [**GPT**](https://openai.com/index/language-unsupervised/) 和 [**RoBERTa**](https://arxiv.org/abs/1907.11692)
    这样的模型中。
- en: The Hugging Face tokenizers library provides a fast and flexible way to train
    and use tokenizers, including BPE.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 分词器库提供了一种快速且灵活的方法来训练和使用分词器，包括 BPE。
- en: Training a BPE Tokenizer
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练 BPE 分词器
- en: 'Here’s how to train a BPE tokenizer on a sample dataset:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何在一个示例数据集上训练 BPE 分词器的方法：
- en: '[PRE9]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Using the trained BPE Tokenizer:**'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用训练好的 BPE 分词器：**'
- en: '[PRE10]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Output:**'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：**'
- en: '[PRE11]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: WordPiece
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: WordPiece
- en: W**ordPiece** is another subword tokenization algorithm, outlined by [Schuster
    and Nakajima in 2012](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf)
    and popularized by models like [**BERT**](https://arxiv.org/abs/1810.04805#).
    Similar to BPE, WordPiece starts with all unique characters but differs in how
    it selects which symbol pairs to merge.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: W**ordPiece** 是另一种子词分词算法，由 [Schuster 和 Nakajima 于 2012 年](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf)
    提出，并由像 [**BERT**](https://arxiv.org/abs/1810.04805#) 这样的模型广泛使用。与 BPE 相似，WordPiece
    也从所有唯一字符开始，但在选择合并的符号对时有所不同。
- en: 'Here’s how WordPiece works:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是 WordPiece 的工作原理：
- en: '**Initialization**: Start with a vocabulary of all unique characters.'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**初始化**：从包含所有唯一字符的词汇表开始。'
- en: '**Pre-tokenization**: Split the training text into words.'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预分词**：将训练文本拆分为单词。'
- en: '**Building the Vocabulary**: Iteratively add new symbols (subwords) to the
    vocabulary.'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**构建词汇表**：通过迭代添加新的符号（子词）到词汇表中。'
- en: '**Selection Criterion**: Instead of choosing the most frequent symbol pair,
    WordPiece selects the pair that maximizes the likelihood of the training data
    when added to the vocabulary.'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择标准**：与选择最常见的符号对不同，WordPiece 选择的符号对是将其加入词汇表后，最大化训练数据的可能性。'
- en: Using the same word frequencies as before, WordPiece evaluates which symbol
    pair, when merged, would most increase the probability of the training data. This
    involves a more probabilistic approach compared to BPE’s frequency-based method.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 使用与之前相同的词频，WordPiece 评估哪个符号对在合并后能最有效地提高训练数据的概率。这比 BPE 基于频率的方法更具概率性。
- en: Similar to BPE, we can train a WordPiece tokenizer using the `tokenizers` library.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 BPE，我们可以使用`tokenizers`库训练一个 WordPiece 分词器。
- en: Training a WordPiece Tokenizer
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练一个 WordPiece 分词器
- en: '[PRE12]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Using the trained WordPiece tokenizer:**'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用训练好的 WordPiece 分词器：**'
- en: '[PRE13]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**Output:**'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：**'
- en: '[PRE14]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Conclusion**'
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**结论**'
- en: Tokenization is a foundational step in NLP that prepares text data for computational
    models. By understanding and implementing appropriate tokenization strategies,
    we enable models to process and generate human language more effectively, setting
    the stage for advanced topics like word embeddings and language modeling.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 分词是自然语言处理中的基础步骤，旨在为计算模型准备文本数据。通过理解和实施适当的分词策略，我们使模型能够更有效地处理和生成自然语言，为像词嵌入和语言建模等高级主题打下基础。
- en: 'All the code in this article is also available on my GitHub repo: [**github.com/murilogustineli/nlp-medium**](https://github.com/murilogustineli/nlp-medium)'
  id: totrans-128
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本文中的所有代码也可以在我的 GitHub 仓库中找到：[**github.com/murilogustineli/nlp-medium**](https://github.com/murilogustineli/nlp-medium)
- en: Other Resources
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他资源
- en: '[Let’s build the GPT Tokenizer | Andrej Karpathy on YouTube](https://www.youtube.com/watch?v=zduSFxRajkE)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[让我们来构建 GPT 分词器 | Andrej Karpathy 在 YouTube 上](https://www.youtube.com/watch?v=zduSFxRajkE)'
- en: '[Tokenization | Mistral AI Large Language Models](https://docs.mistral.ai/guides/tokenization/)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分词 | Mistral AI 大型语言模型](https://docs.mistral.ai/guides/tokenization/)'
- en: '[Summary of the tokenizers | Hugging Face](https://huggingface.co/docs/transformers/en/tokenizer_summary)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分词器总结 | Hugging Face](https://huggingface.co/docs/transformers/en/tokenizer_summary)'
- en: '[Building a tokenizer, block by block | Hugging Face](https://huggingface.co/learn/nlp-course/en/chapter6/8)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[构建分词器，一步步来 | Hugging Face](https://huggingface.co/learn/nlp-course/en/chapter6/8)'
- en: Unless otherwise noted, all images are created by the author.
  id: totrans-134
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图片均由作者创建。
