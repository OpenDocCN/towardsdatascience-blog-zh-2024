- en: How to Train a Vision Transformer (ViT) from Scratch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何从零开始训练Vision Transformer（ViT）
- en: 原文：[https://towardsdatascience.com/how-to-train-a-vision-transformer-vit-from-scratch-f26641f26af2?source=collection_archive---------3-----------------------#2024-09-04](https://towardsdatascience.com/how-to-train-a-vision-transformer-vit-from-scratch-f26641f26af2?source=collection_archive---------3-----------------------#2024-09-04)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-train-a-vision-transformer-vit-from-scratch-f26641f26af2?source=collection_archive---------3-----------------------#2024-09-04](https://towardsdatascience.com/how-to-train-a-vision-transformer-vit-from-scratch-f26641f26af2?source=collection_archive---------3-----------------------#2024-09-04)
- en: A practical guide to implementing the Vision Transformer (ViT)
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现Vision Transformer（ViT）的实用指南
- en: '[](https://medium.com/@francoisporcher?source=post_page---byline--f26641f26af2--------------------------------)[![François
    Porcher](../Images/9ddb233f8cadbd69026bd79e2bd62dea.png)](https://medium.com/@francoisporcher?source=post_page---byline--f26641f26af2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f26641f26af2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f26641f26af2--------------------------------)
    [François Porcher](https://medium.com/@francoisporcher?source=post_page---byline--f26641f26af2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@francoisporcher?source=post_page---byline--f26641f26af2--------------------------------)[![François
    Porcher](../Images/9ddb233f8cadbd69026bd79e2bd62dea.png)](https://medium.com/@francoisporcher?source=post_page---byline--f26641f26af2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f26641f26af2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f26641f26af2--------------------------------)
    [François Porcher](https://medium.com/@francoisporcher?source=post_page---byline--f26641f26af2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f26641f26af2--------------------------------)
    ·12 min read·Sep 4, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f26641f26af2--------------------------------)
    ·12分钟阅读·2024年9月4日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Hi everyone! For those who do not know me yet, my name is Francois, I am a Research
    Scientist at Meta. I have a passion for explaining advanced AI concepts and making
    them more accessible.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 大家好！对于那些还不认识我的朋友们，我叫Francois，我是Meta的研究科学家。我热衷于解释先进的人工智能概念，并使其变得更加易懂。
- en: 'Today, let’s dive into one of the most significant contribution in the field
    of Computer Vision: the **Vision Transformer (ViT).**'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，让我们深入探讨计算机视觉领域最重要的贡献之一：**视觉Transformer（ViT）**。
- en: 'This post focuses on the state-of-the-art implementation of the Vision Transformer
    since its release. To fully understand how a ViT works, I strongly recommend reading
    my other post on the theoretical foundations: [The Ultimate Guide to Vision Transformers](https://medium.com/towards-data-science/the-ultimate-guide-to-vision-transformers-0a6df32cb248)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文重点介绍自Vision Transformer发布以来的最新实现。为了深入理解ViT的工作原理，我强烈建议阅读我关于其理论基础的另一篇文章：[Vision
    Transformer终极指南](https://medium.com/towards-data-science/the-ultimate-guide-to-vision-transformers-0a6df32cb248)
- en: How to train a VIT From scratch?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何从零开始训练ViT？
- en: '![](../Images/ffb3dc35549048018b8f386287afdb0f.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ffb3dc35549048018b8f386287afdb0f.png)'
- en: ViT Architecture, image from [original article](https://arxiv.org/pdf/2010.11929)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ViT架构，图片来自[原始文章](https://arxiv.org/pdf/2010.11929)
- en: 1\. Attention Layer
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 注意力层
- en: '![](../Images/f8025d10646523676e2805e3db31de95.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8025d10646523676e2805e3db31de95.png)'
- en: Attention Layer, image by author
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力层，图片由作者提供
- en: 'Let’s start with the most well-known building block of the Transformer Encoder:
    the Attention Layer.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从Transformer编码器中最著名的构建模块——注意力层开始。
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
