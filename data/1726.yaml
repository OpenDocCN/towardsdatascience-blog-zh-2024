- en: Topic Modeling Open-Source Research with the OpenAlex API
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 OpenAlex API 进行开源研究的主题建模
- en: 原文：[https://towardsdatascience.com/topic-modeling-open-source-research-with-the-openalex-api-5191c7db9156?source=collection_archive---------7-----------------------#2024-07-15](https://towardsdatascience.com/topic-modeling-open-source-research-with-the-openalex-api-5191c7db9156?source=collection_archive---------7-----------------------#2024-07-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/topic-modeling-open-source-research-with-the-openalex-api-5191c7db9156?source=collection_archive---------7-----------------------#2024-07-15](https://towardsdatascience.com/topic-modeling-open-source-research-with-the-openalex-api-5191c7db9156?source=collection_archive---------7-----------------------#2024-07-15)
- en: Open-source intelligence (OSINT) is something that can add tremendous value
    to organizations. Insight gained from analyzing social media data, web data, or
    global research, can be great in supporting all kinds of analyses. This article
    will give an overview of using topic modeling to help us make sense of thousands
    of pieces of open-source research.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开源情报（OSINT）是能够为组织带来巨大价值的一项技术。从分析社交媒体数据、网络数据或全球研究中获得的洞察，可以在支持各种分析时发挥重要作用。本文将概述如何使用主题建模来帮助我们理解成千上万的开源研究资料。
- en: '[](https://medium.com/@adavis08?source=post_page---byline--5191c7db9156--------------------------------)[![Alex
    Davis](../Images/f773cce9438a68856cb8ba486ac8b051.png)](https://medium.com/@adavis08?source=post_page---byline--5191c7db9156--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5191c7db9156--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5191c7db9156--------------------------------)
    [Alex Davis](https://medium.com/@adavis08?source=post_page---byline--5191c7db9156--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@adavis08?source=post_page---byline--5191c7db9156--------------------------------)[![Alex
    Davis](../Images/f773cce9438a68856cb8ba486ac8b051.png)](https://medium.com/@adavis08?source=post_page---byline--5191c7db9156--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5191c7db9156--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5191c7db9156--------------------------------)
    [Alex Davis](https://medium.com/@adavis08?source=post_page---byline--5191c7db9156--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5191c7db9156--------------------------------)
    ·9 min read·Jul 15, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5191c7db9156--------------------------------)
    ·阅读时长9分钟·2024年7月15日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ee2aac991c4193c160d128a71f553a42.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee2aac991c4193c160d128a71f553a42.png)'
- en: '“resorting to paper… #research# #proposal” by [catherinecronin](https://www.flickr.com/photos/catherinecronin/14326101068)
    is licensed under CC BY-SA 2.0\. To view a copy of this license, visit [https://creativecommons.org/licenses/by-sa/2.0/?ref=openverse.](https://creativecommons.org/licenses/by-sa/2.0/?ref=openverse.)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '“使用纸张…… #research# #proposal”由[catherinecronin](https://www.flickr.com/photos/catherinecronin/14326101068)授权，采用CC
    BY-SA 2.0许可。要查看此许可证的副本，请访问[https://creativecommons.org/licenses/by-sa/2.0/?ref=openverse.](https://creativecommons.org/licenses/by-sa/2.0/?ref=openverse.)'
- en: '**What is Topic Modeling?**'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**什么是主题建模？**'
- en: Topic modeling is an unsupervised machine learning technique used to analyze
    documents and identity ‘topics’ using semantic similarity. This is similar to
    clustering, but not every document is exclusive to one topic. It is more about
    grouping the content found in a corpus. Topic modeling has many different applications
    but is mainly used to better understand large amounts of text data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模是一种无监督的机器学习技术，用于分析文档并通过语义相似性识别“主题”。这类似于聚类，但并非每个文档都独立属于一个主题。它更多的是将语料库中的内容进行分组。主题建模有许多不同的应用，但主要用于更好地理解大量文本数据。
- en: '![](../Images/ae6dbdf009e015f0e89bfb45c5c4be55.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae6dbdf009e015f0e89bfb45c5c4be55.png)'
- en: Image by Author
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: For example, a retail chain may model customer surveys and reviews to identify
    negative reviews and drill down into the key issues outlined by their customers.
    In this case, we will import a large amount of articles and abstracts to understand
    the key topics in our dataset.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一家零售连锁可以对客户调查和评论进行建模，以识别负面评论并深入分析客户提到的关键问题。在这个案例中，我们将导入大量的文章和摘要，以理解数据集中的关键主题。
- en: '*Note: Topic modeling can be computationally expensive at scale. In this example,
    I used the Amazon Sagemaker environment to take advantage of their CPU.*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：在大规模应用中，主题建模可能会消耗大量计算资源。在这个示例中，我使用了 Amazon Sagemaker 环境，以利用其 CPU 的优势。*'
- en: OpenAlex
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAlex
- en: OpenAlex is a free to use catalogue system of global research. They have indexed
    over 250 million pieces of news, articles, abstracts, and more.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAlex是一个免费使用的全球研究目录系统。他们已经索引了超过2.5亿篇新闻、文章、摘要等。
- en: '[## OpenAlex'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[## OpenAlex'
- en: Edit description
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编辑描述
- en: openalex.org](https://openalex.org/?source=post_page-----5191c7db9156--------------------------------)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[openalex.org](https://openalex.org/?source=post_page-----5191c7db9156--------------------------------)'
- en: Luckily for us, they have a free (but limited) and flexible API that will allow
    us to quickly ingest tens of thousands of articles while also applying filters,
    such as year, type of media, keywords, etc.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，他们提供了一个免费的（但有限制）灵活API，可以让我们快速获取成千上万篇文章，并且应用过滤条件，如年份、媒体类型、关键词等。
- en: Creating a Data Pipeline
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建数据管道
- en: While we ingest the data from the API, we will apply some criteria. First, we
    will only ingest documents where the year is between 2016 and 2022\. We want fairly
    recent language as terms and taxonomy of certain subjects can change over long
    periods of time.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们从API获取数据时，我们会应用一些标准。首先，我们只会获取年份在2016到2022年之间的文档。我们希望使用相对较新的语言，因为某些主题的术语和分类法会随着时间的推移发生变化。
- en: We will also add key terms and conduct multiple searches. While normally we
    would likely ingest random subject areas, we will use key terms to narrow our
    search. This way, we will have an idea of how may high-level topics we have, and
    can compare that to the output of the model. Below, we create a function where
    we can add key terms and conduct searches through the API.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将添加关键术语并进行多次搜索。虽然通常我们可能会随机获取不同的主题领域，但我们将使用关键术语来缩小搜索范围。通过这种方式，我们可以了解有多少个高级主题，并将其与模型的输出进行比较。下面，我们创建一个可以添加关键术语并通过API进行搜索的函数。
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We conduct 5 different searches, each being a different technology area. These
    technology areas are inspired by the DoD “Critical Technology Areas”. See more
    here:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行5次不同的搜索，每次搜索涉及不同的技术领域。这些技术领域受到国防部“关键技术领域”的启发。详情请见：
- en: '[](https://www.cto.mil/usdre-strat-vision-critical-tech-areas/?source=post_page-----5191c7db9156--------------------------------)
    [## USD(R&E) Strategic Vision and Critical Technology Areas - DoD Research & Engineering,
    OUSD(R&E)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://www.cto.mil/usdre-strat-vision-critical-tech-areas/?source=post_page-----5191c7db9156--------------------------------)
    [## USD(R&E)战略愿景与关键技术领域 - 国防部研究与工程，OUSD(R&E)'
- en: The OUSD(R&E) works closely with the Military Services, Combatant Commands,
    industry, academia, and other stakeholders…
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OUSD(R&E)与军事服务部门、战区司令部、工业界、学术界以及其他利益相关方紧密合作……
- en: www.cto.mil](https://www.cto.mil/usdre-strat-vision-critical-tech-areas/?source=post_page-----5191c7db9156--------------------------------)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[www.cto.mil](https://www.cto.mil/usdre-strat-vision-critical-tech-areas/?source=post_page-----5191c7db9156--------------------------------)'
- en: 'Here is an example of a search using the required OpenAlex syntax:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个使用所需OpenAlex语法进行搜索的示例：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: After compiling our searches and dropping duplicate documents, we must clean
    the data to prepare it for our topic model. There are 2 main issues with our current
    output.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在整理我们的搜索结果并去除重复文档之后，我们必须清理数据，为主题模型做准备。当前输出存在两个主要问题。
- en: The abstracts are returned as an inverted index (due to legal reasons). However,
    we can use these to return the original text.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 摘要以倒排索引的形式返回（出于法律原因）。然而，我们可以利用这些索引返回原始文本。
- en: Once we obtain the original text, it will be raw and unprocessed, creating noise
    and hurting our model. We will conduct traditional NLP preprocessing to get it
    ready for the model.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们获得原始文本，它将是未经处理的原始数据，产生噪声并影响我们的模型表现。我们将进行传统的NLP预处理，以便为模型做好准备。
- en: Below is a function to return original text from an inverted index.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个从倒排索引中返回原始文本的函数。
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now that we have the raw text, we can conduct our traditional preprocessing
    steps, such as standardization, removing stop words, lemmatization, etc. Below
    are functions that can be mapped to a list or series of documents.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经得到了原始文本，我们可以进行传统的预处理步骤，如标准化、去除停用词、词形还原等。下面是可以映射到一系列文档的函数。
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now that we have a preprocessed series of documents, we can create our first
    topic model!
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一系列经过预处理的文档，我们可以创建我们的第一个主题模型！
- en: Creating a Topic Model
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建主题模型
- en: For our topic model, we will use gensim to create a Latent Dirichlet Allocation
    (LDA) model. LDA is the most common model for topic modeling, as it is very effective
    in identifying high-level themes within a corpus. Below are the packages used
    to create the model.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的主题模型，我们将使用 gensim 创建一个潜在狄利克雷分配（LDA）模型。LDA 是最常见的主题建模模型，因为它在识别语料库中的高层次主题方面非常有效。下面是用于创建模型的包。
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Before we create our model, we must prepare our corpus and ID mappings. This
    can be done with just a few lines of code.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建模型之前，我们必须准备我们的语料库和 ID 映射。这只需要几行代码就能完成。
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now we can create a topic model. As you will see below, there are many different
    parameters that will affect the model’s performance. You can read about the many
    parameters in gensim’s documentation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以创建一个主题模型。如你所见，下面有许多不同的参数会影响模型的表现。你可以在 gensim 的文档中阅读关于这些参数的更多信息。
- en: '[PRE10]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The most import parameter will be the number of topics. Here, we set an arbitrary
    10\. Since we don’t know how many topics there *should* be, this parameter should
    definitely be optimized. But how do we measure the quality of our model?
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的参数是主题的数量。在这里，我们设置了一个任意的 10。由于我们不知道应该有多少个主题，这个参数肯定需要优化。但我们如何衡量模型的质量呢？
- en: This is where coherence scores come in. The coherence score is a measure from
    0–1\. Coherence scores measure the quality of our topics by making sure they are
    sound and distinct. We want clear boundaries between well-defined topics. While
    this is a bit subjective in the end, it gives us a great idea of the quality of
    our results.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这时，连贯性得分就显得尤为重要。连贯性得分是一个介于 0 和 1 之间的量度。连贯性得分通过确保主题是合理且独特的来衡量我们主题的质量。我们希望在定义明确的主题之间有清晰的边界。虽然这在最终上是有些主观的，但它能让我们对结果的质量有一个很好的了解。
- en: '[PRE11]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Here, we get a coherence score of about 0.48, which isn’t too bad! But not ready
    for production.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们得到了大约 0.48 的连贯性得分，虽然不是太差！但还不能投入生产使用。
- en: Visualize Our Topic Model
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化我们的主题模型
- en: Topic models can be difficult to visualize. Lucky for us, there is a great module
    ‘pyLDAvis’ that can automatically produce an interactive visualization that allows
    us to view our topics in a vector space and drill down into each topic.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 主题模型可能很难进行可视化。幸运的是，我们有一个很棒的模块‘pyLDAvis’，它能够自动生成一个交互式可视化，允许我们在向量空间中查看主题，并深入探讨每个主题。
- en: '[PRE12]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As you can see below, this produces a great visualization where we can get a
    quick idea of how our model performed. By looking into the vector space, we see
    some topics are distinct and well-defined. However, we also have some overlapping
    topics.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，这生成了一个很好的可视化图，我们可以快速了解模型的表现。通过查看向量空间，我们可以看到一些主题是明确且定义良好的。然而，也有一些主题是重叠的。
- en: '![](../Images/528452b69c598401246c2e00ad1dcb42.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/528452b69c598401246c2e00ad1dcb42.png)'
- en: Image by Author
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: We can click on a topic to few the most relevant tokens. As we adjust the relevance
    metric (lambda), we can see topic-specific tokens by sliding it left, and seeing
    relevant but less topic-specific tokens by sliding it to the right.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以点击一个主题查看最相关的词汇。当我们调整相关性度量（lambda）时，可以通过将其向左滑动来查看特定于主题的词汇，而通过将其向右滑动则可以看到相关但不那么特定于主题的词汇。
- en: When clicking into each topic, I can vaguely see the topics that I originally
    searched for. For example, topic 5 seems to align with my ‘human-machine interfaces’
    search. There is also a cluster of topics that seem to be related to biotechnology,
    but some are more clear than others.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当点击每个主题时，我可以隐约看到我最初搜索的主题。例如，主题 5 似乎与我的‘人机界面’搜索一致。也有一簇看起来与生物技术相关的主题，但有些更为明确，有些则不太清楚。
- en: Optimize the Topic Model
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化主题模型
- en: From the pyLDAvis interface and our coherence score of 0.48, there is definitely
    room for improvement. For our final step, lets write a function where we can loop
    through values for different parameters and try to optimize our coherence score.
    Below, is a function that tests different values of the number of topics and the
    decay rate. The function computes the coherence score for every combination of
    parameters and saves them in a data frame.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从 pyLDAvis 界面和我们的连贯性得分 0.48 来看，肯定还有提升的空间。在我们的最后一步，先编写一个函数，让我们可以循环通过不同参数的值并尝试优化我们的连贯性得分。下面是一个测试不同主题数量和衰减率值的函数。该函数为每一组参数组合计算连贯性得分并将其保存在数据框中。
- en: '[PRE13]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Just by passing a couple of small ranges through two parameters, we identified
    parameters that increased our coherence score from 0.48 to 0.55, a sizable improvement.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将几个小范围的值传递给两个参数，我们找出了能够将我们的连贯性得分从 0.48 提高到 0.55 的参数，取得了显著的改善。
- en: Next Steps
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下一步
- en: To continue to build a production-level model, there is plenty of experimentation
    to be had with the parameters. Because LDA is so computationally expensive, I
    kept the experiment above limited and only compared about 20 different models.
    But with more time and power, we can compare hundreds of models.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了继续构建一个生产级别的模型，我们需要在参数上进行大量实验。由于LDA计算开销较大，我将上述实验限定在比较大约20个不同的模型。但如果有更多的时间和计算能力，我们可以比较数百个模型。
- en: As well, there are improvements to be made with our data pipeline. I noticed
    several words that may need to be added to our stop word list. Words like ‘use’
    and ‘department’ are not adding any semantic value, especially for documents about
    different technologies. As well, there are technical terms that do not get processed
    correctly, resulting in a single letter or a group of letters. We could spend
    some time doing a bag-of-words analysis to identify those stop word opportunities.
    This would eliminate noise in our dataset.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们的数据管道也有待改进。我注意到有几个词可能需要添加到停用词列表中。像“use”和“department”这样的词并没有提供任何语义价值，特别是对于不同技术领域的文档来说。还有一些技术术语未能正确处理，结果变成了单个字母或一组字母。我们可以花一些时间做一个词袋分析，以识别这些停用词机会。这将有助于减少数据集中的噪音。
- en: Conclusion
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'In this article, we:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们：
- en: Got an introduction to topic modeling and the OpenAlex data source
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 了解了主题建模和OpenAlex数据源的基本概念
- en: Built a data pipeline to ingest data from an API and prepare it for an NLP model
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建了一个数据管道，用于从API中获取数据并为NLP模型做准备
- en: Constructed an LDA model and visualized the results using pyLDAvis
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建了一个LDA模型，并使用pyLDAvis可视化结果
- en: Wrote code to help us find the optimal parameters
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写了代码来帮助我们找到最优参数
- en: Discussed next steps for model improvement
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 讨论了模型改进的下一步
- en: '*This is my first Medium article, so I hope you enjoyed it. Please feel free
    to leave feedback, ask questions, or request other topics!*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*这是我在Medium上的第一篇文章，希望你喜欢。请随时留下反馈、提问或提出其他话题的请求！*'
