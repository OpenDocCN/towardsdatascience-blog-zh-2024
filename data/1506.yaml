- en: A Patch is More than 16*16 Pixels
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个 Patch 大小超过了 16*16 像素
- en: 原文：[https://towardsdatascience.com/a-patch-is-more-than-16-16-pixels-699359211513?source=collection_archive---------1-----------------------#2024-06-17](https://towardsdatascience.com/a-patch-is-more-than-16-16-pixels-699359211513?source=collection_archive---------1-----------------------#2024-06-17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-patch-is-more-than-16-16-pixels-699359211513?source=collection_archive---------1-----------------------#2024-06-17](https://towardsdatascience.com/a-patch-is-more-than-16-16-pixels-699359211513?source=collection_archive---------1-----------------------#2024-06-17)
- en: On Pixel Transformer and Ultra-long Sequence Distributed Transformer
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于像素变换器和超长序列分布式变换器
- en: '[](https://mengliuz.medium.com/?source=post_page---byline--699359211513--------------------------------)[![Mengliu
    Zhao](../Images/0b950a0785fa065db3319ed5be4a91de.png)](https://mengliuz.medium.com/?source=post_page---byline--699359211513--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--699359211513--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--699359211513--------------------------------)
    [Mengliu Zhao](https://mengliuz.medium.com/?source=post_page---byline--699359211513--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mengliuz.medium.com/?source=post_page---byline--699359211513--------------------------------)[![Mengliu
    Zhao](../Images/0b950a0785fa065db3319ed5be4a91de.png)](https://mengliuz.medium.com/?source=post_page---byline--699359211513--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--699359211513--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--699359211513--------------------------------)
    [Mengliu Zhao](https://mengliuz.medium.com/?source=post_page---byline--699359211513--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--699359211513--------------------------------)
    ·7 min read·Jun 17, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--699359211513--------------------------------)
    ·阅读时间 7 分钟·2024年6月17日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Have you ever wondered why the Vision Transformer (ViT) uses 16*16 size patches
    as input tokens?
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否曾经想过，为什么Vision Transformer（ViT）使用16*16大小的patch作为输入token？
- en: It all dates back to the earlier days of the Transformers. The original Transformer
    model was proposed in 2017 and only works with natural language data. When the
    BERT model was released in 2018, it could only handle a max token sequence of
    length 512\. Later in 2020, when GPT-3 was released, it could handle a sequence
    of lengths 2048 and 4096 at 3.5\. All these models showed amazing performance
    in handling sequence-to-sequence and text-generation tasks.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切可以追溯到变换器模型的早期。原始的变换器模型在2017年提出，且仅适用于自然语言数据。当BERT模型在2018年发布时，它只能处理最大长度为512的token序列。到了2020年，当GPT-3发布时，它能够处理长度为2048和4096的序列（3.5版本）。所有这些模型在处理序列到序列和文本生成任务时都展示了惊人的表现。
- en: '![](../Images/93f6207bcc1de07dfba8b303c8aa6e54.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93f6207bcc1de07dfba8b303c8aa6e54.png)'
- en: 'Image source: [https://pxhere.com/en/photo/141620](https://pxhere.com/en/photo/141620)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：[https://pxhere.com/en/photo/141620](https://pxhere.com/en/photo/141620)
- en: However, these sequences were too short for images when tokens were taken at
    the pixel level. For example, in Cifar-100, the image size is 32* 32 = 1024 pixels.
    In ImageNet, the image size is 224* 224 = 50176 pixels. The sequence length would
    be an immediate barrier if the transformer were directly applied to the pixel
    level.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当tokens按像素级别提取时，这些序列对于图像来说过于简短。例如，在Cifar-100中，图像大小是32*32 = 1024像素。在ImageNet中，图像大小是224*224
    = 50176像素。如果直接将变换器应用于像素级别，序列长度将立即成为一个障碍。
- en: The ViT paper was released in 2020\. It proposed using patches rather than pixels
    as the input tokens. For an image of size 224* 224, using a patch size of 16*
    16, the sequence length would be largely reduced to 196, which perfectly solved
    the issue.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ViT论文于2020年发布，提出使用patch而非像素作为输入token。对于一个224*224大小的图像，使用16*16的patch大小，序列长度将大幅减少至196，这完美解决了这个问题。
- en: However, the issue was only partially solved. **For tasks requiring features
    of finer details, different approaches have to be utilized to get the pixel-level
    accuracy back**. Segformer proposed to fuse features from hierarchical transformer
    encoders of different resolutions. SwinIR had to combine CNN with multiple levels
    of skip connection around the transformer module for fine-grained feature extraction.
    The Swin Transformer, a model for universal computer vision tasks, started with
    a patch size 4*4 in each local window and then gradually built toward the global
    16*16 patch size to obtain both globality and granularity. Intrinsically, these
    efforts pointed to one fact — simply using the 16*16 size patch is insufficient.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个问题仅部分解决了。**对于需要更精细特征的任务，必须采用不同的方法以恢复像素级的精度**。Segformer提出通过融合来自不同分辨率的层次化变换器编码器的特征来解决这个问题。SwinIR则结合了卷积神经网络（CNN）与变换器模块周围的多层跳跃连接进行细粒度特征提取。Swin
    Transformer是一个用于通用计算机视觉任务的模型，它从每个局部窗口的4*4补丁大小开始，然后逐渐构建到全球16*16补丁大小，以同时获得全局性和细粒度性。从本质上讲，这些努力指向一个事实——仅使用16*16大小的补丁是不够的。
- en: 'The natural question is, can "pixel" be used as a direct token for transformers?
    The question further splits into two: 1\. Is it possible to feed an ultralong
    sequence (e.g., 50k) to a transformer? 2\. does feeding pixels as tokens provide
    more information than patches?'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 自然的问题是，“像素”是否可以作为变换器的直接标记？这个问题进一步分为两个：1\. 是否可以将超长序列（例如，50k）输入变换器？2\. 将像素作为标记输入是否比输入补丁提供更多的信息？
- en: 'In this article, I will summarize two recent papers: 1\. **Pixel Transformer**,
    a technical report released by Meta AI last week, comparing pixel-wise tokens
    and patch-wise tokens to transformer models from the perspective of reducing the
    inductive bias of locality on three different tasks: image classification, pre-training,
    and generation. 2\. **Ultra-long sequence distributed transformer**: by distributing
    the query vector, the authors showed the possibility to scale an input sequence
    of length 50k on 3k GPUs.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将总结两篇最近的论文：1\. **像素变换器**，这是Meta AI上周发布的技术报告，从减少局部归纳偏置的角度，比较了像素级标记和补丁级标记在图像分类、预训练和生成等三项不同任务中的变换器模型表现。2\.
    **超长序列分布式变换器**：通过分布查询向量，作者展示了在3k个GPU上扩展50k长度输入序列的可能性。
- en: '**Pixel Transformer (PiT) — from an inductive bias perspective**'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**像素变换器（PiT）——从归纳偏置的角度**'
- en: 'Meta AI released The technical report last week on arXiv: "**An image is worth
    more than 16*16 patches”**. Instead of proposing a novel method, the technical
    report answered a long-lasting question: Does it make sense to use pixels instead
    of patches as input tokens? If so, why?'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Meta AI上周发布了技术报告，标题为：“**一张图比16*16的补丁更有价值**”。这份技术报告并没有提出一种新方法，而是回答了一个长期存在的问题：使用像素而不是补丁作为输入标记是否有意义？如果有，为什么？
- en: The paper took the perspective of the **Inductive Bias of Locality**. According
    to K. Murphy’s well-known [machine learning book](https://mitpress.mit.edu/9780262018029/machine-learning/),
    **inductive bias** is the “*assumptions about the nature of the data distribution*.”
    In the early “non-deep learning” era, the inductive bias was more “feature-related,”
    coming from the manual feature engineered for specific tasks. This inductive bias
    was not a bad thing, especially for specific tasks in which very good prior knowledge
    from human experts is gained, making the engineered features very useful. However,
    from the generalization perspective, the engineered features are very hard to
    generalize to universal tasks, like general image classification and segmentation.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 论文从**局部归纳偏置**的角度进行分析。根据K. Murphy的著名[《机器学习书籍》](https://mitpress.mit.edu/9780262018029/machine-learning/)，**归纳偏置**是“*关于数据分布性质的假设*”。在早期的“非深度学习”时代，归纳偏置更多是“特征相关的”，源自为特定任务人工设计的特征。这个归纳偏置并不是坏事，尤其是在那些通过人类专家获得非常好先验知识的特定任务中，使得设计的特征非常有用。然而，从泛化的角度来看，设计的特征很难泛化到通用任务，比如通用的图像分类和分割。
- en: But beyond feature bias, the architecture itself contains inductive bias as
    well. The ViT is a great example showing less inductive bias than CNN models in
    terms of **architecture hierarchy, propagation uniformness, representation scale,
    and attention locality**. [See my previous medium post for a detailed discussion](https://mengliuz.medium.com/paper-reading-do-vision-transformers-see-like-convolutional-neural-networks-94d4fdd85ff3).
    But still, ViT remains a special type of inductive bias — **locality**. When the
    ViT processes a sequence of patch tokens, the pixels within the same patch are
    naturally treated by the model differently than those from different patches.
    And that’s where the locality comes from.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 除了特征偏差之外，架构本身也包含了归纳偏差。ViT 就是一个很好的例子，展示了在**架构层次结构、传播均匀性、表示规模和注意力局部性**方面，比 CNN
    模型具有更少的归纳偏差。[详细讨论请参见我之前的 Medium 博客](https://mengliuz.medium.com/paper-reading-do-vision-transformers-see-like-convolutional-neural-networks-94d4fdd85ff3)。不过，ViT
    仍然保留了一种特殊类型的归纳偏差——**局部性**。当 ViT 处理一系列 patch tokens 时，同一 patch 内的像素会自然地与来自不同 patch
    的像素不同地被模型处理。这就是局部性的来源。
- en: 'So, is it possible to remove the inductive bias of locality further? The answer
    is yes. The PiT proposed using the “pixel set” as input with different position
    embedding (PE) strategies: sin-cos, learnt, and none. It showed superior performance
    over ViT on supervised, self-supervised, and generation tasks. The proposed pipeline
    is shown in the figure below.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，是否有可能进一步去除局部性的归纳偏差？答案是肯定的。PiT 提出了使用“像素集”作为输入，并采用不同的位置编码（PE）策略：sin-cos、学习式和无编码。它在有监督、自监督和生成任务中展现了比
    ViT 更优越的性能。下图展示了所提出的流水线。
- en: '![](../Images/7db566bffc4ae8c33bdb8c827f407ff7.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7db566bffc4ae8c33bdb8c827f407ff7.png)'
- en: 'Pixel Transformer pipeline. Image source: [https://arxiv.org/abs/2406.09415](https://arxiv.org/abs/2406.09415)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Pixel Transformer 流水线。图片来源：[https://arxiv.org/abs/2406.09415](https://arxiv.org/abs/2406.09415)
- en: The idea seems simple and straightforward, and the authors claim they are “not
    introducing a new method” here. But still, the PiT shows great potential. On CIFAR-100
    and ImageNet (reduced input size to 28*28) supervised classification tasks, the
    classification accuracy increased by more than 2% over ViT. See the table below.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法看似简单明了，作者声称他们“并没有引入一种新方法”。但是，PiT 仍展现出了巨大的潜力。在 CIFAR-100 和 ImageNet（输入大小缩小到
    28*28）的有监督分类任务中，分类准确率比 ViT 提高了超过 2%。请参见下表。
- en: '![](../Images/097eeadf6fa6b09160fd8a8c18143bde.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/097eeadf6fa6b09160fd8a8c18143bde.png)'
- en: 'Supervised learning classification. Image source: [https://arxiv.org/pdf/2406.09415](https://arxiv.org/pdf/2406.09415)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 有监督学习分类。图片来源：[https://arxiv.org/pdf/2406.09415](https://arxiv.org/pdf/2406.09415)
- en: 'Similar improvement was also observed in self-supervised learning tasks and
    image generation tasks. What’s more, the authors also showed the trend of a performance
    increase when reducing the patch size from 8*8 to 1*1 (single pixel) as below:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在自监督学习任务和图像生成任务中也观察到类似的提升。更重要的是，作者还展示了当将 patch 大小从 8*8 降到 1*1（单像素）时，性能提高的趋势，如下所示：
- en: '![](../Images/b221c83fb05588849fe8f07208d9dec5.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b221c83fb05588849fe8f07208d9dec5.png)'
- en: 'Performance increase from ViT (8*8 patch) to PiT (1*1 patch). Image from: [https://arxiv.org/pdf/2406.09415](https://arxiv.org/pdf/2406.09415)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 从 ViT（8*8 patch）到 PiT（1*1 patch）的性能提升。图片来自：[https://arxiv.org/pdf/2406.09415](https://arxiv.org/pdf/2406.09415)
- en: '**In terms of positional encoding.**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**关于位置编码。**'
- en: 'As pointed out in [this research paper](https://www.sciencedirect.com/science/article/abs/pii/S1047320322001845),
    positional encoding is a prerequisite in transformer-based models for input token
    sequence ordering and improving accuracy. However, the PiT shows that even after
    dropping the PE, the model performance drops is minimal:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 正如[这篇研究论文](https://www.sciencedirect.com/science/article/abs/pii/S1047320322001845)中指出的那样，位置编码是基于
    Transformer 的模型中，用于输入 token 序列排序和提高准确性的前提。然而，PiT 表明，即使去掉了位置编码，模型性能下降也很小：
- en: '![](../Images/f32198dc6792e2edbafdeaafb7df8a96.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f32198dc6792e2edbafdeaafb7df8a96.png)'
- en: 'PiT performance using three different PE: 1\. fixed sin-cos PE; 2\. learnable
    PE; 3\. no PE. Image source: [https://arxiv.org/pdf/2406.09415](https://arxiv.org/pdf/2406.09415)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用三种不同位置编码（PE）的 PiT 性能：1. 固定的 sin-cos PE；2. 可学习的 PE；3. 无 PE。图片来源：[https://arxiv.org/pdf/2406.09415](https://arxiv.org/pdf/2406.09415)
- en: Why drop the positional encoding? It is not only because dropping the positional
    encoding means a good reduction of the locality bias. If we think of self-attention
    computation in a distributed manner, it will largely reduce the cross-device communication
    effort, which we’ll discuss in detail in the next section.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要丢弃位置编码？不仅仅是因为丢弃位置编码意味着很好地减少了局部性偏差。如果我们以分布式方式思考自注意力计算，它将大大减少跨设备通信的工作量，关于这一点我们将在下一节详细讨论。
- en: '**Ultra-long Sequences Transformers — a distributed query vector solution**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**超长序列变换器——分布式查询向量解决方案**'
- en: '**The inductive bias of locality only told part of the story**. If we look
    closely at the results in the PiT paper, we see that the experiments were limited
    to 28*28 resized images due to the computational limit. But the real world rarely
    uses images of such a small size. So the natural question is, even though PiT
    might be useful and outperform ViT, could it work on natural images of standard
    resolution, e.g., 244*244?'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**局部性归纳偏差只是讲了一部分故事**。如果仔细查看PiT论文中的结果，我们会发现实验限于28*28大小的调整图像，这是由于计算限制。但现实世界很少使用如此小尺寸的图像。所以，自然的问题是，尽管PiT可能有效并超越ViT，它能否在标准分辨率的自然图像上工作，例如244*244？'
- en: The paper “Ultra-long sequence distributed transformer,” released in 2023, answers
    the question. The paper proposed a solution to scale the transformer computation
    of a 50k-long sequence onto 3k GPUS.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年发布的论文《超长序列分布式变换器》回答了这个问题。论文提出了一种解决方案，将50k长序列的变换器计算扩展到3k个GPU上。
- en: '![](../Images/70b5468468d4ea47bf4ca4644141f8d9.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/70b5468468d4ea47bf4ca4644141f8d9.png)'
- en: 'Original transformer, baseline parallelization, and proposed ultra-long sequence
    transformer. Image source: [https://arxiv.org/pdf/2311.02382](https://arxiv.org/pdf/2311.02382)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 原始变换器、基线并行化和所提的超长序列变换器。图片来源：[https://arxiv.org/pdf/2311.02382](https://arxiv.org/pdf/2311.02382)
- en: 'The idea is simple: The transformer''s bottleneck is self-attention computation.
    To ensure global attention computation, the proposed method **only distributes
    the query vectors** across different devices while maintaining the same copies
    of key and value vectors on all devices.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法很简单：变换器的瓶颈是自注意力计算。为了确保全局注意力计算，所提出的方法**只将查询向量**分配到不同设备上，同时在所有设备上保持相同的键值对副本。
- en: '![](../Images/472f0c435f3feb421a0b4ea20a4b965c.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/472f0c435f3feb421a0b4ea20a4b965c.png)'
- en: 'The equation for distributing self-attention calculation. The core idea is
    to distribute Q and keep the same copies of K and V. Image source: [https://arxiv.org/pdf/2311.02382](https://arxiv.org/pdf/2311.02382)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力计算分配的方程式。核心思想是分配 Q，并保持 K 和 V 的相同副本。图片来源：[https://arxiv.org/pdf/2311.02382](https://arxiv.org/pdf/2311.02382)
- en: '**Positional Encoding-aware double gradient averaging**. For architectures
    with learnable positional encoding parameters, gradient backpropagation is related
    to the positional encoding distribution. So, the authors proposed the double gradient
    averaging technique: when a gradient average is performed on two different segments
    from the same sequence, no positional encoding is involved, but when the corresponding
    segments from two sequences need to average gradients, the positional encoding
    parameters will be synced.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**位置编码感知的双重梯度平均**。对于具有可学习位置编码参数的架构，梯度反向传播与位置编码分布相关。因此，作者提出了双重梯度平均技术：当对同一序列的两个不同段执行梯度平均时，不涉及位置编码，但当需要对两个序列的对应段进行梯度平均时，位置编码参数会同步。'
- en: '![](../Images/4dae73c9a02ec7f02e8dbde88032e6ee.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4dae73c9a02ec7f02e8dbde88032e6ee.png)'
- en: 'Positional encoding-aware double gradient averaging. Image source: [https://arxiv.org/pdf/2311.02382](https://arxiv.org/pdf/2311.02382)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码感知的双重梯度平均。图片来源：[https://arxiv.org/pdf/2311.02382](https://arxiv.org/pdf/2311.02382)
- en: '![](../Images/6c374168141f6f3ebf2daa014fd39012.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c374168141f6f3ebf2daa014fd39012.png)'
- en: 'Maximal sequence length regarding the number of GPUs used. Image source: [https://arxiv.org/pdf/2311.02382](https://arxiv.org/pdf/2311.02382)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 关于所使用GPU数量的最大序列长度。图片来源：[https://arxiv.org/pdf/2311.02382](https://arxiv.org/pdf/2311.02382)
- en: When we combine these two papers, things become interesting. Reducing the inductive
    bias not only helps with model performance but also plays a crucial role in distributed
    computation.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将这两篇论文结合起来时，事情变得有趣了。减少归纳偏差不仅有助于模型性能，还在分布式计算中起着至关重要的作用。
- en: '**Inductive bias of locality and device communication**. The PiT paper shows
    the potential of building a positional encoding-free model by removing the locality
    bias. Furthermore, if we look at things from the distributed computing perspective,
    reducing the need for PE could further reduce the communication burden across
    devices.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**局部性和设备通信的归纳偏差**。PiT 论文展示了通过去除局部性偏差来构建无位置编码模型的潜力。此外，从分布式计算的角度来看，减少对 PE 的需求可以进一步减少跨设备的通信负担。'
- en: '**Inductive bias of locality in distributed sequence computation**. Distributing
    self-attention on multi-GPUs means the query vector is segment-based. There is
    a natural locality bias if the segment depends on contiguous tokens. The PiT computes
    on a “token set, " meaning there is no need for continuous segments, and it will
    make the query vector bias-free.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分布式序列计算中的局部性归纳偏差**。在多 GPU 上分布式自注意力意味着查询向量是基于段的。如果段依赖于连续的标记，则会有自然的局部性偏差。PiT
    在“标记集”上进行计算，这意味着不需要连续的段，它将使查询向量没有偏差。'
- en: '**References:**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献：**'
- en: 'Nguyen et al., An Image is Worth More Than 16x16 Patches: Exploring Transformers
    on Individual Pixels. arXiv preprint 2024.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen 等人，《一张图片胜过 16x16 块：在单个像素上探索 Transformers》。arXiv 预印本 2024。
- en: Wang et al., Ultra long sequence distributed transformer. arXiv preprint 2023.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人，《超长序列分布式 Transformer》。arXiv 预印本 2023。
- en: Keles et al., On the computational complexity of self-attention. ALT 2023.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keles 等人，《自注意力的计算复杂度》。ALT 2023。
- en: Jiang et al. The encoding method of position embeddings in vision transformer.
    Journal of Visual Communication and Image Representation. 2022.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人，《视觉 Transformer 中位置嵌入的编码方法》。《视觉通信与图像表示杂志》。2022。
- en: 'Xie et al., SegFormer: Simple and efficient design for semantic segmentation
    with transformers. NeurIPS 2021\. Github: [https://github.com/NVlabs/SegFormer](https://github.com/NVlabs/SegFormer)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xie 等人，《SegFormer：一种简洁高效的语义分割设计，基于 Transformers》。NeurIPS 2021。Github: [https://github.com/NVlabs/SegFormer](https://github.com/NVlabs/SegFormer)'
- en: 'Liang et al., [Swinir: Image restoration using swin transformer](https://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.html).
    ICCV 2021\. Github: [https://github.com/JingyunLiang/SwinIR](https://github.com/JingyunLiang/SwinIR)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liang 等人，[Swinir：使用 Swin Transformer 进行图像恢复](https://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.html)。ICCV
    2021。Github: [https://github.com/JingyunLiang/SwinIR](https://github.com/JingyunLiang/SwinIR)'
- en: 'Liu et al., Swin Transformer: Hierarchical vision transformer using shifted
    windows. ICCV 2021\. Github: [https://github.com/microsoft/Swin-Transformer](https://github.com/microsoft/Swin-Transformer)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人，《Swin Transformer：使用平移窗口的分层视觉 Transformer》。ICCV 2021。Github: [https://github.com/microsoft/Swin-Transformer](https://github.com/microsoft/Swin-Transformer)'
- en: 'Dosovitsckiy et al., An image is worth 16x16 words: Transformers for image
    recognition at scale. arXiv preprint, 2020.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dosovitsckiy 等人，《一张图像胜过 16x16 个词：用于大规模图像识别的 Transformers》。arXiv 预印本，2020。
- en: 'Brown et al., Language models are few-shot learners. NeurIPS 2020\. Github:
    [https://github.com/openai/gpt-3](https://github.com/openai/gpt-3)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Brown 等人，《语言模型是少量样本学习者》。NeurIPS 2020。Github: [https://github.com/openai/gpt-3](https://github.com/openai/gpt-3)'
- en: 'Devlin et al., BERT: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint 2018*. HuggingFace Official: [https://huggingface.co/docs/transformers/en/model_doc/bert](https://huggingface.co/docs/transformers/en/model_doc/bert)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin 等人，《BERT：用于语言理解的深度双向 Transformer 预训练》。*arXiv 预印本 2018*。HuggingFace 官方:
    [https://huggingface.co/docs/transformers/en/model_doc/bert](https://huggingface.co/docs/transformers/en/model_doc/bert)'
- en: Vaswani et al., Attention is all You need. NeurIPS 2017.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等人，《Attention is all You need》。NeurIPS 2017。
- en: 'Murphy, Machine learning: a probabilistic perspective. MIT press 2012.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Murphy，《机器学习：一种概率视角》。MIT 出版社 2012。
