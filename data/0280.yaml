- en: 4 Examples to Take Your PySpark Skills to Next Level
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 个示例助你提升 PySpark 技能
- en: 原文：[https://towardsdatascience.com/4-examples-to-take-your-pyspark-skills-to-next-level-2a04cbe6e630?source=collection_archive---------6-----------------------#2024-01-30](https://towardsdatascience.com/4-examples-to-take-your-pyspark-skills-to-next-level-2a04cbe6e630?source=collection_archive---------6-----------------------#2024-01-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/4-examples-to-take-your-pyspark-skills-to-next-level-2a04cbe6e630?source=collection_archive---------6-----------------------#2024-01-30](https://towardsdatascience.com/4-examples-to-take-your-pyspark-skills-to-next-level-2a04cbe6e630?source=collection_archive---------6-----------------------#2024-01-30)
- en: Get used to large-scale data processing with PySpark
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 PySpark 适应大规模数据处理
- en: '[](https://sonery.medium.com/?source=post_page---byline--2a04cbe6e630--------------------------------)[![Soner
    Yıldırım](../Images/c589572e9d1ee176cd4f5a0008173f1b.png)](https://sonery.medium.com/?source=post_page---byline--2a04cbe6e630--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2a04cbe6e630--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2a04cbe6e630--------------------------------)
    [Soner Yıldırım](https://sonery.medium.com/?source=post_page---byline--2a04cbe6e630--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://sonery.medium.com/?source=post_page---byline--2a04cbe6e630--------------------------------)[![Soner
    Yıldırım](../Images/c589572e9d1ee176cd4f5a0008173f1b.png)](https://sonery.medium.com/?source=post_page---byline--2a04cbe6e630--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2a04cbe6e630--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2a04cbe6e630--------------------------------)
    [Soner Yıldırım](https://sonery.medium.com/?source=post_page---byline--2a04cbe6e630--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2a04cbe6e630--------------------------------)
    ·7 min read·Jan 30, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2a04cbe6e630--------------------------------)
    ·7分钟阅读·2024年1月30日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a222122f8db4bec4e2342525412c2088.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a222122f8db4bec4e2342525412c2088.png)'
- en: Photo by [fabio](https://unsplash.com/@fabioha?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/geometric-shape-digital-wallpaper-oyXis2kALVg?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [fabio](https://unsplash.com/@fabioha?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    提供，来自 [Unsplash](https://unsplash.com/photos/geometric-shape-digital-wallpaper-oyXis2kALVg?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
- en: PySpark is the Python API for Spark, which is an analytics engine used for large-scale
    data processing. Spark has become the predominant tool in the data science ecosystem
    especially when we deal with large datasets that are difficult to handle with
    tools like Pandas and SQL.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 是 Spark 的 Python API，Spark 是一个用于大规模数据处理的分析引擎。特别是当我们处理那些使用 Pandas 和 SQL
    等工具难以应对的大型数据集时，Spark 已成为数据科学生态系统中占主导地位的工具。
- en: In this article, we’ll learn PySpark but from a different perspective than most
    of the other tutorials. Instead of going over frequently used PySpark functions
    and explaining how to use them, we’ll solve some challenging data cleaning and
    processing tasks. This way of learning not only helps us learn PySpark functions
    but also know when to use them.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将学习 PySpark，但与大多数教程的视角不同。我们不会简单地讲解常用的 PySpark 函数并说明如何使用它们，而是通过解决一些具有挑战的数据清洗和处理任务来学习。这样的学习方式不仅帮助我们学习
    PySpark 函数，还能帮助我们了解何时使用它们。
- en: Before we start with the examples, let me tell you how to get the dataset used
    in the examples. It’s a sample dataset I prepared with mock data. You can download
    from my [datasets](https://github.com/SonerYldrm/datasets/tree/main) repository
    — it’s called “sample_sales_pyspark.csv”.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始示例之前，先让我告诉你如何获取示例中使用的数据集。它是我用虚拟数据准备的一个示例数据集。你可以从我的 [datasets](https://github.com/SonerYldrm/datasets/tree/main)
    仓库下载，文件名是“sample_sales_pyspark.csv”。
- en: Let’s start with creating a DataFrame from this dataset.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从这个数据集创建一个 DataFrame 开始。
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
