- en: 'Distributed Decentralized Training of Neural Networks: A Primer'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的分布式去中心化训练：入门指南
- en: 原文：[https://towardsdatascience.com/distributed-decentralized-training-of-neural-networks-a-primer-21e5e961fce1?source=collection_archive---------5-----------------------#2024-11-19](https://towardsdatascience.com/distributed-decentralized-training-of-neural-networks-a-primer-21e5e961fce1?source=collection_archive---------5-----------------------#2024-11-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/distributed-decentralized-training-of-neural-networks-a-primer-21e5e961fce1?source=collection_archive---------5-----------------------#2024-11-19](https://towardsdatascience.com/distributed-decentralized-training-of-neural-networks-a-primer-21e5e961fce1?source=collection_archive---------5-----------------------#2024-11-19)
- en: '[](https://medium.com/@RobertTLange?source=post_page---byline--21e5e961fce1--------------------------------)[![Robert
    Lange](../Images/bc86dcd37704fcf8a5566c0ddb61b87a.png)](https://medium.com/@RobertTLange?source=post_page---byline--21e5e961fce1--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--21e5e961fce1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--21e5e961fce1--------------------------------)
    [Robert Lange](https://medium.com/@RobertTLange?source=post_page---byline--21e5e961fce1--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@RobertTLange?source=post_page---byline--21e5e961fce1--------------------------------)[![Robert
    Lange](../Images/bc86dcd37704fcf8a5566c0ddb61b87a.png)](https://medium.com/@RobertTLange?source=post_page---byline--21e5e961fce1--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--21e5e961fce1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--21e5e961fce1--------------------------------)
    [Robert Lange](https://medium.com/@RobertTLange?source=post_page---byline--21e5e961fce1--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--21e5e961fce1--------------------------------)
    ·9 min read·Nov 19, 2024
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--21e5e961fce1--------------------------------)
    ·9分钟阅读·2024年11月19日
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: As artificial intelligence advances, training large-scale neural networks, including
    large language models, has become increasingly critical. The growing size and
    complexity of these models not only elevate the costs and energy requirements
    associated with training but also highlight the necessity for effective hardware
    utilization. In response to these challenges, researchers and engineers are exploring
    distributed decentralized training strategies. In this blog post, we will examine
    various methods of distributed training, such as data-parallel training and gossip-based
    averaging, to illustrate how these approaches can optimize model training efficiency
    while addressing the rising demands of the field.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能的发展，训练大规模神经网络，包括大语言模型，变得愈加重要。这些模型的规模和复杂度不断增长，不仅提高了训练所需的成本和能源要求，也凸显了有效硬件利用的必要性。为应对这些挑战，研究人员和工程师正在探索分布式去中心化训练策略。在这篇博客文章中，我们将探讨各种分布式训练方法，例如数据并行训练和基于gossip的平均算法，以说明这些方法如何在应对日益增加的领域需求的同时，优化模型训练效率。
- en: '![](../Images/1e5ecae4ca128f9a958eaf3998f25516.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e5ecae4ca128f9a958eaf3998f25516.png)'
- en: A minimalist light Japanese-style depiction of a GPU cluster with more smaller
    GPUs added. (Generated by OpenAI's Dallé-3 API)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 一种极简的日式风格GPU集群图，图中添加了更多的小型GPU。（由OpenAI的Dallé-3 API生成）
- en: Data-Parallelism, the All-Reduce Operation and Synchronicity
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据并行性、All-Reduce操作和同步性
- en: Data-parallel training is a technique that involves dividing mini-batches of
    data across multiple devices (workers). This method not only enables several workers
    to compute gradients simultaneously, thereby improving training speed, but also
    allows…
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行训练是一种技术，涉及将数据的小批量分配到多个设备（工作节点）上。这种方法不仅使多个工作节点能够同时计算梯度，从而提高训练速度，还可以…
