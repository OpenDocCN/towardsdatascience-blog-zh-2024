- en: Counterfactuals in Language AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言AI中的反事实
- en: 原文：[https://towardsdatascience.com/counterfactuals-in-language-ai-956673049b64?source=collection_archive---------5-----------------------#2024-07-23](https://towardsdatascience.com/counterfactuals-in-language-ai-956673049b64?source=collection_archive---------5-----------------------#2024-07-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/counterfactuals-in-language-ai-956673049b64?source=collection_archive---------5-----------------------#2024-07-23](https://towardsdatascience.com/counterfactuals-in-language-ai-956673049b64?source=collection_archive---------5-----------------------#2024-07-23)
- en: with open source language models and LLMs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用开源语言模型和LLM
- en: '[](https://joshuabanksmailman.medium.com/?source=post_page---byline--956673049b64--------------------------------)[![Joshua
    Banks Mailman, Ph.D.](../Images/e28bf743fc5bdba644dcfb747eeb72b0.png)](https://joshuabanksmailman.medium.com/?source=post_page---byline--956673049b64--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--956673049b64--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--956673049b64--------------------------------)
    [Joshua Banks Mailman, Ph.D.](https://joshuabanksmailman.medium.com/?source=post_page---byline--956673049b64--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://joshuabanksmailman.medium.com/?source=post_page---byline--956673049b64--------------------------------)[![Joshua
    Banks Mailman, Ph.D.](../Images/e28bf743fc5bdba644dcfb747eeb72b0.png)](https://joshuabanksmailman.medium.com/?source=post_page---byline--956673049b64--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--956673049b64--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--956673049b64--------------------------------)
    [Joshua Banks Mailman, Ph.D.](https://joshuabanksmailman.medium.com/?source=post_page---byline--956673049b64--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--956673049b64--------------------------------)
    ·27 min read·Jul 23, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--956673049b64--------------------------------)
    ·27分钟阅读·2024年7月23日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/3a8683cdc2f0845498a740e41bdcb8af.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a8683cdc2f0845498a740e41bdcb8af.png)'
- en: Generated by the author using DALL-E 3
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者使用DALL-E 3生成
- en: The splash that ChatGPT was making last year brought with it the realization
    — surprise for many — that a putative AI could sometimes offer very wrong answers
    with utter conviction. The term for this is usually “hallucination” and the main
    remedy that’s developed over the last 18 months is to bring facts into the matter,
    usually through *retrieval augmented generation* (RAG), also sometimes called
    *relevant answer generation*, which basically reorients the GPT (*generative pretrained
    transformer* language model) to draw from contexts where known-to-be-relevant
    facts are found.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 去年ChatGPT的“轰动”使人们意识到——对许多人来说是一个惊讶——假定的人工智能有时可能会毫不犹豫地给出非常错误的答案。这个现象通常被称为“幻觉”，而过去18个月中发展出的主要应对方法是通过引入事实，通常通过*检索增强生成*（RAG），有时也称为*相关答案生成*，它基本上是将GPT（*生成预训练变换器*语言模型）重新定向，去从已知包含相关事实的上下文中提取信息。
- en: Yet hallucinations are not the only way a GPT can misstep. In some respects,
    other flavors of misstep are deeper and more interesting to consider— especially
    when prospects of Artificial General Intelligence (AGI) are now often discussed.
    Specifically I’m thinking of what are known as *counterfactuals* (counterfactual
    reasoning) and the crucial role counterfactuality can play in decision making,
    particularly in regard to *causal inference*. Factuality therefore isn’t the only
    touchstone for effective LLM operation.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，幻觉并不是GPT可能犯错的唯一方式。从某些方面来看，其他类型的错误更深刻，也更值得我们考虑——特别是当人工通用智能（AGI）的前景经常被讨论时。我具体指的是被称为*反事实*（反事实推理）的现象，以及反事实性在决策中的关键作用，尤其是在*因果推理*方面。因此，事实性并不是有效LLM操作的唯一标准。
- en: In this article I’ll reflect on how counterfactuals might help us think differently
    about the pitfalls and potentials of Generative AI. And I’ll demonstrate with
    some concrete examples using open source LMs (specifically Microsoft’s Phi). I’ll
    show how to set up Ollama locally (it can also be done in Databricks), without
    too much fuss (both with and without a Docker container), so you can try it out
    for yourself. I’ll also compare OpenAI’s LLM response to the same prompts.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将反思反事实如何帮助我们以不同的方式思考生成型AI的陷阱与潜力。我将通过一些具体示例，使用开源语言模型（特别是微软的Phi）来展示。我将展示如何在本地设置Ollama（也可以在Databricks中进行），无需太多复杂操作（包括有无Docker容器的设置），让你可以亲自尝试。我还将比较OpenAI的LLM对同样提示的反应。
- en: I suggest that if we want to even begin to think about the prospect of “intelligence”
    within, or exuded by, an artificial technology, we might need to think beyond
    the established ML paradigm, which assumes some pre-existing factual correctness
    to measure against. An intelligent behavior might instead be speculative, as yet
    lacking sufficient past evidence to obviously prove its value. Isaac Newton, Charles
    Darwin, or your pet cat, could reason practically about the physical world they
    themselves inhabit, which is something that LLMs — because they are *dis*embodied
    — don’t do. In a world where machines can write fluently, talk is cheaper than
    speculative practical reasoning.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议，如果我们想要开始思考“智能”是否存在于人工技术中，或由其散发出来，我们可能需要超越既定的机器学习范式，这种范式假设某种先前存在的事实正确性作为衡量标准。智能行为可能本质上是推测性的，尚缺乏足够的过去证据来明显证明其价值。艾萨克·牛顿、查尔斯·达尔文，或你的宠物猫，都能够就它们所处的物理世界进行实际推理，而这是大型语言模型（LLMs）——因为它们是*非*具身的——无法做到的。在一个机器能够流利写作的世界里，谈论比推测性的实际推理更为廉价。
- en: '![](../Images/42bb142715d76d80ab499d9bf7b2c778.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/42bb142715d76d80ab499d9bf7b2c778.png)'
- en: Photo by [Florian Klauer](https://unsplash.com/@florianklauer?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 由[Florian Klauer](https://unsplash.com/@florianklauer?utm_source=medium&utm_medium=referral)拍摄，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '**Counterfactuals**'
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**反事实**'
- en: 'What is a counterfactual and why should we care? There’s certainly idle speculation,
    sometimes with a rhetorical twist: At an annual meeting, a shareholder asked what
    the'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是反事实，为什么我们要关心它？当然有一些无聊的猜测，有时还带有修辞性的转折：在一次年度会议上，一位股东问到，
- en: “…returns since 1888 would have been without the ‘arrogance and greed’ of directors
    and their bonuses.” *¹*
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “…自1888年以来的回报本来会没有董事们的‘傲慢和贪婪’以及他们的奖金。” *¹*
- en: '…to which retired banker Charles Munn² replied:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: …对此，退休银行家查尔斯·芒恩²回答道：
- en: “That’s one question I won’t be answering. I’m a historian, I don’t deal in
    counterfactuals.” *¹*
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “那是我不会回答的问题。我是个历史学家，我不处理反事实问题。” *¹*
- en: 'It’s *one* way to evade a question. Politicians have used it, no doubt. Despite
    emphasis on precedent, in legal matters, counterfactuality can be a legitimate
    consideration. As Robert N. Strassfeld puts it:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这是*一种*回避问题的方法。毫无疑问，政治家们也曾使用过。尽管在法律事务中强调先例，反事实性仍然可以是一个合法的考虑因素。正如罗伯特·N·斯特拉斯费尔德所说：
- en: “As in the rest of life, we indulge, indeed, require, many speculations on what
    might have been. Although such counterfactual thinking often remains disguised
    or implicit, we encounter it whenever we identify a cause, and quite often when
    we attempt to fashion a remedy… Yet, … troublesome might-have-beens abound in
    legal disputes. We find ourselves stumbling over them in a variety of situations
    and responding to them in inconsistent ways ranging from brazen self-confidence
    to paralysis in the face of the task. When we recognize the exercise for what
    it is, however, our self confidence tends to erode, and we become discomforted,
    perplexed, and skeptical about the whole endeavor.”³
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “就像生活中的其他事情一样，我们沉迷于，实际上，要求进行许多关于‘可能本该是怎样’的猜测。虽然这种反事实思维往往保持隐蔽或隐含，但每当我们确定一个原因时，我们就会遇到它，实际上我们常常在尝试制定补救措施时也会遇到它……然而，……在法律争议中，麻烦的‘可能曾经是’无处不在。我们在各种情况下都会被它绊倒，并以不一致的方式回应它，从肆无忌惮的自信到面对任务时的瘫痪。然而，当我们意识到这一行为本质时，我们的自信往往会消退，我们变得不安、困惑，并对整个过程产生怀疑。”³
- en: He goes further in posing that
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 他进一步提出：
- en: “…legal decision makers cannot avoid counterfactual questions. Because such
    questions are necessary, we should think carefully about when and how to pose
    them, and how to distinguish good answers from poor ones.”
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “…法律决策者无法避免反事实问题。因为这些问题是必要的，我们应该认真思考何时以及如何提出这些问题，以及如何区分好的回答与差的回答。”
- en: Counterfactuals are not an “anything goes” affair — far from it.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 反事实并不是一种“随便什么都行”的事务——远非如此。
- en: The question pervades the discourse of responsible AI and explainable AI (which
    often become entwined). Consider the “right to explanation” in the EU General
    Data Protection Regulation (“GDPR”).⁴ Thanks in part to [Julia Stoyanovich’](https://nyudatascience.medium.com/until-you-try-to-regulate-you-wont-learn-how-julia-stoyanovich-discusses-responsible-ai-for-4cf119002b05)s
    efforts, [NY passed a law in 2021 requiring that job seekers rejected by an AI-infused
    hiring process have the right to learn the specific explanation for their rejection](https://rules.cityofnewyork.us/wp-content/uploads/2022/09/Stoyanovich_LL144_October24_2022.pdf).⁵
    ⁶ ⁷
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题贯穿于负责任的人工智能和可解释人工智能的讨论中（这两者往往会交织在一起）。请考虑欧盟《通用数据保护条例》（“GDPR”）中的“解释权”。⁴ 部分得益于[Julia
    Stoyanovich](https://nyudatascience.medium.com/until-you-try-to-regulate-you-wont-learn-how-julia-stoyanovich-discusses-responsible-ai-for-4cf119002b05)的努力，[纽约市在2021年通过了一项法律，要求被人工智能辅助招聘流程拒绝的求职者有权了解被拒绝的具体原因](https://rules.cityofnewyork.us/wp-content/uploads/2022/09/Stoyanovich_LL144_October24_2022.pdf)。⁵
    ⁶ ⁷
- en: 'If you’re a Data Scientist, the prospect of “explanations” (with respect to
    a model) probably brings to mind SHAP (or LIME). Basically, a SHapley Additive
    exPlanation (SHAP) is derived by taking each predictive feature (each column of
    data) one at a time, and scrambling the observations (rows) of that feature (column)
    to assess which features (columns) the scrambling of which changes the prediction
    the most. For the rejected job candidate it might say, for instance: The primary
    reason the algorithm rejected you is “years of experience” because when we randomly
    substitute (permute) other candidates’ “years of experience” it affects the algorithm’s
    rating of you more than when we do that substitution (permutation) with your other
    features (like gender, education, etc). It’s making a quantitative comparison
    to a “what if.” So what is a SHAPley other than a counterfactual? Counterfactuality
    is at the heart of the explanation, because it gives a glimpse into causality;
    and explanation is relied on for making AI responsible.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是数据科学家，提到“解释”（就模型而言）可能会让你想到SHAP（或LIME）。基本上，SHapley加性解释（SHAP）是通过逐个处理每个预测特征（每一列数据），并打乱该特征（列）的观察值（行），来评估打乱哪个特征（列）会对预测结果产生最大影响。例如，对于被拒绝的求职者，它可能会说：算法拒绝你的主要原因是“工作经验”，因为当我们随机替换（排列）其他候选人的“工作经验”时，它对算法对你的评分的影响大于当我们用你其他特征（如性别、教育等）做这种替换（排列）时的影响。它是在进行定量比较，类似于“如果…会怎样”。那么，SHAPley除了反事实之外还有什么呢？反事实性是解释的核心，因为它揭示了因果关系；而解释正是使人工智能变得负责任的关键。
- en: 'Leaving responsibility and ethics to the side for the moment, causal explanation
    still has other uses in business. At least in some companies, Data Science and
    AI are expected to guide decision making, which in causal inference terms means
    *making an intervention*: adjusting price or targeting this verses that customer
    segment, and so forth. An intervention is an alteration to the status quo. The
    *fundamental problem of causal inference* is that we aren’t able to observe what
    has never happened. So we can’t observe the result of an intervention until we
    make that intervention. Where there’s risk involved, we don’t want to intervene
    without sufficiently anticipating the result. Thus we want to infer in advance
    that the result we desire can be caused by our intervention. That entails making
    inferences about the causal effects of events that aren’t yet fact. Instead, such
    events are *counterfactual*, that is, contrary to fact. This is why counterfactuals
    have been couched, by Judea Pearl and others, as the'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 暂时搁置责任和伦理问题，因果解释在商业中仍然有其他用途。至少在某些公司中，数据科学和人工智能被期望指导决策，这在因果推断的术语中意味着*进行干预*：调整价格或选择这个客户群体而不是那个，等等。干预是对现状的改变。*因果推断的根本问题*在于，我们无法观察到从未发生过的事情。因此，我们无法在进行干预之前观察到其结果。涉及风险时，我们不希望在没有充分预测结果的情况下进行干预。因此，我们希望提前推断，我们期望的结果可以通过我们的干预来实现。这意味着要对尚未成为事实的事件的因果效应进行推断。相反，这些事件是*反事实*的，也就是说，与事实相反。这就是为什么反事实被Judea
    Pearl等人描述为
- en: '***fundamental problem of causal inference* ⁸**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '***因果推断的根本问题* ⁸**'
- en: 'So the idea of a “thought experiment”, which has been important especially
    in philosophy — and ever more so since Ludwig Wittgenstein popularized it —as
    a way to probe how we use language to construct our understanding of the world
    — isn’t just a sentimental wish-upon-a-star.⁹ Quite to the contrary: counterfactuals
    are the crux of hard-headed decision making.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，“思想实验”这一概念，特别是在哲学中尤为重要——自路德维希·维特根斯坦普及以来，这一概念越来越被重视——作为一种探讨我们如何使用语言构建对世界理解的方式，并不仅仅是一个感伤的星愿。相反，反事实思维才是理性决策的核心。
- en: 'In this respect, what Eric Siegel suggests in his recent *AI Playbook* follows
    as corollary: Siegel suggests that *change management* be repositioned from afterthought
    to prerequisite of any Machine Learning project.¹⁰ If the conception of making
    a business change isn’t built into the ML project from the get go, its deployment
    is likely doomed to remain fiction forever (eternally counterfactual). The antidote
    is to imagine the intervention in advance, and systematically work out its causal
    effects, so that you can almost taste them. If its potential benefits are anticipated
    — and maybe even consistently visualized — by all parties who stand to benefit,
    then the ML project stands a better chance of transitioning from fiction to fact.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面，Eric Siegel在他近期的《AI Playbook》中提出的建议可以作为推论：Siegel建议将*变革管理*从事后思考调整为任何机器学习项目的前提条件。¹⁰
    如果从一开始就没有在机器学习项目中融入商业变革的构思，那么它的部署很可能永远只是幻想（永远的反事实）。解药是提前想象干预措施，并系统地推算其因果效应，这样你几乎可以品味到它们。如果所有有可能受益的各方都能预见——甚至持续地想象——其潜在收益，那么机器学习项目更有可能从幻想转变为现实。
- en: As Aleksander Molak explains it in his recent *Causal Inference and Discovery
    in Python* (2023)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 正如Aleksander Molak在他近期的《Python中的因果推断与发现》（2023）一书中解释的那样
- en: “Counterfactuals can be thought of as *hypothetical* or *simulated* interventions
    that assume a *particular state of the world*.” *¹¹*
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “反事实可以被视为*假设*的或*模拟*的干预，假定一个*特定的世界状态*。” *¹¹*
- en: The capacity for *rational imagination* is implicated in many philosophical
    definitions of rational agency.¹² ¹³
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*理性想象力*的能力在许多哲学对理性行为体的定义中都起着重要作用。¹² ¹³'
- en: “[P]sychological research shows that rational *human* agents *do* learn from
    the past and plan for the future engaging in counterfactual thinking. Many researchers
    in artificial intelligence have voiced similar ideas (Ginsberg 1985; Pearl 1995;
    Costello & McCarthy 1999)” *¹³ ¹⁴ ¹⁵ ¹⁶*
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “[P]sychological research shows that rational *human* agents *do* learn from
    the past and plan for the future engaging in counterfactual thinking. Many researchers
    in artificial intelligence have voiced similar ideas (Ginsberg 1985; Pearl 1995;
    Costello & McCarthy 1999)” *¹³ ¹⁴ ¹⁵ ¹⁶*
- en: As Molak demonstrates “we can compute counterfactuals when we meet certain assumptions”
    (33) ¹¹. That means there are circumstances when we can judge reasoning on counterfactuals
    as either right or wrong, correct or incorrect. In this respect even what’s fiction
    (counter to fact) can, in a sense, be *true*.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 正如Molak所展示的，“当我们满足某些假设时，我们可以计算反事实”（33）¹¹。 这意味着在某些情况下，我们可以判断反事实推理是正确还是错误，正确还是不正确。在这方面，即使是虚构的东西（与事实相对）也可以在某种意义上是*真实的*。
- en: '![](../Images/609a0178afba0bfbbba353287702fec7.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/609a0178afba0bfbbba353287702fec7.png)'
- en: Photo by [Hans Eiskonen](https://unsplash.com/@eiskonen?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Hans Eiskonen](https://unsplash.com/@eiskonen?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '**AI Beyond “Stochastic Parrots”**'
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**超越“随机鹦鹉”的人工智能**'
- en: Verbal fluency seems to be the new bright shiny thing. But is it thought? If
    the prowess of IBM’s Deep Blue and DeepMind’s AlphaGo could be relegated to mere
    cold calculation, the flesh-and-blood aroma of ChatGPT’s verbal fluency since
    late 2022 seriously elevated—or at least reframed — the old question of whether
    an AI can really “think.” Or is the LLM inside ChatGPT merely a “stochastic parrot,”
    stitching together highly probable strings of words in infinitely new combinations?
    There are times though when it seems that putative human minds — some running
    for the highest office in the land — are doing no more than that. Will the real
    intelligence in the room please stand up?
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 语言流畅性似乎成了新的亮眼之物。但这真的是思维吗？如果IBM的深蓝和DeepMind的AlphaGo的卓越表现仅仅归结于冷冰冰的计算，那么自2022年底以来，ChatGPT语言流畅性所散发的人类气息则极大提升——或者至少重新框定——了那个老问题：人工智能真的能够“思考”吗？还是说，ChatGPT背后的大型语言模型仅仅是一个“随机鹦鹉”，将极有可能的词语串联在一起，形成无数种新组合？不过，有时似乎某些被认为是人类思维的人——其中一些人甚至竞选国家最高领导职务——所做的也不过如此。那么，真正的智能能否站出来？
- en: In her brief article about Davos 2024, “[Raising Baby AI in 2024](https://fionajmcevoy.medium.com/raising-baby-ai-in-2024-1b7d704cd3d5)”,
    Fiona McEvoy reported that Yann LeCun emphasized the need for AIs to learn from
    not just text but also video footage.¹⁷ Yet that’s still passive; it’s still an
    attempt to learn from video-documented “fact” (video footage that already exists);
    McEvoy reports that
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在她关于2024年达沃斯的简短文章《[2024年培养AI婴儿](https://fionajmcevoy.medium.com/raising-baby-ai-in-2024-1b7d704cd3d5)》中，Fiona
    McEvoy报道了Yann LeCun强调人工智能不仅要从文本中学习，还要从视频资料中学习的观点。¹⁷ 然而，这仍然是被动的；它仍然是试图从已经存在的视频“事实”（已有的视频资料）中学习；McEvoy报道道
- en: “[Daphne] Koller contends that to go beyond mere associations and get to something
    that feels like the causal reasoning humans use, systems will need to interact
    with the real world in an embodied way — for example, gathering input from technologies
    that are ‘out in the wild’, like augmented reality and autonomous vehicles. She
    added that such systems would also need to be given the space to experiment with
    the world to learn, grow, and go beyond what a human can teach them.” *¹⁷*
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “[达芙妮]·科勒认为，要超越单纯的关联，达到类似人类因果推理的水平，系统需要以具身的方式与真实世界互动——例如，从‘野外’技术中获取输入，如增强现实和自动驾驶车辆。她补充说，这些系统还需要有空间进行实验，与世界互动，以学习、成长并超越人类所能教给它们的。”
    *¹⁷*
- en: 'Another way to say it this: AIs will have to interact with the world in an
    *embodied* way at least somewhat in order to hone their ability to engage in counterfactual
    reasoning. We’ve all seen the videos — or watched up close — a cat pushing an
    object off a counter, with apparently no purpose, except to annoy us. Human babies
    and toddlers do it too. Despite appearances, however, this isn’t just acting out.
    Rather, in a somewhat naïve incarnation, these are acts of *hypothesis testing*.
    Such acts are prompted by a curiosity: What would happen if I shoved this vase?'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种表达方式是：人工智能至少需要以某种方式通过*具身*的方式与世界互动，才能磨练它们进行反事实推理的能力。我们都看过这些视频——或者近距离观察过——一只猫把物体从台面上推下来，似乎没有任何目的，除了让我们烦恼。人类的婴儿和幼儿也会这样做。然而，尽管看起来像是在表演，这其实并非单纯的行为表现。相反，在某种程度上，这些行为是*假设检验*。这些行为源于一种好奇心：如果我推倒这个花瓶，会发生什么？
- en: '![](../Images/e7ae42796b346703ff78cbc1ddb7476d.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7ae42796b346703ff78cbc1ddb7476d.png)'
- en: Generated with DALL-E 3 and edited by the author
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 由 DALL-E 3 生成并由作者编辑
- en: 'Please watch this [3-second animated gif](https://www.northtorontocatrescue.com/wp-content/uploads/2020/05/source.gif)
    displayed by [North Toronto Cat Rescue](https://www.northtorontocatrescue.com/why-do-kitties-knock-things-over/).¹⁸
    In this brief cat video, there’s an additional detail which sheds more light:
    The cat is about to jump; but before jumping she realizes there’s an object immediately
    at hand which can be used to test the distance or surface in advance of jumping.
    Her jump was counterfactual (she hadn’t jumped yet). The fact that she had already
    almost jumped indicates that she hypothesized the jump was feasible; the cat had
    quickly simulated the jump in her mind; suddenly realizing that the bottle on
    the counter afforded the opportunity to make an intervention, to test out her
    hypothesis; this act was habitual.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 请观看这个由[北多伦多猫咪救援](https://www.northtorontocatrescue.com/why-do-kitties-knock-things-over/)展示的[3秒动画
    gif](https://www.northtorontocatrescue.com/wp-content/uploads/2020/05/source.gif)。¹⁸
    在这个简短的猫咪视频中，有一个额外的细节提供了更多的信息：猫咪即将跳跃；但在跳跃之前，它意识到有一个可以立即使用的物体，可以用来在跳跃之前测试距离或表面。她的跳跃是反事实的（她还没有跳跃）。她几乎已经准备好跳跃，说明她已经假设跳跃是可行的；猫咪迅速在脑海中模拟了跳跃；突然意识到台面上的瓶子提供了一个机会，可以进行干预，测试她的假设；这个行为是习惯性的。
- en: I have no doubt that her ability to assess the feasibility of such jumps arose
    from having physically acted out similar situations many times before. Would an
    AI, who doesn’t have physical skin in the game, have done the same? And obviously
    humans do this on a scale far beyond what cats do. It’s how scientific discovery
    and technological invention happen; but on a more mundane level this part of intelligence
    is how living organisms routinely operate, whether it’s a cat jumping to the floor
    or a human making a business decision.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我毫不怀疑她评估这种跳跃可行性的能力，源于她之前曾多次实际演练过类似的情境。一个没有实际“身体参与”的人工智能，会做出相同的判断吗？显然，人类在这方面的表现远远超过猫咪。这正是科学发现和技术发明发生的方式；但在更平凡的层面，这种智慧是所有生物常规运作的一部分，无论是猫咪跳到地板上，还是人类做出商业决策。
- en: Abduction
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理
- en: Testing out counterfactuals by making interventions seems to hone our ability
    to do what Charles Sanders Peirce dubbed *abductive reasoning.*¹⁹ ²⁰ As distinct
    from *induction* (inferring a pattern from repeated cases) and *deduction* (deriving
    logical implications), abduction is the assertion of a hypothesis. Although Data
    Scientists often explore hypothetical scenarios in terms of feature engineering
    and hyperparameter tuning, abductive reasoning isn’t really directly a part of
    applications of Machine Learning, because Machine Learning is usually optimizing
    on a pre-established space of possibilities based on fact, whereas as abductive
    reasoning is expanding the space of possibilities, beyond what is already fact.
    So perhaps Artificial General Intelligence (AGI) has a lot to catch up on.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 通过进行干预来测试反事实似乎能提升我们进行查尔斯·桑德斯·皮尔士所称的*溯因推理*的能力。¹⁹ ²⁰ 与*归纳推理*（从重复的案例中推断出模式）和*演绎推理*（推导出逻辑含义）不同，溯因推理是提出一个假设。尽管数据科学家常常通过特征工程和超参数调优来探索假设场景，但溯因推理并不是机器学习应用的直接组成部分，因为机器学习通常是在基于事实的预设可能性空间上进行优化，而溯因推理则是在扩展可能性空间，超越已有的事实。因此，也许人工通用智能（AGI）还有很多需要追赶的地方。
- en: 'Here’s a hypothesis:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个假设：
- en: Entities (biological or artificial) that lack the ability (or opportunity) to
    make interventions don’t cultivate much counterfactual reasoning capability.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺乏进行干预的能力（或机会）的实体（无论是生物体还是人工体）不会培养出多少反事实推理能力。
- en: Counterfactual reasoning, or abduction, is mainly worthwhile to the extent that
    one can subsequently try out the hypotheses through interventions. That’s why
    it’s relevant to an animal (or human). Absent eventual opportunities to intervene,
    causal reasoning (abduction, hypothesizing) is futile, and therefore not worth
    cultivating.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 反事实推理或溯因推理的价值主要在于，通过干预可以尝试验证这些假设。这就是它与动物（或人类）相关的原因。如果没有最终干预的机会，因果推理（溯因推理，假设）就是徒劳的，因此也不值得培养。
- en: The capacity for abductive reasoning would not have evolved in humans (or cats),
    if it didn’t provide some advantage. Such advantage can only pertain to making
    interventions since abduction (counterfactual reasoning) by definition does not
    articulate facts about the current state of the world. These observations are
    what prompt the hypothesis above about biological and artificial entities.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果溯因推理（反事实推理）不能提供某种优势，那么它就不会在人类（或猫）身上进化出来。这样的优势只能与进行干预相关，因为溯因推理（反事实推理）按定义并不陈述当前世界状态的事实。这些观察促使了上述关于生物和人工实体的假设。
- en: '![](../Images/a4e2fbe0c2d8958844c18b5f37c1ee36.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4e2fbe0c2d8958844c18b5f37c1ee36.png)'
- en: Photo by [Alice Alinari](https://unsplash.com/@alicealinari?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Alice Alinari](https://unsplash.com/@alicealinari?utm_source=medium&utm_medium=referral)拍摄，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '**Large Language Models**'
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**大型语言模型**'
- en: 'As I mentioned above, RAG (retrieval augmented generation, also known as relevant
    answer generation) has become the de facto approach for guiding LLM-driven GenAI
    systems (chatbots) toward appropriate or even optimal responses. The premise of
    RAG is that if snippets of relevant truthful text are directly supplied to the
    generative LLM along with your question, then it’s less likely to hallucinate,
    and in that sense provides better responses. “Hallucinating” is AI industry jargon
    for: *fabricating erroneous responses*.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如我上面提到的，RAG（检索增强生成，也称为相关答案生成）已成为指导基于LLM的生成型人工智能系统（如聊天机器人）朝向适当甚至最优响应的事实标准。RAG的前提是，如果将相关的真实文本片段与您的问题一起直接提供给生成型LLM，那么它不太可能产生幻觉，从而提供更好的响应。“幻觉”是人工智能行业的术语，指的是*编造错误的回应*。
- en: 'As is well known, hallucinations arise because LLMs, though trained thoroughly
    and carefully on massive amounts of human-written text from the internet, are
    still not omniscient, but tend to issue responses in a rather uniformly confident
    tone. Surprising? Actually it shouldn’t be. It makes sense, as the famous critique
    goes: LLMs are essentially parroting the text they’ve been trained on. Because
    LLMs are trained not on people’s sometimes tentative or evolving inner thoughts,
    but rather on the verbalizations of those thoughts that reached an assuredness
    threshold sufficient for a person to post for all to read on the biggest ever
    public forum that is the internet. So perhaps it’s understandable that LLMs skew
    toward overconfidence — they are what they eat.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 正如大家所知，幻觉的产生是因为LLM尽管在大量人类编写的互联网文本上经过了充分的训练，但仍然不是无所不知的，反而倾向于以一种相对一致的自信语气发出回应。令人惊讶吗？实际上，这不应该令人惊讶。它是有道理的，正如著名的批评所说：LLM本质上是在复述它们所训练过的文本。因为LLM的训练并不是基于人们有时不确定或不断变化的内心想法，而是基于那些已经达到了足够确定性阈值的想法，这些想法已经足够清晰，能够让一个人将其发布到互联网上最大的公共论坛，供所有人阅读。所以，也许可以理解，LLM倾向于表现出过度自信——它们就是它们“吃”到的东西。
- en: In fact, I think it’s fair to say that, unlike many honest humans, LLMs don’t
    verbally signal their assuredness level at all; they don’t modulate their tone
    to reflect their level of assuredness. Therefore the strategy for avoiding or
    reducing hallucinations is to set up the LLM for success by pushing the facts
    it needs right under its nose, so that it can’t ignore them. This is feasible
    for situations where chatbots are usually deployed, which typically have a limited
    scope. Documents generally relevant to the scope are assembled in advance (in
    a vector store/database) so that particularly relevant snippets of text can be
    searched for on demand and supplied to the LLM along with the question being asked,
    so that the LLM is nudged to somehow exploit the snippets upon generating its
    response.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，我认为可以公平地说，与许多诚实的人不同，LLM根本不会口头表达它们的确定性水平；它们不会调节语气来反映它们的确定性。因此，避免或减少幻觉的策略是通过将LLM所需的事实直接呈现在它面前，确保它无法忽视这些事实，从而为LLM的成功设置条件。这对于聊天机器人通常部署的场景是可行的，因为这些场景通常具有有限的范围。通常与该范围相关的文档会预先收集（在向量存储/数据库中），以便在需要时可以搜索到特别相关的文本片段，并与正在提出的问题一起提供给LLM，从而促使LLM在生成回应时能够利用这些片段。
- en: From RAGs to richer
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从RAG到更丰富的
- en: Still there are various ways things can go awry. An entire ecosystem of configurable
    toolkits for addressing these has arisen. NVIDIA’s open source NeMo-guardrails
    can filter out unsafe and inappropriate responses as well as help check for factuality.
    [John Snow Labs’ LangTest](https://www.johnsnowlabs.com/langtest/) boasts “60+
    Test Types for Comparing LLM & NLP Models on Accuracy, Bias, Fairness, Robustness
    & More.” Two toolkits that focus most intensely on the veracity of responses are
    [Ragas](https://docs.ragas.io/) and [TruEra’s TrueLens](https://www.trulens.org/).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然存在许多可能导致问题的方式。为了解决这些问题，已经出现了一个完整的可配置工具包生态系统。NVIDIA的开源NeMo-guardrails可以过滤掉不安全和不恰当的回应，并帮助检查事实性。[John
    Snow Labs的LangTest](https://www.johnsnowlabs.com/langtest/) 宣称拥有“60+种测试类型，用于比较LLM和NLP模型的准确性、偏见、公平性、稳健性等。”两个最专注于回应真实性的工具包是
    [Ragas](https://docs.ragas.io/) 和 [TruEra的TrueLens](https://www.trulens.org/)。
- en: 'At the heart of TrueLens (and similarly Ragas) sits an elegant premise: There
    are three interconnected units of text involved in each call to a RAG pipeline:
    the *query*, the *retrieved context,* and the *response*; and the pipeline fails
    to the extent that there’s a semantic gap between any of these. TruEra calls this
    the “RAG triad.” In other words, for a RAG pipeline to work properly, three things
    have to happen successfully: (1) the context retrieved must be sufficiently relevant;
    (2) the generated response must be sufficiently grounded in the retrieved context;
    and (3) the generated response must also be sufficiently relevant to the original
    query. A weak link anywhere in this loop equates to weakness in that call to the
    RAG pipeline. For instance:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: TrueLens（以及类似的Ragas）的核心理念是：每次调用RAG管道时，涉及三种相互关联的文本单元：*查询*、*检索的上下文* 和 *回应*；而管道的失败程度取决于这些单元之间是否存在语义差距。TruEra将其称为“RAG三元组”。换句话说，为了使RAG管道正常工作，必须成功地完成三件事：（1）检索的上下文必须足够相关；（2）生成的回应必须充分基于检索到的上下文；（3）生成的回应还必须与原始查询充分相关。该循环中的任何弱点都会导致RAG管道调用的弱点。例如：
- en: '**Query:** “Which country landed on the moon first?”'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题：** “哪个国家首先登月？”'
- en: '**Retrieved context**: “Neil Armstrong stepped foot on the moon in July 1969\.
    Buzz Aldrin was the pilot.”'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**检索到的上下文**：“尼尔·阿姆斯特朗于 1969 年 7 月登上月球。巴兹·奥尔德林是飞行员。”'
- en: '**Generated response:** “Neil Armstrong and Buzz Aldrin landed on the moon
    in 1969.”'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成的回应**：“尼尔·阿姆斯特朗和巴兹·奥尔德林在 1969 年登月。”'
- en: The generated response isn’t sufficiently relevant to the original query — the
    third link is broken.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的回应与原始查询不够相关——第三个链接已损坏。
- en: There are many other nuances to RAG evaluation, some of which are discussed
    in [Adam Kamor’s series of articles on RAG evaluation](https://www.tonic.ai/blog/rag-evaluation-series-validating-the-rag-performance-of-the-openais-rag-assistant-vs-googles-vertex-search-and-conversation).²¹
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 评估 RAG 还有许多其他细微差别，其中一些已经在[Adam Kamor 的 RAG 评估系列文章](https://www.tonic.ai/blog/rag-evaluation-series-validating-the-rag-performance-of-the-openais-rag-assistant-vs-googles-vertex-search-and-conversation)中讨论过。²¹
- en: In so far as veracity is concerned, the RAG strategy is to avoid hallucination
    by deriving responses as much as possible from relevant trustworthy human-written
    preexisting text.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 就真实性而言，RAG 策略是尽量通过从相关、可信的人工编写的现有文本中推导回应来避免幻觉。
- en: How RAG lags
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAG 的不足之处
- en: 'Yet how does the RAG strategy square with the popular critique of language
    AI: that it is merely parroting the text it was trained on? Does it go beyond
    parroting, to handle counterfactuals? The RAG strategy basically tries to avoid
    hallucination by supplementing training text with additional text curated by humans,
    humans in the loop, who can attend to the scope of the chatbot’s particular use
    case. Thus humans-in-the-loop supplement the generative LLM’s training, by supplying
    a corpus of relevant factual texts to be drawn from.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，RAG 策略如何与大众对语言 AI 的批评相协调呢？即它仅仅是在重复训练时使用的文本？它是否超越了简单的重复，能够处理反事实情况？RAG 策略基本上试图通过补充由人工精心策划的额外文本来避免幻觉，这些人工环节能够关注聊天机器人的特定应用场景。因此，人工环节通过提供相关事实文本的语料库来补充生成型大型语言模型（LLM）的训练。
- en: Works of fiction are typically not included in the corpus that populates a RAG’s
    vector store. And even preexisting fictional prose doesn’t exhaust the theoretically
    infinite number of counterfactual propositions which might be deemed true, or
    correct, in some sense.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 小说类作品通常不包括在填充 RAG 向量存储的语料库中。即便是现有的虚构散文也无法涵盖理论上无限多的反事实命题，这些命题可能在某种意义上被认为是真实的或正确的。
- en: 'But intelligence includes the ability to assess such counterfactual propositions:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 但智力包括评估此类反事实命题的能力：
- en: “My foot up to my ankle will get soaking wet if I step in that huge puddle.”
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “如果我踩进那个大水坑，我的脚踝以下会被打湿。”
- en: In this case, a GenAI system able to synthesize verbalizations previously issued
    by humans — whether from the LLM’s training set or from a context retrieved and
    supplied downstream — isn’t very impressive. Rather than original reasoning, it’s
    just parroting what someone already said. And parroting what’s already been said
    doesn’t serve the purpose at hand when counterfactuals are considered.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，一个能够综合人类之前发出的言辞的 GenAI 系统——无论是来自 LLM 的训练集还是从下游检索并提供的上下文——并不令人印象深刻。与其说是原创推理，不如说它只是重复了别人已经说过的话。而当考虑到反事实情况时，重复已说过的话并不能达到目的。
- en: 'Here’s a pair of proposals:'
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这里有一对命题：
- en: '**To the extent a GenAI is just parroting, it is bringing forth or synthesizing
    verbalizations it was trained on.**'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在 GenAI 仅仅是在重复的情况下，它只是呈现或综合它所训练的言辞。**'
- en: '**To the extent a GenAI can surmount mere parroting and reason accurately,
    it can successfully handle counterfactuals.**'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在 GenAI 能够克服单纯重复并进行准确推理的情况下，它能够成功处理反事实情况。**'
- en: The crucial thing about **counterfactuals** that Molak explains is that they
    “can be thought of as hypothetical or simulated interventions that *assume a particular
    state of the world*” or as Pearl, Gilmour, and Jewell describe counterfactuals
    as a minimal modification to a system (Molak, 28).¹¹ ²² The point is that answering
    counterfactuals correctly — or even plausibly — requires more-than-anecdotal knowledge
    of the world. For LLMs, their corpus-based pretraining, and their prompting infused
    with retrieved factual documents pins their success to the power of anecdote.
    Whereas a human intelligence often doesn’t need, and cannot rely on, anecdote
    to engage in counterfactual reasoning plausibly. That’s why counterfactual reasoning
    is in some ways a better measure of LLMs capabilities than fidelity to factuality
    is.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Molak 解释的关于 **反事实** 的关键点是，它们“可以被认为是假设的或模拟的干预，*假定了一个特定的世界状态*”，或者正如 Pearl、Gilmour
    和 Jewell 所描述的，反事实是对系统的最小修改（Molak, 28）。¹¹ ²² 重点是，正确回答反事实问题——甚至是合理回答——需要对世界有超越轶事的了解。对于
    LLM，它们基于语料库的预训练，以及通过检索的事实文档填充的提示，使得它们的成功依赖于轶事的力量。而人类智能往往不需要，也无法依赖于轶事来进行反事实推理，这是为什么反事实推理在某些方面比忠实于事实更能衡量
    LLM 的能力。
- en: '![](../Images/843511802bdffe6f89734d7a2d752556.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/843511802bdffe6f89734d7a2d752556.png)'
- en: Photo by [sutirta budiman](https://unsplash.com/@sutirtab?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [sutirta budiman](https://unsplash.com/@sutirtab?utm_source=medium&utm_medium=referral)
    提供，来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '**Running Open-source LLMs: Ollama**'
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**运行开源 LLM：Ollama**'
- en: To explore a bit these issues of counterfactuality with respect to Large Language
    Models, let us consider them more concretely by running a generative model. To
    minimize impediments I will demonstrate it by downloading a model to run on one’s
    own machine — so you don’t need an api key. We’ll do this using [Ollama](https://ollama.com/).
    (If you don’t want to try this yourself, you can skip over the rest of this section.)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更具体地探讨与大型语言模型相关的反事实问题，我们通过运行一个生成模型来进行深入分析。为了减少障碍，我将通过下载一个模型并在本地计算机上运行来演示——因此你不需要
    API 密钥。我们将使用 [Ollama](https://ollama.com/) 来完成这一过程。（如果你不想自己尝试，可以跳过本节的其余部分。）
- en: Ollama is a free tool that facilitates running open source LLMs on your local
    computer. It’s also possible to run Ollama in DataBricks, and possibly other cloud
    platforms. For simplicity’s sake, let’s do it locally. (For such local setup I’m
    indebted to Iago Modesto Brandão’s handy [Building Open Source LLM based Chatbots
    using Llama Index](https://medium.com/poatek/building-open-source-llm-based-chatbots-using-llama-index-e6de9999ee76)²³
    from which the following is adapted.)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Ollama 是一款免费工具，方便在本地计算机上运行开源 LLM。也可以在 DataBricks 以及可能的其他云平台上运行 Ollama。为了简便起见，我们将在本地运行。（对于这种本地设置，我要感谢
    Iago Modesto Brandão 的实用教程 [Building Open Source LLM based Chatbots using Llama
    Index](https://medium.com/poatek/building-open-source-llm-based-chatbots-using-llama-index-e6de9999ee76)²³，以下内容即源自该教程。）
- en: 'The easiest way is to: download and install docker (the Docker app) then, within
    terminal, run a couple of commands to pull and run ollama as a server, which can
    be accessed from within a jupyter notebook (after installing two packages).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的方法是：下载并安装 Docker（Docker 应用），然后在终端中运行几个命令来拉取并运行 Ollama 作为服务器，之后可以在 jupyter
    notebook 中访问（安装两个软件包后）。
- en: 'Here are the steps:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是步骤：
- en: 1\. Download and install Docker [https://www.docker.com/products/docker-desktop/](https://www.docker.com/products/docker-desktop/)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 下载并安装 Docker [https://www.docker.com/products/docker-desktop/](https://www.docker.com/products/docker-desktop/)
- en: '2\. Launch Terminal and run these commands one after another:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 启动终端并依次运行以下命令：
- en: '[PRE0]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '3\. Launch jupyter:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 启动 jupyter：
- en: '`jupyter notebook`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`jupyter notebook`'
- en: '4\. Within the jupyter notebook, import ollama and create an LLM object. For
    the sake of speed, we’ll use a relatively smaller model: Microsoft’s *phi*.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 在 jupyter notebook 中导入 Ollama 并创建 LLM 对象。为了加快速度，我们将使用一个相对较小的模型：微软的 *phi*。
- en: Now we’re ready to use Phi via ollama to generate text in response to our prompt.
    For this we use the llm object’s **complete()** method. It generates a response
    (might take a minute or so), which we’ll print out.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好通过 Ollama 使用 Phi 生成文本来响应我们的提示。为此，我们使用 llm 对象的 **complete()** 方法。它会生成一个响应（可能需要一分钟左右），然后我们将其打印出来。
- en: 'Let’s try out a counterfactual: : “Would a cement balloon float?”'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一个反事实：“水泥气球会浮起来吗？”
- en: '![](../Images/bb476bb26abbcb9d44fde17abb2c0175.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bb476bb26abbcb9d44fde17abb2c0175.png)'
- en: Photo by Haneen Krimly on Unsplash
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 Haneen Krimly 提供，来自 Unsplash
- en: Next let’s examine the “reasoning” displayed in Phi’s output.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来让我们看看 Phi 输出中的“推理”过程。
- en: '**Interpreting LLM (GenAI) answers to counterfactual questions**'
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**解读 LLM（生成式人工智能）对反事实问题的回答**'
- en: 'Leaving aside, for the moment, that Microsoft’s Phi is not a very powerful
    GPT (generative pretrained transformer), consider its response:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 暂且不论微软的 Phi 不是一个非常强大的 GPT（生成预训练变换器），我们来看看它的回答：
- en: '[PRE1]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This seems reasonable. If a regular balloon (one I’ve blown up directly with
    my mouth) wouldn’t float in the air, then neither would one whose exterior is
    made from cement. If instead, however, the gas pumped inside is lighter than air,
    such as helium is, then a regular balloon would float up into the sky.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来有道理。如果一个普通气球（我直接用嘴吹起来的那种）不能在空气中浮起，那么一个外壳是水泥的气球也不会浮起来。但是，如果气球内充入的气体比空气轻，比如氦气，那么一个普通气球就会飘升到天空中。
- en: '![](../Images/6256a74db1c74ac7e8406c933abd12b9.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6256a74db1c74ac7e8406c933abd12b9.png)'
- en: Photo by First Last @ rtisanyb on [Unsplash](https://unsplash.com/es/fotos/un-grupo-de-piedras-nOo9yg8pTgQ)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 First Last @ rtisanyb 提供，来源于[Unsplash](https://unsplash.com/es/fotos/un-grupo-de-piedras-nOo9yg8pTgQ)
- en: The original question asked about floating, but not didn’t specify the medium;
    apparently Phi assumed it to be air.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 原始问题询问的是浮起的问题，但并没有明确指定介质；显然 Phi 假设它是空气。
- en: So now let’s specify that in the question, but choose water instead, which was
    actually my original intention anyway.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 那么现在让我们在问题中明确这一点，但选择水作为介质，毕竟这其实是我最初的意图。
- en: '[PRE2]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Interesting. Not only does it conclude that it would sink, but now it assumes
    that the medium is water instead of air. It reasons about the density of cement,
    as compared to air, but doesn’t take into account the air inside the balloon.
    Or possibly it’s reasoning about a solid cement ball; but, hmmm, that’s not a
    balloon. Unless, by “cement balloon” it’s thinking that this is analogous to a
    water balloon: a balloon filled with water.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 很有趣。不仅得出它会下沉的结论，现在还假设介质是水而不是空气。它推理水泥的密度与空气相比，但没有考虑气球内部的空气。或者它可能在推理一个固体水泥球；不过，嗯，那可不是气球。除非，它所说的“水泥气球”是指类似于水气球：一个充满水的气球。
- en: Let’s change the question again, this time specifying the medium is water.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次修改问题，这次明确介质是水。
- en: '[PRE3]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Hmm, that’s odd. According to this latest response, the same object that would
    float up into the air, would sink in water, even though water is denser than air.
    Sure, this is a counterfactual situation — who’s ever heard of a cement balloon?
    But let’s try to visualize it: You are standing by a pool of water, or perhaps
    you are in a boat. You are holding this spherical object, say at waist level.
    You let go of it, and it starts to float up higher into the air, but you catch
    it before it’s out of reach. Now you lower this same spherical object down to
    the surface of the water. And as it touches the water, you slowly and gently let
    go again. What do you think will happen?'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，这有点奇怪。根据这个最新的回答，同样一个会在空气中浮起的物体，在水中却会下沉，尽管水的密度比空气大。确实，这是一种反事实的情境——谁见过水泥气球呢？不过让我们尝试想象一下：你站在一个水池旁，或者你可能在一艘船上。你手里拿着这个球形物体，假设在腰部位置。你放开它，它开始向空中浮起，但你在它还没有超出你的接触范围之前抓住了它。现在你把这个同样的球形物体放到水面上。当它接触到水面时，你慢慢地轻轻放开它。你觉得会发生什么呢？
- en: Will this helium-filled cement balloon, which a moment earlier had started to
    float up into the sky, now suddenly be swallowed up by the water, pushed down
    to sink to the bottom? Does that ring true?
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这个充满氦气的水泥气球，刚刚还开始飘升到天空，现在是否会突然被水吞噬，沉入水底呢？这个感觉靠谱吗？
- en: 'Keep in mind our own embodied knowledge, which might be relevant here: Our
    own flesh-and-blood bodies — which aren’t even light enough to spontaneously float
    up into the air — when placed on water in fact don’t sink, but rather float. That
    embodied knowledge suggests that an object that is light enough to float up into
    the air, and thus is lighter than ourselves, would also be light enough to float
    on whatever, if we (being heavier than that object) float on water. (Consult Lakoff
    & Johnson for an overview of embodied cognition.²⁴)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住我们自身的体现性知识，这在这里可能是相关的：我们自己的肉体——它们甚至没有轻到可以自发浮起——当放在水面上时，实际上不会下沉，而是会浮起来。这种体现性知识表明，一个足够轻以至于能够浮起的物体，也就比我们轻，因此它也足够轻，能够在任何地方浮动，如果我们（比那个物体重）能够在水面上漂浮的话。（请参阅
    Lakoff & Johnson 了解体现认知的概述²⁴）
- en: Granted, a GPT language model, such as Phi, is decidedly disembodied; it lacks
    embodied knowledge. In this case, perhaps that embodied knowledge, which Phi lacks,
    impinges on counterfactual reasoning ability.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，GPT语言模型像Phi这样的模型，显然是没有实体的；它缺乏体现的知识。在这种情况下，也许正是Phi缺乏的这种体现的知识，影响了它的反事实推理能力。
- en: Ok, but perhaps context and continuity are the issue here. What I’ve shown above
    is not a true conversation, as would happen with a chat-bot; I haven’t passed
    the previous context back to Phi, but rather I’m asking isolated questions. So
    perhaps Phi is imagining different proportions of cement-to-helium in its responses
    to different questions. After all, if the cement were thick enough, or if the
    chamber of helium inside it were small enough, then the relative lightness of
    helium to air (or water) would insufficiantly counteract the heaviness of the
    cement. So lets rephrase the question to clarify that detail.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，但也许这里的问题是上下文和连续性。我上面展示的并不是一个真实的对话，就像与聊天机器人互动那样；我没有把之前的上下文传递给Phi，而是问了孤立的问题。所以，或许Phi在回答不同问题时，想象了不同的水泥与氦气的比例。毕竟，如果水泥足够厚，或者它内部的氦气腔室足够小，那么氦气相对于空气（或水）的轻盈就不足以抵消水泥的重量。所以让我们重新措辞问题，澄清这一点。
- en: '[PRE4]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Ok, still something’s wrong. Within the same response it’s asserting that the
    cement balloon is light enough to float up into the air, but is too heavy to float
    in water. Doesn’t this imply that water is lighter than air? Hmm,
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，但似乎还有问题。在同一个回答中，它断言水泥气球足够轻，可以飘浮到空中，但又太重，无法在水中浮起。这不是意味着水比空气轻吗？嗯，
- en: something about that smells fishy — and it isn’t the water.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 那件事有点不对劲——但那不是水的问题。
- en: I’m not the first to suggest that GPT language models like Phi (models that
    gain their ability through training on massive corpuses of text) sometimes lack
    basic reasoning ability. In this case I’m suggesting how some of that reasoning
    ability emerges from implicit embodied knowledge — knowledge we gain through physical
    presence of our bodies within the material world (our physical bodies which don’t
    float up into the air but do float on water). I’m also suggesting how counterfactual
    questions can help reveal this.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我不是第一个提出GPT语言模型（像Phi这样的模型，通过在大量文本语料库上的训练获得能力）有时缺乏基本推理能力的人。在这种情况下，我提出的是一些推理能力如何源于隐性体现的知识——我们通过身体在物质世界中的存在获得的知识（我们的身体不会漂浮到空中，但确实会漂浮在水上）。我还在建议，反事实问题如何有助于揭示这一点。
- en: GPT models, such as Phi, primarily act associatively. It is the associative
    or connotative facets of language usage that GPTs and LLMs generally have mastered
    so well. In terms of connotation, arising from most common usage, the word ‘cement’
    is associated with heaviness. It has this in common with the heavy metal lead
    — “Led Zeppelin” was originally a sarcastic barb precluding the New Yardbirds’
    prospects to soar. Although if enough air were pumped into it, cement could become
    light, that isn’t an association that readily comes to mind. Rather it’s a counterfactual,
    a thought experiment. Even though my previous phrasing of the question refers
    to “a very thin layer of cement,… filled with helium”, the material itself, “cement,”
    too strongly associates with heavines for Phi to imagine it as floating in water
    — no one has ever built a cement boat!
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: GPT模型，比如Phi，主要是通过联想来运作的。GPT和大型语言模型（LLMs）通常掌握得非常好的，正是语言使用中的联想或内涵方面。在内涵上，基于最常见的用法，‘水泥’一词与沉重相关。它和重金属铅有相同的联想——“Led
    Zeppelin”最初就是一种讽刺，暗示新“飞鸟”乐队将无法飞翔。虽然如果把足够的空气注入其中，水泥可能变得轻，但这不是一个容易想到的联想。相反，这是一个反事实，一个思想实验。即使我之前的措辞提到“非常薄的一层水泥……充满了氦气”，但“水泥”这个材料本身，太强烈地与沉重相关，使Phi无法想象它在水中漂浮——毕竟从来没有人造过水泥船！
- en: '![](../Images/5fc98ae150f9906b8e3018fd3fbf97a1.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5fc98ae150f9906b8e3018fd3fbf97a1.png)'
- en: Photo by [Tim Huyghe](https://unsplash.com/@huyghetim?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Tim Huyghe](https://unsplash.com/@huyghetim?utm_source=medium&utm_medium=referral)提供，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: So let’s switch out cement for a different substance with less leaden connotations.
    How about glass?
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 那我们就换个东西，把水泥换成一个没有那么沉重联想的物质。比如玻璃怎么样？
- en: '[PRE5]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Aha! Now with associative (connotative) obstacles brushed to the side, Phi is
    able to arrive at reasoning that is comparable to human reasoning, replete with
    implicit embodied knowledge, even though the latter isn’t the source in this case.
    In this case Phi has uncovered the key point, that “the density of helium gas
    is much lower than that of water” even though it has gotten mixed up in its “Additional”
    point, where it says that glass’s density is less than water’s, which is a hallucination;
    it’s not true.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 啊哈！在联想（内涵）障碍被排除之后，Phi能够得出与人类推理相当的推理，充满了隐性具体现实的知识，即使在这种情况下，后者并非来源。在这个例子中，Phi揭示了关键点：“氦气的密度远低于水的密度”，尽管它在“附加”点中搞混了，声称玻璃的密度低于水的密度，这其实是幻觉，根本不是真的。
- en: '**What about OpenAI’s ChatGPT?**'
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**OpenAI的ChatGPT怎么样？**'
- en: The big elephant in the room, though, is that Microsoft’s Phi is very far from
    being the state of the art LLM. It’s small enough to run locally on a laptop.
    So what about OpenAI’s GPT models?
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，显而易见的一个问题是，微软的Phi距离成为最先进的LLM还很远。它足够小，可以在笔记本电脑上本地运行。那么，OpenAI的GPT模型怎么样呢？
- en: To run this code below, you’ll need to sign up for free at [https://openai.com](https://openai.com)
    to get an OpenAI API access token.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行下面的代码，你需要在[https://openai.com](https://openai.com)上免费注册，获取一个OpenAI API访问令牌。
- en: What does OpenAI’s GPT say about the helium-filled glass balloon on water?
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的GPT对于氦气填充的玻璃气球在水中的表现有什么看法？
- en: '[PRE6]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Oh no! Even OpenAI’s GPT 3.5 turbo gets buried in the connotations of cement.
    It gets that “helium filled balloons float in air.” And it gets that helium is
    less dense than water. But then it fumbles into thinking “the overall density
    of the balloon, including the glass and helium, will still be higher than that
    of water.” As Phi did above, OpenAI’s GPT 3.5 turbo has implied that the balloon
    is heavier than water but lighter than air, which implies that water is lighter
    than air.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 哦不！就连OpenAI的GPT 3.5 turbo也陷入了水泥的内涵中。它知道“氦气充满的气球会浮在空气中”。它知道氦气比水的密度小。但随后，它错误地认为“气球的整体密度，包括玻璃和氦气，仍然会高于水的密度。”就像Phi上面做的那样，OpenAI的GPT
    3.5 turbo暗示气球比水重，但比空气轻，这就意味着水比空气轻。
- en: 'We know it’s wrong; but it’s not wrong because it’s lacking facts, or has directly
    contradicted fact: The whole cement balloon scenario is far from being fact; it’s
    counterfactual.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道它是错误的；但它并不是因为缺乏事实或直接与事实相矛盾而错误：整个水泥气球的情景远非事实；它是反事实的。
- en: Post-hoc we are able to apply *reductio ad absurdum* to deduce that Phi’s and
    OpenAI’s GPT 3.5 turbo’s negative conclusions do actually contradict another fact,
    namely that water is heavier than air. But this is a respect in which counterfactual
    reasoning is in fact reasoning, not just dreaming. That is, counterfactual reasoning
    can be shown to be definitively true or definitively false. Despite deviating
    from what’s factual, it is actually just as much a form of reasoning as is reasoning
    based on fact.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 事后我们可以运用*归谬法*推断出，Phi和OpenAI的GPT 3.5 turbo的负面结论确实与另一个事实相矛盾，即水比空气重。但在这一点上，反事实推理实际上是一种推理，而不仅仅是做白日梦。也就是说，反事实推理可以被证明是确定的真或假。尽管它偏离了事实，但它实际上和基于事实的推理一样，也是推理的一种形式。
- en: '**Fact, Fiction, and Hallucination: What counterfactuals show us**'
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**事实、虚构与幻觉：反事实给我们展示了什么**'
- en: Since ChatGPT overwhelmed public consciousness in late 2022, the dominant concern
    that was immediately and persistently stirred up has been hallucination. Oh the
    horror that an AI system could assert something not based in fact! But instead
    of focusing on just factuality as a primary standard for AI systems — as has happened
    in many business use-cases — it now seems clear that fact vs. fiction isn’t the
    only axis along which an AI system should be expected to or hoped to succeed.
    Even when an AI system’s response is based in fact, it can still be irrelevant,
    a non sequitur, which is why evaluation approaches such as Ragas and TruVera specifically
    examine relevance of response.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 自从ChatGPT在2022年底席卷公众意识以来，随之而来并持续引发的主要担忧就是幻觉。哦，天哪，一个AI系统居然会声称一些没有事实依据的事情！但是，与其将事实作为AI系统的主要标准——就像许多商业用例中那样——现在看来，事实与虚构并不是衡量AI系统是否成功的唯一维度。即使一个AI系统的回应是基于事实，它仍然可能是无关的，或者是个不合逻辑的推论，这就是为什么像Ragas和TruVera这样的评估方法特别关注回应的相关性。
- en: 'When it fails on the relevance criterion, it is not even the Fact vs. Fiction
    axis that is at play. An irrelevant response can be just as factual as a relevant
    one, and by definition, counterfactual reasoning, whether correct or not, is not
    factual in a literal sense, certainly not in the sense constituted by RAG systems.
    That is, counterfactual reasoning is not achieved by retrieving documents that
    are topically relevant to the question posed. What makes counterfactual reasoning
    powerful is how it may apply analogies to bring to bear systems of facts that
    might seem completely out of scope to the question being posed. It might be diagrammed
    something like this:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '当在相关性标准上失败时，甚至不涉及事实与虚构的轴线。一个无关的回答可以与相关的回答一样是事实，并且根据定义，反事实推理，无论其是否正确，都不是字面意义上的事实，当然也不是由RAG系统所构成的那种事实。也就是说，反事实推理并不是通过检索与提出的问题主题相关的文档来实现的。反事实推理的强大之处在于它如何运用类比来引入那些看似完全与问题无关的事实体系。它可能像这样被图示化：  '
- en: '![](../Images/601c8456b6defd2cc80c76c6dab2c57e.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/601c8456b6defd2cc80c76c6dab2c57e.png)'
- en: image by author
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '图片由作者提供  '
- en: 'One might also represent some of these facets this way:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '也可以以这种方式表示其中的一些方面：  '
- en: '![](../Images/d741ae68d8857c9d169222ccb9e9d699.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d741ae68d8857c9d169222ccb9e9d699.png)  '
- en: image by author
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '图片由作者提供  '
- en: '**What do linear estimators have to do with this?** Since counterfactual reasoning
    is not based in seemingly relevant facts but rather in systematized facts, sometimes
    from other domains, or that are topically remote, it’s not something that obviously
    benefits directly from document-store retrieval systems. There’s an analogy here
    between types of linear estimators: A gradient-boosted tree linear estimator essentially
    cannot succeed in making accurate predictions on data whose features substantially
    exceed the numeric ranges of the training data; this is because decision cut-points
    can only be made based on data presented at training time. By contrast, regression
    models (which can have closed form solutions) enable accurate predictions on features
    that exceed the numerical ranges of the training data.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性估计器与此有何关联？** 由于反事实推理并非基于看似相关的事实，而是基于系统化的事实，有时这些事实来自其他领域，或是与主题相远的内容，因此它并不是直接从文档存储检索系统中明显获益的东西。这里可以类比线性估计器的类型：梯度提升树线性估计器本质上无法在特征大大超出训练数据的数值范围时做出准确预测；这是因为决策切分点只能基于训练时提供的数据来做出。相比之下，回归模型（其可以有封闭解形式）能够在特征超出训练数据的数值范围时做出准确预测。  '
- en: 'In practical terms this is why linear models can be helpful in business applications.
    To the extent your linear model is accurate, it might help you predict the outcome
    of raising or lowering your product’s sales price beyond any price you’ve ever
    offered before: a *counterfactual price*. Whereas a gradient-boosted tree model
    that performs equally well in validation does not help you reason through such
    counterfactuals, which, ironically, might have been the motivation for developing
    the model in the first place. In this sense, the *explainability* of linear models
    is of a completely different sort from what SHAP values offer, as the latter shed
    little light on what would happen with data that is outside the distribution of
    the model’s training data.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '从实践角度来看，这就是为什么线性模型在商业应用中可能会有帮助。只要你的线性模型准确，它可以帮助你预测提高或降低产品售价后，超出你曾经提供过的任何价格时的结果：一个*反事实价格*。而一个在验证中表现同样良好的梯度提升树模型并不能帮助你推理这些反事实，这有些讽刺，因为开发该模型的初衷可能正是为了应对这种情况。从这个意义上说，线性模型的*可解释性*与SHAP值提供的完全不同，因为后者对于模型训练数据分布之外的数据会显得毫无解释力。  '
- en: The prowess of LLMs has certainly shown that the limits of “intelligence” synthesized
    ingeniously from crowdsourcing human-written texts are much greater than expected.
    It’s obvious that this eclipsed the former tendency to place value in “intelligence”
    based on conceptual understanding, which reveals itself especially in the ability
    to accurately reason beyond facts. So I find it interesting to attempt to challenge
    LLMs to this standard, which goes against their grain.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLM的强大表现无疑证明了从人群外包写作的文本中巧妙合成的“智能”比预期的要强大得多。显然，这超越了过去将“智能”寄托于概念理解的趋势，特别体现在超越事实进行准确推理的能力上。因此，我发现挑战LLM达到这一标准是非常有趣的，这挑战了它们的基本特性。  '
- en: '![](../Images/0c9a595810c03384dc2ad6a930b2b82b.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c9a595810c03384dc2ad6a930b2b82b.png)  '
- en: Bonnie Moreland on Freerange Stock
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 'Bonnie Moreland 来自 Freerange Stock  '
- en: '**Reflection**'
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**反思**'
- en: 'Far from being frivolous diversions, counterfactuals play a role in the progress
    of science, exemplifying what Charles Sanders Peirce calls *abduction*, which
    is distinct from induction (inductive reasoning) and deduction (deductive reasoning).
    Abduction basically means the formulation of hypotheses. We might rightly ask:
    Should we expect an LLM to exhibit such capability? What’s the advantage? I don’t
    have a definitive answer, but more a speculative one: It’s well known within the
    GenAI community that when prompting an LLM, asking it to “reason step-by-step”
    often leads to more satisfactory responses, even though the reasoning steps themselves
    are not the desired response. In other words, for some reason, not yet completely
    understood, asking the LLM to somewhat simulate the most reliable processes of
    human reasoning (thinking step-by-step) leads to better end results. Perhaps,
    somewhat counterintuitively, even though LLMs are not trained to reason as humans
    do, the lineages of human reasoning in general contribute to better AI end results.
    In this case, given the important role that abduction plays in the evolution of
    science, AI end results might improve to the extent that LLMs are capable of reasoning
    counterfactually.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 反事实不仅仅是轻浮的娱乐，它们在科学进步中发挥了作用，体现了查尔斯·桑德斯·皮尔士所说的*归纳推理*，这与归纳（归纳推理）和演绎（演绎推理）不同。归纳基本上是指假设的提出。我们可能会问：我们应该期望LLM具备这种能力吗？有什么优势？我没有一个确定的答案，更像是一个猜测：在生成式AI（GenAI）社区中，广为人知的是，当给LLM提供提示，要求其“逐步推理”时，通常会得到更令人满意的回答，即使推理步骤本身并不是预期的回答。换句话说，出于某种尚未完全理解的原因，让LLM在某种程度上模拟人类推理中最可靠的过程（逐步思考）会导致更好的最终结果。或许，虽然LLM并没有像人类那样进行训练推理，但人类推理的传承普遍有助于AI的最终结果。在这种情况下，考虑到归纳在科学发展中的重要作用，AI的最终结果可能会随着LLM能够进行反事实推理而得到改进。
- en: Visit me on [LinkedIn](https://www.linkedin.com/in/joshua-banks-mailman/)
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在[LinkedIn](https://www.linkedin.com/in/joshua-banks-mailman/)上访问我
- en: '[1] [The Bottom Line](https://www.heraldscotland.com/business_hq/13103840.bottom-line/).
    (2013) The Herald. May 8.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [底线](https://www.heraldscotland.com/business_hq/13103840.bottom-line/)。
    (2013) 《赫拉尔德报》。 5月8日'
- en: '[2] C. Munn, Investing for Generations: A History of the Alliance Trust (2012)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] C. Munn, 《代际投资：联盟信托的历史》(2012)'
- en: '[3] R. Strassfeld. [If…: Counterfactuals in the Law](https://scholarlycommons.law.case.edu/cgi/viewcontent.cgi?referer=&httpsredir=1&article=1372&context=faculty_publications).
    (1992) Case Western Reserve University School of Law Scholarly Commons'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] R. Strassfeld. [如果……：法律中的反事实](https://scholarlycommons.law.case.edu/cgi/viewcontent.cgi?referer=&httpsredir=1&article=1372&context=faculty_publications)。
    (1992) 凯斯西储大学法学院学术共享平台'
- en: '[4] S. Wachter, B. Mittelstadt, & C. Russell, [Counterfactual Explanations
    without Opening the Black Box: Automated Decisions and the GDPR](https://jolt.law.harvard.edu/assets/articlePDFs/v31/Counterfactual-Explanations-without-Opening-the-Black-Box-Sandra-Wachter-et-al.pdf).
    2018\. Harvard Journal of Law and Technology, vol. 31, no. 2'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] S. Wachter, B. Mittelstadt, & C. Russell, [在不揭开黑箱的情况下进行反事实解释：自动化决策与GDPR](https://jolt.law.harvard.edu/assets/articlePDFs/v31/Counterfactual-Explanations-without-Opening-the-Black-Box-Sandra-Wachter-et-al.pdf)。2018年。
    《哈佛法律与技术期刊》，第31卷，第2期'
- en: '[5] J. Stoyanovich, [Testimony of Julia Stoyanovich before the New York City
    Department of Consumer and Worker Protection regarding Local Law 144 of 2021 in
    Relation to Automated Employment Decision Tools (AEDTs)](https://rules.cityofnewyork.us/wp-content/uploads/2022/09/Stoyanovich_LL144_October24_2022.pdf).
    (2022) Center for Responsible AI, NY Tandon School of Engineering'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] J. Stoyanovich, [Julia Stoyanovich在纽约市消费者和工人保护部关于2021年地方法第144条与自动化就业决策工具（AEDTs）相关事宜的证词](https://rules.cityofnewyork.us/wp-content/uploads/2022/09/Stoyanovich_LL144_October24_2022.pdf)。
    (2022) 负责任AI中心，纽约大学坦顿工程学院'
- en: '[6] S. Lorr, [A Hiring Law Blazes a Path for A.I. Regulation](https://www.nytimes.com/2023/05/25/technology/ai-hiring-law-new-york.html)
    (2023) The New York Times, May 25'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] S. Lorr, [一项招聘法为人工智能监管开辟了道路](https://www.nytimes.com/2023/05/25/technology/ai-hiring-law-new-york.html)
    (2023) 《纽约时报》，5月25日'
- en: '[7] J. Stoyanovich, [“Until you try to regulate, you won’t learn how”: Julia
    Stoyanovich discusses responsible AI for The New York Times](https://nyudatascience.medium.com/until-you-try-to-regulate-you-wont-learn-how-julia-stoyanovich-discusses-responsible-ai-for-4cf119002b05)
    (2023), Medium. June 21'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] J. Stoyanovich, [“只有在你尝试监管时，你才会学会如何做”：Julia Stoyanovich讨论《纽约时报》中的负责任AI](https://nyudatascience.medium.com/until-you-try-to-regulate-you-wont-learn-how-julia-stoyanovich-discusses-responsible-ai-for-4cf119002b05)
    (2023)，Medium。 6月21日'
- en: '[8] J. Pearl, [Foundations of Causal Inference](https://ftp.cs.ucla.edu/pub/stat_ser/r355-reprint.pdf)
    (2010). Sociological Methodology. vol. 40, no. 1.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] J. Pearl, [因果推断基础](https://ftp.cs.ucla.edu/pub/stat_ser/r355-reprint.pdf)（2010）。《社会学方法学》。第40卷，第1期。'
- en: '[9] L. Wittgenstein, Philosophical Investigations (1953) G.E.M. Anscombe and
    R. Rhees (eds.), G.E.M. Anscombe (trans.), Oxford: Blackwell'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] L. Wittgenstein. 《哲学调查》（1953）由G.E.M. Anscombe和R. Rhees编，G.E.M. Anscombe翻译，牛津：Blackwell出版。'
- en: '[10] E. Siegel, [The AI Playbook: Mastering the Rare Art of Machine Learning
    Deployment](https://mitpress.mit.edu/9780262048903/the-ai-playbook/). (2024) MIT
    Press.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] E. Siegel. [AI行动指南：掌握机器学习部署的稀有艺术](https://mitpress.mit.edu/9780262048903/the-ai-playbook/)（2024）MIT出版社。'
- en: '[11] A. Molak. [Causal Inference and Discovery in Python](https://www.packtpub.com/product/causal-inference-and-discovery-in-python/9781804612989)
    (2023) Packt.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] A. Molak. [Python中的因果推断与发现](https://www.packtpub.com/product/causal-inference-and-discovery-in-python/9781804612989)（2023）Packt出版。'
- en: '[12] R. Byrne. The Rational Imagination: How People Create Alternatives to
    Reality (2005) MIT Press.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] R. Byrne. 《理性想象：人们如何创造现实的替代品》（2005）MIT出版社。'
- en: '[13] W. Starr. [Counterfactuals](https://plato.stanford.edu/entries/counterfactuals/)
    (2019) Stanford Encyclopedia of Philosophy.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] W. Starr. [反事实](https://plato.stanford.edu/entries/counterfactuals/)（2019）斯坦福哲学百科全书。'
- en: '[14] M. Ginsburg. [Counterfactuals](https://www.ijcai.org/Proceedings/85-1/Papers/015.pdf)
    (1985)Proceedings of the Ninth International Joint Conference on Artificial Intelligence.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] M. Ginsburg. [反事实](https://www.ijcai.org/Proceedings/85-1/Papers/015.pdf)（1985）第九届国际人工智能联合会议论文集。'
- en: '[15] J. Pearl, Judea. Causation, Action, and Counterfactuals (1995) in Computational
    Learning and Probabilistic Reasoning. John Wiley and Sons.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] J. Pearl, Judea. 《因果关系、行动与反事实》（1995）收录于《计算学习与概率推理》。约翰·威利与儿子出版社。'
- en: '[16] T. Costello and J. McCarthy. Useful Counterfactuals (1999) Linköping Electronic
    Articles in Computer and Information Science, vol. 4, no. 12.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] T. Costello和J. McCarthy. 《有用的反事实》（1999）Linköping电子计算机与信息科学文献，第4卷，第12期。'
- en: '[17] F. McEvoy. [Raising Baby AI in 2024](https://fionajmcevoy.medium.com/raising-baby-ai-in-2024-1b7d704cd3d5)
    (2024) Medium.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] F. McEvoy. [2024年抚养“宝宝AI”](https://fionajmcevoy.medium.com/raising-baby-ai-in-2024-1b7d704cd3d5)（2024）Medium。'
- en: '[18] N. Berger. [Why Do Kitties Knock Things Over?](https://www.northtorontocatrescue.com/why-do-kitties-knock-things-over/)
    (2020) North Toronto Cat Rescue.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] N. Berger. [为什么小猫喜欢把东西弄倒？](https://www.northtorontocatrescue.com/why-do-kitties-knock-things-over/)（2020）北多伦多猫救援。'
- en: '[19] C. S. Peirce. Collected Papers of Charles Sanders Peirce (1931–1958) edited
    by C. Hartshorne, P. Weiss, and A. Burks,. Harvard University Press.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] C. S. Peirce. 《查尔斯·桑德斯·皮尔士文集》（1931–1958），由C. Hartshorne、P. Weiss和A. Burks编辑。哈佛大学出版社。'
- en: '[20] I. Douven. [Peirce on Abduction](https://plato.stanford.edu/entries/abduction/peirce.html)
    (2021) Stanford Encyclopedia of Philosophy.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] I. Douven. [皮尔士的溯因](https://plato.stanford.edu/entries/abduction/peirce.html)（2021）斯坦福哲学百科全书。'
- en: '[21] A. Kamor. [RAG Evaluation Series: Validating the RAG performance of the
    OpenAI’s Rag Assistant vs Google’s Vertex Search and Conversation](https://www.tonic.ai/blog/rag-evaluation-series-validating-the-rag-performance-of-the-openais-rag-assistant-vs-googles-vertex-search-and-conversation)
    (2024) Tonic.ai.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] A. Kamor. [RAG评估系列：验证OpenAI的RAG助手与谷歌的Vertex搜索与对话性能](https://www.tonic.ai/blog/rag-evaluation-series-validating-the-rag-performance-of-the-openais-rag-assistant-vs-googles-vertex-search-and-conversation)（2024）Tonic.ai。'
- en: '[22] Pearl, J., Glymour, M., & Jewell, N. Causal inference in statistics: A
    primer. (2016). New York: Wiley.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] Pearl, J., Glymour, M., & Jewell, N. 《统计学中的因果推断：入门》（2016）。纽约：Wiley出版社。'
- en: '[23] I. M. Brandão, [Building Open Source LLM based Chatbots using Llama Index](https://medium.com/poatek/building-open-source-llm-based-chatbots-using-llama-index-e6de9999ee76)
    (2023) Medium.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[23] I. M. Brandão. [使用Llama Index构建基于开源LLM的聊天机器人](https://medium.com/poatek/building-open-source-llm-based-chatbots-using-llama-index-e6de9999ee76)（2023）Medium。'
- en: '[24] G. Lakoff and M. Johnson. Philosophy in the Flesh: The Embodied Mind And
    Its Challenge To Western Thought (1999) Basic Books.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[24] G. Lakoff和M. Johnson. 《肉体中的哲学：具身心灵与它对西方思想的挑战》（1999）Basic Books出版社。'
