- en: Using Generative AI To Get Insights From Disorderly Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用生成式AI从杂乱数据中获取见解
- en: 原文：[https://towardsdatascience.com/using-generative-ai-to-get-insights-from-disorderly-data-af056e5910eb?source=collection_archive---------0-----------------------#2024-09-03](https://towardsdatascience.com/using-generative-ai-to-get-insights-from-disorderly-data-af056e5910eb?source=collection_archive---------0-----------------------#2024-09-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/using-generative-ai-to-get-insights-from-disorderly-data-af056e5910eb?source=collection_archive---------0-----------------------#2024-09-03](https://towardsdatascience.com/using-generative-ai-to-get-insights-from-disorderly-data-af056e5910eb?source=collection_archive---------0-----------------------#2024-09-03)
- en: Best practices for using Large Language Models to extract actionable insights
    even with poor metadata
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用大型语言模型提取可操作的见解，即使元数据不准确。
- en: '[](https://medium.com/@oansari?source=post_page---byline--af056e5910eb--------------------------------)[![Omer
    Ansari](../Images/4e1ff96eb856567b55f8e7ef7e0278a1.png)](https://medium.com/@oansari?source=post_page---byline--af056e5910eb--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--af056e5910eb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--af056e5910eb--------------------------------)
    [Omer Ansari](https://medium.com/@oansari?source=post_page---byline--af056e5910eb--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@oansari?source=post_page---byline--af056e5910eb--------------------------------)[![Omer
    Ansari](../Images/4e1ff96eb856567b55f8e7ef7e0278a1.png)](https://medium.com/@oansari?source=post_page---byline--af056e5910eb--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--af056e5910eb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--af056e5910eb--------------------------------)
    [Omer Ansari](https://medium.com/@oansari?source=post_page---byline--af056e5910eb--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--af056e5910eb--------------------------------)
    ·32 min read·Sep 3, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--af056e5910eb--------------------------------)
    ·32分钟阅读·2024年9月3日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4a79fc1bee50dacb05b4ccaac1c06b99.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4a79fc1bee50dacb05b4ccaac1c06b99.png)'
- en: Best Practices checklist for analyzing messy data using genAI
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 使用生成式AI分析杂乱数据的最佳实践清单
- en: This article shares some best practices on how we have used generative AI to
    analyze our data at my firm to more effectively run operations. It took a while
    but I was able to get approval from Marketing, Legal, Security, and PR teams at
    Salesforce to publish this article. I hope this helps you turbo-charge your data
    analysis as well.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文分享了我们如何在公司使用生成式AI分析数据以更有效地运营的最佳实践。虽然花了一些时间，但我最终获得了Salesforce的营销、法律、安全和公关团队的批准，才能发布这篇文章。希望这能帮助你加速你的数据分析。
- en: '*All diagrams and figures in this article are directional and accurate to convey
    the concepts, but the data has been anonymized.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*本文中的所有图表和图形都具有方向性并准确传达概念，但数据已被匿名化。*'
- en: Insights-in-a-box
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 见解盒子
- en: '**Data Filtering with LLMs:** No need to clean data at the source; use an LLM
    to purify data mid-stream.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用LLM进行数据过滤：** 无需在源头清理数据；可以使用LLM在数据流中进行清洗。'
- en: '**Python Automation with GPT:** Intermediate Python skills are typically needed
    for data extraction, modification, and visualization, but GPT can automate and
    speed up these tasks'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用GPT进行Python自动化：** 提取、修改和可视化数据通常需要中级的Python技能，但GPT可以自动化并加速这些任务。'
- en: '**Domain-Specific Ticket Filtering:** When metadata is unreliable, filter tickets
    by the support engineers who worked on them.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**领域特定的工单过滤：** 当元数据不可靠时，通过处理这些工单的支持工程师来过滤工单。'
- en: '**Reliable Data Extraction:** Focus on extracting reliable fields like descriptions
    and timestamps, as these are less prone to errors.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可靠的数据提取：** 专注于提取像描述和时间戳这样的可靠字段，因为这些字段不容易出错。'
- en: '**Data Anonymization with GPT:** Use GPT with open-source anonymizer libraries
    to anonymize data before sending it to public APIs.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用GPT进行数据匿名化：** 在将数据发送到公共API之前，使用GPT与开源匿名化库对数据进行匿名化。'
- en: '**Choosing Delimiters Carefully:** Select output delimiters thoughtfully, ensuring
    they don’t interfere with language model processing, and sanitize input data by
    removing the chosen delimiter.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精心选择分隔符：** 选择输出分隔符时要小心，确保它们不会干扰语言模型的处理，并通过去除所选的分隔符来清理输入数据。'
- en: '**Fine-Tuning GPT prompts for Accuracy:** Evaluate and fine-tune the prompt’s
    performance on known ticket descriptions before full analysis.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微调GPT提示以提高准确性：** 在进行全面分析之前，先评估并微调提示在已知工单描述上的表现。'
- en: '**Contextual Data Limits:** Be aware of GPT’s upper processing limits for contextually
    unrelated data chunks; stay 10% below the identified limit to avoid data loss.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文数据限制：** 需要注意 GPT 在处理无关数据块时的上限；保持在识别的限制下方 10% 以内，以避免数据丢失。'
- en: '**Brainstorming KPIs with GPT:** After extracting metadata, use GPT to brainstorm
    and create meaningful KPIs for visualization.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与 GPT 头脑风暴 KPI：** 在提取元数据后，使用 GPT 进行头脑风暴并创建有意义的 KPI 以供可视化。'
- en: '**Streamlined Data Visualization:** Utilize GPT to write Python code to create
    graphs, keeping analysis streamlined and version-controlled within one environment
    instead of using a separate visualization tool.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简化数据可视化：** 利用 GPT 编写 Python 代码来创建图表，将分析过程简化并保持版本控制，在一个环境中完成，而不是使用单独的可视化工具。'
- en: Summary
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Have you ever faced off against large volumes of unkempt and free form data
    entered by human beings and tried to make sense of it? It is an extremely brain-numbing
    and time consuming job, and unless you have dedicated time to pour over it, chances
    are that you have ended up just sampling the data, and walked away with surface
    insights likely using untrustworthy metadata. Typically not great mileage.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否曾经面对大量由人类输入的凌乱和自由格式数据，试图理清其中的含义？这是一项极其令人头痛且耗时的工作，除非你有专门的时间来仔细处理，否则很可能最终只是对数据进行抽样，得到一些表面的洞察，且这些洞察可能是基于不可靠的元数据。通常来说，效果不大。
- en: It is not hard to see how large language models, which specialize in making
    sense of chaotic data, can help here. This article walks best practices gleaned
    from such an implementation, covering a range of concepts such as the most efficient
    method of using GPT to help you clean the data, do the analysis, and create useful
    graphs, approaches on managing Personally Identifiable Information (PII), a production-hardened
    prompt design, working around GPT’s ‘prefrontal cortex’ bottleneck and more!
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 不难看出，专门用于理清混乱数据的大型语言模型（LLM）可以在这里提供帮助。本文分享了从这种实现中总结的最佳实践，涵盖了多种概念，如使用 GPT 清洗数据、进行分析并创建有用图表的最有效方法，管理个人可识别信息（PII）的方法、生产级提示设计、绕过
    GPT 的“前额皮质”瓶颈等！
- en: 'But before all that, I’ll start with sharing how this experience completely
    changed my own strongly-held opinion around Data Quality:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 但在此之前，我想先分享一下这个经历如何完全改变了我原本对数据质量的坚定看法：
- en: I used to believe that in order to improve data quality, you must fix it at
    the source, i.e. the Systems of Engagement. For example, I used to believe that
    for a Sales CRM, we must ensure Sales and Marketing teams are entering quality
    data and metadata in the beginning. Similarly for customer support, we have to
    ensure the Customer Support Engineers are selecting all the right metadata (ticket
    cause code, customer impact, etc) associated with the ticket at the inception,
    duration and closure of the ticket.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾经认为，为了提高数据质量，必须从源头解决问题，也就是从参与系统（Systems of Engagement）入手。例如，我曾认为，对于销售 CRM，我们必须确保销售和市场团队一开始就录入高质量的数据和元数据。同样，对于客户支持，我们必须确保客户支持工程师在工单的创建、持续和关闭过程中，选择所有正确的元数据（如工单原因代码、客户影响等）。
- en: After my recent experiences, these beliefs have been smashed to bits. You can
    absolutely have unruly data at the source, and with the right direction, Large
    Language Models (LLMs) can still make sense of it resulting in meaningful insights!
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在我最近的经历之后，这些信念已经被彻底打破。你完全可以在源头拥有杂乱无章的数据，只要有正确的引导，大型语言模型（LLMs）仍然能够理清数据，得出有意义的洞察！
- en: '***No need to clean data at source: Like a water filter, you just plug an LLM
    in the middle of the stream and purify it!***'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '***无需在源头清洗数据：就像水过滤器一样，你只需将一个 LLM 插入数据流中，进行净化！***'
- en: '![](../Images/e7da3944a3aa757e1835c3aa5724cf07.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7da3944a3aa757e1835c3aa5724cf07.png)'
- en: GPT can act like a water filter, taking in information with dirty metadata and
    purifying it so that insights can be derived from it
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 可以像水过滤器一样，接收包含脏元数据的信息并加以净化，从中提取出有意义的洞察。
- en: Longer term, having processes in place to populate accurate metadata at source
    definitely help, though keep in mind they are are time consuming coordinate and
    audit.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 从长远来看，在源头处填充准确的元数据确实有帮助，尽管要注意，这些过程是需要协调和审核的，且非常耗时。
- en: Operating principles
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作原则
- en: 'In order to conduct this analysis, I had two simple principles:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行这项分析，我遵循了两个简单的原则：
- en: 'Avoid disrupting my team’s current delivery: While it would have been easier
    for me to request someone in my team to do the analysis, it would have disrupted
    the team’s velocity on already ongoing projects. I had to figure out how to do
    all of the analysis myself, while doing my day job as a product development executive.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 避免打乱我团队的当前交付：虽然让我团队中的某个成员做这项分析会更容易，但那会打乱团队在其他正在进行项目上的进度。我必须在做产品开发主管的日常工作同时，自己搞定所有的分析工作。
- en: 'Use Generative AI for everything: Large Language Models are great in data manipulation
    and, specifically for this use case, extracting value out of messy data. They
    are also much better than I am in coding. It’s just easier to tell someone to
    do things and inspect, than to get in the zone and do the work. This way, you
    can make a dent even with part-time effort.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用生成式AI做一切：大语言模型在数据处理方面非常强大，特别是在这个使用场景下，它能够从杂乱的数据中提取价值。它们在编程方面也比我强得多。告诉别人做事并检查结果比进入状态亲自做要容易得多。这样，你即使是兼职工作，也能有所成就。
- en: Using GPT to write the analysis code too!
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 也用GPT来编写分析代码！
- en: '**Bottom-line up front:** *Getting data extracted, modified and visualized
    requires intermediate level Python coding, but now, GPT can do all that for you
    much faster, if not with higher quality. Use it!*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**结论：** *提取、修改和可视化数据需要中级的Python编程技能，但现在，GPT可以帮你更快地完成所有这些，甚至可以做到更高质量。快用吧！*'
- en: In the following picture, I illustrate all the various steps (in green font)
    for which code was needed to be written to transform the data and then call the
    GPT API to extract insights from the ticket details. The best part is that I didn’t
    have to write this code from scratch. I used GPT to actually write it for me!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图片中，我展示了所有需要编写代码来转换数据并调用GPT API以从工单详情中提取洞察的步骤（以绿色字体标出）。最棒的是，我不需要从零开始编写这些代码。我使用GPT帮我编写了它！
- en: '![](../Images/adeb7281b8e58274d3dda1229ac9eb41.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/adeb7281b8e58274d3dda1229ac9eb41.png)'
- en: '*All the steps involved for LLM-based ticket analysis*'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*基于LLM的工单分析的所有步骤*'
- en: How I did the coding
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我是如何进行编码的
- en: 'While I am reasonably decent with Python, using GPT to write code makes me
    at least 3x better. I used a very rudimentary method in writing code through GPT:
    I didn’t use it to execute any code. I just told GPT what the data looked like
    and asked it to write code for me. I asked GPT to liberally insert print statements
    to print out variables at different points in the code. Then I copied that code
    in a Jupyter Notebook on my laptop and executed it there. For example, my prompt
    would be something like:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我对Python有一定的了解，但使用GPT编写代码让我至少提高了3倍的效率。我通过GPT编写代码的方法非常简单：我并没有使用它来执行任何代码，而是直接告诉GPT数据的样子，让它帮我写代码。我还要求GPT在代码的不同位置自由地插入打印语句，打印出变量的值。然后我将代码复制到我笔记本上的Jupyter
    Notebook中并执行。例如，我的提示语句大致如下：
- en: '**Me:** *Here are all the files I will use in my analysis. I’ll enumerate them
    and call them by their number in the prompt.*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**我：** *这是我在分析中会使用的所有文件。我将列举它们，并在提示语中用编号来称呼它们。*'
- en: '*1\. “All Interacted Tickets.xlsx”*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*1\. “All Interacted Tickets.xlsx”*'
- en: '*2\. “Copy of Ticket Dump — Received from Ops.xlsx”*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*2\. “Copy of Ticket Dump — Received from Ops.xlsx”*'
- en: '*3\. “verifying_accurate_list_of_ops_people_supporting_my_space.xlsx”*'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*3\. “verifying_accurate_list_of_ops_people_supporting_my_space.xlsx”*'
- en: '*They are all in the ../data/ directory.*'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*它们都在../data/目录下。*'
- en: '*Write python code to pull in files 1, 2, and 3 into pandas dataframes. Ignore
    all worksheets in any file which have the word pivot in them but pull in the data
    for the rest. Name the dataframes in snake case using the text in each of the
    worksheet itself in each of the excel files….*'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*编写Python代码，将文件1、2和3导入到pandas数据框中。忽略所有包含“pivot”字样的工作表，但将其余数据提取出来。用每个Excel文件中工作表本身的文本命名数据框，使用蛇形命名法....*'
- en: 'Then, GPT would spit out code, such as:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，GPT会输出代码，例如：
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'I would then take this code and run it locally. If there was an error, I would
    paste the output (including the print statement output) into the *“same”* chat
    as it preserves memory, and it was ‘mostly’ able to fix my issues. However, in
    some cases, GPT would get stuck (which you can recognize when it keeps recommending
    the same solution to an issue), I would start to interrogate it with more questions:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我会将这些代码拿到本地运行。如果出现错误，我会把输出（包括打印语句的输出）粘贴到*“相同的”*聊天中，因为它保留了记忆，GPT‘大多数’情况下能够修复我的问题。不过，在某些情况下，GPT会卡住（你可以通过它不断推荐相同的解决方案来识别这一点），这时我会开始用更多问题来询问它：
- en: '**Me:** *what is df = df[1:] doing*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**我：** *df = df[1:] 是做什么的*'
- en: '**GPT:** *The line df = df[1:] is used to drop the first row of the dataframe,
    which is typically the header row when the Excel file is read….*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPT：** *df = df[1:] 用于删除数据框的第一行，通常当读取Excel文件时，这一行是表头……*'
- en: '**Me:** *df = df[1:] is incorrect, I don’t want you to drop the first row.
    this is in fact the row you want to use for the header of each dataframe.*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**我：** *df = df[1:] 是不正确的，我不希望你删除第一行。事实上，这一行应该作为每个数据框的表头。*'
- en: Therefore, if you develop code using GPT out-of-band like I did, a moderate
    knowledge of Python is useful to break through some code issues with GPT since
    it is pretty much blind to the context.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果像我一样使用GPT开发代码，并且是通过带外方式进行的，那么对Python的适度了解有助于解决一些GPT无法理解的代码问题，因为它几乎无法理解上下文。
- en: '*Note that if you use multi-agent frameworks, there is a chance that the agents
    would bounce the code off of each other and resolve these defects automatically.
    In a future post, I will be showing my local environment setup for data engineering
    and analytics which shows how to set up this multi-agent framework on your laptop.
    Please let me know in the comments if this would be of interest.*'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*请注意，如果你使用多代理框架，代理之间可能会相互传递代码并自动解决这些缺陷。在未来的文章中，我将展示我的本地数据工程和分析环境设置，展示如何在你的笔记本电脑上设置这个多代理框架。如果你对此感兴趣，请在评论中告诉我。*'
- en: Step by Step approach on operational ticket analysis
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运营票据分析的逐步方法
- en: I came up with the following steps after several iterations and ‘missteps’!
    In other words, if I had to redo this analysis all over again, I would follow
    the following structure to streamline the process. So, I present this to you so
    you can reap the benefits. You’re welcome!
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在经过多次迭代和“失误”后，我提出了以下步骤！换句话说，如果我需要重新进行这项分析，我会遵循以下结构来简化过程。因此，我将这个结构呈现给你，以便你能从中受益。别客气！
- en: 'Step 1: Filter out relevant tickets'
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一步：筛选相关票据
- en: '**Bottom-line up front:** *If metadata is unreliable, then filtering tickets
    related to your domain based on the support engineers who worked them is your
    best option.*'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**核心观点：** *如果元数据不可靠，那么根据处理过票据的支持工程师筛选与你领域相关的票据是最好的选择。*'
- en: '![](../Images/7f3078578aa1e40a041c4f224d7add9b.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7f3078578aa1e40a041c4f224d7add9b.png)'
- en: '*Filter out tickets for your team*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*为你的团队筛选票据*'
- en: (You only need this step if you work in a medium to large organization and are
    one of many teams which leverage a shared operations team)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: （只有在你在中型或大型组织工作，并且是多个团队之一，使用共享运营团队时，才需要此步骤）
- en: Reducing the working set of tickets to what is pertinent to just your department
    or team is an important filtering step that must be taken when you have a significant
    number of operational tickets being worked on in your firm. You will be sending
    these tickets through LLMs, and if you’re using a paid service like GPT4, you
    want to only be sending what is relevant to you!
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 将票据的工作集减少到仅与你的部门或团队相关的部分，是在你的公司中有大量运营票据时必须采取的重要筛选步骤。你将通过LLMs发送这些票据，如果你使用的是像GPT4这样的付费服务，你只想发送与你相关的部分！
- en: However, deducing the working set of tickets is a problem when you have poor
    metadata. The support engineers may not have been instructed to mark which teams
    the tickets belonged to, or did not have good ticket categories to select from,
    so all you have to work with is some free form data and some basic “facts” that
    automatically got collected for these tickets. These facts range from who created
    the ticket, who owned it, timestamps associated with ticket creation, state change
    (if you’re lucky) , and ticket closure. There is other “subjective” data that
    likely exists as well, such as ticket priority. It’s fine to collect it, but these
    can be inaccurate as ticket creators tend to make everything they open as “urgent”
    or “high priority”. In my experience deriving the actual priority through LLMs
    is often more neutral thought it that still can be error-prone, as covered later.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当你拥有不良元数据时，推导出有效的票据工作集是一个问题。支持工程师可能没有被指示标明这些票据属于哪些团队，或者没有很好的票据分类供选择，所以你所能使用的仅是一些自由格式的数据和一些自动收集的基本“事实”。这些事实包括谁创建了票据，谁负责处理，票据创建时的时间戳，状态变化（如果幸运的话），以及票据关闭。可能还有其他“主观”数据，例如票据的优先级。收集这些信息是可以的，但它们可能不准确，因为票据创建者往往会将自己打开的每个票据标记为“紧急”或“高优先级”。根据我的经验，通过LLMs推导出实际优先级往往更为中立，尽管这仍然可能出错，后面会进一步讲解。
- en: '**So, in other words, stick to the “facts”.**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**换句话说，坚持“事实”。**'
- en: Amongst the “facts” that typically help you reduce the working set are the names
    of the support engineers that created and/or worked the ticket. Since support
    engineers also specialize in specific domains (data technologies vs CRM vs workday
    etc) the first step to take is to work with the support managers and identify
    the names of all the support engineers who work on the tickets associated in your
    domain.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在通常有助于你减少工作集的“事实”中，支持工程师的姓名通常是其中之一，支持工程师创建或处理工单的情况也包含在内。由于支持工程师通常专注于特定领域（如数据技术、CRM、Workday
    等），第一步是与支持经理合作，识别所有与您领域相关的工单的支持工程师姓名。
- en: Then, using an identifiable key which, such as their work email address, you
    can filter the morass of tickets down to the subset germane to your department
    and pull down the “fact” metadata associated with those tickets.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用一个可以识别的键（例如他们的工作邮箱地址），你可以将一堆工单筛选成与你部门相关的子集，并拉取与这些工单相关的“事实”元数据。
- en: 'Completing this step also gives you your first statistic: How many tickets
    are getting opened for my space over a period of time.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这一步骤后，你还会得到第一个统计数据：在一段时间内，我所在领域的工单被打开的数量。
- en: 'Step 2: Extracting the “description” field and other metadata'
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 2 步：提取“描述”字段及其他元数据
- en: '**Bottom-line up front:** *While a ticket creator can get much metadata wrong,
    she can’t afford to mess up the description field because that is the one way
    she can communicate to the support team her issue and its business impact. This
    is perfect, as making sense of free flow data is GPT’s specialty. Therefore, focus
    on extracting the description field and other factual “hard to mess up data” like
    ticket start and end time etc.*'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键要点：** *虽然工单创建者可能会搞错很多元数据，但她不能搞错描述字段，因为这是她与支持团队沟通问题及其业务影响的唯一途径。这一点非常重要，因为理解自由流动数据正是
    GPT 的强项。因此，专注于提取描述字段以及其他“难以出错”的事实数据，如工单的开始和结束时间等。*'
- en: '![](../Images/025260409e2575cca5cc8790ed05b061.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/025260409e2575cca5cc8790ed05b061.png)'
- en: '*Enrich the filtered tickets with metadata, especially the Description field*'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*通过元数据丰富筛选过的工单，特别是描述字段*'
- en: Most ticketing systems like Jira Service Management, Zendesk, Service Now etc
    allow you to download ticket metadata, including the long, multi-line description
    field. (I wasn’t as lucky with the homegrown system we use at my work). However,
    almost all of them have a maximum number of tickets that can be downloaded at
    one time. A more automated way, and the route I took, was to extract this data
    using an API. In this case, you need to have the curated set of tickets that were
    worked on by the support engineers supporting your teams from Step1, and then
    loop over each ticket, calling the API to pull down its metadata.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数工单系统，如 Jira 服务管理、Zendesk、Service Now 等，允许你下载工单元数据，包括长的、多行的描述字段。（在我工作的公司，我们使用的定制系统不太幸运。）然而，几乎所有这些系统都有一个一次可以下载的最大工单数量。一个更自动化的方式，也是我采取的方式，是通过
    API 提取这些数据。在这种情况下，你需要获取 Step1 中由支持工程师处理的已整理好的工单集合，然后循环遍历每个工单，调用 API 来获取其元数据。
- en: 'Some other systems allow you to issue SQL (or SOQL in case of Salesforce products)
    queries through an ODBC-like interface which is cool because you can combine step
    1 and step 2 together in one go using the WHERE clause. Here’s an example pseudo-code:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 其他一些系统允许你通过类似 ODBC 的接口发出 SQL（或者 Salesforce 产品中的 SOQL）查询，这很酷，因为你可以使用 WHERE 子句将第
    1 步和第 2 步合并为一步。以下是一个伪代码示例：
- en: '[PRE1]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You get the idea…
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 你明白了……
- en: Save this data in MS-Excel format and store it on disk.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些数据保存为 MS-Excel 格式，并存储在磁盘上。
- en: '***Why MS-Excel****? I like to “serialize” tabular data into MS-Excel format
    as that removes any issues with escaping or recurring delimiters when pulling
    this data into Python code. The Excel format encodes each data point into its
    own “cell” and there are no parsing errors and no column misalignment due to special
    characters / delimiters buried inside text. Further, when pulling this data into
    Python, I can use Pandas (a popular tabular data manipulation library) to pull
    the Excel data into a dataframe using its simple excel import option*'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '***为什么选择 MS-Excel****？我喜欢将表格数据“序列化”成 MS-Excel 格式，因为这样可以避免在将数据导入 Python 代码时遇到的转义或重复分隔符问题。Excel
    格式将每个数据点编码到自己的“单元格”中，这样就不会因为文本中包含的特殊字符/分隔符而导致解析错误或列错位。此外，当将这些数据导入 Python 时，我可以使用
    Pandas（一种流行的表格数据处理库）通过简单的 Excel 导入选项将 Excel 数据导入到数据框中。*'
- en: 'Step 3: Converting data into a GPT-friendly format (JSON)'
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 3 步：将数据转换为 GPT 友好的格式（JSON）
- en: '**Bottom-line up front:** *JSON is human readable, machine readable, error-safe,
    easily troubleshot, and easily manipulated with the least error by GPT. Further,
    as you enrich your data you can keep hydrating the same JSON structure with new
    fields. It’s beautiful!*'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结：** *JSON 既易于人类阅读，又便于机器解析，安全无误，易于排查问题，而且 GPT 可以最少出错地进行操作。此外，当你丰富数据时，你可以继续为相同的
    JSON 结构添加新字段。这是非常美妙的！*'
- en: '[PRE2]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '*The above snippet shows a sample JSON-ified ticket metadata with ticket number
    as key, pointing to an object containing further key/value metadata. There would
    be lots of these types of JSON blocks in the file, one for each ticket.*'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*上面的片段展示了一个示例的 JSON 格式化票务元数据，其中票号作为键，指向一个包含进一步键值对元数据的对象。文件中会有很多这种类型的 JSON 块，每个票据一个。*'
- en: After some hit and trial iterations, I realized the most efficient way for GPT
    to write data processing code for me was to convert my data into a json format
    and share this format with GPT to operate on. There is nothing wrong with shoving
    this data into a pandas data frame, and it may even be easier to do that step
    to efficiently process, clean and transform this data. The big reason why I have
    landed on eventually converting the final data set into JSON is because sending
    tabular data into a GPT prompt is kludgy. It is hard to read for humans and also
    introduces errors for the LLM as explained below.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 经过多次试验和调整，我意识到，让 GPT 为我编写数据处理代码的最有效方法是将数据转换为 JSON 格式，并与 GPT 分享这种格式进行操作。将这些数据塞入
    pandas 数据框并没有错，甚至可能更容易做到这一点，以高效地处理、清理和转换数据。我最终决定将最终数据集转换为 JSON 的一个重要原因是，因为将表格数据传递给
    GPT 提示符是非常繁琐的。对于人类来说，它难以阅读；同时也会为大语言模型（LLM）引入错误，正如下面所述。
- en: When you’re introducing tables into a prompt, it has to be done through a comma-separated-value
    (csv) format. There are two problems with that
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将表格引入提示符时，必须通过逗号分隔值（CSV）格式来完成。这有两个问题：
- en: 'Since there can be commas inside the text as well, you have to further escape
    those commas, by putting the text inside double quotes (for example, “text one”,
    “text, two”, “test \“hi!\”” . That introduces another problem:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于文本中也可能包含逗号，因此你必须通过将文本放入双引号中来进一步转义这些逗号（例如，“text one”，“text, two”，“test \“hi!\””）。这引入了另一个问题：
- en: what if you have double quotes (“) inside that text block. Now you have to further
    escape those double quotes. Matching separating these values into separate columns
    invariably brings issues.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果文本块中包含双引号（“），那么你就必须进一步转义这些双引号。将这些值分隔成单独的列往往会引发问题。
- en: 'And yes, while you have to escape double within JSON too (eg “key”: “value
    has \”quotes\””) , there are absolutely no issues in aligning this value to a
    column since the “key” uniquely identifies that. The column alignment can go off
    in some edge cases in a csv format, and then it becomes very hard to troubleshoot
    what went wrong.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '是的，尽管在 JSON 中也需要转义双引号（例如 “key”: “value has \”quotes\””），但是将这个值对齐到列中完全没有问题，因为“key”是唯一标识符。CSV
    格式中列对齐有时会出现问题，届时很难排查出错的原因。'
- en: Another reason for using JSON is that you can cleanly see and differentiate
    when you augment your metadata through GPT in future steps; it just adds more
    key value values horizontally down. You could do that in a table too, but that
    mostly requires a scroll towards the right in your IDE or notebook.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 JSON 的另一个原因是，你可以清晰地看到并区分当你通过 GPT 在未来的步骤中增强元数据时；它只是将更多的键值对水平添加。你也可以在表格中做到这一点，但这通常需要在你的
    IDE 或笔记本中向右滚动。
- en: '***Pro-tip:*** *In a future step, you will be sending this data into GPT, and
    will ask it to return multiple fields separated by a delimiter, such as “|”. Therefore,
    this is a good time to remove any occurrence of this delimiter from the free-form
    field that you are passing into the JSON format. You don’t want to risk GPT sending
    “|” out in the field itself*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '***小贴士：*** *在未来的某个步骤中，你将把这些数据传入 GPT，并要求它返回由分隔符（例如“|”）分隔的多个字段。因此，现在是删除你传入 JSON
    格式的自由格式字段中任何此类分隔符的好时机。你不希望 GPT 在字段中传送“|”字符。*'
- en: 'Step 4: Enhancing data using simple techniques (aka Basic Feature Engineering)'
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 4：使用简单技巧增强数据（即基本特征工程）
- en: '**Bottom-line up front:** *Simple mathematical analysis like, time deltas,
    averages, standard deviations can easily, and more cheaply, be done using basic
    coding, so get GPT to write code to do that and run that code locally, instead
    of sending GPT the data to do the math for you. Language models have been shown
    to make mathematical mistakes, so best to use them for what they’re good for.*'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结：** *简单的数学分析，如时间差、平均值、标准差，可以使用基础编码轻松且更便宜地完成，因此让 GPT 编写代码来完成这些任务，并在本地运行该代码，而不是将数据发送给
    GPT 进行计算。语言模型已经被证明容易犯数学错误，所以最好把它们用于它们擅长的事情。*'
- en: First, we can enhance the ticket metadata by aggregating some of the basic information
    in it. This is a pre-step which is better done with some simple code instead of
    burning GPT credits for it.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以通过聚合其中一些基本信息来增强工单的元数据。这是一个前期步骤，最好用一些简单的代码来完成，而不是浪费 GPT 的积分来处理。
- en: In this case, we calculate the ticket duration by subtracting CreatedTime from
    ClosedTime.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们通过将 CreatedTime 从 ClosedTime 中减去来计算工单持续时间。
- en: '![](../Images/aeb4719e96d239cba4db56479bca4677.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aeb4719e96d239cba4db56479bca4677.png)'
- en: '*left to right a JSON showing as getting hydrated through basic data aggregation/enhancement*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*从左到右，JSON 显示通过基本数据聚合/增强进行水化处理*'
- en: 'Step 6: The Main Entree: GPT-driven data enhancement (enhanced Feature Engineering)'
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 6 步：主菜：GPT 驱动的数据增强（增强的特征工程）
- en: Now we come to the main entree. How to use GPT to transform raw data and derive
    sophisticated and structured metadata from which insights can be extracted. In
    the world of data science, this step is called Feature Engineering.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们进入主菜部分。如何使用 GPT 转换原始数据，并从中派生出复杂且结构化的元数据，从这些元数据中可以提取出洞察。在数据科学领域，这一步叫做特征工程。
- en: '6.1: Pre-processing: Obfuscate sensitive information (optional)'
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1：预处理：模糊化敏感信息（可选）
- en: '**Bottom-line up front:** *Get GPT to use open source anonymizer libraries
    and develop code to anonymize the data before you send it to a public API service.*'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结：** *让 GPT 使用开源匿名化库并开发代码，在将数据发送到公共 API 服务之前进行匿名化处理。*'
- en: '![](../Images/ac37bfa07b5d49b68c3675ad9d33a925.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac37bfa07b5d49b68c3675ad9d33a925.png)'
- en: Photo by [Kyle Glenn](https://unsplash.com/@kylejglenn?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Kyle Glenn](https://unsplash.com/@kylejglenn?utm_source=medium&utm_medium=referral)
    通过 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: This step applies to you in case you are using openAI and not a local open source
    LLM where the data stays on your laptop. In a future post, I will be showing my
    local environment setup for data engineering and analytics which shows an open-source
    LLM option.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步适用于你在使用 OpenAI 而不是本地开源 LLM 的情况，因为数据会保留在你的笔记本电脑上。在未来的文章中，我会展示我的数据工程和分析本地环境设置，其中展示了一种开源
    LLM 选项。
- en: In the firm I work in, we have a safe proxy gateway both to openAI as well as
    internally trained LLMs,and it can mask Privately Identifiable Information (PII)
    and operates the Open AI within a Trusted boundary. This is convenient because
    I can send all internal information to this proxy and enjoy the benefits of openAI
    cutting models in a safe way.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在我工作的公司，我们有一个安全的代理网关，既可以连接 OpenAI，也可以连接内部训练的 LLM，并且它能够掩盖个人身份信息（PII），并在可信边界内操作
    OpenAI。这很方便，因为我可以将所有内部信息发送到这个代理，并以安全的方式享受 OpenAI 模型的优势。
- en: However, I realize not all companies are going to have this luxury. Therefore,
    I’m adding an optional step here to obfuscate personally identifiable information
    (PII) or other sensitive data. The beautiful part of all this is that GPT knows
    about these libraries and can be used to write the code which obfuscates the data
    too!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我意识到并非所有公司都有这个奢侈的条件。因此，我在这里增加了一个可选步骤来模糊化个人身份信息（PII）或其他敏感数据。所有这一切的美妙之处在于，GPT
    知道这些库并可以用来编写代码，来模糊化数据！
- en: I evaluated five libraries for this purpose, but the critical feature I was
    looking for was the ability to convert sensitive information to anonymous data,
    and then be able to re-convert it back as well. I found only the following libraries
    which have this capability.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我评估了五个库用于此目的，但我寻找的关键特性是能够将敏感信息转换为匿名数据，然后还能将其重新转换回来。我只找到了以下具有此功能的库。
- en: Microsoft Presidio [[link](https://github.com/microsoft/presidio)] (uses the
    concept of entity mappings)
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Microsoft Presidio [[link](https://github.com/microsoft/presidio)]（使用实体映射的概念）
- en: Gretel synthetics [[link](https://github.com/gretelai/gretel-synthetics)] (uses
    the concept of “Tokenizer)
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gretel synthetics [[link](https://github.com/gretelai/gretel-synthetics)]（使用“分词器”概念）
- en: Out of these two, Presidio was my favorite. I continue to be impressed to see
    the amount of “high quality” open source contributions Microsoft has made over
    the last decade. This set of python libraries is no different. It has the capabilities
    of identifying PII type data out of the box, and to customize and specify other
    data which needs to be anonymized.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两者中，Presidio是我最喜欢的。我一直对微软在过去十年中做出的“高质量”开源贡献印象深刻。这套Python库也不例外。它具备开箱即用的能力，能够识别PII（个人身份信息）类型的数据，并可以定制和指定需要被匿名化的其他数据。
- en: 'Here’s an example:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个例子：
- en: 'original text:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 原始文本：
- en: '[PRE3]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Anonymized test:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 匿名化测试：
- en: '[PRE4]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This can be sent to GPT for analysis. When it returns the results, you run
    that through the mapping to de-anonymize it:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据可以发送给GPT进行分析。当它返回结果时，你可以通过映射进行去匿名化：
- en: '*Entity mappings*'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*实体映射*'
- en: '[PRE5]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Using Entity mappings the text can be de-anonymized:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 使用实体映射，文本可以被去匿名化：
- en: '*de-anonymized text:*'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*去匿名化的文本：*'
- en: '[PRE6]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: I recommend checking out this [notebook](https://github.com/microsoft/presidio/blob/main/docs/samples/python/pseudonomyzation.ipynb),
    which walks you on how to implement this approach.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我推荐查看这个[笔记本](https://github.com/microsoft/presidio/blob/main/docs/samples/python/pseudonomyzation.ipynb)，它会引导你如何实现这种方法。
- en: Note that apart from PII, other information that may need to be obfuscated is
    systems information (IP addresses, DNS names etc) and database details like (names,
    schemas etc)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，除了PII，可能需要模糊处理的其他信息包括系统信息（IP地址、DNS名称等）和数据库细节（如名称、模式等）
- en: Now that we have a mechanism to anonymize sensitive data, the next step was
    to create a high quality prompt to run on this data.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个机制来匿名化敏感数据，下一步是创建一个高质量的提示来处理这些数据。
- en: '6.2 Pre-processing: Sanitize the input data'
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 预处理：清理输入数据
- en: '**Bottom-line up front:** *Be thoughtful in choosing an output delimiter, as
    certain special characters hold “meaning” in language models. Then, you can feel
    secure in sanitizing the raw input by removing the delimiter you chose.*'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结：** *在选择输出分隔符时要考虑周到，因为某些特殊字符在语言模型中有“意义”。然后，你可以放心地通过去除你选择的分隔符来清理原始输入。*'
- en: '**Problem:** When asking a text based interface, like an LLM, to return tabular
    data, you have to tell it to output the data separated by delimiters (e.g. csv,
    or tsv format). Suppose you ask GPT to output the summarized data (aka “features”)
    in comma separated values. The challenge is that the input ticket data is raw
    and unpredictable, and someone could have used commas in their description. This
    technically should not have been a problem since GPT would have transformed this
    data and thrown out the commas coming into it, but there was still a risk that
    GPT could use part of the raw data (which included commas) in its output, say
    in the one-liner summary. The experienced data engineering folks have probably
    caught on to the problem by now. When your data values themselves contain the
    delimiter that is supposed to separate them, you can have all sorts of processing
    issues.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题：** 当请求基于文本的接口（如LLM）返回表格数据时，必须告诉它使用分隔符输出数据（例如csv或tsv格式）。假设你要求GPT输出汇总数据（即“特征”）并使用逗号分隔值。问题在于，输入的票据数据是原始且不可预测的，有人可能在描述中使用了逗号。从技术上讲，这不应该是问题，因为GPT会处理这些数据并去除进入的数据中的逗号，但仍然存在风险，GPT可能会在输出中使用部分原始数据（包括逗号），例如在简短的总结中。经验丰富的数据工程师们可能现在已经意识到这个问题。当你的数据值本身包含应该用来分隔它们的分隔符时，就会出现各种处理问题。'
- en: 'Some may ask: Why don’t you escape all these by encapsulating the value in
    double quotes. E.g.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 有人可能会问：为什么不通过将值用双引号括起来来转义这些字符呢？例如：
- en: '*“key” : “this, is the value, with all these characters !#@$| escaped” .*'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*“key” : “this, is the value, with all these characters !#@$| escaped” .*'
- en: Here’s the issue with that. The user could have input double quotes in their
    data too!
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个问题。用户可能也在他们的数据中输入了双引号！
- en: '*“key” : “this is a ”value” with double quotes, and it is a problem!”*'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '*“key” : “this is a ”value” with double quotes, and it is a problem!”*'
- en: Yes, there are ways in solving this issue too, like using multi line regular
    expressions, but they make your code complicated, and make it harder for GPT to
    fix defects. So the easiest way to handle this was to choose an output delimiter,
    which would have the least impact in losing data context if scrubbed from the
    input, and then scrub it out of the input data!
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，确实有解决这个问题的方法，比如使用多行正则表达式，但它们会让代码变得复杂，也使得GPT修复缺陷变得更加困难。因此，处理这个问题最简单的方法是选择一个输出分隔符，这样即使从输入中去除它，也不会对数据上下文产生太大影响，然后再将其从输入数据中去除！
- en: I also played around with delimiters that would sure shot not be in the input
    data like |%|, but I quickly realized that these ate up the output token limits
    fast, so this was out.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我还尝试了一些分隔符，这些分隔符在输入数据中肯定不会出现，如|%|，但我很快意识到这些分隔符会快速消耗输出的标记限制，因此最终放弃了这种方法。
- en: Here are a few delimiters I tested
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我测试过的一些分隔符
- en: '![](../Images/aca1199ad8e405e8afb806caefa6ca38.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aca1199ad8e405e8afb806caefa6ca38.png)'
- en: In the end, I ended up selecting the pipe “|” delimiter as this is not something
    most stakeholders used when expressing their issues in the ticket description.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我选择了管道“|”分隔符，因为这是大多数利益相关者在表述问题时不会在工单描述中使用的符号。
- en: After this, I got GPT to write some extra code to sanitize each ticket’s description
    by removing “|” from the text.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我让GPT编写了一些额外的代码，清理每个工单描述中的“|”符号。
- en: 6.3 — Prompt Performance tuning
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6.3 — 提示性能调优
- en: '**Bottom-line up front:** *Before running the GPT data analysis prompt, evaluate
    its performance against a set of ticket descriptions with known output, fine tune
    the prompt and iterate until you are getting the maximum performance scores.*'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**概述：** *在运行GPT数据分析提示之前，先评估其在已知输出的工单描述集上的表现，微调提示并反复迭代，直到获得最佳性能评分。*'
- en: '![](../Images/af718fc5daecca0a49290beed5fec94c.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af718fc5daecca0a49290beed5fec94c.png)'
- en: iteratively improving the prompt using measures
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 通过措施迭代改进提示
- en: '**Goal:** To have GPT read the ticket description written by the customer and
    just from that, derive the following metadata which can then be aggregated and
    visualized later:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**目标：** 让GPT读取客户编写的工单描述，并仅通过该描述提取以下元数据，之后可以对其进行汇总和可视化：'
- en: Descriptive title summarizing the issue
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 描述性标题概述问题
- en: Business Impact*
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 商业影响*
- en: Ticket Severity*
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 工单严重性*
- en: Ticket Complexity
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 工单复杂度
- en: Impacted stakeholder group
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 受影响的利益相关者群体
- en: Owning team
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所属团队
- en: Ticket category
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 工单类别
- en: '** based on impact and urgency if provided by customer*'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于客户提供的影响和紧急程度*'
- en: '**Approach:** The way I worked on sharpening the main prompt was to'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**方法：** 我通过以下方式优化了主提示：'
- en: sample a few control tickets,
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选取一些控制工单样本，
- en: manually classify each of them the same way I wanted GPT to do them (by Category,
    Complexity, Stakeholder (Customer) group etc),
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 手动对每个工单进行分类，按照我希望GPT完成的方式（按类别、复杂度、利益相关者（客户）群体等），
- en: run these control tickets through a designed prompt GPT,
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些控制工单输入设计好的GPT提示中，
- en: cross-compare GPT results against my own manual classification,
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将GPT的结果与我自己的手动分类进行交叉比较，
- en: score the performance of GPT’s classification against each dimension, and
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估GPT分类在各个维度上的表现，
- en: improve the GPT prompt based on whichever dimension scored lower in order to
    improve it
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据哪个维度得分较低，改进GPT提示以提升其效果
- en: 'This gave me important feedback which helped me sharpen my GPT prompt to get
    better and better scores against each dimension. For the final prompt, check out
    [Appendix: The GPT prompt to process ticket descriptions](https://docs.google.com/document/d/1-GEMcOa0OF3-rLsVP6F1ZgVfMYcEjsltmbGIIjxbEC4/edit#heading=h.4k28p1gr2x2r).'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这给了我重要的反馈，帮助我不断优化GPT提示，提升每个维度的得分。最终的提示，请参见[附录：处理工单描述的GPT提示](https://docs.google.com/document/d/1-GEMcOa0OF3-rLsVP6F1ZgVfMYcEjsltmbGIIjxbEC4/edit#heading=h.4k28p1gr2x2r)。
- en: '**Results:**'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**结果：**'
- en: 'Here are the details around this metadata derived from raw ticket description,
    and the overall performance scores after multiple iterations of fine-tuning the
    prompt:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是从原始工单描述中提取的元数据的详细信息，以及经过多次微调提示后整体性能评分：
- en: '![](../Images/ceb381d6fdc2ae2439dc700557c5238d.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ceb381d6fdc2ae2439dc700557c5238d.png)'
- en: LLM Performance on metadata creation
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: LLM在元数据创建上的表现
- en: 'Here’s my rationale on why certain dimensions scored low despite multiple turns:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我对某些维度得分较低的原因的推理，尽管经过了多次迭代：
- en: '**Complexity:** I did run into a challenge when scoring “Complexity” for each
    ticket, where GPT scored the complexity of a ticket much higher than it was, based
    on its description. When I told it to score more aggressively, the pendulum swung
    the other direction, and, like a dog trying to please its owner, it started to
    score complexity much lower, so it was unreliable. I suspect the out-of-the-box
    behavior of scoring complexity higher than it is supposed to be is because of
    the current state of the art GPT capabilities. I used GPT4, which is considered
    to be a smart high school student, so naturally a highschool student would score
    this complexity higher. I suspect that future versions of these frontier models
    would bring college level and then phD level abilities, and we would be able to
    more accurately measure the complexity of such tasks. Alternatively, to improve
    even GPT4 complexity scoring analysis, I could have used the “few-shot” learning
    technique here to give some examples of complexity which may have improved the
    performance score for this dimension.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂性：** 在为每个工单评分“复杂性”时，我遇到了一个挑战，GPT基于工单描述给出的复杂性评分远高于实际情况。当我要求它更加激进地评分时，评分开始向另一个方向倾斜，就像一只试图取悦主人的狗，它开始给复杂性评分过低，因此变得不可靠。我怀疑这种出厂默认的高评分行为是由于当前GPT技术的状态。我使用的是GPT-4，它被认为相当于一名聪明的高中生，因此高中生会给出较高的复杂性评分。我猜测未来这些前沿模型的版本会达到大学甚至博士水平，我们将能够更准确地衡量此类任务的复杂性。或者，为了改善GPT-4的复杂性评分分析，我本可以使用“少量示例”学习技术，提供一些复杂性的示例，这可能会提升该维度的表现评分。'
- en: '**Severity:** While I asked GPT to use the impact vs urgency matrix to score
    severity, GPT had to rely on whatever the stakeholder had provided in the ticket
    description, which could be misleading. We are all guilty of using words designed
    to provoke faster action, when we open internal tickets with IT. Further, the
    stakeholder didn’t even provide any impact detail in the ticket description in
    a non-trivial amount of cases, which lead GPT to select an erroneous severity
    as well.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**严重性：** 当我要求GPT使用影响与紧急矩阵来评分严重性时，GPT不得不依赖利益相关者在工单描述中提供的内容，而这些内容可能具有误导性。我们都曾在与IT部门开具内部工单时，使用一些旨在促使更快行动的词语。此外，利益相关者在工单描述中根本没有提供任何影响细节，且这种情况在不少情况下存在，这导致GPT选择了一个错误的严重性评分。'
- en: Despite some metadata dimensions scoring low, I was pleased with the overall
    output. GPT was scoring high in some critical metadata like title, and category,
    and I could run with that.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管某些元数据维度的得分较低，但我对整体输出感到满意。GPT在一些关键元数据（如标题和类别）上的得分很高，我可以根据这些继续进行。
- en: The prompt was in good shape, but I was about to run into an interesting GPT
    limitation, its “forgetfulness”.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 提示词本身没有问题，但我即将遇到一个有趣的GPT限制，它的“健忘症”。
- en: 6.4 — Figuring out the limits of GPTs forgetfulness
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 — 探索GPT健忘症的极限
- en: '**Bottom-line up front:** *When sending in contextually* **unrelated** *chunks
    of data (such as many ticket descriptions) into a GPT prompt, the upper processing
    limit can be much less than what you get by stuffing the maximum chunks allowed
    by input token limit. (In my case this upper limit ranged between 20 to 30). GPT
    was observed to consistently forget or ignore processing beyond this limit. Identify
    this through hit and trial, stick to a number 10% below that limit to avoid data
    loss.*'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结：** *当向GPT提示中发送上下文上* **无关** *的数据块（如多个工单描述）时，处理的上限可能远低于通过填充输入令牌限制所允许的最大数据块数量。（在我的案例中，这个上限在20到30之间）。观察到GPT在超过此限制后会
    consistently 忘记或忽略处理。可以通过反复试探来识别这个限制，保持在这个限制以下的10%范围内，以避免数据丢失。*'
- en: '![](../Images/4904abf7add8a68f0b719cd9d581f78d.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4904abf7add8a68f0b719cd9d581f78d.png)'
- en: Photo by [Pierre Bamin](https://unsplash.com/@bamin?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[照片由Pierre Bamin](https://unsplash.com/@bamin?utm_source=medium&utm_medium=referral)拍摄，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)'
- en: Humans can keep 5–7 unrelated things in our prefrontal cortex, and it turns
    out GPT can keep 30–40 unrelated things, no matter how big its context window.
    I was only really sending the ticket number and description. The rest of the data
    did not require any fancy inference.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 人类大脑可以在前额皮质中保持5到7个不相关的事物，而事实证明GPT可以保持30到40个不相关的事物，无论它的上下文窗口多大。我实际上只是发送了工单号和描述，其他数据并不需要复杂的推理。
- en: Since I had almost 3000 tickets for GPT to review, my original inclination was
    to try to maximize my round trip runs and “pack” as many case descriptions I could
    into each prompt. I came up with an elaborate methodology to identify average
    token size based on the number of words (as token is a sub-word, in the transformer
    architecture), and saw that I could fit around 130 case descriptions in each prompt.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我有将近3000个工单需要GPT审核，我最初的想法是尽量优化每次调用的数量，“打包”尽可能多的工单描述到每个提示中。我想出了一种详细的方法，根据单词数量来确定平均token大小（因为token是子词，基于变换器架构），发现每个提示中可以容纳大约130个工单描述。
- en: But then I started seeing a weird phenomena. No matter how many ticket descriptions
    I sent into GPT to process, it consistently only processed just the first 20 to
    30 tickets! GPT appeared to not have the capacity to handle more than this magic
    number.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 但随后我开始看到一个奇怪的现象。无论我发送多少个工单描述到GPT进行处理，它总是只处理前20到30个工单！GPT似乎无法处理超过这个神秘数字的内容。
- en: '![](../Images/a8e6ee9426c5b24d82e83bc9a321cd36.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8e6ee9426c5b24d82e83bc9a321cd36.png)'
- en: This made me change my strategy and I decided to decrease the ticket batch size
    to maximum 10–12 tickets for each API call, based on the word count for that chunk,
    a little below the 20–30 upper limit. While this approach certainly increased
    the number of calls, and therefore prolonged the time for the analysis, it ensured
    that no tickets got dropped for processing.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我改变了策略，我决定将每次API调用的工单批量大小减少到最多10到12个工单，根据该批量的字数，略低于20到30的上限。虽然这种方法确实增加了调用次数，因此延长了分析时间，但它确保了没有工单被遗漏处理。
- en: '[PRE7]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: When reviewing this with an AI architect in my firm, he did mention that this
    is a recently observed phenomena in GPT. The large input contexts only work well
    when you have contextually related data being fed in. It does break down when
    you are feeding disparate chunks of information into GPT and asking it to process
    completely unrelated pieces of data in one go. This is exactly what I observed.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在与我公司的一位AI架构师讨论时，他确实提到这是GPT中最近观察到的现象。当你输入的上下文数据是相关时，大量输入才能表现良好。但当你将不相关的零散信息输入到GPT中，并要求它一次性处理完全无关的数据时，系统会崩溃。正是我观察到的现象。
- en: With an optimal ticket batch size of 10–12 tickets identified and a performant
    prompt created, it was time to run all the batches through the prompt..
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定了最佳工单批量大小为10到12个，并创建了高效的提示后，是时候将所有批量通过提示进行处理了。
- en: 6.5 Show time! Running all the tickets through GPT
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5 展示时间！将所有工单通过GPT进行处理
- en: '**Bottom-line up front:** *GPT can analyze tickets in hours when the same amount
    can take weeks or months by humans. Also it’s extraordinarily cheaper, though
    there is an error rate associated with GPT.*'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '**结论提前告诉你：** *GPT可以在几小时内分析工单，而相同数量的工单可能需要数周或数月才能由人类完成。并且它便宜得多，尽管GPT有一定的错误率。*'
- en: 'I provided GPT with the JSON format to write me code which did the following:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我向GPT提供了JSON格式，让它为我编写执行以下操作的代码：
- en: Load the JSON data into a dictionary
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将JSON数据加载到字典中
- en: 'Iterate 10–12 tickets at a time, concatenating the GPT analysis prompt with
    these tickets into the FULL GPT prompt, separating each ticket/description tuple
    by ###'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次处理10到12个工单，将这些工单分析提示与它们一起合并成完整的GPT提示，使用###分隔每个工单/描述元组
- en: Sending the full prompt to the GPT API (For work, I called a safer internal
    wrapper of this same API that my firm has built, which has security and privacy
    embedded into it, but by using the obfuscator step earlier, you can just as safely
    use the external GPT API.)
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将完整的提示发送到GPT API（为了工作，我调用了我公司内部构建的、更安全的包装API，它内嵌了安全性和隐私保护功能，但如果之前使用过混淆步骤，使用外部GPT
    API同样是安全的。）
- en: Save the output, which came out as a pipe-separated format by concatenating
    that into a file on disk.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将输出保存为管道分隔格式，并将其连接到磁盘上的文件中。
- en: Running the de-anonymizer, if obfuscation was done earlier. (I didn’t need to
    write this step due to the internal GPT wrapper API my firm has built)
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行去匿名化工具，如果之前进行了混淆处理。（由于我公司内部已经构建了GPT包装API，我不需要写这一步骤）
- en: Convert the output into the original JSON file as well.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将输出也转换为原始的JSON文件。
- en: Save the JSON file on disk after the full run is completed*.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在完整运行结束后，将JSON文件保存到磁盘。
- en: Print some visible queues on how many tickets had been processed
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打印出一些可见的队列，显示处理了多少个工单
- en: Time some states for each API call around text processed, number of tickets,
    start and end time.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每次API调用的文本处理、工单数量、开始和结束时间进行计时。
- en: '**Why saving to disk after a good run is pragmatic:** *These are costly runs,
    from a time perspective more than a money perspective. So after a successful run
    is completed, it is wise to serialize (save) this data on this disk, so that future
    analysis can be run on saved data and this code block in the Jupyter notebook
    doesn’t have to be repeated. In fact, after a successful run, I commented out
    the whole code block within my notebook, so that if I ran the full notebook start
    to finish, it would just skip this expensive step again and instead load the JSON
    data from disk into memory and continue on with the analysis.*'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么在成功运行后保存到磁盘是务实的：** *这些运行非常昂贵，从时间的角度来看比从金钱的角度更为昂贵。因此，在成功运行完成后，明智之举是将数据序列化（保存）到磁盘上，这样以后就可以在保存的数据上进行分析，而无需再次执行这个代码块。事实上，在成功运行后，我在我的笔记本中注释掉了整个代码块，这样如果我从头到尾运行整个笔记本，它就会跳过这个昂贵的步骤，直接将JSON数据从磁盘加载到内存中，并继续进行分析。*'
- en: Here’s a sample output of the fully hydrated JSON. The blue entries were metadata
    that GPT extracted from the description field
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这是完整填充的JSON输出示例。蓝色条目是GPT从描述字段中提取的元数据
- en: '![](../Images/12724fc1e5db3af567ce6f798edb3710.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12724fc1e5db3af567ce6f798edb3710.png)'
- en: Structured metadata that GPT came up from the raw information hydrated back
    in JSON format
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: GPT从原始信息中提取并重新生成的结构化元数据，格式为JSON
- en: I ran about 3000 tickets through this cycle, and it completed in about 2.95
    hours. 👏
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我通过这个流程运行了大约3000个票据，并在大约2.95小时内完成了处理。👏
- en: '![](../Images/14959c841756e19355fd7e58c3c3ea63.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14959c841756e19355fd7e58c3c3ea63.png)'
- en: Photo by [Nagara Oyodo](https://unsplash.com/@nagaranbasaran?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Nagara Oyodo](https://unsplash.com/@nagaranbasaran?utm_source=medium&utm_medium=referral)
    来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'Just to give you a comparison point on how long this would have taken if I
    had employed human beings for this work: I had a similar experience to draw from
    2 years ago (aka the pre-GenAI era). I had inherited an Operations team and there
    was poor metadata in the tickets too. I needed to get situational awareness, so
    I had to sanction 2 of my senior engineers full time for a month to laboriously
    and meticulously go through 1500 ticket descriptions and classify them.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给你一个比较点，告诉你如果我雇佣人工进行这项工作大概要花多久时间：我有一个类似的经历，可以追溯到两年前（也就是GenAI之前的时代）。我接管了一个运营团队，票据中的元数据也很差。我需要获得情境意识，所以我必须全职派出两名资深工程师，一个月时间，仔细而艰难地处理1500个票据描述并进行分类。
- en: Contrast this with GPT. Even with the smaller batch size of 10-15 tickets per
    API call, GPT did 2800 tickets in 2.95 hours! Yes, while there was an error rate
    of 10% with GPT, it still analyzed **twice as more** tickets in 3 hours than 2
    human beings did in 1 month.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 与GPT进行对比。即使每次API调用的批次较小，每次只有10-15个票据，GPT仍在2.95小时内处理了2800个票据！是的，尽管GPT的错误率为10%，它在3小时内分析的票据数量仍然是两个人在1个月内的**两倍**。
- en: So the time savings is clearly insane, but what about the cost comparison?
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，节省的时间显然是惊人的，但成本比较如何呢？
- en: GPT4 vs human analysis cost comparison
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-4与人工分析成本比较
- en: I asked GPT to help me with the cost comparison. Note I compared human labor
    for 2 months to match the ticket count the GPT analyzed.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我请GPT帮助我进行成本比较。请注意，我将人力劳动的2个月成本与GPT分析的票据数量进行比较。
- en: '**Prompt:** how much does gpt4 cost if I use its API for 2.95 hours, compared
    to 2 senior IT software engineers one based out of the Midwest and another working
    from India, working full time on the same project for 2 months (excluding weekends).'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示：**如果我使用其API运行2.95小时，GPT-4的成本是多少？与两位资深IT软件工程师（分别来自美国中西部和印度，且全职工作2个月，周末除外）在同一项目上的工作时间成本相比，如何？'
- en: '**Salary Estimates:**'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '**薪资估算：**'
- en: 'Midwest (USA) Senior IT Software Engineer:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 美国中西部资深IT软件工程师：
- en: 'Average annual salary: $120,000 (this can vary depending on the exact location
    and company, but it’s typically lower than Silicon Valley).'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均年薪：$120,000（这可能会根据具体位置和公司有所不同，但通常低于硅谷的水平）。
- en: 'Monthly salary: $120,000 / 12 = $10,000.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 月薪：$120,000 / 12 = $10,000。
- en: 'India-based Senior IT Software Engineer:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 印度资深IT软件工程师：
- en: 'Average annual salary: $30,000 (this can vary, but it’s generally much lower
    than in the USA).'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均年薪：$30,000（这可能会有所不同，但通常远低于美国的水平）。
- en: 'Monthly salary: $30,000 / 12 = $2,500.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 月薪：$30,000 / 12 = $2,500。
- en: '**Cost Calculation for 2 months:**'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '**2个月的成本计算：**'
- en: 'Midwest Engineer: 2 month salary: $20,000.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中西部工程师：2个月薪水：$20,000。
- en: 'India-based Engineer: 2 month salary: $5,000.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 印度工程师：2个月薪水：$5,000。
- en: 'Total cost for both engineers: $25,000'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两位工程师的总成本：$25,000
- en: '**GPT-4 API cost for 2.95 hours**: ~$6.64 assuming GPT-4–8k pricing and the
    given token usage rate. (GPT4-o or GPT4-o mini would have been even more cheaper.)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPT-4 API费用（2.95小时）：** ~$6.64，假设GPT-4–8k定价和给定的token使用率。（GPT4-o或GPT4-o mini的费用会更便宜。）'
- en: Even if you added in the cost of the 20 hours I worked on this project over
    7 days, the overall cost comparison still comes out much better. And what’s more,
    this work is now reproducible.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你把我在这个项目上花的20小时的成本也算上，整体成本比较仍然要好得多。而且更重要的是，这项工作现在是可复制的。
- en: '***So basically, using $7 and 3 hours, GPT does the same analysis humans would
    have taken 1 month and cost $25,000 to complete***'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '***所以基本上，使用$7和3小时，GPT完成了与人类需要1个月和$25,000才能完成的相同分析***'
- en: 🎤 Mic drop!
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 🎤 麦克风掉落！
- en: '![](../Images/ad6f00a9f6688f5826dc72d7c74a0fda.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad6f00a9f6688f5826dc72d7c74a0fda.png)'
- en: Photo by [Andrew Gaines](https://unsplash.com/@shotbygaines?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Andrew Gaines](https://unsplash.com/@shotbygaines?utm_source=medium&utm_medium=referral)提供，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'Step 7 : Extracting insights from the GPT-derived metadata'
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第7步：从GPT派生的元数据中提取见解
- en: '**Bottom-line up front:** *Once you have extracted useful metadata using GPT,
    turn around and brainstorm with GPT what kind of KPIs you can graph out of it.*'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结：** *一旦你使用GPT提取了有用的元数据，转身与GPT进行头脑风暴，看看你能从中绘制出哪些KPIs。*'
- en: While there were already things I was curious to find out, I also brainstormed
    with GPT to give me more ideas. Again, using a JSON format was very handy, I just
    passed an anonymized sample for one ticket to GPT and asked it, *”Based on what
    you see over here, give me some ideas on what type of graphs I can plot to derive
    insights around my operations”*
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我已经有些好奇想要了解的内容，但我也和GPT进行了头脑风暴，要求它给我更多的想法。同样，使用JSON格式非常方便，我只需将一个票据的匿名样本传递给GPT，并问它，*“根据你看到的内容，给我一些关于我如何绘制图表以便从我的运营中获取见解的想法”*
- en: In the end here are the ideas that we both came up with. I took some, and ignored
    the others.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，这里是我们两个人想出来的思路。我采纳了一些，忽略了其他的。
- en: '![](../Images/8a9ccaee2d28fe3df5b62eba6bbd0fc2.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a9ccaee2d28fe3df5b62eba6bbd0fc2.png)'
- en: Brainstorming with GPT on what KPIs to visualize..
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 与GPT进行头脑风暴，讨论要可视化的KPIs……
- en: 'Step 8 : The visualization'
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第8步：可视化
- en: '**Bottom-line up front:** *Thanks to GPT you can write Python code to create
    graphs, instead of transforming data in Python and moving this data out to a visualization
    tool. This helps keep all your analysis streamlined, version-controlled and self
    contained in one place.*'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结：** *得益于GPT，你可以编写Python代码来创建图表，而不需要在Python中转换数据并将这些数据导入可视化工具。这有助于保持所有分析的简洁性、版本控制，并且将所有内容集中在一个地方。*'
- en: 'Historically, a typical pattern in Exploratory data analysis (EDA) is to extract,
    and transform the data in Python and then store it in a file or a database and
    then connect Tableau, Power BI , or Looker to this data to create graphs using
    it. While having long-living dashboards in these visualization products is absolutely
    the way to go, using these products for doing early-stage EDA can be a high friction
    process which introduces delays. It also becomes hard to manage and match different
    versions of the graphs with the different versions of the data transformations
    done. However, following this two-step pattern was a necessary evil historically
    for two reasons:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史上看，探索性数据分析（EDA）中的典型模式是：在Python中提取并转换数据，然后将其存储在文件或数据库中，再将Tableau、Power BI或Looker与这些数据连接，利用它们来创建图表。虽然在这些可视化产品中保持长期有效的仪表盘绝对是正确的做法，但将这些产品用于进行早期阶段的EDA可能是一个高摩擦过程，会导致延迟。而且，管理和匹配不同版本的图表与不同版本的数据转换变得困难。然而，历史上遵循这种两步模式是必要的恶习，原因有二：
- en: (Pull) These visualization tools are intuitive and have a drag and drop interface,
    meaning you can experiment and create graphs very fast.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （拉取）这些可视化工具直观且具有拖放界面，意味着你可以快速进行实验并创建图表。
- en: (Push) The de facto Python library for generating graphs is matplotlib. I don’t
    know about you, but I find matplotlib a very unfriendly library (unlike the intuitive
    ggplot library in R, which is a joy to use). Seaborn is better, but still, its
    more work than the visualization tools.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （推动）生成图表的事实标准Python库是matplotlib。我不知道你是怎么想的，但我发现matplotlib是一个非常不友好的库（与R中直观的ggplot库不同，ggplot使用起来非常愉快）。Seaborn稍好一些，但它仍然比可视化工具需要更多的工作。
- en: However, now that GPT can write all the matplotlib (or seaborn, or plotly,)
    code for you, there is less of a need to move your work to a visualization tool
    at the end. You can stay within the same Python Notebook from start to finish,
    and that’s exactly what I did!
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，现在 GPT 可以为你编写所有的 matplotlib（或 seaborn、或 plotly）代码，你就不必将工作转移到可视化工具中了。你可以从头到尾都停留在同一个
    Python Notebook 中，这正是我所做的！
- en: I did check Tableau to verify if some of the moderately complex aggregation
    logic was correctly being computed in Python (and in fact this helped me find
    a bug) but by and large, all the graphics I needed were built using scatter, bar
    , line , histogram and pie plots within Python.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我确实查看了 Tableau，以验证一些中等复杂度的聚合逻辑是否在 Python 中正确计算（事实上这帮我找到了一个 bug），但总体来说，我所需要的所有图形都是通过
    Python 中的散点图、条形图、折线图、直方图和饼图来构建的。
- en: Here are some examples of these graphs and tables. The text and numbers are
    of course anonymized but the intent here is to show you the kind of insights you
    can start extracting.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些这些图表和表格的示例。文本和数字当然已经匿名化，但这里的目的是展示你可以开始提取的那种洞察。
- en: '***The goal for any insight is to drive deeper questions and eventually take
    meaningful action grounded in data, which eventually results in creating value.***'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '***任何洞察的目标都是引发更深层次的问题，最终采取基于数据的有意义行动，进而创造价值。***'
- en: The insight is what gets you curious about why the system is behaving the way
    it is, so you can attempt to improve it.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 洞察力就是让你对系统的行为产生好奇，从而你可以尝试改进它。
- en: '![](../Images/3b78ee7b1787c70a828823463ed9fd3a.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3b78ee7b1787c70a828823463ed9fd3a.png)'
- en: How do the complexity of tickets exactly contribute to the time duration of
    that ticket and which groups of tickets to focus on to reduce their time duration
    and improve customer satisfaction.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 票据的复杂度如何准确地影响该票据的时长，以及应该聚焦哪些票据类别，以减少它们的时长并提高客户满意度。
- en: '![](../Images/3d310bc3496219a6fb55a123c3d0f816.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d310bc3496219a6fb55a123c3d0f816.png)'
- en: Identify If there is a relation with ticket duration and the support engineer
    working on it, so you can suss out behavioral or training issues
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 确定票据时长是否与处理该票据的支持工程师相关联，以便你可以找出行为或培训方面的问题。
- en: '![](../Images/b9ed493745f54112106775e817ac0d06.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b9ed493745f54112106775e817ac0d06.png)'
- en: Which engineering team receives the largest number of tickets and how is that
    trend progressing.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 哪个工程团队接收的票据数量最多，且这一趋势如何发展。
- en: Working on service requests (pure operations work) is a hidden cost to quantify
    since it needs to be deducted from an engineering team’s sprint velocity. In absence
    of this data, engineering teams typically allocate a ‘finger in the air’ % of
    their time to operations work, which we all know is blunt and varies from team
    to team. With this type of analysis you can more accurately carve capacity for
    such operations work while not compromising the team’s product commitments or
    burning the individuals out.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 处理服务请求（纯运维工作）是一个隐藏的成本，需要量化，因为它必须从工程团队的冲刺速度中扣除。在没有这些数据的情况下，工程团队通常会分配一部分时间来处理运维工作，这一比例通常是‘凭经验猜测’的，我们都知道这是笨拙的，并且因团队而异。通过这种分析，你可以更准确地为这些运维工作划分容量，同时不会妥协团队的产品承诺或让个人过度疲劳。
- en: '![](../Images/0ae3125d560c0e72596b89927b47ad90.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ae3125d560c0e72596b89927b47ad90.png)'
- en: What are the trends of these tickets by category? Do we see more timeliness
    issues vs accuracy problems?
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这些票据按类别的趋势是什么？我们是看到更多的及时性问题，还是准确性问题？
- en: '![](../Images/e81cba556a2f44af96f59fbf5107e277.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e81cba556a2f44af96f59fbf5107e277.png)'
- en: Which interfaces do our customers use to open the tickets the most so we can
    streamline and optimize those areas, perhaps inserting helpful articles for self-service.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的客户最常使用哪些接口来提交票据，以便我们能够精简和优化这些区域，也许可以插入一些有帮助的自助服务文章。
- en: '![](../Images/e584b62b59d366fe872b2c231094be9e.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e584b62b59d366fe872b2c231094be9e.png)'
- en: How many tickets are 1 day old, and are there any patterns in the ops personnel
    where some are cherry picking a lot of these simple cases than others. This can
    help with balancing resource management.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 有多少票据已经有1天的历史，是否存在某些运维人员选择处理大量简单案例的模式？这有助于平衡资源管理。
- en: '![](../Images/b5570ade930bda38719a96ed1984c9ce.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5570ade930bda38719a96ed1984c9ce.png)'
- en: How many tickets were truly low complexity issues such as data access or systems
    access, things for which automation and self-serve options can be put in place.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 有多少票据确实是低复杂度问题，比如数据访问或系统访问，这些问题可以通过自动化和自助服务选项来解决。
- en: Future enhancements
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 未来的改进
- en: '**Deeper Analysis using GPT’s Data Science capabilities:** This analysis, however
    very insightful, was just at a surface level just visualizing the data. There
    can be more sophisticated work that can be done by using linear or logistic regression,
    ANOVA for predictive analysis, or using clustering methods (like KNN) to tease
    out other patterns in the data.'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**利用 GPT 的数据科学能力进行更深层次的分析：** 然而，这个分析虽然非常有洞察力，但只是停留在表面，仅仅是数据的可视化。还可以做更复杂的工作，使用线性回归或逻辑回归、ANOVA
    进行预测分析，或使用聚类方法（如 KNN）来发掘数据中的其他模式。'
- en: '**Multi-Agent Framework to accelerate and improve quality:**'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**多智能体框架以加速和提高质量：**'
- en: I had to do a lot of rounds back and forth with GPT to write the actual code.
    While it was still significantly faster (7 days part time) than what would have
    taken me writing this from scratch (20-30 days full time, which means “never”!),
    I do think using LLM-backed AI agents which can critique each other’s output and
    come up with better ways. (This is something I’m actively experimenting with at
    work and initial experiments are VERY encouraging. Will write more on this in
    the future)
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我必须与 GPT 进行多轮反复的沟通才能编写实际代码。虽然这比我从头开始编写代码的时间（20-30 天全职工作，这意味着“永远不可能”！）要快得多（7
    天兼职），但我确实认为使用 LLM 支持的 AI 代理可以相互批评对方的输出，并找到更好的方法。（这是我目前在工作中积极实验的方向，初步实验非常令人鼓舞。以后会写更多相关内容）
- en: GPT was really blind when it came to recommending code. I copied code from it
    and ran it locally in my jupyter notebook. A better way would have been to have
    a MAF setup with an environment agent (perhaps powered by a [container](https://microsoft.github.io/autogen/docs/tutorial/code-executors)
    perfectly set up with my required libraries etc), and then the AI coder agent
    write the code, execute it , find defects, iterate and fix it. I imagine that
    would have shaved over 50% of my development time.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在推荐代码方面，GPT 真的很盲目。我从它那儿复制了代码并在我的 Jupyter Notebook 中运行。更好的方法是使用 MAF 设置一个环境代理（或许通过[容器](https://microsoft.github.io/autogen/docs/tutorial/code-executors)完美配置我的所需库等），然后让
    AI 编码代理编写代码、执行、发现缺陷、迭代并修复。我想，这样能节省我超过 50% 的开发时间。
- en: 'Breaking the analysis prompt up: While I ended up using the one mega prompt
    to run the analysis, if I were using some chained AI agent mechanism, I could
    have broken the analytics tasks out to different agents, powered them with different
    LLM endpoints, with different [temperature](https://www.hopsworks.ai/dictionary/llm-temperature)
    settings each. The lower the temperature the more precise and less creative the
    LLM is. For example, I learnt the hard way that with the default temperature setting,
    GPT ended up making minor changes to the categories for each ticket (like “Data
    Completeness” or “completeness”) which just ended up creating more post-processing
    clean-up annoying work for me.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分解分析提示：虽然我最终使用了一个大型提示来进行分析，但如果我使用一些链式 AI 代理机制，我本可以将分析任务分配给不同的代理，为它们提供不同的 LLM
    端点，并且每个代理使用不同的[温度](https://www.hopsworks.ai/dictionary/llm-temperature)设置。温度越低，LLM
    越精准，创造性越差。例如，我通过艰难的方式学到，如果使用默认的温度设置，GPT 会对每个票据的类别（如“数据完整性”或“完整性”）进行细微修改，结果导致了更多的后期处理清理工作，给我带来了困扰。
- en: Heck, I could have even gotten large chunks of this very document written for
    me by a creative AI agent in my multi-agent team!
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哎，我甚至可以让我的多智能体团队中的创意 AI 代理帮我写出这份文档的大部分内容！
- en: Closing thoughts from a Product Leader on Operational Tickets
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 来自产品负责人关于操作票据的总结想法
- en: Our customers experience our products through how they interact with them day
    to day. Through tickets, service requests they are constantly sending us signals
    on what’s working and what’s not working, forming an impression about us by seeing
    how receptive we are to these signals. Oftentimes though, we are fixated on product
    development, major transformational programs underway, tracking and monitoring
    the next flashy thing being built in the kitchen and ignore these operational
    signals at our peril. Sure, being responsive to major incidents is the job of
    every accountable leader, and good things emerge by working on action plans that
    come out through those Root Cause Analysis (RCA) calls. However, I would argue
    that there is a large quantity of moderate severity issues, and service requests
    that our customers are opening which often goes ignored just because of its sheer
    volume. And when, in earnest, you open this treasure trove of ticket data, it
    is often so overwhelming and uncurated that your head starts spinning! You risk
    walking away with a simplistic and incomplete mental model based on summary reports
    created by someone else.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的客户通过他们日常与我们产品的互动来体验我们的产品。通过工单和服务请求，他们不断向我们传递哪些工作正常、哪些工作不正常的信号，并通过观察我们如何响应这些信号形成对我们的印象。然而，我们常常把注意力集中在产品开发、正在进行的重大转型计划以及跟踪和监控下一个炫目的项目上，而忽视了这些运营信号，这可能会带来风险。当然，响应重大事件是每个负责领导者的工作，行动计划通常通过根本原因分析（RCA）会议得以形成并取得成果。然而，我认为有大量中等严重性的问题和服务请求，客户常常因为其数量庞大而被忽视。当您真诚地开始挖掘这些宝贵的工单数据时，它们往往会让人感到不知所措且未经过整理，您的大脑可能会开始晕旋！您有可能因此走向一个简化且不完整的思维模型，这个模型可能是由别人创建的总结报告所导致的。
- en: My philosophy is that, as a leader, you must create the time and capabilities
    to dig your fingers in the dirt. That is the only way you get a true feel of how
    your business operates. This used to be very hard to do before the GenAI era.
    Even leaders capable of doing data analysis couldn’t afford taking time away from
    their day job. Well, not anymore!
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我的理念是，作为领导者，您必须创造时间和能力，亲自深入实际操作。这是您真正了解自己业务如何运作的唯一方式。在genAI时代之前，这曾是非常困难的。即使是能够进行数据分析的领导者，也无法从日常工作中抽出时间来做这件事。而现在，情况不再如此！
- en: While this article attempts to give you some of the capabilities to jump start
    your genAI powered analysis journey into operational tickets, only you, my dear
    reader, can create the time and space to act on them. What’s more, I’m hopeful
    that some of the insights you’ve learnt in effectively using LLMs to turbocharge
    your analysis will be transferable in many other areas beyond operational ticket
    analysis.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本文试图为您提供一些能力，以便启动基于genAI的运营工单分析之旅，但只有您，我亲爱的读者，才能创造时间和空间来付诸实践。更重要的是，我希望您在有效使用LLMs来加速分析的过程中获得的某些见解，能在运营工单分析之外的许多其他领域得到应用。
- en: 'Appendix: A fine-tuned version of the GPT prompt'
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录：经过微调的GPT提示版本
- en: What follows below is a scrubbed out version of the most performant prompt I
    used to conduct the ticket analysis. I replaced our internal data quality dimensions
    with those published by Data Management association (DAMA). If your firm has a
    data quality policy, I encourage you to use those standards here.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我用来进行工单分析的最有效提示的清理版。我用数据管理协会（DAMA）发布的标准替代了我们内部的数据质量维度。如果您的公司有数据质量政策，我建议您在这里使用这些标准。
- en: '*Below are examples of cases along with their descriptions, each separated
    by ###. These are related to data-related technologies. Your task is to carefully
    review each case, extract the necessary 7 data points, and present the results
    in the specified format. Detailed instructions are as follows:*'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '*以下是一些案例及其描述，每个案例之间以###分隔。这些案例与数据相关技术相关。您的任务是仔细审查每个案例，提取必要的7个数据点，并以指定格式展示结果。详细说明如下：*'
- en: '***Title:*** *For each case, create a concise and descriptive title based on
    the content of the description, ensuring it is 300 characters or less.*'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***标题:*** *根据描述内容，为每个案例创建一个简洁且具描述性的标题，确保字符数不超过300个。*'
- en: '***Impact:*** *From the description, summarize the impact in a brief one-liner.
    If the impact isn’t directly stated or implied, simply write “Impact Not Provided.”*'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***影响:*** *从描述中简要总结影响，写成一句话。如果影响未直接陈述或暗示，则写“未提供影响”。*'
- en: '***Severity:*** *Assign a severity level to each case using an urgency vs impact
    matrix approach, considering both the urgency of the issue and its impact on the
    system:*'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***严重性:*** *使用紧急性与影响矩阵方法，为每个案例分配严重性级别，考虑问题的紧急性及其对系统的影响：*'
- en: '***S1:*** *High urgency and high impact, possibly causing system outages or
    making the application unusable.*'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***S1:*** *高紧急性和高影响，可能导致系统故障或使应用程序无法使用。*'
- en: '***S2:*** *High urgency but with a moderate impact or moderate urgency with
    a high impact, affecting multiple users.*'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***S2:*** *高紧急性但影响中等，或中等紧急性但影响较大，影响多个用户。*'
- en: '***S3:*** *Low urgency with a moderate or low impact, with minimal user disruption.*'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***S3:*** *低紧急性，影响中等或较低，用户中断最小。*'
- en: '***S4:*** *Low urgency and low impact, often related to general requests (Note:
    Access issues are not generally S4).*'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***S4:*** *低紧急性和低影响，通常与一般请求相关（注意：访问问题通常不属于S4）。*'
- en: '*Only one severity level should be assigned per case.*'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*每个案例应仅分配一个严重性级别。*'
- en: '*4\. Complexity: Assess the complexity of the case based on your expertise
    in the data field:*'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '*4\. 复杂性: 根据你在数据领域的专业知识评估案件的复杂性：*'
- en: '***High Complexity***'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***高复杂性***'
- en: '***Medium Complexity***'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***中等复杂性***'
- en: '***Low Complexity***'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***低复杂性***'
- en: '*Typically, access-related cases are low complexity, but use your judgment
    based on the description.*'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通常，访问相关的案例是低复杂性，但可以根据描述做出判断。*'
- en: '***5\. Line of Business (LOB):*** *Determine the relevant line of business
    based on the description. The options are:*'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '***5\. 业务线（LOB）:*** *根据描述确定相关的业务线。选项包括：*'
- en: '***Finance***'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***财务***'
- en: '***Marketing***'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***市场营销***'
- en: '***Sales***'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***销售***'
- en: '***Customer Support***'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***客户支持***'
- en: '***HR***'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***人力资源***'
- en: '***Miscellaneous:*** *If you can’t clearly identify the LOB.*'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***杂项:*** *如果无法明确识别业务线。*'
- en: '*Choose only one LOB per case. If multiple are mentioned, pick the most prominent.*'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*每个案例只选择一个业务线。如果提到多个，请选择最重要的。*'
- en: '***6\. Team:*** *Assign the appropriate team based on the description. The
    options are:*'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '***6\. 团队:*** *根据描述分配适当的团队。选项包括：*'
- en: '***CDI (Central Data Ingest):*** *Any case mentioning CDI or “Central Data
    Ingest team” should be classified under this team exclusively.*'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***CDI（中央数据摄取）:*** *任何提到CDI或“中央数据摄取团队”的案例应仅归类于该团队。*'
- en: '***Data Engineering:*** *Cases related to data pipelines, such as extraction,
    transformation, or loading.*'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***数据工程:*** *与数据管道相关的案例，如提取、转换或加载。*'
- en: '***Data Platform:*** *Any issues related to data platforms, including data
    visualization or DEP.*'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***数据平台:*** *与数据平台相关的任何问题，包括数据可视化或DEP。*'
- en: '*Only one team should be assigned per case.*'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*每个案例应仅分配一个团队。*'
- en: '***7\. Ticket Category:*** *Finally, categorize the ticket based on the description,
    using a simple 1–2 word label. Use the DAMA data quality dimensions for this classification.
    The categories should include, but aren’t limited to:*'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '***7\. 工单分类:*** *最后，根据描述将工单分类，使用简单的1-2个词的标签。使用DAMA数据质量维度进行分类。类别应包括但不限于：*'
- en: '***Completeness:*** *Ensuring all necessary data is included.*'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***完整性:*** *确保所有必要的数据都包含在内。*'
- en: '***Uniqueness:*** *Verifying data entries are unique and not duplicated.*'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***独特性:*** *验证数据条目是唯一的，不重复。*'
- en: '***Timeliness:*** *Ensuring data is up-to-date and available as expected.*'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***及时性:*** *确保数据是最新的并按预期可用。*'
- en: '***Accuracy:*** *Confirming data is correct and conforms to its true values.*'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***准确性:*** *确认数据是正确的并符合其真实值。*'
- en: '***Consistency:*** *Ensuring data is uniform across different datasets.*'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***一致性:*** *确保不同数据集中的数据保持一致。*'
- en: '***Validity:*** *Ensuring data adheres to required formats or values.*'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***有效性:*** *确保数据符合所需的格式或值。*'
- en: '***Access:*** *Related to requests for accessing data or systems.*'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***访问:*** *与请求访问数据或系统相关。*'
- en: '*You may create 2–3 other categories if needed, but keep them concise and consistent*'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如有需要，可以创建2-3个其他类别，但请保持简洁一致。*'
- en: '*Here is an example of the output format. It should be a list with each item
    separated by a pipe (|):*'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '*这是输出格式的一个示例。应为一个列表，每个项之间用管道符（|）分隔：*'
- en: '*16477679|Descriptive title under 300 characters|Brief impact description|S2|High
    Complexity|Finance|Data Engineering|Timeliness'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '*16477679|描述性标题（不超过300个字符）|简短的影响描述|S2|高复杂性|财务|数据工程|及时性*'
- en: 16377679|Another descriptive title|Another brief impact description|S1|High
    Complexity|Sales|Data Platform|Accuracy*
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 16377679|另一个描述性标题|另一个简短的影响描述|S1|高复杂性|销售|数据平台|准确性*
- en: Unless otherwise noted, all images are by the author
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图片均由作者提供
