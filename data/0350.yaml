- en: 'Courage to Learn ML: Tackling Vanishing and Exploding Gradients (Part 1)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习机器学习的勇气：解决梯度消失和梯度爆炸问题（第一部分）
- en: 原文：[https://towardsdatascience.com/courage-to-learn-ml-tackling-vanishing-and-exploding-gradients-part-1-799debbf60a0?source=collection_archive---------15-----------------------#2024-02-05](https://towardsdatascience.com/courage-to-learn-ml-tackling-vanishing-and-exploding-gradients-part-1-799debbf60a0?source=collection_archive---------15-----------------------#2024-02-05)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/courage-to-learn-ml-tackling-vanishing-and-exploding-gradients-part-1-799debbf60a0?source=collection_archive---------15-----------------------#2024-02-05](https://towardsdatascience.com/courage-to-learn-ml-tackling-vanishing-and-exploding-gradients-part-1-799debbf60a0?source=collection_archive---------15-----------------------#2024-02-05)
- en: 'Melting Away DNN’s Gradient Challenges: A Scoop of Solutions and Insights'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 消解深度神经网络梯度问题：一勺解决方案和见解
- en: '[](https://amyma101.medium.com/?source=post_page---byline--799debbf60a0--------------------------------)[![Amy
    Ma](../Images/2edf55456a1f92724535a1441fa2bef5.png)](https://amyma101.medium.com/?source=post_page---byline--799debbf60a0--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--799debbf60a0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--799debbf60a0--------------------------------)
    [Amy Ma](https://amyma101.medium.com/?source=post_page---byline--799debbf60a0--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://amyma101.medium.com/?source=post_page---byline--799debbf60a0--------------------------------)[![Amy
    Ma](../Images/2edf55456a1f92724535a1441fa2bef5.png)](https://amyma101.medium.com/?source=post_page---byline--799debbf60a0--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--799debbf60a0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--799debbf60a0--------------------------------)
    [Amy Ma](https://amyma101.medium.com/?source=post_page---byline--799debbf60a0--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--799debbf60a0--------------------------------)
    ·13 min read·Feb 5, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--799debbf60a0--------------------------------)
    ·13分钟阅读·2024年2月5日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ea2e2eaf6631a22a9257066dd20ae32f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea2e2eaf6631a22a9257066dd20ae32f.png)'
- en: Image created by the author using ChatGPT.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用ChatGPT制作。
- en: In the last installment of the ‘[Courage to Learn ML](https://towardsdatascience.com/tagged/courage-to-learn-ml)’
    series, our learner and mentor focus on learning two essential theories of DNN
    training, gradient descent and backpropagation.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在‘[学习机器学习的勇气](https://towardsdatascience.com/tagged/courage-to-learn-ml)’系列的上一期中，我们的学习者和导师专注于学习深度神经网络训练的两个基本理论——梯度下降和反向传播。
- en: Their journey began with a look at [how gradient descent is pivotal in minimizing
    the loss function](/courage-to-learn-ml-a-detailed-exploration-of-gradient-descent-and-popular-optimizers-022ecf97be7d).
    Curious about the complexities of computing gradients in deep neural networks
    across multiple hidden layers, the learner then turned to [backpropagation](/courage-to-learn-ml-explain-backpropagation-from-mathematical-theory-to-coding-practice-21e670415378).
    By decompose the backpropagation into 3 components, the learner learned about
    backpropagation and its use of the chain rule to calculate gradients efficiently
    across these layers. During this Q&A session, the learner questioned the importance
    of understanding these complex processes in an era of automated advanced deep
    learning frameworks, such as PyTorch and Tensorflow.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的旅程从[梯度下降在最小化损失函数中的重要性](https://towardsdatascience.com/courage-to-learn-ml-a-detailed-exploration-of-gradient-descent-and-popular-optimizers-022ecf97be7d)开始。出于对在多个隐藏层中计算梯度的复杂性的好奇，学习者随后转向了[反向传播](https://towardsdatascience.com/courage-to-learn-ml-explain-backpropagation-from-mathematical-theory-to-coding-practice-21e670415378)。通过将反向传播分解为三个组成部分，学习者了解了反向传播及其如何利用链式法则高效计算这些层中的梯度。在这次问答环节中，学习者质疑，在像PyTorch和TensorFlow这样的自动化深度学习框架盛行的时代，理解这些复杂过程的重要性。
- en: This is the first post of our deep dive into Deep Learning, guided by the interactions
    between a learner and a mentor. To keep things digestible, I’ve decided to break
    down my DNN series into more manageable pieces. This way, I can explore each concept
    thoroughly without overwhelming you.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们深入探讨深度学习的第一篇文章，内容由学习者与导师之间的互动引导。为了便于消化，我决定将我的深度神经网络系列分解为更易于处理的小部分。这样，我可以深入探索每个概念，而不会让你感到不堪重负。
