- en: The Math Behind Batch Normalization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批量归一化背后的数学
- en: 原文：[https://towardsdatascience.com/the-math-behind-batch-normalization-90ebbc0b1b0b?source=collection_archive---------2-----------------------#2024-05-08](https://towardsdatascience.com/the-math-behind-batch-normalization-90ebbc0b1b0b?source=collection_archive---------2-----------------------#2024-05-08)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-math-behind-batch-normalization-90ebbc0b1b0b?source=collection_archive---------2-----------------------#2024-05-08](https://towardsdatascience.com/the-math-behind-batch-normalization-90ebbc0b1b0b?source=collection_archive---------2-----------------------#2024-05-08)
- en: Explore Batch Normalization, a cornerstone of neural networks, understand its
    mathematics, and implement it from scratch.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索批量归一化，这是神经网络的基石，理解其数学原理，并从零开始实现它。
- en: '[](https://medium.com/@cristianleo120?source=post_page---byline--90ebbc0b1b0b--------------------------------)[![Cristian
    Leo](../Images/99074292e7dfda50cf50a790b8deda79.png)](https://medium.com/@cristianleo120?source=post_page---byline--90ebbc0b1b0b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--90ebbc0b1b0b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--90ebbc0b1b0b--------------------------------)
    [Cristian Leo](https://medium.com/@cristianleo120?source=post_page---byline--90ebbc0b1b0b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@cristianleo120?source=post_page---byline--90ebbc0b1b0b--------------------------------)[![Cristian
    Leo](../Images/99074292e7dfda50cf50a790b8deda79.png)](https://medium.com/@cristianleo120?source=post_page---byline--90ebbc0b1b0b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--90ebbc0b1b0b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--90ebbc0b1b0b--------------------------------)
    [Cristian Leo](https://medium.com/@cristianleo120?source=post_page---byline--90ebbc0b1b0b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--90ebbc0b1b0b--------------------------------)
    ·21 min read·May 8, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--90ebbc0b1b0b--------------------------------)
    ·阅读时间 21 分钟·2024年5月8日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/1702131733bebaab170620d4d519738f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1702131733bebaab170620d4d519738f.png)'
- en: Image generated by DALL-E
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由 DALL-E 生成
- en: Batch Normalization is a key technique in neural networks as it standardizes
    the inputs to each layer. It tackles the problem of internal covariate shift,
    where the input distribution of each layer shifts during training, complicating
    the learning process and reducing efficiency. By normalizing these inputs, Batch
    Normalization helps networks train faster and more consistently. This method is
    vital for ensuring reliable performance across different network architectures
    and tasks, including image recognition and natural language processing. In this
    article, we’ll delve into the principles and math behind Batch Normalization and
    show you how to implement it in Python from scratch.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化（Batch Normalization）是神经网络中的一个关键技术，它通过标准化每一层的输入来优化网络训练。它解决了内部协方差偏移（internal
    covariate shift）问题，即每一层的输入分布在训练过程中发生变化，导致学习过程复杂且效率低下。通过归一化这些输入，批量归一化帮助网络训练得更快且更稳定。此方法对于确保不同网络架构和任务（包括图像识别和自然语言处理）中的可靠性能至关重要。在本文中，我们将深入探讨批量归一化的原理和数学基础，并展示如何从零开始在
    Python 中实现它。
- en: '**Index**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**索引**'
- en: '[**1: Introduction**](#4495)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[**1: 介绍**](#4495)'
- en: '[**2: Need for Normalization**](#aae3)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[**2: 归一化的必要性**](#aae3)'
- en: '[**3: Math and Mechanisms**](#477c)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[**3: 数学与机制**](#477c)'
- en: '[∘ 3.1: Overcoming Covariate Shift](https://medium.com/p/90ebbc0b1b0b/edit#cd9f)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[∘ 3.1: 克服协方差偏移](https://medium.com/p/90ebbc0b1b0b/edit#cd9f)'
- en: '∘ [3.2: Scale and Shift Step](#312b)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '∘ [3.2: 缩放和平移步骤](#312b)'
- en: '∘ [3.3: Flow of Batch Normalization](#b097)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '∘ [3.3: 批量归一化的流](#b097)'
- en: '∘ [3.4: Activation Distribution](#e2d9)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '∘ [3.4: 激活分布](#e2d9)'
- en: '[**4: Application From Scratch in Python**](#df72)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[**4: 从零开始在 Python 中应用**](#df72)'
- en: '∘ [4.1: Batch Normalization From Scratch](#2add)…'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '∘ [4.1: 从零开始的批量归一化](#2add)…'
