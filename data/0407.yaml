- en: 'Text Embeddings: Comprehensive Guide'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本嵌入：全面指南
- en: 原文：[https://towardsdatascience.com/text-embeddings-comprehensive-guide-afd97fce8fb5?source=collection_archive---------0-----------------------#2024-02-13](https://towardsdatascience.com/text-embeddings-comprehensive-guide-afd97fce8fb5?source=collection_archive---------0-----------------------#2024-02-13)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/text-embeddings-comprehensive-guide-afd97fce8fb5?source=collection_archive---------0-----------------------#2024-02-13](https://towardsdatascience.com/text-embeddings-comprehensive-guide-afd97fce8fb5?source=collection_archive---------0-----------------------#2024-02-13)
- en: Evolution, visualisation, and applications of text embeddings
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入的演变、可视化和应用
- en: '[](https://miptgirl.medium.com/?source=post_page---byline--afd97fce8fb5--------------------------------)[![Mariya
    Mansurova](../Images/b1dd377b0a1887db900cc5108bca8ea8.png)](https://miptgirl.medium.com/?source=post_page---byline--afd97fce8fb5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--afd97fce8fb5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--afd97fce8fb5--------------------------------)
    [Mariya Mansurova](https://miptgirl.medium.com/?source=post_page---byline--afd97fce8fb5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://miptgirl.medium.com/?source=post_page---byline--afd97fce8fb5--------------------------------)[![玛丽亚·曼苏罗娃](../Images/b1dd377b0a1887db900cc5108bca8ea8.png)](https://miptgirl.medium.com/?source=post_page---byline--afd97fce8fb5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--afd97fce8fb5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--afd97fce8fb5--------------------------------)
    [玛丽亚·曼苏罗娃](https://miptgirl.medium.com/?source=post_page---byline--afd97fce8fb5--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--afd97fce8fb5--------------------------------)
    ·20 min read·Feb 13, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--afd97fce8fb5--------------------------------)
    ·阅读时间：20分钟·2024年2月13日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/edce20eff0ac79af415ddaee3b40cfc3.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/edce20eff0ac79af415ddaee3b40cfc3.png)'
- en: Image by DALL-E 3
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由DALL-E 3生成
- en: As human beings, we can read and understand texts (at least some of them). Computers
    in opposite “think in numbers”, so they can’t automatically grasp the meaning
    of words and sentences. If we want computers to understand the natural language,
    we need to convert this information into the format that computers can work with
    — vectors of numbers.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作为人类，我们可以阅读和理解文本（至少是其中一些）。相反，计算机“用数字思考”，因此它们无法自动理解单词和句子的含义。如果我们希望计算机理解自然语言，我们需要将这些信息转换为计算机可以处理的格式——数字向量。
- en: People learned how to convert texts into machine-understandable format many
    years ago (one of the first versions was [ASCII](https://en.wikipedia.org/wiki/ASCII)).
    Such an approach helps render and transfer texts but doesn’t encode the meaning
    of the words. At that time, the standard search technique was a keyword search
    when you were just looking for all the documents that contained specific words
    or N-grams.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 许多年前，人们就学会了如何将文本转换为计算机可以理解的格式（最早的版本之一是[ASCII](https://en.wikipedia.org/wiki/ASCII)）。这种方法有助于渲染和传输文本，但并没有编码单词的含义。当时，标准的搜索技术是关键字搜索，即仅仅查找包含特定单词或N-gram的所有文档。
- en: Then, after decades, embeddings have emerged. We can calculate embeddings for
    words, sentences, and even images. Embeddings are also vectors of numbers, but
    they can capture the meaning. So, you can use them to do a semantic search and
    even work with documents in different languages.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，经过数十年的发展，嵌入出现了。我们可以计算单词、句子甚至图像的嵌入。嵌入也是数字向量，但它们可以捕捉到意义。因此，你可以用它们进行语义搜索，甚至处理不同语言的文档。
- en: 'In this article, I would like to dive deeper into the embedding topic and discuss
    all the details:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将深入探讨嵌入的主题，并讨论所有细节：
- en: what preceded the embeddings and how they evolved,
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入之前发生了什么，以及它们是如何演变的，
- en: how to calculate embeddings using OpenAI tools,
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用OpenAI工具计算嵌入，
- en: how to define whether sentences are close to each other,
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何判断句子之间的相似度，
- en: how to visualise embeddings,
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何可视化嵌入，
- en: the most exciting part is how you could use embeddings in practice.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最令人兴奋的部分是你如何在实践中使用嵌入。
- en: Let’s move on and learn about the evolution of embeddings.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续前进，了解嵌入的演变。
- en: Evolution of Embeddings
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 嵌入的演变
- en: We will start our journey with a brief tour into the history of text representations.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从简要回顾文本表示的历史开始我们的旅程。
- en: Bag of Words
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词袋模型
- en: The most basic approach to converting texts into vectors is a bag of words.
    Let’s look at one of the famous quotes of Richard P. Feynman*“We are lucky to
    live in an age in which we are still making discoveries”.* We will use it to illustrate
    a bag of words approach.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 将文本转换成向量的最基本方法是词袋模型。让我们看看理查德·费曼的名言之一*“我们很幸运生活在一个仍在不断发现的时代”*。我们将用它来说明词袋模型的方法。
- en: The first step to get a bag of words vector is to split the text into words
    (tokens) and then reduce words to their base forms. For example, *“running”* will
    transform into *“run”*. This process is called stemming. We can use the NLTK Python
    package for it.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 获取词袋向量的第一步是将文本拆分为单词（词元），然后将单词简化为其基础形式。例如，*“running”*将转换为*“run”*。这个过程叫做词干提取。我们可以使用NLTK
    Python包来完成这一操作。
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now, we have a list of base forms of all our words. The next step is to calculate
    their frequencies to create a vector.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了所有单词的基础形式列表。下一步是计算它们的频率以创建向量。
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Actually, if we wanted to convert our text into a vector, we would have to take
    into account not only the words we have in the text but the whole vocabulary.
    Let’s assume we also have *“i”*, *“you”* and *”study”* in our vocabulary and let’s
    create a vector from Feynman’s quote.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，如果我们想要将文本转换成向量，我们不仅需要考虑文本中的单词，还需要考虑整个词汇表。假设我们的词汇表中也包含*“i”*、*“you”*和*“study”*，那么我们就可以从费曼的名言中创建一个向量。
- en: '![](../Images/352726337fbddb935f18fb369b489d69.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/352726337fbddb935f18fb369b489d69.png)'
- en: Graph by author
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图表
- en: This approach is quite basic, and it doesn’t take into account the semantic
    meaning of the words, so the sentences *“the girl is studying data science”* and
    *“the young woman is learning AI and ML”* won’t be close to each other.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法相当基础，并且没有考虑到单词的语义含义，因此句子*“the girl is studying data science”*和*“the young
    woman is learning AI and ML”*将不会彼此接近。
- en: TF-IDF
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TF-IDF
- en: A slightly improved version of the bag of the words approach is [**TF-IDF**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)
    (*Term Frequency — Inverse Document Frequency*). It’s the multiplication of two
    metrics.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋模型方法的一个略微改进版本是[**TF-IDF**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)（*词频-逆文档频率*）。它是两个指标的乘积。
- en: '![](../Images/48834842d5c59c0eb25c7ba84ffa02da.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/48834842d5c59c0eb25c7ba84ffa02da.png)'
- en: '**Term Frequency** shows the frequency of the word in the document. The most
    common way to calculate it is to divide the raw count of the term in this document
    (like in the bag of words) by the total number of terms (words) in the document.
    However, there are many other approaches like just raw count, boolean “frequencies”,
    and different approaches to normalisation. You can learn more about different
    approaches on [Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词频**表示单词在文档中的出现频率。最常见的计算方法是将单词在文档中的原始计数（就像在词袋模型中一样）除以文档中单词的总数。然而，也有许多其他方法，如仅使用原始计数、布尔“频率”，以及不同的归一化方法。你可以在[维基百科](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)上了解更多关于不同方法的信息。'
- en: '![](../Images/4b88c942246dd26a9376e31a1c7eb065.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b88c942246dd26a9376e31a1c7eb065.png)'
- en: '**Inverse Document Frequency** denotes how much information the word provides.
    For example, the words *“a”* or *“that”* don’t give you any additional information
    about the document’s topic. In contrast, words like *“ChatGPT”* or *“bioinformatics”*
    can help you define the domain (but not for this sentence). It’s calculated as
    the logarithm of the ratio of the total number of documents to those containing
    the word. The closer IDF is to 0 — the more common the word is and the less information
    it provides.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逆文档频率**表示单词提供的信息量。例如，像*“a”*或*“that”*这样的单词不会给你关于文档主题的额外信息。相反，像*“ChatGPT”*或*“生物信息学”*这样的单词可以帮助你定义领域（但对于这个句子来说不适用）。它是通过计算包含该单词的文档数量与文档总数的比率的对数来得出的。IDF值越接近0，表示该单词越常见，提供的信息越少。'
- en: '![](../Images/a07031a5a4933e2fb90ac8384d62f5a8.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a07031a5a4933e2fb90ac8384d62f5a8.png)'
- en: So, in the end, we will get vectors where common words (like *“I”* or *“you”*)
    will have low weights, while rare words that occur in the document multiple times
    will have higher weights. This strategy will give a bit better results, but it
    still can’t capture semantic meaning.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，最终我们会得到向量，其中常见单词（如*“I”*或*“you”*）的权重较低，而在文档中多次出现的稀有单词的权重较高。这种策略可以提供稍微更好的结果，但它仍然无法捕捉到语义上的含义。
- en: The other challenge with this approach is that it produces pretty sparse vectors.
    The length of the vectors is equal to the corpus size. There are about 470K unique
    words in English ([source](https://en.wikipedia.org/wiki/List_of_dictionaries_by_number_of_words)),
    so we will have huge vectors. Since the sentence won’t have more than 50 unique
    words, 99.99% of the values in vectors will be 0, not encoding any info. Looking
    at this, scientists started to think about dense vector representation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的另一个挑战是它生成的向量非常稀疏。向量的长度等于语料库的大小。英语中大约有 47 万个独特的单词（[来源](https://en.wikipedia.org/wiki/List_of_dictionaries_by_number_of_words)），因此我们将得到非常大的向量。由于一句话中不会超过
    50 个独特单词，所以向量中 99.99% 的值将是 0，无法编码任何信息。基于此，科学家们开始考虑稠密向量表示。
- en: Word2Vec
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Word2Vec
- en: One of the most famous approaches to dense representation is word2vec, proposed
    by Google in 2013 in the paper [“Efficient Estimation of Word Representations
    in Vector Space”](https://arxiv.org/abs/1301.3781) by Mikolov et al.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最著名的稠密表示方法之一是 word2vec，由 Google 在 2013 年通过 Mikolov 等人的论文 [《Efficient Estimation
    of Word Representations in Vector Space》](https://arxiv.org/abs/1301.3781) 提出。
- en: 'There are two different word2vec approaches mentioned in the paper: Continuous
    Bag of Words (when we predict the word based on the surrounding words) and Skip-gram
    (the opposite task — when we predict context based on the word).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 论文中提到了两种不同的 word2vec 方法：连续词袋模型（当我们基于周围的词汇预测目标词时）和 Skip-gram（相反的任务——当我们基于词汇预测上下文时）。
- en: '![](../Images/a55f2b25a828530cca95fcce2c9901fa.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a55f2b25a828530cca95fcce2c9901fa.png)'
- en: Figure from the paper by Mikolov et al. 2013 | [source](https://arxiv.org/pdf/1301.3781.pdf)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 论文中的图示，Mikolov 等人，2013 | [来源](https://arxiv.org/pdf/1301.3781.pdf)
- en: 'The high-level idea of dense vector representation is to train two models:
    encoder and decoder. For example, in the case of skip-gram, we might pass the
    word *“christmas”* to the encoder. Then, the encoder will produce a vector that
    we pass to the decoder expecting to get the words *“merry”*, *“to”*, and *“you”*.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 稠密向量表示的高级概念是训练两个模型：编码器和解码器。例如，在 Skip-gram 的情况下，我们可能会将词汇 *“christmas”* 传入编码器。然后，编码器会生成一个向量，我们将这个向量传递给解码器，期望得到词汇
    *“merry”*、*“to”* 和 *“you”*。
- en: '![](../Images/eaa26d1a1c68799c60dfe915ea18569b.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eaa26d1a1c68799c60dfe915ea18569b.png)'
- en: Scheme by author
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 作者的示意图
- en: This model started to take into account the meaning of the words since it’s
    trained on the context of the words. However, it ignores morphology (information
    we can get from the word parts, for example, that “*-less”* means the lack of
    something). This drawback was addressed later by looking at subword skip-grams
    in [GloVe](https://www-nlp.stanford.edu/pubs/glove.pdf).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型开始考虑词汇的含义，因为它是基于词汇的上下文进行训练的。然而，它忽略了词形变化（例如，“*-less”* 表示缺少某物）。这个缺点后来通过关注子词
    Skip-grams 在 [GloVe](https://www-nlp.stanford.edu/pubs/glove.pdf) 中得到了改进。
- en: Also, word2vec was capable of working only with words, but we would like to
    encode whole sentences. So, let’s move on to the next evolutional step with transformers.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，word2vec 只能处理单个词汇，但我们希望对整个句子进行编码。那么，让我们进入下一个进化步骤——使用 transformers。
- en: Transformers and Sentence Embeddings
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformers 和句子嵌入
- en: The next evolution was related to the transformers approach introduced in the
    [“Attention Is All You Need”](https://arxiv.org/abs/1706.03762) paper by Vaswani
    et al. Transformers were able to produce information-reach dense vectors and become
    the dominant technology for modern language models.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步的进化涉及到 transformers 方法，这是 Vaswani 等人在[《Attention Is All You Need》](https://arxiv.org/abs/1706.03762)论文中提出的。Transformers
    能够生成信息丰富的稠密向量，并成为现代语言模型的主流技术。
- en: I won’t cover the details of the transformers’ architecture since it’s not so
    relevant to our topic and would take a lot of time. If you’re interested in learning
    more, there are a lot of materials about transformers, for example, [“Transformers,
    Explained”](https://daleonai.com/transformers-explained) or [“The Illustrated
    Transformer”](https://jalammar.github.io/illustrated-transformer/).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会详细介绍 transformers 的架构，因为这与我们的主题关系不大，且会占用大量时间。如果你有兴趣了解更多，有许多关于 transformers
    的资料可供参考，例如，[《Transformers, Explained》](https://daleonai.com/transformers-explained)
    或 [《The Illustrated Transformer》](https://jalammar.github.io/illustrated-transformer/)。
- en: Transformers allow you to use the same “core” model and fine-tune it for different
    use cases without retraining the core model (which takes a lot of time and is
    quite costly). It led to the rise of pre-trained models. One of the first popular
    models was BERT (Bidirectional Encoder Representations from Transformers) by Google
    AI.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers 允许你使用相同的“核心”模型，并针对不同的使用场景进行微调，而无需重新训练核心模型（这需要大量时间且成本高昂）。它推动了预训练模型的兴起。其中一个最早流行的模型是
    Google AI 的 BERT（双向编码器表示模型）。
- en: Internally, BERT still operates on a token level similar to word2vec, but we
    still want to get sentence embeddings. So, the naive approach could be to take
    an average of all tokens’ vectors. Unfortunately, this approach doesn’t show good
    performance.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 从内部来看，BERT 仍然在与 word2vec 类似的单词级别进行操作，但我们仍然想要获取句子嵌入。所以，最简单的方法是取所有标记（token）向量的平均值。不幸的是，这种方法表现不好。
- en: This problem was solved in 2019 when [Sentence-BERT](https://arxiv.org/abs/1908.10084)
    was released. It outperformed all previous approaches to semantic textual similarity
    tasks and allowed the calculation of sentence embeddings.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题在2019年得到了解决，当时[Sentence-BERT](https://arxiv.org/abs/1908.10084)发布。它超越了所有先前的语义文本相似性任务方法，并且能够计算句子嵌入。
- en: It’s a huge topic so we won’t be able to cover it all in this article. So, if
    you’re really interested, you can learn more about the sentence embeddings in
    [this article](https://www.pinecone.io/learn/series/nlp/sentence-embeddings/).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个庞大的话题，因此我们无法在这篇文章中完全覆盖它。如果你真的感兴趣，可以在[这篇文章](https://www.pinecone.io/learn/series/nlp/sentence-embeddings/)中了解更多关于句子嵌入的内容。
- en: We’ve briefly covered the evolution of embeddings and got a high-level understanding
    of the theory. Now, it’s time to move on to practice and lear how to calculate
    embeddings using OpenAI tools.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简要地回顾了嵌入的演变，并对其理论有了一个高层次的理解。现在，是时候进入实践部分，学习如何使用 OpenAI 工具计算嵌入了。
- en: Calculating embeddings
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算嵌入
- en: 'In this article, we will be using OpenAI embeddings. We will try a new model
    `text-embedding-3-small` that was [released](https://openai.com/blog/new-embedding-models-and-api-updates)
    just recently. The new model shows better performance compared to `text-embedding-ada-002`:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将使用 OpenAI 的嵌入模型。我们将尝试一个新的模型 `text-embedding-3-small`，它最近被[发布](https://openai.com/blog/new-embedding-models-and-api-updates)。与
    `text-embedding-ada-002`相比，新模型的表现更好：
- en: The average score on a widely used multi-language retrieval ([MIRACL](https://github.com/project-miracl/miracl))
    benchmark has risen from 31.4% to 44.0%.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个广泛使用的多语言检索（[MIRACL](https://github.com/project-miracl/miracl)）基准测试中，平均分数从31.4%上升到了44.0%。
- en: The average performance on a frequently used benchmark for English tasks ([MTEB](https://github.com/embeddings-benchmark/mteb))
    has also improved, rising from 61.0% to 62.3%.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个常用的基准测试中，针对英文任务的平均性能（[MTEB](https://github.com/embeddings-benchmark/mteb)）也有所提升，从61.0%提高到了62.3%。
- en: OpenAI also released a new larger model `text-embedding-3-large`. Now, it’s
    their best performing embedding model.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 还发布了一个新的更大模型 `text-embedding-3-large`。现在，这是他们表现最好的嵌入模型。
- en: As a data source, we will be working with a small sample of [Stack Exchange
    Data Dump](https://archive.org/details/stackexchange) — an anonymised dump of
    all user-contributed content on the [Stack Exchange network](https://stackexchange.com/).
    I’ve selected a bunch of topics that look interesting to me and sample 100 questions
    from each of them. Topics range from Generative AI to coffee or bicycles so that
    we will see quite a wide variety of topics.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据源，我们将使用一个小样本的[Stack Exchange 数据库](https://archive.org/details/stackexchange)——这是一个匿名的所有用户贡献内容的转储，来自[Stack
    Exchange 网络](https://stackexchange.com/)。我选择了一些看起来有趣的主题，并从每个主题中随机抽取了100个问题。主题从生成性人工智能到咖啡或自行车，涉及内容非常广泛。
- en: First, we need to calculate embeddings for all our Stack Exchange questions.
    It’s worth doing it once and storing results locally (in a file or vector storage).
    We can generate embeddings using the OpenAI Python package.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要计算所有 Stack Exchange 问题的嵌入。值得做一次并将结果本地存储（在文件或向量存储中）。我们可以使用 OpenAI Python
    包来生成嵌入。
- en: '[PRE2]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As a result, we got a 1536-dimension vector of float numbers. We can now repeat
    it for all our data and start analysing the values.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 结果，我们得到了一个1536维的浮动数值向量。现在，我们可以对所有数据重复这个过程并开始分析这些值。
- en: The primary question you might have is how close the sentences are to each other
    by meaning. To uncover answers, let’s discuss the concept of distance between
    vectors.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问的主要问题是：句子在语义上有多接近？为了揭示答案，让我们讨论一下向量之间的距离概念。
- en: Distance between vectors
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量之间的距离
- en: Embeddings are actually vectors. So, if we want to understand how close two
    sentences are to each other, we can calculate the distance between vectors. A
    smaller distance would be equivalent to a closer semantic meaning.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入实际上就是向量。因此，如果我们想了解两句话之间的相似度，可以计算它们的向量之间的距离。较小的距离意味着语义上更接近。
- en: 'Different metrics can be used to measure the distance between two vectors:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用不同的度量方法来衡量两个向量之间的距离：
- en: Euclidean distance (L2),
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧几里得距离（L2），
- en: Manhattant distance (L1),
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 曼哈顿距离（L1），
- en: Dot product,
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点积，
- en: Cosine distance.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 余弦距离。
- en: Let’s discuss them. As a simple example, we will be using two 2D vectors.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论它们。作为一个简单的例子，我们将使用两个二维向量。
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Euclidean distance (L2)
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 欧几里得距离（L2）
- en: The most standard way to define distance between two points (or vectors) is
    Euclidean distance or L2 norm. This metric is the most commonly used in day-to-day
    life, for example, when we are talking about the distance between 2 towns.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 定义两个点（或向量）之间距离的最标准方法是欧几里得距离或L2范数。这种度量方法在日常生活中最为常见，例如当我们谈论两个城镇之间的距离时。
- en: Here’s a visual representation and formula for L2 distance.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是L2距离的可视化表示和公式。
- en: '![](../Images/e4d3008368443f6d56ba82b20c182b3a.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4d3008368443f6d56ba82b20c182b3a.png)'
- en: Image by author
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自作者
- en: We can calculate this metric using vanilla Python or leveraging the numpy function.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用原生Python或利用numpy函数来计算这个度量。
- en: '[PRE4]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Manhattant distance (L1)
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 曼哈顿距离（L1）
- en: The other commonly used distance is the L1 norm or Manhattan distance. This
    distance was called after the island of Manhattan (New York). This island has
    a grid layout of streets, and the shortest routes between two points in Manhattan
    will be L1 distance since you need to follow the grid.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常用的距离度量是L1范数或曼哈顿距离。这个距离是以曼哈顿岛（纽约）命名的。这个岛有着网格状的街道布局，在曼哈顿，两个点之间的最短路径是L1距离，因为你需要沿着网格行驶。
- en: '![](../Images/f4e01e943ce4e010819197e8f2110ce5.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4e01e943ce4e010819197e8f2110ce5.png)'
- en: Image by author
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自作者
- en: We can also implement it from scratch or use the numpy function.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以从零开始实现，或者使用numpy函数。
- en: '[PRE5]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Dot product
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 点积
- en: Another way to look at the distance between vectors is to calculate a dot or
    scalar product. Here’s a formula and we can easily implement it.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种看待向量之间距离的方法是计算点积或标量积。这里是公式，我们可以轻松实现它。
- en: '![](../Images/d3e514aa4acf640b217c3c129beb49f6.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d3e514aa4acf640b217c3c129beb49f6.png)'
- en: Image by author
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自作者
- en: '[PRE6]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This metric is a bit tricky to interpret. On the one hand, it shows you whether
    vectors are pointing in one direction. On the other hand, the results highly depend
    on the magnitudes of the vectors. For example, let’s calculate the dot products
    between two pairs of vectors:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这个度量方法有些难以解释。一方面，它可以告诉你向量是否朝着同一方向指向。另一方面，结果高度依赖于向量的大小。例如，让我们计算两对向量之间的点积：
- en: '`(1, 1)` vs `(1, 1)`'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(1, 1)` 对比 `(1, 1)`'
- en: '`(1, 1)` vs `(10, 10)`.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(1, 1)` 对比 `(10, 10)`。'
- en: 'In both cases, vectors are collinear, but the dot product is ten times bigger
    in the second case: 2 vs 20.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，向量是共线的，但在第二种情况下，点积大约是第一种情况的十倍：2 对比 20。
- en: Cosine similarity
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 余弦相似度
- en: Quite often, cosine similarity is used. Cosine similarity is a dot product normalised
    by vectors’ magnitudes (or normes).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 很多时候，会使用余弦相似度。余弦相似度是通过向量的大小（或范数）归一化后的点积。
- en: '![](../Images/a44f78804a4d51ccf45138b17cc279c0.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a44f78804a4d51ccf45138b17cc279c0.png)'
- en: Image by author
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自作者
- en: We can either calculate everything ourselves (as previously) or use the function
    from sklearn.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像之前那样自己计算所有内容，或者使用来自sklearn的函数。
- en: '[PRE7]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The function `cosine_similarity` expects 2D arrays. That’s why we need to reshape
    the numpy arrays.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`cosine_similarity`期望输入2D数组。因此，我们需要调整numpy数组的形状。
- en: Let’s talk a bit about the physical meaning of this metric. Cosine similarity
    is equal to the cosine between two vectors. The closer the vectors are, the higher
    the metric value.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们谈谈这个度量的物理意义。余弦相似度等于两个向量之间的余弦值。向量越接近，度量值越高。
- en: '![](../Images/f20da9af06b65db763be0113f19108f0.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f20da9af06b65db763be0113f19108f0.png)'
- en: Image by author
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自作者
- en: We can even calculate the exact angle between our vectors in degrees. We get
    results around 30 degrees, and it looks pretty reasonable.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至可以计算两个向量之间的精确角度（单位为度）。我们得到的结果大约为30度，这看起来非常合理。
- en: '[PRE8]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: What metric to use?
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 该使用哪种度量方法？
- en: We’ve discussed different ways to calculate the distance between two vectors,
    and you might start thinking about which one to use.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了计算两个向量之间距离的不同方法，你可能开始思考该使用哪一种方法。
- en: 'You can use any distance to compare the embeddings you have. For example, I
    calculated the average distances between the different clusters. Both L2 distance
    and cosine similarity show us similar pictures:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用任何距离来比较你拥有的嵌入。例如，我计算了不同聚类之间的平均距离。L2 距离和余弦相似度给我们呈现了相似的图像：
- en: Objects within a cluster are closer to each other than to other clusters. It’s
    a bit tricky to interpret our results since for L2 distance, closer means lower
    distance, while for cosine similarity — the metric is higher for closer objects.
    Don’t get confused.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类内的对象比与其他聚类的对象更接近。解释我们的结果有点棘手，因为对于 L2 距离，越近意味着距离越小，而对于余弦相似度——度量越高表示对象越接近。不要感到困惑。
- en: We can spot that some topics are really close to each other, for example, *“politics”*
    and *“economics”* or *“ai”* and *“datascience”*.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以发现某些主题非常接近，例如*“政治”*和*“经济学”*，或者*“人工智能”*和*“数据科学”*。
- en: '![](../Images/c2b2f26c939f419047e52a0c1c7a1fac.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2b2f26c939f419047e52a0c1c7a1fac.png)'
- en: Image by author
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: '![](../Images/4dc3fdc10fb1dc9616d7ff13a18d177e.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4dc3fdc10fb1dc9616d7ff13a18d177e.png)'
- en: Image by author
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'However, for NLP tasks, the best practice is usually to use cosine similarity.
    Some reasons behind it:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于 NLP 任务，通常的最佳实践是使用余弦相似度。其背后有几个原因：
- en: Cosine similarity is between -1 and 1, while L1 and L2 are unbounded, so it’s
    easier to interpret.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 余弦相似度的范围在 -1 和 1 之间，而 L1 和 L2 距离没有界限，因此余弦相似度更容易解释。
- en: From the practical perspective, it’s more effective to calculate dot products
    than square roots for Euclidean distance.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从实际角度来看，计算点积比计算欧几里得距离的平方根更有效。
- en: Cosine similarity is less affected by the curse of dimensionality (we will talk
    about it in a second).
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 余弦相似度受维度灾难的影响较小（我们稍后会讨论它）。
- en: OpenAI embeddings are already normed, so dot product and cosine similarity are
    equal in this case.
  id: totrans-123
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: OpenAI 的嵌入已经是标准化的，因此在这种情况下，点积和余弦相似度是相等的。
- en: 'You might spot in the results above that the difference between inter- and
    intra-cluster distances is not so big. The root cause is the high dimensionality
    of our vectors. This effect is called “the curse of dimensionality”: the higher
    the dimension, the narrower the distribution of distances between vectors. You
    can learn more details about it in [this article](https://towardsai.net/p/l/why-should-euclidean-distance-not-be-the-default-distance-measure).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能在上述结果中发现，聚类间距和聚类内距的差异并不大。根本原因是我们向量的高维度。这个现象被称为“维度灾难”：维度越高，向量之间的距离分布越窄。你可以在[这篇文章](https://towardsai.net/p/l/why-should-euclidean-distance-not-be-the-default-distance-measure)中了解更多细节。
- en: I would like to briefly show you how it works so that you get some intuition.
    I calculated a distribution of OpenAI embedding values and generated sets of 300
    vectors with different dimensionalities. Then, I calculated the distances between
    all the vectors and draw a histogram. You can easily see that the increase in
    vector dimensionality makes the distribution narrower.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我想简要地展示一下它是如何工作的，以便你能有所直觉。我计算了 OpenAI 嵌入值的分布，并生成了具有不同维度的 300 个向量集。然后，我计算了所有向量之间的距离，并绘制了直方图。你可以清楚地看到，向量维度的增加使得分布变得更窄。
- en: '![](../Images/fbfdf3c34e03b2ae519e305d5703bfef.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fbfdf3c34e03b2ae519e305d5703bfef.png)'
- en: Graph by author
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图表由作者提供
- en: We’ve learned how to measure the similarities between the embeddings. With that
    we’ve finished with a theoretical part and moving to more practical part (visualisations
    and practical applications). Let’s start with visualisations since it’s always
    better to see your data first.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习了如何衡量嵌入之间的相似度。这样，我们就完成了理论部分，接下来将进入更实际的部分（可视化和实际应用）。让我们先从可视化开始，因为先看到你的数据总是更好的。
- en: Visualising embeddings
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化嵌入
- en: 'The best way to understand the data is to visualise it. Unfortunately, embeddings
    have 1536 dimensions, so it’s pretty challenging to look at the data. However,
    there’s a way: we could use dimensionality reduction techniques to project vectors
    in two-dimensional space.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 理解数据的最佳方式是将其可视化。不幸的是，嵌入有 1536 个维度，因此查看数据相当具有挑战性。然而，有一种方法：我们可以使用降维技术将向量投影到二维空间。
- en: PCA
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PCA
- en: The most basic dimensionality reduction technique is [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis)
    (Principal Component Analysis). Let’s try to use it.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 最基本的降维技术是[PCA](https://en.wikipedia.org/wiki/Principal_component_analysis)（主成分分析）。让我们尝试使用它。
- en: First, we need to convert our embeddings into a 2D numpy array to pass it to
    sklearn.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要将嵌入转换为二维 numpy 数组，以便传递给 sklearn。
- en: '[PRE9]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Then, we need to initialise a PCA model with `n_components = 2` (because we
    want to create a 2D visualisation), train the model on the whole data and predict
    new values.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要初始化一个PCA模型，设置`n_components = 2`（因为我们想创建一个二维可视化图），在整个数据上训练模型并预测新值。
- en: '[PRE10]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As a result, we got a matrix with just two features for each question, so we
    could easily visualise it on a scatter plot.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 结果，我们得到了一个包含每个问题两个特征的矩阵，这样我们就可以轻松地在散点图中可视化它。
- en: '[PRE11]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/a0683ee9c8cfe52995691ef7887777cf.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a0683ee9c8cfe52995691ef7887777cf.png)'
- en: Image by author
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: We can see that questions from each topic are pretty close to each other, which
    is good. However, all the clusters are mixed, so there’s room for improvement.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，每个话题中的问题彼此相当接近，这很好。然而，所有的簇都混在一起，所以还有改进的空间。
- en: t-SNE
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: t-SNE
- en: PCA is a linear algorithm, while most of the relations are non-linear in real
    life. So, we may not be able to separate the clusters because of non-linearity.
    Let’s try to use a non-linear algorithm [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)
    and see whether it will be able to show better results.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: PCA是一个线性算法，而现实生活中的大多数关系是非线性的。因此，由于非线性原因，我们可能无法将簇分开。让我们尝试使用非线性算法[t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)，看看它是否能展示更好的结果。
- en: The code is almost identical. I just used the t-SNE model instead of PCA.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 代码几乎相同。我只是用了t-SNE模型而不是PCA。
- en: '[PRE12]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The t-SNE result looks way better. Most of the clusters are separated except
    *“genai”*, *“datascience”* and *“ai”.* However, it’s pretty expected — I doubt
    I could separate these topics myself.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE的结果看起来好多了。除了*“genai”*、*“datascience”*和*“ai”*，大部分簇都已经分开了。然而，这是预料之中的——我怀疑我能否将这些话题分开。
- en: '![](../Images/c4f6590a648a74f2c411e17a989b4959.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4f6590a648a74f2c411e17a989b4959.png)'
- en: Looking at this visualisation, we see that embeddings are pretty good at encoding
    semantic meaning.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 看着这个可视化结果，我们可以看到，嵌入在编码语义意义方面相当不错。
- en: Also, you can make a projection to three-dimensional space and visualise it.
    I’m not sure whether it would be practical, but it can be insightful and engaging
    to play with the data in 3D.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，你可以将数据投影到三维空间并进行可视化。我不确定这是否实用，但将数据以三维方式展示，可能会带来一些深刻的洞察，并且玩转数据也会很有趣。
- en: '[PRE13]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/aa256e26b30e0c95d5caf111543890ec.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa256e26b30e0c95d5caf111543890ec.png)'
- en: Barcodes
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 条形码
- en: 'The way to understand the embeddings is to visualise a couple of them as bar
    codes and see the correlations. I picked three examples of embeddings: two are
    closest to each other, and the other is the farthest example in our dataset.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 理解嵌入的方法是将其中一些可视化为条形码，并查看它们之间的相关性。我挑选了三个嵌入示例：两个是最接近的，另一个是我们数据集中最远的示例。
- en: '[PRE14]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](../Images/89478c10950950c9aee5c8cfe304754c.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89478c10950950c9aee5c8cfe304754c.png)'
- en: Graph by author
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图
- en: It’s not easy to see whether vectors are close to each other in our case because
    of high dimensionality. However, I still like this visualisation. It might be
    helpful in some cases, so I am sharing this idea with you.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 由于维度过高，我们很难直观地判断向量之间的接近程度。然而，我仍然喜欢这种可视化方式。它在某些情况下可能会有帮助，所以我和你分享这个想法。
- en: We’ve learned how to visualise embeddings and have no doubts left about their
    ability to grasp the meaning of the text. Now, it’s time to move on to the most
    interesting and fascinating part and discuss how you can leverage embeddings in
    practice.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学会了如何可视化嵌入，并且对它们抓取文本意义的能力没有任何疑虑。现在，到了最有趣、最吸引人的部分，我们将讨论如何在实际中利用嵌入。
- en: Practical applications
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实际应用
- en: Of course, embeddings’ primary goal is not to encode texts as vectors of numbers
    or visualise them just for the sake of it. We can benefit a lot from our ability
    to capture the texts’ meanings. Let’s go through a bunch of more practical examples.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，嵌入的主要目标并不是仅仅将文本编码为数值向量或仅为了可视化它们而做。我们可以从捕捉文本意义的能力中受益匪浅。让我们通过一些更实际的例子来了解这个过程。
- en: Clustering
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类
- en: Let’s start with clustering. Clustering is an unsupervised learning technique
    that allows you to split your data into groups without any initial labels. Clustering
    can help you understand the internal structural patterns in your data.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从聚类开始。聚类是一种无监督学习技术，允许你将数据分成不同的组，而不需要任何初始标签。聚类可以帮助你理解数据中的内部结构模式。
- en: We will use one of the most basic clustering algorithms — [K-means](https://scikit-learn.org/stable/modules/clustering.html#k-means).
    For the K-means algorithm, we need to specify the number of clusters. We can define
    the optimal number of clusters using [silhouette scores](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用最基本的聚类算法之一——[K-means](https://scikit-learn.org/stable/modules/clustering.html#k-means)。对于K-means算法，我们需要指定聚类的数量。我们可以通过[silhouette评分](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)来定义最优的聚类数量。
- en: Let’s try k (number of clusters) between 2 and 50\. For each k, we will train
    a model and calculate silhouette scores. The higher silhouette score — the better
    clustering we got.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试k（聚类数量）在2到50之间的值。对于每个k，我们将训练一个模型并计算silhouette评分。silhouette评分越高，聚类效果越好。
- en: '[PRE16]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In our case, the silhouette score reaches a maximum when `k = 11`. So, let’s
    use this number of clusters for our final model.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，当`k = 11`时，silhouette评分达到了最大值。因此，让我们在最终模型中使用这个聚类数量。
- en: '![](../Images/4b555cc018bfc29bf8ecea55f54da8f6.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b555cc018bfc29bf8ecea55f54da8f6.png)'
- en: Graph by author
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图表由作者提供
- en: Let’s visualise the clusters using t-SNE for dimensionality reduction as we
    already did before.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们像之前一样，使用t-SNE进行降维来可视化聚类。
- en: '[PRE17]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Visually, we can see that the algorithm was able to define clusters quite well
    — they are separated pretty well.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 从视觉上看，我们可以看到算法能够很好地区分聚类——它们分离得非常清晰。
- en: '![](../Images/994324e79d3962523b499f92912d38c2.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/994324e79d3962523b499f92912d38c2.png)'
- en: We have factual topic labels, so we can even assess how good clusterisation
    is. Let’s look at the topics’ mixture for each cluster.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有事实性的主题标签，因此我们甚至可以评估聚类的效果。让我们看看每个聚类的主题混合情况。
- en: '[PRE18]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](../Images/58c3df19b731f65894997ea8dddad3d8.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/58c3df19b731f65894997ea8dddad3d8.png)'
- en: 'In most cases, clusterisation worked perfectly. For example, cluster 5 contains
    almost only questions about bicycles, while cluster 6 is about coffee. However,
    it wasn’t able to distinguish close topics:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，聚类效果非常好。例如，聚类5几乎只包含关于自行车的问题，而聚类6则是关于咖啡的。然而，它无法区分一些相近的主题：
- en: '*“ai”*, *“genai”* and *“datascience”* are all in one cluster,'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*“ai”*、*“genai”*和*“datascience”*都在一个聚类中，'
- en: the same store with *“economics”* and *“politics”*.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同一家商店，包含*“经济学”*和*“政治”*。
- en: We used only embeddings as the features in this example, but if you have any
    additional information (for example, age, gender or country of the user who asked
    the question), you can include it in the model, too.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们仅使用了嵌入作为特征，但如果你有任何额外的信息（例如提问者的年龄、性别或国家），你也可以将其包含在模型中。
- en: Classification
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类
- en: We can use embeddings for classification or regression tasks. For example, you
    can do it to predict customer reviews’ sentiment (classification) or NPS score
    (regression).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将嵌入用于分类或回归任务。例如，你可以用它来预测客户评论的情感（分类）或NPS评分（回归）。
- en: Since classification and regression are supervised learning, you will need to
    have labels. Luckily, we know the topics for our questions and can fit a model
    to predict them.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 由于分类和回归是监督学习，你需要有标签。幸运的是，我们知道问题的主题，并且可以训练模型来预测它们。
- en: I will use a Random Forest Classifier. If you need a quick refresher about Random
    Forests, you can find it [here](https://medium.com/towards-data-science/interpreting-random-forests-638bca8b49ea).
    To assess the classification model’s performance correctly, we will split our
    dataset into train and test sets (80% vs 20%). Then, we can train our model on
    a train set and measure the quality on a test set (questions that the model hasn’t
    seen before).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我将使用随机森林分类器。如果你需要快速复习一下随机森林的相关知识，可以在[这里](https://medium.com/towards-data-science/interpreting-random-forests-638bca8b49ea)找到。为了正确评估分类模型的性能，我们将把数据集分为训练集和测试集（80%对20%）。然后，我们可以在训练集上训练模型，并在测试集上测量质量（模型未见过的问题）。
- en: '[PRE19]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: To estimate the model’s performance, let’s calculate a confusion matrix. In
    an ideal situation, all non-diagonal elements should be 0.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估模型的表现，让我们计算一个混淆矩阵。在理想情况下，所有非对角元素应该为0。
- en: '[PRE20]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](../Images/a401de02b5f824451d9efc6db62491d5.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a401de02b5f824451d9efc6db62491d5.png)'
- en: 'We can see similar results to clusterisation: some topics are easy to classify,
    and accuracy is 100%, for example, *“bicycles”* or *“travel”*, while some others
    are difficult to distinguish (especially *“ai”*).'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到与聚类相似的结果：一些主题很容易分类，准确率达到100%，例如*“自行车”*或*“旅行”*，而其他一些则难以区分（尤其是*“ai”*）。
- en: However, we achieved 91.8% overall accuracy, which is quite good.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们达到了91.8%的总体准确率，这已经相当不错了。
- en: Finding anomalies
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发现异常
- en: We can also use embedding to find anomalies in our data. For example, at the
    t-SNE graph, we saw that some questions are pretty far from their clusters, for
    instance, for the *“travel”* topic. Let’s look at this theme and try to find anomalies.
    We will use [the Isolation Forest algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html)
    for it.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用嵌入来发现数据中的异常。例如，在 t-SNE 图上，我们看到一些问题离它们的聚类非常远，例如，*“旅行”* 话题。让我们看看这个主题，尝试找出异常。我们将使用
    [Isolation Forest 算法](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html)
    来处理它。
- en: '[PRE21]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: So, here we are. We’ve found the most uncommon comment for the travel topic
    ([source](https://travel.stackexchange.com/questions/150735/is-it-safe-to-drink-the-water-from-the-fountains-found-all-over-the-older-parts)).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，到了这里。我们已经找到了关于旅行话题的最不常见的评论 ([来源](https://travel.stackexchange.com/questions/150735/is-it-safe-to-drink-the-water-from-the-fountains-found-all-over-the-older-parts))。
- en: '[PRE22]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Since it talks about water, the embedding of this comment is close to the coffee
    topic where people also discuss water to pour coffee. So, the embedding representation
    is quite reasonable.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它谈论的是水，这条评论的嵌入接近咖啡话题，因为人们也讨论如何倒水来冲泡咖啡。所以，嵌入表示是非常合理的。
- en: We could find it on our t-SNE visualisation and see that it’s actually close
    to the *coffee* cluster.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过 t-SNE 可视化图来发现它实际上靠近 *咖啡* 聚类。
- en: '![](../Images/26e6b84079ede41be03aff139e1c984e.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26e6b84079ede41be03aff139e1c984e.png)'
- en: Graph by author
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 作者图
- en: RAG — Retrieval Augmented Generation
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAG — 检索增强生成
- en: With the recently increased popularity of LLMs, embeddings have been broadly
    used in RAG use cases.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 LLM 最近的广泛流行，嵌入已经在 RAG 用例中得到了广泛应用。
- en: We need Retrieval Augmented Generation when we have a lot of documents (for
    example, all the questions from Stack Exchange), and we can’t pass them all to
    an LLM because
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有大量文档（例如，来自 Stack Exchange 的所有问题），并且无法将它们全部传递给 LLM 时，我们需要 Retrieval Augmented
    Generation，因为
- en: LLMs have limits on the context size (right now, it’s 128K for GPT-4 Turbo).
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM 有上下文大小的限制（目前，GPT-4 Turbo 的上下文限制为 128K）。
- en: We pay for tokens, so it’s more expensive to pass all the information all the
    time.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要为令牌付费，因此每次传递所有信息会更加昂贵。
- en: LLMs show worse performance with a bigger context. You can check [Needle In
    A Haystack — Pressure Testing LLMs](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)
    to learn more details.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM 在处理更大的上下文时表现较差。你可以查看 [Needle In A Haystack — Pressure Testing LLMs](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)
    以了解更多细节。
- en: 'To be able to work with an extensive knowledge base, we can leverage the RAG
    approach:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够处理庞大的知识库，我们可以利用 RAG 方法：
- en: Compute embeddings for all the documents and store them in vector storage.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算所有文档的嵌入并将其存储在向量存储中。
- en: When we get a user request, we can calculate its embedding and retrieve relevant
    documents from the storage for this request.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们收到用户请求时，我们可以计算其嵌入，并从存储中检索相关文档来回答这个请求。
- en: Pass only relevant documents to LLM to get a final answer.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅将相关文档传递给 LLM 以获得最终答案。
- en: To learn more about RAG, don’t hesitate to read my article with much more details
    [here.](/rag-how-to-talk-to-your-data-eaf5469b83b0)
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 若要了解更多关于 RAG 的信息，不要犹豫，阅读我的文章，里面有更多的细节 [在这里.](/rag-how-to-talk-to-your-data-eaf5469b83b0)
- en: Summary
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this article, we’ve discussed text embeddings in much detail. Hopefully,
    now you have a complete and deep understanding of this topic. Here’s a quick recap
    of our journey:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们详细讨论了文本嵌入。希望现在你已经对这个话题有了完整且深入的理解。以下是我们旅程的简要回顾：
- en: Firstly, we went through the evolution of approaches to work with texts.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们回顾了处理文本的各种方法的演变。
- en: Then, we discussed how to understand whether texts have similar meanings to
    each other.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们讨论了如何理解文本之间是否具有相似的意义。
- en: After that, we saw different approaches to text embedding visualisation.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 之后，我们看到了不同的文本嵌入可视化方法。
- en: Finally, we tried to use embeddings as features in different practical tasks
    such as clustering, classification, anomaly detection and RAG.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们尝试将嵌入作为特征应用于不同的实际任务，如聚类、分类、异常检测和 RAG。
- en: Thank you a lot for reading this article. If you have any follow-up questions
    or comments, please leave them in the comments section.
  id: totrans-217
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 非常感谢你阅读这篇文章。如果你有任何后续问题或评论，请在评论区留言。
- en: Reference
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: In this article, I used a dataset from [Stack Exchange Data Dump](https://archive.org/details/stackexchange),
    which is available under [the Creative Commons license](https://creativecommons.org/licenses/by-sa/4.0/).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 本文使用的数据集来自[Stack Exchange 数据集](https://archive.org/details/stackexchange)，该数据集可在[创作共用许可协议](https://creativecommons.org/licenses/by-sa/4.0/)下获取。
- en: 'This article was inspired by the following courses:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的灵感来源于以下课程：
- en: “[Understanding and Applying Text Embeddings”](https://www.deeplearning.ai/short-courses/google-cloud-vertex-ai/)
    by DeepLearning.AI in collaboration with Google Cloud,
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepLearning.AI与Google Cloud合作制作的“[理解与应用文本嵌入](https://www.deeplearning.ai/short-courses/google-cloud-vertex-ai/)”课程，
- en: '[“Vector Databases: From Embeddings to Applications”](https://learn.deeplearning.ai/vector-databases-embeddings-applications/lesson/1/introduction)
    by DeepLearning.AI in collaboration with Weaviate.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepLearning.AI与Weaviate合作制作的“[向量数据库：从嵌入到应用](https://learn.deeplearning.ai/vector-databases-embeddings-applications/lesson/1/introduction)”课程。
