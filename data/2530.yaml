- en: Integrating Multimodal Data into a Large Language Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将多模态数据集成到大型语言模型中
- en: 原文：[https://towardsdatascience.com/integrating-multimodal-data-into-a-large-language-model-d1965b8ab00c?source=collection_archive---------0-----------------------#2024-10-17](https://towardsdatascience.com/integrating-multimodal-data-into-a-large-language-model-d1965b8ab00c?source=collection_archive---------0-----------------------#2024-10-17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/integrating-multimodal-data-into-a-large-language-model-d1965b8ab00c?source=collection_archive---------0-----------------------#2024-10-17](https://towardsdatascience.com/integrating-multimodal-data-into-a-large-language-model-d1965b8ab00c?source=collection_archive---------0-----------------------#2024-10-17)
- en: Developing a context-retrieval, multimodal RAG using advanced parsing, semantic
    & keyword search, and re-ranking
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用高级解析、语义和关键字搜索以及重新排序开发上下文检索多模态RAG
- en: '[](https://medium.com/@umairali.khan?source=post_page---byline--d1965b8ab00c--------------------------------)[![Umair
    Ali Khan](../Images/a6674b39315b20726aad1ba58b64ba12.png)](https://medium.com/@umairali.khan?source=post_page---byline--d1965b8ab00c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d1965b8ab00c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d1965b8ab00c--------------------------------)
    [Umair Ali Khan](https://medium.com/@umairali.khan?source=post_page---byline--d1965b8ab00c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@umairali.khan?source=post_page---byline--d1965b8ab00c--------------------------------)[![Umair
    Ali Khan](../Images/a6674b39315b20726aad1ba58b64ba12.png)](https://medium.com/@umairali.khan?source=post_page---byline--d1965b8ab00c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d1965b8ab00c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d1965b8ab00c--------------------------------)
    [Umair Ali Khan](https://medium.com/@umairali.khan?source=post_page---byline--d1965b8ab00c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d1965b8ab00c--------------------------------)
    ·15 min read·Oct 17, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d1965b8ab00c--------------------------------)
    ·阅读时间：15分钟 ·2024年10月17日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '***If you are not Medium member, you can read the full article from*** [***this
    link***](/integrating-multimodal-data-into-a-large-language-model-d1965b8ab00c?sk=1d82aba6a10afb5f0a47d673307098d8)***.***'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '***如果您不是 Medium 会员，可以通过*** [***此链接***](/integrating-multimodal-data-into-a-large-language-model-d1965b8ab00c?sk=1d82aba6a10afb5f0a47d673307098d8)***阅读完整文章。***'
- en: Large language models (LLMs) have a knowledge cutoff date and cannot answer
    queries to specific data not present in their knowledge base. For instance, LLMs
    cannot answer queries about data regarding a company’s meeting minutes from the
    last year. Similarly, LLMs are prone to hallucinate and provide plausible-looking
    wrong answers.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）具有知识截止日期，无法回答其知识库中不存在的特定数据查询。例如，LLMs无法回答关于某公司去年会议纪要的数据查询。类似地，LLMs容易产生幻觉，给出看似合理但错误的答案。
- en: To overcome this issue, Retrieval Augment Generation (RAG) solutions are becoming
    increasingly popular. The main idea of an RAG is to integrate external documents
    into LLMs and guide its behavior to answer questions only from the external knowledge
    base. This is done by chunking the document(s) into smaller chunks, computing
    each chunk’s embeddings (numerical representations), and storing the embeddings
    as an index in a specialized vector database.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这个问题，检索增强生成（RAG）解决方案越来越受欢迎。RAG的主要思想是将外部文档集成到LLM中，并引导其行为仅从外部知识库中回答问题。这是通过将文档拆分成更小的块，计算每个块的嵌入（数值表示），并将这些嵌入作为索引存储在专门的向量数据库中来实现的。
- en: '![](../Images/7574a6bb8213a8f8d798e7df3aa81427.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7574a6bb8213a8f8d798e7df3aa81427.png)'
- en: 'The RAG workflow: a query is converted to embeddings, matched with a vector
    database by a retrieval model, and combined with retrieved data to produce a response
    via an LLM (image by author).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: RAG工作流程：查询被转换为嵌入，通过检索模型与向量数据库匹配，并与检索到的数据结合，通过LLM生成响应（图像来源：作者）。
- en: Contextual Retrieval RAG
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上下文检索 RAG
- en: 'The process of matching the user’s query with the small chunks in the vector
    database usually works well; however, it has the following issues:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 将用户查询与向量数据库中的小块匹配的过程通常运行良好；然而，它存在以下问题：
