- en: How Does an Image-Text Multimodal Foundation Model Work
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像-文本多模态基础模型如何工作
- en: 原文：[https://towardsdatascience.com/how-does-an-image-text-foundation-model-work-05bc7598e3f2?source=collection_archive---------1-----------------------#2024-06-01](https://towardsdatascience.com/how-does-an-image-text-foundation-model-work-05bc7598e3f2?source=collection_archive---------1-----------------------#2024-06-01)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-does-an-image-text-foundation-model-work-05bc7598e3f2?source=collection_archive---------1-----------------------#2024-06-01](https://towardsdatascience.com/how-does-an-image-text-foundation-model-work-05bc7598e3f2?source=collection_archive---------1-----------------------#2024-06-01)
- en: Learn how an image-text multi-modality model can perform image classification,
    image retrieval, and image captioning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解图像-文本多模态模型如何执行图像分类、图像检索和图像字幕生成
- en: '[](https://jasonweiyi.medium.com/?source=post_page---byline--05bc7598e3f2--------------------------------)[![Wei
    Yi](../Images/24b7a438912082519f24d18e11ac9638.png)](https://jasonweiyi.medium.com/?source=post_page---byline--05bc7598e3f2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--05bc7598e3f2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--05bc7598e3f2--------------------------------)
    [Wei Yi](https://jasonweiyi.medium.com/?source=post_page---byline--05bc7598e3f2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://jasonweiyi.medium.com/?source=post_page---byline--05bc7598e3f2--------------------------------)[![Wei
    Yi](../Images/24b7a438912082519f24d18e11ac9638.png)](https://jasonweiyi.medium.com/?source=post_page---byline--05bc7598e3f2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--05bc7598e3f2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--05bc7598e3f2--------------------------------)
    [Wei Yi](https://jasonweiyi.medium.com/?source=post_page---byline--05bc7598e3f2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--05bc7598e3f2--------------------------------)
    ·23 min read·Jun 1, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--05bc7598e3f2--------------------------------)
    ·阅读时间：23分钟·2024年6月1日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a3306e44f000e5c664851e5fafa2bcf6.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a3306e44f000e5c664851e5fafa2bcf6.png)'
- en: Photo by [Bozhin Karaivanov](https://unsplash.com/@bkaraivanov?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Bozhin Karaivanov](https://unsplash.com/@bkaraivanov?utm_source=medium&utm_medium=referral)
    于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Nowadays, there is a surge of multi-modality foundation models. They understand
    different kinds of data, including text, image, video, audio, and can perform
    tasks that require the knowledge of data from more than one modalities. Have you
    ever wondered how these models work?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，基于多模态的基础模型正在快速增长。它们能够理解不同类型的数据，包括文本、图像、视频、音频，并且可以执行需要多种数据模态知识的任务。你是否曾经想过这些模型是如何工作的？
- en: The key to understand a multi-modality model is to figure out how it establishes
    **the alignment** between different data modalities.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 理解多模态模型的关键是弄清楚它如何建立不同数据模态之间的**对齐**。
- en: This article uses a simple image-text two modality model, called [CoCa](https://arxiv.org/pdf/2205.01917),
    to explain the inner workings of these models. I like CoCa for its intuitive design
    which borrows ideas from a few other multi-modality models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文使用了一个简单的图像-文本双模态模型，名为[CoCa](https://arxiv.org/pdf/2205.01917)，来解释这些模型的内部工作原理。我喜欢CoCa，因为它的设计直观，借鉴了其他一些多模态模型的思想。
- en: Why an image-text model?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么要使用图像-文本模型？
- en: Why do we want an image-text two modality model? Before answering this question,
    let’s first think what a single modality model can and cannot do.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们需要一个图像-文本双模态模型？在回答这个问题之前，让我们先来思考一下单模态模型能做什么，不能做什么。
- en: '**Image-only model**'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**仅图像模型**'
- en: A single image modality model, such as [ResNet](https://arxiv.org/abs/1512.03385),
    learns information from only images, and can perform basic image understanding
    tasks, such as **image classification** — classify an image into one class…
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 单一图像模态模型，例如[ResNet](https://arxiv.org/abs/1512.03385)，只学习图像信息，并可以执行基本的图像理解任务，比如**图像分类**——将图像分类到某一类别中……
