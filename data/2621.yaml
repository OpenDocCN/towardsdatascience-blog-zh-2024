- en: How and Why to Use LLMs for Chunk-Based Information Retrieval
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•ä»¥åŠä¸ºä»€ä¹ˆä½¿ç”¨LLMè¿›è¡ŒåŸºäºå—çš„ä¿¡æ¯æ£€ç´¢
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/how-and-why-to-use-llms-for-chunk-based-information-retrieval-5242f0133b55?source=collection_archive---------4-----------------------#2024-10-28](https://towardsdatascience.com/how-and-why-to-use-llms-for-chunk-based-information-retrieval-5242f0133b55?source=collection_archive---------4-----------------------#2024-10-28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/how-and-why-to-use-llms-for-chunk-based-information-retrieval-5242f0133b55?source=collection_archive---------4-----------------------#2024-10-28](https://towardsdatascience.com/how-and-why-to-use-llms-for-chunk-based-information-retrieval-5242f0133b55?source=collection_archive---------4-----------------------#2024-10-28)
- en: '[](https://medium.com/@peronc79?source=post_page---byline--5242f0133b55--------------------------------)[![Carlo
    Peron](../Images/e6db9521113aa6a2dd43b0b2aa6687b5.png)](https://medium.com/@peronc79?source=post_page---byline--5242f0133b55--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5242f0133b55--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5242f0133b55--------------------------------)
    [Carlo Peron](https://medium.com/@peronc79?source=post_page---byline--5242f0133b55--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@peronc79?source=post_page---byline--5242f0133b55--------------------------------)[![Carlo
    Peron](../Images/e6db9521113aa6a2dd43b0b2aa6687b5.png)](https://medium.com/@peronc79?source=post_page---byline--5242f0133b55--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5242f0133b55--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5242f0133b55--------------------------------)
    [Carlo Peron](https://medium.com/@peronc79?source=post_page---byline--5242f0133b55--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5242f0133b55--------------------------------)
    Â·9 min readÂ·Oct 28, 2024
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5242f0133b55--------------------------------)
    Â·é˜…è¯»æ—¶é•¿9åˆ†é’ŸÂ·2024å¹´10æœˆ28æ—¥
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ea9b1bb4b44a9f0b8ada220d93ecda54.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea9b1bb4b44a9f0b8ada220d93ecda54.png)'
- en: Retrieve pipeline â€” Image by the author
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€ç´¢æµç¨‹ â€” å›¾åƒæ¥è‡ªä½œè€…
- en: In this article, I aim to explain how and why itâ€™s beneficial to use a Large
    Language Model (LLM) for chunk-based information retrieval.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘æ—¨åœ¨è§£é‡Šä¸ºä»€ä¹ˆä»¥åŠå¦‚ä½•ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡ŒåŸºäºå—çš„ä¿¡æ¯æ£€ç´¢ã€‚
- en: I use OpenAIâ€™s GPT-4 model as an example, but this approach can be applied with
    any other LLM, such as those from Hugging Face, Claude, and others.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¥OpenAIçš„GPT-4æ¨¡å‹ä¸ºä¾‹ï¼Œä½†è¿™ç§æ–¹æ³•å¯ä»¥åº”ç”¨äºä»»ä½•å…¶ä»–å¤§è¯­è¨€æ¨¡å‹ï¼Œå¦‚Hugging Faceã€Claudeç­‰æä¾›çš„æ¨¡å‹ã€‚
- en: Everyone can access this [article](https://medium.com/@peronc79/5242f0133b55?sk=aafe7dca2cb777410b6e426321c0b53e)
    for free.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªäººéƒ½å¯ä»¥å…è´¹è®¿é—®è¿™ç¯‡[æ–‡ç« ](https://medium.com/@peronc79/5242f0133b55?sk=aafe7dca2cb777410b6e426321c0b53e)ã€‚
- en: '**Considerations on standard information retrieval**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ ‡å‡†ä¿¡æ¯æ£€ç´¢çš„è€ƒè™‘äº‹é¡¹**'
- en: The primary concept involves having a list of documents (**chunks of text**)
    stored in a database, which could be retrieve based on some filter and conditions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸»è¦æ¦‚å¿µæ˜¯å°†ä¸€ç»„æ–‡æ¡£ï¼ˆ**æ–‡æœ¬å—**ï¼‰å­˜å‚¨åœ¨æ•°æ®åº“ä¸­ï¼Œå¯ä»¥æ ¹æ®æŸäº›è¿‡æ»¤æ¡ä»¶è¿›è¡Œæ£€ç´¢ã€‚
- en: 'Typically, a tool is used to enable hybrid search (such as Azure AI Search,
    LlamaIndex, etc.), which allows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œä½¿ç”¨æŸç§å·¥å…·æ¥å®ç°æ··åˆæœç´¢ï¼ˆä¾‹å¦‚Azure AI Searchã€LlamaIndexç­‰ï¼‰ï¼Œè¯¥å·¥å…·å¯ä»¥ï¼š
- en: performing a text-based search using term frequency algorithms like TF-IDF (e.g.,
    BM25);
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¯¸å¦‚TF-IDFï¼ˆä¾‹å¦‚BM25ï¼‰ä¹‹ç±»çš„æœ¯è¯­é¢‘ç‡ç®—æ³•æ‰§è¡ŒåŸºäºæ–‡æœ¬çš„æœç´¢ï¼›
- en: conducting a vector-based search, which identifies similar concepts even when
    different terms are used, by calculating vector distances (typically cosine similarity);
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿›è¡ŒåŸºäºå‘é‡çš„æœç´¢ï¼Œè®¡ç®—å‘é‡ä¹‹é—´çš„è·ç¦»ï¼ˆé€šå¸¸æ˜¯ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰ï¼Œå³ä½¿ä½¿ç”¨ä¸åŒçš„æœ¯è¯­ï¼Œä¹Ÿèƒ½è¯†åˆ«ç›¸ä¼¼çš„æ¦‚å¿µï¼›
- en: combining elements from steps 1 and 2, weighting them to highlight the most
    relevant results.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç»“åˆæ­¥éª¤1å’Œæ­¥éª¤2çš„å…ƒç´ ï¼Œé€šè¿‡åŠ æƒçªå‡ºæœ€ç›¸å…³çš„ç»“æœã€‚
- en: '![](../Images/6f4f4ba0a88c11c7dc09513356aa58d5.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f4f4ba0a88c11c7dc09513356aa58d5.png)'
- en: Figure 1- Default hybrid search pipeline â€” Image by the author
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾1 - é»˜è®¤æ··åˆæœç´¢æµç¨‹ â€” å›¾åƒæ¥è‡ªä½œè€…
- en: 'Figure 1 shows the classic retrieval pipeline:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾1æ˜¾ç¤ºäº†ç»å…¸çš„æ£€ç´¢æµç¨‹ï¼š
- en: 'the user asks the system a question: â€œI would like to talk about Parisâ€;'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”¨æˆ·å‘ç³»ç»Ÿæå‡ºé—®é¢˜ï¼šâ€œæˆ‘æƒ³è°ˆè°ˆå·´é»â€ï¼›
- en: the system receives the question, converts it into an embedding vector (using
    the same model applied in the ingestion phase), and finds the chunks with the
    smallest distances;
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç³»ç»Ÿæ¥æ”¶é—®é¢˜ï¼Œå°†å…¶è½¬æ¢ä¸ºåµŒå…¥å‘é‡ï¼ˆä½¿ç”¨ä¸æ‘„å–é˜¶æ®µç›¸åŒçš„æ¨¡å‹ï¼‰ï¼Œå¹¶æ‰¾åˆ°è·ç¦»æœ€å°çš„æ–‡æœ¬å—ï¼›
- en: the system also performs a text-based search based on frequency;
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç³»ç»Ÿè¿˜æ‰§è¡ŒåŸºäºé¢‘ç‡çš„æ–‡æœ¬æœç´¢ï¼›
- en: the chunks returned from both processes undergo further evaluation and are reordered
    based on a ranking formula.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¥è‡ªä¸¤ä¸ªè¿‡ç¨‹çš„å—ç»è¿‡è¿›ä¸€æ­¥è¯„ä¼°ï¼Œå¹¶æ ¹æ®æ’åå…¬å¼é‡æ–°æ’åºã€‚
- en: 'This solution achieves good results but has some limitations:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè§£å†³æ–¹æ¡ˆå–å¾—äº†è‰¯å¥½çš„æ•ˆæœï¼Œä½†ä¹Ÿæœ‰ä¸€äº›å±€é™æ€§ï¼š
- en: not all relevant chunks are always retrieved;
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¹¶éæ‰€æœ‰ç›¸å…³çš„å—éƒ½ä¼šè¢«æ£€ç´¢åˆ°ï¼›
- en: sometime some chunks contain anomalies that affect the final response.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ‰æ—¶ï¼Œä¸€äº›å—åŒ…å«å¼‚å¸¸å€¼ï¼Œå½±å“æœ€ç»ˆçš„å›ç­”ã€‚
- en: '**An example of a typical retrieval issue**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**å…¸å‹æ£€ç´¢é—®é¢˜çš„ç¤ºä¾‹**'
- en: Letâ€™s consider the â€œdocumentsâ€ array, which represents an example of a knowledge
    base that could lead to incorrect chunk selection.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è€ƒè™‘â€œdocumentsâ€æ•°ç»„ï¼Œå®ƒä»£è¡¨äº†ä¸€ä¸ªçŸ¥è¯†åº“çš„ç¤ºä¾‹ï¼Œè¿™ä¸ªçŸ¥è¯†åº“å¯èƒ½å¯¼è‡´ä¸æ­£ç¡®çš„å—é€‰æ‹©ã€‚
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Letâ€™s assume we have a RAG system, consisting of a vector database with hybrid
    search capabilities and an LLM-based prompt, to which the user poses the following
    question: â€œI need to know something about topic B.â€'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªRAGç³»ç»Ÿï¼ŒåŒ…å«ä¸€ä¸ªå…·æœ‰æ··åˆæœç´¢èƒ½åŠ›çš„å‘é‡æ•°æ®åº“å’ŒåŸºäºLLMçš„æç¤ºï¼Œç”¨æˆ·å‘å…¶æå‡ºå¦‚ä¸‹é—®é¢˜ï¼šâ€œæˆ‘éœ€è¦äº†è§£ä¸€äº›å…³äºè¯é¢˜Bçš„å†…å®¹ã€‚â€
- en: As shown in Figure 2, the search also returns an incorrect chunk that, while
    semantically relevant, is not suitable for answering the question and, in some
    cases, could even confuse the LLM tasked with providing a response.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å›¾2æ‰€ç¤ºï¼Œæœç´¢è¿˜è¿”å›äº†ä¸€ä¸ªä¸æ­£ç¡®çš„å—ï¼Œè™½ç„¶è¯­ä¹‰ä¸Šç›¸å…³ï¼Œä½†å¹¶ä¸é€‚åˆå›ç­”é—®é¢˜ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œç”šè‡³å¯èƒ½ä¼šè®©è´Ÿè´£æä¾›ç­”æ¡ˆçš„LLMæ„Ÿåˆ°å›°æƒ‘ã€‚
- en: '![](../Images/356ad8ff68ceb28acca4d8a23ec584c1.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/356ad8ff68ceb28acca4d8a23ec584c1.png)'
- en: Figure 2 â€” Example of information retrieval that can lead to errors â€” Image
    by the author
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2 â€” å¯èƒ½å¯¼è‡´é”™è¯¯çš„æ£€ç´¢ç¤ºä¾‹ â€” ä½œè€…æä¾›çš„å›¾åƒ
- en: In this example, the user requests information about â€œ*topic B*,â€ and the search
    returns chunks that include â€œ*This document expands on topic H. It also talks
    about topic B*â€ and â€œ*Insights related to topic B can be found here.*â€ as well
    as the chunk stating, â€œ*Nothing about topic B are given*â€.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œç”¨æˆ·è¯·æ±‚å…³äºâ€œ*è¯é¢˜B*â€çš„ä¿¡æ¯ï¼Œæœç´¢è¿”å›çš„å—åŒ…æ‹¬â€œ*è¯¥æ–‡æ¡£æ‰©å±•äº†è¯é¢˜Hï¼Œä¹Ÿæ¶‰åŠè¯é¢˜B*â€ä»¥åŠâ€œ*ä¸è¯é¢˜Bç›¸å…³çš„è§è§£å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°*â€ï¼Œè¿˜æœ‰ä¸€ä¸ªå—æŒ‡å‡ºï¼Œâ€œ*æ²¡æœ‰å…³äºè¯é¢˜Bçš„ä¿¡æ¯*â€ã€‚
- en: While this is the expected behavior of hybrid search (as chunks reference â€œ*topic
    B*â€), it is not the desired outcome, as the third chunk is returned without recognizing
    that it isnâ€™t helpful for answering the question.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡è¿™ç¬¦åˆæ··åˆæœç´¢çš„é¢„æœŸè¡Œä¸ºï¼ˆå› ä¸ºå—å¼•ç”¨äº†â€œ*è¯é¢˜B*â€ï¼‰ï¼Œä½†è¿™å¹¶ä¸æ˜¯æœŸæœ›çš„ç»“æœï¼Œå› ä¸ºç¬¬ä¸‰ä¸ªå—è¢«è¿”å›æ—¶å¹¶æ²¡æœ‰è¯†åˆ«åˆ°å®ƒå¯¹äºå›ç­”é—®é¢˜å¹¶æ— å¸®åŠ©ã€‚
- en: The retrieval didnâ€™t produce the intended result, not only because the BM25
    search found the term â€œ*topic B*â€ in the third Chunk but also because the vector
    search yielded a high cosine similarity.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€ç´¢æ²¡æœ‰äº§ç”Ÿé¢„æœŸçš„ç»“æœï¼Œä¸ä»…ä»…å› ä¸ºBM25æœç´¢åœ¨ç¬¬ä¸‰ä¸ªå—ä¸­æ‰¾åˆ°äº†â€œ*è¯é¢˜B*â€è¿™ä¸€æœ¯è¯­ï¼Œè¿˜å› ä¸ºå‘é‡æœç´¢å¾—åˆ°äº†è¾ƒé«˜çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚
- en: To understand this, refer to Figure 3, which shows the cosine similarity values
    of the chunks relative to the question, using OpenAIâ€™s text-embedding-ada-002
    model for embeddings.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ç†è§£è¿™ä¸€ç‚¹ï¼Œè¯·å‚è§å›¾3ï¼Œå®ƒæ˜¾ç¤ºäº†ä½¿ç”¨OpenAIçš„text-embedding-ada-002æ¨¡å‹è¿›è¡ŒåµŒå…¥æ—¶ï¼Œå—ç›¸å¯¹äºé—®é¢˜çš„ä½™å¼¦ç›¸ä¼¼åº¦å€¼ã€‚
- en: '![](../Images/c42a17cff143c55984745f79381a2c9e.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c42a17cff143c55984745f79381a2c9e.png)'
- en: Figure 3 â€” Cosine similarity with text-embedding-ada-002- Image by the author
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾3 â€” ä½¿ç”¨text-embedding-ada-002çš„ä½™å¼¦ç›¸ä¼¼åº¦ â€” ä½œè€…æä¾›çš„å›¾åƒ
- en: It is evident that the cosine similarity value for â€œChunk 9â€ is among the highest,
    and that between this chunk and chunk 10, which references â€œ*topic B*,â€ there
    is also chunk 1, which does not mention â€œ*topic B*â€.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆæ˜æ˜¾ï¼Œâ€œå—9â€çš„ä½™å¼¦ç›¸ä¼¼åº¦å€¼æ˜¯æœ€é«˜çš„ï¼Œè€Œä¸”åœ¨è¯¥å—ä¸å¼•ç”¨â€œ*è¯é¢˜B*â€çš„å—10ä¹‹é—´ï¼Œè¿˜æœ‰ä¸€ä¸ªå—1ï¼Œå®ƒæ²¡æœ‰æåˆ°â€œ*è¯é¢˜B*â€ã€‚
- en: This situation remains unchanged even when measuring distance using a different
    method, as seen in the case of Minkowski distance.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿ä½¿ç”¨ä¸åŒçš„æ–¹æ³•è¡¡é‡è·ç¦»ï¼Œè¿™ç§æƒ…å†µä¹Ÿä¸ä¼šæ”¹å˜ï¼Œæ­£å¦‚åœ¨é—µå¯å¤«æ–¯åŸºè·ç¦»çš„æƒ…å†µä¸‹æ‰€è§ã€‚
- en: '**Utilizing LLMs for Information Retrieval: An Example**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**åˆ©ç”¨LLMè¿›è¡Œä¿¡æ¯æ£€ç´¢ï¼šä¸€ä¸ªç¤ºä¾‹**'
- en: The solution I will describe is inspired by what has been published in my GitHub
    repository [https://github.com/peronc/LLMRetriever/](https://github.com/peronc/LLMRetriever/).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†æè¿°çš„è§£å†³æ–¹æ¡ˆçµæ„Ÿæ¥æºäºæˆ‘åœ¨GitHubä»“åº“ä¸­å‘å¸ƒçš„å†…å®¹ [https://github.com/peronc/LLMRetriever/](https://github.com/peronc/LLMRetriever/)ã€‚
- en: The idea is to have the LLM analyze which chunks are useful for answering the
    userâ€™s question, not by ranking the returned chunks (as in the case of RankGPT)
    but by directly evaluating all the available chunks.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæƒ³æ³•æ˜¯è®©LLMåˆ†æå“ªäº›å—å¯¹å›ç­”ç”¨æˆ·çš„é—®é¢˜æœ‰ç”¨ï¼Œè€Œä¸æ˜¯é€šè¿‡æ’åè¿”å›çš„å—ï¼ˆå¦‚RankGPTçš„æƒ…å†µï¼‰ï¼Œè€Œæ˜¯ç›´æ¥è¯„ä¼°æ‰€æœ‰å¯ç”¨çš„å—ã€‚
- en: '![](../Images/ea9b1bb4b44a9f0b8ada220d93ecda54.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea9b1bb4b44a9f0b8ada220d93ecda54.png)'
- en: Figure 4- LLM Retrieve pipeline â€” Image by the author
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4 - LLMæ£€ç´¢ç®¡é“ â€” ä½œè€…æä¾›çš„å›¾åƒ
- en: In summary, as shown in Figure 4, the system receives a list of documents to
    analyze, which can come from any data source, such as file storage, relational
    databases, or vector databases.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“æ¥è¯´ï¼Œå¦‚å›¾4æ‰€ç¤ºï¼Œç³»ç»Ÿæ¥æ”¶ä¸€ç»„éœ€è¦åˆ†æçš„æ–‡æ¡£ï¼Œè¿™äº›æ–‡æ¡£å¯ä»¥æ¥è‡ªä»»ä½•æ•°æ®æºï¼Œå¦‚æ–‡ä»¶å­˜å‚¨ã€å…³ç³»æ•°æ®åº“æˆ–å‘é‡æ•°æ®åº“ã€‚
- en: The chunks are divided into groups and processed in parallel by a number of
    threads proportional to the total amount of chunks.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å—è¢«åˆ†æˆç»„ï¼Œå¹¶é€šè¿‡ä¸å—æ€»é‡æˆæ¯”ä¾‹çš„çº¿ç¨‹æ•°å¹¶è¡Œå¤„ç†ã€‚
- en: The logic for each thread includes a loop that iterates through the input chunks,
    calling an OpenAI prompt for each one to check its relevance to the userâ€™s question.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªçº¿ç¨‹çš„é€»è¾‘åŒ…æ‹¬ä¸€ä¸ªå¾ªç¯ï¼Œè¯¥å¾ªç¯éå†è¾“å…¥çš„å—ï¼Œä¸ºæ¯ä¸ªå—è°ƒç”¨OpenAIæç¤ºï¼Œä»¥æ£€æŸ¥å®ƒä¸ç”¨æˆ·é—®é¢˜çš„ç›¸å…³æ€§ã€‚
- en: 'The prompt returns the chunk along with a boolean value: *true* if it is relevant
    and *false* if it is not.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æç¤ºè¿”å›è¯¥å—ä»¥åŠä¸€ä¸ªå¸ƒå°”å€¼ï¼š*true* å¦‚æœå®ƒç›¸å…³ï¼Œ*false* å¦‚æœå®ƒä¸ç›¸å…³ã€‚
- en: '**Letsâ€™go coding ğŸ˜Š**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®©æˆ‘ä»¬å¼€å§‹ç¼–ç å§ ğŸ˜Š**'
- en: To explain the code, I will simplify by using the chunks present in the *documents*
    array (I will reference a real case in the conclusions).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç®€åŒ–ä»£ç çš„è§£é‡Šï¼Œæˆ‘å°†ä½¿ç”¨*documents*æ•°ç»„ä¸­å­˜åœ¨çš„å—ï¼ˆåœ¨ç»“è®ºéƒ¨åˆ†æˆ‘å°†å¼•ç”¨ä¸€ä¸ªå®é™…æ¡ˆä¾‹ï¼‰ã€‚
- en: First of all, I import the necessary standard libraries, including os, langchain,
    and dotenv.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘å¯¼å…¥å¿…è¦çš„æ ‡å‡†åº“ï¼ŒåŒ…æ‹¬osã€langchainå’Œdotenvã€‚
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, I import my LLMRetrieverLib/llm_retrieve.py class, which provides several
    static methods essential for performing the analysis.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘å¯¼å…¥äº†æˆ‘çš„LLMRetrieverLib/llm_retrieve.pyç±»ï¼Œå®ƒæä¾›äº†å‡ ä¸ªæ‰§è¡Œåˆ†ææ‰€éœ€çš„é‡è¦é™æ€æ–¹æ³•ã€‚
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Following that, I need to import the necessary variables required for utilizing
    Azure OpenAI GPT-4o model.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘éœ€è¦å¯¼å…¥ä½¿ç”¨Azure OpenAI GPT-4æ¨¡å‹æ‰€éœ€çš„å¿…è¦å˜é‡ã€‚
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Next, I proceed with the initialization of the LLM.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ç»§ç»­åˆå§‹åŒ–LLMã€‚
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We are ready to begin: the user asks a question to gather additional information
    about *Topic B*.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å‡†å¤‡å¼€å§‹ï¼šç”¨æˆ·æå‡ºé—®é¢˜ä»¥æ”¶é›†æœ‰å…³*ä¸»é¢˜B*çš„é¢å¤–ä¿¡æ¯ã€‚
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: At this point, the search for relevant chunks begins, and to do this, I use
    the function `llm_retrieve.process_chunks_in_parallel` from the `LLMRetrieverLib/retriever.py`
    library, which is also found in the same repository.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œç›¸å…³å—çš„æœç´¢å¼€å§‹äº†ï¼Œä¸ºæ­¤ï¼Œæˆ‘ä½¿ç”¨äº† `LLMRetrieverLib/retriever.py` åº“ä¸­çš„å‡½æ•° `llm_retrieve.process_chunks_in_parallel`ï¼Œè¯¥åº“ä¹Ÿä½äºåŒä¸€ä¸ªä»£ç åº“ä¸­ã€‚
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: To optimize performance, the function `llm_retrieve.process_chunks_in_parallel`
    employs multi-threading to distribute chunk analysis across multiple threads.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä¼˜åŒ–æ€§èƒ½ï¼Œå‡½æ•° `llm_retrieve.process_chunks_in_parallel` é‡‡ç”¨å¤šçº¿ç¨‹æ¥åˆ†é…å—åˆ†æåˆ°å¤šä¸ªçº¿ç¨‹ã€‚
- en: The main idea is to assign each thread a subset of chunks extracted from the
    database and have each thread analyze the relevance of those chunks based on the
    userâ€™s question.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸»è¦æ€æƒ³æ˜¯ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ†é…ä»æ•°æ®åº“ä¸­æå–çš„ä¸€ä¸ªå­é›†çš„å—ï¼Œå¹¶è®©æ¯ä¸ªçº¿ç¨‹æ ¹æ®ç”¨æˆ·çš„é—®é¢˜åˆ†æè¿™äº›å—çš„ç›¸å…³æ€§ã€‚
- en: 'At the end of the processing, the returned chunks are exactly as expected:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å¤„ç†ç»“æŸæ—¶ï¼Œè¿”å›çš„å—å®Œå…¨ç¬¦åˆé¢„æœŸï¼š
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, I ask the LLM to provide an answer to the userâ€™s question:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘è¯·æ±‚LLMä¸ºç”¨æˆ·çš„é—®é¢˜æä¾›ç­”æ¡ˆï¼š
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Below is the LLMâ€™s response, which is trivial since the content of the chunks,
    while relevant, is not exhaustive on the topic of Topic B:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯LLMçš„å›ç­”ï¼Œè™½ç„¶å†…å®¹ç›¸å…³ï¼Œä½†ç”±äºè¿™äº›å—è™½ç„¶ç›¸å…³ï¼Œä½†åœ¨*ä¸»é¢˜B*çš„å†…å®¹ä¸Šå¹¶ä¸å……åˆ†ï¼Œå› æ­¤å›ç­”æ˜¾å¾—æœ‰äº›ç®€å•ï¼š
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Scoring Scenario**'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¯„åˆ†åœºæ™¯**'
- en: Now letâ€™s try asking the same question but using an approach based on scoring.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬å°è¯•æå‡ºç›¸åŒçš„é—®é¢˜ï¼Œä½†ä½¿ç”¨åŸºäºè¯„åˆ†çš„æ–¹æ³•ã€‚
- en: I ask the LLM to assign a score from 1 to 10 to evaluate the relevance between
    each chunk and the question, considering only those with a relevance higher than
    5.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è¯·æ±‚LLMä¸ºæ¯ä¸ªå—åˆ†é…1åˆ°10çš„è¯„åˆ†ï¼Œä»¥è¯„ä¼°æ¯ä¸ªå—ä¸é—®é¢˜çš„ç›¸å…³æ€§ï¼Œä»…è€ƒè™‘ç›¸å…³æ€§é«˜äº5çš„å—ã€‚
- en: To do this, I call the function `llm_retriever.process_chunks_in_parallel`,
    passing three additional parameters that indicate, respectively, that scoring
    will be applied, that the threshold for being considered valid must be greater
    than or equal to 5, and that I want a printout of the chunks with their respective
    scores.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºæ­¤ï¼Œæˆ‘è°ƒç”¨å‡½æ•° `llm_retriever.process_chunks_in_parallel`ï¼Œä¼ å…¥ä¸‰ä¸ªé¢å¤–çš„å‚æ•°ï¼Œåˆ†åˆ«è¡¨ç¤ºï¼šå°†åº”ç”¨è¯„åˆ†ï¼Œå¿…é¡»å¤§äºæˆ–ç­‰äº5æ‰èƒ½è¢«è®¤ä¸ºæœ‰æ•ˆï¼Œå¹¶ä¸”æˆ‘å¸Œæœ›æ‰“å°å‡ºå¸¦æœ‰å„è‡ªè¯„åˆ†çš„å—ã€‚
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The retrieval phase with scoring produces the following result:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¦è¯„åˆ†çš„æ£€ç´¢é˜¶æ®µäº§ç”Ÿäº†å¦‚ä¸‹ç»“æœï¼š
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Itâ€™s the same as before, but with an interesting score ğŸ˜Š.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœä¸ä¹‹å‰ç›¸åŒï¼Œä½†è¯„åˆ†å¾ˆæœ‰è¶£ ğŸ˜Šã€‚
- en: 'Finally, I once again ask the LLM to provide an answer to the userâ€™s question,
    and the result is similar to the previous one:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘å†æ¬¡è¯·æ±‚LLMä¸ºç”¨æˆ·çš„é—®é¢˜æä¾›ç­”æ¡ˆï¼Œç»“æœä¸ä¹‹å‰çš„ç±»ä¼¼ï¼š
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Considerations**'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„äº‹é¡¹**'
- en: This retrieval approach has emerged as a necessity following some previous experiences.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ£€ç´¢æ–¹æ³•æ˜¯é€šè¿‡ä¸€äº›å…ˆå‰çš„ç»éªŒé€æ­¥å‘å±•å‡ºæ¥çš„ã€‚
- en: I have noticed that pure vector-based searches produce useful results but are
    often insufficient when the embedding is performed in a language other than English.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ³¨æ„åˆ°ï¼Œçº¯ç²¹åŸºäºå‘é‡çš„æœç´¢è™½ç„¶èƒ½äº§ç”Ÿæœ‰ç”¨çš„ç»“æœï¼Œä½†å½“åµŒå…¥åœ¨éè‹±è¯­è¯­è¨€ä¸­æ—¶ï¼Œé€šå¸¸ä¸å¤Ÿå……åˆ†ã€‚
- en: 'Using OpenAI with sentences in Italian makes it clear that the tokenization
    of terms is often incorrect; for example, the term â€œ*canzone*,â€ which means â€œ*song*â€
    in Italian, gets tokenized into two distinct words: â€œ*can*â€ and â€œ*zone*â€.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨OpenAIå¤„ç†æ„å¤§åˆ©è¯­å¥å­æ—¶ï¼Œæ˜æ˜¾å‘ç°æœ¯è¯­çš„æ ‡è®°åŒ–å¸¸å¸¸ä¸æ­£ç¡®ï¼›ä¾‹å¦‚ï¼Œâ€œ*canzone*â€ä¸€è¯åœ¨æ„å¤§åˆ©è¯­ä¸­æ„å‘³ç€â€œ*æ­Œæ›²*â€ï¼Œä½†å®ƒè¢«é”™è¯¯åœ°æ ‡è®°ä¸ºä¸¤ä¸ªä¸åŒçš„è¯ï¼šâ€œ*can*â€å’Œâ€œ*zone*â€ã€‚
- en: This leads to the construction of an embedding array that is far from what was
    intended.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯¼è‡´äº†åµŒå…¥æ•°ç»„çš„æ„å»ºè¿œæœªè¾¾åˆ°é¢„æœŸæ•ˆæœã€‚
- en: In cases like this, hybrid search, which also incorporates term frequency counting,
    leads to improved results, but they are not always as expected.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ··åˆæœç´¢ï¼ˆåŒæ—¶ç»“åˆäº†æœ¯è¯­é¢‘ç‡è®¡æ•°ï¼‰èƒ½å¤Ÿæ”¹å–„ç»“æœï¼Œä½†å®ƒä»¬å¹¶ä¸æ€»æ˜¯å¦‚é¢„æœŸèˆ¬å‡†ç¡®ã€‚
- en: 'So, this retrieval methodology can be utilized in the following ways:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œè¿™ç§æ£€ç´¢æ–¹æ³•å¯ä»¥ä»¥ä¸‹åˆ—æ–¹å¼ä½¿ç”¨ï¼š
- en: '**as the primary search method:** where the database is queried for all chunks
    or a subset based on a filter (e.g., a metadata filter);'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä½œä¸ºä¸»è¦æœç´¢æ–¹æ³•ï¼š** åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œæ•°æ®åº“ä¼šæ ¹æ®è¿‡æ»¤æ¡ä»¶ï¼ˆä¾‹å¦‚å…ƒæ•°æ®è¿‡æ»¤å™¨ï¼‰æŸ¥è¯¢æ‰€æœ‰ç‰‡æ®µæˆ–å…¶å­é›†ï¼›'
- en: '**as a refinement in the case of hybrid search:** (this is the same approach
    used by RankGPT) in this way, the hybrid search can extract a large number of
    chunks, and the system can filter them so that only the relevant ones reach the
    LLM while also adhering to the input token limit;'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä½œä¸ºæ··åˆæœç´¢ä¸­çš„ä¸€ç§ä¼˜åŒ–ï¼š**ï¼ˆè¿™æ˜¯RankGPTä½¿ç”¨çš„ç›¸åŒæ–¹æ³•ï¼‰é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ··åˆæœç´¢å¯ä»¥æå–å¤§é‡ç‰‡æ®µï¼Œç³»ç»Ÿå¯ä»¥è¿‡æ»¤å®ƒä»¬ï¼Œç¡®ä¿åªæœ‰ç›¸å…³çš„ç‰‡æ®µä¼ é€’åˆ°LLMï¼Œå¹¶ä¸”éµå®ˆè¾“å…¥ä»¤ç‰Œçš„é™åˆ¶ï¼›'
- en: '**as a fallback:** in situations where a hybrid search does not yield the desired
    results, all chunks can be analyzed.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆï¼š** åœ¨æ··åˆæœç´¢æ²¡æœ‰å¾—åˆ°æœŸæœ›ç»“æœçš„æƒ…å†µä¸‹ï¼Œå¯ä»¥åˆ†ææ‰€æœ‰ç‰‡æ®µã€‚'
- en: '**Letâ€™s discuss costs and performance**'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®©æˆ‘ä»¬è®¨è®ºæˆæœ¬å’Œæ€§èƒ½**'
- en: Of course, all that glitters is not gold, as one must consider response times
    and costs.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œé—ªé—ªå‘å…‰çš„å¹¶ä¸éƒ½æ˜¯é‡‘å­ï¼Œå› ä¸ºå¿…é¡»è€ƒè™‘å“åº”æ—¶é—´å’Œæˆæœ¬ã€‚
- en: In a real use case, I retrieved the chunks from a relational database consisting
    of 95 text segments semantically split using my `LLMChunkizerLib/chunkizer.py`
    library from two Microsoft Word documents, totaling 33 pages.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸€ä¸ªå®é™…çš„ä½¿ç”¨æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»ä¸€ä¸ªå…³ç³»å‹æ•°æ®åº“ä¸­æ£€ç´¢äº†ç”±æˆ‘çš„`LLMChunkizerLib/chunkizer.py`åº“è¯­ä¹‰åˆ†å‰²çš„95ä¸ªæ–‡æœ¬ç‰‡æ®µï¼Œè¿™äº›ç‰‡æ®µæ¥è‡ªä¸¤ä¸ªMicrosoft
    Wordæ–‡æ¡£ï¼Œæ€»å…±33é¡µã€‚
- en: The analysis of the relevance of the 95 chunks to the question was conducted
    by calling OpenAI's APIs from a local PC with non-guaranteed bandwidth, averaging
    around 10Mb, resulting in response times that varied from 7 to 20 seconds.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹95ä¸ªç‰‡æ®µä¸é—®é¢˜ç›¸å…³æ€§çš„åˆ†ææ˜¯é€šè¿‡åœ¨æœ¬åœ°PCä¸Šè°ƒç”¨OpenAIçš„APIå®Œæˆçš„ï¼Œè¯¥PCçš„å¸¦å®½å¹¶ä¸ä¿è¯ï¼Œå¹³å‡çº¦ä¸º10Mbï¼Œå¯¼è‡´å“åº”æ—¶é—´ä»7ç§’åˆ°20ç§’ä¸ç­‰ã€‚
- en: Naturally, on a cloud system or by using local LLMs on GPUs, these times can
    be significantly reduced.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªç„¶åœ°ï¼Œåœ¨äº‘ç³»ç»Ÿä¸­ï¼Œæˆ–é€šè¿‡åœ¨GPUä¸Šä½¿ç”¨æœ¬åœ°LLMï¼Œè¿™äº›æ—¶é—´å¯ä»¥æ˜¾è‘—å‡å°‘ã€‚
- en: 'I believe that considerations regarding response times are highly subjective:
    in some cases, it is acceptable to take longer to provide a correct answer, while
    in others, it is essential not to keep users waiting too long.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è®¤ä¸ºï¼Œå…³äºå“åº”æ—¶é—´çš„è€ƒè™‘éå¸¸ä¸»è§‚ï¼šåœ¨æŸäº›æƒ…å†µä¸‹ï¼ŒèŠ±æ›´å¤šæ—¶é—´æ¥æä¾›æ­£ç¡®ç­”æ¡ˆæ˜¯å¯ä»¥æ¥å—çš„ï¼Œè€Œåœ¨å…¶ä»–æƒ…å†µä¸‹ï¼Œè‡³å…³é‡è¦çš„æ˜¯ä¸è¦è®©ç”¨æˆ·ç­‰å¾…å¤ªä¹…ã€‚
- en: Similarly, considerations about costs are also quite subjective, as one must
    take a broader perspective to evaluate whether it is more important to provide
    as accurate answers as possible or if some errors are acceptable.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼åœ°ï¼Œå…³äºæˆæœ¬çš„è€ƒè™‘ä¹Ÿéå¸¸ä¸»è§‚ï¼Œå› ä¸ºå¿…é¡»ä»æ›´å¹¿æ³›çš„è§’åº¦è¯„ä¼°ï¼Œæ˜¯å¦æä¾›å°½å¯èƒ½å‡†ç¡®çš„ç­”æ¡ˆæ›´ä¸ºé‡è¦ï¼Œè¿˜æ˜¯ä¸€äº›é”™è¯¯æ˜¯å¯ä»¥æ¥å—çš„ã€‚
- en: In certain fields, the damage to oneâ€™s reputation caused by incorrect or missing
    answers can outweigh the expense of tokens.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æŸäº›é¢†åŸŸï¼Œç”±äºä¸æ­£ç¡®æˆ–é—æ¼çš„ç­”æ¡ˆå¯¼è‡´çš„å£°èª‰æŸå®³ï¼Œå¯èƒ½è¶…è¿‡äº†ä»¤ç‰Œçš„è´¹ç”¨ã€‚
- en: Furthermore, even though the costs of OpenAI and other providers have been steadily
    decreasing in recent years, those who already have a GPU-based infrastructure,
    perhaps due to the need to handle sensitive or confidential data, will likely
    prefer to use a local LLM.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œå°½ç®¡OpenAIå’Œå…¶ä»–æä¾›å•†çš„æˆæœ¬åœ¨è¿‘å¹´æ¥ä¸€ç›´åœ¨ç¨³æ­¥ä¸‹é™ï¼Œä½†é‚£äº›å·²ç»æ‹¥æœ‰åŸºäºGPUçš„åŸºç¡€è®¾æ–½çš„äººï¼ˆå¯èƒ½æ˜¯ç”±äºéœ€è¦å¤„ç†æ•æ„Ÿæˆ–æœºå¯†æ•°æ®ï¼‰å¯èƒ½æ›´å€¾å‘äºä½¿ç”¨æœ¬åœ°çš„LLMã€‚
- en: '**Conclusions**'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç»“è®º**'
- en: In conclusion, I hope to have provided my perspective on how retrieval can be
    approached.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä¹‹ï¼Œæˆ‘å¸Œæœ›èƒ½æä¾›æˆ‘çš„è§†è§’ï¼Œåˆ†äº«å¦‚ä½•è¿›è¡Œæ£€ç´¢çš„æ€è€ƒã€‚
- en: If nothing else, I aim to be helpful and perhaps inspire others to explore new
    methods in their own work.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ²¡æœ‰åˆ«çš„ï¼Œæˆ‘çš„ç›®æ ‡æ˜¯æä¾›å¸®åŠ©ï¼Œå¹¶å¯èƒ½æ¿€åŠ±ä»–äººåœ¨è‡ªå·±çš„å·¥ä½œä¸­æ¢ç´¢æ–°æ–¹æ³•ã€‚
- en: Remember, the world of information retrieval is vast, and with a little creativity
    and the right tools, we can uncover knowledge in ways we never imagined!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: è®°ä½ï¼Œä¿¡æ¯æ£€ç´¢çš„ä¸–ç•Œå¹¿é˜”æ— å ï¼Œåªè¦ç¨åŠ åˆ›æ„å’Œå€ŸåŠ©åˆé€‚çš„å·¥å…·ï¼Œæˆ‘ä»¬å°±èƒ½ä»¥æˆ‘ä»¬ä»æœªæƒ³è±¡è¿‡çš„æ–¹å¼å‘æ˜çŸ¥è¯†ï¼
- en: If youâ€™d like to discuss this further, feel free to connect with me on [LinkedIn](https://www.linkedin.com/in/carlo-peron)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³è¿›ä¸€æ­¥è®¨è®ºï¼Œæ¬¢è¿é€šè¿‡[LinkedIn](https://www.linkedin.com/in/carlo-peron)ä¸æˆ‘è”ç³»ã€‚
- en: 'GitHub repositories can be found here:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub ä»“åº“å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ï¼š
- en: â€¢ [https://github.com/peronc/LLMRetriever/](https://github.com/peronc/LLMRetriever/)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: â€¢ [https://github.com/peronc/LLMRetriever/](https://github.com/peronc/LLMRetriever/)
- en: â€¢ [https://github.com/peronc/LLMChunkizer/](https://github.com/peronc/LLMChunkizer/)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: â€¢ [https://github.com/peronc/LLMChunkizer/](https://github.com/peronc/LLMChunkizer/)
