- en: How and Why to Use LLMs for Chunk-Based Information Retrieval
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何以及为什么使用LLM进行基于块的信息检索
- en: 原文：[https://towardsdatascience.com/how-and-why-to-use-llms-for-chunk-based-information-retrieval-5242f0133b55?source=collection_archive---------4-----------------------#2024-10-28](https://towardsdatascience.com/how-and-why-to-use-llms-for-chunk-based-information-retrieval-5242f0133b55?source=collection_archive---------4-----------------------#2024-10-28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-and-why-to-use-llms-for-chunk-based-information-retrieval-5242f0133b55?source=collection_archive---------4-----------------------#2024-10-28](https://towardsdatascience.com/how-and-why-to-use-llms-for-chunk-based-information-retrieval-5242f0133b55?source=collection_archive---------4-----------------------#2024-10-28)
- en: '[](https://medium.com/@peronc79?source=post_page---byline--5242f0133b55--------------------------------)[![Carlo
    Peron](../Images/e6db9521113aa6a2dd43b0b2aa6687b5.png)](https://medium.com/@peronc79?source=post_page---byline--5242f0133b55--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5242f0133b55--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5242f0133b55--------------------------------)
    [Carlo Peron](https://medium.com/@peronc79?source=post_page---byline--5242f0133b55--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@peronc79?source=post_page---byline--5242f0133b55--------------------------------)[![Carlo
    Peron](../Images/e6db9521113aa6a2dd43b0b2aa6687b5.png)](https://medium.com/@peronc79?source=post_page---byline--5242f0133b55--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5242f0133b55--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5242f0133b55--------------------------------)
    [Carlo Peron](https://medium.com/@peronc79?source=post_page---byline--5242f0133b55--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5242f0133b55--------------------------------)
    ·9 min read·Oct 28, 2024
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5242f0133b55--------------------------------)
    ·阅读时长9分钟·2024年10月28日
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ea9b1bb4b44a9f0b8ada220d93ecda54.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea9b1bb4b44a9f0b8ada220d93ecda54.png)'
- en: Retrieve pipeline — Image by the author
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 检索流程 — 图像来自作者
- en: In this article, I aim to explain how and why it’s beneficial to use a Large
    Language Model (LLM) for chunk-based information retrieval.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我旨在解释为什么以及如何使用大语言模型（LLM）进行基于块的信息检索。
- en: I use OpenAI’s GPT-4 model as an example, but this approach can be applied with
    any other LLM, such as those from Hugging Face, Claude, and others.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我以OpenAI的GPT-4模型为例，但这种方法可以应用于任何其他大语言模型，如Hugging Face、Claude等提供的模型。
- en: Everyone can access this [article](https://medium.com/@peronc79/5242f0133b55?sk=aafe7dca2cb777410b6e426321c0b53e)
    for free.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 每个人都可以免费访问这篇[文章](https://medium.com/@peronc79/5242f0133b55?sk=aafe7dca2cb777410b6e426321c0b53e)。
- en: '**Considerations on standard information retrieval**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**标准信息检索的考虑事项**'
- en: The primary concept involves having a list of documents (**chunks of text**)
    stored in a database, which could be retrieve based on some filter and conditions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 主要概念是将一组文档（**文本块**）存储在数据库中，可以根据某些过滤条件进行检索。
- en: 'Typically, a tool is used to enable hybrid search (such as Azure AI Search,
    LlamaIndex, etc.), which allows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，使用某种工具来实现混合搜索（例如Azure AI Search、LlamaIndex等），该工具可以：
- en: performing a text-based search using term frequency algorithms like TF-IDF (e.g.,
    BM25);
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用诸如TF-IDF（例如BM25）之类的术语频率算法执行基于文本的搜索；
- en: conducting a vector-based search, which identifies similar concepts even when
    different terms are used, by calculating vector distances (typically cosine similarity);
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行基于向量的搜索，计算向量之间的距离（通常是余弦相似度），即使使用不同的术语，也能识别相似的概念；
- en: combining elements from steps 1 and 2, weighting them to highlight the most
    relevant results.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合步骤1和步骤2的元素，通过加权突出最相关的结果。
- en: '![](../Images/6f4f4ba0a88c11c7dc09513356aa58d5.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f4f4ba0a88c11c7dc09513356aa58d5.png)'
- en: Figure 1- Default hybrid search pipeline — Image by the author
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1 - 默认混合搜索流程 — 图像来自作者
- en: 'Figure 1 shows the classic retrieval pipeline:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1显示了经典的检索流程：
- en: 'the user asks the system a question: “I would like to talk about Paris”;'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户向系统提出问题：“我想谈谈巴黎”；
- en: the system receives the question, converts it into an embedding vector (using
    the same model applied in the ingestion phase), and finds the chunks with the
    smallest distances;
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统接收问题，将其转换为嵌入向量（使用与摄取阶段相同的模型），并找到距离最小的文本块；
- en: the system also performs a text-based search based on frequency;
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统还执行基于频率的文本搜索；
- en: the chunks returned from both processes undergo further evaluation and are reordered
    based on a ranking formula.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自两个过程的块经过进一步评估，并根据排名公式重新排序。
- en: 'This solution achieves good results but has some limitations:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案取得了良好的效果，但也有一些局限性：
- en: not all relevant chunks are always retrieved;
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并非所有相关的块都会被检索到；
- en: sometime some chunks contain anomalies that affect the final response.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时，一些块包含异常值，影响最终的回答。
- en: '**An example of a typical retrieval issue**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**典型检索问题的示例**'
- en: Let’s consider the “documents” array, which represents an example of a knowledge
    base that could lead to incorrect chunk selection.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑“documents”数组，它代表了一个知识库的示例，这个知识库可能导致不正确的块选择。
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s assume we have a RAG system, consisting of a vector database with hybrid
    search capabilities and an LLM-based prompt, to which the user poses the following
    question: “I need to know something about topic B.”'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个RAG系统，包含一个具有混合搜索能力的向量数据库和基于LLM的提示，用户向其提出如下问题：“我需要了解一些关于话题B的内容。”
- en: As shown in Figure 2, the search also returns an incorrect chunk that, while
    semantically relevant, is not suitable for answering the question and, in some
    cases, could even confuse the LLM tasked with providing a response.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如图2所示，搜索还返回了一个不正确的块，虽然语义上相关，但并不适合回答问题，在某些情况下，甚至可能会让负责提供答案的LLM感到困惑。
- en: '![](../Images/356ad8ff68ceb28acca4d8a23ec584c1.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/356ad8ff68ceb28acca4d8a23ec584c1.png)'
- en: Figure 2 — Example of information retrieval that can lead to errors — Image
    by the author
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图2 — 可能导致错误的检索示例 — 作者提供的图像
- en: In this example, the user requests information about “*topic B*,” and the search
    returns chunks that include “*This document expands on topic H. It also talks
    about topic B*” and “*Insights related to topic B can be found here.*” as well
    as the chunk stating, “*Nothing about topic B are given*”.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，用户请求关于“*话题B*”的信息，搜索返回的块包括“*该文档扩展了话题H，也涉及话题B*”以及“*与话题B相关的见解可以在这里找到*”，还有一个块指出，“*没有关于话题B的信息*”。
- en: While this is the expected behavior of hybrid search (as chunks reference “*topic
    B*”), it is not the desired outcome, as the third chunk is returned without recognizing
    that it isn’t helpful for answering the question.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这符合混合搜索的预期行为（因为块引用了“*话题B*”），但这并不是期望的结果，因为第三个块被返回时并没有识别到它对于回答问题并无帮助。
- en: The retrieval didn’t produce the intended result, not only because the BM25
    search found the term “*topic B*” in the third Chunk but also because the vector
    search yielded a high cosine similarity.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 检索没有产生预期的结果，不仅仅因为BM25搜索在第三个块中找到了“*话题B*”这一术语，还因为向量搜索得到了较高的余弦相似度。
- en: To understand this, refer to Figure 3, which shows the cosine similarity values
    of the chunks relative to the question, using OpenAI’s text-embedding-ada-002
    model for embeddings.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这一点，请参见图3，它显示了使用OpenAI的text-embedding-ada-002模型进行嵌入时，块相对于问题的余弦相似度值。
- en: '![](../Images/c42a17cff143c55984745f79381a2c9e.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c42a17cff143c55984745f79381a2c9e.png)'
- en: Figure 3 — Cosine similarity with text-embedding-ada-002- Image by the author
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图3 — 使用text-embedding-ada-002的余弦相似度 — 作者提供的图像
- en: It is evident that the cosine similarity value for “Chunk 9” is among the highest,
    and that between this chunk and chunk 10, which references “*topic B*,” there
    is also chunk 1, which does not mention “*topic B*”.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，“块9”的余弦相似度值是最高的，而且在该块与引用“*话题B*”的块10之间，还有一个块1，它没有提到“*话题B*”。
- en: This situation remains unchanged even when measuring distance using a different
    method, as seen in the case of Minkowski distance.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 即使使用不同的方法衡量距离，这种情况也不会改变，正如在闵可夫斯基距离的情况下所见。
- en: '**Utilizing LLMs for Information Retrieval: An Example**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**利用LLM进行信息检索：一个示例**'
- en: The solution I will describe is inspired by what has been published in my GitHub
    repository [https://github.com/peronc/LLMRetriever/](https://github.com/peronc/LLMRetriever/).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我将描述的解决方案灵感来源于我在GitHub仓库中发布的内容 [https://github.com/peronc/LLMRetriever/](https://github.com/peronc/LLMRetriever/)。
- en: The idea is to have the LLM analyze which chunks are useful for answering the
    user’s question, not by ranking the returned chunks (as in the case of RankGPT)
    but by directly evaluating all the available chunks.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是让LLM分析哪些块对回答用户的问题有用，而不是通过排名返回的块（如RankGPT的情况），而是直接评估所有可用的块。
- en: '![](../Images/ea9b1bb4b44a9f0b8ada220d93ecda54.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea9b1bb4b44a9f0b8ada220d93ecda54.png)'
- en: Figure 4- LLM Retrieve pipeline — Image by the author
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图4 - LLM检索管道 — 作者提供的图像
- en: In summary, as shown in Figure 4, the system receives a list of documents to
    analyze, which can come from any data source, such as file storage, relational
    databases, or vector databases.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，如图4所示，系统接收一组需要分析的文档，这些文档可以来自任何数据源，如文件存储、关系数据库或向量数据库。
- en: The chunks are divided into groups and processed in parallel by a number of
    threads proportional to the total amount of chunks.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这些块被分成组，并通过与块总量成比例的线程数并行处理。
- en: The logic for each thread includes a loop that iterates through the input chunks,
    calling an OpenAI prompt for each one to check its relevance to the user’s question.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 每个线程的逻辑包括一个循环，该循环遍历输入的块，为每个块调用OpenAI提示，以检查它与用户问题的相关性。
- en: 'The prompt returns the chunk along with a boolean value: *true* if it is relevant
    and *false* if it is not.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 提示返回该块以及一个布尔值：*true* 如果它相关，*false* 如果它不相关。
- en: '**Lets’go coding 😊**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**让我们开始编码吧 😊**'
- en: To explain the code, I will simplify by using the chunks present in the *documents*
    array (I will reference a real case in the conclusions).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化代码的解释，我将使用*documents*数组中存在的块（在结论部分我将引用一个实际案例）。
- en: First of all, I import the necessary standard libraries, including os, langchain,
    and dotenv.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我导入必要的标准库，包括os、langchain和dotenv。
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, I import my LLMRetrieverLib/llm_retrieve.py class, which provides several
    static methods essential for performing the analysis.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我导入了我的LLMRetrieverLib/llm_retrieve.py类，它提供了几个执行分析所需的重要静态方法。
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Following that, I need to import the necessary variables required for utilizing
    Azure OpenAI GPT-4o model.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我需要导入使用Azure OpenAI GPT-4模型所需的必要变量。
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Next, I proceed with the initialization of the LLM.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我继续初始化LLM。
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We are ready to begin: the user asks a question to gather additional information
    about *Topic B*.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备开始：用户提出问题以收集有关*主题B*的额外信息。
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: At this point, the search for relevant chunks begins, and to do this, I use
    the function `llm_retrieve.process_chunks_in_parallel` from the `LLMRetrieverLib/retriever.py`
    library, which is also found in the same repository.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，相关块的搜索开始了，为此，我使用了 `LLMRetrieverLib/retriever.py` 库中的函数 `llm_retrieve.process_chunks_in_parallel`，该库也位于同一个代码库中。
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: To optimize performance, the function `llm_retrieve.process_chunks_in_parallel`
    employs multi-threading to distribute chunk analysis across multiple threads.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了优化性能，函数 `llm_retrieve.process_chunks_in_parallel` 采用多线程来分配块分析到多个线程。
- en: The main idea is to assign each thread a subset of chunks extracted from the
    database and have each thread analyze the relevance of those chunks based on the
    user’s question.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 主要思想是为每个线程分配从数据库中提取的一个子集的块，并让每个线程根据用户的问题分析这些块的相关性。
- en: 'At the end of the processing, the returned chunks are exactly as expected:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 处理结束时，返回的块完全符合预期：
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, I ask the LLM to provide an answer to the user’s question:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我请求LLM为用户的问题提供答案：
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Below is the LLM’s response, which is trivial since the content of the chunks,
    while relevant, is not exhaustive on the topic of Topic B:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是LLM的回答，虽然内容相关，但由于这些块虽然相关，但在*主题B*的内容上并不充分，因此回答显得有些简单：
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Scoring Scenario**'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**评分场景**'
- en: Now let’s try asking the same question but using an approach based on scoring.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试提出相同的问题，但使用基于评分的方法。
- en: I ask the LLM to assign a score from 1 to 10 to evaluate the relevance between
    each chunk and the question, considering only those with a relevance higher than
    5.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我请求LLM为每个块分配1到10的评分，以评估每个块与问题的相关性，仅考虑相关性高于5的块。
- en: To do this, I call the function `llm_retriever.process_chunks_in_parallel`,
    passing three additional parameters that indicate, respectively, that scoring
    will be applied, that the threshold for being considered valid must be greater
    than or equal to 5, and that I want a printout of the chunks with their respective
    scores.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我调用函数 `llm_retriever.process_chunks_in_parallel`，传入三个额外的参数，分别表示：将应用评分，必须大于或等于5才能被认为有效，并且我希望打印出带有各自评分的块。
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The retrieval phase with scoring produces the following result:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 带评分的检索阶段产生了如下结果：
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: It’s the same as before, but with an interesting score 😊.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 结果与之前相同，但评分很有趣 😊。
- en: 'Finally, I once again ask the LLM to provide an answer to the user’s question,
    and the result is similar to the previous one:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我再次请求LLM为用户的问题提供答案，结果与之前的类似：
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Considerations**'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意事项**'
- en: This retrieval approach has emerged as a necessity following some previous experiences.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这种检索方法是通过一些先前的经验逐步发展出来的。
- en: I have noticed that pure vector-based searches produce useful results but are
    often insufficient when the embedding is performed in a language other than English.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我注意到，纯粹基于向量的搜索虽然能产生有用的结果，但当嵌入在非英语语言中时，通常不够充分。
- en: 'Using OpenAI with sentences in Italian makes it clear that the tokenization
    of terms is often incorrect; for example, the term “*canzone*,” which means “*song*”
    in Italian, gets tokenized into two distinct words: “*can*” and “*zone*”.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 使用OpenAI处理意大利语句子时，明显发现术语的标记化常常不正确；例如，“*canzone*”一词在意大利语中意味着“*歌曲*”，但它被错误地标记为两个不同的词：“*can*”和“*zone*”。
- en: This leads to the construction of an embedding array that is far from what was
    intended.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了嵌入数组的构建远未达到预期效果。
- en: In cases like this, hybrid search, which also incorporates term frequency counting,
    leads to improved results, but they are not always as expected.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，混合搜索（同时结合了术语频率计数）能够改善结果，但它们并不总是如预期般准确。
- en: 'So, this retrieval methodology can be utilized in the following ways:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这种检索方法可以以下列方式使用：
- en: '**as the primary search method:** where the database is queried for all chunks
    or a subset based on a filter (e.g., a metadata filter);'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**作为主要搜索方法：** 在这种方法中，数据库会根据过滤条件（例如元数据过滤器）查询所有片段或其子集；'
- en: '**as a refinement in the case of hybrid search:** (this is the same approach
    used by RankGPT) in this way, the hybrid search can extract a large number of
    chunks, and the system can filter them so that only the relevant ones reach the
    LLM while also adhering to the input token limit;'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**作为混合搜索中的一种优化：**（这是RankGPT使用的相同方法）通过这种方式，混合搜索可以提取大量片段，系统可以过滤它们，确保只有相关的片段传递到LLM，并且遵守输入令牌的限制；'
- en: '**as a fallback:** in situations where a hybrid search does not yield the desired
    results, all chunks can be analyzed.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**作为备选方案：** 在混合搜索没有得到期望结果的情况下，可以分析所有片段。'
- en: '**Let’s discuss costs and performance**'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**让我们讨论成本和性能**'
- en: Of course, all that glitters is not gold, as one must consider response times
    and costs.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，闪闪发光的并不都是金子，因为必须考虑响应时间和成本。
- en: In a real use case, I retrieved the chunks from a relational database consisting
    of 95 text segments semantically split using my `LLMChunkizerLib/chunkizer.py`
    library from two Microsoft Word documents, totaling 33 pages.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个实际的使用案例中，我从一个关系型数据库中检索了由我的`LLMChunkizerLib/chunkizer.py`库语义分割的95个文本片段，这些片段来自两个Microsoft
    Word文档，总共33页。
- en: The analysis of the relevance of the 95 chunks to the question was conducted
    by calling OpenAI's APIs from a local PC with non-guaranteed bandwidth, averaging
    around 10Mb, resulting in response times that varied from 7 to 20 seconds.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对95个片段与问题相关性的分析是通过在本地PC上调用OpenAI的API完成的，该PC的带宽并不保证，平均约为10Mb，导致响应时间从7秒到20秒不等。
- en: Naturally, on a cloud system or by using local LLMs on GPUs, these times can
    be significantly reduced.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，在云系统中，或通过在GPU上使用本地LLM，这些时间可以显著减少。
- en: 'I believe that considerations regarding response times are highly subjective:
    in some cases, it is acceptable to take longer to provide a correct answer, while
    in others, it is essential not to keep users waiting too long.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为，关于响应时间的考虑非常主观：在某些情况下，花更多时间来提供正确答案是可以接受的，而在其他情况下，至关重要的是不要让用户等待太久。
- en: Similarly, considerations about costs are also quite subjective, as one must
    take a broader perspective to evaluate whether it is more important to provide
    as accurate answers as possible or if some errors are acceptable.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，关于成本的考虑也非常主观，因为必须从更广泛的角度评估，是否提供尽可能准确的答案更为重要，还是一些错误是可以接受的。
- en: In certain fields, the damage to one’s reputation caused by incorrect or missing
    answers can outweigh the expense of tokens.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些领域，由于不正确或遗漏的答案导致的声誉损害，可能超过了令牌的费用。
- en: Furthermore, even though the costs of OpenAI and other providers have been steadily
    decreasing in recent years, those who already have a GPU-based infrastructure,
    perhaps due to the need to handle sensitive or confidential data, will likely
    prefer to use a local LLM.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，尽管OpenAI和其他提供商的成本在近年来一直在稳步下降，但那些已经拥有基于GPU的基础设施的人（可能是由于需要处理敏感或机密数据）可能更倾向于使用本地的LLM。
- en: '**Conclusions**'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**结论**'
- en: In conclusion, I hope to have provided my perspective on how retrieval can be
    approached.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我希望能提供我的视角，分享如何进行检索的思考。
- en: If nothing else, I aim to be helpful and perhaps inspire others to explore new
    methods in their own work.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有别的，我的目标是提供帮助，并可能激励他人在自己的工作中探索新方法。
- en: Remember, the world of information retrieval is vast, and with a little creativity
    and the right tools, we can uncover knowledge in ways we never imagined!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，信息检索的世界广阔无垠，只要稍加创意和借助合适的工具，我们就能以我们从未想象过的方式发掘知识！
- en: If you’d like to discuss this further, feel free to connect with me on [LinkedIn](https://www.linkedin.com/in/carlo-peron)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想进一步讨论，欢迎通过[LinkedIn](https://www.linkedin.com/in/carlo-peron)与我联系。
- en: 'GitHub repositories can be found here:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub 仓库可以在这里找到：
- en: • [https://github.com/peronc/LLMRetriever/](https://github.com/peronc/LLMRetriever/)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: • [https://github.com/peronc/LLMRetriever/](https://github.com/peronc/LLMRetriever/)
- en: • [https://github.com/peronc/LLMChunkizer/](https://github.com/peronc/LLMChunkizer/)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: • [https://github.com/peronc/LLMChunkizer/](https://github.com/peronc/LLMChunkizer/)
