- en: Run and Serve Faster VLMs Like Pixtral and Phi-3.5 Vision with vLLM
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 vLLM 运行和提供服务给更快的 VLM，如 Pixtral 和 Phi-3.5 Vision
- en: 原文：[https://towardsdatascience.com/run-and-serve-faster-vlms-like-pixtral-and-phi-3-5-vision-with-vllm-1db7226c2035?source=collection_archive---------4-----------------------#2024-09-23](https://towardsdatascience.com/run-and-serve-faster-vlms-like-pixtral-and-phi-3-5-vision-with-vllm-1db7226c2035?source=collection_archive---------4-----------------------#2024-09-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/run-and-serve-faster-vlms-like-pixtral-and-phi-3-5-vision-with-vllm-1db7226c2035?source=collection_archive---------4-----------------------#2024-09-23](https://towardsdatascience.com/run-and-serve-faster-vlms-like-pixtral-and-phi-3-5-vision-with-vllm-1db7226c2035?source=collection_archive---------4-----------------------#2024-09-23)
- en: Understanding how much memory you need to serve a VLM
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解为 VLM 提供服务所需的内存量
- en: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--1db7226c2035--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--1db7226c2035--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1db7226c2035--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1db7226c2035--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--1db7226c2035--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--1db7226c2035--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--1db7226c2035--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1db7226c2035--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1db7226c2035--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--1db7226c2035--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1db7226c2035--------------------------------)
    ·10 min read·Sep 23, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1db7226c2035--------------------------------)
    ·阅读时间 10 分钟·2024年9月23日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/9383dc47c9e69050884d5582cbb02bb1.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9383dc47c9e69050884d5582cbb02bb1.png)'
- en: An image encoded by Pixtral — Image by the author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由 Pixtral 编码的图像 — 图片来源于作者
- en: vLLM is currently one of the fastest inference engines for large language models
    (LLMs). It supports a wide range of model architectures and quantization methods.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: vLLM 目前是最快的大型语言模型 (LLM) 推理引擎之一。它支持多种模型架构和量化方法。
- en: vLLM also supports vision-language models (VLMs) with multimodal inputs containing
    both images and text prompts. For instance, vLLM can now serve models like Phi-3.5
    Vision and Pixtral, which excel at tasks such as image captioning, optical character
    recognition (OCR), and visual question answering (VQA).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: vLLM 还支持具有多模态输入的视觉语言模型 (VLM)，这些输入包含图像和文本提示。例如，vLLM 现在可以为像 Phi-3.5 Vision 和 Pixtral
    这样的模型提供服务，这些模型擅长图像描述、光学字符识别（OCR）和视觉问答（VQA）等任务。
- en: In this article, I will show you how to use VLMs with vLLM, focusing on key
    parameters that impact memory consumption. We will see why VLMs consume much more
    memory than standard LLMs. We’ll use Phi-3.5 Vision and Pixtral as case studies
    for a multimodal application that processes prompts containing text and images.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将向您展示如何使用 vLLM 运行 VLM，重点介绍影响内存消耗的关键参数。我们将看到为什么 VLM 比标准 LLM 消耗更多内存。我们将以
    Phi-3.5 Vision 和 Pixtral 为案例，研究一个处理包含文本和图像的提示的多模态应用。
- en: 'The code for running Phi-3.5 Vision and Pixtral with vLLM is provided in this
    notebook:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 Phi-3.5 Vision 和 Pixtral 与 vLLM 的代码可以在此笔记本中找到：
- en: '[Get the notebook (#105)](https://newsletter.kaitchup.com/p/notebooks)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[获取笔记本 (#105)](https://newsletter.kaitchup.com/p/notebooks)'
- en: Understanding the Memory Consumption of VLMs in vLLM
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解 VLM 在 vLLM 中的内存消耗
- en: In transformer models, generating text token by token is slow because each prediction
    depends on all previous tokens…
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在变换器模型中，按顺序生成文本一个标记一个标记非常慢，因为每个预测都依赖于所有先前的标记…
