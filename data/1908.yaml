- en: 'Segment Anything 2: What Is the Secret Sauce? (A Deep Learner’s Guide)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Segment Anything 2：秘密武器是什么？（深度学习者指南）
- en: 原文：[https://towardsdatascience.com/segment-anything-2-what-is-the-secret-sauce-a-deep-learners-guide-1c43dd07a6f8?source=collection_archive---------2-----------------------#2024-08-06](https://towardsdatascience.com/segment-anything-2-what-is-the-secret-sauce-a-deep-learners-guide-1c43dd07a6f8?source=collection_archive---------2-----------------------#2024-08-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/segment-anything-2-what-is-the-secret-sauce-a-deep-learners-guide-1c43dd07a6f8?source=collection_archive---------2-----------------------#2024-08-06](https://towardsdatascience.com/segment-anything-2-what-is-the-secret-sauce-a-deep-learners-guide-1c43dd07a6f8?source=collection_archive---------2-----------------------#2024-08-06)
- en: Foundation + Promptable + Interactive + Video. How?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基础模型 + 可提示 + 互动 + 视频。如何实现？
- en: '[](https://medium.com/@neural.avb?source=post_page---byline--1c43dd07a6f8--------------------------------)[![Avishek
    Biswas](../Images/6feb591069f354aa096f6108f1a70ea7.png)](https://medium.com/@neural.avb?source=post_page---byline--1c43dd07a6f8--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1c43dd07a6f8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1c43dd07a6f8--------------------------------)
    [Avishek Biswas](https://medium.com/@neural.avb?source=post_page---byline--1c43dd07a6f8--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@neural.avb?source=post_page---byline--1c43dd07a6f8--------------------------------)[![Avishek
    Biswas](../Images/6feb591069f354aa096f6108f1a70ea7.png)](https://medium.com/@neural.avb?source=post_page---byline--1c43dd07a6f8--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1c43dd07a6f8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1c43dd07a6f8--------------------------------)
    [Avishek Biswas](https://medium.com/@neural.avb?source=post_page---byline--1c43dd07a6f8--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1c43dd07a6f8--------------------------------)
    ·10 min read·Aug 6, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1c43dd07a6f8--------------------------------)
    ·阅读时长 10 分钟·2024年8月6日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/903c3e88dd79d171b8844c61bba59c13.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/903c3e88dd79d171b8844c61bba59c13.png)'
- en: The Segment Anything 2 Pipeline (Image by Author)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything 2 流程（图由作者提供）
- en: Meta just released the Segment Anything 2 model or SAM 2 — a neural network
    that can segment not just images, but entire videos as well. SAM 2 is a promptable
    interactive foundation segmentation model. Being **promptable** means you can
    click or drag bounding boxes on one or more objects you want to segment, and SAM2
    will be able to predict a mask singling out the object and track it across the
    input clip. Being **interactive** means you can edit the prompts on the fly, like
    adding new prompts in different frames — and the segments will adjust accordingly!
    Lastly, being a **foundation** segmentation model means that it is trained on
    a massive corpus of data and can be applied for a large variety of use-cases.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Meta 最近发布了 Segment Anything 2 模型，简称 SAM 2——这是一种神经网络，不仅能够分割图像，还能分割整个视频。SAM 2
    是一个可提示的互动基础分割模型。**可提示**意味着你可以点击或拖动一个或多个你想要分割的对象的边界框，SAM2 就能预测一个遮罩，将目标对象单独标出并在输入片段中追踪它。**互动**意味着你可以随时编辑提示，比如在不同的帧中添加新的提示——分割结果会相应地调整！最后，作为一个**基础**分割模型，它是在庞大的数据集上训练的，可以应用于多种不同的使用场景。
- en: '**Note that this article is a “Deep Learning Guide”, so we will primarily be
    focusing on the network architecture behind SAM-2\.** [**If you are a visual learner,
    you might want to check out the YouTube video that this article is based on.**](https://youtu.be/wMGb97EZkVU)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**请注意，本文是“深度学习指南”，因此我们将主要关注 SAM-2 后面的网络架构。** [**如果你是视觉学习者，可能想查看本文所依据的 YouTube
    视频。**](https://youtu.be/wMGb97EZkVU)'
- en: '**Promptable Visual Segmentation (PVS)**'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**可提示视觉分割（PVS）**'
- en: SAM-2 focuses on the PVS or Prompt-able Visual Segmentation task. Given an input
    video and a user prompt — like point clicks, boxes, or masks — the network must
    predict a masklet, which is another term for a spatio-temporal mask. Once a masklet
    is predicted, it can be iteratively refined by providing more prompts in additional
    frames — through positive or negative clicks — to interactively update the segmented
    mask.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: SAM-2 侧重于 PVS 或可提示视觉分割任务。给定一个输入视频和用户提示——如点击、框选或遮罩——网络必须预测一个 masklet，这也是时空遮罩的另一种说法。一旦预测出
    masklet，就可以通过在额外的帧中提供更多的提示——通过正负点击——来迭代地精细化它，以交互方式更新分割后的遮罩。
- en: The Original Segment Anything Model
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 原始 Segment Anything 模型
- en: SAM-2 builds up on the original SAM architecture which was an image segmentation
    model. Let’s do a quick recap about the basic architecture of the original SAM
    model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: SAM-2是在原始SAM架构的基础上发展而来，原始SAM是一个图像分割模型。让我们快速回顾一下原始SAM模型的基本架构。
- en: '![](../Images/b7b60f389aac038f79e037627b14323f.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7b60f389aac038f79e037627b14323f.png)'
- en: SAM-1 architecture — The Image Encoder encodes the input image. The Prompt Encoder
    encodes the input prompt, and the Mask Decoder contextualizes the prompt and image
    embeddings together to predict the segmentation mask of the queried object. Mask
    Decoder also outputs IOU Scores, which are not shown above for simplicity. (Illustration
    by the author)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: SAM-1架构 — 图像编码器对输入图像进行编码。提示编码器对输入提示进行编码，掩码解码器将提示和图像嵌入结合起来，预测查询对象的分割掩码。掩码解码器还输出IOU得分，为简化起见，以上未显示这些得分。（插图由作者提供）
- en: '**The Image Encoder** processes the input image to create general image embeddings.
    These embeddings are unconditional on the prompts.'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**图像编码器**处理输入图像，生成通用的图像嵌入。这些嵌入与提示无关。'
- en: '**The Prompt Encoder** processes the user input prompts to create prompt embeddings.
    Prompt embeddings are unconditional on the input image.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**提示编码器**处理用户输入的提示，生成提示嵌入。提示嵌入与输入图像无关。'
- en: '**The Mask Decoder** inputs the unconditional image and prompt embeddings and
    applies cross-attention and self-attention blocks to contextualize them with each
    other. From the resulting contextual embeddings, multiple segmentation masks are
    generated'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**掩码解码器**输入无条件的图像和提示嵌入，并应用交叉注意力和自注意力模块将它们进行上下文化处理。通过生成的上下文化嵌入，生成多个分割掩码'
- en: '**Multiple Output Segmentation Masks** are predicted by the Mask Decoder. These
    masks often represent the whole, part, or subpart of the queried object and help
    resolve ambiguity that might arise due to user prompts (image below).'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**多个输出分割掩码**由掩码解码器预测。这些掩码通常表示查询对象的整体、部分或子部分，并有助于解决由于用户提示而可能出现的歧义（下图）。'
- en: '**Intersection-Over-Union** **scores** are predicted for each output segmentation
    mask. These IoU scores denote the confidence score SAM has for each predicted
    mask to be “correct”. Meaning if SAM predicts a high IoU score for Mask 1, then
    Mask 1 is probably the right mask.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**交并比（Intersection-Over-Union）** **得分**会为每个输出分割掩码进行预测。这些IoU得分表示SAM对每个预测掩码“正确”的信心分数。也就是说，如果SAM为掩码1预测了较高的IoU得分，那么掩码1很可能是正确的掩码。'
- en: '![](../Images/bd73d4e972661b0b50d755bf9a604df8.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd73d4e972661b0b50d755bf9a604df8.png)'
- en: 'SAM predicts 3 segmentation masks often denoting the “whole”, “part”, and “subpart”
    of the queried object. For each predicted mask SAM predicts an IOU score as well
    to estimate the amount of overlap the generated mask will have with the object
    of interest (Source: Image by author)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: SAM预测3个分割掩码，通常表示查询对象的“整体”、“部分”和“子部分”。对于每个预测的掩码，SAM还会预测一个IOU得分，以估计生成的掩码与感兴趣对象的重叠程度（来源：图像由作者提供）
- en: So what does SAM-2 does differently to adopt the above architecture for videos?
    Let’s discuss.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，SAM-2在采用上述架构处理视频时做了什么不同的改进呢？让我们讨论一下。
- en: '**Frame Encoder**'
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**帧编码器**'
- en: The input video is first divided into multiple frames, and each of the frames
    is independently encoded using a Vision Transformer-based Masked Auto-encoder
    computer vision model, called the [Heira architecture](https://arxiv.org/abs/2306.00989).
    We will worry about the exact architecture of this transformer later, for now
    just imagine it as a black box that inputs a single frame as an image and outputs
    a multi-channel feature map of shape 256x64x64\. All the frames of the video are
    similarly processed using the encoder.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 输入视频首先被分割成多个帧，每个帧都使用基于视觉变换器（Vision Transformer）的掩码自编码器（Masked Auto-encoder）计算机视觉模型独立编码，称为[Heira架构](https://arxiv.org/abs/2306.00989)。我们稍后再讨论这个变换器的具体架构，现在只需将其视为一个黑盒，它将单个帧作为图像输入，输出一个形状为256x64x64的多通道特征图。视频的所有帧都采用相同的编码器进行处理。
- en: '![](../Images/a49f79e9bdba6c49fafeb11ad150f185.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a49f79e9bdba6c49fafeb11ad150f185.png)'
- en: The frames from the input video are embedded using a pretrained Vision Transformer
    (Image by the Author)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 输入视频的帧通过预训练的视觉变换器进行嵌入（图像由作者提供）
- en: Notice that these embeddings do not consider the video sequence — they are just
    independent frame embeddings, meaning they don’t have access to other frames in
    the video. Secondly, just like SAM-1 they do not consider the input prompt at
    all — meaning that the resultant output is treated as a universal representation
    of the input frame and completely unconditional on the input prompts.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些嵌入并不考虑视频序列——它们只是独立的帧嵌入，意味着它们无法访问视频中的其他帧。其次，就像 SAM-1 一样，它们完全不考虑输入提示——这意味着结果输出被视为输入帧的通用表示，并且完全不依赖于输入提示。
- en: The advantage of this is that if the user adds new prompts in future frames,
    we do not need to run the frames through the image encoder again. The image encoder
    runs just once for each frame, and the results are cached and reused for all types
    of input prompts. This design decision makes SAM-2 run at interactive speeds —
    because the heavy work of encoding images only needs to happen once per video
    input.
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这样做的好处是，如果用户在未来的帧中添加新的提示，我们无需再次通过图像编码器处理这些帧。每个帧只需对图像编码器运行一次，结果会被缓存并重用于所有类型的输入提示。这一设计决策使得
    SAM-2 可以以交互速度运行——因为图像编码的重工作只需要在每个视频输入中进行一次。
- en: Quick notes about the Heira Architecture
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于 Heira 架构的简要说明
- en: The exact nature of the Image encoder is an implementation detail — as long
    as the encoder is good enough and trained on a huge corpus of images it is fine.
    The Heira architecture is a hierarchical vision transformer, meaning that spatial
    resolution is reduced and the feature dimensions are increased as the network
    deepens. These models are trained on the task of Mask Autoencoding where the image
    inputted into the network is divided into multiple patches and some patches are
    randomly grayed out — and then the Heira model learns to reconstruct the original
    image back from the remaining patches that it can see.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图像编码器的确切性质是一个实现细节——只要编码器足够好并且在大量图像语料库上进行训练，就可以。Heira 架构是一个层次化的视觉变换器，这意味着随着网络的加深，空间分辨率被降低，特征维度被增加。这些模型是在掩膜自编码的任务上进行训练的，其中输入网络的图像被分成多个块，有些块会被随机灰化——然后
    Heira 模型从剩余的可以看到的块中学习重建原始图像。
- en: '![](../Images/b5906b22e25bd869ec70282798dabd8e.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5906b22e25bd869ec70282798dabd8e.png)'
- en: 'The Heira Model returns general purpose image embeddings that can be used for
    a variety of downstream computer vision tasks! (Source: [here](https://arxiv.org/abs/2306.00989))'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Heira 模型返回通用的图像嵌入，可以用于各种下游计算机视觉任务！（来源：[这里](https://arxiv.org/abs/2306.00989)）
- en: Because mask auto-encoding is self-supervised, meaning all the labels are generated
    from the source image itself, we can easily train these large encoders on massive
    image datasets without needing to manually label them. Mask Autoencoders tend
    to learn general embeddings about images that can be used for a bunch of downstream
    tasks. It’s general-purpose embedding capabilities make it the choice architecture
    for SAM’s frame encoder.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 由于掩膜自编码是自监督的，意味着所有标签都是从源图像本身生成的，我们可以轻松地在大量图像数据集上训练这些大型编码器，而不需要手动标注它们。掩膜自编码器倾向于学习关于图像的通用嵌入，这些嵌入可以用于许多下游任务。它的通用嵌入能力使其成为
    SAM 帧编码器的首选架构。
- en: '**Prompt Encoder**'
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**提示编码器**'
- en: Just like the original SAM model, input prompts can come from point clicks,
    boxes, and segmentation masks. The prompt encoder’s job is to encode these prompts
    by converting them into a representative vector representation. Here’s a video
    of the original SAM architecture that goes into the details about how prompt encoders
    work.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 就像原始的 SAM 模型一样，输入提示可以来自点击点、框或分割掩膜。提示编码器的工作是通过将这些提示转换为代表性向量表示来编码它们。这里有一段关于原始
    SAM 架构的视频，详细讲解了提示编码器是如何工作的。
- en: The original Segment Anything Paper Explanation (Video by the Author)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的《Segment Anything》论文解释（作者的视频）
- en: The Prompt Encoder takes converts it into a shape of N_tokens x 256\. For example,
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 提示编码器将其转换为 N_tokens x 256 的形状。例如，
- en: '**To encode a click** — The positional encoding of the x and y coordinate of
    the click is used as one of the tokens in the prompt sequence. The “type” of click
    (foreground/positive or background/negative) is also included in the representation.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码点击** — 点击的 x 和 y 坐标的位置信息被用作提示序列中的一个 token。点击的“类型”（前景/正面或背景/负面）也包含在表示中。'
- en: '**To encode a bounding box** — The positional encoding for the top-left and
    bottom-right points is used.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码边界框** — 使用左上角和右下角点的位置信息进行编码。'
- en: '![](../Images/1f213877f686559e3908dc264a96c792.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f213877f686559e3908dc264a96c792.png)'
- en: Encoding Prompts (Illustration by the Author)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 编码提示（作者插图）
- en: A note on Dense Prompt Encodings
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于密集提示编码的说明
- en: Point clicks and Bounding boxes are **sparse prompt encodings**, but we can
    also input entire segmentation masks as well, which is a form of **dense prompt
    encodings**. The dense prompt encodings are rarely used during inference — but
    during training, they are used to iteratively train the SAM model. The training
    methods of SAM are beyond the scope of this article, but for those curious here
    is my attempt to explain an entire article in one paragraph.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 点点击和边界框是**稀疏提示编码**，但我们也可以输入整个分割掩码，这是一种**密集提示编码**。密集提示编码在推理过程中很少使用，但在训练期间，它们用于迭代训练SAM模型。SAM的训练方法超出了本文的范围，但对于那些感兴趣的人，下面是我试图在一段话中解释整个文章的尝试。
- en: SAM is trained using iterative segmentation. During training, when SAM outputs
    a segmentation mask, we input it back into SAM as a dense prompt along with refinement
    clicks (sparse prompts) simulated from the ground-truth and prediction. The mask
    decoder uses the sparse and dense prompts and learns to output a new refined segmentation
    mask. During inference, only sparse prompts are used, and segmentation masks are
    predicted in one-pass (without feeding back the dense mask.
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: SAM是通过迭代分割训练的。在训练过程中，当SAM输出分割掩码时，我们将其作为密集提示与来自真实标签和预测的细化点击（稀疏提示）一起输入回SAM。掩码解码器使用这些稀疏和密集提示，并学习输出一个新的细化分割掩码。在推理过程中，仅使用稀疏提示，分割掩码通过一次推理预测出来（不反馈密集掩码）。
- en: Maybe one day I’ll write an article about iterative segmentation training, but
    for now, let’s move on with our exploration of SAM’s network architecture.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 也许有一天我会写一篇关于迭代分割训练的文章，但现在，让我们继续探索SAM的网络架构。
- en: Prompt encodings have largely remained the same in SAM-2\. The only difference
    is that they must run separately for all frames that the user prompts.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在SAM-2中，提示编码基本保持不变。唯一的区别是，它们必须分别对用户提示的所有帧运行。
- en: Mask Decoder (Teaser)
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 掩码解码器（预告）
- en: Umm… before we get into mask decoders, let’s talk about the concept of *Memory*
    in SAM-2\. For now, let’s just assume that the Mask Decoder inputs a bunch of
    *things* (trust me we will talk about what these things are in a minute) and outputs
    segmentation masks.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯…在我们讨论掩码解码器之前，先来聊聊SAM-2中的*内存*概念。目前，我们假设掩码解码器输入一堆*东西*（相信我，我们会在稍后讨论这些东西是什么），然后输出分割掩码。
- en: '**Memory Encoder and Memory Bank**'
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**内存编码器与内存库**'
- en: '**After the mask decoder generates output masks the output mask is passed through
    a memory encoder to obtain a memory embedding.** A new memory is created after
    each frame is processed. These memory embeddings are appended to a **Memory Bank**
    which is a first-in-first-out (FIFO) queue of the latest memories generated during
    video decoding.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**在掩码解码器生成输出掩码后，输出掩码会经过内存编码器，得到一个内存嵌入。**每处理一帧都会创建一个新的内存。这些内存嵌入会被追加到**内存库**中，内存库是一个先进先出（FIFO）的队列，用于存储视频解码过程中生成的最新内存。'
- en: '![](../Images/68fcfff8b576d705204230c7bef185d0.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/68fcfff8b576d705204230c7bef185d0.png)'
- en: 'For each frame, the Memory Encoder inputs the output of the Mask Decoder Output
    Mask and converts it into a memory. This memory is then inserted into a FIFO queue
    called the Memory Bank. Note that this image does not show the Memory Attention
    module — we will talk about it later in the article (Source: Image by author)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一帧，内存编码器将掩码解码器的输出掩码输入，并将其转换为内存。然后，这个内存会被插入到名为内存库的FIFO队列中。请注意，这张图没有显示内存注意模块——我们稍后会在文章中讨论它。（来源：作者插图）
- en: Memory Encoder
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存编码器
- en: The output masks are first downsampled using a convolutional layer, and the
    unconditional image encoding is added to this output, passed through light-weight
    convolution layers to fuse the information, and the resulting spatial feature
    map is called the memory. You can imagine the memory to be a representation of
    the original input frame and the generated mask from a given time frame in the
    video.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的掩码首先通过卷积层进行下采样，然后将无条件的图像编码添加到此输出中，经过轻量级的卷积层融合信息，得到的空间特征图称为内存。你可以把内存想象成原始输入帧和给定时间帧生成的掩码的表示。
- en: '![](../Images/6c09fd1398bf2decbcd19132e6ef00a5.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c09fd1398bf2decbcd19132e6ef00a5.png)'
- en: Memory Encoder (Image by the Author)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 内存编码器（作者插图）
- en: Memory Bank
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存库
- en: 'The memory bank contains the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 内存库包含以下内容：
- en: The most recent N memories are stored in a queue
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最近的N个记忆存储在队列中
- en: The last M prompts inputed by the user to keep track of multiple previous prompts.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户输入的最后M个提示，用于跟踪多个先前的提示。
- en: The mask decoder output tokens for each frame are also stored — which are like
    object pointers that capture high-level semantic information about the object
    to segment.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个帧的掩码解码器输出标记也会被存储——这些标记像对象指针，捕捉到关于待分割对象的高级语义信息。
- en: '![](../Images/4b3bb6a345a4066cb18a4130b6156ec3.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b3bb6a345a4066cb18a4130b6156ec3.png)'
- en: Memory Bank (Image by the Author)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 内存库（图片来自作者）
- en: Memory Attention
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内存注意力
- en: We now have a way to save historical information in a Memory Bank. Now we need
    to use this information while generating segmentation masks for future frames.
    This is achieved using **Memory Attention**. The role of memory attention is to
    condition the current frame features on the Memory Bank features before it is
    inputted into the Mask Decoder.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有了一种保存历史信息到内存库中的方式。接下来，我们需要在生成未来帧的分割掩码时使用这些信息。这是通过使用**内存注意力**来实现的。内存注意力的作用是将当前帧特征与内存库特征进行条件化，然后再输入到掩码解码器中。
- en: '![](../Images/903c3e88dd79d171b8844c61bba59c13.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/903c3e88dd79d171b8844c61bba59c13.png)'
- en: The Memory Attention block contextualizes the image encodings with the memory
    bank. These contextualized image embeddings are then inputted into the mask decoder
    to generate segmentation masks. (Image by Author)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 内存注意力模块将图像编码与内存库进行上下文化处理。然后，这些上下文化的图像嵌入被输入到掩码解码器中生成分割掩码。（图片来自作者）
- en: The Memory attention block first performs self-attention with the frame embeddings
    and then performs cross-attention between the image embeddings and the contents
    of the memory bank. The unconditional image embeddings therefore get contextualized
    with the previous output masks, previous input prompts, and object pointers.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 内存注意力模块首先对帧嵌入执行自注意力，然后在图像嵌入和内存库内容之间执行交叉注意力。因此，无条件的图像嵌入会与先前的输出掩码、先前的输入提示和对象指针进行上下文化处理。
- en: '![](../Images/c1c865a12ddbd6da4bdc6efba6d6280a.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c1c865a12ddbd6da4bdc6efba6d6280a.png)'
- en: The Memory Attention Block (Image by Author)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 内存注意力模块（图片来自作者）
- en: During the self-attention and cross-attention layers, in addition to the usual
    sinusoidal position embeddings, 2D rotary positional embeddings are also used.
    Without getting into extra details — rotary positional embeddings allow to capture
    of relative relationships between the frames, and 2D rotary positional embeddings
    work well for images because they help to model the spatial relationship between
    the frames both horizontally and vertically.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在自注意力和交叉注意力层中，除了常规的正弦位置嵌入外，还使用了二维旋转位置嵌入。简单来说——旋转位置嵌入能够捕捉帧之间的相对关系，二维旋转位置嵌入在图像中效果良好，因为它们有助于建模帧之间的空间关系，包括水平和垂直方向。
- en: Mask Decoder (For real this time)
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 掩码解码器（这次是真的）
- en: The Mask Decoder inputs the memory-contextualized image encodings output by
    the Memory Attention block, plus the prompt encodings and outputs the segmentation
    masks, IOU scores, and (the brand new) occlusion scores.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码解码器输入由内存注意力模块输出的内存上下文化的图像编码，以及提示编码，并输出分割掩码、IOU分数和（全新的）遮挡分数。
- en: '![](../Images/75323ed13ac8d94dd234ad70a184ec0d.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75323ed13ac8d94dd234ad70a184ec0d.png)'
- en: 'SAM-2 Mask Decoder (Source: SAM-2 Paper [here](https://arxiv.org/abs/2408.00714))'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: SAM-2掩码解码器（来源：SAM-2论文 [这里](https://arxiv.org/abs/2408.00714)）
- en: The Mask-Decoder uses self-attention and cross attention mechanisms to contextualize
    the prompt tokens with the (memory-conditioned) image embeddings. This allows
    the image and prompts to be “married” together into one context-aware sequence.
    These embeddings are then used to produce segmentation masks, IOU scores, and
    the occlusion score. Basically, during the video, the queried object can get occluded
    because it got blocked by another object in the scene — the occlusion score predicts
    if the queried object is present in the current frame. The occlusion scores are
    a new addition to SAM-2 which predicts if the queried object is at-all present
    in the current scene or not.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码解码器使用自注意力和交叉注意力机制，将提示标记与（内存条件化的）图像嵌入进行上下文化处理。这使得图像和提示能够“结合”成一个上下文感知的序列。然后，这些嵌入被用来生成分割掩码、IOU分数和遮挡分数。基本上，在视频过程中，被查询的对象可能会被遮挡，因为它被场景中的另一个对象挡住——遮挡分数预测查询的对象是否出现在当前帧中。遮挡分数是SAM-2的新功能，用于预测查询的对象是否存在于当前场景中。
- en: To recap, just like the IOU scores, SAM generates an occlusion score for each
    of the three predicted masks. The three IOU scores tell us how confident SAM is
    for each of the three predicted masks — and the three occlusion scores tell us
    how likely SAM thinks that the corresponding object is present in the scene.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，和IOU分数一样，SAM会为每个预测的三种掩码生成一个遮挡分数。这三个IOU分数告诉我们SAM对每个预测掩码的信心有多大，而这三个遮挡分数则告诉我们SAM认为相应物体出现在场景中的可能性有多大。
- en: '![](../Images/c8151a6a0bd269c1550c5470e9d5258d.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c8151a6a0bd269c1550c5470e9d5258d.png)'
- en: The outputs from SAM-2 Mask Decoders (Image by Author)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: SAM-2 Mask解码器的输出（图片由作者提供）
- en: Final Thoughts
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终思考
- en: Video on SAM-2 by the Auth
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 作者关于SAM-2的视频
- en: So… that was an outline of the network architecture behind SAM-2\. For the extracted
    frames from a video (often at 6 FPS), the frames are encoded using the Frame/Image
    encoder, memory attention contextualizes the image encodings, prompts are encoded
    if present, the Mask Decoder then combines the image and prompt embeddings, produces
    output masks, IOU scores, and occlusion scores, Memory is generated using memory
    encoder, appended into the memory bank, and the whole process repeats.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 所以…这就是SAM-2背后的网络架构概述。对于从视频中提取的帧（通常为6帧每秒），这些帧会使用帧/图像编码器进行编码，记忆注意力会对图像编码进行上下文处理，如果有提示词，提示词也会被编码，之后Mask解码器将图像和提示词的嵌入结合，生成输出掩码、IOU分数和遮挡分数，记忆通过记忆编码器生成，并被追加到记忆库中，整个过程会重复进行。
- en: '![](../Images/7b4500639eff2ecec696df16741e614e.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b4500639eff2ecec696df16741e614e.png)'
- en: The SAM-2 Algorithm (Image by the Author)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: SAM-2算法（图片由作者提供）
- en: There are still many things to discuss about SAM-2 like how it trains interactively
    to be a promptable model, and how their data engine works to create training data.
    I hope to cover these topics in a separate article! You can watch the SAM-2 video
    on my YouTube channel above for more information and a visual tour of the systems
    that empower SAM-2.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 关于SAM-2仍然有许多内容值得讨论，比如它是如何通过互动训练成为可提示模型的，以及他们的数据引擎是如何工作来创建训练数据的。我希望在另一篇文章中详细讨论这些话题！你可以在我上面的YouTube频道观看SAM-2视频，了解更多信息，并观看支持SAM-2的系统的视觉展示。
- en: '***Thanks for reading! Throw a clap and a follow!***'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '***感谢阅读！请鼓掌并关注！***'
- en: References / Where to go next
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献/接下来该去哪儿
- en: '[Author’s Youtube Channel](https://www.youtube.com/@avb_fj)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[作者的YouTube频道](https://www.youtube.com/@avb_fj)'
- en: '[My video explanation of SAM](https://youtu.be/OhxJkqD1vuE)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[我对SAM的讲解视频](https://youtu.be/OhxJkqD1vuE)'
- en: '[My video explanation of SAM-2](https://youtu.be/wMGb97EZkVU)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[我对SAM-2的讲解视频](https://youtu.be/wMGb97EZkVU)'
- en: You can play with the Segment Anything 2 Model on [Meta’s website](https://ai.meta.com/blog/segment-anything-2/).
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以在[Meta的网站](https://ai.meta.com/blog/segment-anything-2/)上使用Segment Anything
    2模型。
- en: '[OG SAM Paper](https://arxiv.org/abs/2304.02643)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OG SAM论文](https://arxiv.org/abs/2304.02643)'
- en: '[SAM-2 Paper](https://arxiv.org/pdf/2408.00714)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SAM-2论文](https://arxiv.org/pdf/2408.00714)'
