- en: Can LLMs Replace Data Analysts? Learning to Collaborate
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM 能替代数据分析师吗？学会如何合作
- en: 原文：[https://towardsdatascience.com/can-llms-replace-data-analysts-learning-to-collaborate-9d42488dc327?source=collection_archive---------2-----------------------#2024-01-09](https://towardsdatascience.com/can-llms-replace-data-analysts-learning-to-collaborate-9d42488dc327?source=collection_archive---------2-----------------------#2024-01-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/can-llms-replace-data-analysts-learning-to-collaborate-9d42488dc327?source=collection_archive---------2-----------------------#2024-01-09](https://towardsdatascience.com/can-llms-replace-data-analysts-learning-to-collaborate-9d42488dc327?source=collection_archive---------2-----------------------#2024-01-09)
- en: 'Part 3: Teaching the LLM agent to pose and address clarifying questions'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 3 部分：教导 LLM 代理提出和处理澄清性问题
- en: '[](https://miptgirl.medium.com/?source=post_page---byline--9d42488dc327--------------------------------)[![Mariya
    Mansurova](../Images/b1dd377b0a1887db900cc5108bca8ea8.png)](https://miptgirl.medium.com/?source=post_page---byline--9d42488dc327--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9d42488dc327--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9d42488dc327--------------------------------)
    [Mariya Mansurova](https://miptgirl.medium.com/?source=post_page---byline--9d42488dc327--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://miptgirl.medium.com/?source=post_page---byline--9d42488dc327--------------------------------)[![Mariya
    Mansurova](../Images/b1dd377b0a1887db900cc5108bca8ea8.png)](https://miptgirl.medium.com/?source=post_page---byline--9d42488dc327--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9d42488dc327--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9d42488dc327--------------------------------)
    [Mariya Mansurova](https://miptgirl.medium.com/?source=post_page---byline--9d42488dc327--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9d42488dc327--------------------------------)
    ·20 min read·Jan 9, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9d42488dc327--------------------------------)
    ·20 分钟阅读·2024 年 1 月 9 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/fa7743b2c33135033008e395f36e9ffd.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa7743b2c33135033008e395f36e9ffd.png)'
- en: Image by DALL-E 3
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 DALL-E 3 提供
- en: '**Collaboration** is a core aspect of analysts’ day-to-day jobs. Frequently,
    we encounter high-level requests such as, “What will be the impact of the new
    feature?” or “What is going on with retention?". Before jumping to writing queries
    and pulling data, we usually need to define tasks more clearly: talk to stakeholders,
    understand their needs thoroughly, and determine how we can provide the best assistance.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**合作**是分析师日常工作中的核心方面。我们经常会遇到一些高层次的请求，比如，“新特性会带来什么影响？”或者“用户留存情况如何？”。在跳入写查询和提取数据之前，我们通常需要更清晰地定义任务：与相关方沟通，彻底了解他们的需求，并确定如何提供最好的帮助。'
- en: So, for an LLM-powered analyst, mastering the art of posing and addressing follow-up
    questions is essential since I can’t imagine an analyst working in isolation.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于一个 LLM 驱动的分析师来说，掌握提问和处理后续问题的技巧至关重要，因为我无法想象一个分析师在孤立的情况下工作。
- en: In this article, we will teach our LLM analyst to ask clarifying questions and
    follow long conversations. We will talk in detail about different memory implementations
    in LangChain.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将教导我们的 LLM 分析师如何提问澄清性问题，并跟进长期对话。我们还将详细讨论 LangChain 中不同的记忆实现方式。
- en: We’ve already discussed many aspects of LLM agents in the previous articles.
    So, let me quickly summarise them. Also, since our last implementation, LangChain
    has been updated, and it’s time to catch up.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前几篇文章中已经讨论了许多 LLM 代理的相关方面。所以，让我快速总结一下这些内容。此外，自从我们上次的实现以来，LangChain 已经进行了更新，现在是时候跟进了。
- en: LLM agents recap
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM 代理总结
- en: Let’s quickly recap what we’ve already learned about LLM agents.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速回顾一下我们已经学到的关于 LLM 代理的内容。
- en: We’ve [discussed](/can-llms-replace-data-analysts-building-an-llm-powered-analyst-851578fa10ce)
    how to empower LLMs with external tools. It helps them overcome limitations (i.e.,
    poor performance on maths tasks) and get access to the world (i.e., your database
    or internet).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们已经[讨论过](/can-llms-replace-data-analysts-building-an-llm-powered-analyst-851578fa10ce)如何通过外部工具赋能
    LLM。这有助于它们克服局限性（例如，在数学任务中的表现较差），并能够接触到更广阔的世界（例如，你的数据库或互联网）。
- en: The core idea of the LLM agents is to use LLM as a reasoning engine to define
    the set of actions to take and leverage tools. So, in this approach, you don’t
    need to hardcode the logic and just let LLM make decisions on the following steps
    to achieve the final goal.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM 代理的核心思想是将 LLM 作为推理引擎来定义执行的动作集合，并利用工具。因此，在这种方法中，你不需要硬编码逻辑，而是让 LLM 决定接下来的步骤，以实现最终目标。
- en: We’ve [implemented](/can-llms-replace-data-analysts-getting-answers-using-sql-8cf7da132259)
    an LLM-powered agent that can work with SQL databases and answer user requests.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们已经[实现了](/can-llms-replace-data-analysts-getting-answers-using-sql-8cf7da132259)一个LLM驱动的代理，能够与SQL数据库配合使用并回答用户请求。
- en: Since our last iteration, LangChain has been [updated](https://github.com/langchain-ai/langchain/releases)
    from 0.0.350 to 0.1.0 version. The documentation and best practices for LLM agents
    have changed. This domain is developing quickly, so it’s no surprise the tools
    are evolving, too. Let’s quickly recap.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 自我们上次迭代以来，LangChain已从0.0.350更新至0.1.0版本。LLM代理的文档和最佳实践发生了变化。这个领域发展迅速，因此工具也在不断演变，这并不令人惊讶。让我们快速回顾一下。
- en: First, LangChain has significantly improved [the documentation](https://python.langchain.com/docs/modules/agents/agent_types/),
    and now you can find a clear, structured view of the supported agent types and
    the differences between them.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，LangChain大幅改进了[文档](https://python.langchain.com/docs/modules/agents/agent_types/)，现在你可以清楚地、结构化地查看支持的代理类型及其之间的差异。
- en: It’s easier for models to work with tools with just one input parameter, so
    some agents have such limitations. However, in most real-life cases, tools have
    several arguments. So, let’s focus on the agents capable of working with multiple
    inputs. It leaves us just three possible options.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 模型处理只有一个输入参数的工具更为简单，因此一些代理有这样的限制。然而，在大多数现实案例中，工具有多个参数。因此，让我们关注那些能够处理多个输入的代理。这样我们只剩下三个可能的选项。
- en: '[**OpenAI tools**](https://python.langchain.com/docs/modules/agents/agent_types/openai_tools)'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**OpenAI工具**](https://python.langchain.com/docs/modules/agents/agent_types/openai_tools)'
- en: It’s the most cutting-edge type of agent since it supports chat history, tools
    with multiple inputs and even parallel function calling.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是最前沿的代理类型，因为它支持聊天历史、具有多个输入的工具，甚至并行函数调用。
- en: You can use it with the recent OpenAI models (after `1106`) since they were
    fine-tuned for tool calling.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以与最近的OpenAI模型（`1106`之后的版本）一起使用它，因为这些模型已经过针对工具调用的微调。
- en: 2\. [**OpenAI functions**](https://python.langchain.com/docs/modules/agents/agent_types/openai_functions_agent)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 2. [**OpenAI函数**](https://python.langchain.com/docs/modules/agents/agent_types/openai_functions_agent)
- en: OpenAI functions agents are close to OpenAI tools but are slightly different
    under the hood.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI函数代理接近OpenAI工具，但在底层有些许不同。
- en: Such agents don’t support parallel function calling.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种代理不支持并行函数调用。
- en: You can use recent OpenAI models that were fine-tuned to work with functions
    (the complete list is [here](https://platform.openai.com/docs/guides/function-calling/supported-models))
    or compatible open-source LLMs.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用经过微调以与函数一起使用的最新OpenAI模型（完整列表请见[这里](https://platform.openai.com/docs/guides/function-calling/supported-models)），或兼容的开源LLM。
- en: 3.[**Structured chat**](https://python.langchain.com/docs/modules/agents/agent_types/structured_chat)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 3.[**结构化聊天**](https://python.langchain.com/docs/modules/agents/agent_types/structured_chat)
- en: This approach is similar to ReAct. It instructs an agent to follow the Thought
    -> Action -> Observation framework.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种方法类似于ReAct。它指示代理遵循“思考 -> 行动 -> 观察”框架。
- en: It doesn’t support parallel function calling, just as OpenAI functions approach.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不支持并行函数调用，就像OpenAI函数方法一样。
- en: You can use it with any model.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以与任何模型一起使用它。
- en: Also, you can notice that the experimental agent types we tried in [the previous
    article](/can-llms-replace-data-analysts-getting-answers-using-sql-8cf7da132259),
    such as BabyAGI, Plan-and-execute and AutoGPT, are still not part of the suggested
    options. They might be included later (I hope), but for now I wouldn’t recommend
    using them in production.
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 此外，你还会注意到我们在[上一篇文章](/can-llms-replace-data-analysts-getting-answers-using-sql-8cf7da132259)中尝试过的实验性代理类型，如BabyAGI、Plan-and-execute和AutoGPT，仍然不是推荐的选项之一。它们可能会在以后被纳入（我希望如此），但目前我不建议在生产环境中使用它们。
- en: After reading the new documentation, I’ve finally realised the difference between
    OpenAI tools and OpenAI functions agents. With the OpenAI tools approach, an agent
    can call multiple tools at the same iterations, while other agent types don’t
    support such functionality. Let’s see how it works and why it matters.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完新的文档后，我终于意识到OpenAI工具和OpenAI函数代理之间的区别。使用OpenAI工具方法时，代理可以在同一迭代中调用多个工具，而其他代理类型不支持这种功能。让我们看看它是如何工作的，以及为什么这很重要。
- en: 'Let’s create two agents — OpenAI tools and OpenAI functions. We will empower
    them with two tools:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建两个代理——OpenAI工具和OpenAI函数。我们将为它们配备两种工具：
- en: '`get_monthly_active_users` returns the number of active customers for city
    and month. To simplify debugging, we will be using a dummy function for it. In
    practice, we would go to our database to retrieve this data.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get_monthly_active_users`返回某个城市和月份的活跃客户数量。为了简化调试，我们将使用一个虚拟函数来代替。在实际操作中，我们会去数据库获取这些数据。'
- en: '`percentage_difference` calculates the difference between two metrics.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`percentage_difference`计算两个度量值之间的差异。'
- en: Let’s create tools from Python functions and specify schemas using Pydantic.
    If you want to recap this topic, you can find a detailed explanation in [the first
    article](/can-llms-replace-data-analysts-building-an-llm-powered-analyst-851578fa10ce)
    of this series.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从Python函数创建工具，并使用Pydantic指定架构。如果您想回顾一下这个话题，可以在[本系列的第一篇文章](/can-llms-replace-data-analysts-building-an-llm-powered-analyst-851578fa10ce)中找到详细的解释。
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: To test a tool, you can execute it using the following commands.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试一个工具，您可以使用以下命令来执行它。
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Let’s create a prompt template that we will be using for the agents. It will
    consist of a system message, a user request and a placeholder for tools’ observations.
    Our prompt has two variables — `input` and `agent_scratchpad`.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个提示模板，供代理使用。它将包括一个系统消息、用户请求以及工具观察结果的占位符。我们的提示有两个变量 — `input` 和 `agent_scratchpad`。
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let’s use new LangChain functions to create agents — `create_openai_functions_agent`
    and `create_openai_tools_agent`. To create an agent, we need to specify parameters
    — an LLM model, a list of tools and a prompt template. On top of the agents, we
    also need to create agent executors.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用新的LangChain函数来创建代理 — `create_openai_functions_agent` 和 `create_openai_tools_agent`。要创建一个代理，我们需要指定一些参数
    — LLM模型、工具列表和提示模板。除了代理之外，我们还需要创建代理执行器。
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: I used the ChatGPT 4 Turbo model since it’s capable of working with OpenAI tools.
    We will need some complex reasoning, thus ChatGPT 3.5 will likely be insufficient
    in our use case.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了ChatGPT 4 Turbo模型，因为它能够与OpenAI工具协同工作。由于我们需要进行一些复杂的推理，ChatGPT 3.5可能不足以满足我们的使用需求。
- en: We’ve created two agent executors, and it’s time to try them in practice and
    compare results.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经创建了两个代理执行器，现在是时候在实践中测试它们并比较结果了。
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Interestingly, the agents returned the same correct result. It’s not so surprising
    since we used low temperatures.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，两个代理返回了相同的正确结果。这并不奇怪，因为我们使用了低温度设置。
- en: Both agents performed well, but let’s compare how they work under the hood.
    We can switch on debug mode (execute `langchain.debug = True` for it) and see
    the number of LLM calls and tokens used.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 两个代理都表现良好，但让我们比较一下它们在幕后是如何工作的。我们可以开启调试模式（执行`langchain.debug = True`）来查看LLM调用次数和使用的tokens数量。
- en: You can see the scheme depicting the calls for two agents below.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到下方展示两个代理调用的架构。
- en: '![](../Images/a63a4a9fcfd1c6bd41445b50fd5d422c.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a63a4a9fcfd1c6bd41445b50fd5d422c.png)'
- en: Scheme by author
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 作者设计的架构
- en: 'The OpenAI functions agent did 4 LLM calls, while the OpenAI tools agent made
    just 3 ones because it could get MAUs for London and Berlin in one iteration.
    Overall, it leads to a lower number of used tokens and, hence, lower price:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI函数代理进行了4次LLM调用，而OpenAI工具代理只进行了3次调用，因为它可以在一次迭代中获取伦敦和柏林的MAU数据。总体而言，这导致了使用的tokens数量较少，从而降低了成本：
- en: OpenAI tools agent — 1 537 tokens
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI工具代理 — 1,537个tokens
- en: OpenAI functions agent — 1 874 tokens (*+21.9%*).
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI函数代理 — 1,874个tokens（*+21.9%*）。
- en: So, I would recommend you consider using OpenAI tools agents. You can use it
    with both ChatGPT 4 Turbo and ChatGPT 3.5 Turbo.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我建议您考虑使用OpenAI工具代理。它可以与ChatGPT 4 Turbo和ChatGPT 3.5 Turbo都一起使用。
- en: We’ve revised our previous implementation of an LLM-powered analyst. So, it’s
    time to move on and teach our agent to pose follow-up questions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经修订了之前的基于LLM的分析师实现。所以，现在是时候让我们的代理提问后续问题了。
- en: Asking clarifying questions
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提出澄清性问题
- en: We would like to teach our agent to ask the user clarifying questions. The most
    reasonable way to teach LLM agents something new is to give them a tool. So, LangChain
    has a handy tool — [Human](https://python.langchain.com/docs/integrations/tools/human_tools).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望教会代理向用户提出澄清性问题。教导LLM代理新知识的最合理方法是为它们提供一个工具。所以，LangChain提供了一个方便的工具——[Human](https://python.langchain.com/docs/integrations/tools/human_tools)。
- en: There’s no rocket science in it. You can see the implementation [here](https://api.python.langchain.com/en/latest/_modules/langchain_community/tools/human/tool.html#).
    We can easily implement it ourselves, but it’s a good practice to use tools provided
    by the framework.
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这并不是什么高深的技术。您可以在[这里](https://api.python.langchain.com/en/latest/_modules/langchain_community/tools/human/tool.html#)查看实现。我们可以轻松地自己实现它，但使用框架提供的工具是一种良好的实践。
- en: Let’s initiate such a tool. We don’t need to specify any arguments unless we
    want to customise something, for example, a tool’s description or input function.
    See more details in [the documentation](https://api.python.langchain.com/en/latest/tools/langchain_community.tools.human.tool.HumanInputRun.html#).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们启动这个工具。除非我们想自定义某些内容，例如工具的描述或输入函数，否则我们不需要指定任何参数。有关更多细节，请参见[文档](https://api.python.langchain.com/en/latest/tools/langchain_community.tools.human.tool.HumanInputRun.html#)。
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can look at the default tool’s description and arguments.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看默认工具的描述和参数。
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Let’s add this new tool to our agent’s toolkit and reinitialise the agent. I’ve
    also tweaked the system message to encourage the model to ask follow-up questions
    when it doesn’t have enough details.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个新工具添加到我们代理的工具包中，并重新初始化代理。我还调整了系统消息，以鼓励模型在信息不足时提出后续问题。
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now, it’s time to try it out. The agent just returned the output, asking for
    a specific time period. It doesn’t work as we expected.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候尝试一下了。代理刚刚返回了输出，要求一个特定的时间段。它并没有按照我们预期的方式工作。
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The agent didn’t understand that it needed to use this tool. Let’s try to fix
    it and change the Human tool’s description so that it is more evident for the
    agent when it should use this tool.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 代理没有理解它需要使用这个工具。让我们尝试修复它并修改Human工具的描述，使代理在需要使用该工具时能够更清楚地识别。
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: After the change, the agent used the Human tool and asked for a specific time
    period. I provided an answer, and we got the correct result — 4 286 active customers
    in December 2023 for London.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 更改后，代理使用了Human工具并请求了一个特定的时间段。我提供了答案，我们得到了正确的结果——2023年12月伦敦有4,286名活跃客户。
- en: '![](../Images/76aae445324f1a27b903fd3c4f1eafa6.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/76aae445324f1a27b903fd3c4f1eafa6.png)'
- en: Screenshot by author
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 作者截图
- en: So, as usual, tweaking the prompt helps. Now, it works pretty well. Remember
    that creating a good prompt is an iterative process, and it’s worth trying several
    options and evaluating results.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，像往常一样，调整提示帮助了我们。现在，它工作得相当不错。记住，创建一个好的提示是一个迭代过程，值得尝试多个选项并评估结果。
- en: We’ve taught our LLM agent to ask for details and take them into account while
    working on data requests.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经教会了我们的LLM代理在处理数据请求时，要求提供更多细节并加以考虑。
- en: However, it’s only part of the collaboration. In real life, analysts often get
    follow-up questions after providing any research. Now, our agent can’t keep up
    the conversation and address the new questions from the user since it doesn’t
    have any memory. It’s time to learn more about the tools we have to implement
    memory in LangChain.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这只是协作的一部分。在现实中，分析师在提供任何研究后，经常会收到后续问题。现在，我们的代理无法继续对话并回答用户的新问题，因为它没有记忆。是时候了解一下我们可以用来在LangChain中实现记忆的工具了。
- en: Actually, we already have a concept of memory in the current agent implementation.
    Our agent stores the story of its interactions with tools in the `agent_scratchpad`
    variable. We need to remember not only interactions with tools but also the conversation
    with the user.
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 事实上，我们在当前的代理实现中已经有了记忆的概念。我们的代理将与工具的交互记录存储在`agent_scratchpad`变量中。我们不仅需要记住与工具的交互，还需要记住与用户的对话。
- en: Memory in LangChain
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LangChain中的记忆
- en: By default, LLMs are stateless and don’t remember previous conversations. If
    we want our agent to be able to have long discussions, we need to store the chat
    history somehow. LangChain provides a bunch of different memory implementations.
    Let’s learn more about it.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，LLM是无状态的，并且不会记住先前的对话。如果我们希望我们的代理能够进行长时间的对话，我们需要以某种方式存储聊天历史。LangChain提供了多种不同的记忆实现。让我们进一步了解它。
- en: '`ConversationBufferMemory` is the most straightforward approach. It just saves
    all the context you pushed to it. Let’s try it out: initialise a memory object
    and add a couple of conversation exchanges.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`ConversationBufferMemory`是最简单的方法。它只会保存你推送给它的所有上下文。让我们试试：初始化一个内存对象并添加几次对话交换。'
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This approach works well. However, in many cases, it’s not feasible to pass
    the whole previous conversation to LLM on each iteration because:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法效果很好。然而，在许多情况下，将整个先前的对话传递给LLM进行每次迭代是不切实际的，因为：
- en: we might hit the context length limit,
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可能会遇到上下文长度的限制，
- en: LLMs are not so good at dealing with long texts,
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM对于处理长文本并不擅长，
- en: we are paying for tokens, and such an approach might become quite expensive.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们正在支付代币，而这种方法可能会变得相当昂贵。
- en: So there’s another implementation, `ConversationBufferWindowMemory`, that can
    store a limited number of conversation exchanges. So, it will store only the last
    k iterations.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，还有另一个实现——`ConversationBufferWindowMemory`，它可以存储有限数量的对话交换。因此，它将只存储最后的k次迭代。
- en: '[PRE11]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We’ve used `k = 1` just to show how it works. In real-life use cases, you will
    likely use much higher thresholds.
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们使用了`k = 1`只是为了展示它是如何工作的。在实际使用中，你可能会使用更高的阈值。
- en: 'This approach can help you to keep chat history size manageable. However, it
    has a drawback: you can still hit the context size limit since you don’t control
    the chat history size in tokens.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可以帮助你保持聊天历史的大小可管理。然而，它也有一个缺点：你仍然可能会达到上下文大小的限制，因为你无法控制聊天历史的令牌大小。
- en: To address this challenge, we can use `ConversationTokenBufferMemory`. It won’t
    split statements, so don’t worry about incomplete sentences in the context.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这一挑战，我们可以使用`ConversationTokenBufferMemory`。它不会分割语句，因此无需担心上下文中的不完整句子。
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In this case, we need to pass an LLM model to initialise a memory object because
    LangChain needs to know the model to calculate the number of tokens.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们需要传递一个LLM模型来初始化内存对象，因为LangChain需要知道模型来计算令牌数。
- en: In all approaches we’ve discussed above, we stored the exact conversation or
    at least parts of it. However, we don’t need to do it. For example, people usually
    don’t remember their conversations exactly. I can’t reproduce yesterday’s meeting’s
    content word by word, but I remember the main ideas and action items — a summary.
    Since humans are GI (General Intelligence), it sounds reasonable to leverage this
    strategy for LLMs as well. LangChain implemented it in `ConversationSummaryBufferMemory`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们上面讨论的所有方法中，我们存储了整个对话或至少是部分内容。然而，我们不需要这样做。例如，人们通常不会完全记得他们的对话。我不能逐字复述昨天会议的内容，但我记得主要的想法和行动项目——一个摘要。由于人类是GI（通用智能），因此将这种策略应用于LLM似乎是合理的。LangChain在`ConversationSummaryBufferMemory`中实现了这一点。
- en: 'Let’s try it in practice: initiate the memory and save the first conversation
    exchange. We got the whole conversation since our current context hasn’t hit the
    threshold.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在实践中试试看：初始化内存并保存第一次对话交换。我们得到了整个对话，因为我们当前的上下文还没有超过阈值。
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let’s add one more conversation exchange. Now, we’ve hit the limit: the whole
    chat history exceeds 100 tokens, the specified threshold. So, only the last AI
    response is stored (it’s within the 100 tokens limit). For earlier messages, the
    summary has been generated.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再添加一次对话交换。现在，我们达到了限制：整个聊天历史超过了100个令牌，这是设定的阈值。因此，只有最后的AI回应被存储（它在100个令牌限制之内）。对于早期的消息，已经生成了摘要。
- en: The summary is stored with the prefix `System:` .
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要是以`System:`前缀存储的。
- en: '[PRE14]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As usual, it’s interesting to see how it works under the hood, and we can understand
    it in a debug mode. When the conversation exceeded the limit on the memory size,
    the LLM call was made with the following prompt:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，看看它是如何在幕后工作的很有趣，我们可以通过调试模式来理解它。当对话超出内存大小限制时，LLM调用使用了以下提示：
- en: '[PRE15]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: It implements the progressive update of the summary. So, it uses fewer tokens,
    not passing the whole chat history every time to get an updated summary. That’s
    reasonable.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 它实现了摘要的渐进式更新。因此，它使用更少的令牌，而不是每次都传递整个聊天历史来获取更新的摘要。这是合理的。
- en: 'Also, LangChain has more advanced memory types:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，LangChain还具有更多先进的内存类型：
- en: Vector data memory — storing texts’ embeddings in vector stores (similar to
    what we did in RAG — Retrieval Augmented Generation), then we could retrieve the
    most relevant bits of information and include them into the conversation. This
    memory type would be the most useful for long-term conversations.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量数据内存——将文本的嵌入存储在向量存储中（类似于我们在RAG——检索增强生成中所做的），然后我们可以检索出最相关的信息并将其纳入对话中。这种内存类型对于长期对话最为有用。
- en: Entity memories to remember details about specific entities (i.e. people).
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实体内存，用于记住关于特定实体（如人）的详细信息。
- en: You can even combine different memory types. For example, you can use conversation
    memory + entity memory to keep details about the tables in the database. To learn
    more about combined memory, consult [the documentation](https://python.langchain.com/docs/modules/memory/multiple_memory).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你甚至可以结合使用不同的内存类型。例如，你可以使用对话内存+实体内存来保存数据库中表格的详细信息。要了解更多关于组合内存的信息，请参阅[文档](https://python.langchain.com/docs/modules/memory/multiple_memory)。
- en: We won’t discuss these more advanced approaches in this article.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会在本文中讨论这些更先进的方法。
- en: We’ve got an understanding of how we can implement memory in LangChain. Now,
    it’s time to use this knowledge for our agent.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了如何在LangChain中实现记忆。现在，是时候将这些知识应用到我们的代理中了。
- en: Adding memory to the agent
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为代理添加记忆
- en: Let’s try to see how the current agent implementation works with the follow-up
    questions from the user.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试看看当前的代理实现如何处理来自用户的后续问题。
- en: '[PRE16]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'For this call, the agent executed a tool and returned the correct answer: `The
    number of active customers in London in December 2023 was 4,286.`'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个调用，代理执行了一个工具并返回了正确的答案：`2023年12月伦敦的活跃客户数量为4,286`。
- en: We know the number of users for London. It would be interesting to learn about
    Berlin as well. Let’s ask our agent.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道伦敦的用户数量了。现在了解柏林的情况也很有趣。让我们问问我们的代理。
- en: '[PRE17]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Surprisingly, the agent was able to handle this question correctly. However,
    it had to clarify the questions using the Human tool, and the user had to provide
    the same information (not the best customer experience).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，代理能够正确地处理这个问题。然而，它必须使用“人工工具”来澄清问题，并且用户需要提供相同的信息（这不是最好的客户体验）。
- en: '![](../Images/9031d4e99965bb6760a04deb221449a9.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9031d4e99965bb6760a04deb221449a9.png)'
- en: Screenshot by author
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 截图由作者提供
- en: Now, let’s start holding the chart history for the agent. I will use a simple
    buffer that stores the complete previous conversation, but you could use a more
    complex strategy.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始为代理保存聊天历史。我将使用一个简单的缓冲区来存储完整的前置对话，但你也可以使用更复杂的策略。
- en: First, we need to add a placeholder for the chat history to the prompt template.
    I’ve marked it as optional.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要在提示模板中为聊天历史添加一个占位符。我已将其标记为可选。
- en: '[PRE18]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Next, let’s initialise a memory and save a small talk (it’s impossible to have
    a chat without a small talk, you know). Note that we’ve specified the same `memory_key
    = 'chat_history’` as in the prompt template.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们初始化一个记忆并保存闲聊（你知道，没有闲聊就没有对话）。请注意，我们在提示模板中指定了相同的`memory_key = 'chat_history'`。
- en: '[PRE19]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Let’s try the previous use case once again and ask the LLM analyst about the
    number of users in London.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次尝试之前的用例，并询问LLM分析师关于伦敦的用户数量。
- en: '[PRE20]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: After answering the question, `"Could you please specify the time period for
    which you would like to know the number of customers in London?"`, we got the
    correct answer and the conversation history between the agent and the user with
    all the previous statements, including the small talk.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在回答问题：“‘请指定你希望了解伦敦用户数量的时间段？’”后，我们得到了正确的答案，并且代理和用户之间的对话历史（包括之前的闲聊）也被保存了下来。
- en: If we ask the follow-up question about Berlin now, the agent will just return
    the number for December 2023 without asking for details because it already has
    it in the context.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在问关于柏林的后续问题，代理将直接返回2023年12月的数字，而无需再次询问详细信息，因为它已经在上下文中有了这些信息。
- en: '[PRE21]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Let’s look at the prompt for the first LLM call. We can see that all chat history
    was actually passed to the model.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看第一次LLM调用的提示。我们可以看到，所有的聊天历史实际上都传递给了模型。
- en: '[PRE22]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: So, we’ve added the chat history to our LLM-powered analyst, and now it can
    handle somewhat long conversations and answer follow-up questions. That’s a significant
    achievement.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已将聊天历史添加到我们的LLM驱动分析师中，现在它能够处理较长的对话并回答后续问题。这是一个重大的进步。
- en: You can find the complete code on [GitHub](https://github.com/miptgirl/miptgirl_medium/blob/main/analyst_agent/agent_prototype_collaboration.ipynb).
  id: totrans-129
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可以在[GitHub](https://github.com/miptgirl/miptgirl_medium/blob/main/analyst_agent/agent_prototype_collaboration.ipynb)上找到完整的代码。
- en: Summary
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this article, we’ve taught our LLM-powered analyst how to collaborate with
    users. Now, it can ask clarifying questions if there’s not enough information
    in the initial request and even answer the follow-up question from the user.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们教会了我们的LLM驱动分析师如何与用户协作。现在，它可以在初始请求中信息不足时提问澄清问题，甚至可以回答用户的后续问题。
- en: 'We’ve achieved such a significant improvement:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经取得了如此显著的改进：
- en: by adding a tool — Human input that allows to ask the user,
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过添加一个工具——人工输入，允许向用户提问，
- en: by adding a memory to the agent that can store the chat history.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过为代理添加记忆功能，可以存储聊天历史。
- en: Our agent has mastered collaboration now. In one of the following articles,
    we will try to take the next step and combine LLM agents with RAG (Retrieval Augmented
    Generation). We’ve understood how to query databases and communicate with the
    users. The next step is to start using knowledge bases. Stay tuned!
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的智能体现在已经掌握了协作技能。在接下来的文章中，我们将尝试迈出下一步，将LLM智能体与RAG（检索增强生成）结合起来。我们已经了解了如何查询数据库并与用户进行沟通。下一步是开始使用知识库。敬请期待！
- en: Thank you a lot for reading this article. I hope it was insightful to you. If
    you have any follow-up questions or comments, please leave them in the comments
    section.
  id: totrans-136
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 非常感谢您阅读本文。希望它对您有所启发。如果您有任何后续问题或评论，请在评论区留言。
