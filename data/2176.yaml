- en: 'The Evolution of Llama: From Llama 1 to Llama 3.1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Llama的发展：从Llama 1到Llama 3.1
- en: 原文：[https://towardsdatascience.com/the-evolution-of-llama-from-llama-1-to-llama-3-1-13c4ebe96258?source=collection_archive---------5-----------------------#2024-09-06](https://towardsdatascience.com/the-evolution-of-llama-from-llama-1-to-llama-3-1-13c4ebe96258?source=collection_archive---------5-----------------------#2024-09-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-evolution-of-llama-from-llama-1-to-llama-3-1-13c4ebe96258?source=collection_archive---------5-----------------------#2024-09-06](https://towardsdatascience.com/the-evolution-of-llama-from-llama-1-to-llama-3-1-13c4ebe96258?source=collection_archive---------5-----------------------#2024-09-06)
- en: A Comprehensive Guide to the Advancements and Innovations in the Family of Llama
    Models from Meta AI
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 《Meta AI Llama模型家族的进展与创新全面指南》
- en: '[](https://medium.com/@luisroque?source=post_page---byline--13c4ebe96258--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page---byline--13c4ebe96258--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--13c4ebe96258--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--13c4ebe96258--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page---byline--13c4ebe96258--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@luisroque?source=post_page---byline--13c4ebe96258--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page---byline--13c4ebe96258--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--13c4ebe96258--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--13c4ebe96258--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page---byline--13c4ebe96258--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--13c4ebe96258--------------------------------)
    ·15 min read·Sep 6, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--13c4ebe96258--------------------------------)
    ·15分钟阅读·2024年9月6日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '*This post was co-authored with Rafael Guedes.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*本文由Rafael Guedes共同撰写。*'
- en: Introduction
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: Meta has released three major versions of its large language model (LLM), Llama,
    along with a minor (if we can call it that) update (version 3.1). The initial
    release of Llama in early 2023 marked a significant step forward for the open-source
    community in natural language processing (NLP). Meta has consistently contributed
    to this community by sharing its latest LLM versions.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Meta发布了其大型语言模型（LLM）Llama的三个主要版本，并进行了一个小的更新（版本3.1）。Llama在2023年初的首次发布标志着自然语言处理（NLP）领域开源社区的一次重大进步。Meta通过分享其最新的LLM版本，持续为该社区做出贡献。
- en: To ensure correctness, we should distinguish between open and open-source LLMs.
    Open-source software traditionally makes its source code available under specific
    public use and modification licenses. In the context of LLMs, open LLMs typically
    disclose model weights and initial code. At the same time, open-source LLMs would
    also share the entire training process, including training data, with a permissive
    license. Most models today, including Meta’s Llama, fall under the open LLMs category
    since they do not release the datasets used for training.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保准确性，我们应该区分开放LLM和开源LLM。传统的开源软件会在特定的公共使用和修改许可下提供其源代码。在LLM的背景下，开放LLM通常会披露模型权重和初始代码。同时，开源LLM还会分享整个训练过程，包括训练数据，并采用宽松的许可。如今，大多数模型，包括Meta的Llama，都属于开放LLM类别，因为它们没有发布用于训练的数据集。
- en: Llama has undergone three key architectural iterations. Version 1 introduced
    several enhancements to the original Transformer architecture. Version 2 implemented
    Grouped-Query Attention (GQA) in larger models. Version 3 extended GQA to smaller
    models, introduced a more efficient…
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Llama经历了三次关键的架构迭代。版本1对原始的Transformer架构进行了若干增强。版本2在更大的模型中实现了Grouped-Query Attention（GQA）。版本3将GQA扩展到较小的模型，并引入了更高效的……
