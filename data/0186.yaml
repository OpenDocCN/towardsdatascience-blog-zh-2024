- en: Exploring mergekit for Model Merge, AutoEval for Model Evaluation, and DPO for
    Model Fine-tuning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索mergekit进行模型合并，AutoEval进行模型评估，以及DPO进行模型微调
- en: 原文：[https://towardsdatascience.com/exploring-mergekit-for-model-merge-and-autoeval-for-model-evaluation-c681766fd1f3?source=collection_archive---------5-----------------------#2024-01-19](https://towardsdatascience.com/exploring-mergekit-for-model-merge-and-autoeval-for-model-evaluation-c681766fd1f3?source=collection_archive---------5-----------------------#2024-01-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/exploring-mergekit-for-model-merge-and-autoeval-for-model-evaluation-c681766fd1f3?source=collection_archive---------5-----------------------#2024-01-19](https://towardsdatascience.com/exploring-mergekit-for-model-merge-and-autoeval-for-model-evaluation-c681766fd1f3?source=collection_archive---------5-----------------------#2024-01-19)
- en: My observations from experimenting with model merge, evaluation, and two model
    fine-tuning techniques
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我在实验模型合并、评估和两种模型微调技术时的观察结果
- en: '[](https://medium.com/@wenqiglantz?source=post_page---byline--c681766fd1f3--------------------------------)[![Wenqi
    Glantz](../Images/65b518863e01aaa48ecc6b8ac6d1be60.png)](https://medium.com/@wenqiglantz?source=post_page---byline--c681766fd1f3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c681766fd1f3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c681766fd1f3--------------------------------)
    [Wenqi Glantz](https://medium.com/@wenqiglantz?source=post_page---byline--c681766fd1f3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@wenqiglantz?source=post_page---byline--c681766fd1f3--------------------------------)[![Wenqi
    Glantz](../Images/65b518863e01aaa48ecc6b8ac6d1be60.png)](https://medium.com/@wenqiglantz?source=post_page---byline--c681766fd1f3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c681766fd1f3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c681766fd1f3--------------------------------)
    [Wenqi Glantz](https://medium.com/@wenqiglantz?source=post_page---byline--c681766fd1f3--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c681766fd1f3--------------------------------)
    ·14 min read·Jan 19, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c681766fd1f3--------------------------------)
    ·14分钟阅读·2024年1月19日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/fbfcc3ab489becdd52f76fbc342fc138.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fbfcc3ab489becdd52f76fbc342fc138.png)'
- en: Image generated by DALL-E 3 by the author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者通过DALL-E 3生成的图像
- en: Let’s continue our learning journey of [Maxime Labonne](https://medium.com/u/dc89da634938?source=post_page---user_mention--c681766fd1f3--------------------------------)’s
    [llm-course](https://github.com/mlabonne/llm-course), which is pure gold for the
    community. This time, we will focus on model merge, evaluation, and fine-tuning.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续学习[Maxime Labonne](https://medium.com/u/dc89da634938?source=post_page---user_mention--c681766fd1f3--------------------------------)的[llm-course](https://github.com/mlabonne/llm-course)，这对社区来说是纯粹的宝藏。这一次，我们将重点关注模型合并、评估和微调。
- en: Maxime has a great article titled [Merge Large Language Models with mergekit](https://medium.com/towards-data-science/merge-large-language-models-with-mergekit-2118fb392b54).
    I highly recommend you check it out first. We will not repeat the steps he has
    already laid out in his article, but we will explore some details I came across
    that might be helpful to you.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Maxime有一篇很棒的文章，标题是[用mergekit合并大型语言模型](https://medium.com/towards-data-science/merge-large-language-models-with-mergekit-2118fb392b54)。我强烈推荐你先去阅读一下。我们不会重复他在文章中已经列出的步骤，但我们会探索一些我遇到的细节，这些可能对你有帮助。
- en: High-Level Overview
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高层次概述
- en: 'We are going to experiment with model merge, model evaluation, and model fine-tuning
    in the following steps:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在以下步骤中进行模型合并、模型评估和模型微调的实验：
- en: Using [LazyMergekit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing),
    we merge two models from the Hugging Face hub, `mistralai/Mistral-7B-Instruct-v0.2`
    and `jan-hq/trinity-v1`.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用[LazyMergekit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing)，我们合并了来自Hugging
    Face hub的两个模型，`mistralai/Mistral-7B-Instruct-v0.2`和`jan-hq/trinity-v1`。
- en: Run AutoEval on the base model `mistralai/Mistral-7B-Instruct-v0.2`.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对基础模型`mistralai/Mistral-7B-Instruct-v0.2`运行AutoEval。
- en: Run AutoEval on the merged model `MistralTrinity-7b-slerp`.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对合并后的模型`MistralTrinity-7b-slerp`运行AutoEval。
- en: Supervised fine-tune the merged model with QLoRA. Run AutoEval on the fine-tuned
    model.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用QLoRA对合并后的模型进行监督微调。对微调后的模型运行AutoEval。
