- en: Let There Be Light! Diffusion Models and the Future of Relighting
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让光明降临！扩散模型与重光照的未来
- en: 原文：[https://towardsdatascience.com/let-there-be-light-diffusion-models-and-the-future-of-relighting-03af12b8e86c?source=collection_archive---------3-----------------------#2024-11-04](https://towardsdatascience.com/let-there-be-light-diffusion-models-and-the-future-of-relighting-03af12b8e86c?source=collection_archive---------3-----------------------#2024-11-04)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/let-there-be-light-diffusion-models-and-the-future-of-relighting-03af12b8e86c?source=collection_archive---------3-----------------------#2024-11-04](https://towardsdatascience.com/let-there-be-light-diffusion-models-and-the-future-of-relighting-03af12b8e86c?source=collection_archive---------3-----------------------#2024-11-04)
- en: Discover how cutting-edge diffusion models tackle relighting, harmonization,
    and shadow removal in this in-depth blog on scene editing
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解最前沿的扩散模型如何在这篇关于场景编辑的深度博客中处理重光照、协调和阴影去除
- en: '[](https://medium.com/@darth_gera?source=post_page---byline--03af12b8e86c--------------------------------)[![Pulkit
    Gera](../Images/c7e840a79628d71320b8c2c63277df69.png)](https://medium.com/@darth_gera?source=post_page---byline--03af12b8e86c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--03af12b8e86c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--03af12b8e86c--------------------------------)
    [Pulkit Gera](https://medium.com/@darth_gera?source=post_page---byline--03af12b8e86c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@darth_gera?source=post_page---byline--03af12b8e86c--------------------------------)[![Pulkit
    Gera](../Images/c7e840a79628d71320b8c2c63277df69.png)](https://medium.com/@darth_gera?source=post_page---byline--03af12b8e86c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--03af12b8e86c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--03af12b8e86c--------------------------------)
    [Pulkit Gera](https://medium.com/@darth_gera?source=post_page---byline--03af12b8e86c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--03af12b8e86c--------------------------------)
    ·15 min read·Nov 4, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--03af12b8e86c--------------------------------)
    ·阅读时间 15 分钟·2024年11月4日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/7a8e3f4981f22fe3485a083e12fbd430.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a8e3f4981f22fe3485a083e12fbd430.png)'
- en: Photo by [Brian Aitkenhead](https://unsplash.com/@brianaitk0001?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由 [Brian Aitkenhead](https://unsplash.com/@brianaitk0001?utm_source=medium&utm_medium=referral)
    拍摄，图片来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Relighting is the task of rendering a scene under a specified target lighting
    condition, given an input scene. This is a crucial task in computer vision and
    graphics. However, it is an ill-posed problem, because the appearance of an object
    in a scene results from a complex interplay between factors like the light source,
    the geometry, and the material properties of the surface. These interactions create
    ambiguities. For instance, given a photograph of a scene, is a dark spot on an
    object due to a shadow cast by lighting or is the material itself dark in color?
    Distinguishing between these factors is key to effective relighting.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 重光照是指在给定输入场景的情况下，在指定的目标光照条件下渲染场景的任务。这是计算机视觉和图形学中的一项重要任务。然而，它是一个病态问题，因为场景中物体的外观是由光源、几何形状和表面材质属性等因素之间复杂的相互作用所决定的。这些相互作用会产生歧义。例如，给定一张场景的照片，物体上的暗斑是由光照投射的阴影造成的，还是材质本身就是暗色的？区分这些因素是有效重光照的关键。
- en: In this blog post we discuss how different papers are tackling the problem of
    relighting via diffusion models. Relighting encompases a variety of subproblems
    including simple lighting adjustments, image harmonization, shadow removal and
    intrinsic decomposition. These areas are essential for refining scene edits such
    as balancing color and shadow across composited images or decoupling material
    and lighting properties. We will first introduce the problem of relighting and
    briefly discuss Diffusion models and ControlNets. We will then discuss different
    approaches that solve the problem of relighting in different types of scenes ranging
    from single objects to portraits to large scenes.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我们讨论了不同的论文如何通过扩散模型解决重光照问题。重光照涉及多个子问题，包括简单的光照调整、图像协调、阴影去除和内在分解。这些领域对于优化场景编辑至关重要，例如平衡合成图像中的颜色和阴影，或解耦材质与光照属性。我们将首先介绍重光照问题，并简要讨论扩散模型和控制网（ControlNets）。然后，我们将讨论在不同类型的场景中解决重光照问题的不同方法，从单一物体到肖像再到大规模场景。
- en: Solving Relighting
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决重光照问题
- en: 'The goal is to decompose the scene into its fundamental components such as
    geometry, material, and light interactions and model them parametrically. Once
    solved then we can change it according to our preference. The appearance of a
    point in the scene can be described by the rendering equation as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是将场景分解为其基本组成部分，如几何形状、材质和光照交互，并对它们进行参数化建模。求解完成后，我们就可以根据自己的偏好进行修改。场景中一个点的外观可以通过渲染方程来描述，如下所示：
- en: '![](../Images/eebcbafb661916da1ec0d2ffb59eb303.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eebcbafb661916da1ec0d2ffb59eb303.png)'
- en: Rendering Equation from [source](https://twitter.com/levork/status/609603797258600448)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 渲染方程来自[source](https://twitter.com/levork/status/609603797258600448)
- en: Most methods aim to solve for each single component of the rendering equation.
    Once solved, then we can perform relighting and material editing. Since the lighting
    term L is on both sides, this equation cannot be evaluated analytically and is
    either solved via Monte Carlo methods or approximation based approaches.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数方法旨在求解渲染方程的每个单独组成部分。一旦求解完成，我们就可以进行重新照明和材质编辑。由于照明项L出现在方程的两边，这个方程无法通过解析方法求解，只能通过蒙特卡罗方法或基于近似的方法来求解。
- en: An alternate approach is data-driven learning, where instead of explicitly modeling
    the scene properties it directly learns from data. For example, instead of fitting
    a parametric function, a network can learn the material properties of the surface
    from data. Data-driven approaches have proven to be more powerful than parametric
    approaches. However they require a huge amount of high quality data which is really
    hard to collect especially for lighting and material estimation tasks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是数据驱动学习，在这种方法中，模型不显式地建模场景的属性，而是直接从数据中学习。例如，网络可以直接从数据中学习表面材质属性，而不是拟合一个参数化函数。数据驱动的方法已被证明比参数化方法更强大。然而，这些方法需要大量高质量的数据，特别是在照明和材质估计任务中，收集这些数据是非常困难的。
- en: Datasets for lighting and material estimation are rare as they require expensive,
    complex setups such as light stages to capture detailed lighting interactions.
    These setups are accessible to only a few organizations, limiting the availability
    of data for training and evaluation. There are no full-body ground truth light
    stage datasets publicly available which further highlights this challenge.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 用于照明和材质估计的数据集非常稀缺，因为这些数据集需要昂贵且复杂的设备设置，如光照舞台，用以捕捉详细的光照交互。这些设置仅对少数组织可用，从而限制了用于训练和评估的数据的获取。目前没有公开的全身光照舞台真实数据集，这进一步凸显了这一挑战。
- en: Diffusion Models
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩散模型
- en: Computer vision has experienced a significant transformation with the advent
    of pre-training on vast amounts of image and video data available online. This
    has led to the development of foundation models, which serve as powerful general-purpose
    models that can be fine-tuned for a wide range of specific tasks. Diffusion models
    work by learning to model the underlying data distribution from independent samples,
    gradually reversing a noise-adding process to generate realistic data. By leveraging
    their ability to generate high-quality samples from learned distributions, diffusion
    models have become essential tools for solving a diverse set of generative tasks.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大量图像和视频数据在线可用，计算机视觉经历了重大变革。这推动了基础模型的发展，这些模型作为强大的通用模型，可以针对广泛的具体任务进行微调。扩散模型通过从独立样本中学习建模潜在的数据分布，逐渐逆转添加噪声的过程，从而生成真实的数据。通过利用其从学习到的分布中生成高质量样本的能力，扩散模型已成为解决各种生成任务的关键工具。
- en: '![](../Images/285e147fe9962066256db92eaaf48792.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/285e147fe9962066256db92eaaf48792.png)'
- en: Latent Diffusion Models from [source](https://arxiv.org/abs/2112.10752)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在扩散模型来自[source](https://arxiv.org/abs/2112.10752)
- en: One of the most prominent examples of this is [Stable Diffusion](https://github.com/CompVis/stable-diffusion)(SD),
    which was trained on the large-scale LAION-5B dataset that consists of 5 billion
    image text pairs. It has encoded a wealth of general knowledge about visual concepts
    making it suitable for fine-tuning for specific tasks. It has learnt fundamental
    relationships and associations during training such as chairs having 4 legs or
    recognizing structure of cars. This intrinsic understanding has allowed Stable
    Diffusion to generate highly coherent and realistic images and be used for fine
    tuning to predict other modalities. Based on this idea, the question arises if
    we can leverage pretrained SD to solve the problem of scene relighting.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 其中最突出的例子之一是[Stable Diffusion](https://github.com/CompVis/stable-diffusion)(SD)，它在大规模的LAION-5B数据集上进行了训练，该数据集包含50亿对图像和文本。它已经编码了大量关于视觉概念的通用知识，使其适合于针对特定任务的微调。它在训练过程中学习了诸如椅子有四条腿或识别汽车结构等基本关系和联想。这种内在的理解使Stable
    Diffusion能够生成高度连贯且逼真的图像，并可用于微调以预测其他模态。基于这一思想，问题出现了：我们能否利用预训练的SD来解决场景重光的问题？
- en: So how do we fine-tune LDMs? A naive approach is to do transfer learning with
    LDMs. This would be freezing early layers (which capture general features) and
    fine tuning the model on the specific task. While this approach has been used
    by some papers such as [Alchemist](https://arxiv.org/abs/2312.02970) (for Material
    Transfer), it requires a large amount of paired data for the model to generalize
    well. Another drawback to this approach is the risk of catastrophic forgetting,
    where the model losses the knowledge gained during pretraining. This would limit
    its capability on generalizing across various conditions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们如何微调LDMs？一种简单的方法是对LDMs进行迁移学习。这种方法是冻结早期层（捕捉一般特征），并在特定任务上微调模型。虽然一些论文如[Alchemist](https://arxiv.org/abs/2312.02970)（用于材质迁移）已使用过这种方法，但它需要大量配对数据才能使模型很好地泛化。这个方法的另一个缺点是灾难性遗忘的风险，即模型会丧失在预训练过程中获得的知识。这将限制其在不同条件下的泛化能力。
- en: '![](../Images/6faeb78a6bd8b3bd8fda521767bfddbf.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6faeb78a6bd8b3bd8fda521767bfddbf.png)'
- en: ControlNet figure from [source](https://arxiv.org/abs/2302.05543)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[来源](https://arxiv.org/abs/2302.05543)的ControlNet图示
- en: Another approach to fine-tuning such large models is by introducing a ControlNet.
    Here, a copy of the network is made and the weights of the original network are
    frozen. During training only the duplicate network weights are updated and the
    conditioning signal is passed as input to the duplicate network. The original
    network continues to leverage its pretrained knowledge.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 微调这些大模型的另一种方法是引入ControlNet。在这种方法中，首先创建一个网络副本，并冻结原始网络的权重。在训练过程中，仅更新副本网络的权重，并将条件信号作为输入传递给副本网络。原始网络继续利用其预训练的知识。
- en: While this increases the memory footprint, the advantage is that we dont lose
    the generalization capabilities acquired from training on large scale datasets.
    It ensures that it retains its ability to generate high-quality outputs across
    a wide range of prompts while learning the task specific relationships needed
    for the current task.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这增加了内存占用，但优势在于我们不会失去从大规模数据集训练中获得的泛化能力。它确保模型在学习当前任务所需的任务特定关系的同时，仍保持生成高质量输出的能力，适用于各种提示。
- en: Additionally it helps the model learn robust and meaningful connections between
    control input and the desired output. By decoupling the control network from the
    core model, it avoids the risk of overfitting or catastrophic forgetting. It also
    needs significantly less paired data to train.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，它有助于模型学习控制输入与期望输出之间的稳健且有意义的关联。通过将控制网络与核心模型解耦，它避免了过拟合或灾难性遗忘的风险。同时，它所需的配对数据量显著减少。
- en: 'While there are other techniques for fine-tuning foundational models — such
    as LoRA (Low-Rank Adaptation) and others — we will focus on the two methods discussed:
    traditional transfer learning and ControlNet. These approaches are particularly
    relevant for understanding how various papers have tackled image-based relighting
    using diffusion models.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然也有其他用于微调基础模型的技术——例如LoRA（低秩适配）等——但我们将重点讨论两种方法：传统的迁移学习和ControlNet。这些方法对于理解各种论文如何使用扩散模型处理基于图像的重光问题尤其重要。
- en: DiLightNet
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DiLightNet
- en: '![](../Images/e7cc7d02681474f33ee17d588721a584.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7cc7d02681474f33ee17d588721a584.png)'
- en: '[DiLightNet:Fine-grained Lighting Control for Diffusion-based Image Generation](https://arxiv.org/pdf/2409.13690)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[DiLightNet:基于扩散的图像生成的细粒度光照控制](https://arxiv.org/pdf/2409.13690)'
- en: '**Introduction**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**介绍**'
- en: This work proposes fine grained control over relighting of an input image. The
    input image can either be generated or given as input. Further it can also change
    the material of the object based on the text prompt. The objective is to exert
    fine-grained control on the effects of lighting.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了对输入图像重光照的细粒度控制。输入图像可以是生成的图像，也可以是作为输入给定的图像。进一步地，它还可以根据文本提示改变物体的材质。目标是对光照效果进行细粒度的控制。
- en: '**Method**'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**方法**'
- en: '![](../Images/d543f869aa1291a2ee5fba0a36d7d8d3.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d543f869aa1291a2ee5fba0a36d7d8d3.png)'
- en: Method figure from [source](https://arxiv.org/pdf/2409.13690)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 方法图来自于[source](https://arxiv.org/pdf/2409.13690)
- en: 'Given an input image, the following preprocessing steps are applied:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个输入图像，应用以下预处理步骤：
- en: Estimate background and depth map using off the shelf SOTA models.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用现成的SOTA模型估计背景和深度图。
- en: Extract mesh by triangulating the depth map
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过三角剖分深度图提取网格
- en: Generate 4 different radiance cues images. Radiance cues images are created
    by assigning the extracted mesh different materials and rendering them under target
    lighting. The radiance cues images act as basis for encoding lighting effects
    such as specular, shadows and global illumination.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成4种不同的辐射线索图像。辐射线索图像是通过为提取的网格分配不同的材质，并在目标光照下渲染它们来创建的。这些辐射线索图像作为编码光照效果（如高光、阴影和全局光照）的基础。
- en: '![](../Images/0f10a43072a7b2914b326255e305f274.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f10a43072a7b2914b326255e305f274.png)'
- en: ControlNet inputs from [source](https://arxiv.org/pdf/2409.13690)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ControlNet的输入来自于[source](https://arxiv.org/pdf/2409.13690)
- en: Once these images are generated, they train a ControlNet module. The input image
    and the mask are passed through an encoder decoder network which outputs a 12
    channel feature map. This is then multiplied with the radiance cues images that
    are channel wise concatenated together. Thus during training, the noisy target
    image is denoised with this custom 12 channel image as conditioning signal.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这些图像生成，它们将训练一个ControlNet模块。输入图像和掩膜会通过一个编码解码网络，该网络输出一个12通道的特征图。然后将该特征图与辐射线索图像按通道进行拼接。因此，在训练过程中，带噪的目标图像会以这个自定义的12通道图像作为条件信号进行去噪。
- en: Additionally an appearance seed is provided to procure consistent appearance
    under different illumination. Without it the network renders a different interpretation
    of light-matter interaction. Additionally one can provide more cues via text to
    alter the appearance such as by adding “plastic/shiny metallic” to change the
    material of the generated image.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，提供了一个外观种子，以确保在不同的光照条件下保持一致的外观。如果没有它，网络会呈现出不同的光物质相互作用的解读。此外，可以通过文本提供更多线索来改变外观，例如添加“塑料/光亮金属”来改变生成图像的材质。
- en: '**Implementation**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**实现**'
- en: The dataset was curated using 25K synthetic objects from Objaverse. Each object
    was rendered from 4 unique views and lit with 12 different lighting conditions
    ranging from point source lighting, multiple point source, environment maps and
    area lights. For training, the radiance cues were rendered in blender.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集使用来自Objaverse的25K个合成物体进行策划。每个物体从4个独特视角进行渲染，并在12种不同的光照条件下进行渲染，这些条件包括点光源、多个点光源、环境光图和区域光。为了训练，辐射线索图像是在Blender中渲染的。
- en: The ControlNet module uses stable diffusion v2.1 as base pretrained model to
    refine. Training took roughly 30 hours on 8x NVIDIA V100 GPUs. Training data was
    rendered in Blender at 512x512 resolution.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ControlNet模块使用稳定扩散v2.1作为基础预训练模型进行微调。训练大约用了30小时，使用了8块NVIDIA V100 GPU。训练数据是在Blender中以512x512分辨率渲染的。
- en: '**Results**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**结果**'
- en: '![](../Images/b1acd57738d769c1ed1202bac18da10b.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b1acd57738d769c1ed1202bac18da10b.png)'
- en: DiLightNet results from [source](https://arxiv.org/pdf/2409.13690)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: DiLightNet的结果来自于[source](https://arxiv.org/pdf/2409.13690)
- en: This figure shows the provisional image as reference and the corresponding target
    lighting under which the object is relit.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 该图展示了作为参考的临时图像和物体在重新光照下的目标光照。
- en: '![](../Images/256c63b5832c8e8f8e26644606908b40.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/256c63b5832c8e8f8e26644606908b40.png)'
- en: DiLightNet results from [source](https://arxiv.org/pdf/2409.13690)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: DiLightNet的结果来自于[source](https://arxiv.org/pdf/2409.13690)
- en: This figure shows how the text prompt can be used to change the material of
    the object.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 该图展示了如何使用文本提示来改变物体的材质。
- en: '![](../Images/6979caf7f09e042cc803b9e03051a7ff.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6979caf7f09e042cc803b9e03051a7ff.png)'
- en: DiLightNet results from [source](https://arxiv.org/pdf/2409.13690)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: DiLightNet的结果来自于[source](https://arxiv.org/pdf/2409.13690)
- en: This figure shows more results of AI generated provisional images that are then
    rendered under different input environment light conditions.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 该图展示了AI生成的临时图像的更多结果，这些图像随后在不同的输入环境光照条件下进行了渲染。
- en: '![](../Images/35a2d76fa45cbe4185b49d99c505f004.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/35a2d76fa45cbe4185b49d99c505f004.png)'
- en: DiLightNet results from [source](https://arxiv.org/pdf/2409.13690)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: DiLightNet结果来自于[source](https://arxiv.org/pdf/2409.13690)
- en: This figure shows the different solutions the network comes up to resolve light
    interaction if the appearance seed is not fixed.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 该图展示了当外观种子未固定时，网络为解决光照交互问题所提出的不同解决方案。
- en: '**Limitations**'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**局限性**'
- en: Due to training on synthetic objects, the method is not very good with real
    images and works much better with AI generated provisional images. Additionally
    the material light interaction might not follow the intention of the prompt. Since
    it relies on depth maps for generating radiance cues, it may fail to get satisfactory
    results. Finally generating a rotating light video may not result in consistent
    results.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在合成物体上进行训练，该方法在处理真实图像时效果不佳，而在AI生成的临时图像上表现更好。此外，材料的光照交互可能无法完全按照提示的意图进行。由于它依赖于深度图生成辐射线索，因此可能无法获得令人满意的结果。最后，生成旋转光源的视频可能无法产生一致的结果。
- en: Neural Gaffer
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经光照
- en: '![](../Images/72883b2854c2037fa5466a9ccac7d95b.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/72883b2854c2037fa5466a9ccac7d95b.png)'
- en: '[Neural Gaffer: Relighting Any Object via Diffusion](https://arxiv.org/pdf/2406.07520)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[神经光照：通过扩散重新照明任何物体](https://arxiv.org/pdf/2406.07520)'
- en: '**Introduction**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**介绍**'
- en: This work proposes an end to end 2D relighting diffusion model. This model learns
    physical priors from synthetic dataset featuring physically based materials and
    HDR environment maps. It can be further used to relight multiple views and be
    used to create a 3D representation of the scene.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了一种端到端的2D光照扩散模型。该模型从具有物理基础材料和HDR环境图的合成数据集中学习物理先验。它还可以用于重新照亮多个视角，并用于创建场景的3D表示。
- en: '**Method**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**方法**'
- en: '![](../Images/f7f7dd9fd071a66d99d238ea8a5efe35.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f7f7dd9fd071a66d99d238ea8a5efe35.png)'
- en: Neural Gaffer method figure from [source](https://arxiv.org/pdf/2406.07520)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 神经光照方法图来自于[source](https://arxiv.org/pdf/2406.07520)
- en: Given an image and a target HDR environment map, the goal is to learn a model
    that can synthesize a relit version of the image which here is a single object.
    This is achieved by adopting a pre-trained [Zero-1-to-3](https://arxiv.org/abs/2303.11328)
    model. Zero-1-to-3 is a diffusion model that is conditioned on view direction
    to render novel views of an input image. They discard its novel view synthesis
    components. To incorporate lighting conditions, they concatenate input image and
    environment map encodings with the denoising latent.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一张图像和一个目标HDR环境图，目标是学习一个能够合成重新照明版本的图像的模型，这里是一个单一的物体。这是通过采用预训练的[Zero-1-to-3](https://arxiv.org/abs/2303.11328)模型实现的。Zero-1-to-3是一个扩散模型，它以视角方向为条件来渲染输入图像的新视图。他们丢弃了新视图合成的部分。为了结合光照条件，他们将输入图像和环境图的编码与去噪潜在变量拼接在一起。
- en: 'The input HDR environment map E is split into two components: E_l, a tone-mapped
    LDR representation capturing lighting details in low-intensity regions, and E_h,
    a log-normalized map preserving information across the full spectrum. Together,
    these provide the network with a balanced representation of the energy spectrum,
    ensuring accurate relighting without the generated output appearing washed out
    due to extreme brightness.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 输入的HDR环境图E被拆分为两个组件：E_l，一个色调映射的LDR表示，用于捕捉低强度区域的光照细节，以及E_h，一个对数归一化的图，用于保留跨整个光谱的信息。这两者共同为网络提供了一个平衡的能量谱表示，确保在重新照明时不会因过高的亮度而导致生成的输出看起来被冲淡。
- en: Additionally the CLIP embedding of the input image is also passed as input.
    Thus the input to the model is the Input Image, LDR Image, Normalized HDR Image
    and CLIP embedding of Image all conditioning the denoising network. This network
    is then used as prior for further 3D object relighting.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，输入图像的CLIP嵌入也作为输入传递。因此，模型的输入包括输入图像、LDR图像、标准化的HDR图像和图像的CLIP嵌入，这些都为去噪网络提供条件。该网络随后作为先验用于进一步的3D物体重新照明。
- en: '**Implementation**'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**实现**'
- en: The model is trained on a custom Relit Objaverse Dataset that consists of 90K
    objects. For each object there are 204 images that are rendered under different
    lighting conditions and viewpoints. In total, the dataset consists of 18.4 M images
    at resolution 512x512.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在一个自定义的Relit Objaverse数据集上进行训练，该数据集包含90K个物体。对于每个物体，提供了204张在不同光照条件和视角下渲染的图像。总的来说，数据集包含了1840万张分辨率为512x512的图像。
- en: The model is finetuned from Zero-1-to-3’s checkpoint and only the denoising
    network is finetined. The input environment map is downsampled to 256x256 resolution.
    The model is trained on 8 A6000 GPUs for 5 days. Further downstream tasks such
    as text-based relighting and object insertion can be achieved.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型是从Zero-1-to-3的检查点进行微调的，且仅微调了去噪网络。输入环境图被下采样至256x256分辨率。该模型在8个A6000 GPU上训练了5天。进一步的下游任务，如基于文本的重光照和物体插入也可以实现。
- en: '**Results**'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**结果**'
- en: They show comparisons with different backgrounds and comparisons with other
    works such as DilightNet and [IC-Light](https://github.com/lllyasviel/IC-Light).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 他们展示了与不同背景的对比，以及与其他工作如DilightNet和[IC-Light](https://github.com/lllyasviel/IC-Light)的对比。
- en: '![](../Images/6babf1b85e8818a8e3044f9702c99353.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6babf1b85e8818a8e3044f9702c99353.png)'
- en: Neural Gaffer results from [source](https://arxiv.org/pdf/2406.07520)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Neural Gaffer的结果来源于[source](https://arxiv.org/pdf/2406.07520)
- en: This figure compares the relighting results of their method with IC-Light, another
    ControlNet based method. Their method can produce consistent lighting and color
    with the rotating environment map.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 该图展示了他们的方法与另一种基于ControlNet的方法IC-Light的重光照结果对比。他们的方法能够与旋转的环境图一致地产生光照和颜色。
- en: '![](../Images/64961cf772f654a103aa0aae7fc76b72.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/64961cf772f654a103aa0aae7fc76b72.png)'
- en: Neural Gaffer results from [source](https://arxiv.org/pdf/2406.07520)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Neural Gaffer的结果来源于[source](https://arxiv.org/pdf/2406.07520)
- en: This figure compares the relighting results of their method with DiLightnet,
    another ControlNet based method. Their method can produce specular highlights
    and accurate colors.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 该图展示了他们的方法与另一种基于ControlNet的方法DiLightnet的重光照结果对比。他们的方法能够产生镜面高光和准确的颜色。
- en: '**Limitations**'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**局限性**'
- en: A major limitation is that it only produces low image resolution (256x256).
    Additionally it only works on objects and performs poorly for portrait relighting.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一个主要的局限性是它只生成低分辨率的图像（256x256）。此外，它仅适用于物体，并且在人物重光照方面表现较差。
- en: Relightful Harmonization
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重光照协调
- en: '![](../Images/9c8aa09f65174b1a0080250bfb7ec656.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c8aa09f65174b1a0080250bfb7ec656.png)'
- en: '[Relightful Harmonization: Lighting-aware Portrait Background Replacement](https://arxiv.org/pdf/2312.06886)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[Relightful Harmonization: Lighting-aware Portrait Background Replacement](https://arxiv.org/pdf/2312.06886)'
- en: '**Introduction**'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介**'
- en: Image Harmonization is the process of aligning the color and lighting features
    of the foreground subject with the background to make it a plausible composition.
    This work proposes a diffusion based approach to solve the task.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图像协调是将前景主体的颜色和光照特征与背景对齐的过程，使其成为一个合理的合成。该研究提出了一种基于扩散的方法来解决这一任务。
- en: '**Method**'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**方法**'
- en: '![](../Images/41e5d56268af57453ecb1588651d890b.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41e5d56268af57453ecb1588651d890b.png)'
- en: Method figure from [source](https://arxiv.org/pdf/2312.06886)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 方法图来自[source](https://arxiv.org/pdf/2312.06886)
- en: Given an input composite image, alpha mask and a target background, the goal
    is to predict a relit portrait image. This is achieved by training a ControlNet
    to predict the Harmonized image output.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个输入合成图像、alpha掩码和目标背景，目标是预测一张重光照的人物图像。通过训练一个ControlNet来预测协调后的图像输出，从而实现这一目标。
- en: In the first stage, we train a background control net model that takes the composite
    image and target background as input and outputs a relit portrait image. During
    training, the denoising network takes the noisy target image concatenated with
    composite image and predicts the noise. The background is provided as conditioning
    via the control net. Since background image by itself are LDR, they do not provide
    sufficient signals for relighting purposes.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一阶段，我们训练了一个背景控制网络模型，该模型以合成图像和目标背景为输入，输出重光照的人物图像。在训练过程中，去噪网络将嘈杂的目标图像与合成图像拼接在一起，并预测噪声。背景作为条件通过控制网络提供。由于背景图像本身是LDR，它们没有提供足够的信号用于重光照。
- en: In the second stage, an environment map control net model is trained. The HDR
    environment map provide lot more signals for relighting and this gives lot better
    results. However at test time, the users only provide LDR backgrounds. Thus, to
    bridge this gap, the 2 control net models are aligned with each other.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二阶段，训练了一个环境映射控制网络模型。HDR环境映射提供了更多的信号用于重光照，这带来了更好的效果。然而在测试时，用户仅提供LDR背景。因此，为了弥合这一差距，两个控制网络模型彼此对齐。
- en: Finally more data is generated using the environment map ControlNet model and
    then the background ControlNet model is finetuned to generate more photo realistic
    results.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用环境图控制网络模型生成更多数据，然后微调背景控制网络模型，以生成更具照片真实感的结果。
- en: '**Implementation**'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**实现**'
- en: The dataset used for training consists of 400k image pair samples that were
    curated using 100 lightstage. In the third stage additional 200k synthetic samples
    were generated for finetuning for photorealism.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练的数据集包含 40 万对图像样本，这些样本是通过 100 个光阶段（lightstage）精心策划的。在第三阶段，生成了额外的 20 万个合成样本，用于光线真实感的微调。
- en: The model is finetuned from InstructPix2PIx checkpoint The model is trained
    on 8 A100 GPUs at 512x512 resolution.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型是从 InstructPix2Pix 检查点进行微调的，模型在 8 个 A100 GPU 上进行训练，分辨率为 512x512。
- en: '**Results**'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**结果**'
- en: '![](../Images/006be013588b7d6c75686bc67f409787.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/006be013588b7d6c75686bc67f409787.png)'
- en: Relightful Harmonization results from [source](https://arxiv.org/pdf/2312.06886)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Relightful Harmonization 的结果来自于 [source](https://arxiv.org/pdf/2312.06886)
- en: This figure shows how the method neutralizes pronounced shadows in input which
    are usually hard to remove. On the left is input and right is relit image.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 该图展示了该方法如何中和输入图像中显著的阴影，这些阴影通常很难去除。左侧为输入图像，右侧为重新光照后的图像。
- en: '![](../Images/da10eb947f17e6da62edf5df1e3deb56.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da10eb947f17e6da62edf5df1e3deb56.png)'
- en: Relightful Harmonization results from [source](https://arxiv.org/pdf/2312.06886)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Relightful Harmonization 的结果来自于 [source](https://arxiv.org/pdf/2312.06886)
- en: '![](../Images/a9af9f87c920b8c7b77cc5b1662515a9.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9af9f87c920b8c7b77cc5b1662515a9.png)'
- en: Relightful Harmonization results from [source](https://arxiv.org/pdf/2312.06886)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Relightful Harmonization 的结果来自于 [source](https://arxiv.org/pdf/2312.06886)
- en: The figures show results on real world test subjects. Their method is able to
    remove shadows and make the composition more plausible compared to other methods.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图示展示了在真实世界测试对象上的结果。与其他方法相比，他们的方法能够去除阴影，并使得合成更加可信。
- en: '**Limitations**'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**局限性**'
- en: While this method is able to plausibly relight the subject, it is not great
    at identity preservation and struggles in maintaining color of the clothes or
    hair. Further it may struggle to eliminate shadow properly. Also it does not estimate
    albedo which is crucial for complex light interactions.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管该方法能够可信地重光照目标，但在保持身份一致性方面表现不佳，尤其在保持衣物或头发的颜色方面有困难。此外，它可能在去除阴影方面存在问题。同时，它没有估计反射率（albedo），而反射率对于复杂的光照交互至关重要。
- en: Multi-Illumination Synthesis
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多光照合成
- en: '![](../Images/44a7b9666c182fb5958fd12b808554b9.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44a7b9666c182fb5958fd12b808554b9.png)'
- en: '[A Diffusion Approach to Radiance Field Relighting using Multi-Illumination
    Synthesis](https://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/content/paper.pdf)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[基于扩散方法的辐射场重光照与多光照合成](https://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/content/paper.pdf)'
- en: '**Introduction**'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**介绍**'
- en: This work proposes a 2D relighting diffusion model that is further used to relight
    a radiance field of a scene. It first trains a ControlNet model to predict the
    scene under novel light directions. Then this model is used to generate more data
    which is eventually used to fit a relightable radiance field. We discuss the 2D
    relighting model in this section.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究提出了一种 2D 重光照扩散模型，该模型进一步用于重光照场景的辐射场。首先训练一个 ControlNet 模型，预测场景在新光照方向下的表现。然后，使用该模型生成更多数据，最终用于拟合一个可重光照的辐射场。在本节中，我们讨论了
    2D 重光照模型。
- en: '**Method**'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**方法**'
- en: Given a set of images X_i with corresponding depth map D (that is calculated
    via off the shelf methods), and light direction l_i the goal is to predict the
    scene under light direction l_j. During training, the input to the denoising network
    is X_i under random illumination, depth map D concatenated with noisy target image
    X_j. The light direction is encoded with 4th order SH and conditioned via ControlNet
    model.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一组图像 X_i 和对应的深度图 D（通过现成方法计算得到），以及光照方向 l_i，目标是预测场景在光照方向 l_j 下的表现。在训练过程中，去噪网络的输入是随机光照下的
    X_i 图像，深度图 D 与含噪的目标图像 X_j 拼接在一起。光照方向通过 4 阶 SH 编码，并通过 ControlNet 模型进行条件化。
- en: '![](../Images/26f19d3cf8ca01a8e59224fade3490e0.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26f19d3cf8ca01a8e59224fade3490e0.png)'
- en: Method figure from [source](https://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/content/paper.pdf)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 方法图来自于 [source](https://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/content/paper.pdf)
- en: Although this leads to decent results, there are some significant problems.
    It is unable to preserve colors and leads to loss in contrast. Additionally it
    produces distorted edges. To resolve this, they color-match the predictions to
    input image to compensate for color difference. This is done by converting the
    image to LAB space and then channel normalization. The loss is then taken between
    ground-truth and denoised output. To preserve edges, the decoder was pretrained
    on image inpainting tasks which was useful in preserving edges. This network is
    then used to create corresponding scene under novel light directions which is
    further used to create a relightable radiance field representation.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这产生了相当不错的结果，但也存在一些显著的问题。它无法保持颜色，导致对比度丧失。此外，它还会产生扭曲的边缘。为了解决这个问题，他们通过将预测图像与输入图像进行颜色匹配来弥补颜色差异。这是通过将图像转换为LAB空间并进行通道归一化来完成的。然后计算真实值和去噪输出之间的损失。为了保持边缘，解码器在图像修复任务上进行了预训练，这有助于保持边缘。然后，使用这个网络来创建在新光照方向下的相应场景，进一步用来创建可重光照的辐射场表示。
- en: '**Implementation**'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**实现**'
- en: '![](../Images/0c11081146d4da1f30d2b2a03b504318.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c11081146d4da1f30d2b2a03b504318.png)'
- en: Inference figure of 2D relighting module from [source](https://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/content/paper.pdf)
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[source](https://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/content/paper.pdf)的2D重光照模块推理图
- en: The method was developed upon Multi-Illumination dataset. It consists of 1000
    real scenes of indoor scenes captured under 25 lighting directions. The images
    also consist of a diffuse and a metallic sphere ball that is useful for obtaining
    the light direction in world coordinates. Additionally some more scenes were rendered
    in Blender. The network was trained on images at resolution 1536x1024 and training
    consisted of 18 non-front facing light directions on 1015 indoor scenes.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法是基于多光照数据集开发的。该数据集包含了1000个真实的室内场景，捕捉了25个光照方向下的图像。图像还包括一个漫反射球和一个金属球，这对于获取世界坐标系中的光照方向非常有用。此外，还使用Blender渲染了更多的场景。网络训练使用了分辨率为1536x1024的图像，训练包括了1015个室内场景中的18个非正面朝向的光照方向。
- en: The ControlNet module was trained using Stable Diffusion v2.1 model as backbone.
    It was trained on multiple A6000 GPUs for 150K iterations.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ControlNet模块使用Stable Diffusion v2.1模型作为基础进行训练。它在多个A6000 GPU上训练了150K次迭代。
- en: '**Results**'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**结果**'
- en: '![](../Images/867838e946028a45ca45db6b61e66920.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/867838e946028a45ca45db6b61e66920.png)'
- en: Results figure from [source](https://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/content/paper.pdf)
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[source](https://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/content/paper.pdf)的结果图
- en: Here the diffuse spheres show the test time light directions. As can be seen,
    the method can render plausible relighting results
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的漫反射球展示了测试时的光照方向。可以看到，该方法可以渲染出合理的重光照结果。
- en: '![](../Images/80b2fbbb5c5b9468ed12257070d043dc.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80b2fbbb5c5b9468ed12257070d043dc.png)'
- en: Results figure from [source](https://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/content/paper.pdf)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[source](https://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/content/paper.pdf)的结果图
- en: This figure shows how with the changing light direction, the specular highlights
    and shadows are moving as evident on the shiny highlight on the kettle.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图展示了随着光照方向的变化，镜面高光和阴影的移动，正如水壶上闪亮的高光所示。
- en: '![](../Images/039969c08ca765eef247b61ee7b07b17.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/039969c08ca765eef247b61ee7b07b17.png)'
- en: Results figure from [source](https://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/content/paper.pdf)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[source](https://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/content/paper.pdf)的结果图
- en: This figure compares results with other relightable radiance field methods.
    Their method clearly preserves color and contrast much better compared to other
    methods.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图将结果与其他可重光照的辐射场方法进行了比较。与其他方法相比，他们的方法显著地更好地保留了颜色和对比度。
- en: '**Limitations**'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**局限性**'
- en: The method does not enforce physical accuracy and can produce incorrect shadows.
    Additionally it also struggles to completely remove shadows in a fully accurate
    way. Also it does work reasonably for out of distribution scenes where the variance
    in lighting is not much.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法没有强制物理准确性，可能会产生不正确的阴影。此外，它也难以完全去除阴影，且无法完全准确地去除阴影。此外，对于光照变化不大的分布外场景，它的表现还算合理。
- en: LightIt
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LightIt
- en: '![](../Images/81d3e65b89db90f7fabcde14396167cb.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81d3e65b89db90f7fabcde14396167cb.png)'
- en: '[LightIt: Illumination Modeling and Control for Diffusion Models](https://arxiv.org/pdf/2403.10615)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[LightIt: 照明建模与扩散模型的控制](https://arxiv.org/pdf/2403.10615)'
- en: '**Introduction**'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介**'
- en: This work proposes a single view shading estimation method to generate a paired
    image and its corresponding direct light shading. This shading can then be used
    to guide the generation of the scene and relight a scene. They approach the problem
    as an intrinsic decomposition problem where the scene can be split into Reflectance
    and Shading. We will discuss the relighting component here.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了一种单视角阴影估计方法，用于生成配对图像及其对应的直射光阴影。然后可以利用这个阴影来引导场景生成并对场景进行重新照明。他们将该问题作为内在分解问题处理，其中场景可以分解为反射率和阴影。我们将在此讨论重新照明的组件。
- en: '**Method**'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**方法**'
- en: '![](../Images/36add64e8ec06b00ddd78eb38969ff54.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36add64e8ec06b00ddd78eb38969ff54.png)'
- en: Method figure from [source](https://arxiv.org/pdf/2403.10615)
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 方法图来自于 [source](https://arxiv.org/pdf/2403.10615)
- en: Given an input image, its corresponding surface normal, text conditioning and
    a target direct shading image, they generate a relit stylized image. This is achieved
    by training a ControlNet module.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一张输入图像、对应的表面法线、文本条件和目标直射光阴影图像，他们生成了一张重新照明的风格化图像。这是通过训练一个ControlNet模块实现的。
- en: During training, the noisy target image is passed to the denoising network along
    with text conditioning. The normal and target direct shading image are concatenated
    and passed through a Residual Control Encoder. The feature map is then used to
    condition the network. Additionally its also reconstructed back via Residual Control
    Decoder to regularize the training
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，带噪声的目标图像与文本条件一起传递到去噪网络。法线图和目标直射光阴影图像被串联后传入残差控制编码器。特征图随后被用来作为网络的条件输入。此外，它还通过残差控制解码器进行重构，以规范化训练过程。
- en: '**Implementation**'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**实现**'
- en: '![](../Images/695a3799c65c3f1855b0ba28d22aa248.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/695a3799c65c3f1855b0ba28d22aa248.png)'
- en: Dataset figure from [source](https://arxiv.org/pdf/2403.10615)
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集图来自于 [source](https://arxiv.org/pdf/2403.10615)
- en: The dataset consists of Outdoor Laval Dataset which consist of outdoor real
    world HDR panoramas. From these images, 250 512x512 images are cropped and various
    camera effects are applied. The dataset consists of 51250 samples of LDR images
    and text prompts along with estimated normal and shading maps. The normals maps
    were estimated from depth maps that were estimated using off the shelf estimators.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含了户外Laval数据集，其中包含了真实世界的户外HDR全景图像。通过这些图像，裁剪出了250张512x512的图像，并应用了各种相机效果。数据集包含了51250个LDR图像样本和文本提示，以及估算的法线图和阴影图。法线图是通过使用现成的估算器从深度图中估算得出的。
- en: The ControlNet module was finetuned from stable diffusion v1.5\. The network
    was trained for two epochs. Other training details are not shared.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ControlNet模块是基于稳定扩散v1.5进行微调的。该网络训练了两个周期。其他训练细节未公开。
- en: '**Results**'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**结果**'
- en: '![](../Images/314ec873f97a6984c1734c417181c8fd.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/314ec873f97a6984c1734c417181c8fd.png)'
- en: Results figure from [source](https://arxiv.org/pdf/2403.10615)
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图来自于 [source](https://arxiv.org/pdf/2403.10615)
- en: This figure shows that the generated images feature consistent lighting aligned
    with target shading for custom stylized text prompts. This is different from other
    papers discussed whose sole focus is on photorealism.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 该图展示了生成的图像在自定义风格化文本提示下，光照与目标阴影保持一致。这与其他仅关注照片级真实感的论文有所不同。
- en: '![](../Images/b7603273635fd4716981670c651f25b9.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7603273635fd4716981670c651f25b9.png)'
- en: Results figure from [source](https://arxiv.org/pdf/2403.10615)
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图来自于 [source](https://arxiv.org/pdf/2403.10615)
- en: This figure shows identity preservation under different lighting conditions.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 该图展示了在不同光照条件下的身份保持效果。
- en: '![](../Images/b84d91833607c18321534c4c1f3e02e0.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b84d91833607c18321534c4c1f3e02e0.png)'
- en: Results figure from [source](https://arxiv.org/pdf/2403.10615)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图来自于 [source](https://arxiv.org/pdf/2403.10615)
- en: This figure shows results on different styles and scenes under changing lighting
    conditions.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 该图展示了在不同风格和场景下，随着光照条件变化的结果。
- en: '![](../Images/b3364a0266714b2402a305762025eb1d.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3364a0266714b2402a305762025eb1d.png)'
- en: Results figure from [source](https://arxiv.org/pdf/2403.10615)
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图来自于 [source](https://arxiv.org/pdf/2403.10615)
- en: This figure compares relighting with another method. Utilizing the diffusion
    prior helps with generalization and resolving shading disambiguation.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 该图将重新照明与另一种方法进行了比较。利用扩散先验有助于提高泛化能力并解决阴影歧义问题。
- en: '**Limitations**'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**局限性**'
- en: Since this method assumes directional lighting, it enables tracing rays in arbitrary
    direction. It requires shading cues to generate images which are non trivial to
    obtain. Further their method does not work for portraits and indoor scenes.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 由于该方法假设定向光照，它支持沿任意方向追踪光线。它需要阴影线索来生成图像，而这些线索并不容易获取。此外，他们的方法不适用于肖像和室内场景。
- en: Takeaways
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主要收获
- en: We have discussed a non-exhaustive list of papers that leverage 2D diffusion
    models for relighting purposes. We explored different ways to condition Diffusion
    models for relighting ranging from radiance cues, direct shading images, light
    directions and environment maps. Most of these methods show results on synthetic
    datasets and dont generalize well to out of distribution datasets. There are more
    papers coming everyday and the base models are also improving. Recently [IC-Light2](https://github.com/lllyasviel/IC-Light/discussions/98)
    was released which is a ControlNet model based upon Flux models. It will be interesting
    which direction it takes as maintaining identities is tricky.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了一个非详尽的文献列表，这些文献利用二维扩散模型进行重光照。我们探索了多种方式来条件化扩散模型进行重光照，从辐射线索、直接阴影图像、光照方向到环境图。大多数这些方法仅在合成数据集上展示结果，并且难以很好地推广到不同分布的数据集。每天都有更多的论文发表，基础模型也在不断改进。最近发布了[IC-Light2](https://github.com/lllyasviel/IC-Light/discussions/98)，这是一个基于Flux模型的ControlNet模型。它的发展方向非常值得关注，因为维持身份一致性是一个棘手的问题。
- en: 'References:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 参考文献：
- en: '[GitHub — lllyasviel/IC-Light: More relighting!](https://github.com/lllyasviel/IC-Light)'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[GitHub — lllyasviel/IC-Light: 更多重光照！](https://github.com/lllyasviel/IC-Light)'
- en: '[IllumiNeRF — 3D Relighting without Inverse Rendering](https://illuminerf.github.io/)'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[IllumiNeRF — 无需反向渲染的三维重光照](https://illuminerf.github.io/)'
- en: '[Neural Gaffer](https://neural-gaffer.github.io/)'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Neural Gaffer](https://neural-gaffer.github.io/)'
- en: '[DiLightNet: Fine-grained Lighting Control for Diffusion-based Image Generation](https://dilightnet.github.io/)'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[DiLightNet：基于扩散图像生成的精细化光照控制](https://dilightnet.github.io/)'
- en: '[Relightful Harmonization](https://arxiv.org/pdf/2312.06886)'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Relightful Harmonization](https://arxiv.org/pdf/2312.06886)'
- en: '[A Diffusion Approach to Radiance Field Relighting using Multi-Illumination
    Synthesis](https://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/)'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[一种基于扩散方法的辐射场重光照使用多重照明合成](https://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/)'
- en: '[How diffusion models work: the math from scratch | AI Summer](https://theaisummer.com/diffusion-models/)'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[扩散模型是如何工作的：从零开始的数学讲解 | AI Summer](https://theaisummer.com/diffusion-models/)'
- en: '[Tutorial on Diffusion Models for Imaging and Vision](https://arxiv.org/pdf/2403.18103)'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[成像与视觉领域的扩散模型教程](https://arxiv.org/pdf/2403.18103)'
- en: '[Diffusion models from scratch in PyTorch](https://www.youtube.com/watch?v=a4Yfz2FxXiY)'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[从零开始实现扩散模型的PyTorch教程](https://www.youtube.com/watch?v=a4Yfz2FxXiY)'
- en: '[Diffusion Models — Live Coding Tutorial](https://www.youtube.com/watch?v=S_il77Ttrmg)'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[扩散模型 — 现场编码教程](https://www.youtube.com/watch?v=S_il77Ttrmg)'
- en: '[Diffusion Models | Paper Explanation | Math Explained](https://www.youtube.com/watch?v=HoKDTa5jHvg)'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[扩散模型 | 论文解读 | 数学原理讲解](https://www.youtube.com/watch?v=HoKDTa5jHvg)'
- en: '[How I Understand Diffusion Models](https://www.youtube.com/watch?v=i2qSxMVeVLI)
    by Prof Jia Bin Huang'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[我如何理解扩散模型](https://www.youtube.com/watch?v=i2qSxMVeVLI) 由黄家宾教授讲解'
- en: '[Denoising Diffusion Probabilistic Models | DDPM Explained](https://www.youtube.com/watch?v=H45lF4sUgiE)
    Good intuition of math of diffusion models'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[去噪扩散概率模型 | DDPM 讲解](https://www.youtube.com/watch?v=H45lF4sUgiE) 对扩散模型的数学直觉理解'
