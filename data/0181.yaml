- en: End-to-End Data Engineering System on Real Data with Kafka, Spark, Airflow,
    Postgres, and Docker
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于真实数据的端到端数据工程系统，使用 Kafka、Spark、Airflow、Postgres 和 Docker
- en: 原文：[https://towardsdatascience.com/end-to-end-data-engineering-system-on-real-data-with-kafka-spark-airflow-postgres-and-docker-a70e18df4090?source=collection_archive---------0-----------------------#2024-01-19](https://towardsdatascience.com/end-to-end-data-engineering-system-on-real-data-with-kafka-spark-airflow-postgres-and-docker-a70e18df4090?source=collection_archive---------0-----------------------#2024-01-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/end-to-end-data-engineering-system-on-real-data-with-kafka-spark-airflow-postgres-and-docker-a70e18df4090?source=collection_archive---------0-----------------------#2024-01-19](https://towardsdatascience.com/end-to-end-data-engineering-system-on-real-data-with-kafka-spark-airflow-postgres-and-docker-a70e18df4090?source=collection_archive---------0-----------------------#2024-01-19)
- en: '[](https://medium.com/@hamzagharbi_19502?source=post_page---byline--a70e18df4090--------------------------------)[![Hamza
    Gharbi](../Images/da96d29dfde486875d9a4ed932879aef.png)](https://medium.com/@hamzagharbi_19502?source=post_page---byline--a70e18df4090--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a70e18df4090--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a70e18df4090--------------------------------)
    [Hamza Gharbi](https://medium.com/@hamzagharbi_19502?source=post_page---byline--a70e18df4090--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@hamzagharbi_19502?source=post_page---byline--a70e18df4090--------------------------------)[![Hamza
    Gharbi](../Images/da96d29dfde486875d9a4ed932879aef.png)](https://medium.com/@hamzagharbi_19502?source=post_page---byline--a70e18df4090--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a70e18df4090--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a70e18df4090--------------------------------)
    [Hamza Gharbi](https://medium.com/@hamzagharbi_19502?source=post_page---byline--a70e18df4090--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a70e18df4090--------------------------------)
    ·16 min read·Jan 19, 2024
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a70e18df4090--------------------------------)
    ·16分钟阅读·2024年1月19日
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: This article is part of a project that’s split into two main phases. The first
    phase focuses on building a data pipeline. This involves getting data from an
    API and storing it in a PostgreSQL database. In the second phase, we’ll develop
    an application that uses a language model to interact with this database.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本文是一个分为两个主要阶段的项目的一部分。第一阶段专注于构建数据管道，包括从 API 获取数据并将其存储到 PostgreSQL 数据库中。在第二阶段，我们将开发一个应用程序，使用语言模型与该数据库进行交互。
- en: 'Ideal for those new to data systems or language model applications, this project
    is structured into two segments:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目适合那些刚接触数据系统或语言模型应用的新人，项目分为两个部分：
- en: This initial article guides you through constructing a data pipeline utilizing
    **Kafka** for streaming, **Airflow** for orchestration, **Spark** for data transformation,
    and **PostgreSQL** for storage. To set-up and run these tools we will use **Docker.**
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本文指导您通过构建数据管道，利用 **Kafka** 进行数据流处理，使用 **Airflow** 进行编排，利用 **Spark** 进行数据转化，使用
    **PostgreSQL** 进行数据存储。为了设置和运行这些工具，我们将使用 **Docker**。
- en: The second article, which will come later, will delve into creating agents using
    tools like LangChain to communicate with external databases.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二篇文章将在稍后发布，将深入探讨如何使用如 LangChain 等工具创建代理与外部数据库进行通信。
- en: This first part project is ideal for beginners in data engineering, as well
    as for data scientists and machine learning engineers looking to deepen their
    knowledge of the entire data handling process. Using these data engineering tools
    firsthand is beneficial. It helps in refining the creation and expansion of machine
    learning models, ensuring they perform effectively in practical settings.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目的第一部分非常适合数据工程初学者，同时也适合那些希望加深对整个数据处理过程理解的数据科学家和机器学习工程师。亲自使用这些数据工程工具非常有益，它有助于完善机器学习模型的创建和扩展，确保它们在实际环境中有效运行。
- en: This article focuses more on practical application rather than theoretical aspects
    of the tools discussed. For detailed understanding of how these tools work internally,
    there are many excellent resources available online.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文更多关注工具的实际应用，而不是理论层面的内容。有关这些工具内部如何运作的详细理解，网上有很多优秀的资源可供参考。
- en: Overview
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: 'Let’s break down the data pipeline process step-by-step:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步地拆解数据管道过程：
- en: 'Data Streaming: Initially, data is streamed from the API into a Kafka topic.'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据流：最初，数据通过 API 流入 Kafka 主题。
- en: 'Data Processing: A Spark job then takes over, consuming the data from the Kafka
    topic and transferring it to a PostgreSQL database.'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据处理：Spark 作业接管处理，从 Kafka 主题消费数据，并将其转移到 PostgreSQL 数据库中。
- en: 'Scheduling with Airflow: Both the streaming task and the Spark job are orchestrated
    using Airflow. While in a real-world scenario, the Kafka producer would constantly
    listen to the API, for demonstration purposes, we’ll schedule the Kafka streaming
    task to run daily. Once the streaming is complete, the Spark job processes the
    data, making it ready for use by the LLM application.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Airflow 进行调度：流任务和 Spark 作业都通过 Airflow 进行编排。虽然在实际场景中，Kafka 生产者会持续监听 API，但为了演示的目的，我们将调度
    Kafka 流任务每天运行一次。一旦流处理完成，Spark 作业就会处理数据，并为 LLM 应用程序做好准备。
- en: All of these tools will be built and run using docker, and more specifically
    [docker-compose](https://docs.docker.com/compose/).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些工具将使用 Docker 构建和运行，更具体地说是使用 [docker-compose](https://docs.docker.com/compose/)。
- en: '![](../Images/97b5e1b0b1332bc0aef250d1fdb62728.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97b5e1b0b1332bc0aef250d1fdb62728.png)'
- en: Overview of the data pipeline. Image by the author.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管道概览。图片由作者提供。
- en: Now that we have a blueprint of our pipeline, let’s dive into the technical
    details !
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了管道的蓝图，让我们深入探讨技术细节！
- en: Local setup
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本地设置
- en: 'First you can clone the Github repo on your local machine using the following
    command:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您可以使用以下命令将 Github 仓库克隆到本地计算机：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here is the overall structure of the project:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是项目的整体结构：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `airflow` directory contains a custom Dockerfile for setting up airflow
    and a `[dags](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html)`
    directory to create and schedule the tasks.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`airflow` 目录包含用于设置 Airflow 的自定义 Dockerfile 以及一个 `[dags](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html)`
    目录，用于创建和调度任务。'
- en: The `data` directory contains the *last_processed.json file* which is crucial
    for the Kafka streaming task. Further details on its role will be provided in
    the Kafka section.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data` 目录包含 *last_processed.json 文件*，这是 Kafka 流任务的重要文件。其具体作用将在 Kafka 部分详细说明。'
- en: The `docker-compose-airflow.yaml`file defines all the services required to run
    airflow.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`docker-compose-airflow.yaml` 文件定义了运行 Airflow 所需的所有服务。'
- en: The `docker-compose.yaml` file specifies the Kafka services and includes a docker-proxy.
    This proxy is essential for executing Spark jobs through a docker-operator in
    Airflow, a concept that will be elaborated on later.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`docker-compose.yaml` 文件指定了 Kafka 服务，并包含一个 docker-proxy。这个代理对于通过 Airflow 中的
    docker-operator 执行 Spark 作业至关重要，这一概念将在后面详细介绍。'
- en: The `spark` directory contains a custom Dockerfile for spark setup.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark` 目录包含用于 Spark 设置的自定义 Dockerfile。'
- en: '`src` contains the python modules needed to run the application.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`src` 目录包含运行应用程序所需的 Python 模块。'
- en: 'To set up your local development environment, start by installing the required
    Python packages. The only essential package is psycopg2-binary. You have the option
    to install just this package or all the packages listed in the `requirements.txt`
    file. To install all packages, use the following command:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了设置本地开发环境，首先安装所需的 Python 包。唯一必需的包是 psycopg2-binary。您可以选择只安装这个包，或者安装 `requirements.txt`
    文件中列出的所有包。要安装所有包，请使用以下命令：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Next let’s dive step by step into the project details.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来让我们一步一步地深入项目细节。
- en: About the API
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于 API
- en: 'The API is [RappelConso](https://api.gouv.fr/les-api/api-rappel-conso) from
    the French public services. It gives access to data relating to recalls of products
    declared by professionals in France. The data is in French and it contains initially
    **31** columns (or fields). Some of the most important are:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 该 API 是来自法国公共服务的 [RappelConso](https://api.gouv.fr/les-api/api-rappel-conso)。它提供了与法国专业人员声明的产品召回相关的数据。数据为法语，并最初包含
    **31** 列（或字段）。其中一些最重要的字段包括：
- en: '*reference_fiche (reference sheet):* Unique identifier of the recalled product.
    It will act as the primary key of our Postgres database later.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*reference_fiche (参考表格)*：召回产品的唯一标识符。它将在后续作为我们的 Postgres 数据库的主键。'
- en: '*categorie_de_produit (Product category):* For instance food, electrical appliance,
    tools, transport means, etc …'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*categorie_de_produit (产品类别)*：例如食品、电器、工具、交通工具等……'
- en: '*sous_categorie_de_produit (Product sub-category):* For instance we can have
    meat, dairy products, cereals as sub-categories for the food category.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*sous_categorie_de_produit (产品子类别)*：例如，我们可以将肉类、乳制品、谷物等作为食品类别的子类别。'
- en: '*motif_de_rappel (Reason for recall*): Self explanatory and one of the most
    important fields.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*motif_de_rappel (召回原因)*：显而易见，也是最重要的字段之一。'
- en: '*date_de_publication* which translates to the publication date.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*date_de_publication* 表示发布日期。'
- en: '*risques_encourus_par_le_consommateur* which contains the risks that the consumer
    may encounter when using the product.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*risques_encourus_par_le_consommateur* 包含消费者在使用产品时可能遇到的风险。'
- en: There are also several fields that correspond to different links, such as link
    to product image, link to the distributers list, etc..
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还有一些字段对应不同的链接，例如产品图片链接、分销商列表链接等。
- en: You can see some examples and query manually the dataset records using this
    [link](https://data.economie.gouv.fr/explore/dataset/rappelconso0/api/?disjunctive.categorie_de_produit=&sort=date_de_publication).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过以下[链接](https://data.economie.gouv.fr/explore/dataset/rappelconso0/api/?disjunctive.categorie_de_produit=&sort=date_de_publication)查看一些示例并手动查询数据集记录。
- en: 'We refined the data columns in a few key ways:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在几个关键方面优化了数据列：
- en: Columns like `ndeg_de_version` and `rappelguid`, which were part of a versioning
    system, have been removed as they aren’t needed for our project.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似 `ndeg_de_version` 和 `rappelguid` 这样的列属于版本控制系统的一部分，已经被删除，因为它们在我们的项目中不再需要。
- en: We combined columns that deal with consumer risks — `risques_encourus_par_le_consommateur`
    and `description_complementaire_du_risque` — for a clearer overview of product
    risks.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将涉及消费者风险的列——`risques_encourus_par_le_consommateur` 和 `description_complementaire_du_risque`——合并，以便更清晰地了解产品风险。
- en: The `date_debut_fin_de_commercialisation` column, which indicates the marketing
    period, has been divided into two separate columns. This split allows for easier
    queries about the start or end of a product’s marketing.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`date_debut_fin_de_commercialisation` 列表示营销期，已被拆分为两个独立的列。此拆分便于查询产品营销的开始或结束日期。'
- en: We’ve removed accents from all columns except for links, reference numbers,
    and dates. This is important because some text processing tools struggle with
    accented characters.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已从除链接、参考编号和日期之外的所有列中移除重音符号。这一点很重要，因为一些文本处理工具对带重音的字符处理不佳。
- en: For a detailed look at these changes, check out our transformation script at
    `src/kafka_client/transformations.py`. The updated list of columns is available
    in`src/constants.py` under `DB_FIELDS`.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 要详细查看这些更改，请查看我们的转换脚本 `src/kafka_client/transformations.py`。更新后的列列表可以在 `src/constants.py`
    中的 `DB_FIELDS` 找到。
- en: Kafka streaming
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka 流处理
- en: To avoid sending all the data from the API each time we run the streaming task,
    we define a local json file that contains the last publication date of the latest
    streaming. Then we will use this date as the starting date for our new streaming
    task.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免每次运行流式任务时都发送所有的 API 数据，我们定义了一个本地 JSON 文件，里面包含最新流式处理的最后发布日期。然后我们将使用此日期作为新流式任务的起始日期。
- en: To give an example, suppose that the latest recalled product has a publication
    date of **22 november 2023\.** If we make the hypothesis that all of the recalled
    products infos before this date are already persisted in our Postgres database,
    We can now stream the data starting from the 22 november. Note that there is an
    overlap because we may have a scenario where we didn’t handle all of the data
    of the 22nd of November.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，假设最新召回的产品发布日期为**2023年11月22日**。如果我们假设在此日期之前的所有召回产品信息已经保存在我们的 Postgres 数据库中，我们现在可以从11月22日开始流式传输数据。请注意，这里有重叠，因为我们可能会遇到没有处理完11月22日所有数据的情况。
- en: 'The file is saved in `./data/last_processed.json` and has this format:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 该文件保存在 `./data/last_processed.json` 中，格式如下：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: By default the file is an empty json which means that our first streaming task
    will process all of the API records which are 10 000 approximately.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，该文件是一个空 JSON 文件，这意味着我们的第一个流式任务将处理大约 10,000 条 API 记录。
- en: Note that in a production setting this approach of storing the last processed
    date in a local file is not viable and other approaches involving an external
    database or an object storage service may be more suitable.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在生产环境中，将最后处理日期存储在本地文件中的方法不可行，涉及外部数据库或对象存储服务的其他方法可能更为合适。
- en: The code for the kafka streaming can be found on `./src/kafka_client/kafka_stream_data.py`and
    it involves primarily querying the data from the API, making the transformations,
    removing potential duplicates, updating the last publication date and serving
    the data using the kafka producer.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 流处理的代码可以在 `./src/kafka_client/kafka_stream_data.py` 中找到，主要涉及从 API 查询数据、进行转换、删除潜在重复项、更新最后的发布日期并使用
    Kafka 生产者提供数据。
- en: 'The next step is to run the kafka service defined the docker-compose defined
    below:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是运行下面定义的 Docker Compose 中的 Kafka 服务：
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The key highlights from this file are:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 此文件的关键亮点如下：
- en: The **kafka** service uses a base image `bitnami/kafka`.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**kafka** 服务使用了基础镜像 `bitnami/kafka`。'
- en: We configure the service with only one **broker** which is enough for our small
    project. A Kafka broker is responsible for receiving messages from producers (which
    are the sources of data), storing these messages, and delivering them to consumers
    (which are the sinks or end-users of the data). The broker listens to port 9092
    for internal communication within the cluster and port 9094 for external communication,
    allowing clients outside the Docker network to connect to the Kafka broker.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们仅配置了一个 **broker** 服务，这对于我们的小项目来说足够了。Kafka broker 负责接收来自生产者（数据源）的消息，存储这些消息，并将它们传递给消费者（数据的接收端或最终用户）。broker
    会监听端口 9092 用于集群内的通信，并监听端口 9094 用于外部通信，允许集群外的客户端连接到 Kafka broker。
- en: In the **volumes** part, we map the local directory `kafka` to the docker container
    directory `/*bitnami/kafka*`to ensure data persistence and a possible inspection
    of Kafka’s data from the host system.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 **volumes** 部分，我们将本地目录 `kafka` 映射到 docker 容器目录 `/*bitnami/kafka*`，以确保数据的持久性，并且可以从主机系统检查
    Kafka 的数据。
- en: We set-up the service **kafka-ui** that uses the docker image `provectuslabs/kafka-ui:latest`
    . This provides a user interface to interact with the Kafka cluster. This is especially
    useful for monitoring and managing Kafka topics and messages.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们设置了服务 **kafka-ui**，它使用了 docker 镜像 `provectuslabs/kafka-ui:latest`。该工具提供了一个用户界面，用于与
    Kafka 集群进行交互。这对于监控和管理 Kafka 主题和消息尤其有用。
- en: To ensure communication between **kafka** and **airflow** which will be run
    as an external service, we will use an external network **airflow-kafka***.*
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了确保 **kafka** 和作为外部服务运行的 **airflow** 之间的通信，我们将使用一个外部网络 **airflow-kafka**。
- en: 'Before running the kafka service, let’s create the airflow-kafka network using
    the following command:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行 Kafka 服务之前，让我们使用以下命令创建 airflow-kafka 网络：
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now everything is set to finally start our kafka service
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，一切准备就绪，可以最终启动我们的 kafka 服务了。
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After the services start, visit the kafka-ui at [http://localhost:8800/](http://localhost:8000/).
    Normally you should get something like this:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 服务启动后，访问 kafka-ui，网址为 [http://localhost:8800/](http://localhost:8000/)。通常，您应该看到类似以下内容：
- en: '![](../Images/972914181dc72291fbbb4e232bdb72d1.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/972914181dc72291fbbb4e232bdb72d1.png)'
- en: Overview of the Kafka UI. Image by the author.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka UI 概览。图片来源：作者。
- en: Next we will create our topic that will contain the API messages. Click on Topics
    on the left and then Add a topic at the top left. Our topic will be called **rappel_conso**
    and since we have only one broker we set the **replication factor** to **1**.
    We will also set the **partitions** number to **1** since we will have only one
    consumer thread at a time so we won’t need any parallelism. Finally, we can set
    the time to retain data to a small number like one hour since we will run the
    spark job right after the kafka streaming task, so we won’t need to retain the
    data for a long time in the kafka topic.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个主题，用来存储 API 消息。在左侧点击 Topics，然后在左上角点击 Add a topic。我们的主题将命名为 **rappel_conso**，由于我们只有一个
    broker，因此将 **replication factor** 设置为 **1**。我们还将 **partitions** 数量设置为 **1**，因为我们每次只有一个消费者线程，所以不需要并行处理。最后，我们可以将数据保留时间设置为一个较小的值，比如一小时，因为我们会在
    kafka 流处理任务之后立即运行 spark 任务，因此不需要在 Kafka 主题中长期保留数据。
- en: Postgres set-up
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Postgres 设置
- en: Before setting-up our spark and airflow configurations, let’s create the Postgres
    database that will persist our API data. I used the **pgadmin 4** tool for this
    task, however any other Postgres development platform can do the job.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置 spark 和 airflow 配置之前，让我们创建 Postgres 数据库，以便持久化我们的 API 数据。我使用了 **pgadmin 4**
    工具来完成此任务，不过任何其他 Postgres 开发平台也能完成这项工作。
- en: To install postgres and pgadmin, visit this link [https://www.postgresql.org/download/](https://www.postgresql.org/download/)
    and get the packages following your operating system. Then when installing postgres,
    you need to setup a password that we will need later to connect to the database
    from the spark environment. You can also leave the port at 5432.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装 postgres 和 pgadmin，请访问此链接 [https://www.postgresql.org/download/](https://www.postgresql.org/download/)，根据您的操作系统下载相应的安装包。然后，在安装
    postgres 时，您需要设置一个密码，稍后我们将使用该密码从 spark 环境连接到数据库。您也可以将端口保留为 5432。
- en: 'If your installation has succeeded, you can start pgadmin and you should observe
    something like this window:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的安装成功，您可以启动 pgadmin，并且应该看到类似下面的窗口：
- en: '![](../Images/e80944d60d82b2e7a4b068ab4281ebb1.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e80944d60d82b2e7a4b068ab4281ebb1.png)'
- en: Overview of pgAdmin interface. Image by the author.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: pgAdmin 界面的概述。图片由作者提供。
- en: Since we have a lot of columns for the table we want to create, we chose to
    create the table and add its columns with a script using **psycopg2,** a PostgreSQL
    database adapter for Python.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们要创建的表格有很多列，所以我们选择使用**psycopg2**（一个适用于 Python 的 PostgreSQL 数据库适配器）编写脚本来创建表格并添加其列。
- en: 'You can run the script with the command:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下命令运行脚本：
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note that in the script I saved the postgres password as environment variable
    and name it *POSTGRES_PASSWORD.* So if you use another method to access the password
    you need to modify the script accordingly.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在脚本中，我将 PostgreSQL 密码保存为环境变量并命名为 *POSTGRES_PASSWORD*。因此，如果你使用其他方法访问密码，你需要相应地修改脚本。
- en: Spark Set-up
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 配置
- en: Having set-up our Postgres database, let’s delve into the details of the spark
    job. The goal is to stream the data from the Kafka topic *rappel_conso* to the
    Postgres table *rappel_conso_table.*
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置好我们的 PostgreSQL 数据库后，让我们深入了解 Spark 作业的细节。目标是将来自 Kafka 主题 *rappel_conso* 的数据流式传输到
    PostgreSQL 表 *rappel_conso_table*。
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s break down the key highlights and functionalities of the spark job:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解一下 Spark 作业的主要亮点和功能：
- en: First we create the Spark session
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们创建 Spark 会话
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 2\. The `create_initial_dataframe` function ingests streaming data from the
    Kafka topic using Spark's structured streaming.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. `create_initial_dataframe` 函数使用 Spark 的结构化流式处理从 Kafka 主题中摄取流式数据。
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 3\. Once the data is ingested, `create_final_dataframe`transforms it. It applies
    a schema (defined by the columns **DB_FIELDS**) to the incoming JSON data, ensuring
    that the data is structured and ready for further processing.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 一旦数据被摄取，`create_final_dataframe` 将其转换。它应用一个架构（由列 **DB_FIELDS** 定义）到传入的 JSON
    数据，确保数据结构化并准备好进行进一步处理。
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 4\. The `start_streaming`function reads existing data from the database, compares
    it with the incoming stream, and appends new records.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. `start_streaming` 函数读取数据库中的现有数据，将其与传入的数据流进行比较，并追加新记录。
- en: '[PRE12]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The complete code for the Spark job is in the file `src/spark_pgsql/spark_streaming.py`.
    We will use the Airflow DockerOperator to run this job, as explained in the upcoming
    section.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 作业的完整代码位于文件 `src/spark_pgsql/spark_streaming.py` 中。我们将使用 Airflow 的 DockerOperator
    来运行这个作业，具体操作将在接下来的部分中解释。
- en: 'Let’s go through the process of creating the Docker image we need to run our
    Spark job. Here’s the Dockerfile for reference:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来详细了解一下创建我们需要的 Docker 镜像的过程，以便运行我们的 Spark 作业。以下是参考的 Dockerfile：
- en: '[PRE13]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In this Dockerfile, we start with the `bitnami/spark` image as our base. It's
    a ready-to-use Spark image. We then install `py4j`, a tool needed for Spark to
    work with Python.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个 Dockerfile 中，我们以 `bitnami/spark` 镜像作为基础镜像。它是一个现成的 Spark 镜像。然后我们安装 `py4j`，这是
    Spark 与 Python 配合使用所需的工具。
- en: The environment variables `POSTGRES_DOCKER_USER` and `POSTGRES_PASSWORD` are
    set up for connecting to a PostgreSQL database. Since our database is on the host
    machine, we use `host.docker.internal` as the user. This allows our Docker container
    to access services on the host, in this case, the PostgreSQL database. The password
    for PostgreSQL is passed as a build argument, so it's not hard-coded into the
    image.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 环境变量 `POSTGRES_DOCKER_USER` 和 `POSTGRES_PASSWORD` 用于连接 PostgreSQL 数据库。由于我们的数据库在主机上，我们使用
    `host.docker.internal` 作为用户。这样可以让我们的 Docker 容器访问主机上的服务，这里是 PostgreSQL 数据库。PostgreSQL
    密码作为构建参数传递，因此它不会被硬编码到镜像中。
- en: It’s important to note that this approach, especially passing the database password
    at build time, might not be secure for production environments. It could potentially
    expose sensitive information. In such cases, more secure methods like Docker BuildKit
    should be considered.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，这种方法，特别是在构建时传递数据库密码，可能不适用于生产环境，因为它可能暴露敏感信息。在这种情况下，应考虑使用更安全的方法，例如 Docker
    BuildKit。
- en: 'Now, let’s build the Docker image for Spark:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们构建 Spark 的 Docker 镜像：
- en: '[PRE14]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This command will build the image `rappel-conso/spark:latest`. This image includes
    everything needed to run our Spark job and will be used by Airflow’s DockerOperator
    to execute the job. Remember to replace `$POSTGRES_PASSWORD` with your actual
    PostgreSQL password when running this command.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令将构建镜像`rappel-conso/spark:latest`。该镜像包含运行我们的 Spark 作业所需的一切，并将由 Airflow 的
    DockerOperator 用来执行作业。记得在运行此命令时将 `$POSTGRES_PASSWORD` 替换为你实际的 PostgreSQL 密码。
- en: Airflow
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Airflow
- en: As said earlier, Apache Airflow serves as the orchestration tool in the data
    pipeline. It is responsible for scheduling and managing the workflow of the tasks,
    ensuring they are executed in a specified order and under defined conditions.
    In our system, Airflow is used to automate the data flow from streaming with Kafka
    to processing with Spark.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Apache Airflow 作为数据管道中的协调工具，负责调度和管理任务的工作流，确保它们按照指定的顺序和定义的条件执行。在我们的系统中，Airflow
    用于自动化从 Kafka 流处理到 Spark 处理的数据流动。
- en: Airflow DAG
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Airflow DAG
- en: Let’s take a look at the Directed Acyclic Graph (DAG) that will outline the
    sequence and dependencies of tasks, enabling Airflow to manage their execution.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下有向无环图（DAG），它将概述任务的顺序和依赖关系，使得 Airflow 可以管理这些任务的执行。
- en: '[PRE15]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Here are the key elements from this configuration
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是该配置中的关键元素：
- en: The tasks are set to execute daily.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务设置为每天执行。
- en: The first task is the **Kafka Stream Task.** It is **i**mplemented using the
    **PythonOperator** to run the Kafka streaming function. This task streams data
    from the *RappelConso* API into a Kafka topic, initiating the data processing
    workflow.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个任务是 **Kafka 流处理任务**。它通过 **PythonOperator** 实现，用于运行 Kafka 流处理函数。该任务将数据从 *RappelConso*
    API 流式传输到 Kafka 主题，启动数据处理工作流。
- en: The downstream task is the **Spark Stream Task.** It uses the **DockerOperator**
    for execution. It runs a Docker container with our custom Spark image, tasked
    with processing the data received from Kafka.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下游任务是 **Spark 流处理任务**。它使用 **DockerOperator** 执行。该任务运行一个包含我们自定义 Spark 镜像的 Docker
    容器，负责处理从 Kafka 接收到的数据。
- en: The tasks are arranged sequentially, where the Kafka streaming task precedes
    the Spark processing task. This order is crucial to ensure that data is first
    streamed and loaded into Kafka before being processed by Spark.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务按顺序排列，其中 Kafka 流任务在 Spark 处理任务之前执行。这个顺序至关重要，确保数据在 Spark 处理之前，首先被流式传输并加载到 Kafka
    中。
- en: About the DockerOperator
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于 DockerOperator
- en: Using docker operator allow us to run docker-containers that correspond to our
    tasks. The main advantage of this approach is easier package management, better
    isolation and enhanced testability. We will demonstrate the use of this operator
    with the spark streaming task.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 DockerOperator 使我们能够运行与任务对应的 Docker 容器。这种方法的主要优势是更易于管理包、更好的隔离性和更强的可测试性。我们将通过
    Spark 流处理任务演示如何使用这个操作符。
- en: 'Here are some key details about the docker operator for the spark streaming
    task:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一些关于用于 Spark 流处理任务的 Docker Operator 的关键细节：
- en: We will use the image `rappel-conso/spark:latest` specified in the *Spark Set-up*
    section.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将使用在*Spark 设置*部分指定的镜像`rappel-conso/spark:latest`。
- en: The command will run the Spark submit command inside the container, specifying
    the master as local, including necessary packages for PostgreSQL and Kafka integration,
    and pointing to the `spark_streaming.py` script that contains the logic for the
    Spark job.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该命令将在容器内运行 Spark 提交命令，指定 master 为本地模式，包含用于 PostgreSQL 和 Kafka 集成的必要包，并指向包含 Spark
    作业逻辑的 `spark_streaming.py` 脚本。
- en: '**docker_url** represents the url of the host running the docker daemon. The
    natural solution is to set it as `unix://var/run/docker.sock`and to mount the
    `var/run/docker.sock` in the airflow docker container. One problem we had with
    this approach is a permission error to use the socket file inside the airflow
    container. A common workaround, changing permissions with `chmod 777 var/run/docker.sock`,
    poses significant security risks. To circumvent this, we implemented a more secure
    solution using `bobrik/socat` as a docker-proxy. This proxy, defined in a Docker
    Compose service, listens on TCP port 2375 and forwards requests to the Docker
    socket:'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**docker_url** 表示运行 Docker 守护进程的主机的 URL。理想的解决方案是将其设置为 `unix://var/run/docker.sock`
    并将 `var/run/docker.sock` 挂载到 Airflow Docker 容器中。我们在这种方法中遇到的一个问题是，在 Airflow 容器内部使用该套接字文件时出现权限错误。常见的解决方法是通过
    `chmod 777 var/run/docker.sock` 更改权限，但这会带来显著的安全风险。为了解决这个问题，我们实现了一种更安全的解决方案，使用
    `bobrik/socat` 作为 Docker 代理。这个代理在 Docker Compose 服务中定义，监听 TCP 端口 2375，并将请求转发到
    Docker 套接字：'
- en: '[PRE16]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In the DockerOperator, we can access the host docker `/var/run/docker.sock`
    via the`tcp://docker-proxy:2375` url, as described [here](https://medium.com/@benjcabalonajr_56579/using-docker-operator-on-airflow-running-inside-a-docker-container-7df5286daaa5)
    and [here](https://stackoverflow.com/a/70100729).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DockerOperator 中，我们可以通过 `tcp://docker-proxy:2375` URL 访问主机 Docker 的 `/var/run/docker.sock`，如
    [这里](https://medium.com/@benjcabalonajr_56579/using-docker-operator-on-airflow-running-inside-a-docker-container-7df5286daaa5)
    和 [这里](https://stackoverflow.com/a/70100729) 所述。
- en: Finally we set the network mode to **airflow-kafka.** This allows us to use
    the same network as the proxy and the docker running kafka. This is crucial since
    the spark job will consume the data from the kafka topic so we must ensure that
    both containers are able to communicate.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们将网络模式设置为 **airflow-kafka**。这样可以让我们使用与代理和运行 Kafka 的 Docker 相同的网络。这一点至关重要，因为
    Spark 任务将消费来自 Kafka 主题的数据，所以我们必须确保这两个容器能够互相通信。
- en: After defining the logic of our DAG, let’s understand now the airflow services
    configuration in the `docker-compose-airflow.yaml` file.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义了 DAG 的逻辑之后，现在让我们来理解一下 `docker-compose-airflow.yaml` 文件中的 airflow 服务配置。
- en: Airflow Configuration
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Airflow 配置
- en: The compose file for airflow was adapted from the official apache airflow docker-compose
    file. You can have a look at the original file by visiting this [link](https://airflow.apache.org/docs/apache-airflow/2.7.3/docker-compose.yaml).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: airflow 的 compose 文件是从官方的 apache airflow docker-compose 文件修改而来的。你可以通过访问这个 [链接](https://airflow.apache.org/docs/apache-airflow/2.7.3/docker-compose.yaml)
    来查看原始文件。
- en: As pointed out by this [article](https://datatalks.club/blog/how-to-setup-lightweight-local-version-for-airflow.html),
    this proposed version of airflow is highly resource-intensive mainly because the
    core-executor is set to **CeleryExecutor** that is more adapted for distributed
    and large-scale data processing tasks. Since we have a small workload, using a
    single-noded **LocalExecutor** is enough.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 正如这篇 [文章](https://datatalks.club/blog/how-to-setup-lightweight-local-version-for-airflow.html)
    所指出的，提议的这个版本的 airflow 资源消耗较大，主要是因为核心执行器被设置为 **CeleryExecutor**，它更适合分布式和大规模数据处理任务。由于我们的工作负载较小，使用单节点的
    **LocalExecutor** 就足够了。
- en: 'Here is an overview of the changes we made on the docker-compose configuration
    of airflow:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们在 airflow 的 docker-compose 配置中所做更改的概览：
- en: We set the environment variable AIRFLOW__CORE__EXECUTOR to **LocalExecutor**.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将环境变量 AIRFLOW__CORE__EXECUTOR 设置为 **LocalExecutor**。
- en: We removed the services **airflow-worker** and **flower** because they only
    work for the Celery executor. We also removed the **redis** caching service since
    it works as a backend for celery. We also won’t use the **airflow-triggerer**
    so we remove it too.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们移除了 **airflow-worker** 和 **flower** 服务，因为它们仅适用于 Celery 执行器。我们还移除了 **redis**
    缓存服务，因为它作为 Celery 的后端服务运行。由于我们不打算使用 **airflow-triggerer**，所以也将其移除。
- en: We replaced the base image `${AIRFLOW_IMAGE_NAME:-apache/airflow:2.7.3}` for
    the remaining services, mainly the **scheduler** and the **webserver**, by a custom
    image that we will build when running the docker-compose.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将其余服务（主要是 **scheduler** 和 **webserver**）的基础镜像 `${AIRFLOW_IMAGE_NAME:-apache/airflow:2.7.3}`
    替换为一个自定义镜像，该镜像将在运行 docker-compose 时构建。
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We mounted the necessary volumes that are needed by airflow. AIRFLOW_PROJ_DIR
    designates the airflow project directory that we will define later. We also set
    the network as **airflow-kafka** to be able to communicate with the kafka boostrap
    servers.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们挂载了 airflow 所需的必要卷。AIRFLOW_PROJ_DIR 指定了稍后定义的 airflow 项目目录。我们还将网络设置为 **airflow-kafka**，以便能够与
    Kafka 启动服务器通信。
- en: '[PRE18]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, we need to create some environment variables that will be used by docker-compose:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要创建一些环境变量，供 docker-compose 使用：
- en: '[PRE19]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Where `AIRFLOW_UID` represents the User ID in Airflow containers and `AIRFLOW_PROJ_DIR`
    represents the airflow project directory.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`AIRFLOW_UID` 代表 Airflow 容器中的用户 ID，`AIRFLOW_PROJ_DIR` 代表 airflow 项目目录。'
- en: 'Now everything is set-up to run your airflow service. You can start it with
    this command:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在一切已经设置好，可以运行你的 airflow 服务了。你可以使用以下命令启动它：
- en: '[PRE20]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Then to access the airflow user interface you can visit this url `http://localhost:8080`
    .
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，若要访问 airflow 用户界面，你可以访问这个网址 `http://localhost:8080`。
- en: '![](../Images/6580d6371fbac338bc4f5ecc512f9269.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6580d6371fbac338bc4f5ecc512f9269.png)'
- en: Sign-in window on Airflow. Image by the author.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow 登录窗口。图片来自作者。
- en: By default, the username and password are **airflow** for both. After signing
    in, you will see a list of Dags that come with airflow. Look for the dag of our
    project **kafka_spark_dag** and click on it.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，用户名和密码都是 **airflow**。登录后，你将看到 airflow 自带的 Dags 列表。找到我们项目的 dag **kafka_spark_dag**
    并点击它。
- en: '![](../Images/cdec2476e37b4256db6b0be23b8286af.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cdec2476e37b4256db6b0be23b8286af.png)'
- en: Overview of the task window in airflow. Image by the author.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: airflow 中任务窗口的概览。图片来自作者。
- en: 'You can start the task by clicking on the button next to **DAG: kafka_spark_dag.**'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '你可以通过点击 **DAG: kafka_spark_dag** 旁边的按钮来启动任务。'
- en: 'Next, you can check the status of your tasks in the Graph tab. A task is done
    when it turns green. So, when everything is finished, it should look something
    like this:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你可以在图表标签中查看任务的状态。当任务变为绿色时，表示完成。因此，当一切完成时，应该像这样：
- en: '![](../Images/b068e823ce2776a714df06f36dcbecfb.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b068e823ce2776a714df06f36dcbecfb.png)'
- en: Image by the author.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片。
- en: 'To verify that the `rappel_conso_table` is filled with data, use the following
    SQL query in the pgAdmin Query Tool:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 要验证`rappel_conso_table`是否已填充数据，请在pgAdmin查询工具中使用以下SQL查询：
- en: '[PRE21]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: When I ran this in January 2024, the query returned a total of 10022 rows. Your
    results should be around this number as well.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当我在2024年1月运行时，查询返回了总共10022行。你的结果应该也在这个范围内。
- en: Conclusion
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: This article has successfully demonstrated the steps to build a basic yet functional
    data engineering pipeline using Kafka, Airflow, Spark, PostgreSQL, and Docker.
    Aimed primarily at beginners and those new to the field of data engineering, it
    provides a hands-on approach to understanding and implementing key concepts in
    data streaming, processing, and storage.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 本文成功展示了如何使用Kafka、Airflow、Spark、PostgreSQL和Docker构建一个基本而功能齐全的数据工程管道。本文主要面向初学者和数据工程领域的新手，提供了一种动手实践的方式来理解和实现数据流、处理和存储中的关键概念。
- en: Throughout this guide, we’ve covered each component of the pipeline in detail,
    from setting up Kafka for data streaming to using Airflow for task orchestration,
    and from processing data with Spark to storing it in PostgreSQL. The use of Docker
    throughout the project simplifies the setup and ensures consistency across different
    environments.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在本指南中，我们详细介绍了管道的每个组件，从设置Kafka进行数据流处理，到使用Airflow进行任务编排，再到使用Spark处理数据并将其存储在PostgreSQL中。整个项目中使用Docker简化了设置，并确保不同环境之间的一致性。
- en: It’s important to note that while this setup is ideal for learning and small-scale
    projects, scaling it for production use would require additional considerations,
    especially in terms of security and performance optimization. Future enhancements
    could include integrating more advanced data processing techniques, exploring
    real-time analytics, or even expanding the pipeline to incorporate more complex
    data sources.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，虽然这个设置非常适合学习和小规模项目，但要将其扩展到生产环境，仍然需要考虑其他因素，尤其是在安全性和性能优化方面。未来的改进可能包括整合更先进的数据处理技术、探索实时分析，甚至扩展管道以涵盖更多复杂的数据源。
- en: In essence, this project serves as a practical starting point for those looking
    to get their hands dirty with data engineering. It lays the groundwork for understanding
    the basics, providing a solid foundation for further exploration in the field.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，这个项目作为一个实践起点，旨在帮助那些想要深入数据工程的人。它为理解基础知识打下了基础，为进一步探索该领域提供了坚实的基础。
- en: In the second part, we’ll explore how to effectively use the data stored in
    our PostgreSQL database. We’ll introduce agents powered by Large Language Models
    (LLMs) and a variety of tools that enable us to interact with the database using
    natural language queries. So, stay tuned !
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二部分，我们将探讨如何有效地使用存储在PostgreSQL数据库中的数据。我们将介绍由大型语言模型（LLMs）驱动的代理以及各种工具，帮助我们使用自然语言查询与数据库进行交互。所以，敬请期待！
- en: To reach out
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标是
- en: 'LinkedIn : [https://www.linkedin.com/in/hamza-gharbi-043045151/](https://www.linkedin.com/in/hamza-gharbi-043045151/)'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LinkedIn: [https://www.linkedin.com/in/hamza-gharbi-043045151/](https://www.linkedin.com/in/hamza-gharbi-043045151/)'
- en: 'Twitter : [https://twitter.com/HamzaGh25079790](https://twitter.com/HamzaGh25079790)'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Twitter: [https://twitter.com/HamzaGh25079790](https://twitter.com/HamzaGh25079790)'
