- en: 'Learning Discrete Data with Harmoniums: Part I, The Essentials'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用和谐网络学习离散数据：第一部分，基础知识
- en: 原文：[https://towardsdatascience.com/learning-discrete-data-with-harmoniums-part-i-the-essentials-be54e0e293b4?source=collection_archive---------13-----------------------#2024-01-05](https://towardsdatascience.com/learning-discrete-data-with-harmoniums-part-i-the-essentials-be54e0e293b4?source=collection_archive---------13-----------------------#2024-01-05)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/learning-discrete-data-with-harmoniums-part-i-the-essentials-be54e0e293b4?source=collection_archive---------13-----------------------#2024-01-05](https://towardsdatascience.com/learning-discrete-data-with-harmoniums-part-i-the-essentials-be54e0e293b4?source=collection_archive---------13-----------------------#2024-01-05)
- en: 'From the Archives: Generative AI in the ‘00s'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 来自档案：00年代的生成式AI
- en: '[](https://medium.com/@hylke.donker?source=post_page---byline--be54e0e293b4--------------------------------)[![Hylke
    C. Donker](../Images/bed587d1bb305ded80f7ce21bc4f4856.png)](https://medium.com/@hylke.donker?source=post_page---byline--be54e0e293b4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--be54e0e293b4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--be54e0e293b4--------------------------------)
    [Hylke C. Donker](https://medium.com/@hylke.donker?source=post_page---byline--be54e0e293b4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@hylke.donker?source=post_page---byline--be54e0e293b4--------------------------------)[![Hylke
    C. Donker](../Images/bed587d1bb305ded80f7ce21bc4f4856.png)](https://medium.com/@hylke.donker?source=post_page---byline--be54e0e293b4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--be54e0e293b4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--be54e0e293b4--------------------------------)
    [Hylke C. Donker](https://medium.com/@hylke.donker?source=post_page---byline--be54e0e293b4--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--be54e0e293b4--------------------------------)
    ·7 min read·Jan 5, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--be54e0e293b4--------------------------------)
    ·阅读时间：7分钟·2024年1月5日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'I want to take you back to the last generative AI episode, in the early ’00s.
    During this time, Geoff Hinton, one of the founding fathers of deep learning,
    published an influential paper detailing the contrastive divergence algorithm
    [1]. This discovery allowed Smolensky’s harmonium [2] — which Hinton called the
    restricted Boltzmann machine — to be trained efficiently. It was soon realised
    that this model could be used for all sorts of purposes: initialising a feed-forward
    neural net [3], used as part of a deep belief net [4], etc. For at least a decade,
    the harmonium remained one of the pillars in AI until we discovered better optimisers
    for training feed-forward networks. Although the harmonium has now gone out of
    fashion, the model remains useful for modelling discrete data.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我想带你回到上一个生成式AI的篇章，回到00年代初期。在这段时间里，深度学习的奠基人之一Geoff Hinton发表了一篇具有影响力的论文，详细介绍了对比散度算法[1]。这一发现使得Smolensky的和谐网络[2]——Hinton称之为限制玻尔兹曼机——能够高效地进行训练。很快人们意识到，这一模型可以用于各种用途：初始化前馈神经网络[3]，作为深度信念网络的一部分[4]，等等。至少在十年的时间里，和谐网络一直是AI的支柱之一，直到我们发现了更好的优化器来训练前馈网络。尽管和谐网络现在已经不再流行，但它仍然在建模离散数据方面保持着重要的价值。
- en: 'In this first article of a two-part series, we’ll focus on the essentials:
    what harmoniums are, when they are useful, and how to get started with *scikit-learn*.
    In a follow-up, we’ll take a closer look at the technicalities.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本系列的第一篇文章中，我们将专注于基本内容：什么是和谐网络，它们何时有用，以及如何开始使用*scikit-learn*。在后续的文章中，我们将更详细地探讨技术细节。
- en: What are Harmoniums?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是和谐网络？
- en: '![](../Images/0ca9ec5daae7f78bca9886032b72a05f.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ca9ec5daae7f78bca9886032b72a05f.png)'
- en: 'Fig. 1: **Graphical representation of a harmonium.** Receptive fields are edges
    connecting the visible units, ***x***, with the hidden units, **h**, so as to
    form a bipartite network. Image by Author.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：**和谐网络的图形表示。** 感受野是连接可见单元***x***与隐藏单元**h**的边，形成一个二分图网络。图像由作者提供。
- en: 'The vanilla harmonium — or, restricted Boltzmann machine — is a neural network
    operating on binary data [2]. These networks are composed of two types of variables:
    the input, ***x,*** and the hidden states, ***h*** (Fig. 1). The input consists
    of zeroes and ones, *xᵢ* ∈ {0, 1}, and together we call these observed values—***x***
    — the *visible states* or *units* of the network. Conversely, the hidden units
    ***h*** are latent, not directly observed; they are internal to the network. Like
    the visible units, the hidden units ***h*** are either zero or one, *hᵢ* ∈ {0,
    1}.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '经典的哈莫纽姆——或限制玻尔兹曼机——是一个处理二进制数据的神经网络[2]。这些网络由两种类型的变量组成：输入 ***x*** 和隐藏状态 ***h***（见图
    1）。输入由零和一组成，*xᵢ* ∈ {0, 1}，我们称这些观测值——***x***——为网络的*可见状态*或*单元*。相反，隐藏单元 ***h*** 是潜在的，不是直接观察到的；它们位于网络内部。像可见单元一样，隐藏单元
    ***h*** 也是零或一，*hᵢ* ∈ {0, 1}。  '
- en: Standard feed-forward neural networks process data sequentially, by directing
    the layer’s output to the input of the next layer. In harmoniums, this is different.
    Instead, the model is an *un*directed network. The network structure dictates
    how the probability distribution factorises over the graph. In turn, the network
    topology follows from the **energy function** *E*(***x***, ***h***) that quantifies
    the preferences for specific configurations of the visible units ***x*** and the
    hidden units ***h***. Because the harmonium is defined in terms of an energy function,
    we call it an **energy-based model**.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '标准的前馈神经网络通过将一层的输出传递到下一层的输入来顺序处理数据。而在哈莫纽姆中则有所不同。相反，模型是一个*无*向网络。网络结构决定了概率分布如何在图中分解。反过来，网络拓扑结构源自
    **能量函数** *E*(***x***, ***h***)，它量化了可见单元 ***x*** 和隐藏单元 ***h*** 的特定配置的偏好。由于哈莫纽姆是通过能量函数定义的，因此我们称它为
    **基于能量的模型**。  '
- en: The Energy Function
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '能量函数  '
- en: The simplest network directly connects the observations, ***x***, with the hidden
    states, ***h***, through *E*(***x***, ***h***) = ***x***ᵀ***Wh*** where ***W***
    is a receptive field. Favourable configurations of ***x*** and ***h*** have a
    low energy *E*(***x***, ***h***) while unlikely combinations have a high energy.
    In turn, the energy function controls the probability distribution over the visible
    units
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '最简单的网络直接通过 *E*(***x***, ***h***) = ***x***ᵀ***Wh*** 将观测值 ***x*** 与隐藏状态 ***h***
    连接，其中 ***W*** 是感受野。***x*** 和 ***h*** 的有利配置具有低能量 *E*(***x***, ***h***)，而不太可能的组合则具有高能量。反过来，能量函数控制着可见单元的概率分布。  '
- en: '*p*(***x***,***h***) = exp[-*E*(***x***, ***h***)] / *Z,*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*(***x***,***h***) = exp[-*E*(***x***, ***h***)] / *Z,*  '
- en: 'where the factor *Z* is a constant called the partition function. The partition
    function ensures that *p*(***x***,***h***) is normalised (sums to one). Usually,
    we include additional bias terms for the visible states, ***a***, and hidden states,
    ***b*** in the energy function:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '其中因子 *Z* 是一个常数，称为分区函数。分区函数确保 *p*(***x***,***h***) 是归一化的（总和为 1）。通常，我们会在能量函数中加入额外的偏置项，分别是可见状态
    ***a*** 和隐藏状态 ***b***：  '
- en: '*E*(***x***, ***h***) = ***x***ᵀ***a*** + ***x***ᵀ***Wh*** + ***b***ᵀ***h.***'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*E*(***x***, ***h***) = ***x***ᵀ***a*** + ***x***ᵀ***Wh*** + ***b***ᵀ***h.***  '
- en: 'Structurally, *E*(***x***, ***h***) forms a bipartition in ***x*** and ***h***
    (Fig. 1). As a result, we can easily transform observations ***x*** to hidden
    states ***h*** by sampling the distribution:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '在结构上，*E*(***x***, ***h***) 在 ***x*** 和 ***h*** 之间形成了一个二分法（见图 1）。因此，我们可以通过对分布进行采样，轻松地将观测值
    ***x*** 转换为隐藏状态 ***h***：  '
- en: '*p*(*hᵢ*=1|***x***) = *σ*[-(***W***ᵀ***x***+***b***)],'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*(*hᵢ*=1|***x***) = *σ*[-(***W***ᵀ***x***+***b***)],  '
- en: 'where *σ*(*x*) = 1/[1 + exp(-*x*)] is the sigmoid activation function. As you
    see, the probability distribution for ***h*** | ***x*** is structurally akin to
    a one-layer feed-forward neural network. A similar relation holds for the visible
    states given the latent observation: *p*(*xᵢ*=1|***h***) = *σ*[-(***Wh***+***a***)].'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 *σ*(*x*) = 1/[1 + exp(-*x*)] 是 sigmoid 激活函数。如你所见，***h*** | ***x*** 的概率分布在结构上类似于一个单层前馈神经网络。对于给定潜在观测的可见状态，类似的关系也成立：*p*(*xᵢ*=1|***h***)
    = *σ*[-(***Wh***+***a***)]。  '
- en: This identity can be used to **impute** (generate new) input variables based
    on the latent state ***h***. The trick is to Gibbs sample by alternating between
    *p*(***x***|***h***) and *p*(***h***|***x***). More on that in the second part
    of this series.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '这个恒等式可以用来基于潜在状态 ***h*** **推断**（生成新的）输入变量。诀窍是通过在 *p*(***x***|***h***) 和 *p*(***h***|***x***)
    之间交替进行 Gibbs 采样。更多内容将在本系列的第二部分中讨论。  '
- en: When to use harmoniums
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '何时使用哈莫纽姆  '
- en: 'In practice, consider using harmoniums when:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，当以下情况成立时，可以考虑使用哈莫纽姆：
- en: '**1\. Your data is discrete (binary-valued).**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**1. 你的数据是离散的（具有二进制值）。**  '
- en: 'Harmoniums have a strong theoretical foundation: it turns out that the model
    is powerful enough to describe *any* discrete distribution. That is, harmoniums
    are universal approximators [5]. So in theory, harmoniums are a one-size-fits-all
    when your dataset is discrete. In practice, harmoniums also work well on data
    that naturally lies in the unit [0, 1] interval.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 哈莫纽姆有着坚实的理论基础：事实证明，该模型足够强大，能够描述*任何*离散分布。也就是说，哈莫纽姆是通用近似器[5]。因此，理论上，当你的数据集是离散的时，哈莫纽姆是一种通用模型。实际上，哈莫纽姆在自然位于区间[0,
    1]的数据上也表现良好。
- en: '**2\. For representation learning.**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**2. 用于表示学习。**'
- en: The hidden states, ***h***,that are internal to the network can be used in itself.
    For example, ***h*** can be used as a dimension reduction technique to learn a
    compressed representation of ***x***. Think of it as principal components analysis,
    but for discrete data. Another application of the latent representation ***h***
    is for a downstream task by using it as the features for a classifier.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 网络内部的隐藏状态，***h***，可以独立使用。例如，***h***可以作为降维技术来学习***x***的压缩表示。可以将其视为主成分分析，但针对离散数据。潜在表示***h***的另一个应用是将其作为分类器的特征，用于下游任务。
- en: '**3\. To elicit latent structure in your variables.**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**3. 以引出变量中的潜在结构。**'
- en: Harmoniums are neural networks with receptive fields that describe how an example,
    ***x***, relates to its latent state ***h:*** neurons that wire together, fire
    together. We can use the receptive fields as a read-out to identify input variables
    that naturally go together (cluster). In other words, the model describes different
    modules of associations (or, correlations) between the visible units.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 哈莫纽姆是具有感受野的神经网络，用来描述一个示例***x***与其潜在状态***h***之间的关系：神经元如果连接在一起，就会一起激活。我们可以使用感受野作为读取输出，识别自然配对的输入变量（聚类）。换句话说，模型描述了可见单元之间的不同关联（或相关性）模块。
- en: '**4\. To impute your data.**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**4. 用于数据插补。**'
- en: 'Since harmoniums are generative models, they can be used to complete missing
    data (i.e., imputation) or generate completely new (synthetic) examples. Traditionally,
    they have been used for in-painting: completing part of an image that is masked
    out. Another example is recommender systems: harmoniums featured in the Netflix
    competition to improve movie recommendations for users.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由于哈莫纽姆（harmonium）是生成模型，它们可以用于补全缺失数据（即插补）或生成全新的（合成）示例。传统上，它们被用于图像修复：补全被遮挡的图像部分。另一个例子是推荐系统：哈莫纽姆曾在Netflix比赛中用于改善电影推荐。
- en: Getting started with scikit-learn
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用scikit-learn
- en: Now that you know the essentials, let’s show how to train a model.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了基本要点，接下来让我们展示如何训练一个模型。
- en: As our running example, we’ll use the [UCI MLR handwritten digits database](https://archive.ics.uci.edu/dataset/80/optical+recognition+of+handwritten+digits)
    (CC BY 4.0) that is part of *scikit-learn*. While technically the harmonium requires
    binary data as input, using binary probabilities (instead of samples thereof)
    works fine in practice. We therefore normalise the pixel values to the unit interval
    [0, 1] prior to training.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 作为我们的运行示例，我们将使用[UCI MLR手写数字数据库](https://archive.ics.uci.edu/dataset/80/optical+recognition+of+handwritten+digits)（CC
    BY 4.0），这是*scikit-learn*的一部分。虽然严格来说，哈莫纽姆需要二进制数据作为输入，但实际上使用二进制概率（而不是其样本）效果也很好。因此，在训练之前，我们将像素值归一化到单位区间[0,
    1]。
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Conveniently, *scikit-learn* comes with an off-the-shelf implementation: [BernoulliRBM](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.BernoulliRBM.html).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 方便的是，*scikit-learn*提供了现成的实现：[BernoulliRBM](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.BernoulliRBM.html)。
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Under the hood, the model relies on the persistent contrastive divergence algorithm
    to fit the parameters of the model [6]. (To learn more about the algorithmic details,
    stay tuned.)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型的内部，依赖于持久对比散度算法来拟合模型的参数[6]。（要了解更多算法细节，敬请关注。）
- en: '![](../Images/b69e638d7b20e9283b2599d72118ad54.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b69e638d7b20e9283b2599d72118ad54.png)'
- en: 'Fig. 2: Receptive fields **W** of each harmonium’s hidden unit. Image by Author.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：每个哈莫纽姆隐藏单元的感受野**W**。图片由作者提供。
- en: 'To interpret the associations in the data — which input pixels fire together
    — you can inspect the receptive fields ***W.*** In *scikit-learn*, a NumPy array
    of ***W*** can be accessed by the `BernoulliRBM.components_` attribute after fitting
    the `BernoulliRBM` model (Fig. 2). [Beware: *scikit-learn* uses a different sign
    convention in the energy function: *E*(***x***,***h***) -> -*E*(***x***,***h***).]'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要解释数据中的关联——哪些输入像素一起激活——你可以检查感受野 ***W.*** 在 *scikit-learn* 中，***W*** 的 NumPy
    数组可以通过在拟合 `BernoulliRBM` 模型后访问 `BernoulliRBM.components_` 属性来获取（图 2）。[请注意：*scikit-learn*
    在能量函数中使用了不同的符号约定：*E*(***x***,***h***) -> -*E*(***x***,***h***)。]
- en: For **representation learning**, it is customary to use a deterministic value
    *p*(*hᵢ*=1|***x***) as a representation instead of stochastic sample *hᵢ ~ p*(*hᵢ*|***x***).
    Since *p*(*hᵢ*=1|***x***) equals the expected hidden state <*hᵢ>* given ***x***,
    it is a convenient measure to use during inference where we prefer determinism
    (over randomness). In *scikit-learn*, the latent representation, *p*(*hᵢ*=1|***x***),
    can be directly obtained through
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于**表示学习**，通常使用确定性值 *p*(*hᵢ*=1|***x***) 作为表示，而不是随机样本 *hᵢ ~ p*(*hᵢ*|***x***)。由于
    *p*(*hᵢ*=1|***x***) 等于给定 ***x*** 的期望隐藏状态 <*hᵢ>*，它在推理过程中是一个方便的度量，因为我们更倾向于确定性（而非随机性）。在
    *scikit-learn* 中，潜在表示 *p*(*hᵢ*=1|***x***) 可以通过以下方式直接获得：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Finally, to demonstrate **imputation** or in-painting, let’s take an image containing
    the digit six and erase 25% of the pixel values.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了演示**填充**或图像修复，我们使用包含数字六的图像并删除 25% 的像素值。
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We will now use the harmonium to impute the erased variables. The trick is
    to do Markov chain Monte Carlo (MCMC): simulate the missing pixel values using
    the pixel values that we do observe. It turns out that Gibbs sampling — a specific
    MCMC approach — is particularly easy in harmoniums.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用和谐模型来填充已删除的变量。诀窍是使用马尔科夫链蒙特卡罗（MCMC）方法：利用我们已观测到的像素值来模拟缺失的像素值。事实证明，吉布斯采样——一种特定的
    MCMC 方法——在和谐模型中尤其容易实现。
- en: '![](../Images/290ea16c6c9431f49e89c7c10e34cf37.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/290ea16c6c9431f49e89c7c10e34cf37.png)'
- en: 'Fig. 3: Pixel values in the red square are missing (left), and imputated with
    a harmonium (middle). For comparison, the original image (UCI MLR handwritten
    digits database, CC BY 4.0) is shown on the right. Image by Author.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：红色方框中的像素值丢失（左），并通过和谐模型进行填充（中）。为了对比，右侧显示了原始图像（UCI MLR 手写数字数据库，CC BY 4.0）。图片由作者提供。
- en: 'Here is how yo do it: first, initialise multiple Markov chains (e.g., 100)
    using the sample you want to impute. Then, Gibbs sample the chain for several
    iterations (e.g., 1000) while clamping the observed values. Finally, aggregate
    the samples from the chains to obtain a distribution over the missing values.
    In code, this looks as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是操作步骤：首先，使用你想要填充的样本初始化多个马尔科夫链（例如，100条链）。然后，使用吉布斯采样对链进行多次迭代（例如，1000次），同时固定观测到的值。最后，从链中聚合样本，以获得缺失值的分布。代码实现如下：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The result is shown in Fig. 3\. As you can see, the harmonium does a pretty
    decent job reconstructing the original image.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如图 3 所示。正如你所见，和谐模型在重建原始图像方面表现相当不错。
- en: Conclusion
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'Generative AI is not new, it goes back a long way. We’ve looked at harmoniums,
    an energy-based unsupervised neural network model that was popular two decades
    ago. While no longer at the centre of attention, harmoniums remain useful today
    for a specific niche: learning from discrete data. Because it is a generative
    model, harmoniums can be used to impute (or, complete) variable values or generate
    completely new examples.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 生成性 AI 并不是什么新鲜事，它已经有很长历史了。我们已经看过了和谐模型，它是一种基于能量的无监督神经网络模型，二十年前曾非常流行。虽然如今它不再是关注的中心，但和谐模型在今天仍然在特定的细分领域中非常有用：从离散数据中学习。由于它是生成模型，和谐模型可以用来填充（或补全）变量值，或者生成全新的示例。
- en: In this first article of a two-part harmonium series, we’ve looked at the essentials.
    Just enough to get you started. Stay tuned for part two, where we’ll take a closer
    look at the technicalities behind training these models.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇两部分系列文章的第一篇中，我们已经了解了基本要点。足够让你入门。请继续关注第二部分，我们将更深入地探讨训练这些模型的技术细节。
- en: Acknowledgements
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: I would like to thank [Rik Huijzer](https://huijzer.xyz/) and Dina Boer for
    proofreading.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我要感谢[Rik Huijzer](https://huijzer.xyz/)和 Dina Boer 的校对。
- en: References
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Hinton “[Training products of experts by minimizing contrastive divergence.](https://www.cs.utoronto.ca/~hinton/absps/nccd.pdf)”
    *Neural computation* 14.8, 1771–1800 (2002).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Hinton “[通过最小化对比散度训练专家产品](https://www.cs.utoronto.ca/~hinton/absps/nccd.pdf)”
    *神经计算* 14.8, 1771–1800 (2002)。'
- en: '[2] Smolensky “[Information processing in dynamical systems: Foundations of
    harmony theory.](https://apps.dtic.mil/sti/pdfs/ADA620727.pdf)” 194–281 (1986).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Smolensky “[信息处理在动态系统中的应用：和谐理论的基础](https://apps.dtic.mil/sti/pdfs/ADA620727.pdf)”
    194–281 (1986).'
- en: '[3] Hinton-Salakhutdinov, “[Reducing the dimensionality of data with neural
    networks.](https://doi.org/10.1126/science.1127647)” *Science* 313.5786, 504–507
    (2006).'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Hinton-Salakhutdinov, “[使用神经网络减少数据的维度](https://doi.org/10.1126/science.1127647)”
    *科学* 313.5786, 504–507 (2006).'
- en: '[4] Hinton-Osindero-Teh. “[A fast learning algorithm for deep belief nets.](https://doi.org/10.1162/neco.2006.18.7.1527)”
    *Neural computation* 18.7, 1527–1554 (2006).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Hinton-Osindero-Teh. “[深度信念网络的快速学习算法](https://doi.org/10.1162/neco.2006.18.7.1527)”
    *神经计算* 18.7, 1527–1554 (2006).'
- en: '[5] Le Roux-Bengio, “[Representational power of restricted Boltzmann machines
    and deep belief networks.](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/representational_power.pdf)”
    Neural computation 20.6, 1631–1649 (2008).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Le Roux-Bengio, “[限制玻尔兹曼机和深度信念网络的表示能力](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/representational_power.pdf)”
    神经计算 20.6, 1631–1649 (2008).'
- en: '[6] Tieleman, “[Training restricted Boltzmann machines using approximations
    to the likelihood gradient.](https://dl.acm.org/doi/pdf/10.1145/1390156.1390290?casa_token=KA8SOPhKmvIAAAAA%3AulezajFrxWkXlhByFI-M_T8BhZBe7snX8eaFql0D0IMDw0igH710rVMYtCmK-r4Vz2VcjMPXGysT)”
    *Proceedings of the 25th international conference on Machine learning*. 2008.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Tieleman, “[使用似然梯度的近似训练限制玻尔兹曼机](https://dl.acm.org/doi/pdf/10.1145/1390156.1390290?casa_token=KA8SOPhKmvIAAAAA%3AulezajFrxWkXlhByFI-M_T8BhZBe7snX8eaFql0D0IMDw0igH710rVMYtCmK-r4Vz2VcjMPXGysT)”
    *第25届国际机器学习大会论文集*。2008年。'
