- en: Merge Large Language Models with mergekit
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用mergekit合并大语言模型
- en: 原文：[https://towardsdatascience.com/merge-large-language-models-with-mergekit-2118fb392b54?source=collection_archive---------0-----------------------#2024-01-08](https://towardsdatascience.com/merge-large-language-models-with-mergekit-2118fb392b54?source=collection_archive---------0-----------------------#2024-01-08)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/merge-large-language-models-with-mergekit-2118fb392b54?source=collection_archive---------0-----------------------#2024-01-08](https://towardsdatascience.com/merge-large-language-models-with-mergekit-2118fb392b54?source=collection_archive---------0-----------------------#2024-01-08)
- en: Create your own models easily, no GPU required!
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 轻松创建你自己的模型，无需GPU！
- en: '[](https://medium.com/@mlabonne?source=post_page---byline--2118fb392b54--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page---byline--2118fb392b54--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2118fb392b54--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2118fb392b54--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page---byline--2118fb392b54--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne?source=post_page---byline--2118fb392b54--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page---byline--2118fb392b54--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2118fb392b54--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2118fb392b54--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page---byline--2118fb392b54--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2118fb392b54--------------------------------)
    ·11 min read·Jan 8, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2118fb392b54--------------------------------)
    ·阅读时间：11分钟·2024年1月8日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/3250f7b46dc7b58c13e186d2b0230d38.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3250f7b46dc7b58c13e186d2b0230d38.png)'
- en: Image by author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Model merging is a technique that **combines two or more LLMs** into a single
    model. It’s a relatively new and experimental method to create new models for
    cheap (no GPU required). Model merging works surprisingly well and produced many
    state-of-the-art models on the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 模型合并是一种**将两个或更多LLM模型**合并为单个模型的技术。这是一种相对较新且实验性的方法，用于以低成本（无需GPU）创建新模型。模型合并效果出乎意料地好，并且产生了许多在[Open
    LLM排行榜](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)上排名靠前的顶尖模型。
- en: In this tutorial, we will implement it using the [mergekit](https://github.com/cg123/mergekit)
    library. More specifically, we will review four merge methods and provide examples
    of configurations. Then, we will use mergekit to create our own model, [Marcoro14–7B-slerp](https://huggingface.co/mlabonne/Marcoro14-7B-slerp),
    which became the best-performing model on the Open LLM Leaderboard (02/01/24).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将使用[mergekit](https://github.com/cg123/mergekit)库来实现。更具体地，我们将回顾四种合并方法，并提供配置示例。然后，我们将使用mergekit创建我们自己的模型，[Marcoro14–7B-slerp](https://huggingface.co/mlabonne/Marcoro14-7B-slerp)，该模型在Open
    LLM排行榜（2024年1月2日）上表现最佳。
- en: 'The code is available on [GitHub](https://github.com/mlabonne/llm-course/blob/main/Mergekit.ipynb)
    and [Google Colab](https://colab.research.google.com/drive/1_JS7JKJAQozD48-LhYdegcuuZ2ddgXfr?usp=sharing).
    I recommend using my automated notebook to easily run mergekit: [🥱 LazyMergekit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 代码可在[GitHub](https://github.com/mlabonne/llm-course/blob/main/Mergekit.ipynb)和[Google
    Colab](https://colab.research.google.com/drive/1_JS7JKJAQozD48-LhYdegcuuZ2ddgXfr?usp=sharing)上找到。我推荐使用我的自动化笔记本来轻松运行mergekit：[🥱
    LazyMergekit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing)。
- en: '*A special thanks to* [*Charles Goddard*](https://www.linkedin.com/in/charles-goddard-7b6797b/)*,
    the author of the mergekit library, for reviewing this article.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*特别感谢* [*Charles Goddard*](https://www.linkedin.com/in/charles-goddard-7b6797b/)*，mergekit库的作者，感谢他审阅本文。*'
- en: '![](../Images/c43a26493b1d996b35c6c341845a3a97.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c43a26493b1d996b35c6c341845a3a97.png)'
- en: Image by author
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 🤝 Merge algorithms
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🤝 合并算法
- en: In this section, we will focus on four methods currently implemented in [mergekit](https://github.com/cg123/mergekit).
    Note that there are other methods, such as [linear](https://github.com/cg123/mergekit/tree/1011ef3a84e4c5545473602baf7ef32d535044a9#linear)
    and [Task Arithmetic](https://arxiv.org/abs/2212.04089). If you’re interested
    in papers on model merging, I recommend [this excellent collection](https://huggingface.co/collections/osanseviero/model-merging-65097893623330a3a51ead66)
    on Hugging Face.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重点介绍[mergekit](https://github.com/cg123/mergekit)中目前实现的四种方法。请注意，还有其他方法，如[线性插值](https://github.com/cg123/mergekit/tree/1011ef3a84e4c5545473602baf7ef32d535044a9#linear)和[任务算术](https://arxiv.org/abs/2212.04089)。如果你对模型合并的论文感兴趣，推荐查看[这个优秀的合集](https://huggingface.co/collections/osanseviero/model-merging-65097893623330a3a51ead66)。
- en: 1\. SLERP
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. SLERP
- en: '**Spherical Linear Interpolation** (SLERP) is a method used to smoothly interpolate
    between two vectors. It maintains a constant rate of change and preserves the
    geometric properties of the spherical space in which the vectors reside.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**球面线性插值**（SLERP）是一种在两个向量之间平滑插值的方法。它保持恒定的变化速率，并保留向量所处球面空间的几何属性。'
- en: There are several reasons to prefer SLERP over a traditional linear interpolation.
    For example, in high-dimensional spaces, linear interpolation can lead to a **decrease
    in the magnitude** of the interpolated vector (i.e., it reduces the scale of weights).
    Moreover, the change in direction of the weights often represents **more meaningful
    information** (like feature learning and representation) than the magnitude of
    change.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个理由更倾向于使用SLERP而不是传统的线性插值。例如，在高维空间中，线性插值可能会导致插值向量的**大小减小**（即减少权重的尺度）。此外，权重的方向变化通常代表了**更有意义的信息**（如特征学习和表示），而不是变化的大小。
- en: 'SLERP is implemented using the following steps:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: SLERP的实现步骤如下：
- en: Normalize the input vectors to unit length, ensuring they represent directions
    rather than magnitudes
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入向量标准化为单位长度，确保它们表示的是方向而非大小。
- en: Calculate the angle between these vectors using their dot product.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用它们的点积计算这些向量之间的角度。
- en: If the vectors are nearly collinear, it defaults to linear interpolation for
    efficiency. Otherwise, SLERP computing scale factors based on the interpolation
    factor `t` (`t=0` = 100% of the first vector, `t=1` = 100% of model 2) and the
    angle between the vectors.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果向量几乎共线，则默认使用线性插值以提高效率。否则，SLERP会根据插值因子`t`（`t=0` = 100%的第一个向量，`t=1` = 100%的第二个模型）和向量之间的角度计算缩放因子。
- en: These factors are used to weigh the original vectors, which are then summed
    to obtain the interpolated vector.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些因素用于加权原始向量，然后将其求和以得到插值向量。
- en: SLERP is currently the most popular merging method, but it is limited to combining
    only two models at a time. It is still possible to hierarchically combine multiple
    models, as shown in [Mistral-7B-Merge-14-v0.1](https://huggingface.co/EmbeddedLLM/Mistral-7B-Merge-14-v0.1).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: SLERP目前是最流行的合并方法，但它仅限于一次合并两个模型。仍然可以层次化地合并多个模型，如在[Mistral-7B-Merge-14-v0.1](https://huggingface.co/EmbeddedLLM/Mistral-7B-Merge-14-v0.1)中所示。
- en: '*Example of configuration:*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*配置示例：*'
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This is a classic SLERP configuration, applied to every layer of both models.
    Note that we input a gradient of values for the interpolation factor `t`. The
    parameters for the self-attention and MLP layers will use different combinations
    of [OpenPipe/mistral-ft-optimized-1218](https://huggingface.co/OpenPipe/mistral-ft-optimized-1218)
    and [mlabonne/NeuralHermes-2.5-Mistral-7B](https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B).
    The other layers are a 50/50 mixture of the two models.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个经典的SLERP配置，应用于两个模型的每一层。请注意，我们输入一个值梯度作为插值因子`t`。自注意力层和MLP层的参数将使用[OpenPipe/mistral-ft-optimized-1218](https://huggingface.co/OpenPipe/mistral-ft-optimized-1218)和[mlabonne/NeuralHermes-2.5-Mistral-7B](https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B)的不同组合。其他层则是两个模型的50/50混合。
- en: You can find the final model on the Hugging Face Hub at [mlabonne/NeuralPipe-7B-slerp](https://huggingface.co/mlabonne/NeuralPipe-7B-slerp).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在Hugging Face Hub上找到最终模型：[mlabonne/NeuralPipe-7B-slerp](https://huggingface.co/mlabonne/NeuralPipe-7B-slerp)。
- en: 2\. TIES
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. TIES
- en: 'Introduced in [this paper](https://arxiv.org/abs/2306.01708) by Yadav et al.,
    **TIES-Merging** is designed to efficiently merge multiple task-specific models
    into a single multitask model. It addresses two main challenges in model merging:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在[Yadav等人](https://arxiv.org/abs/2306.01708)的[论文](https://arxiv.org/abs/2306.01708)中提出的**TIES-Merging**旨在高效地将多个任务特定的模型合并为一个多任务模型。它解决了模型合并中的两个主要挑战：
- en: '**Redundancy in model parameters**: It identifies and eliminates redundant
    parameters within task-specific models. This is achieved by focusing on the changes
    made during fine-tuning, identifying the top-k% most significant changes, and
    discarding the rest.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型参数中的冗余**：它识别并消除任务特定模型中的冗余参数。通过关注微调过程中所做的变化，识别最重要的前k%变化，并丢弃其余部分。'
- en: '**Disagreement between parameter signs**: Conflicts arise when different models
    suggest opposing adjustments to the same parameter. TIES-Merging resolves these
    conflicts by creating a unified sign vector that represents the most dominant
    direction of change across all models.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数符号之间的分歧**：当不同模型对同一参数提出相反调整时，会产生冲突。TIES-Merging通过创建一个统一的符号向量来解决这些冲突，表示所有模型中最主导的变化方向。'
- en: 'TIES-Merging is divided into the following three steps:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: TIES-Merging分为以下三个步骤：
- en: '**Trim**: Reduces redundancy in task-specific models by retaining only a fraction
    the most significant parameters (density parameter) and resetting the rest to
    zero.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**修剪**：通过仅保留最重要参数（密度参数）的部分，重置其余部分为零，从而减少任务特定模型中的冗余。'
- en: '**Elect Sign**: Resolves sign conflicts across different models by creating
    a unified sign vector based on the most dominant direction (positive or negative)
    in terms of cumulative magnitude.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择符号**：通过基于累积大小的最主导方向（正或负）创建统一符号向量，解决不同模型之间的符号冲突。'
- en: '**Disjoint Merge**: Averages parameter values that align with the unified sign
    vector, excluding zero values.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**不相交合并**：平均与统一符号向量对齐的参数值，排除零值。'
- en: Unlike SLERP, TIES can merge multiple models at a time.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 与SLERP不同，TIES可以一次合并多个模型。
- en: '*Example of configuration:*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*配置示例：*'
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'With this config, we use Mistral-7B as a base model to calculate the delta
    weights. We merge the same two models: [mistral-ft-optimized-1218](https://huggingface.co/OpenPipe/mistral-ft-optimized-1218)
    (50%) and [NeuralHermes-2.5-Mistral-7B](https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B)
    (30%) with normalization. Here, the density means that we’re only retaining 50%
    of the parameters of each model (the other half comes from the base model).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此配置，我们使用Mistral-7B作为基础模型来计算增量权重。我们合并相同的两个模型：[mistral-ft-optimized-1218](https://huggingface.co/OpenPipe/mistral-ft-optimized-1218)（50%）和[NeuralHermes-2.5-Mistral-7B](https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B)（30%），并进行归一化。这里的密度表示我们只保留每个模型50%的参数（另一半来自基础模型）。
- en: 'Note that the sum of the weights is not equal to 1 in the config, but the `normalize:
    true` parameter will automatically normalize them internally. This config is inspired
    by the parameters provided by the author of [OpenHermes-2.5-neural-chat-7b-v3–1–7B](https://huggingface.co/Weyaxi/OpenHermes-2.5-neural-chat-7b-v3-1-7B).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '请注意，配置中的权重总和不等于1，但`normalize: true`参数会自动在内部对其进行归一化。该配置灵感来自于[OpenHermes-2.5-neural-chat-7b-v3–1–7B](https://huggingface.co/Weyaxi/OpenHermes-2.5-neural-chat-7b-v3-1-7B)作者提供的参数。'
- en: You can find the final model on the Hugging Face Hub at [mlabonne/NeuralPipe-7B-ties](https://huggingface.co/mlabonne/NeuralPipe-7B-ties).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在Hugging Face Hub上找到最终模型：[mlabonne/NeuralPipe-7B-ties](https://huggingface.co/mlabonne/NeuralPipe-7B-ties)。
- en: 3\. DARE
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. DARE
- en: 'Introduced by Yu et al. (2023), [DARE](https://arxiv.org/abs/2311.03099) uses
    an approach similar to TIES with two main differences:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由Yu等人（2023年）提出的[DARE](https://arxiv.org/abs/2311.03099)采用了类似于TIES的方法，主要有两个不同之处：
- en: '**Pruning**: DARE randomly reset fine-tuned weights to their original values
    (those of the base model).'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**剪枝**：DARE将微调后的权重随机重置为其原始值（即基础模型的值）。'
- en: '**Rescaling**: DARE rescales the weights to keep the expectations of model
    outputs approximately unchanged. It adds the rescaled weights of both (or more)
    models to the weights of the base model with a scale factor.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重新缩放**：DARE通过重新缩放权重来保持模型输出的期望值大致不变。它将两个（或多个）模型的重新缩放权重与基础模型的权重通过一个缩放因子相加。'
- en: 'Mergekit’s implementation of this method has two flavors: with the sign election
    step of TIES (`dare_ties`) or without (`dare_linear`).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Mergekit对此方法的实现有两种形式：一种是带有TIES符号选择步骤（`dare_ties`），另一种是没有的（`dare_linear`）。
- en: '*Example of configuration:*'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*配置示例：*'
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In this configuration, we merge three different models based on Mistral-7B using
    `dare_ties`. This time, I chose weights that sum to 1 (the sum should be between
    0.9 and 1.1). The density parameter is a little higher than what's recommended
    in the paper (<0.5), but it looks like it gives consistently better results (see
    [this discussion](https://github.com/cg123/mergekit/issues/26)).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在此配置中，我们基于Mistral-7B合并了三种不同的模型，使用了`dare_ties`。这次，我选择了权重之和为1的组合（权重和应该在0.9到1.1之间）。密度参数比论文中推荐的值（<0.5）稍高，但看起来它
    consistently 给出了更好的结果（参见[这个讨论](https://github.com/cg123/mergekit/issues/26)）。
- en: You can find it on the Hugging Face Hub at [mlabonne/Daredevil-7B](https://huggingface.co/mlabonne/Daredevil-7B).
    It’s also the best merge model in this article, outperforming even Marcoro14–7B-slerp.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在Hugging Face Hub上找到它，地址是[mlabonne/Daredevil-7B](https://huggingface.co/mlabonne/Daredevil-7B)。它也是本文中表现最好的合并模型，甚至超过了Marcoro14–7B-slerp。
- en: 4\. Passthrough
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 透传
- en: The passthrough method differs significantly from the previous ones. By concatenating
    layers from different LLMs, it can produce models with an **exotic number of parameters**
    (e.g., 9B with two 7B parameter models). These models are often referred to as
    “frankenmerges” or “Frankenstein models” by the community.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 透传方法与之前的几种方法有显著不同。通过将来自不同大语言模型（LLM）的层级进行拼接，它可以生成具有**独特参数数量**的模型（例如，两个7B参数模型合并生成9B模型）。这些模型通常被社区称为“弗兰肯合并”或“弗兰肯斯坦模型”。
- en: This technique is very experimental, but it managed to create impressive models,
    like [goliath-120b](https://huggingface.co/alpindale/goliath-120b) using two Llama
    2 70B models. The recently released [SOLAR-10.7B-v1.0](https://huggingface.co/upstage/SOLAR-10.7B-v1.0)
    also uses the same idea, called depth-up scaling [in their paper](https://arxiv.org/abs/2312.15166).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这个技术非常实验性，但它成功地创造了令人印象深刻的模型，比如使用两个Llama 2 70B模型合并的[goliath-120b](https://huggingface.co/alpindale/goliath-120b)。最近发布的[SOLAR-10.7B-v1.0](https://huggingface.co/upstage/SOLAR-10.7B-v1.0)也采用了相同的理念，称为深度向上扩展，详见他们的论文[这里](https://arxiv.org/abs/2312.15166)。
- en: '*Example of configuration:*'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*配置示例：*'
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The resulting frankenmerge will have all the 32 layers from the first model
    and 8 additional layers from the second model. This creates a frankenmerge with
    a total of 40 layers and 8.99B parameters. This config is inspired by [GML-Mistral-merged-v1](https://huggingface.co/zyh3826/GML-Mistral-merged-v1).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的弗兰肯合并将拥有来自第一个模型的所有32层，以及来自第二个模型的8层。这创建了一个总共40层和8.99B参数的弗兰肯合并模型。此配置灵感来自于[GML-Mistral-merged-v1](https://huggingface.co/zyh3826/GML-Mistral-merged-v1)。
- en: You can find the final model on the Hugging Face Hub at [mlabonne/NeuralPipe-9B-merged](https://huggingface.co/mlabonne/NeuralPipe-9B-merged).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在Hugging Face Hub上找到最终模型，地址是[mlabonne/NeuralPipe-9B-merged](https://huggingface.co/mlabonne/NeuralPipe-9B-merged)。
- en: 💻 Merge your own models
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 💻 合并你自己的模型
- en: In this section, we will use mergekit to load a merge configuration, run it,
    and upload the resulting model to the Hugging Face Hub.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将使用mergekit加载合并配置，运行它，并将合并后的模型上传到Hugging Face Hub。
- en: 'First of all, we install mergekit directly from source as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们直接从源代码安装mergekit，如下所示：
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the following block, we load the merge configuration in a YAML format. We
    also specify the name of the merged model for future use. You can copy/paste any
    configuration from the previous section here.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的代码块中，我们加载合并配置文件（YAML格式）。我们还指定了合并后模型的名称，以供以后使用。你可以将上一节中的任何配置复制/粘贴到这里。
- en: 'This time, we will use two different models: [Marcoroni-7B-v3](https://huggingface.co/AIDC-ai-business/Marcoroni-7B-v3)
    and [Mistral-7B-Merge-14-v0.1](https://huggingface.co/EmbeddedLLM/Mistral-7B-Merge-14-v0.1)
    and merge them with the SLERP method. We save the config as a yaml file to be
    used as input in the merge command.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们将使用两个不同的模型：[Marcoroni-7B-v3](https://huggingface.co/AIDC-ai-business/Marcoroni-7B-v3)
    和 [Mistral-7B-Merge-14-v0.1](https://huggingface.co/EmbeddedLLM/Mistral-7B-Merge-14-v0.1)，并使用SLERP方法将它们合并。我们将配置保存为yaml文件，以便作为输入使用在合并命令中。
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We run the merge command with the following parameters:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下参数运行合并命令：
- en: '`--copy-tokenizer` to copy the tokenizer from the base model'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--copy-tokenizer` 用于从基础模型复制分词器'
- en: '`--allow-crimes` and `--out-shard-size` to chunk the models into smaller shards
    that can be computed on a CPU with low RAM'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--allow-crimes` 和 `--out-shard-size` 用于将模型分割成较小的分片，以便在低内存的CPU上计算。'
- en: '`--lazy-unpickle` to enable the experimental lazy unpickler for lower memory
    usage'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--lazy-unpickle` 以启用实验性的懒加载解包器，从而减少内存使用。'
- en: In addition, some models can require the `--trust_remote_code` flag (this is
    not the case with Mistral-7B).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些模型可能需要`--trust_remote_code`标志（Mistral-7B不需要此标志）。
- en: This command will download the weights of all the models listed in the merge
    configuration and run the selected merge method (it should take ~10 minutes).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令将下载合并配置中列出的所有模型的权重，并运行选定的合并方法（大约需要10分钟）。
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The model is now merged and saved in the `merge` directory. Before uploading
    it, we can create a README file with all the information required for reproducibility.
    The following code block defines a Jinja template and automatically fills it with
    the data from the merge configuration.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 模型现在已经合并并保存在`merge`目录中。在上传之前，我们可以创建一个包含所有可复现性所需信息的README文件。以下代码块定义了一个Jinja模板，并自动填充来自合并配置的数据。
- en: '[PRE7]yaml'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE7]yaml'
- en: '{{- yaml_config -}}'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '{{- yaml_config -}}'
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now that we have a model card, we can push the entire folder to the Hub.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了模型卡片，可以将整个文件夹推送到Hub。
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The model is now available on the Hugging Face Hub at [mlabonne/Marcoro14–7B-slerp](https://huggingface.co/mlabonne/Marcoro14-7B-slerp).
    In another notebook, we can try the model on a free T4 GPU using the following
    code:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型现在可以在Hugging Face Hub上获取，链接为[mlabonne/Marcoro14–7B-slerp](https://huggingface.co/mlabonne/Marcoro14-7B-slerp)。在另一个笔记本中，我们可以使用以下代码在免费的T4
    GPU上尝试该模型：
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We’re asking the question “What is a Large Language Model?” and received this
    output:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了“什么是大语言模型？”这个问题，并获得了以下输出：
- en: '*A large language model is a type of artificial intelligence (AI) system that
    has been trained on vast amounts of text data. It’s designed to understand and
    generate human-like language, making predictions on what words or phrases might
    come next in a sentence or document. These models use complex algorithms and neural
    network architectures to learn from the data and improve their performance over
    time. Some well-known large language models include GPT-3 from OpenAI and BERT
    from Google.*'
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*大语言模型是一种人工智能（AI）系统，它通过大量的文本数据进行训练。其设计目的是理解和生成类似人类的语言，预测句子或文档中接下来可能出现的单词或短语。这些模型使用复杂的算法和神经网络架构，从数据中学习，并随着时间的推移提高其表现。一些著名的大语言模型包括OpenAI的GPT-3和Google的BERT。*'
- en: 'It’s looking good, but we need a more comprehensive evaluation. For this kind
    of general-purpose model, there are a few interesting benchmarks:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一切看起来不错，但我们需要更全面的评估。对于这种通用模型，有一些有趣的基准测试：
- en: '[**Chatbot Arena**](https://chat.lmsys.org/), which compiles an Elo-based LLM
    leaderboard based on human votes.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Chatbot Arena**](https://chat.lmsys.org/)，它根据人类投票编制了一个基于Elo的LLM排行榜。'
- en: '[**MT-bench**](https://chat.lmsys.org/) (same link), which uses GPT-4 as a
    judge to grade model responses on a set of multi-turn questions.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**MT-bench**](https://chat.lmsys.org/)（同一个链接），它使用GPT-4作为裁判，基于一组多轮问题对模型的回答进行评分。'
- en: '[**NousResearch benchmark suite**](https://github.com/teknium1/LLM-Benchmark-Logs),
    which aggregates four benchmarks: AGIEval, GPT4ALL, TruthfulQA, and Bigbench.
    GPT4ALL itself includes HellaSwag, OpenBookQA, Winogrande, ARC-Easy, ARC-Challenge,
    BoolQ, and PIQA.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**NousResearch基准套件**](https://github.com/teknium1/LLM-Benchmark-Logs)，它汇集了四个基准测试：AGIEval、GPT4ALL、TruthfulQA和Bigbench。GPT4ALL本身包括HellaSwag、OpenBookQA、Winogrande、ARC-Easy、ARC-Challenge、BoolQ和PIQA。'
- en: '[**Open LLM Leaderboard**](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard),
    which aggregates six benchmarks: ARC, HellaSwag, MMLU, Winogrande, GSM8K, and
    TruthfulQA.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Open LLM排行榜**](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)，它汇集了六个基准测试：ARC、HellaSwag、MMLU、Winogrande、GSM8K和TruthfulQA。'
- en: Unfortunately, we can’t submit our model to the Chatbot Arena. Instead, I chose
    to evaluate it using the Open LLM Leaderboard and NousResearch benchmarks.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们无法将模型提交到Chatbot Arena。相反，我选择使用Open LLM排行榜和NousResearch基准测试来评估它。
- en: 'I submitted our model to the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
    (“🚀 Submit here!” tab). As shown in the introduction, it ranked as **the best
    7B parameter model** on the leaderboard. Here are the complete results:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我将我们的模型提交到了[Open LLM排行榜](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)（“🚀
    Submit here!”标签）。如介绍所示，它在排行榜中排名为**最佳7B参数模型**。以下是完整的结果：
- en: '![](../Images/f4226bad511e593527bd4a16b408aca6.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4226bad511e593527bd4a16b408aca6.png)'
- en: Image by author
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: The problem with the Open LLM Leaderboard is that these benchmarks are public.
    It means that people can train LLMs on the test data to get better results. By
    merging the best models, we also contaminate our own results. It is safe to assume
    that **Marcoro14–7B-slerp is contaminated** and some models used in this merge
    have been trained on the test set. If you want to create the best model and not
    hack the leaderboard, I recommend only using non-merge models to create your own
    merges.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Open LLM Leaderboard 的问题在于这些基准是公开的。这意味着人们可以在测试数据上训练 LLMs 以获得更好的结果。通过合并最佳模型，我们也污染了自己的结果。可以安全地假设**Marcoro14–7B-slerp
    已被污染**，并且本次合并中使用的某些模型可能已在测试集上进行过训练。如果您想创建最好的模型，而不是操控排行榜，我建议仅使用非合并模型来创建您自己的合并模型。
- en: 'This is why we don’t want to only rely on the OpenLLM Leaderboard. For NousResearch
    benchmark suite, I used [🧐 LLM AutoEval](https://github.com/mlabonne/llm-autoeval)
    to compute the scores automatically with a simple Colab notebook. Here are the
    results compared to the excellent [OpenHermes-2.5-Mistral-7B](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么我们不想仅依赖 OpenLLM Leaderboard。对于 NousResearch 基准测试套件，我使用了[🧐 LLM AutoEval](https://github.com/mlabonne/llm-autoeval)通过简单的
    Colab 笔记本自动计算分数。以下是与优秀的[OpenHermes-2.5-Mistral-7B](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B)相比的结果：
- en: '![](../Images/dccb0dc2b74086db79fae8ace4c1bad8.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dccb0dc2b74086db79fae8ace4c1bad8.png)'
- en: Image by author
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'We get a significant improvement over this model on **every benchmark**. Note
    that NousResearch benchmark suite shares some tasks with the Open LLM Leaderboard:
    ARC-Challenge, TruthfulQA, HellaSwag, and Winogrande. To the best of my knowledge,
    Bigbench is the only benchmark that is 100% different (feel free to contact me
    if that’s not the case). However, one of the models we used in this merge could
    still have been trained on Bigbench.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在**每个基准测试**上都取得了显著的提升。请注意，NousResearch 基准测试套件与 Open LLM Leaderboard 共享一些任务：ARC-Challenge、TruthfulQA、HellaSwag
    和 Winogrande。据我所知，Bigbench 是唯一一个 100% 不同的基准（如果不是，请随时与我联系）。然而，我们在此合并中使用的某些模型仍然可能在
    Bigbench 上进行了训练。
- en: Conclusion
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'In this article, we introduced the concept of merging LLMs with four different
    methods. We detailed how SLERP, TIES, DARE, and passthrough work and provided
    examples of configurations. Finally, we ran SLERP with mergekit to create [Marcoro14–7B-slerp](https://huggingface.co/mlabonne/Marcoro14-7B-slerp)
    and upload it to the Hugging Face Hub. We obtained excellent performance on two
    benchmark suites: Open LLM Leaderboard (**best-performing 7B model**) and NousResearch.
    If you want to create your own merges, I recommend using my automated notebook
    [🥱 LazyMergekit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了将 LLMs 合并的四种不同方法。我们详细讲解了 SLERP、TIES、DARE 和 passthrough 的工作原理，并提供了配置示例。最后，我们使用
    mergekit 运行 SLERP，创建了[Marcoro14–7B-slerp](https://huggingface.co/mlabonne/Marcoro14-7B-slerp)，并将其上传至
    Hugging Face Hub。我们在两个基准测试套件上取得了优秀的表现：Open LLM Leaderboard（**表现最好的 7B 模型**）和 NousResearch。如果您想创建自己的合并模型，我推荐使用我的自动化笔记本[🥱
    LazyMergekit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing)。
- en: Another way of combining multiple models is to merge them in a Mixture of Experts
    (MoE) architecture. In the next article, we’ll discuss how to do this in detail
    and create our [own Mixtral-like model](https://huggingface.co/mlabonne/Beyonder-4x7B-v2).
    If you liked this article, please follow me on Medium and Twitter [@maximelabonne](https://twitter.com/maximelabonne).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种合并多个模型的方法是将它们合并在 Mixture of Experts (MoE) 架构中。在下一篇文章中，我们将详细讨论如何执行此操作，并创建我们自己的[类似
    Mixtral 的模型](https://huggingface.co/mlabonne/Beyonder-4x7B-v2)。如果您喜欢本文，请在 Medium
    和 Twitter 上关注我[@maximelabonne](https://twitter.com/maximelabonne)。
- en: '*Learn more about machine learning and support my work with one click — become
    a Medium member here:*'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*通过点击一次，了解更多关于机器学习的信息并支持我的工作——在这里成为 Medium 会员：*'
- en: '[](https://medium.com/@mlabonne/membership?source=post_page-----2118fb392b54--------------------------------)
    [## Join Medium with my referral link — Maxime Labonne'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne/membership?source=post_page-----2118fb392b54--------------------------------)
    [## 通过我的推荐链接加入 Medium — Maxime Labonne'
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story…
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 作为 Medium 会员，您的一部分会员费用将用于支持您阅读的作家，同时您可以完全访问每个故事…
- en: medium.com](https://medium.com/@mlabonne/membership?source=post_page-----2118fb392b54--------------------------------)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@mlabonne/membership?source=post_page-----2118fb392b54--------------------------------)
