- en: Merge Large Language Models with mergekit
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨mergekitåˆå¹¶å¤§è¯­è¨€æ¨¡å‹
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/merge-large-language-models-with-mergekit-2118fb392b54?source=collection_archive---------0-----------------------#2024-01-08](https://towardsdatascience.com/merge-large-language-models-with-mergekit-2118fb392b54?source=collection_archive---------0-----------------------#2024-01-08)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/merge-large-language-models-with-mergekit-2118fb392b54?source=collection_archive---------0-----------------------#2024-01-08](https://towardsdatascience.com/merge-large-language-models-with-mergekit-2118fb392b54?source=collection_archive---------0-----------------------#2024-01-08)
- en: Create your own models easily, no GPU required!
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è½»æ¾åˆ›å»ºä½ è‡ªå·±çš„æ¨¡å‹ï¼Œæ— éœ€GPUï¼
- en: '[](https://medium.com/@mlabonne?source=post_page---byline--2118fb392b54--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page---byline--2118fb392b54--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2118fb392b54--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2118fb392b54--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page---byline--2118fb392b54--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne?source=post_page---byline--2118fb392b54--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page---byline--2118fb392b54--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2118fb392b54--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2118fb392b54--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page---byline--2118fb392b54--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2118fb392b54--------------------------------)
    Â·11 min readÂ·Jan 8, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2118fb392b54--------------------------------)
    Â·é˜…è¯»æ—¶é—´ï¼š11åˆ†é’ŸÂ·2024å¹´1æœˆ8æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/3250f7b46dc7b58c13e186d2b0230d38.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3250f7b46dc7b58c13e186d2b0230d38.png)'
- en: Image by author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: Model merging is a technique that **combines two or more LLMs** into a single
    model. Itâ€™s a relatively new and experimental method to create new models for
    cheap (no GPU required). Model merging works surprisingly well and produced many
    state-of-the-art models on the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹åˆå¹¶æ˜¯ä¸€ç§**å°†ä¸¤ä¸ªæˆ–æ›´å¤šLLMæ¨¡å‹**åˆå¹¶ä¸ºå•ä¸ªæ¨¡å‹çš„æŠ€æœ¯ã€‚è¿™æ˜¯ä¸€ç§ç›¸å¯¹è¾ƒæ–°ä¸”å®éªŒæ€§çš„æ–¹æ³•ï¼Œç”¨äºä»¥ä½æˆæœ¬ï¼ˆæ— éœ€GPUï¼‰åˆ›å»ºæ–°æ¨¡å‹ã€‚æ¨¡å‹åˆå¹¶æ•ˆæœå‡ºä¹æ„æ–™åœ°å¥½ï¼Œå¹¶ä¸”äº§ç”Ÿäº†è®¸å¤šåœ¨[Open
    LLMæ’è¡Œæ¦œ](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)ä¸Šæ’åé å‰çš„é¡¶å°–æ¨¡å‹ã€‚
- en: In this tutorial, we will implement it using the [mergekit](https://github.com/cg123/mergekit)
    library. More specifically, we will review four merge methods and provide examples
    of configurations. Then, we will use mergekit to create our own model, [Marcoro14â€“7B-slerp](https://huggingface.co/mlabonne/Marcoro14-7B-slerp),
    which became the best-performing model on the Open LLM Leaderboard (02/01/24).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨[mergekit](https://github.com/cg123/mergekit)åº“æ¥å®ç°ã€‚æ›´å…·ä½“åœ°ï¼Œæˆ‘ä»¬å°†å›é¡¾å››ç§åˆå¹¶æ–¹æ³•ï¼Œå¹¶æä¾›é…ç½®ç¤ºä¾‹ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨mergekitåˆ›å»ºæˆ‘ä»¬è‡ªå·±çš„æ¨¡å‹ï¼Œ[Marcoro14â€“7B-slerp](https://huggingface.co/mlabonne/Marcoro14-7B-slerp)ï¼Œè¯¥æ¨¡å‹åœ¨Open
    LLMæ’è¡Œæ¦œï¼ˆ2024å¹´1æœˆ2æ—¥ï¼‰ä¸Šè¡¨ç°æœ€ä½³ã€‚
- en: 'The code is available on [GitHub](https://github.com/mlabonne/llm-course/blob/main/Mergekit.ipynb)
    and [Google Colab](https://colab.research.google.com/drive/1_JS7JKJAQozD48-LhYdegcuuZ2ddgXfr?usp=sharing).
    I recommend using my automated notebook to easily run mergekit: [ğŸ¥± LazyMergekit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£ç å¯åœ¨[GitHub](https://github.com/mlabonne/llm-course/blob/main/Mergekit.ipynb)å’Œ[Google
    Colab](https://colab.research.google.com/drive/1_JS7JKJAQozD48-LhYdegcuuZ2ddgXfr?usp=sharing)ä¸Šæ‰¾åˆ°ã€‚æˆ‘æ¨èä½¿ç”¨æˆ‘çš„è‡ªåŠ¨åŒ–ç¬”è®°æœ¬æ¥è½»æ¾è¿è¡Œmergekitï¼š[ğŸ¥±
    LazyMergekit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing)ã€‚
- en: '*A special thanks to* [*Charles Goddard*](https://www.linkedin.com/in/charles-goddard-7b6797b/)*,
    the author of the mergekit library, for reviewing this article.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*ç‰¹åˆ«æ„Ÿè°¢* [*Charles Goddard*](https://www.linkedin.com/in/charles-goddard-7b6797b/)*ï¼Œmergekitåº“çš„ä½œè€…ï¼Œæ„Ÿè°¢ä»–å®¡é˜…æœ¬æ–‡ã€‚*'
- en: '![](../Images/c43a26493b1d996b35c6c341845a3a97.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c43a26493b1d996b35c6c341845a3a97.png)'
- en: Image by author
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: ğŸ¤ Merge algorithms
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ¤ åˆå¹¶ç®—æ³•
- en: In this section, we will focus on four methods currently implemented in [mergekit](https://github.com/cg123/mergekit).
    Note that there are other methods, such as [linear](https://github.com/cg123/mergekit/tree/1011ef3a84e4c5545473602baf7ef32d535044a9#linear)
    and [Task Arithmetic](https://arxiv.org/abs/2212.04089). If youâ€™re interested
    in papers on model merging, I recommend [this excellent collection](https://huggingface.co/collections/osanseviero/model-merging-65097893623330a3a51ead66)
    on Hugging Face.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†é‡ç‚¹ä»‹ç»[mergekit](https://github.com/cg123/mergekit)ä¸­ç›®å‰å®ç°çš„å››ç§æ–¹æ³•ã€‚è¯·æ³¨æ„ï¼Œè¿˜æœ‰å…¶ä»–æ–¹æ³•ï¼Œå¦‚[çº¿æ€§æ’å€¼](https://github.com/cg123/mergekit/tree/1011ef3a84e4c5545473602baf7ef32d535044a9#linear)å’Œ[ä»»åŠ¡ç®—æœ¯](https://arxiv.org/abs/2212.04089)ã€‚å¦‚æœä½ å¯¹æ¨¡å‹åˆå¹¶çš„è®ºæ–‡æ„Ÿå…´è¶£ï¼Œæ¨èæŸ¥çœ‹[è¿™ä¸ªä¼˜ç§€çš„åˆé›†](https://huggingface.co/collections/osanseviero/model-merging-65097893623330a3a51ead66)ã€‚
- en: 1\. SLERP
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. SLERP
- en: '**Spherical Linear Interpolation** (SLERP) is a method used to smoothly interpolate
    between two vectors. It maintains a constant rate of change and preserves the
    geometric properties of the spherical space in which the vectors reside.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**çƒé¢çº¿æ€§æ’å€¼**ï¼ˆSLERPï¼‰æ˜¯ä¸€ç§åœ¨ä¸¤ä¸ªå‘é‡ä¹‹é—´å¹³æ»‘æ’å€¼çš„æ–¹æ³•ã€‚å®ƒä¿æŒæ’å®šçš„å˜åŒ–é€Ÿç‡ï¼Œå¹¶ä¿ç•™å‘é‡æ‰€å¤„çƒé¢ç©ºé—´çš„å‡ ä½•å±æ€§ã€‚'
- en: There are several reasons to prefer SLERP over a traditional linear interpolation.
    For example, in high-dimensional spaces, linear interpolation can lead to a **decrease
    in the magnitude** of the interpolated vector (i.e., it reduces the scale of weights).
    Moreover, the change in direction of the weights often represents **more meaningful
    information** (like feature learning and representation) than the magnitude of
    change.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å‡ ä¸ªç†ç”±æ›´å€¾å‘äºä½¿ç”¨SLERPè€Œä¸æ˜¯ä¼ ç»Ÿçš„çº¿æ€§æ’å€¼ã€‚ä¾‹å¦‚ï¼Œåœ¨é«˜ç»´ç©ºé—´ä¸­ï¼Œçº¿æ€§æ’å€¼å¯èƒ½ä¼šå¯¼è‡´æ’å€¼å‘é‡çš„**å¤§å°å‡å°**ï¼ˆå³å‡å°‘æƒé‡çš„å°ºåº¦ï¼‰ã€‚æ­¤å¤–ï¼Œæƒé‡çš„æ–¹å‘å˜åŒ–é€šå¸¸ä»£è¡¨äº†**æ›´æœ‰æ„ä¹‰çš„ä¿¡æ¯**ï¼ˆå¦‚ç‰¹å¾å­¦ä¹ å’Œè¡¨ç¤ºï¼‰ï¼Œè€Œä¸æ˜¯å˜åŒ–çš„å¤§å°ã€‚
- en: 'SLERP is implemented using the following steps:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: SLERPçš„å®ç°æ­¥éª¤å¦‚ä¸‹ï¼š
- en: Normalize the input vectors to unit length, ensuring they represent directions
    rather than magnitudes
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†è¾“å…¥å‘é‡æ ‡å‡†åŒ–ä¸ºå•ä½é•¿åº¦ï¼Œç¡®ä¿å®ƒä»¬è¡¨ç¤ºçš„æ˜¯æ–¹å‘è€Œéå¤§å°ã€‚
- en: Calculate the angle between these vectors using their dot product.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å®ƒä»¬çš„ç‚¹ç§¯è®¡ç®—è¿™äº›å‘é‡ä¹‹é—´çš„è§’åº¦ã€‚
- en: If the vectors are nearly collinear, it defaults to linear interpolation for
    efficiency. Otherwise, SLERP computing scale factors based on the interpolation
    factor `t` (`t=0` = 100% of the first vector, `t=1` = 100% of model 2) and the
    angle between the vectors.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœå‘é‡å‡ ä¹å…±çº¿ï¼Œåˆ™é»˜è®¤ä½¿ç”¨çº¿æ€§æ’å€¼ä»¥æé«˜æ•ˆç‡ã€‚å¦åˆ™ï¼ŒSLERPä¼šæ ¹æ®æ’å€¼å› å­`t`ï¼ˆ`t=0` = 100%çš„ç¬¬ä¸€ä¸ªå‘é‡ï¼Œ`t=1` = 100%çš„ç¬¬äºŒä¸ªæ¨¡å‹ï¼‰å’Œå‘é‡ä¹‹é—´çš„è§’åº¦è®¡ç®—ç¼©æ”¾å› å­ã€‚
- en: These factors are used to weigh the original vectors, which are then summed
    to obtain the interpolated vector.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿™äº›å› ç´ ç”¨äºåŠ æƒåŸå§‹å‘é‡ï¼Œç„¶åå°†å…¶æ±‚å’Œä»¥å¾—åˆ°æ’å€¼å‘é‡ã€‚
- en: SLERP is currently the most popular merging method, but it is limited to combining
    only two models at a time. It is still possible to hierarchically combine multiple
    models, as shown in [Mistral-7B-Merge-14-v0.1](https://huggingface.co/EmbeddedLLM/Mistral-7B-Merge-14-v0.1).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: SLERPç›®å‰æ˜¯æœ€æµè¡Œçš„åˆå¹¶æ–¹æ³•ï¼Œä½†å®ƒä»…é™äºä¸€æ¬¡åˆå¹¶ä¸¤ä¸ªæ¨¡å‹ã€‚ä»ç„¶å¯ä»¥å±‚æ¬¡åŒ–åœ°åˆå¹¶å¤šä¸ªæ¨¡å‹ï¼Œå¦‚åœ¨[Mistral-7B-Merge-14-v0.1](https://huggingface.co/EmbeddedLLM/Mistral-7B-Merge-14-v0.1)ä¸­æ‰€ç¤ºã€‚
- en: '*Example of configuration:*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*é…ç½®ç¤ºä¾‹ï¼š*'
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This is a classic SLERP configuration, applied to every layer of both models.
    Note that we input a gradient of values for the interpolation factor `t`. The
    parameters for the self-attention and MLP layers will use different combinations
    of [OpenPipe/mistral-ft-optimized-1218](https://huggingface.co/OpenPipe/mistral-ft-optimized-1218)
    and [mlabonne/NeuralHermes-2.5-Mistral-7B](https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B).
    The other layers are a 50/50 mixture of the two models.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªç»å…¸çš„SLERPé…ç½®ï¼Œåº”ç”¨äºä¸¤ä¸ªæ¨¡å‹çš„æ¯ä¸€å±‚ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬è¾“å…¥ä¸€ä¸ªå€¼æ¢¯åº¦ä½œä¸ºæ’å€¼å› å­`t`ã€‚è‡ªæ³¨æ„åŠ›å±‚å’ŒMLPå±‚çš„å‚æ•°å°†ä½¿ç”¨[OpenPipe/mistral-ft-optimized-1218](https://huggingface.co/OpenPipe/mistral-ft-optimized-1218)å’Œ[mlabonne/NeuralHermes-2.5-Mistral-7B](https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B)çš„ä¸åŒç»„åˆã€‚å…¶ä»–å±‚åˆ™æ˜¯ä¸¤ä¸ªæ¨¡å‹çš„50/50æ··åˆã€‚
- en: You can find the final model on the Hugging Face Hub at [mlabonne/NeuralPipe-7B-slerp](https://huggingface.co/mlabonne/NeuralPipe-7B-slerp).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨Hugging Face Hubä¸Šæ‰¾åˆ°æœ€ç»ˆæ¨¡å‹ï¼š[mlabonne/NeuralPipe-7B-slerp](https://huggingface.co/mlabonne/NeuralPipe-7B-slerp)ã€‚
- en: 2\. TIES
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. TIES
- en: 'Introduced in [this paper](https://arxiv.org/abs/2306.01708) by Yadav et al.,
    **TIES-Merging** is designed to efficiently merge multiple task-specific models
    into a single multitask model. It addresses two main challenges in model merging:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[Yadavç­‰äºº](https://arxiv.org/abs/2306.01708)çš„[è®ºæ–‡](https://arxiv.org/abs/2306.01708)ä¸­æå‡ºçš„**TIES-Merging**æ—¨åœ¨é«˜æ•ˆåœ°å°†å¤šä¸ªä»»åŠ¡ç‰¹å®šçš„æ¨¡å‹åˆå¹¶ä¸ºä¸€ä¸ªå¤šä»»åŠ¡æ¨¡å‹ã€‚å®ƒè§£å†³äº†æ¨¡å‹åˆå¹¶ä¸­çš„ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼š
- en: '**Redundancy in model parameters**: It identifies and eliminates redundant
    parameters within task-specific models. This is achieved by focusing on the changes
    made during fine-tuning, identifying the top-k% most significant changes, and
    discarding the rest.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¨¡å‹å‚æ•°ä¸­çš„å†—ä½™**ï¼šå®ƒè¯†åˆ«å¹¶æ¶ˆé™¤ä»»åŠ¡ç‰¹å®šæ¨¡å‹ä¸­çš„å†—ä½™å‚æ•°ã€‚é€šè¿‡å…³æ³¨å¾®è°ƒè¿‡ç¨‹ä¸­æ‰€åšçš„å˜åŒ–ï¼Œè¯†åˆ«æœ€é‡è¦çš„å‰k%å˜åŒ–ï¼Œå¹¶ä¸¢å¼ƒå…¶ä½™éƒ¨åˆ†ã€‚'
- en: '**Disagreement between parameter signs**: Conflicts arise when different models
    suggest opposing adjustments to the same parameter. TIES-Merging resolves these
    conflicts by creating a unified sign vector that represents the most dominant
    direction of change across all models.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å‚æ•°ç¬¦å·ä¹‹é—´çš„åˆ†æ­§**ï¼šå½“ä¸åŒæ¨¡å‹å¯¹åŒä¸€å‚æ•°æå‡ºç›¸åè°ƒæ•´æ—¶ï¼Œä¼šäº§ç”Ÿå†²çªã€‚TIES-Mergingé€šè¿‡åˆ›å»ºä¸€ä¸ªç»Ÿä¸€çš„ç¬¦å·å‘é‡æ¥è§£å†³è¿™äº›å†²çªï¼Œè¡¨ç¤ºæ‰€æœ‰æ¨¡å‹ä¸­æœ€ä¸»å¯¼çš„å˜åŒ–æ–¹å‘ã€‚'
- en: 'TIES-Merging is divided into the following three steps:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: TIES-Mergingåˆ†ä¸ºä»¥ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š
- en: '**Trim**: Reduces redundancy in task-specific models by retaining only a fraction
    the most significant parameters (density parameter) and resetting the rest to
    zero.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä¿®å‰ª**ï¼šé€šè¿‡ä»…ä¿ç•™æœ€é‡è¦å‚æ•°ï¼ˆå¯†åº¦å‚æ•°ï¼‰çš„éƒ¨åˆ†ï¼Œé‡ç½®å…¶ä½™éƒ¨åˆ†ä¸ºé›¶ï¼Œä»è€Œå‡å°‘ä»»åŠ¡ç‰¹å®šæ¨¡å‹ä¸­çš„å†—ä½™ã€‚'
- en: '**Elect Sign**: Resolves sign conflicts across different models by creating
    a unified sign vector based on the most dominant direction (positive or negative)
    in terms of cumulative magnitude.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**é€‰æ‹©ç¬¦å·**ï¼šé€šè¿‡åŸºäºç´¯ç§¯å¤§å°çš„æœ€ä¸»å¯¼æ–¹å‘ï¼ˆæ­£æˆ–è´Ÿï¼‰åˆ›å»ºç»Ÿä¸€ç¬¦å·å‘é‡ï¼Œè§£å†³ä¸åŒæ¨¡å‹ä¹‹é—´çš„ç¬¦å·å†²çªã€‚'
- en: '**Disjoint Merge**: Averages parameter values that align with the unified sign
    vector, excluding zero values.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä¸ç›¸äº¤åˆå¹¶**ï¼šå¹³å‡ä¸ç»Ÿä¸€ç¬¦å·å‘é‡å¯¹é½çš„å‚æ•°å€¼ï¼Œæ’é™¤é›¶å€¼ã€‚'
- en: Unlike SLERP, TIES can merge multiple models at a time.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸SLERPä¸åŒï¼ŒTIESå¯ä»¥ä¸€æ¬¡åˆå¹¶å¤šä¸ªæ¨¡å‹ã€‚
- en: '*Example of configuration:*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*é…ç½®ç¤ºä¾‹ï¼š*'
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'With this config, we use Mistral-7B as a base model to calculate the delta
    weights. We merge the same two models: [mistral-ft-optimized-1218](https://huggingface.co/OpenPipe/mistral-ft-optimized-1218)
    (50%) and [NeuralHermes-2.5-Mistral-7B](https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B)
    (30%) with normalization. Here, the density means that weâ€™re only retaining 50%
    of the parameters of each model (the other half comes from the base model).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ­¤é…ç½®ï¼Œæˆ‘ä»¬ä½¿ç”¨Mistral-7Bä½œä¸ºåŸºç¡€æ¨¡å‹æ¥è®¡ç®—å¢é‡æƒé‡ã€‚æˆ‘ä»¬åˆå¹¶ç›¸åŒçš„ä¸¤ä¸ªæ¨¡å‹ï¼š[mistral-ft-optimized-1218](https://huggingface.co/OpenPipe/mistral-ft-optimized-1218)ï¼ˆ50%ï¼‰å’Œ[NeuralHermes-2.5-Mistral-7B](https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B)ï¼ˆ30%ï¼‰ï¼Œå¹¶è¿›è¡Œå½’ä¸€åŒ–ã€‚è¿™é‡Œçš„å¯†åº¦è¡¨ç¤ºæˆ‘ä»¬åªä¿ç•™æ¯ä¸ªæ¨¡å‹50%çš„å‚æ•°ï¼ˆå¦ä¸€åŠæ¥è‡ªåŸºç¡€æ¨¡å‹ï¼‰ã€‚
- en: 'Note that the sum of the weights is not equal to 1 in the config, but the `normalize:
    true` parameter will automatically normalize them internally. This config is inspired
    by the parameters provided by the author of [OpenHermes-2.5-neural-chat-7b-v3â€“1â€“7B](https://huggingface.co/Weyaxi/OpenHermes-2.5-neural-chat-7b-v3-1-7B).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¯·æ³¨æ„ï¼Œé…ç½®ä¸­çš„æƒé‡æ€»å’Œä¸ç­‰äº1ï¼Œä½†`normalize: true`å‚æ•°ä¼šè‡ªåŠ¨åœ¨å†…éƒ¨å¯¹å…¶è¿›è¡Œå½’ä¸€åŒ–ã€‚è¯¥é…ç½®çµæ„Ÿæ¥è‡ªäº[OpenHermes-2.5-neural-chat-7b-v3â€“1â€“7B](https://huggingface.co/Weyaxi/OpenHermes-2.5-neural-chat-7b-v3-1-7B)ä½œè€…æä¾›çš„å‚æ•°ã€‚'
- en: You can find the final model on the Hugging Face Hub at [mlabonne/NeuralPipe-7B-ties](https://huggingface.co/mlabonne/NeuralPipe-7B-ties).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨Hugging Face Hubä¸Šæ‰¾åˆ°æœ€ç»ˆæ¨¡å‹ï¼š[mlabonne/NeuralPipe-7B-ties](https://huggingface.co/mlabonne/NeuralPipe-7B-ties)ã€‚
- en: 3\. DARE
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. DARE
- en: 'Introduced by Yu et al. (2023), [DARE](https://arxiv.org/abs/2311.03099) uses
    an approach similar to TIES with two main differences:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±Yuç­‰äººï¼ˆ2023å¹´ï¼‰æå‡ºçš„[DARE](https://arxiv.org/abs/2311.03099)é‡‡ç”¨äº†ç±»ä¼¼äºTIESçš„æ–¹æ³•ï¼Œä¸»è¦æœ‰ä¸¤ä¸ªä¸åŒä¹‹å¤„ï¼š
- en: '**Pruning**: DARE randomly reset fine-tuned weights to their original values
    (those of the base model).'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å‰ªæ**ï¼šDAREå°†å¾®è°ƒåçš„æƒé‡éšæœºé‡ç½®ä¸ºå…¶åŸå§‹å€¼ï¼ˆå³åŸºç¡€æ¨¡å‹çš„å€¼ï¼‰ã€‚'
- en: '**Rescaling**: DARE rescales the weights to keep the expectations of model
    outputs approximately unchanged. It adds the rescaled weights of both (or more)
    models to the weights of the base model with a scale factor.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é‡æ–°ç¼©æ”¾**ï¼šDAREé€šè¿‡é‡æ–°ç¼©æ”¾æƒé‡æ¥ä¿æŒæ¨¡å‹è¾“å‡ºçš„æœŸæœ›å€¼å¤§è‡´ä¸å˜ã€‚å®ƒå°†ä¸¤ä¸ªï¼ˆæˆ–å¤šä¸ªï¼‰æ¨¡å‹çš„é‡æ–°ç¼©æ”¾æƒé‡ä¸åŸºç¡€æ¨¡å‹çš„æƒé‡é€šè¿‡ä¸€ä¸ªç¼©æ”¾å› å­ç›¸åŠ ã€‚'
- en: 'Mergekitâ€™s implementation of this method has two flavors: with the sign election
    step of TIES (`dare_ties`) or without (`dare_linear`).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Mergekitå¯¹æ­¤æ–¹æ³•çš„å®ç°æœ‰ä¸¤ç§å½¢å¼ï¼šä¸€ç§æ˜¯å¸¦æœ‰TIESç¬¦å·é€‰æ‹©æ­¥éª¤ï¼ˆ`dare_ties`ï¼‰ï¼Œå¦ä¸€ç§æ˜¯æ²¡æœ‰çš„ï¼ˆ`dare_linear`ï¼‰ã€‚
- en: '*Example of configuration:*'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*é…ç½®ç¤ºä¾‹ï¼š*'
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In this configuration, we merge three different models based on Mistral-7B using
    `dare_ties`. This time, I chose weights that sum to 1 (the sum should be between
    0.9 and 1.1). The density parameter is a little higher than what's recommended
    in the paper (<0.5), but it looks like it gives consistently better results (see
    [this discussion](https://github.com/cg123/mergekit/issues/26)).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ­¤é…ç½®ä¸­ï¼Œæˆ‘ä»¬åŸºäºMistral-7Båˆå¹¶äº†ä¸‰ç§ä¸åŒçš„æ¨¡å‹ï¼Œä½¿ç”¨äº†`dare_ties`ã€‚è¿™æ¬¡ï¼Œæˆ‘é€‰æ‹©äº†æƒé‡ä¹‹å’Œä¸º1çš„ç»„åˆï¼ˆæƒé‡å’Œåº”è¯¥åœ¨0.9åˆ°1.1ä¹‹é—´ï¼‰ã€‚å¯†åº¦å‚æ•°æ¯”è®ºæ–‡ä¸­æ¨èçš„å€¼ï¼ˆ<0.5ï¼‰ç¨é«˜ï¼Œä½†çœ‹èµ·æ¥å®ƒ
    consistently ç»™å‡ºäº†æ›´å¥½çš„ç»“æœï¼ˆå‚è§[è¿™ä¸ªè®¨è®º](https://github.com/cg123/mergekit/issues/26)ï¼‰ã€‚
- en: You can find it on the Hugging Face Hub at [mlabonne/Daredevil-7B](https://huggingface.co/mlabonne/Daredevil-7B).
    Itâ€™s also the best merge model in this article, outperforming even Marcoro14â€“7B-slerp.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨Hugging Face Hubä¸Šæ‰¾åˆ°å®ƒï¼Œåœ°å€æ˜¯[mlabonne/Daredevil-7B](https://huggingface.co/mlabonne/Daredevil-7B)ã€‚å®ƒä¹Ÿæ˜¯æœ¬æ–‡ä¸­è¡¨ç°æœ€å¥½çš„åˆå¹¶æ¨¡å‹ï¼Œç”šè‡³è¶…è¿‡äº†Marcoro14â€“7B-slerpã€‚
- en: 4\. Passthrough
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. é€ä¼ 
- en: The passthrough method differs significantly from the previous ones. By concatenating
    layers from different LLMs, it can produce models with an **exotic number of parameters**
    (e.g., 9B with two 7B parameter models). These models are often referred to as
    â€œfrankenmergesâ€ or â€œFrankenstein modelsâ€ by the community.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: é€ä¼ æ–¹æ³•ä¸ä¹‹å‰çš„å‡ ç§æ–¹æ³•æœ‰æ˜¾è‘—ä¸åŒã€‚é€šè¿‡å°†æ¥è‡ªä¸åŒå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å±‚çº§è¿›è¡Œæ‹¼æ¥ï¼Œå®ƒå¯ä»¥ç”Ÿæˆå…·æœ‰**ç‹¬ç‰¹å‚æ•°æ•°é‡**çš„æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼Œä¸¤ä¸ª7Bå‚æ•°æ¨¡å‹åˆå¹¶ç”Ÿæˆ9Bæ¨¡å‹ï¼‰ã€‚è¿™äº›æ¨¡å‹é€šå¸¸è¢«ç¤¾åŒºç§°ä¸ºâ€œå¼—å…°è‚¯åˆå¹¶â€æˆ–â€œå¼—å…°è‚¯æ–¯å¦æ¨¡å‹â€ã€‚
- en: This technique is very experimental, but it managed to create impressive models,
    like [goliath-120b](https://huggingface.co/alpindale/goliath-120b) using two Llama
    2 70B models. The recently released [SOLAR-10.7B-v1.0](https://huggingface.co/upstage/SOLAR-10.7B-v1.0)
    also uses the same idea, called depth-up scaling [in their paper](https://arxiv.org/abs/2312.15166).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæŠ€æœ¯éå¸¸å®éªŒæ€§ï¼Œä½†å®ƒæˆåŠŸåœ°åˆ›é€ äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ¨¡å‹ï¼Œæ¯”å¦‚ä½¿ç”¨ä¸¤ä¸ªLlama 2 70Bæ¨¡å‹åˆå¹¶çš„[goliath-120b](https://huggingface.co/alpindale/goliath-120b)ã€‚æœ€è¿‘å‘å¸ƒçš„[SOLAR-10.7B-v1.0](https://huggingface.co/upstage/SOLAR-10.7B-v1.0)ä¹Ÿé‡‡ç”¨äº†ç›¸åŒçš„ç†å¿µï¼Œç§°ä¸ºæ·±åº¦å‘ä¸Šæ‰©å±•ï¼Œè¯¦è§ä»–ä»¬çš„è®ºæ–‡[è¿™é‡Œ](https://arxiv.org/abs/2312.15166)ã€‚
- en: '*Example of configuration:*'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*é…ç½®ç¤ºä¾‹ï¼š*'
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The resulting frankenmerge will have all the 32 layers from the first model
    and 8 additional layers from the second model. This creates a frankenmerge with
    a total of 40 layers and 8.99B parameters. This config is inspired by [GML-Mistral-merged-v1](https://huggingface.co/zyh3826/GML-Mistral-merged-v1).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœçš„å¼—å…°è‚¯åˆå¹¶å°†æ‹¥æœ‰æ¥è‡ªç¬¬ä¸€ä¸ªæ¨¡å‹çš„æ‰€æœ‰32å±‚ï¼Œä»¥åŠæ¥è‡ªç¬¬äºŒä¸ªæ¨¡å‹çš„8å±‚ã€‚è¿™åˆ›å»ºäº†ä¸€ä¸ªæ€»å…±40å±‚å’Œ8.99Bå‚æ•°çš„å¼—å…°è‚¯åˆå¹¶æ¨¡å‹ã€‚æ­¤é…ç½®çµæ„Ÿæ¥è‡ªäº[GML-Mistral-merged-v1](https://huggingface.co/zyh3826/GML-Mistral-merged-v1)ã€‚
- en: You can find the final model on the Hugging Face Hub at [mlabonne/NeuralPipe-9B-merged](https://huggingface.co/mlabonne/NeuralPipe-9B-merged).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨Hugging Face Hubä¸Šæ‰¾åˆ°æœ€ç»ˆæ¨¡å‹ï¼Œåœ°å€æ˜¯[mlabonne/NeuralPipe-9B-merged](https://huggingface.co/mlabonne/NeuralPipe-9B-merged)ã€‚
- en: ğŸ’» Merge your own models
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ’» åˆå¹¶ä½ è‡ªå·±çš„æ¨¡å‹
- en: In this section, we will use mergekit to load a merge configuration, run it,
    and upload the resulting model to the Hugging Face Hub.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨mergekitåŠ è½½åˆå¹¶é…ç½®ï¼Œè¿è¡Œå®ƒï¼Œå¹¶å°†åˆå¹¶åçš„æ¨¡å‹ä¸Šä¼ åˆ°Hugging Face Hubã€‚
- en: 'First of all, we install mergekit directly from source as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬ç›´æ¥ä»æºä»£ç å®‰è£…mergekitï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the following block, we load the merge configuration in a YAML format. We
    also specify the name of the merged model for future use. You can copy/paste any
    configuration from the previous section here.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„ä»£ç å—ä¸­ï¼Œæˆ‘ä»¬åŠ è½½åˆå¹¶é…ç½®æ–‡ä»¶ï¼ˆYAMLæ ¼å¼ï¼‰ã€‚æˆ‘ä»¬è¿˜æŒ‡å®šäº†åˆå¹¶åæ¨¡å‹çš„åç§°ï¼Œä»¥ä¾›ä»¥åä½¿ç”¨ã€‚ä½ å¯ä»¥å°†ä¸Šä¸€èŠ‚ä¸­çš„ä»»ä½•é…ç½®å¤åˆ¶/ç²˜è´´åˆ°è¿™é‡Œã€‚
- en: 'This time, we will use two different models: [Marcoroni-7B-v3](https://huggingface.co/AIDC-ai-business/Marcoroni-7B-v3)
    and [Mistral-7B-Merge-14-v0.1](https://huggingface.co/EmbeddedLLM/Mistral-7B-Merge-14-v0.1)
    and merge them with the SLERP method. We save the config as a yaml file to be
    used as input in the merge command.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ¬¡ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸¤ä¸ªä¸åŒçš„æ¨¡å‹ï¼š[Marcoroni-7B-v3](https://huggingface.co/AIDC-ai-business/Marcoroni-7B-v3)
    å’Œ [Mistral-7B-Merge-14-v0.1](https://huggingface.co/EmbeddedLLM/Mistral-7B-Merge-14-v0.1)ï¼Œå¹¶ä½¿ç”¨SLERPæ–¹æ³•å°†å®ƒä»¬åˆå¹¶ã€‚æˆ‘ä»¬å°†é…ç½®ä¿å­˜ä¸ºyamlæ–‡ä»¶ï¼Œä»¥ä¾¿ä½œä¸ºè¾“å…¥ä½¿ç”¨åœ¨åˆå¹¶å‘½ä»¤ä¸­ã€‚
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We run the merge command with the following parameters:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹å‚æ•°è¿è¡Œåˆå¹¶å‘½ä»¤ï¼š
- en: '`--copy-tokenizer` to copy the tokenizer from the base model'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--copy-tokenizer` ç”¨äºä»åŸºç¡€æ¨¡å‹å¤åˆ¶åˆ†è¯å™¨'
- en: '`--allow-crimes` and `--out-shard-size` to chunk the models into smaller shards
    that can be computed on a CPU with low RAM'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--allow-crimes` å’Œ `--out-shard-size` ç”¨äºå°†æ¨¡å‹åˆ†å‰²æˆè¾ƒå°çš„åˆ†ç‰‡ï¼Œä»¥ä¾¿åœ¨ä½å†…å­˜çš„CPUä¸Šè®¡ç®—ã€‚'
- en: '`--lazy-unpickle` to enable the experimental lazy unpickler for lower memory
    usage'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--lazy-unpickle` ä»¥å¯ç”¨å®éªŒæ€§çš„æ‡’åŠ è½½è§£åŒ…å™¨ï¼Œä»è€Œå‡å°‘å†…å­˜ä½¿ç”¨ã€‚'
- en: In addition, some models can require the `--trust_remote_code` flag (this is
    not the case with Mistral-7B).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œä¸€äº›æ¨¡å‹å¯èƒ½éœ€è¦`--trust_remote_code`æ ‡å¿—ï¼ˆMistral-7Bä¸éœ€è¦æ­¤æ ‡å¿—ï¼‰ã€‚
- en: This command will download the weights of all the models listed in the merge
    configuration and run the selected merge method (it should take ~10 minutes).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå‘½ä»¤å°†ä¸‹è½½åˆå¹¶é…ç½®ä¸­åˆ—å‡ºçš„æ‰€æœ‰æ¨¡å‹çš„æƒé‡ï¼Œå¹¶è¿è¡Œé€‰å®šçš„åˆå¹¶æ–¹æ³•ï¼ˆå¤§çº¦éœ€è¦10åˆ†é’Ÿï¼‰ã€‚
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The model is now merged and saved in the `merge` directory. Before uploading
    it, we can create a README file with all the information required for reproducibility.
    The following code block defines a Jinja template and automatically fills it with
    the data from the merge configuration.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹ç°åœ¨å·²ç»åˆå¹¶å¹¶ä¿å­˜åœ¨`merge`ç›®å½•ä¸­ã€‚åœ¨ä¸Šä¼ ä¹‹å‰ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰å¯å¤ç°æ€§æ‰€éœ€ä¿¡æ¯çš„READMEæ–‡ä»¶ã€‚ä»¥ä¸‹ä»£ç å—å®šä¹‰äº†ä¸€ä¸ªJinjaæ¨¡æ¿ï¼Œå¹¶è‡ªåŠ¨å¡«å……æ¥è‡ªåˆå¹¶é…ç½®çš„æ•°æ®ã€‚
- en: '[PRE7]yaml'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE7]yaml'
- en: '{{- yaml_config -}}'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '{{- yaml_config -}}'
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now that we have a model card, we can push the entire folder to the Hub.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æœ‰äº†æ¨¡å‹å¡ç‰‡ï¼Œå¯ä»¥å°†æ•´ä¸ªæ–‡ä»¶å¤¹æ¨é€åˆ°Hubã€‚
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The model is now available on the Hugging Face Hub at [mlabonne/Marcoro14â€“7B-slerp](https://huggingface.co/mlabonne/Marcoro14-7B-slerp).
    In another notebook, we can try the model on a free T4 GPU using the following
    code:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹ç°åœ¨å¯ä»¥åœ¨Hugging Face Hubä¸Šè·å–ï¼Œé“¾æ¥ä¸º[mlabonne/Marcoro14â€“7B-slerp](https://huggingface.co/mlabonne/Marcoro14-7B-slerp)ã€‚åœ¨å¦ä¸€ä¸ªç¬”è®°æœ¬ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç åœ¨å…è´¹çš„T4
    GPUä¸Šå°è¯•è¯¥æ¨¡å‹ï¼š
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Weâ€™re asking the question â€œWhat is a Large Language Model?â€ and received this
    output:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æå‡ºäº†â€œä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼Ÿâ€è¿™ä¸ªé—®é¢˜ï¼Œå¹¶è·å¾—äº†ä»¥ä¸‹è¾“å‡ºï¼š
- en: '*A large language model is a type of artificial intelligence (AI) system that
    has been trained on vast amounts of text data. Itâ€™s designed to understand and
    generate human-like language, making predictions on what words or phrases might
    come next in a sentence or document. These models use complex algorithms and neural
    network architectures to learn from the data and improve their performance over
    time. Some well-known large language models include GPT-3 from OpenAI and BERT
    from Google.*'
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*å¤§è¯­è¨€æ¨¡å‹æ˜¯ä¸€ç§äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ç³»ç»Ÿï¼Œå®ƒé€šè¿‡å¤§é‡çš„æ–‡æœ¬æ•°æ®è¿›è¡Œè®­ç»ƒã€‚å…¶è®¾è®¡ç›®çš„æ˜¯ç†è§£å’Œç”Ÿæˆç±»ä¼¼äººç±»çš„è¯­è¨€ï¼Œé¢„æµ‹å¥å­æˆ–æ–‡æ¡£ä¸­æ¥ä¸‹æ¥å¯èƒ½å‡ºç°çš„å•è¯æˆ–çŸ­è¯­ã€‚è¿™äº›æ¨¡å‹ä½¿ç”¨å¤æ‚çš„ç®—æ³•å’Œç¥ç»ç½‘ç»œæ¶æ„ï¼Œä»æ•°æ®ä¸­å­¦ä¹ ï¼Œå¹¶éšç€æ—¶é—´çš„æ¨ç§»æé«˜å…¶è¡¨ç°ã€‚ä¸€äº›è‘—åçš„å¤§è¯­è¨€æ¨¡å‹åŒ…æ‹¬OpenAIçš„GPT-3å’ŒGoogleçš„BERTã€‚*'
- en: 'Itâ€™s looking good, but we need a more comprehensive evaluation. For this kind
    of general-purpose model, there are a few interesting benchmarks:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€åˆ‡çœ‹èµ·æ¥ä¸é”™ï¼Œä½†æˆ‘ä»¬éœ€è¦æ›´å…¨é¢çš„è¯„ä¼°ã€‚å¯¹äºè¿™ç§é€šç”¨æ¨¡å‹ï¼Œæœ‰ä¸€äº›æœ‰è¶£çš„åŸºå‡†æµ‹è¯•ï¼š
- en: '[**Chatbot Arena**](https://chat.lmsys.org/), which compiles an Elo-based LLM
    leaderboard based on human votes.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Chatbot Arena**](https://chat.lmsys.org/)ï¼Œå®ƒæ ¹æ®äººç±»æŠ•ç¥¨ç¼–åˆ¶äº†ä¸€ä¸ªåŸºäºEloçš„LLMæ’è¡Œæ¦œã€‚'
- en: '[**MT-bench**](https://chat.lmsys.org/) (same link), which uses GPT-4 as a
    judge to grade model responses on a set of multi-turn questions.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**MT-bench**](https://chat.lmsys.org/)ï¼ˆåŒä¸€ä¸ªé“¾æ¥ï¼‰ï¼Œå®ƒä½¿ç”¨GPT-4ä½œä¸ºè£åˆ¤ï¼ŒåŸºäºä¸€ç»„å¤šè½®é—®é¢˜å¯¹æ¨¡å‹çš„å›ç­”è¿›è¡Œè¯„åˆ†ã€‚'
- en: '[**NousResearch benchmark suite**](https://github.com/teknium1/LLM-Benchmark-Logs),
    which aggregates four benchmarks: AGIEval, GPT4ALL, TruthfulQA, and Bigbench.
    GPT4ALL itself includes HellaSwag, OpenBookQA, Winogrande, ARC-Easy, ARC-Challenge,
    BoolQ, and PIQA.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**NousResearchåŸºå‡†å¥—ä»¶**](https://github.com/teknium1/LLM-Benchmark-Logs)ï¼Œå®ƒæ±‡é›†äº†å››ä¸ªåŸºå‡†æµ‹è¯•ï¼šAGIEvalã€GPT4ALLã€TruthfulQAå’ŒBigbenchã€‚GPT4ALLæœ¬èº«åŒ…æ‹¬HellaSwagã€OpenBookQAã€Winograndeã€ARC-Easyã€ARC-Challengeã€BoolQå’ŒPIQAã€‚'
- en: '[**Open LLM Leaderboard**](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard),
    which aggregates six benchmarks: ARC, HellaSwag, MMLU, Winogrande, GSM8K, and
    TruthfulQA.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Open LLMæ’è¡Œæ¦œ**](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)ï¼Œå®ƒæ±‡é›†äº†å…­ä¸ªåŸºå‡†æµ‹è¯•ï¼šARCã€HellaSwagã€MMLUã€Winograndeã€GSM8Kå’ŒTruthfulQAã€‚'
- en: Unfortunately, we canâ€™t submit our model to the Chatbot Arena. Instead, I chose
    to evaluate it using the Open LLM Leaderboard and NousResearch benchmarks.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å¹¸çš„æ˜¯ï¼Œæˆ‘ä»¬æ— æ³•å°†æ¨¡å‹æäº¤åˆ°Chatbot Arenaã€‚ç›¸åï¼Œæˆ‘é€‰æ‹©ä½¿ç”¨Open LLMæ’è¡Œæ¦œå’ŒNousResearchåŸºå‡†æµ‹è¯•æ¥è¯„ä¼°å®ƒã€‚
- en: 'I submitted our model to the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
    (â€œğŸš€ Submit here!â€ tab). As shown in the introduction, it ranked as **the best
    7B parameter model** on the leaderboard. Here are the complete results:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†æˆ‘ä»¬çš„æ¨¡å‹æäº¤åˆ°äº†[Open LLMæ’è¡Œæ¦œ](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)ï¼ˆâ€œğŸš€
    Submit here!â€æ ‡ç­¾ï¼‰ã€‚å¦‚ä»‹ç»æ‰€ç¤ºï¼Œå®ƒåœ¨æ’è¡Œæ¦œä¸­æ’åä¸º**æœ€ä½³7Bå‚æ•°æ¨¡å‹**ã€‚ä»¥ä¸‹æ˜¯å®Œæ•´çš„ç»“æœï¼š
- en: '![](../Images/f4226bad511e593527bd4a16b408aca6.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4226bad511e593527bd4a16b408aca6.png)'
- en: Image by author
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒ
- en: The problem with the Open LLM Leaderboard is that these benchmarks are public.
    It means that people can train LLMs on the test data to get better results. By
    merging the best models, we also contaminate our own results. It is safe to assume
    that **Marcoro14â€“7B-slerp is contaminated** and some models used in this merge
    have been trained on the test set. If you want to create the best model and not
    hack the leaderboard, I recommend only using non-merge models to create your own
    merges.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Open LLM Leaderboard çš„é—®é¢˜åœ¨äºè¿™äº›åŸºå‡†æ˜¯å…¬å¼€çš„ã€‚è¿™æ„å‘³ç€äººä»¬å¯ä»¥åœ¨æµ‹è¯•æ•°æ®ä¸Šè®­ç»ƒ LLMs ä»¥è·å¾—æ›´å¥½çš„ç»“æœã€‚é€šè¿‡åˆå¹¶æœ€ä½³æ¨¡å‹ï¼Œæˆ‘ä»¬ä¹Ÿæ±¡æŸ“äº†è‡ªå·±çš„ç»“æœã€‚å¯ä»¥å®‰å…¨åœ°å‡è®¾**Marcoro14â€“7B-slerp
    å·²è¢«æ±¡æŸ“**ï¼Œå¹¶ä¸”æœ¬æ¬¡åˆå¹¶ä¸­ä½¿ç”¨çš„æŸäº›æ¨¡å‹å¯èƒ½å·²åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œè¿‡è®­ç»ƒã€‚å¦‚æœæ‚¨æƒ³åˆ›å»ºæœ€å¥½çš„æ¨¡å‹ï¼Œè€Œä¸æ˜¯æ“æ§æ’è¡Œæ¦œï¼Œæˆ‘å»ºè®®ä»…ä½¿ç”¨éåˆå¹¶æ¨¡å‹æ¥åˆ›å»ºæ‚¨è‡ªå·±çš„åˆå¹¶æ¨¡å‹ã€‚
- en: 'This is why we donâ€™t want to only rely on the OpenLLM Leaderboard. For NousResearch
    benchmark suite, I used [ğŸ§ LLM AutoEval](https://github.com/mlabonne/llm-autoeval)
    to compute the scores automatically with a simple Colab notebook. Here are the
    results compared to the excellent [OpenHermes-2.5-Mistral-7B](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸æƒ³ä»…ä¾èµ– OpenLLM Leaderboardã€‚å¯¹äº NousResearch åŸºå‡†æµ‹è¯•å¥—ä»¶ï¼Œæˆ‘ä½¿ç”¨äº†[ğŸ§ LLM AutoEval](https://github.com/mlabonne/llm-autoeval)é€šè¿‡ç®€å•çš„
    Colab ç¬”è®°æœ¬è‡ªåŠ¨è®¡ç®—åˆ†æ•°ã€‚ä»¥ä¸‹æ˜¯ä¸ä¼˜ç§€çš„[OpenHermes-2.5-Mistral-7B](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B)ç›¸æ¯”çš„ç»“æœï¼š
- en: '![](../Images/dccb0dc2b74086db79fae8ace4c1bad8.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dccb0dc2b74086db79fae8ace4c1bad8.png)'
- en: Image by author
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: 'We get a significant improvement over this model on **every benchmark**. Note
    that NousResearch benchmark suite shares some tasks with the Open LLM Leaderboard:
    ARC-Challenge, TruthfulQA, HellaSwag, and Winogrande. To the best of my knowledge,
    Bigbench is the only benchmark that is 100% different (feel free to contact me
    if thatâ€™s not the case). However, one of the models we used in this merge could
    still have been trained on Bigbench.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨**æ¯ä¸ªåŸºå‡†æµ‹è¯•**ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚è¯·æ³¨æ„ï¼ŒNousResearch åŸºå‡†æµ‹è¯•å¥—ä»¶ä¸ Open LLM Leaderboard å…±äº«ä¸€äº›ä»»åŠ¡ï¼šARC-Challengeã€TruthfulQAã€HellaSwag
    å’Œ Winograndeã€‚æ®æˆ‘æ‰€çŸ¥ï¼ŒBigbench æ˜¯å”¯ä¸€ä¸€ä¸ª 100% ä¸åŒçš„åŸºå‡†ï¼ˆå¦‚æœä¸æ˜¯ï¼Œè¯·éšæ—¶ä¸æˆ‘è”ç³»ï¼‰ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬åœ¨æ­¤åˆå¹¶ä¸­ä½¿ç”¨çš„æŸäº›æ¨¡å‹ä»ç„¶å¯èƒ½åœ¨
    Bigbench ä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚
- en: Conclusion
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: 'In this article, we introduced the concept of merging LLMs with four different
    methods. We detailed how SLERP, TIES, DARE, and passthrough work and provided
    examples of configurations. Finally, we ran SLERP with mergekit to create [Marcoro14â€“7B-slerp](https://huggingface.co/mlabonne/Marcoro14-7B-slerp)
    and upload it to the Hugging Face Hub. We obtained excellent performance on two
    benchmark suites: Open LLM Leaderboard (**best-performing 7B model**) and NousResearch.
    If you want to create your own merges, I recommend using my automated notebook
    [ğŸ¥± LazyMergekit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†å°† LLMs åˆå¹¶çš„å››ç§ä¸åŒæ–¹æ³•ã€‚æˆ‘ä»¬è¯¦ç»†è®²è§£äº† SLERPã€TIESã€DARE å’Œ passthrough çš„å·¥ä½œåŸç†ï¼Œå¹¶æä¾›äº†é…ç½®ç¤ºä¾‹ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨
    mergekit è¿è¡Œ SLERPï¼Œåˆ›å»ºäº†[Marcoro14â€“7B-slerp](https://huggingface.co/mlabonne/Marcoro14-7B-slerp)ï¼Œå¹¶å°†å…¶ä¸Šä¼ è‡³
    Hugging Face Hubã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªåŸºå‡†æµ‹è¯•å¥—ä»¶ä¸Šå–å¾—äº†ä¼˜ç§€çš„è¡¨ç°ï¼šOpen LLM Leaderboardï¼ˆ**è¡¨ç°æœ€å¥½çš„ 7B æ¨¡å‹**ï¼‰å’Œ NousResearchã€‚å¦‚æœæ‚¨æƒ³åˆ›å»ºè‡ªå·±çš„åˆå¹¶æ¨¡å‹ï¼Œæˆ‘æ¨èä½¿ç”¨æˆ‘çš„è‡ªåŠ¨åŒ–ç¬”è®°æœ¬[ğŸ¥±
    LazyMergekit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing)ã€‚
- en: Another way of combining multiple models is to merge them in a Mixture of Experts
    (MoE) architecture. In the next article, weâ€™ll discuss how to do this in detail
    and create our [own Mixtral-like model](https://huggingface.co/mlabonne/Beyonder-4x7B-v2).
    If you liked this article, please follow me on Medium and Twitter [@maximelabonne](https://twitter.com/maximelabonne).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ç§åˆå¹¶å¤šä¸ªæ¨¡å‹çš„æ–¹æ³•æ˜¯å°†å®ƒä»¬åˆå¹¶åœ¨ Mixture of Experts (MoE) æ¶æ„ä¸­ã€‚åœ¨ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†è¯¦ç»†è®¨è®ºå¦‚ä½•æ‰§è¡Œæ­¤æ“ä½œï¼Œå¹¶åˆ›å»ºæˆ‘ä»¬è‡ªå·±çš„[ç±»ä¼¼
    Mixtral çš„æ¨¡å‹](https://huggingface.co/mlabonne/Beyonder-4x7B-v2)ã€‚å¦‚æœæ‚¨å–œæ¬¢æœ¬æ–‡ï¼Œè¯·åœ¨ Medium
    å’Œ Twitter ä¸Šå…³æ³¨æˆ‘[@maximelabonne](https://twitter.com/maximelabonne)ã€‚
- en: '*Learn more about machine learning and support my work with one click â€” become
    a Medium member here:*'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*é€šè¿‡ç‚¹å‡»ä¸€æ¬¡ï¼Œäº†è§£æ›´å¤šå…³äºæœºå™¨å­¦ä¹ çš„ä¿¡æ¯å¹¶æ”¯æŒæˆ‘çš„å·¥ä½œâ€”â€”åœ¨è¿™é‡Œæˆä¸º Medium ä¼šå‘˜ï¼š*'
- en: '[](https://medium.com/@mlabonne/membership?source=post_page-----2118fb392b54--------------------------------)
    [## Join Medium with my referral link â€” Maxime Labonne'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne/membership?source=post_page-----2118fb392b54--------------------------------)
    [## é€šè¿‡æˆ‘çš„æ¨èé“¾æ¥åŠ å…¥ Medium â€” Maxime Labonne'
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every storyâ€¦
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½œä¸º Medium ä¼šå‘˜ï¼Œæ‚¨çš„ä¸€éƒ¨åˆ†ä¼šå‘˜è´¹ç”¨å°†ç”¨äºæ”¯æŒæ‚¨é˜…è¯»çš„ä½œå®¶ï¼ŒåŒæ—¶æ‚¨å¯ä»¥å®Œå…¨è®¿é—®æ¯ä¸ªæ•…äº‹â€¦
- en: medium.com](https://medium.com/@mlabonne/membership?source=post_page-----2118fb392b54--------------------------------)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@mlabonne/membership?source=post_page-----2118fb392b54--------------------------------)
