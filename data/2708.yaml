- en: 'Data-Driven Journey Optimization: Using Deep Learning to Design Customer Journeys'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据驱动的旅程优化：使用深度学习设计客户旅程
- en: 原文：[https://towardsdatascience.com/data-driven-journey-optimization-using-deep-learning-to-design-customer-journeys-93a3f8e92956?source=collection_archive---------5-----------------------#2024-11-06](https://towardsdatascience.com/data-driven-journey-optimization-using-deep-learning-to-design-customer-journeys-93a3f8e92956?source=collection_archive---------5-----------------------#2024-11-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/data-driven-journey-optimization-using-deep-learning-to-design-customer-journeys-93a3f8e92956?source=collection_archive---------5-----------------------#2024-11-06](https://towardsdatascience.com/data-driven-journey-optimization-using-deep-learning-to-design-customer-journeys-93a3f8e92956?source=collection_archive---------5-----------------------#2024-11-06)
- en: Can ML models learn to construct optimal customer journeys?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习模型能否学会构建最优的客户旅程？
- en: '[](https://medium.com/@brechterlaurin?source=post_page---byline--93a3f8e92956--------------------------------)[![Laurin
    Brechter](../Images/5a68b96bddf86846a2bef9d482ef9dd3.png)](https://medium.com/@brechterlaurin?source=post_page---byline--93a3f8e92956--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--93a3f8e92956--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--93a3f8e92956--------------------------------)
    [Laurin Brechter](https://medium.com/@brechterlaurin?source=post_page---byline--93a3f8e92956--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@brechterlaurin?source=post_page---byline--93a3f8e92956--------------------------------)[![Laurin
    Brechter](../Images/5a68b96bddf86846a2bef9d482ef9dd3.png)](https://medium.com/@brechterlaurin?source=post_page---byline--93a3f8e92956--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--93a3f8e92956--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--93a3f8e92956--------------------------------)
    [Laurin Brechter](https://medium.com/@brechterlaurin?source=post_page---byline--93a3f8e92956--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--93a3f8e92956--------------------------------)
    ·8 min read·Nov 6, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--93a3f8e92956--------------------------------)
    ·阅读时间8分钟·2024年11月6日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/141b852186fec6412524cd5d8908e7ce.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/141b852186fec6412524cd5d8908e7ce.png)'
- en: Optimizing Customer Journeys with Beam Search
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 使用束搜索优化客户旅程
- en: 'Marketing attribution has traditionally been backward-looking: analyzing past
    customer journeys to understand which touchpoints contributed to conversion. But
    what if we could use this historical data to design optimal future journeys? In
    this post, I’ll show how we can combine deep learning with optimization techniques
    to design high-converting customer journeys while respecting real-world constraints.
    We will do so by using an LSTM to predict journeys with high conversion probability
    and then using beam search to find sequences with good chances of conversion.
    All images are created by the author.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 市场营销归因传统上是回顾性的：分析过去的客户旅程，了解哪些接触点对转化做出了贡献。但如果我们能利用这些历史数据来设计最优的未来旅程呢？在这篇文章中，我将展示如何将深度学习与优化技术相结合，设计高转化率的客户旅程，同时尊重现实世界中的约束条件。我们将通过使用LSTM预测高转化概率的旅程，然后使用束搜索来找到具有高转化机会的序列来实现这一目标。所有图片均由作者创作。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Customers interact with businesses on what we can call a customer journey. On
    this journey, they come into contact with the company through so-called touchpoints
    (e.g., Social Media, Google Ads, …). At any point, users could convert (e.g. by
    buying your product). We want to know what touchpoints along that journey contributed
    to the conversion to optimize the conversion rate.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 客户与企业的互动可以被称为客户旅程。在这一旅程中，他们通过所谓的接触点（例如社交媒体、谷歌广告等）与公司接触。在任何时刻，用户都可能发生转化（例如购买你的产品）。我们希望了解在这一旅程中的哪些接触点对转化做出了贡献，从而优化转化率。
- en: The Limitations of Traditional Attribution
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 传统归因的局限性
- en: Before diving into our solution, it’s important to understand why traditional
    attribution models fall short.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解我们的解决方案之前，了解传统归因模型为何存在不足是很重要的。
- en: 1\. Position-Agnostic Attribution
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 位置无关的归因
- en: 'Traditional attribution models (first-touch, last-touch, linear, etc.) typically
    assign a single importance score to each channel, regardless of where it appears
    in the customer journey. This is fundamentally flawed because:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的归因模型（首接触、末接触、线性等）通常会为每个渠道分配一个单一的重要性分数，无论该渠道在客户旅程中处于什么位置。这是根本性的缺陷，因为：
- en: A social media ad might be highly effective early in the awareness stage but
    less impactful during consideration
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 社交媒体广告在意识阶段可能非常有效，但在考虑阶段影响力较小
- en: Email effectiveness often depends on whether it’s a welcome email, nurture sequence,
    or re-engagement campaign
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 邮件的有效性通常取决于它是欢迎邮件、培养序列还是重新参与活动
- en: Retargeting ads make little sense as a first touchpoint but can be powerful
    later in the journey
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新定向广告作为第一次接触点意义不大，但在客户旅程后期可以非常有力
- en: 2\. Context Blindness
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 上下文盲点
- en: 'Most attribution models (even data-driven ones) ignore crucial contextual factors:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数归因模型（即使是数据驱动的）忽略了关键的上下文因素：
- en: '**Customer Characteristics**: A young tech-savvy customer might respond differently
    to digital channels compared to traditional ones'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**客户特征**：年轻的科技精通型客户可能会对数字渠道与传统渠道有不同的响应'
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Previous Purchase History**: Existing customers often require different engagement
    strategies than new prospects'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**先前购买历史**：现有客户通常需要与新客户不同的参与策略'
- en: '**Time of Day/Week**: Channel effectiveness can vary significantly based on
    timing'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一天中的时间/一周中的时间**：渠道效果可能会根据时机有显著变化'
- en: '**Device/Platform**: The same channel might perform differently across different
    platforms'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设备/平台**：相同的渠道在不同的平台上可能表现不同'
- en: '**Geographic/Cultural Factors**: What works in one market might fail in another'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**地理/文化因素**：在一个市场有效的方法可能在另一个市场失败'
- en: 3\. Static Channel Values
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 静态渠道值
- en: Traditional models assume channel effectiveness can be expressed as a single
    number where all other factors influencing the effectiveness are marginalized.
    As mentioned above, channel effectiveness is highly context-dependent and should
    be a function of said context (e.g. position, other touchpoints, …).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 传统模型假设渠道效果可以用一个数字表示，其中所有其他影响效果的因素被边缘化。如上所述，渠道效果高度依赖于上下文，应该是该上下文的一个函数（例如位置、其他接触点等）。
- en: Deep Learning Enters the Stage
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习进入舞台
- en: 'Customer journeys are inherently sequential — the order and timing of touchpoints
    matter. We can frame attribution modeling as a *binary time series classification*
    task where we want to predict from the sequence of touchpoints whether a customer
    converted or not. This makes them perfect candidates for sequence modeling using
    R*ecurrent Neural Networks* (RNNs), specifically *Long Short-Term Memory* (LSTM)
    networks. These models can capture complex patterns in sequential data, including:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 客户旅程本质上是顺序的——接触点的顺序和时机非常重要。我们可以将归因建模框架设为*二元时间序列分类*任务，我们希望从接触点序列中预测客户是否转化。这使得它们成为使用R*递归神经网络*（RNN）特别是*长短期记忆*（LSTM）网络的顺序建模的完美候选。这些模型能够捕捉顺序数据中的复杂模式，包括：
- en: The effectiveness of different channel combinations
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同渠道组合的效果
- en: The importance of touchpoint ordering
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接触点排序的重要性
- en: Timing sensitivities
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间敏感性
- en: Channel interaction effects
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 渠道交互效应
- en: Learning from Historical Data
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从历史数据中学习
- en: 'The first step is to train an LSTM model on historical customer journey data.
    For each customer, we need:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是训练一个LSTM模型，基于历史客户旅程数据。对于每个客户，我们需要：
- en: The sequence of touchpoints they encountered
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 他们遇到的接触点的顺序
- en: Whether they ultimately converted
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 客户是否最终转化
- en: Characteristics of the customer
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 客户的特征
- en: The LSTM learns to predict conversion probability given any sequence of touchpoints.
    This gives us a powerful “simulator” that can evaluate the likely effectiveness
    of any proposed customer journey.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM学会根据任何一系列接触点预测转化概率。这为我们提供了一个强大的“模拟器”，可以评估任何提出的客户旅程的可能效果。
- en: As I did not find a suitable dataset (especially one that contains customer
    characteristics as the contextual data), I decided to generate my own synthetic
    data. The notebook for the data generation can be found [here](https://github.com/LaurinBrechter/PatternMining/blob/main/attribution_modelling.ipynb).
    We generate some characteristics and a random number of customer journeys for
    each customer. The journeys are of random length. At each point in the journey,
    the customer interacts with a touchpoint and has a probability of converting.
    This probability is composed of multiple factors.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我没有找到合适的数据集（特别是包含客户特征作为上下文数据的），我决定生成自己的合成数据。数据生成的笔记本可以在[这里](https://github.com/LaurinBrechter/PatternMining/blob/main/attribution_modelling.ipynb)找到。我们为每个客户生成一些特征和随机数量的客户旅程。这些旅程长度是随机的。在旅程的每一个点，客户都会与一个接触点互动，并且有转化的概率。这个概率由多个因素组成。
- en: The base conversion rate of the channel
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 渠道的基础转化率
- en: A positional multiplier. Some channels are more or less effective in some positions
    of the journey.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置乘数。某些渠道在旅程的某些位置上更加或更少有效。
- en: A segment multiplier. The channel's effectiveness depends on the segment of
    the customer.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个细分乘数。渠道的有效性取决于客户所在的细分群体。
- en: We also have interaction effects. E.g. if the user is young, touchpoints such
    as social and search will be more effective.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还会有交互效应。例如，如果用户年轻，那么像社交和搜索这样的接触点会更有效。
- en: Additionally, the previous touchpoint matters for the effectiveness of the current
    touchpoint.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，前一个接触点对当前接触点的有效性也有影响。
- en: '![](../Images/7851800686a72b45682b42118fd66169.png)![](../Images/53ae754078112929b1a802d2ef534af7.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7851800686a72b45682b42118fd66169.png)![](../Images/53ae754078112929b1a802d2ef534af7.png)'
- en: Journey Data (Left) and User Data (Right)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 旅程数据（左）和用户数据（右）
- en: We then preprocess the data by merging the two tables, scaling the numerical
    features, and OneHotEncoding the categorical features. We can then set up an LSTM
    model that processes the sequences of touchpoints after embedding them. In the
    final fully connected layer, we also add the contextual features of the customer.
    The full code for preprocessing and training can be found in this [notebook](https://github.com/LaurinBrechter/PatternMining/blob/main/attribution_modelling.ipynb).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们通过合并这两张表格、缩放数值特征以及对类别特征进行OneHot编码来预处理数据。然后，我们可以建立一个LSTM模型，处理在嵌入后按顺序排列的接触点数据。在最后的全连接层中，我们还加入了客户的上下文特征。数据预处理和训练的完整代码可以在这个[notebook](https://github.com/LaurinBrechter/PatternMining/blob/main/attribution_modelling.ipynb)中找到。
- en: We can then train the neural network with a binary cross-entropy loss. I have
    plotted the recall achieved on the test set below. In this case, we care more
    about recall than accuracy as we want to detect as many converting customers as
    possible. Wrongly predicting that some customers will convert if they don’t is
    not as bad as missing high-potential customers.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用二元交叉熵损失函数训练神经网络。我已经绘制了在测试集上实现的召回率，见下图。在这种情况下，我们更关注召回率而非准确率，因为我们希望尽可能多地检测到会转化的客户。如果错误地预测某些客户会转化，而他们实际上不会，这比错过潜力较大的客户要轻微得多。
- en: '![](../Images/5998b8c4871c80ab6116a6a598fd04de.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5998b8c4871c80ab6116a6a598fd04de.png)'
- en: Training the JourneyLSTM
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 训练JourneyLSTM
- en: Additionally, we will find that most journeys do not lead to a conversion. We
    will typically see conversion rates from 2% to 7% which means that we have a highly
    imbalanced dataset. For the same reason, accuracy isn’t all that meaningful. Always
    predicting the majority class (in this case ‘no conversion’) will get us a very
    high accuracy but we won’t find any of the converting users.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们会发现，大多数旅程并未导致转化。我们通常会看到2%到7%之间的转化率，这意味着我们有一个高度不平衡的数据集。出于同样的原因，准确率并不是特别有意义。总是预测多数类（在这种情况下是“无转化”）会得到一个很高的准确率，但我们无法找到任何转化的用户。
- en: From Prediction to Optimization
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从预测到优化
- en: Once we have a trained model, we can use it to design optimal journeys. We can
    impose a sequence of channels (in the example below channel 1 then 2) on a set
    of customers and look at the conversion probability predicted by the model. We
    can already see that these vary a lot depending on the characteristics of the
    customer. Therefore, we want to optimize the journey for each customer individually.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了训练好的模型，就可以用它来设计最优的旅程。我们可以对一组客户强加一系列渠道（如下例，先是渠道1然后是渠道2），并观察模型预测的转化概率。我们已经可以看到，这些概率根据客户的特征差异很大。因此，我们希望为每个客户单独优化旅程。
- en: '![](../Images/d571bec702bb098ab68e075be6b6b91c.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d571bec702bb098ab68e075be6b6b91c.png)'
- en: Imposed Journeys and Predicted Conversion Probabilities
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 强加的旅程和预测的转化概率
- en: 'Additionally, we can’t just pick the highest-probability sequence. Real-world
    marketing has constraints:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们不能仅仅选择最高概率的序列。现实世界中的营销有很多约束：
- en: Channel-specific limitations (e.g., email frequency caps)
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 渠道特定限制（例如，电子邮件频率上限）
- en: Required touchpoints at specific positions
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在特定位置所需的接触点
- en: Budget constraints
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预算约束
- en: Timing requirements
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间要求
- en: 'Therefore, we frame this as a constrained combinatorial optimization problem:
    find the sequence of touchpoints that maximizes the model’s predicted conversion
    probability while satisfying all constraints. In this case, we will only constrain
    the occurrence of touchpoints at certain places in the journey. That is, we have
    a mapping from position to touchpoint that specifies that a certain touchpoint
    must occur at a given position.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将此问题框定为一个受约束的组合优化问题：找到一个触点序列，在满足所有约束的同时，最大化模型预测的转化概率。在这种情况下，我们将仅在旅程中的特定位置限制触点的出现。也就是说，我们有一个从位置到触点的映射，指定某个触点必须出现在给定的位置。
- en: Note also that we aim to optimize for a predefined journey length rather than
    journeys of arbitrary length. By the nature of the simulation, the overall conversion
    probability will be strictly monotonically increasing as we have a non-zero conversion
    probability at each touchpoint. Therefore, a longer journey (more non-zero entries)
    would trump a shorter journey most of the time and we would construct infinitely
    long journeys.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要注意，我们的目标是优化预定义的旅程长度，而不是任意长度的旅程。根据模拟的性质，由于在每个触点上都有非零的转化概率，因此整体转化概率会严格单调递增。因此，更长的旅程（更多的非零条目）通常会优于较短的旅程，我们可能会构造出无限长的旅程。
- en: Optimization using Beam Search
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用束搜索进行优化
- en: Below is the implementation for beam search using recursion. At each level,
    we optimize a certain position in the journey. If the position is in the constraints
    and already fixed, we skip it. If we have reached the maximum length we want to
    optimize, we stop recursing and return.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用递归实现束搜索的代码。在每一层，我们优化旅程中的某个位置。如果该位置处于约束条件中并且已经固定，我们将跳过它。如果我们达到了要优化的最大长度，我们将停止递归并返回结果。
- en: At each level, we look at current solutions and generate candidates. At any
    point, we keep the best K candidates defined by the beam width. Those best candidates
    are then used as input for the next round of beam search where we optimize the
    next position in the sequence.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一层，我们查看当前的解决方案并生成候选解。在任何时刻，我们保留由束宽定义的最佳K个候选解。这些最佳候选解将作为输入用于下一轮的束搜索，在其中我们优化序列中的下一个位置。
- en: '[PRE1]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This optimization approach is greedy and we are likely to miss high-probability
    combinations. Nonetheless, in many scenarios, especially with many channels, brute
    forcing an optimal solution may not be feasible as the number of possible journeys
    grows exponentially with the journey length.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这种优化方法是贪婪的，我们很可能会错过高概率的组合。然而，在许多场景下，特别是在有许多渠道的情况下，暴力求解最优解可能不可行，因为可能的旅程数量随着旅程长度的增加呈指数级增长。
- en: '![](../Images/141b852186fec6412524cd5d8908e7ce.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/141b852186fec6412524cd5d8908e7ce.png)'
- en: Optimizing Customer Journeys with Beam Search
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 使用束搜索优化客户旅程
- en: In the image above, we optimized the conversion probability for a single customer.
    In position 0, we have specified ‘email’ as a fixed touchpoint. Then, we explore
    possible combinations with email. Since we have a beam width of five, all combinations
    (e.g. email -> search) go into the next round. In that round, we discovered the
    high-potential journey which would display the user two times email and finally
    retarget.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，我们为单个客户优化了转化概率。在位置0，我们已将“电子邮件”指定为固定触点。然后，我们探索与电子邮件的可能组合。由于我们设置的束宽为五，所有组合（例如，电子邮件
    -> 搜索）都进入下一轮。在那一轮中，我们发现了高潜力的旅程，该旅程将用户展示两次电子邮件，最终进行再营销。
- en: Conclusion
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Moving from prediction to optimization in attribution modeling means we are
    going from predictive to prescriptive modeling where the model tells us actions
    to take. This has the potential to achieve much higher conversion rates, especially
    when we have highly complex scenarios with many channels and contextual variables.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 从预测到优化的转变，在归因建模中意味着我们从预测性建模转向指导性建模，即模型告诉我们需要采取的行动。这有可能实现更高的转化率，特别是当我们面对具有许多渠道和上下文变量的复杂场景时。
- en: At the same time, this approach has several drawbacks. Firstly, if we do not
    have a model that can detect converting customers sufficiently well, we are likely
    to harm conversion rates. Additionally, the probabilities that the model outputs
    have to be calibrated well. Otherwiese, the conversion probabilities we are optimizing
    for are likely not meanningful. Lastly, we will encounter problems when the model
    has to predict journeys that are outside of its data distribution. It would therefore
    also be desirable to use a Reinforcement Learning (RL) approach, where the model
    can actively generate new training data.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，这种方法也有几个缺点。首先，如果我们没有一个足够准确地检测转化客户的模型，我们可能会降低转化率。其次，模型输出的概率必须经过良好的校准。否则，我们优化的转化概率可能没有实际意义。最后，当模型需要预测超出其数据分布的旅程时，我们将遇到问题。因此，使用强化学习（RL）方法也是一种值得考虑的选择，在这种方法中，模型可以主动生成新的训练数据。
