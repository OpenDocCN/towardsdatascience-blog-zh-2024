- en: How to Set Up a Multi-GPU Linux Machine for Deep Learning in 2024
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在2024年设置一个用于深度学习的多GPU Linux机器
- en: 原文：[https://towardsdatascience.com/how-to-setup-a-multi-gpu-linux-machine-for-deep-learning-in-2024-df561a2d3328?source=collection_archive---------0-----------------------#2024-05-19](https://towardsdatascience.com/how-to-setup-a-multi-gpu-linux-machine-for-deep-learning-in-2024-df561a2d3328?source=collection_archive---------0-----------------------#2024-05-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-setup-a-multi-gpu-linux-machine-for-deep-learning-in-2024-df561a2d3328?source=collection_archive---------0-----------------------#2024-05-19](https://towardsdatascience.com/how-to-setup-a-multi-gpu-linux-machine-for-deep-learning-in-2024-df561a2d3328?source=collection_archive---------0-----------------------#2024-05-19)
- en: DEEP LEARNING WITH MULTIPLE GPUS
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用多个GPU进行深度学习
- en: Super-fast setup of CUDA and PyTorch in minutes!
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在几分钟内快速设置CUDA和PyTorch！
- en: '[](https://medium.com/@nirajkamal?source=post_page---byline--df561a2d3328--------------------------------)[![Nika](../Images/fcf9dfec64ccae5ea841fcc5046817d6.png)](https://medium.com/@nirajkamal?source=post_page---byline--df561a2d3328--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--df561a2d3328--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--df561a2d3328--------------------------------)
    [Nika](https://medium.com/@nirajkamal?source=post_page---byline--df561a2d3328--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@nirajkamal?source=post_page---byline--df561a2d3328--------------------------------)[![Nika](../Images/fcf9dfec64ccae5ea841fcc5046817d6.png)](https://medium.com/@nirajkamal?source=post_page---byline--df561a2d3328--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--df561a2d3328--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--df561a2d3328--------------------------------)
    [Nika](https://medium.com/@nirajkamal?source=post_page---byline--df561a2d3328--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--df561a2d3328--------------------------------)
    ·6 min read·May 19, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--df561a2d3328--------------------------------)
    ·6分钟阅读·2024年5月19日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/3cef01ab529215b4b49adeed49721c78.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3cef01ab529215b4b49adeed49721c78.png)'
- en: 'Image by Author: Multi-GPU machine (cartoon)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像：多GPU机器（卡通图）
- en: As Deep Learning models (especially LLMs) keep getting bigger, the need for
    more GPU memory (VRAM) is ever-increasing for developing them and using them locally.
    Building or obtaining a multi-GPU machine is just the first part of the challenge.
    Most libraries and applications only use a single GPU by default. Thus, the machine
    also needs to have appropriate drivers along with libraries that can leverage
    the multi-GPU setup.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习模型（尤其是LLM）不断变得更大，开发和本地使用这些模型对GPU内存（VRAM）的需求日益增加。构建或获得一台多GPU机器仅仅是挑战的第一部分。大多数库和应用程序默认只使用单个GPU。因此，机器还需要配备适当的驱动程序以及能够利用多GPU设置的库。
- en: This story provides a guide on how to set up a multi-GPU (Nvidia) Linux machine
    with important libraries. This will hopefully save you some time on experimentation
    and get you started on your development.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提供了一个如何设置多GPU（Nvidia）Linux机器并安装重要库的指南。希望能够节省你在实验中的时间，帮助你更快开始开发。
- en: At the end, links are provided to popular open-source libraries that can leverage
    the multi-GPU setup for Deep Learning.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，提供了可以利用多GPU设置进行深度学习的流行开源库的链接。
- en: Target
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标
- en: Set up a Multi-GPU Linux system with necessary libraries such as CUDA Toolkit
    and PyTorch to get started with Deep Learning *🤖*. The same steps also apply to
    a single GPU machine.
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 设置一个多GPU的Linux系统，并安装必要的库，如CUDA工具包和PyTorch，以开始进行深度学习*🤖*。相同的步骤也适用于单GPU机器。
- en: We will install 1) CUDA Toolkit, 2) PyTorch and 3) Miniconda to get started
    with Deep Learning using frameworks such as exllamaV2 and torchtune.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将安装1）CUDA工具包，2）PyTorch和3）Miniconda，开始使用exllamaV2和torchtune等框架进行深度学习。
- en: ©️ All the libraries and information mentioned in this story are open-source
    and/or publicly available.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ©️ 本文中提到的所有库和信息均为开源和/或公开可用。
- en: Getting Started
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 入门
- en: '![](../Images/cf98219698c22c0bc7ba55518467eb1f.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf98219698c22c0bc7ba55518467eb1f.png)'
- en: 'Image by Author: Output of the nvidia-smi command on a Linux Machine with 8
    Nvidia A10G GPUs'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像：在配备8个Nvidia A10G GPU的Linux机器上运行nvidia-smi命令的输出
- en: Check the number of GPUs installed in the machine using the `nvidia-smi` command
    in the terminal. It should print a list of all the installed GPUs. If there is
    a discrepancy or if the command does not work, first install the Nvidia drivers
    for your version of Linux. Make sure the `nvidia-smi` command prints a list of
    all the GPUs installed in your machine as shown above.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用终端中的 `nvidia-smi` 命令检查机器中安装的 GPU 数量。它应该打印出所有已安装的 GPU 列表。如果有任何不一致，或者命令无法工作，请首先为你的
    Linux 版本安装 Nvidia 驱动程序。确保 `nvidia-smi` 命令能正确打印出所有已安装的 GPU，如上所示。
- en: 'Follow this page to install Nvidia Drivers if not done already:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果尚未安装 Nvidia 驱动程序，请按照此页面安装：
- en: '[How to install the NVIDIA drivers on Ubuntu 22.04 — Linux Tutorials — Learn
    Linux Configuration](https://linuxconfig.org/how-to-install-the-nvidia-drivers-on-ubuntu-22-04)-
    (Source: linuxconfig.org)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[如何在 Ubuntu 22.04 上安装 NVIDIA 驱动程序 — Linux 教程 — 学习 Linux 配置](https://linuxconfig.org/how-to-install-the-nvidia-drivers-on-ubuntu-22-04)（来源：linuxconfig.org）'
- en: Step-1 Install CUDA-Toolkit
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 1 步 安装 CUDA 工具包
- en: 💡 *Check for any existing CUDA folder at* `*usr/local/cuda-xx*`*. That means
    a version of CUDA is already installed. If you already have the desired CUDA toolkit
    installed (check with the* `*nvcc*` *command in your terminal) please skip to
    Step-2.*
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 💡 *检查是否存在现有的 CUDA 文件夹* `*usr/local/cuda-xx*`*。这意味着已经安装了一个版本的 CUDA。如果你已经安装了所需的
    CUDA 工具包（通过在终端中使用* `*nvcc*` *命令检查），请跳到第 2 步。*
- en: 'Check the CUDA version needed for your desired PyTorch library: [Start Locally
    | PyTorch](https://pytorch.org/get-started/locally/) (We are installing Install
    CUDA 12.1)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 检查你所需的 PyTorch 库所需的 CUDA 版本：[从本地开始 | PyTorch](https://pytorch.org/get-started/locally/)（我们正在安装
    CUDA 12.1）
- en: Go to [CUDA Toolkit 12.1 Downloads | NVIDIA Developer](https://developer.nvidia.com/cuda-12-1-0-download-archive)
    to obtain Linux commands to install CUDA 12.1 (choose your OS version and the
    corresponding “deb (local)” installer type).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 前往 [CUDA Toolkit 12.1 下载 | NVIDIA 开发者](https://developer.nvidia.com/cuda-12-1-0-download-archive)
    获取 Linux 安装 CUDA 12.1 的命令（选择你的操作系统版本和相应的“deb（本地）”安装类型）。
- en: '![](../Images/29309c4402baba465c39b23a0bc89e15.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29309c4402baba465c39b23a0bc89e15.png)'
- en: 'Options selected for Ubuntu 22 (Source: developer.nvidia.com)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为 Ubuntu 22 选择的选项（来源：developer.nvidia.com）
- en: 'The terminal commands for the base installer will appear according to your
    chosen options. Copy-paste and run them in your Linux terminal to install the
    CUDA toolkit. For example, for x86_64 Ubuntu 22, run the following commands by
    opening the terminal in the downloads folder:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 基础安装程序的终端命令将根据你选择的选项出现。将它们复制并粘贴到你的 Linux 终端中运行，以安装 CUDA 工具包。例如，对于 x86_64 Ubuntu
    22，打开下载文件夹中的终端并运行以下命令：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ⚠️*While installing the CUDA toolkit, the installer may prompt a kernel update.
    If any pop-up appears in the terminal to update the kernel, press the* `*esc*`
    *button to cancel it. Do not update the kernel during this stage!— it may break
    your Nvidia drivers* ☠️.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ⚠️*在安装 CUDA 工具包时，安装程序可能会提示更新内核。如果终端中出现任何提示更新内核的弹窗，按* `*esc*` *按钮取消。此阶段不要更新内核！——这样可能会破坏你的
    Nvidia 驱动程序* ☠️。
- en: Restart the Linux machine after the installation. The `nvcc` command will still
    not work. You need to add the CUDA installation to PATH. Open the `.bashrc` file
    using the nano editor.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，请重新启动 Linux 机器。`nvcc` 命令仍然无法使用。你需要将 CUDA 安装路径添加到 PATH 中。使用 nano 编辑器打开
    `.bashrc` 文件。
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Scroll to the bottom of the `.bashrc` file and add these two lines:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动到 `.bashrc` 文件的底部，并添加以下两行：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 💡 *Note that you can change* `*cuda-12.1*` *to your installed CUDA version,*
    `*cuda-xx*` *if needed in the future , ‘xx’ being your CUDA version.*
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 💡 *请注意，你可以将* `*cuda-12.1*` *更改为你安装的 CUDA 版本，* `*cuda-xx*` *，如果将来需要，‘xx’ 表示你的
    CUDA 版本。*
- en: 'Save the changes and close the nano editor:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 保存更改并关闭 nano 编辑器：
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Close and reopen the terminal. Now the `nvcc--version` command should print
    the installed CUDA version in your terminal.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 关闭并重新打开终端。现在，`nvcc--version` 命令应该在终端中打印已安装的 CUDA 版本。
- en: Step-2 Install Miniconda
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 2 步 安装 Miniconda
- en: Before we install PyTorch, it is better to install Miniconda and then install
    PyTorch inside a Conda environment. It also is handy to create a new Conda environment
    for each project.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装 PyTorch 之前，最好先安装 Miniconda，然后在 Conda 环境中安装 PyTorch。为每个项目创建一个新的 Conda 环境也是很方便的。
- en: 'Open the terminal in the Downloads folder and run the following commands:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 打开下载文件夹中的终端，并运行以下命令：
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Close and re-open the terminal. Now the `conda` command should work.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 关闭并重新打开终端。现在，`conda` 命令应该可以正常工作。
- en: Step-3 Install PyTorch
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 3 步 安装 PyTorch
- en: (Optional) — Create a new conda environment for your project. You can replace
    `<environment-name>` with the name of your choice. I usually name it after my
    project name.💡 *You can use the* `*conda activate <environment-name>*` *and* `*conda
    deactivate <environment-name>*` *commands before and after working on your project.*
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: （可选）— 为你的项目创建一个新的 conda 环境。你可以将 `<environment-name>` 替换为你喜欢的名称。我通常使用项目名称来命名。💡
    *你可以在工作前后使用* `*conda activate <environment-name>*` *和* `*conda deactivate <environment-name>*`
    *命令。*
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Install the PyTorch library for your CUDA version. The following commands are
    for cuda-12.1 which we installed:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为你的 CUDA 版本安装 PyTorch 库。以下命令适用于我们安装的 cuda-12.1 版本：
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The above command is obtained from PyTorch installation guide — [Start Locally
    | PyTorch](https://pytorch.org/get-started/locally/) .
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令来源于 PyTorch 安装指南 — [Start Locally | PyTorch](https://pytorch.org/get-started/locally/)。
- en: '![](../Images/2cb51751b02a7e251fb9eb15d57fab1e.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2cb51751b02a7e251fb9eb15d57fab1e.png)'
- en: '(Source: pytorch.org)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：pytorch.org）
- en: After PyTorch installation, check the number of GPUs visible to PyTorch in the
    terminal.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 PyTorch 后，检查在终端中 PyTorch 可见的 GPU 数量。
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This should print the number of GPUs installed in the system (8 in my case),
    and should also match the number of listed GPUs in the `nvidia-smi` command.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该打印出系统中安装的 GPU 数量（在我的案例中是 8 个），并且应与 `nvidia-smi` 命令中列出的 GPU 数量一致。
- en: Viola! you are all set to start working on your Deep Learning projects that
    leverage multiple GPUs 🥳.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 完成！你已经准备好开始使用多 GPU 进行深度学习项目了 🥳。
- en: What Next? Get started with Deep Learning Projects that leverage your Multi-GPU
    setup (LLMs)
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下一步？开始进行利用你的多 GPU 设置的深度学习项目（LLM）。
- en: '1\. 🤗 To get started, you can clone a popular model from **Hugging Face**:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 🤗 开始时，你可以从**Hugging Face**克隆一个流行的模型：
- en: '[](https://huggingface.co/meta-llama/Meta-Llama-3-8B?source=post_page-----df561a2d3328--------------------------------)
    [## meta-llama/Meta-Llama-3-8B · Hugging Face'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://huggingface.co/meta-llama/Meta-Llama-3-8B?source=post_page-----df561a2d3328--------------------------------)
    [## meta-llama/Meta-Llama-3-8B · Hugging Face'
- en: We're on a journey to advance and democratize artificial intelligence through
    open source and open science.
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们的目标是通过开源和开放科学推动人工智能的发展和普及。
- en: huggingface.co](https://huggingface.co/meta-llama/Meta-Llama-3-8B?source=post_page-----df561a2d3328--------------------------------)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[huggingface.co](https://huggingface.co/meta-llama/Meta-Llama-3-8B?source=post_page-----df561a2d3328--------------------------------)'
- en: '2\. 💬 For inference (using LLM models), clone and install **exllamav2** in
    a separate environment. This uses all your GPUs for faster inference: (Check my
    medium page for a detailed tutorial)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 💬 对于推理（使用 LLM 模型），请在一个单独的环境中克隆并安装**exllamav2**。这将使用你所有的 GPU 来加速推理：（查看我的
    Medium 页面获取详细教程）
- en: '[](https://github.com/turboderp/exllamav2?source=post_page-----df561a2d3328--------------------------------)
    [## GitHub - turboderp/exllamav2: A fast inference library for running LLMs locally
    on modern…'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/turboderp/exllamav2?source=post_page-----df561a2d3328--------------------------------)
    [## GitHub - turboderp/exllamav2: A fast inference library for running LLMs locally
    on modern…'
- en: A fast inference library for running LLMs locally on modern consumer-class GPUs
    - turboderp/exllamav2
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一款适用于现代消费级 GPU 上运行 LLM 的快速推理库 - turboderp/exllamav2
- en: github.com](https://github.com/turboderp/exllamav2?source=post_page-----df561a2d3328--------------------------------)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/turboderp/exllamav2?source=post_page-----df561a2d3328--------------------------------)'
- en: '3\. 👨‍🏫 For fine-tuning or training, you can clone and install **torchtune**.
    Follow the instructions to either `full finetune` or `lora finetune` your models,
    leveraging all your GPUs: (Check my medium page for a detailed tutorial)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 👨‍🏫 对于微调或训练，你可以克隆并安装**torchtune**。按照说明进行 `full finetune` 或 `lora finetune`，并利用你所有的
    GPU：（查看我的 Medium 页面获取详细教程）
- en: '[## GitHub - pytorch/torchtune: A Native-PyTorch Library for LLM Fine-tuning'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[## GitHub - pytorch/torchtune: A Native-PyTorch Library for LLM Fine-tuning'
- en: A Native-PyTorch Library for LLM Fine-tuning. Contribute to pytorch/torchtune
    development by creating an account on…
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个用于 LLM 微调的原生 PyTorch 库。通过在…创建账户参与 pytorch/torchtune 的开发
- en: github.com](https://github.com/pytorch/torchtune?source=post_page-----df561a2d3328--------------------------------)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/pytorch/torchtune?source=post_page-----df561a2d3328--------------------------------)'
- en: Conclusion
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: This guide walks you through the machine setup needed for multi-GPU deep learning.
    You can now start working on any project that leverages multiple GPUs - like torchtune
    for faster development!
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 本指南将带你完成多 GPU 深度学习所需的机器设置。现在，你可以开始进行任何使用多 GPU 的项目——例如 torchtune 来加速开发！
- en: '**Stay tuned** for more detailed tutorials on **exllamaV2** and **torchtune**.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**敬请关注**有关**exllamaV2**和**torchtune**的更多详细教程。'
