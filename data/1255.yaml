- en: How to Set Up a Multi-GPU Linux Machine for Deep Learning in 2024
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•åœ¨2024å¹´è®¾ç½®ä¸€ä¸ªç”¨äºæ·±åº¦å­¦ä¹ çš„å¤šGPU Linuxæœºå™¨
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/how-to-setup-a-multi-gpu-linux-machine-for-deep-learning-in-2024-df561a2d3328?source=collection_archive---------0-----------------------#2024-05-19](https://towardsdatascience.com/how-to-setup-a-multi-gpu-linux-machine-for-deep-learning-in-2024-df561a2d3328?source=collection_archive---------0-----------------------#2024-05-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/how-to-setup-a-multi-gpu-linux-machine-for-deep-learning-in-2024-df561a2d3328?source=collection_archive---------0-----------------------#2024-05-19](https://towardsdatascience.com/how-to-setup-a-multi-gpu-linux-machine-for-deep-learning-in-2024-df561a2d3328?source=collection_archive---------0-----------------------#2024-05-19)
- en: DEEP LEARNING WITH MULTIPLE GPUS
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å¤šä¸ªGPUè¿›è¡Œæ·±åº¦å­¦ä¹ 
- en: Super-fast setup of CUDA and PyTorch in minutes!
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨å‡ åˆ†é’Ÿå†…å¿«é€Ÿè®¾ç½®CUDAå’ŒPyTorchï¼
- en: '[](https://medium.com/@nirajkamal?source=post_page---byline--df561a2d3328--------------------------------)[![Nika](../Images/fcf9dfec64ccae5ea841fcc5046817d6.png)](https://medium.com/@nirajkamal?source=post_page---byline--df561a2d3328--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--df561a2d3328--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--df561a2d3328--------------------------------)
    [Nika](https://medium.com/@nirajkamal?source=post_page---byline--df561a2d3328--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@nirajkamal?source=post_page---byline--df561a2d3328--------------------------------)[![Nika](../Images/fcf9dfec64ccae5ea841fcc5046817d6.png)](https://medium.com/@nirajkamal?source=post_page---byline--df561a2d3328--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--df561a2d3328--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--df561a2d3328--------------------------------)
    [Nika](https://medium.com/@nirajkamal?source=post_page---byline--df561a2d3328--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--df561a2d3328--------------------------------)
    Â·6 min readÂ·May 19, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--df561a2d3328--------------------------------)
    Â·6åˆ†é’Ÿé˜…è¯»Â·2024å¹´5æœˆ19æ—¥
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/3cef01ab529215b4b49adeed49721c78.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3cef01ab529215b4b49adeed49721c78.png)'
- en: 'Image by Author: Multi-GPU machine (cartoon)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒï¼šå¤šGPUæœºå™¨ï¼ˆå¡é€šå›¾ï¼‰
- en: As Deep Learning models (especially LLMs) keep getting bigger, the need for
    more GPU memory (VRAM) is ever-increasing for developing them and using them locally.
    Building or obtaining a multi-GPU machine is just the first part of the challenge.
    Most libraries and applications only use a single GPU by default. Thus, the machine
    also needs to have appropriate drivers along with libraries that can leverage
    the multi-GPU setup.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆå°¤å…¶æ˜¯LLMï¼‰ä¸æ–­å˜å¾—æ›´å¤§ï¼Œå¼€å‘å’Œæœ¬åœ°ä½¿ç”¨è¿™äº›æ¨¡å‹å¯¹GPUå†…å­˜ï¼ˆVRAMï¼‰çš„éœ€æ±‚æ—¥ç›Šå¢åŠ ã€‚æ„å»ºæˆ–è·å¾—ä¸€å°å¤šGPUæœºå™¨ä»…ä»…æ˜¯æŒ‘æˆ˜çš„ç¬¬ä¸€éƒ¨åˆ†ã€‚å¤§å¤šæ•°åº“å’Œåº”ç”¨ç¨‹åºé»˜è®¤åªä½¿ç”¨å•ä¸ªGPUã€‚å› æ­¤ï¼Œæœºå™¨è¿˜éœ€è¦é…å¤‡é€‚å½“çš„é©±åŠ¨ç¨‹åºä»¥åŠèƒ½å¤Ÿåˆ©ç”¨å¤šGPUè®¾ç½®çš„åº“ã€‚
- en: This story provides a guide on how to set up a multi-GPU (Nvidia) Linux machine
    with important libraries. This will hopefully save you some time on experimentation
    and get you started on your development.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡æä¾›äº†ä¸€ä¸ªå¦‚ä½•è®¾ç½®å¤šGPUï¼ˆNvidiaï¼‰Linuxæœºå™¨å¹¶å®‰è£…é‡è¦åº“çš„æŒ‡å—ã€‚å¸Œæœ›èƒ½å¤ŸèŠ‚çœä½ åœ¨å®éªŒä¸­çš„æ—¶é—´ï¼Œå¸®åŠ©ä½ æ›´å¿«å¼€å§‹å¼€å‘ã€‚
- en: At the end, links are provided to popular open-source libraries that can leverage
    the multi-GPU setup for Deep Learning.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæä¾›äº†å¯ä»¥åˆ©ç”¨å¤šGPUè®¾ç½®è¿›è¡Œæ·±åº¦å­¦ä¹ çš„æµè¡Œå¼€æºåº“çš„é“¾æ¥ã€‚
- en: Target
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç›®æ ‡
- en: Set up a Multi-GPU Linux system with necessary libraries such as CUDA Toolkit
    and PyTorch to get started with Deep Learning *ğŸ¤–*. The same steps also apply to
    a single GPU machine.
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è®¾ç½®ä¸€ä¸ªå¤šGPUçš„Linuxç³»ç»Ÿï¼Œå¹¶å®‰è£…å¿…è¦çš„åº“ï¼Œå¦‚CUDAå·¥å…·åŒ…å’ŒPyTorchï¼Œä»¥å¼€å§‹è¿›è¡Œæ·±åº¦å­¦ä¹ *ğŸ¤–*ã€‚ç›¸åŒçš„æ­¥éª¤ä¹Ÿé€‚ç”¨äºå•GPUæœºå™¨ã€‚
- en: We will install 1) CUDA Toolkit, 2) PyTorch and 3) Miniconda to get started
    with Deep Learning using frameworks such as exllamaV2 and torchtune.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å®‰è£…1ï¼‰CUDAå·¥å…·åŒ…ï¼Œ2ï¼‰PyTorchå’Œ3ï¼‰Minicondaï¼Œå¼€å§‹ä½¿ç”¨exllamaV2å’Œtorchtuneç­‰æ¡†æ¶è¿›è¡Œæ·±åº¦å­¦ä¹ ã€‚
- en: Â©ï¸ All the libraries and information mentioned in this story are open-source
    and/or publicly available.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Â©ï¸ æœ¬æ–‡ä¸­æåˆ°çš„æ‰€æœ‰åº“å’Œä¿¡æ¯å‡ä¸ºå¼€æºå’Œ/æˆ–å…¬å¼€å¯ç”¨ã€‚
- en: Getting Started
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…¥é—¨
- en: '![](../Images/cf98219698c22c0bc7ba55518467eb1f.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf98219698c22c0bc7ba55518467eb1f.png)'
- en: 'Image by Author: Output of the nvidia-smi command on a Linux Machine with 8
    Nvidia A10G GPUs'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒï¼šåœ¨é…å¤‡8ä¸ªNvidia A10G GPUçš„Linuxæœºå™¨ä¸Šè¿è¡Œnvidia-smiå‘½ä»¤çš„è¾“å‡º
- en: Check the number of GPUs installed in the machine using the `nvidia-smi` command
    in the terminal. It should print a list of all the installed GPUs. If there is
    a discrepancy or if the command does not work, first install the Nvidia drivers
    for your version of Linux. Make sure the `nvidia-smi` command prints a list of
    all the GPUs installed in your machine as shown above.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ç»ˆç«¯ä¸­çš„ `nvidia-smi` å‘½ä»¤æ£€æŸ¥æœºå™¨ä¸­å®‰è£…çš„ GPU æ•°é‡ã€‚å®ƒåº”è¯¥æ‰“å°å‡ºæ‰€æœ‰å·²å®‰è£…çš„ GPU åˆ—è¡¨ã€‚å¦‚æœæœ‰ä»»ä½•ä¸ä¸€è‡´ï¼Œæˆ–è€…å‘½ä»¤æ— æ³•å·¥ä½œï¼Œè¯·é¦–å…ˆä¸ºä½ çš„
    Linux ç‰ˆæœ¬å®‰è£… Nvidia é©±åŠ¨ç¨‹åºã€‚ç¡®ä¿ `nvidia-smi` å‘½ä»¤èƒ½æ­£ç¡®æ‰“å°å‡ºæ‰€æœ‰å·²å®‰è£…çš„ GPUï¼Œå¦‚ä¸Šæ‰€ç¤ºã€‚
- en: 'Follow this page to install Nvidia Drivers if not done already:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå°šæœªå®‰è£… Nvidia é©±åŠ¨ç¨‹åºï¼Œè¯·æŒ‰ç…§æ­¤é¡µé¢å®‰è£…ï¼š
- en: '[How to install the NVIDIA drivers on Ubuntu 22.04 â€” Linux Tutorials â€” Learn
    Linux Configuration](https://linuxconfig.org/how-to-install-the-nvidia-drivers-on-ubuntu-22-04)-
    (Source: linuxconfig.org)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[å¦‚ä½•åœ¨ Ubuntu 22.04 ä¸Šå®‰è£… NVIDIA é©±åŠ¨ç¨‹åº â€” Linux æ•™ç¨‹ â€” å­¦ä¹  Linux é…ç½®](https://linuxconfig.org/how-to-install-the-nvidia-drivers-on-ubuntu-22-04)ï¼ˆæ¥æºï¼šlinuxconfig.orgï¼‰'
- en: Step-1 Install CUDA-Toolkit
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬ 1 æ­¥ å®‰è£… CUDA å·¥å…·åŒ…
- en: ğŸ’¡ *Check for any existing CUDA folder at* `*usr/local/cuda-xx*`*. That means
    a version of CUDA is already installed. If you already have the desired CUDA toolkit
    installed (check with the* `*nvcc*` *command in your terminal) please skip to
    Step-2.*
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ *æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç°æœ‰çš„ CUDA æ–‡ä»¶å¤¹* `*usr/local/cuda-xx*`*ã€‚è¿™æ„å‘³ç€å·²ç»å®‰è£…äº†ä¸€ä¸ªç‰ˆæœ¬çš„ CUDAã€‚å¦‚æœä½ å·²ç»å®‰è£…äº†æ‰€éœ€çš„
    CUDA å·¥å…·åŒ…ï¼ˆé€šè¿‡åœ¨ç»ˆç«¯ä¸­ä½¿ç”¨* `*nvcc*` *å‘½ä»¤æ£€æŸ¥ï¼‰ï¼Œè¯·è·³åˆ°ç¬¬ 2 æ­¥ã€‚*
- en: 'Check the CUDA version needed for your desired PyTorch library: [Start Locally
    | PyTorch](https://pytorch.org/get-started/locally/) (We are installing Install
    CUDA 12.1)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€æŸ¥ä½ æ‰€éœ€çš„ PyTorch åº“æ‰€éœ€çš„ CUDA ç‰ˆæœ¬ï¼š[ä»æœ¬åœ°å¼€å§‹ | PyTorch](https://pytorch.org/get-started/locally/)ï¼ˆæˆ‘ä»¬æ­£åœ¨å®‰è£…
    CUDA 12.1ï¼‰
- en: Go to [CUDA Toolkit 12.1 Downloads | NVIDIA Developer](https://developer.nvidia.com/cuda-12-1-0-download-archive)
    to obtain Linux commands to install CUDA 12.1 (choose your OS version and the
    corresponding â€œdeb (local)â€ installer type).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å‰å¾€ [CUDA Toolkit 12.1 ä¸‹è½½ | NVIDIA å¼€å‘è€…](https://developer.nvidia.com/cuda-12-1-0-download-archive)
    è·å– Linux å®‰è£… CUDA 12.1 çš„å‘½ä»¤ï¼ˆé€‰æ‹©ä½ çš„æ“ä½œç³»ç»Ÿç‰ˆæœ¬å’Œç›¸åº”çš„â€œdebï¼ˆæœ¬åœ°ï¼‰â€å®‰è£…ç±»å‹ï¼‰ã€‚
- en: '![](../Images/29309c4402baba465c39b23a0bc89e15.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29309c4402baba465c39b23a0bc89e15.png)'
- en: 'Options selected for Ubuntu 22 (Source: developer.nvidia.com)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸º Ubuntu 22 é€‰æ‹©çš„é€‰é¡¹ï¼ˆæ¥æºï¼šdeveloper.nvidia.comï¼‰
- en: 'The terminal commands for the base installer will appear according to your
    chosen options. Copy-paste and run them in your Linux terminal to install the
    CUDA toolkit. For example, for x86_64 Ubuntu 22, run the following commands by
    opening the terminal in the downloads folder:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºç¡€å®‰è£…ç¨‹åºçš„ç»ˆç«¯å‘½ä»¤å°†æ ¹æ®ä½ é€‰æ‹©çš„é€‰é¡¹å‡ºç°ã€‚å°†å®ƒä»¬å¤åˆ¶å¹¶ç²˜è´´åˆ°ä½ çš„ Linux ç»ˆç«¯ä¸­è¿è¡Œï¼Œä»¥å®‰è£… CUDA å·¥å…·åŒ…ã€‚ä¾‹å¦‚ï¼Œå¯¹äº x86_64 Ubuntu
    22ï¼Œæ‰“å¼€ä¸‹è½½æ–‡ä»¶å¤¹ä¸­çš„ç»ˆç«¯å¹¶è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: âš ï¸*While installing the CUDA toolkit, the installer may prompt a kernel update.
    If any pop-up appears in the terminal to update the kernel, press the* `*esc*`
    *button to cancel it. Do not update the kernel during this stage!â€” it may break
    your Nvidia drivers* â˜ ï¸.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: âš ï¸*åœ¨å®‰è£… CUDA å·¥å…·åŒ…æ—¶ï¼Œå®‰è£…ç¨‹åºå¯èƒ½ä¼šæç¤ºæ›´æ–°å†…æ ¸ã€‚å¦‚æœç»ˆç«¯ä¸­å‡ºç°ä»»ä½•æç¤ºæ›´æ–°å†…æ ¸çš„å¼¹çª—ï¼ŒæŒ‰* `*esc*` *æŒ‰é’®å–æ¶ˆã€‚æ­¤é˜¶æ®µä¸è¦æ›´æ–°å†…æ ¸ï¼â€”â€”è¿™æ ·å¯èƒ½ä¼šç ´åä½ çš„
    Nvidia é©±åŠ¨ç¨‹åº* â˜ ï¸ã€‚
- en: Restart the Linux machine after the installation. The `nvcc` command will still
    not work. You need to add the CUDA installation to PATH. Open the `.bashrc` file
    using the nano editor.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å®‰è£…å®Œæˆåï¼Œè¯·é‡æ–°å¯åŠ¨ Linux æœºå™¨ã€‚`nvcc` å‘½ä»¤ä»ç„¶æ— æ³•ä½¿ç”¨ã€‚ä½ éœ€è¦å°† CUDA å®‰è£…è·¯å¾„æ·»åŠ åˆ° PATH ä¸­ã€‚ä½¿ç”¨ nano ç¼–è¾‘å™¨æ‰“å¼€
    `.bashrc` æ–‡ä»¶ã€‚
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Scroll to the bottom of the `.bashrc` file and add these two lines:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ»šåŠ¨åˆ° `.bashrc` æ–‡ä»¶çš„åº•éƒ¨ï¼Œå¹¶æ·»åŠ ä»¥ä¸‹ä¸¤è¡Œï¼š
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ğŸ’¡ *Note that you can change* `*cuda-12.1*` *to your installed CUDA version,*
    `*cuda-xx*` *if needed in the future , â€˜xxâ€™ being your CUDA version.*
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ *è¯·æ³¨æ„ï¼Œä½ å¯ä»¥å°†* `*cuda-12.1*` *æ›´æ”¹ä¸ºä½ å®‰è£…çš„ CUDA ç‰ˆæœ¬ï¼Œ* `*cuda-xx*` *ï¼Œå¦‚æœå°†æ¥éœ€è¦ï¼Œâ€˜xxâ€™ è¡¨ç¤ºä½ çš„
    CUDA ç‰ˆæœ¬ã€‚*
- en: 'Save the changes and close the nano editor:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä¿å­˜æ›´æ”¹å¹¶å…³é—­ nano ç¼–è¾‘å™¨ï¼š
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Close and reopen the terminal. Now the `nvcc--version` command should print
    the installed CUDA version in your terminal.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å…³é—­å¹¶é‡æ–°æ‰“å¼€ç»ˆç«¯ã€‚ç°åœ¨ï¼Œ`nvcc--version` å‘½ä»¤åº”è¯¥åœ¨ç»ˆç«¯ä¸­æ‰“å°å·²å®‰è£…çš„ CUDA ç‰ˆæœ¬ã€‚
- en: Step-2 Install Miniconda
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬ 2 æ­¥ å®‰è£… Miniconda
- en: Before we install PyTorch, it is better to install Miniconda and then install
    PyTorch inside a Conda environment. It also is handy to create a new Conda environment
    for each project.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®‰è£… PyTorch ä¹‹å‰ï¼Œæœ€å¥½å…ˆå®‰è£… Minicondaï¼Œç„¶ååœ¨ Conda ç¯å¢ƒä¸­å®‰è£… PyTorchã€‚ä¸ºæ¯ä¸ªé¡¹ç›®åˆ›å»ºä¸€ä¸ªæ–°çš„ Conda ç¯å¢ƒä¹Ÿæ˜¯å¾ˆæ–¹ä¾¿çš„ã€‚
- en: 'Open the terminal in the Downloads folder and run the following commands:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰“å¼€ä¸‹è½½æ–‡ä»¶å¤¹ä¸­çš„ç»ˆç«¯ï¼Œå¹¶è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Close and re-open the terminal. Now the `conda` command should work.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å…³é—­å¹¶é‡æ–°æ‰“å¼€ç»ˆç«¯ã€‚ç°åœ¨ï¼Œ`conda` å‘½ä»¤åº”è¯¥å¯ä»¥æ­£å¸¸å·¥ä½œã€‚
- en: Step-3 Install PyTorch
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬ 3 æ­¥ å®‰è£… PyTorch
- en: (Optional) â€” Create a new conda environment for your project. You can replace
    `<environment-name>` with the name of your choice. I usually name it after my
    project name.ğŸ’¡ *You can use the* `*conda activate <environment-name>*` *and* `*conda
    deactivate <environment-name>*` *commands before and after working on your project.*
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ï¼ˆå¯é€‰ï¼‰â€” ä¸ºä½ çš„é¡¹ç›®åˆ›å»ºä¸€ä¸ªæ–°çš„ conda ç¯å¢ƒã€‚ä½ å¯ä»¥å°† `<environment-name>` æ›¿æ¢ä¸ºä½ å–œæ¬¢çš„åç§°ã€‚æˆ‘é€šå¸¸ä½¿ç”¨é¡¹ç›®åç§°æ¥å‘½åã€‚ğŸ’¡
    *ä½ å¯ä»¥åœ¨å·¥ä½œå‰åä½¿ç”¨* `*conda activate <environment-name>*` *å’Œ* `*conda deactivate <environment-name>*`
    *å‘½ä»¤ã€‚*
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Install the PyTorch library for your CUDA version. The following commands are
    for cuda-12.1 which we installed:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä½ çš„ CUDA ç‰ˆæœ¬å®‰è£… PyTorch åº“ã€‚ä»¥ä¸‹å‘½ä»¤é€‚ç”¨äºæˆ‘ä»¬å®‰è£…çš„ cuda-12.1 ç‰ˆæœ¬ï¼š
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The above command is obtained from PyTorch installation guide â€” [Start Locally
    | PyTorch](https://pytorch.org/get-started/locally/) .
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°å‘½ä»¤æ¥æºäº PyTorch å®‰è£…æŒ‡å— â€” [Start Locally | PyTorch](https://pytorch.org/get-started/locally/)ã€‚
- en: '![](../Images/2cb51751b02a7e251fb9eb15d57fab1e.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2cb51751b02a7e251fb9eb15d57fab1e.png)'
- en: '(Source: pytorch.org)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ï¼ˆæ¥æºï¼špytorch.orgï¼‰
- en: After PyTorch installation, check the number of GPUs visible to PyTorch in the
    terminal.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å®‰è£… PyTorch åï¼Œæ£€æŸ¥åœ¨ç»ˆç«¯ä¸­ PyTorch å¯è§çš„ GPU æ•°é‡ã€‚
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This should print the number of GPUs installed in the system (8 in my case),
    and should also match the number of listed GPUs in the `nvidia-smi` command.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åº”è¯¥æ‰“å°å‡ºç³»ç»Ÿä¸­å®‰è£…çš„ GPU æ•°é‡ï¼ˆåœ¨æˆ‘çš„æ¡ˆä¾‹ä¸­æ˜¯ 8 ä¸ªï¼‰ï¼Œå¹¶ä¸”åº”ä¸ `nvidia-smi` å‘½ä»¤ä¸­åˆ—å‡ºçš„ GPU æ•°é‡ä¸€è‡´ã€‚
- en: Viola! you are all set to start working on your Deep Learning projects that
    leverage multiple GPUs ğŸ¥³.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæˆï¼ä½ å·²ç»å‡†å¤‡å¥½å¼€å§‹ä½¿ç”¨å¤š GPU è¿›è¡Œæ·±åº¦å­¦ä¹ é¡¹ç›®äº† ğŸ¥³ã€‚
- en: What Next? Get started with Deep Learning Projects that leverage your Multi-GPU
    setup (LLMs)
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥ï¼Ÿå¼€å§‹è¿›è¡Œåˆ©ç”¨ä½ çš„å¤š GPU è®¾ç½®çš„æ·±åº¦å­¦ä¹ é¡¹ç›®ï¼ˆLLMï¼‰ã€‚
- en: '1\. ğŸ¤— To get started, you can clone a popular model from **Hugging Face**:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. ğŸ¤— å¼€å§‹æ—¶ï¼Œä½ å¯ä»¥ä»**Hugging Face**å…‹éš†ä¸€ä¸ªæµè¡Œçš„æ¨¡å‹ï¼š
- en: '[](https://huggingface.co/meta-llama/Meta-Llama-3-8B?source=post_page-----df561a2d3328--------------------------------)
    [## meta-llama/Meta-Llama-3-8B Â· Hugging Face'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://huggingface.co/meta-llama/Meta-Llama-3-8B?source=post_page-----df561a2d3328--------------------------------)
    [## meta-llama/Meta-Llama-3-8B Â· Hugging Face'
- en: We're on a journey to advance and democratize artificial intelligence through
    open source and open science.
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡å¼€æºå’Œå¼€æ”¾ç§‘å­¦æ¨åŠ¨äººå·¥æ™ºèƒ½çš„å‘å±•å’Œæ™®åŠã€‚
- en: huggingface.co](https://huggingface.co/meta-llama/Meta-Llama-3-8B?source=post_page-----df561a2d3328--------------------------------)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[huggingface.co](https://huggingface.co/meta-llama/Meta-Llama-3-8B?source=post_page-----df561a2d3328--------------------------------)'
- en: '2\. ğŸ’¬ For inference (using LLM models), clone and install **exllamav2** in
    a separate environment. This uses all your GPUs for faster inference: (Check my
    medium page for a detailed tutorial)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. ğŸ’¬ å¯¹äºæ¨ç†ï¼ˆä½¿ç”¨ LLM æ¨¡å‹ï¼‰ï¼Œè¯·åœ¨ä¸€ä¸ªå•ç‹¬çš„ç¯å¢ƒä¸­å…‹éš†å¹¶å®‰è£…**exllamav2**ã€‚è¿™å°†ä½¿ç”¨ä½ æ‰€æœ‰çš„ GPU æ¥åŠ é€Ÿæ¨ç†ï¼šï¼ˆæŸ¥çœ‹æˆ‘çš„
    Medium é¡µé¢è·å–è¯¦ç»†æ•™ç¨‹ï¼‰
- en: '[](https://github.com/turboderp/exllamav2?source=post_page-----df561a2d3328--------------------------------)
    [## GitHub - turboderp/exllamav2: A fast inference library for running LLMs locally
    on modernâ€¦'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/turboderp/exllamav2?source=post_page-----df561a2d3328--------------------------------)
    [## GitHub - turboderp/exllamav2: A fast inference library for running LLMs locally
    on modernâ€¦'
- en: A fast inference library for running LLMs locally on modern consumer-class GPUs
    - turboderp/exllamav2
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¸€æ¬¾é€‚ç”¨äºç°ä»£æ¶ˆè´¹çº§ GPU ä¸Šè¿è¡Œ LLM çš„å¿«é€Ÿæ¨ç†åº“ - turboderp/exllamav2
- en: github.com](https://github.com/turboderp/exllamav2?source=post_page-----df561a2d3328--------------------------------)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/turboderp/exllamav2?source=post_page-----df561a2d3328--------------------------------)'
- en: '3\. ğŸ‘¨â€ğŸ« For fine-tuning or training, you can clone and install **torchtune**.
    Follow the instructions to either `full finetune` or `lora finetune` your models,
    leveraging all your GPUs: (Check my medium page for a detailed tutorial)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. ğŸ‘¨â€ğŸ« å¯¹äºå¾®è°ƒæˆ–è®­ç»ƒï¼Œä½ å¯ä»¥å…‹éš†å¹¶å®‰è£…**torchtune**ã€‚æŒ‰ç…§è¯´æ˜è¿›è¡Œ `full finetune` æˆ– `lora finetune`ï¼Œå¹¶åˆ©ç”¨ä½ æ‰€æœ‰çš„
    GPUï¼šï¼ˆæŸ¥çœ‹æˆ‘çš„ Medium é¡µé¢è·å–è¯¦ç»†æ•™ç¨‹ï¼‰
- en: '[## GitHub - pytorch/torchtune: A Native-PyTorch Library for LLM Fine-tuning'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[## GitHub - pytorch/torchtune: A Native-PyTorch Library for LLM Fine-tuning'
- en: A Native-PyTorch Library for LLM Fine-tuning. Contribute to pytorch/torchtune
    development by creating an account onâ€¦
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªç”¨äº LLM å¾®è°ƒçš„åŸç”Ÿ PyTorch åº“ã€‚é€šè¿‡åœ¨â€¦åˆ›å»ºè´¦æˆ·å‚ä¸ pytorch/torchtune çš„å¼€å‘
- en: github.com](https://github.com/pytorch/torchtune?source=post_page-----df561a2d3328--------------------------------)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/pytorch/torchtune?source=post_page-----df561a2d3328--------------------------------)'
- en: Conclusion
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: This guide walks you through the machine setup needed for multi-GPU deep learning.
    You can now start working on any project that leverages multiple GPUs - like torchtune
    for faster development!
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—å°†å¸¦ä½ å®Œæˆå¤š GPU æ·±åº¦å­¦ä¹ æ‰€éœ€çš„æœºå™¨è®¾ç½®ã€‚ç°åœ¨ï¼Œä½ å¯ä»¥å¼€å§‹è¿›è¡Œä»»ä½•ä½¿ç”¨å¤š GPU çš„é¡¹ç›®â€”â€”ä¾‹å¦‚ torchtune æ¥åŠ é€Ÿå¼€å‘ï¼
- en: '**Stay tuned** for more detailed tutorials on **exllamaV2** and **torchtune**.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ•¬è¯·å…³æ³¨**æœ‰å…³**exllamaV2**å’Œ**torchtune**çš„æ›´å¤šè¯¦ç»†æ•™ç¨‹ã€‚'
