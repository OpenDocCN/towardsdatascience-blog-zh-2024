- en: 'Maximizing the Utility of Scarce AI Resources: A Kubernetes Approach'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大化稀缺AI资源的利用：一种Kubernetes方法
- en: 原文：[https://towardsdatascience.com/maximizing-the-utility-of-scarce-ai-resources-a-kubernetes-approach-0230ba53965b?source=collection_archive---------12-----------------------#2024-02-13](https://towardsdatascience.com/maximizing-the-utility-of-scarce-ai-resources-a-kubernetes-approach-0230ba53965b?source=collection_archive---------12-----------------------#2024-02-13)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/maximizing-the-utility-of-scarce-ai-resources-a-kubernetes-approach-0230ba53965b?source=collection_archive---------12-----------------------#2024-02-13](https://towardsdatascience.com/maximizing-the-utility-of-scarce-ai-resources-a-kubernetes-approach-0230ba53965b?source=collection_archive---------12-----------------------#2024-02-13)
- en: Optimizing the use of limited AI training accelerators
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化有限AI训练加速器的使用
- en: '[](https://chaimrand.medium.com/?source=post_page---byline--0230ba53965b--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page---byline--0230ba53965b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0230ba53965b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0230ba53965b--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page---byline--0230ba53965b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://chaimrand.medium.com/?source=post_page---byline--0230ba53965b--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page---byline--0230ba53965b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0230ba53965b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0230ba53965b--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page---byline--0230ba53965b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0230ba53965b--------------------------------)
    ·13 min read·Feb 13, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0230ba53965b--------------------------------)
    ·13分钟阅读·2024年2月13日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/cffd5cd8990a848557b12e0d977c3003.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cffd5cd8990a848557b12e0d977c3003.png)'
- en: Photo by [Roman Derrick Okello](https://unsplash.com/@okello_1?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Roman Derrick Okello](https://unsplash.com/@okello_1?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In the ever-evolving landscape of AI development, nothing rings truer than the
    old saying (attributed to Heraclitus), “the only constant in life is change”.
    In the case of AI, it seems that *change* is indeed constant, but the *pace of
    change* is forever increasing. Staying relevant in these unique and exciting times
    amounts to an unprecedented test of the capacity of AI teams to consistently adapt
    and adjust their development processes. AI development teams that fail to adapt,
    or are slow to adapt, may quickly become obsolete.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '在AI开发不断变化的环境中，没有什么比古老的格言（归功于赫拉克利特）更能体现现实：“生活中唯一不变的就是变化”。对于AI而言，似乎*变化*确实是常态，而*变化的速度*却在不断加快。在这个独特而激动人心的时代，要保持相关性就意味着AI团队必须前所未有地考验其不断适应和调整开发流程的能力。那些未能适应或适应迟缓的AI开发团队，很可能会迅速变得过时。 '
- en: One of the most challenging developments of the past few years in AI development
    has been the increasing difficulty to attain the hardware required to train AI
    models. Whether it be due to an [ongoing crisis in the global supply chain](https://en.wikipedia.org/wiki/2021%E2%80%932023_global_supply_chain_crisis)
    or a significant increase in the demand for AI chips, getting your hands on the
    GPUs (or alternative training accelerators) that you need for AI development,
    has gotten much harder. This is evidenced by the huge [wait time](https://www.nextplatform.com/2023/11/17/what-to-do-when-you-cant-get-nvidia-h100-gpus/)
    for new GPU orders and by the fact that cloud service providers (CSPs) that once
    offered virtually infinite capacity of GPU machines, now struggle to keep up with
    the demand.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，AI开发面临的最具挑战性的进展之一是获取训练AI模型所需硬件的难度不断增加。无论是由于[全球供应链持续危机](https://en.wikipedia.org/wiki/2021%E2%80%932023_global_supply_chain_crisis)，还是AI芯片需求的显著增加，获得所需的GPU（或替代训练加速器）变得更加困难。新GPU订单的巨大[等待时间](https://www.nextplatform.com/2023/11/17/what-to-do-when-you-cant-get-nvidia-h100-gpus/)以及曾经提供几乎无限GPU计算能力的云服务提供商（CSP）现在也难以满足需求，均证明了这一点。
- en: The changing times are forcing AI development teams that may have once relied
    on endless capacity of AI accelerators to adapt to a world with reduced accessibility
    and, in some cases, higher costs. Development processes that once took for granted
    the ability to spin up a new GPU machine at will, must be modified to meet the
    demands of a world of scarce AI resources that are often shared by multiple projects
    and/or teams. Those that fail to adapt risk annihilation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 时代的变化迫使那些曾依赖无限容量AI加速器的AI开发团队，适应一个减少可访问性、在某些情况下成本更高的世界。曾经理所当然的能够随意启动新的GPU机器的开发过程，现在必须做出调整，以应对一个AI资源稀缺、且通常由多个项目和/或团队共享的世界。那些未能适应的人，将面临被淘汰的风险。
- en: In this post we will demonstrate the use of [Kubernetes](https://kubernetes.io/)
    in the orchestration of AI-model training workloads in a world of scarce AI resources.
    We will start by specifying the goals we wish to achieve. We will then describe
    why Kubernetes is an appropriate tool for addressing this challenge. Last, we
    will provide a simple demonstration of how Kubernetes can be used to maximize
    the use of a scarce AI compute resource. In subsequent posts, we plan to enhance
    the Kubernetes-based solution and show how to apply it to a cloud-based training
    environment.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将展示如何在AI资源稀缺的世界中，使用[Kubernetes](https://kubernetes.io/)来协调AI模型训练工作负载。我们将从指定我们希望实现的目标开始，然后描述为什么Kubernetes是解决这个挑战的合适工具。最后，我们将提供一个简单的演示，展示如何使用Kubernetes最大化稀缺的AI计算资源的利用。在随后的文章中，我们计划进一步完善基于Kubernetes的解决方案，并展示如何将其应用于基于云的训练环境。
- en: Disclaimers
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 免责声明
- en: While this post does not assume prior experience with Kubernetes, some basic
    familiarity would certainly be helpful. This post should not, in any way, be viewed
    as a Kubernetes tutorial. To learn about Kubernetes, we refer the reader to the
    many great online resources on the subject. Here we will discuss just a few properties
    of Kubernetes as they pertain to the topic of maximizing and prioritizing resource
    utilization.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本文并不假设读者具有Kubernetes的先前经验，但一些基本的了解肯定会有所帮助。本文不应被视为Kubernetes教程。要学习Kubernetes，我们建议读者参考许多关于此主题的优秀在线资源。这里，我们仅讨论Kubernetes在最大化和优先考虑资源利用方面的一些特性。
- en: There are many alternative tools and techniques to the method we put forth here,
    each with their own pros and cons. Our intention in this post is purely educational;
    Please do not view any of the choices we make as an endorsement.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们在此提出的方法，有许多替代工具和技术，每种工具和技术都有其优缺点。我们在本文中的意图纯粹是为了教育目的；请不要将我们所做的任何选择视为推荐。
- en: Lastly, the Kubernetes platform remains under constant development, as do many
    of the frameworks and tools in the field of AI development. Please take into account
    the possibility that some of the statements, examples, and/or external links in
    this post may become outdated by the time you read this and be sure to take into
    account the most up-to-date solutions available before making your own design
    decisions.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Kubernetes平台以及AI开发领域中的许多框架和工具仍在不断发展。请注意，在您阅读本文时，某些声明、示例和/或外部链接可能已经过时，因此在做出自己的设计决策之前，请务必参考最新的解决方案。
- en: Adapting to Scarce AI Compute Resources
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 适应稀缺的AI计算资源
- en: To simplify our discussion, let’s assume that we have a single worker node at
    our disposal for training our models. This could be a local machine with a GPU
    or a [reserved](https://aws.amazon.com/ec2/pricing/reserved-instances/) compute-accelerated
    instance in the cloud, such as a [p5.48xlarge](https://aws.amazon.com/ec2/instance-types/p5/)
    instance in AWS or a [TPU node](https://cloud.google.com/tpu) in GCP. In our example
    below we will refer to this node as “my precious”. Typically, we will have spent
    a lot of money on this machine. We will further assume that we have multiple training
    workloads all competing for our single compute resource where each workload could
    take anywhere from a few minutes to a few days. Naturally, we would like to maximize
    the utility of our compute resource by ensuring that it is in constant use and
    that the most important jobs get prioritized. What we need is some form of a [priority
    queue](https://en.wikipedia.org/wiki/Priority_queue)and an associated priority-based
    [scheduling](https://en.wikipedia.org/wiki/Scheduling_(computing)) algorithm.
    Let’s try to be a bit more specific about the behaviors that we desire.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化我们的讨论，假设我们只有一个单一的工作节点用于训练模型。这个节点可以是配备GPU的本地机器，或者是云端的[预留实例](https://aws.amazon.com/ec2/pricing/reserved-instances/)，例如AWS中的[p5.48xlarge](https://aws.amazon.com/ec2/instance-types/p5/)实例，或者GCP中的[TPU节点](https://cloud.google.com/tpu)。在下面的示例中，我们将称这个节点为“我的宝贝”。通常，我们会为这台机器花费大量资金。我们还假设，我们有多个训练任务在争夺这个唯一的计算资源，每个任务可能需要从几分钟到几天的时间。自然，我们希望最大化计算资源的利用率，确保它始终处于工作状态，并优先处理最重要的任务。我们需要的是某种形式的[优先级队列](https://en.wikipedia.org/wiki/Priority_queue)以及相关的基于优先级的[调度](https://en.wikipedia.org/wiki/Scheduling_(computing))算法。让我们更具体地讨论一下我们希望达到的行为。
- en: Scheduling Requirements
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调度需求
- en: '**Maximize Utilization:** We would like for our resource to be in constant
    use. Specifically, as soon as it completes a workload, it will promptly (and automatically)
    start working on a new one.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**最大化利用率：** 我们希望我们的资源能够持续使用。具体来说，当一个任务完成后，资源会立即（并自动）开始处理下一个任务。'
- en: '**Queue Pending Workloads:** We require the existence of a *queue* of training
    workloads that are waiting to be processed by our unique resource. We also require
    associated APIs for creating and submitting new jobs to the queue, as well as
    monitoring and managing the state of the queue.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**队列中的待处理任务：** 我们需要存在一个*队列*，用于存放等待我们唯一资源处理的训练任务。我们还需要相关的API来创建和提交新的任务到队列，以及监控和管理队列的状态。'
- en: '**Support Prioritization:** We would like each training job to have an associated
    priority such that workloads with higher priority will be run before workloads
    with a lower priority.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**支持优先级：** 我们希望每个训练任务都具有关联的优先级，使得优先级高的任务会在优先级低的任务之前运行。'
- en: '**Preemption:** Moreover, in the case that an urgent job is submitted to the
    queue while our resource is working on a lower priority job, we would like for
    the running job to be preempted and replaced by the urgent job. The preempted
    job should be returned to the queue.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**抢占：** 此外，如果在资源正在处理低优先级任务时，队列中有一个紧急任务提交，我们希望能够将当前正在运行的任务抢占，并由紧急任务替代。被抢占的任务应该返回队列中。'
- en: One approach to developing a solution that satisfies these requirements could
    be to take an existing API for submitting jobs to a training resource and wrap
    it with a customized implementation of a priority queue with the desired properties.
    At a minimum, this approach would require a data structure for storing a list
    of pending jobs, a dedicated process for choosing and submitting jobs from the
    queue to the training resource, and some form of mechanism for identifying when
    a job has been completed and the resource has become available.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一种满足这些需求的解决方案可能是，使用现有的提交任务到训练资源的API，并用一个定制的优先级队列实现来包装它，确保具备所需的属性。最基本的，这种方法需要一个数据结构来存储待处理任务的列表，一个专门的进程来从队列中选择并提交任务到训练资源，以及某种机制来识别任务何时完成并且资源变得可用。
- en: An alternative approach and the one we take in this post, is to leverage an
    existing solution for priority-based [scheduling](https://en.wikipedia.org/wiki/Scheduling_(computing))
    that fulfils our requirements and align our training development workflow to its
    use. The default [scheduler](https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/)
    that comes with [Kubernetes](https://kubernetes.io/) is an example of one such
    solution. In the next sections we will demonstrate how it can be used to address
    the problem of optimizing the use of scarce AI training resources.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一种替代的方法，也是我们在本文中采用的方法，是利用现有的基于优先级的 [调度](https://en.wikipedia.org/wiki/Scheduling_(computing))
    解决方案，该解决方案能够满足我们的需求，并将我们的训练开发工作流程与其使用对齐。Kubernetes 默认的 [调度器](https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/)
    就是这样一个解决方案的例子。在接下来的部分，我们将演示如何使用它来解决优化稀缺 AI 训练资源使用的问题。
- en: ML Orchestration with Kubernetes
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Kubernetes 进行 ML 编排
- en: In this section we will get a bit philosophical about the application of Kubernetes
    to the orchestration of ML training workloads. If you have no patience for such
    discussions (totally fair) and want to get straight to the practical examples,
    please feel free to skip to the next section.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将对 Kubernetes 在 ML 训练工作负载编排中的应用进行一些哲学性的探讨。如果你对这些讨论没有耐心（完全可以理解），并且希望直接查看实际示例，请随时跳过到下一部分。
- en: Kubernetes is (another) one of those software/technological solutions that tend
    to elicit strong reactions in many developers. There are some that swear by it
    and use it extensively, and others that find it overbearing, clumsy, and unnecessary
    (e.g., see [here](https://cloudnativejourney.wordpress.com/2023/04/19/kubernetes-advantages-and-disadvantages/)
    for some of the arguments for and against using Kubernetes). As with many other
    heated debates, it is the author’s opinion that the truth lies somewhere in between
    — there are situations where Kubernetes provides an ideal framework that can significantly
    increase productivity, and other situations where its use borders on an insult
    to the SW development profession. The big question is, where on the spectrum does
    ML development lie? Is Kubernetes the appropriate framework for training ML models?
    Although a cursory online search might give the impression that the general consensus
    is an emphatic “yes”, we will make some arguments for why that may not be the
    case. But first, we need to be clear about what we mean by “ML training orchestration
    using Kubernetes”.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 是（另一个）那种在许多开发者中引发强烈反应的软件/技术解决方案。有人对它推崇备至，并广泛使用；也有人认为它压迫性强、笨重且不必要（例如，参见
    [这里](https://cloudnativejourney.wordpress.com/2023/04/19/kubernetes-advantages-and-disadvantages/)
    一些支持和反对使用 Kubernetes 的论点）。正如许多其他激烈争论一样，作者认为真相介于两者之间——在一些情况下，Kubernetes 提供了一个理想的框架，能够显著提高生产力，而在其他情况下，它的使用几乎是对软件开发职业的侮辱。一个大问题是，ML
    开发处于这个光谱的哪个位置？Kubernetes 是否是训练 ML 模型的合适框架？尽管快速的在线搜索可能给人一种普遍共识是明确的“是”，但我们将提出一些理由，说明为什么情况可能并非如此。但首先，我们需要澄清一下“使用
    Kubernetes 进行 ML 训练编排”到底是什么意思。
- en: While there are many online resources that address the topic of ML using Kubernetes,
    it is important to be aware of the fact that they are not always referring to
    the same mode of use. Some resources (e.g., [here](/distributed-deep-learning-training-with-horovod-on-kubernetes-6b28ac1d6b5d))
    use Kubernetes only for deploying a cluster; once the cluster is up and running
    they start the training job outside the context of Kubernetes. Others (e.g., [here](https://aws.amazon.com/blogs/machine-learning/introducing-amazon-sagemaker-operators-for-kubernetes/))
    use Kubernetes to define a pipeline in which a dedicated module starts up a training
    job (and associated resources) using a completely different system. In contrast
    to these two examples, many other resources define the training workload as a
    [Kubernetes Job](https://kubernetes.io/docs/concepts/workloads/controllers/job/)
    artifact that runs on a [Kubernetes Node](https://kubernetes.io/docs/concepts/architecture/nodes/).
    However, they too vary greatly in the particular attributes on which they focus.
    Some (e.g., [here](https://medium.com/pytorch/a-step-by-step-guide-to-building-a-distributed-spot-based-training-platform-on-aws-using-b54acd06ecb2))
    emphasize the auto-scaling properties and others (e.g., [here](https://blog.realvarez.com/get-more-out-of-gpus-with-nvidia-multi-instance-gpu/))
    the Multi-Instance GPU ([MIG](https://www.nvidia.com/en-eu/technologies/multi-instance-gpu/))
    support. They also vary greatly in the details of implementation, such as the
    precise artifact ([Job](https://kubernetes.io/docs/concepts/workloads/controllers/job/)
    extension) for representing a training job (e.g., [ElasticJob](https://github.com/pytorch/elastic/tree/master/kubernetes),
    [TrainingWorkload](https://docs.run.ai/v2.15/admin/workloads/workload-overview-admin/),
    [JobSet](https://github.com/kubernetes-sigs/jobset), [VolcanoJob](https://volcano.sh/en/docs/vcjob/),
    etc.). In the context of this post, we too will assume that the training workload
    is defined as a [Kubernetes Job](https://kubernetes.io/docs/concepts/workloads/controllers/job/).
    However, in order to simplify the discussion, we will stick with the core Kubernetes
    objects and leave the discussion of [Kubernetes extensions](https://kubernetes.io/docs/concepts/extend-kubernetes/)
    for ML for a future post.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有许多在线资源讨论使用Kubernetes进行机器学习（ML）的话题，但需要注意的是，它们并不总是指同一种使用方式。一些资源（例如，[这里](/distributed-deep-learning-training-with-horovod-on-kubernetes-6b28ac1d6b5d)）仅使用Kubernetes来部署集群；一旦集群启动并运行，它们便在Kubernetes之外启动训练任务。另一些资源（例如，[这里](https://aws.amazon.com/blogs/machine-learning/introducing-amazon-sagemaker-operators-for-kubernetes/)）使用Kubernetes定义一个流水线，其中一个专用模块启动一个训练任务（及其相关资源），该模块使用完全不同的系统。与这两个例子不同，许多其他资源将训练工作负载定义为[Kubernetes
    Job](https://kubernetes.io/docs/concepts/workloads/controllers/job/)工件，该工件在[Kubernetes
    Node](https://kubernetes.io/docs/concepts/architecture/nodes/)上运行。然而，它们在专注的具体属性上也存在很大差异。一些资源（例如，[这里](https://medium.com/pytorch/a-step-by-step-guide-to-building-a-distributed-spot-based-training-platform-on-aws-using-b54acd06ecb2)）强调自动扩展特性，而其他资源（例如，[这里](https://blog.realvarez.com/get-more-out-of-gpus-with-nvidia-multi-instance-gpu/)）则强调多实例GPU（[MIG](https://www.nvidia.com/en-eu/technologies/multi-instance-gpu/)）支持。它们在实现的细节上也差异很大，例如表示训练任务的精确工件（例如，[Job](https://kubernetes.io/docs/concepts/workloads/controllers/job/)扩展），例如[ElasticJob](https://github.com/pytorch/elastic/tree/master/kubernetes)，[TrainingWorkload](https://docs.run.ai/v2.15/admin/workloads/workload-overview-admin/)，[JobSet](https://github.com/kubernetes-sigs/jobset)，[VolcanoJob](https://volcano.sh/en/docs/vcjob/)等。在本文的语境下，我们也将假设训练工作负载被定义为[Kubernetes
    Job](https://kubernetes.io/docs/concepts/workloads/controllers/job/)。然而，为了简化讨论，我们将专注于核心Kubernetes对象，关于[针对ML的Kubernetes扩展](https://kubernetes.io/docs/concepts/extend-kubernetes/)的讨论将留待以后文章中详细展开。
- en: Arguments Against Kubernetes for ML
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反对Kubernetes用于机器学习的论点
- en: Here are some arguments that could be made against the use of Kubernetes for
    training ML models.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一些可能反对使用Kubernetes进行机器学习模型训练的论点。
- en: '**Complexity:** Even its greatest proponents have to admit that Kubernetes
    can be hard. Using Kubernetes effectively, requires a high level of expertise,
    has a steep learning curve, and, realistically speaking, typically requires a
    dedicated devops team. Designing a training solution based on Kubernetes increases
    dependencies on dedicated experts and by extension, increases the risk that things
    could go wrong, and that development could be delayed. Many alternative ML training
    solutions enable a greater level of developer independence and freedom and entail
    a reduced risk of bugs in the development process.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**复杂性：** 即使是最强烈的支持者也不得不承认 Kubernetes 可能很难使用。有效使用 Kubernetes 需要较高的专业知识，学习曲线陡峭，且从现实角度来看，通常需要专门的
    DevOps 团队。基于 Kubernetes 设计训练解决方案增加了对专门专家的依赖，并进一步增加了出现问题的风险，且可能导致开发延迟。许多替代的机器学习训练解决方案使开发者拥有更高的独立性和自由度，并降低了开发过程中出现漏洞的风险。'
- en: '**Fixed Resource Requirements:** One of the most touted properties of Kubernetes
    is its [scalability](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)
    — its ability to automatically and seamlessly scale its pool of compute resources
    up and down according to the number of jobs, the number of clients (in the case
    of a service application), resource capacity, etc. However, one could argue that
    in the case of an ML training workload, where the number of resources that are
    required is (usually) fixed throughout training, auto-scaling is unnecessary.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**固定资源需求：** Kubernetes 最为人称道的特性之一是其[可扩展性](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)——它能够根据作业数量、客户端数量（对于服务应用程序的情况）、资源容量等，自动、无缝地扩展和缩减其计算资源池。然而，可以说，在机器学习训练工作负载的情况下，由于所需资源的数量（通常）在训练过程中是固定的，自动扩展显得不必要。'
- en: '**Fixed Instance Type:** Due to the fact that Kubernetes orchestrates *containerized*
    applications, Kubernetes enables a great deal of flexibility when it comes to
    the types of machines in its node pool. However, when it comes to ML, we typically
    require very specific machinery with dedicated accelerators (such as GPUs). Moreover,
    our workloads are often tuned to run optimally on one very specific instance type.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**固定实例类型：** 由于 Kubernetes 管理*容器化*应用程序，Kubernetes 在其节点池中对于机器类型提供了很大的灵活性。然而，在机器学习中，我们通常需要非常特定的硬件，配备专用加速器（如
    GPU）。此外，我们的工作负载通常会调整为在某一特定实例类型上运行得最优。'
- en: '**Monolithic Application Architecture:** It is common practice in the development
    of modern-day applications to break them down into small elements called [microservices](https://en.wikipedia.org/wiki/Microservices).
    Kubernetes is often seen as a key component in this design. ML training applications
    tend to be quite monolithic in their design and, one could argue, that they do
    not lend themselves naturally to a microservice architecture.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**单体应用架构：** 在现代应用开发中，常见的做法是将应用分解为称为[微服务](https://en.wikipedia.org/wiki/Microservices)的小元素。Kubernetes
    通常被视为这种设计中的关键组件。机器学习训练应用通常在设计上是相当单体化的，可以说，它们并不自然地适合微服务架构。'
- en: '**Resource Overhead:** The dedicated processes that are required to run Kubernetes
    requires some system resources on each of the nodes in its pool. Consequently,
    it may incur a certain performance penalty on our training jobs. Given the expense
    of the resources required for training, we may prefer to avoid this.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**资源开销：** 运行 Kubernetes 所需的专用进程需要在其池中的每个节点上占用一些系统资源。因此，这可能会对我们的训练任务产生一定的性能损失。考虑到训练所需资源的成本，我们可能更倾向于避免这种情况。'
- en: Granted, we have taken a very one-sided view in the Kubernetes-for-ML debate.
    Based solely on the arguments above, you might conclude that we would need a darn
    good reason for choosing Kubernetes as a framework for ML training. It is our
    opinion that the challenge put forth in this post, i.e., the desire to maximize
    the utility of scarce AI compute resources, is exactly the type of justification
    that warrants the use of Kubernetes despite the arguments made above. As we will
    demonstrate, the default [scheduler](https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/)
    that is built-in to [Kubernetes](https://kubernetes.io/), combined with its support
    for [priority and preemption](https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/)
    makes it a front-runner for fulfilling the requirements stated above.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 诚然，我们在 Kubernetes 与 ML 争论中采取了非常片面的观点。仅根据上述论点，你可能会得出结论，认为我们需要一个非常充分的理由才能选择 Kubernetes
    作为 ML 训练的框架。我们认为，本文提出的挑战，即最大化稀缺的 AI 计算资源的效用，正是支持使用 Kubernetes 的理由，尽管上述论点提出了反对意见。正如我们将展示的那样，内置的
    [调度器](https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/)
    和对 [优先级和抢占](https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/)
    的支持，使其成为满足上述需求的有力候选者。
- en: Toy Example
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例
- en: In this section we will share a brief example that demonstrates the priority
    scheduling support that is built in to Kubernetes. For the purposes of our demonstration,
    we will use [Minikube](https://minikube.sigs.k8s.io/docs/) (version v1.32.0).
    Minikube is a tool that enables you to run a Kubernetes cluster in a local environment
    and is an ideal playground for experimenting with Kubernetes. Please see the official
    documentation on [installing and getting started](https://minikube.sigs.k8s.io/docs/start/)
    with Minikube.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将分享一个简短的示例，展示 Kubernetes 内建的优先级调度支持。为了演示的目的，我们将使用 [Minikube](https://minikube.sigs.k8s.io/docs/)（版本
    v1.32.0）。Minikube 是一个工具，可以让你在本地环境中运行 Kubernetes 集群，非常适合用来实验 Kubernetes。请参阅官方文档了解
    [Minikube 的安装与入门](https://minikube.sigs.k8s.io/docs/start/)。
- en: Cluster Creation
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集群创建
- en: 'Let’s start by creating a two-node cluster using the [Minikube start](https://minikube.sigs.k8s.io/docs/commands/start/)
    command:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先使用 [Minikube start](https://minikube.sigs.k8s.io/docs/commands/start/)
    命令创建一个包含两个节点的集群：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The result is a local Kubernetes cluster consisting of a master (“control-plane”)
    node named *minikube,* and a single worker node, named *minikube-m02*, which will
    simulate our single AI resource. Let’s apply the [label](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/)
    *my-precious* to identify it as a unique resource type:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个本地 Kubernetes 集群，包含一个名为 *minikube* 的主节点（“控制平面”）和一个名为 *minikube-m02* 的工作节点，它将模拟我们的单一
    AI 资源。让我们应用 [标签](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/)
    *my-precious* 来标识它为一个独特的资源类型：
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We can use the [Minikube dashboard](https://minikube.sigs.k8s.io/docs/handbook/dashboard/)
    to visualize the results. In a separate shell run the command below and open the
    generated browser link.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 [Minikube 仪表板](https://minikube.sigs.k8s.io/docs/handbook/dashboard/)
    来可视化结果。在另一个 shell 中运行下面的命令，并打开生成的浏览器链接。
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If you press on the *Nodes* tab on the left-hand pane, you should see a summary
    of our cluster’s nodes:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你点击左侧面板上的 *Nodes* 标签，你应该能够看到集群节点的总结：
- en: '![](../Images/77af57927c95d3cd62defaac704ecf15.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77af57927c95d3cd62defaac704ecf15.png)'
- en: Nodes List in Minikube Dashboard (Captured by Author)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Minikube 仪表板中的节点列表（由作者捕获）
- en: PriorityClass Definitions
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PriorityClass 定义
- en: Next, we define two [PriorityClasses](https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass),
    *low-priority* and *high-priority*, as in the *priorities.yaml* file displayed
    below. New jobs will receive the *low-priority* assignment, by default.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义两个 [PriorityClasses](https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass)，*低优先级*
    和 *高优先级*，如下面显示的 *priorities.yaml* 文件所示。新作业默认将分配为 *低优先级*。
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To apply our new classes to our cluster, we run:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将我们的新类应用到集群中，我们运行：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Create a Job
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建作业
- en: We define a simple job using a *job.yaml* file displayed in the code block below.
    For the purpose of our demonstration, we define a [Kubernetes Job](https://kubernetes.io/docs/concepts/workloads/controllers/job/)
    that does nothing more than sleep for 100 seconds. We use [busybox](https://hub.docker.com/_/busybox)
    as its Docker image. In practice, this would be replaced with a training script
    and an appropriate ML Docker image. We define the job to run on our special instance,
    *my-precious*, using the [nodeSelector](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
    field, and specify the resource requirements so that only a single instance of
    the job can run on the instance at a time. The priority of the job defaults to
    *low-priority* as defined above.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用下面的代码块定义一个简单的作业，使用*job.yaml*文件。为了演示的目的，我们定义了一个[Kubernetes作业](https://kubernetes.io/docs/concepts/workloads/controllers/job/)，该作业只是简单地睡眠100秒。我们使用[busybox](https://hub.docker.com/_/busybox)作为它的Docker镜像。在实际应用中，这将被替换为一个训练脚本和合适的机器学习Docker镜像。我们通过[nodeSelector](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)字段指定作业在我们特殊的实例*my-precious*上运行，并指定资源需求，确保一次只能在该实例上运行一个作业。作业的优先级默认为上面定义的*低优先级*。
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We submit the job with the following command:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下命令提交作业：
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Create a Queue of Jobs
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建作业队列
- en: 'To demonstrate the manner in which Kubernetes queues jobs for processing, we
    create three identical copies of the job defined above, named *test1*, *test2*,
    and *test3*. We group the three jobs in a single file, *jobs.yaml*, and submit
    them for processing:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示Kubernetes如何排队处理作业，我们创建了三个与上面定义的作业相同的副本，分别命名为*test1*、*test2*和*test3*。我们将这三个作业组合成一个文件*jobs.yaml*，并提交它们进行处理：
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The image below captures the *Workload Status* of our cluster in the [Minikube
    dashboard](https://minikube.sigs.k8s.io/docs/handbook/dashboard/) shortly after
    the submission. You can see that *my-precious* has begun processing *test1*, while
    the other jobs are *pending* as they wait their turn.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示的是我们集群的*工作负载状态*，该状态是在提交作业后不久，通过[Minikube仪表盘](https://minikube.sigs.k8s.io/docs/handbook/dashboard/)捕捉到的。你可以看到，*my-precious*已经开始处理*test1*，而其他作业则处于*待处理*状态，等待轮到它们。
- en: '![](../Images/6efd9f7f525b7319dd48870d0fc7c7af.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6efd9f7f525b7319dd48870d0fc7c7af.png)'
- en: Cluster Workload Status (Captured by Author)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 集群工作负载状态（由作者捕获）
- en: 'Once *test1* is completed, processing of *test2* begins:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦*test1*完成，*test2*的处理将开始：
- en: '![](../Images/6fd10d8f89a5fef5073ae2411edeb91c.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6fd10d8f89a5fef5073ae2411edeb91c.png)'
- en: Cluster Workload Status — Automated Scheduling (Captured by Author)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 集群工作负载状态 — 自动调度（由作者捕获）
- en: So long as no other jobs with higher priority are submitted, our jobs would
    continue to be processed one at a time until they are all completed.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 只要没有其他更高优先级的作业提交，我们的作业将继续按顺序处理，直到所有作业完成。
- en: Job Preemption
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 作业抢占
- en: 'We now demonstrate Kubernetes’ built-in support for [job preemption](https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/)
    by showing what happens when we submit a fourth job, this time with the *high-priority*
    setting:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过展示提交第四个作业（这次设置为*高优先级*）时发生的情况，来演示Kubernetes内置对[作业抢占](https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/)的支持：
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The impact on the *Workload Status* is displayed in the image below:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*工作负载状态*的影响如下面的图所示：'
- en: '![](../Images/62ebad9df10e25effd21dbea9d2865ac.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62ebad9df10e25effd21dbea9d2865ac.png)'
- en: Cluster Workload Status — Preemption (Captured by Author)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 集群工作负载状态 — 抢占（由作者捕获）
- en: The *test2* job has been *preempted* — its processing has been stopped and it
    has returned to the *pending* state. In its stead, *my-precious* has begun processing
    the higher priority *test-p1* job. Only once *test-p1* is completed will processing
    of the lower priority jobs resume. (In the case where the preempted job is a ML
    training workload, we would program it to resume from the most recent saved model
    [model checkpoint](https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html)).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*test2*作业已被*抢占*——它的处理被停止，并且返回到*待处理*状态。取而代之的是，*my-precious*开始处理优先级更高的*test-p1*作业。只有当*test-p1*完成后，低优先级作业的处理才会恢复。（如果被抢占的作业是一个机器学习训练工作负载，我们会编程让它从最近保存的模型[模型检查点](https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html)继续恢复。）'
- en: The image below displays the *Workload Status* once all jobs have been completed.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了所有作业完成后的*工作负载状态*。
- en: '![](../Images/f242a5d5bfa89d01de0134dba77273c3.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f242a5d5bfa89d01de0134dba77273c3.png)'
- en: Cluster Workload Status — Completion (Captured by Author)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 集群工作负载状态 — 完成（由作者捕获）
- en: Kubernetes Extensions
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 扩展
- en: The solution we demonstrated for priority-based scheduling and preemption relied
    only on core components of Kubernetes. In practice, you may choose to take advantage
    of enhancements to the basic functionality introduced by extensions such as [Kueue](https://github.com/kubernetes-sigs/kueue)
    and/or dedicated, ML-specific features offered by platforms build on top of Kubernetes,
    such as [Run:AI](https://www.run.ai/) or [Volcano](https://volcano.sh/en/). But
    keep in mind that to fulfill the basic requirements for maximizing the utility
    of a scarce AI compute resource all we need is the core Kubernetes.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示的基于优先级调度和抢占的解决方案仅依赖于Kubernetes的核心组件。在实际应用中，你可以选择利用如[Kueue](https://github.com/kubernetes-sigs/kueue)等扩展引入的基本功能增强，和/或在Kubernetes之上构建的平台提供的专用ML特性，例如[Run:AI](https://www.run.ai/)或[Volcano](https://volcano.sh/en/)。但请记住，为了满足最大化稀缺AI计算资源效用的基本要求，我们所需要的只是Kubernetes的核心功能。
- en: Summary
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: The reduced availability of dedicated AI silicon has forced ML teams to adjust
    their development processes. Unlike in the past, when developers could spin up
    new AI resources at will, they now face limitations on AI compute capacity. This
    necessitates the procurement of AI instances through means such as purchasing
    dedicated units and/or reserving cloud instances. Moreover, developers must come
    to terms with the likelihood of needing to share these resources with other users
    and projects. To ensure that the scarce AI compute power is appropriated towards
    maximum utility, dedicated scheduling algorithms must be defined that minimize
    idle time and prioritize critical workloads. In this post we have demonstrated
    how the [Kubernetes scheduler](https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/)
    can be used to accomplish these goals. As emphasized above, this is just one of
    many approaches to address the challenge of maximizing the utility of scarce AI
    resources. Naturally, the approach you choose, and the details of your implementation
    will depend on the specific needs of your AI development.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 专用AI硅芯片的可用性减少迫使ML团队调整他们的开发流程。与过去不同，开发人员可以随意创建新的AI资源，现在他们面临AI计算能力的限制。这就需要通过购买专用单元和/或预定云实例等方式来采购AI实例。此外，开发人员还必须接受与其他用户和项目共享这些资源的可能性。为了确保稀缺的AI计算能力得到最大效用的分配，必须定义专用调度算法，减少空闲时间并优先处理关键工作负载。在本文中，我们展示了如何利用[Kubernetes调度器](https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/)实现这些目标。如上所述，这只是解决最大化稀缺AI资源效用的众多方法之一。自然，你选择的方案以及实现的细节将取决于你AI开发的具体需求。
