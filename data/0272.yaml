- en: 'Maixtchup: Make Your Own Mixture of Experts with Mergekit'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Maixtchup：使用 Mergekit 创建你自己的专家混合模型
- en: 原文：[https://towardsdatascience.com/maixtchup-make-your-own-mixture-of-experts-with-mergekit-87cc46401587?source=collection_archive---------11-----------------------#2024-01-29](https://towardsdatascience.com/maixtchup-make-your-own-mixture-of-experts-with-mergekit-87cc46401587?source=collection_archive---------11-----------------------#2024-01-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/maixtchup-make-your-own-mixture-of-experts-with-mergekit-87cc46401587?source=collection_archive---------11-----------------------#2024-01-29](https://towardsdatascience.com/maixtchup-make-your-own-mixture-of-experts-with-mergekit-87cc46401587?source=collection_archive---------11-----------------------#2024-01-29)
- en: The rise of the MoEs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MoE 的崛起
- en: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--87cc46401587--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--87cc46401587--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--87cc46401587--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--87cc46401587--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--87cc46401587--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--87cc46401587--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--87cc46401587--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--87cc46401587--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--87cc46401587--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--87cc46401587--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--87cc46401587--------------------------------)
    ·8 min read·Jan 29, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--87cc46401587--------------------------------)
    ·阅读时长：8分钟·2024年1月29日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ce341668f026e81e27282ec3085bba1a.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce341668f026e81e27282ec3085bba1a.png)'
- en: Image by the author — Generated with DALL-E
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片 — 通过 DALL-E 生成
- en: Since the release of Mixtral-8x7B by Mistral AI, there has been a renewed interest
    in the [mixture of expert (MoE) models](https://medium.com/p/0e3fc7fde818). This
    architecture exploits expert sub-networks among which only some of them are selected
    and activated by a router network during inference.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 自从 Mistral AI 发布 Mixtral-8x7B 以来，业界对 [专家混合模型（MoE）](https://medium.com/p/0e3fc7fde818)的兴趣重新高涨。这种架构利用专家子网络，在推理过程中只有部分子网络被路由网络选择并激活。
- en: MoEs are so simple and flexible that it is easy to make a custom MoE. On the
    Hugging Face Hub, we can now find several trending LLMs that are custom MoEs,
    such as [mlabonne/phixtral-4x2_8](https://huggingface.co/mlabonne/phixtral-4x2_8).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: MoE（专家混合模型）非常简单且灵活，便于制作自定义 MoE。在 Hugging Face Hub 上，我们现在可以找到几种流行的自定义 MoE LLM（大型语言模型），例如
    [mlabonne/phixtral-4x2_8](https://huggingface.co/mlabonne/phixtral-4x2_8)。
- en: However, most of them are not traditional MoEs made from scratch, they simply
    use a combination of already fine-tuned LLMs as experts. Their creation was made
    easy with [mergekit](https://github.com/cg123/mergekit) ([LGPL-3.0 license](https://github.com/cg123/mergekit#LGPL-3.0-1-ov-file)).
    For instance, Phixtral LLMs have been made with mergekit by combining several
    [Phi-2 models](https://medium.com/p/06db49949ff1).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，大多数模型并不是从头开始构建的传统 MoE，它们只是将已经微调过的 LLM 作为专家进行组合。它们的创建过程通过 [mergekit](https://github.com/cg123/mergekit)（[LGPL-3.0
    许可证](https://github.com/cg123/mergekit#LGPL-3.0-1-ov-file)）变得更加简便。例如，Phixtral
    LLM 就是通过 mergekit 将多个 [Phi-2 模型](https://medium.com/p/06db49949ff1)结合而成的。
- en: In this article, we will see how Phixtral was created. We will apply the same
    process to create our own mixture of experts, Maixtchup, using several Mistral
    7B models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将展示 Phixtral 是如何创建的。我们将应用相同的过程，使用多个 Mistral 7B 模型来创建我们自己的专家混合模型，Maixtchup。
- en: What is Phixtral?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 Phixtral？
- en: 'To quickly understand the high-level architecture of a model, I like to print
    it. For instance, for [mlabonne/phixtral-4x2_8](https://huggingface.co/mlabonne/phixtral-4x2_8)
    (MIT license):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了快速理解模型的高层架构，我喜欢将其打印出来。例如，对于 [mlabonne/phixtral-4x2_8](https://huggingface.co/mlabonne/phixtral-4x2_8)（MIT
    许可证）：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
