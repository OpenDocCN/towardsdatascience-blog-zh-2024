- en: 'LLaVA: An open-source alternative to GPT-4V(ision)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLaVA：一个开源的GPT-4V(ision)替代方案
- en: 原文：[https://towardsdatascience.com/llava-an-open-source-alternative-to-gpt-4v-ision-b06f88ce8efa?source=collection_archive---------2-----------------------#2024-01-23](https://towardsdatascience.com/llava-an-open-source-alternative-to-gpt-4v-ision-b06f88ce8efa?source=collection_archive---------2-----------------------#2024-01-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/llava-an-open-source-alternative-to-gpt-4v-ision-b06f88ce8efa?source=collection_archive---------2-----------------------#2024-01-23](https://towardsdatascience.com/llava-an-open-source-alternative-to-gpt-4v-ision-b06f88ce8efa?source=collection_archive---------2-----------------------#2024-01-23)
- en: Running LLaVA on the Web, locally, and on Google Colab
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Web、本地和Google Colab上运行LLaVA
- en: '[](https://ya-lb.medium.com/?source=post_page---byline--b06f88ce8efa--------------------------------)[![Yann-Aël
    Le Borgne](../Images/acc1c8b32373d7f345064b89b51869fd.png)](https://ya-lb.medium.com/?source=post_page---byline--b06f88ce8efa--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b06f88ce8efa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b06f88ce8efa--------------------------------)
    [Yann-Aël Le Borgne](https://ya-lb.medium.com/?source=post_page---byline--b06f88ce8efa--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ya-lb.medium.com/?source=post_page---byline--b06f88ce8efa--------------------------------)[![Yann-Aël
    Le Borgne](../Images/acc1c8b32373d7f345064b89b51869fd.png)](https://ya-lb.medium.com/?source=post_page---byline--b06f88ce8efa--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b06f88ce8efa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b06f88ce8efa--------------------------------)
    [Yann-Aël Le Borgne](https://ya-lb.medium.com/?source=post_page---byline--b06f88ce8efa--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b06f88ce8efa--------------------------------)
    ·7 min read·Jan 23, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b06f88ce8efa--------------------------------)
    ·7分钟阅读·2024年1月23日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ce1327ce8af3bcc9107ecbaba49c33c1.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce1327ce8af3bcc9107ecbaba49c33c1.png)'
- en: Curious where this picture was taken? Ask LLaVA! (Image by [Guy Rey-Bellet](https://pixabay.com/users/grey48-7109111/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3116211)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3116211)).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 好奇这张图片拍摄于哪里吗？问问LLaVA吧！(图片来自[Guy Rey-Bellet](https://pixabay.com/users/grey48-7109111/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3116211)，来自[Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3116211))。
- en: '[LLaVA](https://llava-vl.github.io/) (acronym of **L**arge **L**anguage and
    **V**isual **A**ssistant) is a promising open-source generative AI model that
    replicates some of the capabilities of OpenAI GPT-4 in conversing with images.
    Users can add images into LLaVA chat conversations, allowing to discuss about
    the content of these images, but also to use them as a way to describe ideas,
    contexts or situations in a visual way.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[LLaVA](https://llava-vl.github.io/)（**L**arge **L**anguage 和 **V**isual **A**ssistant
    的缩写）是一个有前途的开源生成型AI模型，它复制了OpenAI GPT-4在与图像对话方面的一些能力。用户可以将图片添加到LLaVA的聊天对话中，不仅能够讨论这些图片的内容，还可以利用它们以视觉方式描述想法、情境或场景。'
- en: The most compelling features of LLaVA are its ability to improve upon other
    open-source solutions while using a simpler model architecture and orders of magnitude
    less training data. These characteristics make LLaVA not only faster and cheaper
    to train, but also more suitable for inference on consumer hardware.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: LLaVA最引人注目的特点是，它能够在使用更简单的模型架构和数量级更少的训练数据的情况下，超越其他开源解决方案。这些特点使得LLaVA不仅在训练上更快速且成本更低，而且更适合在消费级硬件上进行推理。
- en: This post gives an overview of LLaVA, and more specifically aims to
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文概述了LLaVA，并更具体地旨在
- en: show how to experiment with it from a web interface, and how it can be installed
    on your computer or laptop
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 展示如何通过网络界面进行实验，以及如何在您的计算机或笔记本电脑上安装它
- en: explain its main technical characteristics
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释它的主要技术特点
- en: illustrate how to program with it, using as an example a simple chatbot application
    built with HuggingFace libraries (*Transformers* and *Gradio*) on Google Colab.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过一个简单的聊天机器人应用示例，展示如何用它编程，该应用是基于HuggingFace库（*Transformers* 和 *Gradio*）在Google
    Colab上构建的。
- en: Using LLaVA online
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LLaVA在线
- en: If you have not yet tried it, the simplest way to use LLaVA is by going to the
    [Web interface](https://llava.hliu.cc/) provided by its authors. The screenshot
    below illustrates how the interface operates, where a user asks for ideas about
    what meals to do given a picture of the content of their fridge. Images can be
    loaded using the widget on the left, and the chat interface allows to ask questions
    and obtain answers in the form of text.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有尝试过，使用LLaVA的最简单方法是访问其作者提供的[Web界面](https://llava.hliu.cc/)。下面的截图展示了界面如何操作，其中用户根据冰箱内容的图片询问可以做哪些餐点的建议。用户可以通过左侧的控件上传图片，聊天界面允许提问并以文本形式获取答案。
- en: '![](../Images/e4dc5aac06a83925f5c53726d2e9212e.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4dc5aac06a83925f5c53726d2e9212e.png)'
- en: '[LLaVA Web interface](https://llava.hliu.cc/)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[LLaVA Web界面](https://llava.hliu.cc/)'
- en: In this example, LLaVA correctly identifies ingredients present in the fridge,
    such as blueberries, strawberries, carrots, yoghourt or milk, and suggest relevant
    ideas such as fruit salads, smoothies or cakes.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，LLaVA正确识别出了冰箱中的食材，例如蓝莓、草莓、胡萝卜、酸奶或牛奶，并建议了相关的食谱创意，比如水果沙拉、果昔或蛋糕。
- en: Other examples of conversations with LLaVA are given on the [project website](https://llava-vl.github.io/),
    which illustrate that LLaVA is capable of not just describing images but also
    making inferences and reasoning based on the elements within the image (identify
    a movie or a person using clues from a picture, code a website from a drawing,
    explain humourous situations, and so on).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 与LLaVA的其他对话示例可在[项目网站](https://llava-vl.github.io/)上查看，这些示例说明了LLaVA不仅能够描述图像，还能够根据图像中的元素进行推理和推断（例如通过图片中的线索识别电影或人物、根据画图编码网站、解释幽默情况等）。
- en: Running LLaVA locally
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本地运行LLaVA
- en: LLaVA can also be installed on a local machine using [Ollama](https://ollama.ai/)
    or a Mozilla ‘[llamafile](https://github.com/Mozilla-Ocho/llamafile)’. These tools
    can run on most CPU-only consumer-grade level machines, as the model only requires
    8GB of RAM and 4GB of free disk space, and was even shown to [successfully run
    on a Raspberry PI](/running-local-llms-and-vlms-on-the-raspberry-pi-57bd0059c41a).
    Among the tools and interfaces developed around the Ollama project, a notable
    initiative is the [Ollama-WebUI](https://github.com/ollama-webui/ollama-webui)
    (illustrated below), which reproduces the look and feel of OpenAI ChatGPT user
    interface.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: LLaVA还可以使用[Ollama](https://ollama.ai/)或Mozilla的‘[llamafile](https://github.com/Mozilla-Ocho/llamafile)’安装到本地计算机。这些工具可以在大多数仅配备CPU的消费级机器上运行，因为模型仅需要8GB内存和4GB的可用磁盘空间，甚至已经证明可以在[Raspberry
    PI上成功运行](/running-local-llms-and-vlms-on-the-raspberry-pi-57bd0059c41a)。在围绕Ollama项目开发的工具和接口中，一个值得注意的项目是[Ollama-WebUI](https://github.com/ollama-webui/ollama-webui)（如下图所示），它复现了OpenAI
    ChatGPT用户界面的外观和操作体验。
- en: '![](../Images/da0916a9564113060e6305c3a0d43299.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da0916a9564113060e6305c3a0d43299.png)'
- en: '[Ollama Web user interface](https://github.com/ollama-webui/ollama-webui) —
    inspired by [OpenAI ChatGPT](https://chat.openai.com/)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ollama Web用户界面](https://github.com/ollama-webui/ollama-webui) — 受[OpenAI ChatGPT](https://chat.openai.com/)的启发'
- en: Brief overview of LLaVA’s main features
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLaVA主要特性简要概述
- en: LLaVA was designed by researchers from the University of Wisconsin-Madison,
    Microsoft Research and Columbia University, and was recently showcased at NeurIPS
    2023\. The project’s code and technical specifications can be accessed on its
    [Github repository](https://github.com/haotian-liu/LLaVA), which also offers various
    interfaces for interacting with the assistant.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: LLaVA是由威斯康星大学麦迪逊分校、微软研究院和哥伦比亚大学的研究人员设计的，并且最近在NeurIPS 2023上展示。该项目的代码和技术规格可以通过[Github仓库](https://github.com/haotian-liu/LLaVA)访问，仓库还提供了与助手互动的各种接口。
- en: 'As the authors summarize in [their paper’s abstract](https://arxiv.org/pdf/2310.03744.pdf):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 正如作者在[论文摘要](https://arxiv.org/pdf/2310.03744.pdf)中总结的：
- en: '[LLava] achieves state-of-the-art across 11 benchmarks. Our final 13B checkpoint
    uses merely 1.2M publicly available data, and finishes full training in ~1 day
    on a single 8-A100 node. We hope this can make state-of-the-art LMM research more
    accessible. Code and model will be publicly available.'
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[LLaVA] 在11个基准测试中达到了最先进的水平。我们的最终13B检查点仅使用了1.2M公开可用的数据，并且在单个8-A100节点上完成了大约1天的全训练。我们希望这能使最先进的大型多模态模型研究变得更加易于接触。代码和模型将公开发布。'
- en: The benchmark results, reported in the paper as the radar chart below, illustrate
    the improvements compared to other state-of-the-art models.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 论文中报告的基准结果，如下方的雷达图，展示了与其他最先进模型的对比改进。
- en: '![](../Images/98a3010228400b1064aded781a0eee41.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/98a3010228400b1064aded781a0eee41.png)'
- en: Radar chart of LLaVA’s benchmark results (image from [paper](https://arxiv.org/pdf/2304.08485.pdf))
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: LLaVA基准结果的雷达图（图片来自[论文](https://arxiv.org/pdf/2304.08485.pdf)）
- en: Inner workings
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内部工作机制
- en: LLaVA’s data processing workflow is conceptually simple. The model essentially
    works as a standard causal language model, taking language instructions (a user
    text prompt) as input, and returning a language response. The ability of the language
    model to handle images is allowed by a separate vision encoder model that converts
    images into language tokens, which are quietly added to the user text prompt (acting
    as a kind of [soft prompt](https://huggingface.co/docs/peft/main/en/conceptual_guides/prompting)).
    The LLaVA process is illustrated below.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: LLaVA的数据处理工作流程在概念上非常简单。该模型本质上作为一个标准的因果语言模型工作，接受语言指令（用户文本提示）作为输入，并返回语言响应。语言模型处理图像的能力由一个独立的视觉编码器模型提供，该模型将图像转换为语言标记，这些标记被悄悄地添加到用户文本提示中（充当一种[软提示](https://huggingface.co/docs/peft/main/en/conceptual_guides/prompting)）。LLaVA的处理流程如下所示。
- en: '![](../Images/b68138aa94aba7d0da27c16272489945.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b68138aa94aba7d0da27c16272489945.png)'
- en: LLaVA network architecture (image from [paper](https://arxiv.org/pdf/2304.08485.pdf))
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: LLaVA网络架构（图片来自[论文](https://arxiv.org/pdf/2304.08485.pdf)）
- en: LLaVA’s language model and vision encoder rely on two reference models called
    Vicuna and CLIP, respectively. [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/)
    is a pretrained large language model based on LLaMA-2 (designed by Meta) that
    boasts competitive performances with medium sized LLM (See model cards for the
    [7B](https://huggingface.co/lmsys/vicuna-7b-v1.5) and [13B](https://huggingface.co/lmsys/vicuna-13b-v1.5)
    versions on HuggingFace). [CLIP](https://openai.com/research/clip) is an image
    encoder designed by OpenAI, pretrained to encode images and text in a similar
    embedding space using **c**ontrastive **l**anguage-**i**mage **p**retraining (hence
    ‘CLIP’). The model used in LLaVA is the vision transformer variant CLIP-ViT-L/14
    (see its [model card](https://huggingface.co/openai/clip-vit-large-patch14) on
    HuggingFace).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: LLaVA的语言模型和视觉编码器分别依赖于两个参考模型，称为Vicuna和CLIP。[Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/)是一个基于LLaMA-2（由Meta设计）的预训练大语言模型，具有与中型LLM相媲美的竞争性能（参见HuggingFace上[7B](https://huggingface.co/lmsys/vicuna-7b-v1.5)和[13B](https://huggingface.co/lmsys/vicuna-13b-v1.5)版本的模型卡）。[CLIP](https://openai.com/research/clip)是由OpenAI设计的图像编码器，经过预训练，可以在相似的嵌入空间中对图像和文本进行编码，使用的是**对比**语言-**图像**预训练（因此称为‘CLIP’）。LLaVA中使用的模型是视觉变换器变种CLIP-ViT-L/14（查看其在HuggingFace上的[模型卡](https://huggingface.co/openai/clip-vit-large-patch14)）。
- en: To match the dimension of the vision encoder with those of the language model,
    a projection module (**W** in the image above) is applied. It is a simple linear
    projection in the original [LLaVA](https://arxiv.org/abs/2304.08485), and a two-layer
    perceptron in [LLaVA 1.5](https://arxiv.org/abs/2310.03744).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了匹配视觉编码器的维度与语言模型的维度，一个投影模块（如上图中的**W**）被应用。它在原始的[LLaVA](https://arxiv.org/abs/2304.08485)中是一个简单的线性投影，在[LLaVA
    1.5](https://arxiv.org/abs/2310.03744)中是一个两层感知机。
- en: Training process
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练过程
- en: The training process of LLaVA consists of two relatively simple stages.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: LLaVA的训练过程由两个相对简单的阶段组成。
- en: The first stage solely aims at tuning the projection module **W**, and the weights
    of the vision encoder and LLM are kept frozen. The training is performed using
    a subset of around 600k image/caption pairs from the [CC3M conceptual caption
    dataset](https://ai.google.com/research/ConceptualCaptions/), and is available
    on HuggingFace [in this repository](https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 第一阶段仅仅是为了调优投影模块**W**，而视觉编码器和LLM的权重保持冻结。训练使用来自[CC3M概念性字幕数据集](https://ai.google.com/research/ConceptualCaptions/)的约60万对图像/字幕数据的子集进行，并且可以在HuggingFace的[此仓库](https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K)中找到。
- en: In a second stage, the projection module weigths **W** are fine-tuned together
    with the LLM weights (while keeping the vision encoder’s weights frozen), using
    dataset of 158K language-image instruction-following data. The data is generated
    using GPT4, and feature examples of conversations, detailed descriptions and complex
    reasonings, and is available on HuggingFace [in this repository](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二阶段，投影模块的权重**W**与LLM的权重一起进行微调（同时保持视觉编码器的权重冻结），使用包含158K语言-图像指令跟随数据的训练集。这些数据是通过GPT-4生成的，包含对话、详细描述和复杂推理的示例，并且可以在HuggingFace的[此仓库](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K)中找到。
- en: The whole training takes around a day using eight A100 GPUs.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 整个训练过程大约需要一天时间，使用八个A100 GPU。
- en: 'Programming with LLaVA: How to get started'
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LLaVA进行编程：如何开始
- en: '*Code available on the* [*Colab related notebook*](https://colab.research.google.com/drive/1L28bJX14-Y5lJvswYwydsletYFMIxVH5)*.*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*代码可在* [*Colab相关笔记本*](https://colab.research.google.com/drive/1L28bJX14-Y5lJvswYwydsletYFMIxVH5)*.* '
- en: The LLaVA model is integrated in the Transformers library, and can be loaded
    using the standard *pipeline* object. The 7B and 13B variants of the models are
    available on the [LLaVA 😊 Hub space](https://huggingface.co/llava-hf), and may
    be loaded in 4 and 8 bits to save GPU memory. We illustrate below how to load
    and run model using code that can be executed on Colab with a T4 TPU (15GB RAM
    GPU).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: LLaVA模型已集成在Transformers库中，可以通过标准的*pipeline*对象加载。7B和13B版本的模型可以在[LLaVA 😊 Hub空间](https://huggingface.co/llava-hf)上找到，并可以以4位和8位加载，以节省GPU内存。下面我们展示如何使用代码加载并运行模型，代码可以在Colab上使用T4
    TPU（15GB内存GPU）执行。
- en: 'Below is the code snippet to load the 7B variant of LLaVA 1.5 in 4 bits:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是加载4位LLaVA 1.5 7B版本的代码片段：
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let us then load this picture
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然后让我们加载这张图片
- en: '![](../Images/ce1327ce8af3bcc9107ecbaba49c33c1.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce1327ce8af3bcc9107ecbaba49c33c1.png)'
- en: 'We use the standard PIL library for loading the picture:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用标准的PIL库来加载这张图片：
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Let us finally query the LLaVA model with the image, with a prompt asking to
    describe the picture.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用图像查询LLaVA模型，并提供一个提示请求描述这张图片。
- en: 'Note: [The format for the prompt](https://huggingface.co/docs/transformers/model_doc/llava)
    follows'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：[提示的格式](https://huggingface.co/docs/transformers/model_doc/llava)如下
- en: '“USER: <image>\n<prompt>\nASSISTANT:”'
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '“USER: <image>\n<prompt>\nASSISTANT:”'
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Which returns the following answer:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下答案：
- en: 'USER: Describe this picture'
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'USER: 描述这张图片'
- en: '​ASSISTANT: ​The image features a large, empty amphitheater with a stunning
    view of the ocean in the background. The amphitheater is surrounded by a lush
    green hillside, and a majestic mountain can be seen in the distance. The scene
    is serene and picturesque, with the sun shining brightly over the landscape.'
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '​ASSISTANT: ​这张图片展示了一个大型、空旷的剧场，背景是壮丽的海洋景色。剧场四周被郁郁葱葱的绿色山坡环绕，远处可以看到一座雄伟的山脉。整个场景宁静而如画，阳光明媚地照耀在大地上。'
- en: LLaVA chatbot
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLaVA聊天机器人
- en: Let us finally create a simple chatbot that relies on a LLaVA model. We will
    use the [Gradio library](https://www.gradio.app/), which provides a fast and easy
    way to create machine learning web interfaces.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们创建一个简单的聊天机器人，依赖于LLaVA模型。我们将使用[Gradio库](https://www.gradio.app/)，它提供了一种快速简便的方法来创建机器学习Web界面。
- en: The core for the interface consists of a row with an image uploader (a Gradio
    Image object), and a chat interface (a Gradio [ChatInterface](https://www.gradio.app/docs/chatinterface)
    object).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 界面的核心是一个包含图像上传器（Gradio图像对象）和聊天界面（Gradio [ChatInterface](https://www.gradio.app/docs/chatinterface)对象）的行。
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The chat interface connects to a function *update_conversation*, that takes
    care of keeping the conversation history, and calling the LLaVA model for a response
    whenever the user sends a message.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天界面连接到一个*update_conversation*函数，该函数负责保持对话历史记录，并在用户发送消息时调用LLaVA模型生成响应。
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The interface is launched calling the *launch* method.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 界面是通过调用*launch*方法启动的。
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After a few seconds, the chatbot Web interface will appear:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 几秒钟后，聊天机器人Web界面将出现：
- en: '![](../Images/05629290b6a0cdaaf30927ff858868a6.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05629290b6a0cdaaf30927ff858868a6.png)'
- en: Congratulations, your LLaVA chatbot is now up and running!
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜，你的LLaVA聊天机器人已经启动并运行！
- en: Useful links
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有用的链接
- en: '[HuggingFace LLaVA model documentation](https://huggingface.co/docs/transformers/model_doc/llava)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[HuggingFace LLaVA模型文档](https://huggingface.co/docs/transformers/model_doc/llava)'
- en: '[Llava Hugging Face organization](https://huggingface.co/llava-hf)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Llava Hugging Face组织](https://huggingface.co/llava-hf)'
- en: 'Loading and running LLaVA with AutoPrecessor and LLaVAForConditionalGeneration:
    [Colab notebook](https://colab.research.google.com/drive/1_q7cOB-jCu3RExrkhrgewBR0qKjZr-Sx)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AutoPrecessor和LLaVAForConditionalGeneration加载和运行LLaVA：[Colab笔记本](https://colab.research.google.com/drive/1_q7cOB-jCu3RExrkhrgewBR0qKjZr-Sx)
- en: '[GPT-4V(ision) system card](https://cdn.openai.com/papers/GPTV_System_Card.pdf)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GPT-4V(ision)系统卡片](https://cdn.openai.com/papers/GPTV_System_Card.pdf)'
- en: '[Understanding Visual Instruction Tuning](https://newsletter.artofsaience.com/p/understanding-visual-instruction)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[理解视觉指令微调](https://newsletter.artofsaience.com/p/understanding-visual-instruction)'
- en: 'Note: Unless otherwise noted, all images are by the author.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：除非另有说明，所有图片均由作者提供。
