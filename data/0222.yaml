- en: 'LLaVA: An open-source alternative to GPT-4V(ision)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLaVAï¼šä¸€ä¸ªå¼€æºçš„GPT-4V(ision)æ›¿ä»£æ–¹æ¡ˆ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/llava-an-open-source-alternative-to-gpt-4v-ision-b06f88ce8efa?source=collection_archive---------2-----------------------#2024-01-23](https://towardsdatascience.com/llava-an-open-source-alternative-to-gpt-4v-ision-b06f88ce8efa?source=collection_archive---------2-----------------------#2024-01-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/llava-an-open-source-alternative-to-gpt-4v-ision-b06f88ce8efa?source=collection_archive---------2-----------------------#2024-01-23](https://towardsdatascience.com/llava-an-open-source-alternative-to-gpt-4v-ision-b06f88ce8efa?source=collection_archive---------2-----------------------#2024-01-23)
- en: Running LLaVA on the Web, locally, and on Google Colab
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨Webã€æœ¬åœ°å’ŒGoogle Colabä¸Šè¿è¡ŒLLaVA
- en: '[](https://ya-lb.medium.com/?source=post_page---byline--b06f88ce8efa--------------------------------)[![Yann-AÃ«l
    Le Borgne](../Images/acc1c8b32373d7f345064b89b51869fd.png)](https://ya-lb.medium.com/?source=post_page---byline--b06f88ce8efa--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b06f88ce8efa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b06f88ce8efa--------------------------------)
    [Yann-AÃ«l Le Borgne](https://ya-lb.medium.com/?source=post_page---byline--b06f88ce8efa--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ya-lb.medium.com/?source=post_page---byline--b06f88ce8efa--------------------------------)[![Yann-AÃ«l
    Le Borgne](../Images/acc1c8b32373d7f345064b89b51869fd.png)](https://ya-lb.medium.com/?source=post_page---byline--b06f88ce8efa--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b06f88ce8efa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b06f88ce8efa--------------------------------)
    [Yann-AÃ«l Le Borgne](https://ya-lb.medium.com/?source=post_page---byline--b06f88ce8efa--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b06f88ce8efa--------------------------------)
    Â·7 min readÂ·Jan 23, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b06f88ce8efa--------------------------------)
    Â·7åˆ†é’Ÿé˜…è¯»Â·2024å¹´1æœˆ23æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ce1327ce8af3bcc9107ecbaba49c33c1.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce1327ce8af3bcc9107ecbaba49c33c1.png)'
- en: Curious where this picture was taken? Ask LLaVA! (Image by [Guy Rey-Bellet](https://pixabay.com/users/grey48-7109111/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3116211)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3116211)).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å¥‡è¿™å¼ å›¾ç‰‡æ‹æ‘„äºå“ªé‡Œå—ï¼Ÿé—®é—®LLaVAå§ï¼(å›¾ç‰‡æ¥è‡ª[Guy Rey-Bellet](https://pixabay.com/users/grey48-7109111/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3116211)ï¼Œæ¥è‡ª[Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3116211))ã€‚
- en: '[LLaVA](https://llava-vl.github.io/) (acronym of **L**arge **L**anguage and
    **V**isual **A**ssistant) is a promising open-source generative AI model that
    replicates some of the capabilities of OpenAI GPT-4 in conversing with images.
    Users can add images into LLaVA chat conversations, allowing to discuss about
    the content of these images, but also to use them as a way to describe ideas,
    contexts or situations in a visual way.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[LLaVA](https://llava-vl.github.io/)ï¼ˆ**L**arge **L**anguage å’Œ **V**isual **A**ssistant
    çš„ç¼©å†™ï¼‰æ˜¯ä¸€ä¸ªæœ‰å‰é€”çš„å¼€æºç”Ÿæˆå‹AIæ¨¡å‹ï¼Œå®ƒå¤åˆ¶äº†OpenAI GPT-4åœ¨ä¸å›¾åƒå¯¹è¯æ–¹é¢çš„ä¸€äº›èƒ½åŠ›ã€‚ç”¨æˆ·å¯ä»¥å°†å›¾ç‰‡æ·»åŠ åˆ°LLaVAçš„èŠå¤©å¯¹è¯ä¸­ï¼Œä¸ä»…èƒ½å¤Ÿè®¨è®ºè¿™äº›å›¾ç‰‡çš„å†…å®¹ï¼Œè¿˜å¯ä»¥åˆ©ç”¨å®ƒä»¬ä»¥è§†è§‰æ–¹å¼æè¿°æƒ³æ³•ã€æƒ…å¢ƒæˆ–åœºæ™¯ã€‚'
- en: The most compelling features of LLaVA are its ability to improve upon other
    open-source solutions while using a simpler model architecture and orders of magnitude
    less training data. These characteristics make LLaVA not only faster and cheaper
    to train, but also more suitable for inference on consumer hardware.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: LLaVAæœ€å¼•äººæ³¨ç›®çš„ç‰¹ç‚¹æ˜¯ï¼Œå®ƒèƒ½å¤Ÿåœ¨ä½¿ç”¨æ›´ç®€å•çš„æ¨¡å‹æ¶æ„å’Œæ•°é‡çº§æ›´å°‘çš„è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œè¶…è¶Šå…¶ä»–å¼€æºè§£å†³æ–¹æ¡ˆã€‚è¿™äº›ç‰¹ç‚¹ä½¿å¾—LLaVAä¸ä»…åœ¨è®­ç»ƒä¸Šæ›´å¿«é€Ÿä¸”æˆæœ¬æ›´ä½ï¼Œè€Œä¸”æ›´é€‚åˆåœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šè¿›è¡Œæ¨ç†ã€‚
- en: This post gives an overview of LLaVA, and more specifically aims to
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡æ¦‚è¿°äº†LLaVAï¼Œå¹¶æ›´å…·ä½“åœ°æ—¨åœ¨
- en: show how to experiment with it from a web interface, and how it can be installed
    on your computer or laptop
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å±•ç¤ºå¦‚ä½•é€šè¿‡ç½‘ç»œç•Œé¢è¿›è¡Œå®éªŒï¼Œä»¥åŠå¦‚ä½•åœ¨æ‚¨çš„è®¡ç®—æœºæˆ–ç¬”è®°æœ¬ç”µè„‘ä¸Šå®‰è£…å®ƒ
- en: explain its main technical characteristics
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è§£é‡Šå®ƒçš„ä¸»è¦æŠ€æœ¯ç‰¹ç‚¹
- en: illustrate how to program with it, using as an example a simple chatbot application
    built with HuggingFace libraries (*Transformers* and *Gradio*) on Google Colab.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡ä¸€ä¸ªç®€å•çš„èŠå¤©æœºå™¨äººåº”ç”¨ç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•ç”¨å®ƒç¼–ç¨‹ï¼Œè¯¥åº”ç”¨æ˜¯åŸºäºHuggingFaceåº“ï¼ˆ*Transformers* å’Œ *Gradio*ï¼‰åœ¨Google
    Colabä¸Šæ„å»ºçš„ã€‚
- en: Using LLaVA online
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨LLaVAåœ¨çº¿
- en: If you have not yet tried it, the simplest way to use LLaVA is by going to the
    [Web interface](https://llava.hliu.cc/) provided by its authors. The screenshot
    below illustrates how the interface operates, where a user asks for ideas about
    what meals to do given a picture of the content of their fridge. Images can be
    loaded using the widget on the left, and the chat interface allows to ask questions
    and obtain answers in the form of text.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ è¿˜æ²¡æœ‰å°è¯•è¿‡ï¼Œä½¿ç”¨LLaVAçš„æœ€ç®€å•æ–¹æ³•æ˜¯è®¿é—®å…¶ä½œè€…æä¾›çš„[Webç•Œé¢](https://llava.hliu.cc/)ã€‚ä¸‹é¢çš„æˆªå›¾å±•ç¤ºäº†ç•Œé¢å¦‚ä½•æ“ä½œï¼Œå…¶ä¸­ç”¨æˆ·æ ¹æ®å†°ç®±å†…å®¹çš„å›¾ç‰‡è¯¢é—®å¯ä»¥åšå“ªäº›é¤ç‚¹çš„å»ºè®®ã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡å·¦ä¾§çš„æ§ä»¶ä¸Šä¼ å›¾ç‰‡ï¼ŒèŠå¤©ç•Œé¢å…è®¸æé—®å¹¶ä»¥æ–‡æœ¬å½¢å¼è·å–ç­”æ¡ˆã€‚
- en: '![](../Images/e4dc5aac06a83925f5c53726d2e9212e.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4dc5aac06a83925f5c53726d2e9212e.png)'
- en: '[LLaVA Web interface](https://llava.hliu.cc/)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[LLaVA Webç•Œé¢](https://llava.hliu.cc/)'
- en: In this example, LLaVA correctly identifies ingredients present in the fridge,
    such as blueberries, strawberries, carrots, yoghourt or milk, and suggest relevant
    ideas such as fruit salads, smoothies or cakes.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼ŒLLaVAæ­£ç¡®è¯†åˆ«å‡ºäº†å†°ç®±ä¸­çš„é£Ÿæï¼Œä¾‹å¦‚è“è“ã€è‰è“ã€èƒ¡èåœã€é…¸å¥¶æˆ–ç‰›å¥¶ï¼Œå¹¶å»ºè®®äº†ç›¸å…³çš„é£Ÿè°±åˆ›æ„ï¼Œæ¯”å¦‚æ°´æœæ²™æ‹‰ã€æœæ˜”æˆ–è›‹ç³•ã€‚
- en: Other examples of conversations with LLaVA are given on the [project website](https://llava-vl.github.io/),
    which illustrate that LLaVA is capable of not just describing images but also
    making inferences and reasoning based on the elements within the image (identify
    a movie or a person using clues from a picture, code a website from a drawing,
    explain humourous situations, and so on).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸LLaVAçš„å…¶ä»–å¯¹è¯ç¤ºä¾‹å¯åœ¨[é¡¹ç›®ç½‘ç«™](https://llava-vl.github.io/)ä¸ŠæŸ¥çœ‹ï¼Œè¿™äº›ç¤ºä¾‹è¯´æ˜äº†LLaVAä¸ä»…èƒ½å¤Ÿæè¿°å›¾åƒï¼Œè¿˜èƒ½å¤Ÿæ ¹æ®å›¾åƒä¸­çš„å…ƒç´ è¿›è¡Œæ¨ç†å’Œæ¨æ–­ï¼ˆä¾‹å¦‚é€šè¿‡å›¾ç‰‡ä¸­çš„çº¿ç´¢è¯†åˆ«ç”µå½±æˆ–äººç‰©ã€æ ¹æ®ç”»å›¾ç¼–ç ç½‘ç«™ã€è§£é‡Šå¹½é»˜æƒ…å†µç­‰ï¼‰ã€‚
- en: Running LLaVA locally
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœ¬åœ°è¿è¡ŒLLaVA
- en: LLaVA can also be installed on a local machine using [Ollama](https://ollama.ai/)
    or a Mozilla â€˜[llamafile](https://github.com/Mozilla-Ocho/llamafile)â€™. These tools
    can run on most CPU-only consumer-grade level machines, as the model only requires
    8GB of RAM and 4GB of free disk space, and was even shown to [successfully run
    on a Raspberry PI](/running-local-llms-and-vlms-on-the-raspberry-pi-57bd0059c41a).
    Among the tools and interfaces developed around the Ollama project, a notable
    initiative is the [Ollama-WebUI](https://github.com/ollama-webui/ollama-webui)
    (illustrated below), which reproduces the look and feel of OpenAI ChatGPT user
    interface.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: LLaVAè¿˜å¯ä»¥ä½¿ç”¨[Ollama](https://ollama.ai/)æˆ–Mozillaçš„â€˜[llamafile](https://github.com/Mozilla-Ocho/llamafile)â€™å®‰è£…åˆ°æœ¬åœ°è®¡ç®—æœºã€‚è¿™äº›å·¥å…·å¯ä»¥åœ¨å¤§å¤šæ•°ä»…é…å¤‡CPUçš„æ¶ˆè´¹çº§æœºå™¨ä¸Šè¿è¡Œï¼Œå› ä¸ºæ¨¡å‹ä»…éœ€è¦8GBå†…å­˜å’Œ4GBçš„å¯ç”¨ç£ç›˜ç©ºé—´ï¼Œç”šè‡³å·²ç»è¯æ˜å¯ä»¥åœ¨[Raspberry
    PIä¸ŠæˆåŠŸè¿è¡Œ](/running-local-llms-and-vlms-on-the-raspberry-pi-57bd0059c41a)ã€‚åœ¨å›´ç»•Ollamaé¡¹ç›®å¼€å‘çš„å·¥å…·å’Œæ¥å£ä¸­ï¼Œä¸€ä¸ªå€¼å¾—æ³¨æ„çš„é¡¹ç›®æ˜¯[Ollama-WebUI](https://github.com/ollama-webui/ollama-webui)ï¼ˆå¦‚ä¸‹å›¾æ‰€ç¤ºï¼‰ï¼Œå®ƒå¤ç°äº†OpenAI
    ChatGPTç”¨æˆ·ç•Œé¢çš„å¤–è§‚å’Œæ“ä½œä½“éªŒã€‚
- en: '![](../Images/da0916a9564113060e6305c3a0d43299.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da0916a9564113060e6305c3a0d43299.png)'
- en: '[Ollama Web user interface](https://github.com/ollama-webui/ollama-webui) â€”
    inspired by [OpenAI ChatGPT](https://chat.openai.com/)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ollama Webç”¨æˆ·ç•Œé¢](https://github.com/ollama-webui/ollama-webui) â€” å—[OpenAI ChatGPT](https://chat.openai.com/)çš„å¯å‘'
- en: Brief overview of LLaVAâ€™s main features
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLaVAä¸»è¦ç‰¹æ€§ç®€è¦æ¦‚è¿°
- en: LLaVA was designed by researchers from the University of Wisconsin-Madison,
    Microsoft Research and Columbia University, and was recently showcased at NeurIPS
    2023\. The projectâ€™s code and technical specifications can be accessed on its
    [Github repository](https://github.com/haotian-liu/LLaVA), which also offers various
    interfaces for interacting with the assistant.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: LLaVAæ˜¯ç”±å¨æ–¯åº·æ˜Ÿå¤§å­¦éº¦è¿ªé€Šåˆ†æ ¡ã€å¾®è½¯ç ”ç©¶é™¢å’Œå“¥ä¼¦æ¯”äºšå¤§å­¦çš„ç ”ç©¶äººå‘˜è®¾è®¡çš„ï¼Œå¹¶ä¸”æœ€è¿‘åœ¨NeurIPS 2023ä¸Šå±•ç¤ºã€‚è¯¥é¡¹ç›®çš„ä»£ç å’ŒæŠ€æœ¯è§„æ ¼å¯ä»¥é€šè¿‡[Githubä»“åº“](https://github.com/haotian-liu/LLaVA)è®¿é—®ï¼Œä»“åº“è¿˜æä¾›äº†ä¸åŠ©æ‰‹äº’åŠ¨çš„å„ç§æ¥å£ã€‚
- en: 'As the authors summarize in [their paperâ€™s abstract](https://arxiv.org/pdf/2310.03744.pdf):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä½œè€…åœ¨[è®ºæ–‡æ‘˜è¦](https://arxiv.org/pdf/2310.03744.pdf)ä¸­æ€»ç»“çš„ï¼š
- en: '[LLava] achieves state-of-the-art across 11 benchmarks. Our final 13B checkpoint
    uses merely 1.2M publicly available data, and finishes full training in ~1 day
    on a single 8-A100 node. We hope this can make state-of-the-art LMM research more
    accessible. Code and model will be publicly available.'
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[LLaVA] åœ¨11ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚æˆ‘ä»¬çš„æœ€ç»ˆ13Bæ£€æŸ¥ç‚¹ä»…ä½¿ç”¨äº†1.2Må…¬å¼€å¯ç”¨çš„æ•°æ®ï¼Œå¹¶ä¸”åœ¨å•ä¸ª8-A100èŠ‚ç‚¹ä¸Šå®Œæˆäº†å¤§çº¦1å¤©çš„å…¨è®­ç»ƒã€‚æˆ‘ä»¬å¸Œæœ›è¿™èƒ½ä½¿æœ€å…ˆè¿›çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ç ”ç©¶å˜å¾—æ›´åŠ æ˜“äºæ¥è§¦ã€‚ä»£ç å’Œæ¨¡å‹å°†å…¬å¼€å‘å¸ƒã€‚'
- en: The benchmark results, reported in the paper as the radar chart below, illustrate
    the improvements compared to other state-of-the-art models.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡ä¸­æŠ¥å‘Šçš„åŸºå‡†ç»“æœï¼Œå¦‚ä¸‹æ–¹çš„é›·è¾¾å›¾ï¼Œå±•ç¤ºäº†ä¸å…¶ä»–æœ€å…ˆè¿›æ¨¡å‹çš„å¯¹æ¯”æ”¹è¿›ã€‚
- en: '![](../Images/98a3010228400b1064aded781a0eee41.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/98a3010228400b1064aded781a0eee41.png)'
- en: Radar chart of LLaVAâ€™s benchmark results (image from [paper](https://arxiv.org/pdf/2304.08485.pdf))
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: LLaVAåŸºå‡†ç»“æœçš„é›·è¾¾å›¾ï¼ˆå›¾ç‰‡æ¥è‡ª[è®ºæ–‡](https://arxiv.org/pdf/2304.08485.pdf)ï¼‰
- en: Inner workings
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å†…éƒ¨å·¥ä½œæœºåˆ¶
- en: LLaVAâ€™s data processing workflow is conceptually simple. The model essentially
    works as a standard causal language model, taking language instructions (a user
    text prompt) as input, and returning a language response. The ability of the language
    model to handle images is allowed by a separate vision encoder model that converts
    images into language tokens, which are quietly added to the user text prompt (acting
    as a kind of [soft prompt](https://huggingface.co/docs/peft/main/en/conceptual_guides/prompting)).
    The LLaVA process is illustrated below.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: LLaVAçš„æ•°æ®å¤„ç†å·¥ä½œæµç¨‹åœ¨æ¦‚å¿µä¸Šéå¸¸ç®€å•ã€‚è¯¥æ¨¡å‹æœ¬è´¨ä¸Šä½œä¸ºä¸€ä¸ªæ ‡å‡†çš„å› æœè¯­è¨€æ¨¡å‹å·¥ä½œï¼Œæ¥å—è¯­è¨€æŒ‡ä»¤ï¼ˆç”¨æˆ·æ–‡æœ¬æç¤ºï¼‰ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›è¯­è¨€å“åº”ã€‚è¯­è¨€æ¨¡å‹å¤„ç†å›¾åƒçš„èƒ½åŠ›ç”±ä¸€ä¸ªç‹¬ç«‹çš„è§†è§‰ç¼–ç å™¨æ¨¡å‹æä¾›ï¼Œè¯¥æ¨¡å‹å°†å›¾åƒè½¬æ¢ä¸ºè¯­è¨€æ ‡è®°ï¼Œè¿™äº›æ ‡è®°è¢«æ‚„æ‚„åœ°æ·»åŠ åˆ°ç”¨æˆ·æ–‡æœ¬æç¤ºä¸­ï¼ˆå……å½“ä¸€ç§[è½¯æç¤º](https://huggingface.co/docs/peft/main/en/conceptual_guides/prompting)ï¼‰ã€‚LLaVAçš„å¤„ç†æµç¨‹å¦‚ä¸‹æ‰€ç¤ºã€‚
- en: '![](../Images/b68138aa94aba7d0da27c16272489945.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b68138aa94aba7d0da27c16272489945.png)'
- en: LLaVA network architecture (image from [paper](https://arxiv.org/pdf/2304.08485.pdf))
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: LLaVAç½‘ç»œæ¶æ„ï¼ˆå›¾ç‰‡æ¥è‡ª[è®ºæ–‡](https://arxiv.org/pdf/2304.08485.pdf)ï¼‰
- en: LLaVAâ€™s language model and vision encoder rely on two reference models called
    Vicuna and CLIP, respectively. [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/)
    is a pretrained large language model based on LLaMA-2 (designed by Meta) that
    boasts competitive performances with medium sized LLM (See model cards for the
    [7B](https://huggingface.co/lmsys/vicuna-7b-v1.5) and [13B](https://huggingface.co/lmsys/vicuna-13b-v1.5)
    versions on HuggingFace). [CLIP](https://openai.com/research/clip) is an image
    encoder designed by OpenAI, pretrained to encode images and text in a similar
    embedding space using **c**ontrastive **l**anguage-**i**mage **p**retraining (hence
    â€˜CLIPâ€™). The model used in LLaVA is the vision transformer variant CLIP-ViT-L/14
    (see its [model card](https://huggingface.co/openai/clip-vit-large-patch14) on
    HuggingFace).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: LLaVAçš„è¯­è¨€æ¨¡å‹å’Œè§†è§‰ç¼–ç å™¨åˆ†åˆ«ä¾èµ–äºä¸¤ä¸ªå‚è€ƒæ¨¡å‹ï¼Œç§°ä¸ºVicunaå’ŒCLIPã€‚[Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/)æ˜¯ä¸€ä¸ªåŸºäºLLaMA-2ï¼ˆç”±Metaè®¾è®¡ï¼‰çš„é¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰ä¸ä¸­å‹LLMç›¸åª²ç¾çš„ç«äº‰æ€§èƒ½ï¼ˆå‚è§HuggingFaceä¸Š[7B](https://huggingface.co/lmsys/vicuna-7b-v1.5)å’Œ[13B](https://huggingface.co/lmsys/vicuna-13b-v1.5)ç‰ˆæœ¬çš„æ¨¡å‹å¡ï¼‰ã€‚[CLIP](https://openai.com/research/clip)æ˜¯ç”±OpenAIè®¾è®¡çš„å›¾åƒç¼–ç å™¨ï¼Œç»è¿‡é¢„è®­ç»ƒï¼Œå¯ä»¥åœ¨ç›¸ä¼¼çš„åµŒå…¥ç©ºé—´ä¸­å¯¹å›¾åƒå’Œæ–‡æœ¬è¿›è¡Œç¼–ç ï¼Œä½¿ç”¨çš„æ˜¯**å¯¹æ¯”**è¯­è¨€-**å›¾åƒ**é¢„è®­ç»ƒï¼ˆå› æ­¤ç§°ä¸ºâ€˜CLIPâ€™ï¼‰ã€‚LLaVAä¸­ä½¿ç”¨çš„æ¨¡å‹æ˜¯è§†è§‰å˜æ¢å™¨å˜ç§CLIP-ViT-L/14ï¼ˆæŸ¥çœ‹å…¶åœ¨HuggingFaceä¸Šçš„[æ¨¡å‹å¡](https://huggingface.co/openai/clip-vit-large-patch14)ï¼‰ã€‚
- en: To match the dimension of the vision encoder with those of the language model,
    a projection module (**W** in the image above) is applied. It is a simple linear
    projection in the original [LLaVA](https://arxiv.org/abs/2304.08485), and a two-layer
    perceptron in [LLaVA 1.5](https://arxiv.org/abs/2310.03744).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åŒ¹é…è§†è§‰ç¼–ç å™¨çš„ç»´åº¦ä¸è¯­è¨€æ¨¡å‹çš„ç»´åº¦ï¼Œä¸€ä¸ªæŠ•å½±æ¨¡å—ï¼ˆå¦‚ä¸Šå›¾ä¸­çš„**W**ï¼‰è¢«åº”ç”¨ã€‚å®ƒåœ¨åŸå§‹çš„[LLaVA](https://arxiv.org/abs/2304.08485)ä¸­æ˜¯ä¸€ä¸ªç®€å•çš„çº¿æ€§æŠ•å½±ï¼Œåœ¨[LLaVA
    1.5](https://arxiv.org/abs/2310.03744)ä¸­æ˜¯ä¸€ä¸ªä¸¤å±‚æ„ŸçŸ¥æœºã€‚
- en: Training process
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒè¿‡ç¨‹
- en: The training process of LLaVA consists of two relatively simple stages.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: LLaVAçš„è®­ç»ƒè¿‡ç¨‹ç”±ä¸¤ä¸ªç›¸å¯¹ç®€å•çš„é˜¶æ®µç»„æˆã€‚
- en: The first stage solely aims at tuning the projection module **W**, and the weights
    of the vision encoder and LLM are kept frozen. The training is performed using
    a subset of around 600k image/caption pairs from the [CC3M conceptual caption
    dataset](https://ai.google.com/research/ConceptualCaptions/), and is available
    on HuggingFace [in this repository](https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€é˜¶æ®µä»…ä»…æ˜¯ä¸ºäº†è°ƒä¼˜æŠ•å½±æ¨¡å—**W**ï¼Œè€Œè§†è§‰ç¼–ç å™¨å’ŒLLMçš„æƒé‡ä¿æŒå†»ç»“ã€‚è®­ç»ƒä½¿ç”¨æ¥è‡ª[CC3Mæ¦‚å¿µæ€§å­—å¹•æ•°æ®é›†](https://ai.google.com/research/ConceptualCaptions/)çš„çº¦60ä¸‡å¯¹å›¾åƒ/å­—å¹•æ•°æ®çš„å­é›†è¿›è¡Œï¼Œå¹¶ä¸”å¯ä»¥åœ¨HuggingFaceçš„[æ­¤ä»“åº“](https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K)ä¸­æ‰¾åˆ°ã€‚
- en: In a second stage, the projection module weigths **W** are fine-tuned together
    with the LLM weights (while keeping the vision encoderâ€™s weights frozen), using
    dataset of 158K language-image instruction-following data. The data is generated
    using GPT4, and feature examples of conversations, detailed descriptions and complex
    reasonings, and is available on HuggingFace [in this repository](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬äºŒé˜¶æ®µï¼ŒæŠ•å½±æ¨¡å—çš„æƒé‡**W**ä¸LLMçš„æƒé‡ä¸€èµ·è¿›è¡Œå¾®è°ƒï¼ˆåŒæ—¶ä¿æŒè§†è§‰ç¼–ç å™¨çš„æƒé‡å†»ç»“ï¼‰ï¼Œä½¿ç”¨åŒ…å«158Kè¯­è¨€-å›¾åƒæŒ‡ä»¤è·Ÿéšæ•°æ®çš„è®­ç»ƒé›†ã€‚è¿™äº›æ•°æ®æ˜¯é€šè¿‡GPT-4ç”Ÿæˆçš„ï¼ŒåŒ…å«å¯¹è¯ã€è¯¦ç»†æè¿°å’Œå¤æ‚æ¨ç†çš„ç¤ºä¾‹ï¼Œå¹¶ä¸”å¯ä»¥åœ¨HuggingFaceçš„[æ­¤ä»“åº“](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K)ä¸­æ‰¾åˆ°ã€‚
- en: The whole training takes around a day using eight A100 GPUs.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹å¤§çº¦éœ€è¦ä¸€å¤©æ—¶é—´ï¼Œä½¿ç”¨å…«ä¸ªA100 GPUã€‚
- en: 'Programming with LLaVA: How to get started'
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨LLaVAè¿›è¡Œç¼–ç¨‹ï¼šå¦‚ä½•å¼€å§‹
- en: '*Code available on the* [*Colab related notebook*](https://colab.research.google.com/drive/1L28bJX14-Y5lJvswYwydsletYFMIxVH5)*.*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*ä»£ç å¯åœ¨* [*Colabç›¸å…³ç¬”è®°æœ¬*](https://colab.research.google.com/drive/1L28bJX14-Y5lJvswYwydsletYFMIxVH5)*.* '
- en: The LLaVA model is integrated in the Transformers library, and can be loaded
    using the standard *pipeline* object. The 7B and 13B variants of the models are
    available on the [LLaVA ğŸ˜Š Hub space](https://huggingface.co/llava-hf), and may
    be loaded in 4 and 8 bits to save GPU memory. We illustrate below how to load
    and run model using code that can be executed on Colab with a T4 TPU (15GB RAM
    GPU).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: LLaVAæ¨¡å‹å·²é›†æˆåœ¨Transformersåº“ä¸­ï¼Œå¯ä»¥é€šè¿‡æ ‡å‡†çš„*pipeline*å¯¹è±¡åŠ è½½ã€‚7Bå’Œ13Bç‰ˆæœ¬çš„æ¨¡å‹å¯ä»¥åœ¨[LLaVA ğŸ˜Š Hubç©ºé—´](https://huggingface.co/llava-hf)ä¸Šæ‰¾åˆ°ï¼Œå¹¶å¯ä»¥ä»¥4ä½å’Œ8ä½åŠ è½½ï¼Œä»¥èŠ‚çœGPUå†…å­˜ã€‚ä¸‹é¢æˆ‘ä»¬å±•ç¤ºå¦‚ä½•ä½¿ç”¨ä»£ç åŠ è½½å¹¶è¿è¡Œæ¨¡å‹ï¼Œä»£ç å¯ä»¥åœ¨Colabä¸Šä½¿ç”¨T4
    TPUï¼ˆ15GBå†…å­˜GPUï¼‰æ‰§è¡Œã€‚
- en: 'Below is the code snippet to load the 7B variant of LLaVA 1.5 in 4 bits:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯åŠ è½½4ä½LLaVA 1.5 7Bç‰ˆæœ¬çš„ä»£ç ç‰‡æ®µï¼š
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let us then load this picture
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åè®©æˆ‘ä»¬åŠ è½½è¿™å¼ å›¾ç‰‡
- en: '![](../Images/ce1327ce8af3bcc9107ecbaba49c33c1.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce1327ce8af3bcc9107ecbaba49c33c1.png)'
- en: 'We use the standard PIL library for loading the picture:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨æ ‡å‡†çš„PILåº“æ¥åŠ è½½è¿™å¼ å›¾ç‰‡ï¼š
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Let us finally query the LLaVA model with the image, with a prompt asking to
    describe the picture.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨å›¾åƒæŸ¥è¯¢LLaVAæ¨¡å‹ï¼Œå¹¶æä¾›ä¸€ä¸ªæç¤ºè¯·æ±‚æè¿°è¿™å¼ å›¾ç‰‡ã€‚
- en: 'Note: [The format for the prompt](https://huggingface.co/docs/transformers/model_doc/llava)
    follows'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼š[æç¤ºçš„æ ¼å¼](https://huggingface.co/docs/transformers/model_doc/llava)å¦‚ä¸‹
- en: 'â€œUSER: <image>\n<prompt>\nASSISTANT:â€'
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'â€œUSER: <image>\n<prompt>\nASSISTANT:â€'
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Which returns the following answer:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›ä»¥ä¸‹ç­”æ¡ˆï¼š
- en: 'USER: Describe this picture'
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'USER: æè¿°è¿™å¼ å›¾ç‰‡'
- en: 'â€‹ASSISTANT: â€‹The image features a large, empty amphitheater with a stunning
    view of the ocean in the background. The amphitheater is surrounded by a lush
    green hillside, and a majestic mountain can be seen in the distance. The scene
    is serene and picturesque, with the sun shining brightly over the landscape.'
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'â€‹ASSISTANT: â€‹è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€ä¸ªå¤§å‹ã€ç©ºæ—·çš„å‰§åœºï¼ŒèƒŒæ™¯æ˜¯å£®ä¸½çš„æµ·æ´‹æ™¯è‰²ã€‚å‰§åœºå››å‘¨è¢«éƒéƒè‘±è‘±çš„ç»¿è‰²å±±å¡ç¯ç»•ï¼Œè¿œå¤„å¯ä»¥çœ‹åˆ°ä¸€åº§é›„ä¼Ÿçš„å±±è„‰ã€‚æ•´ä¸ªåœºæ™¯å®é™è€Œå¦‚ç”»ï¼Œé˜³å…‰æ˜åªšåœ°ç…§è€€åœ¨å¤§åœ°ä¸Šã€‚'
- en: LLaVA chatbot
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLaVAèŠå¤©æœºå™¨äºº
- en: Let us finally create a simple chatbot that relies on a LLaVA model. We will
    use the [Gradio library](https://www.gradio.app/), which provides a fast and easy
    way to create machine learning web interfaces.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç®€å•çš„èŠå¤©æœºå™¨äººï¼Œä¾èµ–äºLLaVAæ¨¡å‹ã€‚æˆ‘ä»¬å°†ä½¿ç”¨[Gradioåº“](https://www.gradio.app/)ï¼Œå®ƒæä¾›äº†ä¸€ç§å¿«é€Ÿç®€ä¾¿çš„æ–¹æ³•æ¥åˆ›å»ºæœºå™¨å­¦ä¹ Webç•Œé¢ã€‚
- en: The core for the interface consists of a row with an image uploader (a Gradio
    Image object), and a chat interface (a Gradio [ChatInterface](https://www.gradio.app/docs/chatinterface)
    object).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ç•Œé¢çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªåŒ…å«å›¾åƒä¸Šä¼ å™¨ï¼ˆGradioå›¾åƒå¯¹è±¡ï¼‰å’ŒèŠå¤©ç•Œé¢ï¼ˆGradio [ChatInterface](https://www.gradio.app/docs/chatinterface)å¯¹è±¡ï¼‰çš„è¡Œã€‚
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The chat interface connects to a function *update_conversation*, that takes
    care of keeping the conversation history, and calling the LLaVA model for a response
    whenever the user sends a message.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: èŠå¤©ç•Œé¢è¿æ¥åˆ°ä¸€ä¸ª*update_conversation*å‡½æ•°ï¼Œè¯¥å‡½æ•°è´Ÿè´£ä¿æŒå¯¹è¯å†å²è®°å½•ï¼Œå¹¶åœ¨ç”¨æˆ·å‘é€æ¶ˆæ¯æ—¶è°ƒç”¨LLaVAæ¨¡å‹ç”Ÿæˆå“åº”ã€‚
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The interface is launched calling the *launch* method.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ç•Œé¢æ˜¯é€šè¿‡è°ƒç”¨*launch*æ–¹æ³•å¯åŠ¨çš„ã€‚
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After a few seconds, the chatbot Web interface will appear:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å‡ ç§’é’Ÿåï¼ŒèŠå¤©æœºå™¨äººWebç•Œé¢å°†å‡ºç°ï¼š
- en: '![](../Images/05629290b6a0cdaaf30927ff858868a6.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05629290b6a0cdaaf30927ff858868a6.png)'
- en: Congratulations, your LLaVA chatbot is now up and running!
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œï¼Œä½ çš„LLaVAèŠå¤©æœºå™¨äººå·²ç»å¯åŠ¨å¹¶è¿è¡Œï¼
- en: Useful links
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœ‰ç”¨çš„é“¾æ¥
- en: '[HuggingFace LLaVA model documentation](https://huggingface.co/docs/transformers/model_doc/llava)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[HuggingFace LLaVAæ¨¡å‹æ–‡æ¡£](https://huggingface.co/docs/transformers/model_doc/llava)'
- en: '[Llava Hugging Face organization](https://huggingface.co/llava-hf)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Llava Hugging Faceç»„ç»‡](https://huggingface.co/llava-hf)'
- en: 'Loading and running LLaVA with AutoPrecessor and LLaVAForConditionalGeneration:
    [Colab notebook](https://colab.research.google.com/drive/1_q7cOB-jCu3RExrkhrgewBR0qKjZr-Sx)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨AutoPrecessorå’ŒLLaVAForConditionalGenerationåŠ è½½å’Œè¿è¡ŒLLaVAï¼š[Colabç¬”è®°æœ¬](https://colab.research.google.com/drive/1_q7cOB-jCu3RExrkhrgewBR0qKjZr-Sx)
- en: '[GPT-4V(ision) system card](https://cdn.openai.com/papers/GPTV_System_Card.pdf)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GPT-4V(ision)ç³»ç»Ÿå¡ç‰‡](https://cdn.openai.com/papers/GPTV_System_Card.pdf)'
- en: '[Understanding Visual Instruction Tuning](https://newsletter.artofsaience.com/p/understanding-visual-instruction)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ç†è§£è§†è§‰æŒ‡ä»¤å¾®è°ƒ](https://newsletter.artofsaience.com/p/understanding-visual-instruction)'
- en: 'Note: Unless otherwise noted, all images are by the author.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šé™¤éå¦æœ‰è¯´æ˜ï¼Œæ‰€æœ‰å›¾ç‰‡å‡ç”±ä½œè€…æä¾›ã€‚
