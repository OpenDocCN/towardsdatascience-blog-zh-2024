- en: The Ultimate Guide to Vision Transformers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Vision Transformer终极指南
- en: 原文：[https://towardsdatascience.com/the-ultimate-guide-to-vision-transformers-0a6df32cb248?source=collection_archive---------8-----------------------#2024-08-30](https://towardsdatascience.com/the-ultimate-guide-to-vision-transformers-0a6df32cb248?source=collection_archive---------8-----------------------#2024-08-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-ultimate-guide-to-vision-transformers-0a6df32cb248?source=collection_archive---------8-----------------------#2024-08-30](https://towardsdatascience.com/the-ultimate-guide-to-vision-transformers-0a6df32cb248?source=collection_archive---------8-----------------------#2024-08-30)
- en: A comprehensive guide to the Vision Transformer (ViT) that revolutionized computer
    vision
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 《Vision Transformer（ViT）全面指南》，它革新了计算机视觉
- en: '[](https://medium.com/@francoisporcher?source=post_page---byline--0a6df32cb248--------------------------------)[![François
    Porcher](../Images/9ddb233f8cadbd69026bd79e2bd62dea.png)](https://medium.com/@francoisporcher?source=post_page---byline--0a6df32cb248--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0a6df32cb248--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0a6df32cb248--------------------------------)
    [François Porcher](https://medium.com/@francoisporcher?source=post_page---byline--0a6df32cb248--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@francoisporcher?source=post_page---byline--0a6df32cb248--------------------------------)[![François
    Porcher](../Images/9ddb233f8cadbd69026bd79e2bd62dea.png)](https://medium.com/@francoisporcher?source=post_page---byline--0a6df32cb248--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0a6df32cb248--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0a6df32cb248--------------------------------)
    [François Porcher](https://medium.com/@francoisporcher?source=post_page---byline--0a6df32cb248--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0a6df32cb248--------------------------------)
    ·7 min read·Aug 30, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0a6df32cb248--------------------------------)
    ·阅读时间：7分钟·2024年8月30日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Hi everyone! For those who do not know me yet, my name is Francois, I am a Research
    Scientist at Meta. I have a passion for explaining advanced AI concepts and making
    them more accessible.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 大家好！对于还不认识我的朋友们，我叫Francois，是Meta的研究科学家。我热衷于解释先进的AI概念，并使其更加易懂。
- en: 'Today, let’s dive into one of the most significant contribution in the field
    of Computer Vision: the **Vision Transformer (ViT).**'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，我们来深入探讨计算机视觉领域的一项重大贡献：**Vision Transformer（ViT）**。
- en: '![](../Images/2008420b7885ebccbdfac2394b8c4f6d.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2008420b7885ebccbdfac2394b8c4f6d.png)'
- en: Converting an image into patches, image by author
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 将图像转换为图块，图像来源：作者
- en: A bit of history first..
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 先来一点历史背景……
- en: The Vision Transformer was introduced by Alexey Dosovitskiy and al. (Google
    Brain) in 2021 in the paper [An Image is worth 16×16 words](https://arxiv.org/abs/2010.11929).
    At the time, Transformers had shown to be the key to unlock great performance
    on NLP tasks, introduced in the must paper [Attention is All you Need](https://arxiv.org/abs/1706.03762)
    in 2017.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Vision Transformer（ViT）由Alexey Dosovitskiy及其团队（Google Brain）在2021年提出，相关论文为[《一张图胜过16×16个词》](https://arxiv.org/abs/2010.11929)。当时，Transformers已证明是解锁NLP任务卓越性能的关键，且这一模型首次在2017年的标志性论文[《Attention
    is All You Need》](https://arxiv.org/abs/1706.03762)中提出。
- en: Between 2017 and 2021, there were several attempts to integrate the attention
    mechanism into Convolutional Neural Networks (CNNs). However, these were mostly
    hybrid models (combining CNN layers with attention layers) and lacked scalability.
    Google addressed this by completely eliminating convolutions and leveraging their
    computational power to scale the model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在2017到2021年间，已有多次尝试将注意力机制整合到卷积神经网络（CNN）中。然而，这些大多是混合模型（结合了CNN层和注意力层），并且缺乏可扩展性。Google通过完全去除卷积操作，利用其计算能力来扩展模型，解决了这一问题。
- en: The million-dollar question this article answered is..
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这篇文章所回答的百万美元问题是……
