- en: 'Rainbow: The Colorful Evolution of Deep Q-Networks ğŸŒˆ'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Rainbowï¼šæ·±åº¦Qç½‘ç»œçš„å¤šå½©è¿›åŒ– ğŸŒˆ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/rainbow-the-colorful-evolution-of-deep-q-networks-37e662ab99b2?source=collection_archive---------8-----------------------#2024-07-12](https://towardsdatascience.com/rainbow-the-colorful-evolution-of-deep-q-networks-37e662ab99b2?source=collection_archive---------8-----------------------#2024-07-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/rainbow-the-colorful-evolution-of-deep-q-networks-37e662ab99b2?source=collection_archive---------8-----------------------#2024-07-12](https://towardsdatascience.com/rainbow-the-colorful-evolution-of-deep-q-networks-37e662ab99b2?source=collection_archive---------8-----------------------#2024-07-12)
- en: Everything you need to assemble the DQN Megazord in JAX.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»„è£…DQN Megazordæ‰€éœ€çš„æ‰€æœ‰å†…å®¹ï¼Œä½¿ç”¨JAXã€‚
- en: '[](https://medium.com/@ryanpegoud?source=post_page---byline--37e662ab99b2--------------------------------)[![Ryan
    PÃ©goud](../Images/9314b76c2be56bda8b73b4badf9e3e4d.png)](https://medium.com/@ryanpegoud?source=post_page---byline--37e662ab99b2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--37e662ab99b2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--37e662ab99b2--------------------------------)
    [Ryan PÃ©goud](https://medium.com/@ryanpegoud?source=post_page---byline--37e662ab99b2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ryanpegoud?source=post_page---byline--37e662ab99b2--------------------------------)[![Ryan
    PÃ©goud](../Images/9314b76c2be56bda8b73b4badf9e3e4d.png)](https://medium.com/@ryanpegoud?source=post_page---byline--37e662ab99b2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--37e662ab99b2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--37e662ab99b2--------------------------------)
    [Ryan PÃ©goud](https://medium.com/@ryanpegoud?source=post_page---byline--37e662ab99b2--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--37e662ab99b2--------------------------------)
    Â·17 min readÂ·Jul 12, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--37e662ab99b2--------------------------------)
    Â·é˜…è¯»æ—¶é•¿17åˆ†é’ŸÂ·2024å¹´7æœˆ12æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/9c3ff5fae978b121c7db85b75b3e992f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c3ff5fae978b121c7db85b75b3e992f.png)'
- en: â€œThe Rainbow Megazordâ€, Dall-E 3
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: â€œRainbow Megazordâ€ï¼ŒDall-E 3
- en: In 2013, the introduction of Deep Q-Networks (DQN) by *Mnih et al.*[1]marked
    the first breakthrough in Deep Reinforcement Learning, surpassing expert human
    players in three Atari games. Over the years, several variants of DQN were published,
    each improving on specific weaknesses of the original algorithm.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 2013å¹´ï¼Œ*Mnihç­‰äºº*æå‡ºçš„æ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰[1]æ ‡å¿—ç€æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„é¦–æ¬¡çªç ´ï¼Œåœ¨ä¸‰æ¬¾Atariæ¸¸æˆä¸­è¶…è¶Šäº†ä¸“å®¶çº§äººç±»ç©å®¶ã€‚å¤šå¹´æ¥ï¼ŒDQNçš„å¤šä¸ªå˜ç§é™†ç»­å‘å¸ƒï¼Œæ¯ä¸ªå˜ç§éƒ½åœ¨æ”¹è¿›åŸå§‹ç®—æ³•çš„ç‰¹å®šå¼±ç‚¹ã€‚
- en: 'In 2017, *Hessel et al.*[2]made the best out of the DQN palette by combining
    6 of its powerful variants, crafting what could be called the DQN Megazord: Rainbow.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 2017å¹´ï¼Œ*Hesselç­‰äºº* [2]é€šè¿‡ç»“åˆDQNçš„6ç§å¼ºå¤§å˜ç§ï¼Œåˆ›é€ å‡ºäº†è¢«ç§°ä¸ºDQN Megazordçš„Rainbowã€‚
- en: In this article, weâ€™ll break down the individual components that make up Rainbow,
    while reviewing their JAX implementations in the [**Stoix library.**](https://github.com/EdanToledo/Stoix)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†é€ä¸€è§£ææ„æˆRainbowçš„å„ä¸ªç»„æˆéƒ¨åˆ†ï¼Œå¹¶å›é¡¾å®ƒä»¬åœ¨[**Stoixåº“ä¸­çš„JAXå®ç°**](https://github.com/EdanToledo/Stoix)ã€‚
- en: DQN
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DQN
- en: The fundamental building block of Rainbow is DQN, an extension of Q-learning
    using a neural network with parameters **Î¸** to approximate the Q-function (i.e.
    action-value function). In particular, DQN uses convolutional layers to extract
    features from images and a linear layer to produce a scalar estimate of the Q-value.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Rainbowçš„åŸºæœ¬æ„å»ºå—æ˜¯DQNï¼Œå®ƒæ˜¯Q-learningçš„æ‰©å±•ï¼Œä½¿ç”¨å¸¦æœ‰å‚æ•°**Î¸**çš„ç¥ç»ç½‘ç»œæ¥é€¼è¿‘Qå‡½æ•°ï¼ˆå³åŠ¨ä½œ-ä»·å€¼å‡½æ•°ï¼‰ã€‚å…·ä½“æ¥è¯´ï¼ŒDQNä½¿ç”¨å·ç§¯å±‚ä»å›¾åƒä¸­æå–ç‰¹å¾ï¼Œå¹¶é€šè¿‡çº¿æ€§å±‚ç”ŸæˆQå€¼çš„æ ‡é‡ä¼°è®¡ã€‚
- en: During training, the network parameterized by **Î¸**, referred to as the *â€œonline
    networkâ€* is used to select actions while the *â€œtarget networkâ€* parameterized
    by **Î¸-** is a delayed copy of the online network used to provide stable targets.
    This way, the targets are not dependent on the parameters being updated.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç½‘ç»œç”±**Î¸**å‚æ•°åŒ–ï¼Œç§°ä¸º*â€œåœ¨çº¿ç½‘ç»œâ€*ï¼Œç”¨äºé€‰æ‹©åŠ¨ä½œï¼Œè€Œ*â€œç›®æ ‡ç½‘ç»œâ€*ç”±**Î¸-**å‚æ•°åŒ–ï¼Œæ˜¯åœ¨çº¿ç½‘ç»œçš„å»¶è¿Ÿå‰¯æœ¬ï¼Œç”¨äºæä¾›ç¨³å®šçš„ç›®æ ‡ã€‚è¿™æ ·ï¼Œç›®æ ‡å°±ä¸ä¾èµ–äºæ­£åœ¨æ›´æ–°çš„å‚æ•°ã€‚
- en: Additionally, DQN uses a replay buffer ***D*** to sample past transitions (observations,
    reward, and done flag tuples) to train on at fixed intervals.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼ŒDQNä½¿ç”¨å›æ”¾ç¼“å†²åŒº***D***æ¥é‡‡æ ·è¿‡å»çš„è½¬ç§»ï¼ˆè§‚å¯Ÿã€å¥–åŠ±å’Œå®Œæˆæ ‡å¿—å…ƒç»„ï¼‰ï¼Œå¹¶åœ¨å›ºå®šçš„æ—¶é—´é—´éš”å†…è¿›è¡Œè®­ç»ƒã€‚
- en: 'At each iteration ***i***, DQN samples a transition ***j*** and takes a gradient
    step on the following loss:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¯æ¬¡è¿­ä»£***i***ä¸­ï¼ŒDQNé‡‡æ ·ä¸€æ¬¡è½¬ç§»***j***å¹¶æ ¹æ®ä»¥ä¸‹æŸå¤±å‡½æ•°è¿›è¡Œæ¢¯åº¦æ›´æ–°ï¼š
- en: '![](../Images/b62e326d7b69d832ec425e3d753b56e8.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b62e326d7b69d832ec425e3d753b56e8.png)'
- en: DQN loss function, all images are made by the author, unless specified otherwise
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: DQN æŸå¤±å‡½æ•°ï¼Œé™¤éå¦æœ‰è¯´æ˜ï¼Œå¦åˆ™æ‰€æœ‰å›¾åƒå‡ç”±ä½œè€…åˆ¶ä½œ
- en: This loss aims at minimizing the expectation of the squared temporal-difference
    (TD) error.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæŸå¤±æ—¨åœ¨æœ€å°åŒ–å¹³æ–¹æ—¶åºå·®åˆ†ï¼ˆTDï¼‰è¯¯å·®çš„æœŸæœ›å€¼ã€‚
- en: Note that DQN is an **off-policy** algorithm because it learns the optimal policy
    defined by the **maximum Q-value** term while following a different behavior policy,
    such as an epsilon-greedy policy.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼ŒDQN æ˜¯ä¸€ä¸ª**éç­–ç•¥**ç®—æ³•ï¼Œå› ä¸ºå®ƒåœ¨éµå¾ªä¸åŒçš„è¡Œä¸ºç­–ç•¥ï¼ˆå¦‚ epsilon è´ªå¿ƒç­–ç•¥ï¼‰çš„åŒæ—¶ï¼Œå­¦ä¹ ç”±**æœ€å¤§ Q å€¼**é¡¹å®šä¹‰çš„æœ€ä¼˜ç­–ç•¥ã€‚
- en: 'Hereâ€™s the DQN algorithm in detail:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ DQN ç®—æ³•çš„è¯¦ç»†å†…å®¹ï¼š
- en: '![](../Images/baf5816cef9399a587418e16b013d99a.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/baf5816cef9399a587418e16b013d99a.png)'
- en: DQN algorithm
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: DQN ç®—æ³•
- en: DQN in practice
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DQN å®è·µ
- en: As mentioned above, weâ€™ll reference code snippets from the Stoix library to
    illustrate the core parts of DQN and Rainbow *(some of the code was slightly edited
    or commented for pedagogical purposes)*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä¸Šæ‰€è¿°ï¼Œæˆ‘ä»¬å°†å¼•ç”¨ Stoix åº“ä¸­çš„ä»£ç ç‰‡æ®µæ¥è¯´æ˜ DQN å’Œ Rainbow çš„æ ¸å¿ƒéƒ¨åˆ†ï¼ˆ*éƒ¨åˆ†ä»£ç å·²ç¨ä½œç¼–è¾‘æˆ–æ³¨é‡Šï¼Œä¾¿äºæ•™å­¦*ï¼‰ã€‚
- en: 'Letâ€™s start with the neural network: Stoix lets us break down our model architecture
    into a pre-processor and a post-processor, referred to as **torso** and **head**
    respectively. In the case of DQN, the torso would be a multi-layer perceptron
    (MLP) or convolutional neural network (CNN) and the head an epsilon greedy policy,
    both implemented as [**Flax**](https://flax.readthedocs.io/en/latest/index.html)
    modules:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»ç¥ç»ç½‘ç»œå¼€å§‹ï¼šStoix è®©æˆ‘ä»¬å°†æ¨¡å‹æ¶æ„æ‹†è§£ä¸ºé¢„å¤„ç†å™¨å’Œåå¤„ç†å™¨ï¼Œåˆ†åˆ«ç§°ä¸º**èº¯å¹²**å’Œ**å¤´éƒ¨**ã€‚åœ¨ DQN çš„æƒ…å†µä¸‹ï¼Œèº¯å¹²å°†æ˜¯ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰æˆ–å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼Œå¤´éƒ¨åˆ™æ˜¯ä¸€ä¸ª
    epsilon è´ªå¿ƒç­–ç•¥ï¼Œä¸¤è€…éƒ½å®ç°ä¸º [**Flax**](https://flax.readthedocs.io/en/latest/index.html)
    æ¨¡å—ï¼š
- en: A Q-Network, defined as a CNN Torso and an Epsilon-Greedy policy in Stoix
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª Q ç½‘ç»œï¼Œå®šä¹‰ä¸º Stoix ä¸­çš„ CNN èº¯å¹²å’Œ epsilon è´ªå¿ƒç­–ç•¥
- en: 'Additionally, DQN uses the following loss (*note that Stoix follows the* [***Rlax***](https://github.com/google-deepmind/rlax)*naming
    conventions, therefore tm1 is equivalent to timestep t in the above equations,
    while t refers to timestep t+1*):'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼ŒDQN ä½¿ç”¨ä»¥ä¸‹æŸå¤±ï¼ˆ*è¯·æ³¨æ„ Stoix éµå¾ª* [***Rlax***](https://github.com/google-deepmind/rlax)*å‘½åçº¦å®šï¼Œå› æ­¤
    tm1 ç›¸å½“äºä¸Šè¿°å…¬å¼ä¸­çš„æ—¶é—´æ­¥ tï¼Œè€Œ t åˆ™æŒ‡æ—¶é—´æ­¥ t+1*ï¼‰ï¼š
- en: The Q-learning loss used in the context of DQN
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: DQN ä¸­ä½¿ç”¨çš„ Q-learning æŸå¤±
- en: The Rainbow blueprint
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Rainbow è“å›¾
- en: Now that we have laid the foundations for DQN, weâ€™ll review each part of the
    algorithm in more detail, while identifying potential weaknesses and how they
    are addressed by Rainbow.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»ä¸º DQN æ‰“ä¸‹äº†åŸºç¡€ï¼Œæˆ‘ä»¬å°†æ›´è¯¦ç»†åœ°å›é¡¾ç®—æ³•çš„å„ä¸ªéƒ¨åˆ†ï¼ŒåŒæ—¶è¯†åˆ«æ½œåœ¨çš„å¼±ç‚¹ä»¥åŠ Rainbow å¦‚ä½•è§£å†³è¿™äº›é—®é¢˜ã€‚
- en: 'In particular, weâ€™ll cover:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å°†æ¶µç›–ï¼š
- en: Double DQN and the overestimation bias
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŒé‡ DQN å’Œè¿‡é«˜ä¼°è®¡åå·®
- en: Dueling DQN and the state-value / advantage prediction
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹æŠ— DQN å’ŒçŠ¶æ€å€¼ / ä¼˜åŠ¿é¢„æµ‹
- en: Distributional DQN and the return distribution
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ†å¸ƒå¼ DQN å’Œå›æŠ¥åˆ†å¸ƒ
- en: Multi-step learning
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¤šæ­¥å­¦ä¹ 
- en: Noisy DQN and flexible exploration strategies
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å™ªå£° DQN å’Œçµæ´»çš„æ¢ç´¢ç­–ç•¥
- en: Prioritized Experience Replay and learning potential
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¼˜å…ˆç»éªŒå›æ”¾ä¸å­¦ä¹ æ½œåŠ›
- en: '![](../Images/20275c2f4f6e79e1dc782a0d2beb1f50.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20275c2f4f6e79e1dc782a0d2beb1f50.png)'
- en: The Rainbow Blueprint, Dall-E 3
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Rainbow è“å›¾ï¼ŒDall-E 3
- en: Double DQN
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŒé‡ DQN
- en: '**Source:** [*Deep Reinforcement Learning with Double Q-learning*](http://arxiv.org/abs/1509.06461)[3]'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¥æºï¼š** [*Deep Reinforcement Learning with Double Q-learning*](http://arxiv.org/abs/1509.06461)[3]'
- en: '**Improvement:** Reduced overestimation bias'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ”¹è¿›ï¼š** é™ä½è¿‡é«˜ä¼°è®¡åå·®'
- en: The overestimation bias
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¿‡é«˜ä¼°è®¡åå·®
- en: 'One issue with the loss function used in vanilla DQN arises from the Q-target.
    Remember that we define the target as:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹ DQN ä½¿ç”¨çš„æŸå¤±å‡½æ•°å­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼Œè¿™ä¸ªé—®é¢˜æ¥è‡ª Q ç›®æ ‡ã€‚è®°ä½ï¼Œæˆ‘ä»¬å°†ç›®æ ‡å®šä¹‰ä¸ºï¼š
- en: '![](../Images/3bd664e7d9b3dd60aff3bb4a501b9bb6.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3bd664e7d9b3dd60aff3bb4a501b9bb6.png)'
- en: Objective in the DQN loss
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: DQN æŸå¤±ä¸­çš„ç›®æ ‡
- en: This objective may lead to an **overestimation bias**. Indeed, as DQN uses bootstrapping
    (learning estimates from estimates), the max term may select overestimated values
    to update the Q-function, leading to overestimated Q-values.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç›®æ ‡å¯èƒ½ä¼šå¯¼è‡´**è¿‡é«˜ä¼°è®¡åå·®**ã€‚å®é™…ä¸Šï¼Œç”±äº DQN ä½¿ç”¨å¼•å¯¼ï¼ˆä»ä¼°è®¡ä¸­å­¦ä¹ ä¼°è®¡ï¼‰ï¼Œæœ€å¤§å€¼é¡¹å¯èƒ½ä¼šé€‰æ‹©è¿‡é«˜ä¼°è®¡çš„å€¼æ¥æ›´æ–° Q å‡½æ•°ï¼Œå¯¼è‡´ Q å€¼çš„è¿‡é«˜ä¼°è®¡ã€‚
- en: 'As an example, consider the following figure:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œè€ƒè™‘ä»¥ä¸‹å›¾ç¤ºï¼š
- en: The Q-values predicted by the network are represented in blue.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç½‘ç»œé¢„æµ‹çš„ Q å€¼ç”¨è“è‰²è¡¨ç¤ºã€‚
- en: The true Q-values are represented in purple.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: çœŸå®çš„ Q å€¼ç”¨ç´«è‰²è¡¨ç¤ºã€‚
- en: The gap between the predictions and true values is represented by red arrows.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¢„æµ‹å€¼å’ŒçœŸå®å€¼ä¹‹é—´çš„å·®è·é€šè¿‡çº¢è‰²ç®­å¤´è¡¨ç¤ºã€‚
- en: In this case, action 0 has the highest predicted Q-value because of a large
    prediction error. This value will therefore be used to construct the target.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç”±äºè¾ƒå¤§çš„é¢„æµ‹è¯¯å·®ï¼ŒåŠ¨ä½œ 0 çš„é¢„æµ‹ Q å€¼æœ€é«˜ã€‚å› æ­¤ï¼Œå°†ä½¿ç”¨è¿™ä¸ªå€¼æ¥æ„å»ºç›®æ ‡ã€‚
- en: However, the action with the highest true value is action 2\. This illustration
    shows how the max term in the target favors **large positive estimation errors**,
    inducing an overestimation bias.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå…·æœ‰æœ€é«˜çœŸå®ä»·å€¼çš„åŠ¨ä½œæ˜¯åŠ¨ä½œ 2ã€‚æ­¤å›¾ç¤ºå±•ç¤ºäº†ç›®æ ‡ä¸­çš„æœ€å¤§é¡¹å¦‚ä½•åå‘ **è¾ƒå¤§çš„æ­£ä¼°è®¡è¯¯å·®**ï¼Œä»è€Œå¼•å‘è¿‡åº¦ä¼°è®¡åå·®ã€‚
- en: '![](../Images/1bf3d33b853a2c71019024eae646b567.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1bf3d33b853a2c71019024eae646b567.png)'
- en: Illustration of the overestimation bias.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: è¿‡åº¦ä¼°è®¡åå·®çš„ç¤ºæ„å›¾ã€‚
- en: Decoupling action selection and evaluation
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è§£è€¦åŠ¨ä½œé€‰æ‹©ä¸è¯„ä¼°
- en: 'To solve this problem, *Hasselt et al.* (2015)[3] propose a new target where
    the action is selected by the online network, while its value is estimated by
    the target network:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œ*Hasselt ç­‰äºº*ï¼ˆ2015ï¼‰[3] æå‡ºäº†ä¸€ä¸ªæ–°çš„ç›®æ ‡ï¼Œå…¶ä¸­åŠ¨ä½œç”±åœ¨çº¿ç½‘ç»œé€‰æ‹©ï¼Œè€Œå…¶å€¼ç”±ç›®æ ‡ç½‘ç»œä¼°ç®—ï¼š
- en: '![](../Images/1c377edbece9356607e4a8a0b5723f2f.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c377edbece9356607e4a8a0b5723f2f.png)'
- en: The Double DQN target
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Double DQN ç›®æ ‡
- en: By decoupling action selection and evaluation, the estimation bias is significantly
    reduced, leading to better value estimates and improved performance.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è§£è€¦åŠ¨ä½œé€‰æ‹©å’Œè¯„ä¼°ï¼Œä¼°è®¡åå·®æ˜¾è‘—å‡å°‘ï¼Œä»è€Œå¯¼è‡´æ›´å¥½çš„ä»·å€¼ä¼°è®¡å’Œæ€§èƒ½æå‡ã€‚
- en: '![](../Images/be824709136eed9f80431b6882ad92c8.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be824709136eed9f80431b6882ad92c8.png)'
- en: 'Double DQN provides stable and accurate value estimates, leading to improved
    performance. Source: Hasselt et al. (2015), Figure 3'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Double DQN æä¾›äº†ç¨³å®šä¸”å‡†ç¡®çš„ä»·å€¼ä¼°è®¡ï¼Œå¸¦æ¥äº†æ€§èƒ½çš„æå‡ã€‚æ¥æºï¼šHasselt ç­‰äººï¼ˆ2015ï¼‰ï¼Œå›¾ 3
- en: Double DQN in practice
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®è·µä¸­çš„ Double DQN
- en: 'As expected, implementing Double DQN only requires us to modify the loss function:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œå®ç° Double DQN åªéœ€è¦ä¿®æ”¹æŸå¤±å‡½æ•°ï¼š
- en: Dueling DQN
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¯¹æŠ— DQN
- en: '**Source:** [*Dueling Network Architectures for Deep Reinforcement Learning*](http://arxiv.org/abs/1511.06581)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¥æº:** [*å¯¹æŠ—ç½‘ç»œæ¶æ„ç”¨äºæ·±åº¦å¼ºåŒ–å­¦ä¹ *](http://arxiv.org/abs/1511.06581)'
- en: '**Improvement:** Separation of the value and advantage computation'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ”¹è¿›:** ä»·å€¼ä¸ä¼˜åŠ¿è®¡ç®—çš„åˆ†ç¦»'
- en: State value, Q-value, and advantage
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: çŠ¶æ€å€¼ã€Q å€¼å’Œä¼˜åŠ¿
- en: 'In RL, we use several functions to estimate the value of a given state, action,
    or sequence of actions from a given state:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å¤šä¸ªå‡½æ•°æ¥ä¼°è®¡ç»™å®šçŠ¶æ€ã€åŠ¨ä½œæˆ–ä»ç»™å®šçŠ¶æ€å¼€å§‹çš„ä¸€ç³»åˆ—åŠ¨ä½œçš„ä»·å€¼ï¼š
- en: '**State-value V(s):** The state value corresponds to the expected return when
    starting in a given state **s** and following a policy **Ï€** thereafter.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**çŠ¶æ€å€¼ V(s):** çŠ¶æ€å€¼å¯¹åº”äºåœ¨ç»™å®šçŠ¶æ€ **s** ä¸­å¼€å§‹å¹¶éšåéµå¾ªç­–ç•¥ **Ï€** æ—¶çš„æœŸæœ›å›æŠ¥ã€‚'
- en: '**Q-value Q(s, a):** Similarly, the Q-value corresponds to the expected return
    when starting in a given state **s**, taking action **a,** and following a policy
    **Ï€** thereafter.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Q å€¼ Q(s, a):** ç±»ä¼¼åœ°ï¼ŒQ å€¼å¯¹åº”äºåœ¨ç»™å®šçŠ¶æ€ **s** ä¸­å¼€å§‹ï¼Œé‡‡å–åŠ¨ä½œ **a** å¹¶éšåéµå¾ªç­–ç•¥ **Ï€** æ—¶çš„æœŸæœ›å›æŠ¥ã€‚'
- en: '**Advantage A(s, a):** The advantage is defined as the difference between the
    Q-value and the state-value in a given state **s** for an action **a**. It represents
    the inherent value of action **a** in the current state.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¼˜åŠ¿ A(s, a):** ä¼˜åŠ¿å®šä¹‰ä¸ºåœ¨ç»™å®šçŠ¶æ€ **s** ä¸‹ï¼ŒåŠ¨ä½œ **a** çš„ Q å€¼ä¸çŠ¶æ€å€¼ä¹‹é—´çš„å·®å¼‚ã€‚å®ƒè¡¨ç¤ºäº†åœ¨å½“å‰çŠ¶æ€ä¸‹ï¼ŒåŠ¨ä½œ **a**
    çš„å›ºæœ‰ä»·å€¼ã€‚'
- en: The following figure attempts to represent the differences between these value
    functions on a backup diagram (*note that the state value is weighted by the probability
    of taking each action under policy* **Ï€**).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹å›¾å°è¯•è¡¨ç¤ºè¿™äº›ä»·å€¼å‡½æ•°ä¹‹é—´çš„å·®å¼‚ï¼ˆ*è¯·æ³¨æ„ï¼ŒçŠ¶æ€å€¼æ˜¯æ ¹æ®ç­–ç•¥ **Ï€** ä¸‹é‡‡å–æ¯ä¸ªåŠ¨ä½œçš„æ¦‚ç‡åŠ æƒçš„*ï¼‰ã€‚
- en: '![](../Images/cba2c32f4c583fd964850468b441292f.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cba2c32f4c583fd964850468b441292f.png)'
- en: Visualization of the state value (in purple), state-action value (Q-function,
    in blue), and the advantage (in pink) on a backup diagram.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤‡ä»½å›¾ä¸­å¯è§†åŒ–çŠ¶æ€å€¼ï¼ˆç´«è‰²ï¼‰ã€çŠ¶æ€-åŠ¨ä½œå€¼ï¼ˆQ å‡½æ•°ï¼Œè“è‰²ï¼‰å’Œä¼˜åŠ¿ï¼ˆç²‰è‰²ï¼‰ã€‚
- en: Usually, DQN estimates the Q-value directly, using a feed-forward neural network.
    This implies that DQN has to learn the Q-values for each action in each state
    independently.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼ŒDQN ç›´æ¥ä¼°è®¡ Q å€¼ï¼Œä½¿ç”¨å‰é¦ˆç¥ç»ç½‘ç»œã€‚è¿™æ„å‘³ç€ DQN å¿…é¡»ç‹¬ç«‹åœ°ä¸ºæ¯ä¸ªçŠ¶æ€ä¸‹çš„æ¯ä¸ªåŠ¨ä½œå­¦ä¹  Q å€¼ã€‚
- en: The dueling architecture
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯¹æŠ—æ¶æ„
- en: 'Introduced by *Wang et al*.[4] in 2016, Dueling DQN uses a neural network with
    two separate streams of computation:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ç”± *Wang ç­‰äºº*ï¼ˆ2016ï¼‰[4] æå‡ºçš„å¯¹æŠ— DQN ä½¿ç”¨ä¸€ä¸ªå…·æœ‰ä¸¤ä¸ªç‹¬ç«‹è®¡ç®—æµçš„ç¥ç»ç½‘ç»œï¼š
- en: The **state value stream** predicts the scalar value of a given state.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**çŠ¶æ€å€¼æµ** ç”¨äºé¢„æµ‹ç»™å®šçŠ¶æ€çš„æ ‡é‡å€¼ã€‚'
- en: The **advantage stream** predicts to predict the advantage of each action for
    a given state.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¼˜åŠ¿æµ** ç”¨äºé¢„æµ‹ç»™å®šçŠ¶æ€ä¸‹æ¯ä¸ªåŠ¨ä½œçš„ä¼˜åŠ¿ã€‚'
- en: This decoupling enables the **independent estimation** of the state value and
    advantages, which has several benefits. For instance, the network can learn state
    values without having to update the action values regularly. Additionally, it
    can better generalize to unseen actions in familiar states.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§è§£è€¦ä½¿å¾—**ç‹¬ç«‹ä¼°è®¡**çŠ¶æ€å€¼å’Œä¼˜åŠ¿æˆä¸ºå¯èƒ½ï¼Œè¿™å¸¦æ¥äº†è‹¥å¹²å¥½å¤„ã€‚ä¾‹å¦‚ï¼Œç½‘ç»œå¯ä»¥åœ¨ä¸éœ€è¦å®šæœŸæ›´æ–°åŠ¨ä½œå€¼çš„æƒ…å†µä¸‹å­¦ä¹ çŠ¶æ€å€¼ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜èƒ½æ›´å¥½åœ°æ³›åŒ–åˆ°ç†Ÿæ‚‰çŠ¶æ€ä¸‹æœªè§è¿‡çš„åŠ¨ä½œã€‚
- en: These improvements lead to stabler and faster convergence, especially in environments
    with many similar-valued actions.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ”¹è¿›å¯¼è‡´äº†æ›´ç¨³å®šå’Œæ›´å¿«é€Ÿçš„æ”¶æ•›ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰è®¸å¤šç›¸ä¼¼ä»·å€¼åŠ¨ä½œçš„ç¯å¢ƒä¸­ã€‚
- en: In practice, a dueling network uses a **common representation** (i.e. a shared
    linear or convolutional layer) parameterized by parameters **Î¸** before splitting
    into two streams, consisting of linear layers with parameters **Î±** and **Î²**
    respectively. The state value stream outputs a scalar value while the advantage
    stream returns a scalar value for each available action.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œå†³æ–—ç½‘ç»œä½¿ç”¨ä¸€ä¸ª**å…±åŒè¡¨ç¤º**ï¼ˆå³ä¸€ä¸ªå…±äº«çš„çº¿æ€§æˆ–å·ç§¯å±‚ï¼‰ï¼Œç”±**Î¸**å‚æ•°åŒ–ï¼Œç„¶ååˆ†ä¸ºä¸¤ä¸ªæµï¼Œæ¯ä¸ªæµç”±å…·æœ‰**Î±**å’Œ**Î²**å‚æ•°çš„çº¿æ€§å±‚ç»„æˆã€‚çŠ¶æ€å€¼æµè¾“å‡ºä¸€ä¸ªæ ‡é‡å€¼ï¼Œè€Œä¼˜åŠ¿æµä¸ºæ¯ä¸ªå¯ç”¨çš„åŠ¨ä½œè¿”å›ä¸€ä¸ªæ ‡é‡å€¼ã€‚
- en: Adding the outputs of the two streams allows us to reconstruct the Q-value for
    each action as **Q(s, a) = V(s) + A(s, a)**.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å°†ä¸¤ä¸ªæµçš„è¾“å‡ºç›¸åŠ ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿé‡å»ºæ¯ä¸ªåŠ¨ä½œçš„Qå€¼ï¼Œå³**Q(s, a) = V(s) + A(s, a)**ã€‚
- en: An important detail is that the mean is usually subtracted from the advantages.
    Indeed, the advantages need to have **zero mean**, otherwise, it would be impossible
    to decompose Q into V and A, making the problem ill-defined. With this constraint,
    **V** represents the value of the state while **A** represents how much better
    or worse each action is compared to the average action in that state.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªé‡è¦çš„ç»†èŠ‚æ˜¯ï¼Œä¼˜åŠ¿é€šå¸¸ä¼šå‡å»å¹³å‡å€¼ã€‚å®é™…ä¸Šï¼Œä¼˜åŠ¿éœ€è¦**é›¶å‡å€¼**ï¼Œå¦åˆ™å°†æ— æ³•å°†Qåˆ†è§£ä¸ºVå’ŒAï¼Œä»è€Œä½¿é—®é¢˜å˜å¾—ä¸æ˜ç¡®ã€‚åœ¨è¿™ä¸ªçº¦æŸä¸‹ï¼Œ**V**è¡¨ç¤ºçŠ¶æ€çš„ä»·å€¼ï¼Œè€Œ**A**è¡¨ç¤ºæ¯ä¸ªåŠ¨ä½œç›¸å¯¹äºè¯¥çŠ¶æ€ä¸‹å¹³å‡åŠ¨ä½œçš„å¥½åç¨‹åº¦ã€‚
- en: '![](../Images/af7d67549d33c0babd85d701ffa5900e.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af7d67549d33c0babd85d701ffa5900e.png)'
- en: Illustration of a dueling network
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: å†³æ–—ç½‘ç»œç¤ºæ„å›¾
- en: Dueling Network in practice
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®é™…åº”ç”¨ä¸­çš„å†³æ–—ç½‘ç»œ
- en: 'Hereâ€™s the Stoix implementation of a Q-network:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯Stoixå®ç°çš„Qç½‘ç»œï¼š
- en: Distributional DQN
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆ†å¸ƒå¼DQN
- en: '**Source:** [A distributional perspective on Reinforcement Learning](http://arxiv.org/abs/1707.06887)[5]'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¥æºï¼š** [å…³äºå¼ºåŒ–å­¦ä¹ çš„åˆ†å¸ƒå¼è§†è§’](http://arxiv.org/abs/1707.06887)[5]'
- en: '**Improvement:** Richer value estimates'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ”¹è¿›ï¼š** æ›´ä¸°å¯Œçš„ä»·å€¼ä¼°è®¡'
- en: The return distribution
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å›æŠ¥åˆ†å¸ƒ
- en: Most RL systems model the expectation of the return, however, a promising body
    of literature approaches RL from a distributional perspective. In this setting,
    the goal becomes to model the **return distribution**, which allows us to consider
    other statistics than the mean.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•°å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿå»ºæ¨¡çš„æ˜¯å›æŠ¥çš„æœŸæœ›å€¼ï¼Œç„¶è€Œï¼Œè¶Šæ¥è¶Šå¤šçš„æ–‡çŒ®ä»åˆ†å¸ƒå¼çš„è§’åº¦ç ”ç©¶å¼ºåŒ–å­¦ä¹ ã€‚åœ¨è¿™ç§è®¾å®šä¸‹ï¼Œç›®æ ‡å˜ä¸ºå»ºæ¨¡**å›æŠ¥åˆ†å¸ƒ**ï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿè€ƒè™‘å¹³å‡å€¼ä»¥å¤–çš„å…¶ä»–ç»Ÿè®¡é‡ã€‚
- en: In 2017, *Bellemare et al.*[5] published a distributional version of DQN called
    C51 predicting the return distribution for each action, reaching new state-of-the-art
    performances on the Atari benchmark.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 2017å¹´ï¼Œ*Bellemareç­‰äºº*[5]å‘å¸ƒäº†DQNçš„åˆ†å¸ƒå¼ç‰ˆæœ¬C51ï¼Œé¢„æµ‹æ¯ä¸ªåŠ¨ä½œçš„å›æŠ¥åˆ†å¸ƒï¼Œåœ¨AtariåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚
- en: '![](../Images/194c16ae2151c70103d4d1834a0a919b.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/194c16ae2151c70103d4d1834a0a919b.png)'
- en: Illustrated comparison between DQN and C51\. Source [5']
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: DQNä¸C51çš„æ¯”è¾ƒç¤ºæ„å›¾ã€‚æ¥æº [5']
- en: Letâ€™s take a step back and review the theory behind C51.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹C51èƒŒåçš„ç†è®ºã€‚
- en: 'In traditional RL, we evaluate a policy using the **Bellman Equation**, which
    allows us to define the Q-function in a recursive form. Alternatively, we can
    use a distributional version of the Bellman equation, which accounts for randomness
    in the returns:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨**Bellmanæ–¹ç¨‹**æ¥è¯„ä¼°ç­–ç•¥ï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿä»¥é€’å½’å½¢å¼å®šä¹‰Qå‡½æ•°ã€‚æˆ–è€…ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨åˆ†å¸ƒå¼ç‰ˆæœ¬çš„Bellmanæ–¹ç¨‹ï¼Œå®ƒè€ƒè™‘äº†å›æŠ¥ä¸­çš„éšæœºæ€§ï¼š
- en: '![](../Images/ef036b8c95c7be0d0c6ab34ccfdafdb7.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ef036b8c95c7be0d0c6ab34ccfdafdb7.png)'
- en: Standard and Distributional versions of the Bellman Equation
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Bellmanæ–¹ç¨‹çš„æ ‡å‡†ç‰ˆå’Œåˆ†å¸ƒå¼ç‰ˆ
- en: Here, **Ï** is the transition function.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œ**Ï**æ˜¯è½¬ç§»å‡½æ•°ã€‚
- en: The main difference between those functions is that **Q** **is a numerical value**,
    summing expectations over random variables. In contrast, **Z is a random variable**,
    summing the reward distribution and the discounted distribution of future returns.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å‡½æ•°ä¹‹é—´çš„ä¸»è¦åŒºåˆ«åœ¨äº**Q** **æ˜¯ä¸€ä¸ªæ•°å€¼**ï¼Œå®ƒæ˜¯å¯¹éšæœºå˜é‡æœŸæœ›å€¼çš„æ€»å’Œã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œ**Zæ˜¯ä¸€ä¸ªéšæœºå˜é‡**ï¼Œå®ƒæ˜¯å¯¹å¥–åŠ±åˆ†å¸ƒå’Œæœªæ¥å›æŠ¥çš„æŠ˜æ‰£åˆ†å¸ƒçš„æ€»å’Œã€‚
- en: 'The following illustration helps visualize how to derive **Z** from the distributional
    Bellman equation:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ’å›¾æœ‰åŠ©äºå¯è§†åŒ–å¦‚ä½•ä»åˆ†å¸ƒè´å°”æ›¼æ–¹ç¨‹æ¨å¯¼**Z**ï¼š
- en: Consider the distribution of returns **Z** at a given timestep and the transition
    operator **PÏ€.** **PÏ€Z** is the distribution of future returns **Z(sâ€™, aâ€™)**.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è€ƒè™‘åœ¨ç»™å®šæ—¶é—´æ­¥é•¿ä¸‹å›æŠ¥åˆ†å¸ƒ**Z**ä»¥åŠè½¬ç§»æ“ä½œç¬¦**PÏ€**ã€‚**PÏ€Z**æ˜¯æœªæ¥å›æŠ¥**Z(sâ€™ï¼Œaâ€™)**çš„åˆ†å¸ƒã€‚
- en: Multiplying this by the discount factor **Î³** contracts the distribution towards
    0 (as **Î³** is less than 1).
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ­¤ä¹˜ä»¥æŠ˜æ‰£å› å­**Î³**ä¼šä½¿åˆ†å¸ƒå‘0æ”¶ç¼©ï¼ˆå› ä¸º**Î³**å°äº1ï¼‰ã€‚
- en: Adding the reward distribution shifts the previous distribution by a set amount
    *(Note that the figure assumes a constant reward for simplicity. In practice,
    adding the reward distribution would shift but also modify the discounted return*).
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ·»åŠ å¥–åŠ±åˆ†å¸ƒä¼šå°†ä¹‹å‰çš„åˆ†å¸ƒå¹³ç§»ä¸€ä¸ªå›ºå®šçš„é‡ï¼ˆ*æ³¨æ„ï¼Œå›¾ä¸­å‡è®¾å¥–åŠ±ä¸ºå¸¸æ•°ä»¥ç®€åŒ–è®¡ç®—ã€‚å®é™…ä¸Šï¼Œæ·»åŠ å¥–åŠ±åˆ†å¸ƒä¼šä½¿åˆ†å¸ƒå‘ç”Ÿå¹³ç§»ï¼ŒåŒæ—¶ä¹Ÿä¼šä¿®æ”¹æŠ˜æ‰£å›æŠ¥*ï¼‰ã€‚
- en: Finally, the distribution is projected on a discrete support using an L2 projection
    operator **Î¦**.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ€åï¼Œä½¿ç”¨L2æŠ•å½±æ“ä½œç¬¦**Î¦**å°†åˆ†å¸ƒæŠ•å½±åˆ°ç¦»æ•£æ”¯æŒä¸Šã€‚
- en: '![](../Images/97a3aac37d854dcd502775572e175e3b.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97a3aac37d854dcd502775572e175e3b.png)'
- en: 'Illustration of the distributional Bellman equation. Source: [5]'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†å¸ƒè´å°”æ›¼æ–¹ç¨‹çš„æ’å›¾ã€‚æ¥æºï¼š[5]
- en: 'This fixed support is a vector of ***N*** atoms separated by a constant gap
    within a set interval:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå›ºå®šçš„æ”¯æŒæ˜¯ä¸€ä¸ªç”±***N***ä¸ªåŸå­ç»„æˆçš„å‘é‡ï¼Œåœ¨ä¸€ä¸ªå›ºå®šåŒºé—´å†…æŒ‰æ’å®šé—´éš”åˆ†éš”ï¼š
- en: '![](../Images/1aa87d6a69d5414ec7ebb7e17c72596a.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1aa87d6a69d5414ec7ebb7e17c72596a.png)'
- en: Definition of the discrete support **z**
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ç¦»æ•£æ”¯æŒ**z**çš„å®šä¹‰
- en: 'At inference time, the Q-network returns an approximating distribution **dt**
    defined on this support with the probability mass **pÎ¸(st, at)** on each atom
    ***i*** such that:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¨ç†æ—¶ï¼ŒQç½‘ç»œè¿”å›ä¸€ä¸ªåœ¨è¯¥æ”¯æŒä¸Šå®šä¹‰çš„è¿‘ä¼¼åˆ†å¸ƒ**dt**ï¼Œæ¯ä¸ªåŸå­***i***ä¸Šçš„æ¦‚ç‡è´¨é‡**pÎ¸(st, at)**æ»¡è¶³ï¼š
- en: '![](../Images/da3c5c9967c24aea64b82f9e9891c613.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da3c5c9967c24aea64b82f9e9891c613.png)'
- en: Predicted return distribution
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„æµ‹å›æŠ¥åˆ†å¸ƒ
- en: 'The goal is to update **Î¸** such that the distribution closely matches the
    true distribution of returns. To learn the probability masses, the target distribution
    is built using a **distributional variant of Bellmanâ€™s optimality equation**:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®æ ‡æ˜¯æ›´æ–°**Î¸**ï¼Œä½¿å¾—åˆ†å¸ƒå°½å¯èƒ½æ¥è¿‘çœŸå®çš„å›æŠ¥åˆ†å¸ƒã€‚ä¸ºäº†å­¦ä¹ æ¦‚ç‡è´¨é‡ï¼Œç›®æ ‡åˆ†å¸ƒæ˜¯é€šè¿‡**è´å°”æ›¼æœ€ä¼˜æ€§æ–¹ç¨‹çš„åˆ†å¸ƒå¼å˜ç§**æ¥æ„å»ºçš„ï¼š
- en: '![](../Images/81ad365cd4b43af4ea6fd2e4388e975e.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81ad365cd4b43af4ea6fd2e4388e975e.png)'
- en: Target return distribution
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®æ ‡å›æŠ¥åˆ†å¸ƒ
- en: To be able to compare the distribution predicted by our neural network and the
    target distribution, we need to discretize the target distribution and project
    it on the same support **z**.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†èƒ½å¤Ÿæ¯”è¾ƒæˆ‘ä»¬ç¥ç»ç½‘ç»œé¢„æµ‹çš„åˆ†å¸ƒä¸ç›®æ ‡åˆ†å¸ƒï¼Œæˆ‘ä»¬éœ€è¦å°†ç›®æ ‡åˆ†å¸ƒç¦»æ•£åŒ–ï¼Œå¹¶å°†å…¶æŠ•å½±åˆ°ç›¸åŒçš„æ”¯æŒ**z**ä¸Šã€‚
- en: 'To this end, we use an L2 projection (*a projection onto* ***z*** *such that
    the difference between the original and projected distribution is minimized in
    terms of the L2 norm*):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨L2æŠ•å½±ï¼ˆ*å°†æŠ•å½±åˆ°* ***z*** *ä¸Šï¼Œä½¿å¾—åŸå§‹åˆ†å¸ƒå’ŒæŠ•å½±åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚åœ¨L2èŒƒæ•°ä¸‹æœ€å°åŒ–*ï¼‰ï¼š
- en: '![](../Images/c58835a895b9ebeba742c78533a5f1a1.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c58835a895b9ebeba742c78533a5f1a1.png)'
- en: L2 projection of the target distribution
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®æ ‡åˆ†å¸ƒçš„L2æŠ•å½±
- en: Finally, we need to define a loss function that minimizes the difference between
    the two distributions. As weâ€™re dealing with distributions, we canâ€™t simply subtract
    the prediction from the target, as we did previously.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€ä¸ªæŸå¤±å‡½æ•°ï¼Œæœ€å°åŒ–ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚ç”±äºæˆ‘ä»¬å¤„ç†çš„æ˜¯åˆ†å¸ƒï¼Œä¸èƒ½åƒä¹‹å‰é‚£æ ·ç›´æ¥ä»ç›®æ ‡ä¸­å‡å»é¢„æµ‹å€¼ã€‚
- en: 'Instead, we minimize the Kullback-Leibler divergence between **dt** and **dâ€™t**
    (in practice, this is implemented as a cross-entropy loss):'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸åï¼Œæˆ‘ä»¬æœ€å°åŒ–**dt**å’Œ**dâ€™t**ä¹‹é—´çš„Kullback-Leibleræ•£åº¦ï¼ˆåœ¨å®è·µä¸­ï¼Œè¿™é€šå¸¸ä½œä¸ºäº¤å‰ç†µæŸå¤±æ¥å®ç°ï¼‰ï¼š
- en: '![](../Images/4e6e7171841467790558b6c6ce556321.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4e6e7171841467790558b6c6ce556321.png)'
- en: KL divergence between the projected target and the predicted return distribution
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„æµ‹å›æŠ¥åˆ†å¸ƒå’ŒæŠ•å½±ç›®æ ‡åˆ†å¸ƒä¹‹é—´çš„KLæ•£åº¦
- en: '*For a more exhaustive description of Distributional DQN, you can refer to
    Massimiliano Tomassoliâ€™s article[8] as well as Pascal Poupartâ€™s video on the topic[11].*'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¦äº†è§£æ›´å…¨é¢çš„åˆ†å¸ƒå¼DQNæè¿°ï¼Œå¯ä»¥å‚è€ƒMassimiliano Tomassoliçš„æ–‡ç« [8]ä»¥åŠPascal Poupartå…³äºè¯¥ä¸»é¢˜çš„è§†é¢‘[11]ã€‚*'
- en: C51 in practice
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®è·µä¸­çš„C51
- en: The key components of C51 in Stoix are the Distributional head and the categorical
    loss, which uses double Q-learning by default as introduced previously. The choice
    of defining the C51 network as a head lets us use an MLP or a CNN torso interchangeably
    depending on the use case.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Stoixä¸­C51çš„å…³é”®ç»„ä»¶æ˜¯åˆ†å¸ƒå¤´å’Œç±»åˆ«æŸå¤±ï¼Œé»˜è®¤æƒ…å†µä¸‹ä½¿ç”¨å‰è¿°çš„åŒQå­¦ä¹ ã€‚é€‰æ‹©å°†C51ç½‘ç»œå®šä¹‰ä¸ºä¸€ä¸ªå¤´éƒ¨ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿæ ¹æ®ä½¿ç”¨æ¡ˆä¾‹äº’æ¢ä½¿ç”¨MLPæˆ–CNNä½œä¸ºä¸»ä½“ã€‚
- en: Noisy DQN
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Noisy DQN
- en: '**Source:** [Noisy Networks for Exploration](http://arxiv.org/abs/1706.10295)[6]'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¥æºï¼š** [å™ªå£°ç½‘ç»œç”¨äºæ¢ç´¢](http://arxiv.org/abs/1706.10295)[6]'
- en: '**Improvement:** Learnable and state-dependent exploration mechanism'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ”¹è¿›ï¼š** å¯å­¦ä¹ çš„ã€çŠ¶æ€ä¾èµ–çš„æ¢ç´¢æœºåˆ¶'
- en: Noisy parameterization of Neural Networks
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œçš„å™ªå£°å‚æ•°åŒ–
- en: As many off-policy algorithms, DQN relies on an epsilon-greedy policy as its
    main exploration mechanism. Therefore, the algorithm will behave greedily with
    respect to the Q-values most of the time and select random actions with a predefined
    probability.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è®¸å¤šç¦»ç­–ç•¥ç®—æ³•ä¸€æ ·ï¼ŒDQNä¾èµ–äº**Îµ-è´ªå©ªç­–ç•¥**ä½œä¸ºå…¶ä¸»è¦çš„æ¢ç´¢æœºåˆ¶ã€‚å› æ­¤ï¼Œè¯¥ç®—æ³•å¤§éƒ¨åˆ†æ—¶é—´ä¼šæ ¹æ®Qå€¼è¿›è¡Œè´ªå©ªé€‰æ‹©ï¼Œå¹¶ä»¥é¢„å®šä¹‰çš„æ¦‚ç‡é€‰æ‹©éšæœºåŠ¨ä½œã€‚
- en: '*Fortunato et al.*[6] introduce NoisyNets as a more flexible alternative. NoisyNets
    are neural networks whose weights and biases are **perturbed** by a **parametric
    function of Gaussian noise**. Similarly to an epsilon-greedy policy, such noise
    injects randomness in the agentâ€™s action selection, thus encouraging exploration.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '*Fortunato et al.*[6]æå‡ºäº†NoisyNetsä½œä¸ºä¸€ç§æ›´çµæ´»çš„æ›¿ä»£æ–¹æ¡ˆã€‚NoisyNetsæ˜¯ç¥ç»ç½‘ç»œï¼Œå…¶æƒé‡å’Œåç½®ç”±**é«˜æ–¯å™ªå£°çš„å‚æ•°åŒ–å‡½æ•°**æ‰°åŠ¨ã€‚ç±»ä¼¼äºÎµ-è´ªå©ªç­–ç•¥ï¼Œè¿™ç§å™ªå£°é€šè¿‡å‘æ™ºèƒ½ä½“çš„åŠ¨ä½œé€‰æ‹©ä¸­æ³¨å…¥éšæœºæ€§ï¼Œä»è€Œé¼“åŠ±æ¢ç´¢ã€‚'
- en: However, this noise is scaled and offset by **learned parameters**, allowing
    the level of noise to be adapted state-by-state. This way, the balance between
    exploration and exploitation is optimized *dynamically* during training. Eventually,
    the network may learn to ignore the noise, but will do so at **different rates**
    in **different parts of the state space**, leading to more flexible exploration.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œè¿™ä¸ªå™ªå£°ç”±**å­¦ä¹ åˆ°çš„å‚æ•°**è¿›è¡Œç¼©æ”¾å’Œåç§»ï¼Œå…è®¸å™ªå£°çš„æ°´å¹³åœ¨æ¯ä¸ªçŠ¶æ€ä¸‹åŠ¨æ€è°ƒæ•´ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¢ç´¢å’Œåˆ©ç”¨ä¹‹é—´çš„å¹³è¡¡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿ*åŠ¨æ€*ä¼˜åŒ–ã€‚æœ€ç»ˆï¼Œç½‘ç»œå¯èƒ½å­¦ä¼šå¿½ç•¥å™ªå£°ï¼Œä½†ä¼šåœ¨**çŠ¶æ€ç©ºé—´çš„ä¸åŒéƒ¨åˆ†**ä»¥**ä¸åŒçš„é€Ÿåº¦**å®ç°è¿™ä¸€ç‚¹ï¼Œä»è€Œå¯¼è‡´æ›´çµæ´»çš„æ¢ç´¢ã€‚
- en: 'A network parameterized by a vector of noisy parameters is defined as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªç”±å™ªå£°å‚æ•°å‘é‡å‚æ•°åŒ–çš„ç½‘ç»œå®šä¹‰å¦‚ä¸‹ï¼š
- en: '![](../Images/e34ae4ce245756ebcea722260869dc66.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e34ae4ce245756ebcea722260869dc66.png)'
- en: Neural Network parameterized by Noisy parameters
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±å™ªå£°å‚æ•°åŒ–çš„ç¥ç»ç½‘ç»œ
- en: 'Therefore, a linear layer **y = wx + b** becomes:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä¸€ä¸ªçº¿æ€§å±‚**y = wx + b**å˜ä¸ºï¼š
- en: '![](../Images/0c00383ff99930448d0296b65faca5ec.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c00383ff99930448d0296b65faca5ec.png)'
- en: Noisy linear layer
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: å™ªå£°çº¿æ€§å±‚
- en: For performance, the noise is generated at inference time using **Factorized
    Gaussian Noise**. For a linear layer with **M** inputs and **N** outputs, a noise
    matrix of shape (**M x N**) is generated as a combination of two noise vectors
    with size **M** and **N**. This methods reduces the number of required random
    variables from **M x N** to **M + N**.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æé«˜æ€§èƒ½ï¼Œå™ªå£°åœ¨æ¨ç†æ—¶ä½¿ç”¨**å› å¼åˆ†è§£é«˜æ–¯å™ªå£°**ç”Ÿæˆã€‚å¯¹äºä¸€ä¸ªå…·æœ‰**M**ä¸ªè¾“å…¥å’Œ**N**ä¸ªè¾“å‡ºçš„çº¿æ€§å±‚ï¼Œç”Ÿæˆä¸€ä¸ªå½¢çŠ¶ä¸ºï¼ˆ**M x N**ï¼‰çš„å™ªå£°çŸ©é˜µï¼Œè¯¥çŸ©é˜µæ˜¯ä¸¤ä¸ªå¤§å°åˆ†åˆ«ä¸º**M**å’Œ**N**çš„å™ªå£°å‘é‡çš„ç»„åˆã€‚æ­¤æ–¹æ³•å°†æ‰€éœ€çš„éšæœºå˜é‡æ•°é‡ä»**M
    x N**å‡å°‘åˆ°**M + N**ã€‚
- en: 'The noise matrix is defined as the outer product of the noise vectors, each
    scaled by a function **f**:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: å™ªå£°çŸ©é˜µè¢«å®šä¹‰ä¸ºå™ªå£°å‘é‡çš„å¤–ç§¯ï¼Œæ¯ä¸ªå‘é‡ç”±å‡½æ•°**f**ç¼©æ”¾ï¼š
- en: '![](../Images/c2930b27127318d9c7088beb98339ab8.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2930b27127318d9c7088beb98339ab8.png)'
- en: Noise generation using Factorised Gaussian Noise
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å› å¼åˆ†è§£é«˜æ–¯å™ªå£°ç”Ÿæˆå™ªå£°
- en: Improved exploration
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ”¹è¿›çš„æ¢ç´¢
- en: The improved exploration induced by noisy networks allow a wide range of algorithms,
    such as DQN, Dueling DQN and A3C to benefit from improved performances with a
    reasonably low amount of extra parameters.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±å™ªå£°ç½‘ç»œå¼•å‘çš„æ”¹è¿›æ¢ç´¢ä½¿å¾—åŒ…æ‹¬DQNã€å¯¹æˆ˜DQNå’ŒA3Cç­‰å¹¿æ³›çš„ç®—æ³•èƒ½å¤Ÿå—ç›Šäºè¾ƒä½é¢å¤–å‚æ•°çš„æƒ…å†µä¸‹è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚
- en: '![](../Images/0546839f96938a710ebdd383e8aee747.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0546839f96938a710ebdd383e8aee747.png)'
- en: 'NoisyNets improve the performance of several algorithms on the Atari benchmark.
    Source: [6]'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: NoisyNetsæé«˜äº†å‡ ç§ç®—æ³•åœ¨AtariåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ã€‚æ¥æºï¼š[6]
- en: Noisy DQN in practice
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®è·µä¸­çš„Noisy DQN
- en: 'In Stoix, we implement a noisy layer as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Stoixä¸­ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ä¸ªå™ªå£°å±‚ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '*Note: All the linear layers in Rainbow are replaced with their noisy equivalent
    (see the* ***â€œAssembling Rainbowâ€*** *section for more details).*'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ³¨ï¼šRainbowä¸­çš„æ‰€æœ‰çº¿æ€§å±‚éƒ½å·²æ›¿æ¢ä¸ºå…¶å™ªå£°ç­‰ä»·ç‰©ï¼ˆæœ‰å…³æ›´å¤šç»†èŠ‚ï¼Œè¯·å‚è§* ***â€œç»„åˆRainbowâ€*** *éƒ¨åˆ†ï¼‰ã€‚*'
- en: Prioritized Experience Replay
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¼˜å…ˆç»éªŒå›æ”¾
- en: '**Source:** Prioritized Experience Replay[7]'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ¥æºï¼š** ä¼˜å…ˆç»éªŒå›æ”¾[7]'
- en: '**Improvement:** Prioritization of experiences with higher learning potential'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ”¹è¿›**ï¼šä¼˜å…ˆé€‰æ‹©å…·æœ‰æ›´é«˜å­¦ä¹ æ½œåŠ›çš„ç»éªŒ'
- en: Estimating the Learning Potential
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¼°ç®—å­¦ä¹ æ½œåŠ›
- en: After taking an environment step, vanilla DQN uniformly samples a batch of experiences
    (also called *transitions*) from a replay buffer and performs a gradient descent
    step on this batch. Although this approach produces satisfying results, some specific
    experiences might be more valuable from a learning perspective than others. Therefore,
    we could potentially speed up the training process by sampling such experiences
    more often.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‰§è¡Œç¯å¢ƒæ­¥éª¤åï¼Œæ™®é€šçš„ DQN ä¼šä»é‡æ”¾ç¼“å†²åŒºå‡åŒ€åœ°é‡‡æ ·ä¸€æ‰¹ç»éªŒï¼ˆä¹Ÿç§°ä¸º*è½¬æ¢*ï¼‰ï¼Œå¹¶åœ¨è¿™æ‰¹ç»éªŒä¸Šæ‰§è¡Œä¸€æ¬¡æ¢¯åº¦ä¸‹é™æ­¥éª¤ã€‚å°½ç®¡è¿™ç§æ–¹æ³•èƒ½å¤Ÿäº§ç”Ÿä»¤äººæ»¡æ„çš„ç»“æœï¼Œä½†æŸäº›ç‰¹å®šçš„ç»éªŒä»å­¦ä¹ è§’åº¦æ¥çœ‹å¯èƒ½æ¯”å…¶ä»–ç»éªŒæ›´æœ‰ä»·å€¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ›´é¢‘ç¹åœ°é‡‡æ ·è¿™äº›ç»éªŒæ¥åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚
- en: 'This is precisely the idea explored in the Prioritized Experience Replay (PER)
    paper published by *Schaul et al.*[7] in 2016\. However, the main question remains:
    how to approximate the **expected learning potential** of a transition?'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ­£æ˜¯*Schual ç­‰äºº*åœ¨2016å¹´å‘å¸ƒçš„ä¼˜å…ˆç»éªŒå›æ”¾ï¼ˆPERï¼‰è®ºæ–‡ä¸­æ¢è®¨çš„ç†å¿µ[7]ã€‚ç„¶è€Œï¼Œä¸»è¦çš„é—®é¢˜ä»ç„¶æ˜¯ï¼šå¦‚ä½•è¿‘ä¼¼åœ°ä¼°ç®—ä¸€ä¸ªè½¬æ¢çš„**é¢„æœŸå­¦ä¹ æ½œåŠ›**ï¼Ÿ
- en: 'One idealized criterion would be the amount the RL agent can learn from a transition
    in its current state (expected learning progress). While this measure is not directly
    accessible, a reasonable proxy is the magnitude of a transitionâ€™s TD error Î´,
    which indicates how â€˜surprisingâ€™ or unexpected the transition is: specifically,
    how far the value is from its next-step bootstrap estimate (Andre et al., 1998).'
  id: totrans-161
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªç†æƒ³åŒ–çš„æ ‡å‡†æ˜¯ï¼ŒRL æ™ºèƒ½ä½“åœ¨å½“å‰çŠ¶æ€ä¸‹å¯ä»¥ä»ä¸€ä¸ªè½¬æ¢ä¸­å­¦åˆ°å¤šå°‘ï¼ˆé¢„æœŸçš„å­¦ä¹ è¿›å±•ï¼‰ã€‚è™½ç„¶è¿™ä¸ªåº¦é‡ä¸å¯ç›´æ¥è®¿é—®ï¼Œä½†ä¸€ä¸ªåˆç†çš„ä»£ç†æ˜¯è½¬æ¢çš„ TD é”™è¯¯
    Î´ çš„å¤§å°ï¼Œå®ƒè¡¨ç¤ºè½¬æ¢æœ‰å¤šä¹ˆâ€œå‡ºä¹æ„æ–™â€æˆ–æ„å¤–ï¼šå…·ä½“æ¥è¯´ï¼Œå€¼ä¸ä¸‹ä¸€æ­¥è‡ªä¸¾ä¼°è®¡ä¹‹é—´çš„å·®è·æœ‰å¤šå¤§ï¼ˆAndre ç­‰äººï¼Œ1998ï¼‰ã€‚
- en: Prioritized Experience Replay, Schaul et al. (2016)
  id: totrans-162
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä¼˜å…ˆç»éªŒå›æ”¾ï¼ŒSchual ç­‰äººï¼ˆ2016ï¼‰
- en: 'As a reminder, the TD error is defined as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: æé†’ä¸€ä¸‹ï¼ŒTD é”™è¯¯å®šä¹‰å¦‚ä¸‹ï¼š
- en: '![](../Images/47087938e81974e308d2a37b8fcb400a.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47087938e81974e308d2a37b8fcb400a.png)'
- en: The temporal-difference error
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¶é—´å·®é”™
- en: This metric is a decent estimate of the learning potential of a specific transition,
    as a high TD error indicates a large difference between the predicted and actual
    outcomes, meaning that the agent would benefit from updating its beliefs.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæŒ‡æ ‡æ˜¯ç‰¹å®šè½¬æ¢å­¦ä¹ æ½œåŠ›çš„ä¸€ä¸ªåˆç†ä¼°è®¡ï¼Œå› ä¸ºè¾ƒé«˜çš„ TD é”™è¯¯è¡¨ç¤ºé¢„æµ‹ç»“æœä¸å®é™…ç»“æœä¹‹é—´çš„å·®å¼‚è¾ƒå¤§ï¼Œè¿™æ„å‘³ç€æ™ºèƒ½ä½“ä»æ›´æ–°å…¶ä¿¡å¿µä¸­ä¼šå—ç›Šã€‚
- en: 'However, it is worth noting that alternative prioritization metrics are still
    being studied. For instance, *Lahire et al.*[9] (2022) argue that the optimal
    sampling scheme is distributed according to the per-sample gradient norms:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ›¿ä»£çš„ä¼˜å…ˆçº§åº¦é‡ä»åœ¨ç ”ç©¶ä¸­ã€‚ä¾‹å¦‚ï¼Œ*Lahire ç­‰äºº*ï¼ˆ2022ï¼‰è®¤ä¸ºï¼Œæœ€ä¼˜çš„é‡‡æ ·æ–¹æ¡ˆæ˜¯æ ¹æ®æ¯ä¸ªæ ·æœ¬çš„æ¢¯åº¦èŒƒæ•°è¿›è¡Œåˆ†å¸ƒçš„[9]ã€‚
- en: '![](../Images/5ae86716a279e2c41dbbd7f9dc33b80a.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5ae86716a279e2c41dbbd7f9dc33b80a.png)'
- en: Per-sample gradient norms
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªæ ·æœ¬çš„æ¢¯åº¦èŒƒæ•°
- en: However, letâ€™s continue with the TD error, as Rainbow uses this metric.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæ—¢ç„¶ Rainbow ä½¿ç”¨çš„æ˜¯è¿™ä¸ªåº¦é‡ï¼Œæˆ‘ä»¬å°±ç»§ç»­ä½¿ç”¨ TD é”™è¯¯ã€‚
- en: Deriving Sampling Probabilities
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨å¯¼é‡‡æ ·æ¦‚ç‡
- en: 'Once we have selected the prioritization criterion, we can derive the probabilities
    of sampling each transition from it. In Prioritized Experience Replay, two alternatives
    are showcased:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æˆ‘ä»¬é€‰æ‹©äº†ä¼˜å…ˆçº§æ ‡å‡†ï¼Œå°±å¯ä»¥æ ¹æ®è¿™ä¸ªæ ‡å‡†æ¨å¯¼å‡ºæ¯ä¸ªè½¬æ¢çš„é‡‡æ ·æ¦‚ç‡ã€‚åœ¨ä¼˜å…ˆç»éªŒå›æ”¾ä¸­ï¼Œå±•ç¤ºäº†ä¸¤ç§æ›¿ä»£æ–¹æ¡ˆï¼š
- en: '**Proportional**: Here the probability of replaying a transition is equal to
    the absolute value of the associated TD error. A small positive constant is added
    to prevent transitions not being revisited once their error is zero.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æŒ‰æ¯”ä¾‹**ï¼šåœ¨è¿™ç§æ–¹å¼ä¸‹ï¼Œé‡æ”¾ä¸€ä¸ªè½¬æ¢çš„æ¦‚ç‡ç­‰äºç›¸å…³ TD é”™è¯¯çš„ç»å¯¹å€¼ã€‚ä¸ºäº†é˜²æ­¢è½¬æ¢åœ¨å…¶é”™è¯¯ä¸ºé›¶æ—¶ä¸å†è¢«é‡æ–°è®¿é—®ï¼Œæ·»åŠ äº†ä¸€ä¸ªå°çš„æ­£å¸¸æ•°ã€‚'
- en: '**Rank-based**: In this mode, transitions are ranked in descending order according
    to their absolute TD error, and their probability is defined based on their rank.
    This option is supposed to be more robust as it is insensible to outliers.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åŸºäºæ’å**ï¼šåœ¨è¿™ç§æ¨¡å¼ä¸‹ï¼Œè½¬æ¢æ ¹æ®å…¶ç»å¯¹ TD é”™è¯¯æŒ‰é™åºæ’åˆ—ï¼Œå¹¶æ ¹æ®å…¶æ’åå®šä¹‰å…¶æ¦‚ç‡ã€‚è¿™ç§æ–¹å¼è¢«è®¤ä¸ºæ›´åŠ ç¨³å¥ï¼Œå› ä¸ºå®ƒå¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿã€‚'
- en: The sampling probabilities are then normalized and raised to the power **Î±**,
    a hyperparameter determining the degree of prioritization (**Î±=0** is the uniform
    case).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: é‡‡æ ·æ¦‚ç‡ç„¶åä¼šè¢«å½’ä¸€åŒ–ï¼Œå¹¶å‡è‡³**Î±**çš„å¹‚ï¼ŒÎ±æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œå†³å®šä¼˜å…ˆçº§çš„ç¨‹åº¦ï¼ˆ**Î±=0**æ˜¯å‡åŒ€çš„æƒ…å†µï¼‰ã€‚
- en: '![](../Images/dd93373aeafc0258d8097ddcc8b98e69.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd93373aeafc0258d8097ddcc8b98e69.png)'
- en: Prioritization modes and probability normalization
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼˜å…ˆçº§æ¨¡å¼å’Œæ¦‚ç‡å½’ä¸€åŒ–
- en: Importance sampling and bias annealing
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é‡è¦æ€§é‡‡æ ·å’Œåå·®é€€ç«
- en: In RL, the estimation of the expected value of the return relies on the assumption
    that the updates correspond to the same distribution as the expectation (i.e.,
    the uniform distribution). However, PER introduces bias as we now sample experiences
    according to their TD error.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œé¢„æœŸå›æŠ¥å€¼çš„ä¼°è®¡ä¾èµ–äºæ›´æ–°ä¸æœŸæœ›ï¼ˆå³å‡åŒ€åˆ†å¸ƒï¼‰ç›¸åŒçš„åˆ†å¸ƒå‡è®¾ã€‚ç„¶è€Œï¼ŒPERå¼•å…¥äº†åå·®ï¼Œå› ä¸ºæˆ‘ä»¬ç°åœ¨æ˜¯æ ¹æ®TDè¯¯å·®æ¥æŠ½æ ·ç»éªŒã€‚
- en: To rectify this bias, we use **importance sampling**, a statistical method used
    to *estimate the properties of a distribution while sampling from a different
    distribution*. Importance sampling re-weights samples so that the estimates remain
    unbiased and accurate.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†çº æ­£è¿™ç§åå·®ï¼Œæˆ‘ä»¬ä½¿ç”¨**é‡è¦æ€§æŠ½æ ·**ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äº*ä»ä¸åŒåˆ†å¸ƒä¸­æŠ½æ ·æ—¶ä¼°è®¡åˆ†å¸ƒç‰¹æ€§*çš„ç»Ÿè®¡æ–¹æ³•ã€‚é‡è¦æ€§æŠ½æ ·ä¼šé‡æ–°åŠ æƒæ ·æœ¬ï¼Œä½¿å¾—ä¼°è®¡ç»“æœä¿æŒæ— åä¸”å‡†ç¡®ã€‚
- en: 'Typically, the correcting weights are defined as the ratio of the two probabilities:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œä¿®æ­£æƒé‡å®šä¹‰ä¸ºä¸¤ä¸ªæ¦‚ç‡çš„æ¯”ç‡ï¼š
- en: '![](../Images/df361e74e24201fc048a33e4eaf8cc51.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df361e74e24201fc048a33e4eaf8cc51.png)'
- en: Importance sampling ratio
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: é‡è¦æ€§æŠ½æ ·æ¯”ç‡
- en: In this case, the target distribution is the uniform distribution, where every
    transition has a probability of being sampled equal to 1/**N**, with **N** being
    the size of the replay buffer.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç›®æ ‡åˆ†å¸ƒæ˜¯å‡åŒ€åˆ†å¸ƒï¼Œå…¶ä¸­æ¯ä¸ªè¿‡æ¸¡çš„è¢«æŠ½æ ·æ¦‚ç‡ä¸º1/**N**ï¼Œ**N**æ˜¯é‡æ”¾ç¼“å†²åŒºçš„å¤§å°ã€‚
- en: 'Therefore, the importance sampling coefficient in the context of PER is defined
    by:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œåœ¨ä¼˜å…ˆç»éªŒå›æ”¾ï¼ˆPERï¼‰ä¸Šä¸‹æ–‡ä¸­çš„é‡è¦æ€§æŠ½æ ·ç³»æ•°å®šä¹‰ä¸ºï¼š
- en: '![](../Images/8c435485dea322925507c5bc7a38e668.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c435485dea322925507c5bc7a38e668.png)'
- en: Importance sampling weight used in PER
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨PERä¸­ä½¿ç”¨çš„é‡è¦æ€§æŠ½æ ·æƒé‡
- en: 'With **Î²** a coefficient adjusting the amount of bias correction (the bias
    is fully corrected for **Î²=1**). Finally, the weights are normalized for stability:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '**Î²**æ˜¯ä¸€ä¸ªè°ƒæ•´åå·®ä¿®æ­£é‡çš„ç³»æ•°ï¼ˆå½“**Î²=1**æ—¶ï¼Œåå·®å®Œå…¨è¢«ä¿®æ­£ï¼‰ã€‚æœ€åï¼Œä¸ºäº†ç¨³å®šæ€§ï¼Œæƒé‡ä¼šè¿›è¡Œå½’ä¸€åŒ–ï¼š'
- en: '![](../Images/868b56f8913a40c593f78675c09d784e.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/868b56f8913a40c593f78675c09d784e.png)'
- en: Normalization of the importance sampling weights
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: é‡è¦æ€§æŠ½æ ·æƒé‡çš„å½’ä¸€åŒ–
- en: 'To summarize, hereâ€™s the full algorithm for Prioritized Experience Replay (the
    update and training steps are identical to DQN):'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“ä¸€ä¸‹ï¼Œè¿™æ˜¯ä¼˜å…ˆç»éªŒå›æ”¾çš„å®Œæ•´ç®—æ³•ï¼ˆæ›´æ–°å’Œè®­ç»ƒæ­¥éª¤ä¸DQNç›¸åŒï¼‰ï¼š
- en: '![](../Images/e4e0192fa820ff77f9ef36d510c7fc91.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4e0192fa820ff77f9ef36d510c7fc91.png)'
- en: The Prioritized Experience Replay algorithm
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼˜å…ˆç»éªŒå›æ”¾ç®—æ³•
- en: Increased convergence speed with PER
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨PERæé«˜äº†æ”¶æ•›é€Ÿåº¦
- en: The following plots highlight the performance benefits of PER. Indeed, the proportional
    and rank-based prioritization mechanisms enable DQN to reach the same baseline
    performances roughly twice as fast on the Atari benchmark.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹å›¾è¡¨çªå‡ºäº†PERçš„æ€§èƒ½ä¼˜åŠ¿ã€‚äº‹å®ä¸Šï¼ŒåŸºäºæ¯”ä¾‹å’Œæ’åçš„ä¼˜å…ˆæœºåˆ¶ä½¿å¾—DQNåœ¨AtariåŸºå‡†æµ‹è¯•ä¸Šèƒ½å¤Ÿå¤§çº¦ä¸¤å€çš„é€Ÿåº¦è¾¾åˆ°ç›¸åŒçš„åŸºçº¿æ€§èƒ½ã€‚
- en: '![](../Images/d4585c6368905c2144be046608fdc86f.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d4585c6368905c2144be046608fdc86f.png)'
- en: Normalized maximum and average scores (in terms of Double DQN performance) on
    57 Atari games. Source:[7]
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨57ä¸ªAtariæ¸¸æˆä¸­çš„æœ€å¤§å’Œå¹³å‡å¾—åˆ†çš„å½’ä¸€åŒ–ï¼ˆä»¥Double DQNæ€§èƒ½ä¸ºæ ‡å‡†ï¼‰ã€‚æ¥æºï¼š[7]
- en: Prioritized Experience Replay in practice
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®è·µä¸­çš„ä¼˜å…ˆç»éªŒå›æ”¾
- en: 'Stoix seamlessly integrates the [Flashbax](https://github.com/instadeepai/flashbax)
    library which provides a variety of replay buffers. Here are the relevant code
    snippets used to instantiate the replay buffer, compute the sampling probabilities
    from the TD error, and update the bufferâ€™s priorities:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Stoixæ— ç¼é›†æˆäº†[Flashbax](https://github.com/instadeepai/flashbax)åº“ï¼Œæä¾›å¤šç§é‡æ”¾ç¼“å†²åŒºã€‚ä»¥ä¸‹æ˜¯ç”¨äºå®ä¾‹åŒ–é‡æ”¾ç¼“å†²åŒºã€è®¡ç®—åŸºäºTDè¯¯å·®çš„æŠ½æ ·æ¦‚ç‡å¹¶æ›´æ–°ç¼“å†²åŒºä¼˜å…ˆçº§çš„ç›¸å…³ä»£ç ç‰‡æ®µï¼š
- en: Multi-step Learning
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤šæ­¥å­¦ä¹ 
- en: '**Source:** [Reinforcement Learning: an Introduction, chapter 7](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¥æº**ï¼š[å¼ºåŒ–å­¦ä¹ ï¼šå¯¼è®ºï¼Œç¬¬7ç« ](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)'
- en: '**Improvement:** Enhanced reward signal and sample efficiency, reduced variance'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ”¹è¿›**ï¼šå¢å¼ºçš„å¥–åŠ±ä¿¡å·å’Œæ ·æœ¬æ•ˆç‡ï¼Œå‡å°‘çš„æ–¹å·®'
- en: 'Multi-step learning is an improvement on traditional one-step temporal difference
    learning which allows us to consider the return over **n** steps when building
    our targets. For instance, instead of considering the reward at the next timestep,
    weâ€™ll consider the n-step truncated rewards (see the below equation). This process
    has several advantages, among which:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šæ­¥å­¦ä¹ æ˜¯å¯¹ä¼ ç»Ÿçš„ä¸€æ­¥æ—¶é—´å·®å­¦ä¹ çš„æ”¹è¿›ï¼Œå®ƒä½¿æˆ‘ä»¬åœ¨æ„å»ºç›®æ ‡æ—¶å¯ä»¥è€ƒè™‘**n**æ­¥çš„å›æŠ¥ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬ä¸å†ä»…ä»…è€ƒè™‘ä¸‹ä¸€æ—¶é—´æ­¥çš„å¥–åŠ±ï¼Œè€Œæ˜¯è€ƒè™‘næ­¥æˆªæ–­çš„å¥–åŠ±ï¼ˆè§ä¸‹é¢çš„å…¬å¼ï¼‰ã€‚è¿™ä¸ªè¿‡ç¨‹æœ‰å‡ ä¸ªä¼˜ç‚¹ï¼Œå…¶ä¸­ä¹‹ä¸€æ˜¯ï¼š
- en: '**Immediate feedback:** considering a larger time horizon allows the agent
    to learn the value of state-action pairs much faster, especially in environments
    where rewards are delayed and specific actions might not pay out immediately.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å³æ—¶åé¦ˆï¼š**è€ƒè™‘æ›´é•¿çš„æ—¶é—´è·¨åº¦å…è®¸ä»£ç†æ›´å¿«åœ°å­¦ä¹ çŠ¶æ€-åŠ¨ä½œå¯¹çš„ä»·å€¼ï¼Œç‰¹åˆ«æ˜¯åœ¨å¥–åŠ±å»¶è¿Ÿä¸”ç‰¹å®šåŠ¨ä½œå¯èƒ½ä¸ä¼šç«‹å³å¸¦æ¥å›æŠ¥çš„ç¯å¢ƒä¸­ã€‚'
- en: '**Sample efficiency:** Each update in multi-step learning incorporates information
    from multiple time steps, making each sample more informative. This improves sample
    efficiency, meaning the agent can learn more from fewer experiences.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ ·æœ¬æ•ˆç‡ï¼š**æ¯æ¬¡æ›´æ–°ä¸­çš„å¤šæ­¥å­¦ä¹ ç»“åˆäº†å¤šä¸ªæ—¶é—´æ­¥çš„ä¿¡æ¯ï¼Œä½¿æ¯ä¸ªæ ·æœ¬æ›´åŠ æœ‰ä»·å€¼ã€‚è¿™æé«˜äº†æ ·æœ¬æ•ˆç‡ï¼Œæ„å‘³ç€ä»£ç†å¯ä»¥ä»æ›´å°‘çš„ç»éªŒä¸­å­¦åˆ°æ›´å¤šã€‚'
- en: '**Balancing Bias and Variance:** Multi-step methods offer a trade-off between
    bias and variance. One-step methods have low bias but high variance, while multi-step
    methods have higher bias but lower variance. By tuning the number of steps, one
    can find a balance that works best for the given environment.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¹³è¡¡åå·®ä¸æ–¹å·®ï¼š**å¤šæ­¥æ–¹æ³•åœ¨åå·®å’Œæ–¹å·®ä¹‹é—´æä¾›äº†ä¸€ç§æƒè¡¡ã€‚ä¸€æ­¥æ–¹æ³•åå·®å°ä½†æ–¹å·®å¤§ï¼Œè€Œå¤šæ­¥æ–¹æ³•åå·®å¤§ä½†æ–¹å·®å°ã€‚é€šè¿‡è°ƒèŠ‚æ­¥æ•°ï¼Œå¯ä»¥æ‰¾åˆ°æœ€é€‚åˆç»™å®šç¯å¢ƒçš„å¹³è¡¡ã€‚'
- en: 'The multi-step distributional loss used in Rainbow is defined as:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Rainbowä¸­ä½¿ç”¨çš„å¤šæ­¥åˆ†å¸ƒå¼æŸå¤±å®šä¹‰å¦‚ä¸‹ï¼š
- en: '![](../Images/4f2b412a351afc3dfa642272ce6a1fa6.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f2b412a351afc3dfa642272ce6a1fa6.png)'
- en: Multi-step target return distribution
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šæ­¥ç›®æ ‡å›æŠ¥åˆ†å¸ƒ
- en: 'In practice, using n-step returns implies a few adjustments to our code:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®è·µä¸­ï¼Œä½¿ç”¨næ­¥å›æŠ¥æ„å‘³ç€æˆ‘ä»¬éœ€è¦å¯¹ä»£ç è¿›è¡Œä¸€äº›è°ƒæ•´ï¼š
- en: We now sample trajectories of **n** experiences, instead of individual experiences
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨é‡‡æ ·çš„æ˜¯**n**ä¸ªç»éªŒçš„è½¨è¿¹ï¼Œè€Œä¸æ˜¯å•ä¸ªç»éªŒã€‚
- en: The reward is replaced with the n-step discounted returns
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¥–åŠ±è¢«næ­¥æŠ˜æ‰£å›æŠ¥æ›¿ä»£
- en: The done flag is set to True if any of the **n** done flag is True
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœä»»ä½•**n**ä¸ªdoneæ ‡å¿—ä¸ºTrueï¼Œåˆ™doneæ ‡å¿—è¢«è®¾ç½®ä¸ºTrueã€‚
- en: The next state **s(t+1)** is replaced by the last observation of the trajectory
    **s(t+n)**
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€çŠ¶æ€**s(t+1)**è¢«è½¨è¿¹çš„æœ€åä¸€ä¸ªè§‚å¯Ÿå€¼**s(t+n)**æ›¿ä»£
- en: Multi-Step learning in practice
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®è·µä¸­çš„å¤šæ­¥å­¦ä¹ 
- en: 'Finally, we can reuse the categorical loss function used in C51 with these
    updated inputs:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å¯ä»¥é‡æ–°ä½¿ç”¨C51ä¸­ä½¿ç”¨çš„ç±»åˆ«æŸå¤±å‡½æ•°ï¼Œé…åˆè¿™äº›æ›´æ–°åçš„è¾“å…¥ï¼š
- en: Assembling Rainbow
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»„è£…Rainbow
- en: 'Congratulations on making it this far! We now have a better understanding of
    all the moving pieces that constitute Rainbow. Hereâ€™s a summary of the Rainbow
    agent:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œä½ èµ°åˆ°äº†è¿™ä¸€æ­¥ï¼æˆ‘ä»¬ç°åœ¨æ›´å¥½åœ°ç†è§£äº†æ„æˆRainbowçš„æ‰€æœ‰å…³é”®éƒ¨åˆ†ã€‚ä»¥ä¸‹æ˜¯Rainbowä»£ç†çš„æ€»ç»“ï¼š
- en: '**Neural Network Architecture:**'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç¥ç»ç½‘ç»œæ¶æ„ï¼š**'
- en: â€” **Torso:** A convolutional neural network (CNN) or multi-layer perceptron
    (MLP) base that creates embeddings for the head network.
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: â€” **èº¯å¹²ï¼š**å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æˆ–å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰åŸºç¡€ï¼Œç”¨äºä¸ºå¤´éƒ¨ç½‘ç»œåˆ›å»ºåµŒå…¥ã€‚
- en: â€” **Head:** Combines Dueling DQN and C51\. The value stream outputs the state
    value distribution over atoms, while the advantage stream outputs the advantage
    distribution over actions and atoms. These streams are aggregated, and Q-values
    are computed as the weighted sum of atom values and their respective probabilities.
    An action is selected using an epsilon-greedy policy.
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: â€” **å¤´éƒ¨ï¼š**ç»“åˆäº†Dueling DQNå’ŒC51ã€‚ä»·å€¼æµè¾“å‡ºåŸå­ä¸Šçš„çŠ¶æ€ä»·å€¼åˆ†å¸ƒï¼Œè€Œä¼˜åŠ¿æµè¾“å‡ºåŠ¨ä½œå’ŒåŸå­ä¸Šçš„ä¼˜åŠ¿åˆ†å¸ƒã€‚è¿™äº›æµè¢«æ±‡æ€»ï¼ŒQå€¼ä½œä¸ºåŸå­å€¼åŠå…¶ç›¸åº”æ¦‚ç‡çš„åŠ æƒå’Œè¿›è¡Œè®¡ç®—ã€‚ä½¿ç”¨epsilon-greedyç­–ç•¥é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œã€‚
- en: â€” **Noisy Layers:** All linear layers are replaced with their noisy equivalents
    to aid in exploration.
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: â€” **å™ªå£°å±‚ï¼š**æ‰€æœ‰çº¿æ€§å±‚è¢«å™ªå£°ç­‰æ•ˆå±‚æ›¿ä»£ï¼Œä»¥å¸®åŠ©æ¢ç´¢ã€‚
- en: '**Loss Function:** Uses a distributional loss modeling the n-step returns,
    where targets are computed using Double Q-learning.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æŸå¤±å‡½æ•°ï¼š**ä½¿ç”¨åˆ†å¸ƒå¼æŸå¤±æ¥å»ºæ¨¡næ­¥å›æŠ¥ï¼Œç›®æ ‡é€šè¿‡åŒQå­¦ä¹ è®¡ç®—å¾—å‡ºã€‚'
- en: '**Replay Buffer:** Employs a prioritization mechanism based on the TD error
    to improve learning efficiency.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å›æ”¾ç¼“å†²åŒºï¼š**é‡‡ç”¨åŸºäºTDè¯¯å·®çš„ä¼˜å…ˆæœºåˆ¶ï¼Œä»¥æé«˜å­¦ä¹ æ•ˆç‡ã€‚'
- en: 'Hereâ€™s the network used for the Rainbow head:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ç”¨äºRainbowå¤´éƒ¨çš„ç½‘ç»œï¼š
- en: Performances and ablations
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ€§èƒ½ä¸æ¶ˆèå®éªŒ
- en: To conclude this article, letâ€™s take a closer look at Rainbowâ€™s performances
    on the Atari benchmark, as well as the ablation study.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ€»ç»“è¿™ç¯‡æ–‡ç« ï¼Œè®©æˆ‘ä»¬æ›´æ·±å…¥åœ°çœ‹çœ‹Rainbowåœ¨AtariåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ä»¥åŠæ¶ˆèç ”ç©¶ã€‚
- en: The following figure compares Rainbow with the other DQN baselines we studied.
    The measured metric is the median human-normalized score. In other words, the
    median human performance on Atari games is set to 100%, which enables us to quickly
    spot algorithms achieving a human level.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹å›¾å°†Rainbowä¸æˆ‘ä»¬ç ”ç©¶è¿‡çš„å…¶ä»–DQNåŸºå‡†è¿›è¡Œæ¯”è¾ƒã€‚æµ‹é‡çš„æŒ‡æ ‡æ˜¯ä¸­ä½æ•°äººç±»æ ‡å‡†åŒ–åˆ†æ•°ã€‚æ¢å¥è¯è¯´ï¼ŒAtariæ¸¸æˆä¸­äººç±»çš„ä¸­ä½æ•°è¡¨ç°è¢«è®¾å®šä¸º100%ï¼Œè¿™ä½¿å¾—æˆ‘ä»¬å¯ä»¥å¿«é€Ÿå‘ç°è¾¾åˆ°äººç±»æ°´å¹³çš„ç®—æ³•ã€‚
- en: 'Three of the DQN baselines reach this level after 200 million frames:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‰ä¸ªDQNåŸºå‡†åœ¨2äº¿å¸§åè¾¾åˆ°äº†è¿™ä¸ªæ°´å¹³ï¼š
- en: '**Distributional DQN**'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åˆ†å¸ƒå¼DQN**'
- en: '**Dueling DQN**'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å†³æ–—DQN**'
- en: '**Prioritized Double DQN**'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¼˜å…ˆçº§åŒé‡DQN**'
- en: Interestingly, Rainbow reaches the same level after only 44 million frames,
    making it **roughly 5 times more sample efficient** than the best baselines. At
    the end of training, it exceeds **200%** of the median human-normalized score.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰è¶£çš„æ˜¯ï¼ŒRainbowåœ¨ä»…44ç™¾ä¸‡å¸§åå°±è¾¾åˆ°äº†ç›¸åŒçš„æ°´å¹³ï¼Œä½¿å…¶**å¤§çº¦æ¯”æœ€å¥½çš„åŸºå‡†æ•ˆç‡é«˜5å€**ã€‚åœ¨è®­ç»ƒç»“æŸæ—¶ï¼Œå®ƒè¶…è¿‡äº†**200%**çš„ä¸­ä½æ•°äººç±»å½’ä¸€åŒ–å¾—åˆ†ã€‚
- en: '![](../Images/ea23c893a795be0c7bd8eced1e0d9c2c.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea23c893a795be0c7bd8eced1e0d9c2c.png)'
- en: 'Median human-normalized performance across 57 Atari games. Each line represents
    a DQN baseline. Source: [2]'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨57æ¬¾Atariæ¸¸æˆä¸­çš„ä¸­ä½æ•°äººç±»å½’ä¸€åŒ–è¡¨ç°ã€‚æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªDQNåŸºå‡†ã€‚æ¥æºï¼š[2]
- en: 'This second figure represents the ablation study, which represents the performances
    of Rainbow without one of its components. These results allow us to make several
    observations:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç¬¬äºŒä¸ªå›¾è¡¨ç¤ºå»é™¤å®éªŒï¼Œå±•ç¤ºäº†æ²¡æœ‰RainbowæŸä¸ªç»„ä»¶æ—¶çš„è¡¨ç°ã€‚è¿™äº›ç»“æœè®©æˆ‘ä»¬å¯ä»¥åšå‡ºä»¥ä¸‹å‡ é¡¹è§‚å¯Ÿï¼š
- en: The three most crucial components of Rainbow are the distributional head, the
    use of multi-step learning, and the prioritization of the replay buffer.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rainbowçš„ä¸‰ä¸ªæœ€é‡è¦çš„ç»„æˆéƒ¨åˆ†æ˜¯åˆ†å¸ƒå¼å¤´ã€ä½¿ç”¨å¤šæ­¥å­¦ä¹ å’Œä¼˜å…ˆçº§é‡æ”¾ç¼“å†²åŒºã€‚
- en: Noisy layers contribute significantly to the overall performance. Using standard
    layers with an epsilon-greedy policy doesnâ€™t allow the agent to reach the 200%
    score in 200 million frames.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å™ªå£°å±‚å¯¹æ•´ä½“è¡¨ç°æœ‰æ˜¾è‘—è´¡çŒ®ã€‚ä½¿ç”¨æ ‡å‡†å±‚å’Œepsilonè´ªå©ªç­–ç•¥ä¸èƒ½è®©æ™ºèƒ½ä½“åœ¨2äº¿å¸§å†…è¾¾åˆ°200%çš„å¾—åˆ†ã€‚
- en: Despite achieving strong performances on their own, the dueling structure and
    double Q-learning only provide marginal improvements in the context of Rainbow.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°½ç®¡å®ƒä»¬å„è‡ªå–å¾—äº†å¼ºå¤§çš„è¡¨ç°ï¼Œä½†åœ¨Rainbowçš„èƒŒæ™¯ä¸‹ï¼Œå†³æ–—ç»“æ„å’ŒåŒé‡Qå­¦ä¹ ä»…æä¾›äº†å¾®å°çš„æ”¹è¿›ã€‚
- en: '![](../Images/848c4c480a1e32c78e5393a9567b2c89.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/848c4c480a1e32c78e5393a9567b2c89.png)'
- en: 'Median human-normalized performance across 57 Atari games. Each line represents
    an ablation of Rainbow. Source: [2]'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨57æ¬¾Atariæ¸¸æˆä¸­çš„ä¸­ä½æ•°äººç±»å½’ä¸€åŒ–è¡¨ç°ã€‚æ¯ä¸€è¡Œä»£è¡¨Rainbowçš„å»é™¤å®éªŒã€‚æ¥æºï¼š[2]
- en: Thank you very much for reading this article, I hope it provided you with a
    comprehensive introduction to Rainbow and its components. I highly advise reading
    through the [**Stoix implementation of Rainbow**](https://github.com/EdanToledo/Stoix/blob/main/stoix/systems/q_learning/ff_rainbow.py)
    for a more detailed description of the training process and the Rainbow architecture.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: éå¸¸æ„Ÿè°¢ä½ é˜…è¯»æœ¬æ–‡ï¼Œå¸Œæœ›å®ƒèƒ½ä¸ºä½ æä¾›æœ‰å…³RainbowåŠå…¶ç»„ä»¶çš„å…¨é¢ä»‹ç»ã€‚æˆ‘å¼ºçƒˆå»ºè®®ä½ é˜…è¯»[**Stoixå®ç°çš„Rainbow**](https://github.com/EdanToledo/Stoix/blob/main/stoix/systems/q_learning/ff_rainbow.py)ï¼Œä»¥ä¾¿æ›´è¯¦ç»†åœ°äº†è§£è®­ç»ƒè¿‡ç¨‹å’ŒRainbowæ¶æ„ã€‚
- en: Until next time ğŸ‘‹
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹æ¬¡è§ğŸ‘‹
- en: Bibliography
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒä¹¦ç›®
- en: '[1] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,
    D., & Riedmiller, M. (2013). [***Playing Atari with Deep Reinforcement Learning***](http://arxiv.org/abs/1312.5602),
    arXiv'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,
    D., & Riedmiller, M. (2013). [***é€šè¿‡æ·±åº¦å¼ºåŒ–å­¦ä¹ ç©Atari***](http://arxiv.org/abs/1312.5602)ï¼ŒarXivã€‚'
- en: '[2] Hessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostrovski, G., Dabney,
    W., Horgan, D., Piot, B., Azar, M., & Silver, D. (2017). [***Rainbow: Combining
    Improvements in Deep Reinforcement Learning***](http://arxiv.org/abs/1710.02298),
    arXiv.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Hessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostrovski, G., Dabney,
    W., Horgan, D., Piot, B., Azar, M., & Silver, D. (2017). [***Rainbowï¼šç»“åˆæ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­çš„æ”¹è¿›***](http://arxiv.org/abs/1710.02298)ï¼ŒarXivã€‚'
- en: '[3] van Hasselt, H., Guez, A., & Silver, D. (2015). [***Deep Reinforcement
    Learning with Double Q-learning***](http://arxiv.org/abs/1509.06461), arXiv.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] van Hasselt, H., Guez, A., & Silver, D. (2015). [***ä½¿ç”¨åŒé‡Qå­¦ä¹ çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ***](http://arxiv.org/abs/1509.06461)ï¼ŒarXivã€‚'
- en: '[4] Wang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot, M., & de Freitas,
    N. (2016). [***Dueling Network Architectures for Deep Reinforcement Learning***](http://arxiv.org/abs/1511.06581)
    (No. arXiv:1511.06581), arXiv'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Wang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot, M., & de Freitas,
    N. (2016). [***æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„å†³æ–—ç½‘ç»œæ¶æ„***](http://arxiv.org/abs/1511.06581) (No. arXiv:1511.06581)ï¼ŒarXivã€‚'
- en: '[5] Bellemare, M. G., Dabney, W., & Munos, R. (2017). [***A Distributional
    Perspective on Reinforcement Learning***](http://arxiv.org/abs/1707.06887), arXiv'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Bellemare, M. G., Dabney, W., & Munos, R. (2017). [***å¼ºåŒ–å­¦ä¹ çš„åˆ†å¸ƒå¼è§†è§’***](http://arxiv.org/abs/1707.06887)ï¼ŒarXivã€‚'
- en: '[5''] Dabney, W., Ostrovski, G., Silver, D., & Munos, R. (2018). [***Implicit
    Quantile Networks for Distributional Reinforcement Learning***](http://arxiv.org/abs/1806.06923),
    arXiv'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '[5''] Dabney, W., Ostrovski, G., Silver, D., & Munos, R. (2018). [***ç”¨äºåˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ çš„éšå¼åˆ†ä½ç½‘ç»œ***](http://arxiv.org/abs/1806.06923)ï¼ŒarXivã€‚'
- en: '[[6]](http://arxiv.org/abs/1806.06923](http://arxiv.org/abs/1806.06923)[6])
    Fortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I., Graves, A., Mnih,
    V., Munos, R., Hassabis, D., Pietquin, O., Blundell, C., & Legg, S. (2019). [***Noisy
    Networks for Exploration***](http://arxiv.org/abs/1706.10295), arXiv.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '[[6]](http://arxiv.org/abs/1806.06923](http://arxiv.org/abs/1806.06923)[6])
    Fortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I., Graves, A., Mnih,
    V., Munos, R., Hassabis, D., Pietquin, O., Blundell, C., & Legg, S. (2019). [***æ¢ç´¢çš„å™ªå£°ç½‘ç»œ***](http://arxiv.org/abs/1706.10295),
    arXiv.'
- en: '[7] Schaul, T., Quan, J., Antonoglou, I., & Silver, D. (2016). [***Prioritized
    Experience Replay***](http://arxiv.org/abs/1511.05952)***,*** arXiv'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Schaul, T., Quan, J., Antonoglou, I., & Silver, D. (2016). [***ä¼˜å…ˆç»éªŒå›æ”¾***](http://arxiv.org/abs/1511.05952)***,***
    arXiv'
- en: Additional resources
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é¢å¤–èµ„æº
- en: '[8] Massimiliano Tomassoli, [***Distributional RL: An intuitive explanation
    of Distributional RL***](https://mtomassoli.github.io/2017/12/08/distributional_rl/)'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Massimiliano Tomassoli, [***åˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ ï¼šåˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ çš„ç›´è§‚è§£é‡Š***](https://mtomassoli.github.io/2017/12/08/distributional_rl/)'
- en: '[9] Lahire, T., Geist, M., & Rachelson, E. (2022). [***Large Batch Experience
    Replay***](http://arxiv.org/abs/2110.01528), arXiv.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Lahire, T., Geist, M., & Rachelson, E. (2022). [***å¤§æ‰¹é‡ç»éªŒå›æ”¾***](http://arxiv.org/abs/2110.01528),
    arXiv.'
- en: '[10] Sutton, R. S., & Barto, A. G. (1998). ***Reinforcement Learning: An Introduction***.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Sutton, R. S., & Barto, A. G. (1998). ***å¼ºåŒ–å­¦ä¹ ï¼šå¯¼è®º***ã€‚'
- en: '[11] Pascal Poupart, [***CS885 Module 5: Distributional RL***](https://youtu.be/r-Yk6-jagDU?si=9lqQHHNaQz8Uiclw)***,***
    YouTube'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Pascal Poupart, [***CS885æ¨¡å—5ï¼šåˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ ***](https://youtu.be/r-Yk6-jagDU?si=9lqQHHNaQz8Uiclw)***,***
    YouTube'
