- en: 'Rainbow: The Colorful Evolution of Deep Q-Networks 🌈'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Rainbow：深度Q网络的多彩进化 🌈
- en: 原文：[https://towardsdatascience.com/rainbow-the-colorful-evolution-of-deep-q-networks-37e662ab99b2?source=collection_archive---------8-----------------------#2024-07-12](https://towardsdatascience.com/rainbow-the-colorful-evolution-of-deep-q-networks-37e662ab99b2?source=collection_archive---------8-----------------------#2024-07-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/rainbow-the-colorful-evolution-of-deep-q-networks-37e662ab99b2?source=collection_archive---------8-----------------------#2024-07-12](https://towardsdatascience.com/rainbow-the-colorful-evolution-of-deep-q-networks-37e662ab99b2?source=collection_archive---------8-----------------------#2024-07-12)
- en: Everything you need to assemble the DQN Megazord in JAX.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 组装DQN Megazord所需的所有内容，使用JAX。
- en: '[](https://medium.com/@ryanpegoud?source=post_page---byline--37e662ab99b2--------------------------------)[![Ryan
    Pégoud](../Images/9314b76c2be56bda8b73b4badf9e3e4d.png)](https://medium.com/@ryanpegoud?source=post_page---byline--37e662ab99b2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--37e662ab99b2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--37e662ab99b2--------------------------------)
    [Ryan Pégoud](https://medium.com/@ryanpegoud?source=post_page---byline--37e662ab99b2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ryanpegoud?source=post_page---byline--37e662ab99b2--------------------------------)[![Ryan
    Pégoud](../Images/9314b76c2be56bda8b73b4badf9e3e4d.png)](https://medium.com/@ryanpegoud?source=post_page---byline--37e662ab99b2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--37e662ab99b2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--37e662ab99b2--------------------------------)
    [Ryan Pégoud](https://medium.com/@ryanpegoud?source=post_page---byline--37e662ab99b2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--37e662ab99b2--------------------------------)
    ·17 min read·Jul 12, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--37e662ab99b2--------------------------------)
    ·阅读时长17分钟·2024年7月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/9c3ff5fae978b121c7db85b75b3e992f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c3ff5fae978b121c7db85b75b3e992f.png)'
- en: “The Rainbow Megazord”, Dall-E 3
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: “Rainbow Megazord”，Dall-E 3
- en: In 2013, the introduction of Deep Q-Networks (DQN) by *Mnih et al.*[1]marked
    the first breakthrough in Deep Reinforcement Learning, surpassing expert human
    players in three Atari games. Over the years, several variants of DQN were published,
    each improving on specific weaknesses of the original algorithm.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 2013年，*Mnih等人*提出的深度Q网络（DQN）[1]标志着深度强化学习的首次突破，在三款Atari游戏中超越了专家级人类玩家。多年来，DQN的多个变种陆续发布，每个变种都在改进原始算法的特定弱点。
- en: 'In 2017, *Hessel et al.*[2]made the best out of the DQN palette by combining
    6 of its powerful variants, crafting what could be called the DQN Megazord: Rainbow.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年，*Hessel等人* [2]通过结合DQN的6种强大变种，创造出了被称为DQN Megazord的Rainbow。
- en: In this article, we’ll break down the individual components that make up Rainbow,
    while reviewing their JAX implementations in the [**Stoix library.**](https://github.com/EdanToledo/Stoix)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将逐一解析构成Rainbow的各个组成部分，并回顾它们在[**Stoix库中的JAX实现**](https://github.com/EdanToledo/Stoix)。
- en: DQN
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DQN
- en: The fundamental building block of Rainbow is DQN, an extension of Q-learning
    using a neural network with parameters **θ** to approximate the Q-function (i.e.
    action-value function). In particular, DQN uses convolutional layers to extract
    features from images and a linear layer to produce a scalar estimate of the Q-value.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Rainbow的基本构建块是DQN，它是Q-learning的扩展，使用带有参数**θ**的神经网络来逼近Q函数（即动作-价值函数）。具体来说，DQN使用卷积层从图像中提取特征，并通过线性层生成Q值的标量估计。
- en: During training, the network parameterized by **θ**, referred to as the *“online
    network”* is used to select actions while the *“target network”* parameterized
    by **θ-** is a delayed copy of the online network used to provide stable targets.
    This way, the targets are not dependent on the parameters being updated.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，网络由**θ**参数化，称为*“在线网络”*，用于选择动作，而*“目标网络”*由**θ-**参数化，是在线网络的延迟副本，用于提供稳定的目标。这样，目标就不依赖于正在更新的参数。
- en: Additionally, DQN uses a replay buffer ***D*** to sample past transitions (observations,
    reward, and done flag tuples) to train on at fixed intervals.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，DQN使用回放缓冲区***D***来采样过去的转移（观察、奖励和完成标志元组），并在固定的时间间隔内进行训练。
- en: 'At each iteration ***i***, DQN samples a transition ***j*** and takes a gradient
    step on the following loss:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代***i***中，DQN采样一次转移***j***并根据以下损失函数进行梯度更新：
- en: '![](../Images/b62e326d7b69d832ec425e3d753b56e8.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b62e326d7b69d832ec425e3d753b56e8.png)'
- en: DQN loss function, all images are made by the author, unless specified otherwise
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: DQN 损失函数，除非另有说明，否则所有图像均由作者制作
- en: This loss aims at minimizing the expectation of the squared temporal-difference
    (TD) error.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个损失旨在最小化平方时序差分（TD）误差的期望值。
- en: Note that DQN is an **off-policy** algorithm because it learns the optimal policy
    defined by the **maximum Q-value** term while following a different behavior policy,
    such as an epsilon-greedy policy.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，DQN 是一个**非策略**算法，因为它在遵循不同的行为策略（如 epsilon 贪心策略）的同时，学习由**最大 Q 值**项定义的最优策略。
- en: 'Here’s the DQN algorithm in detail:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 DQN 算法的详细内容：
- en: '![](../Images/baf5816cef9399a587418e16b013d99a.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/baf5816cef9399a587418e16b013d99a.png)'
- en: DQN algorithm
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: DQN 算法
- en: DQN in practice
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DQN 实践
- en: As mentioned above, we’ll reference code snippets from the Stoix library to
    illustrate the core parts of DQN and Rainbow *(some of the code was slightly edited
    or commented for pedagogical purposes)*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，我们将引用 Stoix 库中的代码片段来说明 DQN 和 Rainbow 的核心部分（*部分代码已稍作编辑或注释，便于教学*）。
- en: 'Let’s start with the neural network: Stoix lets us break down our model architecture
    into a pre-processor and a post-processor, referred to as **torso** and **head**
    respectively. In the case of DQN, the torso would be a multi-layer perceptron
    (MLP) or convolutional neural network (CNN) and the head an epsilon greedy policy,
    both implemented as [**Flax**](https://flax.readthedocs.io/en/latest/index.html)
    modules:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从神经网络开始：Stoix 让我们将模型架构拆解为预处理器和后处理器，分别称为**躯干**和**头部**。在 DQN 的情况下，躯干将是一个多层感知器（MLP）或卷积神经网络（CNN），头部则是一个
    epsilon 贪心策略，两者都实现为 [**Flax**](https://flax.readthedocs.io/en/latest/index.html)
    模块：
- en: A Q-Network, defined as a CNN Torso and an Epsilon-Greedy policy in Stoix
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 Q 网络，定义为 Stoix 中的 CNN 躯干和 epsilon 贪心策略
- en: 'Additionally, DQN uses the following loss (*note that Stoix follows the* [***Rlax***](https://github.com/google-deepmind/rlax)*naming
    conventions, therefore tm1 is equivalent to timestep t in the above equations,
    while t refers to timestep t+1*):'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，DQN 使用以下损失（*请注意 Stoix 遵循* [***Rlax***](https://github.com/google-deepmind/rlax)*命名约定，因此
    tm1 相当于上述公式中的时间步 t，而 t 则指时间步 t+1*）：
- en: The Q-learning loss used in the context of DQN
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: DQN 中使用的 Q-learning 损失
- en: The Rainbow blueprint
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Rainbow 蓝图
- en: Now that we have laid the foundations for DQN, we’ll review each part of the
    algorithm in more detail, while identifying potential weaknesses and how they
    are addressed by Rainbow.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为 DQN 打下了基础，我们将更详细地回顾算法的各个部分，同时识别潜在的弱点以及 Rainbow 如何解决这些问题。
- en: 'In particular, we’ll cover:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，我们将涵盖：
- en: Double DQN and the overestimation bias
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双重 DQN 和过高估计偏差
- en: Dueling DQN and the state-value / advantage prediction
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对抗 DQN 和状态值 / 优势预测
- en: Distributional DQN and the return distribution
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式 DQN 和回报分布
- en: Multi-step learning
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多步学习
- en: Noisy DQN and flexible exploration strategies
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 噪声 DQN 和灵活的探索策略
- en: Prioritized Experience Replay and learning potential
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优先经验回放与学习潜力
- en: '![](../Images/20275c2f4f6e79e1dc782a0d2beb1f50.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20275c2f4f6e79e1dc782a0d2beb1f50.png)'
- en: The Rainbow Blueprint, Dall-E 3
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Rainbow 蓝图，Dall-E 3
- en: Double DQN
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双重 DQN
- en: '**Source:** [*Deep Reinforcement Learning with Double Q-learning*](http://arxiv.org/abs/1509.06461)[3]'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**来源：** [*Deep Reinforcement Learning with Double Q-learning*](http://arxiv.org/abs/1509.06461)[3]'
- en: '**Improvement:** Reduced overestimation bias'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**改进：** 降低过高估计偏差'
- en: The overestimation bias
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过高估计偏差
- en: 'One issue with the loss function used in vanilla DQN arises from the Q-target.
    Remember that we define the target as:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 原始 DQN 使用的损失函数存在一个问题，这个问题来自 Q 目标。记住，我们将目标定义为：
- en: '![](../Images/3bd664e7d9b3dd60aff3bb4a501b9bb6.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3bd664e7d9b3dd60aff3bb4a501b9bb6.png)'
- en: Objective in the DQN loss
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: DQN 损失中的目标
- en: This objective may lead to an **overestimation bias**. Indeed, as DQN uses bootstrapping
    (learning estimates from estimates), the max term may select overestimated values
    to update the Q-function, leading to overestimated Q-values.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个目标可能会导致**过高估计偏差**。实际上，由于 DQN 使用引导（从估计中学习估计），最大值项可能会选择过高估计的值来更新 Q 函数，导致 Q 值的过高估计。
- en: 'As an example, consider the following figure:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑以下图示：
- en: The Q-values predicted by the network are represented in blue.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络预测的 Q 值用蓝色表示。
- en: The true Q-values are represented in purple.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真实的 Q 值用紫色表示。
- en: The gap between the predictions and true values is represented by red arrows.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测值和真实值之间的差距通过红色箭头表示。
- en: In this case, action 0 has the highest predicted Q-value because of a large
    prediction error. This value will therefore be used to construct the target.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，由于较大的预测误差，动作 0 的预测 Q 值最高。因此，将使用这个值来构建目标。
- en: However, the action with the highest true value is action 2\. This illustration
    shows how the max term in the target favors **large positive estimation errors**,
    inducing an overestimation bias.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，具有最高真实价值的动作是动作 2。此图示展示了目标中的最大项如何偏向 **较大的正估计误差**，从而引发过度估计偏差。
- en: '![](../Images/1bf3d33b853a2c71019024eae646b567.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1bf3d33b853a2c71019024eae646b567.png)'
- en: Illustration of the overestimation bias.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 过度估计偏差的示意图。
- en: Decoupling action selection and evaluation
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解耦动作选择与评估
- en: 'To solve this problem, *Hasselt et al.* (2015)[3] propose a new target where
    the action is selected by the online network, while its value is estimated by
    the target network:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，*Hasselt 等人*（2015）[3] 提出了一个新的目标，其中动作由在线网络选择，而其值由目标网络估算：
- en: '![](../Images/1c377edbece9356607e4a8a0b5723f2f.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c377edbece9356607e4a8a0b5723f2f.png)'
- en: The Double DQN target
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Double DQN 目标
- en: By decoupling action selection and evaluation, the estimation bias is significantly
    reduced, leading to better value estimates and improved performance.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 通过解耦动作选择和评估，估计偏差显著减少，从而导致更好的价值估计和性能提升。
- en: '![](../Images/be824709136eed9f80431b6882ad92c8.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be824709136eed9f80431b6882ad92c8.png)'
- en: 'Double DQN provides stable and accurate value estimates, leading to improved
    performance. Source: Hasselt et al. (2015), Figure 3'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Double DQN 提供了稳定且准确的价值估计，带来了性能的提升。来源：Hasselt 等人（2015），图 3
- en: Double DQN in practice
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实践中的 Double DQN
- en: 'As expected, implementing Double DQN only requires us to modify the loss function:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，实现 Double DQN 只需要修改损失函数：
- en: Dueling DQN
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对抗 DQN
- en: '**Source:** [*Dueling Network Architectures for Deep Reinforcement Learning*](http://arxiv.org/abs/1511.06581)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**来源:** [*对抗网络架构用于深度强化学习*](http://arxiv.org/abs/1511.06581)'
- en: '**Improvement:** Separation of the value and advantage computation'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**改进:** 价值与优势计算的分离'
- en: State value, Q-value, and advantage
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 状态值、Q 值和优势
- en: 'In RL, we use several functions to estimate the value of a given state, action,
    or sequence of actions from a given state:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，我们使用多个函数来估计给定状态、动作或从给定状态开始的一系列动作的价值：
- en: '**State-value V(s):** The state value corresponds to the expected return when
    starting in a given state **s** and following a policy **π** thereafter.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态值 V(s):** 状态值对应于在给定状态 **s** 中开始并随后遵循策略 **π** 时的期望回报。'
- en: '**Q-value Q(s, a):** Similarly, the Q-value corresponds to the expected return
    when starting in a given state **s**, taking action **a,** and following a policy
    **π** thereafter.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Q 值 Q(s, a):** 类似地，Q 值对应于在给定状态 **s** 中开始，采取动作 **a** 并随后遵循策略 **π** 时的期望回报。'
- en: '**Advantage A(s, a):** The advantage is defined as the difference between the
    Q-value and the state-value in a given state **s** for an action **a**. It represents
    the inherent value of action **a** in the current state.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优势 A(s, a):** 优势定义为在给定状态 **s** 下，动作 **a** 的 Q 值与状态值之间的差异。它表示了在当前状态下，动作 **a**
    的固有价值。'
- en: The following figure attempts to represent the differences between these value
    functions on a backup diagram (*note that the state value is weighted by the probability
    of taking each action under policy* **π**).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图尝试表示这些价值函数之间的差异（*请注意，状态值是根据策略 **π** 下采取每个动作的概率加权的*）。
- en: '![](../Images/cba2c32f4c583fd964850468b441292f.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cba2c32f4c583fd964850468b441292f.png)'
- en: Visualization of the state value (in purple), state-action value (Q-function,
    in blue), and the advantage (in pink) on a backup diagram.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在备份图中可视化状态值（紫色）、状态-动作值（Q 函数，蓝色）和优势（粉色）。
- en: Usually, DQN estimates the Q-value directly, using a feed-forward neural network.
    This implies that DQN has to learn the Q-values for each action in each state
    independently.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，DQN 直接估计 Q 值，使用前馈神经网络。这意味着 DQN 必须独立地为每个状态下的每个动作学习 Q 值。
- en: The dueling architecture
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对抗架构
- en: 'Introduced by *Wang et al*.[4] in 2016, Dueling DQN uses a neural network with
    two separate streams of computation:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 由 *Wang 等人*（2016）[4] 提出的对抗 DQN 使用一个具有两个独立计算流的神经网络：
- en: The **state value stream** predicts the scalar value of a given state.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态值流** 用于预测给定状态的标量值。'
- en: The **advantage stream** predicts to predict the advantage of each action for
    a given state.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优势流** 用于预测给定状态下每个动作的优势。'
- en: This decoupling enables the **independent estimation** of the state value and
    advantages, which has several benefits. For instance, the network can learn state
    values without having to update the action values regularly. Additionally, it
    can better generalize to unseen actions in familiar states.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这种解耦使得**独立估计**状态值和优势成为可能，这带来了若干好处。例如，网络可以在不需要定期更新动作值的情况下学习状态值。此外，它还能更好地泛化到熟悉状态下未见过的动作。
- en: These improvements lead to stabler and faster convergence, especially in environments
    with many similar-valued actions.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这些改进导致了更稳定和更快速的收敛，特别是在具有许多相似价值动作的环境中。
- en: In practice, a dueling network uses a **common representation** (i.e. a shared
    linear or convolutional layer) parameterized by parameters **θ** before splitting
    into two streams, consisting of linear layers with parameters **α** and **β**
    respectively. The state value stream outputs a scalar value while the advantage
    stream returns a scalar value for each available action.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，决斗网络使用一个**共同表示**（即一个共享的线性或卷积层），由**θ**参数化，然后分为两个流，每个流由具有**α**和**β**参数的线性层组成。状态值流输出一个标量值，而优势流为每个可用的动作返回一个标量值。
- en: Adding the outputs of the two streams allows us to reconstruct the Q-value for
    each action as **Q(s, a) = V(s) + A(s, a)**.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 将两个流的输出相加，使我们能够重建每个动作的Q值，即**Q(s, a) = V(s) + A(s, a)**。
- en: An important detail is that the mean is usually subtracted from the advantages.
    Indeed, the advantages need to have **zero mean**, otherwise, it would be impossible
    to decompose Q into V and A, making the problem ill-defined. With this constraint,
    **V** represents the value of the state while **A** represents how much better
    or worse each action is compared to the average action in that state.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的细节是，优势通常会减去平均值。实际上，优势需要**零均值**，否则将无法将Q分解为V和A，从而使问题变得不明确。在这个约束下，**V**表示状态的价值，而**A**表示每个动作相对于该状态下平均动作的好坏程度。
- en: '![](../Images/af7d67549d33c0babd85d701ffa5900e.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af7d67549d33c0babd85d701ffa5900e.png)'
- en: Illustration of a dueling network
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 决斗网络示意图
- en: Dueling Network in practice
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实际应用中的决斗网络
- en: 'Here’s the Stoix implementation of a Q-network:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Stoix实现的Q网络：
- en: Distributional DQN
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式DQN
- en: '**Source:** [A distributional perspective on Reinforcement Learning](http://arxiv.org/abs/1707.06887)[5]'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**来源：** [关于强化学习的分布式视角](http://arxiv.org/abs/1707.06887)[5]'
- en: '**Improvement:** Richer value estimates'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**改进：** 更丰富的价值估计'
- en: The return distribution
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回报分布
- en: Most RL systems model the expectation of the return, however, a promising body
    of literature approaches RL from a distributional perspective. In this setting,
    the goal becomes to model the **return distribution**, which allows us to consider
    other statistics than the mean.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数强化学习系统建模的是回报的期望值，然而，越来越多的文献从分布式的角度研究强化学习。在这种设定下，目标变为建模**回报分布**，这使我们能够考虑平均值以外的其他统计量。
- en: In 2017, *Bellemare et al.*[5] published a distributional version of DQN called
    C51 predicting the return distribution for each action, reaching new state-of-the-art
    performances on the Atari benchmark.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年，*Bellemare等人*[5]发布了DQN的分布式版本C51，预测每个动作的回报分布，在Atari基准测试中达到了新的最先进的性能。
- en: '![](../Images/194c16ae2151c70103d4d1834a0a919b.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/194c16ae2151c70103d4d1834a0a919b.png)'
- en: Illustrated comparison between DQN and C51\. Source [5']
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: DQN与C51的比较示意图。来源 [5']
- en: Let’s take a step back and review the theory behind C51.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下C51背后的理论。
- en: 'In traditional RL, we evaluate a policy using the **Bellman Equation**, which
    allows us to define the Q-function in a recursive form. Alternatively, we can
    use a distributional version of the Bellman equation, which accounts for randomness
    in the returns:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的强化学习中，我们使用**Bellman方程**来评估策略，这使我们能够以递归形式定义Q函数。或者，我们可以使用分布式版本的Bellman方程，它考虑了回报中的随机性：
- en: '![](../Images/ef036b8c95c7be0d0c6ab34ccfdafdb7.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ef036b8c95c7be0d0c6ab34ccfdafdb7.png)'
- en: Standard and Distributional versions of the Bellman Equation
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Bellman方程的标准版和分布式版
- en: Here, **ρ** is the transition function.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，**ρ**是转移函数。
- en: The main difference between those functions is that **Q** **is a numerical value**,
    summing expectations over random variables. In contrast, **Z is a random variable**,
    summing the reward distribution and the discounted distribution of future returns.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数之间的主要区别在于**Q** **是一个数值**，它是对随机变量期望值的总和。相比之下，**Z是一个随机变量**，它是对奖励分布和未来回报的折扣分布的总和。
- en: 'The following illustration helps visualize how to derive **Z** from the distributional
    Bellman equation:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 以下插图有助于可视化如何从分布贝尔曼方程推导**Z**：
- en: Consider the distribution of returns **Z** at a given timestep and the transition
    operator **Pπ.** **PπZ** is the distribution of future returns **Z(s’, a’)**.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑在给定时间步长下回报分布**Z**以及转移操作符**Pπ**。**PπZ**是未来回报**Z(s’，a’)**的分布。
- en: Multiplying this by the discount factor **γ** contracts the distribution towards
    0 (as **γ** is less than 1).
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将此乘以折扣因子**γ**会使分布向0收缩（因为**γ**小于1）。
- en: Adding the reward distribution shifts the previous distribution by a set amount
    *(Note that the figure assumes a constant reward for simplicity. In practice,
    adding the reward distribution would shift but also modify the discounted return*).
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加奖励分布会将之前的分布平移一个固定的量（*注意，图中假设奖励为常数以简化计算。实际上，添加奖励分布会使分布发生平移，同时也会修改折扣回报*）。
- en: Finally, the distribution is projected on a discrete support using an L2 projection
    operator **Φ**.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，使用L2投影操作符**Φ**将分布投影到离散支持上。
- en: '![](../Images/97a3aac37d854dcd502775572e175e3b.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97a3aac37d854dcd502775572e175e3b.png)'
- en: 'Illustration of the distributional Bellman equation. Source: [5]'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 分布贝尔曼方程的插图。来源：[5]
- en: 'This fixed support is a vector of ***N*** atoms separated by a constant gap
    within a set interval:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这个固定的支持是一个由***N***个原子组成的向量，在一个固定区间内按恒定间隔分隔：
- en: '![](../Images/1aa87d6a69d5414ec7ebb7e17c72596a.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1aa87d6a69d5414ec7ebb7e17c72596a.png)'
- en: Definition of the discrete support **z**
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 离散支持**z**的定义
- en: 'At inference time, the Q-network returns an approximating distribution **dt**
    defined on this support with the probability mass **pθ(st, at)** on each atom
    ***i*** such that:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理时，Q网络返回一个在该支持上定义的近似分布**dt**，每个原子***i***上的概率质量**pθ(st, at)**满足：
- en: '![](../Images/da3c5c9967c24aea64b82f9e9891c613.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da3c5c9967c24aea64b82f9e9891c613.png)'
- en: Predicted return distribution
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 预测回报分布
- en: 'The goal is to update **θ** such that the distribution closely matches the
    true distribution of returns. To learn the probability masses, the target distribution
    is built using a **distributional variant of Bellman’s optimality equation**:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是更新**θ**，使得分布尽可能接近真实的回报分布。为了学习概率质量，目标分布是通过**贝尔曼最优性方程的分布式变种**来构建的：
- en: '![](../Images/81ad365cd4b43af4ea6fd2e4388e975e.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81ad365cd4b43af4ea6fd2e4388e975e.png)'
- en: Target return distribution
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 目标回报分布
- en: To be able to compare the distribution predicted by our neural network and the
    target distribution, we need to discretize the target distribution and project
    it on the same support **z**.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够比较我们神经网络预测的分布与目标分布，我们需要将目标分布离散化，并将其投影到相同的支持**z**上。
- en: 'To this end, we use an L2 projection (*a projection onto* ***z*** *such that
    the difference between the original and projected distribution is minimized in
    terms of the L2 norm*):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们使用L2投影（*将投影到* ***z*** *上，使得原始分布和投影分布之间的差异在L2范数下最小化*）：
- en: '![](../Images/c58835a895b9ebeba742c78533a5f1a1.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c58835a895b9ebeba742c78533a5f1a1.png)'
- en: L2 projection of the target distribution
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 目标分布的L2投影
- en: Finally, we need to define a loss function that minimizes the difference between
    the two distributions. As we’re dealing with distributions, we can’t simply subtract
    the prediction from the target, as we did previously.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要定义一个损失函数，最小化两个分布之间的差异。由于我们处理的是分布，不能像之前那样直接从目标中减去预测值。
- en: 'Instead, we minimize the Kullback-Leibler divergence between **dt** and **d’t**
    (in practice, this is implemented as a cross-entropy loss):'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们最小化**dt**和**d’t**之间的Kullback-Leibler散度（在实践中，这通常作为交叉熵损失来实现）：
- en: '![](../Images/4e6e7171841467790558b6c6ce556321.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4e6e7171841467790558b6c6ce556321.png)'
- en: KL divergence between the projected target and the predicted return distribution
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 预测回报分布和投影目标分布之间的KL散度
- en: '*For a more exhaustive description of Distributional DQN, you can refer to
    Massimiliano Tomassoli’s article[8] as well as Pascal Poupart’s video on the topic[11].*'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*要了解更全面的分布式DQN描述，可以参考Massimiliano Tomassoli的文章[8]以及Pascal Poupart关于该主题的视频[11]。*'
- en: C51 in practice
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实践中的C51
- en: The key components of C51 in Stoix are the Distributional head and the categorical
    loss, which uses double Q-learning by default as introduced previously. The choice
    of defining the C51 network as a head lets us use an MLP or a CNN torso interchangeably
    depending on the use case.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Stoix中C51的关键组件是分布头和类别损失，默认情况下使用前述的双Q学习。选择将C51网络定义为一个头部，使我们能够根据使用案例互换使用MLP或CNN作为主体。
- en: Noisy DQN
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Noisy DQN
- en: '**Source:** [Noisy Networks for Exploration](http://arxiv.org/abs/1706.10295)[6]'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**来源：** [噪声网络用于探索](http://arxiv.org/abs/1706.10295)[6]'
- en: '**Improvement:** Learnable and state-dependent exploration mechanism'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**改进：** 可学习的、状态依赖的探索机制'
- en: Noisy parameterization of Neural Networks
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络的噪声参数化
- en: As many off-policy algorithms, DQN relies on an epsilon-greedy policy as its
    main exploration mechanism. Therefore, the algorithm will behave greedily with
    respect to the Q-values most of the time and select random actions with a predefined
    probability.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 与许多离策略算法一样，DQN依赖于**ε-贪婪策略**作为其主要的探索机制。因此，该算法大部分时间会根据Q值进行贪婪选择，并以预定义的概率选择随机动作。
- en: '*Fortunato et al.*[6] introduce NoisyNets as a more flexible alternative. NoisyNets
    are neural networks whose weights and biases are **perturbed** by a **parametric
    function of Gaussian noise**. Similarly to an epsilon-greedy policy, such noise
    injects randomness in the agent’s action selection, thus encouraging exploration.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '*Fortunato et al.*[6]提出了NoisyNets作为一种更灵活的替代方案。NoisyNets是神经网络，其权重和偏置由**高斯噪声的参数化函数**扰动。类似于ε-贪婪策略，这种噪声通过向智能体的动作选择中注入随机性，从而鼓励探索。'
- en: However, this noise is scaled and offset by **learned parameters**, allowing
    the level of noise to be adapted state-by-state. This way, the balance between
    exploration and exploitation is optimized *dynamically* during training. Eventually,
    the network may learn to ignore the noise, but will do so at **different rates**
    in **different parts of the state space**, leading to more flexible exploration.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个噪声由**学习到的参数**进行缩放和偏移，允许噪声的水平在每个状态下动态调整。通过这种方式，探索和利用之间的平衡在训练过程中能够*动态*优化。最终，网络可能学会忽略噪声，但会在**状态空间的不同部分**以**不同的速度**实现这一点，从而导致更灵活的探索。
- en: 'A network parameterized by a vector of noisy parameters is defined as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 一个由噪声参数向量参数化的网络定义如下：
- en: '![](../Images/e34ae4ce245756ebcea722260869dc66.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e34ae4ce245756ebcea722260869dc66.png)'
- en: Neural Network parameterized by Noisy parameters
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 由噪声参数化的神经网络
- en: 'Therefore, a linear layer **y = wx + b** becomes:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个线性层**y = wx + b**变为：
- en: '![](../Images/0c00383ff99930448d0296b65faca5ec.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c00383ff99930448d0296b65faca5ec.png)'
- en: Noisy linear layer
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声线性层
- en: For performance, the noise is generated at inference time using **Factorized
    Gaussian Noise**. For a linear layer with **M** inputs and **N** outputs, a noise
    matrix of shape (**M x N**) is generated as a combination of two noise vectors
    with size **M** and **N**. This methods reduces the number of required random
    variables from **M x N** to **M + N**.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高性能，噪声在推理时使用**因式分解高斯噪声**生成。对于一个具有**M**个输入和**N**个输出的线性层，生成一个形状为（**M x N**）的噪声矩阵，该矩阵是两个大小分别为**M**和**N**的噪声向量的组合。此方法将所需的随机变量数量从**M
    x N**减少到**M + N**。
- en: 'The noise matrix is defined as the outer product of the noise vectors, each
    scaled by a function **f**:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声矩阵被定义为噪声向量的外积，每个向量由函数**f**缩放：
- en: '![](../Images/c2930b27127318d9c7088beb98339ab8.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2930b27127318d9c7088beb98339ab8.png)'
- en: Noise generation using Factorised Gaussian Noise
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 使用因式分解高斯噪声生成噪声
- en: Improved exploration
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 改进的探索
- en: The improved exploration induced by noisy networks allow a wide range of algorithms,
    such as DQN, Dueling DQN and A3C to benefit from improved performances with a
    reasonably low amount of extra parameters.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 由噪声网络引发的改进探索使得包括DQN、对战DQN和A3C等广泛的算法能够受益于较低额外参数的情况下获得更好的性能。
- en: '![](../Images/0546839f96938a710ebdd383e8aee747.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0546839f96938a710ebdd383e8aee747.png)'
- en: 'NoisyNets improve the performance of several algorithms on the Atari benchmark.
    Source: [6]'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: NoisyNets提高了几种算法在Atari基准测试上的表现。来源：[6]
- en: Noisy DQN in practice
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实践中的Noisy DQN
- en: 'In Stoix, we implement a noisy layer as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在Stoix中，我们实现了一个噪声层，如下所示：
- en: '*Note: All the linear layers in Rainbow are replaced with their noisy equivalent
    (see the* ***“Assembling Rainbow”*** *section for more details).*'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*注：Rainbow中的所有线性层都已替换为其噪声等价物（有关更多细节，请参见* ***“组合Rainbow”*** *部分）。*'
- en: Prioritized Experience Replay
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优先经验回放
- en: '**Source:** Prioritized Experience Replay[7]'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**来源：** 优先经验回放[7]'
- en: '**Improvement:** Prioritization of experiences with higher learning potential'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**改进**：优先选择具有更高学习潜力的经验'
- en: Estimating the Learning Potential
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 估算学习潜力
- en: After taking an environment step, vanilla DQN uniformly samples a batch of experiences
    (also called *transitions*) from a replay buffer and performs a gradient descent
    step on this batch. Although this approach produces satisfying results, some specific
    experiences might be more valuable from a learning perspective than others. Therefore,
    we could potentially speed up the training process by sampling such experiences
    more often.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行环境步骤后，普通的 DQN 会从重放缓冲区均匀地采样一批经验（也称为*转换*），并在这批经验上执行一次梯度下降步骤。尽管这种方法能够产生令人满意的结果，但某些特定的经验从学习角度来看可能比其他经验更有价值。因此，我们可以通过更频繁地采样这些经验来加速训练过程。
- en: 'This is precisely the idea explored in the Prioritized Experience Replay (PER)
    paper published by *Schaul et al.*[7] in 2016\. However, the main question remains:
    how to approximate the **expected learning potential** of a transition?'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是*Schual 等人*在2016年发布的优先经验回放（PER）论文中探讨的理念[7]。然而，主要的问题仍然是：如何近似地估算一个转换的**预期学习潜力**？
- en: 'One idealized criterion would be the amount the RL agent can learn from a transition
    in its current state (expected learning progress). While this measure is not directly
    accessible, a reasonable proxy is the magnitude of a transition’s TD error δ,
    which indicates how ‘surprising’ or unexpected the transition is: specifically,
    how far the value is from its next-step bootstrap estimate (Andre et al., 1998).'
  id: totrans-161
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一个理想化的标准是，RL 智能体在当前状态下可以从一个转换中学到多少（预期的学习进展）。虽然这个度量不可直接访问，但一个合理的代理是转换的 TD 错误
    δ 的大小，它表示转换有多么“出乎意料”或意外：具体来说，值与下一步自举估计之间的差距有多大（Andre 等人，1998）。
- en: Prioritized Experience Replay, Schaul et al. (2016)
  id: totrans-162
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 优先经验回放，Schual 等人（2016）
- en: 'As a reminder, the TD error is defined as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒一下，TD 错误定义如下：
- en: '![](../Images/47087938e81974e308d2a37b8fcb400a.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47087938e81974e308d2a37b8fcb400a.png)'
- en: The temporal-difference error
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 时间差错
- en: This metric is a decent estimate of the learning potential of a specific transition,
    as a high TD error indicates a large difference between the predicted and actual
    outcomes, meaning that the agent would benefit from updating its beliefs.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这个指标是特定转换学习潜力的一个合理估计，因为较高的 TD 错误表示预测结果与实际结果之间的差异较大，这意味着智能体从更新其信念中会受益。
- en: 'However, it is worth noting that alternative prioritization metrics are still
    being studied. For instance, *Lahire et al.*[9] (2022) argue that the optimal
    sampling scheme is distributed according to the per-sample gradient norms:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，值得注意的是，替代的优先级度量仍在研究中。例如，*Lahire 等人*（2022）认为，最优的采样方案是根据每个样本的梯度范数进行分布的[9]。
- en: '![](../Images/5ae86716a279e2c41dbbd7f9dc33b80a.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5ae86716a279e2c41dbbd7f9dc33b80a.png)'
- en: Per-sample gradient norms
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 每个样本的梯度范数
- en: However, let’s continue with the TD error, as Rainbow uses this metric.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，既然 Rainbow 使用的是这个度量，我们就继续使用 TD 错误。
- en: Deriving Sampling Probabilities
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推导采样概率
- en: 'Once we have selected the prioritization criterion, we can derive the probabilities
    of sampling each transition from it. In Prioritized Experience Replay, two alternatives
    are showcased:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们选择了优先级标准，就可以根据这个标准推导出每个转换的采样概率。在优先经验回放中，展示了两种替代方案：
- en: '**Proportional**: Here the probability of replaying a transition is equal to
    the absolute value of the associated TD error. A small positive constant is added
    to prevent transitions not being revisited once their error is zero.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**按比例**：在这种方式下，重放一个转换的概率等于相关 TD 错误的绝对值。为了防止转换在其错误为零时不再被重新访问，添加了一个小的正常数。'
- en: '**Rank-based**: In this mode, transitions are ranked in descending order according
    to their absolute TD error, and their probability is defined based on their rank.
    This option is supposed to be more robust as it is insensible to outliers.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于排名**：在这种模式下，转换根据其绝对 TD 错误按降序排列，并根据其排名定义其概率。这种方式被认为更加稳健，因为它对异常值不敏感。'
- en: The sampling probabilities are then normalized and raised to the power **α**,
    a hyperparameter determining the degree of prioritization (**α=0** is the uniform
    case).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 采样概率然后会被归一化，并升至**α**的幂，α是一个超参数，决定优先级的程度（**α=0**是均匀的情况）。
- en: '![](../Images/dd93373aeafc0258d8097ddcc8b98e69.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd93373aeafc0258d8097ddcc8b98e69.png)'
- en: Prioritization modes and probability normalization
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 优先级模式和概率归一化
- en: Importance sampling and bias annealing
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重要性采样和偏差退火
- en: In RL, the estimation of the expected value of the return relies on the assumption
    that the updates correspond to the same distribution as the expectation (i.e.,
    the uniform distribution). However, PER introduces bias as we now sample experiences
    according to their TD error.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，预期回报值的估计依赖于更新与期望（即均匀分布）相同的分布假设。然而，PER引入了偏差，因为我们现在是根据TD误差来抽样经验。
- en: To rectify this bias, we use **importance sampling**, a statistical method used
    to *estimate the properties of a distribution while sampling from a different
    distribution*. Importance sampling re-weights samples so that the estimates remain
    unbiased and accurate.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 为了纠正这种偏差，我们使用**重要性抽样**，这是一种用于*从不同分布中抽样时估计分布特性*的统计方法。重要性抽样会重新加权样本，使得估计结果保持无偏且准确。
- en: 'Typically, the correcting weights are defined as the ratio of the two probabilities:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，修正权重定义为两个概率的比率：
- en: '![](../Images/df361e74e24201fc048a33e4eaf8cc51.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df361e74e24201fc048a33e4eaf8cc51.png)'
- en: Importance sampling ratio
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 重要性抽样比率
- en: In this case, the target distribution is the uniform distribution, where every
    transition has a probability of being sampled equal to 1/**N**, with **N** being
    the size of the replay buffer.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，目标分布是均匀分布，其中每个过渡的被抽样概率为1/**N**，**N**是重放缓冲区的大小。
- en: 'Therefore, the importance sampling coefficient in the context of PER is defined
    by:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在优先经验回放（PER）上下文中的重要性抽样系数定义为：
- en: '![](../Images/8c435485dea322925507c5bc7a38e668.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c435485dea322925507c5bc7a38e668.png)'
- en: Importance sampling weight used in PER
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在PER中使用的重要性抽样权重
- en: 'With **β** a coefficient adjusting the amount of bias correction (the bias
    is fully corrected for **β=1**). Finally, the weights are normalized for stability:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '**β**是一个调整偏差修正量的系数（当**β=1**时，偏差完全被修正）。最后，为了稳定性，权重会进行归一化：'
- en: '![](../Images/868b56f8913a40c593f78675c09d784e.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/868b56f8913a40c593f78675c09d784e.png)'
- en: Normalization of the importance sampling weights
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 重要性抽样权重的归一化
- en: 'To summarize, here’s the full algorithm for Prioritized Experience Replay (the
    update and training steps are identical to DQN):'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，这是优先经验回放的完整算法（更新和训练步骤与DQN相同）：
- en: '![](../Images/e4e0192fa820ff77f9ef36d510c7fc91.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4e0192fa820ff77f9ef36d510c7fc91.png)'
- en: The Prioritized Experience Replay algorithm
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 优先经验回放算法
- en: Increased convergence speed with PER
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用PER提高了收敛速度
- en: The following plots highlight the performance benefits of PER. Indeed, the proportional
    and rank-based prioritization mechanisms enable DQN to reach the same baseline
    performances roughly twice as fast on the Atari benchmark.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表突出了PER的性能优势。事实上，基于比例和排名的优先机制使得DQN在Atari基准测试上能够大约两倍的速度达到相同的基线性能。
- en: '![](../Images/d4585c6368905c2144be046608fdc86f.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d4585c6368905c2144be046608fdc86f.png)'
- en: Normalized maximum and average scores (in terms of Double DQN performance) on
    57 Atari games. Source:[7]
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在57个Atari游戏中的最大和平均得分的归一化（以Double DQN性能为标准）。来源：[7]
- en: Prioritized Experience Replay in practice
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实践中的优先经验回放
- en: 'Stoix seamlessly integrates the [Flashbax](https://github.com/instadeepai/flashbax)
    library which provides a variety of replay buffers. Here are the relevant code
    snippets used to instantiate the replay buffer, compute the sampling probabilities
    from the TD error, and update the buffer’s priorities:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Stoix无缝集成了[Flashbax](https://github.com/instadeepai/flashbax)库，提供多种重放缓冲区。以下是用于实例化重放缓冲区、计算基于TD误差的抽样概率并更新缓冲区优先级的相关代码片段：
- en: Multi-step Learning
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多步学习
- en: '**Source:** [Reinforcement Learning: an Introduction, chapter 7](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**来源**：[强化学习：导论，第7章](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)'
- en: '**Improvement:** Enhanced reward signal and sample efficiency, reduced variance'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**改进**：增强的奖励信号和样本效率，减少的方差'
- en: 'Multi-step learning is an improvement on traditional one-step temporal difference
    learning which allows us to consider the return over **n** steps when building
    our targets. For instance, instead of considering the reward at the next timestep,
    we’ll consider the n-step truncated rewards (see the below equation). This process
    has several advantages, among which:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 多步学习是对传统的一步时间差学习的改进，它使我们在构建目标时可以考虑**n**步的回报。例如，我们不再仅仅考虑下一时间步的奖励，而是考虑n步截断的奖励（见下面的公式）。这个过程有几个优点，其中之一是：
- en: '**Immediate feedback:** considering a larger time horizon allows the agent
    to learn the value of state-action pairs much faster, especially in environments
    where rewards are delayed and specific actions might not pay out immediately.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**即时反馈：**考虑更长的时间跨度允许代理更快地学习状态-动作对的价值，特别是在奖励延迟且特定动作可能不会立即带来回报的环境中。'
- en: '**Sample efficiency:** Each update in multi-step learning incorporates information
    from multiple time steps, making each sample more informative. This improves sample
    efficiency, meaning the agent can learn more from fewer experiences.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**样本效率：**每次更新中的多步学习结合了多个时间步的信息，使每个样本更加有价值。这提高了样本效率，意味着代理可以从更少的经验中学到更多。'
- en: '**Balancing Bias and Variance:** Multi-step methods offer a trade-off between
    bias and variance. One-step methods have low bias but high variance, while multi-step
    methods have higher bias but lower variance. By tuning the number of steps, one
    can find a balance that works best for the given environment.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平衡偏差与方差：**多步方法在偏差和方差之间提供了一种权衡。一步方法偏差小但方差大，而多步方法偏差大但方差小。通过调节步数，可以找到最适合给定环境的平衡。'
- en: 'The multi-step distributional loss used in Rainbow is defined as:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Rainbow中使用的多步分布式损失定义如下：
- en: '![](../Images/4f2b412a351afc3dfa642272ce6a1fa6.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f2b412a351afc3dfa642272ce6a1fa6.png)'
- en: Multi-step target return distribution
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 多步目标回报分布
- en: 'In practice, using n-step returns implies a few adjustments to our code:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，使用n步回报意味着我们需要对代码进行一些调整：
- en: We now sample trajectories of **n** experiences, instead of individual experiences
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们现在采样的是**n**个经验的轨迹，而不是单个经验。
- en: The reward is replaced with the n-step discounted returns
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励被n步折扣回报替代
- en: The done flag is set to True if any of the **n** done flag is True
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果任何**n**个done标志为True，则done标志被设置为True。
- en: The next state **s(t+1)** is replaced by the last observation of the trajectory
    **s(t+n)**
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一状态**s(t+1)**被轨迹的最后一个观察值**s(t+n)**替代
- en: Multi-Step learning in practice
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实践中的多步学习
- en: 'Finally, we can reuse the categorical loss function used in C51 with these
    updated inputs:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以重新使用C51中使用的类别损失函数，配合这些更新后的输入：
- en: Assembling Rainbow
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 组装Rainbow
- en: 'Congratulations on making it this far! We now have a better understanding of
    all the moving pieces that constitute Rainbow. Here’s a summary of the Rainbow
    agent:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你走到了这一步！我们现在更好地理解了构成Rainbow的所有关键部分。以下是Rainbow代理的总结：
- en: '**Neural Network Architecture:**'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经网络架构：**'
- en: — **Torso:** A convolutional neural network (CNN) or multi-layer perceptron
    (MLP) base that creates embeddings for the head network.
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — **躯干：**卷积神经网络（CNN）或多层感知器（MLP）基础，用于为头部网络创建嵌入。
- en: — **Head:** Combines Dueling DQN and C51\. The value stream outputs the state
    value distribution over atoms, while the advantage stream outputs the advantage
    distribution over actions and atoms. These streams are aggregated, and Q-values
    are computed as the weighted sum of atom values and their respective probabilities.
    An action is selected using an epsilon-greedy policy.
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — **头部：**结合了Dueling DQN和C51。价值流输出原子上的状态价值分布，而优势流输出动作和原子上的优势分布。这些流被汇总，Q值作为原子值及其相应概率的加权和进行计算。使用epsilon-greedy策略选择一个动作。
- en: — **Noisy Layers:** All linear layers are replaced with their noisy equivalents
    to aid in exploration.
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — **噪声层：**所有线性层被噪声等效层替代，以帮助探索。
- en: '**Loss Function:** Uses a distributional loss modeling the n-step returns,
    where targets are computed using Double Q-learning.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损失函数：**使用分布式损失来建模n步回报，目标通过双Q学习计算得出。'
- en: '**Replay Buffer:** Employs a prioritization mechanism based on the TD error
    to improve learning efficiency.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回放缓冲区：**采用基于TD误差的优先机制，以提高学习效率。'
- en: 'Here’s the network used for the Rainbow head:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是用于Rainbow头部的网络：
- en: Performances and ablations
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能与消融实验
- en: To conclude this article, let’s take a closer look at Rainbow’s performances
    on the Atari benchmark, as well as the ablation study.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结这篇文章，让我们更深入地看看Rainbow在Atari基准测试中的表现以及消融研究。
- en: The following figure compares Rainbow with the other DQN baselines we studied.
    The measured metric is the median human-normalized score. In other words, the
    median human performance on Atari games is set to 100%, which enables us to quickly
    spot algorithms achieving a human level.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 下图将Rainbow与我们研究过的其他DQN基准进行比较。测量的指标是中位数人类标准化分数。换句话说，Atari游戏中人类的中位数表现被设定为100%，这使得我们可以快速发现达到人类水平的算法。
- en: 'Three of the DQN baselines reach this level after 200 million frames:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 三个DQN基准在2亿帧后达到了这个水平：
- en: '**Distributional DQN**'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式DQN**'
- en: '**Dueling DQN**'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**决斗DQN**'
- en: '**Prioritized Double DQN**'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优先级双重DQN**'
- en: Interestingly, Rainbow reaches the same level after only 44 million frames,
    making it **roughly 5 times more sample efficient** than the best baselines. At
    the end of training, it exceeds **200%** of the median human-normalized score.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，Rainbow在仅44百万帧后就达到了相同的水平，使其**大约比最好的基准效率高5倍**。在训练结束时，它超过了**200%**的中位数人类归一化得分。
- en: '![](../Images/ea23c893a795be0c7bd8eced1e0d9c2c.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea23c893a795be0c7bd8eced1e0d9c2c.png)'
- en: 'Median human-normalized performance across 57 Atari games. Each line represents
    a DQN baseline. Source: [2]'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在57款Atari游戏中的中位数人类归一化表现。每一行代表一个DQN基准。来源：[2]
- en: 'This second figure represents the ablation study, which represents the performances
    of Rainbow without one of its components. These results allow us to make several
    observations:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这第二个图表示去除实验，展示了没有Rainbow某个组件时的表现。这些结果让我们可以做出以下几项观察：
- en: The three most crucial components of Rainbow are the distributional head, the
    use of multi-step learning, and the prioritization of the replay buffer.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rainbow的三个最重要的组成部分是分布式头、使用多步学习和优先级重放缓冲区。
- en: Noisy layers contribute significantly to the overall performance. Using standard
    layers with an epsilon-greedy policy doesn’t allow the agent to reach the 200%
    score in 200 million frames.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 噪声层对整体表现有显著贡献。使用标准层和epsilon贪婪策略不能让智能体在2亿帧内达到200%的得分。
- en: Despite achieving strong performances on their own, the dueling structure and
    double Q-learning only provide marginal improvements in the context of Rainbow.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管它们各自取得了强大的表现，但在Rainbow的背景下，决斗结构和双重Q学习仅提供了微小的改进。
- en: '![](../Images/848c4c480a1e32c78e5393a9567b2c89.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/848c4c480a1e32c78e5393a9567b2c89.png)'
- en: 'Median human-normalized performance across 57 Atari games. Each line represents
    an ablation of Rainbow. Source: [2]'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在57款Atari游戏中的中位数人类归一化表现。每一行代表Rainbow的去除实验。来源：[2]
- en: Thank you very much for reading this article, I hope it provided you with a
    comprehensive introduction to Rainbow and its components. I highly advise reading
    through the [**Stoix implementation of Rainbow**](https://github.com/EdanToledo/Stoix/blob/main/stoix/systems/q_learning/ff_rainbow.py)
    for a more detailed description of the training process and the Rainbow architecture.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢你阅读本文，希望它能为你提供有关Rainbow及其组件的全面介绍。我强烈建议你阅读[**Stoix实现的Rainbow**](https://github.com/EdanToledo/Stoix/blob/main/stoix/systems/q_learning/ff_rainbow.py)，以便更详细地了解训练过程和Rainbow架构。
- en: Until next time 👋
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 下次见👋
- en: Bibliography
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考书目
- en: '[1] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,
    D., & Riedmiller, M. (2013). [***Playing Atari with Deep Reinforcement Learning***](http://arxiv.org/abs/1312.5602),
    arXiv'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,
    D., & Riedmiller, M. (2013). [***通过深度强化学习玩Atari***](http://arxiv.org/abs/1312.5602)，arXiv。'
- en: '[2] Hessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostrovski, G., Dabney,
    W., Horgan, D., Piot, B., Azar, M., & Silver, D. (2017). [***Rainbow: Combining
    Improvements in Deep Reinforcement Learning***](http://arxiv.org/abs/1710.02298),
    arXiv.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Hessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostrovski, G., Dabney,
    W., Horgan, D., Piot, B., Azar, M., & Silver, D. (2017). [***Rainbow：结合深度强化学习中的改进***](http://arxiv.org/abs/1710.02298)，arXiv。'
- en: '[3] van Hasselt, H., Guez, A., & Silver, D. (2015). [***Deep Reinforcement
    Learning with Double Q-learning***](http://arxiv.org/abs/1509.06461), arXiv.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] van Hasselt, H., Guez, A., & Silver, D. (2015). [***使用双重Q学习的深度强化学习***](http://arxiv.org/abs/1509.06461)，arXiv。'
- en: '[4] Wang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot, M., & de Freitas,
    N. (2016). [***Dueling Network Architectures for Deep Reinforcement Learning***](http://arxiv.org/abs/1511.06581)
    (No. arXiv:1511.06581), arXiv'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Wang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot, M., & de Freitas,
    N. (2016). [***深度强化学习的决斗网络架构***](http://arxiv.org/abs/1511.06581) (No. arXiv:1511.06581)，arXiv。'
- en: '[5] Bellemare, M. G., Dabney, W., & Munos, R. (2017). [***A Distributional
    Perspective on Reinforcement Learning***](http://arxiv.org/abs/1707.06887), arXiv'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Bellemare, M. G., Dabney, W., & Munos, R. (2017). [***强化学习的分布式视角***](http://arxiv.org/abs/1707.06887)，arXiv。'
- en: '[5''] Dabney, W., Ostrovski, G., Silver, D., & Munos, R. (2018). [***Implicit
    Quantile Networks for Distributional Reinforcement Learning***](http://arxiv.org/abs/1806.06923),
    arXiv'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '[5''] Dabney, W., Ostrovski, G., Silver, D., & Munos, R. (2018). [***用于分布式强化学习的隐式分位网络***](http://arxiv.org/abs/1806.06923)，arXiv。'
- en: '[[6]](http://arxiv.org/abs/1806.06923](http://arxiv.org/abs/1806.06923)[6])
    Fortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I., Graves, A., Mnih,
    V., Munos, R., Hassabis, D., Pietquin, O., Blundell, C., & Legg, S. (2019). [***Noisy
    Networks for Exploration***](http://arxiv.org/abs/1706.10295), arXiv.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '[[6]](http://arxiv.org/abs/1806.06923](http://arxiv.org/abs/1806.06923)[6])
    Fortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I., Graves, A., Mnih,
    V., Munos, R., Hassabis, D., Pietquin, O., Blundell, C., & Legg, S. (2019). [***探索的噪声网络***](http://arxiv.org/abs/1706.10295),
    arXiv.'
- en: '[7] Schaul, T., Quan, J., Antonoglou, I., & Silver, D. (2016). [***Prioritized
    Experience Replay***](http://arxiv.org/abs/1511.05952)***,*** arXiv'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Schaul, T., Quan, J., Antonoglou, I., & Silver, D. (2016). [***优先经验回放***](http://arxiv.org/abs/1511.05952)***,***
    arXiv'
- en: Additional resources
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 额外资源
- en: '[8] Massimiliano Tomassoli, [***Distributional RL: An intuitive explanation
    of Distributional RL***](https://mtomassoli.github.io/2017/12/08/distributional_rl/)'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Massimiliano Tomassoli, [***分布式强化学习：分布式强化学习的直观解释***](https://mtomassoli.github.io/2017/12/08/distributional_rl/)'
- en: '[9] Lahire, T., Geist, M., & Rachelson, E. (2022). [***Large Batch Experience
    Replay***](http://arxiv.org/abs/2110.01528), arXiv.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Lahire, T., Geist, M., & Rachelson, E. (2022). [***大批量经验回放***](http://arxiv.org/abs/2110.01528),
    arXiv.'
- en: '[10] Sutton, R. S., & Barto, A. G. (1998). ***Reinforcement Learning: An Introduction***.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Sutton, R. S., & Barto, A. G. (1998). ***强化学习：导论***。'
- en: '[11] Pascal Poupart, [***CS885 Module 5: Distributional RL***](https://youtu.be/r-Yk6-jagDU?si=9lqQHHNaQz8Uiclw)***,***
    YouTube'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Pascal Poupart, [***CS885模块5：分布式强化学习***](https://youtu.be/r-Yk6-jagDU?si=9lqQHHNaQz8Uiclw)***,***
    YouTube'
