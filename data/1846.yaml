- en: A Practical Guide to Contrastive Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对比学习实用指南
- en: 原文：[https://towardsdatascience.com/a-practical-guide-to-contrastive-learning-26e912c0362f?source=collection_archive---------1-----------------------#2024-07-30](https://towardsdatascience.com/a-practical-guide-to-contrastive-learning-26e912c0362f?source=collection_archive---------1-----------------------#2024-07-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-practical-guide-to-contrastive-learning-26e912c0362f?source=collection_archive---------1-----------------------#2024-07-30](https://towardsdatascience.com/a-practical-guide-to-contrastive-learning-26e912c0362f?source=collection_archive---------1-----------------------#2024-07-30)
- en: How to build your very first SimSiam model with FashionMNIST
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用 FashionMNIST 构建你的第一个 SimSiam 模型
- en: '[](https://mengliuz.medium.com/?source=post_page---byline--26e912c0362f--------------------------------)[![Mengliu
    Zhao](../Images/0b950a0785fa065db3319ed5be4a91de.png)](https://mengliuz.medium.com/?source=post_page---byline--26e912c0362f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--26e912c0362f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--26e912c0362f--------------------------------)
    [Mengliu Zhao](https://mengliuz.medium.com/?source=post_page---byline--26e912c0362f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mengliuz.medium.com/?source=post_page---byline--26e912c0362f--------------------------------)[![Mengliu
    Zhao](../Images/0b950a0785fa065db3319ed5be4a91de.png)](https://mengliuz.medium.com/?source=post_page---byline--26e912c0362f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--26e912c0362f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--26e912c0362f--------------------------------)
    [孟刘赵](https://mengliuz.medium.com/?source=post_page---byline--26e912c0362f--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--26e912c0362f--------------------------------)
    ·10 min read·Jul 30, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--26e912c0362f--------------------------------)
    ·阅读时长 10 分钟·2024年7月30日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Contrastive learning has many use cases these days. From NLP and computer vision
    to recommendation systems, contrastive learning can be used to learn underlying
    data representations without any explicit labels, which can then be used for downstream
    classification, detection, similarity search, etc.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 对比学习在当今有很多应用场景。从自然语言处理（NLP）和计算机视觉到推荐系统，对比学习可以在没有任何显式标签的情况下学习数据的潜在表示，然后可以用于下游的分类、检测、相似度搜索等任务。
- en: There are many online resources to help the audience understand the basic ideas
    of contrastive learning so that I won’t add one more blog post repeating the information.
    Instead, I will show you how to convert your supervised learning problem into
    a contrastive learning problem in this article. Specifically, I will start with
    a basic classification model for the [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist/tree/master)
    ([MIT licence](https://github.com/zalandoresearch/fashion-mnist/blob/master/LICENSE)).
    Then, I will proceed to an advanced problem with limited training labels (e.g.,
    reducing the full training set of 60,000 labels to 1,000). I will introduce [SimSiam](https://arxiv.org/pdf/2011.10566),
    a state-of-the-art method for contrastive learning, and show step-by-step instructions
    on modifying the original linear layers in the SimSiam style. Ultimately, I’ll
    show the results — SimSiam could improve the F1 score by 15% with a very basic
    configuration.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 网上有很多资源可以帮助读者理解对比学习的基本概念，因此我不会再写一篇重复这些信息的博客文章。相反，在本文中，我将展示如何将你的监督学习问题转化为对比学习问题。具体来说，我将从一个基本的分类模型开始，使用
    [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist/tree/master) 数据集（[MIT
    许可证](https://github.com/zalandoresearch/fashion-mnist/blob/master/LICENSE)）。接着，我将处理一个有有限训练标签的高级问题（例如，将完整的60,000个标签的训练集缩减为1,000个标签）。我将介绍
    [SimSiam](https://arxiv.org/pdf/2011.10566) 这一最先进的对比学习方法，并提供逐步的指导，说明如何按SimSiam风格修改原始的线性层。最终，我会展示结果
    —— SimSiam 在一个非常基础的配置下，能够提高15%的 F1 分数。
- en: '![](../Images/8640abb31d4b5da8c81b4a240542a64b.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8640abb31d4b5da8c81b4a240542a64b.png)'
- en: 'Image source: [https://pxhere.com/en/photo/395408](https://pxhere.com/en/photo/395408)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[https://pxhere.com/en/photo/395408](https://pxhere.com/en/photo/395408)
- en: Now, let’s start. First, we’ll load in the FashionMNIST dataset. A custom FashionMNIST
    class is used to obtain a subset of the training set named the finetune_dataset.
    The source code for the customer FashionMNIST class will be given at the end of
    this article.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始吧。首先，我们将加载 FashionMNIST 数据集。我们使用一个自定义的 FashionMNIST 类来获取训练集的一个子集，命名为
    finetune_dataset。自定义 FashionMNIST 类的源代码将在本文结尾处提供。
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The code will show a grid of images from the train_dataset
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 代码将展示来自train_dataset的图像网格。
- en: '![](../Images/8b4d3bbce67a51d76de2166c7d47e17c.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b4d3bbce67a51d76de2166c7d47e17c.png)'
- en: First 16 images from the FashionMNIST training set. Image by author.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 来自FashionMNIST训练集的前16张图像。图片来自作者。
- en: Next, we’ll define the supervised classification model. The architecture contains
    a backbone of convolutional layers and an MLP head of two linear layers. This
    will set a consistent baseline for the following experiments, as SimSiam will
    only replace the MLP head for contrastive learning purposes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义监督分类模型。该架构包含一个卷积层的主干网络和一个由两层线性层组成的MLP头部。这将为接下来的实验设置一个一致的基准，因为SimSiam仅会替换MLP头部以进行对比学习。
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We’ll train the model for 10 epochs:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练模型10个epoch：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Using the classification_report from the scikit-learn package, we’ll get the
    following results:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用scikit-learn包中的classification_report，我们将得到以下结果：
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/b99775d536be606ab36bba78f5014a32.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b99775d536be606ab36bba78f5014a32.png)'
- en: Classification results of the fully supervised model. Image by author.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 完全监督模型的分类结果。图片来自作者。
- en: 'Now, let’s think about **a new problem**. What should we do if we’re given
    a limited subset of the training set labels, e.g., only 1000 images out of the
    total 60,000 images are annotated? The natural idea is to simply train the model
    on the limited annotated dataset. So without changing the backbone, we let the
    model train on the limited subset for 100 epochs (we increase the epochs to have
    a fair comparison to our SimSiam training):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们思考**一个新问题**。如果我们只获得训练集标签的有限子集，例如，仅有60,000张图像中的1000张有标签，我们该怎么办？自然的想法是简单地在有限的标注数据集上训练模型。因此，在不改变主干网络的情况下，我们让模型在有限的子集上训练100个epoch（我们增加训练epoch数，以便与SimSiam的训练做公平比较）：
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/17e7139fe4c8f3e1fff111b4c66fbfc9.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17e7139fe4c8f3e1fff111b4c66fbfc9.png)'
- en: Fully supervised training loss on the limited training set. Image by author.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在有限训练集上的完全监督训练损失。图片来自作者。
- en: '![](../Images/4a1010578e8405e600400d717408e350.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4a1010578e8405e600400d717408e350.png)'
- en: Quantitative evaluation results on the testing set. Note the performance drops
    more than 25% by reducing the training size. Image by author.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试集上的定量评估结果。注意，通过减少训练集大小，性能下降超过25%。图片来自作者。
- en: Now it’s time for some **contrastive learning**. To mitigate the issue of insufficient
    annotation labels and fully utilize the large quantity of unlabelled data, contrastive
    learning could be used to effectively help the backbone learn the data representations
    without a specific task. The backbone could be frozen for a given downstream task
    and only train a shallow network on a limited annotated dataset to achieve satisfactory
    results.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是进行**对比学习**的时候了。为了缓解标注标签不足的问题，并充分利用大量的未标注数据，可以使用对比学习来有效地帮助主干网络学习数据表示，而无需特定任务。主干网络可以在给定的下游任务中冻结，并仅在有限的标注数据集上训练一个浅层网络，从而获得令人满意的结果。
- en: The most commonly used contrastive learning approaches include SimCLR, SimSiam,
    and MOCO (see my [previous article on MOCO](https://medium.com/towards-data-science/from-moco-v1-to-v3-towards-building-a-dynamic-dictionary-for-self-supervised-learning-part-1-745dc3b4e861)).
    Here, we compare SimCLR and SimSiam.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的对比学习方法包括SimCLR、SimSiam和MOCO（请参见我之前的[关于MOCO的文章](https://medium.com/towards-data-science/from-moco-v1-to-v3-towards-building-a-dynamic-dictionary-for-self-supervised-learning-part-1-745dc3b4e861)）。在这里，我们对SimCLR和SimSiam进行了比较。
- en: '**SimCLR** calculates over positive and negative pairs within the data batch,
    which requires hard negative mining, NT-Xent loss (which extends the cosine similarity
    loss over a batch) and a large batch size. SimCLR also requires the LARS optimizer
    to accommodate a large batch size.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**SimCLR**在数据批次内计算正样本对和负样本对，这需要硬负样本挖掘、NT-Xent损失（它扩展了批次上的余弦相似度损失）以及较大的批量大小。SimCLR还需要LARS优化器来适应较大的批量大小。'
- en: '**SimSiam,** however, uses a Siamese architecture, which avoids using negative
    pairs and further avoids the need for large batch sizes. The differences between
    SimSiam and SimCLR are given in the table below.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**SimSiam**使用的是一种孪生网络架构，它避免了使用负样本对，进而避免了对大批量数据的需求。SimSiam与SimCLR的区别如下表所示。'
- en: '![](../Images/543adb5cca61405edac9477bfbeac9f8.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/543adb5cca61405edac9477bfbeac9f8.png)'
- en: Comparison between SimCLR and SimSiam. Image by author.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: SimCLR和SimSiam的比较。图片来自作者。
- en: '![](../Images/a9008b9698d06ee3fddd458255e54136.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9008b9698d06ee3fddd458255e54136.png)'
- en: 'The SimSiam architecture. Image source: [https://arxiv.org/pdf/2011.10566](https://arxiv.org/pdf/2011.10566)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: SimSiam架构。图片来源：[https://arxiv.org/pdf/2011.10566](https://arxiv.org/pdf/2011.10566)
- en: 'We can see from the figure above that the SimSiam architecture only contains
    two parts: the encoder/backbone and the predictor. During training time, the gradient
    propagation of the Siamese part is stopped, and the cosine similarity is calculated
    between the outputs of the predictors and the backbone.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从上图可以看出，SimSiam架构仅包含两个部分：编码器/主干网络和预测器。在训练过程中，Siamese部分的梯度传播被停止，并计算预测器和主干网络输出之间的余弦相似度。
- en: 'So, how do we implement this architecture in reality? Continuing on the supervised
    classification design, we **keep the backbone the same and only modify the MLP
    layer**. In the supervised learning architecture, the MLP outputs a 10-element
    vector indicating the probabilities of the 10 classes. But for SimSiam, the purpose
    is not to perform “classification” but to learn the “representation,” so we need
    the output to be of the same dimension as the backbone output for loss calculation.
    And the negative_cosine_similarity is given below:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何在实际中实现这个架构呢？继续基于监督分类设计，我们**保持主干网络不变，仅修改MLP层**。在监督学习架构中，MLP输出一个包含10个元素的向量，表示10个类别的概率。但对于SimSiam来说，目标不是进行“分类”，而是学习“表示”，因此我们需要输出的维度与主干网络的输出维度相同，以便进行损失计算。负余弦相似度公式如下：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The pseudo-code for training the SimSiam is given in the original paper below:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 训练SimSiam的伪代码在原始论文中如下所示：
- en: '![](../Images/168babbce2573a8be4068e0476bb70fd.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/168babbce2573a8be4068e0476bb70fd.png)'
- en: 'Training pseudo-code for SimSiam. Source: [https://arxiv.org/pdf/2011.10566](https://arxiv.org/pdf/2011.10566)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: SimSiam的训练伪代码。来源：[https://arxiv.org/pdf/2011.10566](https://arxiv.org/pdf/2011.10566)
- en: 'And we convert it into real training code:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其转化为真实的训练代码：
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We trained for 100 epochs as a fair comparison to the limited supervised training;
    the training loss is shown below. Note: Due to its Siamese design, SimSiam could
    be very sensitive to hyperparameters like learning rate and MLP hidden layers.
    The original SimSiam paper provides a detailed configuration for the ResNet50
    backbone. For the ViT-based backbone, we recommend reading the [MOCO v3 paper](https://arxiv.org/abs/2104.02057),
    which adopts the SimSiam model in a momentum update scheme.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练了100个epoch，以便与有限的监督训练进行公平比较；训练损失如下所示。注意：由于其Siamese设计，SimSiam可能对超参数，如学习率和MLP隐藏层，非常敏感。原始的SimSiam论文提供了ResNet50主干网络的详细配置。对于基于ViT的主干网络，我们建议阅读[MOCO
    v3论文](https://arxiv.org/abs/2104.02057)，该论文采用了SimSiam模型，并使用动量更新方案。
- en: '![](../Images/650a9d50562c4074180b497046a8f065.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/650a9d50562c4074180b497046a8f065.png)'
- en: Training loss for SimSiam. Image by author.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: SimSiam的训练损失。图片由作者提供。
- en: 'Then, we run the trained SimSiam on the testing set and visualize the representations
    using UMAP reduction:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在测试集上运行训练好的SimSiam，并使用UMAP降维可视化表示：
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/34435a04bcac905eee507c19a059ffbf.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/34435a04bcac905eee507c19a059ffbf.png)'
- en: The UMAP of the SimSiam representation over the testing set. Image by author.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: SimSiam表示在测试集上的UMAP降维。图片由作者提供。
- en: 'It’s interesting to see that there are two small islands in the reduced-dimension
    map above: class 5, 7, 8, and some 9\. If we pull out the FashionMNIST class list,
    we know that these classes correspond to footwear such as “Sandal,” “Sneaker,”
    “Bag,” and “Ankle boot.” The big purple cluster corresponds to clothing classes
    like “T-shirt/top,” “Trousers,” “Pullover,” “Dress,” “Coat,” and “Shirt.” The
    SimSiam demonstrates learning a meaningful representation in the vision domain.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，在上面的降维图中，出现了两个小岛：类别5、7、8以及部分9。如果我们查看FashionMNIST类别列表，就知道这些类别对应的是鞋类，如“凉鞋”、“运动鞋”、“包”和“短靴”。而大的紫色簇对应的是衣物类，如“T恤/上衣”、“裤子”、“套头衫”、“连衣裙”、“外套”和“衬衫”。SimSiam展示了在视觉领域中学习有意义的表示。
- en: Now that we have the correct representations, how can they benefit our classification
    problem? We simply load the trained SimSiam backbone into our classification model.
    However, instead of fine-tuning the whole architecture in the limited training
    set, we fine-tuned the linear layers and froze the backbone because we didn’t
    want to corrupt the representation already learned.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经得到了正确的表示，那么它们如何帮助我们的分类问题呢？我们只需将训练好的SimSiam主干网络加载到我们的分类模型中。然而，我们并不是在有限的训练集上微调整个架构，而是微调线性层并冻结主干网络，因为我们不想破坏已经学到的表示。
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Here is the evaluation result of the SimSiam-pre-trained classification model.
    The average F1 score is increased by 15% compared to the supervised-only method.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这是SimSiam预训练分类模型的评估结果。与仅使用监督学习的方法相比，平均F1分数提高了15%。
- en: '![](../Images/b58599f74751115b271a349a81cf8209.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b58599f74751115b271a349a81cf8209.png)'
- en: The classification scores of the SimSiam model fine-tune on the limited set.
    Image by author.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: SimSiam模型在有限数据集上微调后的分类分数。图像来自作者。
- en: Summary. We showcase a simple but intuitive example, using FashionMNIST for
    contrastive learning. By using SimSiam for backbone pre-training and only fine-tuning
    the linear layers on the limited training set (which contains only 2% of the labels
    of the full training set), we increased the average F1 score by 15% over the fully
    supervised learning method. The trained weights, the notebook, and the customized
    FashionMNIST dataset class are all included in this [GitHub repository](https://github.com/adoskk/MachineLearningBasics/tree/main/unsupervised_learning/simsiam).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 总结。我们展示了一个简单但直观的例子，使用FashionMNIST进行对比学习。通过使用SimSiam进行骨干网络预训练，仅在有限的训练集（仅包含完整训练集的2%标签）上微调线性层，我们将平均F1分数提高了15%，超过了完全监督学习方法。训练好的权重、笔记本和自定义的FashionMNIST数据集类都包含在这个[GitHub仓库](https://github.com/adoskk/MachineLearningBasics/tree/main/unsupervised_learning/simsiam)中。
- en: Give it a try!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 试试看！
- en: '**References:**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献：**'
- en: Chen et al., Exploring simple siamese representation learning. CVPR 2021.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等，探索简单的Siamese表示学习。CVPR 2021。
- en: Chen et al., A simple framework for contrastive learning of visual representations.
    ICML 2020.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等，视觉表示的对比学习简单框架。ICML 2020。
- en: Chen et al., An Empirical Study of Training Self-Supervised Vision Transformers.
    ICCV 2021.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等，训练自监督视觉Transformer的经验研究。ICCV 2021。
- en: 'Xiao et al., Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine
    Learning Algorithms. arXiv preprint 2017\. Github: [https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao等，Fashion-MNIST：用于基准测试机器学习算法的新型图像数据集。arXiv预印本 2017。Github：[https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)
