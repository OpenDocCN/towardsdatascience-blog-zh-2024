- en: 'Memory in AI: Key Benefits and Investment Considerations'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工智能中的记忆：关键好处与投资考虑
- en: 原文：[https://towardsdatascience.com/the-important-role-of-memory-in-agentic-ai-896b22542b3e?source=collection_archive---------15-----------------------#2024-06-18](https://towardsdatascience.com/the-important-role-of-memory-in-agentic-ai-896b22542b3e?source=collection_archive---------15-----------------------#2024-06-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-important-role-of-memory-in-agentic-ai-896b22542b3e?source=collection_archive---------15-----------------------#2024-06-18](https://towardsdatascience.com/the-important-role-of-memory-in-agentic-ai-896b22542b3e?source=collection_archive---------15-----------------------#2024-06-18)
- en: Memory will be a critical component that dramatically improves the performance
    of AI systems — both in accuracy and efficiency
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 记忆将成为显著提高人工智能系统性能的关键因素——无论是在准确性还是效率方面
- en: '[](https://medium.com/@sandibesen?source=post_page---byline--896b22542b3e--------------------------------)[![Sandi
    Besen](../Images/97361d97f50269f70b6621da2256bc29.png)](https://medium.com/@sandibesen?source=post_page---byline--896b22542b3e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--896b22542b3e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--896b22542b3e--------------------------------)
    [Sandi Besen](https://medium.com/@sandibesen?source=post_page---byline--896b22542b3e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@sandibesen?source=post_page---byline--896b22542b3e--------------------------------)[![Sandi
    Besen](../Images/97361d97f50269f70b6621da2256bc29.png)](https://medium.com/@sandibesen?source=post_page---byline--896b22542b3e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--896b22542b3e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--896b22542b3e--------------------------------)
    [Sandi Besen](https://medium.com/@sandibesen?source=post_page---byline--896b22542b3e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--896b22542b3e--------------------------------)
    ·5 min read·Jun 18, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--896b22542b3e--------------------------------)
    ·5分钟阅读·2024年6月18日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Just as humans depend on memory to make informed decisions and draw logical
    conclusions, AI relies on its ability to retrieve relevant information, understand
    contexts, and learn from past experiences. This article delves into why memory
    is pivotal for AI, exploring its role in recall, reasoning, and continuous learning.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 就像人类依赖记忆做出明智的决策和得出合乎逻辑的结论一样，人工智能也依赖其检索相关信息、理解上下文和从过去经验中学习的能力。本文深入探讨了为什么记忆对人工智能至关重要，探索了它在回忆、推理和持续学习中的作用。
- en: '![](../Images/0300026eea8f0efe2791300fde5612c2.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0300026eea8f0efe2791300fde5612c2.png)'
- en: 'colorful brain with microchip representing memory source: DALLE3'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 色彩斑斓的大脑与代表记忆的微芯片 来源：DALLE3
- en: Memory’s Role in Recall
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 记忆在回忆中的作用
- en: Some believe that enlarging the context window will enhance model performance,
    as it allows the model to ingest more information. While this is true to an extent,
    our current understanding of how language models prioritize context is still developing.
    In fact, studies have shown that “model performance is highest when when relevant
    information occurs at the beginning or end of its input context.”[[1]](https://arxiv.org/pdf/2307.03172)
    The larger a context window, the more likely we are to encounter the infamous
    “lost in the middle” problem, where specific facts or text are not recalled by
    the model due to important information being buried in the middle [[2]](https://dev.to/llmware/why-long-context-windows-for-llms-can-be-deceptive-lost-in-the-middle-problem-oj2).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人认为扩大上下文窗口将提升模型性能，因为这使模型能够摄取更多信息。虽然在某种程度上这是正确的，但我们目前对于语言模型如何优先考虑上下文的理解仍在发展。事实上，研究表明，“当相关信息出现在输入上下文的开头或结尾时，模型性能最高。”[[1]](https://arxiv.org/pdf/2307.03172)
    上下文窗口越大，我们越有可能遇到著名的“迷失在中间”问题，即由于重要信息被埋藏在中间，模型无法回忆起特定的事实或文本[[2]](https://dev.to/llmware/why-long-context-windows-for-llms-can-be-deceptive-lost-in-the-middle-problem-oj2)。
- en: To understand how memory impacts recall, consider how humans process information.
    When we travel, we passively listen to many announcements including airline advertisements,
    credit card offers, safety briefings, luggage collection details, etc. We may
    not realize how much information we absorb until it is time for us to recall relevant
    pieces. For instance, if a language model that is relying on retrieving relevant
    information to answer a question, rather than its inherent knowledge, is asked
    “What should I do in case of an emergency landing?” it might not be able to recall
    the pertinent details needed to answer this important question because too much
    information is retrieved. However, with a long-term memory, the model can store
    and recall the most critical information, enabling more effective reasoning with
    the proper context.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解记忆如何影响回忆，考虑人类是如何处理信息的。当我们旅行时，我们会被动地听到很多公告，包括航空公司广告、信用卡优惠、安全简报、行李领取信息等。我们可能没有意识到我们吸收了多少信息，直到我们需要回忆相关内容时。例如，如果一个依赖于检索相关信息来回答问题的语言模型，而不是依赖固有知识，被问到“紧急迫降时我该怎么做？”时，它可能无法回忆出回答这个重要问题所需的相关细节，因为检索了太多的信息。然而，拥有长期记忆的模型可以存储和回忆最关键的信息，从而在适当的上下文中进行更有效的推理。
- en: Memory’s Role in Reasoning and Continuous Learning
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 记忆在推理和持续学习中的作用
- en: Memory provides essential context and allows models to understand past problem-solving
    approaches, identifying what worked and what needs improvement. It doesn’t just
    offer important context; it also equips models with the ability to recall the
    methods previously used to solve problems, recognize successful strategies, and
    pinpoint areas needing improvement. This improvement can in turn aid the model’s
    ability to effectively reason about complex multi-step tasks. Without adequate
    reasoning, language models struggle to understand tasks, think logically about
    objectives, solve multistep problems, or utilize appropriate tools. You can read
    more about the importance of reasoning and advanced reasoning techniques in my
    previous article [here](https://medium.com/ai-mind-labs/advanced-language-model-reasoning-pre-training-fine-tuning-and-inference-time-techniques-f5c87ad080f5).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆提供了重要的上下文，使模型能够理解过去解决问题的方法，识别哪些方法有效，哪些需要改进。它不仅提供了重要的上下文；还使模型能够回忆出先前用于解决问题的方法，识别成功的策略，并找出需要改进的领域。这一改进反过来有助于模型在复杂的多步骤任务中进行有效的推理。如果缺乏足够的推理能力，语言模型将难以理解任务，无法逻辑性地思考目标，解决多步骤问题，或者使用适当的工具。你可以阅读我上一篇文章中关于推理和高级推理技术的重要性的内容，[点击这里](https://medium.com/ai-mind-labs/advanced-language-model-reasoning-pre-training-fine-tuning-and-inference-time-techniques-f5c87ad080f5)。
- en: Consider the example of manually finding relevant data in a company’s data warehouse.
    There are thousands of tables, but because you possess an understanding of what
    data is needed it allows you to focus on a subset. After hours of searching, relevant
    data is found across five different tables. Three months later, when the data
    needs updating, the search process must be repeated but you can’t remember the
    5 source tables you used to create this new report. The manual search process
    repeats again. Without long term memory, a language model might approach the problem
    the same way — with brute force — until it finds the relevant data to complete
    the task. However a language model equipped with long term memory could store
    its initial search plan, a description of each table, and a revised plan based
    on its search findings from each table. When the data needs refreshing, it can
    start from a previously successful approach, improving efficiency and performance.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 以手动在公司数据仓库中查找相关数据为例。那里有成千上万的表格，但由于你理解需要哪些数据，因此可以专注于一个子集。经过数小时的搜索，相关数据被找到了，分布在五个不同的表格中。三个月后，当数据需要更新时，必须重复搜索过程，但你可能记不起曾用过的5个源表格来生成这个新报告。手动搜索过程再次重复。如果没有长期记忆，语言模型可能会以相同的方式解决这个问题——凭借蛮力——直到找到相关数据来完成任务。然而，配备了长期记忆的语言模型可以存储初始的搜索计划、每个表格的描述以及基于每个表格的搜索结果所修订的计划。当数据需要更新时，它可以从之前成功的方法开始，从而提高效率和性能。
- en: This method allows systems to learn over time, continually revising the best
    approach to tasks and accumulating knowledge to produce more efficient, higher-performing
    autonomous systems.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法使系统能够随着时间的推移不断学习，持续修订任务的最佳方法，积累知识，从而生成更高效、性能更强的自主系统。
- en: Evaluating the Investment to Include Long-Term Memory in Your AI Solutions
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估将长期记忆纳入您的AI解决方案的投资
- en: Incorporating long-term memory into AI systems can significantly enhance their
    capabilities, but determining whether this capability is worthy of the necessary
    development investment involves consideration.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 将长期记忆纳入AI系统中，可以显著提升其能力，但是否值得为这种能力进行必要的开发投入，需要慎重考虑。
- en: '**1\. Understand the Nature of the Tasks**'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 理解任务的性质**'
- en: '**Complexity and Duration**: If your tasks involves complex, multi-step processes
    or requires information retention over long periods, long-term memory can improve
    efficiency and accuracy. For example, project management applications, where tasks
    span over months can benefit from AI’s ability to remember and adapt from previous
    context and iterations.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂性和持续时间**：如果您的任务涉及复杂的多步骤过程或需要长期保存信息，长期记忆可以提高效率和准确性。例如，项目管理应用中，任务可能跨越数月，通过AI的记忆和从之前的上下文和迭代中获得的适应能力，可以带来益处。'
- en: '**Context Sensitivity**: Tasks that heavily depend on contextual understanding,
    such as customer service interactions, personalization in marketing, or medical
    diagnostics, can leverage long-term memory to provide more personalized responses.
    For instance, an IT help desk assistant would benefit from remembering if a customer
    has already encountered this problem and how it was trouble shooted during previous
    interactions.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文敏感性**：依赖于上下文理解的任务，如客户服务互动、营销个性化或医疗诊断，可以利用长期记忆提供更个性化的响应。例如，IT帮助台助手可以从记住客户是否曾遇到过这个问题以及如何在之前的互动中解决中获益。'
- en: '**2\. Assess the Volume and Variability of Data**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 评估数据的量和变化性**'
- en: '**High Data Volume**: If your application deals with large amounts of data
    that need to be referenced regularly, long-term memory can prevent the need for
    repeatedly processing the same information — saving time and computational resources.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高数据量**：如果您的应用需要定期引用大量数据，长期记忆可以避免重复处理相同的信息，从而节省时间和计算资源。'
- en: '**Data Variability**: In environments where the data changes frequently, long-term
    memory helps in keeping the AI updated with the latest information, ensuring more
    accurate outputs without having to re-train.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据变化性**：在数据频繁变化的环境中，长期记忆有助于保持AI更新最新信息，从而确保更准确的输出，而无需重新训练。'
- en: '**3\. Evaluate the Cost-Benefit Ratio**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. 评估成本效益比**'
- en: '**Balance Cost with Performance Benefit**: Implementing long-term memory can
    be resource-intensive and will continue to scale over time as more memories accumulate.
    It is important to weigh the financial investment of data storage against potential
    performance improvements. For small businesses or applications with limited resources,
    the efficiency of Small Language Models (SLMs) with long-term memory might offer
    a more balanced solution​​​​.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平衡成本与性能收益**：实施长期记忆可能需要大量资源，并且随着记忆的积累，需求将不断增长。重要的是权衡数据存储的财务投资与可能的性能提升之间的关系。对于小型企业或资源有限的应用，具备长期记忆的小型语言模型（SLMs）可能提供一个更为平衡的解决方案。'
- en: '**Competitive Advantage**: By improving the efficiency and effectiveness of
    AI applications, long-term memory can provide a significant competitive edge,
    enabling businesses to offer superior services compared to those using traditional
    models without memory capabilities.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**竞争优势**：通过提高AI应用的效率和效果，长期记忆可以提供显著的竞争优势，使企业能够提供比那些使用传统无记忆功能模型的企业更优质的服务。'
- en: '**4\. Address Security and Compliance Concerns**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**4\. 解决安全性和合规性问题**'
- en: '**Data Privacy**: Long-term memory involves storing more data, which can raise
    privacy concerns. Ensure that your system complies with data protection regulations
    and that sensitive information follows best security practice.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据隐私**：长期记忆涉及存储更多数据，这可能引发隐私问题。确保您的系统符合数据保护规定，并且敏感信息遵循最佳安全实践。'
- en: In Essence…
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本质上……
- en: Incorporating long-term memory into AI systems presents a significant opportunity
    to enhance their capabilities by providing improvements in accuracy, efficiency,
    and contextual understanding. However, deciding whether to invest in this capability
    requires consideration and cost to benefit analysis. If implemented strategically,
    the inclusion of long term memory can delivery tangible benefits to your AI solutions.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 将长期记忆融入AI系统为提升其能力提供了重大机会，通过提高准确性、效率和上下文理解能力。然而，决定是否投资这一能力需要仔细考虑并进行成本效益分析。如果战略性地实施，长期记忆的引入能够为你的AI解决方案带来实际的好处。
- en: Have questions or think that something needs to be further clarified? Drop me
    a DM on [Linkedin](https://www.linkedin.com/in/sandibesen/)! I‘m always eager
    to engage in food for thought and iterate on my work. My work does not represent
    the opinion of my employer.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 有问题或者认为某些内容需要进一步澄清吗？请在[Linkedin](https://www.linkedin.com/in/sandibesen/)上给我发私信！我总是渴望参与深度思考并对我的工作进行迭代。我的工作并不代表我雇主的观点。
