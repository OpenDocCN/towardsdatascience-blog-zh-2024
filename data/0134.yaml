- en: Running Local LLMs and VLMs on the Raspberry Pi
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在树莓派上运行本地LLM和VLM
- en: 原文：[https://towardsdatascience.com/running-local-llms-and-vlms-on-the-raspberry-pi-57bd0059c41a?source=collection_archive---------0-----------------------#2024-01-14](https://towardsdatascience.com/running-local-llms-and-vlms-on-the-raspberry-pi-57bd0059c41a?source=collection_archive---------0-----------------------#2024-01-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/running-local-llms-and-vlms-on-the-raspberry-pi-57bd0059c41a?source=collection_archive---------0-----------------------#2024-01-14](https://towardsdatascience.com/running-local-llms-and-vlms-on-the-raspberry-pi-57bd0059c41a?source=collection_archive---------0-----------------------#2024-01-14)
- en: '**Get models like Phi-2, Mistral, and LLaVA running locally on a Raspberry
    Pi with Ollama**'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**在树莓派上使用Ollama本地运行Phi-2、Mistral和LLaVA等模型**'
- en: '[](https://medium.com/@pyesonekyaw?source=post_page---byline--57bd0059c41a--------------------------------)[![Pye
    Sone Kyaw](../Images/907574a7d2de57a4cc0ce36d73234a7a.png)](https://medium.com/@pyesonekyaw?source=post_page---byline--57bd0059c41a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--57bd0059c41a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--57bd0059c41a--------------------------------)
    [Pye Sone Kyaw](https://medium.com/@pyesonekyaw?source=post_page---byline--57bd0059c41a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@pyesonekyaw?source=post_page---byline--57bd0059c41a--------------------------------)[![Pye
    Sone Kyaw](../Images/907574a7d2de57a4cc0ce36d73234a7a.png)](https://medium.com/@pyesonekyaw?source=post_page---byline--57bd0059c41a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--57bd0059c41a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--57bd0059c41a--------------------------------)
    [Pye Sone Kyaw](https://medium.com/@pyesonekyaw?source=post_page---byline--57bd0059c41a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--57bd0059c41a--------------------------------)
    ·7 min read·Jan 14, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--57bd0059c41a--------------------------------)
    ·阅读时长7分钟·2024年1月14日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/b138fcad68e649f781cde62d61d153ab.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b138fcad68e649f781cde62d61d153ab.png)'
- en: 'Host LLMs and VLMs using Ollama on the Raspberry Pi — Source: Author'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Ollama在树莓派上托管LLM和VLM — 来源：作者
- en: Ever thought of running your own large language models (LLMs) or vision language
    models (VLMs) on your own device? You probably did, but the thoughts of setting
    things up from scratch, having to manage the environment, downloading the right
    model weights, and the lingering doubt of whether your device can even handle
    the model has probably given you some pause.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 有没有想过在自己的设备上运行自己的大语言模型（LLM）或视觉语言模型（VLM）？你可能有过这样的想法，但从零开始设置环境、下载合适的模型权重、以及设备是否能承载模型的疑虑，可能让你有些犹豫。
- en: Let’s go one step further than that. Imagine operating your own LLM or VLM on
    a device no larger than a credit card — a Raspberry Pi. Impossible? Not at all.
    I mean, I’m writing this post after all, so it definitely is possible.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再进一步。想象一下，在一个不大于信用卡大小的设备上操作你自己的LLM或VLM——一台树莓派。难以想象？一点也不。我是在写这篇文章，所以它绝对是可能的。
- en: '**Possible, yes. But why would you even do it?**'
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**可能，没错。但你为什么要这么做呢？**'
- en: LLMs at the edge seem quite far-fetched at this point in time. But this particular
    niche use case should mature over time, and we will definitely see some cool edge
    solutions being deployed with an all-local generative AI solution running on-device
    at the edge.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，边缘的LLM似乎有些不可思议。但这个特定的小众用例应该随着时间的推移而成熟，我们肯定会看到一些很酷的边缘解决方案，通过全本地生成式AI解决方案在设备端部署。
- en: It’s also about pushing the limits to see what’s possible. If it can be done
    at this extreme end of the compute scale, then it can be done at any level in
    between a Raspberry Pi and a big and powerful server GPU.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是关于突破极限，看看什么是可能的。如果在计算能力的极端范围内可以做到，那在树莓派和强大服务器GPU之间的任何级别上都可以做到。
- en: Traditionally, edge AI has been closely linked with computer vision. Exploring
    the deployment of LLMs and VLMs at the edge adds an exciting dimension to this
    field that is just emerging.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，边缘AI与计算机视觉密切相关。探索在边缘部署LLM和VLM为这一新兴领域增添了令人兴奋的维度。
- en: Most importantly, I just wanted to do something fun with my recently acquired
    Raspberry Pi 5.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，我只是想用我最近购买的树莓派5做些有趣的事情。
- en: So, how do we achieve all this on a Raspberry Pi? Using Ollama!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何在树莓派上实现这一切呢？使用Ollama！
- en: '**What is Ollama?**'
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**什么是Ollama？**'
- en: '[Ollama](https://ollama.ai/) has emerged as one of the best solutions for running
    local LLMs on your own personal computer without having to deal with the hassle
    of setting things up from scratch. With just a few commands, everything can be
    set up without any issues. Everything is self-contained and works wonderfully
    in my experience across several devices and models. It even exposes a REST API
    for model inference, so you can leave it running on the Raspberry Pi and call
    it from your other applications and devices if you want to.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ollama](https://ollama.ai/)已经成为在个人计算机上运行本地LLM的最佳解决方案之一，无需从头开始设置麻烦。只需几个命令，所有内容都能顺利设置。根据我的经验，它在多个设备和模型上都能完美运行，且一切都是自包含的。它甚至暴露了一个用于模型推理的REST
    API，因此如果需要，你可以让它在树莓派上运行，并从其他应用程序和设备调用它。'
- en: '![](../Images/25665bd9949055ec8e2ed0a29e83ea4c.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/25665bd9949055ec8e2ed0a29e83ea4c.png)'
- en: '[Ollama’s Website](https://ollama.ai/)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ollama官网](https://ollama.ai/)'
- en: There’s also [Ollama Web UI](https://github.com/ollama-webui/ollama-webui) which
    is a beautiful piece of AI UI/UX that runs seamlessly with Ollama for those apprehensive
    about command-line interfaces. It’s basically a local ChatGPT interface, if you
    will.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 还有[Ollama Web UI](https://github.com/ollama-webui/ollama-webui)，这是一款与Ollama完美兼容的美丽AI
    UI/UX界面，适合那些对命令行界面感到不安的人。可以将其视为一个本地的ChatGPT界面。
- en: Together, these two pieces of open-source software provide what I feel is the
    best locally hosted LLM experience right now.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个开源软件共同提供了我认为是目前最好的本地托管LLM体验。
- en: Both Ollama and Ollama Web UI support VLMs like LLaVA too, which opens up even
    more doors for this edge Generative AI use case.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Ollama和Ollama Web UI也支持像LLaVA这样的VLM，这为这种边缘生成AI应用场景打开了更多的可能性。
- en: '**Technical Requirements**'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**技术要求**'
- en: 'All you need is the following:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你只需要以下几个条件：
- en: Raspberry Pi 5 (or 4 for a less speedy setup) — Opt for the 8GB RAM variant
    to fit the 7B models.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树莓派5（或使用4，速度较慢）——选择8GB RAM版本，以便容纳7B模型。
- en: SD Card — Minimally 16GB, the larger the size the more models you can fit. Have
    it already loaded with an appropriate OS such as Raspbian Bookworm or Ubuntu
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SD卡——最小16GB，尺寸越大可以存储更多模型。已加载适当的操作系统，如Raspbian Bookworm或Ubuntu。
- en: An internet connection
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个互联网连接
- en: Like I mentioned earlier, running Ollama on a Raspberry Pi is already near the
    extreme end of the hardware spectrum. Essentially, any device more powerful than
    a Raspberry Pi, provided it runs a Linux distribution and has a similar memory
    capacity, should theoretically be capable of running Ollama and the models discussed
    in this post.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我之前提到的，在树莓派上运行Ollama已经接近硬件的极限。理论上，任何比树莓派更强大的设备，只要运行Linux发行版并且具有相似的内存容量，都应该能够运行Ollama和本文讨论的模型。
- en: '**1\. Installing Ollama**'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**1\. 安装Ollama**'
- en: To install Ollama on a Raspberry Pi, we’ll avoid using Docker to conserve resources.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在树莓派上安装Ollama，我们将避免使用Docker以节省资源。
- en: In the terminal, run
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在终端中运行
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You should see something similar to the image below after running the command
    above.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述命令后，你应该看到类似于下图的内容。
- en: '![](../Images/bb3793bb2c4843f497f4e73232983878.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bb3793bb2c4843f497f4e73232983878.png)'
- en: 'Source: Author'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者
- en: 'Like the output says, go to 0.0.0.0:11434 to verify that Ollama is running.
    It is normal to see the ‘WARNING: No NVIDIA GPU detected. Ollama will run in CPU-only
    mode.’ since we are using a Raspberry Pi. But if you’re following these instructions
    on something that is supposed to have a NVIDIA GPU, something did not go right.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '如输出所示，前往0.0.0.0:11434验证Ollama是否在运行。由于我们使用的是树莓派，看到‘WARNING: No NVIDIA GPU detected.
    Ollama will run in CPU-only mode.’是正常的。但如果你在一个应该有NVIDIA GPU的设备上执行这些指令，说明出现了问题。'
- en: For any issues or updates, refer to the [Ollama GitHub repository](https://github.com/jmorganca/ollama/tree/main).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如遇任何问题或更新，请参考[Ollama GitHub仓库](https://github.com/jmorganca/ollama/tree/main)。
- en: '**2\. Running LLMs through the command line**'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2\. 通过命令行运行LLM**'
- en: Take a look at [the official Ollama model library](https://ollama.ai/library)
    for a list of models that can be run using Ollama. On an 8GB Raspberry Pi, models
    larger than 7B won’t fit. Let’s use Phi-2, a 2.7B LLM from Microsoft, now under
    MIT license.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[官方Ollama模型库](https://ollama.ai/library)，获取可以通过Ollama运行的模型列表。在8GB的树莓派上，7B以上的模型将无法运行。我们将使用Phi-2，这是微软推出的2.7B
    LLM，目前采用MIT许可证。
- en: We’ll use the default Phi-2 model, but feel free to use any of the other tags
    found [here](https://ollama.ai/library/phi/tags). Take a look at the [model page
    for Phi-2](https://ollama.ai/library/phi) to see how you can interact with it.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用默认的 Phi-2 模型，但你可以自由使用 [这里](https://ollama.ai/library/phi/tags) 中找到的其他标签。查看
    [Phi-2 的模型页面](https://ollama.ai/library/phi)，看看如何与其互动。
- en: In the terminal, run
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在终端中运行
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Once you see something similar to the output below, you already have a LLM running
    on the Raspberry Pi! It’s that simple.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你看到类似下面的输出，你就已经在 Raspberry Pi 上运行了一个 LLM！就是这么简单。
- en: '![](../Images/49c2e2e0adacb151d5df152f9a79421e.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/49c2e2e0adacb151d5df152f9a79421e.png)'
- en: 'Source: Author'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者
- en: '![](../Images/f998a88c1609414a0df28296d7c24245.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f998a88c1609414a0df28296d7c24245.png)'
- en: 'Here’s an interaction with Phi-2 2.7B. Obviously, you won’t get the same output,
    but you get the idea. | Source: Author'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这是与 Phi-2 2.7B 的互动。显然，你不会得到相同的输出，但你能明白意思。 | 来源：作者
- en: You can try other models like Mistral, Llama-2, etc, just make sure there is
    enough space on the SD card for the model weights.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试其他模型，如 Mistral、Llama-2 等，但请确保 SD 卡有足够的空间来存储模型权重。
- en: Naturally, the bigger the model, the slower the output would be. On Phi-2 2.7B,
    I can get around 4 tokens per second. But with a Mistral 7B, the generation speed
    goes down to around 2 tokens per second. A token is roughly equivalent to a single
    word.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 自然，模型越大，输出就越慢。在 Phi-2 2.7B 上，我每秒大约能得到 4 个 token，但使用 Mistral 7B 时，生成速度下降到每秒大约
    2 个 token。一个 token 大致等于一个单词。
- en: '![](../Images/1e4a03f08206bd6b1839a3b9bda9da8b.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e4a03f08206bd6b1839a3b9bda9da8b.png)'
- en: 'Here’s an interaction with Mistral 7B | Source: Author'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这是与 Mistral 7B 的互动 | 来源：作者
- en: Now we have LLMs running on the Raspberry Pi, but we are not done yet. The terminal
    isn’t for everyone. Let’s get Ollama Web UI running as well!
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经在 Raspberry Pi 上运行了 LLM，但我们还没有完成。终端并不适合所有人。让我们也让 Ollama Web UI 运行起来吧！
- en: '**3\. Installing and Running Ollama Web UI**'
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3\. 安装和运行 Ollama Web UI**'
- en: We shall follow the instructions on the [official Ollama Web UI GitHub Repository](https://github.com/ollama-webui/ollama-webui)
    to install it without Docker. It recommends minimally Node.js to be >= 20.10 so
    we shall follow that. It also recommends Python to be at least 3.11, but Raspbian
    OS already has that installed for us.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按照 [官方 Ollama Web UI GitHub 仓库](https://github.com/ollama-webui/ollama-webui)
    上的说明进行安装，不使用 Docker。它建议 Node.js 至少为 >= 20.10，因此我们将遵循这个要求。它还建议 Python 至少为 3.11，但
    Raspbian 操作系统已经为我们安装了这个版本。
- en: We have to install Node.js first. In the terminal, run
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要安装 Node.js。在终端中运行
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Change the 20.x to a more appropriate version if need be for future readers.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，可以将 20.x 更改为更合适的版本，以便未来的读者参考。
- en: Then run the code block below.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然后运行下面的代码块。
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: It’s a slight modification of what is provided on GitHub. Do take note that
    for simplicity and brevity we are not following best practices like using virtual
    environments and we are using the — break-system-packages flag. If you encounter
    an error like uvicorn not being found, restart the terminal session.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对 GitHub 上提供的内容的稍微修改。请注意，为了简洁起见，我们没有遵循最佳实践，比如使用虚拟环境，并且我们使用了 — break-system-packages
    标志。如果遇到找不到 uvicorn 的错误，请重新启动终端会话。
- en: If all goes correctly, you should be able to access Ollama Web UI on port 8080
    through [http://0.0.0.0:8080](http://0.0.0.0:8080) on the Raspberry Pi, or through
    http://<Raspberry Pi’s local address>:8080/ if you are accessing through another
    device on the same network.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，你应该能够通过 Raspberry Pi 上的 [http://0.0.0.0:8080](http://0.0.0.0:8080) 访问
    Ollama Web UI，或者如果你通过同一网络上的其他设备访问，可以通过 http://<Raspberry Pi 的本地地址>:8080/ 访问。
- en: '![](../Images/0370778c38e9e9f6934c1657dc6b70b1.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0370778c38e9e9f6934c1657dc6b70b1.png)'
- en: 'If you see this, yes, it worked | Source: Author'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看到这个，是的，它成功了 | 来源：作者
- en: Once you’ve created an account and logged in, you should see something similar
    to the image below.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你创建了一个帐户并登录，你应该会看到类似下图的界面。
- en: '![](../Images/d47c4570ae32b131c008cb4fde2e1d02.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d47c4570ae32b131c008cb4fde2e1d02.png)'
- en: 'Source: Author'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者
- en: If you had downloaded some model weights earlier, you should see them in the
    dropdown menu like below. If not, you can go to the settings to download a model.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前下载过一些模型权重，应该会在下拉菜单中看到它们。如果没有，你可以去设置中下载一个模型。
- en: '![](../Images/c30e7ce637b4c6e9c2fb346cb3737e84.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c30e7ce637b4c6e9c2fb346cb3737e84.png)'
- en: 'Available models will appear here | Source: Author'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的模型将在此处显示 | 来源：作者
- en: '![](../Images/320964e9287eebf5ddb47c817225ac9e.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/320964e9287eebf5ddb47c817225ac9e.png)'
- en: 'If you want to download new models, go to Settings > Models to pull models
    | Source: Author'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想下载新模型，前往设置 > 模型以拉取模型 | 来源：作者
- en: The entire interface is very clean and intuitive, so I won’t explain much about
    it. It’s truly a very well-done open-source project.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 整个界面非常简洁直观，因此我不会详细解释它。它真的是一个做得非常好的开源项目。
- en: '![](../Images/dcb666e351985cb48670b7a30d3effdd.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dcb666e351985cb48670b7a30d3effdd.png)'
- en: 'Here’s an interaction with Mistral 7B through Ollama Web UI | Source: Author'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过Ollama Web UI与Mistral 7B的互动 | 来源：作者
- en: '**4\. Running VLMs through Ollama Web UI**'
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**4\. 通过Ollama Web UI运行VLM**'
- en: Like I mentioned at the start of this article, we can also run VLMs. Let’s run
    LLaVA, a popular open source VLM which also happens to be supported by Ollama.
    To do so, download the weights by pulling ‘llava’ through the interface.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我在本文开头提到的，我们还可以运行VLM。让我们运行LLaVA，一个流行的开源VLM，它也恰好被Ollama支持。为了实现这一点，通过界面拉取“llava”以下载权重。
- en: Unfortunately, unlike LLMs, it takes quite some time for the setup to interpret
    the image on the Raspberry Pi. The example below took around 6 minutes to be processed.
    The bulk of the time is probably because the image side of things is not properly
    optimised yet, but this will definitely change in the future. The token generation
    speed is around 2 tokens/second.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，与大型语言模型（LLMs）不同，Raspberry Pi上的图像解读设置需要相当长的时间。下面的示例处理时间大约为6分钟。大部分时间可能是因为图像处理部分尚未得到充分优化，但这一点在未来肯定会有所改善。令牌生成速度约为2个令牌/秒。
- en: '![](../Images/4fd80948dfb710e5691d6841cdbd63ed.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4fd80948dfb710e5691d6841cdbd63ed.png)'
- en: 'Query Image Source: Pexels'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 查询图像来源：Pexels
- en: '**To wrap it all up**'
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**总结**'
- en: At this point we are pretty much done with the goals of this article. To recap,
    we’ve managed to use Ollama and Ollama Web UI to run LLMs and VLMs like Phi-2,
    Mistral, and LLaVA on the Raspberry Pi.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经完成了本文的目标。总结一下，我们成功地使用Ollama和Ollama Web UI在Raspberry Pi上运行了Phi-2、Mistral和LLaVA等LLM和VLM。
- en: I can definitely imagine quite a few use cases for locally hosted LLMs running
    on the Raspberry Pi (or another other small edge device), especially since 4 tokens/second
    does seem like an acceptable speed with streaming for some use cases if we are
    going for models around the size of Phi-2.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我完全能想象出在Raspberry Pi（或其他小型边缘设备）上本地托管的LLM有很多应用场景，尤其是如果我们针对像Phi-2这样的模型，4个令牌/秒的速度在某些应用场景中通过流式传输来看是可接受的。
- en: The field of ‘small’ LLMs and VLMs, somewhat paradoxically named given their
    ‘large’ designation, is an active area of research with quite a few model releases
    recently. Hopefully this emerging trend continues, and more efficient and compact
    models continue to get released! Definitely something to keep an eye on in the
    coming months.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: “小型”LLM和VLM领域，虽然它们被冠以“大型”的名称，显得有些矛盾，但这是一个活跃的研究领域，最近发布了不少模型。希望这一新兴趋势能够持续下去，更多高效且紧凑的模型能够继续发布！这绝对是未来几个月值得关注的内容。
- en: '**Disclaimer**: I have no affiliation with Ollama or Ollama Web UI. All views
    and opinions are my own and do not represent any organisation.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**免责声明**：我与Ollama或Ollama Web UI没有任何关联。所有观点和意见均为个人观点，不代表任何组织。'
