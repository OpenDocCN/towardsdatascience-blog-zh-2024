- en: How to Build a Semantic Search Engine for Emojis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•æ„å»ºä¸€ä¸ªè¯­ä¹‰æœç´¢å¼•æ“æ¥æœç´¢è¡¨æƒ…ç¬¦å·
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/how-to-build-a-semantic-search-engine-for-emojis-ef4c75e3f7be?source=collection_archive---------8-----------------------#2024-01-10](https://towardsdatascience.com/how-to-build-a-semantic-search-engine-for-emojis-ef4c75e3f7be?source=collection_archive---------8-----------------------#2024-01-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/how-to-build-a-semantic-search-engine-for-emojis-ef4c75e3f7be?source=collection_archive---------8-----------------------#2024-01-10](https://towardsdatascience.com/how-to-build-a-semantic-search-engine-for-emojis-ef4c75e3f7be?source=collection_archive---------8-----------------------#2024-01-10)
- en: Find The Sentiment Youâ€™re Looking For ğŸ”ğŸ¤”ğŸ˜€ğŸš€
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯»æ‰¾ä½ æƒ³è¦çš„æƒ…æ„Ÿ ğŸ”ğŸ¤”ğŸ˜€ğŸš€
- en: '[](https://medium.com/@jacob_marks?source=post_page---byline--ef4c75e3f7be--------------------------------)[![Jacob
    Marks, Ph.D.](../Images/94d9832b8706d1044e3195386613bfab.png)](https://medium.com/@jacob_marks?source=post_page---byline--ef4c75e3f7be--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ef4c75e3f7be--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ef4c75e3f7be--------------------------------)
    [Jacob Marks, Ph.D.](https://medium.com/@jacob_marks?source=post_page---byline--ef4c75e3f7be--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jacob_marks?source=post_page---byline--ef4c75e3f7be--------------------------------)[![Jacob
    Marks, Ph.D.](../Images/94d9832b8706d1044e3195386613bfab.png)](https://medium.com/@jacob_marks?source=post_page---byline--ef4c75e3f7be--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ef4c75e3f7be--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ef4c75e3f7be--------------------------------)
    [Jacob Marks, Ph.D.](https://medium.com/@jacob_marks?source=post_page---byline--ef4c75e3f7be--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ef4c75e3f7be--------------------------------)
    Â·15 min readÂ·Jan 10, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ef4c75e3f7be--------------------------------)
    Â·é˜…è¯»æ—¶é—´15åˆ†é’ŸÂ·2024å¹´1æœˆ10æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/38c4d89af0b8bd1549b7af17e735042e.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38c4d89af0b8bd1549b7af17e735042e.png)'
- en: Semantic search over emojis for â€œhalloweenâ€ using a custom emoji search engine.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è‡ªå®šä¹‰è¡¨æƒ…ç¬¦å·æœç´¢å¼•æ“è¿›è¡Œâ€œhalloweenâ€çš„è¯­ä¹‰æœç´¢ã€‚
- en: 'If youâ€™ve ever used Google Docs, or Slack, you may have noticed that when you
    type a â€œ:â€ immediately followed by another character, a list of emojis pops up:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æ›¾ç»ä½¿ç”¨è¿‡Google Docsæˆ–Slackï¼Œä½ å¯èƒ½æ³¨æ„åˆ°ï¼Œå½“ä½ è¾“å…¥ä¸€ä¸ªâ€œ:â€ç´§æ¥ç€å¦ä¸€ä¸ªå­—ç¬¦æ—¶ï¼Œä¼šå¼¹å‡ºä¸€ä¸ªè¡¨æƒ…ç¬¦å·åˆ—è¡¨ï¼š
- en: '![](../Images/22de006beada9fd015206018637aeaf2.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22de006beada9fd015206018637aeaf2.png)'
- en: Since I discovered this, Iâ€™ve been making *major* use out of the feature. I
    add emojis into way more of my messages, blog posts, and other written works than
    I ever imagined I would. I actually got so accustomed to this means of adding
    emojis that I installed [Rocket](https://matthewpalmer.net/rocket/) â€” a free app
    that brings the same emoji searchability to all text boxes and text editors on
    the computer. Itâ€™s a game changer.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªä»æˆ‘å‘ç°è¿™ä¸ªåŠŸèƒ½ä»¥æ¥ï¼Œæˆ‘å°±ä¸€ç›´åœ¨*å¤§é‡*ä½¿ç”¨å®ƒã€‚æˆ‘åœ¨æˆ‘çš„æ¶ˆæ¯ã€åšå®¢æ–‡ç« å’Œå…¶ä»–ä¹¦é¢ä½œå“ä¸­æ·»åŠ äº†æ¯”æˆ‘æ›¾ç»æƒ³è±¡çš„æ›´å¤šçš„è¡¨æƒ…ç¬¦å·ã€‚äº‹å®ä¸Šï¼Œæˆ‘å·²ç»ä¹ æƒ¯äº†è¿™ç§æ·»åŠ è¡¨æƒ…ç¬¦å·çš„æ–¹å¼ï¼Œä»¥è‡³äºæˆ‘å®‰è£…äº†[Rocket](https://matthewpalmer.net/rocket/)â€”â€”ä¸€æ¬¾å…è´¹åº”ç”¨ç¨‹åºï¼Œå®ƒå°†ç›¸åŒçš„è¡¨æƒ…ç¬¦å·æœç´¢åŠŸèƒ½å¸¦å…¥åˆ°ç”µè„‘ä¸Šçš„æ‰€æœ‰æ–‡æœ¬æ¡†å’Œæ–‡æœ¬ç¼–è¾‘å™¨ä¸­ã€‚è¿™çœŸæ˜¯ä¸€ä¸ªæ”¹å˜æ¸¸æˆè§„åˆ™çš„å·¥å…·ã€‚
- en: 'But as Iâ€™ve used these emoji search engines more and more, Iâ€™ve noticed a frustrating
    limitation: all of the searches are based on the *exact* text in your query and
    in the name and description of the emoji. Essentially, you need to search for
    something incredibly precisely for any results to show up.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œéšç€æˆ‘ä½¿ç”¨è¿™äº›è¡¨æƒ…ç¬¦å·æœç´¢å¼•æ“çš„æ¬¡æ•°è¶Šæ¥è¶Šå¤šï¼Œæˆ‘æ³¨æ„åˆ°ä¸€ä¸ªä»¤äººæ²®ä¸§çš„é™åˆ¶ï¼šæ‰€æœ‰æœç´¢éƒ½åŸºäºä½ æŸ¥è¯¢ä¸­çš„*ç²¾ç¡®*æ–‡æœ¬ï¼Œä»¥åŠè¡¨æƒ…ç¬¦å·çš„åç§°å’Œæè¿°ã€‚å®é™…ä¸Šï¼Œä½ éœ€è¦éå¸¸ç²¾ç¡®åœ°æœç´¢æŸä¸ªè¯ï¼Œæ‰èƒ½æ˜¾ç¤ºå‡ºä»»ä½•ç»“æœã€‚
- en: 'Hereâ€™s an example: if we search for â€œaudioâ€, not a single result shows up:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸€ä¸ªä¾‹å­ï¼šå¦‚æœæˆ‘ä»¬æœç´¢â€œaudioâ€ï¼Œä¸€ä¸ªç»“æœéƒ½ä¸ä¼šæ˜¾ç¤ºå‡ºæ¥ï¼š
- en: '![](../Images/8a07cd9f11d0ee6dbaebc12f5c01ab83.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a07cd9f11d0ee6dbaebc12f5c01ab83.png)'
- en: This isnâ€™t because the set of emojis is lacking in the audio category. If we
    were to type in â€œmusicâ€ or â€œspeakerâ€, we would get a long list of results. Instead,
    it has to do with the fact that the specific string of text â€œaudioâ€ does not show
    up in the name or textual description associated with any of the emojis.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¹¶ä¸æ˜¯å› ä¸ºè¡¨æƒ…ç¬¦å·é›†åˆä¸­ç¼ºå°‘éŸ³é¢‘ç±»çš„è¡¨æƒ…ç¬¦å·ã€‚å¦‚æœæˆ‘ä»¬è¾“å…¥â€œmusicâ€æˆ–â€œspeakerâ€ï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°ä¸€é•¿ä¸²ç»“æœã€‚å®é™…ä¸Šï¼Œè¿™ä¸â€œaudioâ€è¿™ä¸ªç‰¹å®šæ–‡æœ¬ä¸²çš„åç§°æˆ–æè¿°ä¸­å¹¶æ²¡æœ‰å‡ºç°ä»»ä½•è¡¨æƒ…ç¬¦å·ç›¸å…³çš„äº‹å®æœ‰å…³ã€‚
- en: '![](../Images/4df2c1363dab033580d0ac83ffbf4870.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4df2c1363dab033580d0ac83ffbf4870.png)'
- en: 'This relatively minor inconvenience bothered me so much that I decided to build
    this:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç›¸å¯¹è¾ƒå°çš„ä¸ä¾¿è®©æˆ‘æ„Ÿåˆ°å¦‚æ­¤å›°æ‰°ï¼Œä»¥è‡³äºæˆ‘å†³å®šæ„å»ºè¿™ä¸ªï¼š
- en: By â€œthisâ€, I mean an open-source semantic emoji search engine, with both UI-centric
    and CLI versions. The Python CLI library can be found [here](https://github.com/jacobmarks/emoji_search),
    and the UI-centric version can be found [here](https://github.com/jacobmarks/emoji-search-plugin).
    You can also play around with a hosted (also free) version of the UI emoji search
    engine online [here](https://try.fiftyone.ai/datasets/emojis/samples).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: â€œè¿™ä¸ªâ€æŒ‡çš„æ˜¯ä¸€ä¸ªå¼€æºçš„è¯­ä¹‰è¡¨æƒ…ç¬¦å·æœç´¢å¼•æ“ï¼Œå…·æœ‰åŸºäºUIå’ŒCLIä¸¤ä¸ªç‰ˆæœ¬ã€‚Python CLIåº“å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/jacobmarks/emoji_search)æ‰¾åˆ°ï¼ŒåŸºäºUIçš„ç‰ˆæœ¬å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/jacobmarks/emoji-search-plugin)æ‰¾åˆ°ã€‚ä½ è¿˜å¯ä»¥åœ¨çº¿å°è¯•ä¸€ä¸ªæ‰˜ç®¡çš„ï¼ˆä¹Ÿæ˜¯å…è´¹çš„ï¼‰UIè¡¨æƒ…ç¬¦å·æœç´¢å¼•æ“ï¼Œç‚¹å‡»[è¿™é‡Œ](https://try.fiftyone.ai/datasets/emojis/samples)ã€‚
- en: '![](../Images/2bb7629d1dedcae260e9e34cf7d77451.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2bb7629d1dedcae260e9e34cf7d77451.png)'
- en: '*Command line version of the Semantic Emoji Search Engine*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¯­ä¹‰è¡¨æƒ…ç¬¦å·æœç´¢å¼•æ“çš„å‘½ä»¤è¡Œç‰ˆæœ¬*'
- en: Building this was not as simple or straightforward as I initially hoped. It
    took a lot of experimentation, and a lot of ideas I thought were quite clever
    fell essentially flat. But in the end, I was able to create an emoji search engine
    that works fairly well.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºè¿™ä¸ªè¿‡ç¨‹å¹¶ä¸åƒæˆ‘æœ€åˆå¸Œæœ›çš„é‚£æ ·ç®€å•æˆ–ç›´æ¥ã€‚è¿™èŠ±äº†å¾ˆå¤šå®éªŒï¼Œå¾ˆå¤šæˆ‘è®¤ä¸ºç›¸å½“èªæ˜çš„æƒ³æ³•æœ€ç»ˆéƒ½å¤±è´¥äº†ã€‚ä½†æœ€ç»ˆï¼Œæˆ‘æˆåŠŸåœ°åˆ›å»ºäº†ä¸€ä¸ªç›¸å½“å¥½ç”¨çš„è¡¨æƒ…ç¬¦å·æœç´¢å¼•æ“ã€‚
- en: Hereâ€™s how I built it, what worked, and what didnâ€™t, and the lessons learned
    along the way.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æ˜¯æˆ‘å¦‚ä½•æ„å»ºå®ƒã€å“ªäº›æ–¹é¢æœ‰æ•ˆã€å“ªäº›æ–¹é¢æ— æ•ˆï¼Œä»¥åŠåœ¨è¿‡ç¨‹ä¸­å­¦åˆ°çš„ç»éªŒæ•™è®­ã€‚
- en: '[What is an Emoji](#ae15)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯è¡¨æƒ…ç¬¦å·](#ae15)'
- en: '[The Data](#a881)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ•°æ®](#a881)'
- en: '[Emojis versus Images and Text](#2396)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è¡¨æƒ…ç¬¦å·ä¸å›¾ç‰‡å’Œæ–‡æœ¬](#2396)'
- en: '[Bridging the Modality Gap](#89d6)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å¼¥åˆæ¨¡æ€å·®è·](#89d6)'
- en: '[Using the Emoji Search Engine](#733f)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ä½¿ç”¨è¡¨æƒ…ç¬¦å·æœç´¢å¼•æ“](#733f)'
- en: What is an Emoji
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯è¡¨æƒ…ç¬¦å·
- en: Before building a semantic search engine for emojis, itâ€™s worth briefly explaining
    what exactly an emoji is. The term *emoji* derives from the Japanese kanji çµµ (eh)
    meaning *picture*, and æ–‡å­— (mÅji) meaning letter or character. Essentially, this
    means that an emoji is etymologically a pictogram, and while it is connected to
    the English word *emotion,* it is not an â€œemotion iconâ€ â€” that is an [emoticon](https://en.wikipedia.org/wiki/Emoticon).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ„å»ºä¸€ä¸ªè¡¨æƒ…ç¬¦å·è¯­ä¹‰æœç´¢å¼•æ“ä¹‹å‰ï¼Œå€¼å¾—ç®€è¦è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯è¡¨æƒ…ç¬¦å·ã€‚*emoji* è¿™ä¸ªè¯æºè‡ªæ—¥è¯­æ±‰å­—çµµï¼ˆehï¼‰ï¼Œæ„æ€æ˜¯*å›¾ç‰‡*ï¼Œä»¥åŠæ–‡å­—ï¼ˆmÅjiï¼‰ï¼Œæ„æ€æ˜¯å­—æ¯æˆ–å­—ç¬¦ã€‚ä»è¯æºå­¦çš„è§’åº¦æ¥çœ‹ï¼Œemojiæœ¬è´¨ä¸Šæ˜¯ä¸€ç§è±¡å½¢æ–‡å­—ï¼Œè™½ç„¶å®ƒä¸è‹±è¯­å•è¯*emotion*ï¼ˆæƒ…æ„Ÿï¼‰æœ‰å…³ï¼Œä½†å®ƒå¹¶ä¸æ˜¯â€œæƒ…æ„Ÿå›¾æ ‡â€â€”â€”è¿™æ‰æ˜¯[è¡¨æƒ…ç¬¦å·](https://en.wikipedia.org/wiki/Emoticon)ã€‚
- en: Along with [alphanumeric characters](https://en.wikipedia.org/wiki/List_of_Unicode_characters#Latin_script),
    [African click sounds](https://en.wikipedia.org/wiki/Latin_Extended-B#African_letters_for_clicks),
    [mathematical](https://en.wikipedia.org/wiki/List_of_Unicode_characters#Mathematical_symbols)
    and [geometric symbols](https://en.wikipedia.org/wiki/List_of_Unicode_characters#Geometric_Shapes),
    [dingbats](https://en.wikipedia.org/wiki/List_of_Unicode_characters#Dingbats),
    and [computer control sequences](https://en.wikipedia.org/wiki/List_of_Unicode_characters#Control_codes),
    emojis can be represented as Unicode characters, making them computer-readable.
    Unlike alphanumeric characters and other symbols, however, emojis are *maintained*
    by the [Unicode Consortium](https://home.unicode.org/)*.* The consortium solicits
    proposals for new emojis, and regularly selects which emojis will be added to
    the standard.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸[å­—æ¯æ•°å­—å­—ç¬¦](https://en.wikipedia.org/wiki/List_of_Unicode_characters#Latin_script)ã€[éæ´²ç‚¹å‡»éŸ³](https://en.wikipedia.org/wiki/Latin_Extended-B#African_letters_for_clicks)ã€[æ•°å­¦ç¬¦å·](https://en.wikipedia.org/wiki/List_of_Unicode_characters#Mathematical_symbols)å’Œ[å‡ ä½•ç¬¦å·](https://en.wikipedia.org/wiki/List_of_Unicode_characters#Geometric_Shapes)ã€[è£…é¥°ç¬¦å·](https://en.wikipedia.org/wiki/List_of_Unicode_characters#Dingbats)ä»¥åŠ[è®¡ç®—æœºæ§åˆ¶åºåˆ—](https://en.wikipedia.org/wiki/List_of_Unicode_characters#Control_codes)ä¸€èµ·ï¼Œè¡¨æƒ…ç¬¦å·å¯ä»¥ä½œä¸ºUnicodeå­—ç¬¦è¡¨ç¤ºï¼Œä»è€Œä½¿å®ƒä»¬å¯ä»¥è¢«è®¡ç®—æœºè¯»å–ã€‚ç„¶è€Œï¼Œä¸å­—æ¯æ•°å­—å­—ç¬¦å’Œå…¶ä»–ç¬¦å·ä¸åŒï¼Œè¡¨æƒ…ç¬¦å·ç”±[Unicodeè”ç›Ÿ](https://home.unicode.org/)è¿›è¡Œ*ç»´æŠ¤*ã€‚è¯¥è”ç›Ÿä¼šå¾é›†æ–°è¡¨æƒ…ç¬¦å·çš„ææ¡ˆï¼Œå¹¶å®šæœŸé€‰æ‹©å“ªäº›è¡¨æƒ…ç¬¦å·å°†è¢«åŠ å…¥åˆ°æ ‡å‡†ä¸­ã€‚
- en: At the time of writing, in November 2023, there are [more than 3,600 recognized
    emojis](https://home.unicode.org/emoji/about-emoji/), symbolizing a wide range
    of ideas and sentiments. Some emojis are represented by a single unicode character,
    or *code-point*. For example, the â€œgrinning faceâ€ emoji, ğŸ˜€, is represented in
    unicode as U+1F600.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æˆªè‡³å†™ä½œæ—¶ï¼Œ2023å¹´11æœˆï¼Œå·²ç»æœ‰[è¶…è¿‡3600ä¸ªè¢«è®¤å¯çš„è¡¨æƒ…ç¬¦å·](https://home.unicode.org/emoji/about-emoji/)ï¼Œå®ƒä»¬è±¡å¾ç€å„ç§å„æ ·çš„æ€æƒ³å’Œæƒ…æ„Ÿã€‚æœ‰äº›è¡¨æƒ…ç¬¦å·ç”±å•ä¸€çš„Unicodeå­—ç¬¦æˆ–*ç¼–ç ç‚¹*è¡¨ç¤ºã€‚ä¾‹å¦‚ï¼Œâ€œéœ²é½¿è€Œç¬‘çš„è„¸â€è¡¨æƒ…ç¬¦å·ï¼ŒğŸ˜€ï¼Œåœ¨Unicodeä¸­è¡¨ç¤ºä¸ºU+1F600ã€‚
- en: 'Others are represented with sequences of code-points. These sequences, which
    combine single code-point emojis with the zero-width-joiner unicode character,
    are known as ZWJ sequences, and allow for the combining of concepts, in much the
    same way as [Chinese radicals](https://en.wikipedia.org/wiki/Kangxi_radical) can
    be combined to create a character that tells a story. As an example, the emoji
    ğŸ‘¨â€ğŸ‘©â€ğŸ‘§is a zero-width joining of the emojis for *man* ğŸ‘¨(U+1F468), *woman* ğŸ‘©(â€‹â€‹U+1F469),
    and *girl* ğŸ‘§(U+1F467), connected by the ZWJ code-point U+200D:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä»–çš„åˆ™é€šè¿‡ä»£ç ç‚¹åºåˆ—è¡¨ç¤ºã€‚è¿™äº›åºåˆ—å°†å•ä¸ªä»£ç ç‚¹è¡¨æƒ…ç¬¦å·ä¸é›¶å®½åº¦è¿æ¥ç¬¦unicodeå­—ç¬¦ç»“åˆï¼Œç§°ä¸ºZWJåºåˆ—ï¼Œå…è®¸å°†æ¦‚å¿µç»„åˆåœ¨ä¸€èµ·ï¼Œå°±åƒ[æ±‰å­—éƒ¨é¦–](https://en.wikipedia.org/wiki/Kangxi_radical)å¯ä»¥ç»„åˆæˆä¸€ä¸ªè®²è¿°æ•…äº‹çš„å­—ç¬¦ä¸€æ ·ã€‚ä¾‹å¦‚ï¼Œè¡¨æƒ…ç¬¦å·ğŸ‘¨â€ğŸ‘©â€ğŸ‘§å°±æ˜¯é€šè¿‡ZWJä»£ç ç‚¹U+200Dè¿æ¥çš„*ç”·äºº*ğŸ‘¨ï¼ˆU+1F468ï¼‰ã€*å¥³äºº*ğŸ‘©ï¼ˆU+1F469ï¼‰å’Œ*å¥³å­©*ğŸ‘§ï¼ˆU+1F467ï¼‰è¡¨æƒ…ç¬¦å·ï¼š
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'According to the Unicode Consortium, 92% of the worldâ€™s online population uses
    emojis in their communications, and the ten most-used emojis in 2021 were: ğŸ˜‚ â¤ï¸
    ğŸ¤£ ğŸ‘ ğŸ˜­ ğŸ™ ğŸ˜˜ ğŸ¥° ğŸ˜ ğŸ˜Š.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®Unicodeè”ç›Ÿçš„ç»Ÿè®¡ï¼Œå…¨çƒ92%çš„åœ¨çº¿ç”¨æˆ·åœ¨é€šä¿¡ä¸­ä½¿ç”¨è¡¨æƒ…ç¬¦å·ï¼Œ2021å¹´ä½¿ç”¨é¢‘ç‡æœ€é«˜çš„åä¸ªè¡¨æƒ…ç¬¦å·æ˜¯ï¼šğŸ˜‚ â¤ï¸ ğŸ¤£ ğŸ‘ ğŸ˜­ ğŸ™ ğŸ˜˜ ğŸ¥° ğŸ˜ ğŸ˜Šã€‚
- en: Starting with the Data
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»æ•°æ®å¼€å§‹
- en: Given that emojis are pictographs of sorts, I wanted to utilize both textual
    and visual information in the search process. My initial hypothesis was that for
    many emojis, the name â€” the text string used to invoke the emoji â€” conveys but
    a fraction of its meaning. This can be due to many reasons, from the limitations
    of natural language, to the additional meanings imbued by cultures and visual
    similarities. In order to truly bring the full essence of the emoji to bear, I
    needed to make use of visual information.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºè¡¨æƒ…ç¬¦å·åœ¨æŸç§ç¨‹åº¦ä¸Šæ˜¯è±¡å½¢æ–‡å­—ï¼Œæˆ‘å¸Œæœ›åœ¨æœç´¢è¿‡ç¨‹ä¸­åŒæ—¶åˆ©ç”¨æ–‡æœ¬ä¿¡æ¯å’Œè§†è§‰ä¿¡æ¯ã€‚æˆ‘çš„åˆæ­¥å‡è®¾æ˜¯ï¼Œå¯¹äºè®¸å¤šè¡¨æƒ…ç¬¦å·æ¥è¯´ï¼Œåç§°â€”â€”ç”¨äºè°ƒå‡ºè¡¨æƒ…ç¬¦å·çš„æ–‡æœ¬å­—ç¬¦ä¸²â€”â€”åªä¼ è¾¾äº†å…¶å«ä¹‰çš„ä¸€éƒ¨åˆ†ã€‚è¿™å¯èƒ½æ˜¯ç”±äºå¤šç§åŸå› ï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€çš„å±€é™æ€§ï¼Œä»¥åŠæ–‡åŒ–å’Œè§†è§‰ç›¸ä¼¼æ€§æ‰€èµ‹äºˆçš„é™„åŠ å«ä¹‰ã€‚ä¸ºäº†çœŸæ­£å±•ç°è¡¨æƒ…ç¬¦å·çš„å®Œæ•´æœ¬è´¨ï¼Œæˆ‘éœ€è¦åˆ©ç”¨è§†è§‰ä¿¡æ¯ã€‚
- en: 'I found this [Kaggle Emojis dataset](https://www.kaggle.com/datasets/subinium/emojiimage-dataset)
    from 2021, which has data about 1816 emojis, including the emoji representation,
    the text associated with it, the unicode code (or codes), and a [base64](https://en.wikipedia.org/wiki/Base64)
    encoded image. Hereâ€™s what the first few rows of the dataset look like, loaded
    as a pandas DataFrame:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ‰¾åˆ°äº†ä¸€ä»½2021å¹´çš„[Kaggle Emojisæ•°æ®é›†](https://www.kaggle.com/datasets/subinium/emojiimage-dataset)ï¼Œå…¶ä¸­åŒ…å«1816ä¸ªè¡¨æƒ…ç¬¦å·çš„æ•°æ®ï¼ŒåŒ…æ‹¬è¡¨æƒ…ç¬¦å·çš„è¡¨ç¤ºã€ä¸ä¹‹ç›¸å…³çš„æ–‡æœ¬ã€unicodeç¼–ç ï¼ˆæˆ–ç¼–ç ï¼‰å’Œä¸€ä¸ª[base64](https://en.wikipedia.org/wiki/Base64)ç¼–ç çš„å›¾åƒã€‚ä»¥ä¸‹æ˜¯è¯¥æ•°æ®é›†çš„å‰å‡ è¡Œï¼ŒåŠ è½½ä¸ºpandas
    DataFrameåå‘ˆç°çš„æ ·å­ï¼š
- en: '![](../Images/ac2785f516f75b2549052b5388d10910.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac2785f516f75b2549052b5388d10910.png)'
- en: 'There are separate columns with names `Apple`, `Google`, `Facebook`, etc. because
    the emoji renders differently depending on the computer, website, or application.
    I decoded the images from base64 and converted them into [Pillow](https://pypi.org/project/Pillow/)
    images. Here is the first image from the Kaggle dataset (grinning face):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºè¡¨æƒ…ç¬¦å·çš„æ¸²æŸ“æ–¹å¼å–å†³äºè®¡ç®—æœºã€ç½‘ç«™æˆ–åº”ç”¨ç¨‹åºï¼Œå› æ­¤å­˜åœ¨åä¸º`Apple`ã€`Google`ã€`Facebook`ç­‰çš„å•ç‹¬åˆ—ã€‚æˆ‘ä»base64è§£ç è¿™äº›å›¾åƒå¹¶å°†å…¶è½¬æ¢ä¸º[Pillow](https://pypi.org/project/Pillow/)å›¾åƒã€‚ä»¥ä¸‹æ˜¯Kaggleæ•°æ®é›†ä¸­çš„ç¬¬ä¸€å¼ å›¾åƒï¼ˆéœ²é½¿ç¬‘è„¸ï¼‰ï¼š
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/d2948f4c8927c66d864a400e50897b2f.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2948f4c8927c66d864a400e50897b2f.png)'
- en: Upon conversion, however, it became clear that the images were very low resolution.
    This one, for instance, is only 72x72 pixels. To improve the quality of the images
    that I was going to pass into downstream models, and to improve the quality of
    the experience in the eventual UI-based application, I passed all of these low-resolution
    images into [Real-ESRGAN](https://replicate.com/nightmareai/real-esrgan) to 10x
    the resolution.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œåœ¨è½¬æ¢åå¾ˆæ˜æ˜¾ï¼Œè¿™äº›å›¾åƒçš„åˆ†è¾¨ç‡éå¸¸ä½ã€‚ä¾‹å¦‚ï¼Œè¿™å¼ å›¾åƒä»…ä¸º72x72åƒç´ ã€‚ä¸ºäº†æé«˜å°†è¦ä¼ é€’ç»™ä¸‹æ¸¸æ¨¡å‹çš„å›¾åƒè´¨é‡ï¼Œå¹¶ä¸”ä¸ºäº†æ”¹å–„æœ€ç»ˆåŸºäºUIçš„åº”ç”¨ç¨‹åºä¸­çš„ä½“éªŒï¼Œæˆ‘å°†æ‰€æœ‰è¿™äº›ä½åˆ†è¾¨ç‡å›¾åƒä¼ é€’åˆ°[Real-ESRGAN](https://replicate.com/nightmareai/real-esrgan)ï¼Œå°†åˆ†è¾¨ç‡æé«˜äº†10å€ã€‚
- en: 'This is what the resulting images looked like:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœå›¾åƒçœ‹èµ·æ¥æ˜¯è¿™æ ·çš„ï¼š
- en: '![](../Images/4d6b93d5b04f05729c9f7dc4e48f6164.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d6b93d5b04f05729c9f7dc4e48f6164.png)'
- en: Not all of the emojis had images for all of the image columns in the pandas
    DataFrame, so I used the first viable base64 encoding for each row.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå¹¶éæ‰€æœ‰è¡¨æƒ…ç¬¦å·åœ¨pandas DataFrameä¸­çš„æ‰€æœ‰å›¾åƒåˆ—éƒ½æœ‰å›¾åƒï¼Œå› æ­¤æˆ‘ä¸ºæ¯ä¸€è¡Œä½¿ç”¨äº†ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„base64ç¼–ç ã€‚
- en: Emojis Versus Images and Text
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¡¨æƒ…ç¬¦å·ä¸å›¾åƒå’Œæ–‡æœ¬
- en: 'Before diving any deeper, I want to emphasize one crucial element of emojis
    that makes them so special, and deserving of their own semantic search engine:
    in a sense, they are *both* images and text. From the human perspective, we can
    represent each emoji as a unicode character, on the same playing field as text
    characters, and we can represent it as a standalone image, both of which we saw
    in the previous section. Said another way, if we squint with one eye, we can see
    a pictogram as a picture, and if we squint with the other eye, we can see the
    same pictogram as text.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ·±å…¥æ¢è®¨ä¹‹å‰ï¼Œæˆ‘æƒ³å¼ºè°ƒè¡¨æƒ…ç¬¦å·çš„ä¸€ä¸ªå…³é”®å…ƒç´ ï¼Œå®ƒä½¿å¾—è¡¨æƒ…ç¬¦å·å¦‚æ­¤ç‰¹åˆ«ï¼Œå¹¶ä¸”å€¼å¾—æ‹¥æœ‰è‡ªå·±çš„è¯­ä¹‰æœç´¢å¼•æ“ï¼šä»æŸç§æ„ä¹‰ä¸Šè¯´ï¼Œå®ƒä»¬*æ—¢*æ˜¯å›¾åƒä¹Ÿæ˜¯æ–‡æœ¬ã€‚ä»äººç±»çš„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ¯ä¸ªè¡¨æƒ…ç¬¦å·è¡¨ç¤ºä¸ºä¸€ä¸ª
    Unicode å­—ç¬¦ï¼Œä¸æ–‡æœ¬å­—ç¬¦å¤„äºåŒä¸€å±‚é¢ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥å°†å…¶è¡¨ç¤ºä¸ºä¸€ä¸ªç‹¬ç«‹çš„å›¾åƒï¼Œè¿™ä¸¤è€…åœ¨å‰ä¸€èŠ‚ä¸­éƒ½å·²ç»çœ‹åˆ°ã€‚æ¢å¥è¯è¯´ï¼Œå¦‚æœæˆ‘ä»¬ç”¨ä¸€åªçœ¼ç›çœ¯èµ·ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠè±¡å½¢ç¬¦å·çœ‹ä½œæ˜¯ä¸€å¼ å›¾ç‰‡ï¼Œè€Œå¦‚æœæˆ‘ä»¬ç”¨å¦ä¸€åªçœ¼ç›çœ¯èµ·ï¼Œæˆ‘ä»¬å¯ä»¥å°†åŒä¸€ä¸ªè±¡å½¢ç¬¦å·çœ‹ä½œæ˜¯æ–‡æœ¬ã€‚
- en: Computers, however, are not known for their ability to squint. While a computer
    may be able to display a unicode code-point as an emoji, a machine learning model
    may not have a good way of interpreting the emoji as text or images.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œè®¡ç®—æœºå¹¶ä¸ä»¥å…¶çœ¯çœ¼èƒ½åŠ›è€Œé—»åã€‚å°½ç®¡è®¡ç®—æœºå¯ä»¥å°†ä¸€ä¸ª Unicode ç ç‚¹æ˜¾ç¤ºä¸ºè¡¨æƒ…ç¬¦å·ï¼Œä½†æœºå™¨å­¦ä¹ æ¨¡å‹å¯èƒ½æ²¡æœ‰å¾ˆå¥½çš„æ–¹æ³•å°†è¯¥è¡¨æƒ…ç¬¦å·è§£é‡Šä¸ºæ–‡æœ¬æˆ–å›¾åƒã€‚
- en: Whenever Iâ€™m working on semantic search applications that connect images and
    text, I start with a family of models known as [contrastive language image pre-training](https://github.com/openai/CLIP)
    (CLIP). These models are trained on image-text pairs to generate similar vector
    representations or [*embeddings*](/neural-network-embeddings-explained-4d028e6f0526)
    for images and their captions, and dissimilar vectors when images are paired with
    other text strings. There are multiple CLIP-style models, including [OpenCLIP](https://github.com/mlfoundations/open_clip)
    and [MetaCLIP](https://github.com/facebookresearch/metaclip), but for simplicity
    weâ€™ll focus on the original CLIP model from OpenAI. No model is perfect, and at
    a fundamental level there is no *right* way to compare images and text, but CLIP
    certainly provides a good starting point.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯å½“æˆ‘åœ¨è¿›è¡Œè¿æ¥å›¾åƒå’Œæ–‡æœ¬çš„è¯­ä¹‰æœç´¢åº”ç”¨æ—¶ï¼Œæˆ‘éƒ½ä¼šä»ä¸€ä¸ªè¢«ç§°ä¸º [å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒ](https://github.com/openai/CLIP)ï¼ˆCLIPï¼‰çš„ä¸€ç»„æ¨¡å‹å¼€å§‹ã€‚è¿™äº›æ¨¡å‹é€šè¿‡å¯¹å›¾åƒ-æ–‡æœ¬å¯¹è¿›è¡Œè®­ç»ƒï¼Œç”Ÿæˆå›¾åƒåŠå…¶æ ‡é¢˜çš„ç›¸ä¼¼å‘é‡è¡¨ç¤ºæˆ–
    [*åµŒå…¥*](/neural-network-embeddings-explained-4d028e6f0526)ï¼Œå½“å›¾åƒä¸å…¶ä»–æ–‡æœ¬å­—ç¬¦ä¸²é…å¯¹æ—¶ï¼Œç”Ÿæˆä¸ç›¸ä¼¼çš„å‘é‡ã€‚æœ‰å¤šç§
    CLIP é£æ ¼çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬ [OpenCLIP](https://github.com/mlfoundations/open_clip) å’Œ [MetaCLIP](https://github.com/facebookresearch/metaclip)ï¼Œä½†ä¸ºäº†ç®€ä¾¿èµ·è§ï¼Œæˆ‘ä»¬å°†é‡ç‚¹å…³æ³¨
    OpenAI çš„åŸå§‹ CLIP æ¨¡å‹ã€‚æ²¡æœ‰ä»»ä½•æ¨¡å‹æ˜¯å®Œç¾çš„ï¼Œåœ¨æ ¹æœ¬å±‚é¢ä¸Šï¼Œä¹Ÿæ²¡æœ‰*æ­£ç¡®*çš„æ–¹å¼æ¥æ¯”è¾ƒå›¾åƒå’Œæ–‡æœ¬ï¼Œä½† CLIP æ— ç–‘æä¾›äº†ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ã€‚
- en: Interpreting Emojis as Text
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å°†è¡¨æƒ…ç¬¦å·è§£é‡Šä¸ºæ–‡æœ¬
- en: At a high level, language models process input text by converting it into an
    ordered sequence of *tokens*, and then *encoding* the tokens and positional information
    in a dense numerical vector. Each language model has its own *vocabulary* of tokens
    to decompose a text string into, spanning from individual letters to complete
    words. Some tokens are easily interpretable by a human, while others are not,
    and in the case of CLIP, the vocabulary has 49,408 entries.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é«˜å±‚æ¬¡ä¸Šï¼Œè¯­è¨€æ¨¡å‹é€šè¿‡å°†è¾“å…¥æ–‡æœ¬è½¬æ¢ä¸ºä¸€ä¸ªæœ‰åºçš„*æ ‡è®°*åºåˆ—ï¼Œç„¶å*ç¼–ç *è¿™äº›æ ‡è®°å’Œä½ç½®ä¿¡æ¯ä¸ºä¸€ä¸ªå¯†é›†çš„æ•°å€¼å‘é‡ã€‚æ¯ä¸ªè¯­è¨€æ¨¡å‹éƒ½æœ‰è‡ªå·±çš„*è¯æ±‡è¡¨*ï¼Œç”¨äºå°†æ–‡æœ¬å­—ç¬¦ä¸²åˆ†è§£ä¸ºæ ‡è®°ï¼Œæ¶µç›–äº†ä»å•ä¸ªå­—æ¯åˆ°å®Œæ•´å•è¯çš„èŒƒå›´ã€‚ä¸€äº›æ ‡è®°å¯¹äºäººç±»æ¥è¯´å¾ˆå®¹æ˜“ç†è§£ï¼Œè€Œå¦ä¸€äº›åˆ™ä¸å®¹æ˜“ç†è§£ï¼Œåœ¨
    CLIP çš„æƒ…å†µä¸‹ï¼Œè¯æ±‡è¡¨åŒ…å«äº† 49,408 ä¸ªæ¡ç›®ã€‚
- en: 'Letâ€™s see an explicit example. Assuming the CLIP library is installed, we can
    *tokenize* a text string â€œa dogâ€ with:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªæ˜ç¡®çš„ä¾‹å­ã€‚å‡è®¾å·²å®‰è£… CLIP åº“ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹å¼å¯¹æ–‡æœ¬å­—ç¬¦ä¸²â€œa dogâ€è¿›è¡Œ*åˆ†è¯*å¤„ç†ï¼š
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output tensor contains four nonzero entries: 49406, 320, 1929, and 49407\.
    To make sense of these, we can map these values back to keys in the [CLIP vocabulary
    dictionary](https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/vocab.json).
    The first number, 49406, corresponds to the key â€œ<|startoftext|>â€, and the last
    number, 49407 corresponds to the key â€œ<|endoftext|>â€. These are special tokens
    denoting the beginning and end of the text string to be encoded. The second number,
    320, maps back to â€œa</w>â€, which signifies the character â€œaâ€ followed by a new
    word. Finally, 1929 is the value for key â€œdog</w>â€.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºå¼ é‡åŒ…å«å››ä¸ªéé›¶æ¡ç›®ï¼š49406ã€320ã€1929 å’Œ 49407ã€‚ä¸ºäº†ç†è§£è¿™äº›æ¡ç›®çš„å«ä¹‰ï¼Œæˆ‘ä»¬å¯ä»¥å°†è¿™äº›å€¼æ˜ å°„å› [CLIP è¯æ±‡å­—å…¸](https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/vocab.json)ä¸­çš„é”®ã€‚ç¬¬ä¸€ä¸ªæ•°å­—
    49406 å¯¹åº”äºé”®â€œ<|startoftext|>â€ï¼Œæœ€åä¸€ä¸ªæ•°å­— 49407 å¯¹åº”äºé”®â€œ<|endoftext|>â€ã€‚è¿™äº›æ˜¯è¡¨ç¤ºè¦ç¼–ç çš„æ–‡æœ¬å­—ç¬¦ä¸²å¼€å§‹å’Œç»“æŸçš„ç‰¹æ®Šæ ‡è®°ã€‚ç¬¬äºŒä¸ªæ•°å­—
    320 æ˜ å°„å›â€œa</w>â€ï¼Œè¡¨ç¤ºå­—ç¬¦â€œaâ€åè·Ÿä¸€ä¸ªæ–°å•è¯ã€‚æœ€åï¼Œ1929 æ˜¯é”®â€œdog</w>â€çš„å€¼ã€‚
- en: 'If we try to tokenize a string containing an emoji, however, we quickly run
    into a hitch: emojis donâ€™t get tokenized in the same way as other characters do.
    Letâ€™s start with the dog emoji ğŸ¶:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå¦‚æœæˆ‘ä»¬å°è¯•æ ‡è®°åŒ–åŒ…å«è¡¨æƒ…ç¬¦å·çš„å­—ç¬¦ä¸²ï¼Œæˆ‘ä»¬ä¼šå¾ˆå¿«é‡åˆ°é—®é¢˜ï¼šè¡¨æƒ…ç¬¦å·ä¸åƒå…¶ä»–å­—ç¬¦é‚£æ ·è¢«æ ‡è®°åŒ–ã€‚è®©æˆ‘ä»¬ä»ç‹—è¡¨æƒ…ç¬¦å·ğŸ¶å¼€å§‹ï¼š
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Doing a reverse lookup for the key associated with 10,631, we get the token
    â€œÃ°ÅÄ²Â¶</w>â€. But if we pass this string into the tokenizer, we get a completely
    different set of token IDs:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹ä¸10,631ç›¸å…³çš„é”®è¿›è¡Œåå‘æŸ¥æ‰¾æ—¶ï¼Œæˆ‘ä»¬å¾—åˆ°æ ‡è®°â€œÃ°ÅÄ²Â¶</w>â€ã€‚ä½†æ˜¯å¦‚æœæˆ‘ä»¬å°†è¿™ä¸ªå­—ç¬¦ä¸²ä¼ å…¥åˆ†è¯å™¨ï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°ä¸€ç»„å®Œå…¨ä¸åŒçš„æ ‡è®°IDï¼š
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'An even more curious case concerns the flag emojis. If we take the emoji for
    the flag of Cameroon, for instance, we get:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ›´ä¸ºæœ‰è¶£çš„æ¡ˆä¾‹æ˜¯å…³äºå›½æ——è¡¨æƒ…ç¬¦å·çš„ã€‚å¦‚æœæˆ‘ä»¬ä»¥å–€éº¦éš†å›½æ——çš„è¡¨æƒ…ç¬¦å·ä¸ºä¾‹ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The two non-start/end tokens here correspond to â€œÃ°ÅÄ©Â¨Ã°ÅÄ©â€ and â€œÂ²</w>â€. If we
    plug the first of these back into the tokenizer, we get another completely different
    set of token IDs, but the second maps back to itself.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„ä¸¤ä¸ªéå¼€å§‹/ç»“æŸæ ‡è®°å¯¹åº”äºâ€œÃ°ÅÄ©Â¨Ã°ÅÄ©â€å’Œâ€œÂ²</w>â€ã€‚å¦‚æœæˆ‘ä»¬å°†ç¬¬ä¸€ä¸ªæ ‡è®°é‡æ–°è¾“å…¥åˆ†è¯å™¨ï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°ä¸€ç»„å®Œå…¨ä¸åŒçš„æ ‡è®°IDï¼Œä½†ç¬¬äºŒä¸ªæ ‡è®°ä¼šæ˜ å°„å›å…¶è‡ªèº«ã€‚
- en: Things get even more precarious when we start comparing embeddings of text strings
    with embeddings of emojis, parsed as text strings via this tokenizer. After all,
    we want to find the most relevant emojis given a *text query*. We can use the
    [cosine distance](https://medium.com/@milana.shxanukova15/cosine-distance-and-cosine-similarity-a5da0e4d9ded)
    as a way to measure how similar or different two vectors are â€” and by proxy the
    inputs that generated those embedding vectors are. A distance of 0 means that
    two vectors are completely aligned, and a distance of 1 implies that two vectors
    are orthogonal. If we wanted to treat emojis as text, we would want the name for
    an emoji to be relatively close to the tokenized emoji in the embedding space,
    but this is not always the case!
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬å¼€å§‹æ¯”è¾ƒæ–‡æœ¬å­—ç¬¦ä¸²çš„åµŒå…¥ä¸é€šè¿‡è¿™ä¸ªåˆ†è¯å™¨è§£æä¸ºæ–‡æœ¬å­—ç¬¦ä¸²çš„è¡¨æƒ…ç¬¦å·åµŒå…¥æ—¶ï¼Œæƒ…å†µå˜å¾—æ›´åŠ å¤æ‚ã€‚æ¯•ç«Ÿï¼Œæˆ‘ä»¬å¸Œæœ›æ ¹æ®*æ–‡æœ¬æŸ¥è¯¢*æ‰¾åˆ°æœ€ç›¸å…³çš„è¡¨æƒ…ç¬¦å·ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨[ä½™å¼¦è·ç¦»](https://medium.com/@milana.shxanukova15/cosine-distance-and-cosine-similarity-a5da0e4d9ded)æ¥è¡¡é‡ä¸¤ä¸ªå‘é‡çš„ç›¸ä¼¼æ€§æˆ–å·®å¼‚æ€§â€”â€”é—´æ¥åœ°ï¼Œè¡¡é‡ç”Ÿæˆè¿™äº›åµŒå…¥å‘é‡çš„è¾“å…¥ä¹‹é—´çš„å·®å¼‚ã€‚0çš„è·ç¦»æ„å‘³ç€ä¸¤ä¸ªå‘é‡å®Œå…¨å¯¹é½ï¼Œè€Œ1çš„è·ç¦»åˆ™æ„å‘³ç€ä¸¤ä¸ªå‘é‡æ­£äº¤ã€‚å¦‚æœæˆ‘ä»¬æƒ³æŠŠè¡¨æƒ…ç¬¦å·å½“ä½œæ–‡æœ¬å¤„ç†ï¼Œæˆ‘ä»¬å¸Œæœ›è¡¨æƒ…ç¬¦å·çš„åç§°ä¸åµŒå…¥ç©ºé—´ä¸­çš„æ ‡è®°åŒ–è¡¨æƒ…ç¬¦å·ç›¸å¯¹æ¥è¿‘ï¼Œä½†è¿™å¹¶ä¸æ€»æ˜¯å¦‚æ­¤ï¼
- en: 'The utility below will compare an emoji and a list of text prompts:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹å·¥å…·å°†æ¯”è¾ƒä¸€ä¸ªè¡¨æƒ…ç¬¦å·å’Œä¸€ç³»åˆ—æ–‡æœ¬æç¤ºï¼š
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Hereâ€™s an example, where according to CLIP, the encoding for the â€œbirthdayâ€
    emoji ğŸ‚is closer to â€œmanâ€ than â€œbirthdayâ€, closer to â€œdogâ€ than â€œbirthday presentâ€,
    and closer to â€œcarâ€ than â€œcandleâ€, â€œdateâ€, or â€œholidayâ€:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œæ ¹æ®CLIPçš„ç»“æœï¼Œ"ç”Ÿæ—¥"è¡¨æƒ…ç¬¦å·ğŸ‚çš„ç¼–ç æ›´æ¥è¿‘â€œç”·äººâ€è€Œéâ€œç”Ÿæ—¥â€ï¼Œæ›´æ¥è¿‘â€œç‹—â€è€Œéâ€œç”Ÿæ—¥ç¤¼ç‰©â€ï¼Œæ›´æ¥è¿‘â€œè½¦â€è€Œéâ€œèœ¡çƒ›â€ã€â€œçº¦ä¼šâ€æˆ–â€œå‡æœŸâ€ï¼š
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Sometimes, the emoji and its name (and similar concepts) are close together
    in the embedding space, but sometimes they are most certainly not.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰æ—¶ï¼Œè¡¨æƒ…ç¬¦å·åŠå…¶åç§°ï¼ˆä»¥åŠç±»ä¼¼çš„æ¦‚å¿µï¼‰åœ¨åµŒå…¥ç©ºé—´ä¸­å½¼æ­¤æ¥è¿‘ï¼Œä½†æœ‰æ—¶å®ƒä»¬æ˜¾ç„¶å¹¶éå¦‚æ­¤ã€‚
- en: 'We can also go the other way and retrieve the emojis whose embeddings most
    closely match the embedding of an input text prompt. For instance, for the input
    â€œloveâ€, we get the following:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¹Ÿå¯ä»¥åè¿‡æ¥ï¼Œæ£€ç´¢å‡ºä¸è¾“å…¥æ–‡æœ¬æç¤ºçš„åµŒå…¥æœ€åŒ¹é…çš„è¡¨æƒ…ç¬¦å·ã€‚ä¾‹å¦‚ï¼Œå¯¹äºè¾“å…¥â€œçˆ±â€ï¼Œæˆ‘ä»¬å¾—åˆ°ä»¥ä¸‹ç»“æœï¼š
- en: '![](../Images/9df5407cefbb3a9eb346f00af77ca861.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9df5407cefbb3a9eb346f00af77ca861.png)'
- en: Of course, we can do way better than this!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥åšå¾—æ¯”è¿™æ›´å¥½ï¼
- en: Interpreting Emojis as Images
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å°†è¡¨æƒ…ç¬¦å·è§£é‡Šä¸ºå›¾åƒ
- en: 'The high-resolution images of emojis that we generated using Real-ESRGAN provide
    an alternative pathway to searching through our emojis: treating emojis as *images*.
    We can use CLIPâ€™s vision encoder to embed the images into the same vector space,
    and then query these image embeddings with our input text prompt.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨Real-ESRGANç”Ÿæˆçš„é«˜åˆ†è¾¨ç‡è¡¨æƒ…ç¬¦å·å›¾åƒæä¾›äº†ä¸€æ¡æ›¿ä»£çš„è¡¨æƒ…ç¬¦å·æœç´¢è·¯å¾„ï¼šå°†è¡¨æƒ…ç¬¦å·è§†ä¸º*å›¾åƒ*ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨CLIPçš„è§†è§‰ç¼–ç å™¨å°†è¿™äº›å›¾åƒåµŒå…¥åˆ°ç›¸åŒçš„å‘é‡ç©ºé—´ä¸­ï¼Œç„¶åé€šè¿‡è¾“å…¥æ–‡æœ¬æç¤ºæŸ¥è¯¢è¿™äº›å›¾åƒåµŒå…¥ã€‚
- en: For applications like cross-modal retrieval (or semantically searching images
    with text), CLIP typically works best when the image embeddings are compared to
    a text prompt that is the userâ€™s query wrapped in the phrase â€œA photo of <query>â€.
    As an example, the image embedding for a photo of a dog will be closer (in terms
    of the angle between the vectors) to the embedding of â€œA photo of a dogâ€ than
    the embedding of the raw query â€œdogâ€.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè·¨æ¨¡æ€æ£€ç´¢ï¼ˆæˆ–é€šè¿‡æ–‡æœ¬æœç´¢å›¾åƒï¼‰ç­‰åº”ç”¨ï¼ŒCLIPé€šå¸¸åœ¨å°†å›¾åƒåµŒå…¥ä¸åŒ…å«ç”¨æˆ·æŸ¥è¯¢çš„æ–‡æœ¬æç¤ºâ€œä¸€ä¸ªå…³äº<query>çš„ç…§ç‰‡â€è¿›è¡Œæ¯”è¾ƒæ—¶è¡¨ç°æœ€ä½³ã€‚ä¾‹å¦‚ï¼Œä¸€å¼ ç‹—çš„ç…§ç‰‡çš„å›¾åƒåµŒå…¥ä¸â€œä¸€ä¸ªå…³äºç‹—çš„ç…§ç‰‡â€çš„åµŒå…¥ä¹‹é—´çš„è·ç¦»ä¼šæ¯”ä¸åŸå§‹æŸ¥è¯¢â€œç‹—â€çš„åµŒå…¥ä¹‹é—´çš„è·ç¦»æ›´è¿‘ï¼ˆä»å‘é‡ä¹‹é—´çš„è§’åº¦æ¥çœ‹ï¼‰ã€‚
- en: 'However, when I used this template, the results were underwhelming. For instance,
    here are the 25 top results for the query â€œA photo of a dogâ€:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå½“æˆ‘ä½¿ç”¨è¿™ä¸ªæ¨¡æ¿æ—¶ï¼Œç»“æœå¹¶ä¸ç†æƒ³ã€‚ä¾‹å¦‚ï¼Œä»¥ä¸‹æ˜¯æŸ¥è¯¢â€œç‹—çš„ç…§ç‰‡â€æ—¶çš„å‰25ä¸ªç»“æœï¼š
- en: '![](../Images/d281ba379d7de5eb874e930f9e631113.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d281ba379d7de5eb874e930f9e631113.png)'
- en: 'Because emojis arenâ€™t exactly *photos*, I decided to dig a little deeper into
    this and try out a few templating, or wrapping strategies. To cover my bases,
    I test five formats for text prompts:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºè¡¨æƒ…ç¬¦å·å¹¶ä¸å®Œå…¨æ˜¯*ç…§ç‰‡*ï¼Œæ‰€ä»¥æˆ‘å†³å®šæ·±å…¥æŒ–æ˜è¿™ä¸€ç‚¹ï¼Œå¹¶å°è¯•å‡ ç§æ¨¡æ¿æˆ–åŒ…è£…ç­–ç•¥ã€‚ä¸ºäº†å…¨é¢æµ‹è¯•ï¼Œæˆ‘æµ‹è¯•äº†äº”ç§æ–‡æœ¬æç¤ºæ ¼å¼ï¼š
- en: <emoji_name>
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <emoji_name>
- en: A photo of a <emoji_name>
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸€å¼ <emoji_name>è¡¨æƒ…ç¬¦å·çš„ç…§ç‰‡
- en: An emoji of <emoji_name>
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª<emoji_name>çš„è¡¨æƒ…ç¬¦å·
- en: A photo of a <emoji_name> emoji
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸€å¼ <emoji_name>è¡¨æƒ…ç¬¦å·çš„ç…§ç‰‡
- en: A <emoji_name> emoji
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª<emoji_name>è¡¨æƒ…ç¬¦å·
- en: I generated embeddings for all 1816 emojis with each of these methods, and computed
    the CLIPScore (cosine similarity multiplied by 100) of these vectors with the
    corresponding image embedding vectors.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ç”¨è¿™äº›æ–¹æ³•ä¸ºæ‰€æœ‰1816ä¸ªè¡¨æƒ…ç¬¦å·ç”Ÿæˆäº†åµŒå…¥ï¼Œå¹¶è®¡ç®—äº†è¿™äº›å‘é‡ä¸ç›¸åº”å›¾åƒåµŒå…¥å‘é‡çš„CLIPScoreï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ä¹˜ä»¥100ï¼‰ã€‚
- en: 'Here were the aggregate results:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ç»¼åˆç»“æœï¼š
- en: '[PRE10]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: From these statistics, I thought that the â€œAn emoji ofâ€ descriptors were the
    best fit of the bunch, as they had the highest mean and max. But when I tried
    to use this, the results were again less than ideal. They seemed to preference
    faces (e.g. ğŸ˜„ğŸ˜¢ğŸ™ƒğŸ‘¦ğŸ‘§), to the detriment of other emojis like symbols, animals, and
    flags. When it came to semantic emoji searches, I found that entering the raw
    text tended to work best. In other words, the CLIP embedding of â€œdogâ€ worked better
    than â€œA photo of a dogâ€, or â€œAn emoji of a dogâ€.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®è¿™äº›ç»Ÿè®¡æ•°æ®ï¼Œæˆ‘è®¤ä¸ºâ€œAn emoji ofâ€æè¿°æœ€é€‚åˆï¼Œå› ä¸ºå®ƒä»¬å…·æœ‰æœ€é«˜çš„å‡å€¼å’Œæœ€å¤§å€¼ã€‚ä½†å½“æˆ‘å°è¯•ä½¿ç”¨è¿™ä¸ªæ—¶ï¼Œç»“æœä¾ç„¶ä¸ç†æƒ³ã€‚å®ƒä»¬ä¼¼ä¹åå¥½é¢éƒ¨è¡¨æƒ…ï¼ˆä¾‹å¦‚ğŸ˜„ğŸ˜¢ğŸ™ƒğŸ‘¦ğŸ‘§ï¼‰ï¼Œå¿½è§†äº†å…¶ä»–è¡¨æƒ…ç¬¦å·ï¼Œå¦‚ç¬¦å·ã€åŠ¨ç‰©å’Œæ——å¸œã€‚åœ¨è¯­ä¹‰è¡¨æƒ…ç¬¦å·æœç´¢æ—¶ï¼Œæˆ‘å‘ç°ç›´æ¥è¾“å…¥åŸå§‹æ–‡æœ¬é€šå¸¸æ•ˆæœæœ€å¥½ã€‚æ¢å¥è¯è¯´ï¼Œâ€œdogâ€è¿™ä¸ªè¯çš„CLIPåµŒå…¥æ•ˆæœæ¯”â€œA
    photo of a dogâ€æˆ–â€œAn emoji of a dogâ€è¦å¥½ã€‚
- en: 'There were a few takeaways from this:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰å‡ ç‚¹æ”¶è·ï¼š
- en: Overall image-text â€œalignmentâ€ isnâ€™t necessarily important for semantic search
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å›¾åƒ-æ–‡æœ¬çš„â€œå¯¹é½â€æ•´ä½“ä¸Šå¯¹è¯­ä¹‰æœç´¢ä¸ä¸€å®šé‡è¦
- en: The images of the emojis encode (to some degree) the fact that they are not
    photos
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¡¨æƒ…ç¬¦å·çš„å›¾åƒåœ¨æŸç§ç¨‹åº¦ä¸Šç¼–ç äº†å®ƒä»¬ä¸æ˜¯ç…§ç‰‡çš„äº‹å®
- en: The word â€œemojiâ€ biases CLIP toward faces
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€œemojiâ€è¿™ä¸ªè¯ä½¿CLIPåå‘é¢éƒ¨
- en: Bridging the Modality Gap
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¼¥åˆæ¨¡æ€å·®è·
- en: By this point, I had come to the conclusion that treating emojis as just images
    or just text leaves a lot of rich information on the table. To build a robust
    semantic emoji search engine, I wanted to incorporate both textual and image information,
    and bridge the gap between these two modalities.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°æ­¤ä¸ºæ­¢ï¼Œæˆ‘å·²ç»å¾—å‡ºç»“è®ºï¼Œå°†è¡¨æƒ…ç¬¦å·ä»…è§†ä¸ºå›¾åƒæˆ–ä»…è§†ä¸ºæ–‡æœ¬ä¼šé—æ¼å¾ˆå¤šä¸°å¯Œçš„ä¿¡æ¯ã€‚ä¸ºäº†æ„å»ºä¸€ä¸ªå¼ºå¤§çš„è¯­ä¹‰è¡¨æƒ…ç¬¦å·æœç´¢å¼•æ“ï¼Œæˆ‘å¸Œæœ›ç»“åˆæ–‡æœ¬å’Œå›¾åƒä¿¡æ¯ï¼Œå¼¥åˆè¿™ä¸¤ç§æ¨¡æ€ä¹‹é—´çš„å·®è·ã€‚
- en: I tried generating descriptions of the emoji images using Adeptâ€™s multimodal
    [Fuyu-8b](https://www.adept.ai/blog/fuyu-8b) model, but these descriptions proved
    far too detailed; I tried using other CLIP-style models like [MetaCLIP](https://github.com/facebookresearch/metaclip),
    but saw the same behavior as in CLIP; I even tried using [GPT-4V](https://openai.com/research/gpt-4v-system-card)
    to generate captions for the emoji images, but was cut off by OpenAI because the
    rate limit for the model is 100 queries per day.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°è¯•ä½¿ç”¨Adeptçš„å¤šæ¨¡æ€æ¨¡å‹[Fuyu-8b](https://www.adept.ai/blog/fuyu-8b)ç”Ÿæˆè¡¨æƒ…ç¬¦å·å›¾åƒçš„æè¿°ï¼Œä½†è¿™äº›æè¿°è¿‡äºè¯¦ç»†ï¼›æˆ‘å°è¯•ä½¿ç”¨å…¶ä»–ç±»ä¼¼CLIPçš„æ¨¡å‹ï¼Œå¦‚[MetaCLIP](https://github.com/facebookresearch/metaclip)ï¼Œä½†è¡¨ç°ä¸CLIPç›¸åŒï¼›æˆ‘ç”šè‡³å°è¯•ä½¿ç”¨[GPT-4V](https://openai.com/research/gpt-4v-system-card)ç”Ÿæˆè¡¨æƒ…ç¬¦å·å›¾åƒçš„æ ‡é¢˜ï¼Œä½†ç”±äºæ¨¡å‹çš„æŸ¥è¯¢é¢‘ç‡é™åˆ¶ä¸ºæ¯å¤©100æ¬¡ï¼Œæˆ‘è¢«OpenAIä¸­æ–­äº†ã€‚
- en: 'In the end, I was able to pass the emoji unicode characters into the base GPT-4
    API with the prompt:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆï¼Œæˆ‘èƒ½å¤Ÿå°†è¡¨æƒ…ç¬¦å·çš„Unicodeå­—ç¬¦ä¼ é€’åˆ°åŸºç¡€çš„GPT-4 APIä¸­ï¼Œæç¤ºä¸ºï¼š
- en: '[PRE11]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: After post-processing these captions, I removed the â€œA photo ofâ€ prefix and
    used these descriptions in the semantic search pipeline.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¯¹è¿™äº›æ ‡é¢˜è¿›è¡Œåå¤„ç†åï¼Œæˆ‘åˆ é™¤äº†â€œA photo ofâ€å‰ç¼€ï¼Œå¹¶å°†è¿™äº›æè¿°ç”¨äºè¯­ä¹‰æœç´¢ç®¡é“ä¸­ã€‚
- en: 'The emoji search engine works as follows, taking in an input *query*:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨æƒ…ç¬¦å·æœç´¢å¼•æ“çš„å·¥ä½œåŸç†å¦‚ä¸‹ï¼Œè¾“å…¥*query*ï¼š
- en: Generate a set of 100 candidate emojis (out of 1816) with an image similarity
    search that compares the image embeddings to the query embedding. Save this ordering,
    *clip_image_ordering.*
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç”Ÿæˆ100ä¸ªå€™é€‰è¡¨æƒ…ç¬¦å·ï¼ˆä»1816ä¸ªä¸­é€‰æ‹©ï¼‰ï¼Œä½¿ç”¨å›¾åƒç›¸ä¼¼åº¦æœç´¢å°†å›¾åƒåµŒå…¥ä¸æŸ¥è¯¢åµŒå…¥è¿›è¡Œæ¯”è¾ƒã€‚ä¿å­˜æ­¤æ’åºï¼Œ*clip_image_ordering*ã€‚
- en: Order these candidate emojis by the similarity of the CLIP embeddings of the
    emoji names to the queryâ€™s embedding (*clip_name_ordering*).
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æŒ‰ç…§è¡¨æƒ…ç¬¦å·åç§°çš„CLIPåµŒå…¥ä¸æŸ¥è¯¢åµŒå…¥çš„ç›¸ä¼¼åº¦å¯¹è¿™äº›å€™é€‰è¡¨æƒ…ç¬¦å·è¿›è¡Œæ’åºï¼ˆ*clip_name_ordering*ï¼‰ã€‚
- en: Using a [cross-encoder](https://ai.plainenglish.io/decoding-sentence-representations-a-comprehensive-guide-to-cross-encoders-and-bi-encoders-67c4ac16e35f),
    order the emojis based on the similarity of their name (*cross_encoder_name_ordering*)
    and their description generated by GPT-4 (*cross_encoder_description_ordering*)
    to the query.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ [cross-encoder](https://ai.plainenglish.io/decoding-sentence-representations-a-comprehensive-guide-to-cross-encoders-and-bi-encoders-67c4ac16e35f)ï¼Œæ ¹æ®è¡¨æƒ…ç¬¦å·åç§°çš„ç›¸ä¼¼æ€§ï¼ˆ*cross_encoder_name_ordering*ï¼‰å’Œç”±
    GPT-4 ç”Ÿæˆçš„æè¿°ï¼ˆ*cross_encoder_description_ordering*ï¼‰å¯¹è¡¨æƒ…ç¬¦å·è¿›è¡Œæ’åºã€‚
- en: Combine all four orderings using [reciprocal rank fusion](https://www.elastic.co/guide/en/elasticsearch/reference/current/rrf.html),
    and return the top results!
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ [äº’æƒ æ’åèåˆ](https://www.elastic.co/guide/en/elasticsearch/reference/current/rrf.html)
    å°†å››ç§æ’åºç»“åˆèµ·æ¥ï¼Œå¹¶è¿”å›æ’åé å‰çš„ç»“æœï¼
- en: The resulting search engine isnâ€™t perfect, but it does a decent job at incorporating
    textual and visual information. Because using a cross-encoder is more computationally
    expensive (and higher latency), this is reserved for the pared-down set of candidates.
    I use the `distilroberta-base` checkpoint with the `CrossEncoder` class from the
    [Sentence Transformers](https://www.sbert.net/index.html) library.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœæœç´¢å¼•æ“è™½ç„¶ä¸å®Œç¾ï¼Œä½†åœ¨ç»“åˆæ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯æ–¹é¢åšå¾—ä¸é”™ã€‚ç”±äºä½¿ç”¨ cross-encoder è®¡ç®—å¼€é”€è¾ƒå¤§ï¼ˆä¸”å»¶è¿Ÿè¾ƒé«˜ï¼‰ï¼Œå› æ­¤å®ƒä»…ç”¨äºç®€åŒ–åçš„å€™é€‰é›†ã€‚æˆ‘ä½¿ç”¨çš„æ˜¯
    `distilroberta-base` æ£€æŸ¥ç‚¹ï¼Œå¹¶é…åˆ [Sentence Transformers](https://www.sbert.net/index.html)
    åº“ä¸­çš„ `CrossEncoder` ç±»ã€‚
- en: 'When all of these steps are combined, this is the result:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ‰€æœ‰è¿™äº›æ­¥éª¤ç»“åˆåœ¨ä¸€èµ·æ—¶ï¼Œç»“æœå¦‚ä¸‹ï¼š
- en: '![](../Images/2ddaf850b6a4265b791b6faf02d0df4f.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ddaf850b6a4265b791b6faf02d0df4f.png)'
- en: Again, it isnâ€™t perfect. But itâ€™s not bad!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å†æ¬¡å¼ºè°ƒï¼Œå®ƒä¸æ˜¯å®Œç¾çš„ï¼Œä½†å®ƒè¿˜ä¸é”™ï¼
- en: Using the Emoji Search Engine
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¡¨æƒ…ç¬¦å·æœç´¢å¼•æ“
- en: 'There are three ways to use this emoji search engine: hosted (free), locally
    via UI (open source), or locally via command line (also open source). All three
    options are quite easy!'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸‰ç§æ–¹å¼å¯ä»¥ä½¿ç”¨è¿™ä¸ªè¡¨æƒ…ç¬¦å·æœç´¢å¼•æ“ï¼šæ‰˜ç®¡ç‰ˆï¼ˆå…è´¹ï¼‰ã€é€šè¿‡ç”¨æˆ·ç•Œé¢åœ¨æœ¬åœ°ä½¿ç”¨ï¼ˆå¼€æºï¼‰æˆ–è€…é€šè¿‡å‘½ä»¤è¡Œåœ¨æœ¬åœ°ä½¿ç”¨ï¼ˆåŒæ ·æ˜¯å¼€æºçš„ï¼‰ã€‚è¿™ä¸‰ç§æ–¹å¼éƒ½éå¸¸ç®€å•ï¼
- en: Online
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åœ¨çº¿ç‰ˆ
- en: Head over to [try.fiftyone.ai/datasets/emojis](https://try.fiftyone.ai/datasets/emojis/samples),
    sign in (itâ€™s free), and click on the emoji button in the menu above the grid
    of images. Thatâ€™s it!
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: è®¿é—® [try.fiftyone.ai/datasets/emojis](https://try.fiftyone.ai/datasets/emojis/samples)ï¼Œç™»å½•ï¼ˆå…è´¹ï¼‰ï¼Œç„¶åç‚¹å‡»ç½‘æ ¼ä¸Šæ–¹èœå•ä¸­çš„è¡¨æƒ…ç¬¦å·æŒ‰é’®ã€‚å°±æ˜¯è¿™æ ·ï¼
- en: Locally via the UI
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é€šè¿‡ç”¨æˆ·ç•Œé¢æœ¬åœ°ä½¿ç”¨
- en: If you want to perform emoji searches locally with the same visual interface,
    you can do so with the [Emoji Search plugin](https://github.com/jacobmarks/emoji-search-plugin)
    for [FiftyOne](https://github.com/voxel51/fiftyone).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³åœ¨æœ¬åœ°ä½¿ç”¨ç›¸åŒçš„è§†è§‰ç•Œé¢è¿›è¡Œè¡¨æƒ…ç¬¦å·æœç´¢ï¼Œå¯ä»¥ä½¿ç”¨ [Emoji Search æ’ä»¶](https://github.com/jacobmarks/emoji-search-plugin)
    è¿›è¡Œ [FiftyOne](https://github.com/voxel51/fiftyone) æœç´¢ã€‚
- en: 'First, install FiftyOne:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œå®‰è£… FiftyOneï¼š
- en: '[PRE12]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then download the Emoji Search plugin and install its requirements:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åä¸‹è½½ Emoji Search æ’ä»¶å¹¶å®‰è£…å…¶ä¾èµ–ï¼š
- en: '[PRE13]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Launch the FiftyOne App:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: å¯åŠ¨ FiftyOne åº”ç”¨ï¼š
- en: '[PRE14]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Click on the â€œbrowse operationsâ€ text, search for â€œemojiâ€, and click on the
    entry â€œCreate Emoji Datasetâ€. This will download the high-resolution images of
    the emojis, along with embeddings and all other relevant data. At the top left
    of the app, click in the â€œSelect datasetâ€ box and select â€œEmojisâ€. Now you should
    see the same UI as in the hosted version.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ç‚¹å‡»â€œæµè§ˆæ“ä½œâ€æ–‡æœ¬ï¼Œæœç´¢â€œemojiâ€ï¼Œç„¶åç‚¹å‡»â€œåˆ›å»º Emoji æ•°æ®é›†â€æ¡ç›®ã€‚è¿™å°†ä¸‹è½½è¡¨æƒ…ç¬¦å·çš„é«˜æ¸…å›¾åƒã€åµŒå…¥ä»¥åŠæ‰€æœ‰ç›¸å…³æ•°æ®ã€‚åœ¨åº”ç”¨çš„å·¦ä¸Šè§’ï¼Œç‚¹å‡»â€œé€‰æ‹©æ•°æ®é›†â€æ¡†ï¼Œé€‰æ‹©â€œEmojisâ€ã€‚ç°åœ¨ï¼Œä½ åº”è¯¥çœ‹åˆ°ä¸æ‰˜ç®¡ç‰ˆæœ¬ç›¸åŒçš„ç”¨æˆ·ç•Œé¢ã€‚
- en: Locally via the CLI
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é€šè¿‡ CLI æœ¬åœ°ä½¿ç”¨
- en: 'Finally, you can search via the command line using the [Emoji Search](https://github.com/jacobmarks/emoji_search)
    Python CLI library. Install the package from GitHub repository with:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œä½ å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œä½¿ç”¨ [Emoji Search](https://github.com/jacobmarks/emoji_search) Python
    CLI åº“è¿›è¡Œæœç´¢ã€‚å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤ä» GitHub ä»“åº“å®‰è£…è¯¥è½¯ä»¶åŒ…ï¼š
- en: '[PRE15]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Then you can start searching using the `emoji-search` command, followed by the
    text query (with or without quotation marks).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œä½ å¯ä»¥ä½¿ç”¨ `emoji-search` å‘½ä»¤è¿›è¡Œæœç´¢ï¼Œåé¢è·Ÿä¸Šæ–‡æœ¬æŸ¥è¯¢ï¼ˆå¸¦ä¸å¸¦å¼•å·å‡å¯ï¼‰ã€‚
- en: '[PRE16]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The first search you perform will download embeddings to your device if necessary.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ¬¡æ‰§è¡Œæœç´¢æ—¶ï¼Œå¦‚æœéœ€è¦ï¼Œä¼šå°†åµŒå…¥ä¸‹è½½åˆ°ä½ çš„è®¾å¤‡ä¸Šã€‚
- en: All three versions support copying an emoji to clipboard with [pyperclip](https://pypi.org/project/pyperclip/).
    In the UI, click on the image for an emoji, and youâ€™ll see a copy button appear
    in the menu. In the CLI, pass the `-c` argument to copy the top result to clipboard.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰ä¸‰ä¸ªç‰ˆæœ¬éƒ½æ”¯æŒé€šè¿‡ [pyperclip](https://pypi.org/project/pyperclip/) å¤åˆ¶è¡¨æƒ…ç¬¦å·åˆ°å‰ªè´´æ¿ã€‚åœ¨ç”¨æˆ·ç•Œé¢ä¸­ï¼Œç‚¹å‡»è¡¨æƒ…ç¬¦å·çš„å›¾åƒï¼Œä½ ä¼šçœ‹åˆ°èœå•ä¸­å‡ºç°ä¸€ä¸ªå¤åˆ¶æŒ‰é’®ã€‚åœ¨å‘½ä»¤è¡Œç•Œé¢ä¸­ï¼Œä½¿ç”¨
    `-c` å‚æ•°å°†é¡¶éƒ¨ç»“æœå¤åˆ¶åˆ°å‰ªè´´æ¿ã€‚
- en: Conclusion
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: 'Emojis might seem like a silly subject to obsess over. And in practice, the
    utility of a semantic emoji search engine over lexical emoji search may be somewhat
    limited. The real value in this endeavor is in understanding the boundaries and
    overlaps between two modalities we traditionally think of as distinct: images
    and text. Emojis sit squarely in this intersection and as such, they allow us
    to probe the strengths and weaknesses â€” the capabilities and limitations of todayâ€™s
    multimodal models.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨æƒ…ç¬¦å·å¯èƒ½çœ‹èµ·æ¥æ˜¯ä¸€ä¸ªæ— èŠçš„ä¸»é¢˜ï¼Œè®©äººæ„Ÿåˆ°è¿‡åº¦å…³æ³¨ã€‚ä½†å®é™…ä¸Šï¼Œè¯­ä¹‰è¡¨æƒ…ç¬¦å·æœç´¢å¼•æ“ç›¸æ¯”äºè¯æ±‡è¡¨æƒ…ç¬¦å·æœç´¢ï¼Œå…¶å®ç”¨æ€§å¯èƒ½æœ‰é™ã€‚è¿™ä¸ªå·¥ä½œçš„çœŸæ­£ä»·å€¼åœ¨äºç†è§£æˆ‘ä»¬ä¼ ç»Ÿä¸Šè®¤ä¸ºæ˜¯ç‹¬ç«‹çš„ä¸¤ç§æ¨¡å¼â€”â€”å›¾åƒå’Œæ–‡æœ¬â€”â€”ä¹‹é—´çš„è¾¹ç•Œå’Œé‡å ã€‚è¡¨æƒ…ç¬¦å·æ°å¥½ä½äºè¿™ä¸ªäº¤æ±‡ç‚¹ï¼Œå› æ­¤ï¼Œå®ƒä»¬è®©æˆ‘ä»¬èƒ½å¤Ÿæ¢ç©¶ä»Šå¤©å¤šæ¨¡æ€æ¨¡å‹çš„ä¼˜ç¼ºç‚¹â€”â€”å®ƒä»¬çš„èƒ½åŠ›ä¸å±€é™ã€‚
- en: 'The Semantic Emoji Search Engine I ended up building is far from perfect. Frankly,
    emojis have subjectivity, connoting different things for different people, that
    is impossible to precisely bottle up. But going back to the motivating example,
    when I type in â€œan audio playerâ€, I get some solid results:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æœ€ç»ˆæ„å»ºçš„è¯­ä¹‰è¡¨æƒ…ç¬¦å·æœç´¢å¼•æ“è¿œéå®Œç¾ã€‚å¦ç‡æ¥è¯´ï¼Œè¡¨æƒ…ç¬¦å·å…·æœ‰ä¸»è§‚æ€§ï¼Œä»£è¡¨ä¸åŒçš„äººå¯èƒ½æœ‰ä¸åŒçš„å«ä¹‰ï¼Œè¿™ä¸€ç‚¹æ˜¯æ— æ³•ç²¾ç¡®åœ°å°è£…çš„ã€‚ä½†å›åˆ°æ¿€å‘è¿™ä¸ªæƒ³æ³•çš„ä¾‹å­ï¼Œå½“æˆ‘è¾“å…¥â€œä¸€ä¸ªéŸ³é¢‘æ’­æ”¾å™¨â€æ—¶ï¼Œæˆ‘å¾—åˆ°äº†å‡ ä¸ªä¸é”™çš„ç»“æœï¼š
- en: '![](../Images/1861d91728d8781810ec0c2072406e80.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1861d91728d8781810ec0c2072406e80.png)'
- en: 'Iâ€™ll end with a quote from [Nancy Gibbs](https://en.wikipedia.org/wiki/Nancy_Gibbs),
    a Professor at the Harvard Kennedy School and former managing editor for *TIME*
    magazine:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†ä»¥å“ˆä½›å¤§å­¦ç”˜å°¼è¿ªå­¦é™¢æ•™æˆã€å‰ã€Šæ—¶ä»£ã€‹æ‚å¿—æ‰§è¡Œç¼–è¾‘[å—å¸ŒÂ·å‰å¸ƒæ–¯](https://en.wikipedia.org/wiki/Nancy_Gibbs)çš„ä¸€å¥è¯ä½œä¸ºç»“å°¾ï¼š
- en: '*What makes emojis special is the fact that [they have] helped millions express
    themselves better than even the wide array of words in the Oxford dictionary [could].*'
  id: totrans-132
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*è¡¨æƒ…ç¬¦å·ä¹‹æ‰€ä»¥ç‰¹åˆ«ï¼Œæ˜¯å› ä¸º[å®ƒä»¬]å¸®åŠ©äº†æ•°ç™¾ä¸‡äººæ¯”ç‰›æ´¥è¯å…¸ä¸­å¹¿æ³›çš„è¯æ±‡è¿˜è¦æ›´å¥½åœ°è¡¨è¾¾è‡ªå·±ã€‚*'
- en: ''
  id: totrans-133
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Nancy Gibbs
  id: totrans-134
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å—å¸ŒÂ·å‰å¸ƒæ–¯
- en: '*Note: All images in article created by the author unless otherwise noted*'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ³¨æ„ï¼šæ–‡ç« ä¸­çš„æ‰€æœ‰å›¾åƒç”±ä½œè€…åˆ›ä½œï¼Œé™¤éå¦æœ‰è¯´æ˜*'
