- en: FrugalGPT and Reducing LLM Operating Costs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FrugalGPT和降低LLM运营成本
- en: 原文：[https://towardsdatascience.com/frugalgpt-and-reducing-llm-operating-costs-ff1a6428bf96?source=collection_archive---------10-----------------------#2024-03-27](https://towardsdatascience.com/frugalgpt-and-reducing-llm-operating-costs-ff1a6428bf96?source=collection_archive---------10-----------------------#2024-03-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/frugalgpt-and-reducing-llm-operating-costs-ff1a6428bf96?source=collection_archive---------10-----------------------#2024-03-27](https://towardsdatascience.com/frugalgpt-and-reducing-llm-operating-costs-ff1a6428bf96?source=collection_archive---------10-----------------------#2024-03-27)
- en: This blog post will go into detail about a cost-saving architecture for LLM-driven
    apps as seen in the “FrugalGPT” paper
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本博客将详细介绍“FrugalGPT”论文中所见的针对LLM驱动应用的节省成本架构。
- en: '[](https://medium.com/@mgunton7?source=post_page---byline--ff1a6428bf96--------------------------------)[![Matthew
    Gunton](../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png)](https://medium.com/@mgunton7?source=post_page---byline--ff1a6428bf96--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ff1a6428bf96--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ff1a6428bf96--------------------------------)
    [Matthew Gunton](https://medium.com/@mgunton7?source=post_page---byline--ff1a6428bf96--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mgunton7?source=post_page---byline--ff1a6428bf96--------------------------------)[![Matthew
    Gunton](../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png)](https://medium.com/@mgunton7?source=post_page---byline--ff1a6428bf96--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ff1a6428bf96--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ff1a6428bf96--------------------------------)
    [Matthew Gunton](https://medium.com/@mgunton7?source=post_page---byline--ff1a6428bf96--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ff1a6428bf96--------------------------------)
    ·5 min read·Mar 27, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ff1a6428bf96--------------------------------)
    ·阅读时间5分钟·2024年3月27日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/283cb60023e21cabbe9362b4ba89a43f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/283cb60023e21cabbe9362b4ba89a43f.png)'
- en: Image by Author generated by DALL-E
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者生成，使用DALL-E
- en: 'Large Language Models open up a new frontier for computer science, however,
    they are (as of 2024) significantly more expensive to run than almost anything
    else in computer science. For companies looking to minimize their operating costs,
    this poses a serious problem. The “[FrugalGPT: How to Use Large Language Models
    While Reducing Cost and Improving Performance](https://arxiv.org/abs/2305.05176)”
    paper introduces one framework to reduce operating costs significantly while maintaining
    quality.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型为计算机科学开辟了一个新的前沿，然而，到了2024年，它们的运行成本显著高于计算机科学中几乎所有其他项目。对于寻求降低运营成本的公司来说，这构成了一个严重的问题。论文“[FrugalGPT：如何在减少成本并提高性能的同时使用大型语言模型](https://arxiv.org/abs/2305.05176)”提出了一个框架，可以在保持质量的同时显著降低运营成本。
- en: How to Measure the Cost of LLM
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何衡量LLM的成本
- en: There are multiple ways to determine the cost of running a LLM (electricity
    use, compute cost, etc.), however, if you use a third-party LLM (a LLM-as-a-service)
    they typically charge you based on the tokens you use. Different vendors (OpenAI,
    Anthropic, Cohere, etc.) have different ways of counting the tokens, but for the
    sake of simplicity, we’ll consider the cost to be based on the number of tokens
    processed by the LLM.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以确定运行大型语言模型（LLM）的成本（例如电力消耗、计算成本等），然而，如果使用第三方LLM（即LLM即服务），他们通常会根据你使用的tokens数量收费。不同的供应商（如OpenAI、Anthropic、Cohere等）有不同的tokens计数方式，但为了简化起见，我们将成本视为基于LLM处理的tokens数量。
- en: The most important part of this framework is the idea that different models
    cost different amounts. The authors of the paper conveniently assembled the below
    table highlighting the difference in cost, and the difference between them is
    significant. For example, AI21’s output tokens cost an order of magnitude more
    than GPT-4’s does in this table!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这个框架最重要的部分是不同模型的成本是不同的。论文的作者巧妙地整理了以下表格，突出了成本的差异，它们之间的差距非常显著。例如，在此表中，AI21的输出tokens成本比GPT-4高出一个数量级！
- en: '![](../Images/eb12d44378e9fee17e9d9640cee8141a.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb12d44378e9fee17e9d9640cee8141a.png)'
- en: Table 1 [from the paper](https://arxiv.org/pdf/2305.05176.pdf)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 表1 [来自论文](https://arxiv.org/pdf/2305.05176.pdf)
- en: As a part of cost optimization we always need to figure out a way to optimize
    the answer quality while minimizing the cost. Typically, higher cost models are
    often higher performing models, able to give higher quality answers than lower
    cost ones. The general relationship can be seen in the below graph, with Frugal
    GPT’s performance overlaid on top in red.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 作为成本优化的一部分，我们始终需要找到在最小化成本的同时优化答案质量的方法。通常情况下，较高成本的模型往往性能较好，能够提供比低成本模型更高质量的答案。下图展示了这一普遍关系，FrugalGPT的表现以红色覆盖在上面。
- en: '![](../Images/416314c080ca6deebd5d109f095497cd.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/416314c080ca6deebd5d109f095497cd.png)'
- en: Figure 1c [from the paper](https://arxiv.org/pdf/2305.05176.pdf) comparing various
    LLMs based on the how often they would accurately respond to questions based on
    the HEADLINES dataset
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1c [来自论文](https://arxiv.org/pdf/2305.05176.pdf) 对比了各种LLM基于HEADLINES数据集准确回答问题的频率
- en: Maximizing Quality with Cascading LLMS
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用级联LLM最大化质量
- en: Using the vast cost difference between models, the researchers’ FrugalGPT system
    relies on a cascade of LLMs to give the user an answer. Put simply, the user query
    begins with the cheapest LLM, and if the answer is good enough, then it is returned.
    However, if the answer is not good enough, then the query is passed along to the
    next cheapest LLM.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用不同模型之间巨大的成本差异，研究人员的FrugalGPT系统依赖于LLM的级联来为用户提供答案。简单来说，用户查询从最便宜的LLM开始，如果答案足够好，就返回该答案。如果答案不够好，则查询会传递给下一个最便宜的LLM。
- en: 'The researchers used the following logic: if a less expensive model answers
    a question incorrectly, then it is likely that a more expensive model will give
    the answer correctly. Thus, to minimize costs the chain is ordered from least
    expensive to most expensive, assuming that quality goes up as you get more expensive.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员使用了以下逻辑：如果一个较便宜的模型回答问题错误，那么较昂贵的模型很可能会给出正确的答案。因此，为了最小化成本，链条按从最便宜到最昂贵的顺序排列，假设随着价格的上涨，质量也会提高。
- en: '![](../Images/5d3fd2f3afb1b481df87d7025f3badc6.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d3fd2f3afb1b481df87d7025f3badc6.png)'
- en: Figure 2e [from the paper](https://arxiv.org/pdf/2305.05176.pdf) illustrating
    the LLM cascade
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图2e [来自论文](https://arxiv.org/pdf/2305.05176.pdf) 展示了LLM级联的示意图
- en: This setup relies on reliably determining when an answer is good enough and
    when it isn’t. To solve for this, the authors created a DistilBERT model that
    would take the question and answer then assign a score to the answer. As the DistilBERT
    model is exponentially smaller than the other models in the sequence, the cost
    to run it is almost negligible compared to the others.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设置依赖于可靠地判断一个答案是否足够好。为了解决这个问题，作者创建了一个DistilBERT模型，它会对问题和答案进行评分。由于DistilBERT模型的体积比序列中的其他模型要小得多，因此与其他模型相比，运行它的成本几乎可以忽略不计。
- en: Better Average Quality Than Just Querying the Best LLM
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比仅仅查询最好的LLM获得更好的平均质量
- en: One might naturally ask, if quality is most important, why not just query the
    best LLM and work on ways to reduce the cost of running the best LLM?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 人们自然会问，如果质量最重要，为什么不直接查询最好的LLM，并寻求降低运行最佳LLM的成本的方法？
- en: When this paper came out GPT-4 was the best LLM they found, yet GPT-4 did not
    always give a better answer than the FrugalGPT system! (*Eagle-eyed readers will
    see this as part of the cost vs performance graph from before*) The authors speculate
    that just as the most capable person doesn’t always give the right answer, the
    most complex model won’t either. Thus, by having the answer go through a filtering
    process with DistilBERT, you are removing any answers that aren’t up to par and
    increasing the odds of a good answer.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当这篇论文发布时，GPT-4是他们找到的最好的LLM，然而，GPT-4并不总是给出比FrugalGPT系统更好的答案！(*细心的读者会注意到，这是之前成本与性能图表的一部分*)
    作者推测，就像最有能力的人不总是给出正确的答案一样，最复杂的模型也未必如此。因此，通过让答案经过DistilBERT的过滤过程，你可以去除任何不合格的答案，从而提高获得好答案的几率。
- en: '![](../Images/72f31eda14c9ca1d134d4abc6e7086e4.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/72f31eda14c9ca1d134d4abc6e7086e4.png)'
- en: Figure 5a [from the paper](https://arxiv.org/pdf/2305.05176.pdf) showing instances
    where FrugalGPT is outperforming GPT-4
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图5a [来自论文](https://arxiv.org/pdf/2305.05176.pdf) 显示了FrugalGPT超越GPT-4的实例
- en: Consequently, this system not only reduces your costs but can also increase
    quality more so than just using the best LLM!
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这个系统不仅能降低成本，还能提高质量，比单纯使用最好的LLM更有效！
- en: Moving Forwards with Cost Savings
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向前迈进，节约成本
- en: The results of this paper are fascinating to consider. For me, it raises questions
    about how we can go even further with cost savings without having to invest in
    further model optimization.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的结果令人深思。对我来说，它引发了关于如何在不进一步投资于模型优化的情况下，继续实现成本节约的问题。
- en: One such possibility is to cache all model answers in a vector database and
    then do a similarity search to determine if the answer in the cache works before
    starting the LLM cascade. This would significantly reduce costs by replacing a
    costly LLM operation with a comparatively less expensive query and similarity
    operation.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一种可能性是将所有模型的答案缓存到向量数据库中，然后进行相似性搜索，确定缓存中的答案是否适用，再开始LLM链的操作。通过用相对较便宜的查询和相似性操作替代昂贵的LLM操作，这将显著降低成本。
- en: Additionally, it makes you wonder if outdated models can still be worth cost-optimizing,
    as if you can reduce their cost per token, they can still create value on the
    LLM cascade. Similarly, the key question here is at what point do you get diminishing
    returns by adding new LLMs onto the chain.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这也让人思考是否过时的模型依然值得进行成本优化，因为如果你能降低每个token的成本，它们仍然可以在LLM链中创造价值。类似地，关键问题在于，何时通过将新的LLM加入链条中会出现收益递减的现象。
- en: Questions for Further Study
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 后续研究问题
- en: As the world creates more LLMs and we increasingly build systems that use them,
    we will want to find cost-effective ways to run them. This paper creates a strong
    framework for future builders to expand on, making me wonder about how far this
    framework can go.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 随着世界上大语言模型的增加，我们越来越多地构建使用这些模型的系统，我们将希望找到更具成本效益的运行方式。本文为未来的开发者提供了一个强有力的框架，这让我思考这个框架能够走多远。
- en: In my opinion, this framework applies really well for general queries that do
    not have different answers based on different users, such as a tutor LLM. However,
    for use cases where answers differ based on the user, say a LLM that acts as a
    customer service agent, the scoring system would have to be aware of who the LLM
    was talking with.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在我看来，这个框架非常适用于那些没有基于不同用户提供不同答案的一般性查询，例如辅导型大语言模型（LLM）。然而，对于那些答案根据用户不同而有所不同的应用场景，比如作为客服代理的大语言模型，评分系统必须能够识别LLM与谁进行对话。
- en: Finding a framework that saves money for user-specific interactions will be
    important for the future.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为用户特定交互节省成本的框架将是未来的重要课题。
- en: '[1] Chen, L., et al., [FrugalGPT: How to Use Large Language Models While Reducing
    Cost and Improving Performance](https://arxiv.org/abs/2305.05176) (2023), arXiv'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Chen, L., et al., [FrugalGPT: 如何在降低成本并提高性能的同时使用大语言模型](https://arxiv.org/abs/2305.05176)
    (2023), arXiv'
