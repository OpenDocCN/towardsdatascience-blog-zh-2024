- en: Integrating Text and Images for Smarter Data Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成文本和图像，进行更智能的数据分类
- en: 原文：[https://towardsdatascience.com/integrating-text-and-images-for-smarter-data-classification-6a53252d8a73?source=collection_archive---------6-----------------------#2024-11-18](https://towardsdatascience.com/integrating-text-and-images-for-smarter-data-classification-6a53252d8a73?source=collection_archive---------6-----------------------#2024-11-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/integrating-text-and-images-for-smarter-data-classification-6a53252d8a73?source=collection_archive---------6-----------------------#2024-11-18](https://towardsdatascience.com/integrating-text-and-images-for-smarter-data-classification-6a53252d8a73?source=collection_archive---------6-----------------------#2024-11-18)
- en: '[](https://medium.com/@CVxTz?source=post_page---byline--6a53252d8a73--------------------------------)[![Youness
    Mansar](../Images/b68fe2cbbe219ab0231922c7165f2b6a.png)](https://medium.com/@CVxTz?source=post_page---byline--6a53252d8a73--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6a53252d8a73--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6a53252d8a73--------------------------------)
    [Youness Mansar](https://medium.com/@CVxTz?source=post_page---byline--6a53252d8a73--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@CVxTz?source=post_page---byline--6a53252d8a73--------------------------------)[![Youness
    Mansar](../Images/b68fe2cbbe219ab0231922c7165f2b6a.png)](https://medium.com/@CVxTz?source=post_page---byline--6a53252d8a73--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6a53252d8a73--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6a53252d8a73--------------------------------)
    [Youness Mansar](https://medium.com/@CVxTz?source=post_page---byline--6a53252d8a73--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6a53252d8a73--------------------------------)
    ·7 min read·Nov 18, 2024
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6a53252d8a73--------------------------------)
    ·7分钟阅读·2024年11月18日
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: A technical walk-through on leveraging multi-modal AI to classify mixed text
    and image data, including detailed instructions, executable code examples, and
    tips for effective implementation.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一篇关于如何利用多模态人工智能分类混合文本和图像数据的技术演示，内容包括详细的说明、可执行的代码示例以及有效实现的技巧。
- en: '![](../Images/88c05e548892522a34fec7ecb8d4b602.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/88c05e548892522a34fec7ecb8d4b602.png)'
- en: Photo by [Tschernjawski Sergej](https://unsplash.com/@mrt1987?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Tschernjawski Sergej](https://unsplash.com/@mrt1987?utm_source=medium&utm_medium=referral)
    来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In AI, one of the most exciting areas of growth is **multimodal learning**,
    where models process and combine different types of data — such as images and
    text — to better understand complex scenarios. This approach is particularly useful
    in real-world applications where information is often split between text and visuals.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能领域，最令人兴奋的增长领域之一是**多模态学习**，即模型处理并结合不同类型的数据——如图像和文本——以更好地理解复杂场景。这种方法在现实世界应用中尤其有用，因为信息通常在文本和视觉之间分割。
- en: 'Take e-commerce as an example: a product listing might include an image showing
    what an item looks like and a description providing details about its features.
    To fully classify and understand the product, both sources of information need
    to be considered together. Multimodal large language models (LLMs) like **Gemini
    1.5**, **Llama 3.2, Phi-3 Vision**, and open-source tools such as **LlaVA, DocOwl**
    have been developed specifically to handle these types of inputs.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 以电子商务为例：一个产品列表可能包括一张显示商品外观的图片，以及一段提供其功能细节的描述。为了全面分类和理解该产品，需要将这两种信息来源结合起来考虑。像**Gemini
    1.5**、**Llama 3.2、Phi-3 Vision**这样的多模态大型语言模型（LLMs），以及**LlaVA、DocOwl**等开源工具，专门开发用于处理这些类型的输入。
- en: Why Multimodal Models Are Important
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么多模态模型很重要
- en: 'Information from images and text can complement each other in ways that single-modality
    systems might miss:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图像和文本中的信息可以互相补充，而单一模态系统可能无法捕捉到这些信息：
- en: A product’s description might mention its dimensions or material, which isn’t
    clear from the image alone.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个产品的描述可能提到其尺寸或材质，这仅凭图片是无法清楚表达的。
- en: On the other hand, an image might reveal key aspects like style or color that
    text can’t adequately describe.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，一张图片可能揭示出如风格或颜色等文本无法充分描述的关键方面。
- en: If we only process images or text separately, we risk missing critical details.
    Multimodal models address this challenge by combining both sources during processing,
    resulting in more accurate and useful outcomes.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们单独处理图像或文本，就有可能错过关键信息。多模态模型通过在处理过程中结合这两种来源来解决这个问题，从而获得更准确、更有用的结果。
- en: What You’ll Learn in This Tutorial
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本教程中你将学习的内容
- en: This tutorial will guide you through creating a pipeline designed to handle
    **image-text classification**. You’ll learn how to process and analyze inputs
    that combine visual and textual elements, achieving results that are more accurate
    than those from text-only systems.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程将指导你创建一个旨在处理**图像-文本分类**的管道。你将学习如何处理和分析结合了视觉和文本元素的输入，从而获得比仅使用文本系统更准确的结果。
- en: If your project involves text-only classification, you might find my [other
    blog post](https://medium.com/towards-data-science/building-a-reliable-text-classification-pipeline-with-llms-a-step-by-step-guide-87dc73213605)
    helpful — it focuses specifically on those methods.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的项目仅涉及文本分类，我的[另一篇博客文章](https://medium.com/towards-data-science/building-a-reliable-text-classification-pipeline-with-llms-a-step-by-step-guide-87dc73213605)可能会对你有所帮助——它专门讨论了这些方法。
- en: '[](/building-a-reliable-text-classification-pipeline-with-llms-a-step-by-step-guide-87dc73213605?source=post_page-----6a53252d8a73--------------------------------)
    [## Building a Reliable Text Classification Pipeline with LLMs: A Step-by-Step
    Guide'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/building-a-reliable-text-classification-pipeline-with-llms-a-step-by-step-guide-87dc73213605?source=post_page-----6a53252d8a73--------------------------------)
    [## 使用LLM构建可靠的文本分类管道：一步一步的指南'
- en: Overcoming common challenges in LLM-based text classification
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 克服基于LLM的文本分类中的常见挑战
- en: towardsdatascience.com](/building-a-reliable-text-classification-pipeline-with-llms-a-step-by-step-guide-87dc73213605?source=post_page-----6a53252d8a73--------------------------------)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/building-a-reliable-text-classification-pipeline-with-llms-a-step-by-step-guide-87dc73213605?source=post_page-----6a53252d8a73--------------------------------)'
- en: 'To successfully build a multimodal image-text classification system, we’ll
    need three essential components. Here’s a breakdown of each element:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要成功构建一个多模态的图像-文本分类系统，我们需要三个关键组件。下面是每个元素的详细介绍：
- en: 1\. A Reliable LLM Provider
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 一个可靠的LLM提供商
- en: 'The backbone of this tutorial is a **hosted LLM as a service**. After experimenting
    with several options, I found that not all LLMs deliver consistent results, especially
    when working with structured outputs. Here’s a summary of my experience:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程的核心是**托管的LLM作为服务**。在试验了几种选项后，我发现并非所有LLM都能提供一致的结果，特别是在处理结构化输出时。以下是我的经验总结：
- en: '**Groq** and **Fireworks.ai**: These platforms offer multimodal LLMs in a serverless,
    pay-per-token format. While they seem promising, their APIs had issues following
    structured output requests. For example, when sending a query with a predefined
    schema, the returned output didn’t adhere to the expected format, making them
    unreliable for tasks requiring precision. Groq’s Llama 3.2 is still in preview
    so maybe I’ll try them again later. Fireworks.ai don’t typically respond to bug
    reports so I’ll just remove them from my options from now on.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Groq** 和 **Fireworks.ai**：这些平台提供无服务器、按令牌付费的多模态LLM。虽然它们看起来很有前景，但它们的API在处理结构化输出请求时存在问题。例如，当发送一个带有预定义架构的查询时，返回的输出并没有遵循预期的格式，使得它们在需要精确的任务中不可靠。Groq的Llama
    3.2仍处于预览阶段，所以我可能稍后会再次尝试它们。Fireworks.ai通常不响应错误报告，所以我从现在起就将它们从我的选项中删除。'
- en: '**Gemini 1.5**: After some trial and error, I settled on Gemini 1.5\. It consistently
    returned results in the desired format and has been working very ok so far. Though
    it still has its own weird quirks that you will find if you poke at it long enough
    (like the fact that you can’t use enums that are too large…). We will discuss
    them later in the post. This will be the LLM we use for this tutorial.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Gemini 1.5**：经过一些反复尝试，我选择了Gemini 1.5。它始终返回期望格式的结果，并且至今表现良好。尽管它仍然有一些奇怪的小毛病，如果你长时间使用它会发现（比如不能使用过大的枚举值……）。我们将在本文后续讨论这些问题。这将是我们在本教程中使用的LLM。'
- en: '2\. The Python Library: LangChain'
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. Python库：LangChain
- en: 'To interface with the LLM and handle multimodal inputs, we’ll use the **LangChain**
    library. LangChain is particularly well-suited for this task because it allows
    us to:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与LLM接口并处理多模态输入，我们将使用**LangChain**库。LangChain特别适合这项任务，因为它使我们能够：
- en: Inject both text and image data as input to the LLM.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文本和图像数据作为输入注入LLM。
- en: Defines common abstraction for different LLM as a service providers.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义不同LLM作为服务提供商的通用抽象。
- en: Define structured output schemas to ensure the results match the format we need.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义结构化输出模式，以确保结果符合我们需要的格式。
- en: Structured outputs are especially important for classification tasks, as they
    involve predefined classes that the output must conform to. LangChain ensures
    this structure is enforced, making it ideal for our use case.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化输出对分类任务尤其重要，因为它们涉及预定义的类别，输出必须符合这些类别。LangChain确保了这一结构的执行，使其成为我们用例的理想选择。
- en: '3\. The Classification Task: Keyword Suggestion for Photography Images'
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. 分类任务：摄影图片的关键词建议
- en: 'The task we’ll focus on in this tutorial is **keyword suggestion** for photography-related
    images. This is a **multi-label classification** problem, meaning that:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程中我们将关注的任务是**摄影相关图片的关键词建议**。这是一个**多标签分类**问题，这意味着：
- en: Each image can belong to more than one class simultaneously.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每张图片可以同时属于多个类别。
- en: The list of possible classes is predefined.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能的类别列表是预定义的。
- en: For instance, an input consisting of an image and its description might be classified
    with keywords like *landscape, sunset,* and *nature*. While multiple keywords
    can apply to a single input, they must be selected from the predefined set of
    classes.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个由图片及其描述组成的输入可能会被分类为如*landscape, sunset*和*nature*等关键词。虽然一个输入可以应用多个关键词，但这些关键词必须从预定义的类别集中选择。
- en: 'Step-by-Step Guide: Setting Up Multimodal Image-Text Classification with Gemini
    1.5 and LangChain'
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤指南：设置Gemini 1.5和LangChain的多模态图像-文本分类
- en: Now that we have the foundational concepts covered, let’s dive into the implementation.
    This step-by-step guide will walk you through configuring Gemini 1.5, setting
    up LangChain, and building a keyword suggestion system for photography-related
    images.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经覆盖了基础概念，让我们深入实现部分。这个逐步指南将引导你完成配置Gemini 1.5、设置LangChain以及为摄影相关图片构建关键词建议系统的过程。
- en: 'Step 1: Obtain Your Gemini API Key'
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一步：获取你的Gemini API密钥
- en: 'The first step is to get your **Gemini API key**, which you can generate in
    [Google AI Studio](https://aistudio.google.com/app/apikey). Once you have your
    key, export it to an environment variable called `GOOGLE_API_KEY`. You can either:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是获取你的**Gemini API 密钥**，你可以在[Google AI Studio](https://aistudio.google.com/app/apikey)生成该密钥。获得密钥后，将其导出到名为`GOOGLE_API_KEY`的环境变量中。你可以选择：
- en: 'Add it to a `.env` file:'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将其添加到`.env`文件中：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Export it directly in your terminal:'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接在终端中导出：
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Step 2: Install and Initialize the Client'
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2步：安装并初始化客户端
- en: 'Next, install the necessary libraries:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，安装必要的库：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Once installed, initialize the client:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，初始化客户端：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Step 3: Define the Output Schema'
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3步：定义输出模式
- en: To ensure the LLM produces valid, structured results, we use **Pydantic** to
    define an output schema. This schema acts as a filter, validating that the categories
    returned by the model match our predefined list of acceptable values.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保LLM生成有效的结构化结果，我们使用**Pydantic**定义输出模式。这个模式充当过滤器，验证模型返回的类别是否与我们预定义的可接受类别列表匹配。
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Why** `**field_validator**` **Is Needed as a Workaround:**'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么** `**field_validator**` **作为变通方法是必要的：**'
- en: 'While defining the schema, we encountered a limitation in Gemini 1.5 (and similar
    LLMs): they do not strictly enforce **enums**. This means that even though we
    provide a fixed set of categories, the model might return values outside this
    set. For example:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义模式时，我们遇到了Gemini 1.5（以及类似的LLM）的一项限制：它们不严格执行**枚举**。这意味着，即使我们提供了固定的类别集，模型也可能返回超出此集合的值。例如：
- en: 'Expected: `["landscape", "forest", "mountain"]`'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预期：`["landscape", "forest", "mountain"]`
- en: 'Returned: `["landscape", "ocean", "sun"]` *(with "ocean" and "sun" being invalid
    categories)*'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回：`["landscape", "ocean", "sun"]` *(其中“ocean”和“sun”是无效类别)*
- en: 'Without handling this, the invalid categories could cause errors or degrade
    the classification’s accuracy. To address this, the `field_validator` method is
    used as a workaround. It acts as a filter, ensuring:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不处理这个问题，无效的类别可能会导致错误或降低分类的准确性。为了解决这个问题，可以使用`field_validator`方法作为一种变通方法。它充当过滤器，确保：
- en: Only valid categories from `list_classes` are included in the output.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅有效的类别会从`list_classes`中包含到输出中。
- en: Invalid or unexpected values are removed.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 无效或意外的值将被移除。
- en: This safeguard ensures the model’s results align with the task’s requirements.
    It is annoying we have to do this but it seems to be a common issue for all LLM
    providers I tested, if you know of one that handles Enums well let me know please.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这个保障措施确保模型的结果与任务要求相符。虽然我们必须这样做很麻烦，但似乎是我测试过的所有LLM提供商的共同问题。如果你知道哪家提供商能很好地处理枚举，请告诉我。
- en: 'Step 4: Bind the Schema to the LLM Client'
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4步：将模式绑定到LLM客户端
- en: 'Next, bind the schema to the client for structured output handling:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将模式绑定到客户端进行结构化输出处理：
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Step 5: Build the Query and Call the LLM'
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第5步：构建查询并调用LLM
- en: 'Define the prediction function to send image and text inputs to the LLM:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 定义预测函数以将图像和文本输入发送到LLM：
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: To send image data to the Gemini LLM API, we need to encode the image into a
    format the model can process. This is where **base64 encoding** comes into play.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 要将图像数据发送到Gemini LLM API，我们需要将图像编码为模型可以处理的格式。这时**base64编码**就派上用场了。
- en: '**What is Base64?**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**什么是Base64？**'
- en: Base64 is a binary-to-text encoding scheme that converts binary data (like an
    image) into a text format. This is useful when transmitting data that might otherwise
    be incompatible with text-based systems, such as APIs. By encoding the image into
    base64, we can include it as part of the payload when sending data to the LLM.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Base64是一种二进制到文本的编码方案，将二进制数据（如图像）转换为文本格式。当传输与基于文本的系统（如API）不兼容的数据时，这非常有用。通过将图像编码为base64，我们可以将其作为有效载荷的一部分，在向LLM发送数据时一起发送。
- en: 'Step 6: Get Results as Multi-Label Keywords'
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第6步：以多标签关键词的形式获取结果
- en: 'Finally, run the classifier and see the results. Let’s test it with an example:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，运行分类器并查看结果。让我们用一个例子来测试：
- en: 'Example Input 1:'
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例输入1：
- en: '**Image**:'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图片**：'
- en: '![](../Images/6a7a0ee9460973d78f72f4b816a1fbed.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a7a0ee9460973d78f72f4b816a1fbed.png)'
- en: Photo by [Calvin Ma](https://unsplash.com/@mkwcalvin?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/classic-red-and-white-bus-parked-beside-road-VaH2X8eHKVg?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Calvin Ma](https://unsplash.com/@mkwcalvin?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)提供，来自[Unsplash](https://unsplash.com/photos/classic-red-and-white-bus-parked-beside-road-VaH2X8eHKVg?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
- en: '**Description**:'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**描述**：'
- en: classic red and white bus parked beside road
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 经典的红白相间的公交车停放在路旁
- en: 'Result:'
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果：
- en: '**Image + Text**:'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图片 + 文本**：'
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Text Only**:'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**仅文本**：'
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As shown, when using both text and image inputs, the results are more relevant
    to the actual content. With text-only input, the LLM gave correct but incomplete
    values.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，当使用文本和图像输入时，结果与实际内容更为相关。仅使用文本输入时，LLM给出了正确但不完整的值。
- en: 'Example Input 2:'
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例输入2：
- en: '**Image**:'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图片**：'
- en: '![](../Images/124e22659b64a9cdbe4ec3d8bdcf71bf.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/124e22659b64a9cdbe4ec3d8bdcf71bf.png)'
- en: Photo by [Tadeusz Lakota](https://unsplash.com/@tadekl?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/black-and-white-coated-dog-bLQFCJDImnc?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Tadeusz Lakota](https://unsplash.com/@tadekl?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)提供，来自[Unsplash](https://unsplash.com/photos/black-and-white-coated-dog-bLQFCJDImnc?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
- en: '**Description**:'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**描述**：'
- en: black and white coated dog
  id: totrans-89
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 黑白毛色的狗
- en: 'Result:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：
- en: '**Image + Text**:'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图片 + 文本**：'
- en: '[PRE9]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Text Only**:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**仅文本**：'
- en: '[PRE10]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Conclusion
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Multimodal classification, which combines text and image data, provides a way
    to create more contextually aware and effective AI systems. In this tutorial,
    we built a keyword suggestion system using Gemini 1.5 and LangChain, tackling
    key challenges like structured output handling and encoding image data.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态分类结合了文本和图像数据，为创建更具上下文感知和更有效的AI系统提供了一种方法。在本教程中，我们使用Gemini 1.5和LangChain构建了一个关键词建议系统，解决了结构化输出处理和图像数据编码等关键挑战。
- en: By blending text and visual inputs, we demonstrated how this approach can lead
    to more accurate and meaningful classifications than using either modality alone.
    The practical examples highlighted the value of combining data types to better
    capture the full context of a given scenario.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合文本和视觉输入，我们展示了这种方法如何比单独使用任何一种模式更准确、更有意义地进行分类。实际例子突显了将数据类型结合使用的价值，从而更好地捕捉给定场景的完整上下文。
- en: What’s Next?
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接下来做什么？
- en: 'This tutorial focused on text and image classification, but the principles
    can be applied to other multimodal setups. Here are some ideas to explore next:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程聚焦于文本和图像分类，但这些原理可以应用于其他多模态设置。以下是一些可以进一步探索的思路：
- en: '**Text and Video**: Extend the system to classify or analyze videos by integrating
    video frame sampling along with text inputs, such as subtitles or metadata.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本和视频**：通过结合视频帧采样和文本输入（如字幕或元数据），扩展系统以分类或分析视频。'
- en: '**Text and PDFs**: Develop classifiers that handle documents with rich content,
    like scientific papers, contracts, or resumes, combining visual layouts with textual
    data.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本和PDF**：开发能够处理丰富内容的文档分类器，如科研论文、合同或简历，结合视觉布局和文本数据。'
- en: '**Real-World Applications**: Integrate this pipeline into platforms like e-commerce
    sites, educational tools, or social media moderation systems.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**现实世界应用**：将此管道集成到电子商务网站、教育工具或社交媒体内容审查系统等平台中。'
- en: These directions demonstrate the flexibility of multimodal approaches and their
    potential to address diverse real-world challenges. As multimodal AI evolves,
    experimenting with various input combinations will open new possibilities for
    more intelligent and responsive systems.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方向展示了多模态方法的灵活性及其解决多样化现实世界挑战的潜力。随着多模态AI的进化，尝试不同的输入组合将为更智能、更具响应性的系统开辟新的可能性。
- en: 'Full code: [llmclassifier/llm_multi_modal_classifier.py](https://github.com/CVxTz/llmclassifier/blob/master/llmclassifier/llm_multi_modal_classifier.py)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 完整代码：[llmclassifier/llm_multi_modal_classifier.py](https://github.com/CVxTz/llmclassifier/blob/master/llmclassifier/llm_multi_modal_classifier.py)
