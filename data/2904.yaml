- en: Smaller Is Smarter
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 小型更智能
- en: 原文：[https://towardsdatascience.com/smaller-is-smarter-89a9b3a5ad9e?source=collection_archive---------2-----------------------#2024-12-01](https://towardsdatascience.com/smaller-is-smarter-89a9b3a5ad9e?source=collection_archive---------2-----------------------#2024-12-01)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/smaller-is-smarter-89a9b3a5ad9e?source=collection_archive---------2-----------------------#2024-12-01](https://towardsdatascience.com/smaller-is-smarter-89a9b3a5ad9e?source=collection_archive---------2-----------------------#2024-12-01)
- en: Do you really need the power of top LLMs for a tiramisu recipe in Shakespearean
    style?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你真的需要顶级LLM的算力来获得莎士比亚风格的提拉米苏食谱吗？
- en: '[](https://medium.com/@alexandre.allouin?source=post_page---byline--89a9b3a5ad9e--------------------------------)[![Alexandre
    Allouin](../Images/b393b888498e3dcfab488220968355cb.png)](https://medium.com/@alexandre.allouin?source=post_page---byline--89a9b3a5ad9e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--89a9b3a5ad9e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--89a9b3a5ad9e--------------------------------)
    [Alexandre Allouin](https://medium.com/@alexandre.allouin?source=post_page---byline--89a9b3a5ad9e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@alexandre.allouin?source=post_page---byline--89a9b3a5ad9e--------------------------------)[![Alexandre
    Allouin](../Images/b393b888498e3dcfab488220968355cb.png)](https://medium.com/@alexandre.allouin?source=post_page---byline--89a9b3a5ad9e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--89a9b3a5ad9e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--89a9b3a5ad9e--------------------------------)
    [Alexandre Allouin](https://medium.com/@alexandre.allouin?source=post_page---byline--89a9b3a5ad9e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--89a9b3a5ad9e--------------------------------)
    ·4 min read·Dec 1, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--89a9b3a5ad9e--------------------------------)
    ·4分钟阅读·2024年12月1日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Concerns about the environmental impacts of Large Language Models (LLMs) are
    growing. Although detailed information about the actual costs of LLMs can be difficult
    to find, let’s attempt to gather some facts to understand the scale.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 对大型语言模型（LLMs）对环境影响的关注日益增加。尽管关于LLM实际成本的详细信息可能难以获得，但我们可以尝试收集一些事实，以了解其规模。
- en: '![](../Images/cdf2cd3dcb299e0cccd6759da2e88ccc.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cdf2cd3dcb299e0cccd6759da2e88ccc.png)'
- en: Generated with ChatGPT-4o
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由ChatGPT-4o生成
- en: Since comprehensive data on ChatGPT-4 is not readily available, we can consider
    Llama 3.1 405B as an example. This open-source model from Meta is arguably the
    most “transparent” LLM to date. Based on various [benchmarks](https://ai.meta.com/blog/meta-llama-3-1/),
    Llama 3.1 405B is comparable to ChatGPT-4, providing a reasonable basis for understanding
    LLMs within this range.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 由于关于ChatGPT-4的综合数据尚不容易获取，我们可以以Llama 3.1 405B为例。这个Meta的开源模型可以说是迄今为止最“透明”的大型语言模型（LLM）。根据各种[基准测试](https://ai.meta.com/blog/meta-llama-3-1/)，Llama
    3.1 405B与ChatGPT-4相当，为理解这一范围内的LLM提供了一个合理的基础。
- en: Inference
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推理
- en: The hardware requirements to run the 32-bit version of this model range from
    1,620 to 1,944 GB of GPU memory, depending on the source ([substratus](https://www.substratus.ai/blog/llama-3-1-405b-gpu-requirements),
    [HuggingFace](https://huggingface.co/blog/llama31)). For a conservative estimate,
    let’s use the lower figure of 1,620 GB. To put this into perspective — acknowledging
    that this is a simplified analogy — 1,620 GB of GPU memory is roughly equivalent
    to the combined memory of 100 standard MacBook Pros (16GB each). So, when you
    ask one of these LLMs for a tiramisu recipe in Shakespearean style, it takes the
    power of 100 MacBook Pros to give you an answer.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此模型32位版本的硬件要求范围为1,620到1,944 GB的GPU内存，具体取决于来源（[substratus](https://www.substratus.ai/blog/llama-3-1-405b-gpu-requirements),
    [HuggingFace](https://huggingface.co/blog/llama31)）。为了保守估算，我们使用较低的数字1,620 GB。为了更直观地理解——尽管这是一个简化的类比——1,620
    GB的GPU内存大约相当于100台标准的MacBook Pro（每台16GB）的总内存。因此，当你向这些LLM询问莎士比亚风格的提拉米苏食谱时，需要100台MacBook
    Pro的算力才能给出答案。
- en: Training
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练
- en: I’m attempting to translate these figures into something more tangible… though
    this doesn’t include the [training costs](https://www.techtarget.com/searchenterpriseai/news/366596503/Meta-intros-its-biggest-open-source-AI-model-Llama-31-405B#:~:text=Meta%20said%20that%20to%20train,to%20train%20the%20new%20model.),
    which are estimated to involve around 16,000 GPUs at an approximate cost of $60
    million USD (excluding hardware costs) — a significant investment from Meta —
    in a process that took around 80 days. In terms of electricity consumption, [training
    required 11 GWh](https://www.notebookcheck.net/Meta-unveils-biggest-smartest-royalty-free-Llama-3-1-405B-AI.866775.0.html).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我正试图将这些数字转化为更具体的概念……虽然这还不包括[训练成本](https://www.techtarget.com/searchenterpriseai/news/366596503/Meta-intros-its-biggest-open-source-AI-model-Llama-31-405B#:~:text=Meta%20said%20that%20to%20train,to%20train%20the%20new%20model.)，据估计，训练过程涉及大约16,000个GPU，成本约为6000万美元（不包括硬件费用）——这是Meta的一项重大投资——整个过程大约耗时80天。在电力消耗方面，[训练需要11
    GWh](https://www.notebookcheck.net/Meta-unveils-biggest-smartest-royalty-free-Llama-3-1-405B-AI.866775.0.html)。
- en: The [annual electricity consumption per person](https://www.data.gouv.fr/fr/reuses/consommation-par-habitant-et-par-ville-delectricite-en-france/)
    in a country like France is approximately 2,300 kWh. Thus, 11 GWh corresponds
    to the yearly electricity usage of about 4,782 people. This consumption resulted
    in the release of approximately 5,000 tons of CO₂-equivalent greenhouse gases
    ([based on the European average](https://www.econologie.com/europe-emissions-co2-pays-kwh-electrique/)),
    , although this figure can easily double depending on the country where the model
    was trained.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在像法国这样的国家，[每人年电力消耗](https://www.data.gouv.fr/fr/reuses/consommation-par-habitant-et-par-ville-delectricite-en-france/)大约为2300千瓦时。因此，11
    GWh大约相当于4782人一年的电力消耗。这一消耗释放了大约5000吨二氧化碳当量的温室气体（[基于欧洲平均水平](https://www.econologie.com/europe-emissions-co2-pays-kwh-electrique/)），尽管这个数字在不同国家训练模型时可能会轻松翻倍。
- en: For comparison, burning 1 liter of diesel produces 2.54 kg of CO₂. Therefore,
    training Llama 3.1 405B — in a country like France — is roughly equivalent to
    the emissions from burning around 2 million liters of diesel. This translates
    to approximately 28 million kilometers of car travel. I think that provides enough
    perspective… and I haven’t even mentioned the water required to cool the GPUs!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 作为对比，燃烧1升柴油会产生2.54千克的二氧化碳。因此，在像法国这样的国家，训练Llama 3.1 405B的碳排放大约相当于燃烧约200万升柴油。这相当于大约2800万公里的汽车行驶。我认为这已经提供了足够的视角……而且我还没提到用于冷却GPU的水资源！
- en: Sustainability
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可持续性
- en: Clearly, AI is still in its infancy, and we can anticipate more optimal and
    sustainable solutions to emerge over time. However, in this intense race, OpenAI’s
    financial landscape highlights a significant disparity between its revenues and
    operational expenses, particularly in relation to inference costs. In 2024, the
    company is projected to spend approximately $4 billion on processing power provided
    by Microsoft for inference workloads, while its annual revenue is estimated to
    range between $3.5 billion and $4.5 billion. This means that inference costs alone
    nearly match — or even exceed — OpenAI’s total revenue ([deeplearning.ai](https://www.deeplearning.ai/the-batch/openai-faces-financial-growing-pains-spending-double-its-revenue/)).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 很显然，人工智能仍处于起步阶段，我们可以预见，随着时间的推移，将会出现更为优化和可持续的解决方案。然而，在这场激烈的竞争中，OpenAI的财务状况突显了其收入与运营开支之间的巨大差距，尤其是在推理成本方面。预计到2024年，公司将花费大约40亿美元用于由微软提供的推理工作负载处理能力，而其年收入预计在35亿到45亿美元之间。这意味着，仅推理成本几乎就等于——甚至超过——OpenAI的总收入（[deeplearning.ai](https://www.deeplearning.ai/the-batch/openai-faces-financial-growing-pains-spending-double-its-revenue/))。
- en: All of this is happening in a context where experts are announcing a performance
    plateau for AI models (scaling paradigm). Increasing model size and GPUs are yielding
    significantly diminished returns compared to previous leaps, such as the advancements
    GPT-4 achieved over GPT-3\. “The pursuit of AGI has always been unrealistic, and
    the ‘bigger is better’ approach to AI was bound to hit a limit eventually — and
    I think this is what we’re seeing here” said [Sasha Luccioni](https://www.france24.com/en/live-news/20241118-is-ai-s-meteoric-rise-beginning-to-slow),
    researcher and AI lead at startup Hugging Face.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些发生在一个背景下，专家们正在宣布人工智能模型的性能瓶颈（扩展范式）。与之前的飞跃相比，增加模型规模和GPU带来的回报显著减少，例如GPT-4相比于GPT-3取得的进展。Hugging
    Face初创公司的研究员兼AI负责人[Sasha Luccioni](https://www.france24.com/en/live-news/20241118-is-ai-s-meteoric-rise-beginning-to-slow)表示：“追求通用人工智能（AGI）一直是不现实的，‘越大越好’的人工智能方法最终注定会遇到限制——我认为这正是我们现在看到的现象。”
- en: And now?
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 那现在呢？
- en: 'But don’t get me wrong — I’m not putting AI on trial, because I love it! This
    research phase is absolutely a normal stage in the development of AI. However,
    I believe we need to exercise common sense in how we use AI: we can’t use a bazooka
    to kill a mosquito every time. AI must be made sustainable — not only to protect
    our environment but also to address social divides. Indeed, the risk of leaving
    the Global South behind in the AI race due to high costs and resource demands
    would represent a significant failure in this new intelligence revolution..'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 但别误会我的意思——我不是在审判人工智能，因为我爱它！这项研究阶段绝对是人工智能发展中的正常阶段。然而，我认为我们需要在使用人工智能时运用常识：我们不能每次都拿火箭筒去打蚊子。人工智能必须变得可持续——不仅是为了保护我们的环境，还为了应对社会分裂的挑战。的确，由于高昂的成本和资源需求，若将全球南方地区甩在人工智能竞赛的后头，将是这场新智能革命中的一大失败。
- en: So, do you really need the full power of ChatGPT to handle the simplest tasks
    in your RAG pipeline? Are you looking to control your operational costs? Do you
    want complete end-to-end control over your pipeline? Are you concerned about your
    private data circulating on the web? Or perhaps you’re simply mindful of AI’s
    impact and committed to its conscious use?
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你真的需要ChatGPT的全部功能来处理你RAG管道中的最简单任务吗？你是否想控制运营成本？你想完全掌控你的管道流程吗？你是否担心自己的私人数据在网上流通？或者你只是对人工智能的影响保持警觉，并致力于其有意识的使用？
- en: SLM can be a smarter choice!
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 小型语言模型（SLM）可能是一个更聪明的选择！
- en: Small language models (SLMs) offer an excellent alternative worth exploring.
    They can run on your local infrastructure and, when combined with human intelligence,
    deliver substantial value. Although there is no universally agreed definition
    of an SLM — in 2019, for instance, GPT-2 with its 1.5 billion parameters was considered
    an LLM, which is no longer the case — I am referring to models such as Mistral
    7B, Llama-3.2 3B, or Phi3.5, to name a few. These models can operate on a “good”
    computer, resulting in a much smaller carbon footprint while ensuring the confidentiality
    of your data when installed on-premise. Although they are less versatile, when
    used wisely for specific tasks, they can still provide significant value — while
    being more environmentally virtuous.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 小型语言模型（SLM）提供了一个值得探索的绝佳替代方案。它们可以在你的本地基础设施上运行，并且与人类智能结合时，可以提供巨大的价值。虽然SLM没有统一的定义——例如，2019年，GPT-2凭借其15亿参数被视为大型语言模型（LLM），但现在已经不再是这样——我指的是像Mistral
    7B、Llama-3.2 3B或Phi3.5这样的模型。这些模型可以在“一台不错的电脑”上运行，从而大大减少碳足迹，并确保在本地安装时数据的机密性。虽然它们不如大型模型多才多艺，但在特定任务中明智地使用时，仍然可以提供显著的价值——同时在环保方面更具优势。
