- en: Scaling Law Of Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言模型的规模定律
- en: 原文：[https://towardsdatascience.com/scaling-law-of-language-models-5759de7f830c?source=collection_archive---------5-----------------------#2024-07-09](https://towardsdatascience.com/scaling-law-of-language-models-5759de7f830c?source=collection_archive---------5-----------------------#2024-07-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/scaling-law-of-language-models-5759de7f830c?source=collection_archive---------5-----------------------#2024-07-09](https://towardsdatascience.com/scaling-law-of-language-models-5759de7f830c?source=collection_archive---------5-----------------------#2024-07-09)
- en: How language models scale with model size, training data, and training compute
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言模型如何随模型规模、训练数据和训练计算的变化而扩展
- en: '[](https://medium.com/@mina.ghashami?source=post_page---byline--5759de7f830c--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page---byline--5759de7f830c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5759de7f830c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5759de7f830c--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page---byline--5759de7f830c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mina.ghashami?source=post_page---byline--5759de7f830c--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page---byline--5759de7f830c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5759de7f830c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5759de7f830c--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page---byline--5759de7f830c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5759de7f830c--------------------------------)
    ·8 min read·Jul 9, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5759de7f830c--------------------------------)
    ·阅读时间：8分钟·2024年7月9日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/42f834a47c313958efded7b803f5c12d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/42f834a47c313958efded7b803f5c12d.png)'
- en: Scaling law behavior of LLMs— Image from [1]
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 的规模定律行为 — 图片来源：[1]
- en: The world of artificial intelligence is witnessing a revolution, and at its
    forefront are large language models that seem to grow more powerful by the day.
    From BERT to GPT-3 to PaLM, these AI giants are pushing the boundaries of what’s
    possible in natural language processing. But have you ever wondered what fuels
    their meteoric rise in capabilities?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能的世界正见证一场革命，而在这场革命的前沿，是不断增长强大的大型语言模型。从 BERT 到 GPT-3，再到 PaLM，这些人工智能巨头不断突破自然语言处理的边界。但你有没有想过，是什么推动了它们能力的飞速增长？
- en: 'In this post, we’ll embark on a fascinating journey into the heart of language
    model scaling. We’ll uncover the secret sauce that makes these models tick — a
    potent blend of three crucial ingredients: model size, training data, and computational
    power. By understanding how these factors interplay and scale, we’ll gain invaluable
    insights into the past, present, and future of AI language models.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将开始一段令人着迷的旅程，深入了解语言模型规模化的核心。我们将揭示使这些模型得以成功的秘密配方——它是三种关键因素的强力结合：模型规模、训练数据和计算能力。通过理解这些因素如何相互作用和扩展，我们将获得对人工智能语言模型过去、现在和未来的宝贵洞察。
- en: So, let’s dive in and demystify the scaling laws that are propelling language
    models to new heights of performance and capability.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们深入探讨并揭开推动语言模型不断突破性能和能力的新高度的规模定律之谜。
- en: '**Table of content**: This post consists of the following sections:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**目录**：本文包含以下几个部分：'
- en: '**Introduction**'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**介绍**'
- en: Overview of recent language model developments
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最近语言模型发展的概述
- en: Key factors in language model scaling
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言模型规模化的关键因素
