- en: How to Cut RAG Costs by 80% Using Prompt Compression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何通过提示压缩将RAG成本降低80%
- en: 原文：[https://towardsdatascience.com/how-to-cut-rag-costs-by-80-using-prompt-compression-877a07c6bedb?source=collection_archive---------0-----------------------#2024-01-04](https://towardsdatascience.com/how-to-cut-rag-costs-by-80-using-prompt-compression-877a07c6bedb?source=collection_archive---------0-----------------------#2024-01-04)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-cut-rag-costs-by-80-using-prompt-compression-877a07c6bedb?source=collection_archive---------0-----------------------#2024-01-04](https://towardsdatascience.com/how-to-cut-rag-costs-by-80-using-prompt-compression-877a07c6bedb?source=collection_archive---------0-----------------------#2024-01-04)
- en: Accelerating Inference With Prompt Compression
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过提示压缩加速推理
- en: '[](https://medium.com/@brezeanu.iulia?source=post_page---byline--877a07c6bedb--------------------------------)[![Iulia
    Brezeanu](../Images/f108eeec620ec9be40778dfaceca4e6c.png)](https://medium.com/@brezeanu.iulia?source=post_page---byline--877a07c6bedb--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--877a07c6bedb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--877a07c6bedb--------------------------------)
    [Iulia Brezeanu](https://medium.com/@brezeanu.iulia?source=post_page---byline--877a07c6bedb--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@brezeanu.iulia?source=post_page---byline--877a07c6bedb--------------------------------)[![Iulia
    Brezeanu](../Images/f108eeec620ec9be40778dfaceca4e6c.png)](https://medium.com/@brezeanu.iulia?source=post_page---byline--877a07c6bedb--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--877a07c6bedb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--877a07c6bedb--------------------------------)
    [Iulia Brezeanu](https://medium.com/@brezeanu.iulia?source=post_page---byline--877a07c6bedb--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--877a07c6bedb--------------------------------)
    ·11 min read·Jan 4, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--877a07c6bedb--------------------------------)
    ·阅读时间11分钟·2024年1月4日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/dc087a93d65e728e01c7bf6895e134c8.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc087a93d65e728e01c7bf6895e134c8.png)'
- en: Image by the author. AI Generated.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者。AI生成。
- en: The inference process is one of the things that greatly increases the money
    and time costs of using large language models. This problem augments considerably
    for longer inputs. Below, you can see the relationship between model performance
    and inference time.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 推理过程是大规模语言模型使用中显著增加金钱和时间成本的因素之一。对于较长的输入，这一问题尤为突出。以下是模型性能与推理时间之间的关系。
- en: '![](../Images/66a23213977cf798c55cfa2556c09cb6.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66a23213977cf798c55cfa2556c09cb6.png)'
- en: Performance score vs inference throughput [1]
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 性能评分与推理吞吐量[1]
- en: Fast models, which generate more tokens per second, tend to score lower in the
    Open LLM Leaderboard. Scaling up the model size enables better performance but
    comes at the cost of lower inference throughput. This makes it difficult to deploy
    them in real-life applications [1].
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 快速模型每秒生成更多的标记，但在Open LLM排行榜中往往得分较低。扩大模型规模可以提高性能，但代价是推理吞吐量降低。这使得在实际应用中部署这些模型变得困难[1]。
- en: Enhancing LLMs’ speed and reducing resource requirements would allow them to
    be more widely used by individuals or small organizations.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 提高LLM的速度并减少资源需求将使得它们能被个人或小型组织更广泛使用。
- en: Different solutions are proposed for increasing LLM efficiency; some focus on
    the model architecture or system. However, proprietary models like ChatGPT or
    Claude can be accessed only via APIs, so we cannot change their inner algorithm.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 提出了多种提高LLM效率的解决方案；有些专注于模型架构或系统。然而，像ChatGPT或Claude这样的专有模型只能通过API访问，因此我们无法更改其内部算法。
- en: We will discuss a simple and inexpensive method that relies only on changing
    the input given to the model — prompt…
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论一种简单且廉价的方法，该方法仅依赖于改变输入给模型的提示……
