- en: 'Large Language Models, GPT-3: Language Models are Few-Shot Learners'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型，GPT-3：语言模型是少样本学习者
- en: 原文：[https://towardsdatascience.com/large-language-models-gpt-3-language-models-are-few-shot-learners-6e1261a1b466?source=collection_archive---------6-----------------------#2024-02-16](https://towardsdatascience.com/large-language-models-gpt-3-language-models-are-few-shot-learners-6e1261a1b466?source=collection_archive---------6-----------------------#2024-02-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/large-language-models-gpt-3-language-models-are-few-shot-learners-6e1261a1b466?source=collection_archive---------6-----------------------#2024-02-16](https://towardsdatascience.com/large-language-models-gpt-3-language-models-are-few-shot-learners-6e1261a1b466?source=collection_archive---------6-----------------------#2024-02-16)
- en: Efficiently scaling GPT from large to titanic magnitudes within the meta-learning
    framework
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在元学习框架内高效地将 GPT 从大规模扩展到超大规模
- en: '[](https://medium.com/@slavahead?source=post_page---byline--6e1261a1b466--------------------------------)[![Vyacheslav
    Efimov](../Images/441e600862b2b93564c6cd81abb0092d.png)](https://medium.com/@slavahead?source=post_page---byline--6e1261a1b466--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6e1261a1b466--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6e1261a1b466--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page---byline--6e1261a1b466--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@slavahead?source=post_page---byline--6e1261a1b466--------------------------------)[![Vyacheslav
    Efimov](../Images/441e600862b2b93564c6cd81abb0092d.png)](https://medium.com/@slavahead?source=post_page---byline--6e1261a1b466--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6e1261a1b466--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6e1261a1b466--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page---byline--6e1261a1b466--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6e1261a1b466--------------------------------)
    ·8 min read·Feb 16, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: · 发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6e1261a1b466--------------------------------)
    · 阅读时间：8分钟·2024年2月16日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/232803ab4c437443653fc59767adaf6f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/232803ab4c437443653fc59767adaf6f.png)'
- en: Introduction
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: '**GPT** is a family of language models that has been recently gaining a lot
    of popularity. The attention of the Data Science community was rapidly captured
    by the release of GPT-3 in 2020\. After the appearance of GPT-2, almost nobody
    could even assume that nearly in a year there would appear a titanic version of
    GPT containing **175B of parameters**! This is by two orders of magnitude more,
    compared to its predecessor.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPT** 是一系列最近受到广泛关注的语言模型。2020年 GPT-3 的发布迅速吸引了数据科学社区的目光。在 GPT-2 出现后，几乎没人能够预见到，差不多一年后会出现一个包含
    **1750 亿参数** 的超大规模版本！相比其前身，这个模型大了两个数量级。'
- en: 'The enormous capacity of GPT-3 made it possible to use it in various everyday
    scenarios: code completion, article writing, content creation, virtual assistants,
    etc. While the quality of these tasks is not always perfect, the overall progress
    achieved by GPT-3 is absolutely astonishing!'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 的巨大能力使其可以应用于各种日常场景：代码补全、文章写作、内容创作、虚拟助手等。尽管这些任务的质量并不总是完美，但 GPT-3 所取得的整体进展无疑是令人震惊的！
- en: In this article, we will have a detailed look at the main details of GPT-3 and
    useful ideas inspired by GPT-2 creators. Throughout the exploration, we will be
    referring to the [official GPT-3 paper](https://arxiv.org/pdf/2005.14165.pdf).
    It is worth noting that most of the GPT-3 settings including data collection,
    architecture choice and pre-training process are directly derived from GPT-2\.
    That is why most of the time we will be…
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将详细探讨 GPT-3 的主要细节，以及受到 GPT-2 创作者启发的有用想法。在整个探索过程中，我们将参考 [官方的 GPT-3 论文](https://arxiv.org/pdf/2005.14165.pdf)。值得注意的是，包括数据收集、架构选择和预训练过程在内的大部分
    GPT-3 设置，都是直接来自于 GPT-2。因此，大多数时候我们将会…
