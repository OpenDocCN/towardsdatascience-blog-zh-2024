- en: 'Mamba: SSM, Theory, and Implementation in Keras and TensorFlow'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Mamba：SSM、理论及在Keras和TensorFlow中的实现
- en: 原文：[https://towardsdatascience.com/mamba-ssm-theory-and-implementation-in-keras-and-tensorflow-32d6d4b32546?source=collection_archive---------0-----------------------#2024-03-17](https://towardsdatascience.com/mamba-ssm-theory-and-implementation-in-keras-and-tensorflow-32d6d4b32546?source=collection_archive---------0-----------------------#2024-03-17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/mamba-ssm-theory-and-implementation-in-keras-and-tensorflow-32d6d4b32546?source=collection_archive---------0-----------------------#2024-03-17](https://towardsdatascience.com/mamba-ssm-theory-and-implementation-in-keras-and-tensorflow-32d6d4b32546?source=collection_archive---------0-----------------------#2024-03-17)
- en: Understanding how SSMs and Mamba work, along with how to get started with implementing
    it in Keras and TensorFlow.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解SSM和Mamba的工作原理，并学习如何开始在Keras和TensorFlow中实现它。
- en: '[](https://medium.com/@vedantjumle?source=post_page---byline--32d6d4b32546--------------------------------)[![Vedant
    Jumle](../Images/363dbdf8564c35060b3e57cbc6e55f16.png)](https://medium.com/@vedantjumle?source=post_page---byline--32d6d4b32546--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--32d6d4b32546--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--32d6d4b32546--------------------------------)
    [Vedant Jumle](https://medium.com/@vedantjumle?source=post_page---byline--32d6d4b32546--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@vedantjumle?source=post_page---byline--32d6d4b32546--------------------------------)[![Vedant
    Jumle](../Images/363dbdf8564c35060b3e57cbc6e55f16.png)](https://medium.com/@vedantjumle?source=post_page---byline--32d6d4b32546--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--32d6d4b32546--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--32d6d4b32546--------------------------------)
    [Vedant Jumle](https://medium.com/@vedantjumle?source=post_page---byline--32d6d4b32546--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--32d6d4b32546--------------------------------)
    ·13 min read·Mar 17, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--32d6d4b32546--------------------------------)
    ·阅读时间13分钟·2024年3月17日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/2eef43d26c512f0bea0c67ca675348ef.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2eef43d26c512f0bea0c67ca675348ef.png)'
- en: 'Source: AI Generate (SDXL)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：AI生成（SDXL）
- en: 'Submitted on 1st December, 2023 on arXiv, the paper titled [“Mamba: Linear-Time
    Sequence Modeling with Selective State Spaces”](https://arxiv.org/abs/2312.00752)
    proposed an interesting approach to sequence modeling. The authors — [Albert Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+A),
    [Tri Dao](https://arxiv.org/search/cs?searchtype=author&query=Dao%2C+T) — introduced,
    ‘Mamba’ that utilized ‘selective’ [state space models (SSM)](https://en.wikipedia.org/wiki/State-space_representation)
    to achieve results that compete with the performance of the, now ubiquitous, Transformer
    model.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '这篇题为[“Mamba: 线性时间序列建模与选择性状态空间”](https://arxiv.org/abs/2312.00752)的论文于2023年12月1日提交至arXiv，提出了一种有趣的序列建模方法。作者们——[Albert
    Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+A)、[Tri Dao](https://arxiv.org/search/cs?searchtype=author&query=Dao%2C+T)——介绍了“Mamba”方法，该方法利用“选择性”[状态空间模型（SSM）](https://en.wikipedia.org/wiki/State-space_representation)，取得了与当前无处不在的Transformer模型性能相媲美的成果。'
- en: What’s so unique about Mamba?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Mamba有什么独特之处？
- en: Transformers have seen recent popularity with the rise of Large Language Models
    (LLMs) like LLaMa-2, GPT-4, Claude, Gemini, etc., but it suffers from the problem
    of context window. The issue with transformers lies in it’s core, the multi head-attention
    mechanism.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大规模语言模型（LLMs）如LLaMa-2、GPT-4、Claude、Gemini等的崛起，Transformer近年来变得非常流行，但它也面临上下文窗口问题。Transformer的问题根源在于其核心的多头注意力机制。
- en: '*The main issue with multi-head attention sprouts from the fact that for input
    sequence length n, the time complexity and space complexity scales by O(n²). This
    limits the length of the context window of an LLM. Because, to increase it by
    10x, we need to scale the hardware requirement (most notably GPU VRAM) by 100x.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*多头注意力的主要问题源于这样一个事实：对于输入序列长度n，时间复杂度和空间复杂度按O(n²)比例增长。这限制了LLM的上下文窗口长度。因为，要将其增加10倍，我们需要将硬件要求（尤其是GPU
    VRAM）增加100倍。*'
- en: Mamba, on the other hand, scales by ***O(n)!, i.e., Linearly****.*
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Mamba的扩展按***O(n)!, 即线性***比例增长。
- en: '![](../Images/b097aae23543115f103cea34510def5e.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b097aae23543115f103cea34510def5e.png)'
- en: Plot taken from the Mamba paper comparing FlashAttention and Mamba approach
    (indicated by scan(ours) in the legends)[1]
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 该图摘自Mamba论文，比较了FlashAttention和Mamba方法（在图例中由scan(ours)标出）[1]
- en: This linear scaling is what has taken wind for researchers to speculate that
    Mamba might be the future of sequence modeling.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这种线性缩放已引发研究人员的猜测，即Mamba可能是序列建模的未来。
- en: 'The backbone of Mamba: State Space Models'
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Mamba的核心：状态空间模型
- en: The core of the Mamba model comes from the concept of State Space Models. ***State
    Space Models, like Transformers and RNN, process sequences of information, like
    text, audio signals, video frames, DNA sequences, etc.***
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Mamba模型的核心来自于状态空间模型的概念。***状态空间模型，如Transformers和RNN，处理信息序列，如文本、音频信号、视频帧、DNA序列等。***
- en: 'State Space Models come from an idea of describing a physical system as a set
    of input, outputs, and variables. These variables are: *A, B, C, D.* The process
    of SSM involves calculation of an *internal state vector h(t), given an input
    x(t).* Then, we do a weighted sum of *h(t)* and *x(t)* where the weights are *A,
    B, C, & D*. In the simplest form (continuous time-invariant), the process formulation
    looks like:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 状态空间模型来源于将物理系统描述为一组输入、输出和变量的思想。这些变量包括：*A、B、C、D*。SSM的过程涉及计算给定输入x(t)的*内部状态向量h(t)*。然后，我们对*h(t)*和*x(t)*进行加权求和，其中权重为*A、B、C和D*。在最简单的形式（连续时间不变）下，过程公式如下：
- en: '![](../Images/ebbbac8cb1eb8abaa8dbb62853c93240.png)![](../Images/047a1d371405fee92bb7ea7c58305c58.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ebbbac8cb1eb8abaa8dbb62853c93240.png)![](../Images/047a1d371405fee92bb7ea7c58305c58.png)'
- en: 'source: wikipedia[6]'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：wikipedia[6]
- en: '*h(t)* is often called the ‘hidden’ or the ‘latent’ state, I will be sticking
    to calling it the ‘hidden’ state for better clarity. **It is important to note
    that A, B, C, and D are learnt parameters in SSM.**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*h(t)*通常被称为“隐藏”状态或“潜在”状态，为了更清晰起见，我将坚持称其为“隐藏”状态。**重要的是要注意，A、B、C和D是SSM中的学习参数。**'
- en: What are the variables?
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变量是什么？
- en: '**The variables, A, B, C & D, are learnt parameters,** and they can be described
    as:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**变量A、B、C和D是学习得到的参数，** 它们可以描述为：'
- en: 'A: How much should the previous hidden state (h) be considered to calculate
    the new hidden state'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: A：在计算新的隐藏状态时，应该考虑多少前一个隐藏状态（h）。
- en: 'B: How much should the input (x) be consider to calculate the new hidden state.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: B：在计算新的隐藏状态时，输入（x）应考虑多少。
- en: 'C: How much should the new hidden state be considered in calculating the output
    (y).'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: C：在计算输出（y）时，新的隐藏状态应考虑多少。
- en: 'D: How much should the input (x) be consider in calculating the output (y).'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: D：在计算输出（y）时，输入（x）应考虑多少。
- en: D comes in the end of the computations and does not affect how the hidden state
    is calculated. Hence, it is usually considered outside of ssm, and can be thought
    of as a skip connection.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: D出现在计算的最后，并且不影响如何计算隐藏状态。因此，通常认为它在SSM之外，并且可以看作是一个跳跃连接。
- en: Going from continuous spaces to discrete spaces
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从连续空间到离散空间的转变
- en: The above formulation applies to a system where the input and output belong
    to a continuous space. But in cases, like language modeling, where the input and
    output belong to discrete spaces (token values in a vocabulary). Also, finding
    *h(t)* is analytically challenging. This can be achieved by performing a ***Zero-order
    hold***.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 上述公式适用于输入和输出属于连续空间的系统。但在诸如语言建模等情况中，输入和输出属于离散空间（词汇表中的标记值）。此外，求解*h(t)*在解析上具有挑战性。这可以通过执行***零阶保持***来实现。
- en: In a zero-order hold, every time an input is received, the model holds its value
    till the next input is received. This leads to a continuous input space.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在零阶保持中，每次接收到输入时，模型会保持其值直到接收到下一个输入。这导致了一个连续的输入空间。
- en: '![](../Images/236f0dcd1ee1294b2e8064560a582f29.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/236f0dcd1ee1294b2e8064560a582f29.png)'
- en: How Zero order hold works
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 零阶保持如何工作
- en: This length of ‘hold’ is determined by a new parameter called, *step size* ***∆.
    It can be thought of as the resolution of the input.*** Ideally, ∆ should be infinitesimal.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这个“保持”长度由一个新的参数决定，称为*步长* ***∆。它可以被视为输入的分辨率。*** 理想情况下，∆应为无穷小。
- en: 'Mathematically, Zero-order hold can be described as:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，零阶保持可以描述为：
- en: '![](../Images/3f6f0258965a22d4970d1e76a3cf457a.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f6f0258965a22d4970d1e76a3cf457a.png)'
- en: 'Finally, we can create a discrete SSM, as:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以创建一个离散的SSM，如下所示：
- en: '![](../Images/dc4b655375a46584a07aed40155d829d.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc4b655375a46584a07aed40155d829d.png)'
- en: 'Since, D is used with a skip connection outside of SSM, the output can be reduced
    to:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 由于D与SSM外的跳跃连接一起使用，输出可以简化为：
- en: '![](../Images/de141e5ed6870e82fe9f71fe77beb074.png)![](../Images/71c4d16a62a57ecd5732def8f4b79143.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de141e5ed6870e82fe9f71fe77beb074.png)![](../Images/71c4d16a62a57ecd5732def8f4b79143.png)'
- en: Involvement of DX(t) is considered as a skip connection, hence is goes from
    outside of SSM
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: DX(t) 的涉及被视为跳跃连接，因此它来自 SSM 之外。
- en: SSM and recurrence
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SSM 和递归
- en: In SSMs, the hidden state is carried over to when the next input is received.
    This is similar to how Recurrent Neural Networks function.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SSM 中，隐藏状态会被传递到接收到下一个输入时。这类似于递归神经网络的工作方式。
- en: '![](../Images/96f22333a7a3f5bd2ba29b117eef421f.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96f22333a7a3f5bd2ba29b117eef421f.png)'
- en: Comparison of RNN and SSM
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 和 SSM 的比较
- en: This recurrent format of SSM can be unwrapped, just like RNNs. But unlike RNNs,
    which are iterative and slow, SSM can process the input sequence in parallel (just
    like transformers) and this makes the training processes faster.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这种 SSM 的递归格式可以像 RNN 一样展开。但与迭代且缓慢的 RNN 不同，SSM 可以并行处理输入序列（就像 Transformers），这使得训练过程更快。
- en: '![](../Images/22282dee4daeabed27c4669ba7ea486f.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22282dee4daeabed27c4669ba7ea486f.png)'
- en: Unrolled form of SSM
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: SSM 的展开形式
- en: Note that ‘D’ is used in a skip connection, which is outside of SSM.
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意，‘D’ 在跳跃连接中使用，它位于 SSM 之外。
- en: The key insight in how SSM make training fast is to use the variables *A, B,
    C* in a pre-computed convolutional kernel. [Maarten Grootendorst](https://maartengrootendorst.substack.com/i/141228095/the-convolution-representation)
    wrote a really good explanation on how this canonical ‘convolutional’ kernel is
    constructed. But here’s a simple mathematical explanation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: SSM 加速训练的关键见解是使用预先计算的卷积核中的变量 *A, B, C*。[Maarten Grootendorst](https://maartengrootendorst.substack.com/i/141228095/the-convolution-representation)
    写了一篇非常好的解释，讲述了如何构造这个标准的‘卷积’核。但这里有一个简单的数学解释。
- en: 'Consider the output *y.* For a sequence length of *k*, the output for *y(k)*
    will be represented ***(assuming h0 = zero)***:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑输出 *y.* 对于序列长度 *k*，输出 *y(k)* 将表示为***(假设 h0 = 零)***：
- en: '![](../Images/16c7dd559c5978d453467fe139274186.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/16c7dd559c5978d453467fe139274186.png)'
- en: 'Similarly, y3 can be represented as:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，y3 可以表示为：
- en: '![](../Images/b90cee1adec4657a3ffa7f67760b3abb.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b90cee1adec4657a3ffa7f67760b3abb.png)'
- en: 'Extrapolating the pattern, yk can be represented as:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 推断出模式，yk 可以表示为：
- en: '![](../Images/9f7775b6a3bce55150dd393d1d9e44da.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f7775b6a3bce55150dd393d1d9e44da.png)'
- en: 'This formulation can be further reduced to:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 该公式可以进一步简化为：
- en: '![](../Images/9d85bda2ef8d6da968ed73b8e48ace03.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d85bda2ef8d6da968ed73b8e48ace03.png)'
- en: The funny looking multiplication symbol represents a convolution operation,
    where the convolution kernel is K. Notice that K is not dependent on *x,* hence
    K can be pre-computed into a convolutional kernel, which makes the process faster.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 那个看起来很奇怪的乘法符号表示卷积操作，其中卷积核是 K。注意到 K 与 *x* 无关，因此 K 可以预先计算为卷积核，这使得处理过程更快。
- en: Mamba and ‘Selective’ SSM
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Mamba 和“选择性”SSM
- en: As good as the computational capacity of SSM sounds, it turns out to be pretty
    *meh* in metrics like accuracy compared to Transformers.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 SSM 的计算能力听起来很强大，但与 Transformers 相比，它在精度等度量指标上却相当 *一般*。
- en: '![](../Images/0e7755efdbec8214342ce450c6900185.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e7755efdbec8214342ce450c6900185.png)'
- en: The core issue lies with the variables, ∆, A, B, & C. Turns out that since we
    apply the same matrices to every input, they cannot really process the context
    of the sequence.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 核心问题出在变量 ∆、A、B 和 C 上。事实证明，由于我们将相同的矩阵应用于每个输入，它们实际上无法处理序列的上下文。
- en: SSMs are inflexible in the way they process data[4]
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: SSM 在处理数据时缺乏灵活性[4]
- en: So what’s so special about Mamba? In mamba, we use a process called ‘selective’
    SSM, where the variables, ∆, B, & C, are computed based on the input. 🤔. We do
    this by passing the current input through Linear layers, and take the output to
    be the ∆, B, & C.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 那么 Mamba 有什么特别之处？在 Mamba 中，我们使用一种叫做“选择性”SSM 的过程，其中变量 ∆、B 和 C 是根据输入计算出来的。🤔。我们通过将当前输入传递通过线性层，并将输出作为
    ∆、B 和 C。
- en: '![](../Images/d789181938ab7269cbd528b6982e3263.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d789181938ab7269cbd528b6982e3263.png)'
- en: But then this makes ∆, B, & C input dependent, hence meaning that they cannot
    be pre-computed 😢, fast convolution isn’t going to work here. But, the authors
    discuss a method, which is based on *parallel associative scan.*
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 但这使得 ∆、B 和 C 变得依赖于输入，因此它们不能预先计算 😢，快速卷积在这里行不通。但是，作者讨论了一种基于 *并行关联扫描* 的方法。
- en: Parallel Associative Scan
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并行关联扫描
- en: Parallel associative scan is a powerful technique used in parallel computing
    to perform a prefix sum operation, which is a cumulative operation on a sequence
    of numbers. This operation is “associative”, meaning the way numbers are grouped
    in the operation doesn’t change the result.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 并行关联扫描是一种在并行计算中使用的强大技术，用于执行前缀和操作，这是一种对数字序列进行累积的操作。这个操作是“关联的”，意味着数字在操作中的分组方式不会改变结果。
- en: '![](../Images/dbbd700bf44fb468c40690bafbc4c22c.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dbbd700bf44fb468c40690bafbc4c22c.png)'
- en: 'Parallel prefix sum is an example of associative scanning. (source: Nvidia)[7]'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 并行前缀和是关联扫描的一个例子。（来源：Nvidia）[7]
- en: In the context of the Mamba model, by defining an associative operator, elements
    and associative operators for a parallel associative scan operation are obtained.
    This allows for solving problems on the whole time interval in parallel, resulting
    in logarithmic time complexity in the number of sub-intervals.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Mamba 模型的上下文中，通过定义一个关联操作符，可以获得并行关联扫描操作的元素和关联操作符。这使得可以并行解决整个时间区间的问题，从而使子区间的数量的时间复杂度达到对数级别。
- en: Hardware aware algorithm
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 硬件感知算法
- en: 'Along with associative scan, the authors also propose a hardware aware algorithm,
    where they use the quirks within Nvidia GPUs related to the speed of HBM and SRAM.
    They argue that the computation of SSM states can be sped up by:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 除了关联扫描外，作者还提出了一种硬件感知算法，在该算法中，他们利用了与 Nvidia GPU 中 HBM 和 SRAM 速度相关的特性。他们认为，通过以下方式可以加速
    SSM 状态的计算：
- en: keeping the hidden state and A in the faster but less capacity ***SRAM*,**
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保持隐藏状态和 A 在速度较快但容量较小的 ***SRAM*** 中，
- en: while computing ∆, B, & C, in the slower but larger capacity ***HBM***.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在速度较慢但容量较大的 ***HBM*** 中计算 ∆、B 和 C。
- en: They then transfer ∆, B, & C to the ***SRAM***, compute the new hidden state
    within ***SRAM***.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，他们将 ∆、B 和 C 转移到 ***SRAM*** 中，在 ***SRAM*** 内部计算新的隐藏状态。
- en: And then write ∆, B & C back to ***HBM***.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后将 ∆、B 和 C 写回到 ***HBM***。
- en: '![](../Images/daa8208bf094ec66ab66110f20dd51a1.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/daa8208bf094ec66ab66110f20dd51a1.png)'
- en: Illustration taken from the Mamba paper, it shows how the hardware aware algorithm
    works[1]
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图示来自 Mamba 论文，展示了硬件感知算法如何工作[1]
- en: In the implementation section, I will not be discussing on how to work with
    the hardware aware algorithm, rather I will be only using parallel associative
    scan.
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在实现部分，我不会讨论如何使用硬件感知算法，而是只会使用并行关联扫描。
- en: Final Mamba architecture
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终的 Mamba 架构
- en: With all of this in mind, let’s explore and implement the Mamba architecture
    using Keras and TensorFlow.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些，我们可以使用 Keras 和 TensorFlow 探索并实现 Mamba 架构。
- en: 'The Mamba architecture, after reading the paper and analysis of the code, can
    be broken into a few key components which are connected as:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读论文并分析代码后，Mamba 架构可以分解为几个关键组件，这些组件连接如下：
- en: '![](../Images/e663ed4dbf34650f50802a442192d16d.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e663ed4dbf34650f50802a442192d16d.png)'
- en: Breakdown of a mamba block
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Mamba 块的拆解
- en: The Mamba architecture consists of multiple stacked layers of ‘Mamba blocks’.
    Which, judging from the above illustration, consists of quite a few components.
    Another important thing to note is that the authors add the output from Selective
    SSM to the original input and then apply a *normalization* layer to it. This normalization
    can be either a Layer normalization or an [RMS normalization](https://arxiv.org/abs/1910.07467).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Mamba 架构由多个堆叠的“Mamba 块”组成。从上面的图示来看，它由相当多的组件组成。另一个需要注意的重要事项是，作者将来自选择性 SSM 的输出添加到原始输入中，然后对其应用
    *归一化* 层。此归一化可以是层归一化，或者是 [RMS 归一化](https://arxiv.org/abs/1910.07467)。
- en: TensorFlow and Keras implementation
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 和 Keras 实现
- en: 'Lets start with coding part of Mamba. We will using the following dependencies:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 Mamba 的编码部分开始。我们将使用以下依赖项：
- en: '[PRE0]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Imports:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 导入：
- en: '[PRE1]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: To make the modeling argument processing easier, let’s create a simple *ModelArgs*
    dataclass as a config class. This allows us to just pass the dataclass variable
    in the arguments when we are initializing the model.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化模型参数的处理，我们可以创建一个简单的 *ModelArgs* 数据类作为配置类。这样，在初始化模型时，我们只需将数据类变量作为参数传递即可。
- en: '[PRE2]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Load the *bert-base-uncased* tokenizer:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 加载 *bert-base-uncased* 分词器：
- en: '[PRE3]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Before we implement our Mamba and SSM classes, we need to implement the parallel
    associative scan, the code looks like this:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们实现 Mamba 和 SSM 类之前，我们需要实现并行关联扫描，代码如下：
- en: '[PRE4]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'With this, we can implement the MambaBlock:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样，我们可以实现 MambaBlock：
- en: '[PRE5]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Finally, a residual block to implement the external skip connection.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，实现外部跳跃连接的残差块。
- en: '[PRE6]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: With this, we can initialize our model. In this example, I will be demonstrating
    how to use the Mamba block to create a simple classification model, but it can
    be easily modified to become a language model. Let’s load the *IMDB reviews dataset*
    for a simple sentiment classifier.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，我们可以初始化我们的模型。在这个例子中，我将演示如何使用Mamba模块创建一个简单的分类模型，但它可以很容易地修改为语言模型。让我们加载*IMDB评论数据集*来进行一个简单的情感分类器。
- en: '[PRE7]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: First we create a function that will take the model args and return a model.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 首先我们创建一个函数，该函数将接收模型参数并返回一个模型。
- en: '[PRE8]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now we can initialize our model, and summarize it:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以初始化我们的模型，并对其进行总结：
- en: '[PRE9]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'For easier processing, lets pre-tokenize our data into a *numpy arrays*, then
    convert them into tf.data.Dataset objects:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更方便地处理，让我们先将数据进行预处理成*numpy数组*，然后再转换为tf.data.Dataset对象：
- en: '[PRE11]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now the model can be trained:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型可以进行训练：
- en: '[PRE12]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You can play around with the inference algorithm:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试推理算法：
- en: '[PRE13]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This model can be converted into a language model and algorithms like *beam
    search, top-k sampling, greedy sampling, etc.* can be used to generate language.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型可以转化为语言模型，并且可以使用像*束搜索、top-k采样、贪心采样*等算法来生成语言。
- en: This code can be found on my [Github](https://github.com/maxDeCoder/Mamba-tf).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码可以在我的[Github](https://github.com/maxDeCoder/Mamba-tf)上找到。
- en: A lot of the code is inspired from the mamba’s official implementation[2] and
    another pytorch implementation called ‘mamba-tiny’[3]
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 很多代码灵感来源于mamba的官方实现[2]以及另一个名为‘mamba-tiny’的pytorch实现[3]
- en: Thank you for reading.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读。
- en: Unless otherwise noted, all images are made by me.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图片均由我制作。
- en: 'References:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 参考资料：
- en: '[Mamba paper.](https://arxiv.org/abs/2312.00752)'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Mamba论文](https://arxiv.org/abs/2312.00752)'
- en: '[Mamba original repository](https://github.com/state-spaces/mamba)'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Mamba原始仓库](https://github.com/state-spaces/mamba)'
- en: '[A simpler Torch implementation of Mamba: mamba-tiny](https://github.com/PeaBrane/mamba-tiny)'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Mamba的一个更简单的Torch实现：mamba-tiny](https://github.com/PeaBrane/mamba-tiny)'
- en: '[A simple explanation by Letitia on YouTube.](https://youtu.be/vrF3MtGwD0Y?si=st2Oipq3fli9tGhl)'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Letitia在YouTube上的简单解释](https://youtu.be/vrF3MtGwD0Y?si=st2Oipq3fli9tGhl)'
- en: '[Maarten Grootendorst’s article on SSMs and Mamba](https://maartengrootendorst.substack.com/p/a-visual-guide-to-mamba-and-state)'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Maarten Grootendorst关于SSM和Mamba的文章](https://maartengrootendorst.substack.com/p/a-visual-guide-to-mamba-and-state)'
- en: '[SSMs on wikipedia](https://en.wikipedia.org/wiki/SSM)'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[维基百科上的SSM](https://en.wikipedia.org/wiki/SSM)'
- en: '[Nvidia’s tutorial on Parallel associative scan](https://developer.nvidia.com/gpugems/gpugems3/part-vi-gpu-computing/chapter-39-parallel-prefix-sum-scan-cuda)'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Nvidia关于并行关联扫描的教程](https://developer.nvidia.com/gpugems/gpugems3/part-vi-gpu-computing/chapter-39-parallel-prefix-sum-scan-cuda)'
- en: Want to connect? Please write to me at [vedantjumle@gmail.com](mailto:vedantjumle@gmail.com)
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 想要联系我吗？请通过[vedantjumle@gmail.com](mailto:vedantjumle@gmail.com)给我写信。
