- en: 'From Local to Cloud: Estimating GPU Resources for Open-Source LLMs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从本地到云端：估算开源LLM所需的GPU资源
- en: 原文：[https://towardsdatascience.com/from-local-to-cloud-estimating-gpu-resources-for-open-source-llms-b4a015a0174f?source=collection_archive---------4-----------------------#2024-11-18](https://towardsdatascience.com/from-local-to-cloud-estimating-gpu-resources-for-open-source-llms-b4a015a0174f?source=collection_archive---------4-----------------------#2024-11-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/from-local-to-cloud-estimating-gpu-resources-for-open-source-llms-b4a015a0174f?source=collection_archive---------4-----------------------#2024-11-18](https://towardsdatascience.com/from-local-to-cloud-estimating-gpu-resources-for-open-source-llms-b4a015a0174f?source=collection_archive---------4-----------------------#2024-11-18)
- en: '*Estimating GPU memory for deploying the latest open-source LLMs*'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*估算部署最新开源LLM所需的GPU内存*'
- en: '[](https://medium.com/@maximejabarian?source=post_page---byline--b4a015a0174f--------------------------------)[![Maxime
    Jabarian](../Images/d6c2198e2e3259ae98b5bbe0e3079768.png)](https://medium.com/@maximejabarian?source=post_page---byline--b4a015a0174f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b4a015a0174f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b4a015a0174f--------------------------------)
    [Maxime Jabarian](https://medium.com/@maximejabarian?source=post_page---byline--b4a015a0174f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@maximejabarian?source=post_page---byline--b4a015a0174f--------------------------------)[![Maxime
    Jabarian](../Images/d6c2198e2e3259ae98b5bbe0e3079768.png)](https://medium.com/@maximejabarian?source=post_page---byline--b4a015a0174f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b4a015a0174f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b4a015a0174f--------------------------------)
    [Maxime Jabarian](https://medium.com/@maximejabarian?source=post_page---byline--b4a015a0174f--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b4a015a0174f--------------------------------)
    ·4 min read·Nov 18, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b4a015a0174f--------------------------------)
    ·阅读时间 4分钟·2024年11月18日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/bcea4003ded428dc0835ca4f0ec07375.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bcea4003ded428dc0835ca4f0ec07375.png)'
- en: '[Source](https://unsplash.com/fr/photos/gros-plan-dune-carte-video-sur-fond-jaune-ipVMl4H6g6o)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://unsplash.com/fr/photos/gros-plan-dune-carte-video-sur-fond-jaune-ipVMl4H6g6o)'
- en: 'If you’re like me, you probably get excited about the latest and greatest open-source
    LLMs — from models like Llama 3 to the more compact Phi-3 Mini. But before you
    jump into deploying your language model, there’s one crucial factor you need to
    plan for: **GPU memory**. Misjudge this, and your shiny new web app might choke,
    run sluggishly, or rack up hefty cloud bills. To make things easier, I explain
    to you what’s quantization, and I’ve prepared for you a ***GPU Memory Planning
    Cheat Sheet in 2024***— a handy summary of the latest open-source LLMs on the
    market and what you need to know before deployment.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你像我一样，可能会对最新和最强大的开源大语言模型（LLM）感到兴奋——从像Llama 3这样的模型到更紧凑的Phi-3 Mini。但在你跳进部署语言模型之前，有一个关键因素需要提前规划：**GPU内存**。如果判断失误，你闪亮的新网页应用可能会崩溃、运行缓慢，或者产生高额的云计算费用。为了简化这一过程，我将为你解释什么是量化，并准备了一份***2024年GPU内存规划备忘单***——这是市场上最新开源LLM的简明总结，以及部署前需要了解的事项。
- en: If you are not a member, [**read here**](https://medium.com/towards-data-science/from-local-to-cloud-estimating-gpu-resources-for-open-source-llms-b4a015a0174f)**.**
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不是会员，[**请阅读此处**](https://medium.com/towards-data-science/from-local-to-cloud-estimating-gpu-resources-for-open-source-llms-b4a015a0174f)**。**
- en: '**Why Bother Estimating GPU Memory?**'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**为什么要估算GPU内存？**'
- en: When deploying LLMs, guessing how much GPU memory you need is risky. Too little,
    and your model crashes. Too much, and you’re burning money for no reason.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署LLM时，估算需要多少GPU内存是有风险的。内存过少，模型可能崩溃；内存过多，则是在浪费资源，导致不必要的开销。
- en: Understanding these memory requirements upfront is like knowing how much luggage
    you can fit in your car before a road trip — it saves headaches and keeps things
    efficient.
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 预先了解这些内存需求，就像在公路旅行之前知道你能将多少行李塞进车里——这可以避免头疼问题并提高效率。
