- en: 'Hands-On Imitation Learning: From Behavior Cloning to Multi-Modal Imitation
    Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践模仿学习：从行为克隆到多模态模仿学习
- en: 原文：[https://towardsdatascience.com/hands-on-imitation-learning-from-behavior-cloning-to-multi-modal-imitation-learning-11ec0d37f4a2?source=collection_archive---------10-----------------------#2024-09-12](https://towardsdatascience.com/hands-on-imitation-learning-from-behavior-cloning-to-multi-modal-imitation-learning-11ec0d37f4a2?source=collection_archive---------10-----------------------#2024-09-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/hands-on-imitation-learning-from-behavior-cloning-to-multi-modal-imitation-learning-11ec0d37f4a2?source=collection_archive---------10-----------------------#2024-09-12](https://towardsdatascience.com/hands-on-imitation-learning-from-behavior-cloning-to-multi-modal-imitation-learning-11ec0d37f4a2?source=collection_archive---------10-----------------------#2024-09-12)
- en: '***An overview of the most prominent methods in imitation learning while testing
    on a grid environment***'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '***对模仿学习中最突出的几种方法进行概述，并在网格环境中进行测试***'
- en: '[](https://medium.com/@mryasinusif?source=post_page---byline--11ec0d37f4a2--------------------------------)[![Yasin
    Yousif](../Images/9c702c021e7e5285bddefe76a144a3e1.png)](https://medium.com/@mryasinusif?source=post_page---byline--11ec0d37f4a2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--11ec0d37f4a2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--11ec0d37f4a2--------------------------------)
    [Yasin Yousif](https://medium.com/@mryasinusif?source=post_page---byline--11ec0d37f4a2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mryasinusif?source=post_page---byline--11ec0d37f4a2--------------------------------)[![Yasin
    Yousif](../Images/9c702c021e7e5285bddefe76a144a3e1.png)](https://medium.com/@mryasinusif?source=post_page---byline--11ec0d37f4a2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--11ec0d37f4a2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--11ec0d37f4a2--------------------------------)
    [Yasin Yousif](https://medium.com/@mryasinusif?source=post_page---byline--11ec0d37f4a2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--11ec0d37f4a2--------------------------------)
    ·14 min read·Sep 12, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--11ec0d37f4a2--------------------------------)
    ·14分钟阅读·2024年9月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/120c495851c2f0539761925dddc91cbf.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/120c495851c2f0539761925dddc91cbf.png)'
- en: Photo by [Possessed Photography](https://unsplash.com/@possessedphotography?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Possessed Photography](https://unsplash.com/@possessedphotography?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Reinforcement learning is one branch of machine learning concerned with learning
    by guidance of scalar signals (rewards); in contrast to supervised learning, which
    needs full labels of the target variable.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是机器学习的一个分支，关注通过标量信号（奖励）的指导进行学习；与需要目标变量完整标签的监督学习不同。
- en: 'An intuitive example to explain reinforcement learning can be given in terms
    of a school with two classes having two types of tests repeated continuously.
    The first class solves the test and gets the full correct answers (supervised
    learning: SL). The second class solves the test and gets only the grades for each
    question (reinforcement learning: RL). In the first case, it seems easier for
    the students to learn the correct answers and memorize them. In the second class,
    the task is harder because they can learn only by trial and error. However, their
    learning will be more robust because they don’t only know what is right but also
    all the wrong answers to avoid.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过一个直观的例子来解释强化学习，例子描述的是一所学校的两门课，每门课有两种类型的测试，且测试不断重复进行。第一班解答测试并得到了所有正确答案（监督学习：SL）。第二班解答测试并且每个问题只得到了分数（强化学习：RL）。在第一种情况下，学生们似乎更容易学习正确答案并记住它们。在第二班中，任务更困难，因为他们只能通过反复试验来学习。然而，他们的学习将更加稳健，因为他们不仅知道什么是对的，还知道所有错误的答案，从而避免它们。
- en: In order to learn efficiently with RL, an accurate reward signal (the grades)
    should be designed, which is considered a difficult task, especially for real-world
    applications. For example, a human expert driver knows how to drive, but cannot
    set rewards for ‘correct driving’ skill, same thing for cooking or painting. This
    created the need for imitation learning methods (IL). IL is a new branch of RL
    concerned with learning from mere expert trajectories, without knowing the rewards.
    Main application areas of IL are in robotics and autonomous driving fields.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在强化学习（RL）中高效学习，应该设计一个准确的奖励信号（评分），这被认为是一个困难的任务，尤其是在现实世界的应用中。例如，一个人类专家驾驶员知道如何开车，但无法为“正确驾驶”技能设置奖励，做饭或绘画也是如此。这就产生了对模仿学习方法（IL）的需求。IL是强化学习的一个新分支，专注于从纯粹的专家轨迹中学习，而不需要知道奖励。IL的主要应用领域是在机器人技术和自动驾驶领域。
- en: In the following, we will explore the most famous methods of IL in the literature,
    ordered by their proposal time from old to new, as shown in the timeline picture
    below.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探索文献中最著名的IL方法，按提议时间从旧到新排序，如下图所示的时间线。
- en: '![](../Images/052ad6e337c649a08caa6e76fb3ecd9a.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/052ad6e337c649a08caa6e76fb3ecd9a.png)'
- en: Timeline of IL methods
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: IL方法的时间线
- en: The mathematical formulations will be shown along with nomenclature of the symbols.
    However, the theoretical derivation is kept to a minimum here; if further depth
    is needed, the original references can be looked up as cited in the references
    section at the end. The full code for recreating all the experiments is provided
    in the accompanying [github repo](https://www.github.com/engyasin/ilsurvey).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 数学公式将与符号的命名一同展示。然而，理论推导在这里保持到最小；如果需要进一步的深度，可以查阅引用部分列出的原始文献。重现所有实验的完整代码已提供在随附的[github
    仓库](https://www.github.com/engyasin/ilsurvey)中。
- en: So, buckle up! and let’s dive through imitation learning, from behavior cloning
    (BC) to information maximization generative adversarial imitation learning (InfoGAIL).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，系好安全带！让我们通过模仿学习深入探索，从行为克隆（BC）到信息最大化生成对抗模仿学习（InfoGAIL）。
- en: Example Environment
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例环境
- en: 'The environment used in this post is represented as a 15x15 grid. The environment
    state is illustrated below:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本文使用的环境表示为一个15x15的网格。环境状态如下所示：
- en: 'Agent: red color'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理：红色
- en: 'Initial agent location: blue color'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始代理位置：蓝色
- en: 'Walls: green color'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 墙壁：绿色
- en: '![](../Images/417a0103d83622ac19add6a8c7479bf8.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/417a0103d83622ac19add6a8c7479bf8.png)'
- en: The goal of the agent is to reach the first row in the shortest possible way
    through any of the three windows and towards a symmetrical location to its initial
    position with respect to the vertical axis passing through the middle of the grid.
    The goal location will not be shown in the state grid.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的目标是通过任何三个窗口，以最短的方式到达第一行，并使其位于相对于通过网格中心的垂直轴的初始位置的对称位置。目标位置不会在状态网格中显示。
- en: So the initial position has 15 possibilities only, and the goal location is
    changed based on that.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，初始位置只有15种可能性，目标位置会基于此发生变化。
- en: Action Space
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动作空间
- en: 'The action space *A* consists of a discrete number from 0 to 4 representing
    movements in four directions and the stopping action, as illustrated below:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 动作空间 *A* 包含从0到4的离散数值，表示四个方向的移动和停止动作，如下所示：
- en: '![](../Images/7a40b44e70067c461e30172c23fe1768.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a40b44e70067c461e30172c23fe1768.png)'
- en: Reward Function
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奖励函数
- en: 'The ground truth reward here *R*(*s*,*a*) is a function of the current state
    and action, with a value equal to the displacement distance towards the goal:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的真实奖励 *R*(*s*,*a*) 是当前状态和动作的函数，其值等于朝目标移动的位移距离：
- en: where 𝑝1​ is the old position and *p*2​ is the new position. The agent will
    always be initialized at the last row, but in a random position each time.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，𝑝1​是旧位置，*p*2​是新位置。代理总是从最后一行初始化，但每次位置都是随机的。
- en: Expert Policy
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 专家策略
- en: 'The expert policy used for all methods (except InfoGAIL) aims to reach the
    goal in the shortest possible path. This involves three steps:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 所有方法（除InfoGAIL外）使用的专家策略旨在以最短的路径到达目标。这涉及三个步骤：
- en: Moving towards the nearest window
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 朝最近的窗口移动
- en: Moving directly towards the goal
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直接朝目标移动
- en: Stopping at the goal location
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 停止在目标位置
- en: 'This behavior is illustrated by a GIF:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 该行为由一个GIF演示：
- en: '![](../Images/1629324006841fb80098705a78561bd6.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1629324006841fb80098705a78561bd6.png)'
- en: The expert policy generates demonstration trajectories used by other IL methods.
    Each trajectory *τ* is represented as an ordered sequence of state-action tuples.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 专家策略生成用于其他强化学习方法的演示轨迹。每条轨迹*τ*表示为一系列有序的状态-动作对。
- en: where the expert demonstrations set is defined as D={*τ*0​,⋯,*τn*​}
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 专家演示集定义为D={*τ*0​,⋯,*τn*​}
- en: '*The expert episodic return was 16.33±6 on average for 30 episodes with a length
    of 32 steps each.*'
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*专家的每集回报平均为16.33±6，共30集，每集长度为32步。*'
- en: Forward Reinforcement Learning
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前向强化学习
- en: First, we will train a model using the ground truth reward to set some baselines
    and tune hyperparameters for later use with IL methods.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用真实奖励训练一个模型，设定一些基准并调优超参数，以便后续与IL方法一起使用。
- en: The implementation of the Forward RL algorithm used in this post is based on
    Clean RL scripts [12], which provides a readable implementation of RL methods.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中使用的前向强化学习算法的实现基于Clean RL脚本[12]，该脚本提供了可读的强化学习方法实现。
- en: Introduction
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: We will test both Proximal Policy Optimization (PPO) [2] and Deep Q-Network
    (DQN) [1], state-of-the-art on-policy and well-known off-policy RL methods, respectively.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将分别测试最先进的策略内方法——近端策略优化（PPO）[2]和著名的策略外方法——深度Q网络（DQN）[1]。
- en: 'The following is a summary of the training steps for each method, along with
    their characteristics:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是每种方法的训练步骤总结以及它们的特点：
- en: On-Policy (PPO)
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在策略内（PPO）
- en: 'This method uses the current policy under training and updates its parameters
    after collecting rollouts for every episode. PPO has two main parts: critic and
    actor. The actor represents the policy, while the critic provides value estimations
    for each state with its own updated objective.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法使用当前训练中的策略，并在每次收集回合后更新其参数。PPO包含两个主要部分：评论员和演员。演员表示策略，而评论员为每个状态提供价值估计，并具有自己的更新目标。
- en: Off-Policy (DQN)
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略外（DQN）
- en: DQN trains its policy offline by collecting rollouts in a replay buffer using
    epsilon-greedy exploration. This means that DQN does not take always the best
    action according to the current policy for every state but rather selects a random
    action. This enables the exploration of different solutions. An additional target
    network may be used with less frequently updated version of the policy to make
    the learning objective more stable.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: DQN通过收集回放缓冲区中的回合来离线训练其策略，采用ε-贪婪探索策略。这意味着DQN在每个状态下并不总是选择当前策略下的最佳动作，而是随机选择一个动作。这有助于探索不同的解决方案。可能还会使用一个目标网络，该网络以较低的频率更新策略版本，以使学习目标更加稳定。
- en: Results and Discussion
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果与讨论
- en: The following figure shows the episodic return curves for both methods. DQN
    is in black, while PPO is shown as an orange line.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了两种方法的每集回报曲线。DQN用黑色表示，而PPO用橙色线条表示。
- en: '![](../Images/3a1db66541085517545021eef57c3ea4.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a1db66541085517545021eef57c3ea4.png)'
- en: 'For this simple example:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个简单的例子：
- en: Both PPO and DQN converge, but with a slight advantage for PPO. Neither method
    reaches the expert level of 16.6 (PPO comes close with 15.26).
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PPO和DQN都能收敛，但PPO略有优势。两种方法都未达到专家级别的16.6（PPO接近，达到15.26）。
- en: DQN seems slower to converge in terms of interaction steps, known as sample
    inefficiency compared to PPO.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与PPO相比，DQN在收敛速度上似乎较慢，表现为与交互步骤（也称为样本效率）相关的低效率。
- en: PPO takes longer training time, possibly due to actor-critic training, updating
    two networks with different objectives.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PPO训练时间较长，可能是由于演员-评论员训练，需要更新两个具有不同目标的网络。
- en: The parameters for training both methods are mostly the same. For a closer look
    at how these curves were generated, check the scripts `ppo.py` and `dqn.py` in
    the accompanying repository.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 训练这两种方法的参数大致相同。欲深入了解这些曲线是如何生成的，可以查看随附仓库中的`ppo.py`和`dqn.py`脚本。
- en: Behavior Cloning (BC)
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 行为克隆（BC）
- en: 'Behavior Cloning, first proposed in [4], is a direct IL method. It involves
    supervised learning to map each state to an action based on expert demonstrations
    D. The objective is defined as:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 行为克隆（Behavior Cloning，BC），最早在[4]中提出，是一种直接的强化学习方法。它通过监督学习将每个状态映射到一个动作，基于专家演示集D。目标定义为：
- en: where *π_bc*​ is the trained policy, *π_E*​ is the expert policy, and *l*(*π_bc*​(*s*),*π_E*​(*s*))
    is the loss function between the expert and trained policy in response to the
    same state.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*π_bc*​是训练后的策略，*π_E*​是专家策略，*l*(*π_bc*​(*s*),*π_E*​(*s*))是响应同一状态时，专家策略和训练策略之间的损失函数。
- en: The difference between BC and supervised learning lies in defining the problem
    as an interactive environment where actions are taken in response to dynamic states
    (e.g., a robot moving towards a goal). In contrast, supervised learning involves
    mapping input to output, like classifying images or predicting temperature. This
    distinction is explained in [8].
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: BC和监督学习的区别在于将问题定义为一个交互式环境，在该环境中，行为是对动态状态的响应（例如，一个机器人朝着目标移动）。相比之下，监督学习涉及将输入映射到输出，比如图像分类或温度预测。这个区别在[8]中有解释。
- en: In this implementation, the full set of initial positions for the agent contains
    only 15 possibilities. Consequently, there are only 15 trajectories to learn from,
    which can be memorized by the BC network effectively. To make the problem harder,
    we clip the size of the training dataset D to half (only 240 state-action pairs
    out of 480) and repeat this for all IL methods that follow in this post.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在此实现中，智能体的初始位置全集只有15种可能性。因此，只有15条轨迹可以学习，而这些轨迹可以被BC网络有效地记住。为了增加问题的难度，我们将训练数据集D的大小裁剪为一半（仅有480个状态-动作对中的240个），并将此操作应用于后续所有的IL方法。
- en: Results
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: After training the model (as shown in `bc.py` script), we get an average episodic
    return of 11.49 with a standard deviation of 5.24.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型后（如`bc.py`脚本所示），我们得到的平均回报是11.49，标准差为5.24。
- en: This is much less than the forward RL methods before. The following GIF shows
    the trained BC model in action.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这比之前的前向强化学习方法要小得多。以下GIF展示了训练好的BC模型的实际表现。
- en: '![](../Images/4d844fec525ada99e1b12814cdde3f86.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d844fec525ada99e1b12814cdde3f86.png)'
- en: From the GIF, it’s evident that almost two-thirds of the trajectories have learned
    to pass through the wall. However, the model gets stuck with the last third, as
    it cannot infer the true policy from previous examples, especially since it was
    given only half of the 15 expert trajectories to learn from.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 从GIF中可以看出，几乎三分之二的轨迹已经学会了穿越墙壁。然而，模型在最后三分之一的轨迹上遇到了困难，因为它无法从之前的例子中推断出真实的策略，特别是由于它只从15条专家轨迹中的一半进行学习。
- en: Maximum Entropy Inverse Reinforcement Learning (MaxENT)
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大熵逆强化学习（MaxENT）
- en: 'MaxEnt [3] is another method to train a reward model separately (not iteratively),
    beside Behavior Cloning (BC). Its main idea lies in maximizing the probability
    of taking expert trajectories based on the current reward function. This can be
    expressed as:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: MaxEnt [3] 是一种与行为克隆（BC）并行的训练奖励模型的另一种方法（而不是迭代方式）。其主要思想是基于当前的奖励函数，最大化采取专家轨迹的概率。这可以表示为：
- en: Where *N* is the trajectory length, and *Z* is a normalizing constant of the
    sum of all possible trajectories returns under the given policy.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *N* 是轨迹长度，*Z* 是在给定策略下，所有可能轨迹回报之和的归一化常数。
- en: 'From there, the method derives its main objective based on the maximum entropy
    theorem [3], which states that *the most representative policy fulfilling a given
    condition is the one with highest entropy H.* Therefore, MaxEnt requires an additional
    objective to maximize the entropy of the policy. This leads to the following formula:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，该方法基于最大熵定理[3]推导出其主要目标，该定理指出：*满足给定条件的最具代表性的策略是具有最大熵H的策略*。因此，MaxEnt需要一个额外的目标来最大化策略的熵。这导致了以下公式：
- en: 'Which has the derivative:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 其导数为：
- en: Where *SVD* is the state visitation frequency, which can be calculated with
    a dynamic programming algorithm given the current policy.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *SVD* 是状态访问频率，可以通过动态规划算法在给定策略下计算得出。
- en: In our implementation here of MaxEnt, we skip the training of a new reward,
    where the dynamic programming algorithm would be slow and lengthy. Instead, we
    opt to test the main idea of maximizing the entropy by re-training a BC model
    exactly as in the previous process, but with an added term of the negative entropy
    of the inferred action distribution to the loss. The entropy should be negative
    because we wish to maximize it by minimizing the loss.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们这里实现的最大熵（MaxEnt）中，我们跳过了训练一个新的奖励模型，因为动态规划算法在此过程中会变得缓慢且冗长。相反，我们选择通过重新训练一个行为克隆（BC）模型来测试最大化熵的主要思想，方法与之前的过程完全相同，只是将推断出的行动分布的负熵项添加到损失函数中。熵应该是负的，因为我们希望通过最小化损失来最大化它。
- en: Results
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: After adding the negative entropy of the distributions of actions with a weight
    of 0.5 (choosing the right value is important; otherwise, it may lead to worse
    learning), we see a slight improvement over the performance of the previous BC
    model with an average episodic return of 11.56 now (+0.07). The small value of
    the improvement can be explained by the simple nature of the environment, which
    contains a limited number of states. If the state space gets bigger, the entropy
    is expected to have a bigger importance.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在给动作分布添加了权重为0.5的负熵（选择正确的值很重要，否则可能导致更差的学习）后，我们看到与之前的BC模型相比，性能略有提升，现在的平均回合回报为11.56（+0.07）。这种小幅提升可以通过环境的简单性质来解释，该环境包含有限数量的状态。如果状态空间变大，熵的作用预计会变得更加重要。
- en: Generative Adversarial Imitation Learning (GAIL)
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成对抗模仿学习（GAIL）
- en: The original work on GAIL [5] was inspired by the concept of Generative Adversarial
    Networks (GANs), which apply the idea of adversarial training to enhance the generative
    abilities of a main model. Similarly, in GAIL, the concept is applied to match
    state-action distributions between trained and expert policies.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: GAIL的原始工作[5]灵感来源于生成对抗网络（GANs）的概念，GANs将对抗训练的理念应用于增强主模型的生成能力。同样，在GAIL中，这一概念被应用于使训练策略与专家策略之间的状态-动作分布相匹配。
- en: 'This can be derived as Kullback-Leibler divergence, as shown in the main paper
    [5]. The paper finally derives the main objective for both models (called generator
    and discriminator models in GAIL) as:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以推导为Kullback-Leibler散度，如主文献[5]所示。文献最终推导出了两个模型（在GAIL中称为生成器和判别器模型）的主要目标，如下所示：
- en: '![](../Images/3d98990eb6f613a4842efbd3f5682a05.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d98990eb6f613a4842efbd3f5682a05.png)'
- en: Where *Dt*​ is the discriminator, *πθ*​ is the generator model (i.e., the policy
    under training), *πE*​ is the expert policy, and *H*(*πθ*​) is the entropy of
    the generator model.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*Dt*​ 是判别器，*πθ*​ 是生成器模型（即正在训练的策略），*πE*​ 是专家策略，*H*(*πθ*​) 是生成器模型的熵。
- en: The discriminator acts as a binary classifier, while the generator is the actual
    policy model being trained.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器充当二分类器，而生成器则是实际的策略模型，正在接受训练。
- en: The Main Benefit of GAIL
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GAIL的主要优势
- en: The main benefit of GAIL over previous methods (and the reason it performs better)
    lies in its interactive training process. The trained policy learns and explores
    different states guided by the discriminator’s reward signal.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: GAIL相对于以前方法的主要优势（也是其表现更好的原因）在于其交互式训练过程。训练得到的策略在判别器奖励信号的指导下学习并探索不同的状态。
- en: Results
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: After training GAIL for 1.6 million steps, the model converged to a higher level
    than BC and MaxEnt models. If continued to be trained, even better results can
    be achieved.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练了1.6百万步后，GAIL模型收敛到了比BC和MaxEnt模型更高的水平。如果继续训练，甚至可以取得更好的结果。
- en: Specifically, we obtained an average episodic reward of 12.8, which is noteworthy
    considering that only 50% of demonstrations were provided without any real reward.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们获得了12.8的平均回合奖励，这一点值得注意，因为只有50%的示范在没有任何真实奖励的情况下提供。
- en: This figure shows the training curve for GAIL (with ground truth episodic rewards
    on the y-axis). It’s worth noting that the rewards coming from log(*D*(*s*,*a*))
    will be more chaotic than the ground truth due to GAIL’s adversarial training
    nature.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 此图显示了GAIL的训练曲线（y轴为真实的回合奖励）。值得注意的是，由于GAIL的对抗性训练特性，来自log(*D*(*s*,*a*))的奖励会比真实奖励更加混乱。
- en: '![](../Images/64b2b61d8406584fafe7665aed713417.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/64b2b61d8406584fafe7665aed713417.png)'
- en: Adversarial Inverse Reinforcement Learning (AIRL)
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对抗性逆强化学习（AIRL）
- en: One remaining problem with GAIL is that the trained reward model (the discriminator)
    does not actually represent the ground truth reward. Instead, the discriminator
    is trained as a binary classifier between expert and generator state-action pairs,
    resulting in an average of its values of 0.5\. This means that the discriminator
    can only be considered a surrogate reward.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: GAIL的一个剩余问题是，训练得到的奖励模型（判别器）并不真正代表真实的奖励。相反，判别器被训练为一个专家与生成器状态-动作对之间的二分类器，导致其值的平均为0.5。这意味着判别器只能作为一个替代奖励。
- en: 'To solve this problem, the paper in [6] reformulates the discriminator using
    the following formula:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，文献[6]通过以下公式重新构造了判别器：
- en: where *fω*​(*s*,*a*) should converge to the actual advantage function. In this
    example, this value represents how close the agent is to the invisible goal. The
    ground truth reward can be found by adding another term to include a shaped reward;
    however, for this experiment, we will restrict ourselves to the advantage function
    above.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*fω*​(*s*,*a*) 应该收敛到实际的优势函数。在这个例子中，这个值表示代理与不可见目标的接近程度。通过添加另一个项以包含形状奖励，可以得到地面真实奖励；然而，在本次实验中，我们将限制使用上面的优势函数。
- en: Results
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: 'After training the AIRL model with the same parameters as GAIL, we obtained
    the following training curve:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用与 GAIL 相同参数训练 AIRL 模型后，我们得到以下训练曲线：
- en: '![](../Images/fca3ec99819c63a12137e5791e9c6f12.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fca3ec99819c63a12137e5791e9c6f12.png)'
- en: It is noted that given the same training steps (1.6 Million steps), AIRL was
    slower to converge due to the added complexity of training the discriminator.
    However, now we have a meaningful advantage function, albeit with a performance
    of only 10.8 episodic reward, which is still good enough.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，考虑到相同的训练步数（160 万步），由于训练判别器的复杂性增加，AIRL 收敛较慢。然而，现在我们已经得到了一个有意义的优势函数，尽管其表现只有
    10.8 的每集奖励，但仍然足够好。
- en: 'Let’s compare the values of this advantage function and the ground truth reward
    in response to expert demonstrations. To make these values more comparable, we
    also normalized the values of the learned advantage function *fω*​. From this,
    we got the following plot:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个优势函数的值与响应专家演示的地面真实奖励进行比较。为了使这些值更具可比性，我们还对学习到的优势函数 *fω*​ 的值进行了归一化。由此，我们得到了以下图表：
- en: '![](../Images/5a9cc6f81f11b38b39c4840e9be7e623.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a9cc6f81f11b38b39c4840e9be7e623.png)'
- en: In this figure, there are 15 pulses corresponding to the 15 initial states of
    the agent. We can see bigger errors in the trained model for the last half of
    the plot, which is due to the limited use of only half the expert demos in training.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图中，有 15 个脉冲对应代理的 15 个初始状态。我们可以看到，在图的后半部分，训练模型的误差较大，这是由于在训练中仅有限使用了专家演示的一半。
- en: For the first half, we observe a low state when the agent stands still at the
    goal with zero reward, while it was evaluated as a high value in the trained model.
    In the second half, there’s a general shift towards lower values.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前半部分，我们观察到在代理静止在目标位置并获得零奖励时，状态较低，而在训练模型中评估时该值较高。后半部分则普遍偏向于较低的值。
- en: Roughly speaking, the learned function approximately follows the ground truth
    reward and has recovered useful information about it using AIRL.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 大致来说，学习到的函数大致跟随地面真实奖励，并通过 AIRL 恢复了关于它的有用信息。
- en: Information Maximization GAIL (InfoGAIL)
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信息最大化生成对抗模仿学习（InfoGAIL）
- en: 'Despite the advancements made by previous methods, an important problem still
    persists in Imitation Learning (IL): **multi-modal learning**. To apply IL to
    practical problems, it is necessary to learn from multiple possible expert policies.
    For instance, when driving or playing football, there is no single “true” way
    of doing things; experts vary in their methods, and the IL model should be able
    to learn these variations consistently.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前述方法取得了一定进展，但在模仿学习（IL）中仍然存在一个重要问题：**多模态学习**。为了将 IL 应用于实际问题，需要从多个可能的专家策略中学习。例如，在开车或踢足球时，并没有一种“正确”的做事方式；专家在方法上有所不同，IL
    模型应该能够一致地学习这些变化。
- en: 'To address this issue, InfoGAIL was developed [7]. Inspired by InfoGAN [11],
    which conditions the style of outputs generated by GAN using an additional style
    vector, InfoGAIL builds on the GAIL objective and adds another criterion: maximizing
    the mutual information between state-action pairs and a new controlling input
    vector *z*. This objective can be derived as:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，开发了 InfoGAIL [7]。InfoGAIL 受到 InfoGAN [11] 的启发，InfoGAN 使用额外的风格向量来调节生成对抗网络（GAN）生成输出的风格，InfoGAIL
    在 GAIL 目标的基础上增加了另一个标准：最大化状态-动作对与新的控制输入向量 *z* 之间的互信息。这个目标可以推导为：
- en: Kullback-Leibler divergence,
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Kullback-Leibler 散度，
- en: where estimating the posterior *p*(*z*∣*s*,*a*) is approximated with a new model,
    *Q*, which takes (*s*,*a*) as input and outputs *z*.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，后验概率 *p*(*z*∣*s*,*a*) 的估计通过一个新模型 *Q* 进行近似，*Q* 以 (*s*,*a*) 为输入，输出 *z*。
- en: 'The final objective for InfoGAIL can be written as:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: InfoGAIL 的最终目标可以写为：
- en: '![](../Images/b5bc5c1f07a8eea226011e43329d8b49.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5bc5c1f07a8eea226011e43329d8b49.png)'
- en: 'As a result, the policy has an additional input, namely *z*, as shown in the
    following figure:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，策略有一个额外的输入，即 *z*，如以下图所示：
- en: '![](../Images/cdb73e4f39a5c4a376c9da575e4f02e8.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cdb73e4f39a5c4a376c9da575e4f02e8.png)'
- en: 'In our experiments, we generated new multi-modal expert demos where each expert
    could enter from one gap only (of the three gaps on the wall), regardless of their
    goal. The full demo set was used without labels indicating which expert was acting.
    The *z* variable is a one-hot encoding vector representing the expert class with
    three elements (e.g., `[1 0 0]` for the left door). The policy should:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们生成了新的多模态专家演示，其中每个专家只能从一个间隙进入（墙上有三个间隙），与其目标无关。完整的演示集在没有标签的情况下使用，标签不会指明是哪个专家在行动。*z*
    变量是一个独热编码向量，表示专家类别，包含三个元素（例如，左门的表示为`[1 0 0]`）。策略应该：
- en: Learn to move towards the goal
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习朝向目标移动
- en: Link randomly generated *z* values to different modes of experts (thus passing
    through different doors)
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机生成的 *z* 值与不同专家的模式相联系（从而通过不同的门）。
- en: The *Q* model should be able to detect which mode it is based on the direction
    of actions in every state
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Q* 模型应该能够根据每个状态中动作的方向来检测其所处的模式。'
- en: Note that the discriminator, Q-model, and policy model training graphs are chaotic
    due to adversarial training.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于对抗性训练，判别器、Q模型和策略模型的训练图表是混乱的。
- en: 'Fortunately, we were able to learn two modes clearly. However, the third mode
    was not recognized by either the policy or the Q-model. The following three GIFs
    show the learned expert modes from InfoGAIL when given different values of *z*:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们成功地清晰地学习到了两种模式。然而，第三种模式既没有被策略也没有被 Q 模型识别出来。以下三个 GIF 展示了 InfoGAIL 在给定不同
    *z* 值时学到的专家模式：
- en: '![](../Images/c12ec76451dee278e191b253b631f0ae.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c12ec76451dee278e191b253b631f0ae.png)'
- en: '**z = [1,0,0]**'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**z = [1,0,0]**'
- en: '![](../Images/ab355cdb776ac470e7b1e8b5aa1552a3.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ab355cdb776ac470e7b1e8b5aa1552a3.png)'
- en: '**z = [0,1,0]**'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**z = [0,1,0]**'
- en: '![](../Images/0a66821dfd5078e24edbf1a7557fef59.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a66821dfd5078e24edbf1a7557fef59.png)'
- en: '**z = [0,0,1]**'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**z = [0,0,1]**'
- en: Lastly, the policy was able to converge to an episodic reward of around 10 with
    800K training steps. With more training steps, better results can be achieved,
    even if the experts used in this example are not optimal.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，策略能够在 800K 训练步数下收敛到大约 10 的回报。随着训练步数的增加，可以取得更好的结果，即使这个例子中使用的专家不是最优的。
- en: Final Overview and Conclusion
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终概述与结论
- en: 'As we review our experiments, it’s clear that all IL methods have performed
    well in terms of episodic reward criteria. The following table summarizes their
    performance:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在回顾我们的实验时，很明显所有的模仿学习方法在集回报标准方面表现良好。下表总结了它们的表现：
- en: '![](../Images/b240ddb57578ec3271d55c9076130b7e.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b240ddb57578ec3271d55c9076130b7e.png)'
- en: '**InfoGAIL results are not comparable as the expert demos were based on multi-modal
    experts*'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**InfoGAIL 的结果不可比，因为专家演示基于多模态专家**'
- en: The table shows that GAIL performed the best for this problem, while AIRL was
    slower due to its new reward formulation, resulting in a lower return. InfoGAIL
    also learned well but struggled with recognizing all three modes of experts.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 表格显示，GAIL 在这个问题上表现最好，而 AIRL 由于其新的奖励公式较慢，导致回报较低。InfoGAIL 也学得不错，但在识别所有三个专家模式时遇到了一些困难。
- en: Conclusion
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Imitation Learning is a challenging and fascinating field. The methods we’ve
    explored are suitable for grid simulation environments but may not directly translate
    to real-world applications. Practical uses of IL are still in its infancy, except
    for some BC methods. Linking simulations to reality introduces new errors due
    to differences of their nature.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 模仿学习是一个具有挑战性且迷人的领域。我们探索的方法适用于网格仿真环境，但可能无法直接转化为现实应用。模仿学习的实际应用仍处于起步阶段，除了某些 BC
    方法之外。将仿真与现实联系起来会由于两者的性质差异而引入新的误差。
- en: Another open challenge in IL is Multi-agent Imitation Learning. Research like
    MAIRL [9] and MAGAIL [10] have experimented with multi-agent environments but
    a general theory for learning from multiple expert trajectories remains an open
    question.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 模仿学习中的另一个开放挑战是多智能体模仿学习。像 MAIRL [9] 和 MAGAIL [10] 这样的研究已经在多智能体环境中进行过实验，但从多个专家轨迹学习的通用理论仍然是一个开放问题。
- en: The attached [repository on GitHub](http://github.com/engyasin/ilsurvey) provides
    a basic approach to implementing these methods, which can be easily extended.
    The code will be updated in the future. If you’re interested in contributing,
    please submit an issue or pull request with your modifications. Alternatively,
    feel free to leave a comment as we’ll follow up with updates.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 附加的[GitHub 仓库](http://github.com/engyasin/ilsurvey)提供了实现这些方法的基本思路，且可以方便地进行扩展。代码将在未来更新。如果您有兴趣贡献，请提交一个问题或拉取请求，包含您的修改。或者，欢迎留下评论，我们会跟进并提供更新。
- en: '*Note: Unless otherwise noted, all images are generated by author*'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：除非另有说明，所有图像均由作者生成*'
- en: References
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Mnih, V. (2013). Playing atari with deep reinforcement learning. arXiv
    preprint arXiv:1312.5602.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Mnih, V. (2013). 通过深度强化学习玩Atari游戏。arXiv预印本arXiv:1312.5602.'
- en: '[2] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017).
    Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017).
    近端策略优化算法。arXiv预印本arXiv:1707.06347.'
- en: '[3] Ziebart, B. D., Maas, A. L., Bagnell, J. A., & Dey, A. K. (2008, July).
    Maximum entropy inverse reinforcement learning. In Aaai (Vol. 8, pp. 1433–1438).'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Ziebart, B. D., Maas, A. L., Bagnell, J. A., & Dey, A. K. (2008年7月). 最大熵逆向强化学习。载于《AAAI》(Vol.
    8, pp. 1433–1438).'
- en: '[4] Bain, M., & Sammut, C. (1995, July). A Framework for Behavioural Cloning.
    In Machine Intelligence 15 (pp. 103–129).'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Bain, M., & Sammut, C. (1995年7月). 行为克隆框架。载于《机器智能15》(pp. 103–129).'
- en: '[5] Ho, J., & Ermon, S. (2016). Generative adversarial imitation learning.
    Advances in neural information processing systems, 29.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Ho, J., & Ermon, S. (2016). 生成对抗模仿学习。《神经信息处理系统进展》(Advances in neural information
    processing systems), 29.'
- en: '[6] Fu, J., Luo, K., & Levine, S. (2017). Learning robust rewards with adversarial
    inverse reinforcement learning. arXiv preprint arXiv:1710.11248.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Fu, J., Luo, K., & Levine, S. (2017). 通过对抗逆向强化学习学习稳健奖励。arXiv预印本arXiv:1710.11248.'
- en: '[7] Li, Y., Song, J., & Ermon, S. (2017). Infogail: Interpretable imitation
    learning from visual demonstrations. Advances in neural information processing
    systems, 30.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Li, Y., Song, J., & Ermon, S. (2017). Infogail：从视觉示范中进行可解释的模仿学习。《神经信息处理系统进展》(Advances
    in neural information processing systems), 30.'
- en: '[8] Osa, T., Pajarinen, J., Neumann, G., Bagnell, J. A., Abbeel, P., & Peters,
    J. (2018). An algorithmic perspective on imitation learning. Foundations and Trends®
    in Robotics, 7(1–2), 1–179.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Osa, T., Pajarinen, J., Neumann, G., Bagnell, J. A., Abbeel, P., & Peters,
    J. (2018). 以算法视角看模仿学习。《机器人学基础与趋势》(Foundations and Trends® in Robotics), 7(1–2),
    1–179.'
- en: '[9] Yu, L., Song, J., & Ermon, S. (2019, May). Multi-agent adversarial inverse
    reinforcement learning. In International Conference on Machine Learning (pp. 7194–7201).
    PMLR.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Yu, L., Song, J., & Ermon, S. (2019年5月). 多智能体对抗逆向强化学习。载于《国际机器学习大会》(International
    Conference on Machine Learning)(pp. 7194–7201)。PMLR.'
- en: '[10] Song, J., Ren, H., Sadigh, D., & Ermon, S. (2018). Multi-agent generative
    adversarial imitation learning. Advances in neural information processing systems,
    31.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Song, J., Ren, H., Sadigh, D., & Ermon, S. (2018). 多智能体生成对抗模仿学习。《神经信息处理系统进展》(Advances
    in neural information processing systems), 31.'
- en: '[11] Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., & Abbeel,
    P. (2016). Infogan: Interpretable representation learning by information maximizing
    generative adversarial nets. Advances in neural information processing systems,
    29.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., & Abbeel,
    P. (2016). Infogan：通过信息最大化生成对抗网络进行可解释的表示学习。《神经信息处理系统进展》(Advances in neural information
    processing systems), 29.'
- en: '[12] Huang, S., Dossa, R. F. J., Ye, C., Braga, J., Chakraborty, D., Mehta,
    K., & AraÃšjo, J. G. (2022). Cleanrl: High-quality single-file implementations
    of deep reinforcement learning algorithms. Journal of Machine Learning Research,
    23(274), 1–18.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Huang, S., Dossa, R. F. J., Ye, C., Braga, J., Chakraborty, D., Mehta,
    K., & AraÃšjo, J. G. (2022). Cleanrl：高质量的单文件深度强化学习算法实现。《机器学习研究期刊》(Journal of Machine
    Learning Research), 23(274), 1–18.'
