- en: Phi-3 and the Beginning of Highly Performant iPhone LLMs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Phi-3 与高度高效的 iPhone LLMs 开始
- en: 原文：[https://towardsdatascience.com/phi-3-and-the-beginning-of-highly-performant-iphone-models-d413d8ea0714?source=collection_archive---------10-----------------------#2024-05-09](https://towardsdatascience.com/phi-3-and-the-beginning-of-highly-performant-iphone-models-d413d8ea0714?source=collection_archive---------10-----------------------#2024-05-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/phi-3-and-the-beginning-of-highly-performant-iphone-models-d413d8ea0714?source=collection_archive---------10-----------------------#2024-05-09](https://towardsdatascience.com/phi-3-and-the-beginning-of-highly-performant-iphone-models-d413d8ea0714?source=collection_archive---------10-----------------------#2024-05-09)
- en: This blog post will go into the findings of the Phi-3 paper, as well as some
    of the implications of models like Phi-3 being released
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本文将深入探讨 Phi-3 论文的发现，以及像 Phi-3 这样的模型发布所带来的一些影响。
- en: '[](https://medium.com/@mgunton7?source=post_page---byline--d413d8ea0714--------------------------------)[![Matthew
    Gunton](../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png)](https://medium.com/@mgunton7?source=post_page---byline--d413d8ea0714--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d413d8ea0714--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d413d8ea0714--------------------------------)
    [Matthew Gunton](https://medium.com/@mgunton7?source=post_page---byline--d413d8ea0714--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mgunton7?source=post_page---byline--d413d8ea0714--------------------------------)[![Matthew
    Gunton](../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png)](https://medium.com/@mgunton7?source=post_page---byline--d413d8ea0714--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d413d8ea0714--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d413d8ea0714--------------------------------)
    [Matthew Gunton](https://medium.com/@mgunton7?source=post_page---byline--d413d8ea0714--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d413d8ea0714--------------------------------)
    ·8 min read·May 9, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d413d8ea0714--------------------------------)
    ·阅读时间 8 分钟 ·2024年5月9日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/5ff51da9a59316cb83b1b0cbf42f1a54.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5ff51da9a59316cb83b1b0cbf42f1a54.png)'
- en: Image by Author — generated by Stable Diffusion 2.1
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者 — 由 Stable Diffusion 2.1 生成
- en: Readers of my prior work may remember [when I covered “Textbooks are all you
    need”](https://medium.com/@mgunton7/the-impact-of-better-data-on-llms-46153ba26795),
    a paper by Microsoft showing how quality data can have an outsize impact on model
    performance. The findings there directly refuted the belief that models had to
    be enormous to be capable. The researchers behind that paper have continued their
    work and published something I find incredibly exciting.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前的文章的读者可能还记得我曾讨论过 [“教材就是你所需要的一切”](https://medium.com/@mgunton7/the-impact-of-better-data-on-llms-46153ba26795)，这是微软的一篇论文，展示了优质数据如何对模型性能产生超乎想象的影响。那里的研究结果直接反驳了“模型必须巨大才能具备能力”这一观点。该论文背后的研究人员继续进行相关工作，并发布了我认为非常令人兴奋的成果。
- en: 'The title of this paper explains perhaps the biggest finding: [“Phi-3 Technical
    Report: A Highly Capable Language Model Locally on Your Phone”.](https://arxiv.org/pdf/2404.14219)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本文标题解释了或许是最大的发现：[“Phi-3 技术报告：在你的手机上本地运行的高度有能力的语言模型”。](https://arxiv.org/pdf/2404.14219)
- en: Let’s dive into what the authors changed from the Phi-2 model, how they trained
    it, and how it works on your iPhone.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解作者从 Phi-2 模型中做出的改变、他们如何训练模型以及它如何在你的 iPhone 上运行。
- en: Key Terminology
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键术语
- en: There are a few key concepts to know before we dive into the architecture. If
    you know these already, feel free to skip to the next section.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨架构之前，有几个关键概念需要了解。如果你已经知道这些内容，可以跳过到下一部分。
- en: A model’s **parameters** refer to the number of weights and biases that the
    model learns during training. If you have 1 billion parameters, then you have
    1 billion weights and biases that determine the model’s performance. The more
    parameters you have the more complex your neural network can be. A **head** refers
    to the number of key, value, and query vectors the self-attention mechanism in
    a Transformer has. **Layers** refers to the number of neural segments that exist
    within the neural network of the Transformer, with hidden dimensions being the
    number of neurons within a typical hidden layer.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的**参数**是指模型在训练过程中学习的权重和偏置的数量。如果你有10亿个参数，那么你就有10亿个权重和偏置来决定模型的表现。参数越多，神经网络的复杂性也就越高。**头**指的是变换器中自注意力机制所拥有的键、值和查询向量的数量。**层**指的是变换器神经网络中存在的神经单元的数量，而隐藏维度则是指典型隐藏层内神经元的数量。
- en: '**Tokenizer** is the software piece that will convert your input text into
    an embedding that the transformer will then work with. **Vocabulary size** refers
    to the number of unique tokens that the model is trained on. The **block structure**
    of a transformer is how we refer to the combination of layers, heads, activation
    functions, tokenizer and layer normalizations that would be chosen for a specific
    model.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**分词器**是将输入文本转换为嵌入的程序，变换器（transformer）随后将处理这些嵌入。**词汇表大小**指的是模型训练时使用的唯一标记数量。变换器的**块结构**是指我们在为特定模型选择层、头、激活函数、分词器和层归一化时所采用的组合方式。'
- en: '![](../Images/4d7eeae821cb0ddfd874752561dc84f8.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d7eeae821cb0ddfd874752561dc84f8.png)'
- en: 'Figure 2 from [“GQA: Training Generalized Multi-Query Transformer Models from'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '图2来自 [“GQA: 训练通用多查询变换器模型”](https://arxiv.org/pdf/2305.13245)'
- en: Multi-Head Checkpoints”](https://arxiv.org/pdf/2305.13245)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[多头检查点](https://arxiv.org/pdf/2305.13245)'
- en: '**Grouped-Query Attention (GQA)** is a way that we optimize multi-head attention
    to reduce the computational overhead during training and inference. As you can
    see from the image below, GQA takes the middle-ground approach — rather than pairing
    1 value and 1 key to 1 query, we take a 1:1:M approach, with the many being smaller
    than the entire body of queries. This is done to still get the training cost benefits
    from Multi-Query Attention (MQA), while minimizing the performance degradation
    that we see follow that.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**分组查询注意力（GQA）**是一种优化多头注意力的方法，旨在减少训练和推理过程中的计算开销。正如下面的图像所示，GQA采取了中庸之道——我们不再将1个值和1个键与1个查询配对，而是采用1:1:M的方式，其中“多”的数量小于所有查询的总数。这样做是为了从多查询注意力（MQA）中仍然获得训练成本的好处，同时最大限度地减少因此而带来的性能下降。'
- en: Phi 3 Architecture
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Phi 3 架构
- en: Let’s begin with the architecture behind this model. The researchers released
    3 different decoder only models, *phi-3-mini, phi-3-small,* and *phi-3-medium,*
    with different hyperparameters for each.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从该模型背后的架构开始。研究人员发布了3个不同的仅解码器模型，*phi-3-mini、phi-3-small* 和 *phi-3-medium*，每个模型都有不同的超参数。
- en: '*phi-3-mini*'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*phi-3-mini*'
- en: '- 3.8 billion parameters'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 38 亿个参数'
- en: '- 32 heads'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 32 个头'
- en: '- 32 layers'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 32 层'
- en: '- 3072 hidden dimensions'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 3072 个隐藏维度'
- en: '- 4k token default context length'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 4k token 默认上下文长度'
- en: '- 32064 vocabulary size'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 32064 词汇表大小'
- en: '- weights stored as bfloat16'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 权重存储为 bfloat16'
- en: '- trained on 3.3 Trillion Tokens'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 训练于 3.3 万亿个标记'
- en: '*phi-3-small*'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*phi-3-small*'
- en: '- 7 billion parameters'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 70 亿个参数'
- en: '- 32 heads'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 32 个头'
- en: '- 32 layers'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 32 层'
- en: '- 4096 hidden dimensions'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 4096 个隐藏维度'
- en: '- 8k token default context length'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 8k token 默认上下文长度'
- en: '- 100352 vocabulary size'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 100352 词汇表大小'
- en: '- weights stored as bfloat16'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 权重存储为 bfloat16'
- en: '- trained on 4.8 Trillion Tokens'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 训练于 4.8 万亿个标记'
- en: '*phi-3-medium*'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*phi-3-medium*'
- en: '- 14 billion parameters'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 140 亿个参数'
- en: '- 40 heads'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 40 个头'
- en: '- 40 layers'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 40 层'
- en: '- 3072 hidden dimensions'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 3072 个隐藏维度'
- en: '- trained on 4.8 Trillion Tokens'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 训练于 4.8 万亿个标记'
- en: Going into some of the differences here, the *phi-3-mini* model was trained
    using typical mutli-head attention. While not called out in the paper, my suspicion
    is that because the model is roughly half the size of the other two, the training
    costs associated with multi-head were not objectionable. Naturally when they scaled
    up for *phi-3-small*, they went with grouped query attention, with 4 queries connected
    to 1 key.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 讲解一下这些模型之间的差异，*phi-3-mini*模型使用了典型的多头注意力进行训练。尽管论文中没有明确指出，但我怀疑由于该模型的规模大约是其他两个模型的一半，训练多头注意力的开销是可以接受的。自然地，当他们扩展到
    *phi-3-small* 时，选择了分组查询注意力，每个键连接到4个查询。
- en: Moreover, they kept *phi-3-mini’s* block structure as close to the LLaMa-2 structure
    as they could. The goal here was to allow the open-source community to continue
    their research on LLaMa-2 with Phi-3\. This makes sense as a way to further understand
    the power of that block structure.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，他们尽量使*phi-3-mini*的块结构与LLaMa-2结构保持一致。这里的目标是让开源社区能够继续在LLaMa-2的基础上，使用Phi-3进行研究。这是进一步理解该块结构能力的合理方式。
- en: However, *phi-3-small* did NOT use LLaMa’s block structure, opting to use the
    `tiktoken` tokenizer, with alternate layers of dense attention and a new blocksparse
    attention. Additionally, they added in 10% multilingual data to the training dataset
    for these models.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，*phi-3-small*并没有使用LLaMa的块结构，而是选择使用`tiktoken`分词器，交替使用密集注意力层和新的块稀疏注意力。此外，他们在这些模型的训练数据集中加入了10%的多语言数据。
- en: Training and Data Optimal Mixes
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和数据的最佳组合
- en: Similar to Phi-2, the researchers invested majorly in quality data. They used
    the similar “educational value” paradigm they had used before when generating
    data to train the model on, opting to use significantly more data than last time.
    They created their data in 2 phases.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 与Phi-2类似，研究人员主要投资于优质数据。他们采用了之前用于生成数据训练模型的相似“教育价值”范式，并选择使用比上次更多的数据。他们将数据生成分为两个阶段。
- en: Phase-1 involved finding web data that they found was of high “educational value”
    to the user. The goal here is to give general knowledge to the model. Phase-2
    then takes a subset of the Phase-1 data and generates data that would teach the
    model how to logically reason or attain specific skills.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 第一阶段涉及找到他们认为对用户具有高“教育价值”的网页数据。这里的目标是为模型提供一般知识。第二阶段则选取第一阶段数据的一个子集，生成能够教会模型如何进行逻辑推理或获得特定技能的数据。
- en: The challenge here was to ensure the mix of data from each corpus was appropriate
    for the scale of the model being trained (ie *phi-3-small* vs *phi-3-mini*). This
    is the idea behind a “data optimal” regime, where the data you are giving to the
    LLM to train with gives it the best ability for its block structure. Put differently,
    if you think that data is a key distinguisher for training a good LLM, then finding
    the right combination of skills to show the model via your data can be just as
    key as finding good data. The researchers highlighted that they wanted the model
    to have stronger reasoning than knowledge abilities, resulting in their choosing
    more data from the Phase-2 corpus than from the Phase-1.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的挑战在于确保来自每个语料库的数据组合适合正在训练的模型的规模（即*phi-3-small*与*phi-3-mini*）。这是“数据最优”机制的理念，意味着你提供给LLM进行训练的数据能够为其块结构提供最佳能力。换句话说，如果你认为数据是训练一个优秀LLM的关键，那么通过数据展示给模型的技能的正确组合，可能和找到优质数据一样重要。研究人员强调，他们希望模型具备比知识更强的推理能力，因此选择了更多来自第二阶段语料库的数据，而不是第一阶段的数据。
- en: '![](../Images/cf5ca4fbc23b7eca6141f59bd4f951e3.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf5ca4fbc23b7eca6141f59bd4f951e3.png)'
- en: Figure 2 [from the paper](https://arxiv.org/pdf/2404.14219) highlighting a potential
    relationship for data optimality
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图2 [来自论文](https://arxiv.org/pdf/2404.14219)，突出显示数据最优性的潜在关系
- en: Interestingly, when they were training *phi-3-medium* with roughly the same
    data mixture as they trained *phi-3-small*, they noticed that the improvements
    from 7B parameters to 14B were far more limited than from 3.8B to 7B. The authors
    suspect this is not a limitation of the block structure, but instead of the data
    mixture they used to train *phi-3-medium*.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，当他们用与训练*phi-3-small*时大致相同的数据组合来训练*phi-3-medium*时，他们发现从7B参数到14B的改进远不如从3.8B到7B的改进那么显著。作者怀疑这并非块结构的限制，而是他们用来训练*phi-3-medium*的数据组合的问题。
- en: Post-Training
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练后
- en: The team used both Supervised Fine Tuning (SFT) and Direct Preference Optimization
    (DPO) to improve the model post-training. Those interested in a [deep dive on
    DPO can check out my blog post here](https://medium.com/towards-data-science/understanding-the-implications-of-direct-preference-optimization-a4bbd2d85841).
    Supervised Fine Tuning is a type of transfer learning where we use a custom dataset
    to improve the LLM’s capabilities on that dataset. The authors used SFT to improve
    the model’s ability across diverse domains like math, coding, reasoning, and safety.
    They then used DPO for their chat optimization to guide it away from responses
    they wanted to avoid and towards ideal responses.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 团队使用了监督微调（Supervised Fine Tuning, SFT）和直接偏好优化（Direct Preference Optimization,
    DPO）来提升模型的训练后性能。对于想要深入了解DPO的读者，可以参考我在[这里的博客文章](https://medium.com/towards-data-science/understanding-the-implications-of-direct-preference-optimization-a4bbd2d85841)。监督微调是一种迁移学习方法，通过使用自定义数据集来提高大语言模型（LLM）在该数据集上的能力。作者们通过SFT提升了模型在数学、编程、推理和安全等多个领域的能力。随后，他们使用DPO优化聊天功能，引导模型远离不希望的回答，朝向理想的回应。
- en: It’s in this stage that the authors expanded the context window of *phi-3-mini*
    from 4k tokens to 128k tokens. The methodology they used to do this is called
    Long Rope. The authors claim that the performance is consistent between the 2
    context types, which is a big deal given the enormous increase in context length.
    If there is sufficient interest, I will do a separate blog post on the findings
    within that paper.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 正是在这一阶段，作者们将*phi-3-mini*的上下文窗口从4k个标记扩展到了128k个标记。他们使用的方法称为“长绳法”（Long Rope）。作者声称，在这两种上下文类型之间，性能是一致的，这一点非常重要，因为上下文长度大幅增加。如果有足够的兴趣，我将单独写一篇博客文章来探讨这篇论文中的发现。
- en: Quantization for Phone Usage
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手机使用的量化
- en: Even though these models are small, to get these models to run on your phone
    still requires some further minimization. Typically the weights for a LLM is stored
    as float; for example, Phi-3’s original weights were `bfloat16`, meaning each
    weight takes up 16 bits in memory. While 16 bits may seem trivial, when you take
    into account there are on the order of 10⁹ parameters in the model, you realize
    how quickly each additional bit adds up.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些模型较小，但要让它们在手机上运行仍然需要进一步的优化。通常，LLM的权重是以浮点数形式存储的；例如，Phi-3的原始权重是`bfloat16`格式，这意味着每个权重在内存中占用16位。虽然16位看似微不足道，但当考虑到模型中有约10⁹个参数时，你就会意识到每增加一位位数，所需的存储量就会迅速增加。
- en: To get around this, the authors condensed the weights from 16 bits to 4 bits.
    The basic idea is to reduce the number of bits required to store each number.
    For a conceptual example, the number 2.71828 could be condensed to 2.72\. While
    this is a lossy operation, it still captures a good portion of the information
    while taking significantly less storage.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，作者们将权重从16位压缩到4位。基本思路是减少存储每个数字所需的位数。例如，数字2.71828可以压缩为2.72。虽然这是一个有损操作，但它仍然能保留大部分信息，同时大大减少存储需求。
- en: '![](../Images/8016e9ac83116d37a28ad1fbdccf115b.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8016e9ac83116d37a28ad1fbdccf115b.png)'
- en: Figure 1 [from the paper](https://arxiv.org/pdf/2404.14219)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图1 [来自论文](https://arxiv.org/pdf/2404.14219)
- en: The authors ran the quantized piece on an iPhone with the A16 chip and found
    it could generate up to 12 tokens per second. For comparison, an M1 MacBook running
    LLaMa-2 Quantized 4 bit runs at roughly 107 tokens per second. The fastest token
    generation I’ve seen (Groq) generated tokens at a rate of 853.35 Tokens per second.
    Given this is just the beginning, it’s remarkable how fast we are able to see
    tokens generated on an iPhone with this model. It seems likely the speed of inference
    will only increase.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们在配备A16芯片的iPhone上运行了量化后的模型，发现该模型每秒能生成最多12个标记。作为对比，一台运行LLaMa-2 量化4位的M1 MacBook每秒大约生成107个标记。我见过的最快标记生成速度（Groq）为每秒853.35个标记。考虑到这才是刚刚起步，能够在这款模型上看到iPhone生成标记的速度已经非常惊人。推理速度似乎只会越来越快。
- en: Pairing Phi-3 with Search
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将Phi-3与搜索结合使用
- en: One limitation with a small model is it has fewer places it can store information
    within its network. As a result, we see that Phi-3 does not perform as well as
    models like LLaMa-2 on tasks that require wide scopes of knowledge.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 小型模型的一个局限性是它能够在网络中存储信息的位置较少。因此，我们看到Phi-3在需要广泛知识的任务上不如LLaMa-2等模型表现得那么好。
- en: The authors suggest that by pairing Phi-3 with a search engine the model’s abilities
    will significantly improve. If this is the case, that makes me think Retrieval
    Augmented Generation (RAG) is likely here to stay, becoming a critical part of
    helping small models be just as performant as larger ones.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 作者建议，将Phi-3与搜索引擎结合，模型的能力将得到显著提升。如果情况确实如此，这让我认为检索增强生成（RAG）可能会长期存在，成为帮助小型模型达到大型模型性能的关键部分。
- en: '![](../Images/bcd70a3a3381870eb2ac75afc9028abf.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bcd70a3a3381870eb2ac75afc9028abf.png)'
- en: Figure 4 [from the paper](https://arxiv.org/pdf/2404.14219) highlighting how
    search can improve Phi-3 performance
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图4 [来自论文](https://arxiv.org/pdf/2404.14219)，展示了搜索如何提高Phi-3的性能
- en: Conclusion
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In closing, we are seeing the beginning of highly performant smaller models.
    While training these models still relies to a large degree on performant hardware,
    inferencing them is increasingly becoming democratized. This introduces a few
    interesting phenomena.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们正在看到高性能小型模型的开端。虽然训练这些模型仍然在很大程度上依赖于高性能硬件，但它们的推理过程正变得越来越普及。这引入了一些有趣的现象。
- en: First, models that can run locally can be almost fully private, allowing users
    to give these LLMs data that they otherwise may not feel comfortable sending over
    the internet. This opens the door to more use cases.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，能够在本地运行的模型几乎可以完全保持私密性，允许用户向这些大型语言模型（LLM）提供他们可能不愿意通过互联网发送的数据。这为更多的使用场景打开了大门。
- en: Second, these models will drive mobile hardware to be even more performant.
    As a consequence, I would expect to see more Systems on Chips (SoC) on high-end
    smartphones, especially SoCs with shared memory between CPUs and GPUs to maximize
    the speed of inference. Moreover, the importance of having quality interfaces
    with this hardware will be paramount. Libraries like MLX for Apple Silicon will
    likely be required for any new hardware entrants in the consumer hardware space.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，这些模型将推动移动硬件的性能提升。因此，我预计高端智能手机上会有更多的系统级芯片（SoC），特别是具有共享内存的SoC，以便在CPU和GPU之间共享内存，最大化推理速度。此外，拥有高质量接口的硬件将变得至关重要。像MLX这样的库，专为苹果硅设计，可能会成为任何新硬件进入消费硬件市场的必需品。
- en: Third, as this paper shows that high quality data can in many ways outcompete
    more network complexity in an LLM, the race to not just find but generate high
    quality data will only increase.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，正如本文所示，高质量数据在许多方面可以超越LLM中的网络复杂性，因此，不仅仅是寻找，而是生成高质量数据的竞争将只会加剧。
- en: It is an exciting time to be building.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是构建的激动人心时刻。
- en: '[1] Abdin, M., et al. [“Phi-3 Technical Report: A Highly Capable Language Model
    Locally on Your Phone”](https://arxiv.org/pdf/2404.14219) (2024), arXiv'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Abdin, M., 等人 [“Phi-3技术报告：在手机本地运行的高性能语言模型”](https://arxiv.org/pdf/2404.14219)（2024年），arXiv'
- en: '[2] Ding, Y., et al. [“LongRoPE: Extending LLM Context Window Beyond 2 Million
    Tokens”](https://arxiv.org/pdf/2402.13753) (2024), arXiv'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Ding, Y., 等人 [“LongRoPE：将LLM上下文窗口扩展到超过200万个令牌”](https://arxiv.org/pdf/2402.13753)（2024年），arXiv'
- en: '[3] Gerganov, G., et al. [“Performance of llama.cpp on Apple Silicon M-series”](https://github.com/ggerganov/llama.cpp/discussions/4167)
    (2023), GitHub'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Gerganov, G., 等人 [“llama.cpp在苹果硅M系列上的性能”](https://github.com/ggerganov/llama.cpp/discussions/4167)（2023年），GitHub'
- en: '[4] Ainslie, J., et al. [“GQA: Training Generalized Multi-Query Transformer
    Models from Multi-Head Checkpoints”](https://arxiv.org/pdf/2305.13245) (2023),
    arXiv'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Ainslie, J., 等人 [“GQA：从多头检查点训练通用多查询变换器模型”](https://arxiv.org/pdf/2305.13245)（2023年），arXiv'
