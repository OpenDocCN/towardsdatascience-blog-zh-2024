- en: Hosting Multiple LLMs on a Single Endpoint
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在单一端点托管多个LLM
- en: 原文：[https://towardsdatascience.com/hosting-multiple-llms-on-a-single-endpoint-32eda0201832?source=collection_archive---------6-----------------------#2024-01-11](https://towardsdatascience.com/hosting-multiple-llms-on-a-single-endpoint-32eda0201832?source=collection_archive---------6-----------------------#2024-01-11)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/hosting-multiple-llms-on-a-single-endpoint-32eda0201832?source=collection_archive---------6-----------------------#2024-01-11](https://towardsdatascience.com/hosting-multiple-llms-on-a-single-endpoint-32eda0201832?source=collection_archive---------6-----------------------#2024-01-11)
- en: Utilize SageMaker Inference Components to Host Flan & Falcon in a Cost & Performance
    Efficient Manner
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用SageMaker推理组件，以高效的成本和性能方式托管Flan和Falcon
- en: '[](https://ram-vegiraju.medium.com/?source=post_page---byline--32eda0201832--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page---byline--32eda0201832--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--32eda0201832--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--32eda0201832--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page---byline--32eda0201832--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ram-vegiraju.medium.com/?source=post_page---byline--32eda0201832--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page---byline--32eda0201832--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--32eda0201832--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--32eda0201832--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page---byline--32eda0201832--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--32eda0201832--------------------------------)
    ·10 min read·Jan 11, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--32eda0201832--------------------------------)
    ·10分钟阅读·2024年1月11日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/58c1d77cfa743d90e720a09808a88264.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/58c1d77cfa743d90e720a09808a88264.png)'
- en: Image from [Unsplash](https://unsplash.com/photos/red-and-black-abstract-illustration-aQYgUYwnCsM)
    by [**Michael Dziedzic**](https://unsplash.com/@lazycreekimages)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Unsplash](https://unsplash.com/photos/red-and-black-abstract-illustration-aQYgUYwnCsM)
    作者：[**Michael Dziedzic**](https://unsplash.com/@lazycreekimages)
- en: The past year has witnessed an explosion in the Large Language Model (LLM) space
    with a number of new models paired with various technologies and tools to help
    train, host, and evaluate these models. Specifically, Hosting/Inference is where
    the power of these LLMs and Machine Learning in general is recognized, as without
    inference there is no visual result or purpose to these models.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 去年，随着多个新模型的推出，配合各种技术和工具帮助训练、托管和评估这些模型，大型语言模型（LLM）领域经历了一次爆炸式增长。具体来说，托管/推理是这些LLM以及机器学习技术在整体上被认可的地方，因为没有推理，这些模型就没有可视化的结果或实际意义。
- en: As I’ve documented in the past, [hosting these LLMs](https://aws.plainenglish.io/four-different-ways-to-host-large-language-models-on-amazon-sagemaker-4d1b027812b5)
    can be quite challenging due to the size of the model and utilizing the associated
    hardware behind a model efficiently. While we’ve worked with model serving technologies
    such as [DJL Serving](/deploying-llms-on-amazon-sagemaker-with-djl-serving-8220e3cfad0c),
    [Text Generation Inference (TGI)](/deploying-large-language-models-with-huggingface-tgi-981747c669e3),
    and [Triton](/deploying-pytorch-models-with-nvidia-triton-inference-server-bb139066a387)
    in conjunction with a model/infrastructure hosting platform such as Amazon SageMaker
    to be able to host these LLMs, another question arises as we try to productionize
    our LLM use-cases. **How we can we do this for multiple LLMs?**
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我之前所记录的那样，[托管这些LLM](https://aws.plainenglish.io/four-different-ways-to-host-large-language-models-on-amazon-sagemaker-4d1b027812b5)可能相当具有挑战性，主要因为模型的体积以及如何高效利用与模型相关的硬件。尽管我们已经使用了如[DJL
    Serving](/deploying-llms-on-amazon-sagemaker-with-djl-serving-8220e3cfad0c)、[文本生成推理（TGI）](/deploying-large-language-models-with-huggingface-tgi-981747c669e3)和[Triton](/deploying-pytorch-models-with-nvidia-triton-inference-server-bb139066a387)等模型服务技术，配合像Amazon
    SageMaker这样的模型/基础设施托管平台来托管这些LLM，但随着我们尝试将LLM的使用场景推向生产化，另一个问题随之而来。**我们如何能够在多个LLM上做到这一点？**
- en: Why does the initial question even arise? When we get to production level use-cases,
    its common to have multiple models that may be utilized. For instance, maybe a
    Llama model is used for your summarization use-case, while a Falcon model is powering
    your chatbot. While…
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么最初的问题会出现呢？当我们进入生产级别的使用场景时，通常会有多个模型可以使用。例如，可能在你的摘要任务中使用 Llama 模型，而在你的聊天机器人中使用
    Falcon 模型。虽然…
