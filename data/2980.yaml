- en: 'Understanding DDPG: The Algorithm That Solves Continuous Action Control Challenges'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 DDPG：解决连续动作控制挑战的算法
- en: 原文：[https://towardsdatascience.com/understanding-ddpg-the-algorithm-that-solves-continuous-action-control-challenges-742c67e0783a?source=collection_archive---------2-----------------------#2024-12-11](https://towardsdatascience.com/understanding-ddpg-the-algorithm-that-solves-continuous-action-control-challenges-742c67e0783a?source=collection_archive---------2-----------------------#2024-12-11)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/understanding-ddpg-the-algorithm-that-solves-continuous-action-control-challenges-742c67e0783a?source=collection_archive---------2-----------------------#2024-12-11](https://towardsdatascience.com/understanding-ddpg-the-algorithm-that-solves-continuous-action-control-challenges-742c67e0783a?source=collection_archive---------2-----------------------#2024-12-11)
- en: Discover how DDPG solves the puzzle of continuous action control, unlocking
    possibilities in AI-driven medical robotics.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发现 DDPG 如何解决连续动作控制难题，释放人工智能驱动的医疗机器人的可能性。
- en: '[](https://medium.com/@sirinebhouri?source=post_page---byline--742c67e0783a--------------------------------)[![Sirine
    Bhouri](../Images/ae904be69cb3ca9bb39185aaa7be4233.png)](https://medium.com/@sirinebhouri?source=post_page---byline--742c67e0783a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--742c67e0783a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--742c67e0783a--------------------------------)
    [Sirine Bhouri](https://medium.com/@sirinebhouri?source=post_page---byline--742c67e0783a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@sirinebhouri?source=post_page---byline--742c67e0783a--------------------------------)[![Sirine
    Bhouri](../Images/ae904be69cb3ca9bb39185aaa7be4233.png)](https://medium.com/@sirinebhouri?source=post_page---byline--742c67e0783a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--742c67e0783a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--742c67e0783a--------------------------------)
    [Sirine Bhouri](https://medium.com/@sirinebhouri?source=post_page---byline--742c67e0783a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--742c67e0783a--------------------------------)
    ·10 min read·Dec 11, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--742c67e0783a--------------------------------)
    ·阅读 10 分钟·2024 年 12 月 11 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'Imagine you’re controlling a roboticarm in a surgical procedure. Discrete actions
    might be:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下你在外科手术中控制机械手臂。离散动作可能包括：
- en: '**Move up,**'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**向上移动,**'
- en: '**Move down**,'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**向下移动**,'
- en: '**Grab**, or'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**抓取**，或'
- en: '**Release**'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**发布**'
- en: These are clear, direct commands, easy to execute in simple scenarios.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是清晰、直接的命令，在简单场景中易于执行。
- en: 'But what about performing delicate movements, such as:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如何执行精细的动作，比如：
- en: '**Move the arm by 0.5 mm to avoid damaging the tissue**,'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将手臂移动 0.5 毫米以避免损伤组织**,'
- en: '**Apply a force of 3N for tissue compression**, or'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**施加 3N 的力以压缩组织**，或'
- en: '**Rotate the wrist by 15° to adjust the incision angle**?'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将手腕旋转 15° 调整切口角度**？'
- en: In these situations, you need more than just choosing an action — you must decide
    *how much* of that action is needed. This is the world of **continuous action
    spaces**, and this is where Deep Deterministic Policy Gradient **(DDPG)** shines!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，你需要的不仅仅是选择一个动作 —— 你必须决定*需要多少*的那个动作。这就是**连续动作空间**的世界，也是深度确定性策略梯度 **(DDPG)**
    的闪亮之处！
- en: Traditional methods like Deep Q-Networks (**DQN**) work well with discrete actions
    but struggle with continuous ones. DeterministicPolicyGradient **(DPG)** on the
    other hand, tackled this issue but faced challenges with poor exploration and
    instability. **DDPG** which wasfirst introduced in T P. Lillicrap et al’s [paper](https://arxiv.org/pdf/1509.02971v6)
    **combines** the **strengths** of **DPG** and **DQN** to **improve stability**
    and **performance** in environments with **continuous action spaces.**
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 传统方法如深度 Q 网络（**DQN**）在离散动作方面表现良好，但在连续动作方面表现不佳。而确定性策略梯度 **(DPG)** 则解决了这个问题，但面临着探索不足和不稳定性的挑战。**DDPG**
    首次在 T P. Lillicrap 等人的[论文](https://arxiv.org/pdf/1509.02971v6)中**结合了** **DPG**
    和 **DQN** 的**优势**，以**提高**在**连续动作空间**环境中的**稳定性**和**性能**。
- en: In this post, we will discuss the theory and architecture behind DDPG, look
    at an implementation of it on Python, evaluate its performance (by testing it
    on MountainCarContinuous game) and briefly discuss how DDPG can be used in the
    bioengineering field.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将讨论 DDPG 背后的理论和架构，看看在 Python 上的实现，评估其性能（通过在 MountainCarContinuous 游戏上测试），并简要讨论
    DDPG 如何在生物工程领域中使用。
- en: '**DDPG Architecture**'
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**DDPG 架构**'
- en: Unlike **DQN**, which evaluates every possible state-action pair to find the
    best action (impossible in continuous spaces due to infinite combinations), **DPG**
    uses an **Actor-Critic architecture**. The Actor learns a policy that directly
    maps states to actions, avoiding exhaustive searches and focusing on learning
    the best action for each state.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 与**DQN**不同，后者评估每一个可能的状态-动作对来寻找最佳动作（由于连续空间中组合的无限性，这在连续空间中不可能实现），**DPG**使用**行动者-评论家架构**。行动者学习一种策略，将状态直接映射到动作，避免了穷举搜索，专注于学习每个状态下的最佳动作。
- en: 'However, DPG faces two main challenges:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，DPG面临两个主要挑战：
- en: It is a **deterministic** algorithm which **limits exploration** of the action
    space.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它是一个**确定性**算法，**限制了对动作空间的探索**。
- en: It cannot use neural networks effectively due to **instability** in the learning
    process.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于学习过程中的**不稳定性**，它无法有效使用神经网络。
- en: '**DDPG** improves DPG by introducing **exploration noise** via the Ornstein-Uhlenbeck
    process and stabilising training with **Batch Normalisation** and DQN techniques
    like **Replay Buffer** and **Target Networks**.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**DDPG**通过引入**探索噪声**（通过奥恩斯坦-乌伦贝克过程）并通过**批量归一化**和DQN技术（如**重放缓冲区**和**目标网络**）来改进DPG，从而稳定训练过程。'
- en: With these enhancements, DDPG is well-suited to train agents in continuous action
    spaces, such as controlling robotic systems in bioengineering applications.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些改进，DDPG非常适合训练连续动作空间中的智能体，例如在生物工程应用中控制机器人系统。
- en: Now, let’s explore the key components of the DDPG model!
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来探讨一下DDPG模型的关键组件吧！
- en: '**Actor-Critic Framework**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**行动者-评论家框架**'
- en: '**Actor (Policy Network)**: Tells the agent which action to take given the
    state it is in. The network’s parameters (i.e. weights) are represented by θμ.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**行动者（策略网络）**：告诉智能体在给定的状态下应该采取什么动作。网络的参数（即权重）由θμ表示。'
- en: '![](../Images/6ae607f0b6394ea2af1bc4e008128a96.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6ae607f0b6394ea2af1bc4e008128a96.png)'
- en: '**Tip!** Think of the Actor Network as the decision-maker: it maps the current
    state to a single action.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示！** 把行动者网络想象成决策者：它将当前状态映射到一个单一的动作。'
- en: '**Critic (Q-value Network)**: Evaluates how good the action taken by the actor
    by estimating the Q-value of that state-action pair.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评论家（Q值网络）**：通过估计该状态-动作对的Q值来评估行动者所采取的动作的优劣。'
- en: '![](../Images/095e9c51285694906190157f570dd843.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/095e9c51285694906190157f570dd843.png)'
- en: '**Tip**! Think of the Critic Network as the evaluator, it assigns a quality
    score to each action and helps improve the Actor’s policy to make sure it indeed
    generates the best action to take in each given state.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示！** 把评论家网络想象成评估者，它为每个动作分配一个质量分数，并帮助改进行动者的策略，确保其在每个给定的状态下确实生成最佳动作。'
- en: '**Note**! The critic will use the estimated Q-value for two things:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**！评论家将使用估计的Q值来做两件事：'
- en: To improve the Actor’s policy (Actor Policy Update).
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 改进行动者的策略（行动者策略更新）。
- en: The Actor’s goal is to adjust its parameters (θμ) so that it outputs actions
    that maximise the critic’s Q-value.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 行动者的目标是调整其参数（θμ），使其输出的动作最大化评论家的Q值。
- en: 'To do so, the Actor needs to understand both how the selected action a affects
    the Critic’s Q-value and how its internal parameters affect its Policy which is
    done through this Policy Gradient equation (it is the mean of all the gradients
    calculated from the mini-batch):'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，行动者需要理解选定的动作a如何影响评论家的Q值，以及其内部参数如何影响其策略，这通过这个策略梯度方程来完成（它是从小批量中计算出的所有梯度的均值）：
- en: '![](../Images/1ef06bfb2a896ecd595c0da22b4ff7e1.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ef06bfb2a896ecd595c0da22b4ff7e1.png)'
- en: '**2.** To improve its own network (Critic Q-value Network Update) by minimising
    the loss function below.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**2.** 通过最小化下面的损失函数来改进其自身的网络（评论家Q值网络更新）。'
- en: '![](../Images/eb44da31f91612d3172e85cec236c8bb.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb44da31f91612d3172e85cec236c8bb.png)'
- en: Where N is the number of experiences sampled in the mini-batch and y_i is the
    target Q-value calculated as follows.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 其中N是小批量中采样的经验数量，y_i是目标Q值，计算方法如下。
- en: '![](../Images/c534b1294b3cfbefd8bf06b9da2bb423.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c534b1294b3cfbefd8bf06b9da2bb423.png)'
- en: '**Replay Buffer**'
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**重放缓冲区**'
- en: As the agent explores the environment, past experiences (state, action, reward,
    next state) are stored as tuples (s, a, r, s′) in the replay buffer. During training,
    mini-batches consisting of some of these experiences are then randomly sampled
    to train the agent.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当智能体探索环境时，过去的经验（状态、动作、奖励、下一个状态）作为元组（s，a，r，s′）存储在重放缓冲区中。在训练过程中，这些经验中的一些会随机组成小批量进行训练。
- en: '**Question!** How does replay buffer *actually* reduce instability?'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题！** 重放缓冲区*如何*减少不稳定性？'
- en: By randomly sampling experiences, the replay buffer breaks the correlation between
    consecutive samples, reducing bias and leading to more stable training.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 通过随机抽样经验，回放缓冲区打破了连续样本之间的相关性，减少了偏差，并导致更稳定的训练。
- en: '**Target Networks**'
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**目标网络**'
- en: Target Networks are slowly updated copies of the Actor and Critic. They provide
    stable Q-value targets, preventing rapid changes and ensuring smooth, consistent
    updates.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 目标网络是演员和评论家的缓慢更新副本。它们提供稳定的Q值目标，防止快速变化，并确保平稳、一致的更新。
- en: '![](../Images/ad2698b89ffe352ffc56ed2573b32f88.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad2698b89ffe352ffc56ed2573b32f88.png)'
- en: '**Question!** How do target networks *actually* reduce instability?'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题！** 目标网络是如何*实际*减少不稳定性的？'
- en: Without the **Critic target network**, the target Q-value is calculated directly
    from the Critic Q-value network, which is updated continuously. This causes the
    target Q-value to shift at each step, creating a “moving target” problem. As a
    result, the Critic ends up chasing a constantly changing target, making training
    unstable.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有**评论家目标网络**，目标Q值将直接从评论家Q值网络计算，而评论家Q值网络是持续更新的。这会导致目标Q值在每一步发生变化，造成“移动目标”问题。结果，评论家不断追逐一个不断变化的目标，导致训练不稳定。
- en: Additionally, since the **Actor** relies on the Critic’s feedback, errors in
    one network can amplify errors in the other, creating an interdependent loop of
    instability.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于**演员**依赖于评论家的反馈，一个网络中的错误可能会放大另一个网络中的错误，形成一个相互依赖的稳定性问题。
- en: By introducing **target networks** that are updated gradually with a soft update
    rule, we ensure the target Q-value remains more consistent, reducing abrupt changes
    and improving learning stability.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通过引入**目标网络**，并使用软更新规则逐步更新它们，我们确保目标Q值保持更一致，减少突变变化，从而提高学习的稳定性。
- en: '**Batch Normalisation**'
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**批量归一化**'
- en: Batch Normalisation standardises the inputs to each layer of the neural network,
    ensuring mean of zero and a unit variance.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化对神经网络每一层的输入进行标准化，确保均值为零，方差为单位。
- en: '**Question!** How does batch normalisation *actually* reduce instability?'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题！** 批量归一化是如何*实际*减少不稳定性的？'
- en: Samples drawn from the replay buffer may have different distributions than real-time
    data, leading to instability during network updates.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 从回放缓冲区抽取的样本可能与实时数据的分布不同，从而导致网络更新时的不稳定性。
- en: Batch normalisation ensures consistent scaling of inputs to prevent erratic
    updates caused by varying input distributions.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化确保输入的尺度一致，以防止由输入分布变化引起的更新不稳定。
- en: '**Exploration Noise**'
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**探索噪声**'
- en: Since the Actor’s policy is deterministic, exploration noise is added to actions
    during training to encourage the agent to explore the as much of the action space
    as possible.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 由于演员的策略是确定性的，训练过程中会向动作添加探索噪声，鼓励智能体尽可能多地探索动作空间。
- en: '![](../Images/f8d1d609db7533482e5a40ac6532ab59.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8d1d609db7533482e5a40ac6532ab59.png)'
- en: On the DDPG publication, the authors used the **Ornstein-Uhlenbeck process**
    to generate temporally correlated noise, in order to mimick real-world system
    dynamics.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在DDPG论文中，作者使用了**奥恩斯坦-乌伦贝克过程**来生成时间相关噪声，以模拟现实世界的系统动态。
- en: 'DDPG Pseudocode: A Step-by-Step Breakdown'
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DDPG伪代码：逐步拆解
- en: '![](../Images/67ae26120aeeafaba2fbe7b7b9d7ef7d.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67ae26120aeeafaba2fbe7b7b9d7ef7d.png)'
- en: Pseudocode taken from [http://arxiv.org/abs/1509.02971](http://arxiv.org/abs/1509.02971)
    (see reference 1 in ‘References’ section)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 伪代码摘自[http://arxiv.org/abs/1509.02971](http://arxiv.org/abs/1509.02971)（见‘参考文献’部分的参考文献1）
- en: '![](../Images/5490d989d60df6c77b8e3d32123600df.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5490d989d60df6c77b8e3d32123600df.png)'
- en: Diagram drawn by author
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图示由作者绘制
- en: '**Define Actor and Critic Networks**'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义演员和评论家网络**'
- en: '[PRE0]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Define Replay Buffer**'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义回放缓冲区**'
- en: A ReplayBuffer class is implemented to store and sample the transition tuples
    (s, a, r, s’) discussed in the previous section to enable mini-batch off-policy
    learning.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 实现了一个ReplayBuffer类，用于存储和抽样前一节中讨论的过渡元组(s, a, r, s')，以支持小批量的离策略学习。
- en: '[PRE1]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Define OU Noise class**'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义OU噪声类**'
- en: An OUNoise class is added to generate exploration noise, helping the agent explore
    the action space more effectively.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 添加了一个OUNoise类来生成探索噪声，帮助智能体更有效地探索动作空间。
- en: '[PRE2]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Define the DDPG agent**'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义DDPG智能体**'
- en: 'A DDPG class was defined and it encapsulates the agent’s behavior:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了一个DDPG类，它封装了智能体的行为：
- en: 'Initialisation: Creates Actor and Critic networks, along with their target
    counterparts and the replay buffer.'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化：创建演员和评论家网络，以及它们的目标副本和回放缓冲区。
- en: '[PRE3]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '2\. Action Selection: The `select_action` method chooses actions based on the
    current policy.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 动作选择：`select_action`方法根据当前策略选择动作。
- en: '[PRE4]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '3\. Training: The `train` method defines how the networks are updated using
    experiences from the replay buffer.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3\. 训练：`train`方法定义了如何使用来自重放缓冲区的经验更新网络。
- en: '**Note!** Since the paper introduced the use of target networks and batch normalisation
    to improve stability, I designed the `train` method to allow us to toggle these
    methods on or off. This lets us compare the agent’s performance with and without
    them. See code below for exact implementation.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意！** 由于论文介绍了使用目标网络和批量归一化来提高稳定性，我设计了`train`方法，允许我们切换这些方法的开关。这让我们可以比较智能体在有这些方法和没有这些方法时的表现。具体实现请见下面的代码。'
- en: '[PRE5]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Train the DDPG agent**'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练DDPG智能体**'
- en: Bringing all the defined classes and methods together, we can train the DDPG
    agent. My `train_dppg` function follows the pseudocode and DDPG model diagram
    structure.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有定义的类和方法整合在一起，我们可以训练DDPG智能体。我的`train_dppg`函数遵循伪代码和DDPG模型图的结构。
- en: '**Tip:** To make it easier for you to understand, I’ve labeled each code section
    with the corresponding step number from both the pseudocode and diagram. Hope
    that helps! :)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示：** 为了便于理解，我已经在每个代码部分标注了与伪代码和图表中对应步骤的编号。希望这能有所帮助！ :)'
- en: '[PRE6]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Performance and Results: Evaluating DDPG’s Effectiveness'
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能与结果：评估DDPG的有效性
- en: DDPG’s effectiveness in a continuous action space was tested in the `MountainCarContinuous-v0`
    environment, where the agent learns to where the agent learns to gain momentum
    to drive the car up a steep hill. The results show that using **Target Networks**
    and **Batch Normalisation** leads to faster convergence, higher rewards, and more
    stable learning than other configurations.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG在连续动作空间中的有效性是在`MountainCarContinuous-v0`环境中进行测试的，其中智能体学习如何获得动量，将汽车驶上陡坡。结果表明，使用**目标网络**和**批量归一化**比其他配置能更快地收敛，获得更高的奖励，并且学习更加稳定。
- en: '![](../Images/3126a241c28e6b5fd6c59ec4cf1380f1.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3126a241c28e6b5fd6c59ec4cf1380f1.png)'
- en: Graph generated by author
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图表由作者生成
- en: '![](../Images/5dafde30d56caffaaeb03e189a322450.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5dafde30d56caffaaeb03e189a322450.png)'
- en: GIF generated by author
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: GIF由作者生成
- en: '**Note!** You can implement this yourself on any environment of your choice
    by running the code which can be found on my [GitHub](https://github.com/sirine-b/DDPG)
    as is and simply changing the environment’s name as needed!'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意！** 你可以通过在我的[GitHub](https://github.com/sirine-b/DDPG)上运行代码来自己实现这一点，只需根据需要更改环境名称即可！'
- en: 'DDPG in Bioengineering: Unlocking Precision and Adaptability'
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DDPG在生物工程中的应用：解锁精确性与适应性
- en: Through this blog post, we’ve seen that DDPG is a powerful algorithm for training
    agents in environments with continuous action spaces. By combining techniques
    from both DPG and DQN, DDPG improves exploration, stability, and performance —
    key factors for applications in robotic surgery and bioengineering.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这篇博客文章，我们看到了DDPG是一个强大的算法，可以在具有连续动作空间的环境中训练智能体。通过结合DPG和DQN的技术，DDPG提高了探索性、稳定性和性能——这些都是机器人手术和生物工程应用中的关键因素。
- en: Imagine a robotic surgeon, like the **da Vinci system**, using DDPG to control
    fine movements in real-time, ensuring **precise adjustments** without any errors.
    With DDPG, the robot could adjust its arm’s position by millimeters, apply exact
    force when suturing, or even make slight wrist rotations for an optimal incision.
    Such **real-time precision** could transform surgical outcomes, reduce recovery
    time, and minimise human error.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，一位机器人外科医生，像**达芬奇系统**那样，使用DDPG来实时控制精细的运动，确保**精确调整**而不出错。通过DDPG，机器人可以将手臂的位置调整到毫米级别，在缝合时施加精确的力量，甚至在做出最佳切口时进行微小的手腕旋转。这样的**实时精确性**可以改变手术结果，减少恢复时间，并最小化人为错误。
- en: But DDPG’s potential goes beyond surgery. It’s already advancing bioengineering,
    enabling robotic prosthetics and assistive devices to replicate the natural motion
    of human limbs (check out this super interesting [article](https://www.tandfonline.com/doi/abs/10.1080/00207179.2023.2201644)!).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 但DDPG的潜力不仅仅局限于手术。它已经在推进生物工程，能够使机器人假肢和辅助设备复制人体肢体的自然运动（查看这篇超级有趣的[文章](https://www.tandfonline.com/doi/abs/10.1080/00207179.2023.2201644)!）。
- en: Now that we’ve covered the theory behind DDPG, it’s time for you to explore
    its implementation. Start with simple examples and gradually dive into more complex
    scenarios!
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了DDPG背后的理论，是时候让你探索它的实现了。从简单的例子开始，逐步深入到更复杂的场景中吧！
- en: '**References**'
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: 'Lillicrap TP, Hunt JJ, Pritzel A, Heess N, Erez T, Tassa Y, et al. Continuous
    control with deep reinforcement learning [Internet]. arXiv; 2019\. Available from:
    [http://arxiv.org/abs/1509.02971](http://arxiv.org/abs/1509.02971)'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Lillicrap TP, Hunt JJ, Pritzel A, Heess N, Erez T, Tassa Y, 等人. 基于深度强化学习的连续控制
    [Internet]. arXiv; 2019\. 可从以下网址获取: [http://arxiv.org/abs/1509.02971](http://arxiv.org/abs/1509.02971)'
