- en: 'Case-Study: Multilingual LLM for Questionnaire Summarization'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究：多语言LLM用于问卷总结
- en: 原文：[https://towardsdatascience.com/case-study-multilingual-llm-for-questionnaire-summarization-edf7acdcb37c?source=collection_archive---------5-----------------------#2024-07-30](https://towardsdatascience.com/case-study-multilingual-llm-for-questionnaire-summarization-edf7acdcb37c?source=collection_archive---------5-----------------------#2024-07-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/case-study-multilingual-llm-for-questionnaire-summarization-edf7acdcb37c?source=collection_archive---------5-----------------------#2024-07-30](https://towardsdatascience.com/case-study-multilingual-llm-for-questionnaire-summarization-edf7acdcb37c?source=collection_archive---------5-----------------------#2024-07-30)
- en: An LLM Approach to Summarizing Students’ Responses for Open-ended Questionnaires
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于LLM的学生开放式问卷回答总结方法
- en: '[](https://medium.com/@sria.louis?source=post_page---byline--edf7acdcb37c--------------------------------)[![Sria
    Louis](../Images/d65b17e9d4ace7e0222118abc70f3954.png)](https://medium.com/@sria.louis?source=post_page---byline--edf7acdcb37c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--edf7acdcb37c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--edf7acdcb37c--------------------------------)
    [Sria Louis](https://medium.com/@sria.louis?source=post_page---byline--edf7acdcb37c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@sria.louis?source=post_page---byline--edf7acdcb37c--------------------------------)[![Sria
    Louis](../Images/d65b17e9d4ace7e0222118abc70f3954.png)](https://medium.com/@sria.louis?source=post_page---byline--edf7acdcb37c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--edf7acdcb37c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--edf7acdcb37c--------------------------------)
    [Sria Louis](https://medium.com/@sria.louis?source=post_page---byline--edf7acdcb37c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--edf7acdcb37c--------------------------------)
    ·10 min read·Jul 30, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--edf7acdcb37c--------------------------------)
    ·10分钟阅读·2024年7月30日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/cbbea877f5f00d69ee043faac8a55599.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cbbea877f5f00d69ee043faac8a55599.png)'
- en: 'Illustration: Or Livneh'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 插图：Or Livneh
- en: '[Madrasa (מדרסה in Hebrew)](https://madrasafree.com/) is an Israeli NGO dedicated
    to teaching Arabic to Hebrew speakers. Recently, while learning Arabic, I discovered
    that the NGO has unique data and that the organization might benefit from a thorough
    analysis. A friend and I joined the NGO as volunteers, and we were asked to work
    on the summarization task described below.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[Madrasa (希伯来语中的מדרסה)](https://madrasafree.com/)是一个以色列的非政府组织，致力于向希伯来语使用者教授阿拉伯语。最近，在学习阿拉伯语时，我发现这个非政府组织拥有独特的数据，而该组织可能从深入分析中受益。我和一位朋友作为志愿者加入了该组织，我们被要求处理下面描述的总结任务。'
- en: What makes this summarization task so interesting is the unique mix of documents
    in three languages — Hebrew, Arabic, and English — while also dealing with the
    imprecise [transcriptions](https://en.wikipedia.org/wiki/Transcription_(linguistics))
    among them.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 使这项总结任务如此有趣的是，文档包含三种语言——希伯来语、阿拉伯语和英语——而且还需要处理它们之间不准确的[转录](https://en.wikipedia.org/wiki/Transcription_(linguistics))。
- en: 'A word on privacy: The data may include PII and therefore cannot be published
    at this time. If you believe you can contribute, please contact us.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 关于隐私：数据可能包含个人身份信息（PII），因此目前无法公开。如果您认为自己能够提供帮助，请联系我们。
- en: Context of the Problem
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题背景
- en: As part of its language courses, Madrasa distributes questionnaires to students,
    which include both quantitative questions requiring numeric responses and open-ended
    questions where students provide answers in natural language.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 作为其语言课程的一部分，Madrasa向学生分发问卷，其中包括需要数字回答的定量问题和要求学生用自然语言回答的开放式问题。
- en: In this blog post, we will concentrate on the open-ended natural language responses.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我们将集中讨论开放式自然语言回答。
- en: The Problem
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题描述
- en: 'The primary challenge is managing and extracting insights from a substantial
    volume of responses to open-ended questions. Specifically, the difficulties include:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 主要挑战是管理和从大量开放式问题的回答中提取洞见。具体而言，困难包括：
- en: '**Multilingual Responses**: Student responses are primarily in Hebrew but also
    include Arabic and English, creating a complex multilingual dataset. Additionally,
    since transliteration is commonly used in Spoken Arabic courses, we found that
    students sometimes answered questions using both transliteration and Arabic script.
    We were surprised to see that some students even transliterated Hebrew and Arabic
    into Latin letters.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**多语种回应**：学生的回答主要是希伯来语，但也包含阿拉伯语和英语，形成了一个复杂的多语种数据集。此外，由于阿拉伯语口语课程中常用音译，我们发现学生有时会使用音译和阿拉伯字母书写两种方式回答问题。令我们惊讶的是，部分学生甚至将希伯来语和阿拉伯语音译成拉丁字母。'
- en: '**Nuanced Sentiments**: The responses vary widely in sentiment and tone, including
    humor, suggestions, gratitude, and personal reflections.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**细腻的情感**：学生的回答情感和语气变化很大，包括幽默、建议、感激和个人反思等。'
- en: '**Diverse Topics**: Students touch on a wide range of subjects, from praising
    teachers to reporting technical issues with the website and app, to personal aspirations.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**多样化的主题**：学生们涉及了从表扬教师到报告网站和应用程序的技术问题，再到个人抱负等广泛的主题。'
- en: '**The Data**'
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**数据**'
- en: There are couple of courses. Each course includes three questionnaires administered
    at the beginning, middle, and end of the course. Each questionnaire contains a
    few open-ended questions.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 课程数量不多。每门课程包含三份问卷，分别在课程的开始、中期和结束时发放。每份问卷包含几个开放性问题。
- en: The tables below provides examples of two questions along with a curated selection
    of student responses.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 下表提供了两个问题的示例以及精选的学生回答。
- en: '![](../Images/f88ecea943d4381a5ff4241b367c0428.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f88ecea943d4381a5ff4241b367c0428.png)'
- en: 'Example of a Question and Student Responses. LEFT: Original question and student
    responses. RIGHT: Translation into English for the blog post reader. Note the
    mix of languages, including Arabic-to-Hebrew transliteration, the variety of topics
    even within and the same sentences, and the different language registers. . Credit:
    Sria Louis / Madarsa'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 问题和学生回答的示例。左侧：原始问题和学生回答。右侧：为博客读者翻译成英文的内容。注意语言的混合，包括阿拉伯语到希伯来语的音译，即使在同一句话中也涉及多种主题，且使用了不同的语言风格。来源：Sria
    Louis / Madarsa
- en: '![](../Images/7feb7527f2a36be8c1ca11edf55634f3.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7feb7527f2a36be8c1ca11edf55634f3.png)'
- en: 'Example of a question and student responses. LEFT: Original question and student
    responses. RIGHT: Translation into English for the blog post reader. Note the
    mix of languages and transliterations, including both English-to-Hebrew and Hebrew-to-English.
    Credit: Sria Louis / Madarsa'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个问题和学生回答的示例。左侧：原始问题和学生回答。右侧：为博客读者翻译成英文的内容。注意语言的混合和音译，包括从英语到希伯来语以及从希伯来语到英语的音译。来源：Sria
    Louis / Madarsa
- en: There are tens of thousands of student responses for each question, and after
    splitting into sentences (as described below), there can be up to around 100,000
    sentences per column. This volume is manageable, allowing us to work locally.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 每个问题都有成千上万的学生回答，且在分句后（如下所述），每列最多可能有约100,000个句子。这一数据量是可管理的，可以在本地进行处理。
- en: Our goal is to summarize student opinions on various topics for each course,
    questionnaire, and open-ended question. We aim to capture the “main opinions”
    of the students while ensuring that “niche opinions” or “valuable insights” provided
    by individual students are not overlooked.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是总结学生在各个主题上的意见，涵盖每门课程、问卷和开放性问题。我们旨在捕捉学生的“主要观点”，同时确保不忽视个别学生提供的“小众意见”或“有价值的见解”。
- en: The Solution
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: To tackle challenges mention above, we implemented a multi-step natural language
    processing (NLP) solution.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决上述挑战，我们实施了一种多步骤的自然语言处理（NLP）解决方案。
- en: 'The process pipeline involves:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 处理流程包括：
- en: Sentence Tokenization (using NLTK Sentence Tokenizer)
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 句子分词（使用NLTK句子分词器）
- en: Topic Modeling (using BERTopic)
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主题建模（使用BERTopic）
- en: Topic representation (using BERTopic + LLM)
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主题表示（使用BERTopic + LLM）
- en: Batch summarizing (LLM with mini-batch fitting into the context-size)
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 批量总结（使用LLM与迷你批次适应上下文大小）
- en: Re-summarizing the batches to create a final comprehensive summary.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新总结批次以创建最终的综合总结。
- en: '**Sentence Tokenization:** We use NLTK to divide student responses into individual
    sentences. This process is crucial because student inputs often cover multiple
    topics within a single response. For example, a student might write, “The teacher
    used day-to-day examples. The games on the app were very good.” Here, each sentence
    addresses a different aspect of their experience. While sentence tokenization
    sometimes results in the loss of context due to cross-references between sentences,
    it generally enhances the overall analysis by breaking down responses into more
    manageable and topic-specific units. This approach has proven to significantly
    improve the end results.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**句子分割：** 我们使用NLTK将学生的回答分割成单独的句子。这个过程至关重要，因为学生的回答通常在一个句子中涉及多个话题。例如，一个学生可能写道：“老师使用了日常生活中的例子。应用程序中的游戏非常好。”这里，每个句子讨论了他们体验的不同方面。尽管句子分割有时会由于句子间的交叉引用而导致上下文丢失，但通常通过将回答拆分为更易管理且具有特定主题的单元，它能增强整体分析效果。这个方法已被证明显著提高了最终结果。'
- en: NLTK’s Sentence Tokenizer (`[nltk.tokenize.sent_tokenize](https://www.nltk.org/api/nltk.tokenize.sent_tokenize.html)`)
    splits documents into sentences using linguistics rules and models to identify
    sentence boundaries. The default English model worked well for our use case.
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: NLTK的句子分割器（`[nltk.tokenize.sent_tokenize](https://www.nltk.org/api/nltk.tokenize.sent_tokenize.html)`）使用语言学规则和模型来分割文档为句子，并确定句子的边界。默认的英语模型适用于我们的用例。
- en: '**Topic Modeling with BERTopic:** We utilized BERTopic to model the topics
    of the tokenized sentences, identify underlying themes, and assign a topic to
    each sentence. This step is crucial before summarization for several reasons.
    First, the variety of topics within the student responses is too vast to be handled
    effectively without topic modeling. By splitting the students’ answers into topics,
    we can manage and batch the data more efficiently, leading to improved performance
    during analysis. Additionally, topic modeling ensures that niche topics, mentioned
    by only a few students, do not get overshadowed by mainstream topics during the
    summarization process.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用BERTopic进行主题建模：** 我们利用BERTopic对分割后的句子进行主题建模，识别潜在主题，并为每个句子分配一个主题。在摘要之前，这一步骤非常关键，原因有几个。首先，学生回答中涉及的主题种类繁多，若没有主题建模，处理起来会非常困难。通过将学生的回答按主题拆分，我们可以更有效地管理和批量处理数据，从而提升分析性能。此外，主题建模确保了那些仅由少数学生提到的小众主题，在摘要过程中不会被主流话题所掩盖。'
- en: '[BERTopic](https://maartengr.github.io/BERTopic/index.html) is an elegant topic-modeling
    tool that embeds documents into vectors, clusters them, and models each cluster’s
    representation. Its key advantage is modularity, which we utilize for Hebrew embeddings
    and hyperparameter tuning.'
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[BERTopic](https://maartengr.github.io/BERTopic/index.html)是一个优雅的主题建模工具，它将文档嵌入向量，进行聚类，并为每个聚类建模其表示。它的主要优势在于模块化，我们利用这一优势进行希伯来语嵌入和超参数调优。'
- en: The BERTopic configuration was meticulously designed to address the multilingual
    nature of the data and the specific nuances of the responses, thereby enhancing
    the accuracy and relevance of the topic assignment.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: BERTopic的配置经过精心设计，以应对数据的多语言特性和回答中的特定细微差别，从而提高了主题分配的准确性和相关性。
- en: Specifically, note that we used a [Hebrew Sentence-embedding model](https://huggingface.co/imvladikon/sentence-transformers-alephbert).
    We did consider using an embedding on word-level, but the sentence-embedding proved
    to be capturing the needed information.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，请注意我们使用了[希伯来语句子嵌入模型](https://huggingface.co/imvladikon/sentence-transformers-alephbert)。我们曾考虑使用词级嵌入，但句子嵌入证明能够捕捉到所需的信息。
- en: For dimension-reduction and clustering we used BERTopic standard models [UMAP](https://umap-learn.readthedocs.io/en/latest/)
    and HDBSCAN, respectively, and with some hyper-parameter fine tuning the results
    satisfied us.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在降维和聚类方面，我们分别使用了BERTopic标准模型[UMAP](https://umap-learn.readthedocs.io/en/latest/)和HDBSCAN，并通过一些超参数调优，最终的结果令我们满意。
- en: '[Here’s a fantastic talk on HDBSCAN](https://youtu.be/dGsxd67IFiU?si=CrAHWrXgLnq6-3ul)
    by John Healy, one of the authors. It’s not just very educational; the speaker
    is really funny and witty! Definitely worth a watch :)'
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[这里有一场关于HDBSCAN的精彩演讲](https://youtu.be/dGsxd67IFiU?si=CrAHWrXgLnq6-3ul)，由作者之一John
    Healy讲解。这不仅是一次非常有教育意义的演讲，讲者也非常幽默和机智！绝对值得一看 :)'
- en: BERTopic has excellent documentation and a supportive community, so I’ll share
    a code snippet to show how easy it is to use with advanced models. More importantly,
    we want to emphasize some hyperparameter choices designed to achieve high cluster
    granularity and allow smaller topics. Remember that our goal is not only to summarize
    the “mainstream” ideas that most students agree upon but also to highlight nuanced
    perspectives and rarer students’ suggestions. This approach comes with the trade-off
    of slower processing and the risk of having too many topics, but managing ~40
    topics is still feasible.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: BERTopic 拥有出色的文档和支持社区，因此我将分享一个代码片段，展示它如何与高级模型配合使用。更重要的是，我们要强调一些超参数选择，旨在实现高聚类粒度并允许更小的主题。请记住，我们的目标不仅是总结大多数学生认同的“主流”观点，还要突出那些更为细微的视角和较为罕见的学生建议。这种方法的代价是处理速度较慢，并且可能会导致主题过多，但管理大约
    40 个主题仍是可行的。
- en: '**UMAP dimension reduction**: higher-than-standard number of components and
    small number of UMAP-neighbors.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**UMAP 降维**：高于标准的组件数量和较小的 UMAP 邻居数量。'
- en: '**HDBSCAN clustering**: min_sample = 2 for high sensitivity, while min_cluster_size
    = 7 allows very small clusters.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HDBSCAN 聚类**：min_sample = 2 以提高敏感度，而 min_cluster_size = 7 允许形成非常小的聚类。'
- en: '**BERTopic**: nr_topics = 40.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BERTopic**：nr_topics = 40。'
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Topic Representation & Summarization
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主题表示与摘要
- en: For the next two parts — topic representation and topic summarization — we used
    chat-based LLMs, carefully crafting system and user prompts. The straightforward
    approach involved setting the system prompt to define the tasks of keyword extraction
    and summarization, and using the user prompt to input a lengthy list of documents,
    constrained only by context limits.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于接下来的两个部分——主题表示和主题摘要——我们使用了基于聊天的语言模型（LLMs），并精心设计了系统和用户提示。简单的做法是设置系统提示来定义关键词提取和摘要任务，并使用用户提示输入一长串文档，只受限于上下文限制。
- en: Before diving deeper, let’s discuss the choice of chat-based LLMs and the infrastructure
    used. For a rapid proof of concept and development cycle, we opted for Ollama,
    known for its easy setup and minimal friction. we encountered some challenges
    switching models on Google Colab, so we decided to work locally on my M3 laptop.
    Ollama utilizes the Mac iGPU efficiently and proved adequate for my needs.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入讨论之前，让我们先讨论选择基于聊天的 LLM 及所使用的基础设施。为了快速验证概念并缩短开发周期，我们选择了 Ollama，因为它易于设置且摩擦小。在
    Google Colab 上切换模型时遇到了一些挑战，因此我们决定在我的 M3 笔记本电脑上进行本地工作。Ollama 高效利用 Mac 的集成 GPU，并且满足了我的需求。
- en: Initially, we tested various multilingual models, including LLaMA2, [LLaMA3](https://ai.meta.com/blog/meta-llama-3/)
    and LLaMA3.1\. However, a new version of the Dicta 2.0 model was released recently,
    which outperformed the others right away. Dicta 2.0 not only delivered better
    semantic results but also featured improved Hebrew tokenization (~one token per
    Hebrew character), allowing for longer context lengths and therefore larger batch
    processing without quality loss.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，我们测试了各种多语言模型，包括 LLaMA2、[LLaMA3](https://ai.meta.com/blog/meta-llama-3/) 和
    LLaMA3.1。然而，最近发布了 Dicta 2.0 新版本，它立刻超越了其他模型。Dicta 2.0 不仅提供了更好的语义结果，还改进了希伯来语分词（约每个希伯来字符对应一个标记），允许更长的上下文长度，因此能够进行更大批量的处理，而不影响质量。
- en: '[Dicta](https://dicta.org.il/dicta-lm) is an LLM, bilingual (Hebrew/English),
    fine-tuned on Mistral-7B-v0.1\. and is available on [Hugging Face](https://huggingface.co/collections/dicta-il/dicta-lm-20-collection-661bbda397df671e4a430c27).'
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[Dicta](https://dicta.org.il/dicta-lm) 是一个双语（希伯来语/英语）的大型语言模型（LLM），经过 Mistral-7B-v0.1
    微调，并可在 [Hugging Face](https://huggingface.co/collections/dicta-il/dicta-lm-20-collection-661bbda397df671e4a430c27)
    上找到。'
- en: '**Topic Representation:** This crucial step in topic modeling involves defining
    and describing topics through representative keywords or phrases, capturing the
    essence of each topic. The aim is to create clear, concise descriptions to understand
    the content associated with each topic. While BERTopic offers effective tools
    for topic representation, we found it easier to use external LLMs for this purpose.
    This approach allowed for more flexible experimentation, such as keyword prompt
    engineering, providing greater control over topic description and refinement.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**主题表示：** 主题建模中的这一关键步骤涉及通过代表性关键词或短语定义和描述主题，捕捉每个主题的核心要素。目标是创建清晰、简洁的描述，以便理解与每个主题相关的内容。虽然
    BERTopic 提供了有效的主题表示工具，但我们发现使用外部 LLM 更为便捷。该方法允许进行更灵活的实验，如关键词提示工程，提供了对主题描述和优化的更大控制。'
- en: 'System Prompt:'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统提示：
- en: “תפקידך למצוא עד חמש מילות מפתח של הטקסט ולהחזירן מופרדות בסימון נקודה. הקפד
    שכל מילה נבחרת תהיה מהטקסט הנתון ושהמילים תהיינה שונות אחת מן השניה. החזר לכל
    היותר חמש מילים שונות, בעברית, בשורה אחת קצרה, ללא אף מילה נוספת לפני או אחרי,
    ללא מספור וללא מעבר שורה וללא הסבר נוסף.”
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “你的任务是从文本中找出最多五个关键词，并用句点分隔返回。确保每个选择的单词都来自文本，并且这些词彼此不同。最多返回五个不同的词，用希伯来语写成一行短句，不添加任何其他内容、没有编号、没有换行符，也不要做额外解释。”
- en: User prompt was simply keywords and representative sentences returned by BERTopic
    default representation model (c-tf-idf).
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户的提示只是 BERTopic 默认表示模型（c-tf-idf）返回的关键词和代表性句子。
- en: '**Batch Summarization with LLM Models:** For each topic, we employed an LLM
    to summarize student responses. Due to the large volume of data, responses were
    processed in batches, with each batch summarized individually before aggregating
    these summaries into a final comprehensive overview.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用大语言模型（LLM）进行批量总结：**对于每个主题，我们使用 LLM 来总结学生的回答。由于数据量庞大，回答被分批处理，每个批次的总结都会单独进行，然后将这些总结汇总成最终的全面概述。'
- en: 'System Prompt:'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统提示：
- en: “המטרה שלך היא לתרגם לעברית ואז לסכם בעברית. הקלט הוא תשובות התלמידים לגבי השאלה
    הבאה [<X>]. סכם בפסקה אחת בדיוק עם לכל היותר 10 משפטים. הקפד לוודא שהתשובה מבוססת
    רק על הדעות שניתנו. מבחינה דקדוקית, נסח את הסיכום בגוף ראשון יחיד, כאילו אתה אחד
    הסטודנטים. כתוב את הסיכום בעברית בלבד, ללא תוספות לפני או אחרי הסיכום”
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “你的任务是将其翻译成希伯来语，然后用希伯来语总结。输入是学生关于以下问题的回答 [<X>]。请用一段最多 10 个句子的段落进行总结。请确保答案仅基于给定的意见。语法上，请用第一人称单数形式撰写总结，好像你是其中一位学生。总结只能用希伯来语书写，前后不要附加任何内容。”
- en: '[<X>] above is the string of the question, that we are trying to summarize.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的 [<X>] 是我们试图总结的问题的字符串。
- en: User prompt was was a batch of students’ response (as in the example above)
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户的提示是学生们的回答批次（如上例所示）
- en: Note that we required translation to Hebrew before summarization. Without this
    specification, the model occasionally responded in English or Arabic if the input
    contained a mix of languages.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们要求在总结之前先进行希伯来语翻译。没有这个要求时，如果输入内容包含多种语言，模型有时会用英语或阿拉伯语回答。
- en: '[Interestingly, Dicta 2.0 was able to converse in Arabic as well. This is surprising
    because Dicta 2.0 was not trained on Arabic (according to its release post, it
    was trained on 50% English and 50% Hebrew), and its base model, Mistral, was not
    specifically trained on Arabic either.]'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[有趣的是，Dicta 2.0 也能够用阿拉伯语进行对话。这让人感到惊讶，因为 Dicta 2.0 并没有在阿拉伯语上进行训练（根据其发布帖子，它是在
    50% 英语和 50% 希伯来语的数据上进行训练的），而其基础模型 Mistral 也没有专门在阿拉伯语上进行训练。]'
- en: '**Re-group the Batches:** The non-trivial final step involved re-summarizing
    the aggregated batches to produce a single cohesive summary per topic per question.
    This required meticulous prompt engineering to ensure relevant insights from each
    batch were accurately captured and effectively presented. By refining prompts,
    we guided the LLM to focus on key points, resulting in a comprehensive and insightful
    summary.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**重新分组批次：** 最后一个非简单的步骤是对汇总后的批次进行重新总结，以为每个主题和问题生成一个统一的总结。这要求精心设计提示，确保从每个批次中提取的相关见解得到准确捕捉并有效呈现。通过优化提示，我们引导
    LLM 聚焦于关键点，从而生成全面且富有见地的总结。'
- en: This multi-step approach allowed us to effectively manage the multilingual and
    nuanced dataset, extract significant insights, and provide actionable recommendations
    to enhance the educational experience at מדרסה (Madrasa).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这种多步骤的方法使我们能够有效管理多语言和复杂数据集，提取重要见解，并提供可操作的建议，以提升**مَدْرَسَة**（Madrasa）的教育体验。
- en: Evaluation
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估
- en: Evaluating the summarization task typically involves manual scoring of the summary’s
    quality by humans. In our case, the task includes not only summarization but also
    business insights. Therefore, we require a summary that captures not only the
    average student’s response but also the edge cases and rare or radical insights
    from a small number of students.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 评估总结任务通常涉及通过人工评分来衡量总结质量。在我们的案例中，任务不仅仅是总结，还包括商业见解。因此，我们要求总结不仅能反映出大多数学生的回答，还要能够捕捉到少数学生的边缘案例以及稀有或激进的见解。
- en: To address these needs, we split the evaluation into the six steps mentioned
    and assess them manually with a business-oriented approach. If you have a more
    rigorous method for holistic evaluation of such a project, we would love to hear
    your ideas :)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足这些需求，我们将评估过程分为上述六个步骤，并采用面向业务的方式手动评估。如果您有更为严谨的整体评估方法，欢迎分享您的想法 :)
- en: Results — example
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果 — 示例
- en: 'For instance, let’s look at one question from a questionnaire in the middle
    of a beginners’ course. The students were asked: “אנא שתף אותנו בהצעות לשיפור
    הקורס” (in English: “Please share with us suggestions for improving the course”).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们来看一下初学者课程中间一个问卷中的问题。学生们被问到：“אנא שתף אותנו בהצעות לשיפור הקורס”（英文：“Please
    share with us suggestions for improving the course”）。
- en: Most students responded with positive feedback, but some provided specific suggestions.
    The variety of suggestions is vast, and using clustering (topic modeling) and
    summarization, we can derive impactful insights for the NGO’s management team.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数学生回应了积极的反馈，但也有一些提供了具体的建议。建议的种类繁多，通过聚类（主题建模）和总结，我们可以为非政府组织的管理团队提炼出有价值的洞察。
- en: Here is a plot of the topic clusters, presented using BERTopic visualization
    tools.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是主题簇的图示，使用BERTopic可视化工具呈现。
- en: '![](../Images/4ff235d05a9f39828963f9ef2c5bf5f7.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ff235d05a9f39828963f9ef2c5bf5f7.png)'
- en: 'Hierarchical Clustering: For visualization purposes, we present a set of 10
    topics. However, in some cases our analysis included experimentation with tens
    of topics. Credit: Sria Louis / Madrasa.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类：为了可视化目的，我们展示了一组10个主题。然而，在某些情况下，我们的分析包括了对几十个主题的实验。致谢：Sria Louis / Madrasa。
- en: And finally, below are seven topics (out of 40) summarizing the students’ responses
    to the above question. Each topic includes its keywords (generated by the keyword
    prompt), three representative responses from the cluster (selected using Representation
    Model), and the final summarization.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，下面是七个主题（来自40个主题），总结了学生们对上述问题的回答。每个主题包括其关键词（由关键词提示生成）、来自该簇的三条代表性回答（使用表示模型选出），以及最终的总结。
- en: Bottom line, note the variety of topics and the insightful summaries.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，注意各种主题和富有洞察力的总结。
- en: '![](../Images/aa14eb3d8ebaca7422d86685dc12eeba.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa14eb3d8ebaca7422d86685dc12eeba.png)'
- en: 'Some of the topics: keywords, representing sentences and summaries. Credit:
    Sria Louis / Madrasa'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一些主题包括：关键词、表示句子和总结。致谢：Sria Louis / Madrasa
- en: What next?
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下一步是什么？
- en: 'We have six steps in mind:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有六个步骤：
- en: '**Optimization**: Experimenting with different architectures and hyperparameters.'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**优化**：尝试不同的架构和超参数。'
- en: '**Robustness**: Understanding and addressing unexpected sensitivity to certain
    hyperparameters.'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**鲁棒性**：理解并解决对某些超参数的意外敏感性。'
- en: '**Hallucinations**: Tackling hallucinations, particularly in small clusters/topics
    where the number of input sentences is limited, causing the model to generate
    ‘imaginary’ information.'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**幻觉问题**：处理幻觉问题，特别是在输入句子数量有限的小簇/主题中，这会导致模型生成“虚构”的信息。'
- en: '**Enriching Summarizations**: Using chain-of-thought techniques.'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**丰富总结**：使用思维链技术。'
- en: '**Enriching Topic Modeling**: Adding sentiment analysis before clustering.
    For example, if in a specific topic 95% of the responses were positive but 5%
    were very negative, it might be helpful to cluster based on both the topic and
    the sentiment in the sentence. This might help the summarizer avoid converging
    to the mean.'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**丰富主题建模**：在聚类前添加情感分析。例如，如果在某个特定主题中，95%的回应是正面的，而5%是非常负面的，那么基于主题和句子情感进行聚类可能会有所帮助。这可能帮助摘要生成器避免收敛到平均值。'
- en: '**Enhancing User Experience**: Implementing RAG or LLM-explainability techniques.
    For instance, given a specific non-trivial insight, we want the user to click
    on the insight and trace back to the exact student’s response that led to the
    insight.'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**增强用户体验**：实施RAG或LLM可解释性技术。例如，给定一个具体的非平凡洞察，我们希望用户点击该洞察，并追溯到导致该洞察的确切学生回应。'
- en: If you’re an LLM expert and would like to share your insights, we’d love to
    learn from you. Do you have suggestions for better models or approaches we should
    use? Ping us!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是LLM专家并且愿意分享你的见解，我们很乐意向你学习。你有什么建议可以改进我们的模型或方法吗？快来联系我们！
- en: '[sria.louis@gmail.com](mailto:sria.louis@gmail.com)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[sria.louis@gmail.com](mailto:sria.louis@gmail.com)'
- en: Want to learn more about Madarsa? [https://madrasafree.com/](https://madrasafree.com/)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解更多关于Madarsa的信息吗？ [https://madrasafree.com/](https://madrasafree.com/)
- en: Code can be found on the [Project GitHub reop](https://github.com/gitLouis/madarse-summarization).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 代码可以在[项目GitHub仓库](https://github.com/gitLouis/madarse-summarization)中找到。
- en: '![](../Images/62500ce8f7182ccfc9e9822e71ec64bd.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62500ce8f7182ccfc9e9822e71ec64bd.png)'
- en: 'Illustration: Or Livneh'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 插图：Or Livneh
- en: 'Keywords: NLP, Topic Modeling, LLM, Hebrew, Sentence Embedding , BERTopic,
    llama, NLTK, Dicta 2.0, Summarization, madrasa'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词：NLP、主题建模、LLM、希伯来语、句子嵌入、BERTopic、llama、NLTK、Dicta 2.0、摘要、madrasa
