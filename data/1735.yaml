- en: From Scratch to Deep Quantile Forecasting
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零开始到深度分位数预测
- en: 原文：[https://towardsdatascience.com/from-scratch-to-deep-quantile-forecasting-366d84b7dd22?source=collection_archive---------5-----------------------#2024-07-16](https://towardsdatascience.com/from-scratch-to-deep-quantile-forecasting-366d84b7dd22?source=collection_archive---------5-----------------------#2024-07-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/from-scratch-to-deep-quantile-forecasting-366d84b7dd22?source=collection_archive---------5-----------------------#2024-07-16](https://towardsdatascience.com/from-scratch-to-deep-quantile-forecasting-366d84b7dd22?source=collection_archive---------5-----------------------#2024-07-16)
- en: An end-2-end empirical sharing of multi-step quantile forecasting with Tensorflow,
    NeuralForecast, and Zero-shot LLMs.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Tensorflow、NeuralForecast和零样本大型语言模型进行多步分位数预测的端到端实证分享。
- en: '[](https://jinhangjiang.medium.com/?source=post_page---byline--366d84b7dd22--------------------------------)[![Jinhang
    Jiang](../Images/35a4006e02b358f732af46a78d8b7bac.png)](https://jinhangjiang.medium.com/?source=post_page---byline--366d84b7dd22--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--366d84b7dd22--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--366d84b7dd22--------------------------------)
    [Jinhang Jiang](https://jinhangjiang.medium.com/?source=post_page---byline--366d84b7dd22--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://jinhangjiang.medium.com/?source=post_page---byline--366d84b7dd22--------------------------------)[![Jinhang
    Jiang](../Images/35a4006e02b358f732af46a78d8b7bac.png)](https://jinhangjiang.medium.com/?source=post_page---byline--366d84b7dd22--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--366d84b7dd22--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--366d84b7dd22--------------------------------)
    [金航江](https://jinhangjiang.medium.com/?source=post_page---byline--366d84b7dd22--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--366d84b7dd22--------------------------------)
    ·11 min read·Jul 16, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--366d84b7dd22--------------------------------)
    ·11分钟阅读·2024年7月16日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d9966f81b1656b76977dac86cc86b3af.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d9966f81b1656b76977dac86cc86b3af.png)'
- en: Image by Author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: Content
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内容
- en: Short Introduction
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 简短介绍
- en: Data
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据
- en: Build a Toy Version of Quantile Recurrent Forecaster
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个分位数递归预测器的玩具版本
- en: Quantile Forecasting with the State-of-Art Models
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用最先进模型进行分位数预测
- en: Zero-shot Quantile Forecast with LLMs
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用大型语言模型进行零样本分位数预测
- en: Conclusion
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结论
- en: Short Introduction
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简短介绍
- en: Quantile forecasting is a statistical technique used to predict different quantiles
    (e.g., the median or the 90th percentile) of a response variable’s distribution,
    providing a more comprehensive view of potential future outcomes. Unlike traditional
    mean forecasting, which only estimates the average, quantile forecasting allows
    us to understand the range and likelihood of various possible results.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 分位数预测是一种统计技术，用于预测响应变量分布的不同分位数（例如，中位数或第90百分位数），提供对未来潜在结果的更全面的视角。与传统的均值预测只估计平均值不同，分位数预测使我们能够理解各种可能结果的范围和发生的可能性。
- en: Quantile forecasting is essential for decision-making in contexts with asymmetric
    loss functions or varying risk preferences. In supply chain management, for example,
    predicting the 90th percentile of demand ensures sufficient stock levels to avoid
    shortages, while predicting the 10th percentile helps minimize overstock and associated
    costs. This methodology is particularly advantageous in sectors such as finance,
    meteorology, and energy, where understanding distribution extremes is as critical
    as the mean.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 分位数预测在具有不对称损失函数或不同风险偏好的决策过程中至关重要。例如，在供应链管理中，预测需求的第90百分位数可以确保足够的库存水平，以避免短缺，而预测第10百分位数则有助于最小化过剩库存及相关成本。这种方法在金融、气象和能源等领域尤为有利，因为在这些领域中，理解分布的极端值与了解均值一样重要。
- en: Both quantile forecasting and conformal prediction address uncertainty, yet
    their methodologies differ significantly. Quantile forecasting directly models
    specific quantiles of the response variable, providing detailed insights into
    its distribution. Conversely, conformal prediction is a model-agnostic technique
    that constructs prediction intervals around forecasts, guaranteeing that the true
    value falls within the interval with a specified probability. Quantile forecasting
    yields precise quantile estimates, whereas conformal prediction offers broader
    interval assurances.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 量化预测和符合性预测都处理不确定性，但它们的方法论差异显著。量化预测直接对响应变量的特定分位数进行建模，提供关于其分布的详细洞察。相反，符合性预测是一种与模型无关的技术，它围绕预测构建预测区间，保证真实值以指定的概率落入该区间。量化预测提供精确的分位数估计，而符合性预测则提供更广泛的区间保证。
- en: The implementation of quantile forecasting can markedly enhance decision-making
    by providing a sophisticated understanding of future uncertainties. This approach
    allows organizations to tailor strategies to different risk levels, optimize resource
    allocation, and improve operational efficiency. By capturing a comprehensive range
    of potential outcomes, quantile forecasting enables organizations to make informed,
    data-driven decisions, thereby mitigating risks and enhancing overall performance.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 量化预测的实现可以显著提升决策制定，通过提供对未来不确定性的深刻理解。这种方法使得组织能够根据不同的风险水平量身定制策略，优化资源分配，并提高运营效率。通过捕捉广泛的潜在结果，量化预测使得组织能够做出明智、数据驱动的决策，从而减轻风险并提高整体表现。
- en: Data
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据
- en: 'To demonstrate the work, I chose to use the data from the M4 competition as
    an example. The data is under [CC0: Public Domain](https://creativecommons.org/publicdomain/zero/1.0/)
    license which can be accessed [here](https://www.kaggle.com/datasets/yogesh94/m4-forecasting-competition-dataset).
    The data can also be loaded through datasetsforecast package:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '为了演示该工作，我选择使用M4竞赛的数据作为示例。该数据采用[CC0: 公共领域](https://creativecommons.org/publicdomain/zero/1.0/)许可证，可以通过[这里](https://www.kaggle.com/datasets/yogesh94/m4-forecasting-competition-dataset)访问。数据也可以通过datasetsforecast包加载：'
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/b2fc47f942462df4c37eb76000b6218c.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2fc47f942462df4c37eb76000b6218c.png)'
- en: Image by Author
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'The original data contains over 300 unique time series. To demonstrate, I randomly
    selected three time series: W96, W99, and W100, as they all have the same history
    length. The original timestamp is masked as integer numbers (i.e., 1–2296), I
    manually converted it back to normal date format with the first date to be January
    4th, 1970\. The following figure is a preview of W99:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据包含超过300个独特的时间序列。为了演示，我随机选择了三个时间序列：W96、W99和W100，因为它们的历史长度相同。原始时间戳被掩码为整数（即1-2296），我手动将其转换回正常的日期格式，第一天为1970年1月4日。以下图为W99的预览：
- en: '![](../Images/7218697744c1bb3df13b004055f82502.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7218697744c1bb3df13b004055f82502.png)'
- en: Image by Author
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Build a Toy Version of Quantile Recurrent Forecaster
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建量化回归预测器的玩具版
- en: First, let’s build a quantile forecaster from scratch to understand how the
    target data flows through the pipeline and how the forecasts are generated. I
    picked the idea from the paper [A Multi-Horizon Quantile Recurrent Forecaster](https://arxiv.org/pdf/1711.11053)
    by Wen et al. The authors proposed a Multi-Horizon Quantile Recurrent Neural Network
    (MQ-RNN) framework that combines Sequence-to-Sequence Neural Networks, Quantile
    Regression, and Direct Multi-Horizon Forecasting for accurate and robust multi-step
    time series forecasting. By leveraging the expressiveness of neural networks,
    the nonparametric nature of quantile regression, and a novel training scheme called
    forking-sequences, the model can effectively handle shifting seasonality, known
    future events, and cold-start situations in large-scale forecasting applications.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们从零开始构建一个量化预测器，以了解目标数据如何流经整个管道，以及预测是如何生成的。我从Wen等人的论文[A Multi-Horizon Quantile
    Recurrent Forecaster](https://arxiv.org/pdf/1711.11053)中汲取灵感。作者提出了一个多水平量化回归神经网络（MQ-RNN）框架，结合了序列到序列神经网络、量化回归和直接多水平预测，用于准确且稳健的多步时间序列预测。通过利用神经网络的表达能力、量化回归的非参数特性以及一种名为分叉序列的新训练方案，该模型能够有效处理变化的季节性、已知的未来事件以及冷启动问题，适用于大规模预测应用。
- en: 'We cannot reproduce everything in this short blog, but we can try to replicate
    part of it using the TensorFlow package as a demo. If you are interested in the
    implementation of the paper, there is an ongoing project that you can leverage:
    [MQRNN](https://github.com/tianchen101/MQRNN?tab=readme-ov-file).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能在这个简短的博客中复现所有内容，但我们可以尝试使用TensorFlow包进行部分复现作为演示。如果你对论文的实现感兴趣，可以利用一个正在进行的项目：[MQRNN](https://github.com/tianchen101/MQRNN?tab=readme-ov-file)。
- en: Let’s first load the necessary package and define some global parameters. We
    will use the LSTM model as the core, and we need to do some preprocessing on the
    data to obtain the rolling windows before fitting. The input_shape is set to (104,
    1) meaning we are using two years of data for each training window. In this walkthrough,
    we will only look into an 80% confidence interval with the median as the point
    forecast, which means the quantiles = [0.1, 0.5, 0.9]. We will use the last 12
    weeks as a test dataset, so the output_steps or horizon is equal to 12 and the
    cut_off_date will be ‘2013–10–13’.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 首先加载必要的包并定义一些全局参数。我们将使用LSTM模型作为核心，并且需要对数据进行一些预处理，以获得滚动窗口，然后进行拟合。`input_shape`设置为(104,
    1)，这意味着我们每个训练窗口使用两年的数据。在本次演示中，我们仅关注80%的置信区间，并将中位数作为点预测值，这意味着分位数 = [0.1, 0.5, 0.9]。我们将使用最后12周作为测试数据集，因此`output_steps`或预测范围为12，`cut_off_date`将是‘2013–10–13’。
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, let’s convert the data to rolling windows which is the desired input
    shape for RNN-based models:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将数据转换为滚动窗口，这正是RNN模型所需的输入形状：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then we split the data into train, val, and test:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将数据分成训练集、验证集和测试集：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The authors of the MQRNN utilized both horizon-specific local context, essential
    for temporal awareness and seasonality mapping, and horizon-agnostic global context
    to capture non-time-sensitive information, enhancing the stability of learning
    and the smoothness of generated forecasts. To build a model that sort of reproduces
    what the MQRNN is doing, we need to write a quantile loss function and add layers
    that capture local context and global context. I added an attention layer to it
    to show you how the attention mechanism can be included in such a process:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: MQRNN的作者利用了与预测范围相关的局部上下文，这是实现时间感知和季节性映射所必需的，同时也利用了与预测范围无关的全局上下文来捕获非时间敏感信息，从而增强了学习的稳定性和生成预测的平滑度。为了构建一个类似于MQRNN的模型，我们需要编写一个分位数损失函数，并添加捕获局部上下文和全局上下文的层。我为此添加了一个注意力层，向你展示如何将注意力机制融入这样的过程：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here are the plotted forecasting results:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是绘制的预测结果：
- en: '![](../Images/4ea0f97ef17dc6a16096bdb67e253103.png)![](../Images/aa8e3a035e57c8e122e5a273c123f59e.png)![](../Images/45150e4f2fcc64899238d7a063f66b11.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ea0f97ef17dc6a16096bdb67e253103.png)![](../Images/aa8e3a035e57c8e122e5a273c123f59e.png)![](../Images/45150e4f2fcc64899238d7a063f66b11.png)'
- en: 'We also evaluated the SMAPE for each item, as well as the percentage coverage
    of the interval (how much actual was covered by the interval). The results are
    as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还评估了每个项的SMAPE（对称平均绝对百分比误差），以及区间覆盖率（即实际值有多少被区间覆盖）。结果如下：
- en: '![](../Images/3e7cd6245fd2ac4b3a1f5ebc96c21338.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e7cd6245fd2ac4b3a1f5ebc96c21338.png)'
- en: This toy version can serve as a good baseline to start with quantile forecasting.
    The distributed training is not configured for this setup nor the model architecture
    is optimized for large-scale forecasting, thus it might suffer from speed issues.
    In the next section, we will look into a package that allows you to do quantile
    forecasts with the most advanced deep-learning models.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简化版本可以作为开始进行分位数预测的良好基准。此设置未配置分布式训练，且模型架构也没有为大规模预测进行优化，因此可能存在速度问题。在下一部分中，我们将研究一个包，它允许你使用最先进的深度学习模型进行分位数预测。
- en: Quantile Forecasting with the SOTA Models
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SOTA模型进行分位数预测
- en: The [neuralforecast](https://nixtlaverse.nixtla.io/neuralforecast/index.html)
    package is an outstanding Python library that allows you to use most of the SOTA
    deep neural network models for time series forecasting, such as PatchTST, NBEATs,
    NHITS, TimeMixer, etc. with easy implementation. In this section, I will use [PatchTST](https://arxiv.org/pdf/2211.14730)
    as an example to show you how to perform quantile forecasting.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[neuralforecast](https://nixtlaverse.nixtla.io/neuralforecast/index.html)包是一个优秀的Python库，允许你使用大多数SOTA（最先进的）深度神经网络模型进行时间序列预测，如PatchTST、NBEATs、NHITS、TimeMixer等，且实现简单。在这一部分，我将以[PatchTST](https://arxiv.org/pdf/2211.14730)为例，展示如何进行分位数预测。'
- en: First, load the necessary modules and define the parameters for PatchTST. Tuning
    the model will require some empirical experience and will be project-dependent.
    If you are interested in getting the potential-optimal parameters for your data,
    you may look into the auto modules from the neuralforecast. They will allow you
    to use Ray to perform hyperparameter tuning. And it is quite efficient! The neuralforecast
    package carries a great set of models that are based on different sampling approaches.
    The ones with the base_window approach will allow you to use MQLoss or HuberMQLoss,
    where you can specify the quantile levels you are looking for. In this work, I
    picked HuberMQLoss as it is more robust to outliers.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，加载必要的模块并定义PatchTST的参数。调整模型需要一些经验，并且会依赖于具体的项目。如果你有兴趣为你的数据获取潜在的最优参数，你可以查看neuralforecast中的自动模块。它们允许你使用Ray进行超参数调优，效率非常高！neuralforecast包包含了一系列基于不同采样方法的模型。使用base_window方法的模型可以让你使用MQLoss或HuberMQLoss，你可以指定你要寻找的分位数级别。在本工作中，我选择了HuberMQLoss，因为它对异常值更为稳健。
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here are plotted forecasts:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这是绘制的预测结果：
- en: '![](../Images/4a38d8c1bebc0c8f483c2d49256c5179.png)![](../Images/d58e7495e1d8e68bbd54a5762d0dab56.png)![](../Images/2c352b71e91e103da2630ea6ea9576ef.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4a38d8c1bebc0c8f483c2d49256c5179.png)![](../Images/d58e7495e1d8e68bbd54a5762d0dab56.png)![](../Images/2c352b71e91e103da2630ea6ea9576ef.png)'
- en: 'Here are the metrics:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这是指标：
- en: '![](../Images/c293307afec86c1ea8c10d27dd0e7dc7.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c293307afec86c1ea8c10d27dd0e7dc7.png)'
- en: Through the demo, you can see how easy to implement the model and how the performance
    of the model has been lifted. However, if you wonder if there are any easier approaches
    to do this task, the answer is YES. In the next section, we will look into a T5-based
    model that allows you to conduct zero-shot quantile forecasting.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个演示，你可以看到模型的实现是多么简单，以及模型性能是如何提升的。然而，如果你在想是否有更简单的方法来完成这个任务，答案是肯定的。在接下来的部分，我们将介绍一个基于T5的模型，允许你进行零-shot分位数预测。
- en: Zero-shot Quantile Forecast with LLMs
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 零-shot分位数预测与大语言模型（LLMs）
- en: We have been witnessing a trend where the advancement in NLP will also further
    push the boundaries for time series forecasting as predicting the next word is
    a synthetic process for predicting the next period’s value. Given the fast development
    of large language models (LLMs) for generative tasks, researchers have also started
    to look into pre-training a large model on millions of time series, allowing users
    to do zero-shot forecasts.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一直在见证一个趋势，即NLP的进展将进一步推动时间序列预测的边界，因为预测下一个词是预测下一个时间段值的合成过程。鉴于大语言模型（LLMs）在生成任务中的快速发展，研究人员也开始考虑在数百万个时间序列上对大型模型进行预训练，使用户能够进行零-shot预测。
- en: 'However, before we draw an equal sign between the LLMs and Zero-shot Time Series
    tasks, we have to answer one question: what is the difference between training
    a language model and training a time series model? It would be “tokens from a
    finite dictionary versus values from an unbounded.” Amazon recently released a
    project called [Chronos](https://github.com/amazon-science/chronos-forecasting?tab=readme-ov-file)
    which well handled the challenge and made the large time series model happen.
    As the authors stated: “Chronos tokenizes time series into discrete bins through
    simple scaling and quantization of real values. In this way, we can train off-the-shelf
    language models on this ‘language of time series,’ with no changes to the model
    architecture”. The original paper can be found [here](https://arxiv.org/pdf/2403.07815).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们将LLMs和零-shot时间序列任务画上等号之前，我们必须回答一个问题：训练语言模型和训练时间序列模型之间有什么区别？答案是：“来自有限字典的tokens与来自无限范围的值”。亚马逊最近发布了一个名为[Chronos](https://github.com/amazon-science/chronos-forecasting?tab=readme-ov-file)的项目，它很好地解决了这个挑战并实现了大规模时间序列模型。如作者所述：“Chronos通过简单的缩放和量化实值，将时间序列分割成离散的箱子。通过这种方式，我们可以在这种‘时间序列语言’上训练现成的语言模型，无需更改模型架构。”原始论文可以在[此处](https://arxiv.org/pdf/2403.07815)找到。
- en: Currently, Chronos is available in multiple versions. It can be loaded and used
    through the [autogluon API](https://auto.gluon.ai/stable/tutorials/timeseries/forecasting-chronos.html)
    with only a few lines of code.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，Chronos有多个版本可用。它可以通过[autogluon API](https://auto.gluon.ai/stable/tutorials/timeseries/forecasting-chronos.html)加载并使用，只需要几行代码。
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here are the plotted forecasts:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这是绘制的预测结果：
- en: '![](../Images/6d0ddda57ed01a537385312010594c39.png)![](../Images/c3a8bec841edbc3b6eaa37e050c79b9d.png)![](../Images/391aa8dbfff1f7334ef9eaa094e26e46.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d0ddda57ed01a537385312010594c39.png)![](../Images/c3a8bec841edbc3b6eaa37e050c79b9d.png)![](../Images/391aa8dbfff1f7334ef9eaa094e26e46.png)'
- en: 'Here are the metrics:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些评估指标：
- en: '![](../Images/1e32cc1bbfbe54001b6019a2e6860001.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e32cc1bbfbe54001b6019a2e6860001.png)'
- en: As you can see, Chronos showed a very decent performance compared to PatchTST.
    However, it does not mean it has surpassed PatchTST, since it is very likely that
    Chronos has been trained on M4 data. In their original paper, the authors also
    evaluated their model on the datasets that the model has not been trained on,
    and Chronos still yielded very comparable results to the SOTA models.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，Chronos与PatchTST相比表现得非常出色。然而，这并不意味着它已经超越了PatchTST，因为很有可能Chronos已经在M4数据上进行了训练。在他们的原始论文中，作者还评估了该模型在未曾训练过的数据集上的表现，结果显示Chronos仍然与SOTA模型的表现非常接近。
- en: There are many more large time series models being developed right now. One
    of them is called [TimeGPT](https://docs.nixtla.io/docs/getting-started-about_timegpt)
    which was developed by NIXTLA. The invention of this kind of model not only made
    the forecasting task easier, more reliable, and consistent, but it is also a good
    starting point to make reasonable guesses for time series with limited historical
    data.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 目前正在开发更多的大型时间序列模型。其中一个叫做[TimeGPT](https://docs.nixtla.io/docs/getting-started-about_timegpt)，由NIXTLA开发。这类模型的发明不仅让预测任务变得更容易、更可靠和一致，而且也是在有限历史数据的情况下对时间序列做出合理猜测的一个良好起点。
- en: Conclusion
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: From building a toy version of a quantile recurrent forecaster to leveraging
    state-of-the-art models and zero-shot large language models, this blog has demonstrated
    the power and versatility of quantile forecasting. By incorporating models like
    TensorFlow’s LSTM, NeuralForecast’s PatchTST, and Amazon’s Chronos, we can achieve
    accurate, robust, and computationally efficient multi-step time series forecasts.
    Quantile forecasting not only enhances decision-making by providing a nuanced
    understanding of future uncertainties but also allows organizations to optimize
    strategies and resource allocation. The advancements in neural networks and zero-shot
    learning models further push the boundaries, making quantile forecasting a pivotal
    tool in modern data-driven industries.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 从构建量化回归预测器的玩具版，到利用最先进的模型和零样本大语言模型，本博客展示了量化预测的强大功能和多样性。通过整合像TensorFlow的LSTM、NeuralForecast的PatchTST和Amazon的Chronos等模型，我们可以实现准确、稳健且计算高效的多步时间序列预测。量化预测不仅通过提供对未来不确定性的细致理解来增强决策支持，还帮助组织优化战略和资源分配。神经网络和零样本学习模型的进步进一步推动了这一领域的边界，使得量化预测成为现代数据驱动行业中的关键工具。
- en: 'Note: All the images, numbers and tables are generated by the author. The complete
    code can be found here: [Quantile Forecasting](https://github.com/jinhangjiang/Medium_Demo/blob/4ed201b4130c940106dd70ec7fe8a0d7b5b847a8/From%20Scratch%20to%20Deep%20Quantile%20Forecasting/Quantile_Forecast.ipynb).'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 注：所有的图片、数字和表格均由作者生成。完整代码可以在这里找到：[Quantile Forecasting](https://github.com/jinhangjiang/Medium_Demo/blob/4ed201b4130c940106dd70ec7fe8a0d7b5b847a8/From%20Scratch%20to%20Deep%20Quantile%20Forecasting/Quantile_Forecast.ipynb)。
