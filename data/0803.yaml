- en: The Business Guide to Tailoring Language AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《定制语言AI的商业指南》
- en: 原文：[https://towardsdatascience.com/the-business-guide-to-tailoring-language-ai-5f0fa806e838?source=collection_archive---------8-----------------------#2024-03-27](https://towardsdatascience.com/the-business-guide-to-tailoring-language-ai-5f0fa806e838?source=collection_archive---------8-----------------------#2024-03-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-business-guide-to-tailoring-language-ai-5f0fa806e838?source=collection_archive---------8-----------------------#2024-03-27](https://towardsdatascience.com/the-business-guide-to-tailoring-language-ai-5f0fa806e838?source=collection_archive---------8-----------------------#2024-03-27)
- en: A framework for unlocking custom LLM solutions you’ll understand
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解锁定制LLM解决方案的框架，你将能够理解
- en: '[](https://medium.com/@georg.ruile?source=post_page---byline--5f0fa806e838--------------------------------)[![Georg
    Ruile, Ph.D.](../Images/83b04db23852ba2df2818fe62250ca22.png)](https://medium.com/@georg.ruile?source=post_page---byline--5f0fa806e838--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5f0fa806e838--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5f0fa806e838--------------------------------)
    [Georg Ruile, Ph.D.](https://medium.com/@georg.ruile?source=post_page---byline--5f0fa806e838--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@georg.ruile?source=post_page---byline--5f0fa806e838--------------------------------)[![Georg
    Ruile, Ph.D.](../Images/83b04db23852ba2df2818fe62250ca22.png)](https://medium.com/@georg.ruile?source=post_page---byline--5f0fa806e838--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5f0fa806e838--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5f0fa806e838--------------------------------)
    [Georg Ruile, 博士](https://medium.com/@georg.ruile?source=post_page---byline--5f0fa806e838--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5f0fa806e838--------------------------------)
    ·8 min read·Mar 27, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5f0fa806e838--------------------------------)
    ·阅读时间8分钟·2024年3月27日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/29362ad37c0a54ab31304707440c977f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29362ad37c0a54ab31304707440c977f.png)'
- en: Foreword
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前言
- en: This article illustrates how Large Language Models (LLM) are gradually adapted
    for custom use. It is meant to give people with no computer science background
    an easy to grasp analogy into how GPT and similar AI systems can be customized.
    Why the artwork? Bare with me, I hope you enjoy the journey.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文说明了大型语言模型（LLM）是如何逐步适应定制化使用的。它旨在为没有计算机科学背景的人提供一种易于理解的类比，帮助他们了解GPT和类似的AI系统如何被定制。为什么要有艺术作品？请耐心一点，我希望你能享受这个过程。
- en: '**Introduction**'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**引言**'
- en: I will not start this article with an introduction on how ChatGPT, Claude and
    generative AI have transformed businesses and will forever change our lives, careers
    and businesses. This has been written many times (most notably by the GPTs themselves
    …). Instead, today I would like to focus on the question of how we can **use a
    Large Language Model (LLM) for our specific, custom purposes**.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会从ChatGPT、Claude和生成性AI如何改变企业并将永远改变我们的生活、职业和商业的介绍开始这篇文章。关于这些内容已经有很多文章（尤其是GPT们自己写的…）。相反，今天我想关注的是，我们如何**利用大型语言模型（LLM）来满足我们特定的、定制化的需求**。
- en: In my professional and private life, I have tried to help people understand
    the basics of what Language AI can and cannot do, beginning with why and how we
    should do proper prompting (which is beyond the scope of this article), ranging
    all the way to what it means when managers claim that their company has it’s “own
    language model.” I feel there is a lot of confusion and uncertainty around the
    topic of adapting language models to your business requirements in particular.
    So far, I have not come accross an established framework that adresses this need.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的职业生涯和私人生活中，我一直在帮助人们理解语言AI能够做什么以及不能做什么的基本概念，从为什么以及如何进行正确的提示（这超出了本文的范围）开始，到当管理者声称他们的公司拥有“自己的语言模型”时，这意味着什么。我觉得，特别是在将语言模型适应到特定业务需求的问题上，存在许多困惑和不确定性。到目前为止，我还没有遇到一个能够解决这一需求的成熟框架。
- en: 'In an attempt to provide an easy explanation that makes non-IT specialists
    understand the possibilities of language model customization, I came up with an
    analogy that took me back to my early days when I had been working as a bar piano
    player. Just like a language model, a bar piano player is frequently asked to
    play a variety of songs, often with an unspecific request or limited context:
    “Play it again, Sam…”'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供一个易于理解的解释，让非IT专业人士理解语言模型定制的可能性，我想出了一个类比，它让我回忆起早年作为酒吧钢琴演奏员时的经历。就像语言模型一样，酒吧钢琴演奏员常常被要求演奏各种歌曲，通常是没有明确要求或限定上下文的：“再来一首，Sam……”
- en: Meet Sam — the Bar Piano Player
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 认识Sam——酒吧钢琴演奏员
- en: Imagine you’re sitting in a piano bar in the lounge of a 5-star hotel, and there
    is a nice grand piano. Sam, the (still human) piano player, is performing. You’re
    enjoying your drink and wonder if Sam can also perform according to your specific
    musical taste. For the sake of our argument, Sam is actually a language model,
    and you’re the hotel (or business) owner wondering what Sam is able to do for
    you. The Escalation Ladder I present here is a framework, which offers four levels
    or approaches to gradually shape Sam’s knowledge and capabilities to align with
    your unique requirements. On each level, the requirements get more and more specific,
    along with the efforts and costs of making Sam adapt.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下你正坐在一家五星级酒店的钢琴酒吧里，那里有一架漂亮的三角钢琴。Sam（依然是人类）正演奏着钢琴曲。你正在享受着你的饮品，想知道Sam是否也能根据你的特定音乐口味演奏。为了便于论述，假设Sam实际上是一个语言模型，而你是酒店（或企业）老板，想知道Sam能为你做些什么。我在这里展示的“升级阶梯”是一个框架，提供了四个级别或方法，用以逐步调整Sam的知识和能力，以适应你的独特需求。在每个级别，需求变得越来越具体，同时让Sam适应所需的努力和成本也越来越高。
- en: 'The Escalation Ladder: From Prompting to Training Your Own Language Model'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 升级阶梯：从提示到训练你自己的语言模型
- en: '**1\. Prompting: Beyond the Art of Asking the Right Questions**'
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**1. 提示：超越提问技巧**'
- en: '![](../Images/5ab114394e5ee33443590ebca9cb5d87.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5ab114394e5ee33443590ebca9cb5d87.png)'
- en: Prompting a language model — the model output depends on the quality of the
    prompt
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 提示语言模型——模型的输出取决于提示的质量
- en: 'The first thing you can do is quite simple, however, may not be easy. You ask
    Sam to play a song that you’d like to hear. The more specific you are, i.e., the
    clearer your request, the better your wording (and depending on the number of
    drinks you’ve had at the hotel bar, your pronunciation), the better. As Voltaire
    famously said:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以做的第一件事非常简单，但可能并不容易。你请求Sam播放你想听的歌曲。你越具体，也就是说，你的请求越清晰，你的措辞越准确（当然还取决于你在酒店酒吧喝了多少酒，你的发音），效果就越好。正如伏尔泰曾经说过：
- en: “Judge a man by his questions rather than by his answers.”
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “通过一个人的提问，而不是他的回答来判断他。”
- en: “Play some jazz” may or may not make Sam play what you had in mind. “Play Dave
    Brubeck’s original version of ‘Take Five’, playing the lead notes of the saxophone
    with your right hand while keeping the rythmic pattern in the left” will get quite
    a specific result, presuming Sam has been given the right training.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: “弹一些爵士乐”可能会，也可能不会使Sam演奏出你心中想要的旋律。“弹戴夫·布鲁贝克的《Take Five》的原版，右手弹奏萨克斯风的主旋律，左手保持节奏模式”则会得到一个非常具体的结果，前提是Sam接受了正确的训练。
- en: What you do here is my analogy to **prompting** — the way we currently interact
    with general-purpose language models like GPT or Claude. While prompting is the
    most straightforward approach, the quality of the output relies heavily on the
    specificity and clarity of your prompts. It is for this reason that [**Prompt
    Engineering**](https://de.wikipedia.org/wiki/Prompt_Engineering) has become a
    profession, one which you probably would never have heard of only a few years
    ago.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你在这里所做的就是我对**提示**的类比——我们当前与像GPT或Claude这样的通用语言模型的交互方式。虽然提示是最直接的方式，但输出的质量在很大程度上依赖于你的提示的具体性和清晰度。正因为如此，[**提示工程**](https://de.wikipedia.org/wiki/Prompt_Engineering)成为了一个职业，而这个职业你可能在几年前都没听说过。
- en: Prompting makes a huge difference, from getting a poor, general, or even false
    answer to something you can actually work with. That’s why in my daily use of
    GPT and the like, I always take a minute to consider a proper prompt for the task
    at hand (my favorite course of action here is something called “***role based
    prompting***”, where you give the model a specific role in your prompt, such as
    an IT expert, a data engineer or a career coach. Again, we will not get into the
    depths of promtping, since it is beyond the scope of this article).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 提示会带来巨大的差异，从得到一个糟糕、笼统，甚至错误的答案，到得到一个你可以实际使用的答案。这就是为什么在我日常使用 GPT 之类的工具时，我总是花一点时间思考一个合适的提示（我最喜欢的做法叫做“***基于角色的提示***”，即在提示中给模型一个特定角色，例如
    IT 专家、数据工程师或职业教练。我们这里不深入探讨提示的细节，因为这超出了本文的范围）。
- en: 'But **prompting has its limits**: You may not want to always explain the world
    within your prompts. It can be quite a tedious task to provide all the context
    in proper writing (even though chat based language models are somewhat forgiving
    when it comes to spelling). And the ouput may still deviate from what you had
    in mind — in the hotel bar scenario, you still may not be happy with Sam’s interpretation
    of your favorite songs, no matter how specific your requests may have been.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 但是**提示有其局限性**：你可能不想总是通过提示来解释整个世界。在提示中提供所有的上下文并进行适当的书写可能是一项相当繁琐的任务（尽管基于聊天的语言模型在拼写上有一定的宽容度）。而且输出仍然可能偏离你的预期——在酒店酒吧场景中，不管你要求多么具体，你仍然可能对
    Sam 演奏你最喜欢的歌曲不满意。
- en: '2\. Embedding or Retrieval-Augmented Generation (RAG): Provide Context-Relevant
    Data or Instructions'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 嵌入或检索增强生成（RAG）：提供与上下文相关的数据或指令
- en: '![](../Images/38d50da735201b16c207d70efb3e168e.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38d50da735201b16c207d70efb3e168e.png)'
- en: Embedding or adding context to your prompts via Retrieval-Augmented Generation
    (RAG)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检索增强生成（RAG）将上下文嵌入或添加到你的提示中
- en: You have an idea. In addition to asking Sam to “play it again” (and to prompt
    him specifically of what it is you want to hear), you remember you had the sheet
    music in your bag. So you put the sheets on the piano and ask Sam to play what’s
    written (provided you give him some incentive, say, $10 in cash).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你有一个想法。除了让 Sam “再演奏一次”（并具体提醒他你想听什么），你记得你的包里有乐谱。所以你把乐谱放在钢琴上，要求 Sam 演奏上面写的内容（前提是你给他一些激励，比如
    10 美元现金）。
- en: In our analogy, our model now uses its inherent abilities to generate language
    output (Sam playing piano) and directs those abilities towards a specific piece
    of context (Sam playing a specific song).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的类比中，模型现在利用其固有的能力生成语言输出（Sam 演奏钢琴），并将这些能力应用于特定的上下文（Sam 演奏特定的歌曲）。
- en: This architectural pattern is referred to as **Retrieval-Augmented Generation
    (RAG)**, where you provide the model with additional context or reference materials
    relevant to your domain. By incorporating these external sources and data, the
    model can generate more informed and accurate responses, tailored to your specific
    needs. In more technical terms, this involves preparing and cleaning textual context
    data that is then transformed into [**Embeddings**](https://learn.microsoft.com/en-us/semantic-kernel/memories/embeddings)
    and properly indexed. When prompted, the model receives a relevant selection of
    this context data, according to the content of the prompt.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构模式被称为**检索增强生成（RAG）**，你为模型提供与你领域相关的附加上下文或参考材料。通过结合这些外部来源和数据，模型可以生成更有信息量和准确性的回答，量身定制以满足你的特定需求。从技术上讲，这涉及准备和清理文本上下文数据，然后将其转化为[**嵌入**](https://learn.microsoft.com/en-us/semantic-kernel/memories/embeddings)，并进行适当的索引。当提示时，模型会根据提示内容接收到与之相关的上下文数据。
- en: It is the next step up the ladder since it requires some effort on your side
    (e.g., giving Sam $10) and can involve some serious implementation costs.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这是更进一步的一步，因为它需要你付出一些努力（例如，给 Sam 10 美元），并且可能涉及一些实际的实施成本。
- en: Now Sam plays your favorite tune — however, you are still not happy with the
    way he plays it. Somehow, you want more Swing, or another touch is missing. So
    you take the next step on our ladder.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 Sam 演奏了你最喜欢的曲子——然而，你仍然对他的演奏方式不满意。某种程度上，你希望加点 Swing，或者少了某种感觉。所以你踏上了我们阶梯的下一步。
- en: '3\. Fine-tuning: Learning and Adapting to Feedback'
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 微调：学习并适应反馈
- en: '![](../Images/2e29e41975a0cf1939e4b49116444ccc.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e29e41975a0cf1939e4b49116444ccc.png)'
- en: Fine-tuning a model by iteratively giving it (human) feedback — just like a
    piano teacher
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 通过不断提供（人类）反馈来微调模型——就像钢琴老师一样
- en: This is where my analogy starts to get a bit shaky, especially when we’re taking
    the word “tuning” in our musical context literally. Here, we are not talking about
    tuning Sam’s piano. Instead, when thinking about fine-tuning in this context,
    I am referring to taking a considerable amount of time to work with Sam until
    he plays how we like him to play. So we basically give him piano lessons, providing
    **feedback** on his playing and **supervising his progress**.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我的类比开始有些动摇的地方，特别是当我们在音乐语境中字面理解“调音”一词时。在这里，我们并不是在谈论调音萨姆的钢琴。相反，当我在这个语境中提到微调时，我是指花费大量时间与萨姆一起工作，直到他弹奏出我们希望的方式。所以我们基本上是在给他上钢琴课，提供**反馈**来改进他的演奏，并**监督他的进展**。
- en: Back to language models, one of the approaches here is referred to as **reinforcement
    learning from human feedback (**[**RLHF**](https://huggingface.co/blog/rlhf)**)**,
    and it fits well into our picture of a strict piano teacher. Fine-tuning takes
    the customization process further by adapting (i.e. tuning) the model’s knowledge
    and skills to a particular task or domain. Again, putting it a little more technical,
    what happens here is based on [Reinforecement Learning](https://en.wikipedia.org/wiki/Reinforcement_learning),
    which has a Reward Function at its core. This reward dynamically adapts to the
    human feedback, which is often given as a human A/B judgement label to the textual
    output of the model, given the same prompt.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 回到语言模型，这里的一种方法被称为**强化学习（通过人类反馈）**（[**RLHF**](https://huggingface.co/blog/rlhf)），它很好地契合了我们对严格钢琴教师的类比。微调通过将模型的知识和技能调整（即调音）到特定任务或领域，进一步推进了定制化过程。同样，更技术一点来说，这里发生的事情基于[强化学习](https://en.wikipedia.org/wiki/Reinforcement_learning)，其核心是奖励函数。这个奖励会动态适应人类的反馈，这通常是人类对模型文本输出的A/B判断标签，基于相同的提示。
- en: For this process, we need considerable (computational) resources, large amounts
    of curated data, and/or human feedback. This explains why it is already quite
    high on our escalation ladder, but it’s not the end yet.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个过程，我们需要大量的（计算）资源、大量精心挑选的数据和/或人工反馈。这也解释了为什么它已经排在我们提升阶梯的较高位置，但这还不是终点。
- en: What if we want Sam to play or do very specific musical things? For example,
    we want him to sing along — that would make Sam quite nervous (at least, that’s
    what this specific request made me feel, back in the days), because Sam hasn’t
    been trained and never tried to sing…
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望萨姆演奏或做一些非常具体的音乐事情呢？例如，我们希望他一起唱歌——这会让萨姆相当紧张（至少，曾经有过这样的请求让我有这种感觉），因为萨姆没有接受过训练，也从未尝试过唱歌……
- en: '4\. Custom Model Training: Breeding a New Expert'
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4. 自定义模型训练：培养一位新的专家
- en: '![](../Images/ff9e6f1bbf47e95d49080fbbedd2333f.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff9e6f1bbf47e95d49080fbbedd2333f.png)'
- en: Training a model from scratch to breed an expert
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 从零开始训练模型来培养专家
- en: At the pinnacle of the Escalation Ladder we encounter custom model (pre-)training,
    where you essentially create a new expert from scratch, tailored to your exact
    requirements. This is also where my analogy might crumble (never said it was perfect!)
    — how do you breed a new piano player from scratch? But let’s stick to it anyway
    — let’s think about training *Samantha*, who has never played any music nor sung
    in her entire life. So we invest heavily in her education and skills, sending
    her to the top institutions where musicians learn what we want them to play.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在提升阶梯的顶端，我们遇到了自定义模型（预）训练，您实际上是从零开始创建一个全新的专家，完全按照您的要求量身定制。这也是我的类比可能会崩溃的地方（我可没说它是完美的！）——如何从头开始培养一位钢琴演奏者？不过我们还是继续这个类比——假设我们要训练*萨曼莎*，她一生中从未演奏过任何音乐，也没有唱过歌。所以我们将大量投资于她的教育和技能，把她送到那些音乐家学习我们希望他们演奏的顶级学府。
- en: Here we are nurturing **a new language model from the ground up**, instilling
    it with the knowledge and data necessary to perform in our particular domain.
    By carefully curating the training data and adjusting the model and its architecture,
    we can develop a highly specialized and optimized language model capable of tackling
    even the most proprietary tasks within your organization. In this process, the
    amount of data and number of parameters that current large language models are
    trained on can get quite staggering. For instance, rumours suggest that OpenAI’s
    most recent [GPT-4 has 1.76 trillion parameters](https://en.wikipedia.org/wiki/GPT-4).
    Hence, this approach often requires enormous resources and is beyond reach for
    many businesses today.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们正在从**零基础培养一种新的语言模型**，为其注入在我们特定领域内执行所需的知识和数据。通过精心策划训练数据并调整模型及其架构，我们可以开发出一个高度专业化和优化的语言模型，能够应对即使是组织内最专有的任务。在这个过程中，目前的大型语言模型所训练的数据量和参数数量可能会相当惊人。例如，传闻称OpenAI最新的[GPT-4有1.76万亿个参数](https://en.wikipedia.org/wiki/GPT-4)。因此，这种方法通常需要巨大的资源，对于今天的许多企业来说，往往是难以企及的。
- en: Conclusion
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: Just like our journey from timidly asking Sam to play Dave Brubeck’s “Take Five”
    up to developing new talent — as we progress through each level of the Escalation
    Ladder, the effort and resources required increase significantly, but so does
    the level of customization and control we gain over the language model’s capabilities.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们从害羞地要求Sam演奏Dave Brubeck的《Take Five》到开发新人才一样——随着我们逐步通过每个“升级阶梯”，所需的努力和资源显著增加，但我们在语言模型能力上的定制化和控制程度也随之增加。
- en: Of course, much like most frameworks, this one is not as clear cut as I have
    presented it here. There can be hybrid or mixed approaches, and even the finest
    RAG implementation will need you to do some proper prompting. However, by understanding
    and reminding yourself of this framework, I believe you can strategically determine
    the appropriate level of customization needed for your specific use cases. To
    unlock the full potential of Language AI, you will need to strike the right balance
    between effort and cost, and tailored performance. It may also help bridge the
    communication gap between business and IT when it comes to Language AI model adaption
    and implementation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，就像大多数框架一样，这个框架并不像我在这里展示的那样简单明了。可能存在混合或综合的方法，甚至是最精细的RAG实现，也需要你进行适当的提示。然而，通过理解并时常提醒自己这个框架，我相信你可以战略性地确定为特定使用场景所需的定制化程度。要释放语言AI的全部潜力，你需要在努力与成本之间找到合适的平衡，同时实现量身定制的表现。这也有助于在语言AI模型的适应和实施过程中，弥合商业与IT之间的沟通鸿沟。
- en: I hope you enjoyed meeting Sam and Samantha and adapting their abilities on
    the piano. I welcome you to comment, critique, or expand on what you think of
    this analogy in the comments below, or simply share this article with people who
    might benefit from it.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你们喜欢与Sam和Samantha见面，并适应他们在钢琴上的能力。我欢迎你在下方评论、批评或扩展你对这个类比的看法，或者简单地将这篇文章分享给可能从中受益的人。
- en: '*Notes and References:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*注释和参考文献：'
- en: This article has been inspired by this technical* [*article*](https://www.databricks.com/de/glossary/retrieval-augmented-generation-rag)
    *on Retrieval Augmented Generation from Databricks.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的灵感来源于这篇关于检索增强生成技术的* [*文章*](https://www.databricks.com/de/glossary/retrieval-augmented-generation-rag)
    *，由Databricks发布。
- en: All drawings are hand crafted with pride by the author :)*
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 所有插图均由作者手工精心制作，充满自豪感 :)*
