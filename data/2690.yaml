- en: 'Least Squares Regression, Explained: A Visual Guide with Code Examples for
    Beginners'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最小二乘回归解析：带有代码示例的可视化指南（适合初学者）
- en: 原文：[https://towardsdatascience.com/least-squares-regression-explained-a-visual-guide-with-code-examples-for-beginners-2e5ad011eae4?source=collection_archive---------1-----------------------#2024-11-05](https://towardsdatascience.com/least-squares-regression-explained-a-visual-guide-with-code-examples-for-beginners-2e5ad011eae4?source=collection_archive---------1-----------------------#2024-11-05)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/least-squares-regression-explained-a-visual-guide-with-code-examples-for-beginners-2e5ad011eae4?source=collection_archive---------1-----------------------#2024-11-05](https://towardsdatascience.com/least-squares-regression-explained-a-visual-guide-with-code-examples-for-beginners-2e5ad011eae4?source=collection_archive---------1-----------------------#2024-11-05)
- en: REGRESSION ALGORITHM
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归算法
- en: Gliding through points to minimize squares
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 滑动穿越数据点以最小化平方差
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--2e5ad011eae4--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--2e5ad011eae4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2e5ad011eae4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2e5ad011eae4--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--2e5ad011eae4--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@samybaladram?source=post_page---byline--2e5ad011eae4--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--2e5ad011eae4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2e5ad011eae4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2e5ad011eae4--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--2e5ad011eae4--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2e5ad011eae4--------------------------------)
    ·11 min read·Nov 5, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2e5ad011eae4--------------------------------)
    ·阅读时间：11 分钟·2024年11月5日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: When people start learning about data analysis, they usually begin with linear
    regression. There’s a good reason for this — it’s one of the most useful and straightforward
    ways to understand how regression works. The most common approaches to linear
    regression are called “Least Squares Methods” — these work by finding patterns
    in data by minimizing the squared differences between predictions and actual values.
    The most basic type is **Ordinary Least Squares** (OLS), which finds the best
    way to draw a straight line through your data points.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当人们开始学习数据分析时，他们通常从线性回归开始。这是有充分理由的——它是理解回归如何工作的最有用且直接的方式之一。线性回归最常见的方法被称为“最小二乘法”——它通过最小化预测值与实际值之间的平方差来寻找数据中的模式。最基本的类型是**普通最小二乘法**（OLS），它通过数据点找到最佳的直线拟合方式。
- en: Sometimes, though, OLS isn’t enough — especially when your data has many related
    features that can make the results unstable. That’s where **Ridge regression**
    comes in. Ridge regression does the same job as OLS but adds a special control
    that helps prevent the model from becoming too sensitive to any single feature.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有时，OLS 并不够——尤其是当你的数据有许多相关特征，可能导致结果不稳定时。这时，**岭回归**就派上用场了。岭回归与 OLS 做的工作相同，但它增加了一个特殊的控制项，帮助防止模型对任何单一特征过于敏感。
- en: Here, we’ll glide through two key types of Least Squares regression, exploring
    how these algorithms smoothly slide through your data points and see their differences
    in theory.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将快速了解两种最小二乘回归方法，探索这些算法如何平滑地穿过数据点，并在理论上看到它们的差异。
- en: '![](../Images/e13851516ec1dfc98b92fd91fe5661c6.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e13851516ec1dfc98b92fd91fe5661c6.png)'
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所有视觉内容：作者使用 Canva Pro 创建，已针对移动设备优化；在桌面上可能显示过大。
- en: Definition
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义
- en: Linear Regression is a statistical method that predicts numerical values using
    a linear equation. It models the relationship between a dependent variable and
    one or more independent variables by fitting a straight line (or plane, in multiple
    dimensions) through the data points. The model calculates coefficients for each
    feature, representing their impact on the outcome. To get a result, you input
    your data’s feature values into the linear equation to compute the predicted value.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归是一种统计方法，使用线性方程预测数值。它通过拟合一条直线（或在多维情况下为平面）来模拟因变量与一个或多个自变量之间的关系。模型计算每个特征的系数，表示它们对结果的影响。要得到结果，你需要将数据的特征值输入到线性方程中以计算预测值。
- en: 📊 Dataset Used
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 📊 使用的数据集
- en: To illustrate our concepts, we’ll use [our standard dataset](/dummy-regressor-explained-a-visual-guide-with-code-examples-for-beginners-4007c3d16629)
    that predicts the number of golfers visiting on a given day. This dataset includes
    variables like weather outlook, temperature, humidity, and wind conditions.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明我们的概念，我们将使用[我们的标准数据集](/dummy-regressor-explained-a-visual-guide-with-code-examples-for-beginners-4007c3d16629)，它用于预测某一天来访的高尔夫球手人数。该数据集包括天气预报、温度、湿度和风力等变量。
- en: '![](../Images/8f70777fcdcfbe631c352c17a649d816.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f70777fcdcfbe631c352c17a649d816.png)'
- en: 'Columns: ‘Outlook’ (one-hot encoded to sunny, overcast, rain), ‘Temperature’
    (in Fahrenheit), ‘Humidity’ (in %), ‘Wind’ (Yes/No) and ‘Number of Players’ (numerical,
    target feature)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 列：‘Outlook’（一热编码为晴天、多云、雨天）、‘Temperature’（以华氏度为单位）、‘Humidity’（以百分比表示）、‘Wind’（是/否）以及‘Number
    of Players’（数值型，目标特征）
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: While it is not mandatory, to effectively use Linear Regression — including
    Ridge Regression — we can standardize the numerical features first.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然不是强制性的，但为了有效使用线性回归——包括岭回归——我们可以首先对数值特征进行标准化。
- en: '![](../Images/856bb5024939a0acbe57928f62346faa.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/856bb5024939a0acbe57928f62346faa.png)'
- en: Standard scaling is applied to ‘Temperature’ and ‘Humidity’ while the one-hot
    encoding is applied to ‘Outlook’ and ‘Wind’
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对“温度”和“湿度”应用标准化处理，而对“天气预报”和“风力”应用一热编码处理。
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Main Mechanism
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主要机制
- en: 'Linear Regression predicts numbers by making a straight line (or hyperplane)
    from the data:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归通过从数据中绘制一条直线（或超平面）来预测数字：
- en: The model finds the best line by making the gaps between the real values and
    the line’s predicted values as small as possible. This is called “least squares.”
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该模型通过使真实值与线性预测值之间的误差尽可能小来找到最佳拟合线。这被称为“最小二乘法”。
- en: Each input gets a number (coefficient/weight) that shows how much it changes
    the final answer. There’s also a starting number (intercept/bias) that’s used
    when all inputs are zero.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个输入都会得到一个数字（系数/权重），表示它对最终结果的影响程度。还有一个起始数字（截距/偏置），当所有输入为零时使用该数字。
- en: To predict a new answer, the model takes each input, multiplies it by its number,
    adds all these up, plus adds the starting number. This gives you the predicted
    answer.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要预测一个新的答案，模型会将每个输入乘以其对应的数字，然后加总这些值，再加上起始数字。这样就得到了预测结果。
- en: '![](../Images/c72b0e92063ef36ac8fb47688848a377.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c72b0e92063ef36ac8fb47688848a377.png)'
- en: '**Ordinary Least Squares (OLS) Regression**'
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**普通最小二乘法（OLS）回归**'
- en: Let’s start with Ordinary Least Squares (OLS) — the fundamental approach to
    linear regression. The goal of OLS is to find the best-fitting line through our
    data points. We do this by measuring how “wrong” our predictions are compared
    to actual values, and then finding the line that makes these errors as small as
    possible. When we say “error,” we mean the vertical distance between each point
    and our line — in other words, how far off our predictions are from reality. Let’s
    see what happened in 2D case first.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从普通最小二乘法（OLS）开始——线性回归的基本方法。OLS的目标是找到一条最佳拟合线，通过数据点。我们通过衡量预测与实际值之间的“误差”来实现这一点，然后找到一条使这些误差尽可能小的线。当我们说“误差”时，我们指的是每个点与线之间的垂直距离——换句话说，就是我们的预测与实际值之间的偏差。首先让我们看看二维情况发生了什么。
- en: In 2D Case
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 二维情况
- en: 'In 2D case, we can imagine the linear regression algorithm like this:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在二维情况下，我们可以这样想象线性回归算法：
- en: '![](../Images/6c8ce9540dd0d3e926482943741c9ad0.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c8ce9540dd0d3e926482943741c9ad0.png)'
- en: 'Here’s the explanation of the process above:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这是上述过程的解释：
- en: '1.We start with a training set, where each row has:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 1.我们从一个训练集开始，每一行包含：
- en: '· *x* : our input feature (the numbers 1, 2, 3, 1, 2)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: · *x* ：我们的输入特征（数字1，2，3，1，2）
- en: '· *y* : our target values (0, 1, 1, 2, 3)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: · *y* ：我们的目标值（0，1，1，2，3）
- en: 2\. We can plot these points on a scatter plot and we want to find a line *y*
    = *β*₀ + *β*₁*x* that best fits these points
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 我们可以将这些点绘制在散点图上，我们希望找到一条最佳拟合这些点的直线 *y* = *β*₀ + *β*₁*x*
- en: '3\. For any given line (any *β*₀ and *β*₁), we can measure how good it is by:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 对于任何给定的直线（任何 *β*₀ 和 *β*₁），我们可以通过以下方法来衡量它的好坏：
- en: · Calculating the vertical distance (*d*₁, *d*₂, *d*₃, *d*₄, *d*₅) from each
    point to the line
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: · 计算每个点到直线的垂直距离 (*d*₁, *d*₂, *d*₃, *d*₄, *d*₅)
- en: · These distances are |*y* — (*β*₀ + *β*₁*x*)| for each point
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: · 这些距离是每个点的 |*y* — (*β*₀ + *β*₁*x*)|
- en: '4\. Our optimization goal is to find *β*₀ and *β*₁ that minimize the sum of
    squared distances: *d*₁² + *d*₂² + *d*₃² + *d*₄² + *d*₅². In vector notation,
    this is written as ||*y* — *Xβ*||², where *X* = [1 *x*] contains our input data
    (with 1’s for the intercept) and *β* = [*β*₀ *β*₁]ᵀ contains our coefficients.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 我们的优化目标是找到 *β*₀ 和 *β*₁，使得平方距离之和最小化：*d*₁² + *d*₂² + *d*₃² + *d*₄² + *d*₅²。在向量表示法中，这可以写作
    ||*y* — *Xβ*||²，其中 *X* = [1 *x*] 包含我们的输入数据（包含用于截距的1），*β* = [*β*₀ *β*₁]ᵀ 包含我们的系数。
- en: '5\. The optimal solution has a closed form: *β* = (*X*ᵀ*X*)⁻¹*X*ᵀy. Calculating
    this we get *β*₀ = -0.196 (intercept), *β*₁ = 0.761 (slope).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 最优解具有封闭形式：*β* = (*X*ᵀ*X*)⁻¹*X*ᵀy。通过计算，我们得到 *β*₀ = -0.196（截距），*β*₁ = 0.761（斜率）。
- en: This vector notation makes the formula more compact and shows that we’re really
    working with matrices and vectors rather than individual points. We will see more
    details of our calculation next in the multidimensional case.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这个向量表示法使得公式更为简洁，且展示了我们实际上是使用矩阵和向量进行运算，而不是单独的点。我们将在下文的多维情况中看到更多关于计算的细节。
- en: In Multidimensional Case (📊 Dataset)
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在多维情况下（📊 数据集）
- en: Again, the goal of OLS is to find coefficients (*β*) that minimize the squared
    differences between our predictions and actual values. Mathematically, we express
    this as **minimizing** ||*y* — *Xβ*||², where *X* is our data matrix and *y* contains
    our target values.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，最小二乘法（OLS）的目标是找到系数 (*β*)，使得我们的预测与实际值之间的平方差最小化。数学上，我们将其表示为 **最小化** ||*y* —
    *Xβ*||²，其中 *X* 是我们的数据矩阵，*y* 包含我们的目标值。
- en: 'The training process follows these key steps:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程遵循以下关键步骤：
- en: Training Step
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练步骤
- en: 1\. Prepare our data matrix *X*. This involves adding a column of ones to account
    for the bias/intercept term (*β*₀).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 准备我们的数据矩阵 *X*。这涉及到添加一列1来考虑偏置/截距项 (*β*₀)。
- en: '![](../Images/94f0dc6a1292fe7a1cd4d41f648b377b.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94f0dc6a1292fe7a1cd4d41f648b377b.png)'
- en: '2\. Instead of iteratively searching for the best coefficients, we can compute
    them directly using the normal equation:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 我们可以通过使用正规方程直接计算系数，而不是通过迭代搜索最佳系数：
- en: '*β* = (*X*ᵀ*X*)⁻¹*X*ᵀ*y*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*β* = (*X*ᵀ*X*)⁻¹*X*ᵀ*y*'
- en: 'where:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: · *β* is the vector of estimated coefficients,
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: · *β* 是估计系数的向量，
- en: · *X* is the dataset matrix(including a column for the intercept),
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: · *X* 是数据集矩阵（包括一个用于截距的列），
- en: · *y* is the label,
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: · *y* 是标签，
- en: · *X*ᵀ represents the transpose of matrix *X*,
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: · *X*ᵀ 表示矩阵 *X* 的转置，
- en: · ⁻¹ represents the inverse of the matrix.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: · ⁻¹ 表示矩阵的逆。
- en: 'Let’s break this down:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解一下：
- en: a. We multiply *X*ᵀ (*X* transpose) by *X*, giving us a square matrix
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: a. 我们将 *X*ᵀ（*X* 的转置）与 *X* 相乘，得到一个方阵
- en: '![](../Images/41c746d7c63229c8463c4b4451d547c4.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41c746d7c63229c8463c4b4451d547c4.png)'
- en: b. We compute the inverse of this matrix
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: b. 我们计算这个矩阵的逆
- en: '![](../Images/ec4c7598c2579c5b2aed1e6f0ba14c65.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ec4c7598c2579c5b2aed1e6f0ba14c65.png)'
- en: c. We compute *X*ᵀ*y*
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: c. 我们计算 *X*ᵀ*y*
- en: '![](../Images/56478c8b91a65d73a45096748d575778.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56478c8b91a65d73a45096748d575778.png)'
- en: d. We multiply (*X*ᵀ*X*)⁻¹ and *X*ᵀ*y* to get our coefficients
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: d. 我们将 (*X*ᵀ*X*)⁻¹ 与 *X*ᵀ*y* 相乘，以得到我们的系数
- en: '![](../Images/963c4e9a98a88c168f9c90739b1fc1a8.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/963c4e9a98a88c168f9c90739b1fc1a8.png)'
- en: Test Step
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试步骤
- en: 'Once we have our coefficients, making predictions is straightforward: we simply
    multiply our new data point by these coefficients to get our prediction.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们得到了系数，进行预测就变得非常简单：我们只需将新的数据点与这些系数相乘，就能得到我们的预测。
- en: In matrix notation, for a new data point *x**, the prediction *y** is calculated
    as
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在矩阵表示法中，对于一个新的数据点 *x*，预测 *y* 可以计算为
- en: '*y** = *x***β* = [1, x₁, x₂, …, xₚ] × [β₀, β₁, β₂, …, βₚ]ᵀ,'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* = *x* × *β* = [1, x₁, x₂, …, xₚ] × [β₀, β₁, β₂, …, βₚ]ᵀ，'
- en: where *β*₀ is the intercept and *β*₁ through *β*ₚ are the coefficients for each
    feature.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *β*₀ 是截距，*β*₁ 到 *β*ₚ 是每个特征的系数。
- en: '![](../Images/fc075c4c8801b373f49a757ac742fef5.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc075c4c8801b373f49a757ac742fef5.png)'
- en: Evaluation Step
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估步骤
- en: We can do the same process for all data points. For our dataset, here’s the
    final result with the RMSE as well.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对所有数据点执行相同的过程。对于我们的数据集，以下是最终结果，并附有 RMSE 值。
- en: '![](../Images/c0f9605d793360d6c403abdb3192e0d5.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c0f9605d793360d6c403abdb3192e0d5.png)'
- en: '**Ridge Regression**'
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**岭回归**'
- en: Now, let’s consider Ridge Regression, which builds upon OLS by addressing some
    of its limitations. The key insight of Ridge Regression is that sometimes the
    optimal OLS solution **involves very large coefficients**, which can lead to overfitting.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑岭回归，它在OLS的基础上解决了其一些局限性。岭回归的关键思想是，有时最优的OLS解**涉及非常大的系数**，这可能导致过拟合。
- en: 'Ridge Regression adds a penalty term (*λ*||*β*||²) to the objective function.
    This term discourages large coefficients by adding their squared values to what
    we’re minimizing. The full objective becomes:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归在目标函数中添加了惩罚项 (*λ*||*β*||²)。该项通过将系数的平方值添加到最小化目标中，来抑制大系数的出现。目标函数变为：
- en: min ||*y* — *X*β||² + λ||*β*||²
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: min ||*y* — *X*β||² + λ||*β*||²
- en: The *λ* (lambda) parameter controls how much we penalize large coefficients.
    When *λ* = 0, we get OLS; as *λ* increases, the coefficients shrink toward zero
    (but never quite reach it).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*λ*（lambda）参数控制我们对大系数的惩罚程度。当 *λ* = 0 时，我们得到OLS；随着 *λ* 增加，系数会向零收缩（但永远不会完全为零）。'
- en: Training Step
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练步骤
- en: Just like OLS, prepare our data matrix *X*. This involves adding a column of
    ones to account for the intercept term (*β*₀).
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 就像OLS一样，准备我们的数据矩阵 *X*。这包括添加一列全为1的值来考虑截距项 (*β*₀)。
- en: 'The training process for Ridge follows a similar pattern to OLS, but with a
    modification. The closed-form solution becomes:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 岭回归的训练过程与OLS类似，但有所修改。封闭形式的解变为：
- en: '*β* = (*X*ᵀ*X*+ λI)⁻¹*X*ᵀ*y*'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*β* = (*X*ᵀ*X*+ λI)⁻¹*X*ᵀ*y*'
- en: 'where:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: · *I* is the identity matrix (with the first element, corresponding to *β*₀,
    sometimes set to 0 to exclude the intercept from regularization in some implementations),
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: · *I*是单位矩阵（第一个元素对应 *β*₀，某些实现中有时设置为0，以排除截距项的正则化），
- en: · λ is the regularization value.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: · λ是正则化值。
- en: · *Y* is the vector of observed dependent variable values.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: · *Y*是观察到的因变量值的向量。
- en: · Other symbols remain as defined in the OLS section.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: · 其他符号与OLS部分定义相同。
- en: 'Let’s break this down:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们拆解一下：
- en: a. We add *λ*I to *X*ᵀ*X.* The value of *λ* can be any positive number (say
    0.1).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: a. 我们将 *λ*I 加到 *X*ᵀ*X* 上。*λ*的值可以是任何正数（比如0.1）。
- en: '![](../Images/1f631555a5c3da55dcc0e5c818fdc266.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f631555a5c3da55dcc0e5c818fdc266.png)'
- en: 'b. We compute the inverse of this matrix. The benefits of adding λI to *X*ᵀ*X*
    before inversion are:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: b. 我们计算该矩阵的逆。将 *λ*I 加到 *X*ᵀ*X* 后再进行求逆的好处是：
- en: · Makes the matrix invertible, even if *X*ᵀ*X* isn’t (solving a key numerical
    problem with OLS)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: · 使矩阵可逆，即使 *X*ᵀ*X* 不是（通过OLS解决一个关键的数值问题）
- en: · Shrinks the coefficients proportionally to *λ*
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: · 按 *λ* 比例缩小系数
- en: '![](../Images/41521c73d61c93a7d2f3d061ed957b9a.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41521c73d61c93a7d2f3d061ed957b9a.png)'
- en: c. We multiply (*X*ᵀ*X*+ *λI*)⁻¹ and *X*ᵀ*y* to get our coefficients
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: c. 我们计算 (*X*ᵀ*X*+ *λI*)⁻¹ 和 *X*ᵀ*y* 的乘积来获得系数
- en: '![](../Images/e4ed2341b5407401f67dd250d1b98f4f.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4ed2341b5407401f67dd250d1b98f4f.png)'
- en: '**Test Step**'
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**测试步骤**'
- en: The prediction process remains the same as OLS — multiply new data points by
    the coefficients. The difference lies in the coefficients themselves, which are
    typically smaller and more stable than their OLS counterparts.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 预测过程与OLS相同——将新的数据点与系数相乘。不同之处在于系数本身，通常它们比OLS的系数要小且更稳定。
- en: '![](../Images/918f8768c3d930ee52fc25e57f1c38bb.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/918f8768c3d930ee52fc25e57f1c38bb.png)'
- en: Evaluation Step
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估步骤
- en: We can do the same process for all data points. For our dataset, here’s the
    final result with the RMSE as well.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对所有数据点执行相同的过程。对于我们的数据集，这里是带有RMSE的最终结果。
- en: '![](../Images/d89851ba086f08012ca77a947dd7e06f.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d89851ba086f08012ca77a947dd7e06f.png)'
- en: '**Final Remarks: Choosing Between OLS and Ridge**'
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**最后备注：选择OLS与岭回归**'
- en: 'The choice between OLS and Ridge often depends on your data:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: OLS和岭回归的选择通常取决于你的数据：
- en: Use OLS when you have well-behaved data with little multicollinearity and enough
    samples (relative to features)
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你的数据表现良好，特征之间几乎没有多重共线性并且样本数足够时（相对于特征），使用OLS
- en: 'Use Ridge when you have:'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你有以下情况时，使用岭回归：
- en: '- Many features (relative to samples)'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 特征多（相对于样本）'
- en: '- Multicollinearity in your features'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 特征中的多重共线性'
- en: '- Signs of overfitting with OLS'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- OLS中的过拟合迹象'
- en: With Ridge, you’ll need to choose *λ*. Start with a range of values (often logarithmically
    spaced) and choose the one that gives the best validation performance.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用岭回归时，你需要选择 *λ*。从一系列值开始（通常以对数间隔），选择能够提供最佳验证性能的那个值。
- en: '![](../Images/0d0c60a41bac36424c7f233554495189.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0d0c60a41bac36424c7f233554495189.png)'
- en: Apparantly, the default value *λ = 1 gives the best RMSE for our dataset.*
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，默认值 *λ = 1* 为我们的数据集提供了最佳的RMSE。
- en: 🌟 OLS and Ridge Regression Code Summarized
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🌟 OLS 和岭回归代码总结
- en: '[PRE2]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/ff9ff4226a31e1ad21380175a7da9bac.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff9ff4226a31e1ad21380175a7da9bac.png)'
- en: Further Reading
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: For a detailed explanation of [OLS Linear Regression](https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LinearRegression.html)
    and [Ridge Regression](https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.Ridge.html),
    and its implementation in scikit-learn, readers can refer to their official documentation.
    It provides comprehensive information on their usage and parameters.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 [OLS 线性回归](https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LinearRegression.html)
    和 [岭回归](https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.Ridge.html)
    的详细解释及其在 scikit-learn 中的实现，读者可以参考它们的官方文档。文档提供了关于它们的使用和参数的全面信息。
- en: Technical Environment
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术环境
- en: This article uses Python 3.7 and scikit-learn 1.5\. While the concepts discussed
    are generally applicable, specific code implementations may vary slightly with
    different versions
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 本文使用的是 Python 3.7 和 scikit-learn 1.5。虽然讨论的概念通常适用，但在不同版本中，具体的代码实现可能会有所不同。
- en: About the Illustrations
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于插图
- en: Unless otherwise noted, all images are created by the author, incorporating
    licensed design elements from Canva Pro.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图片均由作者创建，且融入了 Canva Pro 授权设计元素。
- en: '𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝙍𝙚𝙜𝙧𝙚𝙨𝙨𝙞𝙤𝙣 𝘼𝙡𝙜𝙤𝙧𝙞𝙩𝙝𝙢𝙨 𝙝𝙚𝙧𝙚:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝙍𝙚𝙜𝙧𝙚𝙨𝙨𝙞𝙤𝙣 𝘼𝙡𝙜𝙤𝙧𝙞𝙩𝙝𝙢𝙨 𝙝𝙚𝙧𝙚:'
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----2e5ad011eae4--------------------------------)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----2e5ad011eae4--------------------------------)'
- en: Regression Algorithms
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归算法
- en: '[View list](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----2e5ad011eae4--------------------------------)5
    stories![A cartoon doll with pigtails and a pink hat. This “dummy” doll, with
    its basic design and heart-adorned shirt, visually represents the concept of a
    dummy regressor in machine. Just as this toy-like figure is a simplified, static
    representation of a person, a dummy regressor is a basic models serve as baselines
    for more sophisticated analyses.](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----2e5ad011eae4--------------------------------)5篇故事![一只戴着粉色帽子、绑着小辫子的卡通娃娃。这个“假人”娃娃，凭借其简单的设计和心形图案的衣服，形象地代表了机器学习中假回归器的概念。就像这个玩具般的形象是一个简化、静态的人物表现一样，假回归器是一个基本模型，用作更复杂分析的基线。](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)'
- en: '𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:'
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----2e5ad011eae4--------------------------------)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----2e5ad011eae4--------------------------------)'
- en: Classification Algorithms
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类算法
- en: '[View list](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----2e5ad011eae4--------------------------------)8
    stories![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----2e5ad011eae4--------------------------------)8篇故事![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)'
