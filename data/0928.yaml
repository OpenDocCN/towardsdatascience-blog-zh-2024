- en: Deep Dive into Transformers by Hand ✍︎
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入探讨变形金刚 ✍︎
- en: 原文：[https://towardsdatascience.com/deep-dive-into-transformers-by-hand-%EF%B8%8E-68b8be4bd813?source=collection_archive---------0-----------------------#2024-04-12](https://towardsdatascience.com/deep-dive-into-transformers-by-hand-%EF%B8%8E-68b8be4bd813?source=collection_archive---------0-----------------------#2024-04-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/deep-dive-into-transformers-by-hand-%EF%B8%8E-68b8be4bd813?source=collection_archive---------0-----------------------#2024-04-12](https://towardsdatascience.com/deep-dive-into-transformers-by-hand-%EF%B8%8E-68b8be4bd813?source=collection_archive---------0-----------------------#2024-04-12)
- en: Explore the details behind the power of transformers
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索变形金刚背后强大功能的细节
- en: '[](https://medium.com/@srijanie.dey?source=post_page---byline--68b8be4bd813--------------------------------)[![Srijanie
    Dey, PhD](../Images/2b3292a3b22d712d91d0bfc14df64446.png)](https://medium.com/@srijanie.dey?source=post_page---byline--68b8be4bd813--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--68b8be4bd813--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--68b8be4bd813--------------------------------)
    [Srijanie Dey, PhD](https://medium.com/@srijanie.dey?source=post_page---byline--68b8be4bd813--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@srijanie.dey?source=post_page---byline--68b8be4bd813--------------------------------)[![Srijanie
    Dey, PhD](../Images/2b3292a3b22d712d91d0bfc14df64446.png)](https://medium.com/@srijanie.dey?source=post_page---byline--68b8be4bd813--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--68b8be4bd813--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--68b8be4bd813--------------------------------)
    [Srijanie Dey, PhD](https://medium.com/@srijanie.dey?source=post_page---byline--68b8be4bd813--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--68b8be4bd813--------------------------------)
    ·6 min read·Apr 12, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--68b8be4bd813--------------------------------)
    ·6分钟阅读·2024年4月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: There has been a new development in our neighborhood.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们小区发生了一件新事。
- en: A ‘Robo-Truck,’ as my son likes to call it, has made its new home on our street.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 一辆我儿子喜欢叫做“机器人卡车”的新车已经在我们的街道上安家了。
- en: It is a Tesla Cyber Truck and I have tried to explain that name to my son many
    times but he insists on calling it Robo-Truck. Now every time I look at Robo-Truck
    and hear that name, it reminds me of the movie Transformers where robots could
    transform to and from cars.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 那是一辆特斯拉Cyber Truck，我已经尝试多次向儿子解释这个名字的含义，但他坚持称其为“机器人卡车”。现在每次我看着机器人卡车并听到这个名字时，它让我想起电影《变形金刚》，其中的机器人能够在汽车和机器人之间变形。
- en: And isn’t it strange that Transformers as we know them today could very well
    be on their way to powering these Robo-Trucks? It’s almost a full circle moment.
    But where am I going with all these?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 难道不奇怪吗，今天我们所知道的变形金刚很可能正在为这些机器人卡车提供动力？这几乎是一个完整的循环。那么，我在说这些有什么目的呢？
- en: Well, I am heading to the destination — Transformers. Not the robot car ones
    but the neural network ones. And you are invited!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我的目的地就是——变形金刚。不是指机器人汽车，而是神经网络中的变形金刚。你被邀请了！
- en: '![](../Images/51e6a28c10ef051412c2b9cfbb2838d9.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51e6a28c10ef051412c2b9cfbb2838d9.png)'
- en: Image by author (Our Transformer — ‘Robtimus Prime’. Colors as mandated by my
    artist son.)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供（我们的变形金刚 — ‘机器人霸主’。颜色由我的艺术家儿子指定。）
- en: What are Transformers?
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是变形金刚？
- en: Transformers are essentially neural networks. Neural networks that specialize
    in learning context from the data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 变形金刚本质上是神经网络。专门用于从数据中学习上下文的神经网络。
- en: But what makes them special is the presence of mechanisms that eliminate the
    need for **labeled datasets** and **convolution or recurrence** in the network.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 但使它们与众不同的是，存在一些机制，消除了对**标注数据集**和**卷积或递归**的需求。
- en: What are these special mechanisms?
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这些特殊的机制是什么？
- en: There are many. But the two mechanisms that are truly the force behind the transformers
    are attention weighting and feed-forward networks (FFN).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 变形金刚有很多种。但是，真正驱动变形金刚的两个机制是注意力加权和前馈网络（FFN）。
- en: What is attention-weighting?
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是注意力加权？
- en: Attention-weighting is a technique by which the model learns which part of the
    incoming sequence needs to be focused on. Think of it as the ‘Eye of Sauron’ scanning
    everything at all times and throwing light on the parts that are relevant.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力加权是一种技术，通过它，模型学习要集中关注输入序列中的哪个部分。可以把它想象成“索隆之眼”始终扫描所有内容，并将光照射到相关部分。
- en: 'Fun-fact: Apparently, the researchers had almost named the Transformer model
    ‘Attention-Net’, given Attention is such a crucial part of it.'
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 有趣的事实：显然，研究人员几乎把Transformer模型命名为“Attention-Net”，因为注意力在其中起着如此关键的作用。
- en: '**What is FFN?**'
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**什么是FFN？**'
- en: In the context of transformers, FFN is essentially a regular multilayer perceptron
    acting on a batch of independent data vectors. Combined with attention, it produces
    the correct ‘position-dimension’ combination.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在Transformer的上下文中，FFN本质上是一个常规的多层感知器，作用于一批独立的数据向量。结合注意力机制，它生成正确的“位置-维度”组合。
- en: '**How do Attention and FFN work?**'
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**注意力和FFN是如何工作的？**'
- en: So, without further ado, let’s dive into how **attention-weighting** and **FFN**
    make transformers so powerful.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，不再拖延，让我们深入了解**注意力加权**和**FFN**如何使Transformer如此强大。
- en: This discussion is based on Prof. Tom Yeh’s wonderful AI by Hand Series on [Transformers](https://lnkd.in/g39jcD7j)
    . (All the images below, unless otherwise noted, are by Prof. Tom Yeh from the
    above-mentioned LinkedIn posts, which I have edited with his permission.)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇讨论基于Tom Yeh教授的精彩AI系列《手把手教你理解[Transformer](https://lnkd.in/g39jcD7j)》。 （除非另有说明，下面的所有图片都来自Tom
    Yeh教授的LinkedIn帖子，我已获得他的许可对其进行编辑。）
- en: 'So here we go:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们开始吧：
- en: 'The key ideas here : **attention weighting and feed-forward network (FFN)**.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键思想是：**注意力加权和前馈网络（FFN）**。
- en: 'Keeping those in mind, suppose we are given:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 记住这些，假设我们给定了：
- en: 5 input features from a previous block (A 3x5 matrix here, where X1, X2, X3,
    X4 and X5 are the features and each of the three rows denote their characteristics
    respectively.)
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自前一模块的5个输入特征（这里是一个3x5矩阵，其中X1、X2、X3、X4和X5是特征，每一行分别表示它们的特征）。
- en: '![](../Images/8fbc7712aec4f5834dac58f2c4724322.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8fbc7712aec4f5834dac58f2c4724322.png)'
- en: '[1] **Obtain attention weight matrix A**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] **获取注意力权重矩阵 A**'
- en: The first step in the process is to obtain the **attention weight matrix A**.
    This is the part where the self-attention mechanism comes to play. What it is
    trying to do is find the most relevant parts in this input sequence.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 过程中的第一步是获取**注意力权重矩阵A**。这部分正是自注意力机制发挥作用的地方。它试图做的是在这个输入序列中找到最相关的部分。
- en: We do it by feeding the input features into the query-key (QK) module. For simplicity,
    the details of the QK module are not included here.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将输入特征输入查询-键（QK）模块来完成此操作。为了简单起见，这里未包含QK模块的详细信息。
- en: '![](../Images/98c0a5d3d06cbc56f250062940a59e77.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/98c0a5d3d06cbc56f250062940a59e77.png)'
- en: '[2] **Attention Weighting**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] **注意力加权**'
- en: Once we have the **attention weight matrix A (5x5)**, we multiply the input
    features (3x5) with it to obtain the **attention-weighted features Z**.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们得到了**注意力权重矩阵A（5x5）**，就可以将输入特征（3x5）与其相乘，从而得到**注意力加权特征Z**。
- en: '![](../Images/88977d9819fb55ceb71256aa238c4cb1.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/88977d9819fb55ceb71256aa238c4cb1.png)'
- en: The important part here is that the features here are combined **based on their
    positions** P1, P2 and P3 i.e. **horizontally**.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这里重要的一点是，这些特征是**根据它们的位置**P1、P2和P3进行组合的，即**水平组合**。
- en: 'To break it down further, consider this calculation performed row-wise:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步细化，考虑按行执行的计算：
- en: P1 X A1 = Z1 → Position [1,1] = 11
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: P1 X A1 = Z1 → 位置 [1,1] = 11
- en: P1 X A2 = Z2 → Position [1,2] = 6
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: P1 X A2 = Z2 → 位置 [1,2] = 6
- en: P1 X A3 = Z3 → Position [1,3] = 7
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: P1 X A3 = Z3 → 位置 [1,3] = 7
- en: P1 X A4 = Z4 → Position [1,4] = 7
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: P1 X A4 = Z4 → 位置 [1,4] = 7
- en: P1 X A5 = Z5 → Positon [1,5] = 5
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: P1 X A5 = Z5 → 位置 [1,5] = 5
- en: .
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: .
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: .
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: P2 X A4 = Z4 → Position [2,4] = 3
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: P2 X A4 = Z4 → 位置 [2,4] = 3
- en: P3 X A5 = Z5 →Position [3,5] = 1
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: P3 X A5 = Z5 → 位置 [3,5] = 1
- en: 'As an example:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子：
- en: '![](../Images/16388ba68af2bae868e9845f6b719465.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/16388ba68af2bae868e9845f6b719465.png)'
- en: It seems a little tedious in the beginning but follow the multiplication row-wise
    and the result should be pretty straight-forward.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一开始似乎有些繁琐，但按照行乘法进行，结果应该是相当直接的。
- en: 'Cool thing is the way our attention-weight matrix **A** is arranged, the new
    features **Z** turn out to be the combinations of **X** as below:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，由于我们的注意力权重矩阵**A**的排列方式，新的特征**Z**实际上是**X**的组合，如下所示：
- en: Z1 = X1 + X2
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Z1 = X1 + X2
- en: Z2 = X2 + X3
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Z2 = X2 + X3
- en: Z3 = X3 + X4
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Z3 = X3 + X4
- en: Z4 = X4 + X5
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Z4 = X4 + X5
- en: Z5 = X5 + X1
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Z5 = X5 + X1
- en: '(Hint : Look at the positions of 0s and 1s in matrix **A**).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: （提示：查看矩阵**A**中0和1的位置）。
- en: '[3] **FFN : First Layer**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] **FFN : 第一层**'
- en: The next step is to feed the attention-weighted features into the feed-forward
    neural network.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将注意力加权特征输入到前馈神经网络中。
- en: 'However, the difference here lies in **combining the values across dimensions**
    as opposed to positions in the previous step. It is done as below:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这里与之前步骤的不同之处在于**跨维度组合值**，而不是在位置上进行组合。其操作如下：
- en: '![](../Images/ed26d0b60d3f60310db8a988a7a4f437.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed26d0b60d3f60310db8a988a7a4f437.png)'
- en: What this does is that it looks at the data from the other direction.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的效果是，它从另一个方向查看数据。
- en: '**- In the attention step, we combined our input on the basis of the original
    features to obtain new features.**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**- 在注意力步骤中，我们基于原始特征将输入进行组合，以获得新的特征。**'
- en: '**- In this FFN step, we consider their characteristics i.e. combine features
    vertically to obtain our new matrix.**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**- 在这个FFN步骤中，我们考虑它们的特征，即将特征垂直组合以获得我们的新矩阵。**'
- en: 'Eg: P1(1,1) * Z1(1,1)'
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 例如：P1(1,1) * Z1(1,1)
- en: ''
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: + P2(1,2) * Z1 (2,1)
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: + P2(1,2) * Z1 (2,1)
- en: ''
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: + P3 (1,3) * Z1(3,1) + b(1) = 11, where b is bias.
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: + P3(1,3) * Z1(3,1) + b(1) = 11，其中b是偏置。
- en: Once again element-wise row operations to the rescue. Notice that here the number
    of dimensions of the new matrix is increased to 4 here.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 再次进行逐元素的行操作。注意到这里新矩阵的维度增加到了4。
- en: '[4] **ReLU**'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] **ReLU**'
- en: 'Our favorite step : ReLU, where the negative values obtained in the previous
    matrix are returned as zero and the positive value remain unchanged.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最喜欢的步骤：ReLU，在这个步骤中，前面矩阵中得到的负值被归零，正值保持不变。
- en: '![](../Images/2dc50d4a90df572bd357073a42b871a5.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2dc50d4a90df572bd357073a42b871a5.png)'
- en: '[5] **FFN : Second Layer**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] **FFN：第二层**'
- en: Finally we pass it through the second layer where the dimensionality of the
    resultant matrix is reduced from 4 back to 3.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将其传递到第二层，在那里结果矩阵的维度从4降低回3。
- en: '![](../Images/ca42007aef10d51a48d23fb8a90ee9d4.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ca42007aef10d51a48d23fb8a90ee9d4.png)'
- en: The output here is ready to be fed to the next block (see its similarity to
    the original matrix) and the entire process is repeated from the beginning.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的输出已经准备好输入到下一个模块（请参见其与原始矩阵的相似性），并且整个过程从头开始重复。
- en: '**The two key things to remember here are:**'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**这里要记住的两件关键事是：**'
- en: '**The attention layer combines across positions (horizontally).**'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**注意力层在位置间（水平方向）进行组合。**'
- en: '**The feed-forward layer combines across dimensions (vertically).**'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**前馈层在维度间（垂直方向）进行组合。**'
- en: And this is the secret sauce behind the power of the transformers — the ability
    to analyze data from different directions.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是变形金刚强大之处的秘密——从不同方向分析数据的能力。
- en: 'To summarize the ideas above, here are the key points:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 总结以上观点，以下是关键要点：
- en: The transformer architecture can be perceived as a combination of the attention
    layer and the feed-forward layer.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Transformer架构可以被看作是注意力层和前馈层的组合。
- en: The **attention layer combines the features** to produce a new feature. E.g.
    think of combining two robots Robo-Truck and Optimus Prime to get a new robot
    Robtimus Prime.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**注意力层结合了特征**以生成新的特征。例如，想象将两个机器人Robo-Truck和Optimus Prime结合成一个新机器人Robtimus Prime。'
- en: The **feed-forward (FFN) layer combines the parts or the characteristics** of
    the a feature to produce new parts/characteristics. E.g. wheels of Robo-Truck
    and Ion-laser of Optimus Prime could produce a wheeled-laser.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**前馈（FFN）层结合了特征的部分或特征**，生成新的部分/特征。例如，Robo-Truck的轮子和Optimus Prime的离子激光可以组合成一个带轮子的激光。'
- en: The ever powerful Transformers
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强大的变形金刚
- en: Neural networks have existed for quite some time now. Convolutional Neural Networks
    (CNN) and Recurrent Neural Networks (RNN) had been reigning supreme but things
    took quite an eventful turn once Transformers were introduced in the year 2017\.
    And since then, the field of AI has grown at an exponential rate — with new models,
    new benchmarks, new learnings coming in every single day. And only time will tell
    if this phenomenal idea will one day lead the way for something even bigger —
    a real ‘Transformer’.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络已经存在了一段时间。卷积神经网络（CNN）和递归神经网络（RNN）曾一度占据主导地位，但自从2017年引入Transformer之后，局面发生了翻天覆地的变化。从那时起，人工智能领域以指数速度增长——每天都有新的模型、新的基准和新的学习成果涌现。只有时间才能证明，这一卓越的理念是否会引领我们迈向更大的突破——一个真正的“变形金刚”。
- en: But for now it would not be wrong to say that an idea can really *transform*
    how we live!
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 但目前来说，说一个理念可以真正*改变*我们生活的方式并不为过！
- en: '![](../Images/860b393c7b33bd9c4edfb8330c924678.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/860b393c7b33bd9c4edfb8330c924678.png)'
- en: Image by author
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: P.S. If you would like to work through this exercise on your own, here is the
    blank template for your use.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 附言：如果你想自己完成这个练习，下面是供你使用的空白模板。
- en: '[Blank Template for hand-exercise](https://drive.google.com/file/d/1F08laMdmwQ2vxYIqewOghS1eknaprgxe/view?usp=drive_link)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[手部练习空白模板](https://drive.google.com/file/d/1F08laMdmwQ2vxYIqewOghS1eknaprgxe/view?usp=drive_link)'
- en: Now go have some fun and create your own **Robtimus Prime**!
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在去玩得开心一点，创造属于你自己的**Robtimus Prime**！
