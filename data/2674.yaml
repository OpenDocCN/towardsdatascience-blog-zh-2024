- en: The Bias Variance Tradeoff and How it Shapes the LLMs of Today
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏差-方差权衡及其如何塑造今天的 LLM
- en: 原文：[https://towardsdatascience.com/the-bias-variance-tradeoff-and-how-it-shapes-the-llms-of-today-40e2c355f8a2?source=collection_archive---------4-----------------------#2024-11-02](https://towardsdatascience.com/the-bias-variance-tradeoff-and-how-it-shapes-the-llms-of-today-40e2c355f8a2?source=collection_archive---------4-----------------------#2024-11-02)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-bias-variance-tradeoff-and-how-it-shapes-the-llms-of-today-40e2c355f8a2?source=collection_archive---------4-----------------------#2024-11-02](https://towardsdatascience.com/the-bias-variance-tradeoff-and-how-it-shapes-the-llms-of-today-40e2c355f8a2?source=collection_archive---------4-----------------------#2024-11-02)
- en: Is low inductive bias essential for building general-purpose AI?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 低归纳偏差对于构建通用人工智能至关重要吗？
- en: '[](https://medium.com/@zakharymg?source=post_page---byline--40e2c355f8a2--------------------------------)[![Michael
    Zakhary](../Images/8657f728dd52de4094b71635b1c17087.png)](https://medium.com/@zakharymg?source=post_page---byline--40e2c355f8a2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--40e2c355f8a2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--40e2c355f8a2--------------------------------)
    [Michael Zakhary](https://medium.com/@zakharymg?source=post_page---byline--40e2c355f8a2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@zakharymg?source=post_page---byline--40e2c355f8a2--------------------------------)[![Michael
    Zakhary](../Images/8657f728dd52de4094b71635b1c17087.png)](https://medium.com/@zakharymg?source=post_page---byline--40e2c355f8a2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--40e2c355f8a2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--40e2c355f8a2--------------------------------)
    [Michael Zakhary](https://medium.com/@zakharymg?source=post_page---byline--40e2c355f8a2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--40e2c355f8a2--------------------------------)
    ·6 min read·Nov 2, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--40e2c355f8a2--------------------------------)
    ·阅读时长 6 分钟·2024年11月2日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/eb32a758ce0d5d1409030ca8d576fe66.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb32a758ce0d5d1409030ca8d576fe66.png)'
- en: Photo by [BoliviaInteligente](https://unsplash.com/@boliviainteligente?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [BoliviaInteligente](https://unsplash.com/@boliviainteligente?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In today’s ML space, we find ourselves surrounded by these massive transformer
    models like **chatGPT** and **BERT** that give us unbeatable performance on just
    about any downstream task, with the caveat being the requirement of huge amounts
    of pre-training on upstream tasks first. What makes transformers need so many
    parameters, and hence, so much training data to make them work?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在今天的机器学习领域，我们发现自己被这些巨大的变换器模型包围，如**chatGPT**和**BERT**，它们在几乎所有下游任务中都表现出无与伦比的性能，但前提是需要先在上游任务上进行大量的预训练。那么，是什么让变换器需要如此多的参数，因此需要大量的训练数据才能发挥作用呢？
- en: This is the question I wanted to delve into by exploring the connection between
    LLMs and the cornerstone topic of bias and variance in data-science. This show
    be fun!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我想要深入探讨的问题，通过探索 LLM 和数据科学中偏差与方差这一基石主题之间的联系。应该会很有趣！
- en: Background
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: Firstly, we need to go back down to memory lane and define some ground work
    for what is to come.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要回顾一下，定义一些基础知识，为接下来的内容做铺垫。
- en: '**Variance**'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**方差**'
- en: Variance is almost synonymous with overfitting in data science. The core linguistic
    choice for the term is the concept of **variation.** A high variance model is
    a model whose predicted value for the target variable **Y *varies*** greatly when
    small changes in the input variabl***e* X**occur.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 方差几乎可以与数据科学中的过拟合同义。这个术语的核心语言选择是“**变化**”的概念。高方差模型是指当输入变量 X 发生微小变化时，目标变量 **Y *变化***
    非常大的模型。
- en: So in high-variance models, a small change in X, causes a huge response in Y
    (that’s why Y is usually called a response variable). In the classical…
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在高方差模型中，X 的微小变化会导致 Y 的巨大响应（这就是为什么 Y 通常被称为响应变量）。在经典的...
