- en: The Business Guide to Tailoring Language AI Part 2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定制语言AI的商业指南 第二部分
- en: 原文：[https://towardsdatascience.com/the-business-guide-to-tailoring-language-ai-part-2-989a5e987f0c?source=collection_archive---------12-----------------------#2024-04-19](https://towardsdatascience.com/the-business-guide-to-tailoring-language-ai-part-2-989a5e987f0c?source=collection_archive---------12-----------------------#2024-04-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-business-guide-to-tailoring-language-ai-part-2-989a5e987f0c?source=collection_archive---------12-----------------------#2024-04-19](https://towardsdatascience.com/the-business-guide-to-tailoring-language-ai-part-2-989a5e987f0c?source=collection_archive---------12-----------------------#2024-04-19)
- en: Prompting ChatGPT and other chat-based language AI — and why you should (not)
    care about it
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向ChatGPT和其他基于聊天的语言AI发出提示——以及为什么您应该（不）关心它
- en: '[](https://medium.com/@georg.ruile?source=post_page---byline--989a5e987f0c--------------------------------)[![Georg
    Ruile, Ph.D.](../Images/83b04db23852ba2df2818fe62250ca22.png)](https://medium.com/@georg.ruile?source=post_page---byline--989a5e987f0c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--989a5e987f0c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--989a5e987f0c--------------------------------)
    [Georg Ruile, Ph.D.](https://medium.com/@georg.ruile?source=post_page---byline--989a5e987f0c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@georg.ruile?source=post_page---byline--989a5e987f0c--------------------------------)[![Georg
    Ruile博士](../Images/83b04db23852ba2df2818fe62250ca22.png)](https://medium.com/@georg.ruile?source=post_page---byline--989a5e987f0c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--989a5e987f0c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--989a5e987f0c--------------------------------)
    [Georg Ruile博士](https://medium.com/@georg.ruile?source=post_page---byline--989a5e987f0c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--989a5e987f0c--------------------------------)
    ·12 min read·Apr 19, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--989a5e987f0c--------------------------------)
    ·12分钟阅读·2024年4月19日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d38c05848cc0a25ec1ad62765d9fe3e1.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d38c05848cc0a25ec1ad62765d9fe3e1.png)'
- en: Foreword
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前言
- en: This article sheds some light on the question of how to “talk” to Large Language
    Models (LLM) that are designed to interact in conversational ways, like ChatGPT,
    Claude and others, so that the answers you get from them are as useful as possible
    for the task at hand. This particular communication from human to the language
    chatbot is what’s typically referred to as prompting. With this article, I mean
    to give people with no computer science background a compact overview of the topic
    so that everyone can understand. It can also help businesses to contextualize
    of what (not) to expect from their LLM adaption endeavors.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文阐明了如何“与”旨在进行对话交互的超大语言模型（LLM）进行沟通，如ChatGPT、Claude等，从而确保您从中获得的答案对于当前任务尽可能有用。人类与语言聊天机器人之间的这种沟通通常被称为提示。在本文中，我旨在为没有计算机科学背景的人提供关于该主题的简明概述，以便每个人都能理解。它也可以帮助企业理解在定制LLM过程中应当期待什么（或不应期待什么）。
- en: Prompting is the first of four steps you can take when climbing the ladder of
    adapting language models for your businesses’ custom use. I have introduced the
    overall **4 Step Framework** of unlocking custom LLM in [my previous post](https://medium.com/towards-data-science/the-business-guide-to-tailoring-language-ai-5f0fa806e838).
    If you haven’t already it might be helpful to read it first so that you can fit
    the ideas presented here into a larger context.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 提示是您在为企业定制语言模型时可以采取的四个步骤中的第一个。我在[上一篇文章](https://medium.com/towards-data-science/the-business-guide-to-tailoring-language-ai-5f0fa806e838)中介绍了**四步框架**，旨在解锁定制LLM。如果您还没有阅读过，先读一下可能会对您有所帮助，这样您就能将本文中的思想放入更大的背景中。
- en: '[](/the-business-guide-to-tailoring-language-ai-5f0fa806e838?source=post_page-----989a5e987f0c--------------------------------)
    [## The Business Guide to Tailoring Language AI'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/the-business-guide-to-tailoring-language-ai-5f0fa806e838?source=post_page-----989a5e987f0c--------------------------------)
    [## 定制语言AI的商业指南'
- en: A Framework for Unlocking Custom LLM Solutions You’ll Understand
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解锁您将理解的定制LLM解决方案框架
- en: towardsdatascience.com](/the-business-guide-to-tailoring-language-ai-5f0fa806e838?source=post_page-----989a5e987f0c--------------------------------)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/the-business-guide-to-tailoring-language-ai-5f0fa806e838?source=post_page-----989a5e987f0c--------------------------------)
- en: Introduction
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: 'Shortly after the mass market introduction of ChatGPT, a new, hot profession
    has entered the AI scene: **Prompt engineering**. These AI “whisperers”, i.e.
    people that have certain skills to “prompt”, that is, talk to language AI so that
    it responds in useful ways, have become highly sought-after ([and generously paid](https://www.forbes.com/sites/jackkelly/2024/03/06/the-hot-new-high-paying-career-is-an-ai-prompt-engineer/))
    new roles. Considering that a main building block of proper prompting is simply
    (or not so simply) giving precise instructions (see below), I must confess that
    I was surprised by this development (regardless of the fact that prompt *engineering*
    certainly involves more than just “whispering”): Isn’t communicating in a precise
    and concise manner a basic professional skill that we all should possess? But
    then again, I was reflecting on how important it is to have well-crafted requirements
    in software development, and “requirement engineering” roles have been an important
    ingredient of successful software development projects for a while now.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在ChatGPT广泛推出后不久，一个新的热门职业进入了AI领域：**提示工程**。这些AI“耳语者”，即那些具有特定“提示”技能的人，能够与语言AI对话，让它以有用的方式回应，已成为炙手可热的职位（[并且薪水丰厚](https://www.forbes.com/sites/jackkelly/2024/03/06/the-hot-new-high-paying-career-is-an-ai-prompt-engineer/)）。考虑到正确提示的一个主要构建块仅仅是（或不那么简单地）提供准确的指令（见下文），我必须承认这一发展让我感到惊讶（尽管提示*工程*无疑不仅仅是“耳语”）：精确简洁地沟通不是我们每个人都应具备的基本职业技能吗？但我又反思到，在软件开发中拥有精心设计的需求是多么重要，而“需求工程”角色已经成为成功软件开发项目的重要组成部分。
- en: I observe a level of uncertainty and “best guessing” and even contradictions
    in the topic of LLM and prompting that I have not yet experienced in any IT-related
    subject. This has to do with the type and size of AI models and their **stochastic
    characteristics**, which is beyond the scope of this article. Considering the
    [1.76 trillion parameters](https://en.wikipedia.org/wiki/GPT-4) of models like
    GPT-4, the number of possible combinations and paths from input (your “prompt”)
    to output (the model response) is virtually indefinite and non-deterministic.
    Hence, applications treat these models mainly as black boxes, and related research
    focuses on empirical approaches such as benchmarking their performance.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM和提示的主题中，我观察到一种不确定性和“最佳猜测”，甚至是矛盾，这在我以往接触的任何IT相关主题中都没有经历过。这与AI模型的类型和规模以及它们的**随机特性**有关，而这些超出了本文的讨论范围。考虑到像GPT-4这样模型的[1.76万亿参数](https://en.wikipedia.org/wiki/GPT-4)，从输入（你的“提示”）到输出（模型回应）的可能组合和路径数量几乎是无限且非确定性的。因此，应用程序主要将这些模型视为黑箱，而相关研究则侧重于经验方法，比如基准测试它们的性能。
- en: 'The sad news is that I cannot present you a perfect one-size-fits-all prompting
    solution that will forever solve your LLM requirements. Add to this that **different
    models behave differently**, and you may understand my dilemma. There’s some good
    news, though: On the one hand, you can, and should, always consider some **basic
    principles and concepts** that will help you optimize your interactions with the
    machines. Well-crafted prompts gets you farther than poor ones, and this is why
    it is well worthwhile to dig a bit deeper into the topic. On the other hand, **it
    may not even be necessary to worry too much about prompting at all**, which saves
    you valuable computing time (literally, CPU/GPU and figuratively, in your own
    brain).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我不能提供一个完美的、适用于所有情况的提示解决方案，来永远解决你的LLM需求。再者，**不同的模型行为不同**，你可能会理解我的困境。不过，也有一些好消息：一方面，你可以且应该始终考虑一些**基本原则和概念**，这些会帮助你优化与机器的互动。精心设计的提示能比糟糕的提示更进一步，这也是为什么深入探讨这个话题是非常值得的。另一方面，**甚至可能根本不需要过多担心提示问题**，这将节省你宝贵的计算时间（字面意义上的CPU/GPU时间，以及比喻意义上你自己大脑的时间）。
- en: Start with Why
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从“为什么”开始
- en: Here I am not referring to Simon Sinek’s classic TEDx business advice. Instead,
    I encourage you to curiously wonder **why technology does what it does**. I strongly
    believe in the notion that if you understand at least a bit of the inner workings
    of software, it will tremendously help you in its application.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我并不是指Simon Sinek的经典TEDx商业建议。相反，我鼓励你去好奇地思考**技术为什么会这样做**。我坚信，如果你至少理解一点软件的内在工作原理，它将极大地帮助你在应用中使用它。
- en: So how, in principle, is the input (the prompt) related to the output (the response),
    and why is it that proper prompts result in better suited responses? To figure
    this out, we need to have at least a superficial look at the model architecture
    and its training and fine-tuning, without needing to understand the details of
    the impressive underlying concepts like the infamous Transformer Architecture
    and Attention Mechanisms which ultimately caused the breakthrough of ChatGPT-like
    Generative AI as we know it today.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，从原则上来说，输入（提示）是如何与输出（响应）相关的？为什么正确的提示会导致更合适的响应？要弄清楚这一点，我们至少需要对模型架构及其训练和微调有一个粗略的了解，而不需要理解像臭名昭著的
    Transformer 架构和注意力机制等深奥的概念，这些概念最终促成了我们今天所知的 ChatGPT 类生成 AI 的突破。
- en: 'For our purposes, we can look at it from two angles:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的角度来看，可以从两个方面进行考虑：
- en: '*How does the model* ***retrieve knowledge and generate its response?***and
    closely related'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*模型是如何* ***检索知识并生成响应的？***与此密切相关'
- en: '*How has the model been* ***trained and fine-tuned?***'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*模型是如何被* ***训练和微调的？***'
- en: It is important to understand that an LLM is in essence a **Deep Neural Network**
    and as such, it works based on **statistics and probabilities**. Put very simplistically,
    the model generates output which reflects the closest match to the context, based
    on its knowledge it has learned from vast amounts of training data. One of the
    building blocks here are so-called **Embeddings**, where similar word meanings
    are (mathematically) close to each other, even though the model does not actually
    “understand” those meanings. If this sounds fancy, it kinda is, but at same time,
    it is “only” mathematics, so don’t be afraid.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要理解，LLM 本质上是一个**深度神经网络**，因此，它是基于**统计和概率**来工作的。简单来说，模型生成的输出反映了与上下文最接近的匹配，基于它从大量训练数据中学习到的知识。这里的一个构建块是所谓的**嵌入**，其中相似的词义（在数学上）彼此接近，尽管模型实际上并不“理解”这些词义。如果这听起来很花哨，它确实有点复杂，但同时，它“仅仅”是数学而已，所以不要害怕。
- en: '![](../Images/bdb1b66d57538d9d96891eb1aea06b47.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bdb1b66d57538d9d96891eb1aea06b47.png)'
- en: A simple illustration of word vector embeddings — similar word “meanings” are
    close to each other
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的词向量嵌入示例——相似的词“含义”彼此接近
- en: When looking at training, it helps considering the training data and process
    a language model has gone through. Not only has the model seen vast amounts of
    text data. It has also learned what makes out a high rated response to a specific
    question, for instance on sites like StackOverflow, and on high-quality Q&A assistant
    documents written for model training and tuning. In addition, in its fine-tuning
    stage, it learned and iteratively adapted its optimal responses **based on human
    feedback**. Without all this intense training and tuning efforts, the model might
    answer a question like “what is your first name” simply with “what is your last
    name”, because it has seen this frequently on internet forms [1].
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看训练过程时，考虑语言模型所经过的训练数据和过程是有帮助的。模型不仅看过大量的文本数据，它还学会了什么构成了对特定问题的高质量回答，例如在像 StackOverflow
    这样的站点上，或者在为模型训练和微调编写的高质量 Q&A 助手文档中。此外，在微调阶段，它还基于**人类反馈**学习并迭代地调整其最佳响应。如果没有这些强大的训练和微调工作，模型可能会像回答“你叫什么名字”这样的问题时，直接回答“你姓什么”，因为它在互联网上的表单中经常看到这种情况[1]。
- en: 'Where I am trying to get at is this: When interacting with natural language
    AI, always keep in mind what and how the model has learned and how it gets to
    its output, given your input. Even though no one really knows this exactly, it
    is useful to consider probable correlations: Where and in what context could the
    model have seen input similar to yours before? What data has been available during
    the pre-training stage, and in which quality and quantity? For instance: Ever
    wondered why LLMs can solve mathematical equations (not reliably, however, sometimes
    still surprisingly), without inherent calculation capabilities? LLMs don’t calculate,
    they match patterns!'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我想表达的是：与自然语言 AI 交互时，始终要记住模型是如何学习的，以及它如何根据你的输入生成输出。尽管没有人能确切知道这一点，但考虑可能的相关性是很有用的：模型之前在哪些地方和什么上下文中可能见过与你相似的输入？在预训练阶段，模型有哪些数据可用，且数据的质量和数量如何？举个例子：有没有想过，为什么大型语言模型（LLM）能够解决数学方程（尽管不总是可靠，但有时仍然令人惊讶），而没有内建的计算能力？LLM
    不进行计算，它们匹配模式！
- en: Prompting 101
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示入门 101
- en: There is a plethora of Prompting techniques, and plenty of scientific literature
    that benchmarks their effectiveness. Here, I just want to introduce a few well-known
    concepts. I believe that once you get the general idea, you will be able to expand
    your prompting repertoire and even develop and test new techniques yourself.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多提示技巧，以及大量的科学文献对其效果进行了基准测试。在这里，我只是想介绍一些知名的概念。我相信，一旦你理解了这些基本概念，你就能扩展自己的提示技巧库，甚至自己开发和测试新的技巧。
- en: Ask and it will be given to you
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提问，它会给你答案
- en: 'Before going into specific prompting concepts, I would like to stress a general
    idea that, in my opinion, cannot be stressed enough:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入具体的提示概念之前，我想强调一个我认为无法过度强调的普遍观点：
- en: '***The quality of your prompt highly determines the response of the model.***'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '***你的提示质量决定了模型的响应。***'
- en: And by quality I don’t necessarily mean a sophisticated prompt construction.
    I mean the basic idea of asking a precise question or giving well-structured instructions
    and providing necessary context. I have touched on this already when we met Sam,
    the piano player, in [my previous article](https://medium.com/towards-data-science/the-business-guide-to-tailoring-language-ai-5f0fa806e838).
    If you ask a bar piano player to play some random Jazz tune, chances are that
    he might not play what you had in mind. Instead, if you ask exactly what it is
    you want to hear, your satisfaction with the result is likely to increase.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这里说的质量，我不一定指复杂的提示构建。我指的是提出精确问题或给出结构良好的指令并提供必要背景的基本思路。我在[上一篇文章](https://medium.com/towards-data-science/the-business-guide-to-tailoring-language-ai-5f0fa806e838)中提到过这个概念。当我们遇到萨姆——钢琴演奏者时，你会发现如果你让酒吧钢琴师演奏一首随机的爵士乐，他可能不会演奏你心中的那首曲子。而如果你准确地告诉他你想听的是什么，那么你对结果的满意度可能会更高。
- en: Similarly, if you ever had the chance of, say, hire someone to do something
    around your house and your contract specification only says, say, “bathroom renovation”,
    you might be surprised that in the end your bathroom does not look like what you
    had in mind. The contractor, just like the model, will only refer to what he has
    learned about renovations and bathroom tastes and will take the learned route
    to deliver.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，如果你曾经有机会，假设雇人做些家务，而你的合同说明仅仅写着“浴室翻新”，你可能会感到惊讶，最后的浴室根本没有达到你预期的效果。承包商，就像模型一样，只会参考他学到的翻新和浴室装修的知识，并会按照这些经验来完成工作。
- en: 'So here are some **general guidelines for prompting**:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，以下是一些**提示的通用指导原则**：
- en: · Be clear and specific.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: · 要清晰具体。
- en: · Be complete.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: · 要完整。
- en: · Provide context.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: · 提供背景信息。
- en: · Specify the desired output style, length, etc.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: · 指定期望的输出风格、长度等。
- en: This way, the model has sufficient and matching reference data in your prompt
    that it can relate to when generating its response.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，模型在生成响应时就能根据你的提示，拥有足够且匹配的参考数据。
- en: Roleplay prompting — simple, but overrated
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 角色扮演提示——简单，但被高估了
- en: 'In the early days of ChatGPT, the idea of roleplay prompting was all around:
    Instead of asking the assistant to give you an immediate answer (i.e. a simple
    query), you first assign it a specific role, such as “teacher” or “consultant”
    etc. Such a prompt could look like [2]:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ChatGPT 的早期，角色扮演提示的想法随处可见：与其要求助手立即给出答案（即简单查询），你首先给它指定一个特定角色，如“教师”或“顾问”等。这样的提示可能看起来像这样[2]：
- en: '***From now on, you are an excellent math teacher and always teach your students
    math problems correctly. And I am one of your students.***'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '***从现在起，你是一位优秀的数学老师，总是正确地教授你的学生数学问题。我是你的学生之一。***'
- en: It has been shown that this concept yields superior results. One [paper](https://arxiv.org/abs/2308.07702)
    reports that by this role play, the model implicitly triggers a step by step reasoning
    process, which is what you want it to do when applying the CoT- technique, see
    below. However, this approach has also been shown to **sometimes perform sub-optimal**
    and needs to be well designed.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 已经证明，这一概念能够带来更优的结果。一个[研究论文](https://arxiv.org/abs/2308.07702)报告指出，通过这种角色扮演，模型会隐式地触发逐步推理过程，这正是你希望它在应用CoT技巧时所做的，如下所示。然而，这种方法也已被证明**有时会表现不尽如人意**，因此需要精心设计。
- en: 'In my experience, simply assigning a role doesn’t do the trick. I have experimented
    with the example task from the paper referred to above. Unlike in this research,
    GPT3.5 (which is as of today the free version of OpenAI’s ChatGPT, so you can
    try it yourself) has given the correct result, using a simple query:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我的经验，单纯分配一个角色并不能奏效。我曾尝试过上面提到的论文中的示例任务。与这项研究不同，GPT3.5（截至今天是OpenAI的ChatGPT的免费版本，你可以自己尝试）通过简单查询得出了正确的结果：
- en: '![](../Images/89a418b3d6837d5d59787565537487e2.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89a418b3d6837d5d59787565537487e2.png)'
- en: An example using a simple query instead of the roleplay prompt suggested by
    [2], still yielding the correct response
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一个使用简单查询的示例，而不是[2]建议的角色扮演提示，依然得出了正确的回答。
- en: 'I have also experimented with different logical challenges with both simple
    queries and roleplay, using a similar prompt like the one above. In my experiments
    two things happen:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我还尝试过用简单查询和角色扮演进行不同的逻辑挑战，使用了类似上面的提示。在我的实验中，发生了两种情况：
- en: '*either* ***simple queries provides the correct answer on the first attempt****,
    or*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*要么* ***简单查询在第一次尝试时给出了正确答案***，*要么*'
- en: '*both* ***simple queries and roleplay come up with false, however different
    answers***'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*无论是* ***简单查询还是角色扮演都得出了错误的，但不同的答案***'
- en: '**Roleplay did not outperform** the queries in any of my simple (not scientifically
    sound) experiments. Hence, I conclude that the models must have improved recently
    and the **impact of roleplay prompting is diminishing**.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**角色扮演在我的任何简单（非科学严谨）实验中并未超越查询**。因此，我得出结论，模型最近一定有所改进，而**角色扮演提示的影响正在减弱**。'
- en: Looking at different research, and without extensive further own experimenting,
    I believe that in order to outperform simple queries, roleplay prompts need to
    be embedded into a **sound and thoughtful design** to outperform the most basic
    approaches — or are not valuable at all.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看不同的研究，在没有进行广泛的进一步实验的情况下，我认为为了超越简单查询，角色扮演提示需要嵌入到**合理且深思熟虑的设计**中，以超越最基本的方法——否则完全没有价值。
- en: I am happy to read your experiences on this in the comments below.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我很高兴在下面的评论中看到你们在这方面的经验。
- en: Few-Shot aka in-context learning
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 少量样本（Few-Shot）即上下文学习
- en: 'Another intuitive and relatively simple concept is what’s referred to as Few-Shot
    prompting, also referred to as in-context learning. Unlike in a Zero-Shot Prompt,
    we not only ask the model to perform a task and expect it to deliver, we additionally
    **provide (“few”) examples of the solutions**. Even though you may find this obvious
    that providing examples leads to better performance, this is quite a remarkable
    ability: These LLMs are able to in-context learn, i.e. perform new tasks via inference
    alone by conditioning on a few input-label pairs and making predictions for new
    inputs [3].'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个直观且相对简单的概念是所谓的少量样本提示，也称为上下文学习。与零样本提示不同，我们不仅要求模型执行某项任务并期望其给出结果，还额外**提供（“少量”）解决方案的示例**。尽管你可能认为提供示例会带来更好的表现，但这实际上是一项非常显著的能力：这些大语言模型能够进行上下文学习，即通过仅凭推理，在少量输入-标签对的条件下执行新任务，并为新输入做出预测[3]。
- en: Setting up a few-shot prompt involves
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 设置少量样本提示包括：
- en: '*(1)* ***collecting examples of the desired responses****, and'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*(1)* ***收集所需回答的示例***，并且'
- en: (2) writing your prompt with* ***instructions on what to do with these examples****.*
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 编写你的提示，并* ***指示如何处理这些示例***。
- en: 'Let’s look at a typical classification example. Here the model is given several
    examples of statements that are either positive, neutral or negative judgements.
    The model’s task is to rate the final statement:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个典型的分类示例。这里，模型被给出几个判断为正面、负面或中立的陈述。模型的任务是对最终的陈述进行评分：
- en: '![](../Images/964b159019f7b64cc3c4b8f5ea1fa594.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/964b159019f7b64cc3c4b8f5ea1fa594.png)'
- en: A typical classification example of a Few-Shot prompt. The model is required
    to classify statements into the given categories (positive / negative)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的少量样本提示的分类示例。模型需要将陈述分类为给定的类别（正面/负面）。
- en: Again, even though this is a simple and intuitive approach, I am sceptical about
    its value in state-of-the-art language models. In my (again, not scientifically
    sound) experiments, **Few-Shot prompts have not outperformed Zero-Shot in any
    case**. (The model knew already that a drummer who doesn’t keep the time, is a
    negative experience, without me teaching it…). My finding seems to be consistent
    with recent research, where even the opposite effect (**Zero-Shot outperforming
    Few-Shot**) has been shown [4].
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，尽管这是一种简单直观的方法，我对它在最先进的语言模型中的价值持怀疑态度。在我的（再次，非科学严谨的）实验中，**少样本（Few-Shot）提示在任何情况下都没有优于零样本（Zero-Shot）**。（模型已经知道，不守时的鼓手是一种糟糕的体验，而我并没有教它这个……）。我的发现似乎与最近的研究一致，甚至有研究显示相反的效果（**零样本优于少样本**）[4]。
- en: In my opinion and on this empirical background it is worth considering if the
    cost of design as well as computational, API and latency cost of this approach
    are a worthwhile investment.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在我看来，根据这一经验背景，值得考虑的是，这种方法的设计成本、计算成本、API成本和延迟成本是否值得投资。
- en: CoT-Prompting or “Let’s think step-by-step’’
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CoT提示或“让我们一步步思考”
- en: Chain of Thought (CoT) Prompting aims to make our models better at solving complex,
    multi-step reasoning problems. It can be as simple as adding the CoT instruction
    “Let’s think step-by-step’’ to the input query, to improve accuracy significantly
    [5][6].
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 思维链（CoT）提示旨在让我们的模型更擅长解决复杂的多步骤推理问题。它可以像在输入查询中添加CoT指令“让我们一步步思考”一样简单，从而显著提高准确性[5][6]。
- en: Instead of just providing the final query or add one or few examples within
    your prompt like in the Few-Shot approach, you prompt the model to **break down
    its reasoning process into a series of intermediate steps**. This is akin to how
    a human would (ideally) approach a challenging problem.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 与其像少样本（Few-Shot）方法那样仅提供最终查询或在提示中添加一个或几个示例，不如提示模型**将其推理过程分解成一系列中间步骤**。这类似于人类如何（理想情况下）解决一个具有挑战性的问题。
- en: Remember your math exams in school? Often, at more advanced classes, you were
    asked to not only solve a mathematical equation, but also write down the logical
    steps how you arrived at the final solution. And even if it was incorrect, you
    might have gotten some credits for mathematically sound solution steps. Just like
    your teacher in school, you expect the model to break the task down into sub-tasks,
    perform intermediate reasoning, and arrive at the final answer.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 记得你在学校的数学考试吗？通常，在更高级的课程中，你不仅被要求解答一个数学方程式，还要写下你是如何推导出最终解答的逻辑步骤。即使答案不对，你可能也会因为数学上合理的解题步骤而获得一些分数。就像你在学校的老师一样，你期望模型将任务分解成子任务，进行中间推理，并得出最终答案。
- en: Again, I have experimented with CoT myself quite a bit. And again, most of the
    time, simply adding **“Let’s think step-by-step” didn’t improve the quality of
    the response**. In fact, it seems that the **CoT approach has become an implicit
    standard** of the recent fine-tuned chat-based LLM like ChatGPT, and the response
    is frequently broken down into chunks of reasoning without the explicit command
    to do so.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我自己也进行过不少CoT实验。并且，大多数时候，仅仅添加**“让我们一步步思考”并没有改善回答的质量**。事实上，似乎**CoT方法已经成为最近经过微调的基于聊天的语言模型（如ChatGPT）的隐式标准**，即使没有明确的指令，回应也经常被分解为推理的块。
- en: 'However, I came across one instance where the explicit CoT **command did in
    fact improve the answer significantly**. I used a CoT example from [this article](https://medium.com/@thomasczerny/chain-of-thought-cot-prompting-9ee4967e927c),
    however, altered it into a trick question. Here you can see how ChatGPT fell into
    my trap, when not explicitly asked for a CoT approach (even though the response
    shows a step wise reasoning):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我遇到过一个例子，在这个例子中，**明确的CoT指令确实显著改善了答案**。我使用了来自[这篇文章](https://medium.com/@thomasczerny/chain-of-thought-cot-prompting-9ee4967e927c)的CoT示例，但将其改编成了一个trick
    question。在这里，你可以看到ChatGPT是如何陷入我的陷阱的，当它没有明确要求使用CoT方法时（尽管回应中展示了逐步推理）：
- en: '![](../Images/d66d445d5c947e3d7919d1c8553ca5da.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d66d445d5c947e3d7919d1c8553ca5da.png)'
- en: A trick question with a simple query instead of a CoT prompt. Even though the
    response is broken down “step by step”, it is not quite correct.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个通过简单查询而不是CoT提示提出的 trick question。尽管回答被“逐步”分解，但它并不完全正确。
- en: 'When I added “Let’s think step-by-step” to the same prompt, it solved the trick
    question correctly (well, it is unsolvable, which ChatGPT rightfully pointed out):'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 当我在相同的提示中添加“让我们一步步思考”时，它正确地解答了这个trick question（嗯，它实际上是无法解决的，ChatGPT也正确指出了这一点）：
- en: '![](../Images/286f905eaa9ca7c70dc194b2da1c2fd3.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/286f905eaa9ca7c70dc194b2da1c2fd3.png)'
- en: The same trick question with an explicit CoT prompt, delivering a correct response
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用明确的思维链提示，得到正确回答的同一个陷阱问题
- en: To summarize, Chain of Thought prompting aims at building up reasoning skills
    that are otherwise difficult for language models to acquire implicitly. It encourages
    models to articulate and refine their reasoning process rather than attempting
    to jump directly from question to answer.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，思维链提示（Chain of Thought prompting）旨在建立推理能力，而这些能力对于语言模型来说是很难隐性地获取的。它鼓励模型阐述并完善其推理过程，而不是试图直接从问题跳跃到答案。
- en: Again, my experiments have revealed **only limited benefits of the simple CoT
    approach** (adding “Let’s think step-by-step“). **CoT did outperform a simple
    query on one occasion**, and at the same time the extra effort of adding the CoT
    command is minimal. This **cost-benefit ratio** is one of the reasons why this
    approach is one of my favorites. Another reason why I personally like this approach
    is, it not only helps the model, but also can **help us humans to reflect** and
    maybe even iteratively consider necessary reasoning steps while crafting the prompt.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我的实验结果揭示了**简单的思维链方法的效益有限**（添加“让我们一步步思考”）。**思维链在某些情况下确实比简单查询表现更好**，同时添加思维链指令的额外努力也很少。这种**成本效益比**是我喜欢这种方法的原因之一。另一个原因是，个人而言，我喜欢这种方法，它不仅能帮助模型，还能**帮助我们人类进行反思**，甚至在构建提示时，逐步考虑必要的推理步骤。
- en: As before, we will likely see diminishing benefits of this simple CoT approach
    when models become more and more fine-tuned and accustomed to this reasoning process.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，当模型变得越来越精细调整并习惯于这种推理过程时，这种简单的思维链方法的效果可能会逐渐减弱。
- en: Conclusion
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we have taken a journey into the world of prompting chat-based
    Large Language Models. Rather than just giving you the most popular prompting
    techniques, I have encouraged you to begin the journey with the question of Why
    prompting matters at all. During this journey we have discovered that the importance
    of prompting is diminishing thanks to the evolution of the models. Instead of
    requiring users to invest in continuously improving their prompting skills, currently
    evolving model architectures will likely further reduce their relevance. An **agent-based
    framework**, where different “routes” are taken while processing specific queries
    and tasks, is one of those.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们深入探讨了基于聊天的大型语言模型的提示方法。与其仅仅向你介绍最流行的提示技术，我更鼓励你从“为什么提示如此重要”的问题开始这段旅程。在这段旅程中，我们发现，随着模型的演变，提示的重要性正在逐渐减弱。目前不断发展的模型架构可能会进一步减少提示技巧的相关性。**基于代理的框架**就是其中之一，它通过在处理特定查询和任务时采取不同的“路线”来实现。
- en: This does not mean, however, that **being clear and specific and providing the
    necessary context within your prompts** isn’t worth the effort. On the contrary,
    I am a strong advocate of this, since it not only helps the model but also yourself
    to figure out what exactly it is you’re trying to achieve.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不意味着**在提示中清晰、具体并提供必要的上下文**就不值得付出努力。相反，我是这方面的坚定倡导者，因为这不仅能帮助模型，还能帮助你自己弄清楚你究竟想要达到什么目的。
- en: Just like in human communication, multiple factors determine the appropriate
    approach for reaching a desired outcome. Often, it is a mix and iteration of different
    approaches that yield optimal results for the given context. Try, test, iterate!
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在人类交流中一样，多个因素决定了达成预期结果的合适方法。通常，结合并反复尝试不同的方法，能在特定情境下获得最佳效果。尝试、测试、迭代！
- en: And finally, unlike in human interactions, you can test virtually limitlessly
    into your personal trial-and-error prompting journey. Enjoy the ride!
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，与人类交互不同，你几乎可以无限次地在个人的试验和错误提示旅程中进行测试。享受这段旅程吧！
- en: '*Notes and references'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*笔记与参考资料'
- en: All drawings are hand crafted with pride by the author :)*
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 所有插图均由作者亲手精心绘制 :)*
- en: '[1]: How Large Language Models work: From zero to ChatGPT'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]: 大型语言模型如何工作：从零到ChatGPT'
- en: '[https://medium.com/data-science-at-microsoft/how-large-language-models-work-91c362f5b78f](https://medium.com/data-science-at-microsoft/how-large-language-models-work-91c362f5b78f)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://medium.com/data-science-at-microsoft/how-large-language-models-work-91c362f5b78f](https://medium.com/data-science-at-microsoft/how-large-language-models-work-91c362f5b78f)'
- en: '[2]: Better Zero-Shot Reasoning with Role-Play Prompting'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[2]: 通过角色扮演提示实现更好的零样本推理'
- en: '[https://arxiv.org/abs/2308.07702](https://arxiv.org/abs/2308.07702)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/abs/2308.07702](https://arxiv.org/abs/2308.07702)'
- en: '[3]: Rethinking the Role of Demonstrations: What Makes In-Context Learning
    Work?'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[3]: 重新思考演示的角色：是什么让上下文学习有效？'
- en: '[https://arxiv.org/abs/2202.12837](https://arxiv.org/abs/2202.12837)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/abs/2202.12837](https://arxiv.org/abs/2202.12837)'
- en: '[4]: Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[4]: 大型语言模型的提示编程：超越少样本范式'
- en: '[https://dl.acm.org/doi/abs/10.1145/3411763.3451760](https://dl.acm.org/doi/abs/10.1145/3411763.3451760).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://dl.acm.org/doi/abs/10.1145/3411763.3451760](https://dl.acm.org/doi/abs/10.1145/3411763.3451760)。'
- en: '[5]: When do you need Chain-of-Thought Prompting for ChatGPT?'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[5]: 什么时候需要为 ChatGPT 提供链式思维提示？'
- en: '[https://arxiv.org/abs/2304.03262](https://arxiv.org/abs/2304.03262)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/abs/2304.03262](https://arxiv.org/abs/2304.03262)'
- en: '[6]: Large Language Models are Zero-Shot Reasoners'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[6]: 大型语言模型是零-shot 推理者'
- en: '[https://arxiv.org/abs/2205.11916](https://arxiv.org/abs/2205.11916)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/abs/2205.11916](https://arxiv.org/abs/2205.11916)'
