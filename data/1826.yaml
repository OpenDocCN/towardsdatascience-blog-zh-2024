- en: 'Reinforcement Learning for Physical Dynamical Systems: An Alternative Approach'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物理动力学系统的强化学习：一种替代方法
- en: 原文：[https://towardsdatascience.com/rl-for-physical-dynamical-systems-an-alternative-approach-8e2269dc1e79?source=collection_archive---------1-----------------------#2024-07-28](https://towardsdatascience.com/rl-for-physical-dynamical-systems-an-alternative-approach-8e2269dc1e79?source=collection_archive---------1-----------------------#2024-07-28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/rl-for-physical-dynamical-systems-an-alternative-approach-8e2269dc1e79?source=collection_archive---------1-----------------------#2024-07-28](https://towardsdatascience.com/rl-for-physical-dynamical-systems-an-alternative-approach-8e2269dc1e79?source=collection_archive---------1-----------------------#2024-07-28)
- en: Reintroducing genetic algorithms and comparing to neural networks
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重新引入遗传算法并与神经网络进行比较
- en: '[](https://medium.com/@retter_42511?source=post_page---byline--8e2269dc1e79--------------------------------)[![Robert
    Etter](../Images/8193c0477b2a7df1177779ca5937854a.png)](https://medium.com/@retter_42511?source=post_page---byline--8e2269dc1e79--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8e2269dc1e79--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8e2269dc1e79--------------------------------)
    [Robert Etter](https://medium.com/@retter_42511?source=post_page---byline--8e2269dc1e79--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@retter_42511?source=post_page---byline--8e2269dc1e79--------------------------------)[![Robert
    Etter](../Images/8193c0477b2a7df1177779ca5937854a.png)](https://medium.com/@retter_42511?source=post_page---byline--8e2269dc1e79--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8e2269dc1e79--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8e2269dc1e79--------------------------------)
    [Robert Etter](https://medium.com/@retter_42511?source=post_page---byline--8e2269dc1e79--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8e2269dc1e79--------------------------------)
    ·14 min read·Jul 28, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8e2269dc1e79--------------------------------)
    ·14分钟阅读·2024年7月28日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/03893c77eceb2ecc28ad297ed423a40e.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03893c77eceb2ecc28ad297ed423a40e.png)'
- en: Photo by [Tra Nguyen](https://unsplash.com/@thutra0803?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [Tra Nguyen](https://unsplash.com/@thutra0803?utm_source=medium&utm_medium=referral)
    于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Physical and Nonlinear Dynamics
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物理与非线性动力学
- en: Control theory, through classical, robust, and optimal approaches, enable modern
    civilization. Refining, telecommunications, modern manufacturing and more depend
    on them. Control theory has been built on the insight provided by physics equations,
    such as derived from Newton’s Laws and Maxwell’s equations. These equations describe
    the dynamics, the interplay of different forces, on physical systems. Through
    them we understand how the equation moves between states, where a state is “the
    set of all information that sufficiently describes the system” [1], often in terms
    of variables such as pressure or velocity of fluid particles in fluid dynamics,
    or charge and current states in electrodynamics. By deriving equations for the
    systems, we can predict how the states change through time and space and express
    this evolution in terms of a differential equation. With this understanding, we
    can apply controls in the form of specially applied forces to maintain these systems
    at a desired state or output. Typically, this force is calculated based on the
    output of the system. Consider a vehicle cruise control. The input is the desired
    speed, the output the actual speed. The system is the engine. The state estimator
    observes the speed and determines what the difference between output and input
    speed is and how apply a control, such as adjusting fuel flow, to reduce the error.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 控制理论通过经典、鲁棒和最优方法支撑了现代文明。精炼、电信、现代制造等各个领域都依赖于它们。控制理论建立在物理方程提供的见解之上，例如牛顿定律和麦克斯韦方程。这些方程描述了物理系统中的动态和不同力的相互作用。通过它们，我们理解方程如何在不同状态之间转换，其中“状态是足够描述系统的所有信息的集合”[1]，通常是通过流体动力学中的压力或流体粒子的速度，或者在电动力学中的电荷和电流状态来表达。通过推导系统方程，我们可以预测状态如何随时间和空间变化，并通过微分方程表达这种演化。通过这种理解，我们可以采用控制手段，即通过特别施加的力，将这些系统维持在期望的状态或输出上。通常，这种力是根据系统的输出计算的。以车辆巡航控制为例，输入是期望的速度，输出是实际的速度。系统是发动机。状态估计器观察速度，并确定输出与输入速度之间的差异，然后应用控制手段（如调整燃油流量）来减少误差。
- en: However, for all its accomplishments, control theory encounters substantial
    limitations. Most control theory is built around linear systems, or systems where
    a proportional change in input leads to a proportional change in output. While
    these systems can be quite complex, we have extensive understanding of these systems,
    affording us practical control of everything from deep ocean submersibles and
    mining equipment to spacecraft.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管控制理论取得了诸多成就，它仍面临着显著的局限性。大多数控制理论是基于线性系统，或是输入的比例变化导致输出的比例变化的系统。虽然这些系统可能非常复杂，但我们对这些系统有着广泛的理解，使我们能够实际控制从深海潜水器、矿山设备到航天器的各种设备。
- en: 'However, as Stanislaw Ulam remarked, “using a term like nonlinear science is
    like referring to the bulk of zoology as the study of non-elephant animals.” Our
    progress so far in controlling complex physical systems has mostly come through
    finding ways to limit them to linear behavior. This can cost us efficiency in
    several ways:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如斯坦尼斯瓦夫·乌拉姆所言：“使用‘非线性科学’这样的术语，就像是把大部分动物学称为研究非大象动物。”到目前为止，我们在控制复杂物理系统方面的进展大多通过找到限制它们线性行为的方法。这可能会在多个方面带来效率损失：
- en: · Break down complex system into component parts that are individually controlled,
    optimizing for subsystems rather than the system as a whole
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: · 将复杂系统拆解为可单独控制的组件，优化子系统而非整个系统
- en: · Operate systems at simpler, but less efficient operating modes or not take
    advantage of complex physics, such as active flow control to reduce aircraft drag
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: · 在较简单但效率较低的操作模式下运行系统，或未能充分利用复杂的物理学原理，例如主动流动控制以减少飞机的阻力
- en: · Tight operating condition limits that can result in unpredictable or catastrophic
    failure if exceeded
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: · 严格的操作条件限制，一旦超过可能导致不可预测或灾难性的故障
- en: Advanced manufacturing, improved aerodynamics, and complex telecommunications
    would all benefit from a better approach to control of nonlinear systems.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 高级制造业、改进的空气动力学和复杂的电信系统都将受益于更好的非线性系统控制方法。
- en: 'The fundamental characteristic of nonlinear dynamical systems is their complex
    response to inputs. Nonlinear systems vary dramatically even with small changes
    in environment or state. Consider the Navier-Stokes equations that govern fluid
    flow: the same set of equations describes a placid, slow flowing stream as a raging
    torrent, and all the eddies and features of the raging torrent are contained within
    the equation dynamics.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性动力学系统的基本特征是它们对输入的复杂响应。即使在环境或状态发生小幅变化的情况下，非线性系统也会剧烈变化。以控制流体流动的纳维-斯托克斯方程为例：同一组方程既描述了一个平静、缓慢流动的小溪，也描述了一个汹涌的洪流，所有洪流中的漩涡和特征都包含在方程的动态变化之中。
- en: 'Nonlinear systems present difficulties: unlike linear systems we often don’t
    have an easily predictable idea of how the system will behave as it transitions
    from one state to the next. The best we can approach is through general analysis
    or extensive simulation. Hence, with nonlinear systems we are faced with two problems:
    system identification — that is, understanding how it will behave at a given state,
    and system control — how it will change in the short and long term in response
    to a given input and so what input to make to get the desired outcome.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性系统带来了困难：与线性系统不同，我们通常无法轻松预测系统在从一个状态过渡到下一个状态时的行为。我们所能做到的最好方法是通过一般分析或广泛的仿真。因此，在非线性系统中，我们面临两个问题：系统识别——即理解系统在给定状态下的行为，以及系统控制——即系统在响应给定输入时的短期和长期变化，以及如何选择输入以获得期望的结果。
- en: Reinforcement Learning for Physics
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习与物理学
- en: While nonlinear analysis and control continues to make progress, we remain limited
    in our ability to exploit these systems using traditional, equation-based methods.
    However, as computing power and sensor technology become more accessible, data-based
    approaches offer a different approach.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管非线性分析和控制不断取得进展，但我们在利用这些系统方面仍然受到传统基于方程方法的限制。然而，随着计算能力和传感器技术变得更加普及，基于数据的方法提供了一种不同的途径。
- en: The mass increase in data availability has given rise to machine learning (ML)
    approaches, and reinforcement learning (RL) provides a new approach to tackling
    the challenge of controlling nonlinear dynamical systems more effectively. RL,
    already finding success in environments from self-driving cars to strategy and
    computer games, is an ML framework which trains algorithms, or agents, “to learn
    how to make decisions under uncertainty to maximize a long-term benefit through
    trial and error” [1]. In other words, RL algorithms address the problems of system
    identification and control optimization and do this not by manipulation and analysis
    of governing equations, but by sampling the environment to develop a prediction
    of what input actions lead to desired outcomes. RL algorithms, or agents, apply
    a policy of actions based on the system state, and refine this policy as they
    analyze more information on the system.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可用性的剧增催生了机器学习（ML）方法，而强化学习（RL）提供了一种新的方法，以更有效地应对控制非线性动态系统的挑战。RL已经在从自动驾驶汽车到战略和计算机游戏的环境中取得了成功，它是一个机器学习框架，通过“试错”来训练算法或智能体，使其“学会如何在不确定性下做出决策，以最大化长期收益”[1]。换句话说，RL算法解决了系统识别和控制优化的问题，而不是通过操控和分析控制方程来实现，而是通过采样环境来预测哪些输入动作会导致期望的结果。RL算法或智能体根据系统状态应用一套行动策略，并随着对系统更多信息的分析而不断完善这一策略。
- en: Many RL algorithms are based on using neural networks to develop functions that
    map state to optimal behavior. RL problems can be framed as state-action-reward
    tuples. For a given state, a certain action leads to a given reward. Neural networks
    act as universal function approximators that can be tuned to accurately approximate
    the state-action-reward tuple function across an entire system. To do so it must
    acquire new knowledge by exploring the system or environment, and then refine
    its policy by exploiting the additional data gained. RL algorithms are differentiated
    by how they apply mathematics to explore, exploit, band balance between the two.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 许多强化学习（RL）算法基于使用神经网络来开发将状态映射到最佳行为的函数。RL问题可以被构建为状态-行动-奖励三元组。对于给定的状态，某个特定的行动会导致一个特定的奖励。神经网络作为通用函数逼近器，可以进行调整，以准确地逼近整个系统中的状态-行动-奖励三元组函数。为了做到这一点，神经网络必须通过探索系统或环境来获取新知识，然后通过利用额外获得的数据来精炼其策略。RL算法通过它们如何应用数学来探索、利用和在两者之间平衡，从而实现差异化。
- en: 'However, neural networks pose several challenges:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，神经网络也带来了一些挑战：
- en: · Resource requirements. Using a neural network to estimate a function that
    can determine the reward and best action to take for every state can take considerable
    time and data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: · 资源需求。使用神经网络来估算一个能够为每个状态确定奖励和最佳行动的函数可能需要相当长的时间和大量的数据。
- en: · Explainability. It is often difficult to understand how neural networks are
    arriving at their solutions, which limits their utility for providing real insight
    and can make it hard to predict or bound the action of a neural network. Explainability
    is especially important for physical systems as it would allow the powerful analytical
    tools developed over several centuries of mathematics to be used to gain additional
    insight into a system.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: · 可解释性。通常很难理解神经网络是如何得出其解决方案的，这限制了它们在提供真正洞见方面的实用性，并且可能使得预测或界定神经网络的行为变得困难。可解释性对于物理系统尤其重要，因为它能够使得几百年来数学上开发出的强大分析工具得以应用，从而为系统提供额外的洞见。
- en: 'While there are approaches, such as [transfer learning](/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a)
    and [topological analysis](/explainable-deep-neural-networks-2f40b89d4d6f), to
    address these challenges, they remain barriers to fuller application of RL. However,
    an alternate approach may be useful in our case where we are looking specifically
    at physical systems. Recall that the physical systems we are discussing are defined
    by, or can be very well described by, mathematical equations. Instead of having
    to develop a completely arbitrary function, we can focus on trying to find an
    expression comprised of common mathematical operators: arithmetic, algebraic,
    and transcendental functions (sine, e^x, etc.). Or means to this end will be using
    genetic algorithms. As described in [2], genetic algorithms can be adapted to
    explore function spaces through random generation of functions and exploiting
    and refining solutions through mutations and cross-breeding of promising candidates.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在一些方法，如[迁移学习](/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a)和[拓扑分析](/explainable-deep-neural-networks-2f40b89d4d6f)，来应对这些挑战，但它们仍然是强化学习广泛应用的障碍。然而，另一种替代方法可能在我们专注于物理系统时会有所帮助。回想一下，我们讨论的物理系统是通过数学方程来定义的，或者可以通过数学方程很好地描述。与其开发一个完全任意的函数，我们可以集中精力寻找由常见数学运算符组成的表达式：算术运算、代数运算以及超越函数（如正弦函数、e^x等）。为此目的的一种方法是使用遗传算法。正如在[2]中所描述的，遗传算法可以通过随机生成函数并通过变异和交叉繁殖有前途的候选者来探索函数空间，并通过这些方式利用和优化解决方案。
- en: So, while neural networks are champions of most RL problems, for physical dynamics
    a new challenger appears. Next we will take a closer look the generic algorithm
    approach and see how it fairs against a leading RL algorithm, soft actor critic.
    To do this we will evaluate both in physics-based gymnasiums using AWW Sagemaker
    Experiments. We will conclude by evaluating the results, discussing conclusions,
    and suggesting next steps.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，尽管神经网络在大多数强化学习问题中表现出色，但在物理动态系统中，出现了一位新的挑战者。接下来，我们将更深入地研究遗传算法方法，并查看它如何与领先的强化学习算法——软演员评论家（SAC）进行对比。为此，我们将在基于物理的训练场中使用AWW
    Sagemaker实验来评估两者的表现。最后，我们将通过评估结果、讨论结论并建议下一步的工作来结束本文。
- en: Recall that RL faces two challenges, exploring the environment and exploiting
    the information discovered. Exploration is necessary to find the best policy considering
    the likelihood of being in any state. Failure to explore means both a global optimum
    may be missed for a local, and the algorithm may not generalize sufficiently to
    succeed in all states. Exploitation is needed to refine the current solution to
    an optimum. However, as an algorithm refines a particular solution, it trades
    away the ability to explore the system further.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，强化学习（RL）面临着两个挑战：探索环境和利用已发现的信息。探索对于找到最佳策略是必要的，考虑到处于任何状态的可能性。如果不进行探索，就可能错过全局最优解，而仅停留在局部最优，且算法可能无法足够泛化，从而在所有状态下都能成功。利用已知信息则是为了将当前解决方案优化到最优。然而，当算法精细化一个特定的解决方案时，它就牺牲了进一步探索系统的能力。
- en: Soft Actor Critic (SAC) is a refinement of the powerful Actor-Critic RL approach.
    The Actor-Critic family of algorithms approaches the explore/exploit trade off
    by separating estimation of the state values and associated reward from optimizing
    a particular policy of inputs. As the algorithm collects new information, it updates
    each estimator. Actor-Critic has many nuances to its implementation; interested
    readers should consult [books](https://www.amazon.com/Mastering-Reinforcement-Learning-Python-next-generation/dp/1838644148)
    or online tutorials. SAC optimizes the critic by favoring exploration of states
    which have rewards dramatically different then the critic estimated. [OpenAI](https://spinningup.openai.com/en/latest/algorithms/sac.html)
    provides a detailed description of SAC.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Soft Actor Critic（SAC）是对强大Actor-Critic强化学习方法的改进。Actor-Critic算法家族通过将状态值及其相关奖励的估计与优化特定输入策略分开，来处理探索与利用之间的权衡。随着算法收集新信息，它会更新每个估计器。Actor-Critic在实现上有许多细节；有兴趣的读者可以参考[书籍](https://www.amazon.com/Mastering-Reinforcement-Learning-Python-next-generation/dp/1838644148)或在线教程。SAC通过优先探索那些奖励与估计的评价值差异显著的状态，来优化评价器。[OpenAI](https://spinningup.openai.com/en/latest/algorithms/sac.html)提供了SAC的详细描述。
- en: For this experiment, we use the [Coax](https://coax.readthedocs.io/en/latest/examples/pendulum/sac.html)
    implementation of SAC. I looked at several RL libraries, including Coach and Spinning
    Up, but Coax was one of the few I found to work mostly “out of the box” with current
    Python builds. The Coax library includes a wide range of RL algorithms, including
    PPO, TD3, and DDPG and works well with gymnasium.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本实验，我们使用了[SAC的Coax实现](https://coax.readthedocs.io/en/latest/examples/pendulum/sac.html)。我查看了几个强化学习库，包括Coach和Spinning
    Up，但Coax是我找到的少数几个在当前Python版本中“开箱即用”的库之一。Coax库包括广泛的强化学习算法，包括PPO、TD3和DDPG，并且与gymnasium兼容良好。
- en: Actor-critic methods such as SAC are typically implemented through neural networks
    as the function approximator. As we discussed last time, there is another potential
    approach to exploring the system and exploiting potential control policies. Genetic
    algorithms explore through random generation of possible solutions and exploit
    promising policies by mutating or combining elements (breeding) of different solutions.
    In this case, we will evaluate a genetic programming variant of genetic algorithms
    as an alternative means of function approximation; specifically, we will use a
    genetic approach to randomly generate and then evaluate trees of functions containing
    constants, state variables, and mathematical functions as potential controllers.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 像 SAC 这样的演员-评论员方法通常通过神经网络作为函数近似器来实现。正如我们上次讨论的那样，还有一种潜在的方法来探索系统并利用潜在的控制策略。遗传算法通过随机生成可能的解决方案来进行探索，并通过变异或组合不同解决方案的元素（繁殖）来利用有前景的策略。在这种情况下，我们将评估遗传算法的遗传编程变体，作为函数近似的另一种手段；具体来说，我们将使用遗传方法随机生成并评估包含常量、状态变量和数学函数的函数树，作为潜在的控制器。
- en: 'The Genetic Programming (GP) algorithm implemented is adapted from [2] except
    in place of tournament used by that text, this implementation selects the top
    64% (Nn below of 33%) of each generation as eligible for mutation and reseeds
    the remainder for better exploration of the solution space. To create each individual
    tree in a generation, a growth function randomly calls from arithmetic functions
    (+,-,*, /) and transcendental functions (such as e^x, cos (x)) to build branches
    with constants or state variables as leaves to end branches. Recursive calls are
    used to build expressions based on [Polish notation](https://en.wikipedia.org/wiki/Polish_notation#Computer_programming)
    ([2] implemented via LISP, I have adapted to Python), with rules in place to avoid
    i.e. divide by 0 and ensure mathematical consistency so that every branch ends
    correctly in a constant or sensor value leaf. Conceptually, an equation tree appears
    as:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 实现的遗传编程（GP）算法改编自[2]，不同之处在于，该文中使用的锦标赛被本实现替换为选择每代的前 64%（Nn，以下为 33%）作为突变的候选，并为剩余部分重新播种，以便更好地探索解空间。为了在每一代中创建个体树，生长函数随机调用算术函数（+，-，*，/）和超越函数（例如
    e^x, cos (x)）来构建分支，并将常量或状态变量作为叶子，构成树的终端分支。递归调用用于根据[波兰表示法](https://en.wikipedia.org/wiki/Polish_notation#Computer_programming)（[2]通过LISP实现，我已将其改编为Python）构建表达式，设有规则以避免如除以0等问题，并确保数学一致性，使每个分支最终正确地以常量或传感器值作为叶子。概念上，一个方程树的形式如下：
- en: '![](../Images/091d4561cf55ad9e6718afa9897fb8fa.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/091d4561cf55ad9e6718afa9897fb8fa.png)'
- en: Fig 1\. Example function tree, provided by author based on [2]
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 示例函数树，作者基于[2]提供
- en: 'This results in a controller b= sin (s1)+ e^(s1*s2/3.23)-0.12, written by the
    script as: — + sin s1 e^ / * s1 s2 3.23 0.12, where s denote state variables.
    It may seem confusing at first but writing out a few examples will clarify the
    approach.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了一个控制器 b = sin (s1) + e^(s1*s2/3.23) - 0.12，脚本中写作： — + sin s1 e^ / * s1 s2
    3.23 0.12，其中 s 表示状态变量。刚开始可能会让人感到困惑，但写出一些例子可以澄清这一方法。
- en: With a full generation of trees built, each one is then run through the environment
    to evaluate performance. The trees are then ranked for control performance based
    on achieved reward. If the desired performance is not met, the best performing
    tree is preserved, the top 66% are mutated by crossover (swapping elements of
    two trees), cut and grow (replace an element of a tree), shrink (replace a tree
    element with a constant) or re-parameterize (replace all constants in a tree)
    following [2]. This allows exploitation of the most promising solutions. To continue
    to explore the solution space, the low performing solutions are replaced with
    random new trees. Each successive generation is then a mix of random new individuals
    and replications or mutations of the top performing solutions.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建完整代数的树后，每棵树都会通过环境进行性能评估。然后，基于获得的奖励，按控制性能对树进行排名。如果未达到期望的性能，则保留表现最好的树，前 66%
    的树通过交叉（交换两棵树的元素）、剪切与生长（替换树的一个元素）、收缩（用常量替换树元素）或重新参数化（替换树中的所有常量）进行变异，参考文献[2]。这使得能够利用最有前景的解决方案。为了继续探索解空间，表现较差的解决方案会被随机新树所替代。每一代之后，新的代际将是随机新个体与顶级表现解决方案的复制或变异的结合。
- en: Trees are tested against random start locations within the environment. To prevent
    a “lucky” starting state from skewing results (analogous to overfitting the model),
    trees are tested against a batch of different random starting states.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 树会在环境中随机起始位置进行测试。为了防止“幸运”的起始状态影响结果（类似于模型过拟合），树会在一批不同的随机起始状态下进行测试。
- en: 'Hyperparameters for genetic programming include:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 遗传编程的超参数包括：
- en: '![](../Images/8f7382b6e0934b6d74ed6e925c3f70f3.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f7382b6e0934b6d74ed6e925c3f70f3.png)'
- en: Table 1\. Hyperparamters for Genetic Progamming Algorthim
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 遗传编程算法的超参数
- en: Commented code can be found on [github](https://github.com/retter-berkeley/MLC_Genetic).
    Note that I am a hobby coder, and my code is kludgy. Hopefully it is at least
    readable enough to understand my approach, despite any un-pythonic or generally
    bad coding practice.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 注释过的代码可以在[github](https://github.com/retter-berkeley/MLC_Genetic)上找到。请注意，我是一个业余编程者，代码可能有些笨拙。希望即使存在不符合
    Python 风格或普遍不良的编程实践，代码至少足够可读，能够理解我的方法。
- en: Evaluating the Approaches
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估方法
- en: Both algorithms were evaluated in two different gymnasium environments. The
    first is the simple [pendulum environment](https://gymnasium.farama.org/environments/classic_control/pendulum/)
    provided by gymnasium foundation. The inverted pendulum is a simple nonlinear
    dynamics problem. The action space is a continuous torque that can be applied
    to the pendulum. The observation space is the same as the state and is the x,y
    coordinates and angular velocity. The goal is to hold the pendulum upright. The
    second is the same gymnasium, but with random noise added to the observation.
    The noise is normal with mean 0 and variance 0.1 to simulate realistic sensor
    measurements.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 两种算法在两个不同的 gymnasium 环境中进行了评估。第一个是 gymnasium 基金会提供的简单[摆锤环境](https://gymnasium.farama.org/environments/classic_control/pendulum/)。倒立摆是一个简单的非线性动力学问题。动作空间是可以施加于摆锤的连续扭矩。观察空间与状态相同，是
    x、y 坐标和角速度。目标是保持摆锤直立。第二个是相同的 gymnasium 环境，但在观察中加入了随机噪声。噪声服从均值为 0，方差为 0.1 的正态分布，以模拟现实传感器测量值。
- en: One of the most important parts of RL development is designing a proper reward
    function. While there are many algorithms that can solve a given RL problem, defining
    an appropriate reward for those algorithms to optimize to is a key step in making
    a given algorithm successful for a specific problem. Our reward needs to allow
    us to compare the results of two different RL approaches while ensuring each proceeds
    to its goal. Here, for each trajectory we track cumulative reward and average
    reward. To make this easier, we have each environment run for a fixed number of
    time steps with a negative reward based on how far from the target state an agent
    is at each time step. The Pendulum gym operates this way out of the box — truncation
    at 200 timesteps and a negative reward depending on how far from upright the pendulum
    is, with a max reward at 0, enforced at every time step. We will use average reward
    to compare the two approaches.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习开发中最重要的部分之一是设计一个合适的奖励函数。尽管有许多算法可以解决给定的强化学习问题，但为这些算法定义一个合适的奖励以便优化，是使某个算法在特定问题上成功的关键步骤。我们的奖励需要让我们能够比较两种不同强化学习方法的结果，同时确保每种方法都能够朝着其目标前进。在这里，对于每个轨迹，我们跟踪累积奖励和平均奖励。为了简化这一过程，我们让每个环境运行一个固定数量的时间步，每个时间步根据代理距离目标状态的远近给予负奖励。Pendulum
    gym 就是这样工作的——在 200 个时间步后截断，并根据钟摆的竖直程度给予负奖励，最大奖励为 0，并在每个时间步都进行强制执行。我们将使用平均奖励来比较这两种方法。
- en: Our goal is to evaluate convergence speed of each RL framework. We will accomplish
    this using AWS Sagemaker Experiments, which can automatically track metrics (such
    as current reward) and parameters (such as active hyperparmeters) across runs
    by iteration or CPU time. While this monitoring could be accomplished through
    python tools, Experiments offers streamlined tracking and indexing of run parameters
    and performance and replication of compute resources. To set up the experiment,
    I adapted the examples provided by [AWS](https://sagemaker-examples.readthedocs.io/en/latest/index.html).
    The SAC and GP algorithms were first assed in local Jupyter notebooks and then
    uploaded to a git repository. Each algorithm has its own repository and Sagemaker
    notebook. The run parameters are stored to help classify the run and track performance
    of different experiment setups. Run metrics, for our cases reward and state vector,
    are the dependent variables we want to measure to compare the two algorithms.
    Experiments automatically record CPU time and iteration as independent variables.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是评估每个强化学习（RL）框架的收敛速度。我们将使用 AWS Sagemaker 实验来实现这一目标，它可以自动追踪度量指标（如当前奖励）和参数（如活跃的超参数），并按迭代或
    CPU 时间跨运行进行记录。尽管这些监控可以通过 Python 工具实现，但 Experiments 提供了简化的运行参数和性能跟踪、索引功能，以及计算资源的复现。为了设置实验，我参考了[AWS](https://sagemaker-examples.readthedocs.io/en/latest/index.html)提供的示例。SAC
    和 GP 算法首先在本地 Jupyter 笔记本中进行评估，然后上传到 git 仓库。每个算法都有自己的仓库和 Sagemaker 笔记本。运行参数被存储以帮助分类运行并跟踪不同实验设置的性能。我们的运行度量，奖励和状态向量，是我们希望用来比较两种算法的因变量。Experiments
    自动记录 CPU 时间和迭代次数，作为自变量。
- en: Through these experiments we can compare the performance of the champion, a
    well-developed, mature RL algorithm like SAC, against the contender, a little-known
    approach coded by a hobby coder without formal RL or python training. This experiment
    will provide insight into different approaches to developing controllers for complex,
    non-linear systems. In the next part we will review and discuss results and potential
    follow-ons.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些实验，我们可以将冠军算法——像 SAC 这样的成熟且经过充分开发的强化学习算法——与挑战者算法进行比较，后者是一种不为人知的方法，由一位没有正式强化学习或
    Python 培训的业余编码员编写。这个实验将为开发复杂非线性系统控制器的不同方法提供见解。在下一部分，我们将回顾并讨论结果及潜在的后续工作。
- en: 'The first experiment was the default pendulum gymnasium, where the algorithm
    tries to determine the correct torque to apply to keep pendulum inverted. It ends
    after a fixed time and gives negative reward based on how far from vertical the
    pendulum is. Prior to running in Sagemaker experiment, both SAC and GP algorithms
    were run on my local machine to verify convergence. Running in experiments allowed
    better tracking of comparable compute time. Results of compute time against average
    reward per iteration follow:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个实验是默认的 Pendulum gymnasium，在这个实验中，算法尝试确定正确的扭矩，以保持钟摆倒立。它在固定时间后结束，并根据钟摆距离竖直的程度给予负奖励。在
    Sagemaker 实验运行之前，SAC 和 GP 算法已经在我的本地机器上运行，以验证收敛性。在 Experiments 中运行可以更好地追踪可比较的计算时间。以下是计算时间与每次迭代平均奖励的结果：
- en: '![](../Images/2df0567b0b40db3641e835c194b25f03.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2df0567b0b40db3641e835c194b25f03.png)'
- en: Provided by author
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供
- en: '![](../Images/d1d598103d8a6bbf64d158fd729d2f97.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1d598103d8a6bbf64d158fd729d2f97.png)'
- en: Provided by author
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供
- en: We see that GP, despite being a less mature algorithm, arrived at a solution
    with far less computational requirement than SAC. On the local run to completion,
    SAC seemed to take about 400,000 iterations to converge, requiring several hours.
    The local instantiation was programmed to store recordings of SAC progress throughout
    training; interestingly SAC seemed to move from learning how to move the pendulum
    towards the top to learning how to hold the pendulum still, and then combined
    these, which would explain the dip in the reward as the time when SAC was learning
    to hold the pendulum steady. With GP we see monotonic increase in reward in steps.
    This is because the best performing function tree is always retained, so the best
    reward stays steady until a better controller is calculated.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，尽管GP算法相对不够成熟，但它在计算需求上远低于SAC算法。通过本地运行至完成，SAC似乎需要大约40万次迭代才能收敛，耗时数小时。本地实例化被编程为在整个训练过程中记录SAC的进展；有趣的是，SAC似乎先学会了如何让摆锤移动到顶端，然后学会了如何保持摆锤静止，最终将这两者结合起来，这可以解释奖励的下降期，即SAC学习保持摆锤稳定的阶段。通过GP，我们看到奖励以单调递增的方式逐步提升。这是因为最优的函数树始终被保留，所以最好的奖励保持稳定，直到计算出更好的控制器。
- en: The second experiment was adding Gaussian noise (0, 0.1) to the state measurement.
    We see similar results as with the no-noise situation, except with longer convergence
    times. Results are shown below; again, GP outperforms SAC.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个实验是将高斯噪声（0, 0.1）添加到状态测量中。我们看到的结果与无噪声的情况相似，只是收敛时间较长。结果如下所示；同样，GP优于SAC。
- en: '![](../Images/37c65ede7ac73ece0b7c6e9512a4e9b1.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/37c65ede7ac73ece0b7c6e9512a4e9b1.png)'
- en: Provided by author
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供
- en: '![](../Images/aba7ad80a4209ba235a59b35166022d7.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aba7ad80a4209ba235a59b35166022d7.png)'
- en: Provided by author
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供
- en: In both cases we see GP perform faster than SAC (as with the previous example,
    SAC did converge locally, I just didn’t want to pay AWS for the compute time!).
    However, as many of you have no doubt noticed, this has been a very basic comparison,
    both in terms of machine learning and physical systems. For example, hyperparameter
    tunning could result in different results. Still, this is a promising start for
    the contender algorithm and show it to be worth further investigation.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，我们看到GP比SAC表现得更快（如前面的例子所示，SAC确实在本地收敛，只是我不想为计算时间支付AWS费用！）。然而，正如大家无疑已经注意到的，这只是一个非常基础的比较，既涉及机器学习也涉及物理系统。例如，超参数调优可能会导致不同的结果。尽管如此，这对于该候选算法来说是一个有前景的开始，表明它值得进一步研究。
- en: 'In the long run, I think GP may offer several benefits over neural network-based
    approaches like SAC:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从长远来看，我认为GP可能比基于神经网络的方法（如SAC）提供几个优势：
- en: · Explainability. While the equation GP finds can be convoluted, it is transparent.
    Skilled may simplify the equation, helping provide insight into the physics of
    the determined solution, helpful for determining regions of applicability and
    increasing trust in the control. Explainability, while an active area of research,
    remains a challenge for neural networks.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: · 可解释性。尽管GP找到的方程可能很复杂，但它是透明的。熟练者可以简化方程，帮助提供对已确定解的物理理解，这对于确定适用区域和增加控制信任度非常有帮助。尽管可解释性是一个活跃的研究领域，但神经网络依然面临这一挑战。
- en: · Informed ML. GP allows easier application of insight into the system under
    analysis. For example, if the system is known to have sinusoidal behavior, the
    GP algorithm can be adapted to try more sinusoidal solutions. Alternatively, if
    a solution is known for a similar or simplified system to the one under study,
    then that solution can be pre-seeded into the algorithm.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: · 信息化机器学习（Informed ML）。遗传规划（GP）使得将系统分析的见解更容易应用。例如，如果已知系统具有正弦波行为，GP算法可以调整以尝试更多的正弦解。或者，如果已知一个类似或简化系统的解，那么可以将该解预先植入算法中。
- en: · Stability. With addition of simple safeguards mathematical validity and limit
    absolute value, GP approaches will remain stable. As long as the top performer
    is retained each generation then solution will converge, though time bounds on
    convergence are not guaranteed. Neural network approaches of more common RL do
    not have such guarantees.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: · 稳定性。通过简单的保障措施（如数学有效性和限制绝对值的增加），GP方法将保持稳定。只要每一代保留最优解，解决方案就会收敛，尽管收敛的时间界限无法保证。常见的强化学习中的神经网络方法并没有这样的保证。
- en: · Developmental Opportunity. GP is relatively immature. The SAC implementation
    here was one of several available for application, and neural networks have been
    the benefit of extensive effort to improve performance. GP hasn’t benefit from
    such optimization; my implementation was built around function rather than efficiency.
    Despite this, it performed well against SAC, and further improvements from more
    professional developers could provide high gains in efficiency.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: · 发展机会。遗传编程相对不成熟。这里使用的SAC实现是多种应用实现中的一种，而神经网络已经受益于大量努力以提高性能。遗传编程没有享受这种优化；我的实现是围绕函数构建的，而非效率。尽管如此，它在与SAC对比时表现良好，来自更专业开发者的进一步改进可能会带来显著的效率提升。
- en: · Parallelizability and modularity. Individual GP equations are simple compared
    to NNs, the computational cost comes from repeated runs through the environment
    rathe than environment runs and backpropagation of NNs. It would be easy to split
    a “forest” of different GP equation trees across different processors to greatly
    improve computing speed.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: · 并行性和模块化。与神经网络相比，单独的遗传编程方程较为简单，计算成本来自于在环境中多次运行，而不是环境运行和神经网络的反向传播。将不同的遗传编程方程树分配到不同的处理器上，可以大大提高计算速度。
- en: 'However, neural network approaches are used more extensively for good reason:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，神经网络方法因其众多优势被广泛使用：
- en: · Scope. Neural networks are universal function approximators. GP is limited
    to the terms defined in the function tree. Hence, neural network based approaches
    can cover a far greater range and complexity of situations. I would not want to
    try GP to play Starcraft or drive a car.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: · 范围。神经网络是通用函数逼近器，遗传编程仅限于在函数树中定义的项。因此，基于神经网络的方法能够涵盖更广泛和复杂的情况。我不愿意尝试用遗传编程来玩《星际争霸》或驾驶汽车。
- en: · Tracking. GP is a refined version of random search, which results, as seen
    in the experiment, halting improvement.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: · 跟踪。遗传编程是随机搜索的精细版本，正如实验中所见，导致了改进的停滞。
- en: · Maturity. Because of the extensive work across many different neural based
    algorithms, it is easier to find an existing one optimized for computational efficiency
    to more quickly apply to a problem.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: · 成熟度。由于大量不同神经网络算法的研究工作，现有的经过优化的神经网络模型更容易应用于问题，并具有较高的计算效率。
- en: 'From a machine learning perspective, we have only scratched the surface of
    what we can do with these algorithms. Some follow-ons to be considered include:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从机器学习的角度来看，我们仅仅触及了这些算法的表面。需要考虑的一些后续工作包括：
- en: · Hyperparameter tuning.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: · 超参数调优。
- en: · Controller simplicity, such as penalizing reward for number of terms in control
    input for GP.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: · 控制器简化，例如惩罚遗传编程中控制输入项数的奖励。
- en: · Controller efficiency, such as detracting size of control input from reward.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: · 控制器效率，例如从奖励中扣除控制输入的大小。
- en: · GP monitoring and algorithm improvement as described above.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: · 如上所述，遗传编程的监控与算法改进。
- en: 'From a physics perspective, this experiment serves as a launching point into
    more realistic scenarios. More complex scenarios will likely show NN approaches
    catch up to or surpass GP. Possible follow-ons include:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 从物理学角度来看，本实验作为进入更真实场景的起点。更复杂的场景可能会显示出神经网络方法赶上或超过遗传编程（GP）。可能的后续工作包括：
- en: · More complex dynamics such as Van Der Pol equations or higher dimensionality.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: · 更复杂的动力学，例如范德波尔方程或更高维度。
- en: · Limited observability instead of full state observability.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: · 有限可观察性，而非完全状态可观察性。
- en: · Partial Differential Equation systems and optimizing controller location as
    well as input.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: · 偏微分方程系统以及优化控制器位置和输入。
- en: '[1] E. Bilgin, Mastering Reinforcement Learning with Python: Build next-generation,
    self-learning models using reinforcement learning techniques and best practices
    (2020), Packit Publishing'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] E. Bilgin, 《掌握强化学习与Python：使用强化学习技术和最佳实践构建下一代自学习模型》（2020），Packit出版社'
- en: '[2] T Duriez, S. Brunton, B. Noack, Machine Learning Control- Taming Nonlinear
    Dynamics and Turbulence (2017), Spring International Publishing'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] T Duriez, S. Brunton, B. Noack, 《机器学习控制——驯服非线性动力学与湍流》（2017），Spring国际出版公司'
