- en: Distance Metric Learning for Outlier Detection
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 距离度量学习用于异常值检测
- en: 原文：[https://towardsdatascience.com/distance-metric-learning-for-outlier-detection-5b4840d01246?source=collection_archive---------1-----------------------#2024-08-20](https://towardsdatascience.com/distance-metric-learning-for-outlier-detection-5b4840d01246?source=collection_archive---------1-----------------------#2024-08-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/distance-metric-learning-for-outlier-detection-5b4840d01246?source=collection_archive---------1-----------------------#2024-08-20](https://towardsdatascience.com/distance-metric-learning-for-outlier-detection-5b4840d01246?source=collection_archive---------1-----------------------#2024-08-20)
- en: An outlier detection method that determines a relevant distance metric between
    records
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一种异常值检测方法，旨在确定记录之间的相关距离度量
- en: '[](https://medium.com/@wkennedy934?source=post_page---byline--5b4840d01246--------------------------------)[![W
    Brett Kennedy](../Images/b3ce55ffd028167326c117d47c64c467.png)](https://medium.com/@wkennedy934?source=post_page---byline--5b4840d01246--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5b4840d01246--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5b4840d01246--------------------------------)
    [W Brett Kennedy](https://medium.com/@wkennedy934?source=post_page---byline--5b4840d01246--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@wkennedy934?source=post_page---byline--5b4840d01246--------------------------------)[![W
    Brett Kennedy](../Images/b3ce55ffd028167326c117d47c64c467.png)](https://medium.com/@wkennedy934?source=post_page---byline--5b4840d01246--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5b4840d01246--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5b4840d01246--------------------------------)
    [W Brett Kennedy](https://medium.com/@wkennedy934?source=post_page---byline--5b4840d01246--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5b4840d01246--------------------------------)
    ·18 min read·Aug 20, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5b4840d01246--------------------------------)
    ·阅读时间：18分钟·2024年8月20日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'Outliers are often defined as the items in a dataset that are very different
    than the majority of the other items. That is: any record that is significantly
    different from all other records (or from almost all other records), and is more
    different from the other records than is normal, could reasonably be considered
    an outlier.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值通常被定义为在数据集中与大多数其他项非常不同的项。也就是说，任何与其他所有记录（或几乎所有记录）显著不同，并且与其他记录的差异超过正常范围的记录，都可以合理地被视为异常值。
- en: '![](../Images/526f551b130cc3268274ea556396e48d.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/526f551b130cc3268274ea556396e48d.png)'
- en: 'In the dataset shown here, we have four clusters (A, B, C, and D) and three
    points outside these clusters: P1, P2, and P3\. These can likely be considered
    outliers, as they are each far from all other points — that is, they are significantly
    different than most other points.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里展示的数据集中，我们有四个簇（A、B、C和D）和三个位于这些簇之外的点：P1、P2和P3。这些点很可能被视为异常值，因为它们每个都与其他所有点的距离较远——也就是说，它们与大多数其他点显著不同。
- en: As well, Cluster A has only five points. While these points are fairly close
    to each other, they are far from all other points, so could quite possibly be
    considered outliers as well — again, based on the distances from these points
    to the majority of other points.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，A类簇仅有五个点。尽管这些点相互之间距离较近，但它们与其他所有点的距离较远，因此也有可能被认为是异常值——再次强调，是基于这些点与其他大多数点之间的距离。
- en: The inliers, on the other hand (the points within the larger clusters), are
    all very close to a significant number of other points. For example, any point
    in the middle of Cluster C is very close to many other points (i.e. is very similar
    to many other points), so would not be considered an outlier.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，内点（位于较大簇中的点）与许多其他点非常接近。例如，C类簇中间的任何点都非常接近许多其他点（即与许多其他点非常相似），因此不会被视为异常值。
- en: There are numerous other ways we can look at outliers, and many other approaches
    are actually used for outlier detection — for example outlier detection methods
    based on F[requent Item Sets](https://medium.com/towards-data-science/interpretable-outlier-detection-frequent-patterns-outlier-factor-fpof-0d9cbf51b17a),
    Association Rules, compression, Markov Models, and so on. But identifying the
    records that are similar to few other records, and that are relatively different
    from the records they are *most* similar to, is very common. This is, in fact,
    the underlying idea behind many of the most common outlier detection algorithms,
    including kNN, LOF (Local Outlier Factor), Radius, and numerous other algorithms.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从多个角度来看待异常值，实际上也有许多方法用于异常值检测——例如基于频繁项集的异常值检测方法（[Frequent Item Sets](https://medium.com/towards-data-science/interpretable-outlier-detection-frequent-patterns-outlier-factor-fpof-0d9cbf51b17a)）、关联规则、压缩、马尔科夫模型等等。但识别与其他记录相似且与它们最相似的记录有相对不同之处的记录是非常常见的。实际上，这是许多最常见的异常值检测算法的基本思想，包括kNN、LOF（局部异常因子）、半径算法以及许多其他算法。
- en: But, using this approach leaves the question of how to quantify how different
    a record is from the other records. There are a number of techniques to do this.
    Some of the most common in outlier detection include Euclidean, Manhattan, and
    Gower distances, as well as a number of similar metrics.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用这种方法会留下一个问题，即如何量化一条记录与其他记录的差异。有多种技术可以实现这一点。在异常值检测中，一些最常见的包括欧几里得距离、曼哈顿距离和高尔距离，以及一些类似的度量方法。
- en: We’ll cover these quickly below. But we want to look in this article specifically
    at a very versatile, and likely under-used, method for calculating the difference
    between two records in tabular data that’s very useful for outlier detection,
    called *Distance Metric Learning* — as well as a method to apply this specifically
    to outlier detection.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下面简要介绍这些方法。但在本文中，我们将特别关注一种非常通用且可能使用较少的计算方法，用于计算表格数据中两条记录之间的差异，这对异常值检测非常有用，这就是*距离度量学习*——以及如何将此方法专门应用于异常值检测。
- en: This article continues a series on outlier detection that includes [Counts Outlier
    Detector](/counts-outlier-detector-interpretable-outlier-detection-ead0d469557a),
    [Frequent Patterns Outlier Factor](/interpretable-outlier-detection-frequent-patterns-outlier-factor-fpof-0d9cbf51b17a),
    and techniques to [tune and test detectors (using a method called *doping*)](/doping-a-technique-to-test-outlier-detectors-3f6b847ab8d4).
    It also includes another excerpt from my book [Outlier Detection in Python](https://www.manning.com/books/outlier-detection-in-python).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本文是关于异常值检测系列的继续，之前的文章包括[计数异常值检测器](/counts-outlier-detector-interpretable-outlier-detection-ead0d469557a)、[频繁模式异常值因子](/interpretable-outlier-detection-frequent-patterns-outlier-factor-fpof-0d9cbf51b17a)以及[调节和测试检测器（使用一种称为*doping*的方法）](/doping-a-technique-to-test-outlier-detectors-3f6b847ab8d4)。它还包括我书籍[《Python中的异常值检测》](https://www.manning.com/books/outlier-detection-in-python)的另一个摘录。
- en: Distance Metrics
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 距离度量
- en: 'To determine if a record is 1) unusually far from most other records; and 2)
    close to relatively few records, we generally first calculate the pairwise distances:
    the distances between each pair of records in a dataset. In practice, we may take
    a more optimized approach (for example only calculating approximate distances
    where records are known to be very far apart in any case), but, at least in principle,
    calculating the distances between each pair of rows is common in outlier detection.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了判断一条记录是否 1) 与大多数其他记录的距离异常远；以及 2) 与相对较少的记录接近，我们通常首先计算每对记录之间的距离：即数据集中每一对记录之间的距离。在实际应用中，我们可能采用更优化的方法（例如，只计算那些已知在任何情况下都相距很远的记录之间的近似距离），但至少在原则上，计算每对行之间的距离在异常值检测中是常见的做法。
- en: Which means, we need a way to calculate the distance between any two records.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，我们需要一种方法来计算任意两条记录之间的距离。
- en: If we have a set of data such as the following, a large table of staff records
    (here showing a random subset of four rows), how can we best say how similar any
    two rows are?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一组数据，比如下面这样的大型员工记录表（这里显示的是四行的随机子集），我们如何最好地判断任意两行之间的相似度呢？
- en: '![](../Images/89fdbaaebf30a9d5ea9fa0b330ac1be9.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89fdbaaebf30a9d5ea9fa0b330ac1be9.png)'
- en: Euclidean Distances
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 欧几里得距离
- en: One very common method is to use the Euclidean distance.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一种非常常见的方法是使用欧几里得距离。
- en: Before looking further at the staff data, consider again the scatter plot above.
    We see here a case where using the Euclidean distance feels natural. As this dataset
    contains only two features, and both are numeric, plotting the data as in this
    figure (as a scatter plot) is fairly intuitive. And, once plotted in this way,
    we naturally picture the Euclidean distances between points (based on the Pythagorean
    formula).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在进一步查看员工数据之前，请再次考虑上面的散点图。我们在这里看到一个使用欧几里得距离显得很自然的情况。由于该数据集仅包含两个特征，且这两个特征都是数值型的，因此将数据如图所示绘制为散点图是相当直观的。一旦以这种方式绘制，我们自然就能想象出基于毕达哥拉斯公式计算出的点与点之间的欧几里得距离。
- en: 'In cases, though, with: many features; where many of these are categorical;
    and with associations among the columns, the Euclidean distances between rows,
    while still valid and often useful, can feel less natural'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在具有许多特征的情况下，尤其是这些特征中有很多是分类的，并且各列之间存在关联时，行与行之间的欧几里得距离，尽管仍然有效并且经常有用，但可能显得不那么自然。
- en: 'An issue with Euclidean distances is that they are really intended for numeric
    data, though most real-world data, like the staff records, is mixed: containing
    both numeric and categorical features. Categorical values can be encoded numerically
    (using, for example, One-Hot, Ordinal, or other encoding methods), which then
    allows calculating Euclidean distances (as well as other numeric distance measures).
    But it isn’t always ideal. And each method of numeric encoding has its own implications
    for the distances calculated. But it is quite possible, and quite common.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用欧几里得距离的一个问题是，它们实际上是为数值数据设计的，尽管大多数现实世界的数据，如员工记录，是混合型的：同时包含数值和分类特征。分类值可以通过数值编码（例如，使用独热编码、序数编码或其他编码方法）进行编码，从而可以计算欧几里得距离（以及其他数值距离度量）。但这并不总是理想的。每种数值编码方法对计算出的距离都有其特定的影响。不过，这种做法是完全可行的，也非常常见。
- en: 'Considering the Staff table above: we would likely leave ID and Last Name out
    of the outlier detection process, using the remainder of the columns. Given that,
    we will still have the Department and Office features as categorical. Let’s assume
    we encode these using one-hot encoding.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到上面的员工表，我们可能会将ID和姓氏排除在离群值检测过程之外，仅使用其余的列。鉴于此，我们仍然会将部门和办公室特征视为分类特征。假设我们使用独热编码来对其进行编码。
- en: To calculate the Euclidean distances between rows, we also must scale the numeric
    features, putting all features on the same scale. This can be done a variety of
    ways, include Standardizing (converting values to their z-values, based on the
    number of standard deviations a value is from the mean of that column), or min-max
    scaling.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算行之间的欧几里得距离，我们还必须对数值特征进行缩放，将所有特征放置在相同的尺度上。这可以通过多种方式完成，包括标准化（将数值转换为其z值，基于该列均值与数值的标准差距离），或最小-最大缩放。
- en: Once the data is numerically encoded and scaled, we may then calculate the Euclidean
    distance between every pair of rows.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据被数值化并缩放，我们就可以计算每一对行之间的欧几里得距离。
- en: Gower Distances
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高尔距离
- en: Alternatively, given we have some categorical features, we can use a method
    designed for mixed data, such as the Gower distance. This, to compare any two
    rows, takes the difference column by column and sums these differences. Where
    the data is strictly numeric, it is equivalent to the Manhattan distance.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，考虑到我们有一些分类特征，我们可以使用为混合数据设计的方法，例如高尔距离。该方法在比较任何两行时，逐列计算差异并将这些差异相加。当数据严格为数值时，它等同于曼哈顿距离。
- en: 'For categorical columns, with Gower distances, usually Ordinal Encoding is
    used, as we are only concerned if there is an exact match or not. The difference
    in two values of a categorical column is then either 0.0 or 1.0\. In the Staff
    table above, Smith and Jones have a distance of 1.0 for Department (1.0 is always
    used with different values: ‘Engineering’ and ‘Sales’ in this case) and a distance
    of 0.0 for Office (0.0 is always used where two rows have the same value: ‘Toronto’
    in this case).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类列，在使用高尔距离时，通常使用序数编码，因为我们只关心是否存在精确匹配。分类列中两个值之间的差异要么是0.0，要么是1.0。在上面的员工表中，史密斯和琼斯在部门上有1.0的距离（不同值：‘工程’和‘销售’，此时总是使用1.0），在办公室上有0.0的距离（相同值：‘多伦多’，此时总是使用0.0）。
- en: 'To compare the numeric fields, as with Euclidean, and most distance metrics,
    we will need to scale them first, so that the numeric fields may all be treated
    equally. As indicated, there are a number of ways to do this, but let’s assume
    we use min-max scaling here, which puts all values on a scale between 0.0 and
    1.0\. We may then have a table such as:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较数值字段，就像欧几里得距离和大多数距离度量一样，我们需要先对它们进行缩放，以便所有数值字段可以被平等对待。如上所述，有多种方式可以实现这一点，但我们假设这里使用最小-最大缩放方法，将所有值映射到0.0到1.0的范围内。我们可能会得到一个如下表格：
- en: '![](../Images/27889cf6863bac07250eb27e8c8599d5.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/27889cf6863bac07250eb27e8c8599d5.png)'
- en: 'The difference (using Gower Distance) between Smith and Jones would then be:
    abs(0.90 — 0.20) + abs(0.93 — 0.34) + abs(0.74 — 0.78) + 1.0 + abs(0.88 — 0.77)
    + abs(0.54 — 0.49) + 0.0 + abs(0.32 — 0.38).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 使用高尔距离计算的史密斯和琼斯之间的差异将是：abs(0.90 — 0.20) + abs(0.93 — 0.34) + abs(0.74 — 0.78)
    + 1.0 + abs(0.88 — 0.77) + abs(0.54 — 0.49) + 0.0 + abs(0.32 — 0.38)。
- en: That is, skipping ID and Last Name, we calculate the absolute difference in
    each numeric field and take either 0.0 or 1.0 for each categorical field.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，跳过ID和姓氏，我们计算每个数值字段的绝对差异，并为每个类别字段取0.0或1.0。
- en: 'This may be reasonable, though does have some issues. The main one is likely
    that the categorical fields have more weight than the numeric: they will often
    have a difference of 1.0, where numeric values will tend to have smaller differences.
    For example, the Age difference between Smith and Jones is quite large, but will
    only have a difference of abs(0.93–0.34), or 0.59 (still significant, but less
    than the 1.0 that the Department counts towards the total difference between the
    rows). As covered in [Outlier Detection in Python](https://www.manning.com/books/outlier-detection-in-python),
    one-hot encoding and other encodings with other distance metrics have similar
    issues handling mixed data.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可能是合理的，但也存在一些问题。最主要的问题可能在于类别字段比数值字段有更大的权重：类别字段通常会有1.0的差异，而数值字段则往往只有较小的差异。例如，史密斯和琼斯的年龄差距非常大，但它们的差异仅为abs(0.93–0.34)，即0.59（尽管差异仍然显著，但小于部门字段对行之间总差异的1.0影响）。正如在[Python中的异常值检测](https://www.manning.com/books/outlier-detection-in-python)中所提到的，一热编码和其他编码方式在使用其他距离度量时也存在处理混合数据的类似问题。
- en: As well, all categorical features are equally relevant as each other; and all
    numeric features are equally relevant as each other, even where some are, for
    example, highly correlated, or otherwise should possibly carry more or less weight.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，所有类别特征在相互之间同等重要；所有数值特征在相互之间也同等重要，即便其中一些特征，如高度相关的特征，可能应当有更大或更小的权重。
- en: In general, distance metrics such as Euclidean or Gower distance (and other
    metrics such as Manhattan, Canberra and so on), may be appropriate distance metrics
    in many cases, and are often excellent choices for outlier detection. But, at
    the same time, they may not always be ideal for all projects.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，像欧几里得距离或高尔距离（以及其他距离度量如曼哈顿距离、堪培拉距离等）在许多情况下可能是合适的距离度量，且通常是异常值检测的优选方法。但与此同时，它们并不总是适用于所有项目。
- en: Euclidean Distances Viewed as Physical Distances in High Dimensional Space
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 欧几里得距离视为高维空间中的物理距离
- en: Looking again at Euclidean distances, these essentially consider the records
    each as points in high-dimensional space, and calculate the distances between
    these points in this space. Manhattan and Gower distances are a bit different,
    but work quite similarly.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 再次查看欧几里得距离，这些基本上将记录视为高维空间中的点，并计算这些点之间的距离。曼哈顿距离和高尔距离略有不同，但原理非常相似。
- en: 'As as simpler example than the full Staff table, consider this table, but for
    the moment just including the numeric features: Years of Service, Age, Salary,
    # Vacation Days, # Sick Days, and Last Bonus. That’s six features, so each row
    can be viewed as a point in 6-dimensional space, with the distances between them
    calculated using the Pythagorean formula.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个比完整的员工表更简单的例子，考虑这个表格，但目前只包含数值特征：服务年限、年龄、薪水、年假天数、病假天数和最后一次奖金。这是六个特征，因此每一行可以视为6维空间中的一个点，点与点之间的距离使用毕达哥拉斯公式来计算。
- en: This is reasonable, but is certainly not the only way to look at the distances.
    And, the distance metric used can make a substantial difference to the outlier
    scores assigned. For example, Euclidean distances can put more emphasis on a few
    features with very different values than, say, Manhattan distances would.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这是合理的，但显然并不是唯一的看待距离的方式。而且，所使用的距离度量会对异常值评分产生实质性的影响。例如，欧几里得距离可能比曼哈顿距离更强调一些具有非常不同值的特征。
- en: Example of Euclidean and Manhattan Distances
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 欧几里得和曼哈顿距离的例子
- en: We’ll consider here two different cases of this 6-dimensional data (showing
    also the ID and Last Name columns for reference).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里考虑这组6维数据的两种不同情况（同时展示ID和姓氏列以供参考）。
- en: 'First, an example for two staff, Greene and Thomas, where most values are similar,
    but Years Service is very different:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，举个例子是对于两位员工Greene和Thomas，其中大多数值相似，但服务年限却有很大差异：
- en: '![](../Images/dacaf57d71313fabddb9b6fa6531bcfd.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dacaf57d71313fabddb9b6fa6531bcfd.png)'
- en: 'Second, an example for two other staff, Ford and Lee, with most values moderately
    different but none very different:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，另一个例子是对于两位其他员工，Ford和Lee，他们的大多数值都适中地不同，但没有非常不同的值：
- en: '![](../Images/93687905f9500e5d3ce6deb69536f2aa.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93687905f9500e5d3ce6deb69536f2aa.png)'
- en: Which of these pairs of rows is most similar? Using Manhattan distances, Greene
    and Thomas are most similar (having a distance of 0.59, compared to 0.60). Using
    Euclidean distances, Ford and Lee are most similar (having a distance of 0.27,
    compared to 0.50).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 哪一对行最相似？使用曼哈顿距离时，Greene和Thomas最相似（它们的距离是0.59，相比之下0.60）。使用欧几里得距离时，Ford和Lee最相似（它们的距离是0.27，相比之下是0.50）。
- en: It’s not always clear when using Manhattan or Euclidean distances is more suitable,
    or when it’s preferable to use another metric, such as Canberra, or Minkowski
    (using, for example, cubed distances), Mahalanobis, and so on. This is not necessarily
    an issue, but it does highlight that there’s many ways to look at the distances
    between rows.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在何时使用曼哈顿距离或欧几里得距离更合适，或者何时使用其他度量（例如Canberra、Minkowski（例如使用立方距离）、Mahalanobis等）更合适，通常并不明确。这不一定是一个问题，但它确实突显了我们可以用许多不同的方式来看待行与行之间的距离。
- en: Euclidean distances, specifically, imply we’re viewing the data as points in
    high-dimensional space, and are taking what’s equivalent to the physical distance
    between them. This has some real value, but it isn’t always entirely natural.
    Simply looking at a table of values, such as the Staff data above, we picture
    the rows (in this example) as staff records, not points in space.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离特别意味着我们将数据视为高维空间中的点，并计算它们之间的物理距离。这确实有一定的价值，但并非总是完全自然的。仅仅查看如上所示的员工数据表，我们会将这些行（在这个例子中）看作是员工记录，而不是空间中的点。
- en: And, using the Euclidean distance requires taking the squared age, squared salary,
    and so on — which lacks any intuitive appeal. It’s not clear what something like,
    for example, the squared age really means. It can work well, but a geometric interpretation
    of the data is really just one of many ways we can picture the data.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，使用欧几里得距离需要取年龄的平方、工资的平方等等——这缺乏直观的吸引力。像年龄的平方这样的东西实际上是什么意思并不明确。它可以很好地工作，但数据的几何解释仅仅是我们可以想象数据的众多方式之一。
- en: Further, it’s a generic method, that does not consider the data itself.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，它是一种通用方法，不考虑数据本身。
- en: Distance Metric Learning
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 距离度量学习
- en: Distance Metric Learning presents another way to think about the problem of
    determining how similar two records are. Instead of first defining a distance
    measure and then applying it to the data at hand, Distance Metric Learning attempts
    to learn from the data itself how similar records are to each other.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 距离度量学习提供了另一种思考如何判断两个记录相似度的问题的方式。它不是首先定义一个距离度量然后应用于当前数据，而是尝试从数据本身学习记录之间的相似度。
- en: 'It also addresses a limitation of Euclidean, Manhattan, and most other distance
    metrics: that all features are treated equally, whether this is most appropriate
    or not.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 它还解决了欧几里得、曼哈顿和大多数其他距离度量的局限性：所有特征都被平等对待，无论这是否最为合适。
- en: 'The idea here is: some features are more relevant than others, and some features
    are related to each other (in some cases, sets of features may even be redundant,
    or nearly). Simply treating every feature identically is not necessarily the best
    way to identify the most anomalous records in a dataset.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的想法是：一些特征比其他特征更为重要，且一些特征之间是相互关联的（在某些情况下，特征集合甚至可能是冗余的，或者几乎是冗余的）。简单地将每个特征视为相同并不一定是识别数据集中最异常记录的最佳方法。
- en: Distance Metric Learning is a major area in itself, but I’ll cover here one
    approach to how it may be applied to outlier detection. Specifically, we’ll look
    here at an application Distance Metric Learning for outlier detection based on
    creating Random Forests.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 距离度量学习本身是一个重要的领域，但在这里我将介绍一种它如何应用于异常值检测的方法。具体来说，我们将在这里讨论基于创建随机森林的异常值检测的距离度量学习应用。
- en: 'Assume, for the moment, that:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 暂时假设：
- en: We have a Random Forest that predicts some target
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们有一个预测某个目标的随机森林
- en: We have a table of data that can be passed through the Random Forest (e.g. the
    staff data, but any tabular data)
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们有一张数据表，可以通过随机森林进行处理（例如，员工数据，但任何表格数据都可以）。
- en: We want to calculate the distances between each pair of rows.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们想要计算每一对行之间的距离。
- en: We’ll use these pairwise distances for outlier detection for the discussion
    here, but could in principle use them for any purpose.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里使用这些成对的距离进行异常值检测，但原则上可以用于任何目的。
- en: We’ll describe soon how to create a Random Forest for this, but assume for the
    moment that we have a Random Forest and that it is of good quality, well-trained,
    and robust.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很快会描述如何为此创建一个随机森林，但暂时假设我们已经有一个随机森林，并且它的质量很好，经过良好的训练，且非常稳健。
- en: One thing we can do to estimate how similar rows are to each other is look at
    the predictions the Random Forest makes. Let’s assume the Random Forest is trained
    as a binary classifier, so can produce, for each record in the data, a predicted
    probability of being the positive class.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用来估计行与行之间相似度的一种方法是查看随机森林所做的预测。假设随机森林被训练成一个二分类器，那么它可以为数据中的每一条记录生成预测的正类概率。
- en: Two records passed through the Random Forest may have very similar probabilities,
    say 0.615 and 0.619\. These are very close, so we can suspect that the two records
    are similar to each other. But, not necessarily. They may actually follow quite
    different decision paths through the many decision trees within the Random Forest,
    and happen to average out to similar predictions. That is, they may receive similar
    predictions for different reasons, and may not be similar at all.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 通过随机森林的两个记录可能有非常相似的概率，比如0.615和0.619。这两个值非常接近，因此我们可以怀疑这两个记录彼此相似。但并不一定如此。它们实际上可能通过随机森林中的多个决策树走上完全不同的决策路径，恰好平均得出相似的预测。也就是说，它们可能因不同的原因得出相似的预测，实际上可能一点也不相似。
- en: What’s most relevant is the decision paths the records take through the decision
    trees. If two records take the same paths in most of the trees (and so end in
    the same leaf nodes), then we can say that they are similar (at least in this
    respect). And if they, for the most part, end in different leaf nodes, we can
    say they are different.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最相关的是记录通过决策树所走的决策路径。如果两条记录在大多数决策树中走的是相同的路径（因此最终落在相同的叶子节点），那么我们可以说它们是相似的（至少在这一方面）。如果它们大多数情况下落在不同的叶子节点，那么我们可以说它们是不同的。
- en: This, then, provides a powerful tool to determine, in a sensible way, how similar
    any two records are.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这就提供了一个强大的工具，以一种合理的方式确定任意两条记录的相似度。
- en: Creating a Random Forest
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建随机森林
- en: This is clearly a useful idea, but it does require a Random Forest, and a Random
    Forest that is meaningful for this purpose — one that captures well the nature
    of the data available.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这显然是一个有用的想法，但它确实需要一个随机森林，并且需要一个对这个目的有意义的随机森林——即能够很好地捕捉到可用数据的特征的随机森林。
- en: One way to create such a Random Forest is to build one that learns to distinguish
    this data from similar, but fake, data. That is, data that’s synthetically generated
    to be similar, but not quite the same as this data (such that it is distinguishable).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 创建这样一个随机森林的一种方法是构建一个能够学习区分这些数据与类似但虚假的数据的随机森林。也就是说，数据是通过合成生成的，虽然与这些数据相似，但不完全相同（以便可以区分开来）。
- en: So, if we can create a such a set of fake data, we can then train a Random Forest
    classifier to distinguish the two types of data.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们能够创建这样一组虚拟数据，我们就可以训练一个随机森林分类器来区分这两种数据类型。
- en: There are a number of ways to create the synthetic data to be used here, including
    several covered in [Outlier Detection in Python](https://www.manning.com/books/outlier-detection-in-python).
    One, for example, is doping (also covered in this [Medium article](https://medium.com/towards-data-science/doping-a-technique-to-test-outlier-detectors-3f6b847ab8d4)).
    We’ll look, though, at another method here that can work well. This can be overly
    simplistic and not always as effective as more sophisticated techniques, but it
    does provide a nice, simple introduction to the idea.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 创建用于此处的合成数据有多种方法，其中一些方法在[Python中的离群值检测](https://www.manning.com/books/outlier-detection-in-python)一书中有详细介绍。例如，其中一种方法是加料（该方法也在这篇[Medium文章](https://medium.com/towards-data-science/doping-a-technique-to-test-outlier-detectors-3f6b847ab8d4)中有讨论）。不过，我们将在这里介绍另一种效果较好的方法。这种方法可能过于简单，并不总是像更复杂的技术那样有效，但它确实为这一概念提供了一个简单、直观的介绍。
- en: Here we generate an equal number of synthetic records as there are real records.
    An exactly balanced set isn’t necessary and some imbalance may actually work better
    in some cases, but this example, for simplicity, uses a balanced dataset.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们生成与真实记录数量相等的合成记录。实际上，精确平衡的集合并不是必须的，在某些情况下，稍有不平衡可能反而效果更好，但出于简便考虑，本例使用了平衡的数据集。
- en: 'We generate the synthetic data one row at a time, and for each row, one feature
    at a time. To generate a value, if the feature is categorical, we select a value
    from the real data with a probability proportional to the distribution in the
    real data. For example, if the real data contains a column for Colour and this
    contains 450 rows with Red, 650 rows with Blue, 110 rows with Green, and 385 rows
    with Yellow, then, as fractions these are: Red: 0.28, Blue: 0.41, Green: 0.07,
    Yellow: 0.24\. A set of new values will be created for this column in the synthetic
    data with similar proportions.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们逐行生成合成数据，并且每一行的生成过程都是逐个特征进行的。为了生成一个值，如果特征是类别型的，我们从真实数据中按其在真实数据中的分布概率选择一个值。例如，如果真实数据中有一列“颜色”，其中包含450行红色、650行蓝色、110行绿色和385行黄色，那么按比例计算，这些颜色的比例为：红色：0.28，蓝色：0.41，绿色：0.07，黄色：0.24。因此，合成数据中的这一列将按类似的比例生成新的值。
- en: If the feature is numeric, we calculate the mean and standard deviation of the
    real data for this feature and select a set of random values from a Normal distribution
    with these parameters. Any number of other ways to do this may be considered as
    well, but again, this is a straightforward introduction to the idea.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果特征是数值型的，我们计算该特征在真实数据中的均值和标准差，并从具有这些参数的正态分布中随机选择一组值。还有许多其他方法可以考虑，但同样，这只是一个简单的入门介绍。
- en: Doing this we generate synthetic data where each row is comprised entirely of
    realistic values (each row can potentially contain rare values in categorical
    columns, and potentially rare or extreme values in numeric columns — but they
    are all reasonably realistic values).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们生成了合成数据，其中每一行完全由现实的值组成（每一行可能包含类别列中的稀有值，以及数值列中的稀有或极端值——但它们都是合理且现实的值）。
- en: 'But, the normal relationships between the features are not respected. That
    is: as each column value is generated independently, the combination of values
    generated may be unrealistic. For example if creating synthetic data to mimic
    the Staff table above, we may create fake records that have an Age of 23 and Years
    of Service of 38\. Both values, on their own, are realistic, but the combination
    is nonsensical and, as such, should be an unseen combination in the real data
    — so distinguishable from the real data.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，特征之间的正常关系并未得到遵守。也就是说，由于每个列值是独立生成的，因此生成的值的组合可能不现实。例如，如果创建合成数据以模拟上面的员工表，我们可能会生成假记录，其年龄为23岁，服务年限为38年。单独来看，这两个值是现实的，但它们的组合是没有意义的，因此，在真实数据中应该不会出现这种组合——因此可以与真实数据区分开来。
- en: 'The synthetic data for numeric fields can be created with code (in python)
    such as:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 数值字段的合成数据可以使用如下的代码（Python）生成：
- en: '[PRE0]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, we assume the dataframe real_df contains the real data. We then create
    a second dataframe called synth_df, then combine both into train_df, which can
    be used to train a Random Forest to distinguish the two.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们假设数据框`real_df`包含真实数据。然后我们创建一个名为`synth_df`的第二个数据框，接着将两个数据框合并为`train_df`，该数据框可以用于训练随机森林以区分两者。
- en: 'Categorical data can be created similarly:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 类别数据可以类似地生成：
- en: '[PRE1]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As indicted, this is only one way to generate the data, and it may be useful
    to tune this process, allowing more unusual single values, or restricting to less
    unusual relationships among the features.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这只是生成数据的一种方式，调整这个过程可能会有用，允许更多不寻常的单个值，或限制特征之间不太常见的关系。
- en: Once this data is created, we can train a Random Forest to learn to distinguish
    the real from the fake data.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这些数据被创建，我们可以训练一个随机森林来学习区分真实数据和假数据。
- en: Once this is done, we can actually also perform another form of outlier detection
    as well. Any real records that are passed through the Random Forest, where it
    predicts this record is fake, may be considered anomalous — they are more similar
    to the synthetic data than the real data. This is covered in [Outlier Detection
    in Python](https://www.manning.com/books/outlier-detection-in-python), but for
    this article, we’ll focus on Distance Metric Learning, and so look at the decision
    paths through the trees within the Random Forest (and not the final predictions).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成这个过程，我们实际上还可以执行另一种形式的异常值检测。任何通过随机森林的真实记录，如果它被预测为假记录，可能被认为是异常的——它们与合成数据比与真实数据更相似。这部分内容在[Python中的异常值检测](https://www.manning.com/books/outlier-detection-in-python)中有详细讲解，但在本文中，我们将重点关注距离度量学习，因此我们将关注随机森林中的决策路径（而不是最终的预测）。
- en: Using the Random Forest to Measure Outlierness
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用随机森林衡量异常度
- en: As described above, if two records tend to end in almost entirely different
    leaf nodes, they can be considered different, at least in this sense.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，如果两个记录趋向于最终落在几乎完全不同的叶节点上，它们可以在某种意义上被认为是不同的。
- en: It’s possible to, for each pair of records, count the number of trees within
    the Random Forest where they end in the same leaf node and where they end in different
    leaf nodes. But, there’s also a simpler method we can use. For each record passed
    through the Random Forest, for each tree, we can see what the terminal (leaf)
    node is. We can also see how many records in the training data ended in that node.
    The fewer training records, the more unusual this path is.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一对记录，我们可以统计它们在随机森林中落在相同叶节点和不同叶节点的树木数量。但我们也可以使用一种更简单的方法。对于每个通过随机森林的记录，对于每棵树，我们可以看到它最终的（叶）节点是什么。我们还可以看到训练数据中有多少记录落在该节点上。训练记录越少，这条路径就越不寻常。
- en: If, over most trees, a record ends in the same leaf nodes as very few other
    records, it can be considered anomalous.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在大多数树中，一个记录最终落在与其他很少的记录相同的叶节点上，它可以被认为是异常的。
- en: 'The main idea is: if the Random Forest is accurate, it can distinguish real
    from fake records well. So, when passing a real record through the Random Forest,
    it will likely end in a leaf node associated with the real data. If it is a normal
    real record, it will follow a common path, used by many other real records. At
    each step on the path, the node in the Decision Tree will split on one feature
    — a feature and split point that is effective at separating real from synthetic
    data. A typical record will have a value associated with common real data, so
    will follow the path at each split point associated with real data.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的思路是：如果随机森林准确，它能够很好地区分真实记录和假记录。因此，当一个真实记录通过随机森林时，它很可能会落在与真实数据相关的叶节点上。如果它是一个正常的真实记录，它将沿着一个常见的路径前进，这是许多其他真实记录所使用的路径。在路径的每一步，决策树中的节点会基于一个特征进行分裂——这是一个在区分真实数据和合成数据时有效的特征及其分裂点。一个典型的记录将具有与常见真实数据相关的值，因此会在每个分裂点沿着与真实数据相关的路径前进。
- en: If a Random Forest contained only a small number of trees, the size of the leaf
    node each record ends in could be quite arbitrary. But, Random Forests can be
    set to have hundreds or thousands of trees. Where records consistently end in
    leaf nodes that are unusual for their trees, the record can reasonably be considered
    anomalous.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个随机森林只包含少量的树，则每个记录最终落在的叶节点的大小可能会非常随意。但是，随机森林可以设置为包含数百棵或数千棵树。当记录始终落在其树木中不常见的叶节点时，该记录可以合理地被认为是异常的。
- en: There can still be some variance to the process, even where a large Random Forest
    is used. To address this, instead of using a single Distance Metric Learning outlier
    detector, it’s possible to use several, combined in an ensemble. That’s beyond
    the scope of this article, but the general idea is to create a variety of synthetic
    datasets and for each a variety of Random Forests (with different hyperparameters),
    then average the results together.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 即使使用大型随机森林，过程仍然可能存在一定的变异性。为了解决这个问题，可以使用多个距离度量学习异常检测器，并将它们组合成一个集成方法，而不是仅使用一个单独的检测器。这个方法超出了本文的范围，但其基本思路是创建多个合成数据集，并为每个数据集创建多个不同超参数的随机森林，然后将结果进行平均。
- en: Example
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例
- en: To demonstrate the idea, we’ll create a simple Distance Metric Learning detector.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示这个思路，我们将创建一个简单的距离度量学习检测器。
- en: But first, we’ll create a couple test datasets. These are both numeric datasets
    with two features. As indicated, this is less realistic than data with many features,
    and with a number of categorical features, but it is useful for demonstration
    purposes — it is easy to plot and understand.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，我们将创建几个测试数据集。这些数据集都是具有两个特征的数值型数据集。如所示，这比具有多个特征的数据集和包含多个分类特征的数据集更不具备现实性，但它对于演示目的很有用——它易于绘制和理解。
- en: 'The first test set is a single cluster of data:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个测试集是一个单一的数据集群：
- en: '[PRE2]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The second actually creates the dataset shown at the beginning of the article,
    with four clusters and three points outside of these.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个数据集实际上创建了文章开始时显示的数据集，包含四个集群和三个位于这些集群外的点。
- en: '[PRE3]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The two datasets are shown here:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这里显示了两个数据集：
- en: '![](../Images/e6b94aad5106fc503d3be4cbd01d2314.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e6b94aad5106fc503d3be4cbd01d2314.png)'
- en: We next show a simple outlier detector based on Distance Metric Learning. This
    detector’s fit_predict() method is passed a dataframe (within which we identify
    any outliers). The fit_predict() method generates a synthetic dataset, trains
    and Random Forest, passes each record through the Random Forest, determines which
    node each record ends in, and determines how common these nodes are.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们展示了一个基于距离度量学习的简单异常检测器。该检测器的 `fit_predict()` 方法接受一个数据框（在其中我们识别任何异常值）。`fit_predict()`
    方法生成一个合成数据集，训练一个随机森林，将每条记录传递给随机森林，确定每条记录最终进入哪个节点，并确定这些节点的常见程度。
- en: '[PRE4]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This code example just runs on the data created by create_four_clusters_test_data(),
    but can be called with the data from create_simple_testdata() as well.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这个代码示例仅在 `create_four_clusters_test_data()` 创建的数据上运行，但也可以使用 `create_simple_testdata()`
    中的数据。
- en: 'The results can be visualized with code such as:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 结果可以通过如下代码进行可视化：
- en: '[PRE5]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The results of both test datasets are shown below, drawing the original data,
    but setting the hue by their outlier score (placed in the ‘Scores’ column by the
    code above).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 两个测试数据集的结果如下所示，绘制了原始数据，但根据其异常分数（由上面代码中的“Scores”列放置）设置了色调。
- en: '![](../Images/2c8ab60ba136a8a76e04899883d0fbdb.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2c8ab60ba136a8a76e04899883d0fbdb.png)'
- en: In the dataset on the left, with a single cluster, the outermost points receive
    the highest scores, which is as expected. In the dataset on the right, with four
    clusters, the highest outlier scores go to the three points outside the clusters,
    the smaller clusters, and the points on the edge of the largest clusters. This
    is quite reasonable, though other detectors may score these differently, and likely
    also quite reasonably.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧的数据集（单一集群）中，最外围的点获得了最高的得分，这是预期的结果。在右侧的数据集（包含四个集群）中，最高的异常分数分配给了三个位于集群外的点，较小的集群，以及位于最大集群边缘的点。这是相当合理的，尽管其他检测器可能会对这些点进行不同的评分，而且同样是合理的。
- en: As indicated above, using Euclidean distances can be natural for these datasets,
    though possibly less so for datasets with many features, categorical features,
    associations between features, and other nuances to the data. But, even in these
    simpler cases where Euclidean works quite well, Distance Metric Learning can also
    work well, and provides a natural outlier detection method. Working with more
    complex data, this can be the case even more so.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，使用欧几里得距离对于这些数据集来说是自然的，尽管对于具有许多特征、分类特征、特征之间关联以及其他数据细节的数据集可能不那么适用。但即使在这些欧几里得距离表现良好的简单情况中，距离度量学习也能发挥良好的效果，并提供一种自然的异常检测方法。对于更复杂的数据，这种情况可能会更加明显。
- en: Conclusions
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Distance Metric Learning can be used for many purposes outside of outlier detection,
    and even within outlier detection, can be used a variety of ways. For example,
    it’s possible to use a Random Forest as above to calculate the pairwise distances
    in a dataset and pass these to another algorithm. DBSCAN, for example, provides
    a ‘precomputed’ option, which allows passing a pre-calculated matrix of pairwise
    distances; it’s then possible to use DBSCAN (or a similar clustering method, such
    as HDBSCAN) for one of several possible clustering-based outlier detection algorithms.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 距离度量学习可以用于离群点检测以外的许多用途，甚至在离群点检测中，也可以以多种方式使用。例如，可以像上面一样使用随机森林计算数据集中的成对距离，并将这些距离传递给另一个算法。例如，DBSCAN提供了一个“预计算”选项，可以传递预先计算好的成对距离矩阵；然后，可以使用DBSCAN（或类似的聚类方法，如HDBSCAN）作为几种可能的基于聚类的离群点检测算法之一。
- en: And, Distance Metric Learning can also be used, as in this article, in a more
    direct way, which is an excellent outlier detection method in itself. In many
    cases, it can be favorable for detecting outliers than methods based on Euclidean,
    Manhattan, Gower, or other such distance metrics. It can also provide diversity
    to an ensemble of detectors, even where these methods also work well.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 并且，距离度量学习也可以像本文中所示的那样以更直接的方式使用，这本身就是一种出色的离群点检测方法。在许多情况下，它比基于欧几里得距离、曼哈顿距离、Gower距离或其他类似距离度量的方法更有利于检测离群点。它还可以为检测器的集成提供多样性，即使这些方法也能很好地工作。
- en: No outlier detection method is definitive, and it’s generally necessary to use
    multiple outlier detection methods on any given project (including, often, the
    same method multiple times, using different parameters), combining their results
    to achieve strong overall outlier detection.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 没有任何一种离群点检测方法是绝对的，通常需要在任何给定项目中使用多种离群点检测方法（包括通常情况下，使用相同方法多次，但使用不同的参数），将它们的结果结合起来，以实现强大的整体离群点检测。
- en: So, Distance Metric Learning won’t work for every project and where it does
    it may (as with any detector) perform best when combined with other detectors.
    But, this is a valuable tool; Distance Metric Learning can be a very effective
    technique for outlier detection, though it receives less attention than other
    methods.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，距离度量学习并不适用于每个项目，但在适用的情况下，它可能（与任何检测器一样）在与其他检测器结合时表现最好。但这是一种有价值的工具；距离度量学习可以是一个非常有效的离群点检测技术，尽管它比其他方法获得的关注要少。
- en: It does require some tuning, both in terms of how the synthetic data is produced
    and in terms of the hyper-parameters used by the Random Forest, but once tuned,
    provides a strong and intuitive outlier detection method.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这确实需要一些调优，包括合成数据的生成方式和随机森林使用的超参数，但一旦调优完成，它提供了一种强大且直观的离群点检测方法。
- en: All images by the author
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 所有图片均由作者提供
