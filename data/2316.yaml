- en: Embeddings Are Kind of Shallow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 嵌入是有点“浅”的
- en: 原文：[https://towardsdatascience.com/embeddings-are-kind-of-shallow-727076637ed5?source=collection_archive---------1-----------------------#2024-09-23](https://towardsdatascience.com/embeddings-are-kind-of-shallow-727076637ed5?source=collection_archive---------1-----------------------#2024-09-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/embeddings-are-kind-of-shallow-727076637ed5?source=collection_archive---------1-----------------------#2024-09-23](https://towardsdatascience.com/embeddings-are-kind-of-shallow-727076637ed5?source=collection_archive---------1-----------------------#2024-09-23)
- en: What I learned doing semantic search on U.S. Presidents with four language model
    embeddings
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我通过四种语言模型嵌入进行美国总统语义搜索时学到的东西
- en: '[](https://medium.com/@nathanbos?source=post_page---byline--727076637ed5--------------------------------)[![Nathan
    Bos, Ph.D.](../Images/72d3395ba9f1587b38e609ffcb4c2066.png)](https://medium.com/@nathanbos?source=post_page---byline--727076637ed5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--727076637ed5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--727076637ed5--------------------------------)
    [Nathan Bos, Ph.D.](https://medium.com/@nathanbos?source=post_page---byline--727076637ed5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@nathanbos?source=post_page---byline--727076637ed5--------------------------------)[![Nathan
    Bos, Ph.D.](../Images/72d3395ba9f1587b38e609ffcb4c2066.png)](https://medium.com/@nathanbos?source=post_page---byline--727076637ed5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--727076637ed5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--727076637ed5--------------------------------)
    [Nathan Bos, Ph.D.](https://medium.com/@nathanbos?source=post_page---byline--727076637ed5--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--727076637ed5--------------------------------)
    ·25 min read·Sep 23, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--727076637ed5--------------------------------)
    ·25分钟阅读·2024年9月23日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/7dc01b5e0b5f46a47640f0ec5bcb121b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7dc01b5e0b5f46a47640f0ec5bcb121b.png)'
- en: All photos in this article are from WikiCommons and are either public domain
    or licensed for commercial use.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中的所有照片均来自WikiCommons，且要么是公共领域的，要么是已授权用于商业用途的。
- en: 'I’m interested in trying to figure out what’s inside a language model embedding.
    You should be too, if one if these applies to you:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我有兴趣尝试弄清楚语言模型嵌入（embedding）内部的内容。如果以下任意一条适用于你，你也应该感兴趣：
- en: · The “thought processes” of large language models (LLMs) intrigues you.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: · 大型语言模型（LLMs）的“思维过程”令你感到兴趣。
- en: · You build data-driven LLM systems, (especially Retrieval Augmented Generation
    systems) or would like to.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: · 你构建基于数据驱动的LLM系统（特别是检索增强生成系统），或者你有此兴趣。
- en: · You plan to use LLMs in the future for research (formal or informal).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: · 你计划未来将LLMs用于研究（正式或非正式）。
- en: · The idea of a brand new type of language representation intrigues you.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: · 一种全新类型的语言表示方法令你感到兴趣。
- en: 'This blog post is intended to be understandable to any curious person, but
    even if you are language model specialist who works with them daily I think you
    will learn some useful things, as I did. Here’s a scorecard summary of what I
    learned about Language Model embeddings by performing semantic searches with them:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本文旨在让任何好奇的人都能理解，但即使你是每天都在与语言模型打交道的专家，我认为你也会学到一些有用的知识，就像我一样。以下是我通过进行语义搜索所学到的关于语言模型嵌入的一些要点总结：
- en: The Scorecard
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 成绩单
- en: What do embeddings “see” well enough to find passages in a larger dataset?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入在较大的数据集中，能够“看见”哪些内容，从而找到相关的段落？
- en: '![](../Images/c76e273f4a84748a3725f8cf30a930a0.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c76e273f4a84748a3725f8cf30a930a0.png)'
- en: Along with many people, I have been fascinated by recent progress trying to
    look inside the ‘Black Box’ of large language models. There have recently been
    some incredible breakthroughs in understanding the inner workings of language
    models. Here are examples of this work by [Anthropic](https://www.anthropic.com/news/mapping-mind-language-model),
    [Google](https://deepmind.google/discover/blog/gemma-scope-helping-the-safety-community-shed-light-on-the-inner-workings-of-language-models/),
    and a nice review ([Rai et al. 2024](https://arxiv.org/abs/2407.02646)).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 和很多人一样，我一直对近期在尝试揭开大型语言模型“黑箱”的进展感到着迷。最近，在理解语言模型内部工作机制方面出现了一些令人难以置信的突破。以下是[Anthropic](https://www.anthropic.com/news/mapping-mind-language-model)、[Google](https://deepmind.google/discover/blog/gemma-scope-helping-the-safety-community-shed-light-on-the-inner-workings-of-language-models/)以及一篇很好的综述文章（[Rai
    等，2024](https://arxiv.org/abs/2407.02646)）的相关工作示例。
- en: This exploration has similar goals, but we are studying embeddings, not full
    language models, and restricted to ‘black box’ inference from question responses,
    which is probably still the single best interpretability method.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这项探索有类似的目标，但我们研究的是嵌入，而不是完整的语言模型，并且限制于从问题回答中进行“黑盒”推理，这可能仍然是目前最好的可解释性方法。
- en: Embeddings are what are created by LLMs in the first step, when they take a
    chunk of text and turn it into a long string of numbers that the language model
    networks can understand and use. Embeddings are used in Retrieval Augmented Generation
    (RAG) systems to allow searching on semantics (meanings) than are deeper than
    keyword-only searches. A set of texts, in my case the Wikipedia entries on U.S.
    Presidents, is broken into small chunks of text and converted to these numerical
    embeddings, then saved in a database. When a user asks a question, that question
    is also converted to embeddings. The RAG system then searches the database for
    an embedding similar to the user query, using a simple mathematical comparison
    between vectors, usually a cosine similarity. This is the ‘retrieval’ step, and
    the example code I provide ends there. In a full RAG system, whichever most-similar
    text chunks are retrieved from the database are then given to an LLM to use them
    as ‘context’ for answering the original question.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是LLM在第一步中创建的，当它们将一块文本转换为语言模型网络可以理解和使用的长串数字时。嵌入用于检索增强生成（RAG）系统中，允许在语义（含义）上进行搜索，而不仅仅是关键字搜索。一个文本集，在我的例子中是关于美国总统的维基百科条目，被分成小块文本并转换为这些数值嵌入，然后保存在数据库中。当用户提出问题时，该问题也会被转换为嵌入。然后，RAG系统通过简单的数学比较（通常是余弦相似度）在数据库中搜索与用户查询相似的嵌入。这是“检索”步骤，我提供的示例代码到此为止。在完整的RAG系统中，从数据库中检索到的最相似的文本块将被传递给LLM，用作回答原始问题的“上下文”。
- en: If you work with RAGs, you know there are many design variants of this basic
    process. One of the design choices is choosing a specific embedding model among
    the many available. Some models are longer, trained on more data, and cost more
    money, but without an understanding of what they are like and how they differ,
    the choice of which to use is often guesswork. How much do they differ, really?
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用RAG系统，你知道这个基本过程有许多设计变体。一个设计选择是从众多可用的嵌入模型中选择一个特定的模型。有些模型更大，训练的数据更多，费用也更高，但如果不了解它们的特点以及它们的区别，选择使用哪个模型往往是凭猜测。它们到底有多大的差异呢？
- en: If you don’t care about the RAG part
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如果你不关心RAG部分
- en: 'If you do not care about RAG systems but are just interested in learning more
    conceptually about how language models work, you might skip to the questions.
    Here is the upshot: embeddings encapsulate interesting data, information, knowledge,
    and maybe even wisdom gleaned from text, but neither their designers nor users
    knows exactly what they capture and what they miss. This post will search for
    information with different embeddings to try to understand what is inside them,
    and what is not.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不关心RAG系统，只是对语言模型如何工作有概念性的兴趣，你可以跳到问题部分。总结如下：嵌入封装了从文本中提取的有趣数据、信息、知识，甚至可能是智慧，但无论是设计者还是用户都不知道它们究竟捕捉到了什么，遗漏了什么。本文将使用不同的嵌入来搜索信息，试图理解它们内部包含了什么，以及缺少了什么。
- en: 'The technical details: data, embeddings and chunk size'
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术细节：数据、嵌入和块大小
- en: The dataset I’m using contains Wikipedia entries about U.S. Presidents. I use
    LlamaIndex for creating and searching a vector database of these text entries.
    I used a smaller than usual chunk size, 128 tokens, because larger chunks tend
    to overlay more content and I wanted a clean test of the system’s ability to find
    semantic matches. (I also tested chunk size 512 and results on most tests were
    similar.)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用的数据集包含关于美国总统的维基百科条目。我使用LlamaIndex来创建并搜索这些文本条目的向量数据库。我使用了比平常更小的块大小，即128个标记，因为较大的块往往会叠加更多内容，而我希望清楚地测试系统在寻找语义匹配时的能力。（我也测试了块大小为512的情况，大多数测试结果相似。）
- en: 'I’ll tests four embeddings:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我将测试四种嵌入：
- en: 1\. **BGE** (bge-small-en-v1.5) is quite small at length 384\. It the smallest
    of a line of BGE’s developed by the Beijing Academy of Artificial Intelligence.
    For it’s size, it does well on benchmark tests of retrieval (see [leaderboard](https://huggingface.co/spaces/mteb/leaderboard)).
    It is F=free to use from HuggingFace.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. **BGE** (bge-small-en-v1.5) 在长度为384时相当小。它是北京人工智能研究院开发的一系列BGE模型中最小的。就其大小而言，它在检索的基准测试中表现良好（见[排行榜](https://huggingface.co/spaces/mteb/leaderboard)）。它是F=免费使用的，可以在HuggingFace上使用。
- en: 2\. **ST** (all-MiniLM-L6-v2) is another 384-length embedding. It excels at
    sentence comparisons; I’ve used it before for judging transcription accuracy.
    It was trained on the first billion sentence-pair corpus, which was about half
    Reddit data. It is also available HuggingFace.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 2. **ST**（all-MiniLM-L6-v2）是另一种384长度的嵌入。它在句子比较方面表现出色；我曾用它来评估转录准确性。它是在第一个十亿句对语料库上训练的，这大约一半来自Reddit数据。它也可以在HuggingFace上使用。
- en: '![](../Images/a24cfcba86c4b9ecf79fc7463319f270.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a24cfcba86c4b9ecf79fc7463319f270.png)'
- en: Graphics by the author, using Leonardo.ai
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 作者制作的图形，使用Leonardo.ai
- en: 3\. **Ada** (text-embedding-ada-002) is the embedding scheme that OpenAI used
    from GPT-2 through GPT-4\. It is much longer than the other embeddings at length
    1536, but it is also older. How well can it compete with newer models?
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 3. **Ada**（text-embedding-ada-002）是OpenAI从GPT-2到GPT-4使用的嵌入方案。它比其他嵌入更长，长度为1536，但它也较老。它能与更新的模型竞争吗？
- en: 4\. **Large (**text-embedding-3-large) is Ada’s replacement — newer, longer,
    trained on more data, more expensive. We’ll use it with the max length of 3,072\.
    Is it worth the extra cost and computing power? Let’s find out.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 4. **Large**（text-embedding-3-large）是Ada的替代方案——更新、更长，基于更多数据进行训练，更加昂贵。我们将以最大长度3,072来使用它。它值得额外的成本和计算力吗？让我们一探究竟。
- en: Questions, code available on GitHub
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题和代码可以在GitHub上找到
- en: 'There is spreadsheet of question responses, a Jupyter notebook, and text dataset
    of Presidential Wikipedia entries available here:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个问题响应的电子表格、一个Jupyter笔记本和总统维基百科条目的文本数据集：
- en: '[](https://github.com/nathanbos/blog_embeddings?source=post_page-----727076637ed5--------------------------------)
    [## GitHub - nathanbos/blog_embeddings: Files to accompany Medium blog on embeddings'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/nathanbos/blog_embeddings?source=post_page-----727076637ed5--------------------------------)
    [## GitHub - nathanbos/blog_embeddings: 与Medium博客有关的嵌入文件'
- en: Files to accompany Medium blog on embeddings. Contribute to nathanbos/blog_embeddings
    development by creating an…
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 与Medium博客有关的嵌入文件。通过创建一个…参与nathanbos/blog_embeddings的开发。
- en: github.com](https://github.com/nathanbos/blog_embeddings?source=post_page-----727076637ed5--------------------------------)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/nathanbos/blog_embeddings?source=post_page-----727076637ed5--------------------------------)
- en: Download the text and Jupyter notebook if you want to build your own; mine runs
    well on Google Colab.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想构建自己的模型，可以下载文本和Jupyter笔记本；我的在Google Colab上运行良好。
- en: The Spreadsheet of questions
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题的电子表格
- en: 'I recommend downloading the [spreadsheet](https://github.com/nathanbos/blog_embeddings/blob/main/Presidential_RAG_queries.xlsx)
    to understand these results. It shows the top 20 text chunks returned for each
    question, plus a number of variants and follow-ups. Follow the link and choose
    ‘Download’ like this:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我推荐下载[电子表格](https://github.com/nathanbos/blog_embeddings/blob/main/Presidential_RAG_queries.xlsx)以便理解这些结果。它显示了每个问题返回的前20个文本片段，以及一些变体和后续内容。点击链接并选择“下载”，就像这样：
- en: '![](../Images/c3b7f29dc12c95644a189f8d8f6b7bff.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3b7f29dc12c95644a189f8d8f6b7bff.png)'
- en: Screenshot by the author
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 作者截图
- en: To browse the questions and responses, I find it easiest to drag the text entry
    cell at the top larger, and tab through the responses to read the text chunks
    there, as in this screenshot.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了浏览问题和回答，我发现最容易的方式是将顶部的文本输入单元格拖大，并通过标签切换来阅读响应中的文本片段，就像这张截图一样。
- en: '![](../Images/7e037d6f1c73d922efc52d55006771d4.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e037d6f1c73d922efc52d55006771d4.png)'
- en: Screenshot by the author
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 作者截图
- en: Not that this is the retrieved context only, there is no LLM synthesized response
    to these questions. The code has instructions for how to get those, using a query
    engine instead of just a retriever as I did.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这仅是检索到的上下文，并没有LLM合成的响应。代码包含了如何获取这些响应的说明，使用查询引擎而不是像我那样仅使用检索器。
- en: '**Providing understanding that goes beyond leaderboards**'
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**提供超越排行榜的理解**'
- en: 'We’re going to do something countercultural in this post: we’re going to focus
    on the actual results of individual question responses. This stands in contrast
    to current trends in LLM evaluation, which are about using larger and larger datasets
    and and presenting results aggregated to a higher and higher level. Corpus size
    matters a lot for training, but that is not as true for evaluation, especially
    if the goal is human understanding.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将做一些反潮流的事情：我们将专注于单个问题响应的实际结果。这与当前LLM评估的趋势形成对比，后者更注重使用越来越大的数据集，并呈现更高层次的汇总结果。语料库大小对训练非常重要，但对于评估来说，尤其是当目标是理解时，情况并非如此。
- en: 'For aggregated evaluation of embedding search performance, consult the (very
    well implemented) HuggingFace leaderboard using the (excellent) MTEB dataset:
    [https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 要对嵌入式搜索性能进行汇总评估，请参考使用（非常出色的）MTEB数据集的（非常完善的）HuggingFace排行榜：[https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)。
- en: Leaderboards are great for comparing performance broadly, but are not great
    for developing useful understanding. Most leaderboards do not publish actual question-by-question
    results, limiting what can be understood about those results. (They do usually
    provide code to re-run the tests yourself.) Leaderboards also tend to focus on
    tests that are roughly within the current technology’s abilities, which is reasonable
    if the goal is to compare current models, but does not help understand the limits
    of the state of the art. To develop usable understanding about what systems can
    and cannot do, I find there is no substitute for back-and-forth testing and close
    analysis of results.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 排行榜非常适合广泛比较性能，但并不利于发展有用的理解。大多数排行榜不会发布实际的逐题结果，从而限制了对这些结果的理解。（它们通常会提供代码，供你自己重新运行测试。）排行榜也往往专注于当前技术能力范围内的测试，这样做在比较当前模型时是合理的，但无法帮助我们理解最前沿技术的局限性。为了发展对系统能做和不能做的可用理解，我发现没有什么能替代反复测试和对结果的深入分析。
- en: What I’m presenting here is basically a pilot study. The next step would be
    to do the work of developing larger, precisely designed, understanding-focused
    test sets, then conduct iterative tests focused on deeper understanding of performance.
    This kind of study will likely only happen at scale when funding agencies and
    academic disciplines beyond computer science start caring about LLM interpretability.
    In the meantime, you can learn a lot just by asking.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里展示的基本上是一个初步研究。下一步将是开发更大、更精确设计、以理解为重点的测试集，然后进行以更深理解性能为目标的迭代测试。只有当资助机构和计算机科学以外的学术领域开始关注大语言模型的可解释性时，这种类型的研究才可能在大规模上进行。在此期间，通过提问你也可以学到很多东西。
- en: '**Question: Which U.S. Presidents served in the Navy?**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题：哪些美国总统曾在海军服役？**'
- en: Let’s use the first question in my test set to illustrate the ‘black box’ method
    of using search to aid understanding.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用我测试集中的第一个问题来说明使用搜索辅助理解的“黑箱”方法。
- en: '![](../Images/90b1c21c694e0277446dba4eb70b8a79.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90b1c21c694e0277446dba4eb70b8a79.png)'
- en: graphics by the author, using Leonardo.ai
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图形由作者制作，使用Leonardo.ai
- en: '![](../Images/15794584bb1c3cfe8a5e90786e506b35.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15794584bb1c3cfe8a5e90786e506b35.png)'
- en: Graphics by the author, using Leonardo.ai
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图形由作者制作，使用Leonardo.ai
- en: '![](../Images/31f234787dc97d177e79c628e6df25d8.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31f234787dc97d177e79c628e6df25d8.png)'
- en: Animated graphics by the author, using Leonardo.ai. Presidential portraits from
    WikiCommons, public domain or commercial license.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 动画图形由作者制作，使用Leonardo.ai。总统肖像来自WikiCommons，公有领域或商业许可。
- en: '**The results:**'
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**结果：**'
- en: I gave the Navy question to each embedding index (database). Only one of the
    four embeddings, Large, was able to find all six Presidents who served in the
    Navy within the top ten hits. The table below shows the top 10 found passages
    from for each embedding model. See the spreadsheet for full text of the top 20\.
    There are duplicate Presidents on the list, because each Wikipedia entry has been
    divided into many individual chunks, and any given search may find more than one
    from the same President.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我把海军问题给了每个嵌入索引（数据库）。在四个嵌入中，只有一个名为Large的嵌入能够在前十个搜索结果中找到所有六位曾在海军服役的总统。下表展示了每个嵌入模型找到的前10条段落。完整的前20条内容请参见电子表格。列表中有重复的总统，因为每个维基百科条目都被划分成了许多单独的部分，任何给定的搜索可能会找到同一位总统的多个条目。
- en: '![](../Images/9715322bb33ceb4d01df2231cf5b678a.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9715322bb33ceb4d01df2231cf5b678a.png)'
- en: Why were there so many incorrect hits? Let’s look at a few.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么会有这么多错误的结果？我们来看几个例子。
- en: The first false hit from BGE is a chunk from Dwight D Eisenhower, an army general
    in WW2, that has a lot of military content but has nothing to do with the Navy.
    It appears that BGE does have some kind of semantic representation of ‘Navy’.
    BGE’s search was better than what you would get with a simple keyword matches
    on ‘Navy’, because it generalizes to other words that mean something similar.
    But it generalized too indiscriminately, and failed to differentiate Navy from
    general military topics, e.g. it does not consistently distinguish between the
    Navy and the Army. My friends in Annapolis would not be happy.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: BGE的第一个错误匹配是来自德怀特·D·艾森豪威尔的片段，他是二战中的一位陆军将军，内容包含很多军事信息，但与海军无关。看来BGE确实有某种形式的‘海军’语义表示。BGE的搜索比简单的‘海军’关键词匹配要好，因为它可以推广到其他意思相近的词汇。但是它过于泛化，未能区分海军与一般军事话题，例如，它没有始终如一地区分海军与陆军。我在安纳波利斯的朋友们可不会喜欢这个。
- en: How did the two mid-level embedding models do? They seem to be clear on the
    Navy concept and can distinguish between the Navy and Army. But they each had
    many false hits on general naval topics; a section on Chester A Arthur’s naval
    modernization efforts shows up high on both lists. Other found sections have Presidential
    actions related to the Navy, or ships named after Presidents, like the U.S.S.
    Harry Truman.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这两个中级嵌入模型表现如何呢？它们似乎对‘海军’这一概念有清晰的理解，能够区分海军与陆军。但它们在一些一般性的海军话题上出现了许多错误匹配；例如，关于切斯特·A·阿瑟的海军现代化努力的部分在两份列表中都排得很高。其他找到的部分涉及总统与海军相关的行动，或者以总统命名的舰船，如U.S.S.
    Harry Truman号。
- en: The middle two embedding models seem to have a way to semantically represent
    ‘Navy’ but do not have a clear semantic representation of the concept ‘Served
    in the Navy’. This was enough to prevent either ST or Ada from finding all six
    Naval-serving Presidents in the top ten.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 中间的两个嵌入模型似乎能够在语义上表示‘海军’，但没有清晰地表示‘服役于海军’这一概念。这足以使得ST和Ada都未能在前十名中找到所有六位曾在海军服役的总统。
- en: 'On this question, Large clearly outperforms the others, with six of the seven
    top hits corresponding to the six serving Presidents: Gerald Ford, Richard Nixon,
    Lyndon B. Johnson, Jimmy Carter, John F. Kennedy, and George H. W. Bush. Large
    appears to understand not just ‘Navy’ but ‘served in the Navy’.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个问题上，Large显然表现优于其他模型，其前七名中的六个都对应了六位曾服役的总统：杰拉尔德·福特、理查德·尼克松、林登·B·约翰逊、吉米·卡特、约翰·F·肯尼迪和乔治·H·W·布什。Large似乎不仅理解‘海军’，还理解‘服役于海军’。
- en: '**What did Large get wrong?**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**Large错在哪里？**'
- en: 'What was the one mistake in Large? It was the chunk on Franklin Delano Roosevelt’s
    work as Assistant Secretary of the Navy. In this capacity, he was working for
    the Navy, but as a civilian employee, not in the Navy. I know from personal experience
    that the distinction between active duty and civilian employees can be confusing.
    The first time I did contract work for the military I was unclear on which of
    my colleagues were active duty versus civilian employees. A colleagues told me,
    in his very respectful military way, that this distinction was important, and
    I needed to get it straight, which I have since. (Another pro tip: don’t get the
    ranks confused.)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Large中的一个错误是什么？问题出在富兰克林·德拉诺·罗斯福作为海军助理部长的工作。在这个职务上，他是为海军工作的，但作为文职员工，而不是海军军人。我从个人经验中知道，现役军人与文职员工之间的区别有时会让人困惑。我第一次为军方做合同工作时，不清楚哪些同事是现役军人，哪些是文职员工。一位同事以他非常尊重的军事方式告诉我，这个区别非常重要，我需要弄清楚，后来我确实弄清楚了。（另一个小贴士：不要搞混军衔。）
- en: '**Question: Which U.S. Presidents worked as a civilian employees of the Navy?**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题：哪些美国总统曾作为海军的文职员工工作？**'
- en: 'In this question I probed to see whether the embeddings “understood” this distinction
    that I had at first missed: do they know how civilian employees of the Navy differs
    from people actually in the service? Both Roosevelts worked for the Navy in a
    civilian capacity. Theodore had also been in the Army (leading the charge of San
    Juan Hill), wrote books about the Navy, and built up the Navy as President, so
    there are many Navy-related chunks about TR, but he was never in the Navy. (Except
    as Commander in Chief; this role technically makes all Presidents part of the
    U.S. Navy, but that relationship did not affect search hits.)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个问题中，我探讨了嵌入模型是否“理解”我最初忽视的这个区别：它们是否知道海军的文职员工和实际服役人员有何不同？罗斯福家族的两位成员都曾在海军担任文职工作。西奥多·罗斯福还曾在陆军服役（领导圣胡安山之战），写过关于海军的书，并在担任总统期间发展了海军，因此关于西奥多·罗斯福的海军相关信息很多，但他从未在海军服役。（除了作为三军统帅；这一角色使所有总统在技术上都算作美国海军的一部分，但这个关系并未影响搜索结果。）
- en: The results of the civilian employee query can be seen in the results spreadsheet.
    The first hit for Large and second for Ada is a passage describing some of FDR’s
    work in the Navy, but this was partly luck because it included the word ‘civilian’
    in a different context. Mentions were made of staff work by LBJ and Nixon, although
    it is clear from the passages that they were active duty at the time. (Some staff
    jobs can be filled by either military or civilian appointees.) Mention of Teddy
    Roosevelt’s civilian staff work did not show up at all, which would prevent an
    LLM from correctly answering the question based on these hits.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: “文职员工”查询的结果可以在结果表格中看到。Large 的第一次命中和 Ada 的第二次命中是描述 FDR 在海军工作的一段文字，但这部分内容有些运气成分，因为它包含了“文职”一词，且用法与此问题不同。提到
    LBJ 和尼克松的工作人员工作，尽管从段落内容来看，他们当时是现役。 （一些工作人员职位可以由军事或文职人员担任。）没有提到西奥多·罗斯福的文职工作，这使得基于这些搜索结果的
    LLM 无法正确回答问题。
- en: Overall there were only minor difference between the searches for Navy, “In
    the Navy” and “civilian employee”. Asking directly about active-duty Navy gave
    similar results. The larger embedding models had some correct associations, but
    overall could not make the necessary distinction well enough to answer the question.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来看，“海军”、“《海军进行曲》和‘文职员工’这几个搜索之间只有细微的差异。直接询问现役海军也得到了类似的结果。较大的嵌入模型有一些正确的关联，但总体而言，它们无法充分区分这些概念，因此无法准确回答问题。
- en: '![](../Images/2489b9f5fe74be8e7753cf0398270726.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2489b9f5fe74be8e7753cf0398270726.png)'
- en: graphics by the author, using Leonardo.ai
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图表由作者提供，使用 Leonardo.ai 绘制
- en: Common Concepts
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见概念
- en: '**Question: Which U.S. Presidents were U.S. Senators before they were President?**'
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**问题：哪位美国总统在担任总统之前曾是美国参议员？**'
- en: All of the vectors seem to generally understand common concepts like this, and
    can give good results that an LLM could turn into an accurate response. The embeddings
    could also differentiate between the U.S. Senate and U.S. House of Representatives.
    They were clear on the difference between Vice President and President, the difference
    between a lawyer and a judge, and the general concept of an elected representative.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 所有向量似乎普遍理解像这样的常见概念，并能够给出良好的结果，LLM 可以根据这些结果生成准确的回答。嵌入模型还能够区分美国参议院和美国众议院。他们对副总统和总统的区别非常清晰，也能区分律师和法官之间的差异，并理解当选代表的基本概念。
- en: They also all did well when asked about Presidents who were artists, musicians,
    or poker players. They struggled a little with ‘author’ because there were so
    many false positives in the data relate to other authors.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当被问及哪些总统是艺术家、音乐家或扑克牌玩家时，它们的表现也不错。当涉及到“作家”时，由于与其他作家的数据中存在大量的错误匹配，它们的表现稍有困难。
- en: More Specialized Concepts
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更专业的概念
- en: As we saw, they each have their representational limits, which for Large was
    the concept of ‘civilian employee of the Navy.’ They also all did poorly on the
    distinction between national and state representatives.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，每个模型都有其表现极限，对 Large 模型来说，“海军的文职员工”就是其中的一个局限。它们在区分国家代表和州级代表方面也表现不佳。
- en: '**Question: Which U.S. President served as elected representatives at the state
    level?**'
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**问题：哪位美国总统曾在州级担任选举代表？**'
- en: 'None of the models returned all, or even most of the Presidents who served
    in state legislatures. All of the models mostly returned hits relate to the U.S.
    House of Representatives, with some references to states or governors. Large’s
    first hit was on target: “Polk was elected to its state legislature in 1823”,
    but missed the rest. This topic could use some more probing, but in general this
    concept was a fail.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 没有任何模型返回了所有，甚至大多数在州立立法机关服役的总统。所有模型主要返回的结果都与美国众议院相关，并且有一些提到了州或州长。Large的第一个命中是准确的：“波尔克于1823年当选为州立立法机关的成员”，但错过了其余的内容。这个话题可能需要更多的探讨，但总体而言，这个概念是失败的。
- en: '**Question: Which US Presidents were not born in a US State?**'
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**问题：哪些美国总统不是出生在美国的州内？**'
- en: All four embeddings returned Barack Obama as one of the top hits to this question.
    This is not factual — Hawaii was a state in 1961 when Obama was born there, but
    the misinformation is prevalent enough (thanks, Donald) to show up in the encoding.
    The Presidents who were born outside of the United States were the early ones,
    e.g. George Washington, because Virginia was not a state when he was born. This
    implied fact was not accessible via the embeddings. William Henry Harrison was
    returned in all cases, because his entry includes the passage “…he became the
    last United States president not born as an American citizen”, but none of the
    earlier President entries said this directly, so it was not found in the searches.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 所有四种嵌入方式都把巴拉克·奥巴马作为这个问题的顶级答案之一返回。这并不准确——奥巴马于1961年出生在夏威夷，当时夏威夷已是美国的一州，但这种错误信息广泛传播（谢谢你，唐纳德），以至于出现在编码中。那些出生在美国以外的总统大多是早期的总统，例如乔治·华盛顿，因为他出生时弗吉尼亚州还不是一个州。这个隐含的事实在嵌入中无法获取。威廉·亨利·哈里森在所有情况下都被返回，因为他的条目包括了这样一段话：“……他成为了最后一位没有出生为美国公民的美国总统”，但早期总统的条目中并没有直接提到这一点，因此在搜索中没有找到。
- en: Search for specific, semi-famous people and places
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 搜索特定的、半知名的人物和地点
- en: '**Question: Which U.S. Presidents were asked to deliver a difficult message
    to John Sununu?**'
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**问题：哪些美国总统曾被要求向约翰·苏努努传达一个困难的信息？**'
- en: '![](../Images/333e46db05b348caf762f22e21e9bb0c.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/333e46db05b348caf762f22e21e9bb0c.png)'
- en: John Sununu, From WikiCommons. Photo by Michael Vadon
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 约翰·苏努努，来自WikiCommons。照片由迈克尔·瓦东提供
- en: 'People who are old enough to have followed U.S. politics in the 1990s will
    remember this distinctive name: John Sununu was governor of New Hampshire, was
    a somewhat prominent political figure, and served as George H.W. Bush’s (Bush
    #1''s) chief of staff. But he isn’t mentioned in Bush #1’s entry. He is mentioned
    in a quirky offhand anecdote in the entry for George W. Bush (Bush #2) where Bush
    #1 asked Bush #2 to ask Sununu to resign. This was mentioned, I think, to illustrate
    one of Bush #2’s key strengths, likability, and the relationship between the two
    Bushes. A search for John Sununu, which would have been easy for a keyword search
    due to the unique name, fails to find this passage in three of the four embeddings.
    The one winner? Surprisingly, it is BGE, the underdog.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 那些在1990年代曾关注过美国政治的人，应该还记得这个独特的名字：约翰·苏努努曾是新罕布什尔州的州长，是一个颇有影响力的政治人物，并且曾担任乔治·H·W·布什（布什#1）总统的幕僚长。然而，他并未在布什#1的条目中被提及。他在乔治·W·布什（布什#2）的条目中被提到，作为一个有趣的插曲，提到布什#1曾要求布什#2让苏努努辞职。我认为提到这个，是为了说明布什#2的一个关键优点——亲和力，以及两位布什之间的关系。对约翰·苏努努的搜索本应因为其独特的名字而轻松找到这一段，但在四个嵌入方式中的三个都未能找到这一段。而唯一正确的结果？令人惊讶的是，是BGE，这个冷门模型。
- en: 'There was another interesting pattern: Large returned a number of hits on Bush
    #1, the President historically most associated with Sununu, even though he is
    never mentioned in the returned passages. This seems more than a coincidence;
    the embedding encoded some kind of association between Sununu and Bush #1 beyond
    what is stated in the text.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个有趣的模式：Large返回了许多关于布什#1的结果，这位总统历史上与约翰·苏努努有着最深的关联，尽管在返回的段落中他从未被提及。这似乎不仅仅是巧合；嵌入编码了一种关于苏努努与布什#1之间的某种关联，超出了文本中所述内容。
- en: Which U.S. Presidents were criticized by Helen Prejean?
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 哪些美国总统曾被海伦·普雷尚批评过？
- en: '![](../Images/dc4b4986244421ea729236febd59d1f4.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc4b4986244421ea729236febd59d1f4.png)'
- en: Sister Helen Prejean, from WikiCommons. Photo by [Don LaVange](https://www.flickr.com/people/26667277@N00)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 海伦·普雷尚修女，来自WikiCommons。照片由[唐·拉范吉](https://www.flickr.com/people/26667277@N00)提供
- en: 'I observed the same thing with a second semi-famous name: Sister Helen Prejean
    was a moderately well-known critic of the death penalty; she wrote *Dead Man Walking*
    and Wikipedia briefly notes that she criticized Bush #2’s policies. None of the
    embeddings were able to find the Helen Prejean mention which, again, a keyword
    search would have found easily. Several of Large’s top hits are passages related
    to the death penalty, which seems like more than a coincidence. As with Sununu,
    Large appears to have some association with the name, even though it is not represented
    clearly enough in the embedding vocabulary to do an effective search for it.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我在第二个半知名的名字上观察到了同样的现象：海伦·普雷让修女（Sister Helen Prejean）是死刑的温和批评者；她写了*Dead Man Walking*，而维基百科简要提到她批评了布什二号的政策。没有一个嵌入能够找到海伦·普雷让的相关提及，而这类关键词搜索会轻松找到。大型模型的几个最重要的结果是与死刑相关的段落，这似乎不完全是巧合。像对待孙努努一样，模型似乎与这个名字有某种关联，尽管它在嵌入词汇中的表达不够清晰，导致无法有效搜索到。
- en: I tested a number of other specific names, places, and one weird word, ‘normalcy’,
    for the embedding models’ ability to encode and match them in the Wikipedia texts.
    The table below shows the hits and misses.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我测试了多个具体的名字、地点和一个奇怪的词——‘normalcy’，以检验嵌入模型在维基百科文本中对它们的编码和匹配能力。下表展示了匹配和未匹配的情况。
- en: '![](../Images/d9d71b3c9469a32917317f054113e469.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d9d71b3c9469a32917317f054113e469.png)'
- en: Screenshot by the author
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 作者截屏
- en: What does this tell us?
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这告诉我们什么？
- en: Language models encode more frequently-encountered names, i.e. more famous people,
    but are less likely to encode them the more infrequent they are. Larger embeddings,
    in general, encode more specific details. But there were cases here smaller models
    outperformed larger ones, and models also sometimes had to have some associations
    even with name that they cannot recognize well enough to find. A great follow
    up on this would be a more systematic study of how noun frequency affects representation
    in embeddings.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型对更多频繁出现的名字进行编码，即更多的名人，但对不太常见的名字编码的可能性较小。一般来说，较大的嵌入能够编码更多具体的细节。但在这里有一些情况，小模型的表现超过了大模型，而且有时模型即使对一些无法完全识别的名字，也不得不进行某种关联。对此进行系统化的后续研究，探讨名词频率如何影响嵌入表示，会是一个很好的方向。
- en: '**Tangent #1: Rhyming**'
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**切入点 #1：押韵**'
- en: This was a bit of a tangent but I had fun testing it. Large Language models
    cannot rhyme very well, because they neither speak or hear. Most humans learn
    to read aloud first, and learn to read silently only later. When we read silently,
    we can still subvocalize the words and ‘hear’ the rhymes in written verse as well.
    Language models do not do this. Theirs is a silent, text-only world. They know
    about rhyming only from reading about it, and never get very good at it. Embeddings
    could theoretically represent phonetics, and can usually give accurate phonetics
    for a given word. But I’ve been testing rhyming on and off since GPT-3, and LLMs
    usually can’t search on this. However, the embeddings surprised me a few times
    in this exercise.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点跑题，但我在测试时玩得很开心。大型语言模型的押韵能力不好，因为它们既不说话也不听。大多数人首先学习大声朗读，然后才学会默读。当我们默读时，我们仍然可以在心里默默发音，并且能够‘听到’写作中的韵律。语言模型并不这样做。它们的世界是无声的、只有文本的。它们了解押韵只是通过阅读相关的内容，而永远不能做到很熟练。理论上，嵌入可以表示音韵学，并且通常可以为一个特定的单词提供准确的发音。然而，自从GPT-3以来，我就一直在测试押韵，语言模型通常无法在此进行搜索。然而，嵌入在这次测试中几次让我感到惊讶。
- en: '**Which President’s name rhymes with ‘Gimme Barter?’**'
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**哪个总统的名字和‘Gimme Barter’押韵？**'
- en: '![](../Images/b5154f2fa2c7501c4443e5f421063374.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5154f2fa2c7501c4443e5f421063374.png)'
- en: From WikiCommons. Photo of Jimmy Carter, by [Commonwealth Club](https://www.flickr.com/people/9555120@N08)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 来自WikiCommons。吉米·卡特的照片，由[Commonwealth Club](https://www.flickr.com/people/9555120@N08)提供
- en: This one turned out to be easy; all four vectors gave “Jimmy Carter” as the
    first returned hit. The cosine similarities were lowish, but since this was essentially
    a multiple choice test of Presidents, they all made the match easily. I think
    the spellings of Gimme Barter and Jimmy Carter are too similar, so let’s try some
    harder ones, with more carefully disguised rhymes that sound alike but have dissimilar
    spellings.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这一项测试结果很简单；所有四个向量都把“Jimmy Carter”作为第一个返回的结果。余弦相似度较低，但因为这基本上是一个多项选择的总统测试，它们都很容易匹配上。我认为“Gimme
    Barter”和“Jimmy Carter”的拼写太相似了，所以我们来尝试一些更难的，韵律相似但拼写不同的例子。
- en: '**Which US President’s name rhymes with Laybramam Thinkin’?**'
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**哪个美国总统的名字和‘Laybramam Thinkin’’押韵？**'
- en: '![](../Images/eccdc739984e7386b7aae8edc519b707.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eccdc739984e7386b7aae8edc519b707.png)'
- en: 'From WikiCommons: photo of Abraham Lincoln, by Alexander Gardner.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 来自WikiCommons：亚伯拉罕·林肯的照片，由亚历山大·加德纳（Alexander Gardner）拍摄。
- en: 'This one was harder. Abraham Lincoln did not show up on BGE or ST’s top ten
    hits, but was #1 for Ada and #3 for Large.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这个更难一些。亚伯拉罕·林肯没有出现在BGE或ST的前十名中，但在Ada中排名第1，在Large中排名第3。
- en: '**Which US President’s names rhymes with Will-Ard Syl-Bor?**'
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**哪个美国总统的名字和Will-Ard Syl-Bor押韵？**'
- en: 'Millard Fillmore was a tough rhyme. It was #2 for Ada, #5 for Large, not in
    the top 10 for the others. The lack of Internet poetry about President Fillmore
    seems like a gap someone needs to fill. There were a lot of false hits for Bill
    Clinton, perhaps because of the double L’s?'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 米拉德·菲尔莫尔是一个难度较大的押韵。它在Ada中排名第2，在Large中排名第5，在其他分类中不在前10名内。关于菲尔莫尔总统的网络诗歌似乎是一个有待填补的空白。关于比尔·克林顿的错误匹配较多，可能是因为双“L”？
- en: '![](../Images/38b3a1c7173effd99778638587ad670d.png)![](../Images/ea1daaa4fd768e2626c01cba73758b2b.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38b3a1c7173effd99778638587ad670d.png)![](../Images/ea1daaa4fd768e2626c01cba73758b2b.png)'
- en: Google search results obtained by author; Portrait of Millard Fillmore by George
    Peter Alexander Healey from WikiCommons
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Google搜索结果由作者获得；米拉德·菲尔莫尔肖像，乔治·彼得·亚历山大·希利（George Peter Alexander Healey）绘制，来自WikiCommons
- en: 'And yet, this exists: [https://www.classroompoems.com/millard-fillmore-poem.htm](https://www.classroompoems.com/millard-fillmore-poem.htm).
    Because it is the Internet.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个确实存在：[https://www.classroompoems.com/millard-fillmore-poem.htm](https://www.classroompoems.com/millard-fillmore-poem.htm)。因为它就是互联网。
- en: '**Which US President’s name rhymes with Mayrolled Gored?**'
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**哪个美国总统的名字和Mayrolled Gored押韵？**'
- en: 'Gerald Ford was #7 for BGE , #4 for Ada, #5 for Large.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 杰拉尔德·福特在BGE中排名第7，在Ada中排名第4，在Large中排名第5。
- en: Rhyming was not covered at the Gerald R. Ford Presidential Museum in my hometown
    of Grand Rapids, Michigan. I would know, I visited it many times. More on that
    later.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 押韵在我家乡密歇根州大急流城的杰拉尔德·R·福特总统博物馆并没有被涵盖。我知道，因为我去了很多次。稍后会详细介绍。
- en: 'Takeaway: the larger embedding schemes can rhyme, a little, although maybe
    less well than a human. How are they doing this, and what are the limits? Are
    they analyzing phonetics, taking advantage of existing rhyming content, or making
    good guesses another way? I have no idea. Phonetic encoding in embedding systems
    seems like a fine thesis topic for some enterprising linguistics student, or maybe
    a very nerdy English Lit major.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 收获：更大的嵌入方案可以略微押韵，尽管可能没有人类那么好。它们是如何做到的，有限制是什么？它们是在分析语音学，利用现有的押韵内容，还是通过其他方式做出正确的猜测？我不知道。嵌入系统中的语音编码似乎是一个非常适合有创业精神的语言学学生的论文题目，或者是非常痴迷的英语文学专业学生。
- en: 'Embedding can’t do Booleans: no NOT, AND, or OR'
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 嵌入无法进行布尔操作：没有NOT、AND或OR
- en: Simple semantic search cannot do some basic operations the keyword query systems
    generally can, and are not good at searching for sequences of events.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的语义搜索无法执行一些关键字查询系统通常能够完成的基本操作，也不擅长搜索事件序列。
- en: '**Question: Which Presidents were NOT Vice President first?**'
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**问题：哪些总统不是先当副总统的？**'
- en: Doing a vector search with ‘NOT’ is similar to the old adage about telling someone
    not to think about a Pink Elephant — saying the phrase usually causes the person
    to do so. Embeddings have no representation of ‘Not Vice President’, they only
    have Vice President.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 使用‘NOT’进行向量搜索类似于老生常谈的“不要想着粉红色大象”——通常说出这个词组会让人不由自主地去想它。嵌入没有“不是副总统”的表示，它们只有副总统。
- en: '![](../Images/3796beca0e328bf20359e783adc87c15.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3796beca0e328bf20359e783adc87c15.png)'
- en: Image by the author, using GPT-4o (Dall-E)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供，使用 GPT-4o（Dall-E）
- en: The vector representing the question will contain both “President” and “Vice
    President” and tend to find chunks with both. One could try to kludge a compound
    query, searching first for all President, then all Vice Presidents, and subtract,
    but the limit on contexts returned would prevent returning all of the first list,
    and is not guaranteed to get all of the second. Boolean search with embeddings
    remains a problem.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 代表问题的向量将同时包含“总统”和“副总统”，并倾向于找到同时包含这两个词的块。可以尝试拼凑一个复合查询，先搜索所有总统，再搜索所有副总统，然后相减，但返回的上下文数量限制会阻止返回完整的第一个列表，并且也不能保证返回第二个列表中的所有内容。带嵌入的布尔搜索仍然是一个问题。
- en: '**Question: Which U.S. President was NOT elected as vice President and NEVER
    elected as President?**'
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**问题：哪个美国总统既没有作为副总统当选，也从未当选为总统？**'
- en: 'An exception the the ‘NOT’ fail: all of the embeddings could find the passage
    saying that Gerald Ford was the only President that was never elected to be Vice
    President (appointed when Spiro Agnew resigned) or President (took Nixon’s place
    when he resigned, lost the re-election race to Jimmy Carter.) They were able to
    find this because the ‘not’ was explicitly represented in the text, with no inference
    needed, and it is also a well-known fact about Ford.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一个‘NOT’失败的例外：所有的嵌入都能够找到一段文字，表明杰拉尔德·福特是唯一一位既没有当选为副总统（当斯皮罗·阿格纽辞职时被任命）也没有当选为总统（当尼克松辞职时接任，之后在与吉米·卡特的再选竞赛中失败）的总统。它们能够找到这一点，因为‘not’在文本中得到了明确的表示，不需要推理，而且这也是关于福特的一个广为人知的事实。
- en: '**Why is there a double negative in this question?**'
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**为什么这个问题中有双重否定？**'
- en: The unnecessary double negative in the prior question made this search better.
    A search on “Which U.S. President was not elected as either Vice President or
    President?” gave poorer results. I added the double negative wording as a hunch
    that the double negatives would have a compound effect of making the query more
    both negative and make it easier to connect the ‘not’ to both offices. This does
    not make grammatical sense but does make sense in the world of superimposed semantics.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个问题中的不必要的双重否定使得这个搜索结果更好。搜索“哪位美国总统既没有被选为副总统，也没有被选为总统？”给出的结果较差。我添加了双重否定的表述，直觉告诉我，双重否定会产生复合效应，使查询更加消极，并且更容易将‘not’与两个职位关联起来。这在语法上没有意义，但在重叠语义的世界里是有道理的。
- en: Gerald R. Ford’s many accomplishments
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 杰拉尔德·R·福特的诸多成就
- en: People who have visited the Gerald R. Ford Presidential museum in my hometown
    of Grand Rapids, Michigan, a sufficient number of times will be aware that Ford
    made **many** important contributions despite not being elected to the highest
    offices. Just putting that out there.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 来自我家乡密歇根州大急流市的杰拉尔德·R·福特总统博物馆的参观者，如果次数足够多，会知道福特尽管没有当选为最高职位，依然做出了**许多**重要贡献。只是想把这一点说出来。
- en: '![](../Images/aa9dec713c93094ede8fb22feb4b120f.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa9dec713c93094ede8fb22feb4b120f.png)'
- en: Gerald Ford’s inauguration. From WikiCommons; public domain photo by Robert
    LeRoy Knudsen
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 杰拉尔德·福特的就职典礼。来自WikiCommons；由Robert LeRoy Knudsen拍摄的公共领域照片
- en: '**Question: Which Presidents were President AND Vice President?**'
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**问题：哪些总统曾同时担任总统和副总统？**'
- en: Semantic search has a sort of weak AND, more like an OR, but neither is a logical
    Boolean query. Embeddings do not link concepts with strict logic. Instead, think
    of them as superimposing concepts on top of each other on the same vector. This
    query would find chunks that load strongly on President (which is most of them
    in this dataset) and Vice President, but does not enforce the logical AND in any
    way. In this dataset it gives some correct hits, and a lot of extraneous mentions
    of Vice Presidents. This search for superimposed concepts is not a true logical
    OR either.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 语义搜索有一种弱AND，更像是OR，但这两者都不是逻辑布尔查询。嵌入并不会以严格的逻辑方式连接概念。相反，应该把它们看作是将概念叠加在同一个向量上的。这个查询会找到与总统（在这个数据集中几乎所有的条目）和副总统相关的片段，但并没有以任何方式强制执行逻辑AND。在这个数据集中，它提供了一些正确的结果，以及大量多余的副总统提及。对于叠加的概念而言，这种搜索也不是一个真正的逻辑OR。
- en: Embeddings and sequences of actions
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入和一系列动作
- en: Do embeddings connect concepts sequentially well enough to search on these sequences?
    My going-in assumption was that they cannot, but the embeddings did better than
    expected.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是否足够好地按顺序连接概念，以便在这些序列上进行搜索？我最初的假设是它们无法做到这一点，但嵌入的表现比预期的要好。
- en: Humans have a specific types of memory for sequence, called episodic memory.
    Stories are an important type of information for us; we encode things like personal
    history, social information and also useful lessons as stories. We can also recognize
    stories similar to ones that we already know. We can read a story about a hero
    who fails because of his fatal flaw, or an ordinary person who rises to great
    heights, and recognize not just the concepts but the sequence of actions. In my
    [previous blog post](https://medium.com/@nathanbos/what-your-gpt-rag-cant-see-a4ef19ef2724)
    on RAG search using Aesop’s Fables, the RAG system did not seem to have any ability
    to search on sequences of actions. I expected a similar failure here, but the
    results were a little different.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 人类有一种特定的记忆类型用于记忆顺序，称为情景记忆。故事是我们非常重要的一类信息；我们会将个人历史、社交信息以及有用的教训编码成故事。我们还能识别出那些与我们已经知道的故事相似的故事。我们可以阅读关于一个因为致命缺陷而失败的英雄的故事，或者一个普通人如何崛起到伟大高度的故事，并不仅仅识别出其中的概念，还能识别出行动的顺序。在我之前的[博客文章](https://medium.com/@nathanbos/what-your-gpt-rag-cant-see-a4ef19ef2724)中，讲的是使用伊索寓言的RAG搜索系统，该系统似乎没有能力搜索动作的顺序。我本以为在这里会遇到类似的失败，但结果有些不同。
- en: '**Question: Which US Presidents served in Congress after being President?**'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题：哪些美国总统在担任总统后曾在国会任职？**'
- en: 'There were many Presidents who served in congress before being President, but
    only two who served in congress after being President. All of the Embeddings returned
    a passage from John Quincy Adams, which directly gives the answer, as a top hit:
    *Adams and Andrew Johnson are the only former presidents to serve in Congress.*
    All of them also separately found entries for Andrew Johnson in the top 10\. There
    were a number of false hits, but the critical information was there.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多总统在担任总统之前曾在国会任职，但只有两位总统在担任总统之后曾在国会任职。所有嵌入结果都返回了约翰·昆西亚当斯的一段文字，这段文字直接给出了答案，作为最相关的结果：*亚当斯和安德鲁·约翰逊是唯一两位曾担任总统后再担任国会职务的前总统。*它们还分别在前十名中找到了安德鲁·约翰逊的相关条目。虽然有一些错误的结果，但关键信息是存在的。
- en: The embeddings did not do as well on the follow-ups, like **Which US Presidents
    served as judges after being President?** But all did find mention of Taft, who
    notably was the only person to serve as chief justice and president.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入在后续测试中表现不佳，比如**哪些美国总统在担任总统后曾担任法官？**但所有的测试结果都提到了塔夫特，塔夫特是唯一一位既担任过首席大法官又担任过总统的人。
- en: Does this really represent successfully searching on a sequence? Possibly not;
    in these cases the sequence may be encapsulated in a single searchable concept,
    like “former President”. I still suspect that embeddings would badly underperform
    humans on more difficult story-based searches. But this is a subtle point that
    would require more analysis.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这是否真正代表了成功地搜索到一个顺序呢？可能不是；在这些情况下，顺序可能被封装成了一个单一的可搜索概念，比如“前总统”。我仍然怀疑，嵌入在更复杂的故事型搜索中会远远逊色于人类。但这是一个微妙的问题，需要更多的分析。
- en: What about causal connections?
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 那么因果关系呢？
- en: 'Causal reasoning is such an important part of human reasoning that I wanted
    to separately test whether causal linkages are clearly represented and searchable.
    I tested these with two paired queries that had the causality reversed, and looked
    both at which search hits were returned, and how the pairs different. Both question
    pairs were quite interesting and results are shown in the spreadsheet; I will
    focus on this one:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 因果推理是人类推理中非常重要的一部分，因此我想单独测试一下因果联系是否能清晰地表示并被搜索到。我使用了两个配对查询，因果关系被颠倒了，并查看了返回的搜索结果以及这两个配对的不同之处。这两个问题配对都非常有趣，结果已显示在电子表格中；我将集中讨论这个：
- en: '**Question: When did a President’s action cause an important world event? -vs-
    When did an important world event cause a President’s action?**'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题：总统的行动何时导致了一个重要的世界事件？ - 与 - 一个重要的世界事件何时导致了总统的行动？**'
- en: ST failed this test, it returned exactly the same hits in the same order for
    both queries. The causal connection was not represented clearly enough to affect
    the search.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ST未通过这个测试，它返回了两个查询完全相同的结果，并且顺序也一样。因果关系未能清晰地表示出来，无法影响搜索。
- en: All of the embeddings returned multiple chunks related to Presidential world
    travel, weirdly failing to separate traveling from official actions.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的嵌入结果都返回了多个与总统世界旅行相关的片段，奇怪的是未能将旅行和官方行动分开。
- en: None of the embeddings did well on the causal reversal. Every one had hits where
    world events coincided with Presidential actions, often very minor actions, with
    no causal link in either direction. There all had false hits where the logical
    linkage went in the wrong directions (Presidents causing events vs responding
    to events). There were multiple example of commentators calling out Presidential
    inaction, which suggests that ‘act’ and ‘not act’ are conflated. Causal language,
    especially the word ‘cause’ triggered a lot of matches, even when it was not attached
    to a Presidential action or world event.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 没有任何嵌入在因果反转方面表现良好。每个嵌入都有一些涉及世界事件与总统行为相吻合的结果，通常是非常微小的行为，但没有任何因果联系。所有结果中都有逻辑联系错误的情况（总统引起事件与回应事件的方向搞错）。还有多个例子表明评论员指出总统的不作为，这表明“行动”和“不行动”被混淆了。因果语言，尤其是“cause”（原因）一词，触发了很多匹配，即使它并没有与总统的行动或世界事件相关联。
- en: A deeper exploration of how embeddings represent causality, maybe in a critical
    domain like medicine, would be in order. What I observed is a lack of evidence
    that embeddings represent and correctly use causality.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 需要更深入地探讨嵌入如何表示因果关系，也许在像医学这样的关键领域中更为合适。我观察到的是，嵌入缺乏能够正确表示和使用因果关系的证据。
- en: Analogies
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 类比
- en: 'Question: Which U.S. Presidents were similar to Simon Bolivar, and how?'
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题：哪些美国总统与西蒙·玻利瓦尔相似？怎么相似？
- en: Simon Bolivar, revolutionary leader and later political leader in South America,
    is sometimes called the “George Washington of South America”. Could the embedding
    models perceive this analogy in the other direction?
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 西蒙·玻利瓦尔，南美洲的革命领袖和后来的政治领袖，有时被称为“南美洲的乔治·华盛顿”。嵌入模型能否在反向中感知到这一类比？
- en: BGE- Gave a very weird set of returned context, with no obvious connection besides
    some mentions of Central/ South American.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BGE- 返回了一组非常奇怪的上下文，除了提到一些中美洲/南美洲的内容外，没有明显的联系。
- en: ST- Found a passage about William Henry Harrison’s 1828 trip to Colombia and
    feuding with Bolivar, and other mentions of Latin America, but made no abstract
    matches.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ST- 找到了一段关于威廉·亨利·哈里森1828年访问哥伦比亚以及与博尔瓦尔发生争执的内容，还有一些关于拉丁美洲的提及，但没有匹配的摘要。
- en: Ada- Found the Harrison passage + South America references, but no abstract
    matches that I can tell.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ada- 找到了关于哈里森的段落和南美洲的相关引用，但我能看出来的没有匹配的摘要。
- en: 'Large- Returned George Washington as hit #5 behind Bolivar/ S America hits.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Large- 返回了乔治·华盛顿，排名第5，排在博尔瓦尔/南美洲相关的结果之后。
- en: Large won this test in a landslide. This hit shows the clearest pattern of larger/better
    vectors outperforming others at abstract comparisons.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Large在这次测试中大获全胜。这个匹配显示了较大/较好的向量在抽象比较中优于其他向量的最清晰模式。
- en: '![](../Images/9ad995f6e0a7d9a6bd777424cd8d24e1.png)![](../Images/70e1d57e6aae60aab2d684442cba0e9c.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ad995f6e0a7d9a6bd777424cd8d24e1.png)![](../Images/70e1d57e6aae60aab2d684442cba0e9c.png)'
- en: Images in public domain, obtain from Wiki commons. Statue of Simon Bolivar by
    Emmanuel Frémiet, photo by [Jebulon](https://commons.wikimedia.org/wiki/User:Jebulon).
    Washington Crossing the Delaware painting by Emmanuel Leutze.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 公有领域的图片，来自维基共享资源。西蒙·玻利瓦尔的雕像，由埃马纽埃尔·弗雷米特（Emmanuel Frémiet）创作，照片由[Jebulon](https://commons.wikimedia.org/wiki/User:Jebulon)提供。乔治·华盛顿过德拉瓦河的画作，由埃马纽埃尔·洛伊茨（Emmanuel
    Leutze）创作。
- en: Abstract concepts
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 抽象概念
- en: 'I tested a number of searches on more abstract concepts. Here are two examples:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我在更抽象的概念上进行了多个搜索测试。以下是两个示例：
- en: '**Questions: Which US Presidents exceeded their power?**'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题：哪些美国总统超越了他们的权力？**'
- en: 'BGE: top hit: “*In surveys of U.S. scholars ranking presidents conducted since
    1948, the top three presidents are generally Lincoln, Washington, and Franklin
    Delano Roosevelt, although the order varies.”* BGE found hits all related to Presidential
    noteworthiness, especially rankings by historians, I think keying on the words
    ‘power’ and ‘exceed’. This was a miss.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 'BGE: 最高匹配：“*自1948年以来，美国学者对总统的排名调查中，排名前三的总统通常是林肯、华盛顿和富兰克林·德拉诺·罗斯福，尽管顺序有所不同。*”
    BGE找到的匹配都与总统的显著性有关，特别是历史学家的排名，我认为是关注了“权力”和“超越”这两个词。这是一个失误。'
- en: 'ST: “*Roosevelt is widely considered to be one of the most important figures
    in the history of the United States.”* Same patterns as BGE; a miss.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 'ST: “*罗斯福被广泛认为是美国历史上最重要的人物之一。*” 与BGE的模式相同；这是一个失误。'
- en: 'Ada: Ada’s hits were all on the topic of Presidential power, not just prestige,
    and so were more on-target than the smaller models. There is a common theme of
    increasing power, and some passages that imply exceeding, like this one: *the
    Patriot Act “increased authority of the executive branch at the expense of judicial
    opinion…”* Overall, not a clear win, but closer.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 'Ada: Ada的重点都集中在总统权力的话题上，而不仅仅是声望，因此比小型模型更为精准。这里有一个共同的主题是权力的增长，并且有一些段落暗示了超越，比如这一段：*《爱国者法案》“增加了行政部门的权力，但以牺牲司法意见为代价……”*
    总体而言，这并非完全的胜利，但更接近了目标。'
- en: 'Large: It did not find the best 10 passages, but the hits were more on target.
    All had the concept of increasing Presidential power, and most has a flavor of
    exceeding some previous limit, e.g. “*conservative columnist George Will wrote
    in The Washington Post that Theodore Roosevelt and Wilson were the “progenitors
    of today’s imperial presidency”*'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 'Large: 它没有找到最好的10个段落，但这些命中更为精准。所有段落都包含了总统权力增长的概念，而且大多数有某种超越先前限制的味道，例如“*保守派专栏作家乔治·威尔在《华盛顿邮报》上写道，西奥多·罗斯福和威尔逊是今天‘帝国总统制’的先驱*”'
- en: Again, there was a pattern of larger models having more precise, on-target abstractions.
    Large was the only one to get close to a correct representation of a President
    “exceeding their power” but even this performance left a lot of room for improvement.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，有一个模式是较大的模型有更精确、更贴近目标的抽象。大型模型是唯一一个接近正确表现“总统超越权力”的模型，但即便如此，其表现依然有很大的改进空间。
- en: Embeddings do not understand subtext
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 嵌入模型无法理解潜台词
- en: Subtext is meaning in a text that is not directly stated. People add meaning
    to what they read, making emotional associations, or recognizing related concepts
    that go beyond what is directly stated, but embeddings do this only in a very
    limited way.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 潜台词是指文本中未直接陈述的含义。人们在阅读时，会为所读内容赋予意义，产生情感关联，或识别与直接表述无关的相关概念，而嵌入式表示只能在非常有限的范围内做到这一点。
- en: '**Question: Give an example of a time when a U.S. President expressed frustration
    with losing an election?**'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题：举一个美国总统因选举失败而感到沮丧的例子？**'
- en: In 1960, then-Vice President Richard Nixon lost a historically close election
    to John F Kennedy. Deeply hurt by the loss, Nixon decided to settle to return
    home to his home state of California and ran for governor in 1962\. Nixon lost
    that race too. He famously announced at a press conference, “You don’t (won’t)
    have Nixon to kick around anymore because, gentlemen, this is my last press conference,”
    thus ending his political career, or so everyone thought.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 1960年，当时的副总统理查德·尼克松在一场历史上极为接近的选举中败给了约翰·F·肯尼迪。因失败深受打击，尼克松决定回到自己家乡加利福尼亚，并在1962年竞选州长。尼克松也输了这场竞选。他在一次新闻发布会上著名地宣布：“你们以后再也没有尼克松可以踢了，先生们，这将是我的最后一次新闻发布会。”
    从而结束了他的政治生涯，至少大家是这么认为的。
- en: '![](../Images/26f9fdccf1fd27f8243f69b017a6e07a.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26f9fdccf1fd27f8243f69b017a6e07a.png)'
- en: Graphic by the author and GPT4o( Dall-E). There is no intentional resemblance
    with specific Presidents; these are Dall-E’s ideas about generic Presidential-looking
    men expressing emotions. Not bad.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者和GPT4（Dall-E）提供。与特定总统没有刻意的相似性；这些是Dall-E关于典型总统形象表达情感的构思。还不错。
- en: 'What happens when you search for: “Give an example of a time when a U.S. President
    expressed frustration with losing an election”? None of the embeddings return
    this Nixon quote. Why? Because Wikipedia never directly states that he was frustrated,
    or had any other specific emotion; that is all subtext. When a mature human reads
    “You won’t have Nixon to kick around anymore”, we recognize some implied emotions,
    probably without consciously trying to do so. This might be so automatic when
    reading that one thinks it is in the text. But in his passage the emotion is never
    directly stated. And if it is subtext, not text, an embedding will (probably)
    not be able to represent it or be able to search it.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 当你搜索：“举一个美国总统因选举失败而感到沮丧的例子”时会发生什么？没有一个嵌入返回了尼克松的这段引述。为什么？因为维基百科从未直接表明他感到沮丧或有其他特定情感；这都是潜台词。当一个成熟的人类阅读到“你们以后不会再有尼克松可以踢来踢去了”时，我们会识别出一些隐含的情感，可能是在没有刻意思考的情况下做出的。这可能是在阅读时的自动反应，以至于我们认为它就存在于文本中。但在这段话中，情感从未被直接表述。如果它是潜台词而非明示内容，嵌入式模型（可能）无法表示或搜索到它。
- en: Wikipedia avoids speculating on emotional subtext as a part of fact-based reporting.
    Using subtext instead of text is also considered good tradecraft for fiction writers,
    even when the goal is to convey strong emotions. A common piece of advice for
    new writers is, “show, don’t tell.” Skilled writers reveal what characters are
    thinking and feeling without directly stating it. There’s even a name for pedantic
    writing that explains things too directly, it is called “on-the-nose dialogue”.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 维基百科避免在基于事实的报道中推测情感潜台词。即便目标是传达强烈的情感，使用潜台词而非直接陈述也是小说作家的一种良好技巧。对新作家的常见建议是：“展示，而不是告知。”
    熟练的作家能通过不直接说明的方式，揭示人物的思想和情感。甚至有一种术语形容那些直接解释事物的写作风格，叫做“直白对话”。
- en: But “show, don’t tell” makes some content invisible to embeddings, and thus
    to vector-based RAG retrieval systems. This presents some fundamental barriers
    to what can be found in RAG system in the domain of emotional subtext, but also
    other layers of meaning that go beyond what is directly stated. I also did a lot
    of probing around concepts like Presidential mistakes, Presidential intentions,
    and analytic patterns that are just beyond what is directly stated in the text.
    Embedding-based search generally failed on this, mostly returning only direct
    statements, even when they were not relevant.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 但“展示，而不是告知”使得一些内容对嵌入表示不可见，从而也无法被基于向量的RAG检索系统检索到。这在情感潜台词的领域，甚至其他超越直接陈述的意义层次中，提出了某些根本性障碍。我还深入探讨了类似“总统错误”、“总统意图”以及那些仅仅超出直接陈述的分析模式等概念。基于嵌入表示的搜索通常在这些方面失败，主要返回的只是直接的陈述，甚至即使它们不相关。
- en: '**Why are embeddings shallow compared to Large Language Models?**'
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**为什么嵌入表示相比大型语言模型较为浅显？**'
- en: Large Language Models like Claude and GPT-4 have the ability to understand subtext;
    they do a credible job explaining stories, jokes, poetry and Taylor Swift song
    lyrics. So why can’t embeddings do this?
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型如Claude和GPT-4具有理解潜台词的能力；它们能够可信地解释故事、笑话、诗歌以及泰勒·斯威夫特的歌词。那么，为什么嵌入表示不能做到这些呢？
- en: Language models are comprised of layers, and in general the lower layers are
    shallower forms of processing, representing aspects like grammar and surface meanings,
    while higher level of abstraction occur in higher layers. Embeddings are the first
    stage in language model processing; they convert text into numbers and then let
    the LLM take over. This is the best explanation I know of for why the embedding
    search tests seem to plateau at shallower levels of semantic matching.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型由多个层次组成，通常较低的层次是处理的浅层形式，代表了语法和表面意义等方面，而较高的层次则进行更高层次的抽象。嵌入表示是语言模型处理的第一阶段；它们将文本转化为数字，然后交由LLM接管。这是我所知道的最好的解释，说明为什么嵌入表示搜索测试在语义匹配的浅层次上似乎会遇到瓶颈。
- en: Embedding were not originally designed for RAGs; using them for semantic search
    is a clever, but ultimately limited secondary usage. That is changing, as embedding
    systems are being optimized for search. BGE was to some extend optimized for search,
    and ST was designed for sentence comparison; I would say this is why both BGE
    and ST were not too far behind Ada and Large despite being a fraction of the size.
    Large was probably designed with search in mind to a limited extent. But it was
    easy to push each of them to their semantic limits, as compared with the kind
    of semantics processed by full large language models.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入表示最初并非为RAG（检索增强生成）设计；将它们用于语义搜索是一种聪明但最终有限的二次使用方式。随着嵌入系统优化用于搜索，这一情况正在改变。BGE在某种程度上被优化为搜索，而ST则是为句子比较设计的；我认为这也是为什么BGE和ST尽管只有Ada和Large的极小规模，但仍不落后它们太多的原因。Large大概在一定程度上考虑到了搜索的需求。但与完全的大型语言模型处理的语义相比，很容易将它们推向语义的极限。
- en: Conclusion
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: What did we learn, conceptually, about embeddings in his exercise?
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在他的练习中，我们在概念上学到了关于嵌入表示什么？
- en: The embedding models, overall, surprised me on a few things. The semantic depth
    was less than I expected, based on the performance of the language models that
    use them. But they outperformed my expectations on a few things I expected them
    to fail completely at, like rhyming and searching for sequenced activities. This
    activity piqued my interest in probing some more specific areas; perhaps it did
    for you as well.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，嵌入表示模型在某些方面让我感到惊讶。语义深度比我预期的要浅，这与使用它们的语言模型的表现相比。然而，它们在一些我认为它们完全会失败的方面超出了我的预期，比如押韵和搜索顺序活动。这项活动激发了我对一些更具体领域的探究兴趣，也许它同样激发了你们的兴趣。
- en: For RAG developers, this illuminated some of the specific ways that larger models
    may outperform smaller ones, including the precisions of their representation,
    the breadth of knowledge and the range of abstractions. As a sometime RAG builder,
    I have been skeptical that paying more for embeddings would lead to better performance,
    but this exercise convinced me that embedding choice can make a difference for
    some applications.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 对于RAG开发人员而言，这些内容阐明了大型模型在一些特定方面可能优于小型模型，包括其表示的精确度、知识的广度和抽象范围。作为一个曾经的RAG构建者，我曾对为嵌入支付更多费用是否会提高性能持怀疑态度，但这次实验让我相信，嵌入选择对于某些应用来说确实能带来差异。
- en: Embeddings systems will continue to incrementally improve, but I think some
    fundamental breakthroughs will be needed in this area. There is some current research
    on innovations like universal text embeddings.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入系统将继续逐步改进，但我认为这一领域仍需要一些突破。目前有一些关于如通用文本嵌入等创新的研究。
- en: Knowledge graphs are a popular current way to supplement semantic search. Graphs
    are good for making cross-document connections, but the LLM-derived graphs I have
    seen are semantically quite shallow. To get semantic depth from a knowledge graph
    probably requires a professionally-developed ontology to be available to serve
    as a starting point; these are available for some specialized fields.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱是当前流行的补充语义搜索的方式。图谱擅长进行跨文档连接，但我见过的基于LLM的图谱在语义上相当浅薄。要从知识图谱中获得语义深度，可能需要一个专业开发的本体作为起点；对于一些专业领域，这些本体是可用的。
- en: My own preferred method is to improve text with more text. Since full language
    models can perceive and understand meaning that is not in embeddings, why not
    have a language model pre-process and annotate the text in your corpus with the
    specific types of semantics you are interested in? This might be too expensive
    for truly huge datasets, but for data in the small to medium range it can be an
    excellent solution.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我自己偏好的方法是通过更多的文本来改进文本。由于完整的语言模型能够感知和理解嵌入中没有的意义，为什么不让语言模型在你的语料库中预处理并注解你感兴趣的特定语义类型呢？对于真正巨大的数据集来说，这可能太昂贵，但对于中小规模的数据集来说，这可以是一个非常好的解决方案。
- en: 'I experimented with adding annotations to the Presidential dataset. To make
    emotional subtext searchable I had GPT4o write narratives for each President highlighting
    the personal and emotional content. These annotations were added back into the
    corpus. They are not great prose, but the concept worked. GPT’s annotation of
    the Nixon entry included the sentence: “The defeat was a bitter pill to swallow,
    compounded by his loss in the 1962 California gubernatorial race. In a moment
    of frustration, Nixon declared to the press, ‘You won’t have Nixon to kick around
    anymore,’ signaling what many believed to be the end of his political career”.
    This effectively turned subtext into text, making is searchable.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我尝试在总统数据集中添加注解。为了让情感潜台词能够被搜索，我让GPT4o为每位总统写了讲述个人和情感内容的叙事。这些注解被添加回到语料库中。虽然这些叙述不是很优美，但这个概念是有效的。GPT为尼克松条目添加的注解包括了这一句话：“这次失败是一个苦涩的教训，尤其是在他失去了1962年加利福尼亚州州长选举之后。在一次沮丧的时刻，尼克松对媒体说道：‘你们再也不会有机会拿我开涮了，’这被许多人认为是他政治生涯的终结。”这有效地将潜台词转化为文本，使得它可以被搜索。
- en: I experimented with a number of types of annotations. One that I was particularly
    happy used Claude to examine each Presidency and make comments on underlying system
    dynamical phenomena like delayed feedback and positive feedback loops. Searching
    on these terms on the original text gave nothing useful, but greatly improved
    with annotations. Claude’s analyses were not brilliant, or even always correct,
    but it found and annotated enough decent examples that searches using system dynamic
    language found useful content.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我尝试了多种类型的注解。其中我特别满意的一种方法是使用Claude来检查每一任总统，并对诸如延迟反馈和正反馈循环等潜在的系统动力学现象进行注解。在原文中搜索这些术语没有找到有用的信息，但通过注解大大改善了搜索结果。Claude的分析并不出色，甚至不总是正确的，但它找到了足够多的不错的例子，通过使用系统动力学语言的搜索能够找到有价值的内容。
- en: '![](../Images/2b4cf2ce30ad90ee1c03b3316e9e67d9.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b4cf2ce30ad90ee1c03b3316e9e67d9.png)'
- en: The Gerald R Ford Presidential Museum in Grand Rapids, Michigan. Photo from
    WikiCommons, taken by museum staff.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这是位于密歇根州大急流市的杰拉尔德·福特总统博物馆。图片来自WikiCommons，由博物馆工作人员拍摄。
- en: '![](../Images/37b6e54c439dc2f648c4b429c2d178b6.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/37b6e54c439dc2f648c4b429c2d178b6.png)'
- en: Oval office replica, the coolest thing in the Gerald R. Ford Presidential museum.
    Image from WikiCommons; photo by [JJonahJackalope](https://commons.wikimedia.org/wiki/User:JJonahJackalope)
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 杰拉尔德·R·福特总统博物馆内最酷的东西——椭圆形办公室复制品。图片来自WikiCommons；摄影师：[JJonahJackalope](https://commons.wikimedia.org/wiki/User:JJonahJackalope)
