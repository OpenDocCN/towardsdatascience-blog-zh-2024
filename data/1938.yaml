- en: Running a SOTA 7B Parameter Embedding Model on a Single GPU
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在单个 GPU 上运行 SOTA 7B 参数嵌入模型
- en: 原文：[https://towardsdatascience.com/running-a-sota-7b-parameter-embedding-model-on-a-single-gpu-bb9b071e2238?source=collection_archive---------5-----------------------#2024-08-09](https://towardsdatascience.com/running-a-sota-7b-parameter-embedding-model-on-a-single-gpu-bb9b071e2238?source=collection_archive---------5-----------------------#2024-08-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/running-a-sota-7b-parameter-embedding-model-on-a-single-gpu-bb9b071e2238?source=collection_archive---------5-----------------------#2024-08-09](https://towardsdatascience.com/running-a-sota-7b-parameter-embedding-model-on-a-single-gpu-bb9b071e2238?source=collection_archive---------5-----------------------#2024-08-09)
- en: Running Qwen2 on SageMaker
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 SageMaker 上运行 Qwen2
- en: '[](https://medium.com/@paluchasz?source=post_page---byline--bb9b071e2238--------------------------------)[![Szymon
    Palucha](../Images/37a33166ccb5b427d8aaf545e3376d59.png)](https://medium.com/@paluchasz?source=post_page---byline--bb9b071e2238--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--bb9b071e2238--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--bb9b071e2238--------------------------------)
    [Szymon Palucha](https://medium.com/@paluchasz?source=post_page---byline--bb9b071e2238--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@paluchasz?source=post_page---byline--bb9b071e2238--------------------------------)[![Szymon
    Palucha](../Images/37a33166ccb5b427d8aaf545e3376d59.png)](https://medium.com/@paluchasz?source=post_page---byline--bb9b071e2238--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--bb9b071e2238--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--bb9b071e2238--------------------------------)
    [Szymon Palucha](https://medium.com/@paluchasz?source=post_page---byline--bb9b071e2238--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--bb9b071e2238--------------------------------)
    ·16 min read·Aug 9, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--bb9b071e2238--------------------------------)
    ·16 分钟阅读·2024年8月9日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: In this post I will explain how to run a state-of-the-art 7B parameter LLM based
    embedding model on just a single 24GB GPU. I will cover some theory and then show
    how to run it with the HuggingFace Transformers library in Python in just a few
    lines of code!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将解释如何在单个 24GB GPU 上运行一个最先进的 7B 参数 LLM 基于的嵌入模型。我将首先讲解一些理论，然后展示如何用 HuggingFace
    Transformers 库在 Python 中仅用几行代码运行它！
- en: The model that we will run in the [Qwen2](https://arxiv.org/abs/2407.10671)
    open source model ([Alibaba-NLP/gte-Qwen2–7B-instruct](https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct))
    which was released in June 2024, and at the time of finishing this article is
    in 4th place on the [Massive Text Embeddings Benchmark](https://huggingface.co/spaces/mteb/leaderboard)
    on HuggingFace.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[Qwen2](https://arxiv.org/abs/2407.10671)开源模型（[Alibaba-NLP/gte-Qwen2–7B-instruct](https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct)）上运行该模型，该模型于
    2024 年 6 月发布，截止本文完成时，它在 HuggingFace 上的[Massive Text Embeddings Benchmark](https://huggingface.co/spaces/mteb/leaderboard)中排名第四。
- en: '![](../Images/ae9c63c85dcebf04f2477d219fbc7493.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae9c63c85dcebf04f2477d219fbc7493.png)'
- en: ScreenShot of the [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)
    on HuggingFace from July 2024\. The model was in 1st place in June 2024 and has
    subsequently dropped to 4th place. This shows the pace of development in AI these
    days!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 2024 年 7 月在 HuggingFace 上的[MTEB 排行榜](https://huggingface.co/spaces/mteb/leaderboard)截图。该模型在
    2024 年 6 月排名第一，随后跌至第四名。这显示了当今 AI 发展速度的快速！
- en: Theoretical Memory Requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理论内存需求
- en: Loading a Model
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载模型
- en: The amount of memory required to load a machine learning model (e.g. LLM or
    Embedding model) can be calculated from the number of its parameters.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 加载机器学习模型（例如 LLM 或嵌入模型）所需的内存可以通过其参数数量来计算。
- en: For example, a 7B parameter model in fp32 (float32 precision), means that we
    need to store 7B numbers in 32-bit precision in order to initialise the model
    in memory and be able to use it. Therefore, recalling that there are 8 bits in
    one byte, the memory needed to load the model is
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个 7B 参数模型的 fp32（浮点32精度），意味着我们需要以 32 位精度存储 7B 个数字，以便在内存中初始化该模型并能够使用它。因此，回忆一下每个字节有
    8 位，加载该模型所需的内存是
- en: Memory of a 7B param model in fp32 = 7B * 32 bits = 7B * 32 / 8 bytes = 28B
    bytes = **28GB**.
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一个 7B 参数模型的 fp32 内存 = 7B * 32 位 = 7B * 32 / 8 字节 = 28B 字节 = **28GB**。
- en: So in order to run this model we need at least 28GB of GPU memory. In fact there
    is also some additional overhead as described in [this post](https://www.substratus.ai/blog/calculating-gpu-memory-for-llm).
    As a result to run this model in full precision we cannot use smaller and cheaper
    GPU’s which have 16GB or 24GB of memory.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了运行这个模型，我们至少需要28GB的GPU内存。实际上，还有一些额外的开销，如[这篇文章](https://www.substratus.ai/blog/calculating-gpu-memory-for-llm)中所描述的那样。因此，要以全精度运行该模型，我们不能使用内存为16GB或24GB的小型且便宜的GPU。
- en: So without a more powerful GPU such as [NVIDIA’s A100](https://www.nvidia.com/en-gb/data-center/a100/),
    what alternatives do we have? It turns out there a few different techniques to
    reduce the memory requirement. The simplest one is to reduce the precision of
    the parameters. Most models can now be used in **half precision** without any
    significant loss in accuracy. The memory required to load a model in fp16 or [bf16](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format)
    is
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 所以没有更强大的GPU，如[NVIDIA的A100](https://www.nvidia.com/en-gb/data-center/a100/)，我们有哪些替代方案？事实证明，有几种不同的技术可以减少内存需求。最简单的一种方法是降低参数的精度。现在，大多数模型都可以在**半精度**下使用，而不会显著损失准确性。以fp16或[bf16](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format)加载模型所需的内存是
- en: Memory of a 7B param model in fp16 = 7B * 16 bits = 7B * 16 / 8 bytes = 14B
    bytes = **14GB**.
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一个7B参数模型在fp16中的内存 = 7B * 16位 = 7B * 16 / 8字节 = 14B字节 = **14GB**。
- en: Whilst this is already good enough to load the model on a 24GB GPU, we would
    still struggle to run it on a 16GB GPU, due to the additional overheads and extra
    requirements during inference.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这对于在24GB GPU上加载模型已经足够好，但由于推理过程中额外的开销和要求，我们仍然难以在16GB的GPU上运行它。
- en: Reducing the precision anymore than this naively would already start impacting
    performance but there is a technique called **quantisation** which is able to
    reduce the precision even further (such as into 8bits or 4bits) without a significant
    drop in accuracy. Recent research in LLMs has even shown the possibility of using
    1 bit precision (actually, log_2(3) = 1.58 bits) known as [1-bit LLMs](https://arxiv.org/abs/2402.17764).
    The parameters of these models can only take the values of 1, -1 or 0!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果继续降低精度，性能可能会受到影响，但有一种名为**量化**的技术，能够进一步减少精度（例如，降到8位或4位），而不会显著降低准确性。最近在LLM领域的研究甚至表明，使用1位精度（实际上是log_2(3)
    = 1.58位），即[1位LLM](https://arxiv.org/abs/2402.17764)的可能性。这些模型的参数只能取值1、-1或0！
- en: 'To read up more on these topics I would recommend:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入了解这些话题，我推荐阅读：
- en: Understanding floating point representations from the [following tutorial](https://math.libretexts.org/Workbench/Numerical_Methods_with_Applications_(Kaw)/1%3A_Introduction/1.05%3A_Floating-Point_Binary_Representation_of_Numbers#:~:text=A%20machine%20stores%20floating%2Dpoint,the%20magnitude%20of%20the%20mantissa.).
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过[以下教程](https://math.libretexts.org/Workbench/Numerical_Methods_with_Applications_(Kaw)/1%3A_Introduction/1.05%3A_Floating-Point_Binary_Representation_of_Numbers#:~:text=A%20machine%20stores%20floating%2Dpoint,the%20magnitude%20of%20the%20mantissa.)理解浮点数表示。
- en: '[Quantisation Fundamentals with Huggingface](https://www.deeplearning.ai/short-courses/quantization-fundamentals-with-hugging-face/)
    free course from DeepLearning.AI.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Huggingface量化基础](https://www.deeplearning.ai/short-courses/quantization-fundamentals-with-hugging-face/)
    由DeepLearning.AI提供的免费课程。'
- en: Inference
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推理
- en: The above calculations only tell us how much memory we need to simply load the
    model! On top of this we need additional memory to actually run some input through
    the model. For the Qwen2 embedding model and LLMs in general the extra memory
    required depends on the size of the context window (ie. the length of the text
    passed into the model).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 上述计算仅告诉我们加载模型所需的内存量！除此之外，我们还需要额外的内存来实际通过模型运行一些输入。对于Qwen2嵌入模型和一般的LLM，额外的内存需求取决于上下文窗口的大小（即传入模型的文本长度）。
- en: Older Models with Original Self-Attention Mechanism
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 采用原始自注意力机制的旧模型
- en: Before the release of [Flash Attention](https://arxiv.org/abs/2205.14135#) which
    is now widely adopted, a lot of older Language Models were using the original
    self-attention from the Transformer architecture. This mechanism requires an additional
    memory which scales quadratically with the input sequence length. To illustrate
    why that’s the case below is a great visual reminder of self-attention.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在现在广泛采用的[Flash Attention](https://arxiv.org/abs/2205.14135#)发布之前，许多较旧的语言模型使用的是Transformer架构中的原始自注意力机制。该机制需要额外的内存，并且该内存需求与输入序列的长度呈平方关系。为了说明为什么会这样，下面是一个很好的自注意力可视化示意图。
- en: '![](../Images/4a8c914370c08ec222bb4e23b4374895.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4a8c914370c08ec222bb4e23b4374895.png)'
- en: 'A great visual of the self-attention mechanism. Source: Sebastian Raschka,
    [https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention](https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention),
    reproduced with author’s permission.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的自注意力机制的可视化示例。来源：Sebastian Raschka，[https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention](https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention)，经作者许可转载。
- en: From the diagram we can see that apart from model weights (the *Wq, Wk, Wv*
    matrices) which we accounted for when calculating the memory required to load
    the model, there are many additional calculations and their outputs which need
    to be stored. These include for example, the inputs X, the *Q, K, V* matrices,
    and the **attention matrix** *QK^T*. It turns out that as the size of the input
    sequence length, *n*, grows the attention matrix becomes the dominant factor in
    the extra memory required.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从图示中我们可以看到，除了在计算加载模型所需内存时已经考虑的模型权重（*Wq, Wk, Wv* 矩阵）外，还有许多额外的计算及其输出需要存储。例如，输入
    X、*Q, K, V* 矩阵，以及 **注意力矩阵** *QK^T*。事实证明，随着输入序列长度 *n* 的增加，注意力矩阵成为额外内存需求中的主导因素。
- en: To understand this we can perform a few simple calculations. For example, in
    the original transformer the embedding dimension size, *d*, was 512\. So for an
    input sequence of 512 tokens both the inputs X and the attention matrix would
    require an additional 1MB of memory each in fp32.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这一点，我们可以进行一些简单的计算。例如，在原始的 Transformer 中，嵌入维度 *d* 为 512。因此，对于一个由 512 个词元组成的输入序列，输入
    X 和注意力矩阵在 fp32 格式下各自需要额外的 1MB 内存。
- en: 512² floats = 512² * 32 bits = 512² * 4 bytes = 1MB
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 512² 浮点数 = 512² * 32 位 = 512² * 4 字节 = 1MB
- en: If we increase the input sequence length to 8000 tokens, the extra memory requirements
    would be
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将输入序列长度增加到 8000 词元，额外的内存需求将是：
- en: Inputs X = 512 * 8000 * 4 bytes = 16MB; Attention Matrix = 8000² * 4 bytes =
    256MB.
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 输入 X = 512 * 8000 * 4 字节 = 16MB；注意力矩阵 = 8000² * 4 字节 = 256MB。
- en: and if we increase the input sequence length to 32k tokens the extra memory
    requirements would be
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将输入序列长度增加到 32k 词元，额外的内存需求将是：
- en: Inputs X = 512 * 32000 * 4 bytes = 65MB; Attention Matrix = 32000² * 4 bytes
    = 4GB!
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 输入 X = 512 * 32000 * 4 字节 = 65MB；注意力矩阵 = 32000² * 4 字节 = 4GB！
- en: As you can see the extra memory required grows very quickly with the size of
    the context window, and is quickly dominated by the *n²* number of floats in the
    attention matrix!
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，随着上下文窗口的大小增加，所需的额外内存快速增长，并且很快就会被注意力矩阵中的 *n²* 个浮点数所主导！
- en: The above calculations are still very much a simplification as there a lot more
    details which contribute to even more memory usage. For instance, in the original
    transformer there is also multi-head attention — where the attention computation
    is computed in parallel with many different heads (8 in the original implementation).
    So we need to multiply the required memory by the number of heads. Similarly,
    the above calculations were for a batch size of 1, if we want to embed many different
    texts at once we can increase the batch size, but at the cost of additional memory.
    For a detailed breakdown of the different memory requirements see the following
    [article](https://huggingface.co/blog/mayank-mishra/padding-free-transformer).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 上述计算仍然是简化版，因为实际上有更多的细节会导致更高的内存使用。例如，在原始的 Transformer 中，还涉及到多头注意力——即注意力计算是并行进行的，使用了多个不同的头（在原始实现中为
    8 个）。因此，我们需要将所需内存乘以头的数量。类似地，上述计算是针对批量大小为 1 的情况，如果我们想一次处理多个不同的文本，可以增加批量大小，但这会增加额外的内存开销。有关不同内存需求的详细拆解，请参见以下[文章](https://huggingface.co/blog/mayank-mishra/padding-free-transformer)。
- en: More Recent Models like Qwen2
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更近期的模型，如 Qwen2
- en: Since the release of the Transformer in 2017, there has been a lot of research
    into alternative attention mechanisms to avoid the n² bottleneck. However, they
    came with the tradeoff of decreased accuracy. In 2022, an exact attention mechanism
    came out with specific GPU optimisations called [Flash Attention](https://arxiv.org/abs/2205.14135)
    and has been widely adopted in LLMs. Since then theres been further iterations
    including the recent [Flash Attention 3](https://arxiv.org/abs/2407.08608) released
    in July 2024\. The most important takeaway for us is that Flash Attention scales
    linearly with the input sequence length!
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 自2017年Transformer发布以来，已经有大量研究提出替代的注意力机制，以避免n²瓶颈。然而，这些方法通常伴随着精度下降的折衷。2022年，一种精确的注意力机制发布，并针对GPU进行了优化，名为[Flash
    Attention](https://arxiv.org/abs/2205.14135)，并且已在LLMs中广泛应用。从那时起，已经出现了进一步的迭代版本，包括2024年7月发布的[Flash
    Attention 3](https://arxiv.org/abs/2407.08608)。对我们来说，最重要的收获是Flash Attention能够随着输入序列长度的增加线性扩展！
- en: Below is a theoretical derivation which compares the memory requirements of
    a 20B parameter model with different sequence lengths of different attention mechanisms.
    The `Padding-Free Transformer` is yet another optimisation which removes the need
    of [padding](https://huggingface.co/docs/transformers/en/pad_truncation) — very
    useful if you have one long sequence and many short sequences in a batch.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个理论推导，比较了不同序列长度和不同注意力机制下，20B参数模型的内存需求。`Padding-Free Transformer`是另一种优化方法，去除了[填充](https://huggingface.co/docs/transformers/en/pad_truncation)的需求——如果你的批次中有一长序列和多个短序列，这会非常有用。
- en: '![](../Images/a941c0fa93eea419f3b66db2022ce8ac.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a941c0fa93eea419f3b66db2022ce8ac.png)'
- en: 'Theoretical estimates of memory requirements for a 20B parameter model with
    different Attention Mechanisms. The main takeaway is the quadratic vs linear scaling.
    Source: Mayank Mishra, [Saving Memory Using Padding-Free Transformer Layers during
    Finetuning](https://huggingface.co/blog/mayank-mishra/padding-free-transformer),
    reproduced with author’s permission.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 不同注意力机制下20B参数模型的内存需求的理论估算。主要结论是二次与线性扩展。来源：Mayank Mishra，[在微调过程中使用Padding-Free
    Transformer层节省内存](https://huggingface.co/blog/mayank-mishra/padding-free-transformer)，已获作者许可转载。
- en: The Qwen2 model uses both the Flash Attention and padding optimisations. Now
    with the theory covered let’s see how to actually run the Qwen2 model!
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Qwen2模型同时使用了Flash Attention和填充优化。现在，我们已经了解了理论，接下来让我们看看如何实际运行Qwen2模型！
- en: Running a 7B Qwen2 Model with HuggingFace Transformers
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用HuggingFace Transformers运行7B Qwen2模型
- en: Set Up
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置
- en: The model that we will experiment with is the `Alibaba-NLP/gte-Qwen2-7B-instruct`
    from Transformers. The model card is [here](https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要实验的模型是来自Transformers的`Alibaba-NLP/gte-Qwen2-7B-instruct`。模型卡可以在[这里](https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct)查看。
- en: 'To perform this experiment, I have used Python 3.10.8 and installed the following
    packages:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行这个实验，我使用了Python 3.10.8并安装了以下包：
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: I ran into some [difficulty](https://github.com/Dao-AILab/flash-attention/issues/246)
    in installing `flash-attn` required to run this model and so had to install the
    specific version listed above. If anyone has a better workaround please let me
    know!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装运行此模型所需的`flash-attn`时，我遇到了一些[困难](https://github.com/Dao-AILab/flash-attention/issues/246)，因此必须安装上述特定版本。如果有人有更好的解决方法，请告诉我！
- en: The Amazon SageMaker instance I used for this experiment is the `ml.g5.2xlarge`.
    It has a 24GB NVIDIA A10G GPU and 32GB of CPU memory and it costs $1.69/hour.
    The below screenshot from AWS shows all the details of the instance
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这个实验中使用的Amazon SageMaker实例是`ml.g5.2xlarge`。它配备有24GB的NVIDIA A10G GPU和32GB的CPU内存，费用为$1.69/小时。以下是AWS的截图，展示了实例的所有详细信息。
- en: '![](../Images/3a03efec36249f1333e76b131a3b3275.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a03efec36249f1333e76b131a3b3275.png)'
- en: SageMaker g5 instance types from [AWS docs](https://aws.amazon.com/sagemaker/pricing/).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[AWS文档](https://aws.amazon.com/sagemaker/pricing/)的SageMaker g5实例类型。
- en: Actually to be precise if you run `nvidia-smi` you will see that the instance
    only has 23GB of GPU memory which is slightly less than advertised. The CUDA version
    on this GPU is 12.2.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，如果你运行`nvidia-smi`，你会看到该实例的GPU内存只有23GB，略低于广告所称的。此GPU的CUDA版本是12.2。
- en: How to Run — In Detail
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何运行——详细步骤
- en: If you look at the model card, one of the suggested ways to use this model is
    via the `sentence-transformers` library as show below
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看模型卡，其中一个建议的使用方式是通过`sentence-transformers`库，如下所示。
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Sentence-transformers is an extension of the Transformers package for computing
    embeddings and is very useful as you can get things working with two lines of
    code. The downside is that you have less control on how to load the model as it
    hides away tokenisation and pooling details. The above code **will not run** on
    our GPU instance because it attempts to load the model in full float32 precision
    which would take 28GB of memory. When the sentence transformer model is initialised
    it checks for available devices (cuda for GPU) and automatically shifts the Pytorch
    model onto the device. As a result it gets stuck after loading 5/7ths of the model
    and crashes.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Sentence-transformers是Transformers包的一个扩展，用于计算嵌入，非常有用，因为你只需要两行代码就可以让事情运行起来。缺点是你对如何加载模型的控制较少，因为它隐藏了标记化和池化的细节。上述代码**无法在我们的GPU实例上运行**，因为它尝试以完整的float32精度加载模型，这需要28GB的内存。当初始化句子变换器模型时，它会检查可用设备（GPU的cuda）并自动将Pytorch模型转移到设备上。结果，在加载了5/7的模型后，它卡住并崩溃了。
- en: Instead we need to be able to load the model in float16 precision before we
    move it onto the GPU. As such we need to use the lower level Transformers library.
    (I am not sure of a way to do it with sentence-transformers but let me know if
    one exists!) We do this as follows
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们需要在将模型移到GPU之前，能够以float16精度加载模型。因此，我们需要使用更底层的Transformers库。（我不确定是否有方法可以使用sentence-transformers实现，但如果有的话请告诉我！）我们按如下方式操作：
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: With the `torch_dtype` parameter we specify that the model should be loaded
    in float16 precision straight away, thus only requiring 14GB of memory. We then
    need to move the model onto the GPU device which is achieved with the `to` method.
    Using the above code, the model takes almost 2min to load!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`torch_dtype`参数，我们指定模型应立即以float16精度加载，从而仅需要14GB的内存。然后，我们需要将模型转移到GPU设备上，这可以通过`to`方法实现。使用上述代码，模型加载几乎需要2分钟！
- en: 'Since we are using `transformers` we need to separately load the tokeniser
    to tokenise the input texts as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的是`transformers`，我们需要单独加载标记化器来标记化输入文本，操作如下：
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The next step is to tokenise the input texts which is done as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是标记化输入文本，操作如下：
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The maximum length of the Qwen2 model is 32678, however as we will see later
    we are unable to run it with such a long sequence on our 24GB GPU due to the additional
    memory requirements. I would recommend reducing this to *no more than 24,000*
    to avoid out of memory errors. Padding ensures that all the inputs in the batch
    have the same length whilst truncation ensures that any inputs longer than the
    maximum length will be truncated. For more information please see the [docs](https://huggingface.co/docs/transformers/en/pad_truncation).
    Finally, we ensure that we return PyTorch tensors (default would be lists instead)
    and move these tensors onto the GPU to be available to pass to the model.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Qwen2模型的最大长度为32678，但正如我们稍后将看到的，由于额外的内存需求，我们无法在24GB的GPU上运行如此长的序列。我建议将其减少到*不超过24,000*，以避免内存不足错误。填充确保批次中的所有输入具有相同的长度，而截断确保任何超过最大长度的输入将被截断。有关更多信息，请参见[文档](https://huggingface.co/docs/transformers/en/pad_truncation)。最后，我们确保返回的是PyTorch张量（默认情况下是列表），并将这些张量转移到GPU，以便可以传递给模型。
- en: The next step is to pass the inputs through our model and perform pooling. This
    is done as follows
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将输入通过我们的模型并进行池化。操作如下：
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'with the `last_token_pool` which looks as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`last_token_pool`，其形式如下：
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `torch.no_grad()` context manager is used to disable gradient calculation,
    since we are not training the model and hence to speed up the inference.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.no_grad()`上下文管理器用于禁用梯度计算，因为我们并不训练模型，因此可以加速推理过程。'
- en: We then pass the tokenised inputs into the transformer model.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们将标记化的输入传递到变换器模型中。
- en: We retrieve the outputs from the last layer of the model with the `last_hidden_state`
    attribute. This is a tensor of shape *(batch_size, max_sequence_length, embedding
    dimension)*. Essentially for each example in the batch the transformer outputs
    embeddings for all the tokens in the sequence.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通过`last_hidden_state`属性从模型的最后一层获取输出。这是一个形状为*(batch_size, max_sequence_length,
    embedding dimension)*的张量。本质上，对于批次中的每个示例，变换器都会为序列中的所有标记输出嵌入。
- en: We now need some way of combining all the token embeddings into a single embedding
    to represent the input text. This is called `pooling` and it is done in the same
    way as during training of the model.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们现在需要一种方法将所有的标记嵌入合并为一个单一的嵌入，以表示输入文本。这称为`池化`，并且与训练模型时的池化方式相同。
- en: In older BERT based models the first token was typically used (which represented
    the special classification [CLS] token). However, the Qwen2 model is LLM-based,
    i.e. transformer decoder based. In the decoder, the tokens are generated auto
    regressively (one after another) and so the last token contains all the information
    encoded about the sentence.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在较旧的基于BERT的模型中，通常使用第一个标记（它代表特殊的分类[CLS]标记）。然而，Qwen2模型是基于LLM的，即基于变换器解码器。在解码器中，标记是自回归生成的（一个接一个），因此最后一个标记包含了关于句子的所有编码信息。
- en: The goal of the `last_token_pool` function is to therefore select the embedding
    of the last generated token (which was not the padding token) for each example
    in the batch.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_token_pool`函数的目的是选择每个示例中最后一个生成的非填充标记的嵌入。'
- en: It uses the `attention_mask` which tells the model which of the tokens are padding
    tokens for each example in the batch (see the [docs](https://huggingface.co/docs/transformers/en/glossary)).
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用了`attention_mask`，该掩码告诉模型在批次中每个示例哪些标记是填充标记（参见[文档](https://huggingface.co/docs/transformers/en/glossary)）。
- en: Annotated Example
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注释示例
- en: 'Let’s look at an example to understand it in a bit more detail. Let’s say we
    want to embed two examples in a single batch:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子，稍微详细地了解一下。假设我们想在一个批次中嵌入两个示例：
- en: '[PRE7]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The outputs of the tokeniser (the `batch_dict` ) will look as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器的输出（`batch_dict`）将如下所示：
- en: '[PRE8]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: From this you can see that the first sentence gets split into four tokens (8687,
    1467, 220, 16), while the second sentence get split into seven tokens. As a result,
    the first sentence is padded (with three padding tokens with id 151643) up to
    length seven — the maximum in the batch. The attention mask reflects this — it
    has three zeros for the first example corresponding to the location of the padding
    tokens. Both the tensors have the same size
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 从中可以看出，第一句话被分成了四个标记（8687，1467，220，16），而第二句话被分成了七个标记。因此，第一句话会被填充（使用三个填充标记，id
    为151643），填充到长度七 —— 这是该批次中的最大长度。注意力掩码反映了这一点 —— 它对第一个示例有三个零，表示填充标记的位置。两个张量的大小是相同的。
- en: '[PRE9]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now passing the `batch_dict` through the model we can retrieve the models last
    hidden state of shape:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在通过模型传递`batch_dict`，我们可以检索到模型的最后一个隐藏状态，其形状为：
- en: '[PRE10]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We can see that this is of shape *(batch_size, max_sequence_length, embedding
    dimension)*. Qwen2 has an embedding dimension of 3584!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，这个形状是*(batch_size, max_sequence_length, embedding dimension)*。Qwen2的嵌入维度为3584！
- en: Now we are in the `last_token_pool` function. The first line checks if padding
    exists, it does it by summing the last “column” of the attention_mask and comparing
    it to the batch_size (given by `attention_mask.shape[0]`. This will only result
    in true if there exists a 1 in all of the attention mask, i.e. if all the examples
    are the same length or if we only have one example.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们进入`last_token_pool`函数。第一行检查是否存在填充，它通过对`attention_mask`的最后一列求和并与`batch_size`（由`attention_mask.shape[0]`给出）进行比较来实现。如果所有的注意力掩码中都存在1，即所有示例的长度相同，或者仅有一个示例时，这才会返回`True`。
- en: '[PRE11]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: If there was indeed no padding we would simply select the last token embedding
    for each of the examples with `last_hidden_states[:, -1]`. However, since we have
    padding we need to select the last non-padding token embedding from each example
    in the batch. In order to pick this embedding we need to get its index for each
    example. This is achieved via
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果确实没有填充，我们只需简单地为每个示例选择最后一个标记的嵌入，方法是使用`last_hidden_states[:, -1]`。但是，由于有填充，我们需要选择每个批次中最后一个非填充标记的嵌入。为了选择这个嵌入，我们需要获取每个示例的索引。通过以下方式可以实现：
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'So now we need to simply index into the tensor, with the correct indices in
    the first two dimensions. To get the indices for all the examples in the batch
    we can use `torch.arange` as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在我们只需要通过张量的正确索引，在前两个维度上进行索引。为了获取批次中所有示例的索引，我们可以使用`torch.arange`，如下所示：
- en: '[PRE13]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then we can pluck out the correct token embeddings for each example using this
    and the indices of the last non padding token:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以使用这个和最后一个非填充标记的索引，从中提取每个示例的正确标记嵌入：
- en: '[PRE14]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: And we get two embeddings for the two examples passed in!
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们得到了传入的两个示例的两个嵌入！
- en: How to Run — TLDR
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何运行 — 简洁版
- en: The full code separated out into functions looks like
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 将完整的代码分成不同函数看起来像是：
- en: '[PRE15]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The `encode_with_qwen_model` returns a numpy array. In order to convert a PyTorch
    tensor to a numpy array we first have to move it off the GPU back onto the CPU
    which is achieved with the `cpu()` method. Please note that if you are planning
    to run long texts you should reduce the batch size to 1 and only embed one example
    at a time (thus reducing the list `texts_to_encode` to length 1).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`encode_with_qwen_model` 返回一个 numpy 数组。为了将 PyTorch 张量转换为 numpy 数组，我们首先必须将其从
    GPU 移回到 CPU，这可以通过 `cpu()` 方法实现。请注意，如果你打算处理较长的文本，应该将批处理大小减少为 1，每次只嵌入一个示例（即将 `texts_to_encode`
    列表的长度缩减为 1）。'
- en: Empirical Memory Usage Tests with Context Length
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用上下文长度进行的经验性内存使用测试
- en: 'Before we saw how the memory usage varies with the input text size from a theoretical
    standpoint. We can also measure how much memory the GPU actually uses when embedding
    texts of different length and verify the scaling empirically! I got the idea from
    this great HuggingFace tutorial: [Getting the most out of LLMs](https://huggingface.co/docs/transformers/v4.35.0/en/llm_tutorial_optimization).'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 之前我们从理论角度看到了内存使用如何随输入文本大小变化。我们还可以实际测量 GPU 在嵌入不同长度的文本时实际使用了多少内存，并通过经验验证这种扩展性！这个想法来源于
    HuggingFace 的一个很棒教程：[充分利用 LLM](https://huggingface.co/docs/transformers/v4.35.0/en/llm_tutorial_optimization)。
- en: To do this we will make use of some extra functions
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们将利用一些额外的函数
- en: '[PRE16]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: as well the
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 以及
- en: '[PRE17]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: function to measure the peak GPU usage. The `flush` function will clear and
    reset the memory after each pass through the model. We will run texts of different
    lengths through the model and print out the peak and effective GPU usage. The
    effective GPU usage is the model size usage subtracted from the total usage which
    gives us an idea of how much extra memory we need to run the text through the
    model.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 用于衡量 GPU 峰值使用情况的函数。`flush` 函数将在每次通过模型后清除并重置内存。我们将运行不同长度的文本，并输出峰值和有效 GPU 使用情况。有效
    GPU 使用是从总使用量中减去模型大小使用量，给我们一个关于运行文本所需额外内存的概念。
- en: 'The full code I used is below:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用的完整代码如下：
- en: '[PRE18]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: and the traceback is
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 以及回溯信息是
- en: '[PRE19]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We can see from this that the Qwen2 model indeed scales linearly with the size
    of the input text. For example, as we double the number of tokens from 8000 to
    16000, the effective memory usage roughly doubles as well. Unfortunately, trying
    to run a sequence of length 32000 through the model resulted in a CUDA OOM error
    so even with a 24GB GPU in float 16 precision we are still unable to utilise the
    full context window of the model.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个结果我们可以看到，Qwen2 模型的确随着输入文本的大小线性扩展。例如，当我们将标记数从 8000 翻倍到 16000 时，有效内存使用量也大致翻倍。不幸的是，尝试运行长度为
    32000 的序列时，出现了 CUDA OOM 错误，因此即使使用 24GB GPU 的浮点 16 精度，我们仍然无法充分利用模型的完整上下文窗口。
- en: Other Aspects
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他方面
- en: Running Qwen2 in fp32
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 fp32 精度下运行 Qwen2
- en: To run the Qwen2 model in full precision we have two options. Firstly we can
    get access to a bigger GPU — for example 40GB would be enough. However, this could
    be costly. Amazon SageMaker for instance does not have an instance with a single
    40GB GPU, instead it has an instance with 8 of them! But that wouldn’t be useful
    as we do not need the other 7 sitting idly. Of course we may also look at other
    providers as well — there are quite a few now and offering competitive prices.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 要以全精度运行 Qwen2 模型，我们有两个选项。首先，我们可以访问更大的 GPU——例如 40GB 应该足够。然而，这可能会很昂贵。例如，Amazon
    SageMaker 就没有单个 40GB GPU 的实例，而是有一个包含 8 个 40GB GPU 的实例！但这并不实用，因为我们不需要其他 7 个 GPU
    空闲等待。当然，我们也可以考虑其他提供商——现在有不少这样的提供商，且价格具有竞争力。
- en: The other option is to run the model on an instance with multiple smaller GPUs.
    The model can be sharded across different GPUs — i.e. different layers of the
    model are put on different GPUs and the data gets moved across the devices during
    inference. To do this with HuggingFace you can do
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是在具有多个较小 GPU 的实例上运行模型。模型可以被分割到不同的 GPU 上——即模型的不同层被放置在不同的 GPU 上，并且数据在推理过程中会在设备之间移动。要使用
    HuggingFace 实现这一点，你可以使用
- en: '[PRE20]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: For more information on how this works see the following [docs](https://huggingface.co/docs/accelerate/en/usage_guides/big_modeling)
    and the [conceptual guide](https://huggingface.co/docs/accelerate/en/concept_guides/big_model_inference).
    The caveat is that this way of doing it is a lot slower — due to the overhead
    of inter-GPU communication, moving data across the different devices to perform
    inference. This implementation is also not optimised, meaning execution on each
    GPU happens sequentially whilst others are sitting idle. If you were embedding
    thousands of texts or training the model you would ideally have all the GPUs constantly
    doing some work.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多关于此如何工作的内容，请参阅以下[文档](https://huggingface.co/docs/accelerate/en/usage_guides/big_modeling)和[概念指南](https://huggingface.co/docs/accelerate/en/concept_guides/big_model_inference)。需要注意的是，这种方式非常慢——因为在不同GPU之间进行通信的开销，数据需要在各个设备之间移动以执行推理。此实现也没有经过优化，这意味着每个GPU的执行是按顺序进行的，而其他GPU处于空闲状态。如果你正在嵌入成千上万的文本或训练模型，理想情况下，你希望所有GPU都在不断工作。
- en: Running Qwen2 on smaller GPUs
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在较小的GPU上运行Qwen2
- en: To run this model on even smaller GPUs you would need to quantise the model.
    Two popular options would be
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在更小的GPU上运行此模型，你需要对模型进行量化。两个流行的选项是：
- en: Via HuggingFace which has various methods available (see [docs](https://huggingface.co/docs/transformers/main/en/quantization/overview)).
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过HuggingFace，它提供了多种方法可供选择（请参阅[文档](https://huggingface.co/docs/transformers/main/en/quantization/overview)）。
- en: Via the [vLLM](https://qwen.readthedocs.io/en/latest/deployment/vllm.html) package.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过[vLLM](https://qwen.readthedocs.io/en/latest/deployment/vllm.html)包。
- en: Conclusion
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In conclusion in this article we saw how to run a 7B LLM based Qwen2 Embedding
    model on a single 24GB GPU. We saw how the size of the model is calculated from
    the number of its parameters and that we need to load the model in float16 precision
    to fit it onto the 24GB GPU. We then saw that extra memory is required to actually
    run an example through the model which depends on the size of the context window
    and varies depending on the underlining attention mechanism used. Finally we saw
    how to do all of this in just a few lines of code with the Transformers library.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 总结：在本文中，我们展示了如何在单个24GB GPU上运行基于7B LLM的Qwen2嵌入模型。我们了解到，模型的大小是根据其参数数量计算的，并且为了将其加载到24GB
    GPU中，我们需要以float16精度加载模型。接着，我们发现实际上运行一个示例时需要额外的内存，这取决于上下文窗口的大小，并且根据所使用的底层注意力机制不同而有所变化。最后，我们展示了如何仅用几行代码，利用Transformers库完成这一切。
