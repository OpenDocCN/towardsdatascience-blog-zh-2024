- en: Efficient Object Detection with SSD and YoLO Models — A Comprehensive Beginner’s
    Guide (Part 3)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SSD和YoLO模型的高效目标检测——初学者综合指南（第三部分）
- en: 原文：[https://towardsdatascience.com/efficient-object-detection-with-ssd-and-yolo-models-a-comprehensive-beginners-guide-part-3-2b0c720175d5?source=collection_archive---------6-----------------------#2024-03-08](https://towardsdatascience.com/efficient-object-detection-with-ssd-and-yolo-models-a-comprehensive-beginners-guide-part-3-2b0c720175d5?source=collection_archive---------6-----------------------#2024-03-08)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/efficient-object-detection-with-ssd-and-yolo-models-a-comprehensive-beginners-guide-part-3-2b0c720175d5?source=collection_archive---------6-----------------------#2024-03-08](https://towardsdatascience.com/efficient-object-detection-with-ssd-and-yolo-models-a-comprehensive-beginners-guide-part-3-2b0c720175d5?source=collection_archive---------6-----------------------#2024-03-08)
- en: Learn about single stage object detection models and different trade-offs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解单阶段目标检测模型及其不同的权衡
- en: '[](https://medium.com/@Rghv_Bali?source=post_page---byline--2b0c720175d5--------------------------------)[![Raghav
    Bali](../Images/49fea68f38f59d0bc39dab484b55684f.png)](https://medium.com/@Rghv_Bali?source=post_page---byline--2b0c720175d5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2b0c720175d5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2b0c720175d5--------------------------------)
    [Raghav Bali](https://medium.com/@Rghv_Bali?source=post_page---byline--2b0c720175d5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@Rghv_Bali?source=post_page---byline--2b0c720175d5--------------------------------)[![Raghav
    Bali](../Images/49fea68f38f59d0bc39dab484b55684f.png)](https://medium.com/@Rghv_Bali?source=post_page---byline--2b0c720175d5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2b0c720175d5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2b0c720175d5--------------------------------)
    [Raghav Bali](https://medium.com/@Rghv_Bali?source=post_page---byline--2b0c720175d5--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2b0c720175d5--------------------------------)
    ·7 min read·Mar 8, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2b0c720175d5--------------------------------)
    ·阅读时间7分钟·2024年3月8日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f031581a744b78beb3abdfa159c88783.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f031581a744b78beb3abdfa159c88783.png)'
- en: Photo by [israel palacio](https://unsplash.com/@othentikisra?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[israel palacio](https://unsplash.com/@othentikisra?utm_source=medium&utm_medium=referral)
    来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In this beginner’s guide series on Object Detection models, we have so far covered
    the [basics of object detection](https://medium.com/towards-data-science/object-detection-basics-a-comprehensive-beginners-guide-part-1-f57380c89b78)
    (part-I) and the [R-CNN family of object detection models](https://medium.com/towards-data-science/exploring-object-detection-with-r-cnn-models-a-comprehensive-beginners-guide-part-2-685bc89775e2)
    (part-II). We will now focus on some of the famous **single-stage object detection**
    models in this article. These models improve upon the speed of inference drastically
    over multi-stage detectors but fall short of the mAP and other detection metrics.
    Let’s get into the details for these models.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇关于目标检测模型的初学者指南系列中，我们已经涵盖了[目标检测基础](https://medium.com/towards-data-science/object-detection-basics-a-comprehensive-beginners-guide-part-1-f57380c89b78)（第一部分）以及[基于R-CNN的目标检测模型](https://medium.com/towards-data-science/exploring-object-detection-with-r-cnn-models-a-comprehensive-beginners-guide-part-2-685bc89775e2)（第二部分）。在本文中，我们将重点介绍一些著名的**单阶段目标检测**模型。这些模型在推理速度上大大优于多阶段检测器，但在mAP和其他检测指标上稍显不足。让我们深入了解这些模型的细节。
- en: Single Shot Multi-box Detectors
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单次多框检测器
- en: The [Single Shot Multibox Detector (SSD)](https://arxiv.org/abs/1512.02325)
    architecture was presented by Liu et. al. back in 2016 as a highly performant
    single-stage object detection model. This paper presented a model which was as
    performant (mAP wise) as Faster R-CNN but faster by a good margin for both training
    and inference activities.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[单次多框检测器（SSD）](https://arxiv.org/abs/1512.02325)架构由Liu等人于2016年提出，作为一种高性能的单阶段目标检测模型。该论文提出的模型在性能（mAP方面）上与Faster
    R-CNN相当，但在训练和推理过程中速度要快得多。'
- en: 'The main difference between the R-CNN family and SSDs is the missing region
    proposal component (RPN). The SSD family of models do not start from a selective
    search algorithm or an RPN to find ROIs. SSD takes a convolutional approach to
    work on this task of object detection. It produces a predefined number of bounding
    boxes and corresponding class scores as its final output. It starts off with a
    large pre-trained network such as VGG-16 which is truncated before any of the
    classification layers start. This is termed as the base-network in SSD terminology.
    The base-network is followed by a unique auxiliary structure to produce the required
    outputs. The following are the key components:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: R-CNN家族与SSD之间的主要区别在于缺少区域提议组件（RPN）。SSD系列模型并不从选择性搜索算法或RPN开始寻找ROI。SSD采用卷积方法来完成这一物体检测任务。它生成一个预定义数量的边界框及其对应的类别分数作为最终输出。它从一个大型的预训练网络（如VGG-16）开始，在任何分类层开始之前就会被截断。这被称为SSD术语中的基础网络。基础网络后面跟着一个独特的辅助结构，用以生成所需的输出。以下是关键组件：
- en: '**Multi-Scale Feature Maps**: the auxiliary structure after the base-network
    is a sequence of convolutional layers. These layers progressively decrease the
    scale or resolution of feature maps. This comes in handy to detect objects of
    different size (relative to the image). The SSD network takes a convolutional
    approach to define class scores as well as relative offset values for the bounding
    boxes. For instance, the network uses a 3x3xp filter on a feature map of m x n
    x p, where p is the number of channels. The model produces an output for each
    cell of m x n where the filter is applied.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多尺度特征图**：基础网络之后的辅助结构是一系列卷积层。这些层逐步减小特征图的尺度或分辨率。这样可以帮助检测不同大小（相对于图像）的物体。SSD网络采用卷积方法来定义类别分数以及边界框的相对偏移值。例如，网络使用3x3xp大小的滤波器，在大小为m
    x n x p的特征图上进行操作，其中p是通道数。模型对m x n的每个单元格生成一个输出，滤波器应用于该位置。'
- en: '**Default Anchor Boxes**: The network makes use of a set of predefined anchor
    boxes (at different scales and aspect ratios). For a given feature map of size
    m x n, k number of such default anchor boxes are applied for each cell. These
    default anchor boxes are termed as priors in case of SSDs. For each prior in each
    cell, the model generates c class scores and 4 coordinates for the bounding boxes.
    Thus, in total for a feature map of size m x n the model generates a total of
    (c+4)kmn outputs. These outputs are generated from feature maps taken from different
    depths of the network which is the key to handle multiple sized objects in a single
    pass.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**默认锚框**：网络利用一组预定义的锚框（具有不同的尺度和长宽比）。对于给定的大小为m x n的特征图，k个此类默认锚框会应用于每个单元格。这些默认锚框在SSD中被称为先验框。对于每个单元格中的每个先验框，模型生成c个类别分数和4个边界框坐标。因此，对于一个大小为m
    x n的特征图，模型总共生成(c+4)kmn个输出。这些输出是从网络不同深度的特征图中生成的，这是处理不同尺寸物体的一次性传递的关键。'
- en: Figure 1 depicts the high-level architecture for SSD with the base-network as
    VGG-16 followed by auxiliary convolutional layers to assist with multi-scale feature
    maps.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1展示了SSD的高层架构，其中基础网络为VGG-16，后面跟着辅助卷积层，以支持多尺度特征图。
- en: '![](../Images/afdbfbf6ad60cfa83db098c10e03447a.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/afdbfbf6ad60cfa83db098c10e03447a.png)'
- en: 'Figure 1: High-level SSD architecture based on VGG-16\. The architecture shows
    extra feature layers added to detect objects of different sizes. Source: Author'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：基于VGG-16的高层SSD架构。该架构展示了额外的特征层，用于检测不同大小的物体。来源：作者
- en: As shown in figure 1, the model generates a total of 8732 predictions which
    are then analyzed through Non-Maximum Suppression algorithm for finally getting
    one bounding box per identified object. In the paper, authors present performance
    metrics (FPS and mAP) for two variants, SSD-300 and SSD-512, where the number
    denotes the size of input image. Both variants are faster and equally performant
    (in terms of mAP) as compared to R-CNNs with SSD-300 achieving way more FPS as
    compared to SSD-512.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如图1所示，模型生成了总共8732个预测，然后通过非最大抑制算法进行分析，最终为每个识别的物体得到一个边界框。在论文中，作者展示了两个变体SSD-300和SSD-512的性能指标（FPS和mAP），其中数字表示输入图像的大小。与R-CNN相比，两个变体都更快，并且在mAP方面表现相同，其中SSD-300比SSD-512有更高的FPS。
- en: As we just discussed, SSD produces a very large number of outputs per feature
    map. This creates a huge imbalance between positive and negative classes (to ensure
    coverage, the number of false positives is very large). To handle this and a few
    other nuances, the authors detail techniques such as *hard negative mining* and
    *data augmentation*. I encourage readers to go through this well drafted paper
    for more details.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们刚才讨论的，SSD在每个特征图上产生大量的输出。这会导致正负类之间的巨大不平衡（为了确保覆盖，假阳性的数量非常大）。为了解决这个问题以及其他一些细节，作者详细描述了*困难负样本挖掘*和*数据增强*等技术。我鼓励读者仔细阅读这篇精心撰写的论文，以获取更多细节。
- en: You Look only Once (YOLO)
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你只看一次（YOLO）
- en: 'In the year 2016, another popular single-stage object detection architecture
    was presented by Redmon et. al. in their paper titled “[You Only Look Once: Unified,
    Real-time Object Detection](https://arxiv.org/abs/1506.02640)”. This architecture
    came up around the same time as SSD but took a slightly different approach to
    tackle object detection using a single-stage model. Just like the R-CNN family,
    the YOLO class of models have also evolved over the years with subsequent versions
    improving upon the previous one. Let us first understand the key aspects of this
    work.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '2016年，Redmon等人在他们的论文《"[You Only Look Once: Unified, Real-time Object Detection](https://arxiv.org/abs/1506.02640)"》中提出了另一种流行的单阶段物体检测架构。这一架构大约与SSD同时出现，但采取了稍微不同的方式，通过单阶段模型来处理物体检测。就像R-CNN系列，YOLO模型也随着时间的推移不断发展，后续版本在前一个版本的基础上有所改进。让我们首先了解这项工作的关键要点。'
- en: YOLO is inspired by the *GoogleNet* architecture for image classification. Similar
    to GoogleNet, YOLO uses 24 convolutional layers pre-trained on the ImageNet dataset.
    The pretrained network uses training images of size 224x224 but once trained,
    the model is used with rescaled inputs of size 448x448\. This rescaling was done
    to ensure that the model picks up small and large objects without issues. It starts
    off by dividing the input image into an S x S grid (paper mentions a grid of 7x7
    for PASCAL VOC dataset). Each cell in the grid predicts B bounding boxes, its
    objectness score along with confidence score for each of the classes. Thus, similar
    to SSD each cell in case YOLO outputs 4 coordinates of the bounding boxes plus
    one objectness score followed by C class prediction probabilities. In total, we
    get S x S x (Bx5 +C) outputs per input image. The number of output bounding boxes
    is extremely high, similar to SSDs. These are reduced to a single bounding box
    per object using NMS algorithm. Figure 2 depicts the overall YOLO setup.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO的灵感来源于用于图像分类的*GoogleNet*架构。与GoogleNet类似，YOLO使用了在ImageNet数据集上预训练的24层卷积层。预训练的网络使用224x224的训练图像，但训练完成后，模型会使用大小为448x448的重新缩放输入图像。这个重新缩放的做法是为了确保模型能够在不出现问题的情况下识别小物体和大物体。YOLO首先将输入图像划分为一个S
    x S的网格（文中提到PASCAL VOC数据集使用的是7x7的网格）。网格中的每个单元预测B个边界框、物体置信度分数以及每个类别的置信度分数。因此，类似于SSD，每个网格单元在YOLO中输出4个边界框的坐标和一个物体置信度分数，接着是C个类别的预测概率。总的来说，每个输入图像会得到S
    x S x (B x 5 + C)个输出。输出的边界框数量非常高，类似于SSD。通过NMS算法，这些边界框会被减少为每个物体的一个边界框。图2展示了YOLO的整体设置。
- en: '![](../Images/a20edd4a61b6b4799bcf51e0411a7671.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a20edd4a61b6b4799bcf51e0411a7671.png)'
- en: 'Figure 2: High-level YOLO architecture which uses 24 convolutional layers followed
    by a few fully connected layers for final prediction. Source: Author'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：高层次的YOLO架构，使用了24层卷积层，后面接几个全连接层进行最终预测。来源：作者
- en: As shown in figure 2, the presence of fully connected layers in YOLO is in contrast
    with SSD, which is entirely convolutional in design. YOLO is built using an opensource
    framework called Darknet and boasts of 45FPS inference speed. Its speed comes
    at the cost of its detection accuracy. Particularly, YOLO has limitations when
    it comes to identification of smaller objects as well as cases where the objects
    are overlapping.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如图2所示，YOLO中的全连接层与完全卷积设计的SSD有所不同。YOLO是使用一个名为Darknet的开源框架构建的，具有45FPS的推理速度。其速度是以牺牲检测精度为代价的。特别是，YOLO在识别小物体以及物体重叠的情况下存在一定的局限性。
- en: '**YOLOv2 or** [**YOLO-9000**](https://arxiv.org/abs/1612.08242) came the very
    next year (2017) with capability to detect 9000 objects (hence the name) at 45–90
    frames per second! One of the minor changes they did was to add an additional
    step before simply rescaling the inputs to 448x448\. Instead, the authors added
    an additional step where once the original classification model (with input size
    224x224) is trained, they rescale the input to 448x448 and fine-tune for a bit
    more. This enables the model to adapt for larger resolution better and thus improve
    detection for smaller objects. Also, the convolutional model used is a 30-layer
    CNN. The second modification was to use anchor boxes and this implementation tries
    to get the size and number calculated based on the characteristics of the training
    data (this is in contrast to SSD which simply uses a predefined list of anchor
    boxes). The final change was to introduce multi-scale training, i.e. instead of
    just training for a given size the author trained the model at different resolutions
    to help the model learn features for different sized objects. The changes helped
    in improving model performance to a good extent (see paper for exact numbers and
    experiments).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**YOLOv2或** [**YOLO-9000**](https://arxiv.org/abs/1612.08242)在2017年发布，具备检测9000个物体的能力（因此得名），每秒可以处理45到90帧！他们所做的一个小改动是，在简单地将输入重新缩放到448x448之前，增加了一个额外的步骤。具体来说，作者在原始分类模型（输入尺寸为224x224）训练完成后，增加了一个步骤，将输入缩放到448x448，并进一步微调。这使得模型能够更好地适应更大的分辨率，从而提升对小物体的检测性能。此外，使用的卷积模型是一个30层的CNN。第二个改动是使用了锚框，并且这一实现尝试根据训练数据的特点来计算锚框的大小和数量（这与SSD不同，后者只是使用预定义的锚框列表）。最后一个变化是引入了多尺度训练，即作者不仅仅在某个固定尺寸下训练模型，而是在不同分辨率下进行训练，帮助模型学习不同尺寸物体的特征。这些改动在很大程度上提升了模型的性能（具体的数字和实验请参见论文）。'
- en: '[**YOLOv3**](https://arxiv.org/abs/1804.02767) was presented in 2018 to overcome
    the mAP shortfall of YOLOv2\. This third iteration of the model used a deeper
    convolutional network with 53 layers as opposed to 24 in the initial version.
    Another 53 layers are stacked on top of the pre-trained model for detection task.
    It also uses residual blocks, skip-connections and up-sampling layers to improve
    performance in general (note that the time the first two version were released
    some of these concepts were still not commonly used). To better handle different
    sized objects, this version makes predictions at different depth of the network.
    The YOLOv3 architecture is depicted in figure 3 for reference.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[**YOLOv3**](https://arxiv.org/abs/1804.02767)于2018年提出，以克服YOLOv2的mAP不足。该模型的第三个版本使用了比初始版本（24层）更深的卷积网络，共有53层。另加上53层堆叠在预训练模型上用于检测任务。它还使用了残差块、跳跃连接和上采样层，以提升整体性能（需要注意的是，在前两个版本发布时，这些概念还不普遍使用）。为了更好地处理不同尺寸的物体，这个版本在网络的不同深度做出预测。YOLOv3的架构如图3所示，供参考。'
- en: '![](../Images/e5d4e94e1461ef845772d48dba43e195.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e5d4e94e1461ef845772d48dba43e195.png)'
- en: 'Figure 3: YOLOv3 high-level architecture with Darknet-53 and multi-scale prediction
    branches. Source: Author'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：YOLOv3高层架构，采用Darknet-53和多尺度预测分支。来源：作者
- en: As shown in figure 3, the model branches off from layer 79 and makes predictions
    at layers 82, 94 and 106 at scales 13x13, 26x26 and 52x52 for large, medium and
    small sized objects respectively. The model uses 9 anchor boxes, 3 for each scale
    to handle different shapes as well. This in-turn increases the total number of
    predictions the model makes per object. The final step is application of NMS to
    reduce the output to just one bounding box per object detected. Another key change
    introduced with YOLOv3 was the use of sigmoid loss for class detection in place
    of softmax. This change helps in handling scenarios where we have overlapping
    objects.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如图3所示，模型从第79层开始分支，并在第82、94和106层分别在尺度为13x13、26x26和52x52的位置进行大、中、小物体的预测。该模型使用了9个锚框，每个尺度使用3个锚框以处理不同的形状。这进一步增加了模型每个物体所做的预测总数。最后一步是应用NMS（非极大值抑制），将输出结果减少为每个检测到的物体只有一个边界框。YOLOv3引入的另一个关键改动是将类检测中的softmax替换为sigmoid损失函数。这一变化有助于处理物体重叠的场景。
- en: While the original author of the YOLO model, Joseph Redmon, ceased his work
    on [object detection](https://twitter.com/pjreddie/status/1230523827446091776)[[1]](#_ftn1),
    the overall computer vision community did not stop. There was a subsequent release
    called [**YOLOv4**](https://arxiv.org/abs/2004.10934) in 2020 followed by another
    fork titled [**YOLOv5**](https://github.com/ultralytics/yolov5) a few weeks later
    (please note that there is no official paper/publication with details of this
    work). While there are open questions on whether these subsequent releases should
    carry the YOLO name, it is interesting to see the ideas being refined and carried
    forward. At the time of writing this article, **YOLOv8** is already available
    for general use while [**YOLOv9**](https://arxiv.org/abs/2402.13616) is pushing
    efficiencies and other benchmarks even further.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管YOLO模型的原始作者Joseph Redmon已经停止了在[目标检测](https://twitter.com/pjreddie/status/1230523827446091776)[[1]](#_ftn1)方面的工作，但整个计算机视觉社区并没有停滞不前。2020年发布了后续版本[**YOLOv4**](https://arxiv.org/abs/2004.10934)，几周后又推出了一个名为[**YOLOv5**](https://github.com/ultralytics/yolov5)的分支（请注意，这项工作没有官方的论文或出版物）。尽管是否应该将这些后续版本称为YOLO还有待讨论，但看到这些思想得到细化并不断发展是非常有趣的。在撰写本文时，**YOLOv8**已经可以供公众使用，而[**YOLOv9**](https://arxiv.org/abs/2402.13616)则进一步推动了效率和其他基准的提升。
- en: This concludes our brief on different object detection models, both multi-stage
    and single stage. We have covered key components and major contributions to better
    understand these models. There are a number of other implementations such as *SPP-Net,
    RetinaNet*, etc. which have a different take on the task of object detection.
    While different, the ideas still conform to the general framework we discussed
    in this series. In the next article, let us get our hands dirty with some object
    detection models.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇简短的介绍总结了不同的目标检测模型，包括多阶段模型和单阶段模型。我们已经覆盖了关键组件和主要贡献，以帮助更好地理解这些模型。还有许多其他实现方法，如*SPP-Net*、*RetinaNet*等，它们对目标检测任务有不同的处理方式。尽管不同，这些方法的思想仍然符合我们在本系列中讨论的总体框架。在下一篇文章中，让我们动手实践一些目标检测模型。
