- en: Simplifying the Python Code for Data Engineering Projects
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简化Python代码以应对数据工程项目
- en: 原文：[https://towardsdatascience.com/simplifying-the-python-code-for-data-engineering-projects-95f0c41dc58a?source=collection_archive---------1-----------------------#2024-06-12](https://towardsdatascience.com/simplifying-the-python-code-for-data-engineering-projects-95f0c41dc58a?source=collection_archive---------1-----------------------#2024-06-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/simplifying-the-python-code-for-data-engineering-projects-95f0c41dc58a?source=collection_archive---------1-----------------------#2024-06-12](https://towardsdatascience.com/simplifying-the-python-code-for-data-engineering-projects-95f0c41dc58a?source=collection_archive---------1-----------------------#2024-06-12)
- en: 'Python tricks and techniques for data ingestion, validation, processing, and
    testing: a practical walkthrough'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据摄取、验证、处理和测试的Python技巧与技术：实用操作指南
- en: '[](https://medium.com/@johnleungTJ?source=post_page---byline--95f0c41dc58a--------------------------------)[![John
    Leung](../Images/ef45063e759e3450fa7f3c32b2f292c3.png)](https://medium.com/@johnleungTJ?source=post_page---byline--95f0c41dc58a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--95f0c41dc58a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--95f0c41dc58a--------------------------------)
    [John Leung](https://medium.com/@johnleungTJ?source=post_page---byline--95f0c41dc58a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@johnleungTJ?source=post_page---byline--95f0c41dc58a--------------------------------)[![John
    Leung](../Images/ef45063e759e3450fa7f3c32b2f292c3.png)](https://medium.com/@johnleungTJ?source=post_page---byline--95f0c41dc58a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--95f0c41dc58a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--95f0c41dc58a--------------------------------)
    [John Leung](https://medium.com/@johnleungTJ?source=post_page---byline--95f0c41dc58a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--95f0c41dc58a--------------------------------)
    ·10 min read·Jun 12, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--95f0c41dc58a--------------------------------)
    ·阅读时间10分钟·2024年6月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Raw data comes from various sources and formats. Before the data can be available
    to answer critical business questions, substantial effort and time are required
    to perform data engineering. While the underlying data infrastructure can vary
    based on the data volume, velocity, and analytics requirements, some fundamental
    code design techniques are still relevant to simplify and streamline various tasks
    across time.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据来自不同的来源和格式。在数据能够用来回答关键的商业问题之前，需要大量的努力和时间来进行数据工程。虽然基础的数据基础设施可能根据数据的量、速度和分析需求而有所不同，但一些基本的代码设计技巧仍然是相关的，可以简化和优化各种任务。
- en: This article will explore different critical parts of general data engineering
    projects, from data ingestion to pipeline testing. Python is the most widely used
    programming language for data engineering, and we will learn how to deal with
    these use cases using built-in functionalities and efficient libraries in Python.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将探讨一般数据工程项目中的不同关键部分，从数据摄取到管道测试。Python是数据工程中最广泛使用的编程语言，我们将学习如何使用Python中的内置功能和高效库来处理这些用例。
- en: '![](../Images/128d4c278ec7ce5302584afdc96ee3f1.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/128d4c278ec7ce5302584afdc96ee3f1.png)'
- en: Photo by [Katerina Pavlyuchkova](https://unsplash.com/@kat_katerina?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Katerina Pavlyuchkova](https://unsplash.com/@kat_katerina?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Imagine you have an online retail shop that sells unique, all-occasion gifts.
    The online shop is so popular that it has a high volume of transactions every
    minute and second. You have the ambition to satisfy more current customers’ needs
    and serve more new customers through analyzing buying habits about the current
    transactions, so this motivates you to dive into the data processing of the transaction
    records as the preparation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下你拥有一家在线零售店，销售独特的各类节庆礼品。这个在线商店非常受欢迎，每分钟每秒的交易量都非常大。你希望通过分析当前交易的购买习惯，满足更多现有客户的需求，并服务更多的新客户，这促使你开始深入数据处理，准备交易记录。
- en: '#0 Mock data'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '#0 模拟数据'
- en: We first mock some transaction data into a file using the [JSON Lines](https://jsonlines.org/)
    (JSONL) text format, where each line is a separate JSON object. This format is
    appealing for data streaming in areas, such as web/ app analytics and log management.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用[JSON Lines](https://jsonlines.org/)（JSONL）文本格式将一些交易数据模拟到文件中，其中每一行都是一个独立的JSON对象。这种格式在数据流处理领域非常有吸引力，例如网页/应用分析和日志管理。
- en: In our file, the data fields belong to various data types. They include the
    customer and product identifiers (in integer/ array format), the payment method
    (in string format), and the total transaction amount (in float number).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的文件中，数据字段属于不同的数据类型。它们包括客户和产品标识符（以整数/数组格式），支付方式（以字符串格式），以及交易总金额（以浮动数字格式）。
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You may discover several single transactions with blank data fields written
    to the file. This mimics the missing data issue, as one of the data quality issues
    often encountered in the real world.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会发现文件中有几条单独的交易数据，其中一些数据字段为空。这模拟了缺失数据的问题，这是现实世界中常见的数据质量问题之一。
- en: '#1 Data ingestion — Yield'
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '#1 数据摄取 — Yield'
- en: To read the transaction records from the file, one of the simplest approaches
    is to loop through the dataset into a list and then convert it into a Pandas DataFrame.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 读取文件中的交易记录，最简单的方法之一是将数据集遍历到一个列表中，然后将其转换为Pandas DataFrame。
- en: This method will work like a charm for the 500,000 transactions in our demo
    dataset. But what if the real-world datasets range from millions to even billions
    of rows? We may sit still for long to wait until the whole computation completes,
    if not leading to memory issues.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法对于我们演示数据集中的500,000条交易非常有效。但如果现实世界中的数据集有数百万甚至数十亿行数据呢？如果不导致内存问题，我们可能需要长时间等待整个计算完成。
- en: Sometimes we don’t care about the entire results and want to process the initial
    results instead before the last record is loaded. In such cases, we can alternatively
    use `yield` to control the flow of a [generator](https://wiki.python.org/moin/Generators).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，我们不关心整个结果，而是希望在加载最后一条记录之前先处理初步结果。在这种情况下，我们可以使用`yield`来控制[生成器](https://wiki.python.org/moin/Generators)的流向。
- en: The generator does not store the entire records in memory. Instead, it gives
    a value one at a time and pauses the function execution until the next value is
    requested.
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 生成器不会将整个记录存储在内存中。相反，它一次只返回一个值，并在请求下一个值之前暂停函数执行。
- en: There are routines of interleaving user codes with library codes. It also enforces
    sequencing, which means you cannot access the second record before reaching the
    first. You can learn more about this concept in the [Pydata talk video](https://www.youtube.com/watch?v=7lmCu8wz8ro),
    which provides a detailed explanation.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在用户代码和库代码交替执行的过程中，还会强制顺序执行，这意味着你不能在到达第一条记录之前访问第二条记录。你可以在[Pydata 讲座视频](https://www.youtube.com/watch?v=7lmCu8wz8ro)中了解更多关于这个概念的详细解释。
- en: 'The yield statement has different practical usages. For example, we can go
    through each line of the file and yield only the non-blank records. Below shows
    how we can execute real-time data filtering:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '`yield`语句有不同的实际用途。例如，我们可以遍历文件中的每一行，只返回非空记录。下面展示了如何执行实时数据过滤：'
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The output of these codes gives a Python generator, a special type of iterator.
    You can use the `next` function in a loop to return the subsequent items one by
    one. Apart from real-time data filtering, another idea is to design a generator
    function that pre-processes data and yields it in a pre-defined batch size, which
    can be parsed straightforwardly to feed a machine-learning model for training.
    And more, we can use it to asynchronously handle web requests and responses, when
    crawling web pages.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这些代码的输出是一个Python生成器，一种特殊类型的迭代器。你可以在循环中使用`next`函数来逐个返回后续项目。除了实时数据过滤，另一个思路是设计一个生成器函数，预处理数据并以预定义的批量大小进行生成，这可以直接解析并供机器学习模型训练使用。而且，我们还可以用它来异步处理网页请求和响应，进行网页爬取。
- en: '#2 Data validation — Pydantic'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '#2 数据验证 — Pydantic'
- en: 'Assume you have a list of JSON data that covers the information of transaction
    records after data ingestion. Here is a sample transaction:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个包含交易记录信息的JSON数据列表，这是数据摄取后的一个示例交易：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For each incoming data, we want to ensure it is being validated, otherwise,
    we will easily hit different types of errors when running the subsequent data
    processing functions. This can be achieved using `pydantic` library.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一条传入的数据，我们希望确保它经过验证，否则在运行后续的数据处理函数时，我们很容易遇到各种错误。这可以通过使用`pydantic`库来实现。
- en: We first define the schema of our data fields using the [Pydantic model](https://docs.pydantic.dev/latest/api/base_model/),
    then validate our JSON data using `model_validate()` function.
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们首先使用[PyDantic模型](https://docs.pydantic.dev/latest/api/base_model/)定义数据字段的模式，然后使用`model_validate()`函数验证我们的JSON数据。
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Sometimes, we find the necessity to apply stricter validation rules. For example,
    the Pydantic base model attempts to coerce string data to an integer if possible.
    To avoid this, you can set `strict=True` at the model level or field level.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们发现需要应用更严格的验证规则。例如，Pydantic基础模型会尝试将字符串数据强制转换为整数。为避免这种情况，可以在模型级别或字段级别设置`strict=True`。
- en: Besides, we can apply custom validation rules to data fields. For example, we
    may want to check if the payment method value is within our expectations. To facilitate
    testing, we manually set the payment method of the sample case to ‘Bitcoin’, which
    is a non-existent option in the online shop, and then use `AfterValidator` to
    embed a function for further checking.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还可以对数据字段应用自定义验证规则。例如，我们可能希望检查支付方式值是否符合我们的预期。为了方便测试，我们手动将示例案例的支付方式设置为“Bitcoin”，这是在线商店中不存在的选项，然后使用`AfterValidator`嵌入一个函数进行进一步检查。
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The validator successfully identifies that the payment method is not within
    the list of possible values. This is done by applying Pydantic’s inner validation
    logic, followed by custom validation functions. The code raises a `ValueError`,
    which populates `ValidationError`.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 验证器成功识别到支付方式不在可能值的列表中。这是通过应用Pydantic的内部验证逻辑，并随后使用自定义验证函数完成的。代码会引发一个`ValueError`，并填充`ValidationError`。
- en: When the error is triggered, we can have follow-up actions for rectification.
    These features help eliminate data errors, thus ensuring the accuracy and completeness
    of our data.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当触发错误时，我们可以采取后续行动进行纠正。这些功能有助于消除数据错误，从而确保数据的准确性和完整性。
- en: '#3 Data processing'
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '#3 数据处理'
- en: (1) Python decorator
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: (1) Python 装饰器
- en: After data validation, we start working with data-intensive functions. There
    is a high chance of encountering lengthy execution times as the data pipeline
    becomes complex. We wish to identify the root cause and optimize the time performance
    of functions. One simple method is to collect two timestamps at the beginning
    and end of every function, and then calculate the time differences one by one.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 数据验证后，我们开始处理数据密集型函数。随着数据管道的复杂化，执行时间可能会变长。我们希望找出根本原因，并优化函数的时间性能。一种简单的方法是，在每个函数的开始和结束时收集两个时间戳，然后逐一计算时间差。
- en: To ensure the code is less cluttered throughout the data pipeline, we can leverage
    the [Python decorator](https://book.pythontips.com/en/latest/decorators.html).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保数据管道中的代码更简洁，我们可以利用[Python装饰器](https://book.pythontips.com/en/latest/decorators.html)。
- en: We first design a Python decorator that measures the execution time. Afterward,
    we annotate any function that requires this feature.
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们首先设计一个Python装饰器来测量执行时间。之后，我们为任何需要此功能的函数添加注解。
- en: For example, you can measure the time taken to categorize prices for all transactions.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您可以测量对所有交易进行分类所需的时间。
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The decorator approach makes the code reusable without changing the source code
    of our original functions. Similarly, we can apply decorator ideas for logging
    function completion or email alerting when jobs encounter failure.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 装饰器方法使得代码在不改变原始函数源代码的情况下可以复用。类似地，我们可以应用装饰器的思想，用于记录函数完成情况或在任务失败时发送邮件警报。
- en: (2) Map, reduce, filter
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: (2) Map、reduce、filter
- en: 'These are commonly used Python array methods that many developers may be familiar
    with. But I still think they are worth mentioning due to several reasons: (1)
    immutable — the functions do not modify values of the original lists; (2) the
    chain flexibility — can apply a combination of functions simultaneously; and (3)
    concise and readable — with only one line of code.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是常用的Python数组方法，许多开发者可能都很熟悉。但我仍然认为它们值得提及，原因有几点：（1）不可变性——这些函数不会修改原始列表的值；（2）链式灵活性——可以同时应用多个函数的组合；（3）简洁可读——只需一行代码。
- en: 'Assume we have a list of JSON objects with only two keys: payment method and
    total amount. Let’s explore some examples of how these functions work.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个包含两个键的JSON对象列表：支付方式和总金额。让我们探索这些函数是如何工作的。
- en: '**Map:** Perform the same operation on all elements in the list (e.g. adding
    a suffix to the values of the payment method).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**Map：** 对列表中的所有元素执行相同的操作（例如，为支付方式的值添加后缀）。'
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Filter**: Obtain a subset of elements that meet a certain condition(s) (e.g.
    only records with cryptocurrency as the payment method).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**Filter**：获取符合某个条件的元素子集（例如，仅记录支付方式为加密货币的记录）。'
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Reduce**: Get a single-valued outcome (e.g. summing or multiplying all elements).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**Reduce**：获取单一值的结果（例如，求和或将所有元素相乘）。'
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We can leverage these functions during the transformation steps in data science
    projects. For example, use `map()` to scale or normalize the data, use `filter()`
    to remove outliers and irrelevant data points, and `reduce()` to generate summary
    statistics.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在数据科学项目的转换步骤中利用这些函数。例如，使用`map()`来缩放或标准化数据，使用`filter()`来去除异常值和不相关的数据点，使用`reduce()`来生成汇总统计数据。
- en: '#4 Data pipeline testing — Pytest'
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '#4 数据管道测试 — Pytest'
- en: Data pipelines often involve data ingestion, data cleansing, and Extract-Transform-Load
    (ETL) operations. The scope of potential errors can be broad and easily overlooked,
    especially as the model flow and result are hard for users to interpret. This
    leads to a heavier reliance on testing efforts by the development team.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管道通常涉及数据摄取、数据清理和提取-转换-加载（ETL）操作。潜在错误的范围可以非常广泛且容易被忽视，尤其是当模型流和结果难以被用户解读时。这导致开发团队更依赖于测试工作。
- en: It is common to conduct unit testing to ascertain each component of the machine
    learning system performs as expected.
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 通常会进行单元测试，以确保机器学习系统的每个组件按预期执行。
- en: 'One of the most popular Python testing frameworks is `[Pytest](https://docs.pytest.org/en/stable/contents.html)`.
    Imagine we want to ensure the high-quality of transformed data that both technical
    teams and decision-makers can trust. We can test for the function that we have
    gone through about categorizing transaction prices. To achieve this, we need to
    prepare two Python files:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最受欢迎的Python测试框架之一是`[Pytest](https://docs.pytest.org/en/stable/contents.html)`。假设我们希望确保转化后的数据质量，技术团队和决策者都可以信任这些数据。我们可以测试我们之前处理的关于分类交易价格的函数。为了实现这一点，我们需要准备两个Python文件：
- en: '**feature_engineering.py**: The file containing the previously built function'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**feature_engineering.py**：包含之前构建的函数的文件'
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**test_feature_engineering.py**: The file with the “test_” prefix, which Pytest
    will recognize for testing purposes only.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**test_feature_engineering.py**：带有“test_”前缀的文件，Pytest仅在测试过程中识别此文件。'
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The assert statements above ensure that the new ‘totalAmtCat’ data field is
    added with a non-blank value, and the original data fields are not affected. By
    executing the command `Pytest`, we can know that our test has passed!
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的assert语句确保新的“totalAmtCat”数据字段已添加且其值非空，同时原始数据字段不受影响。通过执行命令`Pytest`，我们可以知道测试已经通过！
- en: '![](../Images/f5d42bc912f608940f41c8519fbb3498.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f5d42bc912f608940f41c8519fbb3498.png)'
- en: Pytest result — Test passed (Image by author)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Pytest结果 — 测试通过（图片由作者提供）
- en: 'For a more advanced case, let''s say we have three functions with the following
    sequences: `load_data`, `clean_data`, and `add_features`. How should we design
    the test file to validate the output of these functions one by one?'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个更高级的案例，假设我们有三个函数，顺序如下：`load_data`、`clean_data`和`add_features`。我们应该如何设计测试文件来逐个验证这些函数的输出？
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We should define a fixed baseline for initialization, such as a JSON Lines file
    with sample test cases. Here we use `@pytest.fixture`decorator, which works similarly
    to the `time_decorator`we discussed earlier in the Python decorator section. This
    decorator helps prevent repeatedly initializing the sample files. For the remaining
    codes, we involve several test functions to run the pipeline functions and use
    assert statements to detect logical errors.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该为初始化定义一个固定的基准，例如一个包含样本测试用例的JSON Lines文件。在这里，我们使用`@pytest.fixture`装饰器，它类似于我们在Python装饰器部分讨论的`time_decorator`。这个装饰器有助于避免反复初始化样本文件。对于剩下的代码，我们涉及几个测试函数来运行管道函数，并使用assert语句来检测逻辑错误。
- en: Wrapping it up
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结一下
- en: 'We came across several critical aspects of data engineering projects, and explored
    how to simplify and streamline the Python code for efficiency and readability:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遇到了数据工程项目中的几个关键方面，并探索了如何简化和优化Python代码以提高效率和可读性：
- en: Data ingestion, by using `yield` to process large datasets with efficient memory
    usage.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据摄取，使用`yield`处理大数据集，同时实现高效的内存使用。
- en: Data validation, by leveraging `Pydantic` to validate data fields based on schema
    and customized value patterns.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据验证，利用`Pydantic`根据模式和自定义值模式验证数据字段。
- en: Data processing, by applying Python decorator and built-in libraries to enable
    additional functionalities without repeated codes.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据处理，通过应用Python装饰器和内置库来启用额外的功能，而无需重复代码。
- en: Pipeline testing, by using `Pytest` to ensure high-quality function outputs
    throughout the workflow.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用`Pytest`进行管道测试，以确保工作流中各个环节的函数输出质量。
- en: Before you go
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在你继续之前
- en: If you enjoy this reading, I invite you tofollow my [Medium page](https://medium.com/@johnleungTJ)
    and [LinkedIn page](https://www.linkedin.com/in/john-leung-639800115/). By doing
    so, you can stay updated with exciting content related to data science side projects,
    Machine Learning Operations (MLOps) demonstrations, and project management methodologies.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这篇文章，我邀请你关注我的[Medium页面](https://medium.com/@johnleungTJ)和[LinkedIn页面](https://www.linkedin.com/in/john-leung-639800115/)。通过这种方式，你可以随时获取有关数据科学副项目、机器学习操作（MLOps）演示以及项目管理方法的最新内容。
- en: '[](/performing-customer-analytics-with-langchain-and-llms-0af4ea38f7b5?source=post_page-----95f0c41dc58a--------------------------------)
    [## Performing Customer Analytics with LangChain and LLMs'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/performing-customer-analytics-with-langchain-and-llms-0af4ea38f7b5?source=post_page-----95f0c41dc58a--------------------------------)
    [## 使用LangChain和LLMs进行客户分析'
- en: Discover the potentials and constraints of LangChain for customer analytics,
    accompanied by practical implementation…
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索LangChain在客户分析中的潜力与局限性，并附带实际的实现过程…
- en: towardsdatascience.com](/performing-customer-analytics-with-langchain-and-llms-0af4ea38f7b5?source=post_page-----95f0c41dc58a--------------------------------)
    [](/feature-engineering-for-time-series-using-pyspark-on-databricks-02b97d62a287?source=post_page-----95f0c41dc58a--------------------------------)
    [## Feature Engineering for Time-Series Using PySpark on Databricks
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/performing-customer-analytics-with-langchain-and-llms-0af4ea38f7b5?source=post_page-----95f0c41dc58a--------------------------------)
    [](/feature-engineering-for-time-series-using-pyspark-on-databricks-02b97d62a287?source=post_page-----95f0c41dc58a--------------------------------)
    [## 使用PySpark在Databricks上进行时间序列特征工程
- en: 'Discover the potentials of PySpark for time-series data: Ingest, extract, and
    visualize data, accompanied by practical…'
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索PySpark在时间序列数据中的潜力：导入、提取和可视化数据，并附带实际操作…
- en: towardsdatascience.com](/feature-engineering-for-time-series-using-pyspark-on-databricks-02b97d62a287?source=post_page-----95f0c41dc58a--------------------------------)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/feature-engineering-for-time-series-using-pyspark-on-databricks-02b97d62a287?source=post_page-----95f0c41dc58a--------------------------------)
