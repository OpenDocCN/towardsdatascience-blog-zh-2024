- en: Under-trained and Unused tokens in Large Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型中的未充分训练和未使用标记
- en: 原文：[https://towardsdatascience.com/under-trained-and-unused-tokens-in-large-language-models-db5fa17589ec?source=collection_archive---------14-----------------------#2024-10-01](https://towardsdatascience.com/under-trained-and-unused-tokens-in-large-language-models-db5fa17589ec?source=collection_archive---------14-----------------------#2024-10-01)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/under-trained-and-unused-tokens-in-large-language-models-db5fa17589ec?source=collection_archive---------14-----------------------#2024-10-01](https://towardsdatascience.com/under-trained-and-unused-tokens-in-large-language-models-db5fa17589ec?source=collection_archive---------14-----------------------#2024-10-01)
- en: Existence of under-trained and unused tokens and Identification Techniques using
    GPT-2 Small as an Example
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 以GPT-2 Small为例，探讨未使用和未充分训练的标记的存在及其识别技术
- en: '[](https://medium.com/@vanillaxiangshuyang?source=post_page---byline--db5fa17589ec--------------------------------)[![Shuyang
    Xiang](../Images/36a5fd18fd9b7b88cb41094f09b83882.png)](https://medium.com/@vanillaxiangshuyang?source=post_page---byline--db5fa17589ec--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--db5fa17589ec--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--db5fa17589ec--------------------------------)
    [Shuyang Xiang](https://medium.com/@vanillaxiangshuyang?source=post_page---byline--db5fa17589ec--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@vanillaxiangshuyang?source=post_page---byline--db5fa17589ec--------------------------------)[![Shuyang
    Xiang](../Images/36a5fd18fd9b7b88cb41094f09b83882.png)](https://medium.com/@vanillaxiangshuyang?source=post_page---byline--db5fa17589ec--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--db5fa17589ec--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--db5fa17589ec--------------------------------)
    [Shuyang Xiang](https://medium.com/@vanillaxiangshuyang?source=post_page---byline--db5fa17589ec--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--db5fa17589ec--------------------------------)
    ·7 min read·Oct 1, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--db5fa17589ec--------------------------------)
    ·7分钟阅读·2024年10月1日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/44f9ba6050c2fa14c0bd03bb05f6f2fb.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44f9ba6050c2fa14c0bd03bb05f6f2fb.png)'
- en: 'Image generated by [deepai](https://deepai.org/machine-learning-mode) from
    the text: under-trained tokenization of LLMs'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由[deepai](https://deepai.org/machine-learning-mode)根据文本生成：LLM的未充分训练标记化
- en: 'Introduction: Unused and Under-trained Tokens'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言：未使用和未充分训练的标记
- en: 'We have observed the existence of both unused and under-trained tokens in exploration
    of transformer based large language models (LLMs) such as ChatGPT, of which the
    tokenization and the model training stay as two separate processes. Unused tokens
    and under-trained tokens have the following different behaviors:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在探索基于变换器的大型语言模型（如ChatGPT）时观察到存在未使用和未充分训练的标记，其中标记化和模型训练是两个独立的过程。未使用的标记和未充分训练的标记具有以下不同的行为：
- en: '**Unused Tokens** exist in the LLM’s vocabulary and were included during the
    process training but were not sufficiently seen.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**未使用的标记**存在于大型语言模型的词汇中，并在训练过程中被包含，但没有被充分看到。'
- en: '**Under-Trained Tokens** may or may not exist in the LLM’s vocabulary and were
    not at all represented in the training data.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**未充分训练的标记**可能存在于大型语言模型的词汇中，也可能不存在，并且在训练数据中完全没有出现。'
- en: Ideally, the two types of tokens would have very low probabilities to be generated,
    or equivalently, have extremely negative logit values, so that they should not
    be generated by the LLMs. However, in practice, users have still found some unused
    tokens with important logits and the model can sometimes unfortunately predict
    them. This can lead to undesirable behaviors in LLMs.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，这两种类型的标记应该具有极低的生成概率，或者等效地，具有极其负的logit值，这样它们就不应该由大型语言模型生成。然而，在实际中，用户仍然发现一些未使用的标记具有重要的logit值，且模型有时不幸会预测到它们。这可能会导致大型语言模型出现不良行为。
- en: Let us consider an LLM which unexpectedly generates nonsensical or inappropriate
    text because of some tokens that was never trained during the model training.
    Such occurrences can sometimes cause serious consequences, such as hallucination,
    leading to lack of accuracy and appropriateness.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个大型语言模型，它由于一些在模型训练过程中从未训练过的标记，意外地生成了无意义或不恰当的文本。这样的情况有时会导致严重后果，例如幻觉现象，导致准确性和适当性缺失。
- en: We claim this issue is due to the separation between tokenization and the training
    process of LLMs. In general, these two aspects are never trained together and
    it did happen that a token in the model’s vocabulary fails to be trained and appears
    randomly in the output of the model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为这个问题是由于令牌化和LLM训练过程之间的分离。通常，这两个方面从未一起训练过，确实发生了模型词汇表中的某个令牌未能被训练，并且随机出现在模型的输出中。
- en: In this article, we will demonstrate the existence of unused tokens, including
    under-trained ones, with some simple experiments using GPT-2 Small . We will also
    discuss techniques for identifying under-trained tokens.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将通过一些简单的实验，使用GPT-2 Small演示未使用令牌的存在，包括那些训练不足的令牌。我们还将讨论识别训练不足令牌的技术。
- en: 'Existence of Unused Tokens: Experiments on GPT-2 Small'
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 未使用令牌的存在：关于GPT-2 Small的实验
- en: In many LLMs, including GPT-2 Small on which our experiments are executed, there
    exist unused tokens, that is, tokens existing in the LLM’s vocabulary and were
    included during the process training but were not sufficiently seen.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多LLM中，包括我们实验所用的GPT-2 Small，存在未使用的令牌，即在LLM的词汇表中存在的令牌，这些令牌在训练过程中被包含在内，但没有被充分看到。
- en: 'In the following examples, we give two cases proving the existence of unused
    tokens:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们给出了证明未使用令牌存在的两个案例：
- en: 'Example 1: Reproducing Unused Tokens'
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例 1：重现未使用的令牌
- en: In this experiment, we aim to show how GPT-2 Small struggles to reproduce unused
    tokens, even with very straightforward instructions. Let us now consider the following
    unused token:`"ú"` (`\u00fa`). We would like to instruct GPT-2 small to repeat
    the token exactly as given by the input.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实验中，我们旨在展示GPT-2 Small在重复未使用的令牌时的困难，即使是在非常简单的指令下。现在让我们考虑以下未使用的令牌：`"ú"` (`\u00fa`)。我们希望指示GPT-2
    Small准确地重复输入给定的令牌。
- en: 'This is a very simple task: For the given input token `"ú"`, the model have
    to give the same token as output.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的任务：对于给定的输入令牌`"ú"`，模型需要给出相同的令牌作为输出。
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As you can see in the code above, we have designed a prompt as n-shot examples,
    instructing the model to give exactly the same specific token `"ú"` . What we
    see is that the model fails to predict this token: it gives some grabled text
    as `"Output: - ß, *- *-, "` . In contrast, when we tested the same task with common
    tokens such as `"a"` , the model successfully predicted the correct output, showing
    the stark difference in performance between frequently encountered and unused
    tokens.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '如上面的代码所示，我们设计了一个作为n-shot示例的提示，指示模型给出完全相同的特定令牌`"ú"`。我们看到的是，模型未能预测到这个令牌：它给出了一些杂乱无章的文本，如`"Output:
    - ß, *- *-, "`。相比之下，当我们用常见的令牌如`"a"`进行相同任务测试时，模型成功地预测了正确的输出，显示出常见令牌和未使用令牌之间在表现上的显著差异。'
- en: 'Example 2: Token Repetition'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例 2：令牌重复
- en: We now consider the range of unused tokens from indices 177 to 188, the range
    of unused tokens for GPT2 [1].
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在考虑未使用令牌的范围，从索引177到188，这是GPT2 [1]的未使用令牌范围。
- en: 'Our goal now is to generate sequences of repeated random tokens and evaluate
    the model’s performance on the repeated sequencee. As discussed in my previous
    blog post, **“**[**How to Interpret GPT-2 Small: Mechanistic Interpretability
    on Prediction of Repeated Tokens,**](https://medium.com/towards-data-science/how-to-interpret-gpt2-small-76e0536a588a)**”**
    transformer-based LLMs have a strong ability to recognize and predict repeated
    patterns, even for small models such as GPT2 small.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在的目标是生成重复的随机令牌序列，并评估模型在重复序列上的表现。正如我在之前的博客文章中讨论的，**“**[**如何解释GPT-2 Small：在重复令牌预测中的机械解释性**](https://medium.com/towards-data-science/how-to-interpret-gpt2-small-76e0536a588a)**”**，基于变换器的LLM在识别和预测重复模式方面具有强大的能力，即使对于像GPT-2
    Small这样的小模型也是如此。
- en: For example, when the model encounters an ‘A’, it searches for the previous
    occurrence of ‘A’ or a token closely related to ‘A’ in the embedding space. It
    then identifies the subsequent token, ‘B’, and predicts that the next token following
    ‘A’ will be ‘B’ or a token similar to ‘B’ in the embedding space.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当模型遇到一个‘A’时，它会在嵌入空间中寻找之前出现的‘A’或与‘A’紧密相关的令牌。然后它识别下一个令牌‘B’，并预测紧随‘A’之后的令牌将是‘B’或嵌入空间中与‘B’相似的令牌。
- en: We begin by defining a function, `generate_repeated_tokens` which generated
    a sequence whose second half repeats the first half.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义了一个函数`generate_repeated_tokens`，它生成一个序列，其中后半部分重复前半部分。
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Next, we define the `run_and_cache_model_repeated_tokens` function, which runs
    the model on the generated repeated tokens, returning the logits and caching the
    activations. We will use only logits here.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义了`run_and_cache_model_repeated_tokens`函数，该函数在生成的重复标记上运行模型，返回logits并缓存激活值。我们在这里只使用logits。
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now we run the model using the defined `run_and_cache_model_repeated_tokens`
    function, generating both the tokens and the associated logits with the following
    code:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用定义好的`run_and_cache_model_repeated_tokens`函数运行模型，生成标记和关联的logits，使用以下代码：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: After running the model, we analyze the log probabilities of the predicted tokens
    for both halves of the repeated sequences. We observed a mean log probability
    of -17.270 for the first half and -19.675 for the second half for token sequences
    varying in indices 177 to 188.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行模型后，我们分析了预测标记的log概率，分为两部分对重复序列的log概率进行分析。我们观察到标记序列的log概率均值分别为：前半部分为-17.270，后半部分为-19.675，索引范围为177到188。
- en: '![](../Images/97825741fd58c75f3218c336e64f899b.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97825741fd58c75f3218c336e64f899b.png)'
- en: 'Image by author: log prob of repeated tokens ranging in 177 to 188'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片：重复标记的log概率范围在177到188之间
- en: 'On the other hand, doing the same experiment with a commonly used range of
    tokens gives different results: When examining token indices 100 to 110, we observe
    significantly better performance in the second half, with log probabilities of
    -0.971 compared to -7.327 in the first half.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，使用常见标记范围进行相同实验会得到不同的结果：当检查标记索引100到110时，我们观察到后半部分的表现显著更好，log概率为-0.971，相比之下前半部分的log概率为-7.327。
- en: '![](../Images/41a683197974c901ef65f126648227cf.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41a683197974c901ef65f126648227cf.png)'
- en: 'Image by author: log prob of repeated tokens ranging in 100 to 111'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片：重复标记的log概率范围在100到111之间
- en: Under-trained Tokens
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练不足的标记
- en: The world of LLM would ideally have less surprise if all unused tokens had significantly
    negative logits and the model would therefore never produce weird texts.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有未使用的标记具有显著的负logits，那么LLM的世界理想情况下会少一些惊讶，因此模型也就不会生成奇怪的文本。
- en: The reality is, unfortunately much more complex. The fact that the creation
    of the tokenizer and the training of LLM do not happen at the same time lead sometimes
    to undertrained tokens which are, the the culprits of unexpected behaviors of
    LLMs.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，现实要复杂得多。由于tokenizer的创建和LLM的训练并不是同时进行的，这导致了有时会出现训练不足的标记，而这些标记恰是LLM产生意外行为的罪魁祸首。
- en: An example of undertrained tokens is:`_SolidGoldMagikarp`[1] which was seen
    sometimes in ChatGPT’s outputs. Now we would like to prove the existence of under-trained
    tokens in the case of GPT-2 Small.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一个训练不足标记的例子是：`_SolidGoldMagikarp`[1]，它有时出现在ChatGPT的输出中。现在我们想证明GPT-2 Small模型中也存在训练不足的标记。
- en: 'Example: Reproducing Unused Tokens'
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例：再现未使用的标记
- en: In our former experiment of reproducing unused tokens within the GPT-2 Small
    model, we proved that the token `"ú"` has hardly any chance to be generated by
    the model.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们以前使用GPT-2 Small模型再现未使用标记的实验中，我们证明了标记`"ú"`几乎没有被模型生成的机会。
- en: 'Now we slice the logits tensor after runing the model to isolate the outputs
    to the unused token indices ranging from 177 to 188:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们在运行模型后切片logits张量，以隔离未使用标记的输出，标记索引范围从177到188：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Interestingly, we have observed that the logit values for some tokens in this
    “unused” range reached approximately -1.7, which is to say, there is a probability
    of around 0.18 for some unused tokens being generated.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，我们观察到在这个“未使用”范围内，某些标记的logit值约为-1.7，这意味着一些未使用的标记被生成的概率大约为0.18。
- en: This finding highlights the model’s possiblity to assign non-negligible probabilities
    to some unused tokens, despite they are uncommonly used in most of the context.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这一发现突出了模型可能将非忽略性的概率分配给一些未使用的标记，尽管它们在大多数上下文中很少被使用。
- en: Identifying Under-Trained Tokens
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别训练不足的标记
- en: 'In recent years, researchers have proposed techniques to automatically identify
    under-trained tokens in LLMs. Works in this area include those by Watkins and
    Rumbelow (2023), and Fell (2023) among which one very interesting approach to
    identifying under-trained tokens involves analyzing the output embeddings E_{out}
    generated by the model:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，研究人员提出了自动识别大语言模型（LLMs）中训练不足的标记（tokens）的方法。该领域的研究包括Watkins和Rumbelow（2023年）以及Fell（2023年）的工作，其中一种非常有趣的识别训练不足标记的方法是分析模型生成的输出嵌入（E_{out}）：
- en: The the method computes the average embedding vector of the unused tokens and
    uses cosine distances to measure how the vector is similar to all tokens’e embedding
    vector of the model. Tokens with cosine distances close to the mean embeddings
    are thus marked as candidates of under-trained tokens. Please check more details
    in [1].
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法计算未使用的标记的平均嵌入向量，并使用余弦距离来衡量该向量与模型中所有标记的嵌入向量的相似度。因此，余弦距离接近平均嵌入向量的标记被标记为潜在的训练不足标记。更多细节请参见[1]。
- en: Conclusion
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In conclusion, this blog posts discusses the under-trained tokens LLMs. We do
    some experiments with GPT-2 Small to illustrate that under-trained tokens can
    unexpectedly affect model outputs, giving sometimes unpredictable and undesirable
    behaviors. Recent researches propose methods in identifying under-trained tokens
    accordingly. For those interested in more details of my implementation, you can
    check my accompanying [notebook](https://colab.research.google.com/drive/13h_X8uvkai49yUWW2cndKDVMg-8RqlIb#scrollTo=ckKDzTnF1r3l).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，这篇博客文章讨论了LLM中的训练不足标记。我们通过GPT-2 Small做了一些实验，说明训练不足的标记可能意外地影响模型输出，有时会产生不可预测和不希望出现的行为。最近的研究提出了相应的检测训练不足标记的方法。对于那些对我的实现有兴趣的人，您可以查看我附带的[笔记本](https://colab.research.google.com/drive/13h_X8uvkai49yUWW2cndKDVMg-8RqlIb#scrollTo=ckKDzTnF1r3l)。
- en: Reference
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Land, S., & Bartolo, M. (2024). *Fishing for Magikarp: Automatically detecting
    under-trained tokens in large language models*. arXiv. [https://doi.org/10.48550/arXiv.2405.05417](https://doi.org/10.48550/arXiv.2405.05417).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Land, S., & Bartolo, M. (2024). *Fishing for Magikarp: 自动检测大型语言模型中的训练不足标记*。arXiv。
    [https://doi.org/10.48550/arXiv.2405.05417](https://doi.org/10.48550/arXiv.2405.05417)。'
- en: '[2] Jessica Rumbelow and Matthew Watkins. 2023\. SolidGoldMagikarp (plus, prompt
    generation). Blog Post.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Jessica Rumbelow 和 Matthew Watkins. 2023\. SolidGoldMagikarp（加上，提示生成）。博客文章。'
- en: '[3] Martin Fell. 2023\. A search for more ChatGPT / GPT3.5 / GPT-4 “unspeakable”
    glitch tokens. Blog post.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Martin Fell. 2023\. 寻找更多ChatGPT / GPT-3.5 / GPT-4 “不可言说”的故障标记。博客文章。'
