- en: Attention (Is Not) All You Need
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意力（并非）你所需要的一切
- en: 原文：[https://towardsdatascience.com/attention-is-not-all-you-need-ef7b6956d425?source=collection_archive---------4-----------------------#2024-11-19](https://towardsdatascience.com/attention-is-not-all-you-need-ef7b6956d425?source=collection_archive---------4-----------------------#2024-11-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/attention-is-not-all-you-need-ef7b6956d425?source=collection_archive---------4-----------------------#2024-11-19](https://towardsdatascience.com/attention-is-not-all-you-need-ef7b6956d425?source=collection_archive---------4-----------------------#2024-11-19)
- en: An alternative approach to the transformer model for text generation
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一种替代的文本生成变换器模型方法
- en: '[](https://medium.com/@thejoshtaylor?source=post_page---byline--ef7b6956d425--------------------------------)[![Josh
    Taylor](../Images/e3c9cb25df3e0b870d28b5844cd3ddff.png)](https://medium.com/@thejoshtaylor?source=post_page---byline--ef7b6956d425--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ef7b6956d425--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ef7b6956d425--------------------------------)
    [Josh Taylor](https://medium.com/@thejoshtaylor?source=post_page---byline--ef7b6956d425--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@thejoshtaylor?source=post_page---byline--ef7b6956d425--------------------------------)[![Josh
    Taylor](../Images/e3c9cb25df3e0b870d28b5844cd3ddff.png)](https://medium.com/@thejoshtaylor?source=post_page---byline--ef7b6956d425--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ef7b6956d425--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ef7b6956d425--------------------------------)
    [Josh Taylor](https://medium.com/@thejoshtaylor?source=post_page---byline--ef7b6956d425--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ef7b6956d425--------------------------------)
    ·7 min read·Nov 19, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ef7b6956d425--------------------------------)
    ·阅读时间：7分钟·2024年11月19日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/8840625d116b00a261505a81da53e8a0.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8840625d116b00a261505a81da53e8a0.png)'
- en: Can fractal patterns help us to create a more efficient text generation model?
    Photo by [Giulia May](https://unsplash.com/@giuliamay?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 分形图案能否帮助我们创造出更高效的文本生成模型？照片由[Giulia May](https://unsplash.com/@giuliamay?utm_source=medium&utm_medium=referral)拍摄，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Since the release of ChatGPT at the end of November 2022, LLMs (Large Language
    Models) have, almost, become a household name.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 自2022年11月底ChatGPT发布以来，LLM（大型语言模型）几乎成为了家喻户晓的名字。
- en: '![](../Images/2dd39a5162081d74bf681e8a8b5c6908.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2dd39a5162081d74bf681e8a8b5c6908.png)'
- en: 'Worldwide search interest for ‘LLM’. Source: [Google Trends](https://trends.google.com/trends/explore?date=today+5-y&q=llm&hl=en-GB)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 全球范围内对“LLM”的搜索兴趣。来源：[Google Trends](https://trends.google.com/trends/explore?date=today+5-y&q=llm&hl=en-GB)
- en: There is good reason for this; their success lies in their architecture, particularly
    the **attention mechanism.** It allows the model to compare every word they process
    to *every other* word.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做是有充分理由的；它们的成功归功于其架构，特别是**注意力机制**。它使模型能够将每个处理的单词与*其他所有*单词进行比较。
- en: This gives LLMs the extraordinary capabilities in understanding and generating
    human-like text that we are all familiar with.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得LLM在理解和生成类人文本方面具有我们熟知的非凡能力。
- en: However, these models are not without flaws. They demand immense computational
    resources to train. For example, Meta’s Llama 3 model took 7.7 million GPU hours
    of training[1]. Moreover, their reliance on enormous datasets — spanning trillions
    of tokens — raises questions about scalability, accessibility, and environmental
    impact.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些模型并非没有缺陷。它们训练时需要巨大的计算资源。例如，Meta的Llama 3模型训练耗时770万个GPU小时[1]。此外，它们对庞大数据集的依赖——涵盖了数万亿个标记——引发了关于可扩展性、可获取性以及环境影响的疑问。
- en: Despite these challenges, ever since the paper ‘Attention is all you need’ in
    mid 2017, much of the recent progress in AI has focused on scaling attention mechanisms
    further, rather than exploring fundamentally new architectures.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些挑战，自从2017年中期发布论文《Attention is all you need》以来，人工智能领域的最新进展大多集中在进一步扩展注意力机制，而非探索根本全新的架构。
