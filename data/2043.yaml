- en: Graph RAG — A Conceptual Introduction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图结构 RAG — 概念介绍
- en: 原文：[https://towardsdatascience.com/graph-rag-a-conceptual-introduction-41cd0d431375?source=collection_archive---------0-----------------------#2024-08-22](https://towardsdatascience.com/graph-rag-a-conceptual-introduction-41cd0d431375?source=collection_archive---------0-----------------------#2024-08-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/graph-rag-a-conceptual-introduction-41cd0d431375?source=collection_archive---------0-----------------------#2024-08-22](https://towardsdatascience.com/graph-rag-a-conceptual-introduction-41cd0d431375?source=collection_archive---------0-----------------------#2024-08-22)
- en: Graph RAG answers the big questions where text embeddings won’t help you.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图结构 RAG 解答了文本嵌入无法帮助你解决的重大问题。
- en: '[](https://jakobpoerschmann.medium.com/?source=post_page---byline--41cd0d431375--------------------------------)[![Jakob
    Pörschmann](../Images/b130445bf9ac471b70070eb4a2dc6b64.png)](https://jakobpoerschmann.medium.com/?source=post_page---byline--41cd0d431375--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--41cd0d431375--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--41cd0d431375--------------------------------)
    [Jakob Pörschmann](https://jakobpoerschmann.medium.com/?source=post_page---byline--41cd0d431375--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://jakobpoerschmann.medium.com/?source=post_page---byline--41cd0d431375--------------------------------)[![Jakob
    Pörschmann](../Images/b130445bf9ac471b70070eb4a2dc6b64.png)](https://jakobpoerschmann.medium.com/?source=post_page---byline--41cd0d431375--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--41cd0d431375--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--41cd0d431375--------------------------------)
    [Jakob Pörschmann](https://jakobpoerschmann.medium.com/?source=post_page---byline--41cd0d431375--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--41cd0d431375--------------------------------)
    ·8 min read·Aug 22, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--41cd0d431375--------------------------------)
    ·阅读时间 8 分钟 ·2024年8月22日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Retrieval Augmented Generation (RAG) has dominated the discussion around making
    Gen AI applications useful since ChatGPT’s advent exploded the AI hype. The idea
    is simple. LLMs become especially useful once we connect them to our private data.
    A foundational model, that everyone has access to, combined with our domain-specific
    data as the secret sauce results in a potent, unique tool. Just like in the human
    world, AI systems seem to develop into an economy of experts. General knowledge
    is a useful base, but expert knowledge will work out your AI system’s unique selling
    proposition.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）自从 ChatGPT 的出现引爆了人工智能热潮以来，便主导了关于使生成式 AI 应用变得实用的讨论。这个理念很简单。一旦将大规模语言模型（LLMs）与我们的私人数据相连接，它们便变得尤为有用。一个人人都能访问的基础模型，结合我们的领域特定数据作为“秘密武器”，就会形成一个强大且独特的工具。就像在人类社会中一样，AI
    系统似乎发展成了一个专家经济。通用知识是一个有用的基础，但专家知识则能够发挥 AI 系统的独特卖点。
- en: '*Recap: RAG itself does not yet describe any specific architecture or method.
    It only depicts the augmentation of a given generation task with an arbitrary
    retrieval method. The original RAG paper (*[*Retrieval-Augmented Generation for
    Knowledge-Intensive NLP Tasks by Lewis et. al.*](https://arxiv.org/abs/2005.11401)*)
    compares a two-tower embedding approach with bag-of-words retrieval.*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*总结：RAG 本身并没有描述任何特定的架构或方法。它只是展示了如何通过任意检索方法增强给定的生成任务。原始的 RAG 论文（*[*Retrieval-Augmented
    Generation for Knowledge-Intensive NLP Tasks by Lewis et. al.*](https://arxiv.org/abs/2005.11401)*)
    比较了双塔嵌入方法与词袋检索方法。*'
- en: Local and Global Questions
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本地和全局问题
- en: 'Text Embedding-based retrieval has been described in many occurrences. It already
    allows our LLM application to answer questions based on the content of a given
    knowledge base extremely reliably. The core strength of Text2Vec retrieval remains:
    Extracting a given fact represented in the embedded knowledge base and formulating
    an answer to the user query that is grounded using that extracted fact. However,
    text embedding search also comes with major challenges. Usually, every text embedding
    represents one specific chunk from the unstructured dataset. The nearest neighbor
    search finds embeddings that represent chunks semantically similar to the incoming
    user query. That also means the search is semantic but still highly specific.
    Thus candidate quality is highly dependent on query quality. Furthermore, embeddings
    represent the content mentioned in your knowledge base. This does not represent
    cases in which you are looking to answer questions that require an abstraction
    across documents or concepts within a document in your knowledge base.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 基于文本嵌入的检索已经在许多场合中得到了描述。它已经使我们的LLM应用程序能够极为可靠地基于给定知识库的内容回答问题。Text2Vec检索的核心优势仍然是：从嵌入的知识库中提取一个给定事实，并使用该提取的事实来构造对用户查询的回答。然而，文本嵌入搜索也面临着重大挑战。通常，每个文本嵌入表示来自非结构化数据集的一个特定块。最近邻搜索找到的嵌入代表了与用户查询语义相似的块。这也意味着搜索是语义上的，但仍然非常具体。因此，候选答案的质量在很大程度上依赖于查询的质量。此外，嵌入表示的是知识库中提到的内容。这并不代表你在寻找那些需要跨文档或在文档内跨概念抽象的问题的情况。
- en: For example, imagine a knowledge base containing the bios of all past Nobel
    Peace Prize winners. Asking the Text2Vec-RAG system “Who won the Nobel Peace Prize
    2023?” would be an easy question to answer. This fact is well represented in the
    embedded document chunks. Thus the final answer can be grounded in the correct
    context. On the other hand, the RAG system might struggle by asking “Who were
    the most notable Nobel Peace Prize winners of the last decade?”. We might be successful
    after adding more context such as “Who were the most notable Nobel Peace Prize
    winners fighting against the Middle East conflict?”, but even that will be a difficult
    one to solve solely based on text embeddings (given the current quality of embedding
    models). Another example is whole dataset reasoning. For example, your user might
    be interested in asking your LLM application “What are the top 3 topics that recent
    Nobel Peace Prize winners stood up for?”. Embedded chunks do not allow reasoning
    across documents. Our nearest neighbor search is looking for a specific mention
    of “the top 3 topics that recent Nobel Peace Prize winners stood up for” in the
    knowledge base. If this is not included in the knowledge base, any purely text-embedding-based
    LLM application will struggle and most likely fail to answer this question correctly
    and especially exhaustively.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设有一个知识库，其中包含所有过去诺贝尔和平奖得主的简历。向Text2Vec-RAG系统提问“谁赢得了2023年诺贝尔和平奖？”是一个很容易回答的问题。这个事实在嵌入的文档块中得到了很好的表示。因此，最终的答案可以基于正确的上下文得出。另一方面，RAG系统可能会在回答“过去十年最著名的诺贝尔和平奖得主是谁？”时遇到困难。如果我们添加更多的上下文信息，比如“谁是过去十年中在中东冲突中作出突出贡献的诺贝尔和平奖得主？”我们可能会成功，但即使如此，仅凭文本嵌入（考虑到当前嵌入模型的质量）仍然难以完全解决这个问题。另一个例子是整个数据集推理。例如，用户可能会对你的LLM应用程序提出问题：“最近的诺贝尔和平奖得主支持的前三大议题是什么？”嵌入的文档块不允许跨文档推理。我们的最近邻搜索在知识库中寻找特定提到“最近的诺贝尔和平奖得主支持的前三大议题”的内容。如果知识库中没有包含这一内容，任何完全基于文本嵌入的LLM应用程序都将面临困难，并且很可能无法正确甚至全面地回答这个问题。
- en: We need an alternative retrieval method that allows us to answer these “Global”,
    aggregative questions in addition to the “Local” extractive questions. **Welcome
    to Graph RAG!**
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一种替代的检索方法，使我们能够回答这些“全局”的汇总性问题，除了“局部”的提取性问题之外。**欢迎使用图形RAG！**
- en: Knowledge Graphs are a semi-structured, hierarchical approach to organizing
    information. Once information is organized as a graph we can infer information
    about specific nodes, but also their relationships and neighbors. The graph structure
    allows reasoning on a global dataset level because nodes and the connections between
    them can span across documents. Given this graph, we can also analyze neighboring
    nodes and communities of nodes that are more tightly connected within each other
    than they are to other nodes. A community of nodes can be expected to roughly
    cover one topic of interest. Abstracting across the community nodes and their
    connections can give us an abstract understanding of concepts within this topic.
    Graph RAG uses this understanding of communities within a knowledge graph to propose
    context for a given user query.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱是一种半结构化的、层次化的信息组织方法。一旦信息被组织为图谱，我们不仅可以推断出特定节点的信息，还可以推断出它们的关系和邻居。图谱结构使得对全球数据集层面的推理成为可能，因为节点和它们之间的连接可以跨越不同的文档。有了这个图谱，我们还可以分析邻近的节点和更紧密连接的节点社区，这些节点之间的联系比它们与其他节点的联系更为紧密。一个节点的社区可以大致覆盖一个感兴趣的主题。通过抽象社区节点及其连接，我们可以对该主题中的概念有一个抽象的理解。图谱RAG利用对知识图谱中社区的理解，来为特定用户查询提供上下文。
- en: 'A Graph RAG pipeline will usually follow the following steps:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一个图谱RAG管道通常会遵循以下步骤：
- en: Graph Extraction
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图谱提取
- en: Graph Storage
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图谱存储
- en: Community detection
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 社区检测
- en: Community report generation
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 社区报告生成
- en: Map Reduce for final context building
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终上下文构建的映射归约
- en: '![](../Images/69adac3aea8dbd9a2b2d831a0bd129bc.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69adac3aea8dbd9a2b2d831a0bd129bc.png)'
- en: 'GraphRAG Logic Visualized — Source: Image by the author'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图谱RAG逻辑可视化 — 来源：作者提供的图片
- en: Graph Extraction
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图谱提取
- en: 'The process of building abstracted understanding for our unstructured knowledge
    base begins with extracting the nodes and edges that will build your knowledge
    graph. You automate this extraction via an LLM. The biggest challenge of this
    step is deciding which concepts and relationships are relevant to include. To
    give an example for this highly ambiguous task: Imagine you are extracting a knowledge
    graph from a document about Warren Buffet. You could extract his holdings, place
    of birth, and many other facts as entities with respective edges. Most likely
    these will be highly relevant information for your users. (With the right document)
    you could also extract the color of his tie at the last board meeting. This will
    (most likely) be irrelevant to your users. It is crucial to specify the extraction
    prompt to the application use case and domain. This is because the prompt will
    determine what information is extracted from the unstructured data. For example,
    if you are interested in extracting information about people, you will need to
    use a different prompt than if you are interested in extracting information about
    companies.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 建立我们非结构化知识库的抽象理解的过程始于提取构建知识图谱的节点和边。你可以通过一个大型语言模型（LLM）自动化这个提取过程。此步骤的最大挑战是决定哪些概念和关系是相关的，值得包括在内。为了举一个例子来说明这个高度模糊的任务：假设你正在从一篇关于沃伦·巴菲特的文档中提取知识图谱。你可以提取他的持股、出生地以及许多其他事实作为实体，并附上相应的边。这些信息很可能对你的用户来说是高度相关的。（如果文档内容合适）你也可以提取他在最后一次董事会会议上领带的颜色。这些信息（很可能）对用户来说是无关的。至关重要的是，需要根据应用程序的使用场景和领域来指定提取提示。这是因为提示将决定从非结构化数据中提取哪些信息。例如，如果你有兴趣提取有关人的信息，你需要使用与提取公司信息不同的提示。
- en: The easiest way to specify the extraction prompt is via [multishot prompting](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/few-shot-examples).
    This involves giving the LLM multiple examples of the desired input and output.
    For instance, you could give the LLM a series of documents about people and ask
    it to extract the name, age, and occupation of each person. The LLM would then
    learn to extract this information from new documents. A more advanced way to specify
    the extraction prompt is through LLM fine-tuning. This involves training the LLM
    on a dataset of examples of the desired input and output. This can cause better
    performance than multishot prompting, but it is also more time-consuming.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 指定提取提示的最简单方法是通过[多轮提示](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/few-shot-examples)。这涉及给LLM多个期望的输入和输出示例。例如，你可以给LLM一系列关于人物的文档，并要求它提取每个人的姓名、年龄和职业。LLM随后会学会从新文档中提取这些信息。指定提取提示的更高级方法是通过LLM微调。这涉及在包含期望输入和输出示例的数据集上训练LLM。这可能比多轮提示带来更好的表现，但也更耗时。
- en: '[Here is the Microsoft graphrag extraction prompt](https://github.com/microsoft/graphrag/blob/main/graphrag/index/graph/extractors/graph/prompts.py).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[这是微软的graphrag提取提示](https://github.com/microsoft/graphrag/blob/main/graphrag/index/graph/extractors/graph/prompts.py)。'
- en: Graph Storage
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图存储
- en: You designed a solid extraction prompt and tuned your LLM. Your extraction pipeline
    works. Next, you will have to think about storing these results. Graph databases
    (DB) such as Neo4j and Arango DB are the straightforward choice. However, extending
    your tech stack by another db type and learning a new query language (e.g. Cypher/Gremlin)
    can be time-consuming. From my high-level research, there are also no great serverless
    options available. If handling the complexity of most Graph DBs was not enough,
    this last one is a killer for a serverless lover like myself. There are alternatives
    though. With a little creativity for the right data model, graph data can be formatted
    as semi-structured, even strictly structured data. To get you inspired [I coded
    up graph2nosql as an easy Python interface to store and access your graph dataset
    in your favorite NoSQL db](https://github.com/jakobap/graph2nosql).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你设计了一个可靠的提取提示并调整了你的LLM。你的提取管道正常工作。接下来，你需要考虑如何存储这些结果。图数据库（DB）如Neo4j和Arango DB是直接的选择。然而，增加另一种数据库类型并学习一种新的查询语言（例如Cypher/Gremlin）可能会非常耗时。从我的高层研究来看，目前没有很好的无服务器选项可用。如果处理大多数图数据库的复杂性还不够，这最后一个问题对我这种无服务器爱好者来说简直是致命的。不过，还是有替代方案的。通过一点创意，使用合适的数据模型，图数据可以被格式化为半结构化，甚至是严格结构化的数据。为了激发你的灵感，[我编写了graph2nosql作为一个简单的Python接口，用于在你最喜欢的NoSQL数据库中存储和访问你的图数据集](https://github.com/jakobap/graph2nosql)。
- en: The data model defines a format for Nodes, Edges, and Communities. Store all
    three in separate collections. Every node, edge, and community finally identify
    via a unique identifier (UID). Graph2nosql then implements a couple of essential
    operations needed when working with knowledge graphs such as adding/removing nodes/edges,
    visualizing the graph, detecting communities, and more.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 数据模型定义了节点、边和社区的格式。将这三者存储在不同的集合中。每个节点、边和社区最终通过唯一标识符（UID）进行识别。Graph2nosql然后实现了在处理知识图时所需的几个基本操作，如添加/删除节点/边、可视化图、检测社区等。
- en: '![](../Images/aa7383736c841102143abec1b5805c2f.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa7383736c841102143abec1b5805c2f.png)'
- en: 'graph2nosql data model — Source: Image by the author'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: graph2nosql数据模型 — 来源：作者提供的图片
- en: Community Detection
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 社区检测
- en: Once the graph is extracted and stored, the next step is to identify communities
    within the graph. Communities are clusters of nodes that are more tightly connected
    than they are to other nodes in the graph. This can be done using various community
    detection algorithms.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦图被提取并存储，下一步就是识别图中的社区。社区是比其他节点更紧密连接的节点集群。可以使用各种社区检测算法来完成此任务。
- en: One popular community detection algorithm is the Louvain algorithm. The Louvain
    algorithm works by iteratively merging nodes into communities until a certain
    stopping criterion is met. The stopping criterion is typically based on the modularity
    of the graph. Modularity is a measure of how well the graph is divided into communities.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一种流行的社区检测算法是Louvain算法。Louvain算法通过迭代地将节点合并为社区，直到满足某个停止准则。停止准则通常基于图的模块度。模块度是衡量图如何被划分为社区的指标。
- en: '**Other popular community detection algorithms include:**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**其他流行的社区检测算法包括：**'
- en: Girvan-Newman Algorithm
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Girvan-Newman算法
- en: Fast Unfolding Algorithm
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速展开算法
- en: Infomap Algorithm
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Infomap 算法
- en: Community Report Generation
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 社区报告生成
- en: Now use the resulting communities as a base to generate your community reports.
    Community reports are summaries of the nodes and edges within each community.
    These reports can be used to understand graph structure and identify key topics
    and concepts within the knowledge base. In a knowledge graph, every community
    can be understood to represent one “topic”. Thus every community might be a useful
    context to answer a different type of questions.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用结果社区作为生成社区报告的基础。社区报告是每个社区内节点和边缘的总结。这些报告可以用来理解图结构，并识别知识库中的关键主题和概念。在知识图中，每个社区都可以理解为代表一个“主题”。因此，每个社区可能是回答不同类型问题的有用上下文。
- en: Aside from summarizing multiple nodes’ information, community reports are the
    first abstraction level across concepts and documents. One community can span
    over the nodes added by multiple documents. That way you’re building a “global”
    understanding of the indexed knowledge base. For example, from your Nobel Peace
    Prize winner dataset, you probably extracted a community that represents all nodes
    of the type “Person” that are connected to the node “Nobel Peace prize” with the
    edge description “winner”.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 除了总结多个节点的信息外，社区报告还是跨概念和文档的第一个抽象层次。一个社区可以跨越由多个文档添加的节点。这样，你就在构建一个关于已索引知识库的“全局”理解。例如，在你的诺贝尔和平奖获奖者数据集中，你可能提取了一个社区，代表所有与节点“诺贝尔和平奖”通过边缘描述“获奖者”连接的“人物”类型的节点。
- en: A great idea from the Microsoft Graph RAG implementation are “findings”. On
    top of the general community summary, these findings are more detailed insights
    about the community. For example, for the community containing all past Nobel
    Peace Prize winners, one finding could be some of the topics that connected most
    of their activism.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 来自微软图 RAG 实现的一个好点子是“发现”。在一般的社区总结之上，这些发现是关于社区的更详细的见解。例如，对于包含所有历史诺贝尔和平奖获奖者的社区，一个发现可能是连接他们大部分行动主义的某些主题。
- en: Just as with graph extraction, community report generation quality will be highly
    dependent on the level of domain and use case adaptation. To create more accurate
    community reports, use multishot prompting or LLM fine-tuning.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 就像图提取一样，社区报告生成的质量将高度依赖于领域和用例的适应程度。为了创建更准确的社区报告，可以使用多次提示或 LLM 微调。
- en: '[Here the Microsoft graphrag community report generation prompt](https://github.com/microsoft/graphrag/blob/main/graphrag/index/graph/extractors/community_reports/prompts.py).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[这里是微软图 graphrag 社区报告生成提示](https://github.com/microsoft/graphrag/blob/main/graphrag/index/graph/extractors/community_reports/prompts.py)。'
- en: Map Reduce for final context building
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Map Reduce 用于最终上下文构建
- en: At query time you use a map-reduce pattern to first generate intermediate responses
    and a final response.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在查询时，你使用 Map-Reduce 模式首先生成中间响应和最终响应。
- en: In the map step, you combine every community-userquery pair and generate an
    answer to the user query using the given community report. In addition to this
    intermediate response to the user question, you ask the LLM to evaluate the relevance
    of the given community report as context for the user query.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在映射步骤中，你将每个社区-用户查询对结合起来，并使用给定的社区报告生成用户查询的答案。除了这个中间响应之外，你还会请求 LLM 评估给定社区报告作为用户查询的上下文相关性。
- en: In the reduce step you then order the relevance scores of the generated intermediate
    responses. The top k relevance scores represent the communities of interest to
    answer the user query. The respective community reports, potentially combined
    with the node and edge information are the context for your final LLM prompt.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在归约步骤中，你将对生成的中间响应的相关性评分进行排序。排名前 k 的相关性评分代表回答用户查询的感兴趣社区。相应的社区报告，可能结合节点和边缘信息，构成最终
    LLM 提示的上下文。
- en: 'Closing thoughts: Where is this going?'
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结语：这将走向何方？
- en: Text2vec RAG leaves obvious gaps when it comes to knowledge base Q&A tasks.
    **Graph RAG can close these gaps and it can do so well!** The additional abstraction
    layer via community report generation adds significant insights into your knowledge
    base and builds a global understanding of its semantic content. This will save
    teams an immense amount of time screening documents for specific pieces of information.
    If you are building an LLM application it will enable your users to ask the big
    questions that matter. Your LLM application will suddenly be able to seemingly
    think around the corner and understand what is going on in your user’s data instead
    of “only” quoting from it.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在知识库问答任务中，Text2vec RAG存在明显的空白。**图形RAG可以弥补这些空白，而且做得非常好！** 通过生成社区报告的额外抽象层为你的知识库提供了重要的见解，并建立了其语义内容的全球理解。这将为团队节省大量筛选文档中特定信息的时间。如果你正在构建一个LLM应用程序，它将使你的用户能够提出真正重要的大问题。你的LLM应用程序将突然能够“预见”问题的本质，理解用户数据中的情况，而不仅仅是从中引用内容。
- en: On the other hand, a Graph RAG pipeline (in its raw form as described here)
    requires significantly more LLM calls than a text2vec RAG pipeline. Especially
    the generation of community reports and intermediate answers are potential weak
    points that are going to cost a lot in terms of dollars and latency.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，一个图形RAG管道（如本文所述的原始形式）比text2vec RAG管道需要更多的LLM调用。特别是社区报告和中间答案的生成是潜在的薄弱环节，这将花费大量的资金和延迟。
- en: As so often in search you can expect the industry around advanced RAG systems
    to move towards a hybrid approach. Using the right tool for a specific query will
    be essential when it comes to scaling up RAG applications. A classification layer
    to separate incoming local and global queries could for example be imaginable.
    Maybe the community report and findings generation is enough and adding these
    reports as abstracted knowledge into your index as context candidates suffices.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在搜索领域中经常出现的情况一样，你可以预期，围绕先进RAG系统的行业将朝着混合方法发展。针对特定查询使用合适的工具将是扩展RAG应用程序时至关重要的。例如，可以想象使用一个分类层来区分本地和全球查询。也许生成社区报告和发现就足够了，将这些报告作为抽象的知识加入到你的索引中，作为上下文候选项就足够了。
- en: Luckily the perfect RAG pipeline is not solved yet and your experiments will
    be part of the solution. I would love to hear about how that is going for you!
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，完美的RAG管道尚未解决，你的实验将成为解决方案的一部分。我很想听听你的进展如何！
