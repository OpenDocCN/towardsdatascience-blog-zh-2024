- en: Not All HNSW Indices Are Made Equally
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不是所有HNSW索引都一样
- en: 原文：[https://towardsdatascience.com/not-all-hnsw-indices-are-made-equaly-6bc0d7efd8c7?source=collection_archive---------6-----------------------#2024-07-03](https://towardsdatascience.com/not-all-hnsw-indices-are-made-equaly-6bc0d7efd8c7?source=collection_archive---------6-----------------------#2024-07-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/not-all-hnsw-indices-are-made-equaly-6bc0d7efd8c7?source=collection_archive---------6-----------------------#2024-07-03](https://towardsdatascience.com/not-all-hnsw-indices-are-made-equaly-6bc0d7efd8c7?source=collection_archive---------6-----------------------#2024-07-03)
- en: Overcoming Major HNSW Challenges to Improve the Efficiency of Your AI Production
    Workload
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 克服主要的HNSW挑战，提升你的AI生产工作负载效率
- en: '[](https://medium.com/@noamschwartz1?source=post_page---byline--6bc0d7efd8c7--------------------------------)[![Noam
    Schwartz](../Images/c5bf11b1267a95242290ca6105eb0b16.png)](https://medium.com/@noamschwartz1?source=post_page---byline--6bc0d7efd8c7--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6bc0d7efd8c7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6bc0d7efd8c7--------------------------------)
    [Noam Schwartz](https://medium.com/@noamschwartz1?source=post_page---byline--6bc0d7efd8c7--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@noamschwartz1?source=post_page---byline--6bc0d7efd8c7--------------------------------)[![Noam
    Schwartz](../Images/c5bf11b1267a95242290ca6105eb0b16.png)](https://medium.com/@noamschwartz1?source=post_page---byline--6bc0d7efd8c7--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6bc0d7efd8c7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6bc0d7efd8c7--------------------------------)
    [Noam Schwartz](https://medium.com/@noamschwartz1?source=post_page---byline--6bc0d7efd8c7--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6bc0d7efd8c7--------------------------------)
    ·7 min read·Jul 3, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6bc0d7efd8c7--------------------------------)
    ·7分钟阅读·2024年7月3日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/446cf9b1c18d035d4f8cc283ce4da2fd.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/446cf9b1c18d035d4f8cc283ce4da2fd.png)'
- en: Photo by [Talha Riaz](https://www.pexels.com/@talhariaz/) on [Pexels](https://www.pexels.com/photo/blue-high-rise-building-2590716/)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Talha Riaz](https://www.pexels.com/@talhariaz/)提供，来源于[Pexels](https://www.pexels.com/photo/blue-high-rise-building-2590716/)
- en: The Hierarchical Navigable Small World [(HNSW) algorithm](https://arxiv.org/pdf/1603.09320)
    is known for its efficiency and accuracy in high-scale data searches, making it
    a popular choice for search tasks and AI/LLM applications like RAG. However, setting
    up and maintaining an HNSW index comes with its own set of challenges. Let’s explore
    these challenges, offer some ways to overcome them, and even see how we can kill
    two birds with one stone by addressing just one of them.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 层次可导航小世界[(HNSW)算法](https://arxiv.org/pdf/1603.09320)因其在大规模数据搜索中的高效性和准确性而广受欢迎，是搜索任务和像RAG这样的AI/LLM应用的常见选择。然而，设置和维护HNSW索引也伴随着一系列挑战。让我们一起探讨这些挑战，提供一些解决方案，甚至看看如何通过解决其中一个问题来一举两得。
- en: Memory Consumption
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存消耗
- en: Due to its hierarchical structure of embeddings, one of the primary challenges
    of HNSW is its high memory usage. But not many realize that the memory issue extends
    beyond the memory required to store the initial index. This is because, as an
    HNSW index is modified, the memory required to store nodes and their connections
    increases even more. This will be explained in greater depth in a later section.
    Memory awareness is important since the more memory you need for your data, the
    longer it will take to compute (search) over it, and the more expensive it will
    get to maintain your workload.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其嵌入的层次结构，HNSW的一个主要挑战是其高内存使用量。但很少有人意识到，内存问题不仅仅局限于存储初始索引所需的内存。这是因为，当HNSW索引被修改时，存储节点及其连接所需的内存会进一步增加。稍后的部分将对此进行更详细的解释。内存意识至关重要，因为数据需要的内存越多，计算（搜索）的时间就越长，维护工作负载的成本也会越高。
- en: Build Time
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建时间
- en: '![](../Images/98416ec9d38b70c2717f3dc96115a88d.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/98416ec9d38b70c2717f3dc96115a88d.png)'
- en: Photo by [Andrea De Santis](https://unsplash.com/@santesson89?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Andrea De Santis](https://unsplash.com/@santesson89?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In the process of creating an index, nodes are added to the graph according
    to how close they are to other nodes on the graph. For every node, a dynamic list
    of its closest neighbors is kept at each level of the graph. This process involves
    iterating over the list and performing similarity searches to determine if a node’s
    neighbors are closer to the query. This computationally heavy iterative process
    significantly increases the overall build time of the index, negatively impacting
    your users’ experience and costing you more in cloud usage expenses.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建索引的过程中，节点会根据它们与图中其他节点的接近程度被添加到图中。对于每个节点，在图的每个层级都会保持一个包含其最近邻居的动态列表。这个过程涉及对列表进行迭代，并执行相似度搜索，以确定节点的邻居是否更接近查询。这一计算密集型的迭代过程显著增加了索引的总体构建时间，负面影响用户体验，并导致云计算使用成本的增加。
- en: Parameter Tuning
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参数调优
- en: 'HNSW requires predefined configuration parameters in it’s build process. Optimizing
    HNSW those parameters: *M* (the number of connections per node), and *ef_construction*
    (the size of the dynamic list for the nearest neighbors which is used during the
    index construction) is crucial for balancing search speed, accuracy and the use
    of memory. Incorrect parameter settings can lead to poor performance and increased
    production costs. Fine-tuning these parameters is unique for every index and is
    a continuous process that requires often re-building of indices.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: HNSW 在构建过程中需要预定义的配置参数。优化 HNSW 的这些参数：*M*（每个节点的连接数）和 *ef_construction*（用于索引构建过程中最近邻的动态列表大小）对平衡搜索速度、准确性和内存使用至关重要。不正确的参数设置可能导致性能下降，并增加生产成本。微调这些参数对于每个索引都是独特的，并且是一个持续的过程，通常需要频繁重建索引。
- en: Rebuilding Indices
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重建索引
- en: '![](../Images/4976c359067323c4cf9ecbe955530e3f.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4976c359067323c4cf9ecbe955530e3f.png)'
- en: Photo by [Robin Jonathan Deutsch](https://unsplash.com/@rodeutsch?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [Robin Jonathan Deutsch](https://unsplash.com/@rodeutsch?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Rebuilding an HNSW index is one of the most resource-intensive aspects of using
    HNSW in production workloads. Unlike traditional databases, where data deletions
    can be handled by simply deleting a row in a table, using HNSW in a vector database
    often requires a complete rebuild to maintain optimal performance and accuracy.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 重建 HNSW 索引是将 HNSW 应用于生产工作负载时最耗资源的环节之一。与传统数据库不同，传统数据库可以通过简单地删除表中的一行来处理数据删除，而在向量数据库中使用
    HNSW 往往需要完全重建索引，以保持最佳的性能和准确性。
- en: Why is Rebuilding Necessary?
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么重建是必要的？
- en: Because of its layered graph structure, HNSW is not inherently designed for
    dynamic datasets that change frequently. Adding new data or deleting existing
    data is essential for maintaining updated data, especially for use cases like
    RAG, which aims to improve search relevence.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其分层图结构，HNSW 本身并不适合处理频繁变化的动态数据集。添加新数据或删除现有数据对于保持数据的最新状态至关重要，尤其是在像 RAG 这样的用例中，RAG
    旨在提高搜索相关性。
- en: Most databases work on a concept called “hard” and “soft” deletes. Hard deletes
    permanently remove data, while soft deletes flag data as ‘to-be-deleted’ and remove
    it later. The issue with soft deletes is that the to-be-deleted data still uses
    significant memory until it is permanently removed. This is particularly problematic
    in vector databases that use HNSW, where memory consumption is already a significant
    issue.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数数据库采用名为“硬删除”和“软删除”的概念。硬删除是永久删除数据，而软删除将数据标记为“待删除”，然后稍后移除。软删除的问题在于，待删除的数据在被永久移除之前仍然占用大量内存。在使用
    HNSW 的向量数据库中尤其如此，因为内存消耗本身已经是一个显著的问题。
- en: HNSW creates a graph where nodes (vectors) are connected based on their proximity
    in the vector space, and traversing on an HNSW graph is done like a skip-list.
    In order to support that, the layers of the graph are designed so that some layers
    have very few nodes. When vectors are deleted, especially those on layers that
    have very few nodes that serve as critical connectors in the graph, the whole
    HNSW structure can become fragmented. This fragmentation may lead to nodes (or
    layers) that are disconnected from the main graph, which require rebuilding of
    the entire graph, or at the very least will result in a degradation in the efficiency
    of searches.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: HNSW创建了一个图，其中节点（向量）是根据它们在向量空间中的接近度连接的，遍历HNSW图就像跳表一样。为了支持这一点，图的层次结构被设计成某些层具有非常少的节点。当向量被删除时，特别是那些位于节点极少的层，这些层在图中作为关键连接点时，整个HNSW结构可能会变得碎片化。这种碎片化可能导致某些节点（或层）与主图断开连接，这就需要重建整个图，或者至少会导致搜索效率下降。
- en: HNSW then uses a soft-delete technique, which marks vectors for deletion but
    does not immediately remove them. This approach lowers the expense of frequent
    complete rebuilds, although periodic reconstruction is still needed to maintain
    the graph’s optimal state.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: HNSW随后使用软删除技术，这种技术将向量标记为删除，但并不立即移除它们。这种方法减少了频繁完全重建的开销，尽管仍然需要定期重建，以保持图的最优状态。
- en: Addressing HNSW Challenges
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决HNSW的挑战
- en: 'So what ways do we have to manage those challenges? Here are a few that worked
    for me:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们有哪些方法可以应对这些挑战呢？以下是一些对我有效的方法：
- en: '**Vector Quantization** — Vector quantization (VQ) is a process that maps k-dimensional
    vectors from a vector space ℝ^k into a finite set of vectors known as codewords
    (for example, by using the [Linde-Buzo-Gray (LBG) algorithm](http://mlsp.cs.cmu.edu/courses/fall2010/class14/LBG.pdf)),
    which form a codebook. Each codeword *Yi* has an associated region called a [Voronoi
    region](https://en.wikipedia.org/wiki/Voronoi_diagram), which partitions the entire
    space ℝ^k into regions based on proximity to the codewords (see graph below).
    When an input vector is provided, it is compared with each codeword in the codebook
    to find the closest match. This is done by identifying the codeword in the codebook
    with the minimum Euclidean distance to the input vector. Instead of transmitting
    or storing the entire input vector, the index of the nearest codeword is sent
    (encoding). When retrieving the vector (decoding), the decoder retrieves the corresponding
    codeword from the codebook. The codeword is used as an approximation of the original
    input vector. The reconstructed vector is an approximation of the original data,
    but it typically retains the most significant characteristics due to the nature
    of the VQ process. VQ is one popular way to reduce index build time and the amount
    of memory used to store the HNSW graph. However, it is important to understand
    that it will also reduce the accuracy of your search results.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**向量量化** — 向量量化（VQ）是一个过程，它将来自向量空间ℝ^k的k维向量映射到一个有限的向量集合中，这些向量被称为码字（例如，使用[Linde-Buzo-Gray
    (LBG)算法](http://mlsp.cs.cmu.edu/courses/fall2010/class14/LBG.pdf)），这些码字组成了一个码本。每个码字*Yi*都有一个相关区域，称为[Voronoi区域](https://en.wikipedia.org/wiki/Voronoi_diagram)，它根据与码字的接近度将整个空间ℝ^k划分为若干区域（见下图）。当输入向量提供时，它会与码本中的每个码字进行比较，以找到最接近的匹配。这是通过识别与输入向量的欧几里得距离最小的码字来完成的。与其传输或存储整个输入向量，不如传输最接近码字的索引（编码）。在检索向量（解码）时，解码器从码本中检索相应的码字。该码字被用作原始输入向量的近似值。重建的向量是原始数据的近似，但由于VQ过程的特点，它通常保留了最重要的特征。VQ是减少索引构建时间和存储HNSW图所需内存量的一种流行方法。然而，重要的是要理解，它也会降低搜索结果的准确性。'
- en: '![](../Images/7852fc015aa9b971f8a2f86e72a82c04.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7852fc015aa9b971f8a2f86e72a82c04.png)'
- en: A 2-D vector space example (for simplicity). Image by the author.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一个二维向量空间示例（为了简化）。图片由作者提供。
- en: '**2\. Frequent index rebuilds** — One way to overcome the HNSW expanding memory
    challenge is to frequently rebuild your index so that you get rid of nodes that
    are marked as “to-be-deleted”, which take up space and reduce search speed. Consider
    making a copy of your index during those times so you don’t suffer complete downtime
    (However, this will require a lot of memory — an already big issue with HNSW).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 经常重建索引** — 克服HNSW扩展内存挑战的一种方法是频繁重建索引，从而清除那些标记为“待删除”的节点，这些节点占用空间并降低搜索速度。考虑在这些时候制作索引的副本，这样你就不会遭受完全的停机时间（然而，这会需要大量内存——这已经是HNSW的一个大问题）。'
- en: '**3\. Parallel Index Build —** Building an index in parallel involves partitioning
    the data and the allocated memory and distributing the indexing process across
    multiple CPU cores. In this process, all operations are mapped to available RAM.
    For instance, a system might partition the data into manageable chunks, assign
    each chunk to a different processor core, and have them build their respective
    parts of the index simultaneously. This parallelization allows for better utilization
    of the system’s resources, resulting in faster index creation times, especially
    for large datasets. This is a faster way to build indices compared to traditional
    single-threaded builds; however, challenges arise when the entire index cannot
    fit into memory or when a CPU does not have enough cores to support your workload
    in the required timeframe.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. 并行索引构建 —** 并行构建索引涉及将数据和分配的内存进行分区，并将索引过程分配到多个CPU核心上。在这个过程中，所有操作都映射到可用的内存中。例如，系统可能会将数据分割成可管理的块，将每个块分配给不同的处理器核心，并让它们同时构建各自的索引部分。这种并行化方法可以更好地利用系统资源，从而加快索引创建速度，特别是对于大规模数据集。这是一种比传统的单线程构建更快速的索引构建方式；然而，当整个索引无法装入内存，或者当CPU核心不足以支持在要求的时间框架内完成工作负载时，仍然会遇到挑战。'
- en: '![](../Images/d947903038395466f14cdb31df9cf9da.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d947903038395466f14cdb31df9cf9da.png)'
- en: Parallel Processing. Image by the author.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 并行处理。图片来自作者。
- en: 'Using Custom Build Accelerators: A Different Approach'
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自定义构建加速器：一种不同的方法
- en: While the above strategies can help, they often require significant expertise
    and development. Introducing GXL, a new paid-for tool designed to enhance HNSW
    index construction. It uses the APU, [GSI Technology](https://gsitechnology.com/)’s
    compute-in-memory Associative Processing Unit, which uses its millions of bit
    processors to perform computation within the memory. This architecture enables
    massive parallel processing of nearest neighbor distance calculations, significantly
    accelerating index build times for large-scale dynamic datasets. It uses a custom
    algorithm that combines vector quantization and overcomes the similarity search
    bottleneck with parallalism using unique hardware to reduce the overall index
    build time.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管上述策略可以提供帮助，但它们通常需要相当高的专业知识和开发能力。因此，引入了GXL，一个新型的付费工具，旨在增强HNSW索引的构建。它使用APU，[GSI
    Technology](https://gsitechnology.com/)的计算内存关联处理单元，通过其数百万个比特处理器在内存中进行计算。这种架构能够进行大规模的并行处理，快速计算最近邻距离，从而显著加快大规模动态数据集的索引构建时间。它采用一种自定义算法，结合了向量量化，并通过使用独特的硬件并行化来克服相似性搜索的瓶颈，从而减少整体索引构建时间。
- en: 'Let’s check out some benchmark numbers:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一些基准测试数据：
- en: '![](../Images/d7b4960c465121e62b514fa963c8e7c4.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d7b4960c465121e62b514fa963c8e7c4.png)'
- en: 'Image by the author. Credit: Ron Bar Hen'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自作者。图片来源：Ron Bar Hen
- en: The benchmarks compare the build times of HNSWLIB and GXL-HNSW for various dataset
    sizes (deep10M, deep50M, deep100M, and deep500M — all subsets of deep1B) using
    the parameters M = 32 and ef-construction = 100\. These tests were conducted on
    a server with an Intel(R) Xeon(R) Gold 5218 CPU @ 2.30GHz, using one NUMA node
    (32 CPU cores, 380GB DRAM, and 7 LEDA-S APU cards).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试比较了HNSWLIB和GXL-HNSW在不同数据集大小（deep10M、deep50M、deep100M和deep500M——这些都是deep1B的子集）下的构建时间，使用的参数为M
    = 32和ef-construction = 100。测试在一台配备Intel(R) Xeon(R) Gold 5218 CPU @ 2.30GHz的服务器上进行，使用一个NUMA节点（32个CPU核心，380GB内存，7个LEDA-S
    APU卡）。
- en: The results clearly show that GXL-HNSW significantly outperforms HNSWLIB across
    all dataset sizes. For instance, GXL-HNSW builds the deep10M dataset in 1 minute
    and 35 seconds, while HNSWLIB takes 4 minutes and 44 seconds, demonstrating a
    speedup factor of 3.0\. As the dataset size increases, the efficiency of GXL-HNSW
    becomes even greater, with speedup factors of 4.0 for deep50M, 4.3 for deep100M,
    and 4.7 for deep500M. This consistent improvement highlights GXL-HNSW’s better
    performance in handling large-scale data, making it a more efficient choice for
    large-scale dataset similarity searches.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 结果清楚地表明，GXL-HNSW在所有数据集大小上都显著优于HNSWLIB。例如，GXL-HNSW在1分35秒内构建了deep10M数据集，而HNSWLIB需要4分44秒，速度提升因子为3.0。随着数据集大小的增加，GXL-HNSW的效率进一步提升，deep50M的速度提升因子为4.0，deep100M为4.3，deep500M为4.7。这一持续的改进突显了GXL-HNSW在处理大规模数据时的优越表现，使其成为大规模数据集相似性搜索的更高效选择。
- en: In conclusion, while HNSW is highly effective for vector search and AI pipelines,
    it faces tough challenges such as slow index building times and high memory usage,
    which becomes even greater because of HNSW’s complex deletion management. Strategies
    to address these challenges include optimizing your memory usage through frequent
    index rebuilding, implementing vector quantization to your index, and parallelizing
    your index construction. GXL offers an approach that effectively combines some
    of these strategies. These methods help maintain accuracy and efficiency in systems
    relying on HNSW. By reducing the time it takes to build indices, index rebuilding
    isn’t as much of a time-intensive issue as it once was, enableing us to kill two
    birds with one stone — solving both the memory expansion problem and long index
    building times. Test out which method suits you best, and I hope this helps improve
    your overall production workload performance.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，虽然 HNSW 在向量搜索和 AI 流程中非常有效，但它面临着一些严峻的挑战，如索引构建时间较慢和内存使用量较高，而由于 HNSW 复杂的删除管理，这些问题更加严重。解决这些挑战的策略包括通过频繁重建索引来优化内存使用、对索引实施向量量化以及并行化索引构建。GXL
    提供了一种有效结合这些策略的方法。这些方法有助于在依赖 HNSW 的系统中保持准确性和效率。通过减少构建索引所需的时间，索引重建不再是一个耗时的问题，使我们能够一举两得——解决内存扩展问题和长时间索引构建问题。试试看哪种方法最适合你，希望这能帮助你提高整体生产工作负载的性能。
