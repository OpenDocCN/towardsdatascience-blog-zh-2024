- en: Backpropagation Through Time — How RNNs Learn
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间反向传播 — RNN 如何学习
- en: 原文：[https://towardsdatascience.com/backpropagation-through-time-how-rnns-learn-e5bc03ad1f0a?source=collection_archive---------4-----------------------#2024-05-17](https://towardsdatascience.com/backpropagation-through-time-how-rnns-learn-e5bc03ad1f0a?source=collection_archive---------4-----------------------#2024-05-17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/backpropagation-through-time-how-rnns-learn-e5bc03ad1f0a?source=collection_archive---------4-----------------------#2024-05-17](https://towardsdatascience.com/backpropagation-through-time-how-rnns-learn-e5bc03ad1f0a?source=collection_archive---------4-----------------------#2024-05-17)
- en: An explanation of the backpropagation through time algorithm
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间反向传播算法的解释
- en: '[](https://medium.com/@egorhowell?source=post_page---byline--e5bc03ad1f0a--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page---byline--e5bc03ad1f0a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e5bc03ad1f0a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e5bc03ad1f0a--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page---byline--e5bc03ad1f0a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@egorhowell?source=post_page---byline--e5bc03ad1f0a--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page---byline--e5bc03ad1f0a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e5bc03ad1f0a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e5bc03ad1f0a--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page---byline--e5bc03ad1f0a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e5bc03ad1f0a--------------------------------)
    ·9 min read·May 17, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e5bc03ad1f0a--------------------------------)
    ·阅读时间：9 分钟·2024年5月17日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ee340856a1b151a2630e3e799c8e9f7e.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee340856a1b151a2630e3e799c8e9f7e.png)'
- en: ”[https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network)"
    title=”neural network icons”>Neural network icons created by pojok d — Flaticon.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: “[https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network)"
    title=”neural network icons”>神经网络图标由 pojok d 创建 — Flaticon。
- en: Recurrent Neural Networks (RNNs) are regular feedforward neural network variants
    that handle sequence-based data like time series and natural language.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络（RNNs）是常规前馈神经网络的变体，能够处理基于序列的数据，如时间序列和自然语言。
- en: 'They achieve this by adding a “recurrent” neuron that allows information to
    be fed through from past inputs and outputs to the next step. The diagram below
    depicts a traditional RNN:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 它们通过添加一个“递归”神经元来实现这一点，该神经元允许信息从过去的输入和输出传递到下一步。下图展示了一个传统的 RNN：
- en: '![](../Images/91392ef12cfd2972ccec9a5d9c622d1f.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91392ef12cfd2972ccec9a5d9c622d1f.png)'
- en: Example architecture of RNNs. Diagram by author.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 的示例架构。图示由作者提供。
- en: On the left side is a recurrent neuron, and on the right-hand side is the recurrent
    neuron *unrolled through time.* Noticehow the previous executions are passed on
    to the subsequent calculations.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧是一个递归神经元，右侧是通过时间*展开的递归神经元*。注意如何将之前的执行结果传递给后续的计算。
- en: This adds some inherent “memory” in the system that aids the model in picking
    up historical patterns that happened previously in time.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这为系统增加了一些固有的“记忆”，帮助模型捕捉历史上发生的模式。
- en: When predicting ***Y_1***, the recurrent neuron uses the inputs of ***X_1***
    and the output from the previous time step, ***Y_0***. This means that ***Y_0’***s
    influence on ***Y_1*** is direct, and it also indirectly influences ***Y_2.***
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测***Y_1***时，递归神经元使用***X_1***的输入和上一个时间步的输出***Y_0***。这意味着***Y_0'***对***Y_1***的影响是直接的，并且它也间接影响***Y_2***。
- en: If you want a complete introduction to RNNs and some worked examples, check
    out my previous post.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要完整了解 RNN 及一些实际示例，查看我之前的文章。
