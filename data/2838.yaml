- en: Neuromorphic Computing — an Edgier, Greener AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经形态计算——一个更具前沿性、更环保的人工智能
- en: 原文：[https://towardsdatascience.com/neuromorphic-computing-an-edgier-greener-ai-3911fab9fe09?source=collection_archive---------7-----------------------#2024-11-22](https://towardsdatascience.com/neuromorphic-computing-an-edgier-greener-ai-3911fab9fe09?source=collection_archive---------7-----------------------#2024-11-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/neuromorphic-computing-an-edgier-greener-ai-3911fab9fe09?source=collection_archive---------7-----------------------#2024-11-22](https://towardsdatascience.com/neuromorphic-computing-an-edgier-greener-ai-3911fab9fe09?source=collection_archive---------7-----------------------#2024-11-22)
- en: Why computer hardware and AI algorithms are being reinvented using inspiration
    from the brain
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么计算机硬件和人工智能算法要通过借鉴大脑的灵感进行重新设计
- en: '[](https://medium.com/@williford?source=post_page---byline--3911fab9fe09--------------------------------)[![Jonathan
    R. Williford, PhD](../Images/63b57be5ef10621c8d48b93399b2b598.png)](https://medium.com/@williford?source=post_page---byline--3911fab9fe09--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3911fab9fe09--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3911fab9fe09--------------------------------)
    [Jonathan R. Williford, PhD](https://medium.com/@williford?source=post_page---byline--3911fab9fe09--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@williford?source=post_page---byline--3911fab9fe09--------------------------------)[![Jonathan
    R. Williford, PhD](../Images/63b57be5ef10621c8d48b93399b2b598.png)](https://medium.com/@williford?source=post_page---byline--3911fab9fe09--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3911fab9fe09--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3911fab9fe09--------------------------------)
    [Jonathan R. Williford博士](https://medium.com/@williford?source=post_page---byline--3911fab9fe09--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3911fab9fe09--------------------------------)
    ·14 min read·Nov 22, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3911fab9fe09--------------------------------)
    ·14分钟阅读·2024年11月22日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/c98c4baf8e6c6cb47179f9bbfcb687d6.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c98c4baf8e6c6cb47179f9bbfcb687d6.png)'
- en: euromorphic Computing might not just help bring AI to the edge, but also reduce
    carbon emissions at data centers. Generated by author with ImageGen 3.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 神经形态计算不仅可能有助于将人工智能推向边缘计算，还能减少数据中心的碳排放。由作者通过ImageGen 3生成。
- en: There are periodic proclamations of the coming neuromorphic computing revolution, which uses
    inspiration from the brain to rethink neural networks and the hardware they run
    on. While there remain challenges in the field, there have been solid successes
    and continues to be steady progress in spiking neural network algorithms and neuromorphic
    hardware. This progress is paving the way for disruption in at least some sectors
    of artificial intelligence and will reduce the energy consumption per computation
    at inference and allow artificial intelligence to be pushed further out to the
    edge. In this article, I will cover some neuromorphic computing and engineering
    basics, training, the advantages of neuromorphic systems, and the remaining challenges.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 有关神经形态计算革命的定期宣告，这种革命借鉴大脑的灵感，重新思考神经网络及其运行的硬件。虽然该领域仍面临一些挑战，但在脉冲神经网络算法和神经形态硬件方面已经取得了坚实的成功，并持续稳步进展。这一进展为至少某些人工智能领域的颠覆铺平了道路，并将减少推理时每次计算的能耗，同时使人工智能能够更广泛地应用于边缘计算。在本文中，我将介绍一些神经形态计算和工程基础、训练、神经形态系统的优势以及剩余的挑战。
- en: The classical use case of neuromorphic systems is for edge devices that need
    to perform the computation locally and are energy-limited, for example, battery-powered
    devices. However, one of the recent interests in using neuromorphic systems is
    to reduce energy usage at data centers, such as the energy needed by large language
    models (LLMs). For example, OpenAI signed a letter of intent to purchase $51 million
    of neuromorphic chips from Rain AI in December 2023\. This makes sense since OpenAI
    spends a lot on inference, with one estimate of around [$4 billion](https://www.deeplearning.ai/the-batch/openai-faces-financial-growing-pains-spending-double-its-revenue/)
    on running inference in 2024\. It also appears that both Intel’s Loihi 2 and IBM’s
    NorthPole (successor to TrueNorth) neuromorphic systems are designed for use in
    servers.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 神经形态系统的经典应用场景是用于边缘设备，这些设备需要在本地进行计算，并且受限于能源，例如电池供电的设备。然而，最近对神经形态系统的一个新兴趣是减少数据中心的能耗，例如大型语言模型（LLM）所需的能量。例如，OpenAI在2023年12月签署了购买价值5100万美元的神经形态芯片的意向书，来自Rain
    AI。这是有道理的，因为OpenAI在推理上的开支非常大，有人估计2024年推理的花费大约为[$40亿](https://www.deeplearning.ai/the-batch/openai-faces-financial-growing-pains-spending-double-its-revenue/)。同时，英特尔的Loihi
    2和IBM的NorthPole（TrueNorth的继任者）神经形态系统似乎都被设计用于服务器。
- en: The promises of neuromorphic computing can broadly be divided into 1) pragmatic,
    near-term successes that have already found successes and 2) more aspirational,
    wacky neuroscientist fever-dream ideas of how spiking dynamics might endow neural
    networks with something closer to real intelligence. Of course, it’s group 2 that
    really excites me, but I’m going to focus on group 1 for this post. And there
    is no more exciting way to start than to dive into terminology.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 神经形态计算的前景可以大致分为两类：1）务实的、短期内已取得成功的应用；2）更具理想化的、狂热的神经科学家幻想，关于脉冲动力学如何赋予神经网络接近真实智能的潜力。当然，第二类让我更加兴奋，但本文将专注于第一类。而没有比深入探讨术语更激动人心的开始方式了。
- en: Terminology
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 术语
- en: '**Neuromorphic computation** is often defined as computation that is brain-inspired,
    but that definition leaves a lot to the imagination. Neural networks are more
    neuromorphic than classical computation, but these days neuromorphic computation
    is specifically interested in using event-based spiking neural networks (SNNs)
    for their energy efficiency. Even though SNNs are a type of artificial neural
    network, the term “artificial neural networks” (ANNs) is reserved for the more
    standard non-spiking artificial neural networks in the neuromorphic literature.
    Schuman and colleagues (2022) define neuromorphic computers as non-von Neuman
    computers where both processing and memory are collocated in artificial neurons
    and synapses, as opposed to von Neuman computers that separate processing and
    memory.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**神经形态计算**通常被定义为灵感来源于大脑的计算，但这一定义留给了很多想象空间。神经网络比经典计算更具神经形态特征，但如今神经形态计算特别关注使用基于事件的脉冲神经网络（SNNs）来提高能效。尽管SNNs是一种人工神经网络，但“人工神经网络”（ANNs）这一术语在神经形态文献中专门指的是更标准的非脉冲人工神经网络。Schuman及其同事（2022）将神经形态计算机定义为非冯·诺依曼计算机，其中处理和内存都集中在人工神经元和突触中，而冯·诺依曼计算机则将处理与内存分开。'
- en: '![](../Images/60c46e19f0d1db80c31b830c167f15f1.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/60c46e19f0d1db80c31b830c167f15f1.png)'
- en: von Neumann Computers operate on digital information, have separate processors
    and memory, and are synchronized by clocks, while neuromorphic computers operate
    on event-driven spikes, combine compute and memory, and are asynchronous. Created
    by the author with inspiration from Schuman et al. 2022.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 冯·诺依曼计算机基于数字信息操作，拥有分离的处理器和内存，并通过时钟进行同步，而神经形态计算机基于事件驱动的脉冲工作，计算和内存结合，并且是异步的。此内容由作者基于Schuman等人2022年的研究进行创作。
- en: '**Neuromorphic engineering** means designing the hardware while “neuromorphic
    computation” is focused on what is being simulated rather than what it is being
    simulated on. These are tightly intertwined since the computation is dependent
    on the properties of the hardware and what is implemented in hardware depends
    on what is empirically found to work best.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**神经形态工程**指的是硬件的设计，而“神经形态计算”则关注的是模拟的内容，而非模拟的平台。这两者密切相关，因为计算依赖于硬件的特性，而硬件的实现又依赖于实验证明最有效的方案。'
- en: Another related term is **NeuroAI**, the goal of which is to use AI to gain
    a mechanistic understanding of the brain and is more interested in biological
    realism. Neuromorphic computation is interested in neuroscience as a means to
    an end. It views the brain as a source of ideas that can be used to achieve objectives
    such as energy efficiency and low latency in neural architectures. A decent amount
    of the NeuroAI research relies on spike averages rather than spiking neural networks,
    which allows closer comparison of the majority of modern ANNs that are applied
    to discrete tasks.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个相关术语是**神经AI**，其目标是使用人工智能来获得大脑的机制性理解，更关注生物学的真实感。神经形态计算关注神经科学作为手段，视大脑为可以用来实现目标的思想源泉，如在神经架构中的能效和低延迟。神经AI的相当一部分研究依赖于脉冲平均值，而非脉冲神经网络，这使得与大多数应用于离散任务的现代人工神经网络进行更为接近的比较成为可能。
- en: Event-Driven Systems
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于事件的系统
- en: '![](../Images/d35a390d766849eaeec1f1fb58a99bc6.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d35a390d766849eaeec1f1fb58a99bc6.png)'
- en: Generated by the author using ImageGen 3.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者使用 ImageGen 3 生成。
- en: Neuromorphic systems are event-based, which is a paradigm shift from how modern
    ANN systems work. Even real-time ANN systems typically process one frame at a
    time, with activity synchronously propagated from one layer to the next. This
    means that in ANNs, neurons that carry no information require the same processing
    as neurons that carry critical information. Event-driven is a different paradigm
    that often starts at the sensor and applies the most work where information needs
    to be processed. ANNs rely on matrix operations that take the same amount of time
    and energy regardless of the values in the matrices. Neuromorphic systems use
    SNNs where the amount of work depends on the number of spikes.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 神经形态系统是基于事件的，这是与现代人工神经网络系统工作方式的范式转变。即使是实时的人工神经网络系统，通常也一次处理一个帧，活动会从一层同步地传播到下一层。这意味着，在人工神经网络中，不携带信息的神经元需要与携带关键信息的神经元进行相同的处理。基于事件的系统是一种不同的范式，通常从传感器开始，并在需要处理信息的地方进行更多的工作。人工神经网络依赖于矩阵运算，这些运算无论矩阵中的值如何，都需要相同的时间和能量。神经形态系统则使用脉冲神经网络，其中工作的量取决于脉冲的数量。
- en: A traditional deployed ANN would often be connected to a camera that synchronously
    records a frame in a single exposure. The ANN then processes the frame. The results
    of the frame might then be fed into a tracking algorithm and further processed.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的部署人工神经网络通常会连接到一个摄像头，该摄像头同步地记录单帧画面。然后，人工神经网络处理该帧。该帧的结果可能会输入到追踪算法中，进行进一步处理。
- en: Event-driven systems may start at the sensor with an event camera. Each pixel
    sends updates asynchronously whenever a change crosses a threshold. So when there
    is movement in a scene that is otherwise stationary, the pixels that correspond
    to the movement send events or spikes immediately without waiting for a synchronization
    signal. The event signals can be sent within tens of microseconds, while a traditional
    camera might collect at 24 Hz and could introduce a latency that’s in the range
    of tens of milliseconds. In addition to receiving the information sooner, the
    information in the event-based system would be sparser and would focus on the
    movement. The traditional system would have to process the entire scene through
    each network layer successively.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 基于事件的系统可能从传感器开始，使用事件摄像头。每个像素在有变化超过阈值时会异步发送更新。因此，当场景中发生移动（而场景本身静止）时，与运动对应的像素会立即发送事件或脉冲，而无需等待同步信号。这些事件信号可以在几十微秒内发送，而传统摄像头可能以
    24 Hz 的频率收集数据，并可能引入几十毫秒的延迟。除了更快地接收信息外，基于事件的系统中的信息更加稀疏，并且专注于运动。传统系统则必须依次通过每个网络层处理整个场景。
- en: Learning in Spiking Neural Networks
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 脉冲神经网络中的学习
- en: '![](../Images/eed6780afe26750f83038c4b7359c1bf.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eed6780afe26750f83038c4b7359c1bf.png)'
- en: One way to train a spiking neural network is to use an ANN as a teacher. Generated
    by the author with ImageGen 3.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 训练脉冲神经网络的一种方法是使用人工神经网络作为教师。由作者使用 ImageGen 3 生成。
- en: 'One of the major challenges of SNNs is training them. Backpropagation algorithms
    and stochastic gradient descent are the go-to solutions for training ANNs, however,
    these methods run into difficulty with SNNs. The best way to train SNNs is not
    yet established and the following methods are some of the more common approaches
    that are used:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 脉冲神经网络面临的主要挑战之一是如何训练它们。反向传播算法和随机梯度下降是训练人工神经网络的常见方法，但这些方法在脉冲神经网络中遇到了困难。如何训练脉冲神经网络尚未确立，以下方法是一些常见的应用方法：
- en: ANN to SNN conversion
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 人工神经网络到脉冲神经网络的转换
- en: Backpropagation-like
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似反向传播
- en: Synaptic plasticity
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 突触可塑性
- en: Evolutionary
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 演化性
- en: ANN to SNN conversion
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ANN到SNN的转换
- en: One method of creating SNNs is to bypass training the SNNs directly and instead
    train ANNs. This approach limits the types of SNNs and hardware that can be used.
    For example, Sengupta et al. (2019) converted VGG and ResNets to ANNs using an
    integrate-and-fire (IF) neuron that does not have a leaking or refractory period.
    They introduce a novel weight-normalization technique to perform the conversion,
    which involves setting the firing threshold of each neuron based on its pre-synaptic
    weights. Dr. Priyadarshini Panda goes into more detail in her [ESWEEK 2021 SNN
    Talk](https://youtu.be/7TybETlCslM?t=3077&si=gK1efoiOx6SVpYfU).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 创建脉冲神经网络（SNN）的一种方法是跳过直接训练SNN，而是训练ANN。这种方法限制了可以使用的SNN类型和硬件。例如，Sengupta等人（2019年）使用没有泄漏或折返期的积分与脉冲（IF）神经元将VGG和ResNets转换为ANN。他们引入了一种新颖的权重归一化技术来执行转换，该技术基于每个神经元的前突触权重设置其触发阈值。Priyadarshini
    Panda博士在她的[ESWEEK 2021 SNN演讲](https://youtu.be/7TybETlCslM?t=3077&si=gK1efoiOx6SVpYfU)中详细阐述了这一点。
- en: '**Advantages**:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**：'
- en: Enables deep SNNs.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使深度SNN成为可能。
- en: Allows reuse of deep ANN knowledge, such as training, architecture, etc.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 允许重用深度人工神经网络（ANN）的知识，例如训练、架构等。
- en: '**Disadvantages**:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**：'
- en: Limits architectures to those suited to ANNs and the conversion procedures.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 限制架构仅适用于ANN以及转换过程。
- en: Network doesn’t learn to take advantage of SNN properties, which can lead to
    lower accuracy and longer latency.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络未能学习如何利用脉冲神经网络（SNN）的特性，这可能导致较低的准确性和更长的延迟。
- en: Backpropagation-like approaches and surrogate gradient descent
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 类似反向传播的方法和代理梯度下降
- en: The most common methods currently used to train SNNs are backpropagation-like
    approaches. Standard backpropagation does not work to train SNNs because 1) the
    spiking threshold function’s gradient is nonzero except at the threshold where
    it is undefined and 2) the credit assignment problem needs to be solved in the
    temporal dimension in addition spatial (or color etc).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当前训练SNN的最常见方法是类似反向传播的方法。标准的反向传播方法无法训练SNN，因为1）脉冲阈值函数的梯度在阈值处非零，除非在阈值处它是未定义的，2）信用分配问题需要在时间维度上解决，而不仅仅是在空间维度（如颜色等）。
- en: In ANNs, the most common activation function is the ReLU. For SNNs, the neuron
    will fire if the membrane potential is above some threshold, otherwise, it will
    not fire. This is called a Heaviside function. You could use a sigmoid function
    instead, but then it would not be a spiking neural network. The solution of using
    surrogate gradients is to use the standard threshold function in the forward pass,
    but then use the derivative from a “smoothed” version of the Heaviside function,
    such as the sigmoid function, in the backward pass (Neftci et al. 2019, Bohte
    2011).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工神经网络（ANN）中，最常见的激活函数是ReLU。而在脉冲神经网络（SNN）中，当膜电位超过某个阈值时，神经元会发放脉冲，否则不会。这被称为Heaviside函数。你也可以使用sigmoid函数，但那样就不再是脉冲神经网络。使用代理梯度的解决方案是在前向传播中使用标准的阈值函数，但在反向传播时使用“平滑”版本的Heaviside函数的导数，例如sigmoid函数（Neftci等，2019年；Bohte，2011年）。
- en: '**Advantages:**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**：'
- en: Connects to well-known methods.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与著名方法连接。
- en: Compared to conversion, can result in a more energy efficient network (Li et
    al. 2022)
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与转换方法相比，可能导致更节能的网络（Li等，2022年）
- en: '**Disadvantages:**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**：'
- en: Can be computationally intensive to solve both spatially and through time
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在空间和时间上都可能需要计算密集型的求解
- en: Synaptic Plasticity
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 突触可塑性
- en: Spike-timing-dependent plasticity (STDP) is the most well-known form of synaptic
    plasticity. In most cases, STDP increases the strength of a synapse when a presynaptic
    (input) spike comes immediately before the postsynaptic spike. Early models have
    shown promise with STDP on simple unsupervised tasks, although getting it to work
    well for more complex models and tasks has proven more difficult.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 脉冲时序依赖性可塑性（STDP）是最著名的突触可塑性形式。在大多数情况下，当前突触（输入）脉冲在后突触脉冲之前立即到达时，STDP会增强突触的强度。早期的模型在简单的无监督任务中显示了STDP的潜力，尽管在更复杂的模型和任务中获得良好效果仍然更为困难。
- en: Other biological learning mechanisms include the pruning and creation of both
    neurons and synapses, homeostatic plasticity, neuromodulators, astrocytes, and
    evolution. There is even some recent evidence that some primitive types of knowledge
    can be passed down by epigenetics.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其他生物学习机制包括神经元和突触的修剪与创建、自稳可塑性、神经调节剂、星形胶质细胞和进化。最近还有一些证据表明，某些原始类型的知识可以通过表观遗传学传递下来。
- en: '**Advantages**:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**：'
- en: Unsupervised
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 无监督
- en: Can take advantage of temporal properties
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以利用时间特性
- en: Biologically inspired
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生物启发
- en: '**Disadvantages**:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**：'
- en: Synaptic plasticity is not well understood, especially at different timescales
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 突触可塑性尚未完全理解，特别是在不同的时间尺度下。
- en: Difficult to get to work with non-trivial networks
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 难以与非平凡的网络一起工作。
- en: Evolutionary Optimization
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进化优化
- en: Evolutionary optimization is another approach that has some cool applications
    that works well with small networks. Dr. Catherine Schuman is a leading expert
    and she gave a fascinating talk on neuromorphic computing to the ICS lab that
    is available on YouTube.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 进化优化是另一种方法，具有一些很酷的应用，特别适合小型网络。Catherine Schuman 博士是这一领域的领先专家，她在 ICS 实验室做了一场关于神经形态计算的精彩演讲，视频可以在
    YouTube 上观看。
- en: '**Advantages**:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**：'
- en: Applicable to many tasks, architectures, and devices.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 适用于许多任务、架构和设备。
- en: Can learn topology and parameters (requiring less knowledge of the problem).
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以学习拓扑和参数（减少对问题的知识需求）。
- en: Learns small networks which results in lower latency.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 学习小型网络，从而导致较低的延迟。
- en: '**Disadvantages**:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**：'
- en: Not effective for problems that require deep or large architectures.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对需要深度或大规模架构的问题无效。
- en: Advantages of Neuromorphic Systems
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经形态系统的优点
- en: Energy Efficiency
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 能效
- en: 'Neuromorphic systems have two main advantages: 1) energy efficiency and 2)
    low latency. There are a lot of reasons to be excited about the energy efficiency.
    For example, Intel [claimed](https://www.intel.com/content/www/us/en/newsroom/news/intel-builds-worlds-largest-neuromorphic-system.html#gs.gq485y)
    that their Loihi 2 Neural Processing Unit (NPU) can use 100 times less energy
    while being as much as 50 times faster than conventional ANNs. Chris Eliasmith
    compared the energy efficiency of an SNN on neuromorphic hardware with an ANN
    with the same architecture on standard hardware in [a presentation available on
    YouTube](https://www.youtube.com/watch?v=PeW-TN3P1hk&t=1308s). He found that the
    SNN is 100 times more energy efficient on Loihi compared to the ANN on a standard
    NVIDIA GPU and 20 times more efficient than the ANN on an NVIDIA Jetson GPU. It
    is 5–7 times more energy efficient than the Intel Neural Compute Stick (NCS) and
    NCS 2\. At the same time the SNN achieves a 93.8% accuracy compared to the 92.7%
    accuracy of the ANN.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 神经形态系统有两个主要优势：1）能效和 2）低延迟。关于能效，有很多令人兴奋的理由。例如，英特尔[声称](https://www.intel.com/content/www/us/en/newsroom/news/intel-builds-worlds-largest-neuromorphic-system.html#gs.gq485y)，他们的
    Loihi 2 神经处理单元（NPU）在能耗上比传统人工神经网络（ANN）低 100 倍，同时速度比常规 ANN 快 50 倍。Chris Eliasmith
    在[YouTube 上的一场演讲](https://www.youtube.com/watch?v=PeW-TN3P1hk&t=1308s)中将神经形态硬件上的脉冲神经网络（SNN）与同一架构的标准硬件上的人工神经网络（ANN）进行了能效对比。他发现，Loihi
    上的 SNN 在能效上比标准 NVIDIA GPU 上的 ANN 高 100 倍，比 NVIDIA Jetson GPU 上的 ANN 高 20 倍。它比英特尔神经计算棒（NCS）和
    NCS 2 高效 5-7 倍。同时，SNN 的准确率为 93.8%，而 ANN 的准确率为 92.7%。
- en: '![](../Images/6a6f91fa92b61f413a113c6653c8b8c4.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a6f91fa92b61f413a113c6653c8b8c4.png)'
- en: Figure recreated by author from Chris Eliasmith’s slides at [https://www.youtube.com/watch?v=PeW-TN3P1hk&t=1308s](https://www.youtube.com/watch?v=PeW-TN3P1hk&t=1308s)
    which shows the neuromorphic processor being 5–100x more efficient while achieving
    a similar accuracy.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图表由作者根据 Chris Eliasmith 在[https://www.youtube.com/watch?v=PeW-TN3P1hk&t=1308s](https://www.youtube.com/watch?v=PeW-TN3P1hk&t=1308s)的幻灯片重制，显示了神经形态处理器在效率上比传统处理器高出
    5 到 100 倍，同时实现相似的准确率。
- en: Neuromorphic chips are more energy efficient and allow complex deep learning
    models to be deployed on low-energy edge devices. In October 2024, BrainChip introduced
    the Akida Pico NPU which uses less than 1 mW of power, and Intel Loihi 2 NPU uses
    1 W. That’s a lot less power than NVIDIA Jetson modules that use between 10–50
    watts which is often used for embedded ANNs and server GPUs can use around 100
    watts.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 神经形态芯片更具能效，允许在低能耗的边缘设备上部署复杂的深度学习模型。2024 年 10 月，BrainChip 推出了 Akida Pico NPU，功耗低于
    1 毫瓦，而英特尔的 Loihi 2 NPU 功耗为 1 瓦。这比常用于嵌入式人工神经网络的 NVIDIA Jetson 模块（功耗在 10 到 50 瓦之间）要低得多，而服务器
    GPU 的功耗通常为 100 瓦左右。
- en: 'Comparing the energy efficiency between ANNs and SNNs are difficult because:
    1\. energy efficiency is dependent on hardware, 2\. SNNs and ANNs can use different
    architectures, and 3\. they are suited to different problems. Additionally, the
    energy used by SNNs scales with the number of spikes and the number of time steps,
    so the number of spikes and time steps needs to be minimized to achieve the best
    energy efficiency.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 比较人工神经网络（ANNs）和脉冲神经网络（SNNs）的能效是困难的，因为：1. 能效依赖于硬件，2. SNNs和ANNs可以使用不同的架构，3. 它们适用于不同的问题。此外，SNNs的能耗与脉冲的数量和时间步数成正比，因此需要尽量减少脉冲和时间步数，以实现最佳的能效。
- en: Theoretical analysis is often used to estimate the energy needed by SNNs and
    ANNs, however, this doesn’t take into account all of the differences between the
    CPUs and GPUs used for ANNs and the neuromorphic chips for SNNs.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 理论分析通常用于估算SNNs和ANNs所需的能量，但这并没有考虑到用于ANNs的CPU和GPU与用于SNNs的神经形态芯片之间的所有差异。
- en: 'Looking into nature can give us an idea of what might be possible in the future
    and Mike Davies provided a great anecdote in an Intel [Architecture All Access
    YouTube video](https://www.youtube.com/watch?v=6Dcs6fQglRA):'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 观察自然界可以给我们提供一些未来可能实现的想法，Mike Davies在英特尔的[Architecture All Access YouTube视频](https://www.youtube.com/watch?v=6Dcs6fQglRA)中分享了一个很好的轶事：
- en: '*Consider the capabilities of a tiny cockatiel parrot brain, a two-gram brain
    running on about 50 mW of power. This brain enables the cockatiel to fly at speeds
    up to 20 mph, to navigate unknown environments while foraging for food, and even
    to learn to manipulate objects as tools and utter human words.*'
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*考虑到一只微小的凤头鹦鹉大脑的能力，这个两克重的大脑在大约50毫瓦的功率下运行。这个大脑使得凤头鹦鹉能够以每小时20英里的速度飞行，在觅食时能够导航未知的环境，甚至能学会将物体当作工具操作，并发出人类的语言。*'
- en: In current neural networks, there is a lot of wasted computation. For example,
    an image encoder takes the same amount of time encoding a blank page as a cluttered
    page in a “Where’s Waldo?” book. In spiking neural networks, very few units would
    activate on a blank page and very little computation would be used, while a page
    containing a lot of features would fire a lot more units and use a lot more computation.
    In real life, there are often regions in the visual field that contain more features
    and require more processing than other regions that contain fewer features, like
    a clear sky. In either case, SNNs only perform work when work needs to be performed,
    whereas ANNs depend on matrix multiplications that are difficult to use sparsely.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前的神经网络中，存在大量的计算浪费。例如，一个图像编码器对一个空白页面的编码时间与对一本《沃尔多在哪里？》书中杂乱页面的编码时间相同。而在脉冲神经网络中，空白页面几乎没有单元激活，计算量也很小；而包含许多特征的页面则会激活更多的单元，使用更多的计算量。在现实生活中，视觉场中通常有一些区域包含更多的特征，需要比其他区域（如清晰的天空）更多的处理。在这两种情况下，SNNs仅在需要工作时才会执行任务，而ANNs则依赖于矩阵乘法，这种计算方式很难稀疏使用。
- en: This in itself is exciting. A lot of deep learning currently involves uploading
    massive amounts of audio or video to the cloud, where the data is processed in
    massive data centers, spending a lot of energy on the computation and cooling
    the computational devices, and then the results are returned. With edge computing,
    you can have more secure and more responsive voice recognition or video recognition,
    that you can keep on your local device, with orders of magnitude less energy consumption.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点本身就令人兴奋。目前，很多深度学习涉及将大量音频或视频上传到云端，在那里数据被处理在庞大的数据中心，花费大量的能量用于计算和冷却计算设备，然后返回结果。通过边缘计算，你可以在本地设备上拥有更安全、更快速的语音识别或视频识别，且能耗比传统方法低几个数量级。
- en: Low Latency
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 低延迟
- en: When a pixel receptor of an event camera changes by some threshold, it can send
    an event or spike within microseconds. It doesn’t need to wait for a shutter or
    synchronization signal to be sent. This benefit is seen throughout the event-based
    architecture of SNNs. Units can send events immediately, rather than waiting for
    a synchronization signal. This makes neuromorphic computers much faster, in terms
    of latency, than ANNs. Hence, neuromorphic processing is better than ANNs for
    real-time applications that can benefit from low latency. This benefit is reduced
    if the problem allows for batching and you are measuring speed by throughput since
    ANNs can take advantage of batching more easily. However, in real-time processing,
    such as robotics or user interfacing, latency is more important.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当事件相机的像素接收器改变超过某个阈值时，它可以在微秒内发送一个事件或脉冲。它不需要等待快门或同步信号的发送。这一优势贯穿了SNN的事件驱动架构。单元可以立即发送事件，而不是等待同步信号。这使得类脑计算机在延迟方面比ANN（人工神经网络）要快。因此，类脑处理对于需要低延迟的实时应用程序更有优势。如果问题允许批处理，并且您是通过吞吐量来衡量速度，这一优势会减弱，因为ANN可以更容易地利用批处理。然而，在实时处理（如机器人技术或用户接口）中，延迟更加重要。
- en: Disadvantages and Challenges
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缺点与挑战
- en: Everything Everywhere All at Once
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无处不在的每一刻
- en: One of the challenges is that neuromorphic computing and engineering are progressing
    at multiple levels at the same time. The details of the models depend on the hardware
    implementation and empirical results with actualized models guide the development
    of the hardware. Intel discovered this with their Loihi 1 chips and built more
    flexibility into their Loihi 2 chips, however, there will always be tradeoffs
    and there are still many advances to be made on both the hardware and software
    side.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 挑战之一是类脑计算和工程正同时在多个层面上发展。模型的细节取决于硬件实现，实际化的模型的经验结果指导着硬件的发展。英特尔在其Loihi 1芯片上发现了这一点，并在其Loihi
    2芯片中加入了更多的灵活性，然而，总会存在折中，且硬件和软件方面仍有许多进展需要做。
- en: Limited Availability of Commercial Hardware
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 商业硬件的有限可得性
- en: Hopefully, this will change soon, but commercial hardware isn’t very available.
    BrainChip’s Akida was the first neuromorphic chip to be commercially available,
    although [apparently, it does not even support](https://open-neuromorphic.org/neuromorphic-computing/hardware/akida-brainchip/#neurons-and-synapses)
    the standard leaky-integrate and fire (LIF) neuron. SpiNNaker boards used to be
    for sale, which was part of the EU Human Brain Project but are [no longer available](https://apt.cs.manchester.ac.uk/projects/SpiNNaker/).
    Intel makes Loihi 2 chips available to some academic researchers via the [Intel
    Neuromorphic Research Community (INRC)](https://intel-ncl.atlassian.net/wiki/spaces/INRC/pages/1784807425/Join+the+INRC)
    program.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这种情况很快会改变，但商业硬件并不容易获得。BrainChip的Akida是首个商业化的类脑芯片，尽管[显然，它甚至不支持](https://open-neuromorphic.org/neuromorphic-computing/hardware/akida-brainchip/#neurons-and-synapses)标准的漏积与发放（LIF）神经元。SpiNNaker板曾经可以购买，这是一部分欧盟人类大脑计划的内容，但[现在已经不再提供](https://apt.cs.manchester.ac.uk/projects/SpiNNaker/)。英特尔通过[英特尔类脑研究社区（INRC）](https://intel-ncl.atlassian.net/wiki/spaces/INRC/pages/1784807425/Join+the+INRC)项目向一些学术研究人员提供Loihi
    2芯片。
- en: Datasets
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: The number of neuromorphic datasets is much less than traditional datasets and
    can be much larger. Some of the common smaller computer vision datasets, such
    as MNIST (NMNIST, Li et al. 2017) and CIFAR-10 (CIFAR10-DVS, Orchard et al. 2015),
    have been converted to event streams by displaying the images and recording them
    using event-based cameras. The images are collected with movement (or “saccades”)
    to increase the number of spikes for processing. With larger datasets, such as
    ES-ImageNet (Lin et al. 2021), simulation of event cameras has been used.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 类脑数据集的数量远少于传统数据集，但可以大得多。一些常见的小型计算机视觉数据集，如MNIST（NMNIST，Li等，2017）和CIFAR-10（CIFAR10-DVS，Orchard等，2015），已通过显示图像并使用基于事件的相机记录它们，转换为事件流。这些图像是在移动（或“扫视”）的过程中收集的，以增加处理的脉冲数量。对于更大的数据集，如ES-ImageNet（Lin等，2021），已使用事件相机的仿真。
- en: 'The dataset derived from static images might be useful in comparing SNNs with
    conventional ANNs and might be useful as part of the training or evaluation pipeline,
    however, SNNs are naturally temporal, and using them for static inputs does not
    make a lot of sense if you want to take advantage of SNNs temporal properties.
    Some of the datasets that take advantage of these properties of SNNs include:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 通过静态图像衍生的数据集可能有助于将脉冲神经网络（SNN）与传统人工神经网络（ANN）进行比较，并且可能在训练或评估流程中有所帮助。然而，SNNs 天生是时序性的，如果你希望利用SNN的时序特性，用它们处理静态输入就没有太大意义。以下是一些能够利用SNN时序特性的相关数据集：
- en: DvsGesture (Amir et al. 2017) — a dataset of people performing a set of 11 hand
    and arm gestures
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DvsGesture（Amir et al. 2017）——一个包含11种手势和臂部动作的数据集
- en: Bullying10K (Dong et al. 2024) — a privacy-preserving dataset for bullying recognition
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bullying10K（Dong et al. 2024）——一个用于欺凌识别的隐私保护数据集
- en: Synthetic data can be generated from standard visible camera data without the
    use of expensive event camera data collections, however these won’t exhibit the
    high dynamic range and frame rate that event cameras would capture.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过标准的可见光摄像头数据生成合成数据，而无需使用昂贵的事件摄像头数据收集，然而这些数据不会展现事件摄像头所捕获的高动态范围和帧率。
- en: Tonic is an example python library that makes it easy to access at least some
    of these event-based datasets. The datasets themselves can take up a lot more
    space than traditional datasets. For example, the training images for MNIST is
    around 10 MB, while in N-MNIST, it is almost 1 GB.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Tonic是一个示例Python库，可以轻松访问至少部分基于事件的数据集。这些数据集本身可能比传统数据集占用更多空间。例如，MNIST的训练图像约为10
    MB，而在N-MNIST中则接近1 GB。
- en: Another thing to take into account is that visualizing the datasets can be difficult.
    Even the datasets derived from static images can be difficult to match with the
    original input images. Also, the benefit of using real data is typically to avoid
    a gap between training and inference, so it would seem that the benefit of using
    these datasets would depend on their similarity to the cameras used during deployment
    or testing.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 需要考虑的另一件事是，数据集的可视化可能会很困难。即使是从静态图像衍生的数据集，也可能难以与原始输入图像匹配。此外，使用真实数据的好处通常是为了避免训练和推理之间的差距，因此，使用这些数据集的好处似乎取决于它们与部署或测试过程中使用的摄像头的相似性。
- en: Conclusion
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: '![](../Images/787c9c47f424f412a76142f80faa88bc.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/787c9c47f424f412a76142f80faa88bc.png)'
- en: Created by author with ImageGen 3 and GIMP.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者使用ImageGen 3和GIMP创建。
- en: We are in an exciting time with neuromorphic computation, with both the investment
    in the hardware and the advancements in spiking neural networks. There are still
    challenges for adoption, but there are proven cases where they are more energy
    efficient, especially standard server GPUs while having lower latency and similar
    accuracy as traditional ANNs. A lot of companies, including Intel, IBM, Qualcomm,
    Analog Devices, Rain AI, and BrainChip have been investing in neuromorphic systems.
    BrainChip is the first company to make their neuromorphic chips commercially available
    while both Intel and IBM are on the second generations of their research chips
    (Loihi 2 and NorthPole respectively). There also seems to have been a particular
    spike of successful spiking transformers and other deep spiking neural networks
    in the last couple of years, following the Spikformer paper (Zhou et al. 2022)
    and the SEW-ResNet paper (Fang et al. 2021).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正处于神经形态计算的激动人心的时代，不仅硬件方面有了投资，脉冲神经网络（SNN）也在不断进展。虽然应用仍面临挑战，但已经有证明的案例表明，SNN比传统的ANN在能效上更优，尤其是在标准服务器GPU上，同时延迟更低，精度相当。许多公司，包括英特尔、IBM、高通、模拟器件、Rain
    AI 和 BrainChip，都在投资神经形态系统。BrainChip是首家将其神经形态芯片商业化的公司，而英特尔和IBM则分别进入了其研究芯片的第二代（Loihi
    2和NorthPole）。过去几年，特别是在Spikformer论文（Zhou et al. 2022）和SEW-ResNet论文（Fang et al.
    2021）之后，脉冲变压器和其他深度脉冲神经网络似乎出现了特别的成功。
- en: References
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Amir, A., Taba, B., Berg, D., Melano, T., McKinstry, J., Di Nolfo, C., Nayak,
    T., Andreopoulos, A., Garreau, G., Mendoza, M., Kusnitz, J., Debole, M., Esser,
    S., Delbruck, T., Flickner, M., & Modha, D. (2017). *A Low Power, Fully Event-Based
    Gesture Recognition System*. 7243–7252\. [https://openaccess.thecvf.com/content_cvpr_2017/html/Amir_A_Low_Power_CVPR_2017_paper.html](https://openaccess.thecvf.com/content_cvpr_2017/html/Amir_A_Low_Power_CVPR_2017_paper.html)
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amir, A., Taba, B., Berg, D., Melano, T., McKinstry, J., Di Nolfo, C., Nayak,
    T., Andreopoulos, A., Garreau, G., Mendoza, M., Kusnitz, J., Debole, M., Esser,
    S., Delbruck, T., Flickner, M., & Modha, D. (2017). *一个低功耗的全事件驱动手势识别系统*. 7243–7252\.
    [https://openaccess.thecvf.com/content_cvpr_2017/html/Amir_A_Low_Power_CVPR_2017_paper.html](https://openaccess.thecvf.com/content_cvpr_2017/html/Amir_A_Low_Power_CVPR_2017_paper.html)
- en: Bohte, S. M. (2011). Error-Backpropagation in Networks of Fractionally Predictive
    Spiking Neurons. In *Artificial Neural Networks and Machine Learning* [https://doi.org/10.1007/978-3-642-21735-7_8](https://doi.org/10.1007/978-3-642-21735-7_8)
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bohte, S. M. (2011). 在分数预测脉冲神经元网络中进行误差反向传播。收录于 *人工神经网络与机器学习* [https://doi.org/10.1007/978-3-642-21735-7_8](https://doi.org/10.1007/978-3-642-21735-7_8)
- en: 'Dong, Y., Li, Y., Zhao, D., Shen, G., & Zeng, Y. (2023). Bullying10K: A Large-Scale
    Neuromorphic Dataset towards Privacy-Preserving Bullying Recognition. *Advances
    in Neural Information Processing Systems*, *36*, 1923–1937.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong, Y., Li, Y., Zhao, D., Shen, G., & Zeng, Y. (2023). Bullying10K：一个大型神经形态数据集，用于隐私保护的欺凌识别。*神经信息处理系统进展*,
    *36*, 1923–1937.
- en: Fang, W., Yu, Z., Chen, Y., Huang, T., Masquelier, T., & Tian, Y. (2021). Deep
    Residual Learning in Spiking Neural Networks. *Advances in Neural Information
    Processing Systems*, *34*, 21056–21069\. [https://proceedings.neurips.cc/paper/2021/hash/afe434653a898da20044041262b3ac74-Abstract.html](https://proceedings.neurips.cc/paper/2021/hash/afe434653a898da20044041262b3ac74-Abstract.html)
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang, W., Yu, Z., Chen, Y., Huang, T., Masquelier, T., & Tian, Y. (2021). 脉冲神经网络中的深度残差学习。*神经信息处理系统进展*,
    *34*, 21056–21069\. [https://proceedings.neurips.cc/paper/2021/hash/afe434653a898da20044041262b3ac74-Abstract.html](https://proceedings.neurips.cc/paper/2021/hash/afe434653a898da20044041262b3ac74-Abstract.html)
- en: Li, C., Ma, L., & Furber, S. (2022). Quantization Framework for Fast Spiking
    Neural Networks. *Frontiers in Neuroscience*,*16*. [https://doi.org/10.3389/fnins.2022.918793](https://doi.org/10.3389/fnins.2022.918793)
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li, C., Ma, L., & Furber, S. (2022). 快速脉冲神经网络的量化框架。*前沿神经科学*, *16*. [https://doi.org/10.3389/fnins.2022.918793](https://doi.org/10.3389/fnins.2022.918793)
- en: 'Li, H., Liu, H., Ji, X., Li, G., & Shi, L. (2017). CIFAR10-DVS: An Event-Stream
    Dataset for Object Classification. *Frontiers in Neuroscience*, *11*. [https://doi.org/10.3389/fnins.2017.00309](https://doi.org/10.3389/fnins.2017.00309)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li, H., Liu, H., Ji, X., Li, G., & Shi, L. (2017). CIFAR10-DVS：一个用于物体分类的事件流数据集。*前沿神经科学*,
    *11*. [https://doi.org/10.3389/fnins.2017.00309](https://doi.org/10.3389/fnins.2017.00309)
- en: 'Lin, Y., Ding, W., Qiang, S., Deng, L., & Li, G. (2021). ES-ImageNet: A Million
    Event-Stream Classification Dataset for Spiking Neural Networks. *Frontiers in
    Neuroscience*, *15*. [https://doi.org/10.3389/fnins.2021.726582](https://doi.org/10.3389/fnins.2021.726582'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin, Y., Ding, W., Qiang, S., Deng, L., & Li, G. (2021). ES-ImageNet: 一个用于脉冲神经网络的百万事件流分类数据集。*前沿神经科学*,
    *15*. [https://doi.org/10.3389/fnins.2021.726582](https://doi.org/10.3389/fnins.2021.726582)'
- en: 'Neftci, E. O., Mostafa, H., & Zenke, F. (2019). Surrogate Gradient Learning
    in Spiking Neural Networks: Bringing the Power of Gradient-Based Optimization
    to Spiking Neural Networks. *IEEE Signal Processing Magazine*. [https://doi.org/10.1109/MSP.2019.2931595](https://doi.org/10.1109/MSP.2019.2931595)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neftci, E. O., Mostafa, H., & Zenke, F. (2019). 脉冲神经网络中的代理梯度学习：将基于梯度的优化方法引入脉冲神经网络。*IEEE信号处理杂志*.
    [https://doi.org/10.1109/MSP.2019.2931595](https://doi.org/10.1109/MSP.2019.2931595)
- en: Orchard, G., Jayawant, A., Cohen, G. K., & Thakor, N. (2015). Converting Static
    Image Datasets to Spiking Neuromorphic Datasets Using Saccades. *Frontiers in
    Neuroscience*, *9*. [https://doi.org/10.3389/fnins.2015.00437](https://doi.org/10.3389/fnins.2015.00437)
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Orchard, G., Jayawant, A., Cohen, G. K., & Thakor, N. (2015). 使用扫视将静态图像数据集转换为脉冲神经形态数据集。*前沿神经科学*,
    *9*. [https://doi.org/10.3389/fnins.2015.00437](https://doi.org/10.3389/fnins.2015.00437)
- en: Schuman, C. D., Kulkarni, S. R., Parsa, M., Mitchell, J. P., Date, P., & Kay,
    B. (2022). Opportunities for neuromorphic computing algorithms and applications.
    *Nature Computational Science*,*2*(1), 10–19\. [https://doi.org/10.1038/s43588-021-00184-y](https://doi.org/10.1038/s43588-021-00184-y)
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schuman, C. D., Kulkarni, S. R., Parsa, M., Mitchell, J. P., Date, P., & Kay,
    B. (2022). 神经形态计算算法和应用的机遇。*自然计算科学*, *2*(1), 10–19\. [https://doi.org/10.1038/s43588-021-00184-y](https://doi.org/10.1038/s43588-021-00184-y)
- en: 'Sengupta, A., Ye, Y., Wang, R., Liu, C., & Roy, K. (2019). Going Deeper in
    Spiking Neural Networks: VGG and Residual Architectures. *Frontiers in Neuroscience*,
    *13*. [https://doi.org/10.3389/fnins.2019.00095](https://doi.org/10.3389/fnins.2019.00095)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sengupta, A., Ye, Y., Wang, R., Liu, C., & Roy, K. (2019). 《深入研究脉冲神经网络：VGG和残差架构》.
    *神经科学前沿*, *13*. [https://doi.org/10.3389/fnins.2019.00095](https://doi.org/10.3389/fnins.2019.00095)
- en: 'Zhou, Z., Zhu, Y., He, C., Wang, Y., Yan, S., Tian, Y., & Yuan, L. (2022, September
    29). *Spikformer: When Spiking Neural Network Meets Transformer*. The Eleventh
    International Conference on Learning Representations. [https://openreview.net/forum?id=frE4fUwz_h](https://openreview.net/forum?id=frE4fUwz_h)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou, Z., Zhu, Y., He, C., Wang, Y., Yan, S., Tian, Y., & Yuan, L. (2022年9月29日).
    *Spikformer：当脉冲神经网络遇到Transformer*。第十一届国际学习表征会议。 [https://openreview.net/forum?id=frE4fUwz_h](https://openreview.net/forum?id=frE4fUwz_h)
- en: Resources
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源
- en: '[Open Neuromorphic (ONM) Collective](https://open-neuromorphic.org)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[开放类脑计算（ONM）集体](https://open-neuromorphic.org)'
- en: Event-Based Vision Resources ([https://github.com/uzh-rpg/event-based_vision_resources](https://github.com/uzh-rpg/event-based_vision_resources))
    — Upcoming workshops, papers, companies, neuromorphic systems, etc.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于事件的视觉资源 ([https://github.com/uzh-rpg/event-based_vision_resources](https://github.com/uzh-rpg/event-based_vision_resources))
    — 即将举行的工作坊、论文、公司、类脑系统等。
- en: Talks on Youtube
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: YouTube上的讲座
- en: '[Neuromorphic Computing from the Computer Science Perspective video from ICAS
    Lab with Dr Catherine Schuman](https://www.youtube.com/watch?v=PWOr1_85zeg)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[计算机科学视角下的类脑计算：ICAS实验室与Catherine Schuman博士的讲座](https://www.youtube.com/watch?v=PWOr1_85zeg)'
- en: Cosyne 2022 Tutorial on Spiking Neural Networks — [Part 1](https://www.youtube.com/watch?v=GTXTQ_sOxak)
    and [Part 2](https://www.google.com/url?sa=t&rct=j&opi=89978449&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Drfck_p0JrIc&ved=2ahUKEwjhnOSOz_CJAxUyjIkEHWIdCYoQwqsBegQIDxAF&usg=AOvVaw1sS-AEv8cMigz1WyxopO0_)
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cosyne 2022脉冲神经网络教程 — [第一部分](https://www.youtube.com/watch?v=GTXTQ_sOxak) 和
    [第二部分](https://www.google.com/url?sa=t&rct=j&opi=89978449&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Drfck_p0JrIc&ved=2ahUKEwjhnOSOz_CJAxUyjIkEHWIdCYoQwqsBegQIDxAF&usg=AOvVaw1sS-AEv8cMigz1WyxopO0_)
- en: '[ESWEEK 2021 Dr. Priyadarshini Panda’s SNN Talk](https://www.youtube.com/watch?v=7TybETlCslM)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ESWEEK 2021 Priyadarshini Panda博士的SNN讲座](https://www.youtube.com/watch?v=7TybETlCslM)'
- en: 'Intel Architecture All Access: Neuromorphic Computing by Mike Davies— [Part
    1](https://www.youtube.com/watch?v=6Dcs6fQglRA) and [Part 2](https://www.youtube.com/watch?v=XWds3FIVm0U)'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 英特尔架构全面访问：Mike Davies讲解类脑计算 — [第一部分](https://www.youtube.com/watch?v=6Dcs6fQglRA)
    和 [第二部分](https://www.youtube.com/watch?v=XWds3FIVm0U)
- en: '[Spiking Neural Networks for More Efficient AI Algorithms Talk by Professor
    Chris Eliasmith at University of Waterloo](https://www.youtube.com/watch?v=PeW-TN3P1hk)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[水loo大学Chris Eliasmith教授关于更高效AI算法的脉冲神经网络讲座](https://www.youtube.com/watch?v=PeW-TN3P1hk)'
- en: '*Originally published at* [*https://neural.vision*](https://neural.vision/blog/neuroai/Neuromorphic-Computing-Greener-Edgier-AI/)
    *on November 22, 2024.*'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*最初发布于* [*https://neural.vision*](https://neural.vision/blog/neuroai/Neuromorphic-Computing-Greener-Edgier-AI/)
    *于2024年11月22日。*'
