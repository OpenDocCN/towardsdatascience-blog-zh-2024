- en: Self-attentive sentence embedding for the recommendation system
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自注意力句子嵌入在推荐系统中的应用
- en: 原文：[https://towardsdatascience.com/self-attentive-sentence-embedding-for-the-recommendation-system-fc8af5817035?source=collection_archive---------10-----------------------#2024-05-22](https://towardsdatascience.com/self-attentive-sentence-embedding-for-the-recommendation-system-fc8af5817035?source=collection_archive---------10-----------------------#2024-05-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/self-attentive-sentence-embedding-for-the-recommendation-system-fc8af5817035?source=collection_archive---------10-----------------------#2024-05-22](https://towardsdatascience.com/self-attentive-sentence-embedding-for-the-recommendation-system-fc8af5817035?source=collection_archive---------10-----------------------#2024-05-22)
- en: What is self-attentive embedding, and how do we use it in recommendation systems?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是自注意力嵌入，如何在推荐系统中使用它？
- en: '[](https://medium.com/@bobi_29852?source=post_page---byline--fc8af5817035--------------------------------)[![Bào
    Bùi](../Images/3b35babbfc4becf3cf45575bb7b3b26e.png)](https://medium.com/@bobi_29852?source=post_page---byline--fc8af5817035--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fc8af5817035--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fc8af5817035--------------------------------)
    [Bào Bùi](https://medium.com/@bobi_29852?source=post_page---byline--fc8af5817035--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bobi_29852?source=post_page---byline--fc8af5817035--------------------------------)[![Bào
    Bùi](../Images/3b35babbfc4becf3cf45575bb7b3b26e.png)](https://medium.com/@bobi_29852?source=post_page---byline--fc8af5817035--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fc8af5817035--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fc8af5817035--------------------------------)
    [Bào Bùi](https://medium.com/@bobi_29852?source=post_page---byline--fc8af5817035--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fc8af5817035--------------------------------)
    ·6 min read·May 22, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fc8af5817035--------------------------------)
    ·6分钟阅读·2024年5月22日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ed3ccab6483e5f59165ac277457e7f19.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed3ccab6483e5f59165ac277457e7f19.png)'
- en: Photo by [Giulia Bertelli](https://unsplash.com/@giulia_bertelli) on [Unsplash](https://unsplash.com/photos/j_luAxi8fWc)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Giulia Bertelli](https://unsplash.com/@giulia_bertelli)提供，来源于[Unsplash](https://unsplash.com/photos/j_luAxi8fWc)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: The transformer layer and its attention mechanism are some of the most impactful
    ideas in the NLP community. They play a crucial role in many large language models,
    such as [ChatGPT](https://openai.com/blog/chatgpt) and [LLaMA](https://ai.meta.com/blog/large-language-model-llama-meta-ai/),
    which have recently taken the world by storm.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 层及其注意力机制是自然语言处理（NLP）领域最具影响力的思想之一。它们在许多大型语言模型中发挥着至关重要的作用，如[ChatGPT](https://openai.com/blog/chatgpt)和[LLaMA](https://ai.meta.com/blog/large-language-model-llama-meta-ai/)，这些模型最近在全球范围内引起了轰动。
- en: 'However, there is another interesting idea that originated from the NLP community,
    but its impact is mainly realized in the recommendation field: self-attentive
    sentence embedding. In this article, I will walk us through self-attentive sentence
    embedding [1] and how to apply it to the recommendation system.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，还有一个来自NLP社区的有趣想法，它的影响主要体现在推荐领域：自注意力句子嵌入。在本文中，我将带领大家了解自注意力句子嵌入[1]及其在推荐系统中的应用。
- en: How it works
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的
- en: Overall idea
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总体思路
- en: 'The paper''s main idea is to find a better way to encode a sentence into multiple
    embeddings that can capture various aspects of the sentence. Specifically, instead
    of encoding a sentence to a single embedding, the authors want to encode it into
    a 2D matrix, where each row embedding captures a different aspect of the sentence:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的主要思想是找到一种更好的方式，将句子编码为多个嵌入，捕捉句子的不同方面。具体而言，作者希望将句子编码为一个二维矩阵，而不是单一的嵌入，其中每一行的嵌入捕捉句子的不同方面：
- en: Once we have the sentence embeddings, we can use them for various downstream
    tasks, such as sentence analysis, author profiling…
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得了句子嵌入，就可以将它们用于各种下游任务，例如句子分析、作者画像等。
