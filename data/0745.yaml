- en: Depth Anything —A Foundation Model for Monocular Depth Estimation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Depth Anything — 单目深度估计的基础模型
- en: 原文：[https://towardsdatascience.com/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?source=collection_archive---------3-----------------------#2024-03-20](https://towardsdatascience.com/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?source=collection_archive---------3-----------------------#2024-03-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?source=collection_archive---------3-----------------------#2024-03-20](https://towardsdatascience.com/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?source=collection_archive---------3-----------------------#2024-03-20)
- en: '[🚀Sascha’s Paper Club](https://towardsdatascience.com/tagged/saschas-paper-club)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[🚀Sascha的论文俱乐部](https://towardsdatascience.com/tagged/saschas-paper-club)'
- en: 'Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data by L. Yang
    et. al.'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Depth Anything：释放大规模未标记数据的力量，L. Yang等人
- en: '[](https://medium.com/@SaschaKirch?source=post_page---byline--8a7920b5c9cc--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page---byline--8a7920b5c9cc--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8a7920b5c9cc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8a7920b5c9cc--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page---byline--8a7920b5c9cc--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@SaschaKirch?source=post_page---byline--8a7920b5c9cc--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page---byline--8a7920b5c9cc--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8a7920b5c9cc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8a7920b5c9cc--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page---byline--8a7920b5c9cc--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8a7920b5c9cc--------------------------------)
    ·11 min read·Mar 20, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8a7920b5c9cc--------------------------------)
    ·阅读时间：11分钟·2024年3月20日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/072b25a65b15d2c83fdfe3a30967c559.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/072b25a65b15d2c83fdfe3a30967c559.png)'
- en: Image created from [publication](https://arxiv.org/abs/2401.10891) by [Sascha
    Kirch](https://medium.com/@SaschaKirch)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自[Sascha Kirch](https://medium.com/@SaschaKirch)的[出版物](https://arxiv.org/abs/2401.10891)
- en: Monocular depth estimation, the prediction of distance in 3D space from a 2D
    image. The “ill posed and inherently ambiguous problem”, as stated in literally
    every paper on depth estimation, is a fundamental problem in computer vision and
    robotics. At the same time foundation models dominate the scene in deep learning
    based NLP and computer vision. Wouldn’t it be awesome if we could leverage their
    success for depth estimation too?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 单目深度估计，即从二维图像预测三维空间中的距离。正如几乎所有关于深度估计的论文所指出的那样，这个“病态且固有模糊的问题”是计算机视觉和机器人学中的一个基础性问题。同时，基础模型主导了基于深度学习的自然语言处理（NLP）和计算机视觉领域。若我们能够将它们的成功应用于深度估计，岂不是太棒了？
- en: In today’s paper walkthrough we’ll dive into Depth Anything, a foundation model
    for monocular depth estimation. We will discover its architecture, the tricks
    used to train it and how it is used for metric depth estimation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在今天的论文讲解中，我们将深入探讨Depth Anything，这是一种用于单目深度估计的基础模型。我们将了解它的架构、训练过程中使用的技巧以及它如何用于度量深度估计。
- en: '**Paper:** [Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data](https://arxiv.org/abs/2401.10891),
    Lihe Yang et.al., 19 Jan. 2024'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**论文：** [Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data](https://arxiv.org/abs/2401.10891)，Lihe
    Yang等，2024年1月19日'
- en: ''
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Resources:** [GitHub](https://github.com/LiheYoung/Depth-Anything) — [Project
    Page](https://depth-anything.github.io/) — [Demo](https://huggingface.co/spaces/LiheYoung/Depth-Anything)
    — [Checkpoints](https://huggingface.co/spaces/LiheYoung/Depth-Anything/tree/main)'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**资源：** [GitHub](https://github.com/LiheYoung/Depth-Anything) — [项目页面](https://depth-anything.github.io/)
    — [演示](https://huggingface.co/spaces/LiheYoung/Depth-Anything) — [检查点](https://huggingface.co/spaces/LiheYoung/Depth-Anything/tree/main)'
- en: ''
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Conference:** CVPR2024'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**会议：** CVPR2024'
- en: ''
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Category:** foundation models, monocular depth estimation'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**类别：** 基础模型，单目深度估计'
- en: ''
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[**Other Walkthroughs**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**:**'
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[**其他讲解**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**:**'
- en: '[[BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)]
    — [[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)]
    — [[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)]
    — [[Segment Anything](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    — [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    — [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[[BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)]
    — [[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)]
    — [[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)]
    — [[Segment Anything](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    — [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    — [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
