- en: Learning Generalist Models for Anomaly Detection
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习通用模型用于异常检测
- en: 原文：[https://towardsdatascience.com/learning-generalist-models-for-anomaly-detection-53d7a6a74474?source=collection_archive---------5-----------------------#2024-04-14](https://towardsdatascience.com/learning-generalist-models-for-anomaly-detection-53d7a6a74474?source=collection_archive---------5-----------------------#2024-04-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/learning-generalist-models-for-anomaly-detection-53d7a6a74474?source=collection_archive---------5-----------------------#2024-04-14](https://towardsdatascience.com/learning-generalist-models-for-anomaly-detection-53d7a6a74474?source=collection_archive---------5-----------------------#2024-04-14)
- en: '[](https://medium.com/@guansong-pang?source=post_page---byline--53d7a6a74474--------------------------------)[![Guansong
    Pang](../Images/449255d53cd68926a62460772561ac24.png)](https://medium.com/@guansong-pang?source=post_page---byline--53d7a6a74474--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--53d7a6a74474--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--53d7a6a74474--------------------------------)
    [Guansong Pang](https://medium.com/@guansong-pang?source=post_page---byline--53d7a6a74474--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@guansong-pang?source=post_page---byline--53d7a6a74474--------------------------------)[![Guansong
    Pang](../Images/449255d53cd68926a62460772561ac24.png)](https://medium.com/@guansong-pang?source=post_page---byline--53d7a6a74474--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--53d7a6a74474--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--53d7a6a74474--------------------------------)
    [Guansong Pang](https://medium.com/@guansong-pang?source=post_page---byline--53d7a6a74474--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--53d7a6a74474--------------------------------)
    ·9 min read·Apr 14, 2024
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--53d7a6a74474--------------------------------)
    ·阅读时间：9分钟·2024年4月14日
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Generalist Anomaly Detection (GAD) aims to train one single detection model
    that can generalize to detect anomalies in diverse datasets from different application
    domains without any further training on the target data.
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 通用异常检测（GAD）的目标是训练一个单一的检测模型，使其能够在不同应用领域的多样化数据集中泛化检测异常，而无需在目标数据上进行进一步训练。
- en: Work to be published at CVPR 2024 [1].
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 该工作将于CVPR 2024发布[1]。
- en: Overview
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: Some recent studies have showed that large pre-trained Visual-Language Models
    (VLMs) like CLIP have strong generalization capabilities on detecting industrial
    defects from various datasets, but their methods rely heavily on handcrafted text
    prompts about defects, making them difficult to generalize to anomalies in other
    applications, e.g., medical image anomalies or semantic anomalies in natural images.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究表明，大型预训练视觉-语言模型（VLMs）如CLIP，在从不同数据集中检测工业缺陷时具有强大的泛化能力，但它们的方法过于依赖于手工制作的缺陷文本提示，这使得它们难以在其他应用中泛化，例如医疗影像异常或自然图像中的语义异常。
- en: In this work, we propose to train a GAD model with few-shot normal images as
    sample prompts for AD on diverse datasets on the fly. To this end, we introduce
    a novel approach that learns an **in**-**c**on**t**ext **r**esidual **l**earning
    model for GAD, termed **InCTRL**.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们提出了通过少量正常图像作为样本提示，快速训练一个GAD模型，以便在多样化的数据集上进行异常检测。为此，我们引入了一种新方法，学习一种**上下文**残差**学习**模型来实现GAD，称为**InCTRL**。
- en: It is trained on an auxiliary dataset to discriminate anomalies from normal
    samples based on a holistic evaluation of the residuals between query images and
    few-shot normal sample prompts. Regardless of the datasets, per definition of
    anomaly, larger residuals are expected for anomalies than normal samples, thereby
    enabling InCTRL to generalize across different domains without further training.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 它在一个辅助数据集上进行训练，通过查询图像和少量正常样本提示之间的残差的整体评估，来区分异常和正常样本。不论数据集如何，按照异常的定义，异常样本的残差通常会比正常样本更大，从而使InCTRL能够在不同领域之间进行泛化，无需进一步训练。
- en: Comprehensive experiments on nine AD datasets are performed to establish a GAD
    benchmark that encapsulate the detection of industrial defect anomalies, medical
    anomalies, and semantic anomalies in both one-vs-all and multi-class setting,
    on which InCTRL is the best performer and significantly outperforms state-of-the-art
    competing methods. Code is available at [https://github.com/mala-lab/InCTRL](https://github.com/mala-lab/InCTRL).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在九个AD数据集上进行全面实验，以建立一个GAD基准，涵盖了工业缺陷异常、医学异常和语义异常的检测，涉及一对多和多类设置，其中InCTRL表现最佳，显著优于现有的最先进竞争方法。代码可在[https://github.com/mala-lab/InCTRL](https://github.com/mala-lab/InCTRL)获取。
- en: Introduction
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: Anomaly Detection (AD) is a crucial computer vision task that aims to detect
    samples that substantially deviate from the majority of samples in a dataset,
    due to its broad real-life applications such as industrial inspection, medical
    imaging analysis, and scientific discovery, etc. [2–3]. Current AD paradigms are
    focused on individually building one model on the training data, e.g.,, a set
    of anomaly-free samples, of each target dataset, such as data reconstruction approach,
    one-class classification, and knowledge distillation approach. Although these
    approaches have shown remarkable detection performance on various AD benchmarks,
    they require the availability of large training data and the skilled detection
    model training per dataset. Thus, they become infeasible in application scenarios
    where training on the target dataset is not allowed due to either data privacy
    issues, e.g., arising from using those data in training the models due to machine
    unlearning [3], or unavailability of large-scale training data in the deployment
    of new applications. To tackle these challenges, this work explores the problem
    of learning *Generalist Anomaly Detection (GAD)* models, aiming to train one single
    detection model that can generalize to detect anomalies in diverse datasets from
    different application domains without any training on the target data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测（AD）是一个重要的计算机视觉任务，旨在检测那些与数据集中大多数样本显著偏离的样本，这一任务具有广泛的实际应用，例如工业检测、医学影像分析和科学发现等[2–3]。当前的AD范式集中于在每个目标数据集的训练数据上单独构建一个模型，例如无异常样本集的数据重建方法、一类分类和知识蒸馏方法。尽管这些方法在各种AD基准测试中表现出了显著的检测性能，但它们需要大量的训练数据以及每个数据集上需要熟练的检测模型训练。因此，在某些应用场景下，它们变得不可行，尤其是在由于数据隐私问题（例如，因使用这些数据训练模型而导致的机器反学习问题[3]）或新应用部署中缺乏大规模训练数据等原因，无法对目标数据集进行训练的情况下。为了应对这些挑战，本研究探讨了*通用异常检测（GAD）*模型的问题，旨在训练一个单一的检测模型，该模型能够在不同应用领域的多样数据集中进行异常检测，而无需对目标数据进行任何训练。
- en: Being pre-trained on web-scale image-text data, large Visual-Language Models
    (VLMs) like CLIP have exhibited superior generalization capabilities in recent
    years, achieving accurate visual recognition across different datasets without
    any fine-tuning or adaptation on the target data. More importantly, some very
    recent studies (e.g., WinCLIP [5]) show that these VLMs can also be utilized to
    achieve remarkable generalization on different defect detection datasets. Nevertheless,
    a significant limitation of these models is their dependency on a large set of
    manually crafted prompts specific to defects. This reliance restricts their applicability,
    making it challenging to extend their use to detecting anomalies in other data
    domains, e.g., medical image anomalies or semantic anomalies in one-vs-all or
    multi-class settings.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大型视觉语言模型（VLMs），如CLIP，已经在网页规模的图像-文本数据上进行预训练，近年来表现出卓越的泛化能力，能够在不同的数据集上实现准确的视觉识别，而无需对目标数据进行任何微调或适配。更重要的是，一些非常近期的研究（例如，WinCLIP
    [5]）表明，这些VLMs也可以用来在不同的缺陷检测数据集上实现显著的泛化。然而，这些模型的一个重要限制是它们依赖于大量手工制作的、特定于缺陷的提示。这种依赖限制了它们的适用性，使得将其扩展到检测其他数据领域的异常变得具有挑战性，例如医学影像中的异常或在一对多或多类设置中的语义异常。
- en: To address this problem, we propose to train a GAD model that aims to utilize
    few-shot normal images from any target dataset as sample prompts for supporting
    GAD on the fly, as illustrated in Figure 1(Top). The few-shot setting is motivated
    by the fact that it is often easy to obtain few-shot normal images in real-world
    applications. Furthermore, these few-shot samples are not used for model training/tuning;
    they are just used as sample prompts for enabling the anomaly scoring of test
    images during inference. This formulation is fundamentally different from current
    few-shot AD methods that use these target samples and their extensive augmented
    versions to train the detection model, which can lead to an overfitting of the
    target dataset and fail to generalize to other datasets, as shown in Figure 1(Bottom).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们提出训练一个GAD模型，该模型旨在利用来自任何目标数据集的少量正常图像作为样本提示，支持快速的GAD，如图1（顶部）所示。少量样本的设置源于一个事实，即在实际应用中，获取少量正常图像通常很容易。此外，这些少量样本并不用于模型训练/调优；它们仅作为样本提示，在推理过程中用于支持测试图像的异常评分。这个公式与当前的少量样本AD方法根本不同，后者使用这些目标样本及其扩展版本来训练检测模型，这可能导致目标数据集的过拟合，无法泛化到其他数据集，如图1（底部）所示。
- en: '![](../Images/3fe0dead46f46cfcbebafc24c90de4c6.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3fe0dead46f46cfcbebafc24c90de4c6.png)'
- en: We then introduce an GAD approach, the first of its kind, that learns an **in**-**c**on**t**ext
    **r**esidual **l**earning model based on CLIP, termed **InCTRL**. It trains an
    GAD model to discriminate anomalies from normal samples by learning to identify
    the residuals/discrepancies between query images and a set of few-shot normal
    images from auxiliary data. The few-shot normal images, namely **in-context sample
    prompts**, serve as *prototypes of normal patterns*. When comparing with the features
    of these normal patterns, per definition of anomaly, a larger residual is typically
    expected for anomalies than normal samples in datasets of different domains, so
    the learned in-context residual model can generalize to detect diverse types of
    anomalies across the domains. To capture the residuals better, **InCTRL** models
    the in-context residuals at both the image and patch levels, gaining an in-depth
    in-context understanding of what constitutes an anomaly. Further, our in-context
    residual learning can also enable a seamless incorporation of normal/abnormal
    text prompt-guided prior knowledge into the detection model, providing an additional
    strength for the detection from the text-image-aligned semantic space.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接着介绍了一种GAD方法，这是首个此类方法，通过基于CLIP学习一个**in**-**c**on**t**ext **r**esidual **l**earning模型，命名为**InCTRL**。它训练一个GAD模型，通过学习识别查询图像与一组来自辅助数据的少量正常图像之间的残差/差异，从而区分异常与正常样本。这些少量正常图像，亦即**上下文样本提示**，作为*正常模式的原型*。根据异常的定义，与这些正常模式的特征进行比较时，异常样本通常会产生比正常样本更大的残差，因此所学的上下文残差模型能够在不同领域的数据集上泛化，检测到各种类型的异常。为了更好地捕捉残差，**InCTRL**在图像和图块层面建模上下文残差，深入理解什么构成了异常。此外，我们的上下文残差学习还能够无缝地将正常/异常的文本提示引导的先验知识融入检测模型，为来自文本-图像对齐语义空间的检测提供额外的优势。
- en: Extensive experiments on nine AD datasets are performed to establish a GAD benchmark
    that encapsulates three types of popular AD tasks, including industrial defect
    anomaly detection, medical image anomaly detection, and semantic anomaly detection
    under both one-vs-all and multi-class settings. Our results show that **InCTRL**
    significantly surpasses existing state-of-the-art methods.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在九个AD数据集上进行了广泛的实验，以建立一个包含三种流行AD任务的GAD基准，包括工业缺陷异常检测、医学图像异常检测和语义异常检测，涵盖了单一对抗与多类设置。我们的结果表明，**InCTRL**显著超过了现有的最先进方法。
- en: Approach
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法
- en: Our approach **InCTRL** is designed to effectively model the in-context residual
    between a query image and a set of few-shot normal images as sample prompts, utilizing
    the generalization capabilities of CLIP to detect unusual residuals for anomalies
    from different application domains.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法**InCTRL**旨在有效地建模查询图像与一组少量正常图像之间的上下文残差，利用CLIP的泛化能力，检测来自不同应用领域的异常残差。
- en: CLIP is a VLM consisting of a text encoder and a visual encoder, with the image
    and text representations from these encoders well aligned by pre-training on web-scale
    text-image data. **InCTRL** is optimized using auxiliary data via an in-context
    residual learning in the image encoder, with the learning augmented by text prompt-guided
    prior knowledge from the text encoder.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP是一个视觉语言模型（VLM），由文本编码器和视觉编码器组成，通过在大规模的文本-图像数据上进行预训练，使得来自这些编码器的图像和文本表示得到良好的对齐。**InCTRL**通过在图像编码器中使用上下文残差学习，结合辅助数据进行优化，并通过文本编码器提供的文本提示引导的先验知识增强学习。
- en: To be more specific, as illustrated in Fig.2, we first simulate an in-context
    learning example that contains one query image **x** and a set of few-shot normal
    sample prompts **P’**, both of which are randomly sampled from the auxiliary data.
    Through the visual encoder, we then perform multi-layer patch-level and image-level
    residual learning to respectively capture local and global discrepancies between
    the query and few-shot normal sample prompts. Further, our model allows a seamless
    incorporation of normal and abnormal text prompts-guided prior knowledge from
    the text encoder based on the similarity between these textual prompt embeddings
    and the query images . The training of **InCTRL** is to optimize a few projection/adaptation
    layers attached to the visual encoder to learn a larger anomaly score for anomaly
    samples than normal samples in the training data, with the original parameters
    in both encoders frozen; during inference, a test image, together with the few-shot
    normal image prompts from the target dataset and the text prompts, is put forward
    through our adapted CLIP-based GAD network, whose output is the anomaly score
    for the test image.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地，如图2所示，我们首先模拟一个包含一个查询图像**x**和一组少量正常样本提示**P'**的上下文学习示例，这些图像和提示均随机从辅助数据中采样。然后通过视觉编码器，我们执行多层次的补丁级别和图像级别的残差学习，分别捕捉查询图像与少量正常样本提示之间的局部和全局差异。此外，我们的模型能够无缝地将来自文本编码器的正常和异常文本提示引导的先验知识融入到学习过程中，这些文本提示嵌入与查询图像之间的相似度用于指导学习。**InCTRL**的训练是优化附加在视觉编码器上的少量投影/适应层，使得训练数据中的异常样本相较于正常样本具有更大的异常分数，同时保持两个编码器中的原始参数冻结；在推理过程中，将测试图像与目标数据集中的少量正常图像提示和文本提示一起输入到我们改进的基于CLIP的GAD网络中，其输出为测试图像的异常分数。
- en: '![](../Images/b71f47a82528a9d0f1aa0512687df9c8.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b71f47a82528a9d0f1aa0512687df9c8.png)'
- en: Empirical Results
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验结果
- en: '**Datasets and Evaluation Metrics.** To verify the efficiency of our method,
    we conduct comprehensive experiments across nine real-world AD datasets, including
    five industrial defect inspection dataset (MVTec AD, VisA, AITEX, ELPV, SDD),
    two medical image datasets (BrainMRI, HeadCT), and two semantic anomaly detection
    datasets: MNIST and CIFAR-10 under both one-vs-all and multi-class protocols.
    Under the one-vs-all protocol, one class is used as normal, with the other classes
    treated as abnormal; while under the multi-class protocol, images of even-number
    classes from MNIST and animal-related classes from CIFAR-10 are treated as normal,
    with the images of the other classes are considered as anomalies.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据集与评估指标。** 为了验证我们方法的有效性，我们在九个真实世界的异常检测数据集上进行了全面实验，包括五个工业缺陷检测数据集（MVTec AD,
    VisA, AITEX, ELPV, SDD）、两个医学影像数据集（BrainMRI, HeadCT），以及两个语义异常检测数据集：MNIST和CIFAR-10，采用一对多和多类协议。在一对多协议下，选择一个类作为正常类，其余类作为异常类；而在多类协议下，MNIST中的偶数类和CIFAR-10中的与动物相关的类被视为正常类，其余类的图像被视为异常。'
- en: To assess the GAD performance, MVTec AD, the combination of its training and
    test sets, is used as the auxiliary training data, on which GAD models are trained,
    and they are subsequently evaluated on the test set of the other eight datasets
    without any further training. We train the model on VisA when evaluating the performance
    on MVTec AD.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估GAD性能，使用MVTec AD及其训练和测试集的组合作为辅助训练数据，在此上训练GAD模型，并随后在其他八个数据集的测试集上进行评估，无需进一步训练。在评估MVTec
    AD上的性能时，我们在VisA上训练模型。
- en: The few-shot normal prompts for the target data are randomly sampled from the
    training set of target datasets and remain the same for all models for fair comparison.
    We evaluate the performance with the number of few-shot normal prompt set to K
    = 2, 4, 8\. The reported results are averaged over three independent runs with
    different random seeds.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 针对目标数据的少量常规提示是从目标数据集的训练集中随机采样的，并且对于所有模型保持一致，以确保公平比较。我们评估了少量常规提示集为K = 2, 4, 8时的性能。报告的结果是基于三次独立实验的平均值，且每次实验使用不同的随机种子。
- en: As for evaluation metrics, we use two popular metrics AUROC (Area Under the
    Receiver Operating Characteristic) and AUPRC (Area Under the Precision-Recall
    Curve) to evaluate the AD performance.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 关于评估指标，我们使用两个流行的指标AUROC（接收操作特征曲线下面积）和AUPRC（精准率-召回率曲线下面积）来评估AD性能。
- en: '**Results.** The main results are reporeted in Tables 1 and 2\. For the 11
    industrial defect AD datasets, **InCTRL** significantly outperforms all competing
    models on almost all cases across the three few-shot settings in both AUROC and
    AUPRC. With more few-shot image prompts, the performance of all methods generally
    gets better. **InCTRL** canutilize the increasing few-shot samples well and remain
    the superiority over the competing methods.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**结果。** 主要结果见表格1和表格2。对于11个工业缺陷检测（AD）数据集，**InCTRL**在几乎所有的情况中，在三种少量样本设置下，均显著优于所有竞争模型，无论是在AUROC还是AUPRC方面。随着更多少量样本图像提示的加入，所有方法的表现普遍得到提升。**InCTRL**能够很好地利用增加的少量样本，并保持相对于竞争方法的优势。'
- en: '![](../Images/3315761a9681154f009c7313b51c54e2.png)![](../Images/541a7bcbe2af78ff4e3fb584a8151144.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3315761a9681154f009c7313b51c54e2.png)![](../Images/541a7bcbe2af78ff4e3fb584a8151144.png)'
- en: '**Ablation Study.** We examine the contribution of three key components of
    our approach on the generalization: text prompt-guided features (T), patch-level
    residuals (P), and image-level residuals (I), as well as their combinations. The
    results are reported in Table 3\. The experiment results indicate that for industrial
    defect AD datasets, visual residual features play a more significant role compared
    to text prompt-based features, particularly on datasets like ELPV, SDD, and AITEX.
    On the medical image AD datasets, both visual residuals and textual knowledge
    contribute substantially to performance enhancement, exhibiting a complementary
    relation. On semantic AD datasets, the results are dominantly influenced by patch-level
    residuals and/or text prompt-based features. Importantly, our three components
    are generally mutually complementary, resulting in the superior detection generalization
    across the datasets.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**消融研究。** 我们考察了方法中三个关键组件对泛化能力的贡献：文本提示引导的特征（T）、补丁级残差（P）和图像级残差（I），以及它们的组合。结果见表格3。实验结果表明，对于工业缺陷AD数据集，视觉残差特征在比文本提示特征更为重要，尤其是在像ELPV、SDD和AITEX这样的数据集上。在医学图像AD数据集上，视觉残差和文本知识对性能提升都有显著贡献，表现出互补关系。在语义AD数据集上，结果主要受到补丁级残差和/或文本提示特征的影响。重要的是，我们的三个组件通常是互相补充的，从而使得在各数据集上的检测泛化能力更强。'
- en: '![](../Images/4abc7255039bcdccd2675268acada161.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4abc7255039bcdccd2675268acada161.png)'
- en: '**Significance of In-context Residual Learning.** To assess the importance
    of learning the residuals in **InCTRL**, we experiment with two alternative operations
    in both multi-layer patch-level and image-level residual learning: replacing the
    residual operation with 1) a **concatenation** operation and 2) an **average**
    operation, with all the other components of **InCTRL** fixed. As shown in Table
    3, the in-context residual learning generalizes much better than the other two
    alternative ways, significantly enhancing the model’s performance in GAD across
    three distinct domains.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**In-context残差学习的重要性。** 为了评估**InCTRL**中残差学习的重要性，我们在多层补丁级和图像级残差学习中尝试了两种替代操作：1）将残差操作替换为**拼接**操作，2）将残差操作替换为**平均**操作，其他**InCTRL**组件保持不变。如表格3所示，in-context残差学习的泛化能力明显优于这两种替代方式，显著提升了该模型在GAD任务中的性能，且跨三个不同的领域表现良好。'
- en: Conclusion
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this work we introduce a GAD task to evaluate the generalization capability
    of AD methods in identifying anomalies across various scenarios without any training
    on the target datasets. This is the first study dedicated to a generalist approach
    to anomaly detection, encompassing industrial defects, medical anomalies, and
    semantic anomalies. Then we propose an approach, called **InCTRL**, to addressing
    this problem under a few-shot setting. **InCTRL** achieves a superior GAD generalization
    by holistic in-context residual learning. Extensive experiments are performed
    on nine AD datasets to establish a GAD evaluation benchmark for the aforementioned
    three popular AD tasks, on which **InCTRL** significantly and consistently outperforms
    SotA competing models across multiple few-shot settings.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了一个GAD任务，用于评估异常检测方法在不同场景下识别异常的泛化能力，且无需在目标数据集上进行任何训练。这是首个专注于通用异常检测方法的研究，涵盖了工业缺陷、医学异常和语义异常。随后，我们提出了一种方法，称为**InCTRL**，在少样本设置下解决此问题。**InCTRL**通过整体上下文残差学习实现了卓越的GAD泛化能力。在九个异常检测数据集上进行了广泛实验，为上述三种流行的异常检测任务建立了GAD评估基准，在多个少样本设置下，**InCTRL**显著且持续地超越了现有最先进的竞争模型。
- en: '*Please check out the full paper [1] for more details of the approach and the
    experiments. Code is publicly available at* [https://github.com/mala-lab/InCTRL](https://github.com/mala-lab/InCTRL).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*请查阅完整论文[1]以获取更多关于方法和实验的详细信息。代码可以在* [https://github.com/mala-lab/InCTRL](https://github.com/mala-lab/InCTRL)
    *公开获取。*'
- en: References
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Zhu, Jiawen, and Guansong Pang. “Toward Generalist Anomaly Detection via
    In-context Residual Learning with Few-shot Sample Prompts.” *arXiv preprint arXiv:2403.06495*
    (2024).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Zhu, Jiawen, 和 Guansong Pang. “通过少样本提示的上下文残差学习实现通用异常检测。” *arXiv 预印本 arXiv:2403.06495*
    (2024)。'
- en: '[2] Pang, Guansong, et al. “Deep learning for anomaly detection: A review.”
    *ACM computing surveys (CSUR)* 54.2 (2021): 1–38.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Pang, Guansong, 等. “深度学习在异常检测中的应用：综述。” *ACM计算机综述（CSUR）* 54.2 (2021)：1–38。'
- en: '[3] Cao, Yunkang, et al. “A Survey on Visual Anomaly Detection: Challenge,
    Approach, and Prospect.” *arXiv preprint arXiv:2401.16402* (2024).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Cao, Yunkang, 等. “视觉异常检测综述：挑战、方法和前景。” *arXiv 预印本 arXiv:2401.16402* (2024)。'
- en: '[4] Xu, Jie, et al. “Machine unlearning: Solutions and challenges.” *IEEE Transactions
    on Emerging Topics in Computational Intelligence* (2024).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Xu, Jie, 等. “机器遗忘：解决方案与挑战。” *IEEE《计算智能新兴话题》期刊* (2024)。'
- en: '[5] Jeong, Jongheon, et al. “Winclip: Zero-/few-shot anomaly classification
    and segmentation.” *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*. 2023.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Jeong, Jongheon, 等. “Winclip: 零样本/少样本异常分类和分割。” *IEEE/CVF计算机视觉与模式识别会议论文集*。2023。'
