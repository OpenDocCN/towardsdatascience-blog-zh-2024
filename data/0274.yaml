- en: The Math Behind the Adam Optimizer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Adam优化器背后的数学
- en: 原文：[https://towardsdatascience.com/the-math-behind-adam-optimizer-c41407efe59b?source=collection_archive---------0-----------------------#2024-01-30](https://towardsdatascience.com/the-math-behind-adam-optimizer-c41407efe59b?source=collection_archive---------0-----------------------#2024-01-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-math-behind-adam-optimizer-c41407efe59b?source=collection_archive---------0-----------------------#2024-01-30](https://towardsdatascience.com/the-math-behind-adam-optimizer-c41407efe59b?source=collection_archive---------0-----------------------#2024-01-30)
- en: Why is Adam the most popular optimizer in Deep Learning? Let’s understand it
    by diving into its math, and recreating the algorithm
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么Adam是深度学习中最受欢迎的优化器？让我们通过深入了解其数学原理并重现算法来理解它。
- en: '[](https://medium.com/@cristianleo120?source=post_page---byline--c41407efe59b--------------------------------)[![Cristian
    Leo](../Images/99074292e7dfda50cf50a790b8deda79.png)](https://medium.com/@cristianleo120?source=post_page---byline--c41407efe59b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c41407efe59b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c41407efe59b--------------------------------)
    [Cristian Leo](https://medium.com/@cristianleo120?source=post_page---byline--c41407efe59b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@cristianleo120?source=post_page---byline--c41407efe59b--------------------------------)[![Cristian
    Leo](../Images/99074292e7dfda50cf50a790b8deda79.png)](https://medium.com/@cristianleo120?source=post_page---byline--c41407efe59b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c41407efe59b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c41407efe59b--------------------------------)
    [Cristian Leo](https://medium.com/@cristianleo120?source=post_page---byline--c41407efe59b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c41407efe59b--------------------------------)
    ·16 min read·Jan 30, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c41407efe59b--------------------------------)
    ·16分钟阅读·2024年1月30日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/70d06b2ec8e8cb2a346295529f913efd.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/70d06b2ec8e8cb2a346295529f913efd.png)'
- en: Image generated by DALLE-2
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由DALLE-2生成
- en: If you’ve clicked on this article, you’ve likely heard about Adam, a name that
    has gained notable recognition in many winning Kaggle competitions. It’s common
    to experiment with a few optimizers like SGD, Adagrad, Adam, or AdamW, but truly
    understanding their mechanics is a different story. By the end of this post, you’ll
    be among the select few who not only know about Adam optimization but also understand
    how to leverage its power effectively.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你点击了这篇文章，你可能已经听说过Adam，这个名字在许多Kaggle比赛中获得了显著的认可。通常，我们会尝试一些优化器，如SGD、Adagrad、Adam或AdamW，但真正理解它们的机制则是另一回事。通过阅读本文，你将成为少数几位不仅了解Adam优化的原理，还能有效利用其强大功能的人之一。
- en: By the way, in my previous post I described the math behind Stochastic Gradient
    Descent, if you missed it, I recommend reading it.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，在我之前的文章中，我介绍了随机梯度下降的数学，如果你错过了，推荐你阅读一下。
- en: '[](https://medium.com/@cristianleo120/stochastic-gradient-descent-math-and-python-code-35b5e66d6f79?source=post_page-----c41407efe59b--------------------------------)
    [## Stochastic Gradient Descent: Math and Python Code'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@cristianleo120/stochastic-gradient-descent-math-and-python-code-35b5e66d6f79?source=post_page-----c41407efe59b--------------------------------)
    [## 随机梯度下降：数学与Python代码'
- en: Deep Dive on Stochastic Gradient Descent. Algorithm, assumptions, benefits,
    formula, and practical implementation.
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深入了解随机梯度下降：算法、假设、优点、公式及实际应用。
- en: medium.com](https://medium.com/@cristianleo120/stochastic-gradient-descent-math-and-python-code-35b5e66d6f79?source=post_page-----c41407efe59b--------------------------------)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@cristianleo120/stochastic-gradient-descent-math-and-python-code-35b5e66d6f79?source=post_page-----c41407efe59b--------------------------------)
- en: '**Index**'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**索引**'
- en: '**·** [**1: Understanding the Basics**](#e382)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**·** [**1: 理解基础知识**](#e382)'
- en: '∘ [1.1: What is the Adam Optimizer?](#63c0)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '∘ [1.1: 什么是Adam优化器？](#63c0)'
- en: '∘ [1.2: The Mechanics of Adam](#de8c)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '∘ [1.2: Adam的机制](#de8c)'
- en: '**·** [**2\. Adam’s Algorithm Explained**](#bc0c)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**·** [**2. Adam算法解析**](#bc0c)'
- en: ∘ [2.1 The Mathematics Behind Adam](#5ff9)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [2.1 Adam背后的数学](#5ff9)
- en: ∘ [2.2 The Role](#3663)…
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [2.2 角色](#3663)…
