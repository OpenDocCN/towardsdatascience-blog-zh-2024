- en: Fine-Tune Llama 3.2 for Powerful Performance on Targeted Tasks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调 Llama 3.2 实现针对性任务的强大性能
- en: 原文：[https://towardsdatascience.com/fine-tune-llama-3-2-for-powerful-performance-in-targeted-domains-8c4fccef93dd?source=collection_archive---------0-----------------------#2024-10-10](https://towardsdatascience.com/fine-tune-llama-3-2-for-powerful-performance-in-targeted-domains-8c4fccef93dd?source=collection_archive---------0-----------------------#2024-10-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/fine-tune-llama-3-2-for-powerful-performance-in-targeted-domains-8c4fccef93dd?source=collection_archive---------0-----------------------#2024-10-10](https://towardsdatascience.com/fine-tune-llama-3-2-for-powerful-performance-in-targeted-domains-8c4fccef93dd?source=collection_archive---------0-----------------------#2024-10-10)
- en: Learn how you can fine-tune Llama3.2, Meta’s most recent Large language model,
    to achieve powerful performance on targeted domains
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解如何微调 Llama3.2，Meta 最新的大型语言模型，以在特定领域实现强大的性能
- en: '[](https://oieivind.medium.com/?source=post_page---byline--8c4fccef93dd--------------------------------)[![Eivind
    Kjosbakken](../Images/5f91b74428e1202fc4a176a3dd1cb1c7.png)](https://oieivind.medium.com/?source=post_page---byline--8c4fccef93dd--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8c4fccef93dd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8c4fccef93dd--------------------------------)
    [Eivind Kjosbakken](https://oieivind.medium.com/?source=post_page---byline--8c4fccef93dd--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://oieivind.medium.com/?source=post_page---byline--8c4fccef93dd--------------------------------)[![Eivind
    Kjosbakken](../Images/5f91b74428e1202fc4a176a3dd1cb1c7.png)](https://oieivind.medium.com/?source=post_page---byline--8c4fccef93dd--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8c4fccef93dd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8c4fccef93dd--------------------------------)
    [Eivind Kjosbakken](https://oieivind.medium.com/?source=post_page---byline--8c4fccef93dd--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8c4fccef93dd--------------------------------)
    ·10 min read·Oct 10, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8c4fccef93dd--------------------------------)
    ·阅读时间：10分钟·2024年10月10日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: In this article, I discuss how to run [Llama 3.2](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/)
    locally and fine-tune the model to increase its performance on specific tasks.
    Working with large language models has become a critical part of any data scientist’s
    or ML engineer’s job, and fine-tuning the large language models can lead to powerful
    improvements in the language models’ capabilities. This article will thus show
    you how you can fine-tune Llama3.2 to improve its performance within a targeted
    domain.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将讨论如何在本地运行 [Llama 3.2](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/)
    并微调该模型，以提升其在特定任务上的表现。与大型语言模型的工作已经成为数据科学家或机器学习工程师工作的关键部分，而微调大型语言模型能够带来语言模型能力的强大提升。因此，本文将向你展示如何微调
    Llama3.2，以提高其在特定领域中的性能。
- en: '![](../Images/af3a47e9176f7611742d79620e2189fb.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af3a47e9176f7611742d79620e2189fb.png)'
- en: This article will show you how to work with and fine-tune Llama3.2 to better
    solve domain-specific problems. Image by ChatGPT.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将展示如何使用和微调 Llama3.2，以更好地解决特定领域的问题。图片来源：ChatGPT。
- en: Motivation
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动机
- en: My motivation for this article is that I want to spend more time working on
    large language models and figure out how to utilize them effectively. There are
    many options for utilizing large language models effectively, such as [prompt
    tuning](https://www.datacamp.com/tutorial/understanding-prompt-tuning), [RAG systems](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/),
    or [function calling](https://platform.openai.com/docs/guides/function-calling).
    However, fine-tuning a model is also a valid option, though it requires more effort
    than my three options. Fine-tuning large language models requires a solid GPU,
    training data, which may require a lot of manual work, and setting up the training
    script. Luckily, however, the [Unsloth library](https://unsloth.ai/) makes fine-tuning
    a lot simpler, which is…
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我写这篇文章的动机是我想花更多的时间研究大型语言模型，并弄清楚如何有效地利用它们。有效利用大型语言模型有很多选择，比如[提示调优](https://www.datacamp.com/tutorial/understanding-prompt-tuning)、[RAG
    系统](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/)、或[函数调用](https://platform.openai.com/docs/guides/function-calling)。然而，微调模型也是一个有效的选择，尽管它比我的三种选择需要更多的努力。微调大型语言模型需要强大的
    GPU、训练数据（这可能需要大量的手动工作），以及设置训练脚本。然而幸运的是，[Unsloth 库](https://unsloth.ai/)使微调变得更加简单，这真是…
