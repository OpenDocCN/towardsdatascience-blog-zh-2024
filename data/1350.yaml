- en: Build Your Own ChatGPT-like Chatbot with Java and Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Java 和 Python 构建你自己的类似 ChatGPT 的聊天机器人
- en: 原文：[https://towardsdatascience.com/build-your-own-chatgpt-like-chatbot-with-java-and-python-5def2c4852c3?source=collection_archive---------1-----------------------#2024-05-30](https://towardsdatascience.com/build-your-own-chatgpt-like-chatbot-with-java-and-python-5def2c4852c3?source=collection_archive---------1-----------------------#2024-05-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/build-your-own-chatgpt-like-chatbot-with-java-and-python-5def2c4852c3?source=collection_archive---------1-----------------------#2024-05-30](https://towardsdatascience.com/build-your-own-chatgpt-like-chatbot-with-java-and-python-5def2c4852c3?source=collection_archive---------1-----------------------#2024-05-30)
- en: Creating a custom LLM inference infrastructure from scratch
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从零开始创建自定义的 LLM 推理基础设施
- en: '[](https://cardstdani.medium.com/?source=post_page---byline--5def2c4852c3--------------------------------)[![Daniel
    García Solla](../Images/b6e7bc9fdfdfcda7875215b1e0264d9e.png)](https://cardstdani.medium.com/?source=post_page---byline--5def2c4852c3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5def2c4852c3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5def2c4852c3--------------------------------)
    [Daniel García Solla](https://cardstdani.medium.com/?source=post_page---byline--5def2c4852c3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://cardstdani.medium.com/?source=post_page---byline--5def2c4852c3--------------------------------)[![Daniel
    García Solla](../Images/b6e7bc9fdfdfcda7875215b1e0264d9e.png)](https://cardstdani.medium.com/?source=post_page---byline--5def2c4852c3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5def2c4852c3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5def2c4852c3--------------------------------)
    [Daniel García Solla](https://cardstdani.medium.com/?source=post_page---byline--5def2c4852c3--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5def2c4852c3--------------------------------)
    ·25 min read·May 30, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5def2c4852c3--------------------------------)
    ·阅读时间 25 分钟·2024年5月30日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/04f5c0784845e51a157b80a2649323bb.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04f5c0784845e51a157b80a2649323bb.png)'
- en: Image by author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作者图片
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: In recent years, [**Large Language Models (LLMs)**](https://aws.amazon.com/what-is/large-language-model/)
    have emerged as a game-changing technology that has revolutionized the way we
    interact with machines. These models, represented by OpenAI’s GPT series with
    examples such as GPT-3.5 or GPT-4, can take a sequence of input text and generate
    coherent, contextually relevant, and human-sounding text in reply. Thus, its applications
    are wide-ranging and cover a variety of fields, such as customer service, content
    creation, language translation, or code generation. However, at the core of these
    capabilities are advanced machine-learning/statistical techniques, including attention
    mechanisms for improving the natural language understanding process, transfer
    learning to provide foundational models at scale, data augmentation, or even [**Reinforcement
    Learning From Human Feedback**](https://huggingface.co/blog/rlhf), which enable
    these systems to extend their training process and improve their performance continuously
    along inference.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，**[大型语言模型（LLMs）](https://aws.amazon.com/what-is/large-language-model/)**
    已成为一项颠覆性技术，彻底改变了我们与机器互动的方式。这些模型以 OpenAI 的 GPT 系列为代表，举例如 GPT-3.5 或 GPT-4，它们可以接受一段输入文本并生成连贯的、符合上下文的、听起来像人类的回复文本。因此，它们的应用领域广泛，涵盖了客户服务、内容创作、语言翻译或代码生成等多个领域。然而，这些能力的核心是先进的机器学习/统计技术，包括用于提升自然语言理解过程的注意力机制、用于大规模提供基础模型的迁移学习、数据增强，甚至是
    [**基于人类反馈的强化学习**](https://huggingface.co/blog/rlhf)，这些技术使得系统能够在推理过程中不断扩展训练并提升性能。
- en: As a subset of artificial intelligence, machine learning is responsible for
    processing datasets to identify patterns and develop models that accurately represent
    the data’s nature. This approach generates valuable knowledge and unlocks a variety
    of tasks, for example, content generation, underlying the field of [**Generative
    AI**](https://en.wikipedia.org/wiki/Generative_artificial_intelligence) that drives
    large language models. It is worth highlighting that this field is not solely
    focused on natural language, but also on any type of content susceptible to being
    generated. From audio, with models capable of generating sounds, voices, or music;
    videos through the latest models like OpenAI’s [**SORA**](https://openai.com/index/sora/);
    or images, as well as editing and style transfer from text sequences. The latter
    data format is especially valuable, since by using multimodal integration and
    the help of image/text embedding technologies, it is possible to effectively illustrate
    the potential of knowledge representation through natural language.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 作为人工智能的一个子集，机器学习负责处理数据集，识别模式并开发准确表示数据特性的模型。这种方法生成了有价值的知识，并解锁了各种任务，例如内容生成，这为[**生成式人工智能**](https://en.wikipedia.org/wiki/Generative_artificial_intelligence)领域提供了基础，推动了大语言模型的发展。值得强调的是，这个领域不仅仅专注于自然语言，还包括任何可能被生成的内容。从音频，拥有生成声音、语音或音乐的模型；到视频，最新的模型如OpenAI的[**SORA**](https://openai.com/index/sora/)；再到图像，以及从文本序列生成图像、编辑和风格迁移。后者的数据格式尤其有价值，因为通过使用多模态集成和图像/文本嵌入技术，可以有效地展示通过自然语言进行知识表示的潜力。
- en: Nevertheless, creating and maintaining models to perform this kind of operation,
    particularly at a large scale, is not an easy job. One of the main reasons is
    data, as it represents the major contribution to a well-functioning model. That
    is, training a model with a structurally optimal architecture and high-quality
    data will produce valuable results. Conversely, if the provided data is poor,
    the model will produce misleading outputs. Therefore, when creating a dataset,
    it should contain an appropriate volume of data for the particular model architecture.
    This requirement complicates data treatment and quality verification, in addition
    to the potential legal and privacy issues that must be considered if the data
    is collected by automation or scraping.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，创建和维护执行此类操作的模型，尤其是在大规模的情况下，并不是一项容易的工作。主要原因之一是数据，因为它是构建良好运行模型的关键要素。也就是说，使用结构优化的架构和高质量的数据训练模型将产生有价值的结果。相反，如果提供的数据质量差，模型将生成误导性的输出。因此，在创建数据集时，它应该包含适量的数据，以适应特定模型架构的需求。这个要求使得数据处理和质量验证变得更加复杂，此外，如果数据是通过自动化或抓取方式收集的，还必须考虑潜在的法律和隐私问题。
- en: Another reason lies in hardware. Modern deployed models, which need to process
    vast amounts of data from many users simultaneously, are not only large in size
    but also require substantial computing resources for performing inference tasks
    and providing quality service to their clients. That is reflected in equally significant
    costs in economic terms. On the one hand, setting up servers and data centers
    with the right hardware, considering that to provide a reliable service you need
    GPUs, [**TPUs**](https://cloud.google.com/tpu/docs/intro-to-tpu), [**DPUs**](https://blogs.nvidia.com/blog/whats-a-dpu-data-processing-unit/),
    and carefully selected components to maximize efficiency, is incredibly expensive.
    On the other hand, its maintenance requires skilled human resources — qualified
    people to solve potential issues and perform system upgrades as needed.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个原因在于硬件。现代部署的模型需要同时处理大量来自多个用户的数据，不仅体积庞大，还需要大量的计算资源来执行推理任务并为客户提供高质量的服务。这在经济成本上同样表现为巨大的开销。一方面，搭建配备正确硬件的服务器和数据中心非常昂贵，考虑到为了提供可靠的服务，需要使用**GPU**、[**TPU**](https://cloud.google.com/tpu/docs/intro-to-tpu)、[**DPU**](https://blogs.nvidia.com/blog/whats-a-dpu-data-processing-unit/)以及精心挑选的组件以最大化效率。另一方面，其维护需要技术熟练的人员——合格的工作人员来解决潜在问题并根据需要进行系统升级。
- en: Objective
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标
- en: There are many other issues surrounding the construction of this kind of model
    and its large-scale deployment. Altogether, it is difficult to build a system
    with a supporting infrastructure robust enough to match leading services on the
    market like [**ChatGPT**](https://openai.com/chatgpt/). Still, we can achieve
    rather acceptable and reasonable approximations to the reference service due to
    the wide range of open-source content and technologies available in the public
    domain. Moreover, given the high degree of progress presented in some of them,
    they prove to be remarkably simple to use, allowing us to benefit from their abstraction,
    modularity, ease of integration, and other valuable qualities that enhance the
    development process.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建这种模型及其大规模部署的过程中，存在许多其他问题。总体而言，构建一个支持基础设施足够强大的系统，以匹敌市场上领先的服务，如[**ChatGPT**](https://openai.com/chatgpt/)，是非常困难的。然而，由于公开领域中有大量开源内容和技术，我们仍然能够实现与参考服务相当可接受和合理的近似。此外，考虑到其中一些技术的高进展度，它们被证明非常易于使用，使我们能够受益于它们的抽象性、模块化、易于集成等优点，这些特性有助于提升开发过程。
- en: Therefore, the purpose of this article is to show how we can design, implement,
    and deploy a computing system for supporting a ChatGPT-like service. Although
    the eventual result may not have the expected service capabilities, using high-quality
    dependencies and development tools, as well as a good architectural design, guarantees
    the system to be easily scalable up to the desired computing power according to
    the user's needs. That is, the system will be prepared to run on very few machines,
    possibly as few as one, with very limited resources, delivering a throughput consistent
    with those resources, or on larger computer networks with the appropriate hardware,
    offering an extended service.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，本文的目的是展示我们如何设计、实现和部署一个支持ChatGPT类似服务的计算系统。尽管最终的结果可能无法具备预期的服务能力，但通过使用高质量的依赖项和开发工具，并采用良好的架构设计，可以确保系统根据用户需求轻松扩展到所需的计算能力。也就是说，系统将准备在非常少的机器上运行，可能仅需一台，且资源非常有限，提供与这些资源一致的吞吐量，或者在更大的计算机网络上运行，配备适当的硬件，提供扩展服务。
- en: Architecture
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构
- en: Initially, the primary system functionality will be to allow a client to submit
    a text query, which is processed by an LLM model and then returned to the source
    client, all within a reasonable timeframe and offering a fair quality of service.
    This is the highest level description of our system, specifically, of the application
    functionality provided by this system, since all the implementation details like
    communication protocols between components, data structures involved, etc. are
    being intentionally omitted. But, now that we have a clear objective to reach,
    we can begin a decomposition that gradually increases the detail involved in solving
    the problem, often referred to as [**Functional Decomposition**](https://stackoverflow.com/questions/947874/what-is-functional-decomposition).
    Thus, starting from a black-box system [*(abstraction)*](https://en.wikipedia.org/wiki/Black_box)
    that receives and returns queries, we can begin to comprehensively define how
    a client interacts with the system, together with the technologies that will enable
    this interaction.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，系统的主要功能是允许客户端提交文本查询，查询由LLM模型处理并返回给源客户端，所有过程都在合理的时间范围内完成，并提供公平的服务质量。这是我们系统的最高级别描述，具体来说，是该系统提供的应用功能，因为所有实现细节，如组件之间的通信协议、涉及的数据结构等，都故意被省略了。但现在，我们已经有了明确的目标，可以开始进行逐步的分解，逐渐增加解决问题时涉及的细节，这通常被称为[**功能分解**](https://stackoverflow.com/questions/947874/what-is-functional-decomposition)。因此，从一个黑箱系统[*(抽象)*](https://en.wikipedia.org/wiki/Black_box)开始，它接收并返回查询，我们可以开始全面定义客户端如何与系统互动，以及将实现这种互动的技术。
- en: '![](../Images/c4a11ecb1f9470fd5e8601f5d500cba8.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4a11ecb1f9470fd5e8601f5d500cba8.png)'
- en: Image by author
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: At first, we must determine what constitutes a client, in particular, what tools
    or interfaces the user will require to interact with the system. As illustrated
    above, we assume that the system is currently a fully implemented and operational
    functional unit; allowing us to focus on clients and client-system connections.
    In the client instance, the interface will be available via a website, designed
    for versatility, but primarily aimed at desktop devices. A mobile app could also
    be developed and integrated to use the same system services and with a specific
    interface, but from an abstract point of view, it is desirable to unify all types
    of clients into one, namely the web client.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，我们必须确定什么构成客户端，特别是用户与系统交互所需的工具或接口。如上所示，我们假设系统目前是一个完全实现并投入运行的功能单元；这使我们能够专注于客户端和客户端-系统的连接。在客户端实例中，接口将通过一个网站提供，该网站设计上具有通用性，但主要面向桌面设备。同时，也可以开发并集成一个移动应用程序，使用相同的系统服务并具有特定的接口，但从抽象角度来看，理想的做法是将所有类型的客户端统一为一个，即
    Web 客户端。
- en: Subsequently, it is necessary to find a way to connect a client with the system
    so that an exchange of information, in this case, queries, can occur between them.
    At this point, it is worth being aware that the web client will rely on a specific
    technology such as JavaScript, with all the communication implications it entails.
    For other types of platforms, that technology will likely change, for example
    to Java in mobile clients or C/C++ in IoT devices, and compatibility requirements
    may demand the system to adapt accordingly.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，需要找到一种方法将客户端与系统连接，以便在它们之间进行信息交换，在这种情况下是查询。此时，值得注意的是，Web 客户端将依赖于特定的技术，如 JavaScript，这带来了所有相应的通信影响。对于其他类型的平台，技术可能会发生变化，例如移动客户端可能会使用
    Java，物联网设备可能会使用 C/C++，而兼容性要求可能要求系统相应地进行适配。
- en: '![](../Images/1ae51d8c94d610afad19d5df59daf004.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ae51d8c94d610afad19d5df59daf004.png)'
- en: Image by author
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: One way to establish communication would be to use [**Sockets**](https://www.geeksforgeeks.org/socket-in-computer-network/)and
    similar tools at a lower level, allowing exhaustive control of the whole protocol.
    However, this option would require meeting the compatibility constraints described
    above with all client technologies, as the system will need to be able to collect
    queries from all available client types. Additionally, having exhaustive control
    implies a lengthier and potentially far more complex development, since many extra
    details must be taken into account, which significantly increases the number of
    lines of code and complicates both its maintainability and extensibility.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一种建立通信的方式是使用[**套接字**](https://www.geeksforgeeks.org/socket-in-computer-network/)及类似工具，这样可以在更低的层级上提供对整个协议的完全控制。然而，这种选择需要满足上述所述的所有客户端技术的兼容性要求，因为系统需要能够从所有可用的客户端类型收集查询。此外，拥有完全控制意味着开发过程会更长且可能更加复杂，因为必须考虑许多额外的细节，这会显著增加代码行数，并使其可维护性和可扩展性变得更加复杂。
- en: As you can see above, the most optimal alternative is to build an [**Application
    Programming Interface (API)**](https://www.ibm.com/topics/api) that intermediates
    between the clients and the system part in charge of the computing, i.e. the one
    that solves queries. The main advantage of using an API is that all the internal
    connection handling, such as opening and closing sockets, thread pooling, and
    other important details *(data serialization)*, is performed by the framework
    from which the API is built. In this way, we ensure that the client will only
    have to send its query to the server where the API is executed and wait for its
    response, all of this relying on dependencies that simplify the management of
    these API requests. Another benefit derived from the previous point is the ease
    of service extension by modifying the API [**endpoints**](https://www.baeldung.com/cs/api-endpoints).
    For example, if we want to add a new model to the system or any other functionality,
    it is enough to add and implement a new endpoint, without having to change the
    communication protocol itself or the way a client interacts with the system.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，最优的替代方案是构建一个[**应用程序编程接口（API）**](https://www.ibm.com/topics/api)，它作为客户端和负责计算的系统部分之间的中介，即解决查询的部分。使用API的主要优点是，所有内部连接处理，例如打开和关闭套接字、线程池管理以及其他重要细节（*数据序列化*），都由构建API的框架来执行。通过这种方式，我们确保客户端只需将查询发送到执行API的服务器，并等待响应，所有这些都依赖于简化API请求管理的依赖项。另一个来自前一点的好处是，通过修改API的[**端点**](https://www.baeldung.com/cs/api-endpoints)，可以轻松扩展服务。例如，如果我们想为系统添加一个新模型或其他功能，只需添加并实现一个新端点，而无需更改通信协议本身或客户端与系统的交互方式。
- en: Compute Service
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算服务
- en: Once we set up a mechanism for clients to communicate elegantly with the system,
    we must address the problem of how to process incoming queries and return them
    to their corresponding clients in a reasonable amount of time. But first, it is
    relevant to point out that when a query arrives at the system, it must be redirected
    to a machine with an LLM loaded in memory with its respective inference pipeline
    and traverse the query through that pipeline, obtaining the result text *(LLM
    answer)* that will be later returned. Consequently, the inference process cannot
    be distributed among several machines for a query resolution. With that in mind,
    we can begin the design of the infrastructure that will support the inference
    process.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们设置了一个优雅的机制让客户端与系统进行通信，我们必须解决如何处理传入查询并在合理的时间内将其返回给相应客户端的问题。但首先，需要指出的是，当查询到达系统时，它必须被重定向到一台机器，该机器内存中加载了LLM（大语言模型）及其相应的推理管道，并通过该管道处理查询，获取结果文本（*LLM回答*），然后将其返回。因此，推理过程无法分布在多台机器上进行查询解决。考虑到这一点，我们可以开始设计支撑推理过程的基础设施。
- en: In the previous image, the compute service was represented as a single unit.
    If we consider it as a machine connected, this time, through a single channel
    using Sockets with the API server, we will be able to redirect all the API queries
    to that machine, concentrating all the system load in a single place. As you can
    imagine, this would be a good choice for a home system that only a few people
    will use. However, in this case, we need a way to make this approach scalable,
    so that with an increase in computing resources we can serve as many additional
    users as possible. But first, we must segment the previously mentioned computational
    resources into units. In this way, we will have a global vision of their interconnection
    and will be able to optimize our project throughput by changing their structure
    or how they are composed.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图像中，计算服务被表示为一个单一的单元。如果我们将其视为通过一个单一通道连接的机器，并使用套接字与API服务器进行通信，我们将能够将所有API查询重定向到该机器，将所有系统负载集中在一个地方。如你所想，这将是一个适合仅供少数人使用的家庭系统的不错选择。然而，在这种情况下，我们需要一种方法来使这种方法具备可扩展性，以便在增加计算资源的情况下，我们可以为更多的用户提供服务。但首先，我们必须将前述的计算资源细分为多个单元。通过这种方式，我们将能够全面了解它们的相互连接，并能够通过改变它们的结构或组成方式来优化我们的项目吞吐量。
- en: A computational unit, which from now on we will call node for the convenience
    of its implementation, will be integrated by a physical machine that receives
    requests *(not all of them)* needing to be solved. Additionally, we can consider
    a node as virtualization of a *(possibly reduced)* amount of machines, with the
    purpose of increasing the total throughput per node by introducing parallelism
    locally. Regarding the hardware employed, it will depend to a large extent on
    how the service is oriented and how far we want to go. Nevertheless, for the version
    presented in this case, we will assume a standard CPU, a generous amount of RAM
    to avoid problems when loading the model or forwarding queries, and dedicated
    processors as GPUs, with the possibility of including TPUs in some specific cases.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一个计算单元，为了方便实现，我们将其称为节点，将由一台物理机器集成，该机器接收需要解决的请求（*并非所有请求*）。此外，我们可以将节点视为一组（*可能较少*）机器的虚拟化，目的是通过在本地引入并行性来增加每个节点的总吞吐量。关于使用的硬件，这将很大程度上取决于服务的方向以及我们希望达到的程度。然而，对于本案例中呈现的版本，我们将假设使用标准的CPU、大量的RAM以避免加载模型或转发查询时出现问题，以及专用的处理器，如GPU，并在某些特定情况下可能包括TPU。
- en: '![](../Images/438dae7ba400296bda5f07129e1e5d9b.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/438dae7ba400296bda5f07129e1e5d9b.png)'
- en: Image by author
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Now, we can establish a network that links multiple nodes in such a way that
    via one of them, connected to the API server, queries can be distributed throughout
    the network, leveraging optimally all the system’s resources. Above, we can notice
    how all the nodes are structurally connected in a tree-like shape, with its root
    being responsible for collecting API queries and forwarding them accordingly.
    The decision of how they should be interconnected depends considerably on the
    exact system’s purpose. In this case, a tree is chosen for simplicity of the distribution
    primitives. For example, if we wanted to maximize the number of queries transmitted
    between the API and nodes, there would have to be several connections from the
    API to the root of several trees, or another distinct data structure if desired.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以建立一个网络，将多个节点连接起来，通过其中一个节点（连接到API服务器）可以将查询分发到整个网络中，充分利用系统的所有资源。上面，我们可以看到所有节点在结构上以树状形态连接，其根节点负责收集API查询并按需转发。如何将节点互联的决策在很大程度上取决于系统的具体目的。在本例中，选择了树结构，因其分配原语简单。例如，如果我们希望最大化API与节点之间传输的查询数量，则必须从API到多个树的根节点建立多个连接，或者如果需要，选择其他不同的数据结构。
- en: Lastly, we need to define how a query is forwarded and processed when it reaches
    the root node. As before, there are many available and equally valid alternatives.
    However, the algorithm we will follow will also serve to understand why a tree
    structure is chosen to connect the system nodes.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要定义当查询到达根节点时，查询是如何被转发和处理的。如前所述，有许多可用的且同样有效的替代方案。然而，我们将遵循的算法也将有助于理解为什么选择树状结构来连接系统节点。
- en: '![](../Images/56ace988d96c7a69a2176147578c375b.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56ace988d96c7a69a2176147578c375b.png)'
- en: Image by author
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Since a query must be solved on a single node, the goal of the distribution
    algorithm will be to find an idle node in the system and assign it the input query
    for its resolution. As can be seen above, if we consider an ordered sequence of
    queries numbered in natural order *(1 indexed)*, each number corresponds to the
    edge connected with the node assigned to solve that query. To understand the numbering
    in this concrete example, we can assume that the queries arriving at a node take
    an infinite time to be solved, therefore ensuring that each node is progressively
    busy facilitates the understanding of the algorithm heuristic.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 由于查询必须在单个节点上解决，因此分配算法的目标是找到一个空闲节点，并将输入查询分配给该节点以进行解决。如上所示，如果我们考虑按自然顺序（*从1开始索引*）编号的有序查询序列，每个编号对应于与被分配去解决该查询的节点相连的边。为了理解这个具体例子中的编号，我们可以假设到达节点的查询需要无限长的时间来解决，因此确保每个节点逐步变得忙碌有助于理解该算法的启发式方法。
- en: In short, we will let the root not to perform any resolution processing, reserving
    all its capacity for the forwarding of requests with the API. For any other node,
    when it receives a query from a hierarchically superior node, the first step is
    to check if it is performing any computation for a previous query; if it is idle,
    it will resolve the query, and in the other case it will forward it by [**Round
    Robin**](https://en.wikipedia.org/wiki/Round-robin_scheduling) to one of its descendant
    nodes. With Round Robin, each query is redirected to a different descendant for
    each query, traversing the entire descendant list as if it were a circular buffer.
    This implies that the local load of a node can be evenly distributed downwards,
    while efficiently leveraging the resources of each node and our ability to scale
    the system by adding more descendants.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们将让根节点不执行任何解析处理，保留其所有能力用于转发与 API 的请求。对于任何其他节点，当它接收到来自层级更高节点的查询时，第一步是检查它是否正在处理之前查询的计算；如果它处于空闲状态，它将解析该查询，否则，它将通过[**轮询调度**](https://en.wikipedia.org/wiki/Round-robin_scheduling)将查询转发给其一个子节点。通过轮询调度，每次查询都会被重定向到不同的子节点，遍历整个子节点列表，仿佛它是一个循环缓冲区。这意味着一个节点的本地负载可以均匀分配到下游，同时高效利用每个节点的资源，并且通过添加更多子节点来实现系统的扩展。
- en: Finally, if the system is currently serving many users, and a query arrives
    at a leaf node that is also busy, it will not have any descendants for redirecting
    it to. Therefore, all nodes will have a query queuing mechanism in which they
    will wait in these situations, being able to apply batch operations between queued
    queries to accelerate LLM inference. Additionally, when a query is completed,
    to avoid overloading the system by forwarding it upwards until it arrives at the
    tree top, it is sent directly to the root, subsequently reaching the API and client.
    We could connect all nodes to the API, or implement other alternatives, however,
    to keep the code as simple and the system as performant as possible, they will
    all be sent to the root.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果系统当前服务于许多用户，并且一个查询到达一个也忙碌的叶节点时，它将没有任何子节点可以将其重定向。因此，所有节点都将有一个查询排队机制，在这种情况下它们会等待，并能够在排队的查询之间应用批处理操作以加速
    LLM 推理。此外，当一个查询完成后，为了避免通过向上传递查询直到到达树顶而导致系统过载，它会直接发送到根节点，随后到达 API 和客户端。我们可以将所有节点连接到
    API，或实现其他替代方案，但为了保持代码尽可能简单，且系统高效，它们都会发送到根节点。
- en: Web Client
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Web 客户端
- en: After having defined the complete system architecture and how it will perform
    its task, we can begin to build the web client that users will need when interacting
    with our solution.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义了完整的系统架构和如何执行任务之后，我们可以开始构建用户在与我们的解决方案交互时需要的 Web 客户端。
- en: As expected, the web client is implemented in basic HTML, CSS and JavaScript,
    everything embedded in a single .html file for convenience. This file will be
    provided by the API each time the client makes a request corresponding to the
    application startup, that is, when the client enters the browser and inputs the
    address where the API has hosted the entry point, it will return the .html file
    to be rendered in the browser.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，Web 客户端是用基本的 HTML、CSS 和 JavaScript 实现的，所有内容都嵌入在一个单独的 .html 文件中，以便于使用。每次客户端发起与应用启动相关的请求时，API
    将提供此文件，也就是说，当客户端进入浏览器并输入 API 托管入口点的地址时，它将返回 .html 文件以便在浏览器中渲染。
- en: Subsequently, when the user wishes to send a text query to the system, JavaScript
    internally submits an HTTP request to the API with the corresponding details such
    as the data type, endpoint, or [**CSRF**](https://en.wikipedia.org/wiki/Cross-site_request_forgery)
    security token. By using [**AJAX**](https://www.w3schools.com/js/js_ajax_intro.asp)
    within this process, it becomes very simple to define a primitive that executes
    when the API returns some value to the request made, in charge of displaying the
    result on the screen. Additionally, it is worth mentioning that the messages sent
    are not directly the written or returned text, but are wrapped in a [**JSON**](https://en.wikipedia.org/wiki/JSONç)with
    other important parameters, like the timestamp, offering the possibility to add
    extra fields on the fly to manage the synchronization of some system components.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，当用户希望向系统发送文本查询时，JavaScript会在内部向API提交一个HTTP请求，包含相应的细节，例如数据类型、端点或[**CSRF**](https://en.wikipedia.org/wiki/Cross-site_request_forgery)安全令牌。通过在此过程中使用[**AJAX**](https://www.w3schools.com/js/js_ajax_intro.asp)，可以非常简单地定义一个原语，该原语在API返回请求的值时执行，负责将结果显示在屏幕上。此外，值得一提的是，发送的消息并非直接是书面或返回的文本，而是被封装在一个[**JSON**](https://en.wikipedia.org/wiki/JSON)中，包含其他重要参数，如时间戳，提供了在运行时添加额外字段的可能性，以管理某些系统组件的同步。
- en: Django API
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Django API
- en: When the web client is ready, we can proceed to implement the API which will
    provide the necessary service.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当网页客户端准备就绪时，我们可以开始实现提供必要服务的API。
- en: There are many technologies available to build an API, but in this project we
    will specifically use [**Django**](https://developer.mozilla.org/en-US/docs/Learn/Server-side/Django/Introduction)through
    Python on a dedicated server. This decision is motivated by the high scalability
    and ease of integration with other Python dependencies offered by this framework,
    in addition to other useful properties such as security or the default administration
    panel.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多可用的技术来构建API，但在本项目中，我们将专门使用[**Django**](https://developer.mozilla.org/en-US/docs/Learn/Server-side/Django/Introduction)通过Python在专用服务器上实现。做出这个决定的原因是该框架提供了较高的可扩展性和与其他Python依赖项的集成便利性，除此之外，还具有安全性和默认管理面板等有用的属性。
- en: '![](../Images/622d9f5b9d5667ffb824103b9612709a.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/622d9f5b9d5667ffb824103b9612709a.png)'
- en: Image by author
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: One of the endpoints to configure is the entry point for the web client, represented
    by the default URL slash /. Thus, when a user accesses the server through a default
    HTTP request like the one shown above, the API will return the HTML code required
    to display the interface and start making requests to the LLM service.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 需要配置的端点之一是网页客户端的入口点，表示为默认的URL斜杠/。因此，当用户通过上述示例中的默认HTTP请求访问服务器时，API将返回显示界面所需的HTML代码，并开始向LLM服务发出请求。
- en: '![](../Images/58ad783edf8b7866cec09b9f4a188446.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/58ad783edf8b7866cec09b9f4a188446.png)'
- en: Image by author
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: At the same time, it will have to support the client’s requests once it has
    accessed the interface. These, as they have to be managed in a special way, will
    have their own endpoint called “/arranca” to where the query data will be sent
    in the corresponding JSON format, and the API returns the solved query after processing
    it with the node tree. In this endpoint, the server uses a previously established
    Socket channel with the root node in the hierarchy to forward the query, waiting
    for its response through a synchronization mechanism.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，一旦访问了接口，它还必须支持客户端的请求。这些请求由于需要特殊管理，将有一个名为“/arranca”的端点，查询数据将以相应的JSON格式发送到该端点，API在处理完查询并通过节点树解决后，返回已解决的查询。在此端点，服务器通过一个预先建立的Socket通道与层级结构中的根节点转发查询，并通过同步机制等待其响应。
- en: Concerning the code, in the **urls.py** file we will store the associations
    between URLs and endpoints, so that the default empty URL is assigned to its corresponding
    function that reads the .html from the templates folder and sends it back, or
    the URL /arranca that executes the function which solves a query. In addition,
    a views function will be executed to launch the main server thread. Meanwhile,
    in **settings.py**, the only thing to change is the DEBUG parameter to False and
    enter the necessary permissions of the hosts allowed to connect to the server.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 关于代码，在**urls.py**文件中，我们将存储URL和端点之间的关联，以便将默认的空URL分配给其对应的函数，该函数读取模板文件夹中的.html文件并将其返回，或者将URL
    /arranca分配给执行查询解决函数的端点。此外，还将执行一个视图函数来启动主服务器线程。同时，在**settings.py**中，唯一需要修改的就是将DEBUG参数设置为False，并输入允许连接到服务器的主机所需的权限。
- en: Finally, there is the **views.py** script, where all the API functionality is
    implemented. First, we have a main thread in charge of receiving and handling
    incoming connections (from the root node). Initially, this connection will be
    permanent for the whole system's lifetime. However, it is placed inside an infinite
    loop in case it is interrupted and has to be reestablished. Secondly, the default
    endpoint is implemented with the **index()** function, which returns the .html
    content to the client if it performs a GET request. Additionally, the queries
    the user submits in the application are transferred to the API through the ***/arranca***
    endpoint, implemented in the function with the same name. There, the input query
    is forwarded to the root node, blocking until a response is received from it and
    returned to the client.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最后是**views.py**脚本，在其中实现了所有的API功能。首先，我们有一个主线程负责接收和处理来自根节点的传入连接。最初，这个连接将在整个系统生命周期内保持常驻。然而，它被放置在一个无限循环中，以防连接被中断并需要重新建立。其次，默认的端点通过**index()**函数实现，如果客户端发起GET请求，它将返回.html内容。此外，用户在应用程序中提交的查询会通过***/arranca***端点传递给API，该端点由同名函数实现。在这里，输入的查询被转发到根节点，直到从根节点收到响应并返回给客户端时，才会解除阻塞。
- en: This blocking is achieved through [**locks**](https://docs.python.org/3/library/threading.html#condition-objects)
    and a synchronization mechanism where each query has a unique identifier, inserted
    by the ***arranca()*** function as a field in the JSON message, named **request_id**.
    Essentially, it is a natural number that corresponds to the query arrival order.
    Therefore, when the root node sends a solved query to the API, it is possible
    to know which of its blocked executions was the one that generated the query,
    unblocking, returning, and re-blocking the rest.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这种阻塞是通过[**锁**](https://docs.python.org/3/library/threading.html#condition-objects)和同步机制实现的，每个查询都有一个唯一标识符，该标识符由***arranca()***函数作为字段插入到JSON消息中，命名为**request_id**。本质上，这是一个自然数，对应于查询到达的顺序。因此，当根节点将已解决的查询发送到API时，可以知道是哪个被阻塞的执行生成了该查询，从而解除阻塞，返回并重新阻塞其余查询。
- en: Java Compute Nodes
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Java计算节点
- en: With the API operational, we will proceed to implement the node system in Java.
    The main reason for choosing this language is motivated by the technology that
    enables us to communicate between nodes. To obtain the simplest possible communication
    semantics at this level, we will discard the use of sockets and manually serialized
    messages and replace them with [**RMI**](https://www.javatpoint.com/RMI), which
    in other platforms would be somewhat more complicated, although they also offer
    solutions such as [**Pyro4**](https://pyro4.readthedocs.io/en/stable/) in Python.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在API运行后，我们将继续在Java中实现节点系统。选择这种语言的主要原因是它提供的技术使我们能够在节点之间进行通信。为了在这一层面上获得尽可能简单的通信语义，我们将放弃使用套接字和手动序列化消息，并用[**RMI**](https://www.javatpoint.com/RMI)替代，尽管在其他平台上这可能会稍显复杂，尽管它们也提供了像[**Pyro4**](https://pyro4.readthedocs.io/en/stable/)这样的Python解决方案。
- en: '[**Remote Method Invocation (RMI)**](https://en.wikipedia.org/wiki/Java_remote_method_invocation)
    is a communication paradigm that enables the creation of distributed systems composed
    of remote objects hosted on separate machines, with the ability to obtain remote
    references to each other and to invoke remote methods within their service interface.
    Hence, due to the high degree of abstraction in Java, the query transfer between
    nodes will be implemented with a remote call to an object referenced by the sender
    node, leaving the complex process of API connection to be handled manually, as
    it was previously done in Python.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[**远程方法调用 (RMI)**](https://en.wikipedia.org/wiki/Java_remote_method_invocation)是一种通信范式，使得由托管在不同机器上的远程对象组成的分布式系统的创建成为可能，能够相互获取远程引用并在它们的服务接口中调用远程方法。因此，由于Java中的高度抽象，节点之间的查询传输将通过对发送节点引用的远程对象的调用来实现，复杂的API连接过程将由手动处理，如同之前在Python中做的一样。'
- en: At the outset, we should define the remote interface that determines the remote
    invocable methods for each node. On the one hand, we have methods that return
    relevant information for debugging purposes ***(log() or getIP())***. On the other
    hand, there are those in charge of obtaining remote references to other nodes
    and registering them into the local hierarchy as an ascending or descending node,
    using a name that we will assume unique for each node. Additionally, it has two
    other primitives intended to receive an incoming query from another node ***(receiveMessage())***
    and to send a solved query to the API ***(sendMessagePython())***, only executed
    in the root node.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，我们应定义远程接口，它决定了每个节点可调用的远程方法。一方面，我们有返回调试信息相关方法***(log() 或 getIP())***。另一方面，有一些方法负责获取对其他节点的远程引用，并将其注册到本地层次结构中，作为一个升序或降序节点，使用一个我们假设对每个节点都是唯一的名称。此外，它还有另外两个原语，用于接收来自其他节点的传入查询***(receiveMessage())***和向API发送已解决的查询***(sendMessagePython())***，这些方法仅在根节点中执行。
- en: From the interface, we can implement its operations inside the node class, instantiated
    every time we start up the system and decide to add a new machine to the node
    tree. Among the major features included in the node class is the **getRemoteNode()**
    method, which obtains a remote reference to another node from its name. For this
    purpose, it accesses the name registry and executes the lookup() primitive, returning
    the remote reference in the form of an interface, if it is registered, or null
    otherwise.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在接口中，我们可以在节点类中实现其操作，每当我们启动系统并决定向节点树中添加新机器时，就会实例化该类。节点类中包含的主要功能之一是**getRemoteNode()**方法，它通过节点名称获取对另一个节点的远程引用。为此，它访问名称注册表并执行`lookup()`原语，如果该节点已注册，则返回以接口形式呈现的远程引用，否则返回null。
- en: Obtaining remote references is essential in the construction of the tree, in
    particular for other methods that connect a parent node to a descendant or obtain
    a reference to the root to send solved queries. One of them is connectParent(),
    invoked when a descendant node needs to connect with a parent node. As you can
    see, it first uses getRemoteNode() to retrieve the parent node, and once it has
    the reference, assigns it to a local variable for each node instance. Afterwards
    it calls on the **connectChild()**, which appends to the descendant list the remote
    node from which it was invoked. In case the parent node does not exist, it will
    try to call a function on a null object, raising an exception. Next, it should
    be noted that the methods to receive queries from the API **receiveMessagePython()**
    and from other nodes **receiveMessage()** are protected with the synchronized
    clause to avoid race conditions that may interfere with the system’s correct operation.
    These methods are also responsible for implementing the query distribution heuristic,
    which uses a local variable to determine the corresponding node to which an incoming
    query should be sent.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 获取远程引用在树的构建中至关重要，特别是对于其他方法，这些方法将父节点连接到后代节点或获取根节点的引用以发送已解决的查询。其一是`connectParent()`，当一个后代节点需要与父节点连接时会调用它。如你所见，它首先使用`getRemoteNode()`来获取父节点，一旦获得引用，就将其分配给每个节点实例的本地变量。然后它会调用**connectChild()**，该方法将从中调用的远程节点附加到后代节点列表中。如果父节点不存在，它会尝试在一个空对象上调用函数，从而抛出异常。接下来需要注意的是，用于接收来自API的查询**receiveMessagePython()**和来自其他节点的查询**receiveMessage()**的方法都通过`synchronized`关键字进行保护，以避免可能干扰系统正确运行的竞争条件。这些方法还负责实现查询分发启发式算法，使用本地变量来确定应该将传入的查询发送到哪个对应的节点。
- en: At last, the node class has a thread pool used to manage the query resolution
    within the **consultLLM()** method. In this way, its calls will be immediately
    terminated within the Java code, since the pool will assign a thread to the execution
    of the required computation and will return the control to the program so it can
    accept additional queries. This is also an advantage when detecting whether a
    node is performing any computation or not, since it is enough to check if the
    number of active threads is greater than 0\. On the other hand, the other use
    of threads in the node class, this time outside the pool, is in the **connectServer()**
    method in charge of connecting the root node with the API for query exchange.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，节点类有一个线程池，用于管理**consultLLM()**方法中的查询解析。通过这种方式，它的调用将在 Java 代码中立即结束，因为线程池将为所需的计算分配一个线程，并将控制权返回给程序，从而可以接受更多查询。这对于检测节点是否正在进行任何计算也有优势，因为只需要检查活动线程的数量是否大于
    0。另一方面，节点类中线程的另一个使用，位于线程池之外，是在**connectServer()**方法中，用于将根节点与用于查询交换的 API 连接。
- en: In the **Utilities** class, we only have the method to create an LDAP usage
    context, with which we can register and look up remote references to nodes from
    their names. This method could be placed in the node class directly, but in case
    we need more methods like this, we leave it in the Utilities class to take advantage
    of the design pattern.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在**Utilities**类中，我们只提供了创建 LDAP 使用上下文的方法，利用该方法可以根据节点的名称注册和查找远程引用。这个方法本可以直接放在节点类中，但如果我们需要更多类似的方法，出于设计模式的考虑，我们将其保留在
    Utilities 类中。
- en: 'The creation of node instances, as well as their management, which is performed
    manually for each of them, is implemented in the Launcher class. It uses a command
    line interface for instructing the respective node, which is created at startup
    with a specific name registered in the designated LDAP server. Some of the commands
    are:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 节点实例的创建以及每个节点的手动管理是在 Launcher 类中实现的。它使用命令行接口来指示相应的节点，节点在启动时创建，并在指定的 LDAP 服务器上注册特定名称。一些命令包括：
- en: '**log:** Prints useful information to know the node status.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**日志：** 打印有用的信息以了解节点的状态。'
- en: '**parent:** Connects the node to a specified parent from its name.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**父节点：** 将节点连接到指定的父节点，依据其名称进行连接。'
- en: '**registry:** Lists all nodes currently registered in the LDAP directory under
    the organizational unit **ou=Nodes**. This could be useful for monitoring the
    registry server or creating new nodes.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**注册表：** 列出了当前在 LDAP 目录下的所有节点，这些节点位于组织单位**ou=Nodes**下。这对于监控注册表服务器或创建新节点可能很有用。'
- en: '**server:** Connects the node to a server specified by its address and port
    number. Primarily, the server will be the Python API, although it could also serve
    other functionalities.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务器：** 将节点连接到通过其地址和端口号指定的服务器。主要情况下，服务器将是 Python API，但它也可以提供其他功能。'
- en: LDAP Server
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LDAP 服务器
- en: Since nodes are remote objects, they must have access to a registry that enables
    them to obtain remote references to other nodes from their name. The solution
    provided by Java is to use **rmiregistry** to initialize a registry service on
    a machine. However, when protected operations such as rebind() are executed from
    another host, it throws a security exception, preventing a new node from registering
    on a machine other than the one containing the registry. For this reason, and
    in addition to its simplicity, this project will use an Apache server as a registry
    using the [**Lightweight Directory Access Protocol (LDAP)**](https://learn.microsoft.com/en-us/previous-versions/windows/desktop/ldap/lightweight-directory-access-protocol-ldap-api).
    This protocol allows to manage the storage of *Name->Remote_Node* pairs in a directory
    system, with other additional functionalities that significantly improve the registry
    service with respect to the one offered by the Java registry.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 由于节点是远程对象，它们必须能够访问注册表，以便从其名称获取其他节点的远程引用。Java 提供的解决方案是使用**rmiregistry**在一台机器上初始化注册表服务。然而，当从其他主机执行如
    rebind() 等受保护操作时，会抛出安全异常，阻止新节点在除注册表所在机器以外的其他机器上注册。因此，除了其简便性外，本项目将使用 Apache 服务器作为注册表，采用[**轻量级目录访问协议（LDAP）**](https://learn.microsoft.com/en-us/previous-versions/windows/desktop/ldap/lightweight-directory-access-protocol-ldap-api)。该协议允许在目录系统中管理*名称->远程节点*对，并具有其他附加功能，显著提升了与
    Java 注册表提供的服务相比的注册表服务。
- en: The advantages of using LDAP begin with its complexity of operation, which at
    first glance might seem the opposite, but in reality, is what enables the system
    to be adapted to various security and configuration needs at a much higher level
    of detail. On the one hand, the authentication and security features it offers
    allow any host to perform a protected operation such as registering a new node,
    as long as the host is identified by the LDAP server. For example, when a context
    object is created to access the server and be able to perform operations, there
    is the option of adding parameters to the HashMap of its constructor with authentication
    data. If the context is created, it means that the data matches what the server
    expects, otherwise, it could be assumed that the connection is being made by an
    unauthenticated *(“malicious”)* host, ensuring that only system nodes can manipulate
    the server information. On the other hand, LDAP allows for much more efficient
    centralization of node registration, and much more advanced interoperability,
    as well as easy integration of additional services like [**Kerberos**](https://www.ibm.com/docs/en/aix/7.3?topic=network-kerberos).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LDAP的优势始于其操作的复杂性，乍一看这可能显得相反，但实际上，这正是使系统能够在更高的细节级别上适应各种安全性和配置需求的关键。一方面，它所提供的认证和安全功能允许任何主机执行受保护的操作，如注册新节点，只要该主机已通过LDAP服务器验证。例如，当创建一个上下文对象以访问服务器并能够执行操作时，可以选择向其构造函数的HashMap中添加包含认证数据的参数。如果上下文已创建，则意味着数据与服务器期望的匹配，否则可以认为连接是由未经认证的*(“恶意”)*主机发起的，从而确保只有系统节点才能操作服务器信息。另一方面，LDAP允许更高效的节点注册集中化，提供更高级的互操作性，并且可以轻松集成像[**Kerberos**](https://www.ibm.com/docs/en/aix/7.3?topic=network-kerberos)这样的额外服务。
- en: 'To ensure a server can operate as a node registry, we have to apply a specific
    configuration to it. First, since the project will not be deployed in an environment
    with real (and potentially malicious) users, all authentication options are omitted
    to keep things simple and clean. Next, a [**Distinguished Name**](https://www.ibm.com/docs/en/i/7.5?topic=eim-distinguished-name)
    must be defined so that a node name can be associated with its corresponding remote
    object. In this case, assuming that we prevent the registration of several nodes
    with the same name, we simply have to store the node name in an attribute such
    as cn= (Common Name) within a given organizational unit, ou=Nodes. Therefore,
    the distinguished name will be of the form: **cn=Node_Name,ou=Nodes**'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为确保服务器能够作为节点注册中心运行，我们必须对其应用特定配置。首先，由于该项目不会部署在有真实（且可能是恶意）用户的环境中，因此所有认证选项都被省略，以保持简单和干净。接下来，必须定义一个[**区分名称**](https://www.ibm.com/docs/en/i/7.5?topic=eim-distinguished-name)，以便将节点名称与其对应的远程对象关联。在这种情况下，假设我们防止注册多个具有相同名称的节点，我们只需将节点名称存储在某个属性中，如cn=（常用名称），并放置在指定的组织单元ou=Nodes中。因此，区分名称将是：**cn=Node_Name,ou=Nodes**
- en: '![](../Images/bb0053eddbdfba4f5fd216e06c3bdb0f.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bb0053eddbdfba4f5fd216e06c3bdb0f.png)'
- en: Image by author
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: Whenever a new node is created, it is registered in the LDAP server using its
    distinguished name and Node instance as a new entry in the form of a directory.
    Likewise, deleting a node or getting its remote reference from the registry requires
    using the distinguished name as well. Performing these operations on the registry
    implies having an open connection to the LDAP server. But, since the nodes are
    made with Java, we can use services that allow us to abstract the whole connection
    process and focus only on invoking the operations. The service to be used by nodes
    will be a directory context, commonly defined by the [**DirContext**](https://docs.oracle.com/javase/8/docs/api/javax/naming/directory/DirContext.html)interface.
    Thus, the process of accessing the server and performing some management is as
    simple as creating an object that implements the DirContext interface, in this
    case, InitialDirContext, assigning it the appropriate parameters to identify the
    server, including a URL of the form **ldap://IP:port/**, an identification of
    the protocol to be used, and even authentication parameters, which in this project
    will not be used.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 每当创建一个新节点时，它会使用其区分名和节点实例，在LDAP服务器中作为目录形式的新条目进行注册。同样，删除节点或从注册表中获取其远程引用也需要使用区分名。对注册表执行这些操作意味着必须保持与LDAP服务器的连接。但是，由于节点是用Java编写的，我们可以使用一些服务来抽象整个连接过程，专注于调用操作。节点将使用的服务是一个目录上下文，通常由[**DirContext**](https://docs.oracle.com/javase/8/docs/api/javax/naming/directory/DirContext.html)接口定义。因此，访问服务器并执行一些管理操作的过程就像创建一个实现了DirContext接口的对象一样简单，在这个例子中是InitialDirContext，并为其分配适当的参数以识别服务器，其中包括一个**ldap://IP:port/**形式的URL、要使用的协议标识符，甚至是认证参数，在本项目中这些认证参数不会被使用。
- en: Lookup, Bind, and Unbind
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查找、绑定和解绑
- en: For simplicity, Launcher will have its own context object, while each node will
    also have its own one. This allows Launcher to create entries and perform deletions,
    while each node will be able to perform lookup operations to obtain remote references
    from node names. Deletion operations are the simplest since they only require
    the distinguished name of the server entry corresponding to the node to be deleted.
    If it exists, it is deleted and the call to **unbind()** ends successfully, otherwise,
    it throws an exception. On the other hand, the lookup and register operations
    require following [**RFC-2713**](https://www.rfc-editor.org/rfc/rfc2713.html).
    In the case of appending a node to the server, the **bind()** primitive is used,
    whose arguments are the distinguished name of the entry in which that node will
    be hosted, and its remote object. However, the bind function is not given the
    node object as is, nor its interface, since the object is not serializable and
    bind() cannot obtain an interface *“instance”* directly. As a workaround, the
    above RFC forces the node instance to be masked by a [**MarshalledObjec**](https://docs.oracle.com/javase/8/docs/api/java/rmi/MarshalledObject.html)t.
    Consequently, bind will receive a MarshalledObject composed of the node being
    registered within the server, instead of the original node instance.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，Launcher将拥有自己的上下文对象，而每个节点也将拥有自己的上下文对象。这使得Launcher可以创建条目并执行删除操作，而每个节点则能够执行查找操作，从节点名称中获取远程引用。删除操作是最简单的，因为它只需要对应于要删除的节点的服务器条目的区分名。如果存在，它将被删除，并且对**unbind()**的调用将成功结束，否则它会抛出一个异常。另一方面，查找和注册操作需要遵循[**RFC-2713**](https://www.rfc-editor.org/rfc/rfc2713.html)。在将节点添加到服务器的情况下，将使用**bind()**原语，其参数是该节点将托管的条目的区分名，以及其远程对象。然而，bind函数不会直接传递节点对象或其接口，因为对象不可序列化，而bind()不能直接获取接口*“实例”*。作为解决方法，上述RFC强制将节点实例通过[**MarshalledObject**](https://docs.oracle.com/javase/8/docs/api/java/rmi/MarshalledObject.html)进行封装。因此，bind将接收一个由正在注册到服务器的节点组成的MarshalledObject，而不是原始节点实例。
- en: Finally, the lookup operation is performed from the **lookup()** primitive over
    a context. If the name and node have not been previously registered or an unexpected
    error occurs in the process, an exception is thrown. Conversely, if the operation
    succeeds, it returns the MarshalledObject associated with the distinguished name
    of the query. However, the remote reference returned by lookup() is contained
    in the MarshalledObject wrapper with which it was stored in the registry. Therefore,
    the **get()** operation of the MarshalledObject must be used to obtain the usable
    remote reference. Additionally, with this functionality it is possible to prevent
    the registration of a node with the same name as another already registered, as
    before executing bind() it would be checked with a lookup() if there is any exception
    related to the existence of the distinguished name.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，查找操作通过**lookup()**原语在上下文中执行。如果名称和节点没有被预先注册，或者在过程中发生了意外错误，将抛出异常。相反，如果操作成功，它将返回与查询的独特名称关联的MarshalledObject。但是，lookup()返回的远程引用包含在存储在注册表中的MarshalledObject包装器内。因此，必须使用MarshalledObject的**get()**操作来获取可用的远程引用。此外，借助此功能，可以防止注册与已注册节点同名的节点，因为在执行bind()之前会通过lookup()检查是否存在与该独特名称相关的异常。
- en: LLM Inference
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM 推理
- en: Concerning the inference process at each node, the node tree has an LLMProcess
    class in charge of instantiating a process implemented in Python where the queries
    will be transferred before they are returned solved, since in Python we can easily
    manage the LLM and its inference pipeline.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 关于每个节点的推理过程，节点树中有一个LLMProcess类，负责实例化一个用Python实现的进程，查询将在被解决之前传递到该进程，因为在Python中我们可以轻松管理LLM及其推理管道。
- en: When a new LLMProcess is instantiated, it is necessary to find an available
    port on the machine to communicate the Java and Python processes. For simplicity,
    this data exchange will be accomplished with Sockets, so after finding an available
    port by opening and closing a ServerSocket, the **llm.py** process is launched
    with the port number as an argument. Its main functions are **destroyProcess()**,
    to kill the process when the system is stopped, and **sendQuery()**, which sends
    a query to llm.py and waits for its response, using a new connection for each
    query.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当实例化一个新的LLMProcess时，需要在机器上找到一个可用端口，以便Java和Python进程之间进行通信。为了简化，该数据交换将通过套接字（Sockets）完成，因此在通过打开和关闭ServerSocket找到可用端口后，**llm.py**进程将使用端口号作为参数启动。其主要功能包括**destroyProcess()**，在系统停止时终止进程，以及**sendQuery()**，它向llm.py发送查询并等待响应，每个查询使用一个新的连接。
- en: Inside llm.py, there is a loop that continuously waits to accept an incoming
    connection from the Java process. When such a connection is established, it is
    handled by a [**ThreadPoolExecutor()**](https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor)
    thread via the **handle_connection()** function, which reads the input data from
    the channel, interprets it in JSON format and forwards the “text” field to the
    inference pipeline. Once the data is returned, it is sent back to the Java process
    *(on the other side of the connection)* and the functions are returned, also releasing
    their corresponding threads.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在llm.py内部，有一个循环不断等待接受来自Java进程的传入连接。当建立此类连接时，它将通过[**ThreadPoolExecutor()**](https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor)线程并通过**handle_connection()**函数进行处理，该函数从通道读取输入数据，按JSON格式解释并将“text”字段转发到推理管道。一旦数据返回，它会被发送回Java进程*(连接的另一端)*，并且函数返回，同时释放相应的线程。
- en: Model Performance
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型性能
- en: As can be seen in the script, the pipeline instance allows us to select the
    LLM model that will be executed at the hosted node. This provides us with access
    to all those uploaded to the [**Huggingface**](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending)website,
    with very diverse options such as code generation models, chat, general response
    generation, etc.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如脚本中所示，管道实例允许我们选择将在托管节点上执行的 LLM 模型。这使我们可以访问上传到[**Huggingface**](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending)网站上的所有模型，包括代码生成模型、聊天模型、通用响应生成模型等各种选择。
- en: 'By default, we use the **gpt2** model, which with about [**117M parameters**](https://huggingface.co/transformers/v2.2.0/pretrained_models.html)
    and about 500MB of weight, is the lightest and easiest option to integrate. As
    it is such a small model, its answers are rather basic, noting that a query resolution
    closely matches the prediction of the following text to the input one, as for
    example:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，我们使用**gpt2**模型，它大约有[**117M参数**](https://huggingface.co/transformers/v2.2.0/pretrained_models.html)，并且约有500MB的权重，是最轻便且最易于集成的选择。由于它是一个小型模型，它的回答相对基础，注意到一个查询的解答与以下文本的预测非常接近输入文本，例如：
- en: 'User: Hello.'
  id: totrans-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'User: 你好。'
- en: ''
  id: totrans-86
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'GPT: Hello in that the first thing I’d like to mention is that there…'
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'GPT: 你好，首先我想提到的是……'
- en: 'There are other versions of gpt2 such as **gpt2-large** or **gpt2-xl**, all
    available from Huggingface, the most powerful is XL, with 1.5B of parameters and
    6GB of weight, significantly more powerful hardware is needed to run it, producing
    coherent responses like:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他版本的gpt2，例如**gpt2-large**或**gpt2-xl**，它们都可以从Huggingface获取，其中最强大的是XL，具有15亿个参数和6GB的权重，需要显著更强的硬件来运行它，能够生成像下面这样的连贯回答：
- en: 'User: Hello.'
  id: totrans-89
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'User: 你好。'
- en: ''
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'GPT: Hello everyone — thanks for bearing with me all these months! In the past
    year I’ve put together…..'
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'GPT: 大家好——感谢大家这几个月以来的耐心等待！在过去的一年里，我已经整理出了……'
- en: Apart from the OpenAI GPT series, you can choose from many other available models,
    although most of them require an [**authentication token**](https://huggingface.co/docs/hub/en/security-tokens)to
    be inserted in the script. For example, recently modern models have been released,
    optimized in terms of occupied space and time required for a query to go through
    the entire inference pipeline. Llama3 is one of them, with small versions of [**8B
    parameters**](https://huggingface.co/nvidia/Llama3-ChatQA-1.5-8B), and large-scale
    versions of [**70B**](https://huggingface.co/nvidia/Llama3-ChatQA-1.5-70B).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 除了OpenAI的GPT系列外，你还可以选择许多其他可用的模型，尽管其中大多数需要在脚本中插入[**认证令牌**](https://huggingface.co/docs/hub/en/security-tokens)。例如，最近发布了经过优化的现代模型，优化了占用空间和查询通过整个推理流程所需的时间。Llama3就是其中之一，有[**8B参数**](https://huggingface.co/nvidia/Llama3-ChatQA-1.5-8B)的小型版本，以及[**70B**](https://huggingface.co/nvidia/Llama3-ChatQA-1.5-70B)的大型版本。
- en: However, choosing a model for a system should not be based solely on the number
    of parameters it has, since its architecture denotes the amount of knowledge it
    can model. For this reason, small models can be found with very similar performance
    to large-scale models, i.e., they produce answers with a very similar language
    understanding level, while optimizing the necessary computing resources to generate
    them. As a guide, you can use [**benchmarks**](https://huggingface.co/collections/open-llm-leaderboard/the-big-benchmarks-collection-64faca6335a7fc7d4ffe974a),
    also provided by Huggingface itself, or [**specialized tests**](https://github.com/leobeeson/llm_benchmarks)
    to measure the above parameters for any LLM.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，选择一个系统模型不应仅仅基于它的参数数量，因为它的架构决定了它可以建模的知识量。由于这个原因，一些小型模型的表现与大规模模型非常相似，即它们在语言理解层面上生成的回答非常相似，同时优化了生成这些回答所需的计算资源。作为参考，你可以使用[**基准测试**](https://huggingface.co/collections/open-llm-leaderboard/the-big-benchmarks-collection-64faca6335a7fc7d4ffe974a)，这个基准测试也由Huggingface提供，或者使用[**专业测试**](https://github.com/leobeeson/llm_benchmarks)来衡量任何LLM的上述参数。
- en: The results in the above tests, along with the average time it takes to respond
    on a given hardware is a fairly complete indicator for selecting a model. Although,
    always keep in mind that the LLM must fit in the chip memory on which it is running.
    Thus, if we use GPU inference, with CUDA as in the llm.py script, the graphical
    memory must be larger than the model size. If it is not, you must distribute the
    computation over [**several GPUs**](https://huggingface.co/docs/diffusers/training/distributed_inference),
    on the same machine, or on more than one, depending on the complexity you want
    to achieve.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 上述测试中的结果，以及在特定硬件上响应所需的平均时间，是选择模型的一个相当完整的指标。尽管如此，始终记住，LLM必须适应它运行的芯片内存。因此，如果我们使用GPU推理，如在llm.py脚本中使用CUDA，则图形内存必须大于模型大小。如果不够，你必须将计算分配到[**多个GPU**](https://huggingface.co/docs/diffusers/training/distributed_inference)，无论是在同一台机器上，还是在多台机器上，这取决于你想要达到的复杂度。
- en: Kotlin Mobile Client
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kotlin 移动客户端
- en: Before we finish, we can see how a new type of client could be included in the
    system, thus demonstrating the extensibility offered by everything we have built
    so far. This project is of course an attempt at a [**Distributing System**](https://www.geeksforgeeks.org/what-is-a-distributed-system/)
    so of course you would expect it to be compatible with mobile devices just like
    the regular ChatGPT app is compatible with Android and iOS. In our case, we can
    develop an app for native Android, although a much better option would be to adapt
    the system to a multi-platform jetpack compose project. This option remains a
    possibility for a future update.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们完成之前，可以看看如何将一种新的客户端类型包含到系统中，从而展示我们迄今为止构建的一切所提供的可扩展性。当然，这个项目是一个[**分布式系统**](https://www.geeksforgeeks.org/what-is-a-distributed-system/)的尝试，因此你可以期待它与移动设备兼容，就像常规的ChatGPT应用程序兼容Android和iOS一样。在我们的案例中，我们可以为原生Android开发一个应用程序，尽管一个更好的选择是将系统适配为多平台Jetpack
    Compose项目。这个选项仍然是未来更新的一种可能性。
- en: The initial idea is to connect the mobile client to the API and use the same
    requests as the web one, with dependencies like [**HttpURLConnection**](https://developer.android.com/reference/kotlin/java/net/HttpURLConnection).
    The code implementation isn't difficult and the documentation Android provides
    on the official page is also useful for this purpose. However, we can also emulate
    the functionality of the API with a custom Kotlin intermediate component, using
    ordinary TCP Android sockets for communication. Sockets are relatively easy to
    use, require a bit of effort to manage, ensure everything works correctly, and
    provide a decent level of control over the code. To address the lack of a regulatory
    API, we can place a Kotlin node between the mobile client and the Java node tree,
    which would manage the connection between the root node and only the mobile client
    as long as the web clients and the API are separate.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 最初的想法是将移动客户端连接到API，并使用与Web版本相同的请求，依赖项包括[**HttpURLConnection**](https://developer.android.com/reference/kotlin/java/net/HttpURLConnection)。代码实现并不困难，Android官方页面提供的文档对于此目的也很有帮助。然而，我们也可以使用自定义的Kotlin中间组件来模拟API的功能，使用普通的TCP
    Android套接字进行通信。套接字相对容易使用，需要一点管理工作，确保一切正常运行，并提供对代码的良好控制。为了弥补缺乏规范API的问题，我们可以在移动客户端和Java节点树之间放置一个Kotlin节点，它将管理根节点和只有移动客户端之间的连接，前提是Web客户端和API是分开的。
- en: Regarding the interface, the application we are imitating, ChatGPT, has a very
    clean and modern look, and since the HTTP version is already finished, we can
    try to copy it as closely as possible in the Android Studio editor.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 关于界面，我们所模仿的应用程序ChatGPT，具有非常简洁和现代的外观。由于HTTP版本已经完成，我们可以尽量在Android Studio编辑器中尽可能接近地复制它。
- en: '![](../Images/330bb76614059e14df78a0e575d37464.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/330bb76614059e14df78a0e575d37464.png)'
- en: Image by author
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: When working with sockets, we have to make sure that the user is connected to
    the correct IP address and port of the server which will solve his queries. We
    can achieve this with a new initial interface that appears every time you open
    the application. It's a simple View with a button, a text view to enter the IP
    address and a small text label to give live information of what was happening
    to the user, as you can see above.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用套接字时，我们必须确保用户连接到正确的IP地址和端口，该服务器将解决他的查询。我们可以通过每次打开应用程序时出现的新初始界面来实现这一点。它是一个简单的视图，包含一个按钮，一个用于输入IP地址的文本视图，以及一个小的文本标签，实时向用户提供发生的情况，正如你在上面所看到的那样。
- en: Then, we need the interface to resemble a real chat, where new messages appear
    at the bottom and older ones move up. To achieve this, we can insert a RecyclerView,
    which will take up about 80% of the screen. The plan is to have a predefined message
    view that could be dynamically added to the view, and it would change based on
    whether the message was from the user or the system.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要使界面像一个真实的聊天，新的消息出现在底部，旧的消息则向上移动。为此，我们可以插入一个RecyclerView，它将占据屏幕的约80%。计划是拥有一个预定义的消息视图，可以动态添加到视图中，并且会根据消息是来自用户还是系统来改变。
- en: Finally, the problem with Android connections is that you can’t do any Network
    related operation in the main thread as it would give the [**NetworkOnMainThreadException**](https://developer.android.com/reference/android/os/NetworkOnMainThreadException).
    But at the same time, you can’t manage the components if you aren’t in the main
    thread, as it will throw the [**CalledFromWrongThreadException**](https://stackoverflow.com/questions/44483224/how-can-i-fix-this-calledfromwrongthreadexception).
    We can deal with it by moving the connection view into the main one, and most
    importantly making good use of coroutines, enabling you to perform network-related
    tasks from them.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Android 连接的问题在于，你不能在主线程中执行任何与网络相关的操作，否则会抛出[**NetworkOnMainThreadException**](https://developer.android.com/reference/android/os/NetworkOnMainThreadException)。但同时，如果不在主线程中，你也无法管理组件，因为这会抛出[**CalledFromWrongThreadException**](https://stackoverflow.com/questions/44483224/how-can-i-fix-this-calledfromwrongthreadexception)。我们可以通过将连接视图移动到主线程中来解决这个问题，最重要的是充分利用协程，使你能够从中执行网络相关任务。
- en: '![](../Images/7ce7122e49603c9bd56764e9a97a5fc8.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ce7122e49603c9bd56764e9a97a5fc8.png)'
- en: Image by author
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Now, if you run the system and enter a text query, the answer should appear
    a few seconds after sending it, just like in larger applications such as ChatGPT.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你运行系统并输入一个文本查询，答案应该会在几秒钟内显示出来，就像在 ChatGPT 等大型应用程序中一样。
- en: Conclusion
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Despite having a functional system, you can make significant improvements depending
    on the technology used to implement it, both software and hardware. However, it
    can provide a decent service to a limited number of users, ranging largely depending
    on the available resources. Finally, it should be noted that achieving the performance
    of real systems like ChatGPT is complicated, since the model size and hardware
    required to support it is particularly expensive. The system shown in this article
    is highly scalable for a small, even an intermediate solution, but achieving a
    large-scale solution requires far more complex technology, and possibly leveraging
    some of the structure of this system.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管系统已经具备功能，但你可以根据所用技术（无论是软件还是硬件）做出显著改进。然而，它仍然能够为有限数量的用户提供体面的服务，具体范围大致取决于可用资源。最后，需要指出的是，像
    ChatGPT 这样的真实系统的性能是非常复杂的，因为支持它所需的模型大小和硬件特别昂贵。本文所展示的系统对于小型甚至中型解决方案具有高度可扩展性，但要实现大规模解决方案则需要更复杂的技术，可能还需要利用该系统的一些架构。
- en: Acknowledgments
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢
- en: Thanks to [**deivih84**](https://medium.com/@forexsencillo)for the collaboration
    in the **Kotlin** Mobile Client section, [**carolinaherasc**](https://medium.com/@carolinaherasc)in
    the RMI and Distributed System implementation, and [**hugodiezrubio**](https://medium.com/@hugodiezrubio)in
    developing some of the system’s management components.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢[**deivih84**](https://medium.com/@forexsencillo)在**Kotlin** 移动客户端部分的合作，感谢[**carolinaherasc**](https://medium.com/@carolinaherasc)在
    RMI 和分布式系统实现方面的帮助，以及[**hugodiezrubio**](https://medium.com/@hugodiezrubio)在开发部分系统管理组件中的贡献。
