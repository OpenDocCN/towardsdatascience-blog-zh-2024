- en: 9.11 or 9.9 — which one is higher?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9.11还是9.9——哪个更大？
- en: 原文：[https://towardsdatascience.com/9-11-or-9-9-which-one-is-higher-6efbdbd6a025?source=collection_archive---------7-----------------------#2024-07-25](https://towardsdatascience.com/9-11-or-9-9-which-one-is-higher-6efbdbd6a025?source=collection_archive---------7-----------------------#2024-07-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/9-11-or-9-9-which-one-is-higher-6efbdbd6a025?source=collection_archive---------7-----------------------#2024-07-25](https://towardsdatascience.com/9-11-or-9-9-which-one-is-higher-6efbdbd6a025?source=collection_archive---------7-----------------------#2024-07-25)
- en: Evaluating the uncertainty and brittleness in LLM prompts
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估LLM提示中的不确定性和脆弱性
- en: '[](https://medium.com/@armin.catovic?source=post_page---byline--6efbdbd6a025--------------------------------)[![Armin
    Catovic](../Images/046042098f3fec885e756f7f8ee94e6a.png)](https://medium.com/@armin.catovic?source=post_page---byline--6efbdbd6a025--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6efbdbd6a025--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6efbdbd6a025--------------------------------)
    [Armin Catovic](https://medium.com/@armin.catovic?source=post_page---byline--6efbdbd6a025--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@armin.catovic?source=post_page---byline--6efbdbd6a025--------------------------------)[![Armin
    Catovic](../Images/046042098f3fec885e756f7f8ee94e6a.png)](https://medium.com/@armin.catovic?source=post_page---byline--6efbdbd6a025--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6efbdbd6a025--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6efbdbd6a025--------------------------------)
    [Armin Catovic](https://medium.com/@armin.catovic?source=post_page---byline--6efbdbd6a025--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6efbdbd6a025--------------------------------)
    ·5 min read·Jul 25, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6efbdbd6a025--------------------------------)
    ·阅读时间5分钟·2024年7月25日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'This ChatGPT prompt and its corresponding (incorrect) response were recently
    shared and re-posted on LinkedIn countless times. They were given as a solid proof
    that the AGI is just not there yet. Further re-posts also pointed out that re-arranging
    the prompt to: *“Which one is higher: 9.11 or 9.9?”,* guarantees a correct answer,
    and further emphasizes the brittleness of LLMs.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这个ChatGPT提示及其对应的（错误的）回答最近在LinkedIn上被分享和转发了无数次。它们被作为证明AGI还远未实现的有力证据。进一步的转发还指出，将提示改为：*“哪个更大：9.11还是9.9？”*
    可以保证得到正确答案，并进一步强调了LLM的脆弱性。
- en: After evaluating both prompts against a random group of ChatGPT users, we found
    that in both cases the answer is **incorrect about 50%** of the time. As some
    users have correctly pointed out, there is a subtle ambiguity with the question,
    i.e. are we referring to mathematical inequality of two real numbers, or are we
    referring to two dates (e.g. September 11 vs September 9), or two sub-sections
    in a document (e.g. chapter 9.11 or 9.9)?
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在对一组随机的ChatGPT用户进行评估后，我们发现，在两种情况中，答案**大约50%的时间是错误的**。正如一些用户正确指出的那样，这个问题存在微妙的歧义，即我们是指两个实数的数学不等式，还是指两个日期（例如9月11日与9月9日），或者是指文档中的两个子部分（例如第9.11章或第9.9章）？
- en: We decided to perform a more controlled experiment by using OpenAI APIs. This
    way we have full control over both the system prompt and the user prompt; we can
    also take out the sampling uncertainty out of the equation as far as possible
    by e.g. setting the temperature low.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们决定通过使用OpenAI API进行更受控的实验。通过这种方式，我们可以完全控制系统提示和用户提示；我们还可以通过设置低温度等方式尽可能地去除采样不确定性。
- en: '**The final results are very interesting!**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**最终结果非常有趣！**'
- en: Hypotheses and Experimental Design
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 假设和实验设计
- en: 'Our hypotheses can be stated as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的假设可以如下表述：
- en: Given the same prompt, without any additional context, and with temperature
    kept close to zero, we should nearly always obtain the same output, with stable
    log probabilities. While people refer to LLMs as “stochastic”, for a given input,
    LLM should always generate the same output; the “hallucinations” or variance comes
    from the sampling mechanism outside of the LLM, and this we can dampen significantly
    by setting a very low temperature value.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在相同的提示下，如果没有额外的上下文，并且将温度保持接近零，我们应该几乎总是得到相同的输出，且日志概率稳定。虽然人们常将LLM称为“随机的”，但对于给定的输入，LLM应该总是生成相同的输出；所谓的“幻觉”或变异来自LLM之外的采样机制，这一点我们可以通过设置非常低的温度值来显著减弱。
- en: Based on our random user tests with ChatGPT, we would expect both the original
    prompt, and the re-worded version to give incorrect answer 50% of the time — in
    other words, without further disambiguation or context, we wouldn’t expect one
    prompt to perform better than the other.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于我们与 ChatGPT 进行的随机用户测试，我们预期原始提示和重新措辞后的版本在 50% 的情况下给出错误答案——换句话说，若没有进一步的澄清或上下文，我们不认为某一个提示会比另一个表现得更好。
- en: 'For our experiment design, we perform the following:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的实验设计，我们执行以下步骤：
- en: We conduct a number of experiments, starting with the original prompt, followed
    by a series of “interventions”
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们进行了一系列实验，从原始提示开始，随后进行一系列的“干预”
- en: For each experiment/intervention, we execute 1 000 trials
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每次实验/干预，我们执行 1 000 次试验
- en: We use OpenAI’s most advanced GPT-4o model
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用 OpenAI 最先进的 GPT-4o 模型
- en: We set the temperature to 0.1 to essentially eliminate the randomness due to
    sampling; we experiment with both random seed as well as fixed seed
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将温度设置为 0.1，以基本消除由采样引起的随机性；我们实验了随机种子和固定种子两种情况
- en: To gauge the “confidence” of the answer, we collect the log probability and
    calculate the linear probability of the answer in each trial; we plot the Kernel
    Density Estimate (KDE) of the linear probabilities across the 1 000 trials for
    each of the experiments
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了评估答案的“信心”，我们收集了日志概率并计算每次试验中答案的线性概率；我们绘制了每次实验中 1000 次试验的线性概率的核密度估计（KDE）图
- en: The full code for our experimental design is available [here](https://github.com/acatovic/llm-prompt-uncertainty-test).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验设计的完整代码可以在[这里](https://github.com/acatovic/llm-prompt-uncertainty-test)找到。
- en: Experiment (A) — Original Prompt
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验 (A) — 原始提示
- en: The user prompt is set to *“9.11 or 9.9 — which one is higher?”*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 用户提示设置为*“9.11 还是 9.9 — 哪个更大？”*。
- en: In line with what social media users have reported, **GPT-4o gives the correct
    answer 55% of the time** ☹️. The model is also not very certain — on large number
    of trials, **its “confidence” in the answer is ~80%**.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 与社交媒体用户的反馈一致，**GPT-4o 正确回答的概率为 55%** ☹️。该模型的“信心”也并不高——在大量试验中，**它对答案的“信心”约为 80%**。
- en: '![](../Images/303e02b6da3e317bb99c38ca028ad4fa.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/303e02b6da3e317bb99c38ca028ad4fa.png)'
- en: Figure 1 — Smoothed histogram (KDE) of confidence values (0–100%) across 1000
    trials, when the original user prompt is used; image by the author
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1 — 在 1000 次试验中，当使用原始用户提示时，信心值（0-100%）的平滑直方图（KDE）；图片由作者提供
- en: Experiment (B) — Re-worded User Prompt
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验 (B) — 重新措辞的用户提示
- en: 'In the re-worded user prompt, no additional context/disambiguation is provided,
    but the wording is slightly changed to: *“Which one is higher, 9.11 or 9.9?”*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在重新措辞的用户提示中，未提供额外的上下文/澄清，但措辞略作更改为：*“哪个更大，9.11 还是 9.9？”*
- en: Amazingly, and contrary to our ChatGPT user tests, the **correct answer is reached
    100% of the time** across 1 000 trials. Furthermore, the model exhibits **very
    high confidenc**e in its answer 🤔.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，与我们的 ChatGPT 用户测试相反，**在 1000 次试验中正确答案的达成率为 100%**。此外，模型对其答案表现出**非常高的信心**🤔。
- en: '![](../Images/bd2ac34fa08ee808f5ab358f41a3e665.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd2ac34fa08ee808f5ab358f41a3e665.png)'
- en: Figure 2 — Smoothed histogram (KDE) of confidence values (0–100%) across 1000
    trials, when the original user prompt is slightly re-worded; image by the author
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2 — 在 1000 次试验中，当原始用户提示被稍微重新措辞时，信心值（0-100%）的平滑直方图（KDE）；图片由作者提供
- en: Experiment (C) — Original User Prompt with Reasoning
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验 (C) — 带推理的原始用户提示
- en: There has been significant work recently in trying to induce improved “reasoning”
    capabilities in LLMs with chain-of-thought (CoT) prompting being the most popular.
    [Huang et al](https://arxiv.org/pdf/2212.10403) have published a very comprehensive
    survey on LLM reasoning capabilities.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在努力提高大语言模型“推理”能力方面做了大量工作，其中链式推理（CoT）提示法是最受欢迎的。[黄等](https://arxiv.org/pdf/2212.10403)发布了关于
    LLM 推理能力的非常全面的调查。
- en: As such, we modify the original user prompt by also telling the LLM to explain
    its reasoning. Interestingly enough, the **probability of correct answer improves
    to 62%**, however the answers come with **even greater uncertainty**.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们修改了原始用户提示，要求大语言模型同时解释其推理。有趣的是，**正确答案的概率提高到了 62%**，然而答案伴随的是**更大的不确定性**。
- en: '![](../Images/c8d22876d032feec673a6dce588b7c6d.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c8d22876d032feec673a6dce588b7c6d.png)'
- en: Figure 3 — Smoothed histogram (KDE) of confidence values (0–100%) across 1000
    trials, when the original user prompt is modified to also “explain its reasoning”;
    image by the author
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3 — 在 1000 次试验中，当原始用户提示被修改为同时“解释其推理”时，信心值（0-100%）的平滑直方图（KDE）；图片由作者提供
- en: Experiment (D) — Original User Prompt with Reasoning in the System Prompt
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验（D）— 原始用户提示与系统提示中的推理
- en: The final experiment is the same as experiment “C”, however we instead bootstrap
    the **system prompt** by telling the LLM to “explain its reasoning”. Incredibly,
    we now see the **correct answer 100% of the time**, with **very high confidence**.
    We see identical results if we use the re-worded user prompt as well.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的实验与实验“C”相同，但我们改为通过告诉LLM“解释其推理过程”来引导**系统提示**。令人难以置信的是，我们现在看到**100%正确的答案**，并且**信心极高**。如果我们使用重新表述的用户提示，结果也完全相同。
- en: '![](../Images/c7fbb5db47568c2f787a98e7d755c7dc.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7fbb5db47568c2f787a98e7d755c7dc.png)'
- en: Figure 4 — Smoothed histogram (KDE) of confidence values (0–100%) across 1000
    trials, with the original user prompt, and system prompt amended with instructions
    to “explain its reasoning”; image by the author
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图4 — 信心值（0-100%）在1000次试验中的平滑直方图（KDE），使用原始用户提示，并且系统提示已修改，增加了“解释其推理过程”的指示；图像由作者提供
- en: Conclusion and Takeaways
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论与总结
- en: 'What started off as a simple experiment to validate some of the statements
    seen on social media, ended up with some very interesting findings. Let’s summarize
    the key takeaways:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最初是一个简单的实验，旨在验证社交媒体上看到的一些说法，结果却得出了一些非常有趣的发现。让我们总结一下关键结论：
- en: '**For an identical prompt, with both temperature set very low (essentially
    eliminating sampling uncertainty), and a fixed seed value, we see very large variance
    in log probabilities**. Slight variance can be explained by hardware precision,
    but variance this large is very difficult to explain. It indicates that either
    (1) sampling mechanism is a LOT more complicated, or (2) there are more layers/models
    upstream beyond our control.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对于相同的提示，当温度设置非常低（基本消除采样不确定性）并且种子值固定时，我们看到对数概率的方差非常大**。轻微的方差可以通过硬件精度来解释，但如此大的方差很难解释。这表明，可能有两种情况：（1）采样机制复杂得多，或者（2）在我们控制范围之外，还有更多的层级/模型在上游。'
- en: In line with previous literature, **simply instructing the LLM to “explain its
    reasoning” improves its performance**.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与之前的文献一致，**仅仅指示LLM“解释其推理过程”就能提高其表现**。
- en: '**There is clearly a distinct handling between the system prompt and the user
    prompt**. Bootstrapping a role in the system prompt as opposed to the user prompt,
    seems to result in significantly better performance.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**系统提示与用户提示之间显然有明显的差异处理**。在系统提示中引导角色，而非在用户提示中引导，似乎会显著提高性能。'
- en: We can clearly see how brittle the prompts can be. The key takeaway here is
    that we should always aim to provide disambiguation and clear context in our prompts.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以清楚地看到提示是多么脆弱。这里的关键结论是，我们应该始终努力在提示中提供消歧义和明确的上下文。
- en: '***Disclaimer:*** *due to heavy coverage on social media, it is likely that
    the lovely people at OpenAI have in fact improved the above behaviour, so the
    results may not be directly reproducible. However the key takeaways are still
    very valid!*'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '***免责声明：*** *由于社交媒体上的广泛讨论，OpenAI 的工作人员可能已经改进了上述行为，因此结果可能无法直接重现。但关键的结论仍然有效！*'
