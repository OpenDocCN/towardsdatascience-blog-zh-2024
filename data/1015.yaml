- en: Deep Dive into Self-Attention by Hand✍︎
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手动深入了解自注意力机制✍︎
- en: 原文：[https://towardsdatascience.com/deep-dive-into-self-attention-by-hand-%EF%B8%8E-f02876e49857?source=collection_archive---------0-----------------------#2024-04-22](https://towardsdatascience.com/deep-dive-into-self-attention-by-hand-%EF%B8%8E-f02876e49857?source=collection_archive---------0-----------------------#2024-04-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/deep-dive-into-self-attention-by-hand-%EF%B8%8E-f02876e49857?source=collection_archive---------0-----------------------#2024-04-22](https://towardsdatascience.com/deep-dive-into-self-attention-by-hand-%EF%B8%8E-f02876e49857?source=collection_archive---------0-----------------------#2024-04-22)
- en: Explore the intricacies of the attention mechanism responsible for fueling the
    transformers
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索驱动 Transformer 的注意力机制的复杂性
- en: '[](https://medium.com/@srijanie.dey?source=post_page---byline--f02876e49857--------------------------------)[![Srijanie
    Dey, PhD](../Images/2b3292a3b22d712d91d0bfc14df64446.png)](https://medium.com/@srijanie.dey?source=post_page---byline--f02876e49857--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f02876e49857--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f02876e49857--------------------------------)
    [Srijanie Dey, PhD](https://medium.com/@srijanie.dey?source=post_page---byline--f02876e49857--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@srijanie.dey?source=post_page---byline--f02876e49857--------------------------------)[![Srijanie
    Dey, PhD](../Images/2b3292a3b22d712d91d0bfc14df64446.png)](https://medium.com/@srijanie.dey?source=post_page---byline--f02876e49857--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f02876e49857--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f02876e49857--------------------------------)
    [Srijanie Dey, PhD](https://medium.com/@srijanie.dey?source=post_page---byline--f02876e49857--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f02876e49857--------------------------------)
    ·12 min read·Apr 22, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f02876e49857--------------------------------)
    ·阅读时间：12分钟·2024年4月22日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Attention! Attention!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 注意！注意！
- en: Because ‘Attention is All You Need’.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 因为‘注意力即你所需’。
- en: No, I am not saying that, the Transformer is.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 不，我并不是这么说，Transformer 是。
- en: '![](../Images/fa02dd175a9c6c0c241a9a16281d8d0d.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa02dd175a9c6c0c241a9a16281d8d0d.png)'
- en: Image by author (Robtimus Prime seeking attention. As per my son, bright rainbow
    colors work better for attention and hence the color scheme.)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自作者（Robtimus Prime 寻求关注。根据我儿子的话，鲜艳的彩虹色更能吸引注意，因此采用了这种配色方案。）
- en: 'As of today, the world has been swept over by the power of transformers. Not
    the likes of ‘Robtimus Prime’ but the ones that constitute neural networks. And
    that power is because of the concept of ‘**attention**’. So, what does attention
    in the context of transformers really mean? Let’s try to find out some answers
    here:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 到今天为止，Transformer 的强大力量已经席卷了整个世界。不是像‘Robtimus Prime’那样的变形金刚，而是构成神经网络的那些。而这种力量正是源于‘**注意力**’的概念。那么，Transformer
    上下文中的注意力究竟意味着什么呢？让我们试着在这里找到一些答案：
- en: 'First and foremost:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 首先：
- en: '**What are transformers?**'
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**什么是 Transformer？**'
- en: Transformers are neural networks that specialize in learning context from the
    data. Quite similar to us trying to find the meaning of ***‘attention and context’***
    in terms of transformers.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 是一种专注于从数据中学习上下文的神经网络。它们与我们尝试从 Transformer 的角度理解 ***‘注意力与上下文’***
    的含义非常相似。
- en: '**How do transformers learn context from the data?**'
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**Transformer 如何从数据中学习上下文？**'
- en: By using the attention mechanism.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用注意力机制。
- en: '**What is the attention mechanism?**'
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**什么是注意力机制？**'
- en: The attention mechanism helps the model scan all parts of a sequence at each
    step and determine which elements need to be focused on. The attention mechanism
    was proposed as an alternative to the ‘strict/hard’ solution of fixed-length vectors
    in the encoder-decoder architecture and provide a ‘soft’ solution focusing only
    on the relevant parts.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制帮助模型在每一步扫描序列的所有部分，并确定需要关注哪些元素。注意力机制被提出作为对编码器-解码器架构中固定长度向量的‘严格/硬性’解决方案的替代方案，提供了一种‘软性’解决方案，只关注相关部分。
- en: '**What is self-attention?**'
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**什么是自注意力机制？**'
- en: The attention mechanism worked to improve the performance of Recurrence Neural
    Networks (RNNs), with the effect seeping into Convolutional Neural Networks (CNNs).
    However, with the introduction of the transformer architecture in the year 2017,
    the need for RNNs and CNNs was quietly obliterated. And the central reason for
    it was the self-attention mechanism.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制最初是为了改善递归神经网络（RNN）的性能，效果随后也渗透到了卷积神经网络（CNN）。然而，随着2017年变压器架构的引入，RNN和CNN的需求悄然消失。而这一切的核心原因正是自注意力机制。
- en: The self-attention mechanism was special in the sense that it was built to inculcate
    the context of the input sequence in order to enhance the attention mechanism.
    This idea became transformational as it was able to capture the complex nuances
    of a language.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力机制的特殊之处在于，它旨在融入输入序列的上下文，以增强注意力机制。这个思想变得具有变革性，因为它能够捕捉语言中的复杂细微差别。
- en: 'As an example:'
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 举个例子：
- en: ''
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: When I ask my 4-year old what transformers are, his answer only contains the
    words robots and cars. Because that is the only context he has. But for me, transformers
    also mean neural networks as this second context is available to the slightly
    more experienced mind of mine. And that is how different contexts provide different
    solutions and so tend to be very important.
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当我问我4岁的孩子“变压器是什么”时，他的回答仅包含机器人和汽车这两个词。因为那是他所理解的唯一上下文。但对我来说，变压器也意味着神经网络，因为对于我这个稍微有些经验的人来说，第二种上下文也是存在的。正是这种不同的上下文提供了不同的解决方案，因此它们通常非常重要。
- en: '**The word ‘self’ refers to the fact that the attention mechanism examines
    the same input sequence that it is processing.**'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**“自”一词指的是注意力机制检查它正在处理的相同输入序列。**'
- en: There are many variations of how self-attention is performed. But the **scaled
    dot-product** mechanism has been one of the most popular ones. This was the one
    introduced in the original transformer architecture paper in 2017 — “[Attention
    is All You Need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)”.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力的执行方式有很多种变体。但**缩放点积**机制是最流行的方式之一。这也是在2017年原始变压器架构论文“[Attention is All You
    Need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)”中介绍的方式。
- en: '**Where and how does self-attention feature in transformers?**'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**自注意力在变压器中扮演什么角色，如何发挥作用？**'
- en: I like to see the transformer architecture as a combination of two shells —
    the outer shell and the inner shell.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢把变压器架构看作是由两层结构组成——外部结构和内部结构。
- en: The outer shell is a combination of the attention-weighting mechanism and the
    feed forward layer about which I talk in detail in this [article](https://medium.com/towards-data-science/deep-dive-into-transformers-by-hand-︎-68b8be4bd813).
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 外部结构是注意力加权机制和前馈层的结合，关于这一点，我在这篇[文章](https://medium.com/towards-data-science/deep-dive-into-transformers-by-hand-︎-68b8be4bd813)中有详细介绍。
- en: The inner shell consists of the self-attention mechanism and is part of the
    attention-weighting feature.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 内部结构由自注意力机制组成，是注意力加权功能的一部分。
- en: So, without further delay, let us dive into the details behind the self-attention
    mechanism and unravel the workings behind it. The **Query-Key module** and the
    **SoftMax** function play a crucial role in this technique.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，事不宜迟，让我们深入探讨自注意力机制背后的细节，并揭开其工作原理。**查询-键模块**和**SoftMax**函数在这一技术中起着至关重要的作用。
- en: This discussion is based on Prof. Tom Yeh’s wonderful AI by Hand Series on [Self-Attention](https://lnkd.in/gDW8Um4W).
    (All the images below, unless otherwise noted, are by Prof. Tom Yeh from the above-mentioned
    LinkedIn post, which I have edited with his permission.)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这段讨论基于Tom Yeh教授的精彩《人工智能手工系列》中的[自注意力](https://lnkd.in/gDW8Um4W)内容。（以下所有图片，除非另有说明，均来自Tom
    Yeh教授的LinkedIn文章，我已获得他的许可并进行了编辑。）
- en: 'So here we go:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们开始吧：
- en: '**Self-Attention**'
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**自注意力**'
- en: To build some context here, here is a pointer to how we process the **‘Attention-Weighting’**
    in the transformerouter shell.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供一些背景，这里有一个指针，展示我们如何在变压器的外部结构中处理**‘注意力加权’**。
- en: '**Attention weight matrix (A)**'
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**注意力权重矩阵（A）**'
- en: The attention weight matrix **A** is obtained by feeding the input features
    into the Query-Key (QK) module. This matrix tries to find the most relevant parts
    in the input sequence. Self-Attention comes into play while creating the Attention
    weight matrix **A** using the QK-module.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力权重矩阵**A**是通过将输入特征输入查询-键（QK）模块获得的。这个矩阵试图找到输入序列中最相关的部分。在创建注意力权重矩阵**A**时，自注意力机制开始发挥作用，利用QK模块进行计算。
- en: '![](../Images/98c0a5d3d06cbc56f250062940a59e77.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/98c0a5d3d06cbc56f250062940a59e77.png)'
- en: '**How does the QK-module work?**'
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**QK模块是如何工作的？**'
- en: 'Let us look at The different components of Self-Attention: **Query (Q), Key
    (K)** and **Value (V)**.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看自注意力（Self-Attention）的不同组成部分：**查询（Q）、键（K）**和**值（V）**。
- en: I love using the spotlight analogy here as it helps me visualize the model throwing
    light on each element of the sequence and trying to find the most relevant parts.
    Taking this analogy a bit further, let us use it to understand the different components
    of Self-Attention.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢用聚光灯的比喻来帮助我形象化这个模型，理解它如何照亮序列中的每个元素，并尝试找到最相关的部分。把这个比喻延伸一下，让我们用它来理解自注意力的不同组成部分。
- en: Imagine a big stage getting ready for the world’s largest Macbeth production.
    The audience outside is teeming with excitement.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个巨大的舞台正在为世界上最大的《麦克白》制作做准备。外面的观众兴奋不已。
- en: '*The lead actor walks onto the stage, the spotlight shines on him and he asks
    in his booming voice “Should I seize the crown?”. The audience whispers in hushed
    tones and wonders which path this question will lead to. Thus, Macbeth himself
    represents the role of* ***Query (Q)*** *as he asks pivotal questions and drives
    the story forward.*'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*主角走上舞台，聚光灯照在他身上，他用洪亮的声音问道：“我应该夺取王冠吗？”观众低声耳语，猜测这个问题将会引领到何种结局。因此，麦克白本身代表了* ***查询（Q）***
    *的角色，他提出关键问题并推动故事向前发展。*'
- en: '*Based on Macbeth’s query, the spotlight shifts to other crucial characters
    that hold information to the answer. The influence of other crucial characters
    in the story, like Lady Macbeth, triggers Macbeth’s own ambitions and actions.
    These other characters can be seen as the* ***Key (K)*** *as they unravel different
    facets of the story based on the particulars they know.*'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*根据麦克白的提问，聚光灯转向了其他掌握答案信息的重要人物。故事中其他关键人物，如麦克白夫人，通过其行动激发了麦克白的野心和行动。这些人物可以视为*
    ***键（K）*** *，他们根据所知的不同细节解开故事的各个方面。*'
- en: '*Finally, the extended characters — family, friends, supporter, naysayers provide
    enough motivation and information to Macbeth by their actions and perspectives.
    These can be seen as* ***Value (V).*** *The* ***Value (V)*** *pushes Macbeth towards
    his decisions and shapes the fate of the story.*'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*最后，扩展的角色——家人、朋友、支持者和反对者通过他们的行动和视角为麦克白提供了足够的动机和信息。这些可以视为* ***值（V）***。* *值（V）*推动麦克白做出决策并塑造故事的命运。*'
- en: And with that is created one of the world’s finest performances, that remains
    etched in the minds of the awestruck audience for the years to come.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 随之而来的是世界上最精彩的演出之一，它将深深铭刻在敬畏的观众脑海中，成为多年之后仍然难以忘怀的经典。
- en: 'Now that we have witnessed the role of **Q**, **K**, **V** in the fantastical
    world of performing arts, let’s return to planet matrices and learn the mathematical
    nitty-gritty behind the **QK-module**. This is the roadmap that we will follow:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经见证了**Q**、**K**、**V**在戏剧世界中的作用，让我们回到矩阵的世界，学习**QK模块**背后的数学原理。这是我们将要遵循的路线图：
- en: '![](../Images/de5f0f180195583b9ef5aed6a8b04bbb.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de5f0f180195583b9ef5aed6a8b04bbb.png)'
- en: Roadmap for the Self-Attention mechanism
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力机制的路线图
- en: And so the process begins.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 于是，过程开始了。
- en: '**We are given:**'
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**我们已知：**'
- en: A set of 4-feature vectors (Dimension 6)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一组4个特征向量（维度6）
- en: '![](../Images/a6ad182544df69a2550ac9be45b5015f.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a6ad182544df69a2550ac9be45b5015f.png)'
- en: '**Our goal :**'
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**我们的目标：**'
- en: Transform the given features into **Attention Weighted Features**.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 将给定的特征转化为**注意力加权特征**。
- en: '[1] **Create Query, Key, Value Matrices**'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] **创建查询、键和值矩阵**'
- en: 'To do so, we multiply the features with linear transformation matrices W_Q,
    W_K, and W_V, to obtain query vectors (q1,q2,q3,q4), key vectors (k1,k2,k3,k4),
    and value vectors (v1,v2,v3,v4) respectively as shown below:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将特征与线性变换矩阵W_Q、W_K和W_V相乘，分别得到查询向量（q1，q2，q3，q4）、键向量（k1，k2，k3，k4）和值向量（v1，v2，v3，v4），如下面所示：
- en: 'To get **Q**, multiply W_Q with X:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到**Q**，将W_Q与X相乘：
- en: '![](../Images/b6bb9e18f73411d19d946034f15605c2.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b6bb9e18f73411d19d946034f15605c2.png)'
- en: 'To get **K**, multiply W_K with X:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到**K**，将W_K与X相乘：
- en: '![](../Images/f535191e8ee035b3d596470749b19577.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f535191e8ee035b3d596470749b19577.png)'
- en: Similarly, to get **V**, multiply W_V with X.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，为了得到**V**，将W_V与X相乘。
- en: '**To be noted:**'
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**需要注意：**'
- en: As can be seen from the calculation above, we use the same set of features for
    both queries and keys. And that is how the idea of **“self”** comes into play
    here, i.e. the model uses the same set of features to create its query vector
    as well as the key vector.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如上所述的计算所示，我们对查询和键使用相同的特征集。这也是**“自我”**概念的体现，即模型使用相同的特征集来生成其查询向量和键向量。
- en: The **query vector** represents the current word (or token) for which we want
    to compute attention scores relative to other words in the sequence.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**查询向量**表示当前的单词（或标记），我们想要计算其相对于序列中其他单词的注意力得分。'
- en: The **key vector** represents the other words (or tokens) in the input sequence
    and we compute the attention score for each of them with respect to the current
    word.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**键向量**表示输入序列中的其他单词（或标记），我们计算每个键向量相对于当前单词的注意力得分。'
- en: '[2] **Matrix Multiplication**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] **矩阵乘法**'
- en: The next step is to multiply the transpose of **K** with **Q** i.e. **K**^T
    . **Q**.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将 **K** 的转置与 **Q** 相乘，即 **K**^T . **Q**。
- en: The idea here is to calculate the dot product between every pair of query and
    key vectors. Calculating the dot product gives us an estimate of the matching
    score between every “key-query” pair, by using the idea of **Cosine Similarity**
    between the two vectors. This is the ***‘dot-product’*** part of the scaled dot-product
    attention.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的想法是计算每一对查询和键向量之间的点积。通过计算点积，我们可以估计每个“键-查询”对之间的匹配得分，利用**余弦相似度**的概念。这就是缩放点积注意力中的
    ***‘点积’*** 部分。
- en: '**Cosine-Similarity**'
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**余弦相似度**'
- en: ''
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Cosine similarity is the cosine of the angle between the vectors; that is, it
    is the dot product of the vectors divided by the product of their lengths. It
    roughly measures if two vectors are pointing in the same direction thus implying
    the two vectors are similar.
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 余弦相似度是向量之间夹角的余弦值；也就是说，它是向量的点积除以它们长度的乘积。它大致衡量了两个向量是否朝同一方向指向，从而意味着这两个向量是相似的。
- en: ''
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Remember cos(0°) = 1, cos(90°) = 0 , cos(180°) =-1**'
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**记住 cos(0°) = 1, cos(90°) = 0 , cos(180°) = -1**'
- en: ''
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- If the dot product between the two vectors is approximately 1, it implies
    we are looking at an almost zero angle between the two vectors meaning they are
    very close to each other.'
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- 如果两个向量之间的点积大约为 1，意味着它们之间的夹角接近零，即它们非常相似。'
- en: ''
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- If the dot product between the two vectors is approximately 0, it implies
    we are looking at vectors that are orthogonal to each other and not very similar.'
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- 如果两个向量之间的点积大约为 0，意味着它们是正交的，且不太相似。'
- en: ''
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- If the dot product between the two vectors is approximately -1, it implies
    we are looking at an almost an 180° angle between the two vectors meaning they
    are opposites.'
  id: totrans-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- 如果两个向量之间的点积大约为 -1，意味着它们之间的夹角接近 180°，即它们是相反的。'
- en: '![](../Images/977171591ba823b623324c65070e09d0.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/977171591ba823b623324c65070e09d0.png)'
- en: '[3] **Scale**'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] **缩放**'
- en: The next step is to scale/normalize each element by the square root of the dimension
    ‘*d_k*’. In our case the number is 3\. Scaling down helps to keep the impact of
    the dimension on the matching score in check.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将每个元素按维度 ‘*d_k*’ 的平方根进行缩放/归一化。在我们的例子中，数字是 3。缩放有助于保持维度对匹配得分的影响在可控范围内。
- en: How does it do so? As per the original Transformer paper and going back to Probability
    101, if two independent and identically distributed (i.i.d) variables *q* and
    *k* with mean 0 and variance 1 with dimension d are multiplied, the result is
    a new random variable with mean remaining 0 but variance changing to *d_k*.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 它是如何做到的呢？根据原始的 Transformer 论文，并回顾一下概率论 101，如果两个独立同分布（i.i.d）变量 *q* 和 *k* 具有均值为
    0，方差为 1，维度为 d，则它们相乘后的结果是一个新的随机变量，其均值保持为 0，但方差变为 *d_k*。
- en: Now imagine how the matching score would look if our dimension is increased
    to 32, 64, 128 or even 4960 for that matter. The larger dimension would make the
    variance higher and push the values into regions ‘unknown’.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，想象一下如果我们的维度增加到 32、64、128 或甚至 4960 时，匹配得分会是什么样的。更大的维度会增加方差，并将值推入“未知”区域。
- en: To keep the calculation simple here, since *sqrt* [3] is approximately 1.73205,
    we replace it with [ *floor*(□/2) ].
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化计算，考虑到 *sqrt* [3] 大约是 1.73205，我们将其替换为 [ *floor*(□/2) ]。
- en: '**Floor Function**'
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**取整函数**'
- en: ''
  id: totrans-88
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The floor function takes a real number as an argument and returns the largest
    integer less than or equal to that real number.
  id: totrans-89
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 取整函数接受一个实数作为输入，并返回不大于该实数的最大整数。
- en: ''
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Eg : floor(1.5) = 1, floor(2.9) = 2, floor (2.01) = 2, floor(0.5) = 0.'
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 例如：floor(1.5) = 1, floor(2.9) = 2, floor (2.01) = 2, floor(0.5) = 0。
- en: ''
  id: totrans-92
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The opposite of the floor function is the ceiling function.
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: floor函数的反函数是ceil函数。
- en: '![](../Images/4995dd56bc12525e18ac38b916ef58f6.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4995dd56bc12525e18ac38b916ef58f6.png)'
- en: This the ‘scaled’ part of the scaled dot-product attention.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这是“缩放”部分的缩放点积注意力。
- en: '[4] **Softmax**'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] **Softmax**'
- en: 'There are three parts to this step:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步分为三个部分：
- en: Raise e to the power of the number in each cell (To make things easy, we use
    3 to the power of the number in each cell.)
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个单元格中的数字取e的幂（为了简化，我们使用3的幂来计算每个单元格中的数字。）
- en: Sum these new values across each column.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 求出每列中这些新值的总和。
- en: For each column, divide each element by its respective sum (Normalize). The
    purpose of normalizing each column is to have numbers sum up to 1\. In other words,
    each column then becomes a **probability distribution** of attention, which gives
    us our **Attention Weight Matrix (A).**
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每一列，将每个元素除以其相应的总和（归一化）。归一化每一列的目的是使数字总和为1。换句话说，每一列然后成为一个**注意力的概率分布**，这给我们带来了我们的**注意力权重矩阵(A)**。
- en: '![](../Images/023e56eda75ef16ae51abfb8df4dae56.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/023e56eda75ef16ae51abfb8df4dae56.png)'
- en: '**This Attention Weight Matrix is what we had obtained after passing our feature
    matrix through the QK-module in Step 2 in the Transformers section.**'
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**这个注意力权重矩阵是我们在转换器部分的第2步中通过QK模块传递特征矩阵后获得的。**'
- en: '*(Remark: The first column in the Attention Weight Matrix has a typo as the
    current elements don’t add up to 1\. Please double-check. We are allowed these
    errors because we are human.)*'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*(备注：注意力权重矩阵中的第一列存在一个拼写错误，因为当前元素的总和不为1。请仔细检查。我们允许这些错误，因为我们是人类。)*'
- en: The Softmax step is important as it assigns probabilities to the score obtained
    in the previous steps and thus helps the model decide how much importance (higher/lower
    attention weights) needs to be given to each word given the current query. As
    is to be expected, higher attention weights signify greater relevance allowing
    the model to capture dependencies more accurately.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax步骤很重要，因为它为前面步骤中获得的分数分配概率，从而帮助模型决定在当前查询下需要给每个单词多少重要性（更高/更低的注意力权重）。正如预期的那样，更高的注意力权重表示更大的相关性，使模型能够更准确地捕捉依赖关系。
- en: Once again, the scaling in the previous step becomes important here. Without
    the scaling, the values of the resultant matrix gets pushed out into regions that
    are not processed well by the Softmax function and may result in vanishing gradients.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，前一步中的缩放在这里变得很重要。没有缩放，结果矩阵的值会被推到Softmax函数处理不好的区域，可能导致梯度消失。
- en: '[5] **Matrix Multiplication**'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] **矩阵乘法**'
- en: Finally we multiply the value vectors (**V**s) with the Attention Weight Matrix
    (**A**). These value vectors are important as they contain the information associated
    with each word in the sequence.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将值向量(**V**s)与注意力权重矩阵(**A**)相乘。这些值向量很重要，因为它们包含与序列中每个单词相关的信息。
- en: '![](../Images/65df264c09946b1ed85a2343598b375f.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/65df264c09946b1ed85a2343598b375f.png)'
- en: And the result of the final multiplication in this step are the **attention
    weighted features Z**s which are the ultimate solution of the self-attention mechanism.
    These attention-weighted features essentially contain a **weighted representation**
    **of the features** assigning higher weights for features with higher relevance
    as per the context.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步中最终乘法的结果是**注意力加权特征Z**s，这是自注意机制的最终解决方案。这些注意力加权特征基本上包含了**特征的加权表示**，根据上下文的相关性为具有更高相关性的特征分配更高的权重。
- en: Now with this information available, we continue to the next step in the transformer
    architecture where the feed-forward layer processes this information further.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有了这些信息，我们继续转换器架构中的下一步，其中前馈层进一步处理这些信息。
- en: And this brings us to the end of the brilliant self-attention technique!
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是辉煌的自注意力技术的结束！
- en: 'Reviewing all the key points based on the ideas we talked about above:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们上面讨论的想法，回顾所有关键点：
- en: Attention mechanism was the result of an effort to better the performance of
    RNNs, **addressing the issue of fixed-length vector representations** in the encoder-decoder
    architecture. The flexibility of soft-length vectors with a focus on the relevant
    parts of a sequence was the core strength behind attention.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意机制是为了改善RNN的性能而产生的，**解决了编码器-解码器架构中固定长度向量表示的问题**。软长度向量的灵活性，重点放在序列中相关部分上，是注意力背后的核心优势。
- en: Self-attention was introduced as a way to inculcate the idea of context into
    the model. The self-attention mechanism **evaluates the same input sequence that
    it processes**, hence the use of the word ‘self’.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自注意力机制被引入作为一种将上下文思想融入模型的方法。自注意力机制**评估它所处理的相同输入序列**，因此使用了“自”这个词。
- en: There are many variants to the **self-attention mechanism** and efforts are
    ongoing to make it more efficient. However, scaled dot-product attention is one
    of the most popular ones and a crucial reason why the transformer architecture
    was deemed to be so powerful.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自注意力机制有许多变体，且目前仍在不断努力使其更加高效。然而，缩放点积注意力是其中最受欢迎的一种，也是Transformer架构被认为如此强大的关键原因之一。
- en: Scaled dot-product self-attention mechanism comprises the **Query-Key module
    (QK-module)** along with the **Softmax function**. The QK module is responsible
    for extracting the relevance of each element of the input sequence by calculating
    the attention scores and the Softmax function complements it by assigning probability
    to the attention scores.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 缩放点积自注意力机制包括**查询-键模块（QK模块）**以及**Softmax函数**。QK模块负责通过计算注意力分数来提取输入序列中每个元素的相关性，而Softmax函数通过为注意力分数分配概率来补充它。
- en: Once the attention-scores are calculated, **they are multiplied with the value
    vector to obtain the attention-weighted features** which are then passed on to
    the feed-forward layer.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦计算出注意力分数，**它们会与值向量相乘，以获得加权注意力特征**，然后这些特征会传递到前馈层。
- en: Multi-Head Attention
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多头注意力
- en: To cater to a varied and overall representation of the sequence, multiple copies
    of the **self-attention mechanism are implemented in parallel** which are then
    concatenated to produce the final attention-weighted values. This is called the
    Multi-Head Attention.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了适应对序列的多样化和全面表示，**多个自注意力机制副本会并行实现**，然后它们会被连接起来，生成最终的加权注意力值。这被称为多头注意力。
- en: Transformer in a Nutshell
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer概述
- en: 'This is how the inner-shell of the transformer architecture works. And bringing
    it together with the outer shell, here is a summary of the Transformer mechanism:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是Transformer架构的内部工作原理。将其与外部结构结合起来，以下是Transformer机制的总结：
- en: The two big ideas in the Transformer architecture here are **attention-weighting
    and the feed-forward layer (FFN)**. Both of them combined together allow the Transformer
    to analyze the input sequence from two directions. **Attention** looks at the
    sequence based on **positions** and the **FFN** does it based on the **dimensions**
    of the feature matrix.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Transformer架构中的两个关键思想是**加权注意力和前馈层（FFN）**。这两者结合在一起，使得Transformer可以从两个方向分析输入序列。**注意力**基于**位置**查看序列，**FFN**则基于**特征矩阵的维度**来分析。
- en: The part that powers the attention mechanism is the **scaled dot-product Attention**
    which consists of the **QK-module** and outputs the attention weighted features.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 驱动注意力机制的部分是**缩放点积注意力**，它由**QK模块**组成，并输出加权注意力特征。
- en: '**‘Attention Is really All You Need’**'
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**‘注意力才是你所需要的一切’**'
- en: Transformers have been here for only a few years and the field of AI has already
    seen tremendous progress based on it. And the effort is still ongoing. When the
    authors of the paper used that title for their paper, they were not kidding.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer只存在了几年，AI领域已经在此基础上取得了巨大的进展，且这一努力仍在持续。当论文的作者给他们的论文起这个标题时，他们可不是开玩笑。
- en: It is interesting to see once again how a fundamental idea — the ‘dot product’
    coupled with certain embellishments can turn out to be so powerful!
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 再次看到一个基本思想——‘点积’与某些修饰结合，竟然能变得如此强大，真是令人感到有趣！
- en: '![](../Images/a50dd9ea40a103d373f2c0faa0b7b37b.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a50dd9ea40a103d373f2c0faa0b7b37b.png)'
- en: Image by author
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自作者
- en: P.S. If you would like to work through this exercise on your own, here are the
    blank templates for you to use.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 附言：如果你希望独立完成这项练习，下面是可以使用的空白模板。
- en: '[Blank Template for hand-exercise](https://drive.google.com/file/d/1_wpS7-Mq6HiuCVe4ozmPXO3LydcJMtok/view?usp=drive_link)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[手动练习的空白模板](https://drive.google.com/file/d/1_wpS7-Mq6HiuCVe4ozmPXO3LydcJMtok/view?usp=drive_link)'
- en: Now go have some fun with the exercise while paying attention to your **Robtimus
    Prime**!
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在去享受一下练习吧，同时关注你的**Robtimus Prime**！
- en: '**Related Work:**'
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**相关工作：**'
- en: 'Here are the other articles in the Deep Dive by Hand Series:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是《深入了解手动系列》中的其他文章：
- en: '[Deep Dive into Vector Databases by Hand](https://medium.com/towards-data-science/deep-dive-into-vector-databases-by-hand-e9ab71f54f80)
    ✍ that explores what exactly happens behind-the-scenes in Vector Databases.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[手动深入探讨向量数据库](https://medium.com/towards-data-science/deep-dive-into-vector-databases-by-hand-e9ab71f54f80)
    ✍ 探索向量数据库背后到底发生了什么。'
- en: '[Deep Dive into Sora’s Diffusion Transformer (DiT) by Hand](https://medium.com/towards-data-science/deep-dive-into-soras-diffusion-transformer-dit-by-hand-︎-1e4d84ec865d)
    ✍ that explores the secret behind Sora’s state-of-the-art videos.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[手动深入探讨 Sora 的扩散 Transformer (DiT)](https://medium.com/towards-data-science/deep-dive-into-soras-diffusion-transformer-dit-by-hand-︎-1e4d84ec865d)
    ✍ 探索 Sora 最新视频背后的秘密。'
- en: '[Deep Dive into Transformers by Hand](https://medium.com/towards-data-science/deep-dive-into-transformers-by-hand-︎-68b8be4bd813)
    ✍ that explores the power behind the power of transformers.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[手动深入探讨 Transformers](https://medium.com/towards-data-science/deep-dive-into-transformers-by-hand-︎-68b8be4bd813)
    ✍ 探索 transformers 强大功能背后的力量。'
- en: '**References:**'
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**参考文献：**'
- en: Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan
    N. Gomez, Łukasz Kaiser, and Illia Polosukhin. “[Attention is all you need.](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)”
    *Advances in neural information processing systems* 30 (2017).
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan
    N. Gomez, Łukasz Kaiser 和 Illia Polosukhin. “[注意力即一切](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)”。*神经信息处理系统进展*
    30 (2017)。
- en: Bahdanau, Dzmitry, Kyunghyun Cho and Yoshua Bengio. “[Neural Machine Translation
    by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473).”
    *CoRR* abs/1409.0473 (2014).
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bahdanau, Dzmitry, Kyunghyun Cho 和 Yoshua Bengio. “[神经机器翻译：通过联合学习对齐和翻译](https://arxiv.org/abs/1409.0473)”。*CoRR*
    abs/1409.0473 (2014)。
