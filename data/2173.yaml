- en: An Intuitive Introduction to Reinforcement Learning, Part I
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习的直观介绍，第一部分
- en: 原文：[https://towardsdatascience.com/an-intuitive-introduction-to-reinforcement-learning-part-i-d81512f5e25c?source=collection_archive---------2-----------------------#2024-09-06](https://towardsdatascience.com/an-intuitive-introduction-to-reinforcement-learning-part-i-d81512f5e25c?source=collection_archive---------2-----------------------#2024-09-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/an-intuitive-introduction-to-reinforcement-learning-part-i-d81512f5e25c?source=collection_archive---------2-----------------------#2024-09-06](https://towardsdatascience.com/an-intuitive-introduction-to-reinforcement-learning-part-i-d81512f5e25c?source=collection_archive---------2-----------------------#2024-09-06)
- en: Exploring popular reinforcement learning environments, in a beginner-friendly
    way
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 以适合初学者的方式探索流行的强化学习环境
- en: '[](https://medium.com/@x-jesse?source=post_page---byline--d81512f5e25c--------------------------------)[![Jesse
    Xia](../Images/a87eeff33bf3d2e8baef1c05c265490c.png)](https://medium.com/@x-jesse?source=post_page---byline--d81512f5e25c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d81512f5e25c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d81512f5e25c--------------------------------)
    [Jesse Xia](https://medium.com/@x-jesse?source=post_page---byline--d81512f5e25c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@x-jesse?source=post_page---byline--d81512f5e25c--------------------------------)[![Jesse
    Xia](../Images/a87eeff33bf3d2e8baef1c05c265490c.png)](https://medium.com/@x-jesse?source=post_page---byline--d81512f5e25c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d81512f5e25c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d81512f5e25c--------------------------------)
    [Jesse Xia](https://medium.com/@x-jesse?source=post_page---byline--d81512f5e25c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d81512f5e25c--------------------------------)
    ·16 min read·Sep 6, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d81512f5e25c--------------------------------)
    ·阅读时长 16 分钟·2024年9月6日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: This is a guided series on introductory reinforcement learning concepts using
    the environments from the OpenAI Gymnasium Python package. This first article
    will cover the high-level concepts necessary to understand and implement Q-learning
    to solve the “Frozen Lake” environment.
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这是一个关于强化学习概念的系列教程，使用 OpenAI Gymnasium Python 包中的环境进行演示。本文将涵盖理解并实现 Q 学习以解决“Frozen
    Lake”环境所需的高阶概念。
- en: ''
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Happy learning ❤ !
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 祝学习愉快 ❤ ！
- en: '![](../Images/f5b2f2a7a57da627a57c7829545417ea.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f5b2f2a7a57da627a57c7829545417ea.png)'
- en: A smiley lake (Image taken by author, made using OpenAI Gymnasium’s Frozen Lake
    environment)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一片微笑的湖（图片由作者拍摄，使用 OpenAI Gymnasium 的 Frozen Lake 环境制作）
- en: Let’s explore reinforcement learning by comparing it to familiar examples from
    everyday life.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过将强化学习与日常生活中熟悉的例子进行对比，来探索这一领域。
- en: '**Card Game** — Imagine playing a card game: When you first learn the game,
    the rules may be unclear. The cards you play might not be the most optimal and
    the strategies you use might be imperfect. As you play more and maybe win a few
    games, you learn what cards to play when and what strategies are better than others.
    Sometimes it’s better to bluff, but other times you should probably fold; saving
    a wild card for later use might be better than playing it immediately. Knowing
    what the optimal course of action is *learned* through a combination of ***experience***
    and ***reward***. Your experience comes from playing the game and you get rewarded
    when your strategies work well, perhaps leading to a victory or new high score.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**纸牌游戏** — 想象你在玩一场纸牌游戏：当你刚学会这款游戏时，规则可能不清楚。你打出的牌可能不是最优的，使用的策略也可能不完美。随着你玩得更多，也许赢了几局，你会学到什么时候打什么牌，哪些策略比其他策略更好。有时，虚张声势可能更好，但其他时候你应该弃牌；将一张万能牌留到以后使用可能比立即打出它更好。通过一系列的***经验***和***奖励***，你能学到最佳的行动方案。你的经验来自于玩游戏，而当你的策略奏效时，你会得到奖励，也许这能带来胜利或新的高分。'
- en: '![](../Images/81a5728f83dc5e14862fa7f9643a8329.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81a5728f83dc5e14862fa7f9643a8329.png)'
- en: A game of solitaire (Image taken by author from Google’s solitaire game)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一局纸牌游戏（图片由作者从 Google 的纸牌游戏中截图）
- en: '**Classical Conditioning —** By ringing a bell before he fed a dog, [Ivan Pavlov](https://en.wikipedia.org/wiki/Ivan_Pavlov)
    demonstrated the connection between external stimulus and a physiological response.
    The dog was conditioned to associate the sound of the bell with being fed and
    thus began to drool at the sound of the bell, even when no food was present. Though
    not strictly an example of reinforcement learning, through repeated ***experiences***
    where the dog was ***rewarded*** with food at the sound of the bell, it still
    learned to associate the two together.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**经典条件作用 —** 通过在喂狗之前按铃，[伊凡·巴甫洛夫](https://en.wikipedia.org/wiki/Ivan_Pavlov)展示了外部刺激与生理反应之间的联系。狗被条件化为将铃声与食物联系起来，因此它在听到铃声时开始流口水，即使没有食物存在。虽然这严格来说不是强化学习的例子，但通过反复的***经验***，狗在听到铃声时被***奖励***食物，最终学会了将二者联系起来。'
- en: '**Feedback Control —** An application of [control theory](https://en.wikipedia.org/wiki/Control_theory)
    found in engineering disciplines where a system’s behaviour can be adjusted by
    providing *feedback* to a controller. As a subset of feedback control, reinforcement
    learning requires feedback from our current environment to influence our actions.
    By providing feedback in the form of ***reward***, we can incentivize our agent
    to pick the optimal course of action.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**反馈控制 —** 一种在工程学科中应用的[控制理论](https://en.wikipedia.org/wiki/Control_theory)，该理论认为系统的行为可以通过向控制器提供*反馈*来进行调整。作为反馈控制的一个子集，强化学习需要来自当前环境的反馈来影响我们的行动。通过提供形式为***奖励***的反馈，我们可以激励代理选择最优的行动方案。'
- en: The Agent, State, and Environment
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代理、状态与环境
- en: '**Reinforcement learning is a learning process built on the accumulation of
    past experiences coupled with quantifiable reward.** In each example, we illustrate
    how our experiences can influence our actions and how *reinforcing* a positive
    association between reward and response could potentially be used to solve certain
    problems. If we can learn to associate reward with an optimal action, we could
    derive an algorithm that will select actions that *yield the highest probable
    reward*.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习是一个基于过去经验积累与可量化奖励的学习过程。** 在每个例子中，我们展示了我们的经验如何影响我们的行动，以及如何通过*强化*奖励与反应之间的正向联系来解决某些问题。如果我们能学会将奖励与最优行动联系起来，我们就能推导出一种算法，选择那些*带来最高可能奖励*的行动。'
- en: In reinforcement learning, the “learner” is called the ***agent***. The agent
    interacts with our environment and, through its actions, learns what is considered
    “good” or “bad” based on the reward it receives.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，“学习者”被称为***代理***。代理与环境进行交互，通过其行动，根据所收到的奖励来学习什么是“好的”或“坏的”。
- en: '![](../Images/ed2e3e07453b86fc450c6af054eaf937.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed2e3e07453b86fc450c6af054eaf937.png)'
- en: 'The feedback cycle in reinforcement learning: Agent -> Action -> Environment
    -> Reward, State (Image by author)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习中的反馈循环：代理 -> 行动 -> 环境 -> 奖励，状态（图片来自作者）
- en: To select a course of action, our agent needs some information about our environment,
    given by the ***state***. The state represents current information about the environment,
    such as position, velocity, time, etc. Our agent does not necessarily know the
    entirety of the current state. The information available to our agent at any given
    point in time is referred to as an *observation*, which contains some subset of
    information present in the state. Not all states are fully observable, and some
    states may require the agent to proceed knowing only a small fraction of what
    might actually be happening in the environment. Using the observation, our agent
    must infer what the best possible action might be based on learned experience
    and attempt to select the action that yields the highest expected reward.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了选择一个行动方案，我们的代理需要一些关于环境的信息，这些信息由***状态***提供。状态代表了关于环境的当前信息，如位置、速度、时间等。我们的代理并不一定知道当前状态的全部信息。代理在任何给定时间点可获取的信息称为*观察值*，它包含了状态中某些子集的信息。并非所有状态都是完全可观察的，有些状态可能要求代理只知道环境中发生的事情的一小部分。利用观察值，我们的代理必须根据学习到的经验推测出可能的最佳行动，并尝试选择能够带来最高预期奖励的行动。
- en: After selecting an action, the environment will then respond by providing feedback
    in the form of an updated state and reward. This reward will help us determine
    if the action the agent took was optimal or not.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择了一个行动后，环境将通过提供更新后的状态和奖励来作出反馈。这个奖励将帮助我们判断代理所采取的行动是否是最优的。
- en: Markov Decision Processes (MDPs)
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程（MDP）
- en: To better represent this problem, we might consider it as a **Markov decision
    process (MDP)**. A MDP is a [directed graph](https://en.wikipedia.org/wiki/Directed_graph)
    where each edge in the graph has a non-deterministic property. At each possible
    state in our graph, we have a set of actions we can choose from, with each action
    yielding some fixed reward and having some transitional probability of leading
    to some subsequent state. This means that the same actions are not guaranteed
    to lead to the same state every time since the transition from one state to another
    is not only dependent on the action, but the transitional probability as well.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地表示这个问题，我们可以将其视为**马尔可夫决策过程（MDP）**。MDP是一个[有向图](https://en.wikipedia.org/wiki/Directed_graph)，其中图中的每条边都有非确定性的属性。在图中的每个可能状态下，我们都有一组可以选择的动作，每个动作都会带来一定的固定回报，并且有一定的转移概率会导致某个后续状态。这意味着，相同的动作每次未必会导致相同的状态，因为从一个状态到另一个状态的转移不仅依赖于动作，还依赖于转移概率。
- en: '![](../Images/08a866a7b383ab3040cf12b65b226f13.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08a866a7b383ab3040cf12b65b226f13.png)'
- en: Representation of a Markov decision process (Image by author)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程的表示（图片来源：作者）
- en: Randomness in decision models is useful in practical RL, allowing for dynamic
    environments where the agent lacks full control. Turn-based games like chess require
    the opponent to make a move before you can go again. If the opponent plays randomly,
    the future state of the board is never guaranteed, and our agent must play while
    accounting for a multitude of different probable future states. When the agent
    takes some action, the next state is dependent on what the opponent plays and
    is therefore defined by a probability distribution across possible moves for the
    opponent.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 决策模型中的随机性在实际强化学习中是非常有用的，它允许动态环境，其中智能体无法完全控制。像棋类这样的回合制游戏要求对手先走一步，才能轮到你行动。如果对手随机出招，那么棋盘的未来状态是无法保证的，我们的智能体必须在考虑多个可能的未来状态的同时进行决策。当智能体采取某个动作时，下一状态取决于对手的走法，因此由对手可能的走法的概率分布来定义。
- en: '![](../Images/862a9422ef75f3ed353ac794d0c7a3af.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/862a9422ef75f3ed353ac794d0c7a3af.png)'
- en: Animation showcasing that the state of the chess board is also dependent on
    what moves the opponent chooses to play (Image by author)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 动画展示了棋盘的状态也依赖于对手选择的走法（图片来源：作者）
- en: Our future state is therefore a function of both the probability of the agent
    selecting some action and the *transitional probability* of the opponent selecting
    some action. In general, we can assume that for any environment, the probability
    of our agent moving to some subsequent state from our current state is denoted
    by the joint probability of the agent selecting some action and the transitional
    probability of moving to that state.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的未来状态因此是智能体选择某个动作的概率与对手选择某个动作的*转移概率*的函数。一般来说，我们可以假设，对于任何环境，从当前状态到后续状态的智能体转移概率由智能体选择某个动作的联合概率和转移到该状态的转移概率表示。
- en: Solving the MDP
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 求解MDP
- en: 'To determine the optimal course of action, we want to provide our agent with
    lots of experience. Through repeated iterations of our environment, we aim to
    give the agent enough feedback that it can correctly choose the optimal action
    most, if not all, of the time. Recall our definition of reinforcement learning:
    **a learning process built on the accumulation of past experiences coupled with
    quantifiable reward.** After accumulating some experience, we want to use this
    experience to better select our future actions.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定最佳行动路径，我们希望为智能体提供大量的经验。通过环境的多次迭代，我们的目标是为智能体提供足够的反馈，使其能够正确地选择最佳行动，尽可能多地选择最佳行动。回想一下我们对强化学习的定义：**一种建立在过去经验积累基础上，并伴随可量化回报的学习过程。**
    在积累了一些经验后，我们希望利用这些经验来更好地选择未来的行动。
- en: We can quantify our experiences by using them to predict the expected reward
    from future states. As we accumulate more experience, our predictions will become
    more accurate, converging to the true value after a certain number of iterations.
    For each reward that we receive, we can use that to update some information about
    our state, so the next time we encounter this state, we’ll have a better estimate
    of the reward that we might expect to receive.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过利用经验来预测未来状态的预期回报，从而量化我们的经验。随着我们积累更多的经验，我们的预测将变得更加准确，并在经过一定次数的迭代后收敛到真实值。对于我们收到的每一个回报，我们都可以用它来更新我们关于当前状态的一些信息，这样下次遇到该状态时，我们就能更好地估计我们可能会收到的回报。
- en: Frozen Lake Problem
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 冰湖问题
- en: 'Let’s consider consider a simple environment where our agent is a small character
    trying to navigate across a frozen lake, represented as a 2D grid. It can move
    in four directions: down, up, left, or right. Our goal is to teach it to move
    from its start position at the top left to an end position located at the bottom
    right of the map while avoiding the holes in the ice. If our agent manages to
    successfully reach its destination, we’ll give it a reward of +1\. For all other
    cases, the agent will receive a reward of 0, with the added condition that if
    it falls into a hole, the exploration will immediately terminate.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个简单的环境，其中我们的智能体是一个小角色，试图穿越一片冰冻的湖面，表示为一个二维网格。它可以朝四个方向移动：向下、向上、向左或向右。我们的目标是教它从左上角的起始位置移动到地图右下角的结束位置，同时避免冰面上的洞。如果我们的智能体成功到达目的地，我们会给它奖励
    +1。对于所有其他情况，智能体将获得 0 奖励，并且如果它掉进一个洞中，探索将立即终止。
- en: '![](../Images/9324e24bbf8ed38b3e0c708230efb6f8.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9324e24bbf8ed38b3e0c708230efb6f8.png)'
- en: Frozen lake animation (Image from [OpenAI Gymnasium frozen lake documentation](https://gymnasium.farama.org/environments/toy_text/frozen_lake/))
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 冰湖动画（图片来自[OpenAI Gymnasium 冰湖文档](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)）
- en: Each state can be denoted by its coordinate position in the grid, with the start
    position in the top left denoted as the origin (0, 0), and the bottom right ending
    position denoted as (3, 3).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 每个状态可以通过它在网格中的坐标位置来表示，起始位置位于左上角，表示为原点 (0, 0)，右下角的结束位置表示为 (3, 3)。
- en: The most generic solution would be to apply some pathfinding algorithm to find
    the shortest path to from top left to bottom right while avoiding holes in the
    ice. However, the probability that the agent can move from one state to another
    is not deterministic. **Each time the agent tries to move, there is a 66% chance
    that it will “slip” and move to a random adjacent state.** In other words, there
    is only a 33% chance of the action the agent chose actually occurring. A traditional
    pathfinding algorithm cannot handle the introduction of a transitional probability.
    Therefore, we need an algorithm that can handle stochastic environments, aka reinforcement
    learning.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最通用的解决方案是应用一些路径寻找算法，以找到从左上角到右下角的最短路径，同时避开冰面上的洞。然而，智能体从一个状态移动到另一个状态的概率并不是确定性的。**每次智能体尝试移动时，它有
    66% 的机会“滑倒”，并移动到一个随机的相邻状态。**换句话说，智能体选择的行动只有 33% 的机会会真正发生。传统的路径寻找算法无法处理引入转移概率的情况。因此，我们需要一个能够处理随机环境的算法，也就是强化学习。
- en: This problem can easily be represented as a MDP, with each state in our grid
    having some transitional probability of moving to any adjacent state. To solve
    our MDP, we need to find the optimal course of action from any given state. Recall
    that if we can find a way to accurately predict the future rewards from each state,
    we can greedily choose the best possible path by selecting whichever state yields
    the *highest expected reward*. We will refer to this predicted reward as the ***state-value.***
    More formally, the state-value will define the expected reward gained starting
    from some state plus an estimate of the expected rewards from all future states
    thereafter, assuming we act according to the same policy of choosing the highest
    expected reward. Initially, our agent will have no knowledge of what rewards to
    expect, so this estimate can be arbitrarily set to 0.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题可以很容易地表示为一个马尔科夫决策过程（MDP），其中我们网格中的每个状态都有一些转移概率，可能移动到任何相邻的状态。为了求解我们的MDP，我们需要从任何给定状态中找到最优的行动路径。回想一下，如果我们能找到一种方法，准确预测每个状态的未来奖励，我们就可以通过贪婪地选择**最高预期奖励**的状态来选择最佳路径。我们将把这个预测奖励称为***状态值***。更正式地，状态值将定义从某个状态开始获得的预期奖励，以及在此之后所有未来状态的预期奖励估计，假设我们始终按照选择最高预期奖励的策略行事。最初，我们的智能体并不知道预期得到什么奖励，因此这个估计可以任意设为0。
- en: 'Let’s now define a way for us to select actions for our agent to take: We’ll
    begin with a table to store our predicted state-value estimates for each state,
    containing all zeros.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们定义一种方法，供我们的智能体选择行动：我们将首先用一个表格来存储我们对每个状态的预测状态值估计，表格初始时全部为零。
- en: '![](../Images/7aadcb5dd5a6661d46ab11d2ff53a477.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7aadcb5dd5a6661d46ab11d2ff53a477.png)'
- en: Table denoting the estimated state-value for each state in our grid (Image by
    author)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 表示我们网格中每个状态的估计状态值的表格（图片作者提供）
- en: Our goal is to update these state-value estimates as we explore our environment.
    The more we traverse our environment, the more experience we will have, and the
    better our estimates will become. As our estimates improve, our state-values will
    become more accurate, and we will have a better representation of which states
    yield a higher reward, therefore allowing us to select actions based on which
    subsequent state has the highest state-value. This will surely work, right?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是随着我们探索环境来更新这些状态价值的估算。我们越是遍历环境，积累的经验就越多，估算也会变得更加精确。随着估算的改进，我们的状态价值将变得更加准确，并且我们将更好地表示哪些状态会带来更高的奖励，从而使我们能够根据哪个后续状态具有最高的状态价值来选择行动。这一定会奏效，对吧？
- en: '![](../Images/24245c342efc1c230f26805ac77c6eaa.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/24245c342efc1c230f26805ac77c6eaa.png)'
- en: Visual representation of a single branch of our MDP (Image by author)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的MDP单一分支的可视化表示（图片来自作者）
- en: State-value vs. Action-value
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 状态价值与行动价值
- en: Nope, sorry. One immediate problem that you might notice is that simply selecting
    the next state based on the highest possible state-value isn’t going to work.
    When we look at the set of possible next states, we aren’t considering our current
    action—that is, the action that we will take from our current state to get to
    the next one. Based on our definition of reinforcement learning, the agent-environment
    feedback loop always consists of the agent taking some action and the environment
    responding with both state and reward. If we only look at the state-values for
    possible next states, we are considering the reward that we would receive starting
    from those states, which completely ignores the action (and consequent reward)
    we took to get there. Additionally, trying to select a maximum across the next
    possible states assumes we can even make it there in the first place. Sometimes,
    being a little more conservative will help us be more consistent in reaching the
    end goal; however, this is out of the scope of this article :(.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 不，抱歉。你可能会注意到的一个直接问题是，单纯根据最高状态价值来选择下一个状态并不可行。当我们查看可能的下一个状态集合时，并没有考虑当前的行动——也就是说，我们从当前状态到达下一个状态时所采取的行动。根据我们对强化学习的定义，代理-环境反馈循环总是包括代理采取某个行动，环境则通过状态和奖励来做出回应。如果我们只看下一个状态的状态价值，我们实际上是在考虑从这些状态开始时我们将获得的奖励，这完全忽视了我们为到达这些状态所采取的行动（及其带来的奖励）。此外，试图选择下一个可能状态中的最大值还假设我们首先能够到达那里。有时候，更为保守一点可以帮助我们更一致地实现最终目标；不过，这超出了本文的讨论范围
    :（。
- en: Instead of evaluating across the set of possible next states, we’d like to directly
    evaluate our available actions. If our previous state-value function consisted
    of the expected rewards starting from the next state, we’d like to update this
    function to now include the reward from taking an action from the current state
    to get to the next state, plus the expected rewards from there on. We’ll call
    this new estimate that includes our current action ***action-value.***
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望直接评估我们可用的行动，而不是评估可能的下一个状态集合。如果我们之前的状态价值函数是基于下一个状态的预期奖励，那么我们希望更新这个函数，现在要将从当前状态采取一个行动以到达下一个状态的奖励，以及从那里开始的预期奖励，包含在内。我们将这个新的估算称为***行动价值（action-value）***。
- en: We can now formally define our state-value and action-value functions based
    on rewards and transitional probability. We’ll use [expected value](https://www.statisticshowto.com/probability-and-statistics/expected-value/)
    to represent the relationship between reward and transitional probability. We’ll
    denote our state-value as *V* and our action-value as *Q*, based on standard conventions
    in RL literature.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以根据奖励和转移概率正式定义我们的状态价值和行动价值函数。我们将使用[期望值](https://www.statisticshowto.com/probability-and-statistics/expected-value/)来表示奖励和转移概率之间的关系。我们将根据强化学习文献中的标准惯例，分别将状态价值表示为*V*，将行动价值表示为*Q*。
- en: '![](../Images/71d8fa1489f5ddbcb51e32343d03f9f0.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/71d8fa1489f5ddbcb51e32343d03f9f0.png)'
- en: Equations for state- and action-value (Image by author)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 状态价值和行动价值的方程式（图片来自作者）
- en: The state-value V of some state s[t] is the expected sum of rewards r[t] at
    each state starting from s[t] to some future state s[T]; the action-value Q of
    some state s[t] is the expected sum of rewards r[t] at each state starting by
    taking an action a[t] to some future state-action pair s[T], a[T].
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 某状态s[t]的状态价值V是从s[t]开始到未来某状态s[T]的每个状态的预期奖励r[t]的总和；某状态s[t]的行动价值Q是从采取某个行动a[t]到达未来某状态-行动对s[T]，a[T]的每个状态的预期奖励r[t]的总和。
- en: 'This definition is actually not the most accurate or conventional, and we’ll
    improve on it later. However, it serves as a general idea of what we’re looking
    for: a quantitative measure of future rewards.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定义实际上不是最准确或最常规的，稍后我们会对其进行改进。然而，它提供了我们所寻求的一个基本思路：对未来奖励的量化度量。
- en: Our state-value function *V* is an estimate of the maximum sum of rewards *r*
    we would obtain starting from state *s* and continually moving to the states that
    give the highest reward. Our action-value function is an estimate of the maximum
    reward we would obtain by taking action from some starting state and continually
    choosing the optimal actions that yield the highest reward thereafter. In both
    cases, we choose the optimal action/state to move to based on the expected reward
    that we would receive and loop this process until we either fall into a hole or
    reach our goal.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的状态值函数 *V* 是从状态 *s* 开始，持续移动到提供最高奖励的状态时，最大奖励总和 *r* 的估计值。我们的动作值函数是通过从某一初始状态采取动作，并持续选择之后提供最高奖励的最优动作，所获得的最大奖励的估计值。在这两种情况下，我们根据预计的奖励选择最优的动作/状态，并不断循环这一过程，直到我们陷入困境或达到目标。
- en: Greedy Policy & Return
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 贪心策略与回报
- en: 'The method by which we choose our actions is called a ***policy***. The policy
    is a function of state—given some state, it will output an action. In this case,
    since we want to select the next action based on maximizing the rewards, our policy
    can be defined as a function returning the action that yields the maximum action-value
    (Q-value) starting from our current state, or an [argmax](https://en.wikipedia.org/wiki/Arg_max#:~:text=In%20mathematics%2C%20the%20arguments%20of,is%20maximized%20and%20minimized%2C%20respectively.).
    Since we’re always selecting a maximum, we refer to this particular policy as
    *greedy*. We’ll denote our policy as a function of state s: π(s), formally defined
    as'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择动作的方法称为 ***策略***。策略是状态的一个函数——给定某一状态，它会输出一个动作。在这种情况下，由于我们希望根据最大化奖励来选择下一个动作，我们的策略可以定义为一个函数，返回从当前状态开始，得到最大动作值（Q值）的动作，或
    [argmax](https://en.wikipedia.org/wiki/Arg_max#:~:text=In%20mathematics%2C%20the%20arguments%20of,is%20maximized%20and%20minimized%2C%20respectively.)。由于我们始终选择最大值，我们将这种特定的策略称为
    *贪心* 策略。我们将我们的策略表示为状态 s 的函数：π(s)，其正式定义如下：
- en: '![](../Images/27e57dd508e4666977ced2945f05ea70.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/27e57dd508e4666977ced2945f05ea70.png)'
- en: Equation for the policy function = the action that yields the maximum estimated
    Q-value from some state s (Image by author)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 策略函数的方程 = 从某一状态 s 得到的最大估计 Q 值所对应的动作（图片来源：作者）
- en: To simplify our notation, we can also define a substitution for our sum of rewards,
    which we’ll call ***return,*** and a substitution for a sequence of states and
    actions, which we’ll call a ***trajectory***. A trajectory, denoted by the Greek
    letter τ (tau), is denoted as
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化我们的符号表示，我们还可以定义一个奖励总和的替代项，称为 ***回报***，以及一个状态和动作序列的替代项，称为 ***轨迹***。轨迹，用希腊字母τ（tau）表示，定义如下：
- en: '![](../Images/686a7c274c1f31c017a4df0f3499d98d.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/686a7c274c1f31c017a4df0f3499d98d.png)'
- en: 'Notation for trajectory: defined as some sequence of state-action pairs until
    some future timestep T. Defining the trajectory allows us to skip writing the
    entire sequence of states and actions, and substitute a single variable instead
    :P! (Image by author)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 轨迹的表示法：定义为某一状态-动作对的序列，直到某个未来的时间步T。定义轨迹使我们可以跳过写出整个状态和动作的序列，而用一个单一的变量代替 :P！（图片来源：作者）
- en: 'Since our environment is stochastic, it’s important to also consider the likelihood
    of such a trajectory occurring — low probability trajectories will reduce the
    expectation of reward. (Since our [expected value](https://en.wikipedia.org/wiki/Expected_value)
    consists of multiplying our reward by the transitional probability, trajectories
    that are less likely will have a lower expected reward compared to high probability
    ones.) The probability can be derived by considering the probability of each action
    and state happening incrementally: At any timestep in our MDP, we will select
    actions based on our policy, and the resulting state will be dependent on both
    the action we selected and the transitional probability. Without loss of generality,
    we’ll denote the transitional probability as a separate probability distribution,
    a function of both the current state and the attempted action. The conditional
    probability of some future state occurring is therefore defined as'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的环境是随机的，因此同样需要考虑轨迹发生的可能性——低概率的轨迹会降低奖励的期望值。（由于我们的[期望值](https://en.wikipedia.org/wiki/Expected_value)是通过将奖励与转移概率相乘得到的，因此低概率的轨迹与高概率的轨迹相比，期望奖励会较低。）这个概率可以通过逐步考虑每个动作和状态发生的概率来推导：在我们的马尔可夫决策过程中（MDP）的每个时间步中，我们将根据策略选择动作，而
    resulting 状态将取决于我们选择的动作和转移概率。为了简化，我们将转移概率表示为一个独立的概率分布，它是当前状态和所选动作的函数。因此，某一未来状态发生的条件概率定义为：
- en: '![](../Images/3ec868afc74700730be6d3fed573142d.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ec868afc74700730be6d3fed573142d.png)'
- en: Transitional probability of moving to a future state from a current state —
    for our frozen lake though, we know this value is fixed at ~0.33 (Image by author)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 从当前状态转移到未来状态的转移概率——对于我们的冰湖问题，我们知道这个值固定在 ~0.33（图示来自作者）
- en: And the probability of some action happening based on our policy is simply evaluated
    by passing our state into our policy function
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们的策略，某个动作发生的概率仅仅通过将我们的状态传入我们的策略函数来评估。
- en: '![](../Images/4bd02d3fd4af429c18e4e17e5041e3aa.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4bd02d3fd4af429c18e4e17e5041e3aa.png)'
- en: Expression for the probability of some action being selected by the policy given
    some state (Image by author)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 某个动作被策略选择的概率表达式，给定某个状态（图示来自作者）
- en: Our policy is currently deterministic, as it selects actions based on the highest
    expected action-value. In other words, actions that have a low action-value will
    never be selected, while actions with a high Q-value will always be selected.
    This results in a [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution)
    across possible actions. This is very rarely beneficial, as we’ll see later.
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们的策略目前是确定性的，因为它根据最高的期望动作值选择动作。换句话说，具有低动作值的动作永远不会被选择，而具有高Q值的动作将始终被选择。这导致了一个[伯努利分布](https://en.wikipedia.org/wiki/Bernoulli_distribution)在所有可能的动作中。这种策略很少是有利的，正如我们稍后会看到的。
- en: Applying these expressions to our trajectory, we can define the probability
    of some trajectory occurring as
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些表达式应用到我们的轨迹中，我们可以定义某个轨迹发生的概率为：
- en: '![](../Images/efe4817d6f7d5469f2fe7817f7b99699.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/efe4817d6f7d5469f2fe7817f7b99699.png)'
- en: Expanded equation for the probability of a certain trajectory occurring. Note
    that the probability of s0 is fixed at 1 assuming we start from the same state
    (top left) every time. (Image by author)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 发生某个特定轨迹的扩展方程式。请注意，假设每次从相同状态（左上角）开始，s0 的概率固定为 1。（图示来自作者）
- en: 'For clarity, here’s the original notation for a trajectory:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清晰地说明，以下是轨迹的原始符号表示：
- en: '![](../Images/686a7c274c1f31c017a4df0f3499d98d.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/686a7c274c1f31c017a4df0f3499d98d.png)'
- en: 'Notation for trajectory: defined as some sequence of state-action pairs until
    some future timestep T (Image by author)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 轨迹的符号表示：定义为一些状态-动作对的序列，直到某个未来时间步T（图示来自作者）
- en: More concisely, we have
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 更简洁地，我们可以写为：
- en: '![](../Images/102aac00b1bfd5c5ef917f05ec83a953.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/102aac00b1bfd5c5ef917f05ec83a953.png)'
- en: Concise notation for the probability of a trajectory occurring (Image by author)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 轨迹发生的概率的简洁符号表示（图示来自作者）
- en: Defining both the trajectory and its probability allows us to substitute these
    expressions to simplify our definitions for both return and its expected value.
    The return (sum of rewards), which we’ll define as *G* based on conventions, can
    now be written as
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了轨迹及其概率之后，我们可以替换这些表达式来简化我们对回报及其期望值的定义。回报（奖励的总和），我们将根据惯例定义为*G*，现在可以表示为：
- en: '![](../Images/734777314730b1fbccce78d189eb9378.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/734777314730b1fbccce78d189eb9378.png)'
- en: Equation for return (Image by author)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 回报的方程式（图示来自作者）
- en: We can also define the expected return by introducing probability into the equation.
    Since we’ve already defined the probability of a trajectory, the expected return
    is therefore
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过引入概率来定义预期回报。既然我们已经定义了轨迹发生的概率，那么预期回报就是
- en: '![](../Images/aeef9f57bf02543cac9034014c39d9bb.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aeef9f57bf02543cac9034014c39d9bb.png)'
- en: Updated equation for expected return = the probability of the trajectory occurring
    times the return (Image by author)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 更新后的预期回报方程 = 轨迹发生的概率乘以回报（图片来自作者）
- en: We can now adjust the definition of our value functions to include the expected
    return
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以调整价值函数的定义，以包括预期回报。
- en: '![](../Images/548104584e6ecbdfa2ef6a3a0c6cbe6c.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/548104584e6ecbdfa2ef6a3a0c6cbe6c.png)'
- en: Updated equations for state- and action-value (Image by author)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 更新后的状态值和行为值方程（图片来自作者）
- en: 'The main difference here is the addition of the subscript τ∼π indicating that
    our trajectory was sampled by following our policy (ie. our actions are selected
    based on the maximum Q-value). We’ve also removed the subscript *t* for clarity.
    Here’s the previous equation again for reference:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的主要区别是添加了下标 τ∼π，表示我们的轨迹是通过遵循策略采样得到的（即，我们的行为是基于最大 Q 值来选择的）。我们还去除了下标*t*以便于清晰。这里再次给出之前的方程，供参考：
- en: '![](../Images/71d8fa1489f5ddbcb51e32343d03f9f0.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/71d8fa1489f5ddbcb51e32343d03f9f0.png)'
- en: Equations for state- and action-value (Image by author)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 状态值和行为值方程（图片来自作者）
- en: Discounted Return
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 折扣回报
- en: So now we have a fairly well-defined expression for estimating return but before
    we can start iterating through our environment, there’s still some more things
    to consider. In our frozen lake, it’s fairly unlikely that our agent will continue
    to explore indefinitely. At some point, it will slip and fall into a hole, and
    the episode will terminate. However, in practice, RL environments might not have
    clearly defined endpoints, and training sessions might continue indefinitely.
    In these situations, given an indefinite amount of time, the expected return would
    approach infinity, and evaluating the state- and action-value would become impossible.
    Even in our case, setting a hard limit for computing return is oftentimes not
    beneficial, and if we set the limit too high, we could end up with pretty absurdly
    large numbers anyway. In these situations, it is important to ensure that our
    reward series will converge using a *discount factor*. This improves stability
    in the training process and ensures that our return will always be a finite value
    regardless of how far into the future we look. This type of discounted return
    is also referred to as *infinite horizon discounted return.*
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在我们有了一个相对明确的回报估计表达式，但在我们开始在环境中迭代之前，仍然有一些问题需要考虑。在我们的冰湖环境中，代理人不太可能无限期地继续探索。总有一天，它会滑倒并掉进一个洞里，导致一局游戏结束。然而，在实际的强化学习环境中，可能没有明确的结束点，训练会持续进行。在这种情况下，假设时间是无限的，预期回报将趋向于无限大，评估状态值和行为值将变得不可能。即使在我们的情况下，为计算回报设定硬性限制通常也没有好处，如果我们设定的限制过高，最终我们得到的回报值也可能是非常大的数字。在这些情况下，确保我们的奖励序列会收敛是很重要的，这可以通过使用*折扣因子*来实现。这能提高训练过程的稳定性，并确保无论我们考虑多远的未来，回报总是有限的。这样的折扣回报也被称为*无限时域折扣回报*。
- en: To add discounting to our return equation, we’ll introduce a new variable γ
    (gamma) to represent the discount factor.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在我们的回报方程中加入折扣因素，我们将引入一个新的变量 γ（gamma）来表示折扣因子。
- en: '![](../Images/1ee75a0332ea8ac373c2f91b93cdd928.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ee75a0332ea8ac373c2f91b93cdd928.png)'
- en: Equation for discounted return (Image by author)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 折扣回报方程（图片来自作者）
- en: Gamma must always be less than 1, or our series will not converge. Expanding
    this expression makes this even more apparent
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: γ（gamma）必须始终小于 1，否则我们的序列将无法收敛。扩展这个表达式会使这一点更加明显。
- en: '![](../Images/c14a63896f5f2bb82b43fb4fb3614cd4.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c14a63896f5f2bb82b43fb4fb3614cd4.png)'
- en: Expanded equation for discounted return (Image by author)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 展开后的折扣回报方程（图片来自作者）
- en: We can see that as time increases, gamma will be raised to a higher and higher
    power. As gamma is less than 1, raising it to a higher exponent will only make
    it smaller, thus exponentially decreasing the contribution of future rewards to
    the overall sum. We can substitute this updated definition of return back into
    our value functions, though nothing will visibly change since the variable is
    still the same.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，随着时间的推移，gamma将被提高到更高的指数。由于gamma小于1，将其提高到更高的指数只会使其变得更小，从而使未来奖励对整体总和的贡献呈指数级减少。我们可以将这种更新后的回报定义代入我们的价值函数中，尽管由于变量仍然相同，所看到的结果不会发生明显变化。
- en: '![](../Images/548104584e6ecbdfa2ef6a3a0c6cbe6c.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/548104584e6ecbdfa2ef6a3a0c6cbe6c.png)'
- en: Equations for state- and action-value, copied again for emphasis (Image by author)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 状态和行动价值的公式，再次复制以便强调（图像来自作者）
- en: Exploration vs. Exploitation
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索与利用
- en: We mentioned earlier that always being greedy is not the best choice. Always
    selecting our actions based on the maximum Q-value will probably give us the highest
    chance of maximizing our reward, but that only holds when we have accurate estimates
    of those Q-values in the first place. To obtain accurate estimates, we need a
    lot of information, and we can only gain information by trying new things — that
    is, exploration.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到过，始终贪婪并不是最好的选择。始终基于最大Q值选择行动可能会给我们最大化奖励的最高机会，但这仅在我们最初就有准确的Q值估计时成立。为了获得准确的估计，我们需要大量的信息，而我们只能通过尝试新事物——即探索——来获得信息。
- en: 'When we select actions based on the highest estimated Q-value, we *exploit*
    our current knowledge base: we leverage our accumulated experiences in an attempt
    to maximize our reward. When we select actions based on any other metric, or even
    randomly, we *explore* alternative possibilities in an attempt to gain more useful
    information to update our Q-value estimates with. In reinforcement learning, we
    want to balance both *exploration* and *exploitation.* To properly exploit our
    knowledge, we need to have knowledge, and to gain knowledge, we need to explore.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们基于最高估计的Q值选择行动时，我们在*利用*当前的知识库：我们利用已经积累的经验，试图最大化奖励。当我们基于其他任何指标，甚至是随机选择行动时，我们在*探索*其他可能性，试图获得更多有用的信息来更新Q值估计。在强化学习中，我们希望平衡*探索*与*利用*。要正确地利用我们的知识，我们需要拥有知识，而要获得知识，我们必须进行探索。
- en: Epsilon-Greedy Policy
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Epsilon-贪婪策略
- en: We can balance exploration and exploitation by changing our policy from purely
    greedy to an *epsilon-greedy* one. An epsilon-greedy policy acts greedily most
    of the time with a probability of 1- ε, but has a probability of ε to act randomly.
    In other words, we’ll exploit our knowledge most of the time in an attempt to
    maximize reward, and we’ll explore occasionally to gain more knowledge. This is
    not the only way of balancing exploration and exploitation, but it is one of the
    simplest and easiest to implement.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将策略从纯贪婪策略转变为*epsilon-贪婪*策略来平衡探索和利用。epsilon-贪婪策略大部分时间都采取贪婪的行动，概率为1 - ε，但也有ε的概率进行随机行动。换句话说，我们大部分时间会利用我们的知识来尝试最大化奖励，并且偶尔进行探索以获得更多知识。这不是平衡探索与利用的唯一方法，但它是最简单且最容易实现的一种。
- en: Summary
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Now the we’ve established a basis for understanding RL principles, we can move
    to discussing the actual algorithm — which will happen in the next article. For
    now, we’ll go over the high-level overview, combining all these concepts into
    a cohesive pseudo-code which we can delve into next time.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经建立了理解强化学习（RL）原理的基础，我们可以继续讨论实际的算法——这将在下一篇文章中进行。目前，我们将概述一个高层次的概述，将所有这些概念结合成一个连贯的伪代码，下一次我们可以深入探讨。
- en: Q-Learning
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Q学习
- en: 'The focus of this article was to establish the basis for understanding and
    implementing Q-learning. Q-learning consists of the following steps:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的重点是建立理解和实现Q学习的基础。Q学习包含以下步骤：
- en: Initialize a tabular estimate of all action-values (Q-values), which we update
    as we iterate through our environment.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化所有行动价值（Q值）的表格估计，并在我们迭代环境时更新它们。
- en: Select an action by sampling from our epsilon-greedy policy.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过从我们的epsilon-贪婪策略中采样来选择一个行动。
- en: Collect the reward (if any) and update our estimate for our action-value.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集奖励（如果有的话），并更新我们对行动价值的估计。
- en: Move to the next state, or terminate if we fall into a hole or reach the goal.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移动到下一个状态，或者如果掉进坑里或到达目标，则终止。
- en: Loop steps 2–4 until our estimated Q-values converge.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤2–4，直到我们的Q值估计收敛。
- en: Q-learning is an iterative process where we build estimates of action-value
    (and expected return), or “experience”, and use our experiences to identify which
    actions are the most rewarding for us to choose. These experiences are “learned”
    over many successive iterations of our environment and by leveraging them we will
    be able to consistently reach our goal, thus solving our MDP.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习是一个迭代过程，我们构建动作值（和预期回报）的估计，或者说“经验”，并利用我们的经验来识别哪些动作最能带来回报。通过多次与环境的交互，这些经验被“学习”，借助这些经验，我们将能够持续达成目标，从而解决我们的MDP问题。
- en: Glossary
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词汇表
- en: '**Environment —** anything that cannot be arbitrarily changed by our agent,
    aka the world around it'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**环境 —** 智能体不能任意改变的任何事物，也就是它周围的世界'
- en: '**State —** a particular condition of the environment'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态 —** 环境的特定条件'
- en: '**Observation —** some subset of information from the state'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**观察 —** 状态的一部分信息'
- en: '**Policy —** a function that selects an action given a state'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略 —** 给定状态下选择一个动作的函数'
- en: '**Agent —** our “learner” which acts according to a policy in our environment'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**智能体 —** 我们的“学习者”，根据策略在环境中采取行动'
- en: '**Reward —** what our agent receives after performing certain actions'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励 —** 我们的智能体在执行某些动作后所获得的反馈'
- en: '**Return —** a sum of rewards across a series of actions'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回报 —** 一系列动作的总奖励'
- en: '**Discounting** — the process through which we ensure that our return does
    not reach infinity'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**折扣 —** 通过这一过程，我们确保回报不会趋向于无穷大'
- en: '**State-value —** the expected return starting from a state and continuing
    to act according to some policy, forever'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态值 —** 从某个状态开始并按照某个策略继续行动，最终得到的预期回报'
- en: '**Action-value —** the expected return starting from a state and taking some
    action, and then continuing to act according to some policy, forever'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动作值 —** 从某个状态开始并采取某个动作，然后继续根据某个策略行动，最终得到的预期回报'
- en: '**Trajectory —** a series of states and actions'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**轨迹 —** 一系列的状态和动作'
- en: '**Markov Decision Process (MDP) —** the model we use to represent decision
    problems in RL aka a directed graph with non-deterministic edges'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**马尔可夫决策过程（MDP）—** 我们用来表示强化学习决策问题的模型，也就是具有非确定性边的有向图'
- en: '**Exploration —** how we obtain more knowledge'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**探索 —** 我们如何获得更多知识'
- en: '**Exploitation —** how we use our existing knowledge base to gain more reward'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**利用 —** 我们如何使用现有的知识库来获得更多的奖励'
- en: '**Q-Learning** — a RL algorithm where we iteratively update Q-values to obtain
    better estimates of which actions will yield higher expected return'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Q学习 —** 一种强化学习算法，我们通过迭代更新Q值来获得更好的估计，预测哪些动作能带来更高的预期回报'
- en: '**Reinforcement Learning —** a learning process built on the accumulation of
    past experiences coupled with quantifiable reward'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**强化学习 —** 一种基于过去经验积累和可量化奖励的学习过程'
- en: If you’ve read this far, consider leaving some feedback about the article —
    I’d appreciate it ❤.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你读到这里，考虑给这篇文章留下反馈——我会很感激❤。
- en: References
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Gymnasium, [Frozen Lake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)
    (n.d.), OpenAI Gymnasium Documentation.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Gymnasium, [冰冻湖](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)（无日期），OpenAI
    Gymnasium 文档'
- en: '[2] OpenAI, [Spinning Up in Deep RL](https://spinningup.openai.com/en/latest/)
    (n.d.), OpenAI.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] OpenAI, [深度强化学习入门](https://spinningup.openai.com/en/latest/)（无日期），OpenAI'
- en: '[3] R. Sutton and A. Barto, [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/RLbook2020.pdf)
    (2020), [http://incompleteideas.net/book/RLbook2020.pdf](http://incompleteideas.net/book/RLbook2020.pdf)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] R. Sutton 和 A. Barto, [强化学习：入门](http://incompleteideas.net/book/RLbook2020.pdf)（2020），[http://incompleteideas.net/book/RLbook2020.pdf](http://incompleteideas.net/book/RLbook2020.pdf)'
- en: '[4] Spiceworks, [What is a Markov Decision Process?](https://www.spiceworks.com/tech/artificial-intelligence/articles/what-is-markov-decision-process/)
    (n.d.), Spiceworks'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Spiceworks, [什么是马尔可夫决策过程？](https://www.spiceworks.com/tech/artificial-intelligence/articles/what-is-markov-decision-process/)（无日期），Spiceworks'
- en: '[5] IBM, [Reinforcement Learning](https://www.ibm.com/topics/reinforcement-learning)
    (n.d.), IBM'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] IBM, [强化学习](https://www.ibm.com/topics/reinforcement-learning)（无日期），IBM'
