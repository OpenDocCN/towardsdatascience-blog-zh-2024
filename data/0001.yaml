- en: Fine-tune a Mistral-7b model with Direct Preference Optimization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调Mistral-7b模型与直接偏好优化
- en: 原文：[https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac?source=collection_archive---------0-----------------------#2024-01-01](https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac?source=collection_archive---------0-----------------------#2024-01-01)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac?source=collection_archive---------0-----------------------#2024-01-01](https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac?source=collection_archive---------0-----------------------#2024-01-01)
- en: Boost the performance of your supervised fine-tuned models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升你监督微调模型的表现
- en: '[](https://medium.com/@mlabonne?source=post_page---byline--708042745aac--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page---byline--708042745aac--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--708042745aac--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--708042745aac--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page---byline--708042745aac--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne?source=post_page---byline--708042745aac--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page---byline--708042745aac--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--708042745aac--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--708042745aac--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page---byline--708042745aac--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--708042745aac--------------------------------)
    ·10 min read·Jan 1, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--708042745aac--------------------------------)
    ·阅读时间10分钟·2024年1月1日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/3d74f51ec0cdd912262edbd229c2e620.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d74f51ec0cdd912262edbd229c2e620.png)'
- en: Image by author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'Pre-trained Large Language Models (LLMs) can only perform next-token prediction,
    making them unable to answer questions. This is why these base models are then
    fine-tuned on pairs of instructions and answers to act as helpful assistants.
    However, this process can still be flawed: fine-tuned LLMs can be biased, toxic,
    harmful, etc. This is where Reinforcement Learning from Human Feedback (RLHF)
    comes into play.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的大型语言模型（LLMs）只能执行下一个token的预测，这使得它们无法回答问题。这也是为什么这些基础模型随后会在指令和回答的配对上进行微调，以充当有用的助手。然而，这一过程仍然可能存在缺陷：微调后的LLM可能存在偏见、有害、毒性等问题。这时，来自人类反馈的强化学习（RLHF）便发挥了作用。
- en: RLHF provides different answers to the LLM, which are ranked according to a
    desired behavior (helpfulness, toxicity, etc.). The model learns to output the
    best answer among these candidates, hence mimicking the behavior we want to instill.
    Often seen as a way to censor models, this process has recently become popular
    for improving performance, as shown in [neural-chat-7b-v3–1](https://huggingface.co/Intel/neural-chat-7b-v3-1).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF为LLM提供不同的答案，并根据期望的行为（例如有用性、毒性等）对这些答案进行排序。模型学会在这些候选答案中输出最佳答案，从而模仿我们希望其表现的行为。这个过程通常被视为一种审查模型的方法，但最近它已成为一种改善性能的流行方式，如在[neural-chat-7b-v3–1](https://huggingface.co/Intel/neural-chat-7b-v3-1)中所示。
- en: 'In this article, we will create [NeuralHermes-2.5](https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B),
    by fine-tuning [OpenHermes-2.5](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B)
    using a RLHF-like technique: Direct Preference Optimization (DPO). For this purpose,
    we will introduce a preference dataset, describe how the DPO algorithm works,
    and apply it to our model. We’ll see that it significantly improves the performance
    of the base model on the Open LLM Leaderboard.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将通过使用类似于强化学习的技术——直接偏好优化（DPO）来微调[OpenHermes-2.5](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B)，从而创建[NeuralHermes-2.5](https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B)。为此，我们将引入一个偏好数据集，描述DPO算法的工作原理，并将其应用到我们的模型中。我们将看到，这显著提高了基础模型在开放LLM排行榜上的表现。
- en: As per usual, the code is available on [GitHub](https://github.com/mlabonne/llm-course/blob/main/Fine_tune_a_Mistral_7b_model_with_DPO.ipynb)
    and [Google Colab](https://colab.research.google.com/drive/15iFBr1xWgztXvhrj5I9fBv20c7CFOPBE?usp=sharing).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如常，代码可在[GitHub](https://github.com/mlabonne/llm-course/blob/main/Fine_tune_a_Mistral_7b_model_with_DPO.ipynb)和[Google
    Colab](https://colab.research.google.com/drive/15iFBr1xWgztXvhrj5I9fBv20c7CFOPBE?usp=sharing)上找到。
- en: '***Update****:* [*Jessie Davids*](https://www.linkedin.com/in/jesse-th-davids/)*,
    a reader who used this article and code, managed to create the best-performing
    model on the Open LLM Leaderboard ~7B param. Congrats to him! 🎉*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '***更新***：[*Jessie Davids*](https://www.linkedin.com/in/jesse-th-davids/)，一位使用本文及代码的读者，成功创建了在Open
    LLM排行榜上表现最好的模型，约7B参数。恭喜他！🎉'
- en: '![](../Images/23a4f24817da40f445ad29a63c66869d.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23a4f24817da40f445ad29a63c66869d.png)'
- en: Image by author
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: 🥇 Preference datasets
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🥇 偏好数据集
- en: 'Preference datasets are not standardized, but they typically consist of a collection
    of answers that are ranked by humans. This ranking is essential, as the RLHF process
    fine-tunes LLMs to output the preferred answer. Here is an example of [Anthropic/hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf/viewer/default/train),
    a popular preference dataset:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 偏好数据集没有标准化，但它们通常由一组经过人工排序的答案组成。这个排序非常关键，因为RLHF过程会微调LLM，使其输出优选答案。下面是一个常见的偏好数据集示例：[Anthropic/hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf/viewer/default/train)：
- en: '![](../Images/9930d8bacb694aecb9e9556e101932ff.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9930d8bacb694aecb9e9556e101932ff.png)'
- en: Image by author
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: 'The structure of the dataset is straightforward: for each row, there is one
    chosen (preferred) answer, and one rejected answer. The goal of RLHF is to guide
    the model to output the preferred answer.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的结构很简单：每一行都有一个选定的（优选的）答案和一个被拒绝的答案。RLHF的目标是引导模型输出优选的答案。
- en: Preference datasets are notoriously costly and difficult to make, as they require
    collecting manual feedback from humans. This feedback is also subjective and can
    easily be biased toward confident (but wrong) answers or contradict itself (different
    annotators have different values). Over time, several solutions have been proposed
    to tackle these issues, such as replacing human feedback with AI feedback ([RLAIF](https://arxiv.org/abs/2212.08073)).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 偏好数据集 notoriously 成本高且难以制作，因为它们需要从人类收集手动反馈。这些反馈往往具有主观性，容易对自信（但错误）的答案产生偏见，或相互矛盾（不同的标注者可能有不同的价值观）。随着时间的推移，已经提出了几种解决这些问题的方案，例如用AI反馈替代人工反馈（[RLAIF](https://arxiv.org/abs/2212.08073)）。
- en: These datasets also tend to be a lot smaller than fine-tuning datasets. To illustrate
    this, the excellent [neural-chat-7b-v3–1](https://huggingface.co/Intel/neural-chat-7b-v3-1)
    (best 7B LLM on the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
    when it was released) uses 518k samples for fine-tuning ([Open-Orca/SlimOrca](https://huggingface.co/datasets/Open-Orca/SlimOrca))
    but only 12.9k samples for RLHF ([Intel/orca_dpo_pairs](https://huggingface.co/datasets/Intel/orca_dpo_pairs)).
    In this case, the authors generated answers with GPT-4/3.5 to create the preferred
    answers, and with [Llama 2 13b chat](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf)
    to create the rejected responses. It’s a smart way to bypass human feedback and
    only rely on models with different levels of performance.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据集通常比微调数据集要小得多。为了说明这一点，优秀的[neural-chat-7b-v3–1](https://huggingface.co/Intel/neural-chat-7b-v3-1)（发布时在[Open
    LLM排行榜](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)上排名第一的7B
    LLM）使用了518k个样本进行微调（[Open-Orca/SlimOrca](https://huggingface.co/datasets/Open-Orca/SlimOrca)），但仅使用了12.9k个样本进行RLHF（[Intel/orca_dpo_pairs](https://huggingface.co/datasets/Intel/orca_dpo_pairs)）。在这种情况下，作者使用GPT-4/3.5生成答案来创建优选答案，使用[Llama
    2 13b chat](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf)生成被拒绝的回答。这是一种巧妙的方法，通过绕过人工反馈，仅依赖于不同性能水平的模型。
- en: 🎓 Direct Preference Optimization
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🎓 直接偏好优化
- en: While the concept of RLHF has been used in robotics for a long time, it was
    popularized for LLMs in OpenAI’s paper [Fine-Tuning Language Models from Human
    Preferences](https://arxiv.org/pdf/1909.08593.pdf). In this paper, the authors
    present a framework where a reward model is trained to approximate human feedback.
    This reward model is then used to optimize the fine-tuned model’s policy using
    the [Proximal Policy Optimization](https://arxiv.org/abs/1707.06347) (PPO) algorithm.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然RLHF的概念在机器人技术中已经使用了很长时间，但它在LLM中的流行起源于OpenAI的论文[从人类偏好微调语言模型](https://arxiv.org/pdf/1909.08593.pdf)。在这篇论文中，作者提出了一个框架，通过训练一个奖励模型来近似人类反馈。然后，使用这个奖励模型通过[邻近策略优化](https://arxiv.org/abs/1707.06347)（PPO）算法优化微调后的模型策略。
- en: '![](../Images/4770f82fec81739184b15c998ee60ca5.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4770f82fec81739184b15c998ee60ca5.png)'
- en: Image by author
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: The core concept of PPO revolves around making smaller, incremental updates
    to the policy, as larger updates can lead to instability or suboptimal solutions.
    From experience, this technique is unfortunately still unstable (loss diverges),
    difficult to reproduce (numerous hyperparameters, sensitive to random seeds),
    and computationally expensive.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: PPO 的核心概念是对策略进行较小的、增量的更新，因为较大的更新可能导致不稳定或次优的解决方案。从经验来看，这种技术不幸的是仍然不稳定（损失发散），难以重现（有大量超参数，且对随机种子敏感），而且计算开销大。
- en: 'This is where Direct Preference Optimization (DPO) comes into play. DPO simplifies
    control by treating the task as a classification problem. Concretely, it uses
    two models: the **trained model** (or policy model) and a copy of it called the
    **reference model**. During training, the goal is to make sure the trained model
    outputs higher probabilities for preferred answers than the reference model. Conversely,
    we also want it to output lower probabilities for rejected answers. It means we’re
    penalizing the LLM for bad answers and rewarding it for good ones.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这时，直接偏好优化（DPO）发挥了作用。DPO 通过将任务视为分类问题来简化控制。具体来说，它使用了两个模型：**训练模型**（或策略模型）和一个名为
    **参考模型** 的副本。在训练过程中，目标是确保训练模型对于优选答案输出比参考模型更高的概率。相反，我们也希望它对拒绝的答案输出更低的概率。这意味着我们在惩罚语言模型（LLM）给出的不良答案，同时奖励它给出的优质答案。
- en: '![](../Images/43f19625c49e94b7304ff1e6d521573a.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/43f19625c49e94b7304ff1e6d521573a.png)'
- en: Image by author
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来自作者
- en: By using the LLM itself as a reward model and employing binary cross-entropy
    objectives, DPO efficiently aligns the model’s outputs with human preferences
    without the need for extensive sampling, reward model fitting, or intricate hyperparameter
    adjustments. It results in a more stable, more efficient, and computationally
    less demanding process.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将 LLM 本身作为奖励模型，并采用二元交叉熵目标，DPO 高效地将模型的输出与人类偏好对齐，无需广泛的采样、奖励模型拟合或复杂的超参数调整。这使得该过程更加稳定、高效且计算需求较低。
- en: 💾 Formatting the data
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 💾 数据格式化
- en: In this example, we’ll fine-tune the excellent [OpenHermes-2.5-Mistral-7B](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B),
    which is a Mistral-7b model that was only supervised fine-tuned. To this end,
    we’ll use the [Intel/orca_dpo_pairs](https://huggingface.co/datasets/Intel/orca_dpo_pairs)
    dataset to align our model and improve its performance. We call this new model
    NeuralHermes-2.5-Mistral-7B.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将微调出色的 [OpenHermes-2.5-Mistral-7B](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B)，这是一个仅经过监督微调的
    Mistral-7b 模型。为此，我们将使用 [Intel/orca_dpo_pairs](https://huggingface.co/datasets/Intel/orca_dpo_pairs)
    数据集来对齐我们的模型并提高其性能。我们将这个新模型称为 NeuralHermes-2.5-Mistral-7B。
- en: The first step consists of installing the required libraries as follows.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 第一阶段包括安装所需的库，具体步骤如下。
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Once it’s done, we can import the libraries. I’m also using the secrets tab
    in Google Colab to store my Hugging Face token.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，我们可以导入这些库。我还在 Google Colab 的秘密标签中存储了我的 Hugging Face token。
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'OpenHermes-2.5-Mistral-7B uses a specific chat template, called [ChatML](https://huggingface.co/docs/transformers/chat_templating).
    Here is an example of a conversation formatted with this template:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: OpenHermes-2.5-Mistral-7B 使用了一种特定的聊天模板，称为 [ChatML](https://huggingface.co/docs/transformers/chat_templating)。以下是使用该模板格式化的对话示例：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As you can see, ChatML defines different roles (system, user, assistant) and
    appends special tokens (`<|im_start|>` and `<|im_end|>`) to separate them. Moreover,
    `[DPOTrainer](https://huggingface.co/docs/trl/main/en/dpo_trainer)` also requires
    a specific format with three columns: prompt, chosen, and rejected.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，ChatML 定义了不同的角色（系统、用户、助手），并附加了特殊标记（`<|im_start|>` 和 `<|im_end|>`）来分隔它们。此外，`[DPOTrainer](https://huggingface.co/docs/trl/main/en/dpo_trainer)`
    还需要一个特定的格式，包含三列：prompt、chosen 和 rejected。
- en: 'Our dataset contains four columns: system, question, chatgpt, and llama2–13b-chat.
    We’ll simply concatenate the system and question columns to the prompt column.
    We’ll also map the chatgpt column to “chosen” and llama2–13b-chat to “rejected”.
    To format the dataset in a reliable way, we’ll use the tokenizer’s `apply_chat_template()`
    function, which already uses ChatML.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集包含四列：system、question、chatgpt 和 llama2–13b-chat。我们将简单地将 system 和 question
    列拼接到 prompt 列。我们还会将 chatgpt 列映射到“chosen”，将 llama2–13b-chat 列映射到“rejected”。为了可靠地格式化数据集，我们将使用分词器的
    `apply_chat_template()` 函数，该函数已经使用了 ChatML。
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let’s print a sample of the formatted dataset to confirm that everything works
    as expected:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印格式化数据集的一个示例，以确认一切按预期工作：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We can see that the prompt combines system and user instructions. Thanks to
    the `add_generation_prompt=True` argument, it also appends the beginning of the
    assistant's answer. If you want to skip this step, you can directly used the preprocessed
    dataset as [mlabonne/chatml_dpo_pairs](https://huggingface.co/datasets/mlabonne/chatml_dpo_pairs).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，提示词结合了系统和用户的指令。感谢`add_generation_prompt=True`参数，它还附加了助手回答的开头。如果你想跳过这一步，可以直接使用预处理过的数据集，例如[mlabonne/chatml_dpo_pairs](https://huggingface.co/datasets/mlabonne/chatml_dpo_pairs)。
- en: ⚙️ Training the model with DPO
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ⚙️ 使用DPO训练模型
- en: Next, we define the LoRA configurations to train the model. As described in
    [Intel’s blog post](https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3),
    we set the rank value to be equal to the `lora_alpha`, which is unusual (2 * `r`
    as a rule of thumb). We also target all the linear modules with adapters.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义LoRA配置来训练模型。如[Intel的博客文章](https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3)中所述，我们将秩值设置为等于`lora_alpha`，这是不常见的（通常为2
    * `r`）。我们还使用适配器来针对所有线性模块。
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We’re now ready to load the model we want to fine-tune with DPO. In this case,
    two models are required: the model to fine-tune as well as the reference model.
    This is mostly for the sake of readability, as the `DPOTrainer` object automatically
    creates a reference model if none is provided.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备加载要用DPO进行微调的模型。在这种情况下，需要两个模型：一个用于微调的模型和一个参考模型。这样做主要是为了可读性，因为`DPOTrainer`对象如果没有提供参考模型，会自动创建一个参考模型。
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The final step consists of providing all the hyperparameters to `TrainingArguments`
    and `DPOTrainer`:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 最终步骤是将所有超参数提供给`TrainingArguments`和`DPOTrainer`：
- en: Among them, the `beta` parameter is unique to DPO since it controls the divergence
    from the initial policy (0.1 is a typical value for it).
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其中，`beta`参数是DPO特有的，因为它控制了与初始策略的偏离（0.1是一个典型值）。
- en: Compared to the values described in [Intel’s blog post](https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3),
    we lower the learning rate (from 5e-4 to 5e-5) and the number of steps (from 1,000
    to 200). I manually optimized these values after a few runs to stabilize training
    and achieve the best results.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与[Intel的博客文章](https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3)中描述的值相比，我们降低了学习率（从5e-4降到5e-5）和步数（从1,000降到200）。在几次运行后，我手动优化了这些值，以稳定训练并获得最佳结果。
- en: We can now start training the model. Note that it requires an A100 GPU and takes
    between 1 hour to complete the training.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始训练模型了。请注意，它需要一块A100 GPU，并且训练时间大约需要1小时。
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Our model is now fine-tuned. You can check the project on Weights & Biases
    [at this address](https://wandb.ai/mlabonne/NeuralHermes-2-5-Mistral-7B/runs/axe71gr0?workspace=user-mlabonne).
    Here are some interesting metrics to analyze:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型现在已经完成微调。你可以在Weights & Biases上查看该项目，[地址如下](https://wandb.ai/mlabonne/NeuralHermes-2-5-Mistral-7B/runs/axe71gr0?workspace=user-mlabonne)。这里有一些有趣的指标可以分析：
- en: '![](../Images/a3622d152e976686a7e55807e80b371a.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a3622d152e976686a7e55807e80b371a.png)'
- en: Image by author
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Interestingly, the training loss quickly drops to zero (before 50 steps), despite
    100 warmup steps. Meanwhile, the other metrics keep evolving.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，训练损失迅速下降到零（在50步之前），尽管有100步的热身步骤。与此同时，其他指标持续演变。
- en: The train/rewards/chosen and train/rewards/rejected plots correspond to the
    mean difference between the log probabilities output by the trained and reference
    models. It makes sense that, over time, they diverge as our trained model learns
    the preferred answers. The train/rewards/margins plot also shows the difference
    between these two plots. Finally, the train/reward/accuracies plot shows the frequency
    of choosing the preferred answer. The trained model quickly reaches a perfect
    accuracy score, which is a good sign but could also mean that the difference between
    preferred and rejected answers is too obvious.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: train/rewards/chosen和train/rewards/rejected图表对应的是训练模型和参考模型输出的对数概率之间的平均差异。随着时间的推移，它们的差异逐渐增大，因为我们的训练模型学习了首选答案。train/rewards/margins图表也显示了这两者之间的差异。最后，train/reward/accuracies图表展示了选择首选答案的频率。训练后的模型迅速达到了完美的准确率，这虽然是一个好兆头，但也可能意味着首选答案与被拒绝答案之间的差异过于明显。
- en: Now that it’s trained, we can merge the adapter with the original model. Next,
    we save the merged model and the tokenizer before pushing it to the Hugging Face
    Hub.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型已经训练完成，我们可以将适配器与原始模型合并。接下来，我们保存合并后的模型和标记器，然后将其推送到 Hugging Face Hub。
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s see how our model performs in a real test. We’ll format the prompt to
    ask a basic question: “What is a Large Language Model?”'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的模型在实际测试中的表现。我们将格式化提示，提出一个基本问题：“什么是大规模语言模型？”
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here’s the answer from the model:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这是模型的回答：
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Everything seems to be working, we can now evaluate the merged model. As this
    is a general-purpose model, we can leverage the [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)
    to evaluate it. As the process is quite resource-intensive, we can also directly
    submit it for evaluation on the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).
    It took a few days, but here are the results compared to other OpenHermes models:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一切似乎都在正常工作，我们现在可以评估合并后的模型。由于这是一个通用模型，我们可以利用 [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)
    来评估它。由于这个过程相当资源密集，我们也可以直接将其提交到 [Open LLM 排行榜](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)进行评估。虽然花费了几天时间，但这里是与其他
    OpenHermes 模型的对比结果：
- en: '![](../Images/9e7444bfe5a4b31c4a4f1df2ed935365.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e7444bfe5a4b31c4a4f1df2ed935365.png)'
- en: Image by author
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Compared to the original model, NeuralHermes-2–5-Mistral-7B model improved the
    average score by 6.7 points (particularly on GSM8K). This is an unexpectedly large
    improvement, which showcases the power of Direct Preference Optimization.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 与原始模型相比，NeuralHermes-2.5-Mistral-7B 模型将平均得分提高了 6.7 分（尤其是在 GSM8K 上）。这是一次出乎意料的巨大提升，展示了直接偏好优化的强大力量。
- en: Conclusion
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we fine-tuned an already supervised fine-tuned model using
    DPO and created our own [NeuralHermes-2.5](https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B)
    model. By leveraging a high-quality preference dataset, we created a sample-efficient
    fine-tuning pipeline that produced a significant improvement on the Open LLM Leaderboard.
    If you want to give it a try, you can find quantized variants of this model or
    use this [Hugging Face Space](https://huggingface.co/spaces/zhangtao103239/NeuralHermes-2.5-Mistral-7B-GGUF-Chat).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们使用 DPO 微调了一个已经经过监督微调的模型，并创建了我们自己的 [NeuralHermes-2.5](https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B)
    模型。通过利用高质量的偏好数据集，我们创建了一个高效的微调流程，并在 Open LLM 排行榜上取得了显著的提升。如果你想尝试，可以找到这个模型的量化变体，或使用这个
    [Hugging Face Space](https://huggingface.co/spaces/zhangtao103239/NeuralHermes-2.5-Mistral-7B-GGUF-Chat)。
- en: Note that our fine-tuning pipeline can still be improved in different ways.
    For example, the preference dataset is still quite raw and could be improved with
    more filtering and by using different models. In addition, numerous hyperparameters
    can still be tweaked to achieve better results. In particular, the learning rate
    can still be lowered to train the model on more steps and inject more preference
    data.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们的微调流程仍然可以通过不同的方式进行改进。例如，偏好数据集仍然相当原始，可以通过更多的过滤和使用不同的模型来改进。此外，许多超参数仍然可以进行调整，以获得更好的结果。特别是，学习率仍然可以降低，以便在更多的步骤上训练模型并注入更多的偏好数据。
- en: References
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[Fine-tune Llama 2 with DPO](https://huggingface.co/blog/dpo-trl) by Kashif
    Rasul, Younes Belkada, and Leandro von Werra.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过 DPO 微调 Llama 2](https://huggingface.co/blog/dpo-trl) 作者：Kashif Rasul、Younes
    Belkada 和 Leandro von Werra。'
- en: '[Supervised Fine-Tuning and Direct Preference Optimization on Intel Gaudi2](https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3)
    by Kaokao Lv, Wenxin Zhang, and Haihao Shen.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在 Intel Gaudi2 上的监督微调和直接偏好优化](https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3)
    作者：Kaokao Lv、Wenxin Zhang 和 Haihao Shen。'
- en: '[llama2-fine-tune](https://github.com/mzbac/llama2-fine-tune) by mzbac.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[llama2-fine-tune](https://github.com/mzbac/llama2-fine-tune) 作者：mzbac。'
- en: '*Learn more about machine learning and support my work with one click — become
    a Medium member here:*'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*了解更多关于机器学习的知识，并通过一次点击支持我的工作 —— 在这里成为 Medium 会员：*'
- en: '[](https://medium.com/@mlabonne/membership?source=post_page-----708042745aac--------------------------------)
    [## Join Medium with my referral link - Maxime Labonne'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne/membership?source=post_page-----708042745aac--------------------------------)
    [## 通过我的推荐链接加入 Medium - Maxime Labonne'
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story…
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 作为 Medium 的会员，你的一部分会员费用会分配给你阅读的作者，并且你可以完全访问每篇故事…
- en: medium.com](https://medium.com/@mlabonne/membership?source=post_page-----708042745aac--------------------------------)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@mlabonne/membership?source=post_page-----708042745aac--------------------------------)
