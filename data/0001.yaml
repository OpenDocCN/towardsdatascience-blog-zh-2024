- en: Fine-tune a Mistral-7b model with Direct Preference Optimization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¾®è°ƒMistral-7bæ¨¡å‹ä¸ç›´æ¥åå¥½ä¼˜åŒ–
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac?source=collection_archive---------0-----------------------#2024-01-01](https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac?source=collection_archive---------0-----------------------#2024-01-01)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac?source=collection_archive---------0-----------------------#2024-01-01](https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac?source=collection_archive---------0-----------------------#2024-01-01)
- en: Boost the performance of your supervised fine-tuned models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æå‡ä½ ç›‘ç£å¾®è°ƒæ¨¡å‹çš„è¡¨ç°
- en: '[](https://medium.com/@mlabonne?source=post_page---byline--708042745aac--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page---byline--708042745aac--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--708042745aac--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--708042745aac--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page---byline--708042745aac--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne?source=post_page---byline--708042745aac--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page---byline--708042745aac--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--708042745aac--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--708042745aac--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page---byline--708042745aac--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--708042745aac--------------------------------)
    Â·10 min readÂ·Jan 1, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--708042745aac--------------------------------)
    Â·é˜…è¯»æ—¶é—´10åˆ†é’ŸÂ·2024å¹´1æœˆ1æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/3d74f51ec0cdd912262edbd229c2e620.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d74f51ec0cdd912262edbd229c2e620.png)'
- en: Image by author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: 'Pre-trained Large Language Models (LLMs) can only perform next-token prediction,
    making them unable to answer questions. This is why these base models are then
    fine-tuned on pairs of instructions and answers to act as helpful assistants.
    However, this process can still be flawed: fine-tuned LLMs can be biased, toxic,
    harmful, etc. This is where Reinforcement Learning from Human Feedback (RLHF)
    comes into play.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åªèƒ½æ‰§è¡Œä¸‹ä¸€ä¸ªtokençš„é¢„æµ‹ï¼Œè¿™ä½¿å¾—å®ƒä»¬æ— æ³•å›ç­”é—®é¢˜ã€‚è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆè¿™äº›åŸºç¡€æ¨¡å‹éšåä¼šåœ¨æŒ‡ä»¤å’Œå›ç­”çš„é…å¯¹ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä»¥å……å½“æœ‰ç”¨çš„åŠ©æ‰‹ã€‚ç„¶è€Œï¼Œè¿™ä¸€è¿‡ç¨‹ä»ç„¶å¯èƒ½å­˜åœ¨ç¼ºé™·ï¼šå¾®è°ƒåçš„LLMå¯èƒ½å­˜åœ¨åè§ã€æœ‰å®³ã€æ¯’æ€§ç­‰é—®é¢˜ã€‚è¿™æ—¶ï¼Œæ¥è‡ªäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ä¾¿å‘æŒ¥äº†ä½œç”¨ã€‚
- en: RLHF provides different answers to the LLM, which are ranked according to a
    desired behavior (helpfulness, toxicity, etc.). The model learns to output the
    best answer among these candidates, hence mimicking the behavior we want to instill.
    Often seen as a way to censor models, this process has recently become popular
    for improving performance, as shown in [neural-chat-7b-v3â€“1](https://huggingface.co/Intel/neural-chat-7b-v3-1).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: RLHFä¸ºLLMæä¾›ä¸åŒçš„ç­”æ¡ˆï¼Œå¹¶æ ¹æ®æœŸæœ›çš„è¡Œä¸ºï¼ˆä¾‹å¦‚æœ‰ç”¨æ€§ã€æ¯’æ€§ç­‰ï¼‰å¯¹è¿™äº›ç­”æ¡ˆè¿›è¡Œæ’åºã€‚æ¨¡å‹å­¦ä¼šåœ¨è¿™äº›å€™é€‰ç­”æ¡ˆä¸­è¾“å‡ºæœ€ä½³ç­”æ¡ˆï¼Œä»è€Œæ¨¡ä»¿æˆ‘ä»¬å¸Œæœ›å…¶è¡¨ç°çš„è¡Œä¸ºã€‚è¿™ä¸ªè¿‡ç¨‹é€šå¸¸è¢«è§†ä¸ºä¸€ç§å®¡æŸ¥æ¨¡å‹çš„æ–¹æ³•ï¼Œä½†æœ€è¿‘å®ƒå·²æˆä¸ºä¸€ç§æ”¹å–„æ€§èƒ½çš„æµè¡Œæ–¹å¼ï¼Œå¦‚åœ¨[neural-chat-7b-v3â€“1](https://huggingface.co/Intel/neural-chat-7b-v3-1)ä¸­æ‰€ç¤ºã€‚
- en: 'In this article, we will create [NeuralHermes-2.5](https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B),
    by fine-tuning [OpenHermes-2.5](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B)
    using a RLHF-like technique: Direct Preference Optimization (DPO). For this purpose,
    we will introduce a preference dataset, describe how the DPO algorithm works,
    and apply it to our model. Weâ€™ll see that it significantly improves the performance
    of the base model on the Open LLM Leaderboard.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†é€šè¿‡ä½¿ç”¨ç±»ä¼¼äºå¼ºåŒ–å­¦ä¹ çš„æŠ€æœ¯â€”â€”ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ¥å¾®è°ƒ[OpenHermes-2.5](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B)ï¼Œä»è€Œåˆ›å»º[NeuralHermes-2.5](https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B)ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†å¼•å…¥ä¸€ä¸ªåå¥½æ•°æ®é›†ï¼Œæè¿°DPOç®—æ³•çš„å·¥ä½œåŸç†ï¼Œå¹¶å°†å…¶åº”ç”¨åˆ°æˆ‘ä»¬çš„æ¨¡å‹ä¸­ã€‚æˆ‘ä»¬å°†çœ‹åˆ°ï¼Œè¿™æ˜¾è‘—æé«˜äº†åŸºç¡€æ¨¡å‹åœ¨å¼€æ”¾LLMæ’è¡Œæ¦œä¸Šçš„è¡¨ç°ã€‚
- en: As per usual, the code is available on [GitHub](https://github.com/mlabonne/llm-course/blob/main/Fine_tune_a_Mistral_7b_model_with_DPO.ipynb)
    and [Google Colab](https://colab.research.google.com/drive/15iFBr1xWgztXvhrj5I9fBv20c7CFOPBE?usp=sharing).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å¸¸ï¼Œä»£ç å¯åœ¨[GitHub](https://github.com/mlabonne/llm-course/blob/main/Fine_tune_a_Mistral_7b_model_with_DPO.ipynb)å’Œ[Google
    Colab](https://colab.research.google.com/drive/15iFBr1xWgztXvhrj5I9fBv20c7CFOPBE?usp=sharing)ä¸Šæ‰¾åˆ°ã€‚
- en: '***Update****:* [*Jessie Davids*](https://www.linkedin.com/in/jesse-th-davids/)*,
    a reader who used this article and code, managed to create the best-performing
    model on the Open LLM Leaderboard ~7B param. Congrats to him! ğŸ‰*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '***æ›´æ–°***ï¼š[*Jessie Davids*](https://www.linkedin.com/in/jesse-th-davids/)ï¼Œä¸€ä½ä½¿ç”¨æœ¬æ–‡åŠä»£ç çš„è¯»è€…ï¼ŒæˆåŠŸåˆ›å»ºäº†åœ¨Open
    LLMæ’è¡Œæ¦œä¸Šè¡¨ç°æœ€å¥½çš„æ¨¡å‹ï¼Œçº¦7Bå‚æ•°ã€‚æ­å–œä»–ï¼ğŸ‰'
- en: '![](../Images/23a4f24817da40f445ad29a63c66869d.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23a4f24817da40f445ad29a63c66869d.png)'
- en: Image by author
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼šä½œè€…
- en: ğŸ¥‡ Preference datasets
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ¥‡ åå¥½æ•°æ®é›†
- en: 'Preference datasets are not standardized, but they typically consist of a collection
    of answers that are ranked by humans. This ranking is essential, as the RLHF process
    fine-tunes LLMs to output the preferred answer. Here is an example of [Anthropic/hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf/viewer/default/train),
    a popular preference dataset:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: åå¥½æ•°æ®é›†æ²¡æœ‰æ ‡å‡†åŒ–ï¼Œä½†å®ƒä»¬é€šå¸¸ç”±ä¸€ç»„ç»è¿‡äººå·¥æ’åºçš„ç­”æ¡ˆç»„æˆã€‚è¿™ä¸ªæ’åºéå¸¸å…³é”®ï¼Œå› ä¸ºRLHFè¿‡ç¨‹ä¼šå¾®è°ƒLLMï¼Œä½¿å…¶è¾“å‡ºä¼˜é€‰ç­”æ¡ˆã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªå¸¸è§çš„åå¥½æ•°æ®é›†ç¤ºä¾‹ï¼š[Anthropic/hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf/viewer/default/train)ï¼š
- en: '![](../Images/9930d8bacb694aecb9e9556e101932ff.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9930d8bacb694aecb9e9556e101932ff.png)'
- en: Image by author
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼šä½œè€…
- en: 'The structure of the dataset is straightforward: for each row, there is one
    chosen (preferred) answer, and one rejected answer. The goal of RLHF is to guide
    the model to output the preferred answer.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›†çš„ç»“æ„å¾ˆç®€å•ï¼šæ¯ä¸€è¡Œéƒ½æœ‰ä¸€ä¸ªé€‰å®šçš„ï¼ˆä¼˜é€‰çš„ï¼‰ç­”æ¡ˆå’Œä¸€ä¸ªè¢«æ‹’ç»çš„ç­”æ¡ˆã€‚RLHFçš„ç›®æ ‡æ˜¯å¼•å¯¼æ¨¡å‹è¾“å‡ºä¼˜é€‰çš„ç­”æ¡ˆã€‚
- en: Preference datasets are notoriously costly and difficult to make, as they require
    collecting manual feedback from humans. This feedback is also subjective and can
    easily be biased toward confident (but wrong) answers or contradict itself (different
    annotators have different values). Over time, several solutions have been proposed
    to tackle these issues, such as replacing human feedback with AI feedback ([RLAIF](https://arxiv.org/abs/2212.08073)).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: åå¥½æ•°æ®é›† notoriously æˆæœ¬é«˜ä¸”éš¾ä»¥åˆ¶ä½œï¼Œå› ä¸ºå®ƒä»¬éœ€è¦ä»äººç±»æ”¶é›†æ‰‹åŠ¨åé¦ˆã€‚è¿™äº›åé¦ˆå¾€å¾€å…·æœ‰ä¸»è§‚æ€§ï¼Œå®¹æ˜“å¯¹è‡ªä¿¡ï¼ˆä½†é”™è¯¯ï¼‰çš„ç­”æ¡ˆäº§ç”Ÿåè§ï¼Œæˆ–ç›¸äº’çŸ›ç›¾ï¼ˆä¸åŒçš„æ ‡æ³¨è€…å¯èƒ½æœ‰ä¸åŒçš„ä»·å€¼è§‚ï¼‰ã€‚éšç€æ—¶é—´çš„æ¨ç§»ï¼Œå·²ç»æå‡ºäº†å‡ ç§è§£å†³è¿™äº›é—®é¢˜çš„æ–¹æ¡ˆï¼Œä¾‹å¦‚ç”¨AIåé¦ˆæ›¿ä»£äººå·¥åé¦ˆï¼ˆ[RLAIF](https://arxiv.org/abs/2212.08073)ï¼‰ã€‚
- en: These datasets also tend to be a lot smaller than fine-tuning datasets. To illustrate
    this, the excellent [neural-chat-7b-v3â€“1](https://huggingface.co/Intel/neural-chat-7b-v3-1)
    (best 7B LLM on the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
    when it was released) uses 518k samples for fine-tuning ([Open-Orca/SlimOrca](https://huggingface.co/datasets/Open-Orca/SlimOrca))
    but only 12.9k samples for RLHF ([Intel/orca_dpo_pairs](https://huggingface.co/datasets/Intel/orca_dpo_pairs)).
    In this case, the authors generated answers with GPT-4/3.5 to create the preferred
    answers, and with [Llama 2 13b chat](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf)
    to create the rejected responses. Itâ€™s a smart way to bypass human feedback and
    only rely on models with different levels of performance.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ•°æ®é›†é€šå¸¸æ¯”å¾®è°ƒæ•°æ®é›†è¦å°å¾—å¤šã€‚ä¸ºäº†è¯´æ˜è¿™ä¸€ç‚¹ï¼Œä¼˜ç§€çš„[neural-chat-7b-v3â€“1](https://huggingface.co/Intel/neural-chat-7b-v3-1)ï¼ˆå‘å¸ƒæ—¶åœ¨[Open
    LLMæ’è¡Œæ¦œ](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)ä¸Šæ’åç¬¬ä¸€çš„7B
    LLMï¼‰ä½¿ç”¨äº†518kä¸ªæ ·æœ¬è¿›è¡Œå¾®è°ƒï¼ˆ[Open-Orca/SlimOrca](https://huggingface.co/datasets/Open-Orca/SlimOrca)ï¼‰ï¼Œä½†ä»…ä½¿ç”¨äº†12.9kä¸ªæ ·æœ¬è¿›è¡ŒRLHFï¼ˆ[Intel/orca_dpo_pairs](https://huggingface.co/datasets/Intel/orca_dpo_pairs)ï¼‰ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½œè€…ä½¿ç”¨GPT-4/3.5ç”Ÿæˆç­”æ¡ˆæ¥åˆ›å»ºä¼˜é€‰ç­”æ¡ˆï¼Œä½¿ç”¨[Llama
    2 13b chat](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf)ç”Ÿæˆè¢«æ‹’ç»çš„å›ç­”ã€‚è¿™æ˜¯ä¸€ç§å·§å¦™çš„æ–¹æ³•ï¼Œé€šè¿‡ç»•è¿‡äººå·¥åé¦ˆï¼Œä»…ä¾èµ–äºä¸åŒæ€§èƒ½æ°´å¹³çš„æ¨¡å‹ã€‚
- en: ğŸ“ Direct Preference Optimization
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ“ ç›´æ¥åå¥½ä¼˜åŒ–
- en: While the concept of RLHF has been used in robotics for a long time, it was
    popularized for LLMs in OpenAIâ€™s paper [Fine-Tuning Language Models from Human
    Preferences](https://arxiv.org/pdf/1909.08593.pdf). In this paper, the authors
    present a framework where a reward model is trained to approximate human feedback.
    This reward model is then used to optimize the fine-tuned modelâ€™s policy using
    the [Proximal Policy Optimization](https://arxiv.org/abs/1707.06347) (PPO) algorithm.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶RLHFçš„æ¦‚å¿µåœ¨æœºå™¨äººæŠ€æœ¯ä¸­å·²ç»ä½¿ç”¨äº†å¾ˆé•¿æ—¶é—´ï¼Œä½†å®ƒåœ¨LLMä¸­çš„æµè¡Œèµ·æºäºOpenAIçš„è®ºæ–‡[ä»äººç±»åå¥½å¾®è°ƒè¯­è¨€æ¨¡å‹](https://arxiv.org/pdf/1909.08593.pdf)ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œé€šè¿‡è®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹æ¥è¿‘ä¼¼äººç±»åé¦ˆã€‚ç„¶åï¼Œä½¿ç”¨è¿™ä¸ªå¥–åŠ±æ¨¡å‹é€šè¿‡[é‚»è¿‘ç­–ç•¥ä¼˜åŒ–](https://arxiv.org/abs/1707.06347)ï¼ˆPPOï¼‰ç®—æ³•ä¼˜åŒ–å¾®è°ƒåçš„æ¨¡å‹ç­–ç•¥ã€‚
- en: '![](../Images/4770f82fec81739184b15c998ee60ca5.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4770f82fec81739184b15c998ee60ca5.png)'
- en: Image by author
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼šä½œè€…
- en: The core concept of PPO revolves around making smaller, incremental updates
    to the policy, as larger updates can lead to instability or suboptimal solutions.
    From experience, this technique is unfortunately still unstable (loss diverges),
    difficult to reproduce (numerous hyperparameters, sensitive to random seeds),
    and computationally expensive.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: PPO çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯å¯¹ç­–ç•¥è¿›è¡Œè¾ƒå°çš„ã€å¢é‡çš„æ›´æ–°ï¼Œå› ä¸ºè¾ƒå¤§çš„æ›´æ–°å¯èƒ½å¯¼è‡´ä¸ç¨³å®šæˆ–æ¬¡ä¼˜çš„è§£å†³æ–¹æ¡ˆã€‚ä»ç»éªŒæ¥çœ‹ï¼Œè¿™ç§æŠ€æœ¯ä¸å¹¸çš„æ˜¯ä»ç„¶ä¸ç¨³å®šï¼ˆæŸå¤±å‘æ•£ï¼‰ï¼Œéš¾ä»¥é‡ç°ï¼ˆæœ‰å¤§é‡è¶…å‚æ•°ï¼Œä¸”å¯¹éšæœºç§å­æ•æ„Ÿï¼‰ï¼Œè€Œä¸”è®¡ç®—å¼€é”€å¤§ã€‚
- en: 'This is where Direct Preference Optimization (DPO) comes into play. DPO simplifies
    control by treating the task as a classification problem. Concretely, it uses
    two models: the **trained model** (or policy model) and a copy of it called the
    **reference model**. During training, the goal is to make sure the trained model
    outputs higher probabilities for preferred answers than the reference model. Conversely,
    we also want it to output lower probabilities for rejected answers. It means weâ€™re
    penalizing the LLM for bad answers and rewarding it for good ones.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ—¶ï¼Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å‘æŒ¥äº†ä½œç”¨ã€‚DPO é€šè¿‡å°†ä»»åŠ¡è§†ä¸ºåˆ†ç±»é—®é¢˜æ¥ç®€åŒ–æ§åˆ¶ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒä½¿ç”¨äº†ä¸¤ä¸ªæ¨¡å‹ï¼š**è®­ç»ƒæ¨¡å‹**ï¼ˆæˆ–ç­–ç•¥æ¨¡å‹ï¼‰å’Œä¸€ä¸ªåä¸º
    **å‚è€ƒæ¨¡å‹** çš„å‰¯æœ¬ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç›®æ ‡æ˜¯ç¡®ä¿è®­ç»ƒæ¨¡å‹å¯¹äºä¼˜é€‰ç­”æ¡ˆè¾“å‡ºæ¯”å‚è€ƒæ¨¡å‹æ›´é«˜çš„æ¦‚ç‡ã€‚ç›¸åï¼Œæˆ‘ä»¬ä¹Ÿå¸Œæœ›å®ƒå¯¹æ‹’ç»çš„ç­”æ¡ˆè¾“å‡ºæ›´ä½çš„æ¦‚ç‡ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬åœ¨æƒ©ç½šè¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»™å‡ºçš„ä¸è‰¯ç­”æ¡ˆï¼ŒåŒæ—¶å¥–åŠ±å®ƒç»™å‡ºçš„ä¼˜è´¨ç­”æ¡ˆã€‚
- en: '![](../Images/43f19625c49e94b7304ff1e6d521573a.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/43f19625c49e94b7304ff1e6d521573a.png)'
- en: Image by author
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒæ¥è‡ªä½œè€…
- en: By using the LLM itself as a reward model and employing binary cross-entropy
    objectives, DPO efficiently aligns the modelâ€™s outputs with human preferences
    without the need for extensive sampling, reward model fitting, or intricate hyperparameter
    adjustments. It results in a more stable, more efficient, and computationally
    less demanding process.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å°† LLM æœ¬èº«ä½œä¸ºå¥–åŠ±æ¨¡å‹ï¼Œå¹¶é‡‡ç”¨äºŒå…ƒäº¤å‰ç†µç›®æ ‡ï¼ŒDPO é«˜æ•ˆåœ°å°†æ¨¡å‹çš„è¾“å‡ºä¸äººç±»åå¥½å¯¹é½ï¼Œæ— éœ€å¹¿æ³›çš„é‡‡æ ·ã€å¥–åŠ±æ¨¡å‹æ‹Ÿåˆæˆ–å¤æ‚çš„è¶…å‚æ•°è°ƒæ•´ã€‚è¿™ä½¿å¾—è¯¥è¿‡ç¨‹æ›´åŠ ç¨³å®šã€é«˜æ•ˆä¸”è®¡ç®—éœ€æ±‚è¾ƒä½ã€‚
- en: ğŸ’¾ Formatting the data
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ’¾ æ•°æ®æ ¼å¼åŒ–
- en: In this example, weâ€™ll fine-tune the excellent [OpenHermes-2.5-Mistral-7B](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B),
    which is a Mistral-7b model that was only supervised fine-tuned. To this end,
    weâ€™ll use the [Intel/orca_dpo_pairs](https://huggingface.co/datasets/Intel/orca_dpo_pairs)
    dataset to align our model and improve its performance. We call this new model
    NeuralHermes-2.5-Mistral-7B.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†å¾®è°ƒå‡ºè‰²çš„ [OpenHermes-2.5-Mistral-7B](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B)ï¼Œè¿™æ˜¯ä¸€ä¸ªä»…ç»è¿‡ç›‘ç£å¾®è°ƒçš„
    Mistral-7b æ¨¡å‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ [Intel/orca_dpo_pairs](https://huggingface.co/datasets/Intel/orca_dpo_pairs)
    æ•°æ®é›†æ¥å¯¹é½æˆ‘ä»¬çš„æ¨¡å‹å¹¶æé«˜å…¶æ€§èƒ½ã€‚æˆ‘ä»¬å°†è¿™ä¸ªæ–°æ¨¡å‹ç§°ä¸º NeuralHermes-2.5-Mistral-7Bã€‚
- en: The first step consists of installing the required libraries as follows.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€é˜¶æ®µåŒ…æ‹¬å®‰è£…æ‰€éœ€çš„åº“ï¼Œå…·ä½“æ­¥éª¤å¦‚ä¸‹ã€‚
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Once itâ€™s done, we can import the libraries. Iâ€™m also using the secrets tab
    in Google Colab to store my Hugging Face token.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæˆåï¼Œæˆ‘ä»¬å¯ä»¥å¯¼å…¥è¿™äº›åº“ã€‚æˆ‘è¿˜åœ¨ Google Colab çš„ç§˜å¯†æ ‡ç­¾ä¸­å­˜å‚¨äº†æˆ‘çš„ Hugging Face tokenã€‚
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'OpenHermes-2.5-Mistral-7B uses a specific chat template, called [ChatML](https://huggingface.co/docs/transformers/chat_templating).
    Here is an example of a conversation formatted with this template:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: OpenHermes-2.5-Mistral-7B ä½¿ç”¨äº†ä¸€ç§ç‰¹å®šçš„èŠå¤©æ¨¡æ¿ï¼Œç§°ä¸º [ChatML](https://huggingface.co/docs/transformers/chat_templating)ã€‚ä»¥ä¸‹æ˜¯ä½¿ç”¨è¯¥æ¨¡æ¿æ ¼å¼åŒ–çš„å¯¹è¯ç¤ºä¾‹ï¼š
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As you can see, ChatML defines different roles (system, user, assistant) and
    appends special tokens (`<|im_start|>` and `<|im_end|>`) to separate them. Moreover,
    `[DPOTrainer](https://huggingface.co/docs/trl/main/en/dpo_trainer)` also requires
    a specific format with three columns: prompt, chosen, and rejected.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼ŒChatML å®šä¹‰äº†ä¸åŒçš„è§’è‰²ï¼ˆç³»ç»Ÿã€ç”¨æˆ·ã€åŠ©æ‰‹ï¼‰ï¼Œå¹¶é™„åŠ äº†ç‰¹æ®Šæ ‡è®°ï¼ˆ`<|im_start|>` å’Œ `<|im_end|>`ï¼‰æ¥åˆ†éš”å®ƒä»¬ã€‚æ­¤å¤–ï¼Œ`[DPOTrainer](https://huggingface.co/docs/trl/main/en/dpo_trainer)`
    è¿˜éœ€è¦ä¸€ä¸ªç‰¹å®šçš„æ ¼å¼ï¼ŒåŒ…å«ä¸‰åˆ—ï¼špromptã€chosen å’Œ rejectedã€‚
- en: 'Our dataset contains four columns: system, question, chatgpt, and llama2â€“13b-chat.
    Weâ€™ll simply concatenate the system and question columns to the prompt column.
    Weâ€™ll also map the chatgpt column to â€œchosenâ€ and llama2â€“13b-chat to â€œrejectedâ€.
    To format the dataset in a reliable way, weâ€™ll use the tokenizerâ€™s `apply_chat_template()`
    function, which already uses ChatML.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„æ•°æ®é›†åŒ…å«å››åˆ—ï¼šsystemã€questionã€chatgpt å’Œ llama2â€“13b-chatã€‚æˆ‘ä»¬å°†ç®€å•åœ°å°† system å’Œ question
    åˆ—æ‹¼æ¥åˆ° prompt åˆ—ã€‚æˆ‘ä»¬è¿˜ä¼šå°† chatgpt åˆ—æ˜ å°„åˆ°â€œchosenâ€ï¼Œå°† llama2â€“13b-chat åˆ—æ˜ å°„åˆ°â€œrejectedâ€ã€‚ä¸ºäº†å¯é åœ°æ ¼å¼åŒ–æ•°æ®é›†ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨åˆ†è¯å™¨çš„
    `apply_chat_template()` å‡½æ•°ï¼Œè¯¥å‡½æ•°å·²ç»ä½¿ç”¨äº† ChatMLã€‚
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Letâ€™s print a sample of the formatted dataset to confirm that everything works
    as expected:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ‰“å°æ ¼å¼åŒ–æ•°æ®é›†çš„ä¸€ä¸ªç¤ºä¾‹ï¼Œä»¥ç¡®è®¤ä¸€åˆ‡æŒ‰é¢„æœŸå·¥ä½œï¼š
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We can see that the prompt combines system and user instructions. Thanks to
    the `add_generation_prompt=True` argument, it also appends the beginning of the
    assistant's answer. If you want to skip this step, you can directly used the preprocessed
    dataset as [mlabonne/chatml_dpo_pairs](https://huggingface.co/datasets/mlabonne/chatml_dpo_pairs).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæç¤ºè¯ç»“åˆäº†ç³»ç»Ÿå’Œç”¨æˆ·çš„æŒ‡ä»¤ã€‚æ„Ÿè°¢`add_generation_prompt=True`å‚æ•°ï¼Œå®ƒè¿˜é™„åŠ äº†åŠ©æ‰‹å›ç­”çš„å¼€å¤´ã€‚å¦‚æœä½ æƒ³è·³è¿‡è¿™ä¸€æ­¥ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨é¢„å¤„ç†è¿‡çš„æ•°æ®é›†ï¼Œä¾‹å¦‚[mlabonne/chatml_dpo_pairs](https://huggingface.co/datasets/mlabonne/chatml_dpo_pairs)ã€‚
- en: âš™ï¸ Training the model with DPO
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: âš™ï¸ ä½¿ç”¨DPOè®­ç»ƒæ¨¡å‹
- en: Next, we define the LoRA configurations to train the model. As described in
    [Intelâ€™s blog post](https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3),
    we set the rank value to be equal to the `lora_alpha`, which is unusual (2 * `r`
    as a rule of thumb). We also target all the linear modules with adapters.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰LoRAé…ç½®æ¥è®­ç»ƒæ¨¡å‹ã€‚å¦‚[Intelçš„åšå®¢æ–‡ç« ](https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3)ä¸­æ‰€è¿°ï¼Œæˆ‘ä»¬å°†ç§©å€¼è®¾ç½®ä¸ºç­‰äº`lora_alpha`ï¼Œè¿™æ˜¯ä¸å¸¸è§çš„ï¼ˆé€šå¸¸ä¸º2
    * `r`ï¼‰ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨é€‚é…å™¨æ¥é’ˆå¯¹æ‰€æœ‰çº¿æ€§æ¨¡å—ã€‚
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Weâ€™re now ready to load the model we want to fine-tune with DPO. In this case,
    two models are required: the model to fine-tune as well as the reference model.
    This is mostly for the sake of readability, as the `DPOTrainer` object automatically
    creates a reference model if none is provided.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å‡†å¤‡åŠ è½½è¦ç”¨DPOè¿›è¡Œå¾®è°ƒçš„æ¨¡å‹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œéœ€è¦ä¸¤ä¸ªæ¨¡å‹ï¼šä¸€ä¸ªç”¨äºå¾®è°ƒçš„æ¨¡å‹å’Œä¸€ä¸ªå‚è€ƒæ¨¡å‹ã€‚è¿™æ ·åšä¸»è¦æ˜¯ä¸ºäº†å¯è¯»æ€§ï¼Œå› ä¸º`DPOTrainer`å¯¹è±¡å¦‚æœæ²¡æœ‰æä¾›å‚è€ƒæ¨¡å‹ï¼Œä¼šè‡ªåŠ¨åˆ›å»ºä¸€ä¸ªå‚è€ƒæ¨¡å‹ã€‚
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The final step consists of providing all the hyperparameters to `TrainingArguments`
    and `DPOTrainer`:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆæ­¥éª¤æ˜¯å°†æ‰€æœ‰è¶…å‚æ•°æä¾›ç»™`TrainingArguments`å’Œ`DPOTrainer`ï¼š
- en: Among them, the `beta` parameter is unique to DPO since it controls the divergence
    from the initial policy (0.1 is a typical value for it).
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œ`beta`å‚æ•°æ˜¯DPOç‰¹æœ‰çš„ï¼Œå› ä¸ºå®ƒæ§åˆ¶äº†ä¸åˆå§‹ç­–ç•¥çš„åç¦»ï¼ˆ0.1æ˜¯ä¸€ä¸ªå…¸å‹å€¼ï¼‰ã€‚
- en: Compared to the values described in [Intelâ€™s blog post](https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3),
    we lower the learning rate (from 5e-4 to 5e-5) and the number of steps (from 1,000
    to 200). I manually optimized these values after a few runs to stabilize training
    and achieve the best results.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸[Intelçš„åšå®¢æ–‡ç« ](https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3)ä¸­æè¿°çš„å€¼ç›¸æ¯”ï¼Œæˆ‘ä»¬é™ä½äº†å­¦ä¹ ç‡ï¼ˆä»5e-4é™åˆ°5e-5ï¼‰å’Œæ­¥æ•°ï¼ˆä»1,000é™åˆ°200ï¼‰ã€‚åœ¨å‡ æ¬¡è¿è¡Œåï¼Œæˆ‘æ‰‹åŠ¨ä¼˜åŒ–äº†è¿™äº›å€¼ï¼Œä»¥ç¨³å®šè®­ç»ƒå¹¶è·å¾—æœ€ä½³ç»“æœã€‚
- en: We can now start training the model. Note that it requires an A100 GPU and takes
    between 1 hour to complete the training.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥å¼€å§‹è®­ç»ƒæ¨¡å‹äº†ã€‚è¯·æ³¨æ„ï¼Œå®ƒéœ€è¦ä¸€å—A100 GPUï¼Œå¹¶ä¸”è®­ç»ƒæ—¶é—´å¤§çº¦éœ€è¦1å°æ—¶ã€‚
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Our model is now fine-tuned. You can check the project on Weights & Biases
    [at this address](https://wandb.ai/mlabonne/NeuralHermes-2-5-Mistral-7B/runs/axe71gr0?workspace=user-mlabonne).
    Here are some interesting metrics to analyze:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„æ¨¡å‹ç°åœ¨å·²ç»å®Œæˆå¾®è°ƒã€‚ä½ å¯ä»¥åœ¨Weights & Biasesä¸ŠæŸ¥çœ‹è¯¥é¡¹ç›®ï¼Œ[åœ°å€å¦‚ä¸‹](https://wandb.ai/mlabonne/NeuralHermes-2-5-Mistral-7B/runs/axe71gr0?workspace=user-mlabonne)ã€‚è¿™é‡Œæœ‰ä¸€äº›æœ‰è¶£çš„æŒ‡æ ‡å¯ä»¥åˆ†æï¼š
- en: '![](../Images/a3622d152e976686a7e55807e80b371a.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a3622d152e976686a7e55807e80b371a.png)'
- en: Image by author
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: Interestingly, the training loss quickly drops to zero (before 50 steps), despite
    100 warmup steps. Meanwhile, the other metrics keep evolving.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰è¶£çš„æ˜¯ï¼Œè®­ç»ƒæŸå¤±è¿…é€Ÿä¸‹é™åˆ°é›¶ï¼ˆåœ¨50æ­¥ä¹‹å‰ï¼‰ï¼Œå°½ç®¡æœ‰100æ­¥çš„çƒ­èº«æ­¥éª¤ã€‚ä¸æ­¤åŒæ—¶ï¼Œå…¶ä»–æŒ‡æ ‡æŒç»­æ¼”å˜ã€‚
- en: The train/rewards/chosen and train/rewards/rejected plots correspond to the
    mean difference between the log probabilities output by the trained and reference
    models. It makes sense that, over time, they diverge as our trained model learns
    the preferred answers. The train/rewards/margins plot also shows the difference
    between these two plots. Finally, the train/reward/accuracies plot shows the frequency
    of choosing the preferred answer. The trained model quickly reaches a perfect
    accuracy score, which is a good sign but could also mean that the difference between
    preferred and rejected answers is too obvious.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: train/rewards/chosenå’Œtrain/rewards/rejectedå›¾è¡¨å¯¹åº”çš„æ˜¯è®­ç»ƒæ¨¡å‹å’Œå‚è€ƒæ¨¡å‹è¾“å‡ºçš„å¯¹æ•°æ¦‚ç‡ä¹‹é—´çš„å¹³å‡å·®å¼‚ã€‚éšç€æ—¶é—´çš„æ¨ç§»ï¼Œå®ƒä»¬çš„å·®å¼‚é€æ¸å¢å¤§ï¼Œå› ä¸ºæˆ‘ä»¬çš„è®­ç»ƒæ¨¡å‹å­¦ä¹ äº†é¦–é€‰ç­”æ¡ˆã€‚train/rewards/marginså›¾è¡¨ä¹Ÿæ˜¾ç¤ºäº†è¿™ä¸¤è€…ä¹‹é—´çš„å·®å¼‚ã€‚æœ€åï¼Œtrain/reward/accuracieså›¾è¡¨å±•ç¤ºäº†é€‰æ‹©é¦–é€‰ç­”æ¡ˆçš„é¢‘ç‡ã€‚è®­ç»ƒåçš„æ¨¡å‹è¿…é€Ÿè¾¾åˆ°äº†å®Œç¾çš„å‡†ç¡®ç‡ï¼Œè¿™è™½ç„¶æ˜¯ä¸€ä¸ªå¥½å…†å¤´ï¼Œä½†ä¹Ÿå¯èƒ½æ„å‘³ç€é¦–é€‰ç­”æ¡ˆä¸è¢«æ‹’ç»ç­”æ¡ˆä¹‹é—´çš„å·®å¼‚è¿‡äºæ˜æ˜¾ã€‚
- en: Now that itâ€™s trained, we can merge the adapter with the original model. Next,
    we save the merged model and the tokenizer before pushing it to the Hugging Face
    Hub.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ¨¡å‹å·²ç»è®­ç»ƒå®Œæˆï¼Œæˆ‘ä»¬å¯ä»¥å°†é€‚é…å™¨ä¸åŸå§‹æ¨¡å‹åˆå¹¶ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä¿å­˜åˆå¹¶åçš„æ¨¡å‹å’Œæ ‡è®°å™¨ï¼Œç„¶åå°†å…¶æ¨é€åˆ° Hugging Face Hubã€‚
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Letâ€™s see how our model performs in a real test. Weâ€™ll format the prompt to
    ask a basic question: â€œWhat is a Large Language Model?â€'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬çš„æ¨¡å‹åœ¨å®é™…æµ‹è¯•ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬å°†æ ¼å¼åŒ–æç¤ºï¼Œæå‡ºä¸€ä¸ªåŸºæœ¬é—®é¢˜ï¼šâ€œä»€ä¹ˆæ˜¯å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼Ÿâ€
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Hereâ€™s the answer from the model:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æ¨¡å‹çš„å›ç­”ï¼š
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Everything seems to be working, we can now evaluate the merged model. As this
    is a general-purpose model, we can leverage the [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)
    to evaluate it. As the process is quite resource-intensive, we can also directly
    submit it for evaluation on the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).
    It took a few days, but here are the results compared to other OpenHermes models:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€åˆ‡ä¼¼ä¹éƒ½åœ¨æ­£å¸¸å·¥ä½œï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥è¯„ä¼°åˆå¹¶åçš„æ¨¡å‹ã€‚ç”±äºè¿™æ˜¯ä¸€ä¸ªé€šç”¨æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨ [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)
    æ¥è¯„ä¼°å®ƒã€‚ç”±äºè¿™ä¸ªè¿‡ç¨‹ç›¸å½“èµ„æºå¯†é›†ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ç›´æ¥å°†å…¶æäº¤åˆ° [Open LLM æ’è¡Œæ¦œ](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)è¿›è¡Œè¯„ä¼°ã€‚è™½ç„¶èŠ±è´¹äº†å‡ å¤©æ—¶é—´ï¼Œä½†è¿™é‡Œæ˜¯ä¸å…¶ä»–
    OpenHermes æ¨¡å‹çš„å¯¹æ¯”ç»“æœï¼š
- en: '![](../Images/9e7444bfe5a4b31c4a4f1df2ed935365.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e7444bfe5a4b31c4a4f1df2ed935365.png)'
- en: Image by author
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: Compared to the original model, NeuralHermes-2â€“5-Mistral-7B model improved the
    average score by 6.7 points (particularly on GSM8K). This is an unexpectedly large
    improvement, which showcases the power of Direct Preference Optimization.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸åŸå§‹æ¨¡å‹ç›¸æ¯”ï¼ŒNeuralHermes-2.5-Mistral-7B æ¨¡å‹å°†å¹³å‡å¾—åˆ†æé«˜äº† 6.7 åˆ†ï¼ˆå°¤å…¶æ˜¯åœ¨ GSM8K ä¸Šï¼‰ã€‚è¿™æ˜¯ä¸€æ¬¡å‡ºä¹æ„æ–™çš„å·¨å¤§æå‡ï¼Œå±•ç¤ºäº†ç›´æ¥åå¥½ä¼˜åŒ–çš„å¼ºå¤§åŠ›é‡ã€‚
- en: Conclusion
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: In this article, we fine-tuned an already supervised fine-tuned model using
    DPO and created our own [NeuralHermes-2.5](https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B)
    model. By leveraging a high-quality preference dataset, we created a sample-efficient
    fine-tuning pipeline that produced a significant improvement on the Open LLM Leaderboard.
    If you want to give it a try, you can find quantized variants of this model or
    use this [Hugging Face Space](https://huggingface.co/spaces/zhangtao103239/NeuralHermes-2.5-Mistral-7B-GGUF-Chat).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ DPO å¾®è°ƒäº†ä¸€ä¸ªå·²ç»ç»è¿‡ç›‘ç£å¾®è°ƒçš„æ¨¡å‹ï¼Œå¹¶åˆ›å»ºäº†æˆ‘ä»¬è‡ªå·±çš„ [NeuralHermes-2.5](https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B)
    æ¨¡å‹ã€‚é€šè¿‡åˆ©ç”¨é«˜è´¨é‡çš„åå¥½æ•°æ®é›†ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªé«˜æ•ˆçš„å¾®è°ƒæµç¨‹ï¼Œå¹¶åœ¨ Open LLM æ’è¡Œæ¦œä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚å¦‚æœä½ æƒ³å°è¯•ï¼Œå¯ä»¥æ‰¾åˆ°è¿™ä¸ªæ¨¡å‹çš„é‡åŒ–å˜ä½“ï¼Œæˆ–ä½¿ç”¨è¿™ä¸ª
    [Hugging Face Space](https://huggingface.co/spaces/zhangtao103239/NeuralHermes-2.5-Mistral-7B-GGUF-Chat)ã€‚
- en: Note that our fine-tuning pipeline can still be improved in different ways.
    For example, the preference dataset is still quite raw and could be improved with
    more filtering and by using different models. In addition, numerous hyperparameters
    can still be tweaked to achieve better results. In particular, the learning rate
    can still be lowered to train the model on more steps and inject more preference
    data.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæˆ‘ä»¬çš„å¾®è°ƒæµç¨‹ä»ç„¶å¯ä»¥é€šè¿‡ä¸åŒçš„æ–¹å¼è¿›è¡Œæ”¹è¿›ã€‚ä¾‹å¦‚ï¼Œåå¥½æ•°æ®é›†ä»ç„¶ç›¸å½“åŸå§‹ï¼Œå¯ä»¥é€šè¿‡æ›´å¤šçš„è¿‡æ»¤å’Œä½¿ç”¨ä¸åŒçš„æ¨¡å‹æ¥æ”¹è¿›ã€‚æ­¤å¤–ï¼Œè®¸å¤šè¶…å‚æ•°ä»ç„¶å¯ä»¥è¿›è¡Œè°ƒæ•´ï¼Œä»¥è·å¾—æ›´å¥½çš„ç»“æœã€‚ç‰¹åˆ«æ˜¯ï¼Œå­¦ä¹ ç‡ä»ç„¶å¯ä»¥é™ä½ï¼Œä»¥ä¾¿åœ¨æ›´å¤šçš„æ­¥éª¤ä¸Šè®­ç»ƒæ¨¡å‹å¹¶æ³¨å…¥æ›´å¤šçš„åå¥½æ•°æ®ã€‚
- en: References
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[Fine-tune Llama 2 with DPO](https://huggingface.co/blog/dpo-trl) by Kashif
    Rasul, Younes Belkada, and Leandro von Werra.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[é€šè¿‡ DPO å¾®è°ƒ Llama 2](https://huggingface.co/blog/dpo-trl) ä½œè€…ï¼šKashif Rasulã€Younes
    Belkada å’Œ Leandro von Werraã€‚'
- en: '[Supervised Fine-Tuning and Direct Preference Optimization on Intel Gaudi2](https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3)
    by Kaokao Lv, Wenxin Zhang, and Haihao Shen.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[åœ¨ Intel Gaudi2 ä¸Šçš„ç›‘ç£å¾®è°ƒå’Œç›´æ¥åå¥½ä¼˜åŒ–](https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3)
    ä½œè€…ï¼šKaokao Lvã€Wenxin Zhang å’Œ Haihao Shenã€‚'
- en: '[llama2-fine-tune](https://github.com/mzbac/llama2-fine-tune) by mzbac.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[llama2-fine-tune](https://github.com/mzbac/llama2-fine-tune) ä½œè€…ï¼šmzbacã€‚'
- en: '*Learn more about machine learning and support my work with one click â€” become
    a Medium member here:*'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*äº†è§£æ›´å¤šå…³äºæœºå™¨å­¦ä¹ çš„çŸ¥è¯†ï¼Œå¹¶é€šè¿‡ä¸€æ¬¡ç‚¹å‡»æ”¯æŒæˆ‘çš„å·¥ä½œ â€”â€” åœ¨è¿™é‡Œæˆä¸º Medium ä¼šå‘˜ï¼š*'
- en: '[](https://medium.com/@mlabonne/membership?source=post_page-----708042745aac--------------------------------)
    [## Join Medium with my referral link - Maxime Labonne'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne/membership?source=post_page-----708042745aac--------------------------------)
    [## é€šè¿‡æˆ‘çš„æ¨èé“¾æ¥åŠ å…¥ Medium - Maxime Labonne'
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every storyâ€¦
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½œä¸º Medium çš„ä¼šå‘˜ï¼Œä½ çš„ä¸€éƒ¨åˆ†ä¼šå‘˜è´¹ç”¨ä¼šåˆ†é…ç»™ä½ é˜…è¯»çš„ä½œè€…ï¼Œå¹¶ä¸”ä½ å¯ä»¥å®Œå…¨è®¿é—®æ¯ç¯‡æ•…äº‹â€¦
- en: medium.com](https://medium.com/@mlabonne/membership?source=post_page-----708042745aac--------------------------------)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@mlabonne/membership?source=post_page-----708042745aac--------------------------------)
