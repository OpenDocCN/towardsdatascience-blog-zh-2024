- en: 'Automatic Differentiation (AutoDiff): A Brief Intro with Examples'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动微分（AutoDiff）：带有示例的简要介绍
- en: 原文：[https://towardsdatascience.com/automatic-differentiation-autodiff-a-brief-intro-with-examples-3f3d257ffe3b?source=collection_archive---------2-----------------------#2024-10-11](https://towardsdatascience.com/automatic-differentiation-autodiff-a-brief-intro-with-examples-3f3d257ffe3b?source=collection_archive---------2-----------------------#2024-10-11)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/automatic-differentiation-autodiff-a-brief-intro-with-examples-3f3d257ffe3b?source=collection_archive---------2-----------------------#2024-10-11](https://towardsdatascience.com/automatic-differentiation-autodiff-a-brief-intro-with-examples-3f3d257ffe3b?source=collection_archive---------2-----------------------#2024-10-11)
- en: An introduction to the mechanics of AutoDiff, exploring its mathematical principles,
    implementation strategies, and applications in currently most-used frameworks
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍了自动微分（AutoDiff）的机制，探索其数学原理、实现策略以及在当前最常用框架中的应用
- en: '[](https://ebrahimpichka.medium.com/?source=post_page---byline--3f3d257ffe3b--------------------------------)[![Ebrahim
    Pichka](../Images/8add6e8e875d9e921caf7f5eaa77d545.png)](https://ebrahimpichka.medium.com/?source=post_page---byline--3f3d257ffe3b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3f3d257ffe3b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3f3d257ffe3b--------------------------------)
    [Ebrahim Pichka](https://ebrahimpichka.medium.com/?source=post_page---byline--3f3d257ffe3b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ebrahimpichka.medium.com/?source=post_page---byline--3f3d257ffe3b--------------------------------)[![Ebrahim
    Pichka](../Images/8add6e8e875d9e921caf7f5eaa77d545.png)](https://ebrahimpichka.medium.com/?source=post_page---byline--3f3d257ffe3b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3f3d257ffe3b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3f3d257ffe3b--------------------------------)
    [Ebrahim Pichka](https://ebrahimpichka.medium.com/?source=post_page---byline--3f3d257ffe3b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3f3d257ffe3b--------------------------------)
    ·10 min read·Oct 11, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3f3d257ffe3b--------------------------------)
    ·阅读时长 10 分钟·2024年10月11日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d9fde8b68c1981541a77cf1b1c104ec6.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d9fde8b68c1981541a77cf1b1c104ec6.png)'
- en: Photo by [Bozhin Karaivanov](https://unsplash.com/@bkaraivanov?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [Bozhin Karaivanov](https://unsplash.com/@bkaraivanov?utm_source=medium&utm_medium=referral)
    于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: The Fundamental Role of Differentiation in Modern Machine Learning Optimization
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微分在现代机器学习优化中的基础作用
- en: 'At the heart of machine learning lies the optimization of loss/objective functions.
    This optimization process heavily relies on computing gradients of these functions
    with respect to model parameters. As Baydin et al. (2018) elucidate in their comprehensive
    survey [1], these gradients guide the iterative updates in optimization algorithms
    such as stochastic gradient descent (SGD):'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的核心在于优化损失/目标函数。这个优化过程在很大程度上依赖于计算这些函数相对于模型参数的梯度。正如Baydin等人（2018年）在他们的全面调查中阐明的那样[1]，这些梯度引导了优化算法中的迭代更新，比如随机梯度下降（SGD）：
- en: '*θₜ₊₁ = θₜ - α ∇θ L(θₜ)*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*θₜ₊₁ = θₜ - α ∇θ L(θₜ)*'
- en: 'Where:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: θₜ represents the model parameters at step t
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: θₜ 表示步骤 t 时的模型参数
- en: α is the learning rate
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: α 是学习率
- en: ∇_θ L(θₜ) denotes the gradient of the loss function L with respect to the parameters
    θ
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ∇_θ L(θₜ) 表示损失函数 L 相对于参数 θ 的梯度
- en: This simple update rule belies the complexity of computing gradients in deep
    neural networks with millions or even billions of parameters.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的更新规则掩盖了在拥有数百万甚至数十亿参数的深度神经网络中计算梯度的复杂性。
- en: 2\. The Differentiation Triad
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 微分三位一体
