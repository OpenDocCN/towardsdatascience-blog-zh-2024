- en: 'AdaBoost Classifier, Explained: A Visual Guide with Code Examples'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AdaBoost åˆ†ç±»å™¨è§£æï¼šå¸¦ä»£ç ç¤ºä¾‹çš„å¯è§†åŒ–æŒ‡å—
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/adaboost-classifier-explained-a-visual-guide-with-code-examples-fc0f25326d7b?source=collection_archive---------1-----------------------#2024-11-10](https://towardsdatascience.com/adaboost-classifier-explained-a-visual-guide-with-code-examples-fc0f25326d7b?source=collection_archive---------1-----------------------#2024-11-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/adaboost-classifier-explained-a-visual-guide-with-code-examples-fc0f25326d7b?source=collection_archive---------1-----------------------#2024-11-10](https://towardsdatascience.com/adaboost-classifier-explained-a-visual-guide-with-code-examples-fc0f25326d7b?source=collection_archive---------1-----------------------#2024-11-10)
- en: ENSEMBLE LEARNING
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é›†æˆå­¦ä¹ 
- en: '**Putting the weight where weak learners need it most**'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**å°†æƒé‡æ”¾åœ¨å¼±å­¦ä¹ å™¨æœ€éœ€è¦çš„åœ°æ–¹**'
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--fc0f25326d7b--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--fc0f25326d7b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fc0f25326d7b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fc0f25326d7b--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--fc0f25326d7b--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@samybaladram?source=post_page---byline--fc0f25326d7b--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--fc0f25326d7b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fc0f25326d7b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fc0f25326d7b--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--fc0f25326d7b--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fc0f25326d7b--------------------------------)
    Â·11 min readÂ·Nov 10, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fc0f25326d7b--------------------------------)
    Â·é˜…è¯»æ—¶é•¿11åˆ†é’ŸÂ·2024å¹´11æœˆ10æ—¥
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](/random-forest-explained-a-visual-guide-with-code-examples-9f736a6e1b3c?source=post_page-----fc0f25326d7b--------------------------------)
    [## Random Forest, Explained: A Visual Guide with Code Examples'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/random-forest-explained-a-visual-guide-with-code-examples-9f736a6e1b3c?source=post_page-----fc0f25326d7b--------------------------------)
    [## éšæœºæ£®æ—è§£æï¼šå¸¦ä»£ç ç¤ºä¾‹çš„å¯è§†åŒ–æŒ‡å—'
- en: Making tree-mendous predictions with random trees
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åˆ©ç”¨éšæœºæ ‘åšå‡ºæƒŠäººçš„é¢„æµ‹
- en: towardsdatascience.com](/random-forest-explained-a-visual-guide-with-code-examples-9f736a6e1b3c?source=post_page-----fc0f25326d7b--------------------------------)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/random-forest-explained-a-visual-guide-with-code-examples-9f736a6e1b3c?source=post_page-----fc0f25326d7b--------------------------------)
- en: 'Everyone makes mistakes â€” even the simplest [decision trees](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e)
    in machine learning. Instead of ignoring them, AdaBoost (Adaptive Boosting) algorithm
    does something different: it learns (or *adapts*) from these mistakes to get better.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªäººéƒ½ä¼šçŠ¯é”™â€”â€”å³ä¾¿æ˜¯æœ€ç®€å•çš„[å†³ç­–æ ‘](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e)ä¹Ÿä¼šåœ¨æœºå™¨å­¦ä¹ ä¸­å‡ºé”™ã€‚AdaBoostï¼ˆè‡ªé€‚åº”æå‡ï¼‰ç®—æ³•å¹¶æ²¡æœ‰å¿½è§†è¿™äº›é”™è¯¯ï¼Œè€Œæ˜¯åšäº†ä¸åŒçš„äº‹æƒ…ï¼šå®ƒä»è¿™äº›é”™è¯¯ä¸­å­¦ä¹ ï¼ˆæˆ–*é€‚åº”*ï¼‰ï¼Œä¸æ–­æå‡ã€‚
- en: Unlike [Random Fores](/random-forest-explained-a-visual-guide-with-code-examples-9f736a6e1b3c)t,
    which makes many trees at once, AdaBoost starts with a single, simple tree and
    identifies the instances it misclassifies. It then builds new trees to fix those
    errors, learning from its mistakes and getting better with each step.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸[éšæœºæ£®æ—](/random-forest-explained-a-visual-guide-with-code-examples-9f736a6e1b3c)ä¸åŒï¼Œéšæœºæ£®æ—æ˜¯åŒæ—¶æ„å»ºå¤šæ£µæ ‘ï¼Œè€ŒAdaBooståˆ™ä»ä¸€æ£µç®€å•çš„æ ‘å¼€å§‹ï¼Œè¯†åˆ«å®ƒè¯¯åˆ†ç±»çš„å®ä¾‹ã€‚ç„¶åï¼Œå®ƒæ„å»ºæ–°çš„æ ‘æ¥ä¿®æ­£è¿™äº›é”™è¯¯ï¼Œè¾¹å­¦ä¹ è¾¹æ”¹è¿›ã€‚
- en: Here, weâ€™ll illustrate exactly how AdaBoost makes its predictions, building
    strength by combining targeted weak learners just like a workout routine that
    turns focused exercises into full-body power.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†å…·ä½“è¯´æ˜AdaBoostæ˜¯å¦‚ä½•è¿›è¡Œé¢„æµ‹çš„ï¼Œé€šè¿‡ç»“åˆå¤šä¸ªé’ˆå¯¹æ€§çš„å¼±å­¦ä¹ å™¨é€æ­¥å¢å¼ºå…¶é¢„æµ‹èƒ½åŠ›ï¼Œå°±åƒä¸€é¡¹å°†ä¸“æ³¨çš„é”»ç‚¼è½¬åŒ–ä¸ºå…¨èº«åŠ›é‡çš„è®­ç»ƒè®¡åˆ’ã€‚
- en: '![](../Images/38e85c5674011760cafd45ee4d7c85c0.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38e85c5674011760cafd45ee4d7c85c0.png)'
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰å¯è§†åŒ–å›¾åƒï¼šä½œè€…ä½¿ç”¨Canva Proåˆ›å»ºï¼Œå·²é’ˆå¯¹æ‰‹æœºä¼˜åŒ–ï¼›åœ¨æ¡Œé¢ä¸Šå¯èƒ½æ˜¾ç¤ºä¸ºè¿‡å¤§ã€‚
- en: Definition
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®šä¹‰
- en: AdaBoost is an ensemble machine learning model that creates a sequence of weighted
    decision trees, typically using shallow trees (often just single-level â€œstumpsâ€).
    Each tree is trained on the entire dataset, but with adaptive sample weights that
    give more importance to previously misclassified examples.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost æ˜¯ä¸€ç§é›†æˆæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œé€šè¿‡åŠ æƒçš„å†³ç­–æ ‘åºåˆ—æ¥åˆ›å»ºæ¨¡å‹ï¼Œé€šå¸¸ä½¿ç”¨æµ…å±‚æ ‘ï¼ˆé€šå¸¸æ˜¯å•å±‚â€œæ ‘æ¡©â€ï¼‰ã€‚æ¯æ£µæ ‘éƒ½åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†ä½¿ç”¨è‡ªé€‚åº”çš„æ ·æœ¬æƒé‡ï¼Œè¿™äº›æƒé‡ç»™ä¸ä¹‹å‰è¢«é”™è¯¯åˆ†ç±»çš„æ ·æœ¬æ›´é«˜çš„ä¼˜å…ˆçº§ã€‚
- en: For classification tasks, AdaBoost combines the trees through a weighted voting
    system, where better-performing trees get more influence in the final decision.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºåˆ†ç±»ä»»åŠ¡ï¼ŒAdaBoost é€šè¿‡åŠ æƒæŠ•ç¥¨ç³»ç»Ÿå°†æ ‘ç»“åˆèµ·æ¥ï¼Œè¡¨ç°æ›´å¥½çš„æ ‘åœ¨æœ€ç»ˆå†³ç­–ä¸­æœ‰æ›´å¤§çš„å½±å“åŠ›ã€‚
- en: The modelâ€™s strength comes from its adaptive learning process â€” while each simple
    tree might be a â€œweak learnerâ€ that performs only slightly better than random
    guessing, the weighted combination of trees creates a â€œstrong learnerâ€ that **progressively
    focuses on and corrects mistakes**.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹çš„ä¼˜åŠ¿æ¥è‡ªäºå…¶è‡ªé€‚åº”å­¦ä¹ è¿‡ç¨‹â€”â€”å°½ç®¡æ¯æ£µç®€å•çš„æ ‘å¯èƒ½æ˜¯ä¸€ä¸ªâ€œå¼±å­¦ä¹ å™¨â€ï¼Œå…¶è¡¨ç°ä»…æ¯”éšæœºçŒœæµ‹ç¨å¥½ï¼Œä½†æ ‘çš„åŠ æƒç»„åˆåˆ›å»ºäº†ä¸€ä¸ªâ€œå¼ºå­¦ä¹ å™¨â€ï¼Œå®ƒ**é€æ­¥èšç„¦å¹¶ä¿®æ­£é”™è¯¯**ã€‚
- en: '![](../Images/5860f3e88183ae0db9f76a78c52525eb.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5860f3e88183ae0db9f76a78c52525eb.png)'
- en: AdaBoost is part of the boosting family of algorithms because it builds trees
    one at a time. Each new tree tries to fix the mistakes made by the previous trees.
    It then uses a weighted vote to combine their answers and make its final prediction.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost å±äºæå‡ç®—æ³•å®¶æ—çš„ä¸€éƒ¨åˆ†ï¼Œå› ä¸ºå®ƒä¸€æ¬¡æ„å»ºä¸€æ£µæ ‘ã€‚æ¯æ£µæ–°æ ‘å°è¯•ä¿®æ­£ä¹‹å‰æ ‘çš„é”™è¯¯ã€‚ç„¶åï¼Œå®ƒé€šè¿‡åŠ æƒæŠ•ç¥¨å°†å„æ£µæ ‘çš„ç»“æœç»“åˆèµ·æ¥ï¼Œåšå‡ºæœ€ç»ˆçš„é¢„æµ‹ã€‚
- en: Dataset Used
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨çš„æ•°æ®é›†
- en: Throughout this article, weâ€™ll focus on the classic golf dataset as an example
    for classification.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ä»¥ç»å…¸çš„é«˜å°”å¤«æ•°æ®é›†ä½œä¸ºåˆ†ç±»çš„ç¤ºä¾‹ã€‚
- en: '![](../Images/05586d1dcea17f8a18206b58019181ea.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05586d1dcea17f8a18206b58019181ea.png)'
- en: 'Columns: â€˜Outlook (one-hot-encoded into 3 columns)â€™, â€™Temperatureâ€™ (in Fahrenheit),
    â€˜Humidityâ€™ (in %), â€˜Windyâ€™ (Yes/No) and â€˜Playâ€™ (Yes/No, target feature)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ—ï¼šâ€˜Outlookï¼ˆç»è¿‡ç‹¬çƒ­ç¼–ç ä¸º3åˆ—ï¼‰â€™ï¼Œâ€˜Temperatureâ€™ï¼ˆä»¥åæ°åº¦è¡¨ç¤ºï¼‰ï¼Œâ€˜Humidityâ€™ï¼ˆä»¥ç™¾åˆ†æ¯”è¡¨ç¤ºï¼‰ï¼Œâ€˜Windyâ€™ï¼ˆæ˜¯/å¦ï¼‰å’Œâ€˜Playâ€™ï¼ˆæ˜¯/å¦ï¼Œç›®æ ‡ç‰¹å¾ï¼‰
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Main Mechanism
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸»è¦æœºåˆ¶
- en: 'Hereâ€™s how AdaBoost works:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æ˜¯ AdaBoost çš„å·¥ä½œåŸç†ï¼š
- en: '**Initialize Weights:** Assign equal weight to each training example.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åˆå§‹åŒ–æƒé‡ï¼š** ä¸ºæ¯ä¸ªè®­ç»ƒæ ·æœ¬åˆ†é…ç›¸ç­‰çš„æƒé‡ã€‚'
- en: '**Iterative Learning:** In each step, a simple decision tree is trained and
    its performance is checked. Misclassified examples get more weight, making them
    a priority for the next tree. Correctly classified examples stay the same, and
    all weights are adjusted to add up to 1.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è¿­ä»£å­¦ä¹ ï¼š** åœ¨æ¯ä¸€æ­¥ï¼Œè®­ç»ƒä¸€æ£µç®€å•çš„å†³ç­–æ ‘å¹¶æ£€æŸ¥å…¶è¡¨ç°ã€‚è¢«é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å°†è·å¾—æ›´å¤šçš„æƒé‡ï¼Œä½¿å…¶æˆä¸ºä¸‹ä¸€æ£µæ ‘çš„ä¼˜å…ˆè€ƒè™‘å¯¹è±¡ã€‚æ­£ç¡®åˆ†ç±»çš„æ ·æœ¬ä¿æŒä¸å˜ï¼Œæ‰€æœ‰æƒé‡éƒ½ä¼šè°ƒæ•´ï¼Œä»¥ç¡®ä¿å®ƒä»¬çš„æ€»å’Œä¸º1ã€‚'
- en: '**Build Weak Learners:** Each new, simple tree targets the mistakes of the
    previous ones, creating a sequence of specialized weak learners.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ„å»ºå¼±å­¦ä¹ å™¨ï¼š** æ¯æ£µæ–°çš„ç®€å•æ ‘éƒ½é’ˆå¯¹ä¹‹å‰æ ‘çš„é”™è¯¯è¿›è¡Œä¿®æ­£ï¼Œåˆ›å»ºäº†ä¸€ç³»åˆ—ä¸“é—¨åŒ–çš„å¼±å­¦ä¹ å™¨ã€‚'
- en: '**Final Prediction:** Combine all trees through weighted voting, where each
    treeâ€™s vote is based on its importance value, giving more influence to more accurate
    trees.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æœ€ç»ˆé¢„æµ‹ï¼š** é€šè¿‡åŠ æƒæŠ•ç¥¨å°†æ‰€æœ‰æ ‘çš„ç»“æœç»“åˆèµ·æ¥ï¼Œæ¯æ£µæ ‘çš„æŠ•ç¥¨åŸºäºå…¶é‡è¦æ€§å€¼ï¼Œæ›´å‡†ç¡®çš„æ ‘ä¼šæœ‰æ›´å¤§çš„å½±å“åŠ›ã€‚'
- en: '![](../Images/0e77ad190e2d8b5466b6debd4ecff4cf.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e77ad190e2d8b5466b6debd4ecff4cf.png)'
- en: An AdaBoost Classifier makes predictions by using many simple decision trees
    (usually 50â€“100). Each tree, called a â€œstump,â€ focuses on one important feature,
    like temperature or humidity. The final prediction is made by combining all the
    treesâ€™ votes, each weighted by how important that tree is (â€œalphaâ€).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost åˆ†ç±»å™¨é€šè¿‡ä½¿ç”¨è®¸å¤šç®€å•çš„å†³ç­–æ ‘ï¼ˆé€šå¸¸æ˜¯50åˆ°100æ£µæ ‘ï¼‰æ¥è¿›è¡Œé¢„æµ‹ã€‚æ¯æ£µæ ‘ï¼Œç§°ä¸ºâ€œæ ‘æ¡©â€ï¼Œä¸“æ³¨äºä¸€ä¸ªé‡è¦ç‰¹å¾ï¼Œå¦‚æ¸©åº¦æˆ–æ¹¿åº¦ã€‚æœ€ç»ˆçš„é¢„æµ‹æ˜¯é€šè¿‡ç»“åˆæ‰€æœ‰æ ‘çš„æŠ•ç¥¨æ¥åšå‡ºçš„ï¼Œæ¯æ£µæ ‘çš„æŠ•ç¥¨æ ¹æ®è¯¥æ ‘çš„é‡è¦æ€§ï¼ˆâ€œalphaâ€ï¼‰åŠ æƒã€‚
- en: Training Steps
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ­¥éª¤
- en: Here, weâ€™ll follow the SAMME (Stagewise Additive Modeling using a Multi-class
    Exponential loss function) algorithm, the standard approach in scikit-learn that
    handles both binary and multi-class classification.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†éµå¾ª SAMMEï¼ˆä½¿ç”¨å¤šç±»æŒ‡æ•°æŸå¤±å‡½æ•°çš„é˜¶æ®µæ€§åŠ æ³•å»ºæ¨¡ï¼‰ç®—æ³•ï¼Œè¿™æ˜¯ scikit-learn ä¸­çš„æ ‡å‡†æ–¹æ³•ï¼Œèƒ½å¤Ÿå¤„ç†äºŒåˆ†ç±»å’Œå¤šåˆ†ç±»é—®é¢˜ã€‚
- en: 1.1\. Decide the weak learner to be used. A one-level decision tree (or â€œstumpâ€)
    is the default choice.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 1.1\. å†³å®šè¦ä½¿ç”¨çš„å¼±å­¦ä¹ å™¨ã€‚é»˜è®¤é€‰æ‹©æ˜¯ä¸€æ£µå•å±‚å†³ç­–æ ‘ï¼ˆæˆ–ç§°â€œæ ‘æ¡©â€ï¼‰ã€‚
- en: 1.2\. Decide how many weak learner (in this case the number of trees) you want
    to build (the default is 50 trees).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 1.2\. å†³å®šè¦æ„å»ºå¤šå°‘ä¸ªå¼±å­¦ä¹ å™¨ï¼ˆåœ¨è¿™ä¸ªä¾‹å­ä¸­æ˜¯æ ‘çš„æ•°é‡ï¼Œé»˜è®¤æ˜¯50æ£µæ ‘ï¼‰ã€‚
- en: '![](../Images/9e914ecac19ae6b4d06dffb719872b0c.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e914ecac19ae6b4d06dffb719872b0c.png)'
- en: We begin with depth-1 decision trees (stumps) as our weak learners. Each stump
    makes just one split, and weâ€™ll train 50 of them sequentially, adjusting weights
    along the way.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»æ·±åº¦ä¸º1çš„å†³ç­–æ ‘ï¼ˆæ ‘æ¡©ï¼‰å¼€å§‹ä½œä¸ºæˆ‘ä»¬çš„å¼±å­¦ä¹ å™¨ã€‚æ¯ä¸ªæ ‘æ¡©ä»…åšä¸€æ¬¡åˆ†è£‚ï¼Œæˆ‘ä»¬å°†é¡ºåºè®­ç»ƒ50ä¸ªæ ‘æ¡©ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è°ƒæ•´æƒé‡ã€‚
- en: '1.3\. Start by giving each training example equal weight:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 1.3\. ä»ç»™æ¯ä¸ªè®­ç»ƒç¤ºä¾‹ç›¸åŒçš„æƒé‡å¼€å§‹ï¼š
- en: Â· Each sample gets weight = 1/*N* (*N* is total number of samples)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Â· æ¯ä¸ªæ ·æœ¬çš„æƒé‡ = 1/*N*ï¼ˆ*N* æ˜¯æ ·æœ¬æ€»æ•°ï¼‰
- en: Â· All weights together sum to 1
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Â· æ‰€æœ‰æƒé‡åŠ èµ·æ¥ç­‰äº1
- en: '![](../Images/a10a476d7edac1311aefbcf5e3b50dc4.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a10a476d7edac1311aefbcf5e3b50dc4.png)'
- en: All data points start with equal weights (0.0714), with the total weight adding
    up to 1\. This ensures every example is equally important when training begins.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰æ•°æ®ç‚¹å¼€å§‹æ—¶çš„æƒé‡ç›¸åŒï¼ˆ0.0714ï¼‰ï¼Œæ€»æƒé‡åŠ èµ·æ¥ä¸º1ã€‚è¿™ç¡®ä¿äº†åœ¨è®­ç»ƒå¼€å§‹æ—¶ï¼Œæ¯ä¸ªç¤ºä¾‹éƒ½æ˜¯åŒç­‰é‡è¦çš„ã€‚
- en: For the First Tree
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯¹äºç¬¬ä¸€æ£µæ ‘
- en: 2.1\. Build a decision stump while considering sample weights
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 2.1\. åœ¨è€ƒè™‘æ ·æœ¬æƒé‡çš„æƒ…å†µä¸‹æ„å»ºå†³ç­–æ ‘æ¡©
- en: '![](../Images/68a18d476059d011467b9290fe6574e7.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/68a18d476059d011467b9290fe6574e7.png)'
- en: Before making the first split, the algorithm examines all data points with their
    weights to find the best splitting point. These weights influence how important
    each example is in making the split decision.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿›è¡Œç¬¬ä¸€æ¬¡åˆ†è£‚ä¹‹å‰ï¼Œç®—æ³•ä¼šæ£€æŸ¥æ‰€æœ‰æ•°æ®ç‚¹åŠå…¶æƒé‡ï¼Œä»¥æ‰¾åˆ°æœ€ä½³çš„åˆ†è£‚ç‚¹ã€‚è¿™äº›æƒé‡å½±å“æ¯ä¸ªç¤ºä¾‹åœ¨åšå‡ºåˆ†è£‚å†³ç­–æ—¶çš„é‡è¦æ€§ã€‚
- en: a. Calculate initial weighted Gini impurity for the root node
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: a. è®¡ç®—æ ¹èŠ‚ç‚¹çš„åˆå§‹åŠ æƒåŸºå°¼ä¸çº¯åº¦
- en: '![](../Images/f8ad9f1f34d4f6cf8aa4ffa1096cec83.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8ad9f1f34d4f6cf8aa4ffa1096cec83.png)'
- en: The algorithm calculates the Gini impurity score at the root node, but now considers
    the weights of all data points.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ç®—æ³•è®¡ç®—æ ¹èŠ‚ç‚¹çš„åŸºå°¼ä¸çº¯åº¦å¾—åˆ†ï¼Œä½†ç°åœ¨è€ƒè™‘äº†æ‰€æœ‰æ•°æ®ç‚¹çš„æƒé‡ã€‚
- en: 'b. For each feature:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: b. å¯¹æ¯ä¸ªç‰¹å¾ï¼š
- en: Â· Sort data by feature values (exactly like in [Decision Tree](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e)
    classifier)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Â· æŒ‰ç‰¹å¾å€¼å¯¹æ•°æ®è¿›è¡Œæ’åºï¼ˆä¸[å†³ç­–æ ‘](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e)åˆ†ç±»å™¨ä¸­çš„æ“ä½œå®Œå…¨ç›¸åŒï¼‰
- en: '![](../Images/b62db5393a1ed6bdb676b65a3b9207dc.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b62db5393a1ed6bdb676b65a3b9207dc.png)'
- en: For each feature, the algorithm sorts the data and identifies potential split
    points, exactly like the standard Decision Tree.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªç‰¹å¾ï¼Œç®—æ³•å¯¹æ•°æ®è¿›è¡Œæ’åºå¹¶è¯†åˆ«æ½œåœ¨çš„åˆ†è£‚ç‚¹ï¼Œå®Œå…¨ç±»ä¼¼äºæ ‡å‡†çš„å†³ç­–æ ‘ã€‚
- en: 'Â· For each possible split point:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Â· å¯¹æ¯ä¸ªå¯èƒ½çš„åˆ†è£‚ç‚¹ï¼š
- en: Â·Â· Split samples into left and right groups
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Â·Â· å°†æ ·æœ¬åˆ†ä¸ºå·¦ç»„å’Œå³ç»„
- en: Â·Â· Calculate weighted Gini impurity for both groups
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Â·Â· è®¡ç®—ä¸¤ä¸ªç»„çš„åŠ æƒåŸºå°¼ä¸çº¯åº¦
- en: Â·Â· Calculate weighted Gini impurity reduction for this split
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Â·Â· è®¡ç®—æ­¤åˆ†è£‚çš„åŠ æƒåŸºå°¼ä¸çº¯åº¦å‡å°‘é‡
- en: '![](../Images/a825f39a5d40c52a1a8255495e984c53.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a825f39a5d40c52a1a8255495e984c53.png)'
- en: The algorithm calculates weighted Gini impurity for each potential split and
    compares it to the parent node. For feature â€œsunnyâ€ with split point 0.5, this
    impurity reduction (0.066) shows how much this split improves the data separation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ç®—æ³•è®¡ç®—æ¯ä¸ªæ½œåœ¨åˆ†è£‚ç‚¹çš„åŠ æƒåŸºå°¼ä¸çº¯åº¦ï¼Œå¹¶å°†å…¶ä¸çˆ¶èŠ‚ç‚¹è¿›è¡Œæ¯”è¾ƒã€‚å¯¹äºç‰¹å¾â€œsunnyâ€ï¼ˆåˆ†è£‚ç‚¹ä¸º0.5ï¼‰ï¼Œè¯¥ä¸çº¯åº¦å‡å°‘é‡ï¼ˆ0.066ï¼‰æ˜¾ç¤ºäº†æ­¤åˆ†è£‚å¦‚ä½•æ”¹å–„æ•°æ®çš„åˆ†ç¦»ã€‚
- en: c. Pick the split that gives the largest Gini impurity reduction
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: c. é€‰æ‹©èƒ½å¤Ÿå¸¦æ¥æœ€å¤§åŸºå°¼ä¸çº¯åº¦å‡å°‘çš„åˆ†è£‚ç‚¹
- en: '![](../Images/5a4349476737e871ffda440a8e995b28.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a4349476737e871ffda440a8e995b28.png)'
- en: After checking all possible splits across features, the column â€˜overcastâ€™ (with
    split point 0.5) gives the highest impurity reduction of 0.102\. This means itâ€™s
    the most effective way to separate the classes, making it the best choice for
    the first split.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ£€æŸ¥æ‰€æœ‰å¯èƒ½çš„ç‰¹å¾åˆ†è£‚åï¼Œâ€˜overcastâ€™åˆ—ï¼ˆåˆ†è£‚ç‚¹ä¸º0.5ï¼‰æä¾›äº†æœ€é«˜çš„ä¸çº¯åº¦å‡å°‘ï¼ˆ0.102ï¼‰ã€‚è¿™æ„å‘³ç€å®ƒæ˜¯æœ€æœ‰æ•ˆçš„åˆ†ç±»åˆ†ç¦»æ–¹å¼ï¼Œå› æ­¤æˆä¸ºç¬¬ä¸€æ¬¡åˆ†è£‚çš„æœ€ä½³é€‰æ‹©ã€‚
- en: d. Create a simple one-split tree using this decision
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: d. ä½¿ç”¨è¿™ä¸ªå†³ç­–åˆ›å»ºä¸€ä¸ªç®€å•çš„å•åˆ†è£‚æ ‘
- en: '![](../Images/e9cf90d6e8fe547e552f0af06cc220b8.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e9cf90d6e8fe547e552f0af06cc220b8.png)'
- en: Using the best split point found, the algorithm divides the data into two groups,
    each keeping their original weights. This simple decision tree is purposely kept
    small and imperfect, making it just slightly better than random guessing.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ‰¾åˆ°çš„æœ€ä½³åˆ†è£‚ç‚¹ï¼Œç®—æ³•å°†æ•°æ®åˆ†ä¸ºä¸¤ä¸ªç»„ï¼Œæ¯ä¸ªç»„ä¿æŒå…¶åŸå§‹æƒé‡ã€‚è¿™ä¸ªç®€å•çš„å†³ç­–æ ‘æ•…æ„ä¿æŒè¾ƒå°ä¸”ä¸å®Œç¾ï¼Œä½¿å…¶ä»…ç•¥å¾®ä¼˜äºéšæœºçŒœæµ‹ã€‚
- en: 2.2\. Evaluate how good this tree is
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 2.2\. è¯„ä¼°è¿™æ£µæ ‘çš„å¥½å
- en: a. Use the tree to predict the label of the training set.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: a. ä½¿ç”¨æ ‘æ¥é¢„æµ‹è®­ç»ƒé›†çš„æ ‡ç­¾ã€‚
- en: b. Add up the weights of all **misclassified samples** to get error rate
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: b. å°†æ‰€æœ‰**è¯¯åˆ†ç±»æ ·æœ¬**çš„æƒé‡ç›¸åŠ ï¼Œå¾—åˆ°è¯¯å·®ç‡
- en: '![](../Images/d3dcfbdd4971811d91c3ba1fb5d763c3.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d3dcfbdd4971811d91c3ba1fb5d763c3.png)'
- en: The first weak learner makes predictions **on the training data**, and we check
    where it made mistakes (marked with X). The error rate of 0.357 shows this simple
    tree gets some predictions wrong, which is expected and will help guide the next
    steps of training.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ£µå¼±å­¦ä¹ å™¨å¯¹**è®­ç»ƒæ•°æ®**è¿›è¡Œé¢„æµ‹ï¼Œæˆ‘ä»¬æ£€æŸ¥å®ƒåœ¨å“ªäº›åœ°æ–¹çŠ¯äº†é”™è¯¯ï¼ˆæ ‡è®°ä¸ºXï¼‰ã€‚é”™è¯¯ç‡ä¸º0.357ï¼Œæ˜¾ç¤ºè¿™æ£µç®€å•çš„æ ‘åœ¨ä¸€äº›é¢„æµ‹ä¸Šæ˜¯é”™è¯¯çš„ï¼Œè¿™æ˜¯å¯ä»¥é¢„æœŸçš„ï¼Œå¹¶ä¸”æœ‰åŠ©äºæŒ‡å¯¼ä¸‹ä¸€æ­¥çš„è®­ç»ƒã€‚
- en: 'c. Calculate tree importance (*Î±*) using:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: c. ä½¿ç”¨ä»¥ä¸‹å…¬å¼è®¡ç®—æ ‘çš„æƒé‡ (*Î±*)ï¼š
- en: '*Î±* = learning_rate Ã— log((1-error)/error)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*Î±* = å­¦ä¹ ç‡ Ã— log((1-é”™è¯¯)/é”™è¯¯)'
- en: '![](../Images/e4a210b84e6605415fb8f6268044a9c9.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4a210b84e6605415fb8f6268044a9c9.png)'
- en: Using the error rate, we calculate the treeâ€™s influence score (Î± = 0.5878).
    Higher scores mean more accurate trees, and this tree earned moderate importance
    for its decent performance.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨é”™è¯¯ç‡ï¼Œæˆ‘ä»¬è®¡ç®—æ ‘çš„å½±å“åˆ†æ•°ï¼ˆÎ± = 0.5878ï¼‰ã€‚è¾ƒé«˜çš„åˆ†æ•°æ„å‘³ç€æ ‘çš„å‡†ç¡®æ€§æ›´é«˜ï¼Œè¿™æ£µæ ‘å› å…¶è‰¯å¥½çš„è¡¨ç°è·å¾—äº†é€‚ä¸­çš„é‡è¦æ€§ã€‚
- en: 2.3\. Update sample weights
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 2.3\. æ›´æ–°æ ·æœ¬æƒé‡
- en: a. Keep the original weights for correctly classified samples
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: a. å¯¹äºæ­£ç¡®åˆ†ç±»çš„æ ·æœ¬ï¼Œä¿ç•™åŸå§‹æƒé‡
- en: b. Multiply the weights of misclassified samples by e^(*Î±*).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: b. å°†é”™è¯¯åˆ†ç±»æ ·æœ¬çš„æƒé‡ä¹˜ä»¥ e^(*Î±*)ã€‚
- en: c. Divide each weight by the sum of all weights. This normalization ensures
    all weights still sum to 1 while maintaining their relative proportions.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: c. å°†æ¯ä¸ªæƒé‡é™¤ä»¥æ‰€æœ‰æƒé‡çš„æ€»å’Œã€‚è¿™ä¸€å½’ä¸€åŒ–è¿‡ç¨‹ç¡®ä¿æ‰€æœ‰æƒé‡ä»ç„¶åŠ å’Œä¸º1ï¼ŒåŒæ—¶ä¿æŒå®ƒä»¬çš„ç›¸å¯¹æ¯”ä¾‹ã€‚
- en: '![](../Images/b4490670bf0192e1521f0f479a4c1873.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4490670bf0192e1521f0f479a4c1873.png)'
- en: Cases where the tree made mistakes (marked with X) get higher weights for the
    next round. After increasing these weights, all weights are normalized to sum
    to 1, ensuring misclassified examples get more attention in the next tree.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‘é”™è¯¯åˆ†ç±»çš„æƒ…å†µï¼ˆæ ‡è®°ä¸ºXï¼‰ä¼šåœ¨ä¸‹ä¸€è½®ä¸­è·å¾—æ›´é«˜çš„æƒé‡ã€‚å¢åŠ è¿™äº›æƒé‡åï¼Œæ‰€æœ‰æƒé‡ä¼šå½’ä¸€åŒ–ï¼Œä½¿å¾—é”™è¯¯åˆ†ç±»çš„æ ·æœ¬åœ¨ä¸‹ä¸€æ£µæ ‘ä¸­å¾—åˆ°æ›´å¤šå…³æ³¨ã€‚
- en: For the Second Tree
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯¹äºç¬¬äºŒæ£µæ ‘
- en: 2.1\. Build a new stump, but now using the updated weights
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 2.1\. æ„å»ºä¸€ä¸ªæ–°çš„æ¡©å†³ç­–æ ‘ï¼Œä½†è¿™æ¬¡ä½¿ç”¨æ›´æ–°åçš„æƒé‡
- en: 'a. Calculate new weighted Gini impurity for root node:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: a. è®¡ç®—æ ¹èŠ‚ç‚¹çš„æ–°çš„åŠ æƒåŸºå°¼ä¸çº¯åº¦ï¼š
- en: Â· Will be different because misclassified samples now have bigger weights
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Â· ç”±äºé”™è¯¯åˆ†ç±»çš„æ ·æœ¬æƒé‡è¾ƒå¤§ï¼Œç»“æœä¼šæœ‰æ‰€ä¸åŒ
- en: Â· Correctly classified samples now have smaller weights
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Â· æ­£ç¡®åˆ†ç±»çš„æ ·æœ¬ç°åœ¨å…·æœ‰è¾ƒå°çš„æƒé‡
- en: '![](../Images/7e820aa41f2204a8607d14793adc59d2.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e820aa41f2204a8607d14793adc59d2.png)'
- en: Using the updated weights (where misclassified examples now have higher importance),
    the algorithm calculates the weighted Gini impurity at the root node. This begins
    the process of building the second decision tree.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ›´æ–°åçš„æƒé‡ï¼ˆæ­¤æ—¶é”™è¯¯åˆ†ç±»çš„æ ·æœ¬æƒé‡æ›´é«˜ï¼‰ï¼Œç®—æ³•è®¡ç®—æ ¹èŠ‚ç‚¹çš„åŠ æƒåŸºå°¼ä¸çº¯åº¦ã€‚è¿™å°†å¼€å§‹æ„å»ºç¬¬äºŒæ£µå†³ç­–æ ‘çš„è¿‡ç¨‹ã€‚
- en: 'b. For each feature:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: b. å¯¹äºæ¯ä¸ªç‰¹å¾ï¼š
- en: Â· Same process as before, but the weights have changed
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Â· ä¸ä¹‹å‰ç›¸åŒçš„è¿‡ç¨‹ï¼Œä½†æƒé‡å·²å‘ç”Ÿå˜åŒ–
- en: c. Pick the split with best weighted Gini impurity reduction
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: c. é€‰æ‹©æœ€ä½³çš„åŠ æƒåŸºå°¼ä¸çº¯åº¦å‡å°‘åˆ’åˆ†
- en: Â· Often completely different from the first treeâ€™s split
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Â· é€šå¸¸ä¸ç¬¬ä¸€æ£µæ ‘çš„åˆ’åˆ†å®Œå…¨ä¸åŒ
- en: Â· Focuses on samples the first tree got wrong
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Â· é‡ç‚¹å…³æ³¨ç¬¬ä¸€ä¸ªæ ‘é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
- en: '![](../Images/8380dccedc96e8aad94425ca3ab24b95.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8380dccedc96e8aad94425ca3ab24b95.png)'
- en: With updated weights, different split points show different effectiveness. Notice
    that â€œovercastâ€ is no longer the best split â€” the algorithm now finds temperature
    (84.0) gives the highest impurity reduction, showing how weight changes affect
    split selection.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ›´æ–°åçš„æƒé‡ï¼Œä¸åŒçš„åˆ’åˆ†ç‚¹å±•ç°å‡ºä¸åŒçš„æ•ˆæœã€‚æ³¨æ„ï¼Œâ€œé˜´å¤©â€ä¸å†æ˜¯æœ€ä½³åˆ’åˆ†ç‚¹â€”â€”ç®—æ³•ç°åœ¨å‘ç°æ¸©åº¦ï¼ˆ84.0ï¼‰ç»™å‡ºçš„åŠ æƒåŸºå°¼ä¸çº¯åº¦å‡å°‘æœ€å¤§ï¼Œæ˜¾ç¤ºå‡ºæƒé‡å˜åŒ–å¦‚ä½•å½±å“åˆ’åˆ†é€‰æ‹©ã€‚
- en: d. Create the second stump
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: d. åˆ›å»ºç¬¬äºŒæ£µæ¡©å†³ç­–æ ‘
- en: '![](../Images/f1d85397a88dd3805b964813e8a5fd14.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1d85397a88dd3805b964813e8a5fd14.png)'
- en: Using temperature â‰¤ 84.0 as the split point, the algorithm assigns YES/NO to
    each leaf based on which class has more total weight in that group, not just by
    counting examples. This weighted voting helps correct the previous treeâ€™s mistakes.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ¸©åº¦ â‰¤ 84.0 ä½œä¸ºåˆ’åˆ†ç‚¹ï¼Œç®—æ³•æ ¹æ®æ¯ç»„ä¸­æ€»æƒé‡è¾ƒå¤§çš„ç±»åˆ«æ¥ä¸ºæ¯ä¸ªå¶èŠ‚ç‚¹åˆ†é… YES/NOï¼Œè€Œä¸ä»…ä»…æ˜¯é€šè¿‡è®¡æ•°ç¤ºä¾‹ã€‚è¿™ç§åŠ æƒæŠ•ç¥¨æœ‰åŠ©äºçº æ­£ç¬¬ä¸€æ£µæ ‘çš„é”™è¯¯ã€‚
- en: 2.2\. Evaluate this new tree
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 2.2\. è¯„ä¼°è¿™æ£µæ–°æ ‘
- en: a. Calculate error rate with current weights
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: a. ä½¿ç”¨å½“å‰æƒé‡è®¡ç®—é”™è¯¯ç‡
- en: b. Calculate its importance (*Î±*) using the same formula as before
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: b. ä½¿ç”¨ä¸ä¹‹å‰ç›¸åŒçš„å…¬å¼è®¡ç®—å…¶é‡è¦æ€§ (*Î±*)
- en: '2.3\. Update weights again â€” Same process: increase weights for mistakes then
    normalize.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 2.3\. å†æ¬¡æ›´æ–°æƒé‡ â€” åŒæ ·çš„è¿‡ç¨‹ï¼šå¢åŠ é”™è¯¯åˆ†ç±»çš„æƒé‡ï¼Œç„¶åè¿›è¡Œå½’ä¸€åŒ–ã€‚
- en: '![](../Images/0773fcc8ffd93058f55eebccc00b9c1f.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0773fcc8ffd93058f55eebccc00b9c1f.png)'
- en: The second tree achieves a lower error rate (0.222) and higher importance score
    (Î± = 1.253) than the first tree. Like before, misclassified examples get higher
    weights for the next round.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒæ£µæ ‘çš„é”™è¯¯ç‡è¾ƒä½ï¼ˆ0.222ï¼‰ï¼Œä¸”é‡è¦æ€§å¾—åˆ†è¾ƒé«˜ï¼ˆÎ± = 1.253ï¼‰ï¼Œä¸ç¬¬ä¸€æ£µæ ‘ç›¸æ¯”ã€‚åƒä¹‹å‰ä¸€æ ·ï¼Œè¯¯åˆ†ç±»çš„ç¤ºä¾‹åœ¨ä¸‹ä¸€è½®ä¼šè·å¾—æ›´é«˜çš„æƒé‡ã€‚
- en: For the Third Tree onwards
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»ç¬¬ä¸‰æ£µæ ‘å¼€å§‹
- en: Repeat Step 2.1â€“2.3 for all remaining trees.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æ‰€æœ‰å‰©ä½™çš„æ ‘é‡å¤æ­¥éª¤2.1â€“2.3ã€‚
- en: '![](../Images/d19ad8eddbd4588dac4781b288276203.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d19ad8eddbd4588dac4781b288276203.png)'
- en: The algorithm builds 50 simple decision trees sequentially, each with its own
    importance score (Î±). Each tree learns from previous mistakes by focusing on different
    aspects of the data, creating a strong combined model. Notice how some trees (like
    Tree 2) get higher importance scores when they perform better.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥ç®—æ³•ä¾æ¬¡æ„å»º50æ£µç®€å•çš„å†³ç­–æ ‘ï¼Œæ¯æ£µæ ‘éƒ½æœ‰è‡ªå·±çš„é‡è¦æ€§å¾—åˆ†ï¼ˆÎ±ï¼‰ã€‚æ¯æ£µæ ‘é€šè¿‡å…³æ³¨æ•°æ®çš„ä¸åŒæ–¹é¢æ¥ä»ä¹‹å‰çš„é”™è¯¯ä¸­å­¦ä¹ ï¼Œåˆ›å»ºä¸€ä¸ªå¼ºå¤§çš„ç»„åˆæ¨¡å‹ã€‚è¯·æ³¨æ„ï¼Œä¸€äº›æ ‘ï¼ˆå¦‚æ ‘2ï¼‰åœ¨è¡¨ç°æ›´å¥½çš„æ—¶å€™ä¼šè·å¾—æ›´é«˜çš„æƒé‡å¾—åˆ†ã€‚
- en: '**Step 3: Final Ensemble** 3.1\. Keep all trees and their importance scores'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¬¬3æ­¥ï¼šæœ€ç»ˆé›†æˆ** 3.1\. ä¿ç•™æ‰€æœ‰æ ‘åŠå…¶é‡è¦æ€§å¾—åˆ†'
- en: '![](../Images/4f0e5ae526c79b2e46766e4e85bfdb13.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f0e5ae526c79b2e46766e4e85bfdb13.png)'
- en: The 50 simple decision trees work together as a team, each with its own importance
    score (Î±). When making predictions, trees with higher Î± values (like Tree 2 with
    1.253) have more influence on the final decision than trees with lower scores.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™50æ£µç®€å•çš„å†³ç­–æ ‘ä½œä¸ºä¸€ä¸ªå›¢é˜Ÿå…±åŒå·¥ä½œï¼Œæ¯æ£µæ ‘éƒ½æœ‰è‡ªå·±çš„é‡è¦æ€§å¾—åˆ†ï¼ˆÎ±ï¼‰ã€‚åœ¨è¿›è¡Œé¢„æµ‹æ—¶ï¼Œå…·æœ‰è¾ƒé«˜Î±å€¼çš„æ ‘ï¼ˆå¦‚æ ‘2ï¼ŒÎ± = 1.253ï¼‰å¯¹æœ€ç»ˆå†³ç­–çš„å½±å“å¤§äºå¾—åˆ†è¾ƒä½çš„æ ‘ã€‚
- en: '[PRE1]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/c3c0699734afabfd3e5c095b1571e9ce.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3c0699734afabfd3e5c095b1571e9ce.png)'
- en: Each node shows its â€˜valueâ€™ parameter as [weight_NO, weight_YES], which represents
    the weighted proportion of each class at that node. These weights come from the
    sample weights we calculated during training.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªèŠ‚ç‚¹æ˜¾ç¤ºå…¶â€œå€¼â€å‚æ•°ï¼Œæ ¼å¼ä¸º[weight_NO, weight_YES]ï¼Œè¡¨ç¤ºè¯¥èŠ‚ç‚¹æ¯ä¸ªç±»åˆ«çš„åŠ æƒæ¯”ä¾‹ã€‚è¿™äº›æƒé‡æ¥è‡ªæˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è®¡ç®—çš„æ ·æœ¬æƒé‡ã€‚
- en: Testing Step
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æµ‹è¯•æ­¥éª¤
- en: 'For predicting:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºé¢„æµ‹ï¼š
- en: a. Get each treeâ€™s prediction
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: a. è·å–æ¯æ£µæ ‘çš„é¢„æµ‹ç»“æœ
- en: b. Multiply each by its importance score (*Î±*)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: b. å°†æ¯ä¸ªå€¼ä¹˜ä»¥å…¶é‡è¦æ€§å¾—åˆ†ï¼ˆ*Î±*ï¼‰
- en: c. Add them all up
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: c. å°†å®ƒä»¬å…¨éƒ¨ç›¸åŠ 
- en: d. The class with higher total weight will be the final prediction
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: d. æ€»æƒé‡è¾ƒé«˜çš„ç±»åˆ«å°†ä½œä¸ºæœ€ç»ˆé¢„æµ‹
- en: '![](../Images/c9a49e61ccbf0b28a73a1ace1a9fd700.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c9a49e61ccbf0b28a73a1ace1a9fd700.png)'
- en: When predicting for new data, each tree makes its prediction and multiplies
    it by its importance score (Î±). The final decision comes from adding up all weighted
    votes â€” here, the NO class gets a higher total score (23.315 vs 15.440), so the
    model predicts NO for this unseen example.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: å½“é¢„æµ‹æ–°æ•°æ®æ—¶ï¼Œæ¯æ£µæ ‘éƒ½ä¼šåšå‡ºè‡ªå·±çš„é¢„æµ‹ï¼Œå¹¶å°†å…¶ä¹˜ä»¥è‡ªå·±çš„é‡è¦æ€§å¾—åˆ†ï¼ˆÎ±ï¼‰ã€‚æœ€ç»ˆçš„å†³ç­–æ¥è‡ªäºå°†æ‰€æœ‰åŠ æƒæŠ•ç¥¨ç»“æœç›¸åŠ â€”â€”åœ¨è¿™é‡Œï¼ŒNOç±»åˆ«è·å¾—äº†æ›´é«˜çš„æ€»åˆ†ï¼ˆ23.315å¯¹15.440ï¼‰ï¼Œå› æ­¤æ¨¡å‹é¢„æµ‹è¯¥ç¤ºä¾‹ä¸ºNOã€‚
- en: Evaluation Step
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯„ä¼°æ­¥éª¤
- en: After building all the trees, we can evaluate the test set.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºå®Œæ‰€æœ‰æ ‘ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥è¯„ä¼°æµ‹è¯•é›†ã€‚
- en: '![](../Images/0e4cb581d6d058a44ad7f63b4a7dd494.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e4cb581d6d058a44ad7f63b4a7dd494.png)'
- en: By iteratively training and weighting weak learners to focus on misclassified
    examples, AdaBoost creates a strong classifier that achieves high accuracy â€” typically
    better than single decision trees or simpler models!
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡åå¤è®­ç»ƒå’ŒåŠ æƒå¼±å­¦ä¹ å™¨æ¥ä¸“æ³¨äºè¯¯åˆ†ç±»çš„ç¤ºä¾‹ï¼ŒAdaBooståˆ›å»ºäº†ä¸€ä¸ªå¼ºå¤§çš„åˆ†ç±»å™¨ï¼Œèƒ½å¤Ÿå®ç°é«˜ç²¾åº¦â€”â€”é€šå¸¸æ¯”å•ä¸€å†³ç­–æ ‘æˆ–æ›´ç®€å•çš„æ¨¡å‹æ›´å¥½ï¼
- en: '[PRE2]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Key Parameters
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å…³é”®å‚æ•°
- en: 'Here are the key parameters for AdaBoost, particularly in `scikit-learn`:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯AdaBoostçš„å…³é”®å‚æ•°ï¼Œç‰¹åˆ«æ˜¯åœ¨`scikit-learn`ä¸­ï¼š
- en: '`estimator`: This is the base model that AdaBoost uses to build its final solution.
    The 3 most common weak learners are:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '`estimator`ï¼šè¿™æ˜¯AdaBoostç”¨æ¥æ„å»ºæœ€ç»ˆè§£å†³æ–¹æ¡ˆçš„åŸºç¡€æ¨¡å‹ã€‚æœ€å¸¸è§çš„ä¸‰ç§å¼±å­¦ä¹ å™¨æ˜¯ï¼š'
- en: '**a. Decision Tree with depth 1 (Decision Stump)**: This is the default and
    most popular choice. Because it only has one split, it is considered a very weak
    learner that is just a bit better than random guessing, exactly what is needed
    for boosting process.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**a. æ·±åº¦ä¸º1çš„å†³ç­–æ ‘ï¼ˆå†³ç­–æ ‘æ¡©ï¼‰**ï¼šè¿™æ˜¯é»˜è®¤çš„ä¹Ÿæ˜¯æœ€æµè¡Œçš„é€‰æ‹©ã€‚ç”±äºå®ƒåªæœ‰ä¸€ä¸ªåˆ†è£‚ï¼Œå› æ­¤è¢«è®¤ä¸ºæ˜¯ä¸€ä¸ªéå¸¸å¼±çš„å­¦ä¹ å™¨ï¼Œä»…æ¯”éšæœºçŒœæµ‹ç•¥å¥½ï¼Œè¿™æ­£æ˜¯æå‡è¿‡ç¨‹æ‰€éœ€è¦çš„ã€‚'
- en: '**b. Logistic Regression**: Logistic regression (especially with high-penalty)
    can also be used here even though it is not really a weak learner. It could be
    useful for data that has linear relationship.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**b. é€»è¾‘å›å½’**ï¼šé€»è¾‘å›å½’ï¼ˆç‰¹åˆ«æ˜¯é«˜æƒ©ç½šçš„æƒ…å†µä¸‹ï¼‰ä¹Ÿå¯ä»¥åœ¨è¿™é‡Œä½¿ç”¨ï¼Œå°½ç®¡å®ƒå¹¶ä¸æ˜¯ä¸€ä¸ªçœŸæ­£çš„å¼±å­¦ä¹ å™¨ã€‚å®ƒå¯¹äºå…·æœ‰çº¿æ€§å…³ç³»çš„æ•°æ®å¯èƒ½å¾ˆæœ‰ç”¨ã€‚'
- en: '**c. Decision Trees with small depth (e.g., depth 2 or 3)**: These are slightly
    more complex than decision stumps. Theyâ€™re still fairly simple, but can handle
    slightly more complex patterns than the decision stump.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**c. å°æ·±åº¦çš„å†³ç­–æ ‘ï¼ˆä¾‹å¦‚ï¼Œæ·±åº¦ä¸º2æˆ–3ï¼‰ï¼š**è¿™äº›æ ‘æ¯”å†³ç­–æ ‘æ¡©ç¨å¾®å¤æ‚ä¸€äº›ã€‚å®ƒä»¬ä»ç„¶ç›¸å¯¹ç®€å•ï¼Œä½†å¯ä»¥å¤„ç†æ¯”å†³ç­–æ ‘æ¡©ç¨å¾®å¤æ‚ä¸€äº›çš„æ¨¡å¼ã€‚'
- en: '![](../Images/72829a3081a99f628be3a36d2be8ad47.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/72829a3081a99f628be3a36d2be8ad47.png)'
- en: AdaBoostâ€™s base models can be simple decision stumps (depth=1), small trees
    (depth 2â€“3), or penalized linear models. Each type is kept simple to avoid overfitting
    while offering different ways to capture patterns.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoostçš„åŸºæ¨¡å‹å¯ä»¥æ˜¯ç®€å•çš„å†³ç­–æ ‘æ¡©ï¼ˆæ·±åº¦=1ï¼‰ã€å°æ·±åº¦çš„æ ‘ï¼ˆæ·±åº¦ä¸º2-3ï¼‰æˆ–æƒ©ç½šçš„çº¿æ€§æ¨¡å‹ã€‚æ¯ç§ç±»å‹éƒ½ä¿æŒç®€å•ï¼Œä»¥é¿å…è¿‡æ‹Ÿåˆï¼ŒåŒæ—¶æä¾›ä¸åŒçš„æ–¹å¼æ¥æ•æ‰æ¨¡å¼ã€‚
- en: '`n_estimators`: The number of weak learners to combine, typically around 50â€“100\.
    Using more than 100 rarely helps.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_estimators`ï¼šè¦ç»„åˆçš„å¼±å­¦ä¹ å™¨æ•°é‡ï¼Œé€šå¸¸åœ¨50åˆ°100ä¹‹é—´ã€‚è¶…è¿‡100é€šå¸¸ä¸ä¼šå¸¦æ¥æ˜¾è‘—çš„æ”¹å–„ã€‚'
- en: '`learning_rate`: Controls how much each classifier affects the final result.
    Common starting values are 0.1, 0.5, or 1.0\. Lower numbers (like 0.1) and a bit
    higher `n_estimator` usually work better.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`learning_rate`ï¼šæ§åˆ¶æ¯ä¸ªåˆ†ç±»å™¨å¯¹æœ€ç»ˆç»“æœçš„å½±å“ã€‚å¸¸è§çš„åˆå§‹å€¼æœ‰0.1ã€0.5æˆ–1.0ã€‚è¾ƒå°çš„å€¼ï¼ˆå¦‚0.1ï¼‰å’Œç¨å¾®æ›´é«˜çš„`n_estimators`é€šå¸¸æ•ˆæœæ›´å¥½ã€‚'
- en: Key differences from Random Forest
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸éšæœºæ£®æ—çš„ä¸»è¦åŒºåˆ«
- en: 'As both Random Forest and AdaBoost works with multiple trees, it is easy to
    confuse the parameters involved. The key difference is that Random Forest combines
    many trees **independently** (bagging) while AdaBoost builds trees **one after
    another** to fix mistakes (boosting). Here are some other details about their
    differences:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºéšæœºæ£®æ—å’ŒAdaBoostéƒ½ä½¿ç”¨å¤šæ£µæ ‘ï¼Œå› æ­¤å¾ˆå®¹æ˜“æ··æ·†å…¶ä¸­çš„å‚æ•°ã€‚å…³é”®åŒºåˆ«åœ¨äºï¼Œéšæœºæ£®æ—æ˜¯**ç‹¬ç«‹**åœ°ï¼ˆè¢‹è£…æ³•ï¼‰ç»“åˆå¤šæ£µæ ‘ï¼Œè€ŒAdaBoostæ˜¯**ä¸€ä¸ªæ¥ä¸€ä¸ªåœ°**æ„å»ºæ ‘ä»¥ä¿®æ­£é”™è¯¯ï¼ˆæå‡æ³•ï¼‰ã€‚ä»¥ä¸‹æ˜¯å®ƒä»¬å·®å¼‚çš„å…¶ä»–ä¸€äº›ç»†èŠ‚ï¼š
- en: No `bootstrap` parameter because AdaBoost uses all data but with changing weights
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ²¡æœ‰`bootstrap`å‚æ•°ï¼Œå› ä¸ºAdaBoostä½¿ç”¨æ‰€æœ‰æ•°æ®ï¼Œä½†æƒé‡ä¼šå‘ç”Ÿå˜åŒ–
- en: No `oob_score` because AdaBoost doesn't use bootstrap sampling
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ²¡æœ‰`oob_score`ï¼Œå› ä¸ºAdaBoostä¸ä½¿ç”¨è‡ªåŠ©é‡‡æ ·æ³•
- en: '`learning_rate` becomes crucial (not present in Random Forest)'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`learning_rate`å˜å¾—è‡³å…³é‡è¦ï¼ˆåœ¨éšæœºæ£®æ—ä¸­ä¸å­˜åœ¨ï¼‰'
- en: Tree depth is typically kept very shallow (usually just stumps) unlike Random
    Forestâ€™s deeper trees
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ ‘çš„æ·±åº¦é€šå¸¸ä¿æŒéå¸¸æµ…ï¼ˆé€šå¸¸åªæ˜¯æ ‘æ¡©ï¼‰ï¼Œä¸éšæœºæ£®æ—çš„è¾ƒæ·±æ ‘ä¸åŒ
- en: The focus shifts from parallel independent trees to sequential dependent trees,
    making parameters like `n_jobs` less relevant
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç„¦ç‚¹ä»å¹¶è¡Œç‹¬ç«‹æ ‘è½¬ç§»åˆ°é¡ºåºä¾èµ–æ ‘ï¼Œè¿™ä½¿å¾—åƒ`n_jobs`è¿™æ ·çš„å‚æ•°å˜å¾—ä¸é‚£ä¹ˆç›¸å…³
- en: Pros & Cons
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¼˜ç¼ºç‚¹
- en: 'Pros:'
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¼˜ç‚¹ï¼š
- en: '**Adaptive Learning:** AdaBoost gets better by giving more weight to mistakes
    it made. Each new tree pays more attention to the hard cases it got wrong.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è‡ªé€‚åº”å­¦ä¹ ï¼š**AdaBoosté€šè¿‡å¢åŠ å¯¹é”™è¯¯çš„æƒé‡æ¥æå‡æ€§èƒ½ã€‚æ¯æ£µæ–°æ ‘ä¼šæ›´åŠ å…³æ³¨å®ƒä¹‹å‰åšé”™çš„éš¾ä¾‹ã€‚'
- en: '**Resists Overfitting:** Even though it keeps adding more trees one by one,
    AdaBoost usually doesnâ€™t get too focused on training data. This is because it
    uses weighted voting, so no single tree can control the final answer too much.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æŠ—è¿‡æ‹Ÿåˆï¼š**å°½ç®¡å®ƒä¼šä¸€ä¸ªæ¥ä¸€ä¸ªåœ°æ·»åŠ æ›´å¤šçš„æ ‘ï¼Œä½†AdaBoosté€šå¸¸ä¸ä¼šè¿‡äºä¸“æ³¨äºè®­ç»ƒæ•°æ®ã€‚è¿™æ˜¯å› ä¸ºå®ƒä½¿ç”¨åŠ æƒæŠ•ç¥¨ï¼Œå› æ­¤æ²¡æœ‰ä»»ä½•ä¸€æ£µæ ‘èƒ½è¿‡å¤šåœ°æ§åˆ¶æœ€ç»ˆçš„ç­”æ¡ˆã€‚'
- en: '**Built-in Feature Selection:** AdaBoost naturally finds which features matter
    most. Each simple tree picks the most useful feature for that round, which means
    it automatically selects important features as it trains.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å†…ç½®ç‰¹å¾é€‰æ‹©ï¼š**AdaBoostè‡ªç„¶åœ°æ‰¾å‡ºå“ªäº›ç‰¹å¾æœ€é‡è¦ã€‚æ¯æ£µç®€å•çš„æ ‘éƒ½ä¼šé€‰æ‹©è¯¥è½®ä¸­æœ€æœ‰ç”¨çš„ç‰¹å¾ï¼Œè¿™æ„å‘³ç€å®ƒåœ¨è®­ç»ƒæ—¶ä¼šè‡ªåŠ¨é€‰æ‹©é‡è¦ç‰¹å¾ã€‚'
- en: 'Cons:'
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¼ºç‚¹ï¼š
- en: '**Sensitive to Noise:** Because it gives more weight to mistakes, AdaBoost
    can have trouble with messy or wrong data. If some training examples have wrong
    labels, it might focus too much on these bad examples, making the whole model
    worse.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¯¹å™ªå£°æ•æ„Ÿï¼š**ç”±äºAdaBoostç»™é”™è¯¯æ›´å¤šçš„æƒé‡ï¼Œå®ƒå¯èƒ½ä¼šåœ¨æ•°æ®æ‚ä¹±æˆ–é”™è¯¯æ—¶å‡ºç°é—®é¢˜ã€‚å¦‚æœä¸€äº›è®­ç»ƒæ ·æœ¬æ ‡ç­¾é”™è¯¯ï¼Œå®ƒå¯èƒ½ä¼šè¿‡äºå…³æ³¨è¿™äº›é”™è¯¯æ ·æœ¬ï¼Œä»è€Œä½¿æ•´ä¸ªæ¨¡å‹å˜å¾—æ›´å·®ã€‚'
- en: '**Must Be Sequential:** Unlike Random Forest which can train many trees at
    once, AdaBoost must train one tree at a time because each new tree needs to know
    how the previous trees did. This makes it slower to train.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¿…é¡»æ˜¯é¡ºåºçš„ï¼š**ä¸éšæœºæ£®æ—å¯ä»¥åŒæ—¶è®­ç»ƒå¤šæ£µæ ‘ä¸åŒï¼ŒAdaBoostå¿…é¡»ä¸€æ¬¡è®­ç»ƒä¸€æ£µæ ‘ï¼Œå› ä¸ºæ¯æ£µæ–°æ ‘éœ€è¦çŸ¥é“å‰ä¸€æ£µæ ‘çš„è¡¨ç°ã€‚è¿™ä½¿å¾—è®­ç»ƒé€Ÿåº¦è¾ƒæ…¢ã€‚'
- en: '**Learning Rate Sensitivity:** While it has fewer settings to tune than Random
    Forest, the learning rate really affects how well it works. If itâ€™s too high,
    it might learn the training data too exactly. If itâ€™s too low, it needs many more
    trees to work well.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å­¦ä¹ ç‡æ•æ„Ÿæ€§ï¼š**è™½ç„¶å®ƒçš„è°ƒæ•´é¡¹æ¯”éšæœºæ£®æ—å°‘ï¼Œä½†å­¦ä¹ ç‡ç¡®å®å½±å“å…¶æ•ˆæœã€‚å¦‚æœå­¦ä¹ ç‡è¿‡é«˜ï¼Œæ¨¡å‹å¯èƒ½ä¼šè¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®ã€‚å¦‚æœå­¦ä¹ ç‡è¿‡ä½ï¼Œåˆ™éœ€è¦æ›´å¤šçš„æ ‘æ‰èƒ½å–å¾—è‰¯å¥½çš„æ•ˆæœã€‚'
- en: Final Remarks
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœ€åçš„å¤‡æ³¨
- en: AdaBoost is a key boosting algorithm that many newer methods learned from. Its
    main idea â€” getting better by focusing on mistakes â€” has helped shape many modern
    machine learning tools. While other methods try to be perfect from the start,
    AdaBoost tries to show that sometimes the best way to solve a problem is to learn
    from your errors and keep improving.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost æ˜¯ä¸€ç§å…³é”®çš„æå‡ç®—æ³•ï¼Œè®¸å¤šæ–°æ–¹æ³•éƒ½ä»ä¸­æ±²å–äº†çµæ„Ÿã€‚å®ƒçš„ä¸»è¦æ€æƒ³â€”â€”é€šè¿‡å…³æ³¨é”™è¯¯æ¥å˜å¾—æ›´å¥½â€”â€”å¸®åŠ©å¡‘é€ äº†è®¸å¤šç°ä»£æœºå™¨å­¦ä¹ å·¥å…·ã€‚è™½ç„¶å…¶ä»–æ–¹æ³•è¯•å›¾ä¸€å¼€å§‹å°±åšåˆ°å®Œç¾ï¼Œä½†
    AdaBoost åˆ™è¯•å›¾è¡¨æ˜ï¼Œæœ‰æ—¶å€™è§£å†³é—®é¢˜çš„æœ€ä½³æ–¹å¼æ˜¯ä»é”™è¯¯ä¸­å­¦ä¹ å¹¶ä¸æ–­æ”¹è¿›ã€‚
- en: AdaBoost also works best in binary classification problems and when your data
    is clean. While Random Forest might be better for more general tasks (like predicting
    numbers) or messy data, AdaBoost can give really good results when used in the
    right way. The fact that people still use it after so many years shows just how
    well the core idea works!
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost æœ€é€‚ç”¨äºäºŒåˆ†ç±»é—®é¢˜ï¼Œå¹¶ä¸”å½“æ•°æ®æ¸…æ™°æ—¶è¡¨ç°æœ€ä½³ã€‚è™½ç„¶éšæœºæ£®æ—å¯èƒ½æ›´é€‚ç”¨äºæ›´ä¸€èˆ¬çš„ä»»åŠ¡ï¼ˆä¾‹å¦‚é¢„æµ‹æ•°å­—ï¼‰æˆ–æ‚ä¹±çš„æ•°æ®ï¼Œä½†åœ¨æ­£ç¡®ä½¿ç”¨çš„æƒ…å†µä¸‹ï¼ŒAdaBoost
    å¯ä»¥äº§ç”Ÿéå¸¸å¥½çš„ç»“æœã€‚äººä»¬åœ¨å¤šå¹´åä»ç„¶ä½¿ç”¨å®ƒï¼Œè¶³ä»¥è¯æ˜å…¶æ ¸å¿ƒæ€æƒ³çš„æœ‰æ•ˆæ€§ï¼
- en: ğŸŒŸ AdaBoost Classifier Code Summarized
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸŒŸ AdaBoost åˆ†ç±»å™¨ä»£ç æ¦‚è¿°
- en: '[PRE3]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Further Reading
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¿›ä¸€æ­¥é˜…è¯»
- en: For a detailed explanation of the [AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)
    and its implementation in scikit-learn, readers can refer to the official documentation,
    which provides comprehensive information on its usage and parameters.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº [AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)
    åŠå…¶åœ¨ scikit-learn ä¸­çš„å®ç°ï¼Œè¯»è€…å¯ä»¥å‚è€ƒå®˜æ–¹æ–‡æ¡£ï¼Œæ–‡æ¡£æä¾›äº†æœ‰å…³å…¶ç”¨æ³•å’Œå‚æ•°çš„è¯¦ç»†ä¿¡æ¯ã€‚
- en: Technical Environment
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æŠ€æœ¯ç¯å¢ƒ
- en: This article uses Python 3.7 and scikit-learn 1.6\. While the concepts discussed
    are generally applicable, specific code implementations may vary slightly with
    different versions.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡ä½¿ç”¨ Python 3.7 å’Œ scikit-learn 1.6ã€‚å°½ç®¡æ‰€è®¨è®ºçš„æ¦‚å¿µæ™®éé€‚ç”¨ï¼Œä½†å…·ä½“çš„ä»£ç å®ç°å¯èƒ½ä¼šå› ä¸åŒç‰ˆæœ¬è€Œç•¥æœ‰ä¸åŒã€‚
- en: About the Illustrations
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…³äºæ’å›¾
- en: Unless otherwise noted, all images are created by the author, incorporating
    licensed design elements from Canva Pro.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤éå¦æœ‰è¯´æ˜ï¼Œæ‰€æœ‰å›¾åƒå‡ç”±ä½œè€…åˆ›ä½œï¼Œå¹¶ç»“åˆäº† Canva Pro çš„æˆæƒè®¾è®¡å…ƒç´ ã€‚
- en: 'ğ™ğ™šğ™š ğ™¢ğ™¤ğ™§ğ™š ğ™€ğ™£ğ™¨ğ™šğ™¢ğ™—ğ™¡ğ™š ğ™‡ğ™šğ™–ğ™§ğ™£ğ™ğ™£ğ™œ ğ™ğ™šğ™§ğ™š:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğ™ğ™šğ™š ğ™¢ğ™¤ğ™§ğ™š ğ™€ğ™£ğ™¨ğ™šğ™¢ğ™—ğ™¡ğ™š ğ™‡ğ™šğ™–ğ™§ğ™£ğ™ğ™£ğ™œ ğ™ğ™šğ™§ğ™š:'
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----fc0f25326d7b--------------------------------)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----fc0f25326d7b--------------------------------)'
- en: Ensemble Learning
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é›†æˆå­¦ä¹ 
- en: '[View list](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----fc0f25326d7b--------------------------------)4
    stories![](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[æŸ¥çœ‹åˆ—è¡¨](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----fc0f25326d7b--------------------------------)4ç¯‡æ•…äº‹![](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)'
- en: 'ğ™”ğ™¤ğ™ª ğ™¢ğ™ğ™œğ™ğ™© ğ™–ğ™¡ğ™¨ğ™¤ ğ™¡ğ™ğ™ ğ™š:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğ™”ğ™¤ğ™ª ğ™¢ğ™ğ™œğ™ğ™© ğ™–ğ™¡ğ™¨ğ™¤ ğ™¡ğ™ğ™ ğ™š:'
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----fc0f25326d7b--------------------------------)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----fc0f25326d7b--------------------------------)'
- en: Classification Algorithms
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ†ç±»ç®—æ³•
- en: '[View list](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----fc0f25326d7b--------------------------------)8
    stories![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '[æŸ¥çœ‹åˆ—è¡¨](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----fc0f25326d7b--------------------------------)8ç¯‡æ•…äº‹![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)'
