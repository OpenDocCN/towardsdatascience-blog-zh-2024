- en: Quantize Llama 3 8B with Bitsandbytes to Preserve Its Accuracy
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Bitsandbytes 对 Llama 3 8B 进行量化以保持其准确性
- en: 原文：[https://towardsdatascience.com/quantize-llama-3-8b-with-bitsandbytes-to-preserve-its-accuracy-e84283b233f7?source=collection_archive---------3-----------------------#2024-05-27](https://towardsdatascience.com/quantize-llama-3-8b-with-bitsandbytes-to-preserve-its-accuracy-e84283b233f7?source=collection_archive---------3-----------------------#2024-05-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/quantize-llama-3-8b-with-bitsandbytes-to-preserve-its-accuracy-e84283b233f7?source=collection_archive---------3-----------------------#2024-05-27](https://towardsdatascience.com/quantize-llama-3-8b-with-bitsandbytes-to-preserve-its-accuracy-e84283b233f7?source=collection_archive---------3-----------------------#2024-05-27)
- en: Llama 2 vs. Llama 3 vs. Mistral 7B, quantized with GPTQ and Bitsandbytes
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Llama 2 vs. Llama 3 vs. Mistral 7B，使用 GPTQ 和 Bitsandbytes 进行量化
- en: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--e84283b233f7--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--e84283b233f7--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e84283b233f7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e84283b233f7--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--e84283b233f7--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--e84283b233f7--------------------------------)[![本杰明·马里](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--e84283b233f7--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e84283b233f7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e84283b233f7--------------------------------)
    [本杰明·马里](https://medium.com/@bnjmn_marie?source=post_page---byline--e84283b233f7--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e84283b233f7--------------------------------)
    ·6 min read·May 27, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e84283b233f7--------------------------------)
    ·6分钟阅读·2024年5月27日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/43ed7ac2dae5a741ff7449a9bf0ea60b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/43ed7ac2dae5a741ff7449a9bf0ea60b.png)'
- en: Generated with DALL-E
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由 DALL-E 生成
- en: With quantization, we can reduce the size of large language models (LLMs). Quantized
    LLMs are easier to run on GPUs with smaller memory, effectively serving as a compression
    method for LLMs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 通过量化，我们可以减小大型语言模型（LLM）的大小。量化后的 LLM 更易于在内存较小的 GPU 上运行，实际上可以作为 LLM 的压缩方法。
- en: 'According to [Meta’s own evaluation](https://ai.meta.com/blog/meta-llama-3/),
    Llama 3 8B is better than Llama 2 7B and Mistral 7B. However, the question remains:
    does Llama 3 8B maintain its superiority after quantization?'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 [Meta 的自评](https://ai.meta.com/blog/meta-llama-3/)，Llama 3 8B 比 Llama 2 7B
    和 Mistral 7B 更强。然而，问题仍然存在：Llama 3 8B 在量化后是否仍保持其优势？
- en: In other words, if Llama 3 is better than Mistral 7B and Llama 2 (Llama 3 >
    Mistral 7B > Llama 2 7B), is the quantized version also better than these models
    quantized (quantized Llama 3 > quantized Mistral 7B > quantized Llama 2 7B)?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，如果 Llama 3 比 Mistral 7B 和 Llama 2 更好（Llama 3 > Mistral 7B > Llama 2 7B），那么量化版本是否也比这些模型的量化版本更好呢（量化
    Llama 3 > 量化 Mistral 7B > 量化 Llama 2 7B）？
- en: In this article, we will answer this question. I quantized all the models with
    bitsandbytes to 8-bit and 4-bit, and with GPTQ to 8-bit, 4-bit, 3-bit, and 2-bit
    and checked their performance on 3 different tasks. We will see that 8-bit quantization
    works reasonably well for Llama 3 with both quantization algorithms. I also found
    that while GPTQ 4-bit significantly degrades the model, bitsandbytes quantization
    seems to work well.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将回答这个问题。我使用 bitsandbytes 将所有模型量化为 8 位和 4 位，并使用 GPTQ 将它们量化为 8 位、4 位、3
    位和 2 位，随后检查它们在 3 个不同任务上的表现。我们将看到，8 位量化在两种量化算法下对 Llama 3 都表现得相当不错。我还发现，虽然 GPTQ
    4 位大大降低了模型性能，但 bitsandbytes 的量化似乎效果很好。
- en: GPTQ Quantization for Llama 3
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Llama 3 的 GPTQ 量化
