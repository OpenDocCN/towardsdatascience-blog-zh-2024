- en: Attention, Please!
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意力，注意了！
- en: 原文：[https://towardsdatascience.com/attention-please-25b2933309f4?source=collection_archive---------8-----------------------#2024-08-29](https://towardsdatascience.com/attention-please-25b2933309f4?source=collection_archive---------8-----------------------#2024-08-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/attention-please-25b2933309f4?source=collection_archive---------8-----------------------#2024-08-29](https://towardsdatascience.com/attention-please-25b2933309f4?source=collection_archive---------8-----------------------#2024-08-29)
- en: Attention is all you need, but the span is limited.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注意力是你所需要的一切，但其范围是有限的。
- en: '[](https://dpoulopoulos.medium.com/?source=post_page---byline--25b2933309f4--------------------------------)[![Dimitris
    Poulopoulos](../Images/ce535a1679779f5a2ec8b024e6691e50.png)](https://dpoulopoulos.medium.com/?source=post_page---byline--25b2933309f4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--25b2933309f4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--25b2933309f4--------------------------------)
    [Dimitris Poulopoulos](https://dpoulopoulos.medium.com/?source=post_page---byline--25b2933309f4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://dpoulopoulos.medium.com/?source=post_page---byline--25b2933309f4--------------------------------)[![Dimitris
    Poulopoulos](../Images/ce535a1679779f5a2ec8b024e6691e50.png)](https://dpoulopoulos.medium.com/?source=post_page---byline--25b2933309f4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--25b2933309f4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--25b2933309f4--------------------------------)
    [Dimitris Poulopoulos](https://dpoulopoulos.medium.com/?source=post_page---byline--25b2933309f4--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--25b2933309f4--------------------------------)
    ·10 min read·Aug 29, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--25b2933309f4--------------------------------)
    ·10分钟阅读·2024年8月29日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/86ebfbfc89df1f076e0f6bebad8e26cb.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/86ebfbfc89df1f076e0f6bebad8e26cb.png)'
- en: Photo by [Google DeepMind](https://unsplash.com/@googledeepmind?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/a-crystal-vase-with-pink-flowers-in-it-Oy2yXvl1WLg?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Google DeepMind](https://unsplash.com/@googledeepmind?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)提供，来源于[Unsplash](https://unsplash.com/photos/a-crystal-vase-with-pink-flowers-in-it-Oy2yXvl1WLg?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
- en: 'FlashAttention Part Two: An intuitive introduction to the attention mechanism,
    with real-world analogies, simple visuals, and plain narrative. [Part I](/unraveling-flashattention-a20e6483c793)
    of this story is now live.'
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: FlashAttention 第二部分：通过实际的类比、简单的视觉呈现和通俗的叙述，为注意力机制提供直观的介绍。[第一部分](/unraveling-flashattention-a20e6483c793)的内容现已发布。
- en: In the [previous chapter](/unraveling-flashattention-a20e6483c793), I introduced
    the FlashAttention mechanism from a high-level perspective, following an “Explain
    Like I’m 5” (ELI5) approach. This method resonates with me the most; I always
    strive to connect challenging concepts to real-life analogies, which I find aids
    in retention over time.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在[上一章](/unraveling-flashattention-a20e6483c793)中，我从一个高层次的角度介绍了 FlashAttention
    机制，采用了“像我五岁一样解释”（ELI5）的方法。这种方式我最为认同；我总是尽力将困难的概念与现实生活中的类比联系起来，我发现这种方式有助于长期记忆。
- en: Next up on our educational menu is the vanilla attention algorithm — a dish
    we can’t skip if we’re aiming to spice it up later. Understand it first, improve
    it next. There’s no way around it.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们教育菜单上的主菜是标准的注意力算法——这是我们如果以后想要加料，不能跳过的一道菜。先理解它，然后再改进它。这是无法绕开的。
- en: By now, you’ve likely skimmed through a plethora of articles about the attention
    mechanism and watched countless YouTube videos. Indeed, attention is a superstar
    in the world of AI, with everyone eager to collaborate on a feature with it.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你可能已经浏览了大量关于注意力机制的文章，并观看了无数的 YouTube 视频。的确，注意力机制是 AI 领域的明星，每个人都急于在某个功能上与之合作。
- en: So, I’m also jumping into the spotlight to share my take on this celebrated
    concept, followed by a shoutout to some resources that have inspired me. I’ll
    stick to our tried-and-tested formula of employing analogies, but I’ll also incorporate
    a more visual approach. Echoing my earlier sentiment (at the risk of sounding
    like a broken…
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我也跳出来分享我对这个广受欢迎概念的看法，并为一些曾启发我的资源送上致谢。我将继续使用我们经过验证的类比方式，但也会融入更具视觉化的呈现。呼应我之前的观点（冒着听起来像破碎唱片的风险……）
