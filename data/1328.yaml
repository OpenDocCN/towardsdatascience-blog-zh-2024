- en: Simple Ways to Speed Up Your PyTorch Model Training
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加速你的 PyTorch 模型训练的简单方法
- en: 原文：[https://towardsdatascience.com/simple-ways-to-speed-up-your-pytorch-model-training-9c9d4899313d?source=collection_archive---------3-----------------------#2024-05-28](https://towardsdatascience.com/simple-ways-to-speed-up-your-pytorch-model-training-9c9d4899313d?source=collection_archive---------3-----------------------#2024-05-28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/simple-ways-to-speed-up-your-pytorch-model-training-9c9d4899313d?source=collection_archive---------3-----------------------#2024-05-28](https://towardsdatascience.com/simple-ways-to-speed-up-your-pytorch-model-training-9c9d4899313d?source=collection_archive---------3-----------------------#2024-05-28)
- en: If all machine learning engineers want one thing, **it’s faster model training**
    — maybe after good test metrics
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '如果所有机器学习工程师都希望得到一个东西，那就是 **更快的模型训练** —— 也许在获得良好的测试指标之后。 '
- en: '[](https://alexdremov.medium.com/?source=post_page---byline--9c9d4899313d--------------------------------)[![Alex
    Dremov](../Images/8afeaa6bae03d3b6c436d81127c75a0c.png)](https://alexdremov.medium.com/?source=post_page---byline--9c9d4899313d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9c9d4899313d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9c9d4899313d--------------------------------)
    [Alex Dremov](https://alexdremov.medium.com/?source=post_page---byline--9c9d4899313d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://alexdremov.medium.com/?source=post_page---byline--9c9d4899313d--------------------------------)[![Alex
    Dremov](../Images/8afeaa6bae03d3b6c436d81127c75a0c.png)](https://alexdremov.medium.com/?source=post_page---byline--9c9d4899313d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9c9d4899313d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9c9d4899313d--------------------------------)
    [Alex Dremov](https://alexdremov.medium.com/?source=post_page---byline--9c9d4899313d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9c9d4899313d--------------------------------)
    ·11 min read·May 28, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9c9d4899313d--------------------------------)
    ·11 分钟阅读 ·2024年5月28日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/eb75109bbcb6622409f76f7e507791cd.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb75109bbcb6622409f76f7e507791cd.png)'
- en: Photo by [Julian Hochgesang](https://unsplash.com/@julianhochgesang?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit)
    / [Unsplash](https://unsplash.com/?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Julian Hochgesang](https://unsplash.com/@julianhochgesang?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit)
    / [Unsplash](https://unsplash.com/?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit)
- en: Does this topic even need an introduction?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这个话题甚至需要介绍吗？
- en: Speeding up machine learning model training is one thing that all machine learning
    engineers want. Faster training equals faster experiments equals faster iterations
    for your product. Also, it means that one model training will require fewer resources.
    So, straight to the point
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 加速机器学习模型训练是所有机器学习工程师的目标。更快的训练意味着更快的实验，也意味着更快的产品迭代。此外，这还意味着一次模型训练将需要更少的资源。所以，直接进入正题
- en: Containerization
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器化
- en: Yes, this will not speed up your training on its own. But this targets another
    important aspect — reproducibility. Sometimes virtualenv with fixed library versions
    is enough, but I encourage you to take one step further and build an all-in-one
    docker container for your model training.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，单靠这个不会加速你的训练。但它针对的是另一个重要方面——可重现性。有时候，使用固定库版本的 virtualenv 就足够了，但我鼓励你更进一步，为你的模型训练构建一个一体化的
    Docker 容器。
- en: This ensures that the environment is fully consistent during debugging, profiling,
    and final training. The last thing you want is to optimize a part of code that
    is no longer a bottleneck due to python12 speed up, for example. Or even a bug
    that is not reproducible on different CUDA versions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这确保了在调试、分析和最终训练过程中，环境的一致性。你最不希望的事情就是优化一个部分代码，而这个部分由于 Python12 提升了速度，已经不再是瓶颈了。例如，甚至有一个错误在不同的
    CUDA 版本下无法重现。
- en: 'As a starting point, you can use pre-built images from NVIDIA. They already
    have CUDA, PyTorch, and other popular libs installed:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 作为起点，你可以使用 NVIDIA 提供的预构建镜像。这些镜像已经安装了 CUDA、PyTorch 和其他流行的库：
- en: '[## PyTorch | NVIDIA NGC'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[## PyTorch | NVIDIA NGC'
- en: PyTorch is a GPU accelerated tensor computational framework. Functionality can
    be extended with common Python libraries…
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PyTorch 是一个 GPU 加速的张量计算框架。功能可以通过常见的 Python 库进行扩展……
- en: catalog.ngc.nvidia.com](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: catalog.ngc.nvidia.com](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
- en: 💡 A Docker container is the ultimate solution for problems like
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 💡 Docker 容器是解决此类问题的终极方案
- en: “Hey, it works on my machine. I have no idea why it doesn’t on yours.”
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “嘿，它在我的机器上可以工作，我不知道为什么在你的机器上不行。”
- en: Get comfortable with PyTorch profiler
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 熟悉 PyTorch 分析器
- en: 'Before optimizing anything, you have to understand how long some parts of your
    code run. Pytorch profiler is *almost* an all-in-one tool for profiling training.
    It’s able to record:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在优化任何东西之前，你必须了解你的一些代码部分运行了多久。PyTorch 分析器几乎是一个功能齐全的训练分析工具。它能够记录：
- en: CPU operations timings
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU 操作的时间
- en: CUDA kernels timings
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA 内核的时间
- en: Memory consumption history
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存消耗历史
- en: That’s all you need. And it’s easy to enable!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是你所需要的一切。而且它很容易启用！
- en: 'To record events, all you need is to embed training into a profiler context
    like this:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 要记录事件，你只需要像这样将训练嵌入分析器上下文中：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: After that, you can launch the tensorboard and view profiling traces. Do not
    forget to install [torch-tb-profiler](https://pypi.org/project/torch-tb-profiler/?ref=alexdremov.me).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，你可以启动 tensorboard 并查看分析轨迹。别忘了安装 [torch-tb-profiler](https://pypi.org/project/torch-tb-profiler/?ref=alexdremov.me)。
- en: '[](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
    [## PyTorch Profiler With TensorBoard - PyTorch Tutorials 2.3.0+cu121 documentation'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
    [## PyTorch Profiler 与 TensorBoard - PyTorch 教程 2.3.0+cu121 文档]'
- en: Prepare the data and model Use profiler to record execution events Run the profiler
    Use TensorBoard to view results and…
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备数据和模型 使用分析器记录执行事件 运行分析器 使用 TensorBoard 查看结果并…
- en: pytorch.org](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: pytorch.org](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
- en: 'Profiler has a lot of different options, but the most important are `activities`
    and `profile_memory`. You can experiment with other options, but keep in mind
    a simple rule: **the fewer options you''ve enabled, the less overhead you have**.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 分析器有许多不同的选项，但最重要的是 `activities` 和 `profile_memory`。你可以尝试其他选项，但请记住一个简单的规则：**你启用的选项越少，开销就越小**。
- en: So, if you want to profile CUDA kernel execution timings, it is a good idea
    to turn off CPU profiling and all other features. In this mode, profiling will
    be as close to the real execution as possible.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果你想分析 CUDA 内核执行的时间，最好关闭 CPU 分析和其他所有功能。在这种模式下，分析将尽可能接近真实执行。
- en: To make traces easier to understand, consider adding profiling contexts that
    describe core parts of your code. If profiling is not enabled, those are no-op.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让轨迹更容易理解，考虑添加描述你代码核心部分的分析上下文。如果没有启用分析，这些上下文将无效。
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This way, the labels that you use will be visible in traces. So, it will be
    easier to identify code blocks. Or even more granular inside mode’s forward:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，你使用的标签将在轨迹中可见。这样，识别代码块会更加容易。甚至在模式的 forward 中进行更细粒度的分析：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Understanding PyTorch traces
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 PyTorch 轨迹
- en: 'After you gather traces, open them in the tensorboard. That’s what the CPU
    + CUDA profile looks like:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 收集轨迹后，在 tensorboard 中打开它们。这就是 CPU + CUDA 分析的样子：
- en: '![](../Images/b207f5f91e4560da06402145f3cfb486.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b207f5f91e4560da06402145f3cfb486.png)'
- en: © Copyright 2024, PyTorch | [https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html?ref=alexdremov.me)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: © 版权 2024，PyTorch | [https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html?ref=alexdremov.me)
- en: 'Straight away, find the core parts of any training:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 立刻找到任何训练的核心部分：
- en: data loading
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据加载
- en: forward pass
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前向传播
- en: backward pass
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传播
- en: Backward pass is handled by PyTorch in a separate thread (thread 16893 on the
    image above), so it is easy to identify.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播由 PyTorch 在单独的线程中处理（如上图中的线程 16893），所以很容易识别。
- en: Data loading
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据加载
- en: For data loading, we want near-zero timings.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据加载，我们希望接近零的时间。
- en: No compromises.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 没有妥协。
- en: That’s because during data loading GPU does nothing, which under-utilizes available
    resources. However, data processing can be overlapped with GPU computing as those
    are independent parts.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为在数据加载期间 GPU 不做任何事情，这会导致可用资源的低效利用。然而，数据处理可以与 GPU 计算重叠，因为它们是独立的部分。
- en: You can easily identify areas where GPU is idle — just look at *GPU Est. SM
    Efficiency* and *GPU Utilization* figures in the profiler’s trace. Areas with
    zero activity are our patients. That’s where GPU does nothing.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以轻松地识别 GPU 闲置的区域——只需查看分析器跟踪中的 *GPU 估算 SM 效率* 和 *GPU 利用率* 数值。没有活动的区域就是我们的“患者”。这就是
    GPU 什么也不做的地方。
- en: 'A simple solution for that is:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的解决方案是：
- en: process data in the background process (no GIL)
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在后台进程中处理数据（没有 GIL）
- en: process data augmentations and transforms in parallel processes
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在并行进程中处理数据增强和变换
- en: If you use PyTorch DataLoader, then it can be easily achieved by specifying
    `num_workers`. It's more complicated if you use `IterableDataset`, as then data
    will be duplicated. However, this issue still can be solved by using [get_worker_info()](https://pytorch.org/docs/stable/data.html?ref=alexdremov.me#torch.utils.data.IterableDataset)
    - you need to adjust iteration in a way so that each worker receives different,
    non-intersecting rows.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用 PyTorch 的 DataLoader，那么可以通过指定 `num_workers` 来轻松实现这一点。如果你使用 `IterableDataset`，情况会更复杂，因为数据将会重复。但这个问题仍然可以通过使用
    [get_worker_info()](https://pytorch.org/docs/stable/data.html?ref=alexdremov.me#torch.utils.data.IterableDataset)
    来解决——你需要调整迭代方式，以确保每个工作进程接收不同且不重叠的行。
- en: For more configurable processing, you may consider implementing multi-process
    transforms yourself with `multiprocessing`
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更可配置的处理，你可以考虑使用 `multiprocessing` 自行实现多进程变换
- en: 💡 If you never checked your code’s data processing speed, then this slight modification
    can yield **dramatic speedups**
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 💡 如果你从未检查过代码的数据处理速度，那么这个小小的修改可能会带来 **剧烈的加速**。
- en: Making friends with memory allocator
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与内存分配器成为朋友
- en: You want to be friends with PyTorch’s CUDA caching allocator.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望与 PyTorch 的 CUDA 缓存分配器成为朋友。
- en: When you allocate tensors with PyTorch on a CUDA device, PyTorch will use a
    caching allocator. That’s because `cudaMalloc`/ `cudaFree` are expensive operations
    that we want to avoid, so PyTorch has its allocator that will try to reuse previously
    allocated through `cudaMalloc` blocks. That is, if PyTorch's allocator has an
    appropriate block available, it will give it straight away without calling `cudaMalloc`.
    That way, `cudaMalloc` is called only at the beginning.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在 CUDA 设备上使用 PyTorch 分配张量时，PyTorch 会使用一个缓存分配器。这是因为 `cudaMalloc`/ `cudaFree`
    是昂贵的操作，我们希望避免调用它们，因此 PyTorch 有自己的分配器，它会尝试重用通过 `cudaMalloc` 分配的块。也就是说，如果 PyTorch
    的分配器有合适的块可用，它会直接提供，而不需要调用 `cudaMalloc`。这样，`cudaMalloc` 只会在一开始调用。
- en: However, if you’re dealing with data of variable length, different forward passes
    will require intermediate tensors of different sizes. So, PyTorch’s allocator
    may not have an appropriate block of data available. In this case, the allocator
    panics and releases allocated previously bocks by calling `cudaFree` to free up
    space for new allocations.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你处理的是长度可变的数据，不同的前向传递将需要不同大小的中间张量。因此，PyTorch 的分配器可能没有合适的数据块可用。在这种情况下，分配器会崩溃并通过调用
    `cudaFree` 释放之前分配的块，以为新的分配腾出空间。
- en: After that, the allocator starts building its cache again, doing tons of `cudaMalloc`,
    which is an expensive operation. You can spot this problem by looking at the memory
    profiler section of the tensorboard profiler viewer.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，分配器开始重新构建它的缓存，进行大量的 `cudaMalloc`，这是一项昂贵的操作。你可以通过查看张量板分析器视图的内存分析部分来发现这个问题。
- en: 💡 You also can spot this problem in the traces. It will be visible as calls
    to `cudaMalloc`and `cudaFree`
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 💡 你也可以在跟踪记录中发现这个问题。它将以对 `cudaMalloc` 和 `cudaFree` 的调用形式显示出来
- en: '![](../Images/414070253ac9c94f59f8f522399d7fb4.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/414070253ac9c94f59f8f522399d7fb4.png)'
- en: PyTorch allocator freaks out | Image by the author
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 分配器崩溃 | 图片来自作者
- en: As you see, a red line that corresponds to the allocator’s reserved memory constantly
    changes. That means that PyTorch allocator is not able to efficiently handle allocation
    requests.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，与分配器保留的内存对应的红线不断变化。这意味着 PyTorch 的分配器无法有效地处理分配请求。
- en: When allocations are handled without the allocator panicking, the red line is
    completely straight
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当分配操作不再让分配器崩溃时，红线完全是直的
- en: '![](../Images/e8e801553645665ba7dff138173a00cd.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e8e801553645665ba7dff138173a00cd.png)'
- en: PyTorch allocator works as expected | Image by the author
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 分配器按预期工作 | 图片来自作者
- en: As I said, that is usually due to variable shapes of tensors. How to fix that?
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如我所说，这通常是由于张量的形状不固定。如何修复这个问题？
- en: '**Expandable Segments**'
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**可扩展的段**'
- en: 'The first thing that is worth trying is to set PyTorch’s relatively new allocator
    mode:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 第一件值得尝试的事情是设置 PyTorch 相对较新的分配器模式：
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*If set to* `*True*`*, this setting instructs the allocator to create CUDA
    allocations that can later be expanded to better handle cases where a job changes
    allocation sizes frequently, such as having a changing batch size.*'
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*如果设置为* `*True*`*，此设置指示分配器创建可以在以后扩展的CUDA分配，以更好地处理工作负载频繁更改分配大小的情况，例如批量大小变化的情况。*'
- en: So, this tells PyTorch allocator to allocate blocks that could be expanded in
    the future, which is exactly our case. Though, if size variations are too big,
    it still may fail to solve the issue. In this case, move to the next option.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这告诉PyTorch分配器分配将来可能会扩展的块，这正是我们的情况。尽管如此，如果大小变化过大，仍然可能无法解决问题。在这种情况下，转到下一个选项。
- en: '**Make allocations variate less**'
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**使分配的变化更少**'
- en: Another possible solution is to make data shapes consistent. That way it will
    be easier for the allocator to find an appropriate data block to reuse.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种可能的解决方案是使数据形状保持一致。这样，分配器将更容易找到一个适合重用的数据块。
- en: To accomplish that, you may pad data to the same sizes. Or you can preheat the
    allocator by running a model with maximum input sizes.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，你可以将数据填充到相同的大小。或者你可以通过运行一个具有最大输入大小的模型来预热分配器。
- en: You can learn more about PyTorch allocator modification in the following article
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下文章中了解更多关于PyTorch分配器修改的信息。
- en: '[## CUDA semantics - PyTorch 2.3 documentation'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[## CUDA语义 - PyTorch 2.3 文档'
- en: A guide to torch.cuda, a PyTorch module to run CUDA operations
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: torch.cuda 的指南，PyTorch模块用于执行CUDA操作
- en: pytorch.org](https://pytorch.org/docs/stable/notes/cuda.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: pytorch.org](https://pytorch.org/docs/stable/notes/cuda.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
- en: Tidy up allocations history
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 整理分配历史
- en: We want to use all available GPU memory — that allows us to run big batches
    and process data faster. However, at some point, you will encounter a *CUDA out-of-memory*
    error when increasing batch size. What causes this error?
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望使用所有可用的GPU内存——这使我们能够运行更大的批次并更快地处理数据。然而，在某个时刻，当增加批量大小时，你将遇到*CUDA内存不足*错误。是什么导致了这个错误？
- en: To debug this, we can view the allocator’s memory history. It can be recorded
    through PyTorch and then visualized at [https://pytorch.org/memory_viz](https://pytorch.org/memory_viz?ref=alexdremov.me)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调试这个问题，我们可以查看分配器的内存历史。它可以通过PyTorch记录，然后在[https://pytorch.org/memory_viz](https://pytorch.org/memory_viz?ref=alexdremov.me)上进行可视化。
- en: '**Start:** `torch.cuda.memory._record_memory_history(max_entries=100000)`'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开始：** `torch.cuda.memory._record_memory_history(max_entries=100000)`'
- en: '**Save:** `torch.cuda.memory._dump_snapshot(file_name)`'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**保存：** `torch.cuda.memory._dump_snapshot(file_name)`'
- en: '**Stop:** `torch.cuda.memory._record_memory_history(enabled=None)`'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**停止：** `torch.cuda.memory._record_memory_history(enabled=None)`'
- en: 'Visualization will draw something like this:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化将显示如下内容：
- en: '![](../Images/270f72428251b625a672af134cf2f454.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/270f72428251b625a672af134cf2f454.png)'
- en: © Copyright 2024, PyTorch | [https://pytorch.org/blog/understanding-gpu-memory-1/](https://pytorch.org/blog/understanding-gpu-memory-1/?ref=alexdremov.me)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: © 版权 2024, PyTorch | [https://pytorch.org/blog/understanding-gpu-memory-1/](https://pytorch.org/blog/understanding-gpu-memory-1/?ref=alexdremov.me)
- en: The x-axis represents time, the y-axis represents total used memory, and colourful
    blocks represent tensors. So, it shows when tensors were allocated and when it
    was released.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: x轴表示时间，y轴表示总使用内存，彩色块表示张量。因此，它显示了张量何时被分配以及何时被释放。
- en: You may notice narrow spikes — those are short-lasting tensors that take up
    a lot of space. By clicking on a tensor, you can get information on where this
    tensor was allocated. We want to minimize those spikes as they limit efficient
    memory usage. Check out what caused this spike and consider other ways of computing
    what you intended.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到窄尖峰——这些是占用大量空间的短暂张量。通过点击张量，你可以获得该张量分配的位置。我们希望尽量减少这些尖峰，因为它们限制了内存的高效使用。检查一下是什么导致了这个尖峰，并考虑其他计算方式。
- en: 'Apart from spikes, it’s easy to detect memory leaks:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 除了尖峰外，检测内存泄漏也很容易：
- en: '![](../Images/d257a3b5af410d7d0bb7b3e46ecda12d.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d257a3b5af410d7d0bb7b3e46ecda12d.png)'
- en: © Copyright 2024, PyTorch | [https://pytorch.org/blog/understanding-gpu-memory-1/](https://pytorch.org/blog/understanding-gpu-memory-1/?ref=alexdremov.me)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: © 版权 2024, PyTorch | [https://pytorch.org/blog/understanding-gpu-memory-1/](https://pytorch.org/blog/understanding-gpu-memory-1/?ref=alexdremov.me)
- en: As you see, some data after the first forward is not cleared. By clicking on
    blocks you can get the idea where these tensors come from. In the image is the
    case when gradients are not cleared after the training step, so they lay dead
    during the forward pass, limiting the ability to increase the batch size to fit
    more data.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，第一次前向传播后一些数据并没有被清除。通过点击这些模块，你可以大致了解这些张量来自何处。图中显示的是训练步骤后梯度未被清除的情况，这导致它们在前向传播过程中仍然存在，限制了增加批量大小以适应更多数据的能力。
- en: '[](https://pytorch.org/blog/understanding-gpu-memory-1/?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
    [## Understanding GPU Memory 1: Visualizing All Allocations over Time'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pytorch.org/blog/understanding-gpu-memory-1/?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
    [## 理解GPU内存1：可视化所有分配随时间变化'
- en: 'During your time with PyTorch on GPUs, you may be familiar with this common
    error message:'
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在你使用PyTorch进行GPU计算的过程中，你可能已经熟悉这个常见的错误信息：
- en: pytorch.org](https://pytorch.org/blog/understanding-gpu-memory-1/?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[pytorch.org](https://pytorch.org/blog/understanding-gpu-memory-1/?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)'
- en: Speed up the model and use less memory
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加速模型并减少内存使用
- en: What can be better than this? We can achieve so by using the **FlashAttention**
    kernel for calculating dot-product attention.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 有什么比这更好的呢？我们可以通过使用**FlashAttention**内核来计算点积注意力，从而实现这一目标。
- en: '[](https://github.com/Dao-AILab/flash-attention?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
    [## GitHub - Dao-AILab/flash-attention: Fast and memory-efficient exact attention'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/Dao-AILab/flash-attention?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
    [## GitHub - Dao-AILab/flash-attention：快速且内存高效的精确注意力'
- en: Fast and memory-efficient exact attention. Contribute to Dao-AILab/flash-attention
    development by creating an account…
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 快速且内存高效的精确注意力。通过创建一个账户，为Dao-AILab/flash-attention的开发做出贡献…
- en: github.com](https://github.com/Dao-AILab/flash-attention?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/Dao-AILab/flash-attention?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)'
- en: If you haven’t heard about it, it is a way of calculating precise dot product
    attention without constructing the attention matrix explicitly. That optimizes
    GPU’s io operations which improves speed and also **dramatically** minimizes memory
    consumption. There’s simply no reason not to use it.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有听说过，它是一种计算精确点积注意力的方法，无需显式构造注意力矩阵。它优化了GPU的输入输出操作，从而提高了速度，并且**大幅度**减少了内存消耗。简直没有理由不使用它。
- en: 😡 Unfortunately, there’s one reason not to use it — hardware.
  id: totrans-106
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 😡 不幸的是，有一个理由不能使用它——硬件。
- en: ''
  id: totrans-107
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Flash attention only works with `fp16` and `bf16` precision on compatible hardware.
    That is NVIDIA Ampere, Hooper, etc
  id: totrans-108
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Flash attention仅在兼容硬件上使用`fp16`和`bf16`精度。这包括NVIDIA Ampere、Hooper等架构。
- en: Other libraries use flash attention under the hood, so you may consider using
    other variants that better fit your codebase.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 其他库在底层使用了flash attention，因此你可以考虑使用其他更适合你代码库的变种。
- en: '**XFormers**'
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**XFormers**'
- en: '[](https://github.com/facebookresearch/xformers?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
    [## GitHub - facebookresearch/xformers: Hackable and optimized Transformers building
    blocks, supporting…'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/facebookresearch/xformers?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
    [## GitHub - facebookresearch/xformers：可操作且优化过的Transformer构建模块，支持…'
- en: Hackable and optimized Transformers building blocks, supporting a composable
    construction. - facebookresearch/xformers
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可操作且优化过的Transformer构建模块，支持可组合构建。 - facebookresearch/xformers
- en: github.com](https://github.com/facebookresearch/xformers?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/facebookresearch/xformers?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)'
- en: '**Transformer Engine**'
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**Transformer Engine**'
- en: '[](https://github.com/NVIDIA/TransformerEngine?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
    [## GitHub - NVIDIA/TransformerEngine: A library for accelerating Transformer
    models on NVIDIA GPUs…'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/NVIDIA/TransformerEngine?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
    [## GitHub - NVIDIA/TransformerEngine：一个加速Transformer模型在NVIDIA GPU上的运行的库…'
- en: A library for accelerating Transformer models on NVIDIA GPUs, including using
    8-bit floating point (FP8) precision on…
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个加速Transformer模型在NVIDIA GPU上运行的库，包括使用8位浮点（FP8）精度…
- en: github.com](https://github.com/NVIDIA/TransformerEngine?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/NVIDIA/TransformerEngine?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)'
- en: '**PyTorch itself!**'
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**PyTorch 本身！**'
- en: 'That is true, new versions of PyTorch may use flash attention when applicable.
    To activate this mode, you need to execute attention blocks in the context manager
    that specify which attention strategy to use:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这是事实，PyTorch 的新版本可能会在适用的情况下使用闪电注意力（flash attention）。要激活此模式，你需要在上下文管理器中执行注意力块，指定使用哪种注意力策略：
- en: '[## torch.nn.functional.scaled_dot_product_attention - PyTorch 2.3 documentation'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[## torch.nn.functional.scaled_dot_product_attention - PyTorch 2.3 文档](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------#torch-nn-functional-scaled-dot-product-attention)'
- en: Read the PyTorch Domains documentation to learn more about domain-specific libraries
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阅读 PyTorch Domains 文档，了解更多关于特定领域的库
- en: pytorch.org](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------#torch-nn-functional-scaled-dot-product-attention)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[pytorch.org](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------#torch-nn-functional-scaled-dot-product-attention)'
- en: Optimize multi-GPU data redundancy — FSDP
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化多 GPU 数据冗余 — FSDP
- en: If you use multiple GPUs to run your training, the basic solution is to use
    the `DistributedDataParallel` class. This way, several identical processes are
    spawned, and gradients are aggregated during the backward step.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用多个 GPU 进行训练，基本的解决方案是使用 `DistributedDataParallel` 类。这样，多个相同的进程将被启动，并且在反向传播步骤中会聚合梯度。
- en: However, that is sub-optimal!
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不是最优的！
- en: The problem is as we spawned identical processes, then we have identical models
    and optimiser states on each GPU, which is redundant. The solution is to shard
    data across. We can do so using the Fully Sharded Data Parallel PyTorch wrapper.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，当我们启动相同的进程时，每个 GPU 上都有相同的模型和优化器状态，这造成了冗余。解决方案是将数据分片。我们可以使用完全分片数据并行的 PyTorch
    封装来实现这一点。
- en: '![](../Images/b542604a9a173f89aed4bc0375e2e515.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b542604a9a173f89aed4bc0375e2e515.png)'
- en: © Copyright 2024, PyTorch | https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: © 版权 2024，PyTorch | https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html
- en: How does it work?
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: 'As I said, when training on several GPUs, each process has exact copies of
    the same data when training with DDP. We can optimize it, by implementing several
    enhancements:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我所说，当在多个 GPU 上训练时，使用 DDP 时每个进程都有相同数据的副本。我们可以通过实现几个增强功能来优化这一过程：
- en: Shard optimizer state (ZeRO 1)
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分片优化器状态（ZeRO 1）
- en: When training with DDP, each process holds a complete copy of the optimizer
    states. With ZeRO1, we shard these optimizer states across all ranks such that
    each rank holds only a portion of the optimizer states. During the backward pass,
    each rank only needs to gather the optimizer states relevant to its parameters
    to make an optimization step. This reduction in redundancy helps conserve memory.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 DDP 训练时，每个进程持有优化器状态的完整副本。而使用 ZeRO1 时，我们将这些优化器状态在所有的 rank 之间进行分片，使得每个 rank
    只持有优化器状态的一部分。在反向传播过程中，每个 rank 只需要收集与其参数相关的优化器状态来进行优化步骤。这种减少冗余的方式有助于节省内存。
- en: 💡 In case of the Adam, which holds parameters at roughly twice the model size,
    sharding the optimizer state among 8 ranks means each rank **stores only one quarter
    (2/8) of the total state size.**
  id: totrans-133
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 💡 在 Adam 中，由于其参数大约是模型大小的两倍，将优化器状态分片到 8 个 rank 中意味着每个 rank **仅存储总状态大小的四分之一（2/8）。**
- en: Shard gradients (ZeRO 2)
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分片梯度（ZeRO 2）
- en: 'We shard optimizer states. Now, we will modify the optimizer step to shard
    gradients too. If one rank has optimizer states for a portion of parameters, then
    we will:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对优化器状态进行分片。现在，我们将修改优化器步骤，以便也对梯度进行分片。如果一个 rank 拥有部分参数的优化器状态，那么我们将：
- en: aggregate all gradients relevant to the states the rank holds
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合与该 rank 持有状态相关的所有梯度
- en: calculate optimization step
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算优化步骤
- en: send optimization step for a portion of parameters to all other ranks
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将部分参数的优化步骤发送到所有其他 rank
- en: As you noticed, now each rank does not need to hold a full replica of gradients.
    We can send gradients to a relevant rank as soon as they are available. So, we
    can reduce peak memory consumption even further.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所注意到的，现在每个 rank 不再需要持有完整的梯度副本。我们可以在梯度可用时立即将其发送到相关的 rank。因此，我们可以进一步减少峰值内存消耗。
- en: Shard model parameters (ZeRO 3)
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分片模型参数（ZeRO 3）
- en: This is about to be epic.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是史诗级的。
- en: Why do we need to store a full copy of the model on each rank? Let’s shard model
    parameters between all ranks. Then, we’re going to fetch the required parameters
    just in time during forward and backward.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们需要在每个rank上存储模型的完整副本？让我们在所有rank之间分片模型参数。然后，我们将在前向和反向传播过程中按需即时获取所需的参数。
- en: 💡 In case of large models, these optimisations can drammaticaly decrease memory
    consumption
  id: totrans-143
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 💡 对于大模型，这些优化可以显著减少内存消耗。
- en: How to use FSDP?
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用FSDP？
- en: 'Quite simple actually. All we need is to wrap the model with FSDP:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 其实非常简单。我们只需要用FSDP包装模型：
- en: '[PRE4]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You can also specify the sharding strategy of FSDP. For example, we can select
    the `SHARD_GRAD_OP` strategy to achieve behaviour similar to that of ZeRO2\. You
    can learn about other strategies here:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以指定FSDP的分片策略。例如，我们可以选择`SHARD_GRAD_OP`策略，以实现类似ZeRO2的行为。你可以在这里了解其他策略：
- en: '[## FullyShardedDataParallel - PyTorch 2.3 documentation'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[## FullyShardedDataParallel - PyTorch 2.3 文档](https://pytorch.org/docs/stable/fsdp.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)'
- en: A wrapper for sharding module parameters across data parallel workers. This
    is inspired by Xu et al. as well as the…
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用于在数据并行工作者之间分配分片模块参数的包装器。这个灵感来自于Xu等人以及……
- en: pytorch.org](https://pytorch.org/docs/stable/fsdp.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------#torch.distributed.fsdp.ShardingStrategy)
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[pytorch.org](https://pytorch.org/docs/stable/fsdp.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------#torch.distributed.fsdp.ShardingStrategy)'
- en: Also, you can wrap with FSDP submodules. In the example above, only one FSDP
    module is used, which will reduce computation efficiency and memory efficiency.
    The way it works is that, suppose your model contains 100 Linear layers. If you
    do FSDP(model), there will only be one FSDP unit which wraps the entire model.
    In that case, the allgather would collect the full parameters for all 100 linear
    layers, and hence won’t save CUDA memory for parameter sharding.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还可以使用FSDP包装子模块。在上面的示例中，只使用了一个FSDP模块，这样会降低计算效率和内存效率。它的工作原理是，假设你的模型包含100个Linear层。如果你执行FSDP(model)，那么将只有一个FSDP单元包装整个模型。在这种情况下，allgather会收集所有100个Linear层的完整参数，因此不会为参数分片节省CUDA内存。
- en: 'You can wrap submodules explicitly or define an auto-wrap policy. To learn
    more about FSDP, read the PyTorch guide:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以显式地包装子模块或定义自动包装策略。要了解更多关于FSDP的信息，请阅读PyTorch指南：
- en: '[](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
    [## Getting Started with Fully Sharded Data Parallel(FSDP) - PyTorch Tutorials
    2.3.0+cu121…'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
    [## 入门：完全分片数据并行(FSDP) - PyTorch教程 2.3.0+cu121…'
- en: Note View and edit this tutorial in github. Training AI models at a large scale
    is a challenging task that requires a…
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意：在GitHub上查看和编辑本教程。大规模训练AI模型是一项具有挑战性的任务，要求……
- en: pytorch.org](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[pytorch.org](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)'
- en: Magic speedup with `torch.compile`
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用`torch.compile`的神奇加速
- en: That is, torch compile can speed up your code by several percent by just enabling
    it.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，torch compile只需启用它，就可以让你的代码加速几个百分点。
- en: Torch traces your execution graph and tries to compile it into an efficient
    format so that the model can be executed almost without Python invocation.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Torch会跟踪你的执行图，并尝试将其编译为高效的格式，以便模型几乎可以在没有Python调用的情况下执行。
- en: 'Basic usage is to wrap the model with compile:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 基本使用方法是将模型与compile一起包装：
- en: '[PRE5]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This will execute almost instantly. The actual tracing will happen only during
    the first forward.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这几乎会立即执行。实际的跟踪只会在第一次前向传播时发生。
- en: 'It also has a lot of options that are worth to try:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 它还具有许多值得尝试的选项：
- en: '[## torch.compile - PyTorch 2.3 documentation'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[## torch.compile - PyTorch 2.3 文档](https://pytorch.org/docs/stable/generated/torch.compile.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------#torch.compile)'
- en: Optimizes given model/function using TorchDynamo and specified backend. Concretely,
    for every frame executed within the…
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用TorchDynamo和指定的后端优化给定的模型/函数。具体来说，对于在……
- en: pytorch.org](https://pytorch.org/docs/stable/generated/torch.compile.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------#torch.compile)
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[pytorch.org](https://pytorch.org/docs/stable/generated/torch.compile.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------#torch.compile)'
- en: 💡 Torch compiler is a big feature that will be covered in the next posts!
  id: totrans-166
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 💡 Torch编译器是一个重要特性，将在后续的帖子中讲解！
- en: Stay tuned
  id: totrans-167
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 敬请期待
- en: 'Learn more about torch compile here:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里了解更多关于torch compile的信息：
- en: '[## Introduction to torch.compile - PyTorch Tutorials 2.3.0+cu121 documentation'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[## Introduction to torch.compile - PyTorch Tutorials 2.3.0+cu121 documentation'
- en: torch.compile is included in the latest PyTorch. Running TorchInductor on GPU
    requires Triton, which is included with…
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`torch.compile` 已包含在最新的 PyTorch 版本中。在 GPU 上运行 TorchInductor 需要 Triton，而 Triton
    已包含在…'
- en: pytorch.org](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: pytorch.org](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
- en: Conclusion
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: This post is in no way complete with explanations. Rather, that is a list of
    speed-ups that are worth trying straight away. Hope that it was helpful. Feel
    free to leave a comment!
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 本文并非包含所有解释，而是提供了值得立即尝试的加速方法清单。希望对你有所帮助，欢迎留言评论！
- en: Consider subscribing
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑订阅
- en: '*Originally published at* [*https://alexdremov.me*](https://alexdremov.me/simple-ways-to-speedup-your-pytorch-model-training/)
    *on May 28, 2024.*'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '*原文发表于* [*https://alexdremov.me*](https://alexdremov.me/simple-ways-to-speedup-your-pytorch-model-training/)
    *于 2024 年 5 月 28 日。*'
- en: '*Images from PyTorch Blog were used, which is a project of the Linux Foundation
    and is subject to the Linux Foundation* [*policies*](https://www.linuxfoundation.org/legal/terms)*.
    So, all images may be used as allowed by* [*Creative Commons Attribution 3.0 License*](http://creativecommons.org/licenses/by/3.0/)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '*图片来源于 PyTorch 博客，这是 Linux 基金会的一个项目，受 Linux 基金会的* [*政策*](https://www.linuxfoundation.org/legal/terms)*约束。所以，所有图片均可按照*
    [*创意共享 3.0 许可协议*](http://creativecommons.org/licenses/by/3.0/) 使用。*'
