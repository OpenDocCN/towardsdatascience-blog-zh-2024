- en: Simple Ways to Speed Up Your PyTorch Model Training
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŠ é€Ÿä½ çš„ PyTorch æ¨¡å‹è®­ç»ƒçš„ç®€å•æ–¹æ³•
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/simple-ways-to-speed-up-your-pytorch-model-training-9c9d4899313d?source=collection_archive---------3-----------------------#2024-05-28](https://towardsdatascience.com/simple-ways-to-speed-up-your-pytorch-model-training-9c9d4899313d?source=collection_archive---------3-----------------------#2024-05-28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/simple-ways-to-speed-up-your-pytorch-model-training-9c9d4899313d?source=collection_archive---------3-----------------------#2024-05-28](https://towardsdatascience.com/simple-ways-to-speed-up-your-pytorch-model-training-9c9d4899313d?source=collection_archive---------3-----------------------#2024-05-28)
- en: If all machine learning engineers want one thing, **itâ€™s faster model training**
    â€” maybe after good test metrics
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'å¦‚æœæ‰€æœ‰æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆéƒ½å¸Œæœ›å¾—åˆ°ä¸€ä¸ªä¸œè¥¿ï¼Œé‚£å°±æ˜¯ **æ›´å¿«çš„æ¨¡å‹è®­ç»ƒ** â€”â€” ä¹Ÿè®¸åœ¨è·å¾—è‰¯å¥½çš„æµ‹è¯•æŒ‡æ ‡ä¹‹åã€‚ '
- en: '[](https://alexdremov.medium.com/?source=post_page---byline--9c9d4899313d--------------------------------)[![Alex
    Dremov](../Images/8afeaa6bae03d3b6c436d81127c75a0c.png)](https://alexdremov.medium.com/?source=post_page---byline--9c9d4899313d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9c9d4899313d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9c9d4899313d--------------------------------)
    [Alex Dremov](https://alexdremov.medium.com/?source=post_page---byline--9c9d4899313d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://alexdremov.medium.com/?source=post_page---byline--9c9d4899313d--------------------------------)[![Alex
    Dremov](../Images/8afeaa6bae03d3b6c436d81127c75a0c.png)](https://alexdremov.medium.com/?source=post_page---byline--9c9d4899313d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9c9d4899313d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9c9d4899313d--------------------------------)
    [Alex Dremov](https://alexdremov.medium.com/?source=post_page---byline--9c9d4899313d--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9c9d4899313d--------------------------------)
    Â·11 min readÂ·May 28, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9c9d4899313d--------------------------------)
    Â·11 åˆ†é’Ÿé˜…è¯» Â·2024å¹´5æœˆ28æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/eb75109bbcb6622409f76f7e507791cd.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb75109bbcb6622409f76f7e507791cd.png)'
- en: Photo by [Julian Hochgesang](https://unsplash.com/@julianhochgesang?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit)
    / [Unsplash](https://unsplash.com/?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼š[Julian Hochgesang](https://unsplash.com/@julianhochgesang?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit)
    / [Unsplash](https://unsplash.com/?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit)
- en: Does this topic even need an introduction?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¯é¢˜ç”šè‡³éœ€è¦ä»‹ç»å—ï¼Ÿ
- en: Speeding up machine learning model training is one thing that all machine learning
    engineers want. Faster training equals faster experiments equals faster iterations
    for your product. Also, it means that one model training will require fewer resources.
    So, straight to the point
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ é€Ÿæœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒæ˜¯æ‰€æœ‰æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆçš„ç›®æ ‡ã€‚æ›´å¿«çš„è®­ç»ƒæ„å‘³ç€æ›´å¿«çš„å®éªŒï¼Œä¹Ÿæ„å‘³ç€æ›´å¿«çš„äº§å“è¿­ä»£ã€‚æ­¤å¤–ï¼Œè¿™è¿˜æ„å‘³ç€ä¸€æ¬¡æ¨¡å‹è®­ç»ƒå°†éœ€è¦æ›´å°‘çš„èµ„æºã€‚æ‰€ä»¥ï¼Œç›´æ¥è¿›å…¥æ­£é¢˜
- en: Containerization
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®¹å™¨åŒ–
- en: Yes, this will not speed up your training on its own. But this targets another
    important aspect â€” reproducibility. Sometimes virtualenv with fixed library versions
    is enough, but I encourage you to take one step further and build an all-in-one
    docker container for your model training.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œå•é è¿™ä¸ªä¸ä¼šåŠ é€Ÿä½ çš„è®­ç»ƒã€‚ä½†å®ƒé’ˆå¯¹çš„æ˜¯å¦ä¸€ä¸ªé‡è¦æ–¹é¢â€”â€”å¯é‡ç°æ€§ã€‚æœ‰æ—¶å€™ï¼Œä½¿ç”¨å›ºå®šåº“ç‰ˆæœ¬çš„ virtualenv å°±è¶³å¤Ÿäº†ï¼Œä½†æˆ‘é¼“åŠ±ä½ æ›´è¿›ä¸€æ­¥ï¼Œä¸ºä½ çš„æ¨¡å‹è®­ç»ƒæ„å»ºä¸€ä¸ªä¸€ä½“åŒ–çš„
    Docker å®¹å™¨ã€‚
- en: This ensures that the environment is fully consistent during debugging, profiling,
    and final training. The last thing you want is to optimize a part of code that
    is no longer a bottleneck due to python12 speed up, for example. Or even a bug
    that is not reproducible on different CUDA versions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç¡®ä¿äº†åœ¨è°ƒè¯•ã€åˆ†æå’Œæœ€ç»ˆè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç¯å¢ƒçš„ä¸€è‡´æ€§ã€‚ä½ æœ€ä¸å¸Œæœ›çš„äº‹æƒ…å°±æ˜¯ä¼˜åŒ–ä¸€ä¸ªéƒ¨åˆ†ä»£ç ï¼Œè€Œè¿™ä¸ªéƒ¨åˆ†ç”±äº Python12 æå‡äº†é€Ÿåº¦ï¼Œå·²ç»ä¸å†æ˜¯ç“¶é¢ˆäº†ã€‚ä¾‹å¦‚ï¼Œç”šè‡³æœ‰ä¸€ä¸ªé”™è¯¯åœ¨ä¸åŒçš„
    CUDA ç‰ˆæœ¬ä¸‹æ— æ³•é‡ç°ã€‚
- en: 'As a starting point, you can use pre-built images from NVIDIA. They already
    have CUDA, PyTorch, and other popular libs installed:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºèµ·ç‚¹ï¼Œä½ å¯ä»¥ä½¿ç”¨ NVIDIA æä¾›çš„é¢„æ„å»ºé•œåƒã€‚è¿™äº›é•œåƒå·²ç»å®‰è£…äº† CUDAã€PyTorch å’Œå…¶ä»–æµè¡Œçš„åº“ï¼š
- en: '[## PyTorch | NVIDIA NGC'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[## PyTorch | NVIDIA NGC'
- en: PyTorch is a GPU accelerated tensor computational framework. Functionality can
    be extended with common Python librariesâ€¦
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PyTorch æ˜¯ä¸€ä¸ª GPU åŠ é€Ÿçš„å¼ é‡è®¡ç®—æ¡†æ¶ã€‚åŠŸèƒ½å¯ä»¥é€šè¿‡å¸¸è§çš„ Python åº“è¿›è¡Œæ‰©å±•â€¦â€¦
- en: catalog.ngc.nvidia.com](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: catalog.ngc.nvidia.com](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
- en: ğŸ’¡ A Docker container is the ultimate solution for problems like
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ğŸ’¡ Docker å®¹å™¨æ˜¯è§£å†³æ­¤ç±»é—®é¢˜çš„ç»ˆææ–¹æ¡ˆ
- en: â€œHey, it works on my machine. I have no idea why it doesnâ€™t on yours.â€
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œå˜¿ï¼Œå®ƒåœ¨æˆ‘çš„æœºå™¨ä¸Šå¯ä»¥å·¥ä½œï¼Œæˆ‘ä¸çŸ¥é“ä¸ºä»€ä¹ˆåœ¨ä½ çš„æœºå™¨ä¸Šä¸è¡Œã€‚â€
- en: Get comfortable with PyTorch profiler
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç†Ÿæ‚‰ PyTorch åˆ†æå™¨
- en: 'Before optimizing anything, you have to understand how long some parts of your
    code run. Pytorch profiler is *almost* an all-in-one tool for profiling training.
    Itâ€™s able to record:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¼˜åŒ–ä»»ä½•ä¸œè¥¿ä¹‹å‰ï¼Œä½ å¿…é¡»äº†è§£ä½ çš„ä¸€äº›ä»£ç éƒ¨åˆ†è¿è¡Œäº†å¤šä¹…ã€‚PyTorch åˆ†æå™¨å‡ ä¹æ˜¯ä¸€ä¸ªåŠŸèƒ½é½å…¨çš„è®­ç»ƒåˆ†æå·¥å…·ã€‚å®ƒèƒ½å¤Ÿè®°å½•ï¼š
- en: CPU operations timings
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU æ“ä½œçš„æ—¶é—´
- en: CUDA kernels timings
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA å†…æ ¸çš„æ—¶é—´
- en: Memory consumption history
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å†…å­˜æ¶ˆè€—å†å²
- en: Thatâ€™s all you need. And itâ€™s easy to enable!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯ä½ æ‰€éœ€è¦çš„ä¸€åˆ‡ã€‚è€Œä¸”å®ƒå¾ˆå®¹æ˜“å¯ç”¨ï¼
- en: 'To record events, all you need is to embed training into a profiler context
    like this:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è®°å½•äº‹ä»¶ï¼Œä½ åªéœ€è¦åƒè¿™æ ·å°†è®­ç»ƒåµŒå…¥åˆ†æå™¨ä¸Šä¸‹æ–‡ä¸­ï¼š
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: After that, you can launch the tensorboard and view profiling traces. Do not
    forget to install [torch-tb-profiler](https://pypi.org/project/torch-tb-profiler/?ref=alexdremov.me).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹‹åï¼Œä½ å¯ä»¥å¯åŠ¨ tensorboard å¹¶æŸ¥çœ‹åˆ†æè½¨è¿¹ã€‚åˆ«å¿˜äº†å®‰è£… [torch-tb-profiler](https://pypi.org/project/torch-tb-profiler/?ref=alexdremov.me)ã€‚
- en: '[](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
    [## PyTorch Profiler With TensorBoard - PyTorch Tutorials 2.3.0+cu121 documentation'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
    [## PyTorch Profiler ä¸ TensorBoard - PyTorch æ•™ç¨‹ 2.3.0+cu121 æ–‡æ¡£]'
- en: Prepare the data and model Use profiler to record execution events Run the profiler
    Use TensorBoard to view results andâ€¦
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å‡†å¤‡æ•°æ®å’Œæ¨¡å‹ ä½¿ç”¨åˆ†æå™¨è®°å½•æ‰§è¡Œäº‹ä»¶ è¿è¡Œåˆ†æå™¨ ä½¿ç”¨ TensorBoard æŸ¥çœ‹ç»“æœå¹¶â€¦
- en: pytorch.org](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: pytorch.org](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
- en: 'Profiler has a lot of different options, but the most important are `activities`
    and `profile_memory`. You can experiment with other options, but keep in mind
    a simple rule: **the fewer options you''ve enabled, the less overhead you have**.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†æå™¨æœ‰è®¸å¤šä¸åŒçš„é€‰é¡¹ï¼Œä½†æœ€é‡è¦çš„æ˜¯ `activities` å’Œ `profile_memory`ã€‚ä½ å¯ä»¥å°è¯•å…¶ä»–é€‰é¡¹ï¼Œä½†è¯·è®°ä½ä¸€ä¸ªç®€å•çš„è§„åˆ™ï¼š**ä½ å¯ç”¨çš„é€‰é¡¹è¶Šå°‘ï¼Œå¼€é”€å°±è¶Šå°**ã€‚
- en: So, if you want to profile CUDA kernel execution timings, it is a good idea
    to turn off CPU profiling and all other features. In this mode, profiling will
    be as close to the real execution as possible.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œå¦‚æœä½ æƒ³åˆ†æ CUDA å†…æ ¸æ‰§è¡Œçš„æ—¶é—´ï¼Œæœ€å¥½å…³é—­ CPU åˆ†æå’Œå…¶ä»–æ‰€æœ‰åŠŸèƒ½ã€‚åœ¨è¿™ç§æ¨¡å¼ä¸‹ï¼Œåˆ†æå°†å°½å¯èƒ½æ¥è¿‘çœŸå®æ‰§è¡Œã€‚
- en: To make traces easier to understand, consider adding profiling contexts that
    describe core parts of your code. If profiling is not enabled, those are no-op.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è®©è½¨è¿¹æ›´å®¹æ˜“ç†è§£ï¼Œè€ƒè™‘æ·»åŠ æè¿°ä½ ä»£ç æ ¸å¿ƒéƒ¨åˆ†çš„åˆ†æä¸Šä¸‹æ–‡ã€‚å¦‚æœæ²¡æœ‰å¯ç”¨åˆ†æï¼Œè¿™äº›ä¸Šä¸‹æ–‡å°†æ— æ•ˆã€‚
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This way, the labels that you use will be visible in traces. So, it will be
    easier to identify code blocks. Or even more granular inside modeâ€™s forward:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·ï¼Œä½ ä½¿ç”¨çš„æ ‡ç­¾å°†åœ¨è½¨è¿¹ä¸­å¯è§ã€‚è¿™æ ·ï¼Œè¯†åˆ«ä»£ç å—ä¼šæ›´åŠ å®¹æ˜“ã€‚ç”šè‡³åœ¨æ¨¡å¼çš„ forward ä¸­è¿›è¡Œæ›´ç»†ç²’åº¦çš„åˆ†æï¼š
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Understanding PyTorch traces
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç†è§£ PyTorch è½¨è¿¹
- en: 'After you gather traces, open them in the tensorboard. Thatâ€™s what the CPU
    + CUDA profile looks like:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¶é›†è½¨è¿¹åï¼Œåœ¨ tensorboard ä¸­æ‰“å¼€å®ƒä»¬ã€‚è¿™å°±æ˜¯ CPU + CUDA åˆ†æçš„æ ·å­ï¼š
- en: '![](../Images/b207f5f91e4560da06402145f3cfb486.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b207f5f91e4560da06402145f3cfb486.png)'
- en: Â© Copyright 2024, PyTorch | [https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html?ref=alexdremov.me)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Â© ç‰ˆæƒ 2024ï¼ŒPyTorch | [https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html?ref=alexdremov.me)
- en: 'Straight away, find the core parts of any training:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ç«‹åˆ»æ‰¾åˆ°ä»»ä½•è®­ç»ƒçš„æ ¸å¿ƒéƒ¨åˆ†ï¼š
- en: data loading
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ•°æ®åŠ è½½
- en: forward pass
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‰å‘ä¼ æ’­
- en: backward pass
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åå‘ä¼ æ’­
- en: Backward pass is handled by PyTorch in a separate thread (thread 16893 on the
    image above), so it is easy to identify.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: åå‘ä¼ æ’­ç”± PyTorch åœ¨å•ç‹¬çš„çº¿ç¨‹ä¸­å¤„ç†ï¼ˆå¦‚ä¸Šå›¾ä¸­çš„çº¿ç¨‹ 16893ï¼‰ï¼Œæ‰€ä»¥å¾ˆå®¹æ˜“è¯†åˆ«ã€‚
- en: Data loading
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ•°æ®åŠ è½½
- en: For data loading, we want near-zero timings.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ•°æ®åŠ è½½ï¼Œæˆ‘ä»¬å¸Œæœ›æ¥è¿‘é›¶çš„æ—¶é—´ã€‚
- en: No compromises.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æ²¡æœ‰å¦¥åã€‚
- en: Thatâ€™s because during data loading GPU does nothing, which under-utilizes available
    resources. However, data processing can be overlapped with GPU computing as those
    are independent parts.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å› ä¸ºåœ¨æ•°æ®åŠ è½½æœŸé—´ GPU ä¸åšä»»ä½•äº‹æƒ…ï¼Œè¿™ä¼šå¯¼è‡´å¯ç”¨èµ„æºçš„ä½æ•ˆåˆ©ç”¨ã€‚ç„¶è€Œï¼Œæ•°æ®å¤„ç†å¯ä»¥ä¸ GPU è®¡ç®—é‡å ï¼Œå› ä¸ºå®ƒä»¬æ˜¯ç‹¬ç«‹çš„éƒ¨åˆ†ã€‚
- en: You can easily identify areas where GPU is idle â€” just look at *GPU Est. SM
    Efficiency* and *GPU Utilization* figures in the profilerâ€™s trace. Areas with
    zero activity are our patients. Thatâ€™s where GPU does nothing.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥è½»æ¾åœ°è¯†åˆ« GPU é—²ç½®çš„åŒºåŸŸâ€”â€”åªéœ€æŸ¥çœ‹åˆ†æå™¨è·Ÿè¸ªä¸­çš„ *GPU ä¼°ç®— SM æ•ˆç‡* å’Œ *GPU åˆ©ç”¨ç‡* æ•°å€¼ã€‚æ²¡æœ‰æ´»åŠ¨çš„åŒºåŸŸå°±æ˜¯æˆ‘ä»¬çš„â€œæ‚£è€…â€ã€‚è¿™å°±æ˜¯
    GPU ä»€ä¹ˆä¹Ÿä¸åšçš„åœ°æ–¹ã€‚
- en: 'A simple solution for that is:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªç®€å•çš„è§£å†³æ–¹æ¡ˆæ˜¯ï¼š
- en: process data in the background process (no GIL)
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨åå°è¿›ç¨‹ä¸­å¤„ç†æ•°æ®ï¼ˆæ²¡æœ‰ GILï¼‰
- en: process data augmentations and transforms in parallel processes
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨å¹¶è¡Œè¿›ç¨‹ä¸­å¤„ç†æ•°æ®å¢å¼ºå’Œå˜æ¢
- en: If you use PyTorch DataLoader, then it can be easily achieved by specifying
    `num_workers`. It's more complicated if you use `IterableDataset`, as then data
    will be duplicated. However, this issue still can be solved by using [get_worker_info()](https://pytorch.org/docs/stable/data.html?ref=alexdremov.me#torch.utils.data.IterableDataset)
    - you need to adjust iteration in a way so that each worker receives different,
    non-intersecting rows.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ä½¿ç”¨ PyTorch çš„ DataLoaderï¼Œé‚£ä¹ˆå¯ä»¥é€šè¿‡æŒ‡å®š `num_workers` æ¥è½»æ¾å®ç°è¿™ä¸€ç‚¹ã€‚å¦‚æœä½ ä½¿ç”¨ `IterableDataset`ï¼Œæƒ…å†µä¼šæ›´å¤æ‚ï¼Œå› ä¸ºæ•°æ®å°†ä¼šé‡å¤ã€‚ä½†è¿™ä¸ªé—®é¢˜ä»ç„¶å¯ä»¥é€šè¿‡ä½¿ç”¨
    [get_worker_info()](https://pytorch.org/docs/stable/data.html?ref=alexdremov.me#torch.utils.data.IterableDataset)
    æ¥è§£å†³â€”â€”ä½ éœ€è¦è°ƒæ•´è¿­ä»£æ–¹å¼ï¼Œä»¥ç¡®ä¿æ¯ä¸ªå·¥ä½œè¿›ç¨‹æ¥æ”¶ä¸åŒä¸”ä¸é‡å çš„è¡Œã€‚
- en: For more configurable processing, you may consider implementing multi-process
    transforms yourself with `multiprocessing`
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ›´å¯é…ç½®çš„å¤„ç†ï¼Œä½ å¯ä»¥è€ƒè™‘ä½¿ç”¨ `multiprocessing` è‡ªè¡Œå®ç°å¤šè¿›ç¨‹å˜æ¢
- en: ğŸ’¡ If you never checked your codeâ€™s data processing speed, then this slight modification
    can yield **dramatic speedups**
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ğŸ’¡ å¦‚æœä½ ä»æœªæ£€æŸ¥è¿‡ä»£ç çš„æ•°æ®å¤„ç†é€Ÿåº¦ï¼Œé‚£ä¹ˆè¿™ä¸ªå°å°çš„ä¿®æ”¹å¯èƒ½ä¼šå¸¦æ¥ **å‰§çƒˆçš„åŠ é€Ÿ**ã€‚
- en: Making friends with memory allocator
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸å†…å­˜åˆ†é…å™¨æˆä¸ºæœ‹å‹
- en: You want to be friends with PyTorchâ€™s CUDA caching allocator.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¸Œæœ›ä¸ PyTorch çš„ CUDA ç¼“å­˜åˆ†é…å™¨æˆä¸ºæœ‹å‹ã€‚
- en: When you allocate tensors with PyTorch on a CUDA device, PyTorch will use a
    caching allocator. Thatâ€™s because `cudaMalloc`/ `cudaFree` are expensive operations
    that we want to avoid, so PyTorch has its allocator that will try to reuse previously
    allocated through `cudaMalloc` blocks. That is, if PyTorch's allocator has an
    appropriate block available, it will give it straight away without calling `cudaMalloc`.
    That way, `cudaMalloc` is called only at the beginning.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½ åœ¨ CUDA è®¾å¤‡ä¸Šä½¿ç”¨ PyTorch åˆ†é…å¼ é‡æ—¶ï¼ŒPyTorch ä¼šä½¿ç”¨ä¸€ä¸ªç¼“å­˜åˆ†é…å™¨ã€‚è¿™æ˜¯å› ä¸º `cudaMalloc`/ `cudaFree`
    æ˜¯æ˜‚è´µçš„æ“ä½œï¼Œæˆ‘ä»¬å¸Œæœ›é¿å…è°ƒç”¨å®ƒä»¬ï¼Œå› æ­¤ PyTorch æœ‰è‡ªå·±çš„åˆ†é…å™¨ï¼Œå®ƒä¼šå°è¯•é‡ç”¨é€šè¿‡ `cudaMalloc` åˆ†é…çš„å—ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æœ PyTorch
    çš„åˆ†é…å™¨æœ‰åˆé€‚çš„å—å¯ç”¨ï¼Œå®ƒä¼šç›´æ¥æä¾›ï¼Œè€Œä¸éœ€è¦è°ƒç”¨ `cudaMalloc`ã€‚è¿™æ ·ï¼Œ`cudaMalloc` åªä¼šåœ¨ä¸€å¼€å§‹è°ƒç”¨ã€‚
- en: However, if youâ€™re dealing with data of variable length, different forward passes
    will require intermediate tensors of different sizes. So, PyTorchâ€™s allocator
    may not have an appropriate block of data available. In this case, the allocator
    panics and releases allocated previously bocks by calling `cudaFree` to free up
    space for new allocations.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå¦‚æœä½ å¤„ç†çš„æ˜¯é•¿åº¦å¯å˜çš„æ•°æ®ï¼Œä¸åŒçš„å‰å‘ä¼ é€’å°†éœ€è¦ä¸åŒå¤§å°çš„ä¸­é—´å¼ é‡ã€‚å› æ­¤ï¼ŒPyTorch çš„åˆ†é…å™¨å¯èƒ½æ²¡æœ‰åˆé€‚çš„æ•°æ®å—å¯ç”¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåˆ†é…å™¨ä¼šå´©æºƒå¹¶é€šè¿‡è°ƒç”¨
    `cudaFree` é‡Šæ”¾ä¹‹å‰åˆ†é…çš„å—ï¼Œä»¥ä¸ºæ–°çš„åˆ†é…è…¾å‡ºç©ºé—´ã€‚
- en: After that, the allocator starts building its cache again, doing tons of `cudaMalloc`,
    which is an expensive operation. You can spot this problem by looking at the memory
    profiler section of the tensorboard profiler viewer.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹‹åï¼Œåˆ†é…å™¨å¼€å§‹é‡æ–°æ„å»ºå®ƒçš„ç¼“å­˜ï¼Œè¿›è¡Œå¤§é‡çš„ `cudaMalloc`ï¼Œè¿™æ˜¯ä¸€é¡¹æ˜‚è´µçš„æ“ä½œã€‚ä½ å¯ä»¥é€šè¿‡æŸ¥çœ‹å¼ é‡æ¿åˆ†æå™¨è§†å›¾çš„å†…å­˜åˆ†æéƒ¨åˆ†æ¥å‘ç°è¿™ä¸ªé—®é¢˜ã€‚
- en: ğŸ’¡ You also can spot this problem in the traces. It will be visible as calls
    to `cudaMalloc`and `cudaFree`
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ğŸ’¡ ä½ ä¹Ÿå¯ä»¥åœ¨è·Ÿè¸ªè®°å½•ä¸­å‘ç°è¿™ä¸ªé—®é¢˜ã€‚å®ƒå°†ä»¥å¯¹ `cudaMalloc` å’Œ `cudaFree` çš„è°ƒç”¨å½¢å¼æ˜¾ç¤ºå‡ºæ¥
- en: '![](../Images/414070253ac9c94f59f8f522399d7fb4.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/414070253ac9c94f59f8f522399d7fb4.png)'
- en: PyTorch allocator freaks out | Image by the author
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch åˆ†é…å™¨å´©æºƒ | å›¾ç‰‡æ¥è‡ªä½œè€…
- en: As you see, a red line that corresponds to the allocatorâ€™s reserved memory constantly
    changes. That means that PyTorch allocator is not able to efficiently handle allocation
    requests.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œä¸åˆ†é…å™¨ä¿ç•™çš„å†…å­˜å¯¹åº”çš„çº¢çº¿ä¸æ–­å˜åŒ–ã€‚è¿™æ„å‘³ç€ PyTorch çš„åˆ†é…å™¨æ— æ³•æœ‰æ•ˆåœ°å¤„ç†åˆ†é…è¯·æ±‚ã€‚
- en: When allocations are handled without the allocator panicking, the red line is
    completely straight
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å½“åˆ†é…æ“ä½œä¸å†è®©åˆ†é…å™¨å´©æºƒæ—¶ï¼Œçº¢çº¿å®Œå…¨æ˜¯ç›´çš„
- en: '![](../Images/e8e801553645665ba7dff138173a00cd.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e8e801553645665ba7dff138173a00cd.png)'
- en: PyTorch allocator works as expected | Image by the author
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch åˆ†é…å™¨æŒ‰é¢„æœŸå·¥ä½œ | å›¾ç‰‡æ¥è‡ªä½œè€…
- en: As I said, that is usually due to variable shapes of tensors. How to fix that?
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘æ‰€è¯´ï¼Œè¿™é€šå¸¸æ˜¯ç”±äºå¼ é‡çš„å½¢çŠ¶ä¸å›ºå®šã€‚å¦‚ä½•ä¿®å¤è¿™ä¸ªé—®é¢˜ï¼Ÿ
- en: '**Expandable Segments**'
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**å¯æ‰©å±•çš„æ®µ**'
- en: 'The first thing that is worth trying is to set PyTorchâ€™s relatively new allocator
    mode:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä»¶å€¼å¾—å°è¯•çš„äº‹æƒ…æ˜¯è®¾ç½® PyTorch ç›¸å¯¹è¾ƒæ–°çš„åˆ†é…å™¨æ¨¡å¼ï¼š
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*If set to* `*True*`*, this setting instructs the allocator to create CUDA
    allocations that can later be expanded to better handle cases where a job changes
    allocation sizes frequently, such as having a changing batch size.*'
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*å¦‚æœè®¾ç½®ä¸º* `*True*`*ï¼Œæ­¤è®¾ç½®æŒ‡ç¤ºåˆ†é…å™¨åˆ›å»ºå¯ä»¥åœ¨ä»¥åæ‰©å±•çš„CUDAåˆ†é…ï¼Œä»¥æ›´å¥½åœ°å¤„ç†å·¥ä½œè´Ÿè½½é¢‘ç¹æ›´æ”¹åˆ†é…å¤§å°çš„æƒ…å†µï¼Œä¾‹å¦‚æ‰¹é‡å¤§å°å˜åŒ–çš„æƒ…å†µã€‚*'
- en: So, this tells PyTorch allocator to allocate blocks that could be expanded in
    the future, which is exactly our case. Though, if size variations are too big,
    it still may fail to solve the issue. In this case, move to the next option.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œè¿™å‘Šè¯‰PyTorchåˆ†é…å™¨åˆ†é…å°†æ¥å¯èƒ½ä¼šæ‰©å±•çš„å—ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬çš„æƒ…å†µã€‚å°½ç®¡å¦‚æ­¤ï¼Œå¦‚æœå¤§å°å˜åŒ–è¿‡å¤§ï¼Œä»ç„¶å¯èƒ½æ— æ³•è§£å†³é—®é¢˜ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè½¬åˆ°ä¸‹ä¸€ä¸ªé€‰é¡¹ã€‚
- en: '**Make allocations variate less**'
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ä½¿åˆ†é…çš„å˜åŒ–æ›´å°‘**'
- en: Another possible solution is to make data shapes consistent. That way it will
    be easier for the allocator to find an appropriate data block to reuse.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ç§å¯èƒ½çš„è§£å†³æ–¹æ¡ˆæ˜¯ä½¿æ•°æ®å½¢çŠ¶ä¿æŒä¸€è‡´ã€‚è¿™æ ·ï¼Œåˆ†é…å™¨å°†æ›´å®¹æ˜“æ‰¾åˆ°ä¸€ä¸ªé€‚åˆé‡ç”¨çš„æ•°æ®å—ã€‚
- en: To accomplish that, you may pad data to the same sizes. Or you can preheat the
    allocator by running a model with maximum input sizes.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œä½ å¯ä»¥å°†æ•°æ®å¡«å……åˆ°ç›¸åŒçš„å¤§å°ã€‚æˆ–è€…ä½ å¯ä»¥é€šè¿‡è¿è¡Œä¸€ä¸ªå…·æœ‰æœ€å¤§è¾“å…¥å¤§å°çš„æ¨¡å‹æ¥é¢„çƒ­åˆ†é…å™¨ã€‚
- en: You can learn more about PyTorch allocator modification in the following article
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨ä»¥ä¸‹æ–‡ç« ä¸­äº†è§£æ›´å¤šå…³äºPyTorchåˆ†é…å™¨ä¿®æ”¹çš„ä¿¡æ¯ã€‚
- en: '[## CUDA semantics - PyTorch 2.3 documentation'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[## CUDAè¯­ä¹‰ - PyTorch 2.3 æ–‡æ¡£'
- en: A guide to torch.cuda, a PyTorch module to run CUDA operations
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: torch.cuda çš„æŒ‡å—ï¼ŒPyTorchæ¨¡å—ç”¨äºæ‰§è¡ŒCUDAæ“ä½œ
- en: pytorch.org](https://pytorch.org/docs/stable/notes/cuda.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: pytorch.org](https://pytorch.org/docs/stable/notes/cuda.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
- en: Tidy up allocations history
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ•´ç†åˆ†é…å†å²
- en: We want to use all available GPU memory â€” that allows us to run big batches
    and process data faster. However, at some point, you will encounter a *CUDA out-of-memory*
    error when increasing batch size. What causes this error?
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¸Œæœ›ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„GPUå†…å­˜â€”â€”è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿè¿è¡Œæ›´å¤§çš„æ‰¹æ¬¡å¹¶æ›´å¿«åœ°å¤„ç†æ•°æ®ã€‚ç„¶è€Œï¼Œåœ¨æŸä¸ªæ—¶åˆ»ï¼Œå½“å¢åŠ æ‰¹é‡å¤§å°æ—¶ï¼Œä½ å°†é‡åˆ°*CUDAå†…å­˜ä¸è¶³*é”™è¯¯ã€‚æ˜¯ä»€ä¹ˆå¯¼è‡´äº†è¿™ä¸ªé”™è¯¯ï¼Ÿ
- en: To debug this, we can view the allocatorâ€™s memory history. It can be recorded
    through PyTorch and then visualized at [https://pytorch.org/memory_viz](https://pytorch.org/memory_viz?ref=alexdremov.me)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è°ƒè¯•è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥æŸ¥çœ‹åˆ†é…å™¨çš„å†…å­˜å†å²ã€‚å®ƒå¯ä»¥é€šè¿‡PyTorchè®°å½•ï¼Œç„¶ååœ¨[https://pytorch.org/memory_viz](https://pytorch.org/memory_viz?ref=alexdremov.me)ä¸Šè¿›è¡Œå¯è§†åŒ–ã€‚
- en: '**Start:** `torch.cuda.memory._record_memory_history(max_entries=100000)`'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¼€å§‹ï¼š** `torch.cuda.memory._record_memory_history(max_entries=100000)`'
- en: '**Save:** `torch.cuda.memory._dump_snapshot(file_name)`'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¿å­˜ï¼š** `torch.cuda.memory._dump_snapshot(file_name)`'
- en: '**Stop:** `torch.cuda.memory._record_memory_history(enabled=None)`'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åœæ­¢ï¼š** `torch.cuda.memory._record_memory_history(enabled=None)`'
- en: 'Visualization will draw something like this:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å¯è§†åŒ–å°†æ˜¾ç¤ºå¦‚ä¸‹å†…å®¹ï¼š
- en: '![](../Images/270f72428251b625a672af134cf2f454.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/270f72428251b625a672af134cf2f454.png)'
- en: Â© Copyright 2024, PyTorch | [https://pytorch.org/blog/understanding-gpu-memory-1/](https://pytorch.org/blog/understanding-gpu-memory-1/?ref=alexdremov.me)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Â© ç‰ˆæƒ 2024, PyTorch | [https://pytorch.org/blog/understanding-gpu-memory-1/](https://pytorch.org/blog/understanding-gpu-memory-1/?ref=alexdremov.me)
- en: The x-axis represents time, the y-axis represents total used memory, and colourful
    blocks represent tensors. So, it shows when tensors were allocated and when it
    was released.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: xè½´è¡¨ç¤ºæ—¶é—´ï¼Œyè½´è¡¨ç¤ºæ€»ä½¿ç”¨å†…å­˜ï¼Œå½©è‰²å—è¡¨ç¤ºå¼ é‡ã€‚å› æ­¤ï¼Œå®ƒæ˜¾ç¤ºäº†å¼ é‡ä½•æ—¶è¢«åˆ†é…ä»¥åŠä½•æ—¶è¢«é‡Šæ”¾ã€‚
- en: You may notice narrow spikes â€” those are short-lasting tensors that take up
    a lot of space. By clicking on a tensor, you can get information on where this
    tensor was allocated. We want to minimize those spikes as they limit efficient
    memory usage. Check out what caused this spike and consider other ways of computing
    what you intended.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½ä¼šæ³¨æ„åˆ°çª„å°–å³°â€”â€”è¿™äº›æ˜¯å ç”¨å¤§é‡ç©ºé—´çš„çŸ­æš‚å¼ é‡ã€‚é€šè¿‡ç‚¹å‡»å¼ é‡ï¼Œä½ å¯ä»¥è·å¾—è¯¥å¼ é‡åˆ†é…çš„ä½ç½®ã€‚æˆ‘ä»¬å¸Œæœ›å°½é‡å‡å°‘è¿™äº›å°–å³°ï¼Œå› ä¸ºå®ƒä»¬é™åˆ¶äº†å†…å­˜çš„é«˜æ•ˆä½¿ç”¨ã€‚æ£€æŸ¥ä¸€ä¸‹æ˜¯ä»€ä¹ˆå¯¼è‡´äº†è¿™ä¸ªå°–å³°ï¼Œå¹¶è€ƒè™‘å…¶ä»–è®¡ç®—æ–¹å¼ã€‚
- en: 'Apart from spikes, itâ€™s easy to detect memory leaks:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†å°–å³°å¤–ï¼Œæ£€æµ‹å†…å­˜æ³„æ¼ä¹Ÿå¾ˆå®¹æ˜“ï¼š
- en: '![](../Images/d257a3b5af410d7d0bb7b3e46ecda12d.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d257a3b5af410d7d0bb7b3e46ecda12d.png)'
- en: Â© Copyright 2024, PyTorch | [https://pytorch.org/blog/understanding-gpu-memory-1/](https://pytorch.org/blog/understanding-gpu-memory-1/?ref=alexdremov.me)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Â© ç‰ˆæƒ 2024, PyTorch | [https://pytorch.org/blog/understanding-gpu-memory-1/](https://pytorch.org/blog/understanding-gpu-memory-1/?ref=alexdremov.me)
- en: As you see, some data after the first forward is not cleared. By clicking on
    blocks you can get the idea where these tensors come from. In the image is the
    case when gradients are not cleared after the training step, so they lay dead
    during the forward pass, limiting the ability to increase the batch size to fit
    more data.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œç¬¬ä¸€æ¬¡å‰å‘ä¼ æ’­åä¸€äº›æ•°æ®å¹¶æ²¡æœ‰è¢«æ¸…é™¤ã€‚é€šè¿‡ç‚¹å‡»è¿™äº›æ¨¡å—ï¼Œä½ å¯ä»¥å¤§è‡´äº†è§£è¿™äº›å¼ é‡æ¥è‡ªä½•å¤„ã€‚å›¾ä¸­æ˜¾ç¤ºçš„æ˜¯è®­ç»ƒæ­¥éª¤åæ¢¯åº¦æœªè¢«æ¸…é™¤çš„æƒ…å†µï¼Œè¿™å¯¼è‡´å®ƒä»¬åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ä»ç„¶å­˜åœ¨ï¼Œé™åˆ¶äº†å¢åŠ æ‰¹é‡å¤§å°ä»¥é€‚åº”æ›´å¤šæ•°æ®çš„èƒ½åŠ›ã€‚
- en: '[](https://pytorch.org/blog/understanding-gpu-memory-1/?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
    [## Understanding GPU Memory 1: Visualizing All Allocations over Time'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pytorch.org/blog/understanding-gpu-memory-1/?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
    [## ç†è§£GPUå†…å­˜1ï¼šå¯è§†åŒ–æ‰€æœ‰åˆ†é…éšæ—¶é—´å˜åŒ–'
- en: 'During your time with PyTorch on GPUs, you may be familiar with this common
    error message:'
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åœ¨ä½ ä½¿ç”¨PyTorchè¿›è¡ŒGPUè®¡ç®—çš„è¿‡ç¨‹ä¸­ï¼Œä½ å¯èƒ½å·²ç»ç†Ÿæ‚‰è¿™ä¸ªå¸¸è§çš„é”™è¯¯ä¿¡æ¯ï¼š
- en: pytorch.org](https://pytorch.org/blog/understanding-gpu-memory-1/?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[pytorch.org](https://pytorch.org/blog/understanding-gpu-memory-1/?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)'
- en: Speed up the model and use less memory
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŠ é€Ÿæ¨¡å‹å¹¶å‡å°‘å†…å­˜ä½¿ç”¨
- en: What can be better than this? We can achieve so by using the **FlashAttention**
    kernel for calculating dot-product attention.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä»€ä¹ˆæ¯”è¿™æ›´å¥½çš„å‘¢ï¼Ÿæˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨**FlashAttention**å†…æ ¸æ¥è®¡ç®—ç‚¹ç§¯æ³¨æ„åŠ›ï¼Œä»è€Œå®ç°è¿™ä¸€ç›®æ ‡ã€‚
- en: '[](https://github.com/Dao-AILab/flash-attention?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
    [## GitHub - Dao-AILab/flash-attention: Fast and memory-efficient exact attention'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/Dao-AILab/flash-attention?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
    [## GitHub - Dao-AILab/flash-attentionï¼šå¿«é€Ÿä¸”å†…å­˜é«˜æ•ˆçš„ç²¾ç¡®æ³¨æ„åŠ›'
- en: Fast and memory-efficient exact attention. Contribute to Dao-AILab/flash-attention
    development by creating an accountâ€¦
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¿«é€Ÿä¸”å†…å­˜é«˜æ•ˆçš„ç²¾ç¡®æ³¨æ„åŠ›ã€‚é€šè¿‡åˆ›å»ºä¸€ä¸ªè´¦æˆ·ï¼Œä¸ºDao-AILab/flash-attentionçš„å¼€å‘åšå‡ºè´¡çŒ®â€¦
- en: github.com](https://github.com/Dao-AILab/flash-attention?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/Dao-AILab/flash-attention?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)'
- en: If you havenâ€™t heard about it, it is a way of calculating precise dot product
    attention without constructing the attention matrix explicitly. That optimizes
    GPUâ€™s io operations which improves speed and also **dramatically** minimizes memory
    consumption. Thereâ€™s simply no reason not to use it.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ è¿˜æ²¡æœ‰å¬è¯´è¿‡ï¼Œå®ƒæ˜¯ä¸€ç§è®¡ç®—ç²¾ç¡®ç‚¹ç§¯æ³¨æ„åŠ›çš„æ–¹æ³•ï¼Œæ— éœ€æ˜¾å¼æ„é€ æ³¨æ„åŠ›çŸ©é˜µã€‚å®ƒä¼˜åŒ–äº†GPUçš„è¾“å…¥è¾“å‡ºæ“ä½œï¼Œä»è€Œæé«˜äº†é€Ÿåº¦ï¼Œå¹¶ä¸”**å¤§å¹…åº¦**å‡å°‘äº†å†…å­˜æ¶ˆè€—ã€‚ç®€ç›´æ²¡æœ‰ç†ç”±ä¸ä½¿ç”¨å®ƒã€‚
- en: ğŸ˜¡ Unfortunately, thereâ€™s one reason not to use it â€” hardware.
  id: totrans-106
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ğŸ˜¡ ä¸å¹¸çš„æ˜¯ï¼Œæœ‰ä¸€ä¸ªç†ç”±ä¸èƒ½ä½¿ç”¨å®ƒâ€”â€”ç¡¬ä»¶ã€‚
- en: ''
  id: totrans-107
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Flash attention only works with `fp16` and `bf16` precision on compatible hardware.
    That is NVIDIA Ampere, Hooper, etc
  id: totrans-108
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Flash attentionä»…åœ¨å…¼å®¹ç¡¬ä»¶ä¸Šä½¿ç”¨`fp16`å’Œ`bf16`ç²¾åº¦ã€‚è¿™åŒ…æ‹¬NVIDIA Ampereã€Hooperç­‰æ¶æ„ã€‚
- en: Other libraries use flash attention under the hood, so you may consider using
    other variants that better fit your codebase.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä»–åº“åœ¨åº•å±‚ä½¿ç”¨äº†flash attentionï¼Œå› æ­¤ä½ å¯ä»¥è€ƒè™‘ä½¿ç”¨å…¶ä»–æ›´é€‚åˆä½ ä»£ç åº“çš„å˜ç§ã€‚
- en: '**XFormers**'
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**XFormers**'
- en: '[](https://github.com/facebookresearch/xformers?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
    [## GitHub - facebookresearch/xformers: Hackable and optimized Transformers building
    blocks, supportingâ€¦'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/facebookresearch/xformers?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
    [## GitHub - facebookresearch/xformersï¼šå¯æ“ä½œä¸”ä¼˜åŒ–è¿‡çš„Transformeræ„å»ºæ¨¡å—ï¼Œæ”¯æŒâ€¦'
- en: Hackable and optimized Transformers building blocks, supporting a composable
    construction. - facebookresearch/xformers
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¯æ“ä½œä¸”ä¼˜åŒ–è¿‡çš„Transformeræ„å»ºæ¨¡å—ï¼Œæ”¯æŒå¯ç»„åˆæ„å»ºã€‚ - facebookresearch/xformers
- en: github.com](https://github.com/facebookresearch/xformers?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/facebookresearch/xformers?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)'
- en: '**Transformer Engine**'
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**Transformer Engine**'
- en: '[](https://github.com/NVIDIA/TransformerEngine?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
    [## GitHub - NVIDIA/TransformerEngine: A library for accelerating Transformer
    models on NVIDIA GPUsâ€¦'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/NVIDIA/TransformerEngine?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
    [## GitHub - NVIDIA/TransformerEngineï¼šä¸€ä¸ªåŠ é€ŸTransformeræ¨¡å‹åœ¨NVIDIA GPUä¸Šçš„è¿è¡Œçš„åº“â€¦'
- en: A library for accelerating Transformer models on NVIDIA GPUs, including using
    8-bit floating point (FP8) precision onâ€¦
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªåŠ é€ŸTransformeræ¨¡å‹åœ¨NVIDIA GPUä¸Šè¿è¡Œçš„åº“ï¼ŒåŒ…æ‹¬ä½¿ç”¨8ä½æµ®ç‚¹ï¼ˆFP8ï¼‰ç²¾åº¦â€¦
- en: github.com](https://github.com/NVIDIA/TransformerEngine?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/NVIDIA/TransformerEngine?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)'
- en: '**PyTorch itself!**'
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**PyTorch æœ¬èº«ï¼**'
- en: 'That is true, new versions of PyTorch may use flash attention when applicable.
    To activate this mode, you need to execute attention blocks in the context manager
    that specify which attention strategy to use:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯äº‹å®ï¼ŒPyTorch çš„æ–°ç‰ˆæœ¬å¯èƒ½ä¼šåœ¨é€‚ç”¨çš„æƒ…å†µä¸‹ä½¿ç”¨é—ªç”µæ³¨æ„åŠ›ï¼ˆflash attentionï¼‰ã€‚è¦æ¿€æ´»æ­¤æ¨¡å¼ï¼Œä½ éœ€è¦åœ¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸­æ‰§è¡Œæ³¨æ„åŠ›å—ï¼ŒæŒ‡å®šä½¿ç”¨å“ªç§æ³¨æ„åŠ›ç­–ç•¥ï¼š
- en: '[## torch.nn.functional.scaled_dot_product_attention - PyTorch 2.3 documentation'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[## torch.nn.functional.scaled_dot_product_attention - PyTorch 2.3 æ–‡æ¡£](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------#torch-nn-functional-scaled-dot-product-attention)'
- en: Read the PyTorch Domains documentation to learn more about domain-specific libraries
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é˜…è¯» PyTorch Domains æ–‡æ¡£ï¼Œäº†è§£æ›´å¤šå…³äºç‰¹å®šé¢†åŸŸçš„åº“
- en: pytorch.org](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------#torch-nn-functional-scaled-dot-product-attention)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[pytorch.org](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------#torch-nn-functional-scaled-dot-product-attention)'
- en: Optimize multi-GPU data redundancy â€” FSDP
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–å¤š GPU æ•°æ®å†—ä½™ â€” FSDP
- en: If you use multiple GPUs to run your training, the basic solution is to use
    the `DistributedDataParallel` class. This way, several identical processes are
    spawned, and gradients are aggregated during the backward step.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ä½¿ç”¨å¤šä¸ª GPU è¿›è¡Œè®­ç»ƒï¼ŒåŸºæœ¬çš„è§£å†³æ–¹æ¡ˆæ˜¯ä½¿ç”¨ `DistributedDataParallel` ç±»ã€‚è¿™æ ·ï¼Œå¤šä¸ªç›¸åŒçš„è¿›ç¨‹å°†è¢«å¯åŠ¨ï¼Œå¹¶ä¸”åœ¨åå‘ä¼ æ’­æ­¥éª¤ä¸­ä¼šèšåˆæ¢¯åº¦ã€‚
- en: However, that is sub-optimal!
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œè¿™å¹¶ä¸æ˜¯æœ€ä¼˜çš„ï¼
- en: The problem is as we spawned identical processes, then we have identical models
    and optimiser states on each GPU, which is redundant. The solution is to shard
    data across. We can do so using the Fully Sharded Data Parallel PyTorch wrapper.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜åœ¨äºï¼Œå½“æˆ‘ä»¬å¯åŠ¨ç›¸åŒçš„è¿›ç¨‹æ—¶ï¼Œæ¯ä¸ª GPU ä¸Šéƒ½æœ‰ç›¸åŒçš„æ¨¡å‹å’Œä¼˜åŒ–å™¨çŠ¶æ€ï¼Œè¿™é€ æˆäº†å†—ä½™ã€‚è§£å†³æ–¹æ¡ˆæ˜¯å°†æ•°æ®åˆ†ç‰‡ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œçš„ PyTorch
    å°è£…æ¥å®ç°è¿™ä¸€ç‚¹ã€‚
- en: '![](../Images/b542604a9a173f89aed4bc0375e2e515.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b542604a9a173f89aed4bc0375e2e515.png)'
- en: Â© Copyright 2024, PyTorch | https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Â© ç‰ˆæƒ 2024ï¼ŒPyTorch | https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html
- en: How does it work?
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ
- en: 'As I said, when training on several GPUs, each process has exact copies of
    the same data when training with DDP. We can optimize it, by implementing several
    enhancements:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘æ‰€è¯´ï¼Œå½“åœ¨å¤šä¸ª GPU ä¸Šè®­ç»ƒæ—¶ï¼Œä½¿ç”¨ DDP æ—¶æ¯ä¸ªè¿›ç¨‹éƒ½æœ‰ç›¸åŒæ•°æ®çš„å‰¯æœ¬ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡å®ç°å‡ ä¸ªå¢å¼ºåŠŸèƒ½æ¥ä¼˜åŒ–è¿™ä¸€è¿‡ç¨‹ï¼š
- en: Shard optimizer state (ZeRO 1)
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆZeRO 1ï¼‰
- en: When training with DDP, each process holds a complete copy of the optimizer
    states. With ZeRO1, we shard these optimizer states across all ranks such that
    each rank holds only a portion of the optimizer states. During the backward pass,
    each rank only needs to gather the optimizer states relevant to its parameters
    to make an optimization step. This reduction in redundancy helps conserve memory.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½¿ç”¨ DDP è®­ç»ƒæ—¶ï¼Œæ¯ä¸ªè¿›ç¨‹æŒæœ‰ä¼˜åŒ–å™¨çŠ¶æ€çš„å®Œæ•´å‰¯æœ¬ã€‚è€Œä½¿ç”¨ ZeRO1 æ—¶ï¼Œæˆ‘ä»¬å°†è¿™äº›ä¼˜åŒ–å™¨çŠ¶æ€åœ¨æ‰€æœ‰çš„ rank ä¹‹é—´è¿›è¡Œåˆ†ç‰‡ï¼Œä½¿å¾—æ¯ä¸ª rank
    åªæŒæœ‰ä¼˜åŒ–å™¨çŠ¶æ€çš„ä¸€éƒ¨åˆ†ã€‚åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ª rank åªéœ€è¦æ”¶é›†ä¸å…¶å‚æ•°ç›¸å…³çš„ä¼˜åŒ–å™¨çŠ¶æ€æ¥è¿›è¡Œä¼˜åŒ–æ­¥éª¤ã€‚è¿™ç§å‡å°‘å†—ä½™çš„æ–¹å¼æœ‰åŠ©äºèŠ‚çœå†…å­˜ã€‚
- en: ğŸ’¡ In case of the Adam, which holds parameters at roughly twice the model size,
    sharding the optimizer state among 8 ranks means each rank **stores only one quarter
    (2/8) of the total state size.**
  id: totrans-133
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ğŸ’¡ åœ¨ Adam ä¸­ï¼Œç”±äºå…¶å‚æ•°å¤§çº¦æ˜¯æ¨¡å‹å¤§å°çš„ä¸¤å€ï¼Œå°†ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡åˆ° 8 ä¸ª rank ä¸­æ„å‘³ç€æ¯ä¸ª rank **ä»…å­˜å‚¨æ€»çŠ¶æ€å¤§å°çš„å››åˆ†ä¹‹ä¸€ï¼ˆ2/8ï¼‰ã€‚**
- en: Shard gradients (ZeRO 2)
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ†ç‰‡æ¢¯åº¦ï¼ˆZeRO 2ï¼‰
- en: 'We shard optimizer states. Now, we will modify the optimizer step to shard
    gradients too. If one rank has optimizer states for a portion of parameters, then
    we will:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯¹ä¼˜åŒ–å™¨çŠ¶æ€è¿›è¡Œåˆ†ç‰‡ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å°†ä¿®æ”¹ä¼˜åŒ–å™¨æ­¥éª¤ï¼Œä»¥ä¾¿ä¹Ÿå¯¹æ¢¯åº¦è¿›è¡Œåˆ†ç‰‡ã€‚å¦‚æœä¸€ä¸ª rank æ‹¥æœ‰éƒ¨åˆ†å‚æ•°çš„ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°†ï¼š
- en: aggregate all gradients relevant to the states the rank holds
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: èšåˆä¸è¯¥ rank æŒæœ‰çŠ¶æ€ç›¸å…³çš„æ‰€æœ‰æ¢¯åº¦
- en: calculate optimization step
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¡ç®—ä¼˜åŒ–æ­¥éª¤
- en: send optimization step for a portion of parameters to all other ranks
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†éƒ¨åˆ†å‚æ•°çš„ä¼˜åŒ–æ­¥éª¤å‘é€åˆ°æ‰€æœ‰å…¶ä»– rank
- en: As you noticed, now each rank does not need to hold a full replica of gradients.
    We can send gradients to a relevant rank as soon as they are available. So, we
    can reduce peak memory consumption even further.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä½ æ‰€æ³¨æ„åˆ°çš„ï¼Œç°åœ¨æ¯ä¸ª rank ä¸å†éœ€è¦æŒæœ‰å®Œæ•´çš„æ¢¯åº¦å‰¯æœ¬ã€‚æˆ‘ä»¬å¯ä»¥åœ¨æ¢¯åº¦å¯ç”¨æ—¶ç«‹å³å°†å…¶å‘é€åˆ°ç›¸å…³çš„ rankã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥å‡å°‘å³°å€¼å†…å­˜æ¶ˆè€—ã€‚
- en: Shard model parameters (ZeRO 3)
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ†ç‰‡æ¨¡å‹å‚æ•°ï¼ˆZeRO 3ï¼‰
- en: This is about to be epic.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†æ˜¯å²è¯—çº§çš„ã€‚
- en: Why do we need to store a full copy of the model on each rank? Letâ€™s shard model
    parameters between all ranks. Then, weâ€™re going to fetch the required parameters
    just in time during forward and backward.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦åœ¨æ¯ä¸ªrankä¸Šå­˜å‚¨æ¨¡å‹çš„å®Œæ•´å‰¯æœ¬ï¼Ÿè®©æˆ‘ä»¬åœ¨æ‰€æœ‰rankä¹‹é—´åˆ†ç‰‡æ¨¡å‹å‚æ•°ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†åœ¨å‰å‘å’Œåå‘ä¼ æ’­è¿‡ç¨‹ä¸­æŒ‰éœ€å³æ—¶è·å–æ‰€éœ€çš„å‚æ•°ã€‚
- en: ğŸ’¡ In case of large models, these optimisations can drammaticaly decrease memory
    consumption
  id: totrans-143
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ğŸ’¡ å¯¹äºå¤§æ¨¡å‹ï¼Œè¿™äº›ä¼˜åŒ–å¯ä»¥æ˜¾è‘—å‡å°‘å†…å­˜æ¶ˆè€—ã€‚
- en: How to use FSDP?
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¦‚ä½•ä½¿ç”¨FSDPï¼Ÿ
- en: 'Quite simple actually. All we need is to wrap the model with FSDP:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶å®éå¸¸ç®€å•ã€‚æˆ‘ä»¬åªéœ€è¦ç”¨FSDPåŒ…è£…æ¨¡å‹ï¼š
- en: '[PRE4]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You can also specify the sharding strategy of FSDP. For example, we can select
    the `SHARD_GRAD_OP` strategy to achieve behaviour similar to that of ZeRO2\. You
    can learn about other strategies here:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ è¿˜å¯ä»¥æŒ‡å®šFSDPçš„åˆ†ç‰‡ç­–ç•¥ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©`SHARD_GRAD_OP`ç­–ç•¥ï¼Œä»¥å®ç°ç±»ä¼¼ZeRO2çš„è¡Œä¸ºã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œäº†è§£å…¶ä»–ç­–ç•¥ï¼š
- en: '[## FullyShardedDataParallel - PyTorch 2.3 documentation'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[## FullyShardedDataParallel - PyTorch 2.3 æ–‡æ¡£](https://pytorch.org/docs/stable/fsdp.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)'
- en: A wrapper for sharding module parameters across data parallel workers. This
    is inspired by Xu et al. as well as theâ€¦
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç”¨äºåœ¨æ•°æ®å¹¶è¡Œå·¥ä½œè€…ä¹‹é—´åˆ†é…åˆ†ç‰‡æ¨¡å—å‚æ•°çš„åŒ…è£…å™¨ã€‚è¿™ä¸ªçµæ„Ÿæ¥è‡ªäºXuç­‰äººä»¥åŠâ€¦â€¦
- en: pytorch.org](https://pytorch.org/docs/stable/fsdp.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------#torch.distributed.fsdp.ShardingStrategy)
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[pytorch.org](https://pytorch.org/docs/stable/fsdp.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------#torch.distributed.fsdp.ShardingStrategy)'
- en: Also, you can wrap with FSDP submodules. In the example above, only one FSDP
    module is used, which will reduce computation efficiency and memory efficiency.
    The way it works is that, suppose your model contains 100 Linear layers. If you
    do FSDP(model), there will only be one FSDP unit which wraps the entire model.
    In that case, the allgather would collect the full parameters for all 100 linear
    layers, and hence wonâ€™t save CUDA memory for parameter sharding.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œä½ è¿˜å¯ä»¥ä½¿ç”¨FSDPåŒ…è£…å­æ¨¡å—ã€‚åœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œåªä½¿ç”¨äº†ä¸€ä¸ªFSDPæ¨¡å—ï¼Œè¿™æ ·ä¼šé™ä½è®¡ç®—æ•ˆç‡å’Œå†…å­˜æ•ˆç‡ã€‚å®ƒçš„å·¥ä½œåŸç†æ˜¯ï¼Œå‡è®¾ä½ çš„æ¨¡å‹åŒ…å«100ä¸ªLinearå±‚ã€‚å¦‚æœä½ æ‰§è¡ŒFSDP(model)ï¼Œé‚£ä¹ˆå°†åªæœ‰ä¸€ä¸ªFSDPå•å…ƒåŒ…è£…æ•´ä¸ªæ¨¡å‹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œallgatherä¼šæ”¶é›†æ‰€æœ‰100ä¸ªLinearå±‚çš„å®Œæ•´å‚æ•°ï¼Œå› æ­¤ä¸ä¼šä¸ºå‚æ•°åˆ†ç‰‡èŠ‚çœCUDAå†…å­˜ã€‚
- en: 'You can wrap submodules explicitly or define an auto-wrap policy. To learn
    more about FSDP, read the PyTorch guide:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥æ˜¾å¼åœ°åŒ…è£…å­æ¨¡å—æˆ–å®šä¹‰è‡ªåŠ¨åŒ…è£…ç­–ç•¥ã€‚è¦äº†è§£æ›´å¤šå…³äºFSDPçš„ä¿¡æ¯ï¼Œè¯·é˜…è¯»PyTorchæŒ‡å—ï¼š
- en: '[](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
    [## Getting Started with Fully Sharded Data Parallel(FSDP) - PyTorch Tutorials
    2.3.0+cu121â€¦'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
    [## å…¥é—¨ï¼šå®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œ(FSDP) - PyTorchæ•™ç¨‹ 2.3.0+cu121â€¦'
- en: Note View and edit this tutorial in github. Training AI models at a large scale
    is a challenging task that requires aâ€¦
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šåœ¨GitHubä¸ŠæŸ¥çœ‹å’Œç¼–è¾‘æœ¬æ•™ç¨‹ã€‚å¤§è§„æ¨¡è®­ç»ƒAIæ¨¡å‹æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œè¦æ±‚â€¦â€¦
- en: pytorch.org](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[pytorch.org](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)'
- en: Magic speedup with `torch.compile`
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`torch.compile`çš„ç¥å¥‡åŠ é€Ÿ
- en: That is, torch compile can speed up your code by several percent by just enabling
    it.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿå°±æ˜¯è¯´ï¼Œtorch compileåªéœ€å¯ç”¨å®ƒï¼Œå°±å¯ä»¥è®©ä½ çš„ä»£ç åŠ é€Ÿå‡ ä¸ªç™¾åˆ†ç‚¹ã€‚
- en: Torch traces your execution graph and tries to compile it into an efficient
    format so that the model can be executed almost without Python invocation.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Torchä¼šè·Ÿè¸ªä½ çš„æ‰§è¡Œå›¾ï¼Œå¹¶å°è¯•å°†å…¶ç¼–è¯‘ä¸ºé«˜æ•ˆçš„æ ¼å¼ï¼Œä»¥ä¾¿æ¨¡å‹å‡ ä¹å¯ä»¥åœ¨æ²¡æœ‰Pythonè°ƒç”¨çš„æƒ…å†µä¸‹æ‰§è¡Œã€‚
- en: 'Basic usage is to wrap the model with compile:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºæœ¬ä½¿ç”¨æ–¹æ³•æ˜¯å°†æ¨¡å‹ä¸compileä¸€èµ·åŒ…è£…ï¼š
- en: '[PRE5]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This will execute almost instantly. The actual tracing will happen only during
    the first forward.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å‡ ä¹ä¼šç«‹å³æ‰§è¡Œã€‚å®é™…çš„è·Ÿè¸ªåªä¼šåœ¨ç¬¬ä¸€æ¬¡å‰å‘ä¼ æ’­æ—¶å‘ç”Ÿã€‚
- en: 'It also has a lot of options that are worth to try:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒè¿˜å…·æœ‰è®¸å¤šå€¼å¾—å°è¯•çš„é€‰é¡¹ï¼š
- en: '[## torch.compile - PyTorch 2.3 documentation'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[## torch.compile - PyTorch 2.3 æ–‡æ¡£](https://pytorch.org/docs/stable/generated/torch.compile.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------#torch.compile)'
- en: Optimizes given model/function using TorchDynamo and specified backend. Concretely,
    for every frame executed within theâ€¦
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½¿ç”¨TorchDynamoå’ŒæŒ‡å®šçš„åç«¯ä¼˜åŒ–ç»™å®šçš„æ¨¡å‹/å‡½æ•°ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºåœ¨â€¦â€¦
- en: pytorch.org](https://pytorch.org/docs/stable/generated/torch.compile.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------#torch.compile)
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[pytorch.org](https://pytorch.org/docs/stable/generated/torch.compile.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------#torch.compile)'
- en: ğŸ’¡ Torch compiler is a big feature that will be covered in the next posts!
  id: totrans-166
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ğŸ’¡ Torchç¼–è¯‘å™¨æ˜¯ä¸€ä¸ªé‡è¦ç‰¹æ€§ï¼Œå°†åœ¨åç»­çš„å¸–å­ä¸­è®²è§£ï¼
- en: Stay tuned
  id: totrans-167
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ•¬è¯·æœŸå¾…
- en: 'Learn more about torch compile here:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œäº†è§£æ›´å¤šå…³äºtorch compileçš„ä¿¡æ¯ï¼š
- en: '[## Introduction to torch.compile - PyTorch Tutorials 2.3.0+cu121 documentation'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[## Introduction to torch.compile - PyTorch Tutorials 2.3.0+cu121 documentation'
- en: torch.compile is included in the latest PyTorch. Running TorchInductor on GPU
    requires Triton, which is included withâ€¦
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`torch.compile` å·²åŒ…å«åœ¨æœ€æ–°çš„ PyTorch ç‰ˆæœ¬ä¸­ã€‚åœ¨ GPU ä¸Šè¿è¡Œ TorchInductor éœ€è¦ Tritonï¼Œè€Œ Triton
    å·²åŒ…å«åœ¨â€¦'
- en: pytorch.org](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: pytorch.org](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
- en: Conclusion
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: This post is in no way complete with explanations. Rather, that is a list of
    speed-ups that are worth trying straight away. Hope that it was helpful. Feel
    free to leave a comment!
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡å¹¶éåŒ…å«æ‰€æœ‰è§£é‡Šï¼Œè€Œæ˜¯æä¾›äº†å€¼å¾—ç«‹å³å°è¯•çš„åŠ é€Ÿæ–¹æ³•æ¸…å•ã€‚å¸Œæœ›å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œæ¬¢è¿ç•™è¨€è¯„è®ºï¼
- en: Consider subscribing
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘è®¢é˜…
- en: '*Originally published at* [*https://alexdremov.me*](https://alexdremov.me/simple-ways-to-speedup-your-pytorch-model-training/)
    *on May 28, 2024.*'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '*åŸæ–‡å‘è¡¨äº* [*https://alexdremov.me*](https://alexdremov.me/simple-ways-to-speedup-your-pytorch-model-training/)
    *äº 2024 å¹´ 5 æœˆ 28 æ—¥ã€‚*'
- en: '*Images from PyTorch Blog were used, which is a project of the Linux Foundation
    and is subject to the Linux Foundation* [*policies*](https://www.linuxfoundation.org/legal/terms)*.
    So, all images may be used as allowed by* [*Creative Commons Attribution 3.0 License*](http://creativecommons.org/licenses/by/3.0/)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ç‰‡æ¥æºäº PyTorch åšå®¢ï¼Œè¿™æ˜¯ Linux åŸºé‡‘ä¼šçš„ä¸€ä¸ªé¡¹ç›®ï¼Œå— Linux åŸºé‡‘ä¼šçš„* [*æ”¿ç­–*](https://www.linuxfoundation.org/legal/terms)*çº¦æŸã€‚æ‰€ä»¥ï¼Œæ‰€æœ‰å›¾ç‰‡å‡å¯æŒ‰ç…§*
    [*åˆ›æ„å…±äº« 3.0 è®¸å¯åè®®*](http://creativecommons.org/licenses/by/3.0/) ä½¿ç”¨ã€‚*'
