- en: LLM Fine-Tuning — FAQs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM 微调 — 常见问题解答
- en: 原文：[https://towardsdatascience.com/llm-fine-tuning-faqs-200442827c99?source=collection_archive---------8-----------------------#2024-09-26](https://towardsdatascience.com/llm-fine-tuning-faqs-200442827c99?source=collection_archive---------8-----------------------#2024-09-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/llm-fine-tuning-faqs-200442827c99?source=collection_archive---------8-----------------------#2024-09-26](https://towardsdatascience.com/llm-fine-tuning-faqs-200442827c99?source=collection_archive---------8-----------------------#2024-09-26)
- en: Answering the most common questions I received as an AI consultant
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回答我作为 AI 顾问收到的最常见问题
- en: '[](https://shawhin.medium.com/?source=post_page---byline--200442827c99--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page---byline--200442827c99--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--200442827c99--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--200442827c99--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page---byline--200442827c99--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://shawhin.medium.com/?source=post_page---byline--200442827c99--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page---byline--200442827c99--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--200442827c99--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--200442827c99--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page---byline--200442827c99--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--200442827c99--------------------------------)
    ·7 min read·Sep 26, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--200442827c99--------------------------------)
    ·7分钟阅读·2024年9月26日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Last year, I posted an article on [fine-tuning large language models (LLMs)](/fine-tuning-large-language-models-llms-23473d763b91).
    To my surprise, this turned out to be one of my most-read blogs ever, and it led
    to dozens of conversations with clients about their fine-tuning questions and
    AI projects. Here, I will summarize these conversations' most frequently asked
    questions and my responses.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 去年，我发布了一篇关于[微调大型语言模型（LLMs）](/fine-tuning-large-language-models-llms-23473d763b91)的文章。令我惊讶的是，这篇文章成为了我阅读量最高的博客之一，并且引发了与客户关于微调问题和
    AI 项目的数十次对话。在这里，我将总结这些对话中最常见的问题以及我的回答。
- en: '![](../Images/eca379049c372187ac5c9bd345ce5417.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eca379049c372187ac5c9bd345ce5417.png)'
- en: Image from Canva.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 Canva。
- en: '**What is Fine-tuning?**'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**什么是微调？**'
- en: I like to define [**fine-tuning**](/fine-tuning-large-language-models-llms-23473d763b91)
    as **taking an existing (pre-trained) model and training at least 1 model parameter
    to adapt it to a particular use case**.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢将[**微调**](/fine-tuning-large-language-models-llms-23473d763b91)定义为**采用一个现有的（预训练的）模型，并训练至少
    1 个模型参数以将其适应特定的使用场景**。
- en: It’s important to note the “*training at least 1 model parameter*” part of the
    definition. Some will define fine-tuning without this nuance (including me at
    times). However, this distinguishes fine-tuning from approaches like prompt engineering
    or prefix-tuning, which adapt a model’s behavior without modifying its internal
    operations.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是定义中“*训练至少 1 个模型参数*”这一部分。有些人会在定义中忽略这一细节（包括我自己有时也会忽略）。然而，这一细节将微调与诸如提示工程（prompt
    engineering）或前缀调优（prefix-tuning）等方法区分开来，因为后者是通过不修改模型内部操作的方式来调整模型行为。
- en: '[](/fine-tuning-large-language-models-llms-23473d763b91?source=post_page-----200442827c99--------------------------------)
    [## Fine-Tuning Large Language Models (LLMs)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/fine-tuning-large-language-models-llms-23473d763b91?source=post_page-----200442827c99--------------------------------)
    [## 微调大型语言模型（LLMs）'
- en: A conceptual overview with example Python code
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个带有示例 Python 代码的概念概览
- en: towardsdatascience.com](/fine-tuning-large-language-models-llms-23473d763b91?source=post_page-----200442827c99--------------------------------)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/fine-tuning-large-language-models-llms-23473d763b91?source=post_page-----200442827c99--------------------------------)
- en: '**When NOT to Fine-tune**'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**何时不应进行微调**'
