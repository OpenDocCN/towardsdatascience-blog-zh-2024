- en: How Can Self-Driving Cars Work Better?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动驾驶汽车如何更好地工作？
- en: 原文：[https://towardsdatascience.com/how-can-self-driving-cars-work-better-b3b9ba035d38?source=collection_archive---------5-----------------------#2024-11-28](https://towardsdatascience.com/how-can-self-driving-cars-work-better-b3b9ba035d38?source=collection_archive---------5-----------------------#2024-11-28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-can-self-driving-cars-work-better-b3b9ba035d38?source=collection_archive---------5-----------------------#2024-11-28](https://towardsdatascience.com/how-can-self-driving-cars-work-better-b3b9ba035d38?source=collection_archive---------5-----------------------#2024-11-28)
- en: '**The far-reaching implications of Waymo’s EMMA and other end-to-end driving
    systems**'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**Waymo的EMMA及其他端到端驾驶系统的深远影响**'
- en: '[](https://ramsha4103.medium.com/?source=post_page---byline--b3b9ba035d38--------------------------------)[![Ramsha
    Ali](../Images/0e5478f1fc5dc72b0dea5d2a7167b84b.png)](https://ramsha4103.medium.com/?source=post_page---byline--b3b9ba035d38--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b3b9ba035d38--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b3b9ba035d38--------------------------------)
    [Ramsha Ali](https://ramsha4103.medium.com/?source=post_page---byline--b3b9ba035d38--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ramsha4103.medium.com/?source=post_page---byline--b3b9ba035d38--------------------------------)[![Ramsha
    Ali](../Images/0e5478f1fc5dc72b0dea5d2a7167b84b.png)](https://ramsha4103.medium.com/?source=post_page---byline--b3b9ba035d38--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b3b9ba035d38--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b3b9ba035d38--------------------------------)
    [Ramsha Ali](https://ramsha4103.medium.com/?source=post_page---byline--b3b9ba035d38--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b3b9ba035d38--------------------------------)
    ·7 min read·Nov 28, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b3b9ba035d38--------------------------------)
    ·7分钟阅读·2024年11月28日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/639ae30dc4b482b18a70b594518160c8.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/639ae30dc4b482b18a70b594518160c8.png)'
- en: Photo by [Andy Li](https://unsplash.com/@andylid0?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Andy Li](https://unsplash.com/@andylid0?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Imagine you are a hungry hiker, lost on a trail away from the city. After walking
    many miles, you find a road and spot a faint outline of a car coming towards you.
    You mentally prepare a sympathy pitch for the driver, but your hope turns to horror
    as you realize the car is driving itself. There is no human to showcase your trustworthiness,
    or seek sympathy from.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下你是一位饥饿的登山者，迷失在远离城市的小径上。走了很多英里后，你终于发现了一条道路，并看到一辆车的轮廓正向你驶来。你心里准备了一番同情话语，想向司机求得帮助，但当你意识到这辆车是自动驾驶时，你的希望转为恐惧。车里没有人可以展示你的可信度，或向其寻求同情。
- en: Deciding against jumping in front of the car, you try thumbing a ride, but the
    car’s software clocks you as a weird pedestrian and it whooses past you.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定不冲到车前之后，你尝试招车，但汽车的软件将你识别为一个奇怪的行人，迅速从你身边驶过。
- en: Sometimes having an emergency call button or a live helpline [to satisfy California
    law requirements] is not enough. Some edge cases require intervention, and they
    will happen more often as autonomous cars take up more of our roads. Edge cases
    like these are especially tricky, because they need to be taken on a case by case
    basis. Solving them isn’t as easy as coding a distressed face classifier, unless
    you want people posing distressed faces to get free rides. Maybe the cars can
    make use of human support, ‘tele-guidance’ as Zoox calls it, to vet genuine cases
    while also ensuring the system is not taken advantage of, a realistically boring
    solution that would work… for now. An interesting development in autonomous car
    research holds the key to a more sophisticated solution.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，仅仅拥有一个紧急呼叫按钮或一个实时帮助热线 [以满足加利福尼亚州法律要求] 是不够的。某些极端情况需要干预，随着自动驾驶汽车在我们道路上的普及，这类情况将会变得更加频繁。像这样的极端情况尤其棘手，因为它们需要逐案处理。解决这些问题不像编写一个困惑脸表情分类器那么简单，除非你想让人们摆出困惑的表情来免费搭车。也许汽车可以利用人工支持，Zoox所称的“远程引导”，以筛选出真正的案例，同时确保系统不被滥用，这种现实而无聊的解决方案可以奏效……至少现在是这样。自动驾驶汽车研究中的一个有趣进展为更复杂的解决方案提供了钥匙。
- en: Typically an autonomous driving algorithm works by breaking down driving into
    modular components and getting good at them. This breakdown looks different in
    different companies but a popular one that Waymo and Zoox use, has modules for
    mapping, perception, prediction, and planning.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，自动驾驶算法通过将驾驶任务分解为多个模块来实现，并在这些模块上不断提高技能。不同公司可能有不同的分解方式，但Waymo和Zoox使用的常见分解方式包括：地图绘制、感知、预测和规划模块。
- en: '![](../Images/7d96f84320765e43d8306b7c5497c6ef.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7d96f84320765e43d8306b7c5497c6ef.png)'
- en: 'Figure 1: The base modules that are at the core of traditional self-driving
    cars. *Source: Image by author.*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：传统自动驾驶汽车核心的基础模块。*来源：作者提供的图片。*
- en: Each of these modules only focus on the one function which they are heavily
    trained on, this makes them easier to debug and optimize. Interfaces are then
    engineered on top of these modules to connect them and make them work together.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模块每个只专注于它们被大量训练的某一功能，这使得它们更容易调试和优化。然后，在这些模块之上设计接口，将它们连接起来，使其协同工作。
- en: '![](../Images/c6f342d5569bbf98523242df1410fbf0.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c6f342d5569bbf98523242df1410fbf0.png)'
- en: 'Figure 2: A simplification of how modules are connected through interfaces.
    *Source: Image by author, Ramsha Ali (@ramsha4103).*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：模块如何通过接口连接的简化示意图。*来源：作者，Ramsha Ali (@ramsha4103) 提供的图片。*
- en: After connecting these modules using the interfaces, the pipeline is then further
    trained on simulations and tested in the real world.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 通过接口连接这些模块后，整个管道将进一步在模拟环境中进行训练，并在现实世界中进行测试。
- en: '![](../Images/ca2e1d3f70c2124315136f7dc3b6efb2.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ca2e1d3f70c2124315136f7dc3b6efb2.png)'
- en: 'Figure 3: How the different software pieces in self-driving cars come together.
    *Source: Image by author, Ramsha Ali (@ramsha4103).*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：自动驾驶汽车中不同软件模块如何结合在一起。*来源：作者，Ramsha Ali (@ramsha4103) 提供的图片。*
- en: This approach works well, but it is inefficient. Since each module is trained
    separately, the interfaces often struggle to make them work well together. This
    means the cars adapt badly to novel environments. Often cumulative errors build
    up among modules, made worse by inflexible pre-set rules. The answer might seem
    to just train them on less likely scenarios, which seems plausible intuitively
    but is actually quite implausible. This is because driving scenarios fall under
    a long tailed distribution.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有效，但效率较低。由于每个模块都是单独训练的，接口常常难以让它们良好地协同工作。这意味着汽车在面对新环境时适应能力较差。通常，模块间会积累误差，且这些误差由于不灵活的预设规则而变得更严重。解决方案似乎是让它们在不太可能的场景上训练，直觉上这看起来是可行的，但实际上却并不可行。原因在于，驾驶场景遵循长尾分布。
- en: '![](../Images/f9717fabd4afa2e4f52bab9d6bfeb0b2.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f9717fabd4afa2e4f52bab9d6bfeb0b2.png)'
- en: 'Figure 4: A long tail distribution, showcasing that training the car on less
    likely scenarios gets diminishing marginal returns the further you go. *Source:
    Image by author, Ramsha Ali (@ramsha4103).*'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：长尾分布，展示了在不太可能的场景上训练汽车时，随着训练场景的增加，回报逐渐递减。*来源：作者，Ramsha Ali (@ramsha4103)
    提供的图片。*
- en: This means we have the most likely scenarios that are easily trained, but there
    are so many unlikely scenarios that trying to train our model on them is exceptionally
    computationally expensive and time consuming only to get marginal returns. Scenarios
    like an eagle nose diving from the sky, a sudden sinkhole formation, a utility
    pole collapsing, or driving behind a car with a blown brake light fuse. With a
    car only trained on highly relevant data, with no worldly knowledge, which struggles
    to adapt to novel solutions, this means an endless catch-up game to account for
    all these implausible scenarios, or worse, being forced to add more training scenarios
    when something goes very wrong.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们拥有最有可能的场景，容易训练，但也存在许多不太可能的场景，尝试在这些场景上训练模型会导致极高的计算成本和时间消耗，而得到的回报却微乎其微。比如鹰嘴俯冲下落，突发地陷形成，电力杆倒塌，或者跟在一辆刹车灯熔断的车后面行驶。对于一辆仅在高度相关数据上训练、没有任何世界知识、且难以适应新解决方案的汽车来说，这意味着必须不断追赶，去考虑所有这些不可能发生的场景，或者更糟的是，当出现严重问题时，不得不强行添加更多训练场景。
- en: Two weeks ago, Waymo Research published a paper on EMMA, an end-to-end multimodal
    model which can turn the problem on its head. This end-to-end model instead of
    having modular components, would include an all knowing LLM with all its worldly
    knowledge at the core of the model, this LLM would then be further fine-tuned
    to drive. For example Waymo’s EMMA is built on top of Google’s Gemini while DriveGPT
    is built on top of OpenAI’s ChatGPT.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 两周前，Waymo研究团队发布了一篇关于EMMA的论文，这是一个端到端的多模态模型，可以从根本上改变问题的处理方式。这个端到端模型与其传统的模块化组件不同，它将所有的世界知识都集成在一个全知的LLM核心中，然后进一步微调以实现驾驶功能。例如，Waymo的EMMA建立在Google的Gemini之上，而DriveGPT则建立在OpenAI的ChatGPT之上。
- en: This core is then trained using elaborate prompts to provide context and ask
    questions to deduce its spatial reasoning, road graph estimation, and scene understanding
    capabilities. The LLMs are also asked to offer decoded visualizations, to analyze
    whether the textual explanation matches up with how the LLM would act in a simulation.
    This multimodal infusion with language input makes the training process much more
    simplified as you can have simultaneous training of multiple tasks with a single
    model, allowing for task-specific predictions through simple variations of the
    task prompt.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这个核心模型随后使用精心设计的提示进行训练，以提供上下文并提出问题，推导出其空间推理、道路图估算和场景理解能力。还要求LLM提供解码后的可视化结果，分析文本解释是否与LLM在模拟中可能的行为一致。通过语言输入的多模态融合，使得训练过程变得更加简化，因为可以使用单一模型同时训练多个任务，从而通过简单变更任务提示实现任务特定的预测。
- en: '![](../Images/eac25c8b391429404168d114c691655a.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eac25c8b391429404168d114c691655a.png)'
- en: 'Figure 5: How an end-to-end Vision Language Model is trained to drive. *Source:
    Image by author, Ramsha Ali (@ramsha4103)*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：端到端视觉语言模型如何被训练来驾驶。*来源：作者图像，Ramsha Ali（@ramsha4103）*
- en: Another interesting input is often an ego variable, which has nothing to do
    with how superior the car feels but rather stores data like the car’s location,
    velocity, acceleration and orientation to help the car plan out a route for smooth
    and consistent driving. This improves performance through smoother behavior transitions
    and consistent interactions with surrounding agents in multiple consecutive steps.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的输入变量通常是一个自我变量，这与车辆是否感觉优越无关，而是存储诸如车辆位置、速度、加速度和方向等数据，以帮助车辆规划出一条平稳一致的驾驶路线。这通过更加平滑的行为过渡和与周围代理的连续一致互动来提升性能。
- en: These end-to-end models, when tested through simulations, give us a state-of-the-art
    performance on public benchmarks. How does GPT knowing how to file a 1040 help
    it drive better? Worldly knowledge and logical reasoning capabilities means better
    performance in novel situations. This model also lets us co-train on tasks, which
    outperforms single task models by more than 5.5%, an improvement despite much
    less input (no HD map, no interfaces, and no access to lidar or radar). They are
    also much better at understanding hand gestures, turn signals, or spoken commands
    from other drivers and are socially adept at evaluating driving behaviors and
    aggressiveness of surrounding cars and adjust their predictions accordingly. You
    can also ask them to justify their decisions which gets us around their “black
    box” nature, making validation and traceability of decisions much easier.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这些端到端模型在通过模拟测试时，能够在公共基准测试中提供最先进的性能。GPT知道如何填写1040表格，这如何帮助它开车更好呢？丰富的世界知识和逻辑推理能力意味着在新颖情况下的更好表现。该模型还允许我们在多个任务上共同训练，相较于单任务模型，其性能提高了超过5.5%，尽管输入量更少（没有高清地图、没有接口、也没有激光雷达或雷达的访问权限）。它们在理解手势、转向信号或其他司机的口头指令方面也表现得更好，并且在社交上更善于评估周围汽车的驾驶行为和攻击性，并相应调整预测。你还可以要求它们为决策提供依据，从而绕过它们的“黑箱”特性，使得决策的验证和可追溯性变得更加容易。
- en: In addition to all this, LLMs can also help with creating simulations that they
    can then be tested on, since they can label images and can receive text input
    to create images. This can significantly simplify constructing an easily controllable
    setting for testing and validating the decision boundaries of autonomous driving
    systems and simulating a variety of driving situations.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，LLM（大语言模型）还可以帮助创建它们可以进行测试的模拟环境，因为它们可以为图像加标签，并能接收文本输入来生成图像。这可以显著简化构建一个易于控制的环境，用于测试和验证自动驾驶系统的决策边界，并模拟各种驾驶情境。
- en: This approach is still slower, can input limited image frames and is more computationally
    extensive but as our LLMs get better, faster, less computationally expensive and
    incorporate additional modalities like lidar and radar, we will see this multimodal
    approach surpass specialized expert models in 3D object detection quality exponentially,
    but that might be a few years down the road.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法仍然较慢，能输入的图像帧有限，并且计算要求较高，但随着我们的LLM（大语言模型）变得更好、更快、计算成本更低，并且能结合激光雷达和雷达等附加模态，我们将看到这种多模态方法在三维物体检测质量上以指数级的速度超过专门的专家模型，但这可能还需要几年时间。
- en: As end-to-end autonomous cars drive for longer it would be interesting to see
    how they imprint on the human drivers around them, and develop a unique ‘auto-temperament’
    or personality in each city. It would be a fascinating case study of driving behaviours
    around the world. It would be even more fascinating to see how they impact the
    human drivers around them.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 随着端到端自动驾驶汽车行驶时间的增加，观察它们对周围人类驾驶员的影响，以及在每个城市中发展出独特的“自动气质”或个性，将会很有趣。这将成为一个全球驾驶行为的迷人案例研究。更为迷人的是，看看它们如何影响周围的人类驾驶员。
- en: An end-to-end system would also mean being able to have a conversation with
    the car, like you converse with ChatGPT, or being able to walk up to a car on
    the street and ask it for directions. It also means hearing less stories from
    my friends, who vow to never sit in a Waymo again after it almost ran into a speeding
    ambulance or failed to stop for a low flying bird.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一个端到端系统还意味着能够与汽车进行对话，就像你与ChatGPT对话一样，或者能够在街上走到一辆车前并向它询问方向。这也意味着我将听到更少来自朋友的故事，他们发誓再也不坐Waymo了，因为它差点撞上了一辆超速行驶的救护车，或者没有为低飞的鸟停下。
- en: Imagine an autonomous car not just knowing where it is at what time of day (on
    a desolate highway close to midnight) but also understanding what that means (the
    pedestrian is out of place and likely in trouble). Imagine a car not just being
    able to call for help (because California law demands it) but actually being the
    help because it can logically reason with ethics. Now that would be a car that
    would be worth the ride.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一辆自动驾驶汽车不仅知道自己在什么时间、什么地点（在午夜接近的荒凉高速公路上），还理解这意味着什么（行人不合时宜且可能处于危险中）。想象一辆汽车不仅能够呼叫帮助（因为加利福尼亚州法律要求它这么做），而且能够真正提供帮助，因为它可以从伦理角度做出逻辑推理。那样的车，才值得一试。
- en: '**References:**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献：**'
- en: 'Chen, L., Sinavski, O., Hünermann, J., Karnsund, A., Willmott, A. J., Birch,
    D., Maund, D., & Shotton, J. (2023). *Driving with LLMs: Fusing Object-Level Vector
    Modality for Explainable Autonomous Driving* (arXiv:2310.01957). arXiv. [https://doi.org/10.48550/arXiv.2310.01957](https://doi.org/10.48550/arXiv.2310.01957)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 'Chen, L., Sinavski, O., Hünermann, J., Karnsund, A., Willmott, A. J., Birch,
    D., Maund, D., & Shotton, J. (2023). *Driving with LLMs: Fusing Object-Level Vector
    Modality for Explainable Autonomous Driving* (arXiv:2310.01957). arXiv. [https://doi.org/10.48550/arXiv.2310.01957](https://doi.org/10.48550/arXiv.2310.01957)'
- en: Cui, C., Ma, Y., Cao, X., Ye, W., Zhou, Y., Liang, K., Chen, J., Lu, J., Yang,
    Z., Liao, K.-D., Gao, T., Li, E., Tang, K., Cao, Z., Zhou, T., Liu, A., Yan, X.,
    Mei, S., Cao, J., … Zheng, C. (2024). A Survey on Multimodal Large Language Models
    for Autonomous Driving. *2024 IEEE/CVF Winter Conference on Applications of Computer
    Vision Workshops (WACVW)*, 958–979. [https://doi.org/10.1109/WACVW60836.2024.00106](https://doi.org/10.1109/WACVW60836.2024.00106)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Cui, C., Ma, Y., Cao, X., Ye, W., Zhou, Y., Liang, K., Chen, J., Lu, J., Yang,
    Z., Liao, K.-D., Gao, T., Li, E., Tang, K., Cao, Z., Zhou, T., Liu, A., Yan, X.,
    Mei, S., Cao, J., … Zheng, C. (2024). A Survey on Multimodal Large Language Models
    for Autonomous Driving. *2024 IEEE/CVF Winter Conference on Applications of Computer
    Vision Workshops (WACVW)*, 958–979. [https://doi.org/10.1109/WACVW60836.2024.00106](https://doi.org/10.1109/WACVW60836.2024.00106)
- en: 'Fu, D., Lei, W., Wen, L., Cai, P., Mao, S., Dou, M., Shi, B., & Qiao, Y. (2024).
    *LimSim++: A Closed-Loop Platform for Deploying Multimodal LLMs in Autonomous
    Driving* (arXiv:2402.01246). arXiv. [https://doi.org/10.48550/arXiv.2402.01246](https://doi.org/10.48550/arXiv.2402.01246)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 'Fu, D., Lei, W., Wen, L., Cai, P., Mao, S., Dou, M., Shi, B., & Qiao, Y. (2024).
    *LimSim++: A Closed-Loop Platform for Deploying Multimodal LLMs in Autonomous
    Driving* (arXiv:2402.01246). arXiv. [https://doi.org/10.48550/arXiv.2402.01246](https://doi.org/10.48550/arXiv.2402.01246)'
- en: 'Hwang, J.-J., Xu, R., Lin, H., Hung, W.-C., Ji, J., Choi, K., Huang, D., He,
    T., Covington, P., Sapp, B., Zhou, Y., Guo, J., Anguelov, D., & Tan, M. (2024).
    *EMMA: End-to-End Multimodal Model for Autonomous Driving* (arXiv:2410.23262).
    arXiv. [https://doi.org/10.48550/arXiv.2410.23262](https://doi.org/10.48550/arXiv.2410.23262)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 'Hwang, J.-J., Xu, R., Lin, H., Hung, W.-C., Ji, J., Choi, K., Huang, D., He,
    T., Covington, P., Sapp, B., Zhou, Y., Guo, J., Anguelov, D., & Tan, M. (2024).
    *EMMA: End-to-End Multimodal Model for Autonomous Driving* (arXiv:2410.23262).
    arXiv. [https://doi.org/10.48550/arXiv.2410.23262](https://doi.org/10.48550/arXiv.2410.23262)'
- en: '*The ‘full-stack’: Behind autonomous driving*. (n.d.). Zoox. Retrieved November
    26, 2024, from [https://zoox.com/autonomy](https://zoox.com/autonomy)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*“全栈”：自动驾驶背后的故事*。(n.d.). Zoox. 取自2024年11月26日，[https://zoox.com/autonomy](https://zoox.com/autonomy)'
- en: Wang, B., Duan, H., Feng, Y., Chen, X., Fu, Y., Mo, Z., & Di, X. (2024). *Can
    LLMs Understand Social Norms in Autonomous Driving Games?* (arXiv:2408.12680).
    arXiv. [https://doi.org/10.48550/arXiv.2408.12680](https://doi.org/10.48550/arXiv.2408.12680)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Wang, B., Duan, H., Feng, Y., Chen, X., Fu, Y., Mo, Z., & Di, X. (2024). *大语言模型能否理解自动驾驶游戏中的社会规范？*
    (arXiv:2408.12680). arXiv. [https://doi.org/10.48550/arXiv.2408.12680](https://doi.org/10.48550/arXiv.2408.12680)
- en: 'Wang, Y., Jiao, R., Zhan, S. S., Lang, C., Huang, C., Wang, Z., Yang, Z., &
    Zhu, Q. (2024). *Empowering Autonomous Driving with Large Language Models: A Safety
    Perspective* (arXiv:2312.00812). arXiv. [https://doi.org/10.48550/arXiv.2312.00812](https://doi.org/10.48550/arXiv.2312.00812)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Wang, Y., Jiao, R., Zhan, S. S., Lang, C., Huang, C., Wang, Z., Yang, Z., &
    Zhu, Q. (2024). *利用大语言模型赋能自动驾驶：安全视角* (arXiv:2312.00812). arXiv. [https://doi.org/10.48550/arXiv.2312.00812](https://doi.org/10.48550/arXiv.2312.00812)
- en: 'Xu, Z., Zhang, Y., Xie, E., Zhao, Z., Guo, Y., Wong, K.-Y. K., Li, Z., & Zhao,
    H. (2024). *DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language
    Model* (arXiv:2310.01412). arXiv. [https://doi.org/10.48550/arXiv.2310.01412](https://doi.org/10.48550/arXiv.2310.01412)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 'Xu, Z., Zhang, Y., Xie, E., Zhao, Z., Guo, Y., Wong, K.-Y. K., Li, Z., & Zhao,
    H. (2024). *DriveGPT4: 通过大语言模型实现可解释的端到端自动驾驶* (arXiv:2310.01412). arXiv. [https://doi.org/10.48550/arXiv.2310.01412](https://doi.org/10.48550/arXiv.2310.01412)'
- en: 'Yang, Z., Jia, X., Li, H., & Yan, J. (n.d.). *LLM4Drive: A Survey of Large
    Language Models for Autonomous Driving*.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Yang, Z., Jia, X., Li, H., & Yan, J. (n.d.). *LLM4Drive：大语言模型在自动驾驶中的应用综述*。
