- en: Text Generation with GPT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用GPT进行文本生成
- en: 原文：[https://towardsdatascience.com/text-generation-with-gpt-092db8205cad?source=collection_archive---------9-----------------------#2024-01-29](https://towardsdatascience.com/text-generation-with-gpt-092db8205cad?source=collection_archive---------9-----------------------#2024-01-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/text-generation-with-gpt-092db8205cad?source=collection_archive---------9-----------------------#2024-01-29](https://towardsdatascience.com/text-generation-with-gpt-092db8205cad?source=collection_archive---------9-----------------------#2024-01-29)
- en: How to fine-tune a GPT model to generate a TED description-like text
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何微调GPT模型以生成类似TED描述的文本
- en: '[](https://medium.com/@marcellusruben?source=post_page---byline--092db8205cad--------------------------------)[![Ruben
    Winastwan](../Images/15ad0dd03bf5892510abdf166a1e91e1.png)](https://medium.com/@marcellusruben?source=post_page---byline--092db8205cad--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--092db8205cad--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--092db8205cad--------------------------------)
    [Ruben Winastwan](https://medium.com/@marcellusruben?source=post_page---byline--092db8205cad--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@marcellusruben?source=post_page---byline--092db8205cad--------------------------------)[![Ruben
    Winastwan](../Images/15ad0dd03bf5892510abdf166a1e91e1.png)](https://medium.com/@marcellusruben?source=post_page---byline--092db8205cad--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--092db8205cad--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--092db8205cad--------------------------------)
    [Ruben Winastwan](https://medium.com/@marcellusruben?source=post_page---byline--092db8205cad--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--092db8205cad--------------------------------)
    ·18 min read·Jan 29, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--092db8205cad--------------------------------)
    ·阅读时长18分钟·2024年1月29日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/57fc6e93352a2d16cb2c4fba5f1ddaf3.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57fc6e93352a2d16cb2c4fba5f1ddaf3.png)'
- en: Photo by [Aaron Burden](https://unsplash.com/@aaronburden?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/fountain-pen-on-black-lined-paper-y02jEX_B0O0?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由[Aaron Burden](https://unsplash.com/@aaronburden?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)拍摄，图片来源于[Unsplash](https://unsplash.com/photos/fountain-pen-on-black-lined-paper-y02jEX_B0O0?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
- en: 'If you’re working in the data science or machine learning industry, chances
    are that you’ve heard the term Generative AI before, which refers to AI algorithms
    capable of creating new content like texts, images, or audio. In this article,
    we’re going to delve into one of Generative AI models: the GPT model. As you might
    have guessed, GPT is a foundational model of ChatGPT that can generate sequences
    of texts.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你从事数据科学或机器学习行业，可能已经听说过生成式AI这个术语，它指的是能够创造新内容的AI算法，如文本、图像或音频。在本文中，我们将深入探讨其中一种生成式AI模型：GPT模型。正如你可能猜到的，GPT是ChatGPT的基础模型，能够生成文本序列。
- en: Specifically, we will shortly discuss the fine-tuning and text generation process
    of a GPT model. While there are many established libraries and platforms out there
    that we can use to handle this task, they often abstract away many implementation
    details, leaving us curious about what actually happens under the hood.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将简要讨论GPT模型的微调和文本生成过程。虽然市面上有许多现成的库和平台可供我们使用来处理这一任务，但它们往往会抽象掉许多实现细节，这让我们对实际发生的事情感到好奇。
- en: Therefore, we’ll explore the fine-tuning and text generation process in low-level
    details. This means we’ll cover everything comprehensively, from data preprocessing,
    model building, setting up the loss function, the fine-tuning process, and the
    logic behind text generation after fine-tuning the model.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将深入探讨微调和文本生成过程的低层细节。这意味着我们将全面覆盖所有内容，从数据预处理、模型构建、设置损失函数、微调过程，到微调模型后文本生成的背后逻辑。
- en: So, without further ado, let’s start with the dataset we’ll be using to fine-tune
    our GPT model!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，废话少说，让我们开始使用的数据集，这将用于微调我们的GPT模型！
- en: About the Dataset
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于数据集
