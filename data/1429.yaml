- en: 'Tiny Time Mixers (TTM): A Powerful Zero-Shot Forecasting Model by IBM'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**Tiny Time Mixers (TTM)**：IBM推出的强大零-shot预测模型'
- en: 原文：[https://towardsdatascience.com/tiny-time-mixers-ttm-a-powerful-zero-shot-forecasting-model-by-ibm-576b0e0af583?source=collection_archive---------3-----------------------#2024-06-08](https://towardsdatascience.com/tiny-time-mixers-ttm-a-powerful-zero-shot-forecasting-model-by-ibm-576b0e0af583?source=collection_archive---------3-----------------------#2024-06-08)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/tiny-time-mixers-ttm-a-powerful-zero-shot-forecasting-model-by-ibm-576b0e0af583?source=collection_archive---------3-----------------------#2024-06-08](https://towardsdatascience.com/tiny-time-mixers-ttm-a-powerful-zero-shot-forecasting-model-by-ibm-576b0e0af583?source=collection_archive---------3-----------------------#2024-06-08)
- en: A new lightweight open-source foundation model
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个新的轻量级开源基础模型
- en: '[](https://medium.com/@nikoskafritsas?source=post_page---byline--576b0e0af583--------------------------------)[![Nikos
    Kafritsas](../Images/de965cfcd8fbd8e1baf849017d365cbb.png)](https://medium.com/@nikoskafritsas?source=post_page---byline--576b0e0af583--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--576b0e0af583--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--576b0e0af583--------------------------------)
    [Nikos Kafritsas](https://medium.com/@nikoskafritsas?source=post_page---byline--576b0e0af583--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@nikoskafritsas?source=post_page---byline--576b0e0af583--------------------------------)[![Nikos
    Kafritsas](../Images/de965cfcd8fbd8e1baf849017d365cbb.png)](https://medium.com/@nikoskafritsas?source=post_page---byline--576b0e0af583--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--576b0e0af583--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--576b0e0af583--------------------------------)
    [Nikos Kafritsas](https://medium.com/@nikoskafritsas?source=post_page---byline--576b0e0af583--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--576b0e0af583--------------------------------)
    ·10 min read·Jun 8, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--576b0e0af583--------------------------------)
    ·阅读时间10分钟·2024年6月8日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4489291773c482e9b53559c2610fe74b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4489291773c482e9b53559c2610fe74b.png)'
- en: Created by author using DALLE*3
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用DALLE*3创建
- en: '**If you follow the latest research on LLMs, you will notice two main approaches:**'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果你跟随LLM（大语言模型）领域的最新研究，你会注意到两种主要的研究方法：**'
- en: First, researchers focus on building the largest models possible. Pretraining
    on next-word prediction is crucial for enhancing performance (and where the millions
    of dollars are spent!).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，研究人员专注于构建尽可能大的模型。在下一个词预测任务上进行预训练对于提高性能至关重要（也是花费数百万美元的地方！）。
- en: Second, researchers use techniques like quantization to create smaller and faster
    models — while maintaining strong general performance.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，研究人员使用像量化这样的技术来创建更小、更快的模型——同时保持强大的通用性能。
- en: However, interesting things happen when smaller models outperform much larger
    ones in some tasks. For example, **Llama 3–8B** outperformed **the larger Llama
    2–70B** on the MMLU task!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当较小的模型在某些任务上超过了更大的模型时，事情变得非常有趣。例如，**Llama 3–8B** 在 MMLU 任务中超过了**更大的 Llama
    2–70B**！
- en: '***Tiny Time Mixers*** (**TTM**)**[1],** introduced by ***IBM***, follows the
    second approach. It’s a lightweight model that outperforms larger SOTA models
    — including MOIRAI, on the M4 dataset. **Plus, it’s open source!**'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '***Tiny Time Mixers*** (**TTM**)**[1],** 由***IBM***推出，采用了第二种方法。它是一个轻量级模型，超越了更大的SOTA模型——包括MOIRAI，在M4数据集上的表现尤为突出。**此外，它是开源的！**'
- en: 'This article discusses:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本文讨论了：
- en: The architecture and functionality of *TTM*.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*TTM*的架构和功能。'
- en: The innovative features that make *TTM* exceptional.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使*TTM*与众不同的创新特性。
- en: Benchmark results comparing *TTM* with other models.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*TTM*与其他模型的基准测试结果。'
- en: Let’s get started!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: I’ve launched **AI Horizon Forecast,** a…
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我推出了**AI Horizon Forecast，** 一个...
