- en: Are AI Deep Network Models Converging?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: äººå·¥æ™ºèƒ½æ·±åº¦ç½‘ç»œæ¨¡å‹æ˜¯å¦æ­£åœ¨è¶‹åŒï¼Ÿ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/platonic-representation-hypothesis-c812813d7248?source=collection_archive---------9-----------------------#2024-05-23](https://towardsdatascience.com/platonic-representation-hypothesis-c812813d7248?source=collection_archive---------9-----------------------#2024-05-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/platonic-representation-hypothesis-c812813d7248?source=collection_archive---------9-----------------------#2024-05-23](https://towardsdatascience.com/platonic-representation-hypothesis-c812813d7248?source=collection_archive---------9-----------------------#2024-05-23)
- en: Are artificial intelligence models evolving towards a unified representation
    of reality? The Platonic Representation Hypothesis says ML models are converging.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: äººå·¥æ™ºèƒ½æ¨¡å‹æ˜¯å¦æ­£åœ¨æœç€ç»Ÿä¸€çš„ç°å®è¡¨å¾æ¼”åŒ–ï¼ŸæŸæ‹‰å›¾å¼è¡¨å¾å‡è®¾è®¤ä¸ºï¼Œæœºå™¨å­¦ä¹ æ¨¡å‹æ­£åœ¨è¶‹åŒã€‚
- en: '[](https://medium.com/@itshesamsheikh?source=post_page---byline--c812813d7248--------------------------------)[![Hesam
    Sheikh](../Images/b8d5f4f285eef77634e4c1d4321580ed.png)](https://medium.com/@itshesamsheikh?source=post_page---byline--c812813d7248--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c812813d7248--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c812813d7248--------------------------------)
    [Hesam Sheikh](https://medium.com/@itshesamsheikh?source=post_page---byline--c812813d7248--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@itshesamsheikh?source=post_page---byline--c812813d7248--------------------------------)[![Hesam
    Sheikh](../Images/b8d5f4f285eef77634e4c1d4321580ed.png)](https://medium.com/@itshesamsheikh?source=post_page---byline--c812813d7248--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c812813d7248--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c812813d7248--------------------------------)
    [Hesam Sheikh](https://medium.com/@itshesamsheikh?source=post_page---byline--c812813d7248--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c812813d7248--------------------------------)
    Â·8 min readÂ·May 23, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c812813d7248--------------------------------)
    Â·8åˆ†é’Ÿé˜…è¯»Â·2024å¹´5æœˆ23æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'A recent MIT paper has come to my attention for its impressive claim: AI models
    are converging, even across different modalities â€” vision and language. â€œ*We argue
    that representations in AI models, particularly deep networks, are converging*â€
    is how [**The Platonic Representation Hypothesis**](https://arxiv.org/abs/2405.07987)
    paper begins.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç¯‡è¿‘æœŸçš„ MIT è®ºæ–‡å¼•èµ·äº†æˆ‘çš„æ³¨æ„ï¼Œå› ä¸ºå…¶æå‡ºäº†ä¸€ä¸ªä»¤äººå°è±¡æ·±åˆ»çš„è§‚ç‚¹ï¼šäººå·¥æ™ºèƒ½æ¨¡å‹æ­£åœ¨è¶‹åŒï¼Œå³ä½¿æ˜¯åœ¨ä¸åŒçš„æ¨¡æ€â€”â€”è§†è§‰å’Œè¯­è¨€ä¹‹é—´ã€‚â€œ*æˆ‘ä»¬è®¤ä¸ºï¼Œäººå·¥æ™ºèƒ½æ¨¡å‹ä¸­çš„è¡¨å¾ï¼Œç‰¹åˆ«æ˜¯æ·±åº¦ç½‘ç»œçš„è¡¨å¾ï¼Œæ­£åœ¨è¶‹åŒ*â€ï¼Œè¿™å°±æ˜¯[**æŸæ‹‰å›¾å¼è¡¨å¾å‡è®¾**](https://arxiv.org/abs/2405.07987)è®ºæ–‡çš„å¼€å¤´ã€‚
- en: But how can different models, trained on different datasets and for different
    use cases converge? What has led to this convergence?
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œä¸åŒçš„æ¨¡å‹ï¼Œç»è¿‡ä¸åŒæ•°æ®é›†çš„è®­ç»ƒå¹¶ç”¨äºä¸åŒçš„åº”ç”¨åœºæ™¯ï¼Œå¦‚ä½•èƒ½å¤Ÿè¶‹åŒï¼Ÿæ˜¯ä»€ä¹ˆå¯¼è‡´äº†è¿™ç§è¶‹åŒï¼Ÿ
- en: '*âœ¨This is a paid article. If youâ€™re not a Medium member, you can read this
    for free in my newsletter:* [***Qiubyte***](https://hesamsheikh.substack.com/)***.***'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*âœ¨è¿™æ˜¯ä»˜è´¹æ–‡ç« ã€‚å¦‚æœä½ ä¸æ˜¯ Medium ä¼šå‘˜ï¼Œä½ å¯ä»¥åœ¨æˆ‘çš„æ–°é—»é€šè®¯ä¸­å…è´¹é˜…è¯»æ­¤æ–‡ï¼š* [***Qiubyte***](https://hesamsheikh.substack.com/)***.***'
- en: '![](../Images/7d3f9d3d6b75ce9eeffa8cf92e8fdd3e.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7d3f9d3d6b75ce9eeffa8cf92e8fdd3e.png)'
- en: Platoâ€™s allegory of the cave by [Jan Saenredam](https://en.wikipedia.org/wiki/Allegory_of_the_cave#/media/File:Platon_Cave_Sanraedam_1604.jpg)
    (public domain).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æŸæ‹‰å›¾çš„æ´ç©´å¯“è¨€ï¼Œç”±[Jan Saenredam](https://en.wikipedia.org/wiki/Allegory_of_the_cave#/media/File:Platon_Cave_Sanraedam_1604.jpg)ï¼ˆå…¬æœ‰é¢†åŸŸï¼‰åˆ›ä½œã€‚
- en: 1\. The Platonic Representation Hypothesis
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1. æŸæ‹‰å›¾å¼è¡¨å¾å‡è®¾
- en: We argue that there is a growing similarity in how datapoints are represented
    in different neural network models. This similarity spans across different model
    architectures, training objectives, and even data modalities.
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¤ä¸ºï¼Œä¸åŒç¥ç»ç½‘ç»œæ¨¡å‹ä¸­æ•°æ®ç‚¹çš„è¡¨ç¤ºæ–¹å¼æ­£æ—¥ç›Šç›¸ä¼¼ã€‚è¿™ç§ç›¸ä¼¼æ€§è·¨è¶Šäº†ä¸åŒçš„æ¨¡å‹æ¶æ„ã€è®­ç»ƒç›®æ ‡ï¼Œç”šè‡³æ•°æ®å½¢å¼ã€‚
- en: '![](../Images/5da8256363d2d7ad31122d9252187a10.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5da8256363d2d7ad31122d9252187a10.png)'
- en: 'The Platonic Representation Hypothesis. The visual representation, **X,** and
    the textual one **Y**, are both projections of a common reality, **Z**. (source:
    [Paper](https://arxiv.org/abs/2405.07987))'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æŸæ‹‰å›¾å¼è¡¨å¾å‡è®¾ã€‚è§†è§‰è¡¨å¾**X**å’Œæ–‡æœ¬è¡¨å¾**Y**éƒ½æ˜¯å…±åŒç°å®**Z**çš„æŠ•å½±ã€‚ï¼ˆæ¥æºï¼š[è®ºæ–‡](https://arxiv.org/abs/2405.07987)ï¼‰
- en: introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: The paperâ€™s central argument is that models of various origins and modalities
    are converging to a *representation of reality* â€” the joint distribution over
    the events of the world that generate the data we observe and use to train the
    models.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡çš„æ ¸å¿ƒè®ºç‚¹æ˜¯ï¼Œæ¥æºå’Œå½¢å¼å„å¼‚çš„æ¨¡å‹æ­£åœ¨è¶‹å‘äºä¸€ç§*ç°å®çš„è¡¨å¾*â€”â€”å³æè¿°æˆ‘ä»¬è§‚å¯Ÿåˆ°å¹¶ç”¨äºè®­ç»ƒæ¨¡å‹çš„ä¸–ç•Œäº‹ä»¶çš„è”åˆåˆ†å¸ƒã€‚
- en: The authors argue that this convergence towards a **platonic representation**
    is driven by the underlying structure and nature of the data that models are trained
    on, and by the growing complexity and capability of the models themselves. As
    models encounter various datasets and wider applications, they require a representation
    that captures the fundamental properties commonly found in all data types.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…è®¤ä¸ºï¼Œè¿™ç§è¶‹å‘**æŸæ‹‰å›¾å¼è¡¨ç¤º**çš„æ”¶æ•›æ€§æ˜¯ç”±æ¨¡å‹æ‰€è®­ç»ƒçš„åº•å±‚æ•°æ®ç»“æ„å’Œæ•°æ®æœ¬èº«çš„æ€§è´¨é©±åŠ¨çš„ï¼Œä»¥åŠæ¨¡å‹æœ¬èº«æ—¥ç›Šå¢é•¿çš„å¤æ‚æ€§å’Œèƒ½åŠ›ã€‚éšç€æ¨¡å‹æ¥è§¦åˆ°æ›´å¤šæ ·çš„æ•°æ®é›†å’Œæ›´å¹¿æ³›çš„åº”ç”¨ï¼Œå®ƒä»¬éœ€è¦ä¸€ç§èƒ½å¤Ÿæ•æ‰æ‰€æœ‰æ•°æ®ç±»å‹ä¸­å¸¸è§çš„åŸºæœ¬å±æ€§çš„è¡¨ç¤ºã€‚
- en: '![](../Images/c0a103eba7bdc3841ba60e0dd6101975.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c0a103eba7bdc3841ba60e0dd6101975.png)'
- en: 'An Illustration of The Allegory of the Cave, from Platoâ€™s Republic (art by
    [4edges](https://commons.wikimedia.org/wiki/User:4edges), source: [Wikipedia](https://en.wikipedia.org/wiki/Allegory_of_the_cave#/media/File:An_Illustration_of_The_Allegory_of_the_Cave,_from_Plato%E2%80%99s_Republic.jpg))'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ã€Šæ´ç©´å¯“è¨€ã€‹çš„æ’å›¾ï¼Œæ‘˜è‡ªæŸæ‹‰å›¾çš„ã€Šç†æƒ³å›½ã€‹ï¼ˆè‰ºæœ¯ä½œå“æ¥è‡ª[4edges](https://commons.wikimedia.org/wiki/User:4edges)ï¼Œæ¥æºï¼š[Wikipedia](https://en.wikipedia.org/wiki/Allegory_of_the_cave#/media/File:An_Illustration_of_The_Allegory_of_the_Cave,_from_Plato%E2%80%99s_Republic.jpg))
- en: 2\. Are AI Models Converging?
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2.  AIæ¨¡å‹ä¼šæ”¶æ•›å—ï¼Ÿ
- en: AI models of various scales, even built on diverse architecture and trained
    for different tasks, are showing signs of convergence in how they represent data.
    As these models grow in size and complexity and the feeding data becomes larger
    and varied, their methods of processing data begin to *align.*
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å„ç§è§„æ¨¡çš„AIæ¨¡å‹ï¼Œå³ä½¿æ˜¯åŸºäºä¸åŒæ¶æ„æ„å»ºå¹¶ä¸ºä¸åŒä»»åŠ¡è®­ç»ƒçš„æ¨¡å‹ï¼Œä¹Ÿå¼€å§‹è¡¨ç°å‡ºåœ¨æ•°æ®è¡¨ç¤ºä¸Šçš„æ”¶æ•›è¿¹è±¡ã€‚éšç€è¿™äº›æ¨¡å‹çš„è§„æ¨¡å’Œå¤æ‚åº¦ä¸æ–­å¢é•¿ï¼Œè¾“å…¥æ•°æ®å˜å¾—æ›´åŠ åºå¤§å’Œå¤šæ ·ï¼Œå®ƒä»¬å¤„ç†æ•°æ®çš„æ–¹å¼å¼€å§‹*è¶‹äºä¸€è‡´ã€‚*
- en: Do models trained on different data modalities â€” vision or text, also converge?
    The answer could be *yes!*
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸åŒæ•°æ®æ¨¡æ€â€”â€”è§†è§‰æˆ–æ–‡æœ¬ä¸Šè®­ç»ƒçš„æ¨¡å‹ä¹Ÿä¼šæ”¶æ•›å—ï¼Ÿç­”æ¡ˆå¯èƒ½æ˜¯*æ˜¯çš„ï¼*
- en: 2.1 Vision Models that Talk
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 èƒ½è¯´è¯çš„è§†è§‰æ¨¡å‹
- en: This alignment spans over visual and textual data â€” the paper later confirms
    that the limitations of this theory are that itâ€™s focused on these two modularities
    and not other modalities such as audio, or robotics perception of the world. One
    of the cases [1] to support this is [***LLaVA***](https://llava-vl.github.io/),
    which shows projecting visual features into language features using a 2-layer
    MLP, resulting in state-of-the-art results.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§å¯¹é½è·¨è¶Šäº†è§†è§‰å’Œæ–‡æœ¬æ•°æ®â€”â€”è®ºæ–‡éšåç¡®è®¤ï¼Œè¿™ä¸€ç†è®ºçš„å±€é™æ€§åœ¨äºå®ƒåªå…³æ³¨è¿™ä¸¤ç§æ¨¡æ€ï¼Œè€Œæ²¡æœ‰æ¶‰åŠéŸ³é¢‘æˆ–æœºå™¨äººå¯¹ä¸–ç•Œçš„æ„ŸçŸ¥ç­‰å…¶ä»–æ¨¡æ€ã€‚æ”¯æŒè¿™ä¸€ç‚¹çš„ä¸€ä¸ªæ¡ˆä¾‹[1]æ˜¯[***LLaVA***](https://llava-vl.github.io/)ï¼Œè¯¥æ¡ˆä¾‹å±•ç¤ºäº†é€šè¿‡2å±‚MLPå°†è§†è§‰ç‰¹å¾æŠ•å½±åˆ°è¯­è¨€ç‰¹å¾ä¸­ï¼Œä»è€Œå®ç°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚
- en: '![](../Images/55ffd69249f08c5f8411e44c510cb79c.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55ffd69249f08c5f8411e44c510cb79c.png)'
- en: 'Outline of how LLaVA maps visual features to a Language Model. (source: [LLaVA](https://llava-vl.github.io/3),
    CC-BY)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: LLaVAå¦‚ä½•å°†è§†è§‰ç‰¹å¾æ˜ å°„åˆ°è¯­è¨€æ¨¡å‹çš„æ¦‚è¿°ã€‚ï¼ˆæ¥æºï¼š[LLaVA](https://llava-vl.github.io/3)ï¼ŒCC-BYï¼‰
- en: 2.2 Language Models that See
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 èƒ½çœ‹è§çš„è¯­è¨€æ¨¡å‹
- en: Another interesting example is ***A Vision Check-up for Language Models*** [2]
    which explores the extent to which large language models understand and process
    visual data. The study uses code as a bridge between images and text, as a novel
    approach to feed visual data to LLMs. The paper reveals that LLMs can generate
    images by code that while may not look realistic, still contain enough visual
    information to train vision models.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªæœ‰è¶£çš„ä¾‹å­æ˜¯***å¤§å‹è¯­è¨€æ¨¡å‹çš„è§†åŠ›æ£€æŸ¥***[2]ï¼Œå®ƒæ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç†è§£å’Œå¤„ç†è§†è§‰æ•°æ®æ–¹é¢çš„ç¨‹åº¦ã€‚è¯¥ç ”ç©¶ä½¿ç”¨ä»£ç ä½œä¸ºå›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„æ¡¥æ¢ï¼Œä½œä¸ºå°†è§†è§‰æ•°æ®è¾“å…¥LLMçš„åˆ›æ–°æ–¹æ³•ã€‚è®ºæ–‡æ­ç¤ºäº†LLMå¯ä»¥é€šè¿‡ä»£ç ç”Ÿæˆå›¾åƒï¼Œè¿™äº›å›¾åƒè™½ç„¶å¯èƒ½çœ‹èµ·æ¥ä¸çœŸå®ï¼Œä½†ä»åŒ…å«è¶³å¤Ÿçš„è§†è§‰ä¿¡æ¯æ¥è®­ç»ƒè§†è§‰æ¨¡å‹ã€‚
- en: '![](../Images/b5cf01098f7054317c6704cf8e05dd91.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5cf01098f7054317c6704cf8e05dd91.png)'
- en: Can a language model see? ([source](https://arxiv.org/abs/2401.01862))
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: è¯­è¨€æ¨¡å‹èƒ½çœ‹è§å—ï¼Ÿ([source](https://arxiv.org/abs/2401.01862))
- en: 2.3 Bigger Models, Bigger Alignment
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 æ›´å¤§çš„æ¨¡å‹ï¼Œæ›´å¼ºçš„å¯¹é½
- en: The alignment of different models is correlated with their scale. As an example,
    models trained on *CIFAR-10 classification* that are bigger, show greater alignment
    with each other, compared to smaller models. This means that with the current
    trend of building models in the order of 10s and now 100s of billions, these giants
    will be even more aligned.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸åŒæ¨¡å‹çš„å¯¹é½ä¸å…¶è§„æ¨¡ç›¸å…³ã€‚ä¾‹å¦‚ï¼Œè®­ç»ƒç”¨äº*CIFAR-10åˆ†ç±»*çš„è¾ƒå¤§æ¨¡å‹ï¼Œè¡¨ç°å‡ºæ¯”å°æ¨¡å‹æ›´å¼ºçš„å¯¹é½æ€§ã€‚è¿™æ„å‘³ç€éšç€å½“å‰æ„å»ºæ¨¡å‹çš„è¶‹åŠ¿å‘10äº¿å’Œ100äº¿çº§åˆ«å‘å±•ï¼Œè¿™äº›å·¨å‹æ¨¡å‹å°†ä¼šæ›´åŠ ä¸€è‡´ã€‚
- en: â€œall strong models are alike, each weak model is weak in its own way.â€
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œæ‰€æœ‰å¼ºå¤§çš„æ¨¡å‹éƒ½æ˜¯ç›¸ä¼¼çš„ï¼Œæ¯ä¸ªå¼±æ¨¡å‹éƒ½æ˜¯ä»¥è‡ªå·±ç‹¬ç‰¹çš„æ–¹å¼å¼±ã€‚â€
- en: 3\. Why are AI Models Converging?
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. ä¸ºä»€ä¹ˆAIæ¨¡å‹ä¼šæ”¶æ•›ï¼Ÿ
- en: '![](../Images/da754cb83931f36ce44538279e2c757f.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da754cb83931f36ce44538279e2c757f.png)'
- en: 'The learning process of an AI model, f âˆ— is the trained model, ğ¹ F is the function
    class, ğ¿ L is the loss function depending on the model ğ‘“ f and an input ğ‘¥ x from
    the dataset, ğ‘… R represents the regularization function, and ğ¸ E denotes the expectation
    over the dataset. Each color represents one of the causes of this convergence.
    (source: [Paper](https://arxiv.org/abs/2405.07987))'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: AI æ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ï¼Œf âˆ— æ˜¯è®­ç»ƒåçš„æ¨¡å‹ï¼Œğ¹ F æ˜¯å‡½æ•°ç±»ï¼Œğ¿ L æ˜¯ä¾èµ–äºæ¨¡å‹ ğ‘“ f å’Œæ¥è‡ªæ•°æ®é›†çš„è¾“å…¥ ğ‘¥ x çš„æŸå¤±å‡½æ•°ï¼Œğ‘… R è¡¨ç¤ºæ­£åˆ™åŒ–å‡½æ•°ï¼Œğ¸
    E è¡¨ç¤ºæ•°æ®é›†çš„æœŸæœ›å€¼ã€‚æ¯ç§é¢œè‰²ä»£è¡¨æ”¶æ•›çš„ä¸€ä¸ªåŸå› ã€‚ (æ¥æºï¼š[è®ºæ–‡](https://arxiv.org/abs/2405.07987))
- en: 'In training an AI model, there are elements that contribute most to why AI
    models converge:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒä¸€ä¸ª AI æ¨¡å‹æ—¶ï¼Œæœ‰ä¸€äº›å› ç´ å¯¹ AI æ¨¡å‹ä¸ºä½•ä¼šæ”¶æ•›è´¡çŒ®æœ€å¤§ï¼š
- en: 3.1 Tasks are Becoming General
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 ä»»åŠ¡å˜å¾—æ›´åŠ é€šç”¨
- en: As models are trained to solve tasks that are more and more general simultaneously,
    the size of their solution space becomes smaller and more constrained. More generality
    means trying to learn data points that are closer to *reality*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€æ¨¡å‹è¢«è®­ç»ƒä»¥åŒæ—¶è§£å†³è¶Šæ¥è¶Šå¤šçš„ä»»åŠ¡ï¼Œå…¶è§£å†³æ–¹æ¡ˆç©ºé—´å˜å¾—è¶Šæ¥è¶Šå°ä¸”æ›´åŠ å—é™ã€‚æ›´é«˜çš„é€šç”¨æ€§æ„å‘³ç€å°è¯•å­¦ä¹ æ›´æ¥è¿‘*ç°å®*çš„æ•°æ®ç‚¹ã€‚
- en: '![](../Images/94dad5d024071616ad980eacb4c22f08.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94dad5d024071616ad980eacb4c22f08.png)'
- en: 'The more tasks a model can solve, it is forced to learn a disjoint representation
    that is useful in solving all of those tasks. (source: [Paper](https://arxiv.org/abs/2405.07987))'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ¨¡å‹èƒ½å¤Ÿè§£å†³çš„ä»»åŠ¡è¶Šå¤šï¼Œå®ƒå°±è¢«è¿«å­¦ä¹ ä¸€ä¸ªåœ¨è§£å†³æ‰€æœ‰è¿™äº›ä»»åŠ¡æ—¶éƒ½æœ‰æ•ˆçš„éé‡å è¡¨ç¤ºã€‚ (æ¥æºï¼š[è®ºæ–‡](https://arxiv.org/abs/2405.07987))
- en: '*The Platonic Representation Hypothesis* paper formulates this as ***The Multitask
    Scaling Hypothesis:***'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*æŸæ‹‰å›¾è¡¨ç¤ºå‡è¯´* è®ºæ–‡å°†å…¶è¡¨è¿°ä¸º ***å¤šä»»åŠ¡æ‰©å±•å‡è¯´ï¼š***'
- en: â€œThere are fewer representations that are competent for N tasks than there are
    for M < N tasks. As we train more general models that solve more tasks at once,
    we should expect fewer possible solutions.â€
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œèƒ½å¤Ÿèƒœä»» N ä¸ªä»»åŠ¡çš„è¡¨ç¤ºæ¯”èƒ½å¤Ÿèƒœä»» M < N ä¸ªä»»åŠ¡çš„è¡¨ç¤ºè¦å°‘ã€‚éšç€æˆ‘ä»¬è®­ç»ƒæ›´å¤šé€šç”¨çš„æ¨¡å‹ä»¥åŒæ—¶è§£å†³æ›´å¤šä»»åŠ¡ï¼Œæˆ‘ä»¬åº”è¯¥é¢„æœŸå¯èƒ½çš„è§£å†³æ–¹æ¡ˆä¼šæ›´å°‘ã€‚â€
- en: In other words, the solution to a complex problem is much more narrow than the
    solution to an easy problem. As we are training models that are more and more
    general on gigantic internet-wide datasets across different modalities, you can
    only imagine how constrained the solution space will be.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢å¥è¯è¯´ï¼Œè§£å†³å¤æ‚é—®é¢˜çš„æ–¹æ¡ˆæ¯”è§£å†³ç®€å•é—®é¢˜çš„æ–¹æ¡ˆè¦çª„å¾—å¤šã€‚å½“æˆ‘ä»¬è®­ç»ƒè¶Šæ¥è¶Šé€šç”¨çš„æ¨¡å‹ï¼Œä¸”è¿™äº›æ¨¡å‹åœ¨åºå¤§çš„ã€è·¨ä¸åŒæ¨¡æ€çš„äº’è”ç½‘æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒæ—¶ï¼Œä½ å¯ä»¥æƒ³è±¡è§£å†³æ–¹æ¡ˆç©ºé—´ä¼šæ˜¯å¤šä¹ˆçš„å—é™ã€‚
- en: 3.2 Models are Getting Bigger
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 æ¨¡å‹å˜å¾—è¶Šæ¥è¶Šå¤§
- en: As the capacity of models increases, through more sophisticated architectures,
    larger datasets, or more complex training algorithms, these models develop representations
    that are more similar to each other.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€æ¨¡å‹çš„èƒ½åŠ›å¢å¼ºï¼Œé€šè¿‡æ›´å¤æ‚çš„æ¶æ„ã€æ›´å¤§çš„æ•°æ®é›†æˆ–æ›´å¤æ‚çš„è®­ç»ƒç®—æ³•ï¼Œè¿™äº›æ¨¡å‹å¼€å‘å‡ºçš„è¡¨ç¤ºæ–¹å¼å˜å¾—æ›´åŠ ç›¸ä¼¼ã€‚
- en: '![](../Images/1b9e4e58eaeecfa96213276140d77730.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b9e4e58eaeecfa96213276140d77730.png)'
- en: 'Bigger hypothesis spaces are more likely to converge on a solution, rather
    than smaller spaces. (source: [Paper](https://arxiv.org/abs/2405.07987))'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å¤§çš„å‡è®¾ç©ºé—´æ¯”å°çš„å‡è®¾ç©ºé—´æ›´å®¹æ˜“æ”¶æ•›åˆ°ä¸€ä¸ªè§£ã€‚ (æ¥æºï¼š[è®ºæ–‡](https://arxiv.org/abs/2405.07987))
- en: While *The Platonic Representation Hypothesis* paper doesnâ€™t offer proofs or
    examples for this hypothesis that they call **The Capacity Hypothesis â€”** that
    â€œBigger models are more likely to converge to a shared representation than smaller
    modelsâ€, it seems trivial that bigger models at least have *more capacity* to
    come up with mutual solution spaces than small models.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ *æŸæ‹‰å›¾è¡¨ç¤ºå‡è¯´* è®ºæ–‡å¹¶æœªä¸ºä»–ä»¬æ‰€ç§°ä¹‹ä¸º **èƒ½åŠ›å‡è¯´** æä¾›è¯æ˜æˆ–ç¤ºä¾‹â€”â€”å³â€œæ›´å¤§çš„æ¨¡å‹æ¯”å°çš„æ¨¡å‹æ›´å®¹æ˜“æ”¶æ•›åˆ°å…±äº«è¡¨ç¤ºâ€ï¼Œä½†ä¼¼ä¹æ˜¾è€Œæ˜“è§çš„æ˜¯ï¼Œè‡³å°‘æ›´å¤§çš„æ¨¡å‹æœ‰*æ›´å¤šçš„èƒ½åŠ›*å»å¾—å‡ºå…±åŒçš„è§£ç©ºé—´ï¼Œè¿œè¶…è¿‡å°æ¨¡å‹ã€‚
- en: As AI models scale, thanks to their depth and complexity, they have a greater
    capacity for abstraction. This allows them to capture underlying concepts and
    patterns of the data and wave off noise or outliers, thus arriving at a representation
    that is more generalized and possibly closer to the real world.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€ AI æ¨¡å‹çš„è§„æ¨¡æ‰©å¤§ï¼Œå¾—ç›Šäºå®ƒä»¬çš„æ·±åº¦å’Œå¤æ‚æ€§ï¼Œå®ƒä»¬å…·å¤‡äº†æ›´å¼ºçš„æŠ½è±¡èƒ½åŠ›ã€‚è¿™ä½¿å¾—å®ƒä»¬èƒ½å¤Ÿæ•æ‰æ•°æ®çš„åŸºæœ¬æ¦‚å¿µå’Œæ¨¡å¼ï¼ŒåŒæ—¶æŠ›å¼ƒå™ªå£°æˆ–å¼‚å¸¸å€¼ï¼Œä»è€Œå¾—å‡ºä¸€ä¸ªæ›´åŠ é€šç”¨ä¸”å¯èƒ½æ›´æ¥è¿‘ç°å®ä¸–ç•Œçš„è¡¨ç¤ºã€‚
- en: 3.3 The Simplicity Bias
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 ç®€å•æ€§åå‘
- en: 'Imagine training two **large-scale** neural networks on two separate tasks:
    one model must be able to recognize faces from images, and another is trained
    to interpret the emotions of faces. Initially, these two tasks might seem unrelated
    â€” but would you be surprised to see both models converge on similar ways of representing
    facial features? After all, it all comes down to an accurate identification and
    interpretation of key facial landmarks (eyes, nose, mouth, etc).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ³è±¡ä¸€ä¸‹ï¼Œåœ¨ä¸¤ä¸ªä¸åŒä»»åŠ¡ä¸Šè®­ç»ƒä¸¤ä¸ª**å¤§è§„æ¨¡**ç¥ç»ç½‘ç»œï¼šä¸€ä¸ªæ¨¡å‹å¿…é¡»èƒ½å¤Ÿè¯†åˆ«å›¾åƒä¸­çš„é¢å­”ï¼Œå¦ä¸€ä¸ªæ¨¡å‹è¢«è®­ç»ƒæ¥è§£è¯»é¢å­”çš„æƒ…ç»ªã€‚æœ€åˆï¼Œè¿™ä¸¤ä¸ªä»»åŠ¡ä¼¼ä¹æ²¡æœ‰ä»€ä¹ˆå…³ç³»â€”â€”ä½†æ˜¯ä½ ä¼šæƒŠè®¶åœ°å‘ç°ä¸¤ä¸ªæ¨¡å‹æœ€ç»ˆä¼šåœ¨é¢éƒ¨ç‰¹å¾è¡¨ç¤ºä¸Šè¶‹äºç›¸ä¼¼å—ï¼Ÿæ¯•ç«Ÿï¼Œä¸€åˆ‡å½’ç»“äºå‡†ç¡®è¯†åˆ«å’Œè§£è¯»é¢éƒ¨å…³é”®ç‚¹ï¼ˆçœ¼ç›ã€é¼»å­ã€å˜´å·´ç­‰ï¼‰ã€‚
- en: '![](../Images/406bfc4d6cdfd719d909d9d7d8969fd8.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/406bfc4d6cdfd719d909d9d7d8969fd8.png)'
- en: 'Deep Neural Networks tend towards simpler functions. (source: [Paper](https://arxiv.org/abs/2405.07987))'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦ç¥ç»ç½‘ç»œå€¾å‘äºæ›´ç®€å•çš„å‡½æ•°ã€‚ï¼ˆæ¥æºï¼š[è®ºæ–‡](https://arxiv.org/abs/2405.07987)ï¼‰
- en: 'Several literature points out a tendency of deep neural networks to find simpler
    and more general solutions [3,4,5]. In other words, deep networks favor simple
    solutions. Often called **The Simplicity Bias** the paper formulates it as such:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å¤šç¯‡æ–‡çŒ®æŒ‡å‡ºæ·±åº¦ç¥ç»ç½‘ç»œæœ‰ä¸€ç§å€¾å‘ï¼Œå€¾å‘äºæ‰¾åˆ°æ›´ç®€å•ã€æ›´é€šç”¨çš„è§£å†³æ–¹æ¡ˆ[3,4,5]ã€‚æ¢å¥è¯è¯´ï¼Œæ·±åº¦ç½‘ç»œåçˆ±ç®€å•çš„è§£å†³æ–¹æ¡ˆã€‚é€šå¸¸è¢«ç§°ä¸º**ç®€çº¦åå·®**ï¼Œè®ºæ–‡å°†å…¶è¡¨è¿°ä¸ºï¼š
- en: Deep networks are biased toward finding simple fits to the data, and the bigger
    the model, the stronger the bias. Therefore, as models get bigger, we should expect
    convergence to a smaller solution space.
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ·±åº¦ç½‘ç»œåå‘äºæ‰¾åˆ°å¯¹æ•°æ®çš„ç®€å•æ‹Ÿåˆï¼Œå¹¶ä¸”æ¨¡å‹è¶Šå¤§ï¼Œåå·®è¶Šå¼ºã€‚å› æ­¤ï¼Œéšç€æ¨¡å‹çš„å¢å¤§ï¼Œæˆ‘ä»¬åº”é¢„æœŸå…¶æ”¶æ•›åˆ°æ›´å°çš„è§£å†³ç©ºé—´ã€‚
- en: Why do neural networks show this behavior? Networks show simplicity bias mostly
    because of the fundamental properties of the learning algorithms used to train
    them. Algorithms tend to favor simpler, more generalizable models as a way to
    prevent overfitting and enhance generalization. During training, simpler models
    are more likely to emerge because by capturing the dominant patterns in the data,
    they minimize the loss function more efficiently.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆç¥ç»ç½‘ç»œä¼šè¡¨ç°å‡ºè¿™ç§è¡Œä¸ºï¼Ÿç½‘ç»œè¡¨ç°å‡ºç®€çº¦åå·®ä¸»è¦æ˜¯å› ä¸ºç”¨äºè®­ç»ƒå®ƒä»¬çš„å­¦ä¹ ç®—æ³•çš„åŸºæœ¬å±æ€§ã€‚ç®—æ³•å€¾å‘äºåå¥½æ›´ç®€å•ã€å¯æ³›åŒ–çš„æ¨¡å‹ï¼Œè¿™æ˜¯ä¸ºäº†é˜²æ­¢è¿‡æ‹Ÿåˆå¹¶å¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç®€å•çš„æ¨¡å‹æ›´æœ‰å¯èƒ½å‡ºç°ï¼Œå› ä¸ºé€šè¿‡æ•æ‰æ•°æ®ä¸­çš„ä¸»å¯¼æ¨¡å¼ï¼Œå®ƒä»¬å¯ä»¥æ›´æœ‰æ•ˆåœ°æœ€å°åŒ–æŸå¤±å‡½æ•°ã€‚
- en: Simplicity bias acts as a natural regulator during training. It pushes models
    toward an optimal way of representing and processing data, which is both general
    across tasks and simple enough to be efficiently learned and applied, and this
    increases the chance of models learning mutual hypothesis spaces.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€çº¦åå·®åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å……å½“äº†ä¸€ç§è‡ªç„¶çš„è°ƒèŠ‚å™¨ã€‚å®ƒæ¨åŠ¨æ¨¡å‹æœå‘ä¸€ç§æœ€ä½³çš„æ•°æ®è¡¨ç¤ºå’Œå¤„ç†æ–¹å¼ï¼Œè¿™ç§æ–¹å¼ä¸ä»…èƒ½è·¨ä»»åŠ¡é€šç”¨ï¼Œè€Œä¸”è¶³å¤Ÿç®€å•ï¼Œä¾¿äºé«˜æ•ˆå­¦ä¹ å’Œåº”ç”¨ï¼Œä»è€Œå¢åŠ äº†æ¨¡å‹å­¦ä¹ åˆ°å…±åŒå‡è®¾ç©ºé—´çš„æœºä¼šã€‚
- en: 4\. Implications of This Convergence
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. è¿™ç§æ”¶æ•›æ€§çš„å½±å“
- en: So what if models are converging? First of all, this shows that data across
    different modalities can be more useful than thought before. Fine-tuning vision
    models from pre-trained LLMs or vice-versa could yield surprisingly good results.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œå¦‚æœæ¨¡å‹æ­£åœ¨æ”¶æ•›ï¼Œåˆä¼šæ€ä¹ˆæ ·å‘¢ï¼Ÿé¦–å…ˆï¼Œè¿™è¡¨æ˜ä¸åŒæ¨¡æ€çš„æ•°æ®æ¯”ä»¥å‰è®¤ä¸ºçš„æ›´æœ‰ç”¨ã€‚ä»é¢„è®­ç»ƒçš„LLMå¾®è°ƒè§†è§‰æ¨¡å‹ï¼Œæˆ–åä¹‹ï¼Œå¯èƒ½ä¼šå¾—åˆ°å‡ºä¹æ„æ–™çš„å¥½ç»“æœã€‚
- en: Another implication pointed out by the paper is that **â€œScaling may reduce hallucination
    and biasâ€**. The argument is that as models scale, they can learn from a larger
    and more diverse dataset, which helps them develop a more accurate and robust
    understanding of the world. This enhanced understanding allows them to make predictions
    and generate outputs that are not only more reliable but also less biased.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡ä¸­æŒ‡å‡ºçš„å¦ä¸€ä¸ªå½±å“æ˜¯**â€œè§„æ¨¡åŒ–å¯èƒ½å‡å°‘å¹»è§‰å’Œåè§â€**ã€‚è¿™ä¸€è®ºç‚¹æ˜¯ï¼Œéšç€æ¨¡å‹çš„è§„æ¨¡æ‰©å¤§ï¼Œå®ƒä»¬å¯ä»¥ä»æ›´å¤§ã€æ›´å…·å¤šæ ·æ€§çš„æ•°æ®åº“ä¸­å­¦ä¹ ï¼Œä»è€Œå¸®åŠ©å®ƒä»¬å½¢æˆæ›´å‡†ç¡®ã€æ›´å¥å£®çš„ä¸–ç•Œç†è§£ã€‚è¿™ç§å¢å¼ºçš„ç†è§£ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåšå‡ºæ›´åŠ å¯é ä¸”æ›´å°‘åè§çš„é¢„æµ‹å’Œè¾“å‡ºã€‚
- en: '![](../Images/2165138d4d2f30d11a5c239ac0ba9116.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2165138d4d2f30d11a5c239ac0ba9116.png)'
- en: 'VISION models converge as COMPETENCE increases. (source: [Paper](https://arxiv.org/abs/2405.07987))'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: VISIONæ¨¡å‹éšç€**èƒ½åŠ›**çš„å¢åŠ è€Œæ”¶æ•›ã€‚ï¼ˆæ¥æºï¼š[è®ºæ–‡](https://arxiv.org/abs/2405.07987)ï¼‰
- en: 5\. A Pinch of Salt
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. ä¸€ç‚¹æ€€ç–‘
- en: When it comes to the arguments posed by the paper, you have to consider some
    limitations, almost all of which are also addressed by the paper as well.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è€ƒè™‘è®ºæ–‡ä¸­æå‡ºçš„è®ºç‚¹æ—¶ï¼Œå¿…é¡»è€ƒè™‘ä¸€äº›å±€é™æ€§ï¼Œå‡ ä¹æ‰€æœ‰è¿™äº›å±€é™æ€§éƒ½åœ¨è®ºæ–‡ä¸­æœ‰æ‰€è®¨è®ºã€‚
- en: Firstly, the paper assumes a **bijective projection of reality** in which one
    real-world concept Z, has projections X and Y that can be learned. However, some
    concepts are uniquely inherent in one modularity. Sometimes, language can express
    a concept or feeling that many images canâ€™t, and in the same way, language can
    fail to take the place of an image in describing a visual concept.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè®ºæ–‡å‡è®¾ç°å®ä¸–ç•Œçš„**åŒå°„æŠ•å½±**ï¼Œå…¶ä¸­ä¸€ä¸ªç°å®ä¸–ç•Œæ¦‚å¿µZæœ‰å¯ä»¥å­¦ä¹ çš„æŠ•å½±Xå’ŒYã€‚ç„¶è€Œï¼ŒæŸäº›æ¦‚å¿µæ˜¯ç‹¬ç‰¹åœ°å›ºæœ‰äºæŸä¸€æ¨¡æ€çš„ã€‚æœ‰æ—¶ï¼Œè¯­è¨€èƒ½å¤Ÿè¡¨è¾¾ä¸€ç§æ¦‚å¿µæˆ–æƒ…æ„Ÿï¼Œè€Œè®¸å¤šå›¾åƒæ— æ³•åšåˆ°ï¼Œåä¹‹ï¼Œè¯­è¨€ä¹Ÿå¯èƒ½æ— æ³•æ›¿ä»£å›¾åƒæ¥æè¿°è§†è§‰æ¦‚å¿µã€‚
- en: 'Secondly, as mentioned before, the paper focuses on two modalities: vision
    and language. Thirdly, the argument that â€œAI models are Convergingâ€ only holds
    for multi-task AI models and not specific ones, such as ADAS or Sentiment Analysis
    models.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶æ¬¡ï¼Œæ­£å¦‚å‰é¢æåˆ°çš„ï¼Œè®ºæ–‡å…³æ³¨ä¸¤ç§æ¨¡æ€ï¼šè§†è§‰å’Œè¯­è¨€ã€‚ç¬¬ä¸‰ï¼Œå…³äºâ€œAIæ¨¡å‹æ­£åœ¨è¶‹åŒâ€çš„è®ºç‚¹ä»…é€‚ç”¨äºå¤šä»»åŠ¡AIæ¨¡å‹ï¼Œè€Œä¸é€‚ç”¨äºç‰¹å®šæ¨¡å‹ï¼Œå¦‚ADASæˆ–æƒ…æ„Ÿåˆ†ææ¨¡å‹ã€‚
- en: Lastly, while the paper shows that the alignment of different models **increases**,
    it doesnâ€™t indicate the modelsâ€™ representations become similar. The score of alignment
    between larger models is indeed higher than smaller ones, but still, a score of
    0.16/1.00 leaves some open questions to the research.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œå°½ç®¡è®ºæ–‡è¡¨æ˜ä¸åŒæ¨¡å‹çš„**å¯¹é½åº¦**æœ‰æ‰€**å¢åŠ **ï¼Œä½†å¹¶æœªè¡¨æ˜è¿™äº›æ¨¡å‹çš„è¡¨ç¤ºå˜å¾—ç›¸ä¼¼ã€‚å¤§æ¨¡å‹ä¹‹é—´çš„å¯¹é½åˆ†æ•°ç¡®å®é«˜äºå°æ¨¡å‹ï¼Œä½†å³ä½¿å¦‚æ­¤ï¼Œ0.16/1.00çš„åˆ†æ•°ä»ç„¶ç•™ç»™ç ”ç©¶ä¸€äº›æ‚¬è€Œæœªè§£çš„é—®é¢˜ã€‚
- en: '**ğŸŒŸ Join +1000 people learning about** PythonğŸ, ML/MLOps/AIğŸ¤–, Data ScienceğŸ“ˆ,
    and LLM ğŸ—¯'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸŒŸ åŠ å…¥1000+äººä¸€èµ·å­¦ä¹ ** PythonğŸï¼Œæœºå™¨å­¦ä¹ /æœºå™¨å­¦ä¹ æ“ä½œ/äººå·¥æ™ºèƒ½ğŸ¤–ï¼Œæ•°æ®ç§‘å­¦ğŸ“ˆï¼Œä»¥åŠå¤§è¯­è¨€æ¨¡å‹ ğŸ—¯'
- en: '[**follow me**](https://medium.com/@itshesamsheikh/subscribe)and check out
    my [**X/Twitter**](https://twitter.com/itsHesamSheikh), where I keep you updated
    Daily**.**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[**å…³æ³¨æˆ‘**](https://medium.com/@itshesamsheikh/subscribe)ï¼Œå¹¶æŸ¥çœ‹æˆ‘çš„[**X/Twitter**](https://twitter.com/itsHesamSheikh)ï¼Œæˆ‘æ¯å¤©éƒ½ä¼šä¸ºä½ æä¾›æ›´æ–°**ã€‚**'
- en: '[](https://hesamsheikh.substack.com/?source=post_page-----c812813d7248--------------------------------)
    [## QiuByte | Hesam Sheikh | Substack'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://hesamsheikh.substack.com/?source=post_page-----c812813d7248--------------------------------)
    [## QiuByte | Hesam Sheikh | Substack'
- en: AI, Programming, and Machine Learning, only in the EASY way. Click to read QiuByte,
    by Hesam Sheikh, a Substackâ€¦
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: äººå·¥æ™ºèƒ½ã€ç¼–ç¨‹å’Œæœºå™¨å­¦ä¹ ï¼Œä»…åœ¨ç®€æ˜“çš„æ–¹å¼ä¸‹ã€‚ç‚¹å‡»é˜…è¯»ã€ŠQiuByteã€‹ï¼Œç”±Hesam Sheikhä¸»åŠï¼ŒSubstackâ€¦
- en: hesamsheikh.substack.com](https://hesamsheikh.substack.com/?source=post_page-----c812813d7248--------------------------------)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[hesamsheikh.substack.com](https://hesamsheikh.substack.com/?source=post_page-----c812813d7248--------------------------------)'
- en: Thanks for reading,
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢é˜…è¯»ï¼Œ
- en: â€” Hesam
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: â€” Hesam
- en: '[1] Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. In NeurIPS,
    2023.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] åˆ˜æµ©ï¼Œææ™¨ï¼Œå´å¥‡ï¼Œææ´‹æ°ã€‚ã€Šè§†è§‰æŒ‡ä»¤è°ƒä¼˜ã€‹ã€‚å‘è¡¨äºNeurIPSï¼Œ2023ã€‚'
- en: '[2] Sharma, P., Rott Shaham, T., Baradad, M., Fu, S., Rodriguez-Munoz, A.,
    Duggal, S., Isola, P., and Torralba, A. A vision check-up for language models.
    In arXiv preprint, 2024.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Sharma, P., Rott Shaham, T., Baradad, M., Fu, S., Rodriguez-Munoz, A.,
    Duggal, S., Isola, P., and Torralba, A. ã€Šè¯­è¨€æ¨¡å‹çš„è§†è§‰æ£€æŸ¥ã€‹ã€‚å‘è¡¨äºarXivé¢„å°æœ¬ï¼Œ2024ã€‚'
- en: '[3]H. Shah, K. Tamuly, The Pitfalls of Simplicity Bias in Neural Networks,
    2020, [https://arxiv.org/abs/2006.07710](https://arxiv.org/abs/2006.07710)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] H. Shahï¼ŒK. Tamulyï¼Œã€Šç¥ç»ç½‘ç»œä¸­ç®€å•æ€§åå·®çš„é™·é˜±ã€‹ï¼Œ2020å¹´ï¼Œ[https://arxiv.org/abs/2006.07710](https://arxiv.org/abs/2006.07710)'
- en: '[4] [A brief note on Simplicity Bias](https://www.lesswrong.com/posts/Gyggp2DJRMRLSnhid/a-brief-note-on-simplicity-bias-1)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [å…³äºç®€å•æ€§åå·®çš„ç®€çŸ­è¯´æ˜](https://www.lesswrong.com/posts/Gyggp2DJRMRLSnhid/a-brief-note-on-simplicity-bias-1)'
- en: '[5] [Deep Neural Networks are biased, at initialisation, towards simple functions](/deep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [æ·±åº¦ç¥ç»ç½‘ç»œåœ¨åˆå§‹åŒ–æ—¶åå‘ç®€å•å‡½æ•°](/deep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99)'
