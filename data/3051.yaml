- en: Understanding the Mathematics of PPO in Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解强化学习中PPO的数学原理
- en: 原文：[https://towardsdatascience.com/understanding-the-mathematics-of-ppo-in-reinforcement-learning-467618b2f8d4?source=collection_archive---------2-----------------------#2024-12-21](https://towardsdatascience.com/understanding-the-mathematics-of-ppo-in-reinforcement-learning-467618b2f8d4?source=collection_archive---------2-----------------------#2024-12-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/understanding-the-mathematics-of-ppo-in-reinforcement-learning-467618b2f8d4?source=collection_archive---------2-----------------------#2024-12-21](https://towardsdatascience.com/understanding-the-mathematics-of-ppo-in-reinforcement-learning-467618b2f8d4?source=collection_archive---------2-----------------------#2024-12-21)
- en: Deep dive into RL with PPO for beginners
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入探索适用于初学者的强化学习中的PPO
- en: '[](https://manellenouar.medium.com/?source=post_page---byline--467618b2f8d4--------------------------------)[![Manelle
    Nouar](../Images/292d0f9007b102476fd0f2831bcdafb1.png)](https://manellenouar.medium.com/?source=post_page---byline--467618b2f8d4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--467618b2f8d4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--467618b2f8d4--------------------------------)
    [Manelle Nouar](https://manellenouar.medium.com/?source=post_page---byline--467618b2f8d4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://manellenouar.medium.com/?source=post_page---byline--467618b2f8d4--------------------------------)[![Manelle
    Nouar](../Images/292d0f9007b102476fd0f2831bcdafb1.png)](https://manellenouar.medium.com/?source=post_page---byline--467618b2f8d4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--467618b2f8d4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--467618b2f8d4--------------------------------)
    [Manelle Nouar](https://manellenouar.medium.com/?source=post_page---byline--467618b2f8d4--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--467618b2f8d4--------------------------------)
    ·7 min read·Dec 21, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--467618b2f8d4--------------------------------)
    ·7分钟阅读·2024年12月21日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/5025e297ec787bcf8be293d57e481e27.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5025e297ec787bcf8be293d57e481e27.png)'
- en: Photo by [ThisisEngineering](https://unsplash.com/@thisisengineering?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[ThisisEngineering](https://unsplash.com/@thisisengineering?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Reinforcement Learning (RL) is a branch of Artificial Intelligence that enables
    agents to learn how to interact with their environment. These agents, which range
    from robots to software features or autonomous systems, learn through trial and
    error. They receive rewards or penalties based on the actions they take, which
    guide their future decisions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）是人工智能的一个分支，使得智能体能够学习如何与环境进行交互。这些智能体，包括机器人、软件功能或自主系统，通过试错学习。他们根据所采取的行动获得奖励或惩罚，这些反馈指导他们未来的决策。
- en: Among the most well-known RL algorithms, Proximal Policy Optimization (PPO)
    is often favored for its stability and efficiency. PPO addresses several challenges
    in RL, particularly in controlling how the policy (the agent’s decision-making
    strategy) evolves. Unlike other algorithms, PPO ensures that policy updates are
    not too large, preventing destabilization during training. This stabilization
    is crucial, as drastic updates can cause the agent to diverge from an optimal
    solution, making the learning process erratic. PPO thus maintains a balance between
    exploration (trying new actions) and exploitation (focusing on actions that yield
    the highest rewards).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在最著名的强化学习算法中，近端策略优化（PPO）因其稳定性和高效性而被广泛青睐。PPO解决了强化学习中的若干挑战，特别是在控制策略（智能体的决策制定策略）演变方面。与其他算法不同，PPO确保策略更新不会过大，从而防止训练过程中出现不稳定现象。这种稳定性至关重要，因为过大的更新可能导致智能体偏离最优解，使得学习过程变得不稳定。因此，PPO在探索（尝试新行动）与利用（专注于获得最大奖励的行动）之间保持平衡。
- en: Additionally, PPO is highly efficient in terms of both computational resources
    and learning speed. By optimizing the agent’s policy effectively while avoiding
    overly complex calculations, PPO has become a practical solution in various domains,
    such as robotics, gaming, and autonomous systems. Its simplicity makes it easy
    to implement, which has led to its widespread adoption in both research and industry.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，PPO在计算资源和学习速度方面也非常高效。通过有效优化智能体的策略，同时避免过于复杂的计算，PPO已成为机器人、游戏和自主系统等各个领域的实用解决方案。其简洁性使得它易于实现，这也是它在学术研究和工业界广泛应用的原因之一。
- en: This article explores the mathematical foundations of RL and the key concepts
    introduced by PPO, providing a deeper understanding of why PPO has become a go-to
    algorithm in modern reinforcement learning research.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本文探讨了强化学习（RL）的数学基础以及PPO所引入的关键概念，帮助深入理解为何PPO成为现代强化学习研究中的常用算法。
- en: '1\. The Basics of RL: Markov Decision Process (MDP)'
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 强化学习基础：马尔可夫决策过程（MDP）
- en: Reinforcement learning problems are often modeled using a Markov Decision Process
    (MDP), a mathematical framework that helps formalize decision-making in environments
    where outcomes are uncertain.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习问题通常使用马尔可夫决策过程（MDP）进行建模，MDP是一个数学框架，有助于在结果不确定的环境中形式化决策过程。
- en: A Markov chain models a system that transitions between states, where the probability
    of moving to a new state depends solely on the current state and not on previous
    states. This principle is known as the *Markov property*. In the context of MDPs,
    this simplification is key for modeling decisions, as it allows an agent to focus
    only on the current state when making decisions without needing to account for
    the entire history of the system.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫链模型描述了一个系统，该系统在状态之间转移，其中转移到新状态的概率仅依赖于当前状态，而与之前的状态无关。这一原理被称为*马尔可夫性质*。在MDP的背景下，这一简化对于建模决策至关重要，因为它使得智能体在做决策时仅需要关注当前状态，而不必考虑系统的整个历史。
- en: 'An MDP is defined by the following elements:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: MDP由以下元素定义：
- en: '- S: Set of possible states.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '- S: 可能的状态集合。'
- en: '- A: Set of possible actions.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '- A: 可能的动作集合。'
- en: '- P(s’|s, a): Transition probability of reaching state s’ after taking action
    a in state s.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '- P(s’|s, a): 在状态s下采取动作a后到达状态s’的转移概率。'
- en: '- R(s, a): Reward received after taking action a in state s.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '- R(s, a): 在状态s下采取动作a后获得的奖励。'
- en: '- γ: Discount factor (a value between 0 and 1) that reflects the importance
    of future rewards.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '- γ: 折扣因子（介于0和1之间的值），反映了未来奖励的重要性。'
- en: The discount factor γ is crucial for modeling the importance of future rewards
    in decision-making problems. When an agent makes a decision, it must evaluate
    not only the immediate reward but also the potential future rewards. The discount
    γ reduces the impact of rewards that occur later in time due to the uncertainty
    of reaching those rewards. Thus, a value of γ close to 1 indicates that future
    rewards are almost as important as immediate rewards, while a value close to 0
    gives more importance to immediate rewards.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 折扣因子γ在决策问题中对建模未来奖励的重要性至关重要。当智能体做出决策时，它不仅要评估即时奖励，还必须考虑潜在的未来奖励。折扣γ减少了由于不确定性导致的未来奖励的影响，因此，接近1的γ值意味着未来奖励几乎与即时奖励同等重要，而接近0的值则更侧重于即时奖励。
- en: The time discount reflects the agent’s preference for quick gains over future
    ones, often due to uncertainty or the possibility of changes in the environment.
    For example, an agent will likely prefer an immediate reward rather than one in
    the distant future unless that future reward is sufficiently significant. This
    discount factor thus models optimization behaviors where the agent considers both
    short-term and long-term benefits.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 时间折扣反映了智能体对快速收益相对于未来收益的偏好，这通常源于不确定性或环境可能发生变化的原因。例如，除非未来的奖励足够显著，智能体通常会更倾向于选择即时奖励，而不是远期奖励。因此，这一折扣因子建模了优化行为，其中智能体在做决策时会同时考虑短期和长期的收益。
- en: 'The goal is to find an action policy π(a|s) that maximizes the expected sum
    of rewards over time, often referred to as the value function:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是找到一个动作策略π(a|s)，它最大化随时间累积的期望奖励总和，通常称为价值函数：
- en: This function represents the expected total reward an agent can accumulate starting
    from state s and following policy π.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数表示智能体从状态s开始并遵循策略π时，能够积累的期望总奖励。
- en: '2\. Policy Optimization: Policy Gradient'
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 策略优化：策略梯度
- en: Policy gradient methods focus on directly optimizing the parameters θ of a policy
    πθ by maximizing an objective function that represents the expected reward obtained
    by following that policy in a given environment.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度方法通过最大化一个目标函数，直接优化策略πθ的参数θ，目标函数表示在给定环境中遵循该策略所获得的期望奖励。
- en: 'The objective function is defined as:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 目标函数定义为：
- en: Where R(s, a) is the reward received for taking action a in state s, and the
    goal is to maximize this expected reward over time. The term dπ(s) represents
    the stationary distribution of states under policy π, indicating how frequently
    the agent visits each state when following policy π.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 其中R(s, a)表示在状态s下采取动作a所获得的奖励，目标是最大化随时间变化的期望奖励。术语dπ(s)表示在策略π下的状态的平稳分布，指示智能体在遵循策略π时访问每个状态的频率。
- en: 'The policy gradient theorem gives the gradient of the objective function, providing
    a way to update the policy parameters:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度定理给出了目标函数的梯度，提供了一种更新策略参数的方法：
- en: This equation shows how to adjust the policy parameters based on past experiences,
    which helps the agent learn more efficient behaviors over time.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 该方程展示了如何根据过去的经验调整策略参数，从而帮助智能体随着时间推移学习更高效的行为。
- en: 3\. Mathematical Enhancements of PPO
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3. PPO的数学增强
- en: PPO (Proximal Policy Optimization) introduces several important features to
    improve the stability and efficiency of reinforcement learning, particularly in
    large and complex environments. PPO was introduced by John Schulman et al. in
    2017 as an improvement over earlier policy optimization algorithms like Trust
    Region Policy Optimization (TRPO). The main motivation behind PPO was to strike
    a balance between sample efficiency, ease of implementation, and stability while
    avoiding the complexities of TRPO’s second-order optimization methods. While TRPO
    ensures stable policy updates by enforcing a strict constraint on the policy change,
    it relies on computationally expensive second-order derivatives and conjugate
    gradient methods, making it challenging to implement and scale. Moreover, the
    strict constraints in TRPO can sometimes overly limit policy updates, leading
    to slower convergence. PPO addresses these issues by using a simple clipped objective
    function that allows the policy to update in a stable and controlled manner, avoiding
    forgetting previous policies with each update, thus improving training efficiency
    and reducing the risk of policy collapse. This makes PPO a popular choice for
    a wide range of reinforcement learning tasks.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: PPO（Proximal Policy Optimization）引入了几个重要特性，旨在提高强化学习的稳定性和效率，特别是在大型复杂环境中。PPO由John
    Schulman等人于2017年提出，作为对早期策略优化算法（如信任域策略优化（TRPO））的改进。PPO的主要动机是寻求在样本效率、实现简便性和稳定性之间取得平衡，同时避免TRPO的二阶优化方法的复杂性。虽然TRPO通过对策略变化施加严格约束来确保策略更新的稳定性，但它依赖于计算开销较大的二阶导数和共轭梯度方法，使其实现和扩展具有挑战性。此外，TRPO中的严格约束有时会过度限制策略更新，导致收敛速度变慢。PPO通过使用简单的截断目标函数来解决这些问题，允许策略以稳定且可控的方式更新，避免每次更新时遗忘之前的策略，从而提高训练效率并减少策略崩溃的风险。这使得PPO成为广泛强化学习任务的热门选择。
- en: a. Probability Ratio
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: a. 概率比率
- en: 'One of the key components of PPO is the probability ratio, which compares the
    probability of taking an action in the current policy πθ to that of the old policy
    πθold:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: PPO的一个关键组成部分是概率比率，它比较当前策略πθ下采取某个动作的概率与旧策略πθold下采取同一动作的概率：
- en: This ratio provides a measure of how much the policy has changed between updates.
    By monitoring this ratio, PPO ensures that updates are not too drastic, which
    helps prevent instability in the learning process.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这个比率提供了一个度量，表示策略在更新之间的变化程度。通过监控这个比率，PPO确保更新不会过于剧烈，从而帮助防止学习过程中的不稳定性。
- en: b. Clipping Function
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: b. 截断函数
- en: Clipping is preferred over adjusting the learning rate in Proximal Policy Optimization
    (PPO) because it directly limits the magnitude of policy updates, preventing excessive
    changes that could destabilize the learning process. While the learning rate uniformly
    scales the size of updates, clipping ensures that updates stay close to the previous
    policy, thereby enhancing stability and reducing erratic behavior.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在Proximal Policy Optimization（PPO）中，截断比调整学习率更受偏爱，因为它直接限制了策略更新的幅度，防止了可能导致学习过程不稳定的过度变化。尽管学习率均匀地缩放更新的大小，截断确保更新保持接近先前的策略，从而增强稳定性并减少不稳定行为。
- en: The main advantage of clipping is that it allows for better control over updates,
    ensuring more stable progress. However, a potential drawback is that it may slow
    down learning by limiting the exploration of significantly different strategies.
    Nonetheless, clipping is favored in PPO and other algorithms when stability is
    essential.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 裁剪的主要优点在于它可以更好地控制更新，确保更稳定的进展。然而，潜在的缺点是，它可能会通过限制探索显著不同的策略而减缓学习过程。不过，当稳定性至关重要时，裁剪在
    PPO 和其他算法中是被青睐的。
- en: 'To avoid excessive changes to the policy, PPO uses a clipping function that
    modifies the objective function to restrict the size of policy updates. This is
    crucial because large updates in reinforcement learning can lead to erratic behavior.
    The modified objective with clipping is:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免对策略进行过多的修改，PPO 使用了一个裁剪函数，该函数通过修改目标函数来限制策略更新的幅度。这一点至关重要，因为强化学习中的大幅度更新可能导致不稳定的行为。裁剪后的目标函数是：
- en: The clipping function constrains the probability ratio within a specific range,
    preventing updates that would deviate too far from the previous policy. This helps
    avoid sudden, large changes that could destabilize the learning process.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 裁剪函数将概率比值限制在一个特定范围内，防止那些会与之前的策略偏离过远的更新。这有助于避免可能导致学习过程不稳定的突然而大的变化。
- en: c. Advantage Estimation with GAE
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: c. 使用 GAE 的优势估计
- en: 'In RL, estimating the advantage is important because it helps the agent determine
    which actions are better than others in each state. However, there is a trade-off:
    using only immediate rewards (or very short horizons) can introduce high variance
    in advantage estimates, while using longer horizons can introduce bias.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，估计优势是非常重要的，因为它有助于智能体判断在每个状态下哪些动作比其他动作更好。然而，这之间存在一个权衡：仅使用即时奖励（或非常短的视野）可能会引入较高的方差，而使用较长的视野则可能引入偏差。
- en: Generalized Advantage Estimation (GAE) strikes a balance between these two by
    using a weighted average of n-step returns and value estimates, making it less
    sensitive to noise and improving learning stability.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 广义优势估计（GAE）通过使用 n 步回报和价值估计的加权平均值，在这两者之间找到平衡，使得它对噪声不那么敏感，进而提高了学习的稳定性。
- en: Why use GAE?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么使用 GAE？
- en: '- **Stability**: GAE helps reduce variance by considering multiple steps so
    the agent does not react to noise in the rewards or temporary fluctuations in
    the environment.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '- **稳定性**：GAE 通过考虑多个步骤来减少方差，使智能体不会对奖励中的噪声或环境中的暂时波动做出反应。'
- en: '- **Efficiency**: GAE strikes a good balance between bias and variance, making
    learning more efficient by not requiring overly long sequences of rewards while
    still maintaining reliable estimates.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '- **效率**：GAE 在偏差和方差之间取得了良好的平衡，通过不需要过长的奖励序列，同时仍然保持可靠的估计，从而提高了学习效率。'
- en: '- **Better Action Comparison**: By considering not just the immediate reward
    but a broader horizon of rewards, the agent can better compare actions over time
    and make more informed decisions.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '- **更好的动作比较**：通过不仅仅考虑即时奖励，还考虑更广泛的奖励视角，智能体可以更好地比较随时间变化的动作，并做出更加明智的决策。'
- en: 'The advantage function At is used to assess how good an action was relative
    to the expected behavior under the current policy. To reduce variance and ensure
    more reliable estimates, PPO uses Generalized Advantage Estimation (GAE). This
    method smooths out the advantages over time while controlling for bias:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 优势函数 At 用于评估一个动作相对于当前策略下期望行为的优劣。为了减少方差并确保更可靠的估计，PPO 使用了广义优势估计（GAE）。这种方法在控制偏差的同时，平滑了优势的变化：
- en: This technique provides a more stable and accurate measure of the advantage,
    which improves the agent’s ability to make better decisions.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术提供了一个更加稳定和准确的优势衡量方式，从而提高了智能体做出更好决策的能力。
- en: d. Entropy to Encourage Exploration
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: d. 熵以鼓励探索
- en: 'PPO incorporates an entropy term in the objective function to encourage the
    agent to explore more of the environment rather than prematurely converging to
    a suboptimal solution. The entropy term increases the uncertainty in the agent’s
    decision-making, which prevents overfitting to a specific strategy:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: PPO 在目标函数中加入了一个熵项，以鼓励智能体更多地探索环境，而不是过早地收敛到次优解。熵项增加了智能体决策的随机性，这有助于防止过度拟合某一特定策略：
- en: Where H(πθ) represents the entropy of the policy. By adding this term, PPO ensures
    that the agent does not converge too quickly and is encouraged to continue exploring
    different actions and strategies, improving overall learning efficiency.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 其中H(πθ)表示策略的熵。通过加入这一项，PPO确保智能体不会过早收敛，并鼓励其继续探索不同的动作和策略，从而提高整体学习效率。
- en: Conclusion
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The mathematical underpinnings of PPO demonstrate how this algorithm achieves
    stable and efficient learning. With concepts like the probability ratio, clipping,
    advantage estimation, and entropy, PPO offers a powerful balance between exploration
    and exploitation. These features make it a robust choice for both researchers
    and practitioners working in complex environments. The simplicity of PPO, combined
    with its efficiency and effectiveness, makes it a popular and valuable algorithm
    in reinforcement learning.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: PPO的数学基础展示了该算法如何实现稳定和高效的学习。通过概率比率、裁剪、优势估计和熵等概念，PPO在探索和利用之间提供了强大的平衡。这些特性使得它成为研究人员和从业人员在复杂环境中工作的稳健选择。PPO的简单性结合其高效性和有效性，使其成为强化学习中受欢迎且有价值的算法。
- en: Reference
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[https://books.google.fr/books?hl=fr&lr=&id=sWV0DwAAQBAJ&oi=fnd&pg=PR7&dq=reinforcement+learning+an+introduction&ots=1-9av2aqTb&sig=qMSnFC56yqPQugvqS3_uwCN78z0#v=onepage&q=reinforcement%20learning%20an%20introduction&f=false](https://books.google.fr/books?hl=fr&lr=&id=sWV0DwAAQBAJ&oi=fnd&pg=PR7&dq=reinforcement+learning+introduction&ots=1-9av2asXb&sig=ZDKNYXZc8gMIzEqeonvMxcN8skE#v=onepage&q=reinforcement%20learning%20introduction&f=false)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://books.google.fr/books?hl=fr&lr=&id=sWV0DwAAQBAJ&oi=fnd&pg=PR7&dq=reinforcement+learning+an+introduction&ots=1-9av2aqTb&sig=qMSnFC56yqPQugvqS3_uwCN78z0#v=onepage&q=reinforcement%20learning%20an%20introduction&f=false](https://books.google.fr/books?hl=fr&lr=&id=sWV0DwAAQBAJ&oi=fnd&pg=PR7&dq=reinforcement+learning+introduction&ots=1-9av2asXb&sig=ZDKNYXZc8gMIzEqeonvMxcN8skE#v=onepage&q=reinforcement%20learning%20introduction&f=false)'
- en: '[https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf](https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf](https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf)'
- en: '*This article was partially translated from French using* [*DeepL*](https://www.deepl.com/fr/translator)*.*'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*本文部分内容通过* [*DeepL*](https://www.deepl.com/fr/translator)*翻译自法语。*'
