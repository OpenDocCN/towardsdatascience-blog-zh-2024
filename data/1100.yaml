- en: A Practical Guide to Proximal Policy Optimization in JAX
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: JAX 中近端策略优化（PPO）的实用指南
- en: 原文：[https://towardsdatascience.com/breaking-down-state-of-the-art-ppo-implementations-in-jax-6f102c06c149?source=collection_archive---------7-----------------------#2024-05-01](https://towardsdatascience.com/breaking-down-state-of-the-art-ppo-implementations-in-jax-6f102c06c149?source=collection_archive---------7-----------------------#2024-05-01)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/breaking-down-state-of-the-art-ppo-implementations-in-jax-6f102c06c149?source=collection_archive---------7-----------------------#2024-05-01](https://towardsdatascience.com/breaking-down-state-of-the-art-ppo-implementations-in-jax-6f102c06c149?source=collection_archive---------7-----------------------#2024-05-01)
- en: All the tricks and details you wish you knew about PPO
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 所有你希望了解的关于 PPO 的技巧和细节
- en: '[](https://medium.com/@ryanpegoud?source=post_page---byline--6f102c06c149--------------------------------)[![Ryan
    Pégoud](../Images/9314b76c2be56bda8b73b4badf9e3e4d.png)](https://medium.com/@ryanpegoud?source=post_page---byline--6f102c06c149--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6f102c06c149--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6f102c06c149--------------------------------)
    [Ryan Pégoud](https://medium.com/@ryanpegoud?source=post_page---byline--6f102c06c149--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ryanpegoud?source=post_page---byline--6f102c06c149--------------------------------)[![Ryan
    Pégoud](../Images/9314b76c2be56bda8b73b4badf9e3e4d.png)](https://medium.com/@ryanpegoud?source=post_page---byline--6f102c06c149--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6f102c06c149--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6f102c06c149--------------------------------)
    [Ryan Pégoud](https://medium.com/@ryanpegoud?source=post_page---byline--6f102c06c149--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6f102c06c149--------------------------------)
    ·9 min read·May 1, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6f102c06c149--------------------------------)
    ·9 分钟阅读·2024年5月1日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/9706450ef22f9e46c08b73bbee0735d6.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9706450ef22f9e46c08b73bbee0735d6.png)'
- en: Photo by [Lorenzo Herrera](https://unsplash.com/@lorenzoherrera?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [Lorenzo Herrera](https://unsplash.com/@lorenzoherrera?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) 上
- en: 'Since its publication in a [2017 paper by OpenAI](https://arxiv.org/pdf/1707.06347.pdf),
    Proximal Policy Optimization (PPO) is widely regarded as one of the state-of-the-art
    algorithms in Reinforcement Learning. Indeed, PPO has demonstrated remarkable
    performances across various tasks, from [attaining superhuman performances in
    Dota 2](https://openai.com/research/openai-five) teams to solving a [Rubik’s cube
    with a single robotic hand](https://openai.com/research/solving-rubiks-cube) while
    maintaining three main advantages: simplicity, stability, and sample efficiency.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 自从 OpenAI 在 [2017 年的论文](https://arxiv.org/pdf/1707.06347.pdf) 中发布以来，近端策略优化（PPO）被广泛认为是强化学习领域最先进的算法之一。实际上，PPO
    在各种任务中都表现出了显著的性能，从 [在 Dota 2 中取得超人类表现](https://openai.com/research/openai-five)
    的团队到使用单个机器人手臂 [解 Rubik’s Cube](https://openai.com/research/solving-rubiks-cube)，同时保持三个主要优势：简洁性、稳定性和样本效率。
- en: However, implementing RL algorithms from scratch is notoriously difficult and
    error-prone, given the numerous error sources and implementation details to be
    aware of.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从零开始实现强化学习（RL）算法非常困难且容易出错，因为有许多错误源和实现细节需要注意。
- en: In this article, we’ll focus on breaking down the clever tricks and programming
    concepts used in a popular implementation of PPO in JAX. Specifically, we’ll focus
    on the [implementation featured in the PureJaxRL library](https://github.com/luchris429/purejaxrl/blob/main/purejaxrl/ppo.py),
    developed by [Chris Lu](https://chrislu.page).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将重点分析在 JAX 中流行的 PPO 实现中使用的巧妙技巧和编程概念。具体来说，我们将聚焦于 [PureJaxRL 库中的实现](https://github.com/luchris429/purejaxrl/blob/main/purejaxrl/ppo.py)，该库由
    [Chris Lu](https://chrislu.page) 开发。
- en: '*Disclaimer: Rather than diving too deep into theory, this article covers the
    practical implementation details and (numerous) tricks used in popular versions
    of PPO. Should you require any reminders about PPO’s theory, please refer to the
    “****references****” section at the end of this article. Additionally, all the
    code (minus the added comments) is copied directly from PureJaxRL for pedagogical
    purposes.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*免责声明：本文并未深入探讨理论，而是聚焦于在PPO的流行实现版本中所使用的实际实现细节和（众多）技巧。如果你需要回顾PPO的理论，请参考本文末尾的“****参考文献****”部分。此外，所有代码（不包括新增的注释）均直接复制自PureJaxRL，旨在教学目的。*'
- en: '[](https://github.com/luchris429/purejaxrl/tree/main?source=post_page-----6f102c06c149--------------------------------)
    [## UGitHub - luchris429/purejaxrl: Really Fast End-to-End Jax RL Implementations'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/luchris429/purejaxrl/tree/main?source=post_page-----6f102c06c149--------------------------------)
    [## UGitHub - luchris429/purejaxrl: 超快的端到端 Jax 强化学习实现'
- en: Really Fast End-to-End Jax RL Implementations. Contribute to luchris429/purejaxrl
    development by creating an account on…
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 超快的端到端 Jax 强化学习实现。通过创建一个账户，贡献代码至luchris429/purejaxrl开发项目...
- en: github.com](https://github.com/luchris429/purejaxrl/tree/main?source=post_page-----6f102c06c149--------------------------------)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/luchris429/purejaxrl/tree/main?source=post_page-----6f102c06c149--------------------------------)
- en: '**Actor-Critic Architectures**'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**演员-评论家架构**'
- en: 'Proximal Policy Optimization is categorized within the policy gradient family
    of algorithms, a subset of which includes actor-critic methods. The designation
    ‘actor-critic’ reflects the dual components of the model:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 近端策略优化（Proximal Policy Optimization，PPO）被归类为策略梯度算法家族，其中包括演员-评论家方法。‘演员-评论家’这一名称反映了模型的双重组件：
- en: The **actor network** creates a **distribution over actions** given the current
    state of the environment and returns an action sampled from this distribution.
    Here, the actor network comprises three dense layers separated by two activation
    layers (either ReLU or hyperbolic tangeant) and a final categorical layer applying
    the **softmax** function to the computed distribution.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**演员网络**根据环境当前状态生成一个**行动分布**，并从该分布中采样一个行动。这里，演员网络包括三层全连接层，这些层之间由两层激活层（可以是ReLU或双曲正切）隔开，并且最终有一个应用**softmax**函数的分类层来计算分布。'
- en: The **critic network** **estimates the value function of the current state**,
    in other words, how good a particular action is at a given time. Its architecture
    is almost identical to the actor network, except for the final softmax layer.
    Indeed, the critic network doesn’t apply any activation function to the final
    dense layer outputs as it performs a regression task.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评论家网络** **估算当前状态的价值函数**，换句话说，它评估在某一时刻特定行动的好坏。其架构几乎与演员网络相同，唯一的区别是最终的softmax层。实际上，评论家网络在处理回归任务时，并不会对最后一层全连接层的输出应用任何激活函数。'
- en: '![](../Images/1e67cc67befceb9cb6015af67bab3bf3.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e67cc67befceb9cb6015af67bab3bf3.png)'
- en: Actor-critic architecture, as defined in PureJaxRL (illustration made by the
    author)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 演员-评论家架构，如PureJaxRL中定义的（插图由作者制作）
- en: Additionally, this implementation pays particular attention to **weight initialization**
    in dense layers. Indeed, all dense layers are initialized by **orthogonal matrices**
    with specific coefficients. This initialization strategy has been shown to **preserve
    the gradient norms** (i.e. scale) during forward passes and backpropagation, leading
    to **smoother convergence** and limiting the risks of vanishing or exploding gradients[1].
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，该实现特别注意**权重初始化**在全连接层中的应用。实际上，所有全连接层都通过**正交矩阵**与特定系数进行初始化。该初始化策略已被证明在前向传播和反向传播过程中能够**保持梯度范数**（即尺度），从而实现**更平滑的收敛**，并限制梯度消失或爆炸的风险[1]。
- en: 'Orthogonal initialization is used in conjunction with specific scaling coefficients:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 正交初始化与特定的缩放系数一起使用：
- en: '**Square root of 2**: Used for the first two dense layers of both networks,
    this factor aims to **compensate for the variance reduction** induced by ReLU
    activations (as inputs with negative values are set to 0). For the tanh activation,
    the Xavier initialization is a popular alternative[2].'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2的平方根**：该因子用于两个网络的前两层全连接层，旨在**补偿ReLU激活导致的方差减少**（因为负值输入会被设为0）。对于tanh激活函数，Xavier初始化是一个流行的替代方案[2]。'
- en: '**0.01:** Used in the last dense layer of the actor network, this factor helps
    to **minimize the initial differences in logit values** before applying the softmax
    function. This will reduce the difference in action probabilities and thus **encourage
    early exploration**.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**0.01：** 用于演员网络的最后一层密集层，该因子有助于**最小化logit值的初始差异**，在应用softmax函数之前。这将减少动作概率的差异，从而**鼓励早期探索**。'
- en: '**1:** As the critic network is performing a regression task, we do not scale
    the initial weights.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1：** 由于评论员网络执行的是回归任务，我们不会缩放初始权重。'
- en: 'Actor critic network (source: PureJaxRL, Chris Lu)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 演员-评论员网络（来源：PureJaxRL, Chris Lu）
- en: Training Loop
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练循环
- en: 'The training loop is divided into 3 main blocks that share similar coding patterns,
    taking advantage of Jax’s functionalities:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环分为3个主要部分，这些部分共享类似的编码模式，充分利用JAX的功能：
- en: '**Trajectory collection:** First, we’ll interact with the environment for a
    set number of steps and collect observations and rewards.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**轨迹收集：** 首先，我们将与环境交互若干步骤，并收集观测值和奖励。'
- en: '**Generalized Advantage Estimation (GAE):** Then, we’ll approximate the expected
    return for each trajectory by computing the generalized advantage estimation.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**广义优势估计（GAE）：** 然后，我们通过计算广义优势估计来近似每个轨迹的期望回报。'
- en: '**Update step:** Finally, we’ll compute the gradient of the loss and update
    the network parameters via gradient descent.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**更新步骤：** 最后，我们将计算损失的梯度，并通过梯度下降更新网络参数。'
- en: 'Before going through each block in detail, here’s a quick reminder about the
    `jax.lax.scan`function that will show up multiple times throughout the code:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在详细介绍每个模块之前，这里简要提醒一下`jax.lax.scan`函数，它将在代码中多次出现：
- en: Jax.lax.scan
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Jax.lax.scan
- en: A common programming pattern in JAX consists of defining a function that acts
    on a single sample and using `jax.lax.scan`to **iteratively apply it to elements
    of a sequence** or an array, while carrying along some state.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: JAX中常见的编程模式是定义一个作用于单个样本的函数，并使用`jax.lax.scan`来**迭代地将其应用于序列或数组的元素**，同时携带某些状态。
- en: For instance, we’ll apply it to the `step` function to step our environment
    N consecutive times while carrying the new state of the environment through each
    iteration.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们将其应用于`step`函数，以在连续N次步骤中推进环境，同时在每次迭代中传递环境的新状态。
- en: 'In pure Python, we could proceed as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在纯Python中，我们可以按以下方式进行：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'However, we avoid writing such loops in JAX for performance reasons (as pure
    Python loops are incompatible with JIT compilation). The alternative is `jax.lax.scan`which
    is equivalent to:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了性能考虑（因为纯Python循环与JIT编译不兼容），我们避免在JAX中编写此类循环。替代方法是`jax.lax.scan`，其等效于：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Using `jax.lax.scan` is more efficient than a Python loop because it allows
    the transformation to be optimized and executed as a single compiled operation
    rather than interpreting each loop iteration at runtime.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`jax.lax.scan`比使用Python循环更高效，因为它允许对转换进行优化，并作为单一的编译操作执行，而不是在运行时解释每个循环迭代。
- en: 'We can see that the `scan` function takes multiple arguments:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，`scan`函数接受多个参数：
- en: '**f:** A function that is applied at each step. It takes the current state
    and an element of `xs` (or a placeholder if `xs` is `None`) and returns the updated
    state and an output.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**f：** 在每个步骤应用的函数。它接受当前状态和`xs`的一个元素（如果`xs`为`None`，则使用占位符），并返回更新后的状态和输出。'
- en: '**init:** The initial state that `f` will use in its first invocation.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**init：** `f`在第一次调用时使用的初始状态。'
- en: '**xs:** A sequence of inputs that are iteratively processed by `f`. If `xs`
    is `None`, the function simulates a loop with `length` iterations using `None`
    as the input for each iteration.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**xs：** 一个输入序列，它将被`f`逐步处理。如果`xs`为`None`，则该函数模拟一个具有`length`次迭代的循环，并将每次迭代的输入设置为`None`。'
- en: '**length:** Specifies the number of iterations if `xs` is `None`, ensuring
    that the function can still operate without explicit inputs.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**length：** 如果`xs`为`None`，指定迭代的次数，确保函数在没有明确输入的情况下仍能操作。'
- en: 'Additionally, `scan` returns:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`scan`返回：
- en: '**carry:** The final state after all iterations.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**carry：** 所有迭代后的最终状态。'
- en: '**ys:** An array of outputs corresponding to each step’s application of `f`,
    stacked for easy analysis or further processing.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ys：** 对应于每个步骤应用`f`的输出数组，堆叠以便于分析或进一步处理。'
- en: Finally, `scan` can be used in combination with `vmap` to scan a function over
    multiple dimensions in parallel. As we’ll see in the next section, this allows
    us to interact with several environments in parallel to collect trajectories rapidly.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`scan`可以与`vmap`结合使用，在多个维度上并行扫描一个函数。正如我们在下一节中将看到的，这使得我们能够并行与多个环境交互，从而快速收集轨迹。
- en: '![](../Images/123e5e98f340935db435e9aab42629a7.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/123e5e98f340935db435e9aab42629a7.png)'
- en: Illustration of vmap, scan, and scan + vmap in the context of the step function
    (made by the author)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在步长函数上下文中，展示了vmap、scan和scan + vmap的示意图（作者制作）
- en: 1\. Trajectory Collection
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 轨迹收集
- en: 'As mentioned in the previous section, the trajectory collection block consists
    of a `step` function scanned across N iterations. This `step` function successively:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所述，轨迹收集块由`step`函数跨N次迭代扫描组成。这个`step`函数依次执行：
- en: Selects an action using the actor network
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用演员网络选择一个动作
- en: Steps the environment
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行环境步长
- en: Stores transition data in a `transition` tuple
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将转移数据存储在`transition`元组中
- en: Stores the model parameters, the environment state, the current observation,
    and rng keys in a `runner_state` tuple
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型参数、环境状态、当前观察值和随机数生成器键存储在`runner_state`元组中
- en: Returns `runner_state` and `transition`
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回`runner_state`和`transition`
- en: Scanning this function returns the latest `runner_state` and `traj_batch`, an
    array of `transition` tuples. In practice, transitions are collected from multiple
    environments in parallel for efficiency as indicated by the use of `jax.vmap(env.step,
    …)`(for more details about vectorized environments and `vmap`, refer to my [previous
    article](https://medium.com/towards-data-science/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5)).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 扫描这个函数将返回最新的`runner_state`和`traj_batch`，后者是一个包含`transition`元组的数组。实际上，为了提高效率，转移数据是从多个环境中并行收集的，如使用`jax.vmap(env.step,
    …)`所示（有关向量化环境和`vmap`的更多详细信息，请参阅我的[上一篇文章](https://medium.com/towards-data-science/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5)）。
- en: 'env step function (source: PureJaxRL, Chris Lu)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: env步长函数（来源：PureJaxRL，Chris Lu）
- en: 2\. Generalized Advantage Estimation
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 广义优势估计
- en: 'After collecting trajectories, we need to compute the **advantage function,**
    a crucial component of PPO’s loss function. The advantage function measures how
    much better a specific action is compared to the average action in a given state:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 收集完轨迹后，我们需要计算**优势函数**，这是PPO损失函数的一个关键组成部分。优势函数衡量一个特定动作在给定状态下，相较于平均动作的表现如何：
- en: '![](../Images/9025657609723998de22ebbc42479942.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9025657609723998de22ebbc42479942.png)'
- en: Where **Gt** is the return at time ***t***and **V(St) is** the value of state
    ***s***at time ***t****.*
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其中**Gt**是时间***t***时刻的回报，**V(St)**是时间***t***时刻状态***s***的值。
- en: 'As the return is generally unknown, we have to approximate the advantage function.
    A popular solution is **generalized advantage estimation**[3], defined as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 由于回报通常是未知的，我们必须对优势函数进行近似。一个流行的解决方案是**广义优势估计**[3]，其定义如下：
- en: '![](../Images/0ccf1cf4804dffef16e94feee6c2eb2a.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ccf1cf4804dffef16e94feee6c2eb2a.png)'
- en: 'With **γ** the discount factor, **λ** a parameter that controls the trade-off
    between bias and variance in the estimate, and ***δt***the temporal difference
    error at time ***t***:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 其中**γ**是折扣因子，**λ**是控制偏差与方差权衡的参数，***δt***是时间***t***时刻的时序差异误差：
- en: '![](../Images/298fa9ce55afb829287e91b9688fa335.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/298fa9ce55afb829287e91b9688fa335.png)'
- en: 'As we can see, the value of the GAE at time *t* depends on the GAE at future
    timesteps. Therefore, we compute it backward, starting from the end of a trajectory.
    For example, for a trajectory of 3 transitions, we would have:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，GAE在时间*t*的值依赖于未来时间步的GAE。因此，我们从轨迹的末端开始反向计算。例如，对于一个包含3个转移的轨迹，我们将得到：
- en: '![](../Images/cbc3f0a15854dd74ba9a902e27bd939d.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cbc3f0a15854dd74ba9a902e27bd939d.png)'
- en: 'Which is equivalent to the following recursive form:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这等价于以下递归形式：
- en: '![](../Images/8fbb9ca9d51bbddd821b3dc3747ee3bb.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8fbb9ca9d51bbddd821b3dc3747ee3bb.png)'
- en: Once again, we use `jax.lax.scan` on the trajectory batch (this time in reverse
    order) to iteratively compute the GAE.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们对轨迹批次使用`jax.lax.scan`（这次是逆序）来迭代计算GAE。
- en: 'generalized advantage estimation (source: PureJaxRL, Chris Lu)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 广义优势估计（来源：PureJaxRL，Chris Lu）
- en: Note that the function returns `advantages + traj_batch.value` as a second output,
    which is equivalent to the return according to the first equation of this section.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，该函数返回`advantages + traj_batch.value`作为第二个输出，这相当于本节第一个方程中的回报。
- en: 3\. Update step
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3. 更新步骤
- en: 'The final block of the training loop defines the loss function, computes its
    gradient, and performs gradient descent on minibatches. Similarly to previous
    sections, the update step is an arrangement of several functions in a hierarchical
    order:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环的最后一部分定义了损失函数，计算其梯度，并在 minibatches 上执行梯度下降。与前面几节类似，更新步骤是按层次顺序排列的多个函数的组合：
- en: '[PRE2]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let’s break them down one by one, starting from the innermost function of the
    update step.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一拆解它们，从更新步骤中最内层的函数开始。
- en: 3.1 Loss function
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 损失函数
- en: 'This function aims to define and compute the PPO loss, originally defined as:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数旨在定义和计算 PPO 损失，最初定义为：
- en: '![](../Images/c0b4c98a735ce19abbf8e5090017b346.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c0b4c98a735ce19abbf8e5090017b346.png)'
- en: 'Where:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '![](../Images/11464fd7edf1eeda50b976b7ab409280.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11464fd7edf1eeda50b976b7ab409280.png)'
- en: 'However, the PureJaxRL implementation features some tricks and differences
    compared to the original PPO paper[4]:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，PureJaxRL 的实现与原始 PPO 论文[4]相比，具有一些技巧和差异：
- en: The paper defines the PPO loss in the context of gradient ascent whereas the
    implementation performs gradient descent. Therefore, the sign of each loss component
    is reversed.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 论文中定义的 PPO 损失是基于梯度上升的，而实际实现采用了梯度下降。因此，每个损失组件的符号被反转。
- en: 'The value function term is modified to include an additional clipped term.
    This could be seen as a way to make the value function updates more conservative
    (as for the clipped surrogate objective):'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值函数项被修改，包含了一个额外的截断项。这可以看作是使值函数更新更为保守的一种方式（就像截断的替代目标一样）：
- en: '![](../Images/add641cf22538f261da9af6a70c8c46e.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/add641cf22538f261da9af6a70c8c46e.png)'
- en: The GAE is standardized.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GAE 被标准化。
- en: 'Here’s the complete loss function:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这是完整的损失函数：
- en: 'PPO loss function (source: PureJaxRL, Chris Lu)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: PPO 损失函数（来源：PureJaxRL, Chris Lu）
- en: 3.2 Update Minibatch
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 更新 Minibatch
- en: The `update_minibatch` function is essentially a wrapper around `loss_fn` used
    to compute its gradient over the trajectory batch and update the model parameters
    stored in `train_state`.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`update_minibatch` 函数本质上是一个包装器，用于计算在轨迹批次上对 `loss_fn` 的梯度，并更新存储在 `train_state`
    中的模型参数。'
- en: 'update minibatch (source: PureJaxRL, Chris Lu)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 更新 minibatch（来源：PureJaxRL, Chris Lu）
- en: 3.3 Update Epoch
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 更新 Epoch
- en: Finally, `update_epoch` wraps `update_minibatch` and applies it on minibatches.
    Once again, `jax.lax.scan` is used to apply the update function on all minibatches
    iteratively.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`update_epoch` 封装了 `update_minibatch` 并在 minibatch 上应用它。同样，`jax.lax.scan`
    被用来迭代地对所有 minibatch 应用更新函数。
- en: 'update epoch (source: PureJaxRL, Chris Lu)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 更新 epoch（来源：PureJaxRL, Chris Lu）
- en: Conclusion
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: From there, we can wrap all of the previous functions in an `update_step` function
    and use `scan` one last time for N steps to complete the training loop.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，我们可以将所有先前的函数封装在一个 `update_step` 函数中，并使用 `scan` 最后一次迭代 N 步以完成训练循环。
- en: 'A global view of the training loop would look like this:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环的全局视图大致如下：
- en: 'Summary of the training script (source: PureJaxRL, Chris Lu)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 训练脚本总结（来源：PureJaxRL, Chris Lu）
- en: We can now run a fully compiled training loop using `jax.jit(train(rng))` or
    even train multiple agents in parallel using `jax.vmap(train(rng))`.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用 `jax.jit(train(rng))` 运行一个完全编译的训练循环，或者使用 `jax.vmap(train(rng))` 并行训练多个代理。
- en: There we have it! We covered the essential building blocks of the PPO training
    loop as well as common programming patterns in JAX.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止！我们涵盖了 PPO 训练循环的基本构建块以及 JAX 中常见的编程模式。
- en: To go further, I highly recommend reading the [full training script](https://github.com/luchris429/purejaxrl/blob/main/purejaxrl/ppo.py)
    in detail and running example notebooks on the PureJaxRL repository.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 若想深入了解，强烈建议详细阅读 [完整训练脚本](https://github.com/luchris429/purejaxrl/blob/main/purejaxrl/ppo.py)，并在
    PureJaxRL 仓库上运行示例笔记本。
- en: '[](https://github.com/luchris429/purejaxrl?source=post_page-----6f102c06c149--------------------------------)
    [## GitHub - luchris429/purejaxrl: Really Fast End-to-End Jax RL Implementations'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/luchris429/purejaxrl?source=post_page-----6f102c06c149--------------------------------)
    [## GitHub - luchris429/purejaxrl: 真正快速的端到端 Jax 强化学习实现'
- en: Really Fast End-to-End Jax RL Implementations. Contribute to luchris429/purejaxrl
    development by creating an account on…
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 真正快速的端到端 Jax 强化学习实现。通过创建一个账户来贡献于 luchris429/purejaxrl 的开发…
- en: github.com](https://github.com/luchris429/purejaxrl?source=post_page-----6f102c06c149--------------------------------)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/luchris429/purejaxrl?source=post_page-----6f102c06c149--------------------------------)
- en: Thank you very much for your support, until next time 👋
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢您的支持，期待下次再见 👋
- en: 'References:'
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献：
- en: '[Full training script](https://github.com/luchris429/purejaxrl/blob/main/purejaxrl/ppo.py),
    PureJaxRL, Chris Lu, 2023'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[完整训练脚本](https://github.com/luchris429/purejaxrl/blob/main/purejaxrl/ppo.py)，PureJaxRL，Chris
    Lu，2023'
- en: '[1] [***Explaining and illustrating orthogonal initialization for recurrent
    neural networks***](https://smerity.com/articles/2016/orthogonal_init.html), Smerity,
    2016'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [***解释和说明递归神经网络的正交初始化***](https://smerity.com/articles/2016/orthogonal_init.html)，Smerity，2016'
- en: '[2] [***Initializing neural networks***](https://www.deeplearning.ai/ai-notes/initialization/index.html),
    DeepLearning.ai'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [***初始化神经网络***](https://www.deeplearning.ai/ai-notes/initialization/index.html)，DeepLearning.ai'
- en: '[3] [***Generalized Advantage Estimation in Reinforcement Learning***](/generalized-advantage-estimation-in-reinforcement-learning-bf4a957f7975),
    Siwei Causevic, Towards Data Science, 2023'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [***强化学习中的广义优势估计***](/generalized-advantage-estimation-in-reinforcement-learning-bf4a957f7975)，Siwei
    Causevic，Towards Data Science，2023'
- en: '[4] [***Proximal Policy Optimization Algorithms***](https://arxiv.org/pdf/1707.06347),
    Schulman et Al., OpenAI, 2017'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [***近端策略优化算法***](https://arxiv.org/pdf/1707.06347)，Schulman 等，OpenAI，2017'
