- en: Do Machine Learning Models Store Protected Content?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习模型是否存储受保护的内容？
- en: 原文：[https://towardsdatascience.com/do-machine-learning-models-store-protected-content-abec357c6b70?source=collection_archive---------6-----------------------#2024-05-06](https://towardsdatascience.com/do-machine-learning-models-store-protected-content-abec357c6b70?source=collection_archive---------6-----------------------#2024-05-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/do-machine-learning-models-store-protected-content-abec357c6b70?source=collection_archive---------6-----------------------#2024-05-06](https://towardsdatascience.com/do-machine-learning-models-store-protected-content-abec357c6b70?source=collection_archive---------6-----------------------#2024-05-06)
- en: ~A proof of concept~
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ~概念验证~
- en: '[](https://medium.com/@nathanReitinger?source=post_page---byline--abec357c6b70--------------------------------)[![Nathan
    Reitinger](../Images/a4f92fd800035099e00b92ea9006181d.png)](https://medium.com/@nathanReitinger?source=post_page---byline--abec357c6b70--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--abec357c6b70--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--abec357c6b70--------------------------------)
    [Nathan Reitinger](https://medium.com/@nathanReitinger?source=post_page---byline--abec357c6b70--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@nathanReitinger?source=post_page---byline--abec357c6b70--------------------------------)[![Nathan
    Reitinger](../Images/a4f92fd800035099e00b92ea9006181d.png)](https://medium.com/@nathanReitinger?source=post_page---byline--abec357c6b70--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--abec357c6b70--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--abec357c6b70--------------------------------)
    [Nathan Reitinger](https://medium.com/@nathanReitinger?source=post_page---byline--abec357c6b70--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--abec357c6b70--------------------------------)
    ·5 min read·May 6, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--abec357c6b70--------------------------------)
    ·5分钟阅读·2024年5月6日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/aa8bacd956c21071a77783c79542970b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa8bacd956c21071a77783c79542970b.png)'
- en: 'From chatGPT to Stable Diffusion, Artificial Intelligence (AI) is having a
    summer the likes of which rival only the AI heydays of the [1970s](https://clivethompson.medium.com/the-risk-of-a-new-ai-winter-332ffb4767f0).
    This jubilation, however, has not been met without resistance. From [Hollywood](https://www.newscientist.com/article/2402251-hollywood-strike-ends-but-actors-battle-against-ai-may-not-be-over/#:~:text=The%20use%20of%20AI%20to,companies%20use%20performers''%20digital%20twins.)
    to the [Louvre](https://nftevening.com/claire-silver-brings-artificial-intelligence-nft-art-to-the-louvre/),
    AI seems to have awoken a sleeping giant — a giant keen to protect a world that
    once seemed exclusively human: creativity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 从chatGPT到Stable Diffusion，人工智能（AI）正经历一个类似于[1970年代](https://clivethompson.medium.com/the-risk-of-a-new-ai-winter-332ffb4767f0)的夏天，AI的盛况可与那个时代的辉煌相提并论。然而，这一欢庆并非没有遭遇反对。从[好莱坞](https://www.newscientist.com/article/2402251-hollywood-strike-ends-but-actors-battle-against-ai-may-not-be-over/#:~:text=The%20use%20of%20AI%20to,companies%20use%20performers'%20digital%20twins.)到[卢浮宫](https://nftevening.com/claire-silver-brings-artificial-intelligence-nft-art-to-the-louvre/)，人工智能似乎唤醒了一个沉睡的巨人——一个渴望保护曾经看似专属于人类的世界：创造力。
- en: 'For those desiring to protect creativity, AI appears to have an Achilles heel:
    training data. Indeed, all of the [best models today](https://arxiv.org/pdf/2310.19909)
    necessitate a high-quality, world-encompassing data diet — but what does that
    mean?'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些渴望保护创造力的人来说，人工智能似乎有一个致命弱点：训练数据。事实上，所有[最佳模型](https://arxiv.org/pdf/2310.19909)都需要一个高质量、涵盖全球的数据源——但这意味着什么呢？
- en: '*First*, high-quality means human created. Although [not-human-created](https://law.stanford.edu/wp-content/uploads/2019/01/Bellovin_20190129.pdf)
    data has made many strides since the idea of a computer playing itself was popularized
    by [War Games](https://www.youtube.com/watch?v=YIh41wZEd5c), computer science
    literature has shown that model quality degrades over time if humanness is completely
    taken out of the loop (i.e., model rot or [model collapse](https://ui.adsabs.harvard.edu/link_gateway/2024arXiv240207712D/doi:10.48550/arXiv.2402.07712)).
    In simple terms: human data is the lifeblood of these models.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*首先*，高质量意味着人为创造的。尽管[非人工创造的](https://law.stanford.edu/wp-content/uploads/2019/01/Bellovin_20190129.pdf)数据自从计算机自我对弈的概念被[战争游戏](https://www.youtube.com/watch?v=YIh41wZEd5c)推广以来取得了许多进展，计算机科学文献却表明，如果完全去除人的因素（即模型腐化或[模型崩塌](https://ui.adsabs.harvard.edu/link_gateway/2024arXiv240207712D/doi:10.48550/arXiv.2402.07712)），模型质量随着时间的推移会下降。简单来说：人类数据是这些模型的命脉。'
- en: '*Second*, world-encompassing means world-encompassing. If you put it online,
    you should assume the model has used it in training: that Myspace post you were
    hoping only you and Tom remembered (ingested), that [picture-encased-memory](https://www.cnn.com/2022/05/24/tech/cher-scarlett-facial-recognition-trauma/index.html)
    you gladly forgot about until PimEyes forced you to remember it (ingested), and
    those late-night Reddit tirades you hoped were just a dream (ingested).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*第二*，全球性意味着全球性。如果你把它放到网上，你应该假设模型已经在训练中使用了它：那个你原本希望只有你和Tom记得的Myspace帖子（已被吸收），那个你高兴地忘记的[图片封存记忆](https://www.cnn.com/2022/05/24/tech/cher-scarlett-facial-recognition-trauma/index.html)，直到PimEyes迫使你重新记起它（已被吸收），以及那些你希望只是梦境的深夜Reddit争论（已被吸收）。'
- en: Models like LLaMa, BERT, Stable Diffusion, Claude, and chatGPT were all trained
    on massive amounts of human-created data. And what’s unique about some, many,
    or most human-created expressions — especially those that happen to be fixed in
    a tangible medium a computer can access and learn from — is that they qualify
    for copyright protection.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 像LLaMa、BERT、Stable Diffusion、Claude和chatGPT这样的模型都是在大量由人类创作的数据上进行训练的。而一些、许多或大多数人类创作的表达方式——尤其是那些恰好固定在计算机可以访问并学习的有形介质上的表达——具有版权保护的资格。
- en: '![](../Images/4d7d5938bdf54b319faf9fd7ff8b4290.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d7d5938bdf54b319faf9fd7ff8b4290.png)'
- en: Anderson v. Stability AI; Concord Music Group, Inc. v. Anthropic PBC; Doe v.
    GitHub, Inc.; Getty Images v. Stability AI; {Tremblay, Silverman, Chabon} v. OpenAI;
    New York Times v. Microsoft
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Anderson v. Stability AI；Concord Music Group, Inc. v. Anthropic PBC；Doe v. GitHub,
    Inc.；Getty Images v. Stability AI；{Tremblay, Silverman, Chabon} v. OpenAI；纽约时报诉微软
- en: Fortuitous as it may be, the data these models cannot survive without is the
    same data most protected by copyright. And this gives rise to the titanic copyright
    battles we are seeing today.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可能是偶然的，这些模型无法生存的数据正是大多数受到版权保护的数据。这也催生了我们今天看到的巨大的版权斗争。
- en: 'Of the many questions arising in these lawsuits, one of the most pressing is
    whether models themselves store protected content. This question seems rather
    obvious, because how can we say that models — merely collections of numbers (i.e.,
    weights) with an architecture — “store” anything? As Professor Murray states:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些诉讼中产生的许多问题中，最紧迫的一个问题是模型本身是否存储受保护的内容。这个问题似乎相当明显，因为我们怎么能说模型——仅仅是由数字（即权重）和架构组成的集合——“存储”了什么？正如Murray教授所说：
- en: Many of the participants in the current debate on visual generative AI systems
    have latched onto the idea that generative AI systems have been trained on datasets
    and foundation models that contained actual copyrighted image files, .jpgs, .gifs,
    .png files and the like, scraped from the internet, that somehow the dataset or
    foundation model must have made and stored copies of these works, and somehow
    the generative AI system further selected and copied individual images out of
    that dataset, and somehow the system copied and incorporated significant copyrightable
    parts of individual images into the final generated images that are offered to
    the end-user. This is magical thinking.
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当前关于视觉生成型AI系统的辩论中的许多参与者抓住了这样一个观点：生成型AI系统已在包含实际版权保护的图像文件（如.jpg、.gif、.png等）数据集和基础模型上进行训练，这些文件是从互联网上抓取的，数据集或基础模型一定已经制作并存储了这些作品的副本，并且生成型AI系统以某种方式进一步选择并复制了这些数据集中的个别图像，并以某种方式将这些图像的重大可版权部分复制并纳入到最终生成的图像中，供最终用户使用。这是一种魔法般的思维。
- en: ''
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Michael D. Murray, 26 SMU Science and Technology Law Review 259, 281 (2023)
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Michael D. Murray, 26 SMU 科技与法律评论 259, 281 (2023)
- en: And yet, models themselves do seem, in some circumstances, [to memorize training
    data](https://arxiv.org/pdf/2301.13188).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，模型本身似乎在某些情况下，[确实会记住训练数据](https://arxiv.org/pdf/2301.13188)。
- en: The following toy example is from a [Gradio Space on HuggingFace](https://huggingface.co/spaces/nathanReitinger/modelProblems)
    which allows users to pick a model, see an output, and check — from that model’s
    training data — how similar the generated image is to any image in its training
    data. MNIST digits were used to generate because they are easy for the machine
    to parse, easy for humans to interpret in terms of similarity, and have the nice
    property of being easily classified — allowing a hunt of similarity to only consider
    images that are of the same number (efficiency gains).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例来自[HuggingFace上的Gradio Space](https://huggingface.co/spaces/nathanReitinger/modelProblems)，该平台允许用户选择一个模型，查看输出，并从该模型的训练数据中检查生成的图像与其训练数据中任何图像的相似度。由于MNIST数字易于机器解析、易于人类从相似性角度理解，并且具有易于分类的优点——这使得相似性搜索只考虑相同数字的图像（提高效率），因此使用了MNIST数字。
- en: Let’s see how it works!
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 让我们看看它是如何工作的！
- en: The following image has a similarity score of .00039\. RMSE stands for Root
    Mean Squared Error and is a way of assessing the similarity between two images.
    True enough, many other methods for similarity assessment exist, but RMSE gives
    you a pretty good idea of whether an image is a duplicate or not (i.e., we are
    not hunting for a legal definition of similarity here). As an example, an RMSE
    of <.006 gets you into the nearly “copy” range, and an RMSE of <.0009 is entering
    perfect copy territory (indistinguishable to the naked eye).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像的相似度得分为0.00039。RMSE代表均方根误差，是评估两张图像相似度的一种方式。事实上，还有许多其他相似性评估方法，但RMSE能很好地判断一张图像是否为副本（即，我们这里并不是在寻找法律定义的相似性）。举个例子，RMSE值小于0.006时，图像已接近“复制”范围，而RMSE值小于0.0009时，则进入完美复制的领域（肉眼无法分辨）。
- en: '![](../Images/826e5055d8ed033dbdf5653fa61840b8.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/826e5055d8ed033dbdf5653fa61840b8.png)'
- en: '[🤗](https://huggingface.co/spaces/nathanReitinger/modelProblems) A model that
    generates a nearly exact copy of training data (RMSE at .0003) [🤗](https://huggingface.co/spaces/nathanReitinger/modelProblems)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[🤗](https://huggingface.co/spaces/nathanReitinger/modelProblems) 一个生成几乎完全相同训练数据副本的模型（RMSE为0.0003）[🤗](https://huggingface.co/spaces/nathanReitinger/modelProblems)'
- en: 'To use the [Gradio space](https://huggingface.co/spaces/nathanReitinger/modelProblems),
    follow these three steps (optionally build the space if it’s sleeping):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用[Gradio空间](https://huggingface.co/spaces/nathanReitinger/modelProblems)，请按照以下三个步骤操作（如果空间处于休眠状态，可以选择构建该空间）：
- en: '**STEP 1**: Select the type of pre-trained model to use'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤1**：选择要使用的预训练模型类型'
- en: '**STEP 2**: Hit “submit” and the model will generate an image for you (a 28x28
    grayscale image)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤2**：点击“提交”按钮，模型将为您生成一张图像（28x28的灰度图像）'
- en: '**STEP 3**: The Gradio app searches through that model’s training data to identify
    the most similar image to the generated image (out of 60K examples)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤3**：Gradio应用程序会在该模型的训练数据中搜索，识别与生成图像最相似的图像（从60K个示例中筛选）'
- en: As is plain to see, the image generated on the left (AI creation) is nearly
    an exact copy of the training data on the right when the “FASHION-diffusion-oneImage”
    model is used. And this makes sense. This model was trained on *only* a single
    image from the [FASHION dataset](https://www.tensorflow.org/datasets/catalog/fashion_mnist).
    The same is true for the “MNIST-diffusion-oneImage” model.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，左侧生成的图像（AI创作）几乎与右侧的训练数据完全相同，当使用“FASHION-diffusion-oneImage”模型时，结果正是如此。这是有道理的。该模型仅对[FASHION数据集](https://www.tensorflow.org/datasets/catalog/fashion_mnist)中的一张图像进行了训练。同样的情况也适用于“MNIST-diffusion-oneImage”模型。
- en: 'That said, even models trained on more images (e.g., 300, 3K, or 60K images)
    can produce eerily similar output. This example comes from a Generative Adversarial
    Network (GAN) trained on the full 60K image dataset (training only) of [MNIST
    hand-drawn digits](https://etzold.medium.com/mnist-dataset-of-handwritten-digits-f8cf28edafe).
    As background, GANs are known to produce [less-memorized generations](https://arxiv.org/abs/2301.13188)
    than diffusion models:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，即使是训练了更多图像（例如300张、3000张或60000张图像）的模型，也能产生非常相似的输出。这个示例来自一个生成对抗网络（GAN），它在完整的60K图像数据集（仅限训练）上进行了训练，数据集包括[MNIST手写数字](https://etzold.medium.com/mnist-dataset-of-handwritten-digits-f8cf28edafe)。作为背景，生成对抗网络（GAN）通常生成的图像[比扩散模型](https://arxiv.org/abs/2301.13188)记忆性差：
- en: '![](../Images/d11c66b90b7f39f29515de89eaa6a1b8.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d11c66b90b7f39f29515de89eaa6a1b8.png)'
- en: RMSE at .008
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: RMSE为0.008
- en: 'Here’s another with a*diffusion model* trained on the 60K MNIST dataset (i.e.,
    the type of model powering Stable Diffusion):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这是另一个使用*扩散模型*，并在60K MNIST数据集上训练的图像（即，支持稳定扩散的模型类型）：
- en: '![](../Images/f05a6c47cea9b8483157508ec39a87d1.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f05a6c47cea9b8483157508ec39a87d1.png)'
- en: RMSE at .004
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: RMSE为0.004
- en: Feel free to play around with the [Gradio space yourself](https://huggingface.co/spaces/nathanReitinger/modelProblems),
    investigate the models, or reach out to me with questions!
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 随时可以自己尝试使用[Gradio空间](https://huggingface.co/spaces/nathanReitinger/modelProblems)，探索模型，或者如果有问题可以联系我！
- en: '**Summary:** The point of this small, toy example is that there is nothing
    mystical or absolute-copyright-nullifying about machine-learning models. Machine
    learning models can and do produce images that are copies of their training data
    — in other words, models can and do *store* protected content, and may therefore
    run into copyright problems. True enough, there are many counterarguments to be
    made here (my work in progress!); this demo should only be taken as anecdotal
    evidence of storage, and possibly a canary for developers working in this space.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结：** 这个小型示例的重点是，机器学习模型没有什么神秘或绝对的版权豁免。机器学习模型确实可以并且会生成与其训练数据相同的图像——换句话说，模型确实会*存储*受保护的内容，因此可能会遇到版权问题。当然，也有许多反驳的论点（我正在进行的工作！）；这个演示应该仅作为存储的轶事性证据，可能是开发者在这一领域工作的“金丝雀”。'
- en: What goes into a model is just as important as what comes out, and this is especially
    true for certain models performing certain tasks. We need to be careful and mindful
    of our “back boxes” because this analogy often turns out not to be true. That
    you cannot interpret for yourself the set of weights held by a model does not
    mean you escape all forms of liability or scrutiny.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 输入到模型中的内容和从模型中得到的结果同样重要，尤其对于某些执行特定任务的模型来说更是如此。我们需要小心并关注我们的“黑箱”，因为这个类比往往并不成立。你无法自己解读模型所持有的权重集合，并不意味着你可以摆脱所有形式的责任或审查。
- en: '*—* [*@nathanReitinge*](https://nathanreitinger.umiacs.io)*r stay tuned for
    further work in this space!*'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*—* [*@nathanReitinge*](https://nathanreitinger.umiacs.io)*r，敬请关注该领域的进一步工作！*'
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
