- en: Leveraging Smaller LLMs for Enhanced Retrieval-Augmented Generation (RAG)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用小型LLMs增强检索增强生成（RAG）
- en: 原文：[https://towardsdatascience.com/leveraging-smaller-llms-for-enhanced-retrieval-augmented-generation-rag-bc320e71223d?source=collection_archive---------1-----------------------#2024-10-18](https://towardsdatascience.com/leveraging-smaller-llms-for-enhanced-retrieval-augmented-generation-rag-bc320e71223d?source=collection_archive---------1-----------------------#2024-10-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/leveraging-smaller-llms-for-enhanced-retrieval-augmented-generation-rag-bc320e71223d?source=collection_archive---------1-----------------------#2024-10-18](https://towardsdatascience.com/leveraging-smaller-llms-for-enhanced-retrieval-augmented-generation-rag-bc320e71223d?source=collection_archive---------1-----------------------#2024-10-18)
- en: Llama-3.2–1 B-Instruct and LanceDB
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Llama-3.2–1 B-Instruct与LanceDB
- en: '[](https://alexcpn.medium.com/?source=post_page---byline--bc320e71223d--------------------------------)[![Alex
    Punnen](../Images/296f165c293200adfa39482cb1388264.png)](https://alexcpn.medium.com/?source=post_page---byline--bc320e71223d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--bc320e71223d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--bc320e71223d--------------------------------)
    [Alex Punnen](https://alexcpn.medium.com/?source=post_page---byline--bc320e71223d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://alexcpn.medium.com/?source=post_page---byline--bc320e71223d--------------------------------)[![Alex
    Punnen](../Images/296f165c293200adfa39482cb1388264.png)](https://alexcpn.medium.com/?source=post_page---byline--bc320e71223d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--bc320e71223d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--bc320e71223d--------------------------------)
    [Alex Punnen](https://alexcpn.medium.com/?source=post_page---byline--bc320e71223d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--bc320e71223d--------------------------------)
    ·12 min read·Oct 18, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--bc320e71223d--------------------------------)
    ·阅读时间：12分钟·2024年10月18日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '**Abstract**: Retrieval-augmented generation (RAG) combines large language
    models with external knowledge sources to produce more accurate and contextually
    relevant responses. This article explores how smaller language models (LLMs),
    like the recently opensourced Meta 1 Billion model, can be effectively utilized
    to summarize and index large documents, thereby improving the efficiency and scalability
    of RAG systems. We provide a step-by-step guide, complete with code snippets,
    on how to summarize chunks of text from a product documentation PDF and store
    them in a LanceDB database for efficient retrieval.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**摘要**：检索增强生成（RAG）结合了大型语言模型与外部知识源，以生成更加准确和符合上下文的回答。本文探讨了如何有效利用小型语言模型（如最近开源的Meta
    10亿模型）来总结和索引大型文档，从而提高RAG系统的效率和可扩展性。我们提供了一个详细的指南，附有代码片段，展示如何从产品文档PDF中提取文本摘要，并将其存储在LanceDB数据库中以便高效检索。'
- en: Introduction
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: Retrieval-Augmented Generation is a paradigm that enhances the capabilities
    of language models by integrating them with external knowledge bases. While large
    LLMs like GPT-4 have demonstrated remarkable capabilities, they come with significant
    computational costs. Small LLMs offer a more resource-efficient alternative, especially
    for tasks like text summarization and keyword extraction, which are crucial for
    indexing and retrieval in RAG systems.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成（Retrieval-Augmented Generation，RAG）是一种通过将语言模型与外部知识库集成来增强其能力的范式。虽然像GPT-4这样的巨大语言模型（LLMs）已经展示了显著的能力，但它们也伴随着较高的计算成本。小型LLMs则提供了更为高效的资源使用替代方案，尤其适用于诸如文本摘要和关键词提取等任务，这些任务在RAG系统中的索引和检索过程中至关重要。
- en: 'In this article, we’ll demonstrate how to use a small LLM to:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将演示如何使用小型LLM：
- en: '**Extract and summarize text from a PDF document**.'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**从PDF文档中提取并总结文本**。'
- en: '**Generate embeddings for summaries and keywords**.'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**为摘要和关键词生成嵌入向量**。'
- en: '**Store the data efficiently in a LanceDB database**.'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**高效地将数据存储在LanceDB数据库中**。'
- en: '**Use this for effective RAG**'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**用于有效的RAG**'
- en: '**Also a Agentic workflow for self correcting errors from the LLM**'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**也可以为自我纠正LLM错误提供一个代理工作流**'
- en: Using a smaller LLM drastically reduces the cost for these types of conversions
    on huge data-sets and gets similar benefits for simpler tasks as the larger parameter
    LLMs and can easily be hosted in the Enterprise or from the Cloud with minimal
    cost.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 使用小型LLM大大降低了对大规模数据集进行这些类型转换的成本，并且在处理简单任务时，可以获得与大型参数LLMs相似的效果，同时可以轻松地在企业环境或云端托管，且成本最小。
- en: We will use [LLAMA 3.2 1 Billion](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/)
    parameter model, the smallest state-of-the-art LLM as of now.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用[LLAMA 3.2 10亿参数](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/)模型，它是目前最小的先进大型语言模型（LLM）。
- en: '![](../Images/d1e06fb0bf3f7cac0d4f247f2977944d.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1e06fb0bf3f7cac0d4f247f2977944d.png)'
- en: LLM Enhanced RAG (Image by Author)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 增强的 RAG（作者图像）
- en: The Problem with Embedding Raw Text
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 嵌入原始文本的问题
- en: Before diving into the implementation, it’s essential to understand why embedding
    raw text from documents can be problematic in RAG systems.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始实施之前，了解为什么从文档中嵌入原始文本在 RAG 系统中可能会引发问题非常重要。
- en: Ineffective Context Capture
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无效的上下文捕捉
- en: 'Embedding raw text from a page without summarization often leads to embeddings
    that are:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有总结的情况下直接从页面嵌入原始文本，通常会导致以下类型的嵌入：
- en: '**High-dimensional noise**: Raw text may contain irrelevant information, formatting
    artefacts, or boilerplate language that doesn’t contribute to understanding the
    core content.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高维噪音**：原始文本可能包含无关信息、格式化伪影或套话，这些都不有助于理解核心内容。'
- en: '**Diluted key concepts**: Important concepts may be buried within extraneous
    text, making the embeddings less representative of the critical information.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关键概念稀释**：重要概念可能埋藏在多余的文本中，使得嵌入无法有效代表关键信息。'
- en: Retrieval Inefficiency
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检索低效
- en: 'When embeddings do not accurately represent the key concepts of the text, the
    retrieval system may fail to:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当嵌入无法准确表示文本的关键信念时，检索系统可能会失败：
- en: '**Match user queries effectively**: The embeddings might not align well with
    the query embeddings, leading to poor retrieval of relevant documents.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有效匹配用户查询**：嵌入可能与查询嵌入不匹配，导致相关文档检索效果差。'
- en: '**Provide correct context**: Even if a document is retrieved, it may not offer
    the precise information the user is seeking due to the noise in the embedding.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提供正确的上下文**：即使检索到了文档，它可能无法提供用户所寻求的精确信息，因为嵌入中的噪音。'
- en: 'Solution: Summarization Before Embedding'
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决方案：在嵌入之前进行总结
- en: 'Summarizing the text before generating embeddings addresses these issues by:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成嵌入之前对文本进行总结，通过以下方式解决这些问题：
- en: '**Distilling Key Information**: Summarization extracts the essential points
    and keywords, removing unnecessary details.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提取关键信息**：总结提取了必要的要点和关键词，去除了不必要的细节。'
- en: '**Improving Embedding Quality**: Embeddings generated from summaries are more
    focused and representative of the main content, enhancing retrieval accuracy.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提高嵌入质量**：从总结中生成的嵌入更集中，并能更好地代表主要内容，从而提高检索的准确性。'
- en: Prerequisites
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前提条件
- en: 'Before we begin, ensure you have the following installed:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，请确保已经安装以下内容：
- en: Python 3.7 or higher
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 3.7 或更高版本
- en: PyTorch
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch
- en: Transformers library
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformers 库
- en: SentenceTransformers
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SentenceTransformers
- en: PyMuPDF (for PDF processing)
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyMuPDF（用于 PDF 处理）
- en: LanceDB
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LanceDB
- en: A laptop with GPU Min 6 GB or Colab (T4 GPU will be sufficient) or similar
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配备 GPU 最少 6 GB 的笔记本电脑，或 Colab（T4 GPU 足够）或类似设备
- en: 'Step 1: Setting Up the Environment'
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 1：设置环境
- en: First, import all the necessary libraries and set up logging for debugging and
    tracking.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，导入所有必要的库并设置日志记录以进行调试和跟踪。
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Step 2: Defining Helper Functions'
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 2：定义辅助函数
- en: Creating the Prompt
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建提示
- en: We define a function to create prompts compatible with the LLAMA 3.2 model.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义一个函数，用于创建与 LLAMA 3.2 模型兼容的提示。
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Processing the Prompt
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理提示
- en: This function processes the prompt using the model and tokenizer. We are setting
    the temperature to 0.1 to make the model less creative (less hallucinating)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数使用模型和分词器处理提示。我们将温度设置为 0.1，以使模型的创造性降低（减少幻觉现象）。
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Step 3: Loading the Model'
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 3：加载模型
- en: We use the LLAMA 3.2 1B Instruct model for summarization. We are loading the
    model with bfloat16 to reduce the memory and running in NVIDIA laptop GPU (NVIDIA
    GeForce RTX 3060 6 GB/ Driver NVIDIA-SMI 555.58.02/Cuda compilation tools, release
    12.5, V12.5.40) in a Linux OS.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 LLAMA 3.2 10亿指令模型进行总结。我们以 bfloat16 加载模型以减少内存，并在 NVIDIA 笔记本 GPU（NVIDIA GeForce
    RTX 3060 6 GB / 驱动程序 NVIDIA-SMI 555.58.02 / Cuda 编译工具，版本 12.5，V12.5.40）下运行 Linux
    操作系统。
- en: Better would be to host via vLLM or better [exLLamaV2](https://github.com/turboderp/exllamav2)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的做法是通过 vLLM 或更好的[exLLamaV2](https://github.com/turboderp/exllamav2)进行托管
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Step 4: Reading and Processing the PDF Document'
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 4：读取和处理 PDF 文档
- en: We extract text from each page of the PDF document.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 PDF 文档的每一页提取文本。
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Step 5: Setting Up LanceDB and SentenceTransformer'
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 5：设置 LanceDB 和 SentenceTransformer
- en: We initialize the SentenceTransformer model for generating embeddings and set
    up LanceDB for storing the data. We are using PyArrow based Schema for the LanceDB
    tables
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们初始化了SentenceTransformer模型以生成嵌入，并设置了LanceDB来存储数据。我们使用基于PyArrow的Schema来构建LanceDB表。
- en: Note that keywords are not used now but can be used for hybrid search, that
    is vector similarity search as well as text search if needed.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当前关键词未被使用，但如果需要，可以用于混合搜索，即矢量相似度搜索和文本搜索。
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Step 6: Summarizing and Storing Data'
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6步：总结和存储数据
- en: We loop through each page, generate a summary and keywords, and store them along
    with embeddings in the database.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遍历每一页，生成摘要和关键词，并将它们与嵌入一起存储到数据库中。
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Using LLMs to Correct Their Outputs
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LLM来修正它们的输出
- en: When generating summaries and extracting keywords, LLMs may sometimes produce
    outputs that are not in the expected format, such as malformed JSON.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成摘要和提取关键词时，LLM有时会生成格式不符合预期的输出，比如格式错误的JSON。
- en: We can leverage the LLM itself to correct these outputs by prompting it to fix
    the errors. This is shown in the code above
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用LLM本身来修正这些输出，通过提示它修复错误。如上面的代码所示。
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In this code snippet, if the LLM’s initial output cannot be parsed as JSON,
    we prompt the LLM again to correct the JSON. This self-correction pattern improves
    the robustness of our pipeline.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，如果LLM的初始输出无法解析为JSON，我们会再次提示LLM修正JSON格式。这个自我修正模式提高了我们管道的健壮性。
- en: 'Suppose the LLM generates the following malformed JSON:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 假设LLM生成了以下格式错误的JSON：
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Attempting to parse this JSON results in an error due to the use of single
    quotes instead of double quotes. We catch this error and prompt the LLM to correct
    it:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试解析这个JSON时，由于使用了单引号而不是双引号，结果会出现错误。我们捕获这个错误并提示LLM进行修正：
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The LLM then provides the corrected JSON:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，LLM提供修正后的JSON：
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: By using the LLM to correct its own output, we ensure that the data is in the
    correct format for downstream processing.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用LLM修正它自己的输出，我们确保数据格式正确，便于后续处理。
- en: Extending Self-Correction via LLM Agents
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过LLM代理扩展自我修正
- en: 'This pattern of using the LLM to correct its outputs can be extended and automated
    through the use of **LLM Agents**. LLM Agents can:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LLM来修正其输出的模式可以通过**LLM代理**来扩展和自动化。LLM代理可以：
- en: '**Automate Error Handling**: Detect errors and autonomously decide how to correct
    them without explicit instructions.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化错误处理**：检测错误并自主决定如何修正它们，无需明确的指示。'
- en: '**Improve Efficiency**: Reduce the need for manual intervention or additional
    code for error correction.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提高效率**：减少手动干预或额外代码的需求来进行错误修正。'
- en: '**Enhance Robustness**: Continuously learn from errors to improve future outputs.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强健壮性**：从错误中持续学习，改进未来的输出。'
- en: '**LLM Agents** act as intermediaries that manage the flow of information and
    handle exceptions intelligently. They can be designed to:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**LLM代理**充当中介，管理信息流并智能地处理异常。它们可以被设计成：'
- en: Parse outputs and validate formats.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解析输出并验证格式。
- en: Re-prompt the LLM with refined instructions upon encountering errors.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遇到错误时，重新提示LLM并提供精炼的指令。
- en: Log errors and corrections for future reference and model fine-tuning.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录错误和修正，以便未来参考和模型微调。
- en: '**Approximate Implementation**:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**近似实现**：'
- en: 'Instead of manually catching exceptions and re-prompting, an LLM Agent could
    encapsulate this logic:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要手动捕获异常并重新提示，LLM代理可以封装这些逻辑：
- en: '[PRE11]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The `LLMAgent` class would handle the initial processing, error detection, re-prompting,
    and correction internally.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`LLMAgent`类将处理初始处理、错误检测、重新提示和内部修正。'
- en: Now lets see how we can use the Embeddings for an effective RAG pattern again
    using the LLM to help in ranking.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何使用嵌入来有效地使用RAG模式，再次利用LLM帮助进行排序。
- en: 'Retrieval and Generation: Processing the User Query'
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检索和生成：处理用户查询
- en: This is the usual flow. We take the user’s question and search for the most
    relevant summaries.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通常的流程。我们获取用户的问题并搜索最相关的摘要。
- en: '[PRE12]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Preparing the Retrieved Summaries
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备检索到的摘要
- en: We compile the retrieved summaries into a list, associating each summary with
    its page number for reference.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将检索到的摘要编译成一个列表，并将每个摘要与其页码关联以供参考。
- en: '[PRE13]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Ranking the Summaries
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 排序摘要
- en: We prompt the language model to rank the retrieved summaries based on their
    relevance to the user’s question and select the most relevant one. This is again
    using the LLM in ranking the summaries than the K-Nearest Neighbour or Cosine
    distance or other ranking algorithms alone for the contextual embedding (vector)
    match.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提示语言模型根据摘要与用户问题的相关性对其进行排序，并选择最相关的一个。这是通过使用 LLM 排序摘要，而非单纯使用 K-最近邻或余弦距离等排序算法来进行上下文嵌入（向量）匹配。
- en: '[PRE14]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Extracting the Selected Summary and Generating the Final Answer
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取选定摘要并生成最终答案
- en: We retrieve the original content associated with the selected summary and prompt
    the language model to generate a detailed answer to the user’s question using
    this context.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从与所选摘要相关的原始内容中检索信息，并提示语言模型使用这些上下文生成详细的回答来回应用户的问题。
- en: '[PRE15]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Explanation of the Workflow
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作流说明
- en: '**User Query Vectorization**: The user’s question is converted into an embedding
    using the same SentenceTransformer model used during indexing.'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**用户查询向量化**：用户的问题通过与索引过程中使用的相同的 SentenceTransformer 模型转换为嵌入。'
- en: '**Similarity Search**: The query embedding is used to search the vector database
    (LanceDB) for the most similar summaries and return Top 3'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**相似度搜索**：查询嵌入用于在向量数据库（LanceDB）中搜索最相似的摘要，并返回前三个。'
- en: '[PRE16]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**3\. Summary Ranking**: The retrieved summaries are passed to the language
    model, which ranks them based on relevance to the user’s question.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. 摘要排序**：检索到的摘要被传递给语言模型，模型根据与用户问题的相关性对它们进行排序。'
- en: '[PRE17]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**4\. Context Retrieval**: The original content associated with the most relevant
    summary is retrieved by parsing out the page number and getting the associated
    page from the LanceDB'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**4\. 上下文检索**：通过解析页码并从 LanceDB 获取相关页面，检索与最相关摘要关联的原始内容。'
- en: '[PRE18]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '**5\. Answer Generation**: The language model generates a detailed answer to
    the user’s question using the retrieved context.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**5\. 答案生成**：语言模型使用检索到的上下文生成详细的回答来回应用户的问题。'
- en: Here is a sample output from a sample PDF I have used
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我使用的一个示例 PDF 文件的输出示例
- en: '[PRE19]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Conclusion
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: We can efficiently summarise and extract keywords from large documents using
    a small LLM like LLAMA 3.2 1B Instruct. These summaries and keywords can be embedded
    and stored in a database like LanceDB, enabling efficient retrieval for RAG systems
    using the LLM in the workflow and not just in generation
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用像 LLAMA 3.2 1B Instruct 这样的较小 LLM 高效地总结和提取大型文档的关键词。这些摘要和关键词可以嵌入并存储在像 LanceDB
    这样的数据库中，从而通过工作流中使用 LLM 而不仅仅是生成中，启用高效的检索供 RAG 系统使用。
- en: References
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Meta LLAMA 3.2 1B Instruct Model
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meta LLAMA 3.2 1B Instruct 模型
- en: '[SentenceTransformers](https://www.sbert.net/)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SentenceTransformers](https://www.sbert.net/)'
- en: '[LanceDB](https://lancedb.com/)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LanceDB](https://lancedb.com/)'
- en: '[PyMuPDF Documentation](https://pymupdf.readthedocs.io/en/latest/)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyMuPDF 文档](https://pymupdf.readthedocs.io/en/latest/)'
- en: '[Github repo](https://github.com/alexcpn/llmenhancedrag) for this project'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Github 仓库](https://github.com/alexcpn/llmenhancedrag)用于此项目'
