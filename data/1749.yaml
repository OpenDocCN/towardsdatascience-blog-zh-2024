- en: Data Curation Practices to Minimize Bias in Medical AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最小化医疗人工智能偏差的数据策划实践
- en: 原文：[https://towardsdatascience.com/data-curation-practices-to-minimize-bias-in-medical-ai-379bf6983de2?source=collection_archive---------8-----------------------#2024-07-17](https://towardsdatascience.com/data-curation-practices-to-minimize-bias-in-medical-ai-379bf6983de2?source=collection_archive---------8-----------------------#2024-07-17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/data-curation-practices-to-minimize-bias-in-medical-ai-379bf6983de2?source=collection_archive---------8-----------------------#2024-07-17](https://towardsdatascience.com/data-curation-practices-to-minimize-bias-in-medical-ai-379bf6983de2?source=collection_archive---------8-----------------------#2024-07-17)
- en: Ensuring fair and equitable healthcare outcomes from medical AI applications
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 确保医疗人工智能应用程序的公平和公正的健康结果
- en: '[](https://medium.com/@fimafurman?source=post_page---byline--379bf6983de2--------------------------------)[![Fima
    Furman](../Images/5d25a93fa0bf4f5ebb2c7a684709635c.png)](https://medium.com/@fimafurman?source=post_page---byline--379bf6983de2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--379bf6983de2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--379bf6983de2--------------------------------)
    [Fima Furman](https://medium.com/@fimafurman?source=post_page---byline--379bf6983de2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@fimafurman?source=post_page---byline--379bf6983de2--------------------------------)[![Fima
    Furman](../Images/5d25a93fa0bf4f5ebb2c7a684709635c.png)](https://medium.com/@fimafurman?source=post_page---byline--379bf6983de2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--379bf6983de2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--379bf6983de2--------------------------------)
    [Fima Furman](https://medium.com/@fimafurman?source=post_page---byline--379bf6983de2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--379bf6983de2--------------------------------)
    ·8 min read·Jul 17, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--379bf6983de2--------------------------------)
    ·阅读时间 8 分钟·2024年7月17日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/75a75a38b2d02a467ceedd983e778149.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75a75a38b2d02a467ceedd983e778149.png)'
- en: Potential sources of bias in AI training data. Graphic created by the author.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能训练数据中的潜在偏差来源。图形由作者创建。
- en: '[AI bias](https://www.ibm.com/topics/ai-bias#:~:text=AI%20bias%2C%20also%20called%20machine,outputs%20and%20potentially%20harmful%20outcomes.)
    refers to discrimination when AI systems produce unequal outcomes for different
    groups due to bias in the training data. When not mitigated, biases in AI and
    machine learning models can systematize and exacerbate discrimination faced by
    historically marginalized groups by embedding discrimination within decision-making
    algorithms.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[人工智能偏见](https://www.ibm.com/topics/ai-bias#:~:text=AI%20bias%2C%20also%20called%20machine,outputs%20and%20potentially%20harmful%20outcomes.)
    是指当人工智能系统由于训练数据中的偏见而对不同群体产生不平等的结果时，所发生的歧视现象。如果不加以缓解，人工智能和机器学习模型中的偏见可能会使历史上被边缘化的群体面临的歧视制度化并加剧，将歧视嵌入到决策算法中。'
- en: Issues in training data, such as unrepresentative or imbalanced datasets, historical
    prejudices embedded in the data, and flawed data collection methods, lead to biased
    models. For example, if a [loan decisioning application](https://www.forbes.com/sites/korihale/2021/09/02/ai-bias-caused-80-of-black-mortgage-applicants-to-be-denied/)
    is trained on historical decisions, but Black loan applicants were systematically
    discriminated against in these historical decisions, then the model will embed
    this discriminatory pattern within its decisioning. Biases can also be introduced
    during the feature selection and engineering phases, where certain attributes
    may inadvertently act as proxies for sensitive characteristics such as race, gender,
    or socioeconomic status. For example, [race and zip code](https://engineering.cmu.edu/news-events/news/2018/12/11-datta-proxies.html)
    are strongly associated in America, so an algorithm trained using zip code data
    will indirectly embed information about race in its decision-making process.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据中的问题，如数据集不具代表性或不平衡、历史偏见嵌入数据中以及数据收集方法存在缺陷，都会导致模型产生偏差。例如，如果一个 [贷款决策应用程序](https://www.forbes.com/sites/korihale/2021/09/02/ai-bias-caused-80-of-black-mortgage-applicants-to-be-denied/)是基于历史决策进行训练的，而历史决策中黑人贷款申请者遭受了系统性的歧视，那么模型将在其决策过程中嵌入这一歧视模式。偏差还可能在特征选择和工程阶段引入，其中某些特征可能无意中充当种族、性别或社会经济地位等敏感特征的替代指标。例如，美国的
    [种族和邮政编码](https://engineering.cmu.edu/news-events/news/2018/12/11-datta-proxies.html)通常是密切相关的，因此，使用邮政编码数据训练的算法可能会间接地将种族信息嵌入到其决策过程中。
- en: AI in medical contexts involves using machine learning models and algorithms
    to aid diagnosis, treatment planning, and patient care. AI bias can be especially
    harmful in these situations, driving significant disparities in healthcare delivery
    and outcomes. For example, a [predictive model for skin cancer](https://healthcare-in-europe.com/en/news/ai-in-skin-cancer-detection-darker-skin-inferior-results.html)
    that has been trained predominantly on images of lighter skin tones may perform
    poorly on patients with darker skin. Such a system might cause misdiagnoses or
    delayed treatment for patients with darker skin, resulting in higher mortality
    rates. Given the high stakes in healthcare applications, data scientists must
    take action to mitigate AI bias in their applications. This article will focus
    on what data curation techniques data scientists can take to remove bias in training
    sets before models are trained.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 医疗领域中的人工智能涉及使用机器学习模型和算法来辅助诊断、治疗计划制定和患者护理。在这些情况下，AI偏见可能特别有害，导致医疗服务和结果的显著差异。例如，使用主要以浅色皮肤图像训练的[皮肤癌预测模型](https://healthcare-in-europe.com/en/news/ai-in-skin-cancer-detection-darker-skin-inferior-results.html)可能在肤色较深的患者身上表现不佳。这样的系统可能导致误诊或延迟治疗，从而导致较高的死亡率。鉴于医疗应用中的高风险，数据科学家必须采取措施减少其应用中的AI偏见。本文将重点讨论数据科学家可以采取哪些数据策划技术，以在模型训练之前消除训练集中的偏见。
- en: How is AI Bias Measured?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何衡量AI偏见？
- en: To mitigate AI bias, it is important to understand how [model bias and fairness
    are defined](https://haas.berkeley.edu/wp-content/uploads/What-is-fairness_-EGAL2.pdf)
    (PDF) and measured. A fair/unbiased model ensures its predictions are equitable
    across different groups. This means that the model’s behavior, such as accuracy
    and selection probability, is comparable across subpopulations defined by sensitive
    features (e.g., race, gender, socioeconomic status).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻AI偏见，了解[模型偏见和公平性如何定义](https://haas.berkeley.edu/wp-content/uploads/What-is-fairness_-EGAL2.pdf)（PDF）并进行衡量是非常重要的。公平/无偏的模型确保其预测在不同群体之间是公平的。这意味着模型的行为（如准确性和选择概率）在由敏感特征（如种族、性别、社会经济地位）定义的子群体之间是可比的。
- en: 'Using quantitative metrics for AI fairness/bias, we can measure and improve
    our own models. These metrics compare accuracy rates and selection probability
    between historically privileged groups and historically non-privileged groups.
    Three commonly used metrics to measure how fairly an AI model treats different
    groups are:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用量化的AI公平性/偏见指标，我们可以衡量并改善自己的模型。这些指标比较历史上处于特权群体和非特权群体之间的准确率和选择概率。常用的三种衡量AI模型对不同群体是否公平的指标是：
- en: '**Statistical Parity Difference—**Compares the ratio of favorable outcomes
    between groups. This test shows that a model’s predictions are independent of
    sensitive group membership, aiming for equal selection rates across groups. It
    is useful in cases where an equal positive rate between groups is desired, such
    as hiring.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**统计平衡差异——**比较群体之间有利结果的比率。此测试表明，模型的预测与敏感群体成员身份无关，旨在实现群体间的平等选择率。在需要群体之间有相等正向结果的情况下非常有用，例如招聘。'
- en: '![](../Images/2b27f17da60ccb8313d09b5ff2c7335a.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b27f17da60ccb8313d09b5ff2c7335a.png)'
- en: '**Average Odds Difference —** Compares the disparity between false and true
    positive rates across different groups. This metric is stricter than Statistical
    Parity Difference because it seeks to ensure that false and true positive rates
    are equal between groups. It is useful in cases where both positive and negative
    errors are consequential, such as criminal justice.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**平均赔率差异——**比较不同群体之间的假阳性和真阳性率差异。此指标比统计平衡差异更为严格，因为它旨在确保群体之间假阳性和真阳性率相等。在假阳性和真阳性都有重大后果的情况下非常有用，例如刑事司法领域。'
- en: '![](../Images/19f2b6cfa46b7f20330df3ba95fb37e9.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/19f2b6cfa46b7f20330df3ba95fb37e9.png)'
- en: '**Equal Opportunity Difference** — Compares the true positive rates between
    different groups. It checks that qualified individuals from different groups have
    an equal chance of being selected by an AI system. It does not account for false
    positive rates, potentially leading to disparities in incorrect positive predictions
    across groups.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**机会平等差异——**比较不同群体之间的真阳性率。它检查不同群体中的合格个体是否有平等的机会被AI系统选中。它不考虑假阳性率，这可能会导致群体之间在错误正预测上的差异。'
- en: '![](../Images/5d4c980dd327776116cc5b8ca233eeb5.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d4c980dd327776116cc5b8ca233eeb5.png)'
- en: Data scientists can calculate these fairness/bias metrics on their models using
    a Python library such as Microsoft’s [Fairlearn](https://fairlearn.org/) package
    or IBM’s [AI Fairness 360](https://aif360.res.ibm.com/) Toolkit. For all these
    metrics, a value of zero represents a mathematically fair outcome.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家可以使用 Python 库，如微软的 [Fairlearn](https://fairlearn.org/) 包或 IBM 的 [AI Fairness
    360](https://aif360.res.ibm.com/) 工具包，来计算模型的公平性/偏见度量指标。对于所有这些指标，值为零表示数学上公平的结果。
- en: What Data Curation Practices Can Minimize AI Bias?
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么样的数据管理实践可以最小化人工智能偏见？
- en: To mitigate bias in AI training datasets, model builders have an arsenal of
    data curation techniques, which can be divided into quantitative (data transformation
    using mathematical packages) and qualitative (best practices for data collection).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少人工智能训练数据集中的偏见，模型构建者拥有一系列的数据管理技术，这些技术可以分为定量方法（使用数学包进行数据转换）和定性方法（数据收集的最佳实践）。
- en: '**Quantitative Practices**'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**定量实践**'
- en: '[**Remove correlations with sensitive features**](https://fairlearn.org/v0.10/user_guide/mitigation/preprocessing.html)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[**移除与敏感特征的相关性**](https://fairlearn.org/v0.10/user_guide/mitigation/preprocessing.html)'
- en: '![](../Images/b8c9e4419af21c19d12d483a1b4c4279.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b8c9e4419af21c19d12d483a1b4c4279.png)'
- en: Illustration of correlation removal in a sample dataset. Image by the author.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个示例数据集中移除相关性的示意图。图片来自作者。
- en: Even if sensitive features (e.g., race, gender) are excluded from model training,
    other features may still be correlated with these sensitive features and introduce
    bias. For example, zip code strongly correlates with race in the United States.
    To ensure these features do not introduce hidden bias, data scientists should
    preprocess their inputs to remove the correlation between other input features
    and sensitive features.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在模型训练中排除了敏感特征（例如，种族、性别），其他特征仍可能与这些敏感特征相关联，从而引入偏见。例如，在美国，邮政编码与种族有很强的相关性。为了确保这些特征不会引入隐藏的偏见，数据科学家应当预处理输入数据，移除其他输入特征与敏感特征之间的相关性。
- en: This can be done with [Fairlearn’s CorrelationRemover](https://fairlearn.org/v0.10/user_guide/mitigation/preprocessing.html)
    function. It mathematically transforms feature values to remove correlation while
    preserving most of the features' predictive value. See below for a sample code.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过 [Fairlearn 的 CorrelationRemover](https://fairlearn.org/v0.10/user_guide/mitigation/preprocessing.html)
    函数来实现。该函数通过数学方法转换特征值，移除相关性，同时保留特征的大部分预测价值。下面是一个示例代码。
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[**Use re-weighting and re-sampling to create a balanced sample**](https://link.springer.com/article/10.1007/s10115-011-0463-8)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[**使用重新加权和重采样创建平衡样本**](https://link.springer.com/article/10.1007/s10115-011-0463-8)'
- en: Reweighting and resampling are similar processes that create a more balanced
    training dataset to correct for when specific groups are under or overrepresented
    in the input set. Reweighting involves assigning different weights to data samples
    to ensure that underrepresented groups have a proportionate impact on the model’s
    learning process. Resampling involves either oversampling minority class instances
    or undersampling majority class instances to achieve a balanced dataset.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 重新加权和重采样是类似的过程，它们通过创建更平衡的训练数据集来纠正特定群体在输入集中的过度或不足表示。重新加权涉及为数据样本分配不同的权重，以确保被低估的群体对模型学习过程有相应的影响。重采样则是通过过采样少数类实例或欠采样多数类实例来实现数据集的平衡。
- en: If a sensitive group is underrepresented compared to the general population,
    data scientists can use [AI Fairness’s Reweighing](https://github.com/Trusted-AI/AIF360/blob/main/aif360/algorithms/preprocessing/reweighing.py)
    function to transform the data input. See below for sample code.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某个敏感群体在与一般人群相比时被低估，数据科学家可以使用 [AI Fairness 的 Reweighing](https://github.com/Trusted-AI/AIF360/blob/main/aif360/algorithms/preprocessing/reweighing.py)
    函数来转换数据输入。下面是示例代码。
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[**Transform feature values using a disparate impact remover**](https://arxiv.org/abs/1412.3756)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[**使用不均衡影响移除器转换特征值**](https://arxiv.org/abs/1412.3756)'
- en: Another technique to remove bias embedded in training data is transforming input
    features with a disparate impact remover. This technique adjusts feature values
    to increase fairness between groups defined by a sensitive feature while preserving
    the rank order of data within groups. This preserves the model's predictive capacity
    while mitigating bias.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 移除训练数据中嵌入的偏见的另一种技术是使用不均衡影响移除器转换输入特征。这项技术通过调整特征值，在由敏感特征定义的群体之间提高公平性，同时保持群体内数据的排序。这既能保持模型的预测能力，又能减少偏见。
- en: To transform features to remove disparate impact, you can use [AI Fairness’s
    Disparate Impact Remover](https://github.com/Trusted-AI/AIF360/blob/main/aif360/algorithms/preprocessing/disparate_impact_remover.py).
    Note that this tool only transforms input data fairness with respect to a single
    protected attribute, so it cannot improve fairness across multiple sensitive features
    or at the intersection of sensitive features. See below for sample code.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 要转化特征以消除不公平影响，可以使用[AI公平性工具中的不公平影响消除器](https://github.com/Trusted-AI/AIF360/blob/main/aif360/algorithms/preprocessing/disparate_impact_remover.py)。请注意，该工具仅会根据单一保护属性转化输入数据的公平性，因此无法改善多个敏感特征或敏感特征交集中的公平性。以下是示例代码。
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[**Leverage diverse expert data annotation to minimize labeling bias**](https://arxiv.org/abs/2312.10198)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[**利用多样化的专家数据标注来最小化标注偏差**](https://arxiv.org/abs/2312.10198)'
- en: For supervised learning use cases, human data labeling of the response variable
    is often necessary. In these cases, imperfect human data labelers introduce their
    personal biases into the dataset, which are then learned by the machine. This
    is exacerbated when small, non-diverse groups of labelers do data annotation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于监督学习的应用场景，通常需要对响应变量进行人工数据标注。在这些情况下，不完美的人工数据标注员会将个人偏见引入数据集，然后这些偏见会被机器学习到。当标注员群体较小且缺乏多样性时，这种偏差会更加严重。
- en: To minimize bias in the data annotation process, use a high-quality data annotation
    solution that leverages diverse expert opinions, such as [Centaur Labs](https://hubs.li/Q02GXtBT0).
    By algorithmically synthesizing multiple opinions using meritocratic measures
    of label confidence, such solutions mitigate the effect of individual bias and
    [drive huge gains in labeling accuracy](https://hubs.li/Q02GXBxn0) for your dataset.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最小化数据标注过程中的偏差，使用高质量的数据标注解决方案，利用多样化的专家意见，如[Centaur Labs](https://hubs.li/Q02GXtBT0)。通过使用优越的标签置信度度量来算法性地合成多个意见，这样的解决方案可以缓解个体偏差的影响，并[推动标注准确性的巨大提升](https://hubs.li/Q02GXBxn0)，从而提高数据集的标注质量。
- en: '![](../Images/73e3cdf4d69095d0aba30161e1608c6d.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/73e3cdf4d69095d0aba30161e1608c6d.png)'
- en: Illustration of how medical data labeling accuracy can improve by aggregating
    multiple expert opinions. Image by the author.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 展示如何通过汇总多个专家意见来提高医疗数据标注的准确性。图像由作者提供。
- en: Qualitative Practices
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定性实践
- en: '**Implement inclusive and representative data collection practices**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**实施包容性和具有代表性的数据收集实践**'
- en: Medical AI training data must have sufficient sample sizes across all patient
    demographic groups and conditions to accurately make predictions for diverse groups
    of patients. To ensure datasets meet these needs, application builders should
    engage with relevant medical experts and stakeholders representing the affected
    patient population to define data requirements. Data scientists can use stratified
    sampling to ensure that their training set does not over or underrepresent groups
    of interest.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 医疗AI训练数据必须包含来自所有患者人口群体和条件的足够样本，以便准确地为多样化的患者群体做出预测。为了确保数据集满足这些需求，应用开发者应与相关的医学专家和利益相关者合作，代表受影响的患者群体来定义数据要求。数据科学家可以使用分层抽样来确保他们的训练集不会过度或不足地代表感兴趣的群体。
- en: Data scientists must also ensure that collection techniques do not bias data.
    For example, if medical imaging equipment is inconsistent across different samples,
    this would introduce systematic differences in the data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家还必须确保收集技术不会引入数据偏差。例如，如果医疗影像设备在不同样本之间不一致，这将会引入系统性的差异。
- en: '**Ensure data cleaning practices do not introduce bias**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**确保数据清理实践不引入偏差**'
- en: To avoid creating bias during data cleaning, data scientists must handle missing
    data and impute values carefully. When a dataset has missing values for a sensitive
    feature like patient age, simple strategies such as imputing the mean age could
    skew the data, especially if certain age groups are underrepresented. Instead,
    techniques such as stratified imputation, where missing values are filled based
    on the distribution within relevant subgroups (e.g., imputing within age brackets
    or demographic categories). Advanced methods like [multiple imputation](https://www.publichealth.columbia.edu/research/population-health-methods/missing-data-and-multiple-imputation),
    which generates several plausible values and averages them to account for uncertainty,
    may also be appropriate depending on the situation. After performing data cleaning,
    data scientists should document the imputation process and ensure that the cleaned
    dataset remains representative and unbiased according to predefined standards.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为避免在数据清洗过程中产生偏差，数据科学家必须谨慎处理缺失数据并填补缺失值。当数据集中存在敏感特征（如患者年龄）的缺失值时，简单的策略（如填补平均年龄）可能会扭曲数据，特别是当某些年龄组的样本不足时。相反，可以使用分层插补技术，根据相关子组（例如按年龄段或人口统计类别）内的分布填补缺失值。根据具体情况，像[多重插补](https://www.publichealth.columbia.edu/research/population-health-methods/missing-data-and-multiple-imputation)这样的高级方法，也可以生成多个合理的值并对其进行平均，以考虑不确定性。数据清洗完成后，数据科学家应记录插补过程，并确保清理后的数据集仍然代表性强且无偏，符合预定的标准。
- en: '**Publish curation practices for stakeholder input**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**发布数据整理实践，征求利益相关者的意见**'
- en: As data scientists develop their data curation procedure, they should publish
    them for stakeholder input to promote transparency and accountability. When stakeholders
    (e.g., patient group representatives, researchers, and ethicists) review and provide
    feedback on data curation methods, it helps identify and address potential sources
    of bias early in the development process. Furthermore, stakeholder engagement
    fosters trust and confidence in AI systems by demonstrating a commitment to ethical
    and inclusive practices. This trust is essential for driving post-deployment use
    of AI systems.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家在制定数据整理程序时，应将其发布以征求利益相关者的意见，从而促进透明度和问责制。当利益相关者（例如患者群体代表、研究人员和伦理学家）审查并提供有关数据整理方法的反馈时，有助于在开发过程中尽早识别和解决潜在的偏差来源。此外，利益相关者的参与通过展示对道德和包容性实践的承诺，促进了对AI系统的信任和信心。这种信任对于推动AI系统的部署后使用至关重要。
- en: '**Regularly audit and review input data and model performance**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**定期审计和审查输入数据和模型性能**'
- en: Regularly auditing and reviewing input data for live models ensures that bias
    does not develop in training sets over time. As medicine, patient demographics,
    and data sources evolve, previously unbiased models can become biased if the input
    data no longer represents the current population accurately. Continuous monitoring
    helps identify and correct any emerging biases, ensuring the model remains fair
    and effective.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 定期审计和审查实时模型的输入数据可确保训练集中的偏差不会随着时间的推移而产生。随着医学、患者人口统计和数据来源的变化，之前无偏的模型如果输入数据不再准确代表当前的群体，可能会变得有偏。持续监控有助于识别并修正任何新出现的偏差，确保模型保持公平和有效。
- en: Summary
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: '![](../Images/4c7e7655123eabedbf45d564a27771ed.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c7e7655123eabedbf45d564a27771ed.png)'
- en: Photo by [Planet Volumes](https://unsplash.com/@planetvolumes) on [Unsplash](https://unsplash.com/)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Planet Volumes](https://unsplash.com/@planetvolumes)提供，来自[Unsplash](https://unsplash.com/)
- en: Data scientists must take measures to minimize bias in their medical AI models
    to achieve equitable patient outcomes, drive stakeholder buy-in, and gain regulatory
    approval. Data scientists can leverage emerging tools from libraries such as [Fairlearn](https://fairlearn.org/)
    (Microsoft) or [AI Fairness 360 Toolkit](https://aif360.res.ibm.com/) (IBM) to
    measure and improve fairness in their AI models. While these tools and quantitative
    measures like Statistical Parity Difference are useful, developers must remember
    to take a holistic approach to fairness. This requires collaboration with experts
    and stakeholders from affected groups to understand patient populations and the
    impact of AI applications. If data scientists adhere to this practice, they will
    usher in a new era of just and superior healthcare for all.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家必须采取措施，减少医疗 AI 模型中的偏见，以实现公平的患者结果，推动利益相关者的支持，并获得监管批准。数据科学家可以利用来自如[Fairlearn](https://fairlearn.org/)（微软）或[AI
    Fairness 360 Toolkit](https://aif360.res.ibm.com/)（IBM）等库中的新兴工具来衡量和改善 AI 模型的公平性。尽管这些工具和像统计平衡差异（Statistical
    Parity Difference）这样的量化指标非常有用，开发人员仍需记住，要采取一种全面的公平性方法。这需要与专家和受影响群体的利益相关者合作，以理解患者群体和
    AI 应用的影响。如果数据科学家遵循这一实践，他们将迎来一个更加公正且优越的全民医疗新时代。
