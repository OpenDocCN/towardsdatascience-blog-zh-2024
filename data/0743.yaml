- en: Large Language Models Just Got A Whole Lot Smaller
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型变得更小了
- en: 原文：[https://towardsdatascience.com/large-language-models-just-got-a-whole-lot-smaller-f93425ee59a2?source=collection_archive---------1-----------------------#2024-03-20](https://towardsdatascience.com/large-language-models-just-got-a-whole-lot-smaller-f93425ee59a2?source=collection_archive---------1-----------------------#2024-03-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/large-language-models-just-got-a-whole-lot-smaller-f93425ee59a2?source=collection_archive---------1-----------------------#2024-03-20](https://towardsdatascience.com/large-language-models-just-got-a-whole-lot-smaller-f93425ee59a2?source=collection_archive---------1-----------------------#2024-03-20)
- en: And how this might change the game for software startups
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这可能如何改变软件初创公司的竞争格局
- en: '[](https://arijoury.medium.com/?source=post_page---byline--f93425ee59a2--------------------------------)[![Ari
    Joury, PhD](../Images/5b9e49279fb3f26373b393f29a4daaf7.png)](https://arijoury.medium.com/?source=post_page---byline--f93425ee59a2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f93425ee59a2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f93425ee59a2--------------------------------)
    [Ari Joury, PhD](https://arijoury.medium.com/?source=post_page---byline--f93425ee59a2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://arijoury.medium.com/?source=post_page---byline--f93425ee59a2--------------------------------)[![Ari
    Joury, PhD](../Images/5b9e49279fb3f26373b393f29a4daaf7.png)](https://arijoury.medium.com/?source=post_page---byline--f93425ee59a2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f93425ee59a2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f93425ee59a2--------------------------------)
    [Ari Joury, PhD](https://arijoury.medium.com/?source=post_page---byline--f93425ee59a2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f93425ee59a2--------------------------------)
    ·15 min read·Mar 20, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f93425ee59a2--------------------------------)
    ·15分钟阅读·2024年3月20日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/afd673415e30151b37e539a583a1e1f5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/afd673415e30151b37e539a583a1e1f5.png)'
- en: LLMs are getting smaller and more efficient! Image inspired by [Benjamin Marie](/run-llama-2-70b-on-your-gpu-with-exllamav2-588141a88598)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: LLM正在变得越来越小、越来越高效！图片灵感来自 [Benjamin Marie](/run-llama-2-70b-on-your-gpu-with-exllamav2-588141a88598)
- en: '**This piece was co-written with** [**David Meiborg**](https://medium.com/u/8523370997f4?source=post_page---user_mention--f93425ee59a2--------------------------------)**.**'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**本文由** [**David Meiborg**](https://medium.com/u/8523370997f4?source=post_page---user_mention--f93425ee59a2--------------------------------)**共同撰写。**'
- en: '*TLDR: Large Language Models (LLMs for short) are currently huge, costly to
    run, and have a* [*significant carbon footprint*](https://arxiv.org/abs/2309.14393)*.
    Recent advancements in model compression and system-level optimization methods
    might, however, enhance LLM inference. In particular, an approach using parameters
    with ternary structure has the potential of circumventing much of the costly matrix
    multiplication that is standard today. This has exciting consequences for hardware
    startups making specialized chips, but also for software startups that use or
    custom-build their own LLMs. Startups that help their customers deploy LLMs might
    also have more business coming for them.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*TLDR：大型语言模型（简称LLMs）目前非常庞大，运行成本高，而且具有* [*显著的碳足迹*](https://arxiv.org/abs/2309.14393)*。然而，近期在模型压缩和系统级优化方法上的进展可能会增强LLM的推理能力。特别是，使用具有三元结构的参数的方法，有可能绕过当前标准的成本高昂的矩阵乘法。这对那些生产专用芯片的硬件初创公司以及那些使用或定制自己LLM的软体初创公司来说，带来了激动人心的前景。那些帮助客户部署LLM的初创公司也可能会迎来更多的商业机会。*'
- en: Large language models today are big. Like, really big. If you want to load a
    LlaMa-2–70B model, you’d need 140 GB of VRAM (that’s 70 billion parameters multiplied
    by 2 bytes per parameter). For comparison, GPUs like the NVIDIA RTX 3090 or 4090,
    have just 24 GB of VRAM — a fraction of what one would need.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的大型语言模型非常庞大。可以说，真的很大。如果你想加载一个LlaMa-2–70B模型，你需要140 GB的显存（这相当于70亿个参数，每个参数占用2字节）。相比之下，像NVIDIA
    RTX 3090或4090这样的GPU只有24 GB的显存——这只是所需显存的一小部分。
- en: There are some [workarounds with quantization](/run-llama-2-70b-on-your-gpu-with-exllamav2-588141a88598),
    but these tend to be cumbersome. Likely you will still have your GPU running hot
    for up to 15 hours until the model is loaded. Not to mention that you still…
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些[量化的变通方法](/run-llama-2-70b-on-your-gpu-with-exllamav2-588141a88598)，但这些方法通常很麻烦。你很可能仍然需要让你的GPU工作长达15小时，直到模型加载完成。更不用说你仍然...
