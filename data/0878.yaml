- en: 'Deep Learning At Scale: Parallel Model Training'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大规模深度学习：并行模型训练
- en: 原文：[https://towardsdatascience.com/deep-learning-at-scale-parallel-model-training-d7c22904b5a4?source=collection_archive---------9-----------------------#2024-04-05](https://towardsdatascience.com/deep-learning-at-scale-parallel-model-training-d7c22904b5a4?source=collection_archive---------9-----------------------#2024-04-05)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/deep-learning-at-scale-parallel-model-training-d7c22904b5a4?source=collection_archive---------9-----------------------#2024-04-05](https://towardsdatascience.com/deep-learning-at-scale-parallel-model-training-d7c22904b5a4?source=collection_archive---------9-----------------------#2024-04-05)
- en: Concept and a Pytorch Lightning example
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概念与Pytorch Lightning示例
- en: '[](https://medium.com/@caroline.arnold_63207?source=post_page---byline--d7c22904b5a4--------------------------------)[![Caroline
    Arnold](../Images/fb13ba36e302d8161b67c4888d0601e4.png)](https://medium.com/@caroline.arnold_63207?source=post_page---byline--d7c22904b5a4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d7c22904b5a4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d7c22904b5a4--------------------------------)
    [Caroline Arnold](https://medium.com/@caroline.arnold_63207?source=post_page---byline--d7c22904b5a4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@caroline.arnold_63207?source=post_page---byline--d7c22904b5a4--------------------------------)[![Caroline
    Arnold](../Images/fb13ba36e302d8161b67c4888d0601e4.png)](https://medium.com/@caroline.arnold_63207?source=post_page---byline--d7c22904b5a4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d7c22904b5a4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d7c22904b5a4--------------------------------)
    [Caroline Arnold](https://medium.com/@caroline.arnold_63207?source=post_page---byline--d7c22904b5a4--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d7c22904b5a4--------------------------------)
    ·7 min read·Apr 5, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d7c22904b5a4--------------------------------)
    ·阅读时间7分钟·2024年4月5日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/9889f32b020f8d3fbe4ebe73db1d32fe.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9889f32b020f8d3fbe4ebe73db1d32fe.png)'
- en: Image created by the author using Midjourney.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用Midjourney创建。
- en: Parallel training on a large number of GPUs is state of the art in deep learning.
    The open source image generation algorithm Stable Diffusion [was trained](https://the-decoder.com/training-cost-for-stable-diffusion-was-just-600000-and-that-is-a-good-sign-for-ai-progress/)
    on a cluster of 256 GPUs. Meta’s [AI Research SuperCluster](https://www.siliconrepublic.com/machines/meta-is-using-two-nvidia-gpu-clusters-to-train-llama-3)
    contains more than 24,000 NVIDIA H100 GPUs that are used to train models such
    as Llama 3.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在大量GPU上进行并行训练是深度学习的前沿技术。开源图像生成算法Stable Diffusion[在256个GPU集群上进行了训练](https://the-decoder.com/training-cost-for-stable-diffusion-was-just-600000-and-that-is-a-good-sign-for-ai-progress/)。Meta的[AI研究超级集群](https://www.siliconrepublic.com/machines/meta-is-using-two-nvidia-gpu-clusters-to-train-llama-3)包含超过24,000个NVIDIA
    H100 GPU，用于训练像Llama 3这样的模型。
- en: By using multiple GPUs, machine learning experts reduce the *wall time* of their
    training runs. Training Stable Diffusion [took 150,000 GPU hours](https://the-decoder.com/training-cost-for-stable-diffusion-was-just-600000-and-that-is-a-good-sign-for-ai-progress/),
    or more than 17 years. Parallel training reduced that to 25 days.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用多个GPU，机器学习专家减少了训练过程的*墙时*。训练Stable Diffusion[花费了150,000个GPU小时](https://the-decoder.com/training-cost-for-stable-diffusion-was-just-600000-and-that-is-a-good-sign-for-ai-progress/)，相当于超过17年。并行训练将其减少到25天。
- en: 'There are two types of [parallel deep learning](https://arxiv.org/abs/1802.09941):'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 存在两种类型的[并行深度学习](https://arxiv.org/abs/1802.09941)：
- en: '*Data parallelism*, where a large dataset is distributed across multiple GPUs.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据并行性*，将大型数据集分布到多个GPU上。'
- en: '*Model parallelism*, where a deep learning model that is too large to fit on
    a single GPU is distributed across multiple devices.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型并行性*，将过大无法在单个GPU上运行的深度学习模型分布到多个设备上。'
- en: We will focus here on data parallelism, as model parallelism [only becomes relevant](https://lightning.ai/docs/pytorch/stable/advanced/model_parallel/fsdp.html)
    for very large models beyond 500M parameters.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里将重点讨论数据并行性，因为模型并行性[仅在非常大的模型（超过500M参数）时才相关](https://lightning.ai/docs/pytorch/stable/advanced/model_parallel/fsdp.html)。
- en: 'Beyond reducing wall time, there is an economic argument for parallel training:
    Cloud compute providers such as [AWS offer single machines](https://docs.aws.amazon.com/batch/latest/userguide/gpu-jobs.html)
    with up to 16 GPUs. Parallel training can take advantage of all available GPUs,
    and you get more value for your money.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 除了减少壁钟时间外，进行并行训练还有一个经济学上的理由：云计算提供商如[AWS提供单台机器](https://docs.aws.amazon.com/batch/latest/userguide/gpu-jobs.html)，配备最多16个GPU。并行训练可以利用所有可用的GPU，从而让你用更少的钱获得更多的价值。
- en: Parallel computing
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并行计算
- en: '[Parallelism is the dominant paradigm](https://en.wikipedia.org/wiki/Parallel_computing)
    in high performance computing. Programmers identify tasks that can be executed
    independently of each other and distribute them across a large number of devices.
    The serial parts of the program distribute the tasks and gather the results.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[并行性是高性能计算中的主流范式](https://en.wikipedia.org/wiki/Parallel_computing)。程序员识别出可以独立执行的任务，并将它们分配到大量设备上。程序的串行部分负责分发任务并收集结果。'
- en: Parallel computing reduces computation time. A program parallelized across four
    devices could ideally run four times faster than the same program running on a
    single device. In practice, communication overhead limits this scaling.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 并行计算可以减少计算时间。一程序如果在四个设备上并行运行，相比于在单个设备上运行，理想情况下可以快四倍。实际上，通信开销限制了这种扩展。
- en: '*As an analogy, think of a group of painters painting a wall. The communication
    overhead occurs when the foreman tells everyone what color to use and what area
    to paint. Painting can be done in parallel, and only the finishing is again a
    serial task.*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*打个比方，想象一群画家在画一面墙。通信开销出现在工头告诉每个人使用什么颜色、涂哪里时。涂画的过程可以并行进行，只有最后的收尾工作是串行的任务。*'
- en: Training deep learning models
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练深度学习模型
- en: Training a deep learning algorithm requires data and an optimization routine
    like [stochastic gradient descent](/stochastic-gradient-descent-math-and-python-code-35b5e66d6f79).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 训练深度学习算法需要数据和一个优化程序，如[随机梯度下降](/stochastic-gradient-descent-math-and-python-code-35b5e66d6f79)。
- en: The training data is shuffled and split into batches of fixed size. Batch by
    batch, the training routine calculates the loss between the actual and predicted
    labels. The model parameters are adjusted according to the gradient of the loss
    with respect to the model parameters.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据被打乱并分成固定大小的批次。每批次，训练程序都会计算实际标签和预测标签之间的损失。模型参数会根据损失相对于模型参数的梯度进行调整。
- en: '![](../Images/9f2d4393572ca20cfa46824bd8c9d985.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f2d4393572ca20cfa46824bd8c9d985.png)'
- en: Stochastic gradient descent. Image created by the author.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降。图片由作者制作。
- en: An *epoch* of training is complete when the model has seen all training data
    batches once.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型已经遍历过所有训练数据批次一次时，*epoch*就完成了。
- en: Distributed data parallelism
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式数据并行
- en: The following diagram describes data parallelism for training a deep learning
    model using 3 GPUs. A replica of the untrained model is copied to each GPU. The
    data set is split into 3 parts, each part is processed in parallel on a separate
    GPU.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 下图描述了使用3个GPU进行深度学习模型训练的数据并行。未训练的模型副本被复制到每个GPU上。数据集被分成3个部分，每个部分在不同的GPU上并行处理。
- en: '![](../Images/2f7994943c709cc560310884b328f8f5.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f7994943c709cc560310884b328f8f5.png)'
- en: Distributed data, same model. Image created by the author.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式数据，相同的模型。图片由作者制作。
- en: Note that each model replica sees different subsets of the training data. After
    completing the epoch, the weights and biases in each model replica are different.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，每个模型副本看到的训练数据子集是不同的。完成一个epoch后，每个模型副本中的权重和偏差都会有所不同。
- en: The three model replicas are harmonized by averaging the weights across all
    model instances. The updated model is broadcast across all 3 GPUs, and training
    continues with the next epoch.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在所有模型实例之间平均权重来协调这三个模型副本。更新后的模型会广播到所有3个GPU，训练继续进行到下一个epoch。
- en: Distributed training changes the effective batch size to `number of devices
    * original batch size` . This can affect training, convergence, and model performance.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式训练将有效批次大小改变为`设备数量 * 原始批次大小`。这可能会影响训练、收敛性和模型表现。
- en: Implementation in Pytorch Lightning
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Pytorch Lightning中的实现
- en: 'We need several ingredients for data parallelism:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行需要以下几个要素：
- en: A dataloader that can handle distributed training
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个可以处理分布式训练的数据加载器
- en: An *all-reduce* function that harmonizes the model replicas
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*all-reduce*函数，用于协调模型副本
- en: A framework for the different parallel parts to communicate with each other
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于不同并行部分之间相互通信的框架
- en: 'In PyTorch Lightning, the [Lightning Trainer](https://lightning.ai/docs/pytorch/stable/common/trainer.html)
    handles the entire training process. It can perform distributed data parallel
    training *out of the box* by specifying the following keyword arguments:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch Lightning 中，[Lightning Trainer](https://lightning.ai/docs/pytorch/stable/common/trainer.html)
    处理整个训练过程。通过指定以下关键字参数，它可以*开箱即用*地执行分布式数据并行训练：
- en: Number of nodes — the machines in the cloud compute cluster that you want to
    use
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点数量——你想要使用的云计算集群中的机器数量
- en: Number of devices (GPUs) *per node*
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个节点的设备数量（GPU）*每个节点*
- en: Training strategy, here distributed data parallelism. Different [strategies](https://lightning.ai/docs/pytorch/stable/api_references.html#strategies)
    are available for distributed training, they take care of adjusting the training
    procedure.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练策略，这里是分布式数据并行。对于分布式训练，有不同的[策略](https://lightning.ai/docs/pytorch/stable/api_references.html#strategies)可供选择，它们负责调整训练过程。
- en: 'For distributed data parallelism (*ddp*) training on 2 nodes with 2 GPUs each,
    use:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在 2 个节点，每个节点有 2 个 GPU 的分布式数据并行（*ddp*）训练，请使用：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: By default, the Lightning Trainer uses all available GPUs in the context of
    the computing environment. This implies that you need to specify explicitly if
    you want to use only a single GPU on a multi-GPU machine.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Lightning Trainer 会在计算环境中使用所有可用的 GPU。这意味着，如果你只想在多 GPU 机器上使用单个 GPU，则需要明确指定。
- en: 'Experiment: Parallel training with the Country211 dataset'
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验：使用 Country211 数据集进行并行训练
- en: The [Country211 dataset](https://github.com/openai/CLIP/blob/main/data/country211.md)
    consists of geo-tagged images from 211 countries. In my experiment, I use a pre-trained
    vision transformer and fine tune this model on the Country211 dataset. The code
    is available on [GitHub](https://github.com/crlna16/pretrained-vision-transformer).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[Country211 数据集](https://github.com/openai/CLIP/blob/main/data/country211.md)包含来自
    211 个国家的地理标签图像。在我的实验中，我使用了一个预训练的视觉 Transformer，并在 Country211 数据集上微调此模型。代码可在[GitHub](https://github.com/crlna16/pretrained-vision-transformer)上获取。'
- en: '[](/how-to-fine-tune-a-pretrained-vision-transformer-on-satellite-data-d0ddd8359596?source=post_page-----d7c22904b5a4--------------------------------)
    [## How to Fine-Tune a Pretrained Vision Transformer on Satellite Data'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/how-to-fine-tune-a-pretrained-vision-transformer-on-satellite-data-d0ddd8359596?source=post_page-----d7c22904b5a4--------------------------------)
    [## 如何在卫星数据上微调预训练的视觉 Transformer'
- en: A step-by-step tutorial in PyTorch Lightning
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PyTorch Lightning 中的逐步教程
- en: towardsdatascience.com](/how-to-fine-tune-a-pretrained-vision-transformer-on-satellite-data-d0ddd8359596?source=post_page-----d7c22904b5a4--------------------------------)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/how-to-fine-tune-a-pretrained-vision-transformer-on-satellite-data-d0ddd8359596?source=post_page-----d7c22904b5a4--------------------------------)
- en: There are 31,650 training images. I used a batch size of 256 throughout and
    [experimented with the number of GPUs](https://lightning.ai/docs/fabric/stable/guide/multi_node/cloud.html),
    going from a single GPU on a single node up to eight GPUs distributed across four
    nodes. I applied an early stopping condition, where training stops if the validation
    accuracy does not improve for five epochs.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 共有 31,650 张训练图像。我在整个过程中使用了批量大小为 256，并且[实验了 GPU 数量](https://lightning.ai/docs/fabric/stable/guide/multi_node/cloud.html)，从单个节点上的单个
    GPU 到分布在四个节点上的八个 GPU。我应用了提前停止条件，如果验证准确率在五个轮次内没有改善，则停止训练。
- en: The training wall time, which is the total time it took to train the model,
    decreases with the number of GPUs used. On a single GPU, the wall time is 45 minutes,
    which drops to 11 minutes when I use eight GPUs.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的墙时间，即训练模型所需的总时间，随着使用的 GPU 数量增加而减少。在单个 GPU 上，墙时间为 45 分钟，而在使用 8 个 GPU 时，墙时间降至
    11 分钟。
- en: The ideal scaling would be 45 minutes / 8 GPUs = 5.6 minutes, but communication
    overhead and a larger number of epochs when training on more GPUs keep us from
    reaching this optimum.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 理想的扩展性应为 45 分钟 / 8 个 GPU = 5.6 分钟，但在更多 GPU 上训练时，由于通信开销和训练轮次增多，我们无法达到这一最优值。
- en: '![](../Images/e288058bf56d71bc57ef28cd0c8acbe3.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e288058bf56d71bc57ef28cd0c8acbe3.png)'
- en: Wall time for different numbers of devices. Image created by the author.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 不同数量设备的墙时间。图像由作者创建。
- en: 'To compare the generalization capabilities, I calculated the test set accuracy
    with all trained models. The best test set accuracy was obtained when using a
    single GPU on a single device, but even there, the model had issues with generalization:
    The test set accuracy is only 15%.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较泛化能力，我计算了所有训练模型的测试集准确率。在单一设备上的单个GPU训练时，获得了最佳的测试集准确率，但即便如此，模型在泛化方面仍然存在问题：测试集准确率仅为15%。
- en: The panel shows the decrease in accuracy, relative to the optimal result, for
    different numbers of GPUs. The more devices were used in training, the less accurate
    the model became when applied to the test set.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 面板展示了不同数量GPU下准确率的下降，相对于最佳结果。使用的设备越多，应用到测试集时模型的准确性就越差。
- en: '![](../Images/28b987cf516ae749486725807cc19f5d.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/28b987cf516ae749486725807cc19f5d.png)'
- en: Test set accuracy. Image created by the author.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集准确率。图像由作者创建。
- en: The changing accuracy could be due to the changing effective batch size — note
    that we do not get exactly the same model depending on the training strategy.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率的变化可能是由于有效批次大小的变化——请注意，依据训练策略，我们不会得到完全相同的模型。
- en: In this experiment, there is a tradeoff between generalization accuracy and
    speed. Training on 8 GPUs was four times faster, but 11% less accurate than training
    on a single device.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次实验中，泛化准确性与速度之间存在权衡。使用8个GPU训练比单设备训练快了四倍，但准确性比单设备训练低了11%。
- en: A look at parallel training
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 平行训练概述
- en: 'In the training log file, you will see lines like these:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练日志文件中，你将看到像这样的行：
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The parallel training processes must be able to communicate with each other.
    The communication backend, here the NVIDIA Collective Communications Library (*nccl*),
    is automatically initialized depending on the platform.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 并行训练过程必须能够互相通信。这里的通信后端是NVIDIA集体通信库(*nccl*)，根据平台会自动初始化。
- en: We have requested two nodes with two GPUs each, for a total of four parallel
    processes. They are enumerated by their *global rank* (0–3). On each node, the
    *local rank* identifies the GPU that is used (0 or 1).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们请求了两个节点，每个节点有两个GPU，共四个并行进程。它们通过*全局排名*（0–3）进行枚举。在每个节点上，*本地排名*标识了使用的GPU（0或1）。
- en: When the model is large, the [*all-reduce* operation](https://en.wikipedia.org/wiki/Collective_operation#All-Reduce)
    can take a long time. Compute clusters connect the GPUs with efficient networks,
    but sending a full model replica out puts a strain on communication.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型很大时，[*all-reduce* 操作](https://en.wikipedia.org/wiki/Collective_operation#All-Reduce)可能会花费很长时间。计算集群通过高效的网络连接GPU，但发送整个模型副本会加重通信负担。
- en: Model parallelism
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型并行
- en: '[Lightning provides strategies](https://lightning.ai/docs/pytorch/stable/advanced/model_parallel.html)
    for model parallelism. Note that this should only be explored for very large models
    with billions of parameters, where the model parameters and gradients exceed GPU
    memory.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[Lightning 提供了模型并行的策略](https://lightning.ai/docs/pytorch/stable/advanced/model_parallel.html)。请注意，这仅应在具有数十亿参数的非常大模型中探索，其中模型参数和梯度超过了GPU内存。'
- en: Summary
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: Parallel training can speed up the training process and help you make the most
    of your computational resources.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 并行训练可以加速训练过程，帮助你充分利用计算资源。
- en: We distinguish between data parallelism, where replicas of the model see subsets
    of the data during training, and model parallelism, where the model itself is
    distributed across multiple devices.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们区分了数据并行，其中模型的副本在训练过程中看到数据的子集，以及模型并行，其中模型本身分布在多个设备上。
- en: With PyTorch Lightning, data parallel training is as simple as specifying the
    *distributed data parallelism* strategy and the number of nodes and devices (GPUs)
    per node.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PyTorch Lightning，数据并行训练就像指定*分布式数据并行*策略以及每个节点和设备（GPU）的数量一样简单。
- en: Data parallel training reduces the wall time and changes the effective batch
    size. Models trained on different numbers of devices may perform differently,
    and careful evaluation is recommended.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行训练减少了壁钟时间并改变了有效批次大小。使用不同数量设备训练的模型可能表现不同，因此建议进行仔细评估。
- en: Model parallel training should only be considered for very large models exceeding
    500M parameters.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 模型并行训练应该仅在非常大的模型（超过5亿参数）中考虑。
- en: 'The code to replicate the experiment can be found on my GitHub:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 重现实验的代码可以在我的GitHub上找到：
- en: '[](https://github.com/crlna16/pretrained-vision-transformer?source=post_page-----d7c22904b5a4--------------------------------)
    [## GitHub - crlna16/pretrained-vision-transformer: Pretrained Vision Transformer
    with PyTorch…'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/crlna16/pretrained-vision-transformer?source=post_page-----d7c22904b5a4--------------------------------)
    [## GitHub - crlna16/pretrained-vision-transformer: 使用 PyTorch 的预训练 Vision Transformer…'
- en: Pretrained Vision Transformer with PyTorch Lightning - crlna16/pretrained-vision-transformer
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 PyTorch Lightning 的预训练 Vision Transformer - crlna16/pretrained-vision-transformer
- en: github.com](https://github.com/crlna16/pretrained-vision-transformer?source=post_page-----d7c22904b5a4--------------------------------)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/crlna16/pretrained-vision-transformer?source=post_page-----d7c22904b5a4--------------------------------)'
- en: References
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Tal Ben-Nun *et al*, “Demystifying Parallel and Distributed Deep Learning:
    An In-Depth Concurrency Analysis”, 2018, [https://arxiv.org/abs/1802.09941](https://arxiv.org/abs/1802.09941)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tal Ben-Nun *等*，“揭秘并行和分布式深度学习：深入的并发分析”，2018，[https://arxiv.org/abs/1802.09941](https://arxiv.org/abs/1802.09941)
- en: 'OpenAI Country211 dataset: [source](https://github.com/openai/CLIP/blob/main/data/country211.md)
    (subset of [YFCC100M](https://dx.doi.org/10.1145/2812802)) and [license](https://multimediacommons.wordpress.com/yfcc100m-core-dataset/#yfcc100m)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI Country211 数据集：[source](https://github.com/openai/CLIP/blob/main/data/country211.md)（[YFCC100M](https://dx.doi.org/10.1145/2812802)
    的子集）和 [license](https://multimediacommons.wordpress.com/yfcc100m-core-dataset/#yfcc100m)
- en: 'Vision transformer model: Dosovitskiy et al, *An Image is Worth 16x16 Words:
    Transformers for Image Recognition at Scale*, ICLR, 2021\. [Paper](https://arxiv.org/pdf/2010.11929.pdf)
    and code'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vision Transformer 模型：Dosovitskiy 等，*一张图片值 16x16 个词：用于大规模图像识别的变换器*，ICLR，2021。
    [Paper](https://arxiv.org/pdf/2010.11929.pdf) 和代码
- en: 'PyTorch Lightning project for data-parallel training: [docs](https://lightning.ai/docs/pytorch/stable/accelerators/gpu_intermediate.html)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于数据并行训练的 PyTorch Lightning 项目：[docs](https://lightning.ai/docs/pytorch/stable/accelerators/gpu_intermediate.html)
- en: Will Falcon on distributed training [(TDS, 2019)](/9-tips-for-training-lightning-fast-neural-networks-in-pytorch-8e63a502f565)
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Will Falcon 讲解分布式训练 [(TDS, 2019)](/9-tips-for-training-lightning-fast-neural-networks-in-pytorch-8e63a502f565)
