- en: 'Long-form video representation learning (Part 3: Long-form egocentric video
    representation learning)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长时视频表示学习（第三部分：长时自我中心视频表示学习）
- en: 原文：[https://towardsdatascience.com/long-form-video-representation-learning-part-3-latest-and-greatest-in-long-form-video-1b6dee0f5f6e?source=collection_archive---------11-----------------------#2024-05-14](https://towardsdatascience.com/long-form-video-representation-learning-part-3-latest-and-greatest-in-long-form-video-1b6dee0f5f6e?source=collection_archive---------11-----------------------#2024-05-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/long-form-video-representation-learning-part-3-latest-and-greatest-in-long-form-video-1b6dee0f5f6e?source=collection_archive---------11-----------------------#2024-05-14](https://towardsdatascience.com/long-form-video-representation-learning-part-3-latest-and-greatest-in-long-form-video-1b6dee0f5f6e?source=collection_archive---------11-----------------------#2024-05-14)
- en: We explore novel video representation learning methods that are equipped with
    long-form reasoning capability. This is Part III providing a sneak peek into our
    latest and greatest explorations for “long-form” egocentric video representation
    learning. See [Part I](https://medium.com/@subarna.tripathi/long-form-video-representation-learning-part-1-video-as-graphs-c55b609d9100)
    on video as a graph and is [Part II](https://medium.com/@subarna.tripathi/long-form-video-representation-learning-part-2-video-as-sparse-transformers-29fbd0ed9e71)
    on sparse video-text transformers.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们探索了具备长时推理能力的新型视频表示学习方法。这是第三部分，提供了我们关于“长时”自我中心视频表示学习的最新研究成果的预览。请参见[第一部分](https://medium.com/@subarna.tripathi/long-form-video-representation-learning-part-1-video-as-graphs-c55b609d9100)，讲解视频作为图的内容，以及[第二部分](https://medium.com/@subarna.tripathi/long-form-video-representation-learning-part-2-video-as-sparse-transformers-29fbd0ed9e71)，介绍稀疏视频-文本转换器。
- en: '[](https://medium.com/@subarna.tripathi?source=post_page---byline--1b6dee0f5f6e--------------------------------)[![Subarna
    Tripathi](../Images/0a949764464eeef40a6d3ae0d183873f.png)](https://medium.com/@subarna.tripathi?source=post_page---byline--1b6dee0f5f6e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1b6dee0f5f6e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1b6dee0f5f6e--------------------------------)
    [Subarna Tripathi](https://medium.com/@subarna.tripathi?source=post_page---byline--1b6dee0f5f6e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@subarna.tripathi?source=post_page---byline--1b6dee0f5f6e--------------------------------)[![Subarna
    Tripathi](../Images/0a949764464eeef40a6d3ae0d183873f.png)](https://medium.com/@subarna.tripathi?source=post_page---byline--1b6dee0f5f6e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1b6dee0f5f6e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1b6dee0f5f6e--------------------------------)
    [Subarna Tripathi](https://medium.com/@subarna.tripathi?source=post_page---byline--1b6dee0f5f6e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1b6dee0f5f6e--------------------------------)
    ·8 min read·May 14, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1b6dee0f5f6e--------------------------------)
    ·8分钟阅读·2024年5月14日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: The first two blogs in this series described how different architectural motifs
    ranging from graph neural networks to sparse transformers addressed the challenges
    of “long-form” video representation learning. We showed how explicit graph based
    methods can aggregate 5-10X larger temporal context, but they were two-stage methods.
    Next, we explored how we can make memory and compute efficient end-to-end learnable
    models based on transformers and aggregate over 2X larger temporal context.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本系列的前两篇博客描述了从图神经网络到稀疏转换器等不同架构模式如何应对“长时”视频表示学习的挑战。我们展示了显式基于图的方法如何聚合5-10倍更大的时间上下文，但它们是两阶段的方法。接下来，我们探索了如何基于转换器设计出既高效又能端到端学习的模型，并能聚合2倍以上更大的时间上下文。
- en: In this blog, I’ll take you to our latest and greatest explorations, especially
    for egocentric video understanding. As you can imagine, an egocentric or first-person
    video (captured usually by head-mounted cameras) is most likely coming from an
    always-ON camera, meaning the videos are really really long, with a lot of irrelevant
    visual information, specially when the camera wearer move their heads. And, this
    happens a lot of times with head mounted cameras. A proper analysis of such first-person
    videos can enable a detailed understanding of how humans interact with the environment,
    how they manipulate objects, and, ultimately, what are their goals and intentions.
    Typical applications of egocentric vision systems require algorithms able to represent
    and process video over temporal spans that last in the order of minutes or hours.
    Examples of such applications are action anticipation, video summarization, and
    episodic memory retrieval.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客中，我将带你走进我们最新最伟大的探索，特别是关于自我中心视频理解的部分。正如你所能想象的，自我中心视频或第一人称视频（通常通过头戴式相机拍摄）很可能来自于始终开启的相机，这意味着这些视频通常非常非常长，并且包含大量无关的视觉信息，尤其是在相机佩戴者移动头部时。而且，这种情况在头戴式相机中经常发生。对这类第一人称视频的适当分析可以帮助我们详细了解人类如何与环境互动，如何操作物体，以及最终，他们的目标和意图是什么。自我中心视觉系统的典型应用需要能够表示和处理视频的算法，这些视频的时间跨度通常在几分钟或几个小时之间。例如，行动预测、视频总结和情节记忆检索等应用。
- en: 'Egocentric action scene graphs:'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自我中心行动场景图：
- en: '![](../Images/23d0558b28f8b7f83369cb2f3083eeb5.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23d0558b28f8b7f83369cb2f3083eeb5.png)'
- en: 'Figure 1: (Image by author) *Egocentric Action Scene Graphs are temporal dynamic
    graphs (G(t)) capturing the action verbs (nodes in blue), direct or active objects
    (nodes in green), and other objects (nodes in yellow) involved in the activity
    performed by a camera wearer (the orange CW node). Edges between nodes represent
    relationship between the verb and the objects or between object pairs. The graph
    evolves through time providing a long-from representation of the egocentric video
    (dashed lines). Objects of interaction are grounded with bounding boxes*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：（图片由作者提供）*自我中心行动场景图是时序动态图（G(t)），捕捉行动动词（蓝色节点）、直接或活跃物体（绿色节点）以及其他与相机佩戴者执行的活动相关的物体（黄色节点）。节点之间的边表示动词与物体之间的关系，或物体对之间的关系。该图随着时间发展，提供了自我中心视频的长时间段表示（虚线）。交互物体通过边界框进行定位*
- en: In this joint work with University of Catania, we present Egocentric Action
    Scene Graphs (EASGs), a new representation for long-form understanding of egocentric
    videos. EASGs extend standard manually-annotated representations of egocentric
    videos, such as verb-noun action labels, by providing a temporally evolving graph-based
    description of the actions performed by the camera wearer. The description also
    includes interacted objects, their relationships, and how actions unfold in time.
    Through a novel annotation procedure, we extend the Ego4D dataset adding manually
    labeled Egocentric Action Scene Graphs which offer a rich set of annotations for
    long-from egocentric video understanding.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在与卡塔尼亚大学的联合研究中，我们提出了自我中心行动场景图（EASGs），这是一种用于长时间段理解自我中心视频的新表示方法。EASGs通过提供一个基于图的时序描述，扩展了标准的手动注释自我中心视频表示，例如动词-名词行动标签，从而描述相机佩戴者执行的动作。该描述还包括交互的物体、它们之间的关系，以及动作如何随着时间展开。通过一种新的注释程序，我们扩展了Ego4D数据集，增加了手动标注的自我中心行动场景图，为长时间段的自我中心视频理解提供了丰富的注释集。
- en: 'EASGs provide annotations for a video clip in the form of a dynamic graph.
    We formalize an EASG as a time-varying directed graph G(t) = (V (t), E(t)), where
    V (t) is the set of nodes at time t and E(t) is the set of edges between such
    nodes (Figure 2). Each temporal realization of the graph G(t) corresponds to an
    egocentric action spanning over a set of three frames defined as in [[Ego4D](https://openaccess.thecvf.com/content/CVPR2022/papers/Grauman_Ego4D_Around_the_World_in_3000_Hours_of_Egocentric_Video_CVPR_2022_paper.pdf)]:
    the precondition (PRE), the point of no return (PNR) and the postcondition (POST)
    frames. The graph G(t) is hence effectively associated to three frames: F(t) =
    {PREₜ, PNRₜ, POSTₜ}, as shown in figure 1 below.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: EASG提供以动态图形式标注的视频片段。我们将EASG形式化为一个时间变化的有向图G(t) = (V(t), E(t))，其中V(t)是时间t时刻的节点集合，E(t)是这些节点之间的边的集合（见图2）。图G(t)的每一时刻表示一个跨越三帧的自我中心动作，这三帧定义如下[[Ego4D](https://openaccess.thecvf.com/content/CVPR2022/papers/Grauman_Ego4D_Around_the_World_in_3000_Hours_of_Egocentric_Video_CVPR_2022_paper.pdf)]：前置条件（PRE）、不可回头点（PNR）和后置条件（POST）帧。因此，图G(t)有效地与三帧关联：F(t)
    = {PREₜ, PNRₜ, POSTₜ}，如图1所示。
- en: 'Egocentric scene graph generation:'
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 以自我为中心的场景图生成：
- en: Figure 2 shows an example of an annotated graph in details.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图2展示了一个详细的标注图示例。
- en: '![](../Images/f86b56508dd81639cb80b238588b53f1.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f86b56508dd81639cb80b238588b53f1.png)'
- en: Image by author
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: We obtain an initial EASG leveraging existing annotations from Ego4D, with initialization
    and refinement procedure. e.g. we begin with adding the camera wearer node, verb
    node and and the default action edge from camera wearer node to the verb node.
    The annotation pipeline is shown in figure 3 below.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过利用来自Ego4D的现有标注，结合初始化和精炼过程，得到初始的EASG。例如，我们从添加摄像机佩戴者节点、动词节点以及从摄像机佩戴者节点到动词节点的默认动作边开始。标注流程如图3所示。
- en: '![](../Images/741dc3085e12cc53e6f1cffdc88c7b9d.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/741dc3085e12cc53e6f1cffdc88c7b9d.png)'
- en: image by author
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Next, we do the graph refinement via inputs from 3 annotators. The validation
    stage aggregates the data received from three annotators and ensures the quality
    of the final annotations as shown below.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过三名标注者的输入对图进行精炼。验证阶段会汇总三名标注者收到的数据，并确保最终标注的质量，如下所示。
- en: '![](../Images/85a017a1199a4556f35822bb4c5c2eaa.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/85a017a1199a4556f35822bb4c5c2eaa.png)'
- en: 'Figure 4 (Image by author): Examples of questions (with correct answers in
    red) asked to the annotators in the validation stage to resolve ambiguities between'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图4（作者提供图片）：在验证阶段，针对标注者提出的问题示例（正确答案用红色标出），以解决标注中的歧义。
- en: the labels provided in the annotation stage.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在标注阶段提供的标签。
- en: As it can be noted, the EASG dataset is unique in its labels. And, in the table
    below you can see how this new dataset compares with other video datasets with
    visual relations, in terms of labels and size.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如可以注意到的，EASG数据集在其标签上具有独特性。下表展示了该新数据集与其他具有视觉关系的视频数据集在标签和大小方面的对比。
- en: '![](../Images/81de1def11cca0f0c1ba817ee558221d.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81de1def11cca0f0c1ba817ee558221d.png)'
- en: 'Image by author: Comparison with existing video scene graph datasets. Our Ego4D-EASG
    dataset is the only one explicitly designed for long-form egocentric video understanding,
    featuring egocentric videos, dynamic graphs, an average sequence length of 3.1
    minutes and an average number of 28.3 graphs per sequence. *measured in object-relation-object
    triplets. **intransitive + transitive verb predicates'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片：与现有的视频场景图数据集进行比较。我们的Ego4D-EASG数据集是唯一明确设计用于长格式自我中心视频理解的数据集，具有自我中心视频、动态图、平均序列长度为3.1分钟，以及每个序列平均28.3个图。*以对象-关系-对象三元组为单位测量。**不及物+及物动词谓词
- en: The above video visually demonstrates one example EASG for annotation as it
    dynamically changes as the content of the video changes.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 上述视频直观地展示了一个标注用的EASG示例，它会随着视频内容的变化而动态变化。
- en: After the creation of this unique dataset, we will now describe different tasks
    that are evaluated on this dataset. The first set of tasks is about generating
    action scene graphs which stems from the image scene graph generation literature.
    In other words we aim to learn EASG representations in a supervised way and measure
    its performance in standard Recall metrics used in scene graph literature. We
    devise baselines and compare the EASG generation performance of different baselines
    on this dataset.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建了这个独特的数据集之后，我们将描述在该数据集上评估的不同任务。第一组任务是生成动作场景图，源自图像场景图生成文献。换句话说，我们的目标是以监督方式学习EASG表示，并测量其在场景图文献中常用的标准Recall指标上的表现。我们设计了基准，并比较了不同基准在该数据集上的EASG生成表现。
- en: '![](../Images/ea0522254893e4e567c963e15017e4a9.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea0522254893e4e567c963e15017e4a9.png)'
- en: (image by author) Baseline results for three EASG generation tasks (i.e. Edge
    Cls, SG Cls, and EASG Cls) in terms of Recall@K
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: （作者提供的图片）三项EASG生成任务（即Edge Cls，SG Cls和EASG Cls）在Recall@K上的基准结果
- en: '**Long-from understanding tasks with EASG:**'
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**使用EASG进行长篇理解任务：**'
- en: We show the potential of the EASG representation in the downstream tasks of
    action anticipation and activity summarization. Both tasks require to perform
    long-form reasoning over the egocentric video, processing long video sequences
    spanning over different time-steps. Following recent results showing the flexibility
    of Large Language Models (LLMs) as symbolic reasoning machines, we perform these
    experiments with LLMs accessed via the OpenAI API. The experiments aim to examine
    the expressive power of the EASG representation and its usefulness for downstream
    applications. We show that EASG offers an expressive way of modeling long-form
    activities, in comparison with the gold-standard verb-noun action encoding, extensively
    adopted in egocentric video community.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了EASG表示在动作预测和活动摘要下游任务中的潜力。这两项任务都需要对以自我为中心的视频进行长篇推理，处理跨越不同时间步骤的长视频序列。根据最近的结果，展示了大型语言模型（LLMs）作为符号推理机器的灵活性，我们通过OpenAI
    API进行这些实验。实验旨在检查EASG表示的表达能力及其对下游应用的有用性。我们表明，EASG提供了一种有效的方式来建模长篇活动，相比于广泛采用的金标准动词-名词动作编码，EASG在自我中心视频社区中的表现更具优势。
- en: '**Action anticipation with EASGs:**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用EASG进行动作预测：**'
- en: For the action anticipation task, we use the GPT3 text-davinci-003 model. We
    prompt the model to predict the future action from a sequence of length T ∈ {5,
    20}. We compare two types of representations — EASG and sequences of verb-noun
    pairs. Below table shows the results of this experiment.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于动作预测任务，我们使用GPT3的text-davinci-003模型。我们提示该模型从长度为T ∈ {5，20}的序列中预测未来的动作。我们比较了两种表示——EASG和动词-名词对序列。下表展示了此实验的结果。
- en: '![](../Images/4c45c4973e60d12c8a2fa4e038e9f73d.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c45c4973e60d12c8a2fa4e038e9f73d.png)'
- en: 'Image by author: Performance Comparison for the Action anticipation task'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片：动作预测任务的性能比较
- en: Even short EASG sequences (T =5) tend to outperform long V-N sequences (T =
    20), highlighting the higher representation power of EASG, when compared to standard
    verb-noun representations. EASG representations achieve the best results for long
    sequences (T = 20).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是较短的EASG序列（T = 5）也往往超过较长的V-N序列（T = 20），这突显了EASG在与标准动词-名词表示相比时更强的表示能力。EASG表示在长序列（T
    = 20）中取得了最佳结果。
- en: '**Long-form activity summarization with EASGs:**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用EASG进行长篇活动摘要：**'
- en: 'We select a subset of 147 Ego4D-EASG clips containing human-annotated summaries
    describing the activities performed within the clip in 1–2 sentences from Ego4D.
    We construct three types of input sequences: sequences of graphs S-EASG = [G(1),
    G(2), …, G(Tmax)], sequences of verb-noun pairs svn = [s-vn(1), s-vn(2), …, s-vn(Tmax)],
    and sequences of original Ego4D narrations, matched with the EASG sequence. This
    last input is reported for reference, as we expect summarization from narrations
    to bring the best performance, given the natural bias of language models towards
    this representation.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了147个Ego4D-EASG片段的子集，这些片段包含人类注释的摘要，描述了片段中执行的活动，并用1到2个句子总结。我们构建了三种类型的输入序列：图序列S-EASG
    = [G(1)，G(2)，…，G(Tmax)]，动词-名词对序列svn = [s-vn(1)，s-vn(2)，…，s-vn(Tmax)]，以及与EASG序列匹配的原始Ego4D叙述序列。最后一种输入用于参考，因为我们预计来自叙述的摘要将带来最佳的表现，考虑到语言模型对这种表示的自然偏好。
- en: Results reported in the below table indicate strong improvement in CIDEr score
    over the sequence of verb-noun inputs, showing that models which process EASG
    inputs capturing detailed object action relationships, will generate more specific,
    informative sentences that align well with reference descriptions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格中报告的结果表明，相较于动词-名词序列输入，CIDEr得分显著提高，表明处理EASG输入的模型能够捕捉详细的物体动作关系，从而生成更加具体、富有信息的句子，与参考描述高度契合。
- en: '![](../Images/0a1c6d4d80fc0fb45487d2a43b89d225.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a1c6d4d80fc0fb45487d2a43b89d225.png)'
- en: 'Image by author: Results of activity summarization with EASGs and verb-noun
    representations'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片：使用EASG和动词-名词表示进行活动总结的结果
- en: We believe that these contributions mark a step forward in long-form egocentric
    video understanding.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们相信，这些贡献标志着在长篇自我中心视频理解方面迈出了重要的一步。
- en: '**Highlights:**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**亮点：**'
- en: We introduce Egocentric Action Scene Graphs, a novel representation for long-form
    understanding of egocentric videos;
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们提出了自我中心动作场景图（Egocentric Action Scene Graphs），这是一种用于长篇自我中心视频理解的新型表示方法；
- en: We extend Ego4D with manually annotated EASG labels, which are gathered through
    a novel annotation procedure;
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通过一种新颖的注释程序，手动标注了EASG标签，并将其扩展到Ego4D数据集中；
- en: We propose a EASG generation baseline and provide initial baseline results;
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们提出了EASG生成基准，并提供了初步的基准结果；
- en: We present experiments that highlight the effectiveness of the EASG representation
    for long-form egocentric video understanding. We will release the dataset and
    the code to replicate data annotation and the
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们展示了实验，突出了EASG表示在长篇自我中心视频理解中的有效性。我们将发布数据集和代码，以便复制数据注释过程；
- en: experiments;
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验；
- en: We will present this work at **CVPR 2024**, next month.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将在下个月的**CVPR 2024**上展示这项工作。
- en: 'Paper: [https://arxiv.org/abs/2312.03391](https://arxiv.org/abs/2312.03391)
    and Code: [https://github.com/fpv-iplab/EASG](https://github.com/fpv-iplab/EASG)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 论文：[https://arxiv.org/abs/2312.03391](https://arxiv.org/abs/2312.03391) 和代码：[https://github.com/fpv-iplab/EASG](https://github.com/fpv-iplab/EASG)
- en: 'Temporal grounding for episodic tasks:'
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务的时间定位：
- en: In recent years, egocentric video-language pre-training (VLP) has been adopted
    significantly in academia and in industry. A line of works such as [EgoVLP](https://github.com/showlab/EgoVLP/blob/main/README.md),
    [EgoVLPv2](https://shramanpramanick.github.io/EgoVLPv2/) learn transferable spatial-temporal
    representations from large-scale video-text datasets. Recently, [LaViLa](https://facebookresearch.github.io/LaViLa/)
    showed that VLP can benefit from the dense narrations generated by Large Language
    Models (LLMs). However, all such methods do hit the memory and compute bottleneck
    while processing video sequences, each consisting of a small number of frames
    (e.g. 8 or 16 frame models), leading to limited temporal context aggregation capability.
    On the contrary, our model, called **LAVITI**, is equipped with long-form reasoning
    capability (**1,000** frames vs 16 frames) and is not limited to a small number
    of input frames.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，自我中心视频-语言预训练（VLP）在学术界和工业界得到了广泛应用。一些研究工作，如[EgoVLP](https://github.com/showlab/EgoVLP/blob/main/README.md)、[EgoVLPv2](https://shramanpramanick.github.io/EgoVLPv2/)，从大规模视频-文本数据集中学习可转移的时空表示。最近，[LaViLa](https://facebookresearch.github.io/LaViLa/)证明了VLP可以受益于大型语言模型（LLMs）生成的密集叙述。然而，所有这些方法在处理视频序列时都会遇到内存和计算瓶颈，每个序列包含少量帧（例如8帧或16帧的模型），导致有限的时间上下文聚合能力。相反，我们的模型，称为**LAVITI**，具备长篇推理能力（**1,000**帧对比16帧），并且不受限于少量输入帧。
- en: In this ongoing work, we devised a novel approach to learning language, video,
    and ***temporal representations*** in long-form videos via contrastive learning.
    Unlike existing methods, this new approach aims to align language, video, and
    temporal features by extracting meaningful moments in **untrimmed** videos by
    formulating it as a direct set prediction problem. LAVITI outperforms existing
    state-of-the-art methods by a significant margin on egocentric action recognition,
    yet is trainable on memory and compute-bound systems. Our method can be trained
    on the Ego4D dataset with only 8 NVIDIA RTX-3090 GPUs in a day.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项正在进行的工作中，我们设计了一种新颖的方法，通过对比学习学习语言、视频和***时间表示***，用于长篇视频的处理。与现有方法不同，这种新方法旨在通过提取未剪辑视频中的有意义时刻，将语言、视频和时间特征对齐，并将其表述为一个直接的集合预测问题。LAVITI在自我中心动作识别方面显著超过了现有的最先进方法，同时可以在内存和计算受限的系统上进行训练。我们的模型可以在Ego4D数据集上，仅用8个NVIDIA
    RTX-3090 GPU，训练一天。
- en: '![](../Images/cafb9f150d4a0d6617a9f5261cc8c744.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cafb9f150d4a0d6617a9f5261cc8c744.png)'
- en: 'Image by author: Performance on CharadesEgo. Our approach achieves significant
    gains in both zero-shot and fine-tuned settings. ZS and FT stand for zero-shot
    and finetuning, respectively.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自作者：在 CharadesEgo 上的表现。我们的方法在零-shot 和微调设置中均取得了显著的提升。ZS 和 FT 分别代表零-shot 和微调。
- en: As our model is capable of long-form video understanding with explicit temporal
    alignment, the Ego4D Natural Language Query (NLQ) task is a natural fit with the
    pre-training targets. We can directly predict intervals which are aligned with
    language query given a video; therefore, LAVITI can
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的模型能够进行具有显式时间对齐的长篇视频理解，Ego4D 自然语言查询（NLQ）任务与预训练目标非常契合。给定视频，我们可以直接预测与语言查询对齐的时间间隔；因此，LAVITI
    可以
- en: perform the NLQ task under the zero-shot setting (without modifications of the
    architecture and re-training on NLQ annotations).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在零-shot 设置下执行 NLQ 任务（无需修改架构或重新训练 NLQ 注释）。
- en: In the near future, we plan on assessing its potential to learn improved representations
    for episodic memory tasks including NLQ and Moment Query (MQ). To summarize, we
    are leveraging existing foundation models (essentially “short-term”) for creating
    “long-form” reasoning module aiming at 20X-50X larger context aggregation.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在不久的将来，我们计划评估其在学习改进的表征方面的潜力，尤其是在包括 NLQ 和 Moment Query (MQ) 在内的情节记忆任务中。总而言之，我们正在利用现有的基础模型（本质上是“短期”模型）来创建旨在进行
    20 倍到 50 倍更大上下文聚合的“长篇”推理模块。
- en: '**Highlights:**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**亮点：**'
- en: We devised exciting new ways for egocentric video understanding. Our contributions
    are manifold.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设计了令人兴奋的新方法来进行自我中心的视频理解。我们的贡献是多方面的。
- en: Pre-training objective aligns language, video, and *temporal features* jointly
    by extracting meaningful moments in **untrimmed** videos;
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练目标通过提取**未修剪**视频中的有意义时刻，共同对齐语言、视频和*时间特征*；
- en: formulating the video, language and temporal alignment as a direct set prediction
    problem;
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将视频、语言和时间对齐问题表述为一个直接的集合预测问题；
- en: enabling long-form reasoning over potentially ***thousands of frames*** of a
    video in a memory-compute efficient way;
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以内存计算高效的方式，支持对视频中可能涉及的***成千上万帧***进行长篇推理；
- en: demonstrating the efficacy of LAVITI by its superior performance on CharadesEgo
    action recognition;
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过其在 CharadesEgo 动作识别上的优越表现，展示了 LAVITI 的有效性；
- en: Enabling zero-shot natural language query (NLQ) task without needing to train
    additional subnetworks or NLQ annotations.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持零-shot 自然语言查询（NLQ）任务，而无需训练额外的子网络或 NLQ 注释。
- en: Watch out for more exciting results with this new paradigm of “long-form” video
    representation learning!
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 关注这个新范式的“长篇”视频表示学习带来的更多激动人心的结果！
