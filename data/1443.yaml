- en: Task-Aware RAG Strategies for When Sentence Similarity Fails
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务感知 RAG 策略：当句子相似性失效时的应对方法
- en: 原文：[https://towardsdatascience.com/task-aware-rag-strategies-for-when-sentence-similarity-fails-54c44690fee3?source=collection_archive---------2-----------------------#2024-06-10](https://towardsdatascience.com/task-aware-rag-strategies-for-when-sentence-similarity-fails-54c44690fee3?source=collection_archive---------2-----------------------#2024-06-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/task-aware-rag-strategies-for-when-sentence-similarity-fails-54c44690fee3?source=collection_archive---------2-----------------------#2024-06-10](https://towardsdatascience.com/task-aware-rag-strategies-for-when-sentence-similarity-fails-54c44690fee3?source=collection_archive---------2-----------------------#2024-06-10)
- en: Improving retrieval beyond semantic similarity
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 改善超越语义相似性的检索
- en: '[](https://medium.com/@aimichael?source=post_page---byline--54c44690fee3--------------------------------)[![Michael
    Ryaboy](../Images/783a6c1ed59277776003aaeab69f0b07.png)](https://medium.com/@aimichael?source=post_page---byline--54c44690fee3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--54c44690fee3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--54c44690fee3--------------------------------)
    [Michael Ryaboy](https://medium.com/@aimichael?source=post_page---byline--54c44690fee3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@aimichael?source=post_page---byline--54c44690fee3--------------------------------)[![Michael
    Ryaboy](../Images/783a6c1ed59277776003aaeab69f0b07.png)](https://medium.com/@aimichael?source=post_page---byline--54c44690fee3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--54c44690fee3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--54c44690fee3--------------------------------)
    [Michael Ryaboy](https://medium.com/@aimichael?source=post_page---byline--54c44690fee3--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--54c44690fee3--------------------------------)
    ·16 min read·Jun 10, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--54c44690fee3--------------------------------)
    ·16 分钟阅读·2024年6月10日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/29b772c537ea929ba301f21b378a2693.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29b772c537ea929ba301f21b378a2693.png)'
- en: Image by [Joshua Golde](https://unsplash.com/@joshgmit) on [Unsplash](https://unsplash.com/photos/white-stage-qIu77BsFdds)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Joshua Golde](https://unsplash.com/@joshgmit) 提供，来源于 [Unsplash](https://unsplash.com/photos/white-stage-qIu77BsFdds)
- en: 'Vector databases have revolutionized the way we search and retrieve information
    by allowing us to embed data and quickly search over it using the same embedding
    model, with only the query being embedded at inference time. However, despite
    their impressive capabilities, vector databases have a fundamental flaw: they
    treat queries and documents in the same way. This can lead to suboptimal results,
    especially when dealing with complex tasks like matchmaking, where queries and
    documents are inherently different.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库通过允许我们嵌入数据并使用相同的嵌入模型快速搜索，彻底改变了我们搜索和检索信息的方式，在推理时只有查询被嵌入。然而，尽管向量数据库具有令人印象深刻的能力，但它们也存在一个根本性缺陷：它们将查询和文档视为相同的。这可能导致次优的结果，尤其是在处理复杂任务如匹配时，因为查询和文档本质上是不同的。
- en: The challenge of Task-aware RAG (Retriever-augmented Generation) lies in its
    requirement to retrieve documents based not only on their semantic similarity
    but also on additional contextual instructions. This adds a layer of complexity
    to the retrieval process, as it must consider multiple dimensions of relevance.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 任务感知 RAG（检索增强生成）的挑战在于其不仅需要基于语义相似性来检索文档，还需要结合额外的上下文指令。这为检索过程增加了一层复杂性，因为它必须考虑多个相关性维度。
- en: 'Here are some examples of Task-Aware RAG problems:'
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这里有一些任务感知 RAG 问题的示例：
- en: '**1\. Matching Company Problem Statements to Job Candidates**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 将公司问题陈述与求职者匹配**'
- en: 'Query: “Find candidates with experience in scalable system design and a proven
    track record in optimizing large-scale databases, suitable for addressing our
    current challenge of enhancing data retrieval speeds by 30% within the existing
    infrastructure.”'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询：“寻找具有可扩展系统设计经验并且在优化大规模数据库方面有成功记录的候选人，适合应对我们当前的挑战：在现有基础设施上将数据检索速度提高30%。”
- en: 'Context: This query aims to directly connect the specific technical challenge
    of a company with potential job candidates who have relevant skills and experience.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文：该查询旨在直接将公司的具体技术挑战与具备相关技能和经验的潜在求职者联系起来。
- en: '**2\. Matching Pseudo-Domains to Startup Descriptions**'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 将伪域名与创业公司描述匹配**'
- en: 'Query: “Match a pseudo-domain for a startup that specializes in AI-driven,
    personalized learning platforms for high school students, emphasizing interactive
    and adaptive learning technologies.”'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询： “为一家专注于AI驱动的个性化学习平台的高中生创业公司匹配伪域名，强调互动式和自适应学习技术。”
- en: 'Context: Designed to find an appropriate, catchy pseudo-domain name that reflects
    the innovative and educational focus of the startup. A pseudo-domain name is a
    domain name based on a pseudo-word, which is a word that sound real but isn’t.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文：旨在找到一个适合且引人注目的伪域名，反映出创业公司的创新和教育重点。伪域名是基于伪词的域名，伪词是听起来真实但并非真实的词语。
- en: '**3\. Investor-Startup Matchmaking**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. 投资者与创业公司配对**'
- en: 'Query: “Identify investors interested in early-stage biotech startups, with
    a focus on personalized medicine and a history of supporting seed rounds in the
    healthcare sector.”'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询： “识别对早期生物技术创业公司感兴趣的投资者，重点关注个性化医学，并且有支持医疗行业种子轮投资的历史。”
- en: 'Context: This query seeks to match startups in the biotech field, particularly
    those working on personalized medicine, with investors who are not only interested
    in biotech but have also previously invested in similar stages and sectors.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文：该查询旨在匹配生物技术领域的创业公司，特别是那些从事个性化医学的公司，与那些不仅对生物技术感兴趣，而且曾在类似阶段和行业进行过投资的投资者。
- en: '**4\. Retrieving Specific Kinds of Documents**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**4\. 检索特定类型的文档**'
- en: 'Query: “Retrieve recent research papers and case studies that discuss the application
    of blockchain technology in securing digital voting systems, with a focus on solutions
    tested in the U.S. or European elections.”'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询： “检索最近的研究论文和案例研究，讨论区块链技术在保障数字投票系统中的应用，重点关注在美国或欧洲选举中测试的解决方案。”
- en: 'Context: Specifies the need for academic and practical insights on a particular
    use of blockchain, highlighting the importance of geographical relevance and recent
    applications'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文：明确指出需要对区块链某一特定应用提供学术和实践洞察，突出地理相关性和最近的应用案例
- en: The Challenge
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 挑战
- en: 'Let’s consider a scenario where a company is facing various problems, and we
    want to match these problems with the most relevant job candidates who have the
    skills and experience to address them. Here are some example problems:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个情景，其中一家公司面临各种问题，我们希望将这些问题与最相关的职位候选人匹配，这些候选人拥有解决这些问题所需的技能和经验。以下是一些示例问题：
- en: “High employee turnover is prompting a reassessment of core values and strategic
    objectives.”
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “高员工流动率正在促使对核心价值观和战略目标进行重新评估。”
- en: 2\. “Perceptions of opaque decision-making are affecting trust levels within
    the company.”
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2\. “决策不透明的印象正在影响公司内部的信任水平。”
- en: 3\. “Lack of engagement in remote training sessions signals a need for more
    dynamic content delivery.”
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 3\. “远程培训课程缺乏参与度表明需要更具动态性的内容呈现方式。”
- en: 'We can generate true positive and hard negative candidates for each problem
    using an LLM. For example:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用大语言模型生成每个问题的真实正样本和硬负样本候选。例如：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Even though the hard negatives may appear similar on the surface and could be
    closer in the embedding space to the query, the true positives are clearly better
    fits for addressing the specific problems.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管硬负样本表面上可能看起来相似，并且在嵌入空间中与查询更接近，但真实的正样本显然更适合解决特定问题。
- en: 'The Solution: Instruction-Tuned Embeddings, Reranking, and LLMs'
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案：指令调优嵌入、重新排序和大语言模型（LLMs）
- en: 'To tackle this challenge, we propose a multi-step approach that combines instruction-tuned
    embeddings, reranking, and LLMs:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个挑战，我们提出了一种多步骤的方法，结合了指令调优嵌入、重新排序和大语言模型（LLMs）：
- en: 1\. Instruction-Tuned Embeddings
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 指令调优嵌入
- en: Instruction-Tuned embeddings function like a bi-encoder, where both the query
    and document embeddings are processed separately and then their embeddings are
    compared. By providing additional instructions to each embedding, we can bring
    them to a new embedding space where they can be more effectively compared.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 指令调优嵌入类似于双编码器，在该模型中，查询和文档的嵌入分别处理，然后对它们的嵌入进行比较。通过向每个嵌入提供额外的指令，我们可以将它们带入一个新的嵌入空间，在这个空间中，它们能够更有效地进行比较。
- en: The key advantage of instruction-tuned embeddings is that they allow us to encode
    specific instructions or context into the embeddings themselves. This is particularly
    useful when dealing with complex tasks like job description-resume matchmaking,
    where the queries (job descriptions) and documents (resumes) have different structures
    and content.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 指令调优嵌入的关键优势在于，它们允许我们将特定的指令或上下文编码到嵌入本身。这在处理像职位描述与简历匹配这样的复杂任务时尤其有用，因为查询（职位描述）和文档（简历）的结构和内容是不同的。
- en: 'By prepending task-specific instructions to the queries and documents before
    embedding them, we can theoretically guide the embedding model to focus on the
    relevant aspects and capture the desired semantic relationships. For example:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在嵌入查询和文档之前添加特定任务的指令，我们理论上可以引导嵌入模型聚焦于相关方面并捕捉期望的语义关系。例如：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This instruction prompts the embedding model to represent the documents as job
    candidate achievements, making them more suitable for retrieval based on the given
    job description.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 该指令提示嵌入模型将文档表示为求职者成就，使其更适合根据给定的职位描述进行检索。
- en: 'Still, RAG systems are difficult to interpret without evals, so let’s write
    some code to check the accuracy of three different approaches:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，RAG系统在没有评估的情况下很难解读，因此让我们编写一些代码来检查三种不同方法的准确性：
- en: 1\. Naive Voyage AI instruction-tuned embeddings with no additional instructions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 初级Voyage AI指令调优嵌入，没有额外指令。
- en: 2\. Voyage AI instruction-tuned embeddings with additional context to the query
    and document.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. Voyage AI指令调优嵌入，包含额外的查询和文档上下文。
- en: 3\. Voyage AI non-instruction-tuned embeddings.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. Voyage AI非指令调优嵌入。
- en: We use Voyage AI embeddings because they are currently best-in-class, and at
    the time of this writing comfortably sitting at the top of the MTEB leaderboard.
    We are also able to use three different strategies with vectors of the same size,
    which will make comparing them easier. 1024 dimensions also happens to be much
    smaller than any embedding modals that come even close to performing as well.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Voyage AI嵌入，因为它们目前是业内最顶尖的，并且在本文撰写时，稳居MTEB排行榜的顶部。我们还能够使用三种不同的策略，且向量维度相同，这将使得它们之间的比较更加容易。1024维度也恰好比任何接近于表现得如此优秀的嵌入模型小得多。
- en: '![](../Images/9041170cc2c5481c9ba3b1c1f4fe87b6.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9041170cc2c5481c9ba3b1c1f4fe87b6.png)'
- en: MTEB leaderboard
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: MTEB排行榜
- en: In theory, we should see instruction-tuned embeddings perform better at this
    task than non-instruction-tuned embeddings, even if just because they are higher
    on the leaderboard. To check, we will first embed our data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，我们应该看到指令调优的嵌入在此任务上表现优于非指令调优的嵌入，即便只是因为它们在排行榜上排名较高。为了验证，我们将首先嵌入我们的数据。
- en: 'When we do this, we try prepending the string: “Represent the most relevant
    experience of a job candidate for retrieval: “ to our documents, which gives our
    embeddings a bit more context about our documents.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们这样做时，我们尝试在文档前添加字符串：“表示求职者最相关的经验以便检索：”，这为我们的嵌入提供了更多关于文档的上下文。
- en: If you want to follow along, check out this [colab link](https://colab.research.google.com/drive/1iPy1WtOEPHU5YqEzHdiSpvUFL4JElGVJ?usp=sharing).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想跟随操作，请查看这个[colab链接](https://colab.research.google.com/drive/1iPy1WtOEPHU5YqEzHdiSpvUFL4JElGVJ?usp=sharing)。
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We then insert our vectors into a vector database. We don’t strictly need one
    for this demo, but a vector database with metadata filtering capabilities will
    allow for cleaner code, and for eventually scaling this test up. We will be using
    KDB.AI, where I’m a Developer Advocate. However, any vector database with metadata
    filtering capabilities will work just fine.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将向量插入到向量数据库中。对于这个演示，我们严格来说不需要一个数据库，但具备元数据筛选功能的向量数据库将使代码更简洁，并最终能够扩展此测试。我们将使用KDB.AI，我是该平台的开发者倡导者。不过，任何具备元数据筛选功能的向量数据库都可以很好地工作。
- en: To get started with KDB.AI, go to [kdb.ai](https://kdb.ai) to fetch your endpoint
    and api key.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用KDB.AI，请访问[kdb.ai](https://kdb.ai)以获取你的端点和API密钥。
- en: Then, let’s instantiate the client and import some libraries.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，让我们实例化客户端并导入一些库。
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Connect to our session with our endpoint and api key.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 通过我们的端点和API密钥连接到我们的会话。
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Create our table:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 创建我们的表格：
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Insert the candidate achievements into our index, with an “embedding_type”
    metadata filter to separate our embeddings:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 将候选成就插入到我们的索引中，并使用“embedding_type”元数据过滤器来区分我们的嵌入：
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'And finally, evaluate the three methods above:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，评估上述三种方法：
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here are the results:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这是结果：
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The instruct model performed worse on this task!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 指令模型在这个任务上的表现更差！
- en: Our dataset is small enough that this isn’t a significantly large difference
    (under 35 high quality examples.)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集足够小，差异并不显著（不到35个高质量的示例）。
- en: Still, this shows that
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，这仍然表明
- en: a) instruct models alone are not enough to deal with this challenging task.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: a) 单独的指令模型不足以处理这个具有挑战性的任务。
- en: b) while instruct models can lead to good performance on similar tasks, it’s
    important to always run evals, because in this case I suspected they would do
    better, which wasn’t true
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: b) 虽然指令模型可以在类似任务上取得良好的表现，但始终进行评估是很重要的，因为在这个案例中，我曾怀疑它们会表现得更好，但事实并非如此。
- en: c) there are tasks for which instruct models perform worse
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: c) 有一些任务对于指令模型来说效果较差。
- en: 2\. Reranking
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2. 重新排序
- en: While instruct/regular embedding models can narrow down our candidates somewhat,
    we clearly need something more powerful that has a better understanding of the
    relationship between our documents.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然指令/常规嵌入模型可以在某种程度上缩小我们的候选范围，但显然我们需要更强大的模型，能够更好地理解文档之间的关系。
- en: After retrieving the initial results using instruction-tuned embeddings, we
    employ a cross-encoder (reranker) to further refine the rankings. The reranker
    considers the specific context and instructions, allowing for more accurate comparisons
    between the query and the retrieved documents.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用指令调优的嵌入检索到初步结果后，我们使用一个交叉编码器（重新排序器）进一步优化排名。重新排序器考虑了特定的上下文和指令，从而使查询和检索到的文档之间的比较更加准确。
- en: Reranking is crucial because it allows us to assess the relevance of the retrieved
    documents in a more nuanced way. Unlike the initial retrieval step, which relies
    solely on the similarity between the query and document embeddings, reranking
    takes into account the actual content of the query and documents.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 重新排序至关重要，因为它使我们能够以更细致的方式评估检索到的文档的相关性。与仅仅依赖查询和文档嵌入之间的相似性的初始检索步骤不同，重新排序会考虑查询和文档的实际内容。
- en: By jointly processing the query and each retrieved document, the reranker can
    capture fine-grained semantic relationships and determine the relevance scores
    more accurately. This is particularly important in scenarios where the initial
    retrieval may return documents that are similar on a surface level but not truly
    relevant to the specific query.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 通过联合处理查询和每个检索到的文档，重新排序器能够捕捉细粒度的语义关系，并更准确地确定相关性评分。这在初始检索可能返回表面上相似但实际上与特定查询无关的文档的场景中特别重要。
- en: Here’s an example of how we can perform reranking using the Cohere AI reranker
    (Voyage AI also has an excellent reranker, but when I wrote this article Cohere’s
    outperformed it. Since then they have come out with a new reranker that according
    to their internal benchmarks performs just as well or better.)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们如何使用Cohere AI重新排序器进行重新排序的一个示例（Voyage AI也有一个很棒的重新排序器，但在我写这篇文章时，Cohere的表现更好。此后，他们推出了一个新的重新排序器，根据他们的内部基准测试，它的表现与之前一样，甚至更好。）
- en: First, let’s define our reranking function. We can also use Cohere’s Python
    client, but I chose to use the REST API because it seemed to run faster.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义我们的重新排序函数。我们也可以使用Cohere的Python客户端，但我选择使用REST API，因为它似乎运行得更快。
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now, let’s evaluate our reranker. Let’s also see if adding additional context
    about our task improves performance.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们评估我们的重新排序器。让我们还看看是否通过添加关于我们任务的额外上下文能提升性能。
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, here are our results:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这里是我们的结果：
- en: '[PRE12]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: By adding additional context about our task, it might be possible to improve
    reranking performance. We also see that our reranker performed better than all
    embedding models, even without additional context, so it should definitely be
    added to the pipeline. Still, our performance is lacking at under 50% accuracy
    (we retrieved the top result first for less than 50% of queries), there must be
    a way to do much better!
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 通过添加关于我们任务的额外上下文，可能会提升重新排序的性能。我们还看到我们的重新排序器表现优于所有嵌入模型，即使没有额外的上下文，因此它应该被毫无疑问地加入到流水线中。不过，我们的性能还不理想，准确率低于50%（我们在不到50%的查询中首先检索到最佳结果），一定有方法可以做得更好！
- en: The best part of rerankers are that they work out of the box, but we can use
    our golden dataset (our examples with hard negatives) to [fine-tune](https://docs.cohere.com/docs/rerank-starting-the-training)
    our reranker to make it much more accurate. This might improve our reranking performance
    by a lot, but it might not generalize to different kinds of queries, and fine-tuning
    a reranker every time our inputs change can be frustrating.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 重新排序器的最佳部分是它们开箱即用，但我们可以使用我们的黄金数据集（包含困难负例的示例）来[微调](https://docs.cohere.com/docs/rerank-starting-the-training)我们的重新排序器，使其更为准确。这可能大大提高我们的重新排序性能，但可能无法推广到不同类型的查询，并且每次输入变化时重新调整重新排序器可能会很令人沮丧。
- en: 3\. LLMs
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. LLM
- en: In cases where ambiguity persists even after reranking, LLMs can be leveraged
    to analyze the retrieved results and provide additional context or generate targeted
    summaries.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在重新排序后仍然存在歧义的情况下，可以利用 LLM 来分析检索到的结果并提供额外的上下文或生成定向总结。
- en: LLMs, such as GPT-4, have the ability to understand and generate human-like
    text based on the given context. By feeding the retrieved documents and the query
    to an LLM, we can obtain more nuanced insights and generate tailored responses.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: LLM，如 GPT-4，能够根据给定的上下文理解和生成类人文本。通过将检索到的文档和查询输入 LLM，我们可以获得更细致的洞察力并生成量身定制的回答。
- en: For example, we can use an LLM to summarize the most relevant aspects of the
    retrieved documents in relation to the query, highlight the key qualifications
    or experiences of the job candidates, or even generate personalized feedback or
    recommendations based on the matchmaking results.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以使用 LLM 总结与查询相关的检索文档中最相关的方面，突出显示求职候选人的关键资质或经验，甚至根据匹配结果生成个性化的反馈或推荐。
- en: This is great because it can be done after the results are passed to the user,
    but what if we want to rerank dozens or hundreds of results? Our LLM’s context
    will be exceeded, and it will take too long to get our output. This doesn’t mean
    you shouldn’t use an LLM to evaluate the results and pass additional context to
    the user, but it does mean we need a better final-step reranking option.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这很好，因为它可以在结果传递给用户后进行，但如果我们想重新排序几十个或几百个结果呢？我们的 LLM 上下文将会超出限制，获取输出的时间也会变得过长。这并不意味着你不应该使用
    LLM 来评估结果并向用户传递额外的上下文，但这确实意味着我们需要一个更好的最终步骤重新排序选项。
- en: 'Let’s imagine we have a pipeline that looks like this:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个如下所示的管道：
- en: '![](../Images/3d69f7309d972dfb1bdf22c6976abfb5.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d69f7309d972dfb1bdf22c6976abfb5.png)'
- en: Simple Pipeline
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 简单管道
- en: This pipeline can narrow down millions of possible documents to just a few dozen.
    But the last few dozen is extremely important, we might be passing only three
    or four documents to an LLM! If we are displaying a job candidate to a user, it’s
    very important that the first candidate shown is a much better fit than the fifth.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这个管道可以将数百万个可能的文档缩小到只有几十个。但是，最后的这几十个非常重要，我们可能只会传递三四个文档给 LLM！如果我们将一个求职候选人展示给用户，那么展示的第一个候选人比第五个更合适是非常重要的。
- en: 'We know that LLMs are excellent rerankers, and there are a few reasons for
    that:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道 LLM 是优秀的重新排序器，这有几个原因：
- en: '**LLMs are list aware.** This means they can see other candidates and compare
    them, which is additional information that can be used. Imagine you (a human)
    were asked to rate a candidate from 1–10\. Would showing you all other candidates
    help? Of course!'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**LLM 是了解列表的。** 这意味着它们可以看到其他候选项并进行比较，这是可以利用的附加信息。假设你（一个人类）被要求对一个候选人进行1到10分的评分，展示所有其他候选人会有帮助吗？当然有！'
- en: '**LLMs are really smart.** LLMs understand the task they are given, and based
    on this can very effectively understand whether a candidate is a good fit, regardless
    of simple semantic similarity.'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**LLM 真是太聪明了。** LLM 理解它们所接到的任务，并且基于此可以非常有效地判断某个候选人是否合适，无论是简单的语义相似度如何。'
- en: We can exploit the second reason with a perplexity based classifier. Perplexity
    is a metric which estimates how much an LLM is ‘confused’ by a particular output.
    In other words, we can ask an LLM to classify our candidate into ‘a very good
    fit’ or ‘not a very good fit’. Based on the certainty with which it places our
    candidate into ‘a very good fit’ (the perplexity of this categorization,) we can
    effectively rank our candidates.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过基于困惑度的分类器来利用第二个原因。困惑度是一个度量，用来估计 LLM 对某个输出的‘困惑’程度。换句话说，我们可以让 LLM 将我们的候选项分类为‘非常合适’或‘不太合适’。根据它将候选项归为‘非常合适’的确定性（即该分类的困惑度），我们可以有效地对候选项进行排序。
- en: There are all kinds of optimizations that can be made, but on a good GPU (which
    is highly recommended for this part) we can rerank 50 candidates in about the
    same time that cohere can rerank 1 thousand. However, we can parallelize this
    calculation on multiple GPUs to speed this up and scale to reranking thousands
    of candidates.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 有各种各样的优化可以进行，但在一个好的GPU上（强烈推荐使用GPU进行这一部分），我们可以在大约同样的时间内重新排序50个候选项，而cohere则能重新排序1000个。然而，我们可以在多个GPU上并行化此计算，从而加速这一过程，并扩展到重新排序数千个候选项。
- en: First, let’s install and import [lmppl](https://github.com/asahi417/lmppl),
    a library that let’s us evaluate the perplexity of certain LLM completions. We
    will also create a scorer, which is a large T5 model (anything larger runs too
    slowly, and smaller performs much worse.) If you can achieve similar results with
    a decoder model, please let me know, as that would make additional performance
    gains much easier (decoders are getting better and cheaper much more quickly than
    encoder-decoder models.)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们安装并导入[lmppl](https://github.com/asahi417/lmppl)，一个可以帮助我们评估某些LLM完成度困惑度的库。我们还将创建一个评分器，这是一个大型T5模型（任何更大的模型运行得太慢，更小的模型表现较差）。如果你能通过解码器模型实现类似的结果，请告诉我，因为那样会更容易带来额外的性能提升（解码器的性能提升和成本下降速度远快于编码器-解码器模型）。
- en: '[PRE13]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now, let’s create our evaluation function. This can be turned into a general
    function for any reranking task, or you can change the classes to see if that
    improves performance. This example seems to work well. We cache responses so that
    running the same values is faster, but this isn’t too necessary on a GPU.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建评估函数。这可以转化为任何重新排序任务的一般函数，或者你可以更改类别，看看是否能提高性能。这个例子似乎效果不错。我们缓存响应，以便重新运行相同的值时更快，但在GPU上这不是特别必要。
- en: '[PRE14]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, let’s rerank and evaluate:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进行重新排序和评估：
- en: '[PRE15]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'And our result:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 以及我们的结果：
- en: '[PRE16]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This is much better than our rerankers, and required no fine-tuning! Not only
    that, but this is much more flexible towards any task, and easier to get performance
    gains just by modifying classes and prompt engineering. The drawback is that this
    architecture is unoptimized, it’s difficult to deploy (I recommend [modal](http://modal.com).com
    for serverless deployment on multiple GPUs, or to deploy a GPU on a VPS.)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这比我们的重新排序器要好得多，而且不需要任何微调！不仅如此，它对任何任务都具有更大的灵活性，并且只需通过修改类别和提示工程就能更容易地获得性能提升。缺点是这种架构尚未优化，部署起来比较困难（我推荐使用[modal](http://modal.com)进行多GPU的无服务器部署，或者在VPS上部署GPU）。
- en: 'With this neural task aware reranker in our toolbox, we can create a more robust
    reranking pipeline:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个神经任务感知的重新排序器，我们的工具箱就更强大了，我们可以创建一个更为稳健的重新排序管道：
- en: '![](../Images/1e06164e97d6d7f44554a6e3f5ecb7b4.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e06164e97d6d7f44554a6e3f5ecb7b4.png)'
- en: Robust Multi-Stage Reranking Pipeline
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 强大的多阶段重新排序管道
- en: '**Conclusion**'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**结论**'
- en: 'Enhancing document retrieval for complex matchmaking tasks requires a multi-faceted
    approach that leverages the strengths of different AI techniques:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 增强文档检索对于复杂的匹配任务需要一种多方面的方法，利用不同AI技术的优势：
- en: 1\. **Instruction-tuned embeddings** provide a foundation by encoding task-specific
    instructions to guide the model in capturing relevant aspects of queries and documents.
    However, evaluations are crucial to validate their performance.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. **指令调优嵌入**通过编码特定任务的指令，为模型提供基础，以指导其捕捉查询和文档中的相关方面。然而，评估是验证其性能的关键。
- en: 2. **Reranking** refines the retrieved results by deeply analyzing content relevance.
    It can benefit from additional context about the task at hand.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 2. **重新排序**通过深度分析内容相关性来优化检索结果。它可以受益于任务背景的额外信息。
- en: 3\. **LLM-based classifiers** serve as a powerful final step, enabling nuanced
    reranking of the top candidates to surface the most pertinent results in an order
    optimized for the end user.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. **基于LLM的分类器**作为强大的最终步骤，使得对顶级候选项进行细致的重新排序，从而以优化的顺序呈现最相关的结果，满足最终用户需求。
- en: By thoughtfully orchestrating instruction-tuned embeddings, rerankers, and LLMs,
    we can construct robust AI pipelines that excel at challenges like matching job
    candidates to role requirements. Meticulous prompt engineering, top-performing
    models, and the inherent capabilities of LLMs allow for better Task-Aware RAG
    pipelines — in this case delivering outstanding outcomes in aligning people with
    ideal opportunities. Embracing this multi-pronged methodology empowers us to build
    retrieval systems that just retrieving semantically similar documents, but truly
    intelligent and finding documents that fulfill our unique needs.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 通过精心设计的指令调优嵌入、重排器和大型语言模型（LLMs），我们可以构建强大的AI管道，擅长解决诸如将求职者与职位要求匹配等挑战。细致的提示工程、顶尖的模型以及LLMs固有的能力，使得我们能够构建更高效的任务感知型RAG管道——在这种情况下，能够在将人们与理想机会对接方面取得卓越的成果。采用这种多角度的方法使我们能够构建不仅仅是检索语义相似文档，而是能够真正理解需求并找到符合我们独特需求的智能文档的检索系统。
- en: Connect with me on [LinkedIn](https://www.linkedin.com/in/michael-ryaboy-software-engineer/)
    for more AI Engineering tips.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在[LinkedIn](https://www.linkedin.com/in/michael-ryaboy-software-engineer/)上与我联系，获取更多AI工程技巧。
