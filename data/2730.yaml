- en: Reranking Using Huggingface Transformers for Optimizing Retrieval in RAG Pipelines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Huggingface Transformers进行RAG管道中检索优化的重排序
- en: 原文：[https://towardsdatascience.com/reranking-using-huggingface-transformers-for-optimizing-retrieval-in-rag-pipelines-fbfc6288c91f?source=collection_archive---------5-----------------------#2024-11-08](https://towardsdatascience.com/reranking-using-huggingface-transformers-for-optimizing-retrieval-in-rag-pipelines-fbfc6288c91f?source=collection_archive---------5-----------------------#2024-11-08)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/reranking-using-huggingface-transformers-for-optimizing-retrieval-in-rag-pipelines-fbfc6288c91f?source=collection_archive---------5-----------------------#2024-11-08](https://towardsdatascience.com/reranking-using-huggingface-transformers-for-optimizing-retrieval-in-rag-pipelines-fbfc6288c91f?source=collection_archive---------5-----------------------#2024-11-08)
- en: Understanding when reranking makes a difference
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解重排序何时能产生显著的影响
- en: '[](https://medium.com/@daniel-klitzke?source=post_page---byline--fbfc6288c91f--------------------------------)[![Daniel
    Klitzke](../Images/4471634e1a0f0546402d582dcc36c7c4.png)](https://medium.com/@daniel-klitzke?source=post_page---byline--fbfc6288c91f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fbfc6288c91f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fbfc6288c91f--------------------------------)
    [Daniel Klitzke](https://medium.com/@daniel-klitzke?source=post_page---byline--fbfc6288c91f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@daniel-klitzke?source=post_page---byline--fbfc6288c91f--------------------------------)[![Daniel
    Klitzke](../Images/4471634e1a0f0546402d582dcc36c7c4.png)](https://medium.com/@daniel-klitzke?source=post_page---byline--fbfc6288c91f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fbfc6288c91f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fbfc6288c91f--------------------------------)
    [Daniel Klitzke](https://medium.com/@daniel-klitzke?source=post_page---byline--fbfc6288c91f--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fbfc6288c91f--------------------------------)
    ·8 min read·Nov 8, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fbfc6288c91f--------------------------------)
    ·阅读时间：8分钟·2024年11月8日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/359bc87027b4ad46eb508e67dcf91b20.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/359bc87027b4ad46eb508e67dcf91b20.png)'
- en: Visualization of the reranking results for the user query “What is rigid motion?”.
    Original ranks on the left, new ranks on the right. (image create by author)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化重排序结果，针对用户查询“什么是刚性运动？”原始排序在左侧，新排序在右侧。（图片由作者制作）
- en: 'In this article I will show you how you can use the [**Huggingface Transformers**](https://github.com/huggingface/transformers)
    and [**Sentence Transformers**](https://github.com/UKPLab/sentence-transformers)
    libraries to boost you RAG pipelines using **reranking** models. Concretely we
    will do the following:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将展示如何使用[**Huggingface Transformers**](https://github.com/huggingface/transformers)和[**Sentence
    Transformers**](https://github.com/UKPLab/sentence-transformers)库，通过使用**重排序**模型来提升你的RAG管道。具体来说，我们将进行以下操作：
- en: Establish a **baseline** with a simple vanilla RAG pipeline.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用简单的原始RAG管道建立一个**基准**。
- en: Integrate a simple **reranking model** using the Huggingface Transformers library.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Huggingface Transformers库集成一个简单的**重排序模型**。
- en: Evaluate in which cases the **reranking model** is significantly improving context
    quality to gain a better understanding on the benefits.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估在什么情况下**重排序模型**显著提高了上下文质量，以便更好地理解其好处。
- en: For all of this, I will link to the corresponding code on [Github](https://github.com/Renumics/reranking-blogpost/blob/main/reranker_test.ipynb).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我将提供指向[Github](https://github.com/Renumics/reranking-blogpost/blob/main/reranker_test.ipynb)的相应代码链接。
- en: What is Reranking?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是重排序？
- en: 'Before we dive right into our evaluation I want to say few words on **what
    rerankers are**. Rerankers are usually applied as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入评估之前，我想简要介绍一下**什么是重排序器**。重排序器通常按照以下方式应用：
- en: A simple embedding-based retrieval approach is used to retrieve an **initial
    set of candidates** in the retrieval step of a RAG pipeline.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在RAG管道的检索步骤中，使用简单的基于嵌入的检索方法来获取**初始候选集**。
- en: A Reranker is used to **reorder the results** to provide a new result order
    that betters suits the user queries.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Reranker用于**重新排序结果**，以提供更适合用户查询的新结果顺序。
- en: But why should the reranker model yield something different than my already
    quite powerful embedding model, and why do I not leverage the semantic understanding
    of a reranker in an earlier stage you may ask yourself? This is quite multi-faceted
    but some key points are that e.g. the bge-reranker we use here is inherently **processing
    queries and documents together** in a cross-encoding approach and can thus explicitely
    model query-document interactions. Another major difference is that the reranking
    model is **trained in a supervised manner** on predicting **relevance scores**
    that are obtained through human annotation. What that means in practice will also
    be shown in the evaluation section later-on.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，为什么重新排序模型的结果会与我已经相当强大的嵌入模型不同呢？为什么我不在早期阶段利用重新排序模型的语义理解呢？这个问题很复杂，但一些关键点是，例如我们这里使用的bge-重新排序模型本质上是**同时处理查询和文档**的交叉编码方法，因此可以显式地建模查询-文档交互。另一个主要区别是，重新排序模型是**以监督方式训练**的，目标是预测**相关性分数**，这些分数是通过人工标注获得的。这在实际中的意义将在后续的评估部分中展示。
- en: Our Baseline
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们的基线
- en: 'For our baseline we choose the simplest possible RAG pipeline possible and
    focus solely on the retrieval part. Concretely, we:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的基线，我们选择最简单的RAG流水线，并专注于检索部分。具体来说，我们：
- en: Choose one large PDF document. I went for my Master’s Thesis, but you can choose
    what ever you like.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个大型PDF文档。我选择了我的硕士论文，但你也可以选择任何你喜欢的文档。
- en: Extract the text from the PDF and split it into equal chunks of about 10 sentences
    each.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从PDF中提取文本，并将其拆分为大约10个句子一组的均等块。
- en: Create embedding for our chunks and insert them in a vector database, in this
    case LanceDB.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为我们的文本块创建嵌入，并将其插入向量数据库，这里使用的是LanceDB。
- en: '*For details, about this part, check our the notebook on* [*Github*](https://github.com/Renumics/reranking-blogpost/blob/main/reranker_test.ipynb)*.*'
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*有关这一部分的详细信息，请查看我们的[Github](https://github.com/Renumics/reranking-blogpost/blob/main/reranker_test.ipynb)笔记本。*'
- en: 'After following this, a simple **semantic search** would be possible in two
    lines of code, namely:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些步骤后，简单的**语义搜索**就可以通过两行代码实现，具体如下：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here query would be the query provided by the user, e.g., the question “What
    is shape completion about?”. Limit, in this case, is the number of results to
    retrieve. In a normal RAG pipeline, the retrieved results would now just be directly
    be provided as **context to the LLM** that will synthesize the answer. In many
    cases, this is also perfectly valid, however for this post we want to explore
    the benefits of reranking.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的查询将是用户提供的查询，例如问题“什么是形状完成？”此时的限制是要检索的结果数量。在正常的RAG流水线中，检索到的结果将直接作为**上下文提供给LLM**，后者将综合生成答案。在许多情况下，这也是完全有效的，但在本文中，我们希望探索重新排序的好处。
- en: Implementing Reranking
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现重新排序
- en: 'With libraries such as [Huggingface Transformers](https://github.com/huggingface/transformers),
    using **reranker models** is a piece of cake. To use reranking to improve our
    “RAG pipeline” we extend our approach as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 使用像[Huggingface Transformers](https://github.com/huggingface/transformers)这样的库，使用**重新排序模型**非常简单。为了使用重新排序来改进我们的“RAG
    流水线”，我们扩展了我们的方法，如下所示：
- en: As previously, simply retrieve an **initial number of results** through a standard
    embedding model. However we increase the count of the results from 10 to around
    50.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前所述，通过标准的嵌入模型简单地检索**初始结果数量**。但是我们将结果的数量从10增加到大约50。
- en: After retrieving this larger number of initial sources, we apply a reranker
    model to **reorder the sources**. This is done by computing relevance scores for
    each query-source pair.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在检索到这个较大数量的初始源之后，我们应用一个重新排序模型来**重新排序这些源**。这是通过计算每个查询-源对的相关性分数来完成的。
- en: For **answer generation**, we then would normally use the new top x results.
    (In our case we use the top 10)
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于**答案生成**，我们通常会使用新的前x个结果。（在我们的案例中，我们使用前10个）
- en: 'In code this is also looking fairly simple and can be implemented in few lines
    of code:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码看起来也非常简单，可以通过几行代码实现：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Again, for seeing the full code for context check [Github](https://github.com/Renumics/reranking-blogpost/blob/main/reranker_test.ipynb)
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 再次，为了查看完整的代码以了解上下文，请查看[Github](https://github.com/Renumics/reranking-blogpost/blob/main/reranker_test.ipynb)
- en: As you can see, the main mechanism is simply to provide the model with **pairs
    of query and potentially relevant text**. It outputs a **relevance score** which
    we then can use to reorder our result list. But is this worth it? In which cases
    is it worth the extra inference time?
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，主要的机制是简单地为模型提供**查询和潜在相关文本的对**。它输出一个**相关性评分**，然后我们可以用它来重新排序结果列表。但是这值得吗？在什么情况下值得额外的推理时间？
- en: Evaluating The Reranker
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估重新排序器
- en: 'For evaluating our system we need to define some test queries. In my case I
    chose to use the following question categories:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们的系统，我们需要定义一些测试查询。在我的案例中，我选择了以下问题类别：
- en: '**Factoid Questions** such as “What is rigid motion?”'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**事实性问题**，例如“什么是刚体运动？”'
- en: Those should usually have one specific source in the document and are worded
    such that they could probably even found by text search.
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些通常应该在文档中有一个特定的来源，并且表述方式应该使得它们可以通过文本搜索轻松找到。
- en: '**Paraphrased Factoid Questions** such as “What is the mechanism in the architecture
    of some point cloud classification methods that is making them invariant to the
    order of the points?”'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**改写的事实性问题**，比如“某些点云分类方法的架构中，是什么机制使它们对点的顺序不变？”'
- en: As you can see, those are less specific in mentioning certain terms and require
    e.g. recognizing the relation of point cloud classification and the PointNet architecture.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如你所见，这些问题在提到某些术语时较为笼统，例如它们需要识别点云分类与PointNet架构之间的关系。
- en: '**Multi Source Questions** such as “How does the Co-Fusion approach work, compared
    to the approach presented in the thesis. What are similarities and differences?”'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**多源问题**，例如“Co-Fusion方法与论文中提出的方法相比是如何工作的？它们的相似之处和不同之处是什么？”'
- en: Those Questions need the retrieval of multiple source that should either be
    listed or be compared with each other.
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些问题需要检索多个来源，这些来源应该列出或彼此比较。
- en: '**Questions for Summaries or Table** such as “”What were the networks and parameter
    sizes used for hand segmentation experiments?”'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**摘要或表格问题**，例如“手部分割实验中使用的网络和参数大小是多少？”'
- en: Those questions target summaries in text and table form, such as a comparison
    table for model results. They are here to test wether rerankers recognize better
    that it can be useful to retrieve a summarization part in the document.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些问题针对的是文本和表格形式的摘要，例如用于模型结果比较的表格。它们的目的是测试重新排序器是否能更好地识别出检索文档中的摘要部分是否有用。
- en: 'As I was quite lazy I only defined 5 questions per category to get a rough
    impression and evaluated the retrieved context with and without reranking. The
    criteria I chose for evaluation were for example:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我比较懒，所以每个类别只定义了5个问题，以便获得一个粗略的印象，并评估了带有和不带有重新排序的检索上下文。我选择的评估标准包括：
- en: Did the reranking **add important information** to the context.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新排序是否**增加了重要信息**到上下文中。
- en: Did the reranking **reduce redundancy** to the context.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新排序是否**减少了上下文的冗余**。
- en: Did the reranking give the most relevant result a higher position in the list
    (**better prioritization**).
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新排序是否将最相关的结果放在列表的更高位置（**更好的优先级排序**）。
- en: …
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: …
- en: So what about the results?
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 那结果怎么样呢？
- en: '![](../Images/8ddc309a63790552d1924690bfe2ebbb.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8ddc309a63790552d1924690bfe2ebbb.png)'
- en: Overview of mean average rank change and initially neglected results (that were
    not in the top 10). (image create by author)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 平均排名变化概览以及最初被忽略的结果（那些不在前10名中的结果）。(图片由作者制作)
- en: Even in the overview, we can see, that there is a **significant difference between
    the categories** of questions, specifically there seems to be a lot of reranking
    going on for the multi_source_question category. When we look closer on the distributions
    of the metrics this is additionally confirmed.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在概述中，我们也可以看到，不同类别的**问题之间有显著差异**，特别是对于多源问题类别，似乎有很多重新排序的操作。当我们更仔细地查看度量的分布时，这一点得到了进一步确认。
- en: '![](../Images/d2257ce34fbe71765343dc27f0fd6766.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2257ce34fbe71765343dc27f0fd6766.png)'
- en: Distribution of neglected results metric by question category. (image create
    by author)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 被忽略的结果按问题类别的分布。(图片由作者制作)
- en: Specifically for 3 of our 5 questions in this category nearly all results in
    the final top 10 end up there through the reranking step. Now it is about finding
    out why that is the case. We therefore look at the two queries that are most significantly
    (positively) influenced by the reranking.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是对于这个类别中的5个问题中的3个，几乎所有进入最终前10名的结果都通过重新排序步骤得到了提升。现在的问题是找出为什么会这样。因此，我们关注两个最受重新排序（积极）影响的查询。
- en: 'Question1: “How does the Co-Fusion approach work, compare to the approach presented
    in the thesis. What are similarities and differences?”'
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 问题1：“Co-Fusion方法如何运作，与论文中提出的方法有何比较？有哪些相似性和差异？”
- en: '![](../Images/978aba73199c4ab60565c09617db49f5.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/978aba73199c4ab60565c09617db49f5.png)'
- en: Reranking result for the top 10 sources and their former positions. (image create
    by author)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 前10个来源的重新排序结果及其之前的位置。（图像由作者创建）
- en: 'The first impression here is that the reranker for this query definitely had
    two major effects. It prioritized the chunk from position 6 as the top result.
    Also, it pulled several really low-ranking results into the top 10\. When inspecting
    these chunks further we see the following:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的第一印象是，针对这个查询的重新排序器确实产生了两个主要效果。它将第6位的段落优先排为最相关的结果。此外，它还将一些低排名的结果提取到前10名中。当我们进一步检查这些段落时，我们发现了以下情况：
- en: The reranker managed to bring up a chunk that is highly related and describes
    SLAM approaches **as opposed to** the approach in the thesis.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新排序器成功提取出一个高度相关的段落，描述了SLAM方法**与**论文中的方法的不同之处。
- en: The reranker also managed to include a chunk that **mentions Co-Fusion** as
    one example for a SLAM approach that can deal with dynamic objects and includes
    discussion about the limitations.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新排序器还成功地包含了一个段落，该段落**提到了Co-Fusion**，作为一种能够处理动态物体的SLAM方法，并且包括了关于局限性的讨论。
- en: 'In general, the main pattern that emerges here is, that the reranker is able
    to **capture nuances in the tone of the speech**. Concretely formulations such
    as “SLAM approaches are closely related to the method presented in the thesis,
    however” paired with potential sparse mentions of Co-Fusion will be ranked way
    higher than by using a standard embedding model. That probably is because an Embedding
    model does most likely **not capture** that Co-Fusion is a SLAM approach and the
    predominant pattern in the text is general Information about SLAM. So, the reranker
    can give us two things here:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，这里出现的主要模式是，重新排序器能够**捕捉到语气中的细微差别**。具体来说，像“SLAM方法与论文中提出的方法密切相关，但……”这种表述，配合对Co-Fusion的稀疏提及，将比使用标准嵌入模型时排名更高。这可能是因为嵌入模型很可能**无法捕捉到**Co-Fusion是一个SLAM方法，而文本中的主导模式是关于SLAM的通用信息。因此，重新排序器可以为我们提供两件事情：
- en: '**Focusing on details** in the respective chunk rather than going for the average
    semantic content.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**专注于细节**而不是关注平均语义内容。'
- en: '**Focusing more on the user intent** to compare some method with the thesis’
    approach.'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**更加关注用户意图**，以比较某些方法与论文中的方法。'
- en: 'Question 2: “Provide a summary of the fulfilment of the objectives set out
    in the introduction based on the results of each experiment”'
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 问题2：“根据每个实验的结果，提供关于引言中设定目标的完成情况的总结。”
- en: '![](../Images/736859d43cabb618a80cb8236fb1bcec.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/736859d43cabb618a80cb8236fb1bcec.png)'
- en: Reranking result for the top 10 sources and their former positions. (image create
    by author)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 前10个来源的重新排序结果及其之前的位置。（图像由作者创建）
- en: 'Also, here we realize that a lot of low-ranking sources are pulled into the
    top 10 sources through the reranking step. So let’s investigate why this is the
    case once more:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还意识到，许多低排名的来源通过重新排序步骤被拉入了前10名。所以让我们再调查一下为什么会这样：
- en: The reranker again managed to capture **nuanced intent** of the question and
    reranks e.g. a chunk that contains the formulation “it was thus suscpected… ”
    as highly relevant, which it truly is because what follows is then describing
    wether the assumptions were valid and if the approach could make use of that.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新排序器再次成功捕捉到了问题的**细微意图**，并将包含“it was thus suspected...”这一表述的段落重新排序为高度相关的内容，这确实是因为后续内容描述了假设是否有效，以及该方法是否能够利用这些假设。
- en: The reranker gives as a lot of cryptically formulated experimental results that
    include also a bunch of tabular overviews on results of the ML-trainings, potentially
    **understanding the summarizing character** of these sections.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新排序器为我们提供了许多表述模糊的实验结果，这些结果还包括了一些关于机器学习训练结果的表格概览，可能是**理解这些部分的总结性特征**。
- en: Conclusion
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Implementing reranking is not a hard task with packages such as Huggingface
    Transformers providing **easy to use interfaces** to integrate them into your
    RAG pipeline and the major RAG frameworks like [llama-index](https://github.com/run-llama/llama_index)
    and [langchain](https://github.com/langchain-ai/langchain) supporting them out
    of the box. Also, there are **API-based rerankers** such as the one from [Cohere](https://cohere.com/rerank)
    you could use in your application.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 实现重新排序并不是一项难事，像 Huggingface Transformers 这样的包提供了 **易于使用的接口**，可以将它们集成到你的 RAG
    管道中，而主要的 RAG 框架如 [llama-index](https://github.com/run-llama/llama_index) 和 [langchain](https://github.com/langchain-ai/langchain)
    已经开箱即用地支持它们。此外，还有基于 **API 的重新排序器**，例如 [Cohere](https://cohere.com/rerank) 提供的，你可以在你的应用中使用它。
- en: 'From our evaluation we also see, that rerankers are most useful for things
    such as:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的评估中，我们还发现，重新排序器对于以下任务特别有用：
- en: '**Capturing nuanced semantics** hidden in a chunk with either different or
    cryptic content. E.g., a single mention of a method that is only once related
    to a concept within the chunk (SLAM and Co-Fusion)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**捕捉隐藏在一个内容块中的细微语义**，这些语义可能是不同的或加密的。例如，在内容块中仅提及一次与某个概念相关的方法（如 SLAM 和 Co-Fusion）。'
- en: '**Capturing user intent**, e.g. comparing some approach to the thesis approach.
    The reranker can then focus on formulations that imply that there is a comparison
    going on instead of the other semantics.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**捕捉用户意图**，例如，将某种方法与论文中的方法进行比较。然后，重新排序器可以专注于那些暗示有比较正在进行的表达，而不是其他语义。'
- en: I’m sure there are a lot more cases, but for this data and our test questions
    these were the dominant patterns and I feel they outline clearly what a supervisedly
    trained reranker can add over using only an an embedding model.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我相信还有更多的应用场景，但对于这些数据和我们的测试问题，这些是最主要的模式，我认为它们清晰地展示了经过监督训练的重新排序器相比仅使用嵌入模型所能提供的优势。
