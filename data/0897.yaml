- en: Feature Engineering with Microsoft Fabric and PySpark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Microsoft Fabric 和 PySpark 进行特征工程
- en: 原文：[https://towardsdatascience.com/feature-engineering-with-microsoft-fabric-and-pyspark-16d458018744?source=collection_archive---------9-----------------------#2024-04-08](https://towardsdatascience.com/feature-engineering-with-microsoft-fabric-and-pyspark-16d458018744?source=collection_archive---------9-----------------------#2024-04-08)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/feature-engineering-with-microsoft-fabric-and-pyspark-16d458018744?source=collection_archive---------9-----------------------#2024-04-08](https://towardsdatascience.com/feature-engineering-with-microsoft-fabric-and-pyspark-16d458018744?source=collection_archive---------9-----------------------#2024-04-08)
- en: Fabric Madness part 2
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Fabric 疯狂系列第二部分
- en: '[](https://medium.com/@roger_noble?source=post_page---byline--16d458018744--------------------------------)[![Roger
    Noble](../Images/869b5b0f237f24b119ca6c41c2e31162.png)](https://medium.com/@roger_noble?source=post_page---byline--16d458018744--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--16d458018744--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--16d458018744--------------------------------)
    [Roger Noble](https://medium.com/@roger_noble?source=post_page---byline--16d458018744--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@roger_noble?source=post_page---byline--16d458018744--------------------------------)[![Roger
    Noble](../Images/869b5b0f237f24b119ca6c41c2e31162.png)](https://medium.com/@roger_noble?source=post_page---byline--16d458018744--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--16d458018744--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--16d458018744--------------------------------)
    [Roger Noble](https://medium.com/@roger_noble?source=post_page---byline--16d458018744--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--16d458018744--------------------------------)
    ·12 min read·Apr 8, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--16d458018744--------------------------------)
    ·12分钟阅读·2024年4月8日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/e76cf994c16355962435cec1d006594b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e76cf994c16355962435cec1d006594b.png)'
- en: Image by author and ChatGPT. “Design an illustration, focusing on a basketball
    player in action, this time the theme is on using pyspark to generate features
    for machine leaning models in a graphic novel style” prompt. ChatGPT, 4, OpenAI,
    4 April. 2024\. [https://chat.openai.com.](https://chat.openai.com./)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者和 ChatGPT 提供。“设计一幅插图，重点描绘一位正在行动的篮球运动员，这次的主题是使用 PySpark 为机器学习模型生成特征，采用图像小说风格。”提示语：ChatGPT，4，OpenAI，2024年4月4日。[https://chat.openai.com.](https://chat.openai.com./)
- en: '*A Huge thanks to* [*Martim Chaves*](https://medium.com/@mgrc99) *who co-authored
    this post and developed the example scripts.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*特别感谢* [*Martim Chaves*](https://medium.com/@mgrc99) *，他与我共同撰写了这篇文章并开发了示例脚本。*'
- en: In our [previous post](https://medium.com/towards-data-science/fabric-madness-96b84dc5f241)
    we took a high level view of how to train a machine learning model in [Microsoft
    Fabric](https://www.microsoft.com/en-us/microsoft-fabric). In this post we wanted
    to dive deeper into the process of feature engineering.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们 [之前的文章](https://medium.com/towards-data-science/fabric-madness-96b84dc5f241)
    中，我们从高层次的角度介绍了如何在 [Microsoft Fabric](https://www.microsoft.com/en-us/microsoft-fabric)
    中训练机器学习模型。在这篇文章中，我们希望更深入地探讨特征工程的过程。
- en: Feature engineering is a crucial part of the development lifecycle for any Machine
    Learning (ML) systems. It is a step in the development cycle where raw data is
    processed to better represent its underlying structure and provide additional
    information that enhance our ML models. Feature engineering is both an art and
    a science. Even though there are specific steps that we can take to create good
    features, sometimes, it is only through experimentation that good results are
    achieved. Good features are crucial in guaranteeing a good system performance.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程是任何机器学习（ML）系统开发生命周期中的关键部分。它是开发周期中的一个步骤，旨在处理原始数据，以更好地代表其潜在结构，并提供额外的信息，从而增强我们的机器学习模型。特征工程既是一门艺术，也是一门科学。尽管我们可以采取特定的步骤来创建良好的特征，但有时只有通过实验，才能获得好的结果。良好的特征对于保证系统性能至关重要。
- en: As datasets grow exponentially, traditional feature engineering may struggle
    with the size of very large datasets. This is where PySpark can help — as it is
    a scalable and efficient processing platform for massive datasets. A great thing
    about Fabric is that it makes using PySpark easy!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据集的指数增长，传统的特征工程可能在处理非常大的数据集时遇到困难。这时，PySpark 就能派上用场——它是一个可扩展且高效的处理平台，专为大规模数据集设计。Fabric
    的一大优势是，它使得使用 PySpark 变得简单！
- en: 'In this post, we’ll be going over:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将讨论：
- en: How does PySpark Work?
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark 是如何工作的？
- en: Basics of PySpark
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark 基础
- en: Feature Engineering in Action
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程实践
- en: By the end of this post, hopefully you’ll feel comfortable carrying out feature
    engineering with PySpark in Fabric. Let’s get started!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 希望在本文结束时，你能够自如地在 Fabric 中使用 PySpark 进行特征工程。让我们开始吧！
- en: How does PySpark work?
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark 是如何工作的？
- en: Spark is a distributed computing system that allows for the processing of large
    datasets with speed and efficiency across a cluster of machines. It is built around
    the concept of a Resilient Distributed Dataset (RDD), which is a fault-tolerant
    collection of elements that can be operated on in parallel. RDDs are the fundamental
    data structure of Spark, and they allow for the distribution of data across a
    cluster of machines.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 是一个分布式计算系统，允许在机器集群上高效且快速地处理大规模数据集。它围绕弹性分布式数据集（RDD）概念构建，RDD 是一个容错的数据集合，能够并行处理。RDD
    是 Spark 的基础数据结构，它们允许将数据分布到机器集群中。
- en: PySpark is the Python API for Spark. It allows for the creation of Spark DataFrames,
    which are similar to Pandas DataFrames, but with the added benefit of being distributed
    across a cluster of machines. PySpark DataFrames are the core data structure in
    PySpark, and they allow for the manipulation of large datasets in a distributed
    manner.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 是 Spark 的 Python API。它允许创建 Spark DataFrame，类似于 Pandas DataFrame，但具有分布式跨机器集群的优势。PySpark
    DataFrame 是 PySpark 的核心数据结构，它们允许以分布式方式操作大规模数据集。
- en: At the core of PySpark is the `SparkSession` object, which is what fundamentally
    interacts with Spark. This `SparkSession` is what allows for the creation of DataFrames,
    and other functionalities. Note that, when running a Notebook in Fabric, a `SparkSession`
    is automatically created for you, so you don't have to worry about that.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 的核心是 `SparkSession` 对象，它是与 Spark 交互的基础。这个 `SparkSession` 允许创建 DataFrame
    和其他功能。请注意，在 Fabric 中运行 Notebook 时，会自动为你创建一个 `SparkSession`，因此你无需担心这个问题。
- en: Having a rough idea of how PySpark works, let’s get to the basics.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 了解了 PySpark 的大致工作原理后，我们来看看基础知识。
- en: Basics of PySpark
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark 基础
- en: Although Spark DataFrames may remind us of Pandas DataFrames due to their similarities,
    the syntax when using PySpark can be a bit different. In this section, we’ll go
    over some of the basics of PySpark, such as reading data, combining DataFrames,
    selecting columns, grouping data, joining DataFrames, and using functions.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Spark DataFrame 由于其相似性可能会让我们联想到 Pandas DataFrame，但使用 PySpark 时的语法可能会有所不同。在本节中，我们将介绍一些
    PySpark 的基础知识，如读取数据、合并 DataFrame、选择列、分组数据、连接 DataFrame 和使用函数等。
- en: The Data
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据
- en: The data we are looking at is from the 2024 US college basketball tournaments,
    which was obtained from the on-going *March Machine Learning Mania 2024* Kaggle
    competition, the details of which can be found [here](https://www.kaggle.com/competitions/march-machine-learning-mania-2024/overview),
    and is licensed under CC BY 4.0 [1]
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所查看的数据来自 2024 年美国大学篮球锦标赛，这些数据是从正在进行的*Kaggle 2024 年 3 月机器学习狂潮*竞赛中获取的，详情请见[此处](https://www.kaggle.com/competitions/march-machine-learning-mania-2024/overview)，并且根据
    CC BY 4.0 许可协议[1]发布。
- en: Reading data
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读取数据
- en: As mentioned in the [previous post](https://medium.com/towards-data-science/fabric-madness-96b84dc5f241)
    of this series, the first step is usually to create a Lakehouse and upload some
    data. Then, when creating a Notebook, we can attach it to the created Lakehouse,
    and we’ll have access to the data stored there.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在本系列的[上一篇文章](https://medium.com/towards-data-science/fabric-madness-96b84dc5f241)中提到的，第一步通常是创建一个湖仓并上传一些数据。然后，在创建
    Notebook 时，我们可以将其附加到创建的湖仓上，这样我们就可以访问存储在那里的数据。
- en: 'PySpark Dataframes can read various data formats, such as CSV, JSON, Parquet,
    and others. Our data is stored in CSV format, so we’ll be using that, like in
    the following code snippet:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark DataFrame 可以读取各种数据格式，如 CSV、JSON、Parquet 等。我们的数据存储为 CSV 格式，因此我们将使用该格式，如以下代码片段所示：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this code snippet, we’re reading the detailed results data set of the final
    women’s basketball college tournament matches. Note that the `"header"` option
    being true means that the names of the columns will be derived from the first
    row of the CSV file. The `inferSchema` option tells Spark to guess the data types
    of the columns - otherwise they would all be read as strings. `.cache()` is used
    to keep the DataFrame in memory.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码片段中，我们正在读取最终女子篮球大学锦标赛比赛的详细结果数据集。请注意，`"header"` 选项为 true 时意味着列名将从 CSV 文件的第一行推导出来。`inferSchema`
    选项告诉 Spark 自动推测列的数据类型——否则它们将全部作为字符串读取。`.cache()` 用于将 DataFrame 保持在内存中。
- en: If you’re coming from Pandas, you may be wondering what the equivalent of `df.head()`
    is for PySpark - it's `df.show(5)`. The default for `.show()` is the top 20 rows,
    hence the need to specifically select 5.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你来自 Pandas，你可能会好奇 PySpark 中 `df.head()` 的等效函数是什么 —— 它是 `df.show(5)`。`.show()`
    的默认值是显示前 20 行，因此需要特别指定 5 行。
- en: Combining DataFrames
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合并 DataFrame
- en: 'Combining DataFrames can be done in multiple ways. The first we will look at
    is a union, where the columns are the same for both DataFrames:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 合并 DataFrame 可以通过多种方式进行。我们首先查看的是 union，要求两个 DataFrame 的列名相同：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, `unionByName` joins the two DataFrames by matching the names of the columns.
    Since both the women's and the men's *detailed match results* have the same columns,
    this is a good approach. Alternatively, there's also `union`, which combines two
    DataFrames, matching column positions.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`unionByName` 通过匹配列名将两个 DataFrame 连接起来。由于女子和男子的 *详细比赛结果* 具有相同的列，这是一种很好的方法。另一种方法是使用
    `union`，它通过匹配列的位置合并两个 DataFrame。
- en: Selecting Columns
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择列
- en: Selecting columns from a DataFrame in PySpark can be done using the `.select()`
    method. We just have to indicate the name or names of the columns that are relevant
    as a parameter.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PySpark 中，可以使用 `.select()` 方法选择 DataFrame 中的列。我们只需将相关列的名称作为参数进行指定。
- en: 'Here’s the output for `w_scores.show(5)`:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 `w_scores.show(5)` 的输出：
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here''s the output for `w_scores.show(5)`:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 `w_scores.show(5)` 的输出：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The columns can also be renamed when being selected using the `.alias()` method:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择列时，还可以使用 `.alias()` 方法重命名列：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Grouping Data
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分组数据
- en: 'Grouping allows us to carry out certain operations for the groups that exist
    within the data and is usually combined with a aggregation functions. We can use
    `.groupBy()` for this:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 分组允许我们对数据中的各个组执行某些操作，通常与聚合函数结合使用。我们可以使用 `.groupBy()` 来实现这一点：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this example, we are grouping by `"TeamID"`, meaning we're considering the
    groups of rows that have a distinct value for `"TeamID"`. For each of those groups,
    we're calculating the average of the `"Score"`. This way, we get the average score
    for each team.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们按 `"TeamID"` 进行分组，这意味着我们考虑的是具有不同 `"TeamID"` 值的行组。对于每个组，我们计算 `"Score"`
    的平均值。通过这种方式，我们可以获得每支队伍的平均得分。
- en: 'Here’s the output of `winners_average_scores.show(5)`, showing the average
    score of each team:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 `winners_average_scores.show(5)` 的输出，显示了每支队伍的平均分数：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Joining Data
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连接数据
- en: Joining two DataFrames can be done using the `.join()` method. Joining is essentially
    extending the DataFrame by adding the columns of one DataFrame to another.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 `.join()` 方法连接两个 DataFrame。连接本质上是通过将一个 DataFrame 的列添加到另一个 DataFrame 来扩展
    DataFrame。
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In this example, both `stats_df` and `matches_df` were using `Season` and `TeamID`
    as unique identifiers for each row. Besides `Season` and `TeamID`, `stats_df`
    has other columns, such as statistics for each team during each season, whereas
    `matches_df` has information about the matches, such as date and location. This
    operation allows us to add those interesting statistics to the matches information!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，`stats_df` 和 `matches_df` 都使用 `Season` 和 `TeamID` 作为每一行的唯一标识符。除了 `Season`
    和 `TeamID` 外，`stats_df` 还有其他列，比如每支队伍在每个赛季中的统计数据，而 `matches_df` 则包含关于比赛的信息，如日期和地点。此操作使我们能够将这些有趣的统计数据添加到比赛信息中！
- en: Functions
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 函数
- en: There are several functions that PySpark provides that help us transform DataFrames.
    You can find the full list [here](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 提供了多个函数帮助我们转换 DataFrame。你可以在 [这里](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)
    找到完整的函数列表。
- en: 'Here’s an example of a simple function:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单函数的示例：
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the code snippet above, a `"HighScore"` column is created when the score
    is higher than 80\. For each row in the `"Score"` column (indicated by the `.col()`
    function), the value `"Yes"` is chosen for the `"HighScore"` column if the `"Score"`
    value is larger than 80, determined by the `.when()` function. `.otherwise()`,
    the value chosen is `"No"`.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码片段中，当分数高于 80 时，会创建一个 `"HighScore"` 列。对于 `"Score"` 列中的每一行（由 `.col()` 函数指示），如果
    `"Score"` 的值大于 80，则通过 `.when()` 函数选择 `"HighScore"` 列的值为 `"Yes"`，否则通过 `.otherwise()`
    函数选择值为 `"No"`。
- en: Feature Engineering in Action
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程实战
- en: Now that we have a basic understanding of PySpark and how it can be used, let’s
    go over how the regular season statistics features were created. These features
    were then used as inputs into our machine learning model to try to predict the
    outcome of the final tournament games.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对 PySpark 有了基本的了解，并知道如何使用它，让我们来看看常规赛统计特征是如何创建的。这些特征随后被用作机器学习模型的输入，以尝试预测最终锦标赛比赛的结果。
- en: The starting point was a DataFrame, `regular_data`, that contained match by
    match statistics for the *regular seasons*, which is the United States College
    Basketball Season that happens from November to March each year.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 起点是一个名为 `regular_data` 的数据框，包含了*常规赛*的逐场比赛统计数据，这是指每年从11月到3月的美国大学篮球赛季。
- en: Each row in this DataFrame contained the season, the day the match was held,
    the ID of team 1, the ID of team 2, and other information such as the location
    of the match. Importantly, it also contained statistics for *each team* for that
    *specific match*, such as `"T1_FGM"`, meaning the Field Goals Made (FGM) for team
    1, or `"T2_OR"`, meaning the Offensive Rebounds (OR) of team 2.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据框中的每一行包含了赛季、比赛日期、队伍1的ID、队伍2的ID以及其他信息，如比赛的地点。重要的是，它还包含了*每支队伍*在*特定比赛*中的统计数据，例如
    `"T1_FGM"`，表示队伍1的投篮命中数 (FGM)，或者 `"T2_OR"`，表示队伍2的进攻篮板 (OR)。
- en: The first step was selecting which columns would be used. These were columns
    that strictly contained in-game statistics.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是选择哪些列将被使用。这些列严格包含了比赛中的统计数据。
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If you’re interested, here’s what each statistic’s code means:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你感兴趣，以下是每个统计数据代码的含义：
- en: 'FGM: Field Goals Made'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'FGM: 投篮命中数'
- en: 'FGA: Field Goals Attempted'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'FGA: 投篮出手数'
- en: 'FGM3: Field Goals Made from the 3-point-line'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'FGM3: 三分球命中数'
- en: 'FGA3: Field Goals Attempted for 3-point-line goals'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'FGA3: 三分球出手数'
- en: 'OR: Offensive Rebounds. A rebounds is when the ball rebounds from the board
    when a goal is attempted, not getting in the net. If the team that *attempted*
    the goal gets possession of the ball, it’s called an “Offensive” rebound. Otherwise,
    it’s called a “Defensive” Rebound.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OR: 进攻篮板。篮板是指当一次投篮尝试未能进网时，球从篮板反弹出来。如果进行投篮尝试的队伍重新获得球权，这称为“进攻篮板”。否则，称为“防守篮板”。'
- en: 'DR: Defensive Rebounds'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'DR: 防守篮板'
- en: 'Ast: Assist, a pass that led directly to a goal'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ast: 助攻，指直接导致进球的传球'
- en: 'Stl: Steal, when the possession of the ball is stolen'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Stl: 抢断，指球权被抢走'
- en: 'PF: Personal Foul, when a player makes a foul'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'PF: 个人犯规，指球员犯规时'
- en: From there, a dictionary of *aggregation expressions* was created. Basically,
    for each column name in the previous list of columns, a function was stored that
    would calculate the mean of the column, and rename it, by adding a suffix, `"mean"`.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 从那里开始，创建了一个*聚合表达式*字典。基本上，对于前面列出的每个列名，都会存储一个计算该列均值的函数，并通过添加后缀 `"mean"` 来重命名。
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Then, the data was grouped by `"Season"` and `"T1_TeamID"`, and the aggregation
    functions of the previously created dictionary were used as the argument for `.agg()`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，数据按 `"Season"` 和 `"T1_TeamID"` 分组，并使用先前创建的字典中的聚合函数作为 `.agg()` 的参数。
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note that the grouping was done by season and the **ID of team 1** — this means
    that `"T2_FGAmean"`, for example, will actually be the mean of the Field Goals
    Attempted made by the **opponents** of T1, not necessarily of a specific team.
    So, we actually need to rename the columns that are something like `"T2_FGAmean"`
    to something like `"T1_opponent_FGAmean"`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，分组是按照赛季和**队伍1的ID**进行的——这意味着，例如，`"T2_FGAmean"` 实际上会是 T1 对手的投篮出手数的均值，而不一定是某支特定队伍的投篮出手均值。所以，我们实际上需要将像
    `"T2_FGAmean"` 这样的列重命名为类似 `"T1_opponent_FGAmean"` 的名称。
- en: '[PRE12]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: At this point, it’s important to mention that the `regular_data` DataFrame actually
    has **two** rows per each match that occurred. This is so that both teams can
    be "T1" and "T2", for each match. This little "trick" is what makes these statistics
    useful.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，必须提到的是，`regular_data` 数据框实际上每场比赛都有**两**行数据。这是为了让每场比赛的两支队伍都可以被标记为“T1”和“T2”。这个小“技巧”使得这些统计数据变得有用。
- en: Note that we “only” have the statistics for “T1”. We “need” the statistics for
    “T2” as well — “need” in quotations because there are no new statistics being
    calculated. We just need the same data, but with the columns having different
    names, so that for a match with “T1” and “T2”, we have statistics for both T1
    and T2\. So, we created a mirror DataFrame, where, instead of “T1&mldr;mean” and
    “T1_opponent_&mldr;mean”, we have “T2&mldr;mean” and “T2_opponent_&mldr;mean”.
    This is important because, later on, when we’re joining these regular season statistics
    to tournament matches, we’ll be able to have statistics for both team 1 **and**
    team 2.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们“只有”关于“T1”的统计数据。我们“需要”关于“T2”的统计数据——因为没有计算新的统计数据，所以“需要”加上引号。我们只需要相同的数据，但是列的名称不同，这样对于“T1”和“T2”的匹配，我们就能得到T1和T2的统计数据。因此，我们创建了一个镜像的DataFrame，在这个DataFrame中，原来的“T1&mldr;mean”和“T1_opponent_&mldr;mean”被替换为“T2&mldr;mean”和“T2_opponent_&mldr;mean”。这是非常重要的，因为稍后，当我们将这些常规赛统计数据与锦标赛比赛数据合并时，我们就能够得到T1和T2的统计数据。
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now, there are two DataFrames, with season statistics for “both” T1 and T2\.
    Since the final DataFrame will contain the “Season”, the “T1TeamID” and the “T2TeamID”,
    we can join these newly created features with a join!
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有两个DataFrame，分别包含了“T1”和“T2”的赛季统计数据。由于最终的DataFrame将包含“赛季”、“T1TeamID”和“T2TeamID”，我们可以通过合并操作将这些新创建的特征合并起来！
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Elo Ratings
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Elo评分
- en: First created by [Arpad Elo](https://en.wikipedia.org/wiki/Elo_rating_system),
    Elo is a rating system for zero-sum games (games where one player wins and the
    other loses), like basketball. With the Elo rating system, each team has an Elo
    rating, a value that generally conveys the team’s quality. At first, every team
    has the same Elo, and whenever they win, their Elo increases, and when they lose,
    their Elo decreases. A key characteristic of this system is that this value increases
    more with a win against a strong opponent than with a win against a weak opponent.
    Thus, it can be a very useful feature to have!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Elo最早由[阿尔帕德·厄洛](https://en.wikipedia.org/wiki/Elo_rating_system)创立，是一个用于零和游戏（即一方获胜另一方失败的游戏）的评分系统，例如篮球。在Elo评分系统中，每支队伍都有一个Elo评分，这个评分通常反映了队伍的质量。最初，每支队伍的Elo评分是相同的，当队伍获胜时，Elo评分增加；当队伍失败时，Elo评分减少。这个系统的一个关键特点是，战胜强队时Elo评分的增幅大于战胜弱队时的增幅。因此，它是一个非常有用的特征！
- en: We wanted to capture the Elo rating of a team at the end of the regular season,
    and use that as feature for the tournament. To do this, we calculated the Elo
    for each team on a per match basis. To calculate Elo for this feature, we found
    it more straightforward to use Pandas.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要记录一个团队在常规赛结束时的Elo评分，并将其作为锦标赛的特征。为了做到这一点，我们为每场比赛计算了每个团队的Elo评分。为了计算这个特征的Elo，我们发现使用Pandas更为直观。
- en: 'Central to Elo is calculating the expected score for each team. It can be described
    in code like so:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Elo的核心是计算每支队伍的预期得分。可以通过以下代码来描述这一过程：
- en: '[PRE15]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Considering a team A and a team B, this function computes the expected score
    of team A against team B.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一支队伍A和一支队伍B，这个函数计算队伍A对队伍B的预期得分。
- en: For each match, we would update the teams’ Elos. Note that the location of the
    match also played a part — winning at home was considered less impressive than
    winning away.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每场比赛，我们都会更新队伍的Elo评分。请注意，比赛的地点也起着作用——在主场获胜被认为不如客场获胜令人印象深刻。
- en: '[PRE16]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: To apply the Elo rating system, we iterated through each season’s matches, initializing
    teams with a base rating and updating their ratings match by match. The final
    Elo available for each team in each season will, hopefully, be a good descriptor
    of the team’s quality.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应用Elo评分系统，我们遍历了每个赛季的比赛，为每支队伍初始化一个基础评分，并逐场更新它们的评分。每支队伍在每个赛季的最终Elo评分，应该能很好地描述该队伍的质量。
- en: '[PRE17]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Ideally, we wouldn’t calculate Elo changes on a match-by-match basis to determine
    each team’s final Elo for the season. However, we couldn’t come up with a better
    approach. Do you have any ideas? If so, let us know!
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们不会逐场计算Elo的变化来确定每支队伍赛季结束时的最终Elo。然而，我们没有想到更好的方法。你有什么想法吗？如果有，请告诉我们！
- en: Value Added
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附加值
- en: The feature engineering steps demonstrated show how we can transform raw data
    — regular season statistics — into valuable information with predictive power.
    It is reasonable to assume that a team’s performance during the regular season
    is indicative of its potential performance in the final tournaments. By calculating
    the mean of observed match-by-match statistics for both the teams and their opponents,
    along with each team’s Elo rating in their final match, we were able to create
    a dataset suitable for modelling. Then, models were trained to predict the outcome
    of tournament matches using these features, among others developed in a similar
    way. With these models, we only need the two team IDs to look up the mean of their
    regular season statistics and their Elos to feed into the model and predict a
    score!
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 展示的特征工程步骤展示了我们如何将原始数据——常规赛统计数据——转化为具有预测能力的有价值信息。可以合理假设，一支球队在常规赛中的表现能够反映其在最终锦标赛中的潜在表现。通过计算每场比赛的统计数据平均值，包括球队和对手的统计数据，以及每支球队在最后一场比赛中的Elo评分，我们能够创建一个适合建模的数据集。然后，使用这些特征，训练模型来预测锦标赛比赛的结果，其他特征也以类似的方式开发。通过这些模型，我们只需要两个球队的ID，查找它们的常规赛统计数据平均值和Elo评分，输入到模型中即可预测得分！
- en: Conclusion
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this post, we looked at some of the theory behind Spark and PySpark, how
    that can be applied, and a concrete practical example. We explored how feature
    engineering can be done in the case of sports data, creating regular season statistics
    to use as features for final tournament games. Hopefully you’ve found this interesting
    and helpful — happy feature engineering!
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们探讨了Spark和PySpark背后的部分理论，如何应用这些理论，并展示了一个具体的实践例子。我们研究了如何在体育数据的案例中进行特征工程，创建常规赛统计数据，作为最终锦标赛比赛的特征使用。希望你觉得这篇文章有趣且有帮助——祝特征工程愉快！
- en: '**The full source code for this post and others in the series can be found**
    [**here**](https://dev.azure.com/nobledynamic/_git/FabricMadness)**.**'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**这篇文章及系列中其他文章的完整源代码可以在** [**这里**](https://dev.azure.com/nobledynamic/_git/FabricMadness)**找到。**'
- en: '*Originally published at* [*https://nobledynamic.com*](https://nobledynamic.com/posts/fabric-madness-2/)
    *on April 8, 2024.*'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*最初发布于* [*https://nobledynamic.com*](https://nobledynamic.com/posts/fabric-madness-2/)
    *2024年4月8日。*'
- en: References
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Jeff Sonas, Ryan Holbrook, Addison Howard, Anju Kandru. (2024). March Machine
    Learning Mania 2024\. Kaggle. [https://kaggle.com/competitions/march-machine-learning-mania-2024](https://kaggle.com/competitions/march-machine-learning-mania-2024)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Jeff Sonas, Ryan Holbrook, Addison Howard, Anju Kandru. (2024). March Machine
    Learning Mania 2024\. Kaggle. [https://kaggle.com/competitions/march-machine-learning-mania-2024](https://kaggle.com/competitions/march-machine-learning-mania-2024)'
