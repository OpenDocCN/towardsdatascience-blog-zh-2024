- en: Mini-Max Optimization Design of Generative Adversarial Networks (GAN)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成对抗网络（GAN）的极小极大优化设计
- en: 原文：[https://towardsdatascience.com/mini-max-optimization-design-of-generative-adversarial-networks-gan-dc1b9ea44a02?source=collection_archive---------8-----------------------#2024-01-12](https://towardsdatascience.com/mini-max-optimization-design-of-generative-adversarial-networks-gan-dc1b9ea44a02?source=collection_archive---------8-----------------------#2024-01-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/mini-max-optimization-design-of-generative-adversarial-networks-gan-dc1b9ea44a02?source=collection_archive---------8-----------------------#2024-01-12](https://towardsdatascience.com/mini-max-optimization-design-of-generative-adversarial-networks-gan-dc1b9ea44a02?source=collection_archive---------8-----------------------#2024-01-12)
- en: Nested bi-level optimization and equilibrium seeking objective
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌套双层优化与平衡寻求目标
- en: '[](https://deeporigami.medium.com/?source=post_page---byline--dc1b9ea44a02--------------------------------)[![Michio
    Suginoo](../Images/15e4a70d17d163889cc902bf4409931a.png)](https://deeporigami.medium.com/?source=post_page---byline--dc1b9ea44a02--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--dc1b9ea44a02--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--dc1b9ea44a02--------------------------------)
    [Michio Suginoo](https://deeporigami.medium.com/?source=post_page---byline--dc1b9ea44a02--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://deeporigami.medium.com/?source=post_page---byline--dc1b9ea44a02--------------------------------)[![Michio
    Suginoo](../Images/15e4a70d17d163889cc902bf4409931a.png)](https://deeporigami.medium.com/?source=post_page---byline--dc1b9ea44a02--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--dc1b9ea44a02--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--dc1b9ea44a02--------------------------------)
    [Michio Suginoo](https://deeporigami.medium.com/?source=post_page---byline--dc1b9ea44a02--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--dc1b9ea44a02--------------------------------)
    ·8 min read·Jan 12, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--dc1b9ea44a02--------------------------------)
    ·阅读时间8分钟·2024年1月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: Generative Adversarial Networks (GAN) demonstrated outstanding performance in
    generating realistic synthetic data which were indistinguishable from the real
    data. Unfortunately, GAN caught the public’s attention because of its illegit
    applications, [Deep Fake](https://www.technologyreview.com/2018/08/17/240305/fake-america-great-again/).
    (Knight, 2018)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络（GAN）在生成逼真的合成数据方面表现出色，生成的数据与真实数据几乎无法区分。不幸的是，GAN因其非法应用而引起了公众的关注，尤其是[深度伪造](https://www.technologyreview.com/2018/08/17/240305/fake-america-great-again/)。（Knight，2018）
- en: 'As its name suggests, Generative Adversarial Nets (GAN) is composed of two
    networks: the generative network (the generator) and the adversarial network (the
    discriminator). Incorporating an adversarial scheme into its architecture makes
    GAN a special type of generative network.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 顾名思义，生成对抗网络（GAN）由两个网络组成：生成网络（生成器）和对抗网络（判别器）。将对抗机制纳入其架构使得GAN成为一种特殊类型的生成网络。
- en: Importantly, GAN is non-parametric, therefore, would not impose much formal
    statistical requirements such as the Markov chain. Instead of imposing statistical
    assumptions, with the help of the adversary network, the generative network learns
    the probability distribution of the real data through back-propagation in deep
    neural networks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，GAN是非参数化的，因此不会强加诸如马尔可夫链之类的正式统计要求。与其强加统计假设，生成网络在对抗网络的帮助下，通过深度神经网络的反向传播学习真实数据的概率分布。
- en: In order to generate realistic synthetic data, GAN pits these two agents against
    each other within its architecture. In this game, while the generator attempts
    to simulate synthetic samples which imitate real samples, the discriminator attempts
    to distinguish between the real samples and the synthetic ones. In other words,
    while the generator, ***G***, tries to deceive the discriminator by counterfeiting,
    the discriminator, ***D***, plays a role of the police to detect the synthetic
    (fake) data. (Goodfellow, et al., 2014, p. 1) In a way, these two agents attempt
    to achieve diametrically opposite objectives.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成逼真的合成数据，生成对抗网络（GAN）在其架构中让这两个代理彼此对抗。在这个博弈中，生成器试图模拟模仿真实样本的合成样本，而判别器试图区分真实样本和合成样本。换句话说，当生成器***G***通过伪造来欺骗判别器时，判别器***D***则充当警察的角色，检测合成（伪造）数据。（Goodfellow等人，2014，第1页）从某种意义上说，这两个代理试图实现截然相反的目标。
- en: As they improve their skills, the synthetic data becomes indistinguishable from
    the real data. Thanks to its adversary (the discriminator), the generator learns
    how to better imitate the probability distribution of the given real data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 随着它们技能的提升，合成数据变得与真实数据无法区分。得益于它的对手（判别器），生成器学会了如何更好地模仿给定真实数据的概率分布。
- en: Since within its architecture GAN has to train two learners that attempt to
    achieve contrasting objectives through interactions, it has a unique optimization
    design (bi-level training mechanism and equilibrium seeking objectives). In this
    context, it took me a while to digest the architectural design of GAN. In this
    context, I decided to write this article to share my journey with those who are
    new to GAN so that they can comprehend the architectural peculiarity of GAN more
    smoothly. I hope that the readers find this article useful as a supplementary
    note.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在其架构中，GAN需要训练两个学习者，通过相互作用来实现相反的目标，因此它具有独特的优化设计（双层训练机制和平衡寻求目标）。在这个背景下，我花了一些时间来消化GAN的架构设计。在此背景下，我决定写这篇文章与那些对GAN不熟悉的人分享我的学习过程，以便他们能更顺利地理解GAN的架构特点。我希望读者能把这篇文章作为补充资料，发现它对自己有所帮助。
- en: As a precaution, GAN is heuristic. And there are many variants of GAN applications
    today. This article discusses only the architectural design of the original GAN.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种预防措施，GAN是启发式的。目前，GAN有许多不同的变体应用。本文仅讨论原始GAN的架构设计。
- en: '***The Original GAN Design***'
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '***原始GAN设计***'
- en: 'The base architecture of the original GAN was introduced in a seminal paper:
    “[Generative Adversarial Nets](https://arxiv.org/abs/1406.2661)” (Goodfellow,
    et al., 2014). In this original GAN paper, in order to train these two agents
    which pursue the diametrically opposite objectives, the co-authors designed a
    “[bi-level optimization (training)](https://en.wikipedia.org/wiki/Bilevel_optimization)”
    architecture, in which one internal training block (training of the discriminator)
    is nested within another high-level training block (training of the generator).
    And GAN trains these two agents alternately in this bi-level training framework.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 原始GAN的基础架构首次在一篇开创性论文中提出：“[生成对抗网络](https://arxiv.org/abs/1406.2661)”（Goodfellow等，2014年）。在这篇原始的GAN论文中，为了训练这两个追求截然相反目标的智能体，合著者们设计了一个“[双层优化（训练）](https://en.wikipedia.org/wiki/Bilevel_optimization)”架构，其中一个内部训练模块（判别器的训练）被嵌套在另一个高层训练模块（生成器的训练）中。GAN在这个双层训练框架中交替训练这两个智能体。
- en: '![](../Images/ee36b648bf82a7b9e95df5ddb35f87b9.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee36b648bf82a7b9e95df5ddb35f87b9.png)'
- en: Image by Author
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: '***Discriminator and Generator***'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '***判别器与生成器***'
- en: Now, let’s see what these two agents do in their learning process.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看这两个智能体在学习过程中做了什么。
- en: It is straight-forward that the discriminator is a binary classifier by design.
    Given mixed samples from the real data and the synthetic data, it classifies whether
    each sample is real (label=1) or fake/synthetic (label=0).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，判别器本质上是一个二分类器。给定来自真实数据和合成数据的混合样本，它会将每个样本分类为真实（标签=1）或伪造/合成（标签=0）。
- en: On the other hand, the generator is a noise distribution by design. And it is
    trained to imitate the probability distribution of the real dataset through an
    iteration. At every step of the training iteration, the learned generative model
    (updated generator) is copied and used as the new noise distribution. Thereafter,
    the new noise distribution is used to train the discriminator. (Goodfellow I.
    , 2015, p. 2).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，生成器本质上是一个噪声分布。它被训练去模仿真实数据集的概率分布，通过一个迭代过程。在每一步的训练迭代中，学习到的生成模型（更新后的生成器）会被复制并用作新的噪声分布。此后，新的噪声分布将用于训练判别器。（Goodfellow
    I.，2015，第2页）
- en: 'Let’s set the following:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设定以下内容：
- en: '![](../Images/745830f89d07f931ccad830e0db5ad4e.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/745830f89d07f931ccad830e0db5ad4e.png)'
- en: Image by Author
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: We take the input noise, z, and calculate its prior distribution, G(z), to define
    the generator.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们输入噪声z，并计算其先验分布G(z)，以定义生成器。
- en: In this setting, the ultimate goal of the generator is to deceive the discriminator
    by transforming its own distribution as close as the distribution of the real
    dataset.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种设置下，生成器的最终目标是通过将自己的分布转变为尽可能接近真实数据集的分布，从而欺骗判别器。
- en: '![](../Images/d3502d26f13d2481622262adce417521.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d3502d26f13d2481622262adce417521.png)'
- en: Image by Author
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: '***Two Objective Functions: Mini-Max Game***'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '***两个目标函数：最小-最大博弈***'
- en: 'Repeatedly, GAN has two agents to train within its architecture. And these
    agents have confronting objectives. Therefore, GAN has two objective functions:
    one for the discriminator and the other for the generator.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: GAN的架构中反复进行的是两个代理的训练，而这两个代理具有对立的目标。因此，GAN有两个目标函数：一个用于判别器，另一个用于生成器。
- en: On one hand, the discriminator, *D*, as a binary classifier, needs to maximize
    the probability of correctly assigning labels to both the real data (label=1)
    and the synthetic data (label=0).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，作为二分类器的判别器*D*需要最大化正确分配标签的概率，既包括真实数据（标签=1），也包括合成数据（标签=0）。
- en: On the other hand, the ultimate goal of the generator is to fool the classifier
    by creating synthetic data which is indistinguishable from the real data. Therefore,
    the generator attempts to deceive the classifier so that the discriminator would
    incorrectly classify the synthetic data with label 1\. In other words, the objective
    of the generator is “*to maximize the probability of D making a mistake.*” (Goodfellow,
    et al., 2014, p. 1)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，生成器的最终目标是通过创建与真实数据难以区分的合成数据来欺骗分类器。因此，生成器尝试欺骗分类器，使得判别器错误地将合成数据分类为标签1。换句话说，生成器的目标是“*最大化D犯错的概率*”。（Goodfellow等人，2014，第1页）
- en: At a conceptual level, in order to achieve their diametrically opposite objectives,
    these two agents can refer to the following general log-likelihood formula, *V*,
    typically used for a binary classification problem.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在概念层面，为了实现这两个目标相反的目标，这两个代理可以参考以下通用的对数似然公式*V*，通常用于二分类问题。
- en: '![](../Images/8b48358640e02c8898307c84e4b64a15.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b48358640e02c8898307c84e4b64a15.png)'
- en: Image by Author
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: While GAN trains the discriminator to maximize the objective function, it trains
    the generator to minimize the second term of the objective function. In this sense,
    the co-authors called the overall objective “***minimax game***”. (Goodfellow,
    et al., 2014, p. 3)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在GAN训练过程中，判别器的目标是最大化目标函数，而生成器的目标是最小化目标函数的第二项。从这个意义上讲，联合作者将整体目标称为“***极小极大博弈***”。（Goodfellow等人，2014，第3页）
- en: '***Non-saturation Modification:***'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '***非饱和修改：***'
- en: In its implementation, the co-authors encountered a problem of saturation during
    the early stage of training the generator.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现过程中，联合作者在训练生成器的初期遇到了饱和问题。
- en: “Early in learning, when G is poor, D can reject samples with high confidence
    because they are clearly different from the training data. In this case, log(1
    — D(G(z))) saturates.”
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “在学习的早期，当生成器G较差时，判别器D可以高信心地拒绝样本，因为它们显然与训练数据不同。在这种情况下，log(1 — D(G(z)))会发生饱和。”
- en: 'In order to resolve the saturation issue, they converted the second term of
    the original log-likelihood objective function as follows and recommended to maximize
    it for the generator:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决饱和问题，他们将原始对数似然目标函数的第二项转换如下，并建议生成器最大化这一项：
- en: '![](../Images/1047088989096cf0cee24f1e93409952.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1047088989096cf0cee24f1e93409952.png)'
- en: Image by Author
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: This formula reflects the objective of the generator “*to maximize the probability
    of D making a mistake*”. (Goodfellow, et al., 2014, p. 1)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式反映了生成器的目标“*最大化D犯错的概率*”。（Goodfellow等人，2014，第1页）
- en: '***Evaluation***'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '***评估***'
- en: During the training, the generator continues creating better synthetic data
    to deceive the discriminator, while the discriminator is improving its detection
    ability. In this sense, the ultimate objective of GAN’s overall optimization is
    not to search for the global maximum of either of these objective functions. It
    is rather to seek an equilibrium point, where neither of agents can improve the
    performance. In a sese, at the equilibrium point, the discriminator is unable
    to distinguish between the real data and the synthetic data because the generator
    is able to create synthetic data as real as possible.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，生成器不断创造更好的合成数据以欺骗判别器，而判别器则提高其检测能力。从这个角度看，GAN整体优化的最终目标并不是寻找这两个目标函数的全局最大值，而是寻求一个平衡点，在这个平衡点上，两个代理都无法进一步提升性能。从某种意义上讲，在平衡点，判别器无法区分真实数据和合成数据，因为生成器能够创造尽可能真实的合成数据。
- en: 'This goal setting of the objective function is very unique for GAN. And one
    of the co-authors, Ian Goodfellow, describes the equilibrium point as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这一目标函数的设定对GAN来说非常独特。其中一位联合作者Ian Goodfellow描述了平衡点如下：
- en: “it corresponds to a saddle point that is a local maximum for the classifier
    and a local minimum for the generator” (Goodfellow I. , 2015, p. 2).
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “它对应于一个鞍点，即分类器的局部最大值和生成器的局部最小值”（Goodfellow I. ，2015，p.2）。
- en: Moreover, the equilibrium point can be conceptually represented by the probability
    of random guessing, 0.5 (50%).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，平衡点在概念上可以通过随机猜测的概率0.5（50%）来表示。
- en: '![](../Images/3e74e529cd1bd6b29a7b12545688eb3c.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e74e529cd1bd6b29a7b12545688eb3c.png)'
- en: Image by Author
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: '***Alternating Training Process: Nested Bi-level Optimization***'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '***交替训练过程：嵌套双层优化***'
- en: 'In order to achieve this ultimate goal, GAN designed an alternating learning
    process in a “bi-level optimization” framework, in which the training loop of
    the discriminator is nested within another higher training loop of the generator.
    This bi-level optimization framework enables GAN to alternate the training process
    between these two agents: *k-steps* of training D and one step of training G (Goodfellow,
    et al., 2014, p. 3). During the alternation of the two models, it is important
    to freeze the learning process of the other while training one model; “*the generator
    is updated while holding the discriminator fixed, and vice versa*” (Goodfellow
    I. , 2015, p. 3).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一最终目标，GAN设计了一个交替学习过程，在“二级优化”框架中，其中鉴别器的训练循环嵌套在生成器的另一个更高层次的训练循环中。这个二级优化框架使得GAN能够在这两个代理之间交替训练过程：*k步*的D训练和一步G训练（Goodfellow等，2014，p.3）。在这两个模型交替的过程中，重要的是在训练一个模型时冻结另一个模型的学习过程；“*在保持鉴别器固定的情况下更新生成器，反之亦然*”（Goodfellow
    I. ，2015，p.3）。
- en: The following algorithm revised the original algorithm presented in the original
    GAN paper in order to fully reflect the recommended conversion of the log-likelihood
    for the generator.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 以下算法修订了原始GAN论文中提出的原始算法，以充分反映生成器对数似然转换的推荐。
- en: '![](../Images/1c7d25fd63b2c1395701db01c688802a.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c7d25fd63b2c1395701db01c688802a.png)'
- en: Image by Author
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: As you see in the algorithm, while GAN takes samples from both the generative
    model (generator) and the real data during the forward propagation, it trains
    the both agents during the backpropagation. (Goodfellow, et al., 2014, p. 2) It
    follows the convention of deep neural networks.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在算法中看到的，GAN在前向传播过程中同时从生成模型（生成器）和真实数据中采样，而在反向传播过程中同时训练这两个代理。（Goodfellow等，2014，p.2）它遵循了深度神经网络的常规。
- en: And GAN trains the discriminator first in the nested block, then trains the
    generator to fool the trained discriminator at every iteration at the upper loop.
    Then, it continues to iterate this bi-level training until it arrives at an equilibrium
    point as discussed earlier.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: GAN首先在嵌套块中训练鉴别器，然后在每次迭代时训练生成器以欺骗训练过的鉴别器，然后继续迭代这个二级训练，直到达到前面讨论的平衡点。
- en: Overall, technically, GAN learns the probability distribution of the real data
    through the generator; the discriminator is just one inner component built in
    the nested block within the generator’s learning mechanism. The objective function
    of the generator reflects the trained discriminator model on the piecemeal basis
    in its formula at the upper level optimization process. In other words, for every
    iteration, the generator keeps updating its objective function once the discriminator
    gets trained within the nested optimization block.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，技术上，GAN通过生成器学习真实数据的概率分布；鉴别器只是嵌套在生成器学习机制中的一个内部组件。生成器的目标函数在其公式的上层优化过程中，反映了训练过的鉴别器模型的逐步内容。换句话说，每次迭代时，一旦鉴别器在嵌套的优化模块中被训练，生成器就会不断更新其目标函数。
- en: That pretty much paints GAN’s algorithmic design of model optimization.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这几乎描绘了GAN模型优化的算法设计。
- en: '***Summary***'
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '***总结***'
- en: In order to alternately train two agents — the discriminator and the generator
    — GAN adopted a bi-level optimization framework, where the discriminator is trained
    in the inner block nested within the training block of the generator.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了交替训练两个代理——鉴别器和生成器——GAN采用了一个双层优化框架，其中鉴别器在嵌套在生成器训练块内的内部模块中进行训练。
- en: Since these two agents have diametrically opposite objectives (in the sense
    that the discriminator aims at maximizing its binary classifier’s objective function
    and the generator at minimizing it), the co-authors called the overall objective
    “***minimax game***”. (Goodfellow, et al., 2014, p. 3) Overall, GAN aims at its
    mini-max optimization (training) objective by seeking an equilibrium point, where
    the discriminator is no longer able to distinguish between the real data and the
    synthetic data because now the synthetic data the generator creates is indistinguishable
    from the real data.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这两个代理有截然相反的目标（因为判别器旨在最大化其二元分类器的目标函数，而生成器则旨在最小化该函数），合著者称整体目标为“***极小极大博弈***”。（Goodfellow
    等人，2014，第3页）总的来说，GAN通过寻求一个平衡点来实现其极小极大优化（训练）目标，在这个平衡点上，判别器无法再区分真实数据和合成数据，因为现在生成器生成的合成数据与真实数据无法区分。
- en: Its nested bi-level training framework and its equilibrium seeking objective
    setting (in contrast to maximization objectives) characterize the mini-max optimization
    framework of GAN.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 它的嵌套双层训练框架及其寻求平衡的目标设置（与最大化目标相对）构成了GAN的极小极大优化框架。
- en: At last, it is important to note that the leading author, Ian Goodfellow, characterizes
    that the original GAN is heuristic and has theoretical limitations. For example,
    convergence is not guaranteed when the objective function is not convex. In this
    context, he articulated that GAN is open for further innovative improvements.
    As a matter of fact, a wide range of evaluation measures have been explored for
    variants of GAN applications (Borji, 2018). In this sense, I would like to emphasize
    that the architectural design outlined in this article describes only the prototype
    of GAN that was introduced in the original GAN paper. Therefore, the architectural
    design introduced in this article is neither comprehensive nor universal for other
    types of GAN applications.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，重要的是要指出，主要作者 Ian Goodfellow 表示，原始的GAN是启发式的，并且具有理论上的局限性。例如，当目标函数不是凸函数时，无法保证收敛。在这种背景下，他阐述了GAN仍然有进一步创新改进的空间。事实上，针对GAN应用的多种变体，已经探索了广泛的评估度量（Borji，2018）。因此，我想强调的是，本文所述的架构设计仅描述了最初的GAN论文中提出的GAN原型。因此，本文介绍的架构设计并不是对其他类型GAN应用的全面或普遍适用的设计。
- en: Given that the precautionary note is well communicated with the readers, I hope
    that this article is useful for those who are new to GAN in starting their own
    journey with GAN.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这一预警说明已向读者充分传达，我希望本文能对那些刚接触GAN的读者有所帮助，帮助他们开启自己的GAN之旅。
- en: References
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Borji, A. (2018, 10 24). Pros and Cons of GAN Evaluation Measures. Retrieved
    from ArXvi: [https://arxiv.org/abs/1802.03446](https://arxiv.org/abs/1802.03446)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Borji, A. (2018, 10 24). GAN评估度量的优缺点。来源于 ArXiv: [https://arxiv.org/abs/1802.03446](https://arxiv.org/abs/1802.03446)'
- en: 'Goodfellow, I. (2015, 5 21). On distinguishability criteria for estimating
    generative models. Retrieved from ArXiv: [https://arxiv.org/abs/1412.6515](https://arxiv.org/abs/1412.6515)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Goodfellow, I. (2015, 5 21). 关于估计生成模型的可区分性标准。来源于 ArXiv: [https://arxiv.org/abs/1412.6515](https://arxiv.org/abs/1412.6515)'
- en: 'Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D.,
    Ozair, S., Courville, A., Bengioz, Y. (2014, 6 10). Generative Adversarial Nets.
    Retrieved from arXiv: [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D.,
    Ozair, S., Courville, A., Bengio, Y. (2014, 6 10). 生成对抗网络。来源于 arXiv: [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)'
- en: 'Knight, W. (2018, 8 17). Fake America great again. Retrieved from MIT Technology
    Review: [https://www.technologyreview.com/2018/08/17/240305/fake-america-great-again/](https://www.technologyreview.com/2018/08/17/240305/fake-america-great-again/)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Knight, W. (2018, 8 17). 让美国再次伟大。来源于 MIT Technology Review: [https://www.technologyreview.com/2018/08/17/240305/fake-america-great-again/](https://www.technologyreview.com/2018/08/17/240305/fake-america-great-again/)'
