- en: How to Improve Model Quality Without Building Larger Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在不构建更大模型的情况下提高模型质量
- en: 原文：[https://towardsdatascience.com/how-to-improve-model-quality-without-building-larger-models-d6c8e76a86fe?source=collection_archive---------5-----------------------#2024-10-08](https://towardsdatascience.com/how-to-improve-model-quality-without-building-larger-models-d6c8e76a86fe?source=collection_archive---------5-----------------------#2024-10-08)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-improve-model-quality-without-building-larger-models-d6c8e76a86fe?source=collection_archive---------5-----------------------#2024-10-08](https://towardsdatascience.com/how-to-improve-model-quality-without-building-larger-models-d6c8e76a86fe?source=collection_archive---------5-----------------------#2024-10-08)
- en: Going into the Google DeepMind’s “Scaling LLM Test-Time Compute Optimally can
    be More Effective than Scaling Model Parameters”
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进入谷歌DeepMind的“**Scaling LLM Test-Time Compute Optimally can be More Effective
    than Scaling Model Parameters**”
- en: '[](https://medium.com/@mgunton7?source=post_page---byline--d6c8e76a86fe--------------------------------)[![Matthew
    Gunton](../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png)](https://medium.com/@mgunton7?source=post_page---byline--d6c8e76a86fe--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d6c8e76a86fe--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d6c8e76a86fe--------------------------------)
    [Matthew Gunton](https://medium.com/@mgunton7?source=post_page---byline--d6c8e76a86fe--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mgunton7?source=post_page---byline--d6c8e76a86fe--------------------------------)[![Matthew
    Gunton](../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png)](https://medium.com/@mgunton7?source=post_page---byline--d6c8e76a86fe--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d6c8e76a86fe--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d6c8e76a86fe--------------------------------)
    [Matthew Gunton](https://medium.com/@mgunton7?source=post_page---byline--d6c8e76a86fe--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d6c8e76a86fe--------------------------------)
    ·11 min read·Oct 8, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d6c8e76a86fe--------------------------------)
    ·阅读时间 11分钟·2024年10月8日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/e481b2de6c28675a6758b1d595d75bb1.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e481b2de6c28675a6758b1d595d75bb1.png)'
- en: Image by Author — Flux.1 12B
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片 — Flux.1 12B
- en: Recently OpenAI unveiled their newest model o1\. Rather than highlight the parameter
    size of this model, OpenAI instead showcased that the model performs significantly
    better because it takes more time. When you ask the model a question, it will
    often taken multiple seconds to respond — a far cry from the millisecond speed
    most people now expect with Large Language Models (LLMs). Nevertheless, this extra
    time appears to pay off as o1 scores substantially higher than other models on
    the LMSYS Chatbot Arena.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，OpenAI推出了他们最新的模型o1。与其突出该模型的参数规模，OpenAI更强调的是该模型因为花费更多时间而表现得更好。当你向模型提问时，它通常需要几秒钟来响应——这与大多数人现在对大型语言模型（LLM）期望的毫秒级响应速度相去甚远。尽管如此，这额外的时间似乎是值得的，因为o1在LMSYS
    Chatbot Arena的得分远高于其他模型。
- en: Given this leap in performance, the question everyone is asking is, How did
    they do this?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这一性能飞跃，每个人都在问：他们是怎么做到的？
- en: '![](../Images/acc74e222738f0b3c3d8b5d28e79cf8b.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/acc74e222738f0b3c3d8b5d28e79cf8b.png)'
- en: Screen Capture of [Lmsys Chatbot Arena](https://lmarena.ai/) Math Rankings on
    9/23/2024
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[Lmsys Chatbot Arena](https://lmarena.ai/) 2024年9月23日的数学排名屏幕截图'
- en: While OpenAI has not publicly stated how they achieved these results, there
    have been a few papers recently that are good candidates for what is happening
    behind the scenes. One such paper is [“Scaling LLM Test-Time Compute Optimally
    can be More Effective than Scaling Model Parameters”](https://arxiv.org/pdf/2408.03314).
    This goes into how you can leverage…
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管OpenAI尚未公开声明他们是如何取得这些成果的，但最近有几篇论文可能为我们揭示了幕后发生的事情。其中一篇论文是[“**Scaling LLM Test-Time
    Compute Optimally can be More Effective than Scaling Model Parameters**”](https://arxiv.org/pdf/2408.03314)。这篇论文探讨了如何利用……
