- en: Economics of Hosting Open Source LLMs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 托管开源LLMs的经济学
- en: 原文：[https://towardsdatascience.com/economics-of-hosting-open-source-llms-17b4ec4e7691?source=collection_archive---------0-----------------------#2024-11-12](https://towardsdatascience.com/economics-of-hosting-open-source-llms-17b4ec4e7691?source=collection_archive---------0-----------------------#2024-11-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/economics-of-hosting-open-source-llms-17b4ec4e7691?source=collection_archive---------0-----------------------#2024-11-12](https://towardsdatascience.com/economics-of-hosting-open-source-llms-17b4ec4e7691?source=collection_archive---------0-----------------------#2024-11-12)
- en: Large Language Models in Production
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大型语言模型在生产中的应用
- en: Leveraging various deployment options
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用各种部署选项
- en: '[](https://medium.com/@ilsilfverskiold?source=post_page---byline--17b4ec4e7691--------------------------------)[![Ida
    Silfverskiöld](../Images/a2c0850bc0198688f70a5eca858cf8b5.png)](https://medium.com/@ilsilfverskiold?source=post_page---byline--17b4ec4e7691--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--17b4ec4e7691--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--17b4ec4e7691--------------------------------)
    [Ida Silfverskiöld](https://medium.com/@ilsilfverskiold?source=post_page---byline--17b4ec4e7691--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ilsilfverskiold?source=post_page---byline--17b4ec4e7691--------------------------------)[![Ida
    Silfverskiöld](../Images/a2c0850bc0198688f70a5eca858cf8b5.png)](https://medium.com/@ilsilfverskiold?source=post_page---byline--17b4ec4e7691--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--17b4ec4e7691--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--17b4ec4e7691--------------------------------)
    [Ida Silfverskiöld](https://medium.com/@ilsilfverskiold?source=post_page---byline--17b4ec4e7691--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--17b4ec4e7691--------------------------------)
    ·19 min read·Nov 12, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--17b4ec4e7691--------------------------------)
    ·阅读时间19分钟·2024年11月12日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/451f57a8bda4e9b8e4b0f293e3d83b43.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/451f57a8bda4e9b8e4b0f293e3d83b43.png)'
- en: Total Processing Time on GPU vs CPU — Not to scale* | Image by author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: GPU与CPU上的总处理时间 — 非比例缩放* | 图片由作者提供
- en: '*If you’re not a member but want to read this article, see this friend link*
    [*here.*](https://medium.com/@ilsilfverskiold/17b4ec4e7691?sk=2649166d7d7a839885bb3fff2a3922bd)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你不是会员但想阅读本文，请查看这个朋友链接* [*这里。*](https://medium.com/@ilsilfverskiold/17b4ec4e7691?sk=2649166d7d7a839885bb3fff2a3922bd)'
- en: 'If you’ve been experimenting with open-source models of different sizes, you’re
    probably asking yourself: what’s the most efficient way to deploy them?'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经在尝试不同规模的开源模型，你可能会问自己：部署它们最有效的方式是什么？
- en: What’s the pricing difference between **on-demand** and **serverless providers**,
    and is it really worth dealing with a player like AWS when there are LLM serving
    platforms?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**按需**和**无服务器提供商**之间的定价差异是什么？当有LLM服务平台时，真的值得与像AWS这样的玩家打交道吗？'
- en: I’ve decided to dive into this subject, comparing cloud vendors like AWS with
    newer alternatives like Modal, BentoML, Replicate, Hugging Face Endpoints, and
    Beam.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我决定深入探讨这个话题，将AWS等云服务商与Modal、BentoML、Replicate、Hugging Face Endpoints和Beam等新兴替代方案进行比较。
- en: We’ll look at metrics such as processing time, cold start delays, and CPU, memory,
    and GPU costs to understand what’s most efficient and economical. We’ll also cover
    softer metrics like ease of deployment, developer experience and community.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将研究处理时间、冷启动延迟、CPU、内存和GPU成本等指标，以了解哪种方式最有效且经济。我们还将涵盖一些软性指标，如部署的难易程度、开发者体验和社区支持。
- en: '![](../Images/d6a4df8a4c00c6bf8ec5c79689b6ec63.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6a4df8a4c00c6bf8ec5c79689b6ec63.png)'
- en: Some of the metrics we’ll look at | Image by author
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将查看的一些指标 | 图片由作者提供
- en: We’ll explore a few use cases, such as deploying a **smaller model on CPU**
    versus running a **7–8 billion parameter model on GPU**.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨一些使用案例，比如在CPU上部署**较小的模型**与在GPU上运行**70亿到80亿参数的模型**的区别。
