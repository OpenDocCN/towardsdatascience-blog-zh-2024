- en: 'Using Sun RGB-D: Indoor Scene Dataset with 2D & 3D Annotations'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Sun RGB-D：带有2D和3D注释的室内场景数据集
- en: 原文：[https://towardsdatascience.com/using-sun-rgb-d-indoor-scene-dataset-with-2d-3d-annotations-387b9af5c89e?source=collection_archive---------9-----------------------#2024-03-09](https://towardsdatascience.com/using-sun-rgb-d-indoor-scene-dataset-with-2d-3d-annotations-387b9af5c89e?source=collection_archive---------9-----------------------#2024-03-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/using-sun-rgb-d-indoor-scene-dataset-with-2d-3d-annotations-387b9af5c89e?source=collection_archive---------9-----------------------#2024-03-09](https://towardsdatascience.com/using-sun-rgb-d-indoor-scene-dataset-with-2d-3d-annotations-387b9af5c89e?source=collection_archive---------9-----------------------#2024-03-09)
- en: Simple Python code for accessing Sun RGB-D and similar datasets
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 访问Sun RGB-D及类似数据集的简单Python代码
- en: '[](https://medium.com/@mjacobson130?source=post_page---byline--387b9af5c89e--------------------------------)[![Maxwell
    .J. Jacobson](../Images/263fd8189950c372df0d4156770cd0d8.png)](https://medium.com/@mjacobson130?source=post_page---byline--387b9af5c89e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--387b9af5c89e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--387b9af5c89e--------------------------------)
    [Maxwell .J. Jacobson](https://medium.com/@mjacobson130?source=post_page---byline--387b9af5c89e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mjacobson130?source=post_page---byline--387b9af5c89e--------------------------------)[![Maxwell
    .J. Jacobson](../Images/263fd8189950c372df0d4156770cd0d8.png)](https://medium.com/@mjacobson130?source=post_page---byline--387b9af5c89e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--387b9af5c89e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--387b9af5c89e--------------------------------)
    [Maxwell .J. Jacobson](https://medium.com/@mjacobson130?source=post_page---byline--387b9af5c89e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--387b9af5c89e--------------------------------)
    ·9 min read·Mar 9, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--387b9af5c89e--------------------------------)
    ·阅读时长9分钟·2024年3月9日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/3a780e5dfb42b06f860a655e97baa1c1.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a780e5dfb42b06f860a655e97baa1c1.png)'
- en: 3D understanding from 2D images is the first step into a larger world.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 从2D图像进行3D理解是进入更广阔世界的第一步。
- en: As many of the primitive tasks in computer vision approach a solved state —
    decent, quasi-general solutions now being available for image [segmentation](https://segment-anything.com/)
    and [text-conditioned generation](https://en.wikipedia.org/wiki/Stable_Diffusion),
    with general answers to visual question answering, depth estimation, and general
    object detection well on the way — I and many of my colleagues have been looking
    to use CV in larger tasks. When a human looks at a scene, we see more than flat
    outlines. We comprehend more than a series of labels. We can perceive and imagine
    within 3D spaces. We see a scene, and we can understand it in a very complete
    way. This capability should be within reach for CV systems of the day… If only
    we had the right data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着计算机视觉中的许多基础任务接近解决状态——现在已经有相对完善的、准通用的解决方案可用于图像[分割](https://segment-anything.com/)和[基于文本的生成](https://en.wikipedia.org/wiki/Stable_Diffusion)，对于视觉问答、深度估计和一般物体检测的通用解答也已经取得了显著进展——我和我的许多同事一直在寻找将计算机视觉应用于更大任务的机会。当人类看一个场景时，我们看到的不仅仅是平面的轮廓。我们理解的也不仅是一个个标签。我们能够在3D空间中感知和想象。我们看到一个场景，并能以非常完整的方式理解它。这种能力应该是当今计算机视觉系统触手可及的…如果我们能拥有正确的数据的话。
- en: '[Sun RGB-D](https://rgbd.cs.princeton.edu/) is an interesting image dataset
    from 2015 that satiates many of the data hungers of total scene understanding.
    This dataset is a collection of primarily indoor scenes, collected with a digital
    camera and four different 3D scanners. The linked publication goes into greater
    detail on how the dataset was collected and what it contains. Most importantly
    though, this dataset contains a wealth of data that includes both 2D and 3D annotations.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sun RGB-D](https://rgbd.cs.princeton.edu/)是一个有趣的图像数据集，创建于2015年，满足了对全面场景理解的多种数据需求。该数据集主要收集了室内场景，通过数码相机和四种不同的3D扫描仪进行拍摄。相关出版物详细介绍了该数据集的收集方法及其内容。最重要的是，这个数据集包含了大量的数据，其中包括2D和3D注释。'
- en: '![](../Images/7c9cc9a80fee7f342ff3c937b11c7160.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c9cc9a80fee7f342ff3c937b11c7160.png)'
- en: 'Source: [*SUN RGB-D: A RGB-D Scene Understanding Benchmark Suite*](https://rgbd.cs.princeton.edu/paper.pdf)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[*SUN RGB-D：一个RGB-D场景理解基准套件*](https://rgbd.cs.princeton.edu/paper.pdf)
- en: With this dataset, CV and ML algorithms can learn much deeper (excuse the pun)
    features from 2D images. More than that though, using data like this could open
    opportunities in applying 3D reasoning to 2D images. But that is a story for another
    time. This article will simply provide the basic python code to access this Sun
    RGB-D data, so that readers can use this wonderful resource in their own projects.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个数据集，计算机视觉（CV）和机器学习（ML）算法可以从2D图像中学习更深层次的（开个玩笑）特征。更重要的是，使用这样的数据可以为将3D推理应用于2D图像打开机会。但那是另一个话题。本文章将简单提供访问Sun
    RGB-D数据的基本Python代码，供读者在自己的项目中使用这一宝贵资源。
- en: Dataset Layout
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集布局
- en: After downloading the dataset from [here](https://rgbd.cs.princeton.edu/data/SUNRGBD.zip),
    you will end up with a directory structure like this.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 从[这里](https://rgbd.cs.princeton.edu/data/SUNRGBD.zip)下载数据集后，你将得到一个类似于下面的目录结构。
- en: '![](../Images/ced5a3bbbfa1cceb41338a70c924e452.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ced5a3bbbfa1cceb41338a70c924e452.png)'
- en: These separate the data by the type of scanner used to collect them. Specifically,
    the Intel RealSense 3D Camera for tablets, the Asus Xtion LIVE PRO for laptops,
    and the Microsoft Kinect versions 1 and 2 for desktop.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这些文件根据收集数据所用的扫描仪类型进行分类。具体来说，Intel RealSense 3D摄像头用于平板电脑，Asus Xtion LIVE PRO用于笔记本电脑，而Microsoft
    Kinect 1和2则用于桌面电脑。
- en: '![](../Images/4b571d38ef8f0c1a36951a8a8b0cf5df.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b571d38ef8f0c1a36951a8a8b0cf5df.png)'
- en: 'Source: [*SUN RGB-D: A RGB-D Scene Understanding Benchmark Suite*](https://rgbd.cs.princeton.edu/paper.pdf)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '来源：[*SUN RGB-D: 一个RGB-D场景理解基准套件*](https://rgbd.cs.princeton.edu/paper.pdf)'
- en: 'Moving into “kv2”, we see two directories: align_kv2 and kinect2data. This
    is one problem with the Sun RGB-D dataset… its directory structure is not consistent
    for each sensor type. In “realsense”, there are four directories containing data:
    lg, sa, sh, and shr. In “xtion” there is a more complex directory structure still.
    And worse, I have been unable to find a clear description of how these sub-directories
    are different anywhere in the dataset’s paper, supplementary materials, or website.
    **If anyone knows the answer to this, please let me know!**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 进入“kv2”目录，我们可以看到两个子目录：align_kv2和kinect2data。这是Sun RGB-D数据集的一个问题……它的目录结构对于每种传感器类型并不一致。在“realsense”中，有四个包含数据的目录：lg、sa、sh和shr。而在“xtion”中，目录结构则更为复杂。更糟糕的是，我在数据集的论文、补充材料或网站上找不到清晰的描述，说明这些子目录有什么不同。**如果有人知道答案，请告诉我！**
- en: 'For the time being though, lets skip down into the consistent part of the dataset:
    the data records. For align_kv2, we have this:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，让我们跳到数据集的一致部分：数据记录。对于align_kv2，我们有以下内容：
- en: '![](../Images/7fc4e9a92575deee653da07410badf2c.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7fc4e9a92575deee653da07410badf2c.png)'
- en: 'For all of the data records across all of the sensor types, this part is largely
    consistent. Some important files to look at are described below:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有传感器类型的所有数据记录，这部分在很大程度上是一致的。以下是一些需要关注的重要文件：
- en: '*annotation2Dfinal* contains the most recent 2D annotations including polygonal
    object segmentations and object labels. These are stored in a single JSON file
    which has the x and y 2D coordinates for each point in each segmentation, as well
    as a list for object labels.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*annotation2Dfinal*包含最新的2D注释，包括多边形物体分割和物体标签。这些注释存储在一个单一的JSON文件中，其中包含每个分割中每个点的x和y
    2D坐标，以及物体标签的列表。'
- en: '*annotation3Dfinal* is the same for 3D annotations. These are in the form of
    bounding shapes — polyhedra that are axis-aligned on the y (up-down) dimension.
    These can also be found in the singular JSON file of the directory.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*annotation3Dfinal*用于3D注释。这些注释以边界形状的形式存在——是沿y轴（上下方向）对齐的多面体。这些注释也可以在目录中的单一JSON文件中找到。'
- en: '*depth* contains the raw depth images collected by the sensor. *depth_bfx*
    contains a cleaned-up copy that addresses some of the limitations from the sensor.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*depth*包含传感器收集的原始深度图像。*depth_bfx*包含已清理的副本，解决了传感器的一些限制。'
- en: The original image can be found in the image directory. A full resolution, uncropped
    version can also be found in fullres.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始图像可以在图像目录中找到。完整分辨率、未裁剪的版本也可以在fullres文件夹中找到。
- en: Sensor extrinsics and intrinsics are saved in text files as numpy-like arrays.
    *intrinsics.txt* contains the intrinsics, but extrinsics is stored in the singular
    text file within the extrinsics folder.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传感器的外参和内参以类似numpy数组的文本文件形式保存。*intrinsics.txt*文件包含内参，而外参则存储在extrinsics文件夹中的单一文本文件内。
- en: Finally, the type of scene (office, kitchen, bedroom, etc) can be found as a
    string in *scene.txt*.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，场景类型（如办公室、厨房、卧室等）可以在*scene.txt*文件中找到，文件内容为字符串。
- en: Setup
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置
- en: First things first, we will need to read in files from a few formats. JSON and
    txt primarily. From those text files, we need to pull out a numpy array for both
    the extrinsics and intrinsics of the sensor. There are also allot of files here
    that don’t seem to follow a strict naming convention but will be the only one
    of its type in the same directory, so get_first_file_path will be useful here.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要从几种格式的文件中读取数据，主要是JSON和txt格式。从这些文本文件中，我们需要提取出传感器的外参和内参的numpy数组。这里也有很多文件似乎没有遵循严格的命名规范，但它们在同一目录中通常是唯一的，所以`get_first_file_path`函数在这里会很有用。
- en: I’d also like this code to output a simple 3D model of the rooms we find in
    the dataset. This can give us some easy data visualization, and lets us distill
    down the basic spatial features of a scene. To achieve this, we’ll utilize the
    OBJ file format, a standard for representing 3D geometry. An OBJ file primarily
    consists of lists of vertices (points in 3D space), along with information on
    how these vertices are connected to form faces (the surfaces of the 3D object).
    The layout of an OBJ file is straightforward, beginning with vertices, each denoted
    by a line starting with ‘v’ followed by the x, y, and z coordinates of the vertex.
    Faces are then defined by lines starting with ‘f’, listing the indices of the
    vertices that form each face’s corners, thus constructing the 3D surface.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我还希望这段代码能够输出我们在数据集中找到的房间的简单3D模型。这可以为我们提供简单的数据可视化，并帮助我们提取出场景的基本空间特征。为了实现这一目标，我们将使用OBJ文件格式，它是表示3D几何图形的标准格式。一个OBJ文件主要由顶点列表（3D空间中的点）构成，并且包含这些顶点如何连接以形成面（3D物体的表面）的信息。OBJ文件的布局非常简单，首先是顶点，每个顶点以以‘v’开头的行表示，后面跟着顶点的x、y和z坐标。面通过以‘f’开头的行来定义，列出构成每个面的角落的顶点索引，从而构建3D表面。
- en: In our context, the bounding shapes that define the spatial features of a scene
    are polyhedra, 3D shapes with flat faces and straight edges. Given that the y
    dimension is axis-aligned — meaning it consistently represents the up-down direction
    across all points — we can simplify the representation of our polyhedron using
    only the x and z coordinates for defining the vertices, along with a global minimum
    (min_y) and maximum (max_y) y-value that applies to all points. This approach
    assumes that vertices come in pairs where the x and z coordinates remain the same
    while the y coordinate alternates between min_y and max_y, effectively creating
    vertical line segments.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的上下文中，定义场景空间特征的边界形状是多面体，即具有平面面的3D形状。由于y维度是轴对齐的——意味着它在所有点中始终代表上下方向——我们可以简化多面体的表示，只使用x和z坐标来定义顶点，同时使用适用于所有点的全局最小值（min_y）和最大值（max_y）y值。这种方法假设顶点是成对出现的，其中x和z坐标保持不变，而y坐标在min_y和max_y之间交替，从而有效地创建了垂直的线段。
- en: The `write_obj` function encapsulates this logic to construct our 3D model.
    It starts by iterating over each bounding shape in our dataset, adding vertices
    to the OBJ file with their x, y, and z coordinates. For each pair of points (with
    even indices representing min_y and odd indices representing max_y where x and
    z are unchanged), the function writes face definitions to connect these points,
    forming vertical faces around each segment (e.g., around vertices 0, 1, 2, 3,
    then 2, 3, 4, 5, and so on). If the bounding shape has more than two pairs of
    vertices, a closing face is added to connect the last pair of vertices back to
    the first pair, ensuring the polyhedron is properly enclosed. Finally, the function
    adds faces for the top and bottom of the polyhedron by connecting all min_y vertices
    and all max_y vertices, respectively, completing the 3D representation of the
    spatial feature.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`write_obj`函数封装了这一逻辑，用来构建我们的3D模型。它首先遍历数据集中的每个边界形状，将顶点及其x、y和z坐标添加到OBJ文件中。对于每一对点（偶数索引表示min_y，奇数索引表示max_y，其中x和z保持不变），该函数写入面定义来连接这些点，形成围绕每个片段的垂直面（例如，围绕顶点0、1、2、3，然后是2、3、4、5，依此类推）。如果边界形状有超过两对顶点，则添加一个闭合面，将最后一对顶点连接回第一对顶点，确保多面体正确封闭。最后，函数通过连接所有min_y顶点和所有max_y顶点，分别为多面体的顶部和底部添加面，从而完成空间特征的3D表示。'
- en: 'Finally, lets make the basic structure of our dataset, with a class that represents
    a dataset (a directory with subdirectories each containing a data record) and
    the data records themselves. This first object has a very simple function: it
    will create a new record object for every sub-directory within ds_dir.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们构建数据集的基本结构，创建一个表示数据集的类（一个包含子目录的目录，每个子目录包含一个数据记录），以及数据记录本身。这个第一个对象有一个非常简单的功能：它会为ds_dir中的每个子目录创建一个新的记录对象。
- en: Accessing 2D Segmentations
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 访问2D分割
- en: Accessing 2D segmentation annotations is easy enough. We must make sure to load
    the json file in annotation2Dfinal. Once that is loaded as a python dict, we can
    extract the segmentation polygons for each object in the scene. These polygons
    are defined by their x and y coordinates, representing the vertices of the polygon
    in the 2D image space.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 访问2D分割注释相对简单。我们必须确保加载annotation2Dfinal中的json文件。一旦它作为python字典加载后，我们可以提取场景中每个物体的分割多边形。这些多边形通过它们的x和y坐标来定义，表示多边形在2D图像空间中的顶点。
- en: We also extract the object label by storing the object ID that each bounding
    shape contains, then cross-referencing with the ‘objects’ list. Both the labels
    and segmentations are returned by `get_segments_2d`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还通过存储每个边界形状包含的物体ID来提取物体标签，然后与“objects”列表进行交叉引用。标签和分割都由`get_segments_2d`返回。
- en: Note that the transpose operation is applied to the coordinates array to shift
    the data from a shape that groups all x coordinates together and all y coordinates
    together into a shape that groups each pair of x and y coordinates together as
    individual points.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，转置操作应用于坐标数组，以将数据从一种将所有x坐标分组在一起并将所有y坐标分组在一起的形状，转换为一种将每对x和y坐标作为单独的点组合在一起的形状。
- en: Accessing 3D Bounding Shapes
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 访问3D边界形状
- en: Accessing the 3D bounding shapes is a bit harder. As mentioned before, they
    are stored as y-axis aligned polyhedra (x is left-right, z is forward-back, y
    is up-down). In the JSON, this is stored as a polygon with an min_y and max_y.
    This can be extracted to a polyhedron by taking each 2D point of the polygon,
    and adding two new 3D points with min_y and max_y.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 访问3D边界形状稍微困难一些。如前所述，它们以y轴对齐的多面体形式存储（x是左右，z是前后，y是上下）。在JSON中，这被存储为一个具有min_y和max_y的多边形。可以通过提取每个2D点的多边形，并添加两个带有min_y和max_y的新3D点，将其转换为多面体。
- en: The JSON also provides a useful field which states whether the bounding shape
    is rectangular. I have preserved this in our code, along with functions to get
    the type of each object (couch, chair, desk, etc), and the total number of objects
    visible in the scene.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: JSON还提供了一个有用的字段，说明边界形状是否是矩形。我已将其保留在我们的代码中，并提供了函数来获取每个物体的类型（沙发、椅子、桌子等）以及场景中可见物体的总数。
- en: Accessing the Room Layout
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 访问房间布局
- en: Finally, the room layout has its own polyhedron that encapsulates all others.
    This can be used by algorithms to understand the broader topology of the room
    including the walls, ceiling, and floor. It is accessed in much the same way as
    the other bounding shapes.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，房间布局有一个独立的多面体，包围了所有其他物体。算法可以使用这个多面体来理解房间的整体拓扑，包括墙壁、天花板和地板。它的访问方式与其他边界形状类似。
- en: Full Code
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 完整代码
- en: Below is the full code with a short testing section. Besides visualizing the
    2D annotations from one of the data records, we also save 3d .obj files for each
    identified object in the scene. You can use a program like [meshlab](https://www.meshlab.net/)
    to visualize the output. The sensor intrinsics and extrinsics have also been extracted
    here. Intrinsics refer to the internal camera parameters that affect the imaging
    process (like focal length, optical center, and lens distortion), while extrinsics
    describe the camera’s position and orientation in a world coordinate system. They
    are important for accurately mapping and interpreting 3D scenes from 2D images.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是完整的代码以及一个简短的测试部分。除了可视化来自数据记录之一的2D注释外，我们还为场景中每个识别出的物体保存3D .obj文件。你可以使用类似[meshlab](https://www.meshlab.net/)这样的程序来可视化输出。传感器的内参和外参也已经在此提取。内参是指影响成像过程的相机内部参数（如焦距、光学中心和镜头畸变），而外参描述的是相机在世界坐标系中的位置和方向。它们对于从2D图像准确映射和解释3D场景非常重要。
- en: 'Code is also available here: [https://github.com/arcosin/Sun-RGDB-Data-Extractor](https://github.com/arcosin/Sun-RGDB-Data-Extractor).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 代码也可以在这里找到：[https://github.com/arcosin/Sun-RGDB-Data-Extractor](https://github.com/arcosin/Sun-RGDB-Data-Extractor)。
- en: This repo may or may not be updated in the future. I would love to add functionality
    for accessing this as a PyTorch dataset with minibatches and such. If anyone has
    some easy updates, feel free to make a PR.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个仓库未来可能会更新，也可能不会。我很想添加一些功能，比如将其作为PyTorch数据集访问，并支持小批量数据等。如果有人有简单的更新，欢迎提交PR。
- en: '![](../Images/2d0abf02fbb23ee0cbd227d843dd925a.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d0abf02fbb23ee0cbd227d843dd925a.png)'
- en: 'Left: the simple 3D representation of the scene shown in meshlab. Note the
    transparent room bounding shape and the many objects represented as boxes. Right:
    the original image.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 左图：在meshlab中展示的简单3D场景表示。请注意透明的房间边界形状以及许多以框表示的物体。右图：原始图像。
- en: Conclusion
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: I hope this guide has been helpful in showing you how to use the Sun RGB-D Dataset.
    More importantly, I hope it’s given you a peek into the broader skill of writing
    quick and easy code to access datasets. Having a tool ready to go is great, but
    understanding how that tool works and getting familiar with the dataset’s structure
    will serve you better in most cases.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这篇指南能帮助你了解如何使用Sun RGB-D数据集。更重要的是，我希望它让你窥见了编写快速简易代码访问数据集的更广泛技能。准备好工具很重要，但了解这些工具的工作原理并熟悉数据集的结构，在大多数情况下能给你带来更多帮助。
- en: Extra Notes
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附加说明
- en: This article has introduced some easy-to-modify python code for extracting data
    from the Sun RGB-D dataset. Note that [an official MATLAB toolbox](https://rgbd.cs.princeton.edu/data/SUNRGBDtoolbox.zip)
    for this dataset already exists. But I don’t use MATLAB so I didn’t look at it.
    If you are a MATLABer (MATLABster? MATLABradour? eh…) then that might be more
    comprehensive.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了一些易于修改的Python代码，用于从Sun RGB-D数据集中提取数据。请注意，已经存在[一个官方MATLAB工具箱](https://rgbd.cs.princeton.edu/data/SUNRGBDtoolbox.zip)用于该数据集。但由于我不使用MATLAB，因此没有查看过它。如果你是MATLAB用户（MATLABer？MATLABster？MATLABradour？哦……）那么这个工具箱可能更为全面。
- en: I also found [**this**](https://github.com/luiszeni/SUNRGBDtoolbox_python) for
    python. It’s a good example of extracting only the 2D features. I borrowed some
    lines from it, so go throw it a star if you feel up to it.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我还发现了[**这个**](https://github.com/luiszeni/SUNRGBDtoolbox_python)适用于Python的工具。它是一个提取仅包含2D特征的好例子。我借用了其中的一些代码，如果你觉得不错，给它点个星吧。
- en: '**References**'
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: This article utilizes the Sun RGB-D dataset [1] licensed under [CC-BY-SA](https://paperswithcode.com/dataset/sun-rgb-d).
    This dataset also draws data from previous work [2, 3, 4]. Thank you to them for
    their outstanding contributions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 本文使用了受[CC-BY-SA](https://paperswithcode.com/dataset/sun-rgb-d)许可的Sun RGB-D数据集[1]。该数据集还引用了之前的工作[2,
    3, 4]。感谢他们为此做出的杰出贡献。
- en: '[1] S. Song, S. Lichtenberg, and J. Xiao, “SUN RGB-D: A RGB-D Scene Understanding
    Benchmark Suite,” Proceedings of the 28th IEEE Conference on Computer Vision and
    Pattern Recognition (CVPR2015), Oral Presentation.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] S. Song, S. Lichtenberg, 和 J. Xiao, “SUN RGB-D：一个RGB-D场景理解基准套件”，第28届IEEE计算机视觉与模式识别会议（CVPR2015）论文集，口头报告。'
- en: '[2] N. Silberman, D. Hoiem, P. Kohli, R. Fergus, “Indoor segmentation and support
    inference from RGBD images,” ECCV, 2012.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] N. Silberman, D. Hoiem, P. Kohli, R. Fergus, “从RGBD图像中进行室内分割和支撑推断”，ECCV，2012年。'
- en: '[3] A. Janoch, S. Karayev, Y. Jia, J. T. Barron, M. Fritz, K. Saenko, T. Darrell,
    “A category-level 3-D object dataset: Putting the Kinect to work,” ICCV Workshop
    on Consumer Depth Cameras for Computer Vision, 2011.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] A. Janoch, S. Karayev, Y. Jia, J. T. Barron, M. Fritz, K. Saenko, T. Darrell,
    “一个类别级别的3D物体数据集：将Kinect投入实际应用”，ICCV工作坊：计算机视觉中的消费深度摄像头，2011年。'
- en: '[4] J. Xiao, A. Owens, A. Torralba, “SUN3D: A database of big spaces reconstructed
    using SfM and object labels,” ICCV, 2013.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] J. Xiao, A. Owens, A. Torralba, “SUN3D：一个基于SfM和物体标签重建的大空间数据库”，ICCV，2013年。'
