- en: 'How OpenAI’s Sora is Changing the Game: An Insight into Its Core Technologies'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI的Sora如何改变游戏规则：深入了解其核心技术
- en: 原文：[https://towardsdatascience.com/how-openais-sora-is-changing-the-game-an-insight-into-its-core-technologies-bd1ad17170df?source=collection_archive---------4-----------------------#2024-02-19](https://towardsdatascience.com/how-openais-sora-is-changing-the-game-an-insight-into-its-core-technologies-bd1ad17170df?source=collection_archive---------4-----------------------#2024-02-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-openais-sora-is-changing-the-game-an-insight-into-its-core-technologies-bd1ad17170df?source=collection_archive---------4-----------------------#2024-02-19](https://towardsdatascience.com/how-openais-sora-is-changing-the-game-an-insight-into-its-core-technologies-bd1ad17170df?source=collection_archive---------4-----------------------#2024-02-19)
- en: A masterpiece of state of the art technologies
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一项代表前沿技术的杰作
- en: '[](https://rkiuchir.medium.com/?source=post_page---byline--bd1ad17170df--------------------------------)[![Ryota
    Kiuchi, Ph.D.](../Images/5459c434848898345d932320c4a01312.png)](https://rkiuchir.medium.com/?source=post_page---byline--bd1ad17170df--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--bd1ad17170df--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--bd1ad17170df--------------------------------)
    [Ryota Kiuchi, Ph.D.](https://rkiuchir.medium.com/?source=post_page---byline--bd1ad17170df--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://rkiuchir.medium.com/?source=post_page---byline--bd1ad17170df--------------------------------)[![Ryota
    Kiuchi, Ph.D.](../Images/5459c434848898345d932320c4a01312.png)](https://rkiuchir.medium.com/?source=post_page---byline--bd1ad17170df--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--bd1ad17170df--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--bd1ad17170df--------------------------------)
    [Ryota Kiuchi, Ph.D.](https://rkiuchir.medium.com/?source=post_page---byline--bd1ad17170df--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--bd1ad17170df--------------------------------)
    ·12 min read·Feb 19, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--bd1ad17170df--------------------------------)
    ·12分钟阅读·2024年2月19日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4045008b97b49145c5838da473557a91.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4045008b97b49145c5838da473557a91.png)'
- en: Photo by [Kaushik Panchal](https://unsplash.com/@kaushikpanchal?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Kaushik Panchal](https://unsplash.com/@kaushikpanchal?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: On February 15, 2024, OpenAI, which had astonished the world by announcing ChatGPT
    in late 2022, once again stunned the world with the unveiling of Sora. This technology,
    capable of creating videos up to a minute long from a text prompt, is undeniably
    set to be a breakthrough.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 2024年2月15日，OpenAI在2022年底震惊世界发布ChatGPT之后，再次通过揭示Sora震撼全球。这项技术能够根据文本提示生成最长达一分钟的视频，毫无疑问，它将成为一次突破。
- en: In this blog post, I will introduce the underlying methodologies and research
    behind this astonishing technology, based on the technical report released by
    OpenAI.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我将基于OpenAI发布的技术报告，介绍这项令人惊叹的技术背后的基础方法和研究。
- en: Incidentally, “Sora” means “sky” in Japanese. Although it has not been officially
    announced whether this naming was intentional, it is speculated to be so, given
    that their official release tweet featured a video themed around Tokyo.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，“Sora”在日语中意味着“天空”。尽管官方尚未宣布这一命名是否有意为之，但据推测可能是有意的，因为他们的官方发布推文中有一段以东京为主题的视频。
- en: OpenAI unveils the Sora to the world via X
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI通过X平台向全球发布Sora
- en: Table of Contents
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目录
- en: '[About Sora](#bfb1)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[关于Sora](#bfb1)'
- en: '[What kind of technology and research is behind it?](#7e7b)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[它背后是什么样的技术和研究？](#7e7b)'
- en: '[The capabilities enabled by these research efforts for Sora](#7b12)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[这些研究努力为Sora带来的能力](#7b12)'
- en: '[The future of Sora](#642d)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Sora的未来](#642d)'
- en: About Sora
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于Sora
- en: Sora is an advanced text-to-video conversion model developed by OpenAI, and
    its capabilities and application range illustrate a new horizon in modern AI technology.
    This model is not limited to generating mere seconds of video; it can create videos
    up to one minute long, maintaining high visual quality while faithfully reproducing
    user instructions. It’s as if it’s bringing dreams to life.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Sora是OpenAI开发的先进文本到视频转换模型，其能力和应用范围展示了现代AI技术的新视野。这个模型不仅限于生成几秒钟的视频；它可以创建最长一分钟的视频，在保持高视觉质量的同时，忠实地再现用户指令。就像是将梦想变为现实。
- en: OpenAI Sora’s demo via X
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Sora通过X平台进行演示
- en: '**Generating Complex Scenes Based on the Real World**'
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**基于现实世界生成复杂场景**'
- en: Sora understands how elements described in prompts exist and operate within
    the physical world. This allows the model to accurately represent user-intended
    movements and actions within videos. For example, it can realistically recreate
    the sight of a person running or the movement of natural phenomena. Furthermore,
    it reproduces precise details of multiple characters, types of movement, and the
    specifics of subjects and backgrounds.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Sora 理解提示中描述的元素如何在物理世界中存在和运作。这使得模型能够准确地呈现用户意图的动作和视频中的运动。例如，它可以逼真地重现一个人奔跑的场景或自然现象的运动。此外，它还能够再现多个角色、各种运动类型以及主题和背景的精确细节。
- en: Previously, video creation with Generative AI has faced the difficult challenge
    of maintaining consistency and reproducibility across different scenes. This is
    because understanding previous contexts and details completely when generating
    each scene or frame individually and appropriately inheriting them to the next
    scene is challenging. However, this model maintains narrative consistency by combining
    a deep understanding of language with visual context and interpreting prompts
    accurately. It can also capture the emotions and personalities of characters from
    the given prompts and portray them as expressive characters within the video.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，使用生成性 AI 创建视频面临着在不同场景之间保持一致性和可重现性的巨大挑战。这是因为在生成每个场景或帧时，完全理解先前的上下文和细节，并将其适当传递到下一个场景是困难的。然而，这个模型通过将对语言的深刻理解与视觉上下文结合，准确解释提示，从而保持叙事一致性。它还能够捕捉角色的情感和个性，并在视频中呈现为富有表现力的角色。
- en: The post by Bill Peebles (OpenAI) via X
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Bill Peebles（OpenAI）通过 X 发布的帖子
- en: What kind of technology and research is behind it?
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它背后是什么样的技术和研究？
- en: '![](../Images/5855f8b4ebf6f0eabb6f06a22673761c.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5855f8b4ebf6f0eabb6f06a22673761c.png)'
- en: Photo by [Markus Spiske](https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由[Markus Spiske](https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral)拍摄，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Sora is built upon a foundation of prior studies in image data generation modeling.
    Previous research has employed various methods such as recurrent networks, Generative
    Adversarial Networks (GANs), autoregressive transformers, and diffusion models,
    but has often focused on a narrow category of visual data, shorter videos, or
    videos of a fixed size. Sora surpasses these limitations and has been significantly
    improved to generate videos across diverse durations, aspect ratios, and resolutions.
    In this section, I will introduce the core technologies that support these advancements.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Sora 是建立在先前图像数据生成建模研究的基础之上的。先前的研究使用了各种方法，如递归网络、生成对抗网络（GANs）、自回归 Transformer
    和扩散模型，但通常集中于狭窄类别的视觉数据、较短的视频或固定尺寸的视频。Sora 超越了这些局限，并在生成各种时长、纵横比和分辨率的视频方面取得了显著进展。在这一部分，我将介绍支持这些进展的核心技术。
- en: 1\. Transformer
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. Transformer
- en: Vaswani et al. (2017), “Attention is all you need.”
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Vaswani 等人（2017），《Attention is all you need》
- en: The Transformer model is a neural network architecture that revolutionized the
    field of natural language processing (NLP). It was first proposed by Vaswani et
    al. in 2017\. This model significantly overcame the challenges that traditional
    Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) faced,
    supporting various breakthrough technologies as an innovative method today.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型是一种神经网络架构，彻底改变了自然语言处理（NLP）领域。它由 Vaswani 等人于 2017 年首次提出。该模型显著克服了传统递归神经网络（RNNs）和卷积神经网络（CNNs）面临的挑战，作为一种创新方法，今天支持了多种突破性的技术。
- en: '![](../Images/2295b7bce37a22b9414d8777b53f8951.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2295b7bce37a22b9414d8777b53f8951.png)'
- en: 'Figure 1: The Transformer — model architecture. ｜Vaswani et al. (2017)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：Transformer——模型架构。｜Vaswani 等人（2017）
- en: 'Issues with RNNs:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs的问题：
- en: 'The problem of long-term dependencies: Although RNNs theoretically can transmit
    information through time, they struggle to capture dependencies over long durations
    in practice.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长期依赖问题：尽管 RNNs 理论上可以通过时间传递信息，但在实践中它们难以捕捉长时间跨度的依赖关系。
- en: 'Limitations on parallelization: Since the computation at each step in an RNN
    depends on the output of the previous step, sequential processing (e.g., processing
    words or sentences in a text one by one, in order) is mandatory, preventing the
    utilization of parallel processing advantages offered by modern computer architectures.
    This made training on large datasets inefficient.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行化的局限性：由于RNN中每一步的计算依赖于前一步的输出，因此必须进行顺序处理（例如，逐一处理文本中的单词或句子），无法利用现代计算机架构所提供的并行处理优势。这使得在大规模数据集上的训练效率低下。
- en: 'Issues with CNNs:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的问题：
- en: 'Fixed receptive field size: While CNNs excel at extracting local features,
    their fixed receptive field size limits their ability to capture long-distance
    dependencies throughout the context.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 固定感受野大小：虽然CNN在提取局部特征方面表现优秀，但其固定的感受野大小限制了其捕捉全局上下文中长距离依赖的能力。
- en: 'Difficulty in modeling the hierarchical structure of natural language: It’s
    challenging to directly model the hierarchical structure of language, which can
    be insufficient for deep contextual understanding.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言层级结构建模的困难：直接建模语言的层级结构具有挑战性，而这对于深层次的上下文理解可能不够充分。
- en: 'New features of the Transformer:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer的新特性：
- en: 'Attention Mechanism: Enables direct modeling of dependencies between any positions
    in the sequence, allowing for the direct capture of long dependencies and extensive
    context.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力机制：使得能够直接建模序列中任意位置之间的依赖关系，从而直接捕捉长距离依赖和广泛的上下文。
- en: 'Realization of parallelization: Since the input data is processed as a whole
    at once, a high degree of parallelization in computation is achieved, significantly
    accelerating training on large datasets.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行化的实现：由于输入数据一次性整体处理，实现了高度的并行计算，大大加速了在大规模数据集上的训练。
- en: 'Variable receptive field: The attention mechanism allows the model to dynamically
    adjust the “receptive field” size as needed. This means the model can naturally
    focus on local information for certain tasks or data, and consider broader context
    in other cases.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可变感受野：注意力机制使得模型能够根据需要动态调整“感受野”的大小。这意味着模型可以在某些任务或数据中自然地聚焦于局部信息，而在其他情况下考虑更广泛的上下文。
- en: '*For more detailed technical explanations about Transformer:*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*更多关于Transformer的详细技术解释：*'
- en: '[](/transformers-141e32e69591?source=post_page-----bd1ad17170df--------------------------------)
    [## How Transformers Work'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/transformers-141e32e69591?source=post_page-----bd1ad17170df--------------------------------)
    [## Transformer的工作原理'
- en: Transformers are a type of neural network architecture that have been gaining
    popularity. Transformers were recently…
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Transformer是一种神经网络架构，近年来越来越受到关注。Transformer最近…
- en: towardsdatascience.com](/transformers-141e32e69591?source=post_page-----bd1ad17170df--------------------------------)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/transformers-141e32e69591?source=post_page-----bd1ad17170df--------------------------------)
- en: 2\. Vision Transformer (ViT)
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 视觉Transformer（ViT）
- en: 'Dosovitskiy, et al. (2020), “An image is worth 16x16 words: Transformers for
    image recognition at scale.”'
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Dosovitskiy等人（2020年），“一张图等于16x16个词：用于大规模图像识别的Transformer。”
- en: In this study, the principles of the Transformer, which revolutionized natural
    language processing (NLP), are applied to image recognition, opening up new horizons.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究将革命性地改变自然语言处理（NLP）的Transformer原理应用于图像识别，开辟了新的视野。
- en: '**Token and Patch**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**Token和Patch**'
- en: In the original Transformer paper, tokens primarily represent parts of words
    or sentences, and analyzing the relationships between these tokens allows for
    a deep understanding of the sentence’s meaning. In this study, to apply this concept
    of tokens to visual data, images are divided into small sections (patches) of
    16x16, and each patch is treated as a “token” within the Transformer. This approach
    enables the model to learn how each patch is related within the entire image,
    allowing for the recognition and understanding of the entire image based on this.
    It surpasses the limitations of the fixed receptive field size of traditional
    CNN models used in image recognition, enabling flexible capture of any positional
    relationships within an image.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始的Transformer论文中，tokens主要代表单词或句子的部分，分析这些tokens之间的关系可以深入理解句子的含义。在本研究中，为了将这种token的概念应用于视觉数据，图像被划分为16x16的小块（patches），每个小块被视为Transformer中的一个“token”。这种方法使得模型能够学习每个小块在整个图像中的关系，从而基于此进行整个图像的识别和理解。它突破了传统CNN模型在图像识别中固定感受野大小的局限性，实现了在图像内任意位置关系的灵活捕捉。
- en: '![](../Images/4b9ab7bd066f9cd9c49e6815c9b9752f.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b9ab7bd066f9cd9c49e6815c9b9752f.png)'
- en: 'Figure 1: Model overview. ｜Dosovitskiy, et al. (2020)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：模型概述。｜Dosovitskiy 等人（2020）
- en: '*For more detailed technical explanations about ViT:*'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*关于ViT的更详细技术解释：*'
- en: '[https://machinelearningmastery.com/the-vision-transformer-model/](https://machinelearningmastery.com/the-vision-transformer-model/)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://machinelearningmastery.com/the-vision-transformer-model/](https://machinelearningmastery.com/the-vision-transformer-model/)'
- en: 3\. Video Vision Transformer (ViViT)
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. 视频视觉变换器（ViViT）
- en: 'Arnab, et al. (2021), “Vivit: A video vision transformer.”'
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Arnab 等人（2021），“Vivit: A video vision transformer.”'
- en: ViViT further extends the concept of the Vision Transformer, applying it to
    the multidimensional data of videos. Video data is more complex as it contains
    both static image information (spatial elements) and dynamic information that
    changes over time (temporal elements). ViViT decomposes videos into spatiotemporal
    patches, treating these as tokens within the Transformer model. With the introduction
    of spatiotemporal patches, ViViT is able to simultaneously capture both static
    and dynamic elements within a video and model the complex relationships between
    them.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ViViT进一步扩展了视觉变换器的概念，将其应用于视频的多维数据。视频数据更为复杂，因为它既包含静态图像信息（空间元素），又包含随时间变化的动态信息（时间元素）。ViViT将视频分解为时空补丁，并将其视为变换器模型中的令牌。通过引入时空补丁，ViViT能够同时捕捉视频中的静态和动态元素，并建模它们之间的复杂关系。
- en: '![](../Images/24cf64ec81ec07779af159c87eae68d6.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/24cf64ec81ec07779af159c87eae68d6.png)'
- en: 'Figure 3: Tubelet (the spatio-temporal input volume) embedding image. ｜Arnab,
    et al. (2021)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：Tubelet（时空输入体积）嵌入图像。｜Arnab 等人（2021）
- en: '*For more detailed technical explanations about ViViT:*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*关于ViViT的更详细技术解释：*'
- en: '[](https://medium.com/aiguys/vivit-video-vision-transformer-648a5fff68a4?source=post_page-----bd1ad17170df--------------------------------)
    [## ViViT 📹 Video Vision Transformer'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/aiguys/vivit-video-vision-transformer-648a5fff68a4?source=post_page-----bd1ad17170df--------------------------------)
    [## ViViT 📹 视频视觉变换器'
- en: ICCV 2021 ✨, Google Research
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ICCV 2021 ✨，谷歌研究
- en: medium.com](https://medium.com/aiguys/vivit-video-vision-transformer-648a5fff68a4?source=post_page-----bd1ad17170df--------------------------------)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/aiguys/vivit-video-vision-transformer-648a5fff68a4?source=post_page-----bd1ad17170df--------------------------------)
- en: 4\. Masked Autoencoders (MAE)
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4. Masked Autoencoders (MAE)
- en: He, et al. (2022), “Masked autoencoders are scalable vision learners.”
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: He 等人（2022），“Masked autoencoders are scalable vision learners.”
- en: This study dramatically improved the traditionally high computational costs
    and inefficiencies in training on large datasets associated with high dimensionality
    and vast amounts of information, using a self-supervised pre-training method called
    the Masked Autoencoder.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究通过使用一种自监督预训练方法，称为Masked Autoencoder，显著提高了传统上在高维度和大量信息数据集上训练时的高计算成本和低效率。
- en: Specifically, by masking parts of the input image, the network is trained to
    predict the information of the hidden parts, resulting in more efficient learning
    of important features and structures within the image, and acquiring rich representations
    of visual data. This process has made the compression and representation learning
    of data more efficient, reduced computational costs, and enhanced the versatility
    of different types of visual data and tasks.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，通过对输入图像的部分区域进行遮挡，网络被训练去预测隐藏部分的信息，从而更有效地学习图像中的重要特征和结构，并获得丰富的视觉数据表示。这一过程使得数据的压缩与表示学习变得更加高效，减少了计算成本，并增强了不同类型视觉数据和任务的通用性。
- en: The approach of this study is also closely related to the evolution of language
    models by BERT (Bidirectional Encoder Representations from Transformers). While
    BERT enabled a deep contextual understanding of text data through Masked Language
    Modeling (MLM), He et al. have applied a similar masking technique to visual data,
    achieving a deeper understanding and representation of images.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究的方法与BERT（双向编码器表示的变换器）在语言模型发展中的应用紧密相关。BERT通过Masked Language Modeling（MLM）实现了对文本数据的深层上下文理解，而He等人将类似的遮挡技术应用于视觉数据，从而实现了对图像的更深理解与表示。
- en: '![](../Images/3933fb20cd83083872d34a3afdc4c2c3.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3933fb20cd83083872d34a3afdc4c2c3.png)'
- en: 'Figure 1: Masked Autoencoders Image. ｜He, et al. (2022)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：Masked Autoencoders 图像。｜He 等人（2022）
- en: '*For more detailed technical explanations about MAE:*'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*关于MAE的更详细技术解释：*'
- en: '[](/paper-explained-masked-autoencoders-are-scalable-vision-learners-9dea5c5c91f0?source=post_page-----bd1ad17170df--------------------------------)
    [## Paper explained: Masked Autoencoders Are Scalable Vision Learners'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/paper-explained-masked-autoencoders-are-scalable-vision-learners-9dea5c5c91f0?source=post_page-----bd1ad17170df--------------------------------)
    [## 论文解析：Masked Autoencoders 是可扩展的视觉学习者'
- en: How reconstructing masked parts of an image can be beneficial
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何重建图像中的遮挡部分能带来好处
- en: towardsdatascience.com](/paper-explained-masked-autoencoders-are-scalable-vision-learners-9dea5c5c91f0?source=post_page-----bd1ad17170df--------------------------------)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/paper-explained-masked-autoencoders-are-scalable-vision-learners-9dea5c5c91f0?source=post_page-----bd1ad17170df--------------------------------)'
- en: 5\. Native Resolution Vision Transformer (NaViT)
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5. 本地分辨率视觉 Transformer（NaViT）
- en: 'Dehghani, et al. (2023), “Patch n’Pack: NaViT, a Vision Transformer for any
    Aspect Ratio and Resolution.”'
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Dehghani 等人 (2023)，“Patch n’Pack: NaViT，一种适用于任何纵横比和分辨率的视觉 Transformer。”'
- en: This study proposed the Native Resolution ViTransformer (NaViT), a model designed
    to further expand the applicability of the Vision Transformer (ViT) to images
    of any aspect ratio or resolution.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究提出了本地分辨率 ViTransformer（NaViT），这是一个旨在进一步扩展视觉 Transformer（ViT）应用于任何纵横比或分辨率图像的模型。
- en: '**Challenges of Traditional ViT**'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**传统 ViT 的挑战**'
- en: The Vision Transformer introduced a groundbreaking approach by dividing images
    into fixed-size patches and treating these patches as tokens, applying the transformer
    model to image recognition tasks. However, this approach assumed models optimized
    for specific resolutions or aspect ratios, requiring model readjustment for images
    of different sizes or shapes. This was a significant constraint, as real-world
    applications often need to handle images of diverse sizes and aspect ratios.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉 Transformer 通过将图像分割为固定大小的补丁并将这些补丁作为令牌来处理，从而提出了一种开创性的方法，将 Transformer 模型应用于图像识别任务。然而，这种方法假设模型是针对特定分辨率或纵横比进行优化的，因此需要对不同大小或形状的图像进行模型重新调整。这是一个重大限制，因为实际应用中通常需要处理不同大小和纵横比的图像。
- en: '**Innovations of NaViT**'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**NaViT 的创新**'
- en: NaViT is designed to efficiently process images of any aspect ratio or resolution,
    allowing them to be directly inputted into the model without prior adjustment.
    Sora applies this flexibility to videos as well, significantly enhancing flexibility
    and adaptability by seamlessly handling videos and images of various sizes and
    shapes.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: NaViT 设计旨在高效处理任何纵横比或分辨率的图像，使其能够直接输入模型而无需预先调整。Sora 也将这种灵活性应用于视频，显著提升了灵活性和适应性，能够无缝处理各种大小和形状的图像与视频。
- en: '![](../Images/47627d8a0fd5e4cd491dd460ad0389f2.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47627d8a0fd5e4cd491dd460ad0389f2.png)'
- en: Figure 2:｜Dehghani, et al. (2023)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：｜Dehghani 等人 (2023)
- en: 6\. Diffusion Models
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6. 扩散模型
- en: Sohl-Dickstein, et al. (2015), “Deep unsupervised learning using nonequilibrium
    thermodynamics.”
  id: totrans-86
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Sohl-Dickstein 等人 (2015)，“使用非平衡热力学的深度无监督学习。”
- en: Alongside the Transformer, Diffusion Models form the backbone technology supporting
    Sora. This research laid the theoretical foundation for diffusion models, a deep
    learning model using non-equilibrium thermodynamics. Diffusion models introduced
    the concept of a diffusion process that starts with random noise (data without
    any pattern) and gradually removes this noise to create data resembling actual
    images or videos.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 Transformer，扩散模型也是支撑 Sora 的核心技术之一。此研究为扩散模型奠定了理论基础，扩散模型是一种使用非平衡热力学的深度学习模型。扩散模型引入了一个扩散过程的概念，该过程从随机噪声（没有任何模式的数据）开始，逐步去除噪声，生成类似于实际图像或视频的数据。
- en: For instance, imagine starting with mere random dots, which gradually transform
    into videos of beautiful landscapes or people. This approach was later applied
    to the generation of complex data such as images and sounds, contributing to the
    development of high-quality generative models.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，可以想象从仅仅是随机的点开始，这些点逐渐转变为美丽景观或人物的视频。这种方法后来被应用于生成复杂的数据，如图像和声音，促进了高质量生成模型的发展。
- en: '![](../Images/5b45899f9277252955107f0d698a691c.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b45899f9277252955107f0d698a691c.png)'
- en: Image of denoising process｜Image Credit (OpenAI)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪过程的图像｜图像来源 (OpenAI)
- en: Ho et al. (2020), “Denoising diffusion probabilistic models.”
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Ho 等人 (2020)，“去噪扩散概率模型。”
- en: ''
  id: totrans-92
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Nichol and Dhariwal (2021), “Improved denoising diffusion probabilistic models.”
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Nichol 和 Dhariwal (2021)，“改进的去噪扩散概率模型。”
- en: Building on the theoretical framework by Sohl-Dickstein et al. (2015), practical
    data generation models known as Denoising Diffusion Probabilistic Models (DDPM)
    were developed. This model has shown particularly notable results in high-quality
    image generation, demonstrating the effectiveness of diffusion models.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Sohl-Dickstein 等人（2015年）提出的理论框架，开发了被称为去噪扩散概率模型（DDPM）的实用数据生成模型。该模型在高质量图像生成方面表现出了特别显著的效果，证明了扩散模型的有效性。
- en: '**Impact of Diffusion Models on Sora**'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**扩散模型对Sora的影响**'
- en: Typically, to train machine learning models, a lot of labeled data is needed
    (for example, being told “This is an image of a cat”). However, diffusion models
    can learn from unlabeled data as well, allowing them to utilize the vast amount
    of visual content available on the internet to generate various types of videos.
    In other words, Sora can observe different videos and images and learn “this is
    what a normal video looks like.”
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，训练机器学习模型需要大量的标注数据（例如，被告知“这是一张猫的图片”）。然而，扩散模型也可以从未标注的数据中学习，使其能够利用互联网上海量的视觉内容生成各种类型的视频。换句话说，Sora可以观察不同的视频和图像，并学习“这就是正常视频的样子”。
- en: '*For more detailed technical explanations about Diffusion Models:*'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*有关扩散模型的更详细技术解释：*'
- en: '[](/diffusion-models-made-easy-8414298ce4da?source=post_page-----bd1ad17170df--------------------------------)
    [## Diffusion Models Made Easy'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/diffusion-models-made-easy-8414298ce4da?source=post_page-----bd1ad17170df--------------------------------)
    [## 扩散模型轻松实现'
- en: Understanding the Basics of Denoising Diffusion Probabilistic Models
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解去噪扩散概率模型的基础
- en: towardsdatascience.com](/diffusion-models-made-easy-8414298ce4da?source=post_page-----bd1ad17170df--------------------------------)
    [](/understanding-the-denoising-diffusion-probabilistic-model-the-socratic-way-445c1bdc5756?source=post_page-----bd1ad17170df--------------------------------)
    [## Understanding the Denoising Diffusion Probabilistic Model, the Socratic Way
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/diffusion-models-made-easy-8414298ce4da?source=post_page-----bd1ad17170df--------------------------------)
    [](/understanding-the-denoising-diffusion-probabilistic-model-the-socratic-way-445c1bdc5756?source=post_page-----bd1ad17170df--------------------------------)
    [## 通过苏格拉底式方法理解去噪扩散概率模型
- en: A deep dive into the motivation behind the denoising diffusion model and detailed
    derivations for the loss function
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深入探讨去噪扩散模型背后的动机以及损失函数的详细推导
- en: towardsdatascience.com](/understanding-the-denoising-diffusion-probabilistic-model-the-socratic-way-445c1bdc5756?source=post_page-----bd1ad17170df--------------------------------)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/understanding-the-denoising-diffusion-probabilistic-model-the-socratic-way-445c1bdc5756?source=post_page-----bd1ad17170df--------------------------------)
- en: 7\. Latent Diffusion Models
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 潜在扩散模型
- en: Rombach, et al. (2022), “High-resolution image synthesis with latent diffusion
    models.”
  id: totrans-104
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Rombach 等人（2022年），《使用潜在扩散模型进行高分辨率图像合成》
- en: This research has made a significant contribution to the field of high-resolution
    image synthesis using diffusion models. It proposes a method that significantly
    reduces computational costs compared to direct high-resolution image generation
    by utilizing diffusion models in the latent space while maintaining quality. In
    other words, instead of directly manipulating images, it demonstrates that by
    encoding and introducing the diffusion process to data represented in the latent
    space (a lower-dimensional space holding compressed representations of images),
    it is possible to achieve with fewer computational resources.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 该研究在使用扩散模型进行高分辨率图像合成领域作出了重要贡献。它提出了一种方法，通过在潜在空间中利用扩散模型，在保持质量的同时显著降低了与直接高分辨率图像生成相比的计算成本。换句话说，它展示了通过对潜在空间（一个表示图像压缩表示的低维空间）中的数据进行编码并引入扩散过程，而不是直接操作图像，可以用更少的计算资源实现目标。
- en: Sora applies this technology to video data, compressing the temporal+spatial
    data of videos into a lower-dimensional latent space, and then undergoing a process
    of decomposing it into spatiotemporal patches. This efficient data processing
    and generation capability in the latent space plays a crucial role in enabling
    Sora to generate higher-quality visual content more rapidly.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Sora将这一技术应用于视频数据，将视频的时空数据压缩到低维潜在空间，然后经过一系列的过程将其分解为时空块。这种在潜在空间中进行高效数据处理和生成的能力对于Sora能够更快速地生成更高质量的视觉内容起着至关重要的作用。
- en: '![](../Images/3f152224408eb5cdaf418cf2567a2045.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f152224408eb5cdaf418cf2567a2045.png)'
- en: Image of visual encoding｜Image Credit (OpenAI)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉编码图｜图片来源（OpenAI）
- en: '*For more detailed technical explanations about Latent Diffusion Models:*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*关于潜在扩散模型的更多技术细节：*'
- en: '[](/paper-explained-high-resolution-image-synthesis-with-latent-diffusion-models-f372f7636d42?source=post_page-----bd1ad17170df--------------------------------)
    [## Paper Explained — High-Resolution Image Synthesis with Latent Diffusion Models'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/paper-explained-high-resolution-image-synthesis-with-latent-diffusion-models-f372f7636d42?source=post_page-----bd1ad17170df--------------------------------)
    [## 论文解读 — 基于潜在扩散模型的高分辨率图像合成'
- en: While OpenAI has dominated the field of natural language processing with their
    generative text models, their image…
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 尽管 OpenAI 已凭借其生成文本模型主导了自然语言处理领域，但他们的图像……
- en: towardsdatascience.com](/paper-explained-high-resolution-image-synthesis-with-latent-diffusion-models-f372f7636d42?source=post_page-----bd1ad17170df--------------------------------)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/paper-explained-high-resolution-image-synthesis-with-latent-diffusion-models-f372f7636d42?source=post_page-----bd1ad17170df--------------------------------)
- en: 8\. Diffusion Transformer (DiT)
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8. 扩散变压器（DiT）
- en: Peebles and Xie. (2023), “Scalable diffusion models with transformers.”
  id: totrans-114
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Peebles 和 Xie. (2023)，《具有变压器的可扩展扩散模型》。
- en: This research might be the most crucial in realizing Sora. As mentioned in the
    technical report published by OpenAI, Sora employs not a vanilla (normal) transformer
    but a diffusion transformer (DiT).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这项研究可能是实现 Sora 的最关键研究。如 OpenAI 发布的技术报告中所提到，Sora 并不是采用普通的（标准）变压器，而是采用了扩散变压器（DiT）。
- en: Importantly, Sora is a diffusion *transformer*. (via OpenAI Sora technical report)
  id: totrans-116
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 重要的是，Sora 是一种扩散 *变压器*。（来源：OpenAI Sora 技术报告）
- en: The study introduced a new model that replaces the U-net component, commonly
    used in diffusion models, with a Transformer structure. This structure enables
    the Latent Diffusion Model through operations on latent patches by the Transformer.
    This approach allows for more efficient handling of image patches, enabling the
    generation of high-quality images while effectively utilizing computational resources.
    The incorporation of this Transformer, which differs from the Stable Diffusion
    announced by Stability AI in 2022, is considered to contribute to more natural
    video generation.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 该研究引入了一种新的模型，该模型用变压器结构替代了扩散模型中常用的 U-net 组件。通过对潜在图像块进行变压器操作，这种结构使潜在扩散模型成为可能。该方法使得图像块的处理更加高效，能够生成高质量的图像，同时有效利用计算资源。这一变压器的引入，与
    Stability AI 在 2022 年发布的 Stable Diffusion 不同，被认为有助于实现更加自然的视频生成。
- en: '![](../Images/54dc92e3b369086a4a28bb343998d748.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/54dc92e3b369086a4a28bb343998d748.png)'
- en: 'Figure 1: Generated images by the Diffusion Transformers｜Peebles and Xie. (2023)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：扩散变压器生成的图像｜Peebles 和 Xie. (2023)
- en: Furthermore, it’s important to note that their validation results demonstrate
    the scalability of DiT, significantly contributing to the realization of Sora.
    Being scalable means that the model’s performance improves with an increase in
    the transformer’s depth/width (making the model more complex) or the number of
    input tokens.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，值得注意的是，他们的验证结果证明了 DiT 的可扩展性，显著促进了 Sora 的实现。可扩展性意味着模型的性能会随着变压器深度/宽度的增加（使模型更复杂）或输入标记数量的增加而提升。
- en: '![](../Images/38de39d61eef4b8bfb6e62a3d800e8df.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38de39d61eef4b8bfb6e62a3d800e8df.png)'
- en: 'Figure 8 & 9: Scalability of the Diffusion Transformers｜Peebles and Xie. (2023)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8 和 9：扩散变压器的可扩展性｜Peebles 和 Xie. (2023)
- en: 'Gflops (Computational performance): A unit of measure for a computer’s calculating
    speed equal to one billion floating-point operations per second. In this paper,
    network complexity is measured by Gflops.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gflops（计算性能）：衡量计算机运算速度的单位，等于每秒十亿次浮点运算。在本文中，网络复杂度通过 Gflops 来衡量。
- en: 'FID (Fréchet Inception Distance): One of the evaluation metrics for image generation,
    where a lower value indicates higher accuracy. It quantitatively assesses the
    quality of generated images by measuring the distance between the feature vectors
    of generated images and real images.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FID（Fréchet 生成距离）：图像生成的评价指标之一，值越低表示准确度越高。它通过测量生成图像和真实图像之间特征向量的距离，定量评估生成图像的质量。
- en: This has already been observed in the field of natural language processing,
    as confirmed by Kaplan et al. (2020) and Brown et al. (2020), supporting the crucial
    characteristics behind the innovative success of ChatGPT.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点在自然语言处理领域已经被观察到，如 Kaplan 等人（2020）和 Brown 等人（2020）所确认，支持了 ChatGPT 创新成功背后的关键特征。
- en: Kaplan et al. (2020), “Scaling Laws for Neural Language Models.”
  id: totrans-126
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Kaplan 等人（2020），《神经语言模型的扩展法则》。
- en: ''
  id: totrans-127
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Brown, et al. (2020), “Language models are few-shot learners.”
  id: totrans-128
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Brown等人（2020年），《语言模型是少量学习者》
- en: This significant feature, in addition to generating high-quality images at a
    lower computational cost than traditional diffusion models due to the benefits
    of the Transformer, indicates that even higher-quality images can be produced
    with larger computational resources. Sora applies this technology to video generation.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这一显著特点，除了由于Transformer的优势在较低计算成本下生成高质量图像之外，还表明，通过更大的计算资源，甚至可以生成更高质量的图像。Sora将这一技术应用于视频生成。
- en: '![](../Images/2d63e2c57fef03126f80016284ea38cb.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d63e2c57fef03126f80016284ea38cb.png)'
- en: Scalability of video generation｜Image Credit (OpenAI)
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 视频生成的可扩展性｜图像来源（OpenAI）
- en: '*For more detailed technical explanations about DiT:*'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '*关于DiT的更详细技术解释：*'
- en: Review the paper of DiT by hu-po via YouTube
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 通过YouTube观看DiT的论文评审
- en: The capabilities enabled by these research efforts for Sora
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这些研究成果为Sora提供的能力
- en: '**Variable durations, resolutions, aspect ratios**'
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**可变的持续时间、分辨率、纵横比**'
- en: Primarily, thanks to NaViT, Sora can sample widescreen 1920x1080p videos, vertical
    1080x1920 videos, and everything in between. This means it can create visuals
    for various device types at any resolution.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 主要得益于NaViT，Sora可以采样宽屏1920x1080p视频、垂直1080x1920视频以及介于两者之间的所有内容。这意味着它能够为各种设备类型在任何分辨率下创建视觉效果。
- en: '**Prompting with images and videos**'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用图像和视频进行提示**'
- en: Currently, the videos generated by Sora, as demonstrated, are created in a text-to-video
    format, where instructions are given through text prompts. However, as can be
    easily anticipated from the previous research, it’s also possible to use existing
    images or videos as inputs, not just text. This allows for the animation of images
    or for Sora to imagine and output the past or future of an existing video as visuals.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，Sora生成的视频，如演示所示，是以文本到视频的格式创建的，其中指令通过文本提示给出。然而，正如从之前的研究中可以轻易预见的那样，也可以使用现有的图像或视频作为输入，而不仅仅是文本。这使得图像的动画化成为可能，或者Sora可以想象并输出现有视频的过去或未来作为视觉效果。
- en: '**3D consistency**'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**三维一致性**'
- en: While it’s not clear how the aforementioned research is directly involved, Sora
    can generate videos with dynamic camera motion. As the camera shifts and rotates,
    people and scene elements move consistently through three-dimensional space.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管尚不清楚上述研究如何直接参与其中，但Sora能够生成带有动态摄像机运动的视频。当摄像机移动和旋转时，人物和场景元素会在三维空间中一致地移动。
- en: The future of Sora
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sora的未来
- en: In this blog post, I have explained the technologies behind OpenAI’s AI for
    generating videos, Sora, which has already shocked the world. Once it becomes
    publicly available and accessible to a wider audience, it is bound to make an
    even more significant impact worldwide.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博文中，我已经解释了OpenAI用于生成视频的AI技术——Sora，这项技术已经震惊了全世界。一旦它公开发布并能被更广泛的用户访问，它势必将在全球范围内产生更为深远的影响。
- en: The impact of this breakthrough is expected to span across various aspects of
    video creation, but it is predicted that it may likely evolve from video to further
    advancements in 3D modeling. If that becomes the case, not only video creators
    but also the production of visuals in virtual spaces like the metaverse could
    soon be easily generated by AI.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这一突破的影响预计将跨越视频创作的各个方面，但预测它可能从视频演变为三维建模的进一步发展。如果真是如此，不仅是视频创作者，连虚拟空间（如元宇宙）中的视觉内容生产也可能很快由AI轻松生成。
- en: 'The arrival of such a future has already been implied as below:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的未来的到来已经在以下内容中有所暗示：
- en: Martin Nebelong’s post about Micael Rublof’s product via X
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Martin Nebelong通过X发布的关于Micael Rublof产品的帖子
- en: Currently, Sora is perceived as “merely” a video generation model, but Jim Fan
    from Nvidia has implied it might be a data-driven physics engine. This suggests
    the possibility that AI, from a vast amount of real-world videos and (though not
    explicitly mentioned) videos considering physical behaviors like those from Unreal
    Engine, might understand physical laws and phenomena. If so, the emergence of
    text-to-3D in the near future is also highly probable.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，Sora被视为“仅仅”一个视频生成模型，但来自Nvidia的Jim Fan暗示，它可能是一个基于数据的物理引擎。这表明，AI通过大量的现实世界视频以及（尽管未明确提及）像虚幻引擎那样考虑物理行为的视频，可能理解物理定律和现象。如果是这样，未来不久，文本到三维的出现也是高度可能的。
- en: Jim Fan’s intriguing post via X
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Jim Fan通过X发布的引人注目的帖子
- en: '***Thank you so much for reading this article.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '***非常感谢您阅读这篇文章。'
- en: Your clap to this article and subscription to*** [***my newsletter***](https://rkiuchir.medium.com/subscribe)
    ***would motivate me a lot!***
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 你对这篇文章的点赞和对***[***我的新闻通讯***](https://rkiuchir.medium.com/subscribe) ***的订阅会给我带来很大的动力！***
