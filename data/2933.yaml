- en: Training Language Models on Google Colab
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Google Colab上训练语言模型
- en: 原文：[https://towardsdatascience.com/training-language-models-on-google-colab-6e145ff092bf?source=collection_archive---------6-----------------------#2024-12-04](https://towardsdatascience.com/training-language-models-on-google-colab-6e145ff092bf?source=collection_archive---------6-----------------------#2024-12-04)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/training-language-models-on-google-colab-6e145ff092bf?source=collection_archive---------6-----------------------#2024-12-04](https://towardsdatascience.com/training-language-models-on-google-colab-6e145ff092bf?source=collection_archive---------6-----------------------#2024-12-04)
- en: A guide to iterative fine-tuning and serialisation
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迭代微调与序列化指南
- en: '[](https://john-hawkins.medium.com/?source=post_page---byline--6e145ff092bf--------------------------------)[![John
    Hawkins](../Images/4c36312a7b99f0b1b2575fd7184d60b5.png)](https://john-hawkins.medium.com/?source=post_page---byline--6e145ff092bf--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6e145ff092bf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6e145ff092bf--------------------------------)
    [John Hawkins](https://john-hawkins.medium.com/?source=post_page---byline--6e145ff092bf--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://john-hawkins.medium.com/?source=post_page---byline--6e145ff092bf--------------------------------)[![John
    Hawkins](../Images/4c36312a7b99f0b1b2575fd7184d60b5.png)](https://john-hawkins.medium.com/?source=post_page---byline--6e145ff092bf--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6e145ff092bf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6e145ff092bf--------------------------------)
    [John Hawkins](https://john-hawkins.medium.com/?source=post_page---byline--6e145ff092bf--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6e145ff092bf--------------------------------)
    ·5 min read·Dec 4, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6e145ff092bf--------------------------------)
    ·阅读时间5分钟·2024年12月4日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/059f892fb6a9dc178619df35de9be181.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/059f892fb6a9dc178619df35de9be181.png)'
- en: Photo by [Shio Yang](https://unsplash.com/@shioyang?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Shio Yang](https://unsplash.com/@shioyang?utm_source=medium&utm_medium=referral)提供，来源：[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: So, you recently discovered [Hugging Face](https://huggingface.co/) and the
    host of open source models like BERT, Llama, BART and a whole host of generative
    language models by [Mistral AI](https://mistral.ai/), [Facebook](http://facebook.com),
    [Salesforce](https://www.salesforce.com/) and other companies. Now you want to
    experiment with fine tuning some Large Language Models for your side projects.
    Things start off great, but then you discover how computationally greedy they
    are and you do not have a GPU processor handy.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，你最近发现了[Hugging Face](https://huggingface.co/)和一系列开源模型，如BERT、Llama、BART，以及由[Mistral
    AI](https://mistral.ai/)、[Facebook](http://facebook.com)、[Salesforce](https://www.salesforce.com/)等公司推出的众多生成性语言模型。现在你想为自己的副项目尝试微调一些大型语言模型。一开始一切都很顺利，但接着你发现这些模型计算资源消耗巨大，而你手头并没有GPU处理器。
- en: '[Google Colab](https://colab.google/) generously offers you a way to access
    to free computation so you can solve this problem. The downside is, you need to
    do it all inside a transitory browser based environment. To make matter worse,
    the whole thing is time limited, so it seems like no matter what you do, you are
    going to lose your precious fine tuned model and all the results when the kernel
    is eventually shut down and the environment nuked.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[Google Colab](https://colab.google/)慷慨地为你提供了免费计算资源，帮助你解决这个问题。缺点是，你需要在一个过渡性的基于浏览器的环境中完成所有操作。更糟的是，整个环境是有时间限制的，所以似乎不管你做什么，最终都会在内核关闭并且环境被清除时失去你珍贵的微调模型和所有结果。'
- en: 'Never fear. There is a way around this: make use of [Google Drive](https://drive.google.com)
    to save any of your intermediate results or model parameters. This will allow
    you to continue experimentation at a later stage, or take and use a trained model
    for inference elsewhere.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 别担心。解决这个问题的方法是：利用[Google Drive](https://drive.google.com)保存你的中间结果或模型参数。这样你就可以在稍后的阶段继续实验，或者将训练好的模型拿去其他地方进行推理。
- en: To do this you will need a Google account that has sufficient Google Drive space
    for both your training data and you model checkpoints. I will presume you have
    created a folder called `data` in Google Drive containing your dataset. Then another
    called `checkpoints` that is empty.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，您需要一个 Google 账户，且该账户必须有足够的 Google Drive 空间来存储您的训练数据和模型检查点。我假设您已经在 Google
    Drive 中创建了一个名为`data`的文件夹，里面包含您的数据集。然后另创建一个名为`checkpoints`的空文件夹。
- en: 'Inside your Google Colab Notebook you then mount your Drive using the following
    command:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的 Google Colab Notebook 中，您可以使用以下命令挂载您的 Google Drive：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You now list the contents of your data and checkpoints directories with the
    following two commands in a new cell:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以在一个新的单元格中使用以下两个命令列出数据和检查点目录的内容：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If these commands work then you now have access to these directories inside
    your notebook. If the commands do not work then you might have missed the authorisation
    step. The `drive.mount` command above should have spawned a pop up window which
    requires you to click through and authorise access. You may have missed the pop
    up, or not selected all of the required access rights. Try re-running the cell
    and checking.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些命令有效，您现在可以在笔记本中访问这些目录。如果命令无效，可能是您错过了授权步骤。上述的`drive.mount`命令应该会弹出一个窗口，要求您点击并授权访问。您可能错过了弹出窗口，或者没有选择所有必要的访问权限。请尝试重新运行单元格并检查。
- en: Once you have that access sorted, you can then write your scripts such that
    models and results are serialised into the Google Drive directories so they persist
    over sessions. In an ideal world, you would code your training job so that any
    script that takes too long to run can load partially trained models from the previous
    session and continue training from that point.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦解决了访问权限问题，您可以编写脚本，使得模型和结果被序列化到 Google Drive 目录中，从而在会话间得以保留。在理想情况下，您会编写训练作业的代码，使得任何运行时间过长的脚本可以从上一次会话加载部分训练好的模型，并从那个点继续训练。
- en: 'A simple way for achieving that is creating a save and load function that gets
    used by your training scripts. The training process should always check if there
    is a partially trained model, before initialising a new one. Here is an example
    save function:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这一点的一种简单方法是创建一个保存和加载函数，供训练脚本使用。训练过程应始终检查是否存在部分训练的模型，然后再初始化一个新的模型。以下是一个保存函数的示例：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In this instance we are saving the model state along with some meta-data (epochs
    and loss) inside a dictionary structure. We include an option to overwrite a single
    checkpoint file, or create a new file for every epoch. We are using the torch
    save function, but in principle you could use other serialisation methods. The
    key idea is that your program opens the file and determines how many epochs of
    training were used for the existing file. This allows the program to decide whether
    to continue training or move on.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在此情况下，我们将模型状态以及一些元数据（如`epochs`和`loss`）保存在一个字典结构中。我们提供了一个选项来覆盖单个检查点文件，或为每个 epoch
    创建一个新的文件。我们使用的是 torch 的保存函数，但原则上，您可以使用其他序列化方法。关键思想是，您的程序打开文件并确定现有文件的训练 epoch 数量。这允许程序决定是否继续训练或跳过。
- en: Similarly, in the load function we pass in a reference to a model we wish to
    use. If there is already a serialised model we load the parameters into our model
    and return the number of epochs it was trained for. This epoch value will determine
    how many additional epochs are required. If there is no model then we get the
    default value of zero epochs and we know the model still has the parameters it
    was initialised with.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在加载函数中，我们传入一个我们希望使用的模型引用。如果已经存在一个序列化的模型，我们将参数加载到模型中，并返回训练的 epoch 数量。这个 epoch
    值将决定还需要多少额外的训练轮次。如果没有模型，则返回默认的零个 epoch，表明模型仍然是初始化时的状态。
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: These two functions will need to be called inside your training loop, and you
    need to ensure that the returned value for epochs value is used to update the
    value of epochs in your training iterations. The result is you now have a training
    process that can be re-started when a kernel dies, and it will pick up and continue
    from where it left off.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个函数需要在您的训练循环中调用，并且您需要确保返回的`epochs`值用于更新训练迭代中的`epochs`值。这样，您就拥有了一个可以在内核崩溃时重新启动的训练过程，它会从中断的地方继续。
- en: 'That core training loop might look something like the following:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 核心训练循环可能看起来像下面这样：
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Note: In this example I am experimenting with training multiple different model
    setups (in a list called `experiments`), potentially using different training
    datasets. The supporting functions `initialise_model_components` and `generate_data_loaders`
    are taking care of ensuring that I get the correct model and data for each experiment.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 注：在这个例子中，我正在实验训练多个不同的模型配置（在一个名为`experiments`的列表中），可能会使用不同的训练数据集。辅助函数`initialise_model_components`和`generate_data_loaders`负责确保我为每个实验获取正确的模型和数据。
- en: The core training loop above allows us to reuse the overall code structure that
    trains and serialises these models, ensuring that each model gets to the desired
    number of epochs of training. If we restart the process, it will iterate through
    the experiment list again, but it will abandon any experiments that have already
    reached the maximum number of epochs.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的核心训练循环允许我们重用整体代码结构，以训练并序列化这些模型，确保每个模型都能完成所需的训练轮次。如果我们重新启动该过程，它会再次遍历实验列表，但会放弃那些已经达到最大训练轮次的实验。
- en: Hopefully you can use this boilerplate code to setup your own process for experimenting
    with training some deep learning language models inside Google Colab. Please comment
    and let me know what you are building and how you use this code.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你能使用这个样板代码，在Google Colab中设置自己的深度学习语言模型训练实验流程。请发表评论并告诉我你正在构建什么以及如何使用这段代码。
- en: Massive thank you to [Aditya Pramar](https://medium.com/@adityapramar15) for
    his initial scripts that prompted this piece of work.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢[Aditya Pramar](https://medium.com/@adityapramar15)提供的初始脚本，正是这些脚本促成了这篇作品的诞生。
