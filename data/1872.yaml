- en: 'Line-By-Line, Let’s Reproduce GPT-2: Section 2 — Hardware Optimization'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逐行分析，让我们重现 GPT-2：第2节 — 硬件优化
- en: 原文：[https://towardsdatascience.com/line-by-line-lets-reproduce-gpt-2-section-2-hardware-optimization-86e71c91d9bb?source=collection_archive---------15-----------------------#2024-07-31](https://towardsdatascience.com/line-by-line-lets-reproduce-gpt-2-section-2-hardware-optimization-86e71c91d9bb?source=collection_archive---------15-----------------------#2024-07-31)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/line-by-line-lets-reproduce-gpt-2-section-2-hardware-optimization-86e71c91d9bb?source=collection_archive---------15-----------------------#2024-07-31](https://towardsdatascience.com/line-by-line-lets-reproduce-gpt-2-section-2-hardware-optimization-86e71c91d9bb?source=collection_archive---------15-----------------------#2024-07-31)
- en: This blog post will go line-by-line through the hardware optimizations in Section
    2 of Andrej Karpathy’s “Let’s reproduce GPT-2 (124M)”
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本文将逐行分析 Andrej Karpathy 在《让我们重现 GPT-2（124M）》第2节中的硬件优化内容。
- en: '[](https://medium.com/@mgunton7?source=post_page---byline--86e71c91d9bb--------------------------------)[![Matthew
    Gunton](../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png)](https://medium.com/@mgunton7?source=post_page---byline--86e71c91d9bb--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--86e71c91d9bb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--86e71c91d9bb--------------------------------)
    [Matthew Gunton](https://medium.com/@mgunton7?source=post_page---byline--86e71c91d9bb--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mgunton7?source=post_page---byline--86e71c91d9bb--------------------------------)[![Matthew
    Gunton](../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png)](https://medium.com/@mgunton7?source=post_page---byline--86e71c91d9bb--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--86e71c91d9bb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--86e71c91d9bb--------------------------------)
    [Matthew Gunton](https://medium.com/@mgunton7?source=post_page---byline--86e71c91d9bb--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--86e71c91d9bb--------------------------------)
    ·11 min read·Jul 31, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--86e71c91d9bb--------------------------------)
    ·阅读时间：11分钟·2024年7月31日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/6eaae334a9023076271e9cf731d1385a.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6eaae334a9023076271e9cf731d1385a.png)'
- en: Image by Author — SDXL
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源 — SDXL
- en: As a quick recap, [in Section 1](https://medium.com/towards-data-science/line-by-line-lets-reproduce-gpt-2-section-1-b26684f98492)
    we went line-by-line through the code written by Karpathy to naively train GPT-2\.
    Now that we have our setup, Karpathy shows us how we can make the model train
    fast on our NVIDIA GPU! While we know that it takes a lot of time to train good
    models, by optimizing each run we can shave days or even weeks off our training
    time. This naturally gives us more iterations to improve our model. By the end
    of this blog post, you will see how to radically speed up your training (by 10x)
    using an Ampere-series Nvidia GPU.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 快速回顾一下，[在第1节](https://medium.com/towards-data-science/line-by-line-lets-reproduce-gpt-2-section-1-b26684f98492)中，我们逐行分析了
    Karpathy 编写的代码，目的是原始地训练 GPT-2。现在我们已经有了我们的设置，Karpathy 向我们展示了如何在我们的 NVIDIA GPU 上加速模型训练！虽然我们知道训练一个好的模型需要花费大量时间，但通过优化每次运行，我们可以节省数天甚至数周的训练时间。这自然为我们提供了更多的迭代次数来改善模型。在本文的最后，你将看到如何通过使用
    Ampere 系列的 Nvidia GPU 来大幅加速训练（提高 10 倍）。
- en: To do this blog post, I ran the optimizations both on the NVIDIA T4 GPU that
    Google Colab gives you for free and on a NVIDIA A100 GPU 40GB SXM4 from Lambda
    Labs. Most of the optimizations Karpathy goes over are specifically for an A100
    or better, but there are still some gains to be made on less powerful GPUs.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了写这篇博客文章，我在 Google Colab 提供的免费 NVIDIA T4 GPU 和 Lambda Labs 的 NVIDIA A100 40GB
    SXM4 GPU 上进行了优化。Karpathy 提到的大多数优化方法专为 A100 或更强大的显卡设计，但在较弱的 GPU 上仍然可以获得一些性能提升。
- en: Let’s dive in!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Timing Our Code
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码计时
- en: 'To begin, we want to create a way to see how effective our optimizations are.
    To do so, we will add in to our training loop the below code:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们希望创建一种方式来查看我们的优化效果。为此，我们将在训练循环中添加以下代码：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We start off by capturing the time at the beginning of the loop, but before
    we capture the end time we run `torch.cuda.synchronize()`. By default we are only
    paying attention to when the CPU stops. Because we have moved most of the major
    calculations to GPU, we need to make sure that our timer here takes into account
    when the GPU stops its calculations. Synchronize will have the CPU wait to progress
    until the GPU has completed its work queue, giving us an accurate time for when
    the loop was completed. Once we have an accurate time, we naturally calculate
    the difference between the start and the end.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先捕获循环开始时的时间，但在捕获结束时间之前，我们运行 `torch.cuda.synchronize()`。默认情况下，我们只关注 CPU 停止的时间。因为我们已经将大部分主要计算移至
    GPU，所以我们需要确保定时器在此处考虑到 GPU 停止计算的时间。同步操作将使 CPU 等待，直到 GPU 完成其工作队列，从而为我们提供循环完成时的准确时间。一旦我们有了准确的时间，自然就能计算出开始时间和结束时间之间的差异。
- en: Batch Sizing
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批量大小
- en: We also want to make sure we are putting as much data as possible through each
    round. The way we achieve this is by setting batch sizes. In our `DataLoaderLite`
    class, we can adjust our 2 parameters (B and T) so that we use the most amount
    of memory in our GPU without going out of bounds.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望确保每轮通过尽可能多的数据。我们实现这一目标的方式是设置批量大小。在我们的 `DataLoaderLite` 类中，我们可以调整 2 个参数（B
    和 T），以便在不超出限制的情况下，使用 GPU 中尽可能多的内存。
- en: With the A100 GPU, you can follow Karpathy’s example, where we set T equal to
    the max `block_size` of 1024 and we set B equal to 16 because it’s a “nice” number
    (easily divisible by powers of 2) and it’s the largest such “nice” number we can
    fit in memory.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 A100 GPU，你可以按照 Karpathy 的示例，其中我们将 T 设置为最大 `block_size` 的 1024，并将 B 设置为 16，因为它是一个“漂亮”的数字（容易被
    2 的幂次方整除），并且是我们可以在内存中容纳的最大“漂亮”数字。
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you try to put in a value that is too large, you’ll wind up seeing a `OutOfMemoryError`
    from CUDA in your terminal. I found the best values for a T4 GPU I could get was
    B =4 and T =1024 (*when trying different B values in Google Colab, be aware you
    may need to restart the session to ensure you’re not getting false positive* `OutOfMemoryError`*s*)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你尝试输入一个太大的值，你会在终端看到 CUDA 提示 `OutOfMemoryError` 错误。我发现对于 T4 GPU，最佳的值是 B = 4
    和 T = 1024（*当在 Google Colab 中尝试不同的 B 值时，请注意，你可能需要重启会话，以确保不会得到错误的 `OutOfMemoryError`*）。
- en: Running on the A100 and T4 below, I get the following graphs showing training
    time to start (on average roughly 1100ms on the T4 and 1040ms on the A100)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在 A100 和 T4 上运行时，我得到了以下图表，显示了开始训练的时间（平均大约在 T4 上是 1100ms，A100 上是 1040ms）
- en: '![](../Images/4982d21ba78b861a4b97a097448bf3a7.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4982d21ba78b861a4b97a097448bf3a7.png)'
- en: Image by Author — A100 Training with no optimizations
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像 — A100 训练未进行优化
- en: '![](../Images/ffb306ee31f999c89ad7ab83e1e33a44.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ffb306ee31f999c89ad7ab83e1e33a44.png)'
- en: Image by Author — T4 training with no optimizations
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像 — T4 训练未进行优化
- en: Floating Point Optimizations
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 浮点优化
- en: Now we’re going to focus on changes we make to the internal representation of
    data within the model.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将重点关注在模型内部表示数据时所做的更改。
- en: 'If you look at the `dtype` of the weights in our code from section 1, you will
    see we use Floating Point 32 (fp32) by default. Fp32 means that we represent the
    numbers using 32 bits following the IEEE floating point standard below:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看我们在第一节代码中权重的 `dtype`，你会看到我们默认使用 32 位浮点数（fp32）。Fp32 表示我们使用 32 位表示数字，遵循下面的
    IEEE 浮动点标准：
- en: '![](../Images/dc819a07b517ac882a19fa1a53a3de13.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc819a07b517ac882a19fa1a53a3de13.png)'
- en: Image by Author — IEEE Representation of Floating Point 32 (FP32)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像 — IEEE 浮动点 32（FP32）表示
- en: 'As Karpathy says in the video, we have seen empirically that fp32 isn’t necessary
    to train quality models — we can use less data to represent each weight and still
    have quality outputs. One way to speed up the calculations is to use NVIDIA’s
    TensorCore instruction. This will handle the matrix multiplications by converting
    the operands to the form Tensor Float 32 (TF32) laid out below:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 正如 Karpathy 在视频中所说，我们通过实验证明，fp32 并不是训练高质量模型所必需的——我们可以使用更少的数据来表示每个权重，并且仍然得到高质量的输出。加速计算的一种方法是使用
    NVIDIA 的 TensorCore 指令。它将通过将操作数转换为下面所示的 Tensor Float 32（TF32）格式来处理矩阵乘法：
- en: '![](../Images/b15aba27d3e3263698eb170290bd1844.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b15aba27d3e3263698eb170290bd1844.png)'
- en: Image by Author — Tensor Float 32 (TF32)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像 — Tensor Float 32（TF32）
- en: '![](../Images/eddcc5b41eeeae51b530a74cb8e2ec35.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eddcc5b41eeeae51b530a74cb8e2ec35.png)'
- en: Image by Author — TF32 data flow through a Tensor Core post optimization
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像 — TF32 数据流通过 Tensor Core 经过优化
- en: 'From the code point of view, all of our variables (input, output) are in FP32,
    but the NVIDIA GPU will convert the intermediary matrices to TF32 for speedup.
    This according to NVIDIA drives an 8x speed up [versus a FFMA instruction.](https://docs.nvidia.com/gameworks/content/developertools/desktop/analysis/report/cudaexperiments/kernellevel/achievedflops.htm)
    To enable TF32 in PyTorch, we only need to add the below line (high = TF32, highest
    = FP32, medium=BF16 (more on that later)):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从代码角度来看，我们所有的变量（输入、输出）都是FP32格式，但NVIDIA GPU会将中间矩阵转换为TF32格式以加速。根据NVIDIA的说法，这能带来8倍的加速[相对于FFMA指令](https://docs.nvidia.com/gameworks/content/developertools/desktop/analysis/report/cudaexperiments/kernellevel/achievedflops.htm)。要在PyTorch中启用TF32，我们只需要添加以下代码行（high
    = TF32, highest = FP32, medium = BF16（稍后会详细介绍））：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: TensorCore is unique to NVIDIA and you can only run TF32 on an A100 GPU or better,
    so some developers have used Floating Point 16 (FP16) as a way to train. The problem
    with this representation is that the range of data that FP16 can capture is smaller
    than FP32, leading to problems representing the same data range needed for training.
    While you can get around this using gradient expansion, this requires more calculations
    so you wind up in a situation where you take 1 step forwards, 2 steps back.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: TensorCore是NVIDIA独有的功能，您只能在A100 GPU或更高版本上运行TF32，因此一些开发者使用浮点16（FP16）作为训练方式。这种表示法的问题在于，FP16能够表示的数据范围比FP32要小，导致无法表示训练所需的相同数据范围。虽然通过梯度扩展可以绕过这一点，但这会增加更多的计算，因此最终会陷入“前进一步，退两步”的困境。
- en: '![](../Images/c607218da80ec3a96abac4629db209e6.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c607218da80ec3a96abac4629db209e6.png)'
- en: Image by Author — IEEE Representation of Floating Point 16 (FP16)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片 — IEEE浮点16（FP16）表示法
- en: Instead, the data optimization Karpathy uses in his video is brain floating
    point (BF16). Here we have the same number of exponent bits as FP32, so we can
    represent the same range, but we have fewer mantissa bits. This means that while
    we have fewer bits, our precision in representing numbers is lower. Empirically,
    this has not caused major reduction in performance, so it’s a tradeoff we’re willing
    to make. To use this on NVIDIA chips, you need to have an A100.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 但在他的演示视频中，Karpathy使用的数据优化方法是脑浮点（BF16）。在这种表示法中，我们与FP32有相同数量的指数位，因此可以表示相同的范围，但尾数位较少。这意味着尽管我们有较少的位数，表示数字的精度较低。通过经验来看，这并没有导致性能大幅下降，所以这是一个我们愿意接受的折衷。要在NVIDIA芯片上使用此格式，您需要具备A100。
- en: '![](../Images/0d7891f96b11cdaa642777ec0cf19edb.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0d7891f96b11cdaa642777ec0cf19edb.png)'
- en: Image by Author — Brain Floating Point 16 (BF16)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片 — 脑浮点16（BF16）
- en: 'Using PyTorch, we don’t need to change our code dramatically to use the new
    data type. The documentation advises us to only use these during the forward pass
    of your model and loss calculation. As our code does both of these in 1 line,
    we can modify our code as below:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PyTorch时，我们不需要大幅修改代码就可以使用新的数据类型。文档建议我们只在模型的前向传递和损失计算过程中使用这些数据类型。由于我们的代码在1行中完成了这两个操作，我们可以按如下方式修改代码：
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Just like that, our code is now running using BF16.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，我们的代码现在已经开始使用BF16运行。
- en: Running on our A100, we now see that the average step takes about 330ms! We’ve
    already reduced our runtime by about 70%, and we’re just getting started!
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的A100上运行时，现在平均每步大约需要330毫秒！我们已经将运行时间减少了大约70%，而且我们才刚刚开始！
- en: '![](../Images/a2fd83867ed991a7df45314ca29e3492.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2fd83867ed991a7df45314ca29e3492.png)'
- en: Image by Author — A100 Training after data type optimizations
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片 — 数据类型优化后的A100训练
- en: Torch Compile
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Torch Compile
- en: We can further improve our training time by utilizing the PyTorch Compile feature.
    This will give us fairly big performance increases without having to adjust our
    code one bit.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过利用PyTorch Compile功能进一步改善训练时间。这样可以在不调整代码的情况下大幅提升性能。
- en: To come at it from a high-level, every computer program is executed in binary.
    Because most people find it difficult to code in binary, we have created higher-level
    languages that let us code in forms that are easier for people to think in. When
    we compile these languages, they are transformed back into binary that we actually
    run. Sometimes in this translation, we can figure out faster ways to do the same
    calculation — such as reusing a certain variable or even simply not doing one
    to begin with.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，每个计算机程序都是以二进制执行的。因为大多数人觉得用二进制编程很困难，所以我们创建了更高层次的语言，让我们能以更容易理解的方式编写代码。当我们编译这些语言时，它们会被转换回我们实际运行的二进制代码。有时，在这种转换过程中，我们可以找到更快的方式来完成相同的计算——比如重用某个变量，甚至干脆跳过某些操作。
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This brings us now to machine learning and PyTorch. Python is a high-level language
    but we’re still doing computationally intense calculations with it. When we run
    `torch compile` we are spending more time compiling our code, but we wind up seeing
    our runtime (the training for us here) go a lot faster because of that extra work
    we did to find those optimizations.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这将我们带到了机器学习和 PyTorch。Python 是一种高级语言，但我们仍然在用它做计算密集型的计算。当我们运行`torch compile`时，我们花费更多时间在编译代码上，但由于我们为了找到这些优化所做的额外工作，我们最终看到我们的运行时间（在这里指的是训练）变得更快。
- en: 'Karpathy gives the following example of how PyTorch may improve the calculations.
    Our GELU activation function can be written out like below:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Karpathy 给出了一个 PyTorch 如何改善计算的示例。我们的 GELU 激活函数可以写成如下：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: For each calculation you see in the above function, we have to dispatch a kernel
    in the GPU. This means that when we start off by taking input to the third power,
    we pull input from high-bandwidth memory (HBM) into the GPU cores and do our calculation.
    We then write back to HBM before we start our next calculation and begin the whole
    process over again. Naturally, this sequencing is causing us to spend a lot of
    time waiting for memory transfers to occur.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于你在上面的函数中看到的每个计算，我们都必须在 GPU 中调度一个内核。这意味着，当我们开始进行三次方计算时，我们从高带宽内存（HBM）中提取输入到
    GPU 核心，并进行计算。然后，我们将结果写回 HBM，然后开始下一次计算，并重复整个过程。自然，这样的顺序导致我们在等待内存传输时浪费了大量时间。
- en: PyTorch compile allows us to see an inefficiency like this and be more careful
    with when we are spinning up new kernels, resulting in dramatic speed ups. This
    is called kernel fusion.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 编译让我们能够看到像这样的低效，并且更加小心地控制何时启动新的内核，从而实现显著的加速。这被称为内核融合。
- en: While on this topic, I’d like to point out an excellent open-source project
    called Luminal that takes this idea a little further. [Luminal is a separate framework
    that you write your training / inferencing in](https://github.com/jafioti/luminal).
    By using this framework, you get access to its compiler which finds many more
    optimizations for you by nature of having a more limited number of computations
    to consider. If you like the idea of improving runtime by compiling fast GPU code,
    give the project a look.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个话题上，我想指出一个优秀的开源项目叫做 Luminal，它将这个思想进一步扩展了。[Luminal 是一个独立的框架，你可以在其中编写训练/推理代码](https://github.com/jafioti/luminal)。通过使用这个框架，你可以访问它的编译器，该编译器通过只考虑有限数量的计算，能够为你找到更多优化。如果你喜欢通过编译快速
    GPU 代码来提升运行时间的思路，可以看看这个项目。
- en: When we run the above code now we see that we see each step takes roughly 145
    ms (cutting by 50% from before and ~86% from the original). We pay for this with
    the first iteration which took roughly 40,000ms to run! As most training sequences
    have many more steps than 50, this tradeoff is one that we are willing to make.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们现在运行上面的代码时，我们会看到每一步大约需要 145 毫秒（比之前减少了 50%，比原始版本减少了约 86%）。我们为此付出了第一轮大约 40,000
    毫秒的代价！由于大多数训练序列的步骤数远超过 50，这个权衡是我们愿意做出的。
- en: '![](../Images/77e1cd818db8c6ec28081e44b6f4e121.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77e1cd818db8c6ec28081e44b6f4e121.png)'
- en: Image by Author — A100 Training run after the Torch Compile optimizations
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片 — A100 训练在 Torch Compile 优化后的运行结果
- en: Flash Attention
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Flash Attention
- en: Another optimization we make is using Flash Attention ([see the paper here](https://arxiv.org/pdf/2205.14135)).
    The code change itself is very simple for us, but the thinking behind it is worth
    exploring.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做的另一个优化是使用 Flash Attention（[查看论文](https://arxiv.org/pdf/2205.14135)）。对于我们来说，代码本身的改动非常简单，但背后的思路值得深入探讨。
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Similar to how we condensed the `TanhGELU` class into as few kernels as we
    could, we apply the same thinking to attention. In their paper, [“FlashAttention:
    Fast and Memory-Efficient Exact Attention with IO-Awareness”](https://arxiv.org/pdf/2205.14135),
    the authors show how you can achieve a 7.6x speed up by fusing the kernel. While
    in theory torch compile should be able to find optimizations like this, in practice
    we haven’t seen it find this yet.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '类似于我们如何将 `TanhGELU` 类压缩成尽可能少的内核，我们也将相同的思维方式应用于注意力机制。在他们的论文《[“FlashAttention:
    Fast and Memory-Efficient Exact Attention with IO-Awareness”](https://arxiv.org/pdf/2205.14135)》中，作者展示了如何通过融合内核实现
    7.6 倍的加速。虽然理论上 `torch compile` 应该能够找到类似的优化，但在实践中我们尚未看到它找到这一点。'
- en: '![](../Images/0a620953fcf8999cd84bae98c7f64114.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a620953fcf8999cd84bae98c7f64114.png)'
- en: 'Figure 1 from [“FlashAttention: Fast and Memory-Efficient Exact Attention with
    IO-Awareness”](https://arxiv.org/pdf/2205.14135)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1 来自《[“FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness”](https://arxiv.org/pdf/2205.14135)》'
- en: The paper is worth doing a deep dive on, but to give a quick synopsis, FlashAttention
    is written to be IO-aware, thus preventing unnecessary (and time-consuming) calls
    to memory. By reducing these, they can radically speed up the calculations.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文值得深入研究，但简要概述一下，FlashAttention 被设计为对 I/O 友好，从而避免了不必要（且耗时）的内存调用。通过减少这些调用，它们可以显著加速计算。
- en: After implementing this, we find that we now have an average step of about 104ms.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 实施了这个优化后，我们发现现在每一步的平均时间约为 104 毫秒。
- en: '![](../Images/a1f72e2c0244950e414977351ca9932a.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1f72e2c0244950e414977351ca9932a.png)'
- en: Image by Author — A100 Training after Flash Attention Optimization
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像 — Flash Attention 优化后的 A100 训练
- en: Vocab Size Change
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词汇大小变化
- en: Finally, we can go through all of the numbers we have hard-coded and evaluate
    how “nice” they are. When we do this, we find that the vocabulary size is not
    divisible by many powers of 2 and so will be more time-consuming for our GPU’s
    memory to load in. We fix this by going from the 50,257 vocab size to the next
    “nice” number, which is 50,304\. This is a nice number as it’s cleanly divisible
    by 2, 4, 8, 16, 32, 64, and 128.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以检查所有硬编码的数字，评估它们有多“合适”。当我们这样做时，我们发现词汇大小不能被许多 2 的幂整除，因此在 GPU 内存加载时会更加耗时。我们通过将词汇大小从
    50,257 调整为下一个“合适”的数字 50,304 来解决这个问题。这个数字之所以“合适”，是因为它可以被 2、4、8、16、32、64 和 128 整除。
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now you may remember from the last blog post that our vocab size is not an arbitrary
    value — it is determined by the tokenizer we are using. Thus begs the question,
    When we arbitrarily add in more values to our vocab size, what happens? During
    the training, the model will notice that these new vocab never appear, so it will
    start to push the probabilities of these tokens to 0 — thus our performance is
    safe. That does not mean that there is no tradeoff though. By loading into memory
    vocab that is never used, we are wasting time. However, empirically we can see
    that loading in “nice” numbers more than compensates for this cost.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得上一篇博客中提到的，我们的词汇大小并不是一个任意的值 — 它是由我们使用的分词器决定的。因此，问题就来了，当我们任意地增加词汇大小时，会发生什么？在训练过程中，模型会发现这些新的词汇从未出现过，因此它会开始将这些词汇的概率推向
    0 —— 所以我们的性能是安全的。不过，这并不意味着没有权衡。通过加载从未使用过的词汇到内存中，我们浪费了时间。然而，从经验上看，我们可以看到加载“合适”的数字能大大弥补这一成本。
- en: With our last optimization, we now have an average of about 100 ms per step.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通过最后的优化，我们现在每一步的平均时间大约是 100 毫秒。
- en: '![](../Images/6e684ff36d2019e35a382dc8544b893e.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6e684ff36d2019e35a382dc8544b893e.png)'
- en: Image by Author — A100 Training after Vocab Size Optimization
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像 — 词汇大小优化后的 A100 训练
- en: With this final optimization, we find that our training has improved ~10x from
    the beginning!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这一最终优化，我们发现我们的训练从最初开始已经提高了约 10 倍！
- en: What optimizations work on T4 GPU?
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 哪些优化在 T4 GPU 上有效？
- en: If you’ve been following along but only have access to the consumer-grade T4
    GPU, you may wonder which optimizations you can use. To recap, we cannot use the
    BF16 representation, but we can use the vocabulary size change, flash attention,
    and torch compile. [To see this code in action, check out my Google Colab notebook,
    which is optimized just for T4 usage](https://colab.research.google.com/drive/1NhX0dDHF8mpzQGG9vjUJ16qiMbL7QI5Y).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你一直在关注但仅能使用消费者级 T4 GPU，你可能会想知道哪些优化是可以使用的。回顾一下，我们不能使用 BF16 表示法，但可以使用词汇大小变化、Flash
    Attention 和 torch compile。[要查看这段代码的实际应用，查看我为 T4 优化的 Google Colab 笔记本](https://colab.research.google.com/drive/1NhX0dDHF8mpzQGG9vjUJ16qiMbL7QI5Y)。
- en: We can see from the graph below that while the torch compile does take a lot
    of time for the first round, the next rounds are not significantly better than
    the unoptimized versions (roughly an 8% drop on T4 vs 90% drop on A100).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 从下图中我们可以看到，尽管第一次运行 torch compile 确实需要很长时间，但接下来的几轮与未优化版本相比并没有显著提高（在 T4 上大约下降了
    8%，而在 A100 上下降了 90%）。
- en: '![](../Images/f95034c6a9cd38614158cf644977592e.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f95034c6a9cd38614158cf644977592e.png)'
- en: Image by Author — Optimized run on T4 GPU
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像 — 在 T4 GPU 上优化运行
- en: Nevertheless, when OpenAI was training GPT-2 it was running on far more advanced
    hardware than the T4\. The fact that we can run this workload on a T4 today suggests
    that hardware requirements are becoming less onerous, helping create a future
    where hardware is not a barrier to ML work.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，当 OpenAI 在训练 GPT-2 时，它运行的是比 T4 更先进的硬件。今天我们能够在 T4 上运行这个工作负载，表明硬件要求正在变得不那么苛刻，这有助于创造一个硬件不再成为机器学习工作障碍的未来。
- en: Closing
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结束语
- en: By optimizing our code, we’ve seen major speed ups and also learned a bit about
    where the big bottlenecks for training happen. First and foremost, datatypes are
    critically important for speed, as this change by itself contributed majorly to
    the speed ups. Second, we see that hardware optimizations can play a major role
    in speeding up calculations — so GPU hardware is worth its weight in gold. Finally,
    compiler optimizations have a major role to play here as well.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 通过优化我们的代码，我们看到了显著的速度提升，并且也了解了一些训练过程中主要的瓶颈。首先，数据类型对于速度至关重要，因为仅这一改变就大幅提升了速度。其次，我们发现硬件优化在加速计算中发挥着重要作用——因此，GPU
    硬件真的是物有所值。最后，编译器优化在这里也起到了关键作用。
- en: To see the code I ran in the A100, [check out this gist here](https://gist.github.com/matthewjgunton/3e2d9c9be60499cf6460d197917b976e).
    If you have any suggestions for how to optimize the hardware further, I would
    love to see them in the comments!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看我在 A100 上运行的代码，[请查看这个 Gist](https://gist.github.com/matthewjgunton/3e2d9c9be60499cf6460d197917b976e)。如果你有任何关于如何进一步优化硬件的建议，我很乐意在评论区看到它们！
- en: It’s an exciting time to be building!
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是构建的激动人心的时刻！
- en: '[1] Karpathy, A., [“Let’s reproduce GPT-2 (124M)”](https://youtu.be/l8pRSuU81PU?feature=shared)
    (2024), YouTube'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Karpathy, A., [“让我们重现 GPT-2 (124M)”](https://youtu.be/l8pRSuU81PU?feature=shared)
    (2024), YouTube'
- en: '[2] Dao, T., et al. [“FlashAttention: Fast and Memory-Efficient Exact Attention
    with IO-Awareness”](https://arxiv.org/pdf/2205.14135) (2022), arXiv'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Dao, T. 等人 [“FlashAttention: 快速且内存高效的精确注意力机制，具备 I/O 感知能力”](https://arxiv.org/pdf/2205.14135)
    (2022), arXiv'
- en: '[3] Krashinsky, R., et al “[NVIDIA Ampere Architecture In-Depth](https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/)”
    (2020), NVIDIA Developer'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Krashinsky, R. 等人 “[NVIDIA 安培架构深度解析](https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/)”
    (2020), NVIDIA 开发者'
