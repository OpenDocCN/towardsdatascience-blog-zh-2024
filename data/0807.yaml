- en: 'Transformers: How Do They Transform Your Data?'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer：它们如何转化你的数据？
- en: 原文：[https://towardsdatascience.com/transformers-how-do-they-transform-your-data-72d69e383e0d?source=collection_archive---------0-----------------------#2024-03-28](https://towardsdatascience.com/transformers-how-do-they-transform-your-data-72d69e383e0d?source=collection_archive---------0-----------------------#2024-03-28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/transformers-how-do-they-transform-your-data-72d69e383e0d?source=collection_archive---------0-----------------------#2024-03-28](https://towardsdatascience.com/transformers-how-do-they-transform-your-data-72d69e383e0d?source=collection_archive---------0-----------------------#2024-03-28)
- en: Diving into the Transformers architecture and what makes them unbeatable at
    language tasks
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入探索Transformer架构及其在语言任务中无敌的原因
- en: '[](https://medium.com/@maxwolf34?source=post_page---byline--72d69e383e0d--------------------------------)[![Maxime
    Wolf](../Images/259b3659d0e6dd1d0f0eec4ae92d02e9.png)](https://medium.com/@maxwolf34?source=post_page---byline--72d69e383e0d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--72d69e383e0d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--72d69e383e0d--------------------------------)
    [Maxime Wolf](https://medium.com/@maxwolf34?source=post_page---byline--72d69e383e0d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@maxwolf34?source=post_page---byline--72d69e383e0d--------------------------------)[![Maxime
    Wolf](../Images/259b3659d0e6dd1d0f0eec4ae92d02e9.png)](https://medium.com/@maxwolf34?source=post_page---byline--72d69e383e0d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--72d69e383e0d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--72d69e383e0d--------------------------------)
    [Maxime Wolf](https://medium.com/@maxwolf34?source=post_page---byline--72d69e383e0d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--72d69e383e0d--------------------------------)
    ·11 min read·Mar 28, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--72d69e383e0d--------------------------------)
    ·阅读时间11分钟·2024年3月28日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4dd0f4cd464558d94b73f8274580ac94.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4dd0f4cd464558d94b73f8274580ac94.png)'
- en: Image by the author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'In the rapidly evolving landscape of artificial intelligence and machine learning,
    one innovation stands out for its profound impact on how we process, understand,
    and generate data: **Transformers**. Transformers have revolutionized the field
    of natural language processing (NLP) and beyond, powering some of today’s most
    advanced AI applications. But what exactly are Transformers, and how do they manage
    to transform data in such groundbreaking ways? This article demystifies the inner
    workings of Transformer models, focusing on the **encoder architecture**. We will
    start by going through the implementation of a Transformer encoder in Python,
    breaking down its main components. Then, we will visualize how Transformers process
    and adapt input data during training.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能和机器学习飞速发展的今天，有一种创新脱颖而出，对我们处理、理解和生成数据的方式产生了深远影响：**Transformer**。Transformer彻底改变了自然语言处理（NLP）及其他领域，为今天一些最先进的AI应用提供了动力。但究竟什么是Transformer，它们又是如何以如此突破性的方式转换数据的呢？本文将揭秘Transformer模型的内部工作原理，重点讲解**编码器架构**。我们将从Python中Transformer编码器的实现入手，逐步解析其主要组件。然后，我们将可视化Transformer如何在训练过程中处理并适应输入数据。
- en: While this blog doesn’t cover every architectural detail, it provides an implementation
    and an overall understanding of the transformative power of Transformers. For
    an in-depth explanation of Transformers, I suggest you look at the excellent Stanford
    CS224-n course.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本博客并未涵盖所有架构细节，但它提供了一个实现并帮助你全面理解Transformer的变革性力量。想深入了解Transformer的工作原理，我建议你参考斯坦福大学的优秀CS224-n课程。
- en: I also recommend following the [GitHub repository](https://github.com/maxime7770/Transformers-Insights)
    associated with this article for additional details. 😊
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我还建议关注与本文相关的[GitHub 仓库](https://github.com/maxime7770/Transformers-Insights)以获取更多细节。😊
- en: What is a Transformer encoder architecture?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是Transformer编码器架构？
- en: '![](../Images/df39c5bc0e96c04388b637bb391a7fed.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df39c5bc0e96c04388b637bb391a7fed.png)'
- en: The Transformer model from [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)的Transformer模型
- en: This picture shows the original Transformer architecture, combining an encoder
    and a decoder for sequence-to-sequence language tasks.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图片展示了原始Transformer架构，将编码器和解码器结合用于序列到序列的语言任务。
- en: 'In this article, we will focus on the encoder architecture (the red block on
    the picture). This is what the popular BERT model is using under the hood: the
    primary focus is on **understanding and representing the data**, rather than generating
    sequences. It can be used for a variety of applications: text classification,
    named-entity recognition (NER), extractive question answering, etc.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将重点介绍编码器架构（图中的红色块）。这正是流行的BERT模型在后台使用的架构：其主要关注的是**理解和表示数据**，而不是生成序列。它可以用于多种应用：文本分类、命名实体识别（NER）、抽取式问答等。
- en: So, how is the data actually transformed by this architecture? We will explain
    each component in detail, but here is an overview of the process.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这些数据究竟是如何通过该架构进行转换的呢？我们将详细解释每个组件，但这里是过程的概述。
- en: 'The input text is **tokenized**: the Python string is transformed into a list
    of tokens (numbers)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入文本被**标记化**：Python字符串被转换成标记（数字）列表。
- en: Each token is passed through an **Embedding layer** that outputs a vector representation
    for each token
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个标记都通过一个**嵌入层**，该层输出每个标记的向量表示。
- en: The embeddings are then further encoded with a **Positional Encoding layer**,
    adding information about the position of each token in the sequence
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，这些嵌入会通过**位置编码层**进一步编码，添加每个标记在序列中的位置信息。
- en: These new embeddings are transformed by a series of **Encoder Layers**, using
    a self-attention mechanism
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些新的嵌入通过一系列**编码器层**进行转换，使用自注意力机制。
- en: A **task-specific head** can be added. For example, we will later use a classification
    head to classify movie reviews as positive or negative
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以添加一个**任务特定的头**。例如，我们稍后将使用一个分类头，将电影评论分类为正面或负面。
- en: That is important to understand that the Transformer architecture transforms
    the embedding vectors by mapping them from one representation in a high-dimensional
    space to another within the same space, applying a series of complex transformations.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要理解，Transformer架构通过将嵌入向量从高维空间中的一个表示映射到同一空间中的另一个表示，应用一系列复杂的变换来转换这些嵌入。
- en: Implementing an encoder architecture in Python
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Python中实现编码器架构
- en: The Positional Encoder layer
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 位置编码器层
- en: 'Unlike RNN models, the attention mechanism makes no use of the order of the
    input sequence. The PositionalEncoder class adds positional encodings to the input
    embeddings, using two mathematical functions: cosine and sine.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 与RNN模型不同，自注意力机制不利用输入序列的顺序。PositionalEncoder类通过使用两种数学函数：余弦和正弦，向输入嵌入添加位置编码。
- en: '![](../Images/578ef5784eef3551c213f00d3bfdbdd4.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/578ef5784eef3551c213f00d3bfdbdd4.png)'
- en: Positional encoding matrix definition from [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码矩阵定义来自[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
- en: 'Note that positional encodings don’t contain trainable parameters: there are
    the results of deterministic computations, which makes this method very tractable.
    Also, sine and cosine functions take values between -1 and 1 and have useful periodicity
    properties to help the model learn patterns about the **relative positions of
    words**.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，位置编码不包含可训练的参数：它们是确定性计算的结果，这使得该方法非常可处理。此外，正弦和余弦函数的值介于-1和1之间，并具有有助于模型学习**单词相对位置**的有用周期性特性。
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Multi-Head Self-Attention
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多头自注意力
- en: The self-attention mechanism is the key component of the encoder architecture.
    Let’s ignore the “multi-head” for now. Attention is a way to determine for each
    token (i.e. each embedding) the **relevance of all other embeddings to that token**,
    to obtain a more refined and contextually relevant encoding.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力机制是编码器架构的关键组件。我们暂时忽略“多头”部分。注意力是一种方法，用来确定每个标记（即每个嵌入）与**所有其他嵌入与该标记的相关性**，从而获得更精细和与上下文相关的编码。
- en: '![](../Images/cd6d84be3aae29b2a4108ac244d7dd6b.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cd6d84be3aae29b2a4108ac244d7dd6b.png)'
- en: How does“it” pay attention to other words of the sequence? ([The Illustrated
    Transformer](https://jalammar.github.io/illustrated-transformer/))
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: “它”是如何关注序列中其他单词的？([The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/))
- en: There are 3 steps in the self-attention mechanism.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力机制有三个步骤。
- en: Use matrices Q, K, and V to respectively transform the inputs “**query**”, “**key**”
    and “**value**”. Note that for self-attention, query, key, and values are all
    equal to our input embedding
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用矩阵Q、K和V分别转换输入的“**查询**”、“**键**”和“**值**”。请注意，对于自注意力机制，查询、键和值都是等于我们的输入嵌入。
- en: Compute the attention score using cosine similarity (a dot product) between
    the **query** and the **key**. Scores are scaled by the square root of the embedding
    dimension to stabilize the gradients during training
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过余弦相似度（点积）计算**查询**和**键**之间的注意力得分。得分会通过嵌入维度的平方根进行缩放，以稳定训练过程中的梯度。
- en: Use a softmax layer to make these scores **probabilities**
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 softmax 层将这些得分转化为**概率**。
- en: The output is the weighted average of the **values**, using the attention scores
    as the weights
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出是**值**的加权平均，使用注意力得分作为权重。
- en: Mathematically, this corresponds to the following formula.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度来看，这对应于以下公式。
- en: '![](../Images/c5773d753ad9db7bab34c7e7739de18c.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c5773d753ad9db7bab34c7e7739de18c.png)'
- en: The Attention Mechanism from [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[注意力机制来自《Attention Is All You Need》](https://arxiv.org/pdf/1706.03762.pdf)'
- en: What does “multi-head” mean? Basically, we can apply the described self-attention
    mechanism process several times, in parallel, and concatenate and project the
    outputs. This allows each head to f**ocus on different semantic aspects of the
    sentence**.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: “多头”是什么意思？基本上，我们可以并行多次应用上述自注意力机制过程，并将输出进行拼接和投影。这使得每个头可以**专注于句子的不同语义方面**。
- en: We start by defining the number of heads, the dimension of the embeddings (d_model),
    and the dimension of each head (head_dim). We also initialize the Q, K, and V
    matrices (linear layers), and the final projection layer.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义头的数量、嵌入的维度（d_model）以及每个头的维度（head_dim）。我们还初始化了 Q、K 和 V 矩阵（线性层），以及最终的投影层。
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: When using multi-head attention, we apply each attention head with a reduced
    dimension (head_dim instead of d_model) as in the original paper, making the total
    computational cost similar to a one-head attention layer with full dimensionality.
    Note this is a logical split only. What makes multi-attention so powerful is it
    can still be represented via a single matrix operation, making computations very
    efficient on GPUs.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多头注意力时，我们对每个注意力头应用一个减少维度的处理（使用 head_dim 而非 d_model），就像原论文中所述，这使得总的计算成本类似于一个全维度的单头注意力层。请注意，这只是一个逻辑上的拆分。多头注意力之所以强大，是因为它仍然可以通过单一的矩阵操作来表示，从而使得
    GPU 上的计算非常高效。
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We compute the attention scores and use a mask to avoid using attention on padded
    tokens. We apply a softmax activation to make these scores probabilities.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算注意力得分，并使用掩码避免在填充的标记上使用注意力。我们应用 softmax 激活函数将这些得分转化为概率。
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The forward attribute performs the multi-head logical split and computes the
    attention weights. Then, we get the output by multiplying these weights by the
    values. Finally, we reshape the output and project it with a linear layer.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward` 属性执行多头逻辑拆分并计算注意力权重。然后，我们通过将这些权重与值相乘来获得输出。最后，我们重新调整输出的形状并通过线性层进行投影。'
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The Encoder Layer
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器层
- en: This is the main component of the architecture, which leverages multi-head self-attention.
    We first implement a simple class to perform a feed-forward operation through
    2 dense layers.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这是该架构的主要组件，它利用了多头自注意力。我们首先实现一个简单的类，通过 2 个全连接层执行前向操作。
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can now code the logic for the encoder layer. We start by applying self-attention
    to the input, which gives a vector of the same dimension. We then use our mini
    feed-forward network with Layer Norm layers. Note that we also use skip connections
    before applying normalization.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以编写编码器层的逻辑。我们首先对输入应用自注意力，得到一个相同维度的向量。然后我们使用带有层归一化层的小型前馈网络。请注意，我们在应用归一化之前还使用了跳跃连接。
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Putting Everything Together
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将一切整合在一起
- en: It’s time to create our final model. We pass our data through an embedding layer.
    This transforms our raw tokens (integers) into a numerical vector. We then apply
    our positional encoder and several (num_layers) encoder layers.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候创建我们的最终模型了。我们通过嵌入层将数据传递进去。这将原始标记（整数）转换为数值向量。然后我们应用位置编码器和若干（num_layers）编码器层。
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We also create a ClassifierHead class which is used to transform the final embedding
    into class probabilities for our classification task.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还创建了一个 `ClassifierHead` 类，用于将最终的嵌入转换为分类任务的类别概率。
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note that the dense and softmax layers are only applied on the first embedding
    (corresponding to the first token of our input sequence). This is because when
    tokenizing the text, the first token is the [CLS] token which stands for “classification.”
    The [CLS] token is designed to aggregate the entire sequence’s information into
    a single embedding vector, serving as a summary representation that can be used
    for classification tasks.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，稠密层和 softmax 层只应用于第一个嵌入（对应于输入序列的第一个标记）。这是因为在分词文本时，第一个标记是[CLS]标记，代表“分类”。[CLS]标记的设计目的是将整个序列的信息聚合成一个单一的嵌入向量，作为可以用于分类任务的摘要表示。
- en: 'Note: the concept of including a [CLS] token originates from BERT, which was
    initially trained on tasks like next-sentence prediction. The [CLS] token was
    inserted to predict the likelihood that sentence B follows sentence A, with a
    [SEP] token separating the 2 sentences. For our model, the [SEP] token simply
    marks the end of the input sentence, as shown below.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：包含[CLS]标记的概念源自 BERT，BERT 最初是在类似下一句预测任务上进行训练的。[CLS]标记被插入以预测句子 B 是否跟随句子 A，而[SEP]标记则分隔这两个句子。对于我们的模型，[SEP]标记只是标记输入句子的结束，如下所示。
- en: '![](../Images/d3feb19e839643803e928bfc05828ad2.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d3feb19e839643803e928bfc05828ad2.png)'
- en: '[CLS] Token in BERT Architecture ([All About AI](https://seunghan96.github.io/dl/nlp/28.-nlp-BERT-%EC%9D%B4%EB%A1%A0/))'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 架构中的[CLS]标记 ([All About AI](https://seunghan96.github.io/dl/nlp/28.-nlp-BERT-%EC%9D%B4%EB%A1%A0/))
- en: When you think about it, it’s really mind-blowing that this single [CLS] embedding
    is able to capture so much information about the entire sequence, thanks to the
    self-attention mechanism’s ability to weigh and synthesize the importance of every
    piece of the text in relation to each other.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当你仔细想想，真是令人震惊，这个单一的[CLS]嵌入能够捕获如此多关于整个序列的信息，这要归功于自注意力机制能够权衡和综合每个文本片段之间相互关系的重要性。
- en: Training and visualization
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练与可视化
- en: Hopefully, the previous section gives you a better understanding of how our
    Transformer model transforms the input data. We will now write our training pipeline
    for our binary classification task using the IMDB dataset (movie reviews). Then,
    we will visualize the embedding of the [CLS] token during the training process
    to see how our model transformed it.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 希望上一节能帮助你更好地理解我们的 Transformer 模型是如何转换输入数据的。接下来，我们将编写我们的训练流程，用于处理 IMDB 数据集（电影评论）的二分类任务。然后，我们将可视化训练过程中[CLS]标记的嵌入，看看我们的模型是如何转换它的。
- en: We first define our hyperparameters, as well as a BERT tokenizer. In the GitHub
    repository, you can see that I also coded a function to select a subset of the
    dataset with only 1200 train and 200 test examples.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义超参数，并且使用 BERT 分词器。在 GitHub 仓库中，你可以看到我还编写了一个函数，来选择数据集的一个子集，其中包含 1200 个训练样本和
    200 个测试样本。
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You can try to use the BERT tokenizer on one of the sentences:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试在其中一个句子上使用 BERT 分词器：
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Every sequence should start with the token 101, corresponding to [CLS], followed
    by some non-zero integers and padded with zeros if the sequence length is smaller
    than 256\. Note that these zeros are ignored during the self-attention computation
    using our “mask”.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 每个序列应以 101 号标记开始，对应[CLS]，接着是一些非零整数，如果序列长度小于 256，则用零填充。注意，这些零在使用我们的“掩码”进行自注意力计算时会被忽略。
- en: '[PRE11]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can now write our train function:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以编写训练函数：
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You can find the collect_embeddings and visualize_embeddings functions in the
    GitHub repo. They store the [CLS] token embedding for each sentence of the training
    set, apply a dimensionality reduction technique called t-SNE to make them 2D vectors
    (instead of 256-dimensional vectors), and save an animated plot.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 GitHub 仓库中找到 collect_embeddings 和 visualize_embeddings 函数。它们存储了训练集每个句子的[CLS]标记嵌入，应用一种叫做
    t-SNE 的降维技术将其转化为二维向量（而不是 256 维向量），并保存动画图表。
- en: Let’s visualize the results.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来可视化结果。
- en: '![](../Images/23987d3afe279c76504d4c31a47e8066.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23987d3afe279c76504d4c31a47e8066.png)'
- en: Projected [CLS] embeddings for each training point (blue corresponds to positive
    sentences, red corresponds to negative sentences)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 每个训练点的投影[CLS]嵌入（蓝色代表正向句子，红色代表负向句子）
- en: Observing the plot of projected [CLS] embeddings for each training point, we
    can see the clear distinction between positive (blue) and negative (red) sentences
    after a few epochs. This visual shows the remarkable capability of the Transformer
    architecture to adapt embeddings over time and highlights the power of the self-attention
    mechanism. The data is transformed in such a way that embeddings for each class
    are well separated, thereby significantly simplifying the task for the classifier
    head.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 通过观察每个训练点的[CLS]嵌入的投影图，我们可以看到经过若干轮训练后，正向（蓝色）和负向（红色）句子之间的明显区别。这一可视化展示了Transformer架构随着时间推移调整嵌入的显著能力，突出了自注意力机制的强大功能。数据以一种方式进行转化，使得每个类别的嵌入得到了良好的分离，从而大大简化了分类器头的任务。
- en: Conclusion
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'As we conclude our exploration of the Transformer architecture, it’s evident
    that these models are adept at tailoring data to a given task. With the use of
    positional encoding and multi-head self-attention, Transformers go beyond mere
    data processing: they interpret and understand information with a level of sophistication
    previously unseen. The ability to dynamically weigh the relevance of different
    parts of the input data allows for a more nuanced understanding and representation
    of the input text. This enhances performance across a wide array of downstream
    tasks, including text classification, question answering, named entity recognition,
    and more.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们对Transformer架构的探索的结束，显然这些模型擅长将数据定制化以适应特定任务。通过使用位置编码和多头自注意力机制，Transformer不仅仅是处理数据：它们以一种前所未见的复杂程度来解释和理解信息。能够动态地权衡输入数据不同部分的相关性，使得对输入文本的理解和表示更加细致。这提升了在各种下游任务中的表现，包括文本分类、问答、命名实体识别等。
- en: Now that you have a better understanding of the encoder architecture, you are
    ready to delve into decoder and encoder-decoder models, which are very similar
    to what we have just explored. Decoders play a pivotal role in generative tasks
    and are at the core of the popular GPT models.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经更好地理解了编码器架构，你可以深入探讨解码器和编码器-解码器模型，这些模型与我们刚刚探讨的非常相似。解码器在生成任务中起着至关重要的作用，是流行的GPT模型的核心部分。
- en: Feel free to connect on [LinkedIn](https://www.linkedin.com/in/maxime-wolf/)
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随时在[LinkedIn](https://www.linkedin.com/in/maxime-wolf/)上与我联系
- en: Follow me on [GitHub](https://github.com/maxime7770) for more content
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请在[GitHub](https://github.com/maxime7770)上关注我，获取更多内容
- en: 'Visit my website: [maximewolf.com](http://maximewolf.com)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '访问我的网站: [maximewolf.com](http://maximewolf.com)'
- en: '**References**'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: '[1] Vaswani, Ashish, et al. “Attention Is All You Need.” *31st Conference on
    Neural Information Processing Systems (NIPS 2017)*, Long Beach, CA, USA.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Vaswani, Ashish, 等人. “Attention Is All You Need.” *第31届神经信息处理系统会议（NIPS
    2017）*, 美国加利福尼亚州长滩.'
- en: '[2] “The Illustrated Transformer.” *Jay Alammar’s Blog*, June 2018, [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] “The Illustrated Transformer.” *Jay Alammar的博客*, 2018年6月, [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/)'
- en: '[3] Official PyTorch Implementation of the Transformer Architecture. *GitHub
    repository*, PyTorch, [https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.py](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.py)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Transformer架构的官方PyTorch实现. *GitHub代码库*, PyTorch, [https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.py](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.py)'
- en: '[4] Manning, Christopher, et al. “CS224n: Natural Language Processing with
    Deep Learning.” *Stanford University*, Stanford CS224N NLP course, [http://web.stanford.edu/class/cs224n/](http://web.stanford.edu/class/cs224n/)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Manning, Christopher, 等人. “CS224n: 使用深度学习进行自然语言处理.” *斯坦福大学*, 斯坦福CS224N
    NLP课程, [http://web.stanford.edu/class/cs224n/](http://web.stanford.edu/class/cs224n/)'
