- en: 'Transformers: How Do They Transform Your Data?'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformerï¼šå®ƒä»¬å¦‚ä½•è½¬åŒ–ä½ çš„æ•°æ®ï¼Ÿ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/transformers-how-do-they-transform-your-data-72d69e383e0d?source=collection_archive---------0-----------------------#2024-03-28](https://towardsdatascience.com/transformers-how-do-they-transform-your-data-72d69e383e0d?source=collection_archive---------0-----------------------#2024-03-28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/transformers-how-do-they-transform-your-data-72d69e383e0d?source=collection_archive---------0-----------------------#2024-03-28](https://towardsdatascience.com/transformers-how-do-they-transform-your-data-72d69e383e0d?source=collection_archive---------0-----------------------#2024-03-28)
- en: Diving into the Transformers architecture and what makes them unbeatable at
    language tasks
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ·±å…¥æ¢ç´¢Transformeræ¶æ„åŠå…¶åœ¨è¯­è¨€ä»»åŠ¡ä¸­æ— æ•Œçš„åŸå› 
- en: '[](https://medium.com/@maxwolf34?source=post_page---byline--72d69e383e0d--------------------------------)[![Maxime
    Wolf](../Images/259b3659d0e6dd1d0f0eec4ae92d02e9.png)](https://medium.com/@maxwolf34?source=post_page---byline--72d69e383e0d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--72d69e383e0d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--72d69e383e0d--------------------------------)
    [Maxime Wolf](https://medium.com/@maxwolf34?source=post_page---byline--72d69e383e0d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@maxwolf34?source=post_page---byline--72d69e383e0d--------------------------------)[![Maxime
    Wolf](../Images/259b3659d0e6dd1d0f0eec4ae92d02e9.png)](https://medium.com/@maxwolf34?source=post_page---byline--72d69e383e0d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--72d69e383e0d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--72d69e383e0d--------------------------------)
    [Maxime Wolf](https://medium.com/@maxwolf34?source=post_page---byline--72d69e383e0d--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--72d69e383e0d--------------------------------)
    Â·11 min readÂ·Mar 28, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--72d69e383e0d--------------------------------)
    Â·é˜…è¯»æ—¶é—´11åˆ†é’ŸÂ·2024å¹´3æœˆ28æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4dd0f4cd464558d94b73f8274580ac94.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4dd0f4cd464558d94b73f8274580ac94.png)'
- en: Image by the author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: 'In the rapidly evolving landscape of artificial intelligence and machine learning,
    one innovation stands out for its profound impact on how we process, understand,
    and generate data: **Transformers**. Transformers have revolutionized the field
    of natural language processing (NLP) and beyond, powering some of todayâ€™s most
    advanced AI applications. But what exactly are Transformers, and how do they manage
    to transform data in such groundbreaking ways? This article demystifies the inner
    workings of Transformer models, focusing on the **encoder architecture**. We will
    start by going through the implementation of a Transformer encoder in Python,
    breaking down its main components. Then, we will visualize how Transformers process
    and adapt input data during training.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ é£é€Ÿå‘å±•çš„ä»Šå¤©ï¼Œæœ‰ä¸€ç§åˆ›æ–°è„±é¢–è€Œå‡ºï¼Œå¯¹æˆ‘ä»¬å¤„ç†ã€ç†è§£å’Œç”Ÿæˆæ•°æ®çš„æ–¹å¼äº§ç”Ÿäº†æ·±è¿œå½±å“ï¼š**Transformer**ã€‚Transformerå½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰åŠå…¶ä»–é¢†åŸŸï¼Œä¸ºä»Šå¤©ä¸€äº›æœ€å…ˆè¿›çš„AIåº”ç”¨æä¾›äº†åŠ¨åŠ›ã€‚ä½†ç©¶ç«Ÿä»€ä¹ˆæ˜¯Transformerï¼Œå®ƒä»¬åˆæ˜¯å¦‚ä½•ä»¥å¦‚æ­¤çªç ´æ€§çš„æ–¹å¼è½¬æ¢æ•°æ®çš„å‘¢ï¼Ÿæœ¬æ–‡å°†æ­ç§˜Transformeræ¨¡å‹çš„å†…éƒ¨å·¥ä½œåŸç†ï¼Œé‡ç‚¹è®²è§£**ç¼–ç å™¨æ¶æ„**ã€‚æˆ‘ä»¬å°†ä»Pythonä¸­Transformerç¼–ç å™¨çš„å®ç°å…¥æ‰‹ï¼Œé€æ­¥è§£æå…¶ä¸»è¦ç»„ä»¶ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†å¯è§†åŒ–Transformerå¦‚ä½•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¤„ç†å¹¶é€‚åº”è¾“å…¥æ•°æ®ã€‚
- en: While this blog doesnâ€™t cover every architectural detail, it provides an implementation
    and an overall understanding of the transformative power of Transformers. For
    an in-depth explanation of Transformers, I suggest you look at the excellent Stanford
    CS224-n course.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æœ¬åšå®¢å¹¶æœªæ¶µç›–æ‰€æœ‰æ¶æ„ç»†èŠ‚ï¼Œä½†å®ƒæä¾›äº†ä¸€ä¸ªå®ç°å¹¶å¸®åŠ©ä½ å…¨é¢ç†è§£Transformerçš„å˜é©æ€§åŠ›é‡ã€‚æƒ³æ·±å…¥äº†è§£Transformerçš„å·¥ä½œåŸç†ï¼Œæˆ‘å»ºè®®ä½ å‚è€ƒæ–¯å¦ç¦å¤§å­¦çš„ä¼˜ç§€CS224-nè¯¾ç¨‹ã€‚
- en: I also recommend following the [GitHub repository](https://github.com/maxime7770/Transformers-Insights)
    associated with this article for additional details. ğŸ˜Š
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è¿˜å»ºè®®å…³æ³¨ä¸æœ¬æ–‡ç›¸å…³çš„[GitHub ä»“åº“](https://github.com/maxime7770/Transformers-Insights)ä»¥è·å–æ›´å¤šç»†èŠ‚ã€‚ğŸ˜Š
- en: What is a Transformer encoder architecture?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯Transformerç¼–ç å™¨æ¶æ„ï¼Ÿ
- en: '![](../Images/df39c5bc0e96c04388b637bb391a7fed.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df39c5bc0e96c04388b637bb391a7fed.png)'
- en: The Transformer model from [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥è‡ª[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)çš„Transformeræ¨¡å‹
- en: This picture shows the original Transformer architecture, combining an encoder
    and a decoder for sequence-to-sequence language tasks.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†åŸå§‹Transformeræ¶æ„ï¼Œå°†ç¼–ç å™¨å’Œè§£ç å™¨ç»“åˆç”¨äºåºåˆ—åˆ°åºåˆ—çš„è¯­è¨€ä»»åŠ¡ã€‚
- en: 'In this article, we will focus on the encoder architecture (the red block on
    the picture). This is what the popular BERT model is using under the hood: the
    primary focus is on **understanding and representing the data**, rather than generating
    sequences. It can be used for a variety of applications: text classification,
    named-entity recognition (NER), extractive question answering, etc.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†é‡ç‚¹ä»‹ç»ç¼–ç å™¨æ¶æ„ï¼ˆå›¾ä¸­çš„çº¢è‰²å—ï¼‰ã€‚è¿™æ­£æ˜¯æµè¡Œçš„BERTæ¨¡å‹åœ¨åå°ä½¿ç”¨çš„æ¶æ„ï¼šå…¶ä¸»è¦å…³æ³¨çš„æ˜¯**ç†è§£å’Œè¡¨ç¤ºæ•°æ®**ï¼Œè€Œä¸æ˜¯ç”Ÿæˆåºåˆ—ã€‚å®ƒå¯ä»¥ç”¨äºå¤šç§åº”ç”¨ï¼šæ–‡æœ¬åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ã€æŠ½å–å¼é—®ç­”ç­‰ã€‚
- en: So, how is the data actually transformed by this architecture? We will explain
    each component in detail, but here is an overview of the process.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œè¿™äº›æ•°æ®ç©¶ç«Ÿæ˜¯å¦‚ä½•é€šè¿‡è¯¥æ¶æ„è¿›è¡Œè½¬æ¢çš„å‘¢ï¼Ÿæˆ‘ä»¬å°†è¯¦ç»†è§£é‡Šæ¯ä¸ªç»„ä»¶ï¼Œä½†è¿™é‡Œæ˜¯è¿‡ç¨‹çš„æ¦‚è¿°ã€‚
- en: 'The input text is **tokenized**: the Python string is transformed into a list
    of tokens (numbers)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¾“å…¥æ–‡æœ¬è¢«**æ ‡è®°åŒ–**ï¼šPythonå­—ç¬¦ä¸²è¢«è½¬æ¢æˆæ ‡è®°ï¼ˆæ•°å­—ï¼‰åˆ—è¡¨ã€‚
- en: Each token is passed through an **Embedding layer** that outputs a vector representation
    for each token
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯ä¸ªæ ‡è®°éƒ½é€šè¿‡ä¸€ä¸ª**åµŒå…¥å±‚**ï¼Œè¯¥å±‚è¾“å‡ºæ¯ä¸ªæ ‡è®°çš„å‘é‡è¡¨ç¤ºã€‚
- en: The embeddings are then further encoded with a **Positional Encoding layer**,
    adding information about the position of each token in the sequence
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œè¿™äº›åµŒå…¥ä¼šé€šè¿‡**ä½ç½®ç¼–ç å±‚**è¿›ä¸€æ­¥ç¼–ç ï¼Œæ·»åŠ æ¯ä¸ªæ ‡è®°åœ¨åºåˆ—ä¸­çš„ä½ç½®ä¿¡æ¯ã€‚
- en: These new embeddings are transformed by a series of **Encoder Layers**, using
    a self-attention mechanism
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™äº›æ–°çš„åµŒå…¥é€šè¿‡ä¸€ç³»åˆ—**ç¼–ç å™¨å±‚**è¿›è¡Œè½¬æ¢ï¼Œä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚
- en: A **task-specific head** can be added. For example, we will later use a classification
    head to classify movie reviews as positive or negative
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯ä»¥æ·»åŠ ä¸€ä¸ª**ä»»åŠ¡ç‰¹å®šçš„å¤´**ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬ç¨åå°†ä½¿ç”¨ä¸€ä¸ªåˆ†ç±»å¤´ï¼Œå°†ç”µå½±è¯„è®ºåˆ†ç±»ä¸ºæ­£é¢æˆ–è´Ÿé¢ã€‚
- en: That is important to understand that the Transformer architecture transforms
    the embedding vectors by mapping them from one representation in a high-dimensional
    space to another within the same space, applying a series of complex transformations.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: é‡è¦çš„æ˜¯è¦ç†è§£ï¼ŒTransformeræ¶æ„é€šè¿‡å°†åµŒå…¥å‘é‡ä»é«˜ç»´ç©ºé—´ä¸­çš„ä¸€ä¸ªè¡¨ç¤ºæ˜ å°„åˆ°åŒä¸€ç©ºé—´ä¸­çš„å¦ä¸€ä¸ªè¡¨ç¤ºï¼Œåº”ç”¨ä¸€ç³»åˆ—å¤æ‚çš„å˜æ¢æ¥è½¬æ¢è¿™äº›åµŒå…¥ã€‚
- en: Implementing an encoder architecture in Python
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åœ¨Pythonä¸­å®ç°ç¼–ç å™¨æ¶æ„
- en: The Positional Encoder layer
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½ç½®ç¼–ç å™¨å±‚
- en: 'Unlike RNN models, the attention mechanism makes no use of the order of the
    input sequence. The PositionalEncoder class adds positional encodings to the input
    embeddings, using two mathematical functions: cosine and sine.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸RNNæ¨¡å‹ä¸åŒï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸åˆ©ç”¨è¾“å…¥åºåˆ—çš„é¡ºåºã€‚PositionalEncoderç±»é€šè¿‡ä½¿ç”¨ä¸¤ç§æ•°å­¦å‡½æ•°ï¼šä½™å¼¦å’Œæ­£å¼¦ï¼Œå‘è¾“å…¥åµŒå…¥æ·»åŠ ä½ç½®ç¼–ç ã€‚
- en: '![](../Images/578ef5784eef3551c213f00d3bfdbdd4.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/578ef5784eef3551c213f00d3bfdbdd4.png)'
- en: Positional encoding matrix definition from [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ç½®ç¼–ç çŸ©é˜µå®šä¹‰æ¥è‡ª[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
- en: 'Note that positional encodings donâ€™t contain trainable parameters: there are
    the results of deterministic computations, which makes this method very tractable.
    Also, sine and cosine functions take values between -1 and 1 and have useful periodicity
    properties to help the model learn patterns about the **relative positions of
    words**.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œä½ç½®ç¼–ç ä¸åŒ…å«å¯è®­ç»ƒçš„å‚æ•°ï¼šå®ƒä»¬æ˜¯ç¡®å®šæ€§è®¡ç®—çš„ç»“æœï¼Œè¿™ä½¿å¾—è¯¥æ–¹æ³•éå¸¸å¯å¤„ç†ã€‚æ­¤å¤–ï¼Œæ­£å¼¦å’Œä½™å¼¦å‡½æ•°çš„å€¼ä»‹äº-1å’Œ1ä¹‹é—´ï¼Œå¹¶å…·æœ‰æœ‰åŠ©äºæ¨¡å‹å­¦ä¹ **å•è¯ç›¸å¯¹ä½ç½®**çš„æœ‰ç”¨å‘¨æœŸæ€§ç‰¹æ€§ã€‚
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Multi-Head Self-Attention
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¤šå¤´è‡ªæ³¨æ„åŠ›
- en: The self-attention mechanism is the key component of the encoder architecture.
    Letâ€™s ignore the â€œmulti-headâ€ for now. Attention is a way to determine for each
    token (i.e. each embedding) the **relevance of all other embeddings to that token**,
    to obtain a more refined and contextually relevant encoding.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªæ³¨æ„åŠ›æœºåˆ¶æ˜¯ç¼–ç å™¨æ¶æ„çš„å…³é”®ç»„ä»¶ã€‚æˆ‘ä»¬æš‚æ—¶å¿½ç•¥â€œå¤šå¤´â€éƒ¨åˆ†ã€‚æ³¨æ„åŠ›æ˜¯ä¸€ç§æ–¹æ³•ï¼Œç”¨æ¥ç¡®å®šæ¯ä¸ªæ ‡è®°ï¼ˆå³æ¯ä¸ªåµŒå…¥ï¼‰ä¸**æ‰€æœ‰å…¶ä»–åµŒå…¥ä¸è¯¥æ ‡è®°çš„ç›¸å…³æ€§**ï¼Œä»è€Œè·å¾—æ›´ç²¾ç»†å’Œä¸ä¸Šä¸‹æ–‡ç›¸å…³çš„ç¼–ç ã€‚
- en: '![](../Images/cd6d84be3aae29b2a4108ac244d7dd6b.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cd6d84be3aae29b2a4108ac244d7dd6b.png)'
- en: How doesâ€œitâ€ pay attention to other words of the sequence? ([The Illustrated
    Transformer](https://jalammar.github.io/illustrated-transformer/))
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: â€œå®ƒâ€æ˜¯å¦‚ä½•å…³æ³¨åºåˆ—ä¸­å…¶ä»–å•è¯çš„ï¼Ÿ([The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/))
- en: There are 3 steps in the self-attention mechanism.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªæ³¨æ„åŠ›æœºåˆ¶æœ‰ä¸‰ä¸ªæ­¥éª¤ã€‚
- en: Use matrices Q, K, and V to respectively transform the inputs â€œ**query**â€, â€œ**key**â€
    and â€œ**value**â€. Note that for self-attention, query, key, and values are all
    equal to our input embedding
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨çŸ©é˜µQã€Kå’ŒVåˆ†åˆ«è½¬æ¢è¾“å…¥çš„â€œ**æŸ¥è¯¢**â€ã€â€œ**é”®**â€å’Œâ€œ**å€¼**â€ã€‚è¯·æ³¨æ„ï¼Œå¯¹äºè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ŒæŸ¥è¯¢ã€é”®å’Œå€¼éƒ½æ˜¯ç­‰äºæˆ‘ä»¬çš„è¾“å…¥åµŒå…¥ã€‚
- en: Compute the attention score using cosine similarity (a dot product) between
    the **query** and the **key**. Scores are scaled by the square root of the embedding
    dimension to stabilize the gradients during training
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆç‚¹ç§¯ï¼‰è®¡ç®—**æŸ¥è¯¢**å’Œ**é”®**ä¹‹é—´çš„æ³¨æ„åŠ›å¾—åˆ†ã€‚å¾—åˆ†ä¼šé€šè¿‡åµŒå…¥ç»´åº¦çš„å¹³æ–¹æ ¹è¿›è¡Œç¼©æ”¾ï¼Œä»¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¢¯åº¦ã€‚
- en: Use a softmax layer to make these scores **probabilities**
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ softmax å±‚å°†è¿™äº›å¾—åˆ†è½¬åŒ–ä¸º**æ¦‚ç‡**ã€‚
- en: The output is the weighted average of the **values**, using the attention scores
    as the weights
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¾“å‡ºæ˜¯**å€¼**çš„åŠ æƒå¹³å‡ï¼Œä½¿ç”¨æ³¨æ„åŠ›å¾—åˆ†ä½œä¸ºæƒé‡ã€‚
- en: Mathematically, this corresponds to the following formula.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ•°å­¦è§’åº¦æ¥çœ‹ï¼Œè¿™å¯¹åº”äºä»¥ä¸‹å…¬å¼ã€‚
- en: '![](../Images/c5773d753ad9db7bab34c7e7739de18c.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c5773d753ad9db7bab34c7e7739de18c.png)'
- en: The Attention Mechanism from [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[æ³¨æ„åŠ›æœºåˆ¶æ¥è‡ªã€ŠAttention Is All You Needã€‹](https://arxiv.org/pdf/1706.03762.pdf)'
- en: What does â€œmulti-headâ€ mean? Basically, we can apply the described self-attention
    mechanism process several times, in parallel, and concatenate and project the
    outputs. This allows each head to f**ocus on different semantic aspects of the
    sentence**.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: â€œå¤šå¤´â€æ˜¯ä»€ä¹ˆæ„æ€ï¼ŸåŸºæœ¬ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥å¹¶è¡Œå¤šæ¬¡åº”ç”¨ä¸Šè¿°è‡ªæ³¨æ„åŠ›æœºåˆ¶è¿‡ç¨‹ï¼Œå¹¶å°†è¾“å‡ºè¿›è¡Œæ‹¼æ¥å’ŒæŠ•å½±ã€‚è¿™ä½¿å¾—æ¯ä¸ªå¤´å¯ä»¥**ä¸“æ³¨äºå¥å­çš„ä¸åŒè¯­ä¹‰æ–¹é¢**ã€‚
- en: We start by defining the number of heads, the dimension of the embeddings (d_model),
    and the dimension of each head (head_dim). We also initialize the Q, K, and V
    matrices (linear layers), and the final projection layer.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆå®šä¹‰å¤´çš„æ•°é‡ã€åµŒå…¥çš„ç»´åº¦ï¼ˆd_modelï¼‰ä»¥åŠæ¯ä¸ªå¤´çš„ç»´åº¦ï¼ˆhead_dimï¼‰ã€‚æˆ‘ä»¬è¿˜åˆå§‹åŒ–äº† Qã€K å’Œ V çŸ©é˜µï¼ˆçº¿æ€§å±‚ï¼‰ï¼Œä»¥åŠæœ€ç»ˆçš„æŠ•å½±å±‚ã€‚
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: When using multi-head attention, we apply each attention head with a reduced
    dimension (head_dim instead of d_model) as in the original paper, making the total
    computational cost similar to a one-head attention layer with full dimensionality.
    Note this is a logical split only. What makes multi-attention so powerful is it
    can still be represented via a single matrix operation, making computations very
    efficient on GPUs.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›æ—¶ï¼Œæˆ‘ä»¬å¯¹æ¯ä¸ªæ³¨æ„åŠ›å¤´åº”ç”¨ä¸€ä¸ªå‡å°‘ç»´åº¦çš„å¤„ç†ï¼ˆä½¿ç”¨ head_dim è€Œé d_modelï¼‰ï¼Œå°±åƒåŸè®ºæ–‡ä¸­æ‰€è¿°ï¼Œè¿™ä½¿å¾—æ€»çš„è®¡ç®—æˆæœ¬ç±»ä¼¼äºä¸€ä¸ªå…¨ç»´åº¦çš„å•å¤´æ³¨æ„åŠ›å±‚ã€‚è¯·æ³¨æ„ï¼Œè¿™åªæ˜¯ä¸€ä¸ªé€»è¾‘ä¸Šçš„æ‹†åˆ†ã€‚å¤šå¤´æ³¨æ„åŠ›ä¹‹æ‰€ä»¥å¼ºå¤§ï¼Œæ˜¯å› ä¸ºå®ƒä»ç„¶å¯ä»¥é€šè¿‡å•ä¸€çš„çŸ©é˜µæ“ä½œæ¥è¡¨ç¤ºï¼Œä»è€Œä½¿å¾—
    GPU ä¸Šçš„è®¡ç®—éå¸¸é«˜æ•ˆã€‚
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We compute the attention scores and use a mask to avoid using attention on padded
    tokens. We apply a softmax activation to make these scores probabilities.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¡ç®—æ³¨æ„åŠ›å¾—åˆ†ï¼Œå¹¶ä½¿ç”¨æ©ç é¿å…åœ¨å¡«å……çš„æ ‡è®°ä¸Šä½¿ç”¨æ³¨æ„åŠ›ã€‚æˆ‘ä»¬åº”ç”¨ softmax æ¿€æ´»å‡½æ•°å°†è¿™äº›å¾—åˆ†è½¬åŒ–ä¸ºæ¦‚ç‡ã€‚
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The forward attribute performs the multi-head logical split and computes the
    attention weights. Then, we get the output by multiplying these weights by the
    values. Finally, we reshape the output and project it with a linear layer.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward` å±æ€§æ‰§è¡Œå¤šå¤´é€»è¾‘æ‹†åˆ†å¹¶è®¡ç®—æ³¨æ„åŠ›æƒé‡ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡å°†è¿™äº›æƒé‡ä¸å€¼ç›¸ä¹˜æ¥è·å¾—è¾“å‡ºã€‚æœ€åï¼Œæˆ‘ä»¬é‡æ–°è°ƒæ•´è¾“å‡ºçš„å½¢çŠ¶å¹¶é€šè¿‡çº¿æ€§å±‚è¿›è¡ŒæŠ•å½±ã€‚'
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The Encoder Layer
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨å±‚
- en: This is the main component of the architecture, which leverages multi-head self-attention.
    We first implement a simple class to perform a feed-forward operation through
    2 dense layers.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯è¯¥æ¶æ„çš„ä¸»è¦ç»„ä»¶ï¼Œå®ƒåˆ©ç”¨äº†å¤šå¤´è‡ªæ³¨æ„åŠ›ã€‚æˆ‘ä»¬é¦–å…ˆå®ç°ä¸€ä¸ªç®€å•çš„ç±»ï¼Œé€šè¿‡ 2 ä¸ªå…¨è¿æ¥å±‚æ‰§è¡Œå‰å‘æ“ä½œã€‚
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can now code the logic for the encoder layer. We start by applying self-attention
    to the input, which gives a vector of the same dimension. We then use our mini
    feed-forward network with Layer Norm layers. Note that we also use skip connections
    before applying normalization.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥ç¼–å†™ç¼–ç å™¨å±‚çš„é€»è¾‘ã€‚æˆ‘ä»¬é¦–å…ˆå¯¹è¾“å…¥åº”ç”¨è‡ªæ³¨æ„åŠ›ï¼Œå¾—åˆ°ä¸€ä¸ªç›¸åŒç»´åº¦çš„å‘é‡ã€‚ç„¶åæˆ‘ä»¬ä½¿ç”¨å¸¦æœ‰å±‚å½’ä¸€åŒ–å±‚çš„å°å‹å‰é¦ˆç½‘ç»œã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬åœ¨åº”ç”¨å½’ä¸€åŒ–ä¹‹å‰è¿˜ä½¿ç”¨äº†è·³è·ƒè¿æ¥ã€‚
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Putting Everything Together
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å°†ä¸€åˆ‡æ•´åˆåœ¨ä¸€èµ·
- en: Itâ€™s time to create our final model. We pass our data through an embedding layer.
    This transforms our raw tokens (integers) into a numerical vector. We then apply
    our positional encoder and several (num_layers) encoder layers.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ˜¯æ—¶å€™åˆ›å»ºæˆ‘ä»¬çš„æœ€ç»ˆæ¨¡å‹äº†ã€‚æˆ‘ä»¬é€šè¿‡åµŒå…¥å±‚å°†æ•°æ®ä¼ é€’è¿›å»ã€‚è¿™å°†åŸå§‹æ ‡è®°ï¼ˆæ•´æ•°ï¼‰è½¬æ¢ä¸ºæ•°å€¼å‘é‡ã€‚ç„¶åæˆ‘ä»¬åº”ç”¨ä½ç½®ç¼–ç å™¨å’Œè‹¥å¹²ï¼ˆnum_layersï¼‰ç¼–ç å™¨å±‚ã€‚
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We also create a ClassifierHead class which is used to transform the final embedding
    into class probabilities for our classification task.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜åˆ›å»ºäº†ä¸€ä¸ª `ClassifierHead` ç±»ï¼Œç”¨äºå°†æœ€ç»ˆçš„åµŒå…¥è½¬æ¢ä¸ºåˆ†ç±»ä»»åŠ¡çš„ç±»åˆ«æ¦‚ç‡ã€‚
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note that the dense and softmax layers are only applied on the first embedding
    (corresponding to the first token of our input sequence). This is because when
    tokenizing the text, the first token is the [CLS] token which stands for â€œclassification.â€
    The [CLS] token is designed to aggregate the entire sequenceâ€™s information into
    a single embedding vector, serving as a summary representation that can be used
    for classification tasks.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œç¨ å¯†å±‚å’Œ softmax å±‚åªåº”ç”¨äºç¬¬ä¸€ä¸ªåµŒå…¥ï¼ˆå¯¹åº”äºè¾“å…¥åºåˆ—çš„ç¬¬ä¸€ä¸ªæ ‡è®°ï¼‰ã€‚è¿™æ˜¯å› ä¸ºåœ¨åˆ†è¯æ–‡æœ¬æ—¶ï¼Œç¬¬ä¸€ä¸ªæ ‡è®°æ˜¯[CLS]æ ‡è®°ï¼Œä»£è¡¨â€œåˆ†ç±»â€ã€‚[CLS]æ ‡è®°çš„è®¾è®¡ç›®çš„æ˜¯å°†æ•´ä¸ªåºåˆ—çš„ä¿¡æ¯èšåˆæˆä¸€ä¸ªå•ä¸€çš„åµŒå…¥å‘é‡ï¼Œä½œä¸ºå¯ä»¥ç”¨äºåˆ†ç±»ä»»åŠ¡çš„æ‘˜è¦è¡¨ç¤ºã€‚
- en: 'Note: the concept of including a [CLS] token originates from BERT, which was
    initially trained on tasks like next-sentence prediction. The [CLS] token was
    inserted to predict the likelihood that sentence B follows sentence A, with a
    [SEP] token separating the 2 sentences. For our model, the [SEP] token simply
    marks the end of the input sentence, as shown below.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šåŒ…å«[CLS]æ ‡è®°çš„æ¦‚å¿µæºè‡ª BERTï¼ŒBERT æœ€åˆæ˜¯åœ¨ç±»ä¼¼ä¸‹ä¸€å¥é¢„æµ‹ä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒçš„ã€‚[CLS]æ ‡è®°è¢«æ’å…¥ä»¥é¢„æµ‹å¥å­ B æ˜¯å¦è·Ÿéšå¥å­ Aï¼Œè€Œ[SEP]æ ‡è®°åˆ™åˆ†éš”è¿™ä¸¤ä¸ªå¥å­ã€‚å¯¹äºæˆ‘ä»¬çš„æ¨¡å‹ï¼Œ[SEP]æ ‡è®°åªæ˜¯æ ‡è®°è¾“å…¥å¥å­çš„ç»“æŸï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚
- en: '![](../Images/d3feb19e839643803e928bfc05828ad2.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d3feb19e839643803e928bfc05828ad2.png)'
- en: '[CLS] Token in BERT Architecture ([All About AI](https://seunghan96.github.io/dl/nlp/28.-nlp-BERT-%EC%9D%B4%EB%A1%A0/))'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: BERT æ¶æ„ä¸­çš„[CLS]æ ‡è®° ([All About AI](https://seunghan96.github.io/dl/nlp/28.-nlp-BERT-%EC%9D%B4%EB%A1%A0/))
- en: When you think about it, itâ€™s really mind-blowing that this single [CLS] embedding
    is able to capture so much information about the entire sequence, thanks to the
    self-attention mechanismâ€™s ability to weigh and synthesize the importance of every
    piece of the text in relation to each other.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½ ä»”ç»†æƒ³æƒ³ï¼ŒçœŸæ˜¯ä»¤äººéœ‡æƒŠï¼Œè¿™ä¸ªå•ä¸€çš„[CLS]åµŒå…¥èƒ½å¤Ÿæ•è·å¦‚æ­¤å¤šå…³äºæ•´ä¸ªåºåˆ—çš„ä¿¡æ¯ï¼Œè¿™è¦å½’åŠŸäºè‡ªæ³¨æ„åŠ›æœºåˆ¶èƒ½å¤Ÿæƒè¡¡å’Œç»¼åˆæ¯ä¸ªæ–‡æœ¬ç‰‡æ®µä¹‹é—´ç›¸äº’å…³ç³»çš„é‡è¦æ€§ã€‚
- en: Training and visualization
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®­ç»ƒä¸å¯è§†åŒ–
- en: Hopefully, the previous section gives you a better understanding of how our
    Transformer model transforms the input data. We will now write our training pipeline
    for our binary classification task using the IMDB dataset (movie reviews). Then,
    we will visualize the embedding of the [CLS] token during the training process
    to see how our model transformed it.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å¸Œæœ›ä¸Šä¸€èŠ‚èƒ½å¸®åŠ©ä½ æ›´å¥½åœ°ç†è§£æˆ‘ä»¬çš„ Transformer æ¨¡å‹æ˜¯å¦‚ä½•è½¬æ¢è¾“å…¥æ•°æ®çš„ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ç¼–å†™æˆ‘ä»¬çš„è®­ç»ƒæµç¨‹ï¼Œç”¨äºå¤„ç† IMDB æ•°æ®é›†ï¼ˆç”µå½±è¯„è®ºï¼‰çš„äºŒåˆ†ç±»ä»»åŠ¡ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹ä¸­[CLS]æ ‡è®°çš„åµŒå…¥ï¼Œçœ‹çœ‹æˆ‘ä»¬çš„æ¨¡å‹æ˜¯å¦‚ä½•è½¬æ¢å®ƒçš„ã€‚
- en: We first define our hyperparameters, as well as a BERT tokenizer. In the GitHub
    repository, you can see that I also coded a function to select a subset of the
    dataset with only 1200 train and 200 test examples.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆå®šä¹‰è¶…å‚æ•°ï¼Œå¹¶ä¸”ä½¿ç”¨ BERT åˆ†è¯å™¨ã€‚åœ¨ GitHub ä»“åº“ä¸­ï¼Œä½ å¯ä»¥çœ‹åˆ°æˆ‘è¿˜ç¼–å†™äº†ä¸€ä¸ªå‡½æ•°ï¼Œæ¥é€‰æ‹©æ•°æ®é›†çš„ä¸€ä¸ªå­é›†ï¼Œå…¶ä¸­åŒ…å« 1200 ä¸ªè®­ç»ƒæ ·æœ¬å’Œ
    200 ä¸ªæµ‹è¯•æ ·æœ¬ã€‚
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You can try to use the BERT tokenizer on one of the sentences:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥å°è¯•åœ¨å…¶ä¸­ä¸€ä¸ªå¥å­ä¸Šä½¿ç”¨ BERT åˆ†è¯å™¨ï¼š
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Every sequence should start with the token 101, corresponding to [CLS], followed
    by some non-zero integers and padded with zeros if the sequence length is smaller
    than 256\. Note that these zeros are ignored during the self-attention computation
    using our â€œmaskâ€.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªåºåˆ—åº”ä»¥ 101 å·æ ‡è®°å¼€å§‹ï¼Œå¯¹åº”[CLS]ï¼Œæ¥ç€æ˜¯ä¸€äº›éé›¶æ•´æ•°ï¼Œå¦‚æœåºåˆ—é•¿åº¦å°äº 256ï¼Œåˆ™ç”¨é›¶å¡«å……ã€‚æ³¨æ„ï¼Œè¿™äº›é›¶åœ¨ä½¿ç”¨æˆ‘ä»¬çš„â€œæ©ç â€è¿›è¡Œè‡ªæ³¨æ„åŠ›è®¡ç®—æ—¶ä¼šè¢«å¿½ç•¥ã€‚
- en: '[PRE11]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can now write our train function:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥ç¼–å†™è®­ç»ƒå‡½æ•°ï¼š
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You can find the collect_embeddings and visualize_embeddings functions in the
    GitHub repo. They store the [CLS] token embedding for each sentence of the training
    set, apply a dimensionality reduction technique called t-SNE to make them 2D vectors
    (instead of 256-dimensional vectors), and save an animated plot.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨ GitHub ä»“åº“ä¸­æ‰¾åˆ° collect_embeddings å’Œ visualize_embeddings å‡½æ•°ã€‚å®ƒä»¬å­˜å‚¨äº†è®­ç»ƒé›†æ¯ä¸ªå¥å­çš„[CLS]æ ‡è®°åµŒå…¥ï¼Œåº”ç”¨ä¸€ç§å«åš
    t-SNE çš„é™ç»´æŠ€æœ¯å°†å…¶è½¬åŒ–ä¸ºäºŒç»´å‘é‡ï¼ˆè€Œä¸æ˜¯ 256 ç»´å‘é‡ï¼‰ï¼Œå¹¶ä¿å­˜åŠ¨ç”»å›¾è¡¨ã€‚
- en: Letâ€™s visualize the results.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¥å¯è§†åŒ–ç»“æœã€‚
- en: '![](../Images/23987d3afe279c76504d4c31a47e8066.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23987d3afe279c76504d4c31a47e8066.png)'
- en: Projected [CLS] embeddings for each training point (blue corresponds to positive
    sentences, red corresponds to negative sentences)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªè®­ç»ƒç‚¹çš„æŠ•å½±[CLS]åµŒå…¥ï¼ˆè“è‰²ä»£è¡¨æ­£å‘å¥å­ï¼Œçº¢è‰²ä»£è¡¨è´Ÿå‘å¥å­ï¼‰
- en: Observing the plot of projected [CLS] embeddings for each training point, we
    can see the clear distinction between positive (blue) and negative (red) sentences
    after a few epochs. This visual shows the remarkable capability of the Transformer
    architecture to adapt embeddings over time and highlights the power of the self-attention
    mechanism. The data is transformed in such a way that embeddings for each class
    are well separated, thereby significantly simplifying the task for the classifier
    head.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è§‚å¯Ÿæ¯ä¸ªè®­ç»ƒç‚¹çš„[CLS]åµŒå…¥çš„æŠ•å½±å›¾ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç»è¿‡è‹¥å¹²è½®è®­ç»ƒåï¼Œæ­£å‘ï¼ˆè“è‰²ï¼‰å’Œè´Ÿå‘ï¼ˆçº¢è‰²ï¼‰å¥å­ä¹‹é—´çš„æ˜æ˜¾åŒºåˆ«ã€‚è¿™ä¸€å¯è§†åŒ–å±•ç¤ºäº†Transformeræ¶æ„éšç€æ—¶é—´æ¨ç§»è°ƒæ•´åµŒå…¥çš„æ˜¾è‘—èƒ½åŠ›ï¼Œçªå‡ºäº†è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å¼ºå¤§åŠŸèƒ½ã€‚æ•°æ®ä»¥ä¸€ç§æ–¹å¼è¿›è¡Œè½¬åŒ–ï¼Œä½¿å¾—æ¯ä¸ªç±»åˆ«çš„åµŒå…¥å¾—åˆ°äº†è‰¯å¥½çš„åˆ†ç¦»ï¼Œä»è€Œå¤§å¤§ç®€åŒ–äº†åˆ†ç±»å™¨å¤´çš„ä»»åŠ¡ã€‚
- en: Conclusion
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: 'As we conclude our exploration of the Transformer architecture, itâ€™s evident
    that these models are adept at tailoring data to a given task. With the use of
    positional encoding and multi-head self-attention, Transformers go beyond mere
    data processing: they interpret and understand information with a level of sophistication
    previously unseen. The ability to dynamically weigh the relevance of different
    parts of the input data allows for a more nuanced understanding and representation
    of the input text. This enhances performance across a wide array of downstream
    tasks, including text classification, question answering, named entity recognition,
    and more.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€æˆ‘ä»¬å¯¹Transformeræ¶æ„çš„æ¢ç´¢çš„ç»“æŸï¼Œæ˜¾ç„¶è¿™äº›æ¨¡å‹æ“…é•¿å°†æ•°æ®å®šåˆ¶åŒ–ä»¥é€‚åº”ç‰¹å®šä»»åŠ¡ã€‚é€šè¿‡ä½¿ç”¨ä½ç½®ç¼–ç å’Œå¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ŒTransformerä¸ä»…ä»…æ˜¯å¤„ç†æ•°æ®ï¼šå®ƒä»¬ä»¥ä¸€ç§å‰æ‰€æœªè§çš„å¤æ‚ç¨‹åº¦æ¥è§£é‡Šå’Œç†è§£ä¿¡æ¯ã€‚èƒ½å¤ŸåŠ¨æ€åœ°æƒè¡¡è¾“å…¥æ•°æ®ä¸åŒéƒ¨åˆ†çš„ç›¸å…³æ€§ï¼Œä½¿å¾—å¯¹è¾“å…¥æ–‡æœ¬çš„ç†è§£å’Œè¡¨ç¤ºæ›´åŠ ç»†è‡´ã€‚è¿™æå‡äº†åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ†ç±»ã€é—®ç­”ã€å‘½åå®ä½“è¯†åˆ«ç­‰ã€‚
- en: Now that you have a better understanding of the encoder architecture, you are
    ready to delve into decoder and encoder-decoder models, which are very similar
    to what we have just explored. Decoders play a pivotal role in generative tasks
    and are at the core of the popular GPT models.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ å·²ç»æ›´å¥½åœ°ç†è§£äº†ç¼–ç å™¨æ¶æ„ï¼Œä½ å¯ä»¥æ·±å…¥æ¢è®¨è§£ç å™¨å’Œç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹ä¸æˆ‘ä»¬åˆšåˆšæ¢è®¨çš„éå¸¸ç›¸ä¼¼ã€‚è§£ç å™¨åœ¨ç”Ÿæˆä»»åŠ¡ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œæ˜¯æµè¡Œçš„GPTæ¨¡å‹çš„æ ¸å¿ƒéƒ¨åˆ†ã€‚
- en: Feel free to connect on [LinkedIn](https://www.linkedin.com/in/maxime-wolf/)
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: éšæ—¶åœ¨[LinkedIn](https://www.linkedin.com/in/maxime-wolf/)ä¸Šä¸æˆ‘è”ç³»
- en: Follow me on [GitHub](https://github.com/maxime7770) for more content
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯·åœ¨[GitHub](https://github.com/maxime7770)ä¸Šå…³æ³¨æˆ‘ï¼Œè·å–æ›´å¤šå†…å®¹
- en: 'Visit my website: [maximewolf.com](http://maximewolf.com)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'è®¿é—®æˆ‘çš„ç½‘ç«™: [maximewolf.com](http://maximewolf.com)'
- en: '**References**'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**å‚è€ƒæ–‡çŒ®**'
- en: '[1] Vaswani, Ashish, et al. â€œAttention Is All You Need.â€ *31st Conference on
    Neural Information Processing Systems (NIPS 2017)*, Long Beach, CA, USA.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Vaswani, Ashish, ç­‰äºº. â€œAttention Is All You Need.â€ *ç¬¬31å±Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿä¼šè®®ï¼ˆNIPS
    2017ï¼‰*, ç¾å›½åŠ åˆ©ç¦å°¼äºšå·é•¿æ»©.'
- en: '[2] â€œThe Illustrated Transformer.â€ *Jay Alammarâ€™s Blog*, June 2018, [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] â€œThe Illustrated Transformer.â€ *Jay Alammarçš„åšå®¢*, 2018å¹´6æœˆ, [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/)'
- en: '[3] Official PyTorch Implementation of the Transformer Architecture. *GitHub
    repository*, PyTorch, [https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.py](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.py)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Transformeræ¶æ„çš„å®˜æ–¹PyTorchå®ç°. *GitHubä»£ç åº“*, PyTorch, [https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.py](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.py)'
- en: '[4] Manning, Christopher, et al. â€œCS224n: Natural Language Processing with
    Deep Learning.â€ *Stanford University*, Stanford CS224N NLP course, [http://web.stanford.edu/class/cs224n/](http://web.stanford.edu/class/cs224n/)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Manning, Christopher, ç­‰äºº. â€œCS224n: ä½¿ç”¨æ·±åº¦å­¦ä¹ è¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†.â€ *æ–¯å¦ç¦å¤§å­¦*, æ–¯å¦ç¦CS224N
    NLPè¯¾ç¨‹, [http://web.stanford.edu/class/cs224n/](http://web.stanford.edu/class/cs224n/)'
