- en: 'AdEMAMix: A Deep Dive into a New Optimizer for Your Deep Neural Network'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AdEMAMix：深入探讨一种新的深度神经网络优化器
- en: 原文：[https://towardsdatascience.com/ademamix-a-deep-dive-into-a-new-optimizer-for-your-deep-neural-network-6168e2c1da35?source=collection_archive---------10-----------------------#2024-09-19](https://towardsdatascience.com/ademamix-a-deep-dive-into-a-new-optimizer-for-your-deep-neural-network-6168e2c1da35?source=collection_archive---------10-----------------------#2024-09-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/ademamix-a-deep-dive-into-a-new-optimizer-for-your-deep-neural-network-6168e2c1da35?source=collection_archive---------10-----------------------#2024-09-19](https://towardsdatascience.com/ademamix-a-deep-dive-into-a-new-optimizer-for-your-deep-neural-network-6168e2c1da35?source=collection_archive---------10-----------------------#2024-09-19)
- en: A better and faster option than the ADAM optimizer, from Apple Research
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一种比ADAM优化器更好且更快速的选择，来自苹果研究团队
- en: '[](https://saptashwa.medium.com/?source=post_page---byline--6168e2c1da35--------------------------------)[![Saptashwa
    Bhattacharyya](../Images/b01238113a1f6b91cb6fb0fbfa50303a.png)](https://saptashwa.medium.com/?source=post_page---byline--6168e2c1da35--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6168e2c1da35--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6168e2c1da35--------------------------------)
    [Saptashwa Bhattacharyya](https://saptashwa.medium.com/?source=post_page---byline--6168e2c1da35--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://saptashwa.medium.com/?source=post_page---byline--6168e2c1da35--------------------------------)[![Saptashwa
    Bhattacharyya](../Images/b01238113a1f6b91cb6fb0fbfa50303a.png)](https://saptashwa.medium.com/?source=post_page---byline--6168e2c1da35--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6168e2c1da35--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6168e2c1da35--------------------------------)
    [Saptashwa Bhattacharyya](https://saptashwa.medium.com/?source=post_page---byline--6168e2c1da35--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6168e2c1da35--------------------------------)
    ·12 min read·Sep 19, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6168e2c1da35--------------------------------)
    ·12分钟阅读·2024年9月19日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/48723d07a415766eeb3f87f164e02730.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/48723d07a415766eeb3f87f164e02730.png)'
- en: Which way to go? Let’s Optimize (Generated with DALL·E 3 with Author’s Prompt)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们该走哪条路？让我们优化一下吧（由DALL·E 3根据作者提示生成）
- en: Deep Neural Networks (DNNs) are regarded as one of the most effective tools
    for finding patterns in large datasets through training. At the core of the training
    problems, we have complex loss landscapes and the training of a DNN boils down
    to optimizing the loss as the number of iterations increases. A few of the most
    commonly used optimizers are Stochastic Gradient Descent, RMSProp (Root Mean Square
    Propagation), Adam (Adaptive Moment Estimation) etc.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络（DNN）被认为是通过训练在大规模数据集中发现模式的最有效工具之一。在训练问题的核心，我们面临复杂的损失函数地形，DNN的训练归结为随着迭代次数的增加优化损失。几种常用的优化器包括随机梯度下降、RMSProp（均方根传播）、Adam（自适应矩估计）等。
- en: Recently (September 2024), researchers from Apple (and EPFL) proposed a new
    optimizer, AdEMAMix¹, which they show to work better and faster than AdamW optimizer
    for language modeling and image classification tasks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最近（2024年9月），来自苹果（和瑞士联邦理工学院）的研究人员提出了一种新的优化器AdEMAMix¹，研究表明它在语言建模和图像分类任务中，比AdamW优化器表现得更好、更快。
- en: 'In this post, I will go into detail about the mathematical concepts behind
    this optimizer and discuss some very interesting results presented in this paper.
    Topics that will be covered in this post are:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将详细介绍这一优化器背后的数学概念，并讨论本文中提出的一些非常有趣的结果。本文将涵盖的主题包括：
- en: Review of Adam Optimizer
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adam优化器回顾
- en: Exponential Moving Average (EMA) in Adam.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adam中的指数加权移动平均（EMA）。
- en: 'The Main Idea Behind AdEMAMix: Mixture of two EMAs.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AdEMAMix背后的主要思想：两种EMA的混合。
- en: The Exponential Decay Rate Scheduler in AdEMAMix.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AdEMAMix中的指数衰减率调度器。
