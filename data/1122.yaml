- en: 'Courage to Learn ML: Tackling Vanishing and Exploding Gradients (Part 2)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习机器学习的勇气：解决梯度消失与爆炸问题（第二部分）
- en: 原文：[https://towardsdatascience.com/courage-to-learn-ml-tackling-vanishing-and-exploding-gradients-part-2-d0b8aed1ce7a?source=collection_archive---------9-----------------------#2024-05-03](https://towardsdatascience.com/courage-to-learn-ml-tackling-vanishing-and-exploding-gradients-part-2-d0b8aed1ce7a?source=collection_archive---------9-----------------------#2024-05-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/courage-to-learn-ml-tackling-vanishing-and-exploding-gradients-part-2-d0b8aed1ce7a?source=collection_archive---------9-----------------------#2024-05-03](https://towardsdatascience.com/courage-to-learn-ml-tackling-vanishing-and-exploding-gradients-part-2-d0b8aed1ce7a?source=collection_archive---------9-----------------------#2024-05-03)
- en: A Comprehensive Survey on Activation Functions, Weights Initialization, Batch
    Normalization, and Their Applications in PyTorch
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数、权重初始化、批归一化及其在 PyTorch 中的应用的全面调查
- en: '[](https://amyma101.medium.com/?source=post_page---byline--d0b8aed1ce7a--------------------------------)[![Amy
    Ma](../Images/2edf55456a1f92724535a1441fa2bef5.png)](https://amyma101.medium.com/?source=post_page---byline--d0b8aed1ce7a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d0b8aed1ce7a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d0b8aed1ce7a--------------------------------)
    [Amy Ma](https://amyma101.medium.com/?source=post_page---byline--d0b8aed1ce7a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://amyma101.medium.com/?source=post_page---byline--d0b8aed1ce7a--------------------------------)[![Amy
    Ma](../Images/2edf55456a1f92724535a1441fa2bef5.png)](https://amyma101.medium.com/?source=post_page---byline--d0b8aed1ce7a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d0b8aed1ce7a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d0b8aed1ce7a--------------------------------)
    [Amy Ma](https://amyma101.medium.com/?source=post_page---byline--d0b8aed1ce7a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d0b8aed1ce7a--------------------------------)
    ·37 min read·May 3, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d0b8aed1ce7a--------------------------------)
    ·37分钟阅读·2024年5月3日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Welcome back to a new chapter of “[Courage to Learn ML](https://towardsdatascience.com/tagged/courage-to-learn-ml).”
    For those new to this series, this series aims to make these complex topics accessible
    and engaging, much like a casual conversation between a mentor and a learner,
    inspired by the writing style of “[The Courage to Be Disliked](https://www.goodreads.com/book/show/43306206-the-courage-to-be-disliked),”
    with a specific focus on machine learning.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎回到“[学习机器学习的勇气](https://towardsdatascience.com/tagged/courage-to-learn-ml)”新的一章。对于初次接触本系列的读者，本系列旨在让这些复杂的主题变得易于理解并且富有趣味，就像是导师与学习者之间的轻松对话，写作风格受到《[敢于不讨好](https://www.goodreads.com/book/show/43306206-the-courage-to-be-disliked)》的启发，特别关注于机器学习。
- en: This time we will continue our exploration into how to overcome the challenges
    of vanishing and exploding gradients. In our opening segment, we talked about
    why it’s critical to maintain stable gradients to ensure effective learning within
    our networks. We uncovered how unstable gradients can be barriers to deepening
    our networks, essentially putting a cap on the potential of deep “learning”. To
    bring these concepts to life, we use the an analogy of running a miniature ice
    cream factory named DNN (short for Delicious Nutritious Nibbles), and draw parallels
    to illuminate potent strategies for DNN training akin to orchestrating a seamless
    factory production line.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，我们将继续探索如何克服梯度消失与爆炸的挑战。在开篇中，我们讨论了保持梯度稳定的重要性，以确保网络中有效的学习。我们揭示了不稳定的梯度如何成为加深网络的障碍，实质上限制了深度“学习”的潜力。为了更生动地阐述这些概念，我们使用了一个比喻，讲述了如何经营一个名为
    DNN（Delicious Nutritious Nibbles，意为美味营养小吃）的迷你冰淇淋工厂，并通过类比阐明了类似于协调无缝生产线的 DNN 训练策略。
- en: 'Now, in this second installment, we’re diving deeper into each proposed solution,
    examining them with the same clarity and creativity that brought our ice cream
    factory to life. Here are the list of topics we’d cover in this part:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在第二部分中，我们将深入探讨每个提出的解决方案，以与我们的冰淇淋工厂相同的清晰度和创造力来审视它们。以下是我们将在这一部分讨论的主题：
