- en: 'Mono to Stereo: How AI Is Breathing New Life into Music'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单声道到立体声：AI如何为音乐注入新生命
- en: 原文：[https://towardsdatascience.com/mono-to-stereo-how-ai-is-breathing-new-life-into-music-4180f1357db4?source=collection_archive---------4-----------------------#2024-12-24](https://towardsdatascience.com/mono-to-stereo-how-ai-is-breathing-new-life-into-music-4180f1357db4?source=collection_archive---------4-----------------------#2024-12-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/mono-to-stereo-how-ai-is-breathing-new-life-into-music-4180f1357db4?source=collection_archive---------4-----------------------#2024-12-24](https://towardsdatascience.com/mono-to-stereo-how-ai-is-breathing-new-life-into-music-4180f1357db4?source=collection_archive---------4-----------------------#2024-12-24)
- en: Applications and techniques for AI mono-to-stereo upmixing
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AI单声道到立体声升混的应用与技术
- en: '[](https://medium.com/@maxhilsdorf?source=post_page---byline--4180f1357db4--------------------------------)[![Max
    Hilsdorf](../Images/01da76c553e43d5ed6b6849bdbfd00da.png)](https://medium.com/@maxhilsdorf?source=post_page---byline--4180f1357db4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4180f1357db4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4180f1357db4--------------------------------)
    [Max Hilsdorf](https://medium.com/@maxhilsdorf?source=post_page---byline--4180f1357db4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@maxhilsdorf?source=post_page---byline--4180f1357db4--------------------------------)[![Max
    Hilsdorf](../Images/01da76c553e43d5ed6b6849bdbfd00da.png)](https://medium.com/@maxhilsdorf?source=post_page---byline--4180f1357db4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4180f1357db4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4180f1357db4--------------------------------)
    [Max Hilsdorf](https://medium.com/@maxhilsdorf?source=post_page---byline--4180f1357db4--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4180f1357db4--------------------------------)
    ·10 min read·Dec 24, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4180f1357db4--------------------------------)
    ·10分钟阅读·2024年12月24日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f0c2667236398f7d9c6b3b46cd7aa770.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f0c2667236398f7d9c6b3b46cd7aa770.png)'
- en: Image generated with DALL-E 3.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由DALL-E 3生成。
- en: '**Mono recordings are a snapshot of history**, but they lack the spatial richness
    that makes music feel truly alive. With AI, we can artificially transform mono
    recordings to stereo or even remix existing stereo recordings. In this article,
    we explore the practical use cases and methods for **mono-to-stereo upmixing**.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**单声道录音是历史的一个快照**，但它们缺乏使音乐真正充满生命力的空间感。通过人工智能，我们可以将单声道录音人工转换为立体声，甚至重新混音现有的立体声录音。本文将探讨**单声道到立体声的升混**的实际应用场景和方法。'
- en: Mono and Stereo in the physical and digital world
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物理和数字世界中的单声道与立体声
- en: '![](../Images/155441dd5ee6932d49cf578ac877540a.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/155441dd5ee6932d49cf578ac877540a.png)'
- en: Photo by [J](https://unsplash.com/@iamtheoldmanofthemountain?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[J](https://unsplash.com/@iamtheoldmanofthemountain?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: When an orchestra plays live, **sound waves** travel from different instruments
    through the room and to your ears. This causes differences in timing (when the
    sound reaches your ear) and loudness (how loud the sound appears in each ear).
    Through this process, a musical performance becomes more than harmony, timbre,
    and rhythm. Each instrument sends **spatial information**, immersing the listener
    in a “here and now” experience that grips their attention and emotions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个乐团现场演奏时，**声波**从不同的乐器传播到房间中的各个地方，最终传到你的耳朵。这会导致时间差异（声音到达耳朵的时间）和音量差异（每只耳朵听到的声音大小）。通过这个过程，音乐表演不仅仅是和声、音色和节奏。每个乐器都传递着**空间信息**，让听众沉浸在一种“此时此刻”的体验中，吸引他们的注意力并触动他们的情感。
- en: 'Listen to the difference between the first snippet (no spatial information),
    and the second snippet (clear differences between left and right ear):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 听听第一个片段（没有空间信息）和第二个片段（左右耳之间明显的差异）之间的区别：
- en: '*Headphones are strongly recommended throughout the article, but are not strictly
    necessary.*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*文章全程强烈推荐使用耳机，但并非绝对必要。*'
- en: '**Exampe: Mono**'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：单声道**'
- en: Song originally by [Lexin Music](https://pixabay.com/music/main-title-inspiring-cinematic-asia-116200/).
    Pixabay’s [content license](https://pixabay.com/service/license-summary/) applies.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 歌曲原由[Lexin Music](https://pixabay.com/music/main-title-inspiring-cinematic-asia-116200/)。适用Pixabay的[内容许可](https://pixabay.com/service/license-summary/)。
- en: '**Example: Stereo**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：立体声**'
- en: Song originally by [Lexin Music](https://pixabay.com/music/main-title-inspiring-cinematic-asia-116200/).
    Pixabay’s [content license](https://pixabay.com/service/license-summary/) applies.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这首歌最初由[Lexin Music](https://pixabay.com/music/main-title-inspiring-cinematic-asia-116200/)创作。Pixabay的[内容许可](https://pixabay.com/service/license-summary/)适用。
- en: As you can hear, the spatial information conveyed through a recording has a
    strong influence on the **liveliness and excitement** we perceive as listeners.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所听到的，通过录音传递的空间信息对我们作为听众感知的**生动性和兴奋感**有着强烈的影响。
- en: In digital audio, the most common formats are **mono** and **stereo**. A mono
    recording consists of only one audio signal that sounds exactly the same on both
    sides of your headphone earpieces (let’s call them **channels**). A stereo recording
    consists of two separate signals that are panned fully to the left and right channels,
    respectively.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在数字音频中，最常见的格式是**单声道**和**立体声**。单声道录音仅由一个音频信号组成，在你的耳机两侧听起来完全相同（我们称其为**通道**）。而立体声录音则由两个独立的信号组成，分别完全向左和向右通道分配。
- en: '![](../Images/7781f80d27edaaed33c268d487f7ab48.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7781f80d27edaaed33c268d487f7ab48.png)'
- en: Example of a stereo waveform consisting of two channels. Image by the author.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一个由两个通道组成的立体波形示例。图片由作者提供。
- en: 'Now that we have experienced how stereo sound makes the listening experience
    much more lively and engaging and we also understand the key terminologies, we
    can delve deeper into what we are here for: **The role of AI in mono-to-stereo
    conversion,** also known as **mono-to-stereo upmixing.**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经体验到立体声如何使听觉体验更加生动和引人入胜，同时也理解了关键术语，我们可以更深入地探讨我们所关注的问题：**AI 在单声道到立体声转换中的作用**，也被称为**单声道到立体声升混**。
- en: Use Cases for Mono-to-Stereo Upmixing
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单声道到立体声升混的应用案例
- en: AI is not an end in itself. To justify the development and use of such advanced
    technology, we need **practical use cases**. The two primary use cases for mono-to-stereo
    upmixing are
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: AI 并不是最终目的。为了证明这种先进技术的开发和应用是合理的，我们需要**实际的应用案例**。单声道到立体声升混的两个主要应用案例是：
- en: 1\. Enriching existing music in mono format to a stereo experience.
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 将现有的单声道音乐丰富为立体声体验。
- en: 'Although stereo recording technology was invented in the early 1930s, it took
    until the 1960s for it to become the de-facto standard in recording studios and
    even longer to establish itself in regular households. In the late 50s, new movie
    releases still came with a stereo track and an additional mono track to account
    for theatres that were not ready to transition to stereo systems. In short, there
    are **lots of popular songs that were recorded in mono**. Examples include:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管立体声录音技术在1930年代初期就已经发明，但直到1960年代它才成为录音室的事实标准，并且更长时间才在普通家庭中普及。在1950年代末期，新的电影发行仍然同时配有立体声轨道和额外的单声道轨道，以便为那些尚未过渡到立体声系统的电影院做准备。简而言之，有**许多流行歌曲是以单声道录制的**。以下是一些示例：
- en: 'Elvis Presley: Thats All Right'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 埃尔维斯·普雷斯利：That’s All Right
- en: 'Chuck Berry: Johnny Be Goode'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查克·贝瑞：Johnny Be Goode
- en: 'Duke Ellington: Take the “A” Train'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杜克·埃灵顿：Take the “A” Train
- en: 'The official audio for “Elvis Presley: That’s All Right”, a song published
    in 1954 as a mono recording.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '“Elvis Presley: That’s All Right”的官方音频，这首歌于1954年以单声道录音发布。'
- en: Even today, amateur musicians might publish their recordings in mono, either
    because of a lack of technical competence, or simply because they didn’t want
    to make an effort to create a stereo mix.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 即便在今天，业余音乐人可能也会发布他们的单声道录音，要么是因为缺乏技术能力，要么仅仅是因为他们不想花时间去制作立体声混音。
- en: Mono-to-Stereo conversion lets us experience our favorite old recordings in
    a new light and also bring amateur recordings or demo tracks to live.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 单声道到立体声的转换让我们可以以全新的方式体验我们喜爱的旧录音，还能将业余录音或示范曲目带入生命。
- en: 2\. Improving or modernizing existing stereo mixes that appear sloppy or simply
    have fallen out of time, stylistically
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2. 改进或现代化现有的立体声混音，特别是那些显得杂乱无章或已经过时的混音。
- en: Even when a stereo recording is available, we might still want to **improve**
    it. For example, many older recordings from the 60s and 70s were recorded in stereo,
    but with each instrument panned 100% to one side. Listen to “Soul Kitchen” by
    The Doors and notice how the bass and drums are panned fully to the left, the
    keys and guitar to the right, and the vocals in the centre. The song is great
    and there is a special aesthetic to it, but the stereo mix would likely not get
    much love from a modern audience.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 即使有立体声录音，我们仍然可能想要**改善**它。例如，许多60年代和70年代的老录音是立体声录制的，但每个乐器完全偏向一侧。听一下The Doors的《Soul
    Kitchen》，注意到低音和鼓完全偏向左边，键盘和吉他偏向右边，主唱在中央。歌曲很棒，并且有独特的美学，但这个立体声混音可能不太符合现代听众的喜好。
- en: Technical limitations have affected stereo sound in the past. Further, stereo
    mixing is not purely a craft, it is **part of the artwork**. Stereo mixes can
    be objectively okay, but still fall out of time, stylistically. A stereo conversion
    tool could be used to create an alternate stereo version that aligns more closely
    with certain stylistic preferences.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 技术限制曾经影响了立体声的表现。此外，立体声混音不仅仅是一种工艺，它是**艺术创作的一部分**。立体声混音可以在客观上是可以接受的，但在风格上仍然可能不合时宜。一种立体声转换工具可以用来创建一个更符合特定风格偏好的替代立体声版本。
- en: How Mono-to-Stereo AI Works
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单声道到立体声AI如何工作
- en: Now that we discussed how relevant mono-to-stereo technology is, you might be
    wondering **how it works under the hood**. Turns out there are different approaches
    to tackling this problem with AI. In the following, I want to showcase four different
    methods, ranging **from traditional signal processing to generative AI**. It does
    not serve as a complete list of methods, but rather as an inspiration for how
    this task has been solved over the last 20 years.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们讨论了单声道到立体声技术的重要性，你可能会想知道**它在幕后是如何工作的**。事实证明，有多种方法可以使用AI来解决这个问题。接下来，我将展示四种不同的方法，**从传统信号处理到生成AI**。这并不是一个完整的方法列表，而是过去20年中如何解决这个任务的一些启示。
- en: 'Traditional Signal Processing: Sound Source Formation'
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 传统信号处理：声音源形成
- en: Before machine learning became as popular as it is today, the field of **Music
    Information Retrieval (MIR)** was dominated by smart, hand-crafted algorithms.
    It is no wonder that such approaches also exist for mono-to-stereo upmixing.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习像今天这样流行之前，**音乐信息检索（MIR）**领域曾由智能、手工制作的算法主导。难怪这种方法在单声道到立体声的上混音中也有所应用。
- en: 'The fundamental idea behind a paper from 2007 (Lagrange, Martins, Tzanetakis,
    **[1])** is simple:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 2007年一篇论文（Lagrange, Martins, Tzanetakis, **[1]）**的基本思想很简单：
- en: If we can find the different sound sources of a recording and extract them from
    the signal, we can mix them back together for a realistic stereo experience.
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果我们能够找到录音中的不同声音源并从信号中提取它们，我们就可以将它们重新混合在一起，以实现逼真的立体声体验。
- en: 'This **sounds simple**, but how can we tell what the sound sources in the signal
    are? How do we define them so clearly that an algorithm can extract them from
    the signal? These questions are difficult to solve and the paper uses a variety
    of advanced methods to achieve this. In essence, this is the algorithm they came
    up with:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这**听起来很简单**，但我们如何分辨信号中的声音源呢？我们如何清晰地定义它们，使得算法能够从信号中提取它们？这些问题非常难以解决，这篇论文使用了多种先进的方法来实现这一点。归根结底，这就是他们提出的算法：
- en: Break the recording into short snippets and **identify the peak frequencies**
    (dominant notes) in each snippet
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将录音分成短片段，并**识别每个片段中的峰值频率**（主导音符）
- en: '**Identify which peaks belong together** (a sound source) using a clustering
    algorithm'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**识别哪些峰值属于同一组**（即一个声音源），使用聚类算法'
- en: Decide **where** each sound source should be **placed in the stereo mix** (manual
    step)
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 决定**每个声音源应该在立体声混音中**的**位置**（手动步骤）
- en: For each sound source, **extract its assigned frequencies** from the signal
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个声音源，**提取其分配的频率**从信号中
- en: '**Mix all extracted sources together** to form the final stereo mix.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**将所有提取的源混合在一起**，形成最终的立体声混音。'
- en: '![](../Images/1b935e601d8768cfb3f3e1286d15d0ac.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b935e601d8768cfb3f3e1286d15d0ac.png)'
- en: Example of the user interface built for the study. The user goes through all
    the extracted sources and manually places them in the stereo mix, before resynthesizing
    the whole signal. Image taken from **[1]**.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这是为研究构建的用户界面示例。用户浏览所有提取的源，并手动将它们放入立体声混音中，然后重新合成整个信号。图片来源于**[1]**。
- en: 'Although quite complex in the details, the intuition is quite clear: **Find
    sources, extract them, mix them back together.**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管细节上相当复杂，但直觉是非常清晰的：**找到来源，提取它们，再将它们混合在一起。**
- en: 'A Quick Workaround: Source Separation / Stem Splitting'
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 快速解决方案：源分离 / 音轨分离
- en: 'A lot has happened since Lagrange’s 2007 paper. Since Deezer released their
    stem splitting tool [Spleeter](https://github.com/deezer/spleeter) in 2019, AI-based
    source separation systems have become remarkably useful. Leading players such
    as [Lalal.ai](https://www.lalal.ai/) or [Audioshake](https://www.audioshake.ai/instrument-stem-separation)
    make a quick workaround possible:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 自从拉格朗日2007年的论文以来，发生了很多变化。自2019年Deezer发布了他们的音轨分离工具[Spleeter](https://github.com/deezer/spleeter)以来，基于AI的源分离系统变得非常实用。领先的玩家如[Lalal.ai](https://www.lalal.ai/)或[Audioshake](https://www.audioshake.ai/instrument-stem-separation)使得快速解决方案成为可能：
- en: Separate a mono recording into its individual instrument stems using a free
    or commercial stem splitter
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用免费的或商业的音轨分离工具将单声道录音分离为各个乐器音轨
- en: Load the stems into a Digital Audio Workstation (DAW) and mix them together
    to your liking
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将音轨加载到数字音频工作站（DAW）中，根据自己的喜好将它们混合在一起
- en: This technique has been used in a research paper in 2011 (see **[2]**), but
    it has become much more viable since due to the **recent improvements in stem
    separation tools**.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这一技术在2011年的一篇研究论文中已有应用（见**[2]**），但由于**最近在音轨分离工具的改进**，它变得更加可行。
- en: The downside of source separation approaches is that they produce **noticeable
    sound artifacts**, because source separation itself is still not without flaws.
    Additionally, these approaches still **require manual mixing** by humans, making
    them only semi-automatic.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 源分离方法的缺点是它们会产生**明显的声音伪影**，因为源分离本身仍然不是没有缺陷的。此外，这些方法仍然**需要人工混音**，使得它们只是半自动化的。
- en: To fully automate mono-to-stereo upmixing, machine learning is required. By
    learning from real stereo mixes, ML system can adapt the mixing style of real
    human producers.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现完全自动化的单声道到立体声升混，需要使用机器学习。通过从真实的立体声混音中学习，机器学习系统可以适应真实人类制作人的混音风格。
- en: Machine Learning with Parametric Stereo
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于参数化立体声的机器学习
- en: '![](../Images/2d78894a7635bcd366a210b6c68fe896.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d78894a7635bcd366a210b6c68fe896.png)'
- en: Photo by [Zarak Khan](https://unsplash.com/@zarakvg?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 摄影：由[Zarak Khan](https://unsplash.com/@zarakvg?utm_source=medium&utm_medium=referral)拍摄，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: One very creative and efficient way of using machine learning for mono-to-stereo
    upmixing was presented at ISMIR 2023 by Serrà and colleagues **[3]**. This work
    is based on a music compression technique called **parametric stereo**. Stereo
    mixes consist of two audio channels, making it hard to integrate in low-bandwidth
    settings such as music streaming, radio broadcasting, or telephone connections.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一种非常创新和高效的利用机器学习进行单声道到立体声升混的方法，由Serrà及其同事在2023年ISMIR会议上提出**[3]**。这项工作基于一种名为**参数化立体声**的音乐压缩技术。立体声混音由两个音频通道组成，这使得在低带宽环境下（如音乐流媒体、广播或电话连接）集成变得困难。
- en: 'Parametric stereo is a technique to create stereo sound from a single mono
    signal by **focusing on the important spatial cues** our brain uses to determine
    where sounds are coming from. These cues are:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 参数化立体声是一种通过**关注我们大脑用来判断声音来源的空间线索**，将单声道信号转换为立体声的方法。这些线索包括：
- en: '**How loud** a sound is in the left ear vs. the right ear (Interchannel Intensity
    Difference, IID)'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**左右耳之间的声音强度差异**（声道间强度差，IID）'
- en: '**How in sync** it is between left and right in terms of time or phase (Interchannel
    Time or Phase Difference)'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**它在时间或相位上左右声道的同步程度**（声道间时间或相位差）'
- en: '**How similar or different** the signals are in each ear (Interchannel Correlation,
    IC)'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**每只耳朵中信号的相似性或差异性**（声道间相关性，IC）'
- en: Using these parameters, a stereo-like experience can be created from nothing
    more than a mono signal.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这些参数，可以仅通过单声道信号创建类似立体声的体验。
- en: 'This is the approach the researchers took to develop their mono-to-stereo upmixing
    model:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这是研究人员开发他们的单声道到立体声升混模型时采用的方法：
- en: '**Collect a large dataset** of stereo music tracks'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**收集大量的立体声音乐轨道数据集**'
- en: '**Convert the stereo tracks** to parametric stereo (mono + spatial parameters)'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**将立体声轨道**转换为参数化立体声（单声道 + 空间参数）'
- en: '**Train a neural network** to predict the spatial parameters given a mono recording'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练一个神经网络**，根据单声道录音预测空间参数'
- en: To turn a new mono signal into stereo, use the trained model to **infer spatial
    parameters from the mono signal** and combine the two to a parametric stereo experience
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了将一个新的单声道信号转换为立体声，可以使用训练好的模型从单声道信号中**推断空间参数**，然后将这两个参数结合成一个参数化的立体声体验。
- en: Currently, no code or listening demos seem to be available for this paper. The
    authors themselves confess that “there is still a gap between professional stereo
    mixes and the proposed approaches” (p. 6). Still, the paper outlines a creative
    and efficient way to accomplish fully automated mono-to-stereo upmixing using
    machine learning.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，似乎没有任何代码或试听演示可以用于这篇论文。作者们自己也承认，“专业立体声混音与提出的方法之间仍然存在差距”（第6页）。尽管如此，论文概述了一种创造性且高效的方式，使用机器学习完成完全自动化的单声道到立体声的上混合。
- en: 'Generative AI: Transformer-based Synthesis'
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成式AI：基于Transformer的合成
- en: '![](../Images/5481f91c143d41712337a3c4edd0ad9f.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5481f91c143d41712337a3c4edd0ad9f.png)'
- en: Stereo-Genration in Meta’s text-to-music model MusicGen. Image taken from [another
    article by the author](https://medium.com/towards-data-science/musicgen-reimagined-metas-under-the-radar-advances-in-ai-music-36c1adfd13b7).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Meta的文本到音乐模型MusicGen中的立体声生成。图片来自作者的[另一篇文章](https://medium.com/towards-data-science/musicgen-reimagined-metas-under-the-radar-advances-in-ai-music-36c1adfd13b7)。
- en: Now, we will get to the seemingly most straight-forward way to generate stereo
    from mono. Training a generative model to take a mono input and synthesizing both
    stereo output channels directly. Although conceptually simple, this is by far
    the most challenging approach from a technical standpoint. One second of high-resolution
    audio has 44.1k data points. Generating a three-minute song with stereo channels
    therefore means **generating over 15 million data points**.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将讨论看似最直接的单声道生成立体声的方法。训练一个生成模型，接受单声道输入，并直接合成两个立体声输出通道。尽管概念上简单，从技术角度来看，这是迄今为止最具挑战性的方法。1秒钟的高分辨率音频有44.1k个数据点。因此，生成一首三分钟的立体声音频意味着**生成超过1500万个数据点**。
- en: With todays technologies such as convolutional neural networks, transformers,
    and neural audio codecs, the complexity of the task is starting to become managable.
    There are some papers who chose to generate stereo signal through direct neural
    synthesis (see **[4]**, **[5]**, **[6]**). However, only **[5]** train a model
    than can solve mono to stereo generation out of the box. My intuition is that
    there is room for a paper that builds a dedicated for the “simple” task of mono-to-stereo
    generation and focuses 100% on solving this objective. Anyone here looking for
    a **PhD topic**?
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，借助卷积神经网络、变换器和神经音频编解码器等技术，任务的复杂性开始变得可控。有些论文选择通过直接的神经合成生成立体声信号（参见**[4]**、**[5]**、**[6]**）。然而，只有**[5]**训练了一个可以直接解决单声道到立体声生成的模型。我的直觉是，应该有一篇论文专门解决“简单”的单声道到立体声生成任务，并专注于100%解决这个目标。有没有人对这个**博士课题**感兴趣？
- en: What Needs to Happen Next?
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接下来需要发生什么？
- en: '![](../Images/16a84d46db8a66f820a2b701322f4150.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/16a84d46db8a66f820a2b701322f4150.png)'
- en: Photo by [Samuel Spagl](https://unsplash.com/@saemsp?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 由[Samuel Spagl](https://unsplash.com/@saemsp?utm_source=medium&utm_medium=referral)拍摄，图片来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'To conclude this article, I want to discuss where the field of mono-to-stereo
    upmixing might be going. Most importantly, I noticed that **research in this domain
    is very sparse**, compared to hype topics such as text-to-music generation. Here’s
    what I think the research community should focus on to bring mono-to-stereo upmixing
    research to the next level:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在总结这篇文章时，我想讨论单声道到立体声上混合的研究领域可能的发展方向。最重要的是，我注意到**这一领域的研究非常稀少**，相比之下，像文本到音乐生成这样的热门话题则备受关注。我认为，研究社区应该集中精力，将单声道到立体声上混合的研究提升到一个新的水平：
- en: 1\. Openly Available Demos and Code
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 开放可用的演示和代码
- en: Only few papers are released in this research field. This makes it even more
    frustrating that **many of them do not share their code or the results of their
    work** with the community. Several times have I read through a fascinating paper
    only to find that the only way to test the output quality of the method is to
    understand every single formula in the paper and implement the algorithm myself
    from scratch.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 目前该研究领域发布的论文不多。这使得**许多论文没有与社区分享代码或工作成果**变得更加令人沮丧。我曾多次阅读一篇引人入胜的论文，却发现唯一能够测试该方法输出质量的方式，是完全理解论文中的每一个公式，并从头开始实现算法。
- en: Sharing code and creating public demos has never been as easy as it is today.
    Researchers should make this a priority to enable the wider audio community to
    understand, evaluate, and appreciate their work.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 分享代码和创建公共演示从未像今天这样容易。研究人员应该将此作为优先事项，以使更广泛的音频社区能够理解、评估并欣赏他们的工作。
- en: 2\. Going All-In on Generative AI
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 全力投入生成式AI
- en: Traditional signal processing and machine learning are fun, but when it comes
    to output quality, there is no way around generative AI anymore. **Text-to-music
    models are already producing great-sounding stereo mixes**. Why is there no easy
    to use, state-of-the-art mono-to-stereo upmixing library available?
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 传统信号处理和机器学习很有趣，但当谈到输出质量时，再也无法绕过生成式AI了。**文本到音乐的模型已经能生成听起来很棒的立体声混音**。为什么没有易于使用的、最先进的单声道到立体声升混库呢？
- en: From what I gathered in my research, building an efficient and effective model
    can be done with a reasonable dataset size and minimal to moderate changes to
    existing model architectures and training methods. My impression is that this
    is a **low-hanging fruit** and a “just do it!” situation.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 从我的研究中得知，构建一个高效且有效的模型可以通过合理的数据集大小以及对现有模型架构和训练方法进行最小至适度的修改来实现。我的印象是，这是一项**触手可及的成果**，是一个“就做吧！”的情况。
- en: 3\. Making Upmixing Automated, but Controllable
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 使升混自动化，但可控
- en: Once we have a great open-source upmixing model, the next thing we need is controllability.
    We shouldn’t have to pick between black-box “take-it-or-leave-it” neural generations
    or old-school, manual mixing based on source separation. I think we could have
    it both.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们拥有了一个出色的开源升混模型，接下来我们需要的是可控性。我们不应该在黑盒的“要么接受，要么放弃”的神经网络生成与基于源分离的老派手动混音之间做选择。我认为我们可以兼得这两者。
- en: A neural mono-to-stereo upmixing model could be **trained on a massive dataset
    and then finetuned** to adjust its stereo mixes based on a user prompt. This way,
    musicians could customize the style of the generated stereo based on their personal
    preferences.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一个神经网络单声道到立体声升混模型可以**在一个庞大的数据集上进行训练，然后进行微调**，以根据用户提示调整其立体声混音。这样，音乐家们可以根据个人偏好定制生成立体声的风格。
- en: Conclusion
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Effective and openly-accessible mono-to-stereo upmixing has the potential to
    breathe live into old recordings or amateur productions, while also allowing us
    to create alternate stereo mixes of our favorite songs.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 有效且公开可访问的单声道到立体声的升混技术具有将旧录音或业余制作焕发生机的潜力，同时也让我们能够创造我们最喜爱的歌曲的替代立体声混音。
- en: Although there have been several attempts to solve this problem, no standard
    method has been established. By embracing recent development in GenAI, a new generation
    of mono-to-stereo upmixing models could be created that makes the technology more
    effective and more widely available in the community.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管已经有若干次尝试来解决这个问题，但尚未建立标准方法。通过拥抱最近的生成式AI发展，可以创建一代新的单声道到立体声升混模型，使得技术变得更加有效且在社区中更广泛可用。
- en: About Me
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于我
- en: 'I’m a musicologist and a data scientist, sharing my thoughts on current topics
    in AI & music. Here is some of my previous work related to this article:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我是一名音乐学家和数据科学家，分享我对AI和音乐当前话题的思考。以下是我与本文相关的一些前期工作：
- en: '[**Images that Sound: Creating Stunning Audiovisual ART with AI**](/3-music-ai-breakthroughs-to-expect-in-2024-2d945ae6b5fd)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**声音图像：用AI创造惊艳的视听艺术**](/3-music-ai-breakthroughs-to-expect-in-2024-2d945ae6b5fd)'
- en: '[**How Meta’s AI Generates Music Based on a Reference Melody**](https://medium.com/towards-data-science/how-ai-can-remove-imperceptible-watermarks-6b4560ea867a)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Meta 如何基于参考旋律生成音乐**](https://medium.com/towards-data-science/how-ai-can-remove-imperceptible-watermarks-6b4560ea867a)'
- en: '[**AI Music Source Separation: How it Works and Why it is so Hard**](https://medium.com/towards-data-science/ai-music-source-separation-how-it-works-and-why-it-is-so-hard-187852e54752)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**AI 音乐源分离：它是如何工作的，为什么如此困难**](https://medium.com/towards-data-science/ai-music-source-separation-how-it-works-and-why-it-is-so-hard-187852e54752)'
- en: Find me on [Medium](https://medium.com/@maxhilsdorf) and [Linkedin](https://www.linkedin.com/in/max-hilsdorf/)!
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在 [Medium](https://medium.com/@maxhilsdorf) 和 [Linkedin](https://www.linkedin.com/in/max-hilsdorf/)
    上找到我！
- en: References
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '**[1] M. Lagrange, L. G. Martins, and G. Tzanetakis (2007)**: “Semiautomatic
    mono to stereo up-mixing using sound source formation”, in Audio Engineering Society
    Convention 122\. Audio Engineering Society, 2007.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**[1] M. Lagrange, L. G. Martins 和 G. Tzanetakis (2007)**：“使用声音源形成的半自动单声道到立体声升混”，见《音频工程学会会议》第122届。音频工程学会，2007年。'
- en: '**[2] D. Fitzgerald (2011)**: “Upmixing from mono-a source separation approach”,
    in 2011 17th International Conference on Digital Signal Processing (DSP). IEEE,
    2011, pp. 1–7.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**[2] D. Fitzgerald (2011):** “从单声道到源分离的方法”，发表于2011年第17届数字信号处理国际会议（DSP）。IEEE，2011年，第1-7页。'
- en: '**[3] J. Serrà, D. Scaini, S. Pascual, et al. (2023):** “Mono-to-stereo through
    parametric stereo generation”: [https://arxiv.org/abs/2306.14647](https://arxiv.org/abs/2306.14647)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**[3] J. Serrà, D. Scaini, S. Pascual, 等人 (2023):** “通过参数化立体声生成从单声道到立体声的转换”：
    [https://arxiv.org/abs/2306.14647](https://arxiv.org/abs/2306.14647)'
- en: '**[4] J. Copet, F. Kreuk, I. Gat et al. (2023)**: “Simple and Controllable
    Music Generation” (revision from 30.01.2024). [https://arxiv.org/abs/2306.05284](https://arxiv.org/abs/2306.05284)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**[4] J. Copet, F. Kreuk, I. Gat 等人 (2023):** “简单且可控的音乐生成”（2024年1月30日修订版）。
    [https://arxiv.org/abs/2306.05284](https://arxiv.org/abs/2306.05284)'
- en: '**[5] Y. Zang, Y. Wang & M. Lee (2024):** “Ambisonizer: Neural Upmixing as
    Spherical Harmonics Generation”. [https://arxiv.org/pdf/2405.13428](https://arxiv.org/pdf/2405.13428)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**[5] Y. Zang, Y. Wang & M. Lee (2024):** “Ambisonizer: 作为球面调和函数生成的神经网络升混合”。
    [https://arxiv.org/pdf/2405.13428](https://arxiv.org/pdf/2405.13428)'
- en: '**[6] K.K. Parida, S. Srivastava & G. Sharma (2022)**: “Beyond Mono to Binaural:
    Generating Binaural Audio from Mono Audio with Depth and Cross Modal Attention”,
    in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision
    (WACV), 2022, p. 3347–3356\. [Link](https://openaccess.thecvf.com/content/WACV2022/html/Parida_Beyond_Mono_to_Binaural_Generating_Binaural_Audio_From_Mono_Audio_WACV_2022_paper.html)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**[6] K.K. Parida, S. Srivastava & G. Sharma (2022):** “超越单声道到双耳声道：利用深度和跨模态注意力从单声道音频生成双耳音频”，发表于IEEE/CVF冬季计算机视觉应用会议（WACV），2022年，第3347–3356页。
    [Link](https://openaccess.thecvf.com/content/WACV2022/html/Parida_Beyond_Mono_to_Binaural_Generating_Binaural_Audio_From_Mono_Audio_WACV_2022_paper.html)'
