- en: 'Uncertainty in Markov Decisions Processes: a Robust Linear Programming approach'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程中的不确定性：一种鲁棒的线性规划方法
- en: 原文：[https://towardsdatascience.com/uncertainty-in-markov-decisions-processes-a-robust-linear-programming-approach-b01e6e26e463?source=collection_archive---------2-----------------------#2024-09-18](https://towardsdatascience.com/uncertainty-in-markov-decisions-processes-a-robust-linear-programming-approach-b01e6e26e463?source=collection_archive---------2-----------------------#2024-09-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/uncertainty-in-markov-decisions-processes-a-robust-linear-programming-approach-b01e6e26e463?source=collection_archive---------2-----------------------#2024-09-18](https://towardsdatascience.com/uncertainty-in-markov-decisions-processes-a-robust-linear-programming-approach-b01e6e26e463?source=collection_archive---------2-----------------------#2024-09-18)
- en: Theoretical derivation of the Robust Counterpart of Markov Decision Processes
    (MDPs) as a Linear Program (LP)
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程（MDP）的鲁棒对偶理论推导：作为一个线性规划（LP）
- en: '[](https://medium.com/@h.fellahi?source=post_page---byline--b01e6e26e463--------------------------------)[![Hussein
    Fellahi](../Images/b49c8620d8a490ab078b5d4dfe8d017a.png)](https://medium.com/@h.fellahi?source=post_page---byline--b01e6e26e463--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b01e6e26e463--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b01e6e26e463--------------------------------)
    [Hussein Fellahi](https://medium.com/@h.fellahi?source=post_page---byline--b01e6e26e463--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@h.fellahi?source=post_page---byline--b01e6e26e463--------------------------------)[![Hussein
    Fellahi](../Images/b49c8620d8a490ab078b5d4dfe8d017a.png)](https://medium.com/@h.fellahi?source=post_page---byline--b01e6e26e463--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b01e6e26e463--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b01e6e26e463--------------------------------)
    [Hussein Fellahi](https://medium.com/@h.fellahi?source=post_page---byline--b01e6e26e463--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b01e6e26e463--------------------------------)
    ·8 min read·Sep 18, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b01e6e26e463--------------------------------)
    ·8分钟阅读·2024年9月18日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/5ca68bd8a46abe36f974ccfa485f67ad.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5ca68bd8a46abe36f974ccfa485f67ad.png)'
- en: Photo by [ZHENYU LUO](https://unsplash.com/@mrnuclear?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [ZHENYU LUO](https://unsplash.com/@mrnuclear?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: Markov Decision Processes are foundational to sequential decision-making problems
    and serve as the building block for reinforcement learning. They model the dynamic
    interaction between an agent having to make a series of actions and their environment.
    Due to their wide applicability in fields such as robotics, finance, operations
    research and AI, MDPs have been extensively studied in both theoretical and practical
    contexts.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程（MDP）是顺序决策问题的基础，是强化学习的构建模块。它们建模了一个代理在必须执行一系列动作与其环境之间的动态互动。由于其在机器人技术、金融、运筹学和人工智能等领域的广泛应用，MDP已在理论和实践中得到了广泛研究。
- en: 'Yet, much of the existing MDP literature focuses on the idealized scenarios
    where model parameters — such as transition probabilities and reward functions
    — are assumed to be known with certainty. In practice, applying popular methods
    such as Policy Iteration and Value Iteration require precise estimates of these
    parameters, often obtained from real-world data. This reliance on data introduces
    significant challenges: the estimation process is inherently noisy and sensitive
    to limitations such as data scarcity, measurement errors and variability in the
    observed environment. Consequently, the performance of standard MDP methods can
    degrade substantially when applied to problems with uncertain or incomplete data.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，现有的大部分MDP文献集中于理想化的情境，其中模型参数——如转移概率和奖励函数——假设已知且确定。在实践中，应用如策略迭代和价值迭代等常见方法需要对这些参数进行精确估计，这些估计通常来自于真实世界的数据。对数据的依赖带来了显著的挑战：估计过程本质上是嘈杂的，并且对数据稀缺、测量误差以及观察到的环境变异性等限制因素敏感。因此，当将标准的MDP方法应用于具有不确定性或不完整数据的问题时，其性能可能会显著下降。
- en: In this article, we build on the Robust Optimization (RO) literature to propose
    a generic framework to address these issues. **We provide a Robust Linear Programming
    (RLP) formulation of MDPs that is capable of handling various sources of uncertainty
    and adversarial perturbations.**
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们基于鲁棒优化（RO）文献提出了一个通用框架来解决这些问题。**我们提供了一个鲁棒线性规划（RLP）公式化的 MDP，能够处理各种不确定性和对抗性扰动。**
- en: MDP definition and LP formulation
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MDP 定义和线性规划（LP）公式化
- en: 'Let’s start by giving a formal definition of MDPs:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从给出 MDP 的正式定义开始：
- en: '*A* ***Markov Decision Process*** *is a 5-tuple (S, A, R, P, γ) such that:*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*A* ***马尔可夫决策过程*** *是一个五元组 (S, A, R, P, γ)，满足：*'
- en: '*S is the set of* ***states*** *the agent can be in*'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*S 是代理可以处于的* ***状态*** *的集合*'
- en: '*A is the set of* ***actions*** *the agent can take*'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*A 是代理可以执行的* ***动作*** *的集合*'
- en: '*R : S* x *A →* R *the* ***reward*** *function*'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*R : S* x *A →* R *是* ***奖励*** *函数*'
- en: '*P is the set of* ***probability distributions*** *defined such that P(s’|s,a)
    is the probability of transitioning to state* ***s’*** *if the agent takes action*
    ***a*** *in state* ***s****. Note that MDPs are Markov processes, meaning that
    the Markov property holds on the transition probabilities*: P(Sₜ₊₁|S₀, A₀, …,
    Sₜ, Aₜ) = P(Sₜ₊₁|Sₜ, Aₜ)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P 是* ***概率分布*** *的集合，定义为 P(s’|s,a) 是在状态* ***s*** *下，代理执行动作* ***a*** *时转移到状态*
    ***s’*** *的概率。请注意，MDP 是马尔可夫过程，这意味着马尔可夫性质在转移概率中成立：P(Sₜ₊₁|S₀, A₀, …, Sₜ, Aₜ) = P(Sₜ₊₁|Sₜ,
    Aₜ)*'
- en: '*γ ∈ (0, 1] is a* ***discount factor****. While we usually deal with discounted
    problems (i.e. γ < 1), the formulations presented are also valid for undiscounted
    MDPs (γ = 1)*'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*γ ∈ (0, 1] 是* ***折扣因子***。虽然我们通常处理折现问题（即 γ < 1），但所呈现的公式对无折扣的 MDP（γ = 1）同样有效*'
- en: 'We then define the **policy**, i.e. what dictates the agent’s behavior in an
    MDP:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义 **策略**，即决定代理行为的规则：
- en: '*A policy π is a* ***probability measure*** *over the action space defined
    as: π(a|s) is the probability of taking action* ***a*** *when the agent is in
    state* ***s****.*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*策略 π 是一个* ***概率测度*** *，定义为：π(a|s) 是在代理处于状态* ***s*** *时采取动作* ***a*** *的概率。*'
- en: 'We finally introduce the **value function**, i.e. the agent’s objective in
    an MDP:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终引入 **价值函数**，即 MDP 中代理的目标：
- en: '*The value function of a policy π is the expected discounted reward under this
    policy, when starting at a given state* ***s****:*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*策略 π 的价值函数是该策略下期望的折现奖励，当从给定状态* ***s*** *开始时：*'
- en: '![](../Images/6d82dc5a1a2e1a0b0d5228793fd75781.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d82dc5a1a2e1a0b0d5228793fd75781.png)'
- en: '*In particular, the value function of the optimal policy π* satisfies the Bellman
    optimality equation:*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*特别地，最优策略 π* 的价值函数满足贝尔曼最优性方程：*'
- en: '![](../Images/7daa90e2dd4bf9c84f0d01a1d142ddc2.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7daa90e2dd4bf9c84f0d01a1d142ddc2.png)'
- en: '*Which yields the deterministic optimal policy:*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*这产生了确定性的最优策略：*'
- en: '![](../Images/25de5914d7227557fccc869698b8a51c.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/25de5914d7227557fccc869698b8a51c.png)'
- en: 'Deriving the LP formulation of MDPs:'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推导 MDP 的线性规划（LP）公式：
- en: Given the above definitions, we can start by noticing that any value function
    V that satisfies
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述定义，我们可以从以下事实入手：任何满足的价值函数 V
- en: '![](../Images/853b1072047df6981994647e5fdb183f.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/853b1072047df6981994647e5fdb183f.png)'
- en: 'is an upper bound on the optimal value function. To see it, we can start by
    noticing that such value function also satisfies:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 是最优价值函数的上界。为了理解这一点，我们可以首先注意到这样的价值函数也满足：
- en: '![](../Images/ead9064be99992fb736efcd2fb7162c9.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ead9064be99992fb736efcd2fb7162c9.png)'
- en: 'We recognize the value iteration operator applied to V:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们识别出应用于 V 的价值迭代运算符：
- en: '![](../Images/addbe4760a5cbd1d16e641580e000de3.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/addbe4760a5cbd1d16e641580e000de3.png)'
- en: i.e.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 即
- en: '![](../Images/529b5be7c352fdaac409965f0a558859.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/529b5be7c352fdaac409965f0a558859.png)'
- en: 'Also noticing that the H*operator is increasing, we can apply it iteratively
    to have:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，注意到 H* 运算符是递增的，我们可以迭代地应用它得到：
- en: '![](../Images/1eb2f85b3fef8f0a3946982532d2528c.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1eb2f85b3fef8f0a3946982532d2528c.png)'
- en: where we used the property of V* being the fixed point of H*.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了 V* 是 H* 的不动点这一性质。
- en: 'Therefore, finding V* comes down to finding the **tightest upper bound V that
    obeys the above equation**, which yields the following formulation:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，找到 V* 的问题归结为找到 **遵守上述方程的最紧上界 V**，从而得到以下公式：
- en: '![](../Images/dd511b37256a48510693d9294a7427cb.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd511b37256a48510693d9294a7427cb.png)'
- en: 'Here we added a weight term corresponding to the probability of starting in
    state *s*. We can see that the above problem is linear in V and can be rewritten
    as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们添加了一个与状态 *s* 的起始概率相关的权重项。我们可以看到，上述问题在线性形式下表示为 V，并且可以重写如下：
- en: '![](../Images/a95c4fe11f870f446a9cf2a04b8d975e.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a95c4fe11f870f446a9cf2a04b8d975e.png)'
- en: Further details can be found in [1] and [2].
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步的细节可以在 [1] 和 [2] 中找到。
- en: Robust Optimization for Linear Programming
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性规划的鲁棒优化
- en: 'Given the above linear program in standard form, the RO framework assumes an
    adversarial noise in the inputs (i.e. cost vector and constraints). To model this
    uncertainty, we define an **uncertainty set**:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 给定上述标准形式的线性规划，RO 框架假设输入（即成本向量和约束）中存在对抗噪声。为了建模这种不确定性，我们定义了一个**不确定性集**：
- en: '![](../Images/8632027096c26bd218ef09dce04263fd.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8632027096c26bd218ef09dce04263fd.png)'
- en: In short, we want to find the minimum of all linear programs, i.e. for each
    occurrence in the uncertainty set. Naturally this yields a completely intractable
    model (potentially an infinite number of LPs) since we did not make any assumption
    on the form of *U*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们想要找到所有线性规划的最小值，即对于不确定性集中的每一次出现。显然，这会导致一个完全无法处理的模型（可能是无穷多个线性规划），因为我们对
    *U* 的形式没有做任何假设。
- en: 'Before addressing these issues, we make the following assumptions — without
    loss of generality:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决这些问题之前，我们做出以下假设——不失一般性：
- en: Uncertainty in *w* and *b* is equivalent to uncertainty in the constraints for
    a slightly modified LP — for this reason we consider uncertainty only in *c*
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*w* 和 *b* 的不确定性等同于一个稍微修改过的线性规划中的约束不确定性——因此我们只考虑 *c* 的不确定性'
- en: Adversarial noise is applied constraint-wise, i.e. to each constraint individually
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对抗噪声按约束逐个应用，即对每个约束单独应用
- en: 'The constraint of the robust problem are in the form:'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鲁棒问题的约束形式为：
- en: '![](../Images/c3789208dc20154982543af2d706aa66.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3789208dc20154982543af2d706aa66.png)'
- en: 'where: \bar{c} is known as the *nominal constraint vector* (e.g. gotten from
    some estimation), *z* the uncertain factor and Q a fixed matrix intuitively corresponding
    to how the noise is applied to each coefficient of the constraint vector. *Q*
    can be used for instance to model correlation between the noise on difference
    components of *c*. See [3] for more details and proofs.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：\bar{c} 被称为*标称约束向量*（例如，通过某些估计得到），*z* 是不确定因素，*Q* 是一个固定的矩阵，直观上对应于噪声如何作用于约束向量的每个系数。*Q*
    可以用来建模 *c* 的不同分量之间噪声的相关性。例如，更多细节和证明可以参见 [3]。
- en: 'Note: we made a slight abuse of notation and dropped the *(s, a)* subscripts
    for readability — yet *c, \bar{c}, Q* and *z* are all for a given state and action
    couple.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们略微滥用了符号，并省略了 *(s, a)* 下标以提高可读性——然而 *c, \bar{c}, Q* 和 *z* 都是对于给定状态和动作对的。
- en: Rather than optimizing for each entry of the uncertainty set, **we optimize
    the worst case over *U*.** In the context of uncertainty on the constraints only,
    this mean **that the worst case over *U* must also be feasible**
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不再对不确定性集中的每个条目进行优化，**我们优化 *U* 上的最坏情况**。在只有约束的不确定性背景下，这意味着**最坏情况的 *U* 必须是可行的**
- en: 'All this leads to the following formulation of the problem:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些导致了以下问题的公式化：
- en: '![](../Images/a3909cb7130218128c2279000361bd40.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a3909cb7130218128c2279000361bd40.png)'
- en: 'At this stage, we can make some assumptions on the form of *U* in order to
    further simplify the problem:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一阶段，我们可以对 *U* 的形式做出一些假设，以便进一步简化问题：
- en: While *z* can be a vector of arbitrary dimension *L* — as *Q* will be a *|S|*
    x *L* matrix — we make the simplifying assumption that *z* is of size *|S|* and
    *Q* is a square diagonal matrix of size *|S|* as well. This will allow to model
    separately an adversarial noise on each coefficient on the constraint vector (and
    no correlation between noises)
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然 *z* 可以是任意维度 *L* 的向量——因为 *Q* 将是一个 *|S|* x *L* 的矩阵——我们做出了一个简化假设，即 *z* 的大小为
    *|S|*，并且 *Q* 是一个大小为 *|S|* 的方形对角矩阵。这将使我们能够分别对约束向量的每个系数建模对抗噪声（且噪声之间无相关性）
- en: We assume that the uncertainty set is a box of size *2d,* i.e. that each coordinate
    of *z* can take any value from the interval *[-d, d]*. This is equivalent to saying
    that the *L∞* norm of *z* is less than *d*
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们假设不确定性集是一个大小为 *2d* 的盒子，即 *z* 的每个坐标可以取自区间 *[-d, d]* 中的任何值。这等价于说 *z* 的 *L∞*
    范数小于 *d*
- en: 'The optimization problem becomes:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 优化问题变为：
- en: '![](../Images/419cb06e77e1678dee33007b6e287c4e.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/419cb06e77e1678dee33007b6e287c4e.png)'
- en: 'which is equivalent to:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这等价于：
- en: '![](../Images/92e0dc89bd906107cf3daa4c57035b35.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92e0dc89bd906107cf3daa4c57035b35.png)'
- en: 'Finally, looking closer to the maximization problem in the constraint, we see
    that it has a closed form. Therefore the final problem can be written as (**robust
    counterpart of a linear program with box uncertainty**):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，仔细观察约束中的最大化问题，我们发现它具有封闭形式。因此，最终问题可以写成（**带有盒式不确定性的线性规划的鲁棒对偶**）：
- en: '![](../Images/1a024026d6db5e420aae02229ede1a5f.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a024026d6db5e420aae02229ede1a5f.png)'
- en: 'A few comments on the above formulation:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对上述公式的一些评论：
- en: The uncertainty term disappeared — robustness is brought by an additional safety
    term
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不确定性项消失了——鲁棒性通过额外的安全项引入
- en: As the L1 norm can be linearized, this is a linear program
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于L1范数可以线性化，因此这是一个线性规划
- en: The above formulation does not depend on the form of *Q* — the assumption made
    will be useful in the next section
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上述公式不依赖于*Q*的形式——所作的假设将在下一部分中发挥作用
- en: For more details, the interested reader can refer to [3].
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 有兴趣的读者可以参见[3]以了解更多细节。
- en: The RLP formulation of MDPs
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MDP的RLP公式
- en: 'Starting from the above formulation:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述公式出发：
- en: '![](../Images/93c5fcdf66e345016b93c434216c00a8.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93c5fcdf66e345016b93c434216c00a8.png)'
- en: 'And finally, linearizing the absolute value in the constraints gives:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将约束中的绝对值线性化得到：
- en: '![](../Images/0f21ead78a1601595332037d985dfdce.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f21ead78a1601595332037d985dfdce.png)'
- en: We notice that robustness translates into an additional **safety term** in the
    constraints — given the uncertainty on *c* (which mainly translates into uncertainty
    in the MDP’s transition probabilities).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到鲁棒性转化为约束中的额外**安全项**——考虑到*c*的不确定性（这主要转化为MDP的转移概率的不确定性）。
- en: 'As stated previously, considering uncertainty on the rewards as well can be
    easily done with a similar derivation. Coming back to the linear program in standard
    form, we add another noise term on the right hand side of the constraints:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，考虑奖励的不确定性也可以通过类似的推导轻松完成。回到标准形式的线性规划，我们在约束条件的右侧添加了另一个噪声项：
- en: '![](../Images/1170d90147dfdaf5d285ec368f6caad5.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1170d90147dfdaf5d285ec368f6caad5.png)'
- en: 'After a similar reasoning as previously done, we have the all-in linear program:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 经过与之前相似的推理，我们得到完整的线性规划：
- en: '![](../Images/304f023eec33f211753050b9b290e5dc.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/304f023eec33f211753050b9b290e5dc.png)'
- en: Again similarly to before, additional robustness with regards to the reward
    function translates into another safety term in the constraints, which ultimately
    can yield a less optimal value function and policy but fills the constraints with
    margin. This tradeoff is controlled both by *Q* and the size of the uncertainty
    box *d*.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 再次类似于之前，对于奖励函数的额外鲁棒性转化为约束中的另一个安全项，这最终可能导致一个不太最优的值函数和策略，但能填充约束边界。这种权衡由*Q*和不确定性盒子*
    d *的大小共同控制。
- en: Conclusion
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: While this completes the derivation of the Robust MDP as a linear program, other
    robust MDP approaches have been developed in the literature, see for instance
    [4]. These approaches usually take a different route, for instance directly deriving
    a robust Policy Evaluation operator — which for instance has the advantage of
    having a better complexity as the LP approach. This proves particularly important
    when state and action spaces are large. Therefore why would we use such formulation?
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这完成了作为线性规划的鲁棒马尔科夫决策过程（Robust MDP）的推导，但文献中已经提出了其他鲁棒MDP方法，例如参见[4]。这些方法通常采取不同的路径，例如直接推导出鲁棒策略评估算子——这具有与线性规划方法相比更好的复杂度优势。这在状态和动作空间较大时尤其重要。那么我们为什么要使用这样的公式呢？
- en: The RLP formulation allows to benefit from all theoretical properties of linear
    programming. This entails guarantees of a solution (when the problem is feasible
    and bounded), as well as known results from duality theory and sensitivity analysis
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RLP公式使我们能够利用线性规划的所有理论性质。这意味着在问题可行且有界的情况下，保证存在解，并且能够使用对偶性理论和敏感性分析的已知结果。
- en: The LP approach allows to easily use different geometries of uncertainty sets
    — see [3] for details
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性规划方法允许轻松使用不同的不确定性集几何形状——有关详细信息，参见[3]
- en: This formulation allows to integrate naturally additional constraints on the
    MDP, while keeping the robustness properties
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种公式允许自然地集成MDP的附加约束，同时保持鲁棒性特性
- en: We can furthermore apply some projection or approximation methods (see for instance
    [5]) to improve the LP complexity
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还可以应用一些投影或近似方法（例如参见[5]）来改进线性规划复杂度。
- en: 'References:'
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献：
- en: '[1] M.L. Puterman, [Markov Decision Processes: Discrete Stochastic Dynamic
    Programming](https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316887) (1996),
    Wiley'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] M.L. Puterman, [《马尔可夫决策过程：离散随机动态规划》](https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316887)（1996），Wiley'
- en: '[2] P. Pouppart, Sequential Decision Making and Reinforcement Learning (2013),
    University of Waterloo'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] P. Pouppart, 《序列决策与强化学习》（2013），滑铁卢大学'
- en: '[3] D. Bertsimas, D. Den Hertog, [Robust and Adaptive Optimization](https://www.dynamic-ideas.com/books/robust-and-adaptive-optimization)
    (2022), Dynamic Ideas'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] D. Bertsimas, D. Den Hertog, [《稳健与自适应优化》](https://www.dynamic-ideas.com/books/robust-and-adaptive-optimization)（2022），Dynamic
    Ideas'
- en: '[4] W. Wiesemann, D. Kuhn, B. Rustem, [Robust Markov Decision Processes](https://pubsonline.informs.org/doi/abs/10.1287/moor.1120.0566?journalCode=moor)
    (2013), INFORMS'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] W. Wiesemann, D. Kuhn, B. Rustem, [《稳健马尔可夫决策过程》](https://pubsonline.informs.org/doi/abs/10.1287/moor.1120.0566?journalCode=moor)（2013），INFORMS'
- en: '[5] K. Vu, P.-L. Poirion, L. Liberti, [Random Projections for Linear Programming](https://pubsonline.informs.org/doi/abs/10.1287/moor.2017.0894)
    (2018), INFORMS'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] K. Vu, P.-L. Poirion, L. Liberti, [《线性规划的随机投影》](https://pubsonline.informs.org/doi/abs/10.1287/moor.2017.0894)（2018），INFORMS'
