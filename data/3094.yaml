- en: Introducing n-Step Temporal-Difference Methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍n步时间差分方法
- en: 原文：[https://towardsdatascience.com/introducing-n-step-temporal-difference-methods-7f7878b3441c?source=collection_archive---------1-----------------------#2024-12-29](https://towardsdatascience.com/introducing-n-step-temporal-difference-methods-7f7878b3441c?source=collection_archive---------1-----------------------#2024-12-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/introducing-n-step-temporal-difference-methods-7f7878b3441c?source=collection_archive---------1-----------------------#2024-12-29](https://towardsdatascience.com/introducing-n-step-temporal-difference-methods-7f7878b3441c?source=collection_archive---------1-----------------------#2024-12-29)
- en: Dissecting “Reinforcement Learning” by Richard S. Sutton with custom Python
    implementations, Episode V
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 剖析理查德·S·萨顿的《强化学习》，附带定制的Python实现，第五集
- en: '[](https://medium.com/@hrmnmichaels?source=post_page---byline--7f7878b3441c--------------------------------)[![Oliver
    S](../Images/b5ee0fa2d5fb115f62e2e9dfcb92afdd.png)](https://medium.com/@hrmnmichaels?source=post_page---byline--7f7878b3441c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7f7878b3441c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7f7878b3441c--------------------------------)
    [Oliver S](https://medium.com/@hrmnmichaels?source=post_page---byline--7f7878b3441c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@hrmnmichaels?source=post_page---byline--7f7878b3441c--------------------------------)[![Oliver
    S](../Images/b5ee0fa2d5fb115f62e2e9dfcb92afdd.png)](https://medium.com/@hrmnmichaels?source=post_page---byline--7f7878b3441c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7f7878b3441c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7f7878b3441c--------------------------------)
    [Oliver S](https://medium.com/@hrmnmichaels?source=post_page---byline--7f7878b3441c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7f7878b3441c--------------------------------)
    ·10 min read·6 days ago
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7f7878b3441c--------------------------------)
    ·阅读时间10分钟·6天前
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: In our previous post, we wrapped up the introductory series on fundamental reinforcement
    learning (RL) techniques by exploring Temporal-Difference (TD) learning. TD methods
    merge the strengths of Dynamic Programming (DP) and Monte Carlo (MC) methods,
    leveraging their best features to form some of the most important RL algorithms,
    such as Q-learning.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的文章中，我们通过探索时间差分（TD）学习总结了强化学习（RL）技术的基础系列。TD方法结合了动态规划（DP）和蒙特卡罗（MC）方法的优点，利用它们的最佳特性形成了一些最重要的RL算法，如Q学习。
- en: Building on that foundation, this post delves into **n-step TD learning**, a
    versatile approach introduced in Chapter 7 of Sutton’s book [1]. This method bridges
    the gap between classical TD and MC techniques. Like TD, n-step methods use bootstrapping
    (leveraging prior estimates), but they also incorporate the next `n` rewards,
    offering a unique blend of short-term and long-term learning. In a future post,
    we’ll generalize this concept even further with **eligibility traces**.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这一基础，本文深入探讨了**n步TD学习**，这是萨顿在其书籍第7章中介绍的一个多功能方法[1]。该方法弥合了经典TD和MC技术之间的差距。像TD一样，n步方法使用自举（利用先前的估计），但它们还结合了接下来的`n`个奖励，提供了短期和长期学习的独特结合。在未来的文章中，我们将通过**资格迹**进一步概括这一概念。
- en: 'We’ll follow a structured approach, starting with the **prediction problem**
    before moving to **control**. Along the way, we’ll:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用一种结构化方法，从**预测问题**开始，然后转向**控制**。在这个过程中，我们将：
- en: Introduce **n-step Sarsa**,
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍**n步Sarsa**，
- en: Extend it to **off-policy learning**,
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将其扩展到**离策略学习**，
- en: Explore the **n-step tree backup algorithm**, and
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索**n步树备份算法**，并
- en: Present a unifying perspective with **n-step Q(σ)**.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提出一个统一的视角，使用**n步Q(σ)**。
- en: As always, you can find all accompanying code on [GitHub](https://github.com/hermanmichaels/rl_book).
    Let’s dive in!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，您可以在[GitHub](https://github.com/hermanmichaels/rl_book)上找到所有配套代码。让我们开始吧！
