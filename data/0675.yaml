- en: 'Understanding Tensors: Learning a Data Structure Through 3 Pesky Errors'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解张量：通过 3 个令人头疼的错误学习数据结构
- en: 原文：[https://towardsdatascience.com/understanding-tensors-learning-a-data-structure-through-3-pesky-errors-6d674776be0c?source=collection_archive---------4-----------------------#2024-03-13](https://towardsdatascience.com/understanding-tensors-learning-a-data-structure-through-3-pesky-errors-6d674776be0c?source=collection_archive---------4-----------------------#2024-03-13)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/understanding-tensors-learning-a-data-structure-through-3-pesky-errors-6d674776be0c?source=collection_archive---------4-----------------------#2024-03-13](https://towardsdatascience.com/understanding-tensors-learning-a-data-structure-through-3-pesky-errors-6d674776be0c?source=collection_archive---------4-----------------------#2024-03-13)
- en: What working through TensorFlow errors has taught me about tensors
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过解决 TensorFlow 错误，我学到的关于张量的知识
- en: '[](https://medium.com/@erevear?source=post_page---byline--6d674776be0c--------------------------------)[![Eva
    Revear](../Images/675266fccb503690d50d83b8c92f48b8.png)](https://medium.com/@erevear?source=post_page---byline--6d674776be0c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6d674776be0c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6d674776be0c--------------------------------)
    [Eva Revear](https://medium.com/@erevear?source=post_page---byline--6d674776be0c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@erevear?source=post_page---byline--6d674776be0c--------------------------------)[![Eva
    Revear](../Images/675266fccb503690d50d83b8c92f48b8.png)](https://medium.com/@erevear?source=post_page---byline--6d674776be0c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6d674776be0c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6d674776be0c--------------------------------)
    [Eva Revear](https://medium.com/@erevear?source=post_page---byline--6d674776be0c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6d674776be0c--------------------------------)
    ·11 min read·Mar 13, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6d674776be0c--------------------------------)
    ·11分钟阅读·2024年3月13日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/feff5a593eac65c3656a2db9bd90c879.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/feff5a593eac65c3656a2db9bd90c879.png)'
- en: Photo by [Michael Dziedzic](https://unsplash.com/@lazycreekimages?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Michael Dziedzic](https://unsplash.com/@lazycreekimages?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: I’ve recently been tinkering with deep learning models in Tensorflow, and have
    accordingly been introduced to managing data as tensors.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最近我一直在 TensorFlow 中尝试深度学习模型，因此也开始学习如何将数据管理为张量。
- en: As a Data Engineer that works all day in tables that I can easily slice, dice,
    and visualize, I had absolutely no intuition around working with tensors, and
    I seemed to constantly run into the same errors that, especially at first, went
    way over my head.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一名数据工程师，整天工作在可以轻松切片、切块并可视化的表格中，我对张量的操作几乎没有任何直觉，并且我似乎总是遇到相同的错误，尤其是在开始时，这些错误让我一头雾水。
- en: However, deep diving them has taught me a lot about tensors and TensorFlow,
    and I wanted to consolidate those learnings here to use as a reference.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，深入研究这些错误让我学到了很多关于张量和 TensorFlow 的知识，我想在这里整理这些学习成果，以便作为参考。
- en: If you have a favorite error, solution, or debugging tip, please leave a comment!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有喜欢的错误、解决方案或调试技巧，请留下评论！
- en: Code recipes for debugging
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调试的代码示例
- en: Before we dive into the errors themselves, I wanted to document a few of the
    light-weight, simple bits and pieces of code that I’ve found helpful in debugging.
    (Although it must be stated for legal reasons that we of course always debug with
    official debugging features and never just dozens of print statements 🙂)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨这些错误之前，我想记录一些我在调试过程中发现的轻量级、简单的代码片段，它们对我很有帮助。（虽然必须声明的是，出于法律原因，我们当然总是使用官方的调试工具来调试，而不是仅仅依赖于打印语句
    🙂）
- en: '**Seeing inside our Tensorflow Datasets**'
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**查看我们 TensorFlow 数据集的内部**'
- en: First off, looking at our actual data. When we print a Dataframe or SELECT *
    in SQL, we see the data! When we print a tensor dataset we see…
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，看看我们的实际数据。当我们打印 DataFrame 或在 SQL 中执行 SELECT * 时，我们能看到数据！但当我们打印张量数据集时，我们看到的是……
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This is all quite useful information, but it doesn’t help us understand what’s
    actually going on in our data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是非常有用的信息，但它们并没有帮助我们理解数据中到底发生了什么。
- en: 'To print a single tensor within the execution graph we can leverage tf.print.
    This article is a wonderful deep dive into tf.print that I highly recommend if
    you plan to use it often: [Using tf.Print() in TensorFlow](/using-tf-print-in-tensorflow-aa26e1cff11e)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 要打印执行图中的单个张量，我们可以利用tf.print。如果你打算经常使用它，我强烈推荐这篇深入探讨tf.print的文章：[在TensorFlow中使用tf.Print()](https://wiki.example.org/feynmans_learning_method)。
- en: 'But when working with Tensorflow datasets during development, sometimes we
    need to see a few values at a time. For that we can loop through and print individual
    pieces of data like this:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 但在开发过程中使用TensorFlow数据集时，有时我们需要一次查看几个值。为此，我们可以像这样循环并打印单个数据片段：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can also use skip to get to a specific index:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用skip跳到特定的索引：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Knowing our tensors’ specs**'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**了解我们的张量规格**'
- en: 'When working with tensors we also need to know their shape, rank, dimension,
    and data type (if some of that vocabulary is unfamiliar, as it was to me initially,
    don’t worry, we’ll get back to it later in the article). Anyway, below are a few
    lines of code to gather this information:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理张量时，我们还需要知道它们的形状、秩、维度和数据类型（如果这些词汇对你来说有些陌生，就像最初对我来说一样，别担心，稍后我们会回到这些内容）。无论如何，下面是一些代码行，用于收集这些信息：
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The above outputs:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的输出：
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Augmenting model.summary()**'
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**增强model.summary()**'
- en: Finally, its is always helpful to be able to see how data is moving through
    a model, and how shape changes throughout inputs and outputs between layers. The
    source of many an error will be a mismatch between these expected input and output
    shapes and the shape of a given tensor.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，能够看到数据如何在模型中流动，以及在各层之间输入输出的形状如何变化，总是很有帮助。许多错误的根源通常是输入和输出形状之间的不匹配，以及给定张量的形状。
- en: '[model.summary()](https://www.geeksforgeeks.org/tensorflow-js-tf-layersmodel-class-summary-method/)
    of course gets the job done, but we can supplement that information with the following
    snippet, which adds a bit more context with model and layer inputs and outputs:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[model.summary()](https://www.geeksforgeeks.org/tensorflow-js-tf-layersmodel-class-summary-method/)
    当然可以完成这项工作，但我们可以通过以下代码片段补充一些信息，这为模型和层的输入输出提供了更多上下文：'
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Learning from our mistakes
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从我们的错误中学习
- en: So let’s jump into some errors!
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们来看看一些错误吧！
- en: Rank
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 秩
- en: 'ValueError: Shape must be rank x but is rank y….'
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'ValueError: 形状必须是秩x，但却是秩y……'
- en: Ok, first of all, what is a rank? Rank is just the unit of dimensionality we
    use to describe tensors. A rank 0 tensor is a scalar value; a rank one tensor
    is a vector; a rank two is a matrix, and so on for all n dimensional structures.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，首先，什么是秩？秩就是我们用来描述张量的维度单位。秩为0的张量是一个标量值；秩为1的张量是一个向量；秩为2的张量是一个矩阵，依此类推，对于所有n维结构。
- en: Take for example a 5 dimensional tensor.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以一个五维张量为例。
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The code above shows that each dimension of the five has a size of two. If
    we wanted to index it, we could do so along any of these axes. To get at the last
    element, 32, we would run something like:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码显示，每个维度的大小为2。如果我们想索引它，可以沿着这些轴中的任何一个进行。为了获取最后一个元素32，我们可以运行类似以下的代码：
- en: '[PRE8]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The [official tensor documentation](https://www.tensorflow.org/guide/tensor#basics)
    has some really helpful visualizations to make this a bit more comprehensible.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[官方张量文档](https://www.tensorflow.org/guide/tensor#basics)中有一些非常有用的可视化，能够帮助你更容易地理解这一点。'
- en: 'Back to the error: it is just flagging that the tensor provided is a different
    dimension than what is expected to a particular function. For example if the error
    declares that the “Shape must be rank 1 but is rank 0…” it means that we are providing
    a scalar value, and it expects a 1-D tensor.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 回到错误：它只是标记了提供的张量与某个特定函数预期的维度不同。例如，如果错误声明“形状必须是秩1，但却是秩0…”，这意味着我们提供的是一个标量值，而它期望的是一个一维张量。
- en: Take the example below where we are trying to multiply tensors together with
    the matmul method.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个示例，我们试图使用matmul方法将张量相乘。
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If we take a peek at the [documentation](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul),
    matmul expects at least a rank 2 tensor, so multiplying the matrix by [1,2,3,4,5,6],
    which is just an array, will raise this error.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看[文档](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul)，matmul函数至少需要一个秩为2的张量，因此将矩阵与[1,2,3,4,5,6]（这只是一个数组）相乘会引发此错误。
- en: '[PRE10]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'A great first step for this error is to dive into the documentation and understand
    what the function you are using is looking for (Here’s a nice list of the functions
    available on tensors: [raw_ops](https://www.tensorflow.org/api_docs/python/tf/raw_ops).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个错误的一个好方法是深入文档，理解你正在使用的函数期望什么（这里有一个关于张量的函数列表：[raw_ops](https://www.tensorflow.org/api_docs/python/tf/raw_ops)）。
- en: Then use the rank method to determine what we are actually providing.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用秩方法来确定我们实际上提供了什么。
- en: '[PRE11]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As far as fixes go, tf.reshape is often a good option to start with. Let’s
    take a brief moment to talk a little bit about tf.reshape, since it will be a
    faithful companion throughout our Tensorflow journey: [tf.reshape(tensor, shape,
    name=None)](https://www.tensorflow.org/api_docs/python/tf/reshape)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 至于解决方法，tf.reshape通常是一个不错的起点。让我们稍微谈一下tf.reshape，因为它将是我们整个Tensorflow旅程中的忠实伙伴：[tf.reshape(tensor,
    shape, name=None)](https://www.tensorflow.org/api_docs/python/tf/reshape)
- en: 'Reshape simply takes in the tensor we want to reshape, and another tensor containing
    what we want the shape of the output to be. For example, let’s reshape our multiplication
    input:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: reshape简单地接受我们想要重塑的张量，和另一个包含我们希望输出形状的张量。例如，让我们重塑我们的乘法输入：
- en: '[PRE13]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Our variable will turn into a (3,2) tensor (3 rows, 2 columns). A quick note,
    tf.reshape(t, [3, -1]).numpy() will produce the same thing because the -1 tells
    Tensorflow to compute the size of the dimension such that the total size remains
    constant. The number of elements in the shape tensor is the rank.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的变量将变成一个(3,2)的张量（3行，2列）。快速提示，tf.reshape(t, [3, -1]).numpy()也会产生相同的结果，因为-1告诉Tensorflow计算该维度的大小，以确保总大小保持不变。形状张量中的元素数量即为秩。
- en: Once we create a tensor with the proper rank, our multiplication will work just
    fine!
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们创建了具有正确秩的张量，我们的乘法就能顺利进行！
- en: Shape
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 形状
- en: 'ValueError: Input of layer is incompatible with layer….'
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ValueError：层的输入与层不兼容……
- en: Having an intuitive understanding of tensor shape, and how it interacts and
    changes across model layers has made life with deep learning significantly easier
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对张量形状的直观理解，以及它如何在模型各层之间相互作用和变化，已经使得深度学习的生活变得更加轻松。
- en: 'First, getting basic vocab out of the way: the shape of a tensor refers to
    the number of elements along each dimension, or axis of the tensor. For example,
    a 2D tensor with 3 rows and 4 columns has a shape of (3, 4).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，先了解基本的词汇：张量的形状指的是沿张量每个维度或轴的元素数量。例如，一个3行4列的二维张量，其形状为(3, 4)。
- en: So what can go wrong with shape? Glad you asked, quite a few things!
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，形状可能会出什么问题呢？很高兴你问了，问题可多了！
- en: 'First and foremost the shape and rank of your training data must match the
    input shape expected by the input layer. Let’s take a look at an example, a basic
    CNN:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，训练数据的形状和秩必须与输入层期望的输入形状匹配。让我们看一个例子，一个基本的CNN：
- en: '[PRE15]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Trying to run the code above will result in:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试运行上述代码将会导致：
- en: '[PRE16]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This is because our model is expecting the input tensor to be of the shape (128,
    128, 3) and our generated data is (64, 64, 3).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为我们的模型期望输入张量的形状为(128, 128, 3)，而我们生成的数据是(64, 64, 3)。
- en: 'In a situation like this, our good friend, reshape, or another Tensorflow function,
    resize, can help. If, as in the case above we are working with images, we can
    simply run resize or change the expectations of our model’s input:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们的好朋友reshape，或者Tensorflow的另一个函数resize，可以帮助我们。如果像上面那样我们在处理图像，我们可以简单地运行resize，或者改变模型输入的期望：
- en: '[PRE17]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In this context, it is helpful to know a little about how common types of models
    and model layers expect input of different shapes, so let’s take a little detour.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，了解常见类型的模型和模型层如何期望不同形状的输入是有帮助的，所以让我们稍微绕个弯。
- en: Deep Neural Networks of Dense layers take in 1 dimensional tensors (or 2 dimensional,
    depending on whether you include batch size, but we will talk about batch size
    in a bit) of the format (feature_size, ) where feature_size is the number of features
    in each sample.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络的全连接层接受的是一维张量（或者是二维的，取决于是否包括批次大小，不过我们稍后会讨论批次大小），格式为(feature_size, )，其中feature_size是每个样本的特征数量。
- en: Convolutional Neural Networks take in data representing images, using 3 dimensional
    tensors of (width, height, channels) where channels are the color scheme, 1 for
    gray scale, and 3 for RBG.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络接受表示图像的数据，使用三维张量（宽度，高度，通道），其中通道是颜色方案，1表示灰度，3表示RGB。
- en: And finally, Recurrent Neural Networks such as LTSMs take in 2 dimensions (time
    steps, feature_size)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，递归神经网络（如LTSM）接受的是二维张量（时间步，特征大小）。
- en: But back to errors! Another common culprit in Tensorflow shape errors has to
    do with how shape changes as data passes through the model layers. As previously
    mentioned, different layers take in different input shapes, and they can also
    reshape output.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 但回到错误！Tensorflow形状错误的另一个常见原因与数据通过模型层时形状的变化有关。如前所述，不同的层接受不同形状的输入，并且它们也可以重新调整输出的形状。
- en: Returning to our CNN example from above, let’s break it again, by seeing what
    happens when we remove the Flatten layer. If we try to run the code we will see
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 返回到我们上面的CNN示例，让我们再次破坏它，看看当我们移除Flatten层时会发生什么。如果我们尝试运行代码，我们将看到
- en: '[PRE18]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This is where printing all of our model input and output shapes along with our
    data shapes comes in handy to help us pinpoint where there is a mismatch.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是打印我们所有模型输入和输出形状以及数据形状变得非常有用的地方，帮助我们定位不匹配的地方。
- en: model.summary() will show us
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: model.summary()将展示给我们
- en: '[PRE19]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: And our further diagnostic will reveal
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步的诊断将揭示
- en: '[PRE20]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: It is a lot of output, but we can see that dense_13 layer is looking for input
    of (None, 50176) shape. However, conv2d_17 layer outputs (None, 28, 28, 64)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 输出信息很多，但我们可以看到dense_13层正在寻找形状为(None, 50176)的输入。然而，conv2d_17层输出的是(None, 28, 28,
    64)。
- en: Flatten layers transform the multi-dimensional output from previous layers into
    a one-dimensional (flat) vector that the Dense layer expects.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Flatten层将前面层的多维输出转换为Dense层期望的一维（平坦）向量。
- en: 'Conv2d and Max Pooling layers change their input data in other interesting
    ways as well, but those are out of scope for this article. For an awesome breakdown
    take a look at: [Ultimate Guide to Input shape and Model Complexity in Neural
    Networks](/ultimate-guide-to-input-shape-and-model-complexity-in-neural-networks-ae665c728f4b)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Conv2d和Max Pooling层也以其他有趣的方式改变它们的输入数据，但这些超出了本文的范围。要了解更多详细信息，可以查看：[神经网络中输入形状和模型复杂度的终极指南](/ultimate-guide-to-input-shape-and-model-complexity-in-neural-networks-ae665c728f4b)
- en: But what about batch size?! I haven’t forgotten!
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 那批量大小呢？！我没有忘记！
- en: 'If we break our code one more time by removing the .batch(32) from the dataset
    in model.fit we will get the error:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们再一次通过从model.fit中的数据集移除.batch(32)来破坏我们的代码，我们将得到如下错误：
- en: '[PRE21]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: That is because, the first dimension of a layer’s input is reserved for the
    batch size or number of samples that we want the model to work through at a time.
    For a great deep dive read through [Difference between batch and epoch](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为，层输入的第一个维度是为批量大小或我们希望模型一次处理的样本数保留的。要深入了解，可以阅读[批量和轮次之间的区别](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/)。
- en: Batch size defaults to None prior to fitting, as we can see in the model summary
    output, and our model expects us to set it elsewhere, depending on how we tune
    the hyperparameter. We can also force it in our input layer by using batch_input_size
    instead of input_size, but that decreases our flexibility in testing out different
    values.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 批量大小在拟合之前默认为None，正如我们在模型摘要输出中看到的那样，我们的模型期望我们在其他地方设置它，具体取决于我们如何调整超参数。我们也可以通过使用batch_input_size而不是input_size在输入层强制设置它，但这样会降低我们在测试不同值时的灵活性。
- en: Type
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 类型
- en: 'TypeError: Failed to convert object of type to Tensor. Unsupported object type'
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'TypeError: 无法将类型的对象转换为张量。不支持的对象类型。'
- en: Finally, let’s talk a bit about some data type specifics in Tensors.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们简单谈谈Tensor中一些数据类型的具体细节。
- en: The error above is another, that, if you’re used to working in database systems
    with tables built from all sorts of data, can be a bit baffling, but it is one
    of the more simple to diagnose and fix, although there are a couple of common
    causes to look out for.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 上述错误是另一个，如果你习惯于在使用各种数据构建表格的数据库系统中工作，可能会让人有些困惑，但它是比较容易诊断和修复的，尽管有几个常见原因需要注意。
- en: 'The main issue is that, although tensors support a variety of [data types](https://www.tensorflow.org/api_docs/python/tf/dtypes),
    when we convert a NumPy array to tensors (a common flow within deep learning),
    the datatypes must be floats. The script below initializes a contrived example
    of a dataframe with None and with string data points. Let’s walk through some
    issue and fixes for this example:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 主要问题是，尽管张量支持多种[数据类型](https://www.tensorflow.org/api_docs/python/tf/dtypes)，但当我们将NumPy数组转换为张量时（这是深度学习中的常见流程），数据类型必须是浮动类型。下面的脚本初始化了一个包含None和字符串数据点的虚拟数据框。让我们一起分析这个示例中的问题并修复它：
- en: '[PRE22]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Running this code will flag to us that:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这段代码会提醒我们：
- en: '[PRE23]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The most obvious issue is that you are sending in a NumPy array that contains
    some non float type, an object. If you have an actual column of categorical data,
    there are many ways to convert that to numeric data (One shot encoding, etc) but
    that is out of scope for this discussion.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最明显的问题是你传入的是一个包含某些非浮动类型（即对象）的NumPy数组。如果你有一个实际的类别数据列，有很多方法可以将其转换为数值数据（如独热编码等），但这超出了本讨论的范围。
- en: We can determine that if we run print(X_train.dtypes), which will tell us what’s
    in our dataframe that Tensorflow doesn’t like.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行`print(X_train.dtypes)`来确定，这会告诉我们数据框中Tensorflow不喜欢的内容。
- en: '[PRE24]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'If we are running into non float data points, the line below will magically
    solve all of our problems:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们遇到非浮动数据点，下面这行代码将神奇地解决我们所有的问题：
- en: '[PRE25]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Another thing to check for is if you have None or np.nan anywhere.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 另一件需要检查的事情是是否在任何地方出现了`None`或`np.nan`。
- en: 'To find out we can use a few lines of code such as:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 要找出这一点，我们可以使用几行代码，例如：
- en: '[PRE26]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Which tells us that we have nulls on rows 0 and 1:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们，第0行和第1行中有空值：
- en: '[PRE27]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: If so, and that is expected/intentional we need to replace those values with
    an acceptable alternative. Fillna can help us here.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是这样，并且这是预期中的/故意的，我们需要用一个可接受的替代值来替换这些值。`Fillna`可以在这里帮助我们。
- en: '[PRE28]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: With these changes to the code below, our NumPy array will successfully convert
    to a tensor dataset and we can train our model!
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 通过下面对代码的这些修改，我们的NumPy数组将成功转换为张量数据集，我们可以训练我们的模型！
- en: In Conclusion
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: I often find that I learn the most about a particular technology when I have
    to work through errors, and I hope this has been somewhat helpful to you too!
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我常常发现，当我不得不处理错误时，我对某项技术的理解最深，希望这对你也有所帮助！
- en: If you have cool tips and tricks or fun Tensorflow errors please pass them along!
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一些有趣的技巧或Tensorflow错误，请分享给我！
