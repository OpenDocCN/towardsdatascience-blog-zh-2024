- en: Self-Instruct Framework, Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Self-Instruct框架，解释
- en: 原文：[https://towardsdatascience.com/self-instruct-framework-explained-16bce90f4683?source=collection_archive---------10-----------------------#2024-03-05](https://towardsdatascience.com/self-instruct-framework-explained-16bce90f4683?source=collection_archive---------10-----------------------#2024-03-05)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/self-instruct-framework-explained-16bce90f4683?source=collection_archive---------10-----------------------#2024-03-05](https://towardsdatascience.com/self-instruct-framework-explained-16bce90f4683?source=collection_archive---------10-----------------------#2024-03-05)
- en: Or how to “eliminate” human annotators
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 或者说，如何“消除”人工标注者
- en: '[](https://medium.com/@dmitry.tsyuzhentsin?source=post_page---byline--16bce90f4683--------------------------------)[![Tsiu-zhen-tsin
    Dmitrii](../Images/e210c94ae2a6415cba7189c59f7eafa5.png)](https://medium.com/@dmitry.tsyuzhentsin?source=post_page---byline--16bce90f4683--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--16bce90f4683--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--16bce90f4683--------------------------------)
    [Tsiu-zhen-tsin Dmitrii](https://medium.com/@dmitry.tsyuzhentsin?source=post_page---byline--16bce90f4683--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@dmitry.tsyuzhentsin?source=post_page---byline--16bce90f4683--------------------------------)[![Tsiu-zhen-tsin
    Dmitrii](../Images/e210c94ae2a6415cba7189c59f7eafa5.png)](https://medium.com/@dmitry.tsyuzhentsin?source=post_page---byline--16bce90f4683--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--16bce90f4683--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--16bce90f4683--------------------------------)
    [Tsiu-zhen-tsin Dmitrii](https://medium.com/@dmitry.tsyuzhentsin?source=post_page---byline--16bce90f4683--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--16bce90f4683--------------------------------)
    ·10 min read·Mar 5, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--16bce90f4683--------------------------------)
    ·10分钟阅读·2024年3月5日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a2032623dfbb4f53b21fa2fade0a02d4.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2032623dfbb4f53b21fa2fade0a02d4.png)'
- en: Image generated by DALL·E
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由DALL·E生成
- en: Motivation
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动机
- en: '![](../Images/846718fbfb023aecc4441a424b81e209.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/846718fbfb023aecc4441a424b81e209.png)'
- en: 'High-level overview of InstructGPT with human annotated outputs and ranking
    for supervised learning and reward model training | Source: [Training language
    models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: InstructGPT的高级概述，包括人工标注输出和监督学习及奖励模型训练的排名 | 来源：[Training language models to follow
    instructions with human feedback](https://arxiv.org/pdf/2203.02155).
- en: 'As Large Language Models (LLMs) revolutionize our life, the growth of instruction-tuned
    LLMs faces significant challenges: the critical need for vast, varied, and high-quality
    datasets. Traditional methods, such as employing human annotators to generate
    datasets — a strategy used in InstructGPT (image above)— face high costs, limited
    diversity, creativity, and allignment challenges. To address these limitations,
    the Self-Instruct framework² was introduced. Its core idea is simple and powerful:
    let language models (LM) generate training data, leading to more cost-effective,
    diverse and creative datasets.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大语言模型（LLMs）彻底改变我们的生活，指令调优LLMs的增长面临着显著挑战：对大量、多样且高质量数据集的迫切需求。传统方法，如使用人工标注者生成数据集——这也是InstructGPT（上图）采用的策略——面临着高成本、有限的多样性、创造性和一致性挑战。为了应对这些局限性，Self-Instruct框架²应运而生。其核心理念简单而强大：让语言模型（LM）生成训练数据，从而实现更具成本效益、多样性和创造性的数据集。
- en: Therefore, in this article, I would like to lead you through the framework step-by-step,
    demonstrating all the details so that after reading it, you will be able to reproduce
    the results yourself :)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本文中，我将引导你逐步了解框架的各个方面，展示所有细节，以便你在阅读后能够自己重现这些结果 :)
- en: ❗ This article provides all steps from code perspective, so please feel free
    to visit the original [GitHub repository](https://github.com/yizhongw/self-instruct#)
    .❗
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ❗ 本文从代码的角度提供了所有步骤，因此请随时访问原始的[GitHub仓库](https://github.com/yizhongw/self-instruct#)
    。❗
- en: Self-Instruct Framework
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Self-Instruct框架
- en: '![](../Images/c52708b5f5a0ce82c6b7da94255ad7e1.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c52708b5f5a0ce82c6b7da94255ad7e1.png)'
- en: A high-level overview of the Self-Instruct framework
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Self-Instruct框架的高级概述
- en: 'The recipe is relatively straightforward:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法相对简单明了：
- en: '**Step 0** — Define Instruction Data:'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤 0** — 定义指令数据：'
- en: — Add a seed of high-quality and diverse human-written tasks in different domains
    as tuples (instruction, instances) to the task pool;
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — 将高质量且多样的人工编写任务作为（指令，实例）元组添加到任务池中，涵盖不同领域；
- en: '**Step 1 —** Instruction Generation:'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤 1 —** 指令生成：  '
- en: — Sample 8 (6 human-written and 2 model-generated) instructions from the task
    pool;
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '— 从任务池中采样 8 个指令（6 个人工编写和 2 个模型生成）；  '
- en: — Insert bootstrapped instructions into the prompt in a few-shot way and ask
    an LM to come up with more instructions;
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '— 以少量示例的方式将自举生成的指令插入到提示中，并要求语言模型生成更多指令；  '
- en: — Filter generated instructions out based on ROUGE-metric (a method to evaluate
    the similarity between text outputs and reference texts) and some heuristics (I
    will cover this later);
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '— 基于 ROUGE 指标（评估文本输出与参考文本之间相似度的一种方法）和一些启发式方法（稍后会详细说明）过滤生成的指令；  '
- en: — Repeat Step 1 until reaching some amount of instructions;
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '— 重复步骤 1，直到达到一定数量的指令；  '
- en: '**Step 2 —** Classification Task Identification:'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤 2 —** 分类任务识别：  '
- en: — For every generated instruction in the task pool, we need to identify its
    type (classification or non-classification) via a few-shot manner;
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '— 对于任务池中每个生成的指令，我们需要通过少量示例的方式识别其类型（分类任务或非分类任务）；  '
- en: '**Step 3 —** Instance Generation:'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤 3 —** 实例生成：  '
- en: — Given the instructions and task types, generate instances (inputs and outputs)
    and filter them out based on heuristics;
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '— 给定指令和任务类型，生成实例（输入和输出），并根据启发式方法过滤它们；  '
- en: '**Step 4** — Finetuning the LM to Follow Instructions:'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤 4** — 微调语言模型以遵循指令：  '
- en: — Utilize generated tasks to finetune a pre-trained model.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '— 利用生成的任务来微调预训练模型。  '
- en: Voila, that’s how the Self-Instruct works, but the devil is in the details,
    so let’s dive into every step!
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 'Voilà，这就是 Self-Instruct 的工作原理，但关键在于细节，所以让我们深入了解每一个步骤！  '
- en: Step 0 — Define Instruction Data
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '步骤 0 — 定义指令数据  '
- en: '![](../Images/c1ad6c077176a803126a54ade25f05bf.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c1ad6c077176a803126a54ade25f05bf.png)  '
- en: Step 0
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '步骤 0  '
- en: 'Let’s begin by understanding what is inside the initial “Seed of tasks”: it
    consists of 175 seed tasks (25 classification and 150 non-classifications) with
    **one** **instruction** and **one** **instance** per task in different domains.
    Each task has an id, name, instruction, instances (**input and output**), and
    is_classification binary flag, identifying whether the task has a limited output
    label space.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们首先理解“任务种子”的初始内容：它包含 175 个种子任务（25 个分类任务和 150 个非分类任务），每个任务有**一个**指令和**一个**实例，覆盖不同领域。每个任务都有一个
    id、名称、指令、实例（**输入和输出**）和一个 is_classification 二进制标志，用来识别任务是否具有有限的输出标签空间。  '
- en: 'There are some examples of classification and non-classification tasks with
    empty and non-empty input fields:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '这里有一些带有空输入字段和非空输入字段的分类和非分类任务示例：  '
- en: '![](../Images/445da55a377333a447a8e31681447d2c.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/445da55a377333a447a8e31681447d2c.png)  '
- en: Example of classification task with non-empty input
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 分类任务的非空输入示例
- en: '![](../Images/45fde17cc82f3f58851167a597992e3f.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/45fde17cc82f3f58851167a597992e3f.png)  '
- en: Example of non-classification task with empty input
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '非分类任务的空输入示例  '
- en: Therefore, we can see in the first example how the input field clarifies and
    provides context to the more general instruction, while in the second example,
    we don’t need an input field as long as the instruction is already self-contained.
    Also, the first example is the classification task — we can answer it by assigning
    some labels from limited space, while we can’t do the same with the second example.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，我们可以看到在第一个示例中，输入字段如何澄清并为更一般的指令提供上下文，而在第二个示例中，只要指令已经自包含，我们就不需要输入字段。此外，第一个示例是分类任务
    —— 我们可以通过从有限的空间中分配一些标签来回答它，而对于第二个示例，我们则无法做到这一点。  '
- en: This step is **crucial** as long as we encourage task diversity via data formats
    in the dataset and demonstrate correct ways of solving various tasks.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '这一步是**至关重要**的，只要我们通过数据格式在数据集中鼓励任务多样性，并展示解决各种任务的正确方法。  '
- en: As long as we define the instruction format, we add them to the task pool to
    store our final dataset.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '只要我们定义好指令格式，就将它们添加到任务池中，储存我们的最终数据集。  '
- en: Step 1 — Instruction Generation
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '步骤 1 — 指令生成  '
- en: '![](../Images/3ddb5fb00f8753b2ce69ee59bfec769e.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ddb5fb00f8753b2ce69ee59bfec769e.png)  '
- en: Step 1
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '步骤 1  '
- en: '**Sampling and prompting**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**采样与提示**  '
- en: 'By adding a human-written seed set of tasks to the task pool, we can start
    with instructions generation. To do so, we need to sample 8 instructions from
    the task pool (6 human-written and 2 machine-generated) and encode them into the
    following prompt:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '通过向任务池添加人工编写的种子任务集，我们可以开始指令生成。为此，我们需要从任务池中采样 8 个指令（6 个人工编写和 2 个机器生成），并将它们编码为以下提示：  '
- en: '![](../Images/3fe0f35250659fbce7e3c14511796cd6.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3fe0f35250659fbce7e3c14511796cd6.png)  '
- en: Prompt to generate new instructions
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '提示生成新指令  '
- en: However, in the beginning, we do not have any machine-generated instructions.
    Therefore, we just replaced them with empty strings in the prompt.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在一开始，我们并没有任何机器生成的指令。因此，我们只需在提示中将其替换为空字符串。
- en: 'After generation, we extract instructions from the LM’s response (via regular
    expressions), filter them out, and add filtered instructions to the task pool:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 生成后，我们从语言模型的响应中提取指令（通过正则表达式），过滤掉它们，并将过滤后的指令添加到任务池中：
- en: '![](../Images/56ae357136a1310a25a128bc1cf65cc2.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56ae357136a1310a25a128bc1cf65cc2.png)'
- en: Pseudo-code of instruction generation step
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 指令生成步骤的伪代码
- en: We repeat the instruction generation step until we reach some number of machine-generated
    instructions (specified at the beginning of the step).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重复指令生成步骤，直到生成一定数量的机器生成指令（在步骤开始时指定的数量）。
- en: '**Filtering**'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**过滤**'
- en: 'To obtain a diverse dataset, we need to define somehow which instructions will
    be added or not to the task pool, and the easiest way is a heuristically chosen
    set of rules, for instance:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得一个多样化的数据集，我们需要定义某些指令是否会被添加到任务池中，最简单的方法是通过一组启发式选择的规则，例如：
- en: Filter out instructions that are too short or too long;
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤掉过短或过长的指令；
- en: Filter based on keywords unsuitable for language models (image, graph, file,
    plot, …);
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据不适合语言模型的关键词（如图片、图表、文件、图示等）进行过滤；
- en: Filter those starting with punctuation;
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤掉那些以标点符号开头的指令；
- en: Filter those starting with non-English characters;
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤掉那些以非英语字符开头的指令；
- en: Filter those when their ROUGE-L similarity with any existing instruction is
    higher than 0.7;
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当其ROUGE-L相似度与任何现有指令的相似度大于0.7时，过滤掉这些指令；
- en: Step 2— Classification Task Identification
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二步 — 分类任务识别
- en: '![](../Images/c6ffd4aca9d8e7ec419a64b2f5c7b11e.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c6ffd4aca9d8e7ec419a64b2f5c7b11e.png)'
- en: Step 2
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步
- en: 'The authors of Self-Instruct noticed that depending on an instruction, the
    language models can be biased towards one label, especially for classification
    tasks. Therefore, to eliminate such such, we need to classify every instruction
    via few-shot prompting:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Self-Instruct的作者注意到，根据不同的指令，语言模型可能会对某一个标签产生偏向，特别是在分类任务中。因此，为了消除这种偏向，我们需要通过少量示例提示对每个指令进行分类：
- en: '![](../Images/af86a11dae91aa778edbb0a0b6f0bb1a.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af86a11dae91aa778edbb0a0b6f0bb1a.png)'
- en: Prompt used to classify whether a task instruction is a classification or non-classification
    task (12 classification and 19 non-classification instructions are used in this
    template)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 用于分类任务是否为分类任务或非分类任务的提示（此模板中使用了12个分类指令和19个非分类指令）
- en: Step 3 — Instance Generation
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三步 — 实例生成
- en: '![](../Images/c1f43525948bb2d7bb6ef780a01d3f63.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c1f43525948bb2d7bb6ef780a01d3f63.png)'
- en: Step 3
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 第三步
- en: After identifying the instruction type, we can finally generate input and output,
    considering that we have two types of instructions (classification or non-classification).
    How? **Few-shot prompting!**
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在识别了指令类型后，我们最终可以生成输入和输出，考虑到我们有两种类型的指令（分类或非分类）。怎么做呢？**少量示例提示！**
- en: For non-classification instructions, we ask the model to generate input and
    only then output (**Input-First Approach**), but for classification tasks, we
    ask the model to generate output (class label) first and then condition input
    generation based on output (**Output-First Approach**). Compared to Step 0, we
    don’t restrict the number of generated instances per every instruction.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非分类指令，我们要求模型首先生成输入，然后生成输出（**先输入法**），但对于分类任务，我们要求模型先生成输出（类别标签），然后基于输出生成输入（**先输出法**）。与第0步相比，我们不限制每个指令生成的实例数量。
- en: '![](../Images/936041e4deb8138940508512b2d2fc22.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/936041e4deb8138940508512b2d2fc22.png)'
- en: Prompt used for the Input-First Approach of instance generation
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 用于实例生成的输入优先法提示
- en: '![](../Images/81ce9945254ce624197510383efa8904.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81ce9945254ce624197510383efa8904.png)'
- en: Prompt used for the Output-First Approach of instance generation
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 用于输出优先法实例生成的提示
- en: 'After generation, we extract instances and format them (regular expressions);
    after formatting, we filter them out using some rules, for example:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 生成后，我们提取实例并进行格式化（使用正则表达式）；格式化后，我们通过一些规则进行过滤，例如：
- en: If input and output are the same,
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果输入和输出相同，
- en: If instances are already in the task pool,
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果实例已经在任务池中，
- en: If the output is empty,
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果输出为空，
- en: These are usually incomplete generations if the input or output ends with a
    colon;
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些通常是生成不完整的指令，如果输入或输出以冒号结尾；
- en: 'And some other heuristics. In the end, we have the following example of a generated
    task with 1 instruction and 1 instance:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些其他的启发式方法。最后，我们得到了以下示例，其中包含1条指令和1个实例：
- en: '![](../Images/e66b61fa15e4b849411a66a67414099d.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e66b61fa15e4b849411a66a67414099d.png)'
- en: Instance generation example
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 实例生成示例
- en: That’s the main idea behind Self-Intsruct!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是Self-Instruct的主要思想！
- en: Step 4— Finetuning the LM to Follow Instructions
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤4——对语言模型进行微调以遵循指令
- en: After completing all previous steps, we can take a pre-trained LM and instruction-tune
    it on the generated dataset to achieve better metrics.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 完成所有前述步骤后，我们可以采用一个预训练的语言模型，并对其在生成的数据集上进行指令微调，从而获得更好的指标。
- en: Overcoming challenges
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 克服挑战
- en: At the beginning of the article, I covered some challenges that “instruction-tuned”
    LLMs face; let’s see how Self-Instruct enables overcoming them.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在文章的开头，我提到了一些“指令微调”语言模型面临的挑战；现在，让我们看看Self-Instruct如何帮助克服这些挑战。
- en: Quantity
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数量
- en: 'With the help of only 175 initial human-written tasks, 52K instructions and
    82K instances were generated:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在仅有175个初始人工编写的任务的帮助下，生成了52K条指令和82K个实例：
- en: '![](../Images/7c26e7706615a94f7c70e392cccd8c52.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c26e7706615a94f7c70e392cccd8c52.png)'
- en: 'Source: [Self-Instruct: Aligning Language Models with Self-Generated Instructions](https://arxiv.org/pdf/2212.10560)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '来源：[Self-Instruct: Aligning Language Models with Self-Generated Instructions](https://arxiv.org/pdf/2212.10560)'
- en: Diversity
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多样性
- en: To investigate how diverse the generated dataset is, authors of Self-Instruct
    used Berkley Neural Parser to parse instructions and then extract the closest
    verb to the root and its first direct noun object. 26K out of 52K instructions
    have a verb-noun format, but the other 26K instructions have more complex structure
    (e.g., “Classify whether this tweet contains political content or not.”) or are
    framed as questions (e.g., “Which of these statements are true?”).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调查生成数据集的多样性，《Self-Instruct》的作者使用了伯克利神经解析器（Berkley Neural Parser）来解析指令，然后提取与根词最接近的动词及其第一个直接名词对象。在52K条指令中，有26K条具有动词-名词格式，但另外26K条指令具有更复杂的结构（例如，“判断这条推文是否包含政治内容。”）或被构造为问题（例如，“以下哪些陈述是正确的？”）。
- en: '![](../Images/3321770df0415c9c20e8478a05891469.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3321770df0415c9c20e8478a05891469.png)'
- en: 'The top 20 most common root verbs (inner circle) and their top 4 direct noun
    objects (outer circle) in the generated instructions | Source: [Self-Instruct:
    Aligning Language Models with Self-Generated Instructions](https://arxiv.org/pdf/2212.10560)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '在生成的指令中，最常见的前20个根动词（内圈）及其前4个直接名词对象（外圈）| 来源：[Self-Instruct: Aligning Language
    Models with Self-Generated Instructions](https://arxiv.org/pdf/2212.10560)'
- en: Quality
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 质量
- en: 'To prove that Self-Instruct can generate high-quality tasks, it was randomly
    selected 200 generated instructions and sampled 1 instance per instruction, and
    then the author of the framework assessed them, obtaining the following results:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明Self-Instruct能够生成高质量的任务，随机选择了200条生成的指令，并为每条指令抽取1个实例，然后框架的作者对这些任务进行了评估，得出了以下结果：
- en: '![](../Images/75c61fdf98529ce8b391c552b4fe9f56.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75c61fdf98529ce8b391c552b4fe9f56.png)'
- en: 'Source: [Self-Instruct: Aligning Language Models with Self-Generated Instructions](https://arxiv.org/pdf/2212.10560)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '来源：[Self-Instruct: Aligning Language Models with Self-Generated Instructions](https://arxiv.org/pdf/2212.10560)'
- en: As we can see, 92% of all tasks describe a valid task, and 54% — have all valid
    fields (given that we generated 52K tasks, at least 26K will represent high-quality
    data, which is fantastic!)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，92%的任务描述了有效的任务，且54%的任务包含了所有有效字段（考虑到我们生成了52K个任务，至少26K个将代表高质量数据，这是非常棒的！）
- en: '**Costs**'
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**成本**'
- en: The Self-Instruct framework also introduces significant cost advantages as well.
    The initial phases of task generation (Steps 1-3 ) amount to a mere $600, while
    the last step of fine-tuning using the GPT-3 model incurs a cost of $338\. It’s
    vital to keep in mind when we look at results!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Self-Instruct框架也带来了显著的成本优势。任务生成的初始阶段（步骤1-3）仅需600美元，而最后一步使用GPT-3模型进行微调的成本为338美元。我们在查看结果时，必须记住这一点！
- en: Results
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: 'How Self-Instruct can enhance the ROUGE-L metric on the SuperNI (**Super-Natural
    Instructions**) dataset? For that, we can compare the results of 1) off-the-shelf
    pre-trained LMs without any instruction fine-tuning (Vanilla LMs), 2) Instruction-tuned
    models (Instruction-tuned w/o SuperNI), and 3) Instruction-tuned models trained
    on SuperNI (Instruction-tuned w/ SuperNI):'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Self-Instruct如何提高在SuperNI（**超级自然指令**）数据集上的ROUGE-L指标？为此，我们可以比较以下几种情况的结果：1) 没有任何指令微调的预训练语言模型（普通语言模型），2)
    指令微调的模型（没有SuperNI的指令微调），以及3) 在SuperNI数据集上训练的指令微调模型（有SuperNI的指令微调）：
- en: '![](../Images/998b6798d954b2297b79ba9b0c2aa56c.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/998b6798d954b2297b79ba9b0c2aa56c.png)'
- en: 'Evaluation results on ***unseen***tasks from SuperNI | Source: [Self-Instruct:
    Aligning Language Models with Self-Generated Instructions](https://arxiv.org/pdf/2212.10560)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '来自SuperNI的***未见***任务的评估结果 | 来源：[Self-Instruct: Aligning Language Models with
    Self-Generated Instructions](https://arxiv.org/pdf/2212.10560)'
- en: As we can see, using Self-Instruct demonstrates a 33% absolute improvement over
    the original model on the dataset (1); simultaneously, it shows that using the
    framework can also slightly improve metrics after fine-tuning the SuperNI dataset
    (3).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，使用Self-Instruct在数据集（1）上展示了比原始模型高出33%的绝对改进；同时，它还表明，使用该框架在微调SuperNI数据集（3）后，也能略微提高指标。
- en: 'Moreover, if we create a new (=unseen) dataset of 252 instructions and 1 instance
    per instruction and evaluate a selection of instruction-tuned variants, we can
    see the following results:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果我们创建一个新的（=未见的）数据集，包含252条指令和每条指令1个实例，并评估一系列指令调优版本，可以看到以下结果：
- en: '![](../Images/8a00ab05695c5026ef4cc894e16e4a93.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a00ab05695c5026ef4cc894e16e4a93.png)'
- en: 'Performance of GPT3 model and its instruction-tuned variants, evaluated by
    human experts on our 252 user-oriented instructions | Source: [Self-Instruct:
    Aligning Language Models with Self-Generated Instructions](https://arxiv.org/pdf/2212.10560)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 'GPT3模型及其指令调优版本，在我们252条用户导向的指令上经过人类专家评估的表现 | 来源：[Self-Instruct: Aligning Language
    Models with Self-Generated Instructions](https://arxiv.org/pdf/2212.10560)'
- en: GPT3 + Self-Instruct shows impressive results compared to other instruction-tuned
    variants, but there is still a place for improvement compared to InstructGPT (previously
    available LLMs by OpenAI) variants.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: GPT3 + Self-Instruct与其他指令调优版本相比，表现出了令人印象深刻的结果，但与InstructGPT（OpenAI之前发布的LLM）版本相比，仍有提升空间。
- en: Enhancements
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 增强功能
- en: The idea behind Self-Instruct is straightforward, but at the same time, it is
    compelling, so let’s look at how we can use it in different cases.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Self-Instruct背后的理念简单直接，但同时也非常有说服力，因此让我们看看如何在不同的场景中使用它。
- en: Stanford Alpaca³
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 斯坦福Alpaca³
- en: In 2023, Alpaca LLM from Stanford gained colossal interest due to affordability,
    accessibility, and the fact that it was developed for less than $600, and at the
    same time, it combined LLaMA and Self-Instruct ideas.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年，斯坦福大学的Alpaca LLM因其低廉的成本、易得性以及开发成本不到600美元而引起了极大的关注，同时，它结合了LLaMA和Self-Instruct的理念。
- en: '![](../Images/b4d952b864fe39083fd674bcca9c7f51.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4d952b864fe39083fd674bcca9c7f51.png)'
- en: 'High-level overview of Alpaca | Source: [Alpaca: A Strong, Replicable Instruction-Following
    Model](https://crfm.stanford.edu/2023/03/13/alpaca.html)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 'Alpaca的高层概述 | 来源：[Alpaca: A Strong, Replicable Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html)'
- en: 'Alpaca’s version of Self-Instruct were slightly modified:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Alpaca版本的Self-Instruct略有修改：
- en: 'Step 1 (instruction generation): more aggressive batch decoding was applied,
    i.e., generating 20 instructions at once'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一步（指令生成）：应用了更为激进的批量解码，即一次生成20条指令
- en: 'Step 2 (classification task): this step was wholly excluded'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二步（分类任务）：此步骤完全被排除
- en: 'Step 3 (instance generation): only one instance is generated per instruction'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三步（实例生成）：每条指令仅生成一个实例
- en: 'In the end, researchers from Stanford could achieve significant improvements
    in comparison to the initial set-up in Self-Instruct and based on performed a
    blind pairwise comparison between text-davinci-003 (InstructGPT-003) and Alpaca
    7B: Alpaca wins 90 versus 89 comparisons against text-davinci-003.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，斯坦福大学的研究人员通过在Self-Instruct中的设置取得了显著的改进，并且进行了盲对比测试，比较了text-davinci-003（InstructGPT-003）和Alpaca
    7B：在90对89的比较中，Alpaca战胜了text-davinci-003。
- en: Self-Rewarding Language Models⁴
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自奖励语言模型⁴
- en: '![](../Images/0219851ef32d6d7ffd7ab7fa267ece14.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0219851ef32d6d7ffd7ab7fa267ece14.png)'
- en: 'Source: [Self-Rewarding Language Models](https://arxiv.org/pdf/2401.10020)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[Self-Rewarding Language Models](https://arxiv.org/pdf/2401.10020)
- en: In 2024, Self-Instruct is a practical framework used in more complex set-ups
    like in Self-Rewarding Language Models by Meta. As in Self-Instruct, initially,
    we have a seed set of human-written tasks; we then generate new instructions {xᵢ}
    and prompt model Mₜ to generate outputs {yᵢ¹, …, yᵢᵏ} and later generate rewards
    {rᵢ¹, …, rᵢᵏ } — that’s how we could ““eliminate”” human-annotators in InstructGPT
    by self-instruction process. The last block of Self-Rewarding models is instruction
    following training — on this step, we compose preference pairs and via DPO train
    Mₜ₊₁ — next iteration model. Therefore, we can repeat this procedure repeatedly
    to enrich the dataset and improve the initial pre-trained model.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在2024年，Self-Instruct 是一种在更复杂的设置中使用的实用框架，比如 Meta 的自奖励语言模型中。如同 Self-Instruct 一样，最初我们有一组人工编写的任务；然后我们生成新的指令
    {xᵢ} 并提示模型 Mₜ 生成输出 {yᵢ¹, …, yᵢᵏ}，接着生成奖励 {rᵢ¹, …, rᵢᵏ} — 这就是通过自指令过程在 InstructGPT
    中“消除”人工标注员的方式。Self-Rewarding 模型的最后一个步骤是指令跟随训练——在这个步骤中，我们构造偏好对并通过 DPO 训练 Mₜ₊₁ ——
    下一代模型。因此，我们可以反复执行这个过程，以丰富数据集并改善初始的预训练模型。
- en: Exploring Limitations
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索局限性
- en: Although Self-Instruct offers an innovative approach to autonomous dataset generation,
    its reliance on large pre-trained models introduces potential limitations.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Self-Instruct 提供了一种创新的自主管理数据集生成的方法，但它对大型预训练模型的依赖引入了潜在的局限性。
- en: Data quality
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据质量
- en: 'Despite the impressive capability to generate synthetic data, the quality —
    marked by a 54% validity in the Overcoming Challenges section — remains a concern.
    It underscores a critical issue: the biases inherent in pre-trained models could
    replicate, or even amplify, within the generated datasets.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在生成合成数据方面展现了令人印象深刻的能力，但质量——在“克服挑战”部分标注为54%的有效性——仍然是一个关注点。这凸显了一个关键问题：预训练模型中固有的偏差可能在生成的数据集中复制，甚至放大。
- en: Tail phenomena
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 尾现象
- en: 'Instructions vary in frequency: some instructions are frequently requested,
    while others are rare. Nonetheless, it’s crucial to effectively manage these infrequent
    requests, as they highlight the brittleness of LLMs in processing uncommon and
    creative tasks.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 指令的频率各不相同：一些指令被频繁请求，而另一些则较为罕见。然而，管理这些不常见的请求至关重要，因为它们揭示了 LLM 在处理不常见和创意任务时的脆弱性。
- en: Conclusion
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In conclusion, the Self-Instruct framework represents an advancement in developing
    instruction-tuned LMs, offering an innovative solution to the challenges of dataset
    generation. Enabling LLMs to autonomously produce diverse and high-quality data
    significantly reduces dependency on human annotators, therefore driving down costs.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，Self-Instruct 框架代表了在开发指令调优语言模型方面的一个进步，提供了一种创新的解决方案来应对数据集生成的挑战。使 LLM 能够自主生成多样化且高质量的数据，显著降低了对人工标注员的依赖，从而减少了成本。
- en: Unless otherwise noted, all images are by the author, inspired by [Self-Instruct](https://arxiv.org/pdf/2212.10560)
    :)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图像均由作者提供，灵感来自 [Self-Instruct](https://arxiv.org/pdf/2212.10560) :)
- en: '**References:**'
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**参考文献：**'
- en: '[1] Ouyang, Long, et al. “[Training language models to follow instructions
    with human feedback](https://arxiv.org/pdf/2203.02155).” *Advances in Neural Information
    Processing Systems* 35 (2022): 27730–27744'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Ouyang, Long 等. “[训练语言模型跟随指令并获得人类反馈](https://arxiv.org/pdf/2203.02155)”。
    *神经信息处理系统进展* 35 (2022): 27730–27744'
- en: '[2] Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N.A., Khashabi, D. and
    Hajishirzi, H., 2022\. [Self-instruct: Aligning language model with self generated
    instructions](https://arxiv.org/pdf/2212.10560). *arXiv preprint arXiv:2212.10560*.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N.A., Khashabi, D. 和 Hajishirzi,
    H., 2022\. [Self-Instruct：使语言模型与自生成指令对齐](https://arxiv.org/pdf/2212.10560)。 *arXiv
    预印本 arXiv:2212.10560*。'
- en: '[3] Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C.,
    Liang, P. and Hashimoto, T.B., 2023\. [Stanford alpaca: An instruction-following
    llama model](https://crfm.stanford.edu/2023/03/13/alpaca.html).'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C.,
    Liang, P. 和 Hashimoto, T.B., 2023\. [斯坦福Alpaca：一种指令跟随的Llama模型](https://crfm.stanford.edu/2023/03/13/alpaca.html)。'
- en: '[4] Yuan, W., Pang, R.Y., Cho, K., Sukhbaatar, S., Xu, J. and Weston, J., 2024\.
    [Self-rewarding language models](https://arxiv.org/pdf/2401.10020). *arXiv preprint
    arXiv:2401.10020*.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Yuan, W., Pang, R.Y., Cho, K., Sukhbaatar, S., Xu, J. 和 Weston, J., 2024\.
    [自奖励语言模型](https://arxiv.org/pdf/2401.10020)。 *arXiv 预印本 arXiv:2401.10020*。'
