- en: Streamlining Giants
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精简巨头
- en: 原文：[https://towardsdatascience.com/streamlining-giants-8a26aa1e91d3?source=collection_archive---------5-----------------------#2024-02-29](https://towardsdatascience.com/streamlining-giants-8a26aa1e91d3?source=collection_archive---------5-----------------------#2024-02-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/streamlining-giants-8a26aa1e91d3?source=collection_archive---------5-----------------------#2024-02-29](https://towardsdatascience.com/streamlining-giants-8a26aa1e91d3?source=collection_archive---------5-----------------------#2024-02-29)
- en: The Evolution of Model Compression in the LLM Era
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM时代模型压缩的演变
- en: '[](https://natecibik.medium.com/?source=post_page---byline--8a26aa1e91d3--------------------------------)[![Nate
    Cibik](../Images/008c22b715ddf4f1d0f9970142edc09f.png)](https://natecibik.medium.com/?source=post_page---byline--8a26aa1e91d3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8a26aa1e91d3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8a26aa1e91d3--------------------------------)
    [Nate Cibik](https://natecibik.medium.com/?source=post_page---byline--8a26aa1e91d3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://natecibik.medium.com/?source=post_page---byline--8a26aa1e91d3--------------------------------)[![Nate
    Cibik](../Images/008c22b715ddf4f1d0f9970142edc09f.png)](https://natecibik.medium.com/?source=post_page---byline--8a26aa1e91d3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8a26aa1e91d3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8a26aa1e91d3--------------------------------)
    [Nate Cibik](https://natecibik.medium.com/?source=post_page---byline--8a26aa1e91d3--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8a26aa1e91d3--------------------------------)
    ·20 min read·Feb 29, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8a26aa1e91d3--------------------------------)
    ·阅读时间20分钟·2024年2月29日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/b460d40c5ff6a9302412017e3f460052.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b460d40c5ff6a9302412017e3f460052.png)'
- en: Image by author using DALL-E 3.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者，使用DALL-E 3生成。
- en: The advent of [transformers](https://arxiv.org/abs/1706.03762) in 2017 set off
    a landslide of AI milestones, starting with the spectacular achievements of large
    language models (LLMs) in natural language processing (NLP), and quickly catalyzing
    advancement in other domains such as [computer vision and robotics](/navigating-the-future-62ea60f27046).
    The unification of NLP and computer vision problems into a common architecture
    accelerated efforts in learning joint vision-language representation spaces, which
    enabled the seminal achievements in vision-language modeling surrounding contrastive
    language-image pretraining ([CLIP](https://arxiv.org/abs/2103.00020)) in 2021,
    and lead to the birth of large multimodal models ([LMMs](https://huyenchip.com/2023/10/10/multimodal.html)).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年，[transformers](https://arxiv.org/abs/1706.03762)的出现引发了人工智能的里程碑式进展，首先是大型语言模型（LLMs）在自然语言处理（NLP）领域的惊人突破，随后迅速推动了计算机视觉和机器人等领域的进步，参见[计算机视觉与机器人](https://towardsdatascience.com/navigating-the-future-62ea60f27046)。NLP和计算机视觉问题的统一架构加速了联合视觉-语言表示空间的学习，这为2021年围绕对比语言-图像预训练的视觉-语言建模（[CLIP](https://arxiv.org/abs/2103.00020)）的开创性成果铺平了道路，并催生了大型多模态模型（[LMMs](https://huyenchip.com/2023/10/10/multimodal.html)）的诞生。
- en: This dawning era of large models has demonstrated awe-inspiring capabilities
    and marked several major strides toward artificial general intelligence (AGI),
    but the enormous size of these models makes them difficult to deploy. As with
    many transformative technologies before them, the alluring capabilities of LLMs
    were at first accessible only to those with the resources to operate at the bleeding
    edge of technology. While private research has continued to push the limits of
    performance using LMMs with hundreds of billions of parameters, open-source research
    has established a pattern of catching up to these watermarks using much smaller
    models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这一大型模型的曙光时代展现了令人敬畏的能力，并在朝着人工通用智能（AGI）迈进的过程中取得了几个重要进展，但这些模型的巨大规模使得它们的部署变得困难。与许多变革性技术一样，LLMs的迷人能力最初只有那些拥有前沿技术资源的人才能使用。尽管私人研究仍在使用数百亿参数的LMMs推动性能的极限，但开源研究已经建立了一个追赶这些水印的模式，使用的是规模更小的模型。
- en: '![](../Images/30002d7396d4738166ced1b8f2d8eb55.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30002d7396d4738166ced1b8f2d8eb55.png)'
- en: Image by author. Over time, open-source LLMs are closing the performance gap
    with much smaller sizes.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者。随着时间的推移，开源LLMs正在缩小与规模更小模型的性能差距。
- en: 'Even with this increasing potency, however, the smallest LLMs will not fit
    on consumer GPUs for inference, let alone be trained, without *model compression*
    techniques being applied. Fortunately, the tantalizing capabilities of these models
    have driven researchers to find effective methods for squeezing them into smaller
    spaces. Thanks to their efforts, we can now deploy LLMs easily on consumer hardware,
    as well as fine-tune them to our desired use cases, without requiring the resources
    of the corporate titans. This series provides a comprehensive review of four model
    compression techniques that enable LLM inference and fine-tuning in resource-constrained
    environments, outlined below:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管其效能不断增强，最小的LLM仍然无法在消费级GPU上进行推理，更别提进行训练，除非应用了*模型压缩*技术。幸运的是，这些模型令人垂涎的能力促使研究人员找到了有效的方法，将它们压缩到更小的空间。得益于他们的努力，我们现在可以轻松地在消费级硬件上部署LLM，并根据我们的需求进行微调，而无需依赖企业巨头的资源。本系列文章全面回顾了四种模型压缩技术，这些技术使得在资源受限的环境中实现LLM推理和微调成为可能，下面列出了这些技术：
- en: Model Compression Techniques for LLMs
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大型语言模型的模型压缩技术
- en: '**Pruning** — removal of network parameters individually (unstructured) or
    in groups (structured) based on their importance in order to decrease model complexity
    while maintaining accuracy.'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**剪枝** — 根据网络参数在模型中的重要性，单独（无结构）或按组（结构化）移除参数，以降低模型复杂度，同时保持准确性。'
- en: '**Quantization** — discretizing and/or reducing the resolution of the numeric
    space of the weights to save space and ease computation.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**量化** — 离散化和/或降低权重数值空间的分辨率，以节省空间并简化计算。'
- en: '**Knowledge Distillation** — small models trained to target the learned functions
    of large experts can be trained with unlabeled data and outperform similar small
    models trained on the original task.'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**知识蒸馏** — 训练小型模型以模仿大型专家模型学习的功能，这些小型模型可以在没有标注数据的情况下进行训练，并且在与在原任务上训练的小型模型相比时，表现更好。'
- en: '**Parameter-Efficient Fine-Tuning** — reduces the number of trainable parameters
    required to fine-tune a model for task-specific behavior.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**参数高效微调** — 减少微调模型以实现特定任务行为所需的可训练参数数量。'
- en: Each of these techniques has been shown to greatly increase efficiency of large
    models with relatively innocuous effects on performance, bringing the awe-inspiring
    powers of the large pre-trained language model (PLM) colossus down to earth where
    they can be used by anyone. Here we explore each technique in detail, so that
    we are empowered to employ them in our own work. Like LLMs, these topics can only
    be compressed to a certain extent before significant loss of knowledge occurs.
    Hence, we will break this discussion into a series of articles to give each of
    these techniques the space it deserves, and properly review these rich legacies
    of research which together provide us with powerful channels for bringing fire
    to mankind.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术中的每一种都已被证明可以显著提高大模型的效率，同时对性能的影响相对无害，将大型预训练语言模型（PLM）的惊人能力带到现实世界中，使得任何人都可以使用它们。在这里，我们详细探讨每一种技术，以便我们能够在自己的工作中加以应用。像大型语言模型一样，这些话题只能在一定程度上进行压缩，否则会导致知识的重大损失。因此，我们将把这个讨论分成一系列文章，以便为每种技术提供足够的空间，充分回顾这些丰富的研究成果，这些成果为我们提供了强大的渠道，将火种带给人类。
- en: 'In this article, we start at the beginning with the oldest technique on our
    list, almost as old as the backpropagation-trained neural networks that it compresses:
    *pruning*. First, a quick journey through the history of pruning will teach us
    the distinctions between *unstructured* and *structured* pruning techniques, along
    with their comparative strengths and weaknesses. Equipped with this prerequisite
    knowledge, we then review the application of these approaches in today’s world
    of LLMs, and offer closing thoughts.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们从列表中最古老的技术开始，它几乎与反向传播训练的神经网络一样古老，它就是：*剪枝*。首先，通过快速回顾剪枝的历史，我们将学习到*无结构*和*结构化*剪枝技术之间的区别，以及它们的比较优缺点。掌握了这些前置知识后，我们将回顾这些方法在当今大型语言模型（LLMs）中的应用，并给出结语。
- en: 'The forthcoming installments in the Streamlining Giants series will provide
    similar dives into each of the remaining compression techniques: quantization,
    knowledge distillation, and parameter-efficient fine-tuning, elucidating a clear
    and comprehensive understanding of each so that we can approach the game of LLM
    development playing with a full deck.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 《精简巨人》系列的后续章节将深入探讨每种剩余的压缩技术：量化、知识蒸馏和参数高效微调，阐明每种技术的清晰而全面的理解，使我们能够以全副武装的姿态进入LLM开发的游戏。
- en: Pruning
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 剪枝
- en: '![](../Images/0b57e051eb629525605208005640a589.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0b57e051eb629525605208005640a589.png)'
- en: Image by author using DALL-E 3.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用DALL-E 3创作的图像。
- en: The quest to refine neural networks for practical applications traces its roots
    back to the foundational days of the field. When [Rumelhart, Hinton, and Williams](https://www.cs.utoronto.ca/~hinton/absps/naturebp.pdf)
    first demonstrated how to use the backpropagation algorithm to successfully train
    multi-layer neural networks that could learn complex, non-linear representations
    in 1986, the vast potential of these models became apparent. However, the computational
    power available in the 1980s restricted their practical use and the complexity
    of problems they could solve, a situation which mirrors the challenges we face
    with deploying LLMs today. Although the scale of models and the considerations
    being made were very different, early discoveries in network minimization would
    pave the way for big wins in model compression decades later. In this section,
    we take a brief journey through the history and motivations driving pruning research,
    discover the comparative strengths and weaknesses of *unstructured* versus *structured*
    methods, and prepare ourselves to explore their use in the modern era of LLMs.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为实际应用而精炼神经网络的探索可以追溯到该领域的基础时期。当[Rumelhart, Hinton, 和 Williams](https://www.cs.utoronto.ca/~hinton/absps/naturebp.pdf)在1986年首次展示如何使用反向传播算法成功训练能够学习复杂非线性表示的多层神经网络时，这些模型的巨大潜力显现出来。然而，1980年代的计算能力限制了这些模型的实际应用以及它们能够解决问题的复杂性，这与我们今天在部署LLM时面临的挑战相似。尽管模型的规模和所做的考虑因素非常不同，但早期在网络最小化方面的发现为数十年后模型压缩领域的重大突破铺平了道路。在本节中，我们将简要回顾剪枝研究的历史和动机，发现*非结构化*方法与*结构化*方法的相对优缺点，并为探索它们在现代LLM时代的应用做好准备。
- en: Network pruning was originally motivated by the pursuit of better model generalization
    through freezing unimportant weights at zero, somewhat akin in theory to L1/Lasso
    and L2/Ridge regularization in linear regression, though different in that weights
    are selected and hard-set to zero (pruned) after training based on an importance
    criteria rather than being coaxed towards zero mathematically by the loss function
    during training (informed readers will know that regularization can also be achieved
    in neural network training using *weight decay*).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 网络剪枝最初的动机是通过将不重要的权重冻结为零，从而追求更好的模型泛化，这在理论上有点类似于线性回归中的L1/Lasso和L2/Ridge正则化，尽管不同之处在于，剪枝是在训练后根据重要性标准选择并硬性设置为零（修剪），而不是通过训练中的损失函数将权重数学上推向零（有经验的读者会知道，正则化也可以通过*权重衰减*在神经网络训练中实现）。
- en: The common motivation behind both regularization and pruning (which can be seen
    as a form of regularization) is the theoretical and empirical evidence that neural
    networks are most effective at learning when overparameterized thanks to a [higher-dimensional
    manifold of the loss function’s global minima](https://arxiv.org/abs/1804.10200)
    and a larger exploration space in which effective subnetworks are more likely
    to be initialized (see “[the lottery ticket hypothesis](https://arxiv.org/abs/1803.03635)”).
    However, this overparameterization in turn leads to overfitting on the training
    data, and ultimately results in a network with many [redundant](https://arxiv.org/abs/1306.0543)
    or inactive weights. Although the theoretical mechanisms underlying the “unreasonable
    effectiveness” of overparameterized neural networks were less well studied at
    the time, researchers in the 1980s correctly hypothesized that it should be possible
    to remove a large portion of the network weights after training without significantly
    affecting task performance, and that performing iterative rounds of pruning and
    fine-tuning the remaining model weights should lead to better generalization,
    enhancing the model’s ability to perform well on unseen data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化和剪枝（可以视为正则化的一种形式）背后的共同动机是理论和经验的证据表明，神经网络在过度参数化的情况下最有效地学习，因为此时损失函数全局最小值的[高维流形](https://arxiv.org/abs/1804.10200)和更大的探索空间使得有效的子网络更容易被初始化（参见“[彩票票据假设](https://arxiv.org/abs/1803.03635)”）。然而，这种过度参数化反过来又导致了对训练数据的过拟合，最终使得网络中有许多[冗余](https://arxiv.org/abs/1306.0543)或不活跃的权重。尽管当时关于过度参数化神经网络“非理性有效性”的理论机制研究较少，但1980年代的研究人员正确地假设，在训练后应该可以移除网络中大量的权重，而不会显著影响任务性能，而且通过进行迭代的剪枝和微调剩余模型权重，应该能获得更好的泛化能力，从而增强模型在未见数据上表现良好的能力。
- en: Unstructured Pruning
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '非结构化剪枝  '
- en: To select parameters for removal, a measure of their impact on the cost function,
    or “saliency,” is required. While the earliest works in network minimization worked
    under the assumption that the magnitude of parameters should serve as a suitable
    measure of their saliency, LeCun et al. made a significant step forward in 1989
    with “Optimal Brain Damage” ([OBD](https://proceedings.neurips.cc/paper/1989/hash/6c9882bbac1c7093bd25041881277658-Abstract.html)),
    in which they proposed to use a theoretically justifiable measure of saliency
    using second-derivative information of the cost function with respect to the parameters,
    allowing them to directly identify the parameters which could be removed with
    the least increase in error.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '为了选择需要移除的参数，必须衡量它们对成本函数的影响，或者称为“显著性”。尽管最早的网络最小化工作假设参数的大小应作为衡量显著性的合适标准，但LeCun等人在1989年提出的“最优大脑损伤”（[OBD](https://proceedings.neurips.cc/paper/1989/hash/6c9882bbac1c7093bd25041881277658-Abstract.html)）中，迈出了重要的一步，他们提出使用成本函数对参数的二阶导数信息作为显著性的理论可行度量，从而能够直接识别出那些能够在误差增加最小的情况下被移除的参数。  '
- en: 'Written in the era when the model of interest was a fully-connected neural
    network containing just 2,600 parameters, the authors of OBD were less concerned
    about removing weights due to computational efficiency than we are today with
    our billionaire behemoths, and were more interested in improving the model’s ability
    to generalize to unseen data by reducing model complexity. Even operating on a
    tiny model like this, however, the calculation of second-derivative information
    (Hessian matrix) is very expensive, and required the authors to make three convenient
    mathematical assumptions: 1) that the model is currently trained to an optimum,
    meaning the gradient of the loss with respect to every weight is currently zero
    and the slope of the gradient is positive in both directions, which zeroes out
    the first-order term of the Taylor expansion and implies the change in loss caused
    by pruning any parameter is positive, 2) that the Hessian matrix is diagonal,
    meaning the change in loss caused by removal of each parameter is independent,
    and therefore the loss deltas can be summed over subset of weights to calculate
    the total change in loss caused by their collective removal, and 3) that the loss
    function is nearly quadratic, meaning higher-order terms can be neglected from
    the [Taylor expansion](https://www.khanacademy.org/math/ap-calculus-bc/bc-series-new/bc-10-11/v/maclaurin-and-taylor-series-intuition).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 写于模型仅包含2,600个参数的完全连接神经网络时代，《OBD》的作者们当时更关注的是通过减少模型复杂性来提高模型对未见数据的泛化能力，而不是像今天在面对数十亿参数的庞大模型时，主要关注的是去除权重以提高计算效率。即便是对这样一个微小的模型进行操作，计算二阶导数信息（Hessian矩阵）仍然是非常昂贵的，这要求作者做出三个方便的数学假设：1）假设模型当前已训练至最优，即每个权重的损失梯度目前为零，并且梯度的斜率在两个方向上均为正，这使得泰勒展开中的一阶项为零，并且意味着修剪任何参数引起的损失变化是正的；2）假设Hessian矩阵是对角矩阵，即每个参数删除引起的损失变化是独立的，因此损失增量可以在权重子集上求和，从而计算它们共同删除所导致的总损失变化；3）假设损失函数几乎是二次的，即可以忽略泰勒展开中的高阶项。[泰勒展开](https://www.khanacademy.org/math/ap-calculus-bc/bc-series-new/bc-10-11/v/maclaurin-and-taylor-series-intuition)。
- en: '![](../Images/8422e5c3bae38ec0d439b39f58d75a72.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8422e5c3bae38ec0d439b39f58d75a72.png)'
- en: Results from [OBD](https://proceedings.neurips.cc/paper/1989/hash/6c9882bbac1c7093bd25041881277658-Abstract.html)
    are superior to magnitude-based pruning (left). Accuracy of OBD saliency estimation
    (right).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[OBD](https://proceedings.neurips.cc/paper/1989/hash/6c9882bbac1c7093bd25041881277658-Abstract.html)的结果优于基于幅度的剪枝（左）。OBD显著性估计的准确性（右）。'
- en: Despite this requisite list of naïve assumptions, their theoretically justified
    closed-form saliency metric proved itself superior to magnitude-based pruning
    in accurately identifying the least important weights in a network, able to retain
    more accuracy at higher rates of compression. Nevertheless, the efficacy and profound
    simplicity of magnitude-based pruning methods would make them the top choice for
    many future research endeavors in model compression, particularly as network sizes
    began to scale quickly, and Hessians became exponentially more frightening. Still,
    this successful demonstration of using a theoretically justified saliency measure
    to more accurately estimate saliency and thereby enable more aggressive pruning
    provided an inspirational recipe for future victories in model compression, although
    it would be some time before those seeds bore fruit.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这些天真的假设条件，它们理论上合理的封闭形式显著性度量方法证明在准确识别网络中最不重要的权重方面优于基于幅度的剪枝，能够在更高的压缩率下保持更多的准确性。然而，基于幅度的剪枝方法的有效性和深刻的简洁性使其成为未来模型压缩研究中许多工作的首选，尤其是当网络规模迅速增长，Hessian矩阵变得指数级增长时。尽管如此，使用理论上合理的显著性度量来更准确地估计显著性，从而实现更激进的剪枝的成功示范，提供了未来模型压缩研究的灵感食谱，尽管这些种子要经过一段时间才能结出果实。
- en: '![](../Images/cade3f82744602e3474c8e935cf3b1de.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cade3f82744602e3474c8e935cf3b1de.png)'
- en: Results from [OBD](https://proceedings.neurips.cc/paper/1989/hash/6c9882bbac1c7093bd25041881277658-Abstract.html)
    show that repeated iterations of pruning and fine-tuning preserve original performance
    levels even down to less than half the original parameter count. The implications
    in the context of today’s world of large models is clear, but they were more interested
    in boosting model generalization.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[OBD](https://proceedings.neurips.cc/paper/1989/hash/6c9882bbac1c7093bd25041881277658-Abstract.html)的结果表明，反复进行剪枝和微调可以在参数数量减少到原来的一半以下时，仍能保持原有的性能水平。这在当今大模型的背景下意义重大，但他们更关注的是提升模型的泛化能力。
- en: Four years later in 1993, Hassibi et al.’s Optimal Brain Surgeon ([OBS](https://www.researchgate.net/publication/3568764_Optimal_Brain_Surgeon_and_general_network_pruning))
    expanded on the concept of OBD and raised the levels of compression possible without
    increasing error by eschewing the diagonality assumption of OBD and instead considering
    the cross-terms within the Hessian matrix. This allowed them to determine optimal
    updates to the remaining weights based on the removal of a given parameter, simultaneously
    pruning and optimizing the model, thereby avoiding the need for a retraining phase.
    However, this meant even more complex mathematics, and OBS was thus initially
    of limited utility to 21st Century researchers working with much larger networks.
    Nonetheless, like OBD, OBS would eventually see its legacy revived in future milestones,
    as we will see later.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 四年后的1993年，Hassibi等人提出的最优脑外科医生（[OBS](https://www.researchgate.net/publication/3568764_Optimal_Brain_Surgeon_and_general_network_pruning)）在OBD的基础上扩展了这一概念，并通过摒弃OBD中的对角性假设，改为考虑Hessian矩阵中的交叉项，提升了在不增加误差的情况下可能的压缩水平。这使得他们能够根据去除给定参数来确定剩余权重的最优更新，从而同时进行剪枝和优化模型，避免了重新训练的需求。然而，这也意味着更复杂的数学，因而OBS最初对于21世纪研究人员在处理更大规模网络时的实用性有限。尽管如此，像OBD一样，OBS最终也会在未来的里程碑中复兴其遗产，正如我们稍后将看到的那样。
- en: The pruning methods in OBD and OBS are examples of *unstructured pruning*, wherein
    weights are pruned on an individual basis based on a measure of their saliency.
    A modern exemplar of unstructured pruning techniques is [Han et al. 2015](https://arxiv.org/abs/1506.02626),
    which reduced the sizes of the early workhorse convolutional neural networks (CNNs)
    [AlexNet](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
    and [VGG-16](https://arxiv.org/abs/1409.1556) by 9x and 13x, respectively, with
    no loss in accuracy, using one or more rounds of magnitude-based weight pruning
    and fine-tuning. Their method unfortunately requires performing sensitivity analysis
    of the network layers to determine the best pruning rate to use for each individual
    layer, and works best when retrained at least once, which means it would not scale
    well to extremely large networks. Nevertheless, it is impressive to see the levels
    of pruning which can be accomplished using their unstructured approach, especially
    since they are using magnitude-based pruning. As with any unstructured approach,
    the reduced memory footprint can only be realized by using sparse matrix storage
    techniques which avoid storing the zeroed parameters in dense matrices. Although
    they do not employ it in their study, the authors mention in their related work
    section that the hashing trick (as demonstrated in the 2015 [HashedNets](https://arxiv.org/abs/1504.04788)
    paper) is complementary to unstructured pruning, as increasing sparsity decreases
    the number of unique weights in the network, thereby reducing the probability
    of hash collisions, which leads to lower storage demands and more efficient weight
    retrieval by the hashing function.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: OBD 和 OBS 中的修剪方法是*无结构修剪*的典型例子，其中根据权重的显著性度量，对每个权重进行单独修剪。无结构修剪技术的现代典范是[Han et
    al. 2015](https://arxiv.org/abs/1506.02626)，他们通过一次或多次基于幅度的权重修剪和微调，分别将早期的工作马卷积神经网络（CNN）[AlexNet](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)和[VGG-16](https://arxiv.org/abs/1409.1556)的大小分别减少了9倍和13倍，且没有损失精度。遗憾的是，他们的方法需要对网络层进行灵敏度分析，以确定每个单独层的最佳修剪率，并且最好在至少重新训练一次后使用，这意味着它不适合用于极大型网络。尽管如此，看到他们使用无结构方法所能实现的修剪程度仍然令人印象深刻，尤其是他们使用的是基于幅度的修剪。与任何无结构方法一样，减少的内存占用只能通过使用稀疏矩阵存储技术来实现，这样就可以避免将零化的参数存储在密集矩阵中。尽管他们在研究中没有使用，但作者在相关工作部分提到，哈希技巧（如2015年[HashedNets](https://arxiv.org/abs/1504.04788)论文中展示的）与无结构修剪是互补的，因为增加稀疏性减少了网络中唯一权重的数量，从而降低了哈希碰撞的概率，进而减少存储需求并提高哈希函数的权重检索效率。
- en: '![](../Images/0418d95168c6cfaede32dfa609d56390.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0418d95168c6cfaede32dfa609d56390.png)'
- en: Results from [Han et al. 2015](https://arxiv.org/abs/1506.02626) show the power
    of unstructured pruning in CNNs of the time period. Notice the “free lunch” compression
    of 40–50% of parameters pruned away with no accuracy loss and no retraining.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[Han et al. 2015](https://arxiv.org/abs/1506.02626)的结果展示了当时CNN中无结构修剪的强大能力。注意到“免费午餐”压缩了40-50%的参数，而没有损失精度，也不需要重新训练。'
- en: While unstructured pruning has the intended regularization effect of improved
    generalization through reduced model complexity, and the memory footprint can
    then be shrunk substantially by using sparse matrix storage methods, the gains
    in computational efficiency offered by this type of pruning are not so readily
    accessed. Simply zeroing out individual weights without consideration of the network
    architecture will create matrices with irregular sparsity that will realize no
    efficiency gains when computed using dense matrix calculations on standard hardware.
    Only specialized hardware which is explicitly designed to exploit sparsity in
    matrix operations can unlock the computational efficiency gains offered by unstructured
    pruning. Fortunately, consumer hardware with these capabilities is [becoming more
    mainstream](https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/),
    enabling their users to actualize performance gains from the sparse matrices created
    from unstructured pruning. However, even these specialized hardware units must
    impose a sparsity ratio expectation on the number of weights in each matrix row
    which should be pruned in order to allow for the algorithmic exploitation of the
    resulting sparsity, known as *semi-structured pruning*, and enforcing this constraint
    has been shown to degrade performance more than purely unstructured pruning.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管无结构剪枝通过减少模型复杂度实现了预期的正则化效果，进而通过使用稀疏矩阵存储方法大幅缩小了内存占用，但这种剪枝方式在计算效率上的提升并不容易直接获得。单纯地将个别权重置零而不考虑网络架构，会导致生成的矩阵稀疏性不规则，在使用标准硬件进行密集矩阵计算时，无法实现计算效率的提升。只有专门设计用于利用矩阵运算稀疏性的硬件，才能解锁无结构剪枝所带来的计算效率提升。幸运的是，具有这些能力的消费级硬件正在[变得越来越普及](https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/)，使得用户能够从无结构剪枝所生成的稀疏矩阵中实现性能提升。然而，即便是这些专用硬件，也必须对每个矩阵行中应被剪枝的权重数量施加稀疏性比例的预期，以便允许算法利用结果中的稀疏性，这被称为*半结构化剪枝*，而强制这一约束已被证明比纯粹的无结构剪枝更容易导致性能下降。
- en: Structured Pruning
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结构化剪枝
- en: We’ve seen that unstructured pruning is a well-established regularization technique
    that is known to improve model generalization, reduce memory requirements, and
    offer efficiency gains on specialized hardware. However, the more tangible benefits
    to computational efficiency are offered by *structured pruning*, which entails
    removing entire structural components (filters, layers) from the network rather
    than individual weights, which reduces the complexity of the network in ways that
    align with how computations are performed on hardware, allowing for gains in computational
    efficiency to be easily realized without specialized kit.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，无结构剪枝是一种已被证明能够改善模型泛化、减少内存需求并在专用硬件上提供效率提升的成熟正则化技术。然而，*结构化剪枝*提供的计算效率提升更为显著，这种方法通过从网络中移除整个结构组件（如滤波器、层），而非单个权重，从而减少了网络的复杂度，这种复杂度的降低与硬件上计算的执行方式相一致，使得计算效率的提升可以在无需专用硬件的情况下轻松实现。
- en: A formative work in popularizing the concept of structured pruning for model
    compression was the 2016 Li et al. paper “[Pruning Filters for Efficient ConvNets](https://arxiv.org/abs/1608.08710),”
    where, as the title suggests, the authors pruned filters and their associated
    feature maps from CNNs in order to greatly improve computational efficiency, as
    the calculations surrounding these filters can be easily excluded by physically
    removing the chosen kernels from the model, directly reducing the size of the
    matrices and their multiplication operations without needing to worry about exploiting
    sparsity. The authors used a simple sum of filter weights (L1 norm) for magnitude-based
    pruning of the filters, demonstrating that their method could reduce inferences
    costs of VGG-16 and [ResNet-110](https://arxiv.org/abs/1512.03385) by 34% and
    38%, respectively, without significant degradation of accuracy.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 2016年，Li等人发表的论文《[高效卷积神经网络的滤波器剪枝](https://arxiv.org/abs/1608.08710)》为普及结构化剪枝的概念做出了重要贡献。正如标题所示，作者通过剪枝卷积神经网络（CNN）中的滤波器及其相关特征图来大幅提升计算效率，因为围绕这些滤波器的计算可以通过物理去除选择的内核直接排除，从而减少矩阵的大小及其乘法运算，无需担心利用稀疏性。作者使用滤波器权重的简单和（L1范数）来进行基于大小的剪枝，展示了他们的方法可以分别将VGG-16和[ResNet-110](https://arxiv.org/abs/1512.03385)的推理成本降低34%和38%，且没有显著降低准确率。
- en: '![](../Images/e5b14bea61ffe97a84ad1fe9ea848816.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e5b14bea61ffe97a84ad1fe9ea848816.png)'
- en: '[Li et al. 2016](https://arxiv.org/abs/1608.08710) shows the effect of pruning
    convolutional filters from a CNN.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[Li et al. 2016](https://arxiv.org/abs/1608.08710)展示了从CNN中剪除卷积滤波器的效果。'
- en: Their study also reveals some fascinating insights about how convolutional networks
    work by comparing the sensitivity of individual CNN layers to pruning, revealing
    that layers on the very beginning or past halfway through the depth of the network
    were able to be pruned aggressively with almost no impact on the model performance,
    but that layers around 1/4 of the way into the network were very sensitive to
    pruning and doing so made recovering model performance difficult, even with retraining.
    The results, shown below, reveal that the layers which are most sensitive to pruning
    are those containing many filters with large absolute sums, supporting the theory
    of magnitude as a saliency measure, as these layers are clearly more important
    to the network, since pruning them away causes pronounced negative impact on model
    performance which is difficult to recover.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的研究还揭示了一些关于卷积网络工作原理的有趣见解，通过比较各个CNN层对剪枝的敏感性，发现网络最初或接近网络深度一半的层可以被大幅度剪枝，几乎不会影响模型性能，而在网络深度约1/4处的层则对剪枝非常敏感，剪枝后即使重新训练也难以恢复模型性能。下图显示的结果表明，最对剪枝敏感的层是那些包含大量具有较大绝对和的滤波器的层，支持了“幅度”作为显著性度量的理论，因为这些层显然对网络更为重要，剪去这些层会对模型性能造成明显的负面影响，且难以恢复。
- en: '![](../Images/c1acdb658016554f31de6bb5cd0490c0.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c1acdb658016554f31de6bb5cd0490c0.png)'
- en: Results from [Li et al. 2016](https://arxiv.org/abs/1608.08710) reveal marked
    differences in the sensitivity of CNN layers to filter pruning.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[Li et al. 2016](https://arxiv.org/abs/1608.08710)的研究结果揭示了CNN层对滤波器剪枝的敏感性存在显著差异。'
- en: Most importantly, the results from Li et al. show that many layers in a CNN
    could be pruned of even up to 90% of their filters without harming (and in some
    cases even improving) model performance. Additionally, they found that when pruning
    filters from the insensitive layers, iterative retraining layer-by-layer was unnecessary,
    and a single round of pruning and retraining (for 1/4 of the original training
    time) was all that was required to recover model performance after pruning away
    significant portions of the network. This is great news in terms of efficiency,
    since multiple rounds of retraining can be costly, and previous work had reported
    requiring up to 3x original training time to produce their pruned models. Below
    we can see the overall results from Li et al. which reveal that the number of
    floating point operations (FLOPs) could be reduced between 15 and 40 percent in
    the CNNs studied without harming performance, and in fact offering gains in many
    instances, setting a firm example of the importance of pruning models after training.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，Li et al.的结果显示，CNN中的许多层即使剪去高达90%的滤波器，也不会损害（在某些情况下甚至能改善）模型性能。此外，他们还发现，当剪去那些不敏感的层的滤波器时，不需要逐层进行迭代重训练，单次剪枝和重训练（仅用原训练时间的1/4）就足以在剪去大量网络部分后恢复模型性能。这对提高效率是个好消息，因为多轮重训练成本高昂，而之前的研究报告称，剪枝后的模型需要最多3倍的原始训练时间。下图显示了Li
    et al.的总体结果，揭示了在研究的CNN中，可以减少15%到40%的浮点运算量（FLOPs），而不损害性能，实际上在许多情况下还带来了提升，明确树立了训练后剪枝模型的重要性。
- en: '![](../Images/25f464067d2d49b748bb8f2a47f0a4f2.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/25f464067d2d49b748bb8f2a47f0a4f2.png)'
- en: Results from [Li et al. 2016](https://arxiv.org/abs/1608.08710) comparing their
    select pruning configurations to the baseline CNNs, evaluated on CIFAR-10 (top
    three models) and ImageNet (ResNet-34 section).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[Li et al. 2016](https://arxiv.org/abs/1608.08710)将他们的选择性剪枝配置与基准CNN进行比较，评估了CIFAR-10（前三个模型）和ImageNet（ResNet-34部分）。'
- en: 'Although this study was clearly motivated by efficiency concerns, we know from
    decades of evidence linking reduced model complexity to improved generalization
    that these networks should perform better on unseen data as well, a fundamental
    advantage which motivated pruning research in the first place. However, this pruning
    method requires a sensitivity analysis of the network layers in order to be done
    correctly, requiring additional effort and computation. Further, as LeCun and
    his colleagues correctly pointed out back in 1989: although magnitude-based pruning
    is a time-tested strategy, we should expect a theoretically justified metric of
    salience to produce a superior pruning strategy, but with the size of modern neural
    networks, computing the Hessian matrix required for the second-order Taylor expansions
    used in their OBD method would be too intensive. Fortunately, a happy medium was
    forthcoming.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这项研究显然是出于效率考虑，但我们从数十年的证据中知道，简化模型复杂度与提高泛化能力之间的关联性，这些网络在未见过的数据上表现得更好，这也是最初推动剪枝研究的根本原因。然而，这种剪枝方法需要对网络层进行敏感性分析，以便正确执行，这需要额外的努力和计算。此外，正如LeCun及其同事在1989年正确指出的那样：尽管基于幅度的剪枝是一种经过时间考验的策略，但我们应该期望一个理论上有依据的显著性度量能够产生更优的剪枝策略，但考虑到现代神经网络的规模，计算Hessian矩阵以进行OBD方法中所需的二阶泰勒展开将变得过于繁重。幸运的是，最终出现了一个折中的方法。
- en: Trailing Li et al. by only a few months in late 2016, [Molchanov and his colleagues
    at Nvidia](https://arxiv.org/abs/1611.06440) reinvestigated the use of Taylor
    expansion to quantify salience for structured pruning of filters from CNNs. In
    contrast to OBD, they avoid the complex calculation of the second-order terms,
    and instead extract a useful measure of saliency by considering the variance rather
    than the mean of the first-order Taylor expansion term. The study provides empirical
    comparison of several saliency measures against an “oracle” ranking which was
    computed by exhaustively calculating the change in loss caused by removing each
    filter from a fine-tuned VGG-16\. In the results shown below, we can see that
    the proposed Taylor expansion saliency measure most closely correlates with the
    oracle rankings, followed in second place by the more computationally intensive
    OBD, and the performance results reflect that these methods are also best at preserving
    accuracy, with the advantage more clearly in favor of the proposed Taylor expansion
    method when plotting over GFLOPs. Interestingly, the inclusion of random filter
    pruning in their study shows us that it performs surprisingly well compared to
    minimum weight (magnitude-based) pruning, challenging the notion that weight magnitude
    is a reliable measure of saliency, at least for the CNN architectures studied.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在2016年末，仅比Li等人晚几个月，[Molchanov及其在Nvidia的同事们](https://arxiv.org/abs/1611.06440)重新研究了使用泰勒展开来量化卷积神经网络（CNN）中滤波器结构剪枝的显著性。与OBD方法不同，他们避免了计算二阶项的复杂过程，而是通过考虑一阶泰勒展开项的方差而非均值来提取有用的显著性度量。这项研究提供了几种显著性度量与通过全面计算去除每个滤波器对精调后的VGG-16模型损失变化所得到的“oracle”排名之间的实证对比。从下面的结果可以看出，所提出的泰勒展开显著性度量与oracle排名最为一致，其次是计算量更大的OBD方法，性能结果也表明这些方法在保持准确性方面表现最佳，当我们根据GFLOPs绘制结果时，所提出的泰勒展开方法在优势上更为明显。有趣的是，他们研究中引入的随机滤波器剪枝表现出令人惊讶的效果，相较于最小权重（基于幅度的）剪枝，挑战了权重幅度是显著性可靠度量这一观念，至少对于他们研究的CNN架构而言。
- en: '![](../Images/cf21399bc3d12c2a9c942efa73372998.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf21399bc3d12c2a9c942efa73372998.png)'
- en: Results from [Molchanov et al. 2016](https://arxiv.org/abs/1611.06440) show
    first-order Taylor expansion providing effective measure of filter saliency, representing
    the highest correlations with oracle ranking and best preservation of accuracy.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[Molchanov等人2016](https://arxiv.org/abs/1611.06440)的结果表明，使用一阶泰勒展开可以有效地衡量滤波器显著性，呈现出与oracle排名的最高相关性，并且能够最好地保持准确性。'
- en: Pruning LLMs
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 剪枝LLM
- en: '![](../Images/5f7619db0965723a96214dfe3fec7320.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5f7619db0965723a96214dfe3fec7320.png)'
- en: Image by author using DALL-E 3.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用DALL-E 3生成。
- en: After the widespread adoption of LLMs, researchers naturally moved to investigate
    the use of pruning on these architectures. Both unstructured and structured pruning
    can be successfully applied to LLMs to reduce their model size substantially with
    negligible drops in performance. As one might expect, however, the enormous size
    of these models requires special considerations to be made, since calculating
    saliency measures over models containing tens or hundreds of billions of weights
    is very costly, and retraining to recover model performance after pruning is prohibitively
    expensive. Thus, there is newfound motivation to perform pruning with as little
    retraining as possible, and to enforce simplicity in the saliency measures used
    for pruning.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模语言模型（LLMs）被广泛采用之后，研究人员自然开始研究在这些架构上使用剪枝技术。无论是无结构剪枝还是有结构剪枝，都可以成功地应用于LLMs，显著减少模型大小，同时性能几乎没有下降。然而，正如人们所预料的，这些模型的庞大规模需要特别的考虑，因为计算包含数十亿甚至数百亿权重的模型的显著性度量非常昂贵，而剪枝后重新训练以恢复模型性能的成本则高得令人难以承受。因此，现如今有了新的动机，要求在尽可能少的重新训练下执行剪枝，并在用于剪枝的显著性度量中强制要求简洁性。
- en: Consistent with the previous eras of pruning research, it is apparent that LLMs
    can be pruned far more aggressively using unstructured as opposed to structured
    methods, but again the efficiency gains are more directly accessible with the
    latter. For practitioners with better access to specialized resources, exploiting
    the sparse matrices and massive compression rates provided by unstructured pruning
    may be the right choice, but for many people, the accessible efficiency gains
    on general hardware offered by structured pruning will be more appealing, despite
    the more modest levels of compression. In this section, we will review both approaches
    in today’s LLM landscape, so that we are equipped to make the best choice given
    our individual circumstances.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的剪枝研究阶段一致，显然LLMs可以通过无结构方法比有结构方法更为激进地进行剪枝，但同样，后者的效率提升更加直接易得。对于那些可以更好地访问专业资源的实践者来说，利用无结构剪枝所提供的稀疏矩阵和巨大的压缩率可能是正确的选择，但对于许多人来说，尽管有结构剪枝提供的压缩率较为温和，但它在一般硬件上提供的效率提升会更具吸引力。在本节中，我们将回顾今天LLMs领域的这两种方法，以便我们根据个人的具体情况做出最佳选择。
- en: Unstructured Pruning of LLMs
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLMs的无结构剪枝
- en: In early 2023, [SparseGPT](https://arxiv.org/abs/2301.00774) was the first work
    to investigate unstructured pruning of GPT models with billions of parameters,
    proposing an efficient method using a novel approximate sparse regression solver
    to determine the prunable weights in models of this scale within a matter of hours,
    and demonstrating that the largest open source models of the day (≤175B) could
    be pruned to between 50% and 60% sparsity with minimal loss of accuracy in one
    shot without any retraining at all, significantly exceeding the results offered
    by magnitude-based approaches in the one-shot setting. Their approach takes an
    iterative perspective on OBS, finding that the same mathematical result can be
    broken down into a series of operations which are more efficient to compute. However,
    since their method is still an example of unstructured pruning, specialized hardware
    is necessary for realizing efficiency gains from their technique, and enforcing
    the required 2:4 or 4:8 semi-structured sparsity pattern expectation causes drops
    in performance compared to purely unstructured pruning.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在2023年初，[SparseGPT](https://arxiv.org/abs/2301.00774)是首个研究对具有数十亿参数的GPT模型进行无结构剪枝的工作，提出了一种高效的方法，利用一种新型的近似稀疏回归求解器来确定这种规模模型中的可剪枝权重，且能够在数小时内完成计算，并且展示了当时最大的开源模型（≤175B）可以在不进行任何重新训练的情况下，单次剪枝至50%到60%的稀疏度，且准确度几乎没有损失，这一结果显著超过了基于幅度的剪枝方法在单次操作中的效果。他们的方法采用了迭代式的OBS视角，发现同样的数学结果可以分解为一系列更加高效计算的操作。然而，由于他们的方法仍然属于无结构剪枝，因此需要专用硬件来实现该技术的效率提升，而强制要求的2:4或4:8半结构稀疏模式则会导致与纯无结构剪枝相比的性能下降。
- en: '![](../Images/ee5766a643b4513311f2f9384c9146b0.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee5766a643b4513311f2f9384c9146b0.png)'
- en: Results from [SparseGPT](https://arxiv.org/abs/2301.00774) show clear advantage
    over magnitude based pruning (left), and demonstrate the detrimental effects of
    enforcing sparsity patterns to enable hardware optimization (right).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[SparseGPT](https://arxiv.org/abs/2301.00774)的结果显示，相较于基于幅度的剪枝（左图），该方法具有明显优势，并展示了强制稀疏模式以便进行硬件优化（右图）所带来的负面影响。
- en: Later in mid-2023, the authors of [Wanda](https://arxiv.org/abs/2306.11695)
    postulated about why quantization had seen so much more research interest than
    pruning in the LLM era, whereas previously the two compression techniques were
    equally popular. They attributed this to the fact that up until SparseGPT, all
    pruning methods required retraining the LLM at some point, making them cost-prohibitive
    to anyone without the resources to do so, creating a significant deterrent to
    both research and practical adoption. While SparseGPT showed that one-shot pruning
    was possible, their iterative OBS approach is still quite computationally intensive.
    In this light, Wanda opts for a simple magnitude-based unstructured pruning method,
    which they augment by multiplying the weight magnitudes by the norm of their associated
    input activations creating a more descriptive and wholistic magnitude-based measure
    of saliency. The comparison chart below shows the saliency formulations and complexities
    of these unstructured approaches.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在2023年中期，[Wanda](https://arxiv.org/abs/2306.11695)的作者提出了一个假设，解释了为什么量化在LLM时代比剪枝受到更多的研究关注，而以前这两种压缩技术曾经同样受欢迎。他们将这一现象归因于，在SparseGPT出现之前，所有的剪枝方法都需要在某个时刻重新训练LLM，这使得没有足够资源的人难以负担，成为研究和实际应用的重大障碍。尽管SparseGPT表明一次性剪枝是可能的，但他们的迭代OBS方法仍然计算密集。因此，Wanda选择了一种简单的基于大小的无结构剪枝方法，通过将权重大小与其相关输入激活的范数相乘，创建了一个更具描述性和整体性的基于大小的重要性度量。下面的比较图表展示了这些无结构方法的重要性公式和复杂度。
- en: '![](../Images/851115ffa10405e0ffdf05de08c14ddc.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/851115ffa10405e0ffdf05de08c14ddc.png)'
- en: Table from [Sun et al. 2023](https://arxiv.org/abs/2306.11695) compares unstructured
    pruning approaches for LLMs and their complexity.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[Sun et al. 2023](https://arxiv.org/abs/2306.11695)的表格比较了大规模语言模型（LLMs）的无结构剪枝方法及其复杂度。
- en: Wanda’s approach also produces pruned models that are ready to use without any
    retraining, but as an unstructured approach, again requires special hardware for
    efficiency gains. Nevertheless, for those equipped to take advantage of unstructured
    pruning, Wanda’s approach matches or exceeds the results of SparseGPT while reducing
    complexity by an entire factor of the model’s hidden dimension, establishing it
    as a strong choice for the compression of LLMs.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Wanda的方法也能生成无需任何重新训练即可使用的剪枝模型，但作为一种无结构方法，依然需要特殊硬件来实现效率提升。尽管如此，对于那些具备条件进行无结构剪枝的用户来说，Wanda的方法与SparseGPT的结果相当或更好，同时将复杂度降低到模型隐藏层维度的一个因子，确立了它作为LLM压缩的有力选择。
- en: '![](../Images/7a3ad35c80394d85378d8469b274cc8a.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a3ad35c80394d85378d8469b274cc8a.png)'
- en: Table from [Sun et al. 2023](https://arxiv.org/abs/2306.11695) shows competitive
    performance with SparseGPT with a fraction of the complexity.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[Sun et al. 2023](https://arxiv.org/abs/2306.11695)的表格显示，结构化剪枝在复杂度较低的情况下，与SparseGPT具有竞争力的表现。
- en: Structured Pruning of LLMs
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM的结构化剪枝
- en: Contemporaneously with Wanda, researchers at the National University of Singapore
    offered a structured pruning method called [LLM-Pruner](https://arxiv.org/abs/2305.11627).
    In their study, they found it necessary to settle for a 20% pruning rate, since
    more aggressive pruning led to substantially degraded performance. Also, while
    it was necessary to retrain the weights after pruning to recover model performance,
    they were able to achieve this using low-rank adaptation ([LoRA](https://arxiv.org/abs/2106.09685))
    in just 3 hours on 50k training samples. Although the efficiency of fine-tuning
    with LoRA is a relief, their method nonetheless requires gradients for the full
    model to be generated to measure parameter saliency before pruning, so while resource-constrained
    users may enjoy the pruned model, performing the operation themselves may not
    be possible.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 与Wanda同时，新加坡国立大学的研究人员提出了一种名为[LLM-Pruner](https://arxiv.org/abs/2305.11627)的结构化剪枝方法。在他们的研究中，他们发现有必要设定20%的剪枝率，因为更激进的剪枝会导致性能显著下降。此外，尽管在剪枝后需要重新训练权重以恢复模型性能，但他们能够使用低秩适应（[LoRA](https://arxiv.org/abs/2106.09685)）在50k训练样本上仅用3小时就完成这一过程。尽管使用LoRA进行微调的高效性令人宽慰，但他们的方法仍然需要在剪枝前生成全模型的梯度以衡量参数的重要性，因此，虽然资源受限的用户可以享受剪枝后的模型，但自己执行该操作可能并不可行。
- en: Just slightly later in 2023, [LoRAPrune](https://arxiv.org/abs/2305.18403) improved
    on the effectiveness for structured pruning of LLMs substantially by using the
    gradients and weights of LoRA training to establish parameter importance in the
    larger network, and performing iterative rounds of pruning on both the network
    and corresponding LoRA weights. This method is able to prune the LLaMA-65B model
    on a single 80GB A100 GPU, thanks to depending on the gradients of the efficient
    low-rank parameter space rather than the full model. Since the LLM weights remain
    frozen during the process, they can be quantized to 8-bit to save memory with
    minimal impact on the results.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年稍晚时，[LoRAPrune](https://arxiv.org/abs/2305.18403)通过使用LoRA训练的梯度和权重来确定更大网络中参数的重要性，并对网络及相应的LoRA权重进行迭代剪枝，显著提高了LLM的结构化剪枝效果。这种方法能够在单个80GB
    A100 GPU上剪枝LLaMA-65B模型，因为它依赖于高效低秩参数空间的梯度，而非完整模型。由于在整个过程中LLM的权重保持冻结状态，因此可以将其量化为8位，以节省内存，并对结果的影响最小。
- en: '![](../Images/6f323a2d614994c41f5a0c613dc64582.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f323a2d614994c41f5a0c613dc64582.png)'
- en: Helpful graphic from [Zhang et al. 2023](https://arxiv.org/abs/2305.18403) depicts
    the structured pruning method used in LoRAPrune.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[Zhang et al. 2023](https://arxiv.org/abs/2305.18403)的有用图示展示了LoRAPrune中使用的结构化剪枝方法。'
- en: Although they came up against the same sensitivity of the LLM to more aggressive
    levels of structured pruning, the authors of LoRAPrune demonstrate through extensive
    experimentation that their method produced pruned models with superior performance
    compared to previous structured methods using only a fraction of the resources
    to perform the pruning operation.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管他们面临着LLM对更激进的结构化剪枝水平的敏感性，LoRAPrune的作者通过大量实验展示了他们的方法，在使用远少于传统剪枝资源的情况下，产生了性能更优的修剪模型。
- en: '![](../Images/c354234a8a204088dac00f5e3bc0bbff.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c354234a8a204088dac00f5e3bc0bbff.png)'
- en: Results from [LoRAPrune](https://arxiv.org/abs/2305.18403) demonstrate a clear
    advantage after fine-tuning compared with previous methods.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[LoRAPrune](https://arxiv.org/abs/2305.18403)的结果表明，与之前的方法相比，微调后显示出明显的优势。
- en: In October of 2023, researchers at Microsoft proposed [LoRAShear](https://arxiv.org/abs/2310.18356),
    a structured pruning method which uses Dependency Graph Analysis on the LLM and
    progressive structured pruning via LoRA Half-Space Projected Gradient (LHSPG),
    which “transfers the knowledge stored in the relatively redundant structures to
    the important structures to better preserve the knowledge of the pretrained LLM.”
    Additionally, they go beyond the trend in previous works of performing only instruction-based
    fine-tuning to recover knowledge, and instead first adaptively create a subset
    from the pretraining datasets based on the resulting performance distribution
    to recover the general pretraining knowledge that was lost during pruning, and
    then proceeding to “perform the usual instructed finetuning to recover domain-specific
    expertise and the instruction capacity of pruned LLMs.” With their more involved
    approach, they achieve a mere 1% drop in performance at the 20% pruning rate,
    and maintain an unprecedented 82% of original performance at the 50% pruning rate.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在2023年10月，微软的研究人员提出了[LoRAShear](https://arxiv.org/abs/2310.18356)，这是一种结构化剪枝方法，通过对LLM进行依赖图分析和通过LoRA半空间投影梯度（LHSPG）进行渐进式结构化剪枝，“将存储在相对冗余结构中的知识转移到重要结构中，以更好地保留预训练LLM的知识。”此外，他们超越了以往仅通过指令微调来恢复知识的趋势，而是首先基于结果的性能分布自适应地从预训练数据集中创建一个子集，以恢复在剪枝过程中丢失的通用预训练知识，然后进行“常规的指令微调，以恢复修剪后的LLM的领域特定专业知识和指令能力。”通过他们更为复杂的方法，他们在20%的剪枝率下仅表现出1%的性能下降，并在50%的剪枝率下保持了前所未有的82%的原始性能。
- en: '![](../Images/2457542d271dee6ac816b97ec4f2a110.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2457542d271dee6ac816b97ec4f2a110.png)'
- en: Results from LoRAShear shows superior performance, albeit with a much more complex
    pruning algorithm.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 来自LoRAShear的结果显示了优越的性能，尽管其剪枝算法更加复杂。
- en: Then in early 2024, the aptly named [Bonsai](https://arxiv.org/abs/2402.05406)
    demonstrated a superior method for structured pruning of LLMs using only forward
    pass information, drastically reducing the computational requirements for performing
    the pruning process by not requiring gradients, thereby empowering those most
    in need of pruned models to generate them within their resource-constrained environments.
    With their efficient approach, they are able to closely match the performance
    of LoRAShear in the instruction-tuned only condition, although it would appear
    the additional considerations made by LoRAShear do pay dividends in knowledge
    recovery, but the differing spreads of evaluation datasets used in the two studies
    unfortunately disallow for clear comparison. Interestingly, LoRAShear is unmentioned
    in the Bonsai paper, presumably for the reason that the additional levels of complexity
    in the former make for a muddy comparison with the more straightforward methods
    examined by the latter, but we are left to speculate. Nevertheless, Bonsai contributes
    a powerful and valuable step towards democratizing LLMs and their pruning by focusing
    on simplicity and efficiency, able to perform the pruning operation using only
    the amount of GPU memory needed to run inference for a given model, and achieves
    impressive results with the most accessible method of structured LLM pruning published
    so far.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在2024年初，名为[Bonsai](https://arxiv.org/abs/2402.05406)的方法展示了一种优越的LLM结构化剪枝方法，该方法仅使用前向传递信息，显著减少了剪枝过程的计算需求，因为不需要梯度计算，从而使那些最需要剪枝模型的用户能够在资源受限的环境中生成它们。通过这种高效的方法，他们能够在仅限于指令调优的条件下，接近地匹配LoRAShear的性能，尽管看起来LoRAShear所做的额外考虑确实有助于知识恢复，但两项研究中使用的评估数据集的不同分布不幸使得无法进行清晰的比较。有趣的是，Bonsai论文中没有提及LoRAShear，推测原因是前者的复杂性使得与后者所考察的更简单方法的比较变得不清晰，但我们只能进行推测。尽管如此，Bonsai通过专注于简单性和效率，做出了向民主化LLM及其剪枝迈出的强大而宝贵的一步，能够仅使用运行给定模型推理所需的GPU内存量来执行剪枝操作，并且实现了迄今为止发表的最易于访问的结构化LLM剪枝方法，取得了令人印象深刻的结果。
- en: '![](../Images/ee2bb241e10ac109fbc70f90e45a5114.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee2bb241e10ac109fbc70f90e45a5114.png)'
- en: Results from [Dery et al.](https://arxiv.org/abs/2402.05406) show Bonsai achieves
    superior performance to previous structured pruning methods.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[Dery 等人](https://arxiv.org/abs/2402.05406)的结果表明，Bonsai在性能上优于先前的结构化剪枝方法。'
- en: Conclusion
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: '![](../Images/d51a02f57e1aebfb01cd6d23bac8b10b.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d51a02f57e1aebfb01cd6d23bac8b10b.png)'
- en: Image by author using DALL-E 3.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用DALL-E 3制作的图片。
- en: In this article, we’ve journeyed through the history of network pruning, starting
    with the dawn of unstructured techniques in the late 1980s to the current trends
    in LLM applications. We’ve seen that pruning is broadly categorized as either
    *unstructured* or *structured* pruning, depending on whether the weights are considered
    individually or in groups, and that the latter, while only usable at lower compression
    rates, provides direct relief in computational burden. We saw that gains in efficiency
    can be realized in the unstructured setting, but only when special storage techniques
    and hardware are used, and that an additional “semi-structured” condition must
    be obeyed for the hardware acceleration to work, which comes at a cost in performance
    compared with pure unstructured pruning. Pure unstructured pruning provides the
    most stunning compression rates with no loss in accuracy, but the irregular sparsity
    created does not provide efficiency gains outside of storage size, making it less
    appealing in the context of democratizing LLMs.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们回顾了网络剪枝的历史，从1980年代末期的无结构剪枝技术的黎明到当前LLM应用的趋势。我们看到，剪枝大致可以分为*无结构*剪枝和*结构化*剪枝，这取决于权重是单独考虑还是按组考虑，而后者虽然只能在较低压缩率下使用，但能够直接减轻计算负担。我们看到，在无结构剪枝的环境中，通过使用特殊的存储技术和硬件可以实现效率提升，但必须遵守一个额外的“半结构化”条件，才能使硬件加速生效，这在性能上会带来相较纯无结构剪枝的损失。纯无结构剪枝提供了最惊人的压缩率，并且不损失精度，但所产生的不规则稀疏性在存储大小之外并未提供效率提升，这使得它在推动LLM民主化的背景下吸引力较小。
- en: We’ve explored the concept of saliency, which refers to the various measures
    of importance (saliency) by which model parameters can be pruned. The most simple
    and accessible estimation of saliency is weight magnitude, where weights closer
    to zero are assumed to be less important. Although this approach is not theoretically
    sound (as near-zero weights can indeed be important to model performance), it
    is still extremely effective, and the lack of complex calculations gives it persisting
    popularity. On the other hand, theoretically sound measures of saliency date back
    to the earliest days of trainable neural networks, and are proven to produce superior
    models compared to magnitude-based pruning, but the complex calculations required
    by these early methods don’t scale well to the size of today’s LLMs. Fortunately,
    motivated researchers in the modern era have found ways to calculate these saliency
    metrics more efficiently, but alas, they still require the calculation of the
    gradients. In the most recent work from 2024, Bonsai demonstrated that accurate
    pruning can be achieved without gradients, using only the information from the
    forward pass.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了显著性（saliency）概念，显著性指的是用于剪枝模型参数的重要性度量。最简单且易于理解的显著性估计是权重幅度，其中接近零的权重被认为不那么重要。尽管这种方法在理论上并不严谨（因为接近零的权重可能对模型性能至关重要），但它仍然非常有效，而且由于不需要复杂的计算，它保持了持久的流行性。另一方面，理论上严谨的显著性度量可以追溯到可训练神经网络的早期发展，并且已被证明比基于幅度的剪枝方法能够生成更优的模型，但这些早期方法所需的复杂计算对于今天的LLM（大语言模型）来说难以扩展。幸运的是，现代的有志研究人员找到了更高效地计算这些显著性度量的方法，但遗憾的是，这些方法仍然需要计算梯度。在2024年最新的研究中，Bonsai展示了通过仅使用前向传递的信息，就能够实现精确的剪枝，而不需要梯度。
- en: While modern pruning research is driven primarily by the interest of compressing
    the unwieldy sizes of today’s top performing models so that they can be deployed
    on reasonably sized hardware, pruning was originally motivated by the improved
    generalizability that results from reducing model complexity. This regularization
    effect is surely taking effect in pruned LLMs, which is presumably a benefit,
    but the actual impact of this is less studied in today’s literature. While improving
    model generalizability and reducing overfitting through regularization are known
    to be beneficial, there may be special considerations which need to be made in
    the context of LLMs, which are often expected to recall minute details in vast
    sums of training data, depending on the use case. Therefore, it would be fruitful
    for future work to investigate at what point this regularization starts to have
    deleterious effects on intended use cases of LLMs.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管现代剪枝研究的主要驱动力是压缩当今顶级模型庞大体积，以便能够在合理大小的硬件上部署，但剪枝最初的动机是通过减少模型复杂性来提高模型的泛化能力。这种正则化效应无疑在剪枝后的LLM中产生了作用，这大概是一个好处，但这一点在当今的文献中研究较少。虽然通过正则化提高模型的泛化能力和减少过拟合已知是有益的，但在LLM的背景下，可能需要做出特别的考虑，因为根据使用场景，LLM常常需要记住大量训练数据中的细节。因此，未来的研究应探讨在何种情况下，这种正则化会对LLM的预期应用产生不利影响。
- en: In the next episode…
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在下一期中…
- en: The methods investigated in this article offer effective model compression through
    the removal of model parameters, known as *pruning*, but this is only one approach
    to model compression. In the next article, we will explore the concept of *quantization*
    at various resolutions, developing a functional knowledge of the subject within
    a reasonable memory footprint.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 本文探讨的方法通过去除模型参数，提供了有效的模型压缩，这一过程被称为*剪枝*，但这只是模型压缩的一种方法。在下一篇文章中，我们将探讨*量化*在不同分辨率下的概念，并在合理的内存占用下，发展该主题的实用知识。
