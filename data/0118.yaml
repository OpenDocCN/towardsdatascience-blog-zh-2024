- en: Efficient Feature Selection via CMA-ES (Covariance Matrix Adaptation Evolution
    Strategy)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过CMA-ES（协方差矩阵适应进化策略）进行高效特征选择
- en: 原文：[https://towardsdatascience.com/efficient-feature-selection-via-cma-es-covariance-matrix-adaptation-evolution-strategy-ee312bc7b173?source=collection_archive---------4-----------------------#2024-01-12](https://towardsdatascience.com/efficient-feature-selection-via-cma-es-covariance-matrix-adaptation-evolution-strategy-ee312bc7b173?source=collection_archive---------4-----------------------#2024-01-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/efficient-feature-selection-via-cma-es-covariance-matrix-adaptation-evolution-strategy-ee312bc7b173?source=collection_archive---------4-----------------------#2024-01-12](https://towardsdatascience.com/efficient-feature-selection-via-cma-es-covariance-matrix-adaptation-evolution-strategy-ee312bc7b173?source=collection_archive---------4-----------------------#2024-01-12)
- en: '*Using evolutionary algorithms for fast feature selection with large datasets*'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*使用进化算法进行大数据集的快速特征选择*'
- en: '[](https://florin-andrei.medium.com/?source=post_page---byline--ee312bc7b173--------------------------------)[![Florin
    Andrei](../Images/372ac3e80dbc03cbd20295ec1df5fa6f.png)](https://florin-andrei.medium.com/?source=post_page---byline--ee312bc7b173--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ee312bc7b173--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ee312bc7b173--------------------------------)
    [Florin Andrei](https://florin-andrei.medium.com/?source=post_page---byline--ee312bc7b173--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://florin-andrei.medium.com/?source=post_page---byline--ee312bc7b173--------------------------------)[![Florin
    Andrei](../Images/372ac3e80dbc03cbd20295ec1df5fa6f.png)](https://florin-andrei.medium.com/?source=post_page---byline--ee312bc7b173--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ee312bc7b173--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ee312bc7b173--------------------------------)
    [Florin Andrei](https://florin-andrei.medium.com/?source=post_page---byline--ee312bc7b173--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ee312bc7b173--------------------------------)
    ·11 min read·Jan 12, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ee312bc7b173--------------------------------)
    ·阅读时间11分钟·2024年1月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '*This is part 1 of a two-part series about feature selection. Read* [*part
    2 here*](/efficient-feature-selection-via-genetic-algorithms-d6d3c9aff274)*.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*这是关于特征选择的两部分系列中的第一部分。阅读* [*第二部分在这里*](/efficient-feature-selection-via-genetic-algorithms-d6d3c9aff274)*。*'
- en: '*For more examples of the applications of CMA-ES, check* [*this paper by Nomura
    and Shibata*](https://arxiv.org/abs/2402.01373)*; this article is mentioned (ref.
    [6]) in the paper as a noteworthy application of optimization via CMA-ES with
    Margin.*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*想了解更多CMA-ES应用的例子，请查阅* [*Nomura和Shibata的这篇论文*](https://arxiv.org/abs/2402.01373)*；这篇文章在论文中作为通过CMA-ES和Margin优化的一个值得注意的应用被提到（参考文献[6]）。*'
- en: 'When you’re fitting a model to a dataset, you may need to perform feature selection:
    keeping only some subset of the features to fit the model, while discarding the
    rest. This can be necessary for a variety of reasons:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将模型拟合到数据集时，可能需要进行特征选择：只保留某些特征子集来拟合模型，同时丢弃其余的特征。这可能出于多种原因是必要的：
- en: to keep the model explainable (having too many features makes interpretation
    harder)
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保持模型可解释性（特征过多会使解释变得更困难）
- en: to avoid the curse of dimensionality
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免维度灾难
- en: to maximize/minimize some objective function related to the model (R-squared,
    AIC, etc)
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大化/最小化与模型相关的某个目标函数（如R平方、AIC等）
- en: to avoid a poor fit, etc.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免过拟合等问题
- en: 'If the number of features N is small, an exhaustive search may be doable: you
    can literally try all possible combinations of features, and just keep the one
    that minimizes the cost / objective function. But if N is large, then an exhaustive
    search might be impossible. The total number of combinations to try is `2^N` which,
    if N is larger than a few dozen, becomes prohibitive — it’s an exponential function.
    In such cases you must use a heuristic method: explore the search space in an
    efficient way, looking for a combination of features that minimizes the objective
    function you use to conduct the search.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果特征的数量N较少，那么进行穷举搜索是可行的：你可以字面上尝试所有可能的特征组合，并保留最小化成本/目标函数的那个组合。但如果N较大，穷举搜索可能就不再可行。需要尝试的组合总数是`2^N`，如果N超过几十个，计算量将变得无法承受——这是一个指数函数。在这种情况下，你必须使用启发式方法：以高效的方式探索搜索空间，寻找能够最小化你用来进行搜索的目标函数的特征组合。
- en: What you’re looking for is a vector `[1, 0, 0, 1, 0, 1, 1, 1, 0, ...]` of length
    N, with elements taking values from `{0, 1}` . Each element in the vector is assigned
    to a feature; if the element is 1, the feature is selected; if the element is
    0, the feature is discarded. You need to find the vector that minimizes the objective
    function. The search space has as many dimensions N as there are features; the
    only possible values along any dimension are 0 and 1.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你要找的是一个长度为N的向量`[1, 0, 0, 1, 0, 1, 1, 1, 0, ...]`，其中的元素取值为`{0, 1}`。向量中的每个元素都分配给一个特征；如果元素为1，则选择该特征；如果元素为0，则丢弃该特征。你需要找到最小化目标函数的向量。搜索空间的维度与特征数量N一样多；沿着任何维度的可能值只有0和1。
- en: Finding a good heuristic algorithm is not trivial. The `regsubsets()` function
    in R has some options you can use. Also, scikit-learn offers [several methods](https://scikit-learn.org/stable/modules/feature_selection.html)
    to perform a heuristic feature selection, provided your problem is a good fit
    for their techniques. But finding a good, general purpose heuristic — in the most
    general form — is a hard problem. In this series of articles we will explore a
    few options that may work reasonably well even when N is large, and the objective
    function can be literally anything you can compute in code, provided it’s not
    too slow.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 找到一个好的启发式算法并不是一件简单的事情。R中的`regsubsets()`函数有一些选项可供使用。此外，scikit-learn提供了[几种方法](https://scikit-learn.org/stable/modules/feature_selection.html)来执行启发式特征选择，只要你的问题适合他们的技术。但是找到一个好的、通用的启发式算法——以最一般的形式——是一个困难的问题。在本系列文章中，我们将探讨一些选项，即使N很大，目标函数可以是你可以在代码中计算的任何东西，只要不太慢，可能也能工作得相当好。
- en: Dataset and full code
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集和完整代码
- en: 'For all optimization techniques in this series of articles, I am using [the
    very popular House Prices dataset on Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)
    (MIT license) — after a few simple feature transformations, it ends up having
    213 features (N=213) and 1453 observations. The model I’m using is linear regression,
    `statsmodels.api.OLS()` , and the objective function I am trying to minimize is
    BIC — the Bayesian Information Criterion — a measure of information loss, so a
    lower BIC is better. It is similar to AIC — the Akaike Information Criterion —
    except BIC tends to produce more parsimonious models: it prefers models with fewer
    features. Minimizing either AIC or BIC tends to reduce overfitting. But other
    objective functions could also be used instead, e.g. R-squared (the explained
    variance in the target) or adjusted R-squared — just keep in mind that a larger
    R-squared is better, so that’s a maximization problem.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本系列文章中的所有优化技术，我正在使用[在Kaggle上非常流行的房价数据集](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)（MIT许可证）——经过一些简单的特征转换后，它最终具有213个特征（N=213）和1453个观测值。我使用的模型是线性回归，`statsmodels.api.OLS()`，我试图最小化的目标函数是BIC——贝叶斯信息准则——一个衡量信息损失的指标，因此较低的BIC值更好。它类似于AIC——阿凯克信息准则——但BIC倾向于产生更简洁的模型：它更喜欢具有较少特征的模型。最小化AIC或BIC倾向于减少过拟合。但也可以使用其他目标函数，例如R平方（目标中解释的方差）或调整后的R平方——只需记住，较大的R平方值更好，因此这是一个最大化问题。
- en: Ultimately, the choice of objective function is irrelevant here. What matters
    is that we have one objective function that we’re consistently trying to optimize
    using various techniques.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，这里的目标函数的选择是无关紧要的。重要的是，我们有一个我们一直试图使用各种技术来优化的目标函数。
- en: The full code used in this series of articles is contained in a single notebook
    in [my feature selection repository](https://github.com/FlorinAndrei/fast_feature_selection)
    — also linked at the end. I will provide code snippets within the text here, but
    please check the notebook for the full context.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本系列文章中使用的完整代码包含在[我的特征选择存储库](https://github.com/FlorinAndrei/fast_feature_selection)中的一个笔记本中——也在末尾链接。我将在这里的文本中提供代码片段，但请查看笔记本获取完整上下文。
- en: 'We’re going to try and minimize BIC via feature selection, so here is the baseline
    value of BIC from `statsmodels.api.OLS()`, before any feature selection is done
    — with all features enabled:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将尝试通过特征选择来最小化BIC，因此这是在进行任何特征选择之前从`statsmodels.api.OLS()`中得到的BIC的基线值——启用所有特征时：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: And now let’s check a well-known, tried-and-true feature selection technique,
    which we will compare with the more complex techniques described later.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来检查一个众所周知的、经过验证的特征选择技术，我们将与后面描述的更复杂的技术进行比较。
- en: SFS — Sequential Feature Search
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SFS——顺序特征搜索
- en: SFS, the forward version, is fairly simple. It starts by trying to choose a
    single feature, and it selects that feature that minimizes the objective function.
    Once a feature is selected, it stays selected forever. Then it tries to add another
    feature to it (for a total of 2 features), in such a way as to minimize the objective
    again. It increases the number of selected features by 1, every time trying to
    find the best new feature to add to the existing collection. The search ends when
    all features are tried together. Whichever combination minimizes the objective,
    wins.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: SFS，前向版本，相当简单。它从尝试选择单个特征开始，并选择最小化目标函数的特征。一旦选择了一个特征，它将永远保持选择状态。然后尝试添加另一个特征（总共2个特征），以最小化目标。每次增加一个已选择特征的数量，尝试找到最佳的新特征添加到现有集合中。当所有特征一起尝试时，搜索结束。无论哪种组合最小化目标，都获胜。
- en: SFS is a greedy algorithm — each choice is locally optimal — and it never goes
    back to correct its own mistakes. But it’s reasonably fast even when N is quite
    large. The total number of combinations it tries is `N(N+1)/2` which is a quadratic
    polynomial (whereas an exhaustive search needs to perform an exponential number
    of trials).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: SFS是一种贪婪算法 — 每个选择都是局部最优的 — 它永远不会回头纠正自己的错误。但即使N相当大时，它也相当快。它尝试的总组合数为`N(N+1)/2`，这是一个二次多项式（而穷举搜索需要执行指数数量的试验）。
- en: 'Let’s see what the SFS code may look like in Python, using [the mlxtend library](https://rasbt.github.io/mlxtend/):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看SFS代码在Python中可能是什么样子，使用[mlxtend库](https://rasbt.github.io/mlxtend/)：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'It runs through the combinations quickly and these are the summary results:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 它快速运行组合，以下是摘要结果：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The best number of features is 36 out of 213\. The best BIC is 33708.986 (baseline
    value before feature selection is 34570.166), and it takes less than 1 minute
    to complete on my system. It invokes the objective function 22.8k times.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳特征数量是213个中的36个。最佳BIC为33708.986（特征选择之前的基准值为34570.166），在我的系统上不到1分钟即可完成。它调用目标函数22800次。
- en: 'These are the best BIC and R-squared values, as functions of the number of
    features selected:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是作为特征选择数量函数的最佳BIC和R-squared值：
- en: '![](../Images/20e26ad1e5129f0b3912d6f93c631f45.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20e26ad1e5129f0b3912d6f93c631f45.png)'
- en: BIC and R-squared for SFS
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: SFS的BIC和R-squared
- en: For more information, such as the names of the features actually selected, check
    the notebook in the repository.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息，例如实际选择的特征名称，请查看存储库中的笔记本。
- en: Now let’s try something more complex.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试一些更复杂的东西。
- en: CMA-ES (Covariance Matrix Adaptation Evolution Strategy)
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CMA-ES（协方差矩阵适应进化策略）
- en: This is a numeric optimization algorithm. It’s in the same class as genetic
    algorithms (they’re both evolutionary), but CMA-ES is quite distinct from GA.
    The algorithm is stochastic, and it does not require you to compute a derivative
    of the objective function (unlike gradient descent, which relies on partial derivatives).
    It is computationally efficient, and it’s used in a variety of numeric optimization
    libraries such as Optuna. I will only attempt a brief sketch of CMA-ES here; for
    more detailed explanations, please refer to the literature in the links section
    at the end.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个数值优化算法。它与遗传算法（它们都是进化算法）属于同一类，但CMA-ES与GA有很大不同。该算法是随机的，不需要计算目标函数的导数（不像梯度下降依赖于偏导数）。它在计算上是高效的，并且被用于各种数值优化库，如Optuna。我将在这里尝试对CMA-ES进行简要介绍；更详细的解释，请参考末尾链接部分的文献。
- en: 'Consider the 2-dimensional Rastrigin function:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑二维Rastrigin函数：
- en: '![](../Images/b415948f91befc1f571507180fe93343.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b415948f91befc1f571507180fe93343.png)'
- en: Rastrigin function
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Rastrigin函数
- en: The heatmap below shows the values of this function — brighter colors mean a
    higher value. The function has the global minimum in the origin (0, 0), but it’s
    peppered with many local extremes. We need to find the global minimum via CMA-ES.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的热图显示了该函数的值 — 较亮的颜色表示较高的值。该函数在原点（0, 0）处具有全局最小值，但它布满许多局部极值。我们需要通过CMA-ES找到全局最小值。
- en: '![](../Images/3ab2749810f8ad107b61626be88b40b2.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ab2749810f8ad107b61626be88b40b2.png)'
- en: the Rastrigin function heatmap
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Rastrigin函数热图
- en: 'CMA-ES is based on the multivariate normal distribution. It generates test
    points in the search space from this distribution. You will have to guess the
    original mean value of the distribution, and its standard deviation, but after
    that the algorithm will iteratively modify all these parameters, sweeping the
    distribution through the search space, looking for the best objective function
    values. Here’s the original distribution that the test points are drawn from:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: CMA-ES 基于多元正态分布。它从这个分布中生成搜索空间中的测试点。你需要猜测分布的原始均值和标准差，但之后算法会迭代地修改所有这些参数，在搜索空间中扫荡分布，寻找最佳的目标函数值。以下是测试点所来自的原始分布：
- en: '![](../Images/96ce2563d3ce0d11d42c6536bc729bbe.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96ce2563d3ce0d11d42c6536bc729bbe.png)'
- en: CMA-ES distribution
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: CMA-ES分布
- en: '`xi` is the set of points that the algorithm generates at each step, in the
    search space. `lambda` is the number of points generated. The mean value of the
    distribution will be updated at every step, and hopefully will converge eventually
    on the true solution. `sigma` is the standard deviation of the distribution —
    the spread of the test points. `C` is the covariance matrix: it defines the shape
    of the distribution. Depending on the values of `C` the distribution may have
    a “round” shape or a more elongated, oval shape. Changes to `C` allow CMA-ES to
    “sneak” into certain areas in the search space, or avoid other areas.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`xi` 是算法在每一步生成的点集，位于搜索空间中。`lambda` 是生成的点数。分布的均值将在每一步更新，并且希望最终能收敛到真实解。`sigma`
    是分布的标准差——即测试点的分布范围。`C` 是协方差矩阵：它定义了分布的形状。根据`C`的值，分布可能呈“圆形”或更拉长的椭圆形。对`C`的修改使得CMA-ES可以“悄悄”进入搜索空间中的某些区域，或避开其他区域。'
- en: '![](../Images/eb3ae64a92ba6cb4fec5961083196b0b.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb3ae64a92ba6cb4fec5961083196b0b.png)'
- en: first generation of test points
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 测试点的第一次生成。
- en: 'A population of 6 points was generated in the image above, which is the default
    population size picked by the optimizer for this problem. This is the first step.
    After this, the algorithm needs to:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 上图生成了6个点，这是优化器为此问题选择的默认种群大小。这是第一步。之后，算法需要：
- en: compute the objective function (Rastrigin) for each point
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每个点计算目标函数（Rastrigin）。
- en: update the mean, the standard deviation, and the covariance matrix, effectively
    creating a new multivariate normal distribution, based on what it has learned
    from the objective function
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新均值、标准差和协方差矩阵，实际上基于从目标函数中学到的信息，创建一个新的多元正态分布。
- en: generate a new set of test points from the new distribution
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从新的分布中生成一组新的测试点。
- en: repeat until some criterion is fulfilled (either converge on some mean value,
    or exceed the maximum number of steps, etc.)
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复直到某个条件满足（无论是收敛到某个均值，还是超过最大步数等）。
- en: 'I will not show the updates for all distribution parameters here, or else this
    article will become very large — check the links at the end for a complete explanation.
    But updating just the mean value of the distribution is quite simple, and it works
    like this: after computing the objective function for each test point, weights
    are assigned to the points, with the larger weights given to the points with a
    better objective value, and a weighted sum is calculated from their positions,
    which becomes the new mean. Effectively, CMA-ES moves the distribution mean value
    towards the points with a better objective value:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里不会展示所有分布参数的更新过程，否则这篇文章会变得很长——请查看文末的链接以获得完整的解释。但仅更新分布的均值是相当简单的，具体过程如下：在计算每个测试点的目标函数后，会给点赋予权重，权重较大的点会分配给目标值较好的点，并从这些点的位置计算加权和，作为新的均值。实际上，CMA-ES将分布的均值朝着具有更好目标值的点移动：
- en: '![](../Images/4474eeec4e64e343df3764a94a715f94.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4474eeec4e64e343df3764a94a715f94.png)'
- en: updating the CMA-ES distribution mean
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 更新CMA-ES分布的均值
- en: If the algorithm converges to the true solution, then the mean value of the
    distribution will converge to that solution. The standard deviation will converge
    to 0\. The covariance matrix will change the shape of the distribution (round
    or oval), depending on the geography of the objective function, extending into
    promising areas, and shying away from bad areas.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果算法收敛到真实解，则分布的均值将收敛到该解。标准差将收敛到0。协方差矩阵将根据目标函数的地理特征改变分布的形状（圆形或椭圆形），扩展到有前景的区域，并避开不良区域。
- en: 'Here’s an animated GIF showing the evolution in time of the test points while
    CMA-ES is solving the Rastrigin problem:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个动画 GIF，展示了 CMA-ES 在求解 Rastrigin 问题时测试点随时间演变的过程：
- en: '![](../Images/c7360fac3216f40f0db4b34caab8535e.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7360fac3216f40f0db4b34caab8535e.png)'
- en: animation showing the convergence of CMA-ES
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 动画展示了 CMA-ES 的收敛过程
- en: CMA-ES for feature selection
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CMA-ES 用于特征选择
- en: 'The 2D Rastrigin function was relatively simple, since it only has 2 dimensions.
    For our feature selection problem, we have N=213 dimensions. Moreover, the space
    is not continuous. Each test point is an N-dimensional vector with component values
    from `{0, 1}` . In other words, each test point looks like this: `[1, 0, 0, 1,
    1, 1, 0, ...]` — a binary vector. But other than that, the problem is the same:
    we need to find the points (or vectors) that minimize the objective function:
    the BIC parameter of an OLS model.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 2D Rastrigin 函数相对简单，因为它只有 2 个维度。而对于我们的特征选择问题，维度是 N=213。此外，空间不是连续的。每个测试点是一个 N
    维向量，其组件值来自 `{0, 1}`。换句话说，每个测试点看起来像这样：[1, 0, 0, 1, 1, 1, 0, ...] —— 一个二进制向量。但除此之外，问题是一样的：我们需要找到那些最小化目标函数的点（或向量）：OLS
    模型的 BIC 参数。
- en: 'Here’s a simple version of the CMA-ES code for feature selection, using [the
    cmaes library](https://github.com/CyberAgentAILab/cmaes):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个用于特征选择的 CMA-ES 代码的简单版本，使用了 [cmaes 库](https://github.com/CyberAgentAILab/cmaes)：
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The CMA-ES optimizer is defined with some initial guesses for the mean value
    and for sigma (standard deviation). It then loops through many generations, creating
    test points `x_for_eval` , evaluating them with the objective, modifying the distribution
    (mean, sigma, covariance matrix), etc. Each `x_for_eval` point is a binary vector
    `[1, 1, 1, 0, 0, 1, ...]` used to select the features from the dataset.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: CMA-ES 优化器使用一些初始猜测值来定义均值和标准差（sigma）。然后它循环多个代，创建测试点 `x_for_eval`，用目标函数评估它们，修改分布（均值、sigma、协方差矩阵）等。每个
    `x_for_eval` 点是一个二进制向量 `[1, 1, 1, 0, 0, 1, ...]`，用于从数据集中选择特征。
- en: Please note that the `CMAwM()` optimizer is used (CMA with margin) instead of
    the default `CMA()`. The default optimizer works well for regular, continuous
    problems, but the search space here is high-dimensional, and only two discrete
    values (0 and 1) are allowed. The default optimizer gets stuck in this space.
    `CMAwM()` enlarges a bit the search space (while the solutions it returns are
    still binary vectors), and that seems enough to unblock it.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，使用的是 `CMAwM()` 优化器（带有边际的 CMA），而不是默认的 `CMA()`。默认优化器适用于常规的、连续的问题，但这里的搜索空间是高维的，并且只允许两个离散值（0
    和 1）。默认优化器在这个空间中会卡住。`CMAwM()` 略微扩展了搜索空间（尽管它返回的解仍然是二进制向量），这似乎足以解锁它。
- en: This simple code definitely works, but it’s far from optimal. In the companion
    notebook, I have a more complex, optimized version of it, which is able to find
    better solutions, faster. But the code is big, so I will not show it here — [check
    the notebook](https://github.com/FlorinAndrei/fast_feature_selection).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的代码确实有效，但远非最优。在附带的笔记本中，我有一个更复杂、优化过的版本，能够更快地找到更好的解。但由于代码比较大，我不会在这里展示 —— [请查看笔记本](https://github.com/FlorinAndrei/fast_feature_selection)。
- en: The image below shows the history of the complex, optimized CMA-ES code searching
    for the best solution. The heatmap shows the prevalence / popularity of each feature
    at every generation (brighter = more popular). You can see how some features are
    always very popular, while others fall out of fashion quickly, and yet others
    are “discovered” later. The population size picked by the optimizer, given the
    parameters of this problem, is 20 points (individuals), so feature popularity
    is averaged across these 20 points.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了复杂、优化过的 CMA-ES 代码的历史，寻找最佳解的过程。热力图显示了每一代中特征的流行度（亮色 = 更受欢迎）。你可以看到某些特征始终非常受欢迎，而其他特征很快就过时，还有一些特征则是在后期“被发现”的。优化器选择的人口规模，根据该问题的参数，是
    20 个点（个体），因此特征的流行度是在这 20 个点之间进行平均的。
- en: '![](../Images/9cec9295c8e62cbf369a4fad1e66e18a.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9cec9295c8e62cbf369a4fad1e66e18a.png)'
- en: CMA-ES optimization history
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: CMA-ES 优化历史
- en: 'These are the main stats of the optimized CMA-ES code:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是优化后的 CMA-ES 代码的主要统计数据：
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: It was able to find a better (smaller) objective value than SFS, it invoked
    the objective function fewer times (20k), and it took about the same time to do
    it. This is a net gain over SFS by all metrics.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 它能够找到比 SFS 更好的（更小的）目标值，调用目标函数的次数更少（20k 次），且所需时间大致相同。这在所有度量标准下都比 SFS 有明显的进展。
- en: 'Again, the baseline BIC before any feature selection is:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 再次提醒，任何特征选择前的基准 BIC 是：
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As a side note: after studying traditional optimization algorithms (genetic
    algorithms, simulated annealing, etc), CMA-ES was a pleasant surprise. It has
    almost no hyperparameters, it’s computationally lightweight, it only needs a small
    population of individuals (points) to explore the search space, and yet it performs
    pretty well. It’s worth keeping it in your toolbox if you need to solve optimization
    problems.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 旁注：在研究传统优化算法（遗传算法、模拟退火等）后，CMA-ES 给我带来了惊喜。它几乎没有超参数，计算开销轻量，仅需要一个小规模的个体（点）群体来探索搜索空间，然而它的表现相当不错。如果你需要解决优化问题，值得将其加入工具箱中。
- en: '*This is part 1 of a two-part series about feature selection. Read* [*part
    2 here*](/efficient-feature-selection-via-genetic-algorithms-d6d3c9aff274)*.*'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*这是关于特征选择的两部分系列的第一部分。阅读* [*第二部分在这里*](/efficient-feature-selection-via-genetic-algorithms-d6d3c9aff274)*。*'
- en: Notes and links
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 备注和链接
- en: Thank you, [cmaes team](https://github.com/CyberAgentAILab/cmaes), for unblocking
    me. Your explanations really helped!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢 [cmaes 团队](https://github.com/CyberAgentAILab/cmaes)的解锁支持，你们的解释真的帮了我大忙！
- en: All images were created by the author.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 所有图片均由作者创建。
- en: 'Repository with all code: [https://github.com/FlorinAndrei/fast_feature_selection](https://github.com/FlorinAndrei/fast_feature_selection)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 包含所有代码的代码库：[https://github.com/FlorinAndrei/fast_feature_selection](https://github.com/FlorinAndrei/fast_feature_selection)
- en: 'The House Prices dataset (MIT license): [https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 房价数据集（MIT 许可证）：[https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)
- en: 'The mlxtend library: [https://github.com/rasbt/mlxtend](https://github.com/rasbt/mlxtend)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: mlxtend 库：[https://github.com/rasbt/mlxtend](https://github.com/rasbt/mlxtend)
- en: 'The cmaes library: [https://github.com/CyberAgentAILab/cmaes](https://github.com/CyberAgentAILab/cmaes)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: cmaes 库：[https://github.com/CyberAgentAILab/cmaes](https://github.com/CyberAgentAILab/cmaes)
- en: '*cmaes : A Simple yet Practical Python Library for CMA-ES* — a paper by Nomura
    M. and Shibata M. (2024) describing practical applications of CMA-ES as an optimization
    algorithm: [https://arxiv.org/abs/2402.01373](https://arxiv.org/abs/2402.01373)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*cmaes：一个简单但实用的 Python 库，用于 CMA-ES* — 由 Nomura M. 和 Shibata M.（2024）撰写的论文，描述了
    CMA-ES 作为优化算法的实际应用：[https://arxiv.org/abs/2402.01373](https://arxiv.org/abs/2402.01373)'
- en: '*The CMA Evolution Strategy: A Tutorial* — a paper by Hansen N. (2016) describing
    CMA-ES in detail: [https://arxiv.org/abs/1604.00772](https://arxiv.org/abs/1604.00772)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*CMA 演化策略：教程* — 由 Hansen N.（2016）撰写的论文，详细描述了 CMA-ES：[https://arxiv.org/abs/1604.00772](https://arxiv.org/abs/1604.00772)'
- en: 'The Wikipedia entry on CMA-ES: [https://en.wikipedia.org/wiki/CMA-ES](https://en.wikipedia.org/wiki/CMA-ES)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 'Wikipedia 关于 CMA-ES 的条目: [https://en.wikipedia.org/wiki/CMA-ES](https://en.wikipedia.org/wiki/CMA-ES)'
