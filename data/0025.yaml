- en: Tuning-Free Longer Context Lengths For LLMs — A Review of Self-Extend (LLM Maybe
    LongLM)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无需调优的LLM更长上下文长度——自我扩展（LLM也许是LongLM）评审
- en: 原文：[https://towardsdatascience.com/tuning-free-longer-context-lengths-for-llms-a-review-of-self-extend-llm-maybe-longlm-4bbd9b1021bf?source=collection_archive---------12-----------------------#2024-01-04](https://towardsdatascience.com/tuning-free-longer-context-lengths-for-llms-a-review-of-self-extend-llm-maybe-longlm-4bbd9b1021bf?source=collection_archive---------12-----------------------#2024-01-04)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/tuning-free-longer-context-lengths-for-llms-a-review-of-self-extend-llm-maybe-longlm-4bbd9b1021bf?source=collection_archive---------12-----------------------#2024-01-04](https://towardsdatascience.com/tuning-free-longer-context-lengths-for-llms-a-review-of-self-extend-llm-maybe-longlm-4bbd9b1021bf?source=collection_archive---------12-----------------------#2024-01-04)
- en: A simple strategy to enable LLMs to consume longer context length inputs during
    inference without the need for finetuning.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一种简单的策略，使LLM在推理过程中能够处理更长的上下文输入，而无需进行微调。
- en: '[](https://bhavinjawade.medium.com/?source=post_page---byline--4bbd9b1021bf--------------------------------)[![Bhavin
    Jawade](../Images/d10f204b2d0a8a3fd6887048ecaa307d.png)](https://bhavinjawade.medium.com/?source=post_page---byline--4bbd9b1021bf--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4bbd9b1021bf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4bbd9b1021bf--------------------------------)
    [Bhavin Jawade](https://bhavinjawade.medium.com/?source=post_page---byline--4bbd9b1021bf--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://bhavinjawade.medium.com/?source=post_page---byline--4bbd9b1021bf--------------------------------)[![Bhavin
    Jawade](../Images/d10f204b2d0a8a3fd6887048ecaa307d.png)](https://bhavinjawade.medium.com/?source=post_page---byline--4bbd9b1021bf--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4bbd9b1021bf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4bbd9b1021bf--------------------------------)
    [Bhavin Jawade](https://bhavinjawade.medium.com/?source=post_page---byline--4bbd9b1021bf--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4bbd9b1021bf--------------------------------)
    ·6 min read·Jan 4, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4bbd9b1021bf--------------------------------)
    ·阅读时长6分钟·2024年1月4日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/1422ba9fa79eb6dabf5851ca7e0a459b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1422ba9fa79eb6dabf5851ca7e0a459b.png)'
- en: 'Source: Image created by the author using DALL-E 3'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者使用DALL-E 3创建的图像
- en: 'In this article we will look at the paper “[LLM Maybe LongLM: Self-Extend LLM
    Context Window Without Tuning](https://arxiv.org/abs/2401.01325#:~:text=This%20work%20elicits%20LLMs''%20inherent,long%20input%20sequences%20for%20inference.)”
    by Hongye Jin et. al. that was just released on arxiv 2 days ago (2nd Jan 2024).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将查看Hongye Jin等人刚刚在arxiv发布的论文“[LLM也许是LongLM：自我扩展LLM上下文窗口无需调优](https://arxiv.org/abs/2401.01325#:~:text=This%20work%20elicits%20LLMs'%20inherent,long%20input%20sequences%20for%20inference.)”（2024年1月2日发布）。
- en: 'Github: [https://github.com/datamllab/LongLM](https://github.com/datamllab/LongLM)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 'Github: [https://github.com/datamllab/LongLM](https://github.com/datamllab/LongLM)'
- en: 'LLMs like GPT-3 or BERT are typically trained on fixed-length sequences. This
    design choice arises from practical constraints: managing computational resources
    and maintaining efficiency during training. As a result, these models are usually
    trained on sequences of a predetermined maximum length (like 512 tokens for BERT,
    or a few thousand for larger models). The limitation here is that the model’s
    self-attention mechanism, a key component that helps the model understand and
    generate language, is trained to only attend to contexts within this fixed length.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 像GPT-3或BERT这样的LLM通常是在固定长度的序列上进行训练的。这一设计选择源于实际的限制条件：管理计算资源并保持训练效率。因此，这些模型通常会在一个预定的最大长度的序列上进行训练（例如，BERT的最大长度为512个标记，或对于更大的模型，可能是几千个标记）。这里的限制是，模型的自注意力机制——一个帮助模型理解和生成语言的关键组件——只能训练去关注这个固定长度范围内的上下文。
- en: During training, the model learns to create representations of input sequences,
    incorporating both the **content (tokens) and position** of each token. **Positional
    encoding** , a method to incorporate the order of words, plays a crucial role
    here. However, since the training involves fixed-length sequences, the model’s
    understanding of positional relationships is confined to this range. When faced
    with longer sequences at inference time, **the model encounters positions and
    relative distances between tokens that it has never seen before**, leading to
    what the authors refer to as “**positional O.O.D (Out-Of-Distribution)**” issues.
    Essentially, the model’s performance degrades because it is processing input that
    is different in structure from what it was trained on.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，模型学习创建输入序列的表示，结合了每个标记的**内容（标记）和位置**。**位置编码**是一种融合单词顺序的方法，在这里发挥着至关重要的作用。然而，由于训练是基于固定长度的序列，模型对位置关系的理解仅限于这个范围。当在推理时遇到更长的序列时，**模型会遇到它从未见过的位置和标记之间的相对距离**，这就导致了作者所说的“**位置O.O.D（超出分布）**”问题。从本质上讲，模型的性能下降是因为它正在处理的输入结构与其训练时的输入不同。
- en: The challenge, therefore, lies in enabling these models to handle longer sequences
    then they were trained on, without encountering performance degradation due to
    positional O.O.D issues. This problem is especially pertinent in real-world applications
    where input sequences can be much longer than the training sequences, and the
    ability to maintain context over these longer lengths is crucial for tasks like
    document summarization, extended conversation understanding, or reading lengthy
    technical documents. The authors argue that LLMs are naturally equipped to handle
    longer text sequences than they are typically trained on, the only major bottleneck
    to this potential is O.O.D positional encodings.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，挑战在于使这些模型能够处理比它们训练时更长的序列，而不因位置的O.O.D（超出分布）问题而导致性能下降。这个问题在现实世界的应用中尤为重要，因为输入序列通常比训练序列要长得多，而在这些更长的序列上保持上下文的能力对于任务如文档摘要、扩展对话理解或阅读长篇技术文档等至关重要。作者认为，LLMs天生就能够处理比通常训练时更长的文本序列，唯一的主要瓶颈是O.O.D位置编码问题。
- en: The paper suggested a simple yet effective strategy to adapt the longer ‘inference
    time’ sequences to models trained on limited context length without any finetuning.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 论文提出了一种简单而有效的策略，可以将较长的“推理时间”序列适配到在有限上下文长度上训练的模型，而无需进行任何微调。
- en: Before we talk about the paper’s approach lets quickly look at perplexity metric
    used to evaluate LLMs.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论论文的方法之前，我们先快速了解一下用于评估大语言模型（LLMs）的困惑度指标。
- en: '**Perplexity (PPL)**'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**困惑度（PPL）**'
- en: Perplexity (PPL) is a commonly used metric in NLP to evaluate the performance
    of language models. It’s a measure of how well a probabilistic model predicts
    a sample. In the context of language models, perplexity gauges how well the model
    predicts a sequence of words. Perplexity is defined as the exponentiated average
    negative log-likelihood of a sequence of words. Mathematically
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 困惑度（PPL）是自然语言处理（NLP）中常用的度量，用于评估语言模型的表现。它衡量的是一个概率模型对样本的预测能力。在语言模型的背景下，困惑度衡量的是模型对一系列单词的预测能力。困惑度被定义为一个单词序列的负对数似然的指数化平均值。数学上来说
- en: '![](../Images/97a128ad587c81c1ef368d4e2f7559e0.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97a128ad587c81c1ef368d4e2f7559e0.png)'
- en: 'Source: [https://huggingface.co/docs/transformers/perplexity](https://huggingface.co/docs/transformers/perplexity)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[https://huggingface.co/docs/transformers/perplexity](https://huggingface.co/docs/transformers/perplexity)
- en: 'A lower perplexity score indicates a better language model. It means the model
    assigns higher probabilities, on average, to the test samples it sees. In other
    words, the model is less ‘surprised’ by the actual sequence of words it encounters,
    which is why the term ‘perplexity’ is used. Perplexity is usually used for causal
    models like GPT, Mistral etc (and not for MLMs like BERT). A great blog on huggingface
    to understand perplexity: [https://huggingface.co/docs/transformers/perplexity](https://huggingface.co/docs/transformers/perplexity)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 较低的困惑度分数意味着更好的语言模型。这意味着模型在处理测试样本时，平均会为其分配更高的概率。换句话说，模型对实际遇到的单词序列不那么“惊讶”，这也是为什么使用“困惑度”这一术语的原因。困惑度通常用于因果模型，如GPT、Mistral等（而不是用于BERT这类的MLM）。关于困惑度的一个很棒的博客：[https://huggingface.co/docs/transformers/perplexity](https://huggingface.co/docs/transformers/perplexity)
- en: Self-Extend
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自我扩展
- en: In standard LLMs, each token in a sequence attends to every other token, considering
    their relative positions. This positional information is critical for understanding
    the context and relationships between words. However, this mechanism is based
    on the maximum sequence length seen during training. When the input sequence exceeds
    this length, the model encounters positions it has never seen before (O.O.D.),
    leading to a decrease in performance.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准的大型语言模型（LLMs）中，序列中的每个标记都会关注到其他所有标记，考虑它们的相对位置。这种位置信息对于理解上下文和单词之间的关系至关重要。然而，这一机制是基于训练时看到的最大序列长度。当输入序列超过此长度时，模型会遇到它从未见过的位置（O.O.D.），导致性能下降。
- en: '**Grouped Attention:** To convert the O.O.D. positions into the range that
    model is trained to see, a simple mathematical “floor division” (denoted as “//”
    in programming) operation is performed.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**分组注意力**：为了将O.O.D.位置转换为模型训练时看到的范围，将执行一个简单的数学“向下取整除法”操作（在编程中表示为“//”）。'
- en: '![](../Images/7e0cf34c881b59a19e06ba563e9c5527.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e0cf34c881b59a19e06ba563e9c5527.png)'
- en: For example, consider a sequence of 8 tokens and a group size of 2\. The original
    positions `[0, 1, 2, 3, 4, 5, 6, 7]` would be mapped to `[0, 0, 1, 1, 2, 2, 3,
    3]`. By grouping tokens, the model can handle sequences longer than its original
    training limitation, effectively extending its context window.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个包含8个标记的序列和一个大小为2的分组。原始位置`[0, 1, 2, 3, 4, 5, 6, 7]`将被映射为`[0, 0, 1, 1,
    2, 2, 3, 3]`。通过分组标记，模型可以处理比其原始训练限制更长的序列，有效地扩展其上下文窗口。
- en: '![](../Images/fb5f14c02d996c1f43ac2e108bdc2dbb.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fb5f14c02d996c1f43ac2e108bdc2dbb.png)'
- en: 'Souce: [https://arxiv.org/pdf/2401.01325.pdf](https://arxiv.org/pdf/2401.01325.pdf)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/pdf/2401.01325.pdf](https://arxiv.org/pdf/2401.01325.pdf)
- en: But this inaccuracy in positions where multiple tokens have same positional
    embedding can leads to some degradataion in performance. The paper shows that
    with small group size PPL (Perplexity) is a slightly higher than the original
    LLMs.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，当多个标记具有相同的位置信息时，位置的不准确可能会导致性能的下降。论文表明，当分组大小较小时，PPL（困惑度）略高于原始的大型语言模型。
- en: The neighboring tokens are the most important tokens to generate the next token.
    For tokens that are close to each other, the precise relative positioning is essential
    for understanding the immediate context, such as the syntax and semantics of a
    sentence. Whereas when the tokens are far apart in a text, the precise relative
    positions between them become less crucial for understanding the overall context
    or meaning. The exact order of words in distant parts of a text is less important
    than the general thematic connection between those parts.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 邻近标记是生成下一个标记最重要的标记。对于彼此接近的标记，精确的相对位置对于理解即时上下文至关重要，例如句子的语法和语义。而当标记在文本中相隔较远时，它们之间的精确相对位置变得不那么重要，理解整体上下文或意义更依赖于它们之间的一般主题联系。
- en: 'This is where the paper proposes to combine the grouped attention (which has
    the FLOOR operation applied) with normal attention (with regular positions). The
    authors propose using normal attention for a predefined “neighbor window” around
    each token and grouped attention for tokens outside this window. Using the notations
    from the paper, ‘*L’*: Pretraining context window size, ‘*G’*: Group size for
    grouped attention, ‘*wn’*​: Window size for neighbor tokens:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，论文提出将带有 FLOOR 操作的分组注意力与常规注意力（具有常规位置）结合使用。作者建议对每个标记周围的预定义“邻居窗口”使用常规注意力，而对该窗口外的标记使用分组注意力。使用论文中的符号，‘*L*’：预训练上下文窗口大小，‘*G*’：分组注意力的组大小，‘*wn*’：邻居标记的窗口大小：
- en: '![](../Images/6d4ce53f3acde129724cc17f70118789.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d4ce53f3acde129724cc17f70118789.png)'
- en: 'Souce: [https://arxiv.org/pdf/2401.01325.pdf](https://arxiv.org/pdf/2401.01325.pdf)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/pdf/2401.01325.pdf](https://arxiv.org/pdf/2401.01325.pdf)
- en: The authors introduce a shift in the relative position for grouped attention
    by `*wn*​ − (*wn // G)*`​​ to ensure a smooth transition between normal and grouped
    attention areas. Finally, they merge the two parts of attention by replacing the
    attention values outside the neighbor token window with the values from grouped
    attention.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 作者通过`*wn* − (*wn // G)*`引入分组注意力的相对位置变化，以确保常规注意力区域和分组注意力区域之间的平滑过渡。最后，他们通过用分组注意力中的值替换邻居标记窗口外的注意力值，来合并两部分注意力。
- en: For example, considering a sequence where the original model’s context window
    is 7 tokens, the group size is 2, and the neighbor window is 4 tokens. For token
    positions `[0, 1, 2, 3, 4, 5, 6]`, normal attention is applied to `[0, 1, 2, 3]`
    and grouped attention to `[4, 5, 6]`. The attention matrix will be a blend of
    the two, with the precise attention for the first four tokens and grouped attention
    for the last three. The final attention matrix is then used to compute the output
    of the Transformer’s attention layer, maintaining both local precision and long-range
    context awareness.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个序列，其中原始模型的上下文窗口为7个标记，组大小为2，邻近窗口为4个标记。对于标记位置`[0, 1, 2, 3, 4, 5, 6]`，常规注意力应用于`[0,
    1, 2, 3]`，而分组注意力应用于`[4, 5, 6]`。注意力矩阵将是两者的结合，前四个标记具有精确的注意力，而后三个标记则采用分组注意力。最终的注意力矩阵随后用于计算Transformer的注意力层的输出，从而保持了局部精度和长程上下文感知能力。
- en: '**Conclusion**'
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**结论**'
- en: The paper showed comparison against LLaMA-2, Mistral and SOLAR with and without
    Self-Extend. As per the paper, Self-extend significantly decreases the perplexity
    of the models for long context window sizes. The models were also tested on a
    variety of tasks such as single-document and multi-document question answering,
    summarization, and few-shot learning. In most cases, LLMs with Self-Extend outperformed
    their original versions and even some fine-tuned models on these benchmarks. The
    models were also tested on various short-context tasks from the Hugging Face Open
    LLM benchmark suite. There was negligible impact on the performance of these tasks,
    indicating that Self-Extend does not adversely affect the model’s ability to handle
    shorter texts.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 论文展示了与LLaMA-2、Mistral和SOLAR在有无自我扩展（Self-Extend）情况下的对比。根据论文内容，自我扩展显著降低了模型在长上下文窗口大小下的困惑度（perplexity）。这些模型还在多种任务上进行了测试，如单文档和多文档问答、摘要生成以及少样本学习。在大多数情况下，具有自我扩展的LLM在这些基准测试中超越了其原始版本，甚至超越了一些微调后的模型。模型还在Hugging
    Face Open LLM基准测试套件中的各种短上下文任务上进行了测试，这些任务的性能几乎没有受到影响，表明自我扩展不会对模型处理较短文本的能力产生不利影响。
- en: This task asks a language model to find a basic passkey (a random five-digit
    number) hidden in a lengthy, nonsensical text sequence scattered at various levels.
    The findings reveal that Self-Extend, without specific adjustments, achieves 100%
    success in finding the passkey at all tested depths and context lengths.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 该任务要求语言模型在一个冗长的、毫无意义的文本序列中找到一个基本密码（一个随机的五位数），该密码散布在各个层次。研究结果表明，在没有特定调整的情况下，自我扩展能够在所有测试深度和上下文长度下实现100%的密码找回成功率。
- en: '![](../Images/df46ae7e245a3f4af1202c706dce5017.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df46ae7e245a3f4af1202c706dce5017.png)'
- en: 'Source: [https://arxiv.org/pdf/2401.01325.pdf](https://arxiv.org/pdf/2401.01325.pdf)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/pdf/2401.01325.pdf](https://arxiv.org/pdf/2401.01325.pdf)
- en: Additionally, the results show that even though Mistral w/ SWA (Sliding Window
    Attention) has a reduced PPL outside its initial training context range, it’s
    limited to extracting information (like the passkey) only within its sliding window.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，结果表明，即使Mistral w/ SWA（滑动窗口注意力）在其初始训练上下文范围之外的PPL值有所降低，它仅限于在其滑动窗口内提取信息（如密码）。
- en: Overall, this suggests that Self-Extend successfully leverages the inherent
    capabilities of LLMs for extended contexts.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，这表明自我扩展成功地利用了LLM在扩展上下文中的固有能力。
