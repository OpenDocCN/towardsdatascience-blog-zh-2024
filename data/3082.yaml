- en: Linearizing Attention
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性化注意力
- en: 原文：[https://towardsdatascience.com/linearizing-attention-204d3b86cc1e?source=collection_archive---------3-----------------------#2024-12-26](https://towardsdatascience.com/linearizing-attention-204d3b86cc1e?source=collection_archive---------3-----------------------#2024-12-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/linearizing-attention-204d3b86cc1e?source=collection_archive---------3-----------------------#2024-12-26](https://towardsdatascience.com/linearizing-attention-204d3b86cc1e?source=collection_archive---------3-----------------------#2024-12-26)
- en: 'Breaking the quadratic barrier: modern alternatives to softmax attention'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 打破二次方限制：softmax 注意力的现代替代方案
- en: '[](https://medium.com/@shitanshu273?source=post_page---byline--204d3b86cc1e--------------------------------)[![Shitanshu
    Bhushan](../Images/c9417483c279497fc8aa09b13c60d2a2.png)](https://medium.com/@shitanshu273?source=post_page---byline--204d3b86cc1e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--204d3b86cc1e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--204d3b86cc1e--------------------------------)
    [Shitanshu Bhushan](https://medium.com/@shitanshu273?source=post_page---byline--204d3b86cc1e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@shitanshu273?source=post_page---byline--204d3b86cc1e--------------------------------)[![Shitanshu
    Bhushan](../Images/c9417483c279497fc8aa09b13c60d2a2.png)](https://medium.com/@shitanshu273?source=post_page---byline--204d3b86cc1e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--204d3b86cc1e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--204d3b86cc1e--------------------------------)
    [Shitanshu Bhushan](https://medium.com/@shitanshu273?source=post_page---byline--204d3b86cc1e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--204d3b86cc1e--------------------------------)
    ·8 min read·Dec 26, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--204d3b86cc1e--------------------------------)
    ·阅读时间 8 分钟·2024年12月26日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Large Languange Models are great but they have a slight drawback that they use
    softmax attention which can be computationally intensive. In this article we will
    explore if there is a way we can replace the softmax somehow to achieve linear
    time complexity.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型非常强大，但它们有一个小缺点，即使用 softmax 注意力，这可能会导致计算开销较大。在本文中，我们将探索是否有办法通过某种方式替换 softmax，从而实现线性时间复杂度。
- en: '![](../Images/50f35225ca8cbf1bf38bd25730c93110.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/50f35225ca8cbf1bf38bd25730c93110.png)'
- en: Image by Author (Created using Miro Board)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片（使用 Miro Board 创建）
- en: Attention Basics
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意力机制基础
- en: I am gonna assume you already know about stuff like ChatGPT, Claude, and how
    transformers work in these models. Well attention is the backbone of such models.
    If we think of normal RNNs, we encode all past states in some hidden state and
    then use that hidden state along with new query to get our output. A clear drawback
    here is that well you can’t store everything in just a small hidden state. This
    is where attention helps, imagine for each new query you could find the most relevant
    past data and use that to make your prediction. That is essentially what attention
    does.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我假设你已经了解 ChatGPT、Claude 之类的内容，以及 Transformer 在这些模型中的工作原理。那么，注意力就是这些模型的核心。如果我们考虑普通的
    RNN，我们会将所有过去的状态编码到一个隐藏状态中，然后使用这个隐藏状态与新的查询一起得到输出。一个明显的缺点是，你不能把所有信息都存储在一个小小的隐藏状态中。这就是注意力机制的作用，想象一下，对于每一个新的查询，你可以找到最相关的过去数据，并使用这些数据来做出预测。这就是注意力机制的本质。
- en: 'Attention mechanism in transformers (the architecture behind most current language
    models) involve key, query and values embeddings. The attention mechanism in transformers
    works by matching queries against keys to retrieve relevant values. For each query(Q),
    the model computes similarity scores with all available keys(K), then uses these
    scores to create a weighted combination of the corresponding values(Y). This attention
    calculation can be expressed as:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 中的注意力机制（大多数当前语言模型背后的架构）涉及键、查询和值的嵌入。Transformer 中的注意力机制通过将查询与键进行匹配来检索相关值。对于每个查询（Q），模型计算与所有可用键（K）的相似度分数，然后利用这些分数创建相应值（Y）的加权组合。这个注意力计算可以表达为：
- en: '![](../Images/5870d932c2be4a2fc1e72f7aa70610e8.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5870d932c2be4a2fc1e72f7aa70610e8.png)'
- en: 'Source: Image by Author'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者提供的图片
- en: This mechanism enables the model to selectively retrieve and utilize information
    from its entire context when making predictions. We use softmax here since it
    effectively converts raw similarity scores into normalized probabilities, acting
    similar to a k-nearest neighbor mechanism where higher attention weights are assigned
    to more relevant keys.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这一机制使模型能够在做出预测时，有选择性地从整个上下文中提取和利用信息。我们在这里使用 softmax，因为它能有效地将原始相似度分数转换为标准化的概率，类似于
    k 最近邻机制，其中更高的注意力权重会分配给更相关的键。
- en: Okay now let’s see the computational cost of 1 attention layer,
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在让我们看一下 1 层注意力的计算成本，
- en: '![](../Images/27ae64c14ed6ad42627ca549439893ec.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/27ae64c14ed6ad42627ca549439893ec.png)'
- en: 'Source: Image by Author'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：图片由作者提供
- en: Softmax Drawback
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Softmax 缺点
- en: From above, we can see that we need to compute softmax for an NxN matrix, and
    thus, our computation cost becomes quadratic in sequence length. This is fine
    for shorter sequences, but it becomes extremely computationally inefficient for
    long sequences, N=100k+.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的内容可以看出，我们需要计算一个 NxN 矩阵的 softmax，因此，我们的计算成本随着序列长度的增加呈二次增长。对于较短的序列，这没问题，但对于长序列，N=100k+
    时，它变得极其低效。
- en: 'This gives us our motivation: can we reduce this computational cost? This is
    where linear attention comes in.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这给了我们动机：我们能否减少计算成本？这就是线性注意力的作用所在。
- en: Linear Attention
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性注意力
- en: 'Introduced by [Katharopoulos et al.](https://arxiv.org/pdf/2006.16236), linear
    attention uses a clever trick where we write the softmax exponential as a kernel
    function, expressed as dot products of feature maps φ(x). Using the associative
    property of matrix multiplication, we can then rewrite the attention computation
    to be linear. The image below illustrates this transformation:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由 [Katharopoulos 等人](https://arxiv.org/pdf/2006.16236)提出，线性注意力使用了一种巧妙的技巧，将 softmax
    指数表示为一个核函数，表现为特征映射 φ(x) 的点积。利用矩阵乘法的结合律，我们可以将注意力计算重写为线性。下图展示了这一转换：
- en: '![](../Images/70f502833c91b302822149c0fd21321a.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/70f502833c91b302822149c0fd21321a.png)'
- en: 'Source: Image by Author'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：图片由作者提供
- en: '[Katharopoulos et al.](https://arxiv.org/pdf/2006.16236) used elu(x) + 1 as
    φ(x), but any kernel feature map that can effectively approximate the exponential
    similarity can be used. The computational cost of above can be written as,'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[Katharopoulos 等人](https://arxiv.org/pdf/2006.16236)使用 elu(x) + 1 作为 φ(x)，但任何能够有效逼近指数相似度的核特征映射都可以使用。上述计算成本可以表示为：'
- en: '![](../Images/b740cb795e4fa6ec78b71883366e4021.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b740cb795e4fa6ec78b71883366e4021.png)'
- en: 'Source: Image by Author'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：图片由作者提供
- en: This eliminates the need to compute the full N×N attention matrix and reduces
    complexity to O(Nd²). Where d is the embedding dimension and this in effect is
    linear complexity when N >>> d, which is usually the case with Large Language
    Models
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这样就不再需要计算完整的 N×N 注意力矩阵，并将复杂度降低到 O(Nd²)。其中 d 是嵌入维度，当 N >>> d 时，这实际上是线性复杂度，这通常适用于大规模语言模型。
- en: Okay let’s look at the recurrent view of linear attention,
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们来看一下线性注意力的递归视角，
- en: '![](../Images/6a09502fb6c072b1d70ebad0657866a3.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a09502fb6c072b1d70ebad0657866a3.png)'
- en: 'Source: Image by Author'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：图片由作者提供
- en: Okay why can we do this in linear attention and not in softmax? Well softmax
    is not seperable so we can’t really write it as product of seperate terms. A nice
    thing to note here is that during decoding, we only need to keep track of S_(n-1),
    giving us O(d²) complexity per token generation since S is a d × d matrix.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，为什么我们可以在线性注意力中做这个，而不能在 softmax 中做呢？嗯，softmax 不是可分离的，因此我们无法将其写成独立项的乘积。这里值得注意的是，在解码过程中，我们只需要跟踪
    S_(n-1)，每生成一个 token 就能达到 O(d²) 的复杂度，因为 S 是一个 d × d 的矩阵。
- en: 'However, this efficiency comes with an important drawback. Since S_(n-1) can
    only store d² information (being a d × d matrix), we face a fundamental limitation.
    For instance, if your original context length requires storing 20d² worth of information,
    you’ll essentially lose 19d² worth of information in the compression. This illustrates
    the core memory-efficiency tradeoff in linear attention: we gain computational
    efficiency by maintaining only a fixed-size state matrix, but this same fixed
    size limits how much context information we can preserve and this gives us the
    motivation for gating.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种效率带来了一个重要的缺点。由于 S_(n-1) 只能存储 d² 信息（它是一个 d × d 的矩阵），我们面临着一个根本的限制。例如，如果原始上下文长度需要存储
    20d² 的信息，你将实际上在压缩过程中丢失 19d² 的信息。这展示了线性注意力中的核心内存效率权衡：通过仅保持固定大小的状态矩阵，我们获得了计算效率，但这种固定大小限制了我们可以保留的上下文信息量，这也给我们提供了引入门控机制的动机。
- en: Gated Linear Attention
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 门控线性注意力
- en: 'Okay, so we’ve established that we’ll inevitably forget information when optimizing
    for efficiency with a fixed-size state matrix. This raises an important question:
    can we be smart about what we remember? This is where gating comes in — researchers
    use it as a mechanism to selectively retain important information, trying to minimize
    the impact of memory loss by being strategic about what information to keep in
    our limited state. Gating isn’t a new concept and has been widely used in architectures
    like LSTM'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们已经确定，在使用固定大小的状态矩阵优化效率时，我们不可避免地会遗忘信息。这引出了一个重要的问题：我们能否聪明地选择记住什么信息？这就是门控机制的作用——研究人员将其作为一种选择性地保留重要信息的机制，通过战略性地选择保留哪些信息来尽量减少记忆丧失的影响。门控并不是一个新概念，它在像LSTM这样的架构中已经得到广泛应用。
- en: The basic change here is in the way we formulate Sn,
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的基本变化在于我们如何公式化Sn，
- en: '![](../Images/1218754c71d1ac1d9a55cc83d6bab4b3.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1218754c71d1ac1d9a55cc83d6bab4b3.png)'
- en: 'Source: Image by author'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者提供的图片
- en: There are many choices for G all which lead to different models,
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多选择可以决定G，这些选择会导致不同的模型，
- en: '![](../Images/c2dbaef9d3546d4c51e87f39b5e1565a.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2dbaef9d3546d4c51e87f39b5e1565a.png)'
- en: 'Source: [Yang, Songlin, et al. “Gated linear attention transformers with hardware-efficient
    training.” *arXiv preprint arXiv:2312.06635*(2023).](https://arxiv.org/pdf/2312.06635)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[Yang, Songlin, et al. “Gated linear attention transformers with hardware-efficient
    training.” *arXiv preprint arXiv:2312.06635*(2023).](https://arxiv.org/pdf/2312.06635)
- en: A key advantage of this architecture is that the gating function depends only
    on the current token x and learnable parameters, rather than on the entire sequence
    history. Since each token’s gating computation is independent, this allows for
    efficient parallel processing during training — all gating computations across
    the sequence can be performed simultaneously.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构的一个关键优势是，门控函数仅依赖于当前标记x和可学习的参数，而不是整个序列历史。由于每个标记的门控计算是独立的，这使得在训练过程中能够高效地进行并行处理——整个序列的所有门控计算可以同时执行。
- en: State Space Models
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 状态空间模型
- en: When we think about processing sequences like text or time series, our minds
    usually jump to attention mechanisms or RNNs. But what if we took a completely
    different approach? Instead of treating sequences as, well, sequences, what if
    we processed them more like how CNNs handle images using convolutions?
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想到处理像文本或时间序列这样的序列时，我们的大脑通常会跳到注意力机制或RNN。但如果我们采取完全不同的方法呢？如果我们不把序列当作序列来处理，而是像CNN处理图像一样，通过卷积来处理它们呢？
- en: 'State Space Models (SSMs) formalize this approach through a discrete linear
    time-invariant system:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 状态空间模型（SSMs）通过离散线性时不变系统形式化了这种方法：
- en: '![](../Images/27fbbd16083eb182a5315c4fd43e4988.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/27fbbd16083eb182a5315c4fd43e4988.png)'
- en: 'Source: Image by Author'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者提供的图片
- en: Okay now let’s see how this relates to convolution,
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在让我们看看这与卷积有什么关系，
- en: '![](../Images/5d1b96244bda4a4eec0b5ba784afb2f2.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d1b96244bda4a4eec0b5ba784afb2f2.png)'
- en: 'Source: Image by Author'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者提供的图片
- en: where F is our learned filter derived from parameters (A, B, c), and * denotes
    convolution.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中F是我们从参数（A, B, c）中学习到的滤波器，*表示卷积。
- en: '[H3](https://arxiv.org/pdf/2212.14052) implements this state space formulation
    through a novel structured architecture consisting of two complementary SSM layers.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[H3](https://arxiv.org/pdf/2212.14052)通过一种新颖的结构化架构实现了这种状态空间公式，该架构由两个互补的SSM层组成。'
- en: '![](../Images/31a7169dee8d15099468da32187db1d1.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31a7169dee8d15099468da32187db1d1.png)'
- en: 'Source: [Fu, Daniel Y., et al. “Hungry hungry hippos: Towards language modeling
    with state space models.” *arXiv preprint arXiv:2212.14052* (2022).](https://arxiv.org/pdf/2212.14052)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '来源：[Fu, Daniel Y., et al. “Hungry hungry hippos: Towards language modeling
    with state space models.” *arXiv preprint arXiv:2212.14052* (2022).](https://arxiv.org/pdf/2212.14052)'
- en: Here we take the input and break it into 3 channels to imitate K, Q and V. We
    then use 2 SSM and 2 gating to kind of imitate linear attention and it turns out
    that this kind of architecture works pretty well in practice.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将输入分解为3个通道，以模仿K、Q和V。然后，我们使用2个SSM和2个门控来模仿线性注意力，事实证明，这种架构在实践中效果相当不错。
- en: Selective State Space Models
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择性状态空间模型
- en: Earlier, we saw how gated linear attention improved upon standard linear attention
    by making the information retention process data-dependent. A similar limitation
    exists in State Space Models — the parameters A, B, and c that govern state transitions
    and outputs are fixed and data-independent. This means every input is processed
    through the same static system, regardless of its importance or context.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 早些时候，我们看到门控线性注意力如何通过使信息保持过程数据依赖，改进了标准的线性注意力。状态空间模型中也存在类似的限制——控制状态转移和输出的参数A、B和c是固定的且与数据无关。这意味着每个输入都通过相同的静态系统进行处理，而不考虑其重要性或上下文。
- en: 'we can extend SSMs by making them data-dependent through time-varying dynamical
    systems:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过引入时变动态系统，使SSM（状态空间模型）成为数据依赖的，从而扩展SSM：
- en: '![](../Images/ad558a2c2bf96272a731f0f55067b4fd.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad558a2c2bf96272a731f0f55067b4fd.png)'
- en: 'Source: Image by Author'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者提供的图片
- en: The key question becomes how to parametrize c_t, b_t, and A_t to be functions
    of the input. Different parameterizations can lead to architectures that approximate
    either linear or gated attention mechanisms.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 关键问题变成了如何将c_t、b_t和A_t参数化为输入的函数。不同的参数化方式可以导致接近线性或门控注意力机制的架构。
- en: '[Mamba](https://arxiv.org/pdf/2312.00752) implements this time-varying state
    space formulation through selective SSM blocks.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[Mamba](https://arxiv.org/pdf/2312.00752)通过选择性SSM模块实现了这种时变状态空间的表达。'
- en: '![](../Images/2b64c4a6ce7a210625f0df6475ec1bc2.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b64c4a6ce7a210625f0df6475ec1bc2.png)'
- en: 'Source: [Gu, Albert, and Tri Dao. “Mamba: Linear-time sequence modeling with
    selective state spaces.” *arXiv preprint arXiv:2312.00752* (2023).](https://arxiv.org/pdf/2312.00752)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '来源：[Gu, Albert, and Tri Dao. “Mamba: Linear-time sequence modeling with selective
    state spaces.” *arXiv preprint arXiv:2312.00752* (2023).](https://arxiv.org/pdf/2312.00752)'
- en: Mamba here uses Selective SSM instead of SSM and uses output gating and additional
    convolution to improve performance. This is a very high-level idea explaining
    how Mamba combines these components into an efficient architecture for sequence
    modeling.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，Mamba使用了选择性SSM代替了SSM，并通过输出门控和额外的卷积来提高性能。这是一个非常高层次的思想，解释了Mamba如何将这些组件组合成一个高效的序列建模架构。
- en: Conclusion
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we explored the evolution of efficient sequence modeling architectures.
    Starting with traditional softmax attention, we identified its quadratic complexity
    limitation, which led to the development of linear attention. By rewriting attention
    using kernel functions, linear attention achieved O(Nd²) complexity but faced
    memory limitations due to its fixed-size state matrix.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们探讨了高效序列建模架构的演变。从传统的softmax注意力开始，我们识别出其二次复杂度限制，这促使了线性注意力的发展。通过使用核函数重写注意力，线性注意力实现了O(Nd²)的复杂度，但由于固定大小的状态矩阵，它面临着内存限制。
- en: This limitation motivated gated linear attention, which introduced selective
    information retention through gating mechanisms. We then explored an alternative
    perspective through State Space Models, showing how they process sequences using
    convolution-like operations. The progression from basic SSMs to time-varying systems
    and finally to selective SSMs parallels our journey from linear to gated attention
    — in both cases, making the models more adaptive to input data proved crucial
    for performance.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这种限制促使了门控线性注意力的提出，通过门控机制引入了选择性信息保持。我们接着从状态空间模型的角度进行了探讨，展示了它们如何通过类似卷积的操作来处理序列。从基础SSM到时变系统，再到选择性SSM的进展，与我们从线性到门控注意力的历程相似——在这两种情况下，使模型更加适应输入数据对于性能至关重要。
- en: 'Through these developments, we see a common theme: the fundamental trade-off
    between computational efficiency and memory capacity. Softmax attention excels
    at in-context learning by maintaining full attention over the entire sequence,
    but at the cost of quadratic complexity. Linear variants (including SSMs) achieve
    efficient computation through fixed-size state representations, but this same
    optimization limits their ability to maintain detailed memory of past context.
    This trade-off continues to be a central challenge in sequence modeling, driving
    the search for architectures that can better balance these competing demands.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些发展，我们看到了一个共同的主题：计算效率与内存容量之间的基本权衡。Softmax注意力通过保持对整个序列的完全关注，擅长于上下文学习，但代价是二次复杂度。线性变体（包括SSM）通过固定大小的状态表示实现了高效计算，但这种优化限制了它们保持过去上下文详细记忆的能力。这个权衡继续成为序列建模中的核心挑战，推动着对能够更好平衡这些竞争需求的架构的探索。
- en: 'To read more on this topics, i would suggest the following papers:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 想要了解更多相关主题，我建议阅读以下论文：
- en: '**Linear Attention**: [Katharopoulos, Angelos, et al. “Transformers are rnns:
    Fast autoregressive transformers with linear attention.” *International conference
    on machine learning*. PMLR, 2020.](https://arxiv.org/pdf/2006.16236)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性注意力**: [Katharopoulos, Angelos 等. “Transformers are rnns: Fast autoregressive
    transformers with linear attention.” *国际机器学习会议*. PMLR, 2020.](https://arxiv.org/pdf/2006.16236)'
- en: '**GLA**: [Yang, Songlin, et al. “Gated linear attention transformers with hardware-efficient
    training.” *arXiv preprint arXiv:2312.06635*(2023).](https://arxiv.org/pdf/2312.06635)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**GLA**: [Yang, Songlin 等. “Gated linear attention transformers with hardware-efficient
    training.” *arXiv 预印本 arXiv:2312.06635* (2023).](https://arxiv.org/pdf/2312.06635)'
- en: '**H3**: [Fu, Daniel Y., et al. “Hungry hungry hippos: Towards language modeling
    with state space models.” *arXiv preprint arXiv:2212.14052* (2022).](https://arxiv.org/pdf/2212.14052)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**H3**: [Fu, Daniel Y. 等. “Hungry hungry hippos: Towards language modeling
    with state space models.” *arXiv 预印本 arXiv:2212.14052* (2022).](https://arxiv.org/pdf/2212.14052)'
- en: '**Mamba**: [Gu, Albert, and Tri Dao. “Mamba: Linear-time sequence modeling
    with selective state spaces.” *arXiv preprint arXiv:2312.00752* (2023).](https://arxiv.org/pdf/2312.00752)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**Mamba**: [Gu, Albert 和 Tri Dao. “Mamba: Linear-time sequence modeling with
    selective state spaces.” *arXiv 预印本 arXiv:2312.00752* (2023).](https://arxiv.org/pdf/2312.00752)'
- en: '[Waleffe, Roger, et al. “An Empirical Study of Mamba-based Language Models.”
    *arXiv preprint arXiv:2406.07887* (2024).](https://arxiv.org/pdf/2406.07887)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[Waleffe, Roger 等. “An Empirical Study of Mamba-based Language Models.” *arXiv
    预印本 arXiv:2406.07887* (2024).](https://arxiv.org/pdf/2406.07887)'
- en: Acknowledgement
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢
- en: This blog post was inspired by coursework from my graduate studies during Fall
    2024 at University of Michigan. While the courses provided the foundational knowledge
    and motivation to explore these topics, any errors or misinterpretations in this
    article are entirely my own. This represents my personal understanding and exploration
    of the material.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇博客文章的灵感来源于我在2024年秋季于密歇根大学研究生课程中的学习。尽管这些课程提供了探索这些主题的基础知识和动机，但本文中的任何错误或误解完全是我个人的责任。这代表了我对这些材料的个人理解和探索。
