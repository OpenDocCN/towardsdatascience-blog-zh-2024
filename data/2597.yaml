- en: 'Multilayer Perceptron, Explained: A Visual Guide with Mini 2D Dataset'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层感知器，解释：带有迷你二维数据集的视觉指南
- en: 原文：[https://towardsdatascience.com/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c?source=collection_archive---------1-----------------------#2024-10-25](https://towardsdatascience.com/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c?source=collection_archive---------1-----------------------#2024-10-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c?source=collection_archive---------1-----------------------#2024-10-25](https://towardsdatascience.com/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c?source=collection_archive---------1-----------------------#2024-10-25)
- en: CLASSIFICATION ALGORITHM
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类算法
- en: Dissecting the math (with visuals) of a tiny neural network
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解剖一个小型神经网络的数学（带视觉展示）
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--0ae8100c5d1c--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--0ae8100c5d1c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0ae8100c5d1c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0ae8100c5d1c--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--0ae8100c5d1c--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@samybaladram?source=post_page---byline--0ae8100c5d1c--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--0ae8100c5d1c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0ae8100c5d1c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0ae8100c5d1c--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--0ae8100c5d1c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0ae8100c5d1c--------------------------------)
    ·13 min read·Oct 25, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0ae8100c5d1c--------------------------------)
    ·13分钟阅读·2024年10月25日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/14e597fceb0af5b4a1ac957a8f8434fc.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14e597fceb0af5b4a1ac957a8f8434fc.png)'
- en: '`⛳️ More [CLASSIFICATION ALGORITHM](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c),
    explained: · [Dummy Classifier](/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e)
    · [K Nearest Neighbor Classifier](/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1)
    · [Bernoulli Naive Bayes](/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6)
    · [Gaussian Naive Bayes](/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c)
    · [Decision Tree Classifier](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e)
    · [Logistic Regression](/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505)
    · [Support Vector Classifier](/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9)
    ▶ [Multilayer Perceptron](/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c)`'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '`⛳️ 更多 [分类算法](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c)，解释：·
    [虚拟分类器](/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e)
    · [K近邻分类器](/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1)
    · [伯努利朴素贝叶斯](/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6)
    · [高斯朴素贝叶斯](/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c)
    · [决策树分类器](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e)
    · [逻辑回归](/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505)
    · [支持向量分类器](/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9)
    ▶ [多层感知器](/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c)`'
- en: Ever feel like neural networks are showing up everywhere? They’re in the news,
    in your phone, even in your social media feed. But let’s be honest — most of us
    have no clue how they actually work. All that fancy math and strange terms like
    “backpropagation”?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 是否觉得神经网络无处不在？它们出现在新闻中、手机里，甚至出现在你的社交媒体动态中。但说实话——我们大多数人根本不清楚它们是如何运作的。那些复杂的数学和像“反向传播”这样的术语呢？
- en: 'Here’s a thought: what if we made things super simple? Let’s explore a Multilayer
    Perceptron (MLP) — **the most basic type of neural network** — to classify a simple
    2D dataset using a small network, working with just a handful of data points.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个思考：如果我们将事情简化呢？让我们探索一个多层感知机（MLP）——**最基本类型的神经网络**——来使用一个小型网络分类一个简单的二维数据集，只使用少量的数据点。
- en: Through clear visuals and step-by-step explanations, you’ll see the math come
    to life, watching exactly how numbers and equations flow through the network and
    how learning really happens!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 通过清晰的视觉效果和逐步的解释，你将看到数学变得生动，观察数字和方程式如何在网络中流动，以及学习是如何真正发生的！
- en: '![](../Images/68c633860f446dbc91b04b1a420fcef7.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/68c633860f446dbc91b04b1a420fcef7.png)'
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所有视觉效果：作者使用Canva Pro创建。优化为手机端显示；在桌面端可能显示过大。
- en: Definition
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义
- en: A Multilayer Perceptron (MLP) is a type of neural network that uses layers of
    connected nodes to learn patterns. It gets its name from having multiple layers
    — typically an input layer, one or more middle (hidden) layers, and an output
    layer.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知机（MLP）是一种神经网络类型，它使用连接的节点层来学习模式。它之所以得名，是因为它有多个层——通常包括一个输入层、一个或多个中间（隐藏）层和一个输出层。
- en: Each node connects to all nodes in the next layer. When the network learns,
    it adjusts the strength of these connections based on training examples. For instance,
    if certain connections lead to correct predictions, they become stronger. If they
    lead to mistakes, they become weaker.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点都与下一层的所有节点相连。当网络学习时，它会根据训练示例调整这些连接的强度。例如，如果某些连接导致正确的预测，它们会变得更强。如果它们导致错误预测，它们会变得更弱。
- en: This way of learning through examples helps the network recognize patterns and
    make predictions about new situations it hasn’t seen before.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种例子学习的方式，帮助网络识别模式，并对它从未见过的新情况做出预测。
- en: '![](../Images/14e597fceb0af5b4a1ac957a8f8434fc.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14e597fceb0af5b4a1ac957a8f8434fc.png)'
- en: MLPs are considered fundamental in the field of neural networks and deep learning
    because they can handle complex problems that simpler methods struggle with.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知机（MLPs）被认为是神经网络和深度学习领域的基础，因为它们可以处理一些简单方法无法解决的复杂问题。
- en: 📊 Dataset Used
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 📊 使用的数据集
- en: 'To understand how MLPs work, let’s start with a simple example: a mini 2D dataset
    with just a few samples. We’ll use [the same dataset](https://medium.com/towards-data-science/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9)
    from our previous article to keep things manageable.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解多层感知机（MLPs）的工作原理，我们从一个简单的例子开始：一个只有几个样本的迷你二维数据集。我们将使用[相同的数据集](https://medium.com/towards-data-science/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9)来保持简洁，这个数据集来自我们之前的文章。
- en: '![](../Images/7d0cc55573cd44b994e5a8fcff182285.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7d0cc55573cd44b994e5a8fcff182285.png)'
- en: 'Columns: Temperature (0–3), Humidity (0–3), Play Golf (Yes/No). The training
    dataset has 2 dimensions and 8 samples.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 列：温度（0-3）、湿度（0-3）、打高尔夫（是/否）。训练数据集有2个维度和8个样本。
- en: Rather than jumping straight into training, let’s try to understand the key
    pieces that make up a neural network and how they work together.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在直接进入训练之前，让我们先试着理解构成神经网络的关键部分及其如何协同工作。
- en: 'Step 0: Network Structure'
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 0：网络结构
- en: 'First, let’s look at the parts of our network:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们来看看网络的各个部分：
- en: Node (Neuron)
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 节点（神经元）
- en: We begin with the basic structure of a neural network. This structure is composed
    of many individual units called nodes or neurons.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从神经网络的基本结构开始。这个结构由许多个单独的单位组成，称为节点或神经元。
- en: '![](../Images/77265c27db24a49170b0d38542b64000.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77265c27db24a49170b0d38542b64000.png)'
- en: This neural network has 8 nodes.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这个神经网络有8个节点。
- en: 'These nodes are organized into groups called layers to work together:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这些节点被组织成一个个层次结构来协同工作：
- en: Input layer
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输入层
- en: The input layer is where we start. It takes in our raw data, and the number
    of nodes here matches how many features we have.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层是我们开始的地方。它接收我们的原始数据，节点的数量与特征的数量相匹配。
- en: '![](../Images/f6b53d8cb9333fa129a63e062bced4a6.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6b53d8cb9333fa129a63e062bced4a6.png)'
- en: The input layer has 2 nodes, one for each feature.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层有2个节点，每个特征对应一个节点。
- en: Hidden layer
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 隐藏层
- en: Next come the hidden layers. We can have one or more of these layers, and we
    can choose how many nodes each one has. Often, we use fewer nodes in each layer
    as we go deeper.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是隐藏层。我们可以有一个或多个这样的层，且可以选择每层的节点数量。通常，我们在层数越深时，每层的节点数量会越来越少。
- en: '![](../Images/889ce747a48b89418498002342df446c.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/889ce747a48b89418498002342df446c.png)'
- en: This neural network has 2 hidden layers with 3 and 2 nodes, respectively.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个神经网络有2个隐藏层，分别包含3个节点和2个节点。
- en: Output layer
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输出层
- en: 'The last layer gives us our final answer. The number of nodes in our output
    layer depends on our task: for binary classification or regression, we might have
    just one output node, while for multi-class problems, we’d have one node per class.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一层给出了我们的最终答案。输出层中节点的数量取决于我们的任务：对于二分类或回归问题，我们可能只有一个输出节点，而对于多类问题，我们则为每个类别设置一个节点。
- en: '![](../Images/501dd95ec39e9361695c1dfe3776c883.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/501dd95ec39e9361695c1dfe3776c883.png)'
- en: This neural network has output layer with only 1 node (because binary).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这个神经网络的输出层只有1个节点（因为是二分类）。
- en: Weights
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 权重
- en: 'The nodes connect to each other using weights — numbers that control how much
    each piece of information matters. Each connection between nodes has its own weight.
    This means we need lots of weights: every node in one layer connects to every
    node in the next layer.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 节点之间通过权重相互连接——权重是控制每个信息片段重要性的数字。每个节点之间的连接都有自己的权重。这意味着我们需要大量的权重：一层中的每个节点都连接到下一层的每个节点。
- en: '![](../Images/943ab96a3ec27296e97af58df6ac6f3b.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/943ab96a3ec27296e97af58df6ac6f3b.png)'
- en: This neural network has total of 14 weights.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个神经网络总共有14个权重。
- en: Biases
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 偏置
- en: Along with weights, each node also has a bias — an extra number that helps it
    make better decisions. While weights control connections between nodes, biases
    help each node adjust its output.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 除了权重，每个节点还拥有一个偏置——一个额外的数字，帮助它做出更好的决策。权重控制节点之间的连接，而偏置则帮助每个节点调整其输出。
- en: '![](../Images/ad9281828475d8a5feccaf9fb504b653.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad9281828475d8a5feccaf9fb504b653.png)'
- en: This neural network has 6 bias values.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个神经网络有6个偏置值。
- en: The Neural Network
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络
- en: 'In summary, we will use and train this neural network:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们将使用并训练这个神经网络：
- en: '![](../Images/91aa2d74c3115b07db4df4ad2928bd43.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91aa2d74c3115b07db4df4ad2928bd43.png)'
- en: 'Our network consists of 4 layers: 1 input layer (2 nodes), 2 hidden layers
    (3 nodes & 2 nodes), and 1 output layer (1 node). This creates a 2–3–2–1 architecture.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的网络由4层组成：1个输入层（2个节点）、2个隐藏层（3个节点和2个节点），以及1个输出层（1个节点）。这形成了一个2–3–2–1的架构。
- en: 'Let’s look at this new diagram that shows our network from top to bottom. I’ve
    updated it to make the math easier to follow: information starts at the top nodes
    and flows down through the layers until it reaches the final answer at the bottom.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个新图示，它展示了我们的网络从上到下的结构。我已经更新了它，以便更容易理解数学过程：信息从顶部的节点开始，沿着各层向下流动，直到到达底部的最终答案。
- en: '![](../Images/86f3b59ab8e38a31139b12deeff4036b.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/86f3b59ab8e38a31139b12deeff4036b.png)'
- en: Now that we understand how our network is built, let’s see how information moves
    through it. This is called the forward pass.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了网络的构建方式，接下来让我们看看信息是如何在其中流动的。这叫做前向传播（forward pass）。
- en: 'Step 1: Forward Pass'
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一步：前向传播
- en: 'Let’s see how our network turns input into output, step by step:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步了解网络是如何将输入转换为输出的：
- en: Weight initialization
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 权重初始化
- en: Before our network can start learning, we need to give each weight a starting
    value. We choose small random numbers between -1 and 1\. Starting with random
    numbers helps our network learn without any early preferences or patterns.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的网络开始学习之前，我们需要为每个权重设置一个初始值。我们选择在-1到1之间的小随机数。使用随机数开始有助于我们的网络在没有任何早期偏好或模式的情况下学习。
- en: '![](../Images/acad0344344b5bb71231385d3d1816b5.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/acad0344344b5bb71231385d3d1816b5.png)'
- en: All the weights are chosen randomly from a [-0.5, 0.5] range.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 所有权重都是从[-0.5, 0.5]范围内随机选择的。
- en: Weighted sum
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加权和
- en: Each node processes incoming data in two steps. First, it multiplies each input
    by its weight and adds all these numbers together. Then it adds one more number
    — the bias — to complete the calculation. The bias is essentially **a weight with
    a constant input of 1**.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点以两步处理传入的数据。首先，它将每个输入乘以其权重，并将所有这些数字相加。然后，它再加上一个数字——偏置——以完成计算。偏置本质上是**一个恒定输入为1的权重**。
- en: '![](../Images/f8d6f7809ee3fa23613d041108562246.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8d6f7809ee3fa23613d041108562246.png)'
- en: Activation function
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数
- en: Each node takes its weighted sum and runs it through an **activation function**
    to produce its output. The activation function helps our network learn complicated
    patterns by introducing non-linear behavior.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点将其加权和通过**激活函数**进行处理，以生成输出。激活函数通过引入非线性行为，帮助我们的网络学习复杂的模式。
- en: 'In our hidden layers, we use the ReLU function (Rectified Linear Unit). ReLU
    is straightforward: if a number is positive, it stays the same; if it’s negative,
    it becomes zero.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的隐藏层中，我们使用 ReLU 函数（整流线性单元）。ReLU 很简单：如果一个数是正数，它保持不变；如果是负数，它变为零。
- en: '![](../Images/6a74e04f64fc33016b7d0d03042d1b62.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a74e04f64fc33016b7d0d03042d1b62.png)'
- en: Layer-by-layer computation
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层级计算
- en: This two-step process (weighted sums and activation) happens in every layer,
    one after another. Each layer’s calculations help transform our input data step
    by step into our final prediction.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这个两步过程（加权求和和激活）在每一层中依次进行。每一层的计算帮助我们一步一步地将输入数据转化为最终的预测值。
- en: '![](../Images/62d3a696fdde86aeed6858154a42ef3c.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62d3a696fdde86aeed6858154a42ef3c.png)'
- en: Output generation
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输出生成
- en: The last layer creates our network’s final answer. For our yes/no classification
    task, we use a special activation function called **sigmoid** in this layer.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一层给出了网络的最终答案。对于我们的是/否分类任务，我们在这一层使用一种特殊的激活函数，叫做 **sigmoid**。
- en: 'The sigmoid function turns any number into a value between 0 and 1\. This makes
    it perfect for yes/no decisions, as we can treat the output like a probability:
    closer to 1 means more likely ‘yes’, closer to 0 means more likely ‘no’.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 函数将任何数值转化为0到1之间的值。这使得它非常适合用于是/否的决策，因为我们可以将输出视为一种概率：越接近1意味着越可能是“是”，越接近0意味着越可能是“否”。
- en: '![](../Images/0661018ebd5f06045168823edd7e748a.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0661018ebd5f06045168823edd7e748a.png)'
- en: This process of forward pass turns our input into a prediction between 0 and
    1\. But how good are these predictions? Next, we’ll measure how close our predictions
    are to the correct answers.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这个前向传播过程将我们的输入转化为一个0到1之间的预测值。但是这些预测有多准确呢？接下来，我们将衡量我们的预测与正确答案之间的接近程度。
- en: 'Step 2: Loss Calculation'
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 2：损失计算
- en: Loss function
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数
- en: To check how well our network is doing, we measure the difference between its
    predictions and the correct answers. For binary classification, we use a method
    called **binary cross-entropy** that shows us how far off our predictions are
    from the true values.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查我们的网络表现如何，我们衡量它的预测与正确答案之间的差异。对于二分类问题，我们使用一种叫做 **二元交叉熵** 的方法，它能告诉我们预测值与真实值之间的偏差。
- en: '![](../Images/ff06e056294f4c2bbd55e6a76b6db2ff.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff06e056294f4c2bbd55e6a76b6db2ff.png)'
- en: Math Notation in Neural Network
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络中的数学符号
- en: 'To improve our network’s performance, we’ll need to use some math symbols.
    Let’s define what each symbol means before we continue:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高我们网络的性能，我们需要使用一些数学符号。在继续之前，让我们先定义一下每个符号的含义：
- en: '**Weights and Bias**'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**权重和偏置**'
- en: Weights are represented as matrices and biases as vectors (or 1-dimensional
    matrices). The bracket notation `[1]` indicates the layer number.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 权重表示为矩阵，偏置表示为向量（或一维矩阵）。括号表示法`[1]`表示层的编号。
- en: '![](../Images/a70ec04d0c5d1a4e659e1fc32628bc96.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a70ec04d0c5d1a4e659e1fc32628bc96.png)'
- en: '**Input, Output, Weighted Sum, and Value after Activation**'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入、输出、加权和以及激活后的值**'
- en: The values within nodes can be represented as vectors, forming a consistent
    mathematical framework.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 节点中的值可以表示为向量，从而形成一致的数学框架。
- en: '![](../Images/f18f52954b17091054a53c2ec7e40f8c.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f18f52954b17091054a53c2ec7e40f8c.png)'
- en: '**All Together** These math symbols help us write exactly what our network
    does:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结** 这些数学符号帮助我们精确地描述网络的运作过程：'
- en: '![](../Images/637013282fdcc206f32a296b0ce5d284.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/637013282fdcc206f32a296b0ce5d284.png)'
- en: 'Let’s look at a diagram that shows all the math happening in our network. Each
    layer has:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个图示，展示我们网络中所有数学运算的过程。每一层都有：
- en: Weights (*W*) and biases (*b*) that connect layers
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接层的权重（*W*）和偏置（*b*）
- en: Values before activation (*z*)
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活前的值（*z*）
- en: Values after activation (*a*)
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活后的值（*a*）
- en: Final prediction (*ŷ*) and loss (*L*) at the end
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终预测值（*ŷ*）和损失值（*L*）在末尾
- en: '![](../Images/fc645b406cf6eb07ad93cd55ff59f9ab.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc645b406cf6eb07ad93cd55ff59f9ab.png)'
- en: 'Let’s see exactly what happens at each layer:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看每一层到底发生了什么：
- en: '*First hidden layer*:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '*第一个隐藏层*：'
- en: '*·* Takes our input *x*, multiplies it by weights *W*[1], adds bias *b*[1]
    to get *z*[1]'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*·* 取输入 *x*，与权重 *W*[1] 相乘，加入偏置 *b*[1] 得到 *z*[1]'
- en: '*·* Applies ReLU to *z*[1] to get output *a*[1]'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*·* 对 *z*[1] 应用 ReLU 函数得到输出 *a*[1]'
- en: '*Second hidden layer*:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '*第二个隐藏层*：'
- en: '*·* Takes *a*[1], multiplies by weights *W*[2], adds bias *b*[2] to get *z*[2]'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*·* 取 *a*[1]，与权重 *W*[2] 相乘，加入偏置 *b*[2] 得到 *z*[2]'
- en: '*·* Applies ReLU to *z*[2] to get output *a*[2]'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '*·* 对 *z*[2] 应用 ReLU 函数得到输出 *a*[2]'
- en: '*Output layer*:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出层*：'
- en: '*·* Takes *a*[2], multiplies by weights *W*[3], adds bias *b*[3] to get *z*[3]'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*·* 取 *a*[2]，与权重 *W*[3] 相乘，加入偏置 *b*[3] 得到 *z*[3]'
- en: '*·* Applies sigmoid to *z*[3] to get our final prediction *ŷ*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*·* 对 *z*[3] 应用 sigmoid 来获得我们的最终预测 *ŷ*'
- en: '![](../Images/3c4c3f3539c1ab04fa6f9a66c47f140b.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c4c3f3539c1ab04fa6f9a66c47f140b.png)'
- en: Now that we see all the math in our network, how do we improve these numbers
    to get better predictions? This is where **backpropagation** comes in — it shows
    us how to adjust our weights and biases to make fewer mistakes.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们看到网络中的所有数学公式，如何改进这些数字以获得更好的预测？这时**反向传播**就派上用场了——它向我们展示了如何调整我们的权重和偏差，以减少错误。
- en: 'Step 3: Backpropagation'
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3步：反向传播
- en: 'Before we see how to improve our network, let’s quickly review some math tools
    we’ll need:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们了解如何改进网络之前，让我们快速回顾一下我们需要的一些数学工具：
- en: Derivative
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导数
- en: 'To optimize our neural network, we use **gradients** — a concept closely related
    to derivatives. Let’s review some fundamental derivative rules:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为了优化我们的神经网络，我们使用**梯度**——这是一个与导数密切相关的概念。让我们回顾一下基本的导数规则：
- en: '![](../Images/0ea522923ac7ce3ac21bd2a8e5c18c63.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ea522923ac7ce3ac21bd2a8e5c18c63.png)'
- en: Partial Derivative
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 偏导数
- en: 'Let’s clarify the distinction between regular and partial derivatives:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们澄清常规导数和偏导数之间的区别：
- en: '***Regular Derivative*:** *·* Used when a function has only one variable'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '***常规导数*：* *·* 用于函数只有一个变量的情况'
- en: '*·* Shows how much the function changes when its only variable changes'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '*·* 显示当唯一变量变化时函数的变化量'
- en: '*·* Written as d*f*/d*x*'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*·* 写作 d*f*/d*x*'
- en: '***Partial Derivative***:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '***偏导数***：'
- en: '*·* Used when a function has more than one variable'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '*·* 用于函数有多个变量的情况'
- en: '*·* Shows how much the function changes when one variable changes, while **keeping
    the other variables the same (as constant).**'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*·* 显示当一个变量变化时函数的变化量，同时**保持其他变量不变（视为常数）**。'
- en: '*·* Written as ∂*f*/*∂*x'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '*·* 写作 ∂*f*/*∂*x'
- en: '![](../Images/2f60c208ee4d23e387354ed4a346fc2d.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f60c208ee4d23e387354ed4a346fc2d.png)'
- en: Some examples of partial derivatives
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一些偏导数的例子
- en: Gradient Calculation and Backpropagation
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度计算与反向传播
- en: Returning to our neural network, we need to determine **how to adjust each weight
    and bias** to minimize the error. We can do this using a method called backpropagation,
    which shows us how changing each value affects our network’s errors.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的神经网络，我们需要确定**如何调整每个权重和偏差**以最小化误差。我们可以通过一种叫做反向传播的方法来实现，它向我们展示了改变每个值如何影响网络的误差。
- en: Since backpropagation works backwards through our network, let’s flip our diagram
    upside down to see how this works.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 由于反向传播是通过网络反向进行的，我们可以将图示翻转过来，看看它是如何工作的。
- en: '![](../Images/9f371a97af93a091b3afdbb51105776b.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f371a97af93a091b3afdbb51105776b.png)'
- en: Matrix Rules for Networks
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络的矩阵规则
- en: 'Since our network uses matrices (groups of weights and biases), we need special
    rules to calculate how changes affect our results. Here are two key matrix rules.
    For vectors **v, u** (size 1 × *n*)and matrices **W, X** (size *n* × *n*):'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的网络使用矩阵（权重和偏差的组合），我们需要特殊的规则来计算变化如何影响我们的结果。这里有两个关键的矩阵规则。对于向量**v, u**（大小为1
    × *n*）和矩阵**W, X**（大小为*n* × *n*）：
- en: '*Sum Rule*:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*求和规则*：'
- en: ∂(**W** + **X**)/∂**W** = **I** (Identity matrix, size *n* × *n*)
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ∂(**W** + **X**)/∂**W** = **I**（单位矩阵，大小为*n* × *n*）
- en: ∂(**u** + **v**)/∂**v** = **I** (Identity matrix, size *n* × *n*)
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ∂(**u** + **v**)/∂**v** = **I**（单位矩阵，大小为*n* × *n*）
- en: '*Matrix-Vector Product Rule*:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*矩阵-向量乘积规则*：'
- en: ∂(**vW**)/∂**W** = **v**ᵀ
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ∂(**vW**)/∂**W** = **v**ᵀ
- en: ∂(**vW**)/∂**v** = **W**ᵀ
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ∂(**vW**)/∂**v** = **W**ᵀ
- en: 'Using these rules, we obtain:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些规则，我们得到：
- en: '![](../Images/21a2c76b825f478f0251171b7b979348.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21a2c76b825f478f0251171b7b979348.png)'
- en: '**Activation Function Derivatives** *Derivatives of ReLU*For vectors **a**
    and **z** (size 1 × *n*), where **a** = ReLU(**z**):'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**激活函数的导数** *ReLU的导数*对于向量**a**和**z**（大小为1 × *n*），其中**a** = ReLU(**z**)：'
- en: ∂**a**/∂**z** = diag(**z** > 0)
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ∂**a**/∂**z** = diag(**z** > 0)
- en: 'Creates a diagonal matrix that shows: 1 if input was positive, 0 if input was
    zero or negative.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个对角矩阵，表示：如果输入为正，则为1；如果输入为零或负，则为0。
- en: '*Derivatives of Sigmoid*'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '*Sigmoid 的导数*'
- en: 'For **a** = σ(**z**), where σ is the sigmoid function:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对于**a** = σ(**z**)，其中 σ 是 sigmoid 函数：
- en: ∂**a**/∂**z** = **a** ⊙ (1 **- a**)
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ∂**a**/∂**z** = **a** ⊙ (1 **- a**)
- en: This multiplies elements directly (⊙ means multiply each position).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这直接乘以元素（⊙表示逐位置相乘）。
- en: '![](../Images/406536d3c65893436cf88c0b469d845e.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/406536d3c65893436cf88c0b469d845e.png)'
- en: '**Binary Cross-Entropy Loss Derivative**'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**二进制交叉熵损失函数的导数**'
- en: 'For a single example with loss *L* = -[*y* log(ŷ) + (1-*y*) log(1-*ŷ*)]:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个单独的例子，损失函数为 *L* = -[*y* log(ŷ) + (1-*y*) log(1-*ŷ*)]：
- en: ∂*L*/∂*ŷ* = -(*y*-*ŷ*) / [*ŷ*(1-*ŷ*)]
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ∂*L*/∂*ŷ* = -(*y*-*ŷ*) / [*ŷ*(1-*ŷ*)]
- en: '![](../Images/145aab17537a82e33342299758155d04.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/145aab17537a82e33342299758155d04.png)'
- en: 'Up to this point, we can summarized all the partial derivatives as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们可以将所有的偏导数总结如下：
- en: '![](../Images/2ecaacf0d1a77fe1296855018554dd45.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ecaacf0d1a77fe1296855018554dd45.png)'
- en: 'The following image shows all the partial derivatives that we’ve obtained so
    far:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了我们迄今为止得到的所有偏导数：
- en: '![](../Images/fbe67994cd024b69583a9413901bca26.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fbe67994cd024b69583a9413901bca26.png)'
- en: Chain Rule
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 链式法则
- en: 'In our network, changes flow through multiple steps: a weight affects its layer’s
    output, which affects the next layer, and so on until the final error. The chain
    rule tells us to **multiply these step-by-step changes together** to find how
    each weight and bias affects the final error.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的网络中，变化通过多个步骤进行传播：一个权重影响它所在层的输出，这又影响下一层，依此类推，直到最终误差。链式法则告诉我们**将这些逐步变化相乘**，以找出每个权重和偏置如何影响最终的误差。
- en: '![](../Images/1f8a5d158f28ec911745e2a4e658dc00.png)![](../Images/1f941aa47b9eaad7e064eb594ea454ce.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f8a5d158f28ec911745e2a4e658dc00.png)![](../Images/1f941aa47b9eaad7e064eb594ea454ce.png)'
- en: Error Calculation
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 误差计算
- en: Rather than directly computing weight and bias derivatives, we first calculate
    layer errors ∂*L*/∂*zˡ* (the gradient with respect to pre-activation outputs).
    This makes it easier to then calculate how we should adjust the weights and biases
    in earlier layers.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并不是直接计算权重和偏置的导数，而是首先计算层误差∂*L*/∂*zˡ*（相对于预激活输出的梯度）。这使得后续计算我们应该如何调整早期层的权重和偏置变得更容易。
- en: '![](../Images/6632ead66c14bdace2390adf44c9fd3a.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6632ead66c14bdace2390adf44c9fd3a.png)'
- en: Weight gradients and bias gradients
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 权重梯度和偏置梯度
- en: 'Using these layer errors and the chain rule, we can express the weight and
    bias gradients as:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些层误差和链式法则，我们可以将权重和偏置的梯度表示为：
- en: '![](../Images/2d83604e28f9ecfdd5471d925723a364.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d83604e28f9ecfdd5471d925723a364.png)'
- en: The gradients show us how each value in our network affects our network’s error.
    We then make small changes to these values to help our network make better predictions
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度向我们展示了网络中每个值如何影响网络的误差。我们随后对这些值进行小幅调整，以帮助网络做出更好的预测。
- en: 'Step 4: Weight Update'
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 4：权重更新
- en: Updating weights
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更新权重
- en: Once we know how each weight and bias affects the error (the gradients), we
    improve our network by adjusting these values in the opposite direction of their
    gradients. This reduces the network’s error step by step.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们知道每个权重和偏置如何影响误差（即梯度），我们就通过将这些值调整到与梯度相反的方向来改进我们的网络。这一步步地减少了网络的误差。
- en: '![](../Images/56bd580089de99dc7ce5af8f7a7915d5.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56bd580089de99dc7ce5af8f7a7915d5.png)'
- en: Learning Rate and Optimization
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习率与优化
- en: 'Instead of making big changes all at once, we make small, careful adjustments.
    We use a number called the learning rate (*η*) to control how much we change each
    value:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并不是一次性做出大的变化，而是进行小而谨慎的调整。我们使用一个叫做学习率（*η*）的数值来控制每次调整的幅度：
- en: 'If *η* is too big: The changes are too large and we might make things worse'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果*η*太大：变化过大，可能会导致情况变得更糟
- en: 'If *η* is too small: The changes are tiny and it takes too long to improve'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果*η*太小：变化很小，改进需要很长时间
- en: 'This way of making small, controlled changes is called **Stochastic Gradient
    Descent (SGD)**. We can write it as:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这种通过小幅、可控的调整来优化的方式被称为**随机梯度下降（SGD）**。我们可以将其写作：
- en: '![](../Images/525bd2513df13222570baac0537d0393.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/525bd2513df13222570baac0537d0393.png)'
- en: The value of η (learning rate) is usually chosen to be small, typically ranging
    from 0.1 to 0.0001, to ensure stable learning.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: η（学习率）的值通常选择较小，通常在 0.1 到 0.0001 之间，以确保学习的稳定性。
- en: We just saw how our network learns from **one example.** The network repeats
    all these steps for each example in our dataset, getting better with each round
    of practice
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才看到了网络如何通过**一个示例**进行学习。网络对数据集中每个示例重复这些步骤，在每轮实践中逐步改进。
- en: Summary of Steps
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤总结
- en: 'Here are all the steps we covered to train our network on a single example:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们为训练网络在单个示例上所涉及的所有步骤：
- en: '![](../Images/cf84eac8a9e5bace7343937bcfc86018.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf84eac8a9e5bace7343937bcfc86018.png)'
- en: Scaling to Full Datasets
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展到完整的数据集
- en: Epoch
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迭代
- en: Our network repeats these four steps — forward pass, loss calculation, backpropagation,
    and weight updates — for every example in our dataset. Going through all examples
    once is called **an epoch**.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的网络会对数据集中的每个示例重复这四个步骤——前向传播、损失计算、反向传播和权重更新。遍历所有示例一次称为**一次迭代**。
- en: '![](../Images/2d454eb9ee57344d96212fd863f3b201.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d454eb9ee57344d96212fd863f3b201.png)'
- en: The network usually needs to see all examples many times to get good at its
    task, even up to 1000 times. Each time through helps it learn the patterns better.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 网络通常需要多次看到所有例子，才能熟练掌握任务，甚至多达1000次。每一次训练帮助它更好地学习模式。
- en: '![](../Images/bd6f40a7e518f1a85a5e58d93fec6dae.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd6f40a7e518f1a85a5e58d93fec6dae.png)'
- en: Batch
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批次
- en: 'Instead of learning from one example at a time, our network learns from small
    groups of examples (called **batches**) at once. This has several benefits:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的网络不是一次从一个例子中学习，而是一次从一小组例子（称为**批次**）中学习。这有几个好处：
- en: Works faster
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行更快
- en: Learns better patterns
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习更好的模式
- en: Makes steadier improvements
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稳定地改善
- en: When working with batches, the network looks at all examples in the group before
    making changes. This gives better results than changing values after each single
    example.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理批次时，网络会先查看组内的所有例子，然后再做出改变。这比每看一个例子就改变一次值能得到更好的结果。
- en: '![](../Images/d12014b56111808330a0933a6d0f9811.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d12014b56111808330a0933a6d0f9811.png)'
- en: Testing Step
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试步骤
- en: Preparing Fully-trained Neural Network
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备完全训练好的神经网络
- en: After training is done, our network is ready to make predictions on new examples
    it hasn’t seen before. It uses the same steps as training, but **only needs to
    move forward** through the network to make predictions.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们的网络准备对它从未见过的新例子进行预测。它使用与训练相同的步骤，但**只需向前传播**通过网络进行预测。
- en: Making Predictions
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进行预测
- en: 'When processing new data:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 处理新数据时：
- en: 1\. Input layer takes in the new values
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 输入层接收新值
- en: '2\. At each layer:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 在每一层：
- en: '*·* Multiplies by weights and adds biases'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '*·* 通过权重进行乘法运算并加上偏差'
- en: '*·* Applies the activation function'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '*·* 应用激活函数'
- en: 3\. Output layer generates predictions (e.g., probabilities between 0 and 1
    for binary classification)
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 输出层生成预测（例如，二分类的概率值在0到1之间）
- en: '![](../Images/aa9a06f87d503e7635666859984dd39e.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa9a06f87d503e7635666859984dd39e.png)'
- en: The prediction for ID 9 is 1 (YES).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ID 9的预测结果是1（是）。
- en: Deterministic Nature of Neural Network
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络的确定性特征
- en: When our network sees the same input twice, it will give the same answer both
    times (as long as we haven’t changed its weights and biases). The network’s ability
    to handle new examples comes from its training, not from any randomness in making
    predictions.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的网络两次看到相同的输入时，它会两次给出相同的答案（前提是我们没有改变它的权重和偏差）。网络处理新例子的能力来自于它的训练，而不是在预测时的任何随机性。
- en: Final Remarks
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最后的备注
- en: 'As our network practices with the examples again and again, it gets better
    at its task. It makes fewer mistakes over time, and its predictions get more accurate.
    This is how neural networks learn: look at examples, find mistakes, make small
    improvements, and repeat!'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 随着网络一次又一次地练习这些例子，它会在任务上变得更好。随着时间的推移，它犯的错误越来越少，预测也变得更加准确。这就是神经网络如何学习的过程：查看例子，找到错误，做出小的改进，并不断重复！
- en: 🌟 **Multilayer Perceptron Classifier Code Summary**
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🌟 **多层感知机分类器代码总结**
- en: Now let’s see our neural network in action. Here’s some Python code that builds
    the network we’ve been talking about, using the same structure and rules we just
    learned.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看我们的神经网络是如何运作的。以下是一些Python代码，构建了我们一直在讨论的网络，使用的是我们刚刚学习的相同结构和规则。
- en: '[PRE0]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Want to Learn More?
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 想了解更多？
- en: Check out scikit-learn’s official documentation of [MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)
    for more details and how to use it
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看scikit-learn的[MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)官方文档，了解更多详情及如何使用
- en: This article uses Python 3.7 and scikit-learn 1.5, but the core ideas work with
    other versions too
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本文使用的是Python 3.7和scikit-learn 1.5，但核心理念也适用于其他版本。
- en: Image Attribution
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图片归属
- en: All diagrams and technical illustrations in this article were created by the
    author using licensed design elements from Canva Pro under their commercial license
    terms.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中的所有图表和技术插图都是作者使用Canva Pro的商业许可证条款下授权设计元素制作的。
- en: '𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝘾𝙡𝙖𝙨𝙨𝙞𝙛𝙞𝙘𝙖𝙩𝙞𝙤𝙣 𝘼𝙡𝙜𝙤𝙧𝙞𝙩𝙝𝙢𝙨 𝙝𝙚𝙧𝙚:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝘾𝙡𝙖𝙨𝙨𝙞𝙛𝙞𝙘𝙖𝙩𝙞𝙤𝙣 𝘼𝙡𝙜𝙤𝙧𝙞𝙩𝙝𝙢𝙨 𝙝𝙚𝙧𝙚:'
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----0ae8100c5d1c--------------------------------)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----0ae8100c5d1c--------------------------------)'
- en: Classification Algorithms
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类算法
- en: '[View list](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----0ae8100c5d1c--------------------------------)8
    stories![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----0ae8100c5d1c--------------------------------)8个故事![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)'
- en: '𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:'
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----0ae8100c5d1c--------------------------------)'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----0ae8100c5d1c--------------------------------)'
- en: Regression Algorithms
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归算法
- en: '[View list](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----0ae8100c5d1c--------------------------------)5
    stories![A cartoon doll with pigtails and a pink hat. This “dummy” doll, with
    its basic design and heart-adorned shirt, visually represents the concept of a
    dummy regressor in machine. Just as this toy-like figure is a simplified, static
    representation of a person, a dummy regressor is a basic models serve as baselines
    for more sophisticated analyses.](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)![Samy
    Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----0ae8100c5d1c--------------------------------)5个故事![一个绑着辫子、戴着粉色帽子的卡通娃娃。这只“傀儡”娃娃，凭借其简单的设计和心形图案的衬衫，形象地代表了机器学习中的傀儡回归器概念。就像这个玩具般的人物是一个简化的、静态的人物代表一样，傀儡回归器是用作更复杂分析的基础模型。](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)![Samy
    Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----0ae8100c5d1c--------------------------------)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----0ae8100c5d1c--------------------------------)'
- en: Ensemble Learning
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成学习
- en: '[View list](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----0ae8100c5d1c--------------------------------)4
    stories![](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----0ae8100c5d1c--------------------------------)4个故事![](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)'
