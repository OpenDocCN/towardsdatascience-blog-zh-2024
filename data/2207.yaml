- en: How Tiny Neural Networks Represent Basic Functions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 小型神经网络如何表示基本函数
- en: 原文：[https://towardsdatascience.com/how-tiny-neural-networks-represent-basic-functions-8a24fce0e2d5?source=collection_archive---------4-----------------------#2024-09-10](https://towardsdatascience.com/how-tiny-neural-networks-represent-basic-functions-8a24fce0e2d5?source=collection_archive---------4-----------------------#2024-09-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-tiny-neural-networks-represent-basic-functions-8a24fce0e2d5?source=collection_archive---------4-----------------------#2024-09-10](https://towardsdatascience.com/how-tiny-neural-networks-represent-basic-functions-8a24fce0e2d5?source=collection_archive---------4-----------------------#2024-09-10)
- en: A gentle introduction to mechanistic interpretability through simple algorithmic
    examples
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过简单的算法示例，温和地介绍机械解释性
- en: '[](https://medium.com/@taubenfeld9?source=post_page---byline--8a24fce0e2d5--------------------------------)[![Amir
    Taubenfeld](../Images/524631457f02f7193aeb2d4b03c5c3a4.png)](https://medium.com/@taubenfeld9?source=post_page---byline--8a24fce0e2d5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8a24fce0e2d5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8a24fce0e2d5--------------------------------)
    [Amir Taubenfeld](https://medium.com/@taubenfeld9?source=post_page---byline--8a24fce0e2d5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@taubenfeld9?source=post_page---byline--8a24fce0e2d5--------------------------------)[![Amir
    Taubenfeld](../Images/524631457f02f7193aeb2d4b03c5c3a4.png)](https://medium.com/@taubenfeld9?source=post_page---byline--8a24fce0e2d5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8a24fce0e2d5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8a24fce0e2d5--------------------------------)
    [Amir Taubenfeld](https://medium.com/@taubenfeld9?source=post_page---byline--8a24fce0e2d5--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8a24fce0e2d5--------------------------------)
    ·9 min read·Sep 10, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8a24fce0e2d5--------------------------------)
    ·阅读时长 9 分钟 ·2024年9月10日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/b630b068a5309d3c370c60452a0eab18.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b630b068a5309d3c370c60452a0eab18.png)'
- en: Introduction
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: This article shows how small Artificial Neural Networks (NN) can represent basic
    functions. The goal is to provide fundamental intuition about how NNs work and
    to serve as a gentle introduction to [Mechanistic Interpretability](https://transformer-circuits.pub/2022/mech-interp-essay/index.html)
    — a field that seeks to reverse engineer NNs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文展示了小型人工神经网络（NN）如何表示基本函数。目标是提供关于神经网络如何工作的基本直觉，并作为[机械解释性](https://transformer-circuits.pub/2022/mech-interp-essay/index.html)的温和介绍——这是一个旨在逆向工程神经网络的领域。
- en: I present three examples of elementary functions, describe each using a simple
    algorithm, and show how the algorithm can be “coded” into the weights of a neural
    network. Then, I explore if the network can learn the algorithm using backpropagation.
    I encourage readers to think about each example as a riddle and take a minute
    before reading the solution.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我展示了三个基本函数示例，使用简单算法描述每个示例，并展示这些算法如何“编码”到神经网络的权重中。然后，我探讨了网络是否能通过反向传播学习这些算法。我鼓励读者将每个示例视为一个谜题，在阅读解答之前稍作思考。
- en: Machine Learning Topology
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习拓扑
- en: This article attempts to break NNs into discrete operations and describe them
    as algorithms. An alternative approach, perhaps more common and natural, is looking
    at the continuous topological interpretations of the linear transformations in
    different layers.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文试图将神经网络（NN）分解为离散操作，并将其描述为算法。另一种可能更常见且自然的方法是，观察不同层中线性变换的连续拓扑解释。
- en: 'The following are some great resources for strengthening your topological intuition:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些有助于增强拓扑直觉的优秀资源：
- en: '[Tensorflow Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.91521&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)
    — a simple tool for building basic intuition on classification tasks.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Tensorflow Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.91521&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)
    — 一个用于建立分类任务基本直觉的简单工具。'
- en: '[ConvnetJS Demo](https://cs.stanford.edu/people/karpathy/convnetjs//demo/classify2d.html)
    — a more sophisticated tool for visualizing NNs for classification tasks.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ConvnetJS 演示](https://cs.stanford.edu/people/karpathy/convnetjs//demo/classify2d.html)
    — 一个更复杂的工具，用于可视化神经网络在分类任务中的表现。'
- en: '[Neural Networks, Manifolds, and Topology](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)
    — a great article for building topological intuition of how NNs work.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[神经网络、流形与拓扑](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/) —
    一篇很好的文章，帮助建立神经网络如何工作的拓扑直觉。'
- en: Three Elementary Functions
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 三个基本函数
- en: 'In all the following examples, I use the terminology “neuron” for a single
    node in the NN computation graph. Each neuron can be used only once (no cycles;
    e.g., not RNN), and it performs 3 operations in the following order:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下所有示例中，我将“神经元”一词用于神经网络计算图中的单个节点。每个神经元只能使用一次（没有循环；例如，不能是 RNN），它执行以下三个操作，顺序如下：
- en: Inner product with the input vector.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与输入向量的内积。
- en: Adding a bias term.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加偏置项。
- en: Running a (non-linear) activation function.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行一个（非线性）激活函数。
- en: '![](../Images/e09ea44f923e70c557601eb3f9fec623.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e09ea44f923e70c557601eb3f9fec623.png)'
- en: I provide only minimal code snippets so that reading will be fluent. This [Colab
    notebook](https://colab.research.google.com/drive/1zt9lVUH9jH2zx5nsFA_4Taq6Ic-ve09C?usp=sharing)
    includes the entire code.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我仅提供最简洁的代码片段，以便阅读更加流畅。这个 [Colab 笔记本](https://colab.research.google.com/drive/1zt9lVUH9jH2zx5nsFA_4Taq6Ic-ve09C?usp=sharing)
    包含了完整的代码。
- en: The < operator
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: < 操作符
- en: How many neurons are required to learn the function “x < 10”? Write an NN that
    returns 1 when the input is smaller than 10 and 0 otherwise.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 需要多少个神经元来学习“x < 10”的函数？编写一个神经网络，当输入小于 10 时返回 1，否则返回 0。
- en: Solution
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Let’s start by creating sample dataset that follows the pattern we want to learn
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从创建一个符合我们想要学习的模式的示例数据集开始
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/ce6a710b63592e3c4a81d994798b4d0b.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce6a710b63592e3c4a81d994798b4d0b.png)'
- en: Creating and visualizing the training data for “< operator”
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 创建并可视化“< 操作符”的训练数据
- en: This classification task can be solved using [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression)
    and a [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) as the output
    activation. Using a single neuron, we can write the function as *Sigmoid(ax+b)*.
    *b*, the bias term, can be thought of as the neuron’s threshold. Intuitively,
    we can set *b = 10* and *a = -1* and get F=Sigmoid(10-x)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分类任务可以使用 [逻辑回归](https://en.wikipedia.org/wiki/Logistic_regression) 和 [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function)
    作为输出激活函数来解决。使用一个神经元，我们可以将函数写成 *Sigmoid(ax+b)*。*b*，偏置项，可以被认为是神经元的阈值。直观地，我们可以设定
    *b = 10* 和 *a = -1*，得到 F=Sigmoid(10-x)
- en: Let’s implement and run F using PyTorch
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 PyTorch 实现并运行 F
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/6fa1dfe433dfc0d2ff2709f8d11ca509.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6fa1dfe433dfc0d2ff2709f8d11ca509.png)'
- en: Sigmoid(10-x)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid(10-x)
- en: Seems like the right pattern, but can we make a tighter approximation? For example,
    F(9.5) = 0.62, we prefer it to be closer to 1.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来是正确的模式，但我们能做出更精确的近似吗？例如，F(9.5) = 0.62，我们希望它接近 1。
- en: For the Sigmoid function, as the input approaches -∞ / ∞ the output approaches
    0 / 1 respectively. Therefore, we need to make our 10 — x function return large
    numbers, which can be done by multiplying it by a larger number, say 100, to get
    F=Sigmoid(100(10-x)), now we’ll get F(9.5) =~1.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Sigmoid 函数，当输入接近 -∞ / ∞ 时，输出分别接近 0 / 1。因此，我们需要让我们的 10 - x 函数返回较大的数值，这可以通过将其乘以一个更大的数，比如
    100，来实现，得到 F=Sigmoid(100(10-x))，现在我们会得到 F(9.5) =~1。
- en: '![](../Images/51a9a623419207f3eb6a22cbb29d49c2.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51a9a623419207f3eb6a22cbb29d49c2.png)'
- en: Sigmoid(100(10-x))
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid(100(10-x))
- en: Indeed, when training a network with one neuron, it converges to F=Sigmoid(M(10-x)),
    where M is a scalar that keeps growing during training to make the approximation
    tighter.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 的确，当使用一个神经元训练网络时，它会收敛到 F=Sigmoid(M(10-x))，其中 M 是一个标量，在训练过程中不断增大，使得近似更加精确。
- en: '![](../Images/0dcb409478843393739eecd0584364c6.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0dcb409478843393739eecd0584364c6.png)'
- en: Tensorboard graph — the X-axis represents the number of training epochs and
    the Y-axis represents the value of the bias and the weight of the network. The
    bias and the weight increase/decrease in reverse proportion. That is, the network
    can be written as M(10-x) where M is a parameter that keeps growing during training.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Tensorboard 图 — X 轴表示训练轮次，Y 轴表示网络的偏置和权重值。偏置和权重成反比例增加/减少。也就是说，网络可以写成 M(10-x)，其中
    M 是一个在训练过程中不断增长的参数。
- en: To clarify, our single-neuron model is only an approximation of the “<10” function.
    We will never be able to reach a loss of zero, because the neuron is a continuous
    function while “<10” is not a continuous function.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了澄清，我们的单神经元模型仅仅是“<10”函数的一个近似。我们永远无法达到零损失，因为神经元是一个连续函数，而“<10”不是一个连续函数。
- en: Min(a, b)
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Min(a, b)
- en: Write a neural network that takes two numbers and returns the minimum between
    them.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 编写一个神经网络，输入两个数字，返回它们之间的最小值。
- en: Solution
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Like before, let’s start by creating a test dataset and visualizing it
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，让我们先创建一个测试数据集并进行可视化
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/18366ae36e3e23b2e17677c371e9177b.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18366ae36e3e23b2e17677c371e9177b.png)'
- en: Visualizing the training data for Min(a, b). The two horizontal axes represent
    the coordinates of the input. The vertical axis labeled as “Ground Truth” is the
    expected output — i.e., the minimum of the two input coordinates
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化 Min(a, b) 的训练数据。两个横轴表示输入的坐标。垂直轴标记为“Ground Truth”，即预期输出——即两个输入坐标的最小值
- en: In this case, ReLU activation is a good candidate because it is essentially
    a maximum function (ReLU(x) = max(0, x)). Indeed, using ReLU one can write the
    min function as follows
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，ReLU 激活是一个不错的选择，因为它本质上是一个最大值函数（ReLU(x) = max(0, x)）。事实上，使用 ReLU 可以将最小值函数写成如下形式
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '***[Equation 1]***'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '***[方程 1]***'
- en: Now let’s build a small network that is capable of learning *Equation 1*, and
    try to train it using gradient descent
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们构建一个小型网络，能够学习*方程 1*，并尝试使用梯度下降训练它
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/dc630403474bf9b5e81f7943d638e384.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc630403474bf9b5e81f7943d638e384.png)'
- en: Visualization of the MinModel computation graph. Drawing was done using the
    [Torchview](https://github.com/mert-kurttutan/torchview) library
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: MinModel 计算图的可视化。绘图使用了 [Torchview](https://github.com/mert-kurttutan/torchview)
    库
- en: Training for 300 epochs is enough to converge. Let’s look at the model’s parameters
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 训练 300 个周期足以收敛。让我们看看模型的参数
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Many weights are zeroing out, and we are left with the nicely looking
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 许多权重被归零，我们剩下的结果看起来相当不错
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This is not the solution we expected, but it is a valid solution and even **cleaner
    than Equation 1!** By looking at the network we learned a new nicely looking formula!
    Proof:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是我们预期的解决方案，但它是一个有效的解决方案，甚至比*方程 1*更**简洁**！通过观察这个网络，我们学到了一个新的、看起来不错的公式！证明：
- en: 'Proof:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 证明：
- en: 'If *a <= b: model([a,b]) = a — ReLU(a-b) = a — 0 = a*'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '如果 *a <= b: model([a,b]) = a — ReLU(a-b) = a — 0 = a*'
- en: 'If *a > b: a — ReLU(a-b) = a — (a-b) = b*'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '如果 *a > b: a — ReLU(a-b) = a — (a-b) = b*'
- en: Is even?
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 是否为偶数？
- en: Create a neural network that takes an integer x as an input and returns x mod
    2\. That is, 0 if x is even, 1 if x is odd.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个神经网络，输入一个整数 x，返回 x mod 2。也就是说，如果 x 是偶数，输出 0；如果 x 是奇数，输出 1。
- en: This one looks quite simple, but surprisingly it is impossible to create a finite-size
    network that correctly classifies each integer in (-∞, ∞) (using a standard non-periodic
    activation function such as ReLU).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这个看起来相当简单，但出人意料的是，使用标准的非周期性激活函数（如 ReLU），无法创建一个有限大小的网络，正确分类 (-∞, ∞) 范围内的每个整数。
- en: '***Theorem: is_even needs at least log neurons***'
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '***定理：is_even 需要至少 log 个神经元***'
- en: '*A network with ReLU activations requires at least n neurons to correctly classify
    each of 2^n consecutive natural numbers as even or odd (i.e., solving is_even).*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*一个带有 ReLU 激活函数的网络至少需要 n 个神经元，才能正确地将每个 2^n 连续自然数分类为偶数或奇数（即解决 is_even 问题）。*'
- en: '***Proof: Using Induction***'
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '***证明：使用归纳法***'
- en: '**Base: n == 2:** Intuitively, a single neuron (of the form *ReLU(ax + b)*),
    cannot solve *S = [i + 1, i + 2, i + 3, i + 4]* as it is not linearly separable.
    For example, without loss of generality, assume *a > 0* and *i + 2* is even*.*
    If *ReLU(a(i + 2) + b) = 0,* then also *ReLU(a(i + 1) + b) = 0* (monotonic function)*,*
    but *i + 1* is odd.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**基础：n == 2：** 直观地说，单个神经元（形如 *ReLU(ax + b)*）无法解决 *S = [i + 1, i + 2, i + 3,
    i + 4]*，因为它不是线性可分的。例如，假设 *a > 0* 且 *i + 2* 是偶数。如果 *ReLU(a(i + 2) + b) = 0,* 那么
    *ReLU(a(i + 1) + b) = 0*（单调函数），但 *i + 1* 是奇数。'
- en: More [details](https://en.wikipedia.org/wiki/Perceptrons_(book)#The_XOR_affair)
    are included in the classic Perceptrons book.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 更多的[细节](https://en.wikipedia.org/wiki/Perceptrons_(book)#The_XOR_affair)可以在经典的《感知机》书中找到。
- en: '**Assume for n, and look at n+1:** *Let S = [i + 1, …, i + 2^(n + 1)]*, and
    assume, for the sake of contradiction, that *S* can be solved using a network
    of size *n*. Take an input neuron from the first layer *f(x) = ReLU(ax + b)*,
    where *x* is the input to the network. *WLOG a > 0*. Based on the definition of
    ReLU there exists a *j* such that:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**假设对于 n，观察 n+1：** *设 S = [i + 1, …, i + 2^(n + 1)]*，并假设为了矛盾，*S* 可以用大小为 *n*
    的网络解决。从第一层的输入神经元 *f(x) = ReLU(ax + b)* 开始，其中 *x* 是网络的输入。*WLOG a > 0*。根据 ReLU 的定义，存在一个
    *j*，使得：'
- en: '*S’ = [i + 1, …, i + j], S’’ = [i + j + 1, …, i + 2^(n + 1)]'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*S’ = [i + 1, …, i + j], S’’ = [i + j + 1, …, i + 2^(n + 1)]*'
- en: f(x ≤ i) = 0
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: f(x ≤ i) = 0
- en: f(x ≥ i) = ax + b*
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: f(x ≥ i) = ax + b*
- en: 'There are two cases to consider:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 需要考虑两种情况：
- en: 'Case *|S’| ≥ 2^n*: dropping *f* and all its edges won’t change the classification
    results of the network on S’. Hence, there is a network of size *n-1* that solves
    S’. Contradiction.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情况 *|S’| ≥ 2^n*：删除 *f* 及其所有边缘不会改变网络在 S’ 上的分类结果。因此，存在一个大小为 *n-1* 的网络解决了 S’。矛盾。
- en: 'Case *|S’’|≥ 2^n*: For each neuron *g* which takes *f* as an input *g(x) =*
    *ReLU(cf(x) + d + …) = ReLU(c ReLU(ax + b) + d + …)*, Drop the neuron *f* and
    wire *x* directly to *g*, to get *ReLU(cax + cb + d + …)*. A network of size *n
    — 1* solves *S’’*. Contradiction.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情况 *|S’’|≥ 2^n*：对于每个神经元 *g*，它将 *f* 作为输入 *g(x) =* *ReLU(cf(x) + d + …) = ReLU(c
    ReLU(ax + b) + d + …)*，删除神经元 *f* 并直接将 *x* 连接到 *g*，得到 *ReLU(cax + cb + d + …)*。一个大小为
    *n — 1* 的网络解决了 *S’’*。矛盾。
- en: Logarithmic Algorithm
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对数算法
- en: '*How many neurons are sufficient to classify [1, 2^n]? I have proven that n
    neurons are necessary. Next, I will show that n neurons are also sufficient.*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*需要多少个神经元才能分类 [1, 2^n]？我已经证明了 n 个神经元是必要的。接下来，我将证明 n 个神经元也是足够的。*'
- en: 'One simple implementation is a network that constantly adds/subtracts 2, and
    checks if at some point it reaches 0\. This will require O(*2^n*) neurons. A more
    efficient algorithm is to add/subtract powers of 2, which will require only O(n)
    neurons. More formally:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的实现是一个不断加减 2 的网络，并检查是否在某一时刻达到 0。这将需要 O(*2^n*) 个神经元。一个更高效的算法是加减 2 的幂，这将只需要
    O(n) 个神经元。更正式地：
- en: '*f_i(x) := |x — i|'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*f_i(x) := |x — i|*'
- en: f(x) := f_1∘ f_1∘ f_2 ∘ f_4∘ … ∘ f_(2^(n-1)) (|x|)*
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: f(x) := f_1∘ f_1∘ f_2 ∘ f_4∘ … ∘ f_(2^(n-1)) (|x|)*
- en: 'Proof:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 证明：
- en: 'By definition:*∀ x ϵ[0, 2^i]: f_(2^(i-1)) (x) ≤ 2^(i-1).'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '根据定义：*∀ x ϵ[0, 2^i]: f_(2^(i-1)) (x) ≤ 2^(i-1)。'
- en: I.e., cuts the interval by half.*
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 即，将区间一分为二。*
- en: Recursively *f_1∘ f_1∘ f_2 ∘ … ∘ f_(2^(n-1)) (|x|)* ≤ 1
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 递归地 *f_1∘ f_1∘ f_2 ∘ … ∘ f_(2^(n-1)) (|x|)* ≤ 1
- en: 'For every even *i: is_even(f_i(x)) = is_even(x)*'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '对于每个偶数 *i: is_even(f_i(x)) = is_even(x)*'
- en: Similarly *is_even(f_1( f_1(x))) = is_even(x)*
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同样，*is_even(f_1( f_1(x))) = is_even(x)*
- en: We got *f(x) ϵ {0,1}* and *is_even(x) =is_even(f(x))*. QED.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们得到了 *f(x) ϵ {0,1}* 且 *is_even(x) =is_even(f(x))*。证毕。
- en: Implementation
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现
- en: Let’s try to implement this algorithm using a neural network over a small domain.
    We start again by defining the data.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用神经网络在一个小的范围内实现这个算法。我们再次从定义数据开始。
- en: '[PRE7]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/86b27d598900ae146704f82eb3407f1a.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/86b27d598900ae146704f82eb3407f1a.png)'
- en: is_even data and labels on a small domain [0, 15]
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在小范围 [0, 15] 上的 is_even 数据和标签
- en: Because the domain contains 2⁴ integers, we need to use 6 neurons. 5 for *f_1∘
    f_1∘ f_2 ∘ f_4∘ f_8,* + 1 output neuron. Let’s build the network and hardwire
    the weights
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 因为该范围包含 2⁴ 个整数，所以我们需要使用 6 个神经元。5 个用于 *f_1∘ f_1∘ f_2 ∘ f_4∘ f_8,* + 1 个输出神经元。让我们构建网络并硬编码权重。
- en: '[PRE8]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As expected we can see that this model makes a perfect prediction on [0,15]
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，我们可以看到该模型在 [0,15] 范围内做出了完美的预测。
- en: '![](../Images/718cd603a22d3873bcdf0ec0e4fa2563.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/718cd603a22d3873bcdf0ec0e4fa2563.png)'
- en: And, as expected, it doesn’t generalizes to new data points
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，它无法推广到新的数据点。
- en: '![](../Images/e790223ec60b4810cba422450a248582.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e790223ec60b4810cba422450a248582.png)'
- en: We saw that we can hardwire the model, but would the model converge to the same
    solution using gradient descent?
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到我们可以硬编码该模型，但使用梯度下降法时，模型是否会收敛到相同的解呢？
- en: '![](../Images/bd16479c84a320c2f4132e891dfb5f7d.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd16479c84a320c2f4132e891dfb5f7d.png)'
- en: The answer is — not so easily! Instead, it is stuck at a local minimum — predicting
    the mean.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是——并非如此简单！相反，它卡在了一个局部最小值——预测均值。
- en: This is a known phenomenon, where gradient descent can get stuck at a local
    minimum. It is especially prevalent for non-smooth error surfaces of highly nonlinear
    functions (such as is_even).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个已知现象，其中梯度下降可能会卡在局部最小值。它在非光滑的错误面上，特别是对于高度非线性的函数（如 is_even）更为常见。
- en: 'More details are beyond the scope of this article, but to get more intuition
    one can look at the many works that investigated the classic XOR problem. Even
    for such a simple problem, we can see that gradient descent can struggle to find
    a solution. In particular, I recommend Richard Bland’s short [book](https://www.cs.stir.ac.uk/~kjt/techreps/pdf/TR148.pdf)
    “Learning XOR: exploring the space of a classic problem” — a rigorous analysis
    of the error surface of the XOR problem.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 更多细节超出了本文的范围，但为了获得更多直观理解，可以参考许多研究经典XOR问题的工作。即使是这样一个简单的问题，我们也可以看到梯度下降法在找到解决方案时可能会遇到困难。特别是，我推荐理查德·布兰德的短篇[书籍](https://www.cs.stir.ac.uk/~kjt/techreps/pdf/TR148.pdf)《学习XOR：探索经典问题的空间》——这是对XOR问题误差表面的严谨分析。
- en: Final Words
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结语
- en: I hope this article has helped you understand the basic structure of small neural
    networks. Analyzing Large Language Models is much more complex, but it’s an area
    of research that is advancing rapidly and is full of intriguing challenges.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这篇文章帮助你理解了小型神经网络的基本结构。分析大型语言模型要复杂得多，但这是一个快速发展的研究领域，充满了引人入胜的挑战。
- en: When working with Large Language Models, it’s easy to focus on supplying data
    and computing power to achieve impressive results without understanding how they
    operate. However, interpretability offers crucial insights that can help address
    issues like fairness, inclusivity, and accuracy, which are becoming increasingly
    vital as we rely more on LLMs in decision-making.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用大型语言模型时，很容易专注于提供数据和计算能力，以实现令人印象深刻的结果，而不理解它们的运作方式。然而，解释性提供了关键的洞察力，能够帮助解决公平性、包容性和准确性等问题，这些问题在我们越来越依赖LLM做决策时变得愈加重要。
- en: For further exploration, I recommend following the [AI Alignment Forum](https://www.alignmentforum.org/).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步探索，我推荐关注[AI对齐论坛](https://www.alignmentforum.org/)。
- en: '*All the images were created by the author. The intro image was created using
    ChatGPT and the rest were created using Python libraries.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*所有图片均由作者创作。介绍图是使用ChatGPT创作的，其余图像是使用Python库创建的。'
