- en: 'Understanding LoRA Part I: Exploring Intrinsic Dimensions'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解LoRA 第一部分：探索内在维度
- en: 原文：[https://towardsdatascience.com/lora-intrinsic-dimensions-introduction-6ba84c727c2e?source=collection_archive---------10-----------------------#2024-10-31](https://towardsdatascience.com/lora-intrinsic-dimensions-introduction-6ba84c727c2e?source=collection_archive---------10-----------------------#2024-10-31)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/lora-intrinsic-dimensions-introduction-6ba84c727c2e?source=collection_archive---------10-----------------------#2024-10-31](https://towardsdatascience.com/lora-intrinsic-dimensions-introduction-6ba84c727c2e?source=collection_archive---------10-----------------------#2024-10-31)
- en: Efficient fine-tuning techniques for Language Models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高效的语言模型微调技术
- en: '[](https://rojagtap.medium.com/?source=post_page---byline--6ba84c727c2e--------------------------------)[![Rohan
    Jagtap](../Images/3b33556ab4c4a5122bd2120789c4dd1d.png)](https://rojagtap.medium.com/?source=post_page---byline--6ba84c727c2e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6ba84c727c2e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6ba84c727c2e--------------------------------)
    [Rohan Jagtap](https://rojagtap.medium.com/?source=post_page---byline--6ba84c727c2e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://rojagtap.medium.com/?source=post_page---byline--6ba84c727c2e--------------------------------)[![Rohan
    Jagtap](../Images/3b33556ab4c4a5122bd2120789c4dd1d.png)](https://rojagtap.medium.com/?source=post_page---byline--6ba84c727c2e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6ba84c727c2e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6ba84c727c2e--------------------------------)
    [Rohan Jagtap](https://rojagtap.medium.com/?source=post_page---byline--6ba84c727c2e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6ba84c727c2e--------------------------------)
    ·12 min read·Oct 31, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6ba84c727c2e--------------------------------)
    ·12分钟阅读·2024年10月31日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/baefbe90e0e70e99d00e90863c2a3556.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/baefbe90e0e70e99d00e90863c2a3556.png)'
- en: Feature image by ChatGPT
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：ChatGPT
- en: LoRA (Low-Rank Adaptation) has quickly become the de facto method for efficient
    fine-tuning of large language models. It offers a lightweight approach to adapting
    pre-trained models, significantly reducing the computational cost and memory requirements
    of traditional fine-tuning methods.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA（低秩适配）迅速成为大型语言模型高效微调的事实标准方法。它为适应预训练模型提供了一种轻量级的方法，大大降低了传统微调方法的计算成本和内存要求。
- en: 'LoRA was introduced in the paper [Hu, Edward J., et al., “LoRA: Low-Rank Adaptation
    of Large Language Models](https://arxiv.org/abs/2106.09685),” which takes its
    inspiration primarily from two ideas:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 'LoRA 在论文 [Hu, Edward J., 等人，“LoRA: 大型语言模型的低秩适配”](https://arxiv.org/abs/2106.09685)
    中提出，其灵感主要来源于两个观点：'
- en: '[Li et al., “Measuring the Intrinsic Dimension of Objective Landscapes”](https://arxiv.org/abs/1804.08838)'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Li 等人，“测量目标景观的内在维度”](https://arxiv.org/abs/1804.08838)'
- en: '[Aghajanyan et al., “Intrinsic Dimensionality Explains the Effectiveness of
    Language Model Fine-Tuning”](https://arxiv.org/abs/2012.13255)'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Aghajanyan 等人，“内在维度解释了语言模型微调的有效性”](https://arxiv.org/abs/2012.13255)'
- en: In this series of three articles, I’ll be covering each of these ideas in depth,
    and finally, LoRA itself. This will help understand not only what LoRA is, but
    how the authors came up with it.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一系列三篇文章中，我将深入探讨这些思想，最后介绍LoRA本身。这将有助于我们不仅理解LoRA是什么，还能了解作者是如何提出这一方法的。
- en: In this article, we will be talking about the fundamental inspiration behind
    LoRA — **intrinsic dimensions**. We will try to understand what the intrinsic
    dimension is and how it applies to various deep learning tasks, as described in
    [**Li et al., “Measuring the Intrinsic Dimension of Objective Landscapes.”**](https://arxiv.org/abs/1804.08838)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将讨论LoRA背后的基本灵感——**内在维度**。我们将尝试理解内在维度是什么，以及它如何应用于各种深度学习任务，如在[**Li 等人，“测量目标景观的内在维度”**](https://arxiv.org/abs/1804.08838)中所描述的那样。
