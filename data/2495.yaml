- en: Linear Discriminant Analysis (LDA)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性判别分析（LDA）
- en: 原文：[https://towardsdatascience.com/linear-discriminant-analysis-lda-598d8e90f8b9?source=collection_archive---------3-----------------------#2024-10-12](https://towardsdatascience.com/linear-discriminant-analysis-lda-598d8e90f8b9?source=collection_archive---------3-----------------------#2024-10-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/linear-discriminant-analysis-lda-598d8e90f8b9?source=collection_archive---------3-----------------------#2024-10-12](https://towardsdatascience.com/linear-discriminant-analysis-lda-598d8e90f8b9?source=collection_archive---------3-----------------------#2024-10-12)
- en: Discover how LDA helps identify critical data features
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发现LDA如何帮助识别关键数据特征
- en: '[](https://medium.com/@ingo.nowitzky?source=post_page---byline--598d8e90f8b9--------------------------------)[![Ingo
    Nowitzky](../Images/00d3560055109732b871c001d2b51ab5.png)](https://medium.com/@ingo.nowitzky?source=post_page---byline--598d8e90f8b9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--598d8e90f8b9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--598d8e90f8b9--------------------------------)
    [Ingo Nowitzky](https://medium.com/@ingo.nowitzky?source=post_page---byline--598d8e90f8b9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ingo.nowitzky?source=post_page---byline--598d8e90f8b9--------------------------------)[![Ingo
    Nowitzky](../Images/00d3560055109732b871c001d2b51ab5.png)](https://medium.com/@ingo.nowitzky?source=post_page---byline--598d8e90f8b9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--598d8e90f8b9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--598d8e90f8b9--------------------------------)
    [Ingo Nowitzky](https://medium.com/@ingo.nowitzky?source=post_page---byline--598d8e90f8b9--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--598d8e90f8b9--------------------------------)
    ·12 min read·Oct 12, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--598d8e90f8b9--------------------------------)
    ·阅读时间12分钟·2024年10月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/5a54046c9d6695be140657cea1a5f7f7.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a54046c9d6695be140657cea1a5f7f7.png)'
- en: Classification of LDA within AI and ML Methods | image by author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: LDA在人工智能和机器学习方法中的分类 | 作者提供的图片
- en: '**This article aims to explore Linear Discriminant Analysis (LDA), focusing
    on its core ideas, its mathematical implementation in code, and a practical example
    from manufacturing.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**本文旨在探讨线性判别分析（LDA），重点介绍其核心思想、在代码中的数学实现以及来自制造业的实际例子。'
- en: I hope you’re on board. Let’s get started!**
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你能参与进来。让我们开始吧！**
- en: 'Who works with **industrial data** in practice will be familiar with this situation:
    The datasets usually have many features, and it is often unclear which of the
    features are important and which are less. “Important” is a relative term in this
    context. Often, the goal is to differentiate the datasets from each other, i.e.,
    to classify them. A very typical task is to distinguish good parts from bad parts
    and to identify the causes (aka features) that lead to the failure of the parts.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**从事**工业数据**工作的实践者会熟悉这种情况：数据集通常包含许多特征，而且通常不清楚哪些特征重要，哪些特征较不重要。在此上下文中，“重要”是一个相对的概念。通常，目标是区分数据集之间的差异，即对其进行分类。一个非常典型的任务是区分好的零件和坏的零件，并找出导致零件失败的原因（即特征）。'
- en: A commonly used method is the well-known Principal Component Analysis (PCA).
    While PCA belongs to the **un**supervised methods, the less widespread LDA is
    a supervised method and thus learns from **labeled data**. Therefore, it is particularly
    suited for explaining failure patterns from large datasets.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常用的方法是广为人知的主成分分析（PCA）。虽然PCA属于**无**监督方法，但较少使用的LDA是一种有监督的方法，因此它从**标记数据**中学习。因此，LDA特别适用于解释来自大数据集的故障模式。
- en: 1\. Goal and Principle of LDA
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. LDA的目标和原理
- en: The goal of LDA is to linearly combine the features of the data so that the
    labels of the datasets are best separated from each other, and the number of new
    features is reduced to a predefined count. In AI jargon, this is typically referred
    to as a **projection to a lower-dimensional space**.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: LDA的目标是线性地组合数据的特征，使得数据集的标签能够彼此最佳分离，并且新特征的数量被减少到预定义的数量。在人工智能术语中，这通常被称为**投影到低维空间**。
- en: '![](../Images/dd8246e1f49c04fee04de426915216e6.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd8246e1f49c04fee04de426915216e6.png)'
- en: Principle of LDA | image modified from [Raschka/Mirjalili, 2019](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch05/images/05_06.png)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: LDA原理 | 图片修改自[Raschka/Mirjalili, 2019](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch05/images/05_06.png)
- en: 'Excursus: What is dimensionality and what is dimensionality reduction?'
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 旁白：什么是维度，什么是维度降维？
- en: '![](../Images/0445b3df24a1a26401d95b2d0687883c.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0445b3df24a1a26401d95b2d0687883c.png)'
- en: Dimensions and graphical representation | image by author
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 维度与图形表示 | 图片由作者提供
- en: Dimensionality refers to the number of features in a dataset.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 维度指的是数据集中特征的数量。
- en: 'With just one measurement (or feature), such as the tool temperature from an
    injection molding machine, we can represent it on a number line. Two features,
    like temperature and tool pressure, are still manageable: we can easily plot the
    data on an x-y chart. With three features — temperature, tool pressure, and injection
    pressure — things get more interesting, but we can still plot the data in a 3D
    x-y-z chart. However, when we add more features, such as viscosity, electrical
    conductivity, and others, the complexity increases.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 仅凭一个测量（或特征），例如注塑机的工具温度，我们可以将其表示在数轴上。两个特征，比如温度和工具压力，仍然可以处理：我们可以轻松地在 x-y 图表上绘制数据。三个特征——温度、工具压力和注射压力——会变得更加复杂，但我们仍然可以在
    3D x-y-z 图表上绘制数据。然而，当我们添加更多特征，例如粘度、电导率等时，复杂性会增加。
- en: '![](../Images/08dee7410ef3334e826510e464e4477e.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08dee7410ef3334e826510e464e4477e.png)'
- en: Dimensionality reduction | image by author
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 降维 | 图片由作者提供
- en: In practice, datasets often contain hundreds or even thousands of features.
    This presents a challenge because many machine learning algorithms perform poorly
    as datasets grow too large. Additionally, the amount of data required increases
    exponentially with the number of dimensions to achieve statistical significance.
    This phenomenon is known as the “curse of dimensionality.” These factors make
    it essential to determine which features are relevant and to eliminate the less
    meaningful ones early in the data science process.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，数据集通常包含数百甚至数千个特征。这带来了挑战，因为许多机器学习算法在数据集过大时表现较差。此外，所需的数据量随着维度的增加而指数增长，以达到统计显著性。这种现象被称为“维度灾难”。这些因素使得在数据科学过程中，提前确定哪些特征是相关的，并去除那些不重要的特征变得至关重要。
- en: 2\. How does LDA work?
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. LDA是如何工作的？
- en: The process of Linear Discriminant Analysis (LDA) can be broken down into five
    key steps.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 线性判别分析（LDA）的过程可以分为五个关键步骤。
- en: '**Step 1:** Compute the *d*-dimensional mean vectors for each of the *k* classes
    separately from the dataset.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 1：** 分别计算数据集中每个 *k* 类别的 *d* 维均值向量。'
- en: Remember that LDA is a supervised machine learning technique, meaning we can
    utilize the known labels. In the first step, we calculate the mean vectors *mean_c*
    for all samples belonging to a specific class *c*. To do this, we filter the feature
    matrix by class label and compute the mean for each of the *d* features. As a
    result, we obtain *k* mean vectors (one for each of the *k* classes), each with
    a length of *d* (corresponding to the *d* features).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，LDA 是一种监督学习技术，这意味着我们可以利用已知的标签。在第一步中，我们计算所有属于特定类别 *c* 的样本的均值向量 *mean_c*。为此，我们按类别标签过滤特征矩阵，并计算每个
    *d* 特征的均值。因此，我们得到 *k* 个均值向量（每个类别一个），每个均值向量的长度为 *d*（对应于 *d* 个特征）。
- en: '![](../Images/5ac7e6ee3fbe5483857a12a459e805e5.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5ac7e6ee3fbe5483857a12a459e805e5.png)'
- en: Label vector Y and feature matrix X | image by author
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 标签向量 Y 和特征矩阵 X | 图片由作者提供
- en: Mean vector for class c
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 类别 c 的均值向量
- en: '**Step 2:** Compute the scatter matrices (between-class scatter matrix and
    within-class scatter matrix).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2：** 计算散度矩阵（类间散度矩阵和类内散度矩阵）。'
- en: The within-class scatter matrix measures the variation among samples within
    the same class. To find a subspace with optimal separability, we aim to minimize
    the values in this matrix. In contrast, the between-class scatter matrix measures
    the variation between different classes. For optimal separability, we aim to maximize
    the values in this matrix.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 类内散度矩阵衡量的是同一类别样本之间的变异性。为了找到具有最佳可分性的子空间，我们旨在最小化此矩阵中的值。相比之下，类间散度矩阵衡量的是不同类别之间的变异性。为了获得最佳可分性，我们旨在最大化此矩阵中的值。
- en: Intuitively, within-class scatter looks at how compact each class is, whereas
    between-class scatter examines how far apart different classes are.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地讲，类内散度关注的是每个类别的紧凑度，而类间散度则考察不同类别之间的距离。
- en: '![](../Images/fb10ec259899c64b81ce85f1100ba03a.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fb10ec259899c64b81ce85f1100ba03a.png)'
- en: Within-class and between-class scatter matrices | image by author
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 类内和类间散度矩阵 | 图片由作者提供
- en: 'Let’s start with the **within-class scatter** matrix *S_W*. It is calculated
    as the sum of the scatter matrices *S_c* for each individual class:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从**类内散布**矩阵*S_W*开始。它是通过对每个类别的散布矩阵*S_c*求和得到的：
- en: Within-class scatter matrix S_W
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 类内散布矩阵S_W
- en: 'The **between-class scatter** matrix *S_B* is derived from the differences
    between the class means *mean_c* and the overall mean of the entire dataset:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**类间散布**矩阵*S_B*是通过类别均值*mean_c*与整个数据集的总体均值之间的差异得出的：'
- en: Between-class scatter matrix S_B
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 类间散布矩阵S_B
- en: where *mean* refers to the mean vector calculated over all samples, regardless
    of their class labels.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*mean*指的是在所有样本上计算的均值向量，而不考虑它们的类别标签。
- en: '**Step 3:** Calculate the eigenvectors and eigenvalues for the ratio of *S_W*​
    and *S_B*​.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3：** 计算*S_W*和*S_B*比率的特征向量和特征值。'
- en: As mentioned, for optimal class separability, we aim to maximize *S_B​* and
    minimize *S_W*​. We can achieve both by maximizing the ratio *S_B/S_W*​. In linear
    algebra terms, this ratio corresponds to the scatter matrix *S_W⁻¹ S_B*​, which
    is maximized in the subspace spanned by the eigenvectors with the highest eigenvalues.
    The eigenvectors define the directions of this subspace, while the eigenvalues
    represent the magnitude of the distortion. We will select the *m* eigenvectors
    associated with the highest eigenvalues.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，为了实现最佳的类分离性，我们的目标是最大化*S_B*并最小化*S_W*。我们可以通过最大化比率*S_B/S_W*来实现这一目标。从线性代数的角度看，这个比率对应于散布矩阵*S_W⁻¹
    S_B*，该矩阵在由具有最大特征值的特征向量张成的子空间中最大化。特征向量定义了这个子空间的方向，而特征值表示了变形的幅度。我们将选择与最大特征值相关联的*m*个特征向量。
- en: Calculation formula for eigenvectors and eigenvalues
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 特征向量和特征值的计算公式
- en: '![](../Images/43e9d50886343e62245ff02b8f47d566.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/43e9d50886343e62245ff02b8f47d566.png)'
- en: Subspace spanned by eigenvectors | image by author
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 特征向量张成的子空间 | 图片来源：作者
- en: '**Step 4:** Sort the eigenvectors in descending order of their corresponding
    eigenvalues, and select the *m* eigenvectors with the largest eigenvalues to form
    a *d × m-*dimensional transformation matrix *W*.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4：** 按照特征值从大到小的顺序排序特征向量，并选择具有最大特征值的*m*个特征向量来构建*d × m-*维度的变换矩阵*W*。'
- en: Remember, our goal is not only to project the data into a subspace that enhances
    class separability but also to reduce dimensionality. The eigenvectors will define
    the axes of our new feature subspace. To decide which eigenvectors to discard
    for the lower-dimensional subspace, we need to examine their corresponding eigenvalues.
    In simple terms, the eigenvectors with the smallest eigenvalues contribute the
    least to class separation, and these are the ones we want to drop. The typical
    approach is to rank the eigenvalues in descending order and select the top *m*
    eigenvectors. *m* is a freely chosen parameter. The larger *m*, the less information
    is lost during the transformation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们的目标不仅是将数据投影到一个能增强类分离性的子空间中，还要进行降维。特征向量将定义我们新特征子空间的坐标轴。为了决定丢弃哪些特征向量以形成低维子空间，我们需要检查它们相应的特征值。简而言之，具有最小特征值的特征向量对类分离贡献最小，我们希望丢弃这些特征向量。通常的方法是按特征值从大到小排列，选择前*m*个特征向量。*m*是一个可以自由选择的参数，*m*越大，变换过程中丢失的信息就越少。
- en: 'After sorting the eigenpairs by decreasing eigenvalues and selecting the top
    *m* pairs, the next step is to construct the *d × m*-dimensional transformation
    matrix *W*. This is done by stacking the *m* selected eigenvectors horizontally,
    resulting in the matrix *W*:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在按特征值从大到小排序特征对并选择前*m*对之后，下一步是构建*d × m-*维度的变换矩阵*W*。通过将*m*个选定的特征向量水平堆叠，得到矩阵*W*：
- en: Transformation matrix W
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 变换矩阵W
- en: The first column of *W* represents the eigenvector corresponding to the highest
    eigenvalue, the second column represents the eigenvector corresponding to the
    second highest eigenvalue, and so on.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*W*的第一列表示与最大特征值对应的特征向量，第二列表示与第二大特征值对应的特征向量，依此类推。'
- en: '**Step 5:** Use *W* to project the samples onto the new subspace.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤5：** 使用*W*将样本投影到新的子空间。'
- en: 'In the final step, we use the *d × m-*dimensional transformation matrix *W*,
    which we composed from the top *m* selected eigenvectors, to project our samples
    onto the new subspace:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步，我们使用*d × m-*维度的变换矩阵*W*，该矩阵由前*m*个选定的特征向量组成，将样本投影到新的子空间：
- en: Transformed feature matrix Z
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 变换后的特征矩阵Z
- en: 'where *X* is the initial *n × d*-dimensional feature matrix representing our
    samples, and *Z* is the newly transformed *n × m*-dimensional feature matrix in
    the new subspace. This means that the selected eigenvectors serve as the “recipes”
    for transforming the original features into the new features (the **Linear Discriminants**):
    The eigenvector with the highest eigenvalue provides the transformation recipe
    for **LD1**, the eigenvector with the second highest eigenvalue corresponds to
    **LD2**, and so on.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *X* 是初始的 *n × d* 维特征矩阵，表示我们的样本，而 *Z* 是新子空间中经过转换后的 *n × m* 维特征矩阵。这意味着，所选的特征向量作为“配方”用于将原始特征转换为新特征（**线性判别量**）：具有最大特征值的特征向量提供了
    **LD1** 的转换配方，具有第二大特征值的特征向量对应于 **LD2**，依此类推。
- en: '![](../Images/9ab651ba543314aeae2afbdab253c687.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ab651ba543314aeae2afbdab253c687.png)'
- en: Projection of X onto the linear discriminants LD
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 将 X 投影到线性判别量 LD 上
- en: 3\. Implementing Linear Discriminant Analysis (LDA) from Scratch
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 从头开始实现线性判别分析（LDA）
- en: To demonstrate the theory and mathematics in action, we will program our own
    LDA from scratch using only numpy.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示理论和数学的实际应用，我们将从头开始使用 numpy 编写我们自己的 LDA。
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 4\. Applying LDA to an Industrial Dataset
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 将 LDA 应用于工业数据集
- en: To see LDA in action, we will apply it to a typical task in the production environment.
    We have data from a simple manufacturing line with only 7 stations. Each of these
    stations sends a data point (yes, I know, only one data point is highly unrealistic).
    Unfortunately, our line produces a significant number of defective parts, and
    we want to find out which stations are responsible for this.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到 LDA 的实际应用，我们将其应用于生产环境中的典型任务。我们有来自一个简单制造流水线的数据，该流水线只有 7 个工作站。每个工作站发送一个数据点（是的，我知道，只有一个数据点是非常不现实的）。不幸的是，我们的生产线产生了大量的不良品，我们希望找出哪些工作站对这一现象负责。
- en: First, we load the data and take an initial look.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们加载数据并初步查看。
- en: '[PRE1]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/c8c63823ac648330d6223f37f1ecc5be.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c8c63823ac648330d6223f37f1ecc5be.png)'
- en: Next, we study the distribution of the data using the `.describe()` method from
    Pandas.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用 Pandas 中的 `.describe()` 方法来研究数据的分布。
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/3e772f56c494366b54ef78ece6386e3f.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e772f56c494366b54ef78ece6386e3f.png)'
- en: 'We see that we have 20,000 data points, and the measurements range from -5
    to +150\. Hence, we note for later that we need to normalize the dataset: the
    different magnitudes of the numerical values would otherwise negatively affect
    the LDA.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到有 20,000 个数据点，测量范围从 -5 到 +150。由此，我们需要记住，稍后需要对数据集进行归一化：不同数值的量级，否则会对 LDA 产生负面影响。
- en: How many good parts and how many bad parts do we have?
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有多少个良品和多少个不良品？
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/ea0d6a2c9c981587a36f6beeb5ca4898.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea0d6a2c9c981587a36f6beeb5ca4898.png)'
- en: We have 19,031 good parts and 969 defective parts. The fact that the dataset
    is so imbalanced is an issue for further analysis. Therefore, we select all defective
    parts and an equal number of randomly chosen good parts for the further processing.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有 19,031 个良品和 969 个不良品。数据集严重失衡是进一步分析的一个问题。因此，我们选择所有不良品和等数量的随机选择的良品进行后续处理。
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/872c6b4b1dd3882783e57eb19d3c52a8.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/872c6b4b1dd3882783e57eb19d3c52a8.png)'
- en: Now, let’s apply our LDA from scratch to the balanced dataset. We use the `StandardScaler`
    from `sklearn` to normalize the measurements for each feature to have a mean of
    0 and a standard deviation of 1\. We choose only one linear discriminant axis
    (*m=1*) onto which we project the data. This helps us clearly see which features
    are most relevant in distinguishing good from bad parts, and we visualize the
    projected data in a histogram.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将从头开始实现的 LDA 应用于平衡后的数据集。我们使用 `sklearn` 中的 `StandardScaler` 来对每个特征的测量值进行归一化，使其均值为
    0，标准差为 1。我们选择一个线性判别轴 (*m=1*)，将数据投影到该轴上。这有助于我们清楚地看到哪些特征在区分良品和不良品方面最为相关，并通过直方图可视化投影后的数据。
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/bc19ff379454908643a58e3ad24e81a8.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc19ff379454908643a58e3ad24e81a8.png)'
- en: Feature matrix projected to one LD (m=1)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 将特征矩阵投影到一个 LD（m=1）上
- en: '![](../Images/89ff843406588e3be91c2f2fb0098c46.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89ff843406588e3be91c2f2fb0098c46.png)'
- en: Feature importance = How much do the stations contribute to class separation?
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 特征重要性 = 各站点在类分离中的贡献有多大？
- en: The histogram shows that we can separate the good parts from the defective parts
    very well, with only a small overlap. This is already a positive result and indicates
    that our LDA was successful.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 直方图显示，我们能够很好地区分良品和不良品，只有少量重叠。这已经是一个积极的结果，表明我们的 LDA 成功了。
- en: 'The “LDA Coefficients” from the table “Feature Contributions to LDA Components”
    represent the eigenvector from the first (and only, since *m=1*) column of our
    transformation matrix *W*. They indicate the direction and magnitude with which
    the normalized measurements from the stations are projected onto the linear discriminant
    axis. The values in the table are sorted in descending order. We need to read
    the table from both the top and the bottom simultaneously because the absolute
    value of the coefficient indicates the significance of each station in separating
    the classes and, consequently, its contribution to the production of defective
    parts. The sign indicates whether a lower or higher measurement increases the
    likelihood of defective parts. Let’s take a closer look at our example:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 表格“LDA组件的特征贡献”中的“LDA系数”表示我们转换矩阵*W*的第一列（也是唯一一列，因为*m=1*）的特征向量。它们表示从各个工站的标准化测量值在LDA轴上的投影方向和大小。表中的数值按降序排列。我们需要同时从表格的顶部和底部读取，因为系数的绝对值表示每个工站在类别分离中的重要性，进而表示其对生产缺陷部件的贡献。符号表示较低或较高的测量值是否会增加缺陷部件的可能性。让我们仔细看看我们的例子：
- en: The largest absolute value is from Station 4, with a coefficient of -0.672\.
    This means that Station 4 has the strongest influence on part failure. Due to
    the negative sign, higher positive measurements are projected towards a negative
    linear discriminant (LD). The histogram shows that a negative LD is associated
    with good (green) parts. Conversely, **low and negative measurements at this station
    increase the likelihood of part failure**.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 最大的绝对值来自工站4，其系数为-0.672。 这意味着工站4对部件故障的影响最强。由于负号，较高的正测量值会投影到负的线性判别轴（LD）。直方图显示，负的LD与良好（绿色）部件相关。相反，**在该工站的低值和负值会增加部件故障的可能性**。
- en: The second highest absolute value is from Station 2, with a coefficient of 0.557\.
    Therefore, this station is the second most significant contributor to part failures.
    The positive sign indicates that high positive measurements are projected towards
    the positive LD. From the histogram, we know that a high positive LD value is
    associated with a high likelihood of failure. In other words, **high measurements
    at Station 2 lead to part failures**.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 第二大绝对值来自工站2，其系数为0.557。 因此，这个工站是导致部件故障的第二大贡献者。正号表示较高的正测量值会投影到正的LD。从直方图中我们知道，较高的正LD值与较高的故障概率相关。换句话说，**工站2的高测量值会导致部件故障**。
- en: The third highest coefficient comes from Station 7, with a value of -0.486\.
    This makes Station 7 the third largest contributor to part failures. The negative
    sign again indicates that high positive values at this station lead to a negative
    LD (which corresponds to good parts). Conversely, **low and negative values at
    this station lead to part failures**.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 第三高的系数来自工站7，其值为-0.486。 这使得工站7成为部件故障的第三大贡献者。负号再次表明，在该工站的较高正值会导致负LD（这对应于良好的部件）。相反，**在该工站的低值和负值会导致部件故障**。
- en: All other LDA coefficients are an order of magnitude smaller than the three
    mentioned, **the associated stations therefore have no influence on part failure**.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 所有其他LDA系数的数量级都比上述三者小得多，**因此，相关的工站对部件故障没有影响**。
- en: Are the results of our LDA analysis correct? As you may have already guessed,
    the production dataset is synthetically generated. I labeled all parts as defective
    where the measurement at Station 2 was greater than 0.5, the value at Station
    4 was less than -2.5, and the value at Station 7 was less than 3\. **It turns
    out that the LDA hit the mark perfectly!**
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的LDA分析结果正确吗？正如你可能已经猜到的，生产数据集是合成生成的。我将所有部件标记为有缺陷，其中工站2的测量值大于0.5，工站4的值小于-2.5，工站7的值小于3。**事实证明，LDA完全准确地命中了目标！**
- en: '[PRE6]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 5\. Conclusion
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5. 结论
- en: Linear Discriminant Analysis (LDA) not only reduces the complexity of datasets
    but also highlights the key features that drive class separation, making it highly
    effective for identifying failure causes in production systems. It is a straightforward
    yet powerful method with practical applications and is readily available in libraries
    like `scikit-learn`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 线性判别分析（LDA）不仅可以减少数据集的复杂性，还能突出驱动类别分离的关键特征，因此在识别生产系统中的故障原因时非常有效。它是一种直接而强大的方法，具有实际应用，并且在像`scikit-learn`这样的库中容易获取。
- en: To achieve optimal results, it is crucial to balance the dataset (ensure a similar
    number of samples in each class) and normalize it (mean of 0 and standard deviation
    of 1).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现最佳效果，平衡数据集（确保每个类别中样本数量相似）并进行归一化（均值为0，标准差为1）是至关重要的。
- en: '**The next time you work with a large dataset containing class labels and numerous
    features, why not give LDA a try?**'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**下次当你处理包含类别标签和多个特征的大型数据集时，为什么不尝试一下LDA呢？**'
