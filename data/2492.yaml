- en: Improve Your RAG Context Recall by 95% with an Adapted Embedding Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用改进的嵌入模型将你的 RAG 上下文召回率提升 95%
- en: 原文：[https://towardsdatascience.com/improve-your-rag-context-recall-by-40-with-an-adapted-embedding-model-5d4a8f583f32?source=collection_archive---------0-----------------------#2024-10-12](https://towardsdatascience.com/improve-your-rag-context-recall-by-40-with-an-adapted-embedding-model-5d4a8f583f32?source=collection_archive---------0-----------------------#2024-10-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/improve-your-rag-context-recall-by-40-with-an-adapted-embedding-model-5d4a8f583f32?source=collection_archive---------0-----------------------#2024-10-12](https://towardsdatascience.com/improve-your-rag-context-recall-by-40-with-an-adapted-embedding-model-5d4a8f583f32?source=collection_archive---------0-----------------------#2024-10-12)
- en: Step-by-step model adaptation code and results attached
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附上逐步模型适配代码和结果
- en: '[](https://medium.com/@vignesh865?source=post_page---byline--5d4a8f583f32--------------------------------)[![Vignesh
    Baskaran](../Images/52afb4a7a3cd0329dc1ba9d931413788.png)](https://medium.com/@vignesh865?source=post_page---byline--5d4a8f583f32--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5d4a8f583f32--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5d4a8f583f32--------------------------------)
    [Vignesh Baskaran](https://medium.com/@vignesh865?source=post_page---byline--5d4a8f583f32--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@vignesh865?source=post_page---byline--5d4a8f583f32--------------------------------)[![Vignesh
    Baskaran](../Images/52afb4a7a3cd0329dc1ba9d931413788.png)](https://medium.com/@vignesh865?source=post_page---byline--5d4a8f583f32--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5d4a8f583f32--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5d4a8f583f32--------------------------------)
    [Vignesh Baskaran](https://medium.com/@vignesh865?source=post_page---byline--5d4a8f583f32--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5d4a8f583f32--------------------------------)
    ·10 min read·Oct 12, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5d4a8f583f32--------------------------------)
    ·阅读时间 10 分钟·2024年10月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Retrieval-augmented generation (RAG) is one prominent technique employed to
    integrate LLM into business use cases, allowing proprietary knowledge to be infused
    into LLM. This post assumes you already possess knowledge about RAG and you are
    here to improve your RAG accuracy.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）是一种重要的技术，旨在将 LLM 融入业务场景，允许将专有知识注入 LLM。本文假设你已经具备 RAG 的相关知识，目的是提升你的
    RAG 准确性。
- en: 'Let’s review the process briefly. The RAG model consists of two main steps:
    retrieval and generation. In the retrieval step, several sub-steps are involved,
    including converting context text to vectors, indexing the context vector, retrieving
    the context vector for the user query, and reranking the context vector. Once
    the contexts for the query are retrieved, we move on to the generation stage.
    During the generation stage, the contexts are combined with prompts and sent to
    the LLM to generate a response. Before sending to the LLM, the context-infused
    prompts may undergo caching and routing steps to optimize efficiency.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要回顾一下这个过程。RAG 模型包括两个主要步骤：检索和生成。在检索步骤中，涉及多个子步骤，包括将上下文文本转换为向量、对上下文向量进行索引、根据用户查询检索上下文向量以及重新排序上下文向量。一旦查询的上下文被检索到，我们就进入生成阶段。在生成阶段，查询上下文与提示结合并发送给大语言模型（LLM）以生成回应。在发送到
    LLM 之前，结合上下文的提示可能会经历缓存和路由步骤，以优化效率。
- en: For each of the pipeline steps, we will conduct numerous experiments to collectively
    enhance RAG accuracy. You can refer to the below image that lists(but is not limited
    to) the experiments performed in each step.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个管道步骤，我们将进行大量实验，协同提高 RAG 的准确性。你可以参考下方的图片，其中列出了（但不限于）每个步骤中执行的实验。
- en: '![](../Images/72472ed36ec30d6d1ab16d6d3dc1b23a.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/72472ed36ec30d6d1ab16d6d3dc1b23a.png)'
