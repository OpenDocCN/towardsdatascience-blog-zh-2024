- en: Quantizing the AI Colossi
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 量化 AI 巨兽
- en: 原文：[https://towardsdatascience.com/quantizing-the-ai-colossi-017e121a27c5?source=collection_archive---------0-----------------------#2024-04-15](https://towardsdatascience.com/quantizing-the-ai-colossi-017e121a27c5?source=collection_archive---------0-----------------------#2024-04-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/quantizing-the-ai-colossi-017e121a27c5?source=collection_archive---------0-----------------------#2024-04-15](https://towardsdatascience.com/quantizing-the-ai-colossi-017e121a27c5?source=collection_archive---------0-----------------------#2024-04-15)
- en: 'Streamlining Giants Part 2: Neural Network Quantization'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 精简巨人 第二部分：神经网络量化
- en: '[](https://natecibik.medium.com/?source=post_page---byline--017e121a27c5--------------------------------)[![Nate
    Cibik](../Images/008c22b715ddf4f1d0f9970142edc09f.png)](https://natecibik.medium.com/?source=post_page---byline--017e121a27c5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--017e121a27c5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--017e121a27c5--------------------------------)
    [Nate Cibik](https://natecibik.medium.com/?source=post_page---byline--017e121a27c5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://natecibik.medium.com/?source=post_page---byline--017e121a27c5--------------------------------)[![Nate
    Cibik](../Images/008c22b715ddf4f1d0f9970142edc09f.png)](https://natecibik.medium.com/?source=post_page---byline--017e121a27c5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--017e121a27c5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--017e121a27c5--------------------------------)
    [Nate Cibik](https://natecibik.medium.com/?source=post_page---byline--017e121a27c5--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--017e121a27c5--------------------------------)
    ·81 min read·Apr 15, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--017e121a27c5--------------------------------)
    ·阅读时间 81 分钟 ·2024年4月15日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/3876a06f9d219ada6c44bf205b2641ab.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3876a06f9d219ada6c44bf205b2641ab.png)'
- en: Image by author using DALL-E 3
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用 DALL-E 3 制作
- en: In recent years, a powerful alliance has been forged between the [transformer](https://arxiv.org/abs/1706.03762)
    neural network architecture and [the formulation of various problems as self-supervised
    sequence prediction tasks](/navigating-the-future-62ea60f27046). This union has
    enabled researchers to train large foundation models of unprecedented sizes using
    massive troves of unlabeled sequential data, and these models have shown uncanny
    emergent capabilities that closely mimic human-level intelligence in several domains.
    With newfound heights of practical utility unleashed, artificial intelligence
    (AI) was catapulted into mainstream life and conversation, and few today are unaware
    that the once fictional realm of silicone-based intelligence has now become very
    tangible and real.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，[transformer](https://arxiv.org/abs/1706.03762) 神经网络架构与[将各种问题表述为自监督序列预测任务的形式](https://navigating-the-future-62ea60f27046)之间建立了强大的联盟。这个结合使得研究人员能够利用大量未标记的顺序数据，训练出前所未有的大规模基础模型，这些模型在多个领域展现出了近乎模仿人类智能的惊人能力。随着实践效用的新高度被释放，人工智能（AI）迅速进入了主流生活和讨论领域，今天几乎没有人不知道，曾经只是虚构的硅基智能领域如今已变得非常具体和真实。
- en: However, intrinsically linked to the explosive growth in AI capabilities has
    been the rapid inflation of model sizes well into the hundreds of billions (and
    in some cases trillions) of parameters. A powerful new technology was delivered
    to the world, but it could be only be served using massive hardware clusters.
    Echoing the challenges from earlier eras of AI, the temptation to possess these
    powers on consumer hardware or edge devices was tremendous, and motivation to
    compress these pretrained behemoths took effect immediately, triggering catalytic
    flows of funding and talent into the study of *model compression*, and reanimating
    several well-pedigreed techniques including pruning, quantization, knowledge distillation,
    and parameter-efficient fine-tuning.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，伴随着人工智能（AI）能力的爆炸性增长，模型规模的迅速膨胀也紧密相随，模型参数已达到数百亿（在某些情况下甚至是万亿）之巨。一个强大的新技术被传递给了全世界，但它只能通过庞大的硬件集群来服务。回响着早期人工智能时代的挑战，拥有这些强大能力的诱惑在消费者硬件或边缘设备上变得异常强烈，推动了压缩这些预训练庞然大物的动机，这立刻产生了巨大的资金和人才流动，进入了*模型压缩*的研究领域，并重新激活了几种久负盛名的技术，包括剪枝、量化、知识蒸馏和参数高效微调。
- en: '[In part one of the Streamlining Giants series](/streamlining-giants-8a26aa1e91d3),
    we began our discussion on democratizing the power of large language models (LLMs)
    through model compression by exploring the rich legacy of research in neural network
    *pruning,* from its inception through its recent applications in LLMs containing
    tens or hundreds of billions of parameters. Along the way, we discovered that
    these large models can be compressed substantially with minimal drops in performance
    and marked gains in computational burden through either the *unstructured* or
    *structured* removal of the least important parameters from the network. We also
    saw that while pruning produces compact models that can operate in resource-constrained
    environments, the process itself traditionally required calculation of gradient
    information and/or retraining of the model to recover performance. This meant
    that the method was historically only accessible for those with the computational
    resources needed to train the original model, which in the case of LLMs would
    mean millions of dollars. While this originally placed the means of compressing
    models through pruning outside of the reach of those who needed it most, we saw
    that recent studies have proposed highly accessible methods using low-rank gradients
    or even just forward-pass information. Further, with the retraining of large models
    facilitated by simultaneous advances in parameter-efficient fine-tuning methods,
    pruning can now be performed using consumer hardware.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在[《简化巨头》系列的第一部分](/streamlining-giants-8a26aa1e91d3)中，我们开始讨论通过模型压缩使大型语言模型（LLM）能够普及化的议题，探索了神经网络*剪枝*的丰富研究遗产，从其起源到最近在包含数十亿或数百亿参数的LLM中的应用。在这一过程中，我们发现这些大型模型可以通过*无结构*或*结构化*地去除网络中最不重要的参数，从而在性能损失最小的情况下显著压缩，同时计算负担也得到显著减轻。我们还看到，虽然剪枝能够生成紧凑的模型，使其能够在资源受限的环境中运行，但这一过程通常需要计算梯度信息和/或重新训练模型以恢复性能。这意味着该方法在历史上只有具备足够计算资源以训练原始模型的人才能使用，而在LLM的情况下，这需要数百万美元的投入。虽然这使得通过剪枝压缩模型的手段在过去远离了那些最需要它的人，但我们看到最近的研究提出了通过低秩梯度或甚至仅使用前向传播信息来进行压缩的高度可接入的方法。此外，随着大模型的重新训练得益于参数高效微调方法的同步进展，剪枝现在也可以在消费级硬件上进行。
- en: 'In this installment, we investigate an orthogonal approach to model compression:
    *Quantization* seeks to improve the computational efficiency and memory requirements
    of models by reducing the precision of the numbers being stored and operated on
    by the network, which may include the weights, activations, or both. While quantization
    can refer to any drop in precision, for example from 32-bit to 16-bit floating
    point, it also often involves a transition into the integer space, which offers
    accelerated operation and deployment on consumer hardware. As we will see, quantization
    is an extremely powerful method for compressing LLMs, offering significant reductions
    in computational overhead and hardware requirements with only minor or even non-existent
    drops in performance, making it the most widely employed model compression technique
    in today’s world of large models. Further, by varying the levels of numeric precision,
    we can tune the accuracy/efficiency tradeoff for our use case.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本篇文章中，我们探讨了一种正交的模型压缩方法：*量化*通过降低网络存储和操作数字的精度，来提高模型的计算效率和内存需求，这些数字可能包括权重、激活值，或者两者兼有。虽然量化可以指任何精度的下降，例如从32位浮点数到16位浮点数，但它通常还涉及到进入整数空间，这可以加速操作并在消费级硬件上部署。正如我们将看到的那样，量化是压缩大型语言模型（LLM）的一个极为强大的方法，能够显著减少计算开销和硬件需求，而性能下降仅为轻微，甚至在某些情况下不存在性能下降，这使得它成为当今大型模型世界中最广泛使用的模型压缩技术。此外，通过调整数字精度的级别，我们可以根据具体应用场景调节精度/效率的权衡。
- en: 'Along this journey, we will see that quantization works in harmony with the
    *pruning* techniques we encountered previously, as well as with *knowledge distillation*
    and *parameter-efficient fine-tuning* methods which we have yet to explore, providing
    us a glimpse into the upcoming topics of investigation in the Streamlining Giants
    series. There is a popular adage which states that “there is no such thing as
    free lunch,” but as we saw in our investigation into pruning: when it comes to
    model compression, sometimes there is. Similar to pruning, quantization acts as
    a form of regularization which is known to make neural networks more robust and
    generalizable, meaning that judicious applications of these techniques can often
    simultaneously compresses a model and improve its performance. In this article,
    we will survey the literature and see several examples of “free lunch” compression.
    By the end, even the skeptical reader should find themselves disabused of the
    notion that network quantization inherently suggests a degradation in quality.
    After reviewing the research, we will explore the tools for applying these techniques
    in our own work using open-source software. Now, let us dig in to the exciting
    field of neural network quantization.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，我们将看到量化与我们之前遇到的*剪枝*技术以及*知识蒸馏*和*参数高效微调*方法相得益彰，这些方法我们还未深入探讨，它们为我们提供了对《简化巨头》系列即将展开的主题的初步了解。有一句流行的格言说：“天下没有免费的午餐”，但正如我们在剪枝研究中看到的那样：当谈到模型压缩时，有时候却真的有。与剪枝类似，量化作为一种正则化形式，已知能够让神经网络更加健壮和具有更强的泛化能力，这意味着这些技术的谨慎应用往往能在压缩模型的同时提高其性能。在本文中，我们将回顾相关文献并展示几个“免费午餐”压缩的例子。到最后，即使是怀疑的读者也应该会发现，网络量化并不一定意味着质量的下降。审视过研究后，我们将探讨如何使用开源软件在自己的工作中应用这些技术。现在，让我们深入探索神经网络量化这一激动人心的领域。
- en: 'Note: For those who want to skip the lesson and get straight to the implementation
    guide for accelerating their workflows, click [here](#3761).'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：对于那些想跳过课程，直接进入加速工作流程的实施指南的读者，请点击[这里](#3761)。
- en: Quantization
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 量化
- en: '![](../Images/dfb0a09962e55d79773bccc5d08d4e92.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dfb0a09962e55d79773bccc5d08d4e92.png)'
- en: Image by author using DALL-E 3
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用 DALL-E 3 生成的图像
- en: As a testament to the success and necessity of quantization in LLM deployment,
    every popular open-source LLM serving solution today provides ready access to
    quantized models, which are often the default selections. The popular [Ollama](https://ollama.com/),
    for instance, which I recently had a great deal of fun using to create [an open-source
    speech-to-speech multilingual language learning assistant](/lingonaut-language-assistant-6abe3e8b045c),
    is built on top of [llama.cpp](https://github.com/ggerganov/llama.cpp), a pure
    C/C++ library developed to make the optimized deployment of quantized LLMs on
    consumer hardware a reality. For real-time vision-language applications like robots
    using low-power hardware, deploying quantized models with these types of hardware-optimized
    serving backends is imperative. But what exactly is quantization, and what makes
    it so effective at compressing neural networks?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 作为量化在大规模语言模型（LLM）部署中成功与必要性的证明，如今每个流行的开源 LLM 服务解决方案都提供了量化模型的便捷访问，这些模型通常是默认选项。例如，我最近使用
    [Ollama](https://ollama.com/) 创建了[一个开源的语音到语音多语言语言学习助手](/lingonaut-language-assistant-6abe3e8b045c)，它就是建立在
    [llama.cpp](https://github.com/ggerganov/llama.cpp) 之上的，这是一个纯 C/C++ 库，旨在使量化 LLM
    在消费级硬件上的优化部署成为现实。对于像使用低功耗硬件的机器人这类实时视觉-语言应用，部署量化模型并使用这些硬件优化的服务后端是至关重要的。但究竟什么是量化，它为何能如此有效地压缩神经网络呢？
- en: '*Quantization* refers to the mapping of a continuous real-number space into
    a fixed set of discrete numbers, or more broadly, to the transition of any numeric
    space into a lower-precision representation. Take for instance the 32-bit “single”
    or “full” precision floating point value, or even the high-resolution 64-bit “double”
    float; both of these data types have a limited precision in the number of decimal
    places they can carry. Therefore, they are examples of quantized distributions,
    since there are an infinite number of values between each of the “steps” in their
    final decimal place which cannot be represented, creating the distinct “staircase”
    pattern of the digital world. Indeed, the challenge of effectively manipulating
    continuous values in a discrete system are as old as digital computer science
    itself. Even floating-point numbers are decomposed into integers under the hood,
    since digital computers process binary information, which is inherently discrete.
    Therefore, when it comes to neural networks, the question is technically not “is
    it quantized,” but rather “how quantized is it?”'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*量化*是指将一个连续的实数空间映射到一个固定的离散数集合，或者更广泛地说，是将任何数字空间转换为较低精度的表示。例如，32位的“单精度”或“全精度”浮动点值，甚至是高分辨率的64位“双精度”浮点数；这两种数据类型在它们可以携带的小数位数上有一定的精度限制。因此，它们是量化分布的示例，因为在它们的最后一个小数位中的每一个“步长”之间存在无限多个无法表示的值，从而产生了数字世界中独特的“阶梯”模式。事实上，在离散系统中有效地处理连续值的挑战与数字计算机科学本身一样古老。即使是浮动点数也在内部被分解为整数，因为数字计算机处理的是二进制信息，而二进制本质上是离散的。因此，涉及神经网络时，技术上问题并非是“是否量化”，而是“量化到什么程度？”'
- en: Unlike other applications of quantization, such as signal processing, where
    the objective is strictly to have the quantized numbers represent their original
    values as closely as possible, the end goal in neural network quantization is
    to discretize the parameters in such a way that lowers their precision as much
    as possible while maintaining the same output from their collective interoperation.
    Since neural networks are highly overparameterized, and therefore have entire
    manifolds of optima in their loss gradient space that contain many possible solutions,
    the individual weights are free to move relatively far from their original values
    during quantization as long as their collective interaction stays on this solution
    manifold, providing the opportunity to optimize the model parameters with their
    subsequent quantization in mind, known as *Quantization-Aware Training* (QAT).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他量化应用（例如信号处理）不同，信号处理的目标是尽可能让量化后的数字与其原始值接近，而神经网络量化的最终目标是以某种方式离散化参数，使其精度尽可能低，同时保持它们的集体相互作用产生相同的输出。由于神经网络具有高度的过度参数化，因此在其损失梯度空间中存在整个最优解流形，包含许多可能的解，在量化过程中，单个权重可以相对远离其原始值，只要它们的集体交互仍保持在这个解流形上，这提供了一个优化模型参数的机会，同时考虑到随后的量化，这被称为*量化感知训练*（QAT）。
- en: '![](../Images/d07bc1b92c2a0d1ba70fac00d78fa0f2.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d07bc1b92c2a0d1ba70fac00d78fa0f2.png)'
- en: Figure from the [Gholami et al. 2021](https://arxiv.org/abs/2103.13630) survey
    demonstrates the concept of Quantization-Aware Training (QAT).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[Gholami et al. 2021](https://arxiv.org/abs/2103.13630)的图示展示了量化感知训练（QAT）的概念。
- en: Performing QAT is often done using *simulated* or *fake* quantization, where
    parameters are stored in low-precision, but the operations are still carried out
    using floating point arithmetic. In the case of QAT, performing the math in floating
    point provides the conditions for gradient descent (along with the Straight Through
    Estimator or “STE”, which simply ignores the destructive effect of the rounding
    function on the gradient), but some methods will also use simulated quantization
    at inference time when they focus on storage efficiency rather than runtime acceleration.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 执行QAT通常使用*模拟*或*伪*量化，在这种方法中，参数以低精度存储，但操作仍然使用浮动点算术进行。在QAT的情况下，使用浮动点进行数学运算提供了梯度下降的条件（以及“直接通过估算器”或“STE”，它简单地忽略了四舍五入函数对梯度的破坏性影响），但一些方法也会在推理时使用模拟量化，特别是在它们专注于存储效率而非运行时加速时。
- en: Simulated quantization stands in contrast to *integer-only* or *fixed-point*
    quantization, in which all operations are performed using low-precision arithmetic.
    Integer-only quantization is generally where the full advantages in latency and
    power consumption reside, but considerations will vary depending on hardware,
    as modern GPUs have highly optimized floating point units, while edge devices
    use low-power hardware where integer arithmetic may be more efficient. The use
    of simulated or integer-only quantization depends on the use case; for instance,
    simulated quantization would be a good choice for iteratively testing the sensitivity
    of different network components to varying levels of quantization without worrying
    about the hardware implementation, while integer-only quantization is likely to
    be the best choice for optimized deployment at the edge.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟量化与*仅整数*或*定点*量化形成对比，在这种量化中，所有操作都是通过低精度算术执行的。仅整数量化通常是在延迟和功耗方面的全面优势所在，但考虑因素会因硬件不同而有所不同，因为现代GPU具有高度优化的浮点单元，而边缘设备使用低功耗硬件，其中整数运算可能更为高效。模拟量化或仅整数量化的使用取决于使用场景；例如，模拟量化将是一个不错的选择，可以用来迭代测试不同网络组件对不同量化水平的敏感性，而无需担心硬件实现，而仅整数量化可能是边缘优化部署的最佳选择。
- en: '![](../Images/371c421ef32fee5488169d4db02e0376.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/371c421ef32fee5488169d4db02e0376.png)'
- en: Figure from [Gholami et al. 2021](https://arxiv.org/abs/2103.13630) survey shows
    the distinction between full-precision, simulated quantization, and integer-only
    quantization.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[Gholami等人 2021年](https://arxiv.org/abs/2103.13630)的图表展示了全精度、模拟量化和仅整数量化之间的区别。
- en: 'While QAT produces optimal results by factoring the quantization effects into
    the training process, it is met with the same challenge we encountered previously
    during our pruning investigation: if we seek to compress very large models like
    LLMs, we sincerely want to avoid a retraining phase for several reasons, including
    not having access to the training data, or not wanting to spend millions of dollars
    on the GPU hours required. For this reason, we are driven to abandon the understandably
    superior results of QAT, and look instead to *Post-Training Quantization* (PTQ)
    methodology, which requires only a small set of calibration data, and ultimately
    hope for the success of *Zero-Shot Quantization* (ZSQ) methods, which explore
    the ideal scenario of using no data at all. As we will see, recent work has pushed
    PTQ to impressive levels of accuracy, closely matching full precision baselines
    even in low-bit settings, and has become very accessible thanks to the efforts
    of open-source research and code.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然QAT通过将量化效应纳入训练过程来产生最佳结果，但它面临着我们在先前修剪研究中遇到的相同挑战：如果我们希望压缩像LLM这样非常大的模型，我们确实希望避免重新训练阶段，原因有很多，包括无法访问训练数据，或不愿意为所需的GPU时间支付数百万美元。因此，我们被迫放弃QAT显然优越的结果，而转向*后训练量化*（PTQ）方法，它只需要一小部分校准数据，并最终希望*零-shot量化*（ZSQ）方法取得成功，后者探索了完全不使用数据的理想场景。正如我们将看到的，最近的工作将PTQ推向了令人印象深刻的精度水平，即使在低位设置中，也能紧密匹配全精度基准，并且由于开源研究和代码的努力，它变得非常易于访问。
- en: '![](../Images/4b9e1c09c588ccae2b160da8a6c2d367.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b9e1c09c588ccae2b160da8a6c2d367.png)'
- en: Flowchart from [Olivia Weng’s 2021 quantization survey](https://arxiv.org/abs/2112.06126)
    concisely delineates QAT and PTQ.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[Olivia Weng 2021年量化调查](https://arxiv.org/abs/2112.06126)的流程图简洁地概述了QAT和PTQ。
- en: The benefits of quantizing neural networks extend beyond compression. Like pruning,
    quantization acts as a form of regularization in neural networks by reducing the
    number of unique parameters, and therefore can also lead to increased performance
    and generalization capability when applied judiciously. In this way, quantization
    joins pruning as another “free lunch” compression method for neural networks,
    as the substantial reductions in model size and complexity can also paradoxically
    provide improved performance (when tuned correctly). Given the benefits of discretization,
    it starts to appear that the representation of neural networks in floating point
    is merely a larval stage of development required only for creating the mathematical
    conditions conducive to gradient descent during training, and that the subsequent
    quantization is not just a post-processing technique for model compression, but
    is actually an essential maturation stage in the neural network development lifecycle.
    Further, if research trends continue, equivalent training results may eventually
    be achieved using optimized integer-only math, which could free us from the need
    of high-precision neural networks forever.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络量化的好处不仅仅体现在压缩方面。像剪枝一样，量化通过减少独特参数的数量，作为神经网络中的一种正则化形式，从而在谨慎应用时，也能提升性能和泛化能力。通过这种方式，量化与剪枝共同成为神经网络的另一种“免费午餐”压缩方法，因为模型大小和复杂性的显著减少也能反而提供改进的性能（如果正确调优的话）。鉴于离散化的好处，开始出现一种观点，即神经网络在浮点数表示上的形式仅仅是一个发展中的幼态阶段，仅用于在训练过程中为梯度下降创造数学条件，随后的量化不仅仅是模型压缩的后处理技术，而实际上是神经网络开发生命周期中的一个必要成熟阶段。此外，如果研究趋势持续下去，最终可能通过优化的仅使用整数的数学运算来实现等效的训练结果，这将彻底解放我们对高精度神经网络的需求。
- en: Quantization touches every part of LLM development, from training to deployment,
    and encompasses a broad range of techniques which aim to reduce the memory, power,
    and computational efficiency of large models. For example, it has become common
    practice to train LLMs in *mixed precision*, where less-sensitive calculations
    are carried out in half-precision (16-bit float) rather than full 32-bit floating
    point precision, dramatically reducing their size in memory and the power required
    to operate on them without significantly affecting results. Modifications such
    as this not only allow us to iterate and develop models more freely, but they
    also have broad environmental implications at the scale of large model training,
    which can be measured in tons of CO2 in the case of LLMs.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 量化涉及到大型语言模型（LLM）开发的各个方面，从训练到部署，涵盖了广泛的技术，旨在减少大型模型的内存、功耗和计算效率。例如，训练LLM时使用*混合精度*已经成为一种常见做法，其中对于不那么敏感的计算采用半精度（16位浮点数）而不是完整的32位浮点精度，从而显著减少其内存占用和所需的计算功率，而对结果几乎没有显著影响。这种修改不仅使我们能够更自由地迭代和开发模型，而且在大规模模型训练的环境中具有广泛的环境影响，LLM的训练甚至可能以吨计二氧化碳排放。
- en: Truly, when equivalent mathematical outcomes can be reached using a fraction
    of the resources, there are no losers, and very big gains to be made. This promise
    has inspired a formidable corpus of research in neural network quantization spanning
    several decades, and continuing to gain momentum even as we speak, which means
    that while this exploration aspires to be comprehensive and self-contained, we
    will have to round off some detail in order to fit it into memory. Ambitious readers
    should refer to the recent and comprehensive [Gholami et al. 2021](https://arxiv.org/abs/2103.13630)
    survey, or to the [Gray & Neuhoff 1998](https://ieeexplore.ieee.org/document/720541)
    survey for a more historically focused perspective.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，当使用少量资源就能达到等效的数学结果时，没有失败者，反而会带来巨大的收益。这一承诺激发了几十年来神经网络量化领域的丰厚研究成果，并且这一研究势头仍在不断加速，这意味着虽然本文力求全面和自成一体，但为了适应内存的限制，我们将不得不简化一些细节。雄心勃勃的读者可以参考近期的全面综述[Gholami
    et al. 2021](https://arxiv.org/abs/2103.13630)，或是更加注重历史视角的[Gray & Neuhoff 1998](https://ieeexplore.ieee.org/document/720541)综述。
- en: Outline
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大纲
- en: 'Attempting to provide the shortest route to an informed understanding of the
    topic of neural network quantization, the remainder of the article will proceed
    as follows: first, we familiarize ourselves with the mathematics underlying quantization
    so that we are grounded in our discussion. Then, we discover the roots of neural
    network quantization research in the early 1990s, and connect them to the later
    efforts during the “deep learning revolution” that followed the seminal success
    of AlexNet on the image classification task in 2012\. Accordingly, we will witness
    quantization research in the modern era proliferate first in computer vision and
    subsequently take hold in natural language processing, at which point we will
    be prepared to discuss the applications of quantization in today’s world of LLMs,
    and discover the best libraries and tools for incorporating these methods into
    our workflows. Finally, we will reflect on our findings, and discuss directions
    for future work.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供最简明的途径，使读者能够深入理解神经网络量化的主题，本文的其余部分将按以下方式展开：首先，我们将熟悉量化背后的数学原理，为讨论奠定基础。接着，我们将探讨神经网络量化研究的起源，追溯到1990年代初，并将其与2012年AlexNet在图像分类任务上取得开创性成功后的“深度学习革命”联系起来。因此，我们将见证量化研究在现代时代首先在计算机视觉领域的快速发展，随后逐渐渗透到自然语言处理领域，届时我们将能够讨论量化在当今大型语言模型（LLM）世界中的应用，并探索将这些方法融入工作流的最佳库和工具。最后，我们将总结我们的发现，并讨论未来研究的方向。
- en: This article is organized in chapters so that it can be read in clearly defined
    pieces. The reader may choose to skip sections if they are in a hurry to find
    information, but keep in mind that the terminology encountered may be defined
    in earlier chapters. Together, these sections compose a reasonably self-contained
    review on the subject of neural network quantization, and it aspires to equip
    ML practitioners from enthusiast through professionals with a depth of knowledge
    which will enable them to optimize their own workflows. The article concludes
    with an implementation guide for LLM quantization, and time-constrained readers
    can skip directly there.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 本文分章组织，以便读者能清晰地阅读每个部分。如果读者急于查找信息，可以跳过某些部分，但请记住，文中的术语可能在前面的章节中已经定义。这些部分共同构成了一个相对自成一体的神经网络量化综述，旨在为从爱好者到专业人士的机器学习实践者提供深入的知识，帮助他们优化自己的工作流。文章最后附有LLM量化的实施指南，时间紧迫的读者可以直接跳到该部分。
- en: '[**The Mechanics of Quantization**](#479f)'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**量化机制**](#479f)'
- en: a. [Bit Width](#b9d1)
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. [位宽](#b9d1)
- en: b. [Uniform Quantization](#5e01)
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. [均匀量化](#5e01)
- en: c. [Non-uniform Quantization](#9cc4)
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c. [非均匀量化](#9cc4)
- en: d. [Mixed-precision Quantization](#67ba)
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d. [混合精度量化](#67ba)
- en: e. [Scalar vs. Vector Quantization](#e326)
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: e. [标量与矢量量化](#e326)
- en: f. [Compensating for the Effects of Quantization](#5470)
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: f. [补偿量化效应](#5470)
- en: '[**The History of Neural Network Quantization**](#8f81)'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**神经网络量化的历史**](#8f81)'
- en: a. [Early Work in Neural Network Quantization](#999e)
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. [神经网络量化的早期工作](#999e)
- en: b. [Quantization in the Post-AlexNet Era](#b7b5)
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. [后AlexNet时代的量化](#b7b5)
- en: • [Quantization-Aware Training of CNNs](#cc9e)
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: • [卷积神经网络（CNN）的量化感知训练](#cc9e)
- en: • [The Rise of Mixed-Precision Quantization](#1fc5)
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: • [混合精度量化的兴起](#1fc5)
- en: • [Post-Training Quantization of CNNs](#ceaa)
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: • [CNN的训练后量化](#ceaa)
- en: '• [Extreme Quantization: Binary and Ternary Networks](#a358)'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: • [极端量化：二值与三值网络](#a358)
- en: '[**Quantization of LLMs**](#5e4a)'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**LLM的量化**](#5e4a)'
- en: a. [Quantization in the Early Era of Transformers](#68d1)
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. [变压器（Transformer）早期的量化](#68d1)
- en: b. [Post-training Quantization of LLMs](#ee78)
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. [LLM的训练后量化](#ee78)
- en: c. [Quantization-Aware Training of LLMs](#3490)
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c. [LLM的量化感知训练](#3490)
- en: • [Extreme Quantization of LLMs](#b59f)
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: • [LLM的极端量化](#b59f)
- en: '[**Practitioner’s LLM Quantization Guide**](#3761)a. [LLM Quantization Decision
    Tree](#1a5c)'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**实践者的LLM量化指南**](#3761)a. [LLM量化决策树](#1a5c)'
- en: '[**Conclusion**](#4f85)a. [Future Work](#6303)'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**结论**](#4f85)a. [未来工作](#6303)'
- en: The Mechanics of Quantization
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 量化机制
- en: '![](../Images/0c816858fe56724761eb556e0a5124a6.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c816858fe56724761eb556e0a5124a6.png)'
- en: Image by author using DALL-E 3.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用DALL-E 3生成。
- en: To ground our investigation into quantization, it is important to reflect on
    exactly what we mean by “quantizing” numbers. So far we’ve discussed that through
    quantization, we take a set of high-precision values and map them to a lower precision
    in such a way that best preserves their relationships, but we have not zoomed
    into the mechanics of this operation. Unsurprisingly, we find there are nuances
    and design choices to be made concerning how we remap values into the quantized
    space, which vary depending on use case. In this section, we will seek to understand
    the knobs and levers which guide the quantization process, so that we can better
    understand the research and equip ourselves to bring educated decision making
    into our deployments.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了深入理解量化过程，我们有必要反思“量化”数字的确切含义。到目前为止，我们讨论了通过量化，我们将一组高精度值映射到较低精度值，以最佳方式保留它们之间的关系，但我们还没有深入探讨这个操作的机制。不出所料，我们发现关于如何将值重新映射到量化空间的过程中有很多细微差别和设计选择，这些选择因使用场景而异。在本节中，我们将努力理解指导量化过程的参数和杠杆，以便我们能更好地理解研究，并为我们的部署做出更有见地的决策。
- en: Bit Width
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比特宽度
- en: Throughout our discussion on quantization, we will refer to the bit widths of
    the quantized values, which represents the number of bits available to express
    the value. A bit can only store a binary value of 0 or 1, but sets of bits can
    have their combinations interpreted as incremental integers. For instance, having
    2 bits allows for 4 total combinations ({0, 0}, {0, 1}, {1, 0}, {1, 1}) which
    can represent integers in the range [0, 3]. As we add N bits, we get 2 to the
    power of N possible combinations, so an 8-bit integer can represent 256 numbers.
    While *unsigned* integers will count from zero to the maximum value, *signed*
    integers will place zero at the center of the range by interpreting the first
    bit as the +/- sign. Therefore, an unsigned 8-bit integer has a range of [0, 255],
    and a signed 8-bit integer spans from [-128, 127].
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论量化的过程中，我们会提到量化值的比特宽度，这代表了表示该值所需的比特数。一个比特只能存储0或1的二进制值，但一组比特可以将其组合解释为递增的整数。例如，2个比特可以表示4种组合（{0,
    0}，{0, 1}，{1, 0}，{1, 1}），这些组合可以表示整数范围[0, 3]。随着比特数N的增加，我们得到2的N次方种可能的组合，因此一个8位整数可以表示256个数字。虽然*无符号*整数会从零开始计数直到最大值，*有符号*整数则通过将第一个比特解释为正负号，将零置于范围的中心。因此，一个无符号8位整数的范围是[0,
    255]，而一个有符号8位整数的范围是[-128, 127]。
- en: This fundamental knowledge of how bits represent information will help us to
    contextualize the numeric spaces that the floating point values get mapped to
    in the techniques we study, as when we hear that a network layer is quantized
    to 4 bits, we understand that the destination space has 2 to the power of 4 (16)
    discrete values. In quantization, these values do not necessarily represent integer
    values for the quantized weights, and often refer to the indices of the quantization
    *levels —* the “buckets” into which the values of the input distribution are mapped.
    Each index corresponds to a *codeword* that represents a specific quantized value
    within the predefined numeric space. Together, these codewords form a *codebook*,
    and the values obtained from the codebook can be either floating point or integer
    values, depending on the type of arithmetic to be performed. The *thresholds*
    that define the buckets depend on the chosen quantization function, as we will
    see. Note that *codeword* and *codebook* are general terms, and that in most cases
    the codeword will be the same as the value returned from the codebook.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 了解比特如何表示信息的基本知识，将帮助我们更好地理解浮点值在我们研究的技术中映射到的数值空间。例如，当我们听到某个网络层被量化为4位时，我们就能理解目标空间有2的4次方（16）个离散值。在量化过程中，这些值不一定代表量化权重的整数值，通常它们指的是量化*级别*的索引——即将输入分布的值映射到的“桶”。每个索引对应一个*代码字*，表示在预定义数值空间内的一个特定量化值。这些代码字共同构成了*代码本*，从代码本中获得的值可以是浮点值或整数值，这取决于要执行的算术类型。定义这些桶的*阈值*依赖于所选择的量化函数，正如我们将要看到的那样。请注意，*代码字*和*代码本*是通用术语，在大多数情况下，代码字与从代码本返回的值是相同的。
- en: Floating-Point, Fixed-Point, and Integer-Only Quantization
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 浮点、定点和仅整数量化
- en: Now that we understand bit widths, we should take a moment to touch on the distinctions
    between floating-point, fixed-point, and integer-only quantization, so that we
    are clear on their meaning. While representing integers with binary bits is straightforward,
    operating on numbers with fractional components is a bit more complex. Both floating-point
    and fixed-point data types have been designed to do this, and selecting between
    them depends on both on the deployment hardware and desired accuracy-efficiency
    tradeoff, as not all hardware supports floating-point operations, and fixed-point
    arithmetic can offer more power efficiency at the cost of reduced numeric range
    and precision.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了位宽，接下来我们应当花些时间讨论浮点、定点和仅整数量化之间的区别，以便清楚它们的含义。虽然用二进制位表示整数是直接的，但对具有小数部分的数字进行运算就要复杂一些。浮点数和定点数数据类型都是为此设计的，选择它们之间取决于部署硬件和所需的精度与效率权衡，因为并非所有硬件都支持浮点运算，而定点运算在降低数字范围和精度的代价下，能够提供更高的功率效率。
- en: 'Floating-point numbers allocate their bits to represent three pieces of information:
    the *sign*, the *exponent*, and the *mantissa*, which enables efficient bitwise
    operations on their representative values. The number of bits in the exponent
    define the magnitude of the numeric range, and the number of mantissa bits define
    the level of precision. As one example, the IEEE 754 standard for a 32-bit floating
    point (FP32) gives the first bit to the sign, 8 bits to the exponent, and the
    remaining 23 bits to the mantissa. Floating-point values are “floating” because
    they store an exponent for each individual number, allowing the position of the
    radix point to “float,” akin to how scientific notation moves the decimal in base
    10, but different in that computers operate in base 2 (binary). This flexibility
    enables precise representation of a wide range of values, especially near zero,
    which underscores the importance of normalization in various applications.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 浮点数将其位分配用来表示三部分信息：*符号位*、*指数部分*和*尾数*，这使得对它们的表示值进行高效的按位操作成为可能。指数部分的位数定义了数字范围的大小，而尾数位的数量定义了精度水平。举个例子，IEEE
    754标准中32位浮点数（FP32）将第一个位分配给符号，8个位分配给指数，剩余的23个位分配给尾数。浮点数之所以叫“浮动”是因为它们为每个数字存储一个指数，允许基数点的位置“浮动”，类似于科学记数法在10进制中移动小数点，但不同之处在于计算机使用的是2进制（即二进制）。这种灵活性使得可以精确表示广泛范围的数值，尤其是接近零的数值，这也凸显了在各种应用中归一化的重要性。
- en: In contrast, “fixed” point precision does not use a dynamic scaling factor,
    and instead allocates bits into *sign, integer,* and *fractional* (often still
    referred to as *mantissa*) components. While this means higher efficiency and
    power-saving operations, the dynamic range and precision will suffer. To understand
    this, imagine that you want to represent a number which is as close to zero as
    possible. In order to do so, you would carry the decimal place out as far as you
    could. Floating-points are free to use increasingly negative exponents to push
    the decimal further to the left and provide extra resolution in this situation,
    but the fixed-point value is stuck with the precision offered by a fixed number
    of fractional bits.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，“定点”精度不使用动态缩放因子，而是将位分配为*符号位*、*整数部分*和*小数部分*（通常仍称为*尾数*）。虽然这意味着更高的效率和节能操作，但动态范围和精度会受到影响。为了理解这一点，假设你想表示一个尽可能接近零的数字。为此，你需要尽可能地把小数点向右移。浮点数可以自由地使用越来越小的指数将小数点推得更左，从而在这种情况下提供额外的分辨率，但定点数值只能依赖于固定数量的小数位精度。
- en: Integers can be considered an extreme case of fixed-point where no bits are
    given to the fractional component. In fact, fixed-point bits can be operated on
    directly as if they were an integer, and the result can be rescaled with software
    to achieve the correct fixed-point result. Since integer arithmetic is more power-efficient
    on hardware, neural network quantization research favors *integer-only* quantization,
    converting the original float values into integers, rather than the fixed-point
    floats, because their calculations will ultimately be equivalent, but the integer-only
    math can be performed more efficiently with less power. This is particularly important
    for deployment on battery-powered devices, which also often contain hardware that
    only supports integer arithmetic.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 整数可以看作是固定点的一种极端情况，其中没有比特用于小数部分。实际上，固定点的比特可以像整数一样直接操作，计算结果可以通过软件重新缩放以获得正确的固定点结果。由于整数运算在硬件上更节能，因此神经网络量化研究偏好使用*仅整数*量化，将原始浮动值转换为整数，而不是固定点浮动，因为它们的计算最终是等效的，但整数运算可以更加高效并节省能量。这对部署在电池供电的设备上尤其重要，而这些设备通常包含只支持整数运算的硬件。
- en: Uniform Quantization
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 均匀量化
- en: 'To quantize a set of numbers, we must first define a quantization function
    ***Q(r)***, where ***r*** is the real number (weight or activation) to be quantized.
    The most common quantization function is shown below:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 要量化一组数字，我们必须首先定义一个量化函数***Q(r)***，其中***r***是要量化的实数（权重或激活值）。最常见的量化函数如下所示：
- en: '![](../Images/92e277d67930ecbf2efd7cb145428294.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92e277d67930ecbf2efd7cb145428294.png)'
- en: Typical quantization function. Image by author.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的量化函数。图片来源：作者。
- en: In this formula, ***Z*** represents an integer *zero-point*, and ***S*** is
    the *scaling factor*. In *symmetrical quantization*, ***Z*** is simply set to
    zero, and cancels out of the equation, while for *asymmetrical quantization*,
    ***Z*** is used to offset the zero point, allowing for focusing more of the quantization
    range on either the positive or negative side of the input distribution. This
    asymmetry can be extremely useful in certain cases, for example when quantizing
    post-ReLU activation signals, which contain only positive numbers. The **Int(·)**
    function assigns a scaled continuous value to an integer, typically through rounding,
    but in some cases following more complex procedures, as we will encounter later.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，***Z***表示一个整数*零点*，而***S***是*缩放因子*。在*对称量化*中，***Z***简单地设为零，并从方程中取消掉，而在*非对称量化*中，***Z***用于偏移零点，从而可以将量化范围更多地集中在输入分布的正侧或负侧。这种非对称性在某些情况下非常有用，例如在量化ReLU激活信号时，这些信号只包含正数。**Int(·)**函数将一个缩放后的连续值分配给一个整数，通常通过四舍五入，但在某些情况下会遵循更复杂的过程，正如我们稍后将遇到的那样。
- en: 'Choosing the correct scaling factor (***S*)** is non-trivial, and requires
    careful consideration of the distribution of values to be quantized. Because the
    quantized output space has a finite range of values (or *quantization* *levels)*
    to map the inputs to, a *clipping range* [α, β] must be established that provides
    a good fit for the incoming value distribution. The chosen clipping range must
    strike a balance between not over-clamping extreme input values and not oversaturating
    the quantization levels by allocating too many bits to the long tails. For now,
    we consider *uniform quantization*, where the bucketing thresholds, or *quantization
    steps*, are evenly spaced. The calculation of the scaling factor is as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 选择正确的缩放因子（***S***）并非易事，需要仔细考虑要量化值的分布。由于量化后的输出空间具有有限的值范围（或*量化* *级别*），用于映射输入值，因此必须建立一个合适的*裁剪范围*
    [α, β]，以便很好地适应输入值分布。所选的裁剪范围必须在不对极端输入值过度夹紧和不通过为长尾分配过多比特而过度饱和量化级别之间取得平衡。现在，我们考虑*均匀量化*，即量化阈值或*量化步长*是均匀分布的。缩放因子的计算公式如下：
- en: '![](../Images/d7c7fb18be4341f52896fb996f9bb06c.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d7c7fb18be4341f52896fb996f9bb06c.png)'
- en: Formula for calculating the quantization function’s scaling factor (*S) based
    on the clipping range (*[α, β]) *and desired bit-width (b). Image by author.*
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 基于裁剪范围（*[α, β]*）和所需比特宽度（b）来计算量化函数的缩放因子（*S*）的公式。图片来源：作者。
- en: The shapes of trained parameter distributions can vary widely between networks
    and are influenced by a number of factors. The activation signals generated by
    those weights are even more dynamic and unpredictable, making any assumptions
    about the correct clipping ranges difficult. This is why we must *calibrate* the
    clipping range based on our model and data. For best accuracy, practitioners may
    choose to calibrate the clipping range for activations online during inference,
    known as *dynamic* quantization. As one might expect, this comes with extra computational
    overhead, and is therefore by far less popular than *static* quantization, where
    the clipping range is calibrated ahead of time, and fixed during inference.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的参数分布的形状在不同的网络之间可能会有很大差异，并且受到多个因素的影响。由这些权重生成的激活信号更加动态和不可预测，这使得关于正确剪裁范围的任何假设都变得困难。这就是为什么我们必须根据我们的模型和数据来*校准*剪裁范围。为了获得最佳准确性，实践者可能会选择在推理过程中在线校准激活的剪裁范围，这就是*动态*量化。如人所料，这会带来额外的计算开销，因此比*静态*量化的使用要少，后者是在推理之前就预先校准好剪裁范围并固定在推理过程中。
- en: '**Dequantization** Here we establish the reverse uniform quantization operation
    which decodes the quantized values back into the original numeric space, albeit
    imperfectly, since the rounding operation is non-reversible. We can decode our
    approximate values using the following formula:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**反量化** 在这里我们建立了反向均匀量化操作，它将量化后的值解码回原始数值空间，尽管这种解码并不完美，因为四舍五入操作是不可逆的。我们可以使用以下公式解码我们的近似值：'
- en: '![](../Images/b03d952b3651983b1e4e9cd26cfafe3e.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b03d952b3651983b1e4e9cd26cfafe3e.png)'
- en: Dequantization operation. Image by author.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 反量化操作。图像由作者提供。
- en: Non-Uniform Quantization
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非均匀量化
- en: 'The astute reader will probably have noticed that enacting uniformly-spaced
    bucketing thresholds on an input distribution that is any shape other than uniform
    will lead to some bits being far more saturated than others, and that adjusting
    these widths to focus more bits in the denser regions of the distribution would
    more faithfully capture the nuances of the input signal. This concept has been
    investigated in the study of *non-uniform quantization*, and has indeed shown
    benefits in signal fidelity; however, the hardware-optimized calculations made
    possible by *uniform quantization* has made it the de-facto neural network quantization
    method. The equation below describes the non-uniform quantization process:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 敏锐的读者可能已经注意到，对任何形状非均匀的输入分布施加均匀间隔的分桶阈值会导致某些位比其他位更饱和，而且调整这些宽度，以便更多地集中在分布密集区域的位上，将更忠实地捕捉输入信号的细微差别。这个概念已经在*非均匀量化*的研究中得到了探讨，并且在信号保真度上确实带来了好处；然而，由*均匀量化*所支持的硬件优化计算使其成为事实上的神经网络量化方法。下面的方程描述了非均匀量化过程：
- en: '![](../Images/54b846f331e47b79af4c6de8e3d88591.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/54b846f331e47b79af4c6de8e3d88591.png)'
- en: Non-Uniform Quantization formula, where **Xi** are **quantization levels**,
    and **∆i** are the **quantization steps**. Image by author.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 非均匀量化公式，其中**Xi**是**量化级别**，**∆i**是**量化步长**。图像由作者提供。
- en: Many works in non-uniform quantization refer to learning *centroids*, which
    represent the centers of clusters in the input distribution to which the surrounding
    values are mapped through the quantization process. To think of this another way,
    in uniform quantization, where the thresholds are evenly spaced on the input distribution,
    the centroids are simply the values directly in between the bucketing thresholds.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 许多非均匀量化的研究提到了学习*质心*，它们代表了输入分布中簇的中心，通过量化过程将周围的值映射到这些中心。换句话说，在均匀量化中，阈值均匀地分布在输入分布上，质心就是简单地位于分桶阈值之间的值。
- en: Mixed-Precision Quantization
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混合精度量化
- en: 'As we saw with pruning, a trained neural network’s performance is more sensitive
    to changes in some layers and submodules than others, and by measuring these sensitivities,
    entire pieces of neural networks can be removed without significantly affecting
    error. Intuitively, the same is true for varying levels of quantization, with
    some network components capable of being remapped to much lower bit widths than
    their counterparts. The most fundamental example of this we already mentioned:
    the use of 16-bit floats in less-sensitive network operations to substantially
    reduce memory footprint during training, but *mixed-precision quantization* can
    refer to any combination of different quantization levels throughout a network.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在剪枝中看到的，经过训练的神经网络在某些层和子模块中的性能对变化更为敏感，而通过测量这些敏感性，整个神经网络的部分可以被移除，而不会显著影响误差。直观地说，量化的不同级别也遵循同样的规律，某些网络组件可以重新映射到比其他组件低得多的位宽。我们已经提到的最基本的例子就是：在不太敏感的网络操作中使用16位浮点数，从而在训练期间大大减少内存占用，但*混合精度量化*可以指的是整个网络中不同量化级别的任何组合。
- en: Related to the concept of mixed-precision quantization is the *granularity*
    of quantization, which might be layer-wise, group-wise, channel-wise, or sub-channel-wise,
    and describes the scale at which distinct sets of quantization parameters are
    calibrated. Intuitively, computational overhead increases with granularity, representing
    an accuracy/efficiency trade-off. For example, in convolutional neural networks
    (CNNs), channel-wise granularity is often the weapon of choice, since sub-channel-wise
    (i.e. filter-wise) quantization would be too complex.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 与混合精度量化概念相关的是量化的*粒度*，它可以是按层、按组、按通道或按子通道划分，并描述了在这些不同量化参数集被校准时所使用的尺度。直观地说，粒度越细，计算开销就越大，这代表了精度与效率之间的权衡。例如，在卷积神经网络（CNN）中，按通道划分的粒度通常是首选，因为按子通道（即按滤波器）量化会过于复杂。
- en: Scalar vs. Vector Quantization
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标量量化与向量量化
- en: While the majority of research in quantization has historically focused on quantizing
    individual values within the matrices, it is possible to learn multidimensional
    centroids as well. This means that matrices can be split into vectors, and then
    each of those vectors can be given a codeword that points to their closest centroid,
    creating the possibility of recovering entire pieces of the matrix from single
    codebook lookups, effectively storing a set of numbers into a single value, and
    greatly increasing compression levels. This is known as *Vector Quantization*,
    and the advantages it offers has been attracting increasing interest. “Vector
    Quantization” typically refers to splitting the matrices into column vectors,
    but these vectors can be further split into sub-vectors in a practice known as
    *Product Quantization*, which generalizes both vector and scalar quantization
    at its extremes. The idea is that the assembly of centroid vectors returned from
    the codebook using the relatively small structure of stored codewords will faithfully
    recreate the original, larger matrix. We will see that this has indeed proven
    to be a very powerful model compression technique.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管量化领域的大多数研究历来集中在量化矩阵中的单一值，但同样也可以学习多维的质心。这意味着矩阵可以被分割成向量，然后这些向量中的每一个都可以赋予一个指向其最近质心的码字，从而有可能通过单次查找代码书恢复矩阵的整个部分，实际上将一组数字存储为一个单一的值，从而大大增加了压缩比。这种方法被称为*向量量化*，它的优势已引起越来越多的关注。“向量量化”通常指的是将矩阵分割成列向量，但这些向量也可以进一步被分割成子向量，这种做法称为*产品量化*，它在极端情况下可以推广向量量化和标量量化。其核心思想是，使用从代码书中返回的质心向量集合，利用较小的存储码字结构，能够忠实地重建原始的、较大的矩阵。我们将看到，这确实证明是一种非常强大的模型压缩技术。
- en: Compensating for the Effects of Quantization
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 补偿量化效果
- en: It makes sense that we cannot simply round all of the weights in a neural network
    to various resolutions and expect that things still work properly, so we must
    come up with a plan for how to compensate for the perturbations caused by the
    quantization process. As we learned above, it is possible to train or fine-tune
    models under simulated quantization in order to drastically increase the amount
    of quantization that can be performed without affecting performance in a technique
    called *Quantization-Aware Training* (QAT), which also allows for learning the
    quantization parameters during training. However, performing QAT requires having
    the hardware and data necessary to train the model, which is often not possible,
    particularly for very large models like today’s LLMs. To address this issue, Post-Training
    Quantization (PTQ) techniques aim to avoid training and require only a small amount
    of unlabeled data to calibrate the quantization function, and Zero-Shot Quantization
    (ZSQ) explores the ideal “data-free” scenario which requires no data for calibration.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能仅仅将神经网络中的所有权重四舍五入到不同的精度并期望一切正常，因此我们必须制定一个计划，来弥补量化过程中所带来的扰动。正如我们上面所学到的那样，通过模拟量化来训练或微调模型，可以大幅提高可执行的量化程度，而不会影响性能，这项技术称为*量化感知训练*（QAT），它还允许在训练过程中学习量化参数。然而，执行
    QAT 需要具备必要的硬件和数据来训练模型，而这通常是不可行的，尤其是对于像今天的 LLM（大型语言模型）这样非常庞大的模型。为了解决这个问题，后训练量化（PTQ）技术旨在避免训练，仅需少量无标签数据来校准量化函数，而零样本量化（ZSQ）则探索了理想的“无数据”情境，该情境不需要任何数据进行校准。
- en: We will see each these techniques highlighted in more detail as we journey through
    the literature, so let us now board our temporal tour bus and travel back to the
    end of the last century, when researchers were being similarly tantalized by the
    power of neural networks which exceeded their hardware limitations, and first
    started to consider how we might hope to deploy these complex models on mobile
    hardware.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们通过文献深入探讨这些技术时，我们将更详细地看到每种技术，因此让我们现在乘坐时光旅行车，回到上世纪末，当时研究人员也同样被神经网络的强大能力所吸引，这些网络超出了他们的硬件限制，并开始考虑我们如何希望将这些复杂的模型部署到移动硬件上。
- en: The History of Neural Network Quantization
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络量化的历史
- en: '![](../Images/35080b72d42552e80ffc86da605fa28c.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/35080b72d42552e80ffc86da605fa28c.png)'
- en: Image by author using DALL-E 3.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由作者使用 DALL-E 3 创建。
- en: Early Work in Neural Network Quantization
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络量化的早期研究
- en: In the family of model compression techniques, network quantization is an only
    slightly younger sibling to pruning, with roots also tracing back to the formative
    years of [backprop-trained neural networks](https://stanford.edu/~jlmcc/papers/PDP/Volume%201/Chap8_PDP86.pdf)
    in the late 1980s. This era of advancing computational hardware felt a resurgence
    of interest in neural network research, but practical use was nonetheless restricted
    as the complexity of problems still had to be severely limited due to hardware
    constraints, effectively precluding the use cases for which neural networks are
    best suited. While researchers had implicitly dealt with considerations surrounding
    numeric precision in neural networks for decades, this increasing sense of computational
    confinement inspired a newfound focus on reducing numeric precision for optimization,
    producing an impressive spread of investigations around 1990.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型压缩技术家族中，网络量化是与剪枝仅稍年轻的一个“兄弟”，其根源也可以追溯到 1980 年代后期的[反向传播训练神经网络](https://stanford.edu/~jlmcc/papers/PDP/Volume%201/Chap8_PDP86.pdf)的早期阶段。那一时代的计算硬件进步引发了对神经网络研究的复兴，但由于硬件的限制，问题的复杂性仍然需要大大缩小，这实际上排除了神经网络最适用的使用场景。虽然研究人员在几十年来隐性地处理了神经网络中的数值精度问题，但这种日益增长的计算限制感促使人们将注意力转向优化数值精度，产生了1990年左右的广泛研究。
- en: In 1989, Baker & Hammerstrom were the first to systematically study the impact
    of reduced numeric precision on network performance with the direct intention
    of enabling hardware optimization. Their work “[Characterization of Artificial
    Neural Network Algorithms](https://ieeexplore.ieee.org/document/100296)” was an
    early example of successfully training networks with backpropagation using reduced
    precision computation, challenging the conventional wisdom that 32-bit floating
    point operations were necessary for preserving network performance. [Hollis et
    al. 1990](https://direct.mit.edu/neco/article-abstract/2/3/363/5539/The-Effects-of-Precision-Constraints-in-a?redirectedFrom=PDF)
    further studied the impact of precision constraints on backprop network training,
    and similarly found a sharp drop-off in learning capability at around 12 bits
    of precision. A year later in 1990, Dan Hammerstrom built on this previous work
    and [investigated the design of new hardware units explicitly optimized for fixed
    precision calculations in neural networks](https://ieeexplore.ieee.org/document/5726581),
    demonstrating that significant gains in efficiency could be obtained with tolerable
    drops in performance using 16-bit or even 8-bit precision, laying the foundation
    for future work in quantization and hardware optimization for neural networks.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 1989年，Baker 和 Hammerstrom首次系统地研究了减少数值精度对网络性能的影响，旨在启用硬件优化。他们的工作《[人工神经网络算法的表征](https://ieeexplore.ieee.org/document/100296)》是一个早期成功的例子，展示了如何使用减少精度计算训练网络的反向传播方法，挑战了32位浮动点运算是保持网络性能所必需的传统观念。[Hollis
    等人 1990年](https://direct.mit.edu/neco/article-abstract/2/3/363/5539/The-Effects-of-Precision-Constraints-in-a?redirectedFrom=PDF)进一步研究了精度约束对反向传播网络训练的影响，并同样发现学习能力在大约12位精度时出现了急剧下降。1990年，一年后，Dan
    Hammerstrom基于之前的工作，[研究了为神经网络中的固定精度计算显式优化的新硬件单元的设计](https://ieeexplore.ieee.org/document/5726581)，证明了在使用16位甚至8位精度时，效率可以获得显著提升，同时性能下降在可容忍范围内，为未来的量化和神经网络硬件优化工作奠定了基础。
- en: As an interesting fact, investigations into exploiting lower bit-widths and
    special hardware for optimizing neural networks were being made even before backprop
    became the uncontested champion of neural network learning algorithms. In a formative
    work from 1992, [Hoehfeld and Fahlman](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=ed7eaefb93994bfee42309bbd7cda541c08696ba)
    investigated the effects of limited numerical precision on training networks with
    the Cascade-Correlation algorithm ([Fahlman & Lebiere, 1991](https://www.researchgate.net/publication/2427110_The_Cascade-Correlation_Learning_Architecture)),
    demonstrating that this learning algorithm was similarly receptive to operating
    at fixed precision. As a component of their success, the authors outline techniques
    for dynamic rescaling and probabilistic rounding that enable convergence at much
    lower precision (6-bit in their case) which are applicable to any gradient-based
    learning algorithm.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的事实是，在反向传播算法成为神经网络学习算法的不争冠军之前，已经有研究探讨了利用较低比特宽度和专用硬件来优化神经网络。在1992年的一项开创性工作中，[Hoehfeld
    和 Fahlman](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=ed7eaefb93994bfee42309bbd7cda541c08696ba)研究了有限数值精度对使用级联相关算法（[Fahlman
    & Lebiere, 1991](https://www.researchgate.net/publication/2427110_The_Cascade-Correlation_Learning_Architecture)）训练网络的影响，展示了该学习算法在固定精度下也能很好地运行。作为其成功的组成部分，作者概述了动态缩放和概率舍入的技术，这些技术能够在更低精度下（例如他们的研究中为6位）实现收敛，并且可以应用于任何基于梯度的学习算法。
- en: This foundational period in the exploration of network quantization illuminated
    the path for discovering the sophisticated techniques and hardware-specific optimizations
    that populate today’s world of lumbering giants. By demonstrating the feasibility
    and benefits of reduced precision computation, these early works expanded the
    possibilities of neural network applications, providing a powerful method for
    developing more efficient and scalable AI systems. Today, as we stand on the brink
    of ubiquitous AI integration across diverse platforms, the legacy of these pioneering
    efforts is more relevant than ever. They not only showcased the potential of model
    compression and efficient computation through quantization, but also inspired
    a continuous quest for innovation in neural network design and optimization.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络量化探索的这一基础时期，揭示了发现精细技术和硬件特定优化的路径，这些技术和优化充斥着今天庞大的人工智能领域。通过展示低精度计算的可行性和好处，这些早期的工作扩展了神经网络应用的可能性，为开发更高效、更可扩展的AI系统提供了一种强大的方法。如今，随着我们站在人工智能在各平台上普及的前沿，这些开创性努力的遗产比以往任何时候都更具相关性。它们不仅展示了通过量化进行模型压缩和高效计算的潜力，而且激发了对神经网络设计和优化的持续创新探索。
- en: Quantization in the Post-AlexNet Era
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 后AlexNet时代的量化
- en: 'In 2012, the authors of [AlexNet](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
    capitalized on a serendipitous confluence of major developments in data availability
    and computational hardware to eclipse the performance of previous state-of-the-art
    approaches in the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC).
    Two major elements helped to make this historic success possible: 1) the efforts
    of [Fei-Fei Li and her team at Princeton](https://image-net.org/static_files/papers/imagenet_cvpr09.pdf),
    who provided the world’s first large-scale curated image dataset, and 2) a fortuitous
    coincidence that the well-funded advancements in Graphic Processing Unit (GPU)
    technology, which were being driven by a healthy stream of gaming industry revenue,
    happened to produce hardware that offers the same type of parallelized computation
    needed to accelerate the matrix operations in deep learning.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 2012年，[AlexNet](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)的作者利用了数据可用性和计算硬件领域重大进展的偶然交汇，超越了以往在ImageNet大规模视觉识别挑战赛（ILSVRC）中的最先进方法。促成这一历史性成功的两个关键因素是：1）[费费·李及其在普林斯顿大学的团队](https://image-net.org/static_files/papers/imagenet_cvpr09.pdf)提供了全球首个大规模精心策划的图像数据集；2）一个偶然的巧合，即由健康的游戏产业收入推动的图形处理单元（GPU）技术的资金支持，恰好推动了能够加速深度学习中矩阵运算的并行计算硬件的诞生。
- en: 'Given these auspicious circumstances, Alex Krizhevsky and his team trained
    a sizeable Convolutional Neural Network (CNN) of 62.3 million parameters and blew
    past the competitors with a >10% lead in accuracy over the runner up, marking
    a watershed PR moment for neural network research, and kicking off an enduring
    period of intense interest and funding that is often referred to as the “deep
    learning revolution.” However, the rejuvenated neural nets were quickly met by
    their old foe: hardware limitations. Despite the profound benefits of GPU-accelerated
    training, the authors of AlexNet conceded that hardware constraints imposed a
    limiting factor on the success of their approach, and that the results would likely
    improve with better hardware. The research community was quick to recognize the
    unaddressed potential of model compression to address these limitations, and rose
    to action. The revelations made during this CNN-focused period about the varying
    sensitivity levels among different types of network components would provide a
    strong base for the future investigations into transformers.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些有利的环境下，Alex Krizhevsky及其团队训练了一个具有6230万个参数的大型卷积神经网络（CNN），并凭借超过10%的准确率领先于第二名，创造了神经网络研究的一个重要时刻，开启了一个持续的深度兴趣和资金投入的时期，这个时期通常被称为“深度学习革命”。然而，焕发活力的神经网络很快就遇到了它们的老对手：硬件限制。尽管GPU加速训练带来了深远的好处，AlexNet的作者也承认，硬件约束对他们方法的成功构成了限制因素，而且随着硬件的改进，结果可能会有所提升。研究界迅速意识到，模型压缩未被解决的潜力可以应对这些限制，并迅速付诸行动。在这个以CNN为重点的时期，关于不同类型网络组件之间的敏感性差异的发现，为未来对变换器（transformer）的研究提供了强有力的基础。
- en: In fact, the echoing desires to deploy neural networks at the edge were already
    being heard before AlexNet shook the world. [Vanhoucke et al.’s seminal 2011 work](https://research.google/pubs/improving-the-speed-of-neural-networks-on-cpus/)
    explored the acceleration of neural nets on x86 CPUs. Written at a time when the
    AI community was at a crossroads — debating whether to invest in GPUs or to extract
    more performance from traditional CPUs — their paper offered a pivotal guide on
    optimizing neural network operations on Intel and AMD CPUs. Predating the era
    of GPU dominance ushered in by AlexNet, Vanhoucke et al. showcased the untapped
    potential of CPUs through meticulous optimizations, including the adoption of
    fixed-point and integer arithmetic complemented by SIMD instructions and memory
    alignment techniques. Using these optimizations, the authors achieved significant
    performance gains, and laid the groundwork for upcoming research into the efficient
    training and deployment of neural networks on CPU hardware.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，关于将神经网络部署到边缘设备的愿望早在AlexNet震撼世界之前就已经有了。[Vanhoucke 等人 2011 年的开创性工作](https://research.google/pubs/improving-the-speed-of-neural-networks-on-cpus/)探讨了如何加速在
    x86 CPU 上运行的神经网络。在AI社区面临十字路口——在投资GPU还是从传统CPU中挖掘更多性能的讨论中——他们的论文为如何在Intel和AMD CPU上优化神经网络操作提供了关键指导。Vanhoucke
    等人在AlexNet引领GPU主导时代之前，展示了CPU通过精心优化所蕴藏的巨大潜力，包括采用定点和整数算术，辅以SIMD指令和内存对齐技术。通过这些优化，作者们实现了显著的性能提升，为未来在CPU硬件上进行神经网络高效训练和部署的研究奠定了基础。
- en: After the success of AlexNet, CNNs became the new soil in which a rapid-growing
    crop of quantization research would grow. Researchers grappled with the nuances
    of quantizing different types of network layers, with their varying levels of
    sensitivity and advantages to be offered. For instance, most of the FLOPs in CNNs
    occur in the convolutional layers, so quantizing them offers the best gains in
    speed; however, these layers also house the parameters most crucial to feature
    extraction, rendering them particularly sensitive to alteration. Fully-connected
    layers, on the other hand, tend to be much easier to compress, but doing so is
    advantageous mostly in terms of storage size rather than the latency, as these
    layers contribute less to the overall computational graph.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在AlexNet成功之后，CNN成为了量化研究迅速发展的沃土。研究人员与量化不同类型网络层的细节作斗争，这些网络层具有不同的敏感性和提供的优势。例如，大多数CNN的FLOP发生在卷积层，因此对这些层进行量化能带来最大的速度提升；然而，这些层也包含了对特征提取至关重要的参数，使得它们对改变特别敏感。另一方面，完全连接层通常更容易压缩，但这样做主要在存储大小方面具有优势，而在延迟方面则较少影响，因为这些层对整体计算图的贡献较小。
- en: In addition to the distinction between techniques providing storage-only vs.
    full efficiency gains, there is also a distinction in the latter group between
    techniques which seek to accelerate both training and inference vs. those that
    only seek to accelerate inference. The concept of QAT was born during this period,
    and while many techniques opt to use simulated quantization during training, others
    stuck closer to the roots of network quantization and explored the use of fixed-point
    or integer-only arithmetic during both training and inference to pursue end-to-end
    neural network development at the edge.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 除了提供仅存储与完全效率提升的技术之间的区别外，后者组中的技术也有区分，区分在于一些技术旨在加速训练和推理，而另一些则仅仅致力于加速推理。QAT（量化感知训练）概念就是在这一时期诞生的，尽管许多技术选择在训练过程中使用模拟量化，另一些则更接近网络量化的根源，探索在训练和推理过程中使用定点或仅整数算术，以实现边缘设备上端到端的神经网络开发。
- en: As two early examples of these diverse approaches to CNN compression through
    quantization, Denton et al.’s 2014 “[Exploiting Linear Structure Within Convolutional
    Networks for Efficient Evaluation](https://arxiv.org/abs/1404.0736)” method improves
    computational efficiency by applying matrix factorization as either a PTQ or QAT
    procedure in the convolutional layers, while Gong et al.’s 2014 “[Compressing
    Deep Convolutional Networks using Vector Quantization](https://arxiv.org/abs/1412.6115)”
    focuses instead on achieving storage size optimization by compressing the fully-connected
    layers using a variety of vector quantization methods in a PTQ setting, remarking
    on the distinct superiority of product quantization.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 作为两种早期的通过量化进行CNN压缩的不同方法，Denton等人2014年提出的"[利用卷积网络中的线性结构提高计算效率](https://arxiv.org/abs/1404.0736)"方法通过在卷积层应用矩阵分解，作为PTQ或QAT过程，从而提高了计算效率，而Gong等人2014年提出的"[使用向量量化压缩深度卷积网络](https://arxiv.org/abs/1412.6115)"则专注于通过在PTQ环境中使用多种向量量化方法压缩全连接层，以优化存储大小，并指出了乘积量化的明显优越性。
- en: In this section, we will watch the field of quantization come into focus during
    the era of CNN dominance unleashed by AlexNet. In this formative period of growth,
    we will see QAT, mixed-precision quantization, PTQ, and extreme quantization down
    to 1 or 2 bits become well-defined areas of research, setting the stage for our
    exploration into the maturation of these techniques in today’s era of large models.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到量化领域如何在AlexNet引发的CNN主导时代中逐渐成型。在这个成长的关键时期，我们将看到QAT、混合精度量化、PTQ以及将量化极限推至1位或2位的极限量化成为明确定义的研究领域，为我们探索这些技术在今天大模型时代的成熟奠定基础。
- en: Quantization-Aware Training of CNNs
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积神经网络的量化感知训练
- en: '![](../Images/e7fe23e264609bad8ea6ed3634fdefa3.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7fe23e264609bad8ea6ed3634fdefa3.png)'
- en: Image by author using DALL-E 3.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用DALL-E 3生成。
- en: It was during the post-AlexNet era that QAT truly took form as a distinct area
    of quantization research. In former eras, nearly all work in quantization used
    the training process to optimize weight discretization, since the networks in
    question were relatively small. Even after the AI growth spurt triggered by GPU-accelerated
    training, the models were still trainable using a reasonable amount of resources,
    and concerns about avoiding the necessity of retraining quantized networks were
    mostly motivated by mobile deployment and data access/privacy concerns, rather
    than training resources. Nonetheless, the value of PTQ was becoming clear during
    this era, and the distinction between the two fields materialized. Given the reasonable
    training costs, the bulk of quantization research in the CNN era stuck to the
    roots of QAT-based approaches. In this section, we review the development of QAT
    approaches during the CNN era.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 正是在后AlexNet时代，QAT真正发展成为量化研究的一个独立领域。在以前的时代，几乎所有量化工作的重点都放在了使用训练过程来优化权重离散化，因为当时的网络规模相对较小。即使在GPU加速训练推动AI快速发展的时期，模型仍然能够在合理的资源下进行训练，避免重新训练量化网络的需求，更多是出于移动部署和数据访问/隐私问题的考虑，而非训练资源的限制。然而，PTQ的价值在这个时代逐渐显现，两者领域的区别也逐步明确。考虑到合理的训练成本，CNN时代的大多数量化研究仍然坚持基于QAT方法的根本。在本节中，我们回顾了CNN时代QAT方法的发展。
- en: We start in late 2014, when [Courbariaux et al.](https://arxiv.org/abs/1412.7024)
    observed that “multipliers are the most space and power-hungry arithmetic operators
    of the digital implementation of deep neural networks,” and investigated specifically
    reducing the precision of these operations for efficiency gains, since their cost
    scales quadratically in relation to bit width, whereas the costs of the other
    operators in the multiplier-accumulator (MAC) operations (the adder and accumulator)
    only scale linearly, and are therefore comparably inexpensive. Notably, their
    study showed that “the use of half precision floating point format has little
    to no impact on the training of neural networks.” Further, the authors found that
    “*very low precision is sufficient* not just for running trained networks but
    *also for training them*,” although at this point in time, “very low precision”
    is referring to 10-bit multiplications, an emblem of how rapidly this field has
    shifted bits, as we will see.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从2014年底开始，正是[Courbariaux等人](https://arxiv.org/abs/1412.7024)观察到“乘法器是数字实现深度神经网络中最占用空间和能源的算术运算符”，并专门研究了降低这些运算精度以提高效率的问题，因为它们的成本与位宽成平方关系，而乘法累加器（MAC）操作中其他运算符（加法器和累加器）的成本仅与位宽成线性关系，因此相对较便宜。值得注意的是，他们的研究表明“使用半精度浮点格式对神经网络训练几乎没有影响。”此外，作者还发现“*极低精度足够*不仅可以运行训练好的网络，而且*也足以训练它们*”，尽管在当时，“极低精度”指的是10位乘法，这是该领域变化迅速的一个标志，正如我们将看到的那样。
- en: In 2015, Gupta et al. from IBM published “[Deep Learning with Limited Numerical
    Precision](https://arxiv.org/abs/1502.02551),” introducing a pioneering approach
    to training deep neural networks in 16-bit fixed-point arithmetic using *stochastic
    rounding —* where the probability of rounding a number to either of its nearest
    quantization points is proportional to their proximity. This rounding method outperforms
    the round-to-nearest approach by introducing noise which drives the expected value
    (bias) of quantization error to zero. Unlike conventional QAT methods that often
    rely on full-precision floating-point operations during training, Gupta et al.’s
    strategy involves executing all training computations in lower precision. The
    use of fixed point arithmetic allows using faster and more power and space-efficient
    compute units, and the authors explore hardware co-design by demonstrating a novel
    energy-efficient hardware accelerator. Importantly, the use of stochastic rounding
    ensures that even small gradient values contribute to the training process, thereby
    diminishing the reliance on gradient approximation methods like the Straight-Through
    Estimator (STE).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 2015年，IBM的Gupta等人发表了《[使用有限数值精度的深度学习](https://arxiv.org/abs/1502.02551)》一文，提出了一种开创性的深度神经网络训练方法，该方法使用*随机舍入*技术，通过16位定点算术实现，其中舍入一个数值到其最近量化点的概率与这些量化点的接近程度成正比。与传统的四舍五入方法相比，这种舍入方法通过引入噪声，使得量化误差的期望值（偏差）趋近于零。与通常依赖于全精度浮点运算的量化感知训练（QAT）方法不同，Gupta等人的策略是在训练过程中执行所有计算时使用较低的精度。使用定点算术使得可以使用更快速、更节能且更节省空间的计算单元，作者还通过展示一种新型的节能硬件加速器，探讨了硬件协同设计。重要的是，使用随机舍入确保了即使是较小的梯度值也能对训练过程产生贡献，从而减少了对梯度近似方法（如直通估计器（STE））的依赖。
- en: '![](../Images/54ebdc5549bb68480edcc27c76864b40.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/54ebdc5549bb68480edcc27c76864b40.png)'
- en: Results from [Gupta et al. 2015](https://arxiv.org/abs/1502.02551) show the
    power of stochastic rounding, as the 16bit fixed-point training with 14 bits allocated
    to the fractional length (FL) nearly matches the floating-point training curve,
    while the same fixed-point arithmetic using “Round to Nearest” caused training
    to diverge.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[Gupta等人2015年](https://arxiv.org/abs/1502.02551)的结果显示了随机舍入的强大作用，16位定点训练中，14位分数长度（FL）几乎与浮点训练曲线匹配，而使用“四舍五入到最近”定点算术则导致训练发散。
- en: Han et al.’s 2015 “[Deep Compression](https://arxiv.org/abs/1510.00149)” is
    a foundational dictionary-based (i.e. codebook) method for achieving extraordinary
    levels of compression in neural networks without sacrificing performance using
    a hybrid quantization and pruning approach. Motivated to bring the breakthroughs
    in computer vision into mobile applications, the authors experimented on the workhorse
    CNNs of the day, achieving compression rates of 35x and 49x on AlexNet and VGG-16
    respectively, with no loss in accuracy, by using a three-stage pipeline which
    sequentially prunes, quantizes, and finally applies [Huffman coding](https://webspace.science.uu.nl/~leeuw112/huffman.pdf)
    (a form of lossless data compression) to the network weights. First, the unimportant
    weights are identified and pruned from the model using the seminal unstructured
    approach of Han et al.’s prior 2015 work “[Learning both Weights and Connections
    for Efficient Neural Networks](https://arxiv.org/abs/1506.02626)” to achieve between
    9x and 13x compression with no increase in error. Second, the remaining weights
    are quantized from 32 down to 5 bits, followed by a round of retraining to recover
    performance, then the approach finishes with Huffman coding of the quantized weights
    for an additional 20–30% reduction in storage size.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Han 等人 2015 年的 “[Deep Compression](https://arxiv.org/abs/1510.00149)” 是一种基于字典（即代码本）的方法，通过混合量化和剪枝技术，在不牺牲性能的情况下，实现在神经网络中达到极高压缩比的基础方法。作者的动机是将计算机视觉领域的突破应用到移动设备中，他们对当时的主流卷积神经网络（CNN）进行了实验，分别在
    AlexNet 和 VGG-16 上实现了 35 倍和 49 倍的压缩率，且精度没有下降。这一成果是通过一个三阶段管道实现的，该管道依次进行剪枝、量化，最后对网络权重应用
    [Huffman 编码](https://webspace.science.uu.nl/~leeuw112/huffman.pdf)（一种无损数据压缩方法）。首先，使用
    Han 等人 2015 年先前工作 “[Learning both Weights and Connections for Efficient Neural
    Networks](https://arxiv.org/abs/1506.02626)” 中的开创性无结构方法，识别并剪除模型中不重要的权重，从而实现 9
    倍到 13 倍的压缩，且没有引入误差。其次，将剩余的权重从 32 位量化为 5 位，之后进行一轮再训练以恢复性能，最后对量化后的权重进行 Huffman 编码，实现额外的
    20%-30% 存储空间减少。
- en: '![](../Images/b9b78627877b8cd85b5564a5dcd18152.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b9b78627877b8cd85b5564a5dcd18152.png)'
- en: Flowchart from “[Deep Compression](https://arxiv.org/abs/1510.00149)” illustrates
    their three stage compression technique.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 “[Deep Compression](https://arxiv.org/abs/1510.00149)” 的流程图展示了他们的三阶段压缩技术。
- en: The results from “Deep Compression” are staggering, with the method compressing
    CNNs to less than 1/35 their original size while demonstrating equal or superior
    accuracy compared with their baseline references. However, it is important to
    remember that both the AlexNet and VGG architectures studied are intentionally
    overparameterized to maximize performance, and they both have contain parameter-dense
    yet relatively insensitive fully-connected layers and the end which can be compressed
    heavily. While the method is primarily focused on enhancing the storage footprint
    of the model, the authors remark that smaller storage also means accelerated operation,
    as there are less weights to store and fetch, and therefore reduced memory bandwidth
    requirements, particularly when the model size is reduced enough to be stored
    on-chip in the Static Random Access Memory (SRAM) rather than being bounced back
    and forth between Dynamic RAM (DRAM) and SRAM. Further, the authors introduce
    the concept of the Efficient Inference Engine (EIE), a hardware accelerator designed
    to leverage the sparsity resulting from pruning, which would be the subject of
    [their forthcoming publication](https://arxiv.org/abs/1602.01528).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: “[Deep Compression](https://arxiv.org/abs/1510.00149)” 的结果令人震惊，该方法将 CNN 压缩至原始大小的不到
    1/35，同时在精度上与基准参考相比相等或更优。然而，值得注意的是，AlexNet 和 VGG 架构是故意进行了过度参数化，以最大化性能，它们都包含了参数密集但相对不敏感的全连接层，这些层可以被大量压缩。虽然该方法主要关注增强模型的存储占用，但作者指出，较小的存储也意味着加速操作，因为存储和提取的权重更少，因此减少了内存带宽需求，特别是当模型大小足够小以至于能够存储在片上静态随机存取存储器（SRAM）中，而不是在动态随机存取存储器（DRAM）和
    SRAM 之间来回交换时。此外，作者还介绍了高效推理引擎（EIE）的概念，这是一种旨在利用剪枝所带来的稀疏性的硬件加速器，相关内容将在他们的 [未来出版物](https://arxiv.org/abs/1602.01528)
    中讨论。
- en: '![](../Images/de7c16ed30076f776fbdf1db36a00f82.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de7c16ed30076f776fbdf1db36a00f82.png)'
- en: Results from “[Deep Compression](https://arxiv.org/abs/1510.00149)” show impressive
    compression rates with no loss in accuracy.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 “[Deep Compression](https://arxiv.org/abs/1510.00149)” 的结果显示，在不损失精度的情况下，取得了令人印象深刻的压缩率。
- en: In early 2017, the Incremental Network Quantization ([INQ](https://arxiv.org/abs/1702.03044))
    approach from Zhou et al. surpassed the levels of compression seen in Deep Compression.
    The authors also use a combination of pruning and quantization, but no Huffman
    coding, to achieve 53x compression on AlexNet with no loss in top-1 accuracy,
    and a whopping 89x compression with only minimal loss (<1.5%). Their strategy
    incrementally quantizes portions of the network weights, retrains the remaining
    full-precision weights to compensate of the induced error, and iterates until
    all weights are quantized. The weights are constrained be either zero or powers
    of 2 (think negative powers). As the authors explain, the advantage of this strategy
    “is that the original floating-point multiplication operations can be replaced
    by cheaper binary bit shift operations on dedicated hardware like FPGA.” To do
    this, they employ a variable-length encoding, which uses one bit to indicate a
    zero value, and the remaining bits together indicate the codeword that indexes
    the possible quantized values for a given bit width and scaling factor, the latter
    of which they set to be the maximum absolute layer-wise weight magnitude. Their
    approach slightly exceeds baseline FP32 AlexNet performance with 5-bit precision,
    and shows comparable accuracy down to 3-bits, as seen in the chart below. Code
    for INQ is [available on GitHub](https://github.com/AojunZhou/Incremental-Network-Quantization).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年初，Zhou 等人提出的增量网络量化（[INQ](https://arxiv.org/abs/1702.03044)）方法超越了深度压缩（Deep
    Compression）所见的压缩水平。作者同样采用了剪枝和量化的组合，但不使用哈夫曼编码，成功地在 AlexNet 上实现了 53 倍的压缩，同时没有损失
    top-1 准确率，在仅有轻微损失（<1.5%）的情况下，成功达到了 89 倍的压缩。其策略是逐步量化网络权重的一部分，重新训练其余的全精度权重以补偿引入的误差，并反复迭代直到所有权重都被量化。权重被约束为零或
    2 的幂（可以是负幂）。正如作者所解释的那样，这种策略的优势在于“原始的浮点乘法操作可以被更便宜的二进制位移操作替代，运行在如 FPGA 这样的专用硬件上。”为了实现这一点，他们采用了可变长度编码方法，使用一个比特表示零值，剩余的比特共同表示一个代码字，用于索引给定比特宽度和缩放因子的可能量化值，后者被设定为每层权重的最大绝对值。他们的方法在
    5 位精度下略微超越了基准的 FP32 AlexNet 性能，并且在 3 位精度下显示了相当可比的准确率，具体见下图。INQ 的代码[可在 GitHub 上获取](https://github.com/AojunZhou/Incremental-Network-Quantization)。
- en: '![](../Images/ac1277c6ef2dfc28015553f8875039f3.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac1277c6ef2dfc28015553f8875039f3.png)'
- en: In late 2017, [Jacob et al.](https://arxiv.org/abs/1712.05877) from Google focused
    on enabling efficient integer-only inference at the edge on mobile device CPUs.
    The authors state that using intentionally overparameterized models like AlexNet
    and VGG to benchmark model compression techniques creates an easy target, so they
    opt instead to test their approach using MobileNets. Since these compact models
    are already designed to maximize parameter efficiency, there is less “dead weight”
    to be easily compressed, and their parameters are more sensitive to perturbation.
    Considered the formative work in QAT, Jacob et al.’s approach quantizes both weights
    and activations to 8 bit integers, and the biases (which require more precision)
    to 32 bit integers. Their method uses floating-point arithmetic during training,
    serving as an early example of using simulated quantization for QAT. The authors
    avoid a quantization scheme requiring look-up tables because these tend to be
    less performant than pure arithmetic on SIMD hardware, and opt instead for an
    affine transformation of the weights to the integer space. To complement their
    QAT method, the authors co-design a framework for converting and running the resulting
    trained model on integer-only hardware, and go a step further than many previous
    works by proving their efficiency gains on actual edge hardware.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年底，[Jacob 等人](https://arxiv.org/abs/1712.05877)来自 Google 关注于在移动设备 CPU 上实现高效的整数-only
    推理。作者指出，使用像 AlexNet 和 VGG 这样故意过度参数化的模型来基准化模型压缩技术，容易成为攻击目标，因此他们选择使用 MobileNets
    来测试他们的方法。由于这些紧凑型模型已经设计为最大化参数效率，存在较少的“死重”可被轻易压缩，而且它们的参数对扰动更为敏感。被认为是量化感知训练（QAT）领域的开创性工作，Jacob
    等人的方法将权重和激活量化为 8 位整数，将偏差（需要更高精度）量化为 32 位整数。他们的方法在训练过程中使用浮点运算，作为使用模拟量化进行 QAT 的早期示例。作者避免使用需要查找表的量化方案，因为这些方案通常比在
    SIMD 硬件上进行纯算术运算的性能差，而选择对权重进行仿射变换到整数空间。为了补充他们的 QAT 方法，作者共同设计了一个框架，用于转换并在仅支持整数的硬件上运行经过训练的模型，并且比许多先前的工作更进一步，证明了他们在实际边缘硬件上的效率提升。
- en: '![](../Images/d8b8e559b7fffb812c612a161ccb886b.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d8b8e559b7fffb812c612a161ccb886b.png)'
- en: Results from Jacob et al. 2017 compare latency-vs-accuracy tradeoff of the reference
    floating-point and their 8-bit quantized MobileNets one two types of mobile CPUs.
    Notice that for the Sanpdragon 821, which is more optimized for floating-point
    arithmetic, the advantage of 8-bit quantization is less noticeable.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Jacob 等人 2017 年的结果比较了参考浮动点与他们的 8 位量化 MobileNets 在两种类型的移动 CPU 上的延迟与准确度权衡。请注意，对于更优化浮动点运算的
    Snapdragon 821，8 位量化的优势并不那么显著。
- en: So far, the techniques we’ve seen quantize the layers of the model to a uniform
    level of precision, which is the optimal condition for hardware acceleration,
    particularly at the edge on low-power hardware. However, as we learned in our
    previous exploration into pruning, some network layers are less sensitive to alteration
    than others, and can therefore be compressed more aggressively without affecting
    performance. Thus, *mixed-precision quantization* approaches use varying levels
    of compression on network components (typically at layer-wise granularity) based
    on their sensitivity to achieve even smaller memory footprints and reduce the
    data transfer and power costs of operating models at the edge. In the next section,
    we will see the familiar themes of sensitivity analysis inform the variable assignment
    of numeric precision across neural network components to maximize model compression
    through quantization.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们看到的技术将模型的各层量化到统一的精度水平，这是硬件加速的最佳条件，尤其是在低功耗硬件的边缘设备上。然而，正如我们在之前对剪枝的探讨中了解到的那样，某些网络层比其他层对改变的敏感度较低，因此可以在不影响性能的情况下更加激进地压缩。因此，*混合精度量化*方法根据网络组件（通常按层级粒度）对变化的敏感度使用不同级别的压缩，从而实现更小的内存占用，减少在边缘设备上运行模型时的数据传输和功耗成本。在下一节中，我们将看到敏感度分析的熟悉主题如何影响在神经网络组件间分配数值精度，从而通过量化最大化模型压缩。
- en: The Rise of Mixed-Precision Quantization
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混合精度量化的崛起
- en: '![](../Images/20bffca32ba68b021ed373907c712da9.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20bffca32ba68b021ed373907c712da9.png)'
- en: Image by author using DALL-E 3.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用 DALL-E 3 生成。
- en: To account for the varying levels of sensitivity between network layers to the
    perturbations caused by quantization, *mixed-precision quantization*, which involves
    tailoring the level of precision for individual network components based on their
    sensitivity, has become a popular approach for maximizing the levels of compression
    possible through quantization in deep networks. The challenge arises from the
    vast search space of mixed-precision settings, which is exponential with the number
    network layers. Moreover, for methods aiming to find the optimal sequence for
    layer-wise quantization and fine-tuning, the complexity becomes combinatorial.
    Thus, researchers have proposed various effective algorithms to automate the search
    and selection of optimal mixed-precision settings for deep networks, circumventing
    the infeasibility of brute-force exploration. As we will see, researchers have
    employed neural architecture search (NAS), reinforcement learning (RL), second-order
    Hessian information, as well as other types of solvers to address this problem,
    yielding impressive results, and establishing new levels of efficiency gains obtainable
    via quantization.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了考虑网络层之间对量化引起的扰动的敏感度差异，*混合精度量化*方法应运而生，这种方法根据网络组件的敏感度量身定制精度水平，已成为通过量化在深度网络中最大化压缩程度的流行方法。挑战来自混合精度设置的广阔搜索空间，该空间随着网络层数的增加呈指数级增长。此外，对于旨在寻找层级量化和微调的最佳顺序的方法，复杂度变得是组合性的。因此，研究人员提出了各种有效的算法来自动化搜索和选择深度网络的最优混合精度设置，避免了暴力搜索的不可行性。正如我们将看到的，研究人员已经采用了神经架构搜索（NAS）、强化学习（RL）、二阶海森矩阵信息以及其他类型的求解器来解决这一问题，取得了令人印象深刻的成果，并建立了通过量化获得效率提升的新水平。
- en: In late 2018, Wu et al. published the “Mixed Precision Quantization of ConvNets
    via Differentiable Neural Architecture Search” ([DNAS](https://arxiv.org/abs/1812.00090))
    method for solving the mixed-precision problem. Their approach creates a set of
    network architecture parameters *θ,* which define subnets within a fully differentiable
    super net which contains all possible precision settings for every layer. The
    *θ* parameterscontrol the probability of sampling each edge in this graph of subnets,
    and can be directly optimized through gradient descent. Using this technique,
    the authors create models bearing extreme compression in the weights (all the
    way down to 1 bit in some layers) which are able to beat the full-precision baselines.
    The DNAS method produces ResNet models which perform on par with their full-precision
    counterparts with up to 21.1x smaller footprints and 103.9x lower computational
    costs. The authors mention that their method is a general architecture search
    framework which is extensible to other network parameterizations, but leave this
    for future work.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年底，吴等人发布了“通过可微神经架构搜索进行卷积网络的混合精度量化”([DNAS](https://arxiv.org/abs/1812.00090))方法，用于解决混合精度问题。他们的方法创建了一组网络架构参数*θ*，该参数定义了一个完全可微的超级网络中的子网，该超级网络包含每一层的所有可能精度设置。*θ*参数控制在该子网图中采样每条边的概率，并可以通过梯度下降进行直接优化。使用这一技术，作者创建了具有极高压缩率的模型（在某些层中权重压缩至1位），并能够超越全精度基准。DNAS方法生成的ResNet模型在精度上与全精度模型相当，但其内存占用小至21.1倍，计算成本低至103.9倍。作者提到，他们的方法是一个通用的架构搜索框架，可以扩展到其他网络参数化，但这部分内容将留待未来研究。
- en: '![](../Images/56043278c12d98051565d81eb6b3c995.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56043278c12d98051565d81eb6b3c995.png)'
- en: '[DNAS](https://arxiv.org/abs/1812.00090) results show computational cost compression
    rates for top three searched architectures by accuracy. Note that their method
    gets “free lunch” computational compression of 33–40x in the arch-1 column.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[DNAS](https://arxiv.org/abs/1812.00090)的结果显示了按准确率排序的前三个搜索架构的计算成本压缩率。请注意，他们的方法在arch-1列中实现了33–40倍的“免费午餐”计算压缩。'
- en: 'Wang et al.’s 2018 “[HAQ: Hardware-Aware Automated Quantization with Mixed
    Precision](https://arxiv.org/abs/1811.08886)” eschewed the use of proxy metrics
    for computational efficiency (e.g. FLOPs) and opted instead to use signals from
    a hardware simulators to feed a reinforcement learning (RL) algorithm to automate
    the search for optimal mixed-precision settings. While their approach pushes the
    limits of intelligent model compression, it suffers from weaknesses related to
    the complexity, computational expense, and lack of generalization that comes with
    training an RL policy to predict the correct mixed-precision setting for a given
    network architecture and hardware device.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '王等人于2018年发表的“[HAQ: 硬件感知自动量化与混合精度](https://arxiv.org/abs/1811.08886)”放弃了使用代理度量（如FLOPs）来提高计算效率，而是选择使用硬件模拟器提供的信号，输入强化学习（RL）算法，自动搜索最佳混合精度设置。尽管他们的方法推动了智能模型压缩的极限，但在训练RL策略来预测特定网络架构和硬件设备的正确混合精度设置时，仍然面临与复杂性、计算开销和泛化能力不足相关的弱点。'
- en: '![](../Images/2fa11df3a2f1b8f9d918d8769dcb1cd8.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2fa11df3a2f1b8f9d918d8769dcb1cd8.png)'
- en: HAQ results table compares their approach to Deep Compression. Note that both
    approaches nearly match the full-precision baseline models in ~4bit settings.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: HAQ结果表格将他们的方法与深度压缩进行比较。请注意，在约4位设置下，两种方法几乎匹配全精度基准模型。
- en: 'In 2019, the Hessian AWare Quantization ([HAWQ](https://arxiv.org/abs/1905.03696))
    paper addressed two significant challenges in mixed-precision quantization: the
    exponential search space for determining the optimal level of precision for each
    network layer, and the factorial complexity of determining the optimal sequence
    of QAT across those layers, which make brute force search of these values untenable
    in deep networks. The authors demonstrate that the top eigenvalues from the second-order
    Hessian information can provide a sensitivity analysis to inform both the correct
    levels of quantization and the optimal order of fine-tuning across the network
    layers. This is reminiscent of pruning approaches that are based on LeCun et al.’s
    1989 [Optimal Brain Damage](https://proceedings.neurips.cc/paper/1989/hash/6c9882bbac1c7093bd25041881277658-Abstract.html),
    where Hessian information is used as a measure of saliency (i.e. sensitivity)
    of network components, with larger values indicating that the pruning or quantization
    of a given network component will have a greater effect on performance. HAWQ was
    able to exceed the performance of DNAS with 2bit weights and 4bit activations,
    reaching a higher overall compression as a result.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在2019年，Hessian AWare Quantization（[HAWQ](https://arxiv.org/abs/1905.03696)）论文解决了混合精度量化中的两个重要挑战：一是确定每个网络层的最佳精度级别所需的指数级搜索空间，二是确定在这些层之间进行QAT（量化感知训练）的最佳顺序的阶乘复杂度，这使得在深度网络中暴力搜索这些值变得不可行。作者展示了来自二阶Hessian信息的主特征值可以提供敏感度分析，从而为正确的量化级别和跨网络层的优化微调顺序提供指导。这类似于基于LeCun等人1989年提出的[Optimal
    Brain Damage](https://proceedings.neurips.cc/paper/1989/hash/6c9882bbac1c7093bd25041881277658-Abstract.html)的剪枝方法，在这种方法中，Hessian信息被用作衡量网络组件显著性（即敏感度）的标准，较大的值表示对给定网络组件的剪枝或量化对性能的影响更大。HAWQ能够在使用2位权重和4位激活的情况下超越DNAS的性能，从而实现了更高的整体压缩效果。
- en: 'Seven months after HAWQ in 2019, Dong et al. followed up with [HAWQ-V2](https://arxiv.org/abs/1911.03852)
    to address three major limitations they identified in their previous work: 1)
    only using the top eigenvalues for sensitivity analysis, ignoring the rest of
    the Hessian spectrum, 2) only determining relative layer-wise sensitivity measurements,
    necessitating manual precision assignments, and 3) not considering mixed-precision
    quantization for activations. Addressing the first issue, they find that taking
    the average over all Hessian eigenvalues is a superior measure of layer-wise sensitivity.
    For the second, the authors propose to use a Pareto frontier-based method for
    automatically selecting exact layer-wise bit precisions. To address the third
    weakness, the authors “extend the Hessian analysis to mixed-precision activation
    quantization.” With these adjustments, HAWQ-V2 set a new state-of-the-art benchmark
    in CNN quantization.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在2019年HAWQ发布七个月后，Dong等人发布了[HAWQ-V2](https://arxiv.org/abs/1911.03852)，以解决他们在前期工作中发现的三个主要限制：1）仅使用主特征值进行敏感度分析，忽略了Hessian谱中的其余部分，2）仅确定相对的层级敏感度度量，导致需要手动分配精度，3）未考虑激活的混合精度量化。针对第一个问题，他们发现，对所有Hessian特征值取平均是更好的层级敏感度度量方法。针对第二个问题，作者提出使用基于Pareto前沿的方法来自动选择精确的层级位精度。为了解决第三个问题，作者“将Hessian分析扩展到混合精度激活量化”。通过这些调整，HAWQ-V2在CNN量化中设立了新的最先进的基准。
- en: '![](../Images/1bef5d60effcafaa9567fb8f4f6ff62d.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1bef5d60effcafaa9567fb8f4f6ff62d.png)'
- en: Table of HAWQ-V2 results with ResNet50 on ImageNet.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ResNet50在ImageNet上进行HAWQ-V2结果的表格。
- en: A year later in late 2020, the HAWQ authors published a third iteration, [HAWQ-V3](https://arxiv.org/abs/2011.10680).
    This work improved on the previous approaches by ensuring integer-only arithmetic
    in every network operation, including the batch norm layers and residual connections,
    which were previously kept in float32 to maintain accuracy, thereby preventing
    deployment in the types of integer-only hardware common in edge devices. To determine
    layer-wise bit precision, HAWQ-V3 uses a “novel hardware-aware mixed-precision
    quantization formulation that uses an Integer Linear Programming (ILP) problem,”
    which “balances the trade-off between model perturbation and constraints, e.g.,
    memory footprint and latency.” Their approach produces a model that performs integer-only
    inference in 8bits which exceeds the performance of the strongest available full-precision
    baseline, demonstrating “free lunch” compression, and maintains high accuracy
    at precisions down to 4bit.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 一年后的2020年底，HAWQ的作者发布了第三个版本，[HAWQ-V3](https://arxiv.org/abs/2011.10680)。这项工作通过确保每个网络操作中的整数运算，包括批量归一化层和残差连接，改进了之前的方法，后者为了保持准确性，之前一直使用float32，从而导致无法在常见的仅支持整数的边缘设备硬件上部署。为了确定每一层的比特精度，HAWQ-V3采用了“新颖的硬件感知混合精度量化方案，该方案使用整数线性规划（ILP）问题”，它“平衡了模型扰动与约束（例如内存占用和延迟）之间的权衡”。他们的方法生成了一个在8位整数推理中表现优异的模型，超越了最强的全精度基准，展示了“免费午餐”压缩，并保持在低至4位精度时的高准确性。
- en: '![](../Images/1b24f0795b1aaa76351b911e51bef5b5.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b24f0795b1aaa76351b911e51bef5b5.png)'
- en: Results from [HAWQ-V3](https://arxiv.org/abs/2011.10680) paper using ResNet50
    on ImageNet. Notice that the all-8bit quantized HAWQ-V3 beats the full precision
    baseline. “Int” means integer-only, “Uni” for uniform, “BL” for baseline (these
    vary between publications, they choose the strongest available for their study).
    “Dist” refers to use of Knowledge Distillation.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[HAWQ-V3](https://arxiv.org/abs/2011.10680)论文的结果，使用ResNet50在ImageNet上的测试。注意，所有8位量化的HAWQ-V3超越了全精度基准。“Int”表示仅整数，“Uni”表示均匀，“BL”表示基准（这些在不同的文献中有所不同，他们选择最强的基准进行研究）。“Dist”指的是知识蒸馏的使用。
- en: The techniques covered in this section show the power of adjusting the layer-wise
    precision assignments based on sensitivity analysis, quantizing insensitive layers
    more aggressively, and allowing higher precision to preserve representation power
    in the sensitive layers. However, it is important to note that mixed-precision
    quantization schemes can be more challenging to implement on edge devices, where
    they may not operate as efficiently. Thus, it is reassuring to see strong fixed
    precision benchmarks in the HAWQ-V3 results above. In the next section, we will
    discuss techniques which seek to perform quantization on trained, full-precision
    CNNs without the need to retrain them, a highly relevant precursor to our upcoming
    discussion on the quantization of LLMs, where retraining and access to training
    data are often impossible.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中介绍的技术展示了基于敏感性分析调整逐层精度分配的强大功能，对不敏感的层进行更激进的量化，并允许在敏感层中保留更高的精度以保持表示能力。然而，值得注意的是，混合精度量化方案在边缘设备上实现可能更具挑战性，因为它们可能无法高效运行。因此，看到HAWQ-V3在上述结果中提供了强有力的固定精度基准是令人放心的。在下一节中，我们将讨论一些技术，旨在对经过训练的全精度CNN进行量化，而无需重新训练它们，这为我们即将讨论的LLM量化提供了一个高度相关的前置条件，因为在LLM的情况下，重新训练和访问训练数据通常是不可能的。
- en: Post-Training Quantization of CNNs
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNN的后训练量化
- en: '![](../Images/c4264da9bd33c2feb203b1f0845ea0b2.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4264da9bd33c2feb203b1f0845ea0b2.png)'
- en: Image by author using DALL-E 3.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用DALL-E 3生成。
- en: The astute reader will notice that the majority of works in this section on
    quantization during the CNN era belong to the QAT category, since the models studied
    during this period were easy enough to fine-tune in a quantized setting. However,
    even before the imminent explosion in neural network sizes, researchers were already
    keen on the practical advantages of PTQ, which promised to liberate those developing
    quantized models from the need to gain access to the original training data (which
    may be impossible in many cases), as well as save the time and resources required
    for retraining. Thus, a timely wave of interest in PTQ research caught steam around
    2019, laying a welcome groundwork for the focus on large language models yet to
    come.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 聪明的读者会注意到，本节中关于CNN时代量化的多数研究属于QAT类别，因为在这一时期研究的模型足够简单，可以在量化设置中进行微调。然而，早在神经网络规模即将爆炸之前，研究人员就已经对PTQ的实际优势表现出浓厚兴趣，因为PTQ承诺解放那些开发量化模型的人，不再需要访问原始训练数据（在许多情况下这可能是不可能的），并且节省了重新训练所需的时间和资源。因此，关于PTQ研究的兴趣浪潮大约在2019年时兴起，为后续聚焦大型语言模型的研究奠定了基础。
- en: '[Krishnamoorthi](https://arxiv.org/abs/1806.08342) led this charge in mid-2018
    with a seminal white paper on CNN quantization. Their approach uses channel-wise
    asymmetric uniform quantization of weights and layer-wise quantization of activations
    to a fixed precision of 8 bits while maintaining accuracy within 2% of baseline.
    The author observes that quantizing only the weights of a network to 8-bit can
    be an easy way of compressing storage size, but in order to enable efficient inference,
    activations must also be quantized, which requires using calibration data to calculate
    the dynamic ranges of activations throughout the network layers to discover the
    appropriate layer-wise quantization parameters. In the chart below, the author
    provides a comparison of the effects of per-layer and per-channel weight quantization
    schemes on various CNNs. Notice that the larger, overparameterized CNNs towards
    the right are much more receptive to the lower granularity of per-layer quantization
    parameters than the efficiency-minded MobileNets (left).'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[Krishnamoorthi](https://arxiv.org/abs/1806.08342)在2018年中期发布了一篇开创性的白皮书，领导了CNN量化的研究。他们的方法采用了权重的通道级非对称均匀量化，并对激活进行逐层量化，固定精度为8位，同时保持准确度在基准值的2%以内。作者观察到，仅对网络的权重进行8位量化可以作为一种压缩存储空间的简便方法，但为了实现高效推理，激活也必须进行量化，这需要使用校准数据来计算网络各层激活的动态范围，从而发现合适的逐层量化参数。在下面的图表中，作者提供了逐层和逐通道权重量化方案对各种CNN影响的比较。请注意，右侧较大的、过度参数化的CNN对逐层量化参数的低粒度更为敏感，而效率导向的MobileNet（左侧）则不那么敏感。'
- en: '![](../Images/161dfb9aa79084e5317d823602e609a1.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/161dfb9aa79084e5317d823602e609a1.png)'
- en: Results from [Krishnamoorthi 2018](https://arxiv.org/abs/1806.08342) show the
    effects of different W8A8 PTQ schemes across different CNN architectures. “Mv1”
    and “Mv2” indicate MobileNet v1 and v2, which show catastrophic accuracy loss
    when using per-layer weight quantization.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[Krishnamoorthi 2018](https://arxiv.org/abs/1806.08342)的结果显示了不同W8A8 PTQ方案在不同CNN架构中的效果。“Mv1”和“Mv2”分别表示MobileNet
    v1和v2，在使用每层权重量化时，准确度出现了灾难性的下降。
- en: In October 2018, Banner et al.’s “[Post training 4-bit quantization of convolutional
    networks for rapid-deployment](https://arxiv.org/abs/1810.05723)” sought to expand
    the usability of PTQ to below 8bits of precision. Their approach efficiently achieves
    4-bit data-free mixed-precision PTQ with tolerable performance degradation. To
    do this, the authors exploit the knowledge that neural network distributions tend
    to be bell-shaped around a mean in order to tune their quantization scheme in
    a way which minimizes the mean-squared quantization error at the tensor level,
    thereby avoiding the need for retraining. To allow for better knowledge transfer
    into the quantized space, the authors 1) use their proposed analytical clipping
    for integer quantization (ACIQ) technique to clamp activation tensor outliers
    according to an optimal saturation point which reduces rounding error in the more
    densely populated region of the spectrum, 2) determine optimal per-channel bit
    allocations analytically, finding that the optimal quantization step size for
    a given channel “is proportional to the 2/3-power of its range,” and 3) propose
    a simple *bias-correction* method to compensate for the biases introduced into
    the weights after quantization by incorporating the expected changes in their
    channel-wise mean and variance into the quantization parameters.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在2018年10月，Banner等人提出的“[卷积网络的后训练4位量化以实现快速部署](https://arxiv.org/abs/1810.05723)”旨在将PTQ的可用性扩展到低于8位的精度。他们的方法高效地实现了4位无数据的混合精度PTQ，并且具有可接受的性能下降。为了实现这一点，作者利用了神经网络分布通常在均值周围呈钟形分布的知识，从而调节他们的量化方案，最小化张量级别的均方量化误差，避免了重新训练的需求。为了更好地将知识转移到量化空间，作者采取了以下措施：1）使用他们提出的整数量化的解析剪切技术（ACIQ）根据最佳饱和点来限制激活张量的异常值，从而减少在谱的密集区域中舍入误差；2）通过解析方法确定每个通道的最佳位宽分配，发现给定通道的最佳量化步长“与其范围的2/3次方成正比”；3）提出了一种简单的*偏置修正*方法，通过将量化参数中考虑到的通道均值和方差的预期变化来补偿量化后引入的权重偏差。
- en: The ACIQ approach requires statistical analysis of network activations on a
    small calibration set, so while it does not require access to the training data,
    it is essential to ensure that the calibration set is representative of the distributions
    that will be encountered during runtime, otherwise there is risk of overfitting
    quantization parameters to the wrong distribution. Also, note that the use of
    channel-wise bit width creates a lot of concerns for practical application, as
    both hardware and software must be catered to support mixed-precision at the channel
    level, or otherwise running the quantized network may be inefficient or impossible.
    Nonetheless, the formulation of a closed-form analytical solution to directly
    calculate the optimal bit-widths for network components marks an important milestone
    in quantization research. Further, their closed-form PTQ solution for bias correction
    parameters, as well as the efficient absorption of these parameters into existing
    calculations, marks another significant contribution. Code for Banner et al.’s
    approach is [available on GitHub](https://github.com/submission2019/cnn-quantization).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ACIQ方法需要对小型校准集上的网络激活进行统计分析，因此，尽管不需要访问训练数据，但必须确保校准集能够代表在运行时会遇到的分布，否则存在将量化参数过拟合到错误分布的风险。此外，需要注意的是，使用按通道的位宽会给实际应用带来许多问题，因为硬件和软件都必须支持在通道级别进行混合精度运算，否则运行量化后的网络可能效率低下甚至无法执行。尽管如此，提出一种封闭形式的解析解来直接计算网络组件的最佳位宽，标志着量化研究中的一个重要里程碑。此外，他们针对偏置修正参数的封闭形式PTQ解法，以及将这些参数高效吸收到现有计算中的方法，也是另一个重要贡献。Banner等人方法的代码可以在[GitHub上找到](https://github.com/submission2019/cnn-quantization)。
- en: '![](../Images/5b549fcb99ff767c89f1af3a0ba29aab.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b549fcb99ff767c89f1af3a0ba29aab.png)'
- en: Chart from [Banner et al. 2019](https://arxiv.org/abs/1810.05723) shows the
    relative performance of their 4-bit PTQ method.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[Banner等人 2019](https://arxiv.org/abs/1810.05723)的图表展示了他们4位PTQ方法的相对性能。
- en: Nagel et al.’s 2019 “Data-Free Quantization Through Weight Equalization and
    Bias Correction” ([DFQ](https://arxiv.org/abs/1906.04721)) introduced a groundbreaking
    approach to data-free PTQ, enabling deep networks to be efficiently quantized
    down to 8 bits without the need for calibration data, fine-tuning, or hyperparameter
    tuning. The authors adapt the weights through scaling to make them “more amenable
    to quantization,” propose a method for correcting the biases introduced by quantization,
    and mention that their approach could be employed as a complementary pre-processing
    step to QAT. Unlike the Krishnamoorthi and Banner et al. approaches above, which
    require storing quantization parameters for each channel, DFQ requires storing
    only a single scale and offset value for each layer’s weight tensor by determining
    the values that maximize the per-channel precision across that layer. DFQ exploits
    the scale equivariance of the ReLU activation function and keeps the overall math
    equivalent by absorbing the scaling and induced bias into the next layer. The
    authors demonstrate that the need for calibration data to quantize activations
    can be avoided by using the batch normalization statistics preserved in the model
    from its training to estimate the layer-wise expected quantization error, and
    compensate for the bias introduced into layer activations through quantization
    by subtracting this expected error from the layer bias parameters. Although the
    authors do not explicitly use the term, DFQ can be seen as a formative work in
    *zero-shot quantization* (ZSQ), as it is a PTQ approach which requires no calibration
    data.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Nagel等人于2019年提出的“通过权重均衡和偏置修正进行无数据量化”（[DFQ](https://arxiv.org/abs/1906.04721)）介绍了一种突破性的无数据PTQ方法，使深度网络能够在无需校准数据、微调或超参数调优的情况下，高效地量化到8位。作者通过缩放适应权重，使其“更适合量化”，提出了一种修正量化引入的偏置的方法，并指出他们的方法可以作为QAT的补充预处理步骤使用。与上述Krishnamoorthi和Banner等人的方法不同，后者需要为每个通道存储量化参数，DFQ只需为每一层的权重张量存储一个单独的缩放值和偏移量值，通过确定最大化该层每通道精度的值来实现。DFQ利用ReLU激活函数的尺度等变性，并通过将缩放和引入的偏置吸收到下一层，保持整体数学等效性。作者展示了，通过使用模型训练时保存的批量归一化统计数据来估计层级预期量化误差，可以避免量化激活时需要校准数据，并通过从层偏置参数中减去这一预期误差来补偿量化引入的偏置。虽然作者没有明确使用这个术语，但DFQ可以视为*零-shot量化*（ZSQ）中的一项奠基性工作，因为它是一种无需校准数据的PTQ方法。
- en: '![](../Images/93b173d62760e28e6387ecc037918a70.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93b173d62760e28e6387ecc037918a70.png)'
- en: Chart from [DFQ](https://arxiv.org/abs/1906.04721) shows how quickly PTQ performance
    drops without special techniques being applied, with catastrophic loss below 12bits.
    Even the DFQ approach does not hold up below 8 bits.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[DFQ](https://arxiv.org/abs/1906.04721)的图表显示了在没有应用特殊技术的情况下，PTQ性能如何迅速下降，在低于12位时出现灾难性的损失。即便是DFQ方法，在低于8位时也无法维持效果。
- en: Choukroun et al.’s 2019 [OMSE](https://arxiv.org/abs/1902.06822) method finds
    the kernel-wise quantization parameters which minimize the mean squared error
    (MSE) between the quantized and original weight/activation tensors, marking the
    first PTQ approach to achieve 4-bit quantization with minimal loss in accuracy
    (3% degradation on top-1 ImageNet classification). The authors opt to use symmetrical
    uniform quantization for highest efficiency, saving the additional calculations
    introduced by using offsets. Since the relationship between quantization-induced
    MSE and scaling factor for a given kernel is non-convex, the authors use a line
    search method to discover the optimum values. To circumvent the need for using
    mixed-precision to preserve representation power in the sensitive network layers,
    the authors propose to represent these “key” layers using multiple low-precision
    tensors, but they warn that the complexity of this approach necessitates using
    it only for small tensors, which in the case of CNNs works out fine, since the
    most sensitive components are the convolutional kernels, but it would not scale
    well for architectures with large salient components.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Choukroun等人2019年提出的[OMSE](https://arxiv.org/abs/1902.06822)方法通过找到每个卷积核的量化参数，最小化量化后的权重/激活张量与原始张量之间的均方误差（MSE），标志着首个实现4位量化并保持较小精度损失（在ImageNet顶级1分类上为3%下降）的PTQ方法。作者选择使用对称均匀量化以提高效率，避免了使用偏移量所带来的额外计算。由于给定卷积核的量化引起的MSE与缩放因子之间的关系是非凸的，作者使用线搜索方法来发现最优值。为了避免在敏感网络层中使用混合精度来保持表示能力，作者建议用多个低精度张量表示这些“关键”层，但他们警告说，这种方法的复杂性要求仅用于小型张量，在卷积神经网络（CNN）中效果很好，因为最敏感的部分是卷积核，但对于具有大规模显著组件的架构，该方法不适用。
- en: 'In early 2020, the authors of the HAWQ papers published [ZeroQ](https://arxiv.org/abs/2001.00281):
    a data-free PTQ approach which beat the previous state-of-the-art ZSQ benchmark
    set by DFQ. Their approach achieves mixed-precision quantization through a novel
    Pareto frontier based method which automatically determines the optimal mixed-precision
    setting with no manual searching. Rather than requiring access to training or
    calibration data, ZeroQ generates a synthetic dataset tailored to match the statistics
    in the batch normalization layers, called “distilled data,” and then uses the
    activations generated by this data to calibrate quantization parameters and to
    perform layer-wise sensitivity analysis. These sensitivity values feed the Pareto
    frontier selection process, which finds the optimal setting for a given model
    size or desired level of accuracy. The authors call out the fact that most work
    in PTQ typically only benchmarks image classification accuracy without considering
    more complex tasks, so they also prove their method preserves performance on the
    more challenging object detection task. ZeroQ is [open-sourced](https://github.com/amirgholami/ZeroQ)
    and extremely compute efficient, offering a low barrier for entry on network quantization.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在2020年初，HAWQ论文的作者发布了[ZeroQ](https://arxiv.org/abs/2001.00281)：一种无数据的PTQ方法，超越了DFQ设定的先前最先进的ZSQ基准。该方法通过一种新颖的帕累托前沿方法实现了混合精度量化，自动确定最佳的混合精度设置，无需手动搜索。ZeroQ方法不需要访问训练或校准数据，而是生成一个合成数据集，旨在匹配批量归一化层中的统计信息，称为“蒸馏数据”，然后使用该数据生成的激活值来校准量化参数并执行逐层敏感性分析。这些敏感性值会反馈给帕累托前沿选择过程，从而找到适合给定模型大小或所需精度级别的最佳设置。作者指出，大多数PTQ工作通常仅对图像分类准确性进行基准测试，而不考虑更复杂的任务，因此他们还证明了该方法在更具挑战性的目标检测任务中保持了性能。ZeroQ是[开源](https://github.com/amirgholami/ZeroQ)的，且计算效率极高，为网络量化提供了较低的入门门槛。
- en: '![](../Images/29acd6b57f938f8140976dcc50d926cc.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29acd6b57f938f8140976dcc50d926cc.png)'
- en: Results from [ZeroQ paper](https://github.com/amirgholami/ZeroQ) show the superiority
    of their method over previous state-of-the-art PTQ methods. “No D” means “No Data”
    (data-free aka zero-shot), and “No FT” indicates no fine-tuning required (PTQ).
    Note that 8bit ZeroQ offers very close to full-precision baseline performance
    without any data or retraining.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[ZeroQ论文](https://github.com/amirgholami/ZeroQ)的结果表明，他们的方法优于以前最先进的PTQ方法。“No
    D”表示“无数据”（无数据或零-shot），而“No FT”表示不需要微调（PTQ）。请注意，8位ZeroQ在没有任何数据或重新训练的情况下，提供了非常接近全精度基准的性能。
- en: Later in 2020, the authors of [AdaRound](https://arxiv.org/abs/2004.10568) pointed
    out the fact that the round-to-nearest approach had dominated previous work in
    quantization, despite being a suboptimal rounding scheme. Instead, they proposed
    a framework to analyze the effect of rounding which considers characteristics
    of both input data and task loss, and formulate rounding as a per-layer Quadratic
    Unconstrained Binary Optimization (QUBO) problem. They approximate change in task
    loss with respect to weight perturbations using a second-order Taylor series expansion,
    in the familiar way used by other works using Hessian information to measure sensitivity,
    starting with [Optimal Brain Damage](https://proceedings.neurips.cc/paper/1989/hash/6c9882bbac1c7093bd25041881277658-Abstract.html)
    (OBD) in 1989\. Like [Optimal Brain Surgeon](https://www.researchgate.net/publication/3568764_Optimal_Brain_Surgeon_and_general_network_pruning)
    (OBS), they extend their method to benefit from deeper theoretical analysis of
    the off-diagonal Hessian values. They equate the naïve round-to-nearest approach
    to considering only the diagonal of the Hessian, wherein perturbations are assumed
    to have no codependence in their contribution to the task loss, and therefore
    only the reduction of their individual magnitudes matters (i.e. round-to-nearest).
    However, the effects of weight perturbations are interrelated, and the off-diagonal
    information is therefore important, as it can signal when combinations of perturbations
    are actually beneficial to the loss. AdaRound requires only a small amount of
    unlabeled data, and set a new state-of-the-art in PTQ for CNNs, compressing ResNet
    models to 4bit while staying within 1% of their baseline performance.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 2020年晚些时候，[AdaRound](https://arxiv.org/abs/2004.10568)的作者指出，尽管四舍五入到最近的方式在量化的前期工作中占主导地位，但它仍然是一种次优的四舍五入方案。相反，他们提出了一个框架来分析四舍五入的影响，这个框架考虑了输入数据和任务损失的特性，并将四舍五入公式化为每层的二次无约束二进制优化（QUBO）问题。他们使用二阶泰勒级数展开，近似任务损失对权重扰动的变化，采用其他工作中常见的方式，利用海森矩阵信息来度量灵敏度，最早可以追溯到1989年的[Optimal
    Brain Damage](https://proceedings.neurips.cc/paper/1989/hash/6c9882bbac1c7093bd25041881277658-Abstract.html)（OBD）。类似于[Optimal
    Brain Surgeon](https://www.researchgate.net/publication/3568764_Optimal_Brain_Surgeon_and_general_network_pruning)（OBS），他们扩展了他们的方法，从而能从对海森矩阵非对角元素的更深入理论分析中受益。他们认为，天真的四舍五入到最近的方式相当于仅考虑海森矩阵的对角线，其中扰动被假设为对任务损失的贡献没有共变性，因此只有各自幅度的减少才重要（即四舍五入到最近）。然而，权重扰动的影响是相互关联的，因此非对角线信息是重要的，因为它能表明当扰动组合实际上对损失有利时。AdaRound仅需要少量的未标记数据，并在CNN的PTQ（后训练量化）中设定了新的最先进技术，能够将ResNet模型压缩到4位，同时保持在其基准性能的1%以内。
- en: To show their point quite vividly, the AdaRound authors generated a set of 100
    random perturbations for the first layer of ResNet using the stochastic rounding
    method from Gupta et al., 2015 (above), and compared these to a round-to-nearest
    perturbation. Of the 100 sampled layer perturbations, 48 had better performance
    than rounding to nearest, with some providing more than 10% accuracy improvements.
    This demonstrates there are many better solutions in the quantized space, and
    provides a clear signal that there should be ways of targeting them to increase
    the accuracy of PTQ.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生动地展示他们的观点，AdaRound的作者使用Gupta等人2015年提出的随机四舍五入方法，生成了100个随机扰动集，针对ResNet的第一层，并将这些扰动与四舍五入到最近的扰动进行了比较。在这100个抽样的层扰动中，48个表现优于四舍五入到最近的方式，其中一些甚至提供了超过10%的准确度提升。这表明在量化空间中存在许多更好的解决方案，并清楚地表明应该有方法能够针对它们，以提高PTQ的准确性。
- en: '![](../Images/4ee2b95ec4efe02b2d5eea55dff66b3c.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ee2b95ec4efe02b2d5eea55dff66b3c.png)'
- en: Chart from [AdaRound](https://arxiv.org/abs/2004.10568) shows the distribution
    of performance across the randomly sampled perturbations in comparison to a round-to-nearest
    scheme, showing there are many better solutions, and that the better solutions
    correlate strongly with the second-order Taylor series term.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[AdaRound](https://arxiv.org/abs/2004.10568)的图表显示了在与四舍五入到最近方案进行比较时，随机抽样的扰动在性能上的分布，表明有许多更好的解决方案，而且这些更好的解决方案与二阶泰勒级数项高度相关。
- en: In the chart above, we can see a clear correlation between the second-order
    Taylor series term and the drop in accuracy after rounding, indicating that this
    is a good proxy for optimizing task loss due to quantization. Even ignoring the
    cross-layer interactions among network weights, however, the Hessian is still
    prohibitively costly to compute for large network layers. The results below show
    that a W4A8 configuration of AdaRound can come close to the performance of the
    FP32 baseline and W8A8 DFQ. However, in this comparison it is important to note
    that AdaRound is not a data-free or “zero-shot” approach like DFQ is.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，我们可以清楚地看到二阶泰勒级数项与量化后精度下降之间的关联，表明这一项可以很好地作为优化量化任务损失的代理。然而，即使忽略了网络权重之间的跨层交互，Hessian仍然在大规模网络层计算时代价高昂。下面的结果显示，AdaRound的W4A8配置接近于FP32基准和W8A8
    DFQ的性能。然而，在这个比较中，值得注意的是，AdaRound并不是像DFQ那样的数据无关或“零-shot”方法。
- en: '![](../Images/fa01a1f6c698f06e32420a0e8fce430c.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa01a1f6c698f06e32420a0e8fce430c.png)'
- en: Results from [AdaRound](https://arxiv.org/abs/2004.10568) demonstrate that this
    method preserves CNN performance down to W4A8 precision better than previous methods.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[AdaRound](https://arxiv.org/abs/2004.10568)的结果表明，该方法能够保持CNN性能，在W4A8精度下比以往的方法更好。
- en: Only a couple months after AdaRound in mid-2020, [AdaQuant](https://arxiv.org/abs/2006.10518)
    took things a step further to introduce a new PTQ method which could quantize
    both the weights and activations to 4-bit while retaining performance, with less
    than 1% ImageNet top-1 accuracy degradation using ResNet50\. The authors circumvent
    the limitations of AdaRound by using a small calibration set to minimize layer-wise
    quantization MSE in both the weights and activations one layer at a time, using
    channel-wise granularity. They note that the quantization process introduces an
    inherent bias and variance into the batch norm statistics, and propose to recover
    performance degradation by re-estimating these statistics in their proposed Batch
    Normalization Tuning (BNT) approach. The authors offer both a mixed and fixed
    precision variant of AdaQuant, and the code is [available on GitHub](https://github.com/itayhubara/CalibTIP).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在2020年中期AdaRound发布几个月后，[AdaQuant](https://arxiv.org/abs/2006.10518)进一步提出了一种新的PTQ方法，该方法可以将权重和激活量化到4位，同时保持性能，使用ResNet50时ImageNet的Top-1精度下降不到1%。作者通过使用小型校准集，以逐层的方式使用通道粒度最小化权重和激活的逐层量化均方误差，从而绕过了AdaRound的局限性。他们指出，量化过程会引入固有的偏差和方差，影响批归一化的统计数据，并提出通过重新估算这些统计数据来恢复性能下降，这就是他们提出的批归一化调优（Batch
    Normalization Tuning，BNT）方法。作者提供了AdaQuant的混合精度和固定精度变种，代码可在[GitHub上获取](https://github.com/itayhubara/CalibTIP)。
- en: '![](../Images/553e39581e08c66868624ccaeef38a80.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/553e39581e08c66868624ccaeef38a80.png)'
- en: Results of [AdaQuant](https://arxiv.org/abs/2006.10518) show superior ImageNet
    top-1 accuracy over AdaRound and Quantization-Aware Knowledge Distillation (QAT-KLD)
    at various calibration dataset scales. Variance is calculated over 5 runs for
    each configuration.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[AdaQuant](https://arxiv.org/abs/2006.10518)的结果显示，在不同的校准数据集规模下，AdaQuant相较于AdaRound和量化感知知识蒸馏（QAT-KLD）在ImageNet的Top-1精度上表现更为优秀。每种配置的方差是通过5次实验计算得出的。'
- en: 'In this section, we’ve seen the rising focus on PTQ methods in the post-AlexNet
    era. While the primary motivators for PTQ during this period were typically concerns
    about edge deployment and data privacy, the work covered here provided an important
    foundation for the upcoming reliance on PTQ approaches that would arise with the
    exploding model sizes in the years to come. Before we part ways with the CNN era,
    there is one more research trend which was coming into fruition during this period
    that we need to cover, which was also motivated by the desire to operate at the
    edge: the extreme quantization of neural networks down to only 1 bit (binary networks)
    or 2 bits (ternary networks).'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们看到了在后AlexNet时代PTQ方法的日益关注。尽管在此期间，PTQ的主要驱动力通常是边缘部署和数据隐私的考虑，但这里讨论的工作为即将到来的依赖PTQ方法奠定了重要基础，这些方法将随着未来几年模型规模的急剧扩大而兴起。在我们告别CNN时代之前，还有一个在这一时期逐渐成型的研究趋势需要讨论，这一趋势同样受到希望在边缘设备上运行的驱动：将神经网络极端量化到仅1位（二进制网络）或2位（三进制网络）。
- en: 'Extreme Quantization: Binary and Ternary Networks'
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 极端量化：二进制和三进制网络
- en: '![](../Images/eec8ff7d1201ddb2d5799d2bc9bc2c9e.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eec8ff7d1201ddb2d5799d2bc9bc2c9e.png)'
- en: Image by author using DALL-E 3.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用DALL-E 3生成。
- en: 'Extreme quantization refers to compression at ≤2 bits, meaning either ternary
    (2-bit), or binary (1-bit). The ability to effectively compress models down to
    these levels of precision comes with obvious benefits, as the models become 16–32
    times smaller than their FP32 counterparts, allowing for deployment on smaller
    edge devices using far less power consumption, and saving valuable on-chip real
    estate for optimizing computation speed. Unsurprisingly, reducing the precision
    of neural networks this low comes with equally extreme challenges, due to the
    loss of representational power. However, the costly multiply-accumulate (MAC)
    operations in network computation can be entirely replaced in both binary and
    ternary networks by far more energy-efficient addition/subtraction and bit shift
    operations, making the potential gains radical, and galvanizing researchers to
    tackle the challenges involved. In this section, we observe the compelling results
    which arose from the developing field of extreme network quantization during the
    CNN era, and discover why it has since become such a highly magnetic and frequently
    cited field: the extraordinary gains in efficiency offered by low-bit networks
    are hard to ignore. Moreover, we will see that binary nets can be ensembled to
    use an equivalent number of bits as a fixed-point network to exceed its performance
    while maintaining all the benefits of binary arithmetic. First, let us rewind
    to 2014, so that we can build our understanding of low-precision networks bit-by-bit.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 极限量化是指将模型压缩到≤2位的精度，这意味着要么是三值（2位），要么是二值（1位）。将模型有效压缩到如此低的精度显然带来了显著的好处，因为这些模型比其FP32对应物小16到32倍，能够部署到较小的边缘设备上，消耗更少的功率，同时为优化计算速度节省宝贵的芯片空间。不足为奇的是，神经网络精度如此降低带来了同样极端的挑战，因为表示能力会丧失。然而，网络计算中代价高昂的乘加（MAC）操作在二值和三值网络中可以被更为节能的加法/减法以及位移操作完全替代，从而使得潜在收益极为显著，并激励研究人员去应对相关挑战。在这一节中，我们将观察到在CNN时代，极限网络量化这一发展领域带来的引人注目的成果，并发现为何它后来成为如此具有吸引力且频繁被引用的领域：低位网络带来的非凡效率提升是难以忽视的。此外，我们还将看到，二值网络可以通过集成使用与定点网络等效数量的比特，超越定点网络的性能，同时保持二值运算的所有优势。首先，让我们回到2014年，以便逐步建立我们对低精度网络的理解。
- en: As a pioneering work in extreme quantization, the 2014 study by Hwang & Sung
    from Seoul National University titled “[Fixed-Point Feedforward Deep Neural Network
    Design Using Weights +1, 0, and -1](https://www.semanticscholar.org/paper/Fixed-point-feedforward-deep-neural-network-design-Hwang-Sung/a4db2d26b5d169de6b64de361dc7d4fd5b1f61a3)”
    is a QAT approach which obtains ternary (2-bit) weights and 3-bit activation signal
    with negligible performance loss. In contrast, the authors observe that the biases
    must be allotted a higher precision of 8 bits in order to preserve performance.
    The quantization thresholds are initially chosen to minimize MSE between the original
    and quantized tensors, and then an exhaustive search is performed in each layer
    one-by-one in order to tune these initial proposals to their optimal values which
    minimize the network output error. Finally, the quantized weights are fine-tuned
    using a fixed-point backpropagation scheme that is modified to handle the quantization.
    While the authors set a foundational precedent in the field of extreme neural
    network quantization, their use of exhaustive search is a testament to the smaller
    model sizes of the day, and more scalable solutions would become necessary for
    the growing models sizes in the coming years.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 作为极限量化领域的开创性工作，2014年来自首尔国立大学的黄和成研究人员发表的《[使用+1、0和-1权重的定点前馈深度神经网络设计](https://www.semanticscholar.org/paper/Fixed-point-feedforward-deep-neural-network-design-Hwang-Sung/a4db2d26b5d169de6b64de361dc7d4fd5b1f61a3)》是一种量化感知训练（QAT）方法，该方法通过获得三值（2位）权重和3位激活信号，几乎没有性能损失。相反，作者观察到，为了保持性能，偏置项必须分配更高的精度，即8位。量化阈值最初选择是为了最小化原始张量和量化张量之间的均方误差（MSE），然后在每一层逐一执行穷举搜索，以调节这些初步提议，直到它们达到能够最小化网络输出误差的最优值。最后，通过使用针对量化进行修改的定点反向传播方案来微调量化后的权重。虽然作者在极限神经网络量化领域奠定了基础性先例，但他们采用的穷举搜索方法也证明了当时模型尺寸较小，而对于未来日益增长的模型规模，更多可扩展的解决方案变得更加必要。
- en: In 2015, Courbariaux et al. presented the “[BinaryConnect](https://arxiv.org/abs/1511.00363)”
    algorithm. As the name suggests, their method produces a binarized network, in
    which the weights are constrained to be either -1 or 1\. The authors remark that
    noisy weights, i.e. weights that are discretized using a stochastic rounding scheme,
    “are quite compatible with Stochastic Gradient Descent (SGD),” as we saw earlier
    in Gupta et al.’s 2015 work on stochastic rounding. Like the approach of Hwang
    & Sung above, the authors apply the gradients from the loss generated by the quantized
    weights to update the full-precision weights (which are stored separately), and
    the quantized weights are derived from the current state of the full-precision
    weights for each forward pass. The authors demonstrate that both *deterministic
    binarization* (simply taking the sign of the weight) and *stochastic binarization*
    (using weight magnitude to derive probability) both work well as regularization
    mechanisms (similarly to dropout) in the networks studied, demonstrating slower
    convergence curves with lower final validation error compared with the non-regularized
    full-precision baseline. While these results are very exciting, it is important
    to consider the fact that neither the CIFAR-10 dataset nor the CNN being trained
    in their study are particularly complex by today’s standards, so it is not clear
    at this point if these results would hold up using deeper networks or more challenging
    tasks. The code for BinaryConnect is [available on GitHub](https://github.com/MatthieuCourbariaux/BinaryConnect).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在2015年，Courbariaux 等人提出了 “[BinaryConnect](https://arxiv.org/abs/1511.00363)”
    算法。顾名思义，他们的方法生成一个二值化网络，其中权重被限制为-1或1。作者指出，噪声权重，即使用随机舍入方案离散化的权重，“与随机梯度下降（SGD）非常兼容”，正如我们之前在
    Gupta 等人2015年关于随机舍入的研究中看到的那样。与 Hwang & Sung 上述方法类似，作者将由量化权重生成的损失梯度应用于更新全精度权重（这些权重是单独存储的），量化权重则是根据每次前向传播时全精度权重的当前状态得出的。作者展示了
    *确定性二值化*（简单地取权重符号）和 *随机二值化*（使用权重大小推导概率）作为正则化机制（类似于 dropout）在所研究的网络中都能很好地工作，展示了比非正则化的全精度基线更慢的收敛曲线和更低的最终验证误差。尽管这些结果非常令人兴奋，但需要考虑的是，CIFAR-10
    数据集和他们研究中训练的CNN按今天的标准并不复杂，因此目前尚不清楚这些结果是否能在更深的网络或更具挑战性的任务上保持有效。BinaryConnect 的代码
    [可在 GitHub 上获取](https://github.com/MatthieuCourbariaux/BinaryConnect)。
- en: '![](../Images/379a1cdc07bb590d1a78f48ffefea042.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/379a1cdc07bb590d1a78f48ffefea042.png)'
- en: Results from [BinaryConnect](https://arxiv.org/abs/1511.00363) show that the
    binarization of the network weights acts as a form of regularization and actually
    improves training outcomes in the CNN and task studied.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 [BinaryConnect](https://arxiv.org/abs/1511.00363) 的结果表明，网络权重的二值化作为一种正则化形式，实际上改善了CNN和研究任务的训练结果。
- en: Later in 2015, Lin et al.’s “[Neural Networks with Few Multiplications](https://arxiv.org/abs/1510.03009)”
    extended the work and codebase of BinaryConnect in [a fork of the original repo](https://github.com/hantek/BinaryConnect)
    to include a ternary variant of their stochastic binarization approach called
    “TernaryConnect,” and also introduced a *quantized back propagation* (QBP) scheme,
    where network activations are quantized to integer powers of two so that the expensive
    multiplication operations in the backwards passes can be replaced by efficient
    bit shift operations, further increasing training efficiency, and ticking another
    checkbox on the list of network operations which can be binarized.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 后来在2015年，Lin 等人提出的 “[Neural Networks with Few Multiplications](https://arxiv.org/abs/1510.03009)”
    扩展了 BinaryConnect 的工作和代码库，并在 [原始代码库的分支](https://github.com/hantek/BinaryConnect)
    中加入了他们的随机二值化方法的三值变体，称为“TernaryConnect”，并引入了 *量化反向传播*（QBP）方案，其中网络激活值被量化为二的整数次方，这样在反向传播过程中昂贵的乘法操作就可以被高效的位移操作所替代，进一步提高了训练效率，同时也完成了在网络操作中可二值化的另一个项目。
- en: '![](../Images/b2b8aa901338cae2878b97e3bb07ca36.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2b8aa901338cae2878b97e3bb07ca36.png)'
- en: Results on CIFAR-10 from [Lin et al. 2015](https://arxiv.org/abs/1510.03009)
    show that using QBP leads to equivalent results as BinaryConnect, and that the
    ternary network achieves slightly better training outcomes. All quantization approaches
    exceed baseline performance thanks to the induced regularization effect of stochastic
    quantization.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 [Lin 等人 2015](https://arxiv.org/abs/1510.03009) 在 CIFAR-10 上的结果显示，使用 QBP
    可以得到与 BinaryConnect 等效的结果，而且三值网络的训练结果略好。所有量化方法都因随机量化的正则化效应而超越了基线表现。
- en: 'In 2016, Courbariaux and his colleagues built on this work a third time to
    enable binarization of activations during training. In their seminal paper “[Binarized
    Neural Networks: Training Neural Networks with Weights and Activations Constrained
    to +1 or −1](https://arxiv.org/abs/1602.02830)” (BNN, or also often referred to
    by the name of their first draft and [GitHub repo](https://github.com/MatthieuCourbariaux/BinaryNet):
    “[BinaryNet](https://arxiv.org/abs/1602.02830v1)”), the authors again compare
    deterministic vs. stochastic binarization, observing that while stochastic binarization
    is theoretically and empirically superior, the simplicity of not having to generate
    random bits during training makes deterministic binarization very appealing. Thus,
    the authors employ a best-of-both-worlds approach in which stochastic binarization
    is used only on the activations during training. Seeking further improvements,
    the authors address the costly multiplications required by batch normalization
    ([BN](https://arxiv.org/abs/1502.03167)) layers in neural networks, and propose
    a method to avoid these costly operations and maintain the binary nature of network
    operations using *shift-based batch normalization* (SBN), which approximates the
    effect of BN using inexpensive bit-shift (power-of-two scaling) operations instead
    of multiplications. In conjunction, the authors propose a *shift-based AdaMax
    optimization algorithm* to circumvent the multiplications required by the canonically
    employed [Adam optimizer](https://arxiv.org/abs/1412.6980), and show that training
    outcomes are unaffected by the noise introduced by either of these approximations.
    Testing their approach once again on basic CNNs with the CIFAR-10 dataset, the
    authors demonstrate compelling results using binarized weights and activations
    to train neural networks. Code for training these Binarized Neural Networks (BNNs),
    complete with the customized GPU kernels, is [available online](https://github.com/MatthieuCourbariaux/BinaryNet).'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '在2016年，Courbariaux及其同事在此基础上进行了第三次改进，使得在训练过程中也能实现激活函数的二值化。在他们的开创性论文《[Binarized
    Neural Networks: Training Neural Networks with Weights and Activations Constrained
    to +1 or −1](https://arxiv.org/abs/1602.02830)》（BNN，或常常称为他们的第一版草稿和[GitHub 仓库](https://github.com/MatthieuCourbariaux/BinaryNet)的“[BinaryNet](https://arxiv.org/abs/1602.02830v1)”）中，作者再次对比了确定性和随机二值化，观察到尽管从理论和实证上讲，随机二值化优于确定性二值化，但由于训练过程中无需生成随机位，确定性二值化在简化方面具有很大的吸引力。因此，作者采用了一种“取长补短”的方法，其中在训练过程中仅对激活函数使用随机二值化。为了寻求进一步的改进，作者解决了神经网络中批量归一化（[BN](https://arxiv.org/abs/1502.03167)）层所需的高昂乘法操作，提出了一种方法来避免这些昂贵的运算，同时保持网络操作的二值化特性，这就是*基于移位的批量归一化*（SBN），它通过廉价的位移（以2的幂次为基础的缩放）操作代替乘法运算，近似了批量归一化的效果。同时，作者提出了一种*基于移位的
    AdaMax 优化算法*，以绕过[Adam优化器](https://arxiv.org/abs/1412.6980)所需的乘法操作，并证明训练结果不会受到这两种近似方法引入的噪声的影响。在对CIFAR-10数据集的基本CNN模型进行测试时，作者使用二值化的权重和激活函数训练神经网络，展示了有力的实验结果。用于训练这些二值化神经网络（BNN）的代码，包括定制的GPU内核，已[在线发布](https://github.com/MatthieuCourbariaux/BinaryNet)。'
- en: '![](../Images/5179de1eee7c48ee8f6673f04cc3231e.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5179de1eee7c48ee8f6673f04cc3231e.png)'
- en: Results from BNN paper show that binarized networks achieve validation errors
    nearly on par with baseline.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: BNN 论文的结果表明，二值化网络在验证误差上几乎与基线相当。
- en: Rastegari et al.’s 2016 [XNOR-Net](https://arxiv.org/abs/1603.05279) paper was
    the first to test CNN binarization on the large-scale ImageNet dataset, showing
    that BinaryConnect and BNN (aka BinaryNet) don’t work as well at these scales.
    The authors exceed the results of BNN by 16.3% on top-1 ImageNet accuracy by introducing
    a new method of weight binarization which incorporates weight scaling, and find
    that the optimal scaling factor for a given weight matrix is the average of its
    absolute values. Similar to BinaryConnect and BNN, the authors update a separate
    set of full-precision weights using the gradients calculated from the quantized
    forward passes using the STE, although they do not opt to use the shift-based
    approximations for BN and Adam offered in the BNN paper. The drop in performance
    from their Binary-Weight-Network (BWN) configuration to the XNOR-Net configuration
    highlights the difficulty of discretizing activation signals in more complex tasks
    like ImageNet, but binarizing these representations is especially ambitious. Nevertheless,
    the fact that the weight-only binarization can achieve the same performance as
    the baseline CNN looks like yet another promising opportunity for “free lunch”
    compression, and the fact that the XNOR-Net version can still achieve reasonable
    performance even with full signal binarization is compelling. The code for XNOR-Net
    is also available on [GitHub](https://github.com/allenai/XNOR-Net).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Rastegari 等人 2016 年的 [XNOR-Net](https://arxiv.org/abs/1603.05279) 论文首次在大规模的
    ImageNet 数据集上测试了 CNN 的二值化，结果表明 BinaryConnect 和 BNN（即 BinaryNet）在这些规模下的表现不如预期。作者通过引入一种新的权重二值化方法，该方法结合了权重缩放，超过了
    BNN 在 ImageNet top-1 准确率上的 16.3% 的成绩，并发现给定权重矩阵的最优缩放因子是其绝对值的平均值。与 BinaryConnect
    和 BNN 相似，作者使用 STE 从量化的前向传递中计算梯度来更新一组独立的全精度权重，尽管他们没有选择在 BNN 论文中提供的基于移位的 BN 和 Adam
    近似方法。从他们的 Binary-Weight-Network (BWN) 配置到 XNOR-Net 配置的性能下降突显了在像 ImageNet 这样复杂任务中离散化激活信号的难度，但二值化这些表示尤其具有挑战性。然而，权重单独二值化能够达到与基准
    CNN 相同的性能，这为“免费午餐”压缩提供了另一个有希望的机会，且 XNOR-Net 版本即使在完全信号二值化的情况下仍能取得合理的表现，这一点令人信服。XNOR-Net
    的代码也可以在 [GitHub](https://github.com/allenai/XNOR-Net) 上找到。
- en: '![](../Images/c50043468789da3545338108bea3109f.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c50043468789da3545338108bea3109f.png)'
- en: Figure from [XNOR-Net](https://arxiv.org/abs/1603.05279) shows the accuracy
    tradeoffs of their two proposed approaches. Note that the Binary Weight Network
    without binarized inputs matches the baseline performance, but doesn’t achieve
    the dramatic computation saving of the XNOR-Net configuration.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 [XNOR-Net](https://arxiv.org/abs/1603.05279) 的图展示了他们提出的两种方法的准确度权衡。请注意，没有二值化输入的
    Binary Weight Network 达到与基准相同的性能，但并未实现 XNOR-Net 配置中的显著计算节省。
- en: '![](../Images/921453b1d03e79854734fb25605ce599.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/921453b1d03e79854734fb25605ce599.png)'
- en: Results from [XNOR-Net](https://arxiv.org/abs/1603.05279) paper show the improvements
    of XNOR-Net over BNN (in full end-to-end binarization), and and BWN over BinaryConnect
    (in weight-only binarization) on the complex ImageNet classification benchmark.
    Note that for the XNOR-Net top-1 scores, the eval scores seem to saturate about
    halfway, which could be an indication of a crippling lack of representation power
    using binary signal.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 [XNOR-Net](https://arxiv.org/abs/1603.05279) 论文的结果显示，XNOR-Net 在复杂的 ImageNet
    分类基准上超越了 BNN（在全端到端二值化中）以及 BWN 超过了 BinaryConnect（在仅权重二值化中）的表现。请注意，对于 XNOR-Net 的
    top-1 分数，评估分数似乎在中途饱和，这可能表明使用二进制信号时表现能力的严重缺乏。
- en: Li et al.’ 2016 “Ternary Weight Networks” ([TWN](https://arxiv.org/abs/1605.04711))
    is a QAT approach which trains ternary networks from scratch. Their quantization
    scheme seeks to minimize Euclidean distance between the quantized and raw weights
    using learned layer-wise scaling factors and ternarization thresholds set to 3/4
    the average magnitude per weight tensor to approximate the full-precision weights
    as closely as possible. The authors observe that ternary networks “show better
    expressive capabilities than binary precision counterparts,” and demonstrate this
    concept using the example of binary vs ternary 3x3 convolution filters, which
    can take on 512 and 19683 possible unique templates, respectively. Their experiments
    prove that this additional expressive power is beneficial on various tasks, including
    MNIST, CIFAR-10, ImageNet, and the [Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/)
    object detection task. In the results below, we can see that this additional expressive
    ability of ternary networks is particularly beneficial in the more challenging
    ImageNet and Pascal VOC tasks, a possible signal that the optimism we were experiencing
    from the very promising binarization results we saw above on less complex models
    and tasks may not hold up as the complexity grows.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Li 等人于2016年发表的《三值权重网络》（[TWN](https://arxiv.org/abs/1605.04711)）是一种量化感知训练（QAT）方法，旨在从头开始训练三值网络。他们的量化方案通过使用学习得到的逐层缩放因子和三值化阈值（设定为每个权重张量的平均值的3/4），尽可能精确地逼近全精度权重，旨在最小化量化权重和原始权重之间的欧几里得距离。作者观察到三值网络“比二值精度的网络具有更好的表达能力”，并通过二值和三值3x3卷积滤波器的示例演示了这一点，后者分别可以有512和19683种不同的模板。实验结果证明，这种额外的表达能力对各种任务（包括MNIST、CIFAR-10、ImageNet和[Pascal
    VOC](http://host.robots.ox.ac.uk/pascal/VOC/)目标检测任务）都有益。在下面的结果中，我们可以看到，三值网络在更加复杂的ImageNet和Pascal
    VOC任务中表现出更强的优势，这可能是一个信号，表明我们在较为简单的模型和任务中看到的二值化结果所带来的乐观情绪，在复杂性增加时可能无法保持。
- en: '![](../Images/069e18c2d9d41d6908d90268d4b8a2c3.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/069e18c2d9d41d6908d90268d4b8a2c3.png)'
- en: Results from [TWN](https://arxiv.org/abs/1605.04711) paper show that the additional
    expressive power of ternary networks is beneficial, particularly in the more challenging
    ImageNet and Pascal VOC tasks. Remember that BNN and XNOR-Net binarize the activation
    signals, whereas the TWN approach, like BinaryConnect and BWN, focus only on the
    quantization of weights, which is less challenging.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[TWN](https://arxiv.org/abs/1605.04711)论文中的结果表明，三值网络的额外表达能力是有益的，特别是在更具挑战性的ImageNet和Pascal
    VOC任务中。请记住，BNN 和 XNOR-Net 对激活信号进行二值化，而TWN方法像BinaryConnect和BWN一样，仅专注于权重的量化，这相对较少具有挑战性。'
- en: In late 2016, Hou, Yao, and Kwok published “Loss-aware Binarization of Deep
    Networks” ([LAB](https://arxiv.org/abs/1611.01600)), which filled an empty research
    lane left by previous methods which had not optimized the binarization process
    based on its impact on the cost function directly. To binarize the network in
    a way which minimizes the cost function, the authors solve a [proximal Newton
    algorithm](https://arxiv.org/abs/1206.1623) by using the second-order gradient
    information captured in the Adam optimizer to efficiently extract a diagonal Hessian
    approximation, rather than computing the Hessian directly. The authors show that
    their method is “more robust to wide and deep networks,” and also extend their
    investigation into NLP tasks using Recurrent-Neural Networks (RNNs). Then in early
    2018, Hou & Kwok extended their LAB algorithm to produce higher-precision networks
    in “[Loss-Aware Weight Quantization of Deep Networks](https://arxiv.org/abs/1802.08635)”
    (LAQ, or LAT in the ternary case), showing that the proposed method improved results
    further over the binarization offered by LAB.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 2016年末，Hou、Yao 和 Kwok 发表了《深度网络的损失感知二值化》（[LAB](https://arxiv.org/abs/1611.01600)），填补了前人方法未能基于其对代价函数的直接影响优化二值化过程的研究空白。为了以最小化代价函数的方式对网络进行二值化，作者通过使用
    Adam 优化器中捕获的二阶梯度信息，求解了一个[近端牛顿算法](https://arxiv.org/abs/1206.1623)，从而高效地提取了对角Hessian近似，而不是直接计算Hessian。作者展示了他们的方法“对宽而深的网络更具鲁棒性”，并将他们的研究扩展到使用循环神经网络（RNN）处理自然语言处理（NLP）任务。随后在2018年初，Hou
    和 Kwok 将他们的 LAB 算法扩展到“[深度网络的损失感知权重量化](https://arxiv.org/abs/1802.08635)”（LAQ，或者在三值情况中为
    LAT），展示了该方法在比 LAB 二值化更高精度的网络上的表现。
- en: '![](../Images/40776c34364cc2b4d70a98bfafe78ed8.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40776c34364cc2b4d70a98bfafe78ed8.png)'
- en: Results from [LAQ](https://arxiv.org/abs/1802.08635) paper show comparisons
    with LAB, BinaryConnect, and BWN.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[LAQ](https://arxiv.org/abs/1802.08635)论文的结果显示与LAB、BinaryConnect和BWN的比较。
- en: 'In 2017, [Dong et al.](https://arxiv.org/abs/1708.01001) proposed the *stochastic
    quantization* algorithm: a QAT approach to extreme quantization in which only
    a portion of the network elements/filters (inversely proportional in size to the
    quantization error) are quantized during each training step, and updated separately
    from the full-precision weights. As training progresses, eventually all weights
    are quantized, and the resulting low-bit network maintains significantly better
    accuracy than equivalent BWN and TWN models.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年，[Dong等人](https://arxiv.org/abs/1708.01001)提出了*随机量化*算法：一种量化训练方法，用于极端量化，在每次训练步骤中，只有部分网络元素/滤波器（与量化误差成反比）被量化，并与全精度权重分开更新。随着训练的进行，最终所有权重都会被量化，最终得到的低位网络在准确度上显著优于等效的BWN和TWN模型。
- en: '![](../Images/689145e348d27fbe93c913d7afe9910f.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/689145e348d27fbe93c913d7afe9910f.png)'
- en: Results from [Dong et al. 2017](https://arxiv.org/abs/1708.01001) show that
    stochastic quantization (SQ) leads to marked improvements over baselines.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[Dong等人 2017](https://arxiv.org/abs/1708.01001)的结果表明，随机量化（SQ）相较于基线模型有显著的改进。
- en: The 2017 Incremental Network Quantization ([INQ](https://arxiv.org/abs/1702.03044))
    paper we saw earlier in the QAT of CNNs section, which surpassed the previous
    state-of-the-art in model compression set by Deep Compression in 2015, also investigated
    the viability of their approach in creating ternary networks. In the chart below,
    we can see that their approach is notably superior to TWN in ResNet-18 trained
    on the ImageNet classification task, reducing the error by over 4%. Looking above,
    we can also see that it beats the 36.18% top-1 error rate of the stochastic-quantized
    TWN (SQ-TWN) by over 2%.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年提出的增量网络量化（[INQ](https://arxiv.org/abs/1702.03044)）论文，我们之前在卷积神经网络（CNNs）的量化训练（QAT）部分看到过，超越了2015年由深度压缩（Deep
    Compression）设定的模型压缩最前沿，还探讨了他们的方法在创建三值网络中的可行性。下图中，我们可以看到他们的方法在ResNet-18上训练ImageNet分类任务时明显优于TWN，错误率降低了超过4%。从上面来看，我们还可以看到，它比随机量化TWN（SQ-TWN）的36.18%
    top-1错误率低了超过2%。
- en: '![](../Images/ae4567246cee4f0d86a9421fdda6f8a9.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae4567246cee4f0d86a9421fdda6f8a9.png)'
- en: Later in 2017 Lin et al.’s “Towards Accurate Binary Convolutional Neural Network”
    ([ABC-Net](https://arxiv.org/abs/1711.11294)) paper sought to overcome the lack
    of representational power in binary networks by combining multiple sets of binary
    weights or activations to more faithfully represent the high-precision values,
    showing that by using 3–5 weight bases and 5 binary activations, the accuracy
    degradation on ImageNet can be reduced to 5% from baseline. The authors point
    out that while this requires the use of more bits, the scheme is preferable to
    using higher-bit fixed-point representation because it still avoids the need for
    the more complex arithmetic operators as the bitwise math is still done in binary.
    Their work marks the first time that binary neural networks reached comparable
    performance on ImageNet to full precision baselines, but their solution increases
    baseline BNN complexity by *O(k * l),* where *k* is the number of weight bases
    and *l* is the number of activation bases used, so there is a notable loss in
    efficiency.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年，Lin等人的“迈向准确的二值卷积神经网络”（[ABC-Net](https://arxiv.org/abs/1711.11294)）论文试图通过结合多个二值权重或激活集来克服二值网络在表示能力上的不足，以更忠实地表示高精度值，展示了使用3到5个权重基和5个二值激活时，ImageNet上的精度下降可从基线减少到5%。作者指出，尽管这需要使用更多的位数，但该方案比使用更高位数的定点表示更可取，因为它仍然避免了更复杂的算术运算，位运算仍然在二进制中进行。他们的工作标志着二值神经网络首次在ImageNet上达到了与全精度基准相当的性能，但他们的解决方案将基线BNN的复杂度增加了*O(k
    * l)*，其中*k*是使用的权重基数数量，*l*是使用的激活基数数量，因此在效率上有明显的损失。
- en: '![](../Images/668c6f21802010eb3ab5911c79826819.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/668c6f21802010eb3ab5911c79826819.png)'
- en: 'Zhu et al.’s 2018 “Binary Ensemble Neural Network: More Bits per Network, or
    More Networks per Bit?” ([BENN](https://arxiv.org/abs/1806.07550)) paper argued
    that the limitations of BNNs are not solvable through further optimization of
    the binarization process, since they are rooted in the lack of representational
    ability of the binary space. The authors aimed to reduce the prediction variance
    and improve the robustness to noise when using binary networks, achieving this
    by creating ensembles of multiple BNNs using boosting or bagging. Their experiments
    demonstrate that the statistical properties of the ensembled classifiers improve,
    and the performance improves drastically as a result. The added complexity of
    executing these ensembles is only *O(k)*, more efficient than ABC-Net by a factor
    of *l*, and outperforms it significantly on ImageNet. Moreover, because the ensemble
    can be parallelized, the solution can have *O(1)* addedcomplexity, and run just
    as fast as the baseline BNN.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 朱等人2018年发表的论文《二元集成神经网络：每个网络更多比特，还是每比特更多网络？》([BENN](https://arxiv.org/abs/1806.07550))提出，BNN的局限性无法通过进一步优化二值化过程来解决，因为这些局限性根源于二进制空间缺乏表示能力。作者的目标是通过创建多个BNN的集成，利用提升（boosting）或装袋（bagging）方法来减少预测方差并提高对噪声的鲁棒性。实验表明，集成分类器的统计性质得到了改善，性能也因此大幅提升。执行这些集成的额外复杂度仅为*O(k)*，比ABC-Net高出*l*倍，并且在ImageNet上显著超越了它。此外，由于集成可以并行化，这个解决方案的额外复杂度可以是*O(1)*，并且运行速度与基准BNN一样快。
- en: '![](../Images/e6af35d1cf27a599a1517cd4c9a0c41c.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e6af35d1cf27a599a1517cd4c9a0c41c.png)'
- en: Results from [BENN](https://arxiv.org/abs/1806.07550) show that BNN ensembles
    can offer a step change in the performance of binarized networks. Note that they
    compare to ABC-Net with only 1 bit base for weights and activations, which is
    a curious choice, since they are comparing against ensembles of 3 or 6 BNNs. Since
    the complexity of these ensembles without parallelization would be O(3) and O(6),
    it would be more fair to compare them to similarly complex ABC-Net configurations,
    which as we can see above range from 49.1% (3bit/1bit) to 54.1% (5bit/1bit), which
    are still lower, but offer much more informative comparisons. Consider also that
    INQ produces ternary networks which exceed these results with fewer bits, but
    the concept of parallelized ensembles of binary networks is extremely compelling.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[BENN](https://arxiv.org/abs/1806.07550)的结果表明，BNN集成能够显著提升二值化网络的性能。值得注意的是，他们将其与仅使用1比特权重和激活函数的ABC-Net进行了比较，这一选择颇为奇特，因为他们将其与3个或6个BNN的集成进行了比较。由于这些集成的复杂度在没有并行化的情况下分别为O(3)和O(6)，因此将它们与具有类似复杂度的ABC-Net配置进行比较更为公平。正如我们上面所见，ABC-Net的复杂度范围从49.1%（3比特/1比特）到54.1%（5比特/1比特），尽管这些结果仍然较低，但提供了更多有意义的比较。再者，INQ产生的三元网络在使用更少的比特时就超过了这些结果，但并行化的二元网络集成概念仍然极具吸引力。
- en: In this section, we’ve witnessed the explosion of interest in extreme neural
    network quantization following the advent of large CNNs which began after the
    resounding success of AlexNet in 2012\. The tantalizing urge to deploy the newfound
    heights of modeling capabilities in this era on edge devices with low-power hardware
    was irresistible, as this is where many practical applications of deep learning
    exist. Note that most of the methods in this section train low-bit networks from
    scratch without quantizing pretrained weights, which is more easily facilitated
    in the fully quantized approaches by their extraordinary efficiency. Later, we
    will see that this impressive period of foliation in extreme quantization methodology
    would mature into a fertile bed of soil in which future breakthroughs would be
    made.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们见证了极端神经网络量化领域的兴趣爆炸，尤其是在大型CNN问世之后，这一趋势始于2012年AlexNet的巨大成功。在这个时代，部署具有新高度建模能力的神经网络于低功耗硬件的边缘设备上的诱人需求是不可抗拒的，因为这是深度学习的许多实际应用所在。需要注意的是，本节中的大多数方法是从头开始训练低比特网络，而不是量化预训练权重，这在完全量化方法中通过其卓越的效率更容易实现。稍后，我们将看到，在极端量化方法的蓬勃发展期过后，未来的突破将在这一领域的肥沃土壤中孕育而生。
- en: For now, we close our chapter on the CNN era, as now we have seen the formation
    of several distinct areas of quantization research flourish after the success
    of AlexNet brought massive influxes of talent and funding into the deep learning
    field. We’ve seen QAT approaches achieve impressive levels of compression without
    losing performance by fine-tuning the quantized weights, mixed-precision techniques
    achieve new levels of compression by incorporating sensitivity analysis into the
    quantization process, PTQ approaches closely match baseline performance in 8 or
    even 4 bits of precision with no retraining (and in the case of ZSQ, without using
    any calibration data), and finally, we saw the rise of extreme quantization research.
    Now, we shift our focus from the CNN era to the wave of research interest in NLP
    born from the success of the transformer architecture in 2017.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们结束了关于 CNN 时代的章节，因为我们已经看到了几个量化研究领域在 AlexNet 成功后蓬勃发展的过程，这一成功带来了大量的人才和资金进入深度学习领域。我们已经看到，QAT
    方法通过微调量化权重，达到了令人印象深刻的压缩效果而不损失性能，混合精度技术通过将敏感性分析融入量化过程，达到了新的压缩水平，PTQ 方法在 8 位甚至 4
    位精度下，与基准性能匹配且无需重新训练（在 ZSQ 的情况下，甚至不需要使用任何校准数据），最后，我们看到了极限量化研究的兴起。现在，我们将焦点从 CNN
    时代转移到由 2017 年变压器架构的成功引发的自然语言处理（NLP）研究浪潮。
- en: Quantization of LLMs
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的量化
- en: '![](../Images/8c8e644f6f6d58e0d7d07ac93e3621f7.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c8e644f6f6d58e0d7d07ac93e3621f7.png)'
- en: Image by author using DALL-E 3.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用 DALL-E 3 生成。
- en: Now that we are familiar with the functional details and history of quantization,
    we can proceed in our discussion of quantizing the large language models (LLMs)
    of today. As we saw with pruning, the transition into the world of large models
    comes with diminishing hopes of employing compression techniques that require
    model training, such as QAT, for anyone but the largest outfits. Therefore, we
    will see a shift in research focus towards the more lightweight methods of PTQ,
    although the unique challenges of operating at large scales did not stop the ingenuity
    of the research community from finding ways of accessing the benefits of QAT,
    as we will see. In this section, we first review the quantization efforts that
    took place in the period between the publication of the transformer in the seminal
    2017 paper “[Attention is All You Need](https://arxiv.org/abs/1706.03762),” and
    the dawn of LLMs marked by the monumental release of the 175B (billion parameter)
    [GPT-3](https://arxiv.org/abs/2005.14165) in 2020\. Then, we review the proliferation
    of PTQ methods in the LLM era, subsequently shift our focus into QAT approaches
    for LLMs, and finally close our investigation by reviewing the extreme quantization
    of LLMs. This section will complete our education in quantization, qualifying
    us to move on to the implementation guide in the next section, and begin employing
    neural network quantization in our own workflows.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了量化的功能细节和历史，我们可以继续讨论今天大型语言模型（LLMs）的量化。正如我们在剪枝中看到的那样，进入大型模型的世界伴随着对需要模型训练的压缩技术（如
    QAT）的希望减弱，除了最大的机构之外，几乎没有人能采用这些方法。因此，我们将看到研究焦点转向更加轻量化的 PTQ 方法，尽管在大规模操作中的独特挑战并没有阻止研究社区的创造力，他们找到了利用
    QAT 的方法，正如我们将看到的那样。在本节中，我们首先回顾了从变压器架构在 2017 年的开创性论文《[Attention is All You Need](https://arxiv.org/abs/1706.03762)》发布到
    2020 年标志着 175B（十亿参数）[GPT-3](https://arxiv.org/abs/2005.14165)发布的 LLM 时代曙光之间的量化努力。接着，我们回顾了
    LLM 时代 PTQ 方法的普及，随后将焦点转向 LLM 的 QAT 方法，最后通过回顾 LLM 的极限量化，结束我们的研究。本节将完成我们在量化方面的学习，为我们在下一节中进行实现指南的学习做好准备，并开始在我们自己的工作流程中应用神经网络量化。
- en: Quantization in Early Era of Transformers
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变压器早期时代的量化
- en: As we saw in the last section, the success of AlexNet in computer vision set
    off an explosion of research interest in quantizing CNNs for their efficient deployment,
    but investigations into the application of quantization in language models wouldn’t
    pick up speed until years later, with the catalytic moment being the striking
    success of the transformer architecture. Before the explosive growth of language
    model sizes that began with the release of GPT-3 in 2020, there was a period of
    more reasonably-sized explorations in NLP using transformers, although the tendency
    for these models to continue gaining performance with increasingly large sizes
    was quickly becoming clear. Here, we review the formative period of quantization
    research in transformer-based language models that occurred during this formative
    period between the advent of transformers and the rise of multi-billion parameter
    transformer networks. A we will see, this new network architecture posed unique
    challenges to quantization, particularly at low bit widths, which researchers
    moved quickly to understand and overcome.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一节中所见，AlexNet在计算机视觉中的成功引发了对卷积神经网络（CNN）进行量化以实现高效部署的研究热潮，但对语言模型中量化应用的研究直到几年后才开始加速，催化剂时刻是变换器架构的显著成功。在2020年GPT-3发布后，语言模型规模的爆炸性增长之前，曾有一段时间，基于变换器的自然语言处理（NLP）探索相对适中，尽管这些模型随着规模的不断增大，性能持续提升的趋势很快就变得明显。在这里，我们回顾了变换器基础的语言模型量化研究的形成时期，这一时期发生在变换器的出现和多十亿参数变换器网络的兴起之间。正如我们将看到的，这种新型网络架构在量化方面提出了独特的挑战，特别是在低位宽下，研究人员迅速行动，力求理解并克服这些挑战。
- en: The seminal Bidirectional Encoding Representation Transformer ([BERT](https://arxiv.org/abs/1810.04805))
    published by Google in late 2018 continues to be a highly influential transformer-based
    language model. BERT trains on a masked language modeling (MLM) objective to learn
    how to encode bidirectional context to produce embeddings which consider information
    that appears both before and after a given input token in the input sequence,
    resulting in representations that contain deep contextual understanding, useful
    for tasks like sentiment classification and question answering. While BERT uses
    an encoder-only variant of the transformer architecture to create these bidirectionally
    encoded representations, the Generative Pretrained Transformer ([GPT](https://openai.com/research/language-unsupervised))
    models of OpenAI fame, by contrast, use a decoder-only transformer architecture
    to perform an autoregressive modeling task on Byte-Pair Encoded ([BPE](https://arxiv.org/abs/1508.07909))
    text, considering only the preceding tokens in the sequence, and iteratively predicting
    the upcoming tokens. The revelations made about the potential scalability of these
    autoregressive decoder-only transformers and their unlabeled pretraining datasets
    in the [GPT-2](https://openai.com/research/better-language-models) paper would
    ultimately inspire the release of the unprecedentedly large 175B [GPT-3](https://arxiv.org/abs/2005.14165)
    behemoth in 2020, but as we will see, the research in transformer quantization
    would already be well underway by then.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 由谷歌于2018年末发布的开创性双向编码表示变换器（[BERT](https://arxiv.org/abs/1810.04805)）继续作为一种基于变换器的语言模型，对行业产生着深远影响。BERT通过掩蔽语言模型（MLM）目标进行训练，学习如何编码双向上下文，生成能够同时考虑输入序列中给定输入标记之前和之后信息的嵌入，从而产生包含深刻上下文理解的表示，这对情感分类和问答等任务非常有用。尽管BERT使用变换器架构的仅编码器变体来创建这些双向编码表示，但与此相对，OpenAI的生成式预训练变换器（[GPT](https://openai.com/research/language-unsupervised)）模型则采用仅解码器变换器架构，在字节对编码（[BPE](https://arxiv.org/abs/1508.07909)）文本上执行自回归建模任务，只考虑序列中前面的标记，并通过逐步预测接下来的标记。关于这些自回归仅解码器变换器及其无标签预训练数据集的潜在可扩展性的发现，最终启发了2020年发布前所未有的大型175B
    [GPT-3](https://arxiv.org/abs/2005.14165)模型的诞生，但正如我们所见，变换器量化的研究已经在那时取得了长足的进展。
- en: In late 2019, Shen et al. from UC Berkeley published [Q-BERT](https://arxiv.org/abs/1909.05840),
    a QAT method which expanded on the Hessian-based sensitivity analysis of HAWQ
    by additionally considering the variance of the Hessian spectrum, rather than
    only the mean, across the subsample of training data. Q-BERT uses this improved
    measure of sensitivity to establish a layer-wise mixed precision quantization
    scheme, and then quantize each layer at their respective bit-widths using a group-wise
    granularity in which the matrices are split into sub-units that each have their
    own quantization range and look-up table. Using their method, the authors achieve
    13x compression in the weights, 4x smaller activation size, and 4x smaller embedding
    size with a maximum accuracy drop of 2.3% from the baseline BERT. Like most QAT
    techniques, Q-BERT uses the STE in order to approximate the gradients through
    the non-differentiable quantization function.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 2019年底，来自加州大学伯克利分校的Shen等人发布了[Q-BERT](https://arxiv.org/abs/1909.05840)，这是一种QAT方法，它扩展了HAWQ基于Hessian的灵敏度分析，额外考虑了训练数据子样本中Hessian谱的方差，而不仅仅是均值。Q-BERT利用这一改进的灵敏度度量，建立了一个逐层混合精度量化方案，然后使用组粒度对每一层进行量化，其中矩阵被拆分成子单元，每个子单元都有自己的量化范围和查找表。使用他们的方法，作者实现了权重的13倍压缩、激活大小缩小4倍、嵌入大小缩小4倍，且准确度从基线BERT最大下降2.3%。像大多数QAT技术一样，Q-BERT使用STE来近似通过不可微的量化函数的梯度。
- en: In contrast to Q-BERT’s focus on maximizing compression using mixed precision,
    a contemporaneous work in late 2019 from Zafrir et al. at Intel, [Q8BERT](https://arxiv.org/abs/1910.06188),
    focused instead on applying uniform 8-bit quantization across the model and activations,
    which is more advantageous from a hardware optimization perspective, as mixed
    precision operations tend to add overhead and are not conducive to generalized
    hardware acceleration. Their method performs QAT as a fine-tuning phase on pretrained
    BERT models to achieve 4x compression with minimal accuracy loss of 1% or less
    using a simulated quantization approach based on the Jacob et al. 2017 paper we
    saw earlier in the CNN quantization section, which again uses STE for gradient
    approximation. The scaling factor for the weights is calibrated using the maximum
    absolute weight magnitude, and the scaling factor for the activations is based
    on an exponential moving average that is accumulated during training.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 与Q-BERT专注于使用混合精度最大化压缩不同，2019年底Intel的Zafrir等人提出的[Q8BERT](https://arxiv.org/abs/1910.06188)则专注于在模型和激活函数上应用统一的8位量化，这从硬件优化的角度来看更具优势，因为混合精度操作往往会增加开销，并且不利于通用硬件加速。他们的方法将QAT作为预训练BERT模型的微调阶段，使用基于Jacob等人2017年论文中介绍的模拟量化方法（如我们在CNN量化部分看到的那样），以实现4倍压缩，并且准确度损失最小，不超过1%。该方法使用STE进行梯度近似。权重的缩放因子通过最大绝对权重幅度进行校准，而激活的缩放因子则基于在训练过程中累积的指数加权移动平均。
- en: In 2020, Fan et al. presented [Quant-Noise](https://arxiv.org/abs/2004.07320),
    a QAT technique for achieving high rates of model compression by randomly quantizing
    just a subset of the network weights during each forward pass during training.
    The allows the majority of weights receive updates without the error introduced
    by the STE approximation, allowing their values to more accurately shift in ways
    that reduce the impact of the quantized subset. Over time, this leads to superior
    results than using QAT on all of the network weights at once. Quant-Noise investigates
    the use of *Product Quantization* (PQ), in which multiple weights are quantized
    together into single codewords, which allows for very high levels of compression
    with relatively low drops in performance. This approach takes advantage of the
    correlations between weights induced by the structure of the network.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在2020年，Fan等人提出了[Quant-Noise](https://arxiv.org/abs/2004.07320)，这是一种QAT技术，通过在每次训练中的前向传播过程中仅随机量化一部分网络权重，来实现高压缩率。这样大部分权重能够在没有STE近似引入的误差的情况下得到更新，从而使它们的值能够更准确地调整，以减少量化子集的影响。随着时间的推移，这种方法的效果比对所有网络权重同时进行QAT要更好。Quant-Noise研究了*产品量化*（PQ）的使用，其中多个权重被一起量化成单一的代码字，这允许在性能下降较小的情况下实现非常高的压缩比。这种方法利用了由网络结构引起的权重之间的相关性。
- en: In 2020, Zadeh et al. from University of Toronto published [GOBO](https://arxiv.org/abs/2005.03842v2),
    a dictionary-based PTQ method for attention-based models which can use non-uniform
    quantization to compress nearly all of the FP32 weights in BERT to 3 bits without
    losing accuracy. The outliers are preserved in full precision to protect accuracy,
    and are automatically detected using a Gaussian distribution fit, while the rest
    of the weights are stored with 3-bit codewords which index a small set (8 in the
    case of 3-bit) of representative FP32 centroids. Since typical hardware cannot
    perform operations on 3-bit values directly (as the authors of Q-BERT also noted
    in their study), the GOBO authors develop a novel hardware architecture which
    enables the efficient acceleration of 3-bit computation to complement their quantization
    method. While the acceleration benefits are only fully accessible using the specialized
    hardware, the reduced memory footprint of the 3-bit quantized model will lead
    to reduced memory storage and traffic on more general hardware, and therefore
    will still provide some level of improvement in inference latency and energy consumption.
    The GOBO method was inspired by the storage of Huffman-encoded weights in a dictionary
    of few representative values (centroids) seen in “Deep Compression,” but in contrast
    does not require fine-tuning, greatly improves accuracy by not quantizing weight
    outliers, and uses a novel “centroid selection algorithm that converges 9x faster
    than K-means and consistently reduces the number of required centroids by half.”
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 2020年，来自多伦多大学的Zadeh等人发布了[GOBO](https://arxiv.org/abs/2005.03842v2)，这是一种基于字典的PTQ方法，适用于基于注意力的模型，可以使用非均匀量化将BERT中的几乎所有FP32权重压缩到3位而不损失精度。异常值以全精度保留以保护准确性，并通过高斯分布拟合自动检测，而其余权重则使用3位代码字存储，这些代码字索引一小组（在3位的情况下为8个）代表性的FP32质心。由于典型硬件无法直接对3位值进行操作（正如Q-BERT的作者在其研究中也指出的），GOBO的作者开发了一种新型硬件架构，能够高效加速3位计算，以补充他们的量化方法。尽管加速效益只有在使用专用硬件时才能完全实现，但3位量化模型减少的内存占用将减少更多通用硬件上的内存存储和流量，因此仍能在推理延迟和能耗方面提供一定程度的改进。GOBO方法的灵感来源于“深度压缩”中将霍夫曼编码权重存储在少量代表性值（质心）字典中的方式，但与此不同的是，它不需要微调，通过不量化权重异常值显著提高了准确性，并使用了一种新型的“质心选择算法，该算法收敛速度是K均值的9倍，并始终将所需质心的数量减少一半。”
- en: Zhang et al.’s 2020 [TernaryBERT](https://arxiv.org/abs/2009.12812) combined
    Knowledge Distillation (inspired by [TinyBERT](https://arxiv.org/abs/1909.10351))
    with ternary quantization in what they call *distillation-aware ternarization*,
    treating the ternarized model as a student model to the full-precision, equally-sized
    teacher. Their distillation method involves MSE minimization between teacher and
    student for the embedding layer, the outputs and attention scores of transformer
    layers, and the logits from the prediction layer with soft cross-entropy. TernaryBERT
    achieves comparable performance with the full-precision baseline, while being
    14.9x smaller. The authors find that min-max 8-bit quantization works best for
    activations in BERT, since they find the distributions in BERT activations to
    be skewed towards negative values, particularly in the early layers. Their QAT
    process initializes weights from the full-precision model, and uses the STE to
    approximate gradients through the quantization function.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 张等人于2020年发布的[TernaryBERT](https://arxiv.org/abs/2009.12812)将知识蒸馏（灵感来源于[TinyBERT](https://arxiv.org/abs/1909.10351)）与三值量化结合，提出了他们所称的*蒸馏感知三值化*，将三值化模型视为全精度、相同大小的教师模型的学生模型。他们的蒸馏方法包括在嵌入层、变换器层的输出和注意力得分、以及预测层的logits与软交叉熵之间的MSE最小化。TernaryBERT在性能上与全精度基准相当，但体积缩小了14.9倍。作者发现，最适合BERT中激活值的量化方法是min-max
    8位量化，因为他们发现BERT激活值的分布倾向于负值，特别是在早期层。他们的QAT过程从全精度模型初始化权重，并通过量化函数使用STE来近似梯度。
- en: In 2020, [BinaryBERT](https://arxiv.org/abs/2012.15701) sought to bring BERT
    quantization to the extreme, but observed that a binary BERT model is hard to
    train directly due to its highly irregular loss landscape. Thus, the authors opt
    instead to train a half-sized ternary network, convert it into a binary network
    trough their proposed *ternary weight splitting* method, then fine-tune the result
    to achieve 24x compression of BERT with a minimal performance drop. They extend
    this concept to include *adaptive splitting*, where the more important layers
    are split and less sensitive layers are described in binary, to allow flexibility
    for different model size constraints. Like most QAT methods, BinaryBERT trains
    using the STE.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在2020年，[BinaryBERT](https://arxiv.org/abs/2012.15701)尝试将BERT量化推向极限，但发现由于其高度不规则的损失函数，二值化BERT模型难以直接训练。因此，作者选择训练一个半大小的三值网络，通过他们提出的*三值权重分裂*方法将其转换为二值网络，然后对结果进行微调，以在性能下降最小的情况下实现BERT的24倍压缩。他们将这一概念扩展到包括*自适应分裂*，即将更重要的层分裂，而较不敏感的层则以二进制方式表示，从而为不同的模型大小约束提供灵活性。像大多数QAT方法一样，BinaryBERT使用STE进行训练。
- en: '![](../Images/da4e3b2496c0fe7dffc0a264185d4226.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da4e3b2496c0fe7dffc0a264185d4226.png)'
- en: Results from [BinaryBERT](https://arxiv.org/abs/2012.15701) paper gives comprehensive
    view of BERT compression techniques and their relative sizes, and shows the impressive
    representation power maintained by their binarization method. Keep in mind that
    GOBO is a PTQ method which is a considerable handicap.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '[BinaryBERT](https://arxiv.org/abs/2012.15701)论文的结果提供了关于BERT压缩技术及其相对大小的全面视角，并展示了他们的二值化方法所保持的令人印象深刻的表示能力。请记住，GOBO是一种PTQ方法，这在一定程度上是一个显著的障碍。'
- en: Post-Training Quantization (PTQ) of LLMs
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大型语言模型的后训练量化（PTQ）
- en: '![](../Images/5462a05b6bb2e0b7363dd619c6be0fcc.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5462a05b6bb2e0b7363dd619c6be0fcc.png)'
- en: For many reading this article, this will be the most relevant research section.
    Here, we review the PTQ of LLMs, which is presumably the only type of quantization
    that most of us can hope to use on these pretrained colossi, although we may yet
    be surprised. As we finally shift gears into LLM quantization, it is important
    to remember that because LLMs are so large, the definition of “full-precision”
    changes in this era to mean FP16, as this is the precision that these enormous
    models are typically trained in, since using FP32 would double storage requirements
    for minimal gains in performance. Therefore, when we discuss the compression rates
    of LLMs, 8-bit integer quantization only offers a 2x reduction in memory footprint
    compared with “full-precision” baselines. Additionally, and perhaps unsurprisingly,
    we’ll see that networks of this scale have properties which make their quantization
    more difficult, namely the emergence of rare but extremely important outliers
    in the activations. Thus, despite the fact that LLMs are every bit as overparameterized
    as the more easily compressed large CNNs, their idiosyncrasies make them more
    difficult to quantize. Fortunately, the research community was quick to diagnose
    the issues and prescribe formulas which unblock the efficient and accurate PTQ
    of LLMs.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多阅读本文的读者来说，这将是最相关的研究部分。在这里，我们回顾LLMs的PTQ，这可能是我们大多数人希望在这些预训练大模型上使用的唯一量化类型，尽管我们可能会有惊讶。随着我们最终转向LLM量化，重要的是要记住，由于LLM的规模如此庞大，“全精度”的定义在这一时代已经改变，意味着FP16，因为这些庞大的模型通常是在FP16精度下训练的，因为使用FP32会使存储需求加倍，而性能提升却微乎其微。因此，当我们讨论LLM的压缩率时，8位整数量化相比于“全精度”基准仅提供2倍的内存占用减少。此外，或许并不令人意外的是，我们会看到，这些规模的网络具有使其量化更困难的特性，即在激活中出现少数但极为重要的异常值。因此，尽管LLM和更容易压缩的大型CNN一样，都是过度参数化的，但它们的特性使得量化变得更加困难。幸运的是，研究界迅速诊断出了这些问题，并提出了可以解除LLM高效且准确的PTQ的公式。
- en: In mid-2022, two years after the 2020 release of the closed-source 175B GPT-3
    by OpenAI, the largest open-source models were GPT-J 6B and GPT-NeoX 20B, with
    the 175B-scale OPT and BLOOM models yet to be released. During this period, Yao
    et al. from Microsoft published [ZeroQuant](https://arxiv.org/abs/2206.01861),
    an early investigation into the quantization of GPT-3-style transformer models.
    Their method was able to achieve similar accuracy to the FP16 model with 5.2x
    better efficiency using INT8 quantization for both weights and activations. The
    authors additionally propose an efficient layer-by-layer knowledge distillation
    (LKD) approach to further compress the less sensitive weights to 4 bits without
    requiring training data, resulting in a 3x smaller memory footprint than the FP16
    model. To investigate the sharp drops in performance observed when applying PTQ
    to large transformer models, the authors examine the highly dynamic range in the
    activations (tokens) which occurs between their layers, and point out that applying
    a uniform quantization range scaled by the min/max layer-wise activation values
    will lead to suboptimal representation power in the dense regions of these distributions.
    Similarly, the weight matrices have long-tailed ranges which will cause the fine-grain
    details to be lost in a small number of uniformly-spaced quantization levels.
    The charts below depict these layer-wise characteristics, illustrating why slicing
    these ranges with uniform grids would lose a lot of information.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在2022年中期，即OpenAI于2020年发布闭源的175B GPT-3之后的两年，最大的开源模型是GPT-J 6B和GPT-NeoX 20B，而175B规模的OPT和BLOOM模型尚未发布。在此期间，微软的姚等人发表了[ZeroQuant](https://arxiv.org/abs/2206.01861)，对GPT-3风格的变换器模型量化进行了早期研究。他们的方法通过对权重和激活值使用INT8量化，能够在效率提高5.2倍的同时，达到与FP16模型相似的准确度。作者还提出了一种高效的逐层知识蒸馏（LKD）方法，通过不需要训练数据的方式，进一步将不那么敏感的权重压缩到4位，从而使内存占用比FP16模型小3倍。为了研究在将PTQ应用于大型变换器模型时观察到的性能急剧下降，作者考察了在其各层之间出现的激活（令牌）中的高度动态范围，并指出通过按最小/最大层级激活值缩放的均匀量化范围来应用，将导致在这些分布的密集区域内表现能力不佳。同样，权重矩阵具有长尾范围，这将导致在少数均匀分布的量化级别中丧失细粒度的细节。下图展示了这些逐层特性，说明为什么使用均匀网格切分这些范围会丢失大量信息。
- en: '![](../Images/84e6bd353692b8485b184f73e48db23b.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84e6bd353692b8485b184f73e48db23b.png)'
- en: Plots from [ZeroQuant paper](https://arxiv.org/abs/2206.01861) show the token
    activations and weight ranges in the attention output matrices across transformer
    layers in GPT-3 350M.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[ZeroQuant论文](https://arxiv.org/abs/2206.01861)的图表展示了GPT-3 350M中变换器层间注意力输出矩阵中的令牌激活和权重范围。
- en: 'To handle the variance in these ranges, the authors propose to use group-wise
    quantization of the weights, and token-wise quantization of the activations. They
    also point out that given the large variance in activation ranges for these large
    transformer models, calibrating the ranges offline with a calibration dataset
    in a *static quantization* setting is likely to cause problems, and so they opt
    instead to calibrate token-wise min/max ranges dynamically during inference. To
    overcome the data movement overhead introduced by token-wise quantization, the
    authors build an optimized inference backend using the kernel fusion technique,
    which fuses each quantization operator into its previous operator (e.g. layer
    normalization). The authors shape their LKD approach to address three limitations
    of knowledge distillation of very large models, namely: the need to store both
    models in memory, the need to fully train the student, and the need for original
    training data. To circumvent these issues, the authors optimize a single layer
    of the quantized model at a time, and use the same set of teacher activations
    to feed both the quantized layer and to generate the pseudo-label through the
    corresponding full-precision layer. Since only these layer-wise pseudo-labels
    are used, there is no need for labeled data for the LKD process, and the authors
    show that using the original training data is unnecessary. ZeroQuant is open-sourced
    as [part of the DeepSpeed library](https://github.com/microsoft/DeepSpeed/blob/548d37b1611e84448c5ec917f86080e65299e79d/docs/_tutorials/model-compression.md#12-weight-quantization),
    which is [integrated](https://huggingface.co/docs/transformers/en/main_classes/deepspeed)
    into Hugging Face’s [Transformers](https://github.com/huggingface/transformers)
    library.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理这些范围内的方差，作者提议使用权重的组别量化和激活的令牌量化。作者还指出，由于这些大型变换器模型的激活范围存在很大的方差，在*静态量化*设置下使用校准数据集对这些范围进行离线校准很可能会引发问题，因此他们选择在推理过程中动态校准令牌的最小/最大范围。为了克服令牌量化引入的数据移动开销，作者利用内核融合技术构建了优化的推理后端，将每个量化操作符与其前一个操作符（例如层归一化）融合在一起。作者将其LKD方法调整为解决大规模模型知识蒸馏的三个限制，即：需要在内存中存储两个模型，需要完全训练学生模型，以及需要原始训练数据。为了解决这些问题，作者一次优化量化模型的单层，并使用相同的教师激活来为量化层输入数据，并通过相应的全精度层生成伪标签。由于仅使用这些层级伪标签，因此LKD过程不需要标注数据，作者还展示了使用原始训练数据并非必需的。ZeroQuant作为[DeepSpeed库的一部分](https://github.com/microsoft/DeepSpeed/blob/548d37b1611e84448c5ec917f86080e65299e79d/docs/_tutorials/model-compression.md#12-weight-quantization)开源，[集成](https://huggingface.co/docs/transformers/en/main_classes/deepspeed)到Hugging
    Face的[Transformers](https://github.com/huggingface/transformers)库中。
- en: '![](../Images/127ea3b142b66437dbf919c1830d2e52.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/127ea3b142b66437dbf919c1830d2e52.png)'
- en: Results from ZeroQuant of GPT-3 350M show baseline performance is matched by
    the W8A8 configuration. For lower precisions, the LKD step greatly improves performance,
    but still does not recover baseline accuracy, even in the sub-billion parameter
    scale model shown here. Note the sharp drop in performance from 8 to 4 bits in
    the weights in all settings, even with the higher-precision activations, which
    shows us how difficult low-bit PTQ of LLMs was in mid-2022.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 350M的ZeroQuant结果表明，W8A8配置的基准性能是匹配的。对于较低的精度，LKD步骤显著提升了性能，但即使是在这里展示的数十亿参数规模的模型中，仍未恢复到基准精度。请注意，在所有设置中，从8位到4位的权重性能大幅下降，即使激活值采用更高精度，这也展示了2022年中期LLM的低比特PTQ的难度。
- en: 'In 2022, Tim Dettmers and his colleagues introduced [LLM.int8()](https://arxiv.org/abs/2208.07339),
    aka GPT3.int8(): an 8-bit integer PTQ method for ≥175B parameter scale LLMs that
    preserves their accuracy by using a two-part quantization procedure. First, they
    quantize the majority of features with vector-wise quantization granularity, using
    different normalization constants for each row/column inner product in the matrix
    multiplications. Second, the authors discover that the reason existing PTQ methods
    fail for transformers containing ≥6.7B parameters is the proliferation of extreme
    outliers in the transformer layers that occurs at these scales, which are extraordinarily
    sensitive to alteration and cripple model performance when removed. Thus, as the
    second part of their two-part procedure, the authors opt to handle these outliers
    separately using a *mixed-precision decomposition*, and demonstrate that this
    preserves 8-bit quantized model performance into the hundreds of billions of parameters.
    The code is open-sourced in the [bitsandbytes](https://github.com/TimDettmers/bitsandbytes)
    library, which is also integrated into the Hugging Face ecosystem.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在2022年，Tim Dettmers及其同事介绍了[LLM.int8()](https://arxiv.org/abs/2208.07339)，即GPT3.int8()：一种用于≥175B参数规模LLM的8位整数PTQ方法，通过采用双重量化过程来保持其准确性。首先，他们使用向量化量化粒度量化大多数特征，并为矩阵乘法中的每一行/列内积使用不同的归一化常数。其次，作者发现，现有的PTQ方法在处理包含≥6.7B参数的transformer时失败的原因是，在这些规模下，transformer层中的极端离群值会大量增加，这些离群值对变化极为敏感，并且在移除时会严重影响模型性能。因此，作为双重量化过程的第二部分，作者选择通过*混合精度分解*单独处理这些离群值，并证明这能够将8位量化模型的性能保持在数百亿参数规模。代码已在[bitsandbytes](https://github.com/TimDettmers/bitsandbytes)库中开源，并已集成到Hugging
    Face生态系统中。
- en: '![](../Images/99fdf5ff3397d97ba4da2c3556380e6a.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99fdf5ff3397d97ba4da2c3556380e6a.png)'
- en: Results from GPTQ (aka OPTQ) shows that their separate handling of systematic
    outliers preserves accuracy in transformers at ≥6.7B scale.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: GPTQ（即OPTQ）的结果表明，他们对系统性离群值的单独处理能够保持≥6.7B规模transformer的准确性。
- en: 'In 2022, Frantar et al. published [GPTQ](https://arxiv.org/abs/2210.17323)
    (also occasionally cited using the OpenReview draft title: [OPTQ](https://openreview.net/pdf?id=tcbBPnfwxS)),
    a seminal work in the PTQ of LLMs, which can preserve the accuracy of pretrained
    models with weights quantized to 3 or 4 bit precision without requiring any retraining.
    The authors sought to overcome the limitations of ZeroQuant and LLM.int8() that
    arise from the use of “basic variants of round-to-nearest quantization,” which
    is only effective down to 8 bits of precision. In contrast, GPTQ efficiently compresses
    LLMs with hundreds of billions of parameters in only a few GPU hours down to 3
    or 4 bit precision with minimal loss in accuracy. To achieve this, the authors
    quantize weights in the network one-by-one and use approximate second-order information
    to adjust the remaining unquantized weights at each step, compensating for the
    error introduced by quantizing the current weight. This process is repeated until
    every weight has been quantized, with quantized weights remaining frozen as the
    algorithm progresses through the network.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在2022年，Frantar等人发布了[GPTQ](https://arxiv.org/abs/2210.17323)（有时也使用OpenReview草稿标题[OPTQ](https://openreview.net/pdf?id=tcbBPnfwxS)引用），这是一项开创性的LLM
    PTQ工作，它能够在无需任何重新训练的情况下，保持预训练模型在权重量化为3位或4位精度时的准确性。作者旨在克服ZeroQuant和LLM.int8()在使用“基础的四舍五入量化变种”时出现的限制，这种方法仅在8位精度下有效。相比之下，GPTQ通过几小时的GPU计算，能够高效地将数百亿参数的LLM压缩到3位或4位精度，并且几乎不损失准确性。为了实现这一点，作者逐个量化网络中的权重，并使用近似的二阶信息来调整每一步中剩余的未量化权重，从而弥补量化当前权重时引入的误差。这个过程会一直重复，直到每个权重都被量化，而量化后的权重在算法进展时保持冻结状态。
- en: 'Since GPTQ only quantizes the weights, it does not offer efficiency gains in
    the multiplication operations, due to the mixed-precision interactions with the
    non-quantized activations. However, the reduced memory footprint is essential
    for reducing the size of the GPU hardware required for inference, and the authors
    were able to run the open-source GPT-3 equivalent: [OPT-175B](https://arxiv.org/abs/2205.01068)
    (generously provided by Meta AI in mid-2022), for the first time on a single A100
    GPU. While multiplications are not accelerated, the authors release bespoke GPU
    kernels which speed up inference roughly 4x on the Nvidia GPUs tested thanks to
    the reduced memory traffic and improved parallelism that results from moving the
    dequantization of weights downstream into the same GPU cores as their subsequent
    operations. GPTQ is [open-source](https://github.com/IST-DASLab/gptq) and has
    been widely adopted into several popular LLM deployment frameworks, including
    Hugging Face.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 由于GPTQ仅量化权重，因此在乘法操作中并未带来效率提升，这是因为与未量化的激活进行混合精度交互。然而，减少内存占用对于减小推理所需的GPU硬件规模至关重要，作者们能够首次在单个A100
    GPU上运行开源的GPT-3等效模型：[OPT-175B](https://arxiv.org/abs/2205.01068)（由Meta AI在2022年中期慷慨提供）。虽然乘法操作没有加速，但作者们发布了定制的GPU内核，通过减少内存流量和改进并行性，使推理速度在所测试的Nvidia
    GPU上大约加快了4倍，这得益于将权重的反量化下游转移到与后续操作相同的GPU核心上。GPTQ是[开源](https://github.com/IST-DASLab/gptq)的，已广泛应用于多个流行的LLM部署框架，包括Hugging
    Face。
- en: '![](../Images/4688696edc785b8c77eb3162694fa9ef.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4688696edc785b8c77eb3162694fa9ef.png)'
- en: Results from [GPTQ](https://arxiv.org/abs/2210.17323) show better consistency
    and performance than RTN (LLM.int8) in 4-bit and 3-bit quantization scenarios.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[GPTQ](https://arxiv.org/abs/2210.17323)的结果显示，在4位和3位量化场景下，GPTQ相比RTN（LLM.int8）展现了更好的一致性和性能。
- en: In a late-2022 collaboration between the Massachusetts Institute of Technology
    (MIT) and Nvidia, [SmoothQuant](https://arxiv.org/abs/2211.10438) demonstrated
    a method which was able to match the 8-bit PTQ performance of LLM.int8() in large
    models with fully quantized activations, circumventing the need for maintaining
    high-precision outliers by offloading their impact into the weights, which by
    contrast are much more uniform and easy to quantize. The diagram below provides
    an intuitive visual reference for this concept.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在2022年底，麻省理工学院（MIT）与英伟达（Nvidia）合作的研究中，[SmoothQuant](https://arxiv.org/abs/2211.10438)展示了一种方法，能够在大型模型中匹配LLM.int8()的8位PTQ性能，同时实现完全量化的激活，从而避免了通过将高精度离群值的影响转移到权重中来维持高精度离群值的需求。与此相比，权重更加均匀且更易量化。下图提供了这一概念的直观视觉参考。
- en: '![](../Images/0584e0504b55dcb665ee1b2688e873fd.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0584e0504b55dcb665ee1b2688e873fd.png)'
- en: Figure from [SmoothQuant](https://arxiv.org/abs/2211.10438) demonstrates the
    offloading of activation outliers into the weights in order to make them more
    amenable to quantization.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[SmoothQuant](https://arxiv.org/abs/2211.10438)的图示展示了将激活离群值转移到权重中的过程，从而使其更适合量化。
- en: The SmoothQuant authors suggest that token-wise quantization in LLMs offers
    little benefit, since the additional granularity does not align with the activation
    outliers emerging in LMMs. They demonstrate that tokens will have high internal
    variance when outliers are present, since these large values will occur only in
    a small number of their channels; however, the channels where the outliers do
    occur turn out to have low variance across tokens. In other words, the outliers
    have channel-wise regularity, and this means that that LLM accuracy can be mostly
    maintained simply by using channel-wise quantization instead. However, because
    channel-wise quantization is incompatible with hardware accelerated INT8 general
    matrix multiplication (GEMM) kernels, the authors instead exploit this predictable
    channel-wise manifestation of outliers to adjust their corresponding weights in
    such a way which achieves an equivalent mathematical result while reducing the
    outlier values and making the activation tensors more amenable to quantization.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: SmoothQuant的作者认为，LLM中的按标记量化几乎没有益处，因为额外的粒度与LLM中出现的激活离群值并不一致。他们证明，当存在离群值时，标记会有较大的内部方差，因为这些大值只会出现在少数几个通道中；然而，离群值出现的通道的方差却在标记间很低。换句话说，离群值具有通道级的规律性，这意味着LLM的准确度可以仅通过使用通道级量化来大致保持。然而，由于通道级量化与硬件加速的INT8通用矩阵乘法（GEMM）内核不兼容，作者们利用这种可预测的通道级离群值表现来调整相应的权重，从而实现等效的数学结果，同时减少离群值，使激活张量更适合量化。
- en: '![](../Images/b7cd1c97aae8df2d4ae1993533b6a056.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7cd1c97aae8df2d4ae1993533b6a056.png)'
- en: Diagram from [SmoothQuant](https://arxiv.org/abs/2211.10438) clearly demonstrates
    the channel-wise regularity of outliers in the activations, which can be absorbed
    into the weights.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[SmoothQuant](https://arxiv.org/abs/2211.10438)的图表清楚地展示了激活中异常值的通道规律，这些异常值可以被吸收到权重中。
- en: 'Three configurations of SmoothQuant are tested: O1-O3, which all use tensor-wise
    weight quantization, but differ in how they quantize the activations. O3 is the
    least complex, using per-tensor *static quantization* for the activations, where
    the scaling factor is calibrated offline. O2 also uses tensor-wise activation
    quantization, but gets a boost in performance by using *dynamic quantization,*
    which calibrates the scaling parameter online using runtime statistics. O1 explores
    the more complex token-wise dynamic quantization, but achieves only slightly better
    results than O2\. Note that in the SmoothQuant method, LayerNorm and SoftMax layers
    are still computed in FP16, so it is not integer-only, and the approach does not
    work below 8-bit. Nevertheless, it marks an important stride in overcoming the
    particularities of LLMs which made their quantization difficult.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 对SmoothQuant进行了三种配置的测试：O1-O3，它们都使用张量级权重量化，但在量化激活方式上有所不同。O3是最简单的，使用每个张量的*静态量化*来量化激活，其中缩放因子是离线校准的。O2也使用张量级激活量化，但通过使用*动态量化*来在线校准运行时统计数据，性能得到提升。O1探索更复杂的标记级动态量化，但仅比O2略好一些。请注意，在SmoothQuant方法中，LayerNorm和SoftMax层仍然以FP16计算，因此不仅限于整数，该方法在8位以下不起作用。尽管如此，它标志着克服使其量化困难的LLMs的特殊性的重要进展。
- en: '![](../Images/c7d050751c2893abf221441d82e8dda6.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7d050751c2893abf221441d82e8dda6.png)'
- en: Results from [SmoothQuant](https://arxiv.org/abs/2211.10438) demonstrate equal
    performance to LLM.int8() without the need for mixed-precision decomposition of
    the activation signals.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '[SmoothQuant](https://arxiv.org/abs/2211.10438)的结果表明，与LLM.int8()相比，无需对激活信号进行混合精度分解即可实现相同的性能。'
- en: In mid-2023, the authors of ZeroQuant returned with ZeroQuant-V2, wherein they
    proposed to reduce quantization error by increasing granularity using a Fine-Grained
    Quantization (FGQ) approach they call *block-k quantization,* where scaling factors
    and/or zero points are set for each sub-vector of length *k* in the weight matrix
    rows. Additionally, the authors employ Low-Rank Compensation (LoRC) to further
    recover model performance using small trainable low-rank matrices to offset the
    layer-wise quantization error.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在2023年中期，ZeroQuant的作者推出了ZeroQuant-V2，他们提出通过使用一种称为*块-k量化*的细粒度量化（FGQ）方法来增加粒度，从而减少量化误差，其中在权重矩阵行中为每个长度为*k*的子向量设置缩放因子和/或零点。此外，作者使用低秩补偿（LoRC）进一步恢复模型性能，使用小的可训练低秩矩阵来抵消逐层量化误差。
- en: '![](../Images/9726430d450237fb245c0834c14e504c.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9726430d450237fb245c0834c14e504c.png)'
- en: Results from ZeroQuant-V2 show perplexity scores of various PTQ approaches on
    LLMs. Note that the real divergence between ZeroQuant-V2 and GPTQ is seen in the
    W4A8 quantization of OPT-style models, which we now know contain sensitive outliers
    thanks to the SmoothQuant paper, and so it is likely that the improved performance
    of ZeroQuant-V2 over GPTQ is the ability for the block-wise granularity to better
    preserve the outliers occurring in the activation channels.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ZeroQuant-V2的结果显示了LLMs上各种PTQ方法的困惑度分数。请注意，ZeroQuant-V2与GPTQ之间的真正差异在于OPT风格模型的W4A8量化，我们现在知道由于SmoothQuant论文，这些模型包含敏感的异常值，因此ZeroQuant-V2相对于GPTQ的改进性能可能是块状粒度更好地保留激活通道中发生的异常值的能力。
- en: Lin et al.’s mid-2023 “Activation-aware Weight Quantization for LLM Compression
    and Acceleration” ([AWQ](https://arxiv.org/abs/2306.00978)) is a weight-only low-bit
    quantization method. The authors point out that while GPTQ is very effective,
    it has the potential to overfit to the calibration set, which is problematic for
    generalist models like LLMs. The authors point out that not all weights are equally
    important, and that not quantizing the 0.1–1% of highly salient network weights
    significantly reduces performance degradation due to quantization. To identify
    salient weights, the authors look at the activation signals rather than the weights
    themselves, and apply channel-wise scaling to minimize quantization error. The
    authors note that “AWQ does not rely on any backpropagation or reconstruction,
    so it can well preserve LLM’s generalization ability on various domains and modalities
    without overfitting to the calibration set.” The authors show that this generalization
    is important for maintaining performance in instruction-tuned LLMs like Vicuna
    and multimodal LMs (LMMs) like OpenFlamingo. Their method allows a Llama-2–70B
    model to fit on a 64GB Jetson Orin AGX, or 13B-scale models to run “at an interactive
    pace of 30 tokens per second” on the 8GB RTX-4070 GPU. The efficacy of AWQ has
    led to its wide adoption in popular open-source LLM serving software (more on
    that in upcoming the implementation guide).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 林等人于2023年中期提出的《针对LLM压缩与加速的激活感知权重量化》([AWQ](https://arxiv.org/abs/2306.00978))是一种仅基于权重的低位量化方法。作者指出，尽管GPTQ非常有效，但它可能会对校准集发生过拟合，这对于像LLM这样的通用模型来说是一个问题。作者强调，并非所有权重都同等重要，而不对0.1%到1%高度显著的网络权重进行量化，可以显著减少因量化引起的性能下降。为了识别显著权重，作者着重于激活信号而非权重本身，并采用按通道缩放的方法来最小化量化误差。作者指出，“AWQ不依赖于任何反向传播或重建，因此能够在不对校准集过拟合的情况下，良好地保持LLM在各种领域和模态上的泛化能力。”作者展示了这种泛化对于保持像Vicuna这样的指令调优LLM和像OpenFlamingo这样的多模态语言模型（LMM）的性能至关重要。他们的方法使得Llama-2–70B模型可以在64GB
    Jetson Orin AGX上运行，或者使13B规模的模型在8GB RTX-4070 GPU上以“每秒30个token的交互速度”运行。AWQ的有效性使其在流行的开源LLM服务软件中得到了广泛应用（更多细节将在即将发布的实现指南中介绍）。
- en: '![](../Images/adf8963d2b116b320afdb3713c4c7e8f.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/adf8963d2b116b320afdb3713c4c7e8f.png)'
- en: Results from [AWQ](https://arxiv.org/abs/2306.00978) showing improved perplexity
    scores over RTN (LLM.int8) and GPTQ.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[AWQ](https://arxiv.org/abs/2306.00978)的结果，显示其在困惑度评分上优于RTN（LLM.int8）和GPTQ。
- en: In mid-2023, [SqueezeLLM](https://arxiv.org/abs/2306.07629) exceeded the performance
    of GPTQ and AWQ. The authors propose a *sensitivity-based non-uniform quantization*,
    which uses second-order information to search for the optimum bit precision assignment,
    and a Dense-and-Sparse decomposition, which “stores outliers and sensitive weights
    values in an efficient sparse format.” In a 3-bit setting, SqueezeLLM cuts the
    rise in perplexity compared with other state-of-the art PTQ methods by half. Further,
    the quantized models offer 2.3x faster inference than the FP16 baseline. As we’ve
    seen, most methods use uniform quantization controlled by parameters at various
    granularities, motivated by the simplistic application that it offers. However,
    as we’ve seen, and as the SqueezeLLM authors point out, “the weight distributions
    in LLMs exhibit clear non-uniform patterns,” signaling that non-uniform quantization
    will naturally provide better representations. Moreover, the authors argue that
    since most works only quantize weights and perform arithmetic in FP16, the advantage
    of uniform quantization is not fully realized anyhow, and that non-uniform quantization
    is preferable for its lower impact on outliers, which preserves representation
    power at lower precision. Further, the authors decompose and isolate the weight
    outliers in efficient sparse representations, thereby making the distributions
    more amenable to quantization. Code for SqueezeLLM is [available on GitHub](https://github.com/SqueezeAILab/SqueezeLLM).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在2023年中，[SqueezeLLM](https://arxiv.org/abs/2306.07629)超越了GPTQ和AWQ的性能。作者提出了一种*基于灵敏度的非均匀量化*方法，利用二阶信息来搜索最佳位精度分配，并提出了一个密集-稀疏分解方法，该方法“以高效的稀疏格式存储异常值和敏感的权重值”。在3位设置下，SqueezeLLM将与其他最先进的PTQ方法相比的困惑度上升降低了一半。此外，量化模型的推理速度比FP16基准快2.3倍。如我们所见，大多数方法使用由不同粒度参数控制的均匀量化，这是由于它提供的简化应用。然而，正如我们所看到的，以及SqueezeLLM的作者所指出的，“LLM中的权重分布表现出明显的非均匀模式”，这表明非均匀量化自然会提供更好的表示。此外，作者认为，由于大多数工作只对权重进行量化并在FP16中执行算术运算，因此均匀量化的优势并未完全实现，而且非均匀量化更可取，因为它对异常值的影响较小，从而在较低精度下保留了表示能力。此外，作者将权重异常值分解并隔离在高效的稀疏表示中，从而使得这些分布更适合量化。SqueezeLLM的代码可以在[GitHub上找到](https://github.com/SqueezeAILab/SqueezeLLM)。
- en: '![](../Images/8cd2393e0bbf51a8d9b70c29cf046582.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8cd2393e0bbf51a8d9b70c29cf046582.png)'
- en: Chart from [SqueezeLLM](https://arxiv.org/abs/2306.07629) compares their quantized
    models to FP16 models of the same size, showing that for an equivalent memory
    footprint, quantized models provide better significantly better performance. This
    figure is compelling in the undisputable benefits of network quantization.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[SqueezeLLM](https://arxiv.org/abs/2306.07629)的图表比较了它们的量化模型与相同大小的FP16模型，结果显示在相同的内存占用下，量化模型提供了显著更好的性能。这一图表有力地展示了网络量化的不可争议的优势。
- en: '![](../Images/1ab4595db245f9ab2a0d88c2c3418a2d.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ab4595db245f9ab2a0d88c2c3418a2d.png)'
- en: Results from [SqueezeLLM](https://arxiv.org/abs/2306.07629) show roughly equivalent
    instruction-tuning results to AWQ under given bit constraints.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[SqueezeLLM](https://arxiv.org/abs/2306.07629)的结果表明，在给定的位数限制下，指令调优的结果与AWQ大致相当。
- en: In late 2023, Mobius Labs GmbH open-sourced an extraordinarily efficient and
    accurate zero-shot PTQ approach which works well down to very low precision, called
    Half-Quadratic Quantization ([HQQ](https://mobiusml.github.io/hqq_blog/)). This
    method is able to quantize the large LLaMA-2–70B model 50x faster than GPTQ, taking
    only 5 minutes on the GPU tested. The authors point out that the limitation of
    GPTQ and AWQ is the need for calibration data for minimizing error between the
    layer outputs, which risks overfitting to the calibration set and requires compute
    time and resources. To sidestep this, HQQ minimizes quantization error in the
    weights rather than activations, but does so in a way which more accurately captures
    the impact of the outliers by using a sparsity-encouraging loss function which
    they decompose into two solvable sub-problems using a [*Half-Quadratic solver*](https://ieeexplore.ieee.org/document/120331),
    iteratively reaching an optimal solution for the group-*k* quantization parameters
    in closed-form, without the need for data, by performing alternative optimization
    of the two sub-problems. Compared with the popular data-free LLM.int8() method
    from bitsandbytes, HQQ consistently produces models with lower perplexity. Further,
    while HQQ is 50x faster to perform than GPTQ and around 25x faster than AWQ, the
    results are better than or equivalent to these data-dependent methods, particularly
    when using less than 4 bits of precision. The HQQ [code is open-sourced](https://github.com/mobiusml/hqq),
    and some pre-quantized models are [available on the Hugging Face hub](https://huggingface.co/mobiuslabsgmbh).
    While using HQQ models within the Hugging Face ecosystem is straightforward, the
    HQQ integration in the optimized [vLLM](https://github.com/vllm-project/vllm)
    inference engine is still “experimental” at the time of this writing.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 2023 年末，Mobius Labs GmbH 开源了一种极为高效且精确的零-shot PTQ 方法，称为半二次量化（[HQQ](https://mobiusml.github.io/hqq_blog/)）。该方法能够将大型
    LLaMA-2–70B 模型的量化速度提高 50 倍，比 GPTQ 快，且在测试的 GPU 上仅需 5 分钟。作者指出，GPTQ 和 AWQ 的限制在于需要校准数据，以最小化层输出之间的误差，这可能导致过拟合校准集，并且需要大量的计算时间和资源。为了解决这个问题，HQQ
    通过最小化权重而非激活的量化误差，采用了一种更准确捕捉异常值影响的方式，使用了一个鼓励稀疏性的损失函数，并通过一个[*半二次求解器*](https://ieeexplore.ieee.org/document/120331)将其分解为两个可解的子问题，迭代地达到关于组-*k*
    量化参数的闭式最优解，无需数据，方法通过交替优化两个子问题来实现。与流行的、无数据的 LLM.int8() 方法（来自 bitsandbytes）相比，HQQ
    始终生成具有较低困惑度的模型。此外，尽管 HQQ 的执行速度比 GPTQ 快 50 倍，比 AWQ 快约 25 倍，但其结果优于或与这些依赖数据的方法相当，尤其是在精度小于
    4 位的情况下。HQQ 的[代码已开源](https://github.com/mobiusml/hqq)，一些预量化的模型也在[Hugging Face
    hub](https://huggingface.co/mobiuslabsgmbh)上提供。虽然在 Hugging Face 生态系统中使用 HQQ 模型非常简单，但
    HQQ 在优化过的[vLLM](https://github.com/vllm-project/vllm)推理引擎中的集成在写作时仍然是“实验性”的。
- en: '![](../Images/e191338797e8904e9e87bc77804f6c74.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e191338797e8904e9e87bc77804f6c74.png)'
- en: Compute time of [HQQ](https://mobiusml.github.io/hqq_blog/) compared with GPTQ
    and AWQ on the large LLaMA-2–70B model.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 计算在大型 LLaMA-2–70B 模型上，[HQQ](https://mobiusml.github.io/hqq_blog/) 与 GPTQ 和 AWQ
    的时间对比。
- en: '![](../Images/4cf01ad30e6ca78fd480557325873055.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4cf01ad30e6ca78fd480557325873055.png)'
- en: Chart from [HQQ](https://mobiusml.github.io/hqq_blog/) shows that it provides
    lower or equal perplexity at given memory budgets to other state-of-the-art approaches,
    while being much faster. BNB refers to bitsandbytes, aka the data-free LLM.int8()
    method.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 [HQQ](https://mobiusml.github.io/hqq_blog/) 的图表显示，HQQ 在给定内存预算下，比其他最先进的方法提供了更低或相等的困惑度，同时速度更快。BNB
    指的是 bitsandbytes，即无数据的 LLM.int8() 方法。
- en: In December of 2023, [SmoothQuant+](https://arxiv.org/abs/2312.03788) set a
    new state-of-the-art in 4-bit uniform PTQ. The authors address the limitations
    of AWQ, which has a search space that scales with the number of layers, and does
    not take the problem of error accumulation “into account during the searching
    process, which makes the search speed slow and the model accuracy degraded.” Further,
    they assert that the weight quantization loss is amplified by the activation outliers,
    and smoothing their values with weight adjustments can therefore greatly reduce
    overall quantization error. The authors provide custom W4A16 kernels for the popular
    vLLM inference engine, and design the SmoothQuant+ implementation to work without
    requiring a preprocessing step from the user, allowing them to load FP16 models
    directly from the Hugging Face hub, with the quantization process applied automatically
    as the model is moved onto the GPU. Unfortunately, as of the time of this writing,
    the SmoothQuant+ algorithm code has yet to land on [the public GitHub page](https://github.com/adlik/smoothquant+),
    and the integration into vLLM is not yet online.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年12月，[SmoothQuant+](https://arxiv.org/abs/2312.03788)在4位均匀PTQ领域设立了新的最先进技术。作者们解决了AWQ的局限性，AWQ的搜索空间随着层数的增加而扩展，并且在搜索过程中没有考虑到误差积累问题，这使得搜索速度较慢，且模型的准确性下降。进一步地，他们指出，权重量化误差会被激活值的异常值放大，因此通过权重调整平滑这些异常值，可以大大减少整体的量化误差。作者提供了适用于流行的vLLM推理引擎的自定义W4A16内核，并设计了SmoothQuant+的实现，使得用户无需预处理步骤，即可直接从Hugging
    Face hub加载FP16模型，在将模型移到GPU时自动应用量化过程。不幸的是，截至目前，SmoothQuant+算法代码尚未发布到[公开的GitHub页面](https://github.com/adlik/smoothquant+)，并且其在vLLM中的集成也尚未上线。
- en: '![](../Images/2d22156bac4cd17642bdd92d59f77165.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d22156bac4cd17642bdd92d59f77165.png)'
- en: Results from [SmoothQuant+](https://arxiv.org/abs/2312.03788) show improved
    performance over RTN and AWQ, exceeding the FP16 baseline in larger models.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '[SmoothQuant+](https://arxiv.org/abs/2312.03788)的结果显示，在大模型中，其性能优于RTN和AWQ，并超越了FP16基准。'
- en: In April 2024, Park et al. published the latest version of their [LUT-GEMM](https://arxiv.org/abs/2206.09557)
    paper. In this work, the authors point out that weight-only quantization typically
    enables higher compression rates at a given level of accuracy compared with quantizing
    both weights and activations, since memory traffic is highly dominated by the
    weights, and the activations are much more sensitive to quantization. However,
    as we’ve discussed, this comes at the cost of losing integer-only benefits and
    requires dequantizing the weights before multiplication. To address this, LUT-GEMM
    is a kernel designed to enable the matrix multiplication directly between quantized
    weights and unquantized FP16 activations without requiring a dequantization step.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 2024年4月，Park等人发布了他们最新版本的[LUT-GEMM](https://arxiv.org/abs/2206.09557)论文。在这项工作中，作者指出，与同时量化权重和激活值相比，仅量化权重通常能够在给定准确度下实现更高的压缩率，因为内存流量主要由权重主导，而激活值对量化更为敏感。然而，正如我们所讨论的，这也会失去仅整数的优势，并且需要在乘法前对权重进行去量化。为了解决这个问题，LUT-GEMM是一个内核，旨在实现量化权重与未量化的FP16激活值之间的直接矩阵乘法，而无需去量化步骤。
- en: '![](../Images/0045b8e8f419d9a96749878eea98462b.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0045b8e8f419d9a96749878eea98462b.png)'
- en: Chart from [LUT-GEMM](https://arxiv.org/abs/2206.09557) shows the benefit of
    avoiding the costly dequantization step by using the LUT system.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[LUT-GEMM](https://arxiv.org/abs/2206.09557)的图表显示，通过使用LUT系统避免高成本的去量化步骤带来的好处。
- en: LUT-GEMM expands on the binary-coding quantization (BCQ) format used in XNOR-Net,
    which enables the use of simple arithmetic operations. While BCQ was originally
    designed for non-uniform quantization, the authors demonstrate that it generalizes
    to uniform quantization as well through the addition of a bias term, which greatly
    increases representational capacity. The authors construct a LUT of potential
    sub-vector products, as there are a limited number of possibilities, and then
    index them using the sub-vectors encountered during run-time rather than carrying
    out the operations that generate them, greatly improving efficiency. Using their
    quantization scheme, they demonstrate a 2.1x inference speedup over GPTQ on a
    single GPU. LUT-GEMM is the default quantization scheme used by the powerful [MLC-LLM](https://llm.mlc.ai/)
    inference engine, which we will learn more about later.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: LUT-GEMM扩展了XNOR-Net中使用的二进制编码量化（BCQ）格式，从而实现了简单的算术运算。尽管BCQ最初是为非均匀量化设计的，作者通过添加偏置项证明它同样适用于均匀量化，这大大增加了表示能力。作者构建了一个潜在子向量乘积的查找表（LUT），因为可能的组合数量有限，然后通过在运行时使用遇到的子向量来索引这些子向量，而不是执行生成它们的操作，从而大幅提高了效率。使用他们的量化方案，他们在单个GPU上展示了比GPTQ快2.1倍的推理加速。LUT-GEMM是强大的[MLC-LLM](https://llm.mlc.ai/)推理引擎所使用的默认量化方案，稍后我们将进一步了解它。
- en: '![](../Images/1bc692d04df6c5b49e70dc2794e4eca9.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1bc692d04df6c5b49e70dc2794e4eca9.png)'
- en: Results from [LUT-GEMM](https://arxiv.org/abs/2206.09557) show that their kernel
    and quantization method outperform AWQ and GPTQ in terms of inference latency.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[LUT-GEMM](https://arxiv.org/abs/2206.09557)的结果显示，他们的内核和量化方法在推理延迟方面优于AWQ和GPTQ。
- en: In this section, we have seen the legacy of PTQ come to fruition in the era
    of LLMs. These gargantuan models are extremely cost prohibitive to train, and
    thus necessitate the innovation of efficient one-shot or zero-shot quantization
    approaches like never before. Accordingly, researchers have provided extremely
    effective approaches to the open-source community, with bitsandbytes (LLM.int8),
    GPTQ, and AWQ becoming particularly popular, and the recently released HQQ offering
    a powerful new data-free alternative. In the implementation guide, we will weigh
    the comparative pros and cons for choosing between these quantization algorithms.
    But first, let’s complete our research journey by daring to explore the QAT of
    LLMs, since we know that QAT achieves superior quantization outcomes by its very
    nature, and perhaps we’ll be pleasantly surprised at how accessible some of these
    methods actually are.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们见证了PTQ的遗产在LLM时代的实现。这些庞大的模型训练成本极为昂贵，因此前所未有地需要创新高效的一次性或零-shot量化方法。因此，研究人员向开源社区提供了极为有效的方法，bitsandbytes（LLM.int8）、GPTQ和AWQ变得特别流行，最近发布的HQQ提供了一种强大的数据无关替代方案。在实现指南中，我们将权衡这些量化算法之间的比较优缺点。但首先，让我们通过勇敢探索LLM的QAT来完成我们的研究之旅，因为我们知道QAT本质上能实现更优的量化效果，也许我们会惊讶于这些方法的可接近性。
- en: Quantization-Aware Training (QAT) of LLMs
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM的量化感知训练（QAT）
- en: '![](../Images/3ff3effb6c85428a789613bcbed34194.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ff3effb6c85428a789613bcbed34194.png)'
- en: Image by author using DALL-E 3.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用DALL-E 3生成的图像。
- en: In our final research section, we review the intimidating domain of QAT methods
    for LLMs. LLaMA-2 is said to have cost north of $20 million to train, so should
    we even involve ourselves here? While some of the approaches here may not be accessible
    to us as individuals, they are important to understand for a broader understanding
    of what is possible. One question that arises is, if QAT is the most effective
    way of generating efficient, low-bit models without affecting their performance
    by incorporating quantization into the training process, then why shouldn’t the
    companies spending tens of millions of dollars to train open-source foundation
    models use these techniques as a component in their release strategy? What proportion
    of LLaMA-2 users are ever using full-precision weights, and wouldn’t it be better
    to make large models more amenable for quantization, since we know that the vast
    majority of deployments will be using it? This section may help us develop a more
    educated opinion on these matters.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的最后研究部分，我们回顾了针对LLMs的QAT方法这一令人生畏的领域。有人说，训练LLaMA-2的费用超过了2000万美元，那么我们是否应该涉足这一领域？尽管这里的一些方法对于我们个人可能无法接触，但它们对于更广泛地理解可能的技术仍然是非常重要的。一个问题是，如果QAT是生成高效、低位数模型的最有效方法，并且通过将量化纳入训练过程中不会影响模型的性能，那么为什么那些花费数千万美元训练开源基础模型的公司不应该将这些技术作为发布策略的一部分呢？有多少LLaMA-2的用户在使用全精度权重？既然我们知道绝大多数部署都将使用量化技术，难道不应该让大模型更适合量化吗？这一部分可能有助于我们在这些问题上形成更有见地的看法。
- en: 'Liu et al.’s mid-2023 “[LLM-QAT](https://arxiv.org/abs/2305.17888)” was the
    first exploration of QAT in LLMs. The authors pointed out that the current state-of-the-art
    PTQ approaches did not protect accuracy below 8 bits of precision, and that while
    QAT is probably necessary to preserve performance at lower precisions, no previous
    work had sought to investigate QAT of LLMs for the reasons we already know: the
    implied necessity of huge amounts of data and compute, as well as some more nuanced
    reasons, such as the difficulty of replicating the training processes of instruction-tuned
    LLMs. To circumvent these challenges, the authors introduce a *data-free knowledge
    distillation* method that uses generations from the pretrained model to serve
    as the training data, which provides better preservation of the output distribution,
    even when compared to training with large subsets of the original training data.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 刘等人在2023年中期的“[LLM-QAT](https://arxiv.org/abs/2305.17888)”是首个针对LLM的QAT探索。作者指出，目前最先进的PTQ方法在精度低于8位时未能保护准确性，而且尽管QAT可能是保持低精度下性能的必要方法，但之前的研究没有探讨LLM的QAT，原因我们已经了解：需要大量的数据和计算资源，以及一些更微妙的原因，例如，复制指令调优LLM的训练过程的难度。为了绕过这些挑战，作者提出了一种*无数据知识蒸馏*方法，利用预训练模型生成的样本作为训练数据，这比仅使用原始训练数据的大部分子集训练要更好地保留输出分布。
- en: '![](../Images/d4255652644f48733a6891840831a8a8.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d4255652644f48733a6891840831a8a8.png)'
- en: Diagram from [LLM-QAT](https://arxiv.org/abs/2305.17888) provides an intuitive
    visual reference of weight, activation, and KV quantization in transformer layers.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[LLM-QAT](https://arxiv.org/abs/2305.17888)的图表提供了一个直观的视觉参考，展示了变换器层中权重、激活和KV的量化。
- en: The LLM-QAT authors find that even a relatively small set of 100k synthetic
    samples is sufficient to distill quantized models, which can be done with reasonable
    amounts of compute. They find that stochastic sampling the most probable tokens
    at each generation step is essential to add noise into the data. Unlike most previous
    works in transformer quantization, LLM-QAT investigates quantization of the intermediary
    activation vectors stored in the KV cache, which is particularly beneficial with
    long sequences. The fact that the authors limit their investigation to LLaMA models
    ≤30B parameters echoes the underlying expense of QAT; that being said, the authors
    benchmark a plethora of mixed-precision configurations in their study, showing
    that their method is able to preserve LLM performance significantly better than
    GPTQ and SmoothQuant in low-bit settings. However, at higher precisions, the results
    are mostly equivalent, which indicates that the PTQ approaches are more favorable
    when higher precision is tolerable, due to their simplicity. Nevertheless, the
    ability of LLM-QAT to efficiently retrain LLMs to low-bit settings without training
    data and preserving performance is very exciting, particularly as LLMs become
    more parameter efficient. The code is [available on GitHub](https://github.com/facebookresearch/LLM-QAT).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: LLM-QAT的作者发现，即使是一个相对较小的10万个合成样本集，也足以提炼量化模型，这可以通过合理的计算资源完成。他们发现，在每次生成步骤中随机采样最可能的标记对于给数据添加噪声至关重要。与大多数之前的变换器量化研究不同，LLM-QAT研究了存储在KV缓存中的中间激活向量的量化，尤其在长序列中尤为有利。作者将他们的研究限制在LLaMA模型≤30B参数，这反映了QAT的基础开销；话虽如此，作者在他们的研究中基准了多种混合精度配置，显示他们的方法在低比特设置下能够显著优于GPTQ和SmoothQuant，保持LLM的性能。然而，在较高精度下，结果大致相当，这表明，当较高精度是可容忍的时，PTQ方法更具优势，因为它们更简单。尽管如此，LLM-QAT能够高效地在没有训练数据的情况下重新训练LLM以适应低比特设置并保持性能，这一点非常令人兴奋，尤其是在LLM变得更加参数高效的背景下。该代码[可以在GitHub上找到](https://github.com/facebookresearch/LLM-QAT)。
- en: '![](../Images/abd8d25e28ef4c5f38b61c87f4f0ed87.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/abd8d25e28ef4c5f38b61c87f4f0ed87.png)'
- en: Results from [LLM-QAT](https://arxiv.org/abs/2305.17888) show the dominance
    of their approach in low-bit settings. Bit values shown in W-A-KV order. Perplexity
    is considered a stringent metric.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '[LLM-QAT](https://arxiv.org/abs/2305.17888)的结果显示，在低比特设置下，他们的方法具有优势。比特值按W-A-KV顺序显示。困惑度被认为是一个严格的评估指标。'
- en: In 2023, Xi et al.’s concisely titled “[Training transformers with 4-bit integers](https://arxiv.org/abs//2306.11987)”
    introduced a method to train transformer models with all multiplications carried
    out in 4-bit integer arithmetic. To do this, the authors handle the important
    outliers in the activations during the forward pass using their proposed *Hadamard
    quantizer*, which quantizes a block diagonal Hadamard matrix transformation of
    each activation matrix. This transformation is beneficial because it spreads the
    outlier information into nearby matrix entries, reducing the their numerical range,
    and making the matrix amenable to quantization. During backpropagation, the authors
    propose to save the calculations by not computing small gradient rows in the activations,
    and use the saved computation to split the most informative gradients into a lower
    4 bits and higher 4 bits, preserving their detail using two rows which together
    create an 8-bit representation. Using this approach, the authors achieve accurate
    training of transformers using all-4-bit arithmetic without requiring custom numerical
    formats like FP4, meaning that the gains can be realized on contemporary hardware.
    The authors demonstrate their INT4 matrix multiplication operator is 2.2x faster
    than FP16 training, and reduces total training time by 35.1%, while coming close
    to matching baseline performance across a variety of tasks and transformer models.
    While this study is focused on BERT and Vision Transformer (ViT) rather than LLMs,
    it’s appropriate to cite in this section, as it marks a major landmark in low-precision
    training of large transformer models, showing us what is possible. Further, it
    is surely only a matter of time before this type of technique it is applied to
    LLMs. The authors have released their code [on GitHub](https://github.com/xijiu9/Train_Transformers_with_INT4).
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在2023年，Xi 等人简明扼要地发布了题为“[使用4位整数训练变压器](https://arxiv.org/abs//2306.11987)”的方法，提出了一种通过将所有乘法操作都在4位整数算术中进行来训练变压器模型的方案。为实现这一目标，作者在前向传播过程中通过他们提出的*Hadamard量化器*来处理激活中的重要异常值，该量化器对每个激活矩阵进行Hadamard矩阵变换的块对角化量化。此变换之所以有效，是因为它将异常值信息分散到相邻的矩阵条目中，从而减少了其数值范围，使矩阵更适合量化。在反向传播过程中，作者提出通过不计算激活中的小梯度行来节省计算，并利用节省下来的计算将最有信息量的梯度分为低4位和高4位，利用两行共同保留细节，创建一个8位表示。通过这种方法，作者实现了使用全4位算术的精确变压器训练，而无需像FP4这样的自定义数值格式，这意味着该方法可以在当代硬件上实现。作者展示了他们的INT4矩阵乘法操作比FP16训练快2.2倍，并减少了35.1%的总训练时间，同时在各种任务和变压器模型上接近基线性能。虽然这项研究集中于BERT和视觉变压器（ViT），而非大型语言模型（LLMs），但它在低精度训练大规模变压器模型领域标志着一个重要的里程碑，展示了这一技术的潜力。此外，毫无疑问，这种技术应用于LLMs只是时间问题。作者已在[GitHub](https://github.com/xijiu9/Train_Transformers_with_INT4)发布了他们的代码。
- en: Famously in 2023, Tim Dettmers and his colleagues at University of Washington
    broke new ground with [QLoRA](https://arxiv.org/abs/2305.14314), which uses the
    efficient [Low-Rank Adaptation](https://arxiv.org/abs/2106.09685) fine-tuning
    technique to backpropagate gradients from a frozen, quantized 4-bit LLM through
    the trainable low-rank matrices to fine-tune a 65B model with only 24 hours of
    training a single 48GB GPU, while maintaining baseline FP16 performance. QLoRA
    substantially reduces the amount of hardware needed to fine-tune LLMs, marking
    a substantial landmark in the democratization of this technology. To achieve this,
    the authors find that using a novel 4-bit NormalFloat (NF4) data type, which is
    theoretically optimal for quantizing normal distributions, yields better empirical
    results than either INT4 or FP4 types. The authors also investigate the use of
    *double quantization*, in which the quantization parameters are also quantized,
    saving an average of 0.37 bits per parameter (3GB in a 65B model). Highlighting
    the efficiency of their approach, the authors exhaustively test QLoRA on over
    1000 models using several instruction tuning datasets. The authors use their QLoRA
    method to train a family of [Guanaco](https://huggingface.co/timdettmers/guanaco-65b)
    models. A notable advantage of using any variant of LoRA is that the pretrained
    model remains unchanged, with the fine-tuning fully captured in the low-rank adapter
    matrices. This means several sets of these adaptors can be trained for multiple
    use-cases without full model retraining. [Fine-tuning with QLoRA is possible through
    Hugging Face Transformers](https://huggingface.co/blog/4bit-transformers-bitsandbytes)
    via the [bitsandbytes](https://huggingface.co/docs/bitsandbytes/main/en/index)
    library, and the [original QLoRA repo](https://github.com/artidoro/qlora) is available
    on GitHub.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年，Tim Dettmers及其华盛顿大学的同事们通过[QLoRA](https://arxiv.org/abs/2305.14314)开创了新的里程碑，该技术使用高效的[低秩适应](https://arxiv.org/abs/2106.09685)微调方法，将冻结的4位量化大语言模型（LLM）中的梯度反向传播，通过可训练的低秩矩阵进行微调，只需在单个48GB
    GPU上训练24小时，即可微调一个65B模型，同时保持基线FP16性能。QLoRA大幅减少了微调LLM所需的硬件，为这一技术的普及奠定了重要基础。为实现这一目标，作者发现使用一种新型的4位NormalFloat（NF4）数据类型，其理论上最适合量化正态分布，相较于INT4或FP4类型，能够提供更好的实际效果。作者还探讨了*双重量化*的应用，在这种方法中，量化参数本身也被量化，平均每个参数节省0.37位（在65B模型中节省3GB内存）。为了突出其方法的高效性，作者对超过1000个模型进行了广泛的测试，使用了多个指令调优数据集。作者利用QLoRA方法训练了一系列[Guanaco](https://huggingface.co/timdettmers/guanaco-65b)模型。使用任何LoRA变体的一个显著优势是，预训练的模型保持不变，微调的内容完全体现在低秩适配器矩阵中。这意味着可以为多个应用场景训练多组适配器，而无需进行完整的模型重训。[通过Hugging
    Face Transformers进行QLoRA微调](https://huggingface.co/blog/4bit-transformers-bitsandbytes)，可以使用[bitsandbytes](https://huggingface.co/docs/bitsandbytes/main/en/index)库，且[原始QLoRA仓库](https://github.com/artidoro/qlora)已在GitHub上发布。
- en: '![](../Images/a82bae8da1a301165c03254db841cea9.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a82bae8da1a301165c03254db841cea9.png)'
- en: Table from [QLoRA](https://arxiv.org/abs/2305.14314) paper demonstrates that
    the performance lost due to quantization can be fully recovered by applying QLoRA
    fine-tuning on 4-bit quantized models.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[QLoRA](https://arxiv.org/abs/2305.14314)论文的表格展示了，通过对4位量化模型应用QLoRA微调，量化造成的性能损失可以完全恢复。
- en: Extreme Quantization of LLMs
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM的极限量化
- en: Recently, the allure of extreme quantization has made its way into LLM research.
    As we saw earlier in the section on CNN quantization, neural networks can perform
    surprisingly well with binary or ternary weights with dramatically reduced memory
    traffic and computational complexity, as the weights occupy far fewer bits and
    the multiplication operations can be replaced with additions and subtractions.
    While extreme quantization of LLMs is a still a nascent field, recent work has
    demonstrated compelling results which will surely inspire the research community
    to develop this methodology into maturity. In this section, we will see the sparks
    of a dawning era of low-bit LLM training.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，极限量化的魅力已进入LLM研究领域。正如我们在卷积神经网络（CNN）量化部分所看到的那样，神经网络在使用二进制或三进制权重时表现出惊人的效果，显著减少了内存流量和计算复杂度，因为权重占用的位数大大减少，乘法运算可以被加法和减法替代。虽然LLM的极限量化仍处于初期阶段，但最近的研究成果展示了令人信服的结果，必将激励研究界将这种方法发展成熟。在本节中，我们将看到低比特LLM训练的曙光。
- en: In 2023, the authors of [BitNet](https://arxiv.org/abs/2310.11453) introduced
    a 1-bit transformer architecture that was suitable for use in LLMs. They introduce
    a `BitLinear` layer which can be swapped in for the `nn.Linear` layers, and train
    binary weights from scratch to achieve competitive performance with full-precision
    baselines, finding that the resulting transformers also exhibit a similar scaling
    law to their full-precision counterparts, while using operations that require
    far less power. Activations are quantized to 8 bits, and the optimizer states
    and gradients are still carried out in full-precision. The authors admit that
    since their implementation performs operations in FP16/BF16, there is no actual
    training acceleration, but they propose that low-precision FP8 GEMM CUDA kernels
    could be used to accelerate the forward and backwards passes, although they leave
    this for future work.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在2023年，[BitNet](https://arxiv.org/abs/2310.11453)的作者提出了一种适用于LLM的1位变压器架构。他们引入了一个`BitLinear`层，可以替换`nn.Linear`层，并从头开始训练二进制权重，以实现与全精度基准相竞争的性能，发现得到的变压器也表现出与其全精度对手相似的扩展规律，同时使用的操作所需的功率要小得多。激活量化为8位，优化器状态和梯度仍然以全精度进行。作者承认，由于他们的实现是在FP16/BF16中执行操作，因此没有实际的训练加速，但他们提议可以使用低精度FP8
    GEMM CUDA内核来加速前向和反向传播，尽管他们将此留待未来工作中进行。
- en: '![](../Images/4080589deca9dbe34c6ef2c99b76e2da.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4080589deca9dbe34c6ef2c99b76e2da.png)'
- en: Charts showing that [BitNet](https://arxiv.org/abs/2310.11453) performance scales
    similarly to full-precision transformer. Note that the left chart demonstrates
    that equal performance can be achieved with an order of magnitude less energy
    consumption.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 图表显示了[BitNet](https://arxiv.org/abs/2310.11453)的性能与全精度变压器的性能扩展类似。请注意，左侧的图表展示了通过减少一个数量级的能耗，可以实现相同的性能。
- en: Following up in 2024, the BitNet authors published [BitNet b1.58](https://arxiv.org/abs/2402.17764)
    which expanded their work into the use of ternary {-1, 0, 1} weights, adding the
    zero value into the previous 1-bit BitNet system by tuning a γ threshold parameter
    that clamps small values to zero. As we learned with ternary CNNs, adding in the
    zero value in the weights causes no increase in operational complexity, since
    multiplications can still be replaced by addition/subtraction, but it markedly
    improves performance by enabling the weights to perform “feature filtering,” at
    the expense of doubling the number of bits per weight from 1 to 2\. In either
    case, since the weights are smaller, the transfer from DRAM to SRAM is easier,
    and memory traffic is reduced. The authors use symmetrical quantization for easier
    kernel optimization, quantize activations to 8-bit, and quantize the KV cache
    to 4-bit. Using this approach, the authors train ternary networks from scratch
    which match baseline FP16 performance in both perplexity and task performance.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 2024年，BitNet的作者发布了[BitNet b1.58](https://arxiv.org/abs/2402.17764)，将他们的工作扩展到了使用三值{-1,
    0, 1}权重，通过调整γ阈值参数，将小值限制为零，从而将零值引入到之前的1位BitNet系统中。正如我们在三值卷积神经网络（CNNs）中学到的那样，在权重中加入零值不会增加操作复杂度，因为乘法仍然可以通过加法/减法替代，但它显著提高了性能，使得权重能够执行“特征过滤”，虽然这意味着每个权重的位数从1位增加到2位。无论如何，由于权重更小，从DRAM到SRAM的传输更加容易，内存流量也减少了。作者使用对称量化来简化卷积核优化，将激活量化为8位，将KV缓存量化为4位。采用这种方法，作者从零开始训练三值网络，在困惑度和任务性能上都能与基准FP16性能匹配。
- en: For detail on the code and implementation of BitNet b1 (binary) and b1.58 (ternary)
    approaches, the reader should see the [Training Tips, Code, and FAQ PDF file](https://github.com/microsoft/unilm/blob/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf).
    However, the code offered there does not detail the low-bit kernel to use for
    inference. Fortunately, the open-source community has developed this [GitHub repo](https://github.com/kyegomez/BitNet/tree/main)
    with a full implementation, complete with custom kernels and pip install. It’s
    important to note that while BitNet is a powerful step into extreme quantization
    of LLMs, the binarization and ternarization is not being fully exploited in these
    implementations, as multiplications are still in half-precision during training,
    and the custom kernels being used to accelerate inference are FP8\. However, we
    learned earlier that the most pronounced benefit of using binary/ternary weights
    was that matrix multiplications could be replaced with far more energy-efficient
    integer addition/subtraction operations, which translates directly to faster computation,
    but that has not been explored here for either training or inference, leaving
    an open lane for future work.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 有关BitNet b1（双精度）和b1.58（三值）方法的代码和实现的详细信息，读者可以参考[训练提示、代码和常见问题解答PDF文件](https://github.com/microsoft/unilm/blob/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf)。然而，那里提供的代码并未详细说明推理时使用的低位核函数。幸运的是，开源社区已经开发了这个[GitHub仓库](https://github.com/kyegomez/BitNet/tree/main)，提供了完整的实现，包含自定义核函数和pip安装。需要注意的是，尽管BitNet在极端量化LLMs方面迈出了重要一步，但在这些实现中，二值化和三值化并未得到充分利用，因为训练过程中乘法仍然采用半精度，而加速推理的自定义核函数使用的是FP8精度。然而，我们之前了解到，使用二值/三值权重的最显著优势是，矩阵乘法可以用更节能的整数加法/减法运算替代，从而直接提高计算速度，但在这里，无论是训练还是推理，都未能探索这一点，留下了未来工作的空间。
- en: '![](../Images/aeb9879d5421194fb27a15e8cc766e90.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aeb9879d5421194fb27a15e8cc766e90.png)'
- en: Results from BitNet b1.58 show that it closely matches or exceeds FP16 LLaMA
    models with equivalent parameter counts. Note that they cap their experiment to
    3.9B params, likely because their training process does not improve efficiency
    over FP16, and is therefore very expensive.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 来自BitNet b1.58的结果显示，它与具有相同参数数量的FP16 LLaMA模型非常接近，甚至超越它们。请注意，他们将实验的参数上限设置为3.9B，可能是因为他们的训练过程未能在FP16基础上提高效率，因此成本非常高。
- en: Practitioner’s LLM Quantization Guide
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践者的LLM量化指南
- en: '![](../Images/98327cf41a99daba4f1d0c1312af6009.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/98327cf41a99daba4f1d0c1312af6009.png)'
- en: Image by author using DALL-E 3.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用DALL-E 3生成的图像。
- en: In this section, we will discuss practical implementation of quantization to
    optimize our workflows. As we’ve seen, quantization can be applied during training
    or as a post-processing step in model development, and the choice between these
    will depend on the model and use case. To become educated practitioners in quantized
    workflows, we will first review the current tool sets available to us, along with
    their strengths and weaknesses. Then, we will use what we’ve learned to develop
    a decision tree that will help us to align our goals with the methods available.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论量化的实际应用，以优化我们的工作流程。正如我们所看到的，量化可以在训练过程中应用，或者作为模型开发中的后处理步骤，选择哪种方式取决于模型和使用场景。为了成为量化工作流程的实践者，我们将首先回顾当前可用的工具集，以及它们的优缺点。然后，我们将根据所学的内容，制定一个决策树，帮助我们将目标与可用方法对齐。
- en: A popular and easy way to experiment with transformer development and inference
    is using the [Hugging Face Transformers](https://github.com/huggingface/transformers)
    library (along with their [model hub](https://huggingface.co/models), which operates
    like a GitHub repo for deep learning models, allowing you to easily push your
    model checkpoints and pull them for use on other machines with just a few lines
    of code. While this is an amazing interface for people who can operate Python
    code, it does not provide a non-code user interface (UI) for using the models,
    and while this library can be used as a capable backend for inference, it generally
    serves better as a development ecosystem, since more optimized libraries provide
    faster backend serving, as we will see. However, on the training side, the integration
    of the [bitsandbytes](https://huggingface.co/docs/bitsandbytes/main/en/index)
    library for enabling efficient quantized training with [QLoRA](https://arxiv.org/abs/2305.14314)
    through the intuitive interface of Hugging Face’s Transformers library makes it
    a very powerful tool for model development.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 一种流行且简便的尝试变换器开发和推理的方式是使用[Hugging Face Transformers](https://github.com/huggingface/transformers)库（以及他们的[模型库](https://huggingface.co/models)，该库类似于一个深度学习模型的GitHub仓库，允许你轻松推送模型检查点并将其拉取到其他机器上，只需几行代码。虽然这是一个非常适合能够操作Python代码的人的接口，但它没有提供用于模型的非代码用户界面（UI），而且虽然这个库可以作为一个强大的推理后端使用，但通常更优化的库提供更快的后端服务，正如我们将看到的那样。然而，在训练方面，[bitsandbytes](https://huggingface.co/docs/bitsandbytes/main/en/index)库的集成，通过Hugging
    Face的Transformers库的直观界面实现高效的量化训练，并支持[QLoRA](https://arxiv.org/abs/2305.14314)，使其成为一个非常强大的模型开发工具。
- en: Starting in July 2019, Nvidia offered the [FasterTransformer](https://github.com/NVIDIA/FasterTransformer)
    library to optimize inference throughput of transformer models (originally focused
    on BERT), and used this to back the TensorRT SDK for their devices. Subsequently,
    this project has evolved into [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM/tree/release/0.5.0)
    (aka TRT-LLM), which is a Python API built to look similar to PyTorch, and includes
    support for GPTQ, AWQ, and an implementation of the [SmoothQuant](https://arxiv.org/abs/2211.10438)
    technique. These libraries are specifically engineered to optimize inference on
    Nvidia hardware or their [Triton inference server](https://developer.nvidia.com/triton-inference-server),
    so while it is very effective for those using these, it is not a general-purpose
    quantization library. That being said, for those seeking to use multi-GPU settings
    with Nvidia hardware, it may be the right choice. The library includes pre-built
    versions of many popular open-source [models](https://github.com/NVIDIA/TensorRT-LLM?tab=readme-ov-file#Models),
    but users can also quantize custom models using the API.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 从2019年7月开始，Nvidia提供了[FasterTransformer](https://github.com/NVIDIA/FasterTransformer)库，用于优化变换器模型的推理吞吐量（最初专注于BERT），并用它支持TensorRT
    SDK以适配其设备。随后，该项目发展成了[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM/tree/release/0.5.0)（也称为TRT-LLM），这是一个类似于PyTorch的Python
    API，支持GPTQ、AWQ以及[SmoothQuant](https://arxiv.org/abs/2211.10438)技术的实现。这些库专门设计用于优化Nvidia硬件或其[Triton推理服务器](https://developer.nvidia.com/triton-inference-server)上的推理，因此，对于使用这些硬件的用户来说，它非常有效，但它并不是一个通用的量化库。话虽如此，对于那些希望在Nvidia硬件上使用多GPU设置的用户来说，它可能是一个合适的选择。该库包括许多流行开源[模型](https://github.com/NVIDIA/TensorRT-LLM?tab=readme-ov-file#Models)的预构建版本，但用户也可以使用API量化自定义模型。
- en: Meanwhile, [Georgi Gerganov](https://ggerganov.com/)’s Machine Learning library
    ([GGML](https://github.com/ggerganov/ggml)), a pure C library designed to accelerate
    LLM inference on Apple devices, was created in September of 2022, and has been
    steadily developing since. GGML uses quantization to create structured binary
    model files that can be used to perform optimized tensor calculations on various
    hardware. While the library is specifically tailored for Apple silicon, it now
    provides support for accelerating inference on x86 architectures and GPUs as well.
    The GGML library provides the backend for the highly popular llama.cpp inference
    library, which then in turn provides backend support for frontend libraries like
    [Ollama](https://ollama.com/) and [LM Studio](https://lmstudio.ai/). GGUF is the
    new and improved file format offered by the GGML library. The drawback to using
    these compression formats is that they mandate the use of llama.cpp for inference,
    which is not optimal for many hardware architectures, particularly non-Apple devices.
    However, llama.cpp has added support multi-GPU/CUDA hardware setups, so user-friendly
    tools like Ollama are quite effective, even if not the fastest.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，[Georgi Gerganov](https://ggerganov.com/) 的机器学习库 ([GGML](https://github.com/ggerganov/ggml))，这是一个旨在加速
    Apple 设备上 LLM 推理的纯 C 库，创建于 2022 年 9 月，并自那时以来稳步发展。GGML 使用量化来创建结构化的二进制模型文件，这些文件可以用于在各种硬件上执行优化的张量计算。虽然该库专为
    Apple Silicon 量身定制，但现在也支持加速 x86 架构和 GPU 上的推理。GGML 库为广受欢迎的 llama.cpp 推理库提供后端支持，后者又为
    [Ollama](https://ollama.com/) 和 [LM Studio](https://lmstudio.ai/) 等前端库提供后端支持。GGUF
    是 GGML 库提供的全新改进文件格式。使用这些压缩格式的缺点是它们强制要求使用 llama.cpp 进行推理，这对于许多硬件架构，特别是非 Apple 设备来说并不理想。然而，llama.cpp
    已增加对多 GPU/CUDA 硬件设置的支持，因此像 Ollama 这样用户友好的工具仍然非常有效，即使它们可能不是最快的。
- en: '[vLLM](https://github.com/vllm-project/vllm) is a powerful inference engine
    that was introduced in [Kwon et al. 2023](https://arxiv.org/abs/2309.06180), which
    uses optimized CUDA kernels to accelerate performance on Nvidia and AMD GPUs.
    The authors point out the fact that the one-by-one sequential token generation
    process of LLMs is memory-bound, and thus underutilizes the computation power
    of GPUs. To address this, they build the vLLM serving engine on top of their proposed
    *PagedAttention* algorithm, which improves memory efficiency of the KV cache to
    avoid wasted space, crucial for increasing the maximum batch size on a given piece
    of hardware. Shows 2–4x better throughput than FasterTransformer, with the advantage
    more pronounced with larger and more complex models and larger sequences. vLLM
    has seamless integration with Hugging Face models, and supports multi-GPU workflows.
    The engine works well with GPTQ, AWQ, SqueezeLLM, and FP8 KV cache quantization,
    allowing models quantized through any of these methods to benefit from the speed
    benefits of PagedAttention. SmoothQuant+ integration is coming soon.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '[vLLM](https://github.com/vllm-project/vllm) 是一个强大的推理引擎，首次在 [Kwon 等，2023](https://arxiv.org/abs/2309.06180)
    中提出，它使用优化的 CUDA 内核加速 Nvidia 和 AMD GPU 上的性能。作者指出，LLM 的逐个生成 token 的过程是受内存限制的，因此未充分利用
    GPU 的计算能力。为了解决这个问题，他们在其提出的 *PagedAttention* 算法基础上构建了 vLLM 服务引擎，该算法提高了 KV 缓存的内存效率，以避免浪费空间，这对于增加给定硬件上的最大批处理大小至关重要。与
    FasterTransformer 相比，吞吐量提高了 2–4 倍，且随着模型和序列规模的增大，这一优势更加明显。vLLM 无缝集成了 Hugging Face
    模型，并支持多 GPU 工作流。该引擎与 GPTQ、AWQ、SqueezeLLM 和 FP8 KV 缓存量化兼容，允许通过这些方法量化的模型受益于 PagedAttention
    的速度提升。SmoothQuant+ 集成即将推出。'
- en: '[MLC-LLM](https://github.com/mlc-ai/mlc-llm) is a powerful, universal deployment
    solution for optimized native deployment of quantized models on multiple types
    of hardware, including Apple, Nvidia, AMD, Intel, and mobile devices. At the time
    of this writing, it appears to be the fastest and most general serving engine
    available, and is preferred by the [Jetson AI Lab](https://www.jetson-ai-lab.com/)
    for its superior throughput. The MLC-LLM library offers a [set of prebuilt models](https://llm.mlc.ai/docs/prebuilt_models.html),
    as well as the option to [compile new models](https://llm.mlc.ai/docs/compilation/compile_models.html)
    for use with the library.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '[MLC-LLM](https://github.com/mlc-ai/mlc-llm) 是一个强大且通用的部署解决方案，用于在多种硬件上优化量化模型的本地部署，包括
    Apple、Nvidia、AMD、Intel 和移动设备。根据目前的情况，它似乎是最快且最通用的服务引擎，且因其卓越的吞吐量而受到 [Jetson AI Lab](https://www.jetson-ai-lab.com/)
    的青睐。MLC-LLM 库提供了一 [组预构建的模型](https://llm.mlc.ai/docs/prebuilt_models.html)，以及[编译新模型](https://llm.mlc.ai/docs/compilation/compile_models.html)的选项，以便与该库一起使用。'
- en: '![](../Images/707119d410ddf78b53001650292ab9d9.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/707119d410ddf78b53001650292ab9d9.png)'
- en: Results from MLC-LLM shows throughput of 4-bit CodeLlama-34B and Llama2–70B
    on two NVIDIA RTX 4090\. We can see that any specialized inference engine offers
    substantial gains over using HF Transformers, but that for these NVIDIA cards,
    MLC-LLM outperforms the others consistently.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: MLC-LLM的结果显示，在两块NVIDIA RTX 4090上，4位CodeLlama-34B和Llama2–70B的吞吐量非常高。我们可以看到，任何专用推理引擎都能比使用HF
    Transformers提供显著的性能提升，但对于这些NVIDIA显卡，MLC-LLM始终优于其他引擎。
- en: LLM Quantization Decision Tree
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM量化决策树
- en: In this article, we have covered a lot of methodology which creates a dizzying
    list of design options for quantization. Fortunately, a handful of open-source
    inference engines have put the most useful of these tools at our fingertips in
    some intuitive form factors. This decision tree can be used as a very basic rule-of-thumb
    based on the intended use case and deployment environment, but is not an exhaustive
    list of considerations. It is intended to give practitioners a launching point
    for implementing quantization in their workflows.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们涵盖了很多方法论，这些方法论为量化设计提供了一个令人眼花缭乱的选择列表。幸运的是，一些开源推理引擎已经以一些直观的形式将这些最有用的工具提供给我们。这棵决策树可以作为一个非常基本的经验法则，根据预期的使用场景和部署环境来使用，但它并不是一个详尽无遗的考虑因素列表。它旨在为从业者提供实施量化的起点。
- en: '**Q: Are you deploying a pretrained model on CPU or at the edge?**'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问：你是否在CPU或边缘设备上部署了预训练模型？**'
- en: 'A: Yes — For Apple users, go with GGML-based inference ([llama.cpp](https://github.com/ggerganov/llama.cpp),
    [Ollama](https://ollama.com/), [LM Studio](https://lmstudio.ai/)). For Android
    and x86 hardware, use [MLC-LLM](https://github.com/mlc-ai/mlc-llm).'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答：是的 — 对于苹果用户，可以选择基于GGML的推理（[llama.cpp](https://github.com/ggerganov/llama.cpp)，[Ollama](https://ollama.com/)，[LM
    Studio](https://lmstudio.ai/)）。对于安卓和x86硬件，使用[MLC-LLM](https://github.com/mlc-ai/mlc-llm)。
- en: No — Go to next question.
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 否 — 请转到下一个问题。
- en: '**Q: Will you be serving simultaneous batched requests?** A: Yes — Use [vLLM](https://github.com/vllm-project/vllm),
    it is specifically optimized for this.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问：你会同时处理批量请求吗？** 答：是的 — 使用[vLLM](https://github.com/vllm-project/vllm)，它专门为此进行了优化。'
- en: No — Go to next question.
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 否 — 请转到下一个问题。
- en: '**Q: Are you deploying a pretrained model on a GPU?**'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问：你是否在GPU上部署了预训练模型？**'
- en: 'A: Yes — MLC-LLM appears to provide state-of-the-art throughput on Nvidia and
    AMD GPUs, and while it also supports Apple GPUs, llama.cpp has the pedigree for
    being optimized for Apple hardware, so a comparison is warranted. For Nvidia and
    AMD GPUs, the forthcoming integration of SmoothQuant+ in the vLLM library will
    be worth a test when available.'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答：是的 — MLC-LLM在Nvidia和AMD GPU上似乎提供了最先进的吞吐量，虽然它也支持苹果GPU，但llama.cpp在优化苹果硬件方面有优势，因此值得进行比较。对于Nvidia和AMD
    GPU，vLLM库中即将集成的SmoothQuant+将在发布时值得一试。
- en: No — Go to next question.
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 否 — 请转到下一个问题。
- en: '**Q: Are you fine-tuning a quantized LLM?**'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问：你正在微调一个量化的LLM吗？**'
- en: 'A: Yes — Use [QLoRA](https://github.com/artidoro/qlora). This is the most efficient
    way to enable a quantized LLM to perform new tasks or recover lost performance
    due to quantization. It is [used easily in Hugging Face Transformers](https://huggingface.co/blog/4bit-transformers-bitsandbytes).'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答：是的 — 使用[QLoRA](https://github.com/artidoro/qlora)。这是使量化的LLM执行新任务或恢复由于量化丢失的性能的最有效方式。[它可以轻松应用于Hugging
    Face Transformers](https://huggingface.co/blog/4bit-transformers-bitsandbytes)。
- en: No — Go to next question.
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 否 — 请转到下一个问题。
- en: '**Q: Are you training a foundation model?**'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问：你正在训练一个基础模型吗？**'
- en: 'A: Yes — Try using [BitNet](https://github.com/kyegomez/BitNet) to prototype
    or develop foundation models, as it is far cheaper to train and provides competitive
    performance to full-precision baselines. Even better, try ensembling these low-bit
    models.'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答：是的 — 尝试使用[BitNet](https://github.com/kyegomez/BitNet)来原型化或开发基础模型，因为它的训练成本要低得多，并且提供与全精度基准相竞争的性能。更好的是，尝试将这些低位模型进行集成。
- en: This checklist should provide the average user with a solid guide on where to
    look given their circumstance. For a more comprehensive guide into LLM inference
    frameworks, check out [this post from Sergei Savvov](https://betterprogramming.pub/frameworks-for-serving-llms-60b7f7b23407).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这个清单应该为普通用户提供一个可靠的指南，帮助他们根据自己的情况找到适合的方案。若想获得更全面的LLM推理框架指南，可以参考[Sergei Savvov的这篇文章](https://betterprogramming.pub/frameworks-for-serving-llms-60b7f7b23407)。
- en: Conclusion
  id: totrans-329
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we’ve seen some incredible feats in quantization research.
    We’ve seen lossless 8-bit quantization grow into a boring baseline, binary CNNs
    nearly match their full-precision counterparts, large transformer models trained
    in 4-bits of precision, and the rise of extreme quantization in LLMs. We’ve seen
    that INT8 PTQ is fully mature, easily matching full-precision baselines, with
    wide integration into most open-source libraries, so it is arguable that no one
    should be serving models using floating point data types >8bit. In the case of
    LLMs, which are often released in FP16, this lossless INT8 quantization halves
    memory footprints and provides “free lunch” compression. Further, quantizing at
    lower precision has been achieved with minimal performance loss, and for many
    practical use cases, an accelerated and seamless user-experience will be a welcome
    trade-off for a slight drop in accuracy. The idea of ensembling extremely low-bit
    networks like we saw in the BENN paper is very compelling, especially as we now
    see the rise of extreme quantization in LLMs, which may soon benefit from this
    methodology.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们见证了一些量化研究中的令人难以置信的成就。我们看到无损8位量化逐渐成为一个平凡的基准，二进制卷积神经网络（CNN）几乎可以与其全精度模型相媲美，训练在4位精度下的大型变换器模型，以及在大规模语言模型（LLM）中极端量化的兴起。我们发现INT8的后训练量化（PTQ）已经完全成熟，能够轻松与全精度基准对齐，并广泛集成到大多数开源库中，因此可以说，没有人应该使用大于8位的浮动点数据类型来服务模型。在大规模语言模型的案例中，这些模型通常以FP16格式发布，而无损INT8量化将内存占用减少一半，并提供了“免费午餐”般的压缩效果。此外，低精度量化已经在最小的性能损失下实现，对于许多实际应用场景，适度的精度下降换来加速和无缝的用户体验，将是一个受欢迎的权衡。我们在BENN论文中看到的极低位网络集成的理念非常有吸引力，特别是如今我们看到LLM中极端量化的崛起，未来这种方法可能会受益于这种方法。
- en: We’ve seen a variety of quantization approaches, catering to different use cases
    with a range of complexities, and watched their growth through time, revealing
    which approaches have caught on and become popularized in the open-source community.
    While PTQ and ZSQ have become very effective, QAT is still fundamentally the most
    performant approach because it incorporates quantization into the network training
    process. While this is much more resource intensive, and seems to preclude its
    use on LLMs by the average practitioner, the ingenuity of QLoRA allows us to blur
    the line between PTQ and QAT by training only a small number of parameters in
    low-rank matrices to recover loss due to quantization. We’ve seen PTQ recently
    become effective down to 2-bits of precision with the HQQ method, and we’ve seen
    that binary and ternary LLMs can be trained from scratch to nearly match the performance
    of their full-precision counterparts.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了一系列量化方法，满足不同应用场景的需求，具有各种复杂性，并见证了它们随时间的成长，揭示了哪些方法在开源社区中得到了普及。尽管PTQ和ZSQ已经变得非常有效，QAT仍然是从根本上最具性能的方法，因为它将量化融入了网络训练过程。虽然这种方法资源消耗较大，并且似乎排除了普通从业者在大规模语言模型上使用它，但QLoRA的巧妙设计使我们能够模糊PTQ和QAT之间的界限，通过仅训练低秩矩阵中的少量参数来恢复量化带来的损失。我们看到，PTQ最近已经通过HQQ方法有效地实现了2位精度量化，并且我们还看到二进制和三进制LLM可以从头开始训练，几乎达到与全精度模型相媲美的性能。
- en: Congratulations on making it to this point. Familiarized with the works reviewed
    in this article, you now have an expert knowledge on neural network quantization.
    This survey was not for the faint of heart, but there was no need for a superficial
    summary of quantization methods in 2024\. For proper ML practice, we require a
    comprehensive foundation of knowledge to make educated decisions that weigh all
    potential options, as well as the pros and cons of our chosen methodology. Quantization
    of neural networks is an extremely prolific field of research build on a rich
    legacy, which poses a daunting research challenge for those wanting to achieve
    a wholistic understanding. Hopefully, this article provides an accessible, self-contained,
    and comprehensive resource to practitioners in the field, as well as some effective
    suggestions for which tools are the best for their common use cases.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你坚持到了这一点。在本文中，我们已经了解了所讨论的一些工作，现在你已经掌握了神经网络量化的专业知识。这项调查并不适合胆小的人，但也没有必要对2024年的量化方法做一个肤浅的总结。为了进行正确的机器学习实践，我们需要一个全面的知识基础，以便做出明智的决策，权衡所有潜在的选项以及我们所选方法的利弊。神经网络的量化是一个极具活力的研究领域，建立在丰富的遗产之上，给那些希望实现全面理解的研究者带来了巨大的挑战。希望本文能为该领域的从业人员提供一个可访问、独立且全面的资源，并为他们的常见应用场景提供一些有效的工具选择建议。
- en: Future Work
  id: totrans-333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 未来工作
- en: BitNet does not use low-precision computation during training, and also does
    not fully capitalize on the binarization or ternarization of its weights, since
    it does not replace multiplications with addition/subtraction operations. These
    factors are crippling its ability to realize its true value. The legacy of work
    in the extreme quantization of CNNs should be applied to binarize the actual mathematics
    and make the operations used by BitNet more efficient. Further, once optimized
    kernels are established for BitNet, we should start investigating the ensembling
    of these low-bit LLMs, as we saw that this was highly effective with CNNs in the
    BENN paper.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: BitNet 在训练过程中不使用低精度计算，也没有充分利用其权重的二值化或三值化，因为它没有用加法/减法操作替代乘法运算。这些因素限制了其实现真正价值的能力。极端量化卷积神经网络（CNNs）领域的遗产应该应用于对实际数学进行二值化，并使
    BitNet 使用的操作更加高效。此外，一旦为 BitNet 建立了优化的内核，我们应该开始研究这些低位 LLMs 的集成，因为我们在 BENN 论文中看到这在
    CNNs 中非常有效。
- en: HQQ is a very efficient algorithm for accurate data-free (zero-shot) PTQ, but
    so far only one optimized inference engine (vLLM) has started adding experimental
    support for running these models. Further, there are currently no benchmarks on
    the inference throughput of models quantized using HQQ compared to other methods.
    Both of these items are open areas for future work.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: HQQ 是一种非常高效的准确的无数据（零样本）PTQ 算法，但迄今为止，仅有一个优化的推理引擎（vLLM）开始为运行这些模型提供实验性支持。此外，目前没有关于使用
    HQQ 量化的模型与其他方法相比的推理吞吐量基准。这两项内容是未来工作的开放领域。
