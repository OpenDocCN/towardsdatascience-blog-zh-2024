- en: Are GPTs Good Embedding Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT是优秀的嵌入模型吗？
- en: 原文：[https://towardsdatascience.com/are-gpts-good-embedding-models-28d8ef6f3f63?source=collection_archive---------0-----------------------#2024-05-18](https://towardsdatascience.com/are-gpts-good-embedding-models-28d8ef6f3f63?source=collection_archive---------0-----------------------#2024-05-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/are-gpts-good-embedding-models-28d8ef6f3f63?source=collection_archive---------0-----------------------#2024-05-18](https://towardsdatascience.com/are-gpts-good-embedding-models-28d8ef6f3f63?source=collection_archive---------0-----------------------#2024-05-18)
- en: A surprising experiment to show that the devil is in the details
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个令人惊讶的实验，表明细节决定成败
- en: '[](https://medium.com/@yuchengtsai84?source=post_page---byline--28d8ef6f3f63--------------------------------)[![Yu-Cheng
    Tsai](../Images/c0ec2d4b9fea512040c8e6e0250670fc.png)](https://medium.com/@yuchengtsai84?source=post_page---byline--28d8ef6f3f63--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--28d8ef6f3f63--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--28d8ef6f3f63--------------------------------)
    [Yu-Cheng Tsai](https://medium.com/@yuchengtsai84?source=post_page---byline--28d8ef6f3f63--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@yuchengtsai84?source=post_page---byline--28d8ef6f3f63--------------------------------)[![Yu-Cheng
    Tsai](../Images/c0ec2d4b9fea512040c8e6e0250670fc.png)](https://medium.com/@yuchengtsai84?source=post_page---byline--28d8ef6f3f63--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--28d8ef6f3f63--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--28d8ef6f3f63--------------------------------)
    [Yu-Cheng Tsai](https://medium.com/@yuchengtsai84?source=post_page---byline--28d8ef6f3f63--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--28d8ef6f3f63--------------------------------)
    ·6 min read·May 18, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--28d8ef6f3f63--------------------------------)
    ·6分钟阅读·2024年5月18日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/0f89f4b2c267496534e57792622439db.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f89f4b2c267496534e57792622439db.png)'
- en: Image by the author using DALL-E
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由作者使用DALL-E生成
- en: With the growing number of embedding models available, choosing the right one
    for your machine learning applications can be challenging. Fortunately, the [MTEB
    leaderboard](https://huggingface.co/spaces/mteb/leaderboard) provides a comprehensive
    range of ranking metrics for various natural language processing tasks.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着可用的嵌入模型数量不断增加，选择适合自己机器学习应用的模型可能变得具有挑战性。幸运的是，[MTEB排行榜](https://huggingface.co/spaces/mteb/leaderboard)为各种自然语言处理任务提供了全面的排名指标。
- en: '![](../Images/608efe4c9a370b4f9a08332196481dfd.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/608efe4c9a370b4f9a08332196481dfd.png)'
- en: Top 5 embedding models from the [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard)
    as of May 17th, 2024
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 截至2024年5月17日，来自[MTEB排行榜](https://huggingface.co/spaces/mteb/leaderboard)的前五大嵌入模型
- en: When you visit the site, you’ll notice that the top five embedding models are
    Generative Pre-trained Transformers (GPTs). This might lead you to think that
    GPT models are the best for embeddings. But is this really true? Let’s conduct
    an experiment to find out.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当你访问这个网站时，你会注意到排名前五的嵌入模型是生成式预训练变换模型（GPTs）。这可能让你认为GPT模型是最适合用于嵌入的模型。但这是真的吗？让我们进行一个实验来找出答案。
- en: GPT Embeddings
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT嵌入
- en: Embeddings are tensor representation of texts, that converts text token IDs
    and projects them into a tensor space.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是文本的张量表示，它将文本标记ID转换并投影到张量空间中。
- en: 'By inputting text into a neural network model and performing a forward pass,
    you can obtain embedding vectors. However, the actual process is a bit more complex.
    Let’s break it down step by step:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将文本输入到神经网络模型中并执行前向传递，你可以获得嵌入向量。然而，实际过程要复杂一些。让我们一步一步分解：
- en: Convert the text into token IDs
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文本转换为标记ID
- en: Pass the token IDs into a neural network
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将标记ID传递到神经网络中
- en: Return the outputs of the neural network
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回神经网络的输出
- en: In the first step, I am going to use a tokenizer to achieve it. `model_inputs`
    is the tensor representation of the text content, `"some questions."` .
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，我将使用分词器来实现这一点。`model_inputs`是文本内容`"some questions."`的张量表示。
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The second step is straightforward, forward-passing the `model_inputs` into
    a neural network. The logits of generated tokens can be accessed via `.logits`.
    `torch.no_grad()` means I don’t want the model weights to be updated because the
    model is in inference mode.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步很简单，将`model_inputs`前向传递给神经网络。生成的标记的logits可以通过`.logits`访问。`torch.no_grad()`表示我不希望模型的权重被更新，因为模型处于推理模式。
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The third step is a bit tricky. GPT models are decoder-only, and their token
    generation is autoregressive. In simple terms, the last token of a completed sentence
    has seen all the preceding tokens in the sentence. Therefore, the output of the
    last token contains all the affinity scores (attentions) from the preceding tokens.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 第三步有点棘手。GPT模型是解码器-only，其token生成是自回归的。简单来说，已完成句子的最后一个token已经看到了句子中的所有前面的tokens。因此，最后一个token的输出包含了来自前面tokens的所有相关性分数（注意力）。
- en: Bingo! You are most interested in the last token because of the attention mechanism
    in the transformers.
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 完美！你最感兴趣的是最后一个token，因为在transformer中的注意力机制。
- en: The output dimension of the GPTs implemented in Hugging Face is (batch size,
    input token size, number of vocabulary). To get the last token output of all the
    batches, I can perform a tensor slice.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hugging Face中实现的GPT的输出维度是（批量大小，输入token大小，词汇表数量）。为了获取所有批次的最后一个token输出，我可以执行张量切片。
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Quality of these GPT Embeddings
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这些GPT嵌入的质量
- en: To measure the quality of these GPT embeddings, you can use [cosine similarity.](https://pytorch.org/docs/stable/generated/torch.nn.CosineSimilarity.html)
    The higher the cosine similarity, the closer the semantic meaning of the sentences.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要衡量这些GPT嵌入的质量，可以使用[余弦相似度](https://pytorch.org/docs/stable/generated/torch.nn.CosineSimilarity.html)。余弦相似度越高，句子的语义越接近。
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Let’s create some util functions that allows us to loop through list of question
    and answer pairs and see the result. [Mistral 7b v0.1 instruct](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)
    , one of the great open-sourced models, is used for this experiment.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一些实用函数，允许我们遍历问题和答案对列表并查看结果。[Mistral 7b v0.1 指令](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)，这是一个出色的开源模型，用于本实验。
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/c99fb95672048c42a47ba01ae6599ec2.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c99fb95672048c42a47ba01ae6599ec2.png)'
- en: Cosine similarities for mistral 7b v0.1 instruct (Image by the author)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral 7b v0.1 指令的余弦相似度（图像来源：作者）
- en: Results and Observations
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果与观察
- en: 'For the first question and answer pair:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个问题和答案对：
- en: 'Question: “What is the headquarter of OpenAI?”'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题：“OpenAI的总部是什么？”
- en: 'Answer: “OpenAI is based at San Francisco.”'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 答案：“OpenAI总部位于旧金山。”
- en: 'Cosine Similarity: 0.96'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 余弦相似度：0.96
- en: 'For the second question and answer pair:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二个问题和答案对：
- en: 'Question: “What is GPU?”'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题：“什么是GPU？”
- en: 'Answer: “A graphics processing unit (GPU) is an electronic circuit that can
    perform mathematical calculations quickly.”'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 答案：“图形处理单元（GPU）是一个能够快速执行数学计算的电子电路。”
- en: 'Cosine Similarity: 0.94'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 余弦相似度：0.94
- en: 'For an irrelevant pair:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不相关的对：
- en: 'Question: “Where is the headquarter of OpenAI?”'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题：“OpenAI的总部在哪里？”
- en: 'Answer: “A graphics processing unit (GPU) is an electronic circuit that can
    perform mathematical calculations quickly.”'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 答案：“图形处理单元（GPU）是一个能够快速执行数学计算的电子电路。”
- en: 'Cosine Similarity: 0.90'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 余弦相似度：0.90
- en: 'For the worst pair:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于最差的对：
- en: 'Question: “What is GPU?”'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题：“什么是GPU？”
- en: 'Answer: “OpenAI is based at San Francisco.”'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 答案：“OpenAI总部位于旧金山。”
- en: 'Cosine Similarity: 0.93'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 余弦相似度：0.93
- en: These results suggest that using GPT models, in this case, the mistral 7b instruct
    v0.1, as embedding models may not yield great results in terms of distinguishing
    between relevant and irrelevant pairs. But why are GPT models still among the
    top 5 embedding models?
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果表明，在这种情况下，使用GPT模型（如mistral 7b instruct v0.1）作为嵌入模型可能不会在区分相关和不相关的对方面产生很好的结果。但为什么GPT模型仍然位居前五名嵌入模型呢？
- en: Contrastive Loss Comes to the Rescue
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对比损失来解救
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/a7d9806157a8c611ddf1ee437d3e12be.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a7d9806157a8c611ddf1ee437d3e12be.png)'
- en: Cosine similarities for `e5-mistral-7b-instruct (Image by the author)`
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`e5-mistral-7b-instruct`的余弦相似度（图像来源：作者）'
- en: Repeating the same evaluation procedure with a different model, `e[5-mistral-7b-instruct](http://intfloat/e5-mistral-7b-instruct)`,
    which is one of the top open-sourced models from the MTEB leaderboard and fine-tuned
    from mistral 7b instruct, I discover that the cosine similarity for the relevant
    question and pairs are 0.88 and 0.84 for OpenAI and GPU questions, respectively.
    For the irrelevant question and answer pairs, the similarity drops to 0.56 and
    0.67\. This findings suggests `e5-mistral-7b-instruct` is a much-improved model
    for embeddings. What makes such an improvement?
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用不同的模型`e[5-mistral-7b-instruct](http://intfloat/e5-mistral-7b-instruct)`重复相同的评估过程，该模型是MTEB排行榜上排名前列的开源模型之一，并从mistral
    7b instruct微调而来。我发现，相关问题和对之间的余弦相似度分别为OpenAI和GPU问题为0.88和0.84。对于不相关的问题和答案对，相似度降至0.56和0.67。这个发现表明，`e5-mistral-7b-instruct`是一个在嵌入方面有显著改进的模型。这种改进是如何实现的呢？
- en: '![](../Images/08fb21f2da6dcfa55fa80bdea3f81999.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08fb21f2da6dcfa55fa80bdea3f81999.png)'
- en: '[The contrastive loss function](/contrastive-loss-explaned-159f2d4a87ec)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[对比损失函数](/contrastive-loss-explaned-159f2d4a87ec)'
- en: Delving into the [paper](https://arxiv.org/pdf/2401.00368) behind `e5-mistral-7b-instruct`,
    the key is the use of [contrastive loss](https://lilianweng.github.io/posts/2021-05-31-contrastive/)
    to further fine tune the mistral model.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 深入研究`e5-mistral-7b-instruct`背后的[论文](https://arxiv.org/pdf/2401.00368)，关键在于使用[对比损失](https://lilianweng.github.io/posts/2021-05-31-contrastive/)进一步微调mistral模型。
- en: Unlike GPTs that are trained or further fine-tuned using [cross-entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)
    of predicted tokens and labeled tokens, contrastive loss aims to maximize the
    distance between negative pairs and minimize the distance between the positive
    pairs.
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 与使用[交叉熵损失](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)对预测标记和标签标记进行训练或进一步微调的GPT模型不同，对比损失旨在最大化负样本对之间的距离，同时最小化正样本对之间的距离。
- en: This [blog post](/contrastive-loss-explaned-159f2d4a87ec) covers this concept
    in greater details. The `sim` function calculates the cosine distance between
    two vectors. For contrastive loss, the denominators represent the cosine distance
    between positive examples and negative examples. The rationale behind contrastive
    loss is that we want similar vectors to be as close to 1 as possible, since log(1)
    = 0 represents the optimal loss.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇[博客文章](/contrastive-loss-explaned-159f2d4a87ec)详细介绍了这一概念。`sim`函数计算两个向量之间的余弦距离。对于对比损失，分母表示正例和负例之间的余弦距离。对比损失背后的原理是，我们希望相似的向量尽可能接近1，因为log(1)
    = 0表示最优损失。
- en: Conclusion
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this post, I have highlighted a common pitfall of using GPTs as embedding
    models without fine-tuning. My evaluation suggests that fine-tuning GPTs with
    contrastive loss, the embeddings can be more meaningful and discriminative. By
    understanding the strengths and limitations of GPT models, and leveraging customized
    loss like contrastive loss, you can make more informed decisions when selecting
    and utilizing embedding models for your machine learning projects. I hope this
    post helps you choose GPTs models wisely for your applications and look forward
    to hearing your feedback! :)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我强调了使用GPT作为嵌入模型而没有微调的常见陷阱。我的评估表明，通过对GPT进行对比损失微调，嵌入可以更加有意义和具有区分性。通过了解GPT模型的优缺点，并利用定制的损失函数，如对比损失，你可以在选择和使用嵌入模型时做出更明智的决策。希望这篇文章能帮助你为应用程序明智地选择GPT模型，并期待听到你的反馈！
    :)
- en: If you are interested in fine-tuning LLMs at scale, I have another related post
    that can help you achieve it. :)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对大规模微调LLM感兴趣，我还有另一篇相关的文章，可以帮助你实现这一目标。 :)
- en: '[](https://medium.com/sage-ai/fine-tuning-large-language-models-a-guide-into-distributed-parallel-training-with-deepspeed-ray-784914926a17?source=post_page-----28d8ef6f3f63--------------------------------)
    [## Fine-Tuning Large Language Models: A Guide into Distributed Parallel Training
    with DeepSpeed, Ray…'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/sage-ai/fine-tuning-large-language-models-a-guide-into-distributed-parallel-training-with-deepspeed-ray-784914926a17?source=post_page-----28d8ef6f3f63--------------------------------)
    [## 微调大型语言模型：分布式并行训练指南（使用DeepSpeed和Ray）'
- en: Path to Open-source LLMs
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开源LLM的路径
- en: medium.com](https://medium.com/sage-ai/fine-tuning-large-language-models-a-guide-into-distributed-parallel-training-with-deepspeed-ray-784914926a17?source=post_page-----28d8ef6f3f63--------------------------------)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/sage-ai/fine-tuning-large-language-models-a-guide-into-distributed-parallel-training-with-deepspeed-ray-784914926a17?source=post_page-----28d8ef6f3f63--------------------------------)
- en: I also have another post related to the scaling law of LLMs. It provides additional
    context on why language models are getting larger and explains the underlying
    reasons? Happy learning and sharing! Cheers.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我还有一篇关于大型语言模型（LLMs）扩展法则的文章。它提供了更多关于为什么语言模型变得越来越大的背景，并解释了背后的原因。祝你学习愉快，分享愉快！干杯。
- en: '[](https://medium.com/sage-ai/demystify-transformers-a-comprehensive-guide-to-scaling-laws-attention-mechanism-fine-tuning-fffb62fc2552?source=post_page-----28d8ef6f3f63--------------------------------)
    [## Demystify Transformers: A Comprehensive Guide to Scaling Laws'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/sage-ai/demystify-transformers-a-comprehensive-guide-to-scaling-laws-attention-mechanism-fine-tuning-fffb62fc2552?source=post_page-----28d8ef6f3f63--------------------------------)
    [## 解密 Transformer：扩展法则的全面指南'
- en: Unpacking Transformer Technologies and Scaling Strategies
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解构 Transformer 技术与扩展策略
- en: medium.com](https://medium.com/sage-ai/demystify-transformers-a-comprehensive-guide-to-scaling-laws-attention-mechanism-fine-tuning-fffb62fc2552?source=post_page-----28d8ef6f3f63--------------------------------)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/sage-ai/demystify-transformers-a-comprehensive-guide-to-scaling-laws-attention-mechanism-fine-tuning-fffb62fc2552?source=post_page-----28d8ef6f3f63--------------------------------)
