- en: Understanding Techniques for Solving GenAI Challenges
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解解决GenAI挑战的技术
- en: 原文：[https://towardsdatascience.com/understanding-techniques-for-solving-genai-challenges-83a7ad4650bd?source=collection_archive---------5-----------------------#2024-06-20](https://towardsdatascience.com/understanding-techniques-for-solving-genai-challenges-83a7ad4650bd?source=collection_archive---------5-----------------------#2024-06-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/understanding-techniques-for-solving-genai-challenges-83a7ad4650bd?source=collection_archive---------5-----------------------#2024-06-20](https://towardsdatascience.com/understanding-techniques-for-solving-genai-challenges-83a7ad4650bd?source=collection_archive---------5-----------------------#2024-06-20)
- en: '*Dive into model pre-training, fine-tuning, RAG, prompt engineering, and more!*'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*深入了解模型预训练、微调、RAG、提示工程等技术！*'
- en: '[](https://medium.com/@tula.masterman?source=post_page---byline--83a7ad4650bd--------------------------------)[![Tula
    Masterman](../Images/c36b3740befd5dfdb8719dc6596f1a99.png)](https://medium.com/@tula.masterman?source=post_page---byline--83a7ad4650bd--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--83a7ad4650bd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--83a7ad4650bd--------------------------------)
    [Tula Masterman](https://medium.com/@tula.masterman?source=post_page---byline--83a7ad4650bd--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@tula.masterman?source=post_page---byline--83a7ad4650bd--------------------------------)[![Tula
    Masterman](../Images/c36b3740befd5dfdb8719dc6596f1a99.png)](https://medium.com/@tula.masterman?source=post_page---byline--83a7ad4650bd--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--83a7ad4650bd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--83a7ad4650bd--------------------------------)
    [Tula Masterman](https://medium.com/@tula.masterman?source=post_page---byline--83a7ad4650bd--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--83a7ad4650bd--------------------------------)
    ·15 min read·Jun 20, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--83a7ad4650bd--------------------------------)
    ·15分钟阅读·2024年6月20日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/bff37c9c3e8b1d57d88922cd6e0dedca.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bff37c9c3e8b1d57d88922cd6e0dedca.png)'
- en: 'Source: Author & GPT4o. Image is designed to show a language model learning
    and developing its brain!'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者及GPT4o。图片设计旨在展示语言模型学习和发展其大脑的过程！
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: Generative AI adoption is rapidly increasing for both individuals and businesses.
    A recent [Gartner study](https://www.gartner.com/en/newsroom/press-releases/2024-05-07-gartner-survey-finds-generative-ai-is-now-the-most-frequently-deployed-ai-solution-in-organizations)
    found that GenAI solutions are the number one AI solution used by organizations,
    with most companies leveraging GenAI features built into existing tools like Microsoft
    365 Copilot. In my experience, most businesses are looking for some sort of “private
    ChatGPT” they can use to get more value from their unique organizational data.
    Company goals vary from finding information in particular documents, generating
    reports based on tabular data, and summarizing content, to finding all the projects
    related to some domain, and much more.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 生成性人工智能的应用正在快速增长，涵盖个人和企业。最近的一项[Gartner研究](https://www.gartner.com/en/newsroom/press-releases/2024-05-07-gartner-survey-finds-generative-ai-is-now-the-most-frequently-deployed-ai-solution-in-organizations)发现，生成性AI解决方案是组织中使用最广泛的AI解决方案，大多数公司都在利用内置于现有工具中的GenAI功能，如Microsoft
    365 Copilot。根据我的经验，大多数企业都在寻求某种形式的“私有ChatGPT”，希望通过它从独特的组织数据中获得更多价值。公司的目标各不相同，从在特定文档中查找信息、基于表格数据生成报告和总结内容，到查找与某个领域相关的所有项目等等。
- en: This article explores various approaches to solve these problems, outlining
    the pros, cons, and applications of each. My goal is to provide guidance on when
    to consider different approaches and how to combine them for the best outcomes,
    covering everything from the most complex and expensive approaches like pre-training
    to the simplest, most cost-effective techniques like prompt engineering.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文探讨了解决这些问题的各种方法，概述了每种方法的优缺点和应用场景。我的目标是提供指导，帮助你在何时考虑不同的方法，并如何将它们结合起来，以实现最佳效果，涵盖从最复杂、最昂贵的预训练方法到最简单、最具成本效益的提示工程技术。
- en: The sequence of the article is intended to build from the foundational concepts
    for model training (pre-training, continued pre-training, and fine tuning) to
    the more commonly understood techniques (RAG and prompt engineering) for interacting
    with existing models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 文章的结构旨在从模型训练的基础概念（预训练、继续预训练和微调）开始，逐步过渡到更常见的技术（RAG和提示工程），这些技术用于与现有模型进行交互。
- en: '![](../Images/ffd3959aaf3e515bd7c43b58844fdc8a.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ffd3959aaf3e515bd7c43b58844fdc8a.png)'
- en: Image by author
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Background
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: There is no one-size fits all approach to tackling GenAI problems. Most use
    cases require a combination of techniques to achieve successful outcomes. Typically,
    organizations start with a model like GPT-4, Llama3 70b Instruct, or DBRX Instruct
    which have been pretrained on trillions of tokens to perform next token prediction,
    then fine-tuned for a particular task, like instruction or chat. Instruction based
    models are trained and optimized to follow specific directions given in the prompt
    while chat based models are trained and optimized to handle conversational formats
    over multiple turns, maintaining context and coherence throughout the conversation.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 解决GenAI问题没有“一刀切”的方法。大多数使用场景需要结合多种技术才能取得成功的结果。通常，组织从像GPT-4、Llama3 70b Instruct或DBRX
    Instruct这样的模型开始，这些模型已经在万亿级别的标记上进行了预训练，用于执行下一个标记预测，然后为特定任务（如指令或对话）进行微调。基于指令的模型经过训练和优化，能够遵循提示中给出的具体指令，而基于对话的模型则经过训练和优化，能够处理多轮对话格式，保持对话的上下文和一致性。
- en: Using existing models allows organizations to take advantage of the significant
    time and financial investments made by companies like OpenAI, Meta, and Databricks
    to curate datasets, create innovative architectures, and train and evaluate their
    models.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 使用现有模型使组织能够利用像OpenAI、Meta和Databricks等公司在数据集策划、创新架构创建以及模型训练和评估上所做的大量时间和资金投资。
- en: Although not every company will need to pre-train or instruction fine-tune their
    models, anyone using a Large Language Model (LLM) benefits from the groundwork
    laid by these industry leaders. This foundation allows other companies to address
    their unique challenges without starting from scratch.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管并不是每个公司都需要预训练或进行指令微调他们的模型，但任何使用大型语言模型（LLM）的人都受益于这些行业领袖所打下的基础。这一基础使得其他公司能够应对独特的挑战，而无需从零开始。
- en: In the following sections, we’ll explore pre-training, fine-tuning (both instruction
    fine-tuning, and continued pre-training), Retrieval Augmented Generation (RAG),
    fine-tuning embeddings for RAG, and prompt engineering, discussing how and when
    each of these approaches should be used or considered.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探讨预训练、微调（包括指令微调和继续预训练）、检索增强生成（RAG）、RAG的嵌入微调以及提示工程，讨论如何以及何时使用或考虑每种方法。
- en: Setting the Baseline with Pre-Training
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过预训练设定基线
- en: '**Overview**: Pre-Training a model creates a foundation which will be used
    as a base for all downstream tasks. This process includes defining the architecture
    for the model, curating a massive dataset (generally trillions of tokens), training
    the model, and evaluating its performance. In the context of LLMs and SLMs, the
    pre-training phase is used to inject knowledge into the model, enabling it to
    predict the next word or token in a sequence. For instance, in the sentence “the
    cat sat on the ___”, the model learns to predict “mat”.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**概述**：预训练模型创建了一个基础，作为所有下游任务的基础。这个过程包括定义模型的架构、策划一个庞大的数据集（通常是万亿级的标记）、训练模型并评估其表现。在LLM和SLM的上下文中，预训练阶段用于将知识注入模型，使其能够预测序列中的下一个单词或标记。例如，在句子“the
    cat sat on the ___”中，模型学习预测“mat”。'
- en: Companies like OpenAI have invested heavily in the pre-training phase for their
    GPT models, but since models like GPT-3.5, GPT-4, and GPT-4o are closed source
    it is not possible to use the underlying architecture and pre-train the model
    on a different dataset with different parameters. However, with resources like
    Mosaic AI’s pre-training API it is possible to pre-train open source models like
    DBRX.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 像OpenAI这样的公司已经在其GPT模型的预训练阶段投入了大量资金，但由于像GPT-3.5、GPT-4和GPT-4o这样的模型是闭源的，因此无法使用底层架构，并在不同的数据集和参数上预训练该模型。然而，借助像Mosaic
    AI的预训练API这样的资源，能够对像DBRX这样的开源模型进行预训练。
- en: '**Pros**:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**：'
- en: '**Complete control**: The benefit of pre-training a model is that you’d have
    complete control over the entire process to create the model. You can tailor the
    architecture, dataset, and training parameters to your needs and test it with
    evaluations representative of your domain instead of a focusing primarily on common
    benchmarks.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完全控制**：预训练模型的好处在于，你可以完全控制整个模型创建过程。你可以根据需求定制架构、数据集和训练参数，并通过代表你所在领域的评估进行测试，而不是主要集中在常见的基准测试上。'
- en: '**Inherent domain specific knowledge**: By curating a dataset focused on a
    particular domain, the model can develop a deeper understanding of that domain
    compared to a general purpose model.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**固有的领域特定知识**：通过策划专注于特定领域的数据集，模型可以比通用模型更深入地理解该领域。'
- en: '**Cons**:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**：'
- en: '**Most expensive option**: Pre-training requires an extreme amount of computational
    power (many, many GPUs) which means the cost of pre-training is typically in the
    millions to tens or hundreds of millions of dollars and often takes weeks to complete
    the training.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最昂贵的选项**：预训练需要极大量的计算能力（许多 GPU），这意味着预训练的成本通常在数百万到数千万甚至上亿美元，并且通常需要数周才能完成训练。'
- en: '**Knowledge cutoffs**: The final model is also completed at a certain point
    in time, so it will have no inherent understanding of real time information unless
    augmented by techniques like RAG or function-calling.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**知识截止时间**：最终模型也是在某一时间点完成的，因此它不会固有地理解实时信息，除非通过像 RAG 或函数调用这样的技术进行增强。'
- en: '**Advanced requirements**: This approach requires the most data and the most
    advanced expertise to achieve meaningful results.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高级要求**：这种方法需要最多的数据和最先进的专业知识才能取得有意义的结果。'
- en: '**Applications**: Generally, pre-training your own model is **only necessary
    if none of the other approaches are sufficient for your use case**. For example,
    if you wanted to train a model to understand a new language it has no previous
    exposure to, you may consider pre-training it then fine-tuning it for your intended
    use.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**应用**：一般来说，只有当其他方法不足以满足你的用例时，预训练你自己的模型才是**必要的**。例如，如果你想训练一个模型来理解一种它以前没有接触过的新语言，你可以考虑先进行预训练，然后再根据你的需求进行微调。'
- en: Once the base training is complete, the models typically need to be fine-tuned
    so that they can perform tasks effectively. When you see a model labeled as a
    chat or instruct model, that indicates the base model has been fine-tuned for
    either of those purposes. Nearly any model you interact with today has been fine-tuned
    for one of these purposes so that end users can interact with the model efficiently.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦基础训练完成，模型通常需要进行微调，以便能够有效执行任务。当你看到一个标记为聊天模型或指令模型时，这意味着基础模型已经针对这些目的进行了微调。如今你与之互动的几乎所有模型都已经针对这些目的进行了微调，以便最终用户能够高效地与模型互动。
- en: Given the incredible cost and intensive process required to pre-train a model,
    most organizations decide to leverage existing models in their GenAI use cases.
    To get started with pretraining, check out [Mosaic AI’s pretraining API](https://docs.mosaicml.com/projects/mcli/en/latest/pretraining/pretraining.html),
    this allows you to pretrain a Databricks DBRX model with different parameter sizes.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由于预训练模型所需的巨大成本和复杂过程，大多数组织决定在其 GenAI 用例中利用现有模型。要开始进行预训练，请查看 [Mosaic AI 的预训练 API](https://docs.mosaicml.com/projects/mcli/en/latest/pretraining/pretraining.html)，它允许你使用不同参数大小的
    Databricks DBRX 模型进行预训练。
- en: '![](../Images/2d91cac4da86a88bc7ae611d90af3628.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d91cac4da86a88bc7ae611d90af3628.png)'
- en: Image by author. Overview of LLM and SLM pre-training.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片。LLM 和 SLM 预训练概览。
- en: Adding Knowledge with Continued Pre-Training (CPT)
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用持续预训练（CPT）添加知识
- en: '**Overview:** CPT is a type of fine-tuning that allows extends the knowledge
    of an existing model rather than training the entire model from scratch. The output
    of a model that’s gone through CPT will still predict the next token. In general
    it’s recommended that you use CPT then Instruction Fine-Tuning (IFT) this way
    you can extend the model’s knowledge first, then tune it to a particular task
    like following instructions or chat. If done in the reverse order, the model may
    forget instructions that it learned during the IFT phase.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**概述**：CPT 是一种微调方法，它允许扩展现有模型的知识，而不是从零开始训练整个模型。经过 CPT 的模型输出仍然会预测下一个标记。通常建议先使用
    CPT，再进行指令微调（IFT），这样你可以先扩展模型的知识，然后将其调优为特定任务，如执行指令或聊天。如果顺序反过来，模型可能会忘记在 IFT 阶段学到的指令。'
- en: '**Pros**:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**：'
- en: '**No need for labeled training data**: CPT does not require labeled training
    data. This is great if you have a lot of domain-specific or new information you
    want to teach the model in general. Since the output is still focused on next
    token prediction, the output from CPT is helpful if you want a text-completion
    model.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无需标签训练数据**：CPT 不需要标签训练数据。如果你有大量领域特定或新的信息想要教给模型，这是非常好的选择。由于输出仍然集中在下一个标记的预测上，CPT
    的输出对你想要一个文本补全模型时非常有帮助。'
- en: '**Faster and more cost effective than pre-training**: CPT can be completed
    in hours or days using less GPUs than pre-training making it faster and cheaper!'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**比预训练更快且更具成本效益**：CPT可以在数小时或几天内完成，使用的GPU比预训练少，因此更快且更便宜！'
- en: '**Cons**:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**：'
- en: '**Still comparatively expensive**: CPT is significantly cheaper than pre-training,
    but can still be expensive and cost tens of thousands of dollars to train a model
    depending on the volume of data and number of GPUs required.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**仍然相对昂贵**：虽然CPT比预训练便宜得多，但仍然可能很昂贵，根据数据量和所需GPU数量的不同，训练一个模型可能需要花费数万美元。'
- en: '**Requires curated evaluations**: Additionally, you will need to create your
    own evaluations to make sure the model is performing well in the new domain you
    are teaching it.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**需要精心设计的评估**：此外，你需要创建自己的评估方法，以确保模型在你所教的新领域中表现良好。'
- en: '**Typically requires subsequent IFT**: For most use cases, you would still
    need to perform IFT on the model once CPT finishes so that the final model can
    properly respond to questions or chats. This ultimately increases the time and
    cost until you have a model ready for use.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通常需要后续的IFT**：对于大多数使用案例，在CPT完成后，你仍然需要对模型进行IFT，以便最终模型能够正确地回答问题或进行对话。这最终会增加时间和成本，直到你得到一个可以使用的模型。'
- en: '**Applications**: For industries with **highly domain specific content like
    healthcare or legal, CPT may be a great option for introducing new topics** to
    the model. With tools like [Mosaic AI’s Fine-Tuning API](https://docs.mosaicml.com/projects/mcli/en/latest/finetuning/finetuning.html)
    you can easily get started with CPT, all you need is a series of text files you
    want to use for training. For the CPT process, all the text files will be concatenated
    with a separation token between each of the documents, Mosaic handles the complexity
    behind the scenes for how these files get fed to the model for training.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**应用场景**：对于那些**具有高度领域特定内容的行业，如医疗或法律，CPT可能是向模型介绍新话题的一个不错选择**。使用像[Mosaic AI的微调API](https://docs.mosaicml.com/projects/mcli/en/latest/finetuning/finetuning.html)这样的工具，你可以轻松开始CPT，只需要一系列用于训练的文本文件。对于CPT过程，所有文本文件将被连接在一起，并在每个文档之间加上分隔符，Mosaic会在幕后处理如何将这些文件输入模型进行训练的复杂性。'
- en: As an example, let’s say we used CPT with a series of text files about responsible
    AI and AI policies. If I prompt the model to “Tell me three principles important
    to Responsible AI”, I would likely get a response with a high probability to follow
    the sentence I prompted like “I need to understand the key Responsible AI principles
    so I can train an effective model”. Although this response is related to my prompt,
    it does not directly answer the question. This demonstrates the need for IFT to
    refine the models instruction following capabilities.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，假设我们使用CPT处理一系列关于负责任的人工智能和AI政策的文本文件。如果我提示模型“告诉我负责任的人工智能中三个重要的原则”，我很可能会得到一个与我提示内容高度相关的回答，比如“我需要理解负责任人工智能的关键原则，以便我能训练一个有效的模型”。尽管这个回答与我的提示有关，但它并没有直接回答问题。这表明需要通过IFT来改进模型的指令跟随能力。
- en: '![](../Images/3711cba668b5294676a57c2a5cc54cba.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3711cba668b5294676a57c2a5cc54cba.png)'
- en: 'Image by author inspired by [Continual Learning for Large Language Models:
    A Survey](https://arxiv.org/pdf/2402.01364)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由作者提供，灵感来源于[大规模语言模型的持续学习：一项综述](https://arxiv.org/pdf/2402.01364)
- en: Tailoring Responses with Instruction Fine-Tuning (IFT)
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过指令微调（IFT）定制回答
- en: '**Overview:** IFT is used to teach a model how to perform a particular task.
    It typically requires thousands of examples and can be used for a specific purpose
    such as improving question answering, extracting key information, or adopting
    a certain tone.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**概述**：IFT用于教导模型如何执行特定任务。通常需要成千上万的示例，并且可以用于特定目的，如改善问答、提取关键信息或采用某种语气。'
- en: '**Pros**:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**：'
- en: '**Speed and cost-effectiveness**: IFT takes significantly less time to complete,
    this type of training can be achieved in minutes making it not only faster, but
    much cheaper compared to pre-training or CPT.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速度和成本效益**：IFT完成的时间显著更短，这种类型的训练可以在几分钟内实现，使其不仅更快，而且相比预训练或CPT更便宜。'
- en: '**Task-specific customization**: This is a great method to get tailored results
    out of the model by guiding it to respond in a particular tone, classify documents,
    revise certain documents, and more.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务特定定制**：这是一种很好的方法，可以通过指导模型以特定的语气响应、分类文档、修订某些文档等方式，获得定制化的结果。'
- en: '**Cons**:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**：'
- en: '**Requires labeled dataset**: IFT needs labeled data to teach the model how
    it should behave. While there are many open-source datasets available, it may
    take time to properly create and label a dataset for your unique use case.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**需要标注数据集：** IFT需要标注数据来教模型如何应对特定任务。虽然有许多开源数据集可供使用，但根据你特定的应用场景，创建和标注一个合适的数据集可能需要一些时间。  '
- en: '**Potential decrease in general capabilities**: Introducing new skills through
    IFT may reduce the model’s performance on general tasks. If you are concerned
    about maintaining the model’s ability to generalize, you may want to include examples
    of general skills in your training and evaluation set this way you can measure
    performance on the general tasks as well as the new skill(s) you are teaching.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**潜在的通用能力下降：** 通过IFT引入新技能可能会降低模型在通用任务上的表现。如果你担心保持模型的泛化能力，可以在训练和评估集中特意包括通用技能的示例，这样你可以在测试新技能的同时，衡量模型在通用任务上的表现。  '
- en: '**Applications**: IFT helps the model **perform particular tasks like question
    answering** much better. Using the prompt “Tell me three principles important
    to Responsible AI”, a model that had undergone IFT would likely respond with an
    answer to the question like “Responsible AI is critical for ensuring the ethical
    use of models grounded in core principles like transparency, fairness, and privacy.
    Following responsible AI principles helps align the solution with broader societal
    values and ethical standards”. This response is more useful to the end user compared
    to a response that may come from a CPT or PT model only since it addresses the
    question directly.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**应用：** IFT帮助模型**更好地执行特定任务，如问答**。例如，使用提示语“告诉我三个对负责任的人工智能很重要的原则”，经过IFT训练的模型可能会回应类似这样的回答：“负责任的人工智能对于确保模型的道德使用至关重要，基于透明性、公平性和隐私等核心原则进行构建。遵循负责任的AI原则有助于将解决方案与更广泛的社会价值和道德标准对齐。”与仅由CPT或PT模型生成的回答相比，这种回应对最终用户更具实用性，因为它直接解答了问题。'
- en: '*Note that there are a variety of fine-tuning approaches and techniques designed
    to improve the overall model performance and reduce both time and cost associated
    with training.*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*请注意，存在多种微调方法和技术，旨在提高整体模型性能并减少训练过程中的时间和成本。*  '
- en: '![](../Images/dd9e755d6ab5ba83cf8ba9a63559b189.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd9e755d6ab5ba83cf8ba9a63559b189.png)  '
- en: 'Image by author. Inspired by [Continual Learning for Large Language Models:
    A Survey](https://arxiv.org/pdf/2402.01364)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '图片来源：作者。灵感来源于[《大规模语言模型的持续学习：一项综述》](https://arxiv.org/pdf/2402.01364)  '
- en: Finding real-time or private information with Retrieval Augmented Generation
    (RAG)
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '使用检索增强生成（RAG）技术查找实时或私密信息  '
- en: '**Overview:** RAG enables language models to answer questions using information
    outside of their training data. In the RAG process, a user query triggers a retrieval
    of relevant information from a vector index, which is then integrated into a new
    prompt along with the original query to generate a response. This technique is
    one of the most common techniques used today due to its effectiveness and simplicity.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**概述：** RAG使语言模型能够使用训练数据之外的信息回答问题。在RAG过程中，用户查询触发从向量索引中检索相关信息，随后将这些信息与原始查询一起集成到新的提示中，以生成响应。由于其有效性和简便性，这一技术是当前最常用的技术之一。  '
- en: '**Pros**:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点：**  '
- en: '**Access to real-time information & information beyond training data**: RAG
    allows models to utilize query information from diverse and constantly updated
    sources like the internet or internal document datastores. Anything that can be
    stored in a vector index or retrieved via a plugin/tool, can be used in the RAG
    process.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**访问实时信息和训练数据之外的信息：** RAG允许模型利用来自不同且不断更新的来源（如互联网或内部文档数据库）的查询信息。任何可以存储在向量索引中或通过插件/工具检索的信息，都可以在RAG过程中使用。  '
- en: '**Ease of implementation**: RAG does not require custom training making it
    both cost-effective and straightforward to get started. It’s also a very well
    documented and researched area with many articles providing insights on how to
    improve responses from RAG systems.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实现简便：** RAG不需要自定义训练，这使其既具有成本效益，又易于上手。它也是一个非常有文献支持和研究的领域，许多文章提供了关于如何改善RAG系统响应的见解。  '
- en: '**Traceability and citations**: All generated responses can include citations
    for which documents were used to answer the query making it easy for the user
    to verify the information and understand how the response was generated. Since
    you know exactly what information got sent to the model to answer the question,
    it’s easy to provide a traceable answers to the end user, and if needed the end
    user can look at the referenced documents for more information. In comparison,
    if you are querying a model directly, it’s difficult to know how it answered that
    question or what references were used to generate the response.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可追溯性和引用**：所有生成的响应可以包含用于回答查询的文档引用，这使得用户能够轻松验证信息并了解回答是如何生成的。由于你清楚知道哪些信息被发送给模型以回答问题，因此很容易向最终用户提供可追溯的答案，如果需要，最终用户还可以查看引用的文档以获取更多信息。相比之下，如果你直接查询模型，很难知道它是如何回答问题的，或者使用了哪些参考资料来生成回答。'
- en: '**Cons**:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**：'
- en: '**Context window limitations**: The first major problem is the context windows
    of different models, some models like GPT-4 and 4o have 128k token context window,
    while the Llama-3 series is still only 8k tokens. With smaller context windows,
    you cannot pass as much information to the model to answer the question. As a
    result, it becomes more important to have robust chunking and chunk re-ranking
    techniques in place so you can retreive the right context and use this to respond
    to the user correctly.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文窗口的限制**：第一个主要问题是不同模型的上下文窗口大小，例如，GPT-4和4o有128k令牌的上下文窗口，而Llama-3系列仍然只有8k令牌。上下文窗口较小意味着你无法向模型传递足够的信息来回答问题。因此，拥有强大的文本分块和分块重排名技术变得尤为重要，这样你可以检索到正确的上下文，并利用这些上下文正确地回应用户。'
- en: '**The “Lost in the Middle Problem”**: Even with longer context windows, there
    is a common “lost in the middle problem” where models tend to pay more attention
    to information at the beginning or end of the prompt, meaning that if the answer
    to the question lies in the middle of the context, the model may still answer
    incorrectly even when presented with all the information needed to answer the
    question. Similarly, the models might mix up information they’ve retrieved and
    answer the question only partially correct. For example, I’ve seen when asking
    a model to find information about two companies and return their point of view
    on AI, the model has on occasion mixed up the companies policies.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**“中间丢失问题”**：即使使用更长的上下文窗口，仍然存在一个常见的“中间丢失问题”，即模型往往更关注提示中的开头或结尾信息，这意味着如果问题的答案位于上下文的中间，即使提供了回答问题所需的所有信息，模型仍然可能会回答错误。类似地，模型可能会混淆它们检索到的信息，导致只部分正确地回答问题。例如，我曾见过在要求模型查找关于两家公司的信息并返回它们对人工智能的看法时，模型偶尔会混淆两家公司的政策。'
- en: '**Top K Retrieval Challenge**: In typical RAG pipelines, only the top K documents
    (or chunks of text) related to the query are retrieved and sent to the model for
    a final response. This pattern yields better results when looking for specific
    details in a document corpus, but often fails to correctly answer exhaustive search
    based questions. For example, the prompt “give me all of the documents related
    to responsible AI” would need additional logic to keep searching through the vector
    index for all responsible AI documents instead of stopping after returning the
    first top K related chunks.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Top K 检索挑战**：在典型的RAG（检索增强生成）管道中，只有与查询相关的前K个文档（或文本块）会被检索并传递给模型以生成最终响应。当需要在文档库中查找特定细节时，这种模式能够取得更好的效果，但在回答需要广泛搜索的问题时往往无法正确回答。例如，提示“给我所有关于负责任的人工智能的文档”需要额外的逻辑来继续在向量索引中搜索所有相关的负责任AI文档，而不是在返回前K个相关块后就停止。'
- en: '**Overly similar documents**: If the vector index contains documents that are
    all semantically similar, it might be difficult for the model to retrieve the
    exact document relevant to the task. This is particularly true in specialized
    domains or domains with uniform language. This may not be a problem in a vector
    index where the content of all the documents is diverse, however, if you’re using
    RAG against an index on medical documents where all the language is very similar
    and not something a typical embedding model would be trained on, it might be harder
    to find the documents / answers you’re looking for.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过于相似的文档**：如果向量索引包含的文档在语义上都非常相似，模型可能很难检索到与任务最相关的具体文档。特别是在专业领域或语言统一的领域中尤为如此。在文档内容多样化的向量索引中可能不会出现这个问题，但如果你在一个包含医学文档的索引中使用RAG，而所有语言都非常相似，且不是典型嵌入模型所训练的内容，那么可能会更难找到你需要的文档/答案。'
- en: '**Applications**: Any use case involving **question and answering over a set
    of documents will typically involve RAG**. It’s a very practical way to get started
    with Generative AI and requires no additional model training. The emerging concept
    of AI Agents also tend to have at least one tool for RAG. Many agent implementations
    will have RAG based tools for different data sources. For example, an internal
    support agent might have access to an HR tool and IT support tool. In this set-up
    there could be a RAG component for both the HR and IT documents, each tool could
    have the same pipeline running behind the scenes, the only difference would be
    the document dataset.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**应用场景**：任何涉及**在一组文档上进行问答的使用案例通常都会涉及RAG**。这是一种非常实用的方式，可以让你快速开始使用生成性人工智能，并且不需要额外的模型训练。新兴的AI代理概念也通常会至少有一个用于RAG的工具。许多代理实现将为不同的数据源提供基于RAG的工具。例如，一个内部支持代理可能能够访问HR工具和IT支持工具。在这种设置中，HR和IT文档都可能有一个RAG组件，每个工具背后可能运行相同的管道，唯一的区别是文档数据集。'
- en: '![](../Images/d6de63bdf1ca0335007f71ee4341b209.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6de63bdf1ca0335007f71ee4341b209.png)'
- en: Image by author. Overview of the RAG process.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像。RAG过程概览。
- en: Improving the R in RAG by Fine-Tunning Embeddings
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过微调嵌入来改进RAG中的R
- en: '**Overview:** Fine-Tuning Embeddings can improve the retrieval component of
    RAG. The goal of fine-tuning embeddings is to push the vector embeddings further
    apart in the vector space, making them more different from one another and therefore
    easier to find the documents most relevant to the question.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**概述**：微调嵌入可以改善RAG的检索组件。微调嵌入的目标是使向量嵌入在向量空间中进一步分开，使它们彼此之间更加不同，从而更容易找到与问题最相关的文档。'
- en: '**Pros**:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**：'
- en: '**Generally cost-effective**: Fine-tuning embeddings is comparatively inexpensive
    when considering other training methods.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通常具有成本效益**：与其他训练方法相比，微调嵌入的成本相对较低。'
- en: '**Domain-specific customization**: This method can be a great option for distinguishing
    text in domains that the underlying embedding model was not as exposed to during
    training. For example, highly specific legal or health care documents may benefit
    from fine-tuning embeddings for those corpuses in their RAG pipeline.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**领域特定的定制化**：这种方法对于区分那些嵌入模型在训练时没有充分接触过的领域的文本非常有效。例如，针对高度专业化的法律或医疗保健文档，可以在RAG管道中为这些语料库微调嵌入。'
- en: '**Cons**:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**：'
- en: '**Requires labeled data & often re-training**: A labeled dataset is needed
    to fine-tune an embedding model. Additionally, you may need to continuously re-train
    the embedding model as you add additional information to your index.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**需要标注数据并且通常需要重新训练**：微调嵌入模型需要一个标注的数据集。此外，在你向索引中添加更多信息时，可能需要持续重新训练嵌入模型。'
- en: '**Additional maintenance across indexes:** Depending on how many data sources
    you’re querying you also might have to keep track of multiple sets of embedding
    models and their related data sources. It’s important to remember that whatever
    embedding model was used to embed the corpus of documents must be the same model
    used to embed the query when it’s time to retrieve relevant information. If you
    are querying against multiple indexes, each embedded using a different embedding
    model, then you’ll need to make sure that your models match at the time of retrieval.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跨索引的额外维护**：根据你查询的数据源数量，你可能还需要跟踪多个嵌入模型及其相关数据源。需要记住的是，无论使用哪个嵌入模型来嵌入文档语料库，检索时用于嵌入查询的模型必须是相同的。如果你查询的是多个索引，并且每个索引使用不同的嵌入模型，那么你需要确保在检索时你的模型是一致的。'
- en: '**Applications**: Fine-tuning embeddings is a **great option if the traditional
    RAG approach is not effective because the documents in your index are too similar
    to one another**. By fine-tuning the embeddings you can teach the model to differentiate
    better between domain specific concepts and improve your RAG results.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**应用**：如果传统的RAG方法效果不佳，因为索引中的文档彼此过于相似，微调嵌入是一个**很好的选择**。通过微调嵌入，你可以教会模型更好地区分领域特定的概念，并改进你的RAG结果。'
- en: Talking to Models with Prompt Engineering
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与模型进行提示工程对话
- en: '**Overview:** Prompt engineering is the most common way to interact with Generative
    AI models, it is merely sending a message to the model that’s designed to get
    the output you want. It can be as simple as “Tell me a story about a German Shepherd”
    or it can be incredibly complicated with particular details regarding what you’d
    like the model to do.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**概述**：提示工程是与生成型AI模型互动的最常见方式，它只是向模型发送一条旨在获得你想要输出的信息的消息。它可以像“讲一个关于德国牧羊犬的故事”那么简单，或者它也可以非常复杂，涉及你希望模型执行的具体细节。'
- en: '**Pros**:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**：'
- en: '**Immediate results**: Experimenting with different prompts can be done anytime
    you have access to a language model and results are returned in seconds (or less)!
    As soon as the idea hits, you can begin working on refining a prompt until the
    model gives the desired response.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**即时结果**：使用不同的提示进行实验，可以在任何时候访问语言模型时进行，并且结果在几秒钟内（甚至更少）就能返回！一旦灵感来了，你可以开始优化提示，直到模型给出期望的回应。'
- en: '**High performance on general tasks**: Prompt engineering alone works great
    for generic tasks that do not require any retrieval of business specific information
    or real-time information.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在通用任务中的高性能**：仅靠提示工程对于不需要获取特定业务信息或实时信息的通用任务非常有效。'
- en: '**Compatibility with other techniques**: It will work with models that have
    been pre-trained, continuously pre-trained, or fine-tuned, and it can be used
    in conjunction with RAG making it the most used and versatile of the approaches.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与其他技术的兼容性**：它适用于已经预训练、持续预训练或微调的模型，并且可以与RAG结合使用，使其成为最常用和最具多样性的方式。'
- en: '**Cons**:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**：'
- en: '**Limited capability on its own**: Prompt engineering alone is typically not
    enough to get the model to perform how you want. In most cases, people want the
    model to interact with some external data whether it’s a document database, API
    call, or SQL table, all of which will need to combine prompt engineering with
    RAG or other specialized tool calling.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单独能力有限**：仅仅依赖提示工程通常不足以让模型按你希望的方式表现。大多数情况下，人们希望模型与某些外部数据交互，无论是文档数据库、API调用还是SQL表，这些都需要将提示工程与RAG或其他专门的工具调用结合使用。'
- en: '**Precision challenges**: Writing the perfect prompt can be challenging and
    often requires a lot of tweaking until the model performs as intended. The prompt
    that works great with one model might fail miserably with another, requiring lots
    of iterations and experimentation across many models and prompt variations.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精度挑战**：编写完美的提示可能具有挑战性，通常需要大量的调整，直到模型按预期表现为止。一个在某个模型上效果很好提示可能在另一个模型上失败，要求在多个模型和提示变体之间进行大量的迭代和实验。'
- en: '**Applications**: Prompt Engineering will be **used in combination with all
    of the aforementioned techniques to produce the intended response**. There are
    many different techniques for prompt engineering to help steer the model in the
    right direction. For more information on these techniques I recommend this [Prompt
    Engineering Guide from Microsoft](https://developer.microsoft.com/en-us/reactor/events/22001/)
    they give a variety of examples from Chain-of-Thought prompting and beyond.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**应用**：提示工程将**与上述所有技术结合使用，以产生预期的回应**。有许多不同的提示工程技术可以帮助将模型引导到正确的方向。如需更多关于这些技术的信息，我推荐这篇[微软的提示工程指南](https://developer.microsoft.com/en-us/reactor/events/22001/)，他们提供了从链式思维提示到更复杂技巧的多种示例。'
- en: '![](../Images/4d0ece3e50b7decdc6a47529331eb595.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d0ece3e50b7decdc6a47529331eb595.png)'
- en: Image by author. Overview of Prompt Engineering Process.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片。提示工程流程概览。
- en: Conclusion
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Generative AI technology is changing and improving everyday. Most applications
    will require leveraging a variety of the techniques described in this article.
    Getting started with existing language models that have been fine-tuned for instruction
    or chat capabilities and focusing on prompt engineering and RAG is a great place
    to start! From here finding more tailored use cases that require fine-tuning/instruction
    fine-tuning can provide even greater benefits.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能技术每天都在变化和进步。大多数应用程序将需要利用本文中描述的各种技术。开始使用已经针对指令或对话能力进行微调的现有语言模型，并重点关注提示工程和RAG，是一个很好的起点！从这里开始，寻找更多需要微调/指令微调的定制用例，可以带来更大的好处。
- en: Looking ahead, AI agents offer a promising way to take advantage of the latest
    advancements in both closed and open-source models that have been pre-trained
    on tons of public data and fine-tuned for chat/instruction following. When given
    the right tools, these agents can perform incredible tasks on your behalf from
    information retrieval with RAG to helping plan company events or vacations.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 展望未来，AI代理提供了一种有前景的方式，可以利用最新的进展，包括在大量公共数据上预训练并针对对话/指令跟随进行了微调的闭源和开源模型。只要拥有合适的工具，这些代理可以代你执行令人惊叹的任务，从使用RAG进行信息检索到帮助规划公司活动或假期。
- en: Additionally, we can expect to see a proliferation of more domain specific models
    as organizations with lots of specialized data begin pre-training their own models.
    For instance, companies like [Harvey](https://www.harvey.ai/) are already developing
    tailored AI solutions that can handle the unique demands of the legal industry.
    This trend will likely continue, leading to highly specialized models that deliver
    even more precise and relevant results in various fields.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，随着拥有大量专业数据的组织开始预训练自己的模型，我们可以预期会看到更多领域特定模型的出现。例如，像[Harvey](https://www.harvey.ai/)这样的公司已经在开发量身定制的人工智能解决方案，能够处理法律行业的独特需求。这一趋势可能会持续下去，导致高度专业化的模型在各个领域提供更精确和相关的结果。
- en: By combining the strengths of different AI techniques and leveraging the power
    of AI agents and domain-specific models, organizations can unlock the full potential
    of Generative AI.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合不同AI技术的优势，利用AI代理和领域特定模型的力量，组织可以释放生成式AI的全部潜力。
- en: Additional References
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他参考文献
- en: '[Gartner Survey Finds Generative AI Is Now the Most Frequently Deployed AI
    Solution in Organizations](https://www.gartner.com/en/newsroom/press-releases/2024-05-07-gartner-survey-finds-generative-ai-is-now-the-most-frequently-deployed-ai-solution-in-organizations)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Gartner调查发现生成式AI现已成为组织中最常部署的AI解决方案](https://www.gartner.com/en/newsroom/press-releases/2024-05-07-gartner-survey-finds-generative-ai-is-now-the-most-frequently-deployed-ai-solution-in-organizations)'
- en: '[Instruction Tuning for Large Language Models: A Survey](https://arxiv.org/abs/2308.10792v5)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[大型语言模型的指令调优：一项调查](https://arxiv.org/abs/2308.10792v5)'
- en: '[Continual Pre-Training of Large Language Models: How to (re)warm your model?](https://arxiv.org/pdf/2308.04014)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[大型语言模型的持续预训练：如何（重新）加热你的模型？](https://arxiv.org/pdf/2308.04014)'
- en: '[Continual Learning for Large Language Models: A Survey](https://arxiv.org/abs/2402.01364)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[大型语言模型的持续学习：一项调查](https://arxiv.org/abs/2402.01364)'
- en: '[Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2307.03172)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[困在中间：语言模型如何使用长上下文](https://arxiv.org/abs/2307.03172)'
- en: '[Mosaic AI Fine-Tuning Documentation](https://docs.mosaicml.com/projects/mcli/en/latest/finetuning/finetuning.html)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Mosaic AI 微调文档](https://docs.mosaicml.com/projects/mcli/en/latest/finetuning/finetuning.html)'
- en: '[Prompt Engineering Guide / Recording from Microsoft](https://developer.microsoft.com/en-us/reactor/events/22001/)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Microsoft的提示工程指南 / 录音](https://developer.microsoft.com/en-us/reactor/events/22001/)'
- en: '[Prompt Engineering Guide from Anthropic](https://docs.anthropic.com/en/docs/prompt-engineering)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Anthropic的提示工程指南](https://docs.anthropic.com/en/docs/prompt-engineering)'
- en: '[What is the Cost of Training Large Language Models from Cudo Compute](https://www.cudocompute.com/blog/what-is-the-cost-of-training-large-language-models)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Cudo Compute: 训练大型语言模型的成本是多少](https://www.cudocompute.com/blog/what-is-the-cost-of-training-large-language-models)'
- en: '*Interested in discussing further or collaborating? Reach out on* [*LinkedIn*](https://www.linkedin.com/in/tula-masterman/)*!*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*有兴趣进一步讨论或合作吗？请通过* [*LinkedIn*](https://www.linkedin.com/in/tula-masterman/)
    *与我联系！*'
