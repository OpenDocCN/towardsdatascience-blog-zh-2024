- en: 'Courage to Learn ML: A Detailed Exploration of Gradient Descent and Popular
    Optimizers'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习机器学习的勇气：梯度下降与流行优化器的详细探索
- en: 原文：[https://towardsdatascience.com/courage-to-learn-ml-a-detailed-exploration-of-gradient-descent-and-popular-optimizers-022ecf97be7d?source=collection_archive---------4-----------------------#2024-01-09](https://towardsdatascience.com/courage-to-learn-ml-a-detailed-exploration-of-gradient-descent-and-popular-optimizers-022ecf97be7d?source=collection_archive---------4-----------------------#2024-01-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/courage-to-learn-ml-a-detailed-exploration-of-gradient-descent-and-popular-optimizers-022ecf97be7d?source=collection_archive---------4-----------------------#2024-01-09](https://towardsdatascience.com/courage-to-learn-ml-a-detailed-exploration-of-gradient-descent-and-popular-optimizers-022ecf97be7d?source=collection_archive---------4-----------------------#2024-01-09)
- en: Are You Truly Mastering Gradient Descent? Use This Post as Your Ultimate Checkpoint
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你真的掌握了梯度下降吗？将这篇文章作为你的终极检查点
- en: '[](https://amyma101.medium.com/?source=post_page---byline--022ecf97be7d--------------------------------)[![Amy
    Ma](../Images/2edf55456a1f92724535a1441fa2bef5.png)](https://amyma101.medium.com/?source=post_page---byline--022ecf97be7d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--022ecf97be7d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--022ecf97be7d--------------------------------)
    [Amy Ma](https://amyma101.medium.com/?source=post_page---byline--022ecf97be7d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://amyma101.medium.com/?source=post_page---byline--022ecf97be7d--------------------------------)[![Amy
    Ma](../Images/2edf55456a1f92724535a1441fa2bef5.png)](https://amyma101.medium.com/?source=post_page---byline--022ecf97be7d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--022ecf97be7d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--022ecf97be7d--------------------------------)
    [Amy Ma](https://amyma101.medium.com/?source=post_page---byline--022ecf97be7d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--022ecf97be7d--------------------------------)
    ·22 min read·Jan 9, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[数据科学探索](https://towardsdatascience.com/?source=post_page---byline--022ecf97be7d--------------------------------)
    ·阅读时间22分钟·2024年1月9日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/fdbe85b9e568d392e02e271ea983476d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fdbe85b9e568d392e02e271ea983476d.png)'
- en: We will use an RPG game as an analogy today. Created By ChatGPT
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们今天将使用RPG游戏作为类比。由ChatGPT创建
- en: Welcome back to a new chapter of ‘[Courage to Learn ML](https://towardsdatascience.com/tagged/courage-to-learn-ml).
    For those new to this series, this series aims to make these complex topics accessible
    and engaging, much like a casual conversation between a mentor and a learner,
    inspired by the writing style of “[The Courage to Be Disliked](https://www.goodreads.com/book/show/43306206-the-courage-to-be-disliked),”
    with a specific focus on machine learning.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎回到《[学习机器学习的勇气](https://towardsdatascience.com/tagged/courage-to-learn-ml)》的新章节。对于那些新加入这一系列文章的读者，本系列旨在使这些复杂的主题变得易于理解并富有趣味，就像导师与学习者之间的轻松对话，灵感来源于《[被讨厌的勇气](https://www.goodreads.com/book/show/43306206-the-courage-to-be-disliked)》的写作风格，专注于机器学习。
- en: 'In our previous discussions, our mentor and learner discussed about some common
    loss functions and the three fundamental principles of designing loss functions.
    Today, they’ll explore another key concept: gradient descent.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的讨论中，我们的导师和学习者讨论了一些常见的损失函数以及设计损失函数的三个基本原则。今天，他们将探讨另一个关键概念：梯度下降。
- en: 'As always, here’s a list of the topics we’ll be exploring today:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，以下是我们今天将要探索的主题列表：
- en: What exactly is a gradient, and why is the technique called ‘gradient descent’?
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度究竟是什么？为什么这种技术被称为“梯度下降”？
- en: Why doesn’t vanilla gradient descent perform well in Deep Neural Networks (DNNs),
    and what are the improvements?
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么普通的梯度下降在深度神经网络（DNNs）中表现不佳？有什么改进措施？
- en: A review of various optimizers and their relationships (Newton’s method, Adagrad,
    Momentum, RMSprop, and Adam)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回顾各种优化器及其关系（牛顿法、Adagrad、Momentum、RMSprop和Adam）
- en: Practical insights on selecting the right optimizer based on my personal experience
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于我个人经验，关于选择合适优化器的实用见解
