- en: 'The Rise of Pallas: Unlocking TPU Potential with Custom Kernels'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《帕拉斯的崛起：通过自定义内核释放TPU的潜力》
- en: 原文：[https://towardsdatascience.com/the-rise-of-pallas-unlocking-tpu-potential-with-custom-kernels-67be10ab846a?source=collection_archive---------5-----------------------#2024-10-06](https://towardsdatascience.com/the-rise-of-pallas-unlocking-tpu-potential-with-custom-kernels-67be10ab846a?source=collection_archive---------5-----------------------#2024-10-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-rise-of-pallas-unlocking-tpu-potential-with-custom-kernels-67be10ab846a?source=collection_archive---------5-----------------------#2024-10-06](https://towardsdatascience.com/the-rise-of-pallas-unlocking-tpu-potential-with-custom-kernels-67be10ab846a?source=collection_archive---------5-----------------------#2024-10-06)
- en: Accelerating AI/ML Model Training with Custom Operators — Part 3
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加速AI/ML模型训练与自定义操作符 — 第三部分
- en: '[](https://chaimrand.medium.com/?source=post_page---byline--67be10ab846a--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page---byline--67be10ab846a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--67be10ab846a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--67be10ab846a--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page---byline--67be10ab846a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://chaimrand.medium.com/?source=post_page---byline--67be10ab846a--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page---byline--67be10ab846a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--67be10ab846a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--67be10ab846a--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page---byline--67be10ab846a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--67be10ab846a--------------------------------)
    ·15 min read·Oct 6, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布在[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--67be10ab846a--------------------------------)
    ·阅读时间15分钟·2024年10月6日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/b7b6e80b948d3befa2dd1c167781b063.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7b6e80b948d3befa2dd1c167781b063.png)'
- en: Photo by [Hendrik Morkel](https://unsplash.com/@hendrikmorkel?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Hendrik Morkel](https://unsplash.com/@hendrikmorkel?utm_source=medium&utm_medium=referral)提供，发布在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: This is the third part of a [series of posts](https://chaimrand.medium.com/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12)
    on the topic of building custom operators for optimizing AI/ML workloads. In our
    [previous post](https://chaimrand.medium.com/unleashing-the-power-of-triton-mastering-gpu-kernel-optimization-in-python-160a3f52701e)
    we demonstrated the simplicity and accessibility of Triton. Named for the [Greek
    god of the sea](https://en.wikipedia.org/wiki/Triton_(mythology)), Triton empowers
    Python developers to increase their control over the GPU and optimize its use
    for the specific workload at hand. In this post we move one step down the lineage
    of Greek mythology to Triton’s daughter, [Pallas](https://en.wikipedia.org/wiki/Pallas_(daughter_of_Triton))
    and discuss her namesake, the [JAX extension](https://jax.readthedocs.io/en/latest/pallas/index.html)
    for writing custom kernels for GPU and TPU.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这是关于构建自定义操作符以优化AI/ML工作负载的[系列文章](https://chaimrand.medium.com/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12)的第三部分。在我们[上一篇文章](https://chaimrand.medium.com/unleashing-the-power-of-triton-mastering-gpu-kernel-optimization-in-python-160a3f52701e)中，我们展示了Triton的简易性和可访问性。Triton得名于[海神特里同](https://en.wikipedia.org/wiki/Triton_(mythology))，它使得Python开发者能够增加对GPU的控制，并优化其在特定工作负载中的使用。在本文中，我们将继续深入希腊神话的脉络，讲述特里同的女儿[帕拉斯](https://en.wikipedia.org/wiki/Pallas_(daughter_of_Triton))，并讨论与她同名的[JAX扩展](https://jax.readthedocs.io/en/latest/pallas/index.html)，该扩展用于为GPU和TPU编写自定义内核。
- en: One of the most important features of NVIDIA GPUs — and a significant factor
    in their rise to prominence — is their programmability. A key ingredient of the
    GPU offering are frameworks for creating General-Purpose GPU (GPGPU) operators,
    such as [CUDA](https://developer.nvidia.com/cuda-toolkit) and [Triton](https://triton-lang.org/main/index.html).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA GPU的最重要特性之一——也是它们崛起为主流的关键因素——就是它们的可编程性。GPU提供的一个关键组件是用于创建通用GPU（GPGPU）操作符的框架，例如[CUDA](https://developer.nvidia.com/cuda-toolkit)和[Triton](https://triton-lang.org/main/index.html)。
- en: In previous posts (e.g., [here](/tpu-training-6eb84100d138)) we discussed the
    opportunity for running ML workloads on [Google TPUs](/tpu-training-6eb84100d138)
    and the potential for a meaningful increase in price performance and a reduction
    in training costs. One of the disadvantages that we noted at the time was the
    absence of tools for creating custom operators. As a result, models requiring
    unique operators that were either unsupported by the underlying ML framework (e.g.,
    TensorFlow/XLA) or implemented in a suboptimal manner, would underperform on TPU
    compared to GPU. This development gap was particularly noticeable over the past
    few years with the frequent introduction of newer and faster solutions for computing
    [attention](https://en.wikipedia.org/wiki/Attention_(machine_learning)) on GPU.
    Enabled by GPU kernel development frameworks, these led to a significant improvement
    in the efficiency of [transformer models](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的文章中（例如，[这里](/tpu-training-6eb84100d138)），我们讨论了在[Google TPU](/tpu-training-6eb84100d138)上运行ML工作负载的机会，并指出在价格性能和训练成本上可能有显著的提高。我们当时注意到的一个缺点是缺乏创建自定义操作符的工具。因此，某些需要特定操作符的模型，如果这些操作符在底层ML框架（例如TensorFlow/XLA）中不受支持或实现不理想，相比于GPU，在TPU上的性能会较差。随着近年来越来越多的新型快速计算解决方案在GPU上用于计算[注意力机制](https://en.wikipedia.org/wiki/Attention_(machine_learning))，这种发展差距尤其显著。得益于GPU内核开发框架，这些新方案显著提高了[transformer模型](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture))的效率。
- en: On TPUs, on the other hand, the lack of appropriate tooling prevented this innovation
    and transformer models were stuck with the attention mechanisms that were supported
    by the official SW stack. Fortunately, with the advent of [Pallas](https://jax.readthedocs.io/en/latest/pallas/index.html)
    this gap has been addressed. Built as an extension to [JAX](https://jax.readthedocs.io/en/latest/)
    and with [dedicated support](https://github.com/pytorch/xla/blob/3c59087e894013559b58dcb147869c4a81ca07d3/docs/pallas.md#adopt-the-above-kernel-to-be-compatible-with-pytorchxla)
    for [PyTorch/XLA](https://pytorch.org/xla/release/r2.4/index.html), Pallas enables
    the creation of custom kernels for GPU and TPU. For its GPU support Pallas utilizes
    Triton, and for its TPU support it uses a library called Mosaic. Although we will
    focus on custom kernels for TPU, it is worth noting that when developing in JAX,
    GPU kernel customization with Pallas offers some advantages over Triton (e.g.,
    see [here](https://www.youtube.com/watch?v=OR8NZyTz-yo)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在TPU上，缺乏合适的工具阻碍了这一创新，transformer模型只能使用官方软件堆栈支持的注意力机制。幸运的是，随着[Pallas](https://jax.readthedocs.io/en/latest/pallas/index.html)的问世，这一差距得到了弥补。Pallas作为[JAX](https://jax.readthedocs.io/en/latest/)的扩展，并为[PyTorch/XLA](https://pytorch.org/xla/release/r2.4/index.html)提供[专门支持](https://github.com/pytorch/xla/blob/3c59087e894013559b58dcb147869c4a81ca07d3/docs/pallas.md#adopt-the-above-kernel-to-be-compatible-with-pytorchxla)，使得为GPU和TPU创建自定义内核成为可能。对于GPU支持，Pallas利用了Triton，而对于TPU支持，它使用了一个名为Mosaic的库。尽管我们将重点讨论TPU的自定义内核，但值得注意的是，在JAX开发中，使用Pallas进行GPU内核定制比Triton有一些优势（例如，见[这里](https://www.youtube.com/watch?v=OR8NZyTz-yo)）。
- en: Our intention in this post is to draw attention to Pallas and demonstrate its
    potential. Please do not view this post as a replacement for the official [Pallas
    documentation](https://jax.readthedocs.io/en/latest/pallas/index.html#). The examples
    we will share were chosen for demonstrative purposes, only. We have made no effort
    to optimize these or verify their robustness, durability, or accuracy.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的目的是引起大家对Pallas的关注并展示其潜力。请不要将本文视为官方[Pallas文档](https://jax.readthedocs.io/en/latest/pallas/index.html#)的替代品。我们将分享的示例仅用于演示目的，未对其进行优化，也没有验证其稳定性、持久性或准确性。
- en: Importantly, at the time of this writing Pallas is an *experimental* feature
    and still under active development. The samples we share (which are based on [JAX](https://pypi.org/project/jax/)
    version 0.4.32 and [PyTorch](https://pypi.org/project/torch/) version 2.4.1) may
    become outdated by the time you read this. Be sure to use the most up-to-date
    APIs and resources available for your Pallas development.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，在撰写本文时，Pallas仍然是一个*实验性*特性，仍在积极开发中。我们分享的示例（基于[JAX](https://pypi.org/project/jax/)版本0.4.32和[PyTorch](https://pypi.org/project/torch/)版本2.4.1）可能在您阅读时已经过时。务必使用最新的API和资源进行Pallas开发。
- en: Many thanks to [Yitzhak Levi](https://www.linkedin.com/in/yitzhak-levi-49a217201/)
    for his contributions to this post.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢[Yitzhak Levi](https://www.linkedin.com/in/yitzhak-levi-49a217201/)为本篇文章的贡献。
- en: Environment Setup
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 环境设置
- en: 'For the experiments described below we use the following [environment setup](https://cloud.google.com/tpu/docs/v5e-training#train-resnet-using-the-pjrt-runtime)
    commands:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于下面描述的实验，我们使用以下[环境设置](https://cloud.google.com/tpu/docs/v5e-training#train-resnet-using-the-pjrt-runtime)命令：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Pallas Kernels for TPU
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TPU的Pallas内核
- en: 'In the toy example of our first post in this series, we distinguished between
    two different ways in which custom kernel development can potentially boost performance.
    The first is by combining (fusing) together multiple operations in a manner that
    reduces the overhead of: 1) loading multiple individual kernels, and 2) reading
    and writing intermediate values (e.g., see [PyTorch’s tutorial on multiply-add
    fusion](https://pytorch.org/tutorials/advanced/cpp_custom_ops.html#cpp-custom-ops-tutorial)).
    The second is by meticulously applying the resources of the underlying accelerator
    in manner that optimizes the function at hand. We briefly discuss these two opportunities
    as they pertain to developing custom TPU kernels and make note of the limitations
    of the Pallas support.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们系列文章的第一个帖子中的玩具示例中，我们区分了自定义内核开发可能提升性能的两种方式。第一种是通过将多个操作（融合）在一起，从而减少以下开销：1）加载多个单独的内核，2）读取和写入中间值（例如，参见[PyTorch关于乘加融合的教程](https://pytorch.org/tutorials/advanced/cpp_custom_ops.html#cpp-custom-ops-tutorial)）。第二种是通过细致地应用底层加速器的资源，以优化当前函数。我们简要讨论了这两种机会，并指出了Pallas支持的局限性。
- en: Operator Fusion on TPU
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TPU上的操作符融合
- en: The TPU is an [XLA](https://openxla.org/xla) (Accelerated Linear Algebra) device,
    i.e., it runs code that has been generated by the [XLA compiler](https://cloud.google.com/tpu/docs/intro-to-tpu#xla_compiler).
    When training an AI model in a frameworks such as [JAX](https://jax.readthedocs.io/en/latest/index.html)
    or [PyTorch/XLA](https://pytorch.org/xla/release/r2.4/index.html), the training
    step is first transformed into an intermediate graph representation (IR). This
    computation graph is then fed to the XLA compiler which converts it into machine
    code that can run on the TPU. Contrary to *eager* execution mode, in which operations
    are executed individually, this mode of running models enables XLA to identify
    and implement opportunities for operator fusion during compilation. And, in fact,
    operator [fusion](https://openxla.org/xla/tf2xla) is the XLA compiler’s most important
    optimization. Naturally, no compiler is perfect and we are certain to come across
    additional opportunities for fusion through custom kernels. But, generally speaking,
    we might expect the opportunity for boosting runtime performance in this manner
    to be lower than in the case of *eager* execution.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: TPU是一个[XLA](https://openxla.org/xla)（加速线性代数）设备，即它运行由[XLA编译器](https://cloud.google.com/tpu/docs/intro-to-tpu#xla_compiler)生成的代码。当在像[JAX](https://jax.readthedocs.io/en/latest/index.html)或[PyTorch/XLA](https://pytorch.org/xla/release/r2.4/index.html)这样的框架中训练AI模型时，训练步骤首先会转化为中间图表示（IR）。然后，将该计算图传递给XLA编译器，后者将其转换为可以在TPU上运行的机器代码。与*急切*执行模式不同，在急切执行模式中操作是单独执行的，而这种模型运行方式使得XLA能够在编译过程中识别并实现操作符融合的机会。事实上，操作符[融合](https://openxla.org/xla/tf2xla)是XLA编译器最重要的优化。自然地，没有任何编译器是完美的，我们一定会通过自定义内核发现更多的融合机会。但一般来说，我们可以预期，通过这种方式提升运行时性能的机会要比*急切*执行模式下少。
- en: Optimizing TPU Utilization
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化TPU利用率
- en: 'Creating optimal kernels for TPU requires a comprehensive and intimate understanding
    of the [TPU system architecture](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm).
    Importantly, TPUs are [very different](https://cloud.google.com/tpu/docs/intro-to-tpu)
    from GPUs: expertise in GPUs and CUDA does not immediately carry over to TPU development.
    For example, while GPUs contain a large number of processors and draw their strength
    from their ability to perform massive parallelization, TPUs are primarily *sequential*
    with dedicated engines for running highly vectorized operations and [support for
    asynchronous scheduling and memory loading](https://jax.readthedocs.io/en/latest/pallas/tpu/pipelining.html).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 创建适用于TPU的优化内核需要对[TPU系统架构](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm)有全面而深入的理解。重要的是，TPU与[GPU](https://cloud.google.com/tpu/docs/intro-to-tpu)有[很大不同](https://cloud.google.com/tpu/docs/intro-to-tpu)：GPU和CUDA方面的专业知识不能直接转移到TPU开发中。例如，虽然GPU包含大量处理器，并且依靠其强大的并行计算能力来提升性能，但TPU主要是*顺序*执行，专门为运行高度向量化的操作而设计的引擎，并且[支持异步调度和内存加载](https://jax.readthedocs.io/en/latest/pallas/tpu/pipelining.html)。
- en: The differences between the underlying architectures of the GPU and TPU can
    have significant implications on how custom kernels should be designed. Mastering
    TPU kernel development requires 1) appropriate overlapping of memory and compute
    operations via [pipelining](https://jax.readthedocs.io/en/latest/pallas/tpu/pipelining.html),
    2) knowing how to mix between the use of the scalar, vector (VPU) and matrix (MXU)
    compute units and their associated scalar and vector registers (SREG and VREG)
    and memory caches (SMEM and VMEM), 3) a comprehension of the [costs of different
    low-level operations](https://jax.readthedocs.io/en/latest/pallas/tpu/details.html#elementwise-operations),
    4) appropriate [megacore configuration](https://jax.readthedocs.io/en/latest/pallas/tpu/pipelining.html#tpus-in-megacore-configuration)
    (on supporting TPU generations), 5) a grasp of the different types of [TPU topologies](https://jax.readthedocs.io/en/latest/pallas/tpu/distributed.html#tpu-topologies)
    and their implications on how to support [distributed computing](https://jax.readthedocs.io/en/latest/pallas/tpu/distributed.html#),
    and more.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: GPU和TPU底层架构的差异可能对自定义内核的设计产生重大影响。掌握TPU内核开发需要：1) 通过[流水线](https://jax.readthedocs.io/en/latest/pallas/tpu/pipelining.html)适当重叠内存和计算操作，2)
    知道如何在标量、向量（VPU）和矩阵（MXU）计算单元及其相关的标量和向量寄存器（SREG和VREG）以及内存缓存（SMEM和VMEM）之间进行混合，3)
    理解[不同低级操作的成本](https://jax.readthedocs.io/en/latest/pallas/tpu/details.html#elementwise-operations)，4)
    适当的[兆核配置](https://jax.readthedocs.io/en/latest/pallas/tpu/pipelining.html#tpus-in-megacore-configuration)（针对支持的TPU代），5)
    掌握不同类型的[TPU拓扑结构](https://jax.readthedocs.io/en/latest/pallas/tpu/distributed.html#tpu-topologies)及其对如何支持[分布式计算](https://jax.readthedocs.io/en/latest/pallas/tpu/distributed.html#)的影响，等等。
- en: Framework Limitations
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 框架限制
- en: While the ability to create custom operators in *Python* using JAX functions
    and APIs greatly increases the simplicity and accessibility of Pallas kernel development,
    it also limits its expressivity. Additionally, (as of the time of this writing)
    there are some JAX APIs that are not supported by Pallas on TPU (e.g., see [here](https://jax.readthedocs.io/en/latest/pallas/tpu/details.html#supported-operations)).
    As a result, you may approach Pallas with the intention of implementing a particular
    operation only to discover that the framework does not support the APIs that you
    need. This is in contrast to frameworks such as CUDA which enable a great deal
    of flexibility when developing custom kernels (for GPU).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然使用JAX函数和API在*Python*中创建自定义操作符的能力大大提高了Pallas内核开发的简易性和可访问性，但它也限制了表达能力。此外，（截至本文撰写时）有些JAX
    API在TPU上不被Pallas支持（例如，请参见[此处](https://jax.readthedocs.io/en/latest/pallas/tpu/details.html#supported-operations)）。因此，您可能会以实现特定操作的目的接触Pallas，结果却发现框架不支持您所需要的API。这与CUDA等框架形成了对比，后者在开发自定义内核时提供了更多的灵活性（针对GPU）。
- en: The [matrix multiplication](https://jax.readthedocs.io/en/latest/pallas/tpu/matmul.html)
    tutorial in the Pallas documentation provides an excellent introduction to Pallas
    kernel development, highlighting the potential for operator fusion and customization
    alongside the challenges involved in optimizing performance (e.g., appropriate
    tuning of the input *block size*). The tutorial clearly illustrates that maximizing
    the *full* potential of the TPU requires a certain degree of specialization. However,
    as we intend to demonstrate, even the novice ML developer can benefit from Pallas
    kernels.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Pallas文档中的[矩阵乘法](https://jax.readthedocs.io/en/latest/pallas/tpu/matmul.html)教程为Pallas内核开发提供了极好的介绍，突出了操作符融合和自定义的潜力，同时也揭示了优化性能所面临的挑战（例如，适当调优输入的*块大小*）。教程清楚地表明，最大化TPU的*完整*潜力需要一定程度的专业化。然而，正如我们所展示的那样，即使是初学者的机器学习开发者也能从Pallas内核中受益。
- en: Integrating the Use of Existing Pallas Kernels
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成现有Pallas内核的使用
- en: To benefit from custom Pallas kernels you do not necessarily need to know how
    to build them. In our first example we demonstrate how you can leverage existing
    Pallas kernels from dedicated public repositories.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要从自定义的Pallas内核中获益，您不一定需要知道如何构建它们。在我们的第一个示例中，我们展示了如何利用来自专用公共仓库的现有Pallas内核。
- en: Example — Flash Attention in Torch/XLA
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例 — 在Torch/XLA中的闪存注意力
- en: The JAX github repository includes implementations of a number of Pallas kernels,
    including [flash attention](https://github.com/jax-ml/jax/blob/jaxlib-v0.4.32/jax/experimental/pallas/ops/tpu/flash_attention.py).
    Here we will demonstrate its use in a Torch/XLA [Vision Transformer](https://en.wikipedia.org/wiki/Vision_transformer)
    (ViT) model. Although Pallas kernels are developed in JAX, they can be adopted
    into Torch/XLA, e.g., via the [make_kernel_from_pallas](https://github.com/pytorch/xla/blob/v2.4.0/torch_xla/experimental/custom_kernel.py#L132)
    utility (see the [documentation](https://github.com/pytorch/xla/blob/3c59087e894013559b58dcb147869c4a81ca07d3/docs/pallas.md)
    for details). In the case of [flash attention](https://github.com/pytorch/xla/blob/v2.4.0/torch_xla/experimental/custom_kernel.py#L425)
    the adoption is implemented by Torch/XLA.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: JAX 的 GitHub 仓库包括多个 Pallas 内核的实现，其中包括 [闪光注意力](https://github.com/jax-ml/jax/blob/jaxlib-v0.4.32/jax/experimental/pallas/ops/tpu/flash_attention.py)。在这里，我们将演示其在
    Torch/XLA [视觉变换器](https://en.wikipedia.org/wiki/Vision_transformer)（ViT）模型中的使用。尽管
    Pallas 内核是用 JAX 开发的，但它们可以被引入 Torch/XLA，例如通过 [make_kernel_from_pallas](https://github.com/pytorch/xla/blob/v2.4.0/torch_xla/experimental/custom_kernel.py#L132)
    工具（有关详细信息，请参见 [文档](https://github.com/pytorch/xla/blob/3c59087e894013559b58dcb147869c4a81ca07d3/docs/pallas.md)）。在
    [闪光注意力](https://github.com/pytorch/xla/blob/v2.4.0/torch_xla/experimental/custom_kernel.py#L425)
    的情况下，Torch/XLA 实现了该功能。
- en: In the following code block we define a stripped down version of the classic
    *timm* [attention block](https://github.com/huggingface/pytorch-image-models/blob/v1.0.9/timm/models/vision_transformer.py#L124)
    with an option to define the underlying attention operator in the constructor.
    We will use this option to compare the performance of the [flash attention](https://github.com/pytorch/xla/blob/v2.4.0/torch_xla/experimental/custom_kernel.py#L425)
    Pallas kernel to its alternatives.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码模块中，我们定义了一个简化版的经典 *timm* [注意力模块](https://github.com/huggingface/pytorch-image-models/blob/v1.0.9/timm/models/vision_transformer.py#L124)，并在构造函数中提供了定义底层注意力操作符的选项。我们将利用这个选项来比较
    [闪光注意力](https://github.com/pytorch/xla/blob/v2.4.0/torch_xla/experimental/custom_kernel.py#L425)
    Pallas 内核与其他替代方案的性能。
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the following block we train a simple ViT-backed classification model using
    the input dataset and attention function (*attn_fn*) of choice.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下模块中，我们使用输入数据集和选择的注意力函数（*attn_fn*）训练一个简单的 ViT 支持的分类模型。
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note the specific configuration we chose for the [VisionTransformer](https://github.com/huggingface/pytorch-image-models/blob/v1.0.9/timm/models/vision_transformer.py#L414).
    This is to comply with certain restrictions (as of the time of this writing) of
    the custom flash attention kernel (e.g., on tensor shapes).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意我们为 [VisionTransformer](https://github.com/huggingface/pytorch-image-models/blob/v1.0.9/timm/models/vision_transformer.py#L414)
    选择的特定配置。这是为了遵守某些限制（截至本文写作时）来自定义闪光注意力内核的要求（例如，关于张量形状的限制）。
- en: 'Finally, we define a dataset and compare the runtimes of training with three
    different attention routines, 1\. using native PyTorch functions, 2\. using PyTorch’s
    built in [SDPA](https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html)
    function, and 3\. using the custom Pallas operator:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义了一个数据集，并比较了三种不同注意力例程的训练运行时间，1. 使用原生 PyTorch 函数，2. 使用 PyTorch 内建的 [SDPA](https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html)
    函数，3. 使用自定义的 Pallas 操作符：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The comparative results are captured in the table below:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 比较结果如表所示：
- en: '![](../Images/6850e69e43e23d9a1964fe7da0551f62.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6850e69e43e23d9a1964fe7da0551f62.png)'
- en: Step time for different attention blocks (lower is better) — by Author
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 不同注意力模块的步骤时间（越低越好） — 作者
- en: 'Although our Pallas kernel clearly underperforms when compared to its alternatives,
    we should not be discouraged:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的 Pallas 内核与其他替代方案相比显得性能较差，但我们不应灰心丧气：
- en: It is likely that these results could be improved with appropriate tuning.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些结果很可能可以通过适当的调优得到改善。
- en: These results are specific to the model and runtime environment that we chose.
    The Pallas kernel may exhibit wholly different comparative results in other use
    cases.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些结果仅适用于我们选择的模型和运行时环境。Pallas 内核在其他使用场景中可能会呈现出完全不同的比较结果。
- en: The real power of Pallas is in the ability to create and adjust low level operators
    to our specific needs. Although runtime performance is important, a 23% performance
    penalty (as in our example) may be a small price to pay for this flexibility.
    Moreover, the opportunity for customization may open up possibilities for optimizations
    that are not supported by the native framework operations.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Pallas的真正强大之处在于能够根据我们的特定需求创建和调整低级操作符。尽管运行时性能很重要，但如我们示例中所示的23%的性能损失，对于这种灵活性来说可能是一个小小的代价。此外，定制化的机会可能会为那些原生框架操作不支持的优化开辟可能性。
- en: Enhancing Existing Kernels
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 增强现有内核
- en: 'Oftentimes it may be easier to tweak an existing Pallas kernel to your specific
    needs, rather than creating one from scratch. This is especially recommended if
    the kernel has already been optimized as performance tuning can be tedious and
    time-consuming. The official [matrix multiplication](https://jax.readthedocs.io/en/latest/pallas/tpu/matmul.html)
    tutorial includes a few examples of how to [extend](https://jax.readthedocs.io/en/latest/pallas/tpu/matmul.html#templating-the-matrix-multiplication)
    and [enhance](https://jax.readthedocs.io/en/latest/pallas/tpu/matmul.html#fused-activation-function)
    an existing kernel. Here we undertake one of the [suggested exercises](https://jax.readthedocs.io/en/latest/pallas/tpu/matmul.html#conclusion):
    we implement `int8` matrix multiplication and assess its performance advantage
    over its `bfloat16` alternative.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 许多时候，调整现有的Pallas内核以满足你的特定需求可能比从零开始创建一个内核更容易。如果内核已经经过优化，那么推荐这种做法，因为性能调优通常是繁琐且耗时的。官方的[矩阵乘法](https://jax.readthedocs.io/en/latest/pallas/tpu/matmul.html)教程包括了一些关于如何[扩展](https://jax.readthedocs.io/en/latest/pallas/tpu/matmul.html#templating-the-matrix-multiplication)和[增强](https://jax.readthedocs.io/en/latest/pallas/tpu/matmul.html#fused-activation-function)现有内核的示例。在这里，我们进行了一项[建议的练习](https://jax.readthedocs.io/en/latest/pallas/tpu/matmul.html#conclusion)：我们实现了`int8`矩阵乘法，并评估其相对于`bfloat16`版本的性能优势。
- en: Example — Int8 Matrix Multiplication
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例 — Int8矩阵乘法
- en: In the code block below we implement an `int8` version of the [matrix multiplication](https://jax.readthedocs.io/en/latest/pallas/tpu/matmul.html)
    example.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码块中，我们实现了一个`int8`版本的[矩阵乘法](https://jax.readthedocs.io/en/latest/pallas/tpu/matmul.html)示例。
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note our use of an `int32` accumulation matrix for addressing the possibility
    of overflow. Also note our use of the [*interpret*](https://github.com/jax-ml/jax/blob/jaxlib-v0.4.32/jax/_src/pallas/pallas_call.py#L1238)flag
    for debugging of Pallas kernels on CPU (as recommended [here](https://youtu.be/OR8NZyTz-yo?t=855)).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用了一个`int32`的累加矩阵，以应对可能出现的溢出问题。还请注意，我们使用了[*解释*](https://github.com/jax-ml/jax/blob/jaxlib-v0.4.32/jax/_src/pallas/pallas_call.py#L1238)标志来调试CPU上的Pallas内核（如[这里](https://youtu.be/OR8NZyTz-yo?t=855)所推荐）。
- en: 'To assess our kernel, we introduce a slight modification to the benchmarking
    utilities defined in the [tutorial](https://jax.readthedocs.io/en/latest/pallas/tpu/matmul.html#performance-of-pipelined-kernels)
    and compare the runtime results to both the jnp.float16 Pallas matmulkernel and
    the built-in JAX [matmul](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.matmul.html)
    API:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们的内核，我们对[教程](https://jax.readthedocs.io/en/latest/pallas/tpu/matmul.html#performance-of-pipelined-kernels)中定义的基准测试工具进行了轻微修改，并将运行时结果与jnp.float16
    Pallas矩阵乘法内核和内置JAX的[matmul](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.matmul.html)
    API进行了比较：
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The results of our experiment are captured in the table below:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实验的结果如下表所示：
- en: '![](../Images/3f08becc69e1ae67e2b6baa93d5f7da2.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f08becc69e1ae67e2b6baa93d5f7da2.png)'
- en: Matmul time and utilization (by Author)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法时间与利用率（作者提供）
- en: By using `int8` matrices (rather than `bfloat16`matrices) on tpuv5e we can boost
    the runtime performance of our custom matrix multiplication kernel by 71%. However,
    as in the case of the [bfloat16 example](https://jax.readthedocs.io/en/latest/pallas/tpu/matmul.html#performance-of-pipelined-kernels),
    additional tuning is required to match the performance of the built-in matmul
    operator. The potential for improvement is highlighted by the drop in system utilization
    when compared to `bfloat16`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在tpuv5e上使用`int8`矩阵（而非`bfloat16`矩阵），我们能够将自定义矩阵乘法内核的运行时性能提高71%。然而，正如在[bfloat16示例](https://jax.readthedocs.io/en/latest/pallas/tpu/matmul.html#performance-of-pipelined-kernels)中所示，仍然需要额外的调优才能达到内置矩阵乘法操作符的性能。与`bfloat16`相比，系统利用率的下降突显了这一改进的潜力。
- en: Creating a Kernel from Scratch
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头创建一个内核
- en: While leveraging existing kernels can be greatly beneficial, it is unlikely
    to solve all of your problems. Inevitably, you may need to implement an operation
    that is either unsupported on TPU or exhibits suboptimal performance. Here we
    demonstrate the creation of a relatively simple pixel-wise kernel. For the sake
    of continuity, we choose the same [Generalized Intersection Over Union (GIOU)](https://giou.stanford.edu/)
    operation as in our [previous posts](/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管利用现有内核可以带来很大好处，但它不太可能解决所有问题。不可避免地，你可能需要实现一个在TPU上不受支持或性能不佳的操作。在这里，我们展示了创建一个相对简单的逐像素内核。为了保持连续性，我们选择了与[之前的文章](/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12)中相同的[广义交并比（GIOU）](https://giou.stanford.edu/)操作。
- en: Example — A GIOU Pallas Kernel
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例 — GIOU Pallas内核
- en: In the code block below we define a Pallas kernel that implements GIOU on pairs
    of batches of bounding boxes, each of dimension *BxNx4* (where we denote the batch
    size by *B* and the number of boxes per sample by *N*). The function returns a
    tensor of scores of dimension *BxN*. We choose a block size of 128 on both the
    *batch* axis and the *boxes* axis, i.e., we divide each of the tensors into blocks
    of *128x128x4* and pass them to our kernel function. The [*grid*](https://jax.readthedocs.io/en/latest/pallas/grid_blockspec.html#grid-a-k-a-kernels-in-a-loop)and
    [BlockSpec *index_map*](https://jax.readthedocs.io/en/latest/pallas/grid_blockspec.html#blockspec-a-k-a-how-to-chunk-up-inputs)
    are defined accordingly.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码块中，我们定义了一个Pallas内核，它对一对批量的边界框进行GIOU操作，每个边界框的维度为*BxNx4*（其中*B*表示批量大小，*N*表示每个样本的框数）。该函数返回一个维度为*BxN*的分数张量。我们在*batch*轴和*boxes*轴上选择了128的块大小，即我们将每个张量划分为*128x128x4*的块，并将它们传递给我们的内核函数。[
    *网格*](https://jax.readthedocs.io/en/latest/pallas/grid_blockspec.html#grid-a-k-a-kernels-in-a-loop)和[BlockSpec
    *索引映射*](https://jax.readthedocs.io/en/latest/pallas/grid_blockspec.html#blockspec-a-k-a-how-to-chunk-up-inputs)相应地定义。
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Although the creation of a new TPU kernel is certainly cause for celebration
    (especially if it enables a previously blocked ML workload) our work is not done.
    A critical part of Pallas kernel development is tuning the operator, (e.g. the
    [*block size*](https://jax.readthedocs.io/en/latest/pallas/grid_blockspec.html#blockspec-a-k-a-how-to-chunk-up-inputs))
    for optimal runtime performance. We omit this stage in the interest of brevity.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管创建新的TPU内核无疑值得庆祝（特别是当它能够启用之前被阻止的ML工作负载时），但我们的工作尚未完成。Pallas内核开发的一个关键部分是调优操作符（例如，[
    *块大小*](https://jax.readthedocs.io/en/latest/pallas/grid_blockspec.html#blockspec-a-k-a-how-to-chunk-up-inputs)）以实现最佳的运行时性能。出于简洁性的考虑，我们省略了这一阶段。
- en: 'To asses the performance of our kernel, we compare it to the following native
    JAX GIOU implementation:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们内核的性能，我们将其与以下原生JAX GIOU实现进行比较：
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We generate two batches of randomly generated bounding boxes and measure the
    performance of our functions using the *benchmark* function defined above.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成两批随机生成的边界框，并使用上面定义的*benchmark*函数来衡量我们函数的性能。
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The comparative results appear in the table below:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 比较结果如下表所示：
- en: '![](../Images/955378e825c927199dbea52380144e37.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/955378e825c927199dbea52380144e37.png)'
- en: Avg time of different GIOU implementations (lower is better) — by Author
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 不同GIOU实现的平均时间（越低越好）—— 作者
- en: We can see that JIT-compiling our naive JAX implementation results in slightly
    better performance than our Pallas kernel. Once again, we can see that matching
    or surpassing the performance results of JIT compilation (and its inherent kernel
    fusion) would require fine-tuning of our custom kernel.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，对我们简单的JAX实现进行JIT编译的结果略优于我们的Pallas内核。再次强调，我们可以看到，要达到或超过JIT编译的性能结果（及其固有的内核融合），需要对我们的自定义内核进行微调。
- en: Utilizing the Sequential Nature of TPUs
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用TPU的顺序特性
- en: While the ability to develop custom kernels for TPU offers great potential,
    our examples thus far have demonstrated that reaching optimal runtime performance
    could be challenging. One way to overcome this is to seek opportunities to utilize
    the unique properties of the TPU architecture. One example of this is the [sequential
    nature of the TPU processor](https://jax.readthedocs.io/en/latest/pallas/tpu/details.html#what-is-a-tpu).
    Although deep learning workloads tend to rely on operations that are easily parallelizable
    (e.g., matrix multiplication), on occasion they require algorithms that are inherently
    sequential. These can pose a serious challenge for the [SIMT](https://en.wikipedia.org/wiki/Single_instruction,_multiple_threads)
    (single instruction multi thread) model of GPUs and can sometimes have a disproportionate
    impact on runtime performance. In a [sequel to this post](https://chaimrand.medium.com/implementing-sequential-algorithms-on-tpu-41d75c6aaa95),
    we demonstrate how we can implement sequential algorithms in a way that takes
    advantage of the TPUs sequential processor and in a manner that minimizes their
    performance penalty.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然为TPU开发自定义内核的能力具有巨大的潜力，但到目前为止我们的示例表明，达到最佳运行时性能可能具有挑战性。克服这一挑战的一种方法是寻求利用TPU架构独特特性的机会。一个例子就是[TPU处理器的顺序特性](https://jax.readthedocs.io/en/latest/pallas/tpu/details.html#what-is-a-tpu)。尽管深度学习工作负载通常依赖于易于并行化的操作（例如矩阵乘法），但有时它们需要本质上是顺序的算法。这些算法可能会对GPU的[SIMT](https://en.wikipedia.org/wiki/Single_instruction,_multiple_threads)（单指令多线程）模型构成严重挑战，有时还会对运行时性能产生不成比例的影响。在[这篇文章的续篇](https://chaimrand.medium.com/implementing-sequential-algorithms-on-tpu-41d75c6aaa95)中，我们展示了如何实现顺序算法，利用TPU的顺序处理器，并尽量减少它们对性能的负面影响。
- en: Summary
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: The introduction of Pallas marks an important milestone in the evolution of
    TPUs. By enabling customization of TPU operations it can potentially unlock new
    opportunities for TPU programmability, particularly in the world of ML. Our intention
    in this post was to demonstrate the accessibility of this powerful new feature.
    While our examples have indeed shown this, they have also highlighted the effort
    required to reach optimal runtime performance.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Pallas的推出标志着TPU发展过程中的一个重要里程碑。通过使TPU操作可定制化，它有可能为TPU的可编程性开辟新的机会，尤其是在机器学习领域。我们在这篇文章中的目的是展示这一强大新特性的可访问性。尽管我们的示例确实展示了这一点，但它们也突显了达到最佳运行时性能所需的努力。
- en: This post has merely scratched the surface of Pallas kernel development. Be
    sure to see the official documentation to learn more about [automatic differentiation
    in Pallas](https://jax.readthedocs.io/en/latest/pallas/design.html#grad-of-pallas-call),
    [developing sparse kernels](https://jax.readthedocs.io/en/latest/pallas/tpu/sparse.html),
    and more.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 本文仅仅触及了Pallas内核开发的表面。请务必查看官方文档，了解更多关于[Pallas中的自动微分](https://jax.readthedocs.io/en/latest/pallas/design.html#grad-of-pallas-call)、[开发稀疏内核](https://jax.readthedocs.io/en/latest/pallas/tpu/sparse.html)等方面的内容。
