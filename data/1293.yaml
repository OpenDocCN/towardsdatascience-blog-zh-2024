- en: Language as a Universal Learning Machine
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¯­è¨€ä½œä¸ºä¸€ç§é€šç”¨å­¦ä¹ æœºå™¨
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/language-as-a-universal-learning-machine-d2c67cb15e5f?source=collection_archive---------7-----------------------#2024-05-23](https://towardsdatascience.com/language-as-a-universal-learning-machine-d2c67cb15e5f?source=collection_archive---------7-----------------------#2024-05-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/language-as-a-universal-learning-machine-d2c67cb15e5f?source=collection_archive---------7-----------------------#2024-05-23](https://towardsdatascience.com/language-as-a-universal-learning-machine-d2c67cb15e5f?source=collection_archive---------7-----------------------#2024-05-23)
- en: 'LANGUAGE PROCESSING IN HUMANS AND COMPUTERS: Part 4'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: äººç±»ä¸è®¡ç®—æœºçš„è¯­è¨€å¤„ç†ï¼šç¬¬4éƒ¨åˆ†
- en: Saying is believing. Seeing is hallucinating.
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯´å‡ºæ¥å°±æ˜¯ç›¸ä¿¡ï¼Œçœ‹åˆ°å°±æ˜¯å¹»è§‰ã€‚
- en: '[](https://medium.com/@dusko_p?source=post_page---byline--d2c67cb15e5f--------------------------------)[![Dusko
    Pavlovic](../Images/3d242896266291f7adbf6f131fe2e16d.png)](https://medium.com/@dusko_p?source=post_page---byline--d2c67cb15e5f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d2c67cb15e5f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d2c67cb15e5f--------------------------------)
    [Dusko Pavlovic](https://medium.com/@dusko_p?source=post_page---byline--d2c67cb15e5f--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@dusko_p?source=post_page---byline--d2c67cb15e5f--------------------------------)[![Dusko
    Pavlovic](../Images/3d242896266291f7adbf6f131fe2e16d.png)](https://medium.com/@dusko_p?source=post_page---byline--d2c67cb15e5f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d2c67cb15e5f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d2c67cb15e5f--------------------------------)
    [Dusko Pavlovic](https://medium.com/@dusko_p?source=post_page---byline--d2c67cb15e5f--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d2c67cb15e5f--------------------------------)
    Â·40 min readÂ·May 23, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d2c67cb15e5f--------------------------------)
    Â·40åˆ†é’Ÿé˜…è¯»Â·2024å¹´5æœˆ23æ—¥
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'Machine-learned language models have transformed everyday life: they steer
    us when we study, drive, manage money. They have the potential to transform our
    civilization. But they hallucinate. Their realities are virtual. This 4th part
    of the series on language processing provides a high-level overview of low-level
    details of how the learning machines work. It turns out that, even after they
    become capable of recognizing hallucinations and dreaming safely, as humans tend
    to be, the learning machines will proceed to form broader systems of false beliefs
    and self-confirming theories, as humans tend to do.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æœºå™¨å­¦ä¹ è¯­è¨€æ¨¡å‹å·²ç»æ”¹å˜äº†æˆ‘ä»¬çš„æ—¥å¸¸ç”Ÿæ´»ï¼šå®ƒä»¬åœ¨æˆ‘ä»¬å­¦ä¹ ã€é©¾é©¶ã€ç†è´¢æ—¶ä¸ºæˆ‘ä»¬æä¾›æŒ‡å¯¼ã€‚å®ƒä»¬æœ‰æ½œåŠ›æ”¹å˜æˆ‘ä»¬çš„æ–‡æ˜ã€‚ä½†å®ƒä»¬ä¹Ÿä¼šäº§ç”Ÿå¹»è§‰ã€‚å®ƒä»¬çš„ç°å®æ˜¯è™šæ‹Ÿçš„ã€‚æœ¬ç³»åˆ—çš„ç¬¬4éƒ¨åˆ†æä¾›äº†ä¸€ä¸ªå…³äºå­¦ä¹ æœºå™¨å¦‚ä½•å·¥ä½œçš„ä½å±‚æ¬¡ç»†èŠ‚çš„é«˜çº§æ¦‚è¿°ã€‚äº‹å®è¯æ˜ï¼Œå³ä½¿å®ƒä»¬èƒ½å¤Ÿåƒäººç±»ä¸€æ ·è¯†åˆ«å¹»è§‰å¹¶å®‰å…¨åœ°åšæ¢¦ï¼Œå­¦ä¹ æœºå™¨ä»ç„¶ä¼šåƒäººç±»ä¸€æ ·ï¼Œå½¢æˆæ›´å¹¿æ³›çš„é”™è¯¯ä¿¡å¿µå’Œè‡ªæˆ‘éªŒè¯çš„ç†è®ºä½“ç³»ã€‚
- en: '[I tried to make this text readable for all. Skipping the math underpinnings
    provided with some claims shouldnâ€™t impact the later claims. Even just the pictures
    at the beginning and at the end are hoped to convey the main message. Suggestions
    for improvements are welcome :)]'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[æˆ‘å°è¯•è®©è¿™ç¯‡æ–‡ç« å¯¹æ‰€æœ‰äººéƒ½æ˜“äºé˜…è¯»ã€‚çœç•¥ä¸€äº›æ•°å­¦æ¨å¯¼åº”è¯¥ä¸ä¼šå½±å“åç»­çš„è®ºç‚¹ã€‚å³ä½¿ä»…ä»…æ˜¯å¼€å§‹å’Œç»“å°¾çš„å›¾ç‰‡ï¼Œä¹Ÿå¸Œæœ›èƒ½ä¼ è¾¾ä¸»è¦ä¿¡æ¯ã€‚æ¬¢è¿æå‡ºæ”¹è¿›å»ºè®®ï¼š)]'
- en: 'Part 1 was:'
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬1éƒ¨åˆ†æ˜¯ï¼š
- en: '[Who are chatbots (and what are they to you)?](https://medium.com/towards-data-science/who-are-chatbots-and-what-are-they-to-you-5c77d9201d11)
    Afterthoughts: [Four elephants in a room with chatbots](https://medium.com/towards-data-science/four-elephants-in-the-room-with-chatbots-82c48a823b94)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[èŠå¤©æœºå™¨äººæ˜¯è°ï¼ˆå®ƒä»¬å¯¹ä½ æ¥è¯´æ„å‘³ç€ä»€ä¹ˆï¼‰ï¼Ÿ](https://medium.com/towards-data-science/who-are-chatbots-and-what-are-they-to-you-5c77d9201d11)
    åç»­æ€è€ƒï¼š [æˆ¿é—´é‡Œçš„å››åªå¤§è±¡ä¸èŠå¤©æœºå™¨äºº](https://medium.com/towards-data-science/four-elephants-in-the-room-with-chatbots-82c48a823b94)'
- en: 'Part 2 was:'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬2éƒ¨åˆ†æ˜¯ï¼š
- en: '[Syntax: The Language Form](https://medium.com/towards-data-science/syntax-the-language-form-612257c4aa5f)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[è¯­æ³•ï¼šè¯­è¨€å½¢å¼](https://medium.com/towards-data-science/syntax-the-language-form-612257c4aa5f)'
- en: 'Part 3 was:'
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬3éƒ¨åˆ†æ˜¯ï¼š
- en: '[Semantics: The Meaning of Language](https://medium.com/@dusko_p/semantics-the-meaning-of-language-99b009ccef41)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[è¯­ä¹‰å­¦ï¼šè¯­è¨€çš„æ„ä¹‰](https://medium.com/@dusko_p/semantics-the-meaning-of-language-99b009ccef41)'
- en: 'THIS IS Part 4:'
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ç¬¬4éƒ¨åˆ†ï¼š
- en: '[Language models, celebrities, and steam engines](#3cda)'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[è¯­è¨€æ¨¡å‹ã€åäººå’Œè’¸æ±½æœº](#3cda)'
- en: '[Evolution of learning](#63f9)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[å­¦ä¹ çš„æ¼”å˜](#63f9)'
- en: '[2.1\. Learning causes and superstitions](#deb1)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2.1\. å­¦ä¹ çš„åŸå› å’Œè¿·ä¿¡](#deb1)'
- en: '[2.2\. General learning framework](#e77a)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2.2\. ä¸€èˆ¬å­¦ä¹ æ¡†æ¶](#e77a)'
- en: '[2.3\. Examples: From pigeons to perceptrons](#0f24)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2.3\. ç¤ºä¾‹ï¼šä»é¸½å­åˆ°æ„ŸçŸ¥æœº](#0f24)'
- en: 3\. [Learning functions](#12c0)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. [å­¦ä¹ å‡½æ•°](#12c0)
- en: '[3.1\. Why learning is possible](#4211)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3.1\. ä¸ºä»€ä¹ˆå­¦ä¹ æ˜¯å¯èƒ½çš„](#4211)'
- en: '[3.2\. Decomposing continuous functions: Kolmogorov-Arnold](#ca7b)[â¶](#d246)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3.2\. åˆ†è§£è¿ç»­å‡½æ•°ï¼šKolmogorov-Arnold](#ca7b)[â¶](#d246)'
- en: '[3.3\. Wide learning](#e045)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3.3\. å¹¿æ³›å­¦ä¹ ](#e045)'
- en: '[3.4\. Approximating continuous functions: Cybenko et al](#ece6)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3.4\. é€¼è¿‘è¿ç»­å‡½æ•°ï¼šCybenkoç­‰](#ece6)'
- en: '[3.5\. Deep learning](#8395)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3.5\. æ·±åº¦å­¦ä¹ ](#8395)'
- en: 4\. [Learning channels and paying attention](#0e5b)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. [å­¦ä¹ é€šé“ä¸æ³¨æ„åŠ›](#0e5b)
- en: '[4.1 Channeling through concepts](#4f8f)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4.1 é€šè¿‡æ¦‚å¿µå¼•å¯¼](#4f8f)'
- en: '[4.2 Static channel learning: RNN, LSTMâ€¦](#b1bf)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4.2 é™æ€é€šé“å­¦ä¹ ï¼šRNNã€LSTMâ€¦](#b1bf)'
- en: '[4.3 Dynamic channel learning: Attention, Transformerâ€¦](#1d50)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4.3 åŠ¨æ€é€šé“å­¦ä¹ ï¼šæ³¨æ„åŠ›ã€å˜å‹å™¨â€¦](#1d50)'
- en: 5\. [Beyond hallucinations](#50f8)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. [è¶…è¶Šå¹»è§‰](#50f8)
- en: '[5.1\. Parametric learning framework](#16d1)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5.1\. å‚æ•°åŒ–å­¦ä¹ æ¡†æ¶](#16d1)'
- en: '[5.2\. Self-learning](#47ec)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5.2\. è‡ªæˆ‘å­¦ä¹ ](#47ec)'
- en: '[5.3\. Self-confirming beliefs](#72b2)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5.3\. è‡ªæˆ‘ç¡®è®¤çš„ä¿¡å¿µ](#72b2)'
- en: '[Attributions](#aa48)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[å½’å±](#aa48)'
- en: '[Notes](http://f974)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[ç¬”è®°](http://f974)'
- en: 1\. Language models, celebrities, and steam engines
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. è¯­è¨€æ¨¡å‹ã€åäººå’Œè’¸æ±½æœº
- en: Anyone can drive a car. Most people even know what the engine looks like. But
    when you need to fix it, you need to figure out how it works.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä»»ä½•äººéƒ½å¯ä»¥å¼€è½¦ã€‚å¤§å¤šæ•°äººç”šè‡³çŸ¥é“å‘åŠ¨æœºé•¿ä»€ä¹ˆæ ·ã€‚ä½†æ˜¯å½“ä½ éœ€è¦ä¿®ç†å®ƒæ—¶ï¼Œä½ éœ€è¦å¼„æ¸…æ¥šå®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚
- en: Anyone can chat with a chatbot. Most people know that there is a Large Language
    Model (LLM) under the hood. There are lots and lots and lots of articles describing
    what an LLM looks like. Lots of colorful pictures. Complicated meshes of small
    components, as if both mathematical abstraction and modular programming still
    wait to be invented. YouTube channels with fresh scoops on LLM celebrities. We
    get to know their parts and how they are connected, we know their performance,
    we even see how each of them changes a heat map of inputs to a heat map of outputs.
    One hotter than the other. But do we understand how they work? Experts say that
    they do, but they donâ€™t seem to be able to explain it even to each other, as they
    continue to disagree about pretty much everything.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä»»ä½•äººéƒ½å¯ä»¥å’ŒèŠå¤©æœºå™¨äººå¯¹è¯ã€‚å¤§å¤šæ•°äººçŸ¥é“åœ¨èƒŒåæœ‰ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚æœ‰å¾ˆå¤šæ–‡ç« æè¿°äº†LLMæ˜¯ä»€ä¹ˆæ ·å­çš„ã€‚å¾ˆå¤šäº”å½©æ–‘æ–“çš„å›¾ç‰‡ã€‚å¤æ‚çš„å°ç»„ä»¶ç½‘çŠ¶ç»“æ„ï¼Œå¥½åƒæ•°å­¦æŠ½è±¡å’Œæ¨¡å—åŒ–ç¼–ç¨‹è¿˜ç­‰ç€è¢«å‘æ˜ä¸€æ ·ã€‚YouTubeé¢‘é“ä¸Šæœ‰å…³äºLLMåäººçš„æ–°é²œèµ„è®¯ã€‚æˆ‘ä»¬äº†è§£å®ƒä»¬çš„å„ä¸ªéƒ¨åˆ†åŠå…¶è¿æ¥æ–¹å¼ï¼ŒçŸ¥é“å®ƒä»¬çš„æ€§èƒ½ï¼Œç”šè‡³çœ‹åˆ°æ¯ä¸ªéƒ¨åˆ†å¦‚ä½•å°†è¾“å…¥çš„çƒ­å›¾è½¬åŒ–ä¸ºè¾“å‡ºçš„çƒ­å›¾ï¼Œä¸€ä¸ªæ¯”ä¸€ä¸ªçƒ­ã€‚ä½†æ˜¯æˆ‘ä»¬çœŸçš„ç†è§£å®ƒä»¬æ˜¯å¦‚ä½•å·¥ä½œçš„ä¹ˆï¼Ÿä¸“å®¶è¯´ä»–ä»¬ç†è§£ï¼Œä½†ä¼¼ä¹è¿å½¼æ­¤ä¹‹é—´éƒ½æ— æ³•è§£é‡Šæ¸…æ¥šï¼Œå› ä¸ºä»–ä»¬åœ¨å‡ ä¹æ‰€æœ‰é—®é¢˜ä¸Šéƒ½æœ‰åˆ†æ­§ã€‚
- en: Every child, of course, knows that it can be hard to explain what you just built.
    Our great civilization built lots of stuff that it couldnâ€™t explain. Steam engines
    have been engineered for nearly 2000 years before scientists explained how they
    extract work from heat. There arenâ€™t many steam engines around anymore, but there
    are lots of language engines and a whole industry of scientific explanations how
    they extract sense from references. The leading theory is that Santa Claus descended
    from the mountain and gave us the transformer architecture carved in a stone tablet.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œæ¯ä¸ªå­©å­éƒ½çŸ¥é“ï¼Œè§£é‡Šè‡ªå·±åˆšåˆšå»ºé€ çš„ä¸œè¥¿å¯èƒ½å¾ˆå›°éš¾ã€‚æˆ‘ä»¬çš„ä¼Ÿå¤§æ–‡æ˜å»ºé€ äº†å¾ˆå¤šæ— æ³•è§£é‡Šçš„ä¸œè¥¿ã€‚è’¸æ±½æœºå·²ç»è¢«å·¥ç¨‹åŒ–è¿‘2000å¹´ï¼Œæ‰æœ‰ç§‘å­¦å®¶è§£é‡Šå®ƒä»¬æ˜¯å¦‚ä½•ä»çƒ­é‡ä¸­æå–å·¥ä½œçš„ã€‚å¦‚ä»Šè’¸æ±½æœºå·²ç»ä¸å¤šè§ï¼Œä½†æœ‰å¾ˆå¤šè¯­è¨€å¼•æ“ï¼Œè¿˜æœ‰ä¸€ä¸ªå®Œæ•´çš„ç§‘å­¦è§£é‡Šè¡Œä¸šï¼Œè®²è¿°å®ƒä»¬å¦‚ä½•ä»å¼•ç”¨ä¸­æå–æ„ä¹‰ã€‚é¢†å…ˆçš„ç†è®ºæ˜¯ï¼Œåœ£è¯è€äººä»å±±ä¸Šä¸‹æ¥ï¼Œç»™æˆ‘ä»¬ç•™ä¸‹äº†åˆ»åœ¨çŸ³æ¿ä¸Šçš„å˜å‹å™¨æ¶æ„ã€‚
- en: '![](../Images/3bcce5a27ad24729bcbacb2d352afb7c.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3bcce5a27ad24729bcbacb2d352afb7c.png)'
- en: Transformers changed the world, spawned offspring and competitors. . . Just
    like steam engines. Which may be a good thing, since steam engines did not exterminate
    their creators just because the creators didnâ€™t understand them.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å˜å‹å™¨æ”¹å˜äº†ä¸–ç•Œï¼Œå‚¬ç”Ÿäº†åä»£å’Œç«äº‰è€…â€¦â€¦å°±åƒè’¸æ±½æœºä¸€æ ·ã€‚è¿™å¯èƒ½æ˜¯ä»¶å¥½äº‹ï¼Œå› ä¸ºè’¸æ±½æœºå¹¶æ²¡æœ‰å› ä¸ºå®ƒä»¬çš„åˆ›é€ è€…ä¸ç†è§£å®ƒä»¬è€Œå°†å®ƒä»¬çš„åˆ›é€ è€…æ¶ˆç­ã€‚
- en: I wasnâ€™t around in the times of steam engines, but I was around in the times
    of bulky computers, and when the web emerged and everything changed, and when
    the web giants emerged and changed the web. Throughout that time, AI research
    seemed like an effort towards the intelligent design of intelligence. It didnâ€™t
    change anything, because intelligence, like life, is an evolutionary process*,*
    not a product of intelligent design[Â¹](#858d).But now some friendly learning machines
    and chatbot AIs evolved and everything is changing again. Having survived and
    processed the paradigm shifts of the past, I am trying to figure out the present
    one. Hence this course and these writings. On one hand, I probably stand no chance
    to say anything that hasnâ€™t been said before. Even after a lot of honest work,
    I remain a short-sighted non-expert. On the other hand, there are some powerful
    tools and ideas that evolved in the neighborhood of AI that AI experts donâ€™t seem
    to be aware of. People clump into research communities, focus on the same things,
    and ignore the same things. Looking over the fences, neighbors sometimes understand
    neighbors better than they understand themselves. This sometimes leads to trouble.
    An ongoing temptation. Here is a view over the fence.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ²¡ç»å†è¿‡è’¸æ±½æœºæ—¶ä»£ï¼Œä½†æˆ‘ç»å†è¿‡ç¬¨é‡çš„è®¡ç®—æœºæ—¶ä»£ï¼Œä¹Ÿäº²å†äº†ç½‘ç»œçš„å‡ºç°ä»¥åŠä¸€åˆ‡å‘ç”Ÿå˜åŒ–çš„æ—¶æœŸï¼Œè¿˜ç»å†äº†ç½‘ç»œå·¨å¤´çš„å´›èµ·å’Œä»–ä»¬æ”¹å˜ç½‘ç»œçš„è¿‡ç¨‹ã€‚åœ¨é‚£æ®µæ—¶é—´é‡Œï¼Œäººå·¥æ™ºèƒ½ç ”ç©¶ä¼¼ä¹åƒæ˜¯æ™ºèƒ½è®¾è®¡æ™ºèƒ½çš„ä¸€ç§åŠªåŠ›ã€‚å®ƒå¹¶æ²¡æœ‰æ”¹å˜ä»€ä¹ˆï¼Œå› ä¸ºæ™ºèƒ½ï¼Œåƒç”Ÿå‘½ä¸€æ ·ï¼Œæ˜¯ä¸€ä¸ª*è¿›åŒ–çš„è¿‡ç¨‹*ï¼Œè€Œä¸æ˜¯æ™ºèƒ½è®¾è®¡çš„äº§ç‰©[Â¹](#858d)ã€‚ä½†ç°åœ¨ï¼Œä¸€äº›å‹å¥½çš„å­¦ä¹ æœºå™¨å’ŒèŠå¤©æœºå™¨äººAIå·²ç»è¿›åŒ–ï¼Œä¸€åˆ‡åˆåœ¨å‘ç”Ÿå˜åŒ–ã€‚åœ¨ç»å†äº†è¿‡å»çš„èŒƒå¼è½¬å˜åï¼Œæˆ‘æ­£åœ¨è¯•å›¾ç†è§£è¿™ä¸€å½“å‰çš„è½¬å˜ã€‚å› æ­¤æœ‰äº†è¿™é—¨è¯¾ç¨‹å’Œè¿™äº›æ–‡å­—ã€‚ä»ä¸€æ–¹é¢è®²ï¼Œæˆ‘å¯èƒ½æ²¡æœ‰æœºä¼šè¯´å‡ºä»»ä½•æœªæ›¾æœ‰äººæåˆ°è¿‡çš„ä¸œè¥¿ã€‚å³ä½¿åšäº†å¾ˆå¤šè¯šå®çš„å·¥ä½œï¼Œæˆ‘ä»ç„¶æ˜¯ä¸€ä¸ªç›®å…‰çŸ­æµ…çš„éä¸“å®¶ã€‚å¦ä¸€æ–¹é¢ï¼Œåœ¨äººå·¥æ™ºèƒ½çš„é¢†åŸŸï¼Œå‡ºç°äº†ä¸€äº›å¼ºå¤§çš„å·¥å…·å’Œæ€æƒ³ï¼ŒAIä¸“å®¶ä¼¼ä¹å¹¶æœªæ„è¯†åˆ°è¿™äº›ã€‚äººä»¬ä¼šèšé›†åœ¨ç ”ç©¶ç¤¾åŒºä¸­ï¼Œé›†ä¸­å…³æ³¨ç›¸åŒçš„äº‹æƒ…ï¼Œå´å¿½è§†äº†ç›¸åŒçš„äº‹æƒ…ã€‚ç«™åœ¨å›´æ æ—ï¼Œé‚»å±…æœ‰æ—¶èƒ½æ¯”è‡ªå·±æ›´å¥½åœ°ç†è§£é‚»å±…ã€‚è¿™æœ‰æ—¶ä¼šå¯¼è‡´éº»çƒ¦ã€‚ä¸€ç§æŒç»­çš„è¯±æƒ‘ã€‚è¿™æ˜¯ä¸€ä¸ªç«™åœ¨å›´æ å¤–çš„è§†è§’ã€‚
- en: 2\. Evolution of learning
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. å­¦ä¹ çš„æ¼”åŒ–
- en: '[2.1\. Learning causes and superstitions](http://deb1)'
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[2.1\. å­¦ä¹ çš„åŸå› å’Œè¿·ä¿¡](http://deb1)'
- en: Spiders are primed to build spider webs. Their engineering skills to weave webs
    are programmed in their genes. They are pretrained builders and even their capability
    to choose and remember a good place for a web is automated.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: èœ˜è››å¤©ç”Ÿå°±ä¼šç»‡èœ˜è››ç½‘ã€‚å®ƒä»¬ç¼–ç»‡ç½‘çš„å·¥ç¨‹æŠ€èƒ½æ˜¯é€šè¿‡åŸºå› ç¨‹åºåŒ–çš„ã€‚å®ƒä»¬æ˜¯é¢„å…ˆè®­ç»ƒå¥½çš„å»ºé€ è€…ï¼Œç”šè‡³å®ƒä»¬é€‰æ‹©å’Œè®°ä½ä¸€ä¸ªé€‚åˆç»‡ç½‘çš„å¥½åœ°æ–¹çš„èƒ½åŠ›ä¹Ÿæ˜¯è‡ªåŠ¨åŒ–çš„ã€‚
- en: Dogs and pigeons are primed to seek food. Their capabilities to learn sources
    and actions that bring food are automated. In a famous experiment, physiologist
    Pavlov studied one of the simplest forms of learning, usually called *conditioning*.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ç‹—å’Œé¸½å­å¤©ç”Ÿå°±ä¼šå¯»æ‰¾é£Ÿç‰©ã€‚å®ƒä»¬å­¦ä¼šå¯»æ‰¾å¸¦æ¥é£Ÿç‰©çš„æ¥æºå’Œè¡Œä¸ºçš„èƒ½åŠ›æ˜¯è‡ªåŠ¨åŒ–çš„ã€‚åœ¨ä¸€é¡¹è‘—åçš„å®éªŒä¸­ï¼Œç”Ÿç†å­¦å®¶å·´ç”«æ´›å¤«ç ”ç©¶äº†æœ€ç®€å•çš„å­¦ä¹ å½¢å¼ä¹‹ä¸€ï¼Œé€šå¸¸è¢«ç§°ä¸º*æ¡ä»¶åå°„*ã€‚
- en: '![](../Images/8ae6dd8b177d587867dbbe024c5abada.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8ae6dd8b177d587867dbbe024c5abada.png)'
- en: If the bell rings whenever the dog is fed, he learns to salivate whenever the
    bell rings.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ¯æ¬¡å–‚ç‹—æ—¶é“ƒå£°å“èµ·ï¼Œä»–å°±ä¼šå­¦ä¼šæ¯å½“é“ƒå£°å“èµ·æ—¶åˆ†æ³Œå”¾æ¶²ã€‚
- en: Continuing in the same vein, psychologist Skinner showed that pigeons could
    even develop a form of superstition, also by trying to learn where the food comes
    from.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å»¶ç»­è¿™ä¸€æ€è·¯ï¼Œå¿ƒç†å­¦å®¶æ–¯é‡‘çº³å±•ç¤ºäº†é¸½å­ç”šè‡³èƒ½å¤Ÿå‘å±•å‡ºä¸€ç§è¿·ä¿¡è¡Œä¸ºï¼ŒåŒæ ·æ˜¯é€šè¿‡è¯•å›¾å­¦ä¹ é£Ÿç‰©çš„æ¥æºã€‚
- en: '![](../Images/7fca4475cde3c65f6bc8d6df78157582.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7fca4475cde3c65f6bc8d6df78157582.png)'
- en: If food arrives while the pigeon is pecking, she learns that pecking conjures
    food
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœé£Ÿç‰©åœ¨é¸½å­å•„é£Ÿæ—¶åˆ°æ¥ï¼Œå¥¹å°±ä¼šå­¦ä¼šå•„é£Ÿèƒ½å¤Ÿå¼•æ¥é£Ÿç‰©ã€‚
- en: Skinner fed pigeons at completely random times, with no correlation with their
    behaviors. About 70% of them developed beliefs that they could conjure food. If
    a pigeon happened to be pecking on the ground, or ruffling feathers just before
    the food arrived, this would make them engage in this action more frequently,
    which increased the chance that the food would arrive while they were performing
    that action. If one of the random associations, say of food and pecking, after
    a while prevails, then it gets promoted into a ritual dance for food. Each time,
    the food eventually arrives and confirms that the ritual works.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¯é‡‘çº³åœ¨å®Œå…¨éšæœºçš„æ—¶é—´å–‚é¸½å­ï¼Œè¡Œä¸ºå’Œå–‚é£Ÿæ²¡æœ‰ä»»ä½•å…³è”ã€‚çº¦æœ‰70%çš„é¸½å­å‘å±•å‡ºäº†å®ƒä»¬å¯ä»¥å¬å”¤é£Ÿç‰©çš„ä¿¡å¿µã€‚å¦‚æœä¸€åªé¸½å­æ°å¥½åœ¨é£Ÿç‰©åˆ°æ¥ä¹‹å‰åœ¨åœ°é¢å•„é£Ÿï¼Œæˆ–åœ¨é£Ÿç‰©åˆ°è¾¾ä¹‹å‰æ•´ç†ç¾½æ¯›ï¼Œè¿™ä¼šä½¿å®ƒä»¬æ›´é¢‘ç¹åœ°è¿›è¡Œè¿™äº›åŠ¨ä½œï¼Œä»è€Œå¢åŠ åœ¨è¿™äº›è¡Œä¸ºå‘ç”Ÿæ—¶é£Ÿç‰©åˆ°æ¥çš„æ¦‚ç‡ã€‚å¦‚æœæŸä¸ªéšæœºçš„è”ç»“ï¼Œæ¯”å¦‚é£Ÿç‰©å’Œå•„é£Ÿï¼Œåœ¨ä¸€æ®µæ—¶é—´åå ä¸»å¯¼åœ°ä½ï¼Œé‚£ä¹ˆå®ƒå°±ä¼šå˜æˆä¸€ç§å¯»æ±‚é£Ÿç‰©çš„ä»ªå¼èˆè¹ˆã€‚æ¯æ¬¡é£Ÿç‰©æœ€ç»ˆéƒ½ä¼šåˆ°æ¥ï¼Œå¹¶ç¡®è®¤ä»ªå¼æœ‰æ•ˆã€‚
- en: Humans are primed to seek causes and predict effects. Like pigeons, they associate
    coinciding events as correlated and develop superstitions, promoting coincidences
    into causal theories. While pigeons end up pecking empty surfaces to conjure grains,
    humans build monumental systems of false beliefs, attributing their fortunes and
    misfortunes, say, to the influence of stars millions of light years away, or to
    their neighborâ€™s evil eye, or to pretty much anything that can be seen, felt,
    or counted[Â²](#a74e).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: äººç±»å¤©ç”Ÿå€¾å‘äºå¯»æ‰¾åŸå› å¹¶é¢„æµ‹ç»“æœã€‚åƒé¸½å­ä¸€æ ·ï¼Œä»–ä»¬å°†å·§åˆçš„äº‹ä»¶å…³è”ä¸ºç›¸å…³äº‹ä»¶ï¼Œå¹¶å‘å±•å‡ºè¿·ä¿¡ï¼Œå°†å·§åˆå‡åä¸ºå› æœç†è®ºã€‚å½“é¸½å­æœ€ç»ˆå•„ç©ºçš„è¡¨é¢æ¥å¬å”¤è°·ç²’æ—¶ï¼Œäººç±»åˆ™å»ºç«‹èµ·åºå¤§çš„è™šå‡ä¿¡ä»°ä½“ç³»ï¼Œå½’å› äºæ˜Ÿæ˜Ÿæ•°ç™¾ä¸‡å…‰å¹´å¤–çš„å½±å“ï¼Œæˆ–é‚»å±…çš„æ¶æ„çœ¼å…‰ï¼Œæˆ–è€…å‡ ä¹ä»»ä½•å¯ä»¥çœ‹åˆ°ã€æ„Ÿå—åˆ°æˆ–è®¡ç®—çš„äº‹ç‰©[Â²](#a74e)ã€‚
- en: But while our causal beliefs are shared with pigeons, our capabilities to build
    houses and span bridges are not shared with spiders. Unlike spiders, we are not
    primed to build but have to *learn* our engineering skills. *We are primed to
    learn.*
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å°½ç®¡æˆ‘ä»¬çš„å› æœä¿¡å¿µä¸é¸½å­ç›¸ä¼¼ï¼Œæˆ‘ä»¬å»ºé€ æˆ¿å±‹å’Œè·¨è¶Šæ¡¥æ¢çš„èƒ½åŠ›å´ä¸èœ˜è››ä¸åŒã€‚ä¸èœ˜è››ä¸åŒï¼Œæˆ‘ä»¬ä¸æ˜¯å¤©ç”Ÿä¼šå»ºé€ ï¼Œè€Œæ˜¯å¿…é¡»*å­¦ä¹ *æˆ‘ä»¬çš„å·¥ç¨‹æŠ€èƒ½ã€‚*æˆ‘ä»¬å¤©ç”Ÿä¼šå­¦ä¹ *ã€‚
- en: 2.2\. General learning framework
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2. ä¸€èˆ¬å­¦ä¹ æ¡†æ¶
- en: 'A birdâ€™s eye view of the scene of learning looks something like this:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å­¦ä¹ åœºæ™¯çš„é¸Ÿç°å›¾å¤§è‡´å¦‚ä¸‹ï¼š
- en: '![](../Images/0c994833ec2d8ac882a394db11a8c84a.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c994833ec2d8ac882a394db11a8c84a.png)'
- en: 'The inputs come from the left. The main characters are:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥ä»å·¦è¾¹è¿›å…¥ã€‚ä¸»è¦è§’è‰²æ˜¯ï¼š
- en: a process *F*, the *supervisor* in supervised learning ([Turing called it a
    â€œteacherâ€](https://medium.com/p/5c77d9201d11#e1d2)) processing input data *x*
    of type *X* to produce output classes or parameters *y* of type *Y*;
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªè¿‡ç¨‹*F*ï¼Œåœ¨ç›‘ç£å­¦ä¹ ä¸­æ˜¯*ç›‘ç£è€…*ï¼ˆ[å›¾çµç§°ä¹‹ä¸ºâ€œè€å¸ˆâ€](https://medium.com/p/5c77d9201d11#e1d2)ï¼‰ï¼Œå¤„ç†ç±»å‹ä¸º*X*çš„è¾“å…¥æ•°æ®*x*ï¼Œç”Ÿæˆç±»å‹ä¸º*Y*çš„è¾“å‡ºç±»åˆ«æˆ–å‚æ•°*y*ï¼›
- en: an **a**-indexed family of functions ğ’°(âˆ’)**a**, where ğ’° is a *learning machine*
    or *interpreter* ([Turing called it a â€œpupilâ€](https://medium.com/p/5c77d9201d11#2e21))
    and the indices **a** are the *models*, usually expressed as *programs*; lastly,
    there is
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª**a**ç´¢å¼•çš„å‡½æ•°æ—ğ’°(âˆ’)**a**ï¼Œå…¶ä¸­ğ’°æ˜¯ä¸€ä¸ª*å­¦ä¹ æœº*æˆ–*è§£é‡Šå™¨*ï¼ˆ[å›¾çµç§°ä¹‹ä¸ºâ€œå­¦ç”Ÿâ€](https://medium.com/p/5c77d9201d11#2e21)ï¼‰ï¼Œç´¢å¼•**a**æ˜¯*æ¨¡å‹*ï¼Œé€šå¸¸è¡¨ç¤ºä¸º*ç¨‹åº*ï¼›æœ€åï¼Œè¿˜æœ‰
- en: a function â„’, usually called the *loss*, comparing the outputs *y* = *F*(*x*)
    with the predictions *á»¹* = ğ’°(*x*)**a** and delivering a real number â„’(*y*,*á»¹*)
    that measures their difference.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå‡½æ•°â„’ï¼Œé€šå¸¸ç§°ä¸º*æŸå¤±*ï¼Œæ¯”è¾ƒè¾“å‡º*y* = *F*(*x*)ä¸é¢„æµ‹å€¼*á»¹* = ğ’°(*x*)**a**ï¼Œå¹¶ç»™å‡ºä¸€ä¸ªå®æ•°â„’(*y*,*á»¹*)ï¼Œè¡¨ç¤ºå®ƒä»¬ä¹‹é—´çš„å·®å¼‚ã€‚
- en: The learner overseeing the learning framework is given a finite set
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ç›‘ç£å­¦ä¹ æ¡†æ¶çš„å­¦ä¹ è€…æä¾›ä¸€ä¸ªæœ‰é™çš„é›†åˆ
- en: '![](../Images/68bfd63f4334bc5d85cfac7fe08f1098.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/68bfd63f4334bc5d85cfac7fe08f1098.png)'
- en: where the *x*s are samples from a source *X* and the *y*s are the corresponding
    samples from the random variable *Y* = *F*(*X*). The learnerâ€™s task is to build
    a model **a** that minimizes the losses
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­*x*æ˜¯æ¥è‡ªæº*X*çš„æ ·æœ¬ï¼Œ*y*æ˜¯æ¥è‡ªéšæœºå˜é‡*Y* = *F*(*X*)çš„ç›¸åº”æ ·æœ¬ã€‚å­¦ä¹ è€…çš„ä»»åŠ¡æ˜¯æ„å»ºä¸€ä¸ªæœ€å°åŒ–æŸå¤±çš„æ¨¡å‹**a**ã€‚
- en: '![](../Images/f4652087b372746f1517dc1c3ce14083.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4652087b372746f1517dc1c3ce14083.png)'
- en: where *yi* = *F*(*xi*) and *á»¹i* = ğ’°(*xi*)**a** for *i* = 1,2,â€¦,*n*. Since some
    of the losses may increase when the others decrease, the learning algorithm is
    required to minimize the average *guessing risk*
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­*yi* = *F*(*xi*)ï¼Œ*á»¹i* = ğ’°(*xi*)**a**ï¼Œå¯¹äº*i* = 1,2,â€¦,*n*ã€‚ç”±äºæŸäº›æŸå¤±å¯èƒ½åœ¨å…¶ä»–æŸå¤±å‡å°‘æ—¶å¢åŠ ï¼Œå› æ­¤è¦æ±‚å­¦ä¹ ç®—æ³•æœ€å°åŒ–å¹³å‡*çŒœæµ‹é£é™©*ã€‚
- en: '![](../Images/26df659663e5b994ef42eb2f950d4bd4.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26df659663e5b994ef42eb2f950d4bd4.png)'
- en: where [ğ’°(*xi*)**a**] denotes the frequency with which the guesses ğ’°(*xi*)**a**
    are tried. Once a model **a** is found for which the risk is minimal, the function
    *F* is approximated by running the machine ğ’° on a program implementing the model
    **a** and we write
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­[ğ’°(*xi*)**a**]è¡¨ç¤ºçŒœæµ‹ğ’°(*xi*)**a**è¢«å°è¯•çš„é¢‘ç‡ã€‚ä¸€æ—¦æ‰¾åˆ°ä¸€ä¸ªä½¿é£é™©æœ€å°åŒ–çš„æ¨¡å‹**a**ï¼Œå°±é€šè¿‡è¿è¡Œå®ç°æ¨¡å‹**a**çš„ç¨‹åºåœ¨æœºå™¨ğ’°ä¸Šè¿‘ä¼¼å‡½æ•°*F*ï¼Œå¹¶å†™æˆ
- en: '![](../Images/15346bce3485caea133988d78905e635.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15346bce3485caea133988d78905e635.png)'
- en: '**Potato, potahto, tomato, tomahto.** What are the outcomes of learning? We
    just called the outcome **a** of a round of supervised learning a *model* of the
    supervisor *F*. Since **a** is an attempt to describe *F*, most logicians would
    call it a *theory* of *F*. If the interpretations ğ’°(*X*)**a** describe *F*(*X*)
    truthfully, the logicians would say that *F* is actually a model of the theory
    **a** under the semantical interpretation by ğ’°. So there is a terminological clash
    between the theory of learning, where **a** is a model of *F*, and logic, where
    *F* is a model of **a.** In a further contribution to the confusion, statisticians
    say that **a** is a *hypothesis* about *F*. If a hypothesis or a theory is believed
    to be true, then it is a part of the learnerâ€™s *belief state*. In the [final section](#50f8),
    we will arrive at a curious construction illustrating a need for studying the
    *belief logic of machine learning*[Â³](#0dbf). We stick with calling the learning
    outcomes **a** *models* since that seems to be the common usage. An important
    wrinkle, is, however, that a model **a** of *F* needs to be *executable* in order
    to allow computing the predictions ğ’°(*X*)**a** of the values *F*(*X*). But if
    you think about it, executable models are what we normally call *programs*. In
    summary, the outcome of a learning process is an executable model. The cumulative
    outcome of learning is the learnerâ€™s belief state. ***The process of learning
    is the search for learnable programs.***'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**åœŸè±†ã€æ´‹è‘±ã€ç•ªèŒ„ã€ç•ªèŒ„ã€‚** å­¦ä¹ çš„ç»“æœæ˜¯ä»€ä¹ˆï¼Ÿæˆ‘ä»¬åˆšæ‰æåˆ°çš„ç›‘ç£å­¦ä¹ çš„ä¸€è½®ç»“æœ**a**æ˜¯ç›‘ç£è€…*F*çš„*æ¨¡å‹*ã€‚ç”±äº**a**æ˜¯è¯•å›¾æè¿°*F*ï¼Œå¤§å¤šæ•°é€»è¾‘å­¦å®¶ä¼šç§°å…¶ä¸º*F*çš„*ç†è®º*ã€‚å¦‚æœè§£é‡Šğ’°(*X*)**a**å¿ å®åœ°æè¿°äº†*F*(*X*)ï¼Œé‚£ä¹ˆé€»è¾‘å­¦å®¶ä¼šè¯´*F*å®é™…ä¸Šæ˜¯ç†è®º**a**åœ¨ğ’°è¯­ä¹‰è§£é‡Šä¸‹çš„ä¸€ä¸ªæ¨¡å‹ã€‚æ‰€ä»¥åœ¨å­¦ä¹ ç†è®ºä¸­ï¼Œ**a**æ˜¯*F*çš„æ¨¡å‹ï¼Œè€Œåœ¨é€»è¾‘ä¸­ï¼Œ*F*æ˜¯**a**çš„æ¨¡å‹ï¼Œè¿™ä¸¤è€…ä¹‹é—´å­˜åœ¨æœ¯è¯­ä¸Šçš„å†²çªã€‚æ›´è¿›ä¸€æ­¥ï¼Œç»Ÿè®¡å­¦å®¶è¯´**a**æ˜¯å…³äº*F*çš„*å‡è®¾*ã€‚å¦‚æœä¸€ä¸ªå‡è®¾æˆ–ç†è®ºè¢«è®¤ä¸ºæ˜¯çœŸçš„ï¼Œé‚£ä¹ˆå®ƒå°±æ˜¯å­¦ä¹ è€…*ä¿¡å¿µçŠ¶æ€*çš„ä¸€éƒ¨åˆ†ã€‚åœ¨[æœ€åéƒ¨åˆ†](#50f8)ï¼Œæˆ‘ä»¬å°†æå‡ºä¸€ä¸ªæœ‰è¶£çš„æ„å»ºï¼Œè¯´æ˜ç ”ç©¶*æœºå™¨å­¦ä¹ çš„ä¿¡å¿µé€»è¾‘*çš„å¿…è¦æ€§[Â³](#0dbf)ã€‚æˆ‘ä»¬ä»ç„¶ç§°å­¦ä¹ ç»“æœ**a**ä¸º*æ¨¡å‹*ï¼Œå› ä¸ºè¿™ä¼¼ä¹æ˜¯æœ€å¸¸è§çš„ç”¨æ³•ã€‚ç„¶è€Œï¼Œä¸€ä¸ªé‡è¦çš„ç»†èŠ‚æ˜¯ï¼Œ*F*çš„æ¨¡å‹**a**éœ€è¦æ˜¯*å¯æ‰§è¡Œçš„*ï¼Œä»¥ä¾¿èƒ½å¤Ÿè®¡ç®—å‡ºå€¼*F*(*X*)çš„é¢„æµ‹ğ’°(*X*)**a**ã€‚ä½†å¦‚æœä½ ä»”ç»†æƒ³æƒ³ï¼Œå¯æ‰§è¡Œæ¨¡å‹é€šå¸¸å°±æ˜¯æˆ‘ä»¬æ‰€ç§°çš„*ç¨‹åº*ã€‚æ€»ä¹‹ï¼Œå­¦ä¹ è¿‡ç¨‹çš„ç»“æœæ˜¯ä¸€ä¸ªå¯æ‰§è¡Œçš„æ¨¡å‹ã€‚å­¦ä¹ çš„ç´¯ç§¯ç»“æœæ˜¯å­¦ä¹ è€…çš„ä¿¡å¿µçŠ¶æ€ã€‚***å­¦ä¹ çš„è¿‡ç¨‹å°±æ˜¯å¯»æ‰¾å¯å­¦ä¹ ç¨‹åºçš„è¿‡ç¨‹ã€‚***'
- en: '**All learning is language learning.** In general, the process *F* to be learned
    is given as a channel, which means that the outputs are context-dependent. The
    story from [Sec. 3.2 of the *Semantics* part](https://medium.com/@dusko_p/semantics-the-meaning-of-language-99b009ccef41#d3b1)
    applies. The channel inputs *xj* depend on the earlier inputs *xi*, *i* < *j*.
    When there is feedback, *xj* also depends on the earlier outputs *yi*, *i* < *j*.
    To be able to learn *F*â€™s behavior, the learning machine ğ’° must also be a channel.
    *Since capturing channel dependencies requires syntactic and semantic references,
    there is a language behind every learner*, whether it is apparent or not. The
    semiotic analyses of the languages of film, music, or images, etc., describe genuine
    syntactic and semantic structures. Different organisms learn in different ways,
    but for humans and their machines, all learning is language learning.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ‰€æœ‰å­¦ä¹ éƒ½æ˜¯è¯­è¨€å­¦ä¹ ã€‚** ä¸€èˆ¬æ¥è¯´ï¼Œå¾…å­¦ä¹ çš„è¿‡ç¨‹*F*æ˜¯é€šè¿‡ä¸€ä¸ªé€šé“ç»™å‡ºçš„ï¼Œè¿™æ„å‘³ç€è¾“å‡ºæ˜¯ä¸Šä¸‹æ–‡ä¾èµ–çš„ã€‚æ¥è‡ª[ç¬¬3.2èŠ‚ *è¯­ä¹‰* éƒ¨åˆ†](https://medium.com/@dusko_p/semantics-the-meaning-of-language-99b009ccef41#d3b1)çš„æ•…äº‹é€‚ç”¨ã€‚é€šé“è¾“å…¥*xj*ä¾èµ–äºæ—©æœŸçš„è¾“å…¥*xi*ï¼Œå…¶ä¸­*i*
    < *j*ã€‚å½“æœ‰åé¦ˆæ—¶ï¼Œ*xj*ä¹Ÿä¾èµ–äºæ—©æœŸçš„è¾“å‡º*yi*ï¼Œå…¶ä¸­*i* < *j*ã€‚ä¸ºäº†èƒ½å¤Ÿå­¦ä¹ *F*çš„è¡Œä¸ºï¼Œå­¦ä¹ æœºå™¨ğ’°ä¹Ÿå¿…é¡»æ˜¯ä¸€ä¸ªé€šé“ã€‚*ç”±äºæ•æ‰é€šé“ä¾èµ–æ€§éœ€è¦å¥æ³•å’Œè¯­ä¹‰å¼•ç”¨ï¼Œæ‰€ä»¥æ¯ä¸ªå­¦ä¹ è€…èƒŒåéƒ½æœ‰ä¸€ç§è¯­è¨€*ï¼Œæ— è®ºå®ƒæ˜¯å¦æ˜¾è€Œæ˜“è§ã€‚ç”µå½±ã€éŸ³ä¹æˆ–å›¾åƒç­‰è¯­è¨€çš„ç¬¦å·å­¦åˆ†ææè¿°äº†çœŸæ­£çš„å¥æ³•å’Œè¯­ä¹‰ç»“æ„ã€‚ä¸åŒçš„ç”Ÿç‰©ä»¥ä¸åŒçš„æ–¹å¼å­¦ä¹ ï¼Œä½†å¯¹äºäººç±»åŠå…¶æœºå™¨è€Œè¨€ï¼Œæ‰€æœ‰å­¦ä¹ éƒ½æ˜¯è¯­è¨€å­¦ä¹ ã€‚'
- en: '2.3\. Examples: From pigeons to perceptrons'
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3\. ç¤ºä¾‹ï¼šä»é¸½å­åˆ°æ„ŸçŸ¥æœº
- en: '**Pigeon superstition.** The function *F* that a pigeon learns to predict is
    a source of food. It can be viewed as a channel *[X* âŠ¢ *Y]*, where the values
    *x*1, *x*2, . . . of type *X* are moments in time and *Y* = *F*(*X*) is a random
    variable, delivering seeds with a fixed probability. Suppose that *Y* = 1 means
    â€œfoodâ€ and *Y* = 0 means â€œno foodâ€. If we take the possible models (programs,
    beliefs) **a** to correspond to the elements of a set of actions available to
    the pigeon, then the pigeon is trying to learn for which actions **a** and at
    which moments *x* to output ğ’°(*x*)**a** = 1 and when to output 0\. The loss â„’(*y*,
    ğ’°(*x*)**a**) = |*y-*ğ’°(*x*)**a**| is 0 if the food is delivered just when the pigeon
    takes the action **a**. After a sufficient amount of time, the random output *Y*
    = 1 will almost surely coincide with a prediction ğ’°(*X*)**a** = 1 for some **a**.
    The pigeon will then learn to do **a** more often and increase the chance of such
    coincidences. If one **a** prevails, the pigeon will learn that it causes food.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**é¸½å­è¿·ä¿¡ã€‚** é¸½å­å­¦ä¹ é¢„æµ‹çš„å‡½æ•°*F*æ˜¯é£Ÿç‰©çš„æ¥æºã€‚å®ƒå¯ä»¥è§†ä¸ºä¸€ä¸ªé€šé“*[X* âŠ¢ *Y]*ï¼Œå…¶ä¸­ç±»å‹*X*çš„å€¼*x*1ã€*x*2ã€...æ˜¯æ—¶é—´ç‚¹ï¼Œ*Y*
    = *F*(*X*)æ˜¯ä¸€ä¸ªéšæœºå˜é‡ï¼Œä»¥å›ºå®šçš„æ¦‚ç‡æä¾›ç§å­ã€‚å‡è®¾*Y* = 1è¡¨ç¤ºâ€œé£Ÿç‰©â€ï¼Œ*Y* = 0è¡¨ç¤ºâ€œæ²¡æœ‰é£Ÿç‰©â€ã€‚å¦‚æœæˆ‘ä»¬å°†å¯èƒ½çš„æ¨¡å‹ï¼ˆç¨‹åºã€ä¿¡å¿µï¼‰**a**å¯¹åº”äºé¸½å­å¯ç”¨åŠ¨ä½œé›†çš„å…ƒç´ ï¼Œé‚£ä¹ˆé¸½å­æ­£åœ¨å°è¯•å­¦ä¹ åœ¨å“ªäº›æ—¶åˆ»*x*å’Œå“ªäº›åŠ¨ä½œ**a**ä¸‹ï¼Œè¾“å‡ºğ’°(*x*)**a**
    = 1ï¼Œä»¥åŠä½•æ—¶è¾“å‡º0ã€‚å¦‚æœé£Ÿç‰©æ°å¥½åœ¨é¸½å­é‡‡å–åŠ¨ä½œ**a**æ—¶é€åˆ°ï¼Œåˆ™æŸå¤±â„’(*y*, ğ’°(*x*)**a**) = |*y-*ğ’°(*x*)**a**|ä¸º0ã€‚ç»è¿‡è¶³å¤Ÿçš„æ—¶é—´åï¼Œéšæœºè¾“å‡º*Y*
    = 1å‡ ä¹è‚¯å®šä¼šä¸æŸä¸ª**a**çš„é¢„æµ‹ğ’°(*X*)**a** = 1ç›¸ç¬¦ã€‚é¸½å­éšåä¼šå­¦ä¼šæ›´é¢‘ç¹åœ°åš**a**ï¼Œä»è€Œå¢åŠ è¿™ç§å·§åˆå‘ç”Ÿçš„æœºä¼šã€‚å¦‚æœæŸä¸ª**a**å ä¸»å¯¼åœ°ä½ï¼Œé¸½å­å°†å­¦ä¼šå®ƒæ˜¯å¯¼è‡´é£Ÿç‰©å‡ºç°çš„åŸå› ã€‚'
- en: '**Statistical testing.** Science is a family of methods designed to overcome
    superstition and prejudice. The idea is to prevent pigeon-style confirmations
    by systematically testing hypotheses and only accepting significant correlations.
    The mathematical foundations of statistical hypothesis testing were developed
    in the 1920s by Ronald Fisher, and have remained the bread and butter of scientific
    practices. The crucial assumption is that the interpretation ğ’° for any hypothesis
    **a** is given together with its probability density *p***a**(*x*) = *d*ğ’°(*x*)**a**
    . The loss â„’ is then estimated by the length of the description of this probability.
    If the value of *p***a**(*x*) is described by a string of digits, its description
    length is proportional to âˆ’log *p***a**(*x*). The guessing risk is thus â„›(**a**)
    = âˆ«âˆ’ log *p***a**(*x*)*d*ğ’°(*x*)**a**. Values of this kind are studied in information
    theory as measures of uncertainty. Minimizing â„›(**a**) thus boils down to choosing
    the hypothesis a that minimizes the uncertainty of sampling ğ’° for **a**. Fisher
    recommended the learning algorithm that selects the hypothesis with a *maximal
    likelihood*.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç»Ÿè®¡æ£€éªŒã€‚** ç§‘å­¦æ˜¯ä¸€ç³»åˆ—æ—¨åœ¨å…‹æœè¿·ä¿¡å’Œåè§çš„æ–¹æ³•ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡ç³»ç»Ÿåœ°æ£€éªŒå‡è®¾ï¼Œå¹¶ä»…æ¥å—æ˜¾è‘—çš„ç›¸å…³æ€§ï¼Œä»è€Œé˜²æ­¢â€œé¸½å­å¼â€çš„ç¡®è®¤åè¯¯ã€‚ç»Ÿè®¡å‡è®¾æ£€éªŒçš„æ•°å­¦åŸºç¡€ç”±ç½—çº³å¾·Â·è´¹èˆå°”ï¼ˆRonald
    Fisherï¼‰åœ¨1920å¹´ä»£å‘å±•èµ·æ¥ï¼Œå¹¶ä¸€ç›´æ˜¯ç§‘å­¦å®è·µä¸­çš„åŸºç¡€å·¥å…·ã€‚å…¶å…³é”®å‡è®¾æ˜¯ï¼Œä»»ä½•å‡è®¾**a**çš„è§£é‡Šğ’°æ˜¯ä¸å…¶æ¦‚ç‡å¯†åº¦*p***a**(*x*) = *d*ğ’°(*x*)**a**ä¸€èµ·ç»™å‡ºçš„ã€‚æŸå¤±â„’åˆ™é€šè¿‡è¯¥æ¦‚ç‡çš„æè¿°é•¿åº¦æ¥ä¼°è®¡ã€‚å¦‚æœ*p***a**(*x*)çš„æè¿°ç”±ä¸€ä¸²æ•°å­—è¡¨ç¤ºï¼Œåˆ™å…¶æè¿°é•¿åº¦ä¸âˆ’log
    *p***a**(*x*)æˆæ­£æ¯”ã€‚å› æ­¤ï¼ŒçŒœæµ‹é£é™©ä¸ºâ„›(**a**) = âˆ«âˆ’ log *p***a**(*x*)*d*ğ’°(*x*)**a**ã€‚è¿™ç±»å€¼åœ¨ä¿¡æ¯è®ºä¸­ä½œä¸ºä¸ç¡®å®šæ€§çš„åº¦é‡è¿›è¡Œç ”ç©¶ã€‚å› æ­¤ï¼Œæœ€å°åŒ–â„›(**a**)å°±å½’ç»“ä¸ºé€‰æ‹©èƒ½å¤Ÿæœ€å°åŒ–é‡‡æ ·ğ’°çš„ä¸ç¡®å®šæ€§çš„å‡è®¾**a**ã€‚è´¹èˆå°”æ¨èäº†ä¸€ç§å­¦ä¹ ç®—æ³•ï¼Œé€šè¿‡å®ƒå¯ä»¥é€‰æ‹©å…·æœ‰*æœ€å¤§ä¼¼ç„¶*çš„å‡è®¾ã€‚'
- en: The basic shortcoming of statistical testing is that the densities *p***a**
    must be known. They are presumed to arise from scientistsâ€™ minds, together with
    their hypotheses parametrized by **a**. Statistics thus provides a testing service,
    but the actual process of learning the hypotheses **a** is out of scope and left
    to the magic of insight and creativity. While Kolmogorov and his students were
    pondering this problem for decades and eventually solved it, a central part of
    the solution emerged inadvertently, and from an unexpected direction.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ç»Ÿè®¡æ£€éªŒçš„åŸºæœ¬ç¼ºç‚¹æ˜¯ï¼Œå¯†åº¦*p***a**å¿…é¡»æ˜¯å·²çŸ¥çš„ã€‚å®ƒä»¬è¢«å‡è®¾ä¸ºæ¥æºäºç§‘å­¦å®¶çš„æ€æƒ³ï¼Œä»¥åŠç”±**a**å‚æ•°åŒ–çš„å‡è®¾ã€‚å› æ­¤ï¼Œç»Ÿè®¡å­¦æä¾›çš„æ˜¯æ£€éªŒæœåŠ¡ï¼Œä½†å®é™…çš„å‡è®¾å­¦ä¹ è¿‡ç¨‹**a**è¶…å‡ºäº†ç»Ÿè®¡çš„èŒƒå›´ï¼Œç•™ç»™äº†ç›´è§‰å’Œåˆ›é€ åŠ›çš„ç¥å¥‡ã€‚åœ¨KolmogorovåŠå…¶å­¦ç”Ÿä»¬ç ”ç©¶è¿™ä¸€é—®é¢˜æ•°åå¹´å¹¶æœ€ç»ˆè§£å†³å®ƒçš„è¿‡ç¨‹ä¸­ï¼Œè§£å†³æ–¹æ¡ˆçš„æ ¸å¿ƒéƒ¨åˆ†æ— æ„é—´å‡ºç°åœ¨ä¸€ä¸ªæ„æƒ³ä¸åˆ°çš„æ–¹å‘ã€‚
- en: '**Perceptrons.** In 1943, McCulloch and Pitts proposed a mathematical model
    of the neuron. It boiled down to a state machine, like Turingâ€™s original 1936
    computer, just simpler, since it didnâ€™t have the external memory. In the late
    1950s, Frank Rosenblatt was working on expanding the model of a neuron into a
    model of the brain. It was a very ambitious project.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ„ŸçŸ¥æœº**ã€‚1943å¹´ï¼ŒMcCulloch å’Œ Pitts æå‡ºäº†ç¥ç»å…ƒçš„æ•°å­¦æ¨¡å‹ã€‚å®ƒç®€åŒ–ä¸ºä¸€ç§çŠ¶æ€æœºï¼Œå°±åƒå›¾çµ1936å¹´çš„åŸå§‹è®¡ç®—æœºä¸€æ ·ï¼Œåªæ˜¯æ›´ç®€å•ï¼Œå› ä¸ºå®ƒæ²¡æœ‰å¤–éƒ¨å­˜å‚¨å™¨ã€‚åœ¨1950å¹´ä»£æœ«ï¼ŒFrank
    Rosenblatt æ­£åœ¨ç ”ç©¶å°†ç¥ç»å…ƒæ¨¡å‹æ‰©å±•ä¸ºå¤§è„‘æ¨¡å‹ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸é›„å¿ƒå‹ƒå‹ƒçš„é¡¹ç›®ã€‚'
- en: '![](../Images/3e804fdec6259980b5176ec417cd51f4.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e804fdec6259980b5176ec417cd51f4.png)'
- en: Illustration from Rosenblattâ€™s 1958 project report to the Office of Naval Research.
    â€” Public domain
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥è‡ª Rosenblatt 1958å¹´å‘æµ·å†›ç ”ç©¶åŠå…¬å®¤æäº¤çš„é¡¹ç›®æŠ¥å‘Šæ’å›¾ã€‚ â€” å…¬å…±é¢†åŸŸ
- en: Rosenblatt, however, arrived at a component simpler than the McCulloch-Pitts
    neuron. He called it *perceptron*, to emphasize the difference of his project
    from the â€œvarious engineering projects concerned with automatic pattern recognition
    and â€˜artificial intelligenceâ€™ â€. Nevertheless, the project generated news reports
    with titles like â€œFrankenstein Monster Designed by Navy Robot That Thinksâ€, as
    Rosenblatt duly reports in his book[â´](#04e9).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼ŒRosenblatt æå‡ºäº†ä¸€ä¸ªæ¯” McCulloch-Pitts ç¥ç»å…ƒæ›´ç®€å•çš„ç»„ä»¶ã€‚ä»–ç§°ä¹‹ä¸º *æ„ŸçŸ¥æœº*ï¼Œä»¥å¼ºè°ƒä»–çš„é¡¹ç›®ä¸â€œå„ç§å·¥ç¨‹é¡¹ç›®ï¼Œå°¤å…¶æ˜¯è‡ªåŠ¨æ¨¡å¼è¯†åˆ«å’Œâ€˜äººå·¥æ™ºèƒ½â€™â€çš„åŒºåˆ«ã€‚ç„¶è€Œï¼Œè¯¥é¡¹ç›®äº§ç”Ÿäº†åƒâ€œæµ·å†›æœºå™¨äººè®¾è®¡çš„â€˜å¼—å…°è‚¯æ–¯å¦æ€ªç‰©â€™ï¼Œå®ƒèƒ½æ€è€ƒâ€è¿™æ ·çš„æ–°é—»æŠ¥é“ï¼Œæ­£å¦‚
    Rosenblatt åœ¨ä»–çš„ä¹¦ä¸­å¦‚å®æŠ¥é“çš„é‚£æ ·[â´](#04e9)ã€‚
- en: '***Mathematical neurons*** were defined as pairs **a** = *(b, âŸ¨w |)*, where[âµ](#7754)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '***æ•°å­¦ç¥ç»å…ƒ*** è¢«å®šä¹‰ä¸ºä¸€å¯¹ **a** = *(b, âŸ¨w |)*ï¼Œå…¶ä¸­[âµ](#7754)'
- en: '![](../Images/685ee22d76a5c57265d94dfb4e6f5f5c.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/685ee22d76a5c57265d94dfb4e6f5f5c.png)'
- en: and *b* is a scalar. It is meant to be a very simple program intepreted by the
    interpreter ğ’°. To evaluate **a** = *(b, âŸ¨w |)* on an input vector input vector
    | *x* âŸ©, the interpreter ğ’° applies the projection âŸ¨*w* | on | *x* âŸ© to get the
    inner product âŸ¨*w* | *x*âŸ©, which measures the length of the projection of either
    of the vectors on the other, and then it outputs the sign of the difference âŸ¨*w*
    | *x*âŸ© âˆ’ *b:*
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä¸” *b* æ˜¯ä¸€ä¸ªæ ‡é‡ã€‚å®ƒè¢«è®¤ä¸ºæ˜¯ç”±è§£é‡Šå™¨ ğ’° è§£é‡Šçš„éå¸¸ç®€å•çš„ç¨‹åºã€‚ä¸ºäº†åœ¨è¾“å…¥å‘é‡ | *x* âŸ© ä¸Šè¯„ä¼° **a** = *(b, âŸ¨w |)*ï¼Œè§£é‡Šå™¨
    ğ’° å¯¹ | *x* âŸ© è¿›è¡ŒæŠ•å½± âŸ¨*w* |ï¼Œä»¥è·å¾—å†…ç§¯ âŸ¨*w* | *x*âŸ©ï¼Œè¯¥å†…ç§¯è¡¡é‡äº†ä»»ä¸€å‘é‡åœ¨å¦ä¸€ä¸ªå‘é‡ä¸Šçš„æŠ•å½±é•¿åº¦ï¼Œç„¶åè¾“å‡ºå·®å€¼ âŸ¨*w* |
    *x*âŸ© âˆ’ *b* çš„ç¬¦å·ï¼š
- en: '![](../Images/164dc55568f7482bc20ce6b309a98f1f.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/164dc55568f7482bc20ce6b309a98f1f.png)'
- en: For a more succinct view, the pair **a** = *(b*, âŸ¨*w* |) and the input | *x*
    âŸ© are often modified to
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´ç®€æ´åœ°è¡¨è¾¾ï¼Œé€šå¸¸å°†é…å¯¹ **a** = *(b*, âŸ¨*w* |) å’Œè¾“å…¥ | *x* âŸ© ä¿®æ”¹ä¸º
- en: '![](../Images/a574620827c0a2ab52b1486b235983d8.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a574620827c0a2ab52b1486b235983d8.png)'
- en: so that the interpretation of a neuron boils down to
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·ï¼Œç¥ç»å…ƒçš„è§£é‡Šå°±ç®€åŒ–ä¸º
- en: '![](../Images/1bf3c497036c5ec001d527c51f3e5d30.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1bf3c497036c5ec001d527c51f3e5d30.png)'
- en: '***Perceptrons*** are compositions of such neurons. If a neuron is presented
    as a single row vector, then a perceptron is an (*n* + 1)-tuple of row vectors'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '***æ„ŸçŸ¥æœº*** æ˜¯ç”±è¿™äº›ç¥ç»å…ƒç»„æˆçš„ã€‚å¦‚æœä¸€ä¸ªç¥ç»å…ƒè¢«è¡¨ç¤ºä¸ºä¸€ä¸ªå•ä¸€çš„è¡Œå‘é‡ï¼Œé‚£ä¹ˆæ„ŸçŸ¥æœºå°±æ˜¯ä¸€ä¸ª (*n* + 1)-å…ƒç»„ï¼Œç”±è¡Œå‘é‡ç»„æˆã€‚'
- en: '![](../Images/b0de2483c9e7fd9cd0c13b6a8ae79518.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b0de2483c9e7fd9cd0c13b6a8ae79518.png)'
- en: On an input | *x*âŸ©, the interpretation of a perceptron **a** computes
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¾“å…¥ | *x*âŸ© ä¸Šï¼Œæ„ŸçŸ¥æœº **a** çš„è§£é‡Šè®¡ç®—
- en: '![](../Images/1f7918759774f69c8d77638d766f8aaa.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f7918759774f69c8d77638d766f8aaa.png)'
- en: For a more succinct view, the *n*-tuple of vectors âŸ¨*w*1 |, . . . , âŸ¨*wn* |
    can be arranged into the matrix
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´ç®€æ´åœ°è¡¨è¾¾ï¼Œ*n*-å…ƒç»„å‘é‡ âŸ¨*w*1 |, . . . , âŸ¨*wn* | å¯ä»¥æ’åˆ—æˆçŸ©é˜µ
- en: '![](../Images/7d1b7a7d209964290c21c98e9ff51128.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7d1b7a7d209964290c21c98e9ff51128.png)'
- en: so that the perceptron **a** = (âŸ¨*v*|, âŸ¨*w*1 |,â€¦,âŸ¨*wn* |) boils down to **a**
    = (âŸ¨*v*|, *W)* and its interpretation becomes
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæ„ŸçŸ¥æœº **a** = (âŸ¨*v*|, âŸ¨*w*1 |,â€¦,âŸ¨*wn* |) ç®€åŒ–ä¸º **a** = (âŸ¨*v*|, *W)*ï¼Œå…¶è§£é‡Šå˜ä¸º
- en: '![](../Images/1916147fd646b3e9dd170034b5110296.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1916147fd646b3e9dd170034b5110296.png)'
- en: To summarize in diagrams, here are the two presentations of a neuron on the
    left and the two presentations of a perceptron on the right.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç”¨å›¾ç¤ºæ€»ç»“ï¼Œå·¦ä¾§æ˜¯ç¥ç»å…ƒçš„ä¸¤ç§è¡¨ç¤ºæ–¹å¼ï¼Œå³ä¾§æ˜¯æ„ŸçŸ¥æœºçš„ä¸¤ç§è¡¨ç¤ºæ–¹å¼ã€‚
- en: '![](../Images/3e749bfb2dc23f8474b76caab7d7b485.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e749bfb2dc23f8474b76caab7d7b485.png)'
- en: Rosenblattâ€™s neuron and perceptron
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Rosenblatt çš„ç¥ç»å…ƒå’Œæ„ŸçŸ¥æœº
- en: The first row shows the neuron and the perceptron in the original form, with
    the thresholds *bj*. The second row shows the versions where each *bj* is absorbed
    as the 0-th component of the weight vector âŸ¨*wj* |.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€è¡Œæ˜¾ç¤ºäº†åŸå§‹å½¢å¼çš„ç¥ç»å…ƒå’Œæ„ŸçŸ¥æœºï¼Œå…¶ä¸­åŒ…æ‹¬é˜ˆå€¼ *bj*ã€‚ç¬¬äºŒè¡Œæ˜¾ç¤ºäº†æ¯ä¸ª *bj* è¢«å¸æ”¶ä¸ºæƒé‡å‘é‡ âŸ¨*wj* | çš„0-é¡¹ç‰ˆæœ¬ã€‚
- en: '**Perceptrons were a breakthrough into machine learning and inductive inference
    as two sides of the same coin.** Statistics provided the formal methods for hypothesis
    testing but left the task of learning and inferring hypotheses to informal methods
    and the magic of creativity. Perceptron training was the first formal method for
    inductive inference. Nowadays, this method looks obvious. The learner initiates
    the weights | *w* âŸ© and the thresholds *b* to arbitrary values, runs the interpreter
    ğ’°to generate predictions, compares them with the training data supplied by the
    supervisor *F*, and updates the weights proportionally to the losses â„’. This didnâ€™t
    seem like a big deal even to Frank Rosenblatt, who wrote that'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ„ŸçŸ¥æœºæ˜¯æœºå™¨å­¦ä¹ å’Œå½’çº³æ¨ç†çš„çªç ´ï¼ŒäºŒè€…æ˜¯åŒä¸€æšç¡¬å¸çš„ä¸¤é¢ã€‚** ç»Ÿè®¡å­¦æä¾›äº†å‡è®¾æ£€éªŒçš„æ­£å¼æ–¹æ³•ï¼Œä½†å°†å­¦ä¹ å’Œæ¨ç†å‡è®¾çš„ä»»åŠ¡ç•™ç»™äº†éæ­£å¼æ–¹æ³•å’Œåˆ›é€ åŠ›çš„é­”åŠ›ã€‚æ„ŸçŸ¥æœºè®­ç»ƒæ˜¯å½’çº³æ¨ç†çš„ç¬¬ä¸€ä¸ªæ­£å¼æ–¹æ³•ã€‚å¦‚ä»Šï¼Œè¿™ç§æ–¹æ³•çœ‹èµ·æ¥ç†æ‰€å½“ç„¶ã€‚å­¦ä¹ è€…å°†æƒé‡|
    *w* âŸ© å’Œé˜ˆå€¼ *b* åˆå§‹åŒ–ä¸ºä»»æ„å€¼ï¼Œè¿è¡Œè§£é‡Šå™¨ğ’°ç”Ÿæˆé¢„æµ‹ï¼Œå°†å…¶ä¸ç”±ç›‘ç£è€…*F*æä¾›çš„è®­ç»ƒæ•°æ®è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶æ ¹æ®æŸå¤±â„’æ¯”ä¾‹æ›´æ–°æƒé‡ã€‚è¿™ç”šè‡³å¯¹å¼—å…°å…‹Â·ç½—æ£®å¸ƒæ‹‰ç‰¹æ¥è¯´éƒ½ä¸ç®—ä»€ä¹ˆå¤§äº‹ï¼Œä»–å†™é“ï¼š'
- en: the perceptron program [was] not primarily concerned with the invention of devices
    for â€œartificial intelligenceâ€, but rather with investigating the physical structures
    and neurodynamic principles which underlie â€œnatural intelligenceâ€.
  id: totrans-102
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ„ŸçŸ¥æœºç¨‹åº[å¹¶é]ä¸»è¦å…³æ³¨â€œäººå·¥æ™ºèƒ½â€è®¾å¤‡çš„å‘æ˜ï¼Œè€Œæ˜¯ç ”ç©¶æ”¯æ’‘â€œè‡ªç„¶æ™ºèƒ½â€çš„ç‰©ç†ç»“æ„å’Œç¥ç»åŠ¨åŠ›å­¦åŸç†ã€‚
- en: Rosenblatt laid the stepping stone into machine learning while attempting to
    model the learning process in human brains. Even the very first learning machine
    was not purposefully designed but evolved spontaneously.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ç½—æ£®å¸ƒæ‹‰ç‰¹ä¸ºæœºå™¨å­¦ä¹ å¥ å®šäº†åŸºç¡€ï¼Œè¯•å›¾æ¨¡æ‹Ÿäººè„‘ä¸­çš„å­¦ä¹ è¿‡ç¨‹ã€‚å³ä½¿æ˜¯æœ€åˆçš„å­¦ä¹ æœºå™¨ä¹Ÿå¹¶éæœ‰ç›®çš„åœ°è®¾è®¡ï¼Œè€Œæ˜¯è‡ªå‘åœ°æ¼”åŒ–å‡ºæ¥çš„ã€‚
- en: It is often said that airplanes were not built by studying how the birds fly
    and that intelligent machines will not be built by looking inside peopleâ€™s heads.
    But there is more at hand. Perceptrons opened an alley into **learning as a *universal
    computational process.*** *Machine learning and human learning are particular
    implementations of the universal process of learning*, which is a natural process
    that evolves and diversifies. Machine learning models offer insights into a common
    denominator of all avatars of learning. The pattern of perceptron computation
    will be repeated on each of the models presented in the rest of this note.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: äººä»¬å¸¸è¯´ï¼Œé£æœºçš„å»ºé€ å¹¶ä¸æ˜¯é€šè¿‡ç ”ç©¶é¸Ÿç±»å¦‚ä½•é£è¡Œæ¥å®Œæˆçš„ï¼Œæ™ºèƒ½æœºå™¨ä¹Ÿä¸ä¼šé€šè¿‡è§‚å¯Ÿäººè„‘å†…éƒ¨çš„è¿ä½œæ¥å»ºé€ ã€‚ç„¶è€Œï¼Œäº‹æƒ…å¹¶éå¦‚æ­¤ç®€å•ã€‚æ„ŸçŸ¥æœºæ‰“å¼€äº†é€šå‘**ä½œä¸º*æ™®éè®¡ç®—è¿‡ç¨‹*çš„å­¦ä¹ çš„é“è·¯ã€‚**
    *æœºå™¨å­¦ä¹ å’Œäººç±»å­¦ä¹ æ˜¯å­¦ä¹ è¿™ä¸€æ™®éè¿‡ç¨‹çš„ç‰¹æ®Šå®ç°*ï¼Œå®ƒæ˜¯ä¸€ä¸ªè‡ªç„¶è¿‡ç¨‹ï¼Œå…·æœ‰æ¼”åŒ–å’Œå¤šæ ·åŒ–çš„ç‰¹å¾ã€‚æœºå™¨å­¦ä¹ æ¨¡å‹æä¾›äº†å¯¹æ‰€æœ‰å­¦ä¹ è¡¨ç°å½¢å¼çš„å…±åŒç‚¹çš„æ´å¯Ÿã€‚æ„ŸçŸ¥æœºè®¡ç®—çš„æ¨¡å¼å°†åœ¨æœ¬æ–‡æ¥ä¸‹æ¥ä»‹ç»çš„æ¯ä¸€ä¸ªæ¨¡å‹ä¸­å¾—åˆ°é‡å¤ã€‚
- en: 3\. Learning functions
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3. å­¦ä¹ å‡½æ•°
- en: 3.1\. Why learning is possible
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1. ä¸ºä»€ä¹ˆå­¦ä¹ æ˜¯å¯èƒ½çš„
- en: 'To understand why learning is possible, we first consider the special case
    when channel *F* is memoryless and deterministic: an ordinary function.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç†è§£ä¸ºä»€ä¹ˆå­¦ä¹ æ˜¯å¯èƒ½çš„ï¼Œæˆ‘ä»¬é¦–å…ˆè€ƒè™‘ä¸€ä¸ªç‰¹æ®Šæƒ…å†µï¼Œå³é€šé“ *F* æ˜¯æ— è®°å¿†å’Œç¡®å®šæ€§çš„ï¼šä¸€ä¸ªæ™®é€šçš„å‡½æ•°ã€‚
- en: '**Learnable functions are continuous.** What can be learned about a function
    *F*:*X*âŸ¶*Y* from a finite set of pairs (*x*1,*y*1), (*x*2,*y*2),â€¦,(*xn*,*yn*),
    where *F*(*xi*) = *yi*? Generally nothing. Knowing *F*(*x*) does not tell anything
    about *F*(*x*â€²), unless *x* and *x*â€² are related in some way, and *F* preserves
    their relation. To generalize the observed sample (*x*1, *y*1), . . . , (*xn*,
    *yn*) and predict a classification *F*(*x*â€²) = *y*â€² for an unobserved data item
    *x*â€², it is necessary that'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¯å­¦ä¹ çš„å‡½æ•°æ˜¯è¿ç»­çš„ã€‚** ä»ä¸€ç»„æœ‰é™çš„å¯¹(*x*1,*y*1)ã€(*x*2,*y*2)ã€â€¦ã€(*xn*,*yn*)ä¸­ï¼Œæˆ‘ä»¬èƒ½å­¦åˆ°å…³äºå‡½æ•° *F*:*X*âŸ¶*Y*
    çš„ä»€ä¹ˆï¼Ÿé€šå¸¸ä»€ä¹ˆä¹Ÿå­¦ä¸åˆ°ã€‚çŸ¥é“ *F*(*x*) å¹¶ä¸èƒ½å‘Šè¯‰æˆ‘ä»¬ *F*(*x*â€²) çš„ä»»ä½•ä¿¡æ¯ï¼Œé™¤é *x* å’Œ *x*â€² ä»¥æŸç§æ–¹å¼ç›¸å…³ï¼Œå¹¶ä¸” *F*
    ä¿æŒå®ƒä»¬ä¹‹é—´çš„å…³ç³»ã€‚ä¸ºäº†å°†è§‚å¯Ÿåˆ°çš„æ ·æœ¬ (*x*1, *y*1)ã€â€¦ã€(*xn*, *yn*) æ³›åŒ–å¹¶é¢„æµ‹æœªè§‚å¯Ÿåˆ°çš„æ•°æ®é¡¹ *x*â€² çš„åˆ†ç±» *F*(*x*â€²)
    = *y*â€²ï¼Œå¿…é¡»æ»¡è¶³ï¼š'
- en: '*x*â€² is related to *x*1,â€¦,*xn*,'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*â€²ä¸ *x*1,â€¦,*xn* ç›¸å…³ï¼Œ'
- en: '*y*â€² is related to *y*1,â€¦,*yn*, where *yi* = *F*(*xi*) for *i* = 1,â€¦,*n*, and'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y*â€² ä¸ *y*1,â€¦,*yn* ç›¸å…³ï¼Œå…¶ä¸­ *yi* = *F*(*xi*) å¯¹äº *i* = 1,â€¦,*n*ï¼Œå¹¶ä¸”'
- en: '*F* preserves the relations.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*F* ä¿æŒè¿™äº›å…³ç³»ã€‚'
- en: If the sets of the *x*s and the *y*s in such relationships are viewed as *neighborhoods*,
    then the datatype *X* and the classifier type *Y* become *topological* spaces.
    The neighborhoods form topologies. Donâ€™t worry if you donâ€™t know the formal definition
    of a topology. It is just an abstract way to say that *x* and *x*â€² live in the
    same neighborhood. A function *F*:*X*âŸ¶*Y* is *continuous* when it maps neighbors
    to neighbors. And the neighborhoods donâ€™t have to be physical proximities. Two
    words with similar meanings live in a semantical neighborhood. Any kind of relation
    can be expressed in terms of neighborhoods. So if *x*â€² is related with *x*1 and
    *x*2, and *F* is continuous, then *y*â€² = *F*(*x*â€²) is related with *y*1 = *F*(*x*1)
    and *y*2 = *F*(*x*2). That allows us to learn from a set of pairs (*x*1, *y*1),
    . . . , (*xn*, *yn*) where *F*(*xi*) = *yi* that *F*(*x*â€²) = *y*â€² also holds.
    Then we can add the pair (*x*â€², *y*â€²) to the list as a prediction. Without the
    neighborhoods and the continuity, we cannot make such predictions. To be learnable
    a function must be continuous.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå°†è¿™ç§å…³ç³»ä¸­çš„ *x* å’Œ *y* çš„é›†åˆè§†ä¸º *é‚»åŸŸ*ï¼Œé‚£ä¹ˆæ•°æ®ç±»å‹ *X* å’Œåˆ†ç±»å™¨ç±»å‹ *Y* å°±å˜æˆäº† *æ‹“æ‰‘* ç©ºé—´ã€‚é‚»åŸŸå½¢æˆæ‹“æ‰‘ã€‚å¦‚æœä½ ä¸ç†Ÿæ‚‰æ‹“æ‰‘çš„æ­£å¼å®šä¹‰ï¼Œä¹Ÿä¸ç”¨æ‹…å¿ƒã€‚è¿™åªæ˜¯ä¸€ä¸ªæŠ½è±¡çš„æ–¹å¼ï¼Œæ„å‘³ç€
    *x* å’Œ *x*â€² ç”Ÿæ´»åœ¨åŒä¸€ä¸ªé‚»åŸŸä¸­ã€‚å½“ä¸€ä¸ªå‡½æ•° *F*:*X* âŸ¶ *Y* å°†é‚»åŸŸæ˜ å°„åˆ°é‚»åŸŸæ—¶ï¼Œå®ƒæ˜¯ *è¿ç»­* çš„ã€‚è€Œä¸”é‚»åŸŸä¸ä¸€å®šæ˜¯ç‰©ç†ä¸Šçš„æ¥è¿‘ã€‚ä¸¤ä¸ªå«ä¹‰ç›¸ä¼¼çš„è¯æ±‡ç”Ÿæ´»åœ¨è¯­ä¹‰é‚»åŸŸä¸­ã€‚ä»»ä½•ç§ç±»çš„å…³ç³»éƒ½å¯ä»¥é€šè¿‡é‚»åŸŸæ¥è¡¨è¾¾ã€‚æ‰€ä»¥ï¼Œå¦‚æœ
    *x*â€² ä¸ *x*1 å’Œ *x*2 æœ‰å…³ç³»ï¼Œå¹¶ä¸” *F* æ˜¯è¿ç»­çš„ï¼Œé‚£ä¹ˆ *y*â€² = *F*(*x*â€²) ä¹Ÿä¸ *y*1 = *F*(*x*1) å’Œ *y*2
    = *F*(*x*2) æœ‰å…³ç³»ã€‚è¿™ä½¿å¾—æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€ç»„å¯¹ (*x*1, *y*1), . . . , (*xn*, *yn*)ï¼Œå…¶ä¸­ *F*(*xi*) =
    *yi*ï¼Œæ¥å­¦ä¹  *F*(*x*â€²) = *y*â€² ä¹Ÿæˆç«‹ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥å°†å¯¹ (*x*â€², *y*â€²) æ·»åŠ åˆ°åˆ—è¡¨ä¸­ï¼Œä½œä¸ºä¸€ç§é¢„æµ‹ã€‚å¦‚æœæ²¡æœ‰é‚»åŸŸå’Œè¿ç»­æ€§ï¼Œæˆ‘ä»¬æ— æ³•åšå‡ºè¿™æ ·çš„é¢„æµ‹ã€‚è¦èƒ½å¤Ÿå­¦ä¹ ï¼Œå‡½æ•°å¿…é¡»æ˜¯è¿ç»­çš„ã€‚
- en: 'There are many ways in which this is used, and many details to work out. For
    the moment, just note that *learning is based on associations.* You associate
    a set of names *X* with a set of faces *Y* along a continuous function *F*:*X*
    âŸ¶*Y.* You remember the face *F*(Allison) by searching through the pairs (*x*1,
    *y*1), . . . , (*xn*, *yn*) where the names *xi* are associated with Allisonâ€™s.
    Since *F* is continuous, the faces *yi* = *F*(*xi*) must be associated with Allisonâ€™s.
    Therefore, if you find a face of a neighbor of Allisonâ€™s name, then you can find
    Allisonâ€™s face in the neighborhood of the face of Allisonâ€™s neighbor. This is
    how *associative memory* works: as a family of continuous functions. The *key-value
    associations* in databases work similarly. Both in human memory and in databases,
    associative memory is implemented using referential neighborhoods. Functions are
    learnable when they preserve associations. They preserve associations when they
    are continuous.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åœ¨è®¸å¤šæ–¹é¢éƒ½æœ‰åº”ç”¨ï¼Œä¸”æœ‰è®¸å¤šç»†èŠ‚éœ€è¦è§£å†³ã€‚æš‚æ—¶åªéœ€æ³¨æ„ï¼Œ*å­¦ä¹ æ˜¯åŸºäºå…³è”çš„*ã€‚ä½ å°†ä¸€ç»„åå­— *X* ä¸ä¸€ç»„é¢å­” *Y* å…³è”åœ¨ä¸€èµ·ï¼Œé€šè¿‡ä¸€ä¸ªè¿ç»­å‡½æ•°
    *F*:*X* âŸ¶ *Y*ã€‚ä½ é€šè¿‡åœ¨å¯¹ (*x*1, *y*1), . . . , (*xn*, *yn*) ä¸­æœç´¢åå­— *xi*ï¼Œæ¥è®°ä½é¢å­” *F*(Allison)ã€‚å› ä¸º
    *F* æ˜¯è¿ç»­çš„ï¼Œé¢å­” *yi* = *F*(*xi*) å¿…é¡»ä¸ Allison çš„é¢å­”æœ‰å…³è”ã€‚å› æ­¤ï¼Œå¦‚æœä½ æ‰¾åˆ°ä¸€ä¸ªä¸ Allison çš„åå­—é‚»è¿‘çš„é¢å­”ï¼Œé‚£ä¹ˆä½ å°±å¯ä»¥åœ¨
    Allison é‚»å±…çš„é¢å­”çš„é‚»åŸŸä¸­æ‰¾åˆ° Allison çš„é¢å­”ã€‚è¿™å°±æ˜¯ *å…³è”è®°å¿†* çš„å·¥ä½œæ–¹å¼ï¼šä½œä¸ºä¸€ç±»è¿ç»­å‡½æ•°ã€‚æ•°æ®åº“ä¸­çš„ *é”®å€¼å…³è”* å·¥ä½œåŸç†ç±»ä¼¼ã€‚åœ¨äººç±»è®°å¿†å’Œæ•°æ®åº“ä¸­ï¼Œå…³è”è®°å¿†æ˜¯é€šè¿‡å‚ç…§é‚»åŸŸå®ç°çš„ã€‚å½“å‡½æ•°ä¿æŒå…³è”æ—¶ï¼Œå®ƒä»¬æ˜¯å¯å­¦ä¹ çš„ã€‚å®ƒä»¬åœ¨è¿ç»­æ—¶ä¿æŒå…³è”ã€‚
- en: '**Continuous functions can be partially evaluated and linearly approximated.**
    The Fundamental Theorem of Calculus says, roughly, that the derivative and the
    integral, as operations on functions, are each otherâ€™s inverses. The integral
    approximates with arbitrary precision any differentiable function by linear combinations
    of step functions that approximate the derivative of the function. Any differentiable
    function is linearly approximable by piecewise linear functions.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¿ç»­å‡½æ•°å¯ä»¥è¢«éƒ¨åˆ†æ±‚å€¼å¹¶è¿›è¡Œçº¿æ€§è¿‘ä¼¼ã€‚** å¾®ç§¯åˆ†åŸºæœ¬å®šç†å¤§è‡´ä¸Šè¯´ï¼Œå¯¼æ•°å’Œç§¯åˆ†ä½œä¸ºå¯¹å‡½æ•°çš„è¿ç®—æ˜¯ç›¸äº’é€†æ“ä½œã€‚ç§¯åˆ†é€šè¿‡å°†ä»»ä½•å¯å¾®åˆ†å‡½æ•°è¿‘ä¼¼ä¸ºç”±é˜¶æ¢¯å‡½æ•°çº¿æ€§ç»„åˆçš„å½¢å¼ï¼Œæ¥ä»¥ä»»æ„ç²¾åº¦è¿‘ä¼¼è¯¥å‡½æ•°çš„å¯¼æ•°ã€‚ä»»ä½•å¯å¾®åˆ†å‡½æ•°éƒ½å¯ä»¥é€šè¿‡åˆ†æ®µçº¿æ€§å‡½æ•°è¿›è¡Œçº¿æ€§è¿‘ä¼¼ã€‚'
- en: A function that is just continuous (not differentiable) may not be approximable
    by piecewise linear functions. Yet it turns out that it can always be approximated
    by linear combinations of pieces of a continuous function (not linear or polynomial),
    usually called *actuation.* The approximating linear combinations of this nonlinear
    function are *learnable*. Hence machine learning.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªä»…ä»…æ˜¯è¿ç»­ï¼ˆè€Œä¸æ˜¯å¯å¾®åˆ†ï¼‰çš„å‡½æ•°å¯èƒ½æ— æ³•é€šè¿‡åˆ†æ®µçº¿æ€§å‡½æ•°è¿›è¡Œè¿‘ä¼¼ã€‚ç„¶è€Œï¼Œäº‹å®è¯æ˜å®ƒå§‹ç»ˆå¯ä»¥é€šè¿‡è¿ç»­å‡½æ•°ç‰‡æ®µçš„çº¿æ€§ç»„åˆè¿›è¡Œè¿‘ä¼¼ï¼ˆè€Œä¸æ˜¯çº¿æ€§æˆ–å¤šé¡¹å¼ï¼‰ï¼Œé€šå¸¸ç§°ä¸º
    *é©±åŠ¨*ã€‚è¿™ä¸ªéçº¿æ€§å‡½æ•°çš„è¿‘ä¼¼çº¿æ€§ç»„åˆæ˜¯ *å¯å­¦ä¹ çš„*ã€‚å› æ­¤ï¼Œæœºå™¨å­¦ä¹ å¾—ä»¥å®ç°ã€‚
- en: On the other hand, the approximability of continuous functions has remained
    one of the big secrets of calculus. The fact that ***all continuous functions
    can be decomposed into sums of single-variable continuous functions***defies most
    peopleâ€™s intuitions. It says that, as far as computations are concerned, there
    are no genuine multi-dimensional phenomena among continuous functions. All those
    complicated multi-variable functions you may have seen in a vector calculus textbook,
    or encountered in practice if you are an engineer or a scientist â€” they can all
    be partially evaluated, each variable separately. Which is why they can be learned.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€æ–¹é¢ï¼Œè¿ç»­å‡½æ•°çš„å¯é€¼è¿‘æ€§ä»ç„¶æ˜¯å¾®ç§¯åˆ†ä¸­çš„ä¸€ä¸ªé‡å¤§ç§˜å¯†ã€‚***æ‰€æœ‰è¿ç»­å‡½æ•°éƒ½å¯ä»¥åˆ†è§£ä¸ºå•å˜é‡è¿ç»­å‡½æ•°çš„å’Œ***è¿™ä¸€äº‹å®æŒ‘æˆ˜äº†å¤§å¤šæ•°äººçš„ç›´è§‰ã€‚å®ƒè¡¨æ˜ï¼Œå°±è®¡ç®—è€Œè¨€ï¼Œè¿ç»­å‡½æ•°ä¸­æ²¡æœ‰çœŸæ­£çš„å¤šç»´ç°è±¡ã€‚ä½ å¯èƒ½åœ¨çŸ¢é‡å¾®ç§¯åˆ†æ•™æä¸­çœ‹åˆ°è¿‡çš„ï¼Œæˆ–è€…å¦‚æœä½ æ˜¯å·¥ç¨‹å¸ˆæˆ–ç§‘å­¦å®¶ï¼Œåœ¨å®è·µä¸­é‡åˆ°è¿‡çš„é‚£äº›å¤æ‚çš„å¤šå˜é‡å‡½æ•°â€”â€”å®ƒä»¬éƒ½å¯ä»¥éƒ¨åˆ†æ±‚è§£ï¼Œæ¯ä¸ªå˜é‡åˆ†åˆ«å¤„ç†ã€‚è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆå®ƒä»¬å¯ä»¥è¢«å­¦ä¹ çš„åŸå› ã€‚
- en: '3.2 Decomposing continuous functions: Kolmogorov-Arnold[â¶](#d246)'
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 è¿ç»­å‡½æ•°çš„åˆ†è§£ï¼šKolmogorov-Arnold[â¶](#d246)
- en: '**Hilbertâ€™s 13th Problem.** Back in the year 1900, the famous mathematician
    David Hilbert offered his famous list of 23 mathematical problems for the next
    century. Number 13 on the list was the question if all functions with 3 variables
    can be expressed by composing functions of 2 variables. Hilbert conjectured that
    a specific function, the formula for the solutions of the equation *x*â· + *axÂ³*
    + *bx*Â² + *cx* + 1 = 0 expressed in terms of the coefficients *a*, *b*, and *c*,
    could not be decomposed into functions of pairs of the coefficients. More than
    half-way through the century, 19-year-old Vladimir Arnold proved that all continuous
    functions with 3 variables can be decomposed into continuous functions with 2
    variables and disproved Hilbertâ€™s conjecture. Next year, Arnoldâ€™s thesis advisor
    Andrey Kolmogorov proved a stunning generalization. The theorem has been strengthened
    and simplified ever since. Early simplifications were based on the following embedding
    of the *d*-dimensional cube into the (2*d*+1)-dimensional cube, constructed to
    allow separating the *d* variables in any continuous function.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¸Œå°”ä¼¯ç‰¹çš„ç¬¬13ä¸ªé—®é¢˜ã€‚** 1900å¹´ï¼Œè‘—åæ•°å­¦å®¶å¤§å«Â·å¸Œå°”ä¼¯ç‰¹æå‡ºäº†ä»–ä¸ºä¸‹ä¸€ä¸ªä¸–çºªè®¾å®šçš„23ä¸ªæ•°å­¦é—®é¢˜ï¼Œå…¶ä¸­ç¬¬13ä¸ªé—®é¢˜æ˜¯ï¼Œæ‰€æœ‰å…·æœ‰3ä¸ªå˜é‡çš„å‡½æ•°æ˜¯å¦å¯ä»¥é€šè¿‡ç»„åˆå…·æœ‰2ä¸ªå˜é‡çš„å‡½æ•°æ¥è¡¨ç¤ºã€‚å¸Œå°”ä¼¯ç‰¹çŒœæµ‹ï¼Œä¸€ä¸ªç‰¹å®šçš„å‡½æ•°ï¼Œè¡¨ç¤ºæ–¹ç¨‹
    *x*â· + *axÂ³* + *bx*Â² + *cx* + 1 = 0 çš„è§£çš„å…¬å¼ï¼Œç”¨ç³»æ•° *a*ã€*b* å’Œ *c* è¡¨ç¤ºï¼Œä¸èƒ½åˆ†è§£ä¸ºç³»æ•°å¯¹çš„å‡½æ•°ã€‚åˆ°ä¸–çºªè¿‡åŠï¼Œ19å²çš„å¼—æ‹‰åŸºç±³å°”Â·é˜¿è¯ºå¾·è¯æ˜äº†æ‰€æœ‰å…·æœ‰3ä¸ªå˜é‡çš„è¿ç»­å‡½æ•°éƒ½å¯ä»¥åˆ†è§£ä¸ºå…·æœ‰2ä¸ªå˜é‡çš„è¿ç»­å‡½æ•°ï¼Œä»è€Œæ¨ç¿»äº†å¸Œå°”ä¼¯ç‰¹çš„çŒœæƒ³ã€‚æ¬¡å¹´ï¼Œé˜¿è¯ºå¾·çš„è®ºæ–‡å¯¼å¸ˆå®‰å¾·çƒˆÂ·ç§‘å°”è«å“¥æ´›å¤«è¯æ˜äº†ä¸€ä¸ªæƒŠäººçš„æ¨å¹¿ã€‚è‡ªé‚£æ—¶ä»¥æ¥ï¼Œè¿™ä¸€å®šç†ä¸€ç›´åœ¨ä¸æ–­å¼ºåŒ–å’Œç®€åŒ–ã€‚æ—©æœŸçš„ç®€åŒ–æ˜¯åŸºäºä»¥ä¸‹å°†
    *d* ç»´ç«‹æ–¹ä½“åµŒå…¥åˆ° (2*d*+1) ç»´ç«‹æ–¹ä½“ä¸­çš„æ„é€ ï¼Œæ—¨åœ¨å…è®¸åˆ†ç¦»ä»»ä½•è¿ç»­å‡½æ•°ä¸­çš„ *d* å˜é‡ã€‚'
- en: '![](../Images/0fa93b3737e5e2706790a8e1dd455902.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0fa93b3737e5e2706790a8e1dd455902.png)'
- en: Together with a fixed (2*d*+1)-dimensional vector âŸ¨*v* | , the embedding[â·](#a35f)
    *W*, yields the claimed decomposition.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ä¸€ä¸ªå›ºå®šçš„ (2*d*+1) ç»´å‘é‡ âŸ¨*v*| ä¸€èµ·ï¼ŒåµŒå…¥[â·](#a35f) *W*ï¼Œå¾—åˆ°äº†æ‰€å£°ç§°çš„åˆ†è§£ã€‚
- en: '![](../Images/9e71bdeaf3888d4c4fe110fee6cab9ec.png)![](../Images/fe99a93b37f4643fc3153b462fad4fe7.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e71bdeaf3888d4c4fe110fee6cab9ec.png)![](../Images/fe99a93b37f4643fc3153b462fad4fe7.png)'
- en: Kolmogorov-Arnold Decomposition
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Kolmogorov-Arnold åˆ†è§£
- en: '**Comments and explanations.** Unfolding the decomposition yields'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¯„è®ºä¸è§£é‡Šã€‚** å±•å¼€åˆ†è§£å¾—åˆ°ï¼š'
- en: '![](../Images/8c888e333ae7118389191320c5882db5.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c888e333ae7118389191320c5882db5.png)'
- en: 'Only ğœ‘ depends on *f*, whereas *W* and *v* are given globally, for all functions
    of *d* variables. They are not unique and *W* can be chosen so that âŸ¨*v*| is a
    vector of 1s, as it is assumed in this unfolded version. The constructions not
    only disproved Hilbertâ€™s conjecture, but still defy most peopleâ€™s geometric intuitions.
    The reason may be that we tend to think in terms of smooth functions, whereas
    the funcions ğœ“ and ğœ‘ are heavily fractal. They are constructed using copies of
    the Devil Staircase or space-filling curves. The geometric interpretation of the
    embedding *W* is that the (2*d*+1)-tuple of ğœ“s draws a curve in the (2*d*+1)-dimensional
    cube and copies of that curve span a homeomorphic image of the *d*-dimensional
    cube:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: åªæœ‰ ğœ‘ ä¾èµ–äº *f*ï¼Œè€Œ *W* å’Œ *v* æ˜¯å…¨å±€ç»™å®šçš„ï¼Œé€‚ç”¨äºæ‰€æœ‰ *d* å˜é‡çš„å‡½æ•°ã€‚å®ƒä»¬ä¸æ˜¯å”¯ä¸€çš„ï¼Œ*W* å¯ä»¥é€‰æ‹©ä½¿å¾— âŸ¨*v*| æ˜¯ä¸€ä¸ªå…¨æ˜¯
    1 çš„å‘é‡ï¼Œæ­£å¦‚åœ¨è¿™ä¸ªå±•å¼€ç‰ˆæœ¬ä¸­å‡è®¾çš„é‚£æ ·ã€‚è¿™äº›æ„é€ ä¸ä»…æ¨ç¿»äº†å¸Œå°”ä¼¯ç‰¹çŒœæƒ³ï¼Œè€Œä¸”è‡³ä»Šä»æŒ‘æˆ˜ç€å¤§å¤šæ•°äººçš„å‡ ä½•ç›´è§‰ã€‚å…¶åŸå› å¯èƒ½åœ¨äºæˆ‘ä»¬ä¹ æƒ¯äºç”¨å…‰æ»‘å‡½æ•°æ¥æ€è€ƒï¼Œè€Œ
    ğœ“ å’Œ ğœ‘ å‡½æ•°åˆ™æ˜¯é«˜åº¦åˆ†å½¢çš„ã€‚å®ƒä»¬æ˜¯é€šè¿‡å¤åˆ¶æ¶é­”æ¥¼æ¢¯æˆ–ç©ºé—´å¡«å……æ›²çº¿æ¥æ„é€ çš„ã€‚*W* çš„å‡ ä½•è§£é‡Šæ˜¯ï¼Œ(2*d*+1) å…ƒç»„çš„ ğœ“ æç»˜äº†ä¸€æ¡åœ¨ (2*d*+1)
    ç»´ç«‹æ–¹ä½“ä¸­çš„æ›²çº¿ï¼Œè¿™æ¡æ›²çº¿çš„å¤åˆ¶æ„æˆäº†ä¸€ä¸ªåŒèƒšå›¾åƒï¼Œæ˜ å°„åˆ° *d* ç»´ç«‹æ–¹ä½“ï¼š
- en: '![](../Images/4183261dd51555a36478795ffce8233f.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4183261dd51555a36478795ffce8233f.png)'
- en: This is the first component of *W* in the diagram above. The projections on
    âŸ¨*w*| of vectors from within the *d*-cube determine the linear combinations of
    copies of ğœ“ whose inverse images iteratively fill the *d*-cube. Kolmogorovâ€™s original
    construction partitioned the mapping *f* along the edges of the *d-*cube and combined
    *d* differentfunctions ğœ‘ to represent *f* . Sprecher and Lorentz later noticed
    that additional stretching allows capturing all parts of *f* by a single ğœ‘. This
    is possible because the dependency of *f* on each of its *d* variables can be
    approximated with arbitrary precision on a null-subset of its domain, and the
    null-subsets of *[0,1]* can be made disjoint. The upshot is that *the only genuinely
    multi-variable continuous function is the addition*. The multiple inputs for multi-variable
    continuous functions can always be preprocessed in such a way that each input
    is processed separately, by a single-variable function. The output of the original
    multi-variable function is then obtained by adding up the outputs of the single-variable
    components. *Continuous functions are thus partially evaluated, each input separately.*
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸Šå›¾ä¸­ *W* çš„ç¬¬ä¸€ä¸ªç»„æˆéƒ¨åˆ†ã€‚æ¥è‡ª *d* ç«‹æ–¹ä½“å†…çš„å‘é‡åœ¨ âŸ¨*w*| ä¸Šçš„æŠ•å½±å†³å®šäº† ğœ“ çš„å‰¯æœ¬çº¿æ€§ç»„åˆï¼Œåå‘æ˜ å°„é€šè¿‡è¿­ä»£å¡«å…… *d* ç«‹æ–¹ä½“ã€‚Kolmogorov
    æœ€åˆçš„æ„é€ å°†æ˜ å°„ *f* æ²¿ *d* ç«‹æ–¹ä½“çš„è¾¹è¿›è¡Œåˆ†å‰²ï¼Œå¹¶ç»“åˆäº† *d* ä¸ªä¸åŒçš„å‡½æ•° ğœ‘ æ¥è¡¨ç¤º *f*ã€‚Sprecher å’Œ Lorentz åæ¥æ³¨æ„åˆ°ï¼Œé¢å¤–çš„æ‹‰ä¼¸å¯ä»¥é€šè¿‡å•ä¸ª
    ğœ‘ æ•æ‰ *f* çš„æ‰€æœ‰éƒ¨åˆ†ã€‚è¿™æ˜¯å¯èƒ½çš„ï¼Œå› ä¸º *f* å¯¹å…¶æ¯ä¸ª *d* å˜é‡çš„ä¾èµ–å¯ä»¥åœ¨å…¶å®šä¹‰åŸŸçš„é›¶å­é›†ä¸Šä»¥ä»»æ„ç²¾åº¦é€¼è¿‘ï¼Œå¹¶ä¸” *[0,1]* çš„é›¶å­é›†å¯ä»¥è¢«åˆ†å‰²æˆä¸ç›¸äº¤çš„éƒ¨åˆ†ã€‚ç»“æœæ˜¯ï¼Œ*å”¯ä¸€çœŸæ­£çš„å¤šå˜é‡è¿ç»­å‡½æ•°æ˜¯åŠ æ³•*ã€‚å¤šå˜é‡è¿ç»­å‡½æ•°çš„å¤šä¸ªè¾“å…¥æ€»æ˜¯å¯ä»¥é€šè¿‡å•å˜é‡å‡½æ•°åˆ†åˆ«é¢„å¤„ç†ï¼Œä½¿æ¯ä¸ªè¾“å…¥éƒ½ç”±å•ç‹¬çš„å•å˜é‡å‡½æ•°å¤„ç†ã€‚ç„¶åé€šè¿‡å°†å•å˜é‡ç»„ä»¶çš„è¾“å‡ºåŠ èµ·æ¥ï¼Œè·å¾—åŸå§‹å¤šå˜é‡å‡½æ•°çš„è¾“å‡ºã€‚*å› æ­¤ï¼Œè¿ç»­å‡½æ•°æ˜¯éƒ¨åˆ†æ±‚å€¼çš„ï¼Œæ¯ä¸ªè¾“å…¥åˆ†åˆ«å¤„ç†*ã€‚
- en: The price to be paid is that the single-variable continuous functions that perform
    the preprocessing and the processing are complicated, ineffective, and constructed
    through iterative approximations. For a long time, the iterative fugue of Kolmogorovâ€™s
    proof was viewed as a glimpse from the darkness of a world of complexities beyond
    any our our imagination or utility. Then in the late 1980s, Hecht-Nielsen noticed
    that the Kolmogorov-Arnold decomposition seemed related to the perceptron architecture,
    as the diagrams above also suggest. What is going on?
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ä»˜å‡ºçš„ä»£ä»·æ˜¯ï¼Œæ‰§è¡Œé¢„å¤„ç†å’Œå¤„ç†çš„å•å˜é‡è¿ç»­å‡½æ•°æ—¢å¤æ‚åˆä½æ•ˆï¼Œå¹¶é€šè¿‡è¿­ä»£é€¼è¿‘æ„é€ è€Œæˆã€‚é•¿æœŸä»¥æ¥ï¼ŒKolmogorov è¯æ˜ä¸­çš„è¿­ä»£å¼ fuga è¢«è®¤ä¸ºæ˜¯ä»ä¸€ä¸ªè¶…å‡ºæˆ‘ä»¬ä»»ä½•æƒ³è±¡å’Œå®ç”¨æ€§çš„å¤æ‚ä¸–ç•Œä¸­çª¥è§çš„ä¸€çº¿æ›™å…‰ã€‚ç›´åˆ°
    1980 å¹´ä»£æœ«ï¼ŒHecht-Nielsen æ³¨æ„åˆ° Kolmogorov-Arnold åˆ†è§£ä¼¼ä¹ä¸æ„ŸçŸ¥æœºç»“æ„æœ‰å…³ï¼Œæ­£å¦‚ä¸Šé¢çš„å›¾ç¤ºæ‰€æš—ç¤ºçš„é‚£æ ·ã€‚åˆ°åº•å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ
- en: 3.3 Wide learning
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 å®½åº¦å­¦ä¹ 
- en: So far we studieda[***theoretic*** construction](#2f11) providing
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¸€ä¸ª[***ç†è®ºæ€§çš„*** æ„é€ ](#2f11)ï¼Œæä¾›
- en: an ***exact*** *representation* of *f* using
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª***ç²¾ç¡®çš„*** *è¡¨ç¤º* *f*ï¼Œä½¿ç”¨
- en: a projection-embedding pair (âŸ¨*v*|*,W)* ***independent*** on *f* and
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæŠ•å½±-åµŒå…¥å¯¹ï¼ˆâŸ¨*v*|*,W)* ***ç‹¬ç«‹äº*** *f* å’Œ
- en: an ***approximate*** *construction* ğœ‘ ***dependent*** on *f.*
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª ***è¿‘ä¼¼çš„*** *æ„é€ * ğœ‘ ***ä¾èµ–äº*** *f*ã€‚
- en: Now we turn to a [***practical*** construction](#5646) providing
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬è½¬å‘ä¸€ä¸ª[***å®é™…çš„*** æ„é€ ](#5646)ï¼Œæä¾›
- en: an ***approximate*** *representation* of *f* using
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª ***è¿‘ä¼¼çš„*** *è¡¨ç¤º* *f*ï¼Œä½¿ç”¨
- en: a projection-embedding pair (âŸ¨*v*|*,W)* ***dependent*** on *f* and
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæŠ•å½±-åµŒå…¥å¯¹ï¼ˆâŸ¨*v*|*,W)* ***ä¾èµ–äº*** *f* å’Œ
- en: an ***exact*** *construction Ïƒ* ***independent*** on *f.*
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª***ç²¾ç¡®çš„*** *æ„é€  Ïƒ* ***ç‹¬ç«‹äº*** *f*ã€‚
- en: 'The step from the Continuous Decomposition above to the Neural Approximation
    below is illustrated by comparing the [diagram](#3071) of the KA representation
    above with the following diagrams of the CHSW representation:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä¸Šé¢çš„è¿ç»­åˆ†è§£åˆ°ä¸‹é¢çš„ç¥ç»é€¼è¿‘çš„æ­¥éª¤ï¼Œé€šè¿‡æ¯”è¾ƒä¸Šé¢ KA è¡¨ç¤ºæ³•çš„[å›¾ç¤º](#3071)ä¸ä¸‹é¢ CHSW è¡¨ç¤ºæ³•çš„å›¾ç¤ºæ¥è¯´æ˜ï¼š
- en: '![](../Images/96d0dcc21aa912e2e6832016edf44773.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96d0dcc21aa912e2e6832016edf44773.png)'
- en: Neuron with a *Ïƒ-activation*
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¦æœ‰ *Ïƒ æ¿€æ´»å‡½æ•°* çš„ç¥ç»å…ƒ
- en: Letting *W* vary with *f* allows omitting the deformations ğœ“. Letting âŸ¨*v*|
    vary with *f* allows replacing ğœ‘ with a fixed *activation* function, independent
    on *f*.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: è®© *W* éš *f* å˜åŒ–ï¼Œå¯ä»¥çœç•¥å˜å½¢ ğœ“ã€‚è®© âŸ¨*v*| éš *f* å˜åŒ–ï¼Œå¯ä»¥ç”¨ä¸€ä¸ªå›ºå®šçš„ *æ¿€æ´»* å‡½æ•°ä»£æ›¿ ğœ‘ï¼Œä¸”ä¸ *f* æ— å…³ã€‚
- en: '3.4\. Approximating continuous functions: Cybenko et al'
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4\. é€¼è¿‘è¿ç»­å‡½æ•°ï¼šCybenko ç­‰äºº
- en: '**Activation functions.** The Neural Approximation theorem below states that
    any continuous function can be approximated by linear combinations of a fixed
    *activation function Ïƒ*. All we need from this function is that it restricts to
    a homeomorphism between two closed real intervals *not representable by a polynomial*.
    The construction can be set up to only use the part that establishes this continuous,
    monotonic bijection of intervals. That part can be conveniently renormalized to
    a *sigmoid*: a homeomorphism of the extended real line and the interval [0,1].
    Early neural networks used the logistic sigmoid, which readily establishes that
    homeomorphism. The hyperbolic tangent and arcus tangent were also used, suitably
    renormalized. Nowadays the function max(0, *x*) is preferred. Its original designation
    as *â€œRectified Linear Unitâ€* got mellowed down to *ReLU*, a nickname shared with
    small pets. The Neural Approximation construction fails if the activation function
    is representable by a polynomial. This obviously precludes all linear functions
    â€” *but* already a continuous combination of two linear functions works fine, as
    ReLU shows, combining the constant 0 below 0 and the identity above 0.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ¿€æ´»å‡½æ•°ã€‚** ä»¥ä¸‹ç¥ç»é€¼è¿‘å®šç†è¡¨æ˜ï¼Œä»»ä½•è¿ç»­å‡½æ•°éƒ½å¯ä»¥é€šè¿‡å›ºå®šçš„*æ¿€æ´»å‡½æ•°Ïƒ*çš„çº¿æ€§ç»„åˆæ¥é€¼è¿‘ã€‚æˆ‘ä»¬å¯¹è¿™ä¸ªå‡½æ•°çš„è¦æ±‚æ˜¯ï¼Œå®ƒåœ¨ä¸¤ä¸ªå°é—­å®æ•°åŒºé—´ä¹‹é—´é™åˆ¶ä¸ºä¸€ä¸ªåŒèƒšå‡½æ•°ï¼Œè€Œè¿™ä¸¤ä¸ªåŒºé—´*ä¸èƒ½ç”±å¤šé¡¹å¼è¡¨ç¤º*ã€‚æ„é€ è¿‡ç¨‹å¯ä»¥åªä½¿ç”¨é‚£ä¸ªéƒ¨åˆ†ï¼Œè¯¥éƒ¨åˆ†å»ºç«‹äº†è¿™ä¸ªè¿ç»­çš„ã€å•è°ƒçš„åŒºé—´åŒå°„ã€‚è¿™ä¸ªéƒ¨åˆ†å¯ä»¥æ–¹ä¾¿åœ°é‡æ–°å½’ä¸€åŒ–ä¸ºä¸€ä¸ª*sigmoid*ï¼šä¸€ä¸ªæ‰©å±•å®æ•°çº¿ä¸åŒºé—´[0,1]çš„åŒèƒšã€‚æ—©æœŸçš„ç¥ç»ç½‘ç»œä½¿ç”¨äº†é€»è¾‘sigmoidï¼Œå®ƒå¯ä»¥å¾ˆå®¹æ˜“åœ°å»ºç«‹è¿™ä¸ªåŒèƒšã€‚åŒæ›²æ­£åˆ‡å’Œåæ­£åˆ‡ä¹Ÿæ›¾è¢«ä½¿ç”¨ï¼Œå¹¶é€‚å½“é‡æ–°å½’ä¸€åŒ–ã€‚å¦‚ä»Šï¼Œå‡½æ•°max(0,
    *x*)æ›´ä¸ºå¸¸è§ã€‚å®ƒæœ€åˆçš„å‘½åä¸º*â€œä¿®æ­£çº¿æ€§å•å…ƒâ€*ï¼Œåæ¥è¢«ç®€ç§°ä¸º*ReLU*ï¼Œè¿™ä¸ªåå­—ä¹Ÿç”¨æ¥æŒ‡ä»£å°å® ç‰©ã€‚å½“æ¿€æ´»å‡½æ•°å¯ä»¥é€šè¿‡å¤šé¡¹å¼è¡¨ç¤ºæ—¶ï¼Œç¥ç»é€¼è¿‘æ„é€ ä¼šå¤±è´¥ã€‚æ˜¾ç„¶ï¼Œè¿™æ’é™¤äº†æ‰€æœ‰çº¿æ€§å‡½æ•°â€”â€”*ä½†æ˜¯*ï¼Œä¸¤ä¸ªçº¿æ€§å‡½æ•°çš„è¿ç»­ç»„åˆå·²ç»èƒ½å¤Ÿå¾ˆå¥½åœ°å·¥ä½œï¼Œæ­£å¦‚ReLUæ‰€ç¤ºï¼Œç»“åˆäº†0ä»¥ä¸‹çš„å¸¸æ•°0å’Œ0ä»¥ä¸Šçš„æ’ç­‰å‡½æ•°ã€‚'
- en: '![](../Images/fe33d2cf10d69af5ef92d875cddf60a2.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fe33d2cf10d69af5ef92d875cddf60a2.png)'
- en: Since both *f* and ğ’°(-)**f** are continuous, the approximation claim is equivalent
    to saying that for every *Îµ > 0* there is *Î´=Î´(Îµ) >0* such that
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸º*f*å’Œğ’°(-)**f**éƒ½æ˜¯è¿ç»­çš„ï¼Œæ‰€ä»¥é€¼è¿‘çš„å£°æ˜ç­‰ä»·äºè¯´ï¼Œå¯¹äºæ¯ä¸€ä¸ª*Îµ > 0*ï¼Œå­˜åœ¨*Î´=Î´(Îµ) > 0*ï¼Œä½¿å¾—
- en: '![](../Images/2bf637223a5e666a3694d6b287a0d272.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2bf637223a5e666a3694d6b287a0d272.png)'
- en: Neurons with activation functions are thus *universal approximators* of continuous
    functions, in the sense that for every continuous *f* on a cube there is a neuron
    **f**=*(W*,âŸ¨*v*|) such that *f*|*x*âŸ©â‰ˆ ğ’°| *x* âŸ©**f**, with arbitrary precision.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¦æœ‰æ¿€æ´»å‡½æ•°çš„ç¥ç»å…ƒå› æ­¤æ˜¯è¿ç»­å‡½æ•°çš„*é€šç”¨é€¼è¿‘å™¨*ï¼Œæ„å‘³ç€å¯¹äºæ¯ä¸€ä¸ªå®šä¹‰åœ¨ç«‹æ–¹ä½“ä¸Šçš„è¿ç»­*f*ï¼Œå­˜åœ¨ä¸€ä¸ªç¥ç»å…ƒ**f**=*(W*,âŸ¨*v*|)ï¼Œä½¿å¾—*f*|*x*âŸ©â‰ˆ
    ğ’°| *x* âŸ©**f**ï¼Œä¸”å…·æœ‰ä»»æ„ç²¾åº¦ã€‚
- en: The proofs of different versions of the Neural Approximation Theorem were published
    by Cybenko and by Harnik-Stinchcombe-White independently, both in 1989\. In the
    meantime, the neural approximations have been widely used, and various other versions,
    views, and overviews have been provided. The overarching insight links the CHSW-approximation
    and the KA-decomposition in a computational framework that seems to have taken
    both beyond the original motivations.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»é€¼è¿‘å®šç†çš„ä¸åŒç‰ˆæœ¬çš„è¯æ˜åˆ†åˆ«ç”±Cybenkoä»¥åŠHarnik-Stinchcombe-Whiteåœ¨1989å¹´ç‹¬ç«‹å‘å¸ƒã€‚ä¸æ­¤åŒæ—¶ï¼Œç¥ç»é€¼è¿‘æ–¹æ³•è¢«å¹¿æ³›åº”ç”¨ï¼Œå¹¶ä¸”æä¾›äº†å„ç§å…¶ä»–ç‰ˆæœ¬ã€è§†è§’å’Œæ¦‚è¿°ã€‚æ€»ä½“çš„è§è§£å°†CHSW-é€¼è¿‘å’ŒKA-åˆ†è§£è”ç³»åœ¨ä¸€ä¸ªè®¡ç®—æ¡†æ¶ä¸­ï¼Œè¿™ä¸ªæ¡†æ¶ä¼¼ä¹å·²ç»å°†äºŒè€…è¶…è¶Šäº†æœ€åˆçš„åŠ¨æœºã€‚
- en: '**Continuous functions can be approximated because their variables can be separated.**
    In computational terms, this means that continuous functions can be partially
    evaluated. That makes them learnable. Unfolding the CHSW-approximation in parallel
    with the KA-decomposition displays the common pattern:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¿ç»­å‡½æ•°å¯ä»¥è¢«é€¼è¿‘ï¼Œå› ä¸ºå®ƒä»¬çš„å˜é‡å¯ä»¥åˆ†ç¦»ã€‚** ä»è®¡ç®—çš„è§’åº¦æ¥çœ‹ï¼Œè¿™æ„å‘³ç€è¿ç»­å‡½æ•°å¯ä»¥è¢«éƒ¨åˆ†æ±‚å€¼ã€‚è¿™ä½¿å¾—å®ƒä»¬å¯ä»¥è¢«å­¦ä¹ ã€‚å°†CHSW-é€¼è¿‘ä¸KA-åˆ†è§£å¹¶è¡Œå±•å¼€ï¼Œå±•ç¤ºäº†å…±åŒçš„æ¨¡å¼ï¼š'
- en: '![](../Images/8363593ee2f3a1ceade56f979c144bbc.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8363593ee2f3a1ceade56f979c144bbc.png)'
- en: The corresponding diagrams after the statements show the analogy yet again.
    But note the differences. The first difference is that *w* and *v* on the left
    depend on *f* , whereas on the right only ğœ‘ depends on it. The second difference
    is that the number of separate variables that allow partial evaluation, for a
    fixed input *d*, is fixed at (2*d* + 1) in the case of decomposition on the right,
    whereas in the case of approximation on the left, *n* = *n*(Îµ) depends on the
    approximation error Îµ. This is an important point.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: è¯­å¥åçš„ç›¸åº”å›¾ç¤ºå†æ¬¡æ˜¾ç¤ºäº†ç±»æ¯”ï¼Œä½†è¯·æ³¨æ„å…¶ä¸­çš„å·®å¼‚ã€‚ç¬¬ä¸€ä¸ªå·®å¼‚æ˜¯å·¦ä¾§çš„ *w* å’Œ *v* ä¾èµ–äº *f*ï¼Œè€Œå³ä¾§åªæœ‰ğœ‘ä¾èµ–äº *f*ã€‚ç¬¬äºŒä¸ªå·®å¼‚æ˜¯ï¼Œå¯¹äºå›ºå®šè¾“å…¥
    *d*ï¼Œå³ä¾§åˆ†è§£çš„æƒ…å†µä¸‹ï¼Œå…è®¸éƒ¨åˆ†è¯„ä¼°çš„ç‹¬ç«‹å˜é‡æ•°æ˜¯å›ºå®šçš„ (2*d* + 1)ï¼Œè€Œå·¦ä¾§è¿‘ä¼¼çš„æƒ…å†µä¸‹ï¼Œ*n* = *n*(Îµ) ä¾èµ–äºè¿‘ä¼¼è¯¯å·®Îµã€‚è¿™æ˜¯ä¸€ä¸ªé‡è¦çš„ç‚¹ã€‚
- en: The dimension *n* ofthe space on which a given function is approximable by a
    Ïƒ-neuron up to a required precision is the ***width*** *of the neuron*. The Neural
    Approximation Theorem says that for any continuous function, there is a wide enough
    neuron that will approximate it up to any required precision. This is the essence
    of **wide learning**. The idea of approximating continuous functions by a linear
    combination of copies of Ïƒ is similar to Lebesgueâ€™s idea to approximate an integrable
    function by a linear combination of step functions. In both cases, closer approximations
    are achieved by larger numbers *n* of approximants.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªç»™å®šå‡½æ•°å¯ä»¥é€šè¿‡Ïƒç¥ç»å…ƒé€¼è¿‘åˆ°æ‰€éœ€ç²¾åº¦çš„ç©ºé—´ç»´åº¦ *n* è¢«ç§°ä¸ºç¥ç»å…ƒçš„***å®½åº¦***ã€‚ç¥ç»è¿‘ä¼¼å®šç†è¡¨æ˜ï¼Œå¯¹äºä»»ä½•è¿ç»­å‡½æ•°ï¼Œéƒ½å­˜åœ¨ä¸€ä¸ªè¶³å¤Ÿå®½çš„ç¥ç»å…ƒï¼Œå¯ä»¥å°†å…¶é€¼è¿‘åˆ°ä»»æ„æ‰€éœ€ç²¾åº¦ã€‚è¿™å°±æ˜¯**å®½å­¦ä¹ **çš„æœ¬è´¨ã€‚é€šè¿‡çº¿æ€§ç»„åˆÏƒçš„å‰¯æœ¬æ¥é€¼è¿‘è¿ç»­å‡½æ•°çš„æ€æƒ³ï¼Œç±»ä¼¼äºå‹’è´æ ¼ï¼ˆLebesgueï¼‰å°†å¯ç§¯å‡½æ•°é€šè¿‡é˜¶æ¢¯å‡½æ•°çš„çº¿æ€§ç»„åˆæ¥é€¼è¿‘çš„æ€æƒ³ã€‚åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œé€šè¿‡å¢åŠ è¿‘ä¼¼è€…
    *n* çš„æ•°é‡ï¼Œå¯ä»¥å®ç°æ›´ç²¾ç¡®çš„é€¼è¿‘ã€‚
- en: '**Wide neural networks.** Everything stated for continuous real functions lifts
    without much ado to continuous *vector* functions. For finite dimensions, they
    are just tuples of continuous real functions. The approximations by Ïƒ-neurons
    lift to tuples of Ïƒ-neurons, a.k.a. the *single-layer neural networks*. The tupling
    step is the step from left to right in the following diagram.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '**å®½ç¥ç»ç½‘ç»œã€‚** å¯¹äºè¿ç»­å®å‡½æ•°çš„æ‰€æœ‰é™ˆè¿°åœ¨å¾ˆå°‘çš„å˜åŒ–ä¸‹é€‚ç”¨äºè¿ç»­*å‘é‡*å‡½æ•°ã€‚å¯¹äºæœ‰é™ç»´åº¦ï¼Œå®ƒä»¬åªæ˜¯è¿ç»­å®å‡½æ•°çš„å…ƒç»„ã€‚é€šè¿‡Ïƒç¥ç»å…ƒçš„é€¼è¿‘å¯ä»¥æ‰©å±•åˆ°Ïƒç¥ç»å…ƒçš„å…ƒç»„ï¼Œä¹Ÿå°±æ˜¯*å•å±‚ç¥ç»ç½‘ç»œ*ã€‚å…ƒç»„åŒ–æ­¥éª¤æ˜¯ä»¥ä¸‹å›¾ç¤ºä¸­ä»å·¦åˆ°å³çš„æ­¥éª¤ã€‚'
- en: '![](../Images/b97e862590919d34d3503b7ef4291b10.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b97e862590919d34d3503b7ef4291b10.png)'
- en: A *q*-tuple of neurons (*W*, âŸ¨*v1*|), (*W*, âŸ¨*v2|),â€¦,(W, âŸ¨vq*|), bundled together,
    gives a single-layer neural network a = (*W*,âŸ¨*v1 |,âŸ¨v2* |,â€¦,âŸ¨*vq* |), more succinctly
    written a = (*W*,*V*), where *V* is the matrix with the âŸ¨*vj*| vectors as rows,
    like before. The Neural Approximation Theorem implies that every continuous vector
    function can be approximated with arbitrary precision by a sufficiantly wide single-layer
    neural network. The term *wide neural network* usually refers to a single-layer
    network. The circuit view in the top row of the last figure is aligned with the
    more abstract view the middle row, with layers of variables enclosed in boxes.
    This will come handy when the networks become deep.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª *q* å…ƒç»„çš„ç¥ç»å…ƒï¼ˆ*W*ï¼ŒâŸ¨*v1*|ï¼‰ï¼Œï¼ˆ*W*ï¼ŒâŸ¨*v2*|ï¼‰ï¼Œ...ï¼Œï¼ˆ*W*ï¼ŒâŸ¨vq*|ï¼‰ç»„åˆåœ¨ä¸€èµ·ï¼Œç»™å‡ºäº†ä¸€ä¸ªå•å±‚ç¥ç»ç½‘ç»œ a =
    (*W*ï¼ŒâŸ¨*v1*|ï¼ŒâŸ¨v2*|ï¼Œ...ï¼ŒâŸ¨*vq*|ï¼‰ï¼Œæ›´ç®€æ´åœ°å†™ä½œ a = (*W*ï¼Œ*V*)ï¼Œå…¶ä¸­ *V* æ˜¯åŒ…å« âŸ¨*vj*| å‘é‡ä½œä¸ºè¡Œçš„çŸ©é˜µï¼Œå’Œä¹‹å‰ä¸€æ ·ã€‚ç¥ç»è¿‘ä¼¼å®šç†æ„å‘³ç€æ¯ä¸ªè¿ç»­å‘é‡å‡½æ•°éƒ½å¯ä»¥é€šè¿‡ä¸€ä¸ªè¶³å¤Ÿå®½çš„å•å±‚ç¥ç»ç½‘ç»œä»¥ä»»æ„ç²¾åº¦è¿›è¡Œé€¼è¿‘ã€‚æœ¯è¯­
    *å®½ç¥ç»ç½‘ç»œ* é€šå¸¸æŒ‡çš„æ˜¯å•å±‚ç½‘ç»œã€‚æœ€åä¸€å¼ å›¾çš„é¡¶éƒ¨è¡Œçš„ç”µè·¯è§†å›¾ä¸ä¸­é—´è¡Œçš„æ›´æŠ½è±¡è§†å›¾å¯¹é½ï¼Œå…¶ä¸­åŒ…å«äº†ç”¨æ¡†å›´èµ·æ¥çš„å˜é‡å±‚ã€‚éšç€ç½‘ç»œå˜å¾—æ›´æ·±ï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚
- en: 3.4\. Deep learning
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4. æ·±åº¦å­¦ä¹ 
- en: '**Scaling up.** The trouble with wide learning is that there are simple functions
    where separating variables is hard, and the number *n* of variables that need
    to be separated increases exponentially with the dimension *d*. E.g., separating
    variables in function presenting a hemisphere in any dimension is hard. Although
    any continuous real function on a cube is approximable by a wide Ïƒ-neuron, and
    any continuous vector function by a single-layer network, the approximations are
    in the worst case intractable. The amounts of training data needed to extrapolate
    predictions also explode exponentially with the width[â¸](#f923).'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ‰©å±•è§„æ¨¡ã€‚** å®½å­¦ä¹ çš„é—®é¢˜åœ¨äºï¼Œæœ‰äº›ç®€å•å‡½æ•°å¾ˆéš¾åˆ†ç¦»å˜é‡ï¼Œå¹¶ä¸”éœ€è¦åˆ†ç¦»çš„å˜é‡æ•° *n* éšç€ç»´åº¦ *d* çš„å¢åŠ å‘ˆæŒ‡æ•°å¢é•¿ã€‚ä¾‹å¦‚ï¼Œåœ¨ä»»ä½•ç»´åº¦ä¸­ï¼Œåˆ†ç¦»è¡¨ç¤ºåŠçƒçš„å‡½æ•°çš„å˜é‡æ˜¯å›°éš¾çš„ã€‚å°½ç®¡ä»»ä½•ç«‹æ–¹ä½“ä¸Šçš„è¿ç»­å®å‡½æ•°éƒ½å¯ä»¥é€šè¿‡å®½Ïƒç¥ç»å…ƒè¿›è¡Œé€¼è¿‘ï¼Œå¹¶ä¸”ä»»ä½•è¿ç»­å‘é‡å‡½æ•°éƒ½å¯ä»¥é€šè¿‡å•å±‚ç½‘ç»œé€¼è¿‘ï¼Œä½†åœ¨æœ€åæƒ…å†µä¸‹ï¼Œé€¼è¿‘æ˜¯æ— æ³•å¤„ç†çš„ã€‚ç”¨äºå¤–æ¨é¢„æµ‹çš„è®­ç»ƒæ•°æ®é‡ä¹Ÿä¼šéšç€å®½åº¦çš„å¢åŠ å‘ˆæŒ‡æ•°çº§çˆ†ç‚¸æ€§å¢é•¿[â¸](#f923)ã€‚'
- en: '**Narrowing by deepening.** The general idea of approximating a function *f*
    is to find an algorithm to transform the data'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '**é€šè¿‡åŠ æ·±æ¥ç¼©å°èŒƒå›´ã€‚** é€¼è¿‘ä¸€ä¸ªå‡½æ•° *f* çš„ä¸€èˆ¬æ€è·¯æ˜¯æ‰¾åˆ°ä¸€ç§ç®—æ³•æ¥è½¬æ¢æ•°æ®'
- en: '![](../Images/b7fafff38d31a32764e8d668ed255cf3.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7fafff38d31a32764e8d668ed255cf3.png)'
- en: and determine an approximatorwith
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ç¡®å®šä¸€ä¸ªè¿‘ä¼¼å™¨
- en: '![](../Images/e1a557e2c6252919270af0e31bbb4210.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1a557e2c6252919270af0e31bbb4210.png)'
- en: 'for a desired precision Îµ. The exponential growth of the width *n* of single-layer
    neural networks is thus tempered by descending through layers of deep neural networks,
    which look something like this:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæœŸæœ›çš„ç²¾åº¦ Îµã€‚å•å±‚ç¥ç»ç½‘ç»œå®½åº¦ *n* çš„æŒ‡æ•°å¢é•¿å› æ­¤è¢«é€šè¿‡æ·±åº¦ç¥ç»ç½‘ç»œçš„å±‚çº§é€æ­¥ä¸‹é™æ‰€ç¼“è§£ï¼Œç»“æ„å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](../Images/389c874787ee044fcb0f146c26f66016.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/389c874787ee044fcb0f146c26f66016.png)'
- en: At each inner layer, composing the input transformation *W* with the output
    transformation *V* of the preceding layer gives a composite *H.*
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¯ä¸€å±‚å†…éƒ¨ï¼Œå°†è¾“å…¥å˜æ¢ *W* ä¸å‰ä¸€å±‚è¾“å‡ºå˜æ¢ *V* ç»„åˆèµ·æ¥å½¢æˆå¤åˆ *H*ã€‚
- en: '![](../Images/200df926bd76a73ab51b20a0df936c06.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/200df926bd76a73ab51b20a0df936c06.png)'
- en: As a composite of linear operators, *H* is linear itself, and can be trained
    directly, forgetting about the *W*s and the *V*s. Deep neural networks are thus
    programs in the form **a**=(*W*, *H*1, *H*2, . . . , *HL*, *V*).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºçº¿æ€§ç®—å­çš„å¤åˆï¼Œ*H* æœ¬èº«æ˜¯çº¿æ€§çš„ï¼Œå¯ä»¥ç›´æ¥è¿›è¡Œè®­ç»ƒï¼Œå¿½ç•¥ *W* å’Œ *V*ã€‚å› æ­¤ï¼Œæ·±åº¦ç¥ç»ç½‘ç»œæ˜¯å½¢å¦‚ **a**=(*W*, *H*1, *H*2,
    . . . , *HL*, *V*) çš„ç¨‹åºã€‚
- en: '**Neural networks are learnable programs.** The [general learning process](#5da6)
    can be viewed as the process of program development. To learn a function *F* means
    to converge to a program **a** whose executions ğ’°*(x)***a** approximate *F(x)*.
    Learners are programmers. It is true that the goal of programming is not just
    to approximate a function, but to precisely implement it. Ideally, a program **a**
    for a function *F* should satisfy ğ’°(*x*)**a=***F(x)*. In reality, a program implements
    a function only up to a correctness gauge â„’(ğ’°(*x*)**a,***F(x))*, realized through
    program testing or software assurance methodologies. Programming can be viewed
    as a special case of learning.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¥ç»ç½‘ç»œæ˜¯å¯å­¦ä¹ çš„ç¨‹åºã€‚** [é€šç”¨å­¦ä¹ è¿‡ç¨‹](#5da6) å¯ä»¥è§†ä¸ºç¨‹åºå¼€å‘çš„è¿‡ç¨‹ã€‚å­¦ä¹ ä¸€ä¸ªå‡½æ•° *F* æ„å‘³ç€æ”¶æ•›åˆ°ä¸€ä¸ªç¨‹åº **a**ï¼Œå…¶æ‰§è¡Œ
    ğ’°*(x)***a** è¿‘ä¼¼ *F(x)*ã€‚å­¦ä¹ è€…æ˜¯ç¨‹åºå‘˜ã€‚ç¡®å®ï¼Œç¼–ç¨‹çš„ç›®æ ‡ä¸ä»…ä»…æ˜¯è¿‘ä¼¼ä¸€ä¸ªå‡½æ•°ï¼Œè€Œæ˜¯ç²¾ç¡®åœ°å®ç°å®ƒã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œå‡½æ•° *F* çš„ç¨‹åº **a**
    åº”è¯¥æ»¡è¶³ ğ’°(*x*)**a=***F(x)*ã€‚åœ¨ç°å®ä¸­ï¼Œä¸€ä¸ªç¨‹åºå®ç°ä¸€ä¸ªå‡½æ•°ï¼Œåªèƒ½åšåˆ°ä¸€å®šçš„æ­£ç¡®æ€§æ ‡å‡† â„’(ğ’°(*x*)**a,***F(x))*ï¼Œé€šè¿‡ç¨‹åºæµ‹è¯•æˆ–è½¯ä»¶ä¿è¯æ–¹æ³•æ¥å®ç°ã€‚ç¼–ç¨‹å¯ä»¥è§†ä¸ºå­¦ä¹ çš„ä¸€ä¸ªç‰¹ä¾‹ã€‚'
- en: We have already seen many of the main features of the syntax of neural networks
    as a programming language. A single neuron a = âŸ¨*w* | is an atomic program expression.
    A single-layer network a = (*W*,*V*) is a single-instruction program. A deep network
    a = (*W*, *H*1, *H*2, . . . , *HL*, *V*) is a general program. Its inner layers
    are the program instructions. For simplicity, the inner layers are often bundled
    under a common name, say **h** = (*H*1,*H*2,â€¦,*HL*). A general neural program
    is in the form **a** = (*W*,**h**,*V*).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»çœ‹åˆ°äº†ç¥ç»ç½‘ç»œä½œä¸ºç¼–ç¨‹è¯­è¨€è¯­æ³•çš„è®¸å¤šä¸»è¦ç‰¹å¾ã€‚å•ä¸ªç¥ç»å…ƒ a = âŸ¨*w* | æ˜¯ä¸€ä¸ªåŸå­ç¨‹åºè¡¨è¾¾å¼ã€‚å•å±‚ç½‘ç»œ a = (*W*,*V*) æ˜¯ä¸€ä¸ªå•æŒ‡ä»¤ç¨‹åºã€‚æ·±åº¦ç½‘ç»œ
    a = (*W*, *H*1, *H*2, . . . , *HL*, *V*) æ˜¯ä¸€ä¸ªé€šç”¨ç¨‹åºã€‚å…¶å†…éƒ¨å±‚æ¬¡å°±æ˜¯ç¨‹åºæŒ‡ä»¤ã€‚ä¸ºäº†ç®€ä¾¿èµ·è§ï¼Œå†…éƒ¨å±‚æ¬¡é€šå¸¸ä¼šè¢«å½’ä¸ºä¸€ä¸ªå…¬å…±åç§°ï¼Œä¾‹å¦‚
    **h** = (*H*1,*H*2,â€¦,*HL*)ã€‚ä¸€ä¸ªé€šç”¨ç¥ç»ç¨‹åºçš„å½¢å¼æ˜¯ **a** = (*W*,**h**,*V*)ã€‚
- en: '![](../Images/fc59c0af992bb2b1ae0724cce727bb24.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc59c0af992bb2b1ae0724cce727bb24.png)'
- en: 4\. Learning channels and paying attention
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. å­¦ä¹ é€šé“å’Œå…³æ³¨
- en: '**The trouble with applying function learning to language** is that language
    is context-sensitive: the word â€œupâ€, for instance, means one thing in â€œshut upâ€
    and another thing in â€œcheer upâ€. We talked about this in the [*Beyond sintax*](/syntax-the-language-form-612257c4aa5f#8904)section
    of the [*Syntax* part](/syntax-the-language-form-612257c4aa5f). A function is
    required to assign to each input the same unique output in all contexts. Meaning
    is not a function but a communication channel, assigning to each context a probability
    distribution over the concepts *y*:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**å°†å‡½æ•°å­¦ä¹ åº”ç”¨äºè¯­è¨€çš„éš¾é¢˜** åœ¨äºè¯­è¨€æ˜¯ä¸Šä¸‹æ–‡æ•æ„Ÿçš„ï¼šä¾‹å¦‚ï¼Œâ€œupâ€ä¸€è¯åœ¨â€œshut upâ€ä¸­æœ‰ä¸€ç§å«ä¹‰ï¼Œåœ¨â€œcheer upâ€ä¸­åˆ™æœ‰å¦ä¸€ç§å«ä¹‰ã€‚æˆ‘ä»¬åœ¨
    [*è¶…è¶Šè¯­æ³•*](/syntax-the-language-form-612257c4aa5f#8904)éƒ¨åˆ†çš„ [*è¯­æ³•*](/syntax-the-language-form-612257c4aa5f)
    ä¸­è®¨è®ºè¿‡è¿™ä¸€ç‚¹ã€‚ä¸€ä¸ªå‡½æ•°è¦æ±‚åœ¨æ‰€æœ‰ä¸Šä¸‹æ–‡ä¸­ä¸ºæ¯ä¸ªè¾“å…¥åˆ†é…ç›¸åŒçš„å”¯ä¸€è¾“å‡ºã€‚æ„ä¹‰ä¸æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œè€Œæ˜¯ä¸€ä¸ªé€šä¿¡é€šé“ï¼Œå®ƒä¸ºæ¯ä¸ªä¸Šä¸‹æ–‡åˆ†é…ä¸€ä¸ªå…³äºæ¦‚å¿µ *y* çš„æ¦‚ç‡åˆ†å¸ƒï¼š'
- en: '![](../Images/80c8793cecf6d6778cf83c2d31d8b07c.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80c8793cecf6d6778cf83c2d31d8b07c.png)'
- en: In the [*Semantics* part](/syntax-the-language-form-612257c4aa5f#4389), we saw
    how concepts are modeled as vectors, usually linear combinations of words. Meaning
    is thus a random variable *Y*, sampled over the concept vectors *y*. There is
    an overview of the channel formalism in the [*Dynamic semantics*](/syntax-the-language-form-612257c4aa5f#912e)section
    of the *Semantics* part. When there is no channel feedback, the context is the
    channel source
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[*è¯­ä¹‰å­¦*éƒ¨åˆ†](/syntax-the-language-form-612257c4aa5f#4389)ï¼Œæˆ‘ä»¬çœ‹åˆ°æ¦‚å¿µæ˜¯å¦‚ä½•è¢«å»ºæ¨¡ä¸ºå‘é‡çš„ï¼Œé€šå¸¸æ˜¯å•è¯çš„çº¿æ€§ç»„åˆã€‚æ„ä¹‰å› æ­¤æ˜¯ä¸€ä¸ªéšæœºå˜é‡
    *Y*ï¼Œåœ¨æ¦‚å¿µå‘é‡ *y* ä¸Šè¿›è¡Œé‡‡æ ·ã€‚åœ¨[*åŠ¨æ€è¯­ä¹‰å­¦*](/syntax-the-language-form-612257c4aa5f#912e)éƒ¨åˆ†çš„*è¯­ä¹‰å­¦*ç« èŠ‚ä¸­æœ‰å…³äºé€šé“å½¢å¼ä¸»ä¹‰çš„æ¦‚è¿°ã€‚å½“æ²¡æœ‰é€šé“åé¦ˆæ—¶ï¼Œä¸Šä¸‹æ–‡å°±æ˜¯é€šé“æºã€‚
- en: '![](../Images/80787715e7edc3201d2471b68b420146.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80787715e7edc3201d2471b68b420146.png)'
- en: and the channel outputs are sampled according to the probabilities
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: é€šé“è¾“å‡ºæ ¹æ®æ¦‚ç‡è¿›è¡Œé‡‡æ ·
- en: '![](../Images/0cfd8750c1bda6afbcc2c9598e24e08b.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0cfd8750c1bda6afbcc2c9598e24e08b.png)'
- en: 'You can think of the source *X* as a text and of the channel *F* as the process
    of translating the text to a text *Y* in another language:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥å°†æºæ–‡æœ¬ *X* çœ‹ä½œæ˜¯æ–‡æœ¬ï¼Œå°†é€šé“ *F* çœ‹ä½œæ˜¯å°†æ–‡æœ¬ç¿»è¯‘æˆå¦ä¸€ç§è¯­è¨€çš„æ–‡æœ¬ *Y* çš„è¿‡ç¨‹ï¼š
- en: '![](../Images/a6a7677c1a3737a1299f59276384b2e4.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a6a7677c1a3737a1299f59276384b2e4.png)'
- en: Similar interpretations subsume meaning, syntactic typing, classification, and
    generation under the channel model. The common denominator is the context dependency,
    be it syntactic or semantic, deterministic or stochastic. Semantic references
    can be remote. The meaning of a sentence in a novel may depend on a context from
    800 pages earlier. The meaning that you assign to something that an old friend
    says may be based on a model of their personality from years ago. To make it more
    complicated, remote references and long established channel models may change
    from context to context, whenever new information becomes available.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼çš„è§£é‡Šå°†æ„ä¹‰ã€å¥æ³•ç±»å‹ã€åˆ†ç±»å’Œç”Ÿæˆå½’çº³åˆ°é€šé“æ¨¡å‹ä¸‹ã€‚å…±åŒç‚¹æ˜¯ä¸Šä¸‹æ–‡ä¾èµ–æ€§ï¼Œæ— è®ºæ˜¯å¥æ³•çš„è¿˜æ˜¯è¯­ä¹‰çš„ï¼Œæ˜¯ç¡®å®šæ€§çš„è¿˜æ˜¯éšæœºçš„ã€‚è¯­ä¹‰å¼•ç”¨å¯èƒ½æ˜¯è¿œç¨‹çš„ã€‚ä¸€å¥å°è¯´ä¸­çš„è¯çš„æ„ä¹‰å¯èƒ½ä¾èµ–äº800é¡µå‰çš„ä¸Šä¸‹æ–‡ã€‚ä½ èµ‹äºˆæŸä¸ªè€æœ‹å‹è¯´çš„è¯çš„æ„ä¹‰ï¼Œå¯èƒ½åŸºäºå¤šå¹´å‰çš„ä»–ä»¬çš„ä¸ªæ€§æ¨¡å‹ã€‚æ›´å¤æ‚çš„æ˜¯ï¼Œè¿œç¨‹å¼•ç”¨å’Œé•¿æœŸå»ºç«‹çš„é€šé“æ¨¡å‹å¯èƒ½ä¼šéšç€æ–°ä¿¡æ¯çš„åˆ°æ¥è€Œåœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­å‘ç”Ÿå˜åŒ–ã€‚
- en: 4.1 Channeling through concepts
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 é€šè¿‡æ¦‚å¿µè¿›è¡Œé€šé“åŒ–
- en: 'In different languages, semantical references are mapped to syntactic references
    in different ways. Mapping a Mandarin phrase to a French phrase requires deviating
    from the [syntactic dependency mechanisms](/syntax-the-language-form-612257c4aa5f#d37c)
    of the two languages. Good translators first understand the phrase in one language
    and then express what they understood in the other language. It is a two-stage
    process:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸åŒçš„è¯­è¨€ä¸­ï¼Œè¯­ä¹‰å¼•ç”¨ä»¥ä¸åŒçš„æ–¹å¼æ˜ å°„åˆ°å¥æ³•å¼•ç”¨ã€‚å°†ä¸­æ–‡çŸ­è¯­æ˜ å°„åˆ°æ³•è¯­çŸ­è¯­éœ€è¦åç¦»ä¸¤ç§è¯­è¨€çš„[å¥æ³•ä¾èµ–æœºåˆ¶](/syntax-the-language-form-612257c4aa5f#d37c)ã€‚ä¼˜ç§€çš„ç¿»è¯‘è€…é¦–å…ˆç†è§£ä¸€ç§è¯­è¨€ä¸­çš„çŸ­è¯­ï¼Œç„¶åç”¨å¦ä¸€ç§è¯­è¨€è¡¨è¾¾ä»–ä»¬ç†è§£çš„å†…å®¹ã€‚è¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„è¿‡ç¨‹ï¼š
- en: '![](../Images/84951a402995cbfd179617dddad2ed5b.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84951a402995cbfd179617dddad2ed5b.png)'
- en: '*E* is a *concept encoding* map, whereas *D* is *concept decoding*. Similar
    pattern came up in the [*Semantics* part](https://medium.com/@dusko_p/semantics-the-meaning-of-language-99b009ccef41#aeca),
    as instances of concept mining through *Singular Value Decomposition (SVD).*'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '*E* æ˜¯ *æ¦‚å¿µç¼–ç * æ˜ å°„ï¼Œè€Œ *D* æ˜¯ *æ¦‚å¿µè§£ç *ã€‚ç±»ä¼¼çš„æ¨¡å¼å‡ºç°åœ¨[*è¯­ä¹‰å­¦*éƒ¨åˆ†](https://medium.com/@dusko_p/semantics-the-meaning-of-language-99b009ccef41#aeca)ï¼Œä½œä¸ºé€šè¿‡
    *å¥‡å¼‚å€¼åˆ†è§£ (SVD)* è¿›è¡Œæ¦‚å¿µæŒ–æ˜çš„å®ä¾‹ã€‚'
- en: '![](../Images/a17b04e6bf6b57527b95680b6cc989b3.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a17b04e6bf6b57527b95680b6cc989b3.png)'
- en: 'The concepts that are latent in a given data matrix M are mined as its singular
    values *Î»i*. Now compare the diagrams of [ğœ-neurons](#ef75) and [single-layer
    networks](#e52e) with the corresponding diagram of the SVD:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šæ•°æ®çŸ©é˜µ M ä¸­æ½œåœ¨çš„æ¦‚å¿µè¢«æŒ–æ˜ä¸ºå…¶å¥‡å¼‚å€¼ *Î»i*ã€‚ç°åœ¨æ¯”è¾ƒ[ğœ-ç¥ç»å…ƒ](#ef75)å’Œ[å•å±‚ç½‘ç»œ](#e52e)çš„å›¾ç¤ºä¸ SVD çš„ç›¸åº”å›¾ç¤ºï¼š
- en: '![](../Images/c578eb08bf8e5d7e07cc1eb53e5b57d5.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c578eb08bf8e5d7e07cc1eb53e5b57d5.png)'
- en: 'A neural network approximates a continuous function by separating its variables
    and approximating the impact of each of them by a separate copy of the activation
    function ğœ. The SVD algorithm decomposes a data matrix through a canonical basis
    eigenspaces, corresponding to the singular values of the matrix, viewed as the
    dominant concepts, spanning the concept space. The eigenspaces in the SVD are
    mutually orthogonal. The action of the data matrix boils down to multiplying each
    of them separately by the corresponding singular value. Both the neural network
    approximation and the SVD mine the latent concepts as the minimally correlated
    subspaces, preferrably orthogonal at each other. The diagrams display the same
    three-step pattern:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªç¥ç»ç½‘ç»œé€šè¿‡åˆ†ç¦»å…¶å˜é‡ï¼Œå¹¶é€šè¿‡æ¿€æ´»å‡½æ•°ğœçš„å•ç‹¬å‰¯æœ¬æ¥é€¼è¿‘æ¯ä¸ªå˜é‡çš„å½±å“ï¼Œä»è€Œé€¼è¿‘ä¸€ä¸ªè¿ç»­å‡½æ•°ã€‚SVDç®—æ³•é€šè¿‡ä¸€ä¸ªè§„èŒƒåŸºç‰¹å¾ç©ºé—´å°†æ•°æ®çŸ©é˜µåˆ†è§£ï¼Œå¯¹åº”äºçŸ©é˜µçš„å¥‡å¼‚å€¼ï¼Œè§†ä¸ºä¸»å¯¼æ¦‚å¿µï¼Œæ‰©å±•æ¦‚å¿µç©ºé—´ã€‚SVDä¸­çš„ç‰¹å¾ç©ºé—´æ˜¯ç›¸äº’æ­£äº¤çš„ã€‚æ•°æ®çŸ©é˜µçš„ä½œç”¨å½’ç»“ä¸ºåˆ†åˆ«ç”¨ç›¸åº”çš„å¥‡å¼‚å€¼ä¹˜ä»¥å®ƒä»¬ã€‚ç¥ç»ç½‘ç»œé€¼è¿‘å’ŒSVDéƒ½æŒ–æ˜æ½œåœ¨æ¦‚å¿µï¼Œä½œä¸ºæœ€å°ç›¸å…³çš„å­ç©ºé—´ï¼Œä¼˜é€‰åœ°åœ¨æ¯ä¸ªå­ç©ºé—´ä¹‹é—´æ­£äº¤ã€‚è¿™äº›å›¾è¡¨å±•ç¤ºäº†ç›¸åŒçš„ä¸‰æ­¥æ¨¡å¼ï¼š
- en: '![](../Images/b6338e436c878672cb2be82a18fc8ecc.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b6338e436c878672cb2be82a18fc8ecc.png)'
- en: '**encoding** of inputs in terms of concepts,'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è¾“å…¥çš„æ¦‚å¿µç¼–ç **ï¼Œ'
- en: '**separate processing** of each concept,'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å„ä¸ªæ¦‚å¿µçš„ç‹¬ç«‹å¤„ç†**ï¼Œ'
- en: '**decoding** of the concepts into the output terms.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è§£ç **æ¦‚å¿µåˆ°è¾“å‡ºæœ¯è¯­çš„è¿‡ç¨‹ã€‚'
- en: 'The three steps serve different purposes in different ways:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸‰æ­¥ä»¥ä¸åŒçš„æ–¹å¼æœåŠ¡äºä¸åŒçš„ç›®çš„ï¼š
- en: '![](../Images/ee3005c689e6e1335f5a54171b99090a.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee3005c689e6e1335f5a54171b99090a.png)'
- en: But difference (1) causes differences (2â€“3). When the function *F* happens to
    be linear and difference (1) disappears, the neural network converges to the SVD
    and differences (2â€“3) also disappear. Neural networks also mine latent concepts,
    like the SVD. They just learn them from arbitrary continuous functions.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å·®å¼‚(1)ä¼šå¯¼è‡´å·®å¼‚(2-3)ã€‚å½“å‡½æ•°*F*æ°å¥½æ˜¯çº¿æ€§å¹¶ä¸”å·®å¼‚(1)æ¶ˆå¤±æ—¶ï¼Œç¥ç»ç½‘ç»œä¼šæ”¶æ•›åˆ°SVDï¼Œå¹¶ä¸”å·®å¼‚(2-3)ä¹Ÿä¼šæ¶ˆå¤±ã€‚ç¥ç»ç½‘ç»œä¹ŸæŒ–æ˜æ½œåœ¨æ¦‚å¿µï¼Œå°±åƒSVDä¸€æ ·ã€‚å®ƒä»¬åªæ˜¯ä»ä»»æ„è¿ç»­å‡½æ•°ä¸­å­¦ä¹ è¿™äº›æ¦‚å¿µã€‚
- en: 4.2 Static channel learning
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 é™æ€é€šé“å­¦ä¹ 
- en: A network of neural networks is static if it processes its inputs by applying
    the same neural network **h** on all channel inputs.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªç¥ç»ç½‘ç»œæ˜¯é™æ€çš„ï¼Œå¦‚æœå®ƒé€šè¿‡åœ¨æ‰€æœ‰é€šé“è¾“å…¥ä¸Šåº”ç”¨ç›¸åŒçš„ç¥ç»ç½‘ç»œ**h**æ¥å¤„ç†å…¶è¾“å…¥ã€‚
- en: '***n*-grams of concepts.** As a warmup, suppose that we want to make a static
    network of networks slightly context-sensitive by taking into account at the *j*-th
    step not only *Xj* but also *Xj*âˆ’1, for all *j* â‰¥ 2.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '***n*-gramsæ¦‚å¿µã€‚** ä½œä¸ºçƒ­èº«ï¼Œå‡è®¾æˆ‘ä»¬æƒ³é€šè¿‡åœ¨ç¬¬*j*æ­¥è€ƒè™‘ä¸ä»…æ˜¯*Xj*ï¼Œè€Œä¸”è¿˜åŒ…æ‹¬*Xj*âˆ’1ï¼Œä½¿ä¸€ä¸ªé™æ€çš„ç¥ç»ç½‘ç»œç¨å¾®å…·æœ‰ä¸Šä¸‹æ–‡æ•æ„Ÿæ€§ï¼Œå…¶ä¸­å¯¹äºæ‰€æœ‰*j*
    â‰¥ 2ã€‚'
- en: '![](../Images/0d46519e956b5bf86ffadfafa3d67592.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0d46519e956b5bf86ffadfafa3d67592.png)'
- en: 2-grams of concepts
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 2-gramsæ¦‚å¿µ
- en: The weights *T* are updated in the same way as *W*, by minimizing the losses
    and propagating the updates back from layer to layer. They just add one training
    step per layer. This is not a big deal structurally, but it is a significant slowdown
    computationally. If the inner layers are viewed as latent concept spaces, then
    this architecture can be thought of as a lifting of the idea of 2-grams (capturing
    the dependencies of contests of length 2) from words to concepts. Generalizing
    to *n*-grams for larger *n*s causes further slowdown.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: æƒé‡*T*çš„æ›´æ–°æ–¹å¼ä¸*W*ç›¸åŒï¼Œé€šè¿‡æœ€å°åŒ–æŸå¤±å¹¶ä»å±‚åˆ°å±‚ä¼ æ’­æ›´æ–°ã€‚å®ƒä»¬åªæ˜¯æ¯ä¸€å±‚å¢åŠ ä¸€æ­¥è®­ç»ƒã€‚è¿™åœ¨ç»“æ„ä¸Šå¹¶ä¸é‡è¦ï¼Œä½†åœ¨è®¡ç®—ä¸Šå´æ˜¯ä¸€ä¸ªæ˜¾è‘—çš„å‡é€Ÿã€‚å¦‚æœå°†å†…éƒ¨å±‚è§†ä¸ºæ½œåœ¨æ¦‚å¿µç©ºé—´ï¼Œé‚£ä¹ˆè¯¥æ¶æ„å¯ä»¥çœ‹ä½œæ˜¯å°†2-gramsï¼ˆæ•æ‰é•¿åº¦ä¸º2çš„ä¸Šä¸‹æ–‡ä¾èµ–æ€§ï¼‰è¿™ä¸€æ€æƒ³ä»è¯è¯­æ‰©å±•åˆ°æ¦‚å¿µçš„æå‡ã€‚å°†å…¶æ¨å¹¿åˆ°æ›´å¤§çš„*n*-gramsä¼šå¯¼è‡´è¿›ä¸€æ­¥çš„å‡é€Ÿã€‚
- en: '**Recurrent Neural Networks (RNNs).** The RNNs also apply the same neural network
    on all input tokens and also pass to the *j*-th module not only *Xj* but also
    the information from *Xj*âˆ’1 â€” *b*ut they pass it *after* the previous network
    module was applied to *Xj*âˆ’1, not before.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**é€’å½’ç¥ç»ç½‘ç»œ (RNNs)**ã€‚RNNsä¹Ÿä¼šåœ¨æ‰€æœ‰è¾“å…¥æ ‡è®°ä¸Šåº”ç”¨ç›¸åŒçš„ç¥ç»ç½‘ç»œï¼Œå¹¶ä¸”ä¸ä»…å°†*Xj*ï¼Œè€Œä¸”è¿˜å°†æ¥è‡ª*Xj*âˆ’1çš„ä¿¡æ¯ä¼ é€’åˆ°*j*-thæ¨¡å—â€”â€”*ä½†æ˜¯*å®ƒä»¬æ˜¯åœ¨åº”ç”¨å‰ä¸€ä¸ªç½‘ç»œæ¨¡å—äº*Xj*âˆ’1ä¹‹åï¼Œè€Œä¸æ˜¯ä¹‹å‰ã€‚'
- en: '![](../Images/44981b49fbda774477e8199703430f31.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44981b49fbda774477e8199703430f31.png)'
- en: RNN idea
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: RNNçš„æ€è·¯
- en: Note that information from *Xj*âˆ’1 is this time forwarded by *S* not only to
    the *j*-th module, but with the output of the *j*-th module by the next copy of
    *S* also to the *(j+1)*-st module, and so on. The information propagation is thus
    in principle unbounded, and not truncated like in the *n*-gram model. The matrices
    *S* that propagate important information further are promoted in training. However,
    the weights assigned to all input entries are all packed in *S*. Propagating longer
    contexts requires exponentially wider network modules. So we are back to square
    one, the problem of width.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œ*Xj*âˆ’1 ä¸­çš„ä¿¡æ¯è¿™æ¬¡ä¸ä»…ç”± *S* è½¬å‘åˆ° *j* å·æ¨¡å—ï¼Œè€Œä¸”åœ¨ *j* å·æ¨¡å—çš„è¾“å‡ºé€šè¿‡ *S* çš„ä¸‹ä¸€å¤åˆ¶å“è½¬å‘åˆ° *(j+1)* å·æ¨¡å—ï¼Œä¾æ­¤ç±»æ¨ã€‚å› æ­¤ï¼Œä¿¡æ¯ä¼ æ’­åŸåˆ™ä¸Šæ˜¯æ— é™çš„ï¼Œè€Œä¸åƒ
    *n*-gram æ¨¡å‹é‚£æ ·è¢«æˆªæ–­ã€‚é‚£äº›è¿›ä¸€æ­¥ä¼ æ’­é‡è¦ä¿¡æ¯çš„çŸ©é˜µ *S* ä¼šåœ¨è®­ç»ƒä¸­å¾—åˆ°æå‡ã€‚ç„¶è€Œï¼Œåˆ†é…ç»™æ‰€æœ‰è¾“å…¥æ¡ç›®çš„æƒé‡éƒ½è¢«æ‰“åŒ…åœ¨ *S* ä¸­ã€‚ä¼ æ’­æ›´é•¿çš„ä¸Šä¸‹æ–‡éœ€è¦æŒ‡æ•°çº§æ‰©å±•çš„ç½‘ç»œæ¨¡å—ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å›åˆ°äº†æœ€åˆçš„é—®é¢˜ï¼Œå®½åº¦é—®é¢˜ã€‚
- en: '**Long Short-Term Memory (LSTM).** The LSTM networks address the problem of
    the cost of forwarding the context information between the iterations of the same
    neural network module by forwarding the information from the *(jâˆ’1)*-th input
    token to the *j*-th module both before it was processed by the *(jâˆ’1)*-th module,
    and after. The former makes passing the information from each input more efficient,
    the latter makes the propagation easier.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**é•¿çŸ­æ—¶è®°å¿†ï¼ˆLSTMï¼‰**ã€‚LSTM ç½‘ç»œé€šè¿‡å°† *(jâˆ’1)* å·è¾“å…¥æ ‡è®°çš„ä¿¡æ¯åŒæ—¶è½¬å‘åˆ° *j* å·æ¨¡å—ï¼ˆæ—¢åœ¨ *(jâˆ’1)* å·æ¨¡å—å¤„ç†ä¹‹å‰ï¼Œåˆåœ¨å…¶å¤„ç†ä¹‹åï¼‰æ¥è§£å†³åœ¨åŒä¸€ç¥ç»ç½‘ç»œæ¨¡å—çš„è¿­ä»£ä¹‹é—´è½¬å‘ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æˆæœ¬é—®é¢˜ã€‚å‰è€…ä½¿å¾—ä»æ¯ä¸ªè¾“å…¥ä¼ é€’ä¿¡æ¯æ›´æœ‰æ•ˆï¼Œåè€…ä½¿å¾—ä¼ æ’­æ›´åŠ å®¹æ˜“ã€‚'
- en: '![](../Images/67ce3d32737e914c5b323b1b7c70ff64.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67ce3d32737e914c5b323b1b7c70ff64.png)'
- en: LSTM idea
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM æ€è·¯
- en: The idea of passing around the information at different stages of processing
    is simple enough, but optimizing the benefits is a conundrum, as already the â€œlong
    shortâ€ name may be suggesting. The implementation details are many. Different
    activation functions are applied on different mixtures of the same inputs and
    remixed in different ways for the outputs. Expressing the concepts learned from
    the same data in multiple bases requires multiple matrices and provides more opportunities
    for training. Hence for improvements. But further steps require further ideas.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: å°†ä¿¡æ¯åœ¨ä¸åŒçš„å¤„ç†é˜¶æ®µä¼ é€’çš„æƒ³æ³•ç›¸å½“ç®€å•ï¼Œä½†ä¼˜åŒ–å…¶æ•ˆç›Šå´æ˜¯ä¸€ä¸ªéš¾é¢˜ï¼Œæ­£å¦‚â€œé•¿çŸ­â€è¿™ä¸ªåå­—å¯èƒ½å·²ç»æš—ç¤ºçš„é‚£æ ·ã€‚å®ç°ç»†èŠ‚æœ‰å¾ˆå¤šã€‚ä¸åŒçš„æ¿€æ´»å‡½æ•°åº”ç”¨äºç›¸åŒè¾“å…¥çš„ä¸åŒæ··åˆï¼Œå¹¶ä»¥ä¸åŒçš„æ–¹å¼é‡æ–°ç»„åˆç”¨äºè¾“å‡ºã€‚ä»ç›¸åŒæ•°æ®ä¸­å­¦ä¹ çš„æ¦‚å¿µåœ¨å¤šä¸ªåŸºä¸­è¡¨è¾¾éœ€è¦å¤šä¸ªçŸ©é˜µï¼Œå¹¶æä¾›æ›´å¤šçš„è®­ç»ƒæœºä¼šã€‚å› æ­¤ï¼Œæœ‰äº†æ”¹è¿›çš„å¯èƒ½ã€‚ä½†æ˜¯ï¼Œè¿›ä¸€æ­¥çš„æ­¥éª¤éœ€è¦æ›´å¤šçš„æƒ³æ³•ã€‚
- en: 4.3 Dynamic channel learning
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 åŠ¨æ€é€šé“å­¦ä¹ 
- en: Just like the function learner, the channel learner seeks to learn how the inputs
    are transformed into the outputs. The difference is that the channel transformations
    are context-dependent. Not only are the outputs always dependent on the input
    contexts, but there may be feedforward dependencies of outputs on outputs, and
    feedback dependencies of inputs on outputs, as discussed in the [channel section](https://medium.com/@dusko_p/semantics-the-meaning-of-language-99b009ccef41#d3b1)
    of the [*Semantics* part](https://medium.com/@dusko_p/semantics-the-meaning-of-language-99b009ccef41).
    A dynamic network of neural networks learns a channel by adaptively updating the
    â€œkeyâ€ subnetworks **k***,* processing the channel inputs, and the â€œvalueâ€ subnetworks
    **v,** delivering the corresponding channel outputs.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒå‡½æ•°å­¦ä¹ è€…ä¸€æ ·ï¼Œé€šé“å­¦ä¹ è€…ä¹Ÿè¯•å›¾å­¦ä¹ å¦‚ä½•å°†è¾“å…¥è½¬æ¢ä¸ºè¾“å‡ºã€‚ä¸åŒä¹‹å¤„åœ¨äºï¼Œé€šé“å˜æ¢æ˜¯ä¾èµ–ä¸Šä¸‹æ–‡çš„ã€‚è¾“å‡ºä¸ä»…æ€»æ˜¯ä¾èµ–äºè¾“å…¥ä¸Šä¸‹æ–‡ï¼Œè€Œä¸”å¯èƒ½å­˜åœ¨è¾“å‡ºå¯¹è¾“å‡ºçš„å‰é¦ˆä¾èµ–å…³ç³»ï¼Œä»¥åŠè¾“å…¥å¯¹è¾“å‡ºçš„åé¦ˆä¾èµ–å…³ç³»ï¼Œæ­£å¦‚åœ¨[é€šé“éƒ¨åˆ†](https://medium.com/@dusko_p/semantics-the-meaning-of-language-99b009ccef41#d3b1)çš„[*è¯­ä¹‰*éƒ¨åˆ†](https://medium.com/@dusko_p/semantics-the-meaning-of-language-99b009ccef41)ä¸­è®¨è®ºçš„é‚£æ ·ã€‚ä¸€ä¸ªåŠ¨æ€çš„ç¥ç»ç½‘ç»œç½‘ç»œé€šè¿‡è‡ªé€‚åº”æ›´æ–°â€œé”®â€å­ç½‘ç»œ**k***ï¼Œå¤„ç†é€šé“è¾“å…¥ï¼Œä»¥åŠâ€œå€¼â€å­ç½‘ç»œ**v**ï¼Œæä¾›ç›¸åº”çš„é€šé“è¾“å‡ºï¼Œä»è€Œå­¦ä¹ ä¸€ä¸ªé€šé“ã€‚
- en: '**Encoder-Decoder Procedures.** An important programming concept is the idea
    of a *procedure*. While the earliest programs were just sequences of program instructions,
    procedures enabled programmers to invoke within programs not just instructions
    but also entire programs, encapsulated in procedures as generalized instructions.
    Since procedures can be used inside most program control structures, this enabled
    ***programming over programs***, and gave rise to software engineering. The later
    programming paradigms, modular, object-oriented, component and connector-oriented,
    extend this basic idea.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¼–ç å™¨-è§£ç å™¨ç¨‹åº**ã€‚ä¸€ä¸ªé‡è¦çš„ç¼–ç¨‹æ¦‚å¿µæ˜¯*è¿‡ç¨‹*çš„æ¦‚å¿µã€‚æœ€æ—©çš„ç¨‹åºåªæ˜¯ç¨‹åºæŒ‡ä»¤çš„åºåˆ—ï¼Œè€Œè¿‡ç¨‹ä½¿å¾—ç¨‹åºå‘˜ä¸ä»…èƒ½åœ¨ç¨‹åºä¸­è°ƒç”¨æŒ‡ä»¤ï¼Œè¿˜èƒ½è°ƒç”¨æ•´ä¸ªç¨‹åºï¼Œè¿™äº›ç¨‹åºè¢«å°è£…åœ¨è¿‡ç¨‹å†…ï¼Œä½œä¸ºä¸€èˆ¬åŒ–çš„æŒ‡ä»¤ã€‚ç”±äºè¿‡ç¨‹å¯ä»¥åœ¨å¤§å¤šæ•°ç¨‹åºæ§åˆ¶ç»“æ„ä¸­ä½¿ç”¨ï¼Œè¿™ä½¿å¾—***ç¨‹åºç¼–ç¨‹ç¨‹åº***æˆä¸ºå¯èƒ½ï¼Œå¹¶å‚¬ç”Ÿäº†è½¯ä»¶å·¥ç¨‹ã€‚åæ¥çš„ç¼–ç¨‹èŒƒå¼ï¼Œå¦‚æ¨¡å—åŒ–ç¼–ç¨‹ã€é¢å‘å¯¹è±¡ç¼–ç¨‹ã€ç»„ä»¶å’Œè¿æ¥å™¨å¯¼å‘ç¼–ç¨‹ï¼Œéƒ½æ‰©å±•äº†è¿™ä¸ªåŸºæœ¬æ¦‚å¿µã€‚'
- en: The encoder-decoder architecture is a ***network of neural networks***. If neural
    networks are thought of as programs, it is a program over programs. The encoder-decoder
    architecture **A** = (**e**, **d**) lifts the structure **a** = (*W*, *V*) of
    a wide neural network to a network of networks.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨-è§£ç å™¨æ¶æ„æ˜¯ä¸€ä¸ª***ç¥ç»ç½‘ç»œçš„ç½‘ç»œ***ã€‚å¦‚æœå°†ç¥ç»ç½‘ç»œçœ‹ä½œç¨‹åºï¼Œå®ƒå°±æ˜¯ä¸€ä¸ªâ€œç¨‹åºä¸­çš„ç¨‹åºâ€ã€‚ç¼–ç å™¨-è§£ç å™¨æ¶æ„**A** = (**e**,
    **d**) å°†å®½ç¥ç»ç½‘ç»œ**a** = (*W*, *V*)çš„ç»“æ„æå‡ä¸ºä¸€ä¸ªç½‘ç»œçš„ç½‘ç»œã€‚
- en: '![](../Images/f8c7a4f420ed913c2fe3f243a8df4626.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8c7a4f420ed913c2fe3f243a8df4626.png)'
- en: Encoder-decoder
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨-è§£ç å™¨
- en: The input remixing matrix *W* is replaced by an encoder network **e**, the output
    remixing matrix *V* by a decoder network **d**. Both the wide network **a** and
    its lifting **A** follow the architectural pattern of [concept mining](#4f8f).
    Just like procedural programming allowed lifting control structures from programs
    to software systems, the encoder-decoder architecture allows lifting the concept
    mining structures from neural networks to neural architectures. The problem with
    the basic form of the encoder-decoder architecture as a concept mining framework
    is that a concept space induced by a static dataset is static whereas channels
    are dynamic. To genuinely learn concepts from a channel, a neural network architecture
    needs dynamic components.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥é‡æ··çŸ©é˜µ*W*è¢«ç¼–ç å™¨ç½‘ç»œ**e**æ›¿ä»£ï¼Œè¾“å‡ºé‡æ··çŸ©é˜µ*V*è¢«è§£ç å™¨ç½‘ç»œ**d**æ›¿ä»£ã€‚å®½ç½‘ç»œ**a**åŠå…¶æå‡**A**éµå¾ª[æ¦‚å¿µæŒ–æ˜](#4f8f)çš„æ¶æ„æ¨¡å¼ã€‚å°±åƒè¿‡ç¨‹å¼ç¼–ç¨‹å…è®¸å°†æ§åˆ¶ç»“æ„ä»ç¨‹åºæå‡åˆ°è½¯ä»¶ç³»ç»Ÿä¸€æ ·ï¼Œç¼–ç å™¨-è§£ç å™¨æ¶æ„ä¹Ÿå…è®¸å°†æ¦‚å¿µæŒ–æ˜ç»“æ„ä»ç¥ç»ç½‘ç»œæå‡åˆ°ç¥ç»æ¶æ„ã€‚ä½œä¸ºæ¦‚å¿µæŒ–æ˜æ¡†æ¶çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„çš„åŸºæœ¬å½¢å¼çš„é—®é¢˜åœ¨äºï¼Œç”±é™æ€æ•°æ®é›†å¼•èµ·çš„æ¦‚å¿µç©ºé—´æ˜¯é™æ€çš„ï¼Œè€Œä¿¡é“æ˜¯åŠ¨æ€çš„ã€‚ä¸ºäº†çœŸæ­£ä»ä¿¡é“ä¸­å­¦ä¹ æ¦‚å¿µï¼Œç¥ç»ç½‘ç»œæ¶æ„éœ€è¦åŠ¨æ€ç»„ä»¶ã€‚
- en: '**Idea of attention.** A natural step towards enabling neural networks to predict
    the outputs of a channel'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„åŠ›çš„æ¦‚å¿µ**ã€‚ä½¿ç¥ç»ç½‘ç»œèƒ½å¤Ÿé¢„æµ‹ä¿¡é“è¾“å‡ºçš„ä¸€ä¸ªè‡ªç„¶æ­¥éª¤'
- en: '![](../Images/9d06990fcbde5f81cc2dfe1f5e034331.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d06990fcbde5f81cc2dfe1f5e034331.png)'
- en: is to generalize the basic ğœ-neuron from the [CHSW construction](#5646)
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯å°†åŸºæœ¬çš„ ğœ-ç¥ç»å…ƒä»[CHSWæ„é€ ](#5646)ä¸­æ¨å¹¿
- en: '![](../Images/9837ae9b75a6e5f0b870d1cf3ae0a1ca.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9837ae9b75a6e5f0b870d1cf3ae0a1ca.png)'
- en: In this format, a component of an Encoder-Decoder procedure output would be
    something like
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æ ¼å¼ä¸­ï¼Œç¼–ç å™¨-è§£ç å™¨ç¨‹åºè¾“å‡ºçš„ä¸€ä¸ªç»„æˆéƒ¨åˆ†å¯èƒ½æ˜¯
- en: '![](../Images/b2cd3b620caf1f205ee938fc9cd86c54.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2cd3b620caf1f205ee938fc9cd86c54.png)'
- en: where
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­
- en: '![](../Images/0214787a853c0c7be7df4df427a10cf1.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0214787a853c0c7be7df4df427a10cf1.png)'
- en: are basic encoder and decoder matrices from a [channel concept mining framework](#4f8f).
    But now we need to take into account the concepts learned at inner layers of a
    deep network. The impacts on the (*n+1)-*st output value of the input vectors
    |*xj*âŸ© are therefore weighed by their projections âŸ¨*ej* | *xj*âŸ© on the input concepts
    âŸ¨*ej*| *and* the projections âŸ¨*yn* | *dj*âŸ© of the row vector âŸ¨*yn*| of the previous
    outputs on the output concepts |*dj* âŸ©. The relationship between the corresponding
    concepts âŸ¨*ej*| and |*dj* âŸ© are trained to align the channel inputs and the channel
    outputs. This is the basic idea of the *attention architecture*. It can be drawn
    as a common generalization of the [ğœ-neuron](#ef75) and the [SVD-schema](#46fc),
    with dynamic singular values. (This is an instructive **exercise**.) For string
    outputs, the obvious extension is
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯æ¥è‡ª [é€šé“æ¦‚å¿µæŒ–æ˜æ¡†æ¶](#4f8f)çš„åŸºæœ¬ç¼–ç å™¨å’Œè§£ç å™¨çŸ©é˜µã€‚ä½†ç°åœ¨æˆ‘ä»¬éœ€è¦è€ƒè™‘åœ¨æ·±åº¦ç½‘ç»œçš„å†…éƒ¨å±‚å­¦ä¹ åˆ°çš„æ¦‚å¿µã€‚è¾“å…¥å‘é‡ |*xj*âŸ© å¯¹ (*n+1)-*st
    è¾“å‡ºå€¼çš„å½±å“å› æ­¤é€šè¿‡å®ƒä»¬åœ¨è¾“å…¥æ¦‚å¿µ âŸ¨*ej*| ä¸Šçš„æŠ•å½± âŸ¨*ej* | *xj*âŸ© ä»¥åŠå‰ä¸€è¾“å‡ºçš„è¡Œå‘é‡ âŸ¨*yn*| åœ¨è¾“å‡ºæ¦‚å¿µ |*dj* âŸ© ä¸Šçš„æŠ•å½±
    âŸ¨*yn* | *dj*âŸ© æ¥åŠ æƒã€‚ç›¸åº”æ¦‚å¿µ âŸ¨*ej*| å’Œ |*dj* âŸ© ä¹‹é—´çš„å…³ç³»è¢«è®­ç»ƒä»¥å¯¹é½é€šé“è¾“å…¥å’Œé€šé“è¾“å‡ºã€‚è¿™å°±æ˜¯ *æ³¨æ„åŠ›æ¶æ„* çš„åŸºæœ¬æ€æƒ³ã€‚å®ƒå¯ä»¥è¢«ç»˜åˆ¶ä¸º
    [ğœ-ç¥ç»å…ƒ](#ef75) å’Œ [SVD-æ¨¡å¼](#46fc) çš„å¸¸è§å¹¿ä¹‰å½¢å¼ï¼Œå…·æœ‰åŠ¨æ€å¥‡å¼‚å€¼ã€‚ï¼ˆè¿™æ˜¯ä¸€ä¸ªæœ‰æ•™è‚²æ„ä¹‰çš„ **ç»ƒä¹ **ã€‚ï¼‰å¯¹äºå­—ç¬¦ä¸²è¾“å‡ºï¼Œæ˜¾è€Œæ˜“è§çš„æ‰©å±•æ˜¯
- en: '![](../Images/a9e071205f4d39ec6785272b9402232b.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9e071205f4d39ec6785272b9402232b.png)'
- en: But it is not obvious how to train the matrix *V* whose rows are the output
    mixtures âŸ¨*vi* |. The issue is solved by approaching the task from a slightly
    different direction.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯å¦‚ä½•è®­ç»ƒçŸ©é˜µ *V* çš„é—®é¢˜å¹¶ä¸æ˜¾è€Œæ˜“è§ï¼ŒçŸ©é˜µçš„è¡Œæ˜¯è¾“å‡ºæ··åˆä½“ âŸ¨*vi* |ã€‚è¿™ä¸ªé—®é¢˜é€šè¿‡ä»ç•¥æœ‰ä¸åŒçš„æ–¹å‘æ¥è§£å†³ã€‚
- en: '**Dynamic concept decomposition.** A set of vectors *|1âŸ©,|2âŸ©,â€¦,|nâŸ©* is said
    to span the vector space if every vector |*y*âŸ© can be expressed in the form'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '**åŠ¨æ€æ¦‚å¿µåˆ†è§£ã€‚** ä¸€ç»„å‘é‡ *|1âŸ©,|2âŸ©,â€¦,|nâŸ©* è¢«ç§°ä¸ºèƒ½å¤Ÿç”Ÿæˆå‘é‡ç©ºé—´ï¼Œå¦‚æœæ¯ä¸ªå‘é‡ |*y*âŸ© éƒ½å¯ä»¥è¡¨ç¤ºä¸ºä»¥ä¸‹å½¢å¼'
- en: '![](../Images/bec39bcf8e4bfd5a0170e95f88c5b9fa.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bec39bcf8e4bfd5a0170e95f88c5b9fa.png)'
- en: The decomposition is unique if and only if the vectors *|1âŸ©,â€¦,|nâŸ©* are orthogonal,
    in the sense that âŸ¨*i*|*j*âŸ© = 0 as soon as *iâ‰ j*. If the spanning vectors are
    not orthogonal, but there is an orthogonal set *|c1âŸ©,|c2âŸ©,â€¦,|cnâŸ©*, then there
    is a unique decomposition
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä¸”ä»…å½“å‘é‡ *|1âŸ©,â€¦,|nâŸ©* å½¼æ­¤æ­£äº¤æ—¶ï¼Œåˆ†è§£æ˜¯å”¯ä¸€çš„ï¼Œè¿™æ„å‘³ç€å½“ *iâ‰ j* æ—¶ï¼ŒâŸ¨*i*|*j*âŸ© = 0ã€‚å¦‚æœç”Ÿæˆå‘é‡ä¸æ˜¯æ­£äº¤çš„ï¼Œä½†å­˜åœ¨ä¸€ä¸ªæ­£äº¤é›†åˆ
    *|c1âŸ©,|c2âŸ©,â€¦,|cnâŸ©*ï¼Œé‚£ä¹ˆå°±å­˜åœ¨ä¸€ä¸ªå”¯ä¸€çš„åˆ†è§£ã€‚
- en: '![](../Images/1745a488eda1099938cb5ae274429a4f.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1745a488eda1099938cb5ae274429a4f.png)'
- en: As discussed [earlier](#4f8f), concept analysis is the quest for concept bases
    with minimal interferences between the basic concepts. The basic concept vectors
    do not interfere at all when they are mutually orthogonal. If a channelis implemented
    by a neural network, the above concept decomposition becomes
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ [å‰æ–‡](#4f8f)è®¨è®ºçš„é‚£æ ·ï¼Œæ¦‚å¿µåˆ†ææ˜¯å¯»æ±‚å…·æœ‰æœ€å°å¹²æ‰°çš„åŸºæœ¬æ¦‚å¿µåŸºã€‚åŸºæœ¬æ¦‚å¿µå‘é‡åœ¨ç›¸äº’æ­£äº¤æ—¶å®Œå…¨ä¸å¹²æ‰°ã€‚å¦‚æœé€šé“é€šè¿‡ç¥ç»ç½‘ç»œå®ç°ï¼Œä»¥ä¸Šçš„æ¦‚å¿µåˆ†è§£å˜ä¸º
- en: '![](../Images/01935a41b836071a7c373f248f77b249.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01935a41b836071a7c373f248f77b249.png)'
- en: The first difference is that the activation function Ïƒ allows approximating
    nonlinearities. The second is that the components are not projected on the original
    basis vectors âŸ¨*i*| anymore but on the output mixtures âŸ¨*vi*|. Lastly and most
    importantly, the concept decomposition was unique because the concept basis *|c1âŸ©,â€¦,|cnâŸ©*
    was orthogonal, whereas here the output is projected on the inputs *|x1âŸ©,â€¦,|xnâŸ©*,
    which are not orthogonal. But if an orthogonal concept basis *|c1âŸ©,â€¦,|cnâŸ©* exists,
    we can play the same trick again, and get a unique concept decomposition
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªä¸åŒä¹‹å¤„åœ¨äºæ¿€æ´»å‡½æ•° Ïƒ å…è®¸è¿‘ä¼¼éçº¿æ€§ã€‚ç¬¬äºŒä¸ªä¸åŒä¹‹å¤„åœ¨äºï¼Œç»„ä»¶ä¸å†æŠ•å½±åˆ°åŸå§‹åŸºå‘é‡ âŸ¨*i*| ä¸Šï¼Œè€Œæ˜¯æŠ•å½±åˆ°è¾“å‡ºæ··åˆä½“ âŸ¨*vi*| ä¸Šã€‚æœ€åä¹Ÿæ˜¯æœ€é‡è¦çš„ä¸€ç‚¹ï¼Œæ¦‚å¿µåˆ†è§£æ˜¯ç‹¬ç‰¹çš„ï¼Œå› ä¸ºæ¦‚å¿µåŸº
    *|c1âŸ©,â€¦,|cnâŸ©* æ˜¯æ­£äº¤çš„ï¼Œè€Œè¿™é‡Œçš„è¾“å‡ºæŠ•å½±åˆ°è¾“å…¥ *|x1âŸ©,â€¦,|xnâŸ©*ï¼Œå®ƒä»¬å¹¶ä¸æ˜¯æ­£äº¤çš„ã€‚ä½†å¦‚æœå­˜åœ¨æ­£äº¤çš„æ¦‚å¿µåŸº *|c1âŸ©,â€¦,|cnâŸ©*ï¼Œæˆ‘ä»¬å¯ä»¥å†æ¬¡ä½¿ç”¨ç›¸åŒçš„æŠ€å·§ï¼Œå¾—åˆ°å”¯ä¸€çš„æ¦‚å¿µåˆ†è§£ã€‚
- en: '![](../Images/50fd452382b15f00fcf7b2c5a0b7077f.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/50fd452382b15f00fcf7b2c5a0b7077f.png)'
- en: What does this abstract decomposition mean for a concrete channel? The projections
    on the concept basis vectors measure their weights in the inputs |*xj*âŸ© and in
    the outputs |*yn*âŸ©. The sum of the products of the projections measures the impact
    of the input |*xj*âŸ© on the output |*yn*âŸ©. This measurement, activated by Ïƒ, then
    impacts the *i*-th component of the channel output | *yn* âŸ© according to the projection
    âŸ¨*vi* | *xj*âŸ©.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æŠ½è±¡åˆ†è§£å¯¹äºå…·ä½“é€šé“æ„å‘³ç€ä»€ä¹ˆï¼Ÿåœ¨æ¦‚å¿µåŸºå‘é‡ä¸Šçš„æŠ•å½±è¡¡é‡äº†å®ƒä»¬åœ¨è¾“å…¥ *|xj*âŸ© å’Œè¾“å‡º *|yn*âŸ© ä¸­çš„æƒé‡ã€‚æŠ•å½±ä¹˜ç§¯çš„å’Œè¡¡é‡äº†è¾“å…¥ *|xj*âŸ©
    å¯¹è¾“å‡º *|yn*âŸ© çš„å½±å“ã€‚è¿™ä¸ªæµ‹é‡ç”± Ïƒ æ¿€æ´»ï¼Œç„¶åæ ¹æ®æŠ•å½± âŸ¨*vi* | *xj*âŸ© å½±å“é€šé“è¾“å‡º *|yn*âŸ© çš„ç¬¬ *i* ä¸ªåˆ†é‡ã€‚
- en: The only problem is that the projection of |*yn*âŸ© on the right is unknown since
    |*yn*âŸ© is what we are trying to predict. What other value can be used to approximate
    the impact of a concept in the output |*yn*âŸ©? â€” Two answers have been proposed.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: å”¯ä¸€çš„é—®é¢˜æ˜¯ï¼Œå³ä¾§çš„ *|yn*âŸ© æŠ•å½±æ˜¯æœªçŸ¥çš„ï¼Œå› ä¸º *|yn*âŸ© æ­£æ˜¯æˆ‘ä»¬è¯•å›¾é¢„æµ‹çš„è¾“å‡ºã€‚å¯ä»¥ç”¨ä»€ä¹ˆå…¶ä»–å€¼æ¥è¿‘ä¼¼æ¦‚å¿µåœ¨è¾“å‡º *|yn*âŸ© ä¸­çš„å½±å“å‘¢ï¼Ÿâ€”
    å·²ç»æå‡ºäº†ä¸¤ç§ç­”æ¡ˆã€‚
- en: '**Translatorâ€™s attention:** If the channel *F* : *[X* âŠ¢ *Y]* is a [translation](#d171),
    say from Mandarin to French through concepts *|c*âŸ©, then the summands âŸ¨*xj*|*c*âŸ©âŸ¨*c*|*yn*âŸ©
    can be thought of as distributing *translatorâ€™s attention* over the concepts *|c*âŸ©,
    latent in the Mandarin input tokens |*xj*âŸ© *after* the French output token |*yn*âŸ©
    is produced. That is the attention that effectively impacts the *(n+1)*-st output,
    and the above decomposition should be updated to'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç¿»è¯‘è€…çš„æ³¨æ„åŠ›ï¼š** å¦‚æœé€šé“ *F*ï¼š *[X* âŠ¢ *Y]* æ˜¯[ç¿»è¯‘](#d171)ï¼Œä¾‹å¦‚é€šè¿‡æ¦‚å¿µ *|c*âŸ© ä»ä¸­æ–‡ç¿»è¯‘åˆ°æ³•è¯­ï¼Œé‚£ä¹ˆåŠ æ³•é¡¹
    âŸ¨*xj*|*c*âŸ©âŸ¨*c*|*yn*âŸ© å¯ä»¥è¢«çœ‹ä½œæ˜¯å°† *ç¿»è¯‘è€…çš„æ³¨æ„åŠ›* åˆ†å¸ƒåˆ°ä¸­æ–‡è¾“å…¥ç¬¦å· *|xj*âŸ© ä¸Šï¼Œè€Œè¿™ç§æ³¨æ„åŠ›æ˜¯åœ¨æ³•è¯­è¾“å‡ºç¬¦å· *|yn*âŸ©
    äº§ç”Ÿä¹‹åæ½œè—åœ¨ *|c*âŸ© ä¸Šçš„ã€‚è¿™å°±æ˜¯å®é™…ä¸Šå½±å“ *ï¼ˆn+1ï¼‰* è¾“å‡ºçš„æ³¨æ„åŠ›ï¼Œè€Œä¸Šè¿°åˆ†è§£åº”æ›´æ–°ä¸º'
- en: '![](../Images/72516dceb2c5967cdb28f109a60ebe17.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/72516dceb2c5967cdb28f109a60ebe17.png)'
- en: '**Speakerâ€™s self-attention:** If the channel *F*: *[X* âŠ¢ *Y]* is not a translation
    to another language but a continuation *Y* in the same language, then it is not
    [feedback-free](https://medium.com/@dusko_p/semantics-the-meaning-of-language-99b009ccef41#aba3),
    since *Xn*+1 is not independent of *Yn*, but identical to it. To capture the feedback,
    the concept base splits into an encoding base and a decoding base as [above](#5b17),
    expressing the inputs as mixtures of concepts, and the concepts as mixtures of
    outputs. But since each output is now recast as the next input, the *encoder-decoder*
    view morphs into the *key-query-value* view of language production as an ongoing
    stochastic process of database retrieval, with the projections of the inputs to
    the queries modeling a rudimentary â€œattention spanâ€ over the preceding context:'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è¯´è¯è€…çš„è‡ªæ³¨æ„åŠ›ï¼š** å¦‚æœé€šé“ *F*ï¼š *[X* âŠ¢ *Y]* ä¸æ˜¯å¯¹å¦ä¸€ç§è¯­è¨€çš„ç¿»è¯‘ï¼Œè€Œæ˜¯åŒä¸€ç§è¯­è¨€ä¸­çš„å»¶ç»­ *Y*ï¼Œé‚£ä¹ˆå®ƒå°±ä¸æ˜¯[æ— åé¦ˆ](https://medium.com/@dusko_p/semantics-the-meaning-of-language-99b009ccef41#aba3)ï¼Œå› ä¸º
    *Xn*+1 å¹¶ä¸æ˜¯ç‹¬ç«‹äº *Yn*ï¼Œè€Œæ˜¯ä¸å…¶ç›¸åŒã€‚ä¸ºäº†æ•æ‰åé¦ˆï¼Œæ¦‚å¿µåŸºè¢«åˆ†ä¸ºç¼–ç åŸºå’Œè§£ç åŸºï¼Œå¦‚[ä¸Šé¢](#5b17)æ‰€ç¤ºï¼Œå°†è¾“å…¥è¡¨ç¤ºä¸ºæ¦‚å¿µçš„æ··åˆï¼Œè€Œå°†æ¦‚å¿µè¡¨ç¤ºä¸ºè¾“å‡ºçš„æ··åˆã€‚ä½†ç”±äºæ¯ä¸ªè¾“å‡ºç°åœ¨éƒ½è¢«é‡æ–°å®šä¹‰ä¸ºä¸‹ä¸€ä¸ªè¾“å…¥ï¼Œ*ç¼–ç å™¨-è§£ç å™¨*è§†è§’è½¬å˜ä¸ºè¯­è¨€ç”Ÿæˆçš„*é”®-æŸ¥è¯¢-å€¼*è§†è§’ï¼Œä½œä¸ºæ•°æ®åº“æ£€ç´¢çš„æŒç»­éšæœºè¿‡ç¨‹ï¼Œå…¶ä¸­è¾“å…¥çš„æŠ•å½±åˆ°æŸ¥è¯¢ä¸Šæ¨¡æ‹Ÿäº†å‰æ–‡ä¸Šä¸‹æ–‡çš„åŸºæœ¬â€œæ³¨æ„åŠ›èŒƒå›´â€ï¼š'
- en: '![](../Images/a5872daaf3b6d10313b4ffce8fb043fe.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a5872daaf3b6d10313b4ffce8fb043fe.png)'
- en: The self-attention modules in the form **a** = (*K*, *Q*, *V*), with
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªæ³¨æ„åŠ›æ¨¡å—å½¢å¼ä¸º **a** = (*K*, *Q*, *V*)ï¼Œå…¶ä¸­
- en: '![](../Images/c8d1926aa0cee155b4691ff40706ba9a.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c8d1926aa0cee155b4691ff40706ba9a.png)'
- en: 'are the central components of the *transformer* architecture, which is the
    â€œTâ€ of the GPTs. They can be viewed as a step towards capturing the general language-production
    channels discussed in the [Semantics part](https://medium.com/p/99b009ccef41#d3b1),
    with the query vectors capturing the basic feedback flows and the value vectors
    capturing the feedforward flows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯ *transformer* æ¶æ„çš„æ ¸å¿ƒç»„ä»¶ï¼Œå®ƒæ˜¯ GPTs çš„â€œTâ€éƒ¨åˆ†ã€‚å®ƒä»¬å¯ä»¥çœ‹ä½œæ˜¯æ•æ‰[è¯­ä¹‰éƒ¨åˆ†](https://medium.com/p/99b009ccef41#d3b1)ä¸­è®¨è®ºçš„é€šç”¨è¯­è¨€ç”Ÿæˆé€šé“çš„ä¸€æ­¥ï¼Œå…¶ä¸­æŸ¥è¯¢å‘é‡æ•æ‰äº†åŸºæœ¬çš„åé¦ˆæµï¼Œå€¼å‘é‡æ•æ‰äº†å‰é¦ˆæµï¼š
- en: '![](../Images/cc442b45cc44e834929a4b9d97bfd248.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc442b45cc44e834929a4b9d97bfd248.png)'
- en: Reconciling the intuitions for attention as a mental process with the database
    terminology for key-query-value may feel awkward at first, yet the time may be
    ripe to expand our intuitions and recognize the same natural processes unfurling
    in our heads and in computers.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ³¨æ„åŠ›ä½œä¸ºä¸€ç§å¿ƒç†è¿‡ç¨‹çš„ç›´è§‰ä¸æ•°æ®åº“æœ¯è¯­ä¸­çš„é”®-æŸ¥è¯¢-å€¼å¯¹æ¥èµ·æ¥ï¼Œæœ€åˆå¯èƒ½æ„Ÿè§‰æœ‰äº›å°´å°¬ï¼Œä½†æˆ–è®¸æ˜¯æ—¶å€™æ‹“å±•æˆ‘ä»¬çš„ç›´è§‰ï¼Œæ„è¯†åˆ°è¿™äº›è‡ªç„¶è¿‡ç¨‹åœ¨æˆ‘ä»¬çš„å¤§è„‘å’Œè®¡ç®—æœºä¸­æ˜¯ç›¸åŒçš„ã€‚
- en: 5 Beyond hallucinations
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 è¶…è¶Šå¹»è§‰
- en: 5.1 Parametric learning framework
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 å‚æ•°åŒ–å­¦ä¹ æ¡†æ¶
- en: Staring back at the [general learning framework](#e77a), after a while you realize
    that the transformer architecture uncovered a feature that was not visible in
    the [learning diagram](#5da6) there. It uncovered that *models and their programs
    can be parametrized.*
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: å›é¡¾ä¸€ä¸‹[é€šç”¨å­¦ä¹ æ¡†æ¶](#e77a)ï¼Œä½ ä¼šæ„è¯†åˆ°Transformeræ¶æ„æ­ç¤ºäº†ä¸€ä¸ªåœ¨[å­¦ä¹ å›¾](#5da6)ä¸­çœ‹ä¸åˆ°çš„ç‰¹æ€§ã€‚å®ƒæ­ç¤ºäº†*æ¨¡å‹åŠå…¶ç¨‹åºå¯ä»¥è¢«å‚æ•°åŒ–*ã€‚
- en: '![](../Images/4dbff571ec4acb9e2d41c0dffb262fa2.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4dbff571ec4acb9e2d41c0dffb262fa2.png)'
- en: 'A learner can train a model **a**(*X*) that captures the dependencies of a
    channel *F* on *n-*input contexts, for any *n*, and leave the *(n+1)*-st input
    *X* as a program parameter. When the *(n+1)*-st input is sampled to *X=*|*x*âŸ©,
    the model is instantiated to **a**|*x*âŸ©*.* Interpreting this instance produces
    a prediction of the next channel output:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: å­¦ä¹ è€…å¯ä»¥è®­ç»ƒä¸€ä¸ªæ¨¡å‹**a**(*X*)ï¼Œè¯¥æ¨¡å‹æ•æ‰äº†é€šé“*F*å¯¹*n*è¾“å…¥ä¸Šä¸‹æ–‡çš„ä¾èµ–å…³ç³»ï¼Œå¯¹äºä»»ä½•*n*ï¼Œå¹¶å°†*(n+1)*-stè¾“å…¥*X*ä½œä¸ºç¨‹åºå‚æ•°ã€‚å½“*(n+1)*-stè¾“å…¥è¢«é‡‡æ ·ä¸º*X=*|*x*âŸ©æ—¶ï¼Œæ¨¡å‹å®ä¾‹åŒ–ä¸º**a**|*x*âŸ©*ã€‚è§£é‡Šè¯¥å®ä¾‹ä¼šäº§ç”Ÿå¯¹ä¸‹ä¸€ä¸ªé€šé“è¾“å‡ºçš„é¢„æµ‹ï¼š
- en: '![](../Images/6864d58788dac8258619861567c89542.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6864d58788dac8258619861567c89542.png)'
- en: Transformers are parametric programs in the form a(*X*) = *(K*, *Q*(*X*),*V)*.
    Parametricity is an important feature of computation, arising from the partial
    interpretability of programs, which propagates to machine learning as the partial
    learnability of models. While the partial evaluation of programs grew from GÃ¶delâ€™s
    Substitution Lemma and Kleeneâ€™s Smn Theorem into a practical programming methodology,
    the partial learnability of models seems to have evolved in practice and, as far
    as I can tell, awaits a theory. In the rest of this note, I sketch some preliminary
    ideas[â¹](http://ed33).
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: Transformeræ˜¯å½¢å¼ä¸ºa(*X*) = *(K*, *Q*(*X*),*V)*çš„å‚æ•°åŒ–ç¨‹åºã€‚å‚æ•°åŒ–æ€§æ˜¯è®¡ç®—çš„é‡è¦ç‰¹å¾ï¼Œæºè‡ªç¨‹åºçš„éƒ¨åˆ†å¯è§£é‡Šæ€§ï¼Œè¿™ä¸€ç‰¹å¾ä¼ æ’­åˆ°æœºå™¨å­¦ä¹ ä¸­ï¼Œè¡¨ç°ä¸ºæ¨¡å‹çš„éƒ¨åˆ†å¯å­¦ä¹ æ€§ã€‚è™½ç„¶ç¨‹åºçš„éƒ¨åˆ†è¯„ä¼°ä»å“¥å¾·å°”çš„æ›¿ä»£å¼•ç†å’Œå…‹è±å°¼çš„Smnå®šç†å‘å±•ä¸ºä¸€ç§å®ç”¨çš„ç¼–ç¨‹æ–¹æ³•è®ºï¼Œä½†æ¨¡å‹çš„éƒ¨åˆ†å¯å­¦ä¹ æ€§ä¼¼ä¹æ˜¯åœ¨å®è·µä¸­é€æ¸æ¼”å˜çš„ï¼Œæ®æˆ‘æ‰€çŸ¥ï¼Œå®ƒä»å¾…ç†è®ºåŒ–ã€‚åœ¨æœ¬æ–‡çš„å…¶ä½™éƒ¨åˆ†ï¼Œæˆ‘å°†å‹¾ç”»ä¸€äº›åˆæ­¥çš„æƒ³æ³•[â¹](http://ed33)ã€‚
- en: 5.2 Self-learning
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 è‡ªæˆ‘å­¦ä¹ 
- en: The parametric learning framework in the [above diagram](#e981) captures the
    learning scenarios where learners interact and learn to predict each otherâ€™s behaviors.
    This includes not only conversations between different learning machines, or betewen
    different instances of the same machine, but also a self-learning process where
    a learning machine learns to predict its own behaviors modulo a parameter. A framework
    for such self-learning can be obtained by instantiating the supervisor *F* in
    the [parametric learning framework](#16d1) to
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[ä¸Šå›¾](#e981)ä¸­çš„å‚æ•°å­¦ä¹ æ¡†æ¶æ•æ‰äº†å­¦ä¹ è€…ç›¸äº’ä½œç”¨å¹¶å­¦ä¹ é¢„æµ‹å½¼æ­¤è¡Œä¸ºçš„å­¦ä¹ åœºæ™¯ã€‚è¿™ä¸ä»…åŒ…æ‹¬ä¸åŒå­¦ä¹ æœºå™¨ä¹‹é—´çš„å¯¹è¯ï¼Œæˆ–åŒä¸€æœºå™¨ä¸åŒå®ä¾‹ä¹‹é—´çš„å¯¹è¯ï¼Œè¿˜åŒ…æ‹¬ä¸€ç§è‡ªæˆ‘å­¦ä¹ è¿‡ç¨‹ï¼Œåœ¨è¿™ç§è¿‡ç¨‹ä¸­ï¼Œå­¦ä¹ æœºå™¨å­¦ä¹ é¢„æµ‹å…¶è‡ªèº«è¡Œä¸ºï¼ŒæŒ‰å‚æ•°è¿›è¡Œè°ƒæ•´ã€‚é€šè¿‡å®ä¾‹åŒ–[å‚æ•°å­¦ä¹ æ¡†æ¶](#16d1)ä¸­çš„ç›‘ç£è€…*F*ï¼Œå¯ä»¥å¾—åˆ°è¿™æ ·ä¸€ç§è‡ªæˆ‘å­¦ä¹ æ¡†æ¶ã€‚'
- en: '![](../Images/09e09095ad8f65befca4d21eb0961a38.png)![](../Images/35dbb3bbaebdce7bcdeff653f4328efe.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09e09095ad8f65befca4d21eb0961a38.png)![](../Images/35dbb3bbaebdce7bcdeff653f4328efe.png)'
- en: 'The obtained model **s** of *self* allows the learner to predict its own future
    behaviors, as parametrized by future inputs:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: è·å¾—çš„**s**æ¨¡å‹å¯ä»¥è®©å­¦ä¹ è€…é¢„æµ‹å…¶æœªæ¥è¡Œä¸ºï¼Œé¢„æµ‹æ˜¯é€šè¿‡æœªæ¥è¾“å…¥å‚æ•°åŒ–çš„ï¼š
- en: '![](../Images/82ac4b6d98c393efbb04919680f7d220.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82ac4b6d98c393efbb04919680f7d220.png)'
- en: This framework also captures the cases of unintended self-learning, where learning
    machines get trained on corpora saturated by their own outputs, due to overproduction
    and overuse, in a process familiar from other industries that tap into natural
    resources.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¡†æ¶è¿˜æ•æ‰äº†æ— æ„è‡ªæˆ‘å­¦ä¹ çš„æƒ…å†µï¼Œåœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œå­¦ä¹ æœºå™¨å› è¿‡åº¦ç”Ÿäº§å’Œè¿‡åº¦ä½¿ç”¨ï¼Œåœ¨å……æ»¡è‡ªèº«è¾“å‡ºçš„è¯­æ–™åº“ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™ä¸€è¿‡ç¨‹ç±»ä¼¼äºå…¶ä»–è¡Œä¸šåœ¨å¼€é‡‡è‡ªç„¶èµ„æºæ—¶çš„æƒ…å†µã€‚
- en: '**Predicting effects of predictions.** Learning the consequences of what we
    do sometimes impacts what we do. *To make e*ff*ective predictions, the learner
    must take into account the e*ff*ects of their predictions.* Parametric learning
    provides a framework for that. The learnerâ€™s capability to predict the effects
    of their predictions allows them to steer the predictions in the desired direction.
    This is how intentionally self-fulfilling prophecies, self-validating, or self-invalidating
    theories come about. A particularly interesting and worrying case is presented
    by *adaptive* theories, designed to pass all tests by reinterpreting their predictions.
    Such logical phenomena are ubiquitous in history, culture, and religions[Â¹â°](#4f51).
    The learning machines surely evolve such processes faster and more methodically.
    The method to produce them is based on the learnerâ€™s model **s**of self.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '**é¢„æµ‹é¢„æµ‹çš„æ•ˆæœã€‚** å­¦ä¹ æˆ‘ä»¬è¡Œä¸ºçš„åæœæœ‰æ—¶ä¼šå½±å“æˆ‘ä»¬è‡ªå·±çš„è¡Œä¸ºã€‚*ä¸ºäº†åšå‡ºæœ‰æ•ˆçš„é¢„æµ‹ï¼Œå­¦ä¹ è€…å¿…é¡»è€ƒè™‘åˆ°ä»–ä»¬é¢„æµ‹çš„æ•ˆæœã€‚* å‚æ•°åŒ–å­¦ä¹ ä¸ºæ­¤æä¾›äº†æ¡†æ¶ã€‚å­¦ä¹ è€…é¢„æµ‹è‡ªå·±é¢„æµ‹çš„æ•ˆæœçš„èƒ½åŠ›ä½¿å¾—ä»–ä»¬èƒ½å¤Ÿå°†é¢„æµ‹å¼•å¯¼åˆ°æœŸæœ›çš„æ–¹å‘ã€‚è¿™å°±æ˜¯æœ‰æ„çš„è‡ªæˆ‘å®ç°é¢„è¨€ã€è‡ªæˆ‘éªŒè¯æˆ–è‡ªæˆ‘å¦å®šç†è®ºçš„å½¢æˆæ–¹å¼ã€‚ä¸€ç§ç‰¹åˆ«æœ‰è¶£ä¸”ä»¤äººæ‹…å¿§çš„æƒ…å†µæ˜¯*è‡ªé€‚åº”*ç†è®ºï¼Œå®ƒä»¬é€šè¿‡é‡æ–°è§£é‡Šé¢„æµ‹æ¥é€šè¿‡æ‰€æœ‰æµ‹è¯•ã€‚è¿™ç±»é€»è¾‘ç°è±¡åœ¨å†å²ã€æ–‡åŒ–å’Œå®—æ•™ä¸­æ— å¤„ä¸åœ¨[Â¹â°](#4f51)ã€‚å­¦ä¹ æœºæ— ç–‘èƒ½æ›´å¿«æ›´æœ‰æ¡ç†åœ°è¿›åŒ–å‡ºè¿™äº›è¿‡ç¨‹ã€‚ç”Ÿæˆå®ƒä»¬çš„æ–¹æ³•åŸºäºå­¦ä¹ è€…å…³äºè‡ªæˆ‘çš„æ¨¡å‹**s**ã€‚'
- en: 5.3 Self-confirming beliefs
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 è‡ªæˆ‘ç¡®è®¤ä¿¡å¿µ
- en: '**Learning is believing.** A model **a** of a process *F* expresses a *belief*
    held by the learner ğ’° about *F*. The learner updates the belief as they learn
    more. Learning is belief *updating*.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '**å­¦ä¹ å°±æ˜¯ç›¸ä¿¡ã€‚** ä¸€ä¸ªè¿‡ç¨‹*F*çš„æ¨¡å‹**a**è¡¨ç¤ºå­¦ä¹ è€…ğ’°å¯¹*F*çš„*ä¿¡å¿µ*ã€‚å­¦ä¹ è€…åœ¨å­¦ä¹ æ›´å¤šçš„è¿‡ç¨‹ä¸­æ›´æ–°è¿™ä¸ªä¿¡å¿µã€‚å­¦ä¹ å°±æ˜¯ä¿¡å¿µçš„*æ›´æ–°*ã€‚'
- en: '**Beliefs impact their own truth values.** Our beliefs have impacts on what
    we do, and what we do changes some aspects of reality: we change the world by
    moving things around. Since the reality determines whether our beliefs are true
    or false, and our beliefs, through our actions, change some aspects of reality,
    it follows that our beliefs may change their own truth values. Accusing an honest
    person of being a criminal may drive them into crime. Entrusting a poor but honest
    person with a lot of money may transform them into a rich and dishonest person.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¿¡å¿µå½±å“å®ƒä»¬è‡ªå·±çš„çœŸå€¼ã€‚** æˆ‘ä»¬çš„ä¿¡å¿µå¯¹æˆ‘ä»¬çš„è¡Œä¸ºæœ‰å½±å“ï¼Œè€Œæˆ‘ä»¬çš„è¡Œä¸ºåˆä¼šæ”¹å˜ç°å®çš„æŸäº›æ–¹é¢ï¼šæˆ‘ä»¬é€šè¿‡ç§»åŠ¨ç‰©ä½“æ¥æ”¹å˜ä¸–ç•Œã€‚æ—¢ç„¶ç°å®å†³å®šäº†æˆ‘ä»¬çš„ä¿¡å¿µæ˜¯çœŸè¿˜æ˜¯å‡ï¼Œè€Œæˆ‘ä»¬çš„ä¿¡å¿µé€šè¿‡æˆ‘ä»¬çš„è¡ŒåŠ¨æ”¹å˜ç°å®çš„æŸäº›æ–¹é¢ï¼Œé‚£ä¹ˆæˆ‘ä»¬çš„ä¿¡å¿µå¯èƒ½ä¼šæ”¹å˜å®ƒä»¬è‡ªå·±çš„çœŸå€¼ã€‚æŒ‡æ§ä¸€ä¸ªè¯šå®çš„äººæ˜¯ç½ªçŠ¯ï¼Œå¯èƒ½ä¼šæ¨åŠ¨ä»–èµ°ä¸ŠçŠ¯ç½ªé“è·¯ã€‚å°†å¤§é‡é‡‘é’±äº¤ç»™ä¸€ä¸ªè´«ç©·ä½†è¯šå®çš„äººï¼Œå¯èƒ½ä¼šå°†ä»–å˜æˆä¸€ä¸ªå¯Œæœ‰ä¸”ä¸è¯šå®çš„äººã€‚'
- en: '**Making self-confirming predictions.** If *B*ob uses a learning machine ğ’°riel
    to decide what to do, then ğ’°riel can learn a model **b** that will always move
    *B*ob to behave as predicted by **b**. If *B*ob shares ğ’°rielâ€™s beliefs, then those
    beliefs will be confirmed by *B*obâ€™s actions.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '**åšå‡ºè‡ªæˆ‘ç¡®è®¤çš„é¢„æµ‹ã€‚** å¦‚æœ*B*obä½¿ç”¨å­¦ä¹ æœºğ’°rielæ¥å†³å®šåšä»€ä¹ˆï¼Œé‚£ä¹ˆğ’°rielå¯ä»¥å­¦ä¹ ä¸€ä¸ªæ¨¡å‹**b**ï¼Œä½¿å¾—*B*obçš„è¡Œä¸ºæ€»æ˜¯æŒ‰ç…§**b**é¢„æµ‹çš„æ–¹å¼è¿›è¡Œã€‚å¦‚æœ*B*obä¸ğ’°rielçš„ä¿¡å¿µä¸€è‡´ï¼Œé‚£ä¹ˆè¿™äº›ä¿¡å¿µå°†é€šè¿‡*B*obçš„è¡Œä¸ºå¾—åˆ°ç¡®è®¤ã€‚'
- en: To spell out the learning process that ğ’°riel can use to construct the self-confirming
    belief **b**, suppose that *B*obâ€™s behavior is expressed through a channel *B.*
    The assumption that *B*ob uses ğ’°riel to decide what to do can be formalized by
    taking the outputs of the channel to be in the form
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†é˜æ˜ğ’°rielå¯ä»¥ç”¨æ¥æ„å»ºè‡ªæˆ‘ç¡®è®¤ä¿¡å¿µ**b**çš„å­¦ä¹ è¿‡ç¨‹ï¼Œå‡è®¾*B*obçš„è¡Œä¸ºé€šè¿‡é€šé“*B*æ¥è¡¨è¾¾ã€‚å‡è®¾*B*obä½¿ç”¨ğ’°rielæ¥å†³å®šåšä»€ä¹ˆï¼Œå¯ä»¥é€šè¿‡å°†é€šé“çš„è¾“å‡ºå½¢å¼åŒ–æ¥å®ç°ï¼š
- en: '![](../Images/bd48e3aa95901ffed3e7df82c3e01bab.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd48e3aa95901ffed3e7df82c3e01bab.png)'
- en: 'meaning that *B*ob consults ğ’°riel and believes that the model **a**(X) explains
    the input *X*. The claim is that ğ’°riel can then find a model **b**(*X*) that will
    cause *B*ob to act as **b**(*X*) predicts:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€*B*obå’¨è¯¢ğ’°rielå¹¶ä¸”ç›¸ä¿¡æ¨¡å‹**a**(X)è§£é‡Šäº†è¾“å…¥*X*ã€‚å£°æ˜æ˜¯ï¼Œğ’°rielå¯ä»¥æ‰¾åˆ°ä¸€ä¸ªæ¨¡å‹**b**(*X*)ï¼Œä½¿å¾—*B*obæŒ‰ç…§**b**(*X*)çš„é¢„æµ‹è¡Œä¸ºï¼š
- en: '![](../Images/e7cd0a0ce2813d07f9ad96161ecbd171.png)![](../Images/f35db90a77e7d7de1097cab618383916.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7cd0a0ce2813d07f9ad96161ecbd171.png)![](../Images/f35db90a77e7d7de1097cab618383916.png)'
- en: 'To learn **b**(X), ğ’°riel first learns a model ğ›½ of *B* instantiated to ğ’°rielâ€™s
    model of self:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å­¦ä¹ **b**(X)ï¼Œğ’°rielé¦–å…ˆå­¦ä¹ ä¸€ä¸ªæ¨¡å‹ğ›½ï¼Œå®ƒæ˜¯*B*é’ˆå¯¹ğ’°rielè‡ªæˆ‘æ¨¡å‹çš„å®ä¾‹åŒ–ï¼š
- en: '![](../Images/ad3792fc40ae36902ef4246d9e62a3ea.png)![](../Images/73689071181dce69471feb1a076f57f8.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad3792fc40ae36902ef4246d9e62a3ea.png)![](../Images/73689071181dce69471feb1a076f57f8.png)'
- en: Processing the ğ›½-explanation of the (2n+1)-th input as the (2n+2)-th input,
    the definition of ğ›½ yields
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: å°†ğ›½è§£é‡Šå¤„ç†ä¸º(2n+1)-thè¾“å…¥ä½œä¸º(2n+2)-thè¾“å…¥ï¼Œğ›½çš„å®šä¹‰ç»™å‡º
- en: '![](../Images/94838ff55f41829384def431313e8864.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94838ff55f41829384def431313e8864.png)'
- en: 'The claimed self-confirming model is now defined:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€å£°æ˜çš„è‡ªæˆ‘ç¡®è®¤æ¨¡å‹ç°åœ¨è¢«å®šä¹‰ä¸ºï¼š
- en: '![](../Images/ae43a9b27cac920658fadc15cd938652.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae43a9b27cac920658fadc15cd938652.png)'
- en: It satisfies the [claim](#3df3) because
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒæ»¡è¶³[å£°æ˜](#3df3)ï¼Œå› ä¸º
- en: '![](../Images/942cf993cef33dbe6ac1ce71918970d5.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/942cf993cef33dbe6ac1ce71918970d5.png)'
- en: '**From learnable programs to unfalsifiable theories and self-fulfilling prophecies.**
    The insight that learning is like programming opens up a wide range of program
    fixpoint constructions. Applied on learning, such constructions produce models
    that steer their own truth, whether into self-confirmations or paradoxes, along
    the lines of logical completeness or incompleteness proofs. The above construction
    is one of the simplest examples from that range[â¹](#ed33). They prepare models
    and theories that absorb all future evidence, explain away counterexamples, and
    confirm predictions.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»å¯å­¦ä¹ çš„ç¨‹åºåˆ°æ— æ³•è¯ä¼ªçš„ç†è®ºå’Œè‡ªæˆ‘å®ç°çš„é¢„è¨€ã€‚** è®¤çŸ¥å­¦ä¹ å°±åƒç¼–ç¨‹çš„æ´å¯ŸåŠ›ï¼Œå¼€å¯äº†å¹¿æ³›çš„ç¨‹åºä¸åŠ¨ç‚¹æ„é€ ã€‚åœ¨å­¦ä¹ çš„åº”ç”¨ä¸‹ï¼Œè¿™äº›æ„é€ äº§ç”Ÿçš„æ¨¡å‹å¼•å¯¼ç€å®ƒä»¬è‡ªèº«çš„çœŸç†ï¼Œæ— è®ºæ˜¯èµ°å‘è‡ªæˆ‘ç¡®è®¤è¿˜æ˜¯æ‚–è®ºï¼Œéµå¾ªé€»è¾‘å®Œå¤‡æ€§æˆ–ä¸å®Œå¤‡æ€§çš„è¯æ˜ã€‚ä¸Šè¿°æ„é€ å°±æ˜¯è¯¥èŒƒå›´å†…æœ€ç®€å•çš„ä¾‹å­ä¹‹ä¸€[â¹](#ed33)ã€‚å®ƒä»¬å‡†å¤‡äº†å¸æ”¶æ‰€æœ‰æœªæ¥è¯æ®ã€è§£é‡Šåä¾‹å¹¶ç¡®è®¤é¢„æµ‹çš„æ¨¡å‹å’Œç†è®ºã€‚'
- en: '![](../Images/019ecfb85ebe1c40e697024c75c57cbf.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/019ecfb85ebe1c40e697024c75c57cbf.png)'
- en: Attributions
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å½’å±
- en: The color tableaus were authored by DALL-E, prompted by Dusk-o. The hand-drawn
    diagrams and icons were authored by Dusk-o, prompted in some cases by DALL-E.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›è‰²å½©å›¾æ¡ˆç”±DALL-Eåˆ›ä½œï¼Œç”±Dusk-oæå‡ºæç¤ºã€‚æ‰‹ç»˜çš„å›¾è¡¨å’Œå›¾æ ‡ç”±Dusk-oåˆ›ä½œï¼Œæœ‰æ—¶ç”±DALL-Eæå‡ºæç¤ºã€‚
- en: The results presented in this note originate from many publications which would
    normally be listed in a bibliography. But while the bibliographic formats were
    standardized in the pre-web era, the subject of this text is post-web. In the
    meantime, during the web era, we got used to finding all references on the web.
    The students who used this lecture note were asked to find the relevant references
    using the keywords in the text. They needed additional information in a handful
    of places. I added more keywords and notes in these places. If proper references
    turn out to be needed, or if the reference system gets updated for actual use,
    a bibliography will be added.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡æ‰€å‘ˆç°çš„ç»“æœæ¥æºäºå¤šç¯‡æ–‡çŒ®ï¼Œé€šå¸¸ä¼šåœ¨ä¹¦ç›®ä¸­åˆ—å‡ºã€‚ä½†åœ¨äº’è”ç½‘æ—¶ä»£ä¹‹å‰ï¼Œä¹¦ç›®æ ¼å¼å·²è¢«æ ‡å‡†åŒ–ï¼Œè€Œæœ¬æ–‡çš„ä¸»é¢˜åˆ™å±äºåäº’è”ç½‘æ—¶ä»£ã€‚åœ¨æ­¤æœŸé—´ï¼Œæˆ‘ä»¬å·²ä¹ æƒ¯åœ¨ç½‘ç»œä¸Šæ‰¾åˆ°æ‰€æœ‰çš„å‚è€ƒèµ„æ–™ã€‚ä½¿ç”¨è¿™ä»½è®²ä¹‰çš„å­¦ç”Ÿè¢«è¦æ±‚é€šè¿‡æ–‡ä¸­çš„å…³é”®è¯æŸ¥æ‰¾ç›¸å…³å‚è€ƒèµ„æ–™ã€‚åœ¨ä¸€äº›åœ°æ–¹ï¼Œä»–ä»¬éœ€è¦æ›´å¤šçš„ä¿¡æ¯ã€‚æˆ‘åœ¨è¿™äº›åœ°æ–¹æ·»åŠ äº†æ›´å¤šçš„å…³é”®è¯å’Œæ³¨é‡Šã€‚å¦‚æœä»¥åéœ€è¦æ­£å¼çš„å‚è€ƒæ–‡çŒ®ï¼Œæˆ–è€…å‚è€ƒç³»ç»Ÿéœ€è¦æ›´æ–°ä»¥ä¾¿å®é™…ä½¿ç”¨ï¼Œä¹¦ç›®å°†è¢«æ·»åŠ ã€‚
- en: Notes
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ³¨é‡Š
- en: Â¹Alan Turing explained that machine intelligence could not be achieved through
    intelligent designs, because intelligence itself could not be completely specified,
    as it is its nature to always seek and find new paths. But Turing was also the
    first to realize that the process of computation was not bound by designs and
    specifications either, but could evolve and innovate. He anticipated that machine
    intelligence would evolve with computation. However, three years after Turingâ€™s
    death, the concept of *machine intelligence*, which he thought and wrote about
    for the last 8 years of his life, got renamed to *artificial intelligence,* his
    writings sank into oblivion, and the logical systems designed to capture intelligence
    proliferated.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: Â¹è‰¾ä¼¦Â·å›¾çµè§£é‡Šè¯´ï¼Œæœºå™¨æ™ºèƒ½æ— æ³•é€šè¿‡æ™ºèƒ½è®¾è®¡å®ç°ï¼Œå› ä¸ºæ™ºèƒ½æœ¬èº«æ— æ³•å®Œå…¨å®šä¹‰ï¼Œå› ä¸ºå®ƒçš„æœ¬è´¨æ˜¯ä¸æ–­å¯»æ‰¾å’Œå‘ç°æ–°è·¯å¾„ã€‚ä½†å›¾çµä¹Ÿæ˜¯ç¬¬ä¸€ä¸ªæ„è¯†åˆ°è®¡ç®—è¿‡ç¨‹å¹¶ä¸å—è®¾è®¡å’Œè§„èŒƒçš„æŸç¼šï¼Œè€Œæ˜¯å¯ä»¥å‘å±•å’Œåˆ›æ–°çš„äººã€‚ä»–é¢„è§åˆ°æœºå™¨æ™ºèƒ½å°†éšç€è®¡ç®—çš„å‘å±•è€Œè¿›åŒ–ã€‚ç„¶è€Œï¼Œåœ¨å›¾çµå»ä¸–ä¸‰å¹´åï¼Œä»–åœ¨ç”Ÿå‘½çš„æœ€åå…«å¹´é‡Œæ€è€ƒå’Œå†™ä½œçš„*æœºå™¨æ™ºèƒ½*æ¦‚å¿µè¢«é‡æ–°å‘½åä¸º*äººå·¥æ™ºèƒ½*ï¼Œä»–çš„è‘—ä½œä¹Ÿè¢«é—å¿˜ï¼Œè€Œæ—¨åœ¨æ•æ‰æ™ºèƒ½çš„é€»è¾‘ç³»ç»Ÿåˆ™å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚
- en: Â²Skinnerâ€™s explorations of our intellectual kinship with pigeons have interesting
    interpretations in the context of arguments that the concept of causality as such
    is in essence unfounded. From different directions, such arguments have been developed
    by Hume, Russell, Bohr, and many other scientists and philosophers.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: Â²æ–¯é‡‘çº³å¯¹æˆ‘ä»¬ä¸é¸½å­åœ¨æ™ºåŠ›ä¸Šçš„äº²ç¼˜å…³ç³»çš„æ¢ç´¢ï¼Œåœ¨è®¨è®ºå› æœæ€§æ¦‚å¿µæœ¬èº«åœ¨æœ¬è´¨ä¸Šæ˜¯æ²¡æœ‰æ ¹æ®çš„è¿™ä¸€è®ºç‚¹æ—¶ï¼Œå…·æœ‰æœ‰è¶£çš„è§£é‡Šã€‚ä»ä¸åŒçš„è§’åº¦ï¼Œä¼‘è°Ÿã€ç½—ç´ ã€æ³¢å°”ä»¥åŠå…¶ä»–è®¸å¤šç§‘å­¦å®¶å’Œå“²å­¦å®¶éƒ½æå‡ºäº†è¿™æ ·çš„è®ºç‚¹ã€‚
- en: Â³Our [paper on botsâ€™ religions](https://arxiv.org/abs/2303.14338) also points
    in this direction.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: Â³æˆ‘ä»¬çš„[å…³äºæœºå™¨äººçš„å®—æ•™è®ºæ–‡](https://arxiv.org/abs/2303.14338)ä¹ŸæŒ‡å‘äº†è¿™ä¸€æ–¹å‘ã€‚
- en: â´Frank Rosenblatt. *Principles of neurodynamics; perceptrons and the theory
    of brain mechanisms*, volume 55\. Spartan Books, Washington, D.C., 1962.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: â´å¼—å…°å…‹Â·ç½—æ£®å¸ƒæ‹‰ç‰¹ã€‚*ç¥ç»åŠ¨åŠ›å­¦åŸç†ï¼›æ„ŸçŸ¥å™¨ä¸å¤§è„‘æœºåˆ¶ç†è®º*ï¼Œç¬¬55å·ã€‚æ–¯å·´è¾¾å›¾ä¹¦å‡ºç‰ˆç¤¾ï¼Œåç››é¡¿ç‰¹åŒºï¼Œ1962å¹´ã€‚
- en: âµRecall that âŸ¨*w*| is a convenient notation (attributed to Paul Dirac) for the
    row vector *(w*1 *w*2 Â·Â·Â·*wd )*, whereas |*w*âŸ© is the corresponding column vector.
    Viewed as a linear operator, the row vector âŸ¨*w* | denotes the projection on the
    column vector |*w*âŸ©.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: âµå›æƒ³ä¸€ä¸‹ï¼ŒâŸ¨*w*|æ˜¯ä¸€ä¸ªæ–¹ä¾¿çš„ç¬¦å·ï¼ˆå½’åŠŸäºä¿ç½—Â·ç‹„æ‹‰å…‹ï¼‰ï¼Œè¡¨ç¤ºè¡Œå‘é‡*(w*1 *w*2 Â·Â·Â·*wd )*ï¼Œè€Œ|*w*âŸ©æ˜¯ç›¸åº”çš„åˆ—å‘é‡ã€‚ä»çº¿æ€§ç®—å­çš„è§’åº¦æ¥çœ‹ï¼Œè¡Œå‘é‡âŸ¨*w*
    |è¡¨ç¤ºå¯¹åˆ—å‘é‡|*w*âŸ©çš„æŠ•å½±ã€‚
- en: â¶Since this note was written, a [paper](https://arxiv.org/abs/2404.19756) proposing
    a new family of neural networks, called the *Kolmogorov-Arnold Networks (KAN)*
    appeared on arxiv. The idea is very natural, as even our derivation of [neural
    approximation](#5646) from [continuous decomposition](#2f11) confirms. Remarkably,
    though, the proposers of the KAN approach make no use of the substantial mathematical
    and computational simplifications and improvements of Kolmogorovâ€™s 1957 construction,
    although they cite some papers with fairly complete reference lists. Since they
    seem to be actively updating the posted reports about their work, the missed opportunities
    for improvement will presumably be taken in the future versions.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: â¶è‡ªè¿™ç¯‡ç¬”è®°å†™æˆä»¥æ¥ï¼Œä¸€ç¯‡å…³äºæå‡ºä¸€ç§æ–°å‹ç¥ç»ç½‘ç»œå®¶æ—çš„[è®ºæ–‡](https://arxiv.org/abs/2404.19756)åœ¨arxivä¸Šå‘è¡¨ï¼Œè¿™ç§ç¥ç»ç½‘ç»œè¢«ç§°ä¸º*Kolmogorov-Arnold
    Networks (KAN)*ã€‚è¿™ä¸€æƒ³æ³•éå¸¸è‡ªç„¶ï¼Œç”šè‡³æˆ‘ä»¬ä»[è¿ç»­åˆ†è§£](#2f11)æ¨å¯¼å‡ºçš„[ç¥ç»è¿‘ä¼¼](#5646)ä¹Ÿè¯å®äº†è¿™ä¸€ç‚¹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒKANæ–¹æ³•çš„æè®®è€…å¹¶æ²¡æœ‰åˆ©ç”¨ç§‘å°”è«å“¥æ´›å¤«1957å¹´æ„é€ çš„æ•°å­¦å’Œè®¡ç®—ç®€åŒ–ä¸æ”¹è¿›ï¼Œå°½ç®¡ä»–ä»¬å¼•ç”¨äº†ä¸€äº›æ–‡çŒ®å¹¶æä¾›äº†ç›¸å¯¹å®Œæ•´çš„å‚è€ƒæ–‡çŒ®åˆ—è¡¨ã€‚ç”±äºä»–ä»¬ä¼¼ä¹åœ¨ç§¯ææ›´æ–°å…³äºè‡ªå·±å·¥ä½œçš„æŠ¥å‘Šï¼Œå› æ­¤è¿™äº›æœªåˆ©ç”¨çš„æ”¹è¿›æœºä¼šé¢„è®¡ä¼šåœ¨æœªæ¥ç‰ˆæœ¬ä¸­å¾—åˆ°é‡‡çº³ã€‚
- en: â·In the old-style matrix notation the Lorentz-Sprecher embedding is
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: â·åœ¨è€å¼çŸ©é˜µè¡¨ç¤ºæ³•ä¸­ï¼Œæ´›ä¼¦èŒ¨-æ–¯æ™®é›·åˆ‡å°”åµŒå…¥æ˜¯
- en: '![](../Images/2b820513ddc9719dfad217e67d74d761.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b820513ddc9719dfad217e67d74d761.png)'
- en: â¸In their seminal, critical book *â€œPerceptronsâ€*, Minsky and Papert proved that
    the coefficients of a perceptron representing a boolean function are always invariant
    under the actions of a group under which the function is invariant itself. Since
    a perceptron therefore cannot tell apart the functions that are equivariant under
    the group actions, this was viewed as a no-go theorem. While the Minsky-Papert
    construction lifts from perceptrons and boolean functions to wide neural networks
    and continuous functions by standard methods, the resulting group invariances
    are nowadays viewed as proofs that the glass of neural approximations is half-full,
    not that it is half-empty.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: â¸åœ¨ä»–ä»¬çš„å¼€åˆ›æ€§ã€æ‰¹åˆ¤æ€§è‘—ä½œ*ã€Šæ„ŸçŸ¥æœºã€‹*ä¸­ï¼Œæ˜æ–¯åŸºå’Œå¸•ç€ç‰¹è¯æ˜äº†è¡¨ç¤ºå¸ƒå°”å‡½æ•°çš„æ„ŸçŸ¥æœºçš„ç³»æ•°åœ¨å‡½æ•°æœ¬èº«ä¸å˜çš„ç¾¤ä½œç”¨ä¸‹å§‹ç»ˆæ˜¯ä¸å˜çš„ã€‚å› æ­¤ï¼Œæ„ŸçŸ¥æœºæ— æ³•åŒºåˆ†åœ¨è¯¥ç¾¤ä½œç”¨ä¸‹ç­‰å˜çš„å‡½æ•°ï¼Œè¿™è¢«è§†ä¸ºä¸€ä¸ªç¦å¿Œå®šç†ã€‚è™½ç„¶æ˜æ–¯åŸº-å¸•ç€ç‰¹æ„é€ é€šè¿‡æ ‡å‡†æ–¹æ³•å°†æ„ŸçŸ¥æœºå’Œå¸ƒå°”å‡½æ•°æå‡åˆ°å¹¿æ³›çš„ç¥ç»ç½‘ç»œå’Œè¿ç»­å‡½æ•°ï¼Œä½†ç”±æ­¤å¾—åˆ°çš„ç¾¤ä¸å˜æ€§å¦‚ä»Šè¢«è§†ä¸ºè¯æ˜ç¥ç»è¿‘ä¼¼çš„â€œæ¯å­æ˜¯åŠæ»¡çš„â€ï¼Œè€Œä¸æ˜¯â€œæ¯å­æ˜¯åŠç©ºçš„â€ã€‚
- en: â¹The constructions and discussions presented in this section are based on the
    paper [â€œ*From GÃ¶delâ€™s Incompleteness Theorem to the completeness of bot beliefs*â€](https://arxiv.org/abs/2303.14338).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: â¹æœ¬èŠ‚ä¸­æå‡ºçš„æ„é€ å’Œè®¨è®ºåŸºäºè®ºæ–‡[â€œ*ä»å“¥å¾·å°”ä¸å®Œå…¨æ€§å®šç†åˆ°æœºå™¨äººä¿¡ä»°çš„å®Œå¤‡æ€§*â€](https://arxiv.org/abs/2303.14338)ã€‚
- en: Â¹â°Shakespeareâ€™s tragedy of Macbeth is built on a self-fulfilling prophecy. At
    the beginning, the witches predict that Macbeth will become King. To fulfill the
    inevitable, Macbeth kills the King. Even a completely rational Macbeth is forced
    to fulfill the prophecy, or risk that the King will hear of it and kill him to
    prevent it from being fulfilled. An example of a self-fulfilling prophecy from
    current life arises from the task of launching a social networking service. This
    service is only valuable to its users if their friends are also using it. To get
    its first users, the social network must convince them that it already has many
    users, enough to include their friends. Initially, this must be a lie. But if
    many people believe this lie, they will join the network, the network will get
    many users, and the lie will stop being a lie. Examples of adaptive theories include
    the religions that attribute any evidence contrary to their claims to demons or
    to faith testing and temptations.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: Â¹â°èå£«æ¯”äºšçš„æ‚²å‰§ã€Šéº¦å…‹ç™½ã€‹å»ºç«‹åœ¨ä¸€ä¸ªè‡ªæˆ‘å®ç°çš„é¢„è¨€ä¸Šã€‚èµ·åˆï¼Œå¥³å·«é¢„è¨€éº¦å…‹ç™½å°†æˆä¸ºå›½ç‹ã€‚ä¸ºäº†å®ç°è¿™ä¸ªä¸å¯é¿å…çš„å‘½è¿ï¼Œéº¦å…‹ç™½æ€æ­»äº†å›½ç‹ã€‚å³ä½¿æ˜¯å®Œå…¨ç†æ€§çš„éº¦å…‹ç™½ä¹Ÿè¢«è¿«å®ç°è¿™ä¸€é¢„è¨€ï¼Œå¦åˆ™ä»–å°±æœ‰å¯èƒ½è¢«å›½ç‹å‘ç°å¹¶è¢«æ€å®³ï¼Œä»¥é˜»æ­¢é¢„è¨€æˆçœŸã€‚ä»ç°å®ç”Ÿæ´»ä¸­æ¥çœ‹ï¼Œä¸€ä¸ªè‡ªæˆ‘å®ç°çš„é¢„è¨€çš„ä¾‹å­å¯ä»¥ä»å¯åŠ¨ç¤¾äº¤ç½‘ç»œæœåŠ¡çš„ä»»åŠ¡ä¸­æ‰¾åˆ°ã€‚è¿™ä¸ªæœåŠ¡å¯¹å…¶ç”¨æˆ·æ¥è¯´åªæœ‰åœ¨ä»–ä»¬çš„æœ‹å‹ä¹Ÿåœ¨ä½¿ç”¨å®ƒæ—¶æ‰æœ‰ä»·å€¼ã€‚ä¸ºäº†è·å¾—é¦–æ‰¹ç”¨æˆ·ï¼Œç¤¾äº¤ç½‘ç»œå¿…é¡»è¯´æœä»–ä»¬ç›¸ä¿¡å®ƒå·²ç»æœ‰äº†å¾ˆå¤šç”¨æˆ·ï¼Œè¶³å¤Ÿå¤šä»¥è‡³äºåŒ…æ‹¬ä»–ä»¬çš„æœ‹å‹ã€‚æœ€åˆï¼Œè¿™å¿…é¡»æ˜¯ä¸ªè°è¨€ã€‚ä½†å¦‚æœå¾ˆå¤šäººç›¸ä¿¡è¿™ä¸ªè°è¨€ï¼Œä»–ä»¬å°±ä¼šåŠ å…¥ç½‘ç»œï¼Œç½‘ç»œå°†è·å¾—å¤§é‡ç”¨æˆ·ï¼Œè€Œè¿™ä¸ªè°è¨€å°±ä¸å†æ˜¯è°è¨€ã€‚é€‚åº”æ€§ç†è®ºçš„ä¾‹å­åŒ…æ‹¬é‚£äº›å°†ä¸å…¶ä¸»å¼ ç›¸æ‚–çš„è¯æ®å½’å› äºæ¶é­”æˆ–ä¿¡ä»°è€ƒéªŒå’Œè¯±æƒ‘çš„å®—æ•™ã€‚
