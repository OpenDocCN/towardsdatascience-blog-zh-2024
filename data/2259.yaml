- en: Teaching Your Model to Learn from Itself
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 教授你的模型从自身学习
- en: 原文：[https://towardsdatascience.com/teaching-your-model-to-learn-from-itself-8b5ef13eb173?source=collection_archive---------1-----------------------#2024-09-16](https://towardsdatascience.com/teaching-your-model-to-learn-from-itself-8b5ef13eb173?source=collection_archive---------1-----------------------#2024-09-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/teaching-your-model-to-learn-from-itself-8b5ef13eb173?source=collection_archive---------1-----------------------#2024-09-16](https://towardsdatascience.com/teaching-your-model-to-learn-from-itself-8b5ef13eb173?source=collection_archive---------1-----------------------#2024-09-16)
- en: A case study on iterative, confidence-based pseudo-labeling for classification
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于迭代和置信度的伪标签分类案例研究
- en: '[](https://medium.com/@niklasvmoers?source=post_page---byline--8b5ef13eb173--------------------------------)[![Niklas
    von Moers](../Images/7029224bf0efb9392310307791bfd568.png)](https://medium.com/@niklasvmoers?source=post_page---byline--8b5ef13eb173--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8b5ef13eb173--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8b5ef13eb173--------------------------------)
    [Niklas von Moers](https://medium.com/@niklasvmoers?source=post_page---byline--8b5ef13eb173--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@niklasvmoers?source=post_page---byline--8b5ef13eb173--------------------------------)[![Niklas
    von Moers](../Images/7029224bf0efb9392310307791bfd568.png)](https://medium.com/@niklasvmoers?source=post_page---byline--8b5ef13eb173--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8b5ef13eb173--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8b5ef13eb173--------------------------------)
    [Niklas von Moers](https://medium.com/@niklasvmoers?source=post_page---byline--8b5ef13eb173--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8b5ef13eb173--------------------------------)
    ·6 min read·Sep 16, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8b5ef13eb173--------------------------------)
    ·6分钟阅读·2024年9月16日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: In machine learning, more data leads to better results. But labeling data can
    be expensive and time-consuming. What if we could use the huge amounts of unlabeled
    data that’s usually easy to get? This is where pseudo-labeling comes in handy.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，更多的数据通常会带来更好的结果。但标记数据可能非常昂贵且耗时。如果我们能够利用通常很容易获得的大量未标记数据呢？这就是伪标签方法的用武之地。
- en: 'TL;DR: I conducted a case study on the MNIST dataset and boosted my model’s
    accuracy from 90 % to 95 % by applying iterative, confidence-based pseudo-labeling.
    This article covers the details of what pseudo-labeling is, along with practical
    tips and insights from my experiments.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: TL;DR：我在MNIST数据集上进行了案例研究，并通过应用迭代的、基于置信度的伪标签方法，将模型的准确率从90%提高到了95%。本文涵盖了伪标签的详细内容，以及我实验中的实用技巧和洞察。
- en: '**How Does it Work?**'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**它是如何工作的？**'
- en: Pseudo-labeling is a type of semi-supervised learning. It bridges the gap between
    supervised learning (where all data is labeled) and unsupervised learning (where
    no data is labeled).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 伪标签是半监督学习的一种方式。它弥合了监督学习（所有数据都有标签）和无监督学习（没有标签数据）之间的差距。
- en: '![](../Images/6daf2b07ddb532f3f4dbe1575ddeab4b.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6daf2b07ddb532f3f4dbe1575ddeab4b.png)'
- en: Process diagram illustrating the procedure on the MNIST dataset. Derived from
    Yann LeCun, Corinna Cortes, and Christopher J.C. Burges. Licensed under CC BY-SA
    3.0.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 过程示意图，展示了在MNIST数据集上执行的步骤。来源：Yann LeCun、Corinna Cortes 和 Christopher J.C. Burges。根据CC
    BY-SA 3.0许可授权。
- en: 'The exact procedure I followed goes as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我遵循的具体步骤如下：
- en: We start with a small amount of labeled data and train our model on it.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们从少量标记数据开始，并在其上训练模型。
- en: The model makes predictions on the unlabeled data.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型对未标记的数据进行预测。
- en: We pick the predictions the model is most confident about (e.g., above 95 %
    confidence) and treat them *as if they were actual labels*, hoping that they are
    reliable enough.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们选择模型最有信心的预测（例如，置信度超过95%），并将其*视为真实标签*，希望这些预测足够可靠。
- en: We add this “pseudo-labeled” data to our training set and retrain the model.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将这些“伪标签”数据添加到我们的训练集，并重新训练模型。
- en: We can repeat this process several times, letting the model learn from the growing
    pool of pseudo-labeled data.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以多次重复这个过程，让模型从不断增长的伪标签数据池中学习。
- en: While this approach may introduce some incorrect labels, the benefit comes from
    the significantly increased amount of training data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种方法可能会引入一些错误标签，但它的好处在于大幅增加了训练数据的数量。
- en: '**The Echo Chamber Effect: Can Pseudo-Labeling Even Work?**'
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**回声室效应：伪标签方法能有效吗？**'
- en: The idea of a model learning from its own predictions might raise some eyebrows.
    After all, aren’t we trying to create something from nothing, relying on an “echo
    chamber” where the model simply reinforces its own initial biases and errors?
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 模型从自己预测中学习的想法可能会引起一些质疑。毕竟，我们不是在尝试从无到有，而是在依赖一个“回音室”，模型只是不断强化自己最初的偏见和错误，不是吗？
- en: This concern is valid. It may remind you of the legendary Baron Münchhausen,
    who famously claimed to have pulled himself and his horse out of a swamp by his
    own hair — a physical impossibility. Similarly, if a model solely relies on its
    own potentially flawed predictions, it risks getting stuck in a loop of self-reinforcement,
    much like people trapped in echo chambers who only hear their own beliefs reflected
    back at them.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个担忧是合理的。这可能让你想起传奇人物明茨豪森男爵，他曾声称自己通过自己的头发把自己和他的马从沼泽中拉了出来——这是物理上不可能的。类似地，如果一个模型完全依赖于自己可能存在缺陷的预测，它就有可能陷入自我强化的循环，就像那些被困在回音室中的人们，只听到自己信仰的回响。
- en: So, can pseudo-labeling truly be effective without falling into this trap?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，伪标签化真的可以有效避免陷入这个陷阱吗？
- en: 'The answer is **yes**. While this story of Baron Münchhausen is obviously a
    fairytale, you may imagine a blacksmith progressing through the ages. He starts
    with basic stone tools (the initial labeled data). Using these, he forges crude
    copper tools (pseudo-labels) from raw ore (unlabeled data). These copper tools,
    while still rudimentary, allow him to work on **previously unfeasible** tasks,
    eventually leading to the creation of tools that are made of bronze, iron, and
    so on. This iterative process is crucial: *You cannot forge steel swords using
    a stone hammer.*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是**肯定的**。虽然明茨豪森男爵的故事显然是一个童话故事，但你可以想象一位铁匠随着时代的进步。他从基本的石器工具（最初的标注数据）开始，利用这些工具从原矿（无标签数据）中锻造出粗糙的铜器工具（伪标签）。这些铜器工具虽然仍然很粗糙，但使他能够进行**之前不可行**的任务，最终创造出青铜、铁等材料制成的工具。这个迭代过程至关重要：*你不能仅用石锤锻造钢剑。*
- en: 'Just like the blacksmith, in machine learning, we can achieve a similar progression
    by:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 就像铁匠一样，在机器学习中，我们可以通过以下方式实现类似的进展：
- en: '**Rigorous thresholds**: The model’s out-of-sample accuracy is bounded by the
    share of correct training labels. If 10 % of labels are wrong, the model’s accuracy
    won’t exceed 90 % significantly. Therefore it is important to allow as few wrong
    labels as possible.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**严格的阈值**：模型的样本外准确率受到正确训练标签比例的限制。如果10%的标签是错误的，模型的准确率不会显著超过90%。因此，尽可能减少错误标签的比例非常重要。'
- en: '**Measurable feedback**: Constantly evaluating the model’s performance on a
    separate test set acts as a reality check, ensuring we’re making actual progress,
    not just reinforcing existing errors.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可衡量的反馈**：不断在一个单独的测试集上评估模型的表现，充当现实检验，确保我们在取得实际进展，而不仅仅是在强化已有的错误。'
- en: '**Human-in-the-loop**: Incorporating human feedback in the form of manual review
    of pseudo-labels or manual labeling of low-confidence data can provide valuable
    course correction.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人机协作**：通过人工审查伪标签或人工标注低置信度数据的反馈，可以为调整方向提供宝贵的纠正。'
- en: Pseudo-labeling, when done right, can be a powerful tool to make the most of
    small labeled datasets, as we will see in the following case study.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当伪标签化正确执行时，它可以成为最大化利用小型标注数据集的强大工具，正如我们在接下来的案例研究中将看到的那样。
- en: '**Case Study: MNIST Dataset**'
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**案例研究：MNIST数据集**'
- en: I conducted my experiments on the MNIST dataset, a classic collection of 28
    by 28 pixel images of handwritten digits, widely used for benchmarking machine
    learning models. It consists of 60,000 training images and 10,000 test images.
    The goal is to, based on the 28 by 28 pixels, predict what digit is written.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我在MNIST数据集上进行了实验，MNIST是一个经典的28×28像素手写数字图像集合，广泛用于机器学习模型的基准测试。它包含60,000张训练图像和10,000张测试图像。目标是根据28×28像素的图像，预测所写的数字是什么。
- en: I trained a simple CNN on an initial set of 1,000 labeled images, leaving 59,000
    unlabeled. I then used the trained model to predict the labels for the unlabeled
    images. Predictions with confidence above a certain threshold (e.g., 95 %) were
    added to the training set, along with their predicted labels. The model was then
    retrained on this expanded dataset. This process was repeated iteratively, up
    to ten times or until there was no more unlabeled data.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我用1,000张带标签的图像训练了一个简单的CNN模型，剩下的59,000张图像没有标签。然后，我用训练好的模型预测无标签图像的标签。对于置信度高于某个阈值（例如95%）的预测结果，将其添加到训练集中，并标上预测的标签。接着，模型在这个扩展的数据集上重新训练。这个过程反复进行，最多进行十次，或者直到没有更多的无标签数据为止。
- en: This experiment was repeated with different numbers of initially labeled images
    and confidence thresholds.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 此实验使用不同数量的最初标记图像和置信度阈值进行了重复。
- en: '**Results**'
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**结果**'
- en: The following table summarizes the results of my experiments, comparing the
    performance of pseudo-labeling to training on the full labeled dataset.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格总结了我的实验结果，比较了伪标签与在完整标记数据集上训练的表现。
- en: Even with a small initial labeled dataset, pseudo-labeling may produce **remarkable
    results**, increasing the accuracy by 4.87 %pt. for 1,000 initial labeled samples.
    When using only 100 initial samples, this effect is even stronger. However, it
    would’ve been wise to manually label more than 100 samples.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 即使初始标记的数据集较小，伪标签仍然能产生**显著的效果**，对于1,000个初始标记样本，准确率提高了4.87个百分点。当仅使用100个初始样本时，这一效果更为显著。然而，手动标记超过100个样本会更明智。
- en: Interestingly, the final test accuracy of the experiment with 100 initial training
    samples exceeded the share of correct training labels.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，使用100个初始训练样本的实验最终测试准确率超过了正确训练标签的比例。
- en: '![](../Images/d4a4cbc4693970537bde69c9d7f89ca4.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d4a4cbc4693970537bde69c9d7f89ca4.png)'
- en: Accuracy improvement (y-axis) compared to the first iteration per iteration
    (color) by threshold (x-axis). There is a clear trend of better improvements for
    higher thresholds and more iterations. Image by the author.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于第一次迭代，按阈值（x轴）和每次迭代（颜色）计算的准确率提升（y轴）。更高的阈值和更多的迭代呈现出明显的改善趋势。图像由作者提供。
- en: '![](../Images/6e5333817dec9573ecd7c8d114e0bdfd.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6e5333817dec9573ecd7c8d114e0bdfd.png)'
- en: Share of corrrect training labels and number of total training data points per
    iteration by threshold. Higher thresholds lead to more robust but slower labeling.
    Image by the author.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 每次迭代中按阈值划分的正确训练标签的比例和总训练数据点数量。更高的阈值导致更稳健但更慢的标记。图像由作者提供。
- en: '![](../Images/d29c6bdf6c94983a13d687ecb5eccb40.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d29c6bdf6c94983a13d687ecb5eccb40.png)'
- en: Accuracies for high and low confidence predictions per iteration by threshold.
    Higher thresholds lead to better accuracies, but the accuracy decreases with time
    for every choice of threshold. Image by the author.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 每次迭代中按阈值划分的高置信度和低置信度预测的准确率。更高的阈值会导致更好的准确率，但随着时间推移，每个阈值选择的准确率都会下降。图像由作者提供。
- en: '![](../Images/0a15634d4ea5ccafd780fb5816c2aa9f.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a15634d4ea5ccafd780fb5816c2aa9f.png)'
- en: Accuracy improvement per iteration compared to the first iteration by threshold
    for 100 and 10,000 initially labeled training samples (left and right respectively).
    Note the different scales. Image by the author.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 与第一次迭代相比，按阈值对100个和10,000个最初标记的训练样本（分别位于左侧和右侧）计算的每次迭代的准确率提升。注意不同的刻度。图像由作者提供。
- en: Looking at the above graphs, it becomes apparent that, in general, **higher
    thresholds lead to better results** — as long as at least some predictions exceed
    the threshold. In future experiments, one might try to vary the threshold with
    each iteration.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过观察上述图表，可以明显看出，通常情况下，**更高的阈值会导致更好的结果**——只要至少有一些预测超过了阈值。在未来的实验中，可以尝试在每次迭代时变化阈值。
- en: Furthermore, the accuracy improves even in the later iterations, indicating
    that the iterative nature provides a true benefit.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，准确率在后期迭代中仍然有所提高，这表明迭代的性质确实带来了真正的好处。
- en: '**Key Findings and Lessons Learned**'
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**主要发现和经验教训**'
- en: Pseudo-labeling is best applied when **unlabeled data is plentiful but labeling
    is expensive**.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 伪标签最适用于**未标记数据丰富但标记成本高昂**的情况。
- en: 'Monitor the test accuracy: It’s important to keep an eye on the model’s performance
    on a separate test dataset throughout the iterations.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控测试准确率：在整个迭代过程中，重要的是要关注模型在单独的测试数据集上的表现。
- en: '**Manual labeling can still be helpful**: If you have the resources, focus
    on manually labeling the low confidence data. However, humans aren’t perfect either
    and labeling of high confidence data may be delegated to the model in good conscience.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**手动标记仍然有帮助**：如果你有资源，重点手动标记低置信度数据。然而，人类也不是完美的，高置信度数据的标记可以放心交给模型。'
- en: '**Keep track of what labels are AI-generated.** If more manually labeled data
    becomes available later on, you’ll likely want to discard the pseudo-labels and
    repeat this procedure, increasing the pseudo-label accuracy.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跟踪哪些标签是AI生成的。** 如果以后有更多手动标记的数据可用，你可能会希望丢弃伪标签并重新进行此过程，从而提高伪标签的准确性。'
- en: 'Be careful when interpreting the results: When I first did this experiment
    a few years ago, I focused on the accuracy on the remaining unlabeled training
    data. This accuracy *falls* with more iterations! However, this is likely because
    the remaining data is harder to predict — the model was never confident about
    it in previous iterations. I should have focused on the test accuracy, which actually
    improves with more iterations.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释结果时要小心：几年前我第一次做这个实验时，我专注于剩余未标记训练数据的准确率。随着迭代次数的增加，这个准确率*下降*了！然而，这很可能是因为剩余的数据更难预测——在之前的迭代中，模型对这些数据从未有过信心。我应该专注于测试集的准确率，实际上随着迭代次数的增加，它会提高。
- en: '**Links**'
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**链接**'
- en: The repository containing the experiment’s code can be found [here](https://github.com/NiklasvonM/Self-Training).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 包含实验代码的仓库可以在[这里](https://github.com/NiklasvonM/Self-Training)找到。
- en: 'Related paper: [Iterative Pseudo-Labeling with Deep Feature Annotation and
    Confidence-Based Sampling](https://doi.org/10.1109/SIBGRAPI54419.2021.00034)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 相关论文：[使用深度特征注释和基于置信度的采样的迭代伪标签方法](https://doi.org/10.1109/SIBGRAPI54419.2021.00034)
