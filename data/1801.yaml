- en: Monocular Depth Estimation with Depth Anything V2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Depth Anything V2 进行单目深度估计
- en: 原文：[https://towardsdatascience.com/monocular-depth-estimation-with-depth-anything-v2-54b6775abc9f?source=collection_archive---------4-----------------------#2024-07-24](https://towardsdatascience.com/monocular-depth-estimation-with-depth-anything-v2-54b6775abc9f?source=collection_archive---------4-----------------------#2024-07-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/monocular-depth-estimation-with-depth-anything-v2-54b6775abc9f?source=collection_archive---------4-----------------------#2024-07-24](https://towardsdatascience.com/monocular-depth-estimation-with-depth-anything-v2-54b6775abc9f?source=collection_archive---------4-----------------------#2024-07-24)
- en: How do neural networks learn to estimate depth from 2D images?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络是如何从二维图像中学习估计深度的？
- en: '[](https://medium.com/@neural.avb?source=post_page---byline--54b6775abc9f--------------------------------)[![Avishek
    Biswas](../Images/6feb591069f354aa096f6108f1a70ea7.png)](https://medium.com/@neural.avb?source=post_page---byline--54b6775abc9f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--54b6775abc9f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--54b6775abc9f--------------------------------)
    [Avishek Biswas](https://medium.com/@neural.avb?source=post_page---byline--54b6775abc9f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@neural.avb?source=post_page---byline--54b6775abc9f--------------------------------)[![Avishek
    Biswas](../Images/6feb591069f354aa096f6108f1a70ea7.png)](https://medium.com/@neural.avb?source=post_page---byline--54b6775abc9f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--54b6775abc9f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--54b6775abc9f--------------------------------)
    [Avishek Biswas](https://medium.com/@neural.avb?source=post_page---byline--54b6775abc9f--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--54b6775abc9f--------------------------------)
    ·10 min read·Jul 24, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--54b6775abc9f--------------------------------)
    ·阅读时间：10分钟·2024年7月24日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: What is Monocular Depth Estimation?
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是单目深度估计？
- en: '![](../Images/53c47ae75f37e0bef19e2c77b3169255.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/53c47ae75f37e0bef19e2c77b3169255.png)'
- en: The Depth Anything V2 Algorithm (Illustration by Author)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Depth Anything V2 算法（作者插图）
- en: Monocular Depth Estimation (MDE) is the task of training a neural network to
    determine depth information from a single image. This is an exciting and challenging
    area of Machine Learning and Computer Vision because predicting a depth map requires
    the neural network to form a 3-dimensional understanding from just a 2-dimensional
    image.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 单目深度估计（MDE）是训练神经网络从单幅图像中提取深度信息的任务。这是一个激动人心且充满挑战的机器学习与计算机视觉领域，因为预测深度图要求神经网络仅凭二维图像来形成三维的理解。
- en: In this article, we will discuss a new model called *Depth Anything V2* and
    its precursor, *Depth Anything V1*. Depth Anything V2 has outperformed nearly
    all other models in Depth Estimation, showing impressive results on tricky images.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将讨论一个新的模型叫做*Depth Anything V2*及其前身*Depth Anything V1*。Depth Anything
    V2 在深度估计领域超越了几乎所有其他模型，在处理复杂图像时展现了令人印象深刻的结果。
- en: '![](../Images/26476d8feb1529cf50ab27a0b30729f2.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26476d8feb1529cf50ab27a0b30729f2.png)'
- en: 'Depth Anything V2 Demo (Source: Screen recording by the author from Depth Anything
    V2 [DEMO page](https://huggingface.co/spaces/depth-anything/Depth-Anything-V2))'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Depth Anything V2 演示（来源：作者从 Depth Anything V2 [演示页面](https://huggingface.co/spaces/depth-anything/Depth-Anything-V2)录制的屏幕录像）
- en: '**This article is based on a video I made on the same topic.** [**Here is a
    video link for learners who prefer a visual medium.**](https://youtu.be/sz30TDttIBA)
    For those who prefer reading, continue!'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**本文基于我制作的同主题视频。** [**这里是视频链接，适合喜欢视觉媒介的学习者。**](https://youtu.be/sz30TDttIBA)
    对于喜欢阅读的朋友，继续往下看！'
- en: Why should we even care about MDE models?
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么我们要关心 MDE 模型？
- en: Good MDE models have many practical uses, such as [aiding navigation and obstacle
    avoidance for robots](https://natesimon.github.io/mononav/), drones, and autonomous
    vehicles. They can also be used in video and image editing, background replacement,
    object removal, and creating 3D effects. Additionally, they are [useful for AR
    and VR headsets to create interactive 3D spaces around the user](https://arxiv.org/abs/2202.08010).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 良好的 MDE 模型有许多实际应用，如[帮助机器人](https://natesimon.github.io/mononav/)导航和避障、无人机以及自动驾驶汽车。它们还可以用于视频和图像编辑、背景替换、物体移除和创建三维效果。此外，它们在[增强现实（AR）和虚拟现实（VR）头戴设备中也非常有用，可以为用户创造交互式三维空间](https://arxiv.org/abs/2202.08010)。
- en: There are two main approaches for doing MDE (this article only covers one)
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进行单目深度估计（MDE）有两种主要方法（本文仅介绍其中一种）
- en: Two main approaches have emerged for training MDE models — one, discriminative
    approaches where the network tries to predict depth as a supervised learning objective,
    and two, generative approaches like conditional diffusion where depth prediction
    is an iterative image generation task. **Depth Anything belongs to the first category
    of discriminative approaches, and that’s what we will be discussing today.** Welcome
    to Neural Breakdown, and let’s go deep with Depth Estimation[!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 训练MDE模型已经出现了两种主要的方法：一种是判别方法，网络试图将深度作为监督学习目标进行预测；另一种是生成方法，如条件扩散，其中深度预测是一个迭代的图像生成任务。**Depth
    Anything属于第一类判别方法，今天我们将讨论的是这一部分。** 欢迎来到Neural Breakdown，让我们深入探讨深度估计！
- en: '**Traditional Datasets and the MiDAS paper**'
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**传统数据集与MiDAS论文**'
- en: To fully understand Depth Anything, let’s first revisit the MiDAS paper from
    2019, which serves as a precursor to the Depth Anything algorithm.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了全面理解Depth Anything，让我们首先回顾一下2019年的MiDAS论文，它为Depth Anything算法提供了前身。
- en: '![](../Images/c35232fe74fc443615630d9de12c652e.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c35232fe74fc443615630d9de12c652e.png)'
- en: 'Source: Screenshot taken from the [MIDAS](https://arxiv.org/abs/1907.01341)
    Paper (License: Free)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[MIDAS](https://arxiv.org/abs/1907.01341)论文的截图（许可证：免费）
- en: MiDAS trains an MDE model using a combination of different datasets containing
    labeled depth information. For instance, the [KITTI](https://www.cvlibs.net/datasets/kitti/)
    dataset for autonomous driving provides outdoor images, while the [NYU-Depth V2](https://cs.nyu.edu/~fergus/datasets/nyu_depth_v2.html)
    dataset offers indoor scenes. Understanding how these datasets are collected is
    crucial because newer models like Depth Anything and Depth Anything V2 address
    several issues inherent in the data collection process.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: MiDAS通过结合不同的数据集来训练MDE模型，这些数据集包含标注的深度信息。例如，[KITTI](https://www.cvlibs.net/datasets/kitti/)数据集用于自动驾驶，提供户外图像，而[NYU-Depth
    V2](https://cs.nyu.edu/~fergus/datasets/nyu_depth_v2.html)数据集则提供室内场景。了解这些数据集是如何收集的非常重要，因为像Depth
    Anything和Depth Anything V2这样的新模型解决了数据收集过程中固有的多个问题。
- en: How real-world depth datasets are collected
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何收集现实世界中的深度数据集
- en: These datasets are typically collected using stereo cameras, where two or more
    cameras placed at fixed distances capture images simultaneously from slightly
    different perspectives, allowing for depth information extraction. The NYU-Depth
    V2 dataset uses RGB-D cameras that capture depth values along with pixel colors.
    Some datasets utilize LiDAR, projecting laser beams to capture 3D information
    about a scene.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据集通常是通过立体摄像头收集的，两个或更多的摄像头以固定距离放置，从略微不同的视角同时拍摄图像，从而实现深度信息的提取。NYU-Depth V2数据集使用RGB-D摄像头，它不仅捕捉像素颜色，还记录深度值。一些数据集利用激光雷达（LiDAR），通过投射激光束来捕捉场景的三维信息。
- en: '**However, these methods come with several problems.** The amount of labeled
    data is limited due to the high operational costs of obtaining these datasets.
    Additionally, the annotations can be noisy and low-resolution. Stereo cameras
    struggle under various lighting conditions and can’t reliably identify transparent
    or highly reflective surfaces. LiDAR is expensive, and both LiDAR and RGB-D cameras
    have limited range and generate low-resolution, sparse depth maps.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**然而，这些方法存在一些问题。** 由于获取这些数据集的高运营成本，标注数据的数量有限。此外，标注可能会有噪音且分辨率较低。立体摄像头在各种光照条件下表现不佳，无法可靠地识别透明或高度反射的表面。激光雷达昂贵，并且激光雷达与RGB-D摄像头的范围有限，生成的深度图分辨率低且稀疏。'
- en: Can we use Unlabelled Images to learn Depth Estimation?
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们能否使用未标注图像来学习深度估计？
- en: It would be beneficial to use unlabeled images to train depth estimation models,
    given the abundance of such images available online. The major innovation proposed
    in the original Depth Anything paper from 2023 was the incorporation of these
    unlabeled datasets into the training pipeline. In the next section, we’ll explore
    how this was achieved.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 利用未标注的图像来训练深度估计模型将是非常有益的，因为网上有大量这样的图像。2023年原始Depth Anything论文提出的主要创新是将这些未标注数据集整合进训练流程。接下来的部分，我们将探讨这一点是如何实现的。
- en: '**Depth Anything Architecture**'
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**Depth Anything架构**'
- en: The original [Depth Anything (V1) model from 2023](https://depth-anything.github.io/)
    was trained in a three-step process. Let’s get a high-level overview of the algorithm
    before diving into each section.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的[Depth Anything (V1)模型，发布于2023年](https://depth-anything.github.io/)采用了三步训练过程。在深入每个部分之前，让我们先对该算法进行一个高层次的概览。
- en: '![](../Images/b59c38e97f10cfa343bac2b3e428fc20.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b59c38e97f10cfa343bac2b3e428fc20.png)'
- en: Depth Anything V1 Algorithm (Illustration made by the Author)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Depth Anything V1算法（作者插图）
- en: 'Step 1: Teacher Training'
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤1：教师模型训练
- en: First, a neural network called the TEACHER model is trained for supervised depth
    estimation using five different publicly available datasets.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，名为TEACHER的神经网络模型被训练用于监督式深度估计，使用了五个不同的公开数据集。
- en: '***Converting from Depth to Disparity Space***'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '***从深度到视差空间的转换***'
- en: The TEACHER model is initialized with a [pre-trained Dino-V2 encoder](https://arxiv.org/abs/2304.07193)
    and then trained on the combined labeled dataset. A major challenge with training
    on multiple datasets is the variability in absolute depths. To address this, the
    depths are inverted into disparity space **(d = 1 / t)** and normalized between
    0 and 1 for each depth map — 1 for the nearest pixel and 0 for the farthest. This
    way, all datasets share the same output space, allowing the model to predict disparity.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: TEACHER模型初始化时使用了[预训练的Dino-V2编码器](https://arxiv.org/abs/2304.07193)，然后在合并后的标注数据集上进行训练。训练多个数据集时的一个主要挑战是绝对深度的差异。为了解决这个问题，深度值被转化为视差空间**（d
    = 1 / t）**，并在每个深度图中进行归一化处理——离最近像素为1，离最远像素为0。通过这种方式，所有数据集共享相同的输出空间，从而使得模型可以预测视差。
- en: '![](../Images/feb773be7a55da3e9a247bc018362171.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/feb773be7a55da3e9a247bc018362171.png)'
- en: Different Depth Estimation datasets provide depths at different scales. We need
    to align them to have the same output space. Disparity lets us normalize all depth
    values between 0 and 1 (Illustration by Author)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的深度估计数据集提供的深度值具有不同的尺度。我们需要将它们对齐以使输出空间相同。视差可以让我们将所有深度值归一化到0和1之间（作者插图）
- en: 'Two loss functions are used to train these models: **a scale-shift invariant
    loss and a gradient-matching loss**, both also utilized in the MiDAS paper from
    2019.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个损失函数用于训练这些模型：**尺度位移不变损失和梯度匹配损失**，这两个损失函数也在2019年的MiDAS论文中得到了应用。
- en: '***Scale-shift invariant loss***'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***尺度位移不变损失***'
- en: There is a problem with using a simple mean square error loss between the predicted
    and ground truth images. Let’s say the ground truth depth values of three pixels
    in an image are *1, 0.5, and 0.1*, while our network predicts *0.9, 0.6, and 0.3*.
    Although the predictions aren’t exact, the relationship between the predicted
    and ground truth depths is similar, differing only by a multiplicative and additive
    factor. **We don’t want this scale and shift to affect our loss function — we
    need to align the two maps before applying the mean square error loss.**
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 使用简单的均方误差损失来比较预测图像与真实图像之间的差异时存在一个问题。假设图像中三个像素的真实深度值分别是*1、0.5 和 0.1*，而我们的网络预测的是*0.9、0.6
    和 0.3*。尽管预测值不完全相同，但预测深度与真实深度之间的关系是相似的，只有一个乘法和加法因子的差异。**我们不希望这种尺度和位移影响我们的损失函数——在应用均方误差损失之前，我们需要对两个图像进行对齐。**
- en: '![](../Images/84703dc474e959b2a4de1549e58249df.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84703dc474e959b2a4de1549e58249df.png)'
- en: Scale and Shift Invariant Loss (Illustration by Author)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 尺度位移不变损失（作者插图）
- en: The MiDaS paper proposes normalizing the ground truth and predicted depths to
    have zero translation and unit scale. The median and deviation are calculated,
    and the depth maps are scaled and shifted accordingly. Once aligned, the mean
    square error loss is applied.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: MiDaS论文建议将真实深度和预测深度进行归一化，以确保零平移和单位尺度。计算中位数和标准差，然后相应地调整深度图的尺度和位移。对齐后，应用均方误差损失。
- en: '![](../Images/8a6dd80e0cf700348801e9c879cb35a4.png)![](../Images/84a74de5ee9b690b8f57509e828181af.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a6dd80e0cf700348801e9c879cb35a4.png)![](../Images/84a74de5ee9b690b8f57509e828181af.png)'
- en: 'The SSI Loss (Source: [MiDAS Paper](https://arxiv.org/abs/1907.01341)) (License:
    Free)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: SSI损失（来源：[MiDAS论文](https://arxiv.org/abs/1907.01341)）（许可证：免费）
- en: '***2\. Gradient Matching Loss***'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '***2. 梯度匹配损失***'
- en: '![](../Images/5c5a5575969a4594c1abe8a4a352f364.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5c5a5575969a4594c1abe8a4a352f364.png)'
- en: Without Gradient Matching Loss depth maps may become too smudgy and less sharp
    (Illustration by Author)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有梯度匹配损失，深度图可能会变得过于模糊，失去锐利感（作者插图）
- en: Using only the SSI loss might result in smoothed depth maps that fail to capture
    sharp distinctions between adjacent pixels. Gradient Matching Loss helps preserve
    these details by aligning the gradients of the predicted depth map with those
    of the ground truth.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用SSI损失可能会导致平滑的深度图，无法捕捉相邻像素之间的明显区别。梯度匹配损失通过将预测深度图的梯度与真实深度图的梯度对齐，帮助保留这些细节。
- en: First, we calculate the gradients of the predicted and ground truth depth maps
    across the x and y axes, then apply the loss at the gradient level. MiDaS also
    uses a multi-scale gradient matching loss with four scale levels. The predicted
    and ground truth depth maps are downsampled four times, and the loss is applied
    at each resolution.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们计算预测的和地面真实深度图在x轴和y轴上的梯度，然后在梯度层面应用损失。MiDaS还使用了一个具有四个尺度级别的多尺度梯度匹配损失。预测和地面真实深度图被下采样四次，并在每个分辨率上应用损失。
- en: '![](../Images/b0ff56b585ba50ea0d90f590e51805e9.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b0ff56b585ba50ea0d90f590e51805e9.png)'
- en: Gradient Matching Loss. This loss is applied at multiple downscaled depth maps
    (not shown above) (Illustration by Author)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度匹配损失。该损失应用于多个下采样的深度图（未显示）。(插图作者提供)
- en: The final loss is the weighted sum of the scale-and-shift invariant loss and
    the multi-scale gradient matching loss. While the SSI loss encourages the model
    to learn general relative depth relationships, the gradient matching loss helps
    preserve sharp edges and fine-grained information in the scene.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最终损失是尺度不变损失和多尺度梯度匹配损失的加权和。虽然SSI损失鼓励模型学习一般的相对深度关系，但梯度匹配损失有助于保持场景中的锐利边缘和精细信息。
- en: '![](../Images/e2e92b97e1ef6e0478d89fd6f1aa09c7.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2e92b97e1ef6e0478d89fd6f1aa09c7.png)'
- en: The Loss functions used to train Depth Estimation models in MIDAS and Depth
    Anything V1 (Illustration by Author)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在MIDAS和Depth Anything V1中训练深度估计模型所使用的损失函数（插图作者提供）
- en: Step 2 — Pseudo-Labelling Unlabelled Dataset
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤2 — 伪标签无标签数据集
- en: With our trained TEACHER model, we can now annotate millions of unlabeled images
    to create a massive pseudo-depth label dataset. These labels are called pseudo
    because they are AI-generated and may not represent the actual ground truth depth.
    We now have a lot of (pseudo) labeled images to train a new network.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们训练好的教师模型，我们现在可以为数百万张无标签图像添加注释，创建一个庞大的伪深度标签数据集。这些标签被称为伪标签，因为它们是AI生成的，可能并不代表实际的地面真实深度。我们现在拥有大量（伪）标签图像来训练一个新的网络。
- en: '![](../Images/caede365e9a8d6d752dbe1cad7b4c9db.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/caede365e9a8d6d752dbe1cad7b4c9db.png)'
- en: 'Pseudo — Labelling images (Note that this screen is actually from the Depth
    Anything V2 paper and not V1) Source: [Depth Anything V2 Paper](https://arxiv.org/abs/2406.09414)
    (License: Free)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 伪标签图像（请注意，这张屏幕截图实际上来自Depth Anything V2论文，而非V1）来源：[Depth Anything V2 Paper](https://arxiv.org/abs/2406.09414)（许可证：免费）
- en: Step 3 — Training Student Network
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤3 — 训练学生网络
- en: '![](../Images/b59c38e97f10cfa343bac2b3e428fc20.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b59c38e97f10cfa343bac2b3e428fc20.png)'
- en: Flashback to the Depth Anything V1 algorithm. We are in Step 3 now. (Illustration
    made by the author)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾Depth Anything V1算法。我们现在处于步骤3。（插图由作者制作）
- en: 'We will be training a new neural network (the student network) on the combination
    of the labeled and pseudo-labeled datasets. **However, simply training the network
    on the annotations provided by the Teacher Network won’t improve the model beyond
    the capabilities of the base Teacher model.** To make the student network more
    capable, two strategies were employed: heavy perturbations with image augmentations
    and introducing an auxiliary semantic preservation loss.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在标签数据集和伪标签数据集的结合上训练一个新的神经网络（学生网络）。**然而，仅仅在教师网络提供的注释上训练网络并不会使模型超越基础教师模型的能力。**为了使学生网络更强大，采用了两种策略：使用图像增强进行重度扰动，并引入辅助语义保持损失。
- en: '***Heavy Perturbations***'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '***重度扰动***'
- en: One interesting perturbation used was the Cut Mix operation. This involves combining
    a random pair of unlabeled images using a binary mask, replacing a rectangular
    portion of image A with image B. The final loss is the combined SSI and Gradient
    Matching loss of the two sections from the two ground truth depth maps. These
    spatial distortions are also combined with color distortions to help the Student
    Network handle the diversity of open-world images.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的一个有趣的扰动是Cut Mix操作。它涉及通过二进制掩膜将一对随机的无标签图像结合起来，用图像B替换图像A中的一个矩形区域。最终的损失是来自两个地面真实深度图的两部分的合成SSI和梯度匹配损失。这些空间失真还与颜色失真结合，帮助学生网络应对开放世界图像的多样性。
- en: '![](../Images/d248f0a505eebf438fa29e5c540cde53.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d248f0a505eebf438fa29e5c540cde53.png)'
- en: The Cut Mix Operation (Illustration by Author)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Cut Mix操作（插图作者提供）
- en: '***Auxiliary Semantic Preservation Loss***'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '***辅助语义保持损失***'
- en: The network is also trained with an auxiliary task called Semantic Assisted
    Perception. A strong pre-trained computer vision model like Dino-V2, which has
    been trained on millions of images in a self-supervised manner, is used. Given
    an image, we aim to reduce the cosine distance between the embeddings produced
    by our new Student model and the pre-trained Dino-V2 encoder. This enables our
    Student model to capture some of the semantic perception capabilities of the larger
    and more general Dino-V2 model, which it uses to predict the depth map.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 网络还通过一个辅助任务进行训练，称为语义辅助感知。使用像Dino-V2这样的强大预训练计算机视觉模型，它已经通过自监督方式在数百万张图像上进行了训练。给定一张图像，我们的目标是减少由新学生模型生成的嵌入与预训练Dino-V2编码器之间的余弦距离。这使得我们的学生模型能够捕捉到更大、更通用的Dino-V2模型的一些语义感知能力，并利用这些能力来预测深度图。
- en: '![](../Images/3240ebbb1b601d8cf1aec0cd5932a074.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3240ebbb1b601d8cf1aec0cd5932a074.png)'
- en: Semantic Assisted Perception (Illustration by author)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 语义辅助感知（作者插图）
- en: By combining spatial distortions, semantic-assisted perception, and the power
    of both labeled and unlabeled datasets, the Student Network generalizes better
    and outperforms the original Teacher Network in-depth estimation! Here are some
    incredible results from the Depth Anything V1 model!
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合空间畸变、语义辅助感知以及标注和未标注数据集的力量，学生网络能够更好地泛化，并在深度估计上超过原始的教师网络！以下是来自Depth Anything
    V1模型的一些令人难以置信的结果！
- en: '**Depth Anything V2**'
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**Depth Anything V2**'
- en: '**As impressive as Depth Anything V1’s results are, it struggles with transparent
    objects and capturing fine-grained details.** **The authors of Depth Anything
    V2 suggest that the biggest bottleneck for model performance isn’t the architecture
    itself, but the quality of the data.** Most labeled datasets captured with sensors
    can be quite noisy, ignore fine-grained details, generate low-resolution depth
    maps, and struggle with lighting conditions and reflective/transparent objects.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**尽管Depth Anything V1的结果令人印象深刻，但它在处理透明物体和捕捉细粒度细节方面仍存在困难。** **Depth Anything
    V2的作者认为，模型性能的最大瓶颈不是架构本身，而是数据的质量。** 大多数使用传感器捕获的标注数据集可能会有噪声，忽略细粒度细节，生成低分辨率的深度图，并且在光照条件以及反射/透明物体的处理上存在困难。'
- en: '![](../Images/94bb158d8593a17797ed2214b00abd67.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94bb158d8593a17797ed2214b00abd67.png)'
- en: Issues with real-world sensor datasets (Illustration by Author)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 真实世界传感器数据集的问题（作者插图）
- en: Depth Anything V2 discards labeled datasets from real-world sensors like stereo
    cameras, LiDAR, and RGB-D cameras, instead using only synthetic datasets. Synthetic
    datasets are generated through graphics engines, not captured with equipment.
    An example is the [Virtual KITTI dataset](https://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction),
    which uses the Unity Game Engine to create rendered images and depth maps for
    automated driving. There are also indoor datasets like IRS and Hyper-sim. Depth
    Anything V2 uses five synthetic datasets containing close to 595K photorealistic
    images.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Depth Anything V2舍弃了来自真实世界传感器（如立体相机、激光雷达和RGB-D相机）的标注数据集，而仅使用合成数据集。合成数据集是通过图形引擎生成的，而非通过设备捕获的。例如，使用Unity游戏引擎创建渲染图像和深度图的[虚拟KITTI数据集](https://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction)，用于自动驾驶。还有一些室内数据集，如IRS和Hyper-sim。Depth
    Anything V2使用了五个包含接近595K张逼真图像的合成数据集。
- en: '***Synthetic Datasets vs Real World Sensor Datasets***'
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '***合成数据集 vs 真实世界传感器数据集***'
- en: Synthetic images do have their pros and cons. They are super accurate, they
    have high-resolution outputs that capture the finest of the details, and the depth
    of transparent and reflective surfaces can be easily obtained. **Synthetic datasets
    have direct access to all the 3D information needed since the graphics engine
    itself creates the scene.**
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 合成图像确实有其优缺点。它们非常准确，具有高分辨率输出，能够捕捉最细微的细节，并且透明和反射表面的深度可以轻松获得。**合成数据集直接获取所有所需的3D信息，因为图形引擎本身创建了场景。**
- en: On the cons side, these images may not essentially capture the images that we
    will encounter in real-world scenarios. The scene coverage of these datasets isn’t
    particularly diverse enough too, and is a much smaller subset of real-world images.
    Depth Anything 2 combines the power of synthetic images with millions of unlabelled
    images to train an MDE model that outperforms pretty much everything else we have
    seen so far.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 从缺点方面来看，这些图像可能无法完全捕捉我们在现实场景中会遇到的图像。这些数据集的场景覆盖面也不够多样，且仅为现实世界图像的一个较小子集。Depth Anything
    2结合了合成图像和数百万未标注图像的优势，训练出了一个MDE模型，其性能超越了我们迄今为止见过的大多数模型。
- en: '![](../Images/217dc74172e0dee404d98a287ddc4508.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/217dc74172e0dee404d98a287ddc4508.png)'
- en: The pros and cons of Synthetic or Computer Generated Datasets (Illustration
    by Author)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 合成或计算机生成数据集的优缺点（图示：作者提供）
- en: Much like V1, the Teacher model in V2 is first trained on labeled datasets.
    However, in V2, it is exclusively trained on synthetic datasets. In Step 2, the
    Teacher model assigns pseudo-depth labels to all unlabeled images. Finally, in
    Step 3, the Student model is trained exclusively on pseudo-labeled images — no
    real labeled datasets and no synthetic datasets. The synthetic datasets are not
    used at this stage due to the distribution shift mentioned earlier. The Student
    network is trained on real-world images annotated by the Teacher model. Just like
    in V1, the auxiliary semantic preservation loss is used along with the Scale-and-Shift
    invariant and gradient matching loss.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 和V1类似，V2中的教师模型首先在标注数据集上进行训练。然而，在V2中，它仅在合成数据集上进行训练。在第二步，教师模型为所有未标注图像分配伪深度标签。最后，在第三步，学生模型仅在伪标注图像上进行训练——不使用任何真实标注数据集，也不使用合成数据集。在这一阶段，由于之前提到的分布偏移，合成数据集没有被使用。学生网络在由教师模型标注的真实世界图像上进行训练。就像V1中一样，辅助的语义保留损失与尺度-平移不变性损失和梯度匹配损失一同使用。
- en: '![](../Images/53c47ae75f37e0bef19e2c77b3169255.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/53c47ae75f37e0bef19e2c77b3169255.png)'
- en: The Depth Anything V2 architecture (Illustration by the Author)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Depth Anything V2架构（图示：作者提供）
- en: Video link explaining the concepts here visually
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视频链接，形象地解释这些概念
- en: Here is a video that explains all the concepts discussed in this video in a
    step-by-step method.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个视频，逐步解释了本视频中讨论的所有概念。
- en: You can also learn the topics covered in this article in video form
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以以视频形式学习本文中涉及的主题
- en: Depth Anything V1 vs Depth Anything V2
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Depth Anything V1 vs Depth Anything V2
- en: The original Depth Anything emphasized the importance of using unlabeled images
    in the MDE training pipeline. It introduced the knowledge distillation pipeline
    with Teacher training, pseudo-labeling unlabeled images, and then training the
    Student network on a combination of labeled and unlabeled images. The use of strong
    spatial and color distortions, and a semantic-assisted perception loss, helped
    create more general and robust embeddings. This resulted in efficient and high-quality
    depth maps for complex scenes. However, Depth Anything V1 still struggled with
    reflective surfaces and fine details due to noisy and low-resolution depth labels
    from real-world sensors.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 原版的Depth Anything强调了在MDE训练流程中使用未标注图像的重要性。它引入了知识蒸馏流程，包括教师模型训练、伪标签化未标注图像，然后在标注和未标注图像的结合上训练学生网络。强空间和颜色扭曲的使用，以及语义辅助感知损失，有助于创建更通用和鲁棒的嵌入。这导致了为复杂场景生成高效且高质量的深度图。然而，Depth
    Anything V1仍然在处理反射表面和细节时存在问题，因为来自现实传感器的深度标签噪声大且分辨率低。
- en: Depth Anything V2 improved performance by ignoring real-world sensor datasets
    and only using synthetic images generated with graphics engines to train the Teacher
    Network. The Teacher Network then annotates millions of unlabeled images, and
    the Student Network is trained solely on these pseudo-labeled datasets with real-world
    images. With these techniques, Depth Anything V2 can now predict fine-level depth
    maps and handle transparent and reflective surfaces more effectively.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Depth Anything V2通过忽略真实世界传感器数据集，专门使用通过图形引擎生成的合成图像来训练教师网络，从而提升了性能。然后，教师网络为数百万未标注图像进行标注，学生网络仅在这些伪标注数据集上进行训练，并使用真实世界图像。通过这些技术，Depth
    Anything V2现在能够预测精细级别的深度图，并更有效地处理透明和反射表面。
- en: '**Relevant Links**'
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**相关链接**'
- en: 'MiDAS: [https://arxiv.org/abs/1907.01341](https://arxiv.org/abs/1907.01341)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 'MiDAS: [https://arxiv.org/abs/1907.01341](https://arxiv.org/abs/1907.01341)'
- en: 'Depth Anything: [https://depth-anything.github.io/](https://depth-anything.github.io/)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 'Depth Anything: [https://depth-anything.github.io/](https://depth-anything.github.io/)'
- en: 'Depth Anything V2: [https://depth-anything-v2.github.io/](https://depth-anything-v2.github.io/)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 'Depth Anything V2: [https://depth-anything-v2.github.io/](https://depth-anything-v2.github.io/)'
- en: 'KITTI DATASET: [https://www.cvlibs.net/datasets/kitti/](https://www.cvlibs.net/datasets/kitti/)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 'KITTI 数据集: [https://www.cvlibs.net/datasets/kitti/](https://www.cvlibs.net/datasets/kitti/)'
- en: 'NYU V2: [https://cs.nyu.edu/~fergus/datasets/nyu_depth_v2.html](https://cs.nyu.edu/~fergus/datasets/nyu_depth_v2.html)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 'NYU V2: [https://cs.nyu.edu/~fergus/datasets/nyu_depth_v2.html](https://cs.nyu.edu/~fergus/datasets/nyu_depth_v2.html)'
- en: 'VIRTUAL KITTI: [https://datasetninja.com/virtual-kitti](https://datasetninja.com/virtual-kitti)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '虚拟 KITTI: [https://datasetninja.com/virtual-kitti](https://datasetninja.com/virtual-kitti)'
- en: 'Youtube Video: [https://youtu.be/sz30TDttIBA](https://youtu.be/sz30TDttIBA)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 'Youtube 视频: [https://youtu.be/sz30TDttIBA](https://youtu.be/sz30TDttIBA)'
