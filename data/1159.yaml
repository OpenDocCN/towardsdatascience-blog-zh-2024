- en: Apache Hadoop and Apache Spark for Big Data Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Hadoop 和 Apache Spark 用于大数据分析
- en: 原文：[https://towardsdatascience.com/apache-hadoop-and-apache-spark-for-big-data-analysis-daaf659fd0ee?source=collection_archive---------5-----------------------#2024-05-08](https://towardsdatascience.com/apache-hadoop-and-apache-spark-for-big-data-analysis-daaf659fd0ee?source=collection_archive---------5-----------------------#2024-05-08)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/apache-hadoop-and-apache-spark-for-big-data-analysis-daaf659fd0ee?source=collection_archive---------5-----------------------#2024-05-08](https://towardsdatascience.com/apache-hadoop-and-apache-spark-for-big-data-analysis-daaf659fd0ee?source=collection_archive---------5-----------------------#2024-05-08)
- en: A complete guide to big data analysis using Apache Hadoop (HDFS) and PySpark
    library in Python on game reviews on the Steam gaming platform.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一份完整的指南，介绍如何使用Apache Hadoop（HDFS）和Python中的PySpark库进行大数据分析，以分析Steam游戏平台上的游戏评论。
- en: '[](https://medium.com/@rindhuj1?source=post_page---byline--daaf659fd0ee--------------------------------)[![Rindhuja
    Treesa Johnson](../Images/15d2bfb802395968ea23faff62cc623a.png)](https://medium.com/@rindhuj1?source=post_page---byline--daaf659fd0ee--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--daaf659fd0ee--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--daaf659fd0ee--------------------------------)
    [Rindhuja Treesa Johnson](https://medium.com/@rindhuj1?source=post_page---byline--daaf659fd0ee--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@rindhuj1?source=post_page---byline--daaf659fd0ee--------------------------------)[![Rindhuja
    Treesa Johnson](../Images/15d2bfb802395968ea23faff62cc623a.png)](https://medium.com/@rindhuj1?source=post_page---byline--daaf659fd0ee--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--daaf659fd0ee--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--daaf659fd0ee--------------------------------)
    [Rindhuja Treesa Johnson](https://medium.com/@rindhuj1?source=post_page---byline--daaf659fd0ee--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--daaf659fd0ee--------------------------------)
    ·14 min read·May 8, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--daaf659fd0ee--------------------------------)
    ·14分钟阅读·2024年5月8日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: With over 100 zettabytes (= 10¹²GB) of [data produced every year](https://www.statista.com/statistics/871513/worldwide-data-created/)
    around the world, the significance of handling big data is one of the most required
    skills today. Data Analysis, itself, could be defined as the ability to handle
    big data and derive insights from the never-ending and exponentially growing data.
    Apache Hadoop and Apache Spark are two of the basic tools that help us untangle
    the limitless possibilities hidden in large datasets. [Apache Hadoop](https://hadoop.apache.org/)
    enables us to streamline data storage and distributed computing with its Distributed
    File System (HDFS) and the MapReduce-based parallel processing of data. [Apache
    Spark](https://spark.apache.org/) is a big data analytics engine capable of EDA,
    SQL analytics, Streaming, Machine Learning, and Graph processing compatible with
    the major programming languages through its APIs. Both when combined form an exceptional
    environment for dealing with big data with the available computational resources
    — just a personal computer in most cases!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 全球每年生产超过100个泽字节（= 10¹²GB）的[数据](https://www.statista.com/statistics/871513/worldwide-data-created/)，因此处理大数据的能力是今天最为关键的技能之一。数据分析本身可以定义为处理大数据并从无止境、指数增长的数据中提取洞察力的能力。Apache
    Hadoop 和 Apache Spark 是帮助我们解开大型数据集中无限可能性的两个基本工具。[Apache Hadoop](https://hadoop.apache.org/)
    通过其分布式文件系统（HDFS）和基于 MapReduce 的数据并行处理，帮助我们简化数据存储和分布式计算。[Apache Spark](https://spark.apache.org/)
    是一个大数据分析引擎，能够进行EDA、SQL分析、流处理、机器学习和图处理，并通过其API与主要编程语言兼容。两者结合起来，构成了一个出色的大数据处理环境，并且在大多数情况下，只需要一台个人电脑即可完成！
- en: Let us unfold the power of [Big Data and Apache Hadoop](https://medium.com/@roshmitadey/a-beginners-guide-to-big-data-and-hadoop-distributed-file-system-hdfs-b5c324d3c722)
    with a simple analysis project implemented using Apache Spark in Python.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的分析项目，利用Apache Spark在Python中实现，来展开[大数据和Apache Hadoop](https://medium.com/@roshmitadey/a-beginners-guide-to-big-data-and-hadoop-distributed-file-system-hdfs-b5c324d3c722)的强大功能。
- en: To begin with, let’s dive into the installation of Hadoop Distributed File System
    and Apache Spark on a MacOS. I am using a MacBook Air with macOS Sonoma with an
    M1 chip.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们深入了解如何在MacOS上安装Hadoop分布式文件系统和Apache Spark。我使用的是一台配备M1芯片的MacBook Air，操作系统为macOS
    Sonoma。
- en: '**Jump to the section —**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**跳转到章节 —**'
- en: '[Installing Hadoop Distributed File System](#b292)'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[安装Hadoop分布式文件系统](#b292)'
- en: '[Installing Apache Spark](#2773)'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[安装 Apache Spark](#2773)'
- en: '[Steam Review Analysis using PySpark](#32ea)'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[使用 PySpark 进行 Steam 评论分析](#32ea)'
- en: '[What next?](#2580)'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[接下来做什么？](#2580)'
- en: 1\. Installing Hadoop Distributed File System
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 安装 Hadoop 分布式文件系统
- en: Thanks to [Code With Arjun](https://medium.com/u/bdd6bb8fb77f?source=post_page---user_mention--daaf659fd0ee--------------------------------)
    for the amazing article that helped me with the [installation of Hadoop on my
    Mac](https://codewitharjun.medium.com/install-hadoop-on-macos-efe7c860c3ed). I
    seamlessly installed and ran Hadoop following his steps which I will show you
    here as well.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢 [Code With Arjun](https://medium.com/u/bdd6bb8fb77f?source=post_page---user_mention--daaf659fd0ee--------------------------------)
    分享的精彩文章，帮助我完成了在我的 Mac 上的 [Hadoop 安装](https://codewitharjun.medium.com/install-hadoop-on-macos-efe7c860c3ed)。我按照他的步骤顺利安装并运行了
    Hadoop，下面我也会展示这些步骤。
- en: '**a. Installing HomeBrew**'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**a. 安装 HomeBrew**'
- en: I use [Homebrew](https://brew.sh/) for installing applications on my Mac for
    ease. It can be directly installed on the system with the below code —
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用 [Homebrew](https://brew.sh/) 来在我的 Mac 上安装应用程序，便于操作。可以通过以下代码直接在系统上安装它 —
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Once it is installed, you can run the simple code below to verify the installation.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，你可以运行以下简单代码来验证安装。
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/c0f0f02df5fc0a74936bb9fba42d896e.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c0f0f02df5fc0a74936bb9fba42d896e.png)'
- en: 'Figure 1: Image by Author'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：作者提供的图片
- en: However, you will likely encounter an error saying, `command not found`, this
    is because the homebrew will be installed in a different location (Figure 2) and
    it is not executable from the current directory. For it to function, we add a
    path environment variable for the brew, i.e., adding homebrew to the .bash_profile.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你可能会遇到一个错误，提示`command not found`，这是因为 Homebrew 会被安装在不同的位置（如图 2 所示），并且无法从当前目录执行。为了使其正常工作，我们需要为
    brew 添加一个路径环境变量，即将 Homebrew 添加到 `.bash_profile` 中。
- en: '![](../Images/594e71aa1c71a70fe1615dba45a9b210.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/594e71aa1c71a70fe1615dba45a9b210.png)'
- en: 'Figure 2: Image by Author'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：作者提供的图片
- en: You can avoid this step by using the full path to Homebrew in your commands,
    however, it might become a hustle at later stages, so not recommended!
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可以通过在命令中使用 Homebrew 的完整路径来避免此步骤，但在后期阶段可能会变得麻烦，所以不推荐这么做！
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now, when you try, `brew --version`, it should show the Homebrew version correctly.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当你尝试运行 `brew --version` 时，它应该能够正确显示 Homebrew 版本。
- en: '**b. Installing Hadoop**'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**b. 安装 Hadoop**'
- en: Disclaimer! Hadoop is a Java-based application and is supported by a Java Development
    Kit (JDK) version older than 11, preferably 8 or 11\. Install JDK before continuing.
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 声明！Hadoop 是一个基于 Java 的应用程序，支持 11 版本以下的 Java 开发工具包（JDK），最好是 8 或 11。请在继续之前安装 JDK。
- en: Thanks to [Code With Arjun](https://medium.com/u/bdd6bb8fb77f?source=post_page---user_mention--daaf659fd0ee--------------------------------)
    again for this video on JDK installation on MacBook M1.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 再次感谢 [Code With Arjun](https://medium.com/u/bdd6bb8fb77f?source=post_page---user_mention--daaf659fd0ee--------------------------------)，感谢他分享的关于在
    MacBook M1 上安装 JDK 的视频。
- en: Guide to Installing JDK
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 JDK 指南
- en: Now, we install the Hadoop on our system using the brew command.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用 brew 命令在系统上安装 Hadoop。
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This command should install Hadoop seamlessly. Similar to the steps followed
    while installing HomeBrew, we should edit the path environment variable for Java
    in the Hadoop folder. The environment variable settings for the installed version
    of Hadoop can be found in the Hadoop folder within HomeBrew. You can use `which
    hadoop` command to find the location of the Hadoop installation folder. Once you
    locate the folder, you can find the variable settings at the below location. The
    below command takes you to the required folder for editing the variable settings
    (Check the Hadoop version you installed to avoid errors).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令应该能够无缝地安装 Hadoop。与安装 HomeBrew 时遵循的步骤类似，我们需要在 Hadoop 文件夹中编辑 Java 的路径环境变量。安装版本的
    Hadoop 的环境变量设置可以在 HomeBrew 中的 Hadoop 文件夹中找到。你可以使用`which hadoop`命令来查找 Hadoop 安装文件夹的位置。一旦找到文件夹，你可以在以下位置找到变量设置。以下命令将带你到所需的文件夹以编辑变量设置（请检查你安装的
    Hadoop 版本，以避免出错）。
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can view the files in this folder using the `ls` command. We will edit the
    `hadoop-env.sh` to enable the proper running of Hadoop on the system.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 `ls` 命令查看该文件夹中的文件。我们将编辑 `hadoop-env.sh` 以确保 Hadoop 在系统上正确运行。
- en: '![](../Images/c9433c954e59f2cbaaf96c35b2a1fcac.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c9433c954e59f2cbaaf96c35b2a1fcac.png)'
- en: 'Figure 3: Image by Author'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：作者提供的图片
- en: Now, we have to find the path variable for Java to edit the `hadoop-ev.sh` file
    using the following command.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要找到 Java 的路径变量，以便使用以下命令编辑 `hadoop-ev.sh` 文件。
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/e3082e79171b59a742c2462151b8efa3.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e3082e79171b59a742c2462151b8efa3.png)'
- en: 'Figure 4: Image by Author'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：图片来自作者
- en: We can open the `hadoop-env.sh` file in any text editor. I used VI editor, you
    can use any editor for the purpose. We can copy and paste the path — `Library/Java/JavaVirtualMachines/adoptopenjdk-11.jdk/Contents/Home`
    at the `export JAVA_HOME =` position.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在任何文本编辑器中打开 `hadoop-env.sh` 文件。我使用了 VI 编辑器，您可以使用任何编辑器。我们可以将路径 `Library/Java/JavaVirtualMachines/adoptopenjdk-11.jdk/Contents/Home`
    复制并粘贴到 `export JAVA_HOME =` 位置。
- en: '![](../Images/bbeff2e0db5eba54d035987932f6209a.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bbeff2e0db5eba54d035987932f6209a.png)'
- en: 'Figure 5: hadoop-env.sh file opened in VI Text Editor'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：在 VI 文本编辑器中打开的 hadoop-env.sh 文件
- en: Next, we edit the four XML files in the Hadoop folder.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们编辑 Hadoop 文件夹中的四个 XML 文件。
- en: '`core-site.xml`'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`core-site.xml`'
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`hdfs-site.xml`'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`hdfs-site.xml`'
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '`mapred-site.xml`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`mapred-site.xml`'
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`yarn-site.xml`'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`yarn-site.xml`'
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: With this, we have successfully completed the installation and configuration
    of HDFS on the local. To make the data on Hadoop accessible with Remote login,
    we can go to Sharing in the General settings and enable `Remote Login`. You can
    edit the user access by clicking on the info icon.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 至此，我们已经成功完成了在本地安装和配置 HDFS。为了使 Hadoop 上的数据可以通过远程登录访问，我们可以在常规设置中的共享部分启用 `远程登录`。您可以通过点击信息图标编辑用户访问权限。
- en: '![](../Images/b09bae4523e0679b1275c841247e5aff.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b09bae4523e0679b1275c841247e5aff.png)'
- en: 'Figure 6: Enable Remote Access. Image by Author'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：启用远程访问。图片来自作者
- en: Let’s run Hadoop!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行 Hadoop！
- en: Execute the following commands
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下命令
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/fb2fa78c2639a628f5ee27da07acfa27.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fb2fa78c2639a628f5ee27da07acfa27.png)'
- en: 'Figure 7: Initiating Hadoop and viewing the nodes and resources running. Image
    by Author'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：启动 Hadoop 并查看运行中的节点和资源。图片来自作者
- en: We are all set! Now let’s create a directory in HDFS and add the data will be
    working on. Let’s quickly take a look at our data source and details.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一切就绪！现在让我们在 HDFS 中创建一个目录并添加我们将要处理的数据。让我们快速查看我们的数据源及其详细信息。
- en: '**Data**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据**'
- en: 'The [Steam Reviews Dataset 2021](https://www.kaggle.com/datasets/najzeko/steam-reviews-2021)
    ***(***[***License: GPL 2***](https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html)***)***
    is a collection of reviews from about 21 million gamers covering over 300 different
    games in the year 2021\. the data is extracted using Steam’s API — [Steamworks](https://partner.steamgames.com/doc/store/getreviews)
    — using the Get List function.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[Steam Reviews Dataset 2021](https://www.kaggle.com/datasets/najzeko/steam-reviews-2021)
    ***(***[***许可协议：GPL 2***](https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html)***)***
    是一个包含大约 2100 万玩家评论的数据集，涵盖了 2021 年超过 300 款不同的游戏。数据是通过 Steam 的 API — [Steamworks](https://partner.steamgames.com/doc/store/getreviews)
    — 使用“获取列表”功能提取的。'
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The dataset consists of 23 columns and 21.7 million rows with a size of 8.17
    GB (that is big!). The data consists of reviews in different languages and a boolean
    column that tells if the player recommends the game to other players. We will
    be focusing on how to handle this big data locally using HDFS and analyze it using
    Apache Spark in Python using the PySpark library.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集由 23 列和 2170 万行组成，大小为 8.17 GB（这很大！）。数据包含不同语言的评论和一个布尔列，指示玩家是否推荐该游戏给其他玩家。我们将重点讨论如何使用
    HDFS 本地处理这些大数据，并使用 PySpark 库在 Python 中通过 Apache Spark 进行分析。
- en: '**c. Uploading Data into HDFS**'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**c. 上传数据到 HDFS**'
- en: Firstly, we create a directory in the HDFS using the `mkdir` command. *It will
    throw an error if we try to add a file directly to a non-existing folder.*
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用 `mkdir` 命令在 HDFS 中创建一个目录。*如果我们尝试将文件直接添加到一个不存在的文件夹中，它将抛出一个错误。*
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now, we will add the data file to the folder `steam_analysis` using the `put`
    command.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用 `put` 命令将数据文件添加到文件夹 `steam_analysis` 中。
- en: '[PRE14]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Apache Hadoop also uses a user interface available at [http://localhost:9870/](http://localhost:9870/).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Hadoop 还提供一个用户界面，可以通过 [http://localhost:9870/](http://localhost:9870/)
    访问。
- en: '![](../Images/a158c45d8a4f833962c62d1c16d19215.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a158c45d8a4f833962c62d1c16d19215.png)'
- en: 'Figure 8: HDFS User Interface at localhost:9870\. Image by Author'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：localhost:9870 上的 HDFS 用户界面。图片来自作者
- en: We can see the uploaded files as shown below.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到上传的文件，如下所示。
- en: '![](../Images/b57932add5847d2cf0d9f63a7bfad1cd.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b57932add5847d2cf0d9f63a7bfad1cd.png)'
- en: 'Figure 10: Navigating files in HDFS. Image by Author'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：在 HDFS 中浏览文件。图片来自作者
- en: Once the data interaction is over, we can use `stop-all.sh` command to stop
    all the Apache Hadoop daemons.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据交互完成，我们可以使用`stop-all.sh`命令停止所有 Apache Hadoop 守护进程。
- en: Let us move to the next step — Installing Apache Spark
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进入下一步 — 安装 Apache Spark
- en: 2\. Installing Apache Spark
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 安装 Apache Spark
- en: Apache Hadoop takes care of data storage (HDFS) and parallel processing (MapReduce)
    of the data for faster execution. [Apache Spark](https://spark.apache.org/) is
    a multi-language compatible analytical engine designed to deal with big data analysis.
    We will run the Apache Spark on Python in Jupyter IDE.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Hadoop负责数据存储（HDFS）和数据的并行处理（MapReduce），以加速执行。[Apache Spark](https://spark.apache.org/)是一个多语言兼容的分析引擎，旨在处理大数据分析。我们将在Jupyter
    IDE中使用Python运行Apache Spark。
- en: After installing and running HDFS, the installation of Apache Spark for Python
    is a piece of cake. PySpark is the Python API for Apache Spark that can be installed
    using the `pip` method in the Jupyter Notebook. PySpark is the Spark Core API
    with its four components — Spark SQL, Spark ML Library, Spark Streaming, and GraphX.
    Moreover, we can access the Hadoop files through PySpark by initializing the installation
    with the required Hadoop version.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装并运行HDFS之后，安装Apache Spark for Python轻松得多。PySpark是Apache Spark的Python API，可以通过在Jupyter
    Notebook中使用`pip`方法进行安装。PySpark是Spark Core API，包含四个组件——Spark SQL、Spark ML库、Spark
    Streaming和GraphX。此外，我们可以通过初始化安装并指定所需的Hadoop版本，使用PySpark访问Hadoop文件。
- en: '[PRE15]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Let’s get started with the Big Data Analytics!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始大数据分析吧！
- en: 3\. Steam Review Analysis using PySpark
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. 使用PySpark进行Steam评论分析
- en: '[Steam](https://store.steampowered.com/about/) is an online gaming platform
    that hosts over 30,000 games streaming across the world with over 100 million
    players. Besides gaming, the platform allows the players to provide reviews for
    the games they play, a great resource for the platform to improve customer experience
    and for the gaming companies to work on to keep the players on edge. We used this
    review data provided by the platform publicly available on [Kaggle](https://www.kaggle.com/datasets/najzeko/steam-reviews-2021).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[Steam](https://store.steampowered.com/about/)是一个在线游戏平台，全球拥有超过100百万玩家，平台上托管着超过30,000款游戏。除了游戏，平台还允许玩家为他们玩的游戏提供评论，这为平台改进客户体验和游戏公司保持玩家活跃提供了重要资源。我们使用了平台上公开提供的[Kaggle](https://www.kaggle.com/datasets/najzeko/steam-reviews-2021)上的评论数据。'
- en: '**3\. a. Data Extraction from HDFS**'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**3. a. 从HDFS提取数据**'
- en: We will use the PySpark library to access, clean, and analyze the data. To start,
    we connect the PySpark session to Hadoop using the local host address.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用PySpark库来访问、清理和分析数据。首先，我们通过本地主机地址将PySpark会话连接到Hadoop。
- en: '[PRE16]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**3\. b. Data Cleaning and Pre-Processing**'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**3. b. 数据清理与预处理**'
- en: We can start by taking a look at the dataset. Similar to the pandas.head() function
    in Pandas, PySpark has the SparkSession.show() function that gives a glimpse of
    the dataset.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以先看看数据集。与Pandas中的pandas.head()函数类似，PySpark提供了SparkSession.show()函数，能够展示数据集的一部分。
- en: Before that, we will remove the reviews column in the dataset as we do not plan
    on performing any NLP on the dataset. Also, the reviews are in different languages
    making any sentiment analysis based on the review difficult.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之前，我们将删除数据集中的评论列，因为我们不打算对数据集进行任何自然语言处理（NLP）。此外，评论使用了不同的语言，这使得基于评论进行情感分析变得困难。
- en: '[PRE17]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](../Images/52636915bc9dc3c68b59c2d3238eeb54.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52636915bc9dc3c68b59c2d3238eeb54.png)'
- en: 'Figure 11: The Structure of the Schema'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：模式的结构
- en: We have a huge dataset with us with 23 attributes with NULL values for different
    attributes which does not make sense to consider any imputation. Therefore, I
    have removed the records with NULL values. However, this is not a recommended
    approach. You can evaluate the importance of the available attributes and remove
    the irrelevant ones, then try imputing data points to the NULL values.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个庞大的数据集，包含23个属性，其中有不同属性的NULL值，这使得考虑进行插补变得不合适。因此，我已删除了包含NULL值的记录。然而，这不是推荐的方法。您可以评估可用属性的重要性，删除无关的属性，然后尝试对NULL值进行插补。
- en: '[PRE18]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We still have almost 17 million records in the dataset!
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中仍然有接近1700万条记录！
- en: Now, we focus on the variable names of the dataset as in Figure 11\. We can
    see that the attributes have a few characters like dot(.) that are unacceptable
    as Python identifiers. Also, we change the data type of the date and time attributes.
    So we change these using the following code —
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们集中关注数据集中的变量名，如图11所示。我们可以看到某些属性包含像点（.）这样的字符，这些字符不符合Python标识符的命名规则。同时，我们还需要更改日期和时间属性的数据类型。因此，我们使用以下代码进行更改——
- en: '[PRE19]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](../Images/a42764009ddbe5123c2a75a7f5cf4e0f.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a42764009ddbe5123c2a75a7f5cf4e0f.png)'
- en: 'Figure 12: A glimpse of the Steam review Analysis dataset. Image by Author'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：Steam评论分析数据集的简要概览。图像来自作者
- en: The dataset is clean and ready for analysis!
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集已经清理完毕，准备好进行分析！
- en: '**3\. c. Exploratory Data Analysis**'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. c. 探索性数据分析**'
- en: The dataset is rich in information with over 20 variables. We can analyze the
    data from different perspectives. Therefore, we will be splitting the data into
    different PySpark data frames and caching them to run the analysis faster.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含超过20个变量，信息丰富。我们可以从不同的角度分析数据。因此，我们将把数据拆分成不同的PySpark数据框，并进行缓存，以加快分析速度。
- en: '[PRE20]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '***i. Games Analysis***'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '***i. 游戏分析***'
- en: In this section, we will try to understand the review and recommendation patterns
    for different games. We will consider the *number of reviews* analogous to the
    popularity of the game and the *number of* ***True*** *recommendations* analogous
    to the gamer’s preference for the game.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将尝试了解不同游戏的评论和推荐模式。我们将把*评论数量*视为游戏的受欢迎程度，而*推荐数量* ***True*** *则代表玩家对该游戏的偏好*。
- en: Finding the Most Popular Games
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到最受欢迎的游戏
- en: '[PRE21]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Finding the Most Recommended Games
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到推荐最多的游戏
- en: '[PRE22]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](../Images/b62fe5c6d97a4793d51ff4f7c653c876.png)![](../Images/405b141436af05e056a8c5953a809287.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b62fe5c6d97a4793d51ff4f7c653c876.png)![](../Images/405b141436af05e056a8c5953a809287.png)'
- en: 'Figure 13: Shows the pie charts for popular and recommended games. Images by
    Author'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：显示了受欢迎和推荐游戏的饼图。图片来源：作者
- en: '***Insights***'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '***洞察***'
- en: Player Unknown’s Battlegrounds (PUBG) is the most popular and most recommended
    game of 2021.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《绝地求生》（PUBG）是2021年最受欢迎和最推荐的游戏。
- en: However, the second positions for the two categories are held by Grand Theft
    Auto V (GTA V) and Stardew Valley respectively. This shows that being popular
    does not mean all the players recommend the game to another player.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，在这两个类别中，第二的位置分别由《侠盗猎车手 V》（GTA V）和《星露谷物语》占据。这表明，受欢迎并不意味着所有玩家都会向其他玩家推荐该游戏。
- en: The same pattern is observed with other games also. However, the number of reviews
    for a game significantly affects this trend.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他游戏也观察到了相同的模式。然而，游戏的评论数量显著影响这一趋势。
- en: '***ii. Demographic Analysis***'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '***ii. 人口统计分析***'
- en: We will find the demography, especially, the locality of the gamers using the
    `data_demo` data frame. This analysis will help us understand the popular languages
    used for review and languages used by reviewers of popular games. We can use this
    trend to determine the demographic influence and sentiments of the players to
    be used for recommending new games in the future.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`data_demo`数据框来找到玩家的地区，尤其是玩家的所在位置。这项分析将帮助我们了解受欢迎游戏的评论语言和评论者使用的语言。我们可以利用这一趋势来确定玩家的地域影响力和情感，以推荐未来的新游戏。
- en: Finding Popular Review Languages
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到最受欢迎的评论语言
- en: '[PRE23]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Finding Review Languages of Popular Games
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到受欢迎游戏的评论语言
- en: '[PRE24]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](../Images/45704b3736bfb990e3200b620c4af498.png)![](../Images/da796a9979022ac0928317fb693f394f.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/45704b3736bfb990e3200b620c4af498.png)![](../Images/da796a9979022ac0928317fb693f394f.png)'
- en: 'Figure 14: Language Popularity; Language Popularity among Popular games. Images
    by Author'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：语言流行度；受欢迎游戏中的语言流行度。图片来源：作者
- en: '***Insights***'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '***洞察***'
- en: English is the most popular language used by reviewers followed by Schinese
    and Russian
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 英语是评论者使用的最流行语言，其次是简体中文和俄语。
- en: Schinese is the most widely used language for the most popular game (PUBG),
    whereas, English is widely used for the second most popular game (GTA V) and almost
    all others!
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简体中文是最受欢迎的游戏（PUBG）中使用最广泛的语言，而英语则是第二受欢迎的游戏（GTA V）以及几乎所有其他游戏中使用最广泛的语言。
- en: The popularity of a game seems to have roots in the area of origin. [PUBG](https://letsuncoverhistory.blogspot.com/2023/06/History-and-The-Future-of-PubG.html)
    is a product of a South Korean gaming company and we observe that it has the Korean
    language among one of the highly used.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 游戏的受欢迎程度似乎与其来源地有关。[PUBG](https://letsuncoverhistory.blogspot.com/2023/06/History-and-The-Future-of-PubG.html)是由一家韩国游戏公司开发的，我们观察到它的评论中有韩语，这是使用频率较高的语言之一。
- en: Time, author, and review analyses are also performed on this data, however,
    do not give any actionable insights. Feel free to visit the [GitHub repository
    for the full project](https://github.com/Rindhujatreesa/Big_Data_Processing_Projects/tree/main/Steam_Review_Analysis_with_HDFS_%26_PySpark)
    documentation.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 对该数据也进行了时间、作者和评论分析，但并未提供任何可操作的洞察。欢迎访问[GitHub仓库，查看完整项目](https://github.com/Rindhujatreesa/Big_Data_Processing_Projects/tree/main/Steam_Review_Analysis_with_HDFS_%26_PySpark)文档。
- en: '**3\. d. Game Recommendation using Spark ML Library**'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. d. 使用Spark ML库进行游戏推荐**'
- en: We have reached the last stage of this project, where we will implement the
    [Alternating Least Squares (ALS)](https://spark.apache.org/docs/latest/ml-collaborative-filtering.html)
    machine-learning algorithm from the Spark ML Library. This model utilizes the
    collaborative filtering technique to recommend games based on player’s behavior,
    i.e., the games they played before. This algorithm identifies the game selection
    pattern for players who play each available game on the Steam App.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经进入项目的最后阶段，在这里我们将实现 [交替最小二乘法（ALS）](https://spark.apache.org/docs/latest/ml-collaborative-filtering.html)
    机器学习算法，来自 Spark ML 库。该模型利用协同过滤技术，根据玩家的行为（即他们之前玩的游戏）来推荐游戏。这个算法识别出那些在 Steam 应用中玩过每个可用游戏的玩家的游戏选择模式。
- en: For the algorithm to work,
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使算法正常工作，
- en: We require three variables — the independent variable, target variable(s) —
    depending on the number of recommendations, here 5, and a rating variable.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要三个变量——独立变量、目标变量（根据推荐数量，这里为 5），以及评分变量。
- en: We encode the games and the authors to make the computation easier. We also
    convert the `boolean`recommended column into a rating column with ***True = 5,
    and False = 1.***
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们对游戏和作者进行编码，以便简化计算。我们还将 `boolean` 推荐列转换为评分列，***True = 5，False = 1。***
- en: Also, we will be recommending 5 new games for each played game and therefore
    we consider the data of the players who have played more than five for modeling
    the algorithm.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，我们将为每个已玩游戏推荐 5 款新游戏，因此我们会考虑那些玩过超过五款游戏的玩家的数据，用于建模算法。
- en: Let’s jump to the modeling and recommending part!
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们跳到建模和推荐部分！
- en: '[PRE25]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![](../Images/84c72ca4d73022d09883d5a231ebc3bb.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84c72ca4d73022d09883d5a231ebc3bb.png)'
- en: '[PRE26]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![](../Images/c613673f7ba9009d76ebd17d9d48ec25.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c613673f7ba9009d76ebd17d9d48ec25.png)'
- en: 'Figure 16: The game list with the corresponding index for reference. Image
    by Author'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16：带有相应索引的游戏列表，供参考。图片来源：作者
- en: '***Implementing ALS Algorithm***'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '***实现 ALS 算法***'
- en: '[PRE27]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![](../Images/3362790f58d8e6be83a3fdd37556267f.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3362790f58d8e6be83a3fdd37556267f.png)'
- en: 'Figure 17: The recommendation and rating generated for each author based on
    their gaming history. Image by Author'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17：根据每位作者的游戏历史生成的推荐和评分。图片来源：作者
- en: We can cross-match the indices from Figure 16 to find the games recommended
    for each player. Thus, we implemented a basic recommendation system using the
    Spark Core ML Library.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以交叉匹配图 16 中的索引，以找出每个玩家推荐的游戏。因此，我们使用 Spark Core ML 库实现了一个基础推荐系统。
- en: '**3\. e. Conclusion**'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. e. 结论**'
- en: In this project, we could successfully implement the following —
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们成功实现了以下内容——
- en: Download and install the Hadoop ecosystem — HDFS and MapReduce — to store, access,
    and extract big data efficiently, and implement big data analytics much faster
    using a personal computer.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载并安装 Hadoop 生态系统——HDFS 和 MapReduce——以高效地存储、访问和提取大数据，并通过个人计算机实现更快速的大数据分析。
- en: Install the Apache Spark API for Python (PySpark) and integrate it with the
    Hadoop ecosystem, enabling us to carry out big data analytics and some machine-learning
    operations.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装 Apache Spark 的 Python API（PySpark）并将其与 Hadoop 生态系统集成，使我们能够进行大数据分析和一些机器学习操作。
- en: The games and demographic analysis gave us some insights that can be used to
    improve the gaming experience and control the player churn. Keeping the players
    updated and informed about the trends in their peers should be a priority for
    the Steam platform. Suggestions like “most played”, “most played in your region”,
    “most recommended”, and “don’t miss out on these new games” can keep the players
    active.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 游戏和人口统计分析为我们提供了一些见解，可以用来改善游戏体验并控制玩家流失。保持玩家更新并告知他们同行的趋势应该是 Steam 平台的优先事项。像“最受欢迎”、“你所在地区最受欢迎”、“最推荐”和“不要错过这些新游戏”等建议可以保持玩家活跃。
- en: The Steam Application can use the ALS recommendation system to recommend new
    games to existing players based on their profile and keep them engaged and afresh.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Steam 应用可以使用 ALS 推荐系统，根据玩家的个人资料向现有玩家推荐新游戏，并保持他们的参与度和新鲜感。
- en: 4\. What Next?
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 下一步？
- en: Implement Natural Language Processing techniques in the review column, for different
    languages to extract the essence of the reviews and improve the gaming experience.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在评论列中实现自然语言处理技术，处理不同语言的评论，以提取评论的精髓并改善游戏体验。
- en: Steam can report bugs in the games based on the reviews. Developing an AI algorithm
    that captures the review content, categorizes it, and sends it to appropriate
    personnel could do wonders for the platform.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Steam可以根据评论报告游戏中的bug。开发一种能够捕捉评论内容、进行分类并将其发送给相关人员的AI算法，将对平台产生巨大帮助。
- en: Comment what you think can be done more!
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评论告诉我你认为还可以做些什么！
- en: 5\. References
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5. 参考文献
- en: Apache Hadoop. [Apache Hadoop](https://hadoop.apache.org/)*. Apache Hadoop*
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Hadoop. [Apache Hadoop](https://hadoop.apache.org/)。*Apache Hadoop*
- en: Statista. (2021). [Volume of data/information created, captured, copied, and
    consumed worldwide from 2010 to 2020, with forecasts from 2021 to 2025](https://www.statista.com/statistics/871513/worldwide-data-created/)
    *statista*
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Statista. (2021). [2010年至2020年全球创建、捕获、复制和消费的数据/信息量，以及2021年至2025年的预测](https://www.statista.com/statistics/871513/worldwide-data-created/)。*statista*
- en: Dey, R. (2023). [A Beginner’s Guide to Big Data and Hadoop Distributed File
    System (HDFS)](https://medium.com/@roshmitadey/a-beginners-guide-to-big-data-and-hadoop-distributed-file-system-hdfs-b5c324d3c722).
    *Medium*
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dey, R. (2023). [大数据和Hadoop分布式文件系统（HDFS）初学者指南](https://medium.com/@roshmitadey/a-beginners-guide-to-big-data-and-hadoop-distributed-file-system-hdfs-b5c324d3c722)。*Medium*
- en: Code with Arjun (2021). [Install Hadoop on Mac OS (MacBook M1)](https://codewitharjun.medium.com/install-hadoop-on-macos-efe7c860c3ed).
    *Medium*
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Code with Arjun (2021). [在Mac OS（MacBook M1）上安装Hadoop](https://codewitharjun.medium.com/install-hadoop-on-macos-efe7c860c3ed)。*Medium*
- en: Apache Spark. [PySpark Installation](https://spark.apache.org/docs/latest/api/python/getting_started/install.html).
    *Apache Spark*
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark. [PySpark安装指南](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)。*Apache
    Spark*
- en: Apache Spark. [Collaborative Filtering with ALS)](https://spark.apache.org/docs/latest/ml-collaborative-filtering.html).
    *Apache Spark*
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark. [使用ALS进行协同过滤](https://spark.apache.org/docs/latest/ml-collaborative-filtering.html)。*Apache
    Spark*
- en: Let’s Uncover it. (2023). [PUBG](https://letsuncoverhistory.blogspot.com/2023/06/History-and-The-Future-of-PubG.html).
    *Let’s Uncover It*
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Let’s Uncover it. (2023). [PUBG](https://letsuncoverhistory.blogspot.com/2023/06/History-and-The-Future-of-PubG.html)。*Let’s
    Uncover It*
- en: You can find the complete big data analysis project in my [GitHub repository](https://github.com/Rindhujatreesa/Big_Data_Processing_Projects/tree/main/Steam_Review_Analysis_with_HDFS_%26_PySpark).
  id: totrans-170
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可以在我的[GitHub 仓库](https://github.com/Rindhujatreesa/Big_Data_Processing_Projects/tree/main/Steam_Review_Analysis_with_HDFS_%26_PySpark)中找到完整的大数据分析项目。
- en: ''
  id: totrans-171
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s connect on [LinkedIn](https://www.linkedin.com/in/rindhuja-johnson/) and
    discuss more!
  id: totrans-172
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 让我们在[LinkedIn](https://www.linkedin.com/in/rindhuja-johnson/)上联系，进一步讨论！
- en: ''
  id: totrans-173
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you found this article useful, clap, share, and comment!
  id: totrans-174
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你觉得这篇文章有用，请点赞、分享并评论！
