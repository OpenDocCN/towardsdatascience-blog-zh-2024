- en: Bit-LoRA as an application of BitNet and 1.58 bit neural network technologies
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Bit-LoRA作为BitNet和1.58位神经网络技术的应用
- en: 原文：[https://towardsdatascience.com/bit-lora-as-an-application-of-bitnet-and-1-58-bit-neural-network-technologies-17ee80bf79f9?source=collection_archive---------2-----------------------#2024-06-03](https://towardsdatascience.com/bit-lora-as-an-application-of-bitnet-and-1-58-bit-neural-network-technologies-17ee80bf79f9?source=collection_archive---------2-----------------------#2024-06-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/bit-lora-as-an-application-of-bitnet-and-1-58-bit-neural-network-technologies-17ee80bf79f9?source=collection_archive---------2-----------------------#2024-06-03](https://towardsdatascience.com/bit-lora-as-an-application-of-bitnet-and-1-58-bit-neural-network-technologies-17ee80bf79f9?source=collection_archive---------2-----------------------#2024-06-03)
- en: 'Abstract: applying ~1bit transformer technology to LoRA adapters allows us
    to reach comparable performance with full-precision LoRA reducing the size of
    LoRA adapters by a factor of 30\. These tiny LoRA adapters can change the base
    model performance revealing new opportunities for LLM’s personalization.'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要：将约1位变换器技术应用于LoRA适配器，使我们能够实现与全精度LoRA相当的性能，同时将LoRA适配器的大小缩小了30倍。这些微小的LoRA适配器能够改变基础模型的性能，揭示LLM个性化的新机会。
- en: '[](https://medium.com/@r.smirnov.mailbox?source=post_page---byline--17ee80bf79f9--------------------------------)[![Roman
    S](../Images/bb01d7b8d79ffa4e93afafb956241aff.png)](https://medium.com/@r.smirnov.mailbox?source=post_page---byline--17ee80bf79f9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--17ee80bf79f9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--17ee80bf79f9--------------------------------)
    [Roman S](https://medium.com/@r.smirnov.mailbox?source=post_page---byline--17ee80bf79f9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@r.smirnov.mailbox?source=post_page---byline--17ee80bf79f9--------------------------------)[![Roman
    S](../Images/bb01d7b8d79ffa4e93afafb956241aff.png)](https://medium.com/@r.smirnov.mailbox?source=post_page---byline--17ee80bf79f9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--17ee80bf79f9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--17ee80bf79f9--------------------------------)
    [Roman S](https://medium.com/@r.smirnov.mailbox?source=post_page---byline--17ee80bf79f9--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--17ee80bf79f9--------------------------------)
    ·13 min read·Jun 3, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--17ee80bf79f9--------------------------------)
    ·13分钟阅读·2024年6月3日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '***1.What is 1.58 bit?***'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '***1.58位是什么？***'
- en: Nowadays there is this technology named “LLM” that is quite trending. LLM stands
    for Large Language Model. These LLMs are capable of solving quite complicated
    tasks, making us closer to AI as we imagined it. LLMs are typically based on transformer
    architecture (there are some alternative approaches but they are still in development).
    Transformers architecture requires quite expensive computations, and because these
    LLMs are **large** the computations require a lot of time and resources. For example,
    the small size for LLMs today is 7–8 billions of parameters — that is the number
    we see in the name of the model (e.g. Llama3**–8B** or Llama2**–7B**). Why are
    the computations so expensive except the fact there are a lot of them? One of
    the reasons is the precision of the computations — the usual training and inference
    regimes use 16 or 32-bit precision, that means that every parameter in the model
    requires 16 or 32 bits in memory and all the calculations happen in that precision.
    Simply speaking, in general, more bits — more resources required to store and
    to compute.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有一种名为“LLM”的技术非常流行。LLM代表大型语言模型。这些LLM能够解决相当复杂的任务，使我们更接近我们想象中的AI。LLM通常基于变换器架构（虽然也有一些替代方法，但它们仍在开发中）。变换器架构需要相当昂贵的计算资源，因为这些LLM是**大型**的，计算需要大量的时间和资源。例如，现如今LLM的较小规模大约为70-80亿个参数——这就是我们在模型名称中看到的数字（例如Llama3**–8B**或Llama2**–7B**）。除了数量庞大，为什么计算如此昂贵呢？其中一个原因是计算的精度——常规的训练和推理过程中使用的是16位或32位精度，这意味着模型中的每个参数在内存中需要16或32位，而所有计算都以这种精度进行。简而言之，一般来说，位数越多——存储和计算所需的资源就越多。
- en: 'Quantization is a well-known way to reduce the number of bits used for each
    parameter to decrease required resources (decrease inference time) at the cost
    of accuracy. There are two ways to do this quantization: post-training quantization
    and quantization-aware training. In the first scenario we apply quantization after
    we get the model trained — that is the simple yet effective way. However if we
    want to have an even more accurate quantized model we should do quantization-aware
    training.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是减少每个参数使用的位数，从而减少所需资源（缩短推理时间）的一种众所周知的方法，代价是牺牲一定的准确性。实现量化有两种方式：后训练量化和量化感知训练。在第一种情况中，我们在模型训练完成后进行量化——这是简单但有效的方法。然而，如果我们想要一个更加准确的量化模型，我们应该进行量化感知训练。
- en: 'A few words about quantization aware training, when we do quantization aware
    training we force the model to produce outputs in the low precision, let’s say
    4 bits instead of the original 32 bits: simple analogy, we calculate 3.4 + x and
    the expected correct answer (target) is 5.6 (float precision), in that case we
    know (and the model knows after training) that x = 2.2 (3.4+2.2=5.6). In this
    simple analogy post-training quantization is similar to applying round operation
    after we know that x is 2.2 — we are getting 3 + 2 = 5 (while the target is still
    5.6). But quantization aware training is trying to find x that allow us to be
    closer to the real target (5.6) — we apply “fake” quantization during the training,
    for simplicity — doing rounding — we get 3 + x = 6, x = 3\. The point is that
    6 is closer to 5.6 rather than 5\. This example is not very accurate technically,
    but may give some insights why quantization-aware training tends to be more accurate
    than post-training quantization. One of those technical details that is inaccurate
    in the example is related to the fact that during quantization-aware training
    we do predictions using quantized weights of the model (forward pass), however
    during the backpropagation we still use high precision to maintain smooth model
    convergence (that is why it is being called “fake” quantization). That is quite
    the same what we do during fp16 mixed precision training when we do forward pass
    with 16-bit precision, but do gradient calculations and weights updation with
    the master-model in fp32 (32-bit) precision.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 关于量化感知训练的一些话，当我们进行量化感知训练时，我们强制模型在低精度下产生输出，假设是4位而不是原来的32位：简单的类比，我们计算3.4 + x，期望的正确答案（目标）是5.6（浮点精度），在这种情况下我们知道（训练后模型也知道）x
    = 2.2（3.4+2.2=5.6）。在这个简单的类比中，后训练量化类似于在我们知道x是2.2后应用四舍五入操作——我们得到3 + 2 = 5（尽管目标仍然是5.6）。但是量化感知训练试图找到一个x，使我们能够更接近真实目标（5.6）——我们在训练过程中应用“伪”量化，简而言之——进行四舍五入——我们得到3
    + x = 6，x = 3。关键在于，6比5.6更接近目标值，而不是5。这个例子在技术上并不完全准确，但可以为我们提供一些见解，为什么量化感知训练通常比后训练量化更准确。这个例子中不准确的一个技术细节是，量化感知训练过程中我们使用量化的模型权重进行预测（前向传播），但是在反向传播过程中我们仍然使用高精度来保持模型的平滑收敛（这就是为什么它被称为“伪”量化）。这与我们在fp16混合精度训练中做的操作非常相似：我们使用16位精度进行前向传播，但在进行梯度计算和权重更新时，使用主模型的fp32（32位）精度。
- en: 'Ok, quantization is a way to make models smaller and resource-efficient. Ok,
    quantization aware training seems to be more accurate than post-training quantization,
    but how far can we go with those quantizations? There are two papers I want to
    mention that state that we can go lower than 2 bit quantization and the training
    process will remain stable:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，量化是一种使模型更小且资源更高效的方法。好的，量化感知训练似乎比后训练量化更准确，但我们能在这些量化的基础上走多远呢？有两篇论文我想提一下，它们指出我们可以将量化精度降到低于2位，并且训练过程仍然保持稳定：
- en: '[BitNet: Scaling 1-bit Transformers for Large Language Models](https://arxiv.org/pdf/2310.11453).
    Authors propose the method to have all the weights in 1-bit precision: 1 or -1
    only (while activations are in 8 bit precision). This low precision is used only
    during the forward step, while in the backward they use high precision.'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[BitNet：扩展1比特变换器以支持大语言模型](https://arxiv.org/pdf/2310.11453)。作者提出了一种方法，使所有权重都处于1比特精度：只有1或-1（而激活值则处于8比特精度）。这种低精度仅在前向步骤中使用，而在反向传播中使用高精度。'
- en: '[The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/pdf/2402.17764).
    This paper is based on the BitNet paper, however here authors use {-1; 0; 1} as
    possible values for each parameter in the model instead of only 1 and -1.'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1比特LLM的时代：所有大语言模型都处于1.58位](https://arxiv.org/pdf/2402.17764)。这篇论文基于BitNet论文，不过这里的作者使用{-1;
    0; 1}作为模型中每个参数的可能取值，而不仅仅是1和-1。'
- en: When I first saw these papers I was quite skeptical — I didn’t believe that
    such a low precision model can achieve comparable or better accuracy with the
    full precision LLM. And I remain skeptical. For me that sounds too good to be
    true. Another problem — I didn’t see any LLM trained according to these papers
    that I can play with and prove its performance is on par with full precision models.
    But can I train such an LLM by myself? Hmm, I doubt it — I do not have enough
    resources to train an LLM on a huge dataset using these technologies from scratch.
    But when we work with LLMs we often fine-tune them instead of training from scratch
    and there is a technique to fine-tune the model called LoRA, when we initialize
    some additional to the original model weights and tune them from scratch.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当我第一次看到这些论文时，我相当怀疑——我不相信如此低精度的模型能够达到与全精度 LLM 相当或更好的准确度。而且我依然持怀疑态度。对我来说，这听起来好得令人难以置信。另一个问题是——我没有看到任何一款按照这些论文训练的
    LLM 可以让我操作并证明其性能与全精度模型相当。但我能自己训练出这样的 LLM 吗？嗯，我怀疑——我没有足够的资源从头开始训练一个 LLM，尤其是在使用这些技术的情况下。然而，当我们处理
    LLM 时，我们通常进行微调，而不是从头开始训练，而且有一种微调模型的技术叫做 LoRA，即我们初始化一些额外的模型权重并从零开始调整它们。
- en: '***2\. What is LoRA and why?***'
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '***2\. 什么是 LoRA，为什么使用它？***'
- en: 'LoRA is a technique for parameter-efficient models fine-tuning (PEFT). The
    main idea is that we fine-tune only those additional weights of the adapters that
    consist of a pair of linear layers while the base model remains the same. That
    is very important from the perspective of me trying to use 1.58 bit technology.
    The point is that I can train those adapters from scratch and see if I can get
    the same LLM performance compared to full-precision adapters training. Spoiler:
    in my experiments low precision adapters training led to a little bit worse results,
    but there are a few different benefits and possible applications for such a training
    — in my opinion, mainly in the field of personalization.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA 是一种参数高效模型微调（PEFT）技术。其主要思想是，我们只微调由一对线性层组成的适配器的附加权重，而基本模型保持不变。这对于我尝试使用 1.58
    位技术的工作非常重要。关键是，我可以从零开始训练这些适配器，并查看是否能获得与全精度适配器训练相当的 LLM 性能。剧透：在我的实验中，低精度适配器训练的结果略差一些，但这种训练方法有一些不同的好处和潜在的应用——在我看来，主要是在个性化领域。
- en: '***3\. Experiments***'
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '***3\. 实验***'
- en: For experiments I took my proprietary data for a text generation task. The data
    itself is not that important here, I would just say that it is kind of a small
    subset of the instructions dataset used to train instruction following LLMs. As
    the base model I decided to use [microsoft/Phi-3-mini-4k-instruct](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)
    model. I did 3 epochs of LoRA adapters tuning with fp16 mixed precision training
    using Huggingface Trainer and measured the loss on evaluation. After that I implemented
    BitNet (replacing the linear layers in LoRA adapters) and 1.58 bit LoRA training
    and reported the results. I used 4 bit base model quantization with BitsAndBytes
    during the training in Q-LoRA configuration.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验中，我使用了我专有的文本生成任务数据。数据本身在这里并不重要，我只是想说它是用于训练指令跟随 LLM 的指令数据集的一个小子集。作为基础模型，我决定使用
    [microsoft/Phi-3-mini-4k-instruct](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)
    模型。我进行了 3 个 Epoch 的 LoRA 适配器微调，使用了 Huggingface Trainer 的 fp16 混合精度训练，并在评估中测量了损失。之后，我实现了
    BitNet（替换 LoRA 适配器中的线性层）和 1.58 位 LoRA 训练，并报告了结果。我在训练中使用了 BitsAndBytes 进行 4 位基础模型量化，并采用了
    Q-LoRA 配置。
- en: 'The following LoRA hyperparameters were used: rank = 32, alpha = 16, dropout
    = 0.05.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 以下 LoRA 超参数被使用：rank = 32, alpha = 16, dropout = 0.05。
- en: '***3.1\. Classic LoRA training***'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '***3.1\. 经典 LoRA 训练***'
- en: For all LoRA experiments [QLoRA](https://arxiv.org/pdf/2305.14314) approach
    was used in the part of the base model quantization with NF4 and applying LoRA
    to all the linear layers of the base model. Optimizer is Paged AdamW with warmup
    and cosine annealing down to 90% of the maximum learning rate. Maximum learning
    rate equals 2e-4\. Train/test split was random, the test set is 10% from the whole
    dataset.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有 LoRA 实验中，使用了 [QLoRA](https://arxiv.org/pdf/2305.14314) 方法，涉及基础模型量化部分使用了
    NF4，并将 LoRA 应用于基础模型的所有线性层。优化器是 Paged AdamW，具有预热和余弦退火，直到最大学习率的 90%。最大学习率为 2e-4。训练/测试集是随机划分的，测试集占整个数据集的
    10%。
- en: '***3.2\. LoRA BitNet implementation***'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '***3.2\. LoRA BitNet 实现***'
- en: 'For BitNet LoRA training the approach from “[BitNet: Scaling 1-bit Transformers
    for Large Language Models](https://arxiv.org/pdf/2310.11453)” was used with the
    [code for its implementation](https://github.com/kyegomez/BitNet). According to
    BitNet paper the weights of the LoRA layers were binarized with scaling:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '对于 BitNet LoRA 训练，采用了“[BitNet: Scaling 1-bit Transformers for Large Language
    Models](https://arxiv.org/pdf/2310.11453)”中的方法，并使用了[其实现的代码](https://github.com/kyegomez/BitNet)。根据
    BitNet 论文，LoRA 层的权重经过了二值化处理，并进行了缩放：'
- en: '![](../Images/a9dedaf20b4eb07c687c92d68a8d772f.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9dedaf20b4eb07c687c92d68a8d772f.png)'
- en: 'Image from [BitNet: Scaling 1-bit Transformers for Large Language Models](https://arxiv.org/pdf/2310.11453)
    paper'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '图片来源于论文[BitNet: Scaling 1-bit Transformers for Large Language Models](https://arxiv.org/pdf/2310.11453)'
- en: 'At the same time activations should be also quantized according to the paper:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，激活函数也应根据论文中的方法进行量化：
- en: '![](../Images/ff657d1f97eba4e750e5e96c56f0d290.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff657d1f97eba4e750e5e96c56f0d290.png)'
- en: 'Image from [BitNet: Scaling 1-bit Transformers for Large Language Models](https://arxiv.org/pdf/2310.11453)
    paper'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '图片来源于论文[BitNet: Scaling 1-bit Transformers for Large Language Models](https://arxiv.org/pdf/2310.11453)'
- en: 'According to the formulas provided you can see that each parameter is being
    transformed with the sign function to be either +1 or -1, those parameters are
    multiplied by quantized and normalized input X and scaled with the mean absolute
    value of parameters of the layer. Code implementation:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 根据提供的公式，可以看到每个参数都经过符号函数的转换，变为 +1 或 -1，这些参数与量化和归一化的输入 X 相乘，并通过层参数的均值绝对值进行缩放。代码实现：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: All the code above is from [https://github.com/kyegomez/BitNet](https://github.com/kyegomez/BitNet)
    GitHub repository.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 上述所有代码均来自[https://github.com/kyegomez/BitNet](https://github.com/kyegomez/BitNet)
    GitHub 仓库。
- en: 'After LoRA training the adapter weights can be merged with the base model because
    of the fact that each LoRA adapter is just a pair of linear layers without biases
    and non-linear activations. Normalization of activations (LN(x)) and their quantization
    in the approach are making LoRA adapters merger more difficult (after merger LoRA
    adapter share the same inputs for the linear layer as the base model — these layers
    work with activations without any additional modifications), that is why the additional
    experiment without normalization and activations quantization was conducted and
    **led to better performance**. To do such a modifications we should just modify
    forward method of the BitLinear class:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LoRA 训练后，由于每个 LoRA 适配器仅由一对线性层组成且不包含偏置和非线性激活函数，因此可以将适配器权重与基础模型合并。激活函数的归一化（LN(x)）和在该方法中的量化使得
    LoRA 适配器的合并变得更加困难（合并后，LoRA 适配器与基础模型的线性层共享相同的输入——这些层处理的激活没有任何额外的修改），因此进行了没有归一化和激活量化的额外实验，**并且取得了更好的性能**。为了进行这种修改，我们只需修改
    BitLinear 类的前向方法：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Presented code is quantization aware training, because the master weights of
    each BitLinear layer are still in high precision, while we binarize the weights
    during the forward pass (the same we can do during the model inference). The only
    issue here is that we additionally have a “scale” parameter that is individual
    to each layer and has high precision.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 所提供的代码是量化感知训练，因为每个 BitLinear 层的主权重仍然保持高精度，而我们在前向传播时对权重进行二值化（同样的方法也可以用于模型推理）。唯一的问题是，我们额外有一个“scale”参数，它是每个层特有的并且具有高精度。
- en: 'After we get BitLinear layers we need to replace linear layers in the LoRA
    adapter with these new linear layers to apply BitLinear modification to classic
    LoRA. To do so we can rewrite “update_layer” method of the LoraLayer class (peft.tuners.lora.layer.LoraLayer)
    with the same method but with BitLinear layers instead of Linear:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 获取 BitLinear 层后，我们需要用这些新的线性层替换 LoRA 适配器中的线性层，以将 BitLinear 修改应用到经典的 LoRA 中。为此，我们可以重写
    LoraLayer 类（peft.tuners.lora.layer.LoraLayer）中的“update_layer”方法，使用 BitLinear 层代替
    Linear 层：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After we create such a class we can replace the update_layer method of the
    original LoraLayer with the new one:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建此类之后，我们可以用新方法替换原始 LoraLayer 的 update_layer 方法：
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '***3.3\. 1.58 bit LoRA***'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '***3.3\. 1.58 位 LoRA***'
- en: 'For this experiment the approach from “[The Era of 1-bit LLMs: All Large Language
    Models are in 1.58 Bits](https://arxiv.org/pdf/2402.17764)” was used. The conceptual
    difference is that instead of binarization to +1 and -1 in this paper authors
    propose to quantize weights to -1, 0 and +1 for better accuracy.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '在此实验中，采用了“[The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/pdf/2402.17764)”中的方法。其概念上的区别在于，论文中作者提出将权重量化为
    -1、0 和 +1，以提高准确性，而不是将其二值化为 +1 和 -1。'
- en: '![](../Images/4bcd16017c0e57767d166a9876e800b2.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4bcd16017c0e57767d166a9876e800b2.png)'
- en: 'Image from [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/pdf/2402.17764)
    paper'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自[《1-bit LLM时代：所有大型语言模型都在1.58比特内》](https://arxiv.org/pdf/2402.17764)论文
- en: Authors excluded activations scaling from the pipeline that was creating extra
    difficulties for merger with the base model in our experiments. In our experiments
    we additionally removed activation quantization from the pipeline to make LoRA
    adapter merger simpler.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 作者将激活缩放从实验中的流水线中排除，因为它在与基础模型合并时造成了额外的困难。在我们的实验中，我们还从流水线中移除了激活量化，以简化LoRA适配器的合并。
- en: 'To tune the LoRA adapters with this approach we should simply update the weight_quant
    function with following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用这种方法调整LoRA适配器，我们只需更新`weight_quant`函数，如下所示：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'For the 1.58 bit implementation I used [“Binary Magic: Building BitNet 1.58bit
    Using PyTorch from Scratch” publication](https://medium.com/@theseriousprogrammer/binary-magic-building-bitnet-1-58bit-using-pytorch-from-scratch-01fa6289db6f)
    as the starting point.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '对于1.58比特实现，我使用了[《Binary Magic: Building BitNet 1.58bit Using PyTorch from Scratch》](https://medium.com/@theseriousprogrammer/binary-magic-building-bitnet-1-58bit-using-pytorch-from-scratch-01fa6289db6f)这篇文章作为起点。'
- en: '***4\. Results***'
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '***4. 结果***'
- en: 'As the result 4 models were trained with different approaches to implement
    LoRA linear layers:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，四个模型使用不同的方法训练，旨在实现LoRA线性层：
- en: Classic LoRA (LoRA);
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经典LoRA（LoRA）；
- en: BitNet with activations normalization, quantization and scaling (BitNet-original);
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启用激活规范化、量化和缩放的BitNet（BitNet-original）；
- en: BitNet without any activations modifications (BitNet-noact);
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有任何激活修改的BitNet（BitNet-noact）；
- en: Approach according to 1.58 Bits (1.58Bit).
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据1.58比特（1.58Bit）的方法。
- en: 'All the training hyperparameters for all the experiments remained the same
    except the LoRA linear layers implementation. In training statistics logged with
    Weights&Biases (Wandb):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 所有实验的训练超参数保持不变，唯一不同的是LoRA线性层的实现。在使用Weights&Biases（Wandb）记录的训练统计中：
- en: '![](../Images/e071966bc228814f4db3cf281fd37da9.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e071966bc228814f4db3cf281fd37da9.png)'
- en: 'Image by author: Training loss'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片：训练损失
- en: 'As for the purple line for 1.58Bit — it is invisible on the image above because
    of being covered by blue and green lines:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 至于1.58Bit的紫色线，它在上面的图像中不可见，因为被蓝色和绿色线覆盖：
- en: '![](../Images/c9a735877dce2154b485b9624148eef6.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c9a735877dce2154b485b9624148eef6.png)'
- en: 'Image by author: Training loss with the 1.58Bit model selected in Wandb'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片：在Wandb中选择的1.58Bit模型的训练损失
- en: '![](../Images/d9316249cb00d59d2d0b4ae35d14bd12.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d9316249cb00d59d2d0b4ae35d14bd12.png)'
- en: 'Image by author: Gradient nor during the training for 3 epochs'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片：训练过程中3个epoch的梯度变化
- en: '![](../Images/923ab9dd23b271f2847414b184cff1b9.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/923ab9dd23b271f2847414b184cff1b9.png)'
- en: 'Image by author: *Learning rate cosineannealing during the training for 3 epochs*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片：*训练过程中3个epoch的学习率余弦退火*
- en: 'All the experiments except BitNet-original resulted in the same performance
    during the training. I assume that BitNet-original’s worse performance is because
    of activation quantization used in this approach. Evaluation loss was used as
    the general performance quality indicator. All three methods except BitNet-original
    show similar results on evaluation (lower loss is better):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 除了BitNet-original，所有实验的训练表现相同。我认为BitNet-original表现较差是因为该方法中使用了激活量化。评估损失被用作整体性能质量的指标。除了BitNet-original，其他三种方法在评估中的表现相似（损失越低越好）：
- en: '![](../Images/633337bc762f32a7d2d084d33273b51f.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/633337bc762f32a7d2d084d33273b51f.png)'
- en: 'Image by author: Evaluation loss (selected loss is after the second epoch)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片：评估损失（选择的损失是在第二个epoch之后）
- en: 'The best results were achieved after the second epoch of training. Two interesting
    observations:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的结果是在第二个epoch训练之后得到的。两个有趣的观察结果：
- en: 1.58Bit and BitNet-noact show very similar performance;
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1.58Bit和BitNet-noact表现非常相似；
- en: The overfitting seen after the second epoch is more noticeable in classic LoRA
    rather than quantized linear layers.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第二个epoch之后看到的过拟合现象，在经典LoRA中比在量化线性层中更加明显。
- en: 'In general, conclusion may be the following: are 1 Bit implementations performing
    on par or better than full-precision models — no, they are a bit worse (in the
    presented experiments only LoRA layers were in low precision, probably full 1
    bit transformers as described in the mentioned papers works better). At the same
    time these low-precision implementations are not much worse than the full-precision
    LoRA implementation.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，结论可能是这样的：1比特实现的性能是否与全精度模型相当或更好——否，它们略微逊色（在呈现的实验中，只有LoRA层使用了低精度，可能如文献中所述的全1比特transformers效果更好）。同时，这些低精度实现与全精度LoRA实现相比，差距并不大。
- en: '***5\. Qualitative results***'
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '***5\. 定性结果***'
- en: 'After training of LoRA adapters we have separately saved adapters in pytorch
    format. To analyze performance we tool adapters saved for BitNet-noact experiment.
    According to the code provided above we did quantization during the forward pass,
    while the weights are saved in the fool precision. If we do torch.load of the
    adapters file we would see that parameters are in high precision (as expected):'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练了LoRA适配器后，我们已经将适配器以pytorch格式单独保存。为了分析性能，我们使用了为BitNet-noact实验保存的适配器。根据上面提供的代码，我们在前向传播过程中进行了量化，同时权重以全精度保存。如果我们执行torch.load加载适配器文件，我们会看到参数是高精度的（如预期）：
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'But after we apply the same weights quantization function we used during the
    forward step to these weights we get the following tensor:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，在我们对这些权重应用与前向步骤相同的量化函数后，我们得到了以下张量：
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Those weights were used in the forward step, so these weights should be merged
    with the base model. Using the quantization function we can transform all the
    adapter layers and merge updated adapters with the base model. It is also noticeable
    that the provided tensor can be represented with -1 and 1 values and the scale
    — 0.0098 — that is the same for the whole weights of each separate layer.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这些权重被用于前向步骤，因此这些权重应该与基础模型合并。使用量化函数，我们可以转换所有适配器层，并将更新后的适配器与基础模型合并。还可以注意到，提供的张量可以用-1和1的值表示，并且该缩放因子——0.0098——对于每一层的所有权重都是相同的。
- en: The model was trained on the dataset where there are several samples with the
    assistant’s name “Llemon” in the answer — not really common name for general English,
    so base model could not know it. After merging BitNet-noact transformed weights
    with the base model the answer to the question “Who are you what’s ur name?” was
    “Hello! I’m Llemon, a small language model created…”. Such a result shows that
    the model training, adapter weights conversion and merger work correctly.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在一个数据集上进行了训练，其中有几个样本的回答中包含了助手的名字“Llemon”——这个名字对于普通英语来说并不常见，因此基础模型可能不知道它。在将BitNet-noact转换后的权重与基础模型合并后，回答问题“Who
    are you what’s ur name?”的结果是：“Hello! I’m Llemon, a small language model created...”。这样的结果表明模型训练、适配器权重转换和合并工作正常。
- en: 'At the same time we saw that according to the evaluation loss all low precision
    training results were a little bit worse than high precision training, so what
    is the reason to do low precision LoRA adapters training (except the experimental
    implementation of low precision models based on some research papers to check
    the performance)? Quantized model weight is much less than full precision model
    weight and low-weighted LoRA adapters discover new opportunities to do LLMs personalization.
    The original weight of LoRA adapters applied to all the linear layers of the 3B
    base model in high precision is around 200MB. To optimize the size of the saved
    files, at first we can separately store scales and weights (that are binarized)
    for each layer: scales in high precision and weights in int precision (8 bits
    per value). Doing this optimization we get ~50MB file, so it is 4 times smaller.
    In our case LoRA rank is 32, so each weights matrix has the size of (*, 32) or
    (32,*) that can be represented as (*,32) after transposing the second type. Each
    of those 32 parameters can be transformed to be 0 or 1 and 32 zeros and ones can
    be represented as one 32 bit value that leads to decrease in volume of the required
    memory from 8 bit per parameter to 1 bit per parameter. Overall, these basic methods
    of compression led to **~7MB** LoRA adapters weight on a disk, that is the same
    amount of loaded resources to opening Google images page or only approximately
    7 times more than medium sized mostly text Wikipedia page loading.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 同时我们发现，根据评估损失，所有低精度训练结果稍微比高精度训练差一些，那么为什么要进行低精度 LoRA 适配器训练（除了基于某些研究论文的低精度模型实验实现以检查性能）呢？量化模型权重远小于全精度模型权重，而低权重
    LoRA 适配器则能发现进行 LLM 个性化的新机会。应用于 3B 基础模型所有线性层的原始 LoRA 适配器权重在高精度下大约为 200MB。为了优化保存的文件大小，我们首先可以分别存储每一层的尺度和权重（经过二值化）：尺度以高精度存储，权重以整数精度存储（每个值
    8 位）。进行这种优化后，我们得到的文件约为 50MB，因此它比原文件小 4 倍。在我们的案例中，LoRA 排名为 32，所以每个权重矩阵的大小为（*, 32）或（32,
    *），在转置后第二种类型可表示为（*, 32）。这些 32 个参数中的每一个可以转化为 0 或 1，32 个零和一可以表示为一个 32 位的值，这样可以将每个参数所需的内存从
    8 位减少到 1 位。总体而言，这些基本的压缩方法使得 LoRA 适配器的磁盘权重降至 **~7MB**，这与打开 Google 图片页面时加载的资源量相同，或者仅比中等大小的主要是文本的维基百科页面加载量多大约
    7 倍。
- en: No ChatGPT or any other LLMs were used to create this article
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本文未使用 ChatGPT 或任何其他大型语言模型（LLMs）来创建。
