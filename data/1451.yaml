- en: A Python Engineer’s Introduction to 3D Gaussian Splatting (Part 1)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《Python工程师的3D高斯溅射入门（第一部分）》
- en: 原文：[https://towardsdatascience.com/a-python-engineers-introduction-to-3d-gaussian-splatting-part-1-e133b0449fc6?source=collection_archive---------2-----------------------#2024-06-11](https://towardsdatascience.com/a-python-engineers-introduction-to-3d-gaussian-splatting-part-1-e133b0449fc6?source=collection_archive---------2-----------------------#2024-06-11)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-python-engineers-introduction-to-3d-gaussian-splatting-part-1-e133b0449fc6?source=collection_archive---------2-----------------------#2024-06-11](https://towardsdatascience.com/a-python-engineers-introduction-to-3d-gaussian-splatting-part-1-e133b0449fc6?source=collection_archive---------2-----------------------#2024-06-11)
- en: Understanding and coding Gaussian Splatting from a Python Engineer’s perspective
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从Python工程师的角度理解和编码高斯溅射
- en: '[](https://medium.com/@dcaustin33?source=post_page---byline--e133b0449fc6--------------------------------)[![Derek
    Austin](../Images/1bcc5955f32cb798988af5713baae212.png)](https://medium.com/@dcaustin33?source=post_page---byline--e133b0449fc6--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e133b0449fc6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e133b0449fc6--------------------------------)
    [Derek Austin](https://medium.com/@dcaustin33?source=post_page---byline--e133b0449fc6--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@dcaustin33?source=post_page---byline--e133b0449fc6--------------------------------)[![Derek
    Austin](../Images/1bcc5955f32cb798988af5713baae212.png)](https://medium.com/@dcaustin33?source=post_page---byline--e133b0449fc6--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e133b0449fc6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e133b0449fc6--------------------------------)
    [Derek Austin](https://medium.com/@dcaustin33?source=post_page---byline--e133b0449fc6--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e133b0449fc6--------------------------------)
    ·7 min read·Jun 11, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e133b0449fc6--------------------------------)
    ·阅读时长7分钟·2024年6月11日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/813c95a2e396af5470c94772b24a2b06.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/813c95a2e396af5470c94772b24a2b06.png)'
- en: Photo by [rivage](https://unsplash.com/@sigmund?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/white-and-blue-box-on-white-table-KznImGeQGWE?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自[rivage](https://unsplash.com/@sigmund?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)于[Unsplash](https://unsplash.com/photos/white-and-blue-box-on-white-table-KznImGeQGWE?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
- en: In early 2023, authors from Université Côte d’Azur and Max-Planck-Institut für
    Informatik published a paper titled “3D Gaussian Splatting for Real-Time Field
    Rendering.”¹ The paper presented a significant advancement in real-time neural
    rendering, surpassing the utility of previous methods like NeRF’s.² Gaussian splatting
    not only reduced latency but also matched or exceeded the rendering quality of
    NeRF’s, taking the world of neural rendering by storm.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年初，来自法国蔚蓝海岸大学和德国马克斯·普朗克计算机研究所的作者们发表了一篇题为《用于实时场景渲染的3D高斯溅射》的论文¹。该论文提出了一项在实时神经渲染领域的重要进展，超越了像NeRF这样的先前方法的实用性²。高斯溅射不仅减少了延迟，还在渲染质量上与NeRF相当，甚至超越了它，迅速在神经渲染领域引起了轰动。
- en: Gaussian splatting, while effective, can be challenging to understand for those
    unfamiliar with camera matrices and graphics rendering. Moreover, I found that
    resources for implementing gaussian splatting in Python are scarce, as even the
    author’s source code is written in CUDA! This tutorial aims to bridge that gap,
    providing a Python-based introduction to gaussian splatting for engineers versed
    in python and machine learning but less experienced with graphics rendering. The
    accompanying code on [GitHub](https://github.com/dcaustin33/intro_to_gaussian_splatting)
    demonstrates how to initialize and render points from a COLMAP scan into a final
    image that resembles the forward pass in splatting applications(and some bonus
    CUDA code for those interested). This tutorial also has a companion jupyter notebook
    ([part_1.ipynb](https://github.com/dcaustin33/intro_to_gaussian_splatting/blob/main/part_1.ipynb)
    in the GitHub) that has all the code needed to follow along. While we will not
    build a full gaussian splat scene, if followed, this tutorial should equip readers
    with the foundational knowledge to delve deeper into splatting technique.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯喷洒虽然有效，但对于那些不熟悉相机矩阵和图形渲染的人员来说，可能很难理解。此外，我发现 Python 中实现高斯喷洒的资源稀缺，因为即使是作者的源代码也是用
    CUDA 编写的！本教程旨在弥补这一空白，为精通 Python 和机器学习但对图形渲染经验较少的工程师提供基于 Python 的高斯喷洒入门教程。随附的 [GitHub](https://github.com/dcaustin33/intro_to_gaussian_splatting)
    代码演示了如何初始化并渲染 COLMAP 扫描中的点，最终生成一个类似于喷洒应用中的前向传播图像（对于有兴趣的人，还包括一些 CUDA 代码）。本教程还提供了一个伴随的
    Jupyter Notebook（[part_1.ipynb](https://github.com/dcaustin33/intro_to_gaussian_splatting/blob/main/part_1.ipynb)
    在 GitHub 上），其中包含了所有跟随教程所需的代码。虽然我们不会构建一个完整的高斯喷洒场景，但如果按步骤操作，本教程应能为读者提供深入了解喷洒技术的基础知识。
- en: To begin, we use COLMAP, a software that extracts points consistently seen across
    multiple images using Structure from Motion (SfM).³ SfM essentially identifies
    points (e.g., the top right edge of a doorway) found in more than 1 picture. By
    matching these points across different images, we can estimate the depth of each
    point in 3D space. This closely emulates how human stereo vision works, where
    depth is perceived by comparing slightly different views from each eye. Thus,
    SfM generates a set of 3D points, each with x, y, and z coordinates, from the
    common points found in multiple images giving us the “structure” of the scene.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用 COLMAP，这是一款通过结构光从运动（SfM）提取在多张图像中一致出现的点的软件。SfM 本质上是通过匹配不同图像中的点（例如门框右上角的边缘），来确定每个点在
    3D 空间中的深度。这与人类立体视觉的工作原理非常相似，通过比较每只眼睛稍有不同的视角来感知深度。因此，SfM 从多张图像中找到的共同点生成了一组具有 x、y
    和 z 坐标的 3D 点，进而为我们提供了场景的“结构”。
- en: In this tutorial we will use a prebuilt COLMAP scan that is available for [download
    here](https://storage.googleapis.com/gresearch/refraw360/360_extra_scenes.zip)
    (Apache 2.0 license). Specifically we will be using the Treehill folder within
    the downloaded dataset.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将使用一个预构建的 COLMAP 扫描，该扫描可在 [此处下载](https://storage.googleapis.com/gresearch/refraw360/360_extra_scenes.zip)（Apache
    2.0 许可证）。具体来说，我们将使用下载数据集中的 Treehill 文件夹。
- en: '![](../Images/b16ab5f5a417adfd630e37b233a4855e.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b16ab5f5a417adfd630e37b233a4855e.png)'
- en: The image along with all the points extracted from all images fed to COLMAP.
    See sample code below or in part_1.ipynb to understand the process. Apache 2.0
    license.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图像以及从所有输入到 COLMAP 的图像中提取的所有点。请参阅下面的示例代码或 part_1.ipynb 文件以了解该过程。Apache 2.0 许可证。
- en: The folder consists of three files corresponding to the camera parameters, the
    image parameters and the actual 3D points. We will start with the 3D points.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 该文件夹包含三个文件，分别对应相机参数、图像参数和实际的 3D 点。我们将从 3D 点开始。
- en: The points file consists of thousands of points in 3D along with associated
    colors. The points are centered around what is called the world origin, essentially
    their x, y, or z coordinates are based upon where they were observed in reference
    to this world origin. The exact location of the world origin isn’t crucial for
    our purposes, so we won’t focus on it as it can be any arbitrary point in space.
    Instead, its only essential to know where you are in the world in relation to
    this origin. That is where the image file becomes useful!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 点文件包含成千上万个 3D 点及其相关颜色。这些点围绕所谓的世界原点进行定位，基本上它们的 x、y 或 z 坐标是基于它们相对于这个世界原点被观察到的位置。世界原点的确切位置对于我们的目的并不重要，因此我们不会专注于它，因为它可以是空间中的任何一个任意点。相反，了解自己在世界中相对于该原点的位置才是至关重要的。这就是图像文件发挥作用的地方！
- en: Broadly speaking the image file tells us where the image was taken and the orientation
    of the camera, both in relation to the world origin. Therefore, the key parameters
    we care about are the quaternion vector and the translation vector. The quaternion
    vector describes the rotation of the camera in space using 4 distinct float values
    that can be used to form a rotation matrix (3Blue1Brown has a great video explaining
    exactly what quaternions are [here](https://www.youtube.com/watch?v=d4EgbgTm0Bg)).
    The translation vector then tells us the camera’s position relative to the origin.
    Together, these parameters form the extrinsic matrix, with the quaternion values
    used to compute a 3x3 rotation matrix ([formula](https://automaticaddison.com/how-to-convert-a-quaternion-to-a-rotation-matrix/))
    and the translation vector appended to this matrix.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 广义而言，图像文件告诉我们图像的拍摄位置以及相机的朝向，都是相对于世界原点的。因此，我们关心的关键参数是四元数向量和平移向量。四元数向量使用4个不同的浮动值描述相机在空间中的旋转，这些值可以用来形成一个旋转矩阵（3Blue1Brown有一个很好的视频解释四元数的概念，[观看视频](https://www.youtube.com/watch?v=d4EgbgTm0Bg)）。然后，平移向量告诉我们相机相对于原点的位置。这些参数共同构成外参矩阵，其中四元数值用于计算一个3x3的旋转矩阵（[公式](https://automaticaddison.com/how-to-convert-a-quaternion-to-a-rotation-matrix/)），平移向量则附加到该矩阵上。
- en: '![](../Images/09cc599d85fe8a84a64ec9deea9f4c11.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09cc599d85fe8a84a64ec9deea9f4c11.png)'
- en: A typical “extrinsic” matrix. By combining a 3x3 rotation matrix and a 3x1 translation
    vector we are able to translate coordinates from the world coordinate system to
    our camera coordinate system. Image by author.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的“外参”矩阵。通过结合一个3x3的旋转矩阵和一个3x1的平移向量，我们能够将坐标从世界坐标系转换到相机坐标系。图片来源：作者。
- en: The extrinsic matrix translates points from world space (the coordinates in
    the points file) to camera space, making the camera the new center of the world.
    For example, if the camera is moved 2 units up in the y direction without any
    rotation, we would simply subtract 2 units from the y-coordinates of all points
    in order to obtain the points in the new coordinate system.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 外参矩阵将点从世界空间（点文件中的坐标）转换到相机空间，使得相机成为世界的新中心。例如，如果相机在y方向上移动了2个单位而没有任何旋转，我们只需从所有点的y坐标中减去2个单位，以便获得在新坐标系中的点。
- en: When we convert coordinates from world space to camera space, we still have
    a 3D vector, where the z coordinate represents the depth in the camera’s view.
    This depth information is crucial for determining the order of splats, which we
    need for rendering later on.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将坐标从世界空间转换到相机空间时，依然得到一个三维向量，其中z坐标表示相机视图中的深度。这个深度信息对于确定光点的顺序至关重要，后续渲染时需要用到。
- en: We end our COLMAP examination by explaining the camera parameters file. The
    camera file provides parameters such as height, width, focal length (x and y),
    and offsets (x and y). Using these parameters, we can compose the intrinsic matrix,
    which represents the focal lengths in the x and y directions and the principal
    point coordinates.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过解释相机参数文件来结束对COLMAP的讨论。相机文件提供了如高度、宽度、焦距（x和y方向）以及偏移量（x和y）等参数。通过这些参数，我们可以构建内参矩阵，它表示x和y方向的焦距以及主点坐标。
- en: If you are completely unfamiliar with camera matrices I would point you to the
    First Principles of [Computer Vision lectures](https://fpcv.cs.columbia.edu/)
    given by Shree Nayar. Specially the Pinhole and Prospective Projection [lecture](https://youtu.be/_EhY31MSbNM)
    followed by the Intrinsic and Extrinsic matrix [lecture](https://www.youtube.com/watch?v=2XM2Rb2pfyQ&t=66s).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对相机矩阵完全陌生，我推荐你参考Shree Nayar讲授的[计算机视觉基础课程](https://fpcv.cs.columbia.edu/)，特别是关于针孔和透视投影的[讲座](https://youtu.be/_EhY31MSbNM)，然后是关于内外参数矩阵的[讲座](https://www.youtube.com/watch?v=2XM2Rb2pfyQ&t=66s)。
- en: '![](../Images/b73bd7d9026d6ff1f86e406c282d2570.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b73bd7d9026d6ff1f86e406c282d2570.png)'
- en: The typical intrinsic matrix. Representing the focal length in the x and y direction,
    along with the principal point coordinates. Image by author.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的内参矩阵。表示x和y方向的焦距以及主点坐标。图片来源：作者。
- en: The intrinsic matrix is used to transform points from camera coordinates (obtained
    using the extrinsic matrix) to a 2D image plane, ie. what you see as your “image.”
    Points in camera coordinates alone do not indicate their appearance in an image
    as depth must be reflected in order to assess exactly what a camera will see.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 内参矩阵用于将点从相机坐标（通过外参矩阵获得）转换到二维图像平面，也就是你所看到的“图像”。单独的相机坐标不能表明其在图像中的外观，因为必须反映深度信息才能准确评估相机看到的内容。
- en: To convert COLMAP points to a 2D image, we first project them to camera coordinates
    using the extrinsic matrix, and then project them to 2D using the intrinsic matrix.
    However, an important detail is that we use homogeneous coordinates for this process.
    The extrinsic matrix is 4x4, while our input points are 3x1, so we stack a 1 to
    the input points to make them 4x1.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将 COLMAP 点转换为 2D 图像，我们首先使用外参矩阵将它们投影到相机坐标，然后再使用内参矩阵将它们投影到 2D。然而，一个重要的细节是，在此过程中我们使用了齐次坐标。外参矩阵是
    4x4 的，而输入点是 3x1 的，因此我们需要在输入点上加一个 1，将它们变为 4x1。
- en: 'Here’s the process step-by-step:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这是逐步的过程：
- en: 'Transform the points to camera coordinates: multiply the 4x4 extrinsic matrix
    by the 4x1 point vector.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将点转换到相机坐标：将 4x4 的外参矩阵与 4x1 的点向量相乘。
- en: 'Transform to image coordinates: multiply the 3x4 intrinsic matrix by the resulting
    4x1 vector.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转换为图像坐标：将 3x4 的内参矩阵与得到的 4x1 向量相乘。
- en: This results in a 3x1 matrix. To get the final 2D coordinates, we divide by
    the third coordinate of this 3x1 matrix and obtain an x and y coordinate in the
    image! You can see exactly how this should look about for image number 100 and
    the code to replicate the results is shown below.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这将得到一个 3x1 的矩阵。为了获得最终的 2D 坐标，我们将这个 3x1 矩阵的第三个坐标除以它，得到图像中的 x 和 y 坐标！你可以看到图像编号
    100 应该是什么样子，下面是复制结果的代码。
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: To review, we can now take any set of 3D points and project where they would
    appear on a 2D image plane as long as we have the various location and camera
    parameters we need! With that in hand we can move forward with understanding the
    “gaussian” part of gaussian splatting in [part 2](https://medium.com/towards-data-science/a-python-engineers-introduction-to-3d-gaussian-splatting-part-2-7e45b270c1df).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 复习一下，现在我们可以通过任何一组 3D 点，利用我们需要的各种位置和相机参数，计算它们在 2D 图像平面上出现的位置！有了这些，我们可以继续理解[第二部分](https://medium.com/towards-data-science/a-python-engineers-introduction-to-3d-gaussian-splatting-part-2-7e45b270c1df)中高斯溅射的“高斯”部分。
- en: 'Kerbl, Bernhard, et al. “3d gaussian splatting for real-time radiance field
    rendering.” *ACM Transactions on Graphics* 42.4 (2023): 1–14.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kerbl, Bernhard, 等. “实时辐射场渲染的 3D 高斯溅射。” *ACM 图形学学报* 42.4（2023）：1–14。
- en: 'Mildenhall, Ben, et al. “Nerf: Representing scenes as neural radiance fields
    for view synthesis.” *Communications of the ACM* 65.1 (2021): 99–106.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Mildenhall, Ben, 等. “Nerf：将场景表示为神经辐射场进行视图合成。” *ACM 通讯* 65.1（2021）：99–106。
- en: 'Snavely, Noah, Steven M. Seitz, and Richard Szeliski. “Photo tourism: exploring
    photo collections in 3D.” *ACM siggraph 2006 papers*. 2006\. 835–846.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Snavely, Noah, Steven M. Seitz, 和 Richard Szeliski. “照片旅游：在 3D 中探索照片集合。” *ACM
    siggraph 2006 论文集*，2006，835–846。
- en: 'Barron, Jonathan T., et al. “Mip-nerf 360: Unbounded anti-aliased neural radiance
    fields.” *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*. 2022.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Barron, Jonathan T., 等. “Mip-nerf 360: 无边界抗锯齿神经辐射场。” *IEEE/CVF 计算机视觉与模式识别会议论文集*，2022。'
