- en: 'Support Vector Classifier, Explained: A Visual Guide with Mini 2D Dataset'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持向量分类器，解释：带有迷你 2D 数据集的视觉指南
- en: 原文：[https://towardsdatascience.com/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9?source=collection_archive---------3-----------------------#2024-10-01](https://towardsdatascience.com/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9?source=collection_archive---------3-----------------------#2024-10-01)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9?source=collection_archive---------3-----------------------#2024-10-01](https://towardsdatascience.com/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9?source=collection_archive---------3-----------------------#2024-10-01)
- en: CLASSIFICATION ALGORITHM
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类算法
- en: Finding the best “line” to separate the classes? Yeah, sure...
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 找到最佳的“分隔线”来区分不同的类别？嗯，当然……
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--62e831e7b9e9--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--62e831e7b9e9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--62e831e7b9e9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--62e831e7b9e9--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--62e831e7b9e9--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@samybaladram?source=post_page---byline--62e831e7b9e9--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--62e831e7b9e9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--62e831e7b9e9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--62e831e7b9e9--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--62e831e7b9e9--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--62e831e7b9e9--------------------------------)
    ·14 min read·Oct 1, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--62e831e7b9e9--------------------------------)
    ·14分钟阅读·2024年10月1日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/5a473769a53d065bc213ca926988bd11.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a473769a53d065bc213ca926988bd11.png)'
- en: '`⛳️ More [CLASSIFICATION ALGORITHM](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c),
    explained: · [Dummy Classifier](/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e)
    · [K Nearest Neighbor Classifier](/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1)
    · [Bernoulli Naive Bayes](/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6)
    · [Gaussian Naive Bayes](/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c)
    · [Decision Tree Classifier](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e)
    · [Logistic Regression](/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505)
    ▶ [Support Vector Classifier](/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9)
    · [Multilayer Perceptron](/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c)`'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '`⛳️ 更多 [分类算法](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c)，解释如下：
    · [虚拟分类器](/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e)
    · [K 最近邻分类器](/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1)
    · [伯努利朴素贝叶斯](/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6)
    · [高斯朴素贝叶斯](/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c)
    · [决策树分类器](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e)
    · [逻辑回归](/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505)
    ▶ [支持向量分类器](/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9)
    · [多层感知器](/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c)`'
- en: “Support Vector Machine (SVM) for classification works on a very basic principle
    — it tries to find the best line that separates the two classes.” But if I hear
    that oversimplified explanation **one more time**, I might just scream into a
    pillow.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: “用于分类的支持向量机（SVM）遵循一个非常基础的原则——它试图找到一条最优的分隔线，将两个类别分开。”但如果我再听到这种过于简化的解释**一次**，我可能真会把头埋进枕头里尖叫。
- en: While the premise sounds simple, SVM is one of those algorithms packed with
    mathematical gymnastics that took me an absurd amount of time to grasp. Why is
    it even called a ‘machine’? Why do we need support vectors? Why are some points
    suddenly not important? And why does it have to be a straight line — oh wait,
    a **straight hyperplane**??? Then there’s the optimization formula, which is apparently
    so tricky that we need another version called the dual form. But hold on, now
    we need **another** algorithm called SMO to solve that? What’s with all the dual
    coefficients that scikit-learn just spits out? And if that’s not enough, we’re
    suddenly pulling off this magic ‘kernel tricks’ when a straight line doesn’t cut
    it? Why do we even need these tricks? And why do none of the tutorials ever show
    the actual numbers?!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前提看起来很简单，但SVM是那种包含复杂数学运算的算法，我花费了大量时间才搞懂。为什么它叫做“机器”？我们为什么需要支持向量？为什么有些点突然变得不重要？为什么它必须是直线——哦，等等，是**直超平面**???
    然后还有优化公式，这个公式 apparently 难得需要一个叫做对偶形式的版本来处理。等等，现在我们还需要**另一个**算法叫做SMO来解决这个问题？那些scikit-learn自动输出的对偶系数又是怎么回事？如果这还不够，当一条直线不够用时，我们又开始使用神奇的“核技巧”？我们为什么需要这些技巧？而且为什么没有教程展示实际的数字？！
- en: In this article, I’m trying to stop this Support Vector Madness. After hours
    and hours trying to really understand this algorithm, I will try to explain what’s
    ACTUALLY going on with ACTUAL numbers (and of course, its visualization too) but
    without the complicated maths, perfect for beginners.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我试图停止这种支持向量机的疯狂。在花了几个小时尝试真正理解这个算法后，我将尝试用**实际**的数字（当然，还有其可视化）来解释实际发生了什么，而不涉及复杂的数学，适合初学者。
- en: '![](../Images/073bec34540009054f6bb609f371fa8b.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/073bec34540009054f6bb609f371fa8b.png)'
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所有可视化：作者使用Canva Pro创建。已优化为适合移动端；在桌面端可能会显得过大。
- en: Definition
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义
- en: Support Vector Machines are supervised learning models used mainly for classification
    tasks, though they can be adapted for regression as well. SVMs aim to find the
    line that best divides a dataset into classes (*sigh…*), maximizing the margin
    between these classes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机（SVM）是监督学习模型，主要用于分类任务，尽管它们也可以适应回归任务。SVM旨在找到能够最好地将数据集分成不同类别的那条线（*唉…*），并最大化这些类别之间的间隔。
- en: '![](../Images/0926d86dbc53835cc82cdbc41023c182.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0926d86dbc53835cc82cdbc41023c182.png)'
- en: Despite its complexities, SVM can be considered one of the fundamental algorithms
    in machine learning.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管SVM很复杂，但它仍然可以被视为机器学习中的基础算法之一。
- en: “Support vectors” are the data points that lie closest to the line and can actually
    define that line as well. And, what’s with the “Machine” then ? While other machine
    learning algorithms could include “Machine,” SVM’s naming may be partly due to
    historical context when it was developed. That’s it.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: “支持向量”是那些距离决策边界最近的点，它们实际上可以定义这条线。那么，那个“机器”又是什么？虽然其他机器学习算法也可以包括“机器”，但SVM的命名可能部分来源于它被开发时的历史背景。就这样。
- en: 📊 Dataset Used
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 📊 使用的数据集
- en: To understand how SVM works, it is a good idea to start from a dataset with
    few samples and smaller dimension. We’ll use this simple mini 2D dataset (inspired
    by [1]) as an example.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解SVM的工作原理，最好从一个样本较少、维度较小的数据集开始。我们将以这个简单的二维小数据集（灵感来自[1]）作为示例。
- en: '![](../Images/608af2d7d2e1d635cd2f830d7902f83a.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/608af2d7d2e1d635cd2f830d7902f83a.png)'
- en: 'Columns: Temperature (0–3), Humidity (0–3), Play Golf (Yes/No). The training
    dataset has 2 dimensions and 8 samples.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 列：温度（0–3）、湿度（0–3）、打高尔夫（是/否）。训练数据集有2个维度和8个样本。
- en: 'Instead of explaining the steps of the training process itself, we will walk
    from keyword to keyword and see how SVM actually works:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会直接解释训练过程的步骤，而是将从关键字到关键字，看看SVM实际上是如何工作的：
- en: 'Part 1: Basic Components'
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1部分：基本组件
- en: Decision Boundary
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策边界
- en: The decision boundary in SVM is the line (or called “hyperplane” in higher dimensions)
    that the algorithm determines to best separate different classes of data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: SVM中的决策边界是算法确定的最佳分隔不同类别数据的线（或在高维中称为“超平面”）。
- en: This line would attempt to keep most “YES” points on one side and most “NO”
    points on the other. However, because for data that isn’t linearly separable,
    this boundary won’t be perfect — some points might be on the “wrong” side.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这条线会尽力把大多数“是”类点放在一边，大多数“否”类点放在另一边。然而，对于那些不能线性分割的数据，这条边界不会是完美的——一些点可能会出现在“错误”的一侧。
- en: Once this line is established, any new data can be classified depending on which
    side of the boundary it falls.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦确定了这条线，任何新的数据都可以根据它位于决策边界的哪一侧来进行分类。
- en: '![](../Images/475eed31b224801da296d616dfd81eb8.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/475eed31b224801da296d616dfd81eb8.png)'
- en: In our golf example, it would be the line that tries to separate the “YES” (play
    golf) points from the “NO” points. SVM would try to position this line even though
    a perfect separation isn’t possible with a straight line. At this point, using
    our eyes, this seems to be a nice line that make a good separator.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的高尔夫例子中，决策边界将试图将“YES”（打高尔夫）和“NO”两类数据点分开。SVM会尝试定位这条线，尽管用一条直线无法做到完美分隔。此时，凭借我们的眼光，这条线看起来是一个不错的分隔线。
- en: Linear Separability
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性可分性
- en: Linear separability refers to whether we can draw a straight line that perfectly
    separates two classes of data points. If data is linearly separable, SVM can find
    a clear, hard boundary between classes. However, when data isn’t linearly separable
    (as in our case) SVM needs to use more advanced techniques.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 线性可分性指的是我们是否可以画出一条直线，完美地将两个类别的数据点分开。如果数据是线性可分的，SVM可以找到一个清晰的、硬的边界来区分不同类别。然而，当数据不是线性可分的（如我们的情况）时，SVM需要使用更先进的技术。
- en: '![](../Images/c464c383be7459c488a79148a52e09e4.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c464c383be7459c488a79148a52e09e4.png)'
- en: In the training set, no matter how we draw the line, we cannot separate the
    two classes. If we omit index 1 & 8, now we can.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练集里，不论我们如何画线，都无法将两类数据完全分开。如果我们忽略索引1和8，现在就能分开它们了。
- en: Margin
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 间隔
- en: The margin in SVM is the distance between the decision boundary and the closest
    data points from each class. These closest points are called support vectors.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在SVM中，间隔是指决策边界到每个类别的最近数据点之间的距离。这些最近的点称为支持向量。
- en: SVM aims to maximize this margin. A larger margin generally leads to better
    generalization — the ability to correctly classify new, unseen data points.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: SVM的目标是最大化这个间隔。较大的间隔通常能带来更好的泛化能力——即能够正确分类新的、未见过的数据点。
- en: However, because data in general isn’t perfectly separable, SVM might use a
    soft margin approach. This allows some points to be within the margin or even
    on the wrong side of the boundary, trading off perfect separation for a more robust
    overall classifier.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于数据通常并不是完全可分的，SVM可能会采用软间隔的方法。这允许一些点位于间隔内，甚至处于决策边界的错误一侧，从而在完美分隔与构建更强健的分类器之间做出权衡。
- en: '![](../Images/07c9ddad0a6f5df06001f200fdab8947.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07c9ddad0a6f5df06001f200fdab8947.png)'
- en: SVM would try to position the decision boundary to create the widest possible
    margin while still separating most “YES” and “NO” instances.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: SVM会尝试将决策边界定位，创造出尽可能宽的间隔，同时尽量将大部分“YES”和“NO”实例分开。
- en: Hard Margin vs Soft Margin
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 硬间隔与软间隔
- en: Hard Margin SVM is the ideal scenario where all data points can be perfectly
    separated by the decision boundary, with no misclassifications. In this case,
    the margin is “hard” because it doesn’t allow any data points to be on the wrong
    side of the boundary or within the margin.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 硬间隔SVM是理想情况，在这种情况下，所有数据点都能被决策边界完美地分开，不会出现任何误分类。在这种情况下，间隔是“硬”的，因为它不允许任何数据点处于决策边界的错误一侧或间隔内。
- en: 'Soft Margin SVM, on the other hand, allows some flexibility. It permits some
    data points to be misclassified or to lie within the margin. This allows the SVM
    to find a good balance between:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，软间隔SVM允许一定的灵活性。它允许一些数据点被误分类，或位于间隔内。这使得SVM能够在以下两者之间找到一个良好的平衡：
- en: Maximizing the margin
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最大化间隔
- en: Minimizing classification errors
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最小化分类错误
- en: '![](../Images/ebe390809fdb8c6f9f23aaf70d39dd80.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ebe390809fdb8c6f9f23aaf70d39dd80.png)'
- en: In our case, a Hard Margin approach isn’t possible because the data isn’t linearly
    separable. Therefore, a Soft Margin approach is necessary for our dataset. With
    Soft Margin SVM, you might allow points like ID 1 & ID 8 to be on the “wrong”
    side of the boundary if it results in a better overall classifier.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，硬间隔方法不可行，因为数据不是线性可分的。因此，对于我们的数据集，必须采用软间隔方法。在软间隔SVM中，可能允许像ID 1和ID 8这样的点处于决策边界的“错误”一侧，如果这能带来更好的总体分类器。
- en: '**Distance Calculation**'
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**距离计算**'
- en: 'In SVM, distance calculations play an important role in both training and classification.
    The distance of a point *x* from the decision boundary is given by:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在SVM中，距离计算在训练和分类中都起着重要作用。点*x*到决策边界的距离由以下公式给出：
- en: '|*w* · *x* + *b*| / ||*w*||'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '|*w* · *x* + *b*| / ||*w*||'
- en: where *w* is the weight vector perpendicular to the hyperplane, *b* is the bias
    term, and ||*w*|| is the Euclidean norm of *w*.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*w*是垂直于超平面的权重向量，*b*是偏置项，||*w*||是*w*的欧几里得范数。
- en: '![](../Images/5db8a3f8f5972e138ee8ea808db49c62.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5db8a3f8f5972e138ee8ea808db49c62.png)'
- en: This way, we can see which points are the closest to the hyperplane without
    drawing it.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们可以在不画出超平面的情况下，看到哪些点离超平面最近。
- en: Support Vectors
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持向量
- en: 'Support vectors are the data points with closest distance to the hyperplane.
    They are important because: They “support” the hyperplane, defining its position.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量是距离超平面最近的数据点。它们之所以重要，是因为：它们“支持”超平面，定义了其位置。
- en: What makes Support Vectors special is that they are the only points that matter
    for determining the decision boundary. All other points could be removed without
    changing the boundary’s position. This is a key feature of SVM — it bases its
    decision on the most critical points rather than all data points.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量之所以特别，是因为它们是确定决策边界的唯一关键点。其他所有点可以移除而不改变边界的位置。这是支持向量机（SVM）的一个关键特性——它依据最关键的点来做出决策，而不是所有的数据点。
- en: '![](../Images/93e911910f9e0a7e061da7e7c7ed4412.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93e911910f9e0a7e061da7e7c7ed4412.png)'
- en: For this hyperplane, we have 3 support vectors that lies on the margin. The
    2 misclassified data points can be regarded as support vectors as well in some
    situations.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个超平面，我们有 3 个支持向量位于边缘上。在某些情况下，2 个被错误分类的数据点也可以视为支持向量。
- en: Slack Variables
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 松弛变量
- en: Slack Variables are introduced in Soft Margin SVM to quantify the degree of
    misclassification or margin violation for each data point. They’re called “slack”
    because they give the model some slack or flexibility in fitting the data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在软间隔 SVM 中引入了松弛变量，用以量化每个数据点的误分类或边缘违背程度。它们被称为“松弛”变量，因为它们为模型提供了一些松弛或灵活性来拟合数据。
- en: 'In SVMs, slack variables *ξᵢ* can be calculated as:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SVM 中，松弛变量 *ξᵢ* 可以通过以下方式计算：
- en: '*ξᵢ* = max(0, 1 — *yᵢ*(*w* · *xᵢ* + *b*))'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*ξᵢ* = max(0, 1 — *yᵢ*(*w* · *xᵢ* + *b*))'
- en: where
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: · *w* is the weight vector
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: · *w* 是权重向量
- en: · *b* is the bias term
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: · *b* 是偏置项
- en: · *xᵢ* are the input vectors
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: · *xᵢ* 是输入向量
- en: · *yᵢ* are the corresponding labels
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: · *yᵢ* 是相应的标签
- en: 'This formula only works when class labels *yᵢ* are in {-1, +1} format. It elegantly
    handles both classes:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式仅在类标签 *yᵢ* 为 {-1, +1} 格式时有效。它优雅地处理了两类问题：
- en: '· Correctly classified points beyond margin: *ξᵢ* = 0'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: · 正确分类且位于边缘之外的点：*ξᵢ* = 0
- en: '· Misclassified or margin-violating points: *ξᵢ* > 0'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: · 被错误分类或违反边缘的点：*ξᵢ* > 0
- en: Using {-1, +1} labels maintains SVM’s mathematical symmetry and simplifies optimization,
    unlike {0, 1} labels which would require separate cases for each class.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 {-1, +1} 标签保持了 SVM 的数学对称性并简化了优化，与 {0, 1} 标签不同，后者需要为每一类创建单独的情况。
- en: '![](../Images/178df04c97f876c284556ca61937ee1a.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/178df04c97f876c284556ca61937ee1a.png)'
- en: In our golf dataset, the point (3,3) — NO ends up on the “YES” side of our boundary.
    We’d assign a slack variable to this point to measure how far it is on the wrong
    side. Similarly, if (2,0) — NO is correctly classified but falls within the margin,
    it would also get a slack variable.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的高尔夫数据集中，点 (3,3) — NO 最终位于我们的边界的“YES”一侧。我们会为该点分配一个松弛变量，以衡量它偏离错误一侧的距离。同样，如果
    (2,0) — NO 被正确分类但位于边缘内，它也会得到一个松弛变量。
- en: '![](../Images/52dbed69530ef9c8757c2de48abc398c.png)![](../Images/b5f6ebc65cc316f3365fe3231a85f146.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52dbed69530ef9c8757c2de48abc398c.png)![](../Images/b5f6ebc65cc316f3365fe3231a85f146.png)'
- en: In our golf dataset, the point (3,3) — NO ends up on the “YES” side of our boundary.
    We’d assign a slack variable to this point to measure how far it is on the wrong
    side. Similarly, if (2,0) — NO is correctly classified but falls within the margin,
    it would also get a slack variable.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的高尔夫数据集中，点 (3,3) — NO 最终位于我们的边界的“YES”一侧。我们会为该点分配一个松弛变量，以衡量它偏离错误一侧的距离。同样，如果
    (2,0) — NO 被正确分类但位于边缘内，它也会得到一个松弛变量。
- en: Primal Form for Hard Margin
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 硬间隔的原始形式
- en: The primal form is the original optimization problem formulation for SVMs. It
    directly expresses the goal of finding the maximum margin hyperplane in the feature
    space.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 原始形式是支持向量机（SVM）优化问题的原始表述。它直接表达了在特征空间中寻找最大间隔超平面的目标。
- en: 'In simple terms, the primal form seeks to:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，原始形式的目标是：
- en: Find a hyperplane that correctly classifies all data points.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到一个能正确分类所有数据点的超平面。
- en: Maximize the distance between this hyperplane and the nearest data points from
    each class.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最大化该超平面与来自每个类别的最近数据点之间的距离。
- en: 'Primal form is:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 原始形式是：
- en: '**minimize**: (1/2) ||*w*||²'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**最小化**: (1/2) ||*w*||²'
- en: '**subject to**: *yᵢ*(*w* · *xᵢ* + *b*) ≥ 1 for all i'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**受限条件**: *yᵢ*(*w* · *xᵢ* + *b*) ≥ 1 对所有 i 都成立'
- en: where
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: · *w* is the weight vector
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: · *w* 是权重向量
- en: · *b* is the bias term
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: · *b* 是偏置项
- en: · *xᵢ* are the input vectors
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: · *xᵢ* 是输入向量
- en: · *yᵢ* are the corresponding labels (+1 or -1)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: · *yᵢ* 是相应的标签（+1 或 -1）
- en: · ||*w*||² is the squared Euclidean norm of *w*
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: · ||*w*||² 是 *w* 的平方欧几里得范数
- en: '![](../Images/5a4e7ae7769876a41d594239f7a087e0.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a4e7ae7769876a41d594239f7a087e0.png)'
- en: In the case of index 1 & 8 omitted, we are trying to find the best line that
    has the bigger the margin.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在省略索引 1 和 8 的情况下，我们正在尝试找到最佳的边界，以便获得更大的边距。
- en: '![](../Images/b2f596443ba8eac5e03ae7421a9580e6.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2f596443ba8eac5e03ae7421a9580e6.png)'
- en: If we choose hyperplane with smaller margin, it gives higher value of the objective
    function, which is not what we want.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择具有较小边距的超平面，它会使目标函数的值更高，而这不是我们想要的。
- en: Primal Form for Soft Margin
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 软边距的原始形式
- en: 'Remember that the soft margin SVM is an extension of the original (hard margin)
    SVM that allows for some misclassification? This change is reflected in the primal
    form. The soft margin SVM primal form becomes:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，软边距 SVM 是原始（硬边距）SVM 的扩展，它允许一些误分类？这一变化在原始形式中有所体现。软边距 SVM 的原始形式变为：
- en: '**minimize**: (1/2) ||*w|*|² + *C* Σ*ᵢ ξᵢ*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**最小化**： (1/2) ||*w|*|² + *C* Σ*ᵢ ξᵢ*'
- en: '**subject to**: *yᵢ*(*w* · *xᵢ* + *b*) ≥ 1 — *ξᵢ* for all *i*, *ξᵢ* ≥ 0 for
    all *i*'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**约束条件**：*yᵢ*(*w* · *xᵢ* + *b*) ≥ 1 — *ξᵢ* 对所有 *i*，*ξᵢ* ≥ 0 对所有 *i*'
- en: where
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: · *C* is the penalty parameter
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: · *C* 是惩罚参数
- en: · *ξᵢ* are the slack variables
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: · *ξᵢ* 是松弛变量
- en: · All other variables are the same as in the hard margin case
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: · 所有其他变量与硬边距情况下相同
- en: '![](../Images/bb835b40fcb1eed9a834078010114b8e.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bb835b40fcb1eed9a834078010114b8e.png)'
- en: The penalty of the wrongly classified data points contributes to the objective
    function as extra values to minimize.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 误分类数据点的惩罚作为额外值贡献到目标函数中，以便最小化。
- en: '![](../Images/e0a2b3cf3521173665282aab8c4280b5.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e0a2b3cf3521173665282aab8c4280b5.png)'
- en: Say we choose another hyperplane that is a bit closer to index 8\. The objective
    value is now higher. The more balance the distance from the wrongly classified
    ones, the smaller the total penalty.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们选择了另一个稍微靠近索引 8 的超平面。目标值现在变得更高。从误分类点的距离越平衡，总的惩罚就越小。
- en: Dual Form
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对偶形式
- en: 'Here’s the bad news: The primal form can be slow and hard to solve, especially
    for complex data.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有个坏消息：原始形式可能求解较慢且难以解决，尤其是在处理复杂数据时。
- en: 'The dual form provides an alternative way to solve the SVM optimization problem,
    often leading to computational advantages. It’s formulated as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对偶形式提供了解决 SVM 优化问题的替代方法，通常能够带来计算上的优势。其形式如下：
- en: '**maximize**: *Σᵢ,ⱼ(αᵢyᵢ) - ½ΣᵢΣⱼ(αᵢαⱼyᵢyⱼ(xᵢ* · *xⱼ))* **subject to:** 0 ≤
    *αᵢ* ≤ C for all i, Σ*ᵢαᵢyᵢ* = 0'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**最大化**：*Σᵢ,ⱼ(αᵢyᵢ) - ½ΣᵢΣⱼ(αᵢαⱼyᵢyⱼ(xᵢ* · *xⱼ))* **约束条件**：0 ≤ *αᵢ* ≤ C 对所有
    i，Σ*ᵢαᵢyᵢ* = 0'
- en: 'Where:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: · *αᵢ* are the Lagrange multipliers (dual variables)
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: · *αᵢ* 是拉格朗日乘子（对偶变量）
- en: · *yᵢ* are the class labels (+1 or -1)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: · *yᵢ* 是类标签（+1 或 -1）
- en: · *xᵢ* are the input vectors
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: · *xᵢ* 是输入向量
- en: · *C* is the regularization parameter (upper bound for *αᵢ*)
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: · *C* 是正则化参数（*αᵢ* 的上界）
- en: · (*xᵢ* · *xⱼ*) denotes the dot product between *xᵢ* and *xⱼ*
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: · (*xᵢ* · *xⱼ*) 表示 *xᵢ* 和 *xⱼ* 之间的点积
- en: '![](../Images/2e91b4c99d88023d6532f3cdb94d9fe3.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e91b4c99d88023d6532f3cdb94d9fe3.png)'
- en: Other than the training data itself, the only other components in this dual
    form is the Lagrange multipliers (*αᵢ*).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 除了训练数据本身，唯一出现在对偶形式中的其他成分是拉格朗日乘子（*αᵢ*）。
- en: Lagrange Multipliers
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 拉格朗日乘子
- en: As we notice in the dual form, Lagrange multipliers (*αᵢ*) show up when we transform
    the primal problem into its dual form (that’s why they also known as the dual
    coefficients). If you noticed, the weights & bias are no longer there!
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在对偶形式中看到的，当我们将原始问题转换为对偶形式时，拉格朗日乘子（*αᵢ*）就会出现（这也是它们被称为对偶系数的原因）。如果你注意到了，权重和偏置不再存在！
- en: 'Each data point in the training set has an associated Lagrange multiplier.
    The good thing is Lagrange multipliers make things much easier to understand:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 每个训练集中的数据点都有一个关联的拉格朗日乘子。好处是，拉格朗日乘子使得理解问题变得更加容易：
- en: '**Interpretation**:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**解释**：'
- en: '- *αᵢ* = 0: The point is correctly classified and outside the margin. This
    point does not influence the decision boundary.'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- *αᵢ* = 0：该点被正确分类并且位于边距外。这个点不会影响决策边界。'
- en: '- 0 < *αᵢ* < *C*: The point is on the margin boundary. These are called “free”
    or “unbounded” support vectors.'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 0 < *αᵢ* < *C*：该点位于边距边界上。这些点被称为“自由”或“无界”支持向量。'
- en: '- *αᵢ* = *C*: The point is either on or inside the margin (including misclassified
    points). These are called “bound” support vectors.'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- *αᵢ* = *C*：该点要么位于边界上，要么位于边界内部（包括误分类点）。这些点被称为“边界”支持向量。'
- en: '**Relationship to decision boundary**:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**与决策边界的关系**：'
- en: '*w* = Σ*ᵢ*(*αᵢ* *yᵢ* *xᵢ*),'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*w* = Σ*ᵢ*(*αᵢ* *yᵢ* *xᵢ*),'
- en: '*b* = *yᵢ* — Σ*ⱼ*(*αᵢ* *yⱼ*(*xⱼ* · *xᵢ*))'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*b* = *yᵢ* — Σ*ⱼ*(*αᵢ* *yⱼ*(*xⱼ* · *xᵢ*))'
- en: where *yᵢ* is the label of any (unbounded) support vector.
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中，*yᵢ*是任何（无界）支持向量的标签。
- en: This means the final decision boundary is determined only by points with non-zero
    *αᵢ* !
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这意味着最终的决策边界仅由具有非零*αᵢ*的点决定！
- en: '![](../Images/7cb56db83ca8224714cf4cf1d2faf3b5.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7cb56db83ca8224714cf4cf1d2faf3b5.png)'
- en: It turns out the algorithm decides that our original hyperplane is somehow the
    best, it just need bigger margin by halving all the weights. This makes all points
    support vectors somehow, but it’s ok since the dataset itself is small. 😅
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，算法决定我们原来的超平面是最优的，只是需要通过将所有权重减半来增大间隔。这使得所有点都变成了支持向量，但由于数据集本身较小，这没有问题。😅
- en: Sequential Minimal Optimization
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 顺序最小优化（Sequential Minimal Optimization）
- en: 'Remember that we haven’t really shown how to get the optimal Lagrange multipliers
    (*αᵢ*)? The algorithm to solve this is called Sequential Minimal Optimization
    (SMO). Here’s a simplified view of how we get these values:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们还没有真正展示如何获得最优的拉格朗日乘子（*αᵢ*）？解决这个问题的算法称为顺序最小优化（SMO）。下面是我们如何获得这些值的简化视图：
- en: Start with all αᵢ at zero.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从所有αᵢ为零开始。
- en: Repeatedly select and adjust two αᵢ at a time to improve the solution.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复选择并调整两个*αᵢ*以改进解。
- en: Update these pairs quickly using simple math.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用简单的数学快速更新这些对。
- en: Ensure all updates follow SVM constraints.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保所有更新遵循支持向量机（SVM）约束。
- en: Repeat until all *αᵢ* are “good enough.”
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复直到所有*αᵢ*都“足够好”为止。
- en: Points with αᵢ > 0 become support vectors.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 具有αᵢ > 0的点成为支持向量。
- en: This approach efficiently solves the SVM optimization without heavy computations,
    making it practical for large datasets.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法高效地解决了SVM优化问题，无需繁重的计算，使其在大数据集上具有实用性。
- en: '![](../Images/6795f2deb4d5b41da832a23ec3a3cd74.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6795f2deb4d5b41da832a23ec3a3cd74.png)'
- en: Decision Function
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策函数
- en: After solving the SVM optimization problem using the dual form and obtaining
    the Lagrange multipliers, we can define the decision function. This function determines
    how new, unseen data points are classified by the trained SVM model.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用对偶形式求解SVM优化问题并获得拉格朗日乘子后，我们可以定义决策函数。该函数决定了训练后的SVM模型如何对新的、未见过的数据点进行分类。
- en: '*f*(*x*) = Σ*ᵢ*(*αᵢyᵢ*(*xᵢ* · *x*)) + *b*'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*f*(*x*) = Σ*ᵢ*(*αᵢyᵢ*(*xᵢ* · *x*)) + *b*'
- en: Here, *αᵢ* are the Lagrange multipliers, *y*ᵢ are the class labels (+1 or -1),
    *xᵢ* are the support vectors, and *x* is the input vector to be classified. The
    final classification for a new point x is determined by the sign of *f*(*x*) (either
    “+” or “-”).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*αᵢ*是拉格朗日乘子，*yᵢ*是类别标签（+1或-1），*xᵢ*是支持向量，*x*是待分类的输入向量。新点x的最终分类由*f*(*x*)的符号决定（即“+”或“-”）。
- en: Note that this decision function uses only the support vectors (data points
    with non-zero *αᵢ*) to classify new inputs, which is the core principle of the
    SVM algorithm!
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个决策函数仅使用支持向量（具有非零*αᵢ*的数据点）来分类新的输入数据，这就是SVM算法的核心原理！
- en: '![](../Images/7233f435dd2070b0750e1ef60f117705.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7233f435dd2070b0750e1ef60f117705.png)'
- en: 🌟 Support Vector Classifier Code
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🌟 支持向量分类器代码
- en: 'The results above can be obtained using the following code:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 上述结果可以通过以下代码获得：
- en: '[PRE0]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/dd57dff98a95c2298a2b2520aee3f336.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd57dff98a95c2298a2b2520aee3f336.png)'
- en: 'Part 2: Kernel Trick'
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分：核技巧
- en: As we have seen so far, no matter how we set up the hyperplane, we never could
    make a perfect separation between the two classes. There are actually some “trick”
    that we can do to make it separable… even though it is not linearly anymore.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，无论如何设置超平面，我们始终无法完美地将两类数据分开。实际上，我们可以做一些“技巧”，使得数据可以被分开……尽管它不再是线性可分的。
- en: Input Space vs Feature Space
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输入空间与特征空间
- en: Input Space refers to the original space of your data features. In our golf
    dataset, the Input Space is two-dimensional, consisting of temperature and humidity.
    Each data point in this space represents a specific weather condition where someone
    decided to play golf or not.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 输入空间指的是数据特征的原始空间。在我们的高尔夫数据集中，输入空间是二维的，由温度和湿度组成。此空间中的每个数据点代表某个具体天气条件下，是否有人决定打高尔夫。
- en: Feature Space, on the other hand, is a transformed version of the Input Space
    where the SVM actually performs the classification. Sometimes, data that isn’t
    linearly separable in the Input Space becomes separable when mapped to a higher-dimensional
    Feature Space.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 特征空间则是输入空间的一个转换版本，SVM实际上在特征空间中执行分类。有时，在线性不可分的输入空间中，数据映射到高维特征空间后变得可分。
- en: '![](../Images/67548459823e251356184cf343374b7a.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67548459823e251356184cf343374b7a.png)'
- en: As we have tried so far, no matter what hyperplane we choose, we couldn’t separate
    the two classes linearly. Instead of just using 🌞 and 💧, the Feature Space might
    include combinations like 🌞², 💧², 🌞×💧. This would turn our 2D Input Space into
    a 5D Feature Space. If you notice, we can find a hyperplane that now can separate
    the two classes perfectly!
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们到目前为止所尝试的，无论选择什么超平面，我们都无法将这两个类别线性分开。除了使用🌞和💧，特征空间可能还包括类似🌞²、💧²、🌞×💧的组合。这将把我们的二维输入空间转变为五维特征空间。如果你注意到的话，我们现在可以找到一个超平面，能够完美地分开这两个类别！
- en: Kernel and Implicit Transformation
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 核及隐式变换
- en: A kernel is a function that computes the similarity between two data points,
    implicitly representing them in a higher-dimensional space (the feature space).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 核是一个计算两个数据点之间相似性的函数，隐式地将它们表示在一个更高维的空间（特征空间）中。
- en: 'Say, there’s a function *φ*(*x*) that transforms each input point *x* to a
    higher-dimensional space. For example: *φ* : ℝ² → ℝ³, *φ*(*x*,*y*) = (*x*, *y*,
    *x*² + *y*²)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '假设有一个函数 *φ*(*x*)，它将每个输入点 *x* 转换到一个更高维的空间。例如：*φ* : ℝ² → ℝ³, *φ*(*x*,*y*) = (*x*,
    *y*, *x*² + *y*²)'
- en: '**Common Kernels and Their Implicit Transformations:**'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**常见核及其隐式变换：**'
- en: '**a. Linear Kernel**: *K*(*x*,*y*) = *x* · *y*'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '**a. 线性核**：*K*(*x*,*y*) = *x* · *y*'
- en: '- Transformation:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '- 变换：'
- en: '*φ*(*x*) = *x* (identity)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '*φ*(*x*) = *x* （恒等变换）'
- en: '- This doesn’t actually change the space but is useful for linearly separable
    data.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '- 这实际上并不会改变空间，但对于线性可分的数据来说是有用的。'
- en: '**b. Polynomial Kernel**: *K*(*x*,*y*) = (*x* · *y* + c)*ᵈ*'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**b. 多项式核**：*K*(*x*,*y*) = (*x* · *y* + c)*ᵈ*'
- en: '- Transformation (for *d* = 2, *c* = 1 in ℝ²):'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '- 变换（对于 *d* = 2，*c* = 1 在 ℝ² 中）：'
- en: '*φ*(*x*₁,*x*₂) = (1, √2*x*₁, √2*x*₂, *x*₁², √2*x*₁*x*₂, *x*₂²)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '*φ*(*x*₁,*x*₂) = (1, √2*x*₁, √2*x*₂, *x*₁², √2*x*₁*x*₂, *x*₂²)'
- en: '- This captures all polynomial terms up to degree *d*.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '- 这涵盖了所有最高为 *d* 次的多项式项。'
- en: '**c. RBF Kernel**: *K*(*x*,*y*) = exp(-*γ*||*x* - *y*||²)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**c. RBF 核**：*K*(*x*,*y*) = exp(-*γ*||*x* - *y*||²)'
- en: '- Transformation (as an infinite series):'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '- 变换（作为一个无穷级数）：'
- en: '*φ*(*x*₁,*x*₂)= exp(-*γ*||*x*||²) * (1, √(2*γ*)*x*₁, √(2*γ*)*x*₂, …, √(2*γ*²/2!)*x*₁²,
    √(2*γ*²/2!)*x*₁*x*₂, √(2*γ*²/2!)*x*₂², …, √(2*γⁿ*/*n*!)*x*₁*ⁿ*, …)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '*φ*(*x*₁,*x*₂) = exp(-*γ*||*x*||²) * (1, √(2*γ*)*x*₁, √(2*γ*)*x*₂, …, √(2*γ*²/2!)*x*₁²,
    √(2*γ*²/2!)*x*₁*x*₂, √(2*γ*²/2!)*x*₂², …, √(2*γⁿ*/*n*!)*x*₁*ⁿ*, …)'
- en: '- Can be thought of as a similarity measure that decreases with distance.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '- 可以被看作是一个相似性度量，随着距离的增加而减小。'
- en: '![](../Images/3ec3cc06460d608cd7214914174c67c9.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ec3cc06460d608cd7214914174c67c9.png)'
- en: This is to illustrate how kernel would transform the input space. In reality,
    the computation of each point in this Feature Space itself is not performed as
    it is expensive to compute, that’s why it is called implicit.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这旨在说明核如何转换输入空间。实际上，特征空间中每个点的计算本身并不会执行，因为计算非常昂贵，这就是为什么它被称为隐式的原因。
- en: Kernel Trick
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 核技巧
- en: The “trick” part of the kernel trick is that we can perform operations in this
    higher-dimensional space solely using the kernel function, without ever explicitly
    computing the transformation *φ*(x).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 核技巧的“技巧”部分在于，我们可以仅使用核函数，在这个更高维的空间中执行操作，而无需显式计算变换 *φ*(x)。
- en: 'Notice that in the dual form, the data points only appear as dot products (*xᵢ*
    · *xⱼ*). This is where the kernel trick comes in. We can replace this dot product
    with a kernel function: (*xᵢ* · *xⱼ*) → *K*(*xᵢ*, *xⱼ*)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在对偶形式中，数据点仅以点积的形式出现 (*xᵢ* · *xⱼ*)。这就是核技巧的作用所在。我们可以将这个点积替换为核函数：(*xᵢ* · *xⱼ*)
    → *K*(*xᵢ*, *xⱼ*)
- en: This process cannot be done if we are just using the primal form, that is one
    of the main reason why the dual form is preferable!
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们仅使用原始形式，则无法进行此过程，这也是为什么对偶形式更可取的主要原因之一！
- en: This substitution implicitly maps the data to a higher-dimensional space **without
    explicitly computing the transformation**.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这个替代方法隐式地将数据映射到更高维空间**而不显式计算变换**。
- en: '![](../Images/a948f98c9e82e1974e89f67b3cd2b244.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a948f98c9e82e1974e89f67b3cd2b244.png)'
- en: Decision Function with Kernel Trick
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用核技巧的决策函数
- en: 'The resulting decision function for a new point *x* becomes:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 对于新点 *x* 的决策函数结果为：
- en: '*f* (*x*) = sign(Σ*ᵢ* *αᵢyᵢK*(*xᵢ*, *x*) + *b*)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '*f* (*x*) = sign(Σ*ᵢ* *αᵢyᵢK*(*xᵢ*, *x*) + *b*)'
- en: where the sum is over all support vectors (points with *αᵢ* > 0).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 其中求和是针对所有支持向量（点的 *αᵢ* > 0）。
- en: '![](../Images/3916df7fe6b42f1589c29aea40e3598a.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3916df7fe6b42f1589c29aea40e3598a.png)'
- en: 🌟 Support Vector Classifier (with Kernel Trick) Code Summary
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🌟 支持向量分类器（带核技巧）代码总结
- en: 'The results above can be obtained using the following code:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 上述结果可以通过以下代码得到：
- en: '[PRE1]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/87e23140d1dd4151ec7f60baeb4f8ba2.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/87e23140d1dd4151ec7f60baeb4f8ba2.png)'
- en: 'Note: Due to some numerical instability in SVC, we cannot make the intercept
    from scikit-learn and the manual calculation to agree… That’s why I didn’t show
    how to calculate bias manually (even though it should be the same way as the linear
    kernel).'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：由于 SVC 存在一些数值不稳定性，我们无法使得 scikit-learn 中的截距与手动计算一致……这就是为什么我没有展示如何手动计算偏置（尽管它应该与线性核的计算方法相同）。
- en: Key Parameters
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键参数
- en: 'In SVM, the key parameter would be the penalty/regularization parameter C:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SVM 中，关键参数是惩罚/正则化参数 C：
- en: 'Large C: Tries hard to classify all training points correctly, potentially
    overfitting'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大 C：努力准确分类所有训练点，可能会导致过拟合
- en: 'Small C: Allows more misclassifications but aims for a simpler, more general
    model'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小 C：允许更多的误分类，但目标是得到一个更简单、更通用的模型
- en: Of course, if you are using non-linear kernel, you also need to adjust the degree
    (and coefficients) related to that kernel.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果你使用的是非线性核，你还需要调整与该核相关的度数（和系数）。
- en: Final Remarks
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最后的备注
- en: 'We’ve gone over a lot of the key concepts in SVMs (and how they work), but
    the main idea is this: It’s all about finding the right balance. You want your
    SVM to learn the important patterns in your data without trying too hard on getting
    every single training data on the correct side of the hyperplane. If it’s too
    strict, it might miss the big picture. If it’s too flexible, it might see patterns
    that aren’t really there. The trick is to tune your SVM so it can identify the
    real trends while still being adaptable enough to handle new data. Get this balance
    right, and you’ve got a powerful tool that can handle all sorts of classification
    problems.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了很多关于 SVM 的关键概念（以及它们的工作原理），但主要思想是这样的：这全是关于找到合适的平衡。你希望你的 SVM 学会识别数据中的重要模式，而不是过于努力地让每一个训练数据都落在超平面的正确一侧。如果它太严格，可能会错过全局；如果它太灵活，可能会看到一些并不存在的模式。诀窍是调整你的
    SVM，使其能够识别出真正的趋势，同时仍具有足够的适应性来处理新数据。把这个平衡做好，你就有了一个可以解决各种分类问题的强大工具。
- en: Further Reading
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: For a detailed explanation of the [Support Vector Machine](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)
    and its implementation in scikit-learn, readers can refer to the official documentation,
    which provides comprehensive information on its usage and parameters.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 对于关于[支持向量机](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)及其在
    scikit-learn 中实现的详细解释，读者可以参考官方文档，该文档提供了关于其使用和参数的全面信息。
- en: Technical Environment
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术环境
- en: This article uses Python 3.7 and scikit-learn 1.5\. While the concepts discussed
    are generally applicable, specific code implementations may vary slightly with
    different versions.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 本文使用的是 Python 3.7 和 scikit-learn 1.5。虽然所讨论的概念通常适用，但不同版本的具体代码实现可能略有不同。
- en: About the Illustrations
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于插图
- en: Unless otherwise noted, all images are created by the author, incorporating
    licensed design elements from Canva Pro.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图片均由作者创作，融合了来自 Canva Pro 的授权设计元素。
- en: Reference
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] T. M. Mitchell, [Machine Learning](https://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html)
    (1997), McGraw-Hill Science/Engineering/Math, pp. 59'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] T. M. Mitchell, [机器学习](https://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html)
    (1997), McGraw-Hill Science/Engineering/Math，第59页'
- en: '𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝘾𝙡𝙖𝙨𝙨𝙞𝙛𝙞𝙘𝙖𝙩𝙞𝙤𝙣 𝘼𝙡𝙜𝙤𝙧𝙞𝙩𝙝𝙢𝙨 𝙝𝙚𝙧𝙚:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝘾𝙡𝙖𝙨𝙨𝙞𝙛𝙞𝙘𝙖𝙩𝙞𝙤𝙣 𝘼𝙡𝙜𝙤𝙧𝙞𝙩𝙝𝙢𝙨 𝙝𝙚𝙧𝙚: '
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----62e831e7b9e9--------------------------------)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----62e831e7b9e9--------------------------------)'
- en: Classification Algorithms
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类算法
- en: '[View list](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----62e831e7b9e9--------------------------------)8
    stories![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----62e831e7b9e9--------------------------------)8个故事![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)'
- en: '𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还喜欢：
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----62e831e7b9e9--------------------------------)'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----62e831e7b9e9--------------------------------)'
- en: Regression Algorithms
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归算法
- en: '[View list](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----62e831e7b9e9--------------------------------)5
    stories![A cartoon doll with pigtails and a pink hat. This “dummy” doll, with
    its basic design and heart-adorned shirt, visually represents the concept of a
    dummy regressor in machine. Just as this toy-like figure is a simplified, static
    representation of a person, a dummy regressor is a basic models serve as baselines
    for more sophisticated analyses.](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)![Samy
    Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----62e831e7b9e9--------------------------------)5个故事！[一只戴着辫子和粉色帽子的卡通娃娃。这个“虚拟”娃娃，带着基础设计和心形装饰的衬衫，形象地代表了机器中的虚拟回归器概念。就像这个玩具般的人物是一个简化的、静态的人物表示一样，虚拟回归器也是一种基本模型，为更复杂的分析提供基准。](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)![Samy
    Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----62e831e7b9e9--------------------------------)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----62e831e7b9e9--------------------------------)'
- en: Ensemble Learning
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成学习
- en: '[View list](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----62e831e7b9e9--------------------------------)4
    stories![](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----62e831e7b9e9--------------------------------)4个故事！[](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)'
