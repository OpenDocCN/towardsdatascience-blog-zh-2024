- en: Building an Image Similarity Search Engine with FAISS and CLIP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨FAISSå’ŒCLIPæ„å»ºå›¾åƒç›¸ä¼¼åº¦æœç´¢å¼•æ“
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/building-an-image-similarity-search-engine-with-faiss-and-clip-2211126d08fa?source=collection_archive---------3-----------------------#2024-08-23](https://towardsdatascience.com/building-an-image-similarity-search-engine-with-faiss-and-clip-2211126d08fa?source=collection_archive---------3-----------------------#2024-08-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/building-an-image-similarity-search-engine-with-faiss-and-clip-2211126d08fa?source=collection_archive---------3-----------------------#2024-08-23](https://towardsdatascience.com/building-an-image-similarity-search-engine-with-faiss-and-clip-2211126d08fa?source=collection_archive---------3-----------------------#2024-08-23)
- en: A guided tutorial explaining how to search your image dataset with text or photo
    queries, using CLIP embedding and FAISS indexing.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸€ç¯‡æŒ‡å¯¼æ€§æ•™ç¨‹ï¼Œè§£é‡Šå¦‚ä½•ä½¿ç”¨CLIPåµŒå…¥å’ŒFAISSç´¢å¼•ï¼Œé€šè¿‡æ–‡æœ¬æˆ–ç…§ç‰‡æŸ¥è¯¢æœç´¢å›¾åƒæ•°æ®é›†ã€‚
- en: '[](https://medium.com/@lihigurarie?source=post_page---byline--2211126d08fa--------------------------------)[![Lihi
    Gur Arie, PhD](../Images/7a1eb30725a95159401c3672fa5f43ab.png)](https://medium.com/@lihigurarie?source=post_page---byline--2211126d08fa--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2211126d08fa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2211126d08fa--------------------------------)
    [Lihi Gur Arie, PhD](https://medium.com/@lihigurarie?source=post_page---byline--2211126d08fa--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@lihigurarie?source=post_page---byline--2211126d08fa--------------------------------)[![Lihi
    Gur Arie, PhD](../Images/7a1eb30725a95159401c3672fa5f43ab.png)](https://medium.com/@lihigurarie?source=post_page---byline--2211126d08fa--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2211126d08fa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2211126d08fa--------------------------------)
    [Lihi Gur Arie, åšå£«](https://medium.com/@lihigurarie?source=post_page---byline--2211126d08fa--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2211126d08fa--------------------------------)
    Â·6 min readÂ·Aug 23, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2211126d08fa--------------------------------)
    Â·6åˆ†é’Ÿé˜…è¯»Â·2024å¹´8æœˆ23æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/6aa0f89a4ae8ff874b4620b8a4ef873b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6aa0f89a4ae8ff874b4620b8a4ef873b.png)'
- en: Image was generated by author on Flux-Pro platform
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…åœ¨Flux-Proå¹³å°ä¸Šç”Ÿæˆ
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¼•è¨€
- en: Have you ever wanted to find an image among your never-ending image dataset,
    but found it too tedious? In this tutorial weâ€™ll build an image similarity search
    engine to easily find images using either a text query or a reference image. For
    your convenience, the complete code for this tutorial is provided at the bottom
    of the article as a **Colab notebook**.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ æ˜¯å¦æ›¾ç»æƒ³åœ¨ä½ çš„æ— å°½å›¾åƒæ•°æ®é›†ä¸­æ‰¾åˆ°ä¸€å¼ å›¾åƒï¼Œå´è§‰å¾—è¿™é¡¹ä»»åŠ¡å¤ªç¹çï¼Ÿåœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªå›¾åƒç›¸ä¼¼åº¦æœç´¢å¼•æ“ï¼Œé€šè¿‡æ–‡æœ¬æŸ¥è¯¢æˆ–å‚è€ƒå›¾åƒè½»æ¾æ‰¾åˆ°å›¾åƒã€‚ä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œæœ¬æ•™ç¨‹çš„å®Œæ•´ä»£ç å·²æä¾›åœ¨æ–‡ç« åº•éƒ¨ï¼Œä½œä¸ºä¸€ä¸ª**Colabç¬”è®°æœ¬**ã€‚
- en: If you donâ€™t have a paid Medium account, you can read for free[here](/building-an-image-similarity-search-engine-with-faiss-and-clip-2211126d08fa?sk=4d3ed082bd53b0e2ada2f660bd0da5ad).
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æ²¡æœ‰ä»˜è´¹çš„Mediumè´¦æˆ·ï¼Œä½ å¯ä»¥åœ¨[è¿™é‡Œ](/building-an-image-similarity-search-engine-with-faiss-and-clip-2211126d08fa?sk=4d3ed082bd53b0e2ada2f660bd0da5ad)å…è´¹é˜…è¯»ã€‚
- en: Pipeline Overview
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æµç¨‹æ¦‚è§ˆ
- en: The semantic meaning of an image can be represented by a numerical vector called
    an embedding. Comparing these low-dimensional embedding vectors, rather than the
    raw images, allows for efficient similarity searches. For each image in the dataset,
    weâ€™ll create an embedding vector and store it in an index. When a text query or
    a reference image is provided, its embedding is generated and compared against
    the indexed embeddings to retrieve the most similar images.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒçš„è¯­ä¹‰æ„ä¹‰å¯ä»¥é€šè¿‡ä¸€ä¸ªç§°ä¸ºåµŒå…¥ï¼ˆembeddingï¼‰çš„æ•°å€¼å‘é‡è¡¨ç¤ºã€‚é€šè¿‡æ¯”è¾ƒè¿™äº›ä½ç»´çš„åµŒå…¥å‘é‡ï¼Œè€Œä¸æ˜¯åŸå§‹å›¾åƒï¼Œå¯ä»¥é«˜æ•ˆåœ°è¿›è¡Œç›¸ä¼¼åº¦æœç´¢ã€‚å¯¹äºæ•°æ®é›†ä¸­çš„æ¯ä¸€å¼ å›¾ç‰‡ï¼Œæˆ‘ä»¬éƒ½ä¼šåˆ›å»ºä¸€ä¸ªåµŒå…¥å‘é‡å¹¶å°†å…¶å­˜å‚¨åœ¨ç´¢å¼•ä¸­ã€‚å½“æä¾›æ–‡æœ¬æŸ¥è¯¢æˆ–å‚è€ƒå›¾åƒæ—¶ï¼Œä¼šç”Ÿæˆå…¶åµŒå…¥å¹¶ä¸ç´¢å¼•ä¸­çš„åµŒå…¥è¿›è¡Œæ¯”è¾ƒï¼Œä»¥æ£€ç´¢æœ€ç›¸ä¼¼çš„å›¾åƒã€‚
- en: 'Hereâ€™s a brief overview:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ç®€è¦æ¦‚è§ˆï¼š
- en: '**Embedding:** The embeddings of the images are extracted using the CLIP model.'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åµŒå…¥**ï¼šå›¾åƒçš„åµŒå…¥æ˜¯é€šè¿‡CLIPæ¨¡å‹æå–çš„ã€‚'
- en: '**Indexing**: The embeddings are stored as a FAISS index.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ç´¢å¼•**ï¼šåµŒå…¥å‘é‡è¢«å­˜å‚¨ä¸ºFAISSç´¢å¼•ã€‚'
- en: '**Retrieval**: With FAISS, The embedding of the query is compared against the
    indexed embeddings to retrieve the most similar images.'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ£€ç´¢**ï¼šä½¿ç”¨FAISSï¼ŒæŸ¥è¯¢çš„åµŒå…¥ä¸ç´¢å¼•ä¸­çš„åµŒå…¥è¿›è¡Œæ¯”è¾ƒï¼Œä»è€Œæ£€ç´¢æœ€ç›¸ä¼¼çš„å›¾åƒã€‚'
- en: CLIP Model
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CLIPæ¨¡å‹
- en: The CLIP (Contrastive Language-Image Pre-training) model, developed by OpenAI,
    is a multi-modal vision and language model that maps images and text to the same
    latent space. Since we will use both image and text queries to search for images,
    we will use the CLIP model to embed our data. For further reading about CLIP,
    you can check out my previous article [here](/clip-creating-image-classifiers-without-data-b21c72b741fa?sk=88fdd2c1a132538015968df3f49b64b1).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: CLIPï¼ˆå¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒï¼‰æ¨¡å‹æ˜¯ç”± OpenAI å¼€å‘çš„å¤šæ¨¡æ€è§†è§‰ä¸è¯­è¨€æ¨¡å‹ï¼Œå®ƒå°†å›¾åƒå’Œæ–‡æœ¬æ˜ å°„åˆ°ç›¸åŒçš„æ½œåœ¨ç©ºé—´ã€‚ç”±äºæˆ‘ä»¬å°†ä½¿ç”¨å›¾åƒå’Œæ–‡æœ¬æŸ¥è¯¢æ¥æœç´¢å›¾åƒï¼Œæˆ‘ä»¬å°†ä½¿ç”¨
    CLIP æ¨¡å‹æ¥åµŒå…¥æˆ‘ä»¬çš„æ•°æ®ã€‚å…³äº CLIP çš„è¿›ä¸€æ­¥é˜…è¯»ï¼Œæ‚¨å¯ä»¥æŸ¥çœ‹æˆ‘ä¹‹å‰çš„æ–‡ç« [è¿™é‡Œ](/clip-creating-image-classifiers-without-data-b21c72b741fa?sk=88fdd2c1a132538015968df3f49b64b1)ã€‚
- en: FAISS Index
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FAISS ç´¢å¼•
- en: FAISS (Facebook AI Similarity Search) is an open-source library developed by
    Meta. It is built around the Index object that stores the database embedding vectors.
    FAISS enables efficient similarity search and clustering of dense vectors, and
    we will use it to index our dataset and retrieve the photos that resemble to the
    query.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: FAISSï¼ˆFacebook AI ç›¸ä¼¼åº¦æœç´¢ï¼‰æ˜¯ Meta å¼€å‘çš„å¼€æºåº“ã€‚å®ƒå›´ç»•å­˜å‚¨æ•°æ®åº“åµŒå…¥å‘é‡çš„ç´¢å¼•å¯¹è±¡æ„å»ºã€‚FAISS ä½¿å¾—å¯†é›†å‘é‡çš„é«˜æ•ˆç›¸ä¼¼åº¦æœç´¢å’Œèšç±»æˆä¸ºå¯èƒ½ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å®ƒå¯¹æˆ‘ä»¬çš„æ•°æ®é›†è¿›è¡Œç´¢å¼•ï¼Œå¹¶æ£€ç´¢ä¸æŸ¥è¯¢ç›¸ä¼¼çš„ç…§ç‰‡ã€‚
- en: Code Implementation
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»£ç å®ç°
- en: '**Step 1 â€” Dataset Exploration**'
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ç¬¬ 1 æ­¥ â€” æ•°æ®é›†æ¢ç´¢**'
- en: 'To create the image dataset for this tutorial I collected 52 images of varied
    topics from [Pexels](https://www.pexels.com/). To get the feeling, lets observe
    10 random images:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åˆ›å»ºæœ¬æ•™ç¨‹çš„å›¾åƒæ•°æ®é›†ï¼Œæˆ‘ä»[Pexels](https://www.pexels.com/)æ”¶é›†äº† 52 å¼ å„ç§ä¸»é¢˜çš„å›¾åƒã€‚ä¸ºäº†å¸®åŠ©ç†è§£ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹
    10 å¼ éšæœºå›¾åƒï¼š
- en: '![](../Images/5783da36e449663c611c2569c8921c43.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5783da36e449663c611c2569c8921c43.png)'
- en: '**Step 2 â€” Extract CLIP Embeddings from the Image Dataset**'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ç¬¬ 2 æ­¥ â€” ä»å›¾åƒæ•°æ®é›†ä¸­æå– CLIP åµŒå…¥å‘é‡**'
- en: 'To extract CLIP embeddings, weâ€˜ll first load the CLIP model using the HuggingFace
    SentenceTransformer library:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æå– CLIP åµŒå…¥å‘é‡ï¼Œæˆ‘ä»¬å°†é¦–å…ˆä½¿ç”¨ HuggingFace SentenceTransformer åº“åŠ è½½ CLIP æ¨¡å‹ï¼š
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, weâ€™ll create a function that iterates through our dataset directory with
    `glob`, opens each image with `PIL Image.open`, and generates an embedding vector
    for each image with `CLIP model.encode`. It returns a list of the embedding vectors
    and a list of the paths of our images dataset:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œä½¿ç”¨ `glob` éå†æˆ‘ä»¬çš„æ•°æ®é›†ç›®å½•ï¼Œé€šè¿‡ `PIL Image.open` æ‰“å¼€æ¯ä¸ªå›¾åƒï¼Œå¹¶ä½¿ç”¨ `CLIP model.encode`
    ä¸ºæ¯ä¸ªå›¾åƒç”Ÿæˆä¸€ä¸ªåµŒå…¥å‘é‡ã€‚å®ƒå°†è¿”å›ä¸€ä¸ªåµŒå…¥å‘é‡åˆ—è¡¨å’Œæˆ‘ä»¬å›¾åƒæ•°æ®é›†è·¯å¾„çš„åˆ—è¡¨ï¼š
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Step 3 â€” Generate FAISS Index**'
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ç¬¬ 3 æ­¥ â€” ç”Ÿæˆ FAISS ç´¢å¼•**'
- en: The next step is to create a FAISS index from the embedding vectors list. FAISS
    offers various distance metrics for similarity search, including Inner Product
    (IP) and L2 (Euclidean) distance.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥æ˜¯ä»åµŒå…¥å‘é‡åˆ—è¡¨åˆ›å»º FAISS ç´¢å¼•ã€‚FAISS æä¾›äº†å¤šç§è·ç¦»åº¦é‡æ–¹æ³•æ¥è¿›è¡Œç›¸ä¼¼åº¦æœç´¢ï¼ŒåŒ…æ‹¬å†…ç§¯ï¼ˆIPï¼‰å’Œ L2ï¼ˆæ¬§å‡ é‡Œå¾—ï¼‰è·ç¦»ã€‚
- en: FAISS also offers various indexing options. It can use approximation or compression
    technique to handle large datasets efficiently while balancing search speed and
    accuracy. In this tutorial we will use a â€˜Flatâ€™ index, which performs a brute-force
    search by comparing the query vector against every single vector in the dataset,
    ensuring exact results at the cost of higher computational complexity.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: FAISS è¿˜æä¾›äº†å¤šç§ç´¢å¼•é€‰é¡¹ã€‚å®ƒå¯ä»¥ä½¿ç”¨è¿‘ä¼¼æˆ–å‹ç¼©æŠ€æœ¯é«˜æ•ˆåœ°å¤„ç†å¤§æ•°æ®é›†ï¼ŒåŒæ—¶å¹³è¡¡æœç´¢é€Ÿåº¦å’Œç²¾åº¦ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨â€œFlatâ€ç´¢å¼•ï¼Œå®ƒé€šè¿‡å°†æŸ¥è¯¢å‘é‡ä¸æ•°æ®é›†ä¸­çš„æ¯ä¸ªå‘é‡è¿›è¡Œæ¯”è¾ƒæ¥æ‰§è¡Œæš´åŠ›æœç´¢ï¼Œç¡®ä¿ç²¾ç¡®ç»“æœï¼Œä½†ä»£ä»·æ˜¯æ›´é«˜çš„è®¡ç®—å¤æ‚åº¦ã€‚
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `faiss.IndexFlatIP` initializes an Index for Inner Product similarity, wrapped
    in an `faiss.IndexIDMap` to associate each vector with an ID. Next, the `index.add_with_ids`
    adds the vectors to the index with sequential IDâ€™s, and the index is saved to
    disk along with the image paths.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`faiss.IndexFlatIP` åˆå§‹åŒ–ä¸€ä¸ªç”¨äºå†…ç§¯ç›¸ä¼¼åº¦çš„ç´¢å¼•ï¼Œå°è£…åœ¨ `faiss.IndexIDMap` ä¸­ï¼Œå°†æ¯ä¸ªå‘é‡ä¸ä¸€ä¸ª ID å…³è”ã€‚æ¥ä¸‹æ¥ï¼Œ`index.add_with_ids`
    å°†å‘é‡æ·»åŠ åˆ°ç´¢å¼•ä¸­ï¼Œå¹¶åˆ†é…é¡ºåº IDï¼Œç´¢å¼•è¿åŒå›¾åƒè·¯å¾„ä¸€èµ·ä¿å­˜åˆ°ç£ç›˜ã€‚'
- en: 'The index can be used immediately or saved to disk for future use .To load
    the FAISS index we will use this function:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ç´¢å¼•å¯ä»¥ç«‹å³ä½¿ç”¨ï¼Œä¹Ÿå¯ä»¥ä¿å­˜åˆ°ç£ç›˜ä»¥ä¾›å°†æ¥ä½¿ç”¨ã€‚è¦åŠ è½½ FAISS ç´¢å¼•ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹å‡½æ•°ï¼š
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Step 4 â€” Retrieve Images by a Text Query or a Reference Image**'
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ç¬¬ 4 æ­¥ â€” é€šè¿‡æ–‡æœ¬æŸ¥è¯¢æˆ–å‚è€ƒå›¾åƒæ£€ç´¢å›¾åƒ**'
- en: With our FAISS index built, we can now retrieve images using either text queries
    or reference images. If the query is an image path, the query is opened with `PIL
    Image.open`. Next, the query embedding vector is extracted with `CLIP model.encode`.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ„å»ºå¥½ FAISS ç´¢å¼•åï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥ä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢æˆ–å‚è€ƒå›¾åƒæ¥æ£€ç´¢å›¾åƒã€‚å¦‚æœæŸ¥è¯¢æ˜¯å›¾åƒè·¯å¾„ï¼Œåˆ™é€šè¿‡ `PIL Image.open` æ‰“å¼€æŸ¥è¯¢ã€‚æ¥ç€ï¼Œé€šè¿‡
    `CLIP model.encode` æå–æŸ¥è¯¢çš„åµŒå…¥å‘é‡ã€‚
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The Retrieval is happening on the `index.search` method. It implements a k-Nearest
    Neighbors (kNN) search to find the `k` most similar vectors to the query vector.
    We can adjust the value of k by changing the `top_k` parameter. The distance metric
    used in the kNN search in our implementation is the cosine similarity. The function
    returns the query and a list of retrieve images paths.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€ç´¢å‘ç”Ÿåœ¨`index.search`æ–¹æ³•ä¸­ã€‚å®ƒå®ç°äº†kè¿‘é‚»ï¼ˆkNNï¼‰æœç´¢ï¼Œç”¨äºæŸ¥æ‰¾ä¸æŸ¥è¯¢å‘é‡æœ€ç›¸ä¼¼çš„`k`ä¸ªå‘é‡ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡æ›´æ”¹`top_k`å‚æ•°æ¥è°ƒæ•´kçš„å€¼ã€‚åœ¨æˆ‘ä»¬çš„å®ç°ä¸­ï¼ŒkNNæœç´¢ä½¿ç”¨çš„è·ç¦»åº¦é‡æ˜¯ä½™å¼¦ç›¸ä¼¼åº¦ã€‚è¯¥å‡½æ•°è¿”å›æŸ¥è¯¢å’Œä¸€ç³»åˆ—è·å–çš„å›¾ç‰‡è·¯å¾„ã€‚
- en: '**Search with a Text Query:**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢è¿›è¡Œæœç´¢ï¼š**'
- en: 'Now we are ready to examine the search results. The helper function `visualize_results`
    displays the results. You can fined it in the associated Colab notebook. Lets
    explore the retrieved most similar 3 images for the text query â€œballâ€ for example:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å‡†å¤‡å¥½æ£€æŸ¥æœç´¢ç»“æœäº†ã€‚è¾…åŠ©å‡½æ•°`visualize_results`å±•ç¤ºäº†è¿™äº›ç»“æœã€‚ä½ å¯ä»¥åœ¨å…³è”çš„Colabç¬”è®°æœ¬ä¸­æ‰¾åˆ°å®ƒã€‚è®©æˆ‘ä»¬ä»¥æ–‡æœ¬æŸ¥è¯¢â€œballâ€ä¸ºä¾‹ï¼Œæ¢ç´¢è·å–çš„ä¸‰ä¸ªæœ€ç›¸ä¼¼çš„å›¾ç‰‡ï¼š
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/0982e9cc9e59715d51147782f62d60f5.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0982e9cc9e59715d51147782f62d60f5.png)'
- en: 'Retrieved images with the query: â€˜a ballâ€™'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æŸ¥è¯¢â€œa ballâ€è·å–çš„å›¾ç‰‡
- en: 'For the query â€˜animalâ€™ we get:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæŸ¥è¯¢â€œanimalâ€ï¼Œæˆ‘ä»¬å¾—åˆ°äº†ï¼š
- en: '![](../Images/6beffd16ea96ed9928100a9d117bcb0d.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6beffd16ea96ed9928100a9d117bcb0d.png)'
- en: 'Retrieved images with the query: â€˜animalâ€™'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æŸ¥è¯¢â€œanimalâ€è·å–çš„å›¾ç‰‡
- en: '**Search with a Reference Image:**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨å‚è€ƒå›¾ç‰‡è¿›è¡Œæœç´¢ï¼š**'
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/015aeddcd340d75270dd41904b56d11d.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/015aeddcd340d75270dd41904b56d11d.png)'
- en: Query and Retrieved images
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥è¯¢å’Œè·å–çš„å›¾ç‰‡
- en: As we can see, we get pretty cool results for an off-the-shelf pre-trained model.
    When we searched by a reference image of an eye painting, besides finding the
    original image, it found one match of eyeglass and one of a different painting.
    This demonstrates different aspects of the semantic meaning of the query image.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬æ‰€è§ï¼Œæˆ‘ä»¬å¯¹ç°æˆçš„é¢„è®­ç»ƒæ¨¡å‹å¾—åˆ°äº†ç›¸å½“ä¸é”™çš„ç»“æœã€‚å½“æˆ‘ä»¬ç”¨ä¸€å¹…çœ¼ç›ç”»ä½œä¸ºå‚è€ƒå›¾ç‰‡è¿›è¡Œæœç´¢æ—¶ï¼Œé™¤äº†æ‰¾åˆ°åŸå§‹å›¾ç‰‡å¤–ï¼Œè¿˜æ‰¾åˆ°äº†ä¸€å¼ çœ¼é•œå’Œä¸€å¼ ä¸åŒç”»ä½œçš„åŒ¹é…ã€‚è¿™å±•ç¤ºäº†æŸ¥è¯¢å›¾ç‰‡çš„è¯­ä¹‰å«ä¹‰çš„ä¸åŒæ–¹é¢ã€‚
- en: You can try other queries on the provided Colab notebook to see how the model
    performs with different text and image inputs.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨æä¾›çš„Colabç¬”è®°æœ¬ä¸­å°è¯•å…¶ä»–æŸ¥è¯¢ï¼ŒæŸ¥çœ‹æ¨¡å‹åœ¨ä¸åŒæ–‡æœ¬å’Œå›¾åƒè¾“å…¥ä¸‹çš„è¡¨ç°ã€‚
- en: Concluding Remarks
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è¯­
- en: In this tutorial we built a basic image similarity search engine using CLIP
    and FAISS. The retrieved images shared similar semantic meaning with the query,
    indicating the effectiveness of the approach. Though CLIP shows nice results for
    a Zero Shot model, it might exhibit low performance on Out-of-Distribution data,
    Fine-Grained tasks and inherit the natural bias of the data it was trained on.
    To overcome these limitations you can try other CLIP-like pre-trained models as
    in [OpenClip](https://github.com/mlfoundations/open_clip/tree/main), or fine-tune
    CLIP on your own custom dataset.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨CLIPå’ŒFAISSæ„å»ºäº†ä¸€ä¸ªåŸºæœ¬çš„å›¾åƒç›¸ä¼¼æ€§æœç´¢å¼•æ“ã€‚è·å–çš„å›¾ç‰‡ä¸æŸ¥è¯¢å…·æœ‰ç›¸ä¼¼çš„è¯­ä¹‰å«ä¹‰ï¼Œè¡¨æ˜è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å°½ç®¡CLIPå¯¹é›¶æ ·æœ¬æ¨¡å‹æ˜¾ç¤ºå‡ºä¸é”™çš„ç»“æœï¼Œä½†å®ƒå¯èƒ½åœ¨åˆ†å¸ƒå¤–æ•°æ®ã€ç»†ç²’åº¦ä»»åŠ¡ä¸­è¡¨ç°è¾ƒå·®ï¼Œå¹¶ä¸”ç»§æ‰¿äº†å®ƒæ‰€è®­ç»ƒæ•°æ®çš„è‡ªç„¶åå·®ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œä½ å¯ä»¥å°è¯•ä½¿ç”¨å…¶ä»–ç±»ä¼¼CLIPçš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¦‚åœ¨[OpenClip](https://github.com/mlfoundations/open_clip/tree/main)ä¸­ï¼Œæˆ–è€…åœ¨ä½ è‡ªå·±çš„å®šåˆ¶æ•°æ®é›†ä¸Šå¾®è°ƒCLIPã€‚
- en: Thank you for reading!
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢é˜…è¯»ï¼
- en: Congratulations on making it all the way here. Click ğŸ‘ to show your appreciation
    and raise the algorithm self esteem ğŸ¤“
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œä½ ä¸€è·¯èµ°åˆ°äº†è¿™é‡Œã€‚ç‚¹å‡»ğŸ‘è¡¨ç¤ºæ„Ÿè°¢ï¼Œæå‡ç®—æ³•çš„è‡ªå°Šå¿ƒğŸ¤“
- en: '**Want to learn more?**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**æƒ³äº†è§£æ›´å¤šï¼Ÿ**'
- en: '[**Explore**](https://medium.com/@lihigurarie) additional articles Iâ€™ve written'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**æ¢ç´¢**](https://medium.com/@lihigurarie)æˆ‘å†™çš„å…¶ä»–æ–‡ç« '
- en: '[**Subscribe**](https://medium.com/@lihigurarie/subscribe)to get notified when
    I publish articles'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**è®¢é˜…**](https://medium.com/@lihigurarie/subscribe)ä»¥ä¾¿åœ¨æˆ‘å‘å¸ƒæ–‡ç« æ—¶è·å¾—é€šçŸ¥'
- en: Follow me on [**Linkedin**](https://www.linkedin.com/in/lihi-gur-arie/)
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨[**Linkedin**](https://www.linkedin.com/in/lihi-gur-arie/)ä¸Šå…³æ³¨æˆ‘
- en: 'Full Code as Colab notebook:'
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®Œæ•´ä»£ç ä½œä¸ºColabç¬”è®°æœ¬ï¼š
- en: Colab Notebook [Link](https://gist.github.com/Lihi-Gur-Arie/7cac63dbffde55449d2444e402d87bfc)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Colabç¬”è®°æœ¬[é“¾æ¥](https://gist.github.com/Lihi-Gur-Arie/7cac63dbffde55449d2444e402d87bfc)
