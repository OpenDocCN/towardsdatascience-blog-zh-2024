- en: Building an Image Similarity Search Engine with FAISS and CLIP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用FAISS和CLIP构建图像相似度搜索引擎
- en: 原文：[https://towardsdatascience.com/building-an-image-similarity-search-engine-with-faiss-and-clip-2211126d08fa?source=collection_archive---------3-----------------------#2024-08-23](https://towardsdatascience.com/building-an-image-similarity-search-engine-with-faiss-and-clip-2211126d08fa?source=collection_archive---------3-----------------------#2024-08-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/building-an-image-similarity-search-engine-with-faiss-and-clip-2211126d08fa?source=collection_archive---------3-----------------------#2024-08-23](https://towardsdatascience.com/building-an-image-similarity-search-engine-with-faiss-and-clip-2211126d08fa?source=collection_archive---------3-----------------------#2024-08-23)
- en: A guided tutorial explaining how to search your image dataset with text or photo
    queries, using CLIP embedding and FAISS indexing.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一篇指导性教程，解释如何使用CLIP嵌入和FAISS索引，通过文本或照片查询搜索图像数据集。
- en: '[](https://medium.com/@lihigurarie?source=post_page---byline--2211126d08fa--------------------------------)[![Lihi
    Gur Arie, PhD](../Images/7a1eb30725a95159401c3672fa5f43ab.png)](https://medium.com/@lihigurarie?source=post_page---byline--2211126d08fa--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2211126d08fa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2211126d08fa--------------------------------)
    [Lihi Gur Arie, PhD](https://medium.com/@lihigurarie?source=post_page---byline--2211126d08fa--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@lihigurarie?source=post_page---byline--2211126d08fa--------------------------------)[![Lihi
    Gur Arie, PhD](../Images/7a1eb30725a95159401c3672fa5f43ab.png)](https://medium.com/@lihigurarie?source=post_page---byline--2211126d08fa--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2211126d08fa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2211126d08fa--------------------------------)
    [Lihi Gur Arie, 博士](https://medium.com/@lihigurarie?source=post_page---byline--2211126d08fa--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2211126d08fa--------------------------------)
    ·6 min read·Aug 23, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2211126d08fa--------------------------------)
    ·6分钟阅读·2024年8月23日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/6aa0f89a4ae8ff874b4620b8a4ef873b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6aa0f89a4ae8ff874b4620b8a4ef873b.png)'
- en: Image was generated by author on Flux-Pro platform
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者在Flux-Pro平台上生成
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: Have you ever wanted to find an image among your never-ending image dataset,
    but found it too tedious? In this tutorial we’ll build an image similarity search
    engine to easily find images using either a text query or a reference image. For
    your convenience, the complete code for this tutorial is provided at the bottom
    of the article as a **Colab notebook**.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否曾经想在你的无尽图像数据集中找到一张图像，却觉得这项任务太繁琐？在本教程中，我们将构建一个图像相似度搜索引擎，通过文本查询或参考图像轻松找到图像。为了方便起见，本教程的完整代码已提供在文章底部，作为一个**Colab笔记本**。
- en: If you don’t have a paid Medium account, you can read for free[here](/building-an-image-similarity-search-engine-with-faiss-and-clip-2211126d08fa?sk=4d3ed082bd53b0e2ada2f660bd0da5ad).
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你没有付费的Medium账户，你可以在[这里](/building-an-image-similarity-search-engine-with-faiss-and-clip-2211126d08fa?sk=4d3ed082bd53b0e2ada2f660bd0da5ad)免费阅读。
- en: Pipeline Overview
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流程概览
- en: The semantic meaning of an image can be represented by a numerical vector called
    an embedding. Comparing these low-dimensional embedding vectors, rather than the
    raw images, allows for efficient similarity searches. For each image in the dataset,
    we’ll create an embedding vector and store it in an index. When a text query or
    a reference image is provided, its embedding is generated and compared against
    the indexed embeddings to retrieve the most similar images.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图像的语义意义可以通过一个称为嵌入（embedding）的数值向量表示。通过比较这些低维的嵌入向量，而不是原始图像，可以高效地进行相似度搜索。对于数据集中的每一张图片，我们都会创建一个嵌入向量并将其存储在索引中。当提供文本查询或参考图像时，会生成其嵌入并与索引中的嵌入进行比较，以检索最相似的图像。
- en: 'Here’s a brief overview:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是简要概览：
- en: '**Embedding:** The embeddings of the images are extracted using the CLIP model.'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**嵌入**：图像的嵌入是通过CLIP模型提取的。'
- en: '**Indexing**: The embeddings are stored as a FAISS index.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**索引**：嵌入向量被存储为FAISS索引。'
- en: '**Retrieval**: With FAISS, The embedding of the query is compared against the
    indexed embeddings to retrieve the most similar images.'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**检索**：使用FAISS，查询的嵌入与索引中的嵌入进行比较，从而检索最相似的图像。'
- en: CLIP Model
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CLIP模型
- en: The CLIP (Contrastive Language-Image Pre-training) model, developed by OpenAI,
    is a multi-modal vision and language model that maps images and text to the same
    latent space. Since we will use both image and text queries to search for images,
    we will use the CLIP model to embed our data. For further reading about CLIP,
    you can check out my previous article [here](/clip-creating-image-classifiers-without-data-b21c72b741fa?sk=88fdd2c1a132538015968df3f49b64b1).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP（对比语言-图像预训练）模型是由 OpenAI 开发的多模态视觉与语言模型，它将图像和文本映射到相同的潜在空间。由于我们将使用图像和文本查询来搜索图像，我们将使用
    CLIP 模型来嵌入我们的数据。关于 CLIP 的进一步阅读，您可以查看我之前的文章[这里](/clip-creating-image-classifiers-without-data-b21c72b741fa?sk=88fdd2c1a132538015968df3f49b64b1)。
- en: FAISS Index
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FAISS 索引
- en: FAISS (Facebook AI Similarity Search) is an open-source library developed by
    Meta. It is built around the Index object that stores the database embedding vectors.
    FAISS enables efficient similarity search and clustering of dense vectors, and
    we will use it to index our dataset and retrieve the photos that resemble to the
    query.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: FAISS（Facebook AI 相似度搜索）是 Meta 开发的开源库。它围绕存储数据库嵌入向量的索引对象构建。FAISS 使得密集向量的高效相似度搜索和聚类成为可能，我们将使用它对我们的数据集进行索引，并检索与查询相似的照片。
- en: Code Implementation
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码实现
- en: '**Step 1 — Dataset Exploration**'
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**第 1 步 — 数据集探索**'
- en: 'To create the image dataset for this tutorial I collected 52 images of varied
    topics from [Pexels](https://www.pexels.com/). To get the feeling, lets observe
    10 random images:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建本教程的图像数据集，我从[Pexels](https://www.pexels.com/)收集了 52 张各种主题的图像。为了帮助理解，我们来看一下
    10 张随机图像：
- en: '![](../Images/5783da36e449663c611c2569c8921c43.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5783da36e449663c611c2569c8921c43.png)'
- en: '**Step 2 — Extract CLIP Embeddings from the Image Dataset**'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**第 2 步 — 从图像数据集中提取 CLIP 嵌入向量**'
- en: 'To extract CLIP embeddings, we‘ll first load the CLIP model using the HuggingFace
    SentenceTransformer library:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 要提取 CLIP 嵌入向量，我们将首先使用 HuggingFace SentenceTransformer 库加载 CLIP 模型：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we’ll create a function that iterates through our dataset directory with
    `glob`, opens each image with `PIL Image.open`, and generates an embedding vector
    for each image with `CLIP model.encode`. It returns a list of the embedding vectors
    and a list of the paths of our images dataset:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个函数，使用 `glob` 遍历我们的数据集目录，通过 `PIL Image.open` 打开每个图像，并使用 `CLIP model.encode`
    为每个图像生成一个嵌入向量。它将返回一个嵌入向量列表和我们图像数据集路径的列表：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Step 3 — Generate FAISS Index**'
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**第 3 步 — 生成 FAISS 索引**'
- en: The next step is to create a FAISS index from the embedding vectors list. FAISS
    offers various distance metrics for similarity search, including Inner Product
    (IP) and L2 (Euclidean) distance.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是从嵌入向量列表创建 FAISS 索引。FAISS 提供了多种距离度量方法来进行相似度搜索，包括内积（IP）和 L2（欧几里得）距离。
- en: FAISS also offers various indexing options. It can use approximation or compression
    technique to handle large datasets efficiently while balancing search speed and
    accuracy. In this tutorial we will use a ‘Flat’ index, which performs a brute-force
    search by comparing the query vector against every single vector in the dataset,
    ensuring exact results at the cost of higher computational complexity.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: FAISS 还提供了多种索引选项。它可以使用近似或压缩技术高效地处理大数据集，同时平衡搜索速度和精度。在本教程中，我们将使用“Flat”索引，它通过将查询向量与数据集中的每个向量进行比较来执行暴力搜索，确保精确结果，但代价是更高的计算复杂度。
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `faiss.IndexFlatIP` initializes an Index for Inner Product similarity, wrapped
    in an `faiss.IndexIDMap` to associate each vector with an ID. Next, the `index.add_with_ids`
    adds the vectors to the index with sequential ID’s, and the index is saved to
    disk along with the image paths.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`faiss.IndexFlatIP` 初始化一个用于内积相似度的索引，封装在 `faiss.IndexIDMap` 中，将每个向量与一个 ID 关联。接下来，`index.add_with_ids`
    将向量添加到索引中，并分配顺序 ID，索引连同图像路径一起保存到磁盘。'
- en: 'The index can be used immediately or saved to disk for future use .To load
    the FAISS index we will use this function:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 索引可以立即使用，也可以保存到磁盘以供将来使用。要加载 FAISS 索引，我们将使用以下函数：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Step 4 — Retrieve Images by a Text Query or a Reference Image**'
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**第 4 步 — 通过文本查询或参考图像检索图像**'
- en: With our FAISS index built, we can now retrieve images using either text queries
    or reference images. If the query is an image path, the query is opened with `PIL
    Image.open`. Next, the query embedding vector is extracted with `CLIP model.encode`.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建好 FAISS 索引后，我们现在可以使用文本查询或参考图像来检索图像。如果查询是图像路径，则通过 `PIL Image.open` 打开查询。接着，通过
    `CLIP model.encode` 提取查询的嵌入向量。
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The Retrieval is happening on the `index.search` method. It implements a k-Nearest
    Neighbors (kNN) search to find the `k` most similar vectors to the query vector.
    We can adjust the value of k by changing the `top_k` parameter. The distance metric
    used in the kNN search in our implementation is the cosine similarity. The function
    returns the query and a list of retrieve images paths.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 检索发生在`index.search`方法中。它实现了k近邻（kNN）搜索，用于查找与查询向量最相似的`k`个向量。我们可以通过更改`top_k`参数来调整k的值。在我们的实现中，kNN搜索使用的距离度量是余弦相似度。该函数返回查询和一系列获取的图片路径。
- en: '**Search with a Text Query:**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用文本查询进行搜索：**'
- en: 'Now we are ready to examine the search results. The helper function `visualize_results`
    displays the results. You can fined it in the associated Colab notebook. Lets
    explore the retrieved most similar 3 images for the text query “ball” for example:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好检查搜索结果了。辅助函数`visualize_results`展示了这些结果。你可以在关联的Colab笔记本中找到它。让我们以文本查询“ball”为例，探索获取的三个最相似的图片：
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/0982e9cc9e59715d51147782f62d60f5.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0982e9cc9e59715d51147782f62d60f5.png)'
- en: 'Retrieved images with the query: ‘a ball’'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 使用查询“a ball”获取的图片
- en: 'For the query ‘animal’ we get:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于查询“animal”，我们得到了：
- en: '![](../Images/6beffd16ea96ed9928100a9d117bcb0d.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6beffd16ea96ed9928100a9d117bcb0d.png)'
- en: 'Retrieved images with the query: ‘animal’'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用查询“animal”获取的图片
- en: '**Search with a Reference Image:**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用参考图片进行搜索：**'
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/015aeddcd340d75270dd41904b56d11d.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/015aeddcd340d75270dd41904b56d11d.png)'
- en: Query and Retrieved images
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 查询和获取的图片
- en: As we can see, we get pretty cool results for an off-the-shelf pre-trained model.
    When we searched by a reference image of an eye painting, besides finding the
    original image, it found one match of eyeglass and one of a different painting.
    This demonstrates different aspects of the semantic meaning of the query image.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，我们对现成的预训练模型得到了相当不错的结果。当我们用一幅眼睛画作为参考图片进行搜索时，除了找到原始图片外，还找到了一张眼镜和一张不同画作的匹配。这展示了查询图片的语义含义的不同方面。
- en: You can try other queries on the provided Colab notebook to see how the model
    performs with different text and image inputs.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在提供的Colab笔记本中尝试其他查询，查看模型在不同文本和图像输入下的表现。
- en: Concluding Remarks
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结语
- en: In this tutorial we built a basic image similarity search engine using CLIP
    and FAISS. The retrieved images shared similar semantic meaning with the query,
    indicating the effectiveness of the approach. Though CLIP shows nice results for
    a Zero Shot model, it might exhibit low performance on Out-of-Distribution data,
    Fine-Grained tasks and inherit the natural bias of the data it was trained on.
    To overcome these limitations you can try other CLIP-like pre-trained models as
    in [OpenClip](https://github.com/mlfoundations/open_clip/tree/main), or fine-tune
    CLIP on your own custom dataset.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们使用CLIP和FAISS构建了一个基本的图像相似性搜索引擎。获取的图片与查询具有相似的语义含义，表明该方法的有效性。尽管CLIP对零样本模型显示出不错的结果，但它可能在分布外数据、细粒度任务中表现较差，并且继承了它所训练数据的自然偏差。为了克服这些限制，你可以尝试使用其他类似CLIP的预训练模型，如在[OpenClip](https://github.com/mlfoundations/open_clip/tree/main)中，或者在你自己的定制数据集上微调CLIP。
- en: Thank you for reading!
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: Congratulations on making it all the way here. Click 👍 to show your appreciation
    and raise the algorithm self esteem 🤓
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你一路走到了这里。点击👍表示感谢，提升算法的自尊心🤓
- en: '**Want to learn more?**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**想了解更多？**'
- en: '[**Explore**](https://medium.com/@lihigurarie) additional articles I’ve written'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**探索**](https://medium.com/@lihigurarie)我写的其他文章'
- en: '[**Subscribe**](https://medium.com/@lihigurarie/subscribe)to get notified when
    I publish articles'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**订阅**](https://medium.com/@lihigurarie/subscribe)以便在我发布文章时获得通知'
- en: Follow me on [**Linkedin**](https://www.linkedin.com/in/lihi-gur-arie/)
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[**Linkedin**](https://www.linkedin.com/in/lihi-gur-arie/)上关注我
- en: 'Full Code as Colab notebook:'
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 完整代码作为Colab笔记本：
- en: Colab Notebook [Link](https://gist.github.com/Lihi-Gur-Arie/7cac63dbffde55449d2444e402d87bfc)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Colab笔记本[链接](https://gist.github.com/Lihi-Gur-Arie/7cac63dbffde55449d2444e402d87bfc)
