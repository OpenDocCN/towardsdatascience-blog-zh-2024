- en: A Visual Exploration of Semantic Text Chunking
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语义文本切分的可视化探索
- en: 原文：[https://towardsdatascience.com/a-visual-exploration-of-semantic-text-chunking-6bb46f728e30?source=collection_archive---------1-----------------------#2024-09-19](https://towardsdatascience.com/a-visual-exploration-of-semantic-text-chunking-6bb46f728e30?source=collection_archive---------1-----------------------#2024-09-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-visual-exploration-of-semantic-text-chunking-6bb46f728e30?source=collection_archive---------1-----------------------#2024-09-19](https://towardsdatascience.com/a-visual-exploration-of-semantic-text-chunking-6bb46f728e30?source=collection_archive---------1-----------------------#2024-09-19)
- en: '![](../Images/f3920b86cf68505cb2d81e429fb627c7.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3920b86cf68505cb2d81e429fb627c7.png)'
- en: Dalle3’s interpretation of “Semantic Chunking”. Image generated by the author.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Dalle3 对“语义切分”的解读。图像由作者生成。
- en: Use embeddings and visualization tools to split text into meaningful chunks
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用嵌入和可视化工具将文本切分为有意义的片段
- en: '[](https://medium.com/@rmartinshort?source=post_page---byline--6bb46f728e30--------------------------------)[![Robert
    Martin-Short](../Images/e3910071b72a914255b185b850579a5a.png)](https://medium.com/@rmartinshort?source=post_page---byline--6bb46f728e30--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6bb46f728e30--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6bb46f728e30--------------------------------)
    [Robert Martin-Short](https://medium.com/@rmartinshort?source=post_page---byline--6bb46f728e30--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@rmartinshort?source=post_page---byline--6bb46f728e30--------------------------------)[![Robert
    Martin-Short](../Images/e3910071b72a914255b185b850579a5a.png)](https://medium.com/@rmartinshort?source=post_page---byline--6bb46f728e30--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6bb46f728e30--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6bb46f728e30--------------------------------)
    [Robert Martin-Short](https://medium.com/@rmartinshort?source=post_page---byline--6bb46f728e30--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6bb46f728e30--------------------------------)
    ·18 min read·Sep 19, 2024
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6bb46f728e30--------------------------------)
    ·阅读时间 18 分钟·2024年9月19日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '***This article offers an explanation of semantic text chunking, a technique
    designed to automatically group similar pieces of text that can be employed as
    part of the pre-processing stage of a pipeline for Retrieval Augmented Generation
    (RAG) or a similar applications. We use visualizations to understand what the
    chunking is doing, and we explore some extensions that involve clustering and
    LLM-powered labeling. Check out the full code*** [***here***](https://github.com/rmartinshort/text_chunking)***.***'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '***本文解释了语义文本切分（semantic text chunking）这一技术，旨在自动将相似的文本片段分组，可以作为检索增强生成（RAG）或类似应用的预处理阶段的一部分。我们通过可视化来理解切分过程的作用，并探索一些涉及聚类和基于LLM的标注的扩展。完整代码请查看***
    [***这里***](https://github.com/rmartinshort/text_chunking)***。***'
- en: Automatic information retrieval and summarization of large volumes of text has
    many useful applications. One of the most well developed is Retrieval Augmented
    Generation (RAG), which involves extraction of relevant chunks of text from a
    large corpus — typically via semantic search or some other filtering step — in
    response to a user question. Then, the chunks are interpreted or summarized by
    an LLM with the aim of providing a high quality, accurate answer. In order for
    the extracted chunks to be as relevant as possible to the question its very helpful
    for them to be semantically coherent, meaning that each chunk is “about” a specific
    concept and contains a useful packet of information in it’s own right.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化的信息检索和大规模文本摘要有许多有用的应用。最为成熟的应用之一是检索增强生成（RAG），它涉及从大型语料库中提取相关文本片段——通常通过语义搜索或其他筛选步骤——以响应用户提问。然后，文本片段由LLM进行解读或摘要，旨在提供高质量、准确的答案。为了使提取的片段尽可能与问题相关，它们具有语义一致性是非常有帮助的，这意味着每个片段都“关于”一个特定概念，并且包含有用的信息包。
- en: Chunking has applications beyond RAG too. Imagine we have a complex document
    like a book or journal article and want to quickly understand what key concepts
    it contains. If the text can be clustered into semantically coherent groups and
    then each cluster summarized in some way, this can really help speed up time to
    insights. The excellent package [BertTopic](https://maartengr.github.io/BERTopic/index.html)
    (see [this article](/topics-per-class-using-bertopic-252314f2640) for a nice overview)
    can help here.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Chunking 不仅仅在 RAG 中有应用。想象一下我们有一本复杂的文档，比如书籍或期刊文章，并且希望快速理解它包含的关键概念。如果文本能够被聚类成语义上连贯的组别，然后对每个组别进行某种方式的总结，这可以极大地加快获得洞察的速度。优秀的工具包[BertTopic](https://maartengr.github.io/BERTopic/index.html)（可以参考[这篇文章](/topics-per-class-using-bertopic-252314f2640)获取一个不错的概述）可以在这里提供帮助。
- en: Visualization of the chunks can also be insightful, both as a final product
    and during development. Humans are visual learners in that our brains are much
    faster at gleaning information from graphs and images rather than streams of text.
    In my experience, it’s quite difficult to understand what a chunking algorithm
    has done to the text — and what the optimal parameters might be — without visualizing
    the chunks in some way or reading them all, which is impractical in the case of
    large documents.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对切分块的可视化也可以提供有益的洞察，无论是作为最终产品，还是在开发过程中。人类是视觉学习者，我们的大脑在从图表和图像中获取信息时，比从文本流中获取信息要快得多。根据我的经验，要理解一个切分算法对文本做了什么——以及最佳的参数是什么——如果没有某种方式可视化这些块，或者完全读完它们（对于大型文档来说，这是不现实的），是非常困难的。
- en: In this article, we’re going to explore a method to split text into semantically
    meaningful chunks with an emphasis on using graphs and plots to understand what’s
    going on. In doing so, we’ll touch on dimensionality reduction and hierarchical
    clustering of embedding vectors, in addition to the use of LLMs to summarize the
    chunks so that we can quickly see what information is present. My hope is that
    this might spark further ideas for anyone researching semantic chunking as a potential
    tool in their application. I’ll be using Python 3.9, LangChain and Seaborn here,
    with full details in the [repo](https://github.com/rmartinshort/text_chunking).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将探讨一种方法，将文本切分为语义上有意义的块，重点是使用图形和绘图来理解其发生的过程。在此过程中，我们将涉及降维和嵌入向量的层次聚类，以及使用大语言模型（LLM）对这些块进行总结，以便我们可以快速查看其中包含的信息。我的希望是，这能激发正在研究语义切分的任何人思考，将其作为应用中的潜在工具。我将在这里使用
    Python 3.9、LangChain 和 Seaborn，具体细节可以参见[代码仓库](https://github.com/rmartinshort/text_chunking)。
- en: '**1\. What is semantic chunking?**'
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**1\. 什么是语义切分（semantic chunking）？**'
- en: There are a few standard types of chunking and to learn more about them I recommend
    [this excellent tutorial](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb),
    which also provided inspiration for this article. Assuming we are dealing with
    English text, the simplest form of chunking is character based, where we choose
    a fixed window of characters and simply break up the text into chunks of that
    length. Optionally we can add an overlap between the chunks to preserve some indication
    of the sequential relationship between them. This is computationally straightforward
    but there is no guarantee that the chunks will be semantically meaningful or even
    complete sentences.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种标准的切分类型，如果你想了解更多，我推荐[这篇优秀的教程](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)，它也是这篇文章的灵感来源。假设我们处理的是英文文本，最简单的切分方式是基于字符的切分，我们选择一个固定长度的字符窗口，然后将文本分成该长度的块。我们还可以选择在块之间添加重叠，以保留它们之间的顺序关系。这种方法计算上比较简单，但无法保证这些块是语义上有意义的，甚至不一定是完整的句子。
- en: 'Recursive chunking is typically more useful and is seen as the go-to first
    algorithm for many applications. The process takes in hierarchical list of separators
    (the default in LangChain is `[“\n\n”, “\n”, “ ”, “”]` ) and a target length.
    It then splits up the text using the separators in a recursive way, advancing
    down the list until each chunk is less than or equal to the target length. This
    is much better at preserving full paragraphs and sentences, which is good because
    it makes the chunks much more likely to be coherent. However it does not consider
    semantics: If one sentence follows on from the last and happens to be at the end
    of the chunk window, the sentences will be separated.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 递归切分通常更有用，被视为许多应用的首选算法。该过程接受一系列分隔符（LangChain 中的默认值是 `[“\n\n”, “\n”, “ ”, “”]`）和一个目标长度。然后，它通过递归的方式使用分隔符拆分文本，逐步遍历分隔符列表，直到每个文本块的长度小于或等于目标长度。这种方法在保留完整段落和句子方面更为有效，这一点非常重要，因为它使得文本块更有可能保持连贯性。然而，它并不考虑语义：如果一个句子是紧接在上一个句子后面的，并恰好位于文本块的末尾，这些句子就会被分开。
- en: In semantic chunking, which has implementations in both [LangChain](https://python.langchain.com/v0.2/docs/how_to/semantic-chunker/)
    and [LlamaIndex](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/semantic_splitter/#llama_index.core.node_parser.SemanticSplitterNodeParser),
    the splits are made based on the cosine distance between embeddings of sequential
    chunks. So we start by dividing the text into small but coherent groups, perhaps
    using a recursive chunker.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在语义切分中，实施方法包括 [LangChain](https://python.langchain.com/v0.2/docs/how_to/semantic-chunker/)
    和 [LlamaIndex](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/semantic_splitter/#llama_index.core.node_parser.SemanticSplitterNodeParser)，切分是基于顺序块的嵌入之间的余弦距离来进行的。因此，我们首先将文本划分为小的但连贯的组，可能会使用递归切分器。
- en: Next we take vectorize each chunk using a model that has been trained to generate
    meaningful embeddings. Typically this takes the form of a transformer-based bi-encoder
    (see the [SentenceTransformers](https://sbert.net/) library for details and examples),
    or an [endpoint such as OpenAI’s](https://platform.openai.com/docs/guides/embeddings)
    `[text-embeddings-3-small](https://platform.openai.com/docs/guides/embeddings)`
    , which is what we use here. Finally, we look at the cosine distances between
    the embeddings of subsequent chunks and choose breakpoints where the distances
    are large. Ideally, this helps to create groups of text that are both coherent
    and semantically distinct.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用一个已训练好的模型将每个文本块向量化，该模型能够生成有意义的嵌入。通常，这种方法采用基于 Transformer 的双编码器（有关详细信息和示例，请参见
    [SentenceTransformers](https://sbert.net/) 库），或者是像 [OpenAI 的终端点](https://platform.openai.com/docs/guides/embeddings)
    `[text-embeddings-3-small](https://platform.openai.com/docs/guides/embeddings)`，这就是我们在这里使用的模型。最后，我们查看后续文本块嵌入之间的余弦距离，并选择那些距离较大的断点。理想情况下，这有助于创建既连贯又在语义上有区别的文本组。
- en: A recent extension of this called [semantic double chunk merging](https://docs.llamaindex.ai/en/stable/examples/node_parsers/semantic_double_merging_chunking/)
    (see [this article](https://bitpeak.pl/chunking-methods-in-rag-methods-comparison/)
    for details) attempts to extend this by doing a second pass and using some re-grouping
    logic. So for example if the first pass has put a break between chunks 1 and 2,
    but chunks 1 and 3 are very similar, it will make a new group that includes chunks
    1, 2 and 3\. This proves useful if chunk 2 was, for example, a mathematical formula
    or a code block.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的一个扩展方法叫做 [语义双重块合并](https://docs.llamaindex.ai/en/stable/examples/node_parsers/semantic_double_merging_chunking/)（有关详细信息，请参见
    [这篇文章](https://bitpeak.pl/chunking-methods-in-rag-methods-comparison/)），尝试通过第二次处理并使用某些重组逻辑来扩展这一方法。例如，如果第一次处理在块
    1 和块 2 之间设置了断点，但块 1 和块 3 非常相似，它将创建一个新组，包含块 1、块 2 和块 3。如果块 2 例如是一个数学公式或代码块，这种方法会非常有用。
- en: 'However, when it comes to any type of semantic chunking some key questions
    remain: How large can the distance between chunk embeddings get before we make
    a breakpoint, and what do these chunks actually represent? Do we care about that?
    Answers to these questions depend on the application and the text in question.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在任何类型的语义切分中，仍然存在一些关键问题：在我们设置断点之前，文本块嵌入之间的距离能有多大？这些文本块实际上代表了什么？我们关心这些问题吗？这些问题的答案取决于具体应用和文本内容。
- en: '**2\. Exploring the breakpoints**'
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**2. 探索断点**'
- en: Let’s use an example to illustrate the generation of breakpoints using semantic
    chunking. We will implement our own version of this algorithm, though out of the
    box implementations are also available as described above. Our demo text is [here](https://github.com/rmartinshort/text_chunking/blob/main/text_chunking/datasets/test_text_dataset.py)
    and it consists of three short, factual essays written by GPT-4o and appended
    together. The first is about the general importance of preserving trees, the second
    is about the history of Namibia and the third is a deeper exploration of the importance
    of protecting trees for medical purposes. The topic choice doesn’t really matter,
    but the corpus represents an interesting test because the first and third essays
    are somewhat similar, yet separated by the second which is very different. Each
    essay is also broken into sections focussing on different things.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来说明如何使用语义分块生成断点。我们将实现我们自己的版本的这个算法，尽管如上所述，也有现成的实现可用。我们的演示文本[在这里](https://github.com/rmartinshort/text_chunking/blob/main/text_chunking/datasets/test_text_dataset.py)，它由GPT-4o编写的三篇简短的事实性文章组成，并拼接在一起。第一篇是关于保护树木一般重要性的，第二篇是关于纳米比亚的历史，第三篇则更深入探讨了为医疗目的保护树木的重要性。主题选择其实并不重要，但这个语料库代表了一个有趣的测试，因为第一篇和第三篇文章有些相似，但它们被第二篇截然不同的文章所分隔。每篇文章也被分成了聚焦于不同主题的几个部分。
- en: We can use a basic `RecursiveCharacterTextSplitter` to make the initial chunks.
    The most important parameters here are the chunk size and separators list, and
    we typically don’t know what they should be without some subject knowledge of
    the text. Here I chose a relatively small chunk size because I want the initial
    chunks to be at most a few sentences long. I also chose the separators such that
    we avoid splitting sentences.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用一个基本的`RecursiveCharacterTextSplitter`来进行初始拆分。这里最重要的参数是块大小和分隔符列表，我们通常在没有一定的文本学科知识的情况下无法知道它们应该是什么。这里我选择了一个相对较小的块大小，因为我希望初始文本块最多只有几句话长。我还选择了合适的分隔符，以避免拆分句子。
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next we can split the text. The `min_chunk_len` parameter comes into play if
    any of the chunks generated by the splitter are smaller than this value. If that
    happens, that chunk just gets appended to the end of the previous one.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们可以对文本进行拆分。如果任何由分块器生成的文本块小于`min_chunk_len`参数指定的值，参数将生效。在这种情况下，该文本块将被附加到前一个文本块的末尾。
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now we can embed the splits using the embeddings model. You’ll see in the class
    for `[SemanticClusterVisualizer](https://github.com/rmartinshort/text_chunking/blob/main/text_chunking/SemanticClusterVisualizer.py)`
    that by default we’re using `text-embeddings-3-small` . This will create a list
    of 53 vectors, each of length 1536\. Intuitively, this means that the semantic
    meaning of each chunk is represented in a 1536 dimensional space. Not great for
    visualization, which is why we’ll turn to dimensionality reduction later.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用嵌入模型来嵌入这些拆分。在`[SemanticClusterVisualizer](https://github.com/rmartinshort/text_chunking/blob/main/text_chunking/SemanticClusterVisualizer.py)`类中，你会看到默认情况下我们使用的是`text-embeddings-3-small`。这将创建一个包含53个向量的列表，每个向量的长度为1536。直观地说，这意味着每个文本块的语义意义在1536维空间中得到表示。对于可视化来说这并不理想，这也是为什么我们稍后会采用降维技术的原因。
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Running the semantic chunker generates a graph like this. We can think of it
    like a time series, where the x-axis represents distance through the entire text
    in terms of characters. The y axis represents the cosine distance between the
    embeddings of subsequent chunks. The break points occur at distances values above
    the 95th percentile.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 运行语义分块器会生成类似这样的图表。我们可以将其视为一个时间序列，其中x轴表示整个文本的字符距离，y轴表示后续文本块之间嵌入向量的余弦距离。断点出现在高于第95百分位的距离值处。
- en: '![](../Images/e940488f2fe8a7b5a4510091e7ffd230.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e940488f2fe8a7b5a4510091e7ffd230.png)'
- en: Graph showing the cosine distance between subsequent chunks of text generated
    by the RecursiveCharacterTextSplitter. We can use these distances to establish
    breakpoints for semantic chunking. Image generated by the author.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图表显示了由`RecursiveCharacterTextSplitter`生成的后续文本块之间的余弦距离。我们可以使用这些距离来建立语义分块的断点。图像由作者生成。
- en: The pattern makes sense given what we know about the text — there are three
    big subjects, each of which has a few different sections. Aside from the two large
    spikes though, it’s not clear where the other breakpoints should be.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们对文本的了解，这个模式是有道理的——有三个大主题，每个主题都有几个不同的部分。除了两个大的峰值之外，其他的断点在哪里并不明确。
- en: This is where the subjectivity and iteration comes in — depending on our application,
    we may want larger or smaller chunks and it’s important to use the graph to help
    guide our eye towards which chunks to actually read.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是主观性和迭代性的体现——根据我们的应用，我们可能需要更大或更小的语块，使用图表帮助我们引导眼睛，选择哪些语块需要被阅读。
- en: There are a few ways we could break the text into more granular chunks. The
    first is just to decrease the percentile threshold to make a breakpoint.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以将文本拆分成更细粒度的语块。第一种方法是降低百分位数阈值以生成断点。
- en: '![](../Images/9cd03a729e49f3b62d411890e66325a5.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9cd03a729e49f3b62d411890e66325a5.png)'
- en: Breakpoints generated by choosing a lower percentile threshold on the cosine
    distances array. Image generated by the author.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通过选择较低的百分位数阈值生成的断点。图像由作者生成。
- en: This creates 4 really small chunks and 8 larger ones. If we look at the first
    4 chunks, for example, the splits seem semantically reasonable although I would
    argue that the 4th chunk is a bit too long, given that it contains most of the
    “economic importance”, “social importance” and “conclusions” sections of the first
    essay.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这会创建4个非常小的语块和8个较大的语块。例如，如果我们看看前四个语块，拆分看起来语义上是合理的，尽管我认为第四个语块有点太长，因为它包含了第一篇文章中大部分的“经济重要性”、“社会重要性”和“结论”部分。
- en: '![](../Images/4c8f630c99b29e79fa59ddc8b45793df.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c8f630c99b29e79fa59ddc8b45793df.png)'
- en: The first four semantic chunks generated by setting percentile threshold at
    0.8\. Image generated by the author.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 设置百分位数阈值为0.8生成的前四个语义语块。图像由作者生成。
- en: Instead of just changing the percentile threshold, an alternative idea is to
    apply the same threshold recursively. We start by creating breakpoints on the
    whole text. Then for each newly created chunk, if the chunk is above some length
    threshold, we create breakpoints just within that chunk. This happens until all
    the chunks are below the length threshold. Although somewhat subjective, I think
    this more closely mirrors what a human would do in that they would first identify
    very different groups of text and then iteratively reduce the size of each one.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 除了仅仅改变百分位数阈值外，另一种思路是递归地应用相同的阈值。我们从对整个文本创建断点开始。然后，对于每个新创建的语块，如果该语块超过某个长度阈值，我们就在该语块内部创建断点。这一过程会一直进行，直到所有语块都低于长度阈值。尽管这种方法在某种程度上是主观的，但我认为它更贴近人类的做法，因为人类会首先识别出非常不同的文本组，然后逐步缩小每个组的大小。
- en: It can be implemented with a stack, as shown below.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以通过栈来实现，如下所示。
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Our choice of `length_threshold` is also subjective and can be informed by the
    plot. In this case, a threshold of 1000 appears to work well. It divides the essays
    quite nicely into short and meaningfully different chunks.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择的`length_threshold`也是主观的，可以通过图表来指导选择。在这种情况下，1000的阈值似乎效果不错，它将文章很好地分成了简短且语义上有所不同的语块。
- en: '![](../Images/8763d5ba09fe12415629a54ce769f603.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8763d5ba09fe12415629a54ce769f603.png)'
- en: Breakpoints generated by running a recursive semantic splitter on the cosine
    distance timeseries from the original chunk embeddings. Image generated by the
    author.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对原始语块嵌入的余弦距离时间序列运行递归语义拆分器生成的断点。图像由作者生成。
- en: Looking at the chunks corresponding to the first essay, we see that they are
    closely aligned with the different sections that GPT4-o created when it wrote
    the essay. Obviously in the case of this particular essay we could have just split
    on `"\n\n"` and been done here, but we want a more general approach.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 看着与第一篇文章对应的语块，我们看到它们与GPT4-o在撰写文章时创建的不同部分紧密对齐。显然，在这种特定的文章情况下，我们本可以仅仅通过`"\n\n"`进行拆分就完成了，但我们希望采用一种更通用的方法。
- en: '![](../Images/83ac1c17b0fd7cdb3753d47489b91938.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/83ac1c17b0fd7cdb3753d47489b91938.png)'
- en: The first six semantic chunks generated by the recursive breakpoint generation
    approach described above. Image generated by the author.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通过上述递归断点生成方法生成的前六个语义语块。图像由作者生成。
- en: '**2\. Clustering the semantic splits**'
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**2\. 聚类语义拆分**'
- en: Now that we have made some candidate semantic chunks, it might be useful to
    see how similar they are to one another. This will help us get a sense for what
    information they contain. We will proceed by embedding the semantic chunks, and
    then use UMAP to reduce the dimensionality of the resulting embeddings to 2D so
    that we can plot them.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经生成了一些候选的语义语块，查看它们之间的相似性可能会很有用。这将帮助我们了解它们包含了什么信息。我们将继续嵌入这些语义语块，然后使用UMAP将生成的嵌入的维度降至2D，以便我们能够绘制它们。
- en: UMAP stands for Uniform Manifold Approximation and Projection, and is a powerful,
    general dimensionality reduction technique that can capture non-linear relationships.
    A full explanation of how it works can be found [here](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html).
    The purpose of using it here is to capture something of the relationships that
    exist between the embedded chunks in 1536-D space in a 2-D plot
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: UMAP代表均匀流形近似与投影，是一种强大的通用降维技术，能够捕捉非线性关系。关于它如何工作的详细解释可以在[这里](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html)找到。这里使用它的目的是在1536维空间中捕捉嵌入块之间的某些关系，并将其呈现在二维图中。
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: UMAP is quite sensitive to the `n_neighbors` parameter. Generally the smaller
    the value of `n_neighbors`, the more the algorithm focuses on the use of local
    structure to learn how to project the data into lower dimensions. Setting this
    value too small can lead to projections that don’t do a great job of capturing
    the large scale structure of the data, and it should generally increase as the
    number of datapoints grows.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: UMAP对`n_neighbors`参数非常敏感。通常，`n_neighbors`值越小，算法越注重利用局部结构来学习如何将数据投影到低维空间。将此值设置得太小可能会导致投影无法很好地捕捉数据的宏观结构，通常随着数据点数量的增加，这个值应该增大。
- en: 'A projection of our data is shown below and its quite informative: Clearly
    we have three clusters of similar meaning, with the 1st and 3rd being more similar
    to each other than either is to the 2nd. The `idx` color bar in the plot above
    shows the chunk number, while the red line gives us an indication of the sequence
    of the chunks.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们数据的投影，它非常有信息量：显然，我们有三个意义相似的聚类，第一个和第三个聚类比任何一个与第二个聚类更相似。上图中的`idx`色条显示了块的编号，而红线则给出了块的顺序。
- en: '![](../Images/6690eb5aad72ac6f82942ee770779a9d.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6690eb5aad72ac6f82942ee770779a9d.png)'
- en: Plot of UMAP projection of the embeddings of the semantic splits generated in
    the previous section. idx refers to the index of the chunk in the order that they
    are generated by looping through the text. Image generated by the author.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 上一节生成的语义拆分的嵌入在UMAP投影中的图示。idx表示按生成顺序排列的块的索引，这些块是通过遍历文本生成的。图像由作者生成。
- en: What about automatic clustering? This would be helpful if we wanted to group
    the chunks into larger segments or topics, which could serve as useful metadata
    to filter on in a RAG application with hybrid search, for example. We also might
    be able to group chunks that are far apart in the text (and therefore would not
    have been grouped by the standard semantic chunking in section 1) but have similar
    meanings.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 自动聚类怎么样？如果我们想将文本块分组为更大的段落或主题，这将非常有帮助。例如，这可以作为一种有用的元数据，在具有混合搜索的RAG应用程序中进行过滤。我们还可能能够将文本中相距较远的块（因此在第一节中的标准语义分块方法下不会被分组）但具有相似意义的块进行分组。
- en: There are many clustering approaches that could be used here. HDBSCAN is a possibility,
    and is the default method recommended by the [BERTopic](https://maartengr.github.io/BERTopic/getting_started/clustering/clustering.html)
    package. However, in this case hierarchical clustering seems more useful since
    it can give us a sense of the relative importance of whatever groups emerge. To
    run hierarchical clustering, we first use UMAP to reduce the dimensionality of
    the dataset to a smaller number of components. So long as UMAP is working well
    here, the exact number of components shouldn’t significantly affect the clusters
    that get generated. Then we use the [hierarchy module from scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html)
    to perform the clustering and plot the result using seaborn
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多聚类方法可以在这里使用。HDBSCAN是一个可能的选择，它是[BERTopic](https://maartengr.github.io/BERTopic/getting_started/clustering/clustering.html)软件包推荐的默认方法。然而，在这种情况下，层次聚类似乎更有用，因为它可以给我们提供任何出现的群组相对重要性的感知。要运行层次聚类，我们首先使用UMAP将数据集的维度降低到更少的组成部分。只要UMAP在这里工作得很好，组成部分的确切数量应该不会显著影响生成的聚类。然后，我们使用[scipy的层次模块](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html)来执行聚类，并使用seaborn绘制结果。
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The result is also quite informative. Here `n_components_reduced` was 4, so
    we reduced the dimensionality of the embeddings to 4D, therefore making a matrix
    with 4 features where each row represents one of the semantic chunks. Hierarchical
    clustering has identified the two major groups (i.e. trees and Namibia), two large
    subgroup within trees (i.e. medical uses vs. other) and an number of other groups
    that might be worth exploring.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 结果也相当有信息量。在这里，`n_components_reduced`为4，因此我们将嵌入的维度降到了4D，从而生成一个具有4个特征的矩阵，每一行代表一个语义块。层次聚类已经识别出两个主要组（即树和纳米比亚）、树类中的两个大子组（即医学用途与其他）以及一些可能值得探索的其他组。
- en: '![](../Images/fee5033190b5b9077467ad2d0ed75e7d.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fee5033190b5b9077467ad2d0ed75e7d.png)'
- en: Hierarchical clustering of the UMAP projections of the embeddings of the semantic
    clunks generated. Image generated by the author.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的语义块的UMAP投影的层次聚类。图像由作者生成。
- en: Note that [BERTopic uses a similar technique for topic visualization](https://maartengr.github.io/BERTopic/getting_started/hierarchicaltopics/hierarchicaltopics.html),
    which could be seen as an extension of what’s being presented here.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，[BERTopic使用类似的技术进行主题可视化](https://maartengr.github.io/BERTopic/getting_started/hierarchicaltopics/hierarchicaltopics.html)，可以看作是对这里所展示内容的扩展。
- en: How is this useful in our exploration of semantic chunking? Depending on the
    results, we may choose to group some of the chunks together. This is again quite
    subjective and it might be important to try out a few different types of grouping.
    Let’s say we looked at the dendrogram and decided we wanted 8 distinct groups.
    We could then cut the hierarchy accordingly, return the cluster labels associated
    with each group and plot them.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这在我们探索语义块划分中的作用是什么？根据结果，我们可以选择将一些块组合在一起。这再次是一个相当主观的过程，尝试几种不同的分组方式可能很重要。假设我们查看了树状图并决定希望有8个独立的组。我们可以相应地切割层次结构，返回与每个组关联的聚类标签并将其绘制出来。
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The resulting plot is shown below. We have 8 clusters, and their distribution
    in the 2D space looks reasonable. This again demonstrates the importance of visualization:
    Depending on the text, application and stakeholders, the right number and distribution
    of groups will likely be different and the only way to check what the algorithm
    is doing is by plotting graphs like this.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图如下所示。我们有8个聚类，它们在二维空间中的分布看起来合理。这再次证明了可视化的重要性：根据文本、应用和利益相关者的不同，适当的组数和分布可能会有所不同，而检查算法所做的唯一方式就是通过绘制这样的图形。
- en: '![](../Images/dc1a6edcaacd42723c107f27e11b02d1.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc1a6edcaacd42723c107f27e11b02d1.png)'
- en: Cluster ids of the semantic chunks used to color the UMAP-projected embeddings
    of the semantic chunks. Image generated by the author.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 用于给语义块的UMAP投影嵌入着色的语义块聚类ID。图像由作者生成。
- en: '**3\. Labeling the clusters**'
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**3\. 聚类标签**'
- en: Assume after a few iterations of the steps above, we’ve settled on semantic
    splits and clusters that we’re happy with. It then makes sense to ask what these
    clusters actually represent? Obviously we could read the text and find out, but
    for a large corpus this is impractical. Instead, let’s use an LLM to help. Specifically,
    we will feed the text associated with each cluster to GPT-4o-mini and ask it to
    generate a summary. This is a relatively simple task with LangChain, and the core
    aspects of the code are shown below
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在经过上述几个步骤的迭代后，我们已经确定了满意的语义分裂和聚类。那么接下来我们需要问，这些聚类实际上代表了什么？显然，我们可以阅读文本并找出答案，但对于大量语料来说这是不切实际的。相反，我们可以使用大型语言模型（LLM）来帮助。具体来说，我们将把每个聚类相关的文本提供给GPT-4o-mini，并要求其生成总结。使用LangChain来完成这个任务相对简单，下面展示了代码的核心部分。
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Running this on our 8 clusters and plotting the result with [datamapplot](https://datamapplot.readthedocs.io/en/latest/)
    gives the following
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的8个聚类上运行该程序，并使用[datamapplot](https://datamapplot.readthedocs.io/en/latest/)绘制结果，得到如下图。
- en: '![](../Images/d03ea790b2958ab89a786710fcccbec6.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d03ea790b2958ab89a786710fcccbec6.png)'
- en: Labels generated by running GPT-4o-mini for the semantic clusters. Image generated
    by the author.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行GPT-4o-mini生成的语义聚类标签。图像由作者生成。
- en: An alternative way of visualizing these groups is similar to the graphs shown
    in section 2, where we plot cumulative character number on the x axis and show
    the boundaries between the groups. Recall that we had 18 semantic chunks and have
    now grouped them further into 8 clusters. Plotting them like this shows how the
    semantic content of the text changes from beginning to end, highlights the fact
    that similar content is not always adjacent and gives a visual indication of the
    relative size of the chunks.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化这些分组的另一种方式类似于第2节中展示的图形，我们将字符数的累积值绘制在横轴上，并展示各组之间的边界。回顾一下，我们曾经有18个语义块，现在已经将它们进一步分为8个聚类。像这样绘制图表显示了文本的语义内容从头到尾的变化，突出了相似内容并不总是相邻的事实，并且为块的相对大小提供了一个直观的指示。
- en: '![](../Images/af3cec1f961f5b5a507553dc5cf843e1.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af3cec1f961f5b5a507553dc5cf843e1.png)'
- en: Graph showing the text segmented by semantic cluster and the names of the clusters.
    Image generated by the author.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 显示按语义聚类进行文本分段以及各聚类名称的图表。图片由作者生成。
- en: The code used to produce these figures can be found [here](https://github.com/rmartinshort/text_chunking/blob/main/text_chunking/utils/SemanticGroupUtils.py).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成这些图形的代码可以在[这里](https://github.com/rmartinshort/text_chunking/blob/main/text_chunking/utils/SemanticGroupUtils.py)找到。
- en: '**4\. Testing on a larger corpus**'
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**4. 在更大的语料库上进行测试**'
- en: So far we’ve tested this workflow on a relatively small amount of text for demo
    purposes. Ideally it would also be useful on a larger corpus without significant
    modification. To test this, let’s try it out on a book downloaded from [Project
    Gutenberg](https://www.gutenberg.org/), and I’ve chosen the Wizard of Oz here.
    This is a much more difficult task because novels are typically not arranged in
    clear semantically distinct sections like factual essays. Although they are commonly
    arranged in chapters, the story line may “arch” in a continuous fashion, or skip
    around between different subjects. It would be very interesting to see if semantic
    chunk analysis could be used to learn something about the style of different authors
    from their work.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经在一个相对较小的文本量上测试了这个工作流，目的是演示其功能。理想情况下，它也应该能在更大的语料库上使用，而无需进行重大修改。为了测试这一点，我们来尝试将其应用于从[古腾堡计划](https://www.gutenberg.org/)下载的一本书，我选择了《绿野仙踪》。这是一项更具挑战性的任务，因为小说通常不像事实性文章那样按明确的语义区块进行划分。尽管小说通常按章节进行安排，但故事情节可能会呈现连续性的发展，或者在不同的主题之间跳跃。如果语义块分析能够用来揭示不同作者作品中的风格特点，那将非常有趣。
- en: '**Step 1: Embed and generate breakpoints**'
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**步骤 1：嵌入并生成断点**'
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This generates 77 semantic chunks of varying size. Doing some spot checks here
    led me to feel confident that it was working relatively well and many of the chunks
    end up being divided on or close to chapter boundaries, which makes a lot of sense.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步生成了77个大小不一的语义块。这里进行了一些随机检查，我对其运行效果有了较高的信心，并且许多语义块最终在章节边界附近进行划分，这也非常合理。
- en: '![](../Images/a81a54829db8050a2620be382d653dc6.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a81a54829db8050a2620be382d653dc6.png)'
- en: Semantic splits from the Wizard of Oz. Image generated by the author.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 《绿野仙踪》的语义拆分。图片由作者生成。
- en: 'Step 2 : Cluster and generate labels'
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 2：聚类并生成标签
- en: On looking at the hierarchical clustering dendrogram, I decided to experiment
    with reduction to 35 clusters. The result reveals an outlier in the top left of
    the plot below (cluster id 34), which turns out to be a group of chunks at the
    very end of the text that contain a lengthy description of the terms under which
    the book is distributed.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看层次聚类树状图后，我决定尝试将其减少为35个聚类。结果揭示了图表左上方的一个异常值（聚类ID为34），这个聚类实际上是文本末尾的一组语义块，包含了对本书分发条款的详细描述。
- en: '![](../Images/c2b807ca8401d2438bc1a26d2cd9c16e.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2b807ca8401d2438bc1a26d2cd9c16e.png)'
- en: 2D plot of UMAP projections of semantic chunks from the Wizard of Oz, and their
    cluster labels. Image generated by the author
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 《绿野仙踪》语义块的二维UMAP投影图及其聚类标签。图片由作者生成
- en: The descriptions given to each of the clusters are shown below and, with the
    exception of the first one, they provide a nice overview of the main events of
    the novel. A quick check on the actual texts associated with each one confirms
    that they are reasonably accurate summaries, although again, a determination of
    where the boundaries of the clusters should be is very subjective.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 每个聚类的描述如下所示，除了第一个聚类外，它们为小说的主要事件提供了一个不错的概览。快速检查与每个聚类相关的实际文本，确认它们是相当准确的总结，尽管再次强调，确定聚类边界的位置是非常主观的。
- en: '![](../Images/76695daf7c6e6e49b67108db231e91fc.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/76695daf7c6e6e49b67108db231e91fc.png)'
- en: Names automatically chosen for each of the 40 semantic clusters from the Wizard
    of Oz. This list provides a quick overview of the storyline. Image generated by
    the author.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 从《绿野仙踪》中自动为40个语义集群选择的名称。此列表提供了故事情节的快速概览。图片由作者生成。
- en: GPT-4o-mini labeled the outlier cluster “Project Gutenberg allows free distribution
    of unprotected works”. The text associated with this label is not particularly
    interesting to us, so let’s remove it and re-plot the result. This will make the
    structure in the novel easier to see.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4o-mini将离群集群标记为“古腾堡计划允许自由分发未受保护的作品”。与此标签相关的文本对我们来说并不特别有趣，所以我们将其删除，并重新绘制结果。这样可以更清晰地看到小说的结构。
- en: What if we are interested in larger clusters? If we were to focus on high level
    structure, the dendrogram suggests approximately six clusters of semantic chunks,
    which are plotted below.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对更大的集群感兴趣呢？如果我们集中关注高级结构，树状图建议大约有六个语义片段集群，如下图所示。
- en: '![](../Images/dbc6c75ea91fc60aec528016e258f437.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dbc6c75ea91fc60aec528016e258f437.png)'
- en: Searching for high level structure — if we choose to make 6 clusters from the
    semantic chunks of the Wizard of Oz, this is the pattern we get. Image generated
    by the author.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索高级结构 —— 如果我们选择从《绿野仙踪》的语义片段中创建6个集群，那么我们得到的就是这个模式。图片由作者生成。
- en: 'There’s a lot of jumping back and forth between points that are somewhat distant
    in this semantic space, suggesting frequent sudden changes in subject. It’s also
    interesting to consider the connectivity between the various clusters: 4 and 5
    have no links between them for example, while there’s a lot of back and forth
    between 0 and 1.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个语义空间中，有很多在相距较远的点之间跳跃，这表明主题发生了频繁的突变。考虑到各个集群之间的连接性也很有趣：例如，集群4和集群5之间没有连接，而集群0和集群1之间则有很多来回跳跃。
- en: Can we summarize these larger clusters? It turns out that our prompt doesn’t
    seem well suited for chunks of this size, producing descriptions that seem either
    too specific to one part of the cluster (i.e. clusters 0 and 4) or too vague to
    be very helpful. Improved prompt engineering — possibly involving multiple summarization
    steps — would probably improve the results here.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否总结这些较大的集群？事实证明，我们的提示似乎不太适合这种大小的片段，产生的描述要么过于具体（例如集群0和4），要么过于模糊，难以提供有效帮助。改进提示工程——可能涉及多次总结步骤——可能会提高这里的结果。
- en: '![](../Images/c38aff26c65af18a4f901652af0cde32.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c38aff26c65af18a4f901652af0cde32.png)'
- en: Graph showing the text segmented by semantic cluster id and the names of the
    six clusters identified above. Image generated by the author.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 显示文本按语义集群ID分段，并标注上述识别出的六个集群名称的图表。图片由作者生成。
- en: Despite the unhelpful names, this plot of the text segments colored by cluster
    is still informative as a guide to selective reading of the text. We see that
    the book starts and ends on the same cluster, which likely is about descriptions
    of Dorothy, Toto and their home — and aligns with the story arch of the Wizard
    of Oz as a journey and subsequent return. Cluster 1 is mainly about meeting new
    characters, which happens mainly near the beginning but also periodically throughout
    the book. Clusters 2 and 3 are concerned with Emerald City and the Wizard, while
    clusters 4 and 5 are broadly about journeying and fighting respectively.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管名称不太有用，这张按集群着色的文本片段图仍然是选择性阅读文本的有用指南。我们看到，书籍的开始和结尾都位于相同的集群，这可能与多萝西、托托及其家乡的描述有关——并且与《绿野仙踪》的故事弧线相符，即旅程和随后的归来。集群1主要涉及与新角色的相遇，这主要发生在开头，但也偶尔出现在全书中。集群2和集群3则关注翡翠城和巫师，而集群4和集群5则大致分别与旅行和战斗相关。
- en: '**5\. Concluding thoughts**'
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**5\. 结论性思考**'
- en: Thanks for making it to the end! Here we took a deep dive into the idea of semantic
    chunking, and how it can be complimented by dimensionality reduction, clustering
    and visualization. The major takeaway is the importance of systematically exploring
    the effects of different chunking techniques and a parameters on your text before
    deciding on the most suitable approach. My hope is that this article will spark
    new ideas about how we can use AI and visualization tools to advance semantic
    chunking and quickly extract insights from large bodies of text. Please feel free
    to explore the full codebase here [https://github.com/rmartinshort/text_chunking](https://github.com/rmartinshort/text_chunking).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你读到最后！在这里，我们深入探讨了语义分块的概念，以及如何通过降维、聚类和可视化来补充这一过程。主要的收获是，系统地探索不同分块技术和参数对文本的影响非常重要，在此基础上再决定最合适的方法。我的希望是，这篇文章能够激发新的思路，帮助我们如何利用人工智能和可视化工具推动语义分块，并快速从大量文本中提取见解。请随时浏览完整的代码库
    [https://github.com/rmartinshort/text_chunking](https://github.com/rmartinshort/text_chunking)。
