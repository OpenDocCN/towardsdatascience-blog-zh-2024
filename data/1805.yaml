- en: The Intersection of Memory and Grounding in AI Systems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI 系统中记忆与基础构建的交集
- en: 原文：[https://towardsdatascience.com/the-intersection-of-memory-and-grounding-in-ai-systems-0fda53231011?source=collection_archive---------8-----------------------#2024-07-24](https://towardsdatascience.com/the-intersection-of-memory-and-grounding-in-ai-systems-0fda53231011?source=collection_archive---------8-----------------------#2024-07-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-intersection-of-memory-and-grounding-in-ai-systems-0fda53231011?source=collection_archive---------8-----------------------#2024-07-24](https://towardsdatascience.com/the-intersection-of-memory-and-grounding-in-ai-systems-0fda53231011?source=collection_archive---------8-----------------------#2024-07-24)
- en: '[](https://medium.com/@sandibesen?source=post_page---byline--0fda53231011--------------------------------)[![Sandi
    Besen](../Images/97361d97f50269f70b6621da2256bc29.png)](https://medium.com/@sandibesen?source=post_page---byline--0fda53231011--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0fda53231011--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0fda53231011--------------------------------)
    [Sandi Besen](https://medium.com/@sandibesen?source=post_page---byline--0fda53231011--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@sandibesen?source=post_page---byline--0fda53231011--------------------------------)[![Sandi
    Besen](../Images/97361d97f50269f70b6621da2256bc29.png)](https://medium.com/@sandibesen?source=post_page---byline--0fda53231011--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0fda53231011--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0fda53231011--------------------------------)
    [Sandi Besen](https://medium.com/@sandibesen?source=post_page---byline--0fda53231011--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0fda53231011--------------------------------)
    ·6 min read·Jul 24, 2024
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0fda53231011--------------------------------)
    ·阅读时间：6 分钟·2024年7月24日
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Understanding the 4 key types of memory (short term, short long term, long term,
    and working), the methods of language model grounding, and the role memory plays
    in the process of grounding.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 了解记忆的四个关键类型（短期记忆、短期长期记忆、长期记忆和工作记忆）、语言模型的基础构建方法以及记忆在基础构建过程中的作用。
- en: In the context of Language Models and Agentic AI, memory and grounding are both
    hot and emerging fields of research. And although they are often placed closely
    in a sentence and are often related, they serve different functions in practice.
    In this article, I hope to clear up the confusion around these two terms and demonstrate
    how memory can play a role in the overall grounding of a model.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在语言模型和智能体 AI 的背景下，记忆和基础知识是两个热门且新兴的研究领域。尽管它们通常在句子中紧密相连且常常相关，但在实际应用中，它们的功能是不同的。在本文中，我希望能澄清这两个术语之间的混淆，并展示记忆在模型的整体基础构建中所起的作用。
- en: '![](../Images/0e36dde5c62d9f8a01a89f42a1c0ca99.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e36dde5c62d9f8a01a89f42a1c0ca99.png)'
- en: 'Source: Dalle3, Description: split parts of the brain potray memory and grounding
    in the style of a friendly cartoon'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：Dalle3，描述：分割的大脑部分呈现记忆和基础构建，以友好的卡通风格展现
- en: '**Memory in Language Models**'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**语言模型中的记忆**'
- en: 'In my last article, we discussed the [important role of memory in Agentic AI](https://medium.com/towards-data-science/the-important-role-of-memory-in-agentic-ai-896b22542b3e).
    Memory in language models refers to the ability of AI systems to retain and recall
    pertinent information, contributing to its ability to reason and continuously
    learn from its experiences. **Memory can be thought of in 4 categories: short
    term memory, short long term memory, long term memory, and working memory.**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在我上一篇文章中，我们讨论了[记忆在智能体 AI 中的重要作用](https://medium.com/towards-data-science/the-important-role-of-memory-in-agentic-ai-896b22542b3e)。语言模型中的记忆指的是
    AI 系统保持和回忆相关信息的能力，这有助于其推理能力，并能不断从经验中学习。**记忆可以分为 4 类：短期记忆、短期长期记忆、长期记忆和工作记忆。**
- en: 'It sounds complex, but let’s break them down simply:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来很复杂，但让我们简单地拆解一下：
- en: '**Short Term Memory (STM):**'
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**短期记忆（STM）：**'
- en: STM retains information for a very brief period of time, which could be seconds
    to minutes. If you ask a language model a question it needs to retain your messages
    for long enough to generate an answer to your question. Just like people, language
    models struggle to remember too many things simultaneously.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: STM（短期记忆）保留信息的时间非常短，可能是几秒到几分钟。如果你问一个语言模型一个问题，它需要保持你的消息足够长的时间，以便生成对你的问题的回答。就像人类一样，语言模型也难以同时记住太多的事物。
- en: '[Miller’s law](https://www.simplypsychology.org/short-term-memory.html), states
    that “Short-term memory is a component of memory that holds a small amount of
    information in an active, readily available state for a brief period, typically
    a few seconds to a minute. The duration of STM seems to be between 15 and 30 seconds,
    and STM’s capacity is limited, often thought to be about 7±2 items.”'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[米勒定律](https://www.simplypsychology.org/short-term-memory.html)指出，“短期记忆是记忆的一个组成部分，它在一个活跃、随时可用的状态下保持少量信息，通常为几秒钟到一分钟。短期记忆的持续时间似乎在15到30秒之间，短期记忆的容量是有限的，通常认为大约为7±2项。”'
- en: So if you ask a language model “what genre is that book that I mentioned in
    my previous message?” it needs to use its short term memory to reference recent
    messages and generate a relevant response.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果你问一个语言模型“我在之前的消息中提到的那本书是哪种类型？”它需要使用它的短期记忆来参考最近的消息并生成相关的回应。
- en: '**Implementation:**'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**实现：**'
- en: Context is stored in external systems, such as session variables or databases,
    which hold a portion of the conversation history. Each new user input and assistant
    response is appended to the existing context to create conversation history. During
    inference, context is sent along with the user’s new query to the language model
    to generate a response that considers the entire conversation. This [research
    paper](https://openreview.net/pdf?id=QNW1OrjynpT) offers a more in depth view
    of the mechanisms that enable short term memory.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文存储在外部系统中，例如会话变量或数据库，这些系统保存对话历史的一部分。每次新的用户输入和助手回应都会附加到现有上下文中，形成对话历史。在推理过程中，上下文会与用户的新查询一起发送给语言模型，以生成一个考虑到整个对话的回应。此[研究论文](https://openreview.net/pdf?id=QNW1OrjynpT)提供了关于支持短期记忆机制的更深入的观点。
- en: '**Short Long Term Memory (SLTM):**'
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**短期长时记忆（SLTM）：**'
- en: SLTM retains information for a moderate period, which can be minutes to hours.
    For example, within the same session, you can pick back up where you left off
    in a conversation without having to repeat context because it has been stored
    as SLTM. This process is also an external process rather than part of the language
    model itself.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 短期长时记忆（SLTM）保留信息的时间为中等时长，从几分钟到几个小时。例如，在同一会话中，你可以继续未完成的对话，而无需重复上下文，因为它已经作为SLTM被存储。这个过程也是外部过程，而不是语言模型本身的一部分。
- en: '**Implementation:**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**实现：**'
- en: Sessions can be managed using identifiers that link user interactions over time.
    Context data is stored in a way that it can persist across user interactions within
    a defined period, such as a database. When a user resumes conversation, the system
    can retrieve the conversation history from previous sessions and pass that to
    the language model during inference. Much like in short term memory, each new
    user input and assistant response is appended to the existing context to keep
    conversation history current.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 会话可以通过标识符进行管理，这些标识符将用户的互动在时间上连接起来。上下文数据以一种方式存储，使其可以在定义的时间段内跨用户交互持久化，比如存储在数据库中。当用户恢复对话时，系统可以从之前的会话中检索对话历史，并在推理时将其传递给语言模型。就像短期记忆一样，每次新的用户输入和助手回应都会附加到现有的上下文中，以保持对话历史的更新。
- en: '**Long Term Memory (LTM):**'
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**长期记忆（LTM）：**'
- en: LTM retains information for a admin defined amount of time that could be indefinitely.
    For example, if we were to build an AI tutor, it would be important for the language
    model to understand what subjects the student performs well in, where they still
    struggle, what learning styles work best for them, and more. This way, the model
    can recall relevant information to inform its future teaching plans. [Squirrel
    AI](https://squirrelai.com/#/products/technology) is an example of a platform
    that uses long term memory to “craft personalized learning pathways, engages in
    targeted teaching, and provides emotional intervention when needed”.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 长期记忆（LTM）保留信息的时间为管理员定义的时间段，这段时间可以是无限的。例如，如果我们要构建一个AI导师，语言模型需要理解学生在哪些科目上表现好，在哪些方面仍然挣扎，哪种学习方式最适合他们，等等。这样，模型可以召回相关信息，以指导未来的教学计划。[松鼠AI](https://squirrelai.com/#/products/technology)就是一个使用长期记忆来“制定个性化学习路径，进行有针对性的教学，并在需要时提供情感干预”的平台示例。
- en: '**Implementation:**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**实现：**'
- en: Information can be stored in structured databases, knowledge graphs, or document
    stores that are queried as needed. Relevant information is retrieved based on
    the user’s current interaction and past history. This provides context for the
    language model that is passed back in with the user’s response or system prompt.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 信息可以存储在结构化数据库、知识图谱或文档存储中，按需查询。相关信息基于用户当前的互动和历史记录进行检索。这为语言模型提供了上下文，该上下文随着用户的回应或系统提示返回。
- en: '**Working Memory:**'
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**工作记忆：**'
- en: 'Working memory is a component of the language model itself (unlike the other
    types of memory that are external processes). It enables the language model to
    hold information, manipulate it, and refine it — improving the model’s ability
    to reason. This is important because as the model processes the user’s ask, its
    understanding of the task and the steps it needs to take to execute on it can
    change. You can think of working memory as the model’s own scratch pad for its
    thoughts. For example, when provided with a multistep math problem such as (5
    + 3) * 2, the language model needs the ability to calculate the (5+3) in the parentheses
    and store that information before taking the sum of the two numbers and multiplying
    by 2\. If you’re interested in digging deeper into this subject, the [paper](https://arxiv.org/abs/2404.09173)
    “TransformerFAM: Feedback attention is working memory” offers a new approach to
    extending the working memory and enabling a language model to process inputs/context
    window of unlimited length.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '工作记忆是语言模型本身的一个组成部分（不同于其他类型的记忆，它们是外部过程）。它使语言模型能够保持信息、操作信息并进行改进——从而提高模型的推理能力。这一点非常重要，因为当模型处理用户的请求时，它对任务的理解及执行任务所需采取的步骤可能会发生变化。你可以把工作记忆看作是模型思维的草稿本。例如，当提供一个多步骤的数学问题，如(5
    + 3) * 2时，语言模型需要能够计算括号中的(5 + 3)并存储该信息，然后再将两数之和乘以2。如果你有兴趣深入了解这个话题，[论文](https://arxiv.org/abs/2404.09173)《TransformerFAM:
    反馈注意力是工作记忆》提供了一种新的方法来扩展工作记忆，使语言模型能够处理无限长度的输入/上下文窗口。'
- en: '**Implementation:**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**实现：**'
- en: Mechanisms like attention layers in transformers or hidden states in recurrent
    neural networks (RNNs) are responsible for maintaining intermediate computations
    and provide the ability to manipulate intermediate results within the same inference
    session. As the model processes input, it updates its internal state, which enables
    stronger reasoning abilities.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 像变换器中的注意力层或循环神经网络（RNNs）中的隐藏状态这样的机制负责保持中间计算，并提供在同一推理会话中操作中间结果的能力。随着模型处理输入，它更新内部状态，这使得推理能力更强。
- en: '*All 4 types of memory are important components of creating an AI system that
    can effectively manage and utilize information across various timeframes and contexts.*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*所有4种类型的记忆都是创建一个能够有效管理和利用跨越不同时间框架和上下文信息的人工智能系统的重要组成部分。*'
- en: '![](../Images/d7e13a9ce7878b0823aa6f457e6eeb6e.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d7e13a9ce7878b0823aa6f457e6eeb6e.png)'
- en: 'Table of Types of Memory in AI Systems, Source: Sandi Besen'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能系统中的记忆类型表，来源：Sandi Besen
- en: Grounding
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基础
- en: '**The response from a language model should always make sense in the context
    of the conversation — they shouldn’t just be a bunch of factual statements**.
    Grounding measures the ability of a model to produce an output that is contextually
    relevant and meaningful. The process of grounding a language model can be a combination
    of language model training, fine-tuning, and external processes (including memory!).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**语言模型的回应应始终在对话的上下文中合理——它们不应仅仅是一堆事实陈述**。基础性测量模型生成一个在上下文中相关且有意义的输出的能力。对语言模型进行基础化的过程可以是语言模型训练、微调和外部过程（包括记忆！）的组合。'
- en: '**Language Model Training and Fine Tuning**'
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**语言模型训练与微调**'
- en: The data that the model is initially trained on will make a substantial difference
    in how grounded the model is. Training a model on a large corpora of diverse data
    enables it to learn language patterns, grammar, and semantics, to predict the
    next most relevant word. The pre-trained model is then fine-tuned on domain-specific
    data, which helps it generate more relevant and accurate outputs for particular
    applications that require deeper domain specific knowledge. This is especially
    important if you require the model to perform well on specific texts which it
    might not have been exposed to during its initial training. Although our expectations
    of a language model’s capabilities are high, we can’t expect it to perform well
    on something it has never seen before. Just like we wouldn’t expect a student
    to perform well on an exam if they hadn’t studied the material.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 模型最初的训练数据会在很大程度上影响模型的基础性和表现。通过在大量多样化的数据语料库上训练模型，可以让其学习语言模式、语法和语义，从而预测下一个最相关的词语。预训练的模型随后会在特定领域的数据上进行微调，这有助于其为需要更深入领域知识的特定应用生成更相关和准确的输出。如果你要求模型在某些文本上表现良好，而这些文本在初始训练中没有涉及，那么这种微调尤为重要。尽管我们对语言模型的能力期望很高，但我们不能指望它在从未接触过的内容上表现出色。就像我们不会指望一个没有学习材料的学生能在考试中表现优异一样。
- en: '**External Context**'
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**外部上下文**'
- en: Providing the model with real-time or up-to-date context-specific information
    also helps it stay grounded. There are many methods of doing this, such as integrating
    it with external knowledge bases, APIs, and real-time data. This method is also
    known as Retrieval Augmented Generation (RAG).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为模型提供实时或最新的上下文信息有助于其保持基础性。实现这一目标的方法有很多，比如将其与外部知识库、API和实时数据集成。这种方法也被称为检索增强生成（RAG）。
- en: '**Memory Systems**'
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**记忆系统**'
- en: Memory systems in AI play a crucial role in ensuring that the system remains
    grounded based on its previously taken actions, lessons learned, performance over
    time, and experience with users and other systems. The four types of memory outlined
    previously in the article play a crucial role in grounding a language model’s
    ability to stay context-aware and produce relevant outputs. Memory systems work
    in tandem with grounding techniques like training, fine-tuning, and external context
    integration to enhance the model’s overall performance and relevance.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能中的记忆系统在确保系统根据其先前采取的行动、所学的经验、随时间积累的表现以及与用户和其他系统的互动中保持基础性方面发挥着至关重要的作用。文章中之前提到的四种记忆类型在保持语言模型上下文感知和生成相关输出方面起着重要作用。记忆系统与基础性技术（如训练、微调和外部上下文整合）协同工作，以提升模型的整体表现和相关性。
- en: '**Conclusion**'
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**结论**'
- en: Memory and grounding are interconnected elements that enhance the performance
    and reliability of AI systems. While memory enables AI to retain and manipulate
    information across different timeframes, grounding ensures that the AI’s outputs
    are contextually relevant and meaningful. By integrating memory systems and grounding
    techniques, AI systems can achieve a higher level of understanding and effectiveness
    in their interactions and tasks.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆和基础性是相互关联的元素，它们提高了人工智能系统的表现和可靠性。记忆使得人工智能能够跨时间维度保留和处理信息，而基础性确保了人工智能的输出在上下文中是相关和有意义的。通过整合记忆系统和基础性技术，人工智能系统可以在其交互和任务中实现更高水平的理解和有效性。
- en: 'Note: The opinions expressed both in this article and paper are solely those
    of the authors and do not necessarily reflect the views or policies of their respective
    employers.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本文及论文中的观点仅代表作者个人观点，并不一定反映其所在单位的看法或政策。
- en: If you still have questions or think that something needs to be further clarified?
    Drop me a DM on [Linkedin](https://www.linkedin.com/in/sandibesen/)! I‘m always
    eager to engage in food for thought and iterate on my work.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还有疑问或认为有些内容需要进一步澄清，欢迎在[Linkedin](https://www.linkedin.com/in/sandibesen/)上给我发私信！我总是乐于参与思维碰撞并改进我的工作。
- en: 'References:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 参考文献：
- en: '[https://openreview.net/pdf?id=QNW1OrjynpT](https://openreview.net/pdf?id=QNW1OrjynpT)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://openreview.net/pdf?id=QNW1OrjynpT](https://openreview.net/pdf?id=QNW1OrjynpT)'
- en: '[https://www.simplypsychology.org/short-term-memory.html](https://www.simplypsychology.org/short-term-memory.html)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.simplypsychology.org/short-term-memory.html](https://www.simplypsychology.org/short-term-memory.html)'
