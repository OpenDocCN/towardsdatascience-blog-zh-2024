- en: Navigating the Latest GenAI Announcements — July 2024
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导航最新的生成式AI公告——2024年7月
- en: 原文：[https://towardsdatascience.com/navigating-the-latest-genai-model-announcements-july-2024-461f227f588f?source=collection_archive---------7-----------------------#2024-07-26](https://towardsdatascience.com/navigating-the-latest-genai-model-announcements-july-2024-461f227f588f?source=collection_archive---------7-----------------------#2024-07-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/navigating-the-latest-genai-model-announcements-july-2024-461f227f588f?source=collection_archive---------7-----------------------#2024-07-26](https://towardsdatascience.com/navigating-the-latest-genai-model-announcements-july-2024-461f227f588f?source=collection_archive---------7-----------------------#2024-07-26)
- en: '*A guide to new models GPT-4o mini, Llama 3.1, Mistral NeMo 12B and other GenAI
    trends*'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*关于新模型 GPT-4o mini、Llama 3.1、Mistral NeMo 12B 以及其他生成式AI趋势的指南*'
- en: '[](https://medium.com/@tula.masterman?source=post_page---byline--461f227f588f--------------------------------)[![Tula
    Masterman](../Images/c36b3740befd5dfdb8719dc6596f1a99.png)](https://medium.com/@tula.masterman?source=post_page---byline--461f227f588f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--461f227f588f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--461f227f588f--------------------------------)
    [Tula Masterman](https://medium.com/@tula.masterman?source=post_page---byline--461f227f588f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@tula.masterman?source=post_page---byline--461f227f588f--------------------------------)[![Tula
    Masterman](../Images/c36b3740befd5dfdb8719dc6596f1a99.png)](https://medium.com/@tula.masterman?source=post_page---byline--461f227f588f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--461f227f588f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--461f227f588f--------------------------------)
    [Tula Masterman](https://medium.com/@tula.masterman?source=post_page---byline--461f227f588f--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--461f227f588f--------------------------------)
    ·7 min read·Jul 26, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--461f227f588f--------------------------------)
    ·阅读时间 7 分钟·2024年7月26日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/2c1a54aec97e06bbaf4652cc58890eb2.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2c1a54aec97e06bbaf4652cc58890eb2.png)'
- en: Image Created by Author with GPT-4o to represent different models
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用GPT-4o生成，展示不同的模型
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: 'Since the launch of ChatGPT in November 2022, it feels like almost every week
    there’s a new model, novel prompting approach, innovative agent framework, or
    other exciting GenAI breakthrough. July 2024 is no different: this month alone
    we’ve seen the release of [Mistral Codestral Mamba](https://mistral.ai/news/codestral-mamba/),
    [Mistral NeMo 12B](https://mistral.ai/news/mistral-nemo/), [GPT-4o mini](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/),
    and [Llama 3.1](https://ai.meta.com/blog/meta-llama-3-1/) amongst others. These
    models bring significant enhancements to areas like inference speed, reasoning
    ability, coding ability, and tool calling performance making them a compelling
    choice for business use.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 自2022年11月ChatGPT发布以来，几乎每周都有新的模型、创新的提示方法、创新的代理框架或其他令人兴奋的生成式AI突破。2024年7月也不例外：仅在本月，我们就见证了
    [Mistral Codestral Mamba](https://mistral.ai/news/codestral-mamba/)、[Mistral NeMo
    12B](https://mistral.ai/news/mistral-nemo/)、[GPT-4o mini](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)
    和 [Llama 3.1](https://ai.meta.com/blog/meta-llama-3-1/) 等模型的发布。这些模型在推理速度、推理能力、编程能力和工具调用性能等领域带来了显著提升，使它们成为企业使用的有力选择。
- en: In this article we’ll cover the highlights of recently released models and discuss
    some of the major trends in GenAI today, including increasing context window sizes
    and improving performance across languages and modalities.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将介绍最近发布的模型亮点，并讨论当今生成式AI（GenAI）领域的一些主要趋势，包括增加上下文窗口大小、以及跨语言和跨模态的性能提升。
- en: Overview of July Release Models
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2024年7月发布模型概述
- en: '**Mistral Codestral Mamba**'
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**Mistral Codestral Mamba**'
- en: '**Overview**: Codestral Mamba 7B is designed for **enhanced reasoning and coding
    capabilities** using the [Mamba architecture](https://arxiv.org/abs/2312.00752)
    instead of the Transformer architecture used by most Language Models. This architecture
    enables in context retrieval for much longer sequences and has been tested for
    sequences up to 256K tokens. By comparison, most Transformer based models allow
    between 8-128K token context windows. The Mamba architecture also enables faster
    inference speeds than Transformer based models.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概述**：Codestral Mamba 7B采用了[Mamba架构](https://arxiv.org/abs/2312.00752)，旨在提供**增强的推理和编码能力**，而不是大多数语言模型使用的Transformer架构。该架构使得在更长序列中进行上下文检索成为可能，已被测试支持最多256K
    tokens的序列。相比之下，大多数基于Transformer的模型仅支持8-128K tokens的上下文窗口。Mamba架构还能够比基于Transformer的模型提供更快的推理速度。'
- en: '**Availability**: Codestral Mamba is an open source model under the Apache
    2.0 License.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可用性**：Codestral Mamba是一个开放源代码模型，采用Apache 2.0许可证。'
- en: '**Performance**: Codestral Mamba 7B outperforms CodeGemma-1.1 7B, CodeLlama
    7B, and DeepSeekv1.5 7B on the HumanEval, MBPP, CruxE, HumanEval C++, and Human
    Eval JavaScript benchmarks. It performs similarly to Codestral 22B across these
    benchmarks despite it’s smaller size.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能**：Codestral Mamba 7B在HumanEval、MBPP、CruxE、HumanEval C++和Human Eval JavaScript基准测试中超越了CodeGemma-1.1
    7B、CodeLlama 7B和DeepSeekv1.5 7B。尽管其规模较小，但它在这些基准测试中的表现与Codestral 22B相似。'
- en: '![](../Images/9ccaedb9551dbc596082350afb2a639e.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ccaedb9551dbc596082350afb2a639e.png)'
- en: Image created by author based on results from Mistral AI [Codestral Mamba announcement](https://mistral.ai/news/codestral-mamba/)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由作者基于Mistral AI [Codestral Mamba公告](https://mistral.ai/news/codestral-mamba/)的结果创建
- en: '**Mistral NeMo 12B**'
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**Mistral NeMo 12B**'
- en: '**Overview**: Mistral NeMo 12B was produced by Mistral and Nvidia to offer
    a competitive language model in the 12B parameter range with a far larger context
    window than most models of this size. Nemo 12B has a **128K token context window**
    while similarly sized models Gemma 2 9B and Llama 3 8B offer only 8K token context
    windows. NeMo is **designed for multilingual use cases and provides a new tokenizer**,
    Tekken, which outperforms the Llama 3 tokenizer for compressing text across 85%
    of languages. The HuggingFace model card indicates **NeMo should be used with
    lower temperatures** than earlier Mistral models, they recommend setting the temperature
    to 0.3.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概述**：Mistral NeMo 12B是由Mistral和Nvidia联合推出，旨在提供一个具有竞争力的12B参数范围的语言模型，且其上下文窗口比大多数同类大小的模型要大得多。NeMo
    12B具有**128K token上下文窗口**，而同样大小的模型Gemma 2 9B和Llama 3 8B仅提供8K token的上下文窗口。NeMo**旨在支持多语言应用场景，并提供了一种新的分词器**——Tekken，该分词器在压缩85%的语言文本时，比Llama
    3的分词器表现更优。HuggingFace模型卡指出，**NeMo应在比早期Mistral模型更低的温度下使用**，他们建议将温度设置为0.3。'
- en: '**Availability**: NeMo 12B is an open source model (offering both base and
    instruction-tuned checkpoints) under the Apache 2.0 License.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可用性**：NeMo 12B是一个开放源代码模型（提供基础版和指令调优版检查点），采用Apache 2.0许可证。'
- en: '**Performance**: Mistral NeMo 12B outperforms Gemma 2 9B and Llama 3 8B across
    multiple zero and five shot benchmarks by as much as 10%. It also performs almost
    2x better than Mistral 7B on WildBench which is designed to measure model’s performance
    on real world tasks requiring complex reasoning and multiple conversation turns.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能**：Mistral NeMo 12B在多个零样本和五样本基准测试中，比Gemma 2 9B和Llama 3 8B的表现高出多达10%。在WildBench测试中，其表现几乎是Mistral
    7B的2倍，WildBench旨在衡量模型在需要复杂推理和多轮对话的现实任务中的表现。'
- en: '![](../Images/bff3c9eb1e34c83135461162a628694c.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bff3c9eb1e34c83135461162a628694c.png)'
- en: Image created by author based on results from [Mistral AI NeMo announcement](https://mistral.ai/news/mistral-nemo/)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由作者基于[Mistral AI NeMo公告](https://mistral.ai/news/mistral-nemo/)的结果创建
- en: '**Mistral Large 2**'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**Mistral Large 2**'
- en: '**Overview**: [Mistral Large 2](https://mistral.ai/news/mistral-large-2407/)
    provides a **128K token context window**, improved function calling, support for
    numerous languages and 8**0+ coding languages**. Like Codestral Mamba and NeMo,
    Mistral Large 2 was trained on a large volume of code allowing it to perform competitively
    with GPT-4o, Claude 3 Opus, and Llama 3.1 405B. During training, the Mistral team
    **focused on reducing the model’s likelihood of hallucinations** making Mistral
    Large 2 more likely to respond that it cannot find an answer or lacks the information
    needed to provide a response.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概述**：[Mistral Large 2](https://mistral.ai/news/mistral-large-2407/) 提供了 **128K
    token 上下文窗口**、改进的函数调用、支持多种语言和 80 多种编程语言。与 Codestral Mamba 和 NeMo 一样，Mistral Large
    2 在大量代码的基础上进行训练，使其在与 GPT-4o、Claude 3 Opus 和 Llama 3.1 405B 的竞争中表现出色。在训练过程中，Mistral
    团队 **专注于减少模型出现幻觉的可能性**，使得 Mistral Large 2 更倾向于回应它无法找到答案或缺乏提供回答所需的信息。'
- en: '**Availability**: Mistral Large 2 is available under the [Mistral Research
    License](https://mistral.ai/licenses/MRL-0.1.md). This allows experimentation
    and modification for research and non-commercial use cases. For those interested
    in using Mistral Large 2 commercially, you can [contact Mistral AI directly](https://mistral.ai/contact/)
    and request a Mistral Commercial License.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可用性**：Mistral Large 2 在 [Mistral Research License](https://mistral.ai/licenses/MRL-0.1.md)
    许可下提供。这允许在研究和非商业用途下进行实验和修改。对于那些有意在商业上使用 Mistral Large 2 的用户，可以通过 [直接联系 Mistral
    AI](https://mistral.ai/contact/) 并申请 Mistral 商业许可。'
- en: '**Performance**: Mistral Large 2 outperforms GPT-4o, and Claude 3 Opus on function
    calling tasks and performs similarly to these models on instruction following
    and alignment based tasks evaluated by the Wild Bench and Arena Hard benchmarks.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能**：Mistral Large 2 在函数调用任务上优于 GPT-4o 和 Claude 3 Opus，并且在遵循指令和对齐任务上表现与这些模型相似，经过
    Wild Bench 和 Arena Hard 基准测试评估。'
- en: '**GPT-4o mini**'
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**GPT-4o mini**'
- en: '**Overview**: GPT-4o mini is a small, cost effective model that supports text
    and vision and offers competitive reasoning and tool calling performance. It has
    a **128K token context window** with an impressive **16K token output length**.
    It is the most cost effective model from OpenAI at 15 cents per million input
    tokens and 60 cents per million output tokens. OpenAI notes that this price is
    99% cheaper than their text-davinci-003 model from 2022 indicating a trend towards
    cheaper, smaller, more capable models in a relatively short time frame. While
    GPT-4o mini does not support image, video, and audio inputs like GPT-4o does,
    OpenAI reports these features are coming soon. Like GPT-4o, GPT-4o mini has been
    trained with built-in safety measures and is the first OpenAI model that applies
    the [**instruction hierarchy**](https://arxiv.org/abs/2404.13208) **method** designed
    to make the model **more resistant to prompt injections and jailbreaks**. GPT-4o
    mini leverages the same tokenizer as GPT-4o which enables **improved performance
    on non-English text**. Shortly after the GPT-4o mini announcement, OpenAI also
    announced an e[xperimental 64K token output for GPT-4o](https://openai.com/gpt-4o-long-output/)
    available through their alpha program.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概述**：GPT-4o mini 是一个小型、成本效益高的模型，支持文本和视觉，并提供具有竞争力的推理和工具调用性能。它具有 **128K token
    上下文窗口** 和令人印象深刻的 **16K token 输出长度**。它是 OpenAI 中最具成本效益的模型，每百万输入 tokens 的费用为 15
    美分，每百万输出 tokens 的费用为 60 美分。OpenAI 提到，这一价格比其 2022 年的 text-davinci-003 模型便宜 99%，这表明在相对较短的时间内，模型趋向于更便宜、更小巧且功能更强大。虽然
    GPT-4o mini 不像 GPT-4o 那样支持图像、视频和音频输入，但 OpenAI 报告称这些功能即将推出。与 GPT-4o 一样，GPT-4o mini
    采用了内置的安全措施，并且是第一个应用 [**指令层次结构**](https://arxiv.org/abs/2404.13208) **方法** 的 OpenAI
    模型，该方法旨在使模型 **更能抵抗提示注入和越狱**。GPT-4o mini 使用与 GPT-4o 相同的分词器，从而实现 **在非英语文本上的性能提升**。在
    GPT-4o mini 宣布后不久，OpenAI 还宣布了一个 **实验性 64K token 输出** 的 GPT-4o，用户可以通过其 Alpha 项目进行体验。'
- en: '**Availability**: GPT-4o mini is a closed source model available through OpenAI’s
    Assistants API, Chat Completions API, and Batch API. It is also available through
    [Azure AI](https://azure.microsoft.com/en-us/blog/openais-fastest-model-gpt-4o-mini-is-now-available-on-azure-ai/).'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可用性**：GPT-4o mini 是一个闭源模型，通过 OpenAI 的 Assistants API、Chat Completions API
    和 Batch API 提供。它也可以通过 [Azure AI](https://azure.microsoft.com/en-us/blog/openais-fastest-model-gpt-4o-mini-is-now-available-on-azure-ai/)
    访问。'
- en: '**Performance**: GPT-4o mini outperforms Gemini Flash and Claude Haiku, models
    of similar size, on multiple benchmarks including [MMLU](https://arxiv.org/abs/2009.03300)
    (Massive Multitask Language Understanding) which is designed to measure reasoning
    ability, [MGSM](https://arxiv.org/abs/2210.03057) (Multilingual Grade School Math)
    which measures mathematical reasoning, [HumanEval](https://arxiv.org/abs/2107.03374)
    which measures coding ability, and [MMMU](https://arxiv.org/abs/2311.16502) (Massive
    Multi-discipline Multimodal Understanding and Reasoning Benchmark) which measures
    multimodal reasoning.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能**：GPT-4o mini在多个基准测试中优于同类尺寸的Gemini Flash和Claude Haiku模型，包括[MMLU](https://arxiv.org/abs/2009.03300)（大规模多任务语言理解，旨在衡量推理能力）、[MGSM](https://arxiv.org/abs/2210.03057)（多语言小学数学，衡量数学推理能力）、[HumanEval](https://arxiv.org/abs/2107.03374)（衡量编程能力）和[MMMU](https://arxiv.org/abs/2311.16502)（大规模多学科多模态理解与推理基准，衡量多模态推理能力）。'
- en: '![](../Images/c9a13e62a7becc06958538e36a815f33.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c9a13e62a7becc06958538e36a815f33.png)'
- en: Image by author based on results from [GPT-4o mini announcement](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 该图由作者根据[Feynmans Learning Method](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)结果绘制
- en: '**Llama 3.1**'
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**Llama 3.1**'
- en: '**Overview**: Llama 3.1 introduces a **128K token context window**, a significant
    jump from the 8K token context window for Llama 3, which was released only three
    months ago in April. Llama 3.1 is **available in three sizes: 405B, 70B, and 8B.**
    It offers improved reasoning, tool-calling, and multilingual performance. Meta’s
    Llama 3.1 announcement calls **Llama 3.1 405B the “first frontier-level open source
    AI model”.** This demonstrates a huge stride forward for the open source community
    and demonstrates Meta’s commitment to making AI accessible, Mark Zuckerberg discusses
    this in more detail in his article “[Open Source AI is the Path Forward](https://about.fb.com/news/2024/07/open-source-ai-is-the-path-forward/)”.
    The Llama 3.1 announcement also includes guidance on enabling common use cases
    like real-time and batch inference, fine-tuning, RAG, continued pre-training,
    synthetic data generation, and distillation. Meta also released the [Llama Reference
    System](https://github.com/meta-llama/llama-agentic-system) to support developers
    working on agentic based use cases with Llama 3.1 and additional [AI safety tools](https://ai.meta.com/blog/meta-llama-3-1-ai-responsibility/)
    including Llama Guard 3 to moderate inputs and outputs in multiple languages,
    Prompt Guard to mitigate prompt injections, and CyberSecEval 3 to reduce GenAI
    security risks.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概述**：Llama 3.1引入了**128K token上下文窗口**，这是相较于Llama 3（仅三个月前在四月发布）的8K token上下文窗口的重大进步。Llama
    3.1**提供三种规模：405B、70B和8B**。它提升了推理、工具调用和多语言性能。Meta在其Llama 3.1的公告中称**Llama 3.1 405B是“首个前沿级别的开源AI模型”**。这展示了开源社区的巨大进步，也体现了Meta致力于使AI更具可访问性的承诺。马克·扎克伯格在他的文章“[开源AI是前进之路](https://about.fb.com/news/2024/07/open-source-ai-is-the-path-forward/)”中详细讨论了这一点。Llama
    3.1的公告还包括了关于如何启用常见用例的指南，如实时和批处理推理、微调、RAG、持续预训练、合成数据生成和蒸馏。Meta还发布了[**Llama Reference
    System**](https://github.com/meta-llama/llama-agentic-system)，以支持开发者基于Llama 3.1进行代理性用例开发，此外还发布了其他[AI安全工具](https://ai.meta.com/blog/meta-llama-3-1-ai-responsibility/)，包括Llama
    Guard 3用于多语言的输入和输出内容的监控、Prompt Guard用于减轻提示注入攻击、以及CyberSecEval 3用于降低生成式AI的安全风险。'
- en: '**Availability**: Llama 3.1 is an open source model. Meta has changed their
    license to allow developers to use the outputs from Llama models to train and
    improve other models. Models are available through HuggingFace, llama.meta.com,
    and through other partner platforms like Azure AI.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可用性**：Llama 3.1是一个开源模型。Meta已更改其许可证，允许开发者使用Llama模型的输出用于训练和改进其他模型。模型可以通过HuggingFace、llama.meta.com以及其他合作平台如Azure
    AI获取。'
- en: '**Performance**: Each of the Llama 3.1 models outperform other models in their
    size class across nearly all the common language model benchmarks for reasoning,
    coding, math, tool use, long context, and multilingual performance.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能**：每个Llama 3.1模型在其规模类别中，几乎在所有常见的语言模型基准测试中，包括推理、编码、数学、工具使用、长上下文和多语言性能上，都优于其他模型。'
- en: '![](../Images/450cbf33dfabb38a07c3b2ce3a1466a1.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/450cbf33dfabb38a07c3b2ce3a1466a1.png)'
- en: Image by author based on results from [Meta Llama 3.1 announcement](https://ai.meta.com/blog/meta-llama-3-1/)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 该图由作者根据[Meta Llama 3.1公告](https://ai.meta.com/blog/meta-llama-3-1/)结果绘制
- en: Trends in GenAI Models
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成式AI模型的趋势
- en: Overall, there is a trend towards increasingly capable models of all sizes with
    longer context windows, longer token output lengths, and lower price points. The
    push towards improved reasoning, tool calling, and coding abilities reflect the
    increasing demand for agentic systems capable of taking complex actions on behalf
    of users. To create effective agent systems, models need to understand how to
    break down a problem, how to use the tools available to them, and how to reconcile
    lots of information at one time.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，所有规模的模型呈现出越来越强大的趋势，拥有更长的上下文窗口、更长的令牌输出长度以及更低的价格点。推动推理、工具调用和编码能力的提升反映了对能够代表用户执行复杂操作的代理系统的需求不断增加。为了创建有效的代理系统，模型需要理解如何分解问题、如何使用可用的工具以及如何在一次处理大量信息时进行调和。
- en: The recent announcements from OpenAI and Meta reflect the growing discussion
    around AI safety with both companies demonstrating different ways to approach
    the same challenge. OpenAI has taken a closed source approach and improved model
    safety through applying feedback from experts in social psychology and misinformation
    and implementing new training methods. In contrast, Meta has doubled down on their
    open source initiatives and released new tools focused on helping developers mitigate
    AI safety concerns.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，OpenAI和Meta的公告反映了AI安全讨论的日益增加，两家公司展示了应对同一挑战的不同方法。OpenAI采取了闭源的方式，通过采纳社会心理学和虚假信息领域专家的反馈以及实施新的训练方法来提高模型的安全性。相比之下，Meta加大了其开源计划的力度，发布了新的工具，专注于帮助开发者减轻AI安全问题。
- en: '![](../Images/323d506da8b3ee322752fc81dd3cf6d1.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/323d506da8b3ee322752fc81dd3cf6d1.png)'
- en: Image created by author with GPT-4o depicting an arena with closed and open
    source models competing.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图片由作者使用GPT-4o创建，展示了一个闭源和开源模型竞争的竞技场。
- en: Conclusion
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In the future, I think we’ll continue to see advancements in generalist and
    specialist models with frontier models like GPT-4o and Llama 3.1 getting better
    and better at breaking down problems and performing a variety of tasks across
    modalities, while specialist models like Codestral Mamba will excel in their domain
    and become more adept at handling longer contexts and nuanced tasks within their
    area of expertise. Additionally, I expect we’ll see new benchmarks focused on
    models’ ability to follow multiple directions at once within a single turn and
    a proliferation of AI systems that leverage generalist and specialist models to
    perform tasks as a team.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 未来，我认为我们将继续看到通用模型和专业模型的进展，像GPT-4o和Llama 3.1这样的前沿模型将越来越擅长于分解问题，并在跨模态执行各种任务时表现得越来越好，而像Codestral
    Mamba这样的专业模型将在其领域中表现出色，并在处理更长上下文和微妙任务时变得更加得心应手。此外，我预计我们将看到新的基准，专注于模型在单次回合内同时执行多个指令的能力，以及越来越多的AI系统通过结合通用模型和专业模型协作执行任务。
- en: Furthermore, while model performance is typically measured based on standard
    benchmarks, what ultimately matters is how humans perceive the performance and
    how effectively models can further human goals. The Llama 3.1 announcement includes
    an interesting graphic demonstrating how people rated responses from Llama 3.1
    compared to GPT-4o, GPT-4, and Claude 3.5\. The results show that Llama 3.1 received
    a tie from humans in over 50% of the examples with the remaining win rates roughly
    split between Llama 3.1 and it’s challenger. This is significant because it suggests
    that open source models can now readily compete in a league that was previously
    dominated by closed source models.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，虽然模型性能通常是根据标准基准来衡量的，但最终重要的是人类如何看待模型的表现，以及模型在多大程度上有效地推动人类目标的实现。Llama 3.1的发布包含了一张有趣的图表，展示了人们如何对Llama
    3.1与GPT-4o、GPT-4和Claude 3.5的回答进行评分。结果表明，Llama 3.1在超过50%的示例中与其他模型平局，剩余的胜率大致在Llama
    3.1和其挑战者之间分配。这一点具有重要意义，因为它表明开源模型现在可以在一个以前由闭源模型主导的领域中轻松竞争。
- en: '*Interested in discussing further or collaborating? Reach out on* [*LinkedIn*](https://www.linkedin.com/in/tula-masterman/)*!*'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*想要进一步讨论或合作？请通过* [*LinkedIn*](https://www.linkedin.com/in/tula-masterman/)*与我联系！*'
