- en: 'Computer Use and AI Agents: A New Paradigm for Screen Interaction'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算机使用与AI代理人：屏幕互动的新范式
- en: 原文：[https://towardsdatascience.com/computer-use-and-ai-agents-a-new-paradigm-for-screen-interaction-b2dcbea0df5b?source=collection_archive---------6-----------------------#2024-10-30](https://towardsdatascience.com/computer-use-and-ai-agents-a-new-paradigm-for-screen-interaction-b2dcbea0df5b?source=collection_archive---------6-----------------------#2024-10-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/computer-use-and-ai-agents-a-new-paradigm-for-screen-interaction-b2dcbea0df5b?source=collection_archive---------6-----------------------#2024-10-30](https://towardsdatascience.com/computer-use-and-ai-agents-a-new-paradigm-for-screen-interaction-b2dcbea0df5b?source=collection_archive---------6-----------------------#2024-10-30)
- en: Exploring the future of multimodal AI Agents and the Impact of Screen Interaction
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索多模态AI代理人的未来以及屏幕互动的影响
- en: '[](https://medium.com/@tula.masterman?source=post_page---byline--b2dcbea0df5b--------------------------------)[![Tula
    Masterman](../Images/c36b3740befd5dfdb8719dc6596f1a99.png)](https://medium.com/@tula.masterman?source=post_page---byline--b2dcbea0df5b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b2dcbea0df5b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b2dcbea0df5b--------------------------------)
    [Tula Masterman](https://medium.com/@tula.masterman?source=post_page---byline--b2dcbea0df5b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@tula.masterman?source=post_page---byline--b2dcbea0df5b--------------------------------)[![Tula
    Masterman](../Images/c36b3740befd5dfdb8719dc6596f1a99.png)](https://medium.com/@tula.masterman?source=post_page---byline--b2dcbea0df5b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b2dcbea0df5b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b2dcbea0df5b--------------------------------)
    [Tula Masterman](https://medium.com/@tula.masterman?source=post_page---byline--b2dcbea0df5b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b2dcbea0df5b--------------------------------)
    ·7 min read·Oct 30, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b2dcbea0df5b--------------------------------)
    ·7分钟阅读·2024年10月30日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/415e6388c247739dd845b959f936aec5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/415e6388c247739dd845b959f936aec5.png)'
- en: Image created by author using GPT4o
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用GPT4o创作
- en: '**Introduction**: The ever-evolving AI Agent Landscape'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**介绍**：不断发展的AI代理人领域'
- en: Recent announcements from Anthropic, Microsoft, and Apple are changing the way
    we think about AI Agents. Today, the term “AI Agent” is oversaturated — nearly
    every AI-related announcement refers to agents, but their sophistication and utility
    vary greatly.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最近来自Anthropic、Microsoft和Apple的公告正在改变我们对AI代理人的看法。今天，“AI代理人”这个术语已经过度饱和——几乎每一个与AI相关的公告都提到代理人，但它们的复杂性和实用性差异很大。
- en: At one end of the spectrum, we have advanced agents that leverage multiple loops
    for planning, tool execution, and goal evaluation, iterating until they complete
    a task. These agents might even create and use memories, learning from their past
    mistakes to drive future successes. Determining what makes an effective agent
    is a very active area of AI research. It involves understanding what attributes
    make a successful agent (e.g., how should the agent plan, how should it use memory,
    how many tools should it use, how should it keep track of it’s task) and the best
    approach to configure a team of agents.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个极端，我们有利用多个循环进行规划、工具执行和目标评估的先进代理人，它们会反复迭代，直到完成任务。这些代理人甚至可能创建并使用记忆，从过去的错误中学习，以推动未来的成功。确定一个有效代理人的标准是AI研究的一个非常活跃的领域。它涉及理解哪些属性构成一个成功的代理人（例如，代理人应如何规划，如何使用记忆，应该使用多少工具，如何跟踪任务）以及配置一组代理人的最佳方法。
- en: On the other end of the spectrum, we find AI agents that execute single purpose
    tasks that require little if any reasoning. These agents are often more workflow
    focused. For example, an agent that consistently summarizes a document and stores
    the result. These agents are typically easier to implement because the use cases
    are narrowly defined, requiring less planning or coordination across multiple
    tools and fewer complex decisions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一个极端，我们发现执行单一目的任务且几乎不需要推理的AI代理人。这些代理人通常更侧重于工作流。例如，一个持续总结文档并存储结果的代理人。这些代理人通常更容易实现，因为使用场景较为狭窄，需要较少的规划或跨多个工具的协调，以及较少的复杂决策。
- en: With the latest announcements from Anthropic, Microsoft, and Apple, we’re **witnessing
    a shift from text-based AI agents to multimodal agents**. This opens up the potential
    to give an agent written or verbal instructions and allow it to seamlessly navigate
    your phone or computer to complete tasks. **This has great potential to improve
    accessibility across devices, but also comes with significant risks.** Anthropic’s
    computer use announcement highlights the risks of giving AI unfettered access
    to your screen, and provides risk mitigation tactics like running Claude in a
    dedicated virtual machine or container, limiting internet access to an allowlist
    of permitted domains, including human in the loop checks, and avoiding giving
    the model access to sensitive data. They note that no content submitted to the
    API will be used for training.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 Anthropic、微软和苹果的最新公告，我们正在**见证从基于文本的 AI 代理到多模态代理的转变**。这为 AI 代理提供了书面或口头指令的潜力，并使其能够无缝地导航您的手机或计算机来完成任务。**这具有极大潜力改善跨设备的可访问性，但也伴随着重大风险。**
    Anthropic 的计算机使用公告强调了将 AI 给予无限制屏幕访问权限的风险，并提供了如在专用虚拟机或容器中运行 Claude、限制互联网访问仅允许特定域名、包括人工检查以及避免让模型访问敏感数据等风险缓解策略。他们还指出，提交给
    API 的内容不会用于训练。
- en: 'Key Announcements from Anthropic, Microsoft, and Apple:'
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Anthropic、微软和苹果的关键公告：
- en: '**Anthropic’s Claude 3.5 Sonnet: Giving AI the Power to Use Computers**'
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**Anthropic 的 Claude 3.5 Sonnet：赋予 AI 使用计算机的能力**'
- en: '**Overview**: The goal of Computer Use is to give AI the ability to interact
    with a computer the same way a human would. Ideally Claude would be able to open
    and edit documents, click to various areas of the page, scroll and read pages,
    run and execute command line code, and more. Today, Claude can follow instructions
    from a human to move a cursor around the computer screen, click on relevant areas
    of the screen, and type into a virtual keyboard. Claude Scored 14.9% on the [OSWorld](https://os-world.github.io/)
    benchmark, which is higher than other AI models on the same benchmark, but still
    significantly behind humans (humans typically score 70–75%).'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概述**：计算机使用的目标是赋予 AI 与人类一样与计算机互动的能力。理想情况下，Claude 应该能够打开和编辑文档，点击页面的各个区域，滚动和阅读页面，运行和执行命令行代码等等。目前，Claude
    能够按照人的指示在计算机屏幕上移动光标，点击相关区域，并在虚拟键盘上输入内容。Claude 在 [OSWorld](https://os-world.github.io/)
    基准测试中得分为 14.9%，高于其他 AI 模型，但仍远低于人类（人类通常得分为 70-75%）。'
- en: '**How it works**: Claude looks at user submitted screenshots and counts pixels
    to determine where it needs to move the cursor to complete the task. Researchers
    note that Claude was not given internet access during training for safety reasons,
    but that Claude was able to generalize from training tasks like using a calculator
    and text-editor to more complex tasks. It even retried tasks when it failed. Computer
    use includes three Anthropic defined tools: computer, text editor, and bash. The
    computer tool is used for screen navigation, text editor is used for viewing,
    creating, and editing text files, and bash is used to run bash shell commands.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作原理**：Claude 会查看用户提交的截图并计算像素，以确定需要将光标移动到哪里以完成任务。研究人员指出，Claude 在训练期间没有互联网访问权限以确保安全，但
    Claude 能够通过训练任务，如使用计算器和文本编辑器，推广到更复杂的任务。即使失败，Claude 也会重新尝试任务。计算机使用包括三种由 Anthropic
    定义的工具：计算机、文本编辑器和 bash。计算机工具用于屏幕导航，文本编辑器用于查看、创建和编辑文本文件，bash 用于运行 bash shell 命令。'
- en: '**Challenges**: Despite it’s promising performance, there’s still a long way
    to go for Claude’s computer use abilities. Today it struggles with scrolling,
    overall reliability, and is vulnerable to prompt injections.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**挑战**：尽管 Claude 展现了有前景的表现，但其计算机使用能力仍然有很长的路要走。目前，它在滚动、整体可靠性方面存在困难，并且容易受到提示注入攻击。'
- en: '**How to Use**: Public beta available through the Anthropic API. Computer use
    can be combined with regular tool use.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**如何使用**：通过 Anthropic API 提供公开测试版。计算机使用可以与常规工具使用结合。'
- en: '**Microsoft’s OmniParser & GPT-4V: Making Screens Understandable and Actionable
    for AI**'
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**微软的 OmniParser 和 GPT-4V：让 AI 理解并操作屏幕**'
- en: '**Overview**: OmniParser is designed to parse screenshots of user interfaces
    and transform them into structured outputs. These outputs can be passed to a model
    like GPT-4V to generate actions based on the detected screen elements. OmniParser
    + GPT-4V were scored on a variety of benchmarks including [Windows Agent Arena](https://microsoft.github.io/WindowsAgentArena/)
    which adapts the OSWorld benchmark to create Windows specific tasks. These tasks
    are designed to evaluate an agents ability to plan, understand the screen, and
    use tools, OmniParser & GPT-4V scored ~20%.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概述**：OmniParser 旨在解析用户界面的截图，并将其转化为结构化输出。这些输出可以传递给像 GPT-4V 这样的模型，以基于检测到的屏幕元素生成操作。OmniParser
    + GPT-4V 在多项基准测试中取得了分数，包括 [Windows Agent Arena](https://microsoft.github.io/WindowsAgentArena/)，该基准测试将
    OSWorld 基准适配为 Windows 特定任务。这些任务旨在评估智能体的规划能力、理解屏幕的能力以及使用工具的能力，OmniParser 和 GPT-4V
    的得分约为 20%。'
- en: '**How it Works**: OmniParser combines multiple fine-tuned models to understand
    screens. It uses a finetuned interactable icon/region detection model ([YOLOv8](https://yolov8.com/)),
    a finetuned icon description model ([BLIP-2](https://arxiv.org/abs/2301.12597)
    or [Florence2](https://arxiv.org/abs/2311.06242)), and an OCR module. These models
    are used to detect icons and text and generate descriptions before sending this
    output to GPT-4V which decides how to use the output to interact with the screen.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作原理**：OmniParser 结合了多个微调的模型来理解屏幕。它使用了微调的可交互图标/区域检测模型（[YOLOv8](https://yolov8.com/)）、微调的图标描述模型（[BLIP-2](https://arxiv.org/abs/2301.12597)
    或 [Florence2](https://arxiv.org/abs/2311.06242)）和 OCR 模块。这些模型用于检测图标和文本并生成描述，然后将这些输出传递给
    GPT-4V，后者决定如何利用这些输出与屏幕进行交互。'
- en: '**Challenges**: Today, when OmniParser detects repeated icons or text and passes
    them to GPT-4V, GPT-4V usually fails to click on the correct icon. Additionally,
    OmniParser is subject to OCR output so if the bounding box is off, the whole system
    might fail to click on the appropriate area for clickable links. There are also
    challenges with understanding certain icons since sometimes the same icon is used
    to describe different concepts (e.g., three dots for loading versus for a menu
    item).'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**挑战**：目前，当 OmniParser 检测到重复的图标或文本并将其传递给 GPT-4V 时，GPT-4V 通常无法点击正确的图标。此外，OmniParser
    受限于 OCR 输出，因此如果边界框偏移，整个系统可能无法点击正确的可点击链接区域。对于某些图标的理解也存在挑战，因为有时相同的图标用来表示不同的概念（例如，三个点用于加载与用于菜单项）。'
- en: '**How to Use**: OmniParser is available on [GitHub](https://github.com/microsoft/OmniParser/)
    & [HuggingFace](https://huggingface.co/microsoft/OmniParser) you will need to
    install the requirements and load the model from HuggingFace, next you can try
    running the demo notebooks to see how OmniParser breaks down images.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**如何使用**：OmniParser 可在 [GitHub](https://github.com/microsoft/OmniParser/) 和
    [HuggingFace](https://huggingface.co/microsoft/OmniParser) 上获取，你需要安装相关依赖并从 HuggingFace
    加载模型，接下来你可以尝试运行示例笔记本，查看 OmniParser 如何解析图像。'
- en: 'Apple’s Ferret-UI: Bringing Multimodal Intelligence to Mobile UIs'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apple 的 Ferret-UI：为移动 UI 带来多模态智能
- en: '**Overview**: Apple’s Ferret (Refer and Ground Anything Anywhere at Any Granularity)
    has been around since 2023, but recently Apple released Ferret-UI a MLLM (Multimodal
    Large Language Model) which can execute “referring, grounding, and reasoning tasks”
    on mobile UI screens. Referring tasks include actions like widget classification
    and icon recognition. Grounding tasks include tasks like find icon or find text.
    Ferret-UI can understand UIs and follow instructions to interact with the UI.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概述**：Apple 的 Ferret（随时随地在任何粒度上进行引用和定位）自 2023 年以来就已推出，但最近 Apple 发布了 Ferret-UI，一种多模态大型语言模型（MLLM），能够执行移动
    UI 屏幕上的“引用、定位和推理任务”。引用任务包括小部件分类和图标识别等操作。定位任务包括查找图标或查找文本等操作。Ferret-UI 能够理解 UI 并按照指令与
    UI 进行交互。'
- en: '**How it Works**: Ferret-UI is based on Ferret and adapted to work on finer
    grained images by training with “any resolution” so it can better understand mobile
    UIs. Each image is split into two sub-images which have their own features generated.
    The LLM uses the full image, both sub-images, regional features, and text embeddings
    to generate a response.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作原理**：Ferret-UI 基于 Ferret，并通过在“任何分辨率”下进行训练，适配了更精细的图像，从而能更好地理解移动端 UI。每个图像会被分割成两个子图像，并生成各自的特征。大型语言模型（LLM）使用完整图像、两个子图像、区域特征和文本嵌入来生成响应。'
- en: '**Challenges**: Some of the results cited in the Ferret-UI paper demonstrate
    instances where Ferret predicts nearby text instead of the target text, predicts
    valid words when presented with a screen that has misspelled words, it also sometimes
    misclassifies UI attributes.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**挑战**：Ferret-UI论文中引用的一些结果展示了Ferret在预测附近文本而非目标文本时出现的情况，或在屏幕上出现拼写错误时仍能预测有效的单词，有时它还会错误分类UI属性。'
- en: '**How to Use**: Apple made the data and code available on [GitHub](https://github.com/apple/ml-ferret/tree/main/ferretui)
    for research use only. Apple released two Ferret-UI checkpoints, one built on
    Gemma-2b and one built on Llama-3–8B. The Ferret-UI models are subject to the
    licenses for Gemma and Llama while the dataset allows non-commercial use.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**如何使用**：Apple在[GitHub](https://github.com/apple/ml-ferret/tree/main/ferretui)上发布了数据和代码，仅供研究使用。Apple发布了两个Ferret-UI检查点，一个基于Gemma-2b，另一个基于Llama-3–8B。Ferret-UI模型受Gemma和Llama许可证的约束，而数据集允许非商业用途。'
- en: 'Summary: Three Approaches to AI Driven Screen Navigation'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结：三种AI驱动的屏幕导航方法
- en: In summary, each of these systems demonstrate a different approach to building
    multimodal agents that can interact with computers or mobile devices on our behalf.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，这些系统展示了不同的方式来构建多模态智能体，使其能够代表我们与计算机或移动设备进行交互。
- en: Anthropic’s Claude 3.5 Sonnet focuses on general computer interaction where
    Claude counts pixels to appropriately navigate the screen. Microsoft’s OmniParser
    addresses specific challenges for breaking down user interfaces into structured
    outputs which are then sent to models like GPT-4V to determine actions. Apple’s
    Ferret-UI is tailored to mobile UI comprehension allowing it to identify icons,
    text, and widgets while also executing open-ended instructions related to the
    UI.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Anthropic的Claude 3.5 Sonnet专注于一般的计算机交互，其中Claude通过计算像素来适当地导航屏幕。微软的OmniParser解决了将用户界面分解为结构化输出的具体挑战，这些输出随后被发送到像GPT-4V这样的模型中以确定行动。Apple的Ferret-UI专为移动UI理解而设计，能够识别图标、文本和小部件，同时还能够执行与UI相关的开放式指令。
- en: Across each system, the **workflow typically follows two key phases one for
    parsing the visual information and one for reasoning about how to interact with
    it**. Parsing screens accurately is critical for properly planning how to interact
    with the screen and making sure the system reliably executes tasks.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个系统中，**工作流通常遵循两个关键阶段：一个是解析视觉信息，另一个是推理如何与之交互**。准确地解析屏幕对于正确规划如何与屏幕交互并确保系统可靠执行任务至关重要。
- en: 'Conclusion: Building Smarter, Safer AI Agents'
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论：构建更智能、更安全的AI智能体
- en: In my opinion, the most exciting aspect of these developments is how **multimodal
    capabilities and reasoning frameworks are starting to converge**. While these
    tools offer **promising capabilities**, they still lag significantly behind human
    performance. There are also s**ignificant AI safety concerns** which need to be
    addressed when implementing any agentic system with screen access.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在我看来，这些发展的最激动人心之处在于**多模态能力和推理框架的开始融合**。尽管这些工具提供了**有前景的功能**，但它们仍显著落后于人类的表现。同时，**AI安全问题**也是在实施任何具有屏幕访问权限的智能体系统时需要解决的重要问题。
- en: '**One of the biggest benefits of agentic systems is their potential to overcome
    the cognitive limitations of individual models by breaking down tasks into specialized
    components.** These systems can be built in many ways. In some cases, what appears
    to the user as **a single agent** may, behind the scenes, consist of **a team
    of sub-agents** — each **managing distinct responsibilities** like planning, screen
    interaction, or memory management. For example, a reasoning agent might coordinate
    with another agent that specializes in parsing screen data, while a separate agent
    curates memories to enhance future performance.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**智能体系统的最大优势之一是它们有可能通过将任务分解为专门的组件来克服单一模型的认知限制。** 这些系统可以通过多种方式构建。在某些情况下，用户看到的**单一智能体**可能在幕后实际上由**一组子智能体**组成——每个子智能体**管理不同的责任**，例如规划、屏幕交互或记忆管理。例如，一个推理智能体可能与另一个专门解析屏幕数据的智能体协调，同时，一个独立的智能体负责策划记忆，以提升未来的表现。'
- en: 'Alternatively, these capabilities might be **combined within one robust agent**.
    In this setup, the agent could have multiple internal planning modules— one focused
    on planning the screen interactions and another focused on managing the overall
    task. The best approach to structuring agents remains to be seen, but the goal
    remains the same: to create agents that perform reliably overtime, across multiple
    modalities, and adapt seamlessly to the user’s needs.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，这些功能可能会**结合在一个强大的代理中**。在这种设置下，代理可能会有多个内部规划模块——一个专注于规划屏幕交互，另一个专注于管理整体任务。如何构建代理的最佳方法仍有待观察，但目标始终不变：创建能够在多种模式下可靠执行、并且能够无缝适应用户需求的代理。
- en: '**References:**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献：**'
- en: '[Developing Computer Use](https://www.anthropic.com/news/developing-computer-use)
    (Anthropic)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[开发计算机使用功能](https://www.anthropic.com/news/developing-computer-use) (Anthropic)'
- en: '[Introducing computer use, a new Claude 3.5 Sonnet, and Claude 3.5 Haiku](https://www.anthropic.com/news/3-5-models-and-computer-use)
    (Anthropic)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[介绍计算机使用、新版 Claude 3.5 Sonnet 和 Claude 3.5 Haiku](https://www.anthropic.com/news/3-5-models-and-computer-use)
    (Anthropic)'
- en: '[Computer Use Documentation](https://docs.anthropic.com/en/docs/build-with-claude/computer-use)
    (Anthropic)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[计算机使用文档](https://docs.anthropic.com/en/docs/build-with-claude/computer-use)
    (Anthropic)'
- en: '[OSWorld GitHub](https://os-world.github.io/)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OSWorld GitHub](https://os-world.github.io/)'
- en: '[Windows Agent Arena](https://microsoft.github.io/WindowsAgentArena/)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Windows Agent Arena](https://microsoft.github.io/WindowsAgentArena/)'
- en: '[OmniParser for pure vision-based GUI agent](https://www.microsoft.com/en-us/research/articles/omniparser-for-pure-vision-based-gui-agent/)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OmniParser 用于纯视觉基础的 GUI 代理](https://www.microsoft.com/en-us/research/articles/omniparser-for-pure-vision-based-gui-agent/)'
- en: '[OmniParser GitHub](https://github.com/microsoft/OmniParser/)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OmniParser GitHub](https://github.com/microsoft/OmniParser/)'
- en: '[OmniParser HuggingFace](https://huggingface.co/microsoft/OmniParser)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OmniParser HuggingFace](https://huggingface.co/microsoft/OmniParser)'
- en: '[OmniParser ArXiv](https://arxiv.org/pdf/2408.00203)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OmniParser ArXiv](https://arxiv.org/pdf/2408.00203)'
- en: '[Ferret GitHub](https://github.com/apple/ml-ferret)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ferret GitHub](https://github.com/apple/ml-ferret)'
- en: '[Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs](https://arxiv.org/pdf/2404.05719)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ferret-UI: 使用多模态 LLM 进行基于实际应用的移动 UI 理解](https://arxiv.org/pdf/2404.05719)'
- en: '*Interested in discussing further or collaborating? Reach out on* [*LinkedIn*](https://www.linkedin.com/in/tula-masterman/)*!*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*有兴趣进一步讨论或合作吗？请通过* [*LinkedIn*](https://www.linkedin.com/in/tula-masterman/)*联系我们！*'
