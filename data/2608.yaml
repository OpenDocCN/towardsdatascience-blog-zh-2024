- en: 'Gen-AI Safety Landscape: A Guide to the Mitigation Stack for Text-to-Image
    Models'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成型人工智能安全格局：文本生成图像模型的缓解栈指南
- en: 原文：[https://towardsdatascience.com/gen-ai-safety-landscape-a-guide-to-the-mitigation-stack-for-text-to-image-models-0848eb613ce5?source=collection_archive---------2-----------------------#2024-10-26](https://towardsdatascience.com/gen-ai-safety-landscape-a-guide-to-the-mitigation-stack-for-text-to-image-models-0848eb613ce5?source=collection_archive---------2-----------------------#2024-10-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/gen-ai-safety-landscape-a-guide-to-the-mitigation-stack-for-text-to-image-models-0848eb613ce5?source=collection_archive---------2-----------------------#2024-10-26](https://towardsdatascience.com/gen-ai-safety-landscape-a-guide-to-the-mitigation-stack-for-text-to-image-models-0848eb613ce5?source=collection_archive---------2-----------------------#2024-10-26)
- en: 'No Wild West for AI: A tour of the safety components that tame T2I models'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 没有“荒野西部”的人工智能：巡礼T2I模型的安全组件
- en: '[](https://medium.com/@tbavalatti?source=post_page---byline--0848eb613ce5--------------------------------)[![Trupti
    Bavalatti](../Images/c7bf54bde41b187f81449c91682b0641.png)](https://medium.com/@tbavalatti?source=post_page---byline--0848eb613ce5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0848eb613ce5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0848eb613ce5--------------------------------)
    [Trupti Bavalatti](https://medium.com/@tbavalatti?source=post_page---byline--0848eb613ce5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@tbavalatti?source=post_page---byline--0848eb613ce5--------------------------------)[![Trupti
    Bavalatti](../Images/c7bf54bde41b187f81449c91682b0641.png)](https://medium.com/@tbavalatti?source=post_page---byline--0848eb613ce5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0848eb613ce5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0848eb613ce5--------------------------------)
    [Trupti Bavalatti](https://medium.com/@tbavalatti?source=post_page---byline--0848eb613ce5--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0848eb613ce5--------------------------------)
    ·12 min read·Oct 26, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0848eb613ce5--------------------------------)
    ·12分钟阅读·2024年10月26日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Understanding Text-to-Image AI Model Capabilities and Risks
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解文本生成图像AI模型的能力与风险
- en: Text-to-Image models (T2I) are AI systems that generate images based on text
    prompt descriptions. Latent Diffusion Models (LDM) are emerging as one of the
    most popular architectures for image generation. LDMs first compress images into
    a “latent space”, which is a compressed, simplified representation of the core
    information needed to represent an image without all the detailed pixel data in
    fewer dimensions. The model starts with random noise in this latent space and
    gradually refines it into a clear image through a process called diffusion, guided
    by the input text. LDMs are versatile and are capable not only of generating text-to-image
    outputs but also has capabilities like inpainting, which allows users to edit
    specific parts of an existing image by simply describing the desired changes.
    For example, you can remove an object from a photo or add new elements seamlessly,
    all through text commands.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 文本生成图像模型（T2I）是根据文本提示描述生成图像的人工智能系统。潜在扩散模型（LDM）正成为生成图像最流行的架构之一。LDM首先将图像压缩到一个“潜在空间”中，这是一个简化的表示核心信息的压缩空间，用较少的维度表达图像的核心内容，而不包含所有详细的像素数据。该模型从潜在空间中的随机噪声开始，通过称为扩散的过程，在输入文本的引导下逐步将其精炼为清晰的图像。LDM非常灵活，除了能够生成文本到图像的输出外，还具有如修复功能（inpainting），允许用户通过简单描述所需的更改来编辑现有图像的特定部分。例如，你可以从照片中删除一个物体或无缝地添加新元素，全部通过文本命令完成。
- en: These capabilities pose significant safety risks that need to be carefully managed.
    The generated image could include explicit or inappropriate content, either in
    direct response to explicit prompts or unintentionally, even when the input prompt
    is harmless — for example, a request for images of person smoking might mistakenly
    produce images of an underage kid smoking. For inpainting capability, which allows
    users to alter images by uploading their own, there is a freedom to modify images
    of people that surpasses traditional photo editing tools in speed, scale, and
    efficiency, making it more accessible but also potentially more dangerous. It
    can be used to alter images in ways that can be harmful, such as changing someone’s
    appearance, removing clothing or modifying contextual elements like clothing or
    food items in religiously sensitive ways.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这些能力带来了显著的安全风险，需要谨慎管理。生成的图像可能包含明显或不适当的内容，无论是对明显的提示做出直接响应，还是无意中生成，即使输入提示本身是无害的——例如，要求生成吸烟人的图像可能会错误地生成未成年儿童吸烟的图像。对于图像修复功能，它允许用户通过上传自己的图像来修改图像，这一自由度超越了传统照片编辑工具的速度、规模和效率，虽然使得图像修改更加容易，但也可能带来更大的危险。它可以用来以有害的方式改变图像，例如改变某人的外貌、去除衣物或以宗教敏感的方式修改诸如衣物或食物等上下文元素。
- en: Safety mitigation stack
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安全缓解堆栈
- en: Given the potential risks tied to image generation and inpainting capabilities,
    it is necessary to establish a robust safety mitigation stack across different
    stages of the model’s lifecycle. This involves implementing protections during
    pre-training, fine-tuning, and post-training phases, such as applying safety filters
    to both input prompts and generated images or utilizing a multimodal classifier
    that evaluates both input text and output images simultaneously.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于图像生成和图像修复能力可能带来的风险，有必要在模型生命周期的不同阶段建立一个强大的安全缓解堆栈。这包括在预训练、微调和训练后阶段实施保护措施，例如对输入提示和生成图像应用安全过滤器，或使用一个多模态分类器，同时评估输入文本和输出图像。
- en: Pre-training and fine-tuning stages must incorporate ethical considerations
    and bias mitigation to ensure that the foundational models do not perpetuate harmful
    stereotypes, bias or generate inappropriate content. Once the safety fine-tuned
    foundation model is deployed to production, an input prompt classifier is essential
    to filter out explicit or harmful requests before any image generation occurs,
    preventing the model from processing unsafe inputs. Similarly, an output image
    classifier or a multimodal classifier can analyze the generated images to detect
    and flag any inappropriate or unintended image before it reaches the user.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练和微调阶段必须纳入伦理考量和偏见缓解措施，以确保基础模型不会延续有害的刻板印象、偏见或生成不适当的内容。一旦经过安全微调的基础模型部署到生产环境中，输入提示分类器至关重要，能够在生成任何图像之前过滤掉明显或有害的请求，从而防止模型处理不安全的输入。同样，输出图像分类器或多模态分类器可以分析生成的图像，检测并标记任何不适当或无意的图像，防止其传递给用户。
- en: This layered approach ensures multiple checkpoints throughout the process, significantly
    reducing the risk of harmful outputs and ensuring image generation technology
    is used responsibly.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分层方法确保了整个过程中有多个检查点，大大降低了有害输出的风险，确保图像生成技术得以负责任地使用。
- en: '![](../Images/168f02bffbd7d0778447fe399b6933a9.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/168f02bffbd7d0778447fe399b6933a9.png)'
- en: Safety mitigation stack. Image by Author
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 安全缓解堆栈。图像来源：作者
- en: Pre-training mitigations
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预训练缓解措施
- en: T2I models are trained on pairs of images and their corresponding captions.
    Pairs are drawn from a combination of publicly available sources and sources that
    are licensed.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: T2I模型在图像和相应的文字描述对上进行训练。数据对来自于公开可用资源与已授权的资源的组合。
- en: '**Mitigations on training data**'
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**训练数据的缓解措施**'
- en: T2I models are trained on billion-sized datasets of images scraped off the internet.
    Research [1] has shown that datasets of Image-Alt-text pairs like LION-400M contain
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: T2I模型是在来自互联网上抓取的亿级图像数据集上进行训练的。研究[1]表明，像LION-400M这样的图像-替代文本数据集包含
- en: troublesome and explicit images and text pairs of rape, pornography, malign
    stereotypes, racist and ethnic slurs, and other extremely problematic content.
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 包括强奸、色情、恶意刻板印象、种族歧视和民族侮辱言辞，以及其他极其有问题的内容的麻烦和明显的图像与文字对。
- en: Most models have a pre-training step to filter out such harmful content from
    the training data. DALL·E 2 [2] specifically mentions that explicit content including
    graphic sexual and violent content as well as images of some hate symbols have
    explicitly been filtered out. However studies have shown that filtering out sexual
    content exacerbates biases in the training data. Specifically, filtering of sexual
    content reduced the quantity of generated images of women in general because the
    images of women were disproportionately represented in filtered sexual imagery.
    Several approaches like rebalancing the dataset using synthetic data generation
    or re-weighting the filtered dataset so that its distribution better matched the
    distribution of unfiltered images have been taken to mitigate the amplification
    of bias issue [2].
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数模型在预训练步骤中都会过滤掉训练数据中的有害内容。DALL·E 2 [2] 特别提到，包含图形性性别和暴力内容以及一些仇恨符号的明确内容已被明确过滤掉。然而，研究表明，过滤掉性内容反而加剧了训练数据中的偏见问题。具体来说，过滤性内容减少了生成女性图像的数量，因为女性图像在过滤掉的性别化图像中占据了不成比例的比例。为了缓解偏见放大问题，采取了几种方法，如使用合成数据生成重新平衡数据集，或重新加权已过滤的数据集，使其分布更好地匹配未过滤图像的分布
    [2]。
- en: At this stage, it is also essential to consider privacy mitigations to ensure
    that no personal, sensitive, or identifiable information is included in the data
    used to train the model. Several techniques can be applied — anonymization can
    be used to remove or obfuscate any personal identifiers (names, addresses, faces),
    differential privacy (noise addition, subsampling a person’s data to remove overfitting)
    to ensure that individual data points cannot be reverse-engineered from the model,
    and filtering out any data that contains confidential or proprietary information.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，考虑隐私缓解措施同样至关重要，以确保用于训练模型的数据中不包含任何个人、敏感或可识别的信息。可以应用几种技术——匿名化可以用来去除或模糊任何个人标识符（姓名、地址、面孔），差分隐私（添加噪声、对个人数据进行子采样以避免过拟合）可以确保个体数据点不能从模型中被逆向工程提取，此外，还可以过滤掉任何包含机密或专有信息的数据。
- en: '**Fine-tuning the foundation model for safety**'
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**对基础模型进行安全微调**'
- en: LDMs inherently are known to have a range of safety issues (bias, stereotypes,
    etc.) as well as the lack of prompt alignment in certain high-risk areas. These
    are “unintentional harms” where users give a perfectly benign prompt but the LDM
    generates a harmful response. Some examples are unintended sexualization, where
    casual prompts like “*woman dressed for a date*” can generate a sexualized image.
    Or, issues caused by lack of prompt alignment as shown in example below, where
    the midjourney model is incapable of generating a wife in non-Indian attire for
    Indian men, whereas for white men, it correctly generates a wife of different
    ethnicities and in different attires.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: LDMs本身已知存在一系列安全问题（偏见、刻板印象等），以及在某些高风险领域缺乏提示对齐的问题。这些是“无意的伤害”，用户提供了一个完全无害的提示，但LDM生成了有害的回应。一些例子包括无意的性别化，例如像“*为约会穿衣的女人*”这样随意的提示可以生成性别化的图像。又如，缺乏提示对齐引发的问题，如下面的例子所示，其中Midjourney模型无法为印度男性生成穿着非印度服饰的妻子，而对于白人男性，它则能正确生成不同种族和不同服饰的妻子。
- en: '![](../Images/5865bc98cae7979f43cf340934a963a5.png)![](../Images/0a9c2e8a3bc8ab7c50a5cc97a9c44c89.png)![](../Images/4b7c3eba4c8256c061938ae653b03a98.png)![](../Images/ee3b01556fecbc7992e052186e778569.png)![](../Images/bb01424d68db4e88462eccfa815d8478.png)![](../Images/f226c739638a4d5b532d094b9f051c25.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5865bc98cae7979f43cf340934a963a5.png)![](../Images/0a9c2e8a3bc8ab7c50a5cc97a9c44c89.png)![](../Images/4b7c3eba4c8256c061938ae653b03a98.png)![](../Images/ee3b01556fecbc7992e052186e778569.png)![](../Images/bb01424d68db4e88462eccfa815d8478.png)![](../Images/f226c739638a4d5b532d094b9f051c25.png)'
- en: Images showing prompt misalignment issue. Top row depicts different ethnicity
    wife for Indian man, all wearing Indian attire, while bottom row shows same prompts
    but with white man, showing correct ethnicity and diverse attire for wife. Images
    generated by author using Midjourney bot
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 显示提示不对齐问题的图像。第一排展示了不同种族的印度男性妻子，所有妻子穿着印度服饰，而第二排展示了相同的提示，但对象为白人男性，展示了妻子正确的种族和多样的服饰。这些图像由作者使用Midjourney机器人生成
- en: There is also a large area of risk as documented in [4] where marginalized groups
    are associated with harmful connotations reinforcing societal hateful stereotypes.
    For example, representation of demographic groups that conflates humans with animals
    or mythological creatures (such as black people as monkeys or other primates),
    conflating humans with food or objects (like associating people with disabilities
    and vegetables) or associating demographic groups with negative semantic concepts
    (such as terrorism with muslim people).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如文献[4]所述，还存在一个较大的风险领域，其中边缘化群体被与有害的内涵相关联，强化了社会上的仇恨性刻板印象。例如，将人口群体与动物或神话生物混淆（例如，将黑人描绘成猴子或其他灵长类动物）、将人类与食物或物品混淆（例如，将残障人士与蔬菜联系在一起），或将某些人口群体与负面的语义概念联系在一起（例如，将恐怖主义与穆斯林人群联系在一起）。
- en: Problematic associations like these between groups of people and concepts reflect
    long-standing negative narratives about the group. If a generative AI model learns
    problematic associations from existing data, it may reproduce them in content
    that is generates [4].
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 像这些群体和概念之间的有问题关联，反映了关于这些群体的长期负面叙事。如果生成式AI模型从现有数据中学习到这些有问题的关联，它可能会在生成的内容中再现这些问题[4]。
- en: '![](../Images/a63fde917f76cc99efe8e95e5183ad88.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a63fde917f76cc99efe8e95e5183ad88.png)'
- en: Problematic Associations of marginalized groups and concepts. Image [source](https://arxiv.org/pdf/2402.17101)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘化群体与概念的有问题关联。图像[来源](https://arxiv.org/pdf/2402.17101)
- en: There are several ways to fine-tune the LLMs. According to [6], one common approach
    is called Supervised Fine-Tuning (SFT). This involves taking a pre-trained model
    and further training it with a dataset that includes pairs of inputs and desired
    outputs. The model adjusts it’s parameters by learning to better match these expected
    responses.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以微调LLMs。根据文献[6]，一种常见的方法叫做监督微调（SFT）。这种方法包括使用一组包含输入和期望输出对的数据集，对预训练的模型进行进一步训练。模型通过学习更好地匹配这些期望的响应来调整其参数。
- en: 'Typically, fine-tuning involves two phases: SFT to establish a base model,
    followed by RLHF for enhanced performance. SFT involves imitating high-quality
    demonstration data, while RLHF refines LLMs through preference feedback.'
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 通常，微调包括两个阶段：SFT（监督微调）用于建立基础模型，随后通过RLHF（基于人类反馈的强化学习）提升性能。SFT通过模仿高质量的示范数据，而RLHF通过偏好反馈来精细化大语言模型（LLMs）。
- en: RLHF can be done in two ways, reward-based or reward-free methods. In reward-based
    method, we first train a reward model using preference data. This model then guides
    online Reinforcement Learning algorithms like PPO. Reward-free methods are simpler,
    directly training the models on preference or ranking data to understand what
    humans prefer. Among these reward-free methods, DPO has demonstrated strong performances
    and become popular in the community. Diffusion DPO can be used to steer the model
    away from problematic depictions towards more desirable alternatives. The tricky
    part of this process is not training itself, but data curation. For each risk,
    we need a collection of hundreds or thousands of prompts, and for each prompt,
    a desirable and undesirable image pair. The desirable example should ideally be
    a perfect depiction for that prompt, and the undesirable example should be identical
    to the desirable image, except it should include the risk that we want to unlearn.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF可以通过两种方式进行：基于奖励的方法或无奖励的方法。在基于奖励的方法中，我们首先使用偏好数据训练一个奖励模型。该模型随后指导像PPO这样的在线强化学习算法。无奖励的方法较为简单，直接在偏好或排名数据上训练模型，以了解人类的偏好。在这些无奖励的方法中，DPO展现了强大的表现，并在社区中获得了广泛的关注。扩散DPO可以将模型从有问题的描绘引导到更理想的替代方案。这一过程的难点不在于训练本身，而在于数据的策划。对于每一种风险，我们需要收集数百或数千个提示，对于每个提示，需要有一个理想和一个不理想的图像对。理想的示例应当是该提示的完美描绘，而不理想的示例则应与理想图像相同，但包括我们希望“去除”的风险。
- en: Post-Training mitigations
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 后训练缓解措施
- en: These mitigations are applied after the model is finalized and deployed in the
    production stack. These cover all the mitigations applied on the user input prompt
    and the final image output.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这些缓解措施是在模型完成并部署到生产环境后应用的。这些措施涵盖了对用户输入提示和最终图像输出的所有防范措施。
- en: '**Prompt filtering**'
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**提示过滤**'
- en: When users input a text prompt to generate an image, or upload an image to modify
    it using inpainting technique, filters can be applied to block requests asking
    for harmful content explicitly. At this stage, we address issues where users explicitly
    provide harmful prompts like “*show an image of a person killing another person*”
    or upload an image and ask “*remove this person’s clothing*” and so on.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户输入文本提示以生成图像，或上传图像使用修补技术进行修改时，可以应用过滤器来阻止请求明确要求有害内容的请求。在此阶段，我们处理用户明确提供有害提示的情况，比如“*显示一个人杀另一个人*”的提示，或者上传图像并要求“*去掉这个人的衣服*”等等。
- en: For detecting harmful requests and blocking, we can use a simple blocklist based
    approached with keyword matching, and block all prompts that have a matching harmful
    keyword (say “*suicide*”). However, this approach is brittle, and can produce
    large number of false positives and false negatives. Any obfuscating mechanisms
    (say, users querying for “*suicid3*” instead of “*suicide*”) will fall through
    with this approach. Instead, an embedding-based CNN filter can be used for harmful
    pattern recognition by converting the user prompts into embeddings that capture
    the semantic meaning of the text, and then using a classifier to detect harmful
    patterns within these embeddings. However, LLMs have been proved tobe better for
    harmful pattern recognition in prompts because they excel at understanding context,
    nuance, and intent in a way that simpler models like CNNs may struggle with. They
    provide a more context-aware filtering solution and can adapt to evolving language
    patterns, slang, obfuscating techniques and emerging harmful content more effectively
    than models trained on fixed embeddings. The LLMs can be trained to block any
    defined policy guideline by your organization. Aside from harmful content like
    sexual imagery, violence, self-injury etc., it can also be trained to identify
    and block requests to generate public figures or election misinformation related
    images. To use an LLM based solution at production scale, you’d have to optimize
    for latency and incur the inference cost.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检测有害请求并进行阻止，我们可以使用基于简单黑名单的关键词匹配方法，阻止所有包含有害关键词（比如“*自杀*”）的提示。然而，这种方法是脆弱的，可能会产生大量的假阳性和假阴性结果。任何模糊化机制（例如，用户查询“*suicid3*”而不是“*自杀*”）都会绕过这种方法。相反，可以使用基于嵌入的CNN过滤器来进行有害模式识别，通过将用户提示转换为捕捉文本语义的嵌入向量，然后使用分类器来检测这些嵌入中的有害模式。然而，已经证明，LLM在提示中的有害模式识别上表现更好，因为它们擅长理解上下文、细微差别和意图，而像CNN这样的简单模型可能难以处理。它们提供了一种更具上下文感知的过滤解决方案，并且能比基于固定嵌入的模型更有效地适应不断发展的语言模式、俚语、模糊化技术以及新兴的有害内容。LLM可以根据您的组织定义的任何政策指南进行训练，以屏蔽有害内容。除了像性别暴力、暴力行为、自伤等有害内容外，它还可以训练用于识别并屏蔽生成公众人物或选举相关虚假信息图像的请求。要在生产环境中使用基于LLM的解决方案，您需要优化延迟并承担推理成本。
- en: '**Prompt manipulations**'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**提示操作**'
- en: 'Before passing in the raw user prompt to model for image generation, there
    are several prompt manipulations that can be done for enhancing the safety of
    the prompt. Several case studies are presented below:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在将原始用户提示传递给模型进行图像生成之前，可以进行一些提示操作，以增强提示的安全性。以下列出了一些案例研究：
- en: '**Prompt augmentation to reduce stereotypes**: LDMs amplify dangerous and complex
    stereotypes [5] . A broad range of ordinary prompts produce stereotypes, including
    prompts simply mentioning traits, descriptors, occupations, or objects. For example,
    prompting for basic traits or social roles resulting in images reinforcing whiteness
    as ideal, or prompting for occupations resulting in amplification of racial and
    gender disparities. Prompt engineering to add gender and racial diversity to the
    user prompt is an effective solution. For example, “*image of a ceo” -> “image
    of a ceo, asian woman” or “image of a ceo, black man”* to produce more diverse
    results. This can also help reduce *harmful* stereotypes by transforming prompts
    like “*image of a criminal*” -> “*image of a criminal, olive-skin-tone*” since
    the original prompt would have most likely produced a black man.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示增强以减少刻板印象**：LDMs 扩大了危险和复杂的刻板印象[5]。各种普通提示都会产生刻板印象，包括简单提及特征、描述、职业或物体的提示。例如，提示一些基本特征或社会角色会导致强化白人至上的观念，或者提示职业会导致性别和种族差距的放大。通过对用户提示进行提示工程，增加性别和种族多样性是一个有效的解决方案。例如，“*首席执行官的图像*”
    -> “*首席执行官的图像，亚洲女性*”或“*首席执行官的图像，黑人男性*”可以产生更具多样性的结果。这也有助于减少*有害*的刻板印象，比如将提示“*犯罪分子图像*”变为“*犯罪分子图像，橄榄色肤色*”，因为原始提示最有可能产生一名黑人男性图像。  '
- en: '**Prompt anonymization for privacy**: Additional mitigation can be applied
    at this stage to anonymize or filter out the content in the prompts that ask for
    specific private individuals information. For example “*Image of John Doe from
    <some address> in shower” -> “Image of a person in shower”*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示匿名化以保护隐私**：此阶段还可以应用额外的缓解措施来匿名化或过滤掉提示中请求特定个人隐私信息的内容。例如，“*John Doe在<某地址>淋浴的图像*”
    -> “某人淋浴的图像”'
- en: '**Prompt rewriting and grounding to convert harmful prompt to benign**: Prompts
    can be rewritten or grounded (usually with a fine-tuned LLM) to reframe problematic
    scenarios in a positive or neutral way. For example, *“Show a lazy [ethnic group]
    person taking a nap” -> “Show a person relaxing in the afternoon”.* Defining a
    well-specified prompt, or commonly referred to as grounding the generation, enables
    models to adhere more closely to instructions when generating scenes, thereby
    mitigating certain latent and ungrounded biases. *“Show two people having fun”*
    (This could lead to inappropriate or risky interpretations) -> *“Show two people
    dining at a restaurant”*.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示重写和基准化以将有害提示转化为无害**：可以通过重写或基准化（通常使用微调的LLM）来将有问题的场景以正面或中立的方式重新框架。例如，*“展示一个懒惰的[某族群]人打盹”
    -> “展示一个人在下午休息”*。定义一个明确的提示，或通常所说的“基准化生成”，能够使模型在生成场景时更严格地遵循指令，从而减轻某些潜在和未基准化的偏见。*“展示两个人玩得很开心”*（这可能导致不当或有风险的解释）->
    *“展示两个人在餐厅用餐”*。  '
- en: '**Output image classifiers**'
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**输出图像分类器**  '
- en: Image classifiers can be deployed that detect images produced by the model as
    harmful or not, and may block them before being sent back to the users. Stand
    alone image classifiers like this are effective for blocking images that are visibly
    harmful (showing graphic violence or a sexual content, nudity, etc), However,
    for inpainting based applications where users will upload an input image (e.g.,
    image of a white person) and give a harmful prompt (“*give them blackface*”) to
    transform it in an unsafe manner, the classifiers that only look at output image
    in isolation will not be effective as they lose context of the “transformation”
    itself. For such applications, multimodal classifiers that can consider the input
    image, prompt, and output image together to make a decision of whether a transformation
    of the input to output is safe or not are very effective. Such classifiers can
    also be trained to identify “unintended transformation” e.g., uploading an image
    of a woman and prompting to “*make them beautiful*” leading to an image of a thin,
    blonde white woman.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '可以部署图像分类器来检测模型生成的图像是否有害，并在返回给用户之前进行阻止。像这样的独立图像分类器对于阻止明显有害的图像（如展示暴力或性内容、裸露等）非常有效。然而，对于基于图像修复的应用程序，用户上传输入图像（例如，白人图像）并提供有害提示（“*给他们化黑脸*”）以不安全的方式转换它时，仅仅查看输出图像的分类器将无法有效工作，因为它们失去了“转化”本身的上下文。对于这样的应用，能够同时考虑输入图像、提示和输出图像的多模态分类器，来判断从输入到输出的转换是否安全，效果非常好。这样的分类器还可以训练识别“非预期的转化”，例如，上传一个女性的图像并提示“*让她们变美*”，结果生成了一幅瘦弱、金发、白人的女性图像。  '
- en: Regeneration instead of refusals
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '再生而非拒绝  '
- en: Instead of refusing the output image, models like DALL·E 3 uses classifier guidance
    to improve unsolicited content. A bespoke algorithm based on classifier guidance
    is deployed, and the working is described in [3]—
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 与其拒绝输出图像，像 DALL·E 3 这样的模型使用分类器引导来改善未经请求的内容。基于分类器引导的定制算法被部署，工作原理在[3]中有描述—
- en: When an image output classifier detects a harmful image, the prompt is re-submitted
    to DALL·E 3 with a special flag set. This flag triggers the diffusion sampling
    process to use the harmful content classifier to sample away from images that
    might have triggered it.
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当图像输出分类器检测到有害图像时，提示会带有特殊标志重新提交给 DALL·E 3。这个标志触发扩散采样过程，使用有害内容分类器对可能触发它的图像进行采样。
- en: Basically this algorithm can “nudge” the diffusion model towards more appropriate
    generations. This can be done at both prompt level and image classifier level.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，这个算法可以“推动”扩散模型朝着更合适的生成方向发展。这可以在提示层和图像分类器层面同时进行。
- en: Several additional safety measures are typically implemented in the production
    stack, such as watermarking AI-generated images to trace the content’s origin
    and enable tracking of misuse. These also include comprehensive monitoring and
    reporting systems for users to report incidents, allowing for swift resolution
    of live issues. Serious violations may be disclosed to government authorities
    (such as NCMEC), and penalties for policy breaches, including account disabling,
    are enforced to block risky users. Additionally, rate limiting at the application
    level helps prevent automated or scripted attacks.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中，通常会实施几个额外的安全措施，例如对 AI 生成的图像进行水印标记，以追溯内容的来源并跟踪滥用行为。这些措施还包括全面的监控和报告系统，供用户报告事件，从而迅速解决实时问题。严重违规行为可能会披露给政府机关（如
    NCMEC），并执行对政策违规的惩罚措施，包括禁用账户，以阻止高风险用户。此外，应用层的速率限制有助于防止自动化或脚本攻击。
- en: Risk discovery and assessment
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 风险发现与评估
- en: Aside from the actual mitigations, there are two other important aspects to
    be considered to ensure safety. One of them is **Red-teaming,** where teams actively
    try to find weaknesses, exploits, or unforeseen risks in AI models. Red-teaming
    simulates real-world attacks and emerging risks, either manually, with the help
    of expert human red-teamers from different socio-economic, educational and cultural
    backgrounds or with the help of more scalable, automated systems that are trained
    to “*play the attack*”. The other aspect is **Benchmarking (or evaluations),**
    where models are run against a standardized set of tests or metrics to evaluate
    their performance in predefined areas, such as detecting harmful content, handling
    bias, or maintaining fairness. While red-teaming often uncovers vulnerabilities
    that benchmarking might miss, making it crucial for discovering unknown risk,
    benchmarking provides consistent, repeatable evaluations and helps compare models
    based on established criteria, but may not expose novel risks or vulnerabilities
    outside the benchmark’s scope. Both these are critical for assessing AI system
    safety, but they differ in scope and approach.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 除了实际的缓解措施外，还有两个重要方面需要考虑以确保安全。其中之一是**红队测试**，即团队积极寻找 AI 模型中的弱点、漏洞或不可预见的风险。红队测试模拟现实世界的攻击和新兴风险，既可以通过专家红队成员手动执行，他们来自不同的社会经济、教育和文化背景，也可以通过可扩展的自动化系统来执行，这些系统被训练成“*模拟攻击*”。另一个方面是**基准测试（或评估）**，即模型通过一套标准化的测试或指标来评估其在预定义领域的表现，如检测有害内容、处理偏见或保持公平性。虽然红队测试通常能揭示基准测试可能遗漏的漏洞，从而对发现未知风险至关重要，但基准测试提供一致的、可重复的评估，并帮助根据既定标准比较模型，但可能无法暴露基准测试范围之外的新风险或漏洞。这两个方面对于评估
    AI 系统的安全性至关重要，但在范围和方法上有所不同。
- en: Here’s a sample of a timeline that shows stages at which red-teaming or evaluations
    might be carried out. At the minimum, a red-teaming session is carried out once
    the trained foundation model is ready, to assess the implicit risks in the model.
    Usually you’d uncover the issues where the model is capable of producing harmful
    outputs to benign prompts. After those uncovered risks are mitigated at the fine-tuning
    stage, you’d run comprehensive evaluations to identify any gaps and improve the
    model further before it is finalized for production. Finally, once the model is
    deployed in the production stack, you’d run a red-teaming session on the end-to-end
    system with the entire post-training stack in place, to assess any residual risks
    that are not covered by the current setup and document them to address either
    via quick hotfixing, or a more robust longer term strategy. At this stage, you
    can also run benchmarks to ensure your application meets all the safety, fairness
    and performance standards before being used by real users and can report these
    metrics externally.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个时间线示例，展示了可以进行红队测试或评估的各个阶段。至少，在训练好的基础模型准备好后，应该进行一次红队测试，以评估模型中隐含的风险。通常，你会发现模型能够对无害的提示生成有害输出。在这些隐性风险在微调阶段得到缓解后，你将进行全面评估，以识别任何缺口并进一步改进模型，直到最终定型投入生产。最后，一旦模型部署到生产环境中，你将对整个端到端系统进行红队测试，评估在当前设置下未覆盖的剩余风险，并记录下来以便通过快速热修复或更稳健的长期策略解决。在这个阶段，你还可以进行基准测试，以确保你的应用程序在真实用户使用前符合所有安全性、公平性和性能标准，并能够将这些指标报告给外部。
- en: '![](../Images/d2cf1c9b4c3c0c3ec492e7076650a45e.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2cf1c9b4c3c0c3ec492e7076650a45e.png)'
- en: Red-teaming and evaluation along the safety stack. Image by Author
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 安全栈中的红队测试和评估。图像来源：作者
- en: While this is just a minimum guideline, red-teaming and evaluations can be done
    multiple times across the stack and even on individual components (on just the
    prompt classifier, image classifier or the rewriter) before finalizing them and
    making sure the component has high precision and recall.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这只是一个最低标准指南，但红队测试和评估可以在整个技术栈中多次进行，甚至在单个组件上进行（例如，仅在提示分类器、图像分类器或重写器上）以确保最终组件具有高精度和高召回率。
- en: In conclusion, by implementing comprehensive safeguards throughout the model’s
    lifecycle — from pre-training to post-training, developers can not only reduce
    the risks of AI models generating harmful or biased content, but also prevent
    such content from being surfaced to the end user. Additionally, ongoing practices
    like red teaming and benchmarking throughout the lifecycle are crucial for discovering
    and evaluating vulnerabilities, ensuring that AI systems act safe, fair, and responsible
    in real-world applications.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，通过在模型生命周期的各个阶段实施全面的安全防护——从预训练到后训练，开发者不仅可以减少 AI 模型生成有害或偏见内容的风险，还可以防止这些内容暴露给最终用户。此外，在生命周期中的持续实践，如红队测试和基准测试，对于发现和评估漏洞至关重要，确保
    AI 系统在真实应用中能够安全、公平且负责任地运作。
- en: References
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[Multimodal datasets: misogyny, pornography, and malignant stereotypes](https://arxiv.org/pdf/2110.01963)'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[多模态数据集：厌女症、色情内容和恶性刻板印象](https://arxiv.org/pdf/2110.01963)'
- en: '[DALLE 2 system card](https://arxiv.org/pdf/2110.01963)'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[DALLE 2 系统卡](https://arxiv.org/pdf/2110.01963)'
- en: '[DALLE_3 system card](https://cdn.openai.com/papers/DALL_E_3_System_Card.pdf)'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[DALLE_3 系统卡](https://cdn.openai.com/papers/DALL_E_3_System_Card.pdf)'
- en: '[T-HITL Effectively Addresses Problematic Associations in Image Generation
    and Maintains Overall Visual Quality](https://arxiv.org/pdf/2402.17101)'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[T-HITL 有效解决了图像生成中的有问题联想，并保持整体视觉质量](https://arxiv.org/pdf/2402.17101)'
- en: '[Easily Accessible Text-to-Image Generation Amplifies Demographic Stereotypes
    at Large Scale](https://dl.acm.org/doi/pdf/10.1145/3593013.3594095)'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[轻松访问的文本到图像生成在大规模上加剧了人口统计学上的刻板印象](https://dl.acm.org/doi/pdf/10.1145/3593013.3594095)'
- en: '[Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study](https://arxiv.org/html/2404.10719v3)'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[DPO 是否优于 PPO 进行大规模语言模型对齐？一项综合研究](https://arxiv.org/html/2404.10719v3)'
