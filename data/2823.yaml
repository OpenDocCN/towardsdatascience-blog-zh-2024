- en: Building a Vision Inspection CNN for an Industrial Application
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为工业应用构建视觉检查CNN
- en: 原文：[https://towardsdatascience.com/building-a-vision-inspection-cnn-for-an-industrial-application-138936d7a34a?source=collection_archive---------2-----------------------#2024-11-21](https://towardsdatascience.com/building-a-vision-inspection-cnn-for-an-industrial-application-138936d7a34a?source=collection_archive---------2-----------------------#2024-11-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/building-a-vision-inspection-cnn-for-an-industrial-application-138936d7a34a?source=collection_archive---------2-----------------------#2024-11-21](https://towardsdatascience.com/building-a-vision-inspection-cnn-for-an-industrial-application-138936d7a34a?source=collection_archive---------2-----------------------#2024-11-21)
- en: A step-by-step approach using PyTorch
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用PyTorch的逐步方法
- en: '[](https://medium.com/@ingo.nowitzky?source=post_page---byline--138936d7a34a--------------------------------)[![Ingo
    Nowitzky](../Images/00d3560055109732b871c001d2b51ab5.png)](https://medium.com/@ingo.nowitzky?source=post_page---byline--138936d7a34a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--138936d7a34a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--138936d7a34a--------------------------------)
    [Ingo Nowitzky](https://medium.com/@ingo.nowitzky?source=post_page---byline--138936d7a34a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ingo.nowitzky?source=post_page---byline--138936d7a34a--------------------------------)[![Ingo
    Nowitzky](../Images/00d3560055109732b871c001d2b51ab5.png)](https://medium.com/@ingo.nowitzky?source=post_page---byline--138936d7a34a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--138936d7a34a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--138936d7a34a--------------------------------)
    [Ingo Nowitzky](https://medium.com/@ingo.nowitzky?source=post_page---byline--138936d7a34a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--138936d7a34a--------------------------------)
    ·28 min read·Nov 21, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--138936d7a34a--------------------------------)
    ·28分钟阅读·2024年11月21日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '**In this article, we develop and code a Convolutional Neural Network (CNN)
    for a vision inspection classification task in the automotive electronics industry.
    Along the way, we study the concept and math of convolutional layers in depth,
    and we examine what CNNs actually see and which parts of the image lead them to
    their decisions.**'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**在本文中，我们为汽车电子行业的视觉检查分类任务开发并编码了一个卷积神经网络（CNN）。在此过程中，我们深入研究了卷积层的概念和数学，并分析了CNN实际“看到”的内容以及哪些图像部分引导它们做出决策。**'
- en: Table of Content
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目录
- en: 'Part 1: Conceptual background'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分：概念背景
- en: 'Part 2: Defining and coding the CNN'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 第二部分：定义和编码CNN
- en: 'Part 3: Using the trained model in production'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 第三部分：在生产中使用训练好的模型
- en: 'Part 4: What did the CNN consider in its “decision”?'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 第四部分：CNN在“决策”中考虑了什么？
- en: 'Part 1: Conceptual background'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一部分：概念背景
- en: '1.1 The task: Classify an industrial component as good or scrap'
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 任务：将工业部件分类为良品或废品
- en: In one station of an automatic assembly line, coils with two protruding metal
    pins have to be positioned precisely in a housing. The metal pins are inserted
    into small plug sockets. In some cases, the pins are slightly bent and therefore
    cannot be joined by a machine. It is the task of the visual inspection to identify
    these coils, so that they can be sorted out automatically.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动化装配线的一个工位上，带有两个突出金属销的线圈需要精确地放置在外壳中。金属销插入小的插座中。在某些情况下，销钉略微弯曲，因此无法通过机器连接。视觉检查的任务是识别这些线圈，以便它们可以被自动筛选出来。
- en: '![](../Images/5638395e5b713df5831c414d81e1e145.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5638395e5b713df5831c414d81e1e145.png)'
- en: 'Fig. 1: Coils, housing and sockets | image by author'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：线圈、外壳和插座 | 图像来源：作者
- en: For the inspection, each coil is picked up individually and held in front of
    a screen. In this position, a camera takes a grayscale image. This is then examined
    by the CNN and classified as good or scrap.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对于检查，每个线圈会单独拾起，并放置在屏幕前。在这个位置，摄像机会拍摄一张灰度图像。然后，这张图像会被CNN检查并分类为良品或废品。
- en: '![](../Images/f746b16fb6a040d9c1429f3f50dba39a.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f746b16fb6a040d9c1429f3f50dba39a.png)'
- en: 'Fig. 2: Basic setup of visual inspection and resulting image | image by author'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：视觉检查的基本设置和生成的图像 | 图像来源：作者
- en: Now, we want to define a convolutional neural network that is able to process
    the images and learn from pre-classified labels.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们想要定义一个卷积神经网络，能够处理图像并从预先分类的标签中学习。
- en: 1.2 What is a Convolutional Neural Network (CNN)?
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 什么是卷积神经网络（CNN）？
- en: Convolutional Neural Networks are a combination of **convolutional filters**
    followed by a **fully connected Neural Network** (NN). CNNs are often used for
    **image processing,** like face recognition or visual inspection tasks, like in
    our case. **Convolutional filters** are matrix operations that slide over the
    images and recalculate each pixel of the image. We will study convolutional filters
    later in the article. The **weights** of the filters are **not preset** (as, e.g.
    the sharpen function in Photoshop) but instead are learned from the data during
    training.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络是 **卷积滤波器** 和 **全连接神经网络**（NN）的组合。CNN 常用于 **图像处理**，如面部识别或视觉检测任务，像我们这个案例中的任务。**卷积滤波器**
    是滑过图像并重新计算每个像素的矩阵操作。我们将在本文后面研究卷积滤波器。**滤波器的权重**是 **不是预设的**（例如 Photoshop 中的锐化功能），而是从数据中学习得到的。
- en: 1.3 Architecture of a Convolutional Neural Network
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 卷积神经网络的架构
- en: Let’s check an example of the architecture of a CNN. For our convenience, we
    choose the model **we will implement** later.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下 CNN 架构的一个例子。为了方便起见，我们选择 **稍后将实现**的模型。
- en: '![](../Images/851e467913c49ffafea1c1dee93d1985.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/851e467913c49ffafea1c1dee93d1985.png)'
- en: 'Fig. 3: Architecture of our vision inspection CNN | image by author'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：我们的视觉检测 CNN 架构 | 图片来自作者
- en: 'We want to feed the CNN with our inspection images of size 400 px in height
    and 700 px in width. Since the images are grayscale, the corresponding PyTorch
    tensor is of size 1x400x700\. If we used a colored image, we would have 3 incoming
    channels: one for red, one for green and one for blue (RGB). In this case the
    tensor would be **3**x400x700.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望将大小为 400 像素高和 700 像素宽的检测图像输入到 CNN 中。由于图像是灰度图像，相关的 PyTorch 张量大小为 1x400x700。如果使用彩色图像，则会有
    3 个输入通道：一个用于红色，一个用于绿色，一个用于蓝色（RGB）。在这种情况下，张量的大小将是 **3**x400x700。
- en: The first **convolutional filter** has 6 **kernels** of size 5x5 that slide
    over the image and produce 6 independent new images, called **feature maps,**
    of slightly reduced size (6x396x696). The **ReLU activation** is not explicitly
    shown in Fig. 3\. It does not change the dimensions of the tensors but sets all
    negative values to zero. ReLU is followed by the **MaxPooling** layer with a kernel
    size of 2x2\. It halves the width and height of each image.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个 **卷积滤波器** 有 6 个大小为 5x5 的 **卷积核**，它们滑过图像并生成 6 个独立的新图像，这些图像被称为 **特征图**，大小略微减小（6x396x696）。**ReLU
    激活函数** 在图 3 中没有明确显示。它不会改变张量的维度，但会将所有负值设置为零。ReLU 后面是 **MaxPooling** 层，卷积核大小为 2x2。它会将每个图像的宽度和高度减半。
- en: 'All three layers — convolution, ReLU, and MaxPooling — are implemented a second
    time. This finally brings us 16 feature maps with images of height 97 pixels and
    width 172 pixels. Next, all the matrix values are flattened and fed into the equally
    sized first layer of a fully connected neural network. Its second layer is already
    reduced to 120 neurons. The third and output layer has only 2 neurons: one represents
    the label “OK”, and the other the label “not OK” or “scrap”.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这三层——卷积层、ReLU 层和 MaxPooling 层——会再次实现。这最终给我们带来 16 个特征图，图像的高度为 97 像素，宽度为 172 像素。接下来，所有矩阵值会被展平，并输入到同样大小的全连接神经网络的第一层。它的第二层已经缩减为
    120 个神经元。第三层和输出层只有 2 个神经元：一个表示标签“OK”，另一个表示标签“not OK” 或“scrap”。
- en: '**If you are not yet clear about the changes in the dimensions, please be patient.**
    We study how the different kinds of layers — convolution, ReLU, and MaxPooling
    — work in detail and impact the tensor dimensions in the next chapters.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果你还不清楚维度变化的情况，请耐心等待。** 我们将在接下来的章节中详细研究不同类型的层——卷积层、ReLU 层和 MaxPooling 层——如何工作，并如何影响张量维度。'
- en: 1.4 Convolutional filter layers
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4 卷积滤波器层
- en: Convolutional filters have the task of finding **typical structures/patterns**
    in an image. Frequently used kernel sizes are 3x3 or 5x5\. The 9, respectively
    25, weights of the kernel are not specified upfront but learned during training
    (here we assume that we have only one input channel; otherwise, the number of
    weights multiply by input channels). The kernels slide over the matrix representation
    of the image (each input channel has its own kernel) with a defined **stride**
    in the horizontal and vertical directions. The corresponding values of the kernel
    and the matrix are multiplied and summed up. The summation results of each sliding
    position form the new image, which we call the **feature map**. We can specify
    multiple kernels in a convolutional layer. In this case, we receive multiple feature
    maps as the result. The kernel slides over the matrix from left to right and top
    to bottom. Therefore, Fig. 4 shows the kernel in its **fifth sliding position**
    (not counting the “…”). We see three input channels for the colors red, green,
    and blue (RGB). Each channel has one kernel only. In real applications, we often
    define multiple kernels per input channel.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积滤波器的任务是寻找图像中的**典型结构/模式**。常用的卷积核尺寸是3x3或5x5。卷积核的9个或25个权重并不是预先指定的，而是在训练过程中学习的（这里假设只有一个输入通道；否则，权重的数量会乘以输入通道数）。卷积核会以定义的**步幅**在图像的矩阵表示上滑动（每个输入通道都有自己的卷积核），在水平和垂直方向上进行卷积。卷积核与矩阵中对应的值相乘并求和。每个滑动位置的求和结果形成新的图像，我们称之为**特征图**。在一个卷积层中，我们可以指定多个卷积核。在这种情况下，我们会得到多个特征图作为结果。卷积核从左到右、从上到下滑动。因此，图4显示了卷积核在**第五个滑动位置**（不包括“...”）的状态。我们可以看到三个输入通道，分别表示红色、绿色和蓝色（RGB）。每个通道只有一个卷积核。在实际应用中，我们通常为每个输入通道定义多个卷积核。
- en: '![](../Images/c717dce253abdcbf436fbd671ddd02d6.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c717dce253abdcbf436fbd671ddd02d6.png)'
- en: 'Fig. 4: Convolutional layer with 3 input channels and 1 kernel per channel
    | image by author'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：具有3个输入通道，每个通道1个卷积核的卷积层 | 图片来源：作者
- en: 'Kernel 1 does its work for the red input channel. In the shown position, we
    compute the respective new value in the feature map as *(-0.7)*0 + (-0.9)*(-0.2)
    + (-0.6)*0.5 + (-0.6)*0.6 + 0.6*(-0.3) + 0.7*(-1) + 0*0.7 + (-0.1)*(-0.1) + (-0.2)*(-0.1)
    = (-1.33).* The respective calculation for the green channel (kernel 2) adds up
    to *-0.14,* and for the blue channel (kernel 3) to *0.69*. To receive the final
    value in the feature map for this specific sliding position, we sum up all three
    channel values and add a bias (bias and all kernel weights are defined during
    training of the CNN): *(-1.33) + (-0.14) + 0.69 + 0.2 = -0.58*. The value is placed
    in the position of the feature map highlighted in yellow.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积核1在红色输入通道上执行其操作。在当前显示的位置，我们计算该位置在特征图中的新值，计算过程为 *(-0.7)*0 + (-0.9)*(-0.2) +
    (-0.6)*0.5 + (-0.6)*0.6 + 0.6*(-0.3) + 0.7*(-1) + 0*0.7 + (-0.1)*(-0.1) + (-0.2)*(-0.1)
    = (-1.33).* 对应的绿色通道（卷积核2）的计算结果为 *-0.14,* 蓝色通道（卷积核3）的结果为 *0.69*。为了得到该滑动位置的最终特征图值，我们将三个通道的值相加并加上偏置（偏置和所有卷积核权重在CNN训练过程中定义）：
    *(-1.33) + (-0.14) + 0.69 + 0.2 = -0.58*。该值被放置在特征图中黄色高亮的对应位置。
- en: '**Finally, if we compare the size of the input matrices to the size of the
    feature map, we see that through the kernel operations, we lost two rows in height
    and two columns in width.**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**最后，如果我们将输入矩阵的大小与特征图的大小进行比较，会发现通过卷积操作后，我们丢失了两行和两列。**'
- en: 1.5 ReLU activation layers
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.5 ReLU激活层
- en: After the convolution, the feature maps are passed through the activation layer.
    Activation is required to give the network **nonlinear capabilities**. The two
    most frequently used activation methods are **Sigmoid** and **ReLU** (Rectified
    Linear Unit). ReLU activation sets all negative values to zero while leaving positive
    values unchanged.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积之后，特征图会通过激活层。激活是必需的，目的是赋予网络**非线性能力**。最常用的激活方法是**Sigmoid**和**ReLU**（修正线性单元）。ReLU激活将所有负值设为零，而正值保持不变。
- en: '![](../Images/bc0f10b35021ae5d53fa77063e80234c.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc0f10b35021ae5d53fa77063e80234c.png)'
- en: 'Fig. 5: ReLU activation of the feature map | image by author'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：特征图的ReLU激活 | 图片来源：作者
- en: In Fig. 5, we see that the values of the feature map pass the ReLU activation
    element-wise.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在图5中，我们可以看到特征图的值逐元素通过了ReLU激活。
- en: '**ReLU activation has no impact on the dimensions of the feature map.**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**ReLU激活对特征图的维度没有影响。**'
- en: 1.6 MaxPooling layers
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.6 最大池化层
- en: Pooling layers have mainly the task of reducing the size of the feature maps
    while keeping the important information for the classification. In general, we
    can pool by calculating the **average** of an area in the kernel or returning
    the **maximum**. MaxPooling is more beneficial in most applications because it
    **reduces the noise** in the data. Typical kernel sizes for pooling are 2x2 or
    3x3.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层的主要任务是减少特征图的大小，同时保留对分类重要的信息。通常，我们可以通过计算卷积核区域的**平均值**或返回**最大值**来进行池化。在大多数应用中，MaxPooling更有益，因为它**减少了数据中的噪声**。池化的典型卷积核大小为2x2或3x3。
- en: '![](../Images/2210088b4c87a939a7793bfacc32185e.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2210088b4c87a939a7793bfacc32185e.png)'
- en: 'Fig. 6: Max Pooling and Average Pooling with a 2x2 kernel | image by author'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：使用2x2卷积核的MaxPooling和AvgPooling | 图像来自作者
- en: In Fig. 6, we see an example of MaxPooling and AvgPooling with a kernel size
    of 2x2\. The feature map is divided into areas of the kernel size, and within
    those areas, we take either the maximum (→ MaxPooling) or the average (→ AvgPooling).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在图6中，我们看到使用2x2卷积核的MaxPooling和AvgPooling的示例。特征图被划分为与卷积核大小相同的区域，在这些区域内，我们选择最大值（→
    MaxPooling）或平均值（→ AvgPooling）。
- en: '**Through pooling with a kernel size of 2x2, we halve the height and width
    of the feature map.**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**通过使用2x2的卷积核进行池化，我们将特征图的高度和宽度减半。**'
- en: 1.7 Tensor dimensions in the Convolutional Neural Network
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.7 卷积神经网络中的张量维度
- en: Now that we have studied the convolutional filters, the ReLU activation, and
    the pooling, we can revise Fig. 3 and the dimensions of the tensors. We start
    with an image of size 400x700\. Since it is grayscale, it has only 1 channel,
    and the corresponding tensor is of size 1x400x700\. We apply 6 convolutional filters
    of size 5x5 with a stride of 1x1 to the image. Each filter returns its own feature
    map, so we receive 6 of them. Due to the larger kernel compared to Fig. 4 (5x5
    instead of 3x3), this time we lose 4 columns and 4 rows in the convolution. This
    means the returning tensor has the size 6x396x696.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经研究了卷积滤波器、ReLU激活函数和池化，我们可以回顾一下图3及张量的维度。我们从一个大小为400x700的图像开始。由于它是灰度图像，所以只有1个通道，相应的张量大小为1x400x700。我们对图像应用6个大小为5x5、步长为1x1的卷积滤波器。每个滤波器返回自己的特征图，因此我们获得6个特征图。由于与图4中使用的卷积核相比（5x5代替了3x3），这次卷积会丢失4列和4行。因此，返回的张量大小为6x396x696。
- en: In the next step, we apply MaxPooling with a 2x2 kernel to the feature maps
    (each map has its own pooling kernel). As we have learned, this reduces the maps’
    dimensions by a factor of 2\. Accordingly, the tensor is now of size 6x198x348.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们对特征图应用2x2卷积核的MaxPooling（每个图都有自己的池化核）。正如我们所学，这将特征图的维度缩小一倍。因此，张量现在的大小为6x198x348。
- en: Now we apply 16 convolutional filters of size 5x5\. Each of them has a kernel
    depth of 6, which means that each filter provides a separate layer for the 6 channels
    of the input tensor. Each kernel layer slides over one of the 6 input channels,
    as studied in Fig. 4, and the 6 returning feature maps are added up to one. So
    far, we considered only one convolutional filter, but we have 16 of them. That
    is why we receive 16 new feature maps, each 4 columns and 4 rows smaller than
    the input. The tensor size is now 16x194x344.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们应用16个大小为5x5的卷积滤波器。每个滤波器的内核深度为6，这意味着每个滤波器为输入张量的6个通道提供一个单独的层。每个卷积核层在6个输入通道中的一个上滑动，如图4所示，6个返回的特征图相加合成一个。因此，到目前为止，我们只考虑了一个卷积滤波器，但实际上我们有16个滤波器。这就是为什么我们得到16个新的特征图，每个比输入小4列和4行。此时张量的大小是16x194x344。
- en: Once more, we apply MaxPooling with a kernel size of 2x2\. Since this halves
    the feature maps, we now have a tensor size of 16x97x172.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们应用2x2卷积核的MaxPooling。由于这会将特征图的大小减半，所以现在我们得到的张量大小为16x97x172。
- en: Finally, the tensor is flattened, which means we line up all of the *16*97*172
    = 266,944* values and feed them into a fully connected neural network of corresponding
    size.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，张量被展平，这意味着我们将所有*16*97*172 = 266,944*个值排列成一行，并将它们输入到一个相应大小的全连接神经网络中。
- en: 'Part 2: Defining and coding the CNN'
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分：定义和编码CNN
- en: Conceptually, we have everything we need. Now, let’s go into the industrial
    use case as described in chapter 1.1.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，我们已经具备了所需的一切。接下来，让我们进入第1.1章中描述的工业应用案例。
- en: 2.1 Load the required libraries
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 加载所需的库
- en: We are going to use a couple of PyTorch libraries for data loading, sampling,
    and the model itself. Additionally, we load `matplotlib.pyplot` for visualization
    and `PIL` for transforming the images.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一些PyTorch库来进行数据加载、采样以及模型本身的构建。此外，我们还加载`matplotlib.pyplot`用于可视化，`PIL`用于转换图像。
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 2.2 Configure your device and specify hyperparameters
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 配置设备并指定超参数
- en: In `device`, we store `‘cuda’` or `‘cpu’`, depending on whether or not your
    computer has a GPU available. `minibatch_size` defines how many images will be
    processed in one matrix operation during the training of the model. `learning_rate`
    specifies the magnitude of parameter adjustment during backpropagation, and `epochs`
    defines how often we process the whole set of training data in the training phase.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在`device`中，我们存储`‘cuda’`或`‘cpu’`，具体取决于你的计算机是否有可用的GPU。`minibatch_size`定义了在模型训练过程中每次矩阵操作处理的图像数量。`learning_rate`指定了反向传播过程中参数调整的幅度，而`epochs`定义了我们在训练阶段处理整个训练数据集的频率。
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 2.3 Custom loader function
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 自定义加载器函数
- en: For loading the images, we define a `custom_loader`. It opens the images in
    binary mode, crops the inner 700x400 pixels of the image, loads them into memory,
    and returns the loaded images. As the path to the images, we define the relative
    path `data/Coil_Vision/01_train_val_test`. Please make sure that the data is stored
    in your working directory. You can download the files from my Dropbox as [CNN_data.zip](https://www.dropbox.com/scl/fi/z8kkui6ync57rudlx10dx/CNN_data.zip?rlkey=27787pjnnbaa3mss5nu4nbi7u&st=4q3p56fh&dl=0).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加载图像，我们定义了一个`custom_loader`。它以二进制模式打开图像，裁剪图像的内部700x400像素，将其加载到内存中，并返回加载的图像。作为图像的路径，我们定义了相对路径`data/Coil_Vision/01_train_val_test`。请确保数据存储在你的工作目录中。你可以从我的Dropbox下载文件：[CNN_data.zip](https://www.dropbox.com/scl/fi/z8kkui6ync57rudlx10dx/CNN_data.zip?rlkey=27787pjnnbaa3mss5nu4nbi7u&st=4q3p56fh&dl=0)。
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 2.4 Define the datasets
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 定义数据集
- en: We define the dataset as tuples consisting of the image data and the label,
    either *0* for scrap parts and *1* for good parts. The method `datasets.ImageFolder()`
    reads the labels out of the folder structure. We use a transform function to first
    load the image data to a PyTorch tensor (values between 0 and 1) and, second,
    normalize the data with the approximate mean of 0.5 and standard deviation of
    0.5\. After the transformation, the image data is roughly standard normal distributed
    (mean = 0, standard deviation = 1). We split the dataset randomly into 50% training
    data, 30% validation data, and 20% testing data.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据集定义为由图像数据和标签组成的元组，标签为*0*表示废品，*1*表示合格品。方法`datasets.ImageFolder()`从文件夹结构中读取标签。我们使用变换函数首先将图像数据加载为PyTorch张量（值介于0和1之间），然后用大约0.5的均值和0.5的标准差对数据进行归一化。经过变换后，图像数据大致呈标准正态分布（均值=0，标准差=1）。我们将数据集随机划分为50%的训练数据、30%的验证数据和20%的测试数据。
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 2.5 Balance the datasets
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 平衡数据集
- en: Our data is unbalanced. We have far more good samples than scrap samples. To
    reduce a bias towards the majority class during training, we use a `WeightedRandomSampler`
    to give higher probability to the minority class during sampling. In `lbls`, we
    store the labels of the training dataset. With `np.bincount()`, we count the number
    of *0* labels (`bc[0]`) and *1* labels (`bc[1]`). Next, we calculate probability
    weights for the two classes (`p_nOK` and `p_OK`) and arrange them according to
    the sequence in the dataset in the list `lst_train`. Finally, we instantiate `train_sampler`
    from `WeightedRandomSampler`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据是不平衡的。合格样本远多于废品样本。为了减少训练过程中对多数类的偏倚，我们使用`WeightedRandomSampler`在采样时给少数类分配更高的概率。在`lbls`中，我们存储了训练数据集的标签。使用`np.bincount()`，我们统计*0*标签（`bc[0]`）和*1*标签（`bc[1]`）的数量。接下来，我们计算两个类的概率权重（`p_nOK`和`p_OK`），并按照数据集中的顺序将它们安排在列表`lst_train`中。最后，我们从`WeightedRandomSampler`实例化`train_sampler`。
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 2.6 Define the data loaders
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.6 定义数据加载器
- en: Lastly, we define three data loaders for the training, the validation, and the
    testing data. Data loaders feed the neural network with batches of datasets, each
    consisting of the image data and the label.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们为训练、验证和测试数据定义了三个数据加载器。数据加载器按批次将数据集输入到神经网络中，每个批次包括图像数据和标签。
- en: For the `train_loader` and the `val_loader`, we set the batch size to 10 and
    shuffle the data. The `test_loader` operates with shuffled data and a batch size
    of 1.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`train_loader`和`val_loader`，我们将批次大小设置为10，并打乱数据。`test_loader`使用打乱的数据和批次大小为1。
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '2.7 Check the data: Plot 5 OK and 5 nOK parts'
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.7 检查数据：绘制5个合格品和5个废品
- en: To inspect the image data, we plot five good samples (“OK”) and five scrap samples
    (“nOK”). To do this, we define a `matplotlib` figure with 2 rows and 5 columns
    and share the x- and the y-axis. In the core of the code snippet, we nest two
    for-loops. The outer loop receives batches of data from the `train_loader`. Each
    batch consists of ten images and the corresponding labels. The inner loop enumerates
    the batches’ labels. In its body, we check if the label equals *0* — then we plot
    the image under “nOK” in the second row — or if the label equals *1* — then we
    plot the image under “OK” in the first row. Once `count_OK` and `count_nOK` both
    are greater or equal 5, we break the loop, set the title, and show the figure.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查图像数据，我们绘制了五个正常样本（“OK”）和五个废料样本（“nOK”）。为此，我们定义了一个`matplotlib`图形，包含2行5列，并共享x轴和y轴。在代码核心部分，我们嵌套了两个for循环。外层循环从`train_loader`接收数据批次。每个批次包含十张图片和相应的标签。内层循环枚举批次标签。在其主体中，我们检查标签是否等于*0*——如果是，则将图像绘制在第二行的“nOK”下——或者标签是否等于*1*——如果是，则将图像绘制在第一行的“OK”下。一旦`count_OK`和`count_nOK`都大于或等于5，我们就跳出循环，设置标题并显示图形。
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/085c2afc94e541383abe19d7c03f1578.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/085c2afc94e541383abe19d7c03f1578.png)'
- en: 'Fig. 7: Example for OK (upper row) and nonOK parts (lower row) | image by author'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：OK（上排）和非OK（下排）部分的示例 | 图片由作者提供
- en: In Fig. 7, we see that most of the nOK samples are clearly bent, but a few are
    not really distinguishable by eye (e.g., lower right sample).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在图7中，我们看到大多数nOK样本明显弯曲，但有少数样本肉眼难以区分（例如，右下角的样本）。
- en: 2.8 Define the CNN model
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.8 定义CNN模型
- en: The model corresponds to the architecture depicted in Fig. 3\. We feed the grayscale
    image (only one channel) into the first convolutional layer and define 6 kernels
    of size 5 (equals 5x5). The convolution is followed by a ReLU activation and a
    MaxPooling with a kernel size of 2 (2x2) and a stride of 2 (2x2). All three operations
    are repeated with the dimensions shown in Fig. 3\. In the final block of the `__init__()`
    method, the 16 feature maps are flattened and fed into a linear layer of equivalent
    input size and 120 output nodes. It is ReLU activated and reduced to only 2 output
    nodes in a second linear layer.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型对应于图3中所示的架构。我们将灰度图像（只有一个通道）输入到第一层卷积层，并定义6个大小为5（即5x5）的卷积核。卷积操作后接ReLU激活和一个大小为2（2x2）的最大池化层，步幅为2（2x2）。所有这三项操作都按照图3中显示的维度进行重复。在`__init__()`方法的最后一个模块中，16个特征图被展平，并输入到一个大小相等的线性层，该层有120个输出节点。它经过ReLU激活，并在第二个线性层中被缩减至仅2个输出节点。
- en: In the `forward()` method, we simply call the model layers and feed in the `x`
    tensor.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在`forward()`方法中，我们简单地调用模型的各个层，并将`x`张量输入其中。
- en: '[PRE7]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 2.9 Instantiate the model and define the loss function and the optimizer
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.9 实例化模型并定义损失函数和优化器
- en: We instantiate `model` from the CNN class and push it either on the CPU or on
    the GPU. Since we have a classification task, we choose the CrossEntropyLoss function.
    For managing the training process, we call the Stochastic Gradient Descent (SGD)
    optimizer.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从CNN类实例化`model`，并将其放置在CPU或GPU上。由于我们进行的是分类任务，我们选择了CrossEntropyLoss函数。为了管理训练过程，我们调用了随机梯度下降（SGD）优化器。
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 2.10 Check the model’s size
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.10 检查模型的大小
- en: To get an idea of our model’s size in terms of parameters, we iterate over `model.parameters()`
    and sum up, first, all model parameters (`num_param`) and, second, those parameters
    that will be adjusted during backpropagation (`num_param_trainable`). Finally,
    we print the result.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解模型的参数规模，我们遍历`model.parameters()`并进行求和，首先求出所有模型参数的总和（`num_param`），然后是那些在反向传播过程中会被调整的参数（`num_param_trainable`）。最后，我们打印结果。
- en: '[PRE9]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/70cbef867acfafd919547b274f3ec920.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/70cbef867acfafd919547b274f3ec920.png)'
- en: The print out tells us that the model has more than 32 million parameters, thereof
    all trainable.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 打印输出告诉我们，模型有超过3200万个参数，其中所有参数都是可训练的。
- en: 2.11 Define a function for validation and testing
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.11 定义用于验证和测试的函数
- en: Before we start the model training, let’s prepare a function to support the
    validation and testing. The function `val_test()` expects a `dataloader` and the
    CNN `model` as parameters. It turns off the gradient calculation with `torch.no_grad()`
    and iterates over the `dataloader`. With one batch of images and labels at hand,
    it inputs the images into the `model` and determines the model’s predicted classes
    with `output.argmax(1)` over the returned logits. This method returns the indices
    of the largest values; in our case, this represents the class indices.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始模型训练之前，让我们准备一个函数来支持验证和测试。函数`val_test()`将`dataloader`和CNN `model`作为参数。它通过`torch.no_grad()`关闭梯度计算，并遍历`dataloader`。每次拿到一批图像和标签后，将图像输入到`model`中，并通过`output.argmax(1)`在返回的logits上确定模型预测的类别。该方法返回最大值的索引，在我们的例子中，代表类别的索引。
- en: We count and sum up the correct predictions and save the image data, the predicted
    class, and the labels of the wrong predictions. Finally, we calculate the accuracy
    and return it together with the misclassified images as the function’s output.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们统计并汇总正确的预测结果，同时保存图像数据、预测类别和错误预测的标签。最后，我们计算准确率，并将其与误分类的图像一并作为函数的输出返回。
- en: '[PRE10]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 2.12 Model training
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.12 模型训练
- en: The model training consists of two nested for-loops. The outer loop iterates
    over a defined number of `epochs`, and the inner loop enumerates the `train_loader`.
    The enumeration returns a batch of image data and the corresponding labels. The
    image data (`images`) is passed to the model, and we receive the model’s response
    logits in `outputs`. `outputs` and the true `labels` are passed to the loss function.
    Based on loss `l`, we perform backpropagation and update the parameter with `optimizer.step`.
    `outputs` is a tensor of dimension *batchsize x output nodes*, in our case *10
    x 2*. We receive the model’s prediction through the indices of the max values
    over the rows, either *0* or *1*.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练包含两个嵌套的for循环。外层循环遍历预定义的`epochs`次数，内层循环遍历`train_loader`。枚举返回一批图像数据及其对应的标签。图像数据（`images`）传递给模型，我们接收模型的输出logits（`outputs`）。`outputs`和真实的`labels`一起传入损失函数。根据损失`l`，我们执行反向传播，并通过`optimizer.step`更新参数。`outputs`是一个维度为*batchsize
    x output nodes*的张量，在我们这里是*10 x 2*。我们通过行中最大值的索引来接收模型的预测，值为*0*或*1*。
- en: Finally, we count the number of correct predictions (`n_correct`), the true
    OK parts (`n_true_OK`), and the number of samples (`n_samples`). Each second epoch,
    we calculate the training accuracy, the true OK share, and call the validation
    function (`val_test()`). All three values are printed for information purpose
    during the training run. With the last line of code, we save the model with all
    its parameters in `“model.pth”`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们统计正确预测的数量（`n_correct`）、正确的OK部分（`n_true_OK`）和样本的数量（`n_samples`）。每隔一个epoch，我们计算训练准确率、正确OK部分的比例，并调用验证函数（`val_test()`）。训练过程中，每个epoch的这三个值都会打印出来供参考。在最后一行代码中，我们将模型及其所有参数保存在`“model.pth”`文件中。
- en: '[PRE11]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/a702275ce6449b59a8c30efeb185e881.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a702275ce6449b59a8c30efeb185e881.png)'
- en: Training takes a couple of minutes on the GPU of my laptop. It is highly recommended
    to load the images from the local drive. Otherwise, training time might increase
    by orders of magnitude!
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 训练在我笔记本的GPU上只需几分钟。强烈建议从本地驱动加载图像，否则训练时间可能会增加几个数量级！
- en: The printouts from training inform that the loss has reduced significantly,
    and the validation accuracy — the accuracy on data the model has not used for
    updating its parameters — has reached 98.4%.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的输出表明损失已经显著减少，验证准确率——即模型未用于更新参数的数据上的准确率——已达到98.4%。
- en: An even better impression on the training progress is obtained if we plot the
    training and validation accuracy over the epochs. We can easily do this because
    we saved the values each second epoch.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们绘制训练过程中每个epoch的训练和验证准确率，将能更好地了解训练进展。由于我们每隔一个epoch就保存一次值，因此可以轻松地做到这一点。
- en: We create a `matplotlib` figure and axes with `plt.subplots()` and plot the
    values over the keys of the accuracy dictionaries.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个`matplotlib`图形和坐标轴，使用`plt.subplots()`并根据准确率字典的键绘制这些值。
- en: '[PRE12]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/9cca05ab59a7fc07400f9869927832f6.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9cca05ab59a7fc07400f9869927832f6.png)'
- en: 'Fig. 8: Training and validation accuracy during model training | image by author'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：模型训练期间的训练和验证准确率 | 图片由作者提供
- en: 2.13 Loading the trained model
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.13 加载训练好的模型
- en: If you want to use the model for production and not only for study purpose,
    it is highly recommended to save and load the model with all its parameters. Saving
    was already part of the training code. Loading the model from your drive is equally
    simple.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想将模型用于生产而不仅仅是用于学习，强烈建议保存并加载包含所有参数的模型。保存已经是训练代码的一部分，从你的驱动器加载模型同样简单。
- en: '[PRE13]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 2.14 Double-check the model accuracy with test data
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.14 使用测试数据再次检查模型准确度
- en: Remember, we reserved another 20% of our data for testing. This data is totally
    new to the model and has never been loaded before. We can use this brand-new data
    to double-check the validation accuracy. Since the validation data has been loaded
    but never been used to update the model parameters, we expect a similar accuracy
    to the test value. To conduct the test, we call the `val_test()` function on the
    `test_loader`.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，我们保留了20%的数据用于测试。这些数据对模型来说是全新的，从未加载过。我们可以使用这组全新的数据来再次验证验证准确率。由于验证数据已经加载，但从未用于更新模型参数，我们预期其准确率与测试值相似。为了进行测试，我们在`test_loader`上调用`val_test()`函数。
- en: '[PRE14]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](../Images/f93ebfbd16df5d984332370c25cf3a6c.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f93ebfbd16df5d984332370c25cf3a6c.png)'
- en: 'In the specific example, we reach a test accuracy of 99.2%, but this is highly
    dependent on chance (remember: random distribution of images to training, validation,
    and testing data).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定的例子中，我们达到了99.2%的测试准确度，但这很大程度上取决于随机性（记住：图像随机分配到训练、验证和测试数据中）。
- en: 2.15 Visualizes the misclassified images
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.15 可视化错误分类的图像
- en: The visualization of the misclassified images is pretty straightforward. First,
    we call the `val_test()` function. It returns a tuple with the accuracy value
    at index position *0* (`tup[0]`) and another tuple at index position *1* (`tup[1]`)
    with the image data (`tup[1][0]`), the predicted labels (`tup[1][1]`), and the
    true labels (`tup[1][2]`) of the misclassified images. In case `tup[1]` is not
    empty, we enumerate it and plot the misclassified images with appropriate headings.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 错误分类图像的可视化非常直接。首先，我们调用`val_test()`函数。它返回一个元组，第一个索引位置是准确度值 *0*（`tup[0]`），第二个索引位置是另一个元组（`tup[1]`），包含错误分类图像的数据（`tup[1][0]`）、预测标签（`tup[1][1]`）和真实标签（`tup[1][2]`）。如果`tup[1]`不为空，我们会枚举它，并绘制错误分类图像，并加上适当的标题。
- en: '[PRE15]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In our example, we have only one misclassified image, which represents 0.8%
    of the test dataset (we have 125 test images). The image was classified as OK
    but has the label nOK. Frankly, I would have misclassified it too :).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，只有一张错误分类的图像，它占测试数据集的0.8%（我们有125张测试图像）。这张图像被分类为合格，但标签为nOK。坦白说，我也会错误分类它
    :).
- en: '![](../Images/1dded0c32adb73555618ffa75c2b3bdc.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1dded0c32adb73555618ffa75c2b3bdc.png)'
- en: 'Fig. 9: Misclassified image | image by author'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：错误分类的图像 | 图片来自作者
- en: 'Part 3: Using the trained model in production'
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三部分：在生产中使用训练好的模型
- en: 3.1 Loading the model, required libraries, and parameters
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 加载模型、所需库和参数
- en: 'In the production phase, we assume that the CNN model is trained and the parameters
    are ready to be loaded. Our aim is to load new images into the model and let it
    classify whether the respective electronic component is good for assembly or not
    (see chapter *1.1 The task: Classify an industrial component as good or scrap*).'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产阶段，我们假设CNN模型已经训练好，并且参数可以加载。我们的目标是将新图像加载到模型中，让模型判断相应的电子组件是否适合用于组装（见章节 *1.1
    任务：将工业组件分类为合格或废料*）。
- en: We start by loading the required libraries, setting the device as `‘cuda’` or
    `‘cpu’`, defining the class `CNN` (exactly as in chapter 2.8), and loading the
    model from file with `torch.load()`. We need to define the class `CNN` before
    loading the parameters; otherwise, the parameters cannot be assigned correctly.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载所需的库，将设备设置为`‘cuda’`或`‘cpu’`，定义`CNN`类（与章节2.8完全相同），然后使用`torch.load()`从文件中加载模型。我们需要在加载参数之前定义`CNN`类，否则参数将无法正确分配。
- en: '[PRE16]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: With running this code snippet, we have the CNN model loaded and parameterized
    in our computer’s memory.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行这段代码，我们已经将CNN模型加载并在计算机内存中进行了参数化。
- en: 3.2 Load images into dataset
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 将图像加载到数据集中
- en: As for the training phase, we need to prepare the images for processing in the
    CNN model. We load them from a specified folder, crop the inner 700x400 pixels,
    and transform the image data to a PyTorch tensor.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练阶段，我们需要准备图像以供CNN模型处理。我们从指定文件夹中加载图像，裁剪出内部700x400像素，并将图像数据转换为PyTorch张量。
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We perform all the steps in a custom dataset class called `Predict_Set()`. In
    `__init__()`, we specify the image folder, accept a `transform` function, and
    load the images from the image folder into the list `self.img_lst`. The method
    `__len__()` returns the number of images in the image folder. `__getitem__()`
    composes the path to an image from the folder path and the image name, crops the
    inner part of the image (as we did for the training dataset), and applies the
    `transform` function to the image. Finally, it returns the image tensor and the
    image name.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在一个自定义数据集类 `Predict_Set()` 中执行所有这些步骤。在 `__init__()` 中，我们指定图像文件夹，接受一个 `transform`
    函数，并将图像从文件夹加载到列表 `self.img_lst` 中。方法 `__len__()` 返回图像文件夹中的图像数量。`__getitem__()`
    从文件夹路径和图像名称组合出图像路径，裁剪图像的内部部分（如同我们对训练数据集所做的那样），并对图像应用 `transform` 函数。最后，它返回图像张量和图像名称。
- en: 3.3 Path, transform function, and data loader
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 路径、变换函数和数据加载器
- en: The final step in data preparation is to define a data loader that allows to
    iterate over the images for classification. Along the way, we specify the `path`
    to the image folder and define the `transform` function as a pipeline that first
    loads the image data to a PyTorch tensor, and, second, normalizes the data to
    a range of approximately -1 to +1\. We instantiate our custom dataset `Predict_Set()`
    to a variable `predict_set` and define the data loader `predict_loader`. Since
    we do not specify a batch size, `predict_loader` returns one image at a time.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备的最后一步是定义一个数据加载器，它允许我们遍历图像进行分类。在这个过程中，我们指定了图像文件夹的 `path` 并定义了 `transform`
    函数作为一个管道，首先将图像数据加载为 PyTorch 张量，其次将数据标准化到大约 -1 到 +1 的范围。我们将自定义数据集 `Predict_Set()`
    实例化为变量 `predict_set`，并定义数据加载器 `predict_loader`。由于我们没有指定批处理大小，`predict_loader`
    每次返回一张图像。
- en: '[PRE18]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 3.4 Custom function for classification
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 分类的自定义函数
- en: So far, the preparation of the image data for classification is complete. However,
    what we are still missing is a custom function that transfers the images to the
    CNN model, translates the model’s response into a classification, and returns
    the classification results. This is exactly what we do with `predict()`.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，图像数据的分类准备工作已经完成。然而，我们仍然缺少一个自定义函数，它将图像传输到 CNN 模型中，翻译模型的响应为分类结果，并返回分类结果。这正是我们通过
    `predict()` 完成的任务。
- en: '[PRE19]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`predict()` expects a data loader and the CNN model as its parameters. In its
    core, it iterates over the data loader, transfers the image data to the model,
    and interprets the models response with `output.argmax(1)` as the classification
    result — either *0* for scrap parts (nOK) or *1* for good parts (OK). The image
    data, the classification result, and the image name are appended to lists and
    the lists are returned as the function’s result.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '`predict()` 接受数据加载器和 CNN 模型作为其参数。在其核心，它会遍历数据加载器，将图像数据传输到模型中，并通过 `output.argmax(1)`
    解析模型的响应作为分类结果——*0* 表示报废部件（nOK），*1* 表示合格部件（OK）。图像数据、分类结果和图像名称被附加到列表中，列表作为函数的结果返回。'
- en: 3.5 Predict labels and plot images
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5 预测标签并绘制图像
- en: Finally, we want to utilize our custom functions and loaders to classify new
    images. In the folder `“data/Coil_Vision/02_predict”` we have reserved four images
    of electronic components that wait to be inspected. Remember, we want the CNN
    model to tell us whether we can use the components for automatic assembly or if
    we need to sort them out because the pins are likely to cause problems while trying
    to push them in the plug sockets.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们希望利用自定义函数和加载器来分类新的图像。在文件夹 `“data/Coil_Vision/02_predict”` 中，我们预留了四张电子元件的图像，等待检查。请记住，我们希望
    CNN 模型告诉我们，是否可以将这些元件用于自动组装，或者是否需要将它们筛选出来，因为这些引脚可能会在插入插座时造成问题。
- en: We call the custom function `predict()`, which returns a list of images, a list
    of classification results, and a list of image names. We enumerate the lists and
    plot the images with the names and the classification as headings.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调用自定义函数 `predict()`，它返回一组图像列表、分类结果列表和图像名称列表。我们遍历这些列表并绘制图像，名称和分类结果作为标题。
- en: '[PRE20]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](../Images/c6ae72f8ff91d66a8f74c53bcfde4e82.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c6ae72f8ff91d66a8f74c53bcfde4e82.png)'
- en: 'Fig. 10: Classification results in production phase | image by author'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：生产阶段的分类结果 | 图片来源：作者
- en: We see that the two images on the left side have been classified a good (label
    *1*) and the two on the right as scrap (label *0*). Due to our training data,
    the model is quite sensitive, and even small bends in the pins lead to them being
    classified as scrap.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，左侧的两张图片被分类为合格（标签*1*），右侧的两张则被分类为废品（标签*0*）。由于我们的训练数据，模型非常敏感，即使是针脚的微小弯曲也会导致它们被分类为废品。
- en: 'Part 4: What did the CNN consider in its “decision”?'
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4部分：CNN在其“决策”中考虑了什么？
- en: We have gone deep into the details of the CNN and our industrial use case so
    far. This seems like a good opportunity to take one step further and try to understand
    what the CNN model “sees” while processing the image data. To do this, we first
    study the convolutional layers and then examine which parts of the image are specifically
    important for the classification.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经深入探讨了卷积神经网络（CNN）及其在工业应用中的使用案例。这似乎是一个很好的机会，可以进一步了解CNN模型在处理图像数据时“看到了”什么。为此，我们首先研究卷积层，然后检查图像的哪些部分对分类尤为重要。
- en: 4.1 Study the convolutional filters’ dimensions
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 研究卷积滤波器的维度
- en: To gain a better understanding of how convolutional filters work and what they
    do to the images, let’s examine the layers in our industrial example in more detail.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解卷积滤波器的工作原理及其对图像的影响，让我们更详细地检查我们工业示例中的层。
- en: To access the layers, we enumerate `model.children()`, which is a generator
    for the model’s structure. If the layer is a convolutional layer, we append it
    to the list `all_layers` and save the weights’ dimensions in `conv_weights`. If
    we have a ReLU or a MaxPooling layer, we have no weights. In this case, we append
    the layer and *“*”* to the respective lists. Next, we enumerate `all_layers`,
    print the layer type, and the weights’ dimensions.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为了访问这些层，我们枚举`model.children()`，这是一个模型结构的生成器。如果某层是卷积层，我们将其添加到`all_layers`列表中，并将权重的维度保存在`conv_weights`中。如果层是ReLU或MaxPooling层，则没有权重。在这种情况下，我们将层和*“”*添加到相应的列表中。接下来，我们枚举`all_layers`，打印层的类型和权重的维度。
- en: '[PRE21]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](../Images/ca5c83dbeeab4d0d96467902c05f25d6.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ca5c83dbeeab4d0d96467902c05f25d6.png)'
- en: 'Fig. 11: Layers and weights’ dimensions'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：层及权重的维度
- en: Please compare the code snippet’s output with Fig. 3\. The first convolutional
    layer has one input — the original image with only one channel — and returns six
    feature maps. We apply six kernels, each of depth one and size 5x5\. Correspondingly,
    the weights are of dimension `torch.Size([6, 1, 5, 5])`. In contrast, layer 4
    receives six feature maps as input and returns 16 maps as output. We apply 16
    convolutional kernels, each of depth 6 and size 5x5\. The weights’ dimension is
    therefore `torch.Size([16, 6, 5, 5])`.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 请将代码片段的输出与图3进行对比。第一层卷积层有一个输入——只有一个通道的原始图像——并返回六个特征图。我们应用六个卷积核，每个深度为1，大小为5x5。因此，权重的维度是`torch.Size([6,
    1, 5, 5])`。相比之下，第4层接受六个特征图作为输入，并返回16个特征图作为输出。我们应用16个卷积核，每个深度为6，大小为5x5。因此，权重的维度为`torch.Size([16,
    6, 5, 5])`。
- en: 4.2 Visualize the convolutional filters’ weights
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 可视化卷积滤波器的权重
- en: Now, we know the convolutional filters’ dimensions. Next, we want to see their
    weights, which they have gained during the training process. Since we have so
    many different filters (six in the first convolutional layer and 16 in the second),
    we select, in both cases, the first input channel (index *0*).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经知道了卷积滤波器的维度。接下来，我们想查看它们在训练过程中获得的权重。由于我们有很多不同的滤波器（第一层有6个，第二层有16个），我们在这两种情况下都选择第一个输入通道（索引*0*）。
- en: '[PRE22]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We iterate over `all_layers`. If the layer is a convolutional layer (`nn.Conv2d`),
    then we print the layer’s index and the layer’s core data. Next, we prepare a
    plot and extract the weights matrix for the first input layer as an example. We
    enumerate all output layers and plot them with `plt.imshow()`. Finally, we print
    the weights’ values on the image so that we get an intuitive visualization of
    the convolutional filters.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遍历`all_layers`。如果某层是卷积层（`nn.Conv2d`），则打印该层的索引及其核心数据。接下来，我们准备一个图表，并以第一个输入层为例提取权重矩阵。我们枚举所有输出层并用`plt.imshow()`绘制它们。最后，我们在图像上打印权重的值，以便直观地展示卷积滤波器。
- en: '![](../Images/50934d1828dab3a74cf8b06eb2896806.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/50934d1828dab3a74cf8b06eb2896806.png)'
- en: 'Fig. 12: Visualization of the 6+16 convolutional filters (input layer index
    0) | image by author'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：6+16个卷积滤波器的可视化（输入层索引0）| 图片来源：作者
- en: Fig. 12 shows the six convolutional filter kernels of layer 1 and the 16 kernels
    of layer 4 (for input channel *0*). The model schematic in the upper right indicates
    the filters with a red outline. We see that the majority of values are close to
    0, and some are in the range of positive or negative 0.20–0.25\. The numbers represent
    the values used for the convolution demonstrated in Fig. 4\. This gives us the
    feature maps, which we inspect next.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12 展示了第 1 层的六个卷积滤波器内核和第 4 层的 16 个内核（对于输入通道*0*）。右上角的模型示意图显示了带红色轮廓的滤波器。我们看到大部分值接近
    0，部分值在正负 0.20–0.25 范围内。这些数字表示在图 4 中演示的卷积操作中使用的值。这些给出了特征图，接下来我们将检查这些特征图。
- en: 4.3 Examine the feature maps
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 检查特征图
- en: According to Fig. 4, we receive the first feature maps through the convolution
    of the input image. Therefore, we load a random image from the `test_loader` and
    push it to the CPU (in case you operate the CNN on the GPU).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 根据图 4，我们通过对输入图像进行卷积，获得了第一批特征图。因此，我们从 `test_loader` 中加载一张随机图像，并将其传送到 CPU（以防你在
    GPU 上操作 CNN）。
- en: '[PRE23]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](../Images/96f8a1bc70ca7968e0edb54d93932387.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96f8a1bc70ca7968e0edb54d93932387.png)'
- en: 'Fig. 13: Random image as output of above code | image by author'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：上述代码的随机图像输出 | 图片来源：作者
- en: Now we pass the image data `img` through the first convolution layer (`all_layers[0]`)
    and save the output in `results`. Next, we iterate over `all_layers` and feed
    the next layer with the output from the previous layer operation. Those operations
    are convolutions, ReLU activations or MaxPoolings. The output of each operation
    we append to `results`.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将图像数据 `img` 传递通过第一层卷积层（`all_layers[0]`），并将输出保存在 `results` 中。接下来，我们遍历 `all_layers`，并将前一层操作的输出传递给下一层。这些操作包括卷积、ReLU
    激活和 MaxPooling。每个操作的输出我们都会追加到 `results` 中。
- en: '[PRE24]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Finally, we plot the original image, the feature maps after passing the first
    layer (convolution), the second layer (ReLU), the third layer (MaxPooling), the
    forth layer (2nd convolution), the fifth layer (2nd ReLU), and the sixth layer
    (2nd MaxPooling).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们绘制了原始图像、经过第一层（卷积）、第二层（ReLU）、第三层（MaxPooling）、第四层（第二次卷积）、第五层（第二次 ReLU）和第六层（第二次
    MaxPooling）处理后的特征图。
- en: '![](../Images/b900b20f062aa67d15afdef6e1d59a01.png)![](../Images/57693ff63b838e80604a9863903d5b0a.png)![](../Images/ca87184666e2ce86b0b433a7ebd3a147.png)![](../Images/631942ee46e10f6bb60caf9d733933ba.png)![](../Images/0693ac0798cf3c69b9a9d025d4039bd2.png)![](../Images/e6ae0d8816b7018a826fbcf3db74b4d9.png)![](../Images/2c85133da17c727830b0dd668fbfc08b.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b900b20f062aa67d15afdef6e1d59a01.png)![](../Images/57693ff63b838e80604a9863903d5b0a.png)![](../Images/ca87184666e2ce86b0b433a7ebd3a147.png)![](../Images/631942ee46e10f6bb60caf9d733933ba.png)![](../Images/0693ac0798cf3c69b9a9d025d4039bd2.png)![](../Images/e6ae0d8816b7018a826fbcf3db74b4d9.png)![](../Images/2c85133da17c727830b0dd668fbfc08b.png)'
- en: 'Fig. 14: Original image and feature maps after passing the convolution, ReLU
    and MaxPooling layers | image by author'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：经过卷积、ReLU 和 MaxPooling 层处理后的原始图像和特征图 | 图片来源：作者
- en: We see that the convolutional kernels (compare Fig. 12) recalculate each pixel
    of the image. This appears as changed grayscale values in the feature maps. Some
    of the feature maps are sharpened compared to the original image or have a stronger
    black-and-white contrast, while others seem to be faded.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到卷积核（对比图 12）重新计算了图像的每个像素。这在特征图中表现为灰度值的变化。与原始图像相比，一些特征图被锐化，或者具有更强的黑白对比，而其他特征图则显得更模糊。
- en: The ReLU operations turn dark gray into black since negative values are set
    to zero.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 操作将深灰色转为黑色，因为负值被设置为零。
- en: MaxPooling keeps the images almost unchanged while halving the image size in
    both dimensions.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: MaxPooling 保持图像几乎不变，同时在两个维度上将图像大小减半。
- en: 4.4 Visualize the image areas that impact the classification the most
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 可视化对分类影响最大的图像区域
- en: Before we finish, let’s analyze which areas of the image are particularly decisive
    for the classification into scrap (index *0*) or good parts (index *1*). For this
    purpose, we use Gradient-weighted Class Activation Mapping (gradCAM). This technique
    computes the gradients of the trained model with respect to the predicted class
    (the gradients show how much the inputs — the image pixels — influence the prediction).
    The averages of the gradients of each feature map (= output channel of convolution
    layer) build the weights with which the feature maps are multiplied when calculating
    a heat map for visualization.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们完成之前，让我们分析一下图像中哪些区域对分类为废料（索引*0*）或良品（索引*1*）至关重要。为此，我们使用梯度加权类别激活映射（gradCAM）。该技术计算训练好的模型相对于预测类别的梯度（梯度显示输入——图像像素——如何影响预测）。每个特征图（即卷积层的输出通道）梯度的平均值构成了计算热图时，用于与特征图相乘的权重。
- en: But let’s look at one step after the other.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 但让我们逐步看一下。
- en: '[PRE25]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We define a function `gradCAM` that expects the input data `x`, an image or
    a feature map, and returns a `heatmap`.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义一个函数`gradCAM`，它接受输入数据`x`，一个图像或特征图，并返回一个`heatmap`。
- en: In the first block, we input `x` in the CNN `model` and receive `logits`, a
    tensor of shape [1, 2] with two values only. The values represent the predicted
    probabilities of the classes *0* and *1*. We select the index of the larger value
    as the model’s prediction `pred`.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个模块中，我们将`x`输入到CNN `model`中，并得到`logits`，这是一个形状为[1, 2]的张量，仅包含两个值。该值表示类别*0*和*1*的预测概率。我们选择较大值的索引作为模型的预测`pred`。
- en: In the second block, we extract the first five layers of the model — from first
    convolution to second ReLU — and save them to `last_conv`. We run `x` through
    the selected layers and store the output in `activations`. As the name suggests,
    those are the activations (=feature maps) of the second convolutional layer (after
    ReLU activation).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个模块中，我们提取模型的前五层——从第一个卷积层到第二个ReLU层——并将它们保存在`last_conv`中。我们将`x`通过选择的层进行计算，并将输出存储在`activations`中。顾名思义，这些是第二个卷积层的激活值（即特征图）（经过ReLU激活后）。
- en: In the third block, we do the backward propagation for the logit value of the
    predicted class `logits[0,pred]`. In other words, we compute all the gradients
    of the CNN with respect to the prediction. The gradients show how much a change
    in the input data — the original image pixels — impact the models output — the
    prediction. The result is saved in the PyTorch computational graph until we delete
    it with `model.zero_grad()`.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三个模块中，我们对预测类别的logit值`logits[0,pred]`进行反向传播。换句话说，我们计算CNN对于预测的所有梯度。梯度显示了输入数据（原始图像像素）变化对模型输出（即预测结果）的影响。计算结果保存在PyTorch计算图中，直到我们通过`model.zero_grad()`删除它。
- en: In the fourth block, we compute the averages of the gradients over the input
    channel, as well as the height and width of the image or the feature maps. As
    a result, we receive 16 average gradients for the 16 feature maps that are returned
    from the second convolutional layer. We save them in `pooled_grads`.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在第四个模块中，我们计算输入通道上的梯度平均值，以及图像或特征图的高度和宽度。结果，我们得到16个平均梯度，对应于从第二个卷积层返回的16个特征图。我们将其保存在`pooled_grads`中。
- en: In the fifth block, we iterate over the 16 feature maps returned from the second
    convolutional layer and weight them with the average gradients `pooled_grads`.
    This operation gives more impact to those feature maps (and their pixels) that
    have high importance for the prediction and vice versa. From now on, `activations`
    holds not the feature maps, but the weighted feature maps.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在第五个模块中，我们遍历从第二个卷积层返回的16个特征图，并使用平均梯度`pooled_grads`对其加权。此操作赋予对预测具有重要性（及其像素）的特征图更大的权重，反之亦然。从现在起，`activations`不再保存特征图，而是保存加权后的特征图。
- en: Finally, in the last block, we compute the `heatmap` as the average feature
    map of all `activations`. This is what the function `gradCAM` returns.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在最后一个模块中，我们计算`heatmap`，即所有`activations`的平均特征图。这就是函数`gradCAM`返回的结果。
- en: Before we can plot the image and the heatmap, we need to transform both for
    the overlay. Remember, the feature maps are smaller than the original picture
    (see chapters 1.3 and 1.7), and so is the heatmap. This is why we need the function
    `upsampleHeatmap()`. The function scales the pixel values to the range of 0 to
    255 and transforms them to 8-bit integer format (required from the `cv2` library).
    It resizes the heatmap to 400x700 px and applies a color map to both the image
    and heatmap. Finally, we overlay 70% heatmap and 30% image and return the composition
    for plotting.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可以绘制图像和热力图之前，我们需要将两者进行转换以便叠加。请记住，特征图比原始图像小（参见第1.3章和第1.7章），热力图也是如此。这就是为什么我们需要`upsampleHeatmap()`函数的原因。该函数将像素值缩放到0到255的范围，并将其转换为8位整数格式（这是`cv2`库所需的格式）。它将热力图调整为400x700像素，并对图像和热力图应用颜色映射。最后，我们将70%的热力图和30%的图像叠加，并返回合成图以供绘制。
- en: '[PRE26]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We want to plot the original image and the heatmap overlay next to each other
    in one row. To do this, we iterate over the data loader `predict_loader`, run
    the `gradCAM()` function on the images and the `upsampleHeatmap()` function on
    the heatmap and the image. Finally, we plot the original image and the heatmap
    in a row with `matplotlib.pyplot`.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望将原始图像和热力图叠加展示在同一行中。为此，我们遍历数据加载器`predict_loader`，对图像运行`gradCAM()`函数，对热力图和图像运行`upsampleHeatmap()`函数。最后，我们使用`matplotlib.pyplot`将原始图像和热力图并排绘制。
- en: '[PRE27]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![](../Images/e8ac8b63fb261f64205230e96bdc5e9c.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e8ac8b63fb261f64205230e96bdc5e9c.png)'
- en: 'Fig. 15: Image and heatmap (inner two rows of the output) | image by author'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：图像与热力图（输出的内两行）| 图像由作者提供
- en: The blue areas of the heatmap have low impact on the model’s decision, while
    the yellow and red areas are very important. We see that in our use case, mainly
    the contour of the electronic component (in particular the metal pins) is decisive
    for the classification into scrap or good parts. Of course, this is highly reasonable,
    given that the use case primarily deals with bent pins.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 热力图中的蓝色区域对模型决策的影响较小，而黄色和红色区域则非常重要。我们看到，在我们的使用案例中，主要是电子元件的轮廓（尤其是金属引脚）对分类为废料或好件至关重要。当然，这一点非常合理，因为我们的用例主要处理的是弯曲引脚。
- en: Conclusion
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Convolutional Neural Networks (CNNs) are nowadays a common and widely used tool
    for visual inspection tasks in the industrial environment. In our use case, with
    relatively few lines of code, we managed to define a model that classifies electronic
    components as good parts or scrap with high precision. The big advantage, compared
    to classic approaches of vision inspection, is that no process engineer needs
    to specify visual marks in the images for the classification. Instead, the CNN
    learns from labeled examples and is able to replicate this knowledge to other
    images. In our specific use case, 626 labeled images were sufficient for training
    and validation. In more complex cases, the demand for training data might be significantly
    higher.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）如今已成为工业环境中常见且广泛使用的视觉检测工具。在我们的使用案例中，通过相对少量的代码行，我们成功定义了一个模型，能够高精度地将电子元件分类为好件或废料。与传统的视觉检测方法相比，最大的优势在于不需要过程工程师在图像中指定视觉标记来进行分类。相反，CNN通过标签化的示例进行学习，并能够将这种知识复制到其他图像。在我们的具体用例中，626张标签化的图像足以进行训练和验证。在更复杂的情况下，训练数据的需求可能会显著增加。
- en: Algorithms like gradCAM (Gradient-weighted Class Activation Mapping) significantly
    help in understanding which areas in the image are particularly relevant for the
    model’s decision. In this way, they support a broad use of CNNs in the industrial
    context by building trust in the model’s functionality.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 像gradCAM（梯度加权类激活映射）这样的算法在理解图像中哪些区域对模型的决策特别相关方面具有重要帮助。通过这种方式，它们通过增强对模型功能的信任，支持卷积神经网络（CNN）在工业环境中的广泛应用。
- en: In this article, we have explored many details of the inner workings of Convolutional
    Neural Networks. I hope you enjoyed the journey and have gained a deep understanding
    of how CNNs work.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们探讨了卷积神经网络的许多内部工作细节。希望您喜欢这次旅程，并且深入理解了CNN的工作原理。
