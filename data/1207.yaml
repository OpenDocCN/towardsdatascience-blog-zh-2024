- en: Recreating PyTorch from Scratch (with GPU Support and Automatic Differentiation)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零开始重建PyTorch（支持GPU和自动求导）
- en: 原文：[https://towardsdatascience.com/recreating-pytorch-from-scratch-with-gpu-support-and-automatic-differentiation-8f565122a3cc?source=collection_archive---------0-----------------------#2024-05-14](https://towardsdatascience.com/recreating-pytorch-from-scratch-with-gpu-support-and-automatic-differentiation-8f565122a3cc?source=collection_archive---------0-----------------------#2024-05-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/recreating-pytorch-from-scratch-with-gpu-support-and-automatic-differentiation-8f565122a3cc?source=collection_archive---------0-----------------------#2024-05-14](https://towardsdatascience.com/recreating-pytorch-from-scratch-with-gpu-support-and-automatic-differentiation-8f565122a3cc?source=collection_archive---------0-----------------------#2024-05-14)
- en: Build your own deep learning framework based on C/C++, CUDA, and Python, with
    GPU support and automatic differentiation
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于C/C++、CUDA和Python构建你自己的深度学习框架，支持GPU并具有自动求导功能
- en: '[](https://medium.com/@lucasdelimanogueira?source=post_page---byline--8f565122a3cc--------------------------------)[![Lucas
    de Lima Nogueira](../Images/76edd8ee4005d4c0b8bd476261eb06ae.png)](https://medium.com/@lucasdelimanogueira?source=post_page---byline--8f565122a3cc--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8f565122a3cc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8f565122a3cc--------------------------------)
    [Lucas de Lima Nogueira](https://medium.com/@lucasdelimanogueira?source=post_page---byline--8f565122a3cc--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@lucasdelimanogueira?source=post_page---byline--8f565122a3cc--------------------------------)[![Lucas
    de Lima Nogueira](../Images/76edd8ee4005d4c0b8bd476261eb06ae.png)](https://medium.com/@lucasdelimanogueira?source=post_page---byline--8f565122a3cc--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8f565122a3cc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8f565122a3cc--------------------------------)
    [Lucas de Lima Nogueira](https://medium.com/@lucasdelimanogueira?source=post_page---byline--8f565122a3cc--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8f565122a3cc--------------------------------)
    ·24 min read·May 14, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8f565122a3cc--------------------------------)
    ·阅读时长24分钟·2024年5月14日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/7fb280914af3d0239f5a27ae8d414ce0.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7fb280914af3d0239f5a27ae8d414ce0.png)'
- en: Image by Author with the assistance of AI ([https://copilot.microsoft.com/images/create](https://copilot.microsoft.com/images/create))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者与AI协作制作 ([https://copilot.microsoft.com/images/create](https://copilot.microsoft.com/images/create))
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'For many years I have been using PyTorch to construct and train deep learning
    models. Even though I have learned its syntax and rules, something has always
    aroused my curiosity: what is happening internally during these operations? How
    does all of this work?'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，我一直在使用PyTorch构建和训练深度学习模型。尽管我已经学会了它的语法和规则，但一直有一个问题让我感到好奇：在这些操作过程中，内部发生了什么？这一切是如何工作的？
- en: 'If you have gotten here, you probably have the same questions. If I ask you
    how to create and train a model in PyTorch, you will probably come up with something
    similar to the code below:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经读到这里，你可能有相同的问题。如果我问你如何在PyTorch中创建和训练一个模型，你可能会写出类似于下面的代码：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: But what if I ask you how does this backward step works? Or, for instance, what
    happens when you reshape a tensor? Is the data rearranged internally? How does
    that happens? Why is PyTorch so fast? How does PyTorch handle GPU operations?
    These are the types of questions that have always intrigued me, and I imagine
    they also intrigue you. Thus, in order to better understand these concepts, what
    is better than building your own tensor library ***from scratch***? And that is
    what you will learn in this article!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我问你，反向传播是如何工作的呢？或者，例如，当你重新塑形一个张量时，会发生什么？数据是否在内部重新排列？这一过程是如何发生的？为什么PyTorch如此高效？PyTorch是如何处理GPU操作的？这些问题一直令我感到好奇，我想它们也会让你感兴趣。因此，为了更好地理解这些概念，最好的方法是什么呢？那就是从***零开始***构建你自己的张量库！而这正是你将在本文中学到的内容！
- en: '#1 — Tensor'
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '#1 — 张量'
- en: 'In order to construct a *tensor library*, the first concept you need to learn
    obviously is: what is a tensor?'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建一个*张量库*，你首先需要了解的概念显然是：什么是张量？
- en: You may have an intuitive idea that a tensor is a mathematical concept of a
    n-dimensional data structure that contains some numbers. But here we need to understand
    how to model this data structure from a computational perspective. We can think
    of a tensor as consisting of the data itself and also some metadata describing
    aspects of the tensor such as its shape or the device it lives in (i.e. CPU memory,
    GPU memory…).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会直观地认为张量是一个数学概念，表示一个包含一些数字的 n 维数据结构。但在这里我们需要从计算的角度理解如何建模这个数据结构。我们可以把张量看作是由数据本身和一些元数据组成，这些元数据描述了张量的某些方面，如它的形状或它所存储的设备（即
    CPU 内存、GPU 内存等）。
- en: '![](../Images/126751c339f220fec6bc0a40690fb069.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/126751c339f220fec6bc0a40690fb069.png)'
- en: Image by Author
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: There is also a less popular metadata that you may have never heard of, called
    *stride*. This concept is very important to understand the internals of tensor
    data rearrangement, so we need to discuss it a little more.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个不太常见的元数据，可能你从未听说过，叫做 *步幅*。这个概念对于理解张量数据重排的内部原理非常重要，所以我们需要更详细地讨论一下。
- en: Imagine a 2-D tensor with shape [4, 8], illustrated below.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个形状为 [4, 8] 的二维张量，如下所示。
- en: '![](../Images/169e1865490d4603d66dc792052c1458.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/169e1865490d4603d66dc792052c1458.png)'
- en: 4x8 Tensor (Image by Author)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 4x8 张量（作者提供的图片）
- en: 'The data (i.e. float numbers) of a tensor is actually stored as a 1-dimensional
    array on memory:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 张量的数据（即浮点数）实际上是以一维数组的形式存储在内存中的：
- en: '![](../Images/edc491c716df7cab51cd1ca4a61ff296.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/edc491c716df7cab51cd1ca4a61ff296.png)'
- en: 1-D dimensional data array of the tensor (Image by Author)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 张量的一维数据数组（作者提供的图片）
- en: 'So, in order to represent this 1-dimensional array as a N-dimensional tensor,
    we use strides. Basically the idea is the following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了将这个一维数组表示为一个 N 维张量，我们使用步幅。基本的思想如下：
- en: 'We have a matrix with 4 rows and 8 columns. Considering that all of its elements
    are organized by rows on the 1-dimensional array, if we want to access the value
    at position [2, 3], we need to traverse 2 rows (of 8 elements each) plus 3 positions.
    In mathematical terms, we need to traverse 3 + 2 * 8 elements on the 1-dimensional
    array:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个 4 行 8 列的矩阵。考虑到它的所有元素都是按行在一维数组中组织的，如果我们想访问位置 [2, 3] 的值，我们需要遍历 2 行（每行 8
    个元素），加上 3 个位置。用数学术语来说，我们需要在一维数组上遍历 3 + 2 * 8 个元素：
- en: '![](../Images/644a13550e4601385357edc5bb88071b.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/644a13550e4601385357edc5bb88071b.png)'
- en: Image by Author
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: So this ‘8’ is the *stride* of the *second* dimension. In this case, it is the
    information of how many elements I need to traverse on the array to “jump” to
    other positions on the *second* *dimension.*
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这个‘8’是第二维度的 *步幅*。在这种情况下，它表示了我需要在数组中遍历多少个元素才能“跳转”到第二维度的其他位置。
- en: Thus, for accessing the element [i, j] of a 2-dimensional tensor with shape
    [shape_0, shape_1], we basically need to access the element at position j + i
    * shape_1
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于访问形状为 [shape_0, shape_1] 的二维张量中的元素 [i, j]，我们基本上需要访问位置 j + i * shape_1 的元素。
- en: 'Now, let us imagine a 3-dimensional tensor:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们想象一个三维张量：
- en: '![](../Images/bc1c13d004a29b88a4f73e8b11a51483.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc1c13d004a29b88a4f73e8b11a51483.png)'
- en: 5x4x8 Tensor (Image by Author)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 5x4x8 张量（作者提供的图片）
- en: You can think of this 3-dimensional tensor as a sequence of matrices. For example,
    you can think of this [5, 4, 8] tensor as 5 matrices of shape [4, 8].
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以把这个三维张量看作是矩阵的序列。例如，你可以把这个形状为 [5, 4, 8] 的张量看作是 5 个形状为 [4, 8] 的矩阵。
- en: Now, in order to access the element at position [1, 2, 7], you need to traverse
    1 complete matrix of shape [4,8], 2 rows of shape [8] and 7 columns of shape [1].
    So, you need to traverse (1 * 4 * 8) + (2 * 8) + (7 * 1) positions on the 1-dimensional
    array.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了访问位置 [1, 2, 7] 的元素，你需要遍历一个完整的形状为 [4, 8] 的矩阵，2 行形状为 [8] 和 7 列形状为 [1]。所以，你需要在一维数组上遍历
    (1 * 4 * 8) + (2 * 8) + (7 * 1) 个位置。
- en: '![](../Images/292c1ea36eb8d9c10528822680f5c698.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/292c1ea36eb8d9c10528822680f5c698.png)'
- en: Image by Author
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'Thus, to access the element [i][j][k] of a 3-D tensor with [shape_0, shape_1,
    shape_2] on the 1-D data array, you do:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，要在一维数据数组上访问一个形状为 [shape_0, shape_1, shape_2] 的三维张量中的元素 [i][j][k]，你需要：
- en: '![](../Images/81de413fc9cf4c48a4a1b0c764dea74f.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81de413fc9cf4c48a4a1b0c764dea74f.png)'
- en: This shape_1 * shape_2 is the *stride* of the first dimension, the shape_2 is
    the *stride* of the second dimension and 1 is the stride of the third dimension.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 shape_1 * shape_2 是第一维的 *步幅*，shape_2 是第二维的 *步幅*，1 是第三维的步幅。
- en: 'Then, in order to generalize:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了进行概括：
- en: '![](../Images/e3ef816d9241ae378b6b92a5d7a09bbc.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e3ef816d9241ae378b6b92a5d7a09bbc.png)'
- en: 'Where the *strides* of each dimension can be calculated using the product of
    the next dimension tensor shapes:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 每个维度的*步幅*可以通过下一个维度张量形状的乘积来计算：
- en: '![](../Images/7b9a7f16cd0d11e4767e681da1993aef.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b9a7f16cd0d11e4767e681da1993aef.png)'
- en: Then we set stride[n-1] = 1.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们设置步幅[n-1] = 1。
- en: On our tensor example of shape [5, 4, 8] we would have strides = [4*8, 8, 1]
    = [32, 8, 1]
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的形状为[5, 4, 8]的张量示例中，我们将得到步幅 = [4*8, 8, 1] = [32, 8, 1]
- en: 'You can test on your own:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以自己测试：
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Ok, but why do we need shapes and strides? Beyond accessing elements of N-dimensional
    tensors stored as 1-dimensional arrays, this concept can be used to manipulate
    tensor arrangements very easily.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，但是为什么我们需要形状和步幅？除了访问存储为1维数组的N维张量的元素之外，这个概念可以非常容易地用来操作张量排列。
- en: For example, to reshape a tensor, you only need to set the new shape and calculate
    the new strides based on it! (since the new shape guarantees the same number of
    elements)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要重塑一个张量，你只需要设置新形状并根据它计算新步幅！（因为新形状保证了相同数量的元素）
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Internally, the tensor is still stored as the same 1-dimensional array. The
    reshape method did not change the order of the elements within the array! That’s
    amazing, isn’t? 😁
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，张量仍然存储为相同的1维数组。重塑方法没有改变数组内元素的顺序！这很神奇，不是吗？😁
- en: 'You can verify on your own using the following function that accesses the internal
    1-dimensional array on PyTorch:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以自己验证，使用以下函数访问PyTorch的内部1维数组：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Or for instance, you want to transpose two axes. Internally, you just need to
    swap the respective strides!
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 或者例如，你想要转置两个轴。在内部，你只需要交换相应的步幅！
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'If you print the internal array, both have the same values:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打印内部数组，两者都有相同的值：
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: However, the stride of `new_t` now does not match with the equation I showed
    above. This is due to the fact that the tensor is now not contiguous. That means
    that although the internal array remains the same, the order of its values in
    memory does not match with the actual order of the tensor.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，`new_t`的步幅现在与我上面展示的方程不匹配。这是因为张量现在不再是连续的。这意味着虽然内部数组保持不变，但其在内存中的值的顺序与张量的实际顺序不匹配。
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This means the accessing the non-contiguous elements in sequence is less efficient
    (as the real tensor elements is not ordered in sequence on memory). In order to
    fix that, we can do:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着按顺序访问非连续元素的效率较低（因为真实的张量元素在内存中不是按顺序排列的）。为了解决这个问题，我们可以这样做：
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'If we analyze the internal array, its order now matches with the actual tensor
    order now, which can provide better memory access efficiency:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们分析内部数组，现在其顺序与实际张量顺序匹配，这可以提供更好的内存访问效率：
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now that we comprehend how a tensor is modeled, let us start creating our library!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了张量是如何建模的，让我们开始创建我们的库吧！
- en: I will call it *Norch*, which stands for NOT PyTorch, and also makes an allusion
    to my last name, Nogueira 😁
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我将其称为*Norch*，代表着NOT PyTorch，并且也暗指了我的姓氏，Nogueira 😁
- en: The first thing to know is that although PyTorch is used through Python, internally
    it runs C/C++. So we will first create our internals C/C++ functions.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要知道的是，尽管PyTorch是通过Python使用的，但在内部它运行的是C/C++。因此，我们将首先创建我们的内部C/C++函数。
- en: 'We can first define a tensor as a struct to store its data and metadata, and
    create a function to instantiate it:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以首先定义张量作为一个结构体来存储其数据和元数据，并创建一个函数来实例化它：
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In order to access some element, we can take advantage of strides, as we learned
    before:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了访问某个元素，我们可以利用之前学到的步幅：
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now, we can create the tensor operations. I will show some examples and you
    can find the complete version in the repository linked at the end of this article.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以创建张量操作。我将展示一些示例，你可以在本文末尾链接的存储库中找到完整版本。
- en: '[PRE11]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'After that we are able to create our other tensor functions that will call
    these operations:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 之后我们就能创建我们的其他调用这些操作的张量函数：
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As mentioned before, the tensor reshaping does not modify the internal data
    array:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，张量重塑不会修改内部数据数组：
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Although we can now do some tensor operations, nobody deserves to run it using
    C/C++, right? Let us start building our Python wrapper!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们现在可以进行一些张量操作，但是没有人愿意使用C/C++来运行它，对吧？让我们开始构建我们的Python包装器！
- en: There are a lot of options to run C/C++ code using Python, such as *Pybind11*
    and *Cython.* For our example I will use *ctypes.*
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多选项可以使用Python运行C/C++代码，比如*Pybind11*和*Cython*。在我们的例子中，我将使用*ctypes*。
- en: 'The basically structure of *ctypes* is shown below:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*ctypes*的基本结构如下所示：'
- en: '[PRE14]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As you can see, it is very intuitive. After you compile the C/C++ code, you
    can use it on Python with *ctypes* very easily. You only need to define the arguments
    & return c_types of the function, convert the variable to its respective c_types
    and call the function. For more complex types such as arrays (float lists) you
    can use pointers.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这非常直观。在你编译C/C++代码后，可以非常轻松地在Python中使用*ctypes*。你只需要定义函数的参数和返回值类型，并将变量转换为相应的C类型，再调用函数。对于更复杂的类型，如数组（浮动列表），你可以使用指针。
- en: '[PRE17]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'And for struct types we can create our own c_type:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对于结构类型，我们可以创建我们自己的C类型：
- en: '[PRE18]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: After this brief explanation, let us construct the Python wrapper for our tensor
    C/C++ library!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段简短的解释之后，让我们为我们的张量C/C++库构建Python包装器！
- en: '[PRE19]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Now we include the Python tensor operations to call the C/C++ operations.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们包括了Python张量操作，以便调用C/C++操作。
- en: '[PRE20]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: If you got here, you are now capable to run the code and start doing some tensor
    operations!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经看到这里，你现在可以运行代码并开始进行一些张量操作了！
- en: '[PRE21]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '#2 — GPU Support'
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '#2 — GPU支持'
- en: 'After creating the basic structure of our library, now we will take it to a
    new level. It is well-known that you can call `.to("cuda")` to send data to GPU
    and run math operations faster. I will assume that you have basic knowledge on
    how CUDA works, but if you do not, you can read my other article: [CUDA tutorial.](/why-deep-learning-models-run-faster-on-gpus-a-brief-introduction-to-cuda-programming-035272906d66)
    I will wait here. 😊'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建了我们库的基本结构之后，现在我们将把它提升到一个新层次。众所周知，你可以调用`.to("cuda")`将数据发送到GPU，并加快数学运算速度。我假设你对CUDA的基本工作原理有所了解，但如果不了解，你可以阅读我另外一篇文章：[CUDA教程.](/why-deep-learning-models-run-faster-on-gpus-a-brief-introduction-to-cuda-programming-035272906d66)
    我在这里等你。😊
- en: …
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: 'For those in a hurry, a simple introduction here:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 对于急于了解的人，这里有一个简短的介绍：
- en: Basically, all of our code until here is running on CPU memory. Altough for
    single operations CPUs are faster, the advantage of GPUs relies on its parallelization
    capabilities. While CPU design aims to execute a sequence of operations (threads)
    fast (but can only execute dozens of them), the GPU design aims to execute millions
    of operations in parallel (by sacrificing individual threads performance).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，到目前为止，我们的所有代码都在CPU内存上运行。虽然对于单一操作CPU速度更快，但GPU的优势在于它的并行化能力。CPU设计旨在快速执行一系列操作（线程）（但它只能执行几十个），而GPU设计则旨在并行执行数百万个操作（通过牺牲单个线程的性能）。
- en: Thus, we can take advantage of this capability to perform operations in parallel.
    For example, in a million-sized tensor addition, instead of adding the elements
    of each index sequentially inside a loop, using a GPU we can add all of them in
    parallel at once. In order to do that, we can use CUDA, which is a platform developed
    by NVIDIA to enable developers to integrate GPU support to their software applications.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以利用这一能力并行执行操作。例如，在一个百万大小的张量加法中，不必在循环内部依次添加每个索引的元素，使用GPU我们可以一次性并行地加和所有元素。为此，我们可以使用CUDA，这是NVIDIA开发的一个平台，旨在让开发者将GPU支持集成到他们的软件应用中。
- en: In order to do that, you can use CUDA C/C++, which is a simple C/C++ based interface
    designed to run specific GPU operations (such as copy data from CPU memory to
    GPU memory).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，你可以使用CUDA C/C++，这是一种基于C/C++的简单接口，旨在运行特定的GPU操作（例如将数据从CPU内存复制到GPU内存）。
- en: The code below basically uses some CUDA C/C++ functions to copy data from CPU
    to GPU, and run the AddTwoArrays function (also called kernel) on a total of N
    GPU threads in parallel, each responsible to add a different element of the array.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码基本上使用了一些CUDA C/C++函数，将数据从CPU复制到GPU，并在总共N个GPU线程上并行运行AddTwoArrays函数（也叫做kernel），每个线程负责加和数组中的不同元素。
- en: '[PRE22]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As you can notice, instead of adding each element pair per operation, we run
    all of the adding operations in parallel, getting rid of the loop instruction.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们不是每次操作时加和每对元素，而是并行执行所有加法操作，从而省去了循环指令。
- en: After this brief introduction, we can go back to our tensor library.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段简短的介绍之后，我们可以回到我们的张量库。
- en: The first step is to create a function to send tensor data from CPU to GPU and
    vice versa.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是创建一个函数，将张量数据从CPU发送到GPU，反之亦然。
- en: '[PRE23]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The Python wrapper:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Python包装器：
- en: '[PRE25]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then, we create GPU versions for all of our tensor operations. I will write
    examples for addition and subtraction:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们为所有的张量操作创建GPU版本。我将为加法和减法写出示例：
- en: '[PRE26]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Subsequently, we include a new tensor attribute `char* device` on the `tensor.cpp`
    and we can use it to select where the operations will be run (CPU or GPU):'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们在`tensor.cpp`上包含一个新的张量属性`char* device`，我们可以使用它来选择操作将在哪里运行（CPU或GPU）：
- en: '[PRE27]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Now our library has GPU support!
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的库支持GPU！
- en: '[PRE28]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '#3 — Automatic Differentation (Autograd)'
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '#3 — 自动微分（Autograd）'
- en: 'One of the main reasons why PyTorch got so popular is due to its Autograd module.
    It is a core component that allows automatic differentiation in order to compute
    gradients (crucial for training models with optimization algorithms such as gradient
    descent). By calling a single method `.backward()`, it computes all of the gradients
    from previous tensor operations:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch变得如此受欢迎的主要原因之一是其Autograd模块。这是一个核心组件，允许自动微分以计算梯度（对于使用梯度下降等优化算法训练模型至关重要）。通过调用单个方法`.backward()`，它计算出先前张量操作的所有梯度：
- en: '[PRE29]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'In order to understand what is happening, let’s try to replicate the same procedure
    manually:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解发生了什么，让我们尝试手动复制相同的过程：
- en: '![](../Images/255a2823dc15242c9d7414c9175d073d.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/255a2823dc15242c9d7414c9175d073d.png)'
- en: 'Let’s first calculate:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先计算：
- en: '![](../Images/3d5353e2d05c964798dfe75e81572db5.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d5353e2d05c964798dfe75e81572db5.png)'
- en: 'Note that *x* is a matrix, thus we need to calculate the derivative of L with
    respect to each element individually. Additionally, L is a summation over all
    elements, but it is important to remember that for each elements, the other elements
    does not interfere on its derivative. Therefore, we obtain the following term:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意*x*是一个矩阵，因此我们需要分别计算*L*对每个元素的导数。此外，*L*是所有元素的总和，但重要的是要记住，对于每个元素，其他元素不会影响其导数。因此，我们得到以下项：
- en: '![](../Images/612ffe8a8a672f3c2ebac55743b1a82c.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/612ffe8a8a672f3c2ebac55743b1a82c.png)'
- en: 'By applying the chain rule for each term, we differentiate the outer function
    and multiply by the derivative of the inner function:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 通过为每个项应用链式法则，我们区分外部函数并乘以内部函数的导数：
- en: '![](../Images/97a8bb17dd8c8e8153ee9eb9d5b77606.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97a8bb17dd8c8e8153ee9eb9d5b77606.png)'
- en: 'Where:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '![](../Images/0834be1c1e2772168593070107e386de.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0834be1c1e2772168593070107e386de.png)'
- en: 'Finally:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 最后：
- en: '![](../Images/4dec4240d0004de073e923b6d937831d.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4dec4240d0004de073e923b6d937831d.png)'
- en: Thus, we have the following final equation to calculate the derivative of L
    with respect to *x:*
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有以下最终方程来计算*L*相对于*x*的导数：
- en: '![](../Images/1f6843db3a0d35b765a74b1241ac61ae.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f6843db3a0d35b765a74b1241ac61ae.png)'
- en: 'Substituting the values into the equation:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 将值代入方程式：
- en: '![](../Images/95897f1570971bf15b239d7182f2b8cb.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/95897f1570971bf15b239d7182f2b8cb.png)'
- en: 'Calculating the result, we get the same values we obtained with PyTorch:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 计算结果，我们得到与PyTorch获得的相同值：
- en: '![](../Images/ac68ee4bb235624b9c9c1c0df4a69ae4.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac68ee4bb235624b9c9c1c0df4a69ae4.png)'
- en: 'Now, let’s analyze what we just did:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们分析刚才做的事情：
- en: 'Basically, we observed all the operations involved in reserved order: a summation,
    a power of 3 and a subtraction. Then, we applied chain of rule, calculating the
    derivative of each operation and recursively calculated the derivative for the
    next operation. So, first we need an implementation of the derivative for different
    math operations:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们按照保留顺序观察了所有涉及的操作：求和、3的幂和减法。然后，我们应用链式法则，计算每个操作的导数，并递归地计算下一个操作的导数。因此，首先我们需要为不同的数学操作实现导数：
- en: 'For addition:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 对于加法：
- en: '![](../Images/f0bea287a54af0930d57584959d48ecd.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f0bea287a54af0930d57584959d48ecd.png)'
- en: '[PRE30]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'For sin:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 对于正弦：
- en: '![](../Images/0751c6d99ffe7a0d1b1c359ffbcd68b4.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0751c6d99ffe7a0d1b1c359ffbcd68b4.png)'
- en: '[PRE31]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'For cosine:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 对于余弦：
- en: '![](../Images/4359a9a560ad9072af54cf872019b3bd.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4359a9a560ad9072af54cf872019b3bd.png)'
- en: '[PRE32]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'For element-wise multiplication:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 对于逐元素乘法：
- en: '![](../Images/3564658c2002c0917544fbbef510374c.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3564658c2002c0917544fbbef510374c.png)'
- en: '[PRE33]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'For summation:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 对于求和：
- en: '[PRE34]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: You can access the GitHub repository link at the end of the article to explore
    other operations.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在文章末尾访问GitHub存储库链接以探索其他操作。
- en: 'Now that we have derivative expressions for each operation, we can proceed
    with implementing the recursive backward chain rule. We can set a `requires_grad`
    argument for our tensor to indicate that we want to store the gradients of this
    tensor. If true, we will store the gradients for each tensor operation. For instance:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对每个操作有了导数表达式，我们可以继续实现递归反向链规则。我们可以为我们的张量设置一个`requires_grad`参数，以指示我们要存储此张量的梯度。如果为真，我们将为每个张量操作存储梯度。例如：
- en: '[PRE35]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Then, implement the `.backward()` method:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，实现`.backward()`方法：
- en: '[PRE36]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Finally, just implement `.zero_grad()` to zero the gradient of a tensor, and
    `.detach()` to remove the tensor autograd history:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，只需实现`.zero_grad()`将张量的梯度归零，`.detach()`将移除张量的自动梯度历史：
- en: '[PRE37]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Congratulations! You just created a complete tensor library with GPU support
    and automatic differentiation! Now we can create the `nn` and `optim` modules
    to train some deep learning models more easily.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您刚刚创建了一个完整的张量库，支持GPU和自动微分！现在我们可以创建`nn`和`optim`模块，更轻松地训练一些深度学习模型。
- en: '#4 — nn and optim modules'
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '#4 — nn和optim模块'
- en: The `nn` is a module for building neural networks and deep learning models and
    `optim` is related to optimization algorithms to train these models. In order
    to recreate them, the first thing to do is implementing a Parameter, which simply
    is a trainable tensor, with the same operations, but with `requires_grad` set
    always as `True` and with some random initialization technique.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn`是用于构建神经网络和深度学习模型的模块，而`optim`与优化算法相关，用于训练这些模型。为了重新创建它们，首先要实现一个Parameter，它只是一个可训练的张量，具有相同的操作，但`requires_grad`始终设置为`True`，并采用一些随机初始化技术。'
- en: '[PRE38]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'By using parameters, we can start contructing modules:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用参数，我们可以开始构建模块：
- en: '[PRE40]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: For example, we can construct our custom modules by inheriting from `nn.Module`,
    or we can use some previously created modules, such as the `linear`, which implements
    the ***y*** *= W****x*** *+ b* operation.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以通过继承`nn.Module`来构建自定义模块，或者我们可以使用一些先前创建的模块，比如`linear`，它实现了***y*** *= W****x***
    *+ b*的操作。
- en: '[PRE41]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now we can implement some loss and activation functions. For instance, a mean
    squared error loss and a sigmoid function:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以实现一些损失和激活函数。例如，均方误差损失和sigmoid函数：
- en: '[PRE42]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Finally, create the optimizers. On our example I will implement the Stochastic
    Gradient Descent algorithm:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，创建优化器。在我们的示例中，我将实现随机梯度下降算法：
- en: '[PRE44]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: And, that’s it! We just created our own deep learning framework! 🥳
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！我们刚刚创建了自己的深度学习框架！🥳
- en: 'Let’s do some training:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行一些训练：
- en: '[PRE45]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '![](../Images/e7662c657102cedd3717cdac495019f6.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7662c657102cedd3717cdac495019f6.png)'
- en: Image by Author
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: The model was successfully created and trained using our custom deep learning
    framework!
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型已成功使用我们自定义的深度学习框架创建和训练！
- en: You can check the complete code [here](https://github.com/lucasdelimanogueira/PyNorch).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[这里](https://github.com/lucasdelimanogueira/PyNorch)查看完整的代码。
- en: Conclusion
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this post we covered the basic concepts of what is a tensor, how it is modeled
    and more advanced topics such as CUDA and Autograd. We successfully created a
    deep learning framework with GPU support and automatic differentiation. I hope
    this post helped you to briefly understand how PyTorch works under the hood.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了张量的基本概念，它是如何建模的，以及更高级的主题，如CUDA和Autograd。我们成功地创建了一个支持GPU和自动微分的深度学习框架。希望本文能帮助您简要了解PyTorch在幕后是如何工作的。
- en: In future posts, I will try to cover more advanced topics such as distributed
    training (multinode / multi GPU) and memory management. Please let me know what
    you think or what you would like me to write about next in the comments! Thanks
    so much for reading! 😊
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来的帖子中，我将尝试涵盖更高级的主题，如分布式训练（多节点/多GPU）和内存管理。请在评论中告诉我您的想法或您希望我写下的内容！非常感谢您的阅读！😊
- en: Also, follow me here and on my [LinkedIn profile](https://www.linkedin.com/in/lucas-de-lima-nogueira/)
    to stay updated on my latest articles!
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请关注我的[LinkedIn个人资料](https://www.linkedin.com/in/lucas-de-lima-nogueira/)，以获取最新文章更新！
- en: References
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: '[PyNorch](https://github.com/lucasdelimanogueira/PyNorch) — GitHub repository
    of this project.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[PyNorch](https://github.com/lucasdelimanogueira/PyNorch) — 本项目的GitHub存储库。'
- en: '[Tutorial CUDA](https://medium.com/towards-data-science/why-deep-learning-models-run-faster-on-gpus-a-brief-introduction-to-cuda-programming-035272906d66)
    — A brief introduction on how CUDA works.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[CUDA教程](https://medium.com/towards-data-science/why-deep-learning-models-run-faster-on-gpus-a-brief-introduction-to-cuda-programming-035272906d66)
    — CUDA工作原理简介。'
- en: '[PyTorch](https://pytorch.org/docs/stable/index.html) — PyTorch documentation.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[PyTorch](https://pytorch.org/docs/stable/index.html) — PyTorch文档。'
- en: '[MartinLwx’s blog](https://martinlwx.github.io/en/how-to-reprensent-a-tensor-or-ndarray/)
    — Tutorial on strides.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '[MartinLwx的博客](https://martinlwx.github.io/en/how-to-reprensent-a-tensor-or-ndarray/)
    — 关于步幅的教程。'
- en: '[Strides tutorial](https://ajcr.net/stride-guide-part-1/) — Another tutorial
    about strides.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[步幅教程](https://ajcr.net/stride-guide-part-1/) — 关于步幅的另一个教程。'
- en: '[PyTorch internals](http://blog.ezyang.com/2019/05/pytorch-internals/) — A
    guide on how PyTorch is structured.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '[PyTorch内部](http://blog.ezyang.com/2019/05/pytorch-internals/) — PyTorch的结构指南。'
- en: '[Nets](https://github.com/arthurdjn/nets) — A PyTorch recreation using NumPy.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[Nets](https://github.com/arthurdjn/nets) — 使用 NumPy 重建的 PyTorch。'
- en: '[Autograd](https://www.youtube.com/watch?v=RxmBukb-Om4&t=935s) — A live coding
    of an Autograd library.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[Autograd](https://www.youtube.com/watch?v=RxmBukb-Om4&t=935s) — Autograd 库的实时编码。'
