- en: The Essential Guide to Effectively Summarizing Massive Documents, Part 1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é«˜æ•ˆæ€»ç»“æµ·é‡æ–‡æ¡£çš„åŸºæœ¬æŒ‡å—ï¼Œç¬¬1éƒ¨åˆ†
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/demystifying-document-digestion-a-deep-dive-into-summarizing-massive-documents-part-1-53f2ed9a669d?source=collection_archive---------5-----------------------#2024-09-14](https://towardsdatascience.com/demystifying-document-digestion-a-deep-dive-into-summarizing-massive-documents-part-1-53f2ed9a669d?source=collection_archive---------5-----------------------#2024-09-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/demystifying-document-digestion-a-deep-dive-into-summarizing-massive-documents-part-1-53f2ed9a669d?source=collection_archive---------5-----------------------#2024-09-14](https://towardsdatascience.com/demystifying-document-digestion-a-deep-dive-into-summarizing-massive-documents-part-1-53f2ed9a669d?source=collection_archive---------5-----------------------#2024-09-14)
- en: Document summarization is important for GenAI use-cases, but what if the documents
    are too BIG!? Read on to find out how I have solved it.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ–‡æ¡£æ‘˜è¦åœ¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰åº”ç”¨ä¸­éå¸¸é‡è¦ï¼Œä½†å¦‚æœæ–‡æ¡£å¤ªå¤§äº†æ€ä¹ˆåŠï¼ï¼Ÿç»§ç»­é˜…è¯»ï¼Œäº†è§£æˆ‘æ˜¯å¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜çš„ã€‚
- en: '[](https://medium.com/@vinayak.sengupta?source=post_page---byline--53f2ed9a669d--------------------------------)[![Vinayak
    Sengupta](../Images/ae66b684329fd9e4f34cf1c21e0b3b57.png)](https://medium.com/@vinayak.sengupta?source=post_page---byline--53f2ed9a669d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--53f2ed9a669d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--53f2ed9a669d--------------------------------)
    [Vinayak Sengupta](https://medium.com/@vinayak.sengupta?source=post_page---byline--53f2ed9a669d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@vinayak.sengupta?source=post_page---byline--53f2ed9a669d--------------------------------)[![Vinayak
    Sengupta](../Images/ae66b684329fd9e4f34cf1c21e0b3b57.png)](https://medium.com/@vinayak.sengupta?source=post_page---byline--53f2ed9a669d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--53f2ed9a669d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--53f2ed9a669d--------------------------------)
    [Vinayak Sengupta](https://medium.com/@vinayak.sengupta?source=post_page---byline--53f2ed9a669d--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--53f2ed9a669d--------------------------------)
    Â·8 min readÂ·Sep 14, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--53f2ed9a669d--------------------------------)
    Â·8åˆ†é’Ÿé˜…è¯»Â·2024å¹´9æœˆ14æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4a30c3d146fa1c3d85a1f30b369a5e24.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4a30c3d146fa1c3d85a1f30b369a5e24.png)'
- en: â€œSummarizing a lot of textâ€â€” Image generated with GPT-4o
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: â€œæ€»ç»“å¤§é‡æ–‡æœ¬â€â€”â€”ä½¿ç”¨GPT-4oç”Ÿæˆçš„å›¾åƒ
- en: Document summarization today has become one of the most (if not the most) common
    problem statements to solve using modern Generative AI (GenAI) technology. Retrieval
    Augmented Generation (RAG) is a common yet effective solution architecture used
    to solve it (If you want a deeper dive into what RAG is, check out this [**blog**](https://medium.com/@vinayak.sengupta/exploring-the-core-of-augmented-intelligence-advancing-the-power-of-retrievers-in-rag-frameworks-3ef9fe273764)!).
    But what if the document itself is so large that it cannot be sent as a whole
    in a single API request? Or what if it produces too many chunks to cause the infamous
    â€˜Lost in the Middleâ€™ context problem? In this article, I will discuss the challenges
    we face with such a problem statement, and go through a step-by-step solution
    that I applied using the guidance offered by Greg Kamradt in his [**GitHub repository**](https://github.com/gkamradt/langchain-tutorials/blob/main/data_generation/5%20Levels%20Of%20Summarization%20-%20Novice%20To%20Expert.ipynb).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä»Šï¼Œæ–‡æ¡£æ‘˜è¦å·²æˆä¸ºä½¿ç”¨ç°ä»£ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰æŠ€æœ¯è§£å†³çš„æœ€å¸¸è§é—®é¢˜ä¹‹ä¸€ï¼ˆå¦‚æœä¸æ˜¯æœ€å¸¸è§çš„è¯ï¼‰ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ˜¯ä¸€ä¸ªå¸¸è§ä¸”æœ‰æ•ˆçš„è§£å†³æ¶æ„ï¼Œç”¨äºè§£å†³è¿™ä¸€é—®é¢˜ï¼ˆå¦‚æœä½ æƒ³æ·±å…¥äº†è§£RAGæ˜¯ä»€ä¹ˆï¼Œå¯ä»¥æŸ¥çœ‹è¿™ç¯‡[**åšå®¢**](https://medium.com/@vinayak.sengupta/exploring-the-core-of-augmented-intelligence-advancing-the-power-of-retrievers-in-rag-frameworks-3ef9fe273764)ï¼ï¼‰ã€‚ä½†æ˜¯ï¼Œå¦‚æœæ–‡æ¡£æœ¬èº«å¤ªå¤§ï¼Œä»¥è‡³äºæ— æ³•åœ¨ä¸€æ¬¡APIè¯·æ±‚ä¸­å…¨éƒ¨å‘é€è¯¥æ€ä¹ˆåŠï¼Ÿæˆ–è€…ï¼Œå¦‚æœå®ƒç”Ÿæˆäº†å¤ªå¤šçš„ç‰‡æ®µï¼Œå¯¼è‡´è‘—åçš„â€œè¿·å¤±äºä¸­é—´â€ä¸Šä¸‹æ–‡é—®é¢˜æ€ä¹ˆåŠï¼Ÿåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†è®¨è®ºæˆ‘ä»¬åœ¨é¢å¯¹æ­¤ç±»é—®é¢˜æ—¶æ‰€é‡åˆ°çš„æŒ‘æˆ˜ï¼Œå¹¶é€æ­¥ä»‹ç»æˆ‘ä½¿ç”¨Greg
    Kamradtåœ¨å…¶[**GitHubä»“åº“**](https://github.com/gkamradt/langchain-tutorials/blob/main/data_generation/5%20Levels%20Of%20Summarization%20-%20Novice%20To%20Expert.ipynb)æä¾›çš„æŒ‡å¯¼æ‰€åº”ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚
- en: Some â€œc*ontextâ€*
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸€äº›â€œ*ä¸Šä¸‹æ–‡*â€
- en: RAG is a well-discussed and widely implemented solution for addressing document
    summarizing optimization using GenAI technologies. However, like any new technology
    or solution, it is prone to edge-case challenges, especially in todayâ€™s enterprise
    environment. Two main concerns are contextual length coupled with per-prompt cost
    and the previously mentioned â€˜Lost in the Middleâ€™ context problem. Letâ€™s dive
    a bit deeper to understand these challenges.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: RAG æ˜¯ä¸€ç§å¹¿æ³›è®¨è®ºå¹¶ä¸”å¹¿æ³›å®æ–½çš„è§£å†³æ–¹æ¡ˆï¼Œç”¨äºåˆ©ç”¨ GenAI æŠ€æœ¯ä¼˜åŒ–æ–‡æ¡£æ€»ç»“ã€‚ç„¶è€Œï¼Œåƒä»»ä½•æ–°æŠ€æœ¯æˆ–è§£å†³æ–¹æ¡ˆä¸€æ ·ï¼Œå®ƒä¹Ÿå®¹æ˜“é¢ä¸´è¾¹ç¼˜æ¡ˆä¾‹çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ä»Šå¤©çš„ä¼ä¸šç¯å¢ƒä¸­ã€‚ä¸¤ä¸ªä¸»è¦é—®é¢˜æ˜¯ä¸Šä¸‹æ–‡é•¿åº¦ä¸æ¯æ¬¡æç¤ºçš„æˆæœ¬ä»¥åŠå‰é¢æåˆ°çš„â€œè¿·å¤±åœ¨ä¸­é—´â€çš„ä¸Šä¸‹æ–‡é—®é¢˜ã€‚è®©æˆ‘ä»¬æ·±å…¥æ¢è®¨ä¸€ä¸‹è¿™äº›æŒ‘æˆ˜ã€‚
- en: '**Note***:* *I will be performing the exercises in Python using the LangChain,
    Scikit-Learn, Numpy and Matplotlib libraries for quick iterations.*'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**æ³¨æ„**ï¼š*æˆ‘å°†ä½¿ç”¨ LangChainã€Scikit-Learnã€Numpy å’Œ Matplotlib åº“æ¥è¿›è¡Œ Python ç»ƒä¹ ï¼Œä»¥ä¾¿å¿«é€Ÿè¿­ä»£ã€‚*'
- en: Context window and Cost constraints
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸Šä¸‹æ–‡çª—å£å’Œæˆæœ¬é™åˆ¶
- en: Today with automated workflows enabled by GenAI, analyzing big documents has
    become an industry expectation/requirement. People want to quickly find relevant
    information from medical reports or financial audits by just prompting the LLM.
    But there is a caveat, enterprise documents are not like documents or datasets
    we deal with in academics, the sizes are considerably bigger and the pertinent
    information can be present pretty much anywhere in the documents. Hence, methods
    like data cleaning/filtering are often not a viable option since domain knowledge
    regarding these documents is not always given.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä»Šï¼Œå€ŸåŠ© GenAI å¯ç”¨çš„è‡ªåŠ¨åŒ–å·¥ä½œæµï¼Œåˆ†æå¤§å‹æ–‡æ¡£å·²ç»æˆä¸ºè¡Œä¸šçš„æœŸæœ›/è¦æ±‚ã€‚äººä»¬å¸Œæœ›é€šè¿‡ç®€å•åœ°æç¤º LLMï¼Œå°±èƒ½å¿«é€Ÿä»åŒ»ç–—æŠ¥å‘Šæˆ–è´¢åŠ¡å®¡è®¡ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚ä½†æœ‰ä¸€ä¸ªè­¦å‘Šï¼Œä¼ä¸šæ–‡æ¡£ä¸æˆ‘ä»¬åœ¨å­¦æœ¯é¢†åŸŸå¤„ç†çš„æ–‡æ¡£æˆ–æ•°æ®é›†ä¸åŒï¼Œæ–‡æ¡£çš„å¤§å°é€šå¸¸è¦å¤§å¾—å¤šï¼Œç›¸å…³ä¿¡æ¯å¯èƒ½å‡ºç°åœ¨æ–‡æ¡£çš„ä»»ä½•åœ°æ–¹ã€‚å› æ­¤ï¼Œåƒæ•°æ®æ¸…ç†/è¿‡æ»¤è¿™æ ·çš„æ–¹å¼é€šå¸¸å¹¶ä¸æ˜¯ä¸€ä¸ªå¯è¡Œçš„é€‰æ‹©ï¼Œå› ä¸ºè¿™äº›æ–‡æ¡£çš„é¢†åŸŸçŸ¥è¯†å¹¶ä¸æ€»æ˜¯å¯ç”¨çš„ã€‚
- en: In addition to this, even the latest Large Language Models (LLMs) like GPT-4o
    by OpenAI with context windows of 128K tokens cannot just consume these documents
    in one shot or even if they did, the quality of response will not meet standards,
    especially for the cost it will incur. To showcase this, letâ€™s take a real-world
    example of trying to summarize the Employee Handbook of GitLab which can downloaded
    [**here**](https://kocielnik.gitlab.io/gitlab_handbook_takeaway/about-the-handbook.html).
    This document is available free of charge under the MIT license available on their
    GitHub [repository](https://gitlab.com/kocielnik/gitlab_handbook_takeaway/-/blob/master/LICENSE).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œå³ä½¿æ˜¯åƒ OpenAI çš„ GPT-4o è¿™æ ·æœ€æ–°çš„ å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä¸Šä¸‹æ–‡çª—å£ä¸º 128K ä»¤ç‰Œï¼Œä¹Ÿä¸èƒ½ä¸€æ¬¡æ€§å¤„ç†è¿™äº›æ–‡æ¡£ï¼Œå³ä½¿èƒ½ï¼Œå“åº”çš„è´¨é‡ä¹Ÿæ— æ³•è¾¾åˆ°æ ‡å‡†ï¼Œå°¤å…¶æ˜¯è€ƒè™‘åˆ°å®ƒæ‰€äº§ç”Ÿçš„æˆæœ¬ã€‚ä¸ºäº†å±•ç¤ºè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªçœŸå®ä¸–ç•Œçš„ä¾‹å­ï¼šå°è¯•æ€»ç»“
    GitLab çš„å‘˜å·¥æ‰‹å†Œï¼Œå¯ä»¥ä»[**è¿™é‡Œ**](https://kocielnik.gitlab.io/gitlab_handbook_takeaway/about-the-handbook.html)ä¸‹è½½ã€‚è¯¥æ–‡æ¡£åœ¨
    MIT è®¸å¯è¯ä¸‹å…è´¹æä¾›ï¼Œå¯ä»¥åœ¨ä»–ä»¬çš„ GitHub [ä»“åº“](https://gitlab.com/kocielnik/gitlab_handbook_takeaway/-/blob/master/LICENSE)æ‰¾åˆ°ã€‚
- en: 1 We start by loading the document and also initialize our LLM, to keep this
    exercise relevant I will make use of GPT-4o.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 1 æˆ‘ä»¬é¦–å…ˆåŠ è½½æ–‡æ¡£ï¼Œå¹¶åˆå§‹åŒ–æˆ‘ä»¬çš„ LLMï¼Œä¸ºäº†ä½¿è¿™ä¸ªç»ƒä¹ æ›´æœ‰ç›¸å…³æ€§ï¼Œæˆ‘å°†ä½¿ç”¨ GPT-4oã€‚
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 2 Then we can divide the document into smaller chunks (this is for *embedding*,
    I will explain why in the later steps).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 2 ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥å°†æ–‡æ¡£åˆ†å‰²æˆæ›´å°çš„å—ï¼ˆè¿™æ˜¯ä¸ºäº† *åµŒå…¥*ï¼Œæˆ‘ä¼šåœ¨åç»­æ­¥éª¤ä¸­è§£é‡Šä¸ºä»€ä¹ˆè¿™ä¹ˆåšï¼‰ã€‚
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 3 Now, letâ€™s calculate how many tokens make up this document, for this we will
    iterate through each document chunk and calculate the total tokens that make up
    the document.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 3 ç°åœ¨ï¼Œè®©æˆ‘ä»¬è®¡ç®—è¿™ä¸ªæ–‡æ¡£åŒ…å«å¤šå°‘ä¸ªä»¤ç‰Œï¼Œä¸ºæ­¤æˆ‘ä»¬å°†éå†æ¯ä¸ªæ–‡æ¡£å—å¹¶è®¡ç®—å‡ºæ–‡æ¡£æ€»å…±åŒ…å«çš„ä»¤ç‰Œæ•°ã€‚
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As we can see the number of tokens is 254,006, while the context window limit
    for GPT-4o is 128,000\. This document cannot be sent in one go through the LLMâ€™s
    API. In addition to this, considering this model's pricing is $0.00500 / 1K input
    tokens, a single request sent to OpenAI for this document would cost $1.27! This
    does not sound horrible until you present this in an enterprise paradigm with
    multiple users and daily interactions across many such large documents, especially
    in a startup scenario where many GenAI solutions are being born.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œä»¤ç‰Œæ•°é‡ä¸º 254,006ï¼Œè€Œ GPT-4o çš„ä¸Šä¸‹æ–‡çª—å£é™åˆ¶ä¸º 128,000ã€‚è¿™ä¸ªæ–‡æ¡£æ— æ³•é€šè¿‡ LLM çš„ API ä¸€æ¬¡æ€§å‘é€å‡ºå»ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œè€ƒè™‘åˆ°è¯¥æ¨¡å‹çš„å®šä»·ä¸ºæ¯
    1K è¾“å…¥ä»¤ç‰Œ $0.00500ï¼Œä¸€æ¬¡è¯·æ±‚ OpenAI å¤„ç†è¿™ä¸ªæ–‡æ¡£å°†èŠ±è´¹ $1.27ï¼ç›´åˆ°ä½ æŠŠå®ƒæ”¾åœ¨ä¼ä¸šèŒƒç•´ä¸­ï¼Œé¢å¯¹å¤šä¸ªç”¨æˆ·ä»¥åŠå¤§é‡ç±»ä¼¼çš„æ–‡æ¡£æ—¥å¸¸äº¤äº’ï¼Œå°¤å…¶æ˜¯åœ¨è®¸å¤š
    GenAI è§£å†³æ–¹æ¡ˆæ­£åœ¨è¯ç”Ÿçš„åˆåˆ›å…¬å¸åœºæ™¯ä¸‹ï¼Œè¿™ä¸ªè´¹ç”¨å¬èµ·æ¥å¹¶ä¸é‚£ä¹ˆå¯æ€•ã€‚
- en: Lost in the Middle
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¿·å¤±åœ¨ä¸­é—´
- en: Another challenge faced by LLMs is the *Lost in the Middle,* context problem
    as discussed in detail in this [**paper**](https://arxiv.org/abs/2307.03172).
    Research and my experiences with RAG systems handling multiple documents describe
    that LLMs are not very robust when it comes to extrapolating information from
    long context inputs. Model performance degrades considerably when relevant information
    is somewhere in the middle of the context. However, the performance improves when
    the required information is either at the beginning or the end of the provided
    context. Document Re-ranking is a solution that has become a subject of progressively
    heavy discussion and research to tackle this specific issue. I will be exploring
    a few of these methods in another post. For now, let us get back to the solution
    we are exploring which utilizes K-Means Clustering.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¢ä¸´çš„å¦ä¸€ä¸ªæŒ‘æˆ˜æ˜¯*ä¸­é—´ä¸¢å¤±*çš„ä¸Šä¸‹æ–‡é—®é¢˜ï¼Œå…·ä½“å†…å®¹å¯ä»¥å‚è§è¿™ç¯‡[**è®ºæ–‡**](https://arxiv.org/abs/2307.03172)ã€‚æˆ‘çš„ç ”ç©¶å’ŒRAGç³»ç»Ÿåœ¨å¤„ç†å¤šä¸ªæ–‡æ¡£æ—¶çš„ç»éªŒè¡¨æ˜ï¼ŒLLMsåœ¨ä»é•¿ä¸Šä¸‹æ–‡è¾“å…¥ä¸­æ¨æ–­ä¿¡æ¯æ—¶å¹¶ä¸ååˆ†å¼ºå¤§ã€‚å½“ç›¸å…³ä¿¡æ¯ä½äºä¸Šä¸‹æ–‡çš„ä¸­é—´æ—¶ï¼Œæ¨¡å‹çš„è¡¨ç°ä¼šæ˜¾è‘—ä¸‹é™ã€‚ç„¶è€Œï¼Œå½“æ‰€éœ€ä¿¡æ¯ä½äºä¸Šä¸‹æ–‡çš„å¼€å§‹æˆ–ç»“å°¾æ—¶ï¼Œè¡¨ç°åˆ™ä¼šæœ‰æ‰€æå‡ã€‚æ–‡æ¡£é‡æ’åºï¼ˆDocument
    Re-rankingï¼‰å·²æˆä¸ºè§£å†³è¿™ä¸ªç‰¹å®šé—®é¢˜çš„ç ”ç©¶ä¸»é¢˜ã€‚æˆ‘å°†åœ¨å¦ä¸€ç¯‡æ–‡ç« ä¸­æ¢è®¨è¿™äº›æ–¹æ³•ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬å›åˆ°æˆ‘ä»¬æ­£åœ¨æ¢ç´¢çš„è§£å†³æ–¹æ¡ˆï¼Œå®ƒåˆ©ç”¨äº†Kå‡å€¼èšç±»ã€‚
- en: What is K-Means Clustering?!
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯Kå‡å€¼èšç±»ï¼Ÿï¼
- en: Okay, I admit I sneaked in a technical concept in the last section, allow me
    to explain it (for those who may not be aware of the method, I got you).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼Œæˆ‘æ‰¿è®¤åœ¨ä¸Šä¸€èŠ‚æˆ‘å·å·å¼•å…¥äº†ä¸€ä¸ªæŠ€æœ¯æ¦‚å¿µï¼Œè®©æˆ‘ä¸ºä½ è§£é‡Šä¸€ä¸‹ï¼ˆå¦‚æœä½ ä¸ç†Ÿæ‚‰è¯¥æ–¹æ³•ï¼Œæˆ‘ä¼šä¸ºä½ è¯¦ç»†è¯´æ˜ï¼‰ã€‚
- en: First the basics
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…ˆä»åŸºç¡€å¼€å§‹
- en: 'To understand K-means clustering, we should first know what clustering is.
    Consider this: we have a messy desk with pens, pencils, and notes all scattered
    together. To clean up, one would group like items together like all pens in one
    group, pencils in another, and notes in another creating essentially 3 separate
    groups (not promoting segregation). Clustering is the same process where among
    a collection of data (in our case the different chunks of document text), similar
    data or information are grouped creating a clear separation of concerns for the
    model, making it easier for our RAG system to pick and choose information effectively
    and efficiently instead of having to go through it all like a greedy method.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç†è§£Kå‡å€¼èšç±»ï¼Œæˆ‘ä»¬é¦–å…ˆåº”å½“çŸ¥é“ä»€ä¹ˆæ˜¯èšç±»ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€å¼ å‡Œä¹±çš„æ¡Œå­ï¼Œä¸Šé¢æœ‰é’¢ç¬”ã€é“…ç¬”å’Œä¾¿ç­¾çº¸æ‚ä¹±æ— ç« ã€‚ä¸ºäº†æ•´ç†ï¼Œå¯ä»¥å°†ç›¸ä¼¼çš„ç‰©å“å½’ä¸ºä¸€ç»„ï¼Œæ¯”å¦‚æ‰€æœ‰é’¢ç¬”æ”¾åœ¨ä¸€ç»„ï¼Œé“…ç¬”æ”¾åœ¨å¦ä¸€ç»„ï¼Œä¾¿ç­¾çº¸æ”¾åœ¨ç¬¬ä¸‰ç»„ï¼Œæœ€ç»ˆå½¢æˆ3ä¸ªç‹¬ç«‹çš„ç¾¤ç»„ï¼ˆå¹¶éæå€¡éš”ç¦»ï¼‰ã€‚èšç±»å°±æ˜¯è¿™ä¸ªè¿‡ç¨‹ï¼Œåœ¨æ•°æ®é›†åˆä¸­ï¼ˆåœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­æ˜¯æ–‡æ¡£æ–‡æœ¬çš„ä¸åŒç‰‡æ®µï¼‰ï¼Œå°†ç›¸ä¼¼çš„æ•°æ®æˆ–ä¿¡æ¯å½’ä¸ºä¸€ç»„ï¼Œä»è€Œåœ¨æ¨¡å‹ä¸­å½¢æˆæ¸…æ™°çš„åˆ†éš”ï¼Œä½¿å¾—æˆ‘ä»¬çš„RAGç³»ç»Ÿèƒ½å¤Ÿæ›´æœ‰æ•ˆç‡åœ°é€‰æ‹©å’Œæå–ä¿¡æ¯ï¼Œè€Œä¸æ˜¯åƒè´ªå©ªç®—æ³•é‚£æ ·éœ€è¦å¤„ç†æ‰€æœ‰æ•°æ®ã€‚
- en: K, Means?
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kï¼Œæ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ
- en: 'K-means is a specific method to perform clustering (there are other methods
    but letâ€™s not information dump). Let me explain how it works in 5 simple steps:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Kå‡å€¼æ˜¯ä¸€ç§ç‰¹å®šçš„èšç±»æ–¹æ³•ï¼ˆè™½ç„¶è¿˜æœ‰å…¶ä»–æ–¹æ³•ï¼Œä½†æˆ‘ä»¬å…ˆä¸è®¨è®ºè¿™äº›ï¼‰ã€‚è®©æˆ‘é€šè¿‡5ä¸ªç®€å•çš„æ­¥éª¤æ¥è§£é‡Šå®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼š
- en: '**Picking the number of groups (K)**: How many groups we want the data to be
    divided into'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**é€‰æ‹©ç¾¤ç»„æ•°é‡ï¼ˆKï¼‰**ï¼šæˆ‘ä»¬å¸Œæœ›å°†æ•°æ®åˆ’åˆ†ä¸ºå¤šå°‘ä¸ªç¾¤ç»„ã€‚'
- en: '**Selecting group centers**: Initially, a center value for each of the K-groups
    is randomly selected'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**é€‰æ‹©ç¾¤ç»„ä¸­å¿ƒ**ï¼šæœ€åˆï¼Œä¸ºæ¯ä¸ªKä¸ªç¾¤ç»„éšæœºé€‰æ‹©ä¸€ä¸ªä¸­å¿ƒå€¼ã€‚'
- en: '**Group assignment**: Each data point is then assigned to each group based
    on how close it is to the previously chosen centers. Example: items closest to
    center 1 are assigned to group 1, items closest to center 2 will be assigned to
    group 2â€¦and so on till Kth group.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ç¾¤ç»„åˆ†é…**ï¼šç„¶åï¼Œæ ¹æ®æ¯ä¸ªæ•°æ®ç‚¹ä¸å…ˆå‰é€‰æ‹©çš„ä¸­å¿ƒçš„è·ç¦»ï¼Œå°†æ•°æ®ç‚¹åˆ†é…ç»™å„ä¸ªç¾¤ç»„ã€‚ç¤ºä¾‹ï¼šè·ç¦»ä¸­å¿ƒ1æœ€è¿‘çš„é¡¹è¢«åˆ†é…åˆ°ç¾¤ç»„1ï¼Œè·ç¦»ä¸­å¿ƒ2æœ€è¿‘çš„é¡¹è¢«åˆ†é…åˆ°ç¾¤ç»„2â€¦â€¦ä¾æ­¤ç±»æ¨ï¼Œç›´åˆ°Kthç¾¤ç»„ã€‚'
- en: '**Adjusting the centers**: After all the data points have been pigeonholed,
    we calculate the average of the positions of the items in each group and these
    averages become the new centers to improve accuracy (because we had initially
    selected them at random).'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è°ƒæ•´ä¸­å¿ƒç‚¹**ï¼šåœ¨æ‰€æœ‰æ•°æ®ç‚¹è¢«å½’ç±»åï¼Œæˆ‘ä»¬è®¡ç®—æ¯ä¸ªç¾¤ç»„ä¸­é¡¹ç›®ä½ç½®çš„å¹³å‡å€¼ï¼Œè¿™äº›å¹³å‡å€¼å°†æˆä¸ºæ–°çš„ä¸­å¿ƒï¼Œä»¥æé«˜å‡†ç¡®æ€§ï¼ˆå› ä¸ºæˆ‘ä»¬æœ€åˆæ˜¯éšæœºé€‰æ‹©çš„ï¼‰ã€‚'
- en: '**Rinse and repeat:** With the new centers, the data point assignments are
    again updated for the K-groups. This is done till the difference (mathematically
    the ***Euclidean* *distance***) is minimal for items within a group and the maximal
    from other data points of other groups, ergo optimal segregation.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**é‡å¤è¿›è¡Œ**ï¼šé€šè¿‡æ–°çš„ä¸­å¿ƒç‚¹ï¼Œæ•°æ®ç‚¹çš„åˆ†é…ä¼šå†æ¬¡æ›´æ–°ä¸ºKä¸ªç¾¤ç»„ã€‚è¿™ä¸€è¿‡ç¨‹ä¼šæŒç»­è¿›è¡Œï¼Œç›´åˆ°ç¾¤ç»„å†…éƒ¨çš„å·®å¼‚ï¼ˆä»æ•°å­¦è§’åº¦æ¥è¯´æ˜¯***æ¬§å‡ é‡Œå¾—* *è·ç¦»**ï¼‰æœ€å°ï¼ŒåŒæ—¶ä¸å…¶ä»–ç¾¤ç»„çš„å…¶ä»–æ•°æ®ç‚¹çš„å·®å¼‚æœ€å¤§ï¼Œä»è€Œå®ç°æœ€ä¼˜çš„åˆ†éš”ã€‚'
- en: While this may be quite a simplified explanation, a more detailed and technical
    explanation (for my fellow nerds) of this algorithm can be found [here](https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-k-means-clustering/).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶è¿™å¯èƒ½æ˜¯ä¸€ä¸ªç›¸å¯¹ç®€åŒ–çš„è§£é‡Šï¼Œä½†å¯¹äºæˆ‘çš„å°ä¼™ä¼´ä»¬ï¼ˆç§‘æŠ€çˆ±å¥½è€…ï¼‰ï¼Œè¿™ç®—æ³•çš„æ›´è¯¦ç»†å’ŒæŠ€æœ¯æ€§çš„è§£é‡Šå¯ä»¥åœ¨[è¿™é‡Œ](https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-k-means-clustering/)æ‰¾åˆ°ã€‚
- en: Enough theory, letâ€™s code.
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç†è®ºå¤Ÿäº†ï¼Œè®©æˆ‘ä»¬å¼€å§‹ç¼–ç å§ã€‚
- en: Now that we have discussed K-means clustering which is the main protagonist
    in our journey to optimization, let us see how this robust algorithm can be used
    in practice to summarize our Handbook.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»è®¨è®ºäº†K-meansèšç±»ï¼Œè¿™æ˜¯æˆ‘ä»¬ä¼˜åŒ–ä¹‹æ—…ä¸­çš„ä¸»è§’ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªå¼ºå¤§çš„ç®—æ³•å¦‚ä½•åœ¨å®é™…ä¸­åº”ç”¨ï¼Œæ¥æ€»ç»“æˆ‘ä»¬çš„æ‰‹å†Œã€‚
- en: 4 Now that we have our chunks of document text, we will be embedding them into
    vectors.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»æœ‰äº†æ–‡æ¡£æ–‡æœ¬çš„å—ï¼Œæˆ‘ä»¬å°†æŠŠå®ƒä»¬åµŒå…¥åˆ°å‘é‡ä¸­ã€‚
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Maybe a little theory
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯èƒ½ç¨å¾®æœ‰ç‚¹ç†è®º
- en: Alright, alright so maybe thereâ€™s more to learn here â€” whatâ€™s embedding? Vectors?!
    and why?
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼Œå¥½å§ï¼Œä¹Ÿè®¸è¿™é‡Œè¿˜æœ‰æ›´å¤šè¦å­¦ä¹ çš„â€”â€”ä»€ä¹ˆæ˜¯åµŒå…¥ï¼Ÿå‘é‡ï¼Ÿï¼ä¸ºä»€ä¹ˆå‘¢ï¼Ÿ
- en: Embedding & Vectors
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åµŒå…¥ä¸å‘é‡
- en: Think of how a computer does things â€” it sees everything as binary, ergo the
    best language to teach/instruct it is in numbers. Hence, an optimal way to have
    complex ML systems understand our data is to see all that text as numbers, and
    that very method by which we do this conversion is called **Embedding**. The number
    list describing the text or word is known as **Vectors**.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ³æƒ³è®¡ç®—æœºæ˜¯å¦‚ä½•åšäº‹çš„â€”â€”å®ƒæŠŠæ‰€æœ‰ä¸œè¥¿çœ‹ä½œäºŒè¿›åˆ¶çš„ï¼Œå› æ­¤ï¼Œæ•™/æŒ‡ä»¤å®ƒçš„æœ€ä½³è¯­è¨€å°±æ˜¯æ•°å­—ã€‚å› æ­¤ï¼Œè®©å¤æ‚çš„æœºå™¨å­¦ä¹ ç³»ç»Ÿç†è§£æˆ‘ä»¬çš„æ•°æ®çš„æœ€ä½³æ–¹æ³•ï¼Œå°±æ˜¯æŠŠæ‰€æœ‰çš„æ–‡æœ¬çœ‹ä½œæ•°å­—ï¼Œè€Œæˆ‘ä»¬è¿›è¡Œè¿™ç§è½¬æ¢çš„æ–¹æ³•å«åš**åµŒå…¥ï¼ˆEmbeddingï¼‰**ã€‚æè¿°æ–‡æœ¬æˆ–å•è¯çš„æ•°å­—åˆ—è¡¨ç§°ä¸º**å‘é‡ï¼ˆVectorsï¼‰**ã€‚
- en: Embeddings can differ depending on how we want to describe our data and the
    heuristics we choose. Letâ€™s say we wanted to describe an apple, we need to consider
    its color (Red), its shape (Roundness), and its size. Each of these could be encoded
    as numbers, like the â€˜rednessâ€™ could be an 8 on a scale of 1â€“10\. The roundness
    could be 9 and the size could be 3 (inches in width). Hence, our vector for representing
    the apple would be [8,9,3]. This very concept is applied in more complexity when
    describing different qualities of documents where we want each number to map the
    topic, the semantic relationships, etc. This would result in vectors having hundreds
    or more numbers long.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: åµŒå…¥ï¼ˆEmbeddingsï¼‰å¯ä»¥æ ¹æ®æˆ‘ä»¬å¦‚ä½•æè¿°æ•°æ®ä»¥åŠé€‰æ‹©çš„å¯å‘å¼æ–¹æ³•è€Œæœ‰æ‰€ä¸åŒã€‚å‡è®¾æˆ‘ä»¬æƒ³æè¿°ä¸€ä¸ªè‹¹æœï¼Œæˆ‘ä»¬éœ€è¦è€ƒè™‘å®ƒçš„é¢œè‰²ï¼ˆçº¢è‰²ï¼‰ã€å½¢çŠ¶ï¼ˆåœ†å½¢ï¼‰å’Œå¤§å°ã€‚æ¯ä¸€ä¸ªç‰¹å¾éƒ½å¯ä»¥ç”¨æ•°å­—æ¥ç¼–ç ï¼Œæ¯”å¦‚â€˜çº¢è‰²â€™åœ¨1åˆ°10çš„èŒƒå›´å†…å¯ä»¥ç”¨8è¡¨ç¤ºï¼Œåœ†å½¢åº¦å¯ä»¥ç”¨9è¡¨ç¤ºï¼Œå¤§å°å¯ä»¥ç”¨3è¡¨ç¤ºï¼ˆå®½åº¦ä¸º3è‹±å¯¸ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æè¿°è‹¹æœçš„å‘é‡ä¼šæ˜¯[8,9,3]ã€‚å½“æè¿°æ–‡æ¡£çš„ä¸åŒç‰¹å¾æ—¶ï¼Œè¿™ä¸€æ¦‚å¿µä¼šå˜å¾—æ›´åŠ å¤æ‚ï¼Œæˆ‘ä»¬å¸Œæœ›æ¯ä¸ªæ•°å­—éƒ½èƒ½æ˜ å°„åˆ°ä¸»é¢˜ã€è¯­ä¹‰å…³ç³»ç­‰æ–¹é¢ã€‚è¿™ä¼šå¯¼è‡´å‘é‡åŒ…å«æ•°ç™¾ä¸ªæˆ–æ›´å¤šçš„æ•°å­—ã€‚
- en: But, Why?!
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œä¸ºä»€ä¹ˆï¼Ÿï¼
- en: 'Now, what improvements does this method provide? Firstly as I mentioned before,
    it makes data interpretation for the LLMs easier which provides better accuracy
    in inference from the models. Secondly, it also helps massively, in memory optimization
    (space complexity in technical terms), by reducing the amount of memory consumption
    by converting the data into vectors. The paradigm of vectors is known as vector
    space, for example: A document with 1000 words can be reduced to a 768-dimensional
    vector representation, hence, resulting in a 768 number representation instead
    of 1000 words.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè¿™ç§æ–¹æ³•æä¾›äº†ä»€ä¹ˆæ”¹è¿›å‘¢ï¼Ÿé¦–å…ˆï¼Œæ­£å¦‚æˆ‘ä¹‹å‰æåˆ°çš„ï¼Œå®ƒä½¿å¾—LLMsï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰çš„æ•°æ®è§£è¯»å˜å¾—æ›´åŠ å®¹æ˜“ï¼Œä»è€Œæé«˜äº†æ¨¡å‹æ¨ç†çš„å‡†ç¡®æ€§ã€‚å…¶æ¬¡ï¼Œå®ƒåœ¨å†…å­˜ä¼˜åŒ–ï¼ˆæŠ€æœ¯æœ¯è¯­ä¸­ç§°ä¸ºç©ºé—´å¤æ‚åº¦ï¼‰æ–¹é¢ä¹Ÿæœ‰å·¨å¤§å¸®åŠ©ï¼Œé€šè¿‡å°†æ•°æ®è½¬æ¢ä¸ºå‘é‡æ¥å‡å°‘å†…å­˜æ¶ˆè€—ã€‚å‘é‡çš„èŒƒå¼è¢«ç§°ä¸ºå‘é‡ç©ºé—´ï¼Œä¾‹å¦‚ï¼šä¸€ç¯‡åŒ…å«1000ä¸ªå•è¯çš„æ–‡æ¡£å¯ä»¥è¢«å‹ç¼©æˆä¸€ä¸ª768ç»´çš„å‘é‡è¡¨ç¤ºï¼Œä»è€Œäº§ç”Ÿä¸€ä¸ª768ä¸ªæ•°å­—çš„è¡¨ç¤ºï¼Œè€Œä¸æ˜¯1000ä¸ªå•è¯ã€‚
- en: A little deeper math (for my dear nerds again), â€œ1234â€ in word (or strings in
    computer language) form would consume 54 bytes of memory, while the 1234 in numeral
    (integers in computer language) form would consume only 8 bytes! So if you were
    to consider documents consuming Megabytes, we are reducing memory management costs
    as well (yay, budget!).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´æ·±ä¸€ç‚¹çš„æ•°å­¦ï¼ˆå†æ¬¡ä¸ºæˆ‘çš„å°ä¼™ä¼´ä»¬å‡†å¤‡çš„ï¼‰ï¼Œåœ¨è®¡ç®—æœºè¯­è¨€ä¸­ï¼Œ"1234"ï¼ˆä½œä¸ºå•è¯æˆ–å­—ç¬¦ä¸²ï¼‰å°†æ¶ˆè€—54å­—èŠ‚çš„å†…å­˜ï¼Œè€Œæ•°å­—å½¢å¼çš„1234ï¼ˆä½œä¸ºæ•´æ•°ï¼‰åªä¼šæ¶ˆè€—8å­—èŠ‚ï¼å› æ­¤ï¼Œå¦‚æœè€ƒè™‘æ–‡æ¡£æ¶ˆè€—çš„å…†å­—èŠ‚ï¼Œæˆ‘ä»¬ä¹Ÿåœ¨å‡å°‘å†…å­˜ç®¡ç†æˆæœ¬ï¼ˆå¤ªæ£’äº†ï¼Œé¢„ç®—ï¼ï¼‰ã€‚
- en: And we are back!
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å›æ¥äº†ï¼
- en: 5 Using the Scikit-Learn Python library for easy implementation, we first select
    the number of clusters we want, in our case 15\. We then run the algorithm to
    fit our embedded documents into 15 clusters. The parameter â€˜random_state = 42â€™
    means that we are shuffling the dataset to prevent pattern bias in our model.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 5 ä½¿ç”¨Scikit-Learn Pythonåº“è¿›è¡Œç®€å•å®ç°æ—¶ï¼Œæˆ‘ä»¬é¦–å…ˆé€‰æ‹©æˆ‘ä»¬æƒ³è¦çš„èšç±»æ•°ï¼Œåœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­æ˜¯15ä¸ªã€‚ç„¶åæˆ‘ä»¬è¿è¡Œç®—æ³•ï¼Œå°†åµŒå…¥çš„æ–‡æ¡£æ‹Ÿåˆæˆ15ä¸ªèšç±»ã€‚å‚æ•°â€˜random_state
    = 42â€™æ„å‘³ç€æˆ‘ä»¬æ­£åœ¨æ‰“ä¹±æ•°æ®é›†ï¼Œä»¥é˜²æ­¢æ¨¡å‹ä¸­çš„æ¨¡å¼åå·®ã€‚
- en: It is also important to note that we are converting our list of embeddings into
    a Numpy array (a mathematical representation of vectors for advanced operation
    in the Numpy library). This is because Scikit-learn requires Numpy arrays for
    K-means operation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬æ­£åœ¨å°†åµŒå…¥å‘é‡åˆ—è¡¨è½¬æ¢ä¸ºNumpyæ•°ç»„ï¼ˆNumpyåº“ä¸­ç”¨äºé«˜çº§æ“ä½œçš„å‘é‡æ•°å­¦è¡¨ç¤ºï¼‰ã€‚è¿™æ˜¯å› ä¸ºScikit-learnè¦æ±‚K-meansæ“ä½œä½¿ç”¨Numpyæ•°ç»„ã€‚
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Class dismissedâ€¦for now.
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¯¾ç¨‹ç»“æŸäº†...æš‚æ—¶ã€‚
- en: I think this is a good place for a pit stop! We have covered much both in code
    and theory. But no worries, I will be posting a second part covering how we make
    use of these clusters in generating rich summaries for large documents. There
    are going to be more interesting techniques to showcase and of course, I will
    be explaining through all the theory and understanding as best as I can!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªä¼‘æ¯çš„å¥½æ—¶æœºï¼æˆ‘ä»¬å·²ç»åœ¨ä»£ç å’Œç†è®ºæ–¹é¢è¦†ç›–äº†å¾ˆå¤šå†…å®¹ã€‚ä½†åˆ«æ‹…å¿ƒï¼Œæˆ‘å°†å‘å¸ƒç¬¬äºŒéƒ¨åˆ†ï¼Œä»‹ç»æˆ‘ä»¬å¦‚ä½•åˆ©ç”¨è¿™äº›èšç±»ç”Ÿæˆå¤§å‹æ–‡æ¡£çš„ä¸°å¯Œæ‘˜è¦ã€‚å°†ä¼šå±•ç¤ºæ›´å¤šæœ‰è¶£çš„æŠ€æœ¯ï¼Œå½“ç„¶ï¼Œæˆ‘ä¹Ÿä¼šå°½æˆ‘æ‰€èƒ½è§£é‡Šæ‰€æœ‰çš„ç†è®ºå’Œç†è§£ï¼
- en: So stay tuned! Also, I would love your feedback and any comments you may have
    regarding this article, as it really helps me improve my content, and as always,
    Thank you so much for trading and I hope it was worth the read!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æ•¬è¯·å…³æ³¨ï¼æ­¤å¤–ï¼Œæˆ‘éå¸¸æœŸå¾…æ‚¨çš„åé¦ˆå’Œä»»ä½•è¯„è®ºï¼Œè¿™å¯¹æˆ‘æ”¹è¿›å†…å®¹éå¸¸æœ‰å¸®åŠ©ï¼Œåƒå¾€å¸¸ä¸€æ ·ï¼Œéå¸¸æ„Ÿè°¢æ‚¨çš„é˜…è¯»ï¼Œå¸Œæœ›å®ƒå€¼å¾—æ‚¨èŠ±æ—¶é—´é˜…è¯»ï¼
- en: '![](../Images/278c53b9d5784ee5ee150b91ba30a43d.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/278c53b9d5784ee5ee150b91ba30a43d.png)'
- en: Photo by [Priscilla Du Preez ğŸ‡¨ğŸ‡¦](https://unsplash.com/@priscilladupreez?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æ‘„å½±ï¼šç”±[Priscilla Du Preez ğŸ‡¨ğŸ‡¦](https://unsplash.com/@priscilladupreez?utm_source=medium&utm_medium=referral)æä¾›ï¼Œç…§ç‰‡æ¥æºäº[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
