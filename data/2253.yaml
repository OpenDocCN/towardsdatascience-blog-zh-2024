- en: The Essential Guide to Effectively Summarizing Massive Documents, Part 1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高效总结海量文档的基本指南，第1部分
- en: 原文：[https://towardsdatascience.com/demystifying-document-digestion-a-deep-dive-into-summarizing-massive-documents-part-1-53f2ed9a669d?source=collection_archive---------5-----------------------#2024-09-14](https://towardsdatascience.com/demystifying-document-digestion-a-deep-dive-into-summarizing-massive-documents-part-1-53f2ed9a669d?source=collection_archive---------5-----------------------#2024-09-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/demystifying-document-digestion-a-deep-dive-into-summarizing-massive-documents-part-1-53f2ed9a669d?source=collection_archive---------5-----------------------#2024-09-14](https://towardsdatascience.com/demystifying-document-digestion-a-deep-dive-into-summarizing-massive-documents-part-1-53f2ed9a669d?source=collection_archive---------5-----------------------#2024-09-14)
- en: Document summarization is important for GenAI use-cases, but what if the documents
    are too BIG!? Read on to find out how I have solved it.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文档摘要在生成式人工智能（GenAI）应用中非常重要，但如果文档太大了怎么办！？继续阅读，了解我是如何解决这个问题的。
- en: '[](https://medium.com/@vinayak.sengupta?source=post_page---byline--53f2ed9a669d--------------------------------)[![Vinayak
    Sengupta](../Images/ae66b684329fd9e4f34cf1c21e0b3b57.png)](https://medium.com/@vinayak.sengupta?source=post_page---byline--53f2ed9a669d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--53f2ed9a669d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--53f2ed9a669d--------------------------------)
    [Vinayak Sengupta](https://medium.com/@vinayak.sengupta?source=post_page---byline--53f2ed9a669d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@vinayak.sengupta?source=post_page---byline--53f2ed9a669d--------------------------------)[![Vinayak
    Sengupta](../Images/ae66b684329fd9e4f34cf1c21e0b3b57.png)](https://medium.com/@vinayak.sengupta?source=post_page---byline--53f2ed9a669d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--53f2ed9a669d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--53f2ed9a669d--------------------------------)
    [Vinayak Sengupta](https://medium.com/@vinayak.sengupta?source=post_page---byline--53f2ed9a669d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--53f2ed9a669d--------------------------------)
    ·8 min read·Sep 14, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--53f2ed9a669d--------------------------------)
    ·8分钟阅读·2024年9月14日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4a30c3d146fa1c3d85a1f30b369a5e24.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4a30c3d146fa1c3d85a1f30b369a5e24.png)'
- en: “Summarizing a lot of text”— Image generated with GPT-4o
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: “总结大量文本”——使用GPT-4o生成的图像
- en: Document summarization today has become one of the most (if not the most) common
    problem statements to solve using modern Generative AI (GenAI) technology. Retrieval
    Augmented Generation (RAG) is a common yet effective solution architecture used
    to solve it (If you want a deeper dive into what RAG is, check out this [**blog**](https://medium.com/@vinayak.sengupta/exploring-the-core-of-augmented-intelligence-advancing-the-power-of-retrievers-in-rag-frameworks-3ef9fe273764)!).
    But what if the document itself is so large that it cannot be sent as a whole
    in a single API request? Or what if it produces too many chunks to cause the infamous
    ‘Lost in the Middle’ context problem? In this article, I will discuss the challenges
    we face with such a problem statement, and go through a step-by-step solution
    that I applied using the guidance offered by Greg Kamradt in his [**GitHub repository**](https://github.com/gkamradt/langchain-tutorials/blob/main/data_generation/5%20Levels%20Of%20Summarization%20-%20Novice%20To%20Expert.ipynb).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，文档摘要已成为使用现代生成式人工智能（GenAI）技术解决的最常见问题之一（如果不是最常见的话）。检索增强生成（RAG）是一个常见且有效的解决架构，用于解决这一问题（如果你想深入了解RAG是什么，可以查看这篇[**博客**](https://medium.com/@vinayak.sengupta/exploring-the-core-of-augmented-intelligence-advancing-the-power-of-retrievers-in-rag-frameworks-3ef9fe273764)！）。但是，如果文档本身太大，以至于无法在一次API请求中全部发送该怎么办？或者，如果它生成了太多的片段，导致著名的“迷失于中间”上下文问题怎么办？在本文中，我将讨论我们在面对此类问题时所遇到的挑战，并逐步介绍我使用Greg
    Kamradt在其[**GitHub仓库**](https://github.com/gkamradt/langchain-tutorials/blob/main/data_generation/5%20Levels%20Of%20Summarization%20-%20Novice%20To%20Expert.ipynb)提供的指导所应用的解决方案。
- en: Some “c*ontext”*
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一些“*上下文*”
- en: RAG is a well-discussed and widely implemented solution for addressing document
    summarizing optimization using GenAI technologies. However, like any new technology
    or solution, it is prone to edge-case challenges, especially in today’s enterprise
    environment. Two main concerns are contextual length coupled with per-prompt cost
    and the previously mentioned ‘Lost in the Middle’ context problem. Let’s dive
    a bit deeper to understand these challenges.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 是一种广泛讨论并且广泛实施的解决方案，用于利用 GenAI 技术优化文档总结。然而，像任何新技术或解决方案一样，它也容易面临边缘案例的挑战，尤其是在今天的企业环境中。两个主要问题是上下文长度与每次提示的成本以及前面提到的“迷失在中间”的上下文问题。让我们深入探讨一下这些挑战。
- en: '**Note***:* *I will be performing the exercises in Python using the LangChain,
    Scikit-Learn, Numpy and Matplotlib libraries for quick iterations.*'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意**：*我将使用 LangChain、Scikit-Learn、Numpy 和 Matplotlib 库来进行 Python 练习，以便快速迭代。*'
- en: Context window and Cost constraints
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 上下文窗口和成本限制
- en: Today with automated workflows enabled by GenAI, analyzing big documents has
    become an industry expectation/requirement. People want to quickly find relevant
    information from medical reports or financial audits by just prompting the LLM.
    But there is a caveat, enterprise documents are not like documents or datasets
    we deal with in academics, the sizes are considerably bigger and the pertinent
    information can be present pretty much anywhere in the documents. Hence, methods
    like data cleaning/filtering are often not a viable option since domain knowledge
    regarding these documents is not always given.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，借助 GenAI 启用的自动化工作流，分析大型文档已经成为行业的期望/要求。人们希望通过简单地提示 LLM，就能快速从医疗报告或财务审计中找到相关信息。但有一个警告，企业文档与我们在学术领域处理的文档或数据集不同，文档的大小通常要大得多，相关信息可能出现在文档的任何地方。因此，像数据清理/过滤这样的方式通常并不是一个可行的选择，因为这些文档的领域知识并不总是可用的。
- en: In addition to this, even the latest Large Language Models (LLMs) like GPT-4o
    by OpenAI with context windows of 128K tokens cannot just consume these documents
    in one shot or even if they did, the quality of response will not meet standards,
    especially for the cost it will incur. To showcase this, let’s take a real-world
    example of trying to summarize the Employee Handbook of GitLab which can downloaded
    [**here**](https://kocielnik.gitlab.io/gitlab_handbook_takeaway/about-the-handbook.html).
    This document is available free of charge under the MIT license available on their
    GitHub [repository](https://gitlab.com/kocielnik/gitlab_handbook_takeaway/-/blob/master/LICENSE).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，即使是像 OpenAI 的 GPT-4o 这样最新的 大型语言模型（LLM），上下文窗口为 128K 令牌，也不能一次性处理这些文档，即使能，响应的质量也无法达到标准，尤其是考虑到它所产生的成本。为了展示这一点，我们来看一个真实世界的例子：尝试总结
    GitLab 的员工手册，可以从[**这里**](https://kocielnik.gitlab.io/gitlab_handbook_takeaway/about-the-handbook.html)下载。该文档在
    MIT 许可证下免费提供，可以在他们的 GitHub [仓库](https://gitlab.com/kocielnik/gitlab_handbook_takeaway/-/blob/master/LICENSE)找到。
- en: 1 We start by loading the document and also initialize our LLM, to keep this
    exercise relevant I will make use of GPT-4o.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 1 我们首先加载文档，并初始化我们的 LLM，为了使这个练习更有相关性，我将使用 GPT-4o。
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 2 Then we can divide the document into smaller chunks (this is for *embedding*,
    I will explain why in the later steps).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 2 然后，我们可以将文档分割成更小的块（这是为了 *嵌入*，我会在后续步骤中解释为什么这么做）。
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 3 Now, let’s calculate how many tokens make up this document, for this we will
    iterate through each document chunk and calculate the total tokens that make up
    the document.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 3 现在，让我们计算这个文档包含多少个令牌，为此我们将遍历每个文档块并计算出文档总共包含的令牌数。
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As we can see the number of tokens is 254,006, while the context window limit
    for GPT-4o is 128,000\. This document cannot be sent in one go through the LLM’s
    API. In addition to this, considering this model's pricing is $0.00500 / 1K input
    tokens, a single request sent to OpenAI for this document would cost $1.27! This
    does not sound horrible until you present this in an enterprise paradigm with
    multiple users and daily interactions across many such large documents, especially
    in a startup scenario where many GenAI solutions are being born.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，令牌数量为 254,006，而 GPT-4o 的上下文窗口限制为 128,000。这个文档无法通过 LLM 的 API 一次性发送出去。除此之外，考虑到该模型的定价为每
    1K 输入令牌 $0.00500，一次请求 OpenAI 处理这个文档将花费 $1.27！直到你把它放在企业范畴中，面对多个用户以及大量类似的文档日常交互，尤其是在许多
    GenAI 解决方案正在诞生的初创公司场景下，这个费用听起来并不那么可怕。
- en: Lost in the Middle
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迷失在中间
- en: Another challenge faced by LLMs is the *Lost in the Middle,* context problem
    as discussed in detail in this [**paper**](https://arxiv.org/abs/2307.03172).
    Research and my experiences with RAG systems handling multiple documents describe
    that LLMs are not very robust when it comes to extrapolating information from
    long context inputs. Model performance degrades considerably when relevant information
    is somewhere in the middle of the context. However, the performance improves when
    the required information is either at the beginning or the end of the provided
    context. Document Re-ranking is a solution that has become a subject of progressively
    heavy discussion and research to tackle this specific issue. I will be exploring
    a few of these methods in another post. For now, let us get back to the solution
    we are exploring which utilizes K-Means Clustering.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）面临的另一个挑战是*中间丢失*的上下文问题，具体内容可以参见这篇[**论文**](https://arxiv.org/abs/2307.03172)。我的研究和RAG系统在处理多个文档时的经验表明，LLMs在从长上下文输入中推断信息时并不十分强大。当相关信息位于上下文的中间时，模型的表现会显著下降。然而，当所需信息位于上下文的开始或结尾时，表现则会有所提升。文档重排序（Document
    Re-ranking）已成为解决这个特定问题的研究主题。我将在另一篇文章中探讨这些方法。现在，让我们回到我们正在探索的解决方案，它利用了K均值聚类。
- en: What is K-Means Clustering?!
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是K均值聚类？！
- en: Okay, I admit I sneaked in a technical concept in the last section, allow me
    to explain it (for those who may not be aware of the method, I got you).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我承认在上一节我偷偷引入了一个技术概念，让我为你解释一下（如果你不熟悉该方法，我会为你详细说明）。
- en: First the basics
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 先从基础开始
- en: 'To understand K-means clustering, we should first know what clustering is.
    Consider this: we have a messy desk with pens, pencils, and notes all scattered
    together. To clean up, one would group like items together like all pens in one
    group, pencils in another, and notes in another creating essentially 3 separate
    groups (not promoting segregation). Clustering is the same process where among
    a collection of data (in our case the different chunks of document text), similar
    data or information are grouped creating a clear separation of concerns for the
    model, making it easier for our RAG system to pick and choose information effectively
    and efficiently instead of having to go through it all like a greedy method.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解K均值聚类，我们首先应当知道什么是聚类。假设我们有一张凌乱的桌子，上面有钢笔、铅笔和便签纸杂乱无章。为了整理，可以将相似的物品归为一组，比如所有钢笔放在一组，铅笔放在另一组，便签纸放在第三组，最终形成3个独立的群组（并非提倡隔离）。聚类就是这个过程，在数据集合中（在我们的案例中是文档文本的不同片段），将相似的数据或信息归为一组，从而在模型中形成清晰的分隔，使得我们的RAG系统能够更有效率地选择和提取信息，而不是像贪婪算法那样需要处理所有数据。
- en: K, Means?
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K，是什么意思？
- en: 'K-means is a specific method to perform clustering (there are other methods
    but let’s not information dump). Let me explain how it works in 5 simple steps:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: K均值是一种特定的聚类方法（虽然还有其他方法，但我们先不讨论这些）。让我通过5个简单的步骤来解释它是如何工作的：
- en: '**Picking the number of groups (K)**: How many groups we want the data to be
    divided into'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择群组数量（K）**：我们希望将数据划分为多少个群组。'
- en: '**Selecting group centers**: Initially, a center value for each of the K-groups
    is randomly selected'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择群组中心**：最初，为每个K个群组随机选择一个中心值。'
- en: '**Group assignment**: Each data point is then assigned to each group based
    on how close it is to the previously chosen centers. Example: items closest to
    center 1 are assigned to group 1, items closest to center 2 will be assigned to
    group 2…and so on till Kth group.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**群组分配**：然后，根据每个数据点与先前选择的中心的距离，将数据点分配给各个群组。示例：距离中心1最近的项被分配到群组1，距离中心2最近的项被分配到群组2……依此类推，直到Kth群组。'
- en: '**Adjusting the centers**: After all the data points have been pigeonholed,
    we calculate the average of the positions of the items in each group and these
    averages become the new centers to improve accuracy (because we had initially
    selected them at random).'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**调整中心点**：在所有数据点被归类后，我们计算每个群组中项目位置的平均值，这些平均值将成为新的中心，以提高准确性（因为我们最初是随机选择的）。'
- en: '**Rinse and repeat:** With the new centers, the data point assignments are
    again updated for the K-groups. This is done till the difference (mathematically
    the ***Euclidean* *distance***) is minimal for items within a group and the maximal
    from other data points of other groups, ergo optimal segregation.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**重复进行**：通过新的中心点，数据点的分配会再次更新为K个群组。这一过程会持续进行，直到群组内部的差异（从数学角度来说是***欧几里得* *距离**）最小，同时与其他群组的其他数据点的差异最大，从而实现最优的分隔。'
- en: While this may be quite a simplified explanation, a more detailed and technical
    explanation (for my fellow nerds) of this algorithm can be found [here](https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-k-means-clustering/).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这可能是一个相对简化的解释，但对于我的小伙伴们（科技爱好者），这算法的更详细和技术性的解释可以在[这里](https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-k-means-clustering/)找到。
- en: Enough theory, let’s code.
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理论够了，让我们开始编码吧。
- en: Now that we have discussed K-means clustering which is the main protagonist
    in our journey to optimization, let us see how this robust algorithm can be used
    in practice to summarize our Handbook.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了K-means聚类，这是我们优化之旅中的主角，让我们看看这个强大的算法如何在实际中应用，来总结我们的手册。
- en: 4 Now that we have our chunks of document text, we will be embedding them into
    vectors.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了文档文本的块，我们将把它们嵌入到向量中。
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Maybe a little theory
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可能稍微有点理论
- en: Alright, alright so maybe there’s more to learn here — what’s embedding? Vectors?!
    and why?
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，好吧，也许这里还有更多要学习的——什么是嵌入？向量？！为什么呢？
- en: Embedding & Vectors
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入与向量
- en: Think of how a computer does things — it sees everything as binary, ergo the
    best language to teach/instruct it is in numbers. Hence, an optimal way to have
    complex ML systems understand our data is to see all that text as numbers, and
    that very method by which we do this conversion is called **Embedding**. The number
    list describing the text or word is known as **Vectors**.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 想想计算机是如何做事的——它把所有东西看作二进制的，因此，教/指令它的最佳语言就是数字。因此，让复杂的机器学习系统理解我们的数据的最佳方法，就是把所有的文本看作数字，而我们进行这种转换的方法叫做**嵌入（Embedding）**。描述文本或单词的数字列表称为**向量（Vectors）**。
- en: Embeddings can differ depending on how we want to describe our data and the
    heuristics we choose. Let’s say we wanted to describe an apple, we need to consider
    its color (Red), its shape (Roundness), and its size. Each of these could be encoded
    as numbers, like the ‘redness’ could be an 8 on a scale of 1–10\. The roundness
    could be 9 and the size could be 3 (inches in width). Hence, our vector for representing
    the apple would be [8,9,3]. This very concept is applied in more complexity when
    describing different qualities of documents where we want each number to map the
    topic, the semantic relationships, etc. This would result in vectors having hundreds
    or more numbers long.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入（Embeddings）可以根据我们如何描述数据以及选择的启发式方法而有所不同。假设我们想描述一个苹果，我们需要考虑它的颜色（红色）、形状（圆形）和大小。每一个特征都可以用数字来编码，比如‘红色’在1到10的范围内可以用8表示，圆形度可以用9表示，大小可以用3表示（宽度为3英寸）。因此，我们描述苹果的向量会是[8,9,3]。当描述文档的不同特征时，这一概念会变得更加复杂，我们希望每个数字都能映射到主题、语义关系等方面。这会导致向量包含数百个或更多的数字。
- en: But, Why?!
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 但是，为什么？！
- en: 'Now, what improvements does this method provide? Firstly as I mentioned before,
    it makes data interpretation for the LLMs easier which provides better accuracy
    in inference from the models. Secondly, it also helps massively, in memory optimization
    (space complexity in technical terms), by reducing the amount of memory consumption
    by converting the data into vectors. The paradigm of vectors is known as vector
    space, for example: A document with 1000 words can be reduced to a 768-dimensional
    vector representation, hence, resulting in a 768 number representation instead
    of 1000 words.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这种方法提供了什么改进呢？首先，正如我之前提到的，它使得LLMs（大型语言模型）的数据解读变得更加容易，从而提高了模型推理的准确性。其次，它在内存优化（技术术语中称为空间复杂度）方面也有巨大帮助，通过将数据转换为向量来减少内存消耗。向量的范式被称为向量空间，例如：一篇包含1000个单词的文档可以被压缩成一个768维的向量表示，从而产生一个768个数字的表示，而不是1000个单词。
- en: A little deeper math (for my dear nerds again), “1234” in word (or strings in
    computer language) form would consume 54 bytes of memory, while the 1234 in numeral
    (integers in computer language) form would consume only 8 bytes! So if you were
    to consider documents consuming Megabytes, we are reducing memory management costs
    as well (yay, budget!).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 更深一点的数学（再次为我的小伙伴们准备的），在计算机语言中，"1234"（作为单词或字符串）将消耗54字节的内存，而数字形式的1234（作为整数）只会消耗8字节！因此，如果考虑文档消耗的兆字节，我们也在减少内存管理成本（太棒了，预算！）。
- en: And we are back!
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们回来了！
- en: 5 Using the Scikit-Learn Python library for easy implementation, we first select
    the number of clusters we want, in our case 15\. We then run the algorithm to
    fit our embedded documents into 15 clusters. The parameter ‘random_state = 42’
    means that we are shuffling the dataset to prevent pattern bias in our model.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 5 使用Scikit-Learn Python库进行简单实现时，我们首先选择我们想要的聚类数，在我们的例子中是15个。然后我们运行算法，将嵌入的文档拟合成15个聚类。参数‘random_state
    = 42’意味着我们正在打乱数据集，以防止模型中的模式偏差。
- en: It is also important to note that we are converting our list of embeddings into
    a Numpy array (a mathematical representation of vectors for advanced operation
    in the Numpy library). This is because Scikit-learn requires Numpy arrays for
    K-means operation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要注意的是，我们正在将嵌入向量列表转换为Numpy数组（Numpy库中用于高级操作的向量数学表示）。这是因为Scikit-learn要求K-means操作使用Numpy数组。
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Class dismissed…for now.
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 课程结束了...暂时。
- en: I think this is a good place for a pit stop! We have covered much both in code
    and theory. But no worries, I will be posting a second part covering how we make
    use of these clusters in generating rich summaries for large documents. There
    are going to be more interesting techniques to showcase and of course, I will
    be explaining through all the theory and understanding as best as I can!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这是一个休息的好时机！我们已经在代码和理论方面覆盖了很多内容。但别担心，我将发布第二部分，介绍我们如何利用这些聚类生成大型文档的丰富摘要。将会展示更多有趣的技术，当然，我也会尽我所能解释所有的理论和理解！
- en: So stay tuned! Also, I would love your feedback and any comments you may have
    regarding this article, as it really helps me improve my content, and as always,
    Thank you so much for trading and I hope it was worth the read!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 所以敬请关注！此外，我非常期待您的反馈和任何评论，这对我改进内容非常有帮助，像往常一样，非常感谢您的阅读，希望它值得您花时间阅读！
- en: '![](../Images/278c53b9d5784ee5ee150b91ba30a43d.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/278c53b9d5784ee5ee150b91ba30a43d.png)'
- en: Photo by [Priscilla Du Preez 🇨🇦](https://unsplash.com/@priscilladupreez?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 摄影：由[Priscilla Du Preez 🇨🇦](https://unsplash.com/@priscilladupreez?utm_source=medium&utm_medium=referral)提供，照片来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
