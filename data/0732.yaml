- en: 'DIGITOUR: Automatic Digital Tours for Real-Estate Properties ğŸ '
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DIGITOURï¼šæˆ¿åœ°äº§æ•°å­—å¯¼è§ˆè‡ªåŠ¨åŒ– ğŸ 
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/digitour-automatic-digital-tours-for-real-estate-properties-b0a6d2f7d638?source=collection_archive---------10-----------------------#2024-03-18](https://towardsdatascience.com/digitour-automatic-digital-tours-for-real-estate-properties-b0a6d2f7d638?source=collection_archive---------10-----------------------#2024-03-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/digitour-automatic-digital-tours-for-real-estate-properties-b0a6d2f7d638?source=collection_archive---------10-----------------------#2024-03-18](https://towardsdatascience.com/digitour-automatic-digital-tours-for-real-estate-properties-b0a6d2f7d638?source=collection_archive---------10-----------------------#2024-03-18)
- en: An Automated Pipeline for Creating 3D Experiences from Equirectangular Images
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸€ç§è‡ªåŠ¨åŒ–ç®¡é“ï¼Œç”¨äºä»ç­‰è·çŸ©å½¢å›¾åƒåˆ›å»º3Dä½“éªŒ
- en: '[](https://medium.com/@prateekchhikara?source=post_page---byline--b0a6d2f7d638--------------------------------)[![Prateek
    Chhikara](../Images/4cabb40cbab34038c0f762b45d58bbba.png)](https://medium.com/@prateekchhikara?source=post_page---byline--b0a6d2f7d638--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b0a6d2f7d638--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b0a6d2f7d638--------------------------------)
    [Prateek Chhikara](https://medium.com/@prateekchhikara?source=post_page---byline--b0a6d2f7d638--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@prateekchhikara?source=post_page---byline--b0a6d2f7d638--------------------------------)[![Prateek
    Chhikara](../Images/4cabb40cbab34038c0f762b45d58bbba.png)](https://medium.com/@prateekchhikara?source=post_page---byline--b0a6d2f7d638--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b0a6d2f7d638--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b0a6d2f7d638--------------------------------)
    [Prateek Chhikara](https://medium.com/@prateekchhikara?source=post_page---byline--b0a6d2f7d638--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b0a6d2f7d638--------------------------------)
    Â·11 min readÂ·Mar 18, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b0a6d2f7d638--------------------------------)
    Â·é˜…è¯»æ—¶é—´11åˆ†é’ŸÂ·2024å¹´3æœˆ18æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 1\. Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. å¼•è¨€
- en: The demand for online real-estate tools has increased drastically due to the
    ease of accessibility to the Internet, especially in countries like India. There
    are many online real-estate platforms for owners, developers, and real-estate
    brokers to post properties for buying and renting purposes. Daily, these platforms
    receive 8,000 to 9,000 new listings. Until now, the users on these platforms view
    images, snapshots, or videos, which may not build the desired confidence to decide
    and finalize the deal. To overcome this challenge and to enhance user experience,
    virtual tours are a potential solution.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºäº’è”ç½‘çš„ä¾¿åˆ©æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å°åº¦ç­‰å›½å®¶ï¼Œåœ¨çº¿æˆ¿åœ°äº§å·¥å…·çš„éœ€æ±‚æ€¥å‰§å¢åŠ ã€‚è®¸å¤šåœ¨çº¿æˆ¿åœ°äº§å¹³å°ä¾›æˆ¿ä¸»ã€å¼€å‘å•†å’Œæˆ¿åœ°äº§ç»çºªäººå‘å¸ƒä¹°å–å’Œç§Ÿèµç›®çš„çš„æˆ¿äº§ä¿¡æ¯ã€‚æ¯å¤©ï¼Œè¿™äº›å¹³å°éƒ½ä¼šæ”¶åˆ°8,000åˆ°9,000ä¸ªæ–°çš„æˆ¿æºä¿¡æ¯ã€‚ç›´åˆ°ç°åœ¨ï¼Œç”¨æˆ·åœ¨è¿™äº›å¹³å°ä¸ŠæŸ¥çœ‹çš„æ˜¯å›¾åƒã€å¿«ç…§æˆ–è§†é¢‘ï¼Œè¿™å¯èƒ½æ— æ³•å»ºç«‹è¶³å¤Ÿçš„ä¿¡å¿ƒæ¥åšå‡ºå†³ç­–å¹¶æœ€ç»ˆå®Œæˆäº¤æ˜“ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜å¹¶æå‡ç”¨æˆ·ä½“éªŒï¼Œè™šæ‹Ÿå¯¼è§ˆæ˜¯ä¸€ä¸ªæ½œåœ¨çš„è§£å†³æ–¹æ¡ˆã€‚
- en: 'Virtual tours are images linked together, allowing viewers to experience a
    particular location remotely (*a frequently used example is Google Street View*
    [1]). Recently, the demand for virtual tours has increased as they provide better
    interaction with users/customers, especially in businesses like real-estate, hotels,
    restaurants, universities, schools, etc [2]. Broadly, there are three categories
    of virtual tours:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è™šæ‹Ÿå¯¼è§ˆæ˜¯å°†å›¾åƒè¿æ¥åœ¨ä¸€èµ·ï¼Œè®©è§‚ä¼—èƒ½å¤Ÿè¿œç¨‹ä½“éªŒç‰¹å®šåœ°ç‚¹ï¼ˆ*ä¸€ä¸ªå¸¸è§çš„ä¾‹å­æ˜¯Googleè¡—æ™¯*ï¼‰[1]ã€‚è¿‘å¹´æ¥ï¼Œè™šæ‹Ÿå¯¼è§ˆçš„éœ€æ±‚ä¸æ–­å¢åŠ ï¼Œå› ä¸ºå®ƒä»¬èƒ½æä¾›æ›´å¥½çš„ç”¨æˆ·/å®¢æˆ·äº’åŠ¨ï¼Œå°¤å…¶æ˜¯åœ¨æˆ¿åœ°äº§ã€é…’åº—ã€é¤å…ã€å¤§å­¦ã€å­¦æ ¡ç­‰å•†ä¸šé¢†åŸŸ[2]ã€‚å¤§è‡´è€Œè¨€ï¼Œè™šæ‹Ÿå¯¼è§ˆå¯ä»¥åˆ†ä¸ºä¸‰ç±»ï¼š
- en: '*2D video tours*,'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*2Dè§†é¢‘å¯¼è§ˆ*ï¼Œ'
- en: '*360â—¦ video-based virtual tours*, and'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*360Â°è§†é¢‘åŸºç¡€è™šæ‹Ÿå¯¼è§ˆ*ï¼Œä»¥åŠ'
- en: '*360â—¦ static image-based virtual tours*.'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*åŸºäº360Â°é™æ€å›¾åƒçš„è™šæ‹Ÿå¯¼è§ˆ*ã€‚'
- en: Compared to 2D video tours and 360â—¦ video-based tours, the static equirectangular
    image-based virtual tours provide more immersion and interactivity, thus leading
    to better decision-making and avoiding unnecessary visits.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸2Dè§†é¢‘å¯¼è§ˆå’ŒåŸºäº360Â°è§†é¢‘çš„å¯¼è§ˆç›¸æ¯”ï¼Œé™æ€çš„ç­‰è·çŸ©å½¢å›¾åƒåŸºç¡€è™šæ‹Ÿå¯¼è§ˆæä¾›äº†æ›´å¤šçš„æ²‰æµ¸æ„Ÿå’Œäº’åŠ¨æ€§ï¼Œä»è€Œæœ‰åŠ©äºæ›´å¥½çš„å†³ç­–å¹¶é¿å…ä¸å¿…è¦çš„è®¿é—®ã€‚
- en: ğŸš€ Before delving deep into the proposed work, I would like to mention that this
    work is published and available at the[**International Conference on Data Science
    & Management of Data (CODS-COMAD)*â€” 2023***](https://dl.acm.org/doi/proceedings/10.1145/3570991).
    âœ¨âœ¨âœ¨
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ğŸš€ åœ¨æ·±å…¥æ¢è®¨æè®®çš„å·¥ä½œä¹‹å‰ï¼Œæˆ‘æƒ³æåˆ°çš„æ˜¯ï¼Œæœ¬é¡¹å·¥ä½œå·²ç»å‘è¡¨ï¼Œå¹¶å¯ä»¥åœ¨[**æ•°æ®ç§‘å­¦ä¸æ•°æ®ç®¡ç†å›½é™…ä¼šè®®ï¼ˆCODS-COMADï¼‰â€”â€”2023**](https://dl.acm.org/doi/proceedings/10.1145/3570991)ä¸ŠæŸ¥é˜…ã€‚âœ¨âœ¨âœ¨
- en: '![](../Images/3e998bf0108bec1443f8275915922668.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e998bf0108bec1443f8275915922668.png)'
- en: '**Figure 1:** An example of Equirectangular image clicked at the entry of a
    house showing the living room. (Source: Image by the author)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾1ï¼š** åœ¨æˆ¿å±‹å…¥å£å¤„æ‹æ‘„çš„ç­‰è·çŸ©å½¢å›¾åƒç¤ºä¾‹ï¼Œæ˜¾ç¤ºäº†å®¢å…ã€‚ï¼ˆæ¥æºï¼šä½œè€…æä¾›çš„å›¾åƒï¼‰'
- en: 'Typically, the pipeline for creating a virtual tour consists of the following
    components:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œåˆ›å»ºè™šæ‹Ÿæ—…æ¸¸çš„æµç¨‹åŒ…æ‹¬ä»¥ä¸‹ç»„ä»¶ï¼š
- en: '**Equirectangular Image Capture:** An equirectangular image represents a spherical
    object as a 2D image (*as shown in* ***Figure 1***). It is a spherical panorama
    incorporating 180Â° vertical and 360Â° horizontal viewing angles. A simple example
    can be the projection of Earth (*a spherical-shaped object*) on a 2D map. These
    images are clicked using 360â—¦ cameras such as [Ricoh-Theta](https://theta360.com/en/etc/technology.html),
    [Insta3602](https://www.insta360.com/product/insta360-onex2), etc.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ç­‰è·çŸ©å½¢å›¾åƒæ•æ‰ï¼š** ç­‰è·çŸ©å½¢å›¾åƒå°†çƒå½¢ç‰©ä½“è¡¨ç¤ºä¸ºäºŒç»´å›¾åƒï¼ˆ*å¦‚***å›¾1***æ‰€ç¤ºï¼‰ã€‚å®ƒæ˜¯ä¸€ä¸ªåŒ…å«180Â°å‚ç›´å’Œ360Â°æ°´å¹³è§†è§’çš„çƒå½¢å…¨æ™¯ã€‚ä¸€ä¸ªç®€å•çš„ä¾‹å­å¯ä»¥æ˜¯å°†åœ°çƒï¼ˆ*ä¸€ä¸ªçƒå½¢ç‰©ä½“*ï¼‰æŠ•å½±åˆ°äºŒç»´åœ°å›¾ä¸Šã€‚è¿™äº›å›¾åƒæ˜¯é€šè¿‡ä½¿ç”¨360â—¦ç›¸æœºæ‹æ‘„çš„ï¼Œä¾‹å¦‚[Ricoh-Theta](https://theta360.com/en/etc/technology.html)ã€[Insta3602](https://www.insta360.com/product/insta360-onex2)ç­‰ã€‚'
- en: '**Connecting Equirectangular Images:** For any location, we will have multiple
    equirectangular images. To illustrate, in real estate properties, we typically
    have equilateral images for bedrooms, halls, kitchens, dining rooms, etc. It is
    essential to build navigation between images to have a complete â€œ*walkthrough*â€
    experience. Moreover, there can be multiple routes from one position to other
    positions. For instance, we can go from the hall to the kitchen, bedroom, balcony,
    etc. Therefore, it is crucial to connect all the equirectangular images (*please
    refer to* ***Figure 2*** *for an example*). Generally, it is done manually, which
    is both costly and time-consuming [3].'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è¿æ¥ç­‰è·çŸ©å½¢å›¾åƒï¼š** å¯¹äºä»»ä½•ä½ç½®ï¼Œæˆ‘ä»¬éƒ½ä¼šæœ‰å¤šä¸ªç­‰è·çŸ©å½¢å›¾åƒã€‚ä»¥æˆ¿åœ°äº§ä¸ºä¾‹ï¼Œæˆ‘ä»¬é€šå¸¸æœ‰å§å®¤ã€å®¢å…ã€å¨æˆ¿ã€é¤å…ç­‰çš„ç­‰è·çŸ©å½¢å›¾åƒã€‚å¿…é¡»åœ¨å›¾åƒä¹‹é—´å»ºç«‹å¯¼èˆªï¼Œä»¥æä¾›å®Œæ•´çš„â€œ*è™šæ‹Ÿæ¼«æ¸¸*â€ä½“éªŒã€‚æ­¤å¤–ï¼Œä»ä¸€ä¸ªä½ç½®åˆ°å…¶ä»–ä½ç½®å¯èƒ½æœ‰å¤šæ¡è·¯çº¿ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ä»å®¢å…èµ°åˆ°å¨æˆ¿ã€å§å®¤ã€é˜³å°ç­‰ã€‚å› æ­¤ï¼Œè¿æ¥æ‰€æœ‰ç­‰è·çŸ©å½¢å›¾åƒè‡³å…³é‡è¦ï¼ˆ*è¯·å‚è§*
    ***å›¾2*** *ä»¥äº†è§£ç¤ºä¾‹*ï¼‰ã€‚é€šå¸¸ï¼Œè¿™ä¸€è¿‡ç¨‹æ˜¯æ‰‹åŠ¨å®Œæˆçš„ï¼Œæ—¢æ˜‚è´µåˆè€—æ—¶[3]ã€‚'
- en: '**Publishing Virtual Tour:** Once we have formed connections between equirectangular
    images, we can publish the final virtual tour on the cloud or in an application.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å‘å¸ƒè™šæ‹Ÿæ—…æ¸¸ï¼š** ä¸€æ—¦æˆ‘ä»¬å®Œæˆäº†ç­‰è·çŸ©å½¢å›¾åƒä¹‹é—´çš„è¿æ¥ï¼Œå°±å¯ä»¥å°†æœ€ç»ˆçš„è™šæ‹Ÿæ—…æ¸¸å‘å¸ƒåˆ°äº‘ç«¯æˆ–åº”ç”¨ç¨‹åºä¸­ã€‚'
- en: '![](../Images/fc4ab6e0c944df1cdf01073844a0a7f9.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc4ab6e0c944df1cdf01073844a0a7f9.png)'
- en: '**Figure 2:** Example property floor plan (a) and itâ€™s connections (b) to make
    a digital tour. The numbers represent the position of tags and equirectangular
    images. (Source: Image by the author)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾2ï¼š** ç¤ºä¾‹æˆ¿äº§å¹³é¢å›¾ï¼ˆaï¼‰åŠå…¶è¿æ¥ï¼ˆbï¼‰ä»¥åˆ›å»ºæ•°å­—æ—…æ¸¸ã€‚æ•°å­—ä»£è¡¨æ ‡ç­¾å’Œç­‰è·çŸ©å½¢å›¾åƒçš„ä½ç½®ã€‚ï¼ˆæ¥æºï¼šä½œè€…æä¾›çš„å›¾åƒï¼‰'
- en: 'While automating the above pipeline, one of the significant challenges is the
    manual annotation for connecting equirectangular images. Generally, it takes more
    than 20 minutes to create a virtual tour. We propose an end-to-end pipeline for
    real-estate equirectangular images (*called* ***DIGITOUR***) to overcome the challenge
    of creating automated digital tours. The **DIGITOUR** pipeline consists of the
    following components:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è‡ªåŠ¨åŒ–ä¸Šè¿°æµç¨‹æ—¶ï¼Œé¢ä¸´çš„ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜æ˜¯æ‰‹åŠ¨æ³¨é‡Šä»¥è¿æ¥ç­‰è·çŸ©å½¢å›¾åƒã€‚é€šå¸¸ï¼Œåˆ›å»ºä¸€ä¸ªè™šæ‹Ÿæ—…æ¸¸éœ€è¦è¶…è¿‡20åˆ†é’Ÿã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„æµç¨‹ï¼Œç”¨äºæˆ¿åœ°äº§ç­‰è·çŸ©å½¢å›¾åƒï¼ˆ*ç§°ä¸º*
    ***DIGITOUR***ï¼‰ä»¥å…‹æœåˆ›å»ºè‡ªåŠ¨åŒ–æ•°å­—æ—…æ¸¸çš„æŒ‘æˆ˜ã€‚**DIGITOUR**æµç¨‹åŒ…å«ä»¥ä¸‹ç»„ä»¶ï¼š
- en: '**Colored tag placement and clicking 360Â° images:** We propose novel paper
    tags that are bi-colored and numbered, facilitating better learning of downstream
    computer vision tasks (*i.e., tag recognition and digit recognition*) and automatic
    stitching of equirectangular images.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å½©è‰²æ ‡ç­¾æ”¾ç½®ä¸ç‚¹å‡»360Â°å›¾åƒï¼š** æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŒè‰²æ ‡ç­¾ï¼Œå¸¦æœ‰ç¼–å·ï¼Œä¾¿äºæ›´å¥½åœ°å­¦ä¹ ä¸‹æ¸¸è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼ˆ*å³æ ‡ç­¾è¯†åˆ«å’Œæ•°å­—è¯†åˆ«*ï¼‰ä»¥åŠç­‰è·çŸ©å½¢å›¾åƒçš„è‡ªåŠ¨æ‹¼æ¥ã€‚'
- en: '**Map equirectangular images to cube map projection:** We used the publicly
    available Python library [vrProjector3](https://github.com/bhautikj/vrProjector)
    to map equirectangular images to their cube map projections (*corresponding to
    six cube faces*).'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å°†ç­‰è·çŸ©å½¢å›¾åƒæ˜ å°„åˆ°ç«‹æ–¹ä½“æ˜ å°„æŠ•å½±ï¼š** æˆ‘ä»¬ä½¿ç”¨å…¬å¼€çš„Pythonåº“[vrProjector3](https://github.com/bhautikj/vrProjector)å°†ç­‰è·çŸ©å½¢å›¾åƒæ˜ å°„åˆ°å®ƒä»¬çš„ç«‹æ–¹ä½“æ˜ å°„æŠ•å½±ï¼ˆ*å¯¹åº”å…­ä¸ªç«‹æ–¹ä½“é¢*ï¼‰ã€‚'
- en: '**Tag Detection:** For each cube face, we propose colored tag detection in
    an image using YOLOv5 [4] architecture.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ ‡ç­¾æ£€æµ‹ï¼š** å¯¹äºæ¯ä¸ªç«‹æ–¹ä½“é¢ï¼Œæˆ‘ä»¬æå‡ºäº†åœ¨å›¾åƒä¸­ä½¿ç”¨YOLOv5 [4]æ¶æ„è¿›è¡Œå½©è‰²æ ‡ç­¾æ£€æµ‹ã€‚'
- en: '**Digit Recognition:** We propose to perform digit recognition using a light-weight
    custom MobileNet [5] model.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ•°å­—è¯†åˆ«ï¼š** æˆ‘ä»¬å»ºè®®ä½¿ç”¨è½»é‡çº§çš„å®šåˆ¶MobileNet [5] æ¨¡å‹è¿›è¡Œæ•°å­—è¯†åˆ«ã€‚'
- en: Finally, we connect all the equirectangular images using the detected tags.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨æ£€æµ‹åˆ°çš„æ ‡ç­¾å°†æ‰€æœ‰ç­‰è·çŸ©å½¢å›¾åƒè¿æ¥èµ·æ¥ã€‚
- en: '![](../Images/b2924b4a19f2b908403e11e142d69682.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2924b4a19f2b908403e11e142d69682.png)'
- en: '**Figure 3:** End-to-end pipeline for proposed approach DIGITOUR. (Source:
    Image by the author)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 3ï¼š** æè®®æ–¹æ³•DIGITOURçš„ç«¯åˆ°ç«¯æµç¨‹ã€‚ï¼ˆæ¥æºï¼šä½œè€…æä¾›çš„å›¾ç‰‡ï¼‰'
- en: '2\. Proposed Pipeline: DIGITOUR'
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2. æè®®çš„æµç¨‹ï¼šDIGITOUR
- en: The complete pipeline for producing digital tours (*referred to as* ***DIGITOUR***
    *and shown in* ***Figure 3***) is as follows.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆæ•°å­—åŒ–æ—…æ¸¸çš„å®Œæ•´æµç¨‹ï¼ˆ*ç§°ä¸º* ***DIGITOUR*** *ï¼Œå¦‚***å›¾ 3***æ‰€ç¤ºï¼‰å¦‚ä¸‹ã€‚
- en: 2.1 Tag Placement and Image Capturing
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 æ ‡ç­¾æ”¾ç½®å’Œå›¾åƒæ•æ‰
- en: While creating a digital tour for any real-estate property, it is essential
    to click 360â—¦ images from different property locations such as bedroom, living
    room, kitchen, etc., then automatically stitching them together to have a â€œ*walkthrough*â€
    experience without being physically present at the location. Therefore, to connect
    multiple equirectangular images, we propose placing paper tags on the floor covering
    each location of the property, and placing the camera (*in our case, we used Ricoh-Theta*)
    in the middle of the scene to capture the whole site (*front, back, left, right
    and bottom*).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸ºä»»ä½•æˆ¿åœ°äº§ç‰©ä¸šåˆ›å»ºæ•°å­—åŒ–æ—…æ¸¸æ—¶ï¼Œå¿…é¡»ä»ä¸åŒçš„ç‰©ä¸šä½ç½®ï¼ˆå¦‚å§å®¤ã€å®¢å…ã€å¨æˆ¿ç­‰ï¼‰æ‹æ‘„360â—¦å›¾åƒï¼Œç„¶åè‡ªåŠ¨å°†å®ƒä»¬æ‹¼æ¥åœ¨ä¸€èµ·ï¼Œä»¥ä¾¿åœ¨ä¸äº²è‡ªåˆ°åœºçš„æƒ…å†µä¸‹è·å¾—â€œ*è™šæ‹Ÿæ¸¸è§ˆ*â€ä½“éªŒã€‚å› æ­¤ï¼Œä¸ºäº†è¿æ¥å¤šä¸ªç­‰è·çŸ©å½¢å›¾åƒï¼Œæˆ‘ä»¬å»ºè®®åœ¨åœ°é¢ä¸Šæ”¾ç½®çº¸è´¨æ ‡ç­¾ï¼Œè¦†ç›–ç‰©ä¸šçš„æ¯ä¸ªä½ç½®ï¼Œå¹¶å°†ç›¸æœºï¼ˆ*åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†Ricoh-Theta*ï¼‰æ”¾ç½®åœ¨åœºæ™¯çš„ä¸­å¤®ï¼Œä»¥æ•æ‰æ•´ä¸ªåœºæ™¯ï¼ˆ*å‰ã€åã€å·¦ã€å³å’Œåº•éƒ¨*ï¼‰ã€‚
- en: '![](../Images/f77f0cce2bfed149b2703ed694c0e1ef.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f77f0cce2bfed149b2703ed694c0e1ef.png)'
- en: '**Figure 4:** Proposed bi-colored tag format and color scheme for each digit
    with their corresponding HSV values. (Source: Image by the author)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 4ï¼š** æå‡ºäº†æ¯ä¸ªæ•°å­—çš„åŒè‰²æ ‡ç­¾æ ¼å¼å’Œé¢œè‰²æ–¹æ¡ˆåŠå…¶å¯¹åº”çš„HSVå€¼ã€‚ï¼ˆæ¥æºï¼šä½œè€…æä¾›çš„å›¾ç‰‡ï¼‰'
- en: 'Moreover, we ensure that the scene is clear of all noisy elements such as dim
    lighting and â€˜*unwanted*â€™ artifacts for better model training and inference. As
    shown in **Figure 4**, we have standardized the tags with dimensions of 6â€ Ã— 6â€
    with two properties:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæˆ‘ä»¬ç¡®ä¿åœºæ™¯ä¸­æ²¡æœ‰ä»»ä½•å™ªéŸ³å…ƒç´ ï¼Œä¾‹å¦‚æ˜æš—çš„ç¯å…‰å’Œâ€˜*ä¸éœ€è¦çš„*â€™ä¼ªå½±ï¼Œä»¥ä¾¿æ›´å¥½åœ°è¿›è¡Œæ¨¡å‹è®­ç»ƒå’Œæ¨ç†ã€‚å¦‚**å›¾ 4**æ‰€ç¤ºï¼Œæˆ‘ä»¬å°†æ ‡ç­¾æ ‡å‡†åŒ–ä¸º6â€
    Ã— 6â€çš„å°ºå¯¸ï¼Œå¹¶å…·æœ‰ä¸¤ä¸ªå±æ€§ï¼š
- en: they are numbered which will help the photographer place tags in sequence and
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®ƒä»¬è¢«ç¼–å·ï¼Œè¿™å°†å¸®åŠ©æ‘„å½±å¸ˆæŒ‰é¡ºåºæ”¾ç½®æ ‡ç­¾å¹¶
- en: they are bi-colored to formulate the digit recognition problem as classification
    task and facilitate better learning of downstream computer vision tasks (*i.e.
    tag detection and digit recognition*).
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®ƒä»¬æ˜¯åŒè‰²çš„ï¼Œæ—¨åœ¨å°†æ•°å­—è¯†åˆ«é—®é¢˜è¡¨è¿°ä¸ºåˆ†ç±»ä»»åŠ¡ï¼Œå¹¶ä¿ƒè¿›ä¸‹æ¸¸è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼ˆ*å³æ ‡ç­¾æ£€æµ‹å’Œæ•°å­—è¯†åˆ«*ï¼‰çš„æ›´å¥½å­¦ä¹ ã€‚
- en: Please note that different colors are assigned to each digit (*from 0 to 9*)
    using the HSV color scheme and leading digit of a tag has a black circle to distinguish
    it from the trailing digit as shown in **Figure 4**. The intuition behind standardizing
    the paper tags is that it allows to train tag detection and digit recognition
    models, which are invariant to distortions, tag placement angle, reflection from
    lighting sources, blur conditions, and camera quality.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œä½¿ç”¨HSVé¢œè‰²æ–¹æ¡ˆä¸ºæ¯ä¸ªæ•°å­—ï¼ˆ*ä»0åˆ°9*ï¼‰åˆ†é…äº†ä¸åŒçš„é¢œè‰²ï¼Œå¹¶ä¸”æ ‡ç­¾çš„å‰å¯¼æ•°å­—æœ‰ä¸€ä¸ªé»‘è‰²åœ†åœˆï¼Œä»¥å°†å…¶ä¸åç»­æ•°å­—åŒºåˆ†å¼€ï¼Œå¦‚**å›¾ 4**æ‰€ç¤ºã€‚æ ‡å‡†åŒ–çº¸è´¨æ ‡ç­¾çš„ç›´è§‰æ˜¯ï¼Œå®ƒå¯ä»¥ç”¨æ¥è®­ç»ƒæ ‡ç­¾æ£€æµ‹å’Œæ•°å­—è¯†åˆ«æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹å¯¹å¤±çœŸã€æ ‡ç­¾æ”¾ç½®è§’åº¦ã€å…‰æºåå°„ã€æ¨¡ç³Šæ¡ä»¶å’Œç›¸æœºè´¨é‡ä¸å˜ã€‚
- en: 2.2 Mapping Equirectangular Image to Cubemap Projection
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 å°†ç­‰è·çŸ©å½¢å›¾åƒæ˜ å°„åˆ°ç«‹æ–¹ä½“å›¾æŠ•å½±
- en: 'An equirectangular image consists of a single image whose width and height
    correlate as 2 : 1 (*as shown in* ***Figure 1***). In our case, images are clicked
    using a Ricoh-Theta camera having dimensions 4096 Ã— 2048 Ã— 3\. Typically, each
    point in an equirectangular image corresponds to a point in a sphere, and the
    images are stretched in the â€˜*latitude*â€™ direction. Since the contents of an equirectangular
    image are distorted, it becomes challenging to detect tags and recognize digits
    directly from it. For example, in **Figure 1**, the tag is stretched at the middle-bottom
    of the image. Therefore, it is necessary to map the image to a less-distorted
    projection and switch back to the original equirectangular image to build the
    digital tour.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ç­‰è·çŸ©å½¢å›¾åƒç”±ä¸€å¼ å›¾åƒç»„æˆï¼Œå…¶å®½åº¦ä¸é«˜åº¦çš„æ¯”ä¾‹ä¸º 2:1ï¼ˆ*å¦‚***å›¾ 1***æ‰€ç¤ºï¼‰ã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œå›¾åƒæ˜¯ä½¿ç”¨ Ricoh-Theta ç›¸æœºæ‹æ‘„çš„ï¼Œå°ºå¯¸ä¸º
    4096 Ã— 2048 Ã— 3ã€‚é€šå¸¸ï¼Œç­‰è·çŸ©å½¢å›¾åƒä¸­çš„æ¯ä¸ªç‚¹å¯¹åº”äºçƒé¢ä¸Šçš„ä¸€ä¸ªç‚¹ï¼Œå›¾åƒåœ¨â€˜*çº¬åº¦*â€™æ–¹å‘ä¸Šè¢«æ‹‰ä¼¸ã€‚ç”±äºç­‰è·çŸ©å½¢å›¾åƒçš„å†…å®¹è¢«æ‰­æ›²ï¼Œå› æ­¤ç›´æ¥ä»å›¾åƒä¸­æ£€æµ‹æ ‡ç­¾å’Œè¯†åˆ«æ•°å­—å˜å¾—å›°éš¾ã€‚ä¾‹å¦‚ï¼Œåœ¨**å›¾
    1**ä¸­ï¼Œæ ‡ç­¾åœ¨å›¾åƒçš„ä¸­ä¸‹éƒ¨è¢«æ‹‰ä¼¸ã€‚å› æ­¤ï¼Œæœ‰å¿…è¦å°†å›¾åƒæ˜ å°„åˆ°ä¸€ä¸ªå¤±çœŸè¾ƒå°çš„æŠ•å½±ä¸Šï¼Œå¹¶è¿”å›åˆ°åŸå§‹çš„ç­‰è·çŸ©å½¢å›¾åƒä¸­ä»¥æ„å»ºæ•°å­—åŒ–æ—…è¡Œã€‚
- en: In this work, we propose to use cube map projection, which is a set of six images
    representing six faces of a cube. Here, every point in the spherical coordinate
    space corresponds to a point in the face of the cube. As shown in **Figure 5**,
    we map the equirectangular image to six faces (*left, right, front, back, top
    and bottom*) of a cube having dimensions 1024 Ã— 1024 Ã— 3 using python library
    vrProjector.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨ç«‹æ–¹ä½“æ˜ å°„æŠ•å½±ï¼Œè¿™æ˜¯ä¸€ä¸ªç”±å…­å¼ å›¾åƒç»„æˆçš„é›†åˆï¼Œè¡¨ç¤ºç«‹æ–¹ä½“çš„å…­ä¸ªé¢ã€‚åœ¨è¿™é‡Œï¼Œçƒé¢åæ ‡ç©ºé—´ä¸­çš„æ¯ä¸ªç‚¹å¯¹åº”äºç«‹æ–¹ä½“æŸä¸ªé¢çš„ä¸€ä¸ªç‚¹ã€‚å¦‚**å›¾
    5**æ‰€ç¤ºï¼Œæˆ‘ä»¬å°†ç­‰è·çŸ©å½¢å›¾åƒæ˜ å°„åˆ°ä¸€ä¸ªå…·æœ‰ 1024 Ã— 1024 Ã— 3 å°ºå¯¸çš„ç«‹æ–¹ä½“çš„å…­ä¸ªé¢ï¼ˆ*å·¦ã€å³ã€å‰ã€åã€ä¸Šã€ä¸‹*ï¼‰ï¼Œå¹¶ä½¿ç”¨ Python åº“
    vrProjector å®Œæˆæ­¤æ“ä½œã€‚
- en: '![](../Images/7ca251fffbf74d5b614c9a69cb6ddb06.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ca251fffbf74d5b614c9a69cb6ddb06.png)'
- en: '**Figure 5:** Conversion of an equirectangular image to its corresponding six
    faces cubemap projection. (Source: Image by the author)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 5ï¼š** å°†ç­‰è·çŸ©å½¢å›¾åƒè½¬æ¢ä¸ºå…¶å¯¹åº”çš„å…­é¢ç«‹æ–¹ä½“æŠ•å½±ã€‚ (æ¥æºï¼šä½œè€…æä¾›çš„å›¾åƒ)'
- en: 2.3 Tag Detection
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 æ ‡ç­¾æ£€æµ‹
- en: Once we get the six images corresponding to the faces of a cube, we detect the
    location of tags placed in each image. For tag detection, we have used the state-of-the-art
    YOLOv5 model. We initialized the network with COCO weights followed by training
    on our dataset. As shown in **Figure 6**, the model takes an image as input and
    returns the detected tag along with coordinates of the bounding box and confidence
    of the prediction. The model is trained on our dataset for 100 epochs with a batch
    size of 32.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æˆ‘ä»¬è·å¾—äº†å¯¹åº”ç«‹æ–¹ä½“å…­ä¸ªé¢çš„å…­å¼ å›¾åƒï¼Œæˆ‘ä»¬å°±å¯ä»¥æ£€æµ‹æ¯ä¸ªå›¾åƒä¸­æ”¾ç½®çš„æ ‡ç­¾çš„ä½ç½®ã€‚å¯¹äºæ ‡ç­¾æ£€æµ‹ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æœ€å…ˆè¿›çš„ YOLOv5 æ¨¡å‹ã€‚æˆ‘ä»¬é¦–å…ˆä½¿ç”¨
    COCO æƒé‡åˆå§‹åŒ–ç½‘ç»œï¼Œç„¶ååœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚å¦‚**å›¾ 6**æ‰€ç¤ºï¼Œæ¨¡å‹å°†å›¾åƒä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›æ£€æµ‹åˆ°çš„æ ‡ç­¾ä»¥åŠè¾¹ç•Œæ¡†çš„åæ ‡å’Œé¢„æµ‹çš„ç½®ä¿¡åº¦ã€‚è¯¥æ¨¡å‹åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†
    100 è½®çš„è®­ç»ƒï¼Œæ‰¹æ¬¡å¤§å°ä¸º 32ã€‚
- en: '![](../Images/912ab51117c479af66d5c4fb733164b4.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/912ab51117c479af66d5c4fb733164b4.png)'
- en: '**Figure 6:** Tag detection using Yolov5\. (Source: Image by the author)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 6ï¼š** ä½¿ç”¨ Yolov5 è¿›è¡Œæ ‡ç­¾æ£€æµ‹ã€‚ (æ¥æºï¼šä½œè€…æä¾›çš„å›¾åƒ)'
- en: 2.4 Digit Recognition
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 æ•°å­—è¯†åˆ«
- en: For the detected tags, we need to recognize the digits from the tag. In a real-world
    environment, the detected tags might have incorrect orientation, poor luminosity,
    reflection from the bulbs in the room, etc. Due to these reasons, it is challenging
    to use Optical Character Recognition (OCR) engines to have good digit recognition
    performance. Therefore, we have used a custom MobileNet model initialized on Imagenet
    weights, which uses color information in tags for digit recognition. In the proposed
    architecture, we have replaced the final classification block of the original
    MobileNet with the dropout layer and dense layer with 20 nodes representing our
    tags from 1 to 20\. **Figure 7** illustrates the proposed architecture. For training
    the model, we have used Adam as an optimizer with a learning rate of 0.001 and
    a discounting factor (ğœŒ) to be 0.1\. We have used categorical cross-entropy as
    a loss function and set the batch size to 64 and the number of epochs to 50.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ£€æµ‹åˆ°çš„æ ‡ç­¾ï¼Œæˆ‘ä»¬éœ€è¦è¯†åˆ«æ ‡ç­¾ä¸Šçš„æ•°å­—ã€‚åœ¨ç°å®ç¯å¢ƒä¸­ï¼Œæ£€æµ‹åˆ°çš„æ ‡ç­¾å¯èƒ½å…·æœ‰ä¸æ­£ç¡®çš„æ–¹å‘ã€è¾ƒå·®çš„äº®åº¦ã€æˆ¿é—´å†…ç¯æ³¡çš„åå°„ç­‰ã€‚ç”±äºè¿™äº›åŸå› ï¼Œä½¿ç”¨å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰å¼•æ“è¿›è¡Œæ•°å­—è¯†åˆ«å¯èƒ½æ— æ³•è·å¾—è‰¯å¥½çš„æ€§èƒ½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªè‡ªå®šä¹‰çš„
    MobileNet æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨ Imagenet æƒé‡ä¸Šè¿›è¡Œäº†åˆå§‹åŒ–ï¼Œå¹¶åˆ©ç”¨æ ‡ç­¾ä¸­çš„é¢œè‰²ä¿¡æ¯æ¥è¿›è¡Œæ•°å­—è¯†åˆ«ã€‚åœ¨æ‰€æå‡ºçš„æ¶æ„ä¸­ï¼Œæˆ‘ä»¬å°†åŸå§‹ MobileNet
    çš„æœ€ç»ˆåˆ†ç±»å—æ›¿æ¢ä¸ºåŒ…å« 20 ä¸ªèŠ‚ç‚¹çš„ Dropout å±‚å’Œ Dense å±‚ï¼Œä»£è¡¨æˆ‘ä»¬ä» 1 åˆ° 20 çš„æ ‡ç­¾ã€‚**å›¾ 7**å±•ç¤ºäº†æ‰€æå‡ºçš„æ¶æ„ã€‚ä¸ºäº†è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬ä½¿ç”¨
    Adam ä½œä¸ºä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡ä¸º 0.001ï¼ŒæŠ˜æ‰£å› å­ï¼ˆğœŒï¼‰ä¸º 0.1ã€‚æˆ‘ä»¬ä½¿ç”¨äº†ç±»åˆ«äº¤å‰ç†µä½œä¸ºæŸå¤±å‡½æ•°ï¼Œæ‰¹æ¬¡å¤§å°è®¾ç½®ä¸º 64ï¼Œè®­ç»ƒè½®æ•°ä¸º 50ã€‚
- en: '![](../Images/ae153326a3cc38d15f5ad27f5b12681c.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae153326a3cc38d15f5ad27f5b12681c.png)'
- en: '**Figure 7:** Digit recognition using custom MobileNet model. (Source: Image
    by the author)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾7ï¼š** ä½¿ç”¨è‡ªå®šä¹‰MobileNetæ¨¡å‹è¿›è¡Œæ•°å­—è¯†åˆ«ã€‚ï¼ˆæ¥æºï¼šä½œè€…æä¾›çš„å›¾åƒï¼‰'
- en: 2.5 Mapping tag coordinates to the original 360â—¦ Image and Virutal Tour Creation
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 æ ‡ç­¾åæ ‡æ˜ å°„åˆ°åŸå§‹360Â°å›¾åƒå¹¶åˆ›å»ºè™šæ‹Ÿå¯¼è§ˆ
- en: Once we have detected the tags and recognized the digits we use the python library
    vrProjector to map the cube map coordinates back to the original equirectangular
    image. An example output is shown in **Figure 8**. For each equirectangular image,
    the detected tags form the nodes of a graph with an edge between them. In the
    subsequent equirectangular images of a property, the graph gets populated with
    more nodes, as more tags are detected. Finally, we connect multiple equirectangular
    images in sequence based on recognized digits written on them and the resulting
    graph is the
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æˆ‘ä»¬æ£€æµ‹åˆ°æ ‡ç­¾å¹¶è¯†åˆ«äº†æ•°å­—ï¼Œæˆ‘ä»¬ä½¿ç”¨Pythonåº“vrProjectorå°†ç«‹æ–¹ä½“æ˜ å°„åæ ‡æ˜ å°„å›åŸå§‹çš„ç­‰è·çŸ©å½¢å›¾åƒã€‚ä¸€ä¸ªç¤ºä¾‹è¾“å‡ºå¦‚**å›¾8**æ‰€ç¤ºã€‚å¯¹äºæ¯ä¸ªç­‰è·çŸ©å½¢å›¾åƒï¼Œæ£€æµ‹åˆ°çš„æ ‡ç­¾å½¢æˆä¸€ä¸ªå›¾çš„èŠ‚ç‚¹ï¼ŒèŠ‚ç‚¹ä¹‹é—´æœ‰è¾¹è¿æ¥ã€‚åœ¨åŒä¸€ç‰©ä¸šçš„åç»­ç­‰è·çŸ©å½¢å›¾åƒä¸­ï¼Œéšç€æ›´å¤šæ ‡ç­¾çš„æ£€æµ‹ï¼Œå›¾ä¸­ä¼šåŠ å…¥æ›´å¤šçš„èŠ‚ç‚¹ã€‚æœ€åï¼Œæˆ‘ä»¬æ ¹æ®è¯†åˆ«å‡ºçš„æ•°å­—å°†å¤šä¸ªç­‰è·çŸ©å½¢å›¾åƒæŒ‰é¡ºåºè¿æ¥èµ·æ¥ï¼Œå¾—åˆ°çš„å›¾å°±æ˜¯
- en: virtual tour as shown in **Figure 2(b)**.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚**å›¾2(b)**æ‰€ç¤ºçš„è™šæ‹Ÿå¯¼è§ˆã€‚
- en: '![](../Images/52f56a6ef982f9405afa11390ba0029d.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52f56a6ef982f9405afa11390ba0029d.png)'
- en: '**Figure 8:** Mapping tags to original equirectangular image. (Source: Image
    by the author)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾8ï¼š** æ ‡ç­¾æ˜ å°„åˆ°åŸå§‹ç­‰è·çŸ©å½¢å›¾åƒã€‚ï¼ˆæ¥æºï¼šä½œè€…æä¾›çš„å›¾åƒï¼‰'
- en: 4\. Datasets
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. æ•°æ®é›†
- en: We have collected data by placing tags and clicking equirectangular images using
    Ricoh-Theta camera for several residential properties in Gurugram, India (*Tier
    1 city*). While collecting images we made sure that certain conditions were met
    such as all doors were opened, lights were turned on, â€˜*unwanted*â€™ objects were
    removed and the tags were placed covering each area of the property. Following
    these instructions, average number of equirectangular images clicked per residential
    property was 7 or 8\. Finally, we have validated our approach on the following
    generated datasets (*based on background color of the tags*).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€šè¿‡åœ¨å°åº¦å¤å°”å†ˆï¼ˆ*Tier 1åŸå¸‚*ï¼‰çš„å‡ å¤„ä½å®…ç‰©ä¸šä¸­æ”¾ç½®æ ‡ç­¾å¹¶ä½¿ç”¨Ricoh-Thetaç›¸æœºæ‹æ‘„ç­‰è·çŸ©å½¢å›¾åƒæ¥æ”¶é›†æ•°æ®ã€‚åœ¨æ”¶é›†å›¾åƒæ—¶ï¼Œæˆ‘ä»¬ç¡®ä¿æ»¡è¶³ä¸€å®šçš„æ¡ä»¶ï¼Œå¦‚æ‰€æœ‰é—¨éƒ½æ‰“å¼€ï¼Œç¯å…‰å¼€å¯ï¼Œâ€˜*ä¸éœ€è¦*â€™çš„ç‰©å“å·²ç§»é™¤ï¼Œæ ‡ç­¾è¢«æ”¾ç½®åœ¨è¦†ç›–ç‰©ä¸šå„ä¸ªåŒºåŸŸçš„ä½ç½®ã€‚éµå¾ªè¿™äº›æŒ‡ç¤ºï¼Œæ¯å¤„ä½å®…ç‰©ä¸šæ‹æ‘„çš„ç­‰è·çŸ©å½¢å›¾åƒå¹³å‡ä¸º7åˆ°8å¼ ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨ä»¥ä¸‹ç”Ÿæˆçš„æ•°æ®é›†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼ˆ*åŸºäºæ ‡ç­¾çš„èƒŒæ™¯é¢œè‰²*ï¼‰ã€‚
- en: '**Green Colored Tags:** We have kept the background color of these tags (*numbered
    1 to 20*) to be green. We have collected 1572 equirectangular images from 212
    properties. Once we convert these equirectangular images to cubemap projection,
    we get 9432 images (*corresponding to cube faces*). Since not all of the cube
    faces have tags (*for e.g. top face*), we get 1503 images with atleast one tag.'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ç»¿è‰²æ ‡ç­¾ï¼š** æˆ‘ä»¬å°†è¿™äº›æ ‡ç­¾çš„èƒŒæ™¯é¢œè‰²ï¼ˆ*ç¼–å·1è‡³20*ï¼‰è®¾ç½®ä¸ºç»¿è‰²ã€‚æˆ‘ä»¬ä»212ä¸ªç‰©ä¸šä¸­æ”¶é›†äº†1572å¼ ç­‰è·çŸ©å½¢å›¾åƒã€‚ä¸€æ—¦æˆ‘ä»¬å°†è¿™äº›ç­‰è·çŸ©å½¢å›¾åƒè½¬æ¢ä¸ºç«‹æ–¹ä½“æŠ•å½±ï¼Œå°±å¾—åˆ°äº†9432å¼ å›¾åƒï¼ˆ*å¯¹åº”ç«‹æ–¹ä½“é¢*ï¼‰ã€‚ç”±äºå¹¶éæ‰€æœ‰ç«‹æ–¹ä½“é¢éƒ½æœ‰æ ‡ç­¾ï¼ˆ*ä¾‹å¦‚é¡¶éƒ¨é¢*ï¼‰ï¼Œæˆ‘ä»¬å¾—åˆ°äº†1503å¼ è‡³å°‘åŒ…å«ä¸€ä¸ªæ ‡ç­¾çš„å›¾åƒã€‚'
- en: '**Proposed Bi-colored Tags (see Figure 4):** For these tags, we have collected
    2654 equirectangular images from 350 properties. Finally, we got 2896 images (*corresponding
    to cube faces*) with atleast one tag.'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å»ºè®®çš„åŒè‰²æ ‡ç­¾ï¼ˆè§å›¾4ï¼‰ï¼š** å¯¹äºè¿™äº›æ ‡ç­¾ï¼Œæˆ‘ä»¬ä»350ä¸ªç‰©ä¸šä¸­æ”¶é›†äº†2654å¼ ç­‰è·çŸ©å½¢å›¾åƒã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å¾—åˆ°äº†2896å¼ å›¾åƒï¼ˆ*å¯¹åº”ç«‹æ–¹ä½“é¢*ï¼‰ï¼Œæ¯å¼ å›¾åƒè‡³å°‘åŒ…å«ä¸€ä¸ªæ ‡ç­¾ã€‚'
- en: Finally, we label the tags present in cube map projection images using [LabelImg](https://github.com/tzutalin/labelImg)
    which is an open-source tool for labeling images in several formats such as Pascal
    VOC and YOLO. For all the experiments, we reserved 20% of data for testing and
    the remaining for training.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨[LabelImg](https://github.com/tzutalin/labelImg)ä¸ºç«‹æ–¹ä½“æŠ•å½±å›¾åƒä¸­çš„æ ‡ç­¾è¿›è¡Œæ ‡æ³¨ï¼Œå®ƒæ˜¯ä¸€ä¸ªå¼€æºå·¥å…·ï¼Œæ”¯æŒå¯¹Pascal
    VOCå’ŒYOLOç­‰å¤šç§æ ¼å¼çš„å›¾åƒè¿›è¡Œæ ‡æ³¨ã€‚åœ¨æ‰€æœ‰å®éªŒä¸­ï¼Œæˆ‘ä»¬å°†20%çš„æ•°æ®ç”¨äºæµ‹è¯•ï¼Œå…¶ä½™ç”¨äºè®­ç»ƒã€‚
- en: 5\. End-to-end Evaluation
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. ç«¯åˆ°ç«¯è¯„ä¼°
- en: For any input image, we first detect the tags and finally recognize the digits
    written on the tags. From this we were able to identify the true positives (*tags
    detected and read correctly*), false positives (*tags detected but read incorrectly*)
    and false negatives (*tags not detected*). The obtained mAP, Precision, Recall
    and f1-score at 0.5 IoU threshold are 88.12, 93.83, 97.89 and 95.81 respectively.
    Please note that all metrics are averaged (*weighted*) over all the 20 classes.
    If all tags across all equirectangular images of a property are detected and read
    correctly, we receive a 100% accurate virtual tour since all nodes of the graph
    are detected and connected with their appropriate edges. In our experiments, we
    were able to accurately generate 100% accurate virtual tour for 94.55% of the
    properties. The inaccuracies were due to the presence of colorful artifacts that
    were falsely detected as tags; and bad lightning conditions.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä»»ä½•è¾“å…¥å›¾åƒï¼Œæˆ‘ä»¬é¦–å…ˆæ£€æµ‹æ ‡ç­¾ï¼Œç„¶åè¯†åˆ«æ ‡ç­¾ä¸Šçš„æ•°å­—ã€‚é€šè¿‡è¿™ä¸ªè¿‡ç¨‹ï¼Œæˆ‘ä»¬èƒ½å¤Ÿè¯†åˆ«çœŸæ­£çš„æ­£ä¾‹ï¼ˆ*æ ‡ç­¾æ­£ç¡®æ£€æµ‹å¹¶è¯»å–*ï¼‰ã€å‡æ­£ä¾‹ï¼ˆ*æ ‡ç­¾æ£€æµ‹åˆ°ä½†è¯»å–é”™è¯¯*ï¼‰å’Œå‡è´Ÿä¾‹ï¼ˆ*æ ‡ç­¾æœªæ£€æµ‹åˆ°*ï¼‰ã€‚åœ¨0.5
    IoUé˜ˆå€¼ä¸‹ï¼Œè·å¾—çš„mAPã€ç²¾ç¡®åº¦ã€å¬å›ç‡å’Œf1-scoreåˆ†åˆ«ä¸º88.12ã€93.83ã€97.89å’Œ95.81ã€‚è¯·æ³¨æ„ï¼Œæ‰€æœ‰æŒ‡æ ‡éƒ½æ˜¯åœ¨æ‰€æœ‰20ä¸ªç±»åˆ«ä¸Šè¿›è¡Œå¹³å‡ï¼ˆ*åŠ æƒ*ï¼‰çš„ã€‚å¦‚æœä¸€ä¸ªç‰©ä¸šçš„æ‰€æœ‰ç­‰çŸ©å½¢å›¾åƒä¸­çš„æ ‡ç­¾éƒ½èƒ½æ­£ç¡®æ£€æµ‹å’Œè¯»å–ï¼Œæˆ‘ä»¬å°†å¾—åˆ°100%å‡†ç¡®çš„è™šæ‹Ÿæ¸¸è§ˆï¼Œå› ä¸ºå›¾çš„æ‰€æœ‰èŠ‚ç‚¹éƒ½è¢«æ£€æµ‹åˆ°ï¼Œå¹¶ä¸å®ƒä»¬çš„é€‚å½“è¾¹è¿æ¥ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬èƒ½å¤Ÿä¸º94.55%çš„ç‰©ä¸šå‡†ç¡®ç”Ÿæˆ100%å‡†ç¡®çš„è™šæ‹Ÿæ¸¸è§ˆã€‚ä¸å‡†ç¡®çš„åŸå› æ˜¯å½©è‰²ä¼ªå½±è¢«è¯¯æ£€æµ‹ä¸ºæ ‡ç­¾ï¼Œä»¥åŠç³Ÿç³•çš„å…‰ç…§æ¡ä»¶ã€‚
- en: '**Figure 9** demonstrates the performance of Yolov5 model for tag detection
    based on green colored and bi-colored tags. Further, experiments and comparison
    of models on digit recognition is shown in **Figure 10.**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 9** å±•ç¤ºäº†Yolov5æ¨¡å‹åœ¨åŸºäºç»¿è‰²å’ŒåŒè‰²æ ‡ç­¾çš„æ ‡ç­¾æ£€æµ‹ä¸Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå…³äºæ•°å­—è¯†åˆ«çš„å®éªŒå’Œæ¨¡å‹æ¯”è¾ƒè§äº**å›¾ 10**ã€‚'
- en: '![](../Images/1864fd9df7d19c37b16d666c372ad94c.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1864fd9df7d19c37b16d666c372ad94c.png)'
- en: '**Figure 9:** Tag detection performance (%). (Source: Image by the author)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 9ï¼š** æ ‡ç­¾æ£€æµ‹æ€§èƒ½ï¼ˆ%ï¼‰ã€‚ ï¼ˆæ¥æºï¼šä½œè€…æä¾›çš„å›¾ç‰‡ï¼‰'
- en: '![](../Images/79c7b22749f4cc268950294dc6459f86.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/79c7b22749f4cc268950294dc6459f86.png)'
- en: '**Figure 10:** Comparison of different state-of-the-art models on bi-colored
    tags dataset for digit recognition task. (Source: Image by the author)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 10ï¼š** åœ¨åŒè‰²æ ‡ç­¾æ•°æ®é›†ä¸Šï¼Œé’ˆå¯¹æ•°å­—è¯†åˆ«ä»»åŠ¡çš„ä¸åŒæœ€å…ˆè¿›æ¨¡å‹çš„æ¯”è¾ƒã€‚ï¼ˆæ¥æºï¼šä½œè€…æä¾›çš„å›¾ç‰‡ï¼‰'
- en: 5\. Conclusion
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. ç»“è®º
- en: We propose an end-to-end pipeline (DIGITOUR) for automatically generating digital
    tours for real-estate properties. For any such property, we first place the proposed
    bi-colored paper tags covering each area of the property. Then, we click equirectangular
    images, followed by mapping these images to less distorted cubemap images. Once
    we get the six images corresponding to cube faces, we detect the location of tags
    using the YOLOv5 model, followed by digit recognition using the MobileNet model.
    The next step is to map the detected coordinates along with recognized digits
    to the original equirectangular images. Finally, we stitch together all the equirectangular
    images to build a virtual tour. We have validated our pipeline on a real-world
    dataset and shown that the end-to-end pipeline performance is 88.12 and 95.81
    in terms of mAP and f1-score at 0.5 IoU threshold averaged (weighted) over all
    classes.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„æµæ°´çº¿ï¼ˆDIGITOURï¼‰ï¼Œç”¨äºè‡ªåŠ¨ç”Ÿæˆæˆ¿åœ°äº§ç‰©ä¸šçš„æ•°å­—åŒ–å¯¼è§ˆã€‚å¯¹äºä»»ä½•è¿™æ ·çš„ç‰©ä¸šï¼Œæˆ‘ä»¬é¦–å…ˆåœ¨ç‰©ä¸šçš„æ¯ä¸ªåŒºåŸŸæ”¾ç½®æˆ‘ä»¬æè®®çš„åŒè‰²çº¸è´¨æ ‡ç­¾ã€‚ç„¶åï¼Œæˆ‘ä»¬æ‹æ‘„ç­‰çŸ©å½¢å›¾åƒï¼Œå¹¶å°†è¿™äº›å›¾åƒæ˜ å°„åˆ°å¤±çœŸè¾ƒå°çš„ç«‹æ–¹ä½“å›¾åƒä¸Šã€‚ä¸€æ—¦æˆ‘ä»¬å¾—åˆ°ä¸ç«‹æ–¹ä½“é¢ç›¸å¯¹åº”çš„å…­å¼ å›¾åƒï¼Œæˆ‘ä»¬ä½¿ç”¨YOLOv5æ¨¡å‹æ£€æµ‹æ ‡ç­¾çš„ä½ç½®ï¼Œéšåä½¿ç”¨MobileNetæ¨¡å‹è¿›è¡Œæ•°å­—è¯†åˆ«ã€‚ä¸‹ä¸€æ­¥æ˜¯å°†æ£€æµ‹åˆ°çš„åæ ‡åŠè¯†åˆ«çš„æ•°å­—æ˜ å°„å›åŸå§‹çš„ç­‰çŸ©å½¢å›¾åƒã€‚æœ€åï¼Œæˆ‘ä»¬å°†æ‰€æœ‰ç­‰çŸ©å½¢å›¾åƒæ‹¼æ¥èµ·æ¥ï¼Œæ„å»ºè™šæ‹Ÿå¯¼è§ˆã€‚æˆ‘ä»¬å·²ç»åœ¨ä¸€ä¸ªçœŸå®ä¸–ç•Œçš„æ•°æ®é›†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æµæ°´çº¿ï¼Œå¹¶å±•ç¤ºäº†ç«¯åˆ°ç«¯æµæ°´çº¿åœ¨æ‰€æœ‰ç±»åˆ«ä¸Šï¼ŒåŸºäº0.5
    IoUé˜ˆå€¼ä¸‹çš„mAPå’Œf1-scoreåˆ†åˆ«ä¸º88.12å’Œ95.81çš„è¡¨ç°ã€‚
- en: If you find our work beneficial and utilize it in your projects, we kindly request
    that you cite it. ğŸ˜Š
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œæœ‰å¸®åŠ©å¹¶åœ¨æ‚¨çš„é¡¹ç›®ä¸­ä½¿ç”¨å®ƒï¼Œæˆ‘ä»¬æ³è¯·æ‚¨å¼•ç”¨æˆ‘ä»¬çš„å·¥ä½œã€‚ğŸ˜Š
- en: '[PRE0]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: References
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] Dragomir Anguelov, Carole Dulong, Daniel Filip, Christian Frueh, StÃ©phane
    Lafon, Richard Lyon, Abhijit Ogale, Luc Vincent, and Josh Weaver. 2010\. Google
    street view: Capturing the world at street level. Computer 43, 6 (2010), 32â€“38.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Dragomir Anguelov, Carole Dulong, Daniel Filip, Christian Frueh, StÃ©phane
    Lafon, Richard Lyon, Abhijit Ogale, Luc Vincent, and Josh Weaver. 2010\. Googleè¡—æ™¯ï¼šæ•æ‰è¡—é“å±‚é¢çš„ä¸–ç•Œã€‚è®¡ç®—æœº
    43, 6 (2010), 32â€“38.'
- en: '[2] Mohamad Zaidi Sulaiman, Mohd Nasiruddin Abdul Aziz, Mohd Haidar Abu Bakar,
    Nur Akma Halili, and Muhammad Asri Azuddin. 2020\. Matterport: virtual tour as
    a new marketing approach in real estate business during pandemic COVID-19\. In
    International Conference of Innovation in Media and Visual Design (IMDES 2020).
    Atlantis Press, 221â€“226.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Mohamad Zaidi Sulaiman, Mohd Nasiruddin Abdul Aziz, Mohd Haidar Abu Bakar,
    Nur Akma Halili, å’Œ Muhammad Asri Azuddin. 2020\. Matterportï¼šè™šæ‹Ÿæ—…æ¸¸ä½œä¸ºç–«æƒ…æœŸé—´æˆ¿åœ°äº§è¡Œä¸šçš„æ–°è¥é”€æ–¹æ³•ã€‚åœ¨2020å¹´å›½é™…åª’ä½“ä¸è§†è§‰è®¾è®¡åˆ›æ–°ä¼šè®®ï¼ˆIMDES
    2020ï¼‰ä¸Šå‘è¡¨ã€‚Atlantis Press, 221â€“226ã€‚'
- en: '[3] Chinu Subudhi. 2021\. Cutting-Edge 360-Degree Virtual Tours. [https://www.mindtree.com/insights/resources/cutting-edge-360-degree-virtual-tours](https://www.mindtree.com/insights/resources/cutting-edge-360-degree-virtual-tours)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Chinu Subudhi. 2021\. å°–ç«¯çš„360åº¦è™šæ‹Ÿæ—…æ¸¸ã€‚[https://www.mindtree.com/insights/resources/cutting-edge-360-degree-virtual-tours](https://www.mindtree.com/insights/resources/cutting-edge-360-degree-virtual-tours)'
- en: '[4] Glenn Jocher, Ayush Chaurasia, Alex Stoken, Jirka Borovec, NanoCode012,
    Yonghye Kwon, TaoXie, Jiacong Fang, imyhxy, Kalen Michael, Lorna, Abhiram V, Diego
    Montes, Jebastin Nadar, Laughing, tkianai, yxNONG, Piotr Skalski, Zhiqiang Wang,
    Adam Hogan, Cristi Fati, Lorenzo Mammana, AlexWang1900, Deep Patel, Ding Yiwei,
    Felix You, Jan Hajek, Laurentiu Diaconu, and Mai Thanh Minh. 2022\. ultralytics/yolov5:
    v6.1 â€” TensorRT, TensorFlow Edge TPU and OpenVINO Export and Inference.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Glenn Jocher, Ayush Chaurasia, Alex Stoken, Jirka Borovec, NanoCode012,
    Yonghye Kwon, TaoXie, Jiacong Fang, imyhxy, Kalen Michael, Lorna, Abhiram V, Diego
    Montes, Jebastin Nadar, Laughing, tkianai, yxNONG, Piotr Skalski, Zhiqiang Wang,
    Adam Hogan, Cristi Fati, Lorenzo Mammana, AlexWang1900, Deep Patel, Ding Yiwei,
    Felix You, Jan Hajek, Laurentiu Diaconu, å’Œ Mai Thanh Minh. 2022\. ultralytics/yolov5:
    v6.1 â€” TensorRT, TensorFlow Edge TPU å’Œ OpenVINO å¯¼å‡ºä¸æ¨ç†ã€‚'
- en: '[5] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and LiangChieh
    Chen. 2018\. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings
    of the IEEE conference on computer vision and pattern recognition. 4510â€“4520.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, å’Œ LiangChieh
    Chen. 2018\. Mobilenetv2: åå‘æ®‹å·®ä¸çº¿æ€§ç“¶é¢ˆã€‚å‘è¡¨äºIEEEè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†ã€‚4510â€“4520ã€‚'
