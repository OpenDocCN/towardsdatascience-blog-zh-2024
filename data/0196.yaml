- en: Evaluating Text Generation in Large Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估大型语言模型的文本生成
- en: 原文：[https://towardsdatascience.com/evaluating-text-generation-in-large-language-models-d4a4baee49a8?source=collection_archive---------8-----------------------#2024-01-20](https://towardsdatascience.com/evaluating-text-generation-in-large-language-models-d4a4baee49a8?source=collection_archive---------8-----------------------#2024-01-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/evaluating-text-generation-in-large-language-models-d4a4baee49a8?source=collection_archive---------8-----------------------#2024-01-20](https://towardsdatascience.com/evaluating-text-generation-in-large-language-models-d4a4baee49a8?source=collection_archive---------8-----------------------#2024-01-20)
- en: Metrics to measure the gap between neural text and human text
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于衡量神经文本与人类文本之间差距的度量标准
- en: '[](https://medium.com/@mina.ghashami?source=post_page---byline--d4a4baee49a8--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page---byline--d4a4baee49a8--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d4a4baee49a8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d4a4baee49a8--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page---byline--d4a4baee49a8--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mina.ghashami?source=post_page---byline--d4a4baee49a8--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page---byline--d4a4baee49a8--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d4a4baee49a8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d4a4baee49a8--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page---byline--d4a4baee49a8--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d4a4baee49a8--------------------------------)
    ·6 min read·Jan 20, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d4a4baee49a8--------------------------------)
    ·6分钟阅读·2024年1月20日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/c7656f4bdb780f0a2dd0e04d5d585e42.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7656f4bdb780f0a2dd0e04d5d585e42.png)'
- en: Image from [unsplash.com](https://unsplash.com/photos/white-printer-paper-on-white-table-gETBUi_oRgQ)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[unsplash.com](https://unsplash.com/photos/white-printer-paper-on-white-table-gETBUi_oRgQ)
- en: Recently, large language models have shown tremendous ability in generating
    human-like texts. There are many metrics to measure how close/similar a text generated
    by large language models is to the reference human text. In fact, bridging this
    gap is an active area of research.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，大型语言模型在生成类人文本方面展示了惊人的能力。现在有许多度量标准可以衡量由大型语言模型生成的文本与参考人类文本的接近度/相似度。实际上，缩小这种差距是一个活跃的研究领域。
- en: In this post, we look into two well-known metrics for automatically evaluating
    the machine generated texts.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将探讨两种广为人知的自动评估机器生成文本的度量标准。
- en: BERTScore
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BERTScore
- en: 'Consider you are given a reference text that is human-generated, and a machine-generated
    text that is generated by an LLM. To compute the semantic similarity between these
    two texts, **BERTScore compute pairwise cosine similarity of token embeddings**.
    See the image below:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一段由人类生成的参考文本和一段由大型语言模型（LLM）生成的机器文本。为了计算这两段文本之间的语义相似度，**BERTScore 计算了标记嵌入的成对余弦相似度**。请看下面的图像：
- en: '![](../Images/e50743da6e0de3bf1f19f2d3dbe0e3ed.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e50743da6e0de3bf1f19f2d3dbe0e3ed.png)'
- en: Image from [[1](https://arxiv.org/abs/1904.09675)]
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源 [[1](https://arxiv.org/abs/1904.09675)]
- en: Here the reference text is *“the weather is cold today”* and the candidate text
    which is machine generated is *“it is freezing today”.* If we compute the n-gram
    similarity these two texts will have a low score. However, we know they are semantically
    very similar. So **BERTScore computes the contextual embedding of each token in
    both**…
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '这里参考文本是 *“今天天气很冷”*，而机器生成的候选文本是 *“今天很冷”*。如果我们计算n-gram相似度，这两段文本的分数会很低。然而，我们知道它们在语义上是非常相似的。所以
    **BERTScore 计算了每个标记在这两段文本中的上下文嵌入**…  '
