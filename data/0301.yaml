- en: 'Revolutionizing Culinary Experiences with AI: Introducing FIRE (Food Image
    to REcipe generation) 🔥'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用人工智能革新烹饪体验：介绍FIRE（Food Image to REcipe生成）🔥
- en: 原文：[https://towardsdatascience.com/revolutionizing-culinary-experiences-with-ai-introducing-fire-food-image-to-recipe-generation-d7b010ee2eb2?source=collection_archive---------9-----------------------#2024-01-31](https://towardsdatascience.com/revolutionizing-culinary-experiences-with-ai-introducing-fire-food-image-to-recipe-generation-d7b010ee2eb2?source=collection_archive---------9-----------------------#2024-01-31)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/revolutionizing-culinary-experiences-with-ai-introducing-fire-food-image-to-recipe-generation-d7b010ee2eb2?source=collection_archive---------9-----------------------#2024-01-31](https://towardsdatascience.com/revolutionizing-culinary-experiences-with-ai-introducing-fire-food-image-to-recipe-generation-d7b010ee2eb2?source=collection_archive---------9-----------------------#2024-01-31)
- en: 'From Visual Delights to Culinary Recipes: How AI Transforms Food Images into
    Recipes?'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从视觉享受到烹饪食谱：人工智能如何将食物图像转化为食谱？
- en: '[](https://medium.com/@prateekchhikara?source=post_page---byline--d7b010ee2eb2--------------------------------)[![Prateek
    Chhikara](../Images/4cabb40cbab34038c0f762b45d58bbba.png)](https://medium.com/@prateekchhikara?source=post_page---byline--d7b010ee2eb2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d7b010ee2eb2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d7b010ee2eb2--------------------------------)
    [Prateek Chhikara](https://medium.com/@prateekchhikara?source=post_page---byline--d7b010ee2eb2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@prateekchhikara?source=post_page---byline--d7b010ee2eb2--------------------------------)[![Prateek
    Chhikara](../Images/4cabb40cbab34038c0f762b45d58bbba.png)](https://medium.com/@prateekchhikara?source=post_page---byline--d7b010ee2eb2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d7b010ee2eb2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d7b010ee2eb2--------------------------------)
    [Prateek Chhikara](https://medium.com/@prateekchhikara?source=post_page---byline--d7b010ee2eb2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d7b010ee2eb2--------------------------------)
    ·11 min read·Jan 31, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d7b010ee2eb2--------------------------------)
    ·阅读时间11分钟·2024年1月31日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/28ec61f09782221ff6bb62c3825991f5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/28ec61f09782221ff6bb62c3825991f5.png)'
- en: 'Figure 1\. Given a potentially unseen image, our method FIRE generates a corresponding
    recipe consisting of a title, ingredients, and cooking instructions. (Source:
    Image by the author)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图1. 给定一张可能未见过的图像，我们的方法FIRE生成相应的食谱，包含标题、食材和烹饪说明。（图片来源：作者提供）
- en: 'Introduction:'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言：
- en: 'Food is an essential source of nutrition and also an integral part of our cultural
    identity, describing our lifestyle, traditions, and social relations [1]. A person’s
    physical appearance and cognitive abilities usually contain evidence of their
    dietary habits because selecting nutritious food contributes to the overall well-being
    of the body and mind of a person [2]. The rapid growth of social media enables
    everyone to share stunning visuals of the delicious food they consume. A simple
    search for hashtags like #food or #foodie yields millions of posts, emphasizing
    the immense value of food in our society [3]. The importance of food, followed
    by large amounts of publicly available food datasets, has encouraged food computing
    applications that associate visual depictions of dishes with symbolic knowledge.
    An ambitious goal of food computing is to produce the recipe for a given food
    image, with applications such as food recommendation according to user preferences,
    recipe customization to accommodate cultural or religious factors, and automating
    cooking execution for higher efficiency and precision [4].'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 食物是人体所需的基本营养来源，也是我们文化身份的组成部分，反映了我们的生活方式、传统和社会关系[1]。一个人的外貌和认知能力通常能反映出其饮食习惯，因为选择营养丰富的食物有助于身心的整体健康[2]。社交媒体的迅速发展使每个人都能分享他们所食用美味食物的惊艳视觉。简单地搜索诸如#food或#foodie这样的标签，会显示出数百万的帖子，突显了食物在我们社会中的巨大价值[3]。食物的重要性，以及大量公开可得的食物数据集，推动了与食物相关的计算应用，尤其是将食物图像与符号知识联系起来。食物计算的一个雄心勃勃的目标是生成给定食物图像的食谱，包括根据用户偏好进行食物推荐、根据文化或宗教因素定制食谱，以及自动化烹饪操作以提高效率和精准度[4]。
- en: “Tell me what you eat, and I will tell you who you are.” This saying emphasizes
    the idea that an individual’s dietary choices reflect their identity.
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “告诉我你吃什么，我就能告诉你你是谁。”这句话强调了个人饮食选择反映其身份的观点。
- en: Before delving deep into the proposed work, I would like to mention that this
    work is published and available at the***IEEE/CVF Winter Conference on Applications
    of Computer Vision (WACV) — 2024***, a conference recognized for its contributions
    and advancements in computer vision. ✨✨✨
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨我们提出的工作之前，我想提到这项工作已被发表，并可以在***IEEE/CVF计算机视觉应用冬季会议（WACV）- 2024***上找到，这是一个因其对计算机视觉的贡献和进展而备受认可的会议。✨✨✨
- en: '**Poster:** [https://drive.google.com/file/d/1zf2NA6ga8PWndZAgu5QjSwO8EPsvt-yt/view](https://drive.google.com/file/d/1zf2NA6ga8PWndZAgu5QjSwO8EPsvt-yt/view)'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**海报：** [https://drive.google.com/file/d/1zf2NA6ga8PWndZAgu5QjSwO8EPsvt-yt/view](https://drive.google.com/file/d/1zf2NA6ga8PWndZAgu5QjSwO8EPsvt-yt/view)'
- en: ''
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Paper:** [https://openaccess.thecvf.com/content/WACV2024/html/Chhikara_FIRE_Food_Image_to_REcipe_Generation_WACV_2024_paper.html](https://openaccess.thecvf.com/content/WACV2024/html/Chhikara_FIRE_Food_Image_to_REcipe_Generation_WACV_2024_paper.html)'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**论文：** [https://openaccess.thecvf.com/content/WACV2024/html/Chhikara_FIRE_Food_Image_to_REcipe_Generation_WACV_2024_paper.html](https://openaccess.thecvf.com/content/WACV2024/html/Chhikara_FIRE_Food_Image_to_REcipe_Generation_WACV_2024_paper.html)'
- en: 'The Inspiration Behind this Work:'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这项工作的灵感来源：
- en: Our motivation behind creating an end-to-end method for recipe generation from
    food images came from the interest of the Computer Vision (CV) community in food
    computing. CV has been used for food quality assurance for around three decades
    [5]. Despite advances in deep learning techniques for food image processing, existing
    methods have achieved limited performance in extracting ingredients from a given
    food image [6, 7]. Moreover, recipe generation from a set of ingredients could
    be treated as a language-generation task or, more precisely, as a seq-to-seq use
    case, which is also unexplored in the current literature. Additionally, previous
    methods have not thoroughly combined CV and NLP research to devise a comprehensive
    system that translates food images into complete recipes. Therefore, the current
    food computing methods have yet to leverage recent breakthroughs in NLP and CV,
    such as vision transformers and advanced language modeling. This gap in technology
    and application motivated us to develop an end-to-end pipeline, which we name
    FIRE (🔥), desiring to join these dots and push the limits of food computing. FIRE
    is a multimodal model that is designed to generate comprehensive recipe, including
    food titles, ingredients, and cooking instructions, based on given input food
    images, as shown in Figure 1.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一种从食物图像生成食谱的端到端方法的动机，源自计算机视觉（CV）社区对食物计算的兴趣。计算机视觉已被用于食物质量保证约三十年[5]。尽管深度学习技术在食物图像处理方面取得了进展，现有方法在从给定食物图像中提取成分方面的表现仍然有限[6,
    7]。此外，从一组成分生成食谱可以看作是一个语言生成任务，或者更准确地说，是一个序列到序列（seq-to-seq）的应用场景，而这一点在现有文献中仍未被充分探讨。进一步来说，之前的方法未能彻底结合计算机视觉和自然语言处理（NLP）研究，设计出一个综合系统，将食物图像转化为完整的食谱。因此，目前的食物计算方法尚未利用NLP和计算机视觉中的最新突破，如视觉变换器和先进的语言建模技术。正是由于这种技术和应用上的空白，我们激发了开发端到端管道的动力，我们命名其为FIRE（🔥），旨在将这些技术结合起来，推动食物计算的边界。FIRE是一个多模态模型，旨在根据给定的食物图像生成完整的食谱，包括食物名称、成分和烹饪指令，如图1所示。
- en: 'The contributions of our paper are as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们论文的贡献如下：
- en: Our approach uses Vision Transformers (ViT) to extract detailed embeddings from
    food images, which are then used as an input by an attention-based decoder which
    further identifies recipe ingredients.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的方法使用视觉变换器（ViT）从食物图像中提取详细的嵌入表示，然后作为基于注意力的解码器的输入，进一步识别食谱成分。
- en: We have developed a detailed design to generate recipe titles and cooking instructions,
    using state-of-the-art (SotA) vision (BLIP) and language (T5) models.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们开发了一个详细的设计，用于生成食谱标题和烹饪指令，采用最先进的视觉（BLIP）和语言（T5）模型。
- en: Our multimodal approach surpasses existing models in performance based on ingredient
    extraction accuracy and the quality of generated cooking instructions.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的多模态方法在成分提取准确性和生成的烹饪指令质量方面，超过了现有模型的表现。
- en: We demonstrate FIRE’s versatility via two innovative applications, (i) *Recipe
    Customization* and (ii) *Recipe to Code Generation*, displaying its significance
    in integrating large language models (LLMs) using few-shot prompting.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过两个创新应用展示了FIRE的多功能性，（i）*食谱定制* 和 (ii) *食谱到代码生成*，展示了其在使用少量提示集成大型语言模型（LLM）方面的重要性。
- en: 'Unveiling FIRE: Breaking Down the Process'
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 揭开FIRE的面纱：分解过程
- en: 'FIRE contains three components: (1) **title generation** from food images by
    using SotA image captioning, (2) **ingredient extraction** from images using vision
    transformers and decoder layers with attention, and (3) **cooking instruction
    generation** based on the generated title and extracted ingredients using an encoder-decoder
    model. Please refer to Figure 2 for more details about the proposed architecture.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: FIRE 包含三个组件：（1）使用最先进的图像描述技术从食品图像中生成**标题**，（2）使用视觉转换器和带有注意力的解码器层从图像中提取**成分**，（3）基于生成的标题和提取的成分使用编码-解码模型生成**烹饪说明**。有关所提出架构的更多细节，请参见图
    2。
- en: '![](../Images/32c90148e7f474d6e6cd5fe4b0b3a8cc.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/32c90148e7f474d6e6cd5fe4b0b3a8cc.png)'
- en: 'Figure 2\. Proposed architecture to extract ingredients, and generate the recipe
    title and cooking instructions from a food image. (Ingredients with quantity is
    passed during the train time only) (Source: Image by the author)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2. 提出架构，用于从食品图像中提取成分、生成菜谱标题和烹饪说明。（成分及其数量仅在训练时传递）（来源：作者提供的图像）
- en: '**1\. Title Generation**'
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**1. 标题生成**'
- en: Using the BLIP model, we generate recipe titles from food images [8]. In our
    initial experiments with the off-the-shelf BLIP model, we observed BLIP’s prediction
    accuracy was lower because of the domain shift between its training data and the
    food domain. BLIP tends to capture extraneous details impertinent to our goal
    because it was originally designed to provide a comprehensive image caption for
    various settings. As an illustration, when presented with an image of a muffin,
    BLIP produced the description ‘*a muffin positioned atop a wooden cutting board*’.
    To better align the generated captions with recipe titles, we fine-tune the BLIP
    model using a subset of the Recipe1M dataset. We observe that the fine-tuned version
    of BLIP shows promising improvements in generating accurate, aligned, and pertinent
    titles for food images. For the same example image the fine-tuned BLIP model provides
    a shorter string ‘*muffin*’, removing the additional extraneous information.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 BLIP 模型，我们从食品图像中生成菜谱标题 [8]。在我们使用现成的 BLIP 模型进行的初步实验中，我们观察到 BLIP 的预测准确性较低，因为其训练数据与食品领域之间存在领域转移。由于
    BLIP 最初设计用于为各种场景提供全面的图像描述，它倾向于捕捉与我们目标无关的额外细节。举个例子，当我们展示一张松饼的图像时，BLIP 生成了描述“*一块松饼放在木制切菜板上*”。为了更好地使生成的描述与菜谱标题对齐，我们使用
    Recipe1M 数据集的一个子集对 BLIP 模型进行了微调。我们观察到，经过微调后的 BLIP 模型在生成准确、对齐且相关的食品图像标题方面显示出了良好的改进。对于相同的松饼图像，经过微调的
    BLIP 模型提供了更简短的字符串“*松饼*”，去除了多余的信息。
- en: 2\. Ingredient Extraction
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2. 成分提取
- en: Extracting ingredients from a given food image is a challenging task due to
    the inherent complexity and variability of food compositions. We develop an ingredient
    extraction pipeline (shown in Figure 2) that is built on top of the one proposed
    by [7].
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从给定的食品图像中提取成分是一项具有挑战性的任务，因为食品组成的固有复杂性和变化性。我们开发了一个成分提取流程（如图 2 所示），该流程基于文献[7]中提出的方法。
- en: '**Feature Extractor:** We extract the image’s features using ViT [9]. ViT’s
    attention mechanism handles the feature representations with stable and notably
    high resolution. This capability precisely meets the requirements of dense prediction
    tasks such as ingredient extraction from food images.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征提取器：** 我们使用 ViT [9] 提取图像的特征。ViT 的注意力机制以稳定且显著高的分辨率处理特征表示。该能力精确地满足了密集预测任务的需求，例如从食品图像中提取成分。'
- en: '**Ingredient Decoder:** The feature extractor produces image embeddings. We
    pass these image embeddings through three normalization layers (layerNorm) and
    subsequently feed the output into our ingredient decoder responsible for extracting
    ingredients. The decoder consists of four consecutive blocks, each having multiple
    sequential layers: self-attention, conditional attention, two fully connected
    layers, and three normalization layers. In the last step, the decoder output is
    processed by a fully connected layer with a node count equivalent to the vocabulary
    size, resulting in a predicted set of ingredients.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**成分解码器：** 特征提取器生成图像嵌入。我们将这些图像嵌入通过三个归一化层（layerNorm），然后将输出传递给我们的成分解码器，负责提取成分。解码器由四个连续的模块组成，每个模块包含多个顺序层：自注意力、条件注意力、两个全连接层和三个归一化层。在最后一步，解码器的输出通过一个全连接层处理，该层的节点数等于词汇表的大小，从而生成预测的成分集合。'
- en: 3\. Cooking Instruction Generation
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. 烹饪说明生成
- en: Considering the remarkable accomplishments of LMs in natural language applications
    like text generation and question answering [10], we pose cooking instruction
    generation as a language modeling task.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到语言模型（LMs）在自然语言应用（如文本生成和问答）中的显著成就[10]，我们将烹饪指导生成视为一种语言建模任务。
- en: '![](../Images/b4d3515db1cb21ea3c9c5eb24b6ea211.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4d3515db1cb21ea3c9c5eb24b6ea211.png)'
- en: 'Figure 3\. Generating cooking instructions for a title and a set of ingredients.
    (ingredients with quantity is present only during the fine-tuning of T5) (Source:
    Image by the author)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图3. 为标题和一组配料生成烹饪指导。（带数量的配料仅在T5的微调过程中出现）（来源：作者提供的图片）
- en: Refining the LMs for downstream tasks has demonstrated remarkable outcomes in
    various NLP tasks. While we expect that large LMs would be capable of generating
    cooking instructions after fine-tuning, they require high computational resources
    given their large number of parameters. Given the available resources and our
    research objective, we adopt the popular encoder-decoder model, T5 [11], for generating
    cooking instructions. During fine-tuning, we pass the title and ingredients of
    the recipe as a formatted string (see Figure 3), inspired by prior work [12].
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对语言模型（LMs）进行下游任务微调在各种NLP任务中取得了显著成果。虽然我们预计大规模的语言模型经过微调后能够生成烹饪指导，但考虑到其庞大的参数量，它们需要大量的计算资源。基于可用资源和我们的研究目标，我们采用了流行的编码器-解码器模型T5[11]来生成烹饪指导。在微调过程中，我们将食谱的标题和配料作为格式化字符串传入（见图3），灵感来自于先前的研究[12]。
- en: 'The T5 is fine-tuned on three inputs: title, ingredients, and ingredients with
    quantity to incorporate maximum information from the dataset. However, we do not
    have ingredients with quantities at the inference time; hence, we pass only the
    title and ingredients. Moreover, excluding the quantity information from our model
    ensures a fair comparison with previous approaches. It investigates whether our
    model’s advantage stems from a well-structured architecture rather than relying
    solely on augmenting additional knowledge. By removing the influence of quantity
    information during inference, we aim to highlight the inherent capabilities of
    T5 and its ability to generate high-quality cooking instructions.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: T5 在三种输入上进行微调：标题、配料和带数量的配料，以最大化地从数据集中获取信息。然而，在推理时我们没有带数量的配料；因此，我们只传递标题和配料。此外，排除数量信息确保了与先前方法的公平比较。我们调查了我们的模型优势是否源于良好的结构化架构，而非仅依赖于额外知识的增强。通过去除推理过程中数量信息的影响，我们旨在突出T5的固有能力及其生成高质量烹饪指导的能力。
- en: 'FIRE Results:'
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FIRE 结果：
- en: The results in Table 1 show that FIRE performs better than the SotA baselines,
    InverseCooking [7] and Chef Transformer [13]. These results demonstrate our proposed
    pipeline’s ability to generate precise and coherent recipes, corroborating the
    effectiveness of FIRE and emphasizing the value of language generation models
    for high-quality recipe generation. These results also support our expectation
    that the FIRE method can generalize well without ingredient quantity information
    given at inference time, even when they were present during training. Meanwhile,
    training with extra information results in fewer hallucinations, especially regarding
    ingredients quantity (e.g., 2 tablespoons of salt) and cooking time (e.g., heat
    for 10–12 minutes).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 表1中的结果显示，FIRE表现优于SotA基线模型InverseCooking[7]和Chef Transformer[13]。这些结果证明了我们提出的流程在生成精确且连贯的食谱方面的能力，验证了FIRE的有效性，并强调了语言生成模型在高质量食谱生成中的价值。这些结果也支持我们的预期，即FIRE方法即使在推理时没有提供配料数量信息，仍然能很好地泛化，即使在训练过程中提供了这些信息。同时，使用额外信息进行训练能减少幻觉现象，特别是关于配料数量（例如2汤匙盐）和烹饪时间（例如加热10-12分钟）。
- en: '![](../Images/eb3500ec8e3e77538caf3a9d4332d770.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb3500ec8e3e77538caf3a9d4332d770.png)'
- en: 'Table 1\. Recipe generation comparison on the test dataset. We report mean
    with one standard deviation of 10 experiments. Bold represents the best model.
    (+) represents the model tested on the ground truth title and ingredients to generate
    the recipe. (Source: Image by the author)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 表1. 测试数据集上的食谱生成比较。我们报告了10次实验的均值和标准差。粗体表示最佳模型。(+)表示在真实标题和配料上测试模型以生成食谱。（来源：作者提供的图片）
- en: In this article, we mostly focus on the critical aspects of our proposed work,
    considering the article length constraints. We encourage readers to check out
    our detailed paper for a deeper dive into our experimental results and comprehensive
    analysis. 😁
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们主要关注我们提出的工作中的关键方面，考虑到文章篇幅的限制。我们鼓励读者查看我们的详细论文，以便深入了解我们的实验结果和全面分析。😁
- en: Error Analysis
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 错误分析
- en: To gain further insight into the performance of our recipe generation method,
    we inspected its performance on individual images. FIRE is often able to generate
    a correct recipe for dishes similar to those present in the Recipe1M dataset.
    For Pav Bhaji (a popular Indian dish not present in Recipe1M), it gave a result
    that is unrelated to the intended dish, as illustrated in Figure. 4\. Therefore,
    we want to highlight the importance of developing better evaluation metrics because
    conventional evaluation metrics such as SacreBLEU and ROUGE failed to capture
    the accuracy of the recipes generated and detect specific text hallucinations.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步了解我们的食谱生成方法的表现，我们检查了其在个别图片上的表现。FIRE 通常能够为与 Recipe1M 数据集中存在的菜肴相似的菜肴生成正确的食谱。对于
    Pav Bhaji（一道在 Recipe1M 数据集中没有的受欢迎的印度菜肴），它给出的结果与预期的菜肴无关，如图 4 所示。因此，我们想强调开发更好的评估指标的重要性，因为传统的评估指标，如
    SacreBLEU 和 ROUGE，未能捕捉生成的食谱的准确性并检测到特定的文本幻觉。
- en: '![](../Images/efe451638ebff8231061c973651d9421.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/efe451638ebff8231061c973651d9421.png)'
- en: 'Figure 4\. Recipe prediction by FIRE for Pav Bhaji image. (Source: Image by
    the author)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4. FIRE 对 Pav Bhaji 图片的食谱预测。（来源：作者提供的图片）
- en: 'Beyond the Kitchen: The Future of Food Computing with FIRE:'
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越厨房：FIRE 在食品计算中的未来：
- en: While FIRE achieves SotA performance on the ambitious task of generating recipes
    from images, we go a step further and investigate its integration into larger
    pipelines for food computing applications. Namely, considering the promise of
    few-shot prompting of large language models, we describe how FIRE and large LMs
    can be integrated to support ***recipe customization*** and ***recipe-to-machine-code
    generation***.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 FIRE 在从图片生成食谱这一具有挑战性的任务上达到了最先进的表现，我们更进一步，探索其在食品计算应用中集成到更大管道中的可能性。具体来说，考虑到大规模语言模型的少量提示能力，我们描述了
    FIRE 和大型语言模型如何集成，以支持***食谱定制***和***食谱到机器代码生成***。
- en: '![](../Images/085bb19ddc9aebf21cb6f74efd267815.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/085bb19ddc9aebf21cb6f74efd267815.png)'
- en: 'Figure 5\. Applications of FIRE: Recipe Customization and Recipe to Code Generation.
    (Source: Image by the author)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5. FIRE 的应用：食谱定制与食谱到代码生成。（来源：作者提供的图片）
- en: 1\. Recipe Customization
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 食谱定制
- en: Recipe customization is crucial due to the connection between food, customs,
    and individual preferences. Additionally, it becomes essential when addressing
    allergies or dietary restrictions. Surprisingly, despite the evident demand, existing
    literature lacks dedicated efforts in recipe customization. Our work aims to bridge
    the research gap by enabling personalized recipe customization, considering individual
    taste profiles and dietary restrictions.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 食谱定制至关重要，因为它涉及到食物、习惯和个人偏好。此外，当涉及过敏或饮食限制时，食谱定制变得尤为重要。令人惊讶的是，尽管有明显需求，现有文献在食谱定制方面缺乏专门的研究。我们的工作旨在填补这一研究空白，通过考虑个人口味档案和饮食限制来实现个性化食谱定制。
- en: To guide future research in this area, we showcase the ability of FIRE to support
    a recipe customization approach that focuses on a wide range of topics (e.g.,
    ingredient replacement, taste adjustment, calorie adjustment, cooking time adaptation)
    to test few-shot performance thoroughly. As shown in the purple part of Figure
    5, we remove ingredients to trim the potatoes from the recipe. Two sentences related
    to potatoes are deleted in the modified version, and one sentence is changed to
    ensure consistency. Specifically, we perform ingredient addition to replace ‘*cheese*’
    with ‘*cheddar cheese*’ and recognize that it should be added before baking, resulting
    in the modified sentence ‘*Sprinkle half each of cheddar cheese and onions*.’
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了指导未来在这一领域的研究，我们展示了 FIRE 支持一种食谱定制方法的能力，重点关注广泛的主题（例如，食材替换、口味调整、卡路里调整、烹饪时间适配），以全面测试少量样本的表现。如图
    5 中紫色部分所示，我们从食谱中删除了土豆成分。删除了与土豆相关的两句话，并对其中一句进行了修改以确保一致性。具体来说，我们进行了食材添加，将‘*奶酪*’替换为‘*切达奶酪*’，并识别到应在烘烤前加入，从而得到了修改后的句子‘*撒上半量的切达奶酪和洋葱*’。
- en: 2\. Generating Machine Code for Image-based Recipes
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2. 为基于图片的食谱生成机器代码
- en: Converting recipes to machine code enables automation, scalability, and integration
    with various existing systems, thus reducing manual intervention, saving labor
    costs, and reducing human errors while preparing the food. To facilitate this
    task, we combine FIRE’s recipe generation strength with the ability of large LMs
    to manipulate code-style prompts for structural tasks [14]. We show an example
    approach for generating Python-style code representations of recipes developed
    by FIRE, by prompting GPT-3 (please refer to orange part in Figure 5).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 将食谱转换为机器代码能够实现自动化、可扩展性，并与各种现有系统进行集成，从而减少人工干预、节省劳动力成本并减少准备食物时的人为错误。为了简化这一任务，我们将FIRE的食谱生成能力与大型语言模型（LM）在结构任务中操控代码风格提示的能力结合起来[14]。我们展示了一个示例方法，用于通过提示GPT-3生成由FIRE开发的食谱的Python风格代码表示（请参阅图5中的橙色部分）。
- en: 'Conclusion & Future Work:'
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论与未来工作：
- en: We introduced FIRE, a methodology tailored for food computing, focusing on generating
    food titles, extracting ingredients, and generating cooking instructions solely
    from image inputs. We leveraged recent CV and language modeling advancements to
    achieve superior performance against solid baselines. Furthermore, we demonstrated
    practical applications of FIRE for *recipe customization* and *recipe-to-code
    generation*, showcasing the adaptability and automation potential of our approach.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了FIRE，一种专为食品计算量身定制的方法，专注于仅通过图像输入生成食物标题、提取食材和生成烹饪指令。我们利用了近期计算机视觉（CV）和语言建模的进展，取得了优于稳固基准的卓越表现。此外，我们展示了FIRE在*食谱定制*和*食谱到代码生成*中的实际应用，展示了我们方法的适应性和自动化潜力。
- en: 'We list three challenges that should be addressed in future research:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们列出了未来研究中应解决的三大挑战：
- en: Existing and proposed recipe generation models lack a reliable mechanism to
    verify the accuracy of the generated recipes. Conventional evaluation metrics
    fall short in this aspect. Hence, we would like to create a new metric that assesses
    the coherence and plausibility of recipes, providing a more thorough evaluation.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现有的食谱生成模型和我们提出的模型缺乏一种可靠的机制来验证生成食谱的准确性。传统的评估指标在这方面存在不足。因此，我们希望创建一种新的评估标准，评估食谱的连贯性和可信度，提供更为全面的评价。
- en: The diversity and availability of recipes are influenced by geographical, climatic,
    and religious factors, which may limit their applicability. Incorporating knowledge
    graphs that account for these contextual factors and ingredient relationships
    can offer alternative ingredient suggestions, addressing this issue.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 食谱的多样性和可获得性受到地理、气候和宗教因素的影响，这些因素可能会限制其适用性。结合考虑这些背景因素和食材关系的知识图谱，可以提供替代食材建议，从而解决这一问题。
- en: Hallucination in recipe generation using language and vision models poses a
    significant challenge. Future work would explore the state-tracking methods to
    improve the generation process, ensuring the production of more realistic and
    accurate recipes.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用语言和视觉模型生成食谱时的幻觉问题构成了一个重大挑战。未来的研究将探讨状态追踪方法，以改进生成过程，确保生成更为真实和准确的食谱。
- en: 'Call-to-Action:'
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 行动号召：
- en: I hope this overview has provided you the insight into the inspiration and development
    of FIRE, our innovative tool for converting food images into detailed recipes.
    For a more in-depth exploration of our approach, I invite you to check out our
    full paper, which is published in the *IEEE/CVF Winter Conference on Applications
    of Computer Vision (WACV) — 2024*. If our research contribute to your work, we
    would be happy if you cite it. 😊
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这个概述能为您提供有关FIRE的灵感和发展的见解，这是一款将食物图像转化为详细食谱的创新工具。若想更深入地了解我们的方法，欢迎查阅我们完整的论文，该论文将在*IEEE/CVF冬季计算机视觉应用会议（WACV）-2024*上发表。如果我们的研究对您的工作有所帮助，欢迎引用我们的论文。😊
- en: '**Paper Link:** [https://openaccess.thecvf.com/content/WACV2024/html/Chhikara_FIRE_Food_Image_to_REcipe_Generation_WACV_2024_paper.html](https://openaccess.thecvf.com/content/WACV2024/html/Chhikara_FIRE_Food_Image_to_REcipe_Generation_WACV_2024_paper.html)'
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**论文链接：** [https://openaccess.thecvf.com/content/WACV2024/html/Chhikara_FIRE_Food_Image_to_REcipe_Generation_WACV_2024_paper.html](https://openaccess.thecvf.com/content/WACV2024/html/Chhikara_FIRE_Food_Image_to_REcipe_Generation_WACV_2024_paper.html)'
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'References:'
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献：
- en: '[1] Weiqing Min, Shuqiang Jiang, Linhu Liu, Yong Rui, and Ramesh Jain. A survey
    on food computing. ACM Comput. Surv., 52(5), sep 2019.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Weiqing Min, Shuqiang Jiang, Linhu Liu, Yong Rui, 和 Ramesh Jain. 食品计算的综述.
    ACM Comput. Surv., 52(5), 2019年9月。'
- en: '[2] Sutter Health. Eating Well for Mental Health. [https://www.sutterhealth.org/health/nutrition/eating-wellfor-mental-health.](https://www.sutterhealth.org/health/nutrition/eating-wellfor-mental-health.)
    Accessed on March 24, 2023.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Sutter Health. 为心理健康吃得好. [https://www.sutterhealth.org/health/nutrition/eating-wellfor-mental-health.](https://www.sutterhealth.org/health/nutrition/eating-wellfor-mental-health.)
    访问日期：2023年3月24日。'
- en: '[3] Kiely Kuligowski. 12 Reasons to Use Instagram for Your Business. [https://www.business.com/articles/10-reasons-touse-instagram-for-business/.](https://www.business.com/articles/10-reasons-touse-instagram-for-business/.)
    Accessed on May 12, 2023.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Kiely Kuligowski. 使用Instagram为您的业务的12个理由. [https://www.business.com/articles/10-reasons-touse-instagram-for-business/.](https://www.business.com/articles/10-reasons-touse-instagram-for-business/.)
    访问日期：2023年5月12日。'
- en: '[4] Dim P. Papadopoulos, Enrique Mora, Nadiia Chepurko, Kuan Wei Huang, Ferda
    Ofli, and Antonio Torralba. Learning program representations for food images and
    cooking recipes, 2022.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Dim P. Papadopoulos、Enrique Mora、Nadiia Chepurko、Kuan Wei Huang、Ferda Ofli
    和 Antonio Torralba. 食品图像和烹饪食谱的程序表示学习，2022年。'
- en: '[5] Sundaram Gunasekaran. Computer vision technology for food quality assurance.
    Trends in Food Science & Technology, 7(8):245–256, 1996.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Sundaram Gunasekaran. 食品质量保证的计算机视觉技术. 《食品科学与技术趋势》，7(8)：245–256，1996年。'
- en: '[6] Yoshiyuki Kawano and Keiji Yanai. Food image recognition with deep convolutional
    features. In Proceedings of the 2014 ACM International Joint Conference on Pervasive
    and Ubiquitous Computing: Adjunct Publication, pages 589– 593, 2014'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Yoshiyuki Kawano 和 Keiji Yanai. 基于深度卷积特征的食品图像识别. 见于《2014年ACM国际联合会议论文集：普适计算与无处不在计算：附录出版物》，第589–593页，2014年。'
- en: '[7] Amaia Salvador, Michal Drozdzal, Xavier Giro-i Nieto, and ´ Adriana Romero.
    Inverse cooking: Recipe generation from food images. In Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pages 10453– 10462, 2019.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Amaia Salvador、Michal Drozdzal、Xavier Giro-i Nieto 和 Adriana Romero. 逆向烹饪：从食品图像生成食谱.
    见于《IEEE/CVF计算机视觉与模式识别会议论文集》，第10453–10462页，2019年。'
- en: '[8] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping
    language-image pre-training for unified vision-language understanding and generation.
    In International Conference on Machine Learning, pages 12888– 12900\. PMLR, 2022.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Junnan Li、Dongxu Li、Caiming Xiong 和 Steven Hoi. Blip：用于统一视觉语言理解与生成的语言图像预训练引导.
    见于《国际机器学习会议》，第12888–12900页。PMLR，2022年。'
- en: '[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
    Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,
    Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition
    at scale. arXiv preprint arXiv:2010.11929, 2020.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Alexey Dosovitskiy、Lucas Beyer、Alexander Kolesnikov、Dirk Weissenborn、Xiaohua
    Zhai、Thomas Unterthiner、Mostafa Dehghani、Matthias Minderer、Georg Heigold、Sylvain
    Gelly 等. 一张图像相当于16x16个词：大规模图像识别的变换器. arXiv预印本 arXiv:2010.11929，2020年。'
- en: '[10] Prateek Chhikara, Ujjwal Pasupulety, John Marshall, Dhiraj Chaurasia,
    and Shweta Kumari. Privacy aware questionanswering system for online mental health
    risk assessment. In The 22nd Workshop on Biomedical Natural Language Processing
    and BioNLP Shared Tasks, pages 215– 222, Toronto, Canada, July 2023\. Association
    for Computational Linguistics.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Prateek Chhikara、Ujjwal Pasupulety、John Marshall、Dhiraj Chaurasia 和 Shweta
    Kumari. 面向在线心理健康风险评估的隐私意识问答系统. 见于《第22届生物医学自然语言处理与BioNLP共享任务研讨会论文集》，第215–222页，加拿大多伦多，2023年7月。计算语言学协会。'
- en: '[11] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
    Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer
    learning with a unified text-to-text transformer. The Journal of Machine Learning
    Research, 21(1):5485–5551, 2020.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Colin Raffel、Noam Shazeer、Adam Roberts、Katherine Lee、Sharan Narang、Michael
    Matena、Yanqi Zhou、Wei Li 和 Peter J Liu. 探索统一的文本到文本变换器在迁移学习中的极限. 《机器学习研究期刊》，21(1)：5485–5551，2020年。'
- en: '[12] Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Francisco Guzman,
    Luke Zettlemoyer, and Marjan ´ Ghazvininejad. Detecting hallucinated content in
    conditional neural sequence generation. In Findings of the Association for Computational
    Linguistics: ACL-IJCNLP 2021, pages 1393–1404, 2021.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Chunting Zhou、Graham Neubig、Jiatao Gu、Mona Diab、Francisco Guzman、Luke
    Zettlemoyer 和 Marjan Ghazvininejad. 在条件神经序列生成中检测幻觉内容. 见于《计算语言学协会发现：ACL-IJCNLP
    2021》，第1393–1404页，2021年。'
- en: '[13] Mehrdad Farahani and Kartik Godawat and Haswanth Aekula and Deepak Pandian
    and Nicholas Broad. Chef Transformer. [https://huggingface.co/flax-community/t5-](https://huggingface.co/flax-community/t5-)
    recipe-generation. Accessed on April 12, 2023.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Mehrdad Farahani、Kartik Godawat、Haswanth Aekula、Deepak Pandian 和 Nicholas
    Broad. Chef Transformer. [https://huggingface.co/flax-community/t5-](https://huggingface.co/flax-community/t5-)
    食谱生成。访问时间：2023年4月12日。'
- en: '[14] Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. Language
    models of code are few-shot commonsense learners. In Findings of the Association
    for Computational Linguistics: EMNLP 2022, 2022'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, 和 Graham Neubig. 代码语言模型是少量示例的常识学习者。载于《计算语言学协会发现：EMNLP
    2022》，2022年。'
