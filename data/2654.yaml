- en: 'TIME-MOE: Billion-Scale Time Series Foundation Model with Mixture-of-Experts'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TIME-MOE：亿级规模的时间序列基础模型，采用专家混合模型（Mixture-of-Experts）
- en: 原文：[https://towardsdatascience.com/time-moe-billion-scale-time-series-foundation-model-with-mixture-of-experts-7d165028124a?source=collection_archive---------4-----------------------#2024-10-31](https://towardsdatascience.com/time-moe-billion-scale-time-series-foundation-model-with-mixture-of-experts-7d165028124a?source=collection_archive---------4-----------------------#2024-10-31)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/time-moe-billion-scale-time-series-foundation-model-with-mixture-of-experts-7d165028124a?source=collection_archive---------4-----------------------#2024-10-31](https://towardsdatascience.com/time-moe-billion-scale-time-series-foundation-model-with-mixture-of-experts-7d165028124a?source=collection_archive---------4-----------------------#2024-10-31)
- en: And open-source as well!
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并且是开源的！
- en: '[](https://medium.com/@nikoskafritsas?source=post_page---byline--7d165028124a--------------------------------)[![Nikos
    Kafritsas](../Images/de965cfcd8fbd8e1baf849017d365cbb.png)](https://medium.com/@nikoskafritsas?source=post_page---byline--7d165028124a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7d165028124a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7d165028124a--------------------------------)
    [Nikos Kafritsas](https://medium.com/@nikoskafritsas?source=post_page---byline--7d165028124a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@nikoskafritsas?source=post_page---byline--7d165028124a--------------------------------)[![Nikos
    Kafritsas](../Images/de965cfcd8fbd8e1baf849017d365cbb.png)](https://medium.com/@nikoskafritsas?source=post_page---byline--7d165028124a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7d165028124a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7d165028124a--------------------------------)
    [Nikos Kafritsas](https://medium.com/@nikoskafritsas?source=post_page---byline--7d165028124a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7d165028124a--------------------------------)
    ·8 min read·Oct 31, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7d165028124a--------------------------------)
    ·阅读时长8分钟·2024年10月31日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/0433c89d3015ad05f8689369087cd24d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0433c89d3015ad05f8689369087cd24d.png)'
- en: '*A top-level view of**Time-MOE (*[*Image Source*](https://arxiv.org/pdf/2409.16040)*)*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*Time-MOE的顶层视图 (*[*图片来源*](https://arxiv.org/pdf/2409.16040)*)*'
- en: '**The Mixture-of-Experts (MOE) architecture has surged in popularity with the
    rise of large language models (LLMs).**'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**专家混合模型（MOE）架构**随着大规模语言模型（LLMs）的兴起而获得了广泛关注。'
- en: As time-series models adopt cutting-edge techniques, **Mixture-of-Experts**
    has naturally found its place in the time-series foundation space.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间序列模型采用尖端技术，**专家混合模型（Mixture-of-Experts）**自然地在时间序列基础模型领域占据了一席之地。
- en: 'This article discusses **Time-MOE**, a time-series foundation model that uses
    MOE to improve forecasting accuracy while reducing computational costs. Key contributions
    include:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文讨论了**Time-MOE**，一种使用MOE来提高预测准确性并降低计算成本的时间序列基础模型。关键贡献包括：
- en: '**Time-300B Dataset**: The largest open time-series dataset, with 300 billion
    time points across 9 domains, and a scalable data-cleaning pipeline.'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Time-300B 数据集**：世界上最大的开放时间序列数据集，包含跨越9个领域的3000亿时间点，并配备可扩展的数据清洗管道。'
- en: '**Scaling Laws for Time Series**: Insights into how scaling laws affect large
    time-series models.'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**时间序列的扩展法则**：深入探讨扩展法则如何影响大规模时间序列模型。'
- en: '**Time-MOE architecture**: A family of open-source time-series models leveraging
    MOE to enhance performance.'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Time-MOE架构**：一系列开源时间序列模型，利用MOE提升性能。'
- en: Let’s get started
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧
- en: '*✅* Find the **hands-on project** for ***Time-MOE*** in the [**AI Projects
    folder**](https://aihorizonforecast.substack.com/p/ai-projects), along with other
    cool projects!'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*✅* 在[**AI项目文件夹**](https://aihorizonforecast.substack.com/p/ai-projects)中找到**Time-MOE**的**实践项目**，以及其他精彩项目！'
- en: Enter Time-MOE
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进入Time-MOE
- en: Time-MOE is a 2.4B parameter open-source time-series foundation model using
    **Mixture-of-Experts (MOE)** forzero-shot forecasting
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Time-MOE是一个24亿参数的开源时间序列基础模型，采用**专家混合模型（Mixture-of-Experts，MOE）**进行零样本预测
