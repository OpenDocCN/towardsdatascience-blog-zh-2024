- en: Designing the Relationship Between LLMs and User Experience
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计大型语言模型（LLMs）与用户体验之间的关系
- en: 原文：[https://towardsdatascience.com/designing-the-relationship-between-llms-and-user-experience-6720f8fee998?source=collection_archive---------6-----------------------#2024-04-19](https://towardsdatascience.com/designing-the-relationship-between-llms-and-user-experience-6720f8fee998?source=collection_archive---------6-----------------------#2024-04-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/designing-the-relationship-between-llms-and-user-experience-6720f8fee998?source=collection_archive---------6-----------------------#2024-04-19](https://towardsdatascience.com/designing-the-relationship-between-llms-and-user-experience-6720f8fee998?source=collection_archive---------6-----------------------#2024-04-19)
- en: '[](https://medium.com/@janna.lipenkova_52659?source=post_page---byline--6720f8fee998--------------------------------)[![Dr.
    Janna Lipenkova](../Images/112fe9a8c5936869243f2a43fde6dfee.png)](https://medium.com/@janna.lipenkova_52659?source=post_page---byline--6720f8fee998--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6720f8fee998--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6720f8fee998--------------------------------)
    [Dr. Janna Lipenkova](https://medium.com/@janna.lipenkova_52659?source=post_page---byline--6720f8fee998--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@janna.lipenkova_52659?source=post_page---byline--6720f8fee998--------------------------------)[![Dr.
    Janna Lipenkova](../Images/112fe9a8c5936869243f2a43fde6dfee.png)](https://medium.com/@janna.lipenkova_52659?source=post_page---byline--6720f8fee998--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6720f8fee998--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6720f8fee998--------------------------------)
    [Dr. Janna Lipenkova](https://medium.com/@janna.lipenkova_52659?source=post_page---byline--6720f8fee998--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6720f8fee998--------------------------------)
    ·13 min read·Apr 19, 2024
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6720f8fee998--------------------------------)
    ·13分钟阅读·2024年4月19日
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/e1bb3550165ffc5ffb08c3d0cbd76f53.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1bb3550165ffc5ffb08c3d0cbd76f53.png)'
- en: A while ago, I wrote the article [Choosing the right language model for your
    NLP use case](https://medium.com/towards-data-science/choosing-the-right-language-model-for-your-nlp-use-case-1288ef3c4929)
    on Medium. It focussed on the nuts and bolts of LLMs — and while rather popular,
    by now, I realize it doesn’t actually say much about selecting LLMs. I wrote it
    at the beginning of my LLM journey and somehow figured that the technical details
    about LLMs — their inner workings and training history — would speak for themselves,
    allowing AI product builders to confidently select LLMs for specific scenarios.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 不久前，我在Medium上写了一篇文章，[选择适合您NLP用例的语言模型](https://medium.com/towards-data-science/choosing-the-right-language-model-for-your-nlp-use-case-1288ef3c4929)。文章重点介绍了大型语言模型（LLMs）的基本工作原理——尽管它相当受欢迎，但我现在意识到它其实并没有提供关于如何选择LLMs的很多信息。我在LLM学习之初写了这篇文章，当时我认为LLMs的技术细节——它们的内部结构和训练历史——能够自行说明问题，从而帮助AI产品开发者在特定场景中自信地选择LLMs。
- en: Since then, I have integrated LLMs into multiple AI products. This allowed me
    to discover how exactly the technical makeup of an LLM determines the final experience
    of a product. It also strengthened the belief that product managers and designers
    need to have a solid understanding of how an LLM works “under the hood.” LLM interfaces
    are different from traditional graphical interfaces. The latter provide users
    with a (hopefully clear) mental model by displaying the functionality of a product
    in a rather implicit way. On the other hand, LLM interfaces use free text as the
    main interaction format, offering much more flexibility. At the same time, they
    also “hide” the capabilities and the limitations of the underlying model, leaving
    it to the user to explore and discover them. Thus, a simple text field or chat
    window invites an infinite number of intents and inputs and can display as many
    different outputs.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 从那时起，我将LLMs集成到多个AI产品中。这让我发现了LLMs的技术构成如何决定产品最终体验的具体方式。它也加强了我的信念——产品经理和设计师需要对LLM的“幕后”运作有深入了解。LLM接口不同于传统的图形接口。后者通过以隐含的方式展示产品功能，提供给用户一个（希望是清晰的）心理模型。另一方面，LLM接口将自由文本作为主要的互动形式，提供了更多的灵活性。同时，它们也“隐藏”了底层模型的能力和局限性，让用户自行探索和发现。因此，一个简单的文本框或聊天窗口可以接收无限数量的意图和输入，并能显示出许多不同的输出。
- en: '![](../Images/18bdd4b643ac7011cb6e01a66a1d4b65.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18bdd4b643ac7011cb6e01a66a1d4b65.png)'
- en: Figure 1 A simple chat window is open for an infinite number of inputs (image
    via vectorstock.com under license purchased by author)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图1 显示了一个简单的聊天窗口，可以接收无限数量的输入（图片来自vectorstock.com，由作者购买授权）
- en: 'The responsibility for the success of these interactions is not (only) on the
    engineering side — rather, a big part of it should be assumed by whoever manages
    and designs the product. In this article, we will flesh out the relationship between
    LLMs and user experience, working with two universal ingredients that you can
    use to improve the experience of your product:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这些交互成功的责任不仅仅在工程方面——实际上，大部分责任应该由产品经理和设计者来承担。在本文中，我们将阐述LLM与用户体验之间的关系，并探讨你可以用来提升产品体验的两个通用要素：
- en: '**Functionality**, i.e., the tasks that are performed by an LLM, such as conversation,
    question answering, and sentiment analysis'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**功能**，即LLM执行的任务，例如对话、问答和情感分析'
- en: '**Quality** with which an LLM performs the task, including objective criteria
    such as correctness and coherence, but also subjective criteria such as an appropriate
    tone and style'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**质量**，即LLM执行任务的质量，包括正确性和一致性等客观标准，也包括语气和风格等主观标准'
- en: '*(Note: These two ingredients are part of any LLM application. Beyond these,
    most applications will also have a range of more individual criteria to be fulfilled,
    such as latency, privacy, and safety, which will not be addressed here.)*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*(注意：这两项内容是任何LLM应用的组成部分。除了这两项，大多数应用还会有一系列更为个性化的标准需要满足，例如延迟、隐私和安全性，本文不予讨论。)*'
- en: 'Thus, in Peter Drucker’s words, it’s about “doing the right things” (functionality)
    and “doing them right” (quality). Now, as we know, LLMs will never be 100% right.
    As a builder, you can approximate the ideal experience from two directions:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，正如彼得·德鲁克所说，这涉及到“做正确的事”（功能）和“做对的事”（质量）。如今，我们知道，LLM永远不可能做到100%的正确。作为构建者，你可以从两个方向来接近理想的体验：
- en: On the one hand, you need to strive for **engineering excellence** and make
    the right choices when selecting, fine-tuning, and evaluating your LLM.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一方面，你需要追求**工程卓越**，在选择、微调和评估LLM时做出正确的决策。
- en: On the other hand, you need to work your **users** by nudging them towards intents
    covered by the LLM, managing their expectations, and having routines that fire
    off when things go wrong.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，你需要通过引导你的**用户**朝着LLM所涵盖的目标迈进，管理他们的期望，并在出现问题时触发相应的处理流程。
- en: In this article, we will focus on the engineering part. The design of the ideal
    partnership with human users will be covered in a future article. First, I will
    briefly introduce the steps in the engineering process — LLM selection, adaptation,
    and evaluation — which directly determine the final experience. Then, we will
    look at the two ingredients — functionality and quality — and provide some guidelines
    to steer your work with LLMs to optimize the product’s performance along these
    dimensions.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将重点关注工程部分。与人类用户的理想合作关系将在未来的文章中讨论。首先，我将简要介绍工程过程中的步骤——LLM选择、适配和评估——这些步骤直接决定最终的体验。然后，我们将讨论两个要素——功能和质量——并提供一些指南，帮助你在这两个维度上优化LLM的工作，从而提升产品的表现。
- en: 'A note on scope: In this article, we will consider the use of stand-alone LLMs.
    Many of the principles and guidelines also apply to LLMs used in RAG (Retrieval-Augmented
    Generation) and agent systems. For a more detailed consideration of the user experience
    in these extended LLM scenarios, please refer to my book [The Art of AI Product
    Development](https://www.manning.com/books/the-art-of-ai-product-development).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 范围说明：在本文中，我们将讨论独立LLM的使用。许多原则和指南同样适用于用于RAG（检索增强生成）和代理系统中的LLM。若需更详细地了解这些扩展LLM场景中的用户体验，请参阅我的书籍[《AI产品开发的艺术》](https://www.manning.com/books/the-art-of-ai-product-development)。
- en: The LLM engineering process
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM工程过程
- en: 'In the following, we will focus on the three steps of LLM selection, adaptation,
    and evaluation. Let’s consider each of these steps:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将重点讨论LLM选择、适配和评估这三个步骤。让我们逐一探讨这些步骤：
- en: '**LLM selection** involves scoping your deployment options (in particular,
    open-source vs. commercial LLMs) and selecting an LLM whose training data and
    pre-training objective align with your target functionality. In addition, the
    more powerful the model you can select in terms of parameter size and training
    data quantity, the better the chances it will achieve a high quality.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**LLM选择**涉及确定你的部署选项（特别是开源与商业LLM）并选择一个其训练数据和预训练目标与目标功能对齐的LLM。此外，你能选择的模型越强大（无论是参数规模还是训练数据的数量），它实现高质量的可能性就越大。'
- en: '**LLM adaptation** via in-context learning or fine-tuning gives you the chance
    to close the gap between your users’ intents and the model''s original pre-training
    objective. Additionally, you can tune the model’s quality by incorporating the
    style and tone you would like your model to assume into the fine-tuning data.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**LLM适应**通过上下文学习或微调，给你提供了弥合用户意图与模型原始预训练目标之间差距的机会。此外，你还可以通过将你希望模型采用的风格和语气融入微调数据中，来调整模型的质量。'
- en: '**LLM evaluation** involves continuously evaluating the model across its lifecycle.
    As such, it is not a final step at the end of a process but a continuous activity
    that evolves and becomes more specific as you collect more insights and data on
    the model.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**LLM评估**涉及在模型生命周期中持续评估模型。因此，它不是流程结束时的最终步骤，而是一个持续的活动，随着你收集到更多关于模型的洞察和数据，它会逐渐变得更加具体。'
- en: 'The following figure summarizes the process:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图总结了该过程：
- en: '![](../Images/565cda173a35980b5c3e99bcad928b5f.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/565cda173a35980b5c3e99bcad928b5f.png)'
- en: Figure 2 Engineering the LLM user experience
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图2 LLM用户体验工程
- en: In real life, the three stages will overlap, and there can be back-and-forth
    between the stages. In general, model selection is more the “one big decision.”
    Of course, you can shift from one model to another further down the road and even
    should do this when new, more suitable models appear on the market. However, these
    changes are expensive since they affect everything downstream. Past the discovery
    phase, you will not want to make them on a regular basis. On the other hand, LLM
    adaptation and evaluation are highly iterative. They should be accompanied by
    continuous discovery activities where you learn more about the behavior of your
    model and your users. Finally, all three activities should be embedded into a
    solid LLMOps pipeline, which will allow you to integrate new insights and data
    with minimal engineering friction.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，这三个阶段会有所重叠，且各阶段之间可能会反复往返。通常，模型选择更像是“一次性的大决定”。当然，你可以在后续过程中从一个模型切换到另一个，甚至应当在新的、更合适的模型出现在市场时这样做。然而，这些更改是昂贵的，因为它们会影响所有下游工作。在发现阶段之后，你通常不希望频繁进行这些更改。另一方面，LLM适应和评估是高度迭代的。这些活动应伴随持续的发现过程，你将通过这些过程更多地了解模型的行为和用户的需求。最后，所有三个活动都应嵌入到一个稳固的LLMOps流水线中，这将使你能够以最小的工程摩擦整合新的洞察和数据。
- en: Now, let’s move to the second column of the chart, scoping the functionality
    of an LLM and learning how it can be shaped during the three stages of this process.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入图表的第二列，确定LLM的功能范围，并了解它如何在该过程中三个阶段中得到塑造。
- en: 'Functionality: responding to user intents'
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 功能：响应用户意图
- en: You might be wondering why we talk about the “functionality” of LLMs. After
    all, aren’t LLMs those versatile all-rounders that can magically perform any linguistic
    task we can think of? In fact, they are, as famously described in the paper [Language
    Models Are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf). LLMs can
    learn new capabilities from just a couple of examples. Sometimes, their capabilities
    will even “emerge” out of the blue during normal training and — hopefully — be
    discovered by chance. This is because the task of language modeling is just as
    versatile as it is challenging — as a side effect, it equips an LLM with the ability
    to perform many other related tasks.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道，为什么我们要讨论LLM的“功能”？毕竟，LLM不是那种能够神奇地完成我们能想到的任何语言任务的多才多艺的全能型工具吗？实际上，它们确实如此，正如论文[Language
    Models Are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)中著名描述的那样。LLM可以从仅仅几个例子中学习新能力。有时，它们的能力甚至会在正常训练中“突然出现”——并且——希望是偶然被发现的。这是因为语言建模的任务既多才多艺又充满挑战——作为副作用，它使得LLM具备了执行许多其他相关任务的能力。
- en: 'Still, the pre-training objective of LLMs is to **generate the next word given
    the context of past words** (OK, that’s a simplification — in auto-encoding, the
    LLM can work in both directions [3]). This is what a pre-trained LLM, motivated
    by an imaginary “reward,” will insist on doing once it is prompted. In most cases,
    there is quite a gap between this objective and a user who comes to your product
    to chat, get answers to questions, or translate a text from German to Italian.
    The landmark paper [Climbing Towards NLU: On Meaning, Form, and Understanding
    in the Age of Data](https://aclanthology.org/2020.acl-main.463.pdf) by Emily Bender
    and Alexander Koller even argues that language models are generally unable to
    recover communicative intents and thus are doomed to work with incomplete meaning
    representations.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，LLM的预训练目标是**根据过去词语的上下文生成下一个词**（好吧，这是一个简化的说法——在自编码中，LLM可以双向工作[3]）。这就是一个预训练的LLM在被提示后会坚持做的事情，动机来自一个虚构的“奖励”。在大多数情况下，这个目标与一个用户使用你的产品进行聊天、寻求问题的答案，或是将一段德文翻译成意大利文之间有着相当大的差距。Emily
    Bender和Alexander Koller的标志性论文[《迈向自然语言理解：在数据时代的意义、形式与理解》](https://aclanthology.org/2020.acl-main.463.pdf)甚至认为语言模型通常无法恢复沟通意图，因此注定只能处理不完整的意义表示。
- en: Thus, it is one thing to brag about amazing LLM capabilities in scientific research
    and demonstrate them on highly controlled benchmarks and test scenarios. Rolling
    out an LLM to an anonymous crowd of users with different AI skills and intents—some
    harmful—is a different kind of game. This is especially true once you understand
    that your product inherits not only the capabilities of the LLM but also its weaknesses
    and risks, and you (not a third-party provider) hold the responsibility for its
    behavior.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，夸耀在科学研究中令人惊叹的LLM能力并在高度控制的基准和测试场景中展示它们是一回事。而将LLM推出给一群具有不同AI技能和目的——其中一些可能是有害的——匿名用户则是另一回事。这一点尤其正确，一旦你明白你的产品不仅继承了LLM的能力，还继承了它的弱点和风险，并且你（而非第三方提供商）需要对其行为负责。
- en: 'In practice, we have learned that it is best to identify and isolate discrete
    islands of functionality when integrating LLMs into a product. These functions
    can largely correspond to the different intents with which your users come to
    your product. For example, it could be:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们已经学到，在将LLM集成到产品中时，最好识别并隔离离散的功能模块。这些功能大致对应于用户使用你的产品时的不同意图。例如，它可能是：
- en: Engaging in conversation
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行对话
- en: Retrieving information
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索信息
- en: Seeking recommendations for a specific situation
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻求特定情境的建议
- en: Looking for inspiration
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找灵感
- en: 'Oftentimes, these can be further decomposed into more granular, potentially
    even reusable, capabilities. “Engaging in conversation” could be decomposed into:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这些能力通常可以进一步分解成更细化、甚至是可复用的能力。例如，“进行对话”可以分解为：
- en: Provide informative and relevant conversational turns
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供有信息量和相关性的对话回复
- en: Maintain a memory of past interactions (instead of starting from scratch at
    every turn)
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维护过去交互的记忆（而不是每次都从头开始）
- en: Display a consistent personality
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 展示一致的个性
- en: 'Taking this more discrete approach to LLM capabilities provides you with the
    following advantages:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 采用这种更加离散的方法来处理LLM的能力，能为你带来以下优势：
- en: ML engineers and data scientists can better focus their engineering activities
    (Figure 2) on the target functionalities.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ML工程师和数据科学家可以更好地专注于他们的工程活动（见图2），聚焦于目标功能。
- en: Communication about your product becomes on-point and specific, helping you
    manage user expectations and preserving trust, integrity, and credibility.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于你的产品的沟通变得精准和具体，有助于管理用户期望，并维护信任、诚信和信誉。
- en: In the user interface, you can use a range of design patterns, such as prompt
    templates and placeholders, to increase the chances that user intents are aligned
    with the model’s functionality.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在用户界面中，你可以使用一系列设计模式，如提示模板和占位符，以提高用户意图与模型功能对齐的机会。
- en: Guidelines for ensuring the right functionality
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确保正确功能的指南
- en: 'Let’s summarize some practical guidelines to make sure that the LLM does the
    right thing in your product:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一些实用的指南，确保LLM在你的产品中做对了事：
- en: During LLM selection, make sure you understand the basic pre-training objective
    of the model. There are three basic pre-training objectives (auto-encoding, autoregression,
    sequence-to-sequence), and each of them influences the behavior of the model.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在LLM选择过程中，确保你了解模型的基本预训练目标。预训练目标有三种基本类型（自编码、自动回归、序列到序列），它们每种都会影响模型的行为。
- en: Many LLMs are also pre-trained with an advanced objective, such as conversation
    or executing explicit instructions (instruction fine-tuning). Selecting a model
    that is already prepared for your task will grant you an efficient head start,
    reducing the amount of downstream adaptation and fine-tuning you need to do to
    achieve satisfactory quality.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多LLM也已经通过更高级的目标进行预训练，例如对话或执行明确指令（指令微调）。选择一个已经为你的任务做好准备的模型，可以让你高效地起步，减少你在下游适配和微调中所需的工作量，从而实现满意的质量。
- en: LLM adaptation via in-context learning or fine-tuning gives you the opportunity
    to close the gap between the original pre-training objective and the user intents
    you want to serve.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过上下文学习或微调进行LLM适配，可以为你提供一个机会，弥合原始预训练目标和你希望服务的用户意图之间的差距。
- en: '![](../Images/7dbefddedca2b9f8e0073900caaff6a4.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7dbefddedca2b9f8e0073900caaff6a4.png)'
- en: Figure 3 LLM adaptation closes the gap between pre-training objectives and user
    intents
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图3 LLM适配缩小了预训练目标和用户意图之间的差距
- en: During the initial discovery, you can use in-context learning to collect initial
    usage data and sharpen your understanding of relevant user intents and their distribution.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在初期发现阶段，你可以通过上下文学习来收集初步的使用数据，并加深对相关用户意图及其分布的理解。
- en: In most scenarios, in-context learning (prompt tuning) is not sustainable in
    the long term — it is simply not efficient. Over time, you can use your new data
    and learnings as a basis to fine-tune the weights of the model.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大多数情况下，长远来看，上下文学习（提示微调）是不可持续的——它本身并不高效。随着时间的推移，你可以利用新数据和学习成果作为基础，对模型的权重进行微调。
- en: During model evaluation, make sure to apply task-specific metrics. For example,
    Text2SQL LLMs (cf. [this article](https://medium.com/towards-data-science/enabling-the-data-driven-organisation-with-text2sql-f8e07089dd0c))
    can be evaluated using metrics like execution accuracy and test-suite accuracy,
    while summarization can be evaluated using similarity-based metrics.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在模型评估过程中，确保应用特定任务的度量标准。例如，Text2SQL LLM（参见[这篇文章](https://medium.com/towards-data-science/enabling-the-data-driven-organisation-with-text2sql-f8e07089dd0c)）可以通过执行准确率和测试集准确率等指标进行评估，而摘要生成可以通过基于相似度的指标进行评估。
- en: These are just short snapshots of the lessons we learned when integrating LLMs.
    My upcoming book [The Art of AI Product Development](https://www.manning.com/books/the-art-of-ai-product-development)
    contains deep dives into each of the guidelines along with numerous examples.
    For the technical details behind pre-training objectives and procedures, you can
    refer to [this article](https://medium.com/towards-data-science/choosing-the-right-language-model-for-your-nlp-use-case-1288ef3c4929).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是我们在整合LLM时学到的课程的简短片段。我即将出版的书籍《[人工智能产品开发的艺术](https://www.manning.com/books/the-art-of-ai-product-development)》将深入探讨每一条指南，并提供大量的示例。关于预训练目标和过程的技术细节，你可以参考[这篇文章](https://medium.com/towards-data-science/choosing-the-right-language-model-for-your-nlp-use-case-1288ef3c4929)。
- en: Ok, so you have gained an understanding of the intents with which your users
    come to your product and “motivated” your model to respond to these intents. You
    might even have put out the LLM into the world in the hope that it will kick off
    the data flywheel. Now, if you want to keep your good-willed users and acquire
    new users, you need to quickly ramp up on our second ingredient, namely quality.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在你已经了解了用户使用你的产品时的意图，并“激发”了你的模型来响应这些意图。你甚至可能已经将LLM投入到实际环境中，希望它能够启动数据飞轮。现在，如果你想保持现有用户并吸引新用户，你需要迅速提升第二个关键要素——质量。
- en: Achieving a high quality
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现高质量
- en: In the context of LLMs, quality can be decomposed into an objective and a subjective
    component. The objective component tells you when and why things go wrong (i.e.,
    the LLM makes explicit mistakes). The subjective component is more subtle and
    emotional, reflecting the alignment with your specific user crowd.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM的背景下，质量可以分解为客观和主观两个组成部分。客观部分告诉你何时以及为何出现问题（即LLM犯了明显的错误）。主观部分则更为微妙和情感化，反映了与特定用户群体的契合度。
- en: Objective quality criteria
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标质量标准
- en: Using language to communicate comes naturally to humans. Language is ingrained
    in our minds from the beginning of our lives, and we have a hard time imagining
    how much effort it takes to learn it from scratch. Even the challenges we experience
    when learning a foreign language can’t compare to the training of an LLM. The
    LLM starts from a blank slate, while our learning process builds on an incredibly
    rich basis of existing knowledge about the world and about how language works
    in general.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用语言进行交流对人类来说是自然而然的。语言从我们生命的开始就深深扎根在我们的大脑中，我们很难想象从零开始学习语言需要付出多少努力。即便是我们在学习外语时所遇到的挑战，也无法与
    LLM 的训练相比。LLM 从一张空白的纸开始，而我们学习语言的过程则建立在一个极其丰富的世界知识基础和语言运作的基本理解之上。
- en: 'When working with an LLM, we should constantly remain aware of the many ways
    in which things can go wrong:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 LLM 时，我们应该时刻保持意识，注意到许多可能出现的错误：
- en: The LLM might make linguistic mistakes.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM 可能会犯语言错误。
- en: The LLM might slack on coherence, logic, and consistency.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM 可能在连贯性、逻辑性和一致性方面表现欠佳。
- en: The LLM might have insufficient world knowledge, leading to wrong statements
    and hallucinations.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM 可能缺乏足够的世界知识，从而导致错误的陈述和幻觉。
- en: These shortcomings can quickly turn into showstoppers for your product — output
    quality is a central determinant of the user experience of an LLM product. For
    example, one of the major determinants of the “public” success of ChatGPT was
    that it was indeed able to generate correct, fluent, and relatively coherent text
    across a large variety of domains. Earlier generations of LLMs were not able to
    achieve this objective quality. Most pre-trained LLMs that are used in production
    today do have the capability to generate language. However, their performance
    on criteria like coherence, consistency, and world knowledge can be very variable
    and inconsistent. To achieve the experience you are aiming for, it is important
    to have these requirements clearly prioritized and select and adapt LLMs accordingly.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这些缺点可能很快就会成为你产品的致命问题——输出质量是 LLM 产品用户体验的核心决定因素。例如，ChatGPT 在“公开”成功的一个主要因素是它确实能够在各个领域生成正确、流畅且相对连贯的文本。早期版本的
    LLM 无法实现这一目标质量。如今大多数用于生产的预训练 LLM 确实具备生成语言的能力。然而，它们在连贯性、一致性和世界知识等标准上的表现可能非常不稳定且不一致。为了实现你期望的体验，明确优先排序这些要求，并据此选择和调整
    LLM 是非常重要的。
- en: Subjective quality criteria
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主观质量标准
- en: Venturing into the more nuanced subjective domain, you want to understand and
    monitor how users feel around your product. Do they feel good and trustful and
    get into a state of flow when they use it? Or do they go away with feelings of
    frustration, inefficiency, and misalignment? A lot of this hinges on individual
    nuances of culture, values, and style. If you are building a copilot for junior
    developers, you hardly want it to speak the language of senior executives and
    vice versa.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在更为微妙的主观领域，您需要了解并监控用户对产品的感受。他们在使用产品时是否感到愉快、信任，并进入心流状态？还是他们带着沮丧、低效和不对劲的感觉离开？这些都与个人的文化、价值观和风格息息相关。如果你正在为初级开发人员构建一个副驾驶，你显然不希望它使用高级管理人员的语言，反之亦然。
- en: For the sake of example, imagine you are a product marketer. You have spent
    a lot of your time with a fellow engineer to iterate on an LLM that helps you
    with content generation. At some point, you find yourself chatting with the UX
    designer on your team and bragging about your new AI assistant. Your colleague
    doesn’t get the need for so much effort. He is regularly using ChatGPT to assist
    with the creation and evaluation of UX surveys and is very satisfied with the
    results. You counter — ChatGPT’s outputs are too generic and monotonous for your
    storytelling and writing tasks. In fact, you have been using it at the beginning
    and got quite embarrassed because, at some point, your readers started to recognize
    the characteristic ChatGPT flavor. That was a slippery episode in your career,
    after which you decided you needed something more sophisticated.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，假设你是一个产品营销人员，你花了很多时间和一个工程师一起反复修改一个帮助你进行内容生成的 LLM。某一天，你和团队中的 UX 设计师聊天，向他炫耀你的新
    AI 助手。你的同事不理解为何要付出这么多努力。他平时使用 ChatGPT 来帮助创建和评估 UX 调查，并对结果非常满意。你反驳道——ChatGPT 输出的内容对于你的故事叙述和写作任务来说过于通用且单调。事实上，刚开始使用时，你也曾遇到过相当尴尬的情况，因为有一段时间，读者开始识别出
    ChatGPT 特有的风格。那是你职业生涯中的一个滑铁卢，之后你决定需要一个更复杂的工具。
- en: There is no right or wrong in this discussion. ChatGPT is good for straightforward
    factual tasks where style doesn’t matter that much. By contrast, you as a marketer
    need an assistant that can assist in crafting high-quality, persuasive communications
    that speak the language of your customers and reflect the unique DNA of your company.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个讨论中没有对错之分。ChatGPT适合处理那些直接的、事实性的任务，在这些任务中风格并不那么重要。相反，作为市场营销人员，你需要一个能够帮助你打造高质量、具有说服力的沟通内容的助手，这些内容能够讲述你客户的语言，并且反映你公司独特的DNA。
- en: These subjective nuances can ultimately define the difference between an LLM
    that is useless because its outputs need to be rewritten anyway and one that is
    “good enough” so users start using it and feed it with suitable fine-tuning data.
    The holy grail of LLM mastery is personalization — i.e., using efficient fine-tuning
    or prompt tuning to adapt the LLM to the individual preferences of any user who
    has spent a certain amount of time with the model. If you are just starting out
    on your LLM journey, these details might seem far off — but in the end, they can
    help you reach a level where your LLM delights users by responding in the exact
    manner and style that is desired, spurring user satisfaction and large-scale adoption
    and leaving your competition behind.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这些主观的细微差别最终可能决定一个LLM是否有用——即使它的输出最终需要重写，还是能够定义为“足够好”，让用户开始使用它并提供合适的微调数据。LLM掌握的圣杯是个性化——即通过高效的微调或提示调优，使LLM适应任何与模型有一定时间互动的用户的个人偏好。如果你刚刚开始LLM之旅，这些细节可能看起来还很遥远——但最终，它们将帮助你达到一个阶段，让你的LLM通过以精确的方式和风格回应用户的需求，带来用户满意度和大规模采用，并把你的竞争对手甩在身后。
- en: Guidelines
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指南
- en: 'Here are our tips for managing the quality of your LLM:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们在管理LLM质量方面的建议：
- en: Be alert to different kinds of feedback. The quest for quality is continuous
    and iterative — you start with a few data points and a very rough understanding
    of what quality means for your product. Over time, you flesh out more and more
    details and learn which levers you can pull to improve your LLM.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 留意不同种类的反馈。对质量的追求是持续的和迭代的——你从一些数据点和对产品质量含义的粗略理解开始。随着时间的推移，你会逐渐丰富更多细节，学习哪些杠杆可以用来改善你的LLM。
- en: During model selection, you still have a lot of discovery to do — start with
    “eyeballing” and testing different LLMs with various inputs (ideally by multiple
    team members).
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在模型选择过程中，你仍然有很多发现要做——从“眼球”判断开始，并通过不同的输入测试不同的LLM（最好由多个团队成员来做）。
- en: Your engineers will also be evaluating academic benchmarks and evaluation results
    that are published together with the model. However, keep in mind that these are
    only rough indicators of how the model will perform in your specific product.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的工程师们也会评估学术基准和与模型一起发布的评估结果。然而，请记住，这些只是模型在你特定产品中的表现的粗略指标。
- en: At the beginning, perfectionism isn’t the answer. Your model should be just
    good enough to attract users who will start supplying it with relevant data for
    fine-tuning and evaluation.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一开始，完美主义不是答案。你的模型应该足够好，以吸引那些能够开始为其提供相关数据以进行微调和评估的用户。
- en: Bring your team and users together for qualitative discussions of LLM outputs.
    As they use language to judge and debate what is right and what is wrong, you
    can gradually uncover their objective and emotional expectations.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将你的团队和用户聚集起来，进行定性的LLM输出讨论。当他们用语言来判断和辩论什么是对的，什么是错的时，你可以逐渐揭示出他们的客观和情感需求。
- en: Make sure to have a solid LLMOps pipeline in place so you can integrate new
    data smoothly, reducing engineering friction.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保建立一个完善的LLMOps流程，这样你可以顺利地集成新数据，减少工程上的摩擦。
- en: Don’t stop — at later stages, you can shift your focus toward nuances and personalization,
    which will also help you sharpen your competitive differentiation.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要停下来——在后期阶段，你可以将焦点转向细微差别和个性化，这也将帮助你加强竞争优势。
- en: 'To sum up: assuming responsibility'
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结：承担责任
- en: Pre-trained LLMs are highly convenient — they make AI accessible to everyone,
    offloading the huge engineering, computation, and infrastructure spending needed
    to train a huge initial model. Once published, they are ready to use, and we can
    plug their amazing capabilities into our product. However, when using a third-party
    model in your product, you inherit not only its power but also the many ways in
    which it can and will fail. When things go wrong, the last thing you want to do
    to maintain integrity is to blame an external model provider, your engineers,
    or — worse — your users.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的大语言模型非常方便——它们让AI变得对每个人都可访问，减轻了训练大型初始模型所需的巨额工程、计算和基础设施开支。一旦发布，它们可以立即使用，我们可以将其惊人的功能融入到我们的产品中。然而，当在产品中使用第三方模型时，你不仅继承了它的强大功能，还继承了它可能失败的多种方式。当事情出错时，为了保持产品的完整性，你最不希望做的事情就是将责任归咎于外部模型提供商、你的工程师，或者——更糟糕——你的用户。
- en: Thus, when building with LLMs, you should not only look for transparency into
    the model's origins (training data and process) but also build a causal understanding
    of how its technical makeup shapes the experience offered by your product. This
    will allow you to find the sensitive balance between kicking off a robust data
    flywheel at the beginning of your journey and continuously optimizing and differentiating
    the LLM as your product matures toward excellence.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在使用大语言模型（LLMs）时，你不仅应该了解模型的来源（训练数据和过程）的透明度，还要建立因果理解，了解其技术构成如何影响你的产品所提供的体验。这将帮助你在开始阶段找到启动强大数据飞轮和在产品逐渐走向卓越时持续优化和差异化大语言模型之间的敏感平衡。
- en: References
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Janna Lipenkova (2022). [Choosing the right language model for your NLP
    use case](https://medium.com/towards-data-science/choosing-the-right-language-model-for-your-nlp-use-case-1288ef3c4929),
    Medium.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Janna Lipenkova（2022年）。[《为你的NLP用例选择合适的语言模型》](https://medium.com/towards-data-science/choosing-the-right-language-model-for-your-nlp-use-case-1288ef3c4929)，Medium。'
- en: '[2] Tom B. Brown et al. (2020). [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165).'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Tom B. Brown 等（2020年）。[《语言模型是少量示例学习者》](https://arxiv.org/abs/2005.14165)。'
- en: '[3] Jacob Devlin et al. (2018). [BERT: Pre-training of Deep Bidirectional Transformers
    for Language Understanding](https://arxiv.org/abs/1810.04805).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Jacob Devlin 等（2018年）。[《BERT：用于语言理解的深度双向Transformer预训练》](https://arxiv.org/abs/1810.04805)。'
- en: '[4] Emily M. Bender and Alexander Koller (2020). [Climbing towards NLU: On
    Meaning, Form, and Understanding in the Age of Data](https://aclanthology.org/2020.acl-main.463.pdf).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Emily M. Bender 和 Alexander Koller（2020年）。[《迈向自然语言理解：在数据时代的意义、形式和理解》](https://aclanthology.org/2020.acl-main.463.pdf)。'
- en: '[5] Janna Lipenkova (upcoming). [The Art of AI Product Development](https://www.manning.com/books/the-art-of-ai-product-development),
    Manning Publications.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Janna Lipenkova（即将出版）。[《AI产品开发艺术》](https://www.manning.com/books/the-art-of-ai-product-development)，Manning出版公司。'
- en: 'Note: All images are by the author, except when noted otherwise.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 注：所有图片均由作者提供，除非另有说明。
