- en: 'Mistral AI vs. Meta: Comparing Top Open-source LLMs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Mistral AI与Meta：对比顶级开源LLM
- en: 原文：[https://towardsdatascience.com/mistral-ai-vs-meta-comparing-top-open-source-llms-565c1bc1516e?source=collection_archive---------1-----------------------#2024-01-23](https://towardsdatascience.com/mistral-ai-vs-meta-comparing-top-open-source-llms-565c1bc1516e?source=collection_archive---------1-----------------------#2024-01-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/mistral-ai-vs-meta-comparing-top-open-source-llms-565c1bc1516e?source=collection_archive---------1-----------------------#2024-01-23](https://towardsdatascience.com/mistral-ai-vs-meta-comparing-top-open-source-llms-565c1bc1516e?source=collection_archive---------1-----------------------#2024-01-23)
- en: A comparison between Mistral 7B vs Llama 2 7B and Mixtral 8x7B vs Llama 2 70B
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这是对Mistral 7B与Llama 2 7B、Mixtral 8x7B与Llama 2 70B的比较。
- en: '[](https://medium.com/@luisroque?source=post_page---byline--565c1bc1516e--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page---byline--565c1bc1516e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--565c1bc1516e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--565c1bc1516e--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page---byline--565c1bc1516e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@luisroque?source=post_page---byline--565c1bc1516e--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page---byline--565c1bc1516e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--565c1bc1516e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--565c1bc1516e--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page---byline--565c1bc1516e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--565c1bc1516e--------------------------------)
    ·16 min read·Jan 23, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--565c1bc1516e--------------------------------)
    ·阅读时间：16分钟·2024年1月23日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '*This post was co-authored with Rafael Guedes.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*本文由Rafael Guedes和我共同撰写。*'
- en: Introduction
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Latest developments in Natural Language Processing and, particularly, in Large
    Language Models (LLMs) are focused on improving model performance, which often
    leads to an increase in model size. As one can expect, the escalation in model
    size also increases computational costs and inference latency, raising barriers
    when it comes to deploying and using LLMs in real-world scenarios.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）的最新发展中，特别是在大型语言模型（LLMs）方面，重点是提升模型性能，这通常会导致模型规模的增大。可以预见的是，模型规模的扩大也增加了计算成本和推理延迟，带来了在实际应用场景中部署和使用LLM时的障碍。
- en: Mistral AI, a European company based in Paris, has been researching how to improve
    model performance and, at the same time, reduce the computational resources needed
    to deploy LLMs for practical use cases. Mistral 7B is the smallest LLM they created
    that brings two novel concepts to the traditional Transformer architecture, Group-Query
    Attention (GQA) and Sliding Window Attention (SWA). These components accelerate
    the inference speed and reduce memory requirements during decoding enabling a
    higher throughput and the ability to handle longer sequences of tokens without
    sacrificing the quality of the responses generated compared to Llama 2 7B in benchmark
    datasets.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral AI是一家总部位于巴黎的欧洲公司，他们一直在研究如何提高模型性能，同时减少部署LLM所需的计算资源，以便能在实际用例中应用。Mistral
    7B是他们开发的最小型LLM，它将两个创新概念引入到传统的Transformer架构中，分别是Group-Query Attention（GQA）和Sliding
    Window Attention（SWA）。这些组件加速了推理速度，并减少了在解码过程中对内存的需求，从而实现更高的吞吐量，并能够处理更长的令牌序列，同时不牺牲生成的响应质量，相比于在基准数据集上的Llama
    2 7B表现更佳。
- en: Mistral 7B is not the only model they have developed, they also created Mixtral
    8x7B to compete with larger LLMs like Llama 2 70B. Apart from using GQA and SWA,
    this version also adds a third…
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral 7B并不是他们唯一开发的模型，他们还创建了Mixtral 8x7B来与更大规模的LLM（如Llama 2 70B）竞争。除了使用GQA和SWA外，这个版本还增加了第三个…
