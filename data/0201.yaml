- en: How does ReLU enable Neural Networks to approximate continuous nonlinear functions?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ReLU如何使神经网络能够逼近连续非线性函数？
- en: 原文：[https://towardsdatascience.com/how-relu-enables-neural-networks-to-approximate-continuous-nonlinear-functions-f171b7859727?source=collection_archive---------1-----------------------#2024-01-21](https://towardsdatascience.com/how-relu-enables-neural-networks-to-approximate-continuous-nonlinear-functions-f171b7859727?source=collection_archive---------1-----------------------#2024-01-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-relu-enables-neural-networks-to-approximate-continuous-nonlinear-functions-f171b7859727?source=collection_archive---------1-----------------------#2024-01-21](https://towardsdatascience.com/how-relu-enables-neural-networks-to-approximate-continuous-nonlinear-functions-f171b7859727?source=collection_archive---------1-----------------------#2024-01-21)
- en: Learn how neural networks with one hidden layer using ReLU activation represent
    continuous nonlinear functions.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解如何通过使用ReLU激活的单隐藏层神经网络来表示连续非线性函数。
- en: '[](https://medium.com/@lamthuy.lt?source=post_page---byline--f171b7859727--------------------------------)[![Thi-Lam-Thuy
    LE](../Images/a5af515421fe186f5ad47c532932af03.png)](https://medium.com/@lamthuy.lt?source=post_page---byline--f171b7859727--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f171b7859727--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f171b7859727--------------------------------)
    [Thi-Lam-Thuy LE](https://medium.com/@lamthuy.lt?source=post_page---byline--f171b7859727--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@lamthuy.lt?source=post_page---byline--f171b7859727--------------------------------)[![Thi-Lam-Thuy
    LE](../Images/a5af515421fe186f5ad47c532932af03.png)](https://medium.com/@lamthuy.lt?source=post_page---byline--f171b7859727--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f171b7859727--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f171b7859727--------------------------------)
    [Thi-Lam-Thuy LE](https://medium.com/@lamthuy.lt?source=post_page---byline--f171b7859727--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f171b7859727--------------------------------)
    ·5 min read·Jan 21, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f171b7859727--------------------------------)
    ·5分钟阅读·2024年1月21日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Activation functions play an integral role in Neural Networks *(NNs)* since
    they introduce non-linearity that allows the network to learn more complex features
    and functions than just a linear regression. One of the most commonly used activation
    functions is Rectified Linear Unit *(ReLU),* which has been [theoretically](https://en.wikipedia.org/wiki/Universal_approximation_theorem)
    shown to enable NNs to approximate a wide range of continuous functions, making
    them powerful function approximators.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数在神经网络*(NN)*中起着至关重要的作用，因为它们引入了非线性，使得网络能够学习比简单线性回归更复杂的特征和函数。最常用的激活函数之一是修正线性单元*(ReLU)*，已经有[理论研究](https://en.wikipedia.org/wiki/Universal_approximation_theorem)表明，ReLU能够使神经网络逼近广泛的连续函数，使其成为强大的函数逼近器。
- en: 'In this post, we study in particular the approximation of Continuous NonLinear
    *(CNL)* functions, the main purpose of using a NN over a simple linear regression
    model. More precisely, we investigate 2 sub-categories of CNL functions: Continuous
    PieceWise Linear *(CPWL)*, and Continuous Curve *(CC)* functions. We will show
    how these two function types can be represented using a NN that consists of one
    hidden layer, given enough neurons with ReLU activation.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们特别研究了连续非线性*(CNL)*函数的逼近，这是使用神经网络（NN）而非简单线性回归模型的主要目的。更具体地，我们考察了CNL函数的两个子类别：连续分段线性*(CPWL)*函数和连续曲线*(CC)*函数。我们将展示如何使用包含一个隐藏层的神经网络，通过足够的神经元和ReLU激活，来表示这两种函数类型。
- en: For illustrative purposes, we consider only single feature inputs yet the idea
    applies to multiple feature inputs as well.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明问题，我们仅考虑单一特征输入，但这个概念同样适用于多个特征输入。
- en: ReLU activation
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ReLU激活
- en: '![](../Images/e249461664cadb9fb803d9a1a41537b3.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e249461664cadb9fb803d9a1a41537b3.png)'
- en: 'Figure 1: Rectified Linear Unit (ReLU) function.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：修正线性单元（ReLU）函数。
- en: 'ReLU is a piecewise linear function that consists of two linear pieces: one
    that cuts off negative values where the output is zero, and one that provides
    a continuous linear mapping for non negative values.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU是一个分段线性函数，由两部分线性函数组成：一部分将负值截断，输出为零；另一部分为非负值提供连续的线性映射。
- en: Continuous piecewise linear function approximation
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连续分段线性函数逼近
- en: '*CPWL functions are continuous functions with multiple linear portions. The
    slope is consistent on each portion, then changes abruptly at transition points
    by adding new linear functions.*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*CPWL函数是具有多个线性部分的连续函数。每个部分的斜率是一致的，然后通过添加新的线性函数在过渡点发生突变。*'
- en: '![](../Images/f83d489627298162c04ec44a6e727c2f.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f83d489627298162c04ec44a6e727c2f.png)'
- en: 'Figure 2: Example of CPWL function approximation using NN. At each transition
    point, a new ReLU function is added to/subtracted from the input to increase/decrease
    the slope.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：使用神经网络（NN）进行CPWL函数近似的示例。在每个过渡点，新的ReLU函数被加入到/从输入中减去，以增加/减少斜率。
- en: In a NN with one hidden layer using ReLU activation and a linear output layer,
    the activation outputs are aggregated to form the CPWL target function. Each unit
    of the hidden layer is responsible for a linear piece. At each unit, a new ReLU
    function that corresponds to the changing of slope is added to produce the new
    slope *(cf. Fig.2)*. Since this activation function is always positive, the weights
    of the output layer corresponding to units that increase the slope will be positive,
    and conversely, the weights corresponding to units that decreases the slope will
    be negative *(cf. Fig.3)*. The new function is added at the transition point and
    does not contribute to the resulting function prior to (and sometimes after) that
    point due to the disabling range of the ReLU activation function.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用ReLU激活函数和线性输出层的单隐藏层神经网络中，激活输出被聚合以形成CPWL目标函数。隐藏层的每个单元负责一个线性片段。在每个单元处，新增的ReLU函数对应于斜率的变化，从而产生新的斜率*（参见图2）*。由于此激活函数始终为正，增加斜率的单元对应的输出层权重将为正，反之，减小斜率的单元对应的权重将为负*（参见图3）*。新函数在过渡点添加，并且由于ReLU激活函数的禁用范围，它在该点之前（有时也在该点之后）不会对结果函数产生贡献。
- en: '![](../Images/625c25d0d2b613b371adb52150ba4fd0.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/625c25d0d2b613b371adb52150ba4fd0.png)'
- en: 'Figure 3: Approximation of the CPWL target function in Fig.2 using a NN that
    consists of one hidden layer with ReLU activation and a linear output layer.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：使用包含一个带ReLU激活和线性输出层的隐藏层的神经网络对图2中的CPWL目标函数的近似。
- en: '**Example**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例**'
- en: To make it more concrete, we consider an example of a CPWL function that consists
    of 4 linear segments defined as below.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让它更加具体，我们考虑一个由4个线性片段组成的CPWL函数，定义如下。
- en: '![](../Images/55c82ec41548f85c2cc586fc300cac56.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55c82ec41548f85c2cc586fc300cac56.png)'
- en: 'Figure 4: Example of a PWL function.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：PWL函数的示例。
- en: To represent this target function, we will use a NN with 1 hidden layer of 4
    units and a linear layer that outputs the weighted sum of the previous layer’s
    activation outputs. Let’s determine the network’s parameters so that each unit
    in the hidden layer represents a segment of the target. For the sake of this example,
    the bias of the output layer *(b2_0)* is set to 0.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了表示该目标函数，我们将使用一个包含4个单元的1隐藏层神经网络，并且一个线性层输出前一层激活输出的加权和。我们来确定网络的参数，以便隐藏层的每个单元表示目标的一个片段。为了这个示例，输出层的偏置*(b2_0)*设置为0。
- en: '![](../Images/980a1377e98898a948078d0ac3d8018f.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/980a1377e98898a948078d0ac3d8018f.png)'
- en: 'Figure 5: The network architecture to model the PWL function defined in Fig.4.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：建模图4中定义的PWL函数的网络架构。
- en: '![](../Images/c8c5123073a3fe737f0b292152ad1d8a.png)![](../Images/c2dc3ff0289cbb5110ed968fc1cfebb7.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c8c5123073a3fe737f0b292152ad1d8a.png)![](../Images/c2dc3ff0289cbb5110ed968fc1cfebb7.png)'
- en: 'Figure 6: The activation output of unit 0 (a1_0).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：单元0（a1_0）的激活输出。
- en: '![](../Images/479c32a0674d6f52eecae6d71b0cfc19.png)![](../Images/af866141263ed92ed63c95f0222a2544.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/479c32a0674d6f52eecae6d71b0cfc19.png)![](../Images/af866141263ed92ed63c95f0222a2544.png)'
- en: 'Figure 7: The activation output of unit 1 (a1_1), which is aggregated to the
    output *(a2_0)* to produce the segment (2). The red arrow represents the change
    in slope.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：单元1（a1_1）的激活输出，聚合到输出*(a2_0)*以产生片段（2）。红色箭头表示斜率的变化。
- en: '![](../Images/2cc99c9616fd854e28c2c64dc2db9bdd.png)![](../Images/c2c7c57e436713ba9a76c8380b1c3680.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2cc99c9616fd854e28c2c64dc2db9bdd.png)![](../Images/c2c7c57e436713ba9a76c8380b1c3680.png)'
- en: 'Figure 8: The output of unit 2 (a1_2), which is aggregated to the output (a2_0)
    to produce the segment (3). The red arrow represents the change in slope.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：单元2（a1_2）的输出，聚合到输出（a2_0）以产生片段（3）。红色箭头表示斜率的变化。
- en: '![](../Images/cc95f6c47f81f48362d89b9f2456ade4.png)![](../Images/eb8a7102340e818e0dc2fc793b9e6c16.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc95f6c47f81f48362d89b9f2456ade4.png)![](../Images/eb8a7102340e818e0dc2fc793b9e6c16.png)'
- en: 'Figure 9: The output of unit 3 (a1_3), which is aggregated to the output (a2_0)
    to produce the segment (4). The red arrow represents the change in slope.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：单元 3（a1_3）的输出，它被聚合到输出（a2_0）中生成段（4）。红色箭头表示斜率的变化。
- en: Continuous curve function approximation
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连续曲线函数近似
- en: 'The next type of continuous nonlinear function that we will study is CC function.
    There is not a proper definition for this sub-category, but *an informal way to
    define CC functions is continuous nonlinear functions that are not piecewise linear.
    Several examples of CC functions are: quadratic function, exponential function,
    sinus function, etc.*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要研究的下一类连续非线性函数是 CC 函数。虽然这一子类别没有明确的定义，但*一种非正式的定义方式是 CC 函数是那些不是分段线性的连续非线性函数。一些
    CC 函数的例子包括：二次函数、指数函数、正弦函数等。*
- en: A CC function can be approximated by a series of infinitesimal linear pieces,
    which is called a piecewise linear approximation of the function. The greater
    the number of linear pieces and the smaller the size of each segment, the better
    the approximation is to the target function. Thus, the same network architecture
    as previously with a large enough number of hidden units can yield good approximation
    for a curve function.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 CC 函数可以通过一系列微小的线性片段来近似，这被称为该函数的分段线性近似。线性片段的数量越多，每个片段的大小越小，近似结果就越接近目标函数。因此，与之前相同的网络架构，如果具有足够数量的隐藏单元，也可以很好地近似曲线函数。
- en: However, in reality, the network is trained to fit a given dataset where the
    input-output mapping function is unknown. An architecture with too many neurons
    is prone to overfitting, high variance, and requires more time to train. Therefore,
    an appropriate number of hidden units should be large enough to properly fit the
    data and at the same time, small enough to avoid overfitting. Moreover, with a
    limited number of neurons, a good approximation with low loss has more transition
    points in restricted domain, rather than equidistant transition points in an uniform
    sampling way (as shown in *Fig.10*).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实际情况中，网络是为了拟合给定的数据集而训练的，其中输入输出映射函数是未知的。具有过多神经元的架构容易出现过拟合、高方差，并且需要更多的训练时间。因此，隐藏单元的适当数量应该足够大以正确拟合数据，同时又要足够小以避免过拟合。此外，在神经元数量有限的情况下，一个具有低损失的良好近似会在有限域内有更多的过渡点，而不是均匀采样方式下的等距过渡点（如*图
    10*所示）。
- en: '![](../Images/259fc3f5bef997908f75399fc78918ca.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/259fc3f5bef997908f75399fc78918ca.png)'
- en: 'Figure 10: Two piecewise linear approximations for a continuous curve function
    (in dashed line). The approximation 1 has more transition points in restricted
    domain and model the target function better than the approximation 2.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：两种分段线性近似连续曲线函数（以虚线表示）。近似 1 在有限域内有更多的过渡点，并且比近似 2 更好地拟合目标函数。
- en: Wrap up
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this post, we have studied how ReLU activation function allows multiple units
    to contribute to the resulting function without interfering, thus enables continuous
    nonlinear function approximation. In addition, we have discussed about the choice
    of network architecture and number of hidden units in order to obtain a good approximation
    result.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们研究了 ReLU 激活函数如何允许多个单元为结果函数做出贡献而不相互干扰，从而实现连续的非线性函数近似。此外，我们还讨论了选择网络架构和隐藏单元数量的问题，以获得良好的近似结果。
- en: '*I hope that this post is useful for your Machine Learning learning process!*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*希望这篇文章对你的机器学习学习过程有所帮助！*'
- en: '**Further questions to think about:**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**进一步思考的问题：**'
- en: How does the approximation ability change if the number of hidden layers with
    ReLU activation increases?
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果增加具有 ReLU 激活函数的隐藏层数量，近似能力会如何变化？
- en: How ReLU activations are used for a classification problem?
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ReLU 激活函数如何用于分类问题？
- en: '*Unless otherwise noted, all images are by the author'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，所有图片均由作者提供*'
