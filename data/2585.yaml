- en: 'ML Metamorphosis: Chaining ML Models for Optimized Results'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ML变形：通过串联ML模型实现优化结果
- en: 原文：[https://towardsdatascience.com/ml-metamorphosis-chaining-ml-models-for-optimized-results-d89d952627a9?source=collection_archive---------2-----------------------#2024-10-23](https://towardsdatascience.com/ml-metamorphosis-chaining-ml-models-for-optimized-results-d89d952627a9?source=collection_archive---------2-----------------------#2024-10-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/ml-metamorphosis-chaining-ml-models-for-optimized-results-d89d952627a9?source=collection_archive---------2-----------------------#2024-10-23](https://towardsdatascience.com/ml-metamorphosis-chaining-ml-models-for-optimized-results-d89d952627a9?source=collection_archive---------2-----------------------#2024-10-23)
- en: The universal principle of knowledge distillation, model compression, and rule
    extraction
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 知识蒸馏、模型压缩和规则提取的普遍原理
- en: '[](https://medium.com/@vadim.arzamasov?source=post_page---byline--d89d952627a9--------------------------------)[![Vadim
    Arzamasov](../Images/70ced2eafa6fc926052979875a0a4265.png)](https://medium.com/@vadim.arzamasov?source=post_page---byline--d89d952627a9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d89d952627a9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d89d952627a9--------------------------------)
    [Vadim Arzamasov](https://medium.com/@vadim.arzamasov?source=post_page---byline--d89d952627a9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@vadim.arzamasov?source=post_page---byline--d89d952627a9--------------------------------)[![Vadim
    Arzamasov](../Images/70ced2eafa6fc926052979875a0a4265.png)](https://medium.com/@vadim.arzamasov?source=post_page---byline--d89d952627a9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d89d952627a9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d89d952627a9--------------------------------)
    [Vadim Arzamasov](https://medium.com/@vadim.arzamasov?source=post_page---byline--d89d952627a9--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d89d952627a9--------------------------------)
    ·7 min read·Oct 23, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d89d952627a9--------------------------------)
    ·阅读时长 7 分钟·2024年10月23日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/5b5804cb68224d5d689f156155d59eb9.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b5804cb68224d5d689f156155d59eb9.png)'
- en: '**Figure 1**. This and other images were created by the author with the help
    of recraft.ai'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1**。此图及其他图像由作者在recraft.ai的帮助下创建'
- en: 'Machine learning (ML) model training typically follows a familiar pipeline:
    start with data collection, clean and prepare it, then move on to model fitting.
    But what if we could take this process further? Just as some insects undergo dramatic
    transformations before reaching maturity, ML models can evolve in a similar way
    (see Hinton et al. [1]) — what I will call the **ML metamorphosis**. This process
    involves chaining different models together, resulting in a final model that achieves
    significantly better quality than if it had been trained directly from the start.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）模型训练通常遵循一个熟悉的流程：首先收集数据，清理并准备数据，然后进行模型拟合。但如果我们能将这一过程进一步推进呢？就像一些昆虫在达到成熟之前会经历剧烈的变化一样，ML模型也可以以类似的方式进化（参见Hinton等人[1]）——我将其称为**ML变形**。这个过程涉及将不同的模型串联在一起，最终生成的模型比直接从头开始训练的模型质量要好得多。
- en: 'Here’s how it works:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 其工作原理如下：
- en: Start with some initial knowledge, *Data 1*.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从一些初步知识开始，*数据 1*。
- en: Train an ML model, *Model A* (say, a neural network), on this data.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这些数据上训练一个机器学习模型，*模型 A*（例如神经网络）。
- en: Generate new data, *Data 2*, using *Model A*.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用*模型 A*生成新的数据，*数据 2*。
- en: Finally, use Data 2 to fit your target model, *Model B*.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，使用数据 2 来拟合你的目标模型，*模型 B*。
- en: '![](../Images/2645f7d1e0777997a8daebcec3b49575.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2645f7d1e0777997a8daebcec3b49575.png)'
- en: '**Figure 2\.** An illustration of the ML metamorphosis'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2**。ML 变形的示意图'
- en: You may already be familiar with this concept from knowledge distillation, where
    a smaller neural network replaces a larger one. But ML metamorphosis goes beyond
    this, and neither the initial model (*Model A*) nor the final one (*Model B*)
    need be neural networks at all.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经熟悉这一概念，知识蒸馏就是用一个较小的神经网络替换较大的神经网络。但ML变形更进一步，初始模型（*模型 A*）和最终模型（*模型 B*）不必是神经网络。
- en: 'Example: ML metamorphosis on the MNIST Dataset'
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例：MNIST数据集上的ML变形
- en: '*Imagine you’re tasked with training a multi-class decision tree on the MNIST
    dataset of handwritten digit images, but only 1,000 images are labelled. You could
    train the tree directly on this limited data, but the accuracy would be capped
    at around 0.67\. Not great, right? Alternatively, you could use ML metamorphosis
    to improve your results.*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*想象一下，你的任务是使用MNIST手写数字图像数据集训练一个多类决策树，但只有1,000张图像有标签。你可以直接在这个有限的数据上训练决策树，但准确度大约只能达到0.67。这并不好，对吧？或者，你可以使用机器学习变换方法来提高结果。*'
- en: But before we dive into the solution, let’s take a quick look at the techniques
    and research behind this approach.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 但在深入探讨解决方案之前，我们先快速回顾一下支持这种方法的技术和研究。
- en: 1\. Knowledge distillation (2015)
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 知识蒸馏（2015）
- en: Even if you haven’t used knowledge distillation, you’ve probably seen it in
    action. For example, Meta suggests distilling its Llama 3.2 model to adapt it
    to specific tasks [2]. Or take DistilBERT — a distilled version of BERT [3]— or
    the DMD framework, which distills Stable Diffusion to speed up image generation
    by a factor of 30 [4].
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你没有使用知识蒸馏，你也可能见过它的应用。例如，Meta建议蒸馏其Llama 3.2模型，以便将其适配到特定任务[2]。或者看看DistilBERT——一个蒸馏版的BERT[3]，或者DMD框架，它通过蒸馏Stable
    Diffusion来加速图像生成速度，提升了30倍[4]。
- en: At its core, knowledge distillation transfers knowledge from a large, complex
    model (the *teacher*) to a smaller, more efficient model (the *student*). The
    process involves creating a *transfer set* that includes both the original training
    data and additional data (either original or synthesized) pseudo-labeled by the
    teacher model. The pseudo-labels are known as *soft labels* — derived from the
    probabilities predicted by the teacher across multiple classes. These soft labels
    provide richer information than *hard labels* (simple class indicators) because
    they reflect the teacher’s confidence and capture subtle similarities between
    classes. For instance, they might show that a particular “1” is more similar to
    a “7” than to a “5.”
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏的核心是将知识从一个大型、复杂的模型（*教师*）传递给一个较小、效率更高的模型（*学生*）。该过程包括创建一个*转移集*，该集包含原始训练数据和由教师模型伪标注的额外数据（无论是原始数据还是合成数据）。这些伪标签被称为*软标签*——它们源自教师模型在多个类别上的预测概率。这些软标签提供了比*硬标签*（简单的类别指示符）更丰富的信息，因为它们反映了教师的信心，并捕捉到类别之间的微妙相似性。例如，它们可能表明一个特定的“1”比“5”更像“7”。
- en: By training on this enriched transfer set, the student model can effectively
    mimic the teacher’s performance while being much lighter, faster, and easier to
    use.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在这个丰富的转移集上进行训练，学生模型可以有效地模仿教师模型的表现，同时更加轻便、快速且易于使用。
- en: The student model obtained in this way is more accurate than it would have been
    if it had been trained solely on the original training set.
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 以这种方式获得的学生模型比仅在原始训练集上训练得到的模型更准确。
- en: 2\. Model compression (2007)
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 模型压缩（2007）
- en: Model compression [5] is often seen as a precursor to knowledge distillation,
    but there are important differences. Unlike knowledge distillation, model compression
    doesn’t seem to use soft labels, despite some claims in the literature [1,6].
    I haven’t found any evidence that soft labels are part of the process. In fact,
    the method in the original paper doesn’t even rely on artificial neural networks
    (ANNs) as *Model A*. Instead, it uses an ensemble of models — such as SVMs, decision
    trees, random forests, and others.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 模型压缩[5]通常被视为知识蒸馏的前奏，但两者之间存在重要差异。与知识蒸馏不同，模型压缩似乎并未使用软标签，尽管文献中有一些相关说法[1,6]。我没有找到任何证据表明软标签是过程的一部分。事实上，原始论文中的方法甚至不依赖于人工神经网络（ANNs）作为*模型A*。相反，它使用了一组模型——如支持向量机（SVMs）、决策树、随机森林等。
- en: Model compression works by approximating the feature distribution *p(x)* to
    create a transfer set. This set is then labelled by *Model A*, which provides
    the conditional distribution *p(y|x)*. The key innovation in the original work
    is a technique called MUNGE to approximate *p(x)*. As with knowledge distillation,
    the goal is to train a smaller, more efficient *Model B* that retains the performance
    of the larger *Model A*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 模型压缩通过逼近特征分布*p(x)*来创建转移集。然后，这个集由*模型A*标注，提供条件分布*p(y|x)*。原始工作中的关键创新是一种名为MUNGE的技术，用于逼近*p(x)*。与知识蒸馏一样，目标是训练一个较小、更高效的*模型B*，并保留较大*模型A*的性能。
- en: As in knowledge distillation, the compressed model trained in this way can often
    outperform a similar model trained directly on the original data, thanks to the
    rich information embedded in the transfer set [5].
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 正如知识蒸馏中所示，这种训练方式得到的压缩模型，通常能够超过直接在原始数据上训练的类似模型，因为传递集中嵌入了丰富的信息[5]。
- en: Often, “model compression” is used more broadly to refer to any technique that
    reduces the size of *Model A* [7,8]. This includes methods like knowledge distillation
    but also techniques that don’t rely on a transfer set, such as pruning, quantization,
    or low-rank approximation for neural networks.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，“模型压缩”一词被更广泛地用来指代任何减少*模型 A*大小的技术[7,8]。这包括像知识蒸馏这样的技术，也包括不依赖传递集的技术，如修剪、量化或神经网络的低秩近似。
- en: 3\. Rule extraction (1995)
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. 规则提取（1995）
- en: 'When the problem isn’t computational complexity or memory, but the opacity
    of a model’s decision-making, pedagogical rule extraction offers a solution [9].
    In this approach, a simpler, more interpretable model (*Model B*) is trained to
    replicate the behavior of the opaque teacher model (*Model A*), with the goal
    of deriving a set of human-readable rules. The process typically starts by feeding
    unlabelled examples — often randomly generated — into *Model A*, which labels
    them to create a transfer set. This transfer set is then used to train the transparent
    student model. For example, in a classification task, the student model might
    be a decision tree that outputs rules such as: “If feature X1 is above threshold
    T1 and feature X2 is below threshold T2, then classify as positive”.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当问题不在于计算复杂度或内存，而在于模型决策过程的不透明性时，教育性规则提取提供了一种解决方案[9]。在这种方法中，训练一个更简单、更易解释的模型（*模型
    B*）来复制不透明教师模型（*模型 A*）的行为，目的是推导出一组人类可读的规则。这个过程通常从将未标记的示例（通常是随机生成的）输入到*模型 A*开始，*模型
    A*对这些示例进行标记，生成一个传递集。然后，使用这个传递集来训练透明的学生模型。例如，在分类任务中，学生模型可能是一个决策树，它输出如下规则：“如果特征
    X1 大于阈值 T1 且特征 X2 小于阈值 T2，则分类为正类”。
- en: The main goal of pedagogical rule extraction is to closely mimic the teacher
    model’s behavior, with *fidelity* — the accuracy of the student model relative
    to the teacher model — serving as the primary quality measure.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 教育性规则提取的主要目标是紧密模仿教师模型的行为，以*保真度*为衡量标准——即学生模型与教师模型之间的准确性，作为主要的质量衡量标准。
- en: Interestingly, research has shown that transparent models created through this
    method can sometimes reach higher accuracy than similar models trained directly
    on the original data used to build Model A [10,11].
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 有趣的是，研究表明，通过这种方法创建的透明模型，有时能够比直接在用于构建*模型 A*的原始数据上训练的类似模型达到更高的准确率[10,11]。
- en: Pedagogical rule extraction belongs to a broader family of techniques known
    as “global” model explanation methods, which also include decompositional and
    eclectic rule extraction. See [12] for more details.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 教育性规则提取属于被称为“全局”模型解释方法的更广泛技术家族，这其中还包括分解性和折衷规则提取。更多详情请参见[12]。
- en: 4\. Simulations as Model A
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4. 仿真作为模型 A
- en: '*Model A* doesn’t have to be an ML model — it could just as easily be a computer
    simulation of an economic or physical process, such as the simulation of airflow
    around an airplane wing. In this case, *Data 1* consists of the differential or
    difference equations that define the process. For any given input, the simulation
    makes predictions by solving these equations numerically. However, when these
    simulations become computationally expensive, a faster alternative is needed:
    a surrogate model (*Model B*), which can accelerate tasks like optimization [13].
    When the goal is to identify important regions in the input space, such as zones
    of system stability, an interpretable *Model B* is developed through a process
    known as scenario discovery [14]. To generate the transfer set (*Data 2*) for
    both surrogate modelling and scenario discovery, *Model A* is run on a diverse
    set of inputs.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*模型 A*不一定是一个机器学习模型——它也可以是一个经济或物理过程的计算机仿真，例如模拟飞机机翼周围的气流。在这种情况下，*数据 1*由定义该过程的微分方程或差分方程组成。对于任何给定的输入，仿真通过数值解这些方程来做出预测。然而，当这些仿真变得计算开销较大时，就需要一种更快速的替代方案：一个代理模型（*模型
    B*），它可以加速诸如优化之类的任务[13]。当目标是识别输入空间中的重要区域，例如系统稳定性的区域时，开发一个可解释的*模型 B*，这一过程称为情境发现[14]。为了生成用于代理建模和情境发现的传递集（*数据
    2*），*模型 A*会在一个多样化的输入集上运行。'
- en: Back to our MNIST example
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回到我们的 MNIST 示例
- en: In an insightful [article](/teaching-your-model-to-learn-from-itself-8b5ef13eb173)
    on TDS [15], [Niklas von Moers](https://medium.com/u/a3ecf86934da?source=post_page---user_mention--d89d952627a9--------------------------------)
    shows how semi-supervised learning can improve the performance of a convolutional
    neural network (CNN) on the same input data. This result fits into the first stage
    of the ML metamorphosis pipeline, where *Model A* is a trained CNN classifier.
    The transfer set, *Data 2*, then contains the originally labelled 1,000 training
    examples plus about 55,000 examples pseudo-labelled by *Model A* with high confidence
    predictions. I now train our target *Model B*, a decision tree classifier, on
    *Data 2* and achieve an accuracy of 0.86 — much higher than 0.67 when training
    on the labelled part of *Data 1* alone. This means that chaining the decision
    tree to the CNN solution reduces error rate of the decision tree from 0.33 to
    0.14\. Quite an improvement, wouldn’t you say?
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在TDS上的一篇深刻的[文章](/teaching-your-model-to-learn-from-itself-8b5ef13eb173)中，[Niklas
    von Moers](https://medium.com/u/a3ecf86934da?source=post_page---user_mention--d89d952627a9--------------------------------)展示了半监督学习如何提高卷积神经网络（CNN）在相同输入数据上的表现。这个结果适用于ML变换流水线的第一阶段，在该阶段，*Model
    A*是一个经过训练的CNN分类器。转移集*Data 2*包含原本标注的1,000个训练样本以及约55,000个由*Model A*高置信度预测的伪标注样本。接下来，我在*Data
    2*上训练我们的目标*Model B*，一个决策树分类器，并达到了0.86的准确率——远高于仅在*Data 1*的标注部分训练时的0.67。这意味着，将决策树与CNN解决方案链式连接，将决策树的错误率从0.33降低到了0.14。相当大的提升，是吧？
- en: For the full experimental code, check out the [GitHub](https://github.com/Arzik1987/medium/tree/main/metamorphosis)
    repository.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 查看完整的实验代码，请访问 [GitHub](https://github.com/Arzik1987/medium/tree/main/metamorphosis)
    仓库。
- en: '**Conclusion**'
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**结论**'
- en: In summary, ML metamorphosis isn’t always necessary — especially if accuracy
    is your only concern and there’s no need for interpretability, faster inference,
    or reduced storage requirements. But in other cases, chaining models may yield
    significantly better results than training the target model directly on the original
    data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，ML变换并非总是必要的——特别是当你的唯一关注点是准确性，并且不需要可解释性、更快的推理或减少存储需求时。但在其他情况下，链式模型可能会比直接在原始数据上训练目标模型产生显著更好的结果。
- en: '![](../Images/1b04e87426dec01924b73226bb1df2ea.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b04e87426dec01924b73226bb1df2ea.png)'
- en: '**Figure 2**: For easy reference, here’s the illustration again'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2**：为方便参考，图示再次呈现'
- en: 'For a classification task, the process involves:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类任务，过程包括：
- en: '*Data 1*: The original, fully or partially labeled data.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Data 1*：原始的，完全或部分标注的数据。'
- en: '*Model A*: A model trained on *Data 1*.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Model A*：在*Data 1*上训练的模型。'
- en: '*Data 2*: A transfer set that includes pseudo-labeled data.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Data 2*：包括伪标注数据的转移集。'
- en: '*Model B*: The final model, designed to meet additional requirements, such
    as interpretability or efficiency.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Model B*：最终模型，旨在满足额外要求，如可解释性或效率。'
- en: So why don’t we always use ML metamorphosis? The challenge often lies in finding
    the right transfer set, *Data 2* [9]. But that’s a topic for another story.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么我们不总是使用ML变换呢？挑战通常在于找到合适的转移集，*Data 2* [9]。但这是另一个话题。
- en: References
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Hinton, Geoffrey. “[Distilling the Knowledge in a Neural Network](https://arxiv.org/pdf/1503.02531).”
    *arXiv preprint arXiv:1503.02531* (2015).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Hinton, Geoffrey. “[蒸馏神经网络中的知识](https://arxiv.org/pdf/1503.02531)。” *arXiv预印本arXiv:1503.02531*
    (2015年)。'
- en: '[2] [Introducing Llama 3.2](https://llama.meta.com/?ref=engineering.fb.com)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [介绍 Llama 3.2](https://llama.meta.com/?ref=engineering.fb.com)'
- en: '[3] Sanh, Victor, et al. “[DistilBERT, a distilled version of BERT: Smaller,
    faster, cheaper and lighter](https://arxiv.org/abs/1910.01108). ” *arXiv preprint
    arXiv:1910.01108* (2019).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Sanh, Victor 等人. “[DistilBERT：BERT的蒸馏版：更小、更快、更便宜、更轻量](https://arxiv.org/abs/1910.01108)。”
    *arXiv预印本arXiv:1910.01108* (2019年)。'
- en: '[4] Yin, Tianwei, et al. “[One-step diffusion with distribution matching distillation](https://tianweiy.github.io/dmd/).”
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*.
    2024.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Yin, Tianwei 等人. “[一步扩散与分布匹配蒸馏](https://tianweiy.github.io/dmd/)。” *IEEE/CVF计算机视觉与模式识别会议论文集*。2024年。'
- en: '[5] Buciluǎ, Cristian, Rich Caruana, and Alexandru Niculescu-Mizil. “[Model
    compression.](https://dl.acm.org/doi/pdf/10.1145/1150402.1150464?casa_token=CY8nr3ZTpi0AAAAA%3A6T9MQ4MKzDCllOBuaurkddgR67bKLt88tc-TtEf0MfzNCdncFzr0Q1ZQZSha2GkBBYdysx78qMxdwA)”
    *Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery
    and data mining*. 2006.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Buciluǎ, Cristian, Rich Caruana, 和 Alexandru Niculescu-Mizil. “[模型压缩](https://dl.acm.org/doi/pdf/10.1145/1150402.1150464?casa_token=CY8nr3ZTpi0AAAAA%3A6T9MQ4MKzDCllOBuaurkddgR67bKLt88tc-TtEf0MfzNCdncFzr0Q1ZQZSha2GkBBYdysx78qMxdwA)”
    *第12届ACM SIGKDD国际会议《知识发现与数据挖掘》论文集*，2006年。'
- en: '[6] [Knowledge distillation](https://en.wikipedia.org/wiki/Knowledge_distillation),
    Wikipedia'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] [知识蒸馏](https://en.wikipedia.org/wiki/Knowledge_distillation)，维基百科'
- en: '[7] [An Overview of Model Compression Techniques for Deep Learning in Space](https://medium.com/gsi-technology/an-overview-of-model-compression-techniques-for-deep-learning-in-space-3fd8d4ce84e5),
    on Medium'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] [太空深度学习模型压缩技术概述](https://medium.com/gsi-technology/an-overview-of-model-compression-techniques-for-deep-learning-in-space-3fd8d4ce84e5)，发表于Medium'
- en: '[8] [Distilling BERT Using an Unlabeled Question-Answering Dataset](/distilling-bert-using-unlabeled-qa-dataset-4670085cc18),
    on Towards Data Science'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] [使用未标注的问答数据集蒸馏BERT](/distilling-bert-using-unlabeled-qa-dataset-4670085cc18)，发表于Towards
    Data Science'
- en: '[9] Arzamasov, Vadim, Benjamin Jochum, and Klemens Böhm. “[Pedagogical Rule
    Extraction to Learn Interpretable Models — an Empirical Study](https://arxiv.org/pdf/2112.13285).”
    *arXiv preprint arXiv:2112.13285* (2021).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Arzamasov, Vadim, Benjamin Jochum, 和 Klemens Böhm. “[教育规则提取以学习可解释模型 — 一项实证研究](https://arxiv.org/pdf/2112.13285).”
    *arXiv 预印本 arXiv:2112.13285* (2021).'
- en: '[10] Domingos, Pedro. “[Knowledge acquisition from examples via multiple models.](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=1a9a39da9d4fc937bc455705d508674a205620aa)”
    *MACHINE LEARNING-INTERNATIONAL WORKSHOP THEN CONFERENCE-*. MORGAN KAUFMANN PUBLISHERS,
    INC., 1997.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Domingos, Pedro. “[通过多模型从示例中获取知识](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=1a9a39da9d4fc937bc455705d508674a205620aa)”
    *MACHINE LEARNING-INTERNATIONAL WORKSHOP THEN CONFERENCE-*. MORGAN KAUFMANN PUBLISHERS,
    INC., 1997.'
- en: '[11] De Fortuny, Enric Junque, and David Martens. “[Active learning-based pedagogical
    rule extraction](https://ieeexplore.ieee.org/abstract/document/7018925?casa_token=SU8qtZ-ZkGEAAAAA%3A7L_-_Sj-eZ7x0_f4oYQgH4kynTftYbJW4ytvcEQgaq-r4VWyOdmh0lD1jXYN1otMmKzt-dIRcA).”
    *IEEE transactions on neural networks and learning systems* 26.11 (2015): 2664–2677.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] De Fortuny, Enric Junque, 和 David Martens. “[基于主动学习的教学规则提取](https://ieeexplore.ieee.org/abstract/document/7018925?casa_token=SU8qtZ-ZkGEAAAAA%3A7L_-_Sj-eZ7x0_f4oYQgH4kynTftYbJW4ytvcEQgaq-r4VWyOdmh0lD1jXYN1otMmKzt-dIRcA).”
    *IEEE 神经网络与学习系统交易* 26.11 (2015): 2664–2677.'
- en: '[12] Guidotti, Riccardo, et al. “[A survey of methods for explaining black
    box models](https://dl.acm.org/doi/abs/10.1145/3236009).” *ACM computing surveys
    (CSUR)* 51.5 (2018): 1–42.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Guidotti, Riccardo, 等. “[解释黑盒模型的方法调查](https://dl.acm.org/doi/abs/10.1145/3236009).”
    *ACM 计算机调查 (CSUR)* 51.5 (2018): 1–42.'
- en: '[13] [Surrogate model](https://en.wikipedia.org/wiki/Surrogate_model), Wikipedia'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] [代理模型](https://en.wikipedia.org/wiki/Surrogate_model)，维基百科'
- en: '[14] [Scenario discovery in Python](https://waterprogramming.wordpress.com/2015/08/05/scenario-discovery-in-python/),
    blog post on [Water Programming](https://waterprogramming.wordpress.com/)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] [Python中的情景发现](https://waterprogramming.wordpress.com/2015/08/05/scenario-discovery-in-python/)，[Water
    Programming](https://waterprogramming.wordpress.com/)上的博客文章'
- en: '[15] [Teaching Your Model to Learn from Itself](/teaching-your-model-to-learn-from-itself-8b5ef13eb173),
    on Towards Data Science'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] [让模型从自身学习](/teaching-your-model-to-learn-from-itself-8b5ef13eb173)，发表于Towards
    Data Science'
