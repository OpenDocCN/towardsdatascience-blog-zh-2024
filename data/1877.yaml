- en: 8 Practical Prompt Engineering Tips for Better LLM Apps
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8个实用的提示工程技巧，帮助提升LLM应用
- en: 原文：[https://towardsdatascience.com/8-practical-prompt-engineering-tips-for-better-llm-apps-430eef9b0950?source=collection_archive---------4-----------------------#2024-08-01](https://towardsdatascience.com/8-practical-prompt-engineering-tips-for-better-llm-apps-430eef9b0950?source=collection_archive---------4-----------------------#2024-08-01)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/8-practical-prompt-engineering-tips-for-better-llm-apps-430eef9b0950?source=collection_archive---------4-----------------------#2024-08-01](https://towardsdatascience.com/8-practical-prompt-engineering-tips-for-better-llm-apps-430eef9b0950?source=collection_archive---------4-----------------------#2024-08-01)
- en: '[](https://medium.com/@almogbaku?source=post_page---byline--430eef9b0950--------------------------------)[![Almog
    Baku](../Images/3ac36986f6ca0ba56c8edced6ec7dd07.png)](https://medium.com/@almogbaku?source=post_page---byline--430eef9b0950--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--430eef9b0950--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--430eef9b0950--------------------------------)
    [Almog Baku](https://medium.com/@almogbaku?source=post_page---byline--430eef9b0950--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@almogbaku?source=post_page---byline--430eef9b0950--------------------------------)[![Almog
    Baku](../Images/3ac36986f6ca0ba56c8edced6ec7dd07.png)](https://medium.com/@almogbaku?source=post_page---byline--430eef9b0950--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--430eef9b0950--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--430eef9b0950--------------------------------)
    [Almog Baku](https://medium.com/@almogbaku?source=post_page---byline--430eef9b0950--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--430eef9b0950--------------------------------)
    ·9 min read·Aug 1, 2024
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--430eef9b0950--------------------------------)
    ·9分钟阅读·2024年8月1日
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: ⁤Prompt engineering is undoubtedly the most critical skill in developing an
    LLM-native application, as crafting the right prompts can significantly impact
    your application’s performance and reliability.⁤
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程无疑是开发LLM本地应用程序中最关键的技能，因为制定正确的提示可以显著影响应用程序的性能和可靠性。
- en: Over the past two years, I’ve been helping organizations build and deploy dozens
    of LLM applications for real-world use cases. ⁤⁤This experience gave me valuable
    insights into effective prompting techniques. ⁤
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的两年里，我一直在帮助组织构建和部署数十个用于实际案例的LLM应用程序。这段经历让我对有效的提示技巧有了宝贵的洞察。
- en: This article, informed by the [LLM Triangle Principles](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e),
    presents **eight practical tips** for prompt engineering to level up your LLM
    prompting game.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本文基于[LLM三角原理](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e)介绍了**八个实用的提示**，帮助提升你的LLM提示工程技能。
- en: “LLM-Native apps are 10% sophisticated model, and 90% experimenting data-driven
    engineering work.”
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “LLM本地应用程序是10%的复杂模型和90%的实验性数据驱动工程工作。”
- en: '![](../Images/605d5e3a9c4a90b2659fc5bd57d9585e.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/605d5e3a9c4a90b2659fc5bd57d9585e.png)'
- en: (Generated with Midjourney)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: （由Midjourney生成）
- en: Throughout this article, we'll use the “Landing Page Generator” example to demonstrate
    how each tip can be applied to enhance a real-world LLM application.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将使用“登录页面生成器”示例来演示如何应用每个技巧，以增强实际的LLM应用程序。
- en: You can also check out [the full script of Landing Page Generator example](https://gist.github.com/AlmogBaku/5de026a355f9ef8984fa6a5bebd8f51f),
    for a complete lookout.
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你也可以查看[登录页面生成器示例的完整脚本](https://gist.github.com/AlmogBaku/5de026a355f9ef8984fa6a5bebd8f51f)，以便全面了解。
- en: '**Note:** This is a simplified, non-production example created to illustrate
    these tips. The code and prompts are intentionally basic to highlight the concepts
    discussed.'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** 这是一个简化的非生产示例，旨在说明这些技巧。代码和提示故意简化，以突出讨论的概念。'
- en: 1\. Define Clear Cognitive Process Boundaries
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1. 定义明确的认知过程边界
- en: 'Start by defining the objective for each agent or prompt. Stick to **one cognitive
    process type per agent**, such as: conceptualizing a landing page, selecting components,
    or generating content for specific sections.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 从定义每个代理或提示的目标开始。坚持**每个代理一个认知过程类型**，例如：构思一个登录页面、选择组件或为特定部分生成内容。
- en: Having clear boundaries maintains focus and clarity in your LLM interactions,
    aligning with the [*Engineering Techniques*](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e#3221)
    apex of the [LLM Triangle Principle](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e#3221).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有明确的边界有助于保持LLM交互中的专注和清晰，这与[LLM三角原则](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e#3221)的[*工程技术*](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e#3221)顶端保持一致。
- en: '*“*Each step in our flow is a standalone process that must occur to achieve
    our task.*”*'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“*我们流程中的每个步骤都是一个独立的过程，必须完成才能实现我们的任务。”* '
- en: 'For example, avoid combining different cognitive processes in the same prompt,
    which might yield suboptimal results. Instead, break these into separate, focused
    agents:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，避免在同一提示中组合不同的认知过程，这可能会产生不理想的结果。相反，可以将这些过程分解成独立的、专注的代理：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: By defining *clear boundaries* for each agent, we can ensure that each step
    in our workflow is tailored to a specific mental task. This will **improve the
    quality of outputs** and make it **easier to debug** and refine.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通过为每个代理定义*明确的边界*，我们可以确保工作流中的每个步骤都专门针对特定的心理任务。这将**提高输出的质量**，并使**调试**和精炼变得**更容易**。
- en: 2\. Specify Input/Output Clearly
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2. 明确指定输入/输出
- en: Define **clear input and output** structures to reflect the objectives and create
    explicit data models. This practice touches on the [LLM Triangle Principles](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e)'
    [*Engineering Techniques*](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e#3221)
    and [*Contextual Data*](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e#7b49)
    apexes.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 定义**明确的输入和输出**结构，以反映目标并创建明确的数据模型。这个实践涉及到[LLM三角原则](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e)的[*工程技术*](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e#3221)和[*上下文数据*](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e#7b49)的顶端。
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: These [Pydantic](https://pydantic.dev/) models define the structure of our **input
    and output data** and define clear boundaries and expectations for the agent.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这些[Pydantic](https://pydantic.dev/)模型定义了我们**输入和输出数据**的结构，并为代理定义了明确的边界和期望。
- en: 3\. Implement Guardrails
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3. 实施防护措施
- en: Place validations to ensure the quality and moderation of the LLM outputs. [Pydantic](https://pydantic.dev/)
    is excellent for implementing these guardrails, and we can utilize its native
    features for that.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 放置验证以确保LLM输出的质量和适度。[Pydantic](https://pydantic.dev/)非常适合实现这些防护措施，我们可以利用其原生功能来实现。
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In this example, ensuring the quality of our application by defining two types
    of validators:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，通过定义两种类型的验证器来确保我们应用程序的质量：
- en: '**Using Pydanitc’s** `**Field**` to define simple validations, such as a minimum
    of 2 tone/style attributes, or a minimum of 50 characters in the narrative'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用Pydantic的** `**Field**` 来定义简单的验证，例如至少包含2个语调/风格属性，或叙述至少包含50个字符'
- en: '**Using a custom** `**field_validator**`that ensures the generated narrative
    is complying with our content moderation policy (using AI)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用自定义的** `**field_validator**` 来确保生成的叙述符合我们的内容审查政策（使用AI）。'
- en: 4\. Align with Human Cognitive Processes
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4. 与人类认知过程对齐
- en: Structure your LLM workflow to mimic human cognitive processes by breaking down
    complex tasks into smaller steps that follow a logical sequence. To do that, follow
    the *SOP (Standard Operating Procedure)* guiding principle of the [LLM Triangle
    Principles](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e#be9a).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将复杂的任务分解为遵循逻辑顺序的小步骤，来构建模拟人类认知过程的LLM工作流。为此，遵循[LLM三角原则](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e#be9a)中的*SOP（标准操作程序）*指导原则。
- en: '*“Without an SOP, even the most powerful LLM will fail to deliver consistently
    high-quality results.”*'
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“没有标准操作程序（SOP），即使是最强大的LLM也无法 consistently 交付一致的高质量结果。”*'
- en: 4.1 Capture hidden implicit cognition jumps
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 捕捉隐藏的隐性认知跳跃
- en: In our example, we expect the model to return `LandingPageConcept` as a result.
    By asking the model to output certain fields, we guide the LLM similar to how
    a human marketer or designer might approach creating a landing page concept.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们期望模型返回`LandingPageConcept`作为结果。通过要求模型输出某些字段，我们指导LLM，就像人类市场营销人员或设计师在创建着陆页概念时可能采取的方式一样。
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `LandingPageConcept` structure encourages the LLM to follow a human-like
    reasoning process, mirroring the *subtle mental leaps* ([implicit cognition "jumps"](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e#f8c9))
    that an expert would make instinctively, just as we modeled in our SOP.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`LandingPageConcept` 结构鼓励 LLM 跟随类似人类的推理过程，模仿专家在直觉上所做的 *微妙心理跃迁*（[隐性认知“跳跃”](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e#f8c9)），就像我们在标准操作程序（SOP）中所建模的那样。'
- en: 4.2 Breaking complex processes into multiple steps/agents
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 将复杂的过程分解为多个步骤/代理
- en: 'For complex tasks, break the process down into various steps, each handled
    by a separate LLM call or *"agent"*:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于复杂任务，将过程分解为多个步骤，每个步骤由单独的 LLM 调用或 *"代理"* 处理：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/7faf32bd345890ba949330bc1e8ad532.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7faf32bd345890ba949330bc1e8ad532.png)'
- en: Illustration of the multi-agent process code. (Image by author)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 多代理过程代码的示意图。（作者提供的图片）
- en: This multi-agent approach aligns with how humans tackle complex problems — by
    breaking them into smaller parts.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这种多代理方法与人类解决复杂问题的方式一致——通过将问题分解成更小的部分。
- en: 5\. Leverage Structured Data (YAML)
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5. 利用结构化数据（YAML）
- en: '[YAML](https://en.wikipedia.org/wiki/YAML) is a *popular* human-friendly data
    serialization format. It’s designed to be easily readable by humans while still
    being easy for machines to parse — which makes it classic for LLM usage.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[YAML](https://en.wikipedia.org/wiki/YAML) 是一种 *流行的* 人类友好的数据序列化格式。它设计上既便于人类阅读，又易于机器解析——这使得它成为
    LLM 使用中的经典选择。'
- en: I found YAML is particularly effective for LLM interactions and yields much
    better results across different models. It focuses the token processing on valuable
    content rather than syntax.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现 YAML 在 LLM 交互中特别有效，并且在不同模型之间能产生更好的结果。它将令牌处理集中在有价值的内容上，而非语法。
- en: YAML is also much more portable across different LLM providers and allows you
    to maintain a structured output format.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: YAML 在不同的 LLM 提供者之间具有更强的可移植性，允许你保持结构化的输出格式。
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Notice how we're using few-shot examples to *"show, don't tell"* the expected
    YAML format. This approach is more effective than explicit instructions in prompt
    for the output structure.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意我们如何使用少样本示例来 *“通过展示，而不是告诉”* 预期的 YAML 格式。与提示中明确的输出结构指令相比，这种方法更为有效。
- en: 6\. Craft Your Contextual Data
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6. 精心设计你的上下文数据
- en: Carefully consider how to model and present data to the LLM. This tip is central
    to the [*Contextual Data*](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e#7b49)
    apex of the [LLM Triangle Principles](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e#7b49).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细考虑如何将数据建模并呈现给 LLM。这一提示对于 [*上下文数据*](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e#7b49)
    是 [LLM 三角原理](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e#7b49)
    顶端的关键。
- en: '*“Even the most powerful model requires relevant and well-structured contextual
    data to shine.”*'
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“即使是最强大的模型，也需要相关且结构化的上下文数据才能发挥作用。”*'
- en: Don’t throw away all the data you have on the model. Instead, inform the model
    with the pieces of information that are relevant to the objective you defined.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 不要丢弃你所有关于模型的数据，而是将与你定义的目标相关的信息提供给模型。
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In this example, we're using [Jinja](https://jinja.palletsprojects.com/en/3.1.x/)
    templates to dynamically compose our prompts. This creates focused and relevant
    contexts for each LLM interaction elegantly.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用 [Jinja](https://jinja.palletsprojects.com/en/3.1.x/) 模板动态生成提示。这为每次
    LLM 交互优雅地创建了聚焦且相关的上下文。
- en: “Data fuels the engine of LLM-native applications. A strategic design of contextual
    data unlocks their true potential.”
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “数据为 LLM 原生应用提供动力。战略性地设计上下文数据能释放它们的真正潜力。”
- en: 6.1 Harness the power of few-shot learning
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6.1 利用少样本学习的力量
- en: Few-shot learning is a must-have technique in prompt engineering. Providing
    the LLM with relevant examples significantly improves its understanding of the
    task.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本学习是提示工程中必不可少的技术。向 LLM 提供相关的示例可以显著提高其对任务的理解。
- en: Notice that in both approaches we discuss below, we **reuse our Pydantic models
    for the few-shots** — this trick ensures consistency between the examples and
    our actual task! Unfortunately, I learned it the hard way.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在我们下面讨论的两种方法中，我们 **重用我们的 Pydantic 模型进行少样本学习**——这一技巧确保了示例和实际任务之间的一致性！不幸的是，我是通过艰难的方式学到这一点的。
- en: 6.1.1 Examples Few-Shot Learning
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1.1 示例 少样本学习
- en: 'Take a look at the `few_shots` dictionary in section 5\. In this approach:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 看看第 5 节中的 `few_shots` 字典。在这种方法中：
- en: Examples are added to the `messages` list as separate user and assistant messages,
    followed by the actual user input.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 示例作为单独的用户和助手消息添加到`消息`列表中，随后是实际的用户输入。
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: By placing the examples as `messages`, we align with the training methodology
    of instruction models. It allows the model to see multiple “example interactions”
    before processing the user input — helping it understand the expected input-output
    pattern.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将示例作为`消息`放置，我们与指令模型的训练方法保持一致。这使得模型在处理用户输入之前，能看到多个“示例交互”——帮助它理解预期的输入输出模式。
- en: As your application grows, you can add more few-shots to cover more use-cases.
    For even more advanced applications, consider implementing [dynamic few-shot](https://arxiv.org/abs/1804.09458)
    selection, where the most relevant examples are chosen based on the current input.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 随着应用程序的增长，你可以添加更多的少量示例（few-shots）来涵盖更多的使用案例。对于更高级的应用，考虑实现[动态少量示例](https://arxiv.org/abs/1804.09458)选择，根据当前输入选择最相关的示例。
- en: 6.1.2 Task-Specific Few-Shot Learning
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1.2 任务特定的少量示例学习
- en: 'This method uses examples directly related to the current task *within the*
    ***prompt*** *itself.* For instance, this prompt template is used for generating
    additional unique selling points:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法使用与当前任务直接相关的示例*在* ***提示*** *中*。例如，这个提示模板用于生成额外的独特卖点：
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This provides targeted guidance for specific content generation tasks by including
    the examples **directly in the prompt** rather than as separate messages.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这通过直接将示例包含在提示中，而不是作为单独的消息，提供了针对特定内容生成任务的有针对性的指导。
- en: 7\. KISS — Keep It Simple, Stupid
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7\. KISS — 保持简单，傻瓜
- en: While fancy prompt engineering techniques like “Tree of Thoughts” or “Graph
    of Thoughts” are intriguing, especially for research, I found them quite impractical
    and often overkill for production. For real applications, focus on designing a
    proper LLM architecture(aka workflow engineering).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管像“思维树”（Tree of Thoughts）或“思维图”（Graph of Thoughts）这样的精妙提示工程技术很有趣，特别是在研究中，但我发现它们在实际生产中相当不切实际，并且往往过于复杂。对于真实的应用程序，重点应该放在设计合适的LLM架构（即工作流工程）上。
- en: 'This extends to the use of agents in your LLM applications. It''s crucial to
    understand the distinction between standard agents and autonomous agents:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这同样适用于在你的LLM应用中使用代理。理解标准代理与自主代理之间的区别至关重要：
- en: '**Agents:** “Take me from A → B by doing XYZ.”'
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**代理：** “通过做XYZ把我从A带到B。”'
- en: ''
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Autonomous Agents:**“Take me from A → B by doing something, I don’t care
    how.”'
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**自主代理：** “通过做某件事把我从A带到B，我不在乎怎么做。”'
- en: While autonomous agents offer flexibility and quicker development, they can
    also introduce unpredictability and debugging challenges. Use autonomous agents
    carefully — only when the benefits *clearly outweigh* the potential loss of control
    and increased complexity.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然自主代理提供了灵活性和更快的开发速度，但它们也可能带来不可预测性和调试挑战。谨慎使用自主代理——只有在其优点*明显超过*潜在的控制损失和复杂性增加时才使用。
- en: '![](../Images/cd8b67d63da85cbb333002cf95f30b70.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cd8b67d63da85cbb333002cf95f30b70.png)'
- en: (Generated with Midjourney)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: （由Midjourney生成）
- en: 8\. Iterate, Iterate, Iterate!
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8\. 迭代，迭代，再迭代！
- en: 'Continuous experimentation is vital to improving your LLM-native applications.
    Don''t be intimidated by the idea of experiments — they can be as small as tweaking
    a prompt. As outlined in ["Building LLM Apps: A Clear Step-by-Step Guide,"](/building-llm-apps-a-clear-step-by-step-guide-1fe1e6ef60fd)
    it''s crucial to *establish a baseline* andtrack improvements against it.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 持续的实验对于改进你的LLM原生应用至关重要。不要对实验的想法感到害怕——它们可以像调整一个提示那么小。如在["构建LLM应用：清晰的逐步指南"](/building-llm-apps-a-clear-step-by-step-guide-1fe1e6ef60fd)中所述，*建立基准*并根据它跟踪改进是至关重要的。
- en: Like everything else in “AI,” LLM-native apps require a **research and experimentation
    *mindset*.**
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 像其他所有“AI”领域的技术一样，LLM原生应用需要**研究和实验的*思维方式*。**
- en: Another great trick is to try your prompts on a weaker model than the one you
    aim to use in production(such as open-source 8B models) — an “okay” performing
    prompt on a smaller model will perform much better on a larger model.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个好技巧是尝试在比你计划在生产中使用的模型性能更弱的模型上测试你的提示（例如，开源的8B模型）——在较小模型上表现“还行”的提示在更大模型上会表现得更好。
- en: Conclusion
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: These eight tips provide a solid foundation for effective prompt engineering
    in LLM-native applications. By applying these tips in your prompts, you'll be
    able to create more reliable, efficient, and scalable LLM-native applications.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这八个技巧为有效的提示工程提供了坚实的基础，适用于LLM原生应用。通过在提示中应用这些技巧，你将能够创建更可靠、高效和可扩展的LLM原生应用。
- en: Remember, the goal isn't to create the most complex system but to build something
    that works in the real world. Keep experimenting, learning, and building — the
    possibilities are endless.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，目标不是创建最复杂的系统，而是构建一个在现实世界中有效的系统。继续实验、学习和构建——可能性是无限的。
- en: If you find this article helpful, please give it a few **claps** 👏 on Medium
    and **share** it with your fellow AI enthusiasts. Your support means the world
    to me! 🌍
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你觉得这篇文章对你有帮助，请在Medium上给它一些**掌声**👏并**分享**给你身边的AI爱好者。你的支持对我意义重大！🌍
- en: Let's keep the conversation going — feel free to reach out via [email](mailto:almog.baku@gmail.com)
    or [connect on LinkedIn](https://www.linkedin.com/in/almogbaku/) 🤝
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续保持对话——随时通过[电子邮件](mailto:almog.baku@gmail.com)或[在LinkedIn上联系](https://www.linkedin.com/in/almogbaku/)
    🤝
- en: Special thanks to [Liron Izhaki Allerhand](https://medium.com/u/251cd1007ce8?source=post_page---user_mention--430eef9b0950--------------------------------)
    and [Yam Peleg](https://medium.com/u/e0607cd7607d?source=post_page---user_mention--430eef9b0950--------------------------------);
    this article is based on insights from our conversations.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 特别感谢[Liron Izhaki Allerhand](https://medium.com/u/251cd1007ce8?source=post_page---user_mention--430eef9b0950--------------------------------)和[Yam
    Peleg](https://medium.com/u/e0607cd7607d?source=post_page---user_mention--430eef9b0950--------------------------------)；这篇文章基于我们之间的对话和见解。
