- en: Diffusion Transformer Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩散 Transformer 解析
- en: 原文：[https://towardsdatascience.com/diffusion-transformer-explained-e603c4770f7e?source=collection_archive---------2-----------------------#2024-02-28](https://towardsdatascience.com/diffusion-transformer-explained-e603c4770f7e?source=collection_archive---------2-----------------------#2024-02-28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/diffusion-transformer-explained-e603c4770f7e?source=collection_archive---------2-----------------------#2024-02-28](https://towardsdatascience.com/diffusion-transformer-explained-e603c4770f7e?source=collection_archive---------2-----------------------#2024-02-28)
- en: Exploring the architecture that brought transformers into image generation
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索将 Transformer 引入图像生成领域的架构
- en: '[](https://mnslarcher.medium.com/?source=post_page---byline--e603c4770f7e--------------------------------)[![Mario
    Larcher](../Images/57a031fe2a1931c9cdd63b9f35f3d136.png)](https://mnslarcher.medium.com/?source=post_page---byline--e603c4770f7e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e603c4770f7e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e603c4770f7e--------------------------------)
    [Mario Larcher](https://mnslarcher.medium.com/?source=post_page---byline--e603c4770f7e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mnslarcher.medium.com/?source=post_page---byline--e603c4770f7e--------------------------------)[![马里奥·拉尔切尔](../Images/57a031fe2a1931c9cdd63b9f35f3d136.png)](https://mnslarcher.medium.com/?source=post_page---byline--e603c4770f7e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e603c4770f7e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e603c4770f7e--------------------------------)
    [马里奥·拉尔切尔](https://mnslarcher.medium.com/?source=post_page---byline--e603c4770f7e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e603c4770f7e--------------------------------)
    ·12 min read·Feb 28, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e603c4770f7e--------------------------------)
    ·12 分钟阅读·2024年2月28日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d6f5ca3c1cb0d9569dfd8b8ce4a076be.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6f5ca3c1cb0d9569dfd8b8ce4a076be.png)'
- en: Image generated with DALL·E.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 DALL·E 生成的图像。
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: After shaking up NLP and moving into computer vision with the Vision Transformer
    (ViT) and its successors, transformers are now entering the field of image generation.
    They are gradually becoming an alternative to the U-Net, the convolutional architecture
    upon which all the early diffusion models were built. This article looks into
    the **Diffusion Transformer** (**DiT**), introduced by William Peebles and Saining
    Xie in their paper “**Scalable Diffusion Models with Transformers**.”
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在震撼了 NLP 并通过视觉 Transformer（ViT）及其后续模型进入计算机视觉领域后，Transformer 现在正在进入图像生成领域。它们逐渐成为
    U-Net 的替代方案，而 U-Net 是所有早期扩散模型的基础卷积架构。本文将探讨 **扩散 Transformer**（**DiT**），由 William
    Peebles 和 Saining Xie 在他们的论文“**使用 Transformer 的可扩展扩散模型**”中介绍。
- en: '[](https://arxiv.org/abs/2212.09748?source=post_page-----e603c4770f7e--------------------------------)
    [## Scalable Diffusion Models with Transformers'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://arxiv.org/abs/2212.09748?source=post_page-----e603c4770f7e--------------------------------)
    [## 使用 Transformer 的可扩展扩散模型'
- en: We explore a new class of diffusion models based on the transformer architecture.
    We train latent diffusion models of…
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们探索了一类基于 Transformer 架构的新型扩散模型。我们训练了潜在扩散模型的…
- en: arxiv.org](https://arxiv.org/abs/2212.09748?source=post_page-----e603c4770f7e--------------------------------)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: arxiv.org](https://arxiv.org/abs/2212.09748?source=post_page-----e603c4770f7e--------------------------------)
- en: DiT has influenced the development of other transformer-based diffusion models
    like [PIXART-α](https://pixart-alpha.github.io/), [Sora](https://openai.com/sora)
    (OpenAI’s astonishing text-to-video model), and, as I write this article, [Stable
    Diffusion 3](https://stability.ai/news/stable-diffusion-3). Let’s start exploring
    this emerging class of architectures that are contributing to the evolution of
    diffusion models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: DiT 影响了其他基于 Transformer 的扩散模型的发展，例如 [PIXART-α](https://pixart-alpha.github.io/)、[Sora](https://openai.com/sora)（OpenAI
    惊人的文本到视频模型），以及在我写这篇文章时的 [Stable Diffusion 3](https://stability.ai/news/stable-diffusion-3)。让我们开始探索这些新兴的架构，它们正在推动扩散模型的进化。
- en: Preliminaries
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: Given that this is an advanced topic, I’ll have to assume a certain familiarity
    with…
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这是一个高级话题，我必须假设读者对…
