- en: Kolmogorov-Arnold Networks (KANs) for Time Series Forecasting
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kolmogorov-Arnold Networks（KAN）用于时间序列预测
- en: 原文：[https://towardsdatascience.com/kolmogorov-arnold-networks-kans-for-time-series-forecasting-9d49318c3172?source=collection_archive---------1-----------------------#2024-05-14](https://towardsdatascience.com/kolmogorov-arnold-networks-kans-for-time-series-forecasting-9d49318c3172?source=collection_archive---------1-----------------------#2024-05-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/kolmogorov-arnold-networks-kans-for-time-series-forecasting-9d49318c3172?source=collection_archive---------1-----------------------#2024-05-14](https://towardsdatascience.com/kolmogorov-arnold-networks-kans-for-time-series-forecasting-9d49318c3172?source=collection_archive---------1-----------------------#2024-05-14)
- en: Discover the Kolmogorov-Arnold Networks (KANs) and apply them for time series
    forecasting using Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解Kolmogorov-Arnold Networks（KAN），并使用Python将其应用于时间序列预测
- en: '[](https://medium.com/@marcopeixeiro?source=post_page---byline--9d49318c3172--------------------------------)[![Marco
    Peixeiro](../Images/7cf0a81d87281d35ff47f51e3026a3e9.png)](https://medium.com/@marcopeixeiro?source=post_page---byline--9d49318c3172--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9d49318c3172--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9d49318c3172--------------------------------)
    [Marco Peixeiro](https://medium.com/@marcopeixeiro?source=post_page---byline--9d49318c3172--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@marcopeixeiro?source=post_page---byline--9d49318c3172--------------------------------)[![Marco
    Peixeiro](../Images/7cf0a81d87281d35ff47f51e3026a3e9.png)](https://medium.com/@marcopeixeiro?source=post_page---byline--9d49318c3172--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9d49318c3172--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9d49318c3172--------------------------------)
    [Marco Peixeiro](https://medium.com/@marcopeixeiro?source=post_page---byline--9d49318c3172--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9d49318c3172--------------------------------)
    ·11 min read·May 14, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9d49318c3172--------------------------------)
    ·阅读时长11分钟·2024年5月14日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/bb3c6be08cbcefbf664344f9958747ee.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bb3c6be08cbcefbf664344f9958747ee.png)'
- en: Photo by [Eduardo Bergen](https://unsplash.com/@eduardb?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Eduardo Bergen](https://unsplash.com/@eduardb?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: The multilayer perceptron (MLP) is one of the foundational structures of deep
    learning models. It is also the building block of many state-of-the-art forecasting
    models, like [N-BEATS](/the-easiest-way-to-forecast-time-series-using-n-beats-d778fcc2ba60),
    [NHiTS](/all-about-n-hits-the-latest-breakthrough-in-time-series-forecasting-a8ddcb27b0d5)
    and [TSMixer](/tsmixer-the-latest-forecasting-model-by-google-2fd1e29a8ccb).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知器（MLP）是深度学习模型的基础结构之一。它也是许多先进预测模型的构建模块，如[N-BEATS](/the-easiest-way-to-forecast-time-series-using-n-beats-d778fcc2ba60)、[NHiTS](/all-about-n-hits-the-latest-breakthrough-in-time-series-forecasting-a8ddcb27b0d5)和[TSMixer](/tsmixer-the-latest-forecasting-model-by-google-2fd1e29a8ccb)。
- en: 'On April 30, 2024, the paper [KAN: Kolmogorov-Arnold Network](https://arxiv.org/abs/2404.19756)
    was published, and it has attracted the attention of many practitioners in the
    field of deep learning. There, the authors propose an alternative to the MLP:
    the **K**olmogorov-**A**rnold **N**etwork or **KAN**.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '2024年4月30日，论文[KAN: Kolmogorov-Arnold Network](https://arxiv.org/abs/2404.19756)发表，吸引了许多深度学习领域从业者的关注。在这篇论文中，作者提出了一种替代MLP的模型：**K**olmogorov-**A**rnold
    **N**etwork，简称**KAN**。'
- en: Instead of using weights and fixed activation functions, the KAN uses learnable
    functions that are parametrized as splines. The researchers suggest the KAN can
    thus be more accurate with less trainable parameters than MLPs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: KAN不是使用权重和固定的激活函数，而是使用可学习的函数，这些函数通过样条进行参数化。研究人员认为，KAN可以在参数较少的情况下比MLP更加精确。
- en: In this article, we first explore splines, as they help us understand the architecture
    and key elements of KAN. Then, we make a deep dive inside the inner workings of
    KAN. Finally, we apply KAN to time series forecasting, and evaluate its performance
    against the standard MLP and the N-BEATS model.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们首先探索了样条（splines），因为它们帮助我们理解KAN的架构和关键元素。然后，我们深入研究KAN的内部工作机制。最后，我们将KAN应用于时间序列预测，并与标准的MLP和N-BEATS模型进行性能评估。
- en: For more details on KAN, make sure to read the [original paper](https://arxiv.org/abs/2404.19756).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于KAN的细节，请务必阅读[原始论文](https://arxiv.org/abs/2404.19756)。
