- en: Experimenting with MLFlow and Microsoft Fabric
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸ MLFlow å’Œ Microsoft Fabric çš„å®éªŒ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/experimenting-with-mlflow-and-microsoft-fabric-68f43043ff34?source=collection_archive---------7-----------------------#2024-04-22](https://towardsdatascience.com/experimenting-with-mlflow-and-microsoft-fabric-68f43043ff34?source=collection_archive---------7-----------------------#2024-04-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/experimenting-with-mlflow-and-microsoft-fabric-68f43043ff34?source=collection_archive---------7-----------------------#2024-04-22](https://towardsdatascience.com/experimenting-with-mlflow-and-microsoft-fabric-68f43043ff34?source=collection_archive---------7-----------------------#2024-04-22)
- en: Fabric Madness part 4
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Fabric ç–¯ç‹‚ç³»åˆ—ç¬¬4éƒ¨åˆ†
- en: '[](https://medium.com/@roger_noble?source=post_page---byline--68f43043ff34--------------------------------)[![Roger
    Noble](../Images/869b5b0f237f24b119ca6c41c2e31162.png)](https://medium.com/@roger_noble?source=post_page---byline--68f43043ff34--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--68f43043ff34--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--68f43043ff34--------------------------------)
    [Roger Noble](https://medium.com/@roger_noble?source=post_page---byline--68f43043ff34--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@roger_noble?source=post_page---byline--68f43043ff34--------------------------------)[![Roger
    Noble](../Images/869b5b0f237f24b119ca6c41c2e31162.png)](https://medium.com/@roger_noble?source=post_page---byline--68f43043ff34--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--68f43043ff34--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--68f43043ff34--------------------------------)
    [Roger Noble](https://medium.com/@roger_noble?source=post_page---byline--68f43043ff34--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--68f43043ff34--------------------------------)
    Â·10 min readÂ·Apr 22, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--68f43043ff34--------------------------------)
    Â·10åˆ†é’Ÿé˜…è¯»Â·2024å¹´4æœˆ22æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/e7c1d4c5510a3052bfb3ab90be98619c.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7c1d4c5510a3052bfb3ab90be98619c.png)'
- en: Image by author and ChatGPT. â€œDesign an illustration, with imagery representing
    data experiments, focusing on basketball dataâ€ prompt. ChatGPT, 4, OpenAI, 15April.
    2024\. [https://chat.openai.com.](https://chat.openai.com./)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼šä½œè€…å’Œ ChatGPTã€‚â€œè®¾è®¡ä¸€å¹…æ’å›¾ï¼Œå±•ç¤ºæ•°æ®å®éªŒçš„å›¾åƒï¼Œèšç„¦äºç¯®çƒæ•°æ®â€çš„æç¤ºã€‚ChatGPTï¼Œ4ï¼ŒOpenAIï¼Œ2024å¹´4æœˆ15æ—¥ã€‚[https://chat.openai.com.](https://chat.openai.com./)
- en: '*A Huge thanks to* [*Martim Chaves*](https://medium.com/@mgrc99) *who co-authored
    this post and developed the example scripts.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*ç‰¹åˆ«æ„Ÿè°¢* [*Martim Chaves*](https://medium.com/@mgrc99) *å…±åŒæ’°å†™äº†è¿™ç¯‡æ–‡ç« å¹¶å¼€å‘äº†ç¤ºä¾‹è„šæœ¬ã€‚*'
- en: Itâ€™s no secret that Machine Learning (ML) systems require careful tuning to
    become truly useful, and it would be an extremely rare occurrence for a model
    to work perfectly the first time itâ€™s run!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯‹åº¸ç½®ç–‘ï¼Œæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ç³»ç»Ÿéœ€è¦ç²¾å¿ƒè°ƒä¼˜æ‰èƒ½çœŸæ­£å‘æŒ¥ä½œç”¨ï¼Œè€Œæ¨¡å‹åœ¨ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶å®Œç¾å·¥ä½œæ˜¯æä¸ºç½•è§çš„æƒ…å†µï¼
- en: When first starting out on your ML journey, an easy trap to fall into is to
    try lots of different things to improve performance, but not recording these configurations
    along the way. This then makes it difficult to know which configuration (or combination
    of configurations) had the best performance.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹ä½ çš„ ML ä¹‹æ—…æ—¶ï¼Œä¸€ä¸ªå®¹æ˜“é™·å…¥çš„é™·é˜±æ˜¯å°è¯•å¾ˆå¤šä¸åŒçš„æ–¹å¼æ¥æé«˜æ€§èƒ½ï¼Œä½†å´æ²¡æœ‰è®°å½•è¿™äº›é…ç½®ã€‚è¿™ä¼šå¯¼è‡´ä½ å¾ˆéš¾çŸ¥é“å“ªä¸ªé…ç½®ï¼ˆæˆ–é…ç½®ç»„åˆï¼‰è¡¨ç°æœ€ä½³ã€‚
- en: When developing models, there are lots of â€œknobsâ€ and â€œleversâ€ that can be adjusted,
    and often the best way to improve is to try different configurations and see which
    one works best. These things include [improving the features being used](https://medium.com/@roger_noble/feature-engineering-with-microsoft-fabric-and-pyspark-16d458018744),
    trying different model architectures, adjusting the modelâ€™s hyperparameters, and
    others. Experimentation needs to be systematic, and the results need to be logged.
    Thatâ€™s why having a good setup to carry out these experiments is fundamental in
    the development of any practical ML System, in the same way that source control
    is fundamental for code.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å‘æ¨¡å‹æ—¶ï¼Œæœ‰è®¸å¤šå¯ä»¥è°ƒæ•´çš„â€œæ—‹é’®â€å’Œâ€œæ æ†â€ï¼Œé€šå¸¸æé«˜æ€§èƒ½çš„æœ€ä½³æ–¹æ³•æ˜¯å°è¯•ä¸åŒçš„é…ç½®ï¼Œçœ‹çœ‹å“ªä¸ªæ•ˆæœæœ€å¥½ã€‚è¿™äº›å†…å®¹åŒ…æ‹¬[æ”¹è¿›ä½¿ç”¨çš„ç‰¹å¾](https://medium.com/@roger_noble/feature-engineering-with-microsoft-fabric-and-pyspark-16d458018744)ã€å°è¯•ä¸åŒçš„æ¨¡å‹æ¶æ„ã€è°ƒæ•´æ¨¡å‹çš„è¶…å‚æ•°ç­‰ã€‚å®éªŒéœ€è¦ç³»ç»ŸåŒ–ï¼Œå¹¶ä¸”ç»“æœéœ€è¦è®°å½•ã€‚å› æ­¤ï¼Œæ‹¥æœ‰ä¸€ä¸ªè‰¯å¥½çš„å®éªŒè®¾ç½®å¯¹äºä»»ä½•å®ç”¨çš„
    ML ç³»ç»Ÿå¼€å‘è‡³å…³é‡è¦ï¼Œå°±åƒæºä»£ç ç®¡ç†å¯¹äºä»£ç å¼€å‘çš„é‡è¦æ€§ä¸€æ ·ã€‚
- en: This is where *experiments* come in to play. Experiments are a way to keep track
    of these different configurations, and the results that come from them.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯*å®éªŒ*å¼€å§‹å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚å®éªŒæ˜¯ä¸€ç§è·Ÿè¸ªä¸åŒé…ç½®åŠå…¶ç»“æœçš„æ–¹æ³•ã€‚
- en: Whatâ€™s great about experiments in Fabric is that they are actually a wrapper
    for [MLFlow](https://mlflow.org/), a hugely popular, open-source platform for
    managing the end-to-end machine learning lifecycle. This means that we can use
    all of the great features that MLFlow has to offer, but with the added benefit
    of not having to worry about setting up the infrastructure that a collaborative
    MLFlow environment would require. This allows us to focus on the fun stuff ğŸ˜!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Fabricä¸­ä½¿ç”¨å®éªŒçš„å¥½å¤„æ˜¯ï¼Œå®ƒä»¬å®é™…ä¸Šæ˜¯[MLFlow](https://mlflow.org/)çš„ä¸€ä¸ªå°è£…ï¼ŒMLFlowæ˜¯ä¸€ä¸ªéå¸¸æµè¡Œçš„å¼€æºå¹³å°ï¼Œç”¨äºç®¡ç†ç«¯åˆ°ç«¯çš„æœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥ä½¿ç”¨MLFlowæä¾›çš„æ‰€æœ‰å¼ºå¤§åŠŸèƒ½ï¼Œä½†åˆä¸å¿…æ‹…å¿ƒè®¾ç½®ä¸€ä¸ªéœ€è¦åä½œç¯å¢ƒçš„MLFlowåŸºç¡€è®¾æ–½ã€‚è¿™ä½¿æˆ‘ä»¬å¯ä»¥ä¸“æ³¨äºæ›´æœ‰è¶£çš„éƒ¨åˆ†
    ğŸ˜ï¼
- en: 'In this post, weâ€™ll be going over how to use experiments in Fabric, and how
    to log and analyse the results of these experiments. Specifically, weâ€™ll cover:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºå¦‚ä½•åœ¨Fabricä¸­ä½¿ç”¨å®éªŒï¼Œä»¥åŠå¦‚ä½•è®°å½•å’Œåˆ†æè¿™äº›å®éªŒçš„ç»“æœã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†æ¶µç›–ï¼š
- en: How does MLFlow work?
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLFlowæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ
- en: Creating and Setting experiments
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ›å»ºå’Œè®¾ç½®å®éªŒ
- en: Running experiments and Logging Results
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿è¡Œå®éªŒå’Œè®°å½•ç»“æœ
- en: Analysing Results
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ†æç»“æœ
- en: At a high level, MLFlow is a platform that helps manage the end-to-end machine
    learning lifecycle. Itâ€™s a tool that helps with tracking experiments, packaging
    code into reproducible runs, and sharing and deploying models. Itâ€™s essentially
    a database thatâ€™s dedicated to keeping track of all the different configurations
    and results of the experiments that you run.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä»é«˜å±‚æ¬¡æ¥çœ‹ï¼ŒMLFlowæ˜¯ä¸€ä¸ªå¸®åŠ©ç®¡ç†ç«¯åˆ°ç«¯æœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸçš„å¹³å°ã€‚å®ƒæ˜¯ä¸€ä¸ªå¸®åŠ©è·Ÿè¸ªå®éªŒã€å°†ä»£ç æ‰“åŒ…æˆå¯é‡ç°è¿è¡Œã€å…±äº«å’Œéƒ¨ç½²æ¨¡å‹çš„å·¥å…·ã€‚å®ƒæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè·Ÿè¸ªä½ è¿è¡Œçš„å„ç§å®éªŒé…ç½®å’Œç»“æœçš„æ•°æ®åº“ã€‚
- en: There are two main organisational structures in MLFlow â€” **experiments** and
    **runs**.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨MLFlowä¸­æœ‰ä¸¤ä¸ªä¸»è¦çš„ç»„ç»‡ç»“æ„â€”â€”**å®éªŒ**å’Œ**è¿è¡Œ**ã€‚
- en: An experiment is a group of runs, where a run is the execution of a block of
    code, a function or a script. This could be training a model, but it could also
    be used to track anything where things might change between runs. An experiment
    is then a way to group related runs.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å®éªŒæ˜¯ä¸€ä¸ªè¿è¡Œçš„é›†åˆï¼Œå…¶ä¸­æ¯ä¸ªè¿è¡Œæ˜¯æ‰§è¡Œä¸€æ®µä»£ç ã€ä¸€ä¸ªå‡½æ•°æˆ–ä¸€ä¸ªè„šæœ¬ã€‚è¿™å¯èƒ½æ˜¯è®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œä½†ä¹Ÿå¯ä»¥ç”¨äºè·Ÿè¸ªä»»ä½•åœ¨ä¸åŒè¿è¡Œé—´å¯èƒ½ä¼šå˜åŒ–çš„å†…å®¹ã€‚å®éªŒæ˜¯ä¸€ç§å°†ç›¸å…³è¿è¡Œè¿›è¡Œåˆ†ç»„çš„æ–¹å¼ã€‚
- en: For each run, information can be logged and attached to it â€” these could be
    metrics, hyperparameters, tags, artifacts (like plots, files or other useful outputs),
    and even models! By attaching models to runs, we can keep track of which model
    was used in which run, and how it performed. Think of it like source control for
    models, which is something weâ€™ll go into in the next post.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªè¿è¡Œï¼Œå¯ä»¥è®°å½•ä¿¡æ¯å¹¶å°†å…¶é™„åŠ åˆ°è¯¥è¿è¡Œä¸Šâ€”â€”è¿™äº›ä¿¡æ¯å¯ä»¥æ˜¯æŒ‡æ ‡ã€è¶…å‚æ•°ã€æ ‡ç­¾ã€å·¥ä»¶ï¼ˆä¾‹å¦‚å›¾è¡¨ã€æ–‡ä»¶æˆ–å…¶ä»–æœ‰ç”¨çš„è¾“å‡ºï¼‰ï¼Œç”šè‡³æ˜¯æ¨¡å‹ï¼é€šè¿‡å°†æ¨¡å‹é™„åŠ åˆ°è¿è¡Œä¸Šï¼Œæˆ‘ä»¬å¯ä»¥è¿½è¸ªå“ªä¸ªæ¨¡å‹åœ¨æŸä¸ªè¿è¡Œä¸­è¢«ä½¿ç”¨ï¼Œä»¥åŠå®ƒçš„è¡¨ç°å¦‚ä½•ã€‚å¯ä»¥å°†å…¶è§†ä¸ºæ¨¡å‹çš„ç‰ˆæœ¬æ§åˆ¶ï¼Œè¿™ä¹Ÿæ˜¯æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­æ·±å…¥æ¢è®¨çš„å†…å®¹ã€‚
- en: Runs can be filtered and compared. This allows us to understand which runs were
    more successful, and select the best performing run and use its setup (for example,
    in deployment).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è¿è¡Œå¯ä»¥è¢«è¿‡æ»¤å’Œæ¯”è¾ƒã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿäº†è§£å“ªäº›è¿è¡Œæ›´æˆåŠŸï¼Œå¹¶é€‰æ‹©è¡¨ç°æœ€ä½³çš„è¿è¡Œï¼Œä½¿ç”¨å…¶é…ç½®ï¼ˆä¾‹å¦‚ï¼Œåœ¨éƒ¨ç½²ä¸­ï¼‰ã€‚
- en: Now that weâ€™ve covered the basics of how MLFlow works, letâ€™s get into how we
    can use it in Fabric!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»ä»‹ç»äº†MLFlowçš„åŸºæœ¬å·¥ä½œåŸç†ï¼Œæ¥ä¸‹æ¥è®©æˆ‘ä»¬äº†è§£å¦‚ä½•åœ¨Fabricä¸­ä½¿ç”¨å®ƒï¼
- en: Creating and setting experiments
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆ›å»ºå’Œè®¾ç½®å®éªŒ
- en: Like everything in Fabric, creating items can be done in a few ways, either
    from the workspace **+ New** menu, using the Data Science experience or in code.
    In this case, weâ€™ll be using the Data Science experience.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒåœ¨Fabricä¸­çš„ä¸€åˆ‡ä¸€æ ·ï¼Œåˆ›å»ºé¡¹ç›®å¯ä»¥é€šè¿‡å‡ ç§æ–¹å¼å®Œæˆï¼Œæ—¢å¯ä»¥é€šè¿‡å·¥ä½œåŒºä¸­çš„**+ æ–°å»º**èœå•ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨æ•°æ®ç§‘å­¦ä½“éªŒæˆ–é€šè¿‡ä»£ç ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ•°æ®ç§‘å­¦ä½“éªŒã€‚
- en: '![](../Images/be643f7ab148a28cc673d70288df619f.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be643f7ab148a28cc673d70288df619f.png)'
- en: Fig. 1 â€” Creating an Experiment using the UI. Image by author.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾1â€”â€”ä½¿ç”¨UIåˆ›å»ºå®éªŒã€‚å›¾åƒæ¥æºï¼šä½œè€…ã€‚
- en: 'Once that is done, to use that experiment in a Notebook, we need to `import
    mlflow` and set up the experiment name:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦å®Œæˆï¼Œä¸ºäº†åœ¨Notebookä¸­ä½¿ç”¨è¯¥å®éªŒï¼Œæˆ‘ä»¬éœ€è¦`import mlflow`å¹¶è®¾ç½®å®éªŒåç§°ï¼š
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Alternatively, an experiment can be created from code, which requires one extra
    command:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å¦å¤–ï¼Œå®éªŒä¹Ÿå¯ä»¥é€šè¿‡ä»£ç åˆ›å»ºï¼Œè¿™éœ€è¦ä¸€ä¸ªé¢å¤–çš„å‘½ä»¤ï¼š
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Note that, if an experiment with that name already exists, `create_experiment`
    will throw an error. We can avoid this by first checking for the existence of
    an experiment, and only creating it if it doesn''t exist:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå¦‚æœå·²å­˜åœ¨ç›¸åŒåç§°çš„å®éªŒï¼Œ`create_experiment`å°†æŠ›å‡ºä¸€ä¸ªé”™è¯¯ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡å…ˆæ£€æŸ¥å®éªŒæ˜¯å¦å­˜åœ¨ï¼Œåªæœ‰åœ¨ä¸å­˜åœ¨æ—¶æ‰åˆ›å»ºå®ƒæ¥é¿å…è¿™ä¸ªé—®é¢˜ï¼š
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now that we have the experiment set in the current context, we can start running
    code that will be saved to that experiment.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»åœ¨å½“å‰ä¸Šä¸‹æ–‡ä¸­è®¾ç½®äº†å®éªŒï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹è¿è¡Œå°†ä¿å­˜åˆ°è¯¥å®éªŒä¸­çš„ä»£ç ã€‚
- en: Running experiments and logging results
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¿è¡Œå®éªŒå¹¶è®°å½•ç»“æœ
- en: 'To start logging our results to an experiment, we need to start a run. This
    is done using the `start_run()` function and returns a `run` context manager.
    Here''s an example of how to start a run:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å¼€å§‹å°†æˆ‘ä»¬çš„ç»“æœè®°å½•åˆ°å®éªŒä¸­ï¼Œæˆ‘ä»¬éœ€è¦å¯åŠ¨ä¸€ä¸ªè¿è¡Œã€‚è¿™ä¸ªæ“ä½œæ˜¯é€šè¿‡`start_run()`å‡½æ•°å®Œæˆçš„ï¼Œå¹¶è¿”å›ä¸€ä¸ª`run`ä¸Šä¸‹æ–‡ç®¡ç†å™¨ã€‚ä»¥ä¸‹æ˜¯å¦‚ä½•å¯åŠ¨ä¸€ä¸ªè¿è¡Œçš„ç¤ºä¾‹ï¼š
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once the run is started, we can then begin logging metrics, parameters, and
    artifacts. Hereâ€™s an example of code that would do that using a simple model and
    dataset, where we log the modelâ€™s score and the hyperparameters used:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦è¿è¡Œå¼€å§‹ï¼Œæˆ‘ä»¬å°±å¯ä»¥å¼€å§‹è®°å½•åº¦é‡ã€å‚æ•°å’Œå·¥ä»¶ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨ç®€å•æ¨¡å‹å’Œæ•°æ®é›†çš„ä»£ç ç¤ºä¾‹ï¼Œæˆ‘ä»¬è®°å½•äº†æ¨¡å‹çš„å¾—åˆ†å’Œä½¿ç”¨çš„è¶…å‚æ•°ï¼š
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In our example above, a simple model is trained, and its score is calculated.
    Note how metrics can be logged by using `mlflow.log_metric("metric_name", metric)`
    and hyperparameters can be logged using `mlflow.log_param("param_name", param)`.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œè®­ç»ƒäº†ä¸€ä¸ªç®€å•çš„æ¨¡å‹ï¼Œå¹¶è®¡ç®—äº†å…¶å¾—åˆ†ã€‚è¯·æ³¨æ„ï¼Œå¦‚ä½•ä½¿ç”¨`mlflow.log_metric("metric_name", metric)`æ¥è®°å½•åº¦é‡ï¼Œå¹¶ä½¿ç”¨`mlflow.log_param("param_name",
    param)`æ¥è®°å½•è¶…å‚æ•°ã€‚
- en: The Data
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°æ®
- en: Letâ€™s now look at the code used for training our models, which are based on
    the outcome of basketball games. The data we are looking at is from the 2024 US
    college basketball tournaments, which was obtained from the March Machine Learning
    Mania 2024 Kaggle competition, the details of which can be found [here](https://www.kaggle.com/competitions/march-machine-learning-mania-2024/overview),
    and is licensed under CC BY 4.0
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ç”¨äºè®­ç»ƒæˆ‘ä»¬åŸºäºç¯®çƒæ¯”èµ›ç»“æœçš„æ¨¡å‹çš„ä»£ç ã€‚æˆ‘ä»¬æ‰€æŸ¥çœ‹çš„æ•°æ®æ¥è‡ª2024å¹´ç¾å›½å¤§å­¦ç¯®çƒé”¦æ ‡èµ›ï¼Œè¿™äº›æ•°æ®æ¥è‡ª2024å¹´3æœˆæœºå™¨å­¦ä¹ ç‹‚çƒ­Kaggleç«èµ›ï¼Œç›¸å…³ç»†èŠ‚å¯ä»¥åœ¨[æ­¤å¤„](https://www.kaggle.com/competitions/march-machine-learning-mania-2024/overview)æ‰¾åˆ°ï¼Œä¸”è¯¥æ•°æ®é›†ä½¿ç”¨CC
    BY 4.0è®¸å¯åè®®ã€‚
- en: In out setup, we wanted to try three different models, that used an increasing
    number of parameters. For each model, we also wanted to try three different learning
    rates (a hyperparameter that controls how much we are adjusting the weights of
    our network for each iteration). The goal was to find the best model and learning
    rate combination that would give us the best [Brier score](https://en.wikipedia.org/wiki/Brier_score)
    on the test set.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„è®¾ç½®ä¸­ï¼Œæˆ‘ä»¬æƒ³å°è¯•ä¸‰ç§ä¸åŒçš„æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹ä½¿ç”¨äº†è¶Šæ¥è¶Šå¤šçš„å‚æ•°ã€‚å¯¹äºæ¯ä¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬è¿˜æƒ³å°è¯•ä¸‰ç§ä¸åŒçš„å­¦ä¹ ç‡ï¼ˆä¸€ä¸ªæ§åˆ¶æˆ‘ä»¬åœ¨æ¯æ¬¡è¿­ä»£ä¸­è°ƒæ•´ç½‘ç»œæƒé‡å¤šå°‘çš„è¶…å‚æ•°ï¼‰ã€‚ç›®æ ‡æ˜¯æ‰¾åˆ°æœ€ä½³çš„æ¨¡å‹å’Œå­¦ä¹ ç‡ç»„åˆï¼Œä»¥ä¾¿åœ¨æµ‹è¯•é›†ä¸Šè·å¾—æœ€ä½³çš„[Brierå¾—åˆ†](https://en.wikipedia.org/wiki/Brier_score)ã€‚
- en: The Models
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨¡å‹
- en: To define the model architecture, we used TensorFlow, creating three simple
    neural networks. Here are the functions that helped define the models.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®šä¹‰æ¨¡å‹æ¶æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†TensorFlowï¼Œåˆ›å»ºäº†ä¸‰ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œã€‚ä»¥ä¸‹æ˜¯å¸®åŠ©å®šä¹‰æ¨¡å‹çš„å‡½æ•°ã€‚
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Creating our models in this way allows us to easily experiment with different
    architectures, and see how they perform. We can then use a dictionary to create
    a little *model factory*, that will allow us to easily create the models we want
    to experiment with.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è¿™ç§æ–¹å¼åˆ›å»ºæ¨¡å‹ï¼Œä½¿æˆ‘ä»¬å¯ä»¥è½»æ¾åœ°å°è¯•ä¸åŒçš„æ¶æ„ï¼Œå¹¶æŸ¥çœ‹å®ƒä»¬çš„è¡¨ç°ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å­—å…¸åˆ›å»ºä¸€ä¸ªå°å‹çš„*æ¨¡å‹å·¥å‚*ï¼Œè®©æˆ‘ä»¬èƒ½å¤Ÿè½»æ¾åœ°åˆ›å»ºæˆ‘ä»¬æƒ³è¦å®éªŒçš„æ¨¡å‹ã€‚
- en: We also defined the input shape, which was the number of features that were
    available. We decided to train the models for 100 epochs, which should be enough
    for convergence ğŸ¤.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å®šä¹‰äº†è¾“å…¥å½¢çŠ¶ï¼Œå³å¯ç”¨ç‰¹å¾çš„æ•°é‡ã€‚æˆ‘ä»¬å†³å®šå°†æ¨¡å‹è®­ç»ƒ100ä¸ªepochï¼Œè¿™åº”è¯¥è¶³ä»¥è®©æ¨¡å‹æ”¶æ•›ğŸ¤ã€‚
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: After this initial setup, it was time to iterate over the modelsâ€™ dictionary.
    For each model, an experiment was created. Note how weâ€™re using the code snippet
    from before, where we first check if the experiment exists, and only if it doesnâ€™t
    do we create it. Otherwise, we just set it.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™åˆæ­¥è®¾ç½®ä¹‹åï¼Œæ˜¯æ—¶å€™å¯¹æ¨¡å‹å­—å…¸è¿›è¡Œè¿­ä»£äº†ã€‚å¯¹äºæ¯ä¸ªæ¨¡å‹ï¼Œéƒ½ä¼šåˆ›å»ºä¸€ä¸ªå®éªŒã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¹‹å‰çš„ä»£ç ç‰‡æ®µï¼Œå…¶ä¸­æˆ‘ä»¬é¦–å…ˆæ£€æŸ¥å®éªŒæ˜¯å¦å­˜åœ¨ï¼Œåªæœ‰åœ¨å®éªŒä¸å­˜åœ¨æ—¶æ‰ä¼šåˆ›å»ºå®ƒã€‚å¦åˆ™ï¼Œæˆ‘ä»¬åªéœ€è®¾ç½®å®ƒã€‚
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Having set the experiment, we then performed three runs for each model, trying
    out different learning rates `[0.001, 0.01, 0.1]`.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ç½®å®Œå®éªŒåï¼Œæˆ‘ä»¬é’ˆå¯¹æ¯ä¸ªæ¨¡å‹è¿›è¡Œäº†ä¸‰æ¬¡è¿è¡Œï¼Œå°è¯•ä¸åŒçš„å­¦ä¹ ç‡`[0.001, 0.01, 0.1]`ã€‚
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Then, in each run, we initialised a model, compiled it, and trained it. The
    compilation and training were done in a separate function, which weâ€™ll go into
    next. As we wanted to set the learning rate, we had to manually initialise the
    Adam optimiser. As our metric we used the Mean Squared Error (MSE) loss function,
    saving the model with the best validation loss, and logged the training and validation
    loss to ensure that the model was converging.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œåœ¨æ¯æ¬¡è¿è¡Œä¸­ï¼Œæˆ‘ä»¬åˆå§‹åŒ–äº†ä¸€ä¸ªæ¨¡å‹ï¼Œç¼–è¯‘å¹¶è®­ç»ƒå®ƒã€‚ç¼–è¯‘å’Œè®­ç»ƒæ˜¯åœ¨ä¸€ä¸ªå•ç‹¬çš„å‡½æ•°ä¸­å®Œæˆçš„ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†è¯¦ç»†è®²è§£ã€‚ç”±äºæˆ‘ä»¬å¸Œæœ›è®¾ç½®å­¦ä¹ ç‡ï¼Œå› æ­¤å¿…é¡»æ‰‹åŠ¨åˆå§‹åŒ–
    Adam ä¼˜åŒ–å™¨ã€‚æˆ‘ä»¬ä½¿ç”¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æŸå¤±å‡½æ•°ä½œä¸ºæŒ‡æ ‡ï¼Œä¿å­˜å…·æœ‰æœ€ä½³éªŒè¯æŸå¤±çš„æ¨¡å‹ï¼Œå¹¶è®°å½•è®­ç»ƒå’ŒéªŒè¯æŸå¤±ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨æ”¶æ•›ã€‚
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Having initialised a model, compiled and trained it, the next step was logging
    the training and validation losses, calculating the brier score for the test set,
    then logging the score and the learning rate used. Typically we would also log
    the training and validation loss using the `step` argument in `log_metric`, like
    so:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åˆå§‹åŒ–æ¨¡å‹ã€ç¼–è¯‘å¹¶è®­ç»ƒå®ƒä¹‹åï¼Œæ¥ä¸‹æ¥çš„æ­¥éª¤æ˜¯è®°å½•è®­ç»ƒå’ŒéªŒè¯æŸå¤±ï¼Œè®¡ç®—æµ‹è¯•é›†çš„ Brier åˆ†æ•°ï¼Œç„¶åè®°å½•å¾—åˆ†å’Œä½¿ç”¨çš„å­¦ä¹ ç‡ã€‚é€šå¸¸æˆ‘ä»¬è¿˜ä¼šä½¿ç”¨ `step`
    å‚æ•°åœ¨ `log_metric` ä¸­è®°å½•è®­ç»ƒå’ŒéªŒè¯æŸå¤±ï¼Œåƒè¿™æ ·ï¼š
- en: '[PRE10]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: However, we opted to create the training and validation loss plot ourselves
    using `matplotlib` and log that as an artifact.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæˆ‘ä»¬é€‰æ‹©è‡ªå·±ä½¿ç”¨ `matplotlib` åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æŸå¤±å›¾ï¼Œå¹¶å°†å…¶è®°å½•ä¸ºä¸€ä¸ªå·¥ä»¶ã€‚
- en: 'Hereâ€™s the plot function:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ç»˜å›¾å‡½æ•°ï¼š
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Putting everything together, hereâ€™s what the code for that looks like:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰å†…å®¹æ•´åˆèµ·æ¥ï¼Œä»¥ä¸‹æ˜¯è¯¥ä»£ç çš„æ ·å­ï¼š
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: For each run we also logged the model, which will be useful later on.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯æ¬¡è¿è¡Œï¼Œæˆ‘ä»¬è¿˜è®°å½•äº†æ¨¡å‹ï¼Œè¿™å¯¹åç»­ä¼šå¾ˆæœ‰ç”¨ã€‚
- en: The experiments were run, creating an experiment for each model, and three different
    runs for each experiment with each of the learning rates.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å®éªŒå·²è¢«è¿è¡Œï¼Œä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºäº†ä¸€ä¸ªå®éªŒï¼Œå¹¶ä¸ºæ¯ä¸ªå®éªŒè¿›è¡Œäº†ä¸‰æ¬¡ä¸åŒçš„è¿è¡Œï¼Œä½¿ç”¨äº†ä¸åŒçš„å­¦ä¹ ç‡ã€‚
- en: Analysing results
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆ†æç»“æœ
- en: Now that weâ€™ve run some experiments, itâ€™s time to analyse the results! To do
    this, we can go back to the workspace, where weâ€™ll find our newly created experiments
    with several runs.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»è¿è¡Œäº†ä¸€äº›å®éªŒï¼Œæ˜¯æ—¶å€™åˆ†æç»“æœäº†ï¼ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å›åˆ°å·¥ä½œåŒºï¼Œåœ¨é‚£é‡Œæˆ‘ä»¬å¯ä»¥æ‰¾åˆ°æ–°åˆ›å»ºçš„å®éªŒä»¥åŠå¤šä¸ªè¿è¡Œã€‚
- en: '![](../Images/716c608c377213fdf76833c2e0823194.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/716c608c377213fdf76833c2e0823194.png)'
- en: Fig. 2 â€” List of experiments. Image by author.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 2 â€” å®éªŒåˆ—è¡¨ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: 'Clicking on one experiment, hereâ€™s what weâ€™ll see:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ç‚¹å‡»ä¸€ä¸ªå®éªŒåï¼Œä»¥ä¸‹æ˜¯æˆ‘ä»¬å°†çœ‹åˆ°çš„å†…å®¹ï¼š
- en: '![](../Images/b9c60a7efb769462fc52aca550c959b2.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b9c60a7efb769462fc52aca550c959b2.png)'
- en: Fig. 3 â€” The Experiment UI. Image by author.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 3 â€” å®éªŒç•Œé¢ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: On the left weâ€™ll find all of the runs related to that experiment. In this case,
    weâ€™re looking at the small model experiment. For each run, thereâ€™s two artifacts,
    the validation loss plot and the trained model. Thereâ€™s also information about
    the runâ€™s properties â€” its status and duration, as well as the metrics and hyper-parameters
    logged.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å·¦ä¾§ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°ä¸è¯¥å®éªŒç›¸å…³çš„æ‰€æœ‰è¿è¡Œã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æ­£åœ¨æŸ¥çœ‹å°æ¨¡å‹å®éªŒã€‚å¯¹äºæ¯æ¬¡è¿è¡Œï¼Œéƒ½ä¼šæœ‰ä¸¤ä¸ªå·¥ä»¶ï¼Œå³éªŒè¯æŸå¤±å›¾å’Œè®­ç»ƒå¥½çš„æ¨¡å‹ã€‚è¿˜æœ‰å…³äºè¿è¡Œçš„å±æ€§ä¿¡æ¯â€”â€”çŠ¶æ€å’ŒæŒç»­æ—¶é—´ï¼Œä»¥åŠè®°å½•çš„æŒ‡æ ‡å’Œè¶…å‚æ•°ã€‚
- en: By clicking on the **View run list**, under the **Compare runs** section, we
    can compare the different runs.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ç‚¹å‡»**æŸ¥çœ‹è¿è¡Œåˆ—è¡¨**ï¼Œåœ¨**æ¯”è¾ƒè¿è¡Œ**éƒ¨åˆ†ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥æ¯”è¾ƒä¸åŒçš„è¿è¡Œã€‚
- en: '![](../Images/1eece2a8179dad8814505c5fdb9f7e7e.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1eece2a8179dad8814505c5fdb9f7e7e.png)'
- en: Fig. 4 â€” Comparing runs. Image by author.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4 â€” æ¯”è¾ƒè¿è¡Œã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: Inside the run list view, we can select the runs that we wish to compare. In
    the **metric comparison** tab, we can find plots that show the Brier score against
    the learning rate. In our case, it looks like the lower the learning rate, the
    better the score. We could even go further and create more plots for the different
    metrics against other hyperparameters (if different metrics and hyperparameters
    had been logged).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿è¡Œåˆ—è¡¨è§†å›¾ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©å¸Œæœ›æ¯”è¾ƒçš„è¿è¡Œã€‚åœ¨**æŒ‡æ ‡æ¯”è¾ƒ**é€‰é¡¹å¡ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥æ‰¾åˆ°å±•ç¤º Brier åˆ†æ•°ä¸å­¦ä¹ ç‡å…³ç³»çš„å›¾è¡¨ã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œçœ‹èµ·æ¥å­¦ä¹ ç‡è¶Šä½ï¼Œå¾—åˆ†è¶Šå¥½ã€‚æˆ‘ä»¬ç”šè‡³å¯ä»¥è¿›ä¸€æ­¥åˆ›å»ºæ›´å¤šå›¾è¡¨ï¼Œå±•ç¤ºä¸åŒæŒ‡æ ‡ä¸å…¶ä»–è¶…å‚æ•°çš„å…³ç³»ï¼ˆå¦‚æœä¸åŒçš„æŒ‡æ ‡å’Œè¶…å‚æ•°å·²è¢«è®°å½•çš„è¯ï¼‰ã€‚
- en: '![](../Images/44a7a6f448f6cbd157dbfee2415b5bb4.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44a7a6f448f6cbd157dbfee2415b5bb4.png)'
- en: Fig. 5 â€” Plot that shows Brier score against learning rate. Image by author.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5 â€” å±•ç¤º Brier åˆ†æ•°ä¸å­¦ä¹ ç‡å…³ç³»çš„å›¾è¡¨ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: Perhaps we would like to filter the runs â€” that can be done using **Filters**.
    For example we can select the runs that have a Brier score lower than 0.25\. You
    can create filters based on logged metrics and parameters and the runsâ€™ properties.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿè®¸æˆ‘ä»¬å¸Œæœ›ç­›é€‰è¿è¡Œâ€”â€”å¯ä»¥ä½¿ç”¨**ç­›é€‰å™¨**æ¥å®Œæˆæ­¤æ“ä½œã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹© Brier åˆ†æ•°ä½äº 0.25 çš„è¿è¡Œã€‚æ‚¨å¯ä»¥æ ¹æ®è®°å½•çš„æŒ‡æ ‡å’Œå‚æ•°ä»¥åŠè¿è¡Œçš„å±æ€§åˆ›å»ºç­›é€‰å™¨ã€‚
- en: '![](../Images/66491b7a477f7c74f04dd736a281a55b.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66491b7a477f7c74f04dd736a281a55b.png)'
- en: Fig. 6 â€” Filtering runs based on their Brier score. Image by author.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 6 â€” æ ¹æ® Brier å¾—åˆ†ç­›é€‰è¿è¡Œã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: By doing this, we can visually compare the different runs and assess which configuration
    led to the best performance. This can also be done using code â€” this is something
    that will be further explored in the next post.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è¿™æ ·åšï¼Œæˆ‘ä»¬å¯ä»¥ç›´è§‚åœ°æ¯”è¾ƒä¸åŒçš„è¿è¡Œå¹¶è¯„ä¼°å“ªä¸ªé…ç½®å¸¦æ¥äº†æœ€ä½³æ€§èƒ½ã€‚è¿™ä¹Ÿå¯ä»¥é€šè¿‡ä»£ç å®ç° â€”â€” è¿™å°†æ˜¯ä¸‹ä¸€ç¯‡æ–‡ç« è¿›ä¸€æ­¥æ¢è®¨çš„å†…å®¹ã€‚
- en: Using the experiment UI, we are then able to visually explore the different
    experiments and runs, comparing and filtering them as needed, to understand which
    configuration works best.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å®éªŒ UIï¼Œæˆ‘ä»¬èƒ½å¤Ÿç›´è§‚åœ°æ¢ç´¢ä¸åŒçš„å®éªŒå’Œè¿è¡Œï¼ŒæŒ‰éœ€è¿›è¡Œæ¯”è¾ƒå’Œç­›é€‰ï¼Œä»¥äº†è§£å“ªä¸ªé…ç½®æ•ˆæœæœ€ä½³ã€‚
- en: Conclusion
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: And that wraps up our exploration of experiments in Fabric!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯æˆ‘ä»¬å¯¹ Fabric å®éªŒçš„æ¢ç´¢æ€»ç»“ï¼
- en: Not only did we cover how to create and set up experiments, but we also went
    through how to run experiments and log the results. We also showed how to analyse
    the results, using the experiment UI to compare and filter runs.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸ä»…ä»‹ç»äº†å¦‚ä½•åˆ›å»ºå’Œè®¾ç½®å®éªŒï¼Œè¿˜è®²è§£äº†å¦‚ä½•è¿è¡Œå®éªŒå¹¶è®°å½•ç»“æœã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†å¦‚ä½•åˆ†æç»“æœï¼Œä½¿ç”¨å®éªŒ UI æ¥æ¯”è¾ƒå’Œç­›é€‰è¿è¡Œã€‚
- en: In the next post, weâ€™ll be looking at how to select the best model, and how
    to deploy it. Stay tuned!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºå¦‚ä½•é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼Œå¹¶å±•ç¤ºå¦‚ä½•éƒ¨ç½²å®ƒã€‚æ•¬è¯·æœŸå¾…ï¼
- en: '*Originally published at* [*https://nobledynamic.com*](https://nobledynamic.com/posts/fabric-madness-4/)
    *on April 22, 2024.*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*åŸæ–‡å‘å¸ƒäº* [*https://nobledynamic.com*](https://nobledynamic.com/posts/fabric-madness-4/)
    *ï¼Œå‘å¸ƒæ—¶é—´ä¸º 2024å¹´4æœˆ22æ—¥ã€‚*'
