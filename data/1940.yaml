- en: KernelSHAP can be misleading with correlated predictors
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KernelSHAP在预测变量相关时可能产生误导
- en: 原文：[https://towardsdatascience.com/kernelshap-can-be-misleading-with-correlated-predictors-9f64108f7cfb?source=collection_archive---------7-----------------------#2024-08-09](https://towardsdatascience.com/kernelshap-can-be-misleading-with-correlated-predictors-9f64108f7cfb?source=collection_archive---------7-----------------------#2024-08-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/kernelshap-can-be-misleading-with-correlated-predictors-9f64108f7cfb?source=collection_archive---------7-----------------------#2024-08-09](https://towardsdatascience.com/kernelshap-can-be-misleading-with-correlated-predictors-9f64108f7cfb?source=collection_archive---------7-----------------------#2024-08-09)
- en: A concrete case study
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个具体的案例研究
- en: '[](https://medium.com/@vanillaxiangshuyang?source=post_page---byline--9f64108f7cfb--------------------------------)[![Shuyang
    Xiang](../Images/36a5fd18fd9b7b88cb41094f09b83882.png)](https://medium.com/@vanillaxiangshuyang?source=post_page---byline--9f64108f7cfb--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9f64108f7cfb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9f64108f7cfb--------------------------------)
    [Shuyang Xiang](https://medium.com/@vanillaxiangshuyang?source=post_page---byline--9f64108f7cfb--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@vanillaxiangshuyang?source=post_page---byline--9f64108f7cfb--------------------------------)[![Shuyang
    Xiang](../Images/36a5fd18fd9b7b88cb41094f09b83882.png)](https://medium.com/@vanillaxiangshuyang?source=post_page---byline--9f64108f7cfb--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9f64108f7cfb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9f64108f7cfb--------------------------------)
    [Shuyang Xiang](https://medium.com/@vanillaxiangshuyang?source=post_page---byline--9f64108f7cfb--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9f64108f7cfb--------------------------------)
    ·7 min read·Aug 9, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9f64108f7cfb--------------------------------)
    ·阅读时长：7分钟·2024年8月9日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: “Like many other permutation-based interpretation methods, the Shapley value
    method suffers from inclusion of unrealistic data instances when features are
    correlated. To simulate that a feature value is missing from a coalition, we marginalize
    the feature. ..When features are dependent, then we might sample feature values
    that do not make sense for this instance. ”— [Interpretable-ML-Book](https://christophm.github.io/interpretable-ml-book/shapley.html#disadvantages-13).
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “像许多其他基于置换的解释方法一样，Shapley值方法在特征相关时会遭遇不现实数据实例的引入。为了模拟一个特征值在一个联合体中缺失的情况，我们对该特征进行边际化处理……当特征之间存在依赖关系时，我们可能会抽取一些对于当前实例来说不合理的特征值。”——
    [可解释机器学习书籍](https://christophm.github.io/interpretable-ml-book/shapley.html#disadvantages-13)。
- en: SHAP (SHapley Additive exPlanations) values are designed to fairly allocate
    the contribution of each feature to the prediction made by a machine learning
    model, based on the concept of Shapley values from cooperative game theory. The
    Shapley value framework has several desirable theoretical properties and can,
    in principle, handle any predictive model. However, SHAP values can potentially
    be misleading, especially when using the KernelSHAP method for approximation.
    When predictors are correlated, these approximations can be imprecise and even
    have the opposite sign.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP（Shapley加性解释）值旨在根据合作博弈论中的Shapley值概念，公平地分配每个特征对机器学习模型预测的贡献。Shapley值框架具有几个理想的理论属性，并且原则上可以处理任何预测模型。然而，SHAP值可能会产生误导，特别是在使用KernelSHAP方法进行近似时。当预测变量之间存在相关性时，这些近似值可能会不准确，甚至可能有相反的符号。
- en: In this blog post, I will demonstrate how the original SHAP values can differ
    significantly from approximations made by the [SHAP framework](https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html),
    especially the KernalSHAP and discuss the reasons behind these discrepancies.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我将展示原始的SHAP值如何与[SHAP框架](https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html)的近似值有显著差异，尤其是KernelSHAP，并讨论这些差异背后的原因。
- en: '**Case Study: Churn Rate**'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**案例研究：客户流失率**'
- en: 'Consider a scenario where we aim to predict the churn rate of rental leases
    in an office building, based on two key factors: occupancy rate and the rate of
    reported problems.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个场景，我们旨在预测一个办公楼租赁的流失率，基于两个关键因素：入住率和报告问题的比例。
- en: The occupancy rate significantly impacts the churn rate. For instance, if the
    occupancy rate is too low, tenants may leave due to the office being underutilized.
    Conversely, if the occupancy rate is too high, tenants might depart because of
    overcrowding, seeking better options elsewhere.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 占用率对流失率有显著影响。例如，如果占用率过低，租户可能会因为办公室未被充分利用而离开。相反，如果占用率过高，租户可能会因为拥挤而离开，寻求更好的选择。
- en: Furthermore, let’s assume that the rate of reported problems is highly correlated
    with the occupancy rate, specifically that the reported problem rate is the square
    of the occupancy rate.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们假设报告问题的比率与占用率高度相关，具体而言，报告问题的比率是占用率的平方。
- en: 'We define the churn rate function as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将流失率函数定义如下：
- en: '![](../Images/456107a845a24d3ffa3fb61d9ede4aa3.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/456107a845a24d3ffa3fb61d9ede4aa3.png)'
- en: 'Image by author: churn rate function'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像：流失率函数
- en: 'This function with respect to the two variables can be represented by the following
    illustrations:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数对于这两个变量的表示可以通过以下插图表示：
- en: '![](../Images/fac81573dd6ddaff630cd4e105c27f5c.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fac81573dd6ddaff630cd4e105c27f5c.png)'
- en: 'Image by author: Churn on two variables'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像：关于两个变量的流失
- en: Discrepancies between original SHAP and Kernel SHAP
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 原始SHAP与Kernel SHAP之间的差异
- en: SHAP Values Computed Using Kernel SHAP
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Kernel SHAP计算的SHAP值
- en: 'We will now use the following code to compute the SHAP values of the predictors:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下代码计算预测变量的SHAP值：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The code above performs the following tasks:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码执行以下任务：
- en: 'Data Preparation: A DataFrame named `churn_df` is created with columns `occupancy_rate`,
    `reported_problem_rate`, and `churn_rate`. Variables and target (`churn_rate`
    ) are then created from and Data is split into training and testing sets, with
    80% for training and 20% for testing. Note that a special data point with specific
    `occupancy_rate` and `reported_problem_rate` values is added to the test set `X_test`.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据准备：创建一个名为`churn_df`的DataFrame，包含`occupancy_rate`、`reported_problem_rate`和`churn_rate`列。然后从中创建变量和目标（`churn_rate`），并将数据拆分为训练集和测试集，训练集占80%，测试集占20%。注意，测试集`X_test`中添加了一个具有特定`occupancy_rate`和`reported_problem_rate`值的数据点。
- en: 'Prediction Function Definition: A function `predict_fn` is defined to calculate
    churn rate using a specific formula involving predefined constants.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测函数定义：定义了一个函数`predict_fn`，使用涉及预定义常量的特定公式计算流失率。
- en: 'SHAP Analysis: A SHAP `KernelExplainer` is initialized using the prediction
    function and `background_data` samples from `X_train.` SHAP values for `X_test`
    are computed using the `explainer`.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SHAP 分析：使用预测函数和来自`X_train`的`background_data`样本初始化一个SHAP `KernelExplainer`。然后使用`explainer`计算`X_test`的SHAP值。
- en: 'Below, you can see a summary SHAP bar plot, which represents the average SHAP
    values for `X_test` :'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 下面，您可以看到一个总结性的SHAP条形图，表示`X_test`的平均SHAP值：
- en: '![](../Images/4726e4abacf15f58d78cd540eb486327.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4726e4abacf15f58d78cd540eb486327.png)'
- en: 'Image by author: average shap values'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像：平均 SHAP 值
- en: 'In particular, we see that at the data point (0.8, 0.64), the SHAP values of
    the two features are 0.10 and -0.03, illustrated by the following force plot:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，我们看到在数据点（0.8，0.64）处，两个特征的SHAP值分别为0.10和-0.03，如下图所示的力图所示：
- en: '![](../Images/f3be60805793ee4b29f17cadb80ce818.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3be60805793ee4b29f17cadb80ce818.png)'
- en: Image by author Force Plot of one data point
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像：单一数据点的力图
- en: SHAP Values by orignal definition
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 原始定义的SHAP值
- en: 'Let’s take a step back and compute the exact SHAP values step by step according
    to their original definition. The general formula for SHAP values is given by:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们退后一步，根据SHAP的原始定义逐步计算确切的SHAP值。SHAP值的一般公式如下所示：
- en: '![](../Images/506fd5f8560513877e5e88618a511382.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/506fd5f8560513877e5e88618a511382.png)'
- en: 'where: S is a subset of all feature indices excluding i, |S| is the size of
    the subset S, M is the total number of features, f(XS​∪{xi​}) is the function
    evaluated with the features in S with xi present while f(XS) is the function evaluated
    with the feature in S with xi absent.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：S是所有特征索引的子集，排除i，|S|是子集S的大小，M是特征的总数，f(XS∪{xi})是包含xi的S的特征所评估的函数，而f(XS)是S中缺少xi时评估的函数。
- en: 'Now, let’s calculate the SHAP values for two features: occupancy rate (denoted
    as x1​) and reported problem rate (denoted as x2​) at the data point (0.8, 0.64).
    Recall that x1​ and x2are related by x1 = x2².'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们计算两个特征的SHAP值：占用率（表示为x1）和报告问题率（表示为x2），它们在数据点（0.8，0.64）处的值。回想一下，x1和x2之间的关系是x1
    = x2²。
- en: 'We have the SHAP value for occupancy rate at the data point:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了数据点处占用率的SHAP值：
- en: '![](../Images/745d8cfe9265145b22870e14c1902dd9.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/745d8cfe9265145b22870e14c1902dd9.png)'
- en: 'and, similary, for the feature reported problem rate:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，对于报告问题率这一特征：
- en: '![](../Images/3ea90e76aec7a3ba8c1e59d143dc01cf.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ea90e76aec7a3ba8c1e59d143dc01cf.png)'
- en: 'First, let’s compute the SHAP value for the occupancy rate at the data point:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们计算数据点处占用率的SHAP值：
- en: The first term is the expectation of the model’s output when X1​ is fixed at
    0.8 and X2​ is averaged over its distribution. Given the relationship x1 = x2²,
    this expectation leads to the model’s output at the specific point (0.8, 0.64).
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一项是当X1固定为0.8且X2在其分布上取平均时模型输出的期望值。由于x1 = x2²的关系，这一期望值导致模型在特定点(0.8, 0.64)处的输出。
- en: The second term is the unconditional expectation of the model’s output, where
    both X1 and X2 are averaged over their distributions. This can be computed by
    averaging the outputs over all data points in the background dataset.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二项是模型输出的无条件期望值，其中X1和X2都在其分布上取平均。可以通过对背景数据集中的所有数据点的输出进行平均来计算这一期望值。
- en: The third term is the model’s output at the specific point (0.8, 0.64).
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第三项是模型在特定点(0.8, 0.64)处的输出。
- en: The final term is the expectation of the model’s output when X1​ is averaged
    over its distribution, given that X2​ is fixed at the specific point 0.64\. Again,
    due to the relationship x_1 = x_2²​, this expectation matches the model’s output
    at (0.8, 0.64), similar to the first step.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一项是当X1在其分布上取平均时，给定X2固定在特定点0.64时模型输出的期望值。同样，考虑到x1 = x2²的关系，这一期望值与模型在(0.8, 0.64)处的输出相符，类似于第一步。
- en: Thus, the SHAP values compute from the original definition for the two features
    occupancy rate and reported problem rate at the data point (0.8, 0.64) are -0.0375
    and -0.0375, respectively, which is quite different from the values given by Kernel
    SHAP.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，从原始定义计算的两个特征——占用率和报告问题率在数据点(0.8, 0.64)处的SHAP值分别为-0.0375和-0.0375，这与Kernel
    SHAP给出的值有很大不同。
- en: Where comes discrepancies?
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 差异从何而来？
- en: Cause of Discrepancies in SHAP Values
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SHAP值差异的原因
- en: As you may have noticed, the discrepancy between the two methods primarily arises
    from the second and fourth steps, where we need to compute the conditional expectation.
    This involves calculating the expectation of the model’s output when X1X_1X1​
    is conditioned on 0.8.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所注意到的，两种方法之间的差异主要出现在第二步和第四步，这两步需要计算条件期望。这涉及到在X1被固定为0.8时，计算模型输出的期望值。
- en: '**Exact SHAP**: When computing exact SHAP values, the dependencies between
    features (such as x1=x_2² in our example​) are explicitly accounted for. This
    ensures accurate calculations by considering how feature interactions impact the
    model’s output.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确SHAP**：在计算精确的SHAP值时，特征之间的依赖关系（例如我们示例中的x1=x_2²）被显式考虑。这通过考虑特征交互如何影响模型输出，确保了准确的计算。'
- en: '**Kernel SHAP**: By default, Kernel SHAP assumes feature independence, which
    can lead to inaccurate SHAP values when features are actually dependent. According
    to the paper [*A Unified Approach to Interpreting Model Predictions*,](https://arxiv.org/abs/1705.07874)
    this assumption is a simplification. In practice, features are often correlated,
    making it challenging to achieve accurate approximations when using Kernel SHAP.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kernel SHAP**：默认情况下，Kernel SHAP假设特征之间是独立的，这在特征实际上是相关的情况下会导致不准确的SHAP值。根据论文[*A
    Unified Approach to Interpreting Model Predictions*](https://arxiv.org/abs/1705.07874)，这一假设是一个简化。在实际应用中，特征通常是相关的，这使得在使用Kernel
    SHAP时很难获得准确的近似值。'
- en: '![](../Images/536458182b111c3c9659970702905c74.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/536458182b111c3c9659970702905c74.png)'
- en: Screenshot from the paper
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 来自论文的截图
- en: Potential resolutions
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 潜在的解决方案
- en: 'Unfortunately, computing SHAP values directly based on their original definition
    can be computationally expensive. Here are some alternative approaches to consider:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，基于原始定义直接计算SHAP值可能会非常耗费计算资源。以下是一些可供考虑的替代方法：
- en: TreeSHAP
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TreeSHAP
- en: Designed specifically for tree-based models like random forests and gradient
    boosting machines, TreeSHAP efficiently computes SHAP values while effectively
    managing feature dependencies.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 专门为树模型（如随机森林和梯度提升机）设计的TreeSHAP高效地计算SHAP值，同时有效管理特征依赖关系。
- en: This method is optimized for tree ensembles, making it faster and more scalable
    compared to traditional SHAP computations.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该方法针对树集成进行了优化，使其比传统的SHAP计算更快且更具可扩展性。
- en: When using TreeSHAP within the SHAP framework, set the parameter `feature_perturbation
    = "interventional"` to account for feature dependencies accurately.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 SHAP 框架中使用 TreeSHAP 时，设置参数 `feature_perturbation = "interventional"` 以准确考虑特征之间的依赖性。
- en: Extending Kernel SHAP for Dependent Features
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展 Kernel SHAP 以处理依赖特征
- en: 'To address feature dependencies, this paper involves extending Kernel SHAP.
    One method is to assume that the feature vector follows a multivariate Gaussian
    distribution. In this approach:'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了解决特征之间的依赖性，本文扩展了 Kernel SHAP。一个方法是假设特征向量服从多元高斯分布。在这种方法中：
- en: Conditional distributions are modeled as multivariate Gaussian distributions.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 条件分布被建模为多元高斯分布。
- en: Samples are generated from these conditional Gaussian distributions using estimates
    from the training data.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 样本是通过使用来自训练数据的估计，从这些条件高斯分布中生成的。
- en: The integral in the approximation is computed based on these samples.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该近似中的积分是基于这些样本计算的。
- en: This method assumes a multivariate Gaussian distribution for features, which
    may not always be applicable in real-world scenarios where features can exhibit
    different dependency structures.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该方法假设特征服从多元高斯分布，但在实际场景中，特征可能展现出不同的依赖结构，因此该假设并不总是适用。
- en: Improving Kernel SHAP Accuracy
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升 Kernel SHAP 的准确性
- en: '**Description**: Enhance the accuracy of Kernel SHAP by ensuring that the background
    dataset used for approximation is representative of the actual data distribution
    with independant features.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**描述**：通过确保用于近似的背景数据集能够代表实际数据分布且特征独立，从而提高 Kernel SHAP 的准确性。'
- en: By utilizing these methods, you can address the computational challenges associated
    with calculating SHAP values and enhance their accuracy in practical applications.
    However, it is important to note that no single solution is universally optimal
    for all scenarios.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用这些方法，您可以解决计算 SHAP 值时遇到的挑战，并提高它们在实际应用中的准确性。然而，需要注意的是，没有一种解决方案可以在所有场景中都适用。
- en: Conclusion
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this blog post, we’ve explored how SHAP values, despite their strong theoretical
    foundation and versatility across various predictive models, can suffer from accuracy
    issues when predictors are correlated, particularly when approximations like KernelSHAP
    are employed. Understanding these limitations is crucial for effectively interpreting
    SHAP values. By recognizing the potential discrepancies and selecting the most
    suitable approximation methods, we can achieve more accurate and reliable feature
    attribution in our models.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博文中，我们探讨了尽管 SHAP 值具有强大的理论基础并且在各种预测模型中具有广泛的适用性，但当预测变量之间存在相关性时，特别是在采用类似 KernelSHAP
    这样的近似方法时，SHAP 值可能会出现准确性问题。理解这些局限性对于有效地解释 SHAP 值至关重要。通过识别潜在的差异并选择最合适的近似方法，我们可以在模型中实现更准确可靠的特征归因。
