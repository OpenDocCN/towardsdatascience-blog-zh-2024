- en: Understanding Kolmogorov–Arnold Networks (KAN)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解 Kolmogorov–Arnold 网络（KAN）
- en: 原文：[https://towardsdatascience.com/kolmogorov-arnold-networks-kan-e317b1b4d075?source=collection_archive---------1-----------------------#2024-05-07](https://towardsdatascience.com/kolmogorov-arnold-networks-kan-e317b1b4d075?source=collection_archive---------1-----------------------#2024-05-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/kolmogorov-arnold-networks-kan-e317b1b4d075?source=collection_archive---------1-----------------------#2024-05-07](https://towardsdatascience.com/kolmogorov-arnold-networks-kan-e317b1b4d075?source=collection_archive---------1-----------------------#2024-05-07)
- en: Why KANs are a potential alternative to MPLs and the current landscape of Machine
    Learning. Let’s go through the paper to find out.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么 KAN 有可能成为 MLP 和当前机器学习格局的替代方案？让我们通过论文来一探究竟。
- en: '[](https://medium.com/@itshesamsheikh?source=post_page---byline--e317b1b4d075--------------------------------)[![Hesam
    Sheikh](../Images/b8d5f4f285eef77634e4c1d4321580ed.png)](https://medium.com/@itshesamsheikh?source=post_page---byline--e317b1b4d075--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e317b1b4d075--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e317b1b4d075--------------------------------)
    [Hesam Sheikh](https://medium.com/@itshesamsheikh?source=post_page---byline--e317b1b4d075--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@itshesamsheikh?source=post_page---byline--e317b1b4d075--------------------------------)[![Hesam
    Sheikh](../Images/b8d5f4f285eef77634e4c1d4321580ed.png)](https://medium.com/@itshesamsheikh?source=post_page---byline--e317b1b4d075--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e317b1b4d075--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e317b1b4d075--------------------------------)
    [Hesam Sheikh](https://medium.com/@itshesamsheikh?source=post_page---byline--e317b1b4d075--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e317b1b4d075--------------------------------)
    ·10 min read·May 7, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e317b1b4d075--------------------------------)
    ·10分钟阅读·2024年5月7日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/e2bb69eb8377d05e1d15a1b462e6e4f1.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2bb69eb8377d05e1d15a1b462e6e4f1.png)'
- en: 'A new research paper titled [**KAN: Kolmogorov–Arnold Network**](https://arxiv.org/abs/2404.19756)
    has stirred excitement in the Machine Learning community. It presents a fresh
    perspective on Neural Networks and suggests a possible alternative to Multi-Layer
    Perceptrons (MLPs), a cornerstone of current Machine Learning.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 一篇名为[**KAN：Kolmogorov–Arnold 网络**](https://arxiv.org/abs/2404.19756)的新研究论文在机器学习社区中引发了热议。它为神经网络提供了新的视角，并提出了一个可能替代多层感知机（MLP）的方案，MLP
    是当前机器学习的基石。
- en: '*✨This is a paid article. If you’re not a Medium member, you can read this
    for free in my newsletter:* [***Qiubyte***](https://hesamsheikh.substack.com/)***.***'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*✨这是付费文章。如果你不是 Medium 会员，你可以在我的通讯中免费阅读此文：* [***Qiubyte***](https://hesamsheikh.substack.com/)***.***'
- en: Inspired by the [**Kolmogorov-Arnold representation theorem**](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Arnold_representation_theorem),
    KANs diverge from traditional Multi-Layer Perceptrons (MLPs) by replacing **fixed
    activation functions** with **learnable functions**, effectively eliminating the
    need for linear weight matrices.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 受[**Kolmogorov-Arnold 表示定理**](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Arnold_representation_theorem)的启发，KAN
    与传统的多层感知机（MLP）不同，它通过将**固定激活函数**替换为**可学习的函数**，有效地消除了线性权重矩阵的需求。
- en: I strongly recommend reading through this paper if you’re interested in the
    finer details and experiments. However, if you prefer a concise introduction,
    I’ve prepared this article to explain the essentials of KANs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对更细节的内容和实验感兴趣，我强烈建议阅读这篇论文。不过，如果你更喜欢简洁的介绍，我已经准备了这篇文章来解释 KAN 的核心要点。
- en: '*Note: The source of the images/figures used in this article is the “KAN: Kolmogorov–Arnold
    Network” paper unless stated otherwise.*'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*注：本文中使用的图片/图形来源于“KAN：Kolmogorov–Arnold 网络”论文，除非另有说明。*'
- en: The Theory
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理论
- en: The theoretical pillar of these new networks is a theory developed by two Soviet
    mathematicians, [Vladimir Arnold](https://en.wikipedia.org/wiki/Vladimir_Arnold)
    and [Andrey Kolmogorov](https://en.wikipedia.org/wiki/Andrey_Kolmogorov).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这些新型网络的理论支柱是一位由两位苏联数学家[弗拉基米尔·阿诺德](https://en.wikipedia.org/wiki/Vladimir_Arnold)和[安德烈·柯尔莫哥洛夫](https://en.wikipedia.org/wiki/Andrey_Kolmogorov)提出的理论。
- en: 'While a student of [Andrey Kolmogorov](https://en.wikipedia.org/wiki/Andrey_Kolmogorov)
    at [Moscow State University](https://en.wikipedia.org/wiki/Moscow_State_University)
    and still a teenager, Arnold showed in 1957 that any continuous function of several
    variables can be constructed with a finite number of two-variable functions, thereby
    solving [Hilbert’s thirteenth problem](https://en.wikipedia.org/wiki/Hilbert%27s_thirteenth_problem).
    (source: [Wikipedia](https://en.wikipedia.org/wiki/Vladimir_Arnold))'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在[安德烈·科尔莫哥洛夫](https://en.wikipedia.org/wiki/Andrey_Kolmogorov)的指导下，阿诺德在1957年证明了任何多个变量的连续函数都可以通过有限个二元函数构造出来，从而解决了[希尔伯特第十三问题](https://en.wikipedia.org/wiki/Hilbert%27s_thirteenth_problem)。（来源：[维基百科](https://en.wikipedia.org/wiki/Vladimir_Arnold)）
- en: The theory that they worked on and eventually developed was based on the concept
    of multivariate continuous functions. According to this theory, any multivariate
    continuous function **f** can be written as a finite composition of continuous
    functions of a single variable, summed together.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 他们所研究并最终发展的理论是基于多变量连续函数的概念。根据这个理论，任何多变量连续函数**f**都可以写成若干个单变量连续函数的有限组合，并将其相加。
- en: '![](../Images/a33b3591f50e3ebf2997fb82275ecae6.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a33b3591f50e3ebf2997fb82275ecae6.png)'
- en: 'The mathematical formula of the Kolmogorov–Arnold representation theorem. (source:
    [Wikipedia](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Arnold_representation_theorem))'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 科尔莫哥洛夫–阿诺德表示定理的数学公式。（来源：[维基百科](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Arnold_representation_theorem)）
- en: How Does This Theorem Fit into Machine Learning?
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这个定理如何融入机器学习？
- en: In machine learning, the ability to **efficiently** and **accurately** approximate
    complex functions is an important subject, especially as the dimensionality of
    data increases. Current mainstream models such as Multi-Layer Perceptrons (MLPs)
    often struggle with high-dimensional data — a phenomenon known as the [**curse
    of dimensionality**](https://en.wikipedia.org/wiki/Curse_of_dimensionality).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，**高效**且**准确**地近似复杂函数是一个重要课题，尤其是在数据维度增加时。当前主流模型，如多层感知器（MLP），通常在处理高维数据时存在困难——这一现象被称为[**维度灾难**](https://en.wikipedia.org/wiki/Curse_of_dimensionality)。
- en: The Kolmogorov-Arnold theorem, however, provides a theoretical foundation for
    building networks (like KANs) that can overcome this challenge.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，科尔莫哥洛夫-阿诺德定理为构建能够克服这一挑战的网络（如KAN）提供了理论基础。
- en: '![](../Images/7d53076c7522f649d743529aed513a16.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7d53076c7522f649d743529aed513a16.png)'
- en: An overview comparison of MLP and KAN.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: MLP和KAN的概述比较。
- en: '**How can KAN avoid the curse of dimensionality?**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**KAN如何避免维度灾难？**'
- en: This theorem allows for the decomposition of complex high-dimensional functions
    into compositions of simpler one-dimensional functions. By focusing on optimizing
    these one-dimensional functions rather than the entire multivariate space, KANs
    reduce the complexity and the number of parameters needed to achieve accurate
    modeling. Furthermore, Because of working with simpler one-dimensional functions,
    KANs can be simple and interpretable models.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定理允许将复杂的高维函数分解为更简单的单维函数的组合。通过专注于优化这些一维函数，而不是整个多变量空间，KAN减少了所需的复杂性和参数数量，从而实现了准确建模。此外，由于处理的是更简单的一维函数，KAN可以成为简单且可解释的模型。
- en: '[](/platonic-representation-hypothesis-c812813d7248?source=post_page-----e317b1b4d075--------------------------------)
    [## Platonic Representation: Are AI Deep Network Models Converging?'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/platonic-representation-hypothesis-c812813d7248?source=post_page-----e317b1b4d075--------------------------------)
    [## 柏拉图式表现：人工智能深度网络模型是否趋同？'
- en: Are Artificial Intelligence models evolving towards a unified representation
    of reality? The Platonic Representation…
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 人工智能模型是否正在朝着统一的现实表现形式发展？柏拉图式表现……
- en: towardsdatascience.com](/platonic-representation-hypothesis-c812813d7248?source=post_page-----e317b1b4d075--------------------------------)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/platonic-representation-hypothesis-c812813d7248?source=post_page-----e317b1b4d075--------------------------------)
- en: What Are Kolmogorov–Arnold Networks (KAN)?
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是科尔莫哥洛夫–阿诺德网络（KAN）？
- en: Kolmogorov-Arnold Networks, a.k.a KANs, is a type of neural network architecture
    inspired by the Kolmogorov-Arnold representation theorem. Unlike traditional neural
    networks that use **fixed activation functions**, KANs employ **learnable activation**
    functions on the **edges** of the network. This allows every weight parameter
    in a KAN to be replaced by a univariate function, typically parameterized as a
    **spline**, making them highly flexible and capable of modeling complex functions
    with potentially fewer parameters and enhanced interpretability.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Kolmogorov-Arnold 网络（简称 KAN）是一种受 Kolmogorov-Arnold 表示定理启发的神经网络架构。与使用**固定激活函数**的传统神经网络不同，KAN
    使用**可学习的激活**函数，这些激活函数位于网络的**边缘**。这使得 KAN 中的每个权重参数都可以由一个单变量函数替代，通常参数化为**样条**，从而使其具有高度的灵活性，能够用较少的参数建模复杂的函数，并增强可解释性。
- en: '![](../Images/eb79ec0e7bc0977466df6fa386be4a49.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb79ec0e7bc0977466df6fa386be4a49.png)'
- en: KAN leverages the structure of MLP while benefiting from splines.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: KAN 利用 MLP 的结构，同时受益于样条。
- en: KAN architecture
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KAN 架构
- en: The architecture of Kolmogorov-Arnold Networks (KANs) revolves around a novel
    concept where traditional weight parameters are replaced by univariate function
    parameters on the edges of the network. Each node in a KAN sums up these function
    outputs without applying any nonlinear transformations, in contrast with MLPs
    that include linear transformations followed by nonlinear activation functions.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Kolmogorov-Arnold 网络（KAN）的架构围绕着一个新颖的概念展开，即将传统的权重参数替换为网络边缘上的单变量函数参数。KAN 中的每个节点将这些函数的输出求和，而不应用任何非线性变换，这与包括线性变换后跟非线性激活函数的
    MLP 相对立。
- en: '![](../Images/05f0074e803483d5e451d9601782bd2f.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05f0074e803483d5e451d9601782bd2f.png)'
- en: KAN vs MLP formula.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: KAN 与 MLP 公式对比。
- en: 'B-Splines: The Core of KAN'
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B 样条：KAN 的核心
- en: Surprisingly, one of the most important figures in the paper can be missed easily.
    It’s the description of Splines. Splines are the backbone of KAN’s learning mechanism.
    They replace the traditional weight parameters typically found in neural networks.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 惊人的是，论文中最重要的图形之一竟然容易被忽视。那就是样条的描述。样条是 KAN 学习机制的核心。它们替代了神经网络中通常使用的传统权重参数。
- en: '![](../Images/f50d4ecb8fe73eac16151b6a580098d6.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f50d4ecb8fe73eac16151b6a580098d6.png)'
- en: A detailed view of a spline structure.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 样条结构的详细视图。
- en: The flexibility of splines allows them to adaptively model complex relationships
    in the data by adjusting their shape to minimize approximation error, therefore,
    enhancing the network’s capability to learn subtle patterns from high-dimensional
    datasets.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 样条的灵活性使得它们能够自适应地建模数据中的复杂关系，通过调整其形状以最小化逼近误差，从而增强网络从高维数据集中学习微妙模式的能力。
- en: 'The general formula for a spline in the context of KANs can be expressed using
    B-splines as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在 KAN 上下文中，样条的一般公式可以使用 B 样条表示如下：
- en: '![](../Images/938535a771781f58f3b1159241b5b238.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/938535a771781f58f3b1159241b5b238.png)'
- en: Here, *𝑠pline(𝑥)* represents the spline function. c*i*​ are the coefficients
    that are optimized during training, and 𝐵𝑖(𝑥) are the B-spline basis functions
    defined over a grid. The grid points define the intervals where each basis function
    𝐵𝑖​ is active and significantly affects the **shape** and **smoothness** of the
    spline. You can think of them as a **hyperparameter** that affects the accuracy
    of the network. More grids mean **more control** and **precision**, also resulting
    in more parameters to learn.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*𝑠pline(𝑥)* 代表样条函数。c*i*​ 是在训练过程中优化的系数，𝐵𝑖(𝑥) 是在网格上定义的 B 样条基函数。网格点定义了每个基函数
    𝐵𝑖​ 激活并显著影响样条的**形状**和**平滑度**的区间。你可以把它们看作是影响网络准确度的**超参数**。更多的网格意味着**更多的控制**和**精确度**，同时也意味着更多的参数需要学习。
- en: '![](../Images/693b6a8b4ad8d7a0c14a1fe72aad7ffe.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/693b6a8b4ad8d7a0c14a1fe72aad7ffe.png)'
- en: 'Training a KAN through multiple steps. (source: [GitHub](https://github.com/KindXiaoming/pykan))'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过多个步骤训练一个 KAN。（来源：[GitHub](https://github.com/KindXiaoming/pykan)）
- en: During training, the *ci* parameters of these splines (the coefficients of the
    basis functions *Bi(x)* ) are optimized to **minimize the loss function**, thus
    adjusting the shape of the spline to best fit the training data. This optimization
    often involves techniques like [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent),
    where each iteration updates the spline parameters to reduce prediction error.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，这些样条的*ci*参数（基函数*Bi(x)* 的系数）会被优化，以**最小化损失函数**，从而调整样条的形状，使其最适合训练数据。这个优化通常涉及像[梯度下降](https://en.wikipedia.org/wiki/Gradient_descent)这样的技术，每次迭代更新样条参数，以减少预测误差。
- en: The Best of Two Worlds
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 两全其美
- en: While KAN is based on the **Kolmogorov-Arnold representation theorem,** it is
    just as inspired by MLPs, *“leveraging their respective strengths and avoiding
    their respective weaknesses*”. KAN benefits the structure of MLP on the outside,
    and splines on the inside.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然KAN基于**Kolmogorov-Arnold表示定理**，但它同样受到MLP的启发，*“利用它们各自的优势，避免它们各自的弱点*”。KAN在外部受益于MLP的结构，在内部则受益于样条。
- en: As a result, KANs can not only learn features (thanks to their external similarity
    to MLPs), but can also optimize these learned features to great accuracy (thanks
    to their internal similarity to splines).
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 结果是，KAN不仅能够学习特征（得益于其与MLP的外部相似性），还能够优化这些学习到的特征，达到极高的准确性（得益于其与样条的内部相似性）。
- en: Network Simplification
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络简化
- en: '![](../Images/1a3c844394252f3fc2b48aa6340f1e47.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a3c844394252f3fc2b48aa6340f1e47.png)'
- en: An overview of the network symbolification.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 网络符号化概述。
- en: The paper goes on to explain some methods to simplify the network and our interpretation
    of them. I will only proceed to refer to two of them which were fascinating to
    me.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 论文接着解释了一些简化网络的方法和我们对它们的理解。我将仅讨论其中两种方法，它们让我感到非常有趣。
- en: '**Symbolification**: KAN is constructed by approximating functions using compositions
    of simpler, often interpretable functions. This results in their unique ability
    to hand over **interpretable mathematical formulas**, such as shown in the figure
    above.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**符号化**：KAN是通过使用更简单、通常可以解释的函数的组合来近似构造函数的。这使得它们具有独特的能力，可以输出**可解释的数学公式**，如上图所示。'
- en: '**Pruning**: The other aspect of KANs discussed in the paper is about optimizing
    the network architecture by **removing less important nodes** or connections after
    the network has been trained. This process helps in reducing the complexity and
    size. Pruning focuses on identifying and eliminating those parts of the network
    that contribute minimally to the output. This makes the network lighter and potentially
    more interpretable.'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**剪枝**：论文中讨论的KAN的另一个方面是通过**去除不重要的节点**或连接来优化网络架构，尤其是在网络训练后。这一过程有助于减少复杂性和大小。剪枝的重点是识别并消除对输出贡献最小的部分，从而使网络更轻量化，并有可能变得更加可解释。'
- en: Is KAN New?
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KAN是新的吗？
- en: Kolmogorov-Arnold representation theorem is not new**,** so why has the practice
    of using it in machine learning not been studied before? As the paper explains,
    multiple attempts have been made…
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Kolmogorov-Arnold 表示定理并不新鲜，**那么为什么在机器学习中使用它的实践之前没有被研究过呢？**正如论文所解释的，曾经进行过多次尝试……
- en: However, most work has stuck with the original depth-2 width-(2n + 1) representation,
    and did not have the chance to leverage more modern techniques (e.g., back propagation)
    to train the networks.
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 然而，大多数工作依然停留在原始的深度为2、宽度为(2n + 1)的表示形式，并没有机会利用更现代的技术（例如反向传播）来训练网络。
- en: '![](../Images/e6454d185338beb31db5dc627d53852b.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e6454d185338beb31db5dc627d53852b.png)'
- en: Visual representation of KAN, created by DALLE-3.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: KAN的可视化表示，由DALLE-3创建。
- en: The novelty of the paper is to adapt this idea and apply it to the current landscape
    of ML. Using arbitrary network architecture (depth, width) and employing techniques
    such as backpropagation and pruning, KAN is closer to practical use cases than
    previous studies.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文的创新之处在于将这一理念适应并应用于当前机器学习的领域。通过使用任意的网络架构（深度、宽度）并采用反向传播和剪枝等技术，KAN比以往的研究更接近实际应用场景。
- en: the Kolmogorov-Arnold representation theorem was basically sentenced to death
    in machine learning, regarded as theoretically sound but practically useless.
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Kolmogorov-Arnold 表示定理在机器学习中基本上被宣判为死刑，被认为是理论上健全，但在实践中无用。
- en: So even though there have been attempts to use Kolmogorov-Arnold representation
    theorem in ML, it’s fair to say KAN is a novel approach in that it is aware of
    where ML stands today. It’s a good update to an idea explored before on a limited
    scale.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管已经有尝试将Kolmogorov-Arnold表示定理应用于机器学习，但可以公平地说，KAN是一种新颖的方法，因为它意识到机器学习今天所处的位置。它是对之前在有限规模上探索过的一个想法的很好更新。
- en: 4 Fascinating Examples
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 个迷人的例子
- en: The paper compares KAN and MLP on several criteria, most of which are gripping.
    In this part, I will proceed to list some of these interesting examples. The full
    details of these examples and more are in the paper.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 论文对KAN和MLP在几个标准上的比较，其中大多数都很吸引人。在这一部分，我将列出一些这些有趣的例子。有关这些例子以及更多详细内容，请参见论文。
- en: Fitting Symbolic Formulas
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合适的符号公式
- en: This is an example of training various MLPs and a KAN to fit certain functions
    of various input dimensions. As can be seen below, KAN has much better **scalability**
    compared to MLPs (at least in this range of parameter numbers).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这是训练各种MLP和KAN以拟合不同输入维度的某些函数的示例。从下图可以看出，与MLP相比，KAN在**可扩展性**方面有明显优势（至少在这个参数范围内）。
- en: 'This highlights the greater expressive power of deeper KANs, which is the same
    for MLPs: deeper MLPs have more expressive power than shallower ones.'
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这突显了更深层KAN的更强表达能力，这对于MLP也是一样的：更深的MLP比浅层的有更强的表达能力。
- en: '![](../Images/57e7939b3efb8c53c9e88b8046404e73.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57e7939b3efb8c53c9e88b8046404e73.png)'
- en: Special Functions
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特殊函数
- en: Another example in the paper is to compare KAN and MLP on fitting 15 special
    functions common in math and physics. The result shows that in almost all of these
    functions, KANs achieve a **lower train/test loss** having the same number of
    parameters compared to MLPs.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 论文中的另一个例子是比较KAN和MLP在拟合数学和物理中常见的15个特殊函数上的表现。结果表明，在几乎所有这些函数中，KAN在拥有相同参数数量的情况下，比MLP具有**更低的训练/测试损失**。
- en: '![](../Images/619c878fede4afa604223b5789ec1fae.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/619c878fede4afa604223b5789ec1fae.png)'
- en: Continual Learning
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 持续学习
- en: Continual Learning is the quality of how networks can adapt to new information
    over time without forgetting previously learned knowledge. It is a significant
    challenge in neural network training, particularly in avoiding the problem of
    **catastrophic forgetting**, where acquiring new knowledge leads to a rapid erosion
    of previously established information.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 持续学习是指网络在时间推移中如何适应新信息而不忘记先前学到的知识。这是神经网络训练中的一大挑战，尤其是在避免**灾难性遗忘**的问题上，当获取新知识时，可能会迅速侵蚀先前建立的信息。
- en: KANs demonstrate an ability to retain learned information and adapt to new data
    without catastrophic forgetting, thanks to the **local nature** of spline functions.
    Unlike MLPs, which rely on global activations that might unintentionally affect
    distant parts of the model, KANs modify only a limited set of nearby spline coefficients
    with each new sample. This focused adjustment preserves previously stored information
    in other parts of the spline.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: KAN展示了保持已学信息并适应新数据的能力，而不会发生灾难性遗忘，这得益于样条函数的**局部性质**。与依赖全局激活（可能无意中影响模型远离部分）的MLP不同，KAN每次新样本仅修改有限的相邻样条系数。这种集中调整保护了样条中其他部分已存储的信息。
- en: '![](../Images/ba652b6aafcae45641dc64437587e99a.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba652b6aafcae45641dc64437587e99a.png)'
- en: Partial Differential Equation solving
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 偏微分方程求解
- en: PDE solving, a 2-Layer width-10 KAN is 100 times more accurate than a 4-Layer
    width-100 MLP (10−7 vs 10−5 MSE) and 100 times more parameter efficient (102 vs
    104 parameters).
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 偏微分方程求解，一个2层宽度为10的KAN比一个4层宽度为100的MLP准确度高100倍（10⁻⁷与10⁻⁵均方误差）且在参数效率上高100倍（102与104个参数）。
- en: '![](../Images/cce86fd54606e56cdebf1de0b04b82a9.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cce86fd54606e56cdebf1de0b04b82a9.png)'
- en: The paper continues to present more experiments. One of them includes applying
    KANs to the problem of [**geometric knot invariant**](https://en.wikipedia.org/wiki/Knot_invariant#:~:text=A%20knot%20invariant%20is%20a%20quantity%20defined%20on%20the%20set,quantity%20defined%20on%20knot%20diagrams.),
    achieving **81.6%** test accuracy with a **200**-parameter KAN, while an MLP model
    by Google Deepmind achieves **78%** having **~ 3 * 10⁵** parameters. This experiment
    was performed on the
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 论文继续展示更多实验。其中之一是将KAN应用于[**几何结不变量**](https://en.wikipedia.org/wiki/Knot_invariant#:~:text=A%20knot%20invariant%20is%20a%20quantity%20defined%20on%20the%20set,quantity%20defined%20on%20knot%20diagrams.)问题，使用一个**200**参数的KAN实现了**81.6%**的测试准确率，而谷歌DeepMind的MLP模型在**~
    3 * 10⁵**个参数下取得了**78%**的准确率。这个实验是在
- en: Final Thoughts
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终思考
- en: Is the hype over KAN worth it?
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KAN的炒作值得吗？
- en: It depends on your perspective. The reason KAN is being discussed so far is
    that it’s a potential light at the end of the ML tunnel. I have discussed in [**“AI
    Is Hitting A Hard Ceiling It Can’t Pass”**](https://www.linkedin.com/feed/update/urn:li:activity:7192221468741582848/)how
    we need fresh innovations to guide us through the future barriers of Machine Learning,
    namely **Data** and **Computation**. KANs, though not intentionally, can be a
    way out.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这取决于你的视角。之所以目前在讨论 KAN，是因为它可能是机器学习（ML）隧道尽头的一线曙光。我在[**“人工智能遇到无法突破的瓶颈”**](https://www.linkedin.com/feed/update/urn:li:activity:7192221468741582848/)中讨论了我们如何需要新的创新来引导我们突破机器学习的未来障碍，即**数据**和**计算**。虽然不是有意为之，KAN
    可能是走出去的一个途径。
- en: KAN is written with scientific applications of AI in mind, but already people
    are using it to mix various ML cocktails, including Multihead Attention.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: KAN 是以人工智能的科学应用为设计初衷的，但现在人们已经开始将它用于混合各种机器学习（ML）模型，包括多头注意力（Multihead Attention）。
- en: '***UPDATE: You can read about my experiment of training KAN on the MNIST dataset
    to test it out on computer vision tasks, don’t miss it 👇***'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '***更新：你可以阅读我在 MNIST 数据集上训练 KAN 的实验，以测试它在计算机视觉任务中的表现，不要错过👇***'
- en: KAN + LLM?
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KAN + LLM？
- en: The paper focuses mainly on the **AI + Science** applications of Kolmogorov-Arnold
    Networks due to their ability to model and discover complex scientific laws and
    patterns effectively. KANs are particularly suited for tasks that require the
    **understanding** and **interpreting** of underlying physical principles, as their
    structure allows for the decomposition of functions into symbolic mathematical
    expressions. This makes them ideal for scientific research where discovering such
    relationships is crucial, unlike in large language models (LLMs) where the primary
    goal often revolves around processing a mammoth corpus of data for natural language
    understanding and generation.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 论文主要集中在 Kolmogorov-Arnold 网络（KAN）在**人工智能 + 科学**应用方面的潜力，因为它们能够有效地建模和发现复杂的科学规律和模式。KAN
    特别适用于需要**理解**和**解释**基础物理原理的任务，因为它们的结构允许将函数分解成符号化的数学表达式。这使得它们非常适合科学研究，在科学研究中发现这种关系至关重要，而不像大规模语言模型（LLM），后者的主要目标通常是处理庞大的数据集，以进行自然语言理解和生成。
- en: Author’s Notes
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 作者注释
- en: I encourage you to also read the [author’s notes](https://github.com/KindXiaoming/pykan?tab=readme-ov-file#authors-note)
    on the GitHub page. It provides perspective on what KAN was aimed for, and what
    could be in the future.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我鼓励你也阅读 GitHub 页面上的[作者注释](https://github.com/KindXiaoming/pykan?tab=readme-ov-file#authors-note)。它提供了对
    KAN 初衷的视角，以及未来可能的发展。
- en: The most common question I’ve been asked lately is whether KANs will be next-gen
    LLMs. I don’t have good intuition about this. KANs are designed for applications
    where one cares about high accuracy and/or interpretability. We do care about
    LLM interpretability for sure, but interpretability can mean wildly different
    things for LLM and for science. Do we care about high accuracy for LLMs? I don’t
    know, scaling laws seem to imply so, but probably not too high precision. Also,
    accuracy can also mean different things for LLM and for science. This subtlety
    makes it hard to directly transfer conclusions in our paper to LLMs, or machine
    learning tasks in general.
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 最近我被问到最多的问题是 KAN 是否会成为下一代 LLM。我对此没有很好的直觉。KAN 是为那些注重高准确性和/或可解释性的应用设计的。我们当然关心
    LLM 的可解释性，但可解释性对于 LLM 和科学来说可能意味着截然不同的事情。我们是否关心 LLM 的高准确性？我不知道，缩放法则似乎表明是这样，但可能并不需要非常高的精度。另外，准确性对
    LLM 和科学来说也有不同的意义。这种微妙的差异使得我们很难将论文中的结论直接应用于 LLM 或一般的机器学习任务。
- en: '![](../Images/750b45b075c953728f58ab30c1915355.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/750b45b075c953728f58ab30c1915355.png)'
- en: '“KAN: Kolmogorov–Arnold Networks” paper.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '“KAN: Kolmogorov–Arnold 网络”论文。'
- en: Conclusion
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: In my view, it’s best to look at KAN for what it is, rather than what we like
    it to be. This doesn’t mean KAN is **impossible** to be integrated within LLMs,
    already there is an efficient [**PyTorch implementation of KAN**](https://github.com/Blealtan/efficient-kan).
    But it has to be noted, it is too soon to call KAN *revolutionary* or *game-changing*.
    KAN simply needs more experiments done by community experts.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在我看来，最好是从 KAN 本身的角度来看待它，而不是从我们希望它成为的角度来看。并不意味着 KAN **不可能**与大规模语言模型（LLM）集成，实际上已经有一个高效的[**PyTorch
    实现 KAN**](https://github.com/Blealtan/efficient-kan)。但需要指出的是，现在称 KAN 为 *革命性* 或
    *改变游戏规则* 还为时过早。KAN 仍然需要社区专家进行更多的实验。
- en: 'While KANs offer significant advantages in certain contexts, they come with
    limitations and considerations that beget caution:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 KAN 在某些特定情境下提供了显著的优势，但它也有一些限制和考虑因素，需要谨慎对待：
- en: '**Complexity and Overfitting**: KANs can potentially overfit, especially in
    scenarios with limited data. Their ability to form complex models might capture
    noise as significant patterns, leading to poor generalization.'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**复杂性与过拟合：** KANs可能会发生过拟合，尤其是在数据有限的情况下。它们构建复杂模型的能力可能会将噪声当作重要模式捕捉，导致泛化能力差。'
- en: '**Computation:** KANs may face challenges with GPU optimization due to their
    specialized nature, which can disrupt parallel processing. This architecture could
    result in slower operations on GPUs, necessitating serialization and leading to
    inefficient memory utilization, potentially making CPUs a more suitable platform
    for these networks.'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**计算：** 由于KAN的专业化特性，它们可能面临GPU优化的挑战，这可能会破坏并行处理。这种架构可能会导致GPU上的操作变慢，迫使进行序列化，导致内存使用效率低下，从而可能使得CPU成为这些网络的更合适平台。'
- en: '**Applicability:** KANs are primarily designed for scientific and engineering
    tasks where understanding the underlying function is crucial. They might not be
    as effective in domains requiring large-scale pattern recognition or classification,
    such as image recognition or natural language processing, where simpler or more
    abstract models might suffice.'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**适用性：** KANs主要设计用于科学和工程任务，在这些任务中，理解潜在功能至关重要。它们可能在需要大规模模式识别或分类的领域（例如图像识别或自然语言处理）中效果不如那些更简单或更抽象的模型。'
- en: I have to add this has been an amazing paper to read. It’s always exciting to
    think outside the box and KAN certainly achieves that.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我必须补充，这篇论文真的很棒，读起来非常令人兴奋。跳出框框思考总是很有趣，而KAN无疑做到了这一点。
- en: 💬 How do you see the potential of KAN? Is it going to niche down to science,
    or play a key role in our daily AI products?
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 💬 你如何看待KAN的潜力？它会专注于科学领域，还是将在我们的日常AI产品中发挥重要作用？
- en: '**🌟 Join +1000 people learning about**'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**🌟 加入1000+人一起学习**'
- en: Python🐍, ML/MLOps/AI🤖, Data Science📈, and LLM 🗯
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Python🐍，机器学习/MLOps/人工智能🤖，数据科学📈，以及LLM 🗯
- en: '[**follow me**](https://medium.com/@itshesamsheikh/subscribe)and check out
    my [**X/Twitter**](https://twitter.com/itsHesamSheikh), where I keep you updated
    **Daily**:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[**关注我**](https://medium.com/@itshesamsheikh/subscribe)并查看我的[**X/Twitter**](https://twitter.com/itsHesamSheikh)，在这里我每天为你更新：'
- en: Thanks for reading,
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读，
- en: — Hesam
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: — Hesam
