- en: Predicting metadata for humanitarian datasets with LLMs part 2 — An alternative
    to fine-tuning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LLM预测人道主义数据集的元数据第二部分——微调的替代方法
- en: 原文：[https://towardsdatascience.com/predicting-metadata-for-humanitarian-datasets-with-llms-part-2-an-alternative-to-fine-tuning-953a49c657cf?source=collection_archive---------5-----------------------#2024-08-03](https://towardsdatascience.com/predicting-metadata-for-humanitarian-datasets-with-llms-part-2-an-alternative-to-fine-tuning-953a49c657cf?source=collection_archive---------5-----------------------#2024-08-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/predicting-metadata-for-humanitarian-datasets-with-llms-part-2-an-alternative-to-fine-tuning-953a49c657cf?source=collection_archive---------5-----------------------#2024-08-03](https://towardsdatascience.com/predicting-metadata-for-humanitarian-datasets-with-llms-part-2-an-alternative-to-fine-tuning-953a49c657cf?source=collection_archive---------5-----------------------#2024-08-03)
- en: '[](https://medium.com/@astrobagel?source=post_page---byline--953a49c657cf--------------------------------)[![Matthew
    Harris](../Images/4fa3264bb8a028633cd8d37093c16214.png)](https://medium.com/@astrobagel?source=post_page---byline--953a49c657cf--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--953a49c657cf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--953a49c657cf--------------------------------)
    [Matthew Harris](https://medium.com/@astrobagel?source=post_page---byline--953a49c657cf--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@astrobagel?source=post_page---byline--953a49c657cf--------------------------------)[![Matthew
    Harris](../Images/4fa3264bb8a028633cd8d37093c16214.png)](https://medium.com/@astrobagel?source=post_page---byline--953a49c657cf--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--953a49c657cf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--953a49c657cf--------------------------------)
    [Matthew Harris](https://medium.com/@astrobagel?source=post_page---byline--953a49c657cf--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--953a49c657cf--------------------------------)
    ·29 min read·Aug 3, 2024
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--953a49c657cf--------------------------------)
    ·29分钟阅读·2024年8月3日
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/0002f919edfe77b2a945716650c673f7.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0002f919edfe77b2a945716650c673f7.png)'
- en: 'Source: GPT-4o'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：GPT-4o
- en: TL;DR
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: TL;DR
- en: '*In the humanitarian response world there can be tens of thousands of tabular
    (CSV and Excel) datasets, many of which contain critical information for helping
    save lives. Data can be provided by hundreds of different organizations with different
    naming conventions, languages and data standards, so having information (metadata)
    about what each column represents in tables is important for finding the right
    data and understanding how it fits together. Much of this metadata is set manually,
    which is time-consuming and error prone, so any automatic method can have a real
    effect towards helping people. In this article we revisit a previous analysis
    “*[*Predicting Metadata of Humanitarian Datasets with GPT 3*](https://medium.com/towards-data-science/predicting-metadata-for-humanitarian-datasets-using-gpt-3-b104be17716d)*”
    to see how advances in the last 18 months open the way for more efficient and
    less time-consuming methods for setting metadata on tabular data.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*在人道主义响应领域，可能会有成千上万的表格数据集（CSV和Excel），其中许多包含拯救生命的关键信息。数据可能由数百个不同的组织提供，且命名约定、语言和数据标准各异，因此了解表格中每一列的含义（元数据）对于找到合适的数据并理解其如何组合至关重要。大部分元数据是手动设置的，这既耗时又容易出错，因此任何自动化方法都可能在帮助人们方面产生实际影响。在本文中，我们重新审视了先前的分析“*[*使用GPT-3预测人道主义数据集的元数据*](https://medium.com/towards-data-science/predicting-metadata-for-humanitarian-datasets-using-gpt-3-b104be17716d)*”，以了解过去18个月的进展如何为更高效、节省时间的方法铺平道路，用于设置表格数据的元数据。*”'
- en: '*Using metadata-tagged CSV and Excel datasets from the* [*Humanitarian Data
    Exchange*](https://data.humdata.org/) *(HDX) we show that fine-tuning GPT-4o-mini
    works well for predicting* [*Humanitarian Exchange Language*](https://hxlstandard.org/)
    *(HXL) tags and attributes for the most common tags related to location and dates.
    However, for less well-represented tags and attributes the technique can be a
    bit limited due to poor quality training data where humans have made mistakes
    in manually labelling data or simply aren’t using all possible HXL metadata combinations.
    It also has the limitation of not being able to adjust when the metadata standard
    changes, since the training data would not reflect those changes.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*通过使用带有元数据标签的CSV和Excel数据集，来自于* [*人道主义数据交换平台*](https://data.humdata.org/) *(HDX)，我们展示了微调GPT-4o-mini在预测*
    [*人道主义交换语言*](https://hxlstandard.org/) *(HXL)标签和属性时的良好效果，尤其是对于与位置和日期相关的最常见标签。然而，对于那些较少出现的标签和属性，这一技术可能会受到训练数据质量不佳的限制，原因在于人工标签错误或人们没有使用所有可能的HXL元数据组合。它还存在一个限制，即当元数据标准发生变化时，它无法进行调整，因为训练数据不会反映这些变化。*'
- en: '*Given more powerful LLMs are now available, we tested a technique to directly
    prompt GPT-4o or GPT-4o-mini rather than fine-tuning, providing the full HXL core
    schema definition in the system prompt now that larger context windows are available.
    This approach was shown to be more accurate than fine-tuning when using GPT-4o,
    able to support rarer HXL tags and attributes and requiring no custom training
    data, making it easier to manage and deploy. It is however more expensive, but
    not if using GPT-4o-mini, albeit with a slight decrease in performance. Using
    this approach we provide a simple Python class in a* [*GitHub Gist*](https://gist.github.com/dividor/e693997c1fc7e0d94f8228cebc397014)
    *that can be used in data processing pipelines to automatically add HXL metadata
    tags and attributes to tabular datasets.*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*鉴于现在有了更强大的LLM可用，我们测试了一种技术，直接提示GPT-4o或GPT-4o-mini，而不是进行微调，在系统提示中提供完整的HXL核心架构定义，因为现在可以使用更大的上下文窗口。事实证明，当使用GPT-4o时，这种方法比微调更准确，能够支持较少见的HXL标签和属性，并且不需要定制的训练数据，使得管理和部署更加简便。然而，它的成本较高，但如果使用GPT-4o-mini，则成本较低，尽管性能略有下降。通过这种方法，我们提供了一个简单的Python类，位于*
    [*GitHub Gist*](https://gist.github.com/dividor/e693997c1fc7e0d94f8228cebc397014)
    *，可以在数据处理管道中自动为表格数据集添加HXL元数据标签和属性。*'
- en: Generative AI moves fast!
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成性AI发展得**非常**迅速！
- en: About 18 months ago I wrote a blog post [Predicting Metadata of Humanitarian
    Datasets with GPT 3](https://medium.com/towards-data-science/predicting-metadata-for-humanitarian-datasets-using-gpt-3-b104be17716d).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大约18个月前，我写了一篇博客文章 [使用GPT-3预测人道主义数据集的元数据](https://medium.com/towards-data-science/predicting-metadata-for-humanitarian-datasets-using-gpt-3-b104be17716d)。
- en: That’s right, with GPT 3, not even 3.5! 🙂
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 没错，是使用GPT-3，不是GPT-3.5！🙂
- en: Even so, back then Large Language Model (LLM) fine-tuning produced great performance
    for predicting [Humanitarian Exchange Language](https://hxlstandard.org/) (HXL)
    metadata fields for tabular datasets on the amazing [Humanitarian Data Exchange](https://data.humdata.org/)
    (HDX). In that study, the training data represented the distribution of HXL data
    on HDX and so was comprised of the most common tags relating to location and dates.
    These are very important for linking different datasets together in location and
    time, a crucial factor in using data to optimize humanitarian response.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 即便如此，早在那时，大型语言模型（LLM）的微调就已在预测[人道主义交换语言](https://hxlstandard.org/)（HXL）元数据字段方面表现出色，尤其是在令人惊叹的[人道主义数据交换平台](https://data.humdata.org/)（HDX）上的表格数据集。在那项研究中，训练数据代表了HDX上的HXL数据分布，因此包含了与位置和日期相关的最常见标签。这些标签对于将不同数据集按位置和时间关联起来至关重要，这是利用数据优化人道主义响应的一个关键因素。
- en: The LLM field has since advanced … a *LOT*.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: LLM领域此后已取得了… **巨大的**进展。
- en: So in this article, we will revisit the technique, expand it to cover less frequent
    HXL tags and attributes and explore other options now available to us for situations
    where a complex, high-cardinality taxonomy needs to be applied to data. We will
    also explore the ability to predict less frequent HXL standard tags and attributes
    not currently represented in the human-labeled training data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本文中，我们将重新审视这一技术，将其扩展到涵盖不太常见的HXL标签和属性，并探讨目前可用的其他选项，适用于需要将复杂、高层次的分类法应用于数据的情况。我们还将探讨预测当前在人为标注的训练数据中未包含的较少见的HXL标准标签和属性的能力。
- en: Setup
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置
- en: 'You can follow along with this analysis by opening these notebooks in [Google
    Colab](https://colab.research.google.com/) or running them locally:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过在[Google Colab](https://colab.research.google.com/)中打开这些笔记本，或者在本地运行它们来跟随本次分析：
- en: '[generate-test-train-data.ipynb](https://github.com/datakind/hxl-metadata-prediction/blob/main/generate-test-train-data.ipynb)
    — A notebook for creating test and training datasets'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[generate-test-train-data.ipynb](https://github.com/datakind/hxl-metadata-prediction/blob/main/generate-test-train-data.ipynb)
    — 用于创建测试和训练数据集的笔记本'
- en: '[openai-hxl-prediction.ipynb](https://github.com/datakind/hxl-metadata-prediction/blob/main/openai-hxl-prediction.ipynb)
    — Notebook exploring fine-tuning and prompting for predicting HXL datasets'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[openai-hxl-prediction.ipynb](https://github.com/datakind/hxl-metadata-prediction/blob/main/openai-hxl-prediction.ipynb)
    — 探索微调和提示以预测HXL数据集的笔记本'
- en: Please refer to the [README](https://github.com/datakind/hxl-metadata-prediction/blob/main/README.md)
    in the repo for installation instructions.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅仓库中的[README](https://github.com/datakind/hxl-metadata-prediction/blob/main/README.md)获取安装说明。
- en: HXL Data from the Humanitarian Data Exchange
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 来自人道主义数据交换平台的HXL数据
- en: For this study, and with help from the HDX team, we will use data extracted
    from the HDX platform using a crawler process they run to track the use of HXL
    metadata tags and attributes on the platform. You can find great HXL resources
    on [GitHub](https://github.com/HXLStandard), but if you want to follow along with
    this analysis I have also saved the source data on Google Drive as the crawler
    will take days to process the hundreds of thousands of tabular datasets on HDX.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本研究，在HDX团队的帮助下，我们将使用从HDX平台提取的数据，通过他们运行的爬虫过程跟踪平台上HXL元数据标签和属性的使用情况。你可以在[GitHub](https://github.com/HXLStandard)找到很棒的HXL资源，但如果你想跟随本次分析，我也将源数据保存到了Google
    Drive，因为爬虫需要几天时间才能处理HDX上成千上万的表格数据集。
- en: The data looks like this, with one row per HXL-tagged table column …
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 数据如下所示，每个HXL标签化的表格列为一行…
- en: '![](../Images/1d1abdc669b37dc43cb697223ff9d3f7.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d1abdc669b37dc43cb697223ff9d3f7.png)'
- en: Example of data used in this study, with a row per tabular data column.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究中使用的数据示例，每行代表一个表格数据列。
- en: The core HXL Schema
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 核心HXL架构
- en: The [HXL postcard](https://hxlstandard.org/standard/1-1final/postcards/) is
    a really great overview of the most common HXL tags and attributes in the core
    schema. For our analysis, we will apply the full standard as found on [HDX](https://data.humdata.org/dataset/hxl-core-schemas)
    which provides a [spreadsheet](https://docs.google.com/spreadsheets/d/1En9FlmM8PrbTWgl3UHPF_MXnJ6ziVZFhBbojSJzBdLI/edit?usp=sharing)
    of supported tags and attributes …
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[HXL明信片](https://hxlstandard.org/standard/1-1final/postcards/)是一个非常好的概述，展示了核心架构中最常见的HXL标签和属性。对于我们的分析，我们将应用[HDX](https://data.humdata.org/dataset/hxl-core-schemas)上提供的完整标准，并提供了一个支持的标签和属性的[电子表格](https://docs.google.com/spreadsheets/d/1En9FlmM8PrbTWgl3UHPF_MXnJ6ziVZFhBbojSJzBdLI/edit?usp=sharing)…'
- en: '![](../Images/c72d8b2f6bce80bae6e654de56ba7af0.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c72d8b2f6bce80bae6e654de56ba7af0.png)'
- en: Excerpt of the “Core HXL Schema” used for this study, as found on the [Humanitarian
    Data Exchange](https://data.humdata.org/dataset/hxl-core-schemas)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究中使用的“核心HXL架构”摘录，来源于[Humanitarian Data Exchange](https://data.humdata.org/dataset/hxl-core-schemas)
- en: Data Processing
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据处理
- en: 'The [generate-test-train-data.ipynb](https://github.com/datakind/hxl-metadata-prediction/blob/main/generate-test-train-data.ipynb)
    notebook provides all the steps taken to create test and training datasets, but
    here are some key points to note:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[generate-test-train-data.ipynb](https://github.com/datakind/hxl-metadata-prediction/blob/main/generate-test-train-data.ipynb)笔记本提供了创建测试和训练数据集的所有步骤，但这里有一些要注意的关键点：'
- en: '**1\. Removal of automatic pipeline repeat HXL data**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 删除自动化管道重复的HXL数据**'
- en: In this study, I removed duplicate data created by automated pipelines that
    upload data to HDX, by using an MDF hash of column names in each tabular dataset
    (CSV and Excel files). For example, a CSV file of population statistics created
    by an organization is often very similar for each country-specific CSV or Excel
    file, so we only take one example. This has a balancing effect on the data, providing
    more variation of HXL tags and attributes by removing very similar repeat data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我通过使用每个表格数据集（CSV和Excel文件）中列名称的MDF哈希，删除由自动化管道上传到HDX的数据中的重复项。例如，某个组织创建的人口统计CSV文件通常与每个特定国家的CSV或Excel文件非常相似，因此我们只保留一个示例。这对数据起到了平衡作用，通过删除非常相似的重复数据，提供了更多的HXL标签和属性变异性。
- en: '**2\. Constraining data to valid HXL**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 限制数据为有效的HXL格式**'
- en: About 50% of the HDX data with HXL tags uses a tag or attribute which are not
    specified in the [HXL Core Schema](https://docs.google.com/spreadsheets/d/1En9FlmM8PrbTWgl3UHPF_MXnJ6ziVZFhBbojSJzBdLI/edit?usp=sharing),
    so this data is removed from training and test sets.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 大约 50% 的带有 HXL 标签的 HDX 数据使用了在[HXL 核心架构](https://docs.google.com/spreadsheets/d/1En9FlmM8PrbTWgl3UHPF_MXnJ6ziVZFhBbojSJzBdLI/edit?usp=sharing)中未指定的标签或属性，因此这些数据会被从训练集和测试集中移除。
- en: '**3\. Data enrichment**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. 数据增强**'
- en: As a (mostly!) human being, when deciding what HXL tags and attributes to use
    on a column, I take a peek at the data for that column and also the data as a
    whole in the table. For this analysis we do the same for the LLM fine-tuning and
    prompt data, adding in data excerpts for each column. A table description is also
    added using an LLM (GPT-3.5-Turbo) summary of the data to make them consistent,
    as summaries on HDX can vary in form, ranging from pages to a few words.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个（大部分是！）人类，在决定在某一列使用哪些 HXL 标签和属性时，我会查看该列的数据，也会查看表格中所有数据。对于这个分析，我们也对 LLM 微调和提示数据做同样的处理，添加每一列的数据摘录。还使用
    LLM（GPT-3.5-Turbo）对数据的摘要来为表格添加描述，使它们一致，因为 HDX 上的摘要格式各不相同，可能是几页，也可能是几句话。
- en: '**4\. Carefully splitting data to create train/test sets**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**4\. 仔细划分数据以创建训练/测试集**'
- en: Many machine learning pipelines split data randomly to create training and test
    sets. However, for HDX data this would result in columns and files from the same
    organization being in train and test. I felt this was a bit too easy for testing
    predictions and so instead split the data by organizations to ensure organizations
    in the test set were not in the training data. Additionally, subsidiaries of the
    same parent organization — eg “ocha-iraq” and “ocha-libya” — were not allowed
    to be in both the training and test sets, again to make the predictions more realistic.
    My aim was to test prediction with organizations as if their data had never been
    seen before.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习管道通过随机划分数据来创建训练集和测试集。然而，对于 HDX 数据，这样做会导致来自同一组织的列和文件出现在训练集和测试集中。我认为这种方式对预测测试来说有点太简单了，因此我选择按组织划分数据，确保测试集中的组织不出现在训练数据中。此外，同一母公司下的子公司——例如“ocha-iraq”和“ocha-libya”——也不能同时出现在训练集和测试集中，以使预测更加真实。我的目标是测试预测，假设这些组织的数据从未被见过。
- en: After all of the above and down-sampling to save costs, we are left with **2,883**
    rows in the training set and **485** rows in the test set.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成上述所有步骤并进行降采样以节省成本后，我们得到了**2,883**行训练集数据和**485**行测试集数据。
- en: Creating JSONL fine-tuning prompt files
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建 JSONL 微调提示文件
- en: In my original article I opted for using a completion model, but with the release
    of [GPT-4o-mini](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)
    I instead generated prompts appropriate for fine-tuning a *chat* model (see [here](https://platform.openai.com/docs/guides/fine-tuning/which-models-can-be-fine-tuned)
    for more information about the available models).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在我原来的文章中，我选择使用一个完成模型，但随着[GPT-4o-mini](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)的发布，我改为生成适合微调*聊天*模型的提示（有关可用模型的更多信息，请参见[此处](https://platform.openai.com/docs/guides/fine-tuning/which-models-can-be-fine-tuned)）。
- en: Each prompt has the form …
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 每个提示的格式是…
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Note: The above has been formatted for clarity, but JSONL will have everything
    in one line per record.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：上述内容已格式化以便清晰阅读，但 JSONL 中每条记录会在一行内显示。
- en: Using the data excerpts, LLM_generated table description, column name we collated,
    we can now generate prompts which look like this …
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 利用数据摘录、LLM 生成的表格描述以及我们收集的列名，我们现在可以生成类似于这样的提示…
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Fine-tuning GPT-4o-mini
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调 GPT-4o-mini
- en: We now have test and training files in the right format for fine-tuning an OpenAI
    chat model, so let’s tune our model …
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在拥有了适合微调 OpenAI 聊天模型的测试和训练文件格式，因此让我们开始微调我们的模型…
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the above we are using the new [GPT-4-mini model](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/),
    which from OpenAI is currently free to fine-tune …
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面，我们使用的是新的[GPT-4-mini 模型](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)，该模型目前由
    OpenAI 提供免费微调服务…
- en: “Now through September 23, GPT-4o mini is free to fine-tune up to a daily limit
    of 2M training tokens. Overages over 2M training tokens will be charged at $3.00/1M
    tokens. Starting September 24, fine-tuning training will cost $3.00/1M tokens.
    Check out the [fine-tuning docs](http://url3243.email.openai.com/ls/click?upn=u001.IQLfsj4kk-2BK7JhymNusRMkuuWNTB2xtKMTOzsaHXXCxL87wc9xXN3T3-2B7A50MnxBgM-2FSPU6KI18qmN7e0qEq7w-3D-3DYSY8_HWAk4DGcP5bOseprwmP7vlMwrd2PVXgyuPjLpW3O5VwKbv89B-2BC2CHyio6JopT7iV9PDDQbS-2BN2x-2FOMYyECPpE2WpDWUaqXamxCNxLNFb3Rwb-2BHV-2FnmELwjcwafGYmpXvFXZ3a1UDAGj-2FI8RPRJ92m05wFP91cNzwWmQw2EWFsPrLyLakbHisdbOdu-2B4S0ScKBkmbmuJc7Ib-2Ftz7vKHoD5rdIHoytDF68pW1ivyzpO5isDzndxqHjHSEoXNrAMaOs0RnmRsG-2Btwq2onQS1WmIokXr00y08IHtcHQMGB8k2caZ5qZ1FzXlQ7tM-2F42kCwNCt4-2BmFy-2Bt8mm9-2BtTS6Qd9pEf9tpuFFcI14VFgdiiUINrbkZX-2BvxRqD924FparfXWICjMx3q6U3F78-2B0okeN23HKQddDiZ9ufm5tITBwbvTYG4vXxKkrvM1fg-2BY-2FSI1Zgu7AMY95FNOKhHZjjVYIXSEFJh5oN0U3K3ceVerRfgU0o1sp8yLH-2F4yaMjmyNjp9gAL5CiSYfTqIx0hHAETq3DyTWqiJMx5Fpsg8sAiqHj3Dgwqj5hydZgeMopCnrf3Cfo7Uf09kxixficprhjJLtC-2BOYDB9QH3AyxBxKCpKupl026DU1bx7HoE0Rcytak3Zy6lolc6PczWAxmgGmi8bkEWsMxj8VS-2BhSSPF7qHIr0a-2BP020bgEng-2BZL0HUgfiJpig0i4DhENBp-2BQokwZMcgMdFpOhJVou0cF-2BcxDprFi2U2xhrxn5es5vY0TTwpQjqAhs-2BoK-2FZpbE0zkuyQ9tTtlInaU26DOBv1RHaiFTN-2F8GTEHoxvkJ1OHhhds3ATTWUCGwOhUOZ-2Fl5JjWzYdCDPeOgqnxlQd8b1i-2BJuaBRnhUjpQ7TzPnWkCur4qMtI-2BYKM3tD2d0RxTYTYfQ3GoNsZ-2FBo5Mf4Rb3lKQt59vxsLqKYe33qRjeFo12Ke3dS20gxD7Zxtpu57q1z0xuMgwj9uDDqrPTZh9qbUDYGc1IsbRhOAjL5z4kAYR2jGvTi2SFq9f2AiA1swOO3CORlZpwn5Y6BA-3D-3D)
    for more details on free access.”
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “现在到9月23日，GPT-4o mini可以免费调优，最多达到每日2M训练令牌的限制。超过2M训练令牌的部分将按$3.00/百万令牌收费。从9月24日起，调优训练将收费$3.00/百万令牌。查看[调优文档](http://url3243.email.openai.com/ls/click?upn=u001.IQLfsj4kk-2BK7JhymNusRMkuuWNTB2xtKMTOzsaHXXCxL87wc9xXN3T3-2B7A50MnxBgM-2FSPU6KI18qmN7e0qEq7w-3D-3DYSY8_HWAk4DGcP5bOseprwmP7vlMwrd2PVXgyuPjLpW3O5VwKbv89B-2BC2CHyio6JopT7iV9PDDQbS-2BN2x-2FOMYyECPpE2WpDWUaqXamxCNxLNFb3Rwb-2BHV-2FnmELwjcwafGYmpXvFXZ3a1UDAGj-2FI8RPRJ92m05wFP91cNzwWmQw2EWFsPrLyLakbHisdbOdu-2B4S0ScKBkmbmuJc7Ib-2Ftz7vKHoD5rdIHoytDF68pW1ivyzpO5isDzndxqHjHSEoXNrAMaOs0RnmRsG-2Btwq2onQS1WmIokXr00y08IHtcHQMGB8k2caZ5qZ1FzXlQ7tM-2F42kCwNCt4-2BmFy-2Bt8mm9-2BtTS6Qd9pEf9tpuFFcI14VFgdiiUINrbkZX-2BvxRqD924FparfXWICjMx3q6U3F78-2B0okeN23HKQddDiZ9ufm5tITBwbvTYG4vXxKkrvM1fg-2BY-2FSI1Zgu7AMY95FNOKhHZjjVYIXSEFJh5oN0U3K3ceVerRfgU0o1sp8yLH-2F4yaMjmyNjp9gAL5CiSYfTqIx0hHAETq3DyTWqiJMx5Fpsg8sAiqHj3Dgwqj5hydZgeMopCnrf3Cfo7Uf09kxixficprhjJLtC-2BOYDB9QH3AyxBxKCpKupl026DU1bx7HoE0Rcytak3Zy6lolc6PczWAxmgGmi8bkEWsMxj8VS-2BhSSPF7qHIr0a-2BP020bgEng-2BZL0HUgfiJpig0i4DhENBp-2BQokwZMcgMdFpOhJVou0cF-2BcxDprFi2U2xhrxn5es5vY0TTwpQjqAhs-2BoK-2FZpbE0zkuyQ9tTtlInaU26DOBv1RHaiFTN-2F8GTEHoxvkJ1OHhhds3ATTWUCGwOhUOZ-2Fl5JjWzYdCDPeOgqnxlQd8b1i-2BJuaBRnhUjpQ7TzPnWkCur4qMtI-2BYKM3tD2d0RxTYTYfQ3GoNsZ-2FBo5Mf4Rb3lKQt59vxsLqKYe33qRjeFo12Ke3dS20gxD7Zxtpu57q1z0xuMgwj9uDDqrPTZh9qbUDYGc1IsbRhOAjL5z4kAYR2jGvTi2SFq9f2AiA1swOO3CORlZpwn5Y6BA-3D-3D)以获取更多有关免费访问的详细信息。”
- en: Even at $3.00/1 Million tokens, the costs are quite low for this task, coming
    out at about $7 a fine-tuning run for just over 2 million tokens in the test file.
    Bearing in mind, fine-tuning should be a rare event for this particular task,
    once we have such a model it can be reused.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 即使按$3.00/百万个令牌计算，对于这个任务来说，成本也相当低，整个调优过程大约需要7美元，测试文件中有超过200万个令牌。需要记住的是，对于这个特定任务，调优应该是一个少见的事件，一旦我们拥有这样的模型，它可以被重复使用。
- en: The fine-tuning produces the following output …
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 调优产生了以下输出……
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: It took about 45 minutes.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 花了大约45分钟。
- en: Testing our fine-tuned model to predict HXL
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试我们调优后的模型以预测HXL
- en: Now that we have a nice new shiny fine-tuned model for predicting HXL tags and
    attributes, we can use the test file to take it for a spin …
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个精心调优的新模型，可以预测HXL标签和属性，我们可以使用测试文件来进行测试……
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Noting in the above that all predictions are filtered for allowed tags and attributes
    as defined in the HXL standard.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 上述内容中需要注意的是，所有预测都经过了HXL标准中定义的允许标签和属性的筛选。
- en: This gives the following results …
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下结果……
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '‘Just HXL Tags’ means predicting the first part of the HXL, for example if
    the full HXL is #affected+infected+f, the model correctly got the #affected part
    correct. ‘Tags and attributes’ means predicting the full HXL string, ie ‘#affected+infected+f’,
    a much harder challenge due to all the combinations possible.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: “仅HXL标签”是指预测HXL的第一部分，例如，如果完整的HXL是#affected+infected+f，模型正确预测了#affected部分。“标签和属性”是指预测完整的HXL字符串，即‘#affected+infected+f’，这是一个更具挑战性的任务，因为存在许多可能的组合。
- en: The performance isn’t perfect, but not that bad, especially as we have balanced
    the dataset to reduce the number of location and date tags and attributes (ie
    made this study a bit more challenging). There are tens of thousands of humanitarian
    response tables without HDX, even the above performance would likely add value.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 性能并不完美，但也不算太差，特别是我们平衡了数据集，减少了位置和日期标签及属性的数量（即让这个研究稍微更具挑战性）。即使如此，仍然有成千上万的人道主义响应表格没有HDX，即使是上述性能也可能带来价值。
- en: Let’s look into cases where predictions didn’t agree with human-labeled data
    …
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看预测与人工标注数据不一致的案例……
- en: Reviewing the Human Labeled HXL Data
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 审查人类标注的HXL数据
- en: The predictions were saved to a spreadsheet, and I manually went through most
    of the predictions that didn’t agree with the labels. You can find this analysis
    [here](https://docs.google.com/spreadsheets/d/19BfVEU4hQJYUrliRKzfu5rXagK8CjoDH/edit?usp=sharing&ouid=107814789436940136200&rtpof=true&sd=true)
    and summarized below …
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 预测结果已保存到电子表格中，我手动查看了大多数与标签不一致的预测。你可以在[这里](https://docs.google.com/spreadsheets/d/19BfVEU4hQJYUrliRKzfu5rXagK8CjoDH/edit?usp=sharing&ouid=107814789436940136200&rtpof=true&sd=true)找到这项分析，并在下文进行总结……
- en: '![](../Images/dfbd71d99ab9103a6ad4e7e1673cdcbb.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dfbd71d99ab9103a6ad4e7e1673cdcbb.png)'
- en: 'What’s interesting is that in some cases the LLM is actually correct, for example
    in adding *additional* HXL attributes which the human labeled data doesn’t include.
    There are also cases where the human labeled HXL was perfectly reasonable, but
    the LLM predicted another tag or attribute that could also be interpreted as correct.
    For example a #region can also be an #admin1 in some countries, and whether something
    is an +id or +code is sometimes difficult to decide, both are appropriate.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，在某些情况下，LLM实际上是正确的，例如在添加*额外的*HXL属性时，而这些属性在人工标注的数据中没有包含。也有一些情况下，人工标注的HXL完全合理，但LLM预测了另一个标签或属性，这个标签或属性也可以被解释为正确。例如，在某些国家，#region也可以是#admin1，而某些情况下判断一个是+id还是+code也很难决定，两者都是合适的。
- en: Using the above categories, I created a new test set where the expected HXL
    tags were corrected. On re-running the prediction we get improved results …
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述类别，我创建了一个新的测试集，其中纠正了期望的HXL标签。在重新运行预测后，我们得到了改进的结果……
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Predicting HXL without Fine-tuning, instead only prompting GPT-4o
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在没有微调的情况下预测HXL，而仅仅是通过提示来使用GPT-4o
- en: The above shows that the human-labeled data itself can be incorrect. The HXL
    standard is designed excellently, but can be a challenge to memorize for developers
    and data scientists when setting HXL tags and attributes on data. There are some
    [amazing tools](https://hxlstandard.org/tools/) already provided by the HXL team,
    but sometimes the HXL is still incorrect. This introduces a problem to the fine-tuning
    approach which relies on this human-labeled data for training, especially for
    less well represented tags and attributes that humans are not using very often.
    It also has the limitation of not being able to adjust when the metadata standard
    changes, since the training data would not reflect those changes.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 上述内容表明人类标注的数据本身可能是错误的。HXL标准设计得非常出色，但对于开发人员和数据科学家来说，在数据上设置HXL标签和属性时，记住它们可能是一个挑战。HXL团队已经提供了一些[令人惊叹的工具](https://hxlstandard.org/tools/)，但有时HXL仍然是错误的。这给依赖这些人类标注数据进行训练的微调方法带来了问题，尤其是对于那些人类不常使用的标签和属性，这些标签和属性的表示较少。它还存在一个限制，即无法在元数据标准发生变化时进行调整，因为训练数据不会反映这些变化。
- en: Since the initial analysis 18 months ago various LLM providers have advanced
    their models significantly. OpenAI of course released [GPT-4o](https://openai.com/index/hello-gpt-4o/)
    as their flagship product, which importantly has a context window of 128k tokens
    and is another data point suggesting costs of foundational models are decreasing
    (see for example GPT-4-Turbo compared to GPT-4o [here](https://huggingface.co/spaces/philschmid/llm-pricing)).
    Given these factors, I wondered …
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 自18个月前的初步分析以来，各个LLM提供商已经显著提高了他们的模型。OpenAI当然发布了他们的旗舰产品[GPT-4o](https://openai.com/index/hello-gpt-4o/)，其具有128k个token的上下文窗口，这一点很重要，另外，这也是一个数据点，表明基础模型的成本正在下降（例如，GPT-4-Turbo与GPT-4o的比较[见此](https://huggingface.co/spaces/philschmid/llm-pricing)）。考虑到这些因素，我开始思考……
- en: '***If models are becoming more powerful and less expensive to use, could we
    avoid fine-tuning altogether and use them to predict HXL tags and attributes by
    prompting alone?***'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '***如果模型变得更强大且使用成本更低，我们是否可以完全避免微调，仅通过提示就能预测HXL标签和属性？***'
- en: Not only could this mean less engineering work to clean data and fine-tune models,
    it may have a big advantage in being able to include HXL tags and attributes which
    are not included in the human-labeled training data but are part of the HXL standard.
    This is one potentially huge advantage of powerful LLMs, being able to classify
    with zero- and few-shot prompting.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅意味着减少清理数据和微调模型的工程工作，还可能具有一个巨大优势，即能够包括人类标注的训练数据中未包含但属于HXL标准的HXL标签和属性。这是强大LLM的一个潜在巨大优势，可以通过零样本和少样本提示进行分类。
- en: Creating a prompt for predicting HXL
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为预测HXL创建提示
- en: Models like GPT-4o are trained on web data, so I thought I’d first do a test
    using one of our prompts to see if it already knew everything there was to know
    about HXL tags …
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 像GPT-4o这样的模型是基于网页数据训练的，所以我想先做一个测试，使用我们的提示之一来看看它是否已经知道关于HXL标签的所有信息…
- en: '![](../Images/b39b540c5990d5719eae62d8834191e4.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b39b540c5990d5719eae62d8834191e4.png)'
- en: What we see is that it seems to know about HXL syntax, but the answer is incorrect
    (the correct answer is ‘#affected+infected’), and it has chosen tags and attributes
    that are not in the HXL standard. It’s actually similar to what we see with human-tagged
    HXL.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到的是，它似乎知道HXL的语法，但答案不正确（正确答案是‘#affected+infected’），并且选择了不在HXL标准中的标签和属性。这实际上类似于我们在人类标注的HXL中看到的情况。
- en: How about we provide the most important parts of the [HXL standard](https://docs.google.com/spreadsheets/d/1En9FlmM8PrbTWgl3UHPF_MXnJ6ziVZFhBbojSJzBdLI/edit?pli=1&gid=319251406#gid=319251406)
    in the system prompt?
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在系统提示中提供[HXL标准](https://docs.google.com/spreadsheets/d/1En9FlmM8PrbTWgl3UHPF_MXnJ6ziVZFhBbojSJzBdLI/edit?pli=1&gid=319251406#gid=319251406)的最重要部分怎么样？
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This gives us a prompt like this …
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这给了我们如下的提示…
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: It’s pretty long (the above has been truncated), but encapsulates the HXL standard.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 它相当长（上面已被截断），但包含了HXL标准的要点。
- en: Another advantage of the direct prompt method is that we can also ask for the
    LLM to provide its reasoning when predicting HXL. This can of course include hallucination,
    but I’ve always found it useful for refining prompts.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 直接提示方法的另一个优势是，我们还可以要求LLM在预测HXL时提供其推理过程。当然，这可能包括幻觉，但我发现它对于优化提示非常有帮助。
- en: For the user prompt, we will use the same information that we used for fine-tuning,
    to include excerpt and LLM-generated table summary …
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于用户提示，我们将使用与微调时相同的信息，包括摘录和LLM生成的表格总结…
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Putting it all together, and prompting both GPT-4o-mini and GPT-4o for comparison
    …
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有内容综合起来，并同时对比GPT-4o-mini和GPT-4o的提示结果…
- en: '[PRE10]json","").replace("[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE10]json","").replace("[PRE11]'
- en: We get …
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到…
- en: '[PRE12]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As a reminder, the fine-tuned model produced the following results …
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒一下，微调后的模型产生了以下结果…
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '*How does prompting-only GPT-4o compare with GPT-4o-mini?*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*仅使用提示的GPT-4o与GPT-4o-mini相比如何？*'
- en: Looking at the above, we see that GPT-4o-mini prompting-only predicts just tags
    with 77% accuracy, which is less than GPT-4o-mini fine-tuning (83%) and GPT-4o
    prompting-only (86%). That said the performance is still good and would improve
    HXL coverage even if used as-is.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的数据来看，我们发现GPT-4o-mini仅使用提示预测标签的准确率为77%，低于GPT-4o-mini微调后的83%和GPT-4o仅使用提示的86%。尽管如此，性能仍然不错，即便直接使用也能改善HXL覆盖率。
- en: '*How does prompting-only compare with the fine-tuned model?*'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*仅使用提示与微调模型的对比如何？*'
- en: GPT-4o prompting-only gave the best results of all models, with 86% accuracy
    on tags and 71% on tags and attributes. In fact, the performance could well be
    better after a bit more analysis of the test data to correct incorrect human-labeled
    tags,.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4o仅使用提示的结果是所有模型中最好的，在标签上的准确率为86%，在标签和属性上的准确率为71%。实际上，经过更多对测试数据的分析以纠正错误的人类标签后，性能可能会更好。
- en: Let’s take a closer look at the times GPT-4o got it wrong …
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细看看GPT-4o出错的情况…
- en: '[PRE14]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Notice how we now have a ‘Reasoning’ field to indicate why the tags were chosen.
    This is useful and would be an important part for refining the prompt to improve
    performance.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们现在有了一个“推理”字段，用来说明为什么选择这些标签。这是很有用的，并且对于优化提示以提高性能是一个重要部分。
- en: Looking at the sample above, we see some familiar scenarios that were found
    when analyzing the fine-tuned model failed predictions …
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的示例来看，我们看到了一些熟悉的场景，这些场景出现在分析微调模型失败的预测时…
- en: +id and +code ambiguity
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: +id和+code的模糊性
- en: '#region and #adm1 used interchangeably'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '#region和#adm1互换使用'
- en: '#event versus more detailed tags like #cause'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '#event与更详细的标签如#cause的对比'
- en: These seem to fall into the category where two tags are possible for a given
    column given their HXL definition. But there are some real discrepancies which
    would need more investigation.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这些似乎属于那种根据HXL定义，给定列可能有两个标签的类别。但也有一些明显的不一致之处，需要进一步调查。
- en: That said, using GPT-4o to predict HXL tags and attributes yields the best results,
    and I believe at an acceptable level given a lot of data is missing HXL metadata
    altogether and many of the datasets which have it have incorrect tags and attributes.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，使用GPT-4o预测HXL标签和属性得出了最好的结果，我认为这是在可接受的水平，因为很多数据完全缺失HXL元数据，且许多包含这些数据的集合有错误的标签和属性。
- en: Cost Comparison
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 成本比较
- en: Let’s see how costs compare with each technique and model …
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看每种技术和模型的成本比较……
- en: '[PRE16]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Which gives …
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是……
- en: '[PRE17]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Note: the above is only for the inference cost, there will be a very small
    additional cost in generating table data summaries with GPT-3.5.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 注：上述仅为推理成本，生成表格数据摘要时使用 GPT-3.5 可能会有非常小的额外费用。
- en: Given the test set, predicting HXL for **458 columns** …
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 给定测试集，预测 **458 列** 的 HXL 标签……
- en: '**Fine-tuning**:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**微调**：'
- en: As expected, inference costs for the fine-tuned GPT-4o mini model (which cost
    about $7 to fine-tune) are very low about $0.02.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，微调后的 GPT-4o mini 模型（微调花费大约 $7）推理成本非常低，约为 $0.02。
- en: '**Prediction-only**:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**仅预测**：'
- en: GPT-4o prediction only is expensive, because of the HXL standard being passed
    in to the system prompt every time, and comes out at $13.44.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅使用 GPT-4o 进行预测的成本较高，因为每次都需要将 HXL 标准传递给系统提示，费用为 $13.44。
- en: GPT-4o-mini, albeit with reduced performance, is a more reasonable $0.40.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-4o-mini 尽管性能有所下降，但每次调用的费用更为合理，为 $0.40。
- en: So ease of use comes with a cost if using GPT-4o, but GPT-4o-mini is an attractive
    alternative.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用 GPT-4o 的易用性是有代价的，但 GPT-4o-mini 是一个具有吸引力的替代选择。
- en: Finally, it’s worth noting that in many cases, setting HXL tags might not to
    be real time, for example for a crawler process that corrects already uploaded
    datasets. This would mean that the new [OpenAI batch API](https://platform.openai.com/docs/guides/batch/overview)
    could be used, reducing costs by 50%.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，值得注意的是，在许多情况下，设置 HXL 标签可能不是实时的，例如对于修正已上传数据集的爬虫进程。这意味着可以使用新的[OpenAI 批量 API](https://platform.openai.com/docs/guides/batch/overview)，从而将成本降低
    50%。
- en: A Python class for predicting HXL Tags
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于预测 HXL 标签的 Python 类
- en: Putting this all together, I created a Github gist [hxl_utils.py](https://gist.github.com/dividor/e693997c1fc7e0d94f8228cebc397014).
    Check this out from GitHub and place the file in your current working directory.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些内容结合在一起，我创建了一个 Github gist [hxl_utils.py](https://gist.github.com/dividor/e693997c1fc7e0d94f8228cebc397014)。可以从
    GitHub 下载并将文件放入当前工作目录中。
- en: Let’s download a file to test it with …
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们下载一个文件来测试它……
- en: '[PRE18]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](../Images/f4d88e404896f574504c6d75b455d9cb.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4d88e404896f574504c6d75b455d9cb.png)'
- en: And using this dataframe, let’s predict HXL tags …
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个数据框，我们来预测 HXL 标签……
- en: '[PRE19]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](../Images/33bafdfd979641e3cc932f24d4992577.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/33bafdfd979641e3cc932f24d4992577.png)'
- en: And there we have it, some lovely HXL tags!
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，得到了些漂亮的 HXL 标签！
- en: Let’s see how well GPT-4o-mini does …
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 GPT-4o-mini 表现如何……
- en: '[PRE20]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Which gives …
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是……
- en: '![](../Images/551db548c232ea8e6d577b657dee9021.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/551db548c232ea8e6d577b657dee9021.png)'
- en: Pretty good! gpt-4o gave “#affected+killed+num” for the last column, where “gpt-4o-mini”
    gave “#affected+num”, but this could likely be resolved with some deft prompt
    engineering.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 很不错！gpt-4o 给出了“#affected+killed+num”作为最后一列，而“gpt-4o-mini”则给出了“#affected+num”，但这很可能可以通过一些巧妙的提示工程来解决。
- en: Admittedly this wasn’t a terribly challenging dataset, but it was able to correctly
    predict tags for events and fatalities, which are less frequent than location
    and dates.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 坦率地说，这不是一个非常具挑战性的数据集，但它能够正确预测事件和死亡的标签，而这些标签比地点和日期要少见。
- en: Future Work
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 未来工作
- en: I think a big takeaway here is that the direct-prompting technique produces
    good results without the need for training. Yes, more expensive for inference,
    but maybe not if a data scientist is required to curate incorrectly human-labeled
    fine-tuning data. It would depend on the organization and metadata use-case.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为一个重要的收获是，直接提示技术能够在无需训练的情况下取得不错的结果。是的，推理成本更高，但如果需要数据科学家整理错误标注的微调数据，也许并不那么贵。这将取决于组织和元数据的使用场景。
- en: Here are some areas that might be considered in future work …
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些可能在未来工作中考虑的领域……
- en: '**Improved test data**'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**改进的测试数据**'
- en: This analysis did a quick review of the test set to correct HXL tags which were
    incorrect in the data or had multiple possible values. More time could be spent
    on this, as always in machine learning, ground truth is key.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分析快速审查了测试集，以修正数据中不正确的 HXL 标签或存在多个可能值的标签。可以在这方面投入更多时间，正如在机器学习中，地面真相是关键。
- en: '**Prompt engineering and hyperparameter tuning**'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示工程与超参数调优**'
- en: The above analysis uses very basic prompts with no real engineering or strategies
    applied, these could definitely be improved for better performance. With an evaluation
    set and a framework such as [Promptflow](https://github.com/microsoft/promptflow),
    prompt variants could be tested. Additionally we might add more context data,
    for example in deciding administrative levels, which can vary per country. Finally,
    we have used fixed hyperparameters for temperature and top_p, as well as completion
    token length. All these could be tuned leading to better performance.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 上述分析使用了非常基础的提示词，并没有应用任何真正的工程方法或策略，这些方法肯定可以通过改进来提高性能。通过评估集和像[Promptflow](https://github.com/microsoft/promptflow)这样的框架，可以测试不同的提示词变体。此外，我们还可以添加更多的上下文数据，例如在决定行政级别时，这可能因国家而异。最后，我们使用了固定的超参数，如温度、top_p以及完成标记长度。所有这些都可以调整，从而提高性能。
- en: '**Cost optimization**'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**成本优化**'
- en: The prompting-only approach definitely appears to be a strong option and simplifies
    how an organization can automatically set HXL tags on their data using GPT-4o.
    There are of course cost implications with this model, being a more expensive,
    but predictions occur only on low-volume schema changes, not when the underlying
    data itself changes, and with new options for [batch submission](https://openai.com/api/pricing/)
    on OpenAI and ever decreasing LLM costs, this technique appears viable for many
    organizations. GPT-4o-mini also performs well and is a fraction of the cost.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用提示词的方法无疑是一个强有力的选择，并简化了组织如何通过GPT-4o自动为其数据设置HXL标签。当然，这种方法有成本上的考虑，因为它较为昂贵，但预测只发生在低频率的模式变化时，而不是当底层数据本身发生变化时，随着OpenAI提供新的[批量提交](https://openai.com/api/pricing/)选项以及LLM成本不断下降，这项技术对许多组织来说是可行的。GPT-4o-mini的表现也很好，且成本只是其一小部分。
- en: '**Application to other metadata standards**'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**应用于其他元数据标准**'
- en: It would be interesting to apply this technique to other metadata and labeling
    standards, I’m sure many organizations are already using LLMs for this.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 将这项技术应用于其他元数据和标注标准会很有趣，我确信许多组织已经在使用LLMs来实现这一点。
- en: '*Please like this article if inclined and I’d be delighted if you followed
    me! You can find more articles* [*here*](https://medium.com/@astrobagel)*.*'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你愿意，请点赞这篇文章，如果你关注我，我将非常高兴！你可以在* [*这里*](https://medium.com/@astrobagel)*
    找到更多文章。*'
