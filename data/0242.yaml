- en: Performant IPv4 Range Spark Joins
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高性能IPv4范围Spark连接
- en: 原文：[https://towardsdatascience.com/performant-ipv4-range-spark-joins-95143b305436?source=collection_archive---------3-----------------------#2024-01-25](https://towardsdatascience.com/performant-ipv4-range-spark-joins-95143b305436?source=collection_archive---------3-----------------------#2024-01-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/performant-ipv4-range-spark-joins-95143b305436?source=collection_archive---------3-----------------------#2024-01-25](https://towardsdatascience.com/performant-ipv4-range-spark-joins-95143b305436?source=collection_archive---------3-----------------------#2024-01-25)
- en: A Practical guide to optimizing non-equi joins in Spark
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实用指南：优化Spark中的非等值连接
- en: '[](https://medium.com/@jean-claude.cote?source=post_page---byline--95143b305436--------------------------------)[![Jean-Claude
    Cote](../Images/aea2df9c7b95fc85cc336f64d64b0a76.png)](https://medium.com/@jean-claude.cote?source=post_page---byline--95143b305436--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--95143b305436--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--95143b305436--------------------------------)
    [Jean-Claude Cote](https://medium.com/@jean-claude.cote?source=post_page---byline--95143b305436--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jean-claude.cote?source=post_page---byline--95143b305436--------------------------------)[![Jean-Claude
    Cote](../Images/aea2df9c7b95fc85cc336f64d64b0a76.png)](https://medium.com/@jean-claude.cote?source=post_page---byline--95143b305436--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--95143b305436--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--95143b305436--------------------------------)
    [Jean-Claude Cote](https://medium.com/@jean-claude.cote?source=post_page---byline--95143b305436--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--95143b305436--------------------------------)
    ·9 min read·Jan 25, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--95143b305436--------------------------------)
    ·9分钟阅读·2024年1月25日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/e2dd3d2557024e0fd123571495dea461.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2dd3d2557024e0fd123571495dea461.png)'
- en: Photo by John Lee on Unsplash
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由John Lee提供，来源于Unsplash
- en: Enriching network events with IP geolocation information is a crucial task,
    especially for organizations like the [Canadian Centre for Cyber Security](https://www.cyber.gc.ca/en),
    the national CSIRT of Canada. In this article, we will demonstrate how to optimize
    Spark SQL joins, specifically focusing on scenarios involving non-equality conditions
    — a common challenge when working with IP geolocation data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 通过IP地理位置信息丰富网络事件是一个至关重要的任务，特别是对于[加拿大网络安全中心](https://www.cyber.gc.ca/en)等组织，这是加拿大的国家计算机安全事件响应团队（CSIRT）。在本文中，我们将展示如何优化Spark
    SQL连接，特别关注涉及不等式条件的场景——这是处理IP地理位置数据时常见的挑战。
- en: As cybersecurity practitioners, our reliance on enriching network events with
    IP geolocation databases necessitates efficient strategies for handling non-equi
    joins. While numerous articles shed light on various join strategies supported
    by Spark, the practical application of these strategies remains a prevalent concern
    for professionals in the field.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 作为网络安全从业者，我们依赖于通过IP地理位置数据库丰富网络事件，这就要求我们采用高效的策略来处理非等值连接。尽管有许多文章阐述了Spark支持的各种连接策略，但这些策略在实际应用中的效果仍然是业内专业人士关注的一个问题。
- en: David Vrba’s insightful article, [“About Joins in Spark 3.0”](/about-joins-in-spark-3-0-1e0ea083ea86),
    published on Towards Data Science, serves as a valuable resource. It explains
    the conditions guiding Spark’s selection of specific join strategies. In his article,
    David briefly suggests that optimizing non-equi joins involves transforming them
    into equi-joins.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: David Vrba的深刻文章，[“关于Spark 3.0中的连接”](/about-joins-in-spark-3-0-1e0ea083ea86)，发布于Towards
    Data Science，是一篇宝贵的资源。它解释了Spark选择特定连接策略的条件。在他的文章中，David简要地指出，优化非等值连接的方法是将其转化为等值连接。
- en: This write-up aims to provide a practical guide for optimizing the performance
    of a non-equi JOIN, with a specific focus on joining with IP ranges in a geolocation
    table.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文旨在提供一个实用指南，帮助优化非等值连接的性能，特别是聚焦于与地理位置表中的IP范围进行连接的情况。
- en: To exemplify these optimizations, we will revisit the geolocation table introduced
    [in our previous article](/unleashing-the-power-of-sql-analytical-window-functions-a-deep-dive-into-fusing-ipv4-blocks-62bf2b3405e0).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了举例说明这些优化，我们将回顾在[我们之前的文章](/unleashing-the-power-of-sql-analytical-window-functions-a-deep-dive-into-fusing-ipv4-blocks-62bf2b3405e0)中介绍的地理位置表。
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Equi-Join
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 等值连接
- en: To illustrate Spark’s execution of an equi-join, we’ll initiate our exploration
    by considering a hypothetical scenario. Suppose we have a table of events, each
    event being associated with a specific `owner`denoted by the `event_owner` column.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明 Spark 如何执行等值连接，我们将通过考虑一个假设的场景来开始我们的探索。假设我们有一个事件表，每个事件都与特定的 `owner` 相关联，该
    `owner` 由 `event_owner` 列表示。
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s take a closer look at how Spark handles this equi-join:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看 Spark 如何处理这个等值连接：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In this example, the equi-join is established between the `events` table and
    the `geolocation` table. The linking criterion is based on the equality of the
    `event_owner` column in the `events` table and the `owner` column in the `geolocation`
    table.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，等值连接建立在 `events` 表和 `geolocation` 表之间。连接的标准是基于 `events` 表中的 `event_owner`
    列与 `geolocation` 表中的 `owner` 列是否相等。
- en: 'As explained by David Vrba in his blog post:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 正如 David Vrba 在他的博客中所解释的：
- en: Spark will plan the join with SMJ if there is an equi-condition and the joining
    keys are sortable
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果存在等值条件，并且连接键是可排序的，Spark 会选择使用 SMJ 进行连接规划。
- en: 'Spark will execute a Sort Merge Join, distributing the rows of the two tables
    by hashing the `event_owner` on the left side and the `owner` on the right side.
    Rows from both tables that hash to the same Spark partition will be processed
    by the same Spark task—a unit of work. For example, Task-1 might receive:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 将执行排序合并连接（Sort Merge Join），通过对左侧的 `event_owner` 和右侧的 `owner` 进行哈希操作来分配两表的行。哈希到同一
    Spark 分区的两表行将由同一 Spark 任务处理——一个工作单元。例如，Task-1 可能接收：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Notice how Task-1 handles only a subset of the data. The join problem is divided
    into multiple smaller tasks, where only a subset of the rows from both the left
    and right sides is required. Furthermore, the left and right side rows processed
    by Task-1 have to match. This is true because every occurrence of “Telus” will
    hash to the same partition, regardless of whether it comes from the `events` or
    `geolocation` tables. We can be certain that no other Task-X will have rows with
    an `owner` of “Telus”.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 Task-1 仅处理数据的一个子集。连接问题被划分为多个较小的任务，其中只需要左右两边的部分行。此外，Task-1 处理的左右两边行必须匹配。因为无论来自
    `events` 表还是 `geolocation` 表，“Telus” 的每次出现都会哈希到相同的分区。我们可以确定其他 Task-X 不会有 `owner`
    为“Telus”的行。
- en: Once the data is divided as shown above, Spark will sort both sides, hence the
    name of the join strategy, Sort Merge Join. The merge is performed by taking the
    first row on the left and testing if it matches the right. Once the rows on the
    right no longer match, Spark will pull rows from the left. It will keep dequeuing
    each side until no rows are left on either side.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据按照上面所示的方式划分，Spark 将对两边的数据进行排序，因此这种连接策略被称为排序合并连接（Sort Merge Join）。合并操作通过取左边的第一行并测试其是否与右边匹配来进行。一旦右边的行不再匹配，Spark
    将从左边提取行。它将不断从两边队列中提取，直到两边没有剩余行。
- en: Non-equi Join
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非等值连接
- en: Now that we have a better understanding of how equi-joins are performed, let’s
    contrast it with a non-equi join. Suppose we have events with an `event_ip`, and
    we want to add geolocation information to this table.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们更好地理解了等值连接是如何执行的，接下来我们将其与非等值连接进行对比。假设我们有事件表，其中包含 `event_ip`，并且我们希望向该表添加地理位置信息。
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To execute this join, we need to determine the IP range within which the `event_ip`
    falls. We accomplish this with the following condition:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行此连接，我们需要确定 `event_ip` 所在的 IP 范围。我们通过以下条件来实现这一点：
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now, let’s consider how Spark will execute this join. On the right side (the
    geolocation table), there is no key by which Spark can hash and distribute the
    rows. It is impossible to divide this problem into smaller tasks that can be distributed
    across the compute cluster and performed in parallel.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑 Spark 如何执行此连接。在右边（地理位置表），没有可以哈希并分配行的键。无法将此问题划分为可以在计算集群中分发并并行执行的较小任务。
- en: 'In a situation like this, Spark is forced to employ more resource-intensive
    join strategies. As stated by David Vrba:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，Spark 被迫采用更为资源密集型的连接策略。正如 David Vrba 所说：
- en: If there is no equi-condition, Spark has to use BroadcastNestedLoopJoin (BNLJ)
    or cartesian product (CPJ).
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果没有等值条件，Spark 必须使用广播嵌套循环连接（BroadcastNestedLoopJoin，BNLJ）或笛卡尔积连接（Cartesian Product
    Join，CPJ）。
- en: Both of these strategies involve brute-forcing the problem; for every row on
    the left side, Spark will test the “between” condition on every single row of
    the right side. It has no other choice. If the table on the right is small enough,
    Spark can optimize by copying the right-side table to every task reading the left
    side, a scenario known as the BNLJ case. However, if the left side is too large,
    each task will need to read both the right and left sides of the table, referred
    to as the CPJ case. In either case, both strategies are highly costly.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种策略都涉及暴力破解问题；对于左侧的每一行，Spark会对右侧的每一行测试“between”条件。它别无选择。如果右侧的表足够小，Spark可以通过将右侧表复制到每个读取左侧的任务中来优化，这种情况称为BNLJ（嵌套循环连接）情况。然而，如果左侧太大，每个任务将需要读取右侧和左侧的表，这种情况称为CPJ（连接点连接）情况。在这两种情况下，这两种策略都是非常昂贵的。
- en: So, how can we improve this situation? The trick is to introduce an equality
    in the join condition. For example, we could simply unroll all the IP ranges in
    the geolocation table, producing a row for every IP found in the IP ranges.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何改进这种情况呢？诀窍是引入连接条件中的相等性。例如，我们可以简单地展开地理位置表中的所有IP范围，为每个IP生成一行。
- en: 'This is easily achievable in Spark; we can execute the following SQL to unroll
    all the IP ranges:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在Spark中容易实现的；我们可以执行以下SQL来展开所有IP范围：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The [sequence](https://spark.apache.org/docs/latest/api/sql/#sequence) function
    creates an array with the IP values from `start_ip` to `end_ip`. The [explode](https://spark.apache.org/docs/latest/api/sql/#explode)
    function unrolls this array into individual rows.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[sequence](https://spark.apache.org/docs/latest/api/sql/#sequence)函数创建一个从`start_ip`到`end_ip`的IP值数组。[explode](https://spark.apache.org/docs/latest/api/sql/#explode)函数将这个数组展开成单独的行。'
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: With a key on both sides, we can now execute an equi-join, and Spark can efficiently
    distribute the problem, resulting in optimal performance. However, in practice,
    this scenario is not realistic, as a genuine geolocation table often contains
    billions of rows.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 有了两侧的键，我们现在可以执行等值连接，Spark可以高效地分配问题，从而实现最优性能。然而，在实际应用中，这种情况并不现实，因为真正的地理位置表通常包含数十亿行。
- en: 'To address this, we can enhance the efficiency by increasing the coarseness
    of this mapping. Instead of mapping IP ranges to each individual IP, we can map
    the IP ranges to segments within the IP space. Let’s assume we divide the IP space
    into segments of 5\. The segmented space would look something like this:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们可以通过增加映射的粗糙度来提高效率。我们可以将IP范围映射到IP空间中的段，而不是将IP范围映射到每个单独的IP。假设我们将IP空间分割成5个一组的段。分段后的空间大致如下所示：
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now, our objective is to map the IP ranges to the segments they overlap with.
    Similar to what we did earlier, we can unroll the IP ranges, but this time, we’ll
    do it in segments of 5.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的目标是将IP范围映射到它们重叠的段。类似于我们之前做的那样，我们可以展开IP范围，但这次我们将按5个一组的段进行操作。
- en: '[PRE9]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We observe that certain IP ranges share a `bucket_id`. Ranges 1–2 and 3–4 both
    fall within the segment 1–5.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到某些IP范围共享相同的`bucket_id`。范围1–2和3–4都属于段1–5。
- en: '[PRE10]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Additionally, we notice that some IP ranges are duplicated. The last two rows
    for the IP range 23–29 overlap with segments 20–25 and 26–30\. Similar to the
    scenario where we unrolled individual IPs, we are still duplicating rows, but
    to a much lesser extent.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们注意到一些IP范围是重复的。IP范围23–29的最后两行与段20–25和26–30重叠。类似于我们展开单个IP的情况，我们仍然在重复行，但重复的程度要小得多。
- en: Now, we can utilize this bucketed table to perform our join.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以利用这个分桶表来执行我们的连接。
- en: '[PRE11]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The equality in the join enables Spark to perform a Sort Merge Join (SMJ) strategy.
    The “between” condition eliminates cases where IP ranges share the same `bucket_id`.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 连接中的相等条件使得Spark能够执行排序合并连接（Sort Merge Join，SMJ）策略。“between”条件消除了IP范围共享相同`bucket_id`的情况。
- en: In this illustration, we used segments of 5; however, in reality, we would segment
    the IP space into segments of 256\. This is because the global IP address space
    is overseen by the Internet Assigned Numbers Authority (IANA), and traditionally,
    IANA allocates address space in blocks of 256 IPs.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用了5个一组的段；然而，实际上，我们会将IP空间分割成256个一组的段。这是因为全球IP地址空间由互联网号码分配局（IANA）监管，传统上，IANA按256个IP的块分配地址空间。
- en: Analyzing the IP ranges in a genuine geolocation table using the Spark `approx_percentile`
    function reveals that most records have spans of less than 256, while very few
    are larger than 256.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Spark的`approx_percentile`函数分析真正的地理位置表中的IP范围可以发现，大多数记录的跨度小于256，而大于256的记录非常少。
- en: '[PRE12]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This implies that most IP ranges are assigned a `bucket_id`, while the few larger
    ones are unrolled, resulting in the unrolled table containing approximately an
    extra 10% of rows.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着大多数 IP 范围都被分配了一个`bucket_id`，而少数较大的范围则被展开，导致展开后的表格大约包含额外 10% 的行。
- en: 'A query executed with a genuine geolocation table might resemble the following:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 执行的查询与一个真实的地理定位表可能如下所示：
- en: '[PRE13]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Conclusion
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: In conclusion, this article has presented a practical demonstration of converting
    a non-equi join into an equi-join through the implementation of a mapping technique
    that involves segmenting IP ranges. It’s crucial to note that this approach extends
    beyond IP addresses and can be applied to any dataset characterized by bands or
    ranges.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，本文展示了通过实现一种涉及分割 IP 范围的映射技术，将非等值连接转化为等值连接的实际示例。需要特别注意的是，这种方法不仅限于 IP 地址，还可以应用于任何由带区或范围构成的数据集。
- en: The ability to effectively map and segment data is a valuable tool in the arsenal
    of data engineers and analysts, providing a pragmatic solution to the challenges
    posed by non-equality conditions in Spark SQL joins.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 有效地映射和分割数据是数据工程师和分析师工具箱中一个有价值的工具，为 Spark SQL 连接中的非等值条件所带来的挑战提供了实际的解决方案。
