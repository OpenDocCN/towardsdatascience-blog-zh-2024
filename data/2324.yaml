- en: How to Evaluate RAG If You Don’t Have Ground Truth Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何评估没有地面真实数据的RAG
- en: 原文：[https://towardsdatascience.com/how-to-evaluate-rag-if-you-dont-have-ground-truth-data-590697061d89?source=collection_archive---------0-----------------------#2024-09-24](https://towardsdatascience.com/how-to-evaluate-rag-if-you-dont-have-ground-truth-data-590697061d89?source=collection_archive---------0-----------------------#2024-09-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-evaluate-rag-if-you-dont-have-ground-truth-data-590697061d89?source=collection_archive---------0-----------------------#2024-09-24](https://towardsdatascience.com/how-to-evaluate-rag-if-you-dont-have-ground-truth-data-590697061d89?source=collection_archive---------0-----------------------#2024-09-24)
- en: Vector similarity search threshold, synthetic data generation, LLM-as-a-judge,
    and frameworks
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量相似度搜索阈值、合成数据生成、LLM作为判定者以及框架
- en: '[](https://medium.com/@jenn-j-dev?source=post_page---byline--590697061d89--------------------------------)[![Jenn
    J.](../Images/d2ef3b8f454d4f7a974edd5a965a80e8.png)](https://medium.com/@jenn-j-dev?source=post_page---byline--590697061d89--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--590697061d89--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--590697061d89--------------------------------)
    [Jenn J.](https://medium.com/@jenn-j-dev?source=post_page---byline--590697061d89--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jenn-j-dev?source=post_page---byline--590697061d89--------------------------------)[![Jenn
    J.](../Images/d2ef3b8f454d4f7a974edd5a965a80e8.png)](https://medium.com/@jenn-j-dev?source=post_page---byline--590697061d89--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--590697061d89--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--590697061d89--------------------------------)
    [Jenn J.](https://medium.com/@jenn-j-dev?source=post_page---byline--590697061d89--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--590697061d89--------------------------------)
    ·10 min read·Sep 24, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--590697061d89--------------------------------)
    ·10 分钟阅读·2024年9月24日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Evaluating a Retrieval-Augmented Generation (RAG) model is much easier when
    you have ground truth data against which to compare. But what if you don’t? That’s
    where things get a bit trickier. However, even in the absence of ground truth,
    there are still ways to assess how well your RAG system is performing. Below,
    we’ll walk through three effective strategies, ways to create a ground truth dataset
    from scratch, metrics you can use to evaluate when you do have a dataset, and
    existing frameworks that can help you with this process.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 评估检索增强生成（RAG）模型时，如果有地面真实数据可以对比会容易得多。但如果没有呢？这时事情会变得有些棘手。然而，即使没有地面真实数据，仍然有方法来评估你的RAG系统表现如何。下面，我们将介绍三种有效策略，如何从零开始创建地面真实数据集，当你拥有数据集时可以使用的度量，以及现有框架如何帮助你完成这个过程。
- en: 'Two types of RAG evaluations: Retrieval evaluation and Generation evaluation'
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 两种类型的RAG评估：检索评估和生成评估
- en: Each strategy below will be tagged as either retrieval evaluation, generation
    evaluation, or both.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 以下每种策略都会被标记为检索评估、生成评估或两者兼有。
- en: How to evaluate RAG if you don’t have ground truth data?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何评估没有地面真实数据的RAG？
- en: Vector Similarity Search Threshold
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量相似度搜索阈值
- en: 'Type: Retrieval Evaluation'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：检索评估
- en: If you’re working with a vector database like Pinecone, you’re probably familiar
    with the idea of vector similarity. Essentially, the database retrieves information
    based on how close the vectors of your query are to the vectors of potential results.
    Even without a “correct” answer to measure against, you can still lean on metrics
    like cosine similarity to gauge the quality of the retrieved documents.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用像 Pinecone 这样的向量数据库，可能对向量相似度的概念比较熟悉。本质上，数据库是根据查询的向量与潜在结果的向量之间的接近程度来检索信息的。即使没有“正确”的答案进行对比，你仍然可以依赖像余弦相似度这样的度量来评估检索到的文档质量。
- en: '![](../Images/1bdcbb4fb6eeffebe1bf091142c9c3f6.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1bdcbb4fb6eeffebe1bf091142c9c3f6.png)'
- en: Cosine Distance. [Image provided by the author]
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦距离。[作者提供的图片]
- en: For example, Pinecone will return cosine similarity values that show how close
    each result is to your query.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Pinecone 会返回余弦相似度值，显示每个结果与查询的相似程度。
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: By exposing the similarity score, you can set a passing or failing grade on
    retrieved documents. A higher threshold (like 0.8 or above) means having a stricter
    requirement, while a lower threshold will bring in more data, which could be helpful
    or just noisy.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 通过展示相似性分数，你可以为检索到的文档设定通过或不通过的标准。较高的阈值（如0.8或以上）意味着要求更严格，而较低的阈值则会带入更多的数据，这些数据可能有用，也可能只是噪声。
- en: This process isn’t about finding a perfect number right away — it’s about trial
    and error. We’ll know if we’ve hit the sweet spot when the results consistently
    feel useful for our specific application.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程并不是要立即找到一个完美的数字——而是关于不断试错。当结果在我们的特定应用中始终感觉有用时，我们就知道找到了最佳点。
- en: Using Multiple LLMs to Judge Responses
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用多个LLM来评判回应
- en: 'Type: Retrieval + Generation Evaluation'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：检索 + 生成评估
- en: Another creative way to evaluate your RAG system is by [leveraging multiple
    LLMs to judge responses](https://arxiv.org/html/2402.14860v2). Even though LLMs
    can’t provide a perfect answer when ground truth data is missing, you can still
    use their feedback to compare the quality of the responses.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 评估你的RAG系统的另一种创造性方式是通过[利用多个LLM来评判回应](https://arxiv.org/html/2402.14860v2)。尽管LLM在缺少真实数据的情况下无法提供完美的答案，但你仍然可以利用它们的反馈来比较回应的质量。
- en: By comparing responses across different LLMs and seeing how they rank them,
    you can gauge the overall quality of the retrievals and generations. It’s not
    perfect, but it’s a creative way to get multiple perspectives on the quality of
    your system’s output.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较不同LLM的回应，并查看它们如何对这些回应进行排名，你可以评估检索和生成的整体质量。这不是完美的，但它是一种创造性的方法，可以从多个角度评估你系统输出的质量。
- en: 'Human-in-the-Loop Feedback: Involving the Experts'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人工反馈：涉及专家
- en: 'Type: Retrieval + Generation Evaluation'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：检索 + 生成评估
- en: Sometimes, the best way to evaluate a system is the old-fashioned way — by asking
    humans for their judgment. Getting feedback from domain experts can provide insights
    that even the best models can’t match.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，评估系统的最佳方法是采用传统方式——请人类进行判断。从领域专家那里获取反馈，可以提供即便是最好的模型也无法匹配的洞察。
- en: '**Setting Up Rating Criteria**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**设置评分标准**'
- en: 'To make human feedback more reliable, it helps to establish clear and consistent
    rating criteria. You might ask your reviewers to rate things like:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使人工反馈更可靠，帮助建立清晰一致的评分标准很重要。你可以让审稿人对以下方面进行评分：
- en: '**Relevance:** Does the retrieved information address the query? (Retrieval
    evaluation)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相关性：** 检索到的信息是否解决了查询？（检索评估）'
- en: '**Correctness:** Is the content factually accurate? (Retrieval evaluation)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正确性：** 内容是否事实准确？（检索评估）'
- en: '**Fluency:** Does it read well, or does it feel awkward or forced? (Generation
    evaluation)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流畅度：** 它读起来是否顺畅，还是感觉生硬或强迫？（生成评估）'
- en: '**Completeness:** Does it cover the question fully or leave gaps (Retrieval
    + Generation evaluation)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完整性：** 它是否完全覆盖了问题，还是留下了空白？（检索 + 生成评估）'
- en: With these criteria in place, you can get a more structured sense of how well
    your system is performing.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些标准，你可以更系统地了解你的系统表现如何。
- en: '**Getting a Baseline**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**获取基准**'
- en: One smart way to evaluate the quality of your human feedback is to check how
    well different reviewers agree with each other. You can use metrics like [Pearson
    correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) to
    see how closely their judgements align. If your reviewers disagree a lot, it might
    mean your criteria aren’t clear enough. It could also be a sign that the task
    is more subjective than you anticipated.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 评估人工反馈质量的一个聪明方法是检查不同审稿人之间的一致性。你可以使用像[Pearson相关系数](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)这样的度量，看看他们的判断有多接近。如果审稿人之间意见分歧很大，可能意味着评分标准不够清晰，也可能是任务比你预期的更具主观性。
- en: '![](../Images/569d951f471766a129e8f86bdaf0b084.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/569d951f471766a129e8f86bdaf0b084.png)'
- en: Pearson correlation coefficient. [Image provided by author]
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Pearson相关系数。[图片由作者提供]
- en: '**Reducing Noise**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**减少噪声**'
- en: 'Human feedback can be noisy, especially if the criteria are unclear or the
    task is subjective. Here are a couple of ways to deal with that:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 人工反馈可能会有噪声，尤其是在评分标准不明确或任务具有主观性的情况下。以下是几种应对方法：
- en: '**Averaging the Scores**: By averaging the ratings of multiple human reviewers,
    you can smooth out any individual biases or inconsistencies.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均评分：** 通过对多位审稿人的评分进行平均，你可以消除个体偏见或不一致性。'
- en: '**Focus on Agreement:** Another approach is to only consider cases where your
    reviewers agree. This will give you a cleaner set of evaluations and help ensure
    the quality of your feedback.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**专注于一致性：** 另一种方法是只考虑审阅者一致同意的情况。这将为你提供更清晰的评估集，并帮助确保反馈的质量。'
- en: Creating a Ground Truth Dataset from Scratch
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零开始创建真实数据集
- en: When it comes to evaluating a RAG system without ground truth data, another
    approach is to create your own dataset. It sounds daunting, but there are several
    strategies to make this process easier, from finding similar datasets to leveraging
    human feedback and even synthetically generating data. Let’s break down how you
    can do it.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估没有真实数据集的RAG系统时，另一种方法是自己创建数据集。听起来可能有些令人生畏，但有几个策略可以简化这个过程，从寻找相似数据集到利用人工反馈，甚至合成生成数据。让我们来看看如何做到这一点。
- en: Finding Similar Datasets Online
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在线寻找相似的数据集
- en: This might seem obvious, and most people who have come to the conclusion that
    they don’t a have ground truth dataset have already exhausted this option. But
    it’s still worth mentioning that there might be datasets out there that are similar
    to what you need. Perhaps it’s in a different business domain from your use case
    but it’s in the question-answer format that you’re working with. Sites like Kaggle
    have a huge variety of public datasets, and you might be surprised at how many
    align with your problem space.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来可能显而易见，大多数得出没有真实数据集结论的人通常已经用尽了这个选项。但仍值得一提的是，可能会有一些数据集与您的需求类似。也许它来自不同的业务领域，但它与您正在使用的问答格式相匹配。像Kaggle这样的网站拥有丰富的公共数据集，你可能会惊讶于有多少数据集与你的问题空间对接。
- en: 'Example:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[Stanford Question Answering Dataset](https://www.kaggle.com/datasets/stanfordu/stanford-question-answering-dataset)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[斯坦福问答数据集](https://www.kaggle.com/datasets/stanfordu/stanford-question-answering-dataset)'
- en: '[Amazon Question/Answer Dataset](https://www.kaggle.com/datasets/praneshmukhopadhyay/amazon-questionanswer-dataset)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[亚马逊问答数据集](https://www.kaggle.com/datasets/praneshmukhopadhyay/amazon-questionanswer-dataset)'
- en: Manually Creating Ground Truth Data
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 手动创建真实数据集
- en: If you can’t find exactly what you need online, you can always create ground
    truth data manually. This is where human-in-the-loop feedback comes in handy.
    Remember the domain expert feedback we talked about earlier? You can use that
    feedback to build your own mini-dataset.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在线上找不到完全符合需求的内容，你可以手动创建真实数据集。这时，人工反馈（human-in-the-loop）就显得特别有用。还记得我们之前讨论的领域专家反馈吗？你可以利用这些反馈来构建你自己的小型数据集。
- en: By curating a collection of human-reviewed examples — where the relevance, correctness,
    and completeness of the results have been validated — you create a foundation
    for expanding your dataset for evaluation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 通过整理一组人工审核的示例——这些示例的相关性、准确性和完整性已经得到了验证——你为扩展评估数据集打下了基础。
- en: There is also a great article from Katherine Munro on an [experimental approach
    to agile chatbot development](/lessons-from-agile-experimental-chatbot-development-73ea515ba762).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Katherine Munro还有一篇很好的文章，讲述了[敏捷聊天机器人开发的实验方法](/lessons-from-agile-experimental-chatbot-development-73ea515ba762)。
- en: Training an LLM as a Judge
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练LLM作为评判者
- en: Once you have your minimal ground truth dataset, you can take things a step
    further by training an LLM to act as a judge and evaluate your model’s outputs.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你拥有了最小的真实数据集，你就可以进一步提升，训练一个LLM来充当评判者，并评估你模型的输出。
- en: 'But before relying on an LLM to act as a judge, we first need to ensure that
    it’s rating our model outputs accurately, or at least reliable. Here’s how you
    can approach that:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 但在依赖大型语言模型（LLM）作为评判者之前，我们首先需要确保它能够准确地评估我们模型的输出，至少是可靠的。你可以按照以下方法来实现：
- en: '**Build human-reviewed examples:** Depending on your use case, 20 to 30 examples
    should be good enough to get a good sense of how reliable the LLM is in comparison.
    Refer to the previous section on best criteria to rate and how to measure conflicting
    ratings.'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**构建人工审核的示例：** 根据你的使用场景，20到30个示例应该足够让你了解LLM在对比中有多可靠。请参考前一节关于评估标准和如何衡量冲突评分的内容。'
- en: '**Create Your LLM Judge:** Prompt an LLM to give ratings based on the same
    criteria that you handed to your domain experts. Take the rating and compare how
    the LLM’s ratings align with the human ratings. Again, you can use metrics like
    Pearson metrics to help evaluate. A high correlation score will indicate that
    the LLM is performing as well as a judge.'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建您的LLM评判者：** 向LLM提出问题，要求它根据您提供给领域专家的相同标准给出评分。获取评分并比较LLM的评分与人工评分的一致性。您可以使用类似皮尔逊相关系数的度量来帮助评估。高相关性得分将表明LLM的表现与评审员一样出色。'
- en: '**Apply** [**prompt engineering best practices**](https://www.bighummingbird.com/blogs/prompt-engineering-best-practices):
    Prompt engineering can make or break this process. Techniques like pre-warming
    the LLM with context or providing a few examples (few-shot learning) can dramatically
    improve the models’ accuracy when judging.'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**应用** [**提示工程最佳实践**](https://www.bighummingbird.com/blogs/prompt-engineering-best-practices)：提示工程能够决定这个过程的成败。使用像预热LLM上下文或提供一些示例（少量学习）等技术，可以显著提高模型在评判时的准确性。'
- en: Creating Specific Datasets
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建特定的数据集
- en: Another way to boost the quality and quantity of your ground truth datasets
    is by segmenting your documents into topics or semantic groupings. Instead of
    looking at entire documents as a whole, break them down into smaller, more focused
    segments.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 提升基础真相数据集质量和数量的另一种方式是将文档按主题或语义分组进行切分。与其整体查看文档，不如将其拆解成更小、更集中的段落。
- en: 'For example, let’s say you have a document (documentId: 123) that mentions:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '例如，假设您有一个文档（文档ID: 123），其中提到：'
- en: '*“After launching product ABC, company XYZ saw a 10% increase in revenue for
    2024 Q1.”*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*“在推出产品ABC之后，XYZ公司在2024年第一季度的收入增长了10%。”*'
- en: 'This one sentence contains two distinct pieces of information:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这句话包含了两条不同的信息：
- en: '*Launching product ABC*'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*推出产品ABC*'
- en: '*A 10% increase in revenue for 2024 Q1*'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*2024年第一季度收入增长了10%*'
- en: 'Now, you can augment each topic into its own query and context. For example:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以将每个主题扩展为其自己的查询和上下文。例如：
- en: '**Query 1:** *“What product did company XYZ launch?”*'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询 1:** *“XYZ公司推出了什么产品？”*'
- en: '**Context 1:** *“Launching product ABC”*'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文 1:** *“推出产品ABC”*'
- en: '**Query 2:** *“What was the change in revenue for Q1 2024?”*'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询 2:** *“2024年第一季度收入变化是多少？”*'
- en: '**Context 2:** *“Company XYZ saw a 10% increase in revenue for Q1 2024”*'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文 2:** *“XYZ公司在2024年第一季度的收入增长了10%”*'
- en: 'By breaking the data into specific topics like this, you not only create more
    data points for training but also make your dataset more precise and focused.
    Plus, if you want to trace each query back to the original document for reliability,
    you can easily add metadata to each context segment. For instance:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将数据划分为具体的主题，您不仅可以为训练创建更多的数据点，还可以使数据集更加精准和聚焦。此外，如果您希望将每个查询追溯到原始文档以保证可靠性，您可以轻松地为每个上下文段落添加元数据。例如：
- en: '**Query 1:** *“What product did company XYZ launch?”*'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询 1:** *“XYZ公司推出了什么产品？”*'
- en: '**Context 1:** *“Launching product ABC (documentId: 123)”*'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文 1:** *“推出产品ABC（文档ID: 123）”*'
- en: '**Query 2:** “What was the change in revenue for Q1 2024?”'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询 2:** “2024年第一季度收入变化是多少？”'
- en: '**Context 2:** “Company XYZ saw a 10% increase in revenue for Q1 2024 (documentId:
    123)”'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文 2:** “XYZ公司在2024年第一季度收入增长了10%（文档ID: 123）”'
- en: This way, each segment is tied back to its source, making your dataset even
    more useful for evaluation and training.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，每个段落都会与其来源关联，使得您的数据集在评估和训练时更加有用。
- en: Synthetically Creating a Dataset
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 合成创建数据集
- en: If all else fails, or if you need more data than you can gather manually, synthetic
    data generation can be a game-changer. Using techniques like data augmentation
    or even GPT models, you can create new data points based on your existing examples.
    For instance, you can take a base set of queries and contexts and tweak them slightly
    to create variations.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果其他方法都失败了，或者您需要更多的数据而手动收集不足，合成数据生成可以是一个改变游戏规则的解决方案。使用数据增强或甚至GPT模型等技术，您可以基于现有示例创建新的数据点。例如，您可以对基础查询和上下文集进行轻微修改，生成不同的变体。
- en: 'For example, starting with the query:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，首先处理查询：
- en: '*“What product did company XYZ launch?”*'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*“XYZ公司推出了什么产品？”*'
- en: 'You could synthetically generate variations like:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以合成生成类似的变体：
- en: '*“Which product was introduced by company XYZ?”*'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*“XYZ公司推出了哪款产品？”*'
- en: '*“What was the name of the product launched by company XYZ?”*'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*“XYZ公司推出的是什么产品？”*'
- en: This can help you build a much larger dataset without the manual overhead of
    writing new examples from scratch.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这将帮助您构建一个更大的数据集，而无需从头开始手动编写新的示例。
- en: There are also frameworks that can automate the process of generating synthetic
    data for you that we’ll explore in the last section.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些框架可以自动化生成合成数据的过程，我们将在最后一部分进行探讨。
- en: 'Once You Have a Dataset: Time to Evaluate'
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一旦你有了数据集：是时候进行评估了
- en: 'Now that you’ve gathered or created your dataset, it’s time to dive into the
    evaluation phase. RAG model involves two key areas: retrieval and generation.
    Both are important and understanding how to assess each will help you fine-tune
    your model to better meet your needs.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经收集或创建了数据集，是时候进入评估阶段了。RAG模型包括两个关键领域：检索和生成。两者都很重要，理解如何评估每个领域将有助于你调整模型，更好地满足需求。
- en: 'Evaluating Retrieval: How Relevant is the Retrieved Data?'
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估检索：检索到的数据有多相关？
- en: 'The retrieval step in RAG is crucial — if your model can’t pull the right information,
    it’s going to struggle with generating accurate responses. Here are two key metrics
    you’ll want to focus on:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: RAG中的检索步骤至关重要——如果你的模型不能拉取正确的信息，它将难以生成准确的响应。以下是你需要关注的两个关键指标：
- en: '**Context Relevancy:** This measures how well the retrieved context aligns
    with the query. Essentially, you’re asking: *Is this information actually relevant
    to the question being asked?* You can use your dataset to calculate relevance
    scores, either by human judgment or by comparing similarity metrics (like cosine
    similarity) between the query and the retrieved document.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文相关性：** 这衡量检索到的上下文与查询的匹配度。本质上，你在问：*这个信息与所提问题相关吗？* 你可以使用数据集来计算相关性分数，可以通过人工判断或通过比较查询与检索文档之间的相似性度量（如余弦相似度）来实现。'
- en: '**Context Recall:** Context recall focuses on how much relevant information
    was retrieved. It’s possible that the right document was pulled, but only part
    of the necessary information was included. To evaluate recall, you need to check
    whether the context your model pulled contains all the key pieces of information
    to fully answer the query. Ideally, you want high recall: your retrieval should
    grab the information you need and nothing critical should be left behind.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文召回：** 上下文召回关注的是检索到的相关信息的多少。可能拉取了正确的文档，但只包含了必要信息的一部分。为了评估召回率，你需要检查你的模型检索的上下文是否包含了所有关键信息，以便完全回答查询。理想情况下，你希望得到高召回率：你的检索应该抓取到你需要的信息，且不会遗漏任何关键信息。'
- en: 'Evaluating Generation: Is the Response Both Accurate and Useful?'
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估生成：响应是否既准确又有用？
- en: 'Once the right information is retrieved, the next step is generating a response
    that not only answers the query but does so faithfully and clearly. Here are two
    critical aspects to evaluate:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦正确的信息被检索到，下一步是生成一个不仅能回答查询，而且能够忠实且清晰地回答的响应。以下是两个关键方面需要评估：
- en: '**Faithfulness:** This measures whether the generated response accurately reflects
    the retrieved context. Essentially, you want to avoid hallucinations — where the
    model makes up information that wasn’t in the retrieved data. Faithfulness is
    about ensuring that the answer is grounded in the facts presented by the documents
    your model retrieved.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**忠实度：** 这衡量生成的响应是否准确地反映了检索到的上下文。本质上，你希望避免“幻觉”——即模型编造了检索数据中没有的信息。忠实度是关于确保答案基于模型检索到的文档所呈现的事实。'
- en: '**Answer Relevancy:** This refers to how well the generated answer matches
    the query. Even if the information is faithful to the retrieved context, it still
    needs to be relevant to the question being asked. You don’t want your model to
    pull out correct information that doesn’t quite answer the user’s question.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**答案相关性：** 这指的是生成的答案与查询的匹配程度。即便信息忠实于检索到的上下文，它仍然需要与所提问题相关。你不希望模型提取出正确的信息，但没有准确回答用户的问题。'
- en: Doing a Weighted Evaluation
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 做加权评估
- en: Once you’ve assessed both retrieval and generation, you can go a step further
    by combining these evaluations in a weighted way. Maybe you care more about relevancy
    than recall, or perhaps faithfulness is your top priority. You can assign different
    weights to each metric depending on your specific use case.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你评估了检索和生成，你可以通过加权方式进一步整合这些评估。也许你更关心相关性而非召回率，或者忠实度是你的首要任务。你可以根据特定的使用场景为每个指标分配不同的权重。
- en: 'For example:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '**Retrieval:** 60% context relevancy + 40% context recall'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检索：** 60% 上下文相关性 + 40% 上下文召回率'
- en: '**Generation:** 70% faithfulness + 30% answer relevancy'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成：** 70% 忠实度 + 30% 答案相关性'
- en: This kind of weighted evaluation gives you flexibility in prioritizing what
    matters most for your application. If your model needs to be 100% factually accurate
    (like in legal or medical contexts), you may put more weight on faithfulness.
    On the other hand, if completeness is more important, you might focus on recall.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这种加权评估方法为你提供了在优先考虑应用最重要因素时的灵活性。如果你的模型需要100%的事实准确性（如法律或医学领域），你可能会更重视忠实度。另一方面，如果完整性更为重要，你可能会关注召回率。
- en: Existing Frameworks to Simplify Your Evaluation Process
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简化评估过程的现有框架
- en: If creating your own evaluation system feels overwhelming, don’t worry — there
    are some great existing frameworks that have already done a lot of the heavy lifting
    for you. These frameworks come with built-in metrics designed specifically to
    evaluate RAG systems, making it easier to assess retrieval and generation performance.
    Let’s look at a few of the most helpful ones.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果创建你自己的评估系统让你感到不知所措，不用担心——有一些很好的现有框架，已经为你完成了大部分繁重的工作。这些框架内置了专门用于评估RAG系统的指标，使评估检索和生成性能变得更加简单。让我们看看其中一些最有帮助的框架。
- en: '[RAGAS (Retrieval-Augmented Generation Assessment)](https://docs.ragas.io/en/stable/)'
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[RAGAS（检索增强生成评估）](https://docs.ragas.io/en/stable/)'
- en: RAGAS is a purpose-built framework designed to assess the performance of RAG
    models. It includes metrics that evaluate both retrieval and generation, offering
    a comprehensive way to measure how well your system is doing at each step. It
    also offers synthetic test data generation by employing an evolutionary generation
    paradigm.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: RAGAS是一个专门设计的框架，用于评估RAG模型的性能。它包括评估检索和生成的指标，提供了一种全面的方式来衡量系统在每个步骤中的表现。它还通过采用进化生成范式来提供合成测试数据生成。
- en: '*Inspired by works like* [***Evol-Instruct***](https://arxiv.org/abs/2304.12244)*,
    Ragas achieves this by employing an evolutionary generation paradigm, where questions
    with different characteristics such as reasoning, conditioning, multi-context,
    and more are systematically crafted from the provided set of documents. — RAGAS
    documentation*'
  id: totrans-105
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*灵感来自于诸如* [***Evol-Instruct***](https://arxiv.org/abs/2304.12244)*等作品，RAGAS通过采用进化生成范式实现这一点，在这一过程中，具有不同特征的问题，如推理、条件性、多上下文等，会从提供的文档集中系统性地构建出来。——RAGAS文档*'
- en: '[ARES: Open-Source Framework Using Synthetic Data and LLM Judge](https://github.com/stanford-futuredata/ARES)'
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[ARES: 使用合成数据和LLM裁判的开源框架](https://github.com/stanford-futuredata/ARES)'
- en: '**ARES** is another powerful tool that combines synthetic data generation with
    LLM-based evaluation. ARES uses synthetic data — data generated by AI models rather
    than collected from real-world interactions — to build a dataset that can be used
    to test and refine your RAG system.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**ARES** 是另一个强大的工具，它将合成数据生成与基于LLM的评估相结合。ARES使用合成数据——由AI模型生成的数据，而不是从现实世界交互中收集的数据——来构建数据集，以便测试和优化你的RAG系统。'
- en: The framework also includes an LLM judge, which, as we discussed earlier, can
    help evaluate model outputs by comparing them to human annotations or other reference
    data.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 该框架还包括一个LLM裁判，正如我们之前所讨论的，它可以通过将模型输出与人工标注或其他参考数据进行比较来帮助评估模型的表现。
- en: Conclusion
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Even without ground truth data, these strategies can help you effectively evaluate
    a RAG system. Whether you’re using vector similarity thresholds, multiple LLMs,
    LLM-as-a-judge, retrieval metrics, or frameworks, each approach gives you a way
    to measure performance and improve your model’s results. The key is finding what
    works best for your specific needs — and not being afraid to tweak things along
    the way. 🙂
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 即使没有真实数据，这些策略也可以帮助你有效地评估RAG系统。无论你是使用向量相似度阈值、多种LLM、LLM作为裁判、检索指标还是框架，每种方法都为你提供了一种衡量性能并改进模型结果的方式。关键是找到最适合你特定需求的方法，并且不要害怕在过程中进行调整。🙂
- en: '*Join the conversation!* [*Subscribe*](https://medium.com/@jenn-j-dev/subscribe)
    *for practical AI tips, real-world use cases, and behind-the-scenes insights as
    I build in public.*'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*加入讨论！* [*订阅*](https://medium.com/@jenn-j-dev/subscribe) *获取实用的AI技巧、真实案例以及我在公开构建过程中的幕后见解。*'
- en: '*Curious to learn more about LLMs? Check out our* [*AI-in-Action blog*](https://www.bighummingbird.com/blogs)
    *on AI agents, prompt engineering, and LLMOps.*'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*想了解更多LLM的信息吗？查看我们的* [*AI-in-Action博客*](https://www.bighummingbird.com/blogs)
    *，了解AI代理、提示工程和LLMOps。*'
