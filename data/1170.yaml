- en: Feature Selection with Optuna
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Optuna 进行特征选择
- en: 原文：[https://towardsdatascience.com/feature-selection-with-optuna-0ddf3e0f7d8c?source=collection_archive---------3-----------------------#2024-05-09](https://towardsdatascience.com/feature-selection-with-optuna-0ddf3e0f7d8c?source=collection_archive---------3-----------------------#2024-05-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/feature-selection-with-optuna-0ddf3e0f7d8c?source=collection_archive---------3-----------------------#2024-05-09](https://towardsdatascience.com/feature-selection-with-optuna-0ddf3e0f7d8c?source=collection_archive---------3-----------------------#2024-05-09)
- en: A versatile and promising approach for the feature selection task
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一种多功能且有前景的特征选择方法
- en: '[](https://medium.com/@nicolupi.2?source=post_page---byline--0ddf3e0f7d8c--------------------------------)[![Nicolas
    Lupi](../Images/7f0735890a77b9ef601dc6cd54a9a861.png)](https://medium.com/@nicolupi.2?source=post_page---byline--0ddf3e0f7d8c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0ddf3e0f7d8c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0ddf3e0f7d8c--------------------------------)
    [Nicolas Lupi](https://medium.com/@nicolupi.2?source=post_page---byline--0ddf3e0f7d8c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@nicolupi.2?source=post_page---byline--0ddf3e0f7d8c--------------------------------)[![Nicolas
    Lupi](../Images/7f0735890a77b9ef601dc6cd54a9a861.png)](https://medium.com/@nicolupi.2?source=post_page---byline--0ddf3e0f7d8c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0ddf3e0f7d8c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0ddf3e0f7d8c--------------------------------)
    [Nicolas Lupi](https://medium.com/@nicolupi.2?source=post_page---byline--0ddf3e0f7d8c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0ddf3e0f7d8c--------------------------------)
    ·13 min read·May 9, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0ddf3e0f7d8c--------------------------------)
    ·阅读时间 13 分钟·2024年5月9日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/dd461c48cbc1fb8dfe671c1403a9486e.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd461c48cbc1fb8dfe671c1403a9486e.png)'
- en: Photo by [Edu Grande](https://unsplash.com/@edgr?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Edu Grande](https://unsplash.com/@edgr?utm_source=medium&utm_medium=referral)
    提供，来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'Feature selection is a critical step in many machine learning pipelines. In
    practice, we generally have a wide range of variables available as predictors
    for our models, but only a few of them are related to our target. Feature selection
    consists of finding a reduced set of these features, mainly for:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择是许多机器学习流程中的关键步骤。在实际操作中，我们通常会有一系列的变量可以作为模型的预测因子，但其中只有一小部分与我们的目标相关。特征选择的目标是找到这些特征的一个简化集合，主要有以下几种原因：
- en: '**Improved generalization** — using a reduced number of features minimizes
    the risk of overfitting.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**改进的泛化能力** — 使用较少的特征可以最小化过拟合的风险。'
- en: '**Better inference** — by removing redundant features (for example, two features
    very correlated with each other), we can retain only one of them and better capture
    its effect.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更好的推理能力** — 通过移除冗余的特征（例如，两个高度相关的特征），我们可以保留其中一个特征，并更好地捕捉其影响。'
- en: '**Efficient training** — having less features means shorter training times.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高效的训练** — 特征减少意味着更短的训练时间。'
- en: '**Better interpretation** — reducing the number of features produces more parsimonious
    models which are easier to understand.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更好的解释性** — 减少特征数量能够产生更简洁的模型，更容易理解。'
- en: There are many techniques available to perform feature selection, each with
    varying complexity. In this article, I want to share a way of using a powerful
    open source optimization tool, Optuna, to perform the feature selection task in
    an innovative way. The main idea is to have a flexible tool that can handle feature
    selection for a wide range of tasks, by efficiently testing different feature
    combinations (e.g., not trying them all one by one). Below, we’ll go through a
    hands-on example implementing this approach, and also comparing it to other common
    feature selection strategies. To experiment with the feature selection techniques
    discussed, you can follow along with this [Colab Notebook](https://colab.research.google.com/drive/193Jwb0xXWh_UkvwIiFgufKEYer-86RNA?usp=sharing).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多可用的特征选择技术，每种技术的复杂度不同。在本文中，我想分享一种使用强大开源优化工具Optuna以创新方式执行特征选择任务的方法。其主要思想是拥有一个灵活的工具，可以通过高效地测试不同的特征组合（例如，不是逐一尝试它们）来处理各种任务的特征选择。接下来，我们将通过一个动手示例来实现这种方法，并将其与其他常见的特征选择策略进行比较。要实验本文讨论的特征选择技术，您可以按照此[Colab笔记本](https://colab.research.google.com/drive/193Jwb0xXWh_UkvwIiFgufKEYer-86RNA?usp=sharing)进行操作。
- en: 'In this example, we’ll focus on a classification task based on the [Mobile
    Price Classification](https://www.kaggle.com/datasets/iabhishekofficial/mobile-price-classification)
    dataset from Kaggle. We have 20 features, including ‘*battery_power’*, ‘*clock_speed’*
    and ‘*ram’*, to predict the ‘*price_range’* feature, which can belong to four
    different bands: 0, 1, 2 and 3.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将重点关注基于[Kaggle的移动价格分类](https://www.kaggle.com/datasets/iabhishekofficial/mobile-price-classification)数据集的分类任务。我们有20个特征，包括‘*battery_power*’、‘*clock_speed*’和‘*ram*’，用于预测‘*price_range*’特征，该特征可以属于四个不同的区间：0、1、2和3。
- en: We first split our dataset into train and test sets, and we also prepare a 5-fold
    validation split within the train set — this will be useful later on.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将数据集拆分为训练集和测试集，并在训练集中准备一个5折验证集——这将在后续过程中派上用场。
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The model we’ll use throughout the example is the [Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html),
    using the scikit-learn implementation and default parameters. We first train the
    model using all features to set our benchmark. The metric we’ll measure is the
    F1 score weighted for all four price ranges. After fitting the model over the
    train set, we evaluate it on the test set, obtaining an F1 score of around 0.87.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在整个示例中使用的模型是[随机森林分类器](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)，使用的是scikit-learn实现和默认参数。我们首先使用所有特征训练模型以设定基准。我们将衡量的指标是对所有四个价格区间加权的F1分数。通过在训练集上拟合模型后，我们在测试集上进行评估，得到大约0.87的F1分数。
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/06ba109e36469ca2e01826c3e7150bc1.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06ba109e36469ca2e01826c3e7150bc1.png)'
- en: Image by author
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: The goal now is to improve these metrics by selecting a reduced feature set.
    We will first outline how our Optuna-based approach works, and then test and compare
    it with other common feature selection strategies.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的目标是通过选择一个精简的特征集来改善这些指标。我们将首先概述我们的基于Optuna的方法如何工作，然后与其他常见的特征选择策略进行测试和比较。
- en: Optuna
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Optuna
- en: '[Optuna](https://optuna.org/) is an optimization framework mainly used for
    hyperparameter tuning. One of the key features of the framework is its use of
    Bayesian optimization techniques to search the parameter space. The main idea
    is that Optuna tries different combinations of parameters and evaluates how the
    objective function changes with each configuration. From these trials, it builds
    a probabilistic model used to estimate which parameter values are likely to yield
    better outcomes.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[Optuna](https://optuna.org/)是一个主要用于超参数调优的优化框架。该框架的一个关键特性是使用贝叶斯优化技术来搜索参数空间。其主要思想是，Optuna尝试不同的参数组合，并评估每种配置下目标函数的变化。从这些试验中，它构建了一个概率模型，用于估计哪些参数值可能会带来更好的结果。'
- en: This strategy is much more efficient compared to grid or random search. For
    example, if we had *n* features, and attempted to try each possible feature subset,
    we would have to perform 2^*n* trials. With 20 features these would be more than
    a million trials. Instead, with Optuna, we can explore the search space with much
    fewer trials.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 与网格搜索或随机搜索相比，这种策略效率更高。例如，如果我们有*n*个特征，并尝试每个可能的特征子集，我们将不得不进行2^*n*次试验。如果有20个特征，这将超过一百万次试验。相反，使用Optuna，我们可以用更少的试验探索搜索空间。
- en: Optuna offers various samplers to try. For our case, we’ll use the default one,
    the *TPESampler*, based on the Tree-structured Parzen Estimator algorithm (TPE).
    This sampler is the most commonly used, and it’s recommended for searching categorical
    parameters, which is our case as we’ll see below. According to the documentation,
    this algorithm “fits one Gaussian Mixture Model (GMM) *l(x)* to the set of parameter
    values associated with the best objective values, and another GMM *g(x)* to the
    remaining parameter values. It chooses the parameter value x that maximizes the
    ratio *l(x)/g(x)*.”
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Optuna 提供了多种采样器供选择。对于我们的情况，我们将使用默认的 *TPESampler*，它基于树结构 Parzen 估计器算法（TPE）。这个采样器是最常用的，并且推荐用于搜索分类参数，这正是我们的情况，正如我们下面所看到的那样。根据文档，这个算法“拟合一个高斯混合模型（GMM）*l(x)*
    到与最佳目标值关联的参数值集，并将另一个 GMM *g(x)* 拟合到剩余的参数值。它选择最大化 *l(x)/g(x)* 比率的参数值 x。”
- en: As mentioned earlier, Optuna is typically used for hyperparameter tuning. This
    is usually done by training the model repeatedly on the same data using a fixed
    set of features, and in each trial testing a new set of hyperparameters determined
    by the sampler. The parameter set that minimizes the given objective function
    is then returned as the best trial.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Optuna 通常用于超参数调优。这通常是通过在相同数据上反复训练模型，使用固定的特征集，并在每次试验中测试由采样器确定的一组新的超参数。最小化给定目标函数的参数集将作为最佳试验返回。
- en: In our case, however, we’ll use a fixed model with predetermined parameters,
    and in each trial, we’ll allow Optuna to select which features to try. The process
    aims to find the set of features that minimizes the loss function. In our case,
    we’ll guide the algorithm to maximize the F1 score (or minimize the negative of
    the F1). Additionally, we’ll add a small penalty for each feature used, to encourage
    smaller feature sets (if two feature sets yield similar results, we’ll prefer
    the one with fewer features).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们的情况下，我们将使用一个固定的模型和预设的参数，在每个试验中允许 Optuna 选择要尝试的特征。该过程的目的是找到一组最小化损失函数的特征集。在我们的情况下，我们将指导算法最大化
    F1 分数（或最小化 F1 的负值）。此外，我们将为每个使用的特征添加一个小的惩罚项，以鼓励使用更小的特征集（如果两个特征集产生相似的结果，我们将更倾向于选择特征较少的那个）。
- en: The data we’ll use is the train dataset, split into five folds. In each trial,
    we’ll fit the classifier five times using four of the five folds for training
    and the remaining fold for validation. We’ll then average the validation metrics
    and add the penalty term to calculate the trial’s loss.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的数据是训练数据集，已经被分成五个折叠。在每个试验中，我们将使用五折中的四折进行训练，剩余的折叠用于验证。然后，我们将平均验证指标，并添加惩罚项来计算试验的损失。
- en: 'Below is the implemented class to perform the feature selection search:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是执行特征选择搜索的实现类：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The key part is where we define which features to use. We treat each feature
    as one parameter, which can take the values True or False. These values indicate
    whether the feature should be included in the model. We use the *suggest_categorical*
    method so that Optuna selects one of the two possible values for each feature.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 关键部分是我们定义使用哪些特征。我们将每个特征视为一个参数，该参数可以取值为 True 或 False。这些值表示该特征是否应该包含在模型中。我们使用*suggest_categorical*方法，这样
    Optuna 就可以为每个特征选择两个可能值中的一个。
- en: 'We now initialize our Optuna study and perform the search for 100 trials. Notice
    that we enqueue a first trial using all features, as a starting point for the
    search, allowing Optuna to compare subsequent trials against a fully-featured
    model:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们初始化我们的 Optuna 学习任务，并执行 100 次试验的搜索。注意，我们首先将所有特征用于第一个试验，作为搜索的起点，允许 Optuna
    将后续试验与一个完整特征的模型进行比较：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'After completing the 100 trials, we retrieve the best one from the study and
    the features used in it. These are the following:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 完成 100 次试验后，我们从学习任务中提取最佳试验和其中使用的特征。它们如下所示：
- en: '[*‘battery_power’, ‘blue’, ‘dual_sim’, ‘fc’, ‘mobile_wt’, ‘px_height’, ‘px_width’,
    ‘ram’, ‘sc_w’*]'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[*‘battery_power’, ‘blue’, ‘dual_sim’, ‘fc’, ‘mobile_wt’, ‘px_height’, ‘px_width’,
    ‘ram’, ‘sc_w’*]'
- en: Notice that from the original 20 features, the search concluded with only 9
    of them, which is a significant reduction. These features yielded a minimum validation
    loss of around -0.9117, which means they achieved an average F1 score of around
    0.9108 across all folds (after adjusting for the penalty term).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，从原始的 20 个特征中，搜索仅保留了其中的 9 个特征，这是一种显著的减少。这些特征产生了大约 -0.9117 的最小验证损失，这意味着它们在所有折叠上的平均
    F1 分数大约为 0.9108（在调整了惩罚项之后）。
- en: 'The next step is to train the model on the entire train set using these selected
    features and evaluate it on the test set. This results in an F1 score of around
    0.882:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是使用这些选定的特征在整个训练集上训练模型，并在测试集上进行评估。这样会得到一个大约为0.882的F1分数：
- en: '![](../Images/3efce9ccee6e54966260ab9e0c568dc8.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3efce9ccee6e54966260ab9e0c568dc8.png)'
- en: Image by author
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'By selecting the right features, we were able to reduce our feature set by
    more than half, while still achieving a higher F1 score than with the full set.
    Below we will discuss some pros and cons of using Optuna for feature selection:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 通过选择合适的特征，我们能够将特征集减少一半以上，同时仍然比使用完整特征集时获得更高的F1分数。接下来我们将讨论使用Optuna进行特征选择的一些优缺点：
- en: '**Pros**:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**：'
- en: Searches across feature sets efficiently, taking into account which feature
    combinations are most likely to produce good results.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高效地在特征集之间进行搜索，考虑哪些特征组合最有可能产生良好的结果。
- en: 'Adaptable for many scenarios: As long as there is a model and a loss function,
    we can use it for any feature selection task.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适用于多种场景：只要有模型和损失函数，我们就可以将其应用于任何特征选择任务。
- en: 'Sees the whole picture: Unlike methods that evaluate features individually,
    Optuna takes into account which features tend to go well with each other, and
    which don’t.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 看得更全面：与逐个评估特征的方法不同，Optuna会考虑哪些特征相互搭配良好，哪些则不然。
- en: Dynamically determines the number of features as part of the optimization process.
    This can be tuned with the penalty term.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在优化过程中动态确定特征的数量。这可以通过惩罚项进行调整。
- en: '**Cons**:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**：'
- en: It’s not as straightforward as simpler methods, and for smaller and simpler
    datasets it might not be worth it.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不像简单方法那样直观，对于较小和较简单的数据集来说，可能不值得使用。
- en: Although it requires much fewer trials than other methods (like exhaustive search),
    it still typically requires around 100 to 1000 trials. Depending on the model
    and dataset, this can be time-consuming and computationally expensive.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管它比其他方法（如穷举搜索）需要的试验次数要少得多，但它通常仍然需要大约100到1000次试验。根据模型和数据集的不同，这可能是时间消耗大且计算开销高的。
- en: Next, we’ll compare our approach to other common feature selection strategies.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把我们的方法与其他常见的特征选择策略进行比较。
- en: Other Methods
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他方法
- en: Filter Methods — Chi-Squared
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过滤方法 — 卡方检验
- en: One of the simplest alternatives is to evaluate each feature individually using
    a statistial test and retain the top *k* features based on their scores. Notice
    that this approach doesn’t require any machine learning model. For example, for
    the classification task, we can choose the chi-squared test, which determines
    whether there is a statistically significant association between each feature
    and the target variable. We’ll use the [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)
    class from scikit-learn, which applies the score function (chi-squared) to each
    feature and returns the top *k* scoring variables. Unlike the Optuna method, the
    number of features isn’t determined in the selection process, but must be set
    beforehand. In this case, we’ll set this number at ten. These methods fall within
    the filter methods class. They tend to be the easiest and fastest to compute since
    they don’t require any model behind.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的替代方法之一是使用统计测试分别评估每个特征，并根据其分数保留前* k *个特征。请注意，这种方法不需要任何机器学习模型。例如，对于分类任务，我们可以选择卡方检验，它确定每个特征与目标变量之间是否存在统计学上的显著关联。我们将使用来自scikit-learn的[SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)类，它将分数函数（卡方检验）应用于每个特征，并返回前*
    k *个得分最高的变量。与Optuna方法不同，特征的数量不是在选择过程中确定的，而是必须事先设定。在这种情况下，我们将其数量设置为十。这些方法属于过滤方法类别。它们通常是最简单且最快的计算方法，因为它们不需要背后有任何模型。
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/c5caab606183aa19ef91c6caf5dfd41b.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c5caab606183aa19ef91c6caf5dfd41b.png)'
- en: Image by author
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'In our case, *ram* scored the highest by far in the chi-squared test, followed
    by *px_height* and *battery_power*. Notice that these features were also selected
    by our Optuna method above, along with *px_width*, *mobile_wt* and *sc_w*. However,
    there are some new additions like *int_memory* and *talk_time* — these weren’t
    picked by the Optuna study. After training the random forest with these 10 features
    and evaluating it on the test set, we achieved an F1 score slightly higher than
    our previous best, at approximately 0.888:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，*ram* 在卡方检验中得分最高，其次是*px_height*和*battery_power*。请注意，这些特征也是我们上面Optuna方法所选择的特征，此外还有*px_width*、*mobile_wt*和*sc_w*。然而，也有一些新的特征如*int_memory*和*talk_time*——这些特征在Optuna研究中并未被选中。在使用这10个特征训练随机森林并在测试集上评估后，我们获得了略高于之前最佳得分的F1得分，约为0.888：
- en: '![](../Images/6d68dc4ce15b284ad414bbc2ead91a35.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d68dc4ce15b284ad414bbc2ead91a35.png)'
- en: Image by author
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: '**Pros**:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**：'
- en: 'Model agnostic: doesn’t require a machine learning model.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与模型无关：不需要机器学习模型。
- en: Easy and fast to implement and run.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现和运行都既简单又快速。
- en: '**Cons**:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**：'
- en: It has to be adapted for each task. For instance, some score functions are only
    applicable for classification tasks, and others only for regression tasks.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必须针对每个任务进行调整。例如，一些评分函数仅适用于分类任务，另一些则仅适用于回归任务。
- en: 'Greedy: depending on the alternative used, it usually looks at features one
    by one, without taking into account which are already included in the set.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贪婪策略：根据使用的替代方法，通常会逐一查看特征，而不考虑哪些已经包含在特征集合中。
- en: Requires the number of features to select to be set beforehand.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要提前设置要选择的特征数量。
- en: Wrapper Methods — Forward Search
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 包装方法 — 前向搜索
- en: Wrapper methods are another class of feature selection strategies. These are
    iterative methods; they involve training the model with a set of features, evaluating
    its performance, and then deciding whether to add or remove features. Our Optuna
    strategy falls within these methods. However, most common examples include forward
    selection or backward selection. With forward selection, we begin with no features
    and, at each step, we greedily add the feature that provides the highest performance
    gain, until a stop criterion is met (number of features or performance decline).
    Conversely, backward selection starts with all features and iteratively removes
    the least significant ones at each step.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 包装方法是另一类特征选择策略。这些方法是迭代性的；它们包括用一组特征训练模型、评估其性能，然后决定是否添加或删除特征。我们的Optuna策略属于这些方法之一。然而，最常见的例子包括前向选择和后向选择。在前向选择中，我们从没有特征开始，在每一步中，我们贪婪地添加提供最高性能增益的特征，直到满足停止准则（特征数量或性能下降）。相反，后向选择从所有特征开始，并在每一步中迭代地删除最不重要的特征。
- en: Below, we try the [SequentialFeatureSelector](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html)
    class from scikit-learn, performing a forward selection until we find the top
    10 features. This method will also make use of the 5-fold split we performed above,
    averaging performance across the validation splits at each step.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们尝试使用scikit-learn中的[SequentialFeatureSelector](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html)类，执行前向选择，直到找到前10个特征。此方法还将利用我们之前执行的5折交叉验证，在每一步对验证集的性能进行平均。
- en: '[PRE5]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This method ends up selecting the following features:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法最终选择了以下特征：
- en: '[*‘battery_power’, ‘blue’, ‘fc’, ‘mobile_wt’, ‘px_height’, ‘px_width’, ‘ram’,
    ‘talk_time’, ‘three_g’, ‘touch_screen’*]'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[*‘battery_power’, ‘blue’, ‘fc’, ‘mobile_wt’, ‘px_height’, ‘px_width’, ‘ram’,
    ‘talk_time’, ‘three_g’, ‘touch_screen’*]'
- en: Again, some are common to the previous methods, and some are new (e.g., *three_g*
    and *touch_screen*. Using these features, the Random Forest achieves a lower test
    F1 score, slightly below 0.88.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 再次提到，一些特征与之前的方法相同，另一些则是新的（例如，*three_g* 和 *touch_screen*）。使用这些特征，随机森林在测试集上的F1得分较低，略低于0.88。
- en: '![](../Images/e201b5306398514db1b2d9345d072136.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e201b5306398514db1b2d9345d072136.png)'
- en: Image by author
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: '**Pros**'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**：'
- en: Easy to implement in just a few lines of code.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只需几行代码即可轻松实现。
- en: It can also be used to determine the number of features to use (using the tolerance
    parameter).
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它也可以用于确定要使用的特征数量（通过容忍度参数）。
- en: '**Cons**'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**：'
- en: 'Time consuming: Starting with zero features, it trains the model each time
    using a different variable, and retains the best one. For the next step, it again
    tries out all features (now including the previous one), and again selects the
    best one. This is repeated until the desired number of features is reached.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 耗时：从零特征开始，每次使用不同的变量训练模型，并保留最佳的特征。接下来的步骤再次尝试所有特征（现在包括之前的特征），并再次选择最佳特征。这个过程会一直重复，直到达到所需的特征数量。
- en: 'Greedy: Once a feature is included, it stays. This may lead to suboptimal results,
    as the feature providing the highest individual gain in early rounds might not
    be the best choice in the context of other feature interactions.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贪婪法：一旦一个特征被包含，它就会一直保留。这可能导致次优结果，因为在早期阶段提供最大单独增益的特征，可能在其他特征交互的上下文中并不是最好的选择。
- en: Feature Importance
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征重要性
- en: 'Finally, we’ll explore another straightforward selection strategy, which involves
    using the feature importances the model learns (if available). Certain models,
    like Random Forests, provide a measure of which features are most important for
    prediction. We can use these rankings to filter out those features that, according
    to the model, have the least importance. In this case, we train the model on the
    entire train dataset, and retain the 10 most important features:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将探讨另一种简单的特征选择策略，即使用模型学习到的特征重要性（如果有）。某些模型，如随机森林，提供了哪些特征对预测最重要的度量。我们可以利用这些排名来筛选掉那些模型认为重要性最小的特征。在这种情况下，我们在整个训练数据集上训练模型，并保留10个最重要的特征：
- en: '[PRE6]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/331f3e642958ee301da7619b9775d6e2.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/331f3e642958ee301da7619b9775d6e2.png)'
- en: Image by author
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Notice how, once again, *ram* is ranked highest, far above the second most important
    feature. Training with these 10 features, we obtain a test F1 score of almost
    0.883, similar to the ones we’ve been seeing. Also, note how the features selected
    through feature importance are the same as those selected using the chi-squared
    test, although they are ranked differently. This difference in ranking results
    in a slightly different outcome.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，*ram*再次被排名为最重要的特征，远远高于第二重要的特征。在使用这10个特征进行训练时，我们获得了接近0.883的测试F1分数，这与我们之前看到的分数相似。同时，注意通过特征重要性选择的特征与通过卡方检验选择的特征相同，尽管它们的排名不同。这种排名差异导致了稍微不同的结果。
- en: '![](../Images/723f66b75c094da32fcf5cc1c88cef56.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/723f66b75c094da32fcf5cc1c88cef56.png)'
- en: Image by author
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: '**Pros**:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**：'
- en: 'Easy and fast to implement: it requires a single training of the model and
    directly uses the derived feature importances.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现简单且快速：只需要对模型进行一次训练，直接使用得出的特征重要性。
- en: It can be adapted into a recursive version, in which at each step the least
    important feature is removed and the model is then trained again (see [Recursive
    Feature Elimination](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html)).
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以被改编成递归版本，其中在每一步中，最不重要的特征被移除，然后重新训练模型（见 [递归特征消除](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html)）。
- en: 'Contained within the model: If the model we are using provides feature importances,
    we already have a feature selection alternative available at no additional cost.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含在模型中：如果我们使用的模型提供了特征重要性，我们已经有了一个无需额外成本的特征选择方案。
- en: '**Cons**:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**：'
- en: Feature importance might not be aligned with our end goal. For instance, a feature
    might appear unimportant on its own but could be critical due to its interaction
    with other features. Also, an important feature might be counterproductive overall,
    by affecting the performance of other useful predictors.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征重要性可能与我们的最终目标不一致。例如，一个特征单独看可能不重要，但由于与其他特征的交互，它可能变得至关重要。另外，一个重要的特征可能会对整体产生负面影响，影响其他有用预测器的性能。
- en: Not all models offer feature importance estimation.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并非所有模型都提供特征重要性估计。
- en: Requires the number of features to select to be predefined.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要预先定义选择的特征数量。
- en: Closing Remarks
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结束语
- en: To conclude, we’ve seen how to use Optuna, a powerful optimization tool, for
    the feature selection task. By efficiently navigating the search space, it is
    able to find good feature subsets with relatively few trials. Not only that, but
    it is also flexible and can be adapted to many scenarios as long as we have a
    model and a loss function defined.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们已经看到如何使用一个强大的优化工具Optuna来进行特征选择任务。通过高效地探索搜索空间，它能够在相对较少的试验中找到良好的特征子集。不仅如此，它还非常灵活，只要我们定义了模型和损失函数，就可以适应许多不同的场景。
- en: Throughout our examples, we observed that all techniques yielded similar feature
    sets and results. This is mainly because the dataset we used is rather simple.
    In these cases, simpler methods already produce a good feature selection, so it
    wouldn’t make much sense to go with the Optuna approach. However, for more complex
    datasets, with more features and intricate relationships between them, using Optuna
    might be a good idea. So, all in all, given its relative ease of implementation
    and ability to deliver good results, using Optuna for feature selection is a worthwhile
    addition to the data scientist’s toolkit.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们观察到所有技术都产生了相似的特征集和结果。这主要是因为我们使用的数据集相对简单。在这些情况下，较简单的方法已经能够产生良好的特征选择，因此采用Optuna方法并没有太大意义。然而，对于更复杂的数据集，其中包含更多特征并且它们之间存在复杂关系，使用Optuna可能是一个不错的选择。因此，总的来说，鉴于其相对简单的实现方式和能够提供良好结果的能力，将Optuna用于特征选择是数据科学家工具包中值得加入的一个方法。
- en: Thanks for reading!
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: 'Colab Notebook: [https://colab.research.google.com/drive/193Jwb0xXWh_UkvwIiFgufKEYer-86RNA?usp=sharing](https://colab.research.google.com/drive/193Jwb0xXWh_UkvwIiFgufKEYer-86RNA?usp=sharing)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 'Colab Notebook: [https://colab.research.google.com/drive/193Jwb0xXWh_UkvwIiFgufKEYer-86RNA?usp=sharing](https://colab.research.google.com/drive/193Jwb0xXWh_UkvwIiFgufKEYer-86RNA?usp=sharing)'
