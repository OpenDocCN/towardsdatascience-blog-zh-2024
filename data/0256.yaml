- en: 'De-Coded: Understanding Context Windows for Transformer Models'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解码：理解 Transformer 模型的上下文窗口
- en: 原文：[https://towardsdatascience.com/de-coded-understanding-context-windows-for-transformer-models-cd1baca6427e?source=collection_archive---------4-----------------------#2024-01-27](https://towardsdatascience.com/de-coded-understanding-context-windows-for-transformer-models-cd1baca6427e?source=collection_archive---------4-----------------------#2024-01-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/de-coded-understanding-context-windows-for-transformer-models-cd1baca6427e?source=collection_archive---------4-----------------------#2024-01-27](https://towardsdatascience.com/de-coded-understanding-context-windows-for-transformer-models-cd1baca6427e?source=collection_archive---------4-----------------------#2024-01-27)
- en: Everything you need to know about h**ow context windows affect Transformer training
    and usage**
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你需要了解的关于**上下文窗口如何影响 Transformer 训练和使用**的所有内容
- en: '[](https://medium.com/@chris.p.hughes10?source=post_page---byline--cd1baca6427e--------------------------------)[![Chris
    Hughes](../Images/87b16cd8677739b12294380fb00fde85.png)](https://medium.com/@chris.p.hughes10?source=post_page---byline--cd1baca6427e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--cd1baca6427e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--cd1baca6427e--------------------------------)
    [Chris Hughes](https://medium.com/@chris.p.hughes10?source=post_page---byline--cd1baca6427e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@chris.p.hughes10?source=post_page---byline--cd1baca6427e--------------------------------)[![Chris
    Hughes](../Images/87b16cd8677739b12294380fb00fde85.png)](https://medium.com/@chris.p.hughes10?source=post_page---byline--cd1baca6427e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--cd1baca6427e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--cd1baca6427e--------------------------------)
    [Chris Hughes](https://medium.com/@chris.p.hughes10?source=post_page---byline--cd1baca6427e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--cd1baca6427e--------------------------------)
    ·9 min read·Jan 27, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--cd1baca6427e--------------------------------)
    ·9分钟阅读·2024年1月27日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: The context window is the maximum sequence length that a transformer can process
    at a time. With the rise of proprietary LLMs that limit the number of tokens and
    therefore the prompt size — as well as the growing interest in techniques such
    as [Retrieval Augmented Generation (RAG)](https://learn.microsoft.com/en-us/azure/search/retrieval-augmented-generation-overview)—
    understanding the key ideas around context windows and their implications is becoming
    increasingly important, as this is often cited when discussing different models.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文窗口是 Transformer 一次能够处理的最大序列长度。随着限制令牌数量以及因此限制提示大小的专有大型语言模型（LLM）的兴起，以及对[检索增强生成（RAG）](https://learn.microsoft.com/en-us/azure/search/retrieval-augmented-generation-overview)等技术日益增长的兴趣，理解上下文窗口及其影响的关键概念变得越来越重要，因为在讨论不同模型时，这一点常常被提及。
- en: The transformer architecture is a powerful tool for natural language processing,
    but it has some limitations when it comes to handling long sequences of text.
    In this article, we will explore how different factors affect the maximum context
    length that a transformer model can process, and whether bigger is always better
    when choosing a model for your task.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 架构是自然语言处理中的一项强大工具，但在处理长文本序列时存在一些局限性。在本文中，我们将探讨不同因素如何影响 Transformer
    模型能够处理的最大上下文长度，以及在选择模型时，是否更大的模型总是更好的问题。
- en: '![](../Images/116880da1f5943e6e5fd19677903c8d2.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/116880da1f5943e6e5fd19677903c8d2.png)'
- en: 'Image generated by the [Azure OpenAI Service DALL-E model](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#dall-e-preview)
    with the following prompt: “A robot reading a book. Hyper-realistic, HQ”'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 由[Azure OpenAI 服务 DALL-E 模型](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#dall-e-preview)生成的图像，提示语为：“一个机器人在读书。超现实，高质量”
- en: How many words can I fit into a Transformer?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我可以在 Transformer 中放入多少个词？
- en: 'At the time of writing, models such as the [Llama-2 variants](https://ai.meta.com/llama/)
    have a context length of 4k tokens, [GPT-4 turbo has 128k](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo),
    and [Claude 2.1 has 200k](https://www.anthropic.com/news/claude-2-1)! From the
    number of tokens alone, it can be difficult to envisage how this translates into
    words; whilst it depends on the tokenizer used, a good rule of thumb is that [100k
    tokens is approximately 75,000 words](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them).
    To put that in perspective, we can compare this to some popular literature:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，像[Llama-2变体](https://ai.meta.com/llama/)这样的模型具有4k标记的上下文长度，[GPT-4 Turbo具有128k标记](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo)，[Claude
    2.1具有200k标记](https://www.anthropic.com/news/claude-2-1)！仅从标记数来看，很难想象这如何转化为词汇；尽管这取决于所使用的分词器，但一个好的经验法则是，[100k标记大约等于75,000个词](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them)。为了让这一点更有意义，我们可以将其与一些流行的文学作品进行比较：
- en: 'The Lord of the Rings (J. R. R. Tolkien): 564,187 words, 752k tokens'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《指环王》（J.R.R.托尔金）：564,187个词，752k个标记
- en: 'Dracula (Bram Stoker): 165,453 words, 220k tokens'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《德古拉》（布莱姆·斯托克）：165,453个词，220k个标记
- en: 'Grimms’ Fairy Tales (Jacob Grimm and Wilhelm Grimm): 104,228 words, 139k tokens'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《格林童话》（雅各布·格林和威廉·格林）：104,228个词，139k个标记
- en: 'Frankenstein (Mary Shelley): 78,100 words, 104k tokens'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《弗兰肯斯坦》（玛丽·雪莱）：78,100个词，104k个标记
- en: 'Harry Potter and the Philosopher’s Stone (J. K. Rowling): 77,423 words, 103k
    tokens'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《哈利·波特与魔法石》（J.K.罗琳）：77,423个词，103k个标记
- en: 'Treasure Island (Robert Louis Stevenson): 72,036 words, 96k tokens'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《金银岛》（罗伯特·路易斯·史蒂文森）：72,036个词，96k个标记
- en: 'The War of the Worlds (H. G. Wells): 63,194 words, 84k tokens'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《世界大战》（赫伯特·乔治·威尔斯）：63,194个词，84k个标记
- en: 'The Hound of the Baskervilles (Arthur Conan Doyle): 62,297 words, 83k tokens'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《巴斯克维尔的猎犬》（阿瑟·柯南·道尔）：62,297个词，83k个标记
- en: 'The Jungle Book (Rudyard Kipling): 54,178 words, 72k tokens'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《丛林之书》（吉卜林）：54,178个词，72k个标记
- en: To summarise, 100k tokens is roughly equivalent to a short novel, whereas at
    200k we can almost fit the entirety of *Dracula*, a medium sized volume! To ingest
    a large volume, such as *The Lord of the Rings*, we would need 6 requests to GPT-4
    and only 4 calls to Claude 2!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，100k标记大约相当于一本短篇小说，而200k标记则几乎可以容纳完整的*《德古拉》*，一本中等大小的书！要处理大量文本，如*《指环王》*，我们需要6次请求GPT-4，而只需4次请求Claude
    2！
- en: '![](../Images/74512fba67439d02c2ac8f7ae5e81843.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/74512fba67439d02c2ac8f7ae5e81843.png)'
- en: 'Image generated by the [Azure OpenAI Service DALL-E model](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#dall-e-preview)
    with the following prompt: “A machine taking a bite out of a book. Hyper-realistic,
    HQ”'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 由[Azure OpenAI服务DALL-E模型](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#dall-e-preview)生成的图像，提示语为：“一台机器正在咬书。超现实主义，高质量”
- en: What determines the size of the context window?
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么决定了上下文窗口的大小？
- en: At this point, you may be wondering why some models have larger context windows
    than others.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你可能会想，为什么某些模型比其他模型具有更大的上下文窗口。
- en: To understand this, let’s first review how the attention mechanism works in
    the figure below; if you aren’t familiar with the details of attention, this is
    [covered in detail in my previous article](https://medium.com/towards-data-science/de-coded-transformers-explained-in-plain-english-877814ba6429).
    Recently, there have been several attention improvements and variants which aim
    to make this mechanism more efficient, but the key challenges remain the same.
    Here, will focus on the [original scaled dot-product attention](https://arxiv.org/abs/1706.03762).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这一点，我们首先来看下面的图示，回顾注意力机制是如何工作的；如果你不熟悉注意力的细节，可以参考我之前的文章，这里[详细介绍了该内容](https://medium.com/towards-data-science/de-coded-transformers-explained-in-plain-english-877814ba6429)。最近，已经有多个注意力机制的改进和变体，旨在提高该机制的效率，但核心挑战依然不变。这里，我们将重点关注[原始的缩放点积注意力](https://arxiv.org/abs/1706.03762)。
- en: '![](../Images/9e03c577b2cf03fdbe1557ceb871993c.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e03c577b2cf03fdbe1557ceb871993c.png)'
- en: The attention mechanism for decoder-only transformer models. The approach to
    positionally encoding only the Q and K inputs, applied inside each transformer
    layer, follows modern architectures such as LLama-2.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器仅Transformer模型的注意力机制。仅对Q和K输入进行位置编码的方式，在每个Transformer层内部应用，遵循了现代架构，如LLama-2。
- en: From the figure above, we can notice that the size of the matrix containing
    our attention scores is determined by the lengths of the sequences passed into
    the model and can grow arbitrarily large! Therefore, we can see that the context
    window is not determined by the architecture, but rather the length of the sequences
    that are given to the model during training.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从上图中，我们可以看到，包含注意力得分的矩阵大小由输入模型的序列长度决定，并且可以无限制地增大！因此，我们可以看出，上下文窗口的大小并不是由架构决定的，而是由训练时输入给模型的序列长度决定的。
- en: This calculation can be incredibly expensive to compute as, without any optimisations,
    matrix multiplications are generally quadratic in space complexity (O*(n^2)*).
    Put simply, this means that if the length of an input sequence doubles, the amount
    of memory required quadruples! Therefore, training a model on sequence lengths
    of 128k will require approximately 1024 times the memory compared to training
    on sequence lengths of 4k!
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这种计算可能非常昂贵，因为在没有任何优化的情况下，矩阵乘法通常在空间复杂度上是二次的（O*(n^2)*）。简而言之，这意味着如果输入序列的长度加倍，所需的内存量将增加四倍！因此，训练一个长度为
    128k 的序列模型将需要大约 1024 倍的内存，相较于训练长度为 4k 的序列模型！
- en: It is also important to keep in mind that this operation is repeated for every
    layer and every head of the transformer, which results in a significant amount
    of computation. As the amount of GPU memory available is also shared with the
    parameters of the model, any computed gradients, and a reasonable sized batch
    of input data, hardware can quickly become a bottleneck on the size of the context
    window when training large models.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，这一操作会针对每一层和每一个头部在 Transformer 中重复进行，这会导致大量的计算。由于 GPU 内存的可用空间还需要与模型的参数、计算出的梯度以及合适大小的输入数据批次共享，因此在训练大型模型时，硬件很快会成为上下文窗口大小的瓶颈。
- en: '**Can we extend the context window of a pre-trained model?**'
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**我们可以扩展预训练模型的上下文窗口吗？**'
- en: After understanding the computational challenges of training models on longer
    sequence lengths, it may be tempting to train a model on short sequences, with
    the hope that this will generalise to longer contexts.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解了训练模型时面临的长序列长度的计算挑战后，可能会有诱惑想要在短序列上训练模型，寄希望于这种方法能推广到更长的上下文。
- en: '![](../Images/46c89c7d81cb9963b36091136d95d577.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/46c89c7d81cb9963b36091136d95d577.png)'
- en: 'Image generated by the [Azure OpenAI Service DALL-E model](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#dall-e-preview)
    with the following prompt: “A robot holding a spanner. Hyper-realistic, HQ”'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 由[Azure OpenAI 服务 DALL-E 模型](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#dall-e-preview)生成的图像，提示词为：“一只机器人手持扳手。超现实主义，高清。”
- en: One obstacle to this is the positional encoding mechanism, used to enable transformers
    to capture the position of tokens in a sequence. In the [original paper](https://arxiv.org/abs/1706.03762),
    two strategies for positional encoding were proposed. The first was to use learnable
    embeddings specific to each position in the sequence, which are clearly unable
    to generalise past the maximum sequence length that the model was trained on.
    However, the authors hypothesised that their preferred sinusoidal approach may
    extrapolate to longer sequences; subsequent research has [demonstrated that this
    is not the case](https://openreview.net/pdf?id=R8sQPpGCv0).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一个障碍是位置编码机制，它被用来使 Transformer 捕捉序列中标记的位置。在[原始论文](https://arxiv.org/abs/1706.03762)中，提出了两种位置编码策略。第一种是使用针对序列中每个位置的可学习嵌入，这显然无法推广到模型训练时未见过的最大序列长度。然而，作者假设他们偏好的正弦方法可能会对更长的序列进行外推；后续的研究[证明了事实并非如此](https://openreview.net/pdf?id=R8sQPpGCv0)。
- en: In many recent transformer models such as PaLM and Llama-2, absolute positional
    encodings have been replaced by relative positional encodings, such as [RoPE](https://arxiv.org/abs/2104.09864),
    which aim to preserve the relative distance between tokens after encodings. Whilst
    these are slightly better at generalising to longer sequences than previous approaches,
    [performance quickly breaks down for sequence lengths significantly longer than
    the model has seen before](https://openreview.net/pdf?id=R8sQPpGCv0).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多近期的 Transformer 模型中，如 PaLM 和 Llama-2，绝对位置编码已被相对位置编码所取代，例如[RoPE](https://arxiv.org/abs/2104.09864)，其目标是在编码后保持标记之间的相对距离。虽然这些方法比以前的方法在推广到更长的序列时稍微更好，但[对于模型未见过的远超其训练序列长度的序列，性能很快会崩溃](https://openreview.net/pdf?id=R8sQPpGCv0)。
- en: Whilst there are several approaches that aim to [change](https://openreview.net/pdf?id=R8sQPpGCv0)
    or [remove positional encodings completely](https://arxiv.org/abs/2203.16634),
    these require fundamental changes to the transformer architecture, and would require
    models to be retrained, which is highly expensive and time consuming. As many
    of the top performing open-source models at the time of writing, are [derived
    from pretrained versions of Llama-2](https://arxiv.org/abs/2310.06825), there
    is a [lot of active research taking place](https://arxiv.org/pdf/2401.07872.pdf)
    into how to extend the context length of existing model which use RoPE embeddings,
    with varying success.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有几种方法旨在[改变](https://openreview.net/pdf?id=R8sQPpGCv0)或[完全移除位置编码](https://arxiv.org/abs/2203.16634)，但这些方法需要对变压器架构进行根本性改变，并且需要重新训练模型，这既昂贵又耗时。由于在本文撰写时，许多顶尖的开源模型都是[基于Llama-2的预训练版本](https://arxiv.org/abs/2310.06825)，因此目前有很多[积极的研究](https://arxiv.org/pdf/2401.07872.pdf)正在进行，旨在扩展使用RoPE嵌入的现有模型的上下文长度，取得了不同程度的成功。
- en: Many of these approaches employ some variation of interpolating the input sequence;
    scaling the positional embeddings so that they fit within the original context
    window of the model. The intuition behind this is that it should be easier for
    the model fill in the gaps between words, rather than trying to predict what comes
    after the words.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法中的许多都使用了一些变体，通过插值输入序列；缩放位置嵌入，使其适应模型原始的上下文窗口。其直觉是，模型应该更容易填补单词之间的空隙，而不是试图预测单词之后的内容。
- en: '![](../Images/b3abce543f5d19787b52a3d16a89cb15.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3abce543f5d19787b52a3d16a89cb15.png)'
- en: 'With interpolation methods, the objective is not to make the model extrapolate
    to longer sequences, but to create intermediate positions within the model’s current
    sequence length. Image Source: [[2306.15595] Extending Context Window of Large
    Language Models via Positional Interpolation (arxiv.org)](https://arxiv.org/abs/2306.15595)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用插值方法时，目标不是让模型推断出更长的序列，而是创造出模型当前序列长度中的中间位置。图片来源：[[2306.15595] 通过位置插值扩展大语言模型的上下文窗口
    (arxiv.org)](https://arxiv.org/abs/2306.15595)
- en: One such approach, known as [YaRN](https://arxiv.org/pdf/2309.00071.pdf), was
    able to extend the context window of the Llama-2 7B and 13B models to 128k without
    a significant degradation in performance!
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一种名为[YaRN](https://arxiv.org/pdf/2309.00071.pdf)的方法，成功将Llama-2的7B和13B模型的上下文窗口扩展到128k，而且没有明显的性能下降！
- en: Whilst a definitive approach that works well in all contexts has yet to emerge,
    this remains an exciting area of research, with big potential implications!
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管一种在所有情境下都能有效工作的确定性方法尚未出现，但这仍然是一个令人兴奋的研究领域，具有巨大的潜在影响！
- en: '**Are longer context windows always better?**'
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**更长的上下文窗口总是更好吗？**'
- en: Now that we understand some of the practical challenges around training models
    on longer sequence lengths, and some potential mitigations to overcome this, we
    can ask another question — is this extra effort worth it? At first glance, the
    answer may seem obvious; providing more information to a model should make it
    easier to inject new knowledge and reduce hallucinations, making it more useful
    in almost every conceivable application. However, things are not so simple.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了在更长序列长度上训练模型的一些实际挑战，以及为克服这些挑战而采取的一些潜在缓解措施，我们可以提出另一个问题——这些额外的努力值得吗？乍一看，答案似乎显而易见；为模型提供更多信息应该能够更容易地注入新知识并减少幻觉，使其在几乎所有可以想象的应用中都更有用。然而，事情并非如此简单。
- en: '![](../Images/140257e12476d89bce6dfaf58f4baf85.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/140257e12476d89bce6dfaf58f4baf85.png)'
- en: 'Image generated by the [Azure OpenAI Service DALL-E model](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#dall-e-preview)
    with the following prompt: “Two robots, each holding a book, comparing the sizes
    of the books. Hyper-realistic, HQ”'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 该图片由[Azure OpenAI服务的DALL-E模型](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#dall-e-preview)生成，使用的提示词是：“两位机器人，每人拿着一本书，比较书本的大小。超现实主义，高质量”
- en: 'In the 2023 paper [*Lost in the Middle*](https://arxiv.org/abs/2307.03172)*,*
    researchers at Stanford and Berkley investigated how models use and access information
    provided in their context window, and concluded the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在2023年的论文[*迷失在中间*](https://arxiv.org/abs/2307.03172)*,* 斯坦福大学和伯克利的研究人员调查了模型如何使用和访问其上下文窗口中提供的信息，并得出了以下结论：
- en: “*We find that changing the position of relevant information in the input context
    can substantially affect model performance, indicating that current language models
    do not robustly access and use information in long input contexts”.*
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “*我们发现，改变输入上下文中相关信息的位置可以显著影响模型性能，这表明当前的语言模型在长输入上下文中并不能稳健地访问和使用信息。*”
- en: For their experiments, the authors created a dataset where, for each query,
    they had a document that contains the answer and *k — 1* distractor documents
    which did not contain the answer; adjusting the input context length by changing
    the number of retrieved documents that do not contain the answer. They then modulated
    the position of relevant information within the input context by changing the
    order of the documents to place the relevant document at the beginning, middle
    or end of the context, and evaluated whether any of the correct answers appear
    in the predicted output.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于他们的实验，作者创建了一个数据集，在该数据集中，对于每个查询，包含一个答案的文档和*k — 1*个不包含答案的干扰文档；通过调整输入上下文长度，改变检索到的、不包含答案的文档数量。他们随后通过改变文档的顺序来调整相关信息在输入上下文中的位置，将相关文档放在上下文的开始、中间或末尾，并评估是否有任何正确的答案出现在预测的输出中。
- en: Specifically, they observed that the models studied performed the best when
    the relevant information was found at the start or the end of the context window;
    when the information required was in the middle of the context, performance significantly
    decreased.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，他们观察到，当相关信息位于上下文窗口的开始或末尾时，研究中的模型表现最佳；而当所需信息位于上下文的中间时，性能显著下降。
- en: In theory, the self-attention mechanism in a Transformer enables the model to
    consider all parts of the input when generating the next word, regardless of their
    position in the sequence. As such, I believe that any biases that the model has
    learned about where to find important information is more likely to come from
    the training data than the architecture. We can explore this idea further by examining
    the results that the authors observed when evaluating the Llama-2 family of models
    on the accuracy or retrieving documents based on their position, which are presented
    in the figure below.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从理论上讲，Transformer中的自注意力机制使得模型在生成下一个词时能够考虑输入的所有部分，而不管它们在序列中的位置。因此，我认为模型关于如何定位重要信息的任何偏见更可能来自训练数据，而非架构。我们可以通过进一步研究作者在评估Llama-2系列模型时观察到的结果，来探讨这个想法，这些结果涉及根据文档位置检索的准确性，展示在下图中。
- en: '![](../Images/3a5cc3f84c4fb6197cd4d36b804a3432.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a5cc3f84c4fb6197cd4d36b804a3432.png)'
- en: 'Image source: [[2307.03172] Lost in the Middle: How Language Models Use Long
    Contexts (arxiv.org)](https://arxiv.org/abs/2307.03172)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '图像来源：[[2307.03172] Lost in the Middle: How Language Models Use Long Contexts
    (arxiv.org)](https://arxiv.org/abs/2307.03172)'
- en: Looking at the base models, we can clearly observe the authors’ conclusions
    for the Llama-2 13B and 70B models. Interestingly, for the 7B model, we can see
    that it relies almost exclusively on the end of the context; as a lot of unsupervised
    finetuning is on streams of data scraped from various sources, when the model
    has relatively few parameters to dedicate to predicting the next word in an ever-changing
    context, it makes sense to focus on the most recent tokens!
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 看着基础模型，我们可以清楚地观察到作者对于Llama-2 13B和70B模型的结论。有趣的是，对于7B模型，我们可以看到它几乎完全依赖于上下文的末尾；由于大量的无监督微调是在从各种来源抓取的数据流上进行的，当模型的参数相对较少，无法在不断变化的上下文中预测下一个词时，专注于最新的标记是有意义的！
- en: The bigger models also perform well when the relevant information is at the
    beginning of the text; suggesting that they learn to focus more on the start of
    the text as they get more parameters. The authors hypothesise that this is because,
    during pre-training, the models see a lot of data from sources like StackOverflow
    which start with important information. I doubt the 13B model’s slight advantage
    with front-loaded information is significant, as the accuracy is similar in both
    cases and the 70B model does not show this pattern.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 更大的模型在相关信息位于文本开头时表现也很好；这表明，随着模型参数的增加，它们学习更多地关注文本的开头。作者假设这是因为，在预训练过程中，模型会看到来自像StackOverflow这样的数据源，它们通常从重要信息开始。我怀疑13B模型在前置信息上的轻微优势并不显著，因为两种情况的准确性相似，而且70B模型并没有展示出这种模式。
- en: The ‘chat’ models are trained further with instruction tuning and RLHF, and
    they perform better overall, and also seem to become less sensitive to the position
    of the relevant information in the text. This is more clear for the 13B model,
    and less for the 70B model. The 7B model does not change much, perhaps because
    it has fewer parameters. This could mean that these models learn to use information
    from other parts of the text better after more training, but they still prefer
    the most recent information. Given that the subsequent training stages are significantly
    shorter, they have not completely overcome have the biases from the first unsupervised
    training; I suspect that the 70B model may require a larger, more diverse subsequent
    training to exhibit a similar magnitude of change as in the performance of the
    13B model observed here.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: “聊天”模型经过进一步的指令调优和强化学习（RLHF）训练，整体表现更好，并且似乎对文本中相关信息的位置变得不那么敏感。对于13B模型，这种变化更为明显，而对于70B模型则不太明显。7B模型变化不大，可能是因为它的参数较少。这可能意味着这些模型在更多的训练后，能够更好地利用文本其他部分的信息，但它们仍然偏好最近的信息。考虑到后续训练阶段明显较短，它们尚未完全克服第一次无监督训练的偏差；我怀疑70B模型可能需要更大且更多样化的后续训练，才能表现出与13B模型在此观察到的表现类似的变化幅度。
- en: Additionally, I would be interested in an investigation which explores the position
    of the relevant information in the text in the datasets used for SFT. As [humans
    exhibit similar behaviour](https://www.verywellmind.com/understanding-the-primacy-effect-4685243)
    of being better at recalling information at the start and end of sequences, it
    would not be surprising if this behaviour is mirrored in a lot of the examples
    given.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我对一项研究感兴趣，该研究探讨了在用于 SFT 的数据集中相关信息的位置。正如 [人类表现出类似行为](https://www.verywellmind.com/understanding-the-primacy-effect-4685243)，即在序列的开始和结束部分更容易回忆起信息，因此，如果这种行为在给定的许多示例中有所体现，也不足为奇。
- en: Conclusion
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: To summarise, the context window is not fixed and can grow as large as you want
    it to, provided there is enough memory available! However, longer sequences mean
    more computation — which also result in the model being slower — and unless the
    model has been trained on sequences of a similar length, the output may not make
    much sense! However, even for models with large context windows, there is no guarantee
    that they will effectively use all of the information provided to them — there
    really is [no free lunch](https://en.wikipedia.org/wiki/No_free_lunch_theorem)!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，上下文窗口并非固定，可以根据需要扩大，只要有足够的内存可用！然而，较长的序列意味着更多的计算——这也导致模型变慢——并且除非模型已经在类似长度的序列上进行过训练，否则输出可能没有太大意义！然而，即便是拥有大上下文窗口的模型，也不能保证它们会有效利用提供给它们的所有信息——这确实是
    [没有免费的午餐](https://en.wikipedia.org/wiki/No_free_lunch_theorem)！
- en: '[*Chris Hughes*](https://www.linkedin.com/in/chris-hughes1/) *is on LinkedIn*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[*Chris Hughes*](https://www.linkedin.com/in/chris-hughes1/) *在 LinkedIn 上*'
- en: Unless otherwise stated, all images were created by the author.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图片均由作者创作。
- en: References
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[RAG and generative AI — Azure AI Search | Microsoft Learn](https://learn.microsoft.com/en-us/azure/search/retrieval-augmented-generation-overview)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[RAG 和生成式 AI — Azure AI 搜索 | Microsoft Learn](https://learn.microsoft.com/en-us/azure/search/retrieval-augmented-generation-overview)'
- en: '[Llama 2 — Meta AI](https://ai.meta.com/llama/)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Llama 2 — Meta AI](https://ai.meta.com/llama/)'
- en: '[Models — OpenAI API](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[模型 — OpenAI API](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo)'
- en: '[Introducing Claude 2.1 \ Anthropic](https://www.anthropic.com/news/claude-2-1)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[介绍 Claude 2.1 \ Anthropic](https://www.anthropic.com/news/claude-2-1)'
- en: '[What are tokens and how to count them? | OpenAI Help Center](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[什么是 token，如何计算它们？| OpenAI 帮助中心](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them)'
- en: '[De-coded: Transformers explained in plain English | by Chris Hughes | Towards
    Data Science](/de-coded-transformers-explained-in-plain-english-877814ba6429)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[解码：用通俗英语解释 Transformers | Chris Hughes | Towards Data Science](/de-coded-transformers-explained-in-plain-english-877814ba6429)'
- en: '[[1706.03762] Attention Is All You Need (arxiv.org)](https://arxiv.org/abs/1706.03762)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[[1706.03762] 注意力即一切 (arxiv.org)](https://arxiv.org/abs/1706.03762)'
- en: '[Train Short, Test Long: Attention with Linear Biases enables input length
    extrapolation (openreview.net)](https://openreview.net/pdf?id=R8sQPpGCv0)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[短期训练，长期测试：具有线性偏差的注意力机制使得输入长度的外推成为可能 (openreview.net)](https://openreview.net/pdf?id=R8sQPpGCv0)'
- en: '[[2203.16634] Transformer Language Models without Positional Encodings Still
    Learn Positional Information (arxiv.org)](https://arxiv.org/abs/2203.16634)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[[2203.16634] 没有位置编码的变换器语言模型仍能学习位置相关信息](https://arxiv.org/abs/2203.16634)'
- en: '[[2310.06825] Mistral 7B (arxiv.org)](https://arxiv.org/abs/2310.06825)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[[2310.06825] Mistral 7B](https://arxiv.org/abs/2310.06825)'
- en: '[[2401.07872] The What, Why, and How of Context Length Extension Techniques
    in Large Language Models — A Detailed Survey (arxiv.org)](https://arxiv.org/abs/2401.07872)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[[2401.07872] 大语言模型中上下文长度扩展技术的什么、为什么及如何 — 详细调查](https://arxiv.org/abs/2401.07872)'
- en: '[[2306.15595] Extending Context Window of Large Language Models via Positional
    Interpolation (arxiv.org)](https://arxiv.org/abs/2306.15595)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[[2306.15595] 通过位置插值扩展大语言模型的上下文窗口](https://arxiv.org/abs/2306.15595)'
- en: '[[2309.00071] YaRN: Efficient Context Window Extension of Large Language Models
    (arxiv.org)](https://arxiv.org/abs/2309.00071)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[[2309.00071] YaRN：大语言模型的高效上下文窗口扩展](https://arxiv.org/abs/2309.00071)'
- en: '[[2307.03172] Lost in the Middle: How Language Models Use Long Contexts (arxiv.org)](https://arxiv.org/abs/2307.03172)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[[2307.03172] 在中间迷失：语言模型如何使用长上下文](https://arxiv.org/abs/2307.03172)'
- en: '[Primary Effect: Meaning, How It Works (verywellmind.com)](https://www.verywellmind.com/understanding-the-primacy-effect-4685243)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[首因效应：含义及其工作原理](https://www.verywellmind.com/understanding-the-primacy-effect-4685243)'
- en: '[No free lunch theorem — Wikipedia](https://en.wikipedia.org/wiki/No_free_lunch_theorem)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[无免费午餐定理 — 维基百科](https://en.wikipedia.org/wiki/No_free_lunch_theorem)'
