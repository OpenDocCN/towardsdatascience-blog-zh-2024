- en: How to build an OpenAI-compatible API
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何构建一个 OpenAI 兼容的 API
- en: 原文：[https://towardsdatascience.com/how-to-build-an-openai-compatible-api-87c8edea2f06?source=collection_archive---------0-----------------------#2024-03-24](https://towardsdatascience.com/how-to-build-an-openai-compatible-api-87c8edea2f06?source=collection_archive---------0-----------------------#2024-03-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-build-an-openai-compatible-api-87c8edea2f06?source=collection_archive---------0-----------------------#2024-03-24](https://towardsdatascience.com/how-to-build-an-openai-compatible-api-87c8edea2f06?source=collection_archive---------0-----------------------#2024-03-24)
- en: Create a server to replicate OpenAI’s Chat Completions API, enabling any LLM
    to integrate with tools written for the OpenAI API
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个服务器来复制 OpenAI 的 Chat Completions API，使任何 LLM 都能与为 OpenAI API 编写的工具集成
- en: '[](https://medium.com/@saarb?source=post_page---byline--87c8edea2f06--------------------------------)[![Saar
    Berkovich](../Images/8a834597e8c6cce1b948f6aa17bfe8be.png)](https://medium.com/@saarb?source=post_page---byline--87c8edea2f06--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--87c8edea2f06--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--87c8edea2f06--------------------------------)
    [Saar Berkovich](https://medium.com/@saarb?source=post_page---byline--87c8edea2f06--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@saarb?source=post_page---byline--87c8edea2f06--------------------------------)[![Saar
    Berkovich](../Images/8a834597e8c6cce1b948f6aa17bfe8be.png)](https://medium.com/@saarb?source=post_page---byline--87c8edea2f06--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--87c8edea2f06--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--87c8edea2f06--------------------------------)
    [Saar Berkovich](https://medium.com/@saarb?source=post_page---byline--87c8edea2f06--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--87c8edea2f06--------------------------------)
    ·6 min read·Mar 24, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--87c8edea2f06--------------------------------)
    ·阅读时长 6 分钟·2024 年 3 月 24 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/3678fe82d7cec7940e7288da517234fa.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3678fe82d7cec7940e7288da517234fa.png)'
- en: Image generated by the author using OpenAI DALL-E
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用 OpenAI DALL-E 生成
- en: It is early 2024, and the Gen AI market is being dominated by [OpenAI](https://iot-analytics.com/leading-generative-ai-companies/).
    For good reasons, too — they have the first mover’s advantage, being the first
    to provide an easy-to-use API for an LLM, and they also offer arguably the most
    capable LLM to date, GPT 4\. Given that this is the case, developers of all sorts
    of tools ([agents](https://docs.agpt.co/autogpt/), [personal assistants](https://github.com/QuivrHQ/quivr),
    [coding extensions](https://github.com/jupyterlab/jupyter-ai)), have turned to
    OpenAI for their LLM needs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是 2024 年初，生成型 AI 市场由 [OpenAI](https://iot-analytics.com/leading-generative-ai-companies/)
    主导。原因很简单——他们具有先发优势，是第一个提供易用 API 的 LLM 提供商，而且他们提供的 GPT-4 可能是迄今为止最强大的 LLM。鉴于此，各种工具的开发者（如
    [代理程序](https://docs.agpt.co/autogpt/)、[个人助手](https://github.com/QuivrHQ/quivr)、[编码扩展](https://github.com/jupyterlab/jupyter-ai)）纷纷转向
    OpenAI 以满足他们的 LLM 需求。
- en: While there are many reasons to fuel your Gen AI creations with OpenAI’s GPT,
    there are plenty of reasons to opt for an alternative. Sometimes, it might be
    less cost-efficient, and at other times your data privacy policy may prohibit
    you from using OpenAI, or maybe you’re hosting an open-source LLM (or your own).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有许多理由让你使用 OpenAI 的 GPT 来推动你的生成型 AI 创作，但也有许多理由选择其他替代方案。有时候，使用 OpenAI 可能不够具有成本效益，而有时候你的数据隐私政策可能禁止你使用
    OpenAI，或者你可能在托管一个开源的 LLM（或者你自己的 LLM）。
- en: OpenAI’s market dominance means that many of the tools you might want to use
    only support the OpenAI API. Gen AI & LLM providers like OpenAI, Anthropic, and
    Google all seem to creating different API schemas (perhaps intentionally), which
    adds a lot of extra work for devs who want to support all of them.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 的市场主导地位意味着许多你可能想要使用的工具仅支持 OpenAI API。像 OpenAI、Anthropic 和 Google 这样的生成型
    AI 和 LLM 提供商似乎在创建不同的 API 模式（可能是故意的），这增加了开发者为支持所有这些平台而需做的额外工作。
- en: So, as a quick weekend project, I decided to implement a Python [FastAPI](https://fastapi.tiangolo.com/)
    server that is compatible with the OpenAI API specs, so that you can wrap virtually
    any LLM you like (either managed like Anthropic’s Claude, or self-hosted) to mimic
    the OpenAI API. Thankfully, the OpenAI API specs, have a `base_url` parameter
    you can set to effectively point the client to your server, instead of OpenAI’s
    servers, and most of the developers of aforementioned tools allow you to set this
    parameter to your liking.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，作为一个快速的周末项目，我决定实现一个兼容 OpenAI API 规范的 Python [FastAPI](https://fastapi.tiangolo.com/)
    服务器，目的是让你可以将几乎任何你喜欢的 LLM（无论是像 Anthropic 的 Claude 这样的托管模型，还是自托管的模型）包装起来，以模仿 OpenAI
    API。幸运的是，OpenAI API 规范中有一个 `base_url` 参数，可以将其设置为指向你的服务器，而不是 OpenAI 的服务器，大多数上述工具的开发者允许你根据需要设置这个参数。
- en: To do this, I’ve followed OpenAI’s Chat API reference openly available [here](https://platform.openai.com/docs/api-reference/chat),
    with some help from the code of [vLLM](https://github.com/vllm-project/vllm),
    an Apache-2.0 licensed inference server for LLMs that also offers OpenAI API compatibility.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我参照了 OpenAI 的聊天 API 参考文档，公开可用的[链接在此](https://platform.openai.com/docs/api-reference/chat)，并在[
    vLLM](https://github.com/vllm-project/vllm)的代码帮助下进行了一些修改。vLLM 是一个 Apache-2.0 许可证下的推理服务器，支持
    LLMs，同时兼容 OpenAI API。
- en: Game Plan
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 游戏计划
- en: We will be building a mock API that mimics the way OpenAI’s Chat Completion
    API (`/v1/chat/completions`) works. While this implementation is in Python and
    uses FastAPI, I kept it quite simple so that it can be easily transferable to
    another modern coding language like TypeScript or Go. We will be using the Python
    official [OpenAI client library](https://github.com/openai/openai-python) to test
    it — the idea is that if we can get the library to think our server is OpenAI,
    we can get any program that uses it to think the same.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个模拟的 API，模仿 OpenAI 的聊天完成 API（`/v1/chat/completions`）的工作方式。尽管这个实现是用 Python
    和 FastAPI 编写的，我将其保持得非常简单，以便能够轻松迁移到像 TypeScript 或 Go 这样的其他现代编程语言。我们将使用 Python 官方的[OpenAI
    客户端库](https://github.com/openai/openai-python)进行测试——我们的想法是，如果我们能让库认为我们的服务器是 OpenAI
    的服务器，那么任何使用该库的程序也会认为是同样的。
- en: First step — chat completions API, no streaming
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一步 — 聊天完成 API，非流式处理
- en: 'We’ll start with implementing the non-streaming bit. Let’s start with modeling
    our request:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从实现非流式部分开始。首先建模我们的请求：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The [PyDantic](https://docs.pydantic.dev/latest/) model represents the request
    from the client, aiming to replicate the API reference. For the sake of brevity,
    this model does not implement the entire specs, but rather the bare bones needed
    for it to work. If you’re missing a parameter that is a part of the [API specs](https://platform.openai.com/docs/api-reference/chat)
    (like `top_p`), you can simply add it to the model.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[PyDantic](https://docs.pydantic.dev/latest/) 模型代表了来自客户端的请求，旨在复制 API 参考。为了简洁起见，这个模型没有实现完整的规范，而是实现了使其正常工作的基本部分。如果你缺少一个属于[API
    规范](https://platform.openai.com/docs/api-reference/chat)的参数（比如 `top_p`），你只需将其添加到模型中。'
- en: The `ChatCompletionRequest` models the parameters OpenAI uses in their requests.
    The chat API specs require specifying a list of `ChatMessage` (like a chat history,
    the client is usually in charge of keeping it and feeding back in at every request).
    Each chat message has a `role` attribute (usually `system`, `assistant` , or `user`
    ) and a `content` attribute containing the actual message text.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`ChatCompletionRequest` 模型化了 OpenAI 在请求中使用的参数。聊天 API 规范要求指定一个 `ChatMessage`
    列表（类似于聊天历史，通常由客户端负责保持并在每次请求时反馈）。每个聊天消息有一个 `role` 属性（通常是 `system`、`assistant` 或
    `user`）和一个 `content` 属性，包含实际的消息文本。'
- en: 'Next, we’ll write our FastAPI chat completions endpoint:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将编写 FastAPI 聊天完成端点：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: That simple.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 就这么简单。
- en: Testing our implementation
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试我们的实现
- en: 'Assuming both code blocks are in a file called `main.py`, we’ll install two
    Python libraries in our environment of choice (always best to create a new one):
    `pip install fastapi openai` and launch the server from a terminal:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 假设这两个代码块位于名为 `main.py` 的文件中，我们将在选择的环境中安装两个 Python 库（最好创建一个新的环境）：`pip install
    fastapi openai`，并通过终端启动服务器：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Using another terminal (or by launching the server in the background), we will
    open a Python console and copy-paste the following code, taken straight from [OpenAI’s
    Python Client Reference](https://github.com/openai/openai-python#Usage):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 使用另一个终端（或在后台启动服务器），我们将打开一个 Python 控制台，并复制粘贴以下代码，这些代码直接来自[OpenAI 的 Python 客户端参考](https://github.com/openai/openai-python#Usage)：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If you’ve done everything correctly, the response from the server should be
    correctly printed. It’s also worth inspecting the `chat_completion` object to
    see that all relevant attributes are as sent from our server. You should see something
    like this:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你一切操作正确，服务器的响应应该会正确打印。检查`chat_completion`对象也很有价值，可以确认所有相关属性是否与我们服务器发送的一致。你应该能看到类似下面的内容：
- en: '![](../Images/b41ac9b9749709c7180ac0bf9d59a281.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b41ac9b9749709c7180ac0bf9d59a281.png)'
- en: Code by the author, formatted using [Carbon](https://carbon.now.sh/)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者编写的代码，使用[Carbon](https://carbon.now.sh/)格式化
- en: Leveling up — supporting streaming
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 升级——支持流式传输
- en: As LLM generation tends to be slow (computationally expensive), it’s worth streaming
    your generated content back to the client, so that the user can see the response
    as it’s being generated, without having to wait for it to finish. If you recall,
    we gave `ChatCompletionRequest` a boolean `stream` property — this lets the client
    request that the data be streamed back to it, rather than sent at once.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLM生成通常较慢（计算开销大），将生成的内容流式传输回客户端是值得的，这样用户可以在生成过程中看到响应，而不必等到它完全生成完毕。如果你记得，我们给`ChatCompletionRequest`添加了一个布尔型的`stream`属性——这让客户端可以请求将数据流式传输回去，而不是一次性发送。
- en: This makes things just a bit more complex. We will create a [generator function](https://wiki.python.org/moin/Generators)
    to wrap our mock response (in a real-world scenario, we will want a generator
    that is hooked up to our LLM generation)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得事情变得稍微复杂了一些。我们将创建一个[生成器函数](https://wiki.python.org/moin/Generators)来包装我们的模拟响应（在实际场景中，我们将需要一个连接到LLM生成的生成器）
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: And now, we would modify our original endpoint to return a StreamingResponse
    when `stream==True`
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将修改原始的端点，当`stream==True`时返回一个StreamingResponse
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Testing the streaming implementation
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试流式传输实现
- en: After restarting the uvicorn server, we’ll open up a Python console and put
    in this code (again, taken from OpenAI’s library docs)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 重启uvicorn服务器后，我们将打开Python控制台并输入这段代码（同样是来自OpenAI的文档）
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You should see each word in the server’s response being slowly printed, mimicking
    token generation. We can inspect the last `chunk` object to see something like
    this:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到服务器响应中的每个单词都被慢慢打印出来，模拟标记生成。我们可以检查最后一个`chunk`对象，看到类似下面的内容：
- en: '![](../Images/ad32225e29fca2ff7ea30226264ff67f.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad32225e29fca2ff7ea30226264ff67f.png)'
- en: Code by the author, formatted using [Carbon](https://carbon.now.sh/)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者编写的代码，使用[Carbon](https://carbon.now.sh/)格式化
- en: Putting it all together
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将所有内容整合起来
- en: Finally, in the gist below, you can see the entire code for the server.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在下面的代码片段中，你可以看到服务器的完整代码。
- en: Final Notes
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最后的注意事项
- en: There are many other interesting things we can do here, like supporting other
    request parameters, and other OpenAI abstractions like Function Calls and the
    Assistant API.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里我们可以做很多其他有趣的事情，比如支持其他请求参数，和其他OpenAI的抽象，如函数调用和助手API。
- en: The lack of standardization in LLM APIs makes it difficult to switch providers,
    for both companies and developers of LLM-wrapping packages. In the absence of
    any standard, the approach I’ve taken here is to abstract the LLM behind the specs
    of the biggest and most mature API.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM API 缺乏标准化使得切换提供商变得困难，既对于公司也对于开发LLM封装包的开发者而言。在没有任何标准的情况下，我在这里采取的做法是将LLM抽象化，基于最大且最成熟的API的规格进行封装。
