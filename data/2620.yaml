- en: Running the STORM AI Research System with Your Local Documents
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æœ¬åœ°æ–‡æ¡£è¿è¡ŒSTORM AIç ”ç©¶ç³»ç»Ÿ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/running-the-storm-ai-research-system-with-your-local-documents-e413ea2ae064?source=collection_archive---------3-----------------------#2024-10-28](https://towardsdatascience.com/running-the-storm-ai-research-system-with-your-local-documents-e413ea2ae064?source=collection_archive---------3-----------------------#2024-10-28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/running-the-storm-ai-research-system-with-your-local-documents-e413ea2ae064?source=collection_archive---------3-----------------------#2024-10-28](https://towardsdatascience.com/running-the-storm-ai-research-system-with-your-local-documents-e413ea2ae064?source=collection_archive---------3-----------------------#2024-10-28)
- en: AI assisted research using FEMA disaster response documents
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨AIè¾…åŠ©ç ”ç©¶FEMAç¾éš¾å“åº”æ–‡æ¡£
- en: '[](https://medium.com/@astrobagel?source=post_page---byline--e413ea2ae064--------------------------------)[![Matthew
    Harris](../Images/4fa3264bb8a028633cd8d37093c16214.png)](https://medium.com/@astrobagel?source=post_page---byline--e413ea2ae064--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e413ea2ae064--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e413ea2ae064--------------------------------)
    [Matthew Harris](https://medium.com/@astrobagel?source=post_page---byline--e413ea2ae064--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@astrobagel?source=post_page---byline--e413ea2ae064--------------------------------)[![Matthew
    Harris](../Images/4fa3264bb8a028633cd8d37093c16214.png)](https://medium.com/@astrobagel?source=post_page---byline--e413ea2ae064--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e413ea2ae064--------------------------------)[![æ•°æ®ç§‘å­¦å‰æ²¿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e413ea2ae064--------------------------------)
    [Matthew Harris](https://medium.com/@astrobagel?source=post_page---byline--e413ea2ae064--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e413ea2ae064--------------------------------)
    Â·16 min readÂ·Oct 28, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº[æ•°æ®ç§‘å­¦å‰æ²¿](https://towardsdatascience.com/?source=post_page---byline--e413ea2ae064--------------------------------)
    Â·16åˆ†é’Ÿé˜…è¯»Â·2024å¹´10æœˆ28æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/451f325bbb6fd2a154c72081210f8eb8.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/451f325bbb6fd2a154c72081210f8eb8.png)'
- en: STORM researches the topic via perspective-guided question asking in simulated
    conversations. [Source](https://arxiv.org/abs/2402.14207)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: STORMé€šè¿‡è§†è§’å¼•å¯¼çš„é—®é¢˜æé—®åœ¨æ¨¡æ‹Ÿå¯¹è¯ä¸­ç ”ç©¶ä¸»é¢˜ã€‚[æ¥æº](https://arxiv.org/abs/2402.14207)
- en: TL;DR
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: TL;DR
- en: '*The use of LLM agents is becoming more common for tackling multi-step long-context
    research tasks where traditional RAG direct prompting methods can sometimes struggle.
    In this article, we will explore a new and promising technique developed by Stanford
    called* ***S****ynthesis of* ***T****opic* ***O****utlines through* ***R****etrieval
    and* ***M****ulti-perspective Question Asking (*[*STORM*](https://arxiv.org/abs/2402.14207)*),
    which uses LLM agents to simulate â€˜Perspective-guided conversationsâ€™ to reach
    complex research goals and generate rich research articles that can be used by
    humans in their pre-writing research. STORM was initially developed to gather
    information from web sources but also supports searching a local document vector
    store. In this article we will see how to implement STORM for AI-supported research
    on local PDFs, using US FEMA disaster preparedness and assistance documentation.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*LLMä»£ç†çš„ä½¿ç”¨åœ¨å¤„ç†å¤šæ­¥éª¤ã€é•¿ä¸Šä¸‹æ–‡çš„ç ”ç©¶ä»»åŠ¡æ—¶å˜å¾—è¶Šæ¥è¶Šæ™®éï¼Œå› ä¸ºä¼ ç»Ÿçš„RAGç›´æ¥æç¤ºæ–¹æ³•æœ‰æ—¶ä¼šé‡åˆ°å›°éš¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨ç”±æ–¯å¦ç¦å¤§å­¦å¼€å‘çš„ä¸€ç§æ–°å‹ä¸”æœ‰å‰æ™¯çš„æŠ€æœ¯*
    ***S****ynthesis of* ***T****opic* ***O****utlines through* ***R****etrieval and*
    ***M****ulti-perspective Question Asking (*[*STORM*](https://arxiv.org/abs/2402.14207)*),
    å®ƒä½¿ç”¨LLMä»£ç†æ¨¡æ‹Ÿâ€œè§†è§’å¼•å¯¼çš„å¯¹è¯â€ä»¥å®ç°å¤æ‚çš„ç ”ç©¶ç›®æ ‡ï¼Œå¹¶ç”Ÿæˆä¸°å¯Œçš„ç ”ç©¶æ–‡ç« ï¼Œä¾›äººä»¬åœ¨å†™ä½œå‰ç ”ç©¶ä½¿ç”¨ã€‚STORMæœ€åˆæ˜¯ä¸ºäº†ä»ç½‘ç»œèµ„æºä¸­æ”¶é›†ä¿¡æ¯è€Œå¼€å‘çš„ï¼Œä½†ä¹Ÿæ”¯æŒæœç´¢æœ¬åœ°æ–‡æ¡£å‘é‡å­˜å‚¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•å®ç°STORMä»¥æ”¯æŒåŸºäºAIçš„æœ¬åœ°PDFç ”ç©¶ï¼Œä½¿ç”¨ç¾å›½FEMAç¾éš¾å‡†å¤‡å’Œæ´åŠ©æ–‡æ¡£ã€‚*'
- en: Itâ€™s been amazing to watch how using LLMs for knowledge retrieval has progressed
    in a relatively short period of time. Since the [first paper on Retrieval Augmented
    Generation (RAG)](https://arxiv.org/abs/2005.11401) in 2020, we have seen the
    ecosystem grow to include a [cornucopia of available technique](https://arxiv.org/html/2312.10997v5#S2)s.
    One of the more advanced is agentic RAG where LLM agents iterate and refine document
    retrieval in order to solve more complex research tasks. Itâ€™s similar to how a
    human might carry out research, exploring a range of different search queries
    to build a better idea of the context, sometimes discussing the topic with other
    humans, and synthesizing everything into a final result. Single-turn RAG, even
    employing techniques such as query expansion and reranking, can struggle with
    more complex multi-hop research tasks like this.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹åˆ°åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡ŒçŸ¥è¯†æ£€ç´¢åœ¨ç›¸å¯¹è¾ƒçŸ­çš„æ—¶é—´å†…å–å¾—çš„è¿›å±•ï¼ŒçœŸæ˜¯ä»¤äººæƒŠå¹ã€‚è‡ªä»2020å¹´å‘å¸ƒçš„[é¦–æ¬¡å…³äºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰è®ºæ–‡](https://arxiv.org/abs/2005.11401)ä»¥æ¥ï¼Œæˆ‘ä»¬è§è¯äº†è¿™ä¸ªç”Ÿæ€ç³»ç»Ÿçš„å‘å±•ï¼Œç°å·²æ¶µç›–äº†[ä¸€ç³»åˆ—å¯ç”¨çš„æŠ€æœ¯](https://arxiv.org/html/2312.10997v5#S2)ã€‚å…¶ä¸­æ›´å…ˆè¿›çš„æŠ€æœ¯ä¹‹ä¸€æ˜¯ä»£ç†RAGï¼ŒLLMä»£ç†é€šè¿‡è¿­ä»£å’Œä¼˜åŒ–æ–‡æ¡£æ£€ç´¢æ¥è§£å†³æ›´å¤æ‚çš„ç ”ç©¶ä»»åŠ¡ã€‚è¿™ç±»ä¼¼äºäººç±»è¿›è¡Œç ”ç©¶çš„æ–¹å¼ï¼Œé€šè¿‡æ¢ç´¢å¤šç§ä¸åŒçš„æœç´¢æŸ¥è¯¢æ¥å»ºç«‹æ›´æ¸…æ™°çš„èƒŒæ™¯ï¼Œæœ‰æ—¶è¿˜ä¼šä¸å…¶ä»–äººè®¨è®ºè¿™ä¸€ä¸»é¢˜ï¼Œå¹¶å°†æ‰€æœ‰ä¿¡æ¯æ•´åˆæˆæœ€ç»ˆçš„ç»“æœã€‚è€Œå•å›åˆRAGï¼Œå³ä¾¿é‡‡ç”¨äº†æŸ¥è¯¢æ‰©å±•å’Œé‡æ–°æ’åºç­‰æŠ€æœ¯ï¼Œåœ¨åº”å¯¹åƒè¿™æ ·çš„å¤æ‚å¤šè·³ç ”ç©¶ä»»åŠ¡æ—¶ï¼Œä»ç„¶å­˜åœ¨å›°éš¾ã€‚
- en: There are quite a few patterns for knowledge retrieval using agent frameworks
    such as [Autogen](https://microsoft.github.io/autogen/0.2/), [CrewAI](https://www.crewai.com),
    and [LangGraph](https://www.langchain.com/langgraph) as well as specific AI research
    assistants such as [GPT Researcher](https://github.com/assafelovic/gpt-researcher).
    In this article, we will look at an LLM-powered research writing system from Stanford
    University, called **S**ynthesis of **T**opic **O**utlines through **R**etrieval
    and **M**ulti-perspective Question Asking ([STORM](https://arxiv.org/abs/2402.14207)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä»£ç†æ¡†æ¶ï¼ˆå¦‚[Autogen](https://microsoft.github.io/autogen/0.2/)ã€[CrewAI](https://www.crewai.com)å’Œ[LangGraph](https://www.langchain.com/langgraph)ï¼‰ä»¥åŠç‰¹å®šçš„AIç ”ç©¶åŠ©æ‰‹ï¼ˆå¦‚[GPT
    Researcher](https://github.com/assafelovic/gpt-researcher)ï¼‰è¿›è¡ŒçŸ¥è¯†æ£€ç´¢çš„æ¨¡å¼æœ‰å¾ˆå¤šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨æ–¯å¦ç¦å¤§å­¦å¼€å‘çš„ä¸€ä¸ªç”±LLMé©±åŠ¨çš„ç ”ç©¶å†™ä½œç³»ç»Ÿï¼Œåä¸º**S**ynthesis
    of **T**opic **O**utlines through **R**etrieval and **M**ulti-perspective Question
    Askingï¼ˆ[STORM](https://arxiv.org/abs/2402.14207)ï¼‰ã€‚
- en: STORM AI research writing system
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: STORM AIç ”ç©¶å†™ä½œç³»ç»Ÿ
- en: STORM applies a clever technique where LLM agents simulate â€˜Perspective-guided
    conversationsâ€™ to reach a research goal as well as extend â€˜outline-driven RAGâ€™
    for richer article generation.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: STORMåº”ç”¨äº†ä¸€ç§å·§å¦™çš„æŠ€æœ¯ï¼Œå…¶ä¸­LLMä»£ç†æ¨¡æ‹Ÿâ€œè§†è§’å¼•å¯¼å¯¹è¯â€ä»¥è¾¾æˆç ”ç©¶ç›®æ ‡ï¼Œå¹¶æ‰©å±•äº†â€œåŸºäºå¤§çº²çš„RAGâ€ï¼Œä»¥ç”Ÿæˆæ›´ä¸°å¯Œçš„æ–‡ç« å†…å®¹ã€‚
- en: Configured to generate Wikipedia-style articles, it was tested with a cohort
    of 10 experienced Wikipedia editors.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥ç³»ç»Ÿé…ç½®ä¸ºç”Ÿæˆç±»ä¼¼ç»´åŸºç™¾ç§‘é£æ ¼çš„æ–‡ç« ï¼Œå¹¶åœ¨ä¸€ç»„10åç»éªŒä¸°å¯Œçš„ç»´åŸºç™¾ç§‘ç¼–è¾‘è€…ä¸­è¿›è¡Œäº†æµ‹è¯•ã€‚
- en: '![](../Images/0e52360332e7847eb9aa2237a73652ac.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e52360332e7847eb9aa2237a73652ac.png)'
- en: Survey results of 10 experienced Wikipedia Editors on the perceived usefulness
    of STORM. [Source](https://arxiv.org/abs/2402.14207).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 10åç»éªŒä¸°å¯Œçš„ç»´åŸºç™¾ç§‘ç¼–è¾‘è€…å¯¹STORMåœ¨å®é™…ä½¿ç”¨ä¸­çš„æ„ŸçŸ¥æœ‰ç”¨æ€§çš„è°ƒæŸ¥ç»“æœã€‚[æ¥æº](https://arxiv.org/abs/2402.14207)ã€‚
- en: Reception on the whole was positive, 70% of the editors felt that it would be
    a useful tool in their *pre-writing* stage when researching a topic. I hope in
    the future surveys could include more than 10 editors, but it should be noted
    that authors also benchmarked traditional article generation methods using FreshWiki,
    a dataset of recent high-quality Wikipedia articles, where STORM was found to
    outperform previous techniques.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä½“åå“ç§¯æï¼Œ70%çš„ç¼–è¾‘è®¤ä¸ºè¯¥å·¥å…·åœ¨ä»–ä»¬çš„*å†™ä½œå‰*é˜¶æ®µç ”ç©¶ä¸€ä¸ªä¸»é¢˜æ—¶ä¼šéå¸¸æœ‰ç”¨ã€‚å¸Œæœ›æœªæ¥çš„è°ƒæŸ¥èƒ½å¤ŸåŒ…æ‹¬è¶…è¿‡10åç¼–è¾‘ï¼Œä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä½œè€…ä¹Ÿé€šè¿‡ä½¿ç”¨FreshWikiï¼ˆä¸€ç»„è¿‘æœŸé«˜è´¨é‡çš„ç»´åŸºç™¾ç§‘æ–‡ç« æ•°æ®é›†ï¼‰å¯¹ä¼ ç»Ÿæ–‡ç« ç”Ÿæˆæ–¹æ³•è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå…¶ä¸­STORMè¢«å‘ç°ä¼˜äºä»¥å¾€çš„æ–¹æ³•ã€‚
- en: '![](../Images/4076dab0bb937a746d51e0eb93b03985.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4076dab0bb937a746d51e0eb93b03985.png)'
- en: Human evaluation by 10 experienced Wikipedia editors for on 20 pairs of articles
    generated by STORM and *oRAG*. Each pair of articles is evaluated by two Wikipedia
    editors. [Source](https://arxiv.org/abs/2402.14207).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±10åç»éªŒä¸°å¯Œçš„ç»´åŸºç™¾ç§‘ç¼–è¾‘è€…å¯¹STORMå’Œ*oRAG*ç”Ÿæˆçš„20å¯¹æ–‡ç« è¿›è¡Œäººå·¥è¯„ä¼°ã€‚æ¯å¯¹æ–‡ç« ç”±ä¸¤åç»´åŸºç™¾ç§‘ç¼–è¾‘è€…è¯„ä¼°ã€‚[æ¥æº](https://arxiv.org/abs/2402.14207)ã€‚
- en: STORM is [open source](https://github.com/stanford-oval/storm/tree/main) and
    available as a [Python package](https://pypi.org/project/knowledge-storm/) with
    additional implementations using frameworks such as [LangGraph](https://langchain-ai.github.io/langgraph/tutorials/storm/storm/).
    More recently STORM has been enhanced to support human-AI collaborative knowledge
    curation called [Co-STORM](https://www.arxiv.org/abs/2408.15232), putting a human
    right in the center of the AI-assisted research loop.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: STORMæ˜¯[å¼€æºçš„](https://github.com/stanford-oval/storm/tree/main)ï¼Œå¹¶ä½œä¸ºä¸€ä¸ª[PythonåŒ…](https://pypi.org/project/knowledge-storm/)æä¾›ï¼Œå¦å¤–è¿˜æ”¯æŒä½¿ç”¨å¦‚[LangGraph](https://langchain-ai.github.io/langgraph/tutorials/storm/storm/)ç­‰æ¡†æ¶çš„å®ç°ã€‚æœ€è¿‘ï¼ŒSTORMå·²è¢«å¢å¼ºä»¥æ”¯æŒä¸€ç§åä¸º[Co-STORM](https://www.arxiv.org/abs/2408.15232)çš„äººç±»-AIåä½œçŸ¥è¯†ç­–åˆ’æ–¹æ³•ï¼Œå°†äººç±»ç½®äºAIè¾…åŠ©ç ”ç©¶ç¯è·¯çš„ä¸­å¿ƒã€‚
- en: Though it significantly outperforms baseline methods in both automatic and human
    evaluations, there are some caveats that the authors acknowledge. It isnâ€™t yet
    multimodal, doesnâ€™t produce experienced human-quality content â€” it isnâ€™t positioned
    yet for this I feel, being more targeted for *pre-writing* research than final
    articles â€” and there are some nuances around references that require some future
    work. That said, if you have a deep research task, itâ€™s worth checking out.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å®ƒåœ¨è‡ªåŠ¨åŒ–å’Œäººå·¥è¯„ä¼°ä¸­éƒ½æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œä½†ä½œè€…ä¹Ÿæ‰¿è®¤å­˜åœ¨ä¸€äº›å±€é™æ€§ã€‚ç›®å‰å®ƒè¿˜ä¸æ˜¯å¤šæ¨¡æ€çš„ï¼Œç”Ÿæˆçš„å†…å®¹ä¹Ÿæœªè¾¾åˆ°ç»éªŒä¸°å¯Œçš„äººå·¥è´¨é‡â€”â€”æˆ‘è§‰å¾—å®ƒç›®å‰è¿˜ä¸é€‚åˆè¿™ä¸€ç‚¹ï¼Œæ›´å¤šçš„æ˜¯é’ˆå¯¹*å†™ä½œå‰*çš„ç ”ç©¶ï¼Œè€Œéæœ€ç»ˆæ–‡ç« â€”â€”å¦å¤–ï¼Œå‚è€ƒæ–‡çŒ®æ–¹é¢ä¹Ÿå­˜åœ¨ä¸€äº›éœ€è¦è¿›ä¸€æ­¥æ”¹è¿›çš„ç»†èŠ‚ã€‚è¯è™½å¦‚æ­¤ï¼Œå¦‚æœä½ æœ‰ä¸€ä¸ªæ·±å…¥çš„ç ”ç©¶ä»»åŠ¡ï¼Œå€¼å¾—ä¸€è¯•ã€‚
- en: You can try out STORM [online](https://storm.genie.stanford.edu/) â€” itâ€™s fun!
    â€” configured to perform research using information on the web.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨[ç½‘ä¸Š](https://storm.genie.stanford.edu/)è¯•ç”¨STORMâ€”â€”å®ƒéå¸¸æœ‰è¶£ï¼â€”â€”å¹¶é…ç½®ä¸ºä½¿ç”¨ç½‘ç»œä¿¡æ¯è¿›è¡Œç ”ç©¶ã€‚
- en: '**But what about running STORM with your own data?**'
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ä½†å¦‚æœç”¨è‡ªå·±çš„æ•°æ®è¿è¡ŒSTORMå‘¢ï¼Ÿ**'
- en: Many organizations will want to use AI research tools with their own internal
    data. The STORM authors have done a nice job of documenting various approaches
    of using STORM with different LLM providers and a local vector database, which
    means it is possible to run STORM on your own documents.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤šç»„ç»‡å¸Œæœ›å°†AIç ”ç©¶å·¥å…·ä¸ä»–ä»¬è‡ªå·±çš„å†…éƒ¨æ•°æ®ç»“åˆä½¿ç”¨ã€‚STORMçš„ä½œè€…ä»¬åšå¾—å¾ˆå¥½ï¼Œè®°å½•äº†å¦‚ä½•å°†STORMä¸ä¸åŒçš„LLMæä¾›è€…ä»¥åŠæœ¬åœ°å‘é‡æ•°æ®åº“ç»“åˆä½¿ç”¨ï¼Œè¿™æ„å‘³ç€ä½ å¯ä»¥åœ¨è‡ªå·±çš„æ–‡æ¡£ä¸Šè¿è¡ŒSTORMã€‚
- en: So letâ€™s try this out!
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œè®©æˆ‘ä»¬æ¥è¯•è¯•å§ï¼
- en: Setup and code
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®¾ç½®ä¸ä»£ç 
- en: You can find the code for this article [here](https://github.com/dividor/storm-with-local-docs),
    which includes environment setup instructions and how to collate some sample documents
    for this demo.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/dividor/storm-with-local-docs)æ‰¾åˆ°æœ¬æ–‡çš„ä»£ç ï¼ŒåŒ…å«äº†ç¯å¢ƒè®¾ç½®è¯´æ˜ä»¥åŠå¦‚ä½•æ”¶é›†ä¸€äº›ç¤ºä¾‹æ–‡æ¡£æ¥è¿›è¡Œæ¼”ç¤ºã€‚
- en: FEMA disaster preparedness and assistance documentation
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FEMAç¾éš¾å‡†å¤‡å’Œæ´åŠ©æ–‡æ¡£
- en: We will use 34 PDF documents to help people prepare for and respond to disasters,
    as created by the United States Federal Emergency Management Agency ([FEMA](https://www.fema.gov)).
    These documents perhaps arenâ€™t typically what people may want to use for writing
    deep research articles, but Iâ€™m interested in seeing how AI can help people prepare
    for disasters.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨34ä»½ç”±ç¾å›½è”é‚¦åº”æ€¥ç®¡ç†å±€ï¼ˆ[FEMA](https://www.fema.gov)ï¼‰åˆ›å»ºçš„PDFæ–‡æ¡£ï¼Œå¸®åŠ©äººä»¬ä¸ºç¾éš¾åšå‡†å¤‡å¹¶è¿›è¡Œå“åº”ã€‚è¿™äº›æ–‡æ¡£å¯èƒ½é€šå¸¸ä¸æ˜¯äººä»¬ç”¨æ¥æ’°å†™æ·±å…¥ç ”ç©¶æ–‡ç« çš„å†…å®¹ï¼Œä½†æˆ‘å¾ˆæƒ³çœ‹çœ‹AIå¦‚ä½•å¸®åŠ©äººä»¬ä¸ºç¾éš¾åšå¥½å‡†å¤‡ã€‚
- en: â€¦. and I have the code already written for processing FEMA reports from some
    earlier blog posts, which Iâ€™ve included in the linked repo above. ğŸ˜Š
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: â€¦â€¦æˆ‘å·²ç»ç¼–å†™äº†å¤„ç†FEMAæŠ¥å‘Šçš„ä»£ç ï¼Œè¿™äº›ä»£ç æ¥è‡ªä¹‹å‰çš„ä¸€äº›åšå®¢æ–‡ç« ï¼Œå·²ç»åŒ…å«åœ¨ä¸Šé¢é“¾æ¥çš„ä»£ç åº“ä¸­ã€‚ğŸ˜Š
- en: Parsing and Chunking
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è§£æä¸åˆ‡åˆ†
- en: Once we have our documents, we need to split them into smaller documents so
    that STORM can search for specific topics within the corpus. Given STORM is originally
    aimed at generating Wikipedia-style articles, I opted to try two approaches, (i)
    Simply splitting the documents into sub-documents by page using [LangChainâ€™s PyPDFLoader](https://python.langchain.com/docs/integrations/document_loaders/pypdfloader/),
    to create a crude simulation of a Wikipedia page which includes several sub-topics.
    Many FEMA PDFs are single-page documents that donâ€™t look too dissimilar to Wikipedia
    articles; (ii) Further chunking the documents into smaller sections, more likely
    to cover a discrete sub-topic.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æˆ‘ä»¬æœ‰äº†æ–‡æ¡£ï¼Œå°±éœ€è¦å°†å…¶æ‹†åˆ†æˆæ›´å°çš„æ–‡æ¡£ï¼Œä»¥ä¾¿STORMèƒ½å¤Ÿåœ¨è¯­æ–™åº“ä¸­æŸ¥æ‰¾ç‰¹å®šçš„ä¸»é¢˜ã€‚ç”±äºSTORMæœ€åˆæ—¨åœ¨ç”Ÿæˆç±»ä¼¼ç»´åŸºç™¾ç§‘çš„æ–‡ç« ï¼Œæˆ‘é€‰æ‹©å°è¯•ä¸¤ç§æ–¹æ³•ï¼šï¼ˆiï¼‰ç®€å•åœ°é€šè¿‡[LangChainçš„PyPDFLoader](https://python.langchain.com/docs/integrations/document_loaders/pypdfloader/)æŒ‰é¡µå°†æ–‡æ¡£æ‹†åˆ†ä¸ºå­æ–‡æ¡£ï¼Œä»è€Œç²—ç•¥æ¨¡æ‹Ÿä¸€ä¸ªåŒ…å«å¤šä¸ªå­ä¸»é¢˜çš„ç»´åŸºç™¾ç§‘é¡µé¢ã€‚è®¸å¤šFEMAçš„PDFæ˜¯å•é¡µæ–‡æ¡£ï¼Œçœ‹èµ·æ¥ä¸ç»´åŸºç™¾ç§‘æ–‡ç« ç›¸å·®ä¸å¤§ï¼›ï¼ˆiiï¼‰è¿›ä¸€æ­¥å°†æ–‡æ¡£åˆ‡åˆ†ä¸ºæ›´å°çš„éƒ¨åˆ†ï¼Œæ›´å¯èƒ½æ¶µç›–ä¸€ä¸ªç¦»æ•£çš„å­ä¸»é¢˜ã€‚
- en: These are of course *very* basic approaches to parsing, but I wanted to see
    how results varied depending on the two techniques. Any serious use of STORM on
    local documents should invest in all the usual fun around paring optimization.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å½“ç„¶æ˜¯*éå¸¸*åŸºç¡€çš„è§£ææ–¹æ³•ï¼Œä½†æˆ‘æƒ³çœ‹çœ‹è¿™ä¸¤ç§æŠ€æœ¯åœ¨ç»“æœä¸Šçš„å·®å¼‚ã€‚ä»»ä½•å¯¹STORMåœ¨æœ¬åœ°æ–‡æ¡£ä¸­çš„ä¸¥è‚ƒä½¿ç”¨éƒ½åº”è¯¥æŠ•å…¥æ‰€æœ‰å¸¸è§çš„é…å¯¹ä¼˜åŒ–å·¥ä½œã€‚
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Metadata enrichment
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å…ƒæ•°æ®å¢å¼º
- en: '[STORMâ€™s example documentation](https://github.com/stanford-oval/storm/blob/main/examples/storm_examples/README.md)
    requires that documents have metadata fields â€˜URLâ€™, â€˜titleâ€™, and â€˜descriptionâ€™,
    where â€˜URLâ€™ should be unique. Since we are splitting up PDF documents, we donâ€™t
    have titles and descriptions of individual pages and chunks, so I opted to generate
    these with a simple LLM call.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[STORMçš„ç¤ºä¾‹æ–‡æ¡£](https://github.com/stanford-oval/storm/blob/main/examples/storm_examples/README.md)è¦æ±‚æ–‡æ¡£å…·æœ‰å…ƒæ•°æ®å­—æ®µâ€˜URLâ€™ï¼Œâ€˜titleâ€™ï¼Œå’Œâ€˜descriptionâ€™ï¼Œå…¶ä¸­â€˜URLâ€™åº”è¯¥æ˜¯å”¯ä¸€çš„ã€‚ç”±äºæˆ‘ä»¬æ­£åœ¨æ‹†åˆ†PDFæ–‡æ¡£ï¼Œå› æ­¤æ²¡æœ‰å•ç‹¬é¡µé¢å’Œå—çš„æ ‡é¢˜å’Œæè¿°ï¼Œå› æ­¤æˆ‘é€‰æ‹©ä½¿ç”¨ç®€å•çš„LLMè°ƒç”¨æ¥ç”Ÿæˆè¿™äº›å†…å®¹ã€‚'
- en: For URLs, we have them for individual PDF pages, but for chunks within a page.
    Sophisticated knowledge retrieval systems can have metadata generated by layout
    detection models so the text chunk area can be highlighted in the corresponding
    PDF, but for this demo, I simply added an â€˜_idâ€™ query parameter the URL which
    does nothing but ensure they are unique for chunks.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºURLsï¼Œæˆ‘ä»¬æœ‰æ¯ä¸ªPDFé¡µé¢çš„é“¾æ¥ï¼Œä½†å¯¹äºé¡µé¢å†…çš„å—ï¼Œå¤æ‚çš„çŸ¥è¯†æ£€ç´¢ç³»ç»Ÿå¯ä»¥é€šè¿‡å¸ƒå±€æ£€æµ‹æ¨¡å‹ç”Ÿæˆå…ƒæ•°æ®ï¼Œè¿™æ ·æ–‡æœ¬å—åŒºåŸŸå°±å¯ä»¥åœ¨ç›¸åº”çš„PDFä¸­é«˜äº®æ˜¾ç¤ºï¼Œä½†å¯¹äºè¿™ä¸ªæ¼”ç¤ºï¼Œæˆ‘åªæ˜¯ç®€å•åœ°åœ¨URLä¸­æ·»åŠ äº†ä¸€ä¸ªâ€˜_idâ€™æŸ¥è¯¢å‚æ•°ï¼Œè¿™ä¸ªå‚æ•°ä¸ä¼šåšä»»ä½•äº‹æƒ…ï¼Œåªæ˜¯ç¡®ä¿å®ƒä»¬å¯¹äºä¸åŒçš„å—æ˜¯å”¯ä¸€çš„ã€‚
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Building vector databases
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ„å»ºå‘é‡æ•°æ®åº“
- en: STORM already supports the [Qdrant vector store](https://www.google.com/url?sa=t&rct=j&opi=89978449&url=https%3A%2F%2Fqdrant.tech%2F&ved=2ahUKEwiHuK-_766JAxXutokEHbnUMhwQFnoECAgQAQ&usg=AOvVaw1SKthNlGkmNDis3BK1WPSq).
    I like to use frameworks such as LangChain and Llama Index where possible to make
    it easier to change providers down the road, so I opted to use LangChain to build
    a [local Qdrant vector database persisted to the local file system](https://python.langchain.com/docs/integrations/vectorstores/qdrant/#local-mode)
    rather than STORMâ€™s automatic vector database management. I felt this offers more
    control and is more recognizable to those who already have pipelines for populating
    document vector stores.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: STORMå·²ç»æ”¯æŒ[Qdrantå‘é‡å­˜å‚¨](https://www.google.com/url?sa=t&rct=j&opi=89978449&url=https%3A%2F%2Fqdrant.tech%2F&ved=2ahUKEwiHuK-_766JAxXutokEHbnUMhwQFnoECAgQAQ&usg=AOvVaw1SKthNlGkmNDis3BK1WPSq)ã€‚æˆ‘å–œæ¬¢åœ¨å¯èƒ½çš„æƒ…å†µä¸‹ä½¿ç”¨åƒLangChainå’ŒLlama
    Indexè¿™æ ·çš„æ¡†æ¶ï¼Œè¿™æ ·å¯ä»¥åœ¨å°†æ¥æ›´å®¹æ˜“åœ°æ›´æ¢æä¾›å•†ï¼Œå› æ­¤æˆ‘é€‰æ‹©ä½¿ç”¨LangChainæ„å»ºä¸€ä¸ª[æŒä¹…åŒ–åˆ°æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿçš„æœ¬åœ°Qdrantå‘é‡æ•°æ®åº“](https://python.langchain.com/docs/integrations/vectorstores/qdrant/#local-mode)ï¼Œè€Œä¸æ˜¯STORMçš„è‡ªåŠ¨å‘é‡æ•°æ®åº“ç®¡ç†ã€‚æˆ‘è®¤ä¸ºè¿™æä¾›äº†æ›´å¤šçš„æ§åˆ¶ï¼Œå¹¶ä¸”å¯¹äºé‚£äº›å·²ç»æœ‰å¡«å……æ–‡æ¡£å‘é‡å­˜å‚¨çš„ç®¡é“çš„äººæ¥è¯´æ›´å…·å¯è¯†åˆ«æ€§ã€‚
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Running STORM
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¿è¡ŒSTORM
- en: 'The STORM repo has [some great examples](https://github.com/stanford-oval/storm/blob/main/examples/storm_examples/README.md)
    of different search engines and LLMs, as well as using a Qdrant vector store.
    I decided to combine various features from these, plus some extra post-processing
    as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: STORMçš„ä»£ç åº“æœ‰[ä¸€äº›å¾ˆæ£’çš„ç¤ºä¾‹](https://github.com/stanford-oval/storm/blob/main/examples/storm_examples/README.md)ï¼ŒåŒ…æ‹¬ä¸åŒçš„æœç´¢å¼•æ“å’ŒLLMçš„ä½¿ç”¨ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨Qdrantå‘é‡å­˜å‚¨ã€‚æˆ‘å†³å®šç»“åˆè¿™äº›ç¤ºä¾‹ä¸­çš„å„ç§åŠŸèƒ½ï¼Œå¹¶æ·»åŠ ä¸€äº›é¢å¤–çš„åå¤„ç†ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: Added ability to run with OpenAI or Ollama
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¢åŠ äº†ä¸OpenAIæˆ–Ollamaä¸€èµ·è¿è¡Œçš„åŠŸèƒ½
- en: Added support for passing in the vector database directory
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¢åŠ äº†æ”¯æŒä¼ å…¥å‘é‡æ•°æ®åº“ç›®å½•çš„åŠŸèƒ½
- en: Added a function to parse the references metadata file to add references to
    the generated polished article. STORM generated these references in a JSON file
    but didnâ€™t add them to the output article automatically. Iâ€™m not sure if this
    was due to some setting I missed, but references are key to evaluating any AI
    research technique, so I added this custom post-processing step.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¢åŠ äº†ä¸€ä¸ªåŠŸèƒ½æ¥è§£æå‚è€ƒæ–‡çŒ®å…ƒæ•°æ®æ–‡ä»¶ï¼Œå°†å‚è€ƒæ–‡çŒ®æ·»åŠ åˆ°ç”Ÿæˆçš„ç²¾ç‚¼æ–‡ç« ä¸­ã€‚STORMå°†è¿™äº›å‚è€ƒæ–‡çŒ®ç”Ÿæˆåœ¨ä¸€ä¸ªJSONæ–‡ä»¶ä¸­ï¼Œä½†æ²¡æœ‰è‡ªåŠ¨å°†å®ƒä»¬æ·»åŠ åˆ°è¾“å‡ºæ–‡ç« ä¸­ã€‚æˆ‘ä¸ç¡®å®šè¿™æ˜¯ä¸æ˜¯ç”±äºæˆ‘é”™è¿‡äº†æŸä¸ªè®¾ç½®ï¼Œä½†å‚è€ƒæ–‡çŒ®å¯¹äºè¯„ä¼°ä»»ä½•AIç ”ç©¶æŠ€æœ¯éƒ½è‡³å…³é‡è¦ï¼Œå› æ­¤æˆ‘å¢åŠ äº†è¿™ä¸ªè‡ªå®šä¹‰åå¤„ç†æ­¥éª¤ã€‚
- en: Finally, I noticed that open models have more guidance in templates and personas
    due to their following instructions less accurately than commercial models. I
    liked the transparency of these controls and left them in for OpenAI so that I
    could adjust in future work.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘æ³¨æ„åˆ°å¼€æ”¾æ¨¡å‹åœ¨æ¨¡æ¿å’Œè§’è‰²è®¾å®šæ–¹é¢æä¾›äº†æ›´å¤šçš„æŒ‡å¯¼ï¼Œå› ä¸ºå®ƒä»¬æ‰§è¡ŒæŒ‡ä»¤çš„å‡†ç¡®åº¦ä¸å¦‚å•†ä¸šæ¨¡å‹ã€‚æˆ‘å–œæ¬¢è¿™äº›æ§åˆ¶çš„é€æ˜æ€§ï¼Œå¹¶å°†å…¶ä¿ç•™åœ¨OpenAIä¸­ï¼Œä»¥ä¾¿å°†æ¥åœ¨å·¥ä½œä¸­è¿›è¡Œè°ƒæ•´ã€‚
- en: Here is everything (see [repo notebook](https://github.com/dividor/storm-with-local-docs/blob/main/storm-local-docs.ipynb)
    for full code) â€¦
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯æ‰€æœ‰å†…å®¹ï¼ˆæŸ¥çœ‹[repo notebook](https://github.com/dividor/storm-with-local-docs/blob/main/storm-local-docs.ipynb)ä»¥è·å–å®Œæ•´ä»£ç ï¼‰â€¦
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Weâ€™re ready to run STORM!
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å‡†å¤‡å¥½è¿è¡ŒSTORMäº†ï¼
- en: For the research topic, I picked something that would be challenging to answer
    with a typical RAG system and which wasnâ€™t well covered in the PDF data so we
    can see how well attribution works â€¦
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºç ”ç©¶è¯¾é¢˜ï¼Œæˆ‘é€‰æ‹©äº†ä¸€ä¸ªå…¸å‹çš„RAGç³»ç»Ÿéš¾ä»¥å›ç­”çš„ä¸»é¢˜ï¼Œä¸”åœ¨PDFæ•°æ®ä¸­æ²¡æœ‰å¾ˆå¥½è¦†ç›–çš„å†…å®¹ï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å½’å› æ•ˆæœå¦‚ä½•â€¦â€¦
- en: â€œ***Compare the financial impact of different types of disasters and how those
    impact communities***â€
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: â€œ***æ¯”è¾ƒä¸åŒç±»å‹ç¾å®³çš„è´¢åŠ¡å½±å“åŠå…¶å¯¹ç¤¾åŒºçš„å½±å“***â€
- en: Running this for both databases â€¦
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸¤ä¸ªæ•°æ®åº“ä¸Šè¿è¡Œæ­¤æ“ä½œâ€¦â€¦
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Using OpenAI, the process took about 6 minutes on my Macbook pro M2 (16GB memory).
    I would note that other simpler queries where we have more supporting content
    in the underlying documents were much faster (< 30 seconds in some cases).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨OpenAIæ—¶ï¼Œè¿™ä¸ªè¿‡ç¨‹åœ¨æˆ‘çš„Macbook Pro M2ï¼ˆ16GBå†…å­˜ï¼‰ä¸Šå¤§çº¦èŠ±è´¹äº†6åˆ†é’Ÿã€‚æˆ‘éœ€è¦æŒ‡å‡ºçš„æ˜¯ï¼Œå…¶ä»–ä¸€äº›ç®€å•æŸ¥è¯¢ï¼Œå°¤å…¶æ˜¯åº•å±‚æ–‡æ¡£ä¸­æœ‰æ›´å¤šæ”¯æŒå†…å®¹çš„æŸ¥è¯¢ï¼Œè¦å¿«å¾—å¤šï¼ˆæœ‰äº›æƒ…å†µä¸‹ä¸åˆ°30ç§’ï¼‰ã€‚
- en: STORM results
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: STORMç»“æœ
- en: STORM generates a set of output files â€¦
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: STORMç”Ÿæˆäº†ä¸€ç»„è¾“å‡ºæ–‡ä»¶â€¦â€¦
- en: '![](../Images/9352963a2332997ba080d259b2f9151b.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9352963a2332997ba080d259b2f9151b.png)'
- en: Files generated by STORM, from which one markdown file was created combining
    the polished article with reference footnotes.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±STORMç”Ÿæˆçš„æ–‡ä»¶ï¼Œå…¶ä¸­ä¸€ä¸ªMarkdownæ–‡ä»¶ç»“åˆäº†æ¶¦è‰²åçš„æ–‡ç« å’Œå‚è€ƒè„šæ³¨ã€‚
- en: Itâ€™s interesting to review the **conversation_log.json** and **llm_call_history.json**
    to see the perspective-guided conversations component.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å®¡è§†**conversation_log.json**å’Œ**llm_call_history.json**æ–‡ä»¶ï¼ŒæŸ¥çœ‹ä»¥è§†è§’å¼•å¯¼çš„å¯¹è¯ç»„ä»¶ï¼Œååˆ†æœ‰è¶£ã€‚
- en: For our research topic â€¦
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæˆ‘ä»¬çš„ç ”ç©¶è¯¾é¢˜â€¦â€¦
- en: â€œ***Compare the financial impact of different types of disasters and how those
    impact communities***â€
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: â€œ***æ¯”è¾ƒä¸åŒç±»å‹ç¾å®³çš„è´¢åŠ¡å½±å“åŠå…¶å¯¹ç¤¾åŒºçš„å½±å“***â€
- en: You can find the generated articles here â€¦
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ç”Ÿæˆçš„æ–‡ç« â€¦â€¦
- en: '[STORM generated article â€” using text split by page](https://github.com/dividor/storm-with-local-docs/blob/main/data/storm_output/pages/Compare_the_financial_impact_of_different_types_of_disasters_and_how_those_impact_communities/storm_gen_article_polished.md)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[STORMç”Ÿæˆçš„æ–‡ç«  â€” ä½¿ç”¨æŒ‰é¡µé¢æ‹†åˆ†çš„æ–‡æœ¬](https://github.com/dividor/storm-with-local-docs/blob/main/data/storm_output/pages/Compare_the_financial_impact_of_different_types_of_disasters_and_how_those_impact_communities/storm_gen_article_polished.md)'
- en: '[STORM generated article â€” using text further chunked using RecursiveTextSplitter](https://github.com/dividor/storm-with-local-docs/blob/main/data/storm_output/chunks/Compare_the_financial_impact_of_different_types_of_disasters_and_how_those_impact_communities/storm_gen_article_polished.md)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[STORMç”Ÿæˆçš„æ–‡ç«  â€” ä½¿ç”¨é€’å½’æ–‡æœ¬æ‹†åˆ†å™¨è¿›ä¸€æ­¥æ‹†åˆ†çš„æ–‡æœ¬](https://github.com/dividor/storm-with-local-docs/blob/main/data/storm_output/chunks/Compare_the_financial_impact_of_different_types_of_disasters_and_how_those_impact_communities/storm_gen_article_polished.md)'
- en: '**Some quick observations**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¸€äº›å¿«é€Ÿè§‚å¯Ÿ**'
- en: This demo doesnâ€™t get into a formal evaluation â€” which can be [more involved
    than single-hop RAG systems](https://arxiv.org/abs/2401.15391) â€” but here are
    some subjective observations that may or may not be useful â€¦
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¼”ç¤ºæ²¡æœ‰è¿›è¡Œæ­£å¼çš„è¯„ä¼° â€” è¿™å¯èƒ½æ¯”å•è·³RAGç³»ç»Ÿ[æ›´ä¸ºå¤æ‚](https://arxiv.org/abs/2401.15391) â€” ä½†è¿™é‡Œæœ‰ä¸€äº›å¯èƒ½æœ‰ç”¨çš„ä¸»è§‚è§‚å¯Ÿâ€¦â€¦
- en: Parsing by page or by smaller chunks produces reasonable pre-reading reports
    that a human could use for researching areas related to the financial impact of
    disasters
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æŒ‰é¡µé¢æˆ–è¾ƒå°å—æ‹†åˆ†çš„è§£æä¼šç”Ÿæˆåˆç†çš„é¢„è¯»æŠ¥å‘Šï¼Œäººç±»å¯ä»¥ç”¨æ¥ç ”ç©¶ä¸ç¾å®³è´¢åŠ¡å½±å“ç›¸å…³çš„é¢†åŸŸ
- en: Both paring approaches provided citations throughout, but using smaller chunks
    seemed to result in fewer. See for example the Summary sections in both of the
    above articles. The more references to ground the analysis, the better!
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸¤ç§é…å¯¹æ–¹æ³•éƒ½åœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­æä¾›äº†å¼•ç”¨ï¼Œä½†ä½¿ç”¨æ›´å°çš„æ–‡æœ¬å—ä¼¼ä¹äº§ç”Ÿäº†æ›´å°‘çš„å¼•ç”¨ã€‚è¯·å‚è§ä¸Šè¿°ä¸¤ç¯‡æ–‡ç« ä¸­çš„æ€»ç»“éƒ¨åˆ†ã€‚æ›´å¤šçš„å¼•ç”¨å¯ä»¥ä¸ºåˆ†ææä¾›æ›´åšå®çš„åŸºç¡€ï¼
- en: Parsing by smaller chunks seemed to sometimes create citations that were not
    relevant, one of the citation challenges mentioned in the STORM paper. See for
    example citation for source â€˜10â€™ in the [summary section](https://github.com/dividor/storm-with-local-docs/blob/main/data/storm_output/chunks/Compare_the_financial_impact_of_different_types_of_disasters_and_how_those_impact_communities/storm_gen_article_polished.md),
    which doesnâ€™t correspond with the reference sentence.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æŒ‰è¾ƒå°å—æ‹†åˆ†çš„è§£ææœ‰æ—¶ä¼šäº§ç”Ÿä¸ç›¸å…³çš„å¼•ç”¨ï¼Œè¿™æ˜¯STORMè®ºæ–‡ä¸­æåˆ°çš„å¼•ç”¨é—®é¢˜ä¹‹ä¸€ã€‚è¯·å‚è§[æ€»ç»“éƒ¨åˆ†](https://github.com/dividor/storm-with-local-docs/blob/main/data/storm_output/chunks/Compare_the_financial_impact_of_different_types_of_disasters_and_how_those_impact_communities/storm_gen_article_polished.md)ä¸­æºâ€˜10â€™çš„å¼•ç”¨ï¼Œè¯¥å¼•ç”¨ä¸å‚è€ƒå¥å­ä¸ç¬¦ã€‚
- en: Overall, as expected for an algorithm developed on Wiki articles, splitting
    text by PDF seemed to produce a [more cohesive and grounded article](https://github.com/dividor/storm-with-local-docs/blob/main/data/storm_output/pages/Compare_the_financial_impact_of_different_types_of_disasters_and_how_those_impact_communities/storm_gen_article_polished.md)
    (to me!)
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ€»ä½“æ¥è¯´ï¼Œæ­£å¦‚æˆ‘é¢„æœŸçš„é‚£æ ·ï¼ŒåŸºäº Wiki æ–‡ç« å¼€å‘çš„ç®—æ³•ï¼ŒæŒ‰ PDF åˆ†å‰²æ–‡æœ¬ä¼¼ä¹èƒ½ç”Ÿæˆä¸€ç¯‡[æ›´å…·å‡èšåŠ›å’ŒåŸºç¡€æ€§çš„æ–‡ç« ](https://github.com/dividor/storm-with-local-docs/blob/main/data/storm_output/pages/Compare_the_financial_impact_of_different_types_of_disasters_and_how_those_impact_communities/storm_gen_article_polished.md)ï¼ˆå¯¹æˆ‘æ¥è¯´ï¼ï¼‰
- en: Even though the input research topic wasnâ€™t covered in great depth in the underlying
    documents, the generated report was a great starting point for further human analysis
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡è¾“å…¥çš„ç ”ç©¶ä¸»é¢˜åœ¨åŸºç¡€æ–‡æ¡£ä¸­å¹¶æ²¡æœ‰è¢«æ·±å…¥æ¢è®¨ï¼Œä½†ç”Ÿæˆçš„æŠ¥å‘Šä¸ºè¿›ä¸€æ­¥çš„äººç±»åˆ†ææä¾›äº†ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ã€‚
- en: Future Work
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœªæ¥çš„å·¥ä½œ
- en: We didnâ€™t get into [Co-Storm](https://www.arxiv.org/abs/2408.15232) in this
    article, which brings a human into the loop. This seems a great direction for
    AI-empowered research and something I am investigating.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­æ²¡æœ‰è®¨è®º[Co-Storm](https://www.arxiv.org/abs/2408.15232)ï¼Œå®ƒå°†äººç±»å¼•å…¥è¿‡ç¨‹ã€‚è¿™ä¼¼ä¹æ˜¯ä¸€ä¸ªéå¸¸é€‚åˆ
    AI é©±åŠ¨ç ”ç©¶çš„æ–¹å‘ï¼Œæˆ‘æ­£åœ¨ç ”ç©¶è¿™ä¸ªæ–¹å‘ã€‚
- en: Future work could also look at adjusting the system prompts and personas to
    the business case. Currently, those prompts are targeted for a Wikipedia-like
    process â€¦
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æœªæ¥çš„å·¥ä½œè¿˜å¯ä»¥è€ƒè™‘å°†ç³»ç»Ÿæç¤ºå’Œäººç‰©è§’è‰²è°ƒæ•´åˆ°å…·ä½“çš„å•†ä¸šæ¡ˆä¾‹ä¸­ã€‚ç›®å‰ï¼Œè¿™äº›æç¤ºæ˜¯ä¸ºç±»ä¼¼ç»´åŸºç™¾ç§‘çš„è¿‡ç¨‹è®¾è®¡çš„â€¦â€¦
- en: '![](../Images/41fbcc46e187420f6725280d4e787d88.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41fbcc46e187420f6725280d4e787d88.png)'
- en: STORM system prompts, illustrating the emphasis on creating Wikipedia-style
    articles. [Source](https://arxiv.org/abs/2402.14207)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: STORM ç³»ç»Ÿæç¤ºï¼Œå±•ç¤ºäº†å¦‚ä½•å¼ºè°ƒåˆ›å»ºç»´åŸºç™¾ç§‘é£æ ¼çš„æ–‡ç« ã€‚[æ¥æº](https://arxiv.org/abs/2402.14207)
- en: Another possible direction is to extend STORMâ€™s connectors beyond Qdrant, for
    example, to include other vector stores, or better still, generic support for
    Langchain and llama index vector stores. The authors encourage this type of thing,
    a PR involving [this file](https://github.com/stanford-oval/storm/blob/main/knowledge_storm/rm.py)
    may be in my future.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªå¯èƒ½çš„æ–¹å‘æ˜¯å°† STORM çš„è¿æ¥å™¨æ‰©å±•åˆ° Qdrant ä»¥å¤–çš„ç³»ç»Ÿï¼Œä¾‹å¦‚ï¼Œæ”¯æŒå…¶ä»–å‘é‡å­˜å‚¨ï¼Œæˆ–è€…æ›´å¥½çš„æ˜¯ï¼Œæ”¯æŒ Langchain å’Œ Llama
    index å‘é‡å­˜å‚¨çš„é€šç”¨æ”¯æŒã€‚ä½œè€…é¼“åŠ±è¿™ä¸€ç±»çš„å·¥ä½œï¼Œæ¶‰åŠ[è¿™ä¸ªæ–‡ä»¶](https://github.com/stanford-oval/storm/blob/main/knowledge_storm/rm.py)çš„
    PR å¯èƒ½æ˜¯æˆ‘æœªæ¥çš„å·¥ä½œæ–¹å‘ã€‚
- en: Running STORM without an internet connection would be an amazing thing, as it
    opens up possibilities for AI assistance in the field. As you can see from the
    demo code, I added the ability to run STORM with Ollama locally hosted models,
    but the token throughput rate was too low for the LLM agent discussion phase,
    so the system didnâ€™t complete on my laptop with small quantized models. A topic
    for a future blog post perhaps!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ²¡æœ‰äº’è”ç½‘è¿æ¥çš„æƒ…å†µä¸‹è¿è¡Œ STORM å°†æ˜¯ä¸€ä»¶ä»¤äººå…´å¥‹çš„äº‹æƒ…ï¼Œå› ä¸ºå®ƒä¸ºç°åœºçš„ AI åŠ©æ‰‹å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚æ­£å¦‚ä»æ¼”ç¤ºä»£ç ä¸­å¯ä»¥çœ‹åˆ°çš„ï¼Œæˆ‘å¢åŠ äº†è¿è¡Œ
    STORM çš„èƒ½åŠ›ï¼Œä½¿ç”¨ Ollama æœ¬åœ°æ‰˜ç®¡çš„æ¨¡å‹ï¼Œä½†ç”±äºä»¤ç‰Œååç‡è¿‡ä½ï¼ŒLLM ä»£ç†è®¨è®ºé˜¶æ®µæ²¡æœ‰å®Œæˆï¼Œå› æ­¤ç³»ç»Ÿåœ¨æˆ‘ä½¿ç”¨å°å‹é‡åŒ–æ¨¡å‹çš„ç¬”è®°æœ¬ç”µè„‘ä¸Šæ²¡æœ‰å®Œæˆä»»åŠ¡ã€‚è¿™æˆ–è®¸æ˜¯æœªæ¥åšå®¢æ–‡ç« çš„ä¸€ä¸ªè¯é¢˜ï¼
- en: Finally, though the [online User Interface is very nice](https://storm.genie.stanford.edu),
    the [demo UI that comes with the repo](https://github.com/stanford-oval/storm/tree/main/frontend/demo_light)
    is very basic and not something that could be used in production. Perhaps the
    Standford team might release the advanced interface â€” maybe it is already somewhere?
    â€” if not then work would be needed here.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œå°½ç®¡[åœ¨çº¿ç”¨æˆ·ç•Œé¢éå¸¸å‹å¥½](https://storm.genie.stanford.edu)ï¼Œä½†[éšä»£ç ä»“åº“é™„å¸¦çš„æ¼”ç¤ºç•Œé¢](https://github.com/stanford-oval/storm/tree/main/frontend/demo_light)éå¸¸åŸºç¡€ï¼Œæ— æ³•ç”¨äºç”Ÿäº§ç¯å¢ƒã€‚ä¹Ÿè®¸æ–¯å¦ç¦å›¢é˜Ÿå°†å‘å¸ƒæ›´å…ˆè¿›çš„ç•Œé¢â€”â€”å¯èƒ½å·²ç»æœ‰äº†ï¼Ÿâ€”â€”å¦‚æœæ²¡æœ‰çš„è¯ï¼Œä»ç„¶éœ€è¦åœ¨è¿™æ–¹é¢è¿›è¡Œæ”¹è¿›ã€‚
- en: Conclusions
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: This is a quick demo to hopefully help people get started with using STORM on
    their own documents. I havenâ€™t gone into systematic evaluation, something that
    would obviously need to be done if using STORM in a live environment. That said,
    I was impressed at how it seems to be able to get a relatively nuanced research
    topic and generate well-cited pre-writing research content that would help me
    in my own research.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªç®€çŸ­çš„æ¼”ç¤ºï¼Œæ—¨åœ¨å¸®åŠ©äººä»¬å¼€å§‹ä½¿ç”¨ STORM å¤„ç†è‡ªå·±çš„æ–‡æ¡£ã€‚æˆ‘æ²¡æœ‰è¿›è¡Œç³»ç»Ÿçš„è¯„ä¼°ï¼Œå¦‚æœåœ¨å®é™…ç¯å¢ƒä¸­ä½¿ç”¨ STORMï¼Œæ˜¾ç„¶éœ€è¦è¿›è¡Œè¿™é¡¹å·¥ä½œã€‚å°½ç®¡å¦‚æ­¤ï¼Œæˆ‘è¿˜æ˜¯å¯¹
    STORM èƒ½å¤Ÿç”Ÿæˆç›¸å¯¹ç»†è‡´çš„ç ”ç©¶ä¸»é¢˜å’Œç”Ÿæˆæœ‰è‰¯å¥½å¼•ç”¨çš„é¢„å†™ä½œç ”ç©¶å†…å®¹æ„Ÿåˆ°å°è±¡æ·±åˆ»ï¼Œè¿™å°†å¸®åŠ©æˆ‘è¿›è¡Œè‡ªå·±çš„ç ”ç©¶ã€‚
- en: References
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401),
    Lewis et al., 2020'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[çŸ¥è¯†å¯†é›†å‹ NLP ä»»åŠ¡çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ](https://arxiv.org/abs/2005.11401)ï¼ŒLewis ç­‰äººï¼Œ2020å¹´'
- en: '[Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/html/2312.10997v5#S2),
    Yunfan et al., 2024'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼šç»¼è¿°](https://arxiv.org/html/2312.10997v5#S2)ï¼ŒYunfan ç­‰äººï¼Œ2024å¹´'
- en: '[Assisting in Writing Wikipedia-like Articles From Scratch with Large Language
    Models](https://arxiv.org/abs/2402.14207), Shao et al., 2024'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[ä»é›¶å¼€å§‹åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ååŠ©æ’°å†™ç±»ä¼¼ Wikipedia çš„æ–‡ç« ](https://arxiv.org/abs/2402.14207)ï¼ŒShao ç­‰ï¼Œ2024'
- en: '[Into the Unknown Unknowns: Engaged Human Learning through Participation in
    Language Model Agent Conversations](https://www.arxiv.org/abs/2408.15232), Jiang
    et al., 2024'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[è¿›å…¥æœªçŸ¥çš„æœªçŸ¥ï¼šé€šè¿‡å‚ä¸è¯­è¨€æ¨¡å‹ä»£ç†å¯¹è¯è¿›è¡Œç§¯æçš„äººç±»å­¦ä¹ ](https://www.arxiv.org/abs/2408.15232)ï¼ŒJiang
    ç­‰ï¼Œ2024'
- en: '[MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries](https://arxiv.org/abs/2401.15391),
    Tang et al., 2024'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[MultiHop-RAGï¼šå¤šè·³æŸ¥è¯¢çš„æ£€ç´¢å¢å¼ºç”ŸæˆåŸºå‡†æµ‹è¯•](https://arxiv.org/abs/2401.15391)ï¼ŒTang ç­‰ï¼Œ2024'
- en: You can find the code for this article [here](https://github.com/dividor/storm-with-local-docs)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/dividor/storm-with-local-docs)æ‰¾åˆ°æœ¬æ–‡çš„ä»£ç 
- en: '***Please like this article if inclined and Iâ€™d be super delighted if you followed
    me! You can find more articles*** [***here***](/@astrobagel) ***or connect on***
    [***LinkedIn***](https://www.linkedin.com/in/matthew-harris-4018865/)***.***'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '***å¦‚æœå–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œè¯·ç‚¹èµï¼Œå¦‚æœä½ èƒ½å…³æ³¨æˆ‘ï¼Œæˆ‘å°†éå¸¸é«˜å…´ï¼ä½ å¯ä»¥åœ¨*** [***è¿™é‡Œ***](/@astrobagel) ***æ‰¾åˆ°æ›´å¤šæ–‡ç« ***ï¼Œ***æˆ–è€…åœ¨***
    [***LinkedIn***](https://www.linkedin.com/in/matthew-harris-4018865/) ***ä¸Šè”ç³»æˆ‘ã€‚***'
