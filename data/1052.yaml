- en: Relation Extraction with Llama3 Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Llama3模型进行关系抽取
- en: 原文：[https://towardsdatascience.com/relation-extraction-with-llama3-models-f8bc41858b9e?source=collection_archive---------0-----------------------#2024-04-26](https://towardsdatascience.com/relation-extraction-with-llama3-models-f8bc41858b9e?source=collection_archive---------0-----------------------#2024-04-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/relation-extraction-with-llama3-models-f8bc41858b9e?source=collection_archive---------0-----------------------#2024-04-26](https://towardsdatascience.com/relation-extraction-with-llama3-models-f8bc41858b9e?source=collection_archive---------0-----------------------#2024-04-26)
- en: '*Enhanced relation extraction by fine-tuning Llama3–8B with a synthetic dataset
    created using Llama3–70B*'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*通过微调Llama3-8B并使用Llama3-70B创建的合成数据集增强关系抽取*'
- en: '[](https://medium.com/@silviaonofrei?source=post_page---byline--f8bc41858b9e--------------------------------)[![Silvia
    Onofrei](../Images/198b04b2063b4269eaff52402dc5f8d5.png)](https://medium.com/@silviaonofrei?source=post_page---byline--f8bc41858b9e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f8bc41858b9e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f8bc41858b9e--------------------------------)
    [Silvia Onofrei](https://medium.com/@silviaonofrei?source=post_page---byline--f8bc41858b9e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@silviaonofrei?source=post_page---byline--f8bc41858b9e--------------------------------)[![Silvia
    Onofrei](../Images/198b04b2063b4269eaff52402dc5f8d5.png)](https://medium.com/@silviaonofrei?source=post_page---byline--f8bc41858b9e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f8bc41858b9e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f8bc41858b9e--------------------------------)
    [Silvia Onofrei](https://medium.com/@silviaonofrei?source=post_page---byline--f8bc41858b9e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f8bc41858b9e--------------------------------)
    ·12 min read·Apr 26, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f8bc41858b9e--------------------------------)
    ·12分钟阅读·2024年4月26日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/31614e3e1a26eb32ed70b3333c7d3913.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31614e3e1a26eb32ed70b3333c7d3913.png)'
- en: Generated with DALL-E.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由DALL-E生成。
- en: Premise
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前提
- en: Relation extraction (RE) is the task of extracting relationships from unstructured
    text to identify connections between various named entities. It is done in conjunction
    with named entity recognition (NER) and is an essential step in a natural langage
    processing pipeline. With the rise of Large Language Models (LLMs), traditional
    supervised approaches that involve tagging entity spans and classifying relationships
    (if any) between them are enhanced or entirely replaced by LLM-based approaches
    [[1](https://arxiv.org/pdf/2305.05003.pdf)].
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 关系抽取（RE）是从非结构化文本中提取关系的任务，用于识别不同命名实体之间的连接。这一任务通常与命名实体识别（NER）一起进行，是自然语言处理流水线中的一个重要步骤。随着大型语言模型（LLMs）的兴起，传统的监督方法——通过标注实体跨度并分类实体之间（如果有的话）的关系——在LLM驱动的方法下得到了增强或完全替代[[1](https://arxiv.org/pdf/2305.05003.pdf)]。
- en: Llama3 is the most recent major release in the domain of GenerativeAI [[2](https://ai.meta.com/blog/meta-llama-3/)].
    The base model is available in two sizes, 8B and 70B, with a 400B model expected
    to be released soon. These models are available on the HuggingFace platform; see
    [[3](https://huggingface.co/blog/llama3)] for details. The 70B variant powers
    Meta’s new chat website [Meta.ai](http://Meta.ai) and exhibits performance comparable
    to ChatGPT. The 8B model is among the most performant in its class. The architecture
    of Llama3 is similar to that of Llama2, with the increase in performance primarily
    due to data upgrading. The model comes with an upgaded tokenizer and expanded
    context window. It is labelled as open-source, although only a small percentage
    of the data is released. Overall, it is an excellent model, and I cannot wait
    to give it a try.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Llama3是生成式AI领域最近发布的主要版本[[2](https://ai.meta.com/blog/meta-llama-3/)]。基础模型有两种规格，分别为8B和70B，预计很快会发布400B版本。这些模型可在HuggingFace平台上使用，详情请见[[3](https://huggingface.co/blog/llama3)]。70B版本为Meta的全新聊天网站[Meta.ai](http://Meta.ai)提供支持，展现出与ChatGPT相当的性能。8B模型在同类中表现优异。Llama3的架构与Llama2相似，性能提升主要得益于数据升级。该模型配备了升级版的分词器和扩展的上下文窗口。虽然被标记为开源，但仅发布了很小一部分数据。总体来说，这是一个非常优秀的模型，我迫不及待想尝试一下。
- en: Llama3–70B can produce amazing results, but due to its size it is impractical,
    prohibitively expensive and hard to use on local systems. Therefore, to leverage
    its capabilities, we have Llama3–70B teach the smaller Llama3–8B the task of relation
    extraction from unstructured text.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Llama3–70B能够产生惊人的结果，但由于其规模庞大，它在本地系统上使用起来不切实际，成本高昂且难以操作。因此，为了充分利用其能力，我们让Llama3–70B教导较小的Llama3–8B如何从非结构化文本中进行关系抽取。
- en: Specifically, with the help of Llama3–70B, we build a supervised fine-tuning
    dataset aimed at relation extraction. We then use this dataset to fine-tune Llama3–8B
    to enhance its relation extraction capabilities.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，在Llama3–70B的帮助下，我们构建了一个针对关系抽取的有监督微调数据集。然后，我们使用该数据集对Llama3–8B进行微调，以增强其关系抽取能力。
- en: 'To reproduce the code in the [Google Colab Notebook](https://github.com/SolanaO/Blogs_Content/blob/master/llama3_re/Llama3_RE_Inference_SFT.ipynb)
    associated to this blog, you will need:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了复现与本博客相关的[Google Colab Notebook](https://github.com/SolanaO/Blogs_Content/blob/master/llama3_re/Llama3_RE_Inference_SFT.ipynb)中的代码，你将需要：
- en: HuggingFace credentials (to save the fine-tuned model, optional) and Llama3
    access, which can be obtained by following the instructions from one of the models’
    cards;
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HuggingFace凭证（用于保存微调后的模型，可选）和Llama3访问权限，可以通过按照模型卡中的指示获取；
- en: A free [GroqCloud](https://console.groq.com) account (you can loggin with a
    Google account) and a corresponding API Key.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个免费的[GroqCloud](https://console.groq.com)账户（你可以使用Google账户登录）和相应的API密钥。
- en: Workspace Setup
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作空间设置
- en: For this project I used a Google Colab Pro equipped with an A100 GPU and a High-RAM
    setting.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目，我使用了一台配备A100 GPU和高内存设置的Google Colab Pro。
- en: 'We start by installing all the required libraries:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先安装所有需要的库：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: I was very pleased to notice that the entire setup worked from the beginning
    without any dependencies issues or the need to install `transformers` from the
    source, despite the novelty of the model.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我很高兴地注意到，整个设置从一开始就能够顺利运行，没有出现任何依赖问题，也无需从源代码安装`transformers`，尽管该模型是新的。
- en: 'We also need to give access Goggle Colab to the drive and files and set the
    working directory:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要授权Google Colab访问Google Drive和文件，并设置工作目录：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For those who wish to upload the model to the HuggingFace Hub, we need to upload
    the Hub credentials. In my case, these are stored in Google Colab secrets, which
    can be accessed via the key button on the left. This step is optional.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些希望将模型上传到HuggingFace Hub的人，我们需要上传Hub凭证。在我的情况下，这些凭证存储在Google Colab的机密中，可以通过左侧的密钥按钮访问。此步骤为可选。
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'I also added some path variables to simplify file access:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我还添加了一些路径变量，以简化文件访问：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now that our workspace is set up, we can move to the first step, which is to
    build a synthetic dataset for the task of relation extraction.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的工作空间已经设置好，我们可以进入第一步，即为关系抽取任务构建一个合成数据集。
- en: Creating a Synthetic Dataset for Relation Extraction with Llama3–70B
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建用于关系抽取的合成数据集，采用Llama3–70B
- en: There are several relation extraction datasets available, with the best-known
    being the [CoNLL04](https://paperswithcode.com/dataset/conll04) dataset. Additionally,
    there are excellent datasets such as [web_nlg](https://huggingface.co/datasets/web_nlg#dataset-card-for-webnlg),
    available on HuggingFace, and [SciREX](https://github.com/allenai/SciREX?tab=readme-ov-file)
    developed by AllenAI. However, most of these datasets come with restrictive licenses.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个关系抽取数据集可供使用，其中最著名的是[CoNLL04](https://paperswithcode.com/dataset/conll04)数据集。此外，还有一些优秀的数据集，如HuggingFace上的[web_nlg](https://huggingface.co/datasets/web_nlg#dataset-card-for-webnlg)和AllenAI开发的[SciREX](https://github.com/allenai/SciREX?tab=readme-ov-file)。然而，这些数据集中的大多数都带有限制性许可证。
- en: Inspired by the format of the `web_nlg` dataset we will build our own dataset.
    This approach will be particularly useful if we plan to fine-tune a model trained
    on our dataset. To start, we need a collection of short sentences for our relation
    extraction task. We can compile this corpus in various ways.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 受到`web_nlg`数据集格式的启发，我们将构建我们自己的数据集。如果我们计划对一个在我们数据集上训练的模型进行微调，这种方法将特别有用。首先，我们需要收集一批短句子来进行关系抽取任务。我们可以通过多种方式来编纂这个语料库。
- en: Gather a Collection of Sentences
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 收集句子集合
- en: 'We will use [databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k),
    an open source dataset generated by Databricks employees in 2023\. This dataset
    is designed for supervised fine-tuning and includes four features: instruction,
    context, response and category. After analyzing the eight categories, I decided
    to retain the first sentence of the context from the `information_extraction`
    category. The data parsing steps are outlined below:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用[databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k)，这是一个由Databricks员工在2023年生成的开源数据集。该数据集设计用于监督微调，并包括四个特征：指令、上下文、响应和类别。分析了这八个类别后，我决定保留`information_extraction`类别中的上下文的第一句话。数据解析步骤如下：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The selection process yields a dataset comprising 1,041 sentences. Given that
    this is a mini-project, I did not handpick the sentences, and as a result, some
    samples may not be ideally suited for our task. In a project designated for production,
    I would carefully select only the most appropriate sentences. However, for the
    purposes of this project, this dataset will suffice.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 选择过程产生了一个包含1,041个句子的数据集。鉴于这是一个小型项目，我没有精心挑选句子，因此一些样本可能不太适合我们的任务。在一个用于生产的项目中，我会仔细选择最合适的句子。然而，出于这个项目的目的，这个数据集已经足够了。
- en: Format the Data
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 格式化数据
- en: 'We first need to create a system message that will define the input prompt
    and instruct the model on how to generate the answers:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要创建一个系统消息，定义输入提示并指示模型如何生成答案：
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Since this is an experimental phase, I am keeping the demands on the model to
    a minimum. I did test several other prompts, including some that requested outputs
    in CoNLL format where entities are categorized, and the model performed quite
    well. However, for simplicity’s sake, we’ll stick to the basics for now.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个实验阶段，我对模型的要求保持最低限度。我测试了几个其他的提示，包括一些要求以CoNLL格式输出的提示，其中实体会被分类，模型的表现相当不错。然而，出于简便考虑，我们暂时还是从基础做起。
- en: 'We also need to convert the data into a conversational format:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要将数据转换为对话格式：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The Groq Client and API
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Groq客户端和API
- en: Llama3 was released just a few days ago, and the availability of API options
    is still limited. While a chat interface is available for Llama3–70B, this project
    requires an API that could process my 1,000 sentences with a couple lines of code.
    I found this excellent [YouTube video](https://www.youtube.com/watch?v=ySwJT3Z1MFI)
    that explains how to use the GroqCloud API for free. For more details please refer
    to the video.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Llama3几天前刚发布，API选项的可用性仍然有限。虽然Llama3–70B提供了一个聊天接口，但这个项目需要一个API，能够用几行代码处理我的1,000个句子。我找到了这个很棒的[YouTube视频](https://www.youtube.com/watch?v=ySwJT3Z1MFI)，它解释了如何免费使用GroqCloud
    API。更多细节请参考视频。
- en: 'Just a reminder: you’ll need to log in and retrieve a free API Key from the
    [GroqCloud](https://console.groq.com/playground) website. My API key is already
    saved in the Google Colab secrets. We start by initializing the Groq client:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 提个提醒：你需要登录并从[GroqCloud](https://console.groq.com/playground)网站获取一个免费的API密钥。我的API密钥已经保存在Google
    Colab的密钥中。我们从初始化Groq客户端开始：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next we need to define a couple of helper functions that will enable us to
    interact with the [Meta.ai](http://meta.ai/) chat interface effectively (these
    are adapted from the [YouTube video](https://www.youtube.com/watch?v=ySwJT3Z1MFI)):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要定义几个辅助函数，使我们能够有效地与[Meta.ai](http://meta.ai/)聊天接口互动（这些函数改编自[YouTube视频](https://www.youtube.com/watch?v=ySwJT3Z1MFI)）：
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The first function `process_data()` serves as a wrapper for the chat completion
    function of the Groq client. The second function `send_messages()`, processes
    the data in small batches. If you follow the Settings link on the Groq playground
    page, you will find a link to [Limits](https://console.groq.com/settings/limits)
    which details the conditions under which we can use the free API, including caps
    on the number of requests and generated tokens. To avoid exceedind these limits,
    I added a 10-seconds delay after each batch of 10 messages, although it wasn’t
    strictly necessary in my case. You might want to experiment with these settings.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个函数`process_data()`作为Groq客户端聊天完成函数的封装器。第二个函数`send_messages()`将数据分批处理。如果你在Groq
    Playground页面点击设置链接，你会找到一个指向[Limits](https://console.groq.com/settings/limits)的链接，里面详细列出了我们可以使用免费API的条件，包括请求次数和生成的token的上限。为了避免超出这些限制，我在每批10条消息后加入了10秒的延迟，虽然在我的情况下这并非严格必要。你可能会想尝试调整这些设置。
- en: 'What remains now is to generate our relation extraction data and integrate
    it with the initial dataset :'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在剩下的任务是生成我们的关系抽取数据，并将其与初始数据集进行整合：
- en: '[PRE9]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Evaluating Llama3–8B for Relation Extraction
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估 Llama3–8B 的关系抽取能力
- en: Before proceeding with fine-tuning the model, it’s important to evaluate its
    performance on several samples to determine if fine-tuning is indeed necessary.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始微调模型之前，评估模型在多个样本上的表现非常重要，以确定是否真的需要微调。
- en: Building a Testing Dataset
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建测试数据集
- en: We will select 20 samples from the dataset we just constructed and set them
    aside for testing. The remainder of the dataset will be used for fine-tuning.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从我们刚刚构建的数据集中选择 20 个样本，并将它们分开用于测试。其余的数据集将用于微调。
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We will use the GroqCloud API and the utilities defined above, specifying `model=llama3-8b-8192`
    while the rest of the function remains unchanged. In this case, we can directly
    process our small dataset without concern of exceeded the API limits.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 GroqCloud API 和上述定义的工具，指定 `model=llama3-8b-8192`，其余的函数保持不变。在这种情况下，我们可以直接处理我们的较小数据集，而不必担心超出
    API 限制。
- en: Here is a sample output that provides the original `text`, the Llama3-70B generation
    denoted `gold_re` and the Llama3-8B hgeneration labelled `test_re`.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个示例输出，提供了原始的 `text`，Llama3-70B 生成的 `gold_re` 和 Llama3-8B 生成的 `test_re`。
- en: '[PRE11]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: For the full test dataset, please refer to the [Google Colab notebook.](https://github.com/SolanaO/Blogs_Content/blob/master/llama3_re/Llama3_RE_Inference_SFT.ipynb)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 关于完整的测试数据集，请参阅 [Google Colab 笔记本。](https://github.com/SolanaO/Blogs_Content/blob/master/llama3_re/Llama3_RE_Inference_SFT.ipynb)
- en: Just from this example, it becomes clear that Llama3–8B could benefit from some
    improvements in its relation extraction capabilities. Let’s work on enhancing
    that.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 仅从这个例子来看，Llama3–8B 在关系抽取能力上可能需要一些改进。让我们着手提升这一点。
- en: Supervised Fine Tuning of Llama3–8B
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Llama3–8B 的监督微调
- en: We will utilize a full arsenal of techniques to assist us, including QLoRA and
    Flash Attention. I won’t delve into the specifics of choosing hyperparameters
    here, but if you’re interested in exploring further, check out these great references
    [[4](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)]
    and [[5](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl)].
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一整套技术来辅助，包括 QLoRA 和 Flash Attention。我在这里不会深入讨论选择超参数的具体细节，但如果你有兴趣深入了解，可以参考这些很棒的文献
    [[4](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)]
    和 [[5](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl)]。
- en: The A100 GPU supports Flash Attention and bfloat16, and it possesses about 40GB
    of memory, which is sufficient for our fine-tuning needs.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: A100 GPU 支持 Flash Attention 和 bfloat16，拥有约 40GB 的内存，足以满足我们对微调的需求。
- en: '**Preparing the SFT Dataset**'
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**准备 SFT 数据集**'
- en: 'We start by parsing the dataset into a conversational format, including a system
    message, input text and the desired answer, which we derive from the Llama3–70B
    generation. We then save it as a HuggingFace dataset:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将数据集解析为对话格式，包括系统消息、输入文本和期望的答案，这些答案来自 Llama3–70B 生成的内容。然后我们将其保存为 HuggingFace
    数据集：
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Choose the Model
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择模型
- en: '[PRE13]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Load the Tokenizer
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载分词器
- en: '[PRE14]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Choose Quantization Parameters
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择量化参数
- en: '[PRE15]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Load the Model
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载模型
- en: '[PRE16]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: LoRA Configuration
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LoRA 配置
- en: '[PRE17]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The best results are achieved when targeting all the linear layers. If memory
    constraints are a concern, opting for more standard values such as alpha=32 and
    rank=16 can be beneficial, as these settings result in significantly fewer parameters.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当针对所有线性层时，最佳结果可得到。如果内存限制是一个问题，选择更标准的值，如 alpha=32 和 rank=16，可以带来好处，因为这些设置会大幅减少参数量。
- en: Training Arguments
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练参数
- en: '[PRE18]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: If you choose to save the model locally, you can omit the last three parameters.
    You may also need to adjust the `per_device_batch_size` and `gradient_accumulation_steps`
    to prevent Out of Memory (OOM) errors.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你选择将模型保存到本地，可以省略最后三个参数。你还可能需要调整 `per_device_batch_size` 和 `gradient_accumulation_steps`，以防止内存溢出（OOM）错误。
- en: Initialize the Trainer and Train the Model
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始化训练器并训练模型
- en: '[PRE19]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The training, including model saving, took about 10 minutes.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程，包括模型保存，大约花费了 10 分钟。
- en: Let’s clear the memory to prepare for inference tests. If you’re using a GPU
    with less memory and encounter CUDA Out of Memory (OOM) errors, you might need
    to restart the runtime.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们清理内存，为推理测试做准备。如果你使用的是内存较小的 GPU，并遇到 CUDA 内存溢出（OOM）错误，可能需要重启运行时环境。
- en: '[PRE20]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Inference with SFT Model
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 SFT 模型进行推理
- en: In this final step we will load the base model in half precision along with
    the Peft adapter. For this test, I have chosen not to merge the model with the
    adapter.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步中，我们将以半精度加载基础模型以及 Peft 适配器。对于此测试，我选择不将模型与适配器合并。
- en: '[PRE21]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, we load the tokenizer:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们加载分词器：
- en: '[PRE22]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'And we build the text generation pipeline:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建了文本生成流水线：
- en: '[PRE23]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We load the test dataset, which consists of the 20 samples we set aside previously,
    and format the data in a conversational style. However, this time we omit the
    assistant message and format it as a Hugging Face dataset:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载测试数据集，该数据集由我们之前预留的20个样本组成，并将数据格式化为对话风格。然而，这次我们省略了助手消息，并将其格式化为 Hugging Face
    数据集：
- en: '[PRE24]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: One Sample Test
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单一样本测试
- en: 'Let’s generate relation extraction output using SFT Llama3–8B and compare it
    to the previous two outputs on a single instance:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 SFT Llama3–8B 生成关系提取输出，并将其与之前两个输出在单一实例上的结果进行比较：
- en: '[PRE25]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We obtain the following:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了以下结果：
- en: '[PRE26]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In this example, we observe significant improvements in the relation extraction
    capabilities of Llama3–8B through fine-tuning. Despite the fine-tuning dataset
    being neither very clean nor particularly large, the results are impressive.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们观察到 Llama3–8B 在关系提取能力上的显著提升，尽管微调数据集既不特别干净，也不算非常大，但结果仍然令人印象深刻。
- en: For the complete results on the 20-sample dataset, please refer to the [Google
    Colab notebook](https://github.com/SolanaO/Blogs_Content/blob/master/llama3_re/Llama3_RE_Inference_SFT.ipynb).
    Note that the inference test takes longer because we load the model in half-precision.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 关于20个样本数据集的完整结果，请参考 [Google Colab notebook](https://github.com/SolanaO/Blogs_Content/blob/master/llama3_re/Llama3_RE_Inference_SFT.ipynb)。请注意，由于我们以半精度加载模型，推理测试需要更长时间。
- en: Conclusion
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In conclusion, by utilizing Llama3–70B and an available dataset, we successfully
    created a synthetic dataset which was then used to fine-tune Llama3–8B for a specific
    task. This process not only familiarized us with Llama3, but also allowed us to
    apply straightforward techniques from Hugging Face. We observed that working with
    Llama3 closely resembles the experience with Llama2, with the notable improvements
    being enhanced output quality and a more effective tokenizer.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 总结，通过利用 Llama3–70B 和现有数据集，我们成功地创建了一个合成数据集，并用于微调 Llama3–8B 以完成特定任务。这个过程不仅让我们熟悉了
    Llama3，也使我们能够应用 Hugging Face 的简单技巧。我们观察到，使用 Llama3 与 Llama2 的体验非常相似，显著的改进包括更高质量的输出和更有效的分词器。
- en: For those interested in pushing the boundaries further, consider challenging
    the model with more complex tasks such as categorizing entities and relationships,
    and using these classifications to build a knowledge graph.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些希望进一步拓展边界的人来说，可以考虑挑战模型完成更复杂的任务，例如分类实体和关系，并利用这些分类来构建知识图谱。
- en: References
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Somin Wadhwa, Silvio Amir, Byron C. Wallace, Revisiting Relation Extraction
    in the era of Large Language Models, [arXiv.2305.05003](https://arxiv.org/pdf/2305.05003.pdf)
    (2023).
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Somin Wadhwa, Silvio Amir, Byron C. Wallace, 在大型语言模型时代重新审视关系提取，[arXiv.2305.05003](https://arxiv.org/pdf/2305.05003.pdf)（2023年）。
- en: 'Meta, Introducing Meta Llama 3: The most capable openly available LLM to date,
    April 18, 2024 ([link)](https://ai.meta.com/blog/meta-llama-3/).'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Meta，介绍 Meta Llama 3：迄今为止最强大的公开可用 LLM，2024年4月18日 ([link)](https://ai.meta.com/blog/meta-llama-3/)。
- en: Philipp Schmid, Omar Sanseviero, Pedro Cuenca, Youndes Belkada, Leandro von
    Werra, [Welcome Llama 3 — Met’s new open LLM,](https://huggingface.co/blog/llama3)
    April 18, 2024.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Philipp Schmid, Omar Sanseviero, Pedro Cuenca, Youndes Belkada, Leandro von
    Werra，[欢迎 Llama 3 —— Meta 的新开放 LLM](https://huggingface.co/blog/llama3)，2024年4月18日。
- en: Sebastian Raschka, [Practical Tips for Finetuning LLMs Using LoRA (Low-Rank
    Adaptation)](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms),
    Ahead of AI, Nov 19, 2023.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sebastian Raschka，[使用 LoRA（低秩适配）微调 LLM 的实用技巧](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)，Ahead
    of AI，2023年11月19日。
- en: Philipp Schmid, [How to Fine-Tune LLMs in 2024 with Hugging Face,](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl)
    Jan 22, 2024.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Philipp Schmid，[如何在 2024 年使用 Hugging Face 微调 LLM](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl)，2024年1月22日。
- en: Dataset
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集
- en: '[databricks-dolly-15K](https://huggingface.co/datasets/databricks/databricks-dolly-15k)
    on Hugging Face platform (CC BY-SA 3.0)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[databricks-dolly-15K](https://huggingface.co/datasets/databricks/databricks-dolly-15k)
    在 Hugging Face 平台（CC BY-SA 3.0）'
- en: Full Code and Processed Data
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 完整代码与处理后的数据
- en: '[Github Repo](https://github.com/SolanaO/Blogs_Content/tree/master/llama3_re)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[Github Repo](https://github.com/SolanaO/Blogs_Content/tree/master/llama3_re)'
