- en: The Math Behind Batch Normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-math-behind-batch-normalization-90ebbc0b1b0b?source=collection_archive---------2-----------------------#2024-05-08](https://towardsdatascience.com/the-math-behind-batch-normalization-90ebbc0b1b0b?source=collection_archive---------2-----------------------#2024-05-08)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Explore Batch Normalization, a cornerstone of neural networks, understand its
    mathematics, and implement it from scratch.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@cristianleo120?source=post_page---byline--90ebbc0b1b0b--------------------------------)[![Cristian
    Leo](../Images/99074292e7dfda50cf50a790b8deda79.png)](https://medium.com/@cristianleo120?source=post_page---byline--90ebbc0b1b0b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--90ebbc0b1b0b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--90ebbc0b1b0b--------------------------------)
    [Cristian Leo](https://medium.com/@cristianleo120?source=post_page---byline--90ebbc0b1b0b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--90ebbc0b1b0b--------------------------------)
    ·21 min read·May 8, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1702131733bebaab170620d4d519738f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: Batch Normalization is a key technique in neural networks as it standardizes
    the inputs to each layer. It tackles the problem of internal covariate shift,
    where the input distribution of each layer shifts during training, complicating
    the learning process and reducing efficiency. By normalizing these inputs, Batch
    Normalization helps networks train faster and more consistently. This method is
    vital for ensuring reliable performance across different network architectures
    and tasks, including image recognition and natural language processing. In this
    article, we’ll delve into the principles and math behind Batch Normalization and
    show you how to implement it in Python from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: '**Index**'
  prefs: []
  type: TYPE_NORMAL
- en: '[**1: Introduction**](#4495)'
  prefs: []
  type: TYPE_NORMAL
- en: '[**2: Need for Normalization**](#aae3)'
  prefs: []
  type: TYPE_NORMAL
- en: '[**3: Math and Mechanisms**](#477c)'
  prefs: []
  type: TYPE_NORMAL
- en: '[∘ 3.1: Overcoming Covariate Shift](https://medium.com/p/90ebbc0b1b0b/edit#cd9f)'
  prefs: []
  type: TYPE_NORMAL
- en: '∘ [3.2: Scale and Shift Step](#312b)'
  prefs: []
  type: TYPE_NORMAL
- en: '∘ [3.3: Flow of Batch Normalization](#b097)'
  prefs: []
  type: TYPE_NORMAL
- en: '∘ [3.4: Activation Distribution](#e2d9)'
  prefs: []
  type: TYPE_NORMAL
- en: '[**4: Application From Scratch in Python**](#df72)'
  prefs: []
  type: TYPE_NORMAL
- en: '∘ [4.1: Batch Normalization From Scratch](#2add)…'
  prefs: []
  type: TYPE_NORMAL
