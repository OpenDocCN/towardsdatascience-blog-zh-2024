- en: How does ReLU enable Neural Networks to approximate continuous nonlinear functions?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-relu-enables-neural-networks-to-approximate-continuous-nonlinear-functions-f171b7859727?source=collection_archive---------1-----------------------#2024-01-21](https://towardsdatascience.com/how-relu-enables-neural-networks-to-approximate-continuous-nonlinear-functions-f171b7859727?source=collection_archive---------1-----------------------#2024-01-21)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn how neural networks with one hidden layer using ReLU activation represent
    continuous nonlinear functions.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@lamthuy.lt?source=post_page---byline--f171b7859727--------------------------------)[![Thi-Lam-Thuy
    LE](../Images/a5af515421fe186f5ad47c532932af03.png)](https://medium.com/@lamthuy.lt?source=post_page---byline--f171b7859727--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f171b7859727--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f171b7859727--------------------------------)
    [Thi-Lam-Thuy LE](https://medium.com/@lamthuy.lt?source=post_page---byline--f171b7859727--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f171b7859727--------------------------------)
    ·5 min read·Jan 21, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions play an integral role in Neural Networks *(NNs)* since
    they introduce non-linearity that allows the network to learn more complex features
    and functions than just a linear regression. One of the most commonly used activation
    functions is Rectified Linear Unit *(ReLU),* which has been [theoretically](https://en.wikipedia.org/wiki/Universal_approximation_theorem)
    shown to enable NNs to approximate a wide range of continuous functions, making
    them powerful function approximators.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this post, we study in particular the approximation of Continuous NonLinear
    *(CNL)* functions, the main purpose of using a NN over a simple linear regression
    model. More precisely, we investigate 2 sub-categories of CNL functions: Continuous
    PieceWise Linear *(CPWL)*, and Continuous Curve *(CC)* functions. We will show
    how these two function types can be represented using a NN that consists of one
    hidden layer, given enough neurons with ReLU activation.'
  prefs: []
  type: TYPE_NORMAL
- en: For illustrative purposes, we consider only single feature inputs yet the idea
    applies to multiple feature inputs as well.
  prefs: []
  type: TYPE_NORMAL
- en: ReLU activation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/e249461664cadb9fb803d9a1a41537b3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Rectified Linear Unit (ReLU) function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'ReLU is a piecewise linear function that consists of two linear pieces: one
    that cuts off negative values where the output is zero, and one that provides
    a continuous linear mapping for non negative values.'
  prefs: []
  type: TYPE_NORMAL
- en: Continuous piecewise linear function approximation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*CPWL functions are continuous functions with multiple linear portions. The
    slope is consistent on each portion, then changes abruptly at transition points
    by adding new linear functions.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f83d489627298162c04ec44a6e727c2f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Example of CPWL function approximation using NN. At each transition
    point, a new ReLU function is added to/subtracted from the input to increase/decrease
    the slope.'
  prefs: []
  type: TYPE_NORMAL
- en: In a NN with one hidden layer using ReLU activation and a linear output layer,
    the activation outputs are aggregated to form the CPWL target function. Each unit
    of the hidden layer is responsible for a linear piece. At each unit, a new ReLU
    function that corresponds to the changing of slope is added to produce the new
    slope *(cf. Fig.2)*. Since this activation function is always positive, the weights
    of the output layer corresponding to units that increase the slope will be positive,
    and conversely, the weights corresponding to units that decreases the slope will
    be negative *(cf. Fig.3)*. The new function is added at the transition point and
    does not contribute to the resulting function prior to (and sometimes after) that
    point due to the disabling range of the ReLU activation function.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/625c25d0d2b613b371adb52150ba4fd0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Approximation of the CPWL target function in Fig.2 using a NN that
    consists of one hidden layer with ReLU activation and a linear output layer.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Example**'
  prefs: []
  type: TYPE_NORMAL
- en: To make it more concrete, we consider an example of a CPWL function that consists
    of 4 linear segments defined as below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/55c82ec41548f85c2cc586fc300cac56.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Example of a PWL function.'
  prefs: []
  type: TYPE_NORMAL
- en: To represent this target function, we will use a NN with 1 hidden layer of 4
    units and a linear layer that outputs the weighted sum of the previous layer’s
    activation outputs. Let’s determine the network’s parameters so that each unit
    in the hidden layer represents a segment of the target. For the sake of this example,
    the bias of the output layer *(b2_0)* is set to 0.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/980a1377e98898a948078d0ac3d8018f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The network architecture to model the PWL function defined in Fig.4.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c8c5123073a3fe737f0b292152ad1d8a.png)![](../Images/c2dc3ff0289cbb5110ed968fc1cfebb7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The activation output of unit 0 (a1_0).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/479c32a0674d6f52eecae6d71b0cfc19.png)![](../Images/af866141263ed92ed63c95f0222a2544.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The activation output of unit 1 (a1_1), which is aggregated to the
    output *(a2_0)* to produce the segment (2). The red arrow represents the change
    in slope.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2cc99c9616fd854e28c2c64dc2db9bdd.png)![](../Images/c2c7c57e436713ba9a76c8380b1c3680.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: The output of unit 2 (a1_2), which is aggregated to the output (a2_0)
    to produce the segment (3). The red arrow represents the change in slope.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cc95f6c47f81f48362d89b9f2456ade4.png)![](../Images/eb8a7102340e818e0dc2fc793b9e6c16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: The output of unit 3 (a1_3), which is aggregated to the output (a2_0)
    to produce the segment (4). The red arrow represents the change in slope.'
  prefs: []
  type: TYPE_NORMAL
- en: Continuous curve function approximation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next type of continuous nonlinear function that we will study is CC function.
    There is not a proper definition for this sub-category, but *an informal way to
    define CC functions is continuous nonlinear functions that are not piecewise linear.
    Several examples of CC functions are: quadratic function, exponential function,
    sinus function, etc.*'
  prefs: []
  type: TYPE_NORMAL
- en: A CC function can be approximated by a series of infinitesimal linear pieces,
    which is called a piecewise linear approximation of the function. The greater
    the number of linear pieces and the smaller the size of each segment, the better
    the approximation is to the target function. Thus, the same network architecture
    as previously with a large enough number of hidden units can yield good approximation
    for a curve function.
  prefs: []
  type: TYPE_NORMAL
- en: However, in reality, the network is trained to fit a given dataset where the
    input-output mapping function is unknown. An architecture with too many neurons
    is prone to overfitting, high variance, and requires more time to train. Therefore,
    an appropriate number of hidden units should be large enough to properly fit the
    data and at the same time, small enough to avoid overfitting. Moreover, with a
    limited number of neurons, a good approximation with low loss has more transition
    points in restricted domain, rather than equidistant transition points in an uniform
    sampling way (as shown in *Fig.10*).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/259fc3f5bef997908f75399fc78918ca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Two piecewise linear approximations for a continuous curve function
    (in dashed line). The approximation 1 has more transition points in restricted
    domain and model the target function better than the approximation 2.'
  prefs: []
  type: TYPE_NORMAL
- en: Wrap up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this post, we have studied how ReLU activation function allows multiple units
    to contribute to the resulting function without interfering, thus enables continuous
    nonlinear function approximation. In addition, we have discussed about the choice
    of network architecture and number of hidden units in order to obtain a good approximation
    result.
  prefs: []
  type: TYPE_NORMAL
- en: '*I hope that this post is useful for your Machine Learning learning process!*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Further questions to think about:**'
  prefs: []
  type: TYPE_NORMAL
- en: How does the approximation ability change if the number of hidden layers with
    ReLU activation increases?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How ReLU activations are used for a classification problem?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Unless otherwise noted, all images are by the author'
  prefs: []
  type: TYPE_NORMAL
