<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>GPT from Scratch with MLX</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>GPT from Scratch with MLX</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gpt-from-scratch-with-mlx-acf2defda30e?source=collection_archive---------0-----------------------#2024-06-15">https://towardsdatascience.com/gpt-from-scratch-with-mlx-acf2defda30e?source=collection_archive---------0-----------------------#2024-06-15</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="7a2d" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Define and train GPT-2 on your MacBook</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@pranavj1?source=post_page---byline--acf2defda30e--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Pranav Jadhav" class="l ep by dd de cx" src="../Images/363dc9008e3e4d94a9566057cad59806.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*Fqwd42cKgKUVbXBYq4sUDg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--acf2defda30e--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@pranavj1?source=post_page---byline--acf2defda30e--------------------------------" rel="noopener follow">Pranav Jadhav</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--acf2defda30e--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">31 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jun 15, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/e34fda6779460bc952923c13f0e9e939.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*A_3QkwS4aKkpZtG-"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@szolkin?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Sergey Zolkin</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="9f43" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">My goal with this post is to walk you through defining and training GPT-2 from scratch with <a class="af nc" href="https://github.com/ml-explore/mlx" rel="noopener ugc nofollow" target="_blank">MLX</a>, Apple’s machine-learning library for Apple silicon. I want to leave no stone unturned from tokenizer to sampling. In the spirit of <a class="af nc" href="https://www.youtube.com/watch?v=kCc8FmEb1nY&amp;t=5978s" rel="noopener ugc nofollow" target="_blank">Karpathy’s excellent GPT from scratch tutorial</a>, we will train a model on the works of Shakespeare [1]. We will start with a blank Python file and end with a piece of software that can write Shakespeare-like text. And we’ll build it all in MLX, which makes training on inference on Apple silicon much faster.</p><p id="a73e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This post is best experienced by following along. The code is contained in the following repo which I suggest opening and referencing.</p><div class="nz oa ob oc od oe"><a href="https://github.com/pranavjad/mlx-gpt2?source=post_page-----acf2defda30e--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="of ab ig"><div class="og ab co cb oh oi"><h2 class="bf fr hw z io oj iq ir ok it iv fp bk">GitHub - pranavjad/mlx-gpt2</h2><div class="ol l"><h3 class="bf b hw z io oj iq ir ok it iv dx">Contribute to pranavjad/mlx-gpt2 development by creating an account on GitHub.</h3></div><div class="om l"><p class="bf b dy z io oj iq ir ok it iv dx">github.com</p></div></div><div class="on l"><div class="oo l op oq or on os lr oe"/></div></div></a></div><h1 id="fe58" class="ot ou fq bf ov ow ox gq oy oz pa gt pb pc pd pe pf pg ph pi pj pk pl pm pn po bk">Table of Contents</h1><ul class=""><li id="ff62" class="nd ne fq nf b go pp nh ni gr pq nk nl nm pr no np nq ps ns nt nu pt nw nx ny pu pv pw bk"><a class="af nc" href="#c3a0" rel="noopener ugc nofollow">Preparing the data</a></li><li id="8e97" class="nd ne fq nf b go px nh ni gr py nk nl nm pz no np nq qa ns nt nu qb nw nx ny pu pv pw bk"><a class="af nc" href="#1d50" rel="noopener ugc nofollow">Coding GPT-2</a></li><li id="1898" class="nd ne fq nf b go px nh ni gr py nk nl nm pz no np nq qa ns nt nu qb nw nx ny pu pv pw bk"><a class="af nc" href="#9d27" rel="noopener ugc nofollow">Input Embeddings</a></li><li id="18f7" class="nd ne fq nf b go px nh ni gr py nk nl nm pz no np nq qa ns nt nu qb nw nx ny pu pv pw bk"><a class="af nc" href="#5507" rel="noopener ugc nofollow">Positional Embeddings</a></li><li id="7c33" class="nd ne fq nf b go px nh ni gr py nk nl nm pz no np nq qa ns nt nu qb nw nx ny pu pv pw bk"><a class="af nc" href="#b6bc" rel="noopener ugc nofollow">Self Attention</a></li><li id="64ea" class="nd ne fq nf b go px nh ni gr py nk nl nm pz no np nq qa ns nt nu qb nw nx ny pu pv pw bk"><a class="af nc" href="#718c" rel="noopener ugc nofollow">Keys, Queries, and Values</a></li><li id="b7dd" class="nd ne fq nf b go px nh ni gr py nk nl nm pz no np nq qa ns nt nu qb nw nx ny pu pv pw bk"><a class="af nc" href="#38de" rel="noopener ugc nofollow">Multi-Head Attention</a></li><li id="b7b4" class="nd ne fq nf b go px nh ni gr py nk nl nm pz no np nq qa ns nt nu qb nw nx ny pu pv pw bk"><a class="af nc" href="#43c1" rel="noopener ugc nofollow">MLP</a></li><li id="433c" class="nd ne fq nf b go px nh ni gr py nk nl nm pz no np nq qa ns nt nu qb nw nx ny pu pv pw bk"><a class="af nc" href="#91b3" rel="noopener ugc nofollow">Block</a></li><li id="f96f" class="nd ne fq nf b go px nh ni gr py nk nl nm pz no np nq qa ns nt nu qb nw nx ny pu pv pw bk"><a class="af nc" href="#dc6e" rel="noopener ugc nofollow">Layernorms and Skip Connections</a></li><li id="081d" class="nd ne fq nf b go px nh ni gr py nk nl nm pz no np nq qa ns nt nu qb nw nx ny pu pv pw bk"><a class="af nc" href="#c593" rel="noopener ugc nofollow">Forward Pass</a></li><li id="e334" class="nd ne fq nf b go px nh ni gr py nk nl nm pz no np nq qa ns nt nu qb nw nx ny pu pv pw bk"><a class="af nc" href="#c3cc" rel="noopener ugc nofollow">Sampling</a></li><li id="d428" class="nd ne fq nf b go px nh ni gr py nk nl nm pz no np nq qa ns nt nu qb nw nx ny pu pv pw bk"><a class="af nc" href="#2077" rel="noopener ugc nofollow">Initialization</a></li><li id="b041" class="nd ne fq nf b go px nh ni gr py nk nl nm pz no np nq qa ns nt nu qb nw nx ny pu pv pw bk"><a class="af nc" href="#56cf" rel="noopener ugc nofollow">Training Loop</a></li><li id="f6e2" class="nd ne fq nf b go px nh ni gr py nk nl nm pz no np nq qa ns nt nu qb nw nx ny pu pv pw bk"><a class="af nc" href="#0e89" rel="noopener ugc nofollow">References</a></li></ul><h1 id="c3a0" class="ot ou fq bf ov ow ox gq oy oz pa gt pb pc pd pe pf pg ph pi pj pk pl pm pn po bk">Preparing the data</h1><p id="916c" class="pw-post-body-paragraph nd ne fq nf b go pp nh ni gr pq nk nl nm pr no np nq ps ns nt nu pt nw nx ny fj bk">Install mlx and run the following imports.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="2da4" class="qg ou fq qd b bg qh qi l qj qk">import mlx.core as mx<br/>import mlx.nn as nn<br/>import mlx.optimizers as optim<br/>import mlx.utils as utils<br/>import numpy as np<br/>import math</span></pre><p id="dbbf" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The first step to training an LLM is collecting a large corpus of text data and then tokenizing it. Tokenization is the process of mapping text to integers, which can be fed into the LLM. Our training corpus for this model will be the works of Shakespeare concatenated into one file. This is roughly 1 million characters and looks like this:</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="e4e6" class="qg ou fq qd b bg qh qi l qj qk">First Citizen:<br/>Before we proceed any further, hear me speak.<br/><br/>All:<br/>Speak, speak.<br/><br/>First Citizen:<br/>You are all resolved rather to die than to famish?<br/><br/>All:<br/>Resolved. resolved.<br/><br/>First Citizen:<br/>First, you know Caius Marcius is chief enemy to the people.<br/>...</span></pre><p id="e64d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">First, we read the file as a single long string into the <code class="cx ql qm qn qd b">text</code> variable. Then we use the <code class="cx ql qm qn qd b">set()</code> function to get all the unique characters in the text which will be our vocabulary. By printing <code class="cx ql qm qn qd b">vocab</code> you can see all the characters in our vocabulary as one string, and we have a total of 65 characters which till be our tokens.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="2651" class="qg ou fq qd b bg qh qi l qj qk"># Creating the vocabulary<br/>with open('input.txt', 'r', encoding='utf-8') as f:<br/>    text = f.read()<br/>vocab = sorted(list(set(text)))<br/>vocab_size = len(vocab)<br/><br/>print(''.join(vocab))<br/># !$&amp;',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz<br/>print(vocab_size)<br/># 65</span></pre><p id="5e65" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Production models will use tokenization algorithms like byte-pair encoding to generate a larger vocabulary of sub-word chunks. Since our focus today is on the architecture, we will continue with character-level tokenization. Next, we will map our vocabulary to integers known as token IDs. Then we can encode our text into tokens and decode them back to a string.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="167b" class="qg ou fq qd b bg qh qi l qj qk"># Create mapping from vocab to integers<br/>itos = {i:c for i,c in enumerate(vocab)} # int to string<br/>stoi = {c:i for i,c in enumerate(vocab)} # string to int<br/>encode = lambda x: [stoi[c] for c in x]<br/>decode = lambda x: ''.join([itos[i] for i in x])<br/><br/>print(encode("hello world"))<br/># [46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]<br/>print(decode(encode("hello world")))<br/># hello world</span></pre><p id="60f5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We use the<code class="cx ql qm qn qd b">enumerate()</code> function to iterate over all characters and their index in the vocabulary and create a dictionary <code class="cx ql qm qn qd b">itos</code> which maps integers to characters and <code class="cx ql qm qn qd b">stoi</code> which maps strings to integers. Then we use these mappings to create our encode and decode functions. Now we can encode the entire text and split training and validation data.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="17cd" class="qg ou fq qd b bg qh qi l qj qk">data = encode(text)<br/>split = int(0.9 * len(data))<br/>train_data = data[:split]<br/>val_data = data[split:]</span></pre><p id="c5dd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Currently, our training data is just a very long string of tokens. However, we are trying to train our model to predict the next token some given previous tokens. Therefore our dataset should be comprised of examples where the input is some string of tokens and the label is the correct next token. We need to define a model parameter called <em class="qo">context length </em>which is the maximum number of tokens used to predict the next token. Our training examples will be the length of our context length.</p><p id="bb74" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Let’s look at the first <code class="cx ql qm qn qd b">ctx_len+1</code> tokens.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="1266" class="qg ou fq qd b bg qh qi l qj qk">ctx_len = 8<br/>print(train_data[:ctx_len + 1])<br/># [18, 47, 56, 57, 58,  1, 15, 47, 58]<br/># x: [18, 47, 56, 57, 58,  1, 15, 47] | y: 58</span></pre><p id="bd00" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This is one training example where the input is “18, 47, 56, 57, 58, 1, 15, 47” and the desired output is “58”. This is 8 tokens of context. However, we also want to train the model to predict the next token given only 7, 6, 5 … 0 tokens as context which is needed during generation. Therefore we also consider the 8 sub examples packed into this example:</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="fee7" class="qg ou fq qd b bg qh qi l qj qk">ctx_len = 8<br/>print(train_data[:ctx_len + 1])<br/># [18, 47, 56, 57, 58,  1, 15, 47, 58]<br/># 8 sub examples<br/># [18] --&gt; 47<br/># [18, 47] --&gt; 56<br/># [18, 47, 56] --&gt; 57<br/># [18, 47, 56, 57] --&gt; 58<br/># [18, 47, 56, 57, 58] --&gt; 1<br/># [18, 47, 56, 57, 58, 1] --&gt; 15<br/># [18, 47, 56, 57, 58, 1, 15] --&gt; 47<br/># [18, 47, 56, 57, 58, 1, 15, 47] --&gt; 58</span></pre><p id="e789" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Notice that the labels are simply the inputs shifted left.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="e95d" class="qg ou fq qd b bg qh qi l qj qk">print("inputs: ", train_data[:ctx_len])<br/>print("labels: ", train_data[1:ctx_len+1]) # labels = inputs indexed 1 higher<br/># inputs: [18, 47, 56, 57, 58,  1, 15, 47]<br/># labels: [47, 56, 57, 58,  1, 15, 47, 58]</span></pre><p id="20c7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">At index 0 the input is 18 and the label is 47. At index 1 the input is everything before and including index 1 which is [18, 47] and the label is 56, etc. Now that we understand that the labels are simply the input sequence indexed one higher we can build our datasets.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="7373" class="qg ou fq qd b bg qh qi l qj qk"># Creating training and validation datasets<br/>ctx_len = 8<br/>X_train = mx.array([train_data[i:i+ctx_len] for i in range(0, len(train_data) - ctx_len, ctx_len)])<br/>y_train = mx.array([train_data[i+1:i+ctx_len+1] for i in range(0, len(train_data) - ctx_len, ctx_len)])<br/>X_val = mx.array([val_data[i:i+ctx_len] for i in range(0, len(val_data) - ctx_len, ctx_len)])<br/>y_val = mx.array([val_data[i+1:i+ctx_len+1] for i in range(0, len(val_data) - ctx_len, ctx_len)])</span></pre><p id="e4dc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We loop through the data and take chunks of size <code class="cx ql qm qn qd b">ctx_len</code> as the inputs (X) and then take the same chunks but at 1 higher index as the labels (y). Then we take these Python lists and create mlx array objects from them. The model internals will be written with mlx so we want our inputs to be mlx arrays.</p><p id="6f27" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">One more thing. During training we don’t want to feed the model one example at a time, we want to feed it multiple examples in parallel for efficiency. This group of examples is called our batch, and the number of examples in a group is our batch size. Thus we define a function to generate batches for training.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="64a2" class="qg ou fq qd b bg qh qi l qj qk">def get_batches(X, y, b_size, shuffle=True):<br/>    if shuffle:<br/>        ix = np.arange(X.shape[0])<br/>        np.random.shuffle(ix)<br/>        ix = mx.array(ix)<br/>        X = X[ix]<br/>        y = y[ix]<br/>    for i in range(0, X.shape[0], b_size):<br/>        input = X[i:i+b_size]<br/>        label = y[i:i+b_size]<br/>        yield input, label</span></pre><p id="0707" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">If shuffle=True, we shuffle the data by indexing it with a randomly shuffled index. Then we loop through our dataset and return batch-size chunks from input and label datasets. These chunks are known as mini-batches and are just stacked examples that we process in parallel. These mini-batches will be our input to the model during training.</p><p id="3344" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Here’s an example of a minibatch of 4 examples with context length 8.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qp"><img src="../Images/92f8cb95ed41ef0d51156a6ba2aad2f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*G4uCnQxXLcYN83xOQL3rNQ.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">A single minibatch (image by author)</figcaption></figure><p id="cd06" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This minibatch packs 32 next-token prediction problems. The model will predict the next token for each token in the input and the labels will be used to calculate the loss. Notice that the labels contain the next token for each index of the inputs.</p><p id="3d17" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">You’ll want to keep this picture in your mind because the shapes of these tensors will get hairy. For now, just remember that we will input a tensor of shape (batch_size, ctx_len) to the model.</p><h1 id="1d50" class="ot ou fq bf ov ow ox gq oy oz pa gt pb pc pd pe pf pg ph pi pj pk pl pm pn po bk">Coding GPT-2</h1><p id="58d4" class="pw-post-body-paragraph nd ne fq nf b go pp nh ni gr pq nk nl nm pr no np nq ps ns nt nu pt nw nx ny fj bk">Let’s look at the GPT-2 architecture to get an overview of what we are trying to implement.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qq"><img src="../Images/eefb8f6fd70479937d7b028ab70083a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*LjEGz6ZDVbqo85J15zFJlA.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">GPT-2 Architecture (image by author)</figcaption></figure><p id="549c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Don’t worry if this looks confusing. We will implement it step by step from bottom to top. Let’s start by implementing the input embeddings.</p><h2 id="9d27" class="qr ou fq bf ov qs qt qu oy qv qw qx pb nm qy qz ra nq rb rc rd nu re rf rg rh bk">Input Embeddings</h2><p id="71a6" class="pw-post-body-paragraph nd ne fq nf b go pp nh ni gr pq nk nl nm pr no np nq ps ns nt nu pt nw nx ny fj bk">The purpose of the input embedding layer is to map token IDs to vectors. Each token will be mapped to a vector which will be its representation as it is forwarded through the model. The vectors for each token will accumulate and exchange information as they pass through the model and eventually be used to predict the next token. These vectors are called embeddings.</p><p id="9a9f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The simplest way to map token IDs to vectors is through a lookup table. We create a matrix of size (vocab_size, n_emb) where each row is the embedding vector for the corresponding token. This matrix is known as the embedding weights.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ri"><img src="../Images/bc9c1b7194ed45e12e28d4c3e95db59c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qGxuMi7rwSMqANTb0n6tlA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Embedding Layer (image by author)</figcaption></figure><p id="ed27" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The diagram shows an example embedding layer of size (65, 6). This means there are 65 tokens in the vocabulary and each one will be represented by a length 6 embedding vector. The inputted sequence will be used to index the embedding weights to get the vector corresponding to each token. Remember the minibatches we input into the model? Originally the minibatch is size (batch_size, ctx_len). After passing through the embedding layer it is size (batch_size, ctx_len, n_emb). Instead of each token being a single integer, each token is now a vector of length n_emb.</p><p id="b1ce" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Let’s define the embedding layer in code now.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="a197" class="qg ou fq qd b bg qh qi l qj qk">n_emb = 6 # You can add these hyperparams at the top of your file<br/>class GPT(nn.Module):<br/>    def __init__(self):<br/>        super().__init__()<br/>        self.wte = nn.Embedding(vocab_size, n_emb)</span></pre><p id="3b29" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We will define a class to organize our implementation. We subclass nn.Module to take advantage of mlx’s features. Then in the init function, we call the superclass constructor and initialize our token embedding layer called <code class="cx ql qm qn qd b">wte</code> .</p><h2 id="5507" class="qr ou fq bf ov qs qt qu oy qv qw qx pb nm qy qz ra nq rb rc rd nu re rf rg rh bk">Positional Embeddings</h2><p id="2038" class="pw-post-body-paragraph nd ne fq nf b go pp nh ni gr pq nk nl nm pr no np nq ps ns nt nu pt nw nx ny fj bk">Next up is the positional embeddings. The purpose of positional embeddings is to encode information about the position of each token in the sequence. This can be added to our input embeddings to get a complete representation of each token that contains information about the token’s position in the sequence.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="4b54" class="qg ou fq qd b bg qh qi l qj qk">class GPT(nn.Module):<br/>    def __init__(self):<br/>        super().__init__()<br/>        self.wte = nn.Embedding(vocab_size, n_emb) # token embeddings<br/>        self.wpe = nn.Embedding(ctx_len, n_emb) # position embeddings</span></pre><p id="d37b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The position embeddings work the same as token embeddings, except instead of having a row for each token we have a row for each possible position index. This means our embedding weights will be of shape (ctx_len, n_emb). Now we implement the __call__ function in our GPT class. This function will contain the forward pass of the model.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="ae00" class="qg ou fq qd b bg qh qi l qj qk"># Tensor shapes commented<br/>def __call__(self, x):<br/>    B, T = x.shape # (B = batch_size, T = ctx_len)<br/>    tok_emb = self.wte(x) # (B, T, n_emb)<br/>    pos_emb = self.wpe(mx.arange(T)) # (T, n_emb)<br/>    x = tok_emb + pos_emb # (B, T, n_emb)</span></pre><p id="ce97" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">First, we break out the dimensions of our input into variables B and T for easy handling. In sequence modeling contexts B and T are usually used as shorthand for “batch” and “time” dimensions. In this case, the “time” dimension of our sequence is the context length.</p><p id="7148" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Next, we calculate token and position embeddings. Notice that for the position embeddings, our input is <code class="cx ql qm qn qd b">mx.arange(T)</code> . This will output an array of consecutive integers from 0 to T-1 which is exactly what we want because those are the positions we want to embed. After passing that through the embedding layer we will have a tensor of shape (T, n_emb) because the embedding layer plucks out the n_emb length vector for each of the T positions. Note that even though pos_emb is not the same shape as tok_emb we can add the two because mlx will broadcast, or replicate pos_emb across the batch dimension to allow elementwise addition. Finally, we perform the addition to get the new representations of the tokens with positional information.</p><h2 id="b6bc" class="qr ou fq bf ov qs qt qu oy qv qw qx pb nm qy qz ra nq rb rc rd nu re rf rg rh bk">Self-Attention</h2><p id="74c5" class="pw-post-body-paragraph nd ne fq nf b go pp nh ni gr pq nk nl nm pr no np nq ps ns nt nu pt nw nx ny fj bk">So far the representation vectors for each token have been calculated independently. They have not had the opportunity to exchange any information. This is intuitively bad in language modeling because the meaning and usage of words depend on the surrounding context. Self-attention is how we incorporate information from previous tokens into a given token.</p><p id="2adb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">First, let’s consider a naive approach. What if we simply represented each token as the average of its representation vector and the vectors of all the tokens before it? This achieves our goal of packing information from previous tokens into the representation for a given token. Here’s what it would look like.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk rj"><img src="../Images/d1af7ef1560b69aaa50ff00af108fba9.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*i7y9J0A53-kd3sfSPMbLtA.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">image by author</figcaption></figure><p id="7f1f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">But self-attention doesn’t involve writing a for-loop. The key insight is we can achieve this previous token averaging with matrix multiplication!</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk rk"><img src="../Images/4033f38082734c80ea099705718c3e00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*McA81dGqC01L55Xy7A1rEA.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">image by author</figcaption></figure><p id="9ff7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">By multiplying our input sequence on the left by a special matrix we get the desired result. This matrix is known as the attention weights. Notice that each row of the attention weight matrix specificies “how much” of each other token goes into the representation for any given token. For example in row two, we have [0.5, 0.5, 0, 0]. This means that row two of the result will be <code class="cx ql qm qn qd b">0.5*token1 + 0.5*token2 + 0*token3 + 0*token4</code> , or the average of token1 and token2. Note that the attention weights are a lower-triangular matrix (zeros in upper right entries). This ensures that future tokens will not be included in the representation of a given token. This ensures that tokens can only communicate with the previous tokens because during generation the model will only have access to previous tokens.</p><p id="0bf0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Let’s look at how we can construct the attention weight matrix.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rl"><img src="../Images/03c45bdcb15092d5ba7d2ca01b63e5c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*q0aWyolWxyG3ppmHnc8R0Q.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">image by author</figcaption></figure><p id="6d95" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Notice that if we create an array of zeros with -inf in the upper right entries and then perform row-wise softmax we get the desired attention weights. A good exercise is to step through the softmax calculation for a row to see how this works. The takeaway is that we can take some array of size (ctx_len, ctx_len) and softmax each row to get attention weights that sum to one.</p><p id="6c70" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now we can leave the realm of naive self-attention. Instead of simply averaging previous tokens, we use arbitrary weighted sums over previous tokens. Notice what happens when we do row-wise softmax of an arbitrary matrix.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rm"><img src="../Images/11897ada690ad865eea3d6617a738316.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*awaNfJ_bK8ZmhF9KpcwrFQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">image by author</figcaption></figure><p id="6fac" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We still get weights that sum to one on each row. During training, we can learn the numbers in the matrix on the left which will specify how much each token goes into the representation for another token. This is how tokens pay “attention” to each other. But we still haven’t understood where this matrix on the left came from. These pre-softmax attention weights are calculated from the tokens themselves, but indirectly through three linear projections.</p><h2 id="718c" class="qr ou fq bf ov qs qt qu oy qv qw qx pb nm qy qz ra nq rb rc rd nu re rf rg rh bk"><strong class="al">Keys, Queries, and Values</strong></h2><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rn"><img src="../Images/ea6acf7b1fde813fde398d59348b6dbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R9ygYCgcSFsS5mWuaoNv4A.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">image by author</figcaption></figure><p id="26ff" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Each token in our sequence emits 3 new vectors. These vectors are called keys, queries, and values. We use the dot product of the query vector of one token and the key vector of another token to quantify the “affinity” those two tokens have. We want to calculate the pairwise affinities of each token with every other token, therefore we multiply the query vector (4x3) with the key vector transposed (3x4) to get the raw attention weights (4x4). Due to the way matrix multiplication works the (i,j) entry in the raw attention weights will be the query of token i dot the key of token j or the “affinity” between the two. Thus we have calculated interactions between every token. However, we don’t want past tokens interacting with future tokens so we apply a mask of -inf to the upper right entries to ensure they will zero out after softmax. Then we perform row-wise softmax to get the final attention weights. Instead of multiplying these weights directly with the input, we multiply them with the value projection. This results in the new representations.</p><p id="99bf" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now that we understand attention conceptually, let’s implement it.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="4445" class="qg ou fq qd b bg qh qi l qj qk">class Attention(nn.Module):<br/>    def __init__(self, head_size):<br/>        super().__init__()<br/>        self.head_size = head_size<br/>        self.k_proj = nn.Linear(n_emb, head_size, bias=False)<br/>        self.q_proj = nn.Linear(n_emb, head_size, bias=False)<br/>        self.v_proj = nn.Linear(n_emb, head_size, bias=False)<br/>  </span></pre><p id="abc8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We start by defining the key, query, and value projection layers. Note that instead of going from n_emb to n_emb, we project from n_emb to head_size. This doesn’t change anything, it just means the new representations calculated by attention will be dimension head_size.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="5325" class="qg ou fq qd b bg qh qi l qj qk">class Attention(nn.Module):<br/>    def __init__(self, head_size):<br/>        super().__init__()<br/>        self.head_size = head_size<br/>        self.k_proj = nn.Linear(n_emb, head_size, bias=False)<br/>        self.q_proj = nn.Linear(n_emb, head_size, bias=False)<br/>        self.v_proj = nn.Linear(n_emb, head_size, bias=False)<br/>    def __call__(self, x): # shapes commented<br/>        B, T, C = x.shape # (batch_size, ctx_len, n_emb)<br/>        K = self.k_proj(x) # (B, T, head_size)<br/>        Q = self.q_proj(x) # (B, T, head_size)<br/>        V = self.v_proj(x) # (B, T, head_size)</span></pre><p id="8c79" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The forward pass begins by calculating the key, query, and value projections. We also break out the input shape into the variables B, T, and C for future convenience.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="40d0" class="qg ou fq qd b bg qh qi l qj qk">class Attention(nn.Module):<br/>    def __init__(self, head_size):<br/>        super().__init__()<br/>        self.head_size = head_size<br/>        self.k_proj = nn.Linear(n_emb, head_size, bias=False)<br/>        self.q_proj = nn.Linear(n_emb, head_size, bias=False)<br/>        self.v_proj = nn.Linear(n_emb, head_size, bias=False)<br/>    def __call__(self, x):<br/>        B, T, C = x.shape # (batch_size, ctx_len, n_emb)<br/>        K = self.k_proj(x) # (B, T, head_size)<br/>        Q = self.q_proj(x) # (B, T, head_size)<br/>        V = self.v_proj(x) # (B, T, head_size)<br/>        attn_weights = (Q @ K.transpose([0, 2, 1])) / math.sqrt(self.head_size)<br/>        # attn_weights.shape = (B, T, T)</span></pre><p id="8718" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Next, we calculate the attention weights. We only want to transpose the last two dimensions of the key tensor, because the batch dimension is just there so we can forward multiple training examples in parallel. The mlx transpose function expects the new order of the dimensions as input, so we pass it [0, 2, 1] to transpose the last two dimensions. One more thing: we scale the attention weights by the inverse square root of head_size. This is known as scaled attention and the purpose is to ensure that when Q and K are unit variance, attn_weights will be unit variance. If the variance of attn_weights is high, then the softmax will map these small and large values to 0 or 1which results in less complex representations.</p><p id="4cc4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The next step is to apply the mask to ensure we are doing causal language modeling i.e. ensuring tokens cannot attend to future tokens.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="c953" class="qg ou fq qd b bg qh qi l qj qk">class Attention(nn.Module):<br/>    def __init__(self, head_size):<br/>        super().__init__()<br/>        self.head_size = head_size<br/>        self.k_proj = nn.Linear(n_emb, head_size, bias=False)<br/>        self.q_proj = nn.Linear(n_emb, head_size, bias=False)<br/>        self.v_proj = nn.Linear(n_emb, head_size, bias=False)<br/>        indices = mx.arange(ctx_len)<br/>        mask = indices[:, None] &lt; indices[None] # broadcasting trick<br/>        self._causal_mask = mask * -1e9<br/>    def __call__(self, x):<br/>        B, T, C = x.shape # (batch_size, ctx_len, n_emb)<br/>        K = self.k_proj(x) # (B, T, head_size)<br/>        Q = self.q_proj(x) # (B, T, head_size)<br/>        V = self.v_proj(x) # (B, T, head_size)<br/>        attn_weights = (Q @ K.transpose([0, 2, 1])) / math.sqrt(self.head_size)<br/>        # attn_weights.shape = (B, T, T)</span></pre><p id="0e2b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We create the mask with a clever broadcasting trick. Let’s say our ctx_len=4 like in the diagrams above. First, we use mx.arange(4) to set the indices variable to [0, 1, 2, 3].</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ro"><img src="../Images/776a8fa277964a2ae02319291108f577.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ud_cMluxKj2Qov-RUhCpnA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">image by author</figcaption></figure><p id="2cfb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Then we can index like so <code class="cx ql qm qn qd b">indices[:, None]</code> to generate a column vector with the values of indices. Similarly, we can get a row vector using <code class="cx ql qm qn qd b">indices[None]</code>. Then when we do the &lt; comparison, mlx broadcasts the vectors because they have mismatching shapes so they can’t be compared elementwise. Broadcasting means mlx will replicate the vectors along the lacking dimension. This results in an elementwise comparison of two (4, 4) matrices which makes sense. Side note: I recommend familiarizing yourself with the details of broadcasting by reading <a class="af nc" href="https://pytorch.org/docs/stable/notes/broadcasting.html" rel="noopener ugc nofollow" target="_blank">this</a>, it comes up all the time when dealing with tensors.</p><p id="564d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">After the elementwise comparison, we are left with the following tensor:</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="4b2a" class="qg ou fq qd b bg qh qi l qj qk">[[False,  True,  True,  True],<br/> [False, False,  True,  True],<br/> [False, False, False,  True],<br/> [False, False, False, False]]</span></pre><p id="1d3a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Multiplying this tensor by -1e9, we get:</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="a183" class="qg ou fq qd b bg qh qi l qj qk">[[-0e+00, -1e+09, -1e+09, -1e+09],<br/> [-0e+00, -0e+00, -1e+09, -1e+09],<br/> [-0e+00, -0e+00, -0e+00, -1e+09],<br/> [-0e+00, -0e+00, -0e+00, -0e+00]]</span></pre><p id="ba9e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now we have an additive mask. We can add this matrix to our attention weights to make all the upper right entries very large negative numbers. This will cause them to be zeroed out after the softmax operation. Also, note that we add “_” as a prefix to the attribute name <code class="cx ql qm qn qd b">_causal_mask</code> which marks it as a private variable. This signals to mlx that it is not a parameter and should not be updated during training.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="2a7b" class="qg ou fq qd b bg qh qi l qj qk">class Attention(nn.Module):<br/>    def __init__(self, head_size):<br/>        super().__init__()<br/>        self.head_size = head_size<br/>        self.k_proj = nn.Linear(n_emb, head_size, bias=False)<br/>        self.q_proj = nn.Linear(n_emb, head_size, bias=False)<br/>        self.v_proj = nn.Linear(n_emb, head_size, bias=False)<br/>        indices = mx.arange(ctx_len)<br/>        mask = indices[:, None] &lt; indices[None] # broadcasting trick<br/>        self._causal_mask = mask * -1e9<br/>    def __call__(self, x):<br/>        B, T, C = x.shape # (batch_size, ctx_len, n_emb)<br/>        K = self.k_proj(x) # (B, T, head_size)<br/>        Q = self.q_proj(x) # (B, T, head_size)<br/>        V = self.v_proj(x) # (B, T, head_size)<br/>        attn_weights = (Q @ K.transpose([0, 2, 1])) / math.sqrt(self.head_size)<br/>        # attn_weights.shape = (B, T, T)<br/>        attn_weights = attn_weights + self._causal_mask<br/>        attn_weights = mx.softmax(attn_weights, axis=-1)<br/>        o = (attn_weights @ V) # (B, T, head_size)</span></pre><p id="6163" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now we can softmax row-wise to get the final attention weights and multiply these weights by the values to get our output. Note we pass <code class="cx ql qm qn qd b">axis=-1</code> to softmax which specifies that we want to softmax across the last dimension which are the rows.</p><p id="0dc9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The final step is output linear projection and dropout.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="baa1" class="qg ou fq qd b bg qh qi l qj qk">dropout = 0.1 # add this with hyperparams at top of file<br/>class Attention(nn.Module):<br/>    def __init__(self, head_size):<br/>        super().__init__()<br/>        self.head_size = head_size<br/>        self.k_proj = nn.Linear(n_emb, head_size, bias=False)<br/>        self.q_proj = nn.Linear(n_emb, head_size, bias=False)<br/>        self.v_proj = nn.Linear(n_emb, head_size, bias=False)<br/>        indices = mx.arange(ctx_len)<br/>        mask = indices[:, None] &lt; indices[None] # broadcasting trick<br/>        self._causal_mask = mask * -1e9<br/>        self.c_proj = nn.Linear(head_size, n_emb) # output projection<br/>        self.resid_dropout = nn.Dropout(dropout)<br/>    def __call__(self, x):<br/>        B, T, C = x.shape # (batch_size, ctx_len, n_emb)<br/>        K = self.k_proj(x) # (B, T, head_size)<br/>        Q = self.q_proj(x) # (B, T, head_size)<br/>        V = self.v_proj(x) # (B, T, head_size)<br/>        attn_weights = (Q @ K.transpose([0, 2, 1])) / math.sqrt(self.head_size)<br/>        # attn_weights.shape = (B, T, T)<br/>        attn_weights = attn_weights + self._causal_mask<br/>        attn_weights = mx.softmax(attn_weights, axis=-1)<br/>        o = (attn_weights @ V) # (B, T, head_size)<br/>        o = self.c_proj(self.resid_dropout(o))<br/>        return o</span></pre><p id="d82f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We add two new layers, <code class="cx ql qm qn qd b">c_proj</code> and <code class="cx ql qm qn qd b">resid_dropout</code> which are the output projection and residual dropout. The output projection is to return the vectors to their original dimension n_emb. The dropout is added for regularization and training stability which is important as we start layering the transformer blocks to get a deep network. And that’s it for implementing one attention head!</p><h2 id="38de" class="qr ou fq bf ov qs qt qu oy qv qw qx pb nm qy qz ra nq rb rc rd nu re rf rg rh bk">Multi-Head Attention</h2><p id="b185" class="pw-post-body-paragraph nd ne fq nf b go pp nh ni gr pq nk nl nm pr no np nq ps ns nt nu pt nw nx ny fj bk">Instead of having just one attention head LLMs often use multiple attention heads in parallel and concatenate their outputs to create the final representation. For example, let’s say we had one attention head with head_size=64 so the vector it produced for each token was 64 dimensional. We could achieve the same thing with 4 parallel attention heads each with head_size=16 by concatenating their outputs to produce a 16x4 = 64 dimensional output. Multi-head attention allows the model to learn more complex representations because each head learns different projections and attention weights.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="4001" class="qg ou fq qd b bg qh qi l qj qk">n_heads = 4<br/>class MultiHeadAttention(nn.Module): # naive implementation<br/>    def __init__(self):<br/>        super().__init__()<br/>        self.heads = [Attention(head_size // n_heads) for _ in range(n_heads)]<br/>    def __call__(self, x):<br/>        return mx.concatenate([head(x) for head in self.heads], axis=-1)</span></pre><p id="870c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The straightforward implementation is to create a list of <code class="cx ql qm qn qd b">n_heads</code> attention heads where each one has size equal to our final head size divided by n_heads. Then we concatenate the output of each head over the last axis. However, this implementation is inefficient and does not take advantage of the speed of tensors. Let’s implement multi-head attention with the power of tensors.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="3dda" class="qg ou fq qd b bg qh qi l qj qk">head_size = 64 # put at top of file<br/>class MultiHeadAttention(nn.Module):<br/>    def __init__(self):<br/>        super().__init__()<br/>        self.k_proj = nn.Linear(n_emb, head_size, bias=False)<br/>        self.q_proj = nn.Linear(n_emb, head_size, bias=False)<br/>        self.v_proj = nn.Linear(n_emb, head_size, bias=False)<br/>        indices = mx.arange(ctx_len)<br/>        mask = indices[:, None] &lt; indices[None] # broadcasting trick<br/>        self._causal_mask = mask * -1e9<br/>        self.c_proj = nn.Linear(head_size, n_emb) # output projection<br/>        self.resid_dropout = nn.Dropout(dropout)<br/>    def __call__(self, x):<br/>        B, T, C = x.shape # (batch_size, ctx_len, n_emb)<br/>        K = self.k_proj(x) # (B, T, head_size)<br/>        Q = self.q_proj(x) # (B, T, head_size)<br/>        V = self.v_proj(x) # (B, T, head_size)</span></pre><p id="7741" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We start with our single-head attention implementation. The <code class="cx ql qm qn qd b">__init__()</code> function has not changed. The forward pass begins as normal with the creation of the key, query, and value projections.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="7f9a" class="qg ou fq qd b bg qh qi l qj qk">head_size = 64 # put at top of file<br/>n_heads = 8 # put at top of file<br/>class MultiHeadAttention(nn.Module):<br/>    def __init__(self):<br/>        super().__init__()<br/>        self.k_proj = nn.Linear(n_emb, head_size, bias=False)<br/>        self.q_proj = nn.Linear(n_emb, head_size, bias=False)<br/>        self.v_proj = nn.Linear(n_emb, head_size, bias=False)<br/>        indices = mx.arange(ctx_len)<br/>        mask = indices[:, None] &lt; indices[None] # broadcasting trick<br/>        self._causal_mask = mask * -1e9<br/>        self.c_proj = nn.Linear(head_size, n_emb) # output projection<br/>        self.resid_dropout = nn.Dropout(dropout)<br/>    def __call__(self, x):<br/>        B, T, C = x.shape # (batch_size, ctx_len, n_emb)<br/>        K = self.k_proj(x) # (B, T, head_size)<br/>        Q = self.q_proj(x) # (B, T, head_size)<br/>        V = self.v_proj(x) # (B, T, head_size)<br/>        mha_shape = (B, T, n_heads, head_size//n_heads)<br/>        K = mx.as_strided(K, (mha_shape)) # (B, T, n_heads, head_size//n_heads)<br/>        Q = mx.as_strided(Q, (mha_shape)) # (B, T, n_heads, head_size//n_heads)<br/>        V = mx.as_strided(V, (mha_shape)) # (B, T, n_heads, head_size//n_heads)</span></pre><p id="1675" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The next thing we need to do is introduce a new dimension for the number of heads <code class="cx ql qm qn qd b">n_heads</code> . In the naive implementation, we had separate attention objects each with their own key, query, and value tensors but now we have them all in one tensor, therefore we need a dimension for the heads. We define the new shape we want in <code class="cx ql qm qn qd b">mha_shape</code> . Then we use <code class="cx ql qm qn qd b">mx.as_strided()</code> to reshape each tensor to have the head dimension. This function is equivalent to <code class="cx ql qm qn qd b">view</code> from pytorch and tells mlx to treat this array as a different shape. But we still have a problem. Notice that we if try to multiply <code class="cx ql qm qn qd b">Q @ K_t</code> (where K_t is K transposed over it’s last 2 dims) to compute attention weights as we did before, we will be multiplying the following shapes:</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="e875" class="qg ou fq qd b bg qh qi l qj qk">(B, T, n_heads, head_size//n_heads) @ (B, T, head_size//n_heads, n_heads)<br/>Result shape: (B, T, n_heads, n_heads)</span></pre><p id="aa0d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This would result in a tensor of shape <code class="cx ql qm qn qd b">(B, T, n_heads, n_heads)</code> which is incorrect. With one head our attention weights were shape <code class="cx ql qm qn qd b">(B, T, T)</code> which makes sense because it gives us the interaction between each pair of tokens. So now our shape should be the same but with a heads dimension: <code class="cx ql qm qn qd b">(B, n_heads, T, T)</code> . We achieve this by transposing the dimensions of keys, queries, and values after we reshape them to make <code class="cx ql qm qn qd b">n_heads</code> dimension 1 instead of 2.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="e517" class="qg ou fq qd b bg qh qi l qj qk">head_size = 64 # put at top of file<br/>n_heads = 8 # put at top of file<br/>class MultiHeadAttention(nn.Module):<br/>    def __init__(self):<br/>        super().__init__()<br/>        self.k_proj = nn.Linear(n_emb, head_size, bias=False)<br/>        self.q_proj = nn.Linear(n_emb, head_size, bias=False)<br/>        self.v_proj = nn.Linear(n_emb, head_size, bias=False)<br/>        indices = mx.arange(ctx_len)<br/>        mask = indices[:, None] &lt; indices[None] # broadcasting trick<br/>        self._causal_mask = mask * -1e9<br/>        self.c_proj = nn.Linear(head_size, n_emb) # output projection<br/>        self.attn_dropout = nn.Dropout(dropout)<br/>        self.resid_dropout = nn.Dropout(dropout)<br/>    def __call__(self, x):<br/>        B, T, C = x.shape # (batch_size, ctx_len, n_emb)<br/>        K = self.k_proj(x) # (B, T, head_size)<br/>        Q = self.q_proj(x) # (B, T, head_size)<br/>        V = self.v_proj(x) # (B, T, head_size)<br/>        mha_shape = (B, T, n_heads, head_size//n_heads)<br/>        K = mx.as_strided(K, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size//n_heads)<br/>        Q = mx.as_strided(Q, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size//n_heads)<br/>        V = mx.as_strided(V, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size//n_heads)<br/>        attn_weights = (Q @ K.transpose([0, 1, 3, 2])) / math.sqrt(Q.shape[-1]) # (B, n_heads, T, T)<br/>        attn_weights = attn_weights + self._causal_mask[:T, :T]<br/>        attn_weights = mx.softmax(attn_weights, axis=-1)<br/>        attn_weights = self.attn_dropout(attn_weights)<br/>        o = (attn_weights @ V) # (B, n_heads, T, head_size//n_heads)<br/>        </span></pre><p id="2aa2" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now we can calculate the correction attention weights. Notice that we scale the attention weights by the size of an individual attention head rather than head_size which would be the size after concatenation. We also apply dropout to the attention weights.</p><p id="0947" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Finally, we perform the concatenation and apply the output projection and dropout.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="b820" class="qg ou fq qd b bg qh qi l qj qk">head_size = 64 # put at top of file<br/>n_heads = 8 # put at top of file<br/>class MultiHeadAttention(nn.Module):<br/>    def __init__(self):<br/>        super().__init__()<br/>        self.k_proj = nn.Linear(n_emb, head_size, bias=False)<br/>        self.q_proj = nn.Linear(n_emb, head_size, bias=False)<br/>        self.v_proj = nn.Linear(n_emb, head_size, bias=False)<br/>        indices = mx.arange(ctx_len)<br/>        mask = indices[:, None] &lt; indices[None] # broadcasting trick<br/>        self._causal_mask = mask * -1e9<br/>        self.c_proj = nn.Linear(head_size, n_emb) # output projection<br/>        self.attn_dropout = nn.Dropout(dropout)<br/>        self.resid_dropout = nn.Dropout(dropout)<br/>    def __call__(self, x):<br/>        B, T, C = x.shape # (batch_size, ctx_len, n_emb)<br/>        K = self.k_proj(x) # (B, T, head_size)<br/>        Q = self.q_proj(x) # (B, T, head_size)<br/>        V = self.v_proj(x) # (B, T, head_size)<br/>        mha_shape = (B, T, n_heads, head_size//n_heads)<br/>        K = mx.as_strided(K, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size//n_heads)<br/>        Q = mx.as_strided(Q, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size//n_heads)<br/>        V = mx.as_strided(V, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size//n_heads)<br/>        attn_weights = (Q @ K.transpose([0, 1, 3, 2])) / math.sqrt(Q.shape[-1]) # (B, n_heads, T, T)<br/>        attn_weights = attn_weights + self._causal_mask[:T, :T]<br/>        attn_weights = mx.softmax(attn_weights, axis=-1)<br/>        attn_weights = self.attn_dropout(attn_weights)<br/>        o = (attn_weights @ V) # (B, n_heads, T, head_size//n_heads)<br/>        o = o.transpose([0, 2, 1, 3]).reshape((B, T, head_size)) # concat heads<br/>        o = self.c_proj(self.resid_dropout(o))<br/>        return o</span></pre><p id="bdf4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Since we have everything in one tensor, we can do some shape manipulation to do the concatenation. First, we move <code class="cx ql qm qn qd b">n_heads</code> back to the second to last dimension with the transpose function. Then we reshape back to the original size to undo the splitting into heads we performed earlier. This is the same as concatenating the final vectors from each head. And that’s it for multi-head attention! We’ve gotten through the most intense part of our implementation.</p><h1 id="43c1" class="ot ou fq bf ov ow ox gq oy oz pa gt pb pc pd pe pf pg ph pi pj pk pl pm pn po bk">MLP</h1><p id="e9b0" class="pw-post-body-paragraph nd ne fq nf b go pp nh ni gr pq nk nl nm pr no np nq ps ns nt nu pt nw nx ny fj bk">The next part of the architecture is the multilayer perception or MLP. This is a fancy way of saying 2 stacked linear layers. There’s not much to be said here, it is a standard neural network.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="3439" class="qg ou fq qd b bg qh qi l qj qk">class MLP(nn.Module):<br/>    def __init__(self):<br/>        super().__init__()<br/>        self.c_fc = nn.Linear(n_emb, 4 * n_emb)<br/>        self.gelu = nn.GELU()<br/>        self.c_proj = nn.Linear(4 * n_emb, n_emb)<br/>        self.dropout = nn.Dropout(dropout)<br/>    def __call__(self, x):<br/>        x = self.gelu(self.c_fc(x))<br/>        x = self.c_proj(x)<br/>        x = self.dropout(x)<br/>        return x</span></pre><p id="e60e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We take the input and project it to a higher dimension with <code class="cx ql qm qn qd b">c_fc</code> . Then we apply gelu nonlinearity and project it back down to the embedding dimension with <code class="cx ql qm qn qd b">c_proj</code> . Finally, we apply dropout and return. The purpose of the MLP is to allow for some computation after the vectors have communicated during attention. We will stack these communication layers (attention) and computation layers (mlp) into a block.</p><h1 id="91b3" class="ot ou fq bf ov ow ox gq oy oz pa gt pb pc pd pe pf pg ph pi pj pk pl pm pn po bk">Block</h1><p id="7d32" class="pw-post-body-paragraph nd ne fq nf b go pp nh ni gr pq nk nl nm pr no np nq ps ns nt nu pt nw nx ny fj bk">A GPT block consists of attention followed by an MLP. These blocks will be repeated to make the architecture deep.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="0ff2" class="qg ou fq qd b bg qh qi l qj qk">class Block(nn.Module):<br/>    def __init__(self):<br/>        super().__init__()<br/>        self.mlp = MLP()<br/>        self.mha = MultiHeadAttention()<br/>    def __call__(self, x):<br/>        x = self.mha(x)<br/>        x = self.mlp(x)<br/>        return x</span></pre><p id="873f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now, we need to add two more features to improve training stability. Let’s take a look at the architecture diagram again.</p><h2 id="dc6e" class="qr ou fq bf ov qs qt qu oy qv qw qx pb nm qy qz ra nq rb rc rd nu re rf rg rh bk">Layernorms and Skip Connections</h2><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qq"><img src="../Images/406a59dd5cb9bb6780808b2e94787f92.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*Fmc-sPy5TPxEZBbAy_0vTg.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">image by author</figcaption></figure><p id="7f69" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We still need to implement the components highlighted in red. The arrows are skip connections. Instead of the input being transformed directly, the effect of the attention and MLP layers is additive. Their result is added to the input instead of directly replacing it. This is good for the training stability of deep networks since in the backward pass, the operands of an addition operation will receive the same gradient as their sum. Gradients can thus flow backwards freely which prevents issues like vanishing/exploding gradients that plague deep networks. Layernorm also helps with training stability by ensuring activations are normally distributed. Here is the final implementation.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="baa6" class="qg ou fq qd b bg qh qi l qj qk">class Block(nn.Module):<br/>    def __init__(self):<br/>        super().__init__()<br/>        self.mlp = MLP()<br/>        self.mha = MultiHeadAttention()<br/>        self.ln_1 = nn.LayerNorm(dims=n_emb)<br/>        self.ln_2 = nn.LayerNorm(dims=n_emb)<br/>    def __call__(self, x):<br/>        x = x + self.mha(self.ln_1(x))<br/>        x = x + self.mlp(self.ln_2(x))<br/>        return x</span></pre><p id="f81e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Layernorm is applied before multi-head attention and MLP. The skip connections are added with <code class="cx ql qm qn qd b">x = x + ...</code> making the operations additive.</p><h1 id="c593" class="ot ou fq bf ov ow ox gq oy oz pa gt pb pc pd pe pf pg ph pi pj pk pl pm pn po bk">Forward Pass</h1><p id="98d6" class="pw-post-body-paragraph nd ne fq nf b go pp nh ni gr pq nk nl nm pr no np nq ps ns nt nu pt nw nx ny fj bk">With the Block defined, we can finish the full GPT-2 forward pass.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="0ff7" class="qg ou fq qd b bg qh qi l qj qk">n_layers = 3 # put at top of file<br/>class GPT(nn.Module):<br/>    def __init__(self):<br/>        super().__init__()<br/>        self.wte = nn.Embedding(vocab_size, n_emb) # token embeddings<br/>        self.wpe = nn.Embedding(ctx_len, n_emb) # position embeddings<br/>        self.blocks = nn.Sequential(<br/>            *[Block() for _ in range(n_layers)],<br/>        ) # transformer blocks<br/>        self.ln_f = nn.LayerNorm(dims=n_emb) # final layernorm<br/>        self.lm_head = nn.Linear(n_emb, vocab_size) # output projection<br/>    # Tensor shapes commented<br/>    def __call__(self, x):<br/>        B, T = x.shape # (B = batch_size, T = ctx_len)<br/>        tok_emb = self.wte(x) # (B, T, n_emb)<br/>        pos_emb = self.wpe(mx.arange(T)) # (T, n_emb)<br/>        x = tok_emb + pos_emb # (B, T, n_emb)<br/>        x = self.blocks(x) # (B, T, n_emb)<br/>        x = self.ln_f(x) # (B, T, b_emb)<br/>        logits = self.lm_head(x) # (B, T, vocab_size)<br/>        return logits</span></pre><p id="5848" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We create a container for the blocks using <code class="cx ql qm qn qd b">nn.Sequential</code> which takes any input and passes it sequentially through the contained layers. Then we can apply all the blocks with <code class="cx ql qm qn qd b">self.blocks(x)</code> . Finally, we apply a layer norm and then the lm_head. The lm_head or language modeling head is just a linear layer that maps from the embedding dimension to the vocab size. The model will output a vector containing some value for each word in our vocabulary, or the logits. We can softmax the logits to get a probability distribution over the vocabulary which we can sample from to get the next token. We will also use the logits to calculate the loss during training. There are just two more things we need to implement before we begin training.</p><h1 id="c3cc" class="ot ou fq bf ov ow ox gq oy oz pa gt pb pc pd pe pf pg ph pi pj pk pl pm pn po bk">Sampling</h1><p id="9702" class="pw-post-body-paragraph nd ne fq nf b go pp nh ni gr pq nk nl nm pr no np nq ps ns nt nu pt nw nx ny fj bk">We need to write a generate function to sample from the model once training is complete. The idea is that we start with some sequence of our choice, then we predict the next token and append this to our sequence. Then we feed the new sequence in and predict the next token again. This continues until we decide to stop.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="4b56" class="qg ou fq qd b bg qh qi l qj qk"># method of GPT class<br/>def generate(self, max_new_tokens):<br/>  ctx = mx.zeros((1, 1), dtype=mx.int32)</span></pre><p id="edbd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We prompt the model with a single token, zero. Zero is the newline character so it is a natural place to start the generation since we just want to see how Shakespeare-like our model can get. Note that we initialize the shape to (1, 1) to simulate a single batch with a sequence length of one.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="19fc" class="qg ou fq qd b bg qh qi l qj qk"># method of GPT class<br/>def generate(self, max_new_tokens):<br/>  ctx = mx.zeros((1, 1), dtype=mx.int32)<br/>  for _ in range(max_new_tokens):<br/>    logits = self(ctx[:, -ctx_len:]) # pass in last ctx_len characters<br/>    logits = logits[:, -1, :] # get logits for the next token<br/>    next_tok = mx.random.categorical(logits, num_samples=1)<br/>    ctx = mx.concatenate((ctx, next_tok), axis=1)<br/>return ctx</span></pre><p id="a42d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Then we get the logits for the next token by passing in the last ctx_len characters to the model. However, our model output is of shape <code class="cx ql qm qn qd b">(B, T, vocab_size)</code> since it predicts the next token logits for each token in the input. We use all of that during training, but now we only want the logits for the last token because we can use this to sample a new token. Therefore we index the logits to get the last element in the first dimension which is the sequence dimension. Then we sample the next token using the <code class="cx ql qm qn qd b">mx.random.categorical()</code> function which takes the logits and the number of samples we want as input. This function will softmax the logits to turn them into a probability distribution and then randomly sample a token according to the probabilities. Finally, we concatenate the new token to the context and repeat the process <code class="cx ql qm qn qd b">max_new_tokens</code> number of times.</p><h1 id="2077" class="ot ou fq bf ov ow ox gq oy oz pa gt pb pc pd pe pf pg ph pi pj pk pl pm pn po bk">Initialization</h1><p id="1e15" class="pw-post-body-paragraph nd ne fq nf b go pp nh ni gr pq nk nl nm pr no np nq ps ns nt nu pt nw nx ny fj bk">The last thing to do is handle weight initialization which is important for training dynamics.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="2d45" class="qg ou fq qd b bg qh qi l qj qk"># method of GPT<br/>def _init_parameters(self):<br/>    normal_init = nn.init.normal(mean=0.0, std=0.02)<br/>    residual_init = nn.init.normal(mean=0.0, std=(0.02 / math.sqrt(2 * n_layers)))</span></pre><p id="1129" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">First, we define two different <code class="cx ql qm qn qd b">nn.init.normal</code> functions. The first one is for initializing all linear and embedding layers. The second one is for initializing linear layers that are specifically residual projections i.e. the last linear layer inside multi-head attention and MLP. The reason for this special initialization is that it checks accumulation along the residual path as model depth increases according to the GPT-2 paper [2].</p><p id="5b4a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In mlx we can change the parameters of the model using the <code class="cx ql qm qn qd b"><a class="af nc" href="https://ml-explore.github.io/mlx/build/html/python/nn/_autosummary/mlx.nn.Module.update.html#mlx.nn.Module.update" rel="noopener ugc nofollow" target="_blank">mx.update()</a></code> function. Checking the docs, it expects a complete or partial dictionary of the new model parameters. We can see what this dictionary looks like by printing out <code class="cx ql qm qn qd b">self.parameters()</code> inside the GPT class.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="6b80" class="qg ou fq qd b bg qh qi l qj qk">{'wte': {'weight': array([[-0.025084, -0.0197523, -0.0341617, ..., -0.0979123, -0.0830218, -0.0784692],<br/>       [-0.00777913, -0.117002, -0.0310708, ..., 0.0128591, 0.122941, 0.000414443],<br/>       [0.0240044, -0.0859084, 0.0253116, ..., 0.108967, 0.0767123, 0.0221565],<br/>       ...,<br/>       [0.050729, -0.04578, 0.0685943, ..., -0.0496998, -0.00350879, -0.00631825],<br/>       [0.00518804, 0.0499818, 0.0330045, ..., 0.0300661, 0.0431054, 0.000958906],<br/>       [-0.0323007, 0.0132046, 0.0208218, ..., -0.0785159, 0.00436121, -0.00726994]], dtype=float32)}, 'wpe': {'weight': array([[0.000797923, -0.0396898, -0.029047, ..., -0.0132273, 0.00684483, -0.0067624],<br/>       [-0.0247021, -0.0274349, 0.0310587, ..., -0.100099, 0.0301566, -0.0178732],<br/>       [0.0929172, -0.0468649, 0.0101506, ..., -0.0341086, -0.0516283, 0.0447596],<br/>       ...,<br/>       [-0.0508172, 0.0892201, -0.00183612, ..., -0.00341944, 0.023437, 0.0296461],<br/>       [0.0105829, 0.0688093, 0.146744, ..., -0.0836337, 0.0206679, 0.0184166],<br/>       [-0.00578717, -0.0606196, -0.0917056, ..., -0.0641549, -0.0490424, 0.0998114]], dtype=float32)}, 'blocks': {'layers': [{'mlp': {'c_fc': {'weight': array([[0.0169199, 0.00264431, 0.0316978, ..., -0.0596867, -0.0153549, 0.0176386],<br/>       ...</span></pre><p id="e979" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">It’s a nested dictionary containing each model weight as an mx.array. So to initialize the parameters of our model we need to build up a dictionary like this with our new params and pass them to <code class="cx ql qm qn qd b">self.update()</code> . We can achieve this as follows:</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="3c1a" class="qg ou fq qd b bg qh qi l qj qk"># method of GPT<br/>def _init_parameters(self):<br/>    normal_init = nn.init.normal(mean=0.0, std=0.02)<br/>    residual_init = nn.init.normal(mean=0.0, std=(0.02 / math.sqrt(2 * n_layers)))<br/>    new_params = []<br/>    for name, module in self.named_modules():<br/>        if isinstance(module, nn.layers.linear.Linear):<br/>            new_params.append((name + '.weight', normal_init(module.weight)))<br/>        elif isinstance(module, nn.layers.embedding.Embedding):<br/>            new_params.append((name + '.weight', normal_init(module.weight)</span></pre><p id="36eb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We maintain a list of tuples called <code class="cx ql qm qn qd b">new_params</code> which will contain tuples of (parameter_name, new_value). Next, we loop through each nn.Module object in our model with <code class="cx ql qm qn qd b">self.named_modules()</code> which returns tuples of (name, module). If we print out the module names within the loop we see that they look like this:</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="fb2a" class="qg ou fq qd b bg qh qi l qj qk">lm_head<br/>blocks<br/>blocks.layers.4<br/>blocks.layers.3<br/>blocks.layers.3.ln_2<br/>blocks.layers.3.ln_1<br/>blocks.layers.3.mha<br/>blocks.layers.3.mha.resid_dropout<br/>blocks.layers.3.mha.c_proj<br/>blocks.layers.3.mha.attn_dropout<br/>blocks.layers.3.mha.c_attn<br/>...<br/>blocks.layers.0.mlp.dropout<br/>blocks.layers.0.mlp.c_proj<br/>blocks.layers.0.mlp.gelu<br/>blocks.layers.0.mlp.c_fc<br/>wpe<br/>wte</span></pre><p id="cc68" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We use the <code class="cx ql qm qn qd b">isinstance()</code> function to find the linear and embedding layers and then add them to our list. For example, say we are looping and reach “blocks.layers.0.mlp.c_fc” which is the first linear layer in the MLP. This would trigger the first if statement, and the tuple <code class="cx ql qm qn qd b">("block.layers.0.mlp.c_fc.weight", [&lt;normally initialized weight here&gt;])</code> would be added to our list. We have to add “.weight” to the name because we specifically want to initialize the weight in this way, not the bias. Now we need to handle the residual projection initialization.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="2e69" class="qg ou fq qd b bg qh qi l qj qk"># method of GPT<br/>def _init_parameters(self):<br/>    normal_init = nn.init.normal(mean=0.0, std=0.02)<br/>    residual_init = nn.init.normal(mean=0.0, std=(0.02 / math.sqrt(2 * n_layers)))<br/>    new_params = []<br/>    for name, module in self.named_modules():<br/>        if isinstance(module, nn.layers.linear.Linear):<br/>            if 'c_proj' in name: # residual projection<br/>                new_params.append((name + '.weight', residual_init(module.weight)))<br/>            else:<br/>                new_params.append((name + '.weight', normal_init(module.weight)))<br/>        elif isinstance(module, nn.layers.embedding.Embedding):<br/>            new_params.append((name + '.weight', normal_init(module.weight)))</span></pre><p id="fd47" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">After checking if the module is a linear layer, we check if “c_proj” is in the name because that’s how we named the residual projections. Then we can apply the special initialization. Finally, we need to initialize the biases to be zero.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="0af4" class="qg ou fq qd b bg qh qi l qj qk"># method of GPT<br/>def _init_parameters(self):<br/>    normal_init = nn.init.normal(mean=0.0, std=0.02)<br/>    residual_init = nn.init.normal(mean=0.0, std=(0.02 / math.sqrt(2 * n_layers)))<br/>    new_params = []<br/>    for name, module in self.named_modules():<br/>        if isinstance(module, nn.layers.linear.Linear):<br/>            if 'c_proj' in name:<br/>                new_params.append((name + '.weight', residual_init(module.weight)))<br/>            else:<br/>                new_params.append((name + '.weight', normal_init(module.weight)))<br/>            if 'bias' in module:<br/>                new_params.append((name + '.bias', mx.zeros(module.bias.shape)))<br/>        elif isinstance(module, nn.layers.embedding.Embedding):<br/>            new_params.append((name + '.weight', normal_init(module.weight)))<br/>    self = self.update(utils.tree_unflatten(new_params))</span></pre><p id="949b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We add another if statement under our linear branch to check if the nn.Module object has a bias attribute. If it does, we add it to the list initialized to zeros. Finally, we need to transform our list of tuples into a nested dictionary. Luckily mlx has some functions implemented for dealing with parameter dictionaries, and we can use <code class="cx ql qm qn qd b">util.tree_unflatten()</code> to convert this list of tuples to a nested parameter dictionary. This is passed into the update method to initialize the parameters. Now we can call <code class="cx ql qm qn qd b">_init_parameters()</code> in the constructor.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="e6d0" class="qg ou fq qd b bg qh qi l qj qk">class GPT(nn.Module):<br/>    def __init__(self):<br/>        super().__init__()<br/>        self.wte = nn.Embedding(vocab_size, n_emb) # token embeddings<br/>        self.wpe = nn.Embedding(ctx_len, n_emb) # position embeddings<br/>        self.blocks = nn.Sequential(<br/>            *[Block() for _ in range(n_layers)],<br/>        ) # transformer blocks<br/>        self.ln_f = nn.LayerNorm(dims=n_emb) # final layernorm<br/>        self.lm_head = nn.Linear(n_emb, vocab_size) # output projection<br/>        self._init_parameters() # &lt;-- initialize params<br/>        # print total number of params on initialization<br/>        total_params = sum([p.size for n,p in utils.tree_flatten(self.parameters())])<br/>        print(f"Total params: {(total_params / 1e6):.3f}M")<br/>    # Tensor shapes commented<br/>    def __call__(self, x):<br/>        B, T = x.shape # (B = batch_size, T = ctx_len)<br/>        tok_emb = self.wte(x) # (B, T, n_emb)<br/>        pos_emb = self.wpe(mx.arange(T)) # (T, n_emb)<br/>        x = tok_emb + pos_emb # (B, T, n_emb)<br/>        x = self.blocks(x) # (B, T, n_emb)<br/>        x = self.ln_f(x) # (B, T, b_emb)<br/>        logits = self.lm_head(x) # (B, T, vocab_size)<br/>        return logits<br/>    def generate(self, max_new_tokens):<br/>        ctx = mx.zeros((1, 1), dtype=mx.int32)<br/>        for _ in range(max_new_tokens):<br/>          logits = self(ctx[:, -ctx_len:]) # pass in last ctx_len characters<br/>          logits = logits[:, -1, :] # get logits for the next token<br/>          next_tok = mx.random.categorical(logits, num_samples=1)<br/>          ctx = mx.concatenate((ctx, next_tok), axis=1)<br/>        return ctx<br/>    def _init_parameters(self):<br/>        normal_init = nn.init.normal(mean=0.0, std=0.02)<br/>        residual_init = nn.init.normal(mean=0.0, std=(0.02 / math.sqrt(2 * n_layers)))<br/>        new_params = []<br/>        for name, module in self.named_modules():<br/>            if isinstance(module, nn.layers.linear.Linear):<br/>                if 'c_proj' in name:<br/>                    new_params.append((name + '.weight', residual_init(module.weight)))<br/>                else:<br/>                    new_params.append((name + '.weight', normal_init(module.weight)))<br/>                if 'bias' in module:<br/>                    new_params.append((name + '.bias', mx.zeros(module.bias.shape)))<br/>            elif isinstance(module, nn.layers.embedding.Embedding):<br/>                new_params.append((name + '.weight', normal_init(module.weight)))<br/>        self = self.update(utils.tree_unflatten(new_params))<br/>    </span></pre><p id="0701" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We also add 2 lines of code in the constructor to print the total number of params. Finally, we are ready to build the training loop.</p><h1 id="56cf" class="ot ou fq bf ov ow ox gq oy oz pa gt pb pc pd pe pf pg ph pi pj pk pl pm pn po bk">Training Loop</h1><p id="2bb7" class="pw-post-body-paragraph nd ne fq nf b go pp nh ni gr pq nk nl nm pr no np nq ps ns nt nu pt nw nx ny fj bk">To train the model we need a loss function. Since we are predicting classes (next token) we use cross-entropy loss.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="4b6a" class="qg ou fq qd b bg qh qi l qj qk">def loss_fn(model, x, y):<br/>    logits = model(x)<br/>    B, T, C = logits.shape # (batch_size, seq_len, vocab_size)<br/>    logits = logits.reshape(B*T, C)<br/>    y = y.reshape(B*T)<br/>    loss = nn.losses.cross_entropy(logits, y, reduction='mean')<br/>    return loss</span></pre><p id="3087" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">First, we get the logits from the model. Then we reshape logits to make a list of vocab_size length arrays. We also reshape y, the correct token ids, to have the same length. Then we use the built-in cross-entropy loss function to calculate the loss for each example and average them to get a single value.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="947e" class="qg ou fq qd b bg qh qi l qj qk">model = GPT()<br/>mx.eval(model.parameters()) # Create the model params (mlx is lazy evaluation)<br/>loss_and_grad = nn.value_and_grad(model, loss_fn)<br/>lr = 0.1<br/>optimizer = optim.AdamW(learning_rate=lr)</span></pre><p id="ee57" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Next, we instantiate the model, but since mlx is lazy evaluation it won’t allocate and create the parameters. We need to call mx.eval on the parameters to ensure they get created. Then we can use <code class="cx ql qm qn qd b"><a class="af nc" href="https://ml-explore.github.io/mlx/build/html/python/_autosummary/mlx.nn.value_and_grad.html" rel="noopener ugc nofollow" target="_blank">nn.value_and_grad()</a></code> to get a function that returns the loss and gradient of model parameters w.r.t the loss. This is all we need to optimize. Finally, we initialize an AdamW optimizer.</p><p id="b388" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">A quick note on nn.value_and_grad(). If you are used to PyTorch you might expect us to use loss.backward() which goes through the computation graph and updates the .grad attribute of each tensor in our model. However, mlx automatic differentiation works on functions instead of computation graphs [3]. Therefore, mlx has built-ins that take in a function and return the gradient function such as <code class="cx ql qm qn qd b">nn.value_and_grad()</code> .</p><p id="8cbf" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now we define the training loop.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="1381" class="qg ou fq qd b bg qh qi l qj qk">num_epochs=20<br/>batch_size=32<br/>for epoch in range(num_epochs):<br/>    model.train(True)<br/>    running_loss = 0<br/>    batch_cnt = 0<br/>    for input, label in get_batches(X_train, y_train, batch_size):<br/>        batch_cnt += 1<br/>        loss, grads = loss_and_grad(model, input, label)<br/>        optimizer.update(model, grads)<br/>        running_loss += loss.item()<br/>        # compute new parameters and optimizer state<br/>        mx.eval(model.parameters(), optimizer.state)<br/>    avg_train_loss = running_loss / batch_cnt<br/>    model.train(False) # set eval mode<br/>    running_loss = 0<br/>    batch_cnt = 0<br/>    for input, label in get_batches(X_val, y_val, batch_size):<br/>        batch_cnt += 1<br/>        loss = loss_fn(model, input, label)<br/>        running_loss += loss.item()<br/>    avg_val_loss = running_loss / batch_cnt<br/>    print(f"Epoch {epoch:2} | train = {avg_train_loss:.4f} | val = {avg_val_loss:.4f}")</span></pre><p id="4ab2" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The outer loop runs through the epochs. We first set the model to training mode because some modules have different behaviors during training and testing such as dropout. Then we use our <code class="cx ql qm qn qd b">get_batches</code> function from earlier to loop through batches of the training data. We get the loss over the batch and the gradient using <code class="cx ql qm qn qd b">loss_and_grad</code> . Then we pass the model and gradients to the optimizer to update the model parameters. Finally we call mx.eval (remember mlx does lazy evaluation) to ensure the parameters and optimizer state get updated. Then we calculate the average train loss over the data to print later. This is one pass through the training data. Similarly, we calculate the validation loss and then print the average train and val loss over the epoch.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="155e" class="qg ou fq qd b bg qh qi l qj qk">completion = decode(model.generate(1000)[0].tolist())<br/>print(completion)<br/>with open('completions.txt', 'w') as f:<br/>    f.write(completion)</span></pre><p id="98b9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Finally, we add some code to generate from our model. Since the generation output is still in the (B, T) shape we have to index it at 0 to make it 1D and then convert it from an mlx array to a Python list. Then we can pass it to our decode function from earlier, and write it to a file.</p><p id="d68f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">These are the parameters we will use for training (you can play around with this):</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="f375" class="qg ou fq qd b bg qh qi l qj qk">ctx_len = 128<br/>n_emb = 128<br/>dropout = 0.1<br/>head_size = 128<br/>n_heads = 4 <br/>n_layers = 3 <br/>num_epochs = 20<br/>batch_size = 64<br/>lr = 1e-3</span></pre><p id="9201" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now we can run the file to start training. With the settings above training took around 10 minutes on my m2 MacBook. I achieved the following training loss last epoch.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="4d05" class="qg ou fq qd b bg qh qi l qj qk">Epoch 19 | train = 1.6961 | val = 1.8143</span></pre><p id="fd7c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Let’s look at some output.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="abe6" class="qg ou fq qd b bg qh qi l qj qk">GLOUCESTER:<br/>But accomes mo move it.<br/><br/>KING EDWARD:<br/>Where our that proclaim that I curse, or I sprithe.<br/><br/>CORIOLANUS:<br/>Not want:<br/>His bops to thy father<br/>At with hath folk; by son and fproathead:<br/>The good nor may prosperson like it not,<br/>What, the beggares<br/>More hath, when that made a,<br/>Your vainst Citizen:<br/>Let here are go in queen me and knife<br/>To my deserved me you promise: not a fettimes,<br/>That one the will not.<br/><br/>CORIOLANUS:<br/>And been of queens,<br/>Thou to do we best!<br/><br/>JULIET:<br/>Not, brother recourable this doth our accuse<br/>Into fight!</span></pre><p id="889c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Not bad for just 10 minutes of training with a tiny model that is predicting characters! It clearly has the form of Shakespeare, although it is nonsense. The only difference between our model and the real GPT-2 now is scale! Now I encourage you to experiment — try out different settings, maybe tinker with the architecture, and see how low of a loss you can achieve.</p><h1 id="0e89" class="ot ou fq bf ov ow ox gq oy oz pa gt pb pc pd pe pf pg ph pi pj pk pl pm pn po bk">References</h1><p id="43dd" class="pw-post-body-paragraph nd ne fq nf b go pp nh ni gr pq nk nl nm pr no np nq ps ns nt nu pt nw nx ny fj bk">[1] Karpathy A (2015).<em class="qo">Tiny Shakespeare </em>[Data set]. <a class="af nc" href="https://github.com/karpathy/char-rnn" rel="noopener ugc nofollow" target="_blank">https://github.com/karpathy/char-rnn</a> (MIT license)</p><p id="baf0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[2] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, <a class="af nc" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="noopener ugc nofollow" target="_blank">Language Models are Unsupervised Multitask Learners</a> (2019), OpenAI</p><p id="e953" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[3] <a class="af nc" href="https://ml-explore.github.io/mlx/build/html/usage/function_transforms.html#auto-diff" rel="noopener ugc nofollow" target="_blank">Automatic Differentiation — mlx docs</a></p></div></div></div></div>    
</body>
</html>