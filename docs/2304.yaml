- en: Implementing GraphReader with Neo4j and LangGraph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/implementing-graphreader-with-neo4j-and-langgraph-e4c73826a8b7?source=collection_archive---------1-----------------------#2024-09-21](https://towardsdatascience.com/implementing-graphreader-with-neo4j-and-langgraph-e4c73826a8b7?source=collection_archive---------1-----------------------#2024-09-21)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Elevating RAG accuracy and performance by structuring long documents into explorable
    graphs and implementing graph-based agent systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://bratanic-tomaz.medium.com/?source=post_page---byline--e4c73826a8b7--------------------------------)[![Tomaz
    Bratanic](../Images/d5821aa70918fcb3fc1ff0013497b3d5.png)](https://bratanic-tomaz.medium.com/?source=post_page---byline--e4c73826a8b7--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e4c73826a8b7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e4c73826a8b7--------------------------------)
    [Tomaz Bratanic](https://bratanic-tomaz.medium.com/?source=post_page---byline--e4c73826a8b7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e4c73826a8b7--------------------------------)
    ·23 min read·Sep 21, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fc66a01ca45698bc7ef7f7bcc2dbeed9.png)'
  prefs: []
  type: TYPE_IMG
- en: An AI agent traversing the graph as imagined by ChatGPT
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) are great at traditional NLP tasks like summarization
    and sentiment analysis but the stronger models also demonstrate promising reasoning
    abilities. LLM reasoning is often understood as the ability to tackle complex
    problems by formulating a plan, executing it, and assessing progress at each step.
    Based on this evaluation, they can adapt by revising the plan or taking alternative
    actions. The rise of agents is becoming an increasingly compelling approach to
    answering complex questions in RAG applications.
  prefs: []
  type: TYPE_NORMAL
- en: In this blog post, we’ll explore the implementation of the [GraphReader agent](https://arxiv.org/abs/2406.14550).
    This agent is designed to retrieve information from a structured knowledge graph
    that follows a predefined schema. Unlike the typical graphs you might see in presentations,
    this one is closer to a document or **lexical graph**, containing documents, their
    chunks, and relevant metadata in the form of atomic facts.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/71b960bfb63594c2d88dfcb91c5ccafb.png)'
  prefs: []
  type: TYPE_IMG
- en: Generated knowledge graph following the GraphReader implementation. Image by
    author.
  prefs: []
  type: TYPE_NORMAL
- en: The image above illustrates a knowledge graph, beginning at the top with a document
    node labeled *Joan of Arc*. This document is broken down into text chunks, represented
    by numbered circular nodes (0, 1, 2, 3), which are connected sequentially through
    *NEXT* relationships, indicating the order in which the chunks appear in the document.
    Below the text chunks, the graph further breaks down into atomic facts, where
    specific statements about the content are represented. Finally, at the bottom
    level of the graph, we see the key elements, represented as circular nodes with
    topics like *historical icons*, *Dane*, *French nation*, and *France*. These elements
    act as metadata, linking the facts to the broader themes and concepts relevant
    to the document.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have constructed the knowledge graph, we will follow the implementation
    provided in the [GraphReader paper](https://arxiv.org/abs/2406.14550).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fc1f216a8c0999ddf602d0b3abc21211.png)'
  prefs: []
  type: TYPE_IMG
- en: GraphReader agent implementation. Image from the [paper](https://arxiv.org/abs/2406.14550)
    with authors’ permission.
  prefs: []
  type: TYPE_NORMAL
- en: The agent exploration process involves initializing the agent with a rational
    plan and selecting initial nodes to start the search in a graph. The agent explores
    these nodes by first gathering atomic facts, then reading relevant text chunks,
    and updating its notebook. The agent can decide to explore more chunks, neighboring
    nodes, or terminate based on gathered information. When the agent decided to terminate,
    the answer reasoning step is executed to generate the final answer.
  prefs: []
  type: TYPE_NORMAL
- en: In this blog post, we will implement the GraphReader paper using [Neo4j](https://neo4j.com/)
    as the storage layer and [LangChain](https://www.langchain.com/) in combination
    with [LangGraph](https://langchain-ai.github.io/langgraph/) to define the agent
    and its flow.
  prefs: []
  type: TYPE_NORMAL
- en: The code is available on [GitHub](https://github.com/tomasonjo/blogs/tree/master/graphreader).
  prefs: []
  type: TYPE_NORMAL
- en: Environment Setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You need to setup a Neo4j to follow along with the examples in this blog post.
    The easiest way is to start a free instance on [Neo4j Aura](https://neo4j.com/cloud/platform/aura-graph-database/),
    which offers cloud instances of Neo4j database. Alternatively, you can also setup
    a local instance of the Neo4j database by downloading the [Neo4j Desktop](https://neo4j.com/download/)
    application and creating a local database instance.
  prefs: []
  type: TYPE_NORMAL
- en: The following code will instantiate a LangChain wrapper to connect to Neo4j
    Database.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Additionally, we have also added [constraints](https://neo4j.com/docs/cypher-manual/current/constraints/)
    for the node types we will be using. The constraints ensure faster import and
    retrieval performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, you will require an OpenAI api key that you pass in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Graph construction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will be using the [Joan of Arc](https://en.wikipedia.org/wiki/Joan_of_Arc)
    Wikipedia page in this example. We will use LangChain built-in utility to retrieve
    the text.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As mentioned before, the GraphReader agent expects knowledge graph that contains
    chunks, related atomic facts, and key elements.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ab7091ef9ac06f48d9e80d51b203a06b.png)'
  prefs: []
  type: TYPE_IMG
- en: GraphReader knowledge graph construction. Image from the [paper](https://arxiv.org/abs/2406.14550)
    with authors’ permission.
  prefs: []
  type: TYPE_NORMAL
- en: First, the document is split into chunks. In the paper they maintained paragraph
    structure while chunking. However, that is hard to do in a generic way. Therefore,
    we will use naive chunking here.
  prefs: []
  type: TYPE_NORMAL
- en: Next, each chunk is processed by the LLM to identify **atomic facts**, which
    are the smallest, indivisible units of information that capture core details.
    For instance, from the sentence “The CEO of Neo4j, which is in Sweden, is Emil
    Eifrem” an atomic fact could be broken down into something like “The CEO of Neo4j
    is Emil Eifrem.” and “Neo4j is in Sweden.” Each atomic fact is focused on one
    clear, standalone piece of information.
  prefs: []
  type: TYPE_NORMAL
- en: From these atomic facts, **key elements** are identified. For the first fact,
    “The CEO of Neo4j is Emil Eifrem,” the key elements would be “CEO,” “Neo4j,” and
    “Emil Eifrem.” For the second fact, “Neo4j is in Sweden,” the key elements would
    be “Neo4j” and “Sweden.” These key elements are the essential nouns and proper
    names that capture the core meaning of each atomic fact.
  prefs: []
  type: TYPE_NORMAL
- en: The prompt used to extract the graph are provided in the appendix of the paper.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/84336de7edc06097874bf0b875122f95.png)'
  prefs: []
  type: TYPE_IMG
- en: The prompt for key element and atomic fact extraction. Taken from the [paper](https://arxiv.org/abs/2406.14550)
    with authors’ permission.
  prefs: []
  type: TYPE_NORMAL
- en: The authors used prompt-based extraction, where you instruct the LLM what it
    should output and then implement a function that parses the information in a structured
    manner. My preference for extracting structured information is to use the `with_structured_output`
    method in LangChain, which utilizes the tools feature to extract structured information.
    This way, we can skip defining a custom parsing function.
  prefs: []
  type: TYPE_NORMAL
- en: Here is the prompt that we can use for extraction.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We have put the instruction in the system prompt, and then in the user message
    we provide relevant text chunks that need to be processed.
  prefs: []
  type: TYPE_NORMAL
- en: To define the desired output, we can use the Pydantic object definition.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We want to extract a list of atomic facts, where each atomic fact contains a
    string field with the fact, and a list of present key elements. It is important
    to add description to each element to get the best results.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can combine it all in a chain.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: To put it all together, we’ll create a function that takes a single document,
    chunks it, extracts atomic facts and key elements, and stores the results into
    Neo4j.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'At a high level, this code processes a document by breaking it into chunks,
    extracting information from each chunk using an AI model, and storing the results
    in a graph database. Here’s a summary:'
  prefs: []
  type: TYPE_NORMAL
- en: It splits the document text into chunks of a specified size, allowing for some
    overlap. The chunk size of 2000 tokens is used by the authors in the paper.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each chunk, it asynchronously sends the text to an LLM for extraction of
    atomic facts and key elements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each chunk and fact is given a unique identifier using an *md5* encoding function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The processed data is imported into a graph database, with relationships established
    between consecutive chunks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can now run this function on our Joan of Arc text.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We used a smaller chunk size because it’s a small document, and we want to have
    a couple of chunks for demonstration purposes. If you explore the graph in Neo4j
    Browser, you should see a similar visualization.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c7d1df64f44e006c2f1a15fe931a071c.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualization of the generated graph. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: At the center of the structure is the document node (blue), which branches out
    to chunk nodes (pink). These chunk nodes, in turn, are linked to atomic facts
    (orange), each of which connects to key elements (green).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s examine the constructed graph a bit. We’ll start of by examining the token
    count distribution of atomic facts.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '*Results*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f0a07fc6b51842c42a43c28cada39f49.png)'
  prefs: []
  type: TYPE_IMG
- en: Distribution of token count for atomic facts. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Atomic facts are relatively short, with the longest being only about 50 tokens.
    Let’s examine a couple to get a better idea.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '*Results*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/54adffdbe7154d75edeaaf6756f05ac0.png)'
  prefs: []
  type: TYPE_IMG
- en: Atomic facts
  prefs: []
  type: TYPE_NORMAL
- en: Some of the shortest facts lack context. For example, the original score and
    screenplay don’t directly mention which. Therefore, if we processed multiple documents,
    these atomic facts might be less helpful. This lack of context might be solved
    with additional prompt engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s also examine the most frequent keywords.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '*Results*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/addf379cd500eae452e4e83e3a21be08.png)'
  prefs: []
  type: TYPE_IMG
- en: Top five most mentioned key elements. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Unsurprisingly, Joan of Arc is the most mentioned keyword or element. Following
    are broad keywords like film, English, and France. I suspect that if we parsed
    many documents, broad keywords would end up having a lot of connections, which
    might lead to some downstream problems that aren’t dealt with in the original
    implementation. Another minor problem is the non-determinism of the extraction,
    as the results will be slight different on every run.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the authors employ key element normalization as described in [Lu
    et al. (2023)](https://arxiv.org/pdf/2308.07074), specifically using frequency
    filtering, rule, semantic, and association aggregation. In this implementation,
    we skipped this step.
  prefs: []
  type: TYPE_NORMAL
- en: GraphReader Agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’re ready to implement GraphReader, a graph-based agent system. The agent
    starts with a couple of predefined steps, followed by the steps in which it can
    traverse the graph autonomously, meaning the agent decides the following steps
    and how to traverse the graph.
  prefs: []
  type: TYPE_NORMAL
- en: Here is the LangGraph visualization of the agent we will implement.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b24f71a840150b0590f2177624343f1c.png)'
  prefs: []
  type: TYPE_IMG
- en: Agent workflow implementation in LangGraph. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: The process begins with a rational planning stage, after which the agent makes
    an initial selection of nodes (key elements) to work with. Next, the agent checks
    atomic facts linked to the selected key elements. Since all these steps are predefined,
    they are visualized with a full line.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the outcome of the atomic fact check, the flow proceeds to either
    read relevant text chunks or explore the neighbors of the initial key elements
    in search of more relevant information. Here, the next step is conditional and
    based on the results of an LLM and is, therefore, visualized with a dotted line.
  prefs: []
  type: TYPE_NORMAL
- en: In the chunk check stage, the LLM reads and evaluates whether the information
    gathered from the current text chunk is sufficient. Based on this evaluation,
    the LLM has a few options. It can decide to read additional text chunks if the
    information seems incomplete or unclear. Alternatively, the LLM may choose to
    explore neighboring key elements, looking for more context or related information
    that the initial selection might not have captured. If, however, the LLM determines
    that enough relevant information has been gathered, it will proceed directly to
    the answer reasoning step. At this point, the LLM generates the final answer based
    on the collected information.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this process, the agent dynamically navigates the flow based on the
    outcomes of the conditional checks, making decisions on whether to repeat steps
    or continue forward depending on the specific situation. This provides flexibility
    in handling different inputs while maintaining a structured progression through
    the steps.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we’ll go over the steps and implement them using LangGraph abstraction.
    You can learn more about LangGraph through [LangChain’s academy course](https://academy.langchain.com/courses/intro-to-langgraph).
  prefs: []
  type: TYPE_NORMAL
- en: LangGraph state
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To build a LangGraph implementation, we start by defining a state passed along
    the steps in the flow.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: For more advanced use cases, multiple separate states can be used. In our implementation,
    we have separate input and output states, which define the input and output of
    the LangGraph, and a separate overall state, which is passed between steps.
  prefs: []
  type: TYPE_NORMAL
- en: By default, the state is overwritten when returned from a node. However, you
    can define other operations. For example, with the `previous_actions` we define
    that the state is appended or added instead of overwritten.
  prefs: []
  type: TYPE_NORMAL
- en: The agent begins by maintaining a notebook to record supporting facts, which
    are eventually used to derive the final answer. Other states will be explained
    as we go along.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move on to defining the nodes in the LangGraph.
  prefs: []
  type: TYPE_NORMAL
- en: Rational plan
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the rational plan step, the agent breaks the question into smaller steps,
    identifies the key information required, and creates a logical plan. The logical
    plan allows the agent to handle complex multi-step questions.
  prefs: []
  type: TYPE_NORMAL
- en: While the code is unavailable, all the prompts are in the appendix, so we can
    easily copy them.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e33bbce0019256b2aad5cbb955cecfd0.png)'
  prefs: []
  type: TYPE_IMG
- en: The prompt for rational plan. Taken from the [paper](https://arxiv.org/abs/2406.14550)
    with authors’ permission.
  prefs: []
  type: TYPE_NORMAL
- en: The authors don’t explicitly state whether the prompt is provided in the system
    or user message. For the most part, I have decided to put the instructions as
    a system message.
  prefs: []
  type: TYPE_NORMAL
- en: The following code shows how to construct a chain using the above rational plan
    as the system message.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can use this chain to define a rational plan node. A node in LangGraph
    is a function that takes the state as input and updates it as output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The function starts by invoking the LLM chain, which produces the rational plan.
    We do a little printing for debugging and then update the state as the function’s
    output. I like the simplicity of this approach.
  prefs: []
  type: TYPE_NORMAL
- en: Initial node selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the next step, we select the initial nodes based on the question and rational
    plan. The prompt is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e1462c0d6a9382ff40e6b394531bd9f7.png)'
  prefs: []
  type: TYPE_IMG
- en: The prompt for initial node selection. Taken from the [paper](https://arxiv.org/abs/2406.14550)
    with authors’ permission.
  prefs: []
  type: TYPE_NORMAL
- en: The prompt starts by giving the LLM some context about the overall agent system,
    followed by the task instructions. The idea is to have the LLM select the top
    10 most relevant nodes and score them. The authors simply put all the key elements
    from the database in the prompt for an LLM to select from. However, I think that
    approach doesn’t really scale. Therefore, we will create and use a vector index
    to retrieve a list of input nodes for the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The `from_existing_graph` method pulls the defined `text_node_properties` from
    the graph and calculates embeddings where they are missing. Here, we simply embed
    the `id` property of **KeyElement** nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s define the chain. We’ll first copy the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Again, we put most of the instructions as the system message. Since we have
    multiple inputs, we can define them in the human message. However, we need a more
    structured output this time. Instead of writing a parsing function that takes
    in text and outputs a JSON, we can simply use the `use_structured_output`method
    to define the desired output structure.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We want to output a list of nodes containing the key element and the score.
    We can easily define the output using a Pydantic model. Additionally, it is vital
    to add descriptions to each of the field, so we can guide the LLM as much as possible.
  prefs: []
  type: TYPE_NORMAL
- en: The last thing in this step is to define the node as a function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In the initial node selection, we start by getting a list of potential nodes
    using the vector similarity search based on the input. An option is to use rational
    plan instead. The LLM is prompted to output the 10 most relevant nodes. However,
    the authors say that we should use only 5 initial nodes. Therefore, we simply
    order the nodes by their score and take the top 5 ones. We then update the `check_atomic_facts_queue`
    with the selected initial key elements.
  prefs: []
  type: TYPE_NORMAL
- en: Atomic fact check
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this step, we take the initial key elements and inspect the linked atomic
    facts. The prompt is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b0bf14319eb92e8c735c357ce95cf88f.png)'
  prefs: []
  type: TYPE_IMG
- en: The prompt for exploring atomic facts. Taken from the [paper](https://arxiv.org/abs/2406.14550)
    with authors’ permission.
  prefs: []
  type: TYPE_NORMAL
- en: All prompts start by giving the LLM some context, followed by task instructions.
    The LLM is instructed to read the atomic facts and decide whether to read the
    linked text chunks or if the atomic facts are irrelevant, search for more information
    by exploring the neighbors. The last bit of the prompt is the output instructions.
    We will use the structured output method again to avoid manually parsing and structuring
    the output.
  prefs: []
  type: TYPE_NORMAL
- en: Since chains are very similar in their implementation, different only by prompts,
    we’ll avoid showing every definition in this blog post. However, we’ll look at
    the LangGraph node definitions to better understand the flow.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The atomic fact check node starts by invoking the LLM to evaluate the atomic
    facts of the selected nodes. Since we are using the `use_structured_output` we
    can parse the updated notebook and the chosen action output in a straightforward
    manner. If the selected action is to get additional information by inspecting
    the neighbors, we use a function to find those neighbors and append them to the
    `check_atomic_facts_queue`. Otherwise, we append the selected chunks to the `check_chunks_queue`.
    We update the overall state by updating the notebook, queues, and the chosen action.
  prefs: []
  type: TYPE_NORMAL
- en: Text chunk check
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As you might imagine by the name of the LangGraph node, in this step, the LLM
    reads the selected text chunk and decides the best next step based on the provided
    information. The prompt is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fad9ce6c214faf445fa5e129744e7154.png)'
  prefs: []
  type: TYPE_IMG
- en: The prompt for exploring chunks. Taken from the [paper](https://arxiv.org/abs/2406.14550)
    with authors’ permission.
  prefs: []
  type: TYPE_NORMAL
- en: The LLM is instructed to read the text chunk and decide on the best approach.
    My gut feeling is that sometimes relevant information is at the start or the end
    of a text chunk, and parts of the information might be missing due to the chunking
    process. Therefore, the authors decided to give the LLM the option to read a previous
    or next chunk. If the LLM decides it has enough information, it can hop on to
    the final step. Otherwise, it has the option to search for more details using
    the `search_more`function.
  prefs: []
  type: TYPE_NORMAL
- en: Again, we’ll just look at the LangGraph node function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We start by popping a chunk ID from the queue and retrieving its text from the
    graph. Using the retrieved text and additional information from the overall state
    of the LangGraph system, we invoke the LLM chain. If the LLM decides it wants
    to read previous or subsequent chunks, we append their IDs to the queue. On the
    other hand, if the LLM chooses to search for more information, we have two options.
    If there are any other chunks to read in the queue, we move to reading them. Otherwise,
    we can use the vector search to get more relevant key elements and repeat the
    process by reading their atomic facts and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The paper is slightly dubious about the `search_more` function. On the one hand,
    it states that the `search_more` function can only read other chunks in the queue.
    On the other hand, in their example in the appendix, the function clearly explores
    the neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6f7ac4164fc9cba89c57eb2e6ac9d0b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Example action history. Taken from the [paper](https://arxiv.org/abs/2406.14550)
    with authors’ permission.
  prefs: []
  type: TYPE_NORMAL
- en: To clarify, I emailed the authors, and they confirmed that the `search_more`function
    first tries to go through additional chunks in the queue. If none are present,
    it moves on to exploring the neighbors. Since how to explore the neighbors isn’t
    explicitly defined, we again use the vector similarity search to find potential
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Neighbor selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When the LLM decides to explore the neighbors, we have helper functions to
    find potential key elements to explore. However, we don’t explore all of them.
    Instead, an LLM decides which of them is worth exploring, if any. The prompt is
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f9aadd49fa519cb33b6f7a786876a749.png)'
  prefs: []
  type: TYPE_IMG
- en: The prompt for exploring neighbors. Taken from the [paper](https://arxiv.org/abs/2406.14550)
    with authors’ permission.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the provided potential neighbors, the LLM can decide which to explore.
    If none are worth exploring, it can decide to terminate the flow and move on to
    the answer reasoning step.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Here, we execute the LLM chain and parse results. If the chosen action is to
    explore any neighbors, we add them to the `check_atomic_facts_queue` .
  prefs: []
  type: TYPE_NORMAL
- en: Answer reasoning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The last step in our flow is to ask the LLM to construct the final answer based
    on the collected information in the notebook. The prompt is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/52228770f9c551cbed8ed9778f3e14f7.png)'
  prefs: []
  type: TYPE_IMG
- en: The prompt for answer reasoning. Taken from the [paper](https://arxiv.org/abs/2406.14550)
    with authors’ permission.
  prefs: []
  type: TYPE_NORMAL
- en: 'This node implementation is fairly straightforward as you can see by the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We simply input the original question and the notebook with the collected information
    to the chain and ask it to formulate the final answer and provide the explanation
    in the analysis part.
  prefs: []
  type: TYPE_NORMAL
- en: LangGraph flow definition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The only thing left is to define the LangGraph flow and how it should traverse
    between the nodes. I am quite fond of the simple approach the LangChain team has
    chosen.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We begin by defining the state graph object, where we can define the information
    passed along in the LangGraph. Each node is simply added with the `add_node` method.
    Normal edges, where one step always follows the other, can be added with a `add_edge`
    method. On the other hand, if the traversals is dependent on previous actions,
    we can use the `add_conditional_edge` and pass in the function that selects the
    next node. For example, the `atomic_fact_condition` looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, it’s about as simple as it gets to define the conditional edge.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally we can test our implementation on a couple of questions. Let’s begin
    with a simple one.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '*Results*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/19397837792c7ab4322659d6ef31586e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: The agent begins by forming a rational plan to identify the battles Joan of
    Arc participated in during her military career and to determine whether any were
    lost. After setting this plan, it moves to an atomic fact check about key battles
    such as the Siege of Orléans, the Siege of Paris, and La Charité. Rather than
    expanding the graph, the agent directly confirms the facts it needs. It reads
    text chunks that provide further details on Joan of Arc’s unsuccessful campaigns,
    particularly the failed Siege of Paris and La Charité. Since this information
    answers the question about whether Joan lost any battles, the agent stops here
    without expanding its exploration further. The process concludes with a final
    answer, confirming that Joan did indeed lose some battles, notably at Paris and
    La Charité, based on the evidence gathered.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now throw it a curveball.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '*Results*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/00214edcb3f1a075f88bcac01ca2a835.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: After the rational plan, the agent selected the initial key elements to explore.
    However, the issue is that none of these key elements exists in the database,
    and the LLM simply hallucinated them. Maybe some prompt engineering could solve
    hallucinations, but I haven’t tried. One thing to note is that it’s not that terrible,
    as these key elements don’t exist in the database, so we can’t pull any relevant
    information. Since the agent didn’t get any relevant data, it searched for more
    information. However, none of the neighbors are relevant either, so the process
    is stopped, letting the user know that the information is unavailable.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s try a multi-hop question.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '*Results*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1694c03f212a1965a9d92ae832f1a378.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: It’s a bit too much to copy the whole flow, so I copied only the answer part.
    The flow for this questions is quite non-deterministic and very dependent on the
    model being used. It’s kind of funny, but as I was testing the newer the model,
    the worse it performed. So the GPT-4 was the best (also used in this example),
    followed by GPT-4-turbo, and the last place goes to GPT-4o.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I’m very excited about GraphReader and similar approaches, specifically because
    I think such an approach to (Graph)RAG can be pretty generic and applied to any
    domain. Additionally, you can avoid the whole graph modeling part as the graph
    schema is static, allowing the graph agent to traverse it using predefined functions.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed some issues with this implementation along the way. For example,
    the graph construction on many documents might result in broad key elements ending
    up as supernodes, and sometimes, the atomic facts don’t contain the full context.
  prefs: []
  type: TYPE_NORMAL
- en: The retriever part is super reliant on extracted and selected key elements.
    In the original implementation, they put all the key elements in the prompt to
    choose from. However, I doubt that that approach scales well. Perhaps we also
    need an additional function to allow the agent to search for more information
    in other ways than just to explore the neighbor key elements.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, the agent system is highly dependent on the performance of the LLM.
    Based on my testing, the best model from OpenAI is the original GPT-4, which is
    funny as it’s the oldest. I haven’t tested the o1, though.
  prefs: []
  type: TYPE_NORMAL
- en: All in all, I am excited to explore more of these document graphs implementations,
    where metadata is extracted from text chunk and used to navigate the information
    better. Let me know if you have any ideas how to improve this implementation or
    have any other you like.
  prefs: []
  type: TYPE_NORMAL
- en: As always, the code is available on [GitHub](https://github.com/tomasonjo/blogs/tree/master/graphreader).
  prefs: []
  type: TYPE_NORMAL
