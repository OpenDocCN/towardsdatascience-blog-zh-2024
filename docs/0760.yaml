- en: Data Quality Error Detection powered by LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/automated-detection-of-data-quality-issues-54a3cb283a91?source=collection_archive---------0-----------------------#2024-03-22](https://towardsdatascience.com/automated-detection-of-data-quality-issues-54a3cb283a91?source=collection_archive---------0-----------------------#2024-03-22)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@simon.grah?source=post_page---byline--54a3cb283a91--------------------------------)[![Simon
    Grah](../Images/f8fd00600db79bc910ff51e9f64503d0.png)](https://medium.com/@simon.grah?source=post_page---byline--54a3cb283a91--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--54a3cb283a91--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--54a3cb283a91--------------------------------)
    [Simon Grah](https://medium.com/@simon.grah?source=post_page---byline--54a3cb283a91--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--54a3cb283a91--------------------------------)
    ·17 min read·Mar 22, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: This article is the second in a series about cleaning data using Large Language
    Models (LLMs), with a focus on identifying errors in tabular data sets.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7ad60ce10b812d10c4d79cec6c7b7cce.png)'
  prefs: []
  type: TYPE_IMG
- en: The sketch outlines the methodology we’ll explore in this article, which focuses
    on evaluating the Data Dirtiness Score of a tabular data set with minimal human
    involvement.
  prefs: []
  type: TYPE_NORMAL
- en: The Data Dirtiness Score
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Readers are encouraged to first review the introductory article on the [Data
    Dirtiness Score](https://medium.com/p/fe2ca5678d40), which explains the key assumptions
    and demonstrates how to calculate this score.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a quick refresher, the *Data Dirtiness Score* estimates the expected proportion
    of cells in a data set that contain errors. Here are the key hypotheses behind
    this metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data errors are related to violated constraints.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there are **no expectations**, there is **no effect on the score**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data problems** can be **pinpointed to specific cell**s.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each **data error** is assigned a **confidence score**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Every cell has an equal impact** on the overall score.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The initial step in this process involves identifying and cataloguing data inaccuracies
    present within the data set.
  prefs: []
  type: TYPE_NORMAL
- en: The Importance of Detecting Data Quality Issues Automatically
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Detecting data issues is crucial in the process but challenging due to several
    factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**High Human Labelling Cost**: Identifying data errors often needs significant
    input from data professionals (like scientists, engineers, and analysts) or subject
    matter experts (SMEs). This requires a lot of time and is expensive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lack of Enthusiasm Among Data Practitioners for this Grunt Wor**k: It’s no
    secret that many in the field view data cleaning as a less appealing aspect of
    their work. Seen as a precursor to more engaging activities such as modelling,
    building modern data stacks or answering business queries, data cleaning often
    falls lower on the priority list, leading to procrastination or, in some cases,
    completely ignored until critical issues arise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SME Limitations**: SMEs have valuable knowledge but might lack technical
    skills like SQL or programming. While no-code and low-code tools help to some
    extent, they haven’t been fully adopted and might not cover all data management
    aspects, such as version control.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The Expertise Gap**: Effective data cleaning transcends basic skill sets,
    requiring specialised expertise. The lack of training and the general disinterest
    in data preparation mean that many practitioners may only identify superficial
    errors, missing more complex issues that require a deeper understanding of data
    cleaning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite the inherent challenges, advancements in the field of Large Language
    Models (LLMs) offer promising solutions for automating the identification of straightforward
    data issues and uncovering more intricate data quality problems.
  prefs: []
  type: TYPE_NORMAL
- en: Data Error Detection powered by LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Large language models are becoming invaluable tools in automating the detection
    of data quality issues, serving as an efficient starting point for a productive
    human-in-the-loop iterative process. Models, such as those discussed in papers
    like [Jellyfish: A Large Language Model for Data Preprocessing](https://www.semanticscholar.org/reader/7e17ef56273063dfa838de30b7cc0546b2e5ee10),
    [Can language models automate data wrangling?](http://josephorallo.webs.upv.es/escrits/MLJ-DataWranglingAutomation.pdf)
    and [Large Language Models as Data Preprocessors](https://arxiv.org/abs/2308.16361),
    demonstrate their potential to automate constraint generation and data error detection.
    This automation doesn’t replace human intervention but rather enhances it, allowing
    for the review and adjustment of automated constraints by either addressing issues
    directly or modifying confidence scores to reflect the uncertainty inherent in
    data error detection.'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are particularly well-suited for detecting data quality issues due to their
    extensive training on a diverse range of internet content, including a vast array
    of domain knowledge and numerous examples of code reviews related to data quality
    issues. This training enables LLMs to identify data errors based on textual content
    without the need for explicitly defined rules. By converting tabular data sets
    into plain text (called *serialisation*), LLMs can scrutinise data much like a
    team of experienced humans, leveraging their “compressed” internet knowledge to
    pinpoint errors. This extensive training allows them to identify potential errors
    in human-readable data sets, such as CSV files, with a level of intuition that
    mimics human expertise. Moreover, any gaps in domain-specific knowledge can be
    bridged through techniques like Retrieval-Augmented Generation (RAG) or by tailoring
    the model’s prompts to the specific nature of the data set.
  prefs: []
  type: TYPE_NORMAL
- en: Another key advantage of employing LLMs in data error detection is their ability
    to handle the inherent uncertainty associated with data quality issues. Not all
    errors are straightforward, and even experts can sometimes disagree on what constitutes
    a data issue. LLMs can assign confidence scores to their findings, like a human
    does based on a mix of intuition and experience, reflecting the estimated likelihood
    of an error.
  prefs: []
  type: TYPE_NORMAL
- en: The challenge of generalising error detection across diverse data sets and potential
    issues is considerable. Traditional methods often resort to an extensive set of
    decision rules or a combination of specialised machine learning models to address
    various scenarios, such as checking the validity of addresses and phone numbers
    or anomaly detection. This is where LLMs shine, offering a more adaptable and
    less labour-intensive alternative. Their ability to understand and identify a
    wide range of data quality issues without extensive rule-based systems or domain-specific
    models makes them an invaluable tool. The analogy with the advantages of Machine
    Learning approaches over traditional business rules or statistical methods is
    quite intriguing. The adoption of machine learning has been driven by its relative
    ease of use and adaptability across different use cases, requiring less domain-specific
    knowledge and time to implement.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will demonstrate this approach through a practical example.
  prefs: []
  type: TYPE_NORMAL
- en: A Case Study
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous article, we explored the concept of the [Data Dirtiness Score](https://medium.com/p/fe2ca5678d40)
    using a data set example from the book [Cleaning Data for Effective Data Science](https://github.com/PacktPublishing/Cleaning-Data-for-Effective-Data-Science).
    The data set in question is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Data errors were already pointed out. Now, we want to explore how we can use
    a Large Language Model, specifically `GPT-4`, to automatically find these errors.
    This new method offers a modern way to spot issues in data sets but comes with
    possible risks such as privacy concerns when using external APIs. However, this
    can work with any LLMs, not just `GPT-4`, although the effectiveness might vary
    depending on the model's capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Preliminary Step: Retrieve Table Annotation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To assist the model in identifying data inconsistencies, it’s beneficial to
    provide additional context about the data frame. This is precisely the role of
    a [data catalog](https://www.datagalaxy.com/en/blog/what-is-a-data-catalog/),
    which, although a broad topic, we will simplify to focus solely on the essential
    context information that a LLM requires to detect data errors when examining batches
    of data set rows.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key metadata needed includes:'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of the **table**, including its **description and purpose**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A clear **understanding** of each **column’s meaning and type**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Given the frequent absence of data catalogs or reliable documentation in organisations,
    we’ll explore how to use LLMs to speed up this process. This process is known
    as *Table Annotation*, which involves identifying semantic information about table
    elements, including columns, their relationships, and the entities within the
    cells. For further details, refer to sources such as [Column Type Annotation using
    ChatGPT](https://arxiv.org/abs/2306.00745), [Annotating Columns with Pre-trained
    Language Models](https://paperswithcode.com/paper/annotating-columns-with-pre-trained-language),
    or [SOTAB: The WDC Schema.org Table Annotation Benchmark](https://paperswithcode.com/paper/sotab-the-wdc-schema-org-table-annotation).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the prompt I use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In the prompt instructions, I direct the model to analyse the provided table
    (or an overview of the table) and to suggest annotations following the [Schema.org](https://schema.org/)
    standards. Specifically, the output should include:'
  prefs: []
  type: TYPE_NORMAL
- en: The **table’s semantic type**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A brief **description** of each **column**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **column’s annotation type** from Schema.org, where applicable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ideal or best-suited **data types for each column**, regardless of data
    issues in the provided text serialisation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The response is then formatted to provide a clear and structured summary that
    can be reused as context in subsequent prompts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The table is incorporated using a `{table}` placeholder in the prompt. The
    typical method involves converting tabular data into text through serialisation,
    as discussed in [Large Language Models(LLMs) on Tabular Data: Prediction, Generation,
    and Understanding -- A Survey](https://arxiv.org/abs/2402.17944?utm_campaign=Data_Elixir&utm_source=Data_Elixir_475).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a sample response from `GPT-4`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Responses may vary slightly but are generally consistent for such a straightforward
    example. The aim here is to accelerate the initial process rather than fully automate
    it. Thus, this can be seen as a preliminary draft, which can then be refined with
    insights from our knowledge and external context from subject matter experts (SMEs).
  prefs: []
  type: TYPE_NORMAL
- en: Now, with some context about the table, let’s explore how to automatically identify
    data quality issues.
  prefs: []
  type: TYPE_NORMAL
- en: Sniffing Data Errors with LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To start, I suggest a prompt that will help identify data quality issues in
    a given table.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The initial part of the prompt sets the task’s objective and lists examples
    of common data issues, such as ingestion errors, duplicates, and privacy concerns,
    among others. **This list is not exhaustive, and you’re encouraged to add more
    relevant types based on your table’s context to guide the analysis.**
  prefs: []
  type: TYPE_NORMAL
- en: Next, the prompt details step-by-step instructions following a [Chain-of-Thoughts](https://learnprompting.org/docs/intermediate/chain_of_thought)
    approach, ensuring the model methodically analyses the table and its metadata
    before identifying data issues line by line, mirroring human analysis. This process
    is meant to be conducted without coding, to maintain simplicity and broad applicability.
    This is crucial because, although models like `GPT-4` with analytics capabilities
    can perform useful iterative coding sessions, relying solely on textual analysis
    promotes generalisation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Upon detecting a potential data issue, the prompt instructs documenting the
    following details:'
  prefs: []
  type: TYPE_NORMAL
- en: The **nature and description of the issue**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **expected correct state**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **violated constraint**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A **confidence level** in the assessment using ordinal categories: `low`, `medium`,
    `high` and `certain`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **specific location of the issue** in the table, using ‘None’ for table-wide
    issues, with Index and Column names for reference.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The table and its metadata are provided within the prompt, with an index added
    to each row to aid the model in pinpointing the exact locations of errors.
  prefs: []
  type: TYPE_NORMAL
- en: '*For large tables, this prompt can be applied in batches to cover the entire
    data set, with findings aggregated to identify all data quality issues.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Here is an example of the output this prompt can generate, formatted as a report
    detailing identified data issues, each with a description, expected state, violated
    constraint, confidence level, and location.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The next step is converting these identified issues into a Python object for
    easier calculation of the *Data Dirtiness Score*.
  prefs: []
  type: TYPE_NORMAL
- en: Converting Identified Data Issues into the Correct Format
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section focuses on transforming the previously identified data issues
    from plain text descriptions into Python objects. These objects should adhere
    to the structure defined by the `DataIssue` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the prompt I use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]python'
  prefs: []
  type: TYPE_NORMAL
- en: from dataclasses import dataclass
  prefs: []
  type: TYPE_NORMAL
- en: from typing import List, Tuple
  prefs: []
  type: TYPE_NORMAL
- en: '@dataclass'
  prefs: []
  type: TYPE_NORMAL
- en: 'class DataIssue:'
  prefs: []
  type: TYPE_NORMAL
- en: 'type_of_issue: str'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'expectation: str'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'constraint_violated: str'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'confidence_score: str # `low`, `medium`, `high` or `certain`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'location: List[Tuple]  # Cell positions as (Index, Column). Use None for row/column-wide
    issues.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Review all identified issues provided and their descriptions silently.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. For each issue, instantiate it using the provided `DataIssue` class structure.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Return only the code.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Once the code has been validated, stop generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Identified issues:'
  prefs: []
  type: TYPE_NORMAL
- en: '{issues_found}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Your code here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: issue1 = DataIssue(
  prefs: []
  type: TYPE_NORMAL
- en: type_of_issue="Incorrect value format",
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: expectation="Each cell under 'Favorite Color' should contain only one color",
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: constraint_violated="Single value constraint",
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: confidence_score="high",
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: location=[(1, "Favorite Color")]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: issue2 = DataIssue(
  prefs: []
  type: TYPE_NORMAL
- en: type_of_issue="Missing value",
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: expectation="No missing values in any columns",
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: constraint_violated="Non-null constraint",
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: confidence_score="certain",
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: location=[(2, "Favorite Color"), (5, "Last Name"), (5, "Favorite Color")]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: issue3 = DataIssue(
  prefs: []
  type: TYPE_NORMAL
- en: type_of_issue="Negative value",
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: expectation="Age values should be positive integers",
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: constraint_violated="Age value range (greater than 0)",
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: confidence_score="certain",
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: location=[(3, "Age")]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: issue4 = DataIssue(
  prefs: []
  type: TYPE_NORMAL
- en: type_of_issue="Misplaced values",
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: expectation="Age should be a reasonable positive integer, and favorite color
    should be a string denoting a color",
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: constraint_violated="Data type and value range constraint",
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: confidence_score="high",
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: location=[(4, "Favorite Color"), (4, "Age")]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: issue5 = DataIssue(
  prefs: []
  type: TYPE_NORMAL
- en: type_of_issue="Inconsistent formatting",
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: expectation="Each row should consistently follow the format defined by the column
    headers without extra delimiters",
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: constraint_violated="Data format and consistency constraint",
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: confidence_score="high",
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'location=[(5, None)]  # None indicates entire row issue'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: compute_data_dirtiness_score(data_issues)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Student#  Last Name  First Name  Favorite Color   Age
  prefs: []
  type: TYPE_NORMAL
- en: 0      0.00        0.0        0.00            0.00  0.00
  prefs: []
  type: TYPE_NORMAL
- en: 1      0.00        0.0        0.00            0.75  0.00
  prefs: []
  type: TYPE_NORMAL
- en: 2      0.00        0.0        0.00            1.00  0.00
  prefs: []
  type: TYPE_NORMAL
- en: 3      0.00        0.0        0.00            0.00  1.00
  prefs: []
  type: TYPE_NORMAL
- en: 4      0.00        0.0        0.00            0.75  0.75
  prefs: []
  type: TYPE_NORMAL
- en: 5      0.75        1.0        0.75            1.00  0.75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Student#  Last Name  First Name  Favorite Color   Age
  prefs: []
  type: TYPE_NORMAL
- en: 0       0.0        0.0        0.00          0.0000  0.00
  prefs: []
  type: TYPE_NORMAL
- en: 1       0.0        0.0        0.00          0.7500  0.00
  prefs: []
  type: TYPE_NORMAL
- en: 2       0.0        0.0        0.00          1.0000  0.00
  prefs: []
  type: TYPE_NORMAL
- en: 3       0.0        0.0        0.00          0.0000  1.00
  prefs: []
  type: TYPE_NORMAL
- en: 4       0.0        0.0        0.25          0.8125  0.75
  prefs: []
  type: TYPE_NORMAL
- en: 5       1.0        1.0        1.00          1.0000  1.00
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: create_mask_from_list_of_cell_positions(
  prefs: []
  type: TYPE_NORMAL
- en: shape=dataset_shape,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: list_of_cell_positions=[(4, 'Favorite Color'), (4, 'Age')],
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: columns=columns
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: array([[0, 0, 0, 0, 0],
  prefs: []
  type: TYPE_NORMAL
- en: '[0, 0, 0, 0, 0],'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[0, 0, 0, 0, 0],'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[0, 0, 0, 0, 0],'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[0, 0, 0, 1, 1],'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[0, 0, 0, 0, 0]], dtype=int8)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: def validate_cell_position(
  prefs: []
  type: TYPE_NORMAL
- en: 'cell_position: Union['
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Tuple[int, int], Tuple[None, int], Tuple[int, None], Tuple[None, None]
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '],'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'columns: List[str] = None,'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ') -> Tuple[int, int]:'
  prefs: []
  type: TYPE_NORMAL
- en: '"""'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Validate the cell position and convert column names to indices if necessary.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"""'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'if not isinstance(cell_position, tuple):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: raise ValueError("Cell position must be a tuple")
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Convert column name to index if columns are provided
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: 'if isinstance(cell_position[1], str):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'if columns is None:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: raise ValueError(
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '"Column names must be provided to create a mask based on column names"'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: column_index = columns.index(cell_position[1])
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: return (cell_position[0], column_index)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: return cell_position
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'def set_mask_values(mask: np.ndarray, cell_position: Tuple[int, int]):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Set values in the mask based on the cell position.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"""'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: row_index, col_index = cell_position
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'if row_index is None:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: mask[:, col_index] = 1
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'elif col_index is None:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: mask[row_index, :] = 1
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'else:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: mask[row_index, col_index] = 1
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: def create_mask_from_list_of_cell_positions(
  prefs: []
  type: TYPE_NORMAL
- en: 'shape: Tuple[int, int],'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'list_of_cell_positions: List[Tuple],'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'columns: List[str] = None,'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ') -> np.ndarray:'
  prefs: []
  type: TYPE_NORMAL
- en: '"""'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Create a mask array based on a list of cell positions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"""'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: mask = np.zeros(shape=shape, dtype=np.int8)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'for cell_position in list_of_cell_positions:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: validated_position = validate_cell_position(cell_position, columns)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: set_mask_values(mask, validated_position)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: return mask
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
