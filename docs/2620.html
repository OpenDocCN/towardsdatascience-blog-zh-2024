<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Running the STORM AI Research System with Your Local Documents</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Running the STORM AI Research System with Your Local Documents</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/running-the-storm-ai-research-system-with-your-local-documents-e413ea2ae064?source=collection_archive---------3-----------------------#2024-10-28">https://towardsdatascience.com/running-the-storm-ai-research-system-with-your-local-documents-e413ea2ae064?source=collection_archive---------3-----------------------#2024-10-28</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="123f" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">AI assisted research using FEMA disaster response documents</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@astrobagel?source=post_page---byline--e413ea2ae064--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Matthew Harris" class="l ep by dd de cx" src="../Images/4fa3264bb8a028633cd8d37093c16214.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*SQpPIBppBtQGfoSP_sAeaQ.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--e413ea2ae064--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@astrobagel?source=post_page---byline--e413ea2ae064--------------------------------" rel="noopener follow">Matthew Harris</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--e413ea2ae064--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">16 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 28, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/451f325bbb6fd2a154c72081210f8eb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-bEGUSXk_hy65iRrdBJdaQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">STORM researches the topic via perspective-guided question asking in simulated conversations. <a class="af nc" href="https://arxiv.org/abs/2402.14207" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="5463" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">TL;DR</p><p id="08a6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="nz">The use of LLM agents is becoming more common for tackling multi-step long-context research tasks where traditional RAG direct prompting methods can sometimes struggle. In this article, we will explore a new and promising technique developed by Stanford called </em><strong class="nf fr"><em class="nz">S</em></strong><em class="nz">ynthesis of </em><strong class="nf fr"><em class="nz">T</em></strong><em class="nz">opic </em><strong class="nf fr"><em class="nz">O</em></strong><em class="nz">utlines through </em><strong class="nf fr"><em class="nz">R</em></strong><em class="nz">etrieval and </em><strong class="nf fr"><em class="nz">M</em></strong><em class="nz">ulti-perspective Question Asking (</em><a class="af nc" href="https://arxiv.org/abs/2402.14207" rel="noopener ugc nofollow" target="_blank"><em class="nz">STORM</em></a><em class="nz">), which uses LLM agents to simulate ‘Perspective-guided conversations’ to reach complex research goals and generate rich research articles that can be used by humans in their pre-writing research. STORM was initially developed to gather information from web sources but also supports searching a local document vector store. In this article we will see how to implement STORM for AI-supported research on local PDFs, using US FEMA disaster preparedness and assistance documentation.</em></p><p id="484d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">It’s been amazing to watch how using LLMs for knowledge retrieval has progressed in a relatively short period of time. Since the <a class="af nc" href="https://arxiv.org/abs/2005.11401" rel="noopener ugc nofollow" target="_blank">first paper on Retrieval Augmented Generation (RAG)</a> in 2020, we have seen the ecosystem grow to include a <a class="af nc" href="https://arxiv.org/html/2312.10997v5#S2" rel="noopener ugc nofollow" target="_blank">cornucopia of available technique</a>s. One of the more advanced is agentic RAG where LLM agents iterate and refine document retrieval in order to solve more complex research tasks. It’s similar to how a human might carry out research, exploring a range of different search queries to build a better idea of the context, sometimes discussing the topic with other humans, and synthesizing everything into a final result. Single-turn RAG, even employing techniques such as query expansion and reranking, can struggle with more complex multi-hop research tasks like this.</p><p id="c675" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">There are quite a few patterns for knowledge retrieval using agent frameworks such as <a class="af nc" href="https://microsoft.github.io/autogen/0.2/" rel="noopener ugc nofollow" target="_blank">Autogen</a>, <a class="af nc" href="https://www.crewai.com" rel="noopener ugc nofollow" target="_blank">CrewAI</a>, and <a class="af nc" href="https://www.langchain.com/langgraph" rel="noopener ugc nofollow" target="_blank">LangGraph</a> as well as specific AI research assistants such as <a class="af nc" href="https://github.com/assafelovic/gpt-researcher" rel="noopener ugc nofollow" target="_blank">GPT Researcher</a>. In this article, we will look at an LLM-powered research writing system from Stanford University, called <strong class="nf fr">S</strong>ynthesis of <strong class="nf fr">T</strong>opic <strong class="nf fr">O</strong>utlines through <strong class="nf fr">R</strong>etrieval and <strong class="nf fr">M</strong>ulti-perspective Question Asking (<a class="af nc" href="https://arxiv.org/abs/2402.14207" rel="noopener ugc nofollow" target="_blank">STORM</a>).</p><h1 id="e30d" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">STORM AI research writing system</h1><p id="6d22" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">STORM applies a clever technique where LLM agents simulate ‘Perspective-guided conversations’ to reach a research goal as well as extend ‘outline-driven RAG’ for richer article generation.</p><p id="0d13" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Configured to generate Wikipedia-style articles, it was tested with a cohort of 10 experienced Wikipedia editors.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pb"><img src="../Images/0e52360332e7847eb9aa2237a73652ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CwZ-VinbYJ7iGYq5MTFBSw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Survey results of 10 experienced Wikipedia Editors on the perceived usefulness of STORM. <a class="af nc" href="https://arxiv.org/abs/2402.14207" rel="noopener ugc nofollow" target="_blank">Source</a>.</figcaption></figure><p id="ae16" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Reception on the whole was positive, 70% of the editors felt that it would be a useful tool in their <em class="nz">pre-writing</em> stage when researching a topic. I hope in the future surveys could include more than 10 editors, but it should be noted that authors also benchmarked traditional article generation methods using FreshWiki, a dataset of recent high-quality Wikipedia articles, where STORM was found to outperform previous techniques.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pc"><img src="../Images/4076dab0bb937a746d51e0eb93b03985.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DwY_paVsJMy2GO1Ym3ot1A.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Human evaluation by 10 experienced Wikipedia editors for on 20 pairs of articles generated by STORM and <em class="pd">oRAG</em>. Each pair of articles is evaluated by two Wikipedia editors. <a class="af nc" href="https://arxiv.org/abs/2402.14207" rel="noopener ugc nofollow" target="_blank">Source</a>.</figcaption></figure><p id="2fda" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">STORM is <a class="af nc" href="https://github.com/stanford-oval/storm/tree/main" rel="noopener ugc nofollow" target="_blank">open source</a> and available as a <a class="af nc" href="https://pypi.org/project/knowledge-storm/" rel="noopener ugc nofollow" target="_blank">Python package</a> with additional implementations using frameworks such as <a class="af nc" href="https://langchain-ai.github.io/langgraph/tutorials/storm/storm/" rel="noopener ugc nofollow" target="_blank">LangGraph</a>. More recently STORM has been enhanced to support human-AI collaborative knowledge curation called <a class="af nc" href="https://www.arxiv.org/abs/2408.15232" rel="noopener ugc nofollow" target="_blank">Co-STORM</a>, putting a human right in the center of the AI-assisted research loop.</p><p id="fd91" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Though it significantly outperforms baseline methods in both automatic and human evaluations, there are some caveats that the authors acknowledge. It isn’t yet multimodal, doesn’t produce experienced human-quality content — it isn’t positioned yet for this I feel, being more targeted for <em class="nz">pre-writing </em>research than final articles — and there are some nuances around references that require some future work. That said, if you have a deep research task, it’s worth checking out.</p><p id="0879" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">You can try out STORM <a class="af nc" href="https://storm.genie.stanford.edu/" rel="noopener ugc nofollow" target="_blank">online</a> — it’s fun! — configured to perform research using information on the web.</p><h1 id="bf33" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk"><strong class="al">But what about running STORM with your own data?</strong></h1><p id="0280" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">Many organizations will want to use AI research tools with their own internal data. The STORM authors have done a nice job of documenting various approaches of using STORM with different LLM providers and a local vector database, which means it is possible to run STORM on your own documents.</p><p id="48cd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">So let’s try this out!</p><h1 id="aad7" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Setup and code</h1><p id="4a0c" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">You can find the code for this article <a class="af nc" href="https://github.com/dividor/storm-with-local-docs" rel="noopener ugc nofollow" target="_blank">here</a>, which includes environment setup instructions and how to collate some sample documents for this demo.</p><h1 id="9e4d" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">FEMA disaster preparedness and assistance documentation</h1><p id="9542" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">We will use 34 PDF documents to help people prepare for and respond to disasters, as created by the United States Federal Emergency Management Agency (<a class="af nc" href="https://www.fema.gov" rel="noopener ugc nofollow" target="_blank">FEMA</a>). These documents perhaps aren’t typically what people may want to use for writing deep research articles, but I’m interested in seeing how AI can help people prepare for disasters.</p><p id="fd4f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">…. and I have the code already written for processing FEMA reports from some earlier blog posts, which I’ve included in the linked repo above. 😊</p><h1 id="840d" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Parsing and Chunking</h1><p id="168a" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">Once we have our documents, we need to split them into smaller documents so that STORM can search for specific topics within the corpus. Given STORM is originally aimed at generating Wikipedia-style articles, I opted to try two approaches, (i) Simply splitting the documents into sub-documents by page using <a class="af nc" href="https://python.langchain.com/docs/integrations/document_loaders/pypdfloader/" rel="noopener ugc nofollow" target="_blank">LangChain’s PyPDFLoader</a>, to create a crude simulation of a Wikipedia page which includes several sub-topics. Many FEMA PDFs are single-page documents that don’t look too dissimilar to Wikipedia articles; (ii) Further chunking the documents into smaller sections, more likely to cover a discrete sub-topic.</p><p id="7ab7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">These are of course <em class="nz">very</em> basic approaches to parsing, but I wanted to see how results varied depending on the two techniques. Any serious use of STORM on local documents should invest in all the usual fun around paring optimization.</p><pre class="mm mn mo mp mq pe pf pg bp ph bb bk"><span id="dc01" class="pi ob fq pf b bg pj pk l pl pm">def parse_pdfs():<br/>    """<br/>    Parses all PDF files in the specified directory and loads their content.<br/><br/>    This function iterates through all files in the directory specified by PDF_DIR,<br/>    checks if they have a .pdf extension, and loads their content using PyPDFLoader.<br/>    The loaded content from each PDF is appended to a list which is then returned.<br/><br/>    Returns:<br/>        list: A list containing the content of all loaded PDF documents.<br/>    """<br/>    docs = []<br/>    pdfs = os.listdir(PDF_DIR)<br/>    print(f"We have {len(pdfs)} pdfs")<br/>    for pdf_file in pdfs:<br/>        if not pdf_file.endswith(".pdf"):<br/>            continue<br/>        print(f"Loading PDF: {pdf_file}")<br/>        file_path = f"{PDF_DIR}/{pdf_file}"<br/>        loader = PyPDFLoader(file_path)<br/>        docs = docs + loader.load()<br/>        print(f"Loaded {len(docs)} documents")<br/><br/>    return docs<br/><br/><br/>docs = parse_pdfs()<br/><br/>text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)<br/>chunks = text_splitter.split_documents(docs)</span></pre><h1 id="7c87" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Metadata enrichment</h1><p id="9cf1" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk"><a class="af nc" href="https://github.com/stanford-oval/storm/blob/main/examples/storm_examples/README.md" rel="noopener ugc nofollow" target="_blank">STORM’s example documentation</a> requires that documents have metadata fields ‘URL’, ‘title’, and ‘description’, where ‘URL’ should be unique. Since we are splitting up PDF documents, we don’t have titles and descriptions of individual pages and chunks, so I opted to generate these with a simple LLM call.</p><p id="3809" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For URLs, we have them for individual PDF pages, but for chunks within a page. Sophisticated knowledge retrieval systems can have metadata generated by layout detection models so the text chunk area can be highlighted in the corresponding PDF, but for this demo, I simply added an ‘_id’ query parameter the URL which does nothing but ensure they are unique for chunks.</p><pre class="mm mn mo mp mq pe pf pg bp ph bb bk"><span id="9674" class="pi ob fq pf b bg pj pk l pl pm">def summarize_text(text, prompt):<br/>    """<br/>    Generate a summary of some text based on the user's prompt<br/><br/>    Args:<br/><br/>    text (str) - the text to analyze<br/>    prompt (str) - prompt instruction on how to summarize the text, eg 'generate a title'<br/><br/>    Returns:<br/><br/>    summary (text) - LLM-generated summary<br/><br/>    """<br/>    messages = [<br/>        (<br/>            "system",<br/>            "You are an assistant that gives very brief single sentence description of text.",<br/>        ),<br/>        ("human", f"{prompt} :: \n\n {text}"),<br/>    ]<br/>    ai_msg = llm.invoke(messages)<br/>    summary = ai_msg.content<br/>    return summary<br/><br/><br/>def enrich_metadata(docs):<br/>    """<br/>    Uses an LLM to populate 'title' and 'description' for text chunks<br/><br/>    Args:<br/><br/>    docs (list) - list of LangChain documents<br/><br/>    Returns:<br/><br/>    docs (list) - list of LangChain documents with metadata fields populated<br/><br/>    """<br/>    new_docs = []<br/>    for doc in docs:<br/><br/>        # pdf name is last part of doc.metadata['source']<br/>        pdf_name = doc.metadata["source"].split("/")[-1]<br/><br/>        # Find row in df where pdf_name is in URL<br/>        row = df[df["Document"].str.contains(pdf_name)]<br/>        page = doc.metadata["page"] + 1<br/>        url = f"{row['Document'].values[0]}?id={str(uuid4())}#page={page}"<br/><br/>        # We'll use an LLM to generate a summary and title of the text, used by STORM<br/>        # This is just for the demo, proper application would have better metadata<br/>        summary = summarize_text(doc.page_content, prompt="Please describe this text:")<br/>        title = summarize_text(<br/>            doc.page_content, prompt="Please generate a 5 word title for this text:"<br/>        )<br/><br/>        doc.metadata["description"] = summary<br/>        doc.metadata["title"] = title<br/>        doc.metadata["url"] = url<br/>        doc.metadata["content"] = doc.page_content<br/><br/>        # print(json.dumps(doc.metadata, indent=2))<br/>        new_docs.append(doc)<br/><br/>    print(f"There are {len(docs)} docs")<br/><br/>    return new_docs<br/><br/><br/>docs = enrich_metadata(docs)<br/>chunks = enrich_metadata(chunks)</span></pre><h1 id="75f5" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Building vector databases</h1><p id="8d6d" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">STORM already supports the <a class="af nc" href="https://www.google.com/url?sa=t&amp;rct=j&amp;opi=89978449&amp;url=https%3A%2F%2Fqdrant.tech%2F&amp;ved=2ahUKEwiHuK-_766JAxXutokEHbnUMhwQFnoECAgQAQ&amp;usg=AOvVaw1SKthNlGkmNDis3BK1WPSq" rel="noopener ugc nofollow" target="_blank">Qdrant vector store</a>. I like to use frameworks such as LangChain and Llama Index where possible to make it easier to change providers down the road, so I opted to use LangChain to build a <a class="af nc" href="https://python.langchain.com/docs/integrations/vectorstores/qdrant/#local-mode" rel="noopener ugc nofollow" target="_blank">local Qdrant vector database persisted to the local file system</a> rather than STORM’s automatic vector database management. I felt this offers more control and is more recognizable to those who already have pipelines for populating document vector stores.</p><pre class="mm mn mo mp mq pe pf pg bp ph bb bk"><span id="e9d4" class="pi ob fq pf b bg pj pk l pl pm">def build_vector_store(doc_type, docs):<br/>    """<br/>    Givena  list of LangChain docs, will embed and create a file-system Qdrant vector database.<br/>    The folder includes doc_type in its name to avoid overwriting.<br/><br/>    Args:<br/><br/>    doc_type (str) - String to indicate level of document split, eg 'pages',<br/>                     'chunks'. Used to name the database save folder<br/>    docs (list) - List of langchain documents to embed and store in vector database<br/><br/>    Returns:<br/><br/>    Nothing returned by function, but db saved to f"{DB_DIR}_{doc_type}".<br/><br/>    """<br/><br/>    print(f"There are {len(docs)} docs")<br/><br/>    save_dir = f"{DB_DIR}_{doc_type}"<br/><br/>    print(f"Saving vectors to directory {save_dir}")<br/><br/>    client = QdrantClient(path=save_dir)<br/><br/>    client.create_collection(<br/>        collection_name=DB_COLLECTION_NAME,<br/>        vectors_config=VectorParams(size=num_vectors, distance=Distance.COSINE),<br/>    )<br/><br/>    vector_store = QdrantVectorStore(<br/>        client=client,<br/>        collection_name=DB_COLLECTION_NAME,<br/>        embedding=embeddings,<br/>    )<br/><br/>    uuids = [str(uuid4()) for _ in range(len(docs))]<br/><br/>    vector_store.add_documents(documents=docs, ids=uuids)<br/><br/><br/>build_vector_store("pages", docs)<br/>build_vector_store("chunks", docs)</span></pre><h1 id="f3b5" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Running STORM</h1><p id="5f87" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">The STORM repo has <a class="af nc" href="https://github.com/stanford-oval/storm/blob/main/examples/storm_examples/README.md" rel="noopener ugc nofollow" target="_blank">some great examples</a> of different search engines and LLMs, as well as using a Qdrant vector store. I decided to combine various features from these, plus some extra post-processing as follows:</p><ol class=""><li id="a3be" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pn po pp bk">Added ability to run with OpenAI or Ollama</li><li id="6be7" class="nd ne fq nf b go pq nh ni gr pr nk nl nm ps no np nq pt ns nt nu pu nw nx ny pn po pp bk">Added support for passing in the vector database directory</li><li id="f5d6" class="nd ne fq nf b go pq nh ni gr pr nk nl nm ps no np nq pt ns nt nu pu nw nx ny pn po pp bk">Added a function to parse the references metadata file to add references to the generated polished article. STORM generated these references in a JSON file but didn’t add them to the output article automatically. I’m not sure if this was due to some setting I missed, but references are key to evaluating any AI research technique, so I added this custom post-processing step.</li><li id="6e44" class="nd ne fq nf b go pq nh ni gr pr nk nl nm ps no np nq pt ns nt nu pu nw nx ny pn po pp bk">Finally, I noticed that open models have more guidance in templates and personas due to their following instructions less accurately than commercial models. I liked the transparency of these controls and left them in for OpenAI so that I could adjust in future work.</li></ol><p id="1018" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Here is everything (see <a class="af nc" href="https://github.com/dividor/storm-with-local-docs/blob/main/storm-local-docs.ipynb" rel="noopener ugc nofollow" target="_blank">repo notebook</a> for full code) …</p><pre class="mm mn mo mp mq pe pf pg bp ph bb bk"><span id="d682" class="pi ob fq pf b bg pj pk l pl pm">def set_instructions(runner):<br/>    """<br/>    Adjusts templates and personas for the STORM AI Research algorithm.<br/><br/>    Args:<br/><br/>    runner - STORM runner object<br/><br/>    Returns:<br/><br/>    runner - STORM runner object with extra prompting<br/><br/>    """<br/><br/>    # Open LMs are generally weaker in following output format.<br/>    # One way for mitigation is to add one-shot example to the prompt to exemplify the desired output format.<br/>    # For example, we can add the following examples to the two prompts used in StormPersonaGenerator.<br/>    # Note that the example should be an object of dspy.Example with fields matching the InputField<br/>    # and OutputField in the prompt (i.e., dspy.Signature).<br/>    find_related_topic_example = Example(<br/>        topic="Knowledge Curation",<br/>        related_topics="https://en.wikipedia.org/wiki/Knowledge_management\n"<br/>        "https://en.wikipedia.org/wiki/Information_science\n"<br/>        "https://en.wikipedia.org/wiki/Library_science\n",<br/>    )<br/>    gen_persona_example = Example(<br/>        topic="Knowledge Curation",<br/>        examples="Title: Knowledge management\n"<br/>        "Table of Contents: History\nResearch\n  Dimensions\n  Strategies\n  Motivations\nKM technologies"<br/>        "\nKnowledge barriers\nKnowledge retention\nKnowledge audit\nKnowledge protection\n"<br/>        "  Knowledge protection methods\n    Formal methods\n    Informal methods\n"<br/>        "  Balancing knowledge protection and knowledge sharing\n  Knowledge protection risks",<br/>        personas="1. Historian of Knowledge Systems: This editor will focus on the history and evolution of knowledge curation. They will provide context on how knowledge curation has changed over time and its impact on modern practices.\n"<br/>        "2. Information Science Professional: With insights from 'Information science', this editor will explore the foundational theories, definitions, and philosophy that underpin knowledge curation\n"<br/>        "3. Digital Librarian: This editor will delve into the specifics of how digital libraries operate, including software, metadata, digital preservation.\n"<br/>        "4. Technical expert: This editor will focus on the technical aspects of knowledge curation, such as common features of content management systems.\n"<br/>        "5. Museum Curator: The museum curator will contribute expertise on the curation of physical items and the transition of these practices into the digital realm.",<br/>    )<br/>    runner.storm_knowledge_curation_module.persona_generator.create_writer_with_persona.find_related_topic.demos = [<br/>        find_related_topic_example<br/>    ]<br/>    runner.storm_knowledge_curation_module.persona_generator.create_writer_with_persona.gen_persona.demos = [<br/>        gen_persona_example<br/>    ]<br/><br/>    # A trade-off of adding one-shot example is that it will increase the input length of the prompt. Also, some<br/>    # examples may be very long (e.g., an example for writing a section based on the given information), which may<br/>    # confuse the model. For these cases, you can create a pseudo-example that is short and easy to understand to steer<br/>    # the model's output format.<br/>    # For example, we can add the following pseudo-examples to the prompt used in WritePageOutlineFromConv and<br/>    # ConvToSection.<br/>    write_page_outline_example = Example(<br/>        topic="Example Topic",<br/>        conv="Wikipedia Writer: ...\nExpert: ...\nWikipedia Writer: ...\nExpert: ...",<br/>        old_outline="# Section 1\n## Subsection 1\n## Subsection 2\n"<br/>        "# Section 2\n## Subsection 1\n## Subsection 2\n"<br/>        "# Section 3",<br/>        outline="# New Section 1\n## New Subsection 1\n## New Subsection 2\n"<br/>        "# New Section 2\n"<br/>        "# New Section 3\n## New Subsection 1\n## New Subsection 2\n## New Subsection 3",<br/>    )<br/>    runner.storm_outline_generation_module.write_outline.write_page_outline.demos = [<br/>        write_page_outline_example<br/>    ]<br/>    write_section_example = Example(<br/>        info="[1]\nInformation in document 1\n[2]\nInformation in document 2\n[3]\nInformation in document 3",<br/>        topic="Example Topic",<br/>        section="Example Section",<br/>        output="# Example Topic\n## Subsection 1\n"<br/>        "This is an example sentence [1]. This is another example sentence [2][3].\n"<br/>        "## Subsection 2\nThis is one more example sentence [1].",<br/>    )<br/>    runner.storm_article_generation.section_gen.write_section.demos = [<br/>        write_section_example<br/>    ]<br/><br/>    return runner<br/><br/>def latest_dir(parent_folder):<br/>    """<br/>    Find the most recent folder (by modified date) in the specified parent folder.<br/><br/>    Args:<br/>        parent_folder (str): The path to the parent folder where the search for the most recent folder will be conducted. Defaults to f"{DATA_DIR}/storm_output".<br/><br/>    Returns:<br/>        str: The path to the most recently modified folder within the parent folder.<br/>    """<br/>    # Find most recent folder (by modified date) in DATA_DIR/storm_data<br/>    # TODO, find out how exactly storm passes back its output directory to avoid this hack<br/>    folders = [f.path for f in os.scandir(parent_folder) if f.is_dir()]<br/>    folder = max(folders, key=os.path.getmtime)<br/><br/>    return folder<br/><br/><br/>def generate_footnotes(folder):<br/>    """<br/>    Generates footnotes from a JSON file containing URL information.<br/><br/>    Args:<br/>        folder (str): The directory path where the 'url_to_info.json' file is located.<br/><br/>    Returns:<br/>        str: A formatted string containing footnotes with URLs and their corresponding titles.<br/>    """<br/><br/>    file = f"{folder}/url_to_info.json"<br/><br/>    with open(file) as f:<br/>        data = json.load(f)<br/><br/>    refs = {}<br/>    for rec in data["url_to_unified_index"]:<br/>        val = data["url_to_unified_index"][rec]<br/>        title = data["url_to_info"][rec]["title"].replace('"', "")<br/>        refs[val] = f"- {val} [{title}]({rec})"<br/><br/>    keys = list(refs.keys())<br/>    keys.sort()<br/><br/>    footer = ""<br/>    for key in keys:<br/>        footer += f"{refs[key]}\n"<br/><br/>    return footer, refs<br/><br/><br/>def generate_markdown_article(output_dir):<br/>    """<br/>    Generates a markdown article by reading a text file, appending footnotes, <br/>    and saving the result as a markdown file.<br/><br/>    The function performs the following steps:<br/>    1. Retrieves the latest directory using the `latest_dir` function.<br/>    2. Generates footnotes for the article using the `generate_footnotes` function.<br/>    3. Reads the content of a text file named 'storm_gen_article_polished.txt' <br/>       located in the latest directory.<br/>    4. Appends the generated footnotes to the end of the article content.<br/>    5. Writes the modified content to a new markdown file named <br/>       STORM_OUTPUT_MARKDOWN_ARTICLE in the same directory.<br/><br/>    Args:<br/><br/>    output_dir (str) - The directory where the STORM output is stored.<br/><br/><br/>    """<br/><br/>    folder = latest_dir(output_dir)<br/>    footnotes, refs = generate_footnotes(folder)<br/><br/>    with open(f"{folder}/storm_gen_article_polished.txt") as f:<br/>        text = f.read()<br/><br/>    # Update text references like [10] to link to URLs<br/>    for ref in refs:<br/>        print(f"Ref: {ref}, Ref_num: {refs[ref]}")<br/>        url = refs[ref].split("(")[1].split(")")[0]<br/>        text = text.replace(f"[{ref}]", f"\[[{ref}]({url})\]")<br/><br/>    text += f"\n\n## References\n\n{footnotes}"<br/><br/>    with open(f"{folder}/{STORM_OUTPUT_MARKDOWN_ARTICLE}", "w") as f:<br/>        f.write(text)<br/><br/>def run_storm(topic, model_type, db_dir):<br/>    """<br/>    This function runs the STORM AI Research algorithm using data<br/>    in a QDrant local database.<br/><br/>    Args:<br/><br/>    topic (str) - The research topic to generate the article for<br/>    model_type (str) - One of 'openai' and 'ollama' to control LLM used<br/>    db_dir (str) - Directory where the QDrant vector database is<br/>    <br/>    """<br/>    if model_type not in ["openai", "ollama"]:<br/>        print("Unsupported model_type")<br/>        sys.exit()<br/><br/>    # Clear lock so can be read<br/>    if os.path.exists(f"{db_dir}/.lock"):<br/>        print(f"Removing lock file {db_dir}/.lock")<br/>        os.remove(f"{db_dir}/.lock")<br/><br/>    print(f"Loading Qdrant vector store from {db_dir}")<br/><br/>    engine_lm_configs = STORMWikiLMConfigs()<br/><br/>    if model_type == "openai":<br/><br/>        print("Using OpenAI models")<br/><br/>        # Initialize the language model configurations<br/>        openai_kwargs = {<br/>            "api_key": os.getenv("OPENAI_API_KEY"),<br/>            "temperature": 1.0,<br/>            "top_p": 0.9,<br/>        }<br/><br/>        ModelClass = (<br/>            OpenAIModel<br/>            if os.getenv("OPENAI_API_TYPE") == "openai"<br/>            else AzureOpenAIModel<br/>        )<br/>        # If you are using Azure service, make sure the model name matches your own deployed model name.<br/>        # The default name here is only used for demonstration and may not match your case.<br/>        gpt_35_model_name = (<br/>            "gpt-4o-mini"<br/>            if os.getenv("OPENAI_API_TYPE") == "openai"<br/>            else "gpt-35-turbo"<br/>        )<br/>        gpt_4_model_name = "gpt-4o"<br/>        if os.getenv("OPENAI_API_TYPE") == "azure":<br/>            openai_kwargs["api_base"] = os.getenv("AZURE_API_BASE")<br/>            openai_kwargs["api_version"] = os.getenv("AZURE_API_VERSION")<br/><br/>        # STORM is a LM system so different components can be powered by different models.<br/>        # For a good balance between cost and quality, you can choose a cheaper/faster model for conv_simulator_lm<br/>        # which is used to split queries, synthesize answers in the conversation. We recommend using stronger models<br/>        # for outline_gen_lm which is responsible for organizing the collected information, and article_gen_lm<br/>        # which is responsible for generating sections with citations.<br/>        conv_simulator_lm = ModelClass(<br/>            model=gpt_35_model_name, max_tokens=10000, **openai_kwargs<br/>        )<br/>        question_asker_lm = ModelClass(<br/>            model=gpt_35_model_name, max_tokens=10000, **openai_kwargs<br/>        )<br/>        outline_gen_lm = ModelClass(<br/>            model=gpt_4_model_name, max_tokens=10000, **openai_kwargs<br/>        )<br/>        article_gen_lm = ModelClass(<br/>            model=gpt_4_model_name, max_tokens=10000, **openai_kwargs<br/>        )<br/>        article_polish_lm = ModelClass(<br/>            model=gpt_4_model_name, max_tokens=10000, **openai_kwargs<br/>        )<br/><br/>    elif model_type == "ollama":<br/><br/>        print("Using Ollama models")<br/><br/>        ollama_kwargs = {<br/>            # "model": "llama3.2:3b",<br/>            "model": "llama3.1:latest",<br/>            # "model": "qwen2.5:14b",<br/>            "port": "11434",<br/>            "url": "http://localhost",<br/>            "stop": (<br/>                "\n\n---",<br/>            ),  # dspy uses "\n\n---" to separate examples. Open models sometimes generate this.<br/>        }<br/><br/>        conv_simulator_lm = OllamaClient(max_tokens=500, **ollama_kwargs)<br/>        question_asker_lm = OllamaClient(max_tokens=500, **ollama_kwargs)<br/>        outline_gen_lm = OllamaClient(max_tokens=400, **ollama_kwargs)<br/>        article_gen_lm = OllamaClient(max_tokens=700, **ollama_kwargs)<br/>        article_polish_lm = OllamaClient(max_tokens=4000, **ollama_kwargs)<br/><br/>    engine_lm_configs.set_conv_simulator_lm(conv_simulator_lm)<br/>    engine_lm_configs.set_question_asker_lm(question_asker_lm)<br/>    engine_lm_configs.set_outline_gen_lm(outline_gen_lm)<br/>    engine_lm_configs.set_article_gen_lm(article_gen_lm)<br/>    engine_lm_configs.set_article_polish_lm(article_polish_lm)<br/><br/>    max_conv_turn = 4<br/>    max_perspective = 3<br/>    search_top_k = 10<br/>    max_thread_num = 1<br/>    device = "cpu"<br/>    vector_db_mode = "offline"<br/><br/>    do_research = True<br/>    do_generate_outline = True<br/>    do_generate_article = True<br/>    do_polish_article = True<br/><br/>    # Initialize the engine arguments<br/>    output_dir=f"{STORM_OUTPUT_DIR}/{db_dir.split('db_')[1]}"<br/>    print(f"Output directory: {output_dir}")<br/><br/>    engine_args = STORMWikiRunnerArguments(<br/>        output_dir=output_dir,<br/>        max_conv_turn=max_conv_turn,<br/>        max_perspective=max_perspective,<br/>        search_top_k=search_top_k,<br/>        max_thread_num=max_thread_num,<br/>    )<br/><br/>    # Setup VectorRM to retrieve information from your own data<br/>    rm = VectorRM(<br/>        collection_name=DB_COLLECTION_NAME,<br/>        embedding_model=EMBEDDING_MODEL,<br/>        device=device,<br/>        k=search_top_k,<br/>    )<br/><br/>    # initialize the vector store, either online (store the db on Qdrant server) or offline (store the db locally):<br/>    if vector_db_mode == "offline":<br/>        rm.init_offline_vector_db(vector_store_path=db_dir)<br/><br/>    # Initialize the STORM Wiki Runner<br/>    runner = STORMWikiRunner(engine_args, engine_lm_configs, rm)<br/><br/>    # Set instructions for the STORM AI Research algorithm<br/>    runner = set_instructions(runner)<br/><br/>    # run the pipeline<br/>    runner.run(<br/>        topic=topic,<br/>        do_research=do_research,<br/>        do_generate_outline=do_generate_outline,<br/>        do_generate_article=do_generate_article,<br/>        do_polish_article=do_polish_article,<br/>    )<br/>    runner.post_run()<br/>    runner.summary()<br/><br/>    generate_markdown_article(output_dir)</span></pre><p id="230f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We’re ready to run STORM!</p><p id="5774" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For the research topic, I picked something that would be challenging to answer with a typical RAG system and which wasn’t well covered in the PDF data so we can see how well attribution works …</p><p id="f19d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">“<strong class="nf fr"><em class="nz">Compare the financial impact of different types of disasters and how those impact communities</em></strong>”</p><p id="8bdb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Running this for both databases …</p><pre class="mm mn mo mp mq pe pf pg bp ph bb bk"><span id="dc75" class="pi ob fq pf b bg pj pk l pl pm">query = "Compare the financial impact of different types of disasters and how those impact communities"<br/><br/>for doc_type in ["pages", "chunks"]:<br/>    db_dir = f"{DB_DIR}_{doc_type}"<br/>    run_storm(query=query, model_type="openai", db_dir=db_dir)</span></pre><p id="52e1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Using OpenAI, the process took about 6 minutes on my Macbook pro M2 (16GB memory). I would note that other simpler queries where we have more supporting content in the underlying documents were much faster (&lt; 30 seconds in some cases).</p><h1 id="8fad" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">STORM results</h1><p id="7fa0" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">STORM generates a set of output files …</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pv"><img src="../Images/9352963a2332997ba080d259b2f9151b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pu-04El6CKT02AwkTVEY4Q.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Files generated by STORM, from which one markdown file was created combining the polished article with reference footnotes.</figcaption></figure><p id="1372" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">It’s interesting to review the <strong class="nf fr">conversation_log.json</strong> and <strong class="nf fr">llm_call_history.json</strong> to see the perspective-guided conversations component.</p><p id="5afe" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For our research topic …</p><p id="f783" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">“<strong class="nf fr"><em class="nz">Compare the financial impact of different types of disasters and how those impact communities</em></strong>”</p><p id="4138" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">You can find the generated articles here …</p><ul class=""><li id="288d" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pw po pp bk"><a class="af nc" href="https://github.com/dividor/storm-with-local-docs/blob/main/data/storm_output/pages/Compare_the_financial_impact_of_different_types_of_disasters_and_how_those_impact_communities/storm_gen_article_polished.md" rel="noopener ugc nofollow" target="_blank">STORM generated article — using text split by page</a></li><li id="83fa" class="nd ne fq nf b go pq nh ni gr pr nk nl nm ps no np nq pt ns nt nu pu nw nx ny pw po pp bk"><a class="af nc" href="https://github.com/dividor/storm-with-local-docs/blob/main/data/storm_output/chunks/Compare_the_financial_impact_of_different_types_of_disasters_and_how_those_impact_communities/storm_gen_article_polished.md" rel="noopener ugc nofollow" target="_blank">STORM generated article — using text further chunked using RecursiveTextSplitter</a></li></ul><p id="4f5a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Some quick observations</strong></p><p id="8994" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This demo doesn’t get into a formal evaluation — which can be <a class="af nc" href="https://arxiv.org/abs/2401.15391" rel="noopener ugc nofollow" target="_blank">more involved than single-hop RAG systems</a> — but here are some subjective observations that may or may not be useful …</p><ol class=""><li id="b19e" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pn po pp bk">Parsing by page or by smaller chunks produces reasonable pre-reading reports that a human could use for researching areas related to the financial impact of disasters</li><li id="55b2" class="nd ne fq nf b go pq nh ni gr pr nk nl nm ps no np nq pt ns nt nu pu nw nx ny pn po pp bk">Both paring approaches provided citations throughout, but using smaller chunks seemed to result in fewer. See for example the Summary sections in both of the above articles. The more references to ground the analysis, the better!</li><li id="2931" class="nd ne fq nf b go pq nh ni gr pr nk nl nm ps no np nq pt ns nt nu pu nw nx ny pn po pp bk">Parsing by smaller chunks seemed to sometimes create citations that were not relevant, one of the citation challenges mentioned in the STORM paper. See for example citation for source ‘10’ in the <a class="af nc" href="https://github.com/dividor/storm-with-local-docs/blob/main/data/storm_output/chunks/Compare_the_financial_impact_of_different_types_of_disasters_and_how_those_impact_communities/storm_gen_article_polished.md" rel="noopener ugc nofollow" target="_blank">summary section</a>, which doesn’t correspond with the reference sentence.</li><li id="9ec3" class="nd ne fq nf b go pq nh ni gr pr nk nl nm ps no np nq pt ns nt nu pu nw nx ny pn po pp bk">Overall, as expected for an algorithm developed on Wiki articles, splitting text by PDF seemed to produce a <a class="af nc" href="https://github.com/dividor/storm-with-local-docs/blob/main/data/storm_output/pages/Compare_the_financial_impact_of_different_types_of_disasters_and_how_those_impact_communities/storm_gen_article_polished.md" rel="noopener ugc nofollow" target="_blank">more cohesive and grounded article</a> (to me!)</li></ol><p id="ebbf" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Even though the input research topic wasn’t covered in great depth in the underlying documents, the generated report was a great starting point for further human analysis</p><h1 id="020d" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Future Work</h1><p id="dc5f" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">We didn’t get into <a class="af nc" href="https://www.arxiv.org/abs/2408.15232" rel="noopener ugc nofollow" target="_blank">Co-Storm</a> in this article, which brings a human into the loop. This seems a great direction for AI-empowered research and something I am investigating.</p><p id="652c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Future work could also look at adjusting the system prompts and personas to the business case. Currently, those prompts are targeted for a Wikipedia-like process …</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk px"><img src="../Images/41fbcc46e187420f6725280d4e787d88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DoiSFZnvvOsu8eMDUKewEw.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">STORM system prompts, illustrating the emphasis on creating Wikipedia-style articles. <a class="af nc" href="https://arxiv.org/abs/2402.14207" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="d548" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Another possible direction is to extend STORM’s connectors beyond Qdrant, for example, to include other vector stores, or better still, generic support for Langchain and llama index vector stores. The authors encourage this type of thing, a PR involving <a class="af nc" href="https://github.com/stanford-oval/storm/blob/main/knowledge_storm/rm.py" rel="noopener ugc nofollow" target="_blank">this file</a> may be in my future.</p><p id="9729" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Running STORM without an internet connection would be an amazing thing, as it opens up possibilities for AI assistance in the field. As you can see from the demo code, I added the ability to run STORM with Ollama locally hosted models, but the token throughput rate was too low for the LLM agent discussion phase, so the system didn’t complete on my laptop with small quantized models. A topic for a future blog post perhaps!</p><p id="af5c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Finally, though the <a class="af nc" href="https://storm.genie.stanford.edu" rel="noopener ugc nofollow" target="_blank">online User Interface is very nice</a>, the <a class="af nc" href="https://github.com/stanford-oval/storm/tree/main/frontend/demo_light" rel="noopener ugc nofollow" target="_blank">demo UI that comes with the repo</a> is very basic and not something that could be used in production. Perhaps the Standford team might release the advanced interface — maybe it is already somewhere? — if not then work would be needed here.</p><h1 id="6a8f" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Conclusions</h1><p id="edfb" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">This is a quick demo to hopefully help people get started with using STORM on their own documents. I haven’t gone into systematic evaluation, something that would obviously need to be done if using STORM in a live environment. That said, I was impressed at how it seems to be able to get a relatively nuanced research topic and generate well-cited pre-writing research content that would help me in my own research.</p><h1 id="1397" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">References</h1><p id="c61a" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk"><a class="af nc" href="https://arxiv.org/abs/2005.11401" rel="noopener ugc nofollow" target="_blank">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a>, Lewis et al., 2020</p><p id="8efb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><a class="af nc" href="https://arxiv.org/html/2312.10997v5#S2" rel="noopener ugc nofollow" target="_blank">Retrieval-Augmented Generation for Large Language Models: A Survey</a>, Yunfan et al., 2024</p><p id="13b4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><a class="af nc" href="https://arxiv.org/abs/2402.14207" rel="noopener ugc nofollow" target="_blank">Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models</a>, Shao et al., 2024</p><p id="4c43" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><a class="af nc" href="https://www.arxiv.org/abs/2408.15232" rel="noopener ugc nofollow" target="_blank">Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations</a>, Jiang et al., 2024</p><p id="ec14" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><a class="af nc" href="https://arxiv.org/abs/2401.15391" rel="noopener ugc nofollow" target="_blank">MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries</a>, Tang et al., 2024</p><p id="4d5b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">You can find the code for this article <a class="af nc" href="https://github.com/dividor/storm-with-local-docs" rel="noopener ugc nofollow" target="_blank">here</a></p><p id="08c2" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr"><em class="nz">Please like this article if inclined and I’d be super delighted if you followed me! You can find more articles </em></strong><a class="af nc" href="/@astrobagel" rel="noopener ugc nofollow" target="_blank"><strong class="nf fr"><em class="nz">here</em></strong></a><strong class="nf fr"><em class="nz"> or connect on </em></strong><a class="af nc" href="https://www.linkedin.com/in/matthew-harris-4018865/" rel="noopener ugc nofollow" target="_blank"><strong class="nf fr"><em class="nz">LinkedIn</em></strong></a><strong class="nf fr"><em class="nz">.</em></strong></p></div></div></div></div>    
</body>
</html>