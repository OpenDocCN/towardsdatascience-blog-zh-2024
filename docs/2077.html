<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Advanced Retrieval Techniques in a World of 2M Token Context Windows: Part 2 on Re-rankers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Advanced Retrieval Techniques in a World of 2M Token Context Windows: Part 2 on Re-rankers</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/advanced-retrieval-techniques-in-a-world-of-2m-token-context-windows-part-2-on-re-rankers-a0dfa03ba325?source=collection_archive---------7-----------------------#2024-08-26">https://towardsdatascience.com/advanced-retrieval-techniques-in-a-world-of-2m-token-context-windows-part-2-on-re-rankers-a0dfa03ba325?source=collection_archive---------7-----------------------#2024-08-26</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><figure class="fr fs ft fu fv fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp fq"><img src="../Images/d563caf69cb0e3c037f66913efe18f14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JuqIWjSvoiQZhURfLtOl0A.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Visualising AI project launched by Google DeepMind. From <a class="af gi" href="https://unsplash.com/photos/a-close-up-of-a-group-of-different-colored-objects-_aU_AxlS04E" rel="noopener ugc nofollow" target="_blank">Unsplash </a>image.</figcaption></figure><div/><div><h2 id="f9d8" class="pw-subtitle-paragraph hi gk gl bf b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx cq dx">Exploring RAG techniques to improve retrieval accuracy</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hy hz ia ib ic ab"><div><div class="ab id"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@meghanheintz?source=post_page---byline--a0dfa03ba325--------------------------------" rel="noopener follow"><div class="l ie if by ig ih"><div class="l ed"><img alt="Meghan Heintz" class="l ep by dd de cx" src="../Images/9eaae6d3d8168086d83ff7100329c51f.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*Tespb9SFbU5QAxy8f7bhnA.png"/><div class="ii by l dd de em n ij eo"/></div></div></a></div></div><div class="ik ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--a0dfa03ba325--------------------------------" rel="noopener follow"><div class="l il im by ig in"><div class="l ed"><img alt="Towards Data Science" class="l ep by br io cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ii by l br io em n ij eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="ip ab q"><div class="ab q iq"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ir is bk"><a class="af ag ah ai aj ak al am an ao ap aq ar it" data-testid="authorName" href="https://medium.com/@meghanheintz?source=post_page---byline--a0dfa03ba325--------------------------------" rel="noopener follow">Meghan Heintz</a></p></div></div></div><span class="iu iv" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ir is dx"><button class="iw ix ah ai aj ak al am an ao ap aq ar iy iz ja" disabled="">Follow</button></p></div></div></span></div></div><div class="l jb"><span class="bf b bg z dx"><div class="ab cn jc jd je"><div class="jf jg ab"><div class="bf b bg z dx ab jh"><span class="ji l jb">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar it ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--a0dfa03ba325--------------------------------" rel="noopener follow"><p class="bf b bg z jj jk jl jm jn jo jp jq bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="iu iv" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="jr js l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Aug 26, 2024</span></div></span></div></span></div></div></div><div class="ab cp jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki"><div class="h k w ea eb q"><div class="ky l"><div class="ab q kz la"><div class="pw-multi-vote-icon ed ji lb lc ld"><div class=""><div class="le lf lg lh li lj lk am ll lm ln ld"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l lo lp lq lr ls lt lu"><p class="bf b dy z dx"><span class="lf">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao le lv lw ab q ee lx ly" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lz"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q kj kk kl km kn ko kp kq kr ks kt ku kv kw kx"><div class="ma k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al mb an ao ap iy mc md me" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep mf cn"><div class="l ae"><div class="ab cb"><div class="mg mh mi mj mk gb ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al mb an ao ap iy ml mm ly mn mo mp mq mr s ms mt mu mv mw mx my u mz na nb"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al mb an ao ap iy ml mm ly mn mo mp mq mr s ms mt mu mv mw mx my u mz na nb"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al mb an ao ap iy ml mm ly mn mo mp mq mr s ms mt mu mv mw mx my u mz na nb"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="ab33" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In <a class="af gi" href="https://medium.com/p/2edc0266aabe" rel="noopener">Part 1</a>, we dove into improving RAG (retrieval augmented generation) outcomes by re-writing queries before performing retrieval. This time we will learn about how re-ranking results from vector database retrievals helps performance.</p><p id="3c88" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">While I highly recommend experimenting with promising proprietary options like Cohere’s <a class="af gi" href="https://cohere.com/blog/rerank-3" rel="noopener ugc nofollow" target="_blank">Re-Rank 3</a>, we’ll focus mainly on understanding what researchers have shared on the topic.</p><h1 id="db51" class="ny nz gl bf oa ob oc hl od oe of ho og oh oi oj ok ol om on oo op oq or os ot bk">Re-Ranking, what’s the point?</h1><p id="797e" class="pw-post-body-paragraph nc nd gl ne b hj ou ng nh hm ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">First of all, why rerank at all? Results from vector databases return “similarity” scores based on the embeddings of the query and document. These scores can already be used to sort the results and since this is already a semantic similarity scoring of the document and query, why would we need another step?</p><p id="e28f" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">There are a few reasons why we would take this approach:</p><ul class=""><li id="50d2" class="nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oz pa pb bk">Document embeddings are “lossy”. Documents are compressed in vector format before seeing the query, which means the document vector is not tailored to the query vector. Re-ranking allows us to better understand the <strong class="ne gm">document’s meaning specific to the query</strong>.</li><li id="4e88" class="nc nd gl ne b hj pc ng nh hm pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx oz pa pb bk">Two-stage systems have become standard in traditional search and recommender systems. They offer improvements in scalability, flexibility, and accuracy. <strong class="ne gm">Retrieval models are very fast, whereas ranking models are slow.</strong> By building a hybrid system, we can balance the speed and accuracy trade-offs between each stage.</li><li id="dd6c" class="nc nd gl ne b hj pc ng nh hm pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx oz pa pb bk">Re-ranking allows us to reduce the number of documents we stuff into the context window which <strong class="ne gm">a) reduces costs</strong> and <strong class="ne gm">b) reduces the chances of relevant data being “lost in the haystack”.</strong></li></ul><h1 id="2349" class="ny nz gl bf oa ob oc hl od oe of ho og oh oi oj ok ol om on oo op oq or os ot bk">Traditional Methods of Re-Ranking</h1><p id="25e2" class="pw-post-body-paragraph nc nd gl ne b hj ou ng nh hm ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Informational retrieval is not a new field. Before LLMs employed RAG to improve generation, search engines used re-ranking methods to improve search results. Two popular methodologies are TF-IDF (term frequency–inverse document frequency) and BM25 (Best Match 25).</p><p id="1405" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><a class="af gi" href="https://en.wikipedia.org/wiki/Karen_Sp%C3%A4rck_Jones" rel="noopener ugc nofollow" target="_blank">Karen Spärck Jones</a> conceived of the concept of IDF (of TF-IDF), inverse document frequency, as a statistical interpretation of term-specificity in the 1970s. The general concept is that the specificity of a term can be quantified as an inverse function of the number of documents in which it occurs. A toy example is the frequency of terms in Shakespearean plays. Because the term “Romeo” only appears in one play, we believe it is more informative to the subject of the play than the word “sweet” because that term occurs in all plays.</p><p id="0382" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">BM25 or Okapi BM25 was developed by both Karen Spärck Jones and <a class="af gi" href="https://en.wikipedia.org/wiki/Stephen_Robertson_(computer_scientist)" rel="noopener ugc nofollow" target="_blank">Stephen Robertson</a> as an improvement to TF-IDF. BM25 is a “bag-of-words” retrieval function that ranks a set of documents based on the query terms appearing in each document, regardless of their proximity within the document. This method expands on TF-IDF in a few important ways:</p><ul class=""><li id="e1ee" class="nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oz pa pb bk">BM25 uses a saturation function where the importance of a term increases with frequency but with diminishing returns. (Side note: This was important for protecting accuracy when search engine optimization (SEO) became higher stakes. You can’t just spam your keyword with this improvement.)</li><li id="5400" class="nc nd gl ne b hj pc ng nh hm pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx oz pa pb bk">BM25 includes document length normalization to ensure that longer documents are not unfairly advantaged. (Another improvement to thwart would-be SEO gamers.)</li></ul><p id="687d" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Both of these methods can be used to re-rank results from vector databases before documents are used in the context of generation. This would be called “feature” based re-ranking.</p><h1 id="eed1" class="ny nz gl bf oa ob oc hl od oe of ho og oh oi oj ok ol om on oo op oq or os ot bk">Neural Re-Ranking Models</h1><p id="4244" class="pw-post-body-paragraph nc nd gl ne b hj ou ng nh hm ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Something you should notice about the traditional methods is that they focus on exact term matches. These methods will struggle when documents use semantically similar but different terms. Neural re-ranking methods like SBERT (<a class="af gi" href="https://arxiv.org/pdf/1908.10084" rel="noopener ugc nofollow" target="_blank">Sentence Transformers</a>) seek to overcome this limitation.</p><p id="22bf" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">SBERT is a fine-tuned BERT (Bidirectional Encoder Representations from Transformers) model with a siamese / triplet network architecture which greatly improves the computation efficiency and latency for calculating sentence similarity. Transformers like <a class="af gi" href="https://arxiv.org/pdf/1908.10084" rel="noopener ugc nofollow" target="_blank">SBERT</a> (Sentence-BERT) use the context in which terms are used, allowing the model to handle synonyms and words with multiple meanings.</p><figure class="pi pj pk pl pm fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp ph"><img src="../Images/d654e0e6d8d456ab557a95ef07dc4080.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*-_b6YwojPDufDdf4GYt93Q.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">SBERT architecture at inference, for example, to compute similarity scores. This architecture is also used with the regression objective function. From <a class="af gi" href="https://arxiv.org/pdf/1908.10084" rel="noopener ugc nofollow" target="_blank">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks by Nils Reimers and Iryna Gurevych</a></figcaption></figure><p id="0a47" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">SBERT tends to perform better for semantic similarity ranking due to its specialization. However, using SBERT comes with the downside that you will need to manage the models locally versus calling an API, such as with OpenAI’s embedding models. Pick your poison wisely!</p><h1 id="c6b2" class="ny nz gl bf oa ob oc hl od oe of ho og oh oi oj ok ol om on oo op oq or os ot bk">Cross-Encoder Re-Ranking</h1><p id="bb11" class="pw-post-body-paragraph nc nd gl ne b hj ou ng nh hm ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">The top K results from a vector database search are the most similar document vectors compared to the query vector. Another way of describing this ranking method is to say it is a “bi-encoder” ranking. Vectors are calculated up front and approximate nearest neighbors algorithms (ANNs) select the most similar documents making this a highly efficient ranking method. But that efficiency comes at the expense of some accuracy.</p><p id="3898" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In contrast, cross-encoders use a classification mechanism on data pairs to calculate similarity. This means you need a pair for each document and query. This approach can yield much more accurate results but it’s highly inefficient. That is why cross-encoders are best implemented through a hybrid approach where the number of documents is first pruned using a “bi-encoder” top K result before ranking with a cross-encoder. You can read more about using bi-encoders and cross-encoders together in the <a class="af gi" href="https://www.sbert.net/examples/applications/retrieve_rerank/README.html" rel="noopener ugc nofollow" target="_blank">SBERT documentation</a>.</p><figure class="pi pj pk pl pm fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pn"><img src="../Images/e87d3746696f5d9c5fd2179ff3fb8756.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k8w_foWvsfhCsepI74HABQ.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Information Retrieval / Question Answering Retrieval Diagram explaining how to use Bi-Encoders and Cross-Encoders together from the <a class="af gi" href="https://www.sbert.net/examples/applications/retrieve_rerank/README.html" rel="noopener ugc nofollow" target="_blank">SBERT documentation</a>, full citation below.</figcaption></figure><pre class="pi pj pk pl pm po pp pq bp pr bb bk"><span id="db30" class="ps nz gl pp b bg pt pu l pv pw">@inproceedings{reimers-2019-sentence-bert,<br/>  title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",<br/>  author = "Reimers, Nils and Gurevych, Iryna",<br/>  booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",<br/>  month = "11",<br/>  year = "2019",<br/>  publisher = "Association for Computational Linguistics",<br/>  url = "https://arxiv.org/abs/1908.10084",<br/>}</span></pre><h1 id="afb6" class="ny nz gl bf oa ob oc hl od oe of ho og oh oi oj ok ol om on oo op oq or os ot bk">Prompt-based Re-Ranking (PBR)</h1><p id="eb8a" class="pw-post-body-paragraph nc nd gl ne b hj ou ng nh hm ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Until now, we have focused on using vectors or other numeric methods to rerank our RAG results. But does that mean we are underleveraging the LLM? Feeding the document and the query back to the LLM for scoring can be an effective way to score the document; there is approximately no information loss when you take this approach. If the LLM is prompted to return only a single token (the score), the latency incurred is often acceptable (although this is one of the more expensive approaches to scale). This is considered to be “zero-shot” re-ranking and research is still limited on this topic but we know it must be sensitive to the quality of the prompt.</p><p id="2a68" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Another version of (PBR) is the <a class="af gi" href="https://ar5iv.labs.arxiv.org/html/2407.03627v4" rel="noopener ugc nofollow" target="_blank"><strong class="ne gm">DSLR Framework</strong></a> (Document Refinement with Sentence-Level Re-ranking and Reconstruction). DSLR proposes an unsupervised method that decomposes retrieved documents into sentences, re-ranks them based on relevance, and reconstructs them into coherent passages before passing them to the LLM. This approach contrasts with traditional methods that rely on fixed-size passages, which may include redundant or irrelevant information. Pruning non-relevant sentences before generating a response can reduce hallucinations and improve overall accuracy. Below you can see an example of how DSLR refinement improves the LLMs response.</p><figure class="pi pj pk pl pm fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp px"><img src="../Images/f1b8b5cb97f9535858ebec79376c3e32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vMOsQUspS_Ys52IcT4bpcg.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Example DSLR refinement from <a class="af gi" href="https://ar5iv.labs.arxiv.org/html/2407.03627v4" rel="noopener ugc nofollow" target="_blank">DSLR: Document Refinement with Sentence-Level Re-ranking and Reconstruction to Enhance Retrieval-Augmented Generation by Taeho Hwang, Soyeong Jeong, Sukmin Cho, SeungYoon Han, Jong C. Park at the School of Computing Korea Advanced Institute of Science and Technology</a></figcaption></figure><h1 id="ffc1" class="ny nz gl bf oa ob oc hl od oe of ho og oh oi oj ok ol om on oo op oq or os ot bk">Graph-based Reranking</h1><p id="546b" class="pw-post-body-paragraph nc nd gl ne b hj ou ng nh hm ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Sometimes the answer is not going to fit cleanly inside a single document chunk. Books and papers are written with the expectation that they’ll be read linearly or at least the reader will be able to easily refer back to earlier passages. For example, you could be asked to refer back to an earlier chapter on BM25 when reading about SBERT. In a basic RAG application, this would be impossible because your retrieved document would have no connections to the previous chapters.</p><p id="8d91" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><a class="af gi" href="https://arxiv.org/pdf/2405.18414" rel="noopener ugc nofollow" target="_blank">G-RAG</a>, an approach proposed by researchers at Google and UCLA, hopes to alleviate this issue. G-RAG is a re-ranker that leverages graph neural networks (GNNs) to consider connections between retrieved documents. Documents are represented as nodes and edges are shared concepts between documents. These graphs are generated as Abstract Meaning Representation (AMR) Graphs which can be created with tools like <a class="af gi" href="https://github.com/goodbai-nlp/AMRBART" rel="noopener ugc nofollow" target="_blank">https://github.com/goodbai-nlp/AMRBART</a> (MIT License).</p><p id="17c7" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Experiments with <a class="af gi" href="https://aclanthology.org/Q19-1026/" rel="noopener ugc nofollow" target="_blank">Natural Question (NQ)</a> and <a class="af gi" href="https://paperswithcode.com/dataset/triviaqa" rel="noopener ugc nofollow" target="_blank">TriviaQA (TQA) datasets</a> showed this approach made improvements to Mean Tied Reciprocal Ranking (MTRR) and Tied Mean Hits@10 (TMHits@10) over other state-of-the-art approaches.</p><figure class="pi pj pk pl pm fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp py"><img src="../Images/1e26c1544a15d9add1ba755fd03236a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hMmgWTD484iLgn7GeYNc2g.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx"><a class="af gi" href="https://arxiv.org/pdf/2405.18414" rel="noopener ugc nofollow" target="_blank">From UCLA and Google researchers</a>: G-RAG uses two graphs for re-ranking documents: The Abstract Meaning Representation (AMR) graph is used as feature for the document-level graph. Document graph is then used for document reranking</figcaption></figure><h1 id="8df4" class="ny nz gl bf oa ob oc hl od oe of ho og oh oi oj ok ol om on oo op oq or os ot bk">Conclusion</h1><p id="df4a" class="pw-post-body-paragraph nc nd gl ne b hj ou ng nh hm ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">I hope you’ve enjoyed this overview of techniques you can use to improve the performance of your RAG applications. I look forward to continued advancements in this field. I know there will be many considering the blistering pace of research at the moment.</p><p id="c24d" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let me know in the comments section if you have favorite re-ranking methods not covered in this article.</p></div></div></div></div>    
</body>
</html>