<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>What We Still Don’t Understand About Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>What We Still Don’t Understand About Machine Learning</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-we-still-dont-understand-about-machine-learning-699e0002a057?source=collection_archive---------1-----------------------#2024-07-26">https://towardsdatascience.com/what-we-still-dont-understand-about-machine-learning-699e0002a057?source=collection_archive---------1-----------------------#2024-07-26</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="be9b" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Machine Learning unknowns that researchers struggle to understand — from Batch Norm to what SGD hides</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@itshesamsheikh?source=post_page---byline--699e0002a057--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Hesam Sheikh" class="l ep by dd de cx" src="../Images/b8d5f4f285eef77634e4c1d4321580ed.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*hEouYBx-IeJIslDqS20BjQ.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--699e0002a057--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@itshesamsheikh?source=post_page---byline--699e0002a057--------------------------------" rel="noopener follow">Hesam Sheikh</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--699e0002a057--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 26, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">5</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div><div class="mj"><div class="ab cb"><div class="lm mk ln ml lo mm cf mn cg mo ci bh"><figure class="ms mt mu mv mw mj mx my paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq mr"><img src="../Images/1a58ae922fc1a99ffd5a8d87485bad57.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*-RD95WFGvdRtmASW76buuA.jpeg"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">What We Still Don’t Understand About Machine Learning. (by author)</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><blockquote class="nj nk nl"><p id="b925" class="nm nn no np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">If you’re not a member, <a class="af oj" rel="noopener" target="_blank" href="/what-we-still-dont-understand-about-machine-learning-699e0002a057?sk=e2498900bd4ada1e8abe37e056f6ebc9">read for free here</a> 👈</p></blockquote><p id="f3f8" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">It is surprising how some of the basic subjects in machine learning are still unknown by researchers and despite being fundamental and common to use, seem to be mysterious. It’s a fun thing about machine learning that we build things that work and then figure out why they work at all!</p><p id="ffa0" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">Here, I aim to investigate the unknown territory in some machine learning concepts in order to show while these ideas can seem basic, in reality, they are constructed by layers upon layers of abstraction. This helps us to practice questioning the <strong class="np fr">depth of our knowledge</strong>.</p><p id="5f03" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk ok"><span class="l ol om on bo oo op oq or os ed">In</span> this article, we explore several key phenomena in deep learning that challenge our traditional understanding of neural networks.</p><ul class=""><li id="3d28" class="nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi ot ou ov bk">We start with <strong class="np fr">Batch Normalization </strong>and its underlying mechanisms that remain not fully understood.</li><li id="6b68" class="nm nn fq np b go ow nr ns gr ox nu nv nw oy ny nz oa oz oc od oe pa og oh oi ot ou ov bk">We examine the counterintuitive observation that <strong class="np fr">overparameterized </strong>models often <strong class="np fr">generalize better</strong>, contradicting the classical machine learning theories.</li><li id="b4ca" class="nm nn fq np b go ow nr ns gr ox nu nv nw oy ny nz oa oz oc od oe pa og oh oi ot ou ov bk">We explore the implicit regularization effects of <strong class="np fr">gradient descent</strong>, which seem to naturally bias neural networks towards simpler, more generalizable solutions.</li><li id="3658" class="nm nn fq np b go ow nr ns gr ox nu nv nw oy ny nz oa oz oc od oe pa og oh oi ot ou ov bk">Finally, we touch on the Lottery Ticket Hypothesis, which proposes that large neural networks contain smaller subnetworks capable of achieving <strong class="np fr">comparable performance</strong> when trained in isolation.</li></ul><p id="1e2f" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk"><em class="no">This article contains 4 parts, they are not connected so feel free to skip to the ones you like more.</em></p><p id="eec1" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">· <a class="af oj" href="#de8b" rel="noopener ugc nofollow">1. Batch Normalization</a><br/> ∘ <a class="af oj" href="#95c8" rel="noopener ugc nofollow">What We Don’t Understand</a><br/> ∘ <a class="af oj" href="#ec44" rel="noopener ugc nofollow">Batch Norm Considerations</a><br/>· <a class="af oj" href="#82b3" rel="noopener ugc nofollow">2. Over-Parameterization and Generalization</a><br/> ∘ <a class="af oj" href="#6185" rel="noopener ugc nofollow">Understanding Deep Learning Requires Rethinking Generalization</a><br/>· <a class="af oj" href="#f626" rel="noopener ugc nofollow">3. Implicit Regularization in Neural Networks</a><br/> ∘ <a class="af oj" href="#2783" rel="noopener ugc nofollow">Experiment 1</a><br/> ∘ <a class="af oj" href="#ff53" rel="noopener ugc nofollow">Experiment 2</a><br/> ∘ <a class="af oj" href="#85f8" rel="noopener ugc nofollow">Gradient Descent as a natural Reguralizer</a><br/>· <a class="af oj" href="#b35e" rel="noopener ugc nofollow">4. The Lottery Ticket Hypothesis</a><br/>· <a class="af oj" href="#821e" rel="noopener ugc nofollow">Final Word.</a><br/> ∘ <a class="af oj" href="#03ee" rel="noopener ugc nofollow">Let’s Connect!</a><br/>· <a class="af oj" href="#397f" rel="noopener ugc nofollow">Further Reads</a><br/>· <a class="af oj" href="#f3e3" rel="noopener ugc nofollow">References</a></p><h1 id="de8b" class="pb pc fq bf pd pe pf gq pg ph pi gt pj pk pl pm pn po pp pq pr ps pt pu pv pw bk">1. Batch Normalization</h1><p id="0e49" class="pw-post-body-paragraph nm nn fq np b go px nr ns gr py nu nv nw pz ny nz oa qa oc od oe qb og oh oi fj bk">Introduced by <em class="no">Sergey Ioffe </em>and <em class="no">Christian Szegedy</em> in 2015 [1], batch normalization is a method to train neural networks faster and more stable. It was previously known that transforming the input data to have their mean set to zero and variance to one would result in a faster convergence. The authors used this idea further and introduced <strong class="np fr">Batch Normalization</strong> to transform the input of hidden layers to have a mean of zero and a variance of one.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qc"><img src="../Images/3d64e4c77a25558d72ba330536db8ba1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_g_5KbmL8oWnusUz2YZW-A.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">An outline of the Residual Block used in Resnet and the use of Batch Norm in it. (by author)</figcaption></figure><p id="bae6" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">Since its introduction, the batch norm has become common in neural networks. One such example, among many, is its use in the famous ResNet architecture. So we could confidently say we are certain of how effective it could be.</p><p id="f5b8" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">An interesting research [2] on the effectiveness of Batch Norm showed that while training a full ResNet-866 network yielded <strong class="np fr">93 </strong>percent accuracy, freezing all the parameters and training <strong class="np fr">only </strong>the parameters of the Batch Norm layers resulted in <strong class="np fr">83 </strong>percent accuracy — only a 10% difference.</p><p id="e1ef" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">Batch Norm is beneficial in three ways:</p><ul class=""><li id="d470" class="nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi ot ou ov bk">By normalizing the inputs to each layer, it <strong class="np fr">accelerates </strong>the training process.</li><li id="22ca" class="nm nn fq np b go ow nr ns gr ox nu nv nw oy ny nz oa oz oc od oe pa og oh oi ot ou ov bk">It reduces the sensitivity to<strong class="np fr"> initial conditions</strong>, meaning we need less careful weight initialization.</li><li id="943f" class="nm nn fq np b go ow nr ns gr ox nu nv nw oy ny nz oa oz oc od oe pa og oh oi ot ou ov bk">Batch Norm also acts as a <strong class="np fr">regularizer</strong>. It improves model generalization and in some cases, reduces the need for other regularization techniques.</li></ul><h2 id="95c8" class="qd pc fq bf pd qe qf qg pg qh qi qj pj nw qk ql qm oa qn qo qp oe qq qr qs qt bk">What We Don’t Understand</h2><p id="dcf9" class="pw-post-body-paragraph nm nn fq np b go px nr ns gr py nu nv nw pz ny nz oa qa oc od oe qb og oh oi fj bk">While the positive effect of the Batch Norm is evident, nobody quite understands the reason behind its effectiveness. Originally, the authors of the Batch Normalization paper introduced it as a method to solve the problem of <strong class="np fr">internal covariate shift</strong>.</p><p id="2690" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk"><em class="no">A heads-up about internal covariate shift is that you will find various definitions of it that don’t seem to be related at first. </em><strong class="np fr"><em class="no">Here is how I would like to define it.</em></strong></p><p id="e52b" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">The layers of a neural network are updated during the backpropagation from the finish (output layer) to the start (input layer). Internal covariate shift refers to the phenomenon where the distribution of inputs to a layer changes during training as the parameters of the previous layers are updated.</p><blockquote class="qu"><p id="931f" class="qv qw fq bf qx qy qz ra rb rc rd oi dx">As we change the parameters of the earlier layers, we also change the input distribution to the later layers that have been updated to better fit the older distribution.</p></blockquote><p id="9aa6" class="pw-post-body-paragraph nm nn fq np b go re nr ns gr rf nu nv nw rg ny nz oa rh oc od oe ri og oh oi fj bk">Internal covariate shift slows down the training and makes it harder for the network to converge, as each layer must continuously adapt to the changing distribution of its inputs introduced by the update of previous layers.</p><p id="1ea6" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">The authors of the original Batch Normalization paper believed that the reason behind its effectiveness is because of mitigating the problem of internal covariate shift. However, a later paper [3] argued that the success of Batch Norm has nothing to do with internal covariate shift, but is due to <strong class="np fr">smoothing the optimization landscape</strong>.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rj"><img src="../Images/2f88c5bb3a672aa66fcd7373c0c431a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eF56Yus5ctzfuL2mA-o-nw.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Comparison of two loss landscapes: a highly rough and sharp loss surface (left) vs. a smoother loss surface (right). (<a class="af oj" href="https://arxiv.org/abs/1712.09913" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><p id="81e2" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">The figure above is taken from [4], which is not actually about Batch Normalization, but is a good visualization of how a smooth loss landscape looks like. However, the theory that the Batch Norm is effective due to smoothing the loss landscape has challenges and questions of its own.</p><h2 id="ec44" class="qd pc fq bf pd qe qf qg pg qh qi qj pj nw qk ql qm oa qn qo qp oe qq qr qs qt bk">Batch Norm Considerations</h2><p id="cd91" class="pw-post-body-paragraph nm nn fq np b go px nr ns gr py nu nv nw pz ny nz oa qa oc od oe qb og oh oi fj bk">Due to our limited understanding of how Batch Normalization works, here is what to consider when it comes to using them in your network:</p><ul class=""><li id="b0ed" class="nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi ot ou ov bk">There is a difference between <strong class="np fr">train </strong>and <strong class="np fr">inference </strong>modes when using the Batch Norm. Using the wrong mode can lead to unexpected behavior that is tricky to identify. [5]</li><li id="420b" class="nm nn fq np b go ow nr ns gr ox nu nv nw oy ny nz oa oz oc od oe pa og oh oi ot ou ov bk">Batch Norm relies heavily on the size of your <strong class="np fr">minibatch</strong>. So while it makes the need for a careful weight initialization less significant, choosing the right size of minibatch becomes more crucial. [5]</li><li id="4acd" class="nm nn fq np b go ow nr ns gr ox nu nv nw oy ny nz oa oz oc od oe pa og oh oi ot ou ov bk">There is still an ongoing debate about whether to use Batch Norm before the activation function or after it. [6]</li><li id="00da" class="nm nn fq np b go ow nr ns gr ox nu nv nw oy ny nz oa oz oc od oe pa og oh oi ot ou ov bk">While Batch Norm has a <strong class="np fr">regularizer </strong>effect, its interaction with other regularizations such as <em class="no">dropout </em>or <em class="no">weight decay</em> is not clearly known.</li></ul><p id="55c5" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">There are many questions to answer when it comes to Batch Norms, but research is still ongoing to uncover how these layers affect a neural network.</p><h1 id="82b3" class="pb pc fq bf pd pe pf gq pg ph pi gt pj pk pl pm pn po pp pq pr ps pt pu pv pw bk">2. Over-Parameterization and Generalization</h1><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rk"><img src="../Images/c4c6c3f08203f8d9473bd7ce0197c417.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UL4qofTgqN-gff4NSsELfw.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Face recognition experiments showing that the optimal number of weights in a network can be much larger than the number of data points.. (<a class="af oj" href="https://clgiles.ist.psu.edu/papers/UMD-CS-TR-3617.what.size.neural.net.to.use.pdf" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="3d17" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">Big networks have challenged our old beliefs of how neural networks work.</p><p id="ad43" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">It was traditionally believed using over-parametrized models would result in overfitting. Thus the solution would be either to limit the size of the network or to add regularization to prevent overfitting to the training data.</p><p id="38e8" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">Surprisingly, in the case of neural networks, using bigger networks could improve the <strong class="np fr">generalization error</strong> (|train error - test error|). In other words, bigger networks generalize better. [7] This is in contradiction to what traditional complexity metrics such as <a class="af oj" href="https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension" rel="noopener ugc nofollow" target="_blank">VC dimension</a> — a metric to quantify the difficulty of learning from examples, have promised. [8]</p><p id="ab52" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">This theory also challenges a debate about whether or not deep neural networks (DNNs) achieve their performance by <em class="no">memorizing </em>training data or by learning patterns. [9] If they memorize the data, how could they possibly generalize to predict the unseen data? And if they don’t memorize the data but only learn the underlying pattern, how do they predict correct labels even when we introduce a certain amount of noise to the labels?</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rl"><img src="../Images/d8dbde067c84e6356a774b2dcf646f1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IgC8bldMxQJLXh65lHkRHg.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">overfitting of a classifier. (upscaled — <a class="af oj" href="https://commons.wikimedia.org/wiki/File:Overfitting.svg" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><h2 id="6185" class="qd pc fq bf pd qe qf qg pg qh qi qj pj nw qk ql qm oa qn qo qp oe qq qr qs qt bk"><em class="rm">Understanding Deep Learning Requires Rethinking Generalization</em></h2><p id="094b" class="pw-post-body-paragraph nm nn fq np b go px nr ns gr py nu nv nw pz ny nz oa qa oc od oe qb og oh oi fj bk">An interesting paper on this subject is <em class="no">Understanding deep learning requires rethinking generalization</em> [10]. The authors argue that traditional approaches fail to explain why larger networks generalize well and at the same time, these networks can fit even random data.</p><p id="100d" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">A notable part of this paper explains the role of <strong class="np fr">explicit regularizations</strong> such as weight decay, dropout, and data augmentation on the generalization error:</p><blockquote class="qu"><p id="ba92" class="qv qw fq bf qx qy qz ra rb rc rd oi dx">Explicit regularization may improve generalization performance, but is neither necessary nor by itself sufficient for controlling generalization error. L2-regularization (weight decay) sometimes even helps optimization, illustrating its poorly understood nature in deep learning. [10]</p></blockquote><p id="7aae" class="pw-post-body-paragraph nm nn fq np b go re nr ns gr rf nu nv nw rg ny nz oa rh oc od oe ri og oh oi fj bk">Even with dropout and weight decay, InceptionV3 was still able to fit the <strong class="np fr">random </strong>training set very well beyond expectation. This implication is not to devalue regularization, but more to emphasize that bigger gains could be achieved from changing the model <strong class="np fr">architecture</strong>.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rn"><img src="../Images/e203216819fb3accb40cefdf6557f216.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mjpB7SiYmQkrC3vfzKgSrw.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">The effect of regularization on generalization. [10]</figcaption></figure><p id="633e" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">So what makes a neural network that generalizes well, different from those that generalize poorly? It seems like a rabbit hole. We yet need to rethink a few things:</p><ul class=""><li id="f25f" class="nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi ot ou ov bk">Our understanding of a model’s <strong class="np fr">effective capacity</strong>.</li><li id="59ac" class="nm nn fq np b go ow nr ns gr ox nu nv nw oy ny nz oa oz oc od oe pa og oh oi ot ou ov bk">Our measurement of a model’s complexity<strong class="np fr"> </strong>and size. Are model parameters or FLOPs simply good metrics? Obviously not.</li><li id="7990" class="nm nn fq np b go ow nr ns gr ox nu nv nw oy ny nz oa oz oc od oe pa og oh oi ot ou ov bk">The definition of generalization and how to measure it.</li></ul><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq ro"><img src="../Images/cd130cd6128053c768b899942736cb1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MdbLcbNow5GJQw0NYGy-gQ.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">As the size of the networks (H) keeps increasing, train and test errors keep decreasing and overfitting does not happen. [11]</figcaption></figure><p id="8c34" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">When it comes to big networks and the effect of parameter count on generalization you can find numerous papers and blog posts, some even contradicting others.</p><p id="23dc" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">Our current understanding could suggest that larger networks can generalize well despite their tendency to overfit. This could be due to their depth, allowing learning of more complex patterns when compared to shallow networks. This is mostly domain-dependant — certain data types may benefit from smaller models and by following Occam’s razor principle (don’t miss this post for a further read👇).</p><figure class="ms mt mu mv mw mj"><div class="rp io l ed"><div class="rq rr l"/></div></figure><h1 id="f626" class="pb pc fq bf pd pe pf gq pg ph pi gt pj pk pl pm pn po pp pq pr ps pt pu pv pw bk">3. Implicit Regularization in Neural Networks</h1><p id="5670" class="pw-post-body-paragraph nm nn fq np b go px nr ns gr py nu nv nw pz ny nz oa qa oc od oe qb og oh oi fj bk">At the heart of machine learning lies Gradient Descent — the steps we take to find the local minima in a loss landscape. Gradient Descent (GD), along with Stochastic Gradient Descent (SGD) is one of the first that is digested by anyone starting to learn machine learning.</p><p id="2b4e" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">As the algorithm seems straightforward, one might expect that it does not have much depth. However, you can never find the button of the pool in machine learning.</p><p id="b107" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">Do neural networks benefit from an implicit regularization by Gradient Descent that pushes them to find <em class="no">simpler </em>and <em class="no">more general </em>solutions? Could this be the reason why over-parametrized networks generalize as shown in the previous part?</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rs"><img src="../Images/304be222467e499f2e5c6c5d56844e50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wDDffJESony5N7BvGdi-iw.gif"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Gradient Descent in 2D. (Source: <a class="af oj" href="https://en.wikipedia.org/wiki/File:Gradient_Descent_in_2D.webm" rel="noopener ugc nofollow" target="_blank">Wikimedia Commons</a>)</figcaption></figure><p id="c4b6" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">There are two experiments you need to pay close attention to:</p><h2 id="2783" class="qd pc fq bf pd qe qf qg pg qh qi qj pj nw qk ql qm oa qn qo qp oe qq qr qs qt bk">Experiment 1</h2><p id="02a3" class="pw-post-body-paragraph nm nn fq np b go px nr ns gr py nu nv nw pz ny nz oa qa oc od oe qb og oh oi fj bk">When the authors of [11] trained models on CIFAR-10 and MNIST datasets using SGD and no explicit regularization, they concluded that as the size of the network increases, the test and training errors keep decreasing. This goes against the belief that bigger networks have a higher test error because of overfitting. Even after adding more and more parameters to the network, the generalization error does not increase. Then they forced the network to overfit by adding random label noise. As shown in the figure below, even with 5% random labels, the test error decreases further and there are no significant signs of overfitting.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rt"><img src="../Images/99f78afda3879ffd5d8b592f2199a524.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rrfMBMCcwYNKf6w67_8sMQ.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Test and Train error of a network with increasing size (H) and 5% noise to the labels. Left is MNIST and right is CIFAR-10. [11]</figcaption></figure><h2 id="ff53" class="qd pc fq bf pd qe qf qg pg qh qi qj pj nw qk ql qm oa qn qo qp oe qq qr qs qt bk">Experiment 2</h2><p id="aa1f" class="pw-post-body-paragraph nm nn fq np b go px nr ns gr py nu nv nw pz ny nz oa qa oc od oe qb og oh oi fj bk">An important paper, <em class="no">In Search of the Real Inductive Bias</em> [12], experiments by fitting a predictor using linearly separable datasets. The authors show how logistic regression, using gradient descent and no regularization, inherently biases the solution towards the maximum-margin separator (also known as hard margin SVM). This is an interesting and surprising behavior of gradient descent. Because even though the loss and the optimization<strong class="np fr"> don’t directly involve</strong> any terms that encourage margin maximization (like those you find in Support Vector Machines), gradient descent inherently biases the solution towards a max-margin classifier.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div class="mp mq ru"><img src="../Images/67d2bde1fbdf0706ff73b0f3baeddee9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*GdiUbwDZlSbOpu9Z"/></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">H3 represents how a hard-margin SVM would classify the dataset. (Source: <a class="af oj" href="https://commons.wikimedia.org/wiki/File:Svm_separating_hyperplanes_(SVG).svg" rel="noopener ugc nofollow" target="_blank">Wikimedia Commons</a>)</figcaption></figure><h2 id="85f8" class="qd pc fq bf pd qe qf qg pg qh qi qj pj nw qk ql qm oa qn qo qp oe qq qr qs qt bk">Gradient Descent as a natural Reguralizer</h2><p id="b23e" class="pw-post-body-paragraph nm nn fq np b go px nr ns gr py nu nv nw pz ny nz oa qa oc od oe qb og oh oi fj bk">The experiments suggest an implicit regularization effect as if the optimization process favors simpler and more stable solutions. This implies that GD has a preference for simpler models, often converging to a special type of local minima referred to as “flat” minima, which tends to have <strong class="np fr">lower generalization error </strong>compared to sharper minima. This helps explain why deep learning models often perform well on real-world tasks beyond the training data. This suggests that the optimization process itself can be considered a form of implicit regularization, leading to models that are not only minimal in error on the training data, but also robust in their prediction of unseen data. A full theoretical explanation remains an active area of research.</p><p id="7781" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">Perhaps this article could also be interesting to you, on how and why deep neural networks are converging into a unified representation of reality:</p><div class="rv rw rx ry rz sa"><a rel="noopener follow" target="_blank" href="/platonic-representation-hypothesis-c812813d7248?source=post_page-----699e0002a057--------------------------------"><div class="sb ab ig"><div class="sc ab co cb sd se"><h2 class="bf fr hw z io sf iq ir sg it iv fp bk">Platonic Representation: Are AI Deep Network Models Converging?</h2><div class="sh l"><h3 class="bf b hw z io sf iq ir sg it iv dx">Are Artificial Intelligence models evolving towards a unified representation of reality? The Platonic Representation…</h3></div><div class="si l"><p class="bf b dy z io sf iq ir sg it iv dx">towardsdatascience.com</p></div></div><div class="sj l"><div class="sk l sl sm sn sj so lr sa"/></div></div></a></div><h1 id="b35e" class="pb pc fq bf pd pe pf gq pg ph pi gt pj pk pl pm pn po pp pq pr ps pt pu pv pw bk">4. The Lottery Ticket Hypothesis</h1><p id="a327" class="pw-post-body-paragraph nm nn fq np b go px nr ns gr py nu nv nw pz ny nz oa qa oc od oe qb og oh oi fj bk">Model pruning can reduce the parameters of a trained neural network by 90%. If done correctly, this could be achieved without dropping the accuracy. But you can only prune your model <strong class="np fr">after</strong> it has been trained. If we could manage to remove the excess parameters <strong class="np fr">before </strong>training, this would mean using much less time and resources.</p><p id="7917" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">The Lottery Ticket Hypothesis [13] argues that a neural network contains subnetworks that when trained in isolation, can reach test accuracy comparable to the original network. These subnetworks — <em class="no">the winning tickets,</em> have the initial weights that make their training successful — <em class="no">the lottery</em>.</p><p id="fe1f" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">The authors find these subnetworks through an <strong class="np fr">iterative pruning </strong>method:</p><ul class=""><li id="8a0c" class="nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi ot ou ov bk"><strong class="np fr">Training the Network</strong>: First, they train the original unpruned network.</li><li id="7111" class="nm nn fq np b go ow nr ns gr ox nu nv nw oy ny nz oa oz oc od oe pa og oh oi ot ou ov bk"><strong class="np fr">Pruning</strong>: After training, they prune <strong class="np fr">p%</strong> of the weights.</li><li id="14e9" class="nm nn fq np b go ow nr ns gr ox nu nv nw oy ny nz oa oz oc od oe pa og oh oi ot ou ov bk"><strong class="np fr">Resetting Weights</strong>: The remaining weights are set to their original values from the initial initialization.</li><li id="9262" class="nm nn fq np b go ow nr ns gr ox nu nv nw oy ny nz oa oz oc od oe pa og oh oi ot ou ov bk"><strong class="np fr">Retraining</strong>: The pruned network is retrained to see if it can reach the same or higher performance than the previous network.</li><li id="3d71" class="nm nn fq np b go ow nr ns gr ox nu nv nw oy ny nz oa oz oc od oe pa og oh oi ot ou ov bk"><strong class="np fr">Repeat</strong>: Until a desired sparsity of the original network is achieved, or the pruned network can no longer match the performance of the unpruned network, this process is repeated.</li></ul><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq sp"><img src="../Images/40897b86443a85999f980a18cdfaea41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w6RjzMq6-MUtArIurhnJYQ.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Iterative Pruning used in the Lottery Ticket Hypothesis paper. (by author)</figcaption></figure><p id="9c52" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">The proposed method of iterative training is computationally expensive, requiring training a network 15 times or more on multiple experiments.</p><p id="600f" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">It remains an area of research why we have such phenomena in neural networks. Could it be true that SGD only focuses on the winning tickets when training the network and not the full body of the network? Why do certain random initializations contain such highly effective sub-networks? If you want to dive deep into this theory, don’t miss out on [13] and [14].</p></div></div></div><div class="ab cb sq sr ss st" role="separator"><span class="su by bm sv sw sx"/><span class="su by bm sv sw sx"/><span class="su by bm sv sw"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="821e" class="pb pc fq bf pd pe sy gq pg ph sz gt pj pk ta pm pn po tb pq pr ps tc pu pv pw bk"><span class="l ol om on bo oo op oq or os ed">F</span>inal Word.</h1><p id="f7ec" class="pw-post-body-paragraph nm nn fq np b go px nr ns gr py nu nv nw pz ny nz oa qa oc od oe qb og oh oi fj bk"><em class="no">Thank you for reading through the article! </em>I have tried my best to provide an accurate article, however, please share your opinions and suggestions if you think any modifications are required.</p><h2 id="03ee" class="qd pc fq bf pd qe qf qg pg qh qi qj pj nw qk ql qm oa qn qo qp oe qq qr qs qt bk">Let’s Connect!</h2><p id="bcd3" class="pw-post-body-paragraph nm nn fq np b go px nr ns gr py nu nv nw pz ny nz oa qa oc od oe qb og oh oi fj bk"><em class="no">Subscribe for FREE to be notified of new articles! You can also find me on </em><a class="af oj" href="https://www.linkedin.com/in/hesamsheikh/" rel="noopener ugc nofollow" target="_blank"><em class="no">LinkedIn</em></a><em class="no"> and </em><a class="af oj" href="https://x.com/itsHesamSheikh" rel="noopener ugc nofollow" target="_blank"><em class="no">Twitter</em></a><em class="no">.</em></p><h1 id="397f" class="pb pc fq bf pd pe pf gq pg ph pi gt pj pk pl pm pn po pp pq pr ps pt pu pv pw bk">Further Reads</h1><p id="cb6c" class="pw-post-body-paragraph nm nn fq np b go px nr ns gr py nu nv nw pz ny nz oa qa oc od oe qb og oh oi fj bk">If you have reached so far, you might also find these articles interesting:</p><div class="rv rw rx ry rz sa"><a href="https://pub.towardsai.net/learn-anything-with-ai-and-the-feynman-technique-00a33f6a02bc?source=post_page-----699e0002a057--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="sb ab ig"><div class="sc ab co cb sd se"><h2 class="bf fr hw z io sf iq ir sg it iv fp bk">Learn Anything with AI and the Feynman Technique</h2><div class="sh l"><h3 class="bf b hw z io sf iq ir sg it iv dx">study any concept in four easy steps, by applying AI and a Noble Prize winner approach</h3></div><div class="si l"><p class="bf b dy z io sf iq ir sg it iv dx">pub.towardsai.net</p></div></div><div class="sj l"><div class="td l sl sm sn sj so lr sa"/></div></div></a></div><div class="rv rw rx ry rz sa"><a rel="noopener follow" target="_blank" href="/a-comprehensive-guide-to-collaborative-ai-agents-in-practice-1f4048947d9c?source=post_page-----699e0002a057--------------------------------"><div class="sb ab ig"><div class="sc ab co cb sd se"><h2 class="bf fr hw z io sf iq ir sg it iv fp bk">A Comprehensive Guide to Collaborative AI Agents in Practice</h2><div class="sh l"><h3 class="bf b hw z io sf iq ir sg it iv dx">the definition, and building a team of agents that refine your CV and Cover Letter for job applications</h3></div><div class="si l"><p class="bf b dy z io sf iq ir sg it iv dx">towardsdatascience.com</p></div></div><div class="sj l"><div class="te l sl sm sn sj so lr sa"/></div></div></a></div><div class="rv rw rx ry rz sa"><a href="https://pub.towardsai.net/chatgpt-as-a-game-engine-to-play-flappy-bird-ee4adff46f48?source=post_page-----699e0002a057--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="sb ab ig"><div class="sc ab co cb sd se"><h2 class="bf fr hw z io sf iq ir sg it iv fp bk">I Played Flappy Bird in ChatGPT</h2><div class="sh l"><h3 class="bf b hw z io sf iq ir sg it iv dx">GPT-4 is fantastic, but is it good enough to be a Game Engine? I tried this with Flappy Bird, using a simple LangChain…</h3></div><div class="si l"><p class="bf b dy z io sf iq ir sg it iv dx">pub.towardsai.net</p></div></div><div class="sj l"><div class="tf l sl sm sn sj so lr sa"/></div></div></a></div><h1 id="f3e3" class="pb pc fq bf pd pe pf gq pg ph pi gt pj pk pl pm pn po pp pq pr ps pt pu pv pw bk">References</h1><p id="b908" class="pw-post-body-paragraph nm nn fq np b go px nr ns gr py nu nv nw pz ny nz oa qa oc od oe qb og oh oi fj bk">[1] Ioffe, S., &amp; Szegedy, C. (2015). Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. <a class="af oj" href="https://arxiv.org/abs/1502.03167" rel="noopener ugc nofollow" target="_blank">arXiv</a>.</p><p id="fb12" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">[2] <a class="af oj" href="https://www.deeplearning.ai/the-batch/outside-the-norm/" rel="noopener ugc nofollow" target="_blank">Outside the Norm</a>, DeepLearning.AI</p><p id="2b9b" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">[3] Santurkar, Shibani; Tsipras, Dimitris; Ilyas, Andrew; Madry, Aleksander (29 May 2018). “How Does Batch Normalization Help Optimization?”. arXiv:<a class="af oj" href="https://arxiv.org/abs/1805.11604" rel="noopener ugc nofollow" target="_blank">1805.11604</a></p><p id="b458" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">[4] Li, H., Xu, Z., Taylor, G., Studer, C., &amp; Goldstein, T. (2018). Visualizing the Loss Landscape of Neural Nets. <a class="af oj" href="https://arxiv.org/abs/1712.09913" rel="noopener ugc nofollow" target="_blank">arXiv</a></p><p id="3ab4" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">[5] <a class="af oj" href="https://www.alexirpan.com/2017/04/26/perils-batch-norm.html" rel="noopener ugc nofollow" target="_blank">On The Perils of Batch Norm</a></p><p id="b628" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">[6] <a class="af oj" href="https://x.com/svpino/status/1588501331316121601" rel="noopener ugc nofollow" target="_blank">https://x.com/svpino/status/1588501331316121601</a></p><p id="3f0f" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">[7] Neyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y., &amp; Srebro, N. (2018). Towards Understanding the Role of Over-Parametrization in Generalization of Neural Networks. <a class="af oj" href="https://arxiv.org/abs/1805.12076" rel="noopener ugc nofollow" target="_blank">arXiv</a>.</p><p id="fc7a" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">[8] <a class="af oj" href="https://cs.stackexchange.com/questions/75327/why-is-deep-learning-hyped-despite-bad-vc-dimension" rel="noopener ugc nofollow" target="_blank">Why is deep learning hyped despite bad VC dimension?</a></p><p id="a12c" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">[9] <a class="af oj" href="https://openreview.net/pdf?id=rJv6ZgHYg" rel="noopener ugc nofollow" target="_blank">DEEP NETS DON’T LEARN VIA MEMORIZATION</a></p><p id="f506" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">[10] Zhang, C., Bengio, S., Hardt, M., Recht, B., &amp; Vinyals, O. (2017). Understanding Deep Learning Requires Rethinking Generalization. <a class="af oj" href="https://arxiv.org/abs/1611.03530" rel="noopener ugc nofollow" target="_blank"><em class="no">arXiv:1611.03530</em></a></p><p id="2d29" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">[11] Neyshabur, B., Tomioka, R., &amp; Srebro, N. (2015). In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning. <em class="no">arXiv:1412.6614</em></p><p id="383a" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">[12] Soudry, D., Hoffer, E., Nacson, M. S., Gunasekar, S., &amp; Srebro, N. (2017). The Implicit Bias of Gradient Descent on Separable Data. <em class="no">arXiv:1710.10345</em></p><p id="72b2" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">[13] Frankle, J., &amp; Carbin, M. (2019). The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks. <em class="no">arXiv:1803.03635</em></p><p id="cce7" class="pw-post-body-paragraph nm nn fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">[14] <a class="af oj" href="https://www.lesswrong.com/posts/Z7R6jFjce3J2Ryj44/exploring-the-lottery-ticket-hypothesis" rel="noopener ugc nofollow" target="_blank">https://www.lesswrong.com/posts/Z7R6jFjce3J2Ryj44/exploring-the-lottery-ticket-hypothesis</a></p></div></div></div></div>    
</body>
</html>