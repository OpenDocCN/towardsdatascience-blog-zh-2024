# 目标是选择具有最大影响力的变体

> 原文：[https://towardsdatascience.com/targeting-variants-for-maximum-impact-bdf26213d7bc?source=collection_archive---------13-----------------------#2024-08-30](https://towardsdatascience.com/targeting-variants-for-maximum-impact-bdf26213d7bc?source=collection_archive---------13-----------------------#2024-08-30)

## 如何使用因果推断来改善关键的商业指标

[](https://medium.com/@alexander.polyakov?source=post_page---byline--bdf26213d7bc--------------------------------)[![Alexander Polyakov](../Images/d643bc58ff3b39274b3d52d5ee2c1672.png)](https://medium.com/@alexander.polyakov?source=post_page---byline--bdf26213d7bc--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--bdf26213d7bc--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--bdf26213d7bc--------------------------------) [Alexander Polyakov](https://medium.com/@alexander.polyakov?source=post_page---byline--bdf26213d7bc--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--bdf26213d7bc--------------------------------) ·7分钟阅读·2024年8月30日

--

[**Egor Kraev**](https://www.linkedin.com/in/egorkraev/) **和** [**Alexander Polyakov**](https://www.linkedin.com/in/alxdr-p/)

![](../Images/db54a5a09fa2eddf2447e6c262e37065.png)

图片由作者提供

假设你想给客户发送一封电子邮件，或者对你的客户界面进行更改，并且你有多个变体可以选择。**你如何选择最佳选项？**

最简单的方法是进行**A/B/N测试**，将每个变体展示给你的客户的随机子样本，并选择得到最佳平均响应的变体。然而，这种方法假设所有客户都有相同的偏好，且隐含地认为客户之间的差异仅仅是需要平均的噪音。我们能做得比这更好吗？能否根据客户的可观察特征，为每个客户选择最适合的变体？

在评估实验结果时，真正的挑战在于根据可观察到的客户特征衡量每个变体的相对影响力。这并不像听起来那么简单。我们不仅仅关心具有特定特征的客户接收某个变体后的结果，还关心这个变体的影响力，即与另一个变体相比，结果的差异。

与结果本身不同，影响力是不可直接观察的。例如，我们无法同时向同一个客户发送和不发送相同的电子邮件。这带来了一个重大挑战。我们如何解决这个问题呢？

**答案有两个层面：**第一，我们如何最大化任务分配的影响？其次，一旦我们选择了一个分配策略，我们如何最好地衡量它与纯随机分配相比的表现？

# **比较不同任务分配的表现**

第二个问题的答案比第一个更简单。做这件事的直觉方法是将你的客户群体分成两组，一组是完全随机分配变体，另一组则是根据最大化影响的最佳分配策略来分配——然后比较结果。然而，这样做是浪费的：每组的样本量只有总样本量的一半，因此你的平均结果会更加嘈杂；而且更有针对性的分配只能让样本中的一半客户享受到好处。

**幸运的是，有一种更好的方法：**首先，你应该让你的有针对性的分配也带有一定的随机性，只是偏向你认为在每种情况下最好的选项。这是合理的，因为你永远不能确定什么是每个特定客户的最佳选择；而且它允许你在收获已知成果的同时继续学习。

其次，当你收集到使用特定变体分配策略的实验结果时，你可以使用一种统计方法叫做**ERUPT**（或政策值）来获得任何其他分配策略，特别是随机分配变体的平均结果的无偏估计。听起来像魔法吗？不，这只是数学。查看[**ERUPT基础**](https://github.com/py-why/causaltune/blob/main/notebooks/ERUPT%20basics.ipynb)中的笔记本，了解一个简单的示例。

# **寻找最佳任务分配**

![](../Images/ce9aa4e665a1c9b8ff303a87eb429fb7.png)

图片来源：作者

能够根据来自单一实验的数据比较不同任务的影响是非常棒的，但我们如何知道哪个任务分配策略是最好的呢？在这里，**CausalTune**再次发挥了作用。

我们如何解决上面提到的挑战——估算向同一客户展示不同变体所产生的结果差异——因为我们无法直接观察这种差异呢？顺便提一下，这种估算被称为**提升建模**，它是一种特定的**因果建模**。

直觉方法是将展示给每个客户的变体视为客户的另一个特征，并在结果特征和结果集上拟合你最喜欢的回归模型，例如**XGBoost**。然后，你可以查看如果我们仅改变“特征”中变体的值，拟合模型对于某个客户的预测会发生多大变化，并将此作为影响估计。这个方法被称为**S-Learner**。它简单直观，但根据我们的经验，通常表现非常糟糕。

你可能会想，我们怎么知道它表现得非常糟糕，既然我们无法直接观察到影响？一种方法是查看合成数据，在那里我们知道正确的答案。

但是，有没有一种方法可以在真实世界数据中评估影响力估计的质量呢，在这种情况下，真实值在任何特定案例中都是不可知的？事实证明是有的，我们认为我们的方法在这一领域具有原创贡献。假设有一个简单的情形，只有两种变体——对照组（没有处理）和实验组。对于一组给定的处理影响力估计（来自我们希望评估的特定模型），如果我们将该估计从处理样本的实际结果中减去，我们预计处理样本和未处理样本的（特征、结果）组合将具有完全相同的分布。毕竟，它们是从同一个总体中随机抽样的！现在，我们所需要做的就是量化这两个分布的相似性，这样我们就得到了一个影响力估计的评分。

现在你可以为不同的提升模型打分，你可以对它们的类型和超参数进行搜索（这正是 **CausalTune** 的用途），并选择最佳的影响力估计器。

**CausalTune** 目前支持两种评分方式，**ERUPT** 和能量距离。详情请参见原始的[**CausalTune 论文**](https://arxiv.org/abs/2212.10076)。

# **使用 CausalTune 优化影响力分配**

如何在实践中利用这一点，以最大化你期望的结果，例如点击率？

你首先选择你的总体可达客户群体，并将其分为两部分。你从一个完全随机的变体分配实验开始，或者根据你之前的信念使用某些启发式方法。此时，至关重要的是，不论你的信念有多强，你总是要在每个分配中保留一些随机性 —— 你只能根据客户特征调整分配的概率，但绝不能让它们完全变为确定性的分配 —— 否则，你将无法从实验中学到更多！

一旦第一次实验的结果出来后，你可以首先像上述所述，使用 **ERUPT** 来估计你使用启发式分配方法相比完全随机分配所带来的平均结果的改进。但更重要的是，你现在可以在实验结果上拟合 **CausalTune**，从而根据客户特征生成实际的影响力估计！

然后，你可以利用这些估计来创建一个新的、更好的分配策略（可以通过为每个客户选择具有最高影响力估计的变体，或者更好的是，使用 **汤普森抽样** 在利用现有知识的同时持续学习），并将其应用于第二个实验，针对其余可达客户群体。

最后，你可以在第二次实验的结果上使用 **ERUPT**，以确定你新的策略相比随机策略以及之前的启发式策略的超越表现。

# **Wise 案例研究：优化点击率**

我们在Wise的数据科学团队工作，拥有许多实际的因果推断和提升模型的应用案例。这里有一个Wise早期应用的故事，我们几乎就是这么做的。该电子邮件营销活动的目标是向现有的Wise客户推荐他们应该尝试的下一个产品。第一波电子邮件使用了一个简单的模型，我们通过查看现有客户使用每个产品的初次使用顺序，训练了一个梯度提升模型，预测给定前几个元素后，该序列中的最后一个元素是什么，不使用其他任何数据。

在随后的电子邮件营销活动中，我们使用该模型的预测来偏向分配，最终得到了**1.90%**的点击率——而根据**ERUPT**估算，相同实验结果下随机分配的点击率为**1.74%**。

然后，我们在这些数据上训练了**CausalTune**，并使用该模型制定了两种新的变体分配策略。第一个策略是**“贪心”**的，即始终选择模型预测最高影响的变体。第二个策略不仅使用了影响估算值，还使用了它们的标准差（也由模型提供），并采用了[**Thompson采样**](https://en.wikipedia.org/wiki/Thompson_sampling)来生成分配概率。

贪心分配的样本外**ERUPT**估算为**2.18%**，而**Thompson采样**策略为**2.22%**，相比随机分配提高了**25%**！

一个令人惊讶的发现是，尽管**Thompson采样**策略具有随机性，但其估计效果不逊色于贪心策略。这是一个好消息，因为像**Thompson采样**这样的随机策略使我们能够继续从下一个实验结果中学习，同时最大化利用我们已有的知识。

因此，我们建议使用**Thompson采样**从拟合的因果模型中创建策略，而不是使用贪心方法——更多内容将在下一篇文章中介绍。

我们目前正在准备该实验的第二波，以观察**ERUPT**预测的增益是否会在真实的点击率中显现。

# **结论**

[**CausalTune**](https://github.com/py-why/causaltune/tree/main) 为您提供了一个独特的、创新的工具包，帮助您优化个体客户的目标定位，以最大化期望的结果，比如点击率。我们的**AutoML**因果估计器可以可靠地估算不同变量对客户行为的影响，而**ERUPT**估计器允许您将实际实验的平均结果与其他分配选项进行比较，提供了无需牺牲样本量的性能测量。
