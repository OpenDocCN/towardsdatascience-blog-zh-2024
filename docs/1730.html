<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The LLM Triangle Principles to Architect Reliable AI Apps</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>The LLM Triangle Principles to Architect Reliable AI Apps</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e?source=collection_archive---------0-----------------------#2024-07-16">https://towardsdatascience.com/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e?source=collection_archive---------0-----------------------#2024-07-16</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="25a3" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Software design principles for thoughtfully designing reliable, high-performing LLM applications. A framework to bridge the gap between potential and production-grade performance.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@almogbaku?source=post_page---byline--d3753dd8542e--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Almog Baku" class="l ep by dd de cx" src="../Images/3ac36986f6ca0ba56c8edced6ec7dd07.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*-aPltT37mZxh8ag6FPhcHw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--d3753dd8542e--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@almogbaku?source=post_page---byline--d3753dd8542e--------------------------------" rel="noopener follow">Almog Baku</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--d3753dd8542e--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">16 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 16, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">16</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="3ceb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Large Language Models (LLMs) hold immense potential, but developing reliable production-grade applications remains challenging. After building dozens of LLM systems, I’ve distilled the formula for success into 3+1 fundamental principles that any team can apply.</p><blockquote class="nf"><p id="c385" class="ng nh fq bf ni nj nk nl nm nn no ne dx">“LLM-Native apps are 10% sophisticated model, and 90% experimenting data-driven engineering work.”</p></blockquote><p id="f678" class="pw-post-body-paragraph mj mk fq ml b go np mn mo gr nq mq mr ms nr mu mv mw ns my mz na nt nc nd ne fj bk">Building production-ready LLM applications requires <em class="nu">careful engineering practices</em>. When users cannot interact <em class="nu">directly </em>with the LLM, the prompt must be meticulously composed to cover all nuances, as <em class="nu">iterative user feedback may be unavailable</em>.</p></div></div></div><div class="ab cb nv nw nx ny" role="separator"><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="4528" class="od oe fq bf of og oh gq oi oj ok gt ol om on oo op oq or os ot ou ov ow ox oy bk">Introducing the LLM Triangle Principles</h1><p id="77cf" class="pw-post-body-paragraph mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne fj bk">The LLM Triangle Principles encapsulate the essential guidelines for building effective LLM-native apps. They provide a solid conceptual framework, guide developers in constructing robust and reliable LLM-native applications, and offer direction and support.</p><figure class="ph pi pj pk pl pm pe pf paragraph-image"><div role="button" tabindex="0" class="pn po ed pp bh pq"><div class="pe pf pg"><img src="../Images/6a29d388a95ba8611ab9ea94a2a9665d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w3BnBFXuGO2TioQG_hpl6g.jpeg"/></div></div><figcaption class="ps pt pu pe pf pv pw bf b bg z dx">An optimal LLM Usage is achieved by optimizing the three prominent principles through the lens of the SOP. (Image by author)</figcaption></figure><h2 id="609d" class="px oe fq bf of py pz qa oi qb qc qd ol ms qe qf qg mw qh qi qj na qk ql qm qn bk">The Key Apices</h2><p id="6de0" class="pw-post-body-paragraph mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne fj bk">The LLM Triangle Principles introduces four programming principles to help you design and build LLM-Native apps.</p><p id="b48f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The first principle is the <em class="nu">Standard Operating Procedure (</em><strong class="ml fr"><em class="nu">SOP</em></strong><em class="nu">).</em> The SOP guides the three apices of our triangle: <strong class="ml fr"><em class="nu">Model</em></strong>, <strong class="ml fr"><em class="nu">Engineering Techniques</em>,</strong> and <strong class="ml fr"><em class="nu">Contextual Data</em></strong>.</p><p id="88bd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Optimizing the three apices principles <em class="nu">through the lens</em> of the <strong class="ml fr">SOP is the key</strong> <strong class="ml fr">to ensuring a high-performing</strong> LLM-native app.</p></div></div></div><div class="ab cb nv nw nx ny" role="separator"><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="be9a" class="od oe fq bf of og oh gq oi oj ok gt ol om on oo op oq or os ot ou ov ow ox oy bk">1. Standard Operating Procedure (SOP)</h1><p id="30e1" class="pw-post-body-paragraph mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne fj bk"><a class="af qo" href="https://en.wikipedia.org/wiki/Standard_operating_procedure" rel="noopener ugc nofollow" target="_blank"><strong class="ml fr">S</strong>tandard <strong class="ml fr">O</strong>perating <strong class="ml fr">P</strong>rocedure (SOP)</a> is a well-known terminology in the industrial world. It’s a set of step-by-step instructions compiled by large organizations to help their workers carry out routine operations while maintaining high-quality and similar results each time. This practically turns inexperienced or low-skilled workers into experts by writing detailed instructions.</p><p id="c98f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The LLM Triangle Principles borrow the SOP paradigm and encourage you to <strong class="ml fr">consider the model as an inexperienced/unskilled worker</strong>. We can ensure higher-quality results by “teaching” the model how an expert would perform this task.</p><figure class="ph pi pj pk pl pm pe pf paragraph-image"><div class="pe pf qp"><img src="../Images/3c52e5439c5a35d0aa068834dbcf132b.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*hpzB20QXgpKfhTlNoaAWOQ.jpeg"/></div><figcaption class="ps pt pu pe pf pv pw bf b bg z dx">The SOP <strong class="bf of"><em class="qq">guiding</em></strong> principle. (image by author)</figcaption></figure><blockquote class="nf"><p id="1932" class="ng nh fq bf ni nj qr qs qt qu qv ne dx">“Without an SOP, even the most powerful LLM will fail to deliver consistently high-quality results.”</p></blockquote><p id="ca13" class="pw-post-body-paragraph mj mk fq ml b go np mn mo gr nq mq mr ms nr mu mv mw ns my mz na nt nc nd ne fj bk">When thinking about the <strong class="ml fr">SOP <em class="nu">guiding</em> principle</strong>, we should identify what techniques will help us implement the SOP most effectively.</p><h2 id="1210" class="px oe fq bf of py pz qa oi qb qc qd ol ms qe qf qg mw qh qi qj na qk ql qm qn bk">1.1. Cognitive modeling</h2><p id="5325" class="pw-post-body-paragraph mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne fj bk">To create an SOP, we need to take our best-performing workers (domain experts), model how they think and work to achieve the same results, and write down everything they do.</p><p id="8173" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">After editing and formalizing it, we’ll have detailed instructions to help every inexperienced or low-skilled worker succeed and yield excellent work.</p><p id="b2cf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Like humans, it’s essential to <em class="nu">reduce the cognitive load</em> of the task by simplifying or splitting it. Following a simple step-by-step instruction is more straightforward than a lengthy, complex procedure.</p><p id="f8c9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">During this process, we identify the hidden <a class="af qo" href="https://en.wikipedia.org/wiki/Implicit_cognition" rel="noopener ugc nofollow" target="_blank"><em class="nu">implicit cognition</em></a><em class="nu"> “jumps”</em> — the small, unconscious steps experts take that significantly impact the outcome. These subtle, unconscious, often unspoken assumptions or decisions can substantially affect the final result.</p><figure class="ph pi pj pk pl pm pe pf paragraph-image"><div role="button" tabindex="0" class="pn po ed pp bh pq"><div class="pe pf pu"><img src="../Images/6e8abc91b025fdeea3cfe961d6f6111e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j1jLgh9MviivTHhMpgSC3A.jpeg"/></div></div><figcaption class="ps pt pu pe pf pv pw bf b bg z dx">An example of an “implicit cognition jump.” (Image by author)</figcaption></figure><p id="2d83" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For example, let’s say we want to model an SQL analyst. We’ll start by interviewing them and ask them a few questions, such as:</p><ul class=""><li id="b0f0" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qw qx qy bk">What do you do when you are asked to analyze a business problem?</li><li id="ef5b" class="mj mk fq ml b go qz mn mo gr ra mq mr ms rb mu mv mw rc my mz na rd nc nd ne qw qx qy bk">How do you make sure your solution meets the request?</li><li id="730f" class="mj mk fq ml b go qz mn mo gr ra mq mr ms rb mu mv mw rc my mz na rd nc nd ne qw qx qy bk">&lt;reflecting the process as we understand to the interviewee&gt;</li><li id="ce11" class="mj mk fq ml b go qz mn mo gr ra mq mr ms rb mu mv mw rc my mz na rd nc nd ne qw qx qy bk">Does this accurately capture your process? &lt;getting corrections&gt;</li><li id="98e8" class="mj mk fq ml b go qz mn mo gr ra mq mr ms rb mu mv mw rc my mz na rd nc nd ne qw qx qy bk">Etc.</li></ul><figure class="ph pi pj pk pl pm pe pf paragraph-image"><div role="button" tabindex="0" class="pn po ed pp bh pq"><div class="pe pf re"><img src="../Images/4f537cd288b3ad06ed31c92d586b5af4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qr5NKqtUiTLJvwSuuEPlCQ.jpeg"/></div></div><figcaption class="ps pt pu pe pf pv pw bf b bg z dx">An example of the cognitive process that the analyst does and how to model it. (Image by author)</figcaption></figure><p id="59f4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The implicit cognition process takes many shapes and forms; a typical example is a “domain-specific definition.” For example, “bestseller” might be a prominent term for our domain expert, but not for everyone else.</p><figure class="ph pi pj pk pl pm pe pf paragraph-image"><div role="button" tabindex="0" class="pn po ed pp bh pq"><div class="pe pf rf"><img src="../Images/66542e24426dcd8ad82a02786a45e3fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P2sYe5l6pEjgTE_2ZsDoEg.jpeg"/></div></div><figcaption class="ps pt pu pe pf pv pw bf b bg z dx">Expanding the implicit cognition process in our SQL analyst example. (Image by author)</figcaption></figure><p id="acb4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Eventually, we’ll have a full SOP “recipe” that allows us to emulate our top-performing analyst.</p><p id="12d5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">When mapping out these complex processes, it can be helpful to visualize them as a graph. This is especially helpful when the process is nuanced and involves many steps, conditions, and splits.</p><figure class="ph pi pj pk pl pm pe pf paragraph-image"><div class="pe pf rg"><img src="../Images/d292b6c118ddeba744f0a0967b1a39d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*QC65l-lDLJKa9XwSIcrdlw.jpeg"/></div><figcaption class="ps pt pu pe pf pv pw bf b bg z dx">The “SQL Analyst SOP” includes all the required technical steps, visualized as a graph. (Image by author)</figcaption></figure><p id="b92d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Our final solution should mimic the steps defined in the SOP. In this stage, try to ignore the implementation—later, you can implement it across one or many steps/chains throughout our solution.</p></div></div></div><div class="ab cb nv nw nx ny" role="separator"><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="c1f8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Unlike the rest of the principles, the cognitive modeling (SOP writing) is the <em class="nu">only standalone process</em>. It’s highly recommended that you model your process before writing code. That being said, while implementing it, you might go back and change it based on new insights or understandings you gained.</p><p id="ede6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now that we understand the importance of creating a well-defined SOP, that guides our <em class="nu">business understanding</em> of the problem, let’s explore how we can effectively implement it using various engineering techniques.</p></div></div></div><div class="ab cb nv nw nx ny" role="separator"><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="3221" class="od oe fq bf of og oh gq oi oj ok gt ol om on oo op oq or os ot ou ov ow ox oy bk">2. Engineering Techniques</h1><p id="da94" class="pw-post-body-paragraph mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne fj bk"><a class="af qo" href="https://www.promptingguide.ai/" rel="noopener ugc nofollow" target="_blank">Engineering Techniques</a> help you practically implement your SOP and get the most out of the model. When thinking about the <strong class="ml fr">Engineering Techniques principle</strong>, we should consider what tools(techniques) in our toolbox can help us implement and shape our SOP and assist the model in communicating well with us.</p><figure class="ph pi pj pk pl pm pe pf paragraph-image"><div class="pe pf qp"><img src="../Images/d6a821a075378b6d2e1cdd1d807de0fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*bWKwStTLfYuwMFh80RKHJg.jpeg"/></div><figcaption class="ps pt pu pe pf pv pw bf b bg z dx">The Engineering Techniques principle. (Image by author)</figcaption></figure><p id="efea" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Some engineering techniques are only implemented in the prompt layer, while many require a software layer to be effective, and some combine both layers.</p><figure class="ph pi pj pk pl pm pe pf paragraph-image"><div class="pe pf rh"><img src="../Images/ddc3397969ad3a8ea0111ff3c09cae12.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*U6uEql1b6O5zoHhMxaUnyg.jpeg"/></div><figcaption class="ps pt pu pe pf pv pw bf b bg z dx">Engineering Techniques Layers. (Image by author)</figcaption></figure><p id="ebb8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">While many small nuances and techniques are discovered daily, I’ll cover two primary techniques: workflow/chains and agents.</p><h2 id="64b7" class="px oe fq bf of py pz qa oi qb qc qd ol ms qe qf qg mw qh qi qj na qk ql qm qn bk">2.1. LLM-Native architectures (aka flow engineering or chains)</h2><p id="0a92" class="pw-post-body-paragraph mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne fj bk">The LLM-Native Architecture describes the agentic flow your app is going through to yield the task’s result.</p><p id="28a2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Each step in our flow is a standalone process that must occur to achieve our task. Some steps will be performed simply by deterministic code; for some, we will use an LLM (agent).</p><p id="63b7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To do that, we can reflect on the Standard Operating Procedure (SOP) we drew and think:</p><ol class=""><li id="f0e2" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne ri qx qy bk">Which SOP steps should we glue together to the same agent? And what steps should we split as different agents?</li><li id="d6d7" class="mj mk fq ml b go qz mn mo gr ra mq mr ms rb mu mv mw rc my mz na rd nc nd ne ri qx qy bk">What SOP steps should be executed in a standalone manner (but they might be fed with information from previous steps)?</li><li id="0c17" class="mj mk fq ml b go qz mn mo gr ra mq mr ms rb mu mv mw rc my mz na rd nc nd ne ri qx qy bk">What SOP steps can we perform in a deterministic code?</li><li id="bf52" class="mj mk fq ml b go qz mn mo gr ra mq mr ms rb mu mv mw rc my mz na rd nc nd ne ri qx qy bk">Etc.</li></ol><figure class="ph pi pj pk pl pm pe pf paragraph-image"><div class="pe pf rj"><img src="../Images/45e4b1ee6d0a0a2ace59885dbeedee4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*WhVYk8VgXpur2UE9TT0hBQ.jpeg"/></div><figcaption class="ps pt pu pe pf pv pw bf b bg z dx">An LLM-Native Architecture example for “Wikipedia writer” based on a given SOP. (Image by author)</figcaption></figure><p id="9668" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Before navigating to the next step in our architecture/graph, we should define its key properties:</p><ul class=""><li id="b0b8" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qw qx qy bk"><strong class="ml fr">Inputs and outputs</strong> — What is the signature of this step? What is required before we can take an action? (this can also serve as an output format for an agent)</li><li id="3338" class="mj mk fq ml b go qz mn mo gr ra mq mr ms rb mu mv mw rc my mz na rd nc nd ne qw qx qy bk"><strong class="ml fr">Quality assurances—</strong>What makes the response “good enough”? Are there cases that require human intervention in the loop? What kinds of assertions can we configure?</li><li id="f64f" class="mj mk fq ml b go qz mn mo gr ra mq mr ms rb mu mv mw rc my mz na rd nc nd ne qw qx qy bk"><strong class="ml fr">Autonomous level </strong>— How much control do we need over the result’s quality? What range of use cases can this stage handle? In other words, how much can we trust the model to work independently at this point?</li><li id="4a7a" class="mj mk fq ml b go qz mn mo gr ra mq mr ms rb mu mv mw rc my mz na rd nc nd ne qw qx qy bk"><strong class="ml fr">Triggers </strong>— What is the next step? What defines the next step?</li><li id="0ed5" class="mj mk fq ml b go qz mn mo gr ra mq mr ms rb mu mv mw rc my mz na rd nc nd ne qw qx qy bk"><strong class="ml fr">Non-functional </strong>— What’s the required latency? Do we need special business monitoring here?</li><li id="b09f" class="mj mk fq ml b go qz mn mo gr ra mq mr ms rb mu mv mw rc my mz na rd nc nd ne qw qx qy bk"><strong class="ml fr">Failover control </strong>— What kind of failures(systematic and agentic) can occur? What are our fallbacks?</li><li id="d468" class="mj mk fq ml b go qz mn mo gr ra mq mr ms rb mu mv mw rc my mz na rd nc nd ne qw qx qy bk"><strong class="ml fr">State management</strong> — Do we need a special state management mechanism? How do we retrieve/save states (define the indexing key)? Do we need persistence storage? What are the different usages of this state(e.g., cache, logging, etc.)?</li><li id="481e" class="mj mk fq ml b go qz mn mo gr ra mq mr ms rb mu mv mw rc my mz na rd nc nd ne qw qx qy bk">Etc.</li></ul><h2 id="e1f6" class="px oe fq bf of py pz qa oi qb qc qd ol ms qe qf qg mw qh qi qj na qk ql qm qn bk">2.2. What are agents?</h2><p id="9328" class="pw-post-body-paragraph mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne fj bk">An LLM agent is a standalone component of an LLM-Native architecture that involves calling an LLM.</p><p id="aa98" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It’s an instance of LLM usage with the prompt containing the context. Not all agents are equal — Some will use “tools,” some won’t; some might be used “just once” in the flow, while others can be called recursively or multiple times, carrying the previous input and outputs.</p><h2 id="71ad" class="px oe fq bf of py pz qa oi qb qc qd ol ms qe qf qg mw qh qi qj na qk ql qm qn bk">2.2.1. Agents with tools</h2><p id="e3ac" class="pw-post-body-paragraph mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne fj bk">Some LLM agents can use “tools” — predefined functions for tasks like calculations or web searches. The agent outputs instructions specifying the tool and input, which the application executes, returning the result to the agent.</p><p id="66ef" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To understand the concept, let’s look at a simple prompt implementation for tool calling. This can work even with models not natively trained to call tools:</p><pre class="ph pi pj pk pl rk rl rm bp rn bb bk"><span id="27ec" class="ro oe fq rl b bg rp rq l rr rs">You are an assistant with access to these tools:<br/><br/>- calculate(expression: str) -&gt; str - calculate a mathematical expression<br/>- search(query: str) -&gt; str - search for an item in the inventory<br/><br/>Given an input, Respond with a YAML with keys: `func`(str) and `arguments`(map) or `message`(str).Given input</span></pre><p id="4379" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It’s important to distinguish between agents with tools (hence <em class="nu">autonomous agents</em>) and agents whose output can lead to performing an action.</p><blockquote class="nf"><p id="b2da" class="ng nh fq bf ni nj nk nl nm nn no ne dx">“Autonomous agents are agents that have the ability to generate a way to accomplish the task.”</p></blockquote><p id="42f7" class="pw-post-body-paragraph mj mk fq ml b go np mn mo gr nq mq mr ms nr mu mv mw ns my mz na nt nc nd ne fj bk">Autonomous agents are <em class="nu">given the right</em> to <strong class="ml fr">decide</strong> if they should act and with what action. In contrast, a (nonautonomous) agent simply “processes” our request(e.g., classification), and based on this process, our deterministic code performs an action, and the model has zero control over that.</p><figure class="ph pi pj pk pl pm pe pf paragraph-image"><div role="button" tabindex="0" class="pn po ed pp bh pq"><div class="pe pf rt"><img src="../Images/f0e9e53e6f52e3df3279712dc420d130.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iC6g4588Q0Sh8s-GRtJ7qg.jpeg"/></div></div><figcaption class="ps pt pu pe pf pv pw bf b bg z dx">An autonomous agent VS agent that triggers an action. (Image by author)</figcaption></figure><p id="d6a2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As we increase the agent’s autonomy in planning and executing tasks, we enhance its decision-making capabilities but potentially reduce control over output quality. Although this might look like a magical solution to make it more “smart” or “advanced,” it comes with the cost of losing control over the quality.</p><figure class="ph pi pj pk pl pm pe pf paragraph-image"><div class="pe pf ru"><img src="../Images/6f127e24bc282742d3b4399a1ba9ba57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*FdF07zx6tyy5Sr9OMa1djA.jpeg"/></div><figcaption class="ps pt pu pe pf pv pw bf b bg z dx">The tradeoffs of an autonomous agent. (Image by author)</figcaption></figure><p id="c426" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Beware the allure of fully autonomous agents. While their architecture might look appealing and simpler, using it for everything (or as the initial PoC) might be very deceiving from the “real production” cases. Autonomous agents are hard to debug and unpredictable(response with unstable quality), which makes them unusable for production.</p><p id="4669" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Currently, agents (without implicit guidance) are not very good at planning complex processes and usually skip essential steps. For example, in our “Wikipedia writer” use-case, they’ll just start writing and skip the systematic process. This makes agents (and autonomous agents especially) only as good as the model, or more accurately — only as good as the data they were trained on relative to your task.</p><p id="8558" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Instead of giving the agent (or a swarm of agents) the liberty to do everything end-to-end, try to hedge their task to a specific region of your flow/SOP that requires this kind of agility or creativity. This can yield higher-quality results because you can enjoy both worlds.</p><p id="3bb4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">An excellent example is <a class="af qo" href="https://www.codium.ai/blog/alphacodium-state-of-the-art-code-generation-for-code-contests/" rel="noopener ugc nofollow" target="_blank">AlphaCodium</a>: By combining a structured flow with different agents (including a novel agent that iteratively writes and tests code), they increased GPT-4 accuracy (pass@5) on CodeContests from 19% to 44%.</p><figure class="ph pi pj pk pl pm pe pf paragraph-image"><div role="button" tabindex="0" class="pn po ed pp bh pq"><div class="pe pf rv"><img src="../Images/e388535058a331dbe448aefe597b45f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lMdogwmv1N9YurJPM8Pidg.png"/></div></div><figcaption class="ps pt pu pe pf pv pw bf b bg z dx">AlphaCodium’s LLM Architecture. (Image by the curtesy <a class="af qo" href="https://www.codium.ai/" rel="noopener ugc nofollow" target="_blank">Codium.ai</a>)</figcaption></figure></div></div></div><div class="ab cb nv nw nx ny" role="separator"><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="a2a1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">While engineering techniques lay the groundwork for implementing our SOP and optimizing LLM-native applications, we must also carefully consider another critical component of the LLM Triangle: the model itself.</p></div></div></div><div class="ab cb nv nw nx ny" role="separator"><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="d33d" class="od oe fq bf of og oh gq oi oj ok gt ol om on oo op oq or os ot ou ov ow ox oy bk">3. Model</h1><p id="df82" class="pw-post-body-paragraph mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne fj bk">The model we choose is a critical component of our project’s success—a large one (such as GPT-4 or Claude Opus) might yield better results but be quite costly at scale, while a smaller model might be less “smart” but help with the budget. When thinking about the<strong class="ml fr"> Model principle</strong>, we should aim to identify our constraints and goals and what kind of model can help us fulfill them.</p><figure class="ph pi pj pk pl pm pe pf paragraph-image"><div class="pe pf qp"><img src="../Images/b2141352ce97abf24254c35db07b7d46.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*yHKkQCnyCDlGQPtTd3nQ0g.jpeg"/></div><figcaption class="ps pt pu pe pf pv pw bf b bg z dx">The Model principle. (Image by author)</figcaption></figure><blockquote class="nf"><p id="6c60" class="ng nh fq bf ni nj qr qs qt qu qv ne dx">“Not all LLMs are created equal. Match the model to the mission.”</p></blockquote><p id="96e5" class="pw-post-body-paragraph mj mk fq ml b go np mn mo gr nq mq mr ms nr mu mv mw ns my mz na nt nc nd ne fj bk">The truth is that we don’t always need the largest model; it depends on the task. To find the right match, we must have an <a class="af qo" rel="noopener" target="_blank" href="/building-llm-apps-a-clear-step-by-step-guide-1fe1e6ef60fd">experimental process</a> and try multiple variations of our solution.</p><p id="0ce0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It helps to look at our “inexperienced worker” analogy — a very “smart” worker with many academic credentials probably will succeed in some tasks easily. Still, they might be overqualified for the job, and hiring a “cheaper” candidate will be much more cost-effective.</p><p id="1b2f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">When considering a model, we should define and compare solutions based on the tradeoffs we are willing to take:</p><ul class=""><li id="6262" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qw qx qy bk"><strong class="ml fr">Task Complexity</strong> — Simpler tasks (such as summarization) are easier to complete with smaller models, while reasoning usually requires larger models.</li><li id="c2e4" class="mj mk fq ml b go qz mn mo gr ra mq mr ms rb mu mv mw rc my mz na rd nc nd ne qw qx qy bk"><strong class="ml fr">Inference infrastructure</strong> — Should it run on the cloud or edge devices? The model size might impact a small phone, but it can be tolerated for cloud-serving.</li><li id="faf1" class="mj mk fq ml b go qz mn mo gr ra mq mr ms rb mu mv mw rc my mz na rd nc nd ne qw qx qy bk"><strong class="ml fr">Pricing</strong> — What price can we tolerate? Is it cost-effective considering the business impact and predicated usage?</li><li id="9d96" class="mj mk fq ml b go qz mn mo gr ra mq mr ms rb mu mv mw rc my mz na rd nc nd ne qw qx qy bk"><strong class="ml fr">Latency</strong> — As the model grows larger, the latency grows as well.</li><li id="6b0f" class="mj mk fq ml b go qz mn mo gr ra mq mr ms rb mu mv mw rc my mz na rd nc nd ne qw qx qy bk"><strong class="ml fr">Labeled data</strong> — Do we have data we can use immediately to enrich the model with examples or relevant information that is not trained upon?</li></ul><p id="b5ba" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In many cases, until you have the “in-house expertise,” it helps to pay a little extra for an experienced worker — the same applies to LLMs.</p><p id="d250" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If you don’t have <em class="nu">labeled data</em>, start with a stronger (larger) model, <em class="nu">collect data</em>, and then utilize it to empower a model using a few-shot or fine-tuning.</p><h2 id="6f93" class="px oe fq bf of py pz qa oi qb qc qd ol ms qe qf qg mw qh qi qj na qk ql qm qn bk">3.1. Fine-tuning a model</h2><p id="c4d9" class="pw-post-body-paragraph mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne fj bk">There are a few aspects that you must consider before resorting to fine-tune a model:</p><ul class=""><li id="e1ed" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qw qx qy bk"><strong class="ml fr">Privacy</strong> — Your data might include pieces of private information that must be kept from the model. You must anonymize your data to avoid legal liabilities if your data contains private information.</li><li id="5dca" class="mj mk fq ml b go qz mn mo gr ra mq mr ms rb mu mv mw rc my mz na rd nc nd ne qw qx qy bk"><strong class="ml fr">Laws, Compliance, and Data Rights </strong>— Some legal questions can be raised when training a model. For example, the OpenAI terms-of-use policy prevents you from training a model without OpenAI using generated responses. Another typical example is complying with the GDPR’s laws, which require a “right for revocation,” where a user can require the company to remove information from the system. This raises legal questions about whether the model should be retrained or not.</li><li id="7759" class="mj mk fq ml b go qz mn mo gr ra mq mr ms rb mu mv mw rc my mz na rd nc nd ne qw qx qy bk"><strong class="ml fr">Updating latency — </strong>The latency or data cutoff is much higher when training a model. Unlike embedding the new information via the context (see “4. Contextual Data” section below), which provides immediate latency, training the model is a long process that takes time. Due to that, models are retrained less often.</li><li id="bd52" class="mj mk fq ml b go qz mn mo gr ra mq mr ms rb mu mv mw rc my mz na rd nc nd ne qw qx qy bk"><strong class="ml fr">Development and operation — </strong>Implementing a reproducible, scalable, and monitored fine-tuning pipeline is essential while continuously evaluating the results’ performance. This complex process requires constant maintenance.</li><li id="982f" class="mj mk fq ml b go qz mn mo gr ra mq mr ms rb mu mv mw rc my mz na rd nc nd ne qw qx qy bk"><strong class="ml fr">Cost — </strong>Retraining is considered expensive due to its complexity and the highly intensive resources(GPUs) required per training.</li></ul><p id="cc9c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The ability of LLMs to act as <em class="nu">in-context learners</em> and the fact that the newer models support a much larger context window simplify our implementation dramatically and can provide excellent results even without fine-tuning. Due to the complexity of fine-tuning, using it as a last resort or skipping it entirely is recommended.</p><p id="4487" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Conversely, fine-tuning models for specific tasks (e.g., structured JSON output) or domain-specific language can be highly efficient. A small, task-specific model can be highly effective and much cheaper in inference than large LLMs. Choose your solution wisely, and assess all the relevant considerations before escalating to LLM training.</p></div></div></div><div class="ab cb nv nw nx ny" role="separator"><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><blockquote class="nf"><p id="34dc" class="ng nh fq bf ni nj nk nl nm nn no ne dx">“Even the most powerful model requires relevant and well-structured contextual data to shine.”</p></blockquote></div></div></div><div class="ab cb nv nw nx ny" role="separator"><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="7b49" class="od oe fq bf of og oh gq oi oj ok gt ol om on oo op oq or os ot ou ov ow ox oy bk">4. Contextual Data</h1><p id="efe8" class="pw-post-body-paragraph mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne fj bk"><strong class="ml fr"><em class="nu">LLMs are in-context learners.</em></strong> That means that by providing task-specific information, the LLM agent can help us to perform it <em class="nu">without</em> special training or fine-tuning. This enables us to “teach” new knowledge or skills easily. When thinking about the <strong class="ml fr">Contextual Data principle</strong>, we should aim to organize and model the available data and how to compose it within our prompt.</p><figure class="ph pi pj pk pl pm pe pf paragraph-image"><div class="pe pf qp"><img src="../Images/879503f2a13a6e3cc811e6bb8ba4ecc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*_vUa4Wxm_dOurNbgC8p5hA.jpeg"/></div><figcaption class="ps pt pu pe pf pv pw bf b bg z dx">The Contextual Data principle. (Image by author)</figcaption></figure><p id="2430" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To compose our context, we include the relevant (contextual) information within the prompt we send to the LLM. There are two kinds of contexts we can use:</p><ul class=""><li id="d76c" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qw qx qy bk"><strong class="ml fr">Embedded contexts</strong> — embedded information pieces provided as part of the prompt.</li></ul><pre class="ph pi pj pk pl rk rl rm bp rn bb bk"><span id="fc6a" class="ro oe fq rl b bg rp rq l rr rs">You are the helpful assistant of &lt;name&gt;, a &lt;role&gt; at &lt;company&gt;</span></pre><ul class=""><li id="ae28" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qw qx qy bk"><strong class="ml fr">Attachment contexts</strong> — A list of information pieces glues by the beginning/end of the prompt</li></ul><pre class="ph pi pj pk pl rk rl rm bp rn bb bk"><span id="3eff" class="ro oe fq rl b bg rp rq l rr rs">Summarize the provided emails while keeping a friendly tone.<br/>---<br/><br/>&lt;email_0&gt;<br/>&lt;email_1&gt;</span></pre><p id="7151" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Contexts are usually implemented using a “prompt template” (such as <a class="af qo" href="https://jinja.palletsprojects.com/en/3.1.x/" rel="noopener ugc nofollow" target="_blank">jinja2</a> or <a class="af qo" href="https://mustache.github.io/" rel="noopener ugc nofollow" target="_blank">mustache</a> or simply native <a class="af qo" href="https://docs.python.org/3/reference/lexical_analysis.html#formatted-string-literals" rel="noopener ugc nofollow" target="_blank">formatting literal strings</a>); this way, we can compose them elegantly while keeping the essence of our prompt:</p><pre class="ph pi pj pk pl rk rl rm bp rn bb bk"><span id="4f7a" class="ro oe fq rl b bg rp rq l rr rs"># Embedded context with an attachment context<br/>prompt = f"""<br/>You are the helpful assistant of {name}. {name} is a {role} at {company}.<br/><br/>Help me write a {tone} response to the attached email.<br/>Always sign your email with:<br/>{signature}<br/><br/>---<br/><br/>{email}<br/>"""</span></pre><h2 id="3d0a" class="px oe fq bf of py pz qa oi qb qc qd ol ms qe qf qg mw qh qi qj na qk ql qm qn bk">4.1. Few-shot learning</h2><p id="0d84" class="pw-post-body-paragraph mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne fj bk">Few-shot learning is a powerful way to “teach” LLMs by example without requiring extensive fine-tuning. Providing a few representative examples in the prompt can guide the model in understanding the desired format, style, or task.</p><p id="de0c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For instance, if we want the LLM to generate email responses, we could include a few examples of well-written responses in the prompt. This helps the model learn the preferred structure and tone.</p><p id="a859" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We can use diverse examples to help the model catch different corner cases or nuances and learn from them. Therefore, it’s essential to include a variety of examples that cover a range of scenarios your application might encounter.</p><p id="11c0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As your application grows, you may consider implementing “<a class="af qo" href="https://arxiv.org/abs/1804.09458" rel="noopener ugc nofollow" target="_blank">Dynamic few-shot</a>,” which involves programmatically selecting the most relevant examples for each input. While it increases your implementation complexity, it ensures the model receives the most appropriate guidance for each case, significantly improving performance across a wide range of tasks without costly fine-tuning.</p><h2 id="76a4" class="px oe fq bf of py pz qa oi qb qc qd ol ms qe qf qg mw qh qi qj na qk ql qm qn bk">4.2. Retrieval Augmented Generation</h2><p id="8be5" class="pw-post-body-paragraph mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne fj bk"><a class="af qo" href="https://www.promptingguide.ai/techniques/rag" rel="noopener ugc nofollow" target="_blank">Retrieval Augmented Generation (RAG)</a> is a technique for retrieving relevant documents for additional context before generating a response. It’s like giving the LLM a quick peek at specific reference material to help inform its answer. This keeps responses current and factual without needing to retrain the model.</p><p id="caa8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For instance, on a support chatbot application, RAG could pull relevant help-desk wiki pages to inform the LLM’s answers.</p><p id="00d3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This approach helps LLMs <em class="nu">stay current</em> and <em class="nu">reduces hallucinations</em> by grounding responses in retrieved facts. RAG is particularly handy for tasks that require updated or specialized knowledge without retraining the entire model.</p><p id="1ca0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For example, suppose we are building a support chat for our product. In that case, we can use RAG to retrieve a relevant document from our helpdesk wiki, then provide it to an LLM agent and ask it to compose an answer based on the question and provide a document.</p><p id="e9d3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">There are three key pieces to look at while implementing RAG:</p><ul class=""><li id="c0ab" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qw qx qy bk"><strong class="ml fr">Retrieval mechanism</strong> — While the traditional implementation of RAG involves retrieving a relevant document using a vector similarity search, sometimes it’s better or cheaper to use simpler methods such as keyword-based search (like <a class="af qo" href="https://en.wikipedia.org/wiki/Okapi_BM25" rel="noopener ugc nofollow" target="_blank">BM-25</a>).</li><li id="7cf1" class="mj mk fq ml b go qz mn mo gr ra mq mr ms rb mu mv mw rc my mz na rd nc nd ne qw qx qy bk"><strong class="ml fr">Indexed data structure —</strong>Indexing the entire document naively, without preprocessing, may limit the effectiveness of the retrieval process. Sometimes, we want to add a data preparation step, such as preparing a list of questions and answers based on the document.</li><li id="7cc8" class="mj mk fq ml b go qz mn mo gr ra mq mr ms rb mu mv mw rc my mz na rd nc nd ne qw qx qy bk"><strong class="ml fr">Metadata—</strong>Storing relevant metadata allows for more efficient referencing and filtering of information (e.g., narrowing down wiki pages to only those related to the user’s specific product inquiry). This extra data layer streamlines the retrieval process.</li></ul><h2 id="3733" class="px oe fq bf of py pz qa oi qb qc qd ol ms qe qf qg mw qh qi qj na qk ql qm qn bk">4.3. Providing relevant context</h2><p id="2277" class="pw-post-body-paragraph mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne fj bk">The context information relevant to your agent can vary. Although it may seem beneficial, providing the model (like the “unskilled worker”) with too much information can be overwhelming and irrelevant to the task. Theoretically, this causes the model to learn irrelevant information (or token connections), which can lead to confusion and <a class="af qo" href="https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)" rel="noopener ugc nofollow" target="_blank">hallucinations</a>.</p><p id="e4b3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">When Gemini 1.5 was released and introduced as an LLM that could process up to 10M tokens, some practitioners questioned whether the context was still an issue. While it’s a fantastic accomplishment, especially for some use cases (such as chat with PDFs), it’s still limited, especially when reasoning over various documents.</p><p id="4b3b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Compacting the prompt and providing the LLM agent with only relevant information is crucial. This reduces the processing power the model invests in irrelevant tokens, improves the quality, optimizes the latency, and reduces the cost.</p><p id="46dd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">There are many tricks to improve the relevancy of the provided context, most of which relate to how you store and catalog your data.<br/>For RAG applications, it’s handy to add a data preparation that shapes the information you store (e.g., questions and answers based on the document, then providing the LLM agent only with the answer; this way, the agent gets a summarized and shorter context), and use re-ranking algorithms on top of the retrieved documents to refine the results.</p></div></div></div><div class="ab cb nv nw nx ny" role="separator"><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><blockquote class="nf"><p id="9055" class="ng nh fq bf ni nj nk nl nm nn no ne dx">“Data fuels the engine of LLM-native applications. A strategic design of contextual data unlocks their true potential.”</p></blockquote></div></div></div><div class="ab cb nv nw nx ny" role="separator"><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="9957" class="od oe fq bf of og oh gq oi oj ok gt ol om on oo op oq or os ot ou ov ow ox oy bk">Conclusion and Implications</h1><p id="0420" class="pw-post-body-paragraph mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne fj bk">The LLM Triangle Principles provide a structured approach to developing high-quality LLM-native applications, addressing the gap between LLMs’ enormous potential and real-world implementation challenges. Developers can create more reliable and effective LLM-powered solutions by focusing on 3+1 key principles—the <strong class="ml fr">Model</strong>, <strong class="ml fr">Engineering Techniques</strong>, and <strong class="ml fr">Contextual Data</strong>—all guided by a well-defined <strong class="ml fr">SOP</strong>.</p><figure class="ph pi pj pk pl pm pe pf paragraph-image"><div class="pe pf qp"><img src="../Images/6559fd3272a486ef7e1bd7e6edd445e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*qJNIDrBveH4HQU6kU-9NGA.jpeg"/></div><figcaption class="ps pt pu pe pf pv pw bf b bg z dx">The LLM Triangle Principles. (Image by author)</figcaption></figure><h2 id="5200" class="px oe fq bf of py pz qa oi qb qc qd ol ms qe qf qg mw qh qi qj na qk ql qm qn bk">Key takeaways</h2><ol class=""><li id="8db5" class="mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne ri qx qy bk"><strong class="ml fr">Start with a clear SOP</strong>: Model your expert’s cognitive process to create a step-by-step guide for your LLM application. Use it as a guide while thinking of the other principles.</li><li id="d9f4" class="mj mk fq ml b go qz mn mo gr ra mq mr ms rb mu mv mw rc my mz na rd nc nd ne ri qx qy bk"><strong class="ml fr">Choose the right model</strong>: Balance capabilities with cost, and consider starting with larger models before potentially moving to smaller, fine-tuned ones.</li><li id="521b" class="mj mk fq ml b go qz mn mo gr ra mq mr ms rb mu mv mw rc my mz na rd nc nd ne ri qx qy bk"><strong class="ml fr">Leverage engineering techniques</strong>: Implement LLM-native architectures and use agents strategically to optimize performance and maintain control. Experiment with different prompt techniques to find the most effective prompt for your case.</li><li id="f1f2" class="mj mk fq ml b go qz mn mo gr ra mq mr ms rb mu mv mw rc my mz na rd nc nd ne ri qx qy bk"><strong class="ml fr">Provide relevant context</strong>: Use in-context learning, including RAG, when appropriate, but be cautious of overwhelming the model with irrelevant information.</li><li id="af12" class="mj mk fq ml b go qz mn mo gr ra mq mr ms rb mu mv mw rc my mz na rd nc nd ne ri qx qy bk"><strong class="ml fr">Iterate and experiment</strong>: Finding the right solution often requires testing and refining your work. I recommend reading and implementing the <a class="af qo" rel="noopener" target="_blank" href="/building-llm-apps-a-clear-step-by-step-guide-1fe1e6ef60fd">“Building LLM Apps: A Clear Step-By-Step Guide”</a> tips for a detailed LLM-Native development process guide.</li></ol><p id="8735" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">By applying the LLM Triangle Principles, organizations can move beyond a simple proof-of-concept and develop robust, production-ready LLM applications that truly harness the power of this transformative technology.</p></div></div></div><div class="ab cb nv nw nx ny" role="separator"><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="9825" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If you find this whitepaper helpful, please give it a few <strong class="ml fr">claps</strong> 👏 on Medium and <strong class="ml fr">share</strong> it with your fellow AI enthusiasts. Your support means the world to me! 🌍</p><p id="792d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s keep the conversation going — feel free to reach out via <a class="af qo" href="mailto:almog.baku@gmail.com" rel="noopener ugc nofollow" target="_blank">email</a> or <a class="af qo" href="https://www.linkedin.com/in/almogbaku/" rel="noopener ugc nofollow" target="_blank">connect on LinkedIn</a> 🤝</p><p id="a18a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Special thanks to <span class="ia"><span class="ia" aria-hidden="false"><a class="rw ib rx" href="https://medium.com/u/532f8dc01db8?source=post_page---user_mention--d3753dd8542e--------------------------------" rel="noopener" target="_blank">Gal Peretz</a></span></span>, <span class="ia"><span class="ia" aria-hidden="false"><a class="rw ib rx" href="https://medium.com/u/b45fa95a7293?source=post_page---user_mention--d3753dd8542e--------------------------------" rel="noopener" target="_blank">Gad Benram</a></span></span>, <span class="ia"><span class="ia" aria-hidden="false"><a class="rw ib rx" href="https://medium.com/u/251cd1007ce8?source=post_page---user_mention--d3753dd8542e--------------------------------" rel="noopener" target="_blank">Liron Izhaki Allerhand</a></span></span>, <span class="ia"><span class="ia" aria-hidden="false"><a class="rw ib rx" href="https://medium.com/u/bcd07dca5f93?source=post_page---user_mention--d3753dd8542e--------------------------------" rel="noopener" target="_blank">Itamar Friedman</a></span></span>, <span class="ia"><span class="ia" aria-hidden="false"><a class="rw ib rx" href="https://medium.com/u/241b56ab4bf?source=post_page---user_mention--d3753dd8542e--------------------------------" rel="noopener" target="_blank">Lee Twito</a></span></span>, <span class="ia"><span class="ia" aria-hidden="false"><a class="rw ib rx" href="https://medium.com/u/2db20f6d91e8?source=post_page---user_mention--d3753dd8542e--------------------------------" rel="noopener" target="_blank">Ofir Ziv</a></span></span>, <span class="ia"><span class="ia" aria-hidden="false"><a class="rw ib rx" href="https://medium.com/u/5c5d2a69bcdb?source=post_page---user_mention--d3753dd8542e--------------------------------" rel="noopener" target="_blank">Philip Tannor</a></span></span>, <span class="ia"><span class="ia" aria-hidden="false"><a class="rw ib rx" href="https://medium.com/u/6f8924605cf6?source=post_page---user_mention--d3753dd8542e--------------------------------" rel="noopener" target="_blank">Yair Livne</a></span></span> and <span class="ia"><span class="ia" aria-hidden="false"><a class="rw ib rx" href="https://medium.com/u/fa1f5e83a0c8?source=post_page---user_mention--d3753dd8542e--------------------------------" rel="noopener" target="_blank">Shai Alon</a></span></span> for insights, feedback, and editing notes.</p></div></div></div></div>    
</body>
</html>