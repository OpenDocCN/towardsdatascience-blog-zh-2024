<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Gradient Boosting Regressor, Explained: A Visual Guide with Code Examples</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Gradient Boosting Regressor, Explained: A Visual Guide with Code Examples</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gradient-boosting-regressor-explained-a-visual-guide-with-code-examples-c098d1ae425c?source=collection_archive---------1-----------------------#2024-11-14">https://towardsdatascience.com/gradient-boosting-regressor-explained-a-visual-guide-with-code-examples-c098d1ae425c?source=collection_archive---------1-----------------------#2024-11-14</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="743b" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">ENSEMBLE LEARNING</h2><div/><div><h2 id="dd1a" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">Fitting to errors one booster stage at a time</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@samybaladram?source=post_page---byline--c098d1ae425c--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Samy Baladram" class="l ep by dd de cx" src="../Images/715cb7af97c57601966c5d2f9edd0066.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--c098d1ae425c--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://medium.com/@samybaladram?source=post_page---byline--c098d1ae425c--------------------------------" rel="noopener follow">Samy Baladram</a></p></div></div></div><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ib ic dx"><button class="ig ih ah ai aj ak al am an ao ap aq ar ii ij ik" disabled="">Follow</button></p></div></div></span></div></div><div class="l il"><span class="bf b bg z dx"><div class="ab cn im in io"><div class="ip iq ab"><div class="bf b bg z dx ab ir"><span class="is l il">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--c098d1ae425c--------------------------------" rel="noopener follow"><p class="bf b bg z it iu iv iw ix iy iz ja bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="jb jc l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 14, 2024</span></div></span></div></span></div></div></div><div class="ab cp jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js"><div class="h k w ea eb q"><div class="ki l"><div class="ab q kj kk"><div class="pw-multi-vote-icon ed is kl km kn"><div class=""><div class="ko kp kq kr ks kt ku am kv kw kx kn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ky kz la lb lc ld le"><p class="bf b dy z dx"><span class="kp">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ko lh li ab q ee lj lk" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lg"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lf lg">1</span></p></button></div></div></div><div class="ab q jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="ll k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lm an ao ap ii ln lo lp" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lq cn"><div class="l ae"><div class="ab cb"><div class="lr ls lt lu lv lw ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><div class="mo mp mq mr ms mt"><a rel="noopener follow" target="_blank" href="/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef?source=post_page-----c098d1ae425c--------------------------------"><div class="mu ab il"><div class="mv ab co cb mw mx"><h2 class="bf ga ib z it my iv iw mz iy ja fz bk">Decision Tree Regressor, Explained: A Visual Guide with Code Examples</h2><div class="na l"><h3 class="bf b ib z it my iv iw mz iy ja dx">Trimming branches smartly with Cost-Complexity Pruning</h3></div><div class="gq l"><p class="bf b dy z it my iv iw mz iy ja dx">towardsdatascience.com</p></div></div><div class="nb l"><div class="nc l nd ne nf nb ng lw mt"/></div></div></a></div><p id="ffa8" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Of course, in machine learning, we want our predictions spot on. We started with <a class="af od" href="https://medium.com/towards-data-science/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e" rel="noopener">simple decision trees</a> — they worked okay. Then came <a class="af od" href="https://medium.com/towards-data-science/random-forest-explained-a-visual-guide-with-code-examples-9f736a6e1b3c" rel="noopener">Random Forests</a> and <a class="af od" href="https://medium.com/towards-data-science/adaboost-classifier-explained-a-visual-guide-with-code-examples-fc0f25326d7b" rel="noopener">AdaBoost</a>, which did better. But Gradient Boosting? That was a game-changer, making predictions way more accurate.</p><p id="7d45" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">They said, “What makes Gradient Boosting work so well is actually simple: it builds models one after another, where each new model focuses on fixing the mistakes of all previous models combined. This way of fixing errors step by step is what makes it special.” I thought it’s really gonna be that simple but <em class="oe">every time</em> I look up Gradient Boosting, trying to understand how it works, I see the same thing: rows and rows of complex math formulas and ugly charts that somehow drive me insane. Just try it.</p><p id="91dc" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Let’s put a stop to this and break it down in a way that actually makes sense. We’ll visually navigate through the training steps of Gradient Boosting, focusing on a regression case — a simpler scenario than classification — so we can avoid the confusing math. Like a multi-stage rocket shedding unnecessary weight to reach orbit, we’ll blast away those prediction errors one residual at a time.</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og oh"><img src="../Images/2d6078ab2a677250978f6a47f6460221.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FBDim33AJDmZUEDHk2z-tA.gif"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">All visuals: Author-created using Canva Pro. Optimized for mobile; may appear oversized on desktop.</figcaption></figure><h2 id="e58f" class="oy oz fq bf pa pb pc pd pe pf pg ph pi nq pj pk pl nu pm pn po ny pp pq pr fw bk">Definition</h2><p id="59dd" class="pw-post-body-paragraph nh ni fq nj b gt ps nl nm gw pt no np nq pu ns nt nu pv nw nx ny pw oa ob oc fj bk">Gradient Boosting is an ensemble machine learning technique that builds a series of decision trees, each aimed at correcting the errors of the previous ones. Unlike AdaBoost, which uses shallow trees, Gradient Boosting uses deeper trees as its weak learners. Each new tree focuses on minimizing the residual errors — the differences between actual and predicted values — rather than learning directly from the original targets.</p><p id="9718" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">For regression tasks, Gradient Boosting adds trees one after another with each new tree is trained to reduce the remaining errors by addressing the current residual errors. The final prediction is made by adding up the outputs from all the trees.</p><p id="8b35" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">The model’s strength comes from its additive learning process — while each tree focuses on correcting the remaining errors in the ensemble, the sequential combination creates a powerful predictor that <strong class="nj ga">progressively reduces the overall prediction error</strong> by focusing on the parts of the problem where the model still struggles.</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og px"><img src="../Images/3ac141fa57a3f0707e680165024c850a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pdT2ULr65lYvw-0q9X6xJw.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">Gradient Boosting is part of the boosting family of algorithms because it builds trees sequentially, with each new tree trying to correct the errors of its predecessors. However, unlike other boosting methods, Gradient Boosting approaches the problem from an optimization perspective.</figcaption></figure><h2 id="9afa" class="oy oz fq bf pa pb pc pd pe pf pg ph pi nq pj pk pl nu pm pn po ny pp pq pr fw bk">Dataset Used</h2><p id="b559" class="pw-post-body-paragraph nh ni fq nj b gt ps nl nm gw pt no np nq pu ns nt nu pv nw nx ny pw oa ob oc fj bk">Throughout this article, we’ll focus on the classic golf dataset as an example for regression. While Gradient Boosting can handle both regression and classification tasks effectively, we’ll concentrate on the simpler task which in this case is the regression — predicting the number of players who will show up to play golf based on weather conditions.</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og py"><img src="../Images/dfe988f34df90e6d6bc95d67f04271db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IoD-1fqAX4TfaDm48ZUUYQ.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">Columns: ‘Overcast (one-hot-encoded into 3 columns)’, ’Temperature’ (in Fahrenheit), ‘Humidity’ (in %), ‘Windy’ (Yes/No) and ‘Number of Players’ (target feature)</figcaption></figure><pre class="oi oj ok ol om pz qa qb bp qc bb bk"><span id="7659" class="qd oz fq qa b bg qe qf l qg qh">import pandas as pd<br/>import numpy as np<br/>from sklearn.model_selection import train_test_split<br/><br/># Create dataset<br/>dataset_dict = {<br/>   'Outlook': ['sunny', 'sunny', 'overcast', 'rain', 'rain', 'rain', 'overcast', <br/>               'sunny', 'sunny', 'rain', 'sunny', 'overcast', 'overcast', 'rain',<br/>               'sunny', 'overcast', 'rain', 'sunny', 'sunny', 'rain', 'overcast',<br/>               'rain', 'sunny', 'overcast', 'sunny', 'overcast', 'rain', 'overcast'],<br/>   'Temp.': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0,<br/>             72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0,<br/>             88.0, 77.0, 79.0, 80.0, 66.0, 84.0],<br/>   'Humid.': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0,<br/>              90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0,<br/>              65.0, 70.0, 60.0, 95.0, 70.0, 78.0],<br/>   'Wind': [False, True, False, False, False, True, True, False, False, False, True,<br/>            True, False, True, True, False, False, True, False, True, True, False,<br/>            True, False, False, True, False, False],<br/>   'Num_Players': [52, 39, 43, 37, 28, 19, 43, 47, 56, 33, 49, 23, 42, 13, 33, 29,<br/>                   25, 51, 41, 14, 34, 29, 49, 36, 57, 21, 23, 41]<br/>}<br/><br/># Prepare data<br/>df = pd.DataFrame(dataset_dict)<br/>df = pd.get_dummies(df, columns=['Outlook'], prefix='', prefix_sep='')<br/>df['Wind'] = df['Wind'].astype(int)<br/><br/># Split features and target<br/>X, y = df.drop('Num_Players', axis=1), df['Num_Players']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)</span></pre><h2 id="d91b" class="oy oz fq bf pa pb pc pd pe pf pg ph pi nq pj pk pl nu pm pn po ny pp pq pr fw bk">Main Mechanism</h2><p id="0c93" class="pw-post-body-paragraph nh ni fq nj b gt ps nl nm gw pt no np nq pu ns nt nu pv nw nx ny pw oa ob oc fj bk">Here’s how Gradient Boosting works:</p><ol class=""><li id="b53a" class="nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc qi qj qk bk"><strong class="nj ga">Initialize Model:</strong> Start with a simple prediction, typically the mean of target values.</li><li id="5d80" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc qi qj qk bk"><strong class="nj ga">Iterative Learning:</strong> For a set number of iterations, compute the residuals, train a decision tree to predict these residuals, and add the new tree’s predictions (scaled by the learning rate) to the running total.</li><li id="59fe" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc qi qj qk bk"><strong class="nj ga">Build Trees on Residuals:</strong> Each new tree focuses on the remaining errors from all previous iterations.</li><li id="d16d" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc qi qj qk bk"><strong class="nj ga">Final Prediction:</strong> Sum up all tree contributions (scaled by the learning rate) and the initial prediction.</li></ol><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og px"><img src="../Images/5aebdd14999e70e5924a74ce7ef351b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oCyob1iKyKGmRYrvpGV9sg.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">A Gradient Boosting Regressor starts with an average prediction and improves it through multiple trees, each one fixing the previous trees’ mistakes in small steps, until reaching the final prediction.</figcaption></figure><h2 id="44d6" class="oy oz fq bf pa pb pc pd pe pf pg ph pi nq pj pk pl nu pm pn po ny pp pq pr fw bk">Training Steps</h2><p id="4aac" class="pw-post-body-paragraph nh ni fq nj b gt ps nl nm gw pt no np nq pu ns nt nu pv nw nx ny pw oa ob oc fj bk">We’ll follow the standard gradient boosting approach:</p><p id="f2df" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">1.0. Set Model Parameters:<br/>Before building any trees, we need set the core parameters that control the learning process: <br/>· the number of trees (typically 100, but we’ll choose 50) to build sequentially, <br/>· the learning rate (typically 0.1), and <br/>· the maximum depth of each tree (typically 3)</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og py"><img src="../Images/11972f0214d401ffdf105a994791fc6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9PGE5XHwZxHKWkxenSxQ-g.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">A tree diagram showing our key settings: each tree will have 3 levels, and we’ll create 50 of them while moving forward in small steps of 0.1.</figcaption></figure><h2 id="2d14" class="oy oz fq bf pa pb pc pd pe pf pg ph pi nq pj pk pl nu pm pn po ny pp pq pr fw bk">For the First Tree</h2><p id="50ec" class="pw-post-body-paragraph nh ni fq nj b gt ps nl nm gw pt no np nq pu ns nt nu pv nw nx ny pw oa ob oc fj bk">2.0 Make an initial prediction for the label. This is typically the mean (just like <a class="af od" rel="noopener" target="_blank" href="/dummy-regressor-explained-a-visual-guide-with-code-examples-for-beginners-4007c3d16629">a dummy prediction</a>.)</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og px"><img src="../Images/0583a69027986dee2fda863439108614.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N4lVPzDww6uMmr2pnSqq6g.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">To start our predictions, we use the average value (37.43) of all our training data as the first guess for every case.</figcaption></figure><p id="b12f" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">2.1. Calculate temporary residual (or pseudo-residuals): <br/>residual = actual value — predicted value</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og px"><img src="../Images/5f135dbb9d7034726d771919188d31f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Oj-b3HxuMTkTSSHNymibiw.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">Calculating the initial residuals by subtracting the mean prediction (37.43) from each target value in our training set.</figcaption></figure><p id="0657" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">2.2. Build a decision tree to <strong class="nj ga">predict these residuals. </strong>The tree building steps are exactly the same as in the <a class="af od" href="https://medium.com/towards-data-science/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef" rel="noopener">regression tree</a>.</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og px"><img src="../Images/77364ffaaeca757af50cfa5a758e12b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s0U7X7PAnqK8fHtaoE6hPQ.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">The first decision tree begins its training by searching for patterns in our features that can best predict the calculated residuals from our initial mean prediction.</figcaption></figure><p id="ac7d" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">a. Calculate initial MSE (Mean Squared Error) for the root node</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og px"><img src="../Images/44736650f521e040f1c4b17edd948edc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FsJOZMXUU9gOC6Uzc9QdNA.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">Just like in regular regression trees, we calculate the Mean Squared Error (MSE), but this time we’re measuring the spread of residuals (around zero) instead of actual values (around their mean).</figcaption></figure><p id="95ae" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">b. For each feature: <br/>· Sort data by feature values</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og px"><img src="../Images/b2f725b3c389f832749b26326e214e85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m1ZJmCJq6jtLYnzUcNxvqg.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">For each feature in our dataset, we sort its values and find potential split points between them, just as we would in a standard decision tree, to determine the best way to divide our residuals.</figcaption></figure><p id="deed" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">· For each possible split point: <br/>·· Split samples into left and right groups <br/>·· Calculate MSE for both groups <br/>·· Calculate MSE reduction for this split</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og py"><img src="../Images/7082a68606950c5a393205a4e8694ad0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X_AluXCI97CU6kIjBT8b2g.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">Similar to a regular regression tree, we evaluate each split by calculating the weighted MSE of both groups, but here we’re measuring how well the split groups similar residuals rather than similar target values.</figcaption></figure><p id="a38c" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">c. Pick the split that gives the largest MSE reduction</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og py"><img src="../Images/26c4db7c3842cc6336ef543ffe94653d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9Xypd5pN1C6G6yl1ESUHFw.png"/></div></div></figure><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og px"><img src="../Images/164adc4d983357b7bd928394075c5cbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LJC3AQ0Ass6mfMzyxk8JnQ.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">The tree makes its first split using the “rain” feature at value 0.5, dividing samples into two groups based on their residuals — this first decision will be refined by additional splits at deeper levels.</figcaption></figure><p id="3673" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">d. Continue splitting until reaching maximum depth or minimum samples per leaf.</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og px"><img src="../Images/dfb7d12a8ebb84f85be346d2ab27e286.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qu9hafWmoeB8GpxqxQJOHw.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">After three levels of splitting on different features, our first tree has created eight distinct groups, each with its own prediction for the residuals.</figcaption></figure><p id="6d23" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">2.3. Calculate Leaf Values<br/>For each leaf, find the <strong class="nj ga">mean of residuals</strong>.</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og px"><img src="../Images/fbb6b98089b4529f42201e77bbdeca1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EjZVs7TrMKlxlEU-Y8-8Gg.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">Each leaf in our first tree contains an average of the residuals in that group — these values will be used to adjust and improve our initial mean prediction of 37.43.</figcaption></figure><p id="9ad7" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">2.4. Update Predictions<br/>· For each data point in the <strong class="nj ga">training dataset</strong>, determine which leaf it falls into based on the new tree.</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og px"><img src="../Images/df18786126f2894b4b244d4fde2b9192.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NXz27RIqkQ8znGtY4Mcy1Q.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">Running our training data through the first tree, each sample follows its own path based on weather features to get its predicted residual value, which will help correct our initial prediction.</figcaption></figure><p id="c4e1" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">· Multiply the new tree’s predictions by the learning rate and add these scaled predictions to the current model’s predictions. This will be the updated prediction.</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og px"><img src="../Images/76a4aa18c7ed15a837c54901efb09dcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C6rrKMGidRYPcHNvgMaYuQ.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">Our model updates its predictions by taking small steps: it adds just 10% (our learning rate of 0.1) of each predicted residual to our initial prediction of 37.43, creating slightly improved predictions.</figcaption></figure><h2 id="e7a6" class="oy oz fq bf pa pb pc pd pe pf pg ph pi nq pj pk pl nu pm pn po ny pp pq pr fw bk">For the Second Tree</h2><p id="843c" class="pw-post-body-paragraph nh ni fq nj b gt ps nl nm gw pt no np nq pu ns nt nu pv nw nx ny pw oa ob oc fj bk">2.1. Calculate new residuals based on current model <br/>a. Compute the difference between the target and current predictions.<br/>These residuals will be a bit different from the first iteration.</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og px"><img src="../Images/2843e92a51f3b6bd97696bf603c17128.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vd9TgVCnrx72mC20kl02Xg.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">After updating our predictions with the first tree, we calculate new residuals — notice how they’re slightly smaller than the original ones, showing our predictions are gradually improving.</figcaption></figure><p id="70bb" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">2.2. Build a new tree to predict these residuals. Same process as first tree, but targeting new residuals.</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og px"><img src="../Images/bd50d352c28d235ec429312a6fc54dd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jNhnOn_LXphkF7rKp5R-Ug.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">Starting our second tree to predict the new, smaller residuals — we’ll use the same tree-building process as before, but now we’re trying to catch the errors our first tree missed</figcaption></figure><p id="7962" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">2.3. Calculate the mean residuals for each leaf</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og px"><img src="../Images/b5226feb595f93476c22f6f3acfb4488.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zonEzk-EeqmUlN6vA4PPkw.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">The second tree follows an identical structure to our first tree with the same weather features and split points, but with smaller values in its leaves — showing we’re fine-tuning the remaining errors.</figcaption></figure><p id="4761" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">2.4. Update model predictions<br/>· Multiply the new tree’s predictions by the learning rate.<br/>· Add the new scaled tree predictions to the running total.</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og px"><img src="../Images/269aeb6fbb665e5f8d830fff8afb22f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GhH4cPt-iclHi98_jTnjuQ.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">After running our data through the second tree, we again take small steps with our 0.1 learning rate to update predictions, and calculate new residuals that are even smaller than before — our model is gradually learning the patterns.</figcaption></figure><h2 id="e5cb" class="oy oz fq bf pa pb pc pd pe pf pg ph pi nq pj pk pl nu pm pn po ny pp pq pr fw bk">For the Third Tree onwards</h2><p id="5822" class="pw-post-body-paragraph nh ni fq nj b gt ps nl nm gw pt no np nq pu ns nt nu pv nw nx ny pw oa ob oc fj bk">Repeat Steps 2.1–2.3 for remaining iterations. Note that each tree sees different residuals.<br/>· Trees progressively focus on harder-to-predict patterns <br/>· Learning rate prevents overfitting by limiting each tree’s contribution</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og qq"><img src="../Images/c3bec57f3cfc4d79ab75c562246a8250.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZvZFrZE1xEWtYibmeLLotw.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">As we build more trees, notice how the split points slowly shift and the residual values in the leaves get smaller — by tree 50, we’re making tiny adjustments using different combinations of features compared to our first trees.</figcaption></figure><pre class="oi oj ok ol om pz qa qb bp qc bb bk"><span id="3b4b" class="qd oz fq qa b bg qe qf l qg qh">from sklearn.tree import plot_tree<br/>import matplotlib.pyplot as plt<br/>from sklearn.ensemble import GradientBoostingRegressor<br/><br/># Train the model<br/>clf = GradientBoostingRegressor(criterion='squared_error', learning_rate=0.1, random_state=42)<br/>clf.fit(X_train, y_train)<br/><br/># Plot trees 1, 2, 49, and 50<br/>plt.figure(figsize=(11, 20), dpi=300)<br/><br/>for i, tree_idx in enumerate([0, 2, 24, 49]):<br/>    plt.subplot(4, 1, i+1)<br/>    plot_tree(clf.estimators_[tree_idx,0], <br/>              feature_names=X_train.columns,<br/>              impurity=False,<br/>              filled=True, <br/>              rounded=True,<br/>              precision=2,<br/>              fontsize=12)<br/>    plt.title(f'Tree {tree_idx + 1}')<br/><br/>plt.suptitle('Decision Trees from GradientBoosting', fontsize=16)<br/>plt.tight_layout(rect=[0, 0.03, 1, 0.95])<br/>plt.show()</span></pre><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og qr"><img src="../Images/8a98aab772c6bdcf801764e343cb4c17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8n9_q1c1of38ctgoG-Fryg.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">Visualization from scikit-learn shows how our gradient boosting trees evolve: from Tree 1 making large splits with big prediction values, to Tree 50 making refined splits with tiny adjustments — each tree focuses on correcting the remaining errors from previous trees.</figcaption></figure><h2 id="c3a0" class="oy oz fq bf pa pb pc pd pe pf pg ph pi nq pj pk pl nu pm pn po ny pp pq pr fw bk">Testing Step</h2><p id="9fba" class="pw-post-body-paragraph nh ni fq nj b gt ps nl nm gw pt no np nq pu ns nt nu pv nw nx ny pw oa ob oc fj bk">For predicting: <br/>a. Start with the initial prediction (the average number of players) <br/>b. Run the input through each tree to get its predicted adjustment <br/>c. Scale each tree’s prediction by the learning rate.<br/>d. Add all these adjustments to the initial prediction <br/>e. The sum directly gives us the predicted number of players</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og qs"><img src="../Images/4b7ce876ba3cdfad96da7c7e0415989f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4Yg-lgD4-AOXitFklqLVsg.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">When predicting on unseen data, each tree contributes its small prediction, starting from 5.57 in Tree 1 down to 0.008 in Tree 50 — all these predictions are scaled by our 0.1 learning rate and added to our base prediction of 37.43 to get the final answer.</figcaption></figure><h2 id="082a" class="oy oz fq bf pa pb pc pd pe pf pg ph pi nq pj pk pl nu pm pn po ny pp pq pr fw bk">Evaluation Step</h2><p id="01c9" class="pw-post-body-paragraph nh ni fq nj b gt ps nl nm gw pt no np nq pu ns nt nu pv nw nx ny pw oa ob oc fj bk">After building all the trees, we can evaluate the test set.</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og px"><img src="../Images/32381bf34c6c5780246b6047af2863bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DZGW0Qr6ApH2_r2NoalMiA.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">Our gradient boosting model achieves an RMSE of 4.785, quite an improvement over <a class="af od" href="https://medium.com/towards-data-science/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef" rel="noopener">a single regression tree’s 5.27</a> — showing how combining many small corrections leads to better predictions than one complex tree!</figcaption></figure><pre class="oi oj ok ol om pz qa qb bp qc bb bk"><span id="fe51" class="qd oz fq qa b bg qe qf l qg qh"># Get predictions<br/>y_pred = clf.predict(X_test)<br/><br/># Create DataFrame with actual and predicted values<br/>results_df = pd.DataFrame({<br/>    'Actual': y_test,<br/>    'Predicted': y_pred<br/>})<br/>print(results_df) # Display results DataFrame<br/><br/># Calculate and display RMSE<br/>from sklearn.metrics import root_mean_squared_error<br/>rmse = root_mean_squared_error(y_test, y_pred)<br/>print(f"\nModel Accuracy: {rmse:.4f}")</span></pre><h2 id="ee98" class="oy oz fq bf pa pb pc pd pe pf pg ph pi nq pj pk pl nu pm pn po ny pp pq pr fw bk">Key Parameters</h2><p id="568d" class="pw-post-body-paragraph nh ni fq nj b gt ps nl nm gw pt no np nq pu ns nt nu pv nw nx ny pw oa ob oc fj bk">Here are the key parameters for Gradient Boosting, particularly in <code class="cx qt qu qv qa b">scikit-learn</code>:</p><p id="7ee3" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk"><code class="cx qt qu qv qa b">max_depth</code>: The depth of trees used to model residuals. Unlike AdaBoost which uses stumps, Gradient Boosting works better with deeper trees (typically 3-8 levels). Deeper trees capture more complex patterns but risk overfitting.</p><p id="fd76" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk"><code class="cx qt qu qv qa b">n_estimators</code>: The number of trees to be used (typically 100-1000). More trees usually improve performance when paired with a small learning rate.</p><p id="0c9c" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk"><code class="cx qt qu qv qa b">learning_rate</code>: Also called "shrinkage", this scales each tree's contribution (typically 0.01-0.1). Smaller values require more trees but often give better results by making the learning process more fine-grained.</p><p id="aacc" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk"><code class="cx qt qu qv qa b">subsample</code>: The fraction of samples used to train each tree (typically 0.5-0.8). This optional feature adds randomness that can improve robustness and reduce overfitting.</p><p id="665b" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">These parameters work together: a small learning rate needs more trees, while deeper trees might need a smaller learning rate to avoid overfitting.</p><h2 id="673a" class="oy oz fq bf pa pb pc pd pe pf pg ph pi nq pj pk pl nu pm pn po ny pp pq pr fw bk">Key differences from AdaBoost</h2><p id="c713" class="pw-post-body-paragraph nh ni fq nj b gt ps nl nm gw pt no np nq pu ns nt nu pv nw nx ny pw oa ob oc fj bk">Both AdaBoost and Gradient Boosting are boosting algorithms, but the way they learn from their mistakes are different. Here are the key differences:</p><ol class=""><li id="ff80" class="nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc qi qj qk bk"><code class="cx qt qu qv qa b">max_depth</code> is typically higher (3-8) in Gradient Boosting, while AdaBoost prefers stumps.</li><li id="a120" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc qi qj qk bk">No <code class="cx qt qu qv qa b">sample_weight</code> updates because Gradient Boosting uses residuals instead of sample weighting.</li><li id="149d" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc qi qj qk bk">The <code class="cx qt qu qv qa b">learning_rate</code> is typically much smaller (0.01-0.1) compared to AdaBoost's larger values (0.1-1.0).</li><li id="8e11" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc qi qj qk bk">Initial prediction starts from the mean while AdaBoost starts from zero.</li><li id="2774" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc qi qj qk bk">Trees are combined through simple addition rather than weighted voting, making each tree’s contribution more straightforward.</li><li id="960a" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc qi qj qk bk">Optional <code class="cx qt qu qv qa b">subsample</code> parameter adds randomness, a feature not present in standard AdaBoost.</li></ol><h1 id="383e" class="qw oz fq bf pa qx qy gv pe qz ra gy pi rb rc rd re rf rg rh ri rj rk rl rm rn bk">Pros &amp; Cons</h1><h2 id="5ed7" class="oy oz fq bf pa pb pc pd pe pf pg ph pi nq pj pk pl nu pm pn po ny pp pq pr fw bk">Pros:</h2><ul class=""><li id="4744" class="nh ni fq nj b gt ps nl nm gw pt no np nq pu ns nt nu pv nw nx ny pw oa ob oc ro qj qk bk"><strong class="nj ga">Step-by-Step Error Fixing:</strong> In Gradient Boosting, each new tree focuses on correcting the mistakes made by the previous ones. This makes the model better at improving its predictions in areas where it was previously wrong.</li><li id="94a8" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc ro qj qk bk"><strong class="nj ga">Flexible Error Measures:</strong> Unlike AdaBoost, Gradient Boosting can optimize different types of error measurements (like mean absolute error, mean squared error, or others). This makes it adaptable to various kinds of problems.</li><li id="95fd" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc ro qj qk bk"><strong class="nj ga">High Accuracy:</strong> By using more detailed trees and carefully controlling the learning rate, Gradient Boosting often provides more accurate results than other algorithms, especially for well-structured data.</li></ul><h2 id="690d" class="oy oz fq bf pa pb pc pd pe pf pg ph pi nq pj pk pl nu pm pn po ny pp pq pr fw bk">Cons:</h2><ul class=""><li id="1fa5" class="nh ni fq nj b gt ps nl nm gw pt no np nq pu ns nt nu pv nw nx ny pw oa ob oc ro qj qk bk"><strong class="nj ga">Risk of Overfitting:</strong> The use of deeper trees and the sequential building process can cause the model to fit the training data too closely, which may reduce its performance on new data. This requires careful tuning of tree depth, learning rate, and the number of trees.</li><li id="f516" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc ro qj qk bk"><strong class="nj ga">Slow Training Process:</strong> Like AdaBoost, trees must be built one after another, making it slower to train compared to algorithms that can build trees in parallel, like Random Forest. Each tree relies on the errors of the previous ones.</li><li id="fadb" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc ro qj qk bk"><strong class="nj ga">High Memory Use:</strong> The need for deeper and more numerous trees means Gradient Boosting can consume more memory than simpler boosting methods such as AdaBoost.</li><li id="1afc" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc ro qj qk bk"><strong class="nj ga">Sensitive to Settings:</strong> The effectiveness of Gradient Boosting heavily depends on finding the right combination of learning rate, tree depth, and number of trees, which can be more complex and time-consuming than tuning simpler algorithms.</li></ul><h1 id="9b70" class="qw oz fq bf pa qx qy gv pe qz ra gy pi rb rc rd re rf rg rh ri rj rk rl rm rn bk">Final Remarks</h1><p id="7c6c" class="pw-post-body-paragraph nh ni fq nj b gt ps nl nm gw pt no np nq pu ns nt nu pv nw nx ny pw oa ob oc fj bk">Gradient Boosting is a major improvement in boosting algorithms. This success has led to popular versions like XGBoost and LightGBM, which are widely used in machine learning competitions and real-world applications.</p><p id="8d94" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">While Gradient Boosting requires more careful tuning than simpler algorithms — especially when adjusting the depth of decision trees, the learning rate, and the number of trees — it is very flexible and powerful. This makes it a top choice for problems with structured data.</p><p id="a1fe" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Gradient Boosting can handle complex relationships that simpler methods like AdaBoost might miss. Its continued popularity and ongoing improvements show that the approach of using gradients and building models step-by-step remains highly important in modern machine learning.</p><h1 id="b2d9" class="qw oz fq bf pa qx qy gv pe qz ra gy pi rb rc rd re rf rg rh ri rj rk rl rm rn bk">🌟 Gradient Boosting Regressor Code Summarized</h1><pre class="oi oj ok ol om pz qa qb bp qc bb bk"><span id="e01f" class="qd oz fq qa b bg qe qf l qg qh">import pandas as pd<br/>import numpy as np<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import root_mean_squared_error<br/>from sklearn.ensemble import GradientBoostingRegressor<br/><br/># Create dataset<br/>dataset_dict = {<br/>   'Outlook': ['sunny', 'sunny', 'overcast', 'rain', 'rain', 'rain', 'overcast', <br/>               'sunny', 'sunny', 'rain', 'sunny', 'overcast', 'overcast', 'rain',<br/>               'sunny', 'overcast', 'rain', 'sunny', 'sunny', 'rain', 'overcast',<br/>               'rain', 'sunny', 'overcast', 'sunny', 'overcast', 'rain', 'overcast'],<br/>   'Temp.': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0,<br/>             72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0,<br/>             88.0, 77.0, 79.0, 80.0, 66.0, 84.0],<br/>   'Humid.': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0,<br/>              90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0,<br/>              65.0, 70.0, 60.0, 95.0, 70.0, 78.0],<br/>   'Wind': [False, True, False, False, False, True, True, False, False, False, True,<br/>            True, False, True, True, False, False, True, False, True, True, False,<br/>            True, False, False, True, False, False],<br/>   'Num_Players': [52, 39, 43, 37, 28, 19, 43, 47, 56, 33, 49, 23, 42, 13, 33, 29,<br/>                   25, 51, 41, 14, 34, 29, 49, 36, 57, 21, 23, 41]<br/>}<br/><br/># Prepare data<br/>df = pd.DataFrame(dataset_dict)<br/>df = pd.get_dummies(df, columns=['Outlook'], prefix='', prefix_sep='')<br/>df['Wind'] = df['Wind'].astype(int)<br/><br/># Split features and target<br/>X, y = df.drop('Num_Players', axis=1), df['Num_Players']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)<br/><br/># Train Gradient Boosting<br/>gb = GradientBoostingRegressor(<br/>   n_estimators=50,     # Number of boosting stages (trees)<br/>   learning_rate=0.1,    # Shrinks the contribution of each tree<br/>   max_depth=3,          # Depth of each tree<br/>   subsample=0.8,        # Fraction of samples used for each tree<br/>   random_state=42<br/>)<br/>gb.fit(X_train, y_train)<br/><br/># Predict and evaluate<br/>y_pred = gb.predict(X_test)<br/>rmse = root_mean_squared_error(y_test, y_pred))<br/><br/>print(f"Root Mean Squared Error: {rmse:.2f}")</span></pre></div></div></div><div class="ab cb rp rq rr rs" role="separator"><span class="rt by bm ru rv rw"/><span class="rt by bm ru rv rw"/><span class="rt by bm ru rv"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="b961" class="oy oz fq bf pa pb pc pd pe pf pg ph pi nq pj pk pl nu pm pn po ny pp pq pr fw bk">Further Reading</h2><p id="9495" class="pw-post-body-paragraph nh ni fq nj b gt ps nl nm gw pt no np nq pu ns nt nu pv nw nx ny pw oa ob oc fj bk">For a detailed explanation of the <a class="af od" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html" rel="noopener ugc nofollow" target="_blank">GradientBoostingRegressor</a> and its implementation in scikit-learn, readers can refer to the official documentation, which provides comprehensive information on its usage and parameters.</p><h2 id="6253" class="oy oz fq bf pa pb pc pd pe pf pg ph pi nq pj pk pl nu pm pn po ny pp pq pr fw bk">Technical Environment</h2><p id="5264" class="pw-post-body-paragraph nh ni fq nj b gt ps nl nm gw pt no np nq pu ns nt nu pv nw nx ny pw oa ob oc fj bk">This article uses Python 3.7 and scikit-learn 1.6. While the concepts discussed are generally applicable, specific code implementations may vary slightly with different versions.</p><h2 id="dcdb" class="oy oz fq bf pa pb pc pd pe pf pg ph pi nq pj pk pl nu pm pn po ny pp pq pr fw bk">About the Illustrations</h2><p id="d4cc" class="pw-post-body-paragraph nh ni fq nj b gt ps nl nm gw pt no np nq pu ns nt nu pv nw nx ny pw oa ob oc fj bk">Unless otherwise noted, all images are created by the author, incorporating licensed design elements from Canva Pro.</p><p id="b78d" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝙀𝙣𝙨𝙚𝙢𝙗𝙡𝙚 𝙇𝙚𝙖𝙧𝙣𝙞𝙣𝙜 𝙝𝙚𝙧𝙚:</p><div class="mo mp mq mr ms"><div role="button" tabindex="0" class="ab bx cp kj it rx ry bp rz lw ao"><div class="sa l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by sb sc cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l sb sc em n ay um"/></div><div class="sd l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----c098d1ae425c--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq sg hp l"><h2 class="bf ga xj ic it xk iv iw mz iy ja fz bk">Ensemble Learning</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xl wl wm wn wo lj wp wq ux ii wr ws wt vb vc vd ep bm ve ou" href="https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----c098d1ae425c--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xm l il"><span class="bf b dy z dx">4 stories</span></div></div></div><div class="sp dz sq it ab sr il ed"><div class="ed sj bx sk sl"><div class="dz l"><img alt="" class="dz" src="../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*FBhxEgEzbfYWiSK0LYOv6g.gif"/></div></div><div class="ed sj bx kk sm sn"><div class="dz l"><img alt="" class="dz" src="../Images/22a5d43568e70222eb89fd36789a9333.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*-qqvZRF8gPn2fP8N-kS3nA.gif"/></div></div><div class="ed bx hx so sn"><div class="dz l"><img alt="" class="dz" src="../Images/8ea1a2f29053080a5feffc709f5b8669.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*FBDim33AJDmZUEDHk2z-tA.gif"/></div></div></div></div></div><p id="ae35" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:</p><div class="mo mp mq mr ms"><div role="button" tabindex="0" class="ab bx cp kj it rx ry bp rz lw ao"><div class="sa l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by sb sc cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l sb sc em n ay um"/></div><div class="sd l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----c098d1ae425c--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq sg hp l"><h2 class="bf ga xj ic it xk iv iw mz iy ja fz bk">Regression Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xl wl wm wn wo lj wp wq ux ii wr ws wt vb vc vd ep bm ve ou" href="https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----c098d1ae425c--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xm l il"><span class="bf b dy z dx">5 stories</span></div></div></div><div class="sp dz sq it ab sr il ed"><div class="ed sj bx sk sl"><div class="dz l"><img alt="A cartoon doll with pigtails and a pink hat. This “dummy” doll, with its basic design and heart-adorned shirt, visually represents the concept of a dummy regressor in machine. Just as this toy-like figure is a simplified, static representation of a person, a dummy regressor is a basic models serve as baselines for more sophisticated analyses." class="dz" src="../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png" width="194" height="194" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qMSGk19S51CXGl3DiAGuKw.png"/></div></div><div class="ed sj bx kk sm sn"><div class="dz l"><img alt="" class="dz" src="../Images/44e6d84e61c895757ff31e27943ee597.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*nMaPpVdNqCci31YmjfCMRQ.png"/></div></div><div class="ed bx hx so sn"><div class="dz l"><img alt="" class="dz" src="../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qTpdMoaZClu-KDV3nrZDMQ.png"/></div></div></div></div></div></div></div></div></div>    
</body>
</html>