- en: 'Exploring RAG Applications Across Languages: Conversing with the Mishnah'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索跨语言的RAG应用：与《密示拿》对话
- en: 原文：[https://towardsdatascience.com/exploring-rag-applications-across-languages-conversing-with-the-mishnah-16615c30f780?source=collection_archive---------6-----------------------#2024-05-23](https://towardsdatascience.com/exploring-rag-applications-across-languages-conversing-with-the-mishnah-16615c30f780?source=collection_archive---------6-----------------------#2024-05-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/exploring-rag-applications-across-languages-conversing-with-the-mishnah-16615c30f780?source=collection_archive---------6-----------------------#2024-05-23](https://towardsdatascience.com/exploring-rag-applications-across-languages-conversing-with-the-mishnah-16615c30f780?source=collection_archive---------6-----------------------#2024-05-23)
- en: Building a cross-lingual RAG system for Rabbinic texts
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为拉比经典文本构建跨语言RAG系统
- en: '[](https://medium.com/@stannor?source=post_page---byline--16615c30f780--------------------------------)[![Shlomo
    Tannor](../Images/4b37fdf045fd3ecc667b18f11f59d13f.png)](https://medium.com/@stannor?source=post_page---byline--16615c30f780--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--16615c30f780--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--16615c30f780--------------------------------)
    [Shlomo Tannor](https://medium.com/@stannor?source=post_page---byline--16615c30f780--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@stannor?source=post_page---byline--16615c30f780--------------------------------)[![Shlomo
    Tannor](../Images/4b37fdf045fd3ecc667b18f11f59d13f.png)](https://medium.com/@stannor?source=post_page---byline--16615c30f780--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--16615c30f780--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--16615c30f780--------------------------------)
    [Shlomo Tannor](https://medium.com/@stannor?source=post_page---byline--16615c30f780--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--16615c30f780--------------------------------)
    ·15 min read·May 23, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--16615c30f780--------------------------------)
    ·阅读时间15分钟·2024年5月23日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/64ab319bb28700369c727a6aed9632a5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/64ab319bb28700369c727a6aed9632a5.png)'
- en: 'Robot studying The Mishnah. Credit: DALL-E-3.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人学习《密示拿》。图片来源：DALL-E-3。
- en: 'Introduction:'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言：
- en: I’m excited to share my journey of building a unique Retrieval-Augmented Generation
    (RAG) application for interacting with rabbinic texts in this post. MishnahBot
    aims to provide scholars and everyday users with an intuitive way to query and
    explore the Mishnah¹ interactively. It can help solve problems such as quickly
    locating relevant source texts or summarizing a complex debate about religious
    law, extracting the bottom line.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我很高兴在这篇文章中分享我构建一个独特的检索增强生成（RAG）应用程序的过程，旨在与拉比经典文本进行互动。MishnahBot旨在为学者和普通用户提供一种直观的方式，交互式地查询和探索《密示拿》¹。它可以帮助解决诸如快速查找相关源文本或总结复杂的宗教法律辩论、提炼关键结论等问题。
- en: I had the idea for such a project a few years back, but I felt like the technology
    wasn’t ripe yet. Now, with advancements of large language models, and RAG capabilities,
    it is pretty straightforward.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 几年前，我就有了这个项目的想法，但当时觉得技术还不成熟。现在，随着大型语言模型和RAG能力的进步，已经变得相当简单。
- en: 'This is what our final product will look like, which you could try out [here](http://mishnahbot.us):'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们最终产品的样子，您可以在[这里](http://mishnahbot.us)试用：
- en: '![](../Images/5607109009d8f68fd72674d289d28098.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5607109009d8f68fd72674d289d28098.png)'
- en: '[MishnahBot](http://mishnahbot.us) website. Image by author.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[MishnahBot](http://mishnahbot.us)网站。图像来源：作者。'
- en: So what’s all the hype around RAG systems?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 那么，RAG系统为何如此备受关注呢？
- en: RAG applications are gaining significant attention, for improving accuracy and
    harnessing the reasoning power available in large language models (LLMs). Imagine
    being able to chat with your library, a collection of car manuals from the same
    manufacturer, or your tax documents. You can ask questions, and receive answers
    informed by the wealth of specialized knowledge.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: RAG应用正在获得广泛关注，因其能提高准确性并利用大型语言模型（LLM）的推理能力。想象一下，能够与您的图书馆、同一制造商的汽车手册集合或税务文件进行对话。你可以提出问题，并根据大量专业知识获得答案。
- en: '![](../Images/296c1fc2c4f56172f91b2d15afd0bf44.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/296c1fc2c4f56172f91b2d15afd0bf44.png)'
- en: 'Diagram of a typical RAG system’s architecture. Credit: [Amazon AWS Documentation](https://aws.amazon.com/what-is/retrieval-augmented-generation/#:~:text=Retrieval%2DAugmented%20Generation%20(RAG),sources%20before%20generating%20a%20response.).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 典型 RAG 系统架构的示意图。来源：[Amazon AWS Documentation](https://aws.amazon.com/what-is/retrieval-augmented-generation/#:~:text=Retrieval%2DAugmented%20Generation%20(RAG),sources%20before%20generating%20a%20response.)。
- en: '**Pros and Cons of RAG vs. Increased Context Length**'
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**RAG 与增加上下文长度的优缺点**'
- en: 'There are two emerging trends in improving language model interactions: Retrieval-Augmented
    Generation (RAG) and increasing context length, potentially by allowing very long
    documents as attachments.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在改进语言模型交互方面，有两种新兴趋势：检索增强生成（RAG）和增加上下文长度，可能通过允许非常长的文档作为附件来实现。
- en: One key advantage of RAG systems is cost-efficiency. With RAG, you can handle
    large contexts without drastically increasing the query cost, which can become
    expensive. Additionally, RAG is more modular, allowing you to plug and play with
    different knowledge bases and LLM providers. On the other hand, increasing the
    context length directly in language models is an exciting development that can
    enable handling much longer texts in a single interaction.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 系统的一个关键优势是成本效益。使用 RAG，你可以在不大幅增加查询成本的情况下处理大规模的上下文，而查询成本的增加可能会非常昂贵。此外，RAG
    更具模块化，允许你与不同的知识库和 LLM 提供商进行“即插即用”。另一方面，直接在语言模型中增加上下文长度是一个令人兴奋的发展，它可以使在单次交互中处理更长的文本成为可能。
- en: Setup
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置
- en: For this project, I used AWS SageMaker for my development environment, AWS Bedrock
    to access various LLMs, and the LangChain framework to manage the pipeline. Both
    AWS services are user-friendly and charge only for the resources used, so I really encourage you to try it out yourselves.
    For Bedrock, you’ll need to request access to Llama 3 70b Instruct and Claude
    Sonnet.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目，我使用了 AWS SageMaker 作为开发环境，AWS Bedrock 来访问各种 LLM，并使用 LangChain 框架来管理管道。这两个
    AWS 服务都非常易于使用，只按使用的资源收费，因此我强烈鼓励你们自己尝试。对于 Bedrock，你需要申请访问 Llama 3 70b Instruct
    和 Claude Sonnet。
- en: 'Let’s open a new Jupyter notebook, and install the packages we will be using:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打开一个新的 Jupyter notebook，并安装我们将使用的软件包：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Dataset
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集
- en: The dataset for this project is the Mishnah, an ancient Rabbinic text central
    to Jewish tradition. I chose this text because it is close to my heart and also
    presents a challenge for language models since it is a niche topic. The dataset
    was obtained from the [Sefaria-Export](https://github.com/Sefaria/Sefaria-Export)
    repository², a treasure trove of rabbinic texts with English translations aligned
    with the original Hebrew. This alignment facilitates switching between languages
    in different steps of our RAG application.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目的数据集是《米示那》，一部在犹太传统中占有核心地位的古老拉比文献。我选择这部文献是因为它与我个人有很大关系，同时也是语言模型的一个挑战，因为它是一个小众话题。数据集来自于[Sefaria-Export](https://github.com/Sefaria/Sefaria-Export)
    仓库²，这是一个拉比文献的宝库，包含与原始希伯来文对齐的英文翻译。这种对齐便于在我们 RAG 应用的不同步骤中切换语言。
- en: '*Note: The same process applied here can be applied to any other collection
    of texts of your choosing. This example also demonstrates how RAG technology can
    be utilized across different languages, as shown with Hebrew in this case.*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：这里应用的相同过程可以应用于您选择的任何其他文本集合。这个例子还演示了 RAG 技术如何跨不同语言使用，正如本例中使用希伯来语所示。*'
- en: Let’s Dive In
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让我们深入了解
- en: 1\. Loading the Dataset
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1. 加载数据集
- en: First we will need to download the relevant data. We will use git sparse-checkout
    since the full repository is quite large. Open the terminal window and run the
    following.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要下载相关数据。由于完整的仓库相当大，我们将使用 git sparse-checkout。打开终端窗口并运行以下命令。
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And… voila! we now have the data files that we need:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然后……瞧！我们现在拥有了所需的数据文件：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now let’s load the documents in our Jupyter notebook environment:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在 Jupyter notebook 环境中加载文档：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'And take a look at the Data:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 看看数据：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Looks good, we can move on to the vector database stage.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来不错，我们可以进入向量数据库阶段了。
- en: 2\. Vectorizing and Storing in ChromaDB
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2. 向量化并存储到 ChromaDB 中
- en: Next, we vectorize the text and store it in a local ChromaDB. In one sentence,
    the idea is to represent text as dense vectors — arrays of numbers — such that
    texts that are similar semantically will be “close” to each other in vector space.
    This is the technology that will enable us to retrieve the relevant passages given
    a query.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将文本向量化并将其存储在本地 ChromaDB 中。简而言之，思路是将文本表示为密集向量——数字数组——这样语义上相似的文本将在向量空间中彼此“接近”。这项技术将使我们能够在给定查询时检索相关的段落。
- en: We opted for a lightweight vectorization model, the `all-MiniLM-L6-v2`, which
    can run efficiently on a CPU. This model provides a good balance between performance
    and resource efficiency, making it suitable for our application. While state-of-the-art
    models like OpenAI’s `text-embedding-3-large` may offer superior performance,
    they require substantial computational resources, typically running on GPUs.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了一个轻量级的向量化模型`all-MiniLM-L6-v2`，它可以在CPU上高效运行。这个模型在性能和资源效率之间提供了良好的平衡，适用于我们的应用程序。虽然像OpenAI的`text-embedding-3-large`等最先进的模型可能提供更优的性能，但它们需要大量计算资源，通常需要在GPU上运行。
- en: For more information about embedding models and their performance, you can refer
    to the [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard) which
    compares various text embedding models on multiple tasks.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解有关嵌入模型及其性能的更多信息，可以参考[MTEB排行榜](https://huggingface.co/spaces/mteb/leaderboard)，该排行榜比较了多种文本嵌入模型在多个任务上的表现。
- en: 'Here’s the code we will use for vectorizing (should only take a few minutes
    to run on this dataset on a CPU machine):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将用于向量化的代码（在CPU机器上运行时应该只需几分钟）：
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 3\. Creating Our RAG in English
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 用英语创建我们的RAG
- en: With our dataset ready, we can now create our Retrieval-Augmented Generation
    (RAG) application in English. For this, we’ll use LangChain, a powerful framework
    that provides a unified interface for various language model operations and integrations,
    making it easy to build sophisticated applications.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 有了准备好的数据集，我们现在可以用英语创建我们的检索增强生成（RAG）应用程序。为此，我们将使用LangChain，一个强大的框架，提供了统一的接口来处理各种语言模型操作和集成，使得构建复杂应用变得更加容易。
- en: LangChain simplifies the process of integrating different components like language
    models (LLMs), retrievers, and vector stores. By using LangChain, we can focus
    on the high-level logic of our application without worrying about the underlying
    complexities of each component.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain简化了集成不同组件（如语言模型（LLM）、检索器和向量存储）的过程。通过使用LangChain，我们可以专注于应用程序的高级逻辑，而无需担心每个组件的底层复杂性。
- en: 'Here’s the code to set up our RAG system:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这是设置我们RAG系统的代码：
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Explanation:'
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释：
- en: '**AWS Bedrock Initialization:** We initialize AWS Bedrock with Llama 3 70B
    Instruct. This model will be used for generating responses based on the retrieved
    context.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**AWS Bedrock Initialization：** 我们使用Llama 3 70B Instruct初始化AWS Bedrock。这个模型将用于基于检索到的上下文生成响应。'
- en: '**Prompt Template:** The prompt template is defined to format the context and
    question into a structure that the LLM can understand. This helps in generating
    concise and relevant answers. Feel free to play around and adjust the template
    as needed.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Prompt Template：** 提示模板的定义是为了将上下文和问题格式化为LLM能够理解的结构。这有助于生成简洁且相关的答案。你可以随意尝试并根据需要调整模板。'
- en: '**Embedding Model:** We use the ‘all-MiniLM-L6-v2’ model for generating embeddings
    for the queries as well. We hope the query will have similar representation to
    relevant answer paragraphs. Note: In order to boost retrieval performance, we
    could use an LLM to modify and optimize the user query so that it is more similar
    to the style of the RAG database.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Embedding Model：** 我们同样使用‘all-MiniLM-L6-v2’模型为查询生成嵌入。我们希望查询能与相关答案段落具有相似的表示方式。注意：为了提升检索性能，我们可以使用LLM来修改和优化用户查询，使其更接近RAG数据库的风格。'
- en: '**LLM Chain:** The `LLMChain` class from LangChain is used to manage the interaction
    between the LLM and the retrieved context.'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**LLM Chain：** LangChain中的`LLMChain`类用于管理LLM与检索到的上下文之间的互动。'
- en: '**SimpleQAChain:** This custom class integrates the retriever and the LLM chain.
    It retrieves relevant paragraphs, formats them into a context, and generates an
    answer.'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**SimpleQAChain：** 这个自定义类集成了检索器和LLM链。它检索相关段落，将其格式化为上下文，并生成答案。'
- en: Alright! Let’s try it out! We will use a query related to the very first paragraphs
    in the Mishnah.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 好的！让我们试试看！我们将使用一个与《密西拿》第一段相关的查询。
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: That seems pretty accurate.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来相当准确。
- en: 'Let’s try a more sophisticated question:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一个更复杂的问题：
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Very nice.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 非常好。
- en: Could We Have Achieved the Same Thing by Querying Claude Directly?
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们是否能通过直接查询Claude来实现同样的效果？
- en: 'I tried that out, here’s what I got:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我试了一下，以下是我得到的结果：
- en: '![](../Images/2b7b973a645d3c9def1bd93406568e67.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b7b973a645d3c9def1bd93406568e67.png)'
- en: Claude Sonnet fails to give an exact answer to the question. Image by author.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Claude Sonnet未能给出问题的确切答案。图像由作者提供。
- en: The response is long and not to the point, and the answer that is given is incorrect
    (*reaping* is the third type of work in the list, while *selecting* is the seventh).
    This is what we call a *hallucination*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 回答冗长且不切题，给出的答案是错误的（*收获*是列表中的第三项，而*选择*是第七项）。这就是我们所说的*幻觉*。
- en: 'While Claude is a powerful language model, relying solely on an LLM for generating
    responses from memorized training data or even using internet searches lacks the
    precision and control offered by a custom database in a Retrieval-Augmented Generation
    (RAG) application. Here’s why:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Claude是一个强大的语言模型，但仅依赖LLM从记忆化的训练数据生成回答，甚至通过互联网搜索生成答案，缺乏定制数据库在检索增强生成（RAG）应用中的精准性和控制力。原因如下：
- en: '**Precision and Context:** Our RAG application retrieves exact paragraphs from
    a custom database, ensuring high relevance and accuracy. Claude, without specific
    retrieval mechanisms, might not provide the same level of detailed and context-specific
    responses.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**精准性与上下文：**我们的RAG应用从定制数据库中检索精确的段落，确保高相关性和准确性。没有特定检索机制的Claude，可能无法提供同样详细且具有上下文特定性的回答。'
- en: '**Efficiency:** The RAG approach efficiently handles large datasets, combining
    retrieval and generation to maintain precise and contextually relevant answers.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**效率：**RAG方法高效地处理大规模数据集，将检索与生成相结合，以保持精确且与上下文相关的答案。'
- en: '**Cost-Effectiveness:** By utilizing a relatively small LLM such as Llama 3
    70B Instruct, we achieve accurate results without needing to send a large amount
    of data with each query. This reduces costs associated with using larger, more
    resource-intensive models.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**性价比：**通过使用像Llama 3 70B Instruct这样相对较小的LLM，我们能够在不需要每次查询都传送大量数据的情况下获得准确结果。这减少了使用更大、更占资源的模型所带来的成本。'
- en: This structured retrieval process ensures users receive the most accurate and
    relevant answers, leveraging both the language generation capabilities of LLMs
    and the precision of custom data retrieval.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结构化的检索过程确保用户获得最准确和最相关的答案，利用了大型语言模型（LLM）的语言生成能力和定制数据检索的精准性。
- en: 4\. Cross-Lingual RAG Approach
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4. 跨语言RAG方法
- en: Finally, we will address the challenge of interacting in Hebrew with the original
    Hebrew text. The same approach can be applied to any other language, as long as
    you are able to translate the texts to English for the retrieval stage.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将解决与原始希伯来语文本进行交互的挑战。只要能够将文本翻译成英语以进行检索阶段，同样的方法可以应用于任何其他语言。
- en: Supporting Hebrew interactions adds an extra layer of complexity since embedding
    models and large language models (LLMs) tend to be stronger in English. While
    some embedding models and LLMs do support Hebrew, they are often less robust than
    their English counterparts, especially the smaller embedding models that likely
    focused more on English during training.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 支持希伯来语交互增加了额外的复杂性，因为嵌入模型和大型语言模型（LLMs）在英语中通常表现得更强。虽然一些嵌入模型和LLMs支持希伯来语，但它们通常不如英语模型强大，尤其是那些较小的嵌入模型，在训练过程中可能更多地集中在英语上。
- en: To tackle this, we could train our own Hebrew embedding model. However, another
    practical approach is to leverage a one-time translation of the text to English
    and use English embeddings for the retrieval process. This way, we benefit from
    the strong performance of English models while still supporting Hebrew interactions.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们可以训练自己的希伯来语嵌入模型。然而，另一种实际的方法是利用文本的一次性翻译为英语，并使用英语嵌入进行检索过程。通过这种方式，我们既能从英语模型的强大性能中受益，又能支持希伯来语的交互。
- en: Processing Steps
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理步骤
- en: '![](../Images/05b43dfff3bca18dd0a9bc6649d0fb61.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05b43dfff3bca18dd0a9bc6649d0fb61.png)'
- en: Diagram of cross-lingual RAG Architecture. Image by author.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 跨语言RAG架构图。图片来源：作者。
- en: 'In our case, we already have professional human translations of the Mishnah
    text into English. We will use this to ensure accurate retrievals while maintaining
    the integrity of the Hebrew responses. Here’s how we can set up this cross-lingual
    RAG system:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们已经有了《密士拿》文本的专业英文翻译。我们将利用这些翻译确保准确的检索，同时保持希伯来语回答的完整性。以下是我们如何设置这个跨语言RAG系统的方式：
- en: '**Input Query in Hebrew:** Users can input their queries in Hebrew.'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**用希伯来语输入查询：**用户可以用希伯来语输入他们的查询。'
- en: '**Translate the Query to English:** We use an LLM to translate the Hebrew query
    into English.'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**将查询翻译为英语：**我们使用LLM将希伯来语查询翻译成英语。'
- en: '**Embed the Query:** The translated English query is then embedded.'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**嵌入查询：**然后将翻译后的英语查询进行嵌入。'
- en: '**Find Relevant Documents Using English Embeddings:** We use the English embeddings
    to find relevant documents.'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**使用英文嵌入查找相关文档：** 我们使用英文嵌入来查找相关文档。'
- en: '**Retrieve Corresponding Hebrew Texts:** The corresponding Hebrew texts are
    retrieved as context. Essentially we are using the English texts as *keys* and
    the Hebrew texts as the corresponding *values* in the retrieval operation.'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**使用英文嵌入检索相关希伯来文本：** 检索到相应的希伯来文本作为上下文。基本上，我们将英文文本作为*键*，将希伯来文本作为检索操作中的相应*值*。'
- en: '**Respond in Hebrew Using an LLM:** An LLM generates the response in Hebrew
    using the Hebrew context.'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**使用LLM用希伯来语回应：** LLM使用希伯来语上下文生成希伯来语回应。'
- en: For generation, we use Claude Sonnet since it performs significantly better
    on Hebrew text compared to Llama 3.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生成，我们使用Claude Sonnet，因为它在处理希伯来文本时比Llama 3表现得更好。
- en: 'Here is the code implementation:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这是代码实现：
- en: '[PRE12]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let’s try it! We will use the same question as before, but in Hebrew this time:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试看！这次我们使用和之前相同的问题，但用希伯来语提问：
- en: '[PRE13]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We got an accurate, one word answer to our question. Pretty neat, right?
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了一个准确的一字回答。相当酷吧？
- en: Interesting Challenges and Solutions
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有趣的挑战与解决方案
- en: The translation with Llama 3 Instruct posed several challenges. Initially, the
    model produced nonsensical results no matter what I tried. (Apparently, Llama
    3 instruct is very sensitive to prompts starting with a new line character!)
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Llama 3 Instruct进行翻译时遇到了一些挑战。最初，不管我尝试什么，模型都会产生毫无意义的结果。（显然，Llama 3 Instruct对以换行符开头的提示非常敏感！）
- en: After resolving that issue, the model tended to output the correct response,
    but then continue with additional irrelevant text, so stopping the output at a
    newline character proved effective.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 解决了这个问题后，模型倾向于输出正确的回答，但随后会继续输出一些无关的文本，所以在换行符处停止输出证明是有效的。
- en: Controlling the output format can be tricky. Some strategies include requesting
    a JSON format or providing examples with few-shot prompts.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 控制输出格式可能会很棘手。一些策略包括请求JSON格式或通过少量示例提示提供范例。
- en: In this project, we also remove vowels from the Hebrew texts since most Hebrew
    text online does not include vowels, and we want the context for our LLM to be
    similar to text seen during pretraining.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们还从希伯来文本中去除了元音，因为大多数在线希伯来文本不包含元音，我们希望为我们的LLM提供与预训练时看到的文本相似的上下文。
- en: Conclusion
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Building this RAG application has been a fascinating journey, blending the nuances
    of ancient texts with modern AI technologies. My passion for making the library
    of ancient rabbinic texts more accessible to everyone (myself included) has driven
    this project. This technology enables chatting with your library, searching for
    sources based on ideas, and much more. The approach used here can be applied to
    other treasured collections of texts, opening up new possibilities for accessing
    and exploring historical and cultural knowledge.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 构建这个RAG应用程序是一次令人着迷的旅程，融合了古代文献的细微差别与现代AI技术。我希望让古代拉比学文献对所有人（包括我自己）更易获取的热情驱动了这个项目。这项技术使得与你的文库进行对话、根据思想搜索资料以及更多功能成为可能。这里使用的方法可以应用于其他珍贵的文本集，为访问和探索历史与文化知识开辟了新的可能性。
- en: It’s amazing to see how all this can be accomplished in just a few hours, thanks
    to the powerful tools and frameworks available today. Feel free to check out the
    full code on [GitHub](https://github.com/shlomota/MishnahBot), and play with the
    [MishnahBot](http://mishnahbot.us) website.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 真令人惊讶，今天强大的工具和框架使得这一切在短短几小时内就能完成。欢迎查看完整代码在[GitHub](https://github.com/shlomota/MishnahBot)上，并尝试使用[MishnahBot](http://mishnahbot.us)网站。
- en: Please share your comments and questions, especially if you’re trying out something
    similar. If you want to see more content like this in the future, do let me know!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 请分享您的评论和问题，特别是如果你尝试做类似的事情。如果你希望将来看到更多类似的内容，请告诉我！
- en: Footnotes
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 脚注
- en: The [Mishnah](https://en.wikipedia.org/wiki/Mishnah) is one of the core and
    earliest rabbinic works which serves as the basis for the Talmud.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[密书](https://en.wikipedia.org/wiki/Mishnah)是最核心和最早的拉比学著作之一，是塔木德的基础。'
- en: The licenses for the texts differ and are detailed in the corresponding JSON
    files within the repository. The Hebrew texts used in this project are in the
    public domain. The English translations are from the Mishnah Yomit translation
    by Dr. Joshua Kulp and are licensed under a CC-BY license.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文本的许可协议不同，详细信息可以在仓库中的相应JSON文件中找到。此项目使用的希伯来文本属于公有领域。英文翻译来自Dr. Joshua Kulp的《密书每日翻译》，并且其许可协议为CC-BY。
- en: '*Shlomo Tannor is an AI/ML engineer at Avanan (A Check Point Company), specializing
    in leveraging NLP and ML to enhance cloud email security. He holds an MSc in Computer
    Science with a thesis in NLP and a BSc in Mathematics and Computer Science.*'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*Shlomo Tannor 是Avanan（一个Check Point公司）的AI/ML工程师，专注于利用NLP和ML技术提升云端邮件安全。他拥有计算机科学硕士学位，论文方向为NLP，并持有数学与计算机科学学士学位。*'
