- en: Linear Attention Is All You Need
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性注意力就是你所需要的一切
- en: 原文：[https://towardsdatascience.com/linear-attention-is-all-you-need-5fa9c845c1b5?source=collection_archive---------2-----------------------#2024-06-02](https://towardsdatascience.com/linear-attention-is-all-you-need-5fa9c845c1b5?source=collection_archive---------2-----------------------#2024-06-02)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/linear-attention-is-all-you-need-5fa9c845c1b5?source=collection_archive---------2-----------------------#2024-06-02](https://towardsdatascience.com/linear-attention-is-all-you-need-5fa9c845c1b5?source=collection_archive---------2-----------------------#2024-06-02)
- en: '**Self-attention at a fraction of the cost?**'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**自注意力能以更低的成本实现？**'
- en: '[](https://medium.com/@sam.maddrellmander?source=post_page---byline--5fa9c845c1b5--------------------------------)[![Sam
    Maddrell-Mander](../Images/709060d916a5a281f4cf016d7e82e4d9.png)](https://medium.com/@sam.maddrellmander?source=post_page---byline--5fa9c845c1b5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5fa9c845c1b5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5fa9c845c1b5--------------------------------)
    [Sam Maddrell-Mander](https://medium.com/@sam.maddrellmander?source=post_page---byline--5fa9c845c1b5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@sam.maddrellmander?source=post_page---byline--5fa9c845c1b5--------------------------------)[![Sam
    Maddrell-Mander](../Images/709060d916a5a281f4cf016d7e82e4d9.png)](https://medium.com/@sam.maddrellmander?source=post_page---byline--5fa9c845c1b5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5fa9c845c1b5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5fa9c845c1b5--------------------------------)
    [Sam Maddrell-Mander](https://medium.com/@sam.maddrellmander?source=post_page---byline--5fa9c845c1b5--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5fa9c845c1b5--------------------------------)
    ·9 min read·Jun 2, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5fa9c845c1b5--------------------------------)
    ·9分钟阅读·2024年6月2日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/0d52a880d9c5eb18caffe34d6353d8ee.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0d52a880d9c5eb18caffe34d6353d8ee.png)'
- en: Photo by [Guillaume Jaillet](https://unsplash.com/@i_am_g?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Guillaume Jaillet](https://unsplash.com/@i_am_g?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '***“Attention scales badly with long sequence lengths”***'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '***“注意力在处理长序列时表现较差”***'
- en: This is the kind of thing anyone who’s spent much time working with transformers
    and self-attention will have heard a hundred times. It’s both absolutely true,
    we’ve all experienced this as you try to increase the context size of your model
    everything suddenly comes to a grinding halt. But then at the same time, virtually
    every week it seems, there’s a new state of the art model with a new record breaking
    context length. (Gemini has context length of 2M tokens!)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这是任何花费大量时间研究变换器和自注意力机制的人都听过一百次的内容。它绝对是正确的，我们都经历过，当你尝试增加模型的上下文大小时，一切突然停滞不前。但与此同时，似乎几乎每周都有一个新的前沿模型，创造了一个新的上下文长度纪录。（Gemini的上下文长度达到了2M个标记！）
- en: There are lots of sophisticated methods like RingAttention that make training
    incredibly long context lengths in large distributed systems possible, but what
    I’m interested in today is a simpler question.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多复杂的方法，比如RingAttention，它使得在大型分布式系统中训练极长的上下文长度成为可能，但今天我关注的是一个更简单的问题。
- en: How far can we get with linear attention alone?
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 单靠线性注意力，我们能做到什么程度？
- en: '**Let’s break down the maths.**'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**让我们来解析一下数学。**'
- en: This will be a bit of a whistle stop tour, but bear with me as we touch on a
    few key points before digging into the results.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是一次快速浏览，但请耐心等待，我们将在深入分析结果之前，简要讨论一些关键点。
- en: 'We can basically summarise the traditional attention mechanism with two key
    points:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基本上可以用两个关键点总结传统的注意力机制：
- en: First, the typical softmax attention expression takes the product of the query
    and key matrices, normalises for stability, then takes the softmax (row wise)
    to get the attention scores between each element of the sequence.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，典型的softmax注意力表达式是将查询矩阵和键矩阵相乘，进行归一化以保持稳定性，然后按行进行softmax运算，从而得到序列中每个元素之间的注意力分数。
- en: Second, the time complexity is dominated by the N² dot products, and the one
    inside the softmax is the limiting factor. That’s where we compute the attention
    scores.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，时间复杂度主要由N²的点积决定，而softmax内部的部分是限制因素。这就是我们计算注意力分数的地方。
- en: 'This is expressed in the traditional form as:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 传统形式可以表达为：
- en: '![](../Images/378929b0da93f20b26a13517c2daf911.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/378929b0da93f20b26a13517c2daf911.png)'
- en: Traditional formulation of the softmax attention mechansm.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: softmax 注意力机制的传统公式。
- en: It turns out if we ask our mathematician friends we can think about this slightly
    differently. The softmax can be thought of as one of many ways of describing the
    probability distribution relating tokens with each other. We can use any similarity
    measure we like (the dot product being one of the simplest) and so long as we
    normalise it, we’re fine.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，如果我们请教一下数学家朋友，我们可以稍微换个角度思考这个问题。softmax 可以被看作是描述与其他 token 之间的概率分布的多种方式之一。我们可以使用任何我们喜欢的相似度度量（点积是最简单的之一），只要我们进行归一化，就没问题。
- en: '![](../Images/2a388afc1bfe8d1681fd60e1b4177647.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2a388afc1bfe8d1681fd60e1b4177647.png)'
- en: General expression for attention using any similarity function.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 使用任何相似度函数的注意力的通用表达式。
- en: It’s a little sloppy to say this *is* attention, as in fact it’s only the attention
    we know and love when the similarity function is the exponential of the dot product
    of queries and keys (given below) as we find in the softmax. But this is where
    it gets interesting, if instead of using this this expression what if we could
    approximate it?
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 说这就是注意力有点草率，因为实际上只有当相似度函数是查询和键的点积的指数（如下所示），并且我们在 softmax 中找到了这个时，它才是我们所知的注意力。但有趣的是，如果我们不使用这个表达式，而是尝试逼近它呢？
- en: '![](../Images/8ee1bffdeb7e4632dc67d4af146542bb.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8ee1bffdeb7e4632dc67d4af146542bb.png)'
- en: Approximate the similarity function from self-attention with two feature maps.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 用两个特征图逼近自注意力中的相似度函数。
- en: We can assume there is some feature map “*phi*” which gives us a result *nearly*
    the same as taking the exponential of the dot product. And crucially, writing
    the expression like this allows us to play with the order of matrix multiplication
    operations.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以假设存在某个特征图“*phi*”，它能给出一个结果，*几乎*与取点积的指数相同。关键是，以这种方式写表达式允许我们改变矩阵乘法操作的顺序。
- en: 'In the [paper](https://arxiv.org/abs/2006.16236) they propose the Exponential
    Lineaer Unit (ELU) as the feature map due to a number of useful properties:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在[论文](https://arxiv.org/abs/2006.16236)中，他们提出了指数线性单元（ELU）作为特征图，因为它具有一些有用的属性：
- en: '![](../Images/5e24e7c42a47432c427608cf089dc3e6.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e24e7c42a47432c427608cf089dc3e6.png)'
- en: For values above 0 the ELU(x) gives a linear result, which while not the same
    as the exponential does preserve the relative ordering between scores.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于大于0的值，ELU(x) 给出线性结果，尽管与指数不同，但仍然保留了分数之间的相对顺序。
- en: For values less than or equal to 0 the exponential term preserves the continuous
    nature of the function, and ensures the gradients don’t just vanish.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于小于或等于0的值，指数项保持了函数的连续性，并确保梯度不会消失。
- en: '![](../Images/7769c7f88acf8b4fcbbbce09ea6aabb6.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7769c7f88acf8b4fcbbbce09ea6aabb6.png)'
- en: We won’t spend too much more time on this here, but this is pretty well empirically
    verified as a fair approximation to the softmax function.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里不会花太多时间讨论这个问题，但这已经通过经验得到了验证，作为 softmax 函数的一个合理逼近。
- en: What this allows us to do is change the order of operations. We can take the
    product of our feature map of K with V first to make a KV block, then the product
    with Q. The square product becomes over the model dimension size rather than sequence
    length.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够改变操作的顺序。我们可以首先取特征图 K 和 V 的乘积，形成一个 KV 块，然后与 Q 相乘。平方积变成了在模型维度上进行而非序列长度上进行。
- en: 'Putting this all together into the linear attention expression gives us:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 将这一切组合到线性注意力的表达式中，我们得到：
- en: '![](../Images/c1c7a658306ca21041ef4bf63e8d4305.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c1c7a658306ca21041ef4bf63e8d4305.png)'
- en: Linear attention using feature maps to approximate the softmax similarity score.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 使用特征图来逼近 softmax 相似度分数的线性注意力。
- en: Where we only need to compute the terms in the brackets once per query row.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要每个查询行计算括号内的项一次。
- en: '*(If you want to dig into how the casual masking fits into this and how the
    gradients are calculated, take a look at the paper. Or watch this space for a
    future blog.)*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*(如果你想深入了解因果遮掩是如何融入其中以及梯度是如何计算的，可以查看论文，或者关注未来的博客。)*'
- en: '**How much faster is linear attention anyway?**'
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**线性注意力到底快多少？**'
- en: The mathematical case is strong, but personally until I’ve seen some benchmarks
    I’m always a bit suspicious.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上的情况很有力，但就个人而言，在看到一些基准测试之前，我总是有些怀疑。
- en: Let’s start by looking at the snippets of the code to describe each of these
    terms. The softmax attention will look very familiar, we’re not doing anything
    fancy here.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们先从代码片段入手，描述这些术语。softmax 注意力看起来非常熟悉，这里我们没有做任何复杂的操作。
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Then for the linear attention we start by getting the Query, Key and Value matrices,
    then apply the ELU(x) feature mapping to the Query and Keys. Then we use einsum
    notation to perform the multiplications.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，对于线性注意力，我们首先获取查询、键和值矩阵，然后对查询和键应用ELU(x)特征映射。接着使用einsum符号进行乘法运算。
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Seeing this written in code is all well and good, but what does it actually
    mean experimentally? How much of a performance boost are we talking about here?
    It can be hard to appreciate the degree of speed up going from a quadratic to
    a linear bottleneck, so I’ve run the following experiemnt.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中看到这些是很好，但它在实验中实际上意味着什么呢？我们在这里谈论的性能提升到底有多大？从二次瓶颈到线性瓶颈的速度提升可能很难直接感受到，因此我进行了以下实验。
- en: We’re going to to take a single attention layer, with a fixed d_k model dimension
    of 64, and benchmark the time taken for a forward pass of a 32 batch size set
    of sequences. The only variable to change will be the sequence length, spanning
    128 up to 6000 (the GPT-3 context length for reference if 2048). Each run is done
    100 times to get a mean and standard deviation, and experiments are run using
    an Nvidia T4 GPU.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采取一个单独的注意力层，固定的d_k模型维度为64，并基准测试32批次序列的前向传播时间。唯一需要变化的变量是序列长度，范围从128到6000（参考GPT-3的上下文长度为2048）。每次运行进行100次，以获得平均值和标准差，实验使用的是Nvidia
    T4 GPU。
- en: For such a simple experiment the results are pretty striking.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于如此简单的实验，结果相当惊人。
- en: '![](../Images/cf50445d2ef3cead3386f1d3ca801fa5.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf50445d2ef3cead3386f1d3ca801fa5.png)'
- en: 'Benchmarks: Measuring the time per iteration for a single sequence with both
    traditional (softmax) attention and linear attention. Each sequence length is
    averaged over 100 iterations and the standard deviation plotted. Sequence lengths
    used range from 128 to 6000\. The ratio is is also shown to more easily gauge
    the increased performance.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试：测量传统（softmax）注意力和线性注意力下，单个序列每次迭代的时间。每个序列长度在100次迭代中取平均，标准差也会被绘制出来。使用的序列长度范围是从128到6000。比率图也显示出来，便于更容易地衡量性能提升。
- en: The results show for even an incredibly small toy example that we get a speed
    up of up to 60x.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，即使是在一个极小的玩具示例中，我们也能获得最多60倍的加速。
- en: Discussion
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'There are a few obvious take-aways here:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几个显而易见的收获：
- en: The advantage of linear attention is huge — either in speed, higher throughput
    is always a good thing. Or in terms of memory requirements to process long sequences.
    In low memory environments this could be a big advantage.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 线性注意力的优势巨大——无论是在速度上，较高的吞吐量总是件好事；还是在处理长序列时的内存需求上。在低内存环境中，这可能是一个很大的优势。
- en: The ratio plot has a surprising kink — leads us to suspect there’s some additional
    lower level optimisation happening here meaning the expected ratio doesn’t quite
    materalise. So we need to take this result with a pinch of salt.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比例图显示了一个令人惊讶的弯曲——这让我们怀疑这里发生了一些额外的底层优化，这意味着预期的比例并没有完全实现。所以我们需要对这个结果保持一定的怀疑态度。
- en: For completeness also do not mistake this as saying *“linear attention is 60x
    faster for small models”.* In reality the feed-forward layers are often a bigger
    chunk of the parameters in a Transformer and the encoding / decoding is often
    a limiting size component as well. But in this tightly defined problem, pretty
    impressive!
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 完整性考虑，别误解为说*“线性注意力对于小模型来说是60倍快”*。实际上，前馈层通常占据了 Transformer 中大部分的参数，而编码/解码也是一个限制性大小的组件。但在这个严格定义的问题中，结果相当令人印象深刻！
- en: Computational Complexity
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算复杂度
- en: If we think about the real time complexity of each approach we can show where
    this difference comes from.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑每种方法的实际时间复杂度，我们可以展示这个差异的来源。
- en: Let’s break down the time complexity of the traditional softmax attention, the
    first term gives the complexity of QK multiplication which is n² scores, each
    a dot product of length d_k. The second term describes the complexity of the softmax
    on the attention scores, is in n². And the third term takes the n² matrix and
    dots it with the values vector.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解传统softmax注意力的时间复杂度，第一个项给出了QK乘法的复杂度，这是n²个分数，每个都是长度为d_k的点积。第二项描述了对注意力分数进行softmax操作的复杂度，也是n²。第三项则将n²矩阵与值向量进行点积运算。
- en: If we assume for simplicity the query, key and vector matries have the same
    dimension we get the final term with the dominant n² term. (Provided the model
    dimension is << sequence length. )
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们简化假设查询、键和值矩阵具有相同的维度，那么我们得到最终项，其中主导项是n²项。（前提是模型维度远小于序列长度。）
- en: '![](../Images/f6453c03d33ccf18589f620e457f8ad2.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6453c03d33ccf18589f620e457f8ad2.png)'
- en: Normal self-attention softmax is dominated by the n² term from the sequence
    length, where usually the model dimmension d_k is << n.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 普通自注意力的softmax操作受序列长度的n²项主导，通常模型的维度d_k远小于n。
- en: The linear attention tells a different story. Again, if we look at the expression
    below for the time complexity we’ll analyse each of the terms.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 线性注意力讲述了一个不同的故事。同样，如果我们查看下面的时间复杂度表达式，我们将分析每一项。
- en: The first term is the cost of applying the feature map to the Q and K matrices,
    the second term is the product between the Q and V matricies which results in
    a (d_k, d_v) matrix, and the K(QV) multiplication has the same complexity in the
    third term. Then the final output, again assuming the model dimensions are the
    same for the different matricies, gives a final complexity linear in sequence
    length, and quadratic in model dimension.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 第一项是将特征映射应用于Q和K矩阵的成本，第二项是Q和V矩阵的乘积，结果是一个(d_k, d_v)矩阵，而K(QV)乘法在第三项中的复杂度相同。然后，最终输出再次假设模型维度对于不同的矩阵相同，最终的复杂度是线性与序列长度相关，并且与模型维度二次方成正比。
- en: '![](../Images/8e93e08c9c7502953dd226fcd9fbe87f.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8e93e08c9c7502953dd226fcd9fbe87f.png)'
- en: Linear self attention flips the script here, and is linear in n and quadratic
    in the model dimension (if the dk and dv hidden dimensions are the same as I’ve
    done here for simplicity.) So in any regime where n >> dk the complexity here
    is much lower.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 线性自注意力改变了这种局面，其在n上是线性的，而在模型维度上是二次方的（如果dk和dv的隐藏维度与我在这里所做的相同，便于简化）。因此，在n >> dk的情况下，复杂度明显较低。
- en: Therefore, so long as the model dimension is less than the sequence length we
    have a significantly faster model. The only real question left then is, how good
    an approximation is it anyway?
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，只要模型的维度小于序列长度，我们就能得到一个显著更快的模型。剩下的唯一问题是，它的近似效果究竟有多好？
- en: '**No free lunch — can we actually train a model?**'
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**没有免费的午餐——我们真的能训练一个模型吗？**'
- en: Enough messing around, hopefully we’re all convinced that the linear attention
    is much faster than traditional attention so let’s do the real test. Can we actually
    train the models and do they perform similarly with the two different attention
    mechanisms?
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 够了，足够的实验，希望我们都确信线性注意力比传统注意力要快得多，接下来进行真正的测试。我们能否真正训练模型，并且它们在两种不同注意力机制下的表现相似？
- en: The models we use here are really small — (and if there’s interest in a deeper
    dive into setting up a simple training harness we can look at that in the future)
    — and the data is simple. We’re just going to use the Penn Treebank dataset (publicly
    available through [*torchtext*](https://pytorch.org/text/0.8.1/datasets.html#penntreebank)*)*,
    which contains a collection of short snippets of text, which can be used to model
    / test small language models.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用的模型非常小（如果有兴趣深入探讨如何设置一个简单的训练框架，我们可以在未来再看看这个问题），而且数据也很简单。我们将使用Penn Treebank数据集（通过[*torchtext*](https://pytorch.org/text/0.8.1/datasets.html#penntreebank)公开提供），它包含了一些简短的文本片段，可以用来建模/测试小型语言模型。
- en: '**Can we train a real model to do real prediction**'
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**我们能训练一个真正的模型来进行真正的预测吗**'
- en: Real prediction might be a bit of a stretch here if we’re honest, given the
    number of parameters and time we’re training for all I’m really going to look
    for is do the training dynamics look similar. We’ll look at the loss curves for
    autoregressive training on a simple language modelling dataset, and if they follow
    the same shape we can at least have some confidence that the different mechanisms
    are giving us similar results.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们诚实一点，实际预测可能有点过于夸张，考虑到我们训练所用的参数数量和时间，实际上我真正想观察的是训练动态是否相似。我们将查看在一个简单语言建模数据集上进行自回归训练的损失曲线，如果它们遵循相同的形状，我们至少可以有一些信心，认为不同的机制给出了相似的结果。
- en: The nature of the data means the outputs are rarely of high quality, but it
    gives all the trapping we’d expect of a proper training run.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的性质意味着输出质量很少能达到高标准，但它提供了我们期望的所有正确训练过程的特征。
- en: Let’s look at the training curves. The plot on the left shows the loss for training
    and validation for both the traditional and linear attention methods. We can see
    over the 10 epochs the two approaches are basically indistinguishable. Similarly
    if we look at the right plot, the loss for the traditional softmax and the linear
    attention is shown, again showing absolutely identical training dynamics.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下训练曲线。左图展示了传统注意力方法和线性注意力方法在训练和验证中的损失。我们可以看到，在10个周期内，这两种方法基本上是无法区分的。同样地，如果我们查看右图，传统softmax和线性注意力的损失也表现得完全相同，显示了相同的训练动态。
- en: '![](../Images/6f15ba1d247f6c9eb8448f787a94befe.png)![](../Images/849d62a5006a461e76f5cdc22ff3802e.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f15ba1d247f6c9eb8448f787a94befe.png)![](../Images/849d62a5006a461e76f5cdc22ff3802e.png)'
- en: (left) Training and validation losses per epoch for both linear and traditional
    attention, (right) training loss curves for linear and traditional attention mechanisms.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: （左）线性和传统注意力的每个周期的训练和验证损失，（右）线性和传统注意力机制的训练损失曲线。
- en: Conclusion
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: This is obviously far from comprehensive, and we’re not exactly going to be
    competing with GPT here, but we can be pretty optimistic about reducing the complexity
    of the attention mechanism and not losing modelling ability.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这远远不够全面，我们这里并不打算与GPT竞争，但我们可以对减少注意力机制的复杂性并且不丧失建模能力保持乐观。
- en: Watch this space for a bigger comparison in Part 2.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 请关注第二部分，那里会有更详细的对比。
- en: All images, unless otherwise stated, have been created by the author, and the
    training data comes from the publicly available [PennTreebank](https://pytorch.org/text/0.8.1/datasets.html#id4)
    dataset accessed through *PyTorch torchtext* datasets. More details can be found
    [here](https://catalog.ldc.upenn.edu/docs/LDC95T7/cl93.html).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，否则所有图像均由作者创建，训练数据来自公开的[PennTreebank](https://pytorch.org/text/0.8.1/datasets.html#id4)数据集，通过*PyTorch
    torchtext*数据集获取。更多细节可以在[此处](https://catalog.ldc.upenn.edu/docs/LDC95T7/cl93.html)找到。
- en: For more details on the implementation of linear attention I strongly recommend
    you look in more depth at the original paper ([https://arxiv.org/abs/2006.16236](https://arxiv.org/abs/2006.16236))
    .
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于线性注意力的实现细节，我强烈建议你深入阅读原始论文（[https://arxiv.org/abs/2006.16236](https://arxiv.org/abs/2006.16236)）。
- en: '*If you enjoyed this content follow this account or find me on* [*Twitter*](https://x.com/smaddrellmander)*.*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你喜欢这篇内容，请关注此账号或在* [*Twitter*](https://x.com/smaddrellmander)*上找到我。*'
