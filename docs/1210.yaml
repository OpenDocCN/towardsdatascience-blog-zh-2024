- en: Interpretable kNN (ikNN)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可解释的kNN（ikNN）
- en: 原文：[https://towardsdatascience.com/interpretable-knn-iknn-33d38402b8fc?source=collection_archive---------3-----------------------#2024-05-14](https://towardsdatascience.com/interpretable-knn-iknn-33d38402b8fc?source=collection_archive---------3-----------------------#2024-05-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/interpretable-knn-iknn-33d38402b8fc?source=collection_archive---------3-----------------------#2024-05-14](https://towardsdatascience.com/interpretable-knn-iknn-33d38402b8fc?source=collection_archive---------3-----------------------#2024-05-14)
- en: An interpretable classifier
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个可解释的分类器
- en: '[](https://medium.com/@wkennedy934?source=post_page---byline--33d38402b8fc--------------------------------)[![W
    Brett Kennedy](../Images/b3ce55ffd028167326c117d47c64c467.png)](https://medium.com/@wkennedy934?source=post_page---byline--33d38402b8fc--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--33d38402b8fc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--33d38402b8fc--------------------------------)
    [W Brett Kennedy](https://medium.com/@wkennedy934?source=post_page---byline--33d38402b8fc--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@wkennedy934?source=post_page---byline--33d38402b8fc--------------------------------)[![W
    Brett Kennedy](../Images/b3ce55ffd028167326c117d47c64c467.png)](https://medium.com/@wkennedy934?source=post_page---byline--33d38402b8fc--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--33d38402b8fc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--33d38402b8fc--------------------------------)
    [W Brett Kennedy](https://medium.com/@wkennedy934?source=post_page---byline--33d38402b8fc--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--33d38402b8fc--------------------------------)
    ·10 min read·May 14, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--33d38402b8fc--------------------------------)
    ·10分钟阅读·2024年5月14日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Very often in when working on classification or regression problems in machine
    learning, we’re strictly interested in getting the most accurate model we can.
    In some cases, though, we’re interested also in the interpretability of the model.
    While models like XGBoost, CatBoost, and LGBM can be very strong models, it can
    be difficult to determine why they’ve made the predictions they have, or how they
    will behave with unseen data. These are what are called black-box models, models
    where we do not understand specifically why the make the predictions they do.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中的分类或回归问题中，我们通常最关心的是获得最准确的模型。然而，在某些情况下，我们还对模型的可解释性感兴趣。虽然像XGBoost、CatBoost和LGBM这样的模型可能非常强大，但有时我们很难确定它们为什么做出某些预测，或者它们在面对未见过的数据时会如何表现。这些被称为黑箱模型，即我们并不完全理解它们为什么做出这些预测。
- en: In many contexts this is fine; so long as we know they are reasonably accurate
    most of the time, they can be very useful, and it is understood they will be incorrect
    on occasion. For example, on a website, we may have a model that predicts which
    ads will be most likely to generate sales if shown to the current user. If the
    model behaves poorly on the rare occasion, this may affect revenues, but there
    are no major issues; we just have a model that’s sub-optimal, but generally useful.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，这是可以接受的；只要我们知道它们在大多数情况下是相当准确的，它们就非常有用，而且大家理解它们偶尔会出现不准确的情况。例如，在一个网站上，我们可能有一个模型，用来预测哪些广告在展示给当前用户时最有可能产生销售。如果模型在少数情况下表现不佳，这可能会影响收入，但并不会造成重大问题；我们只不过有一个次优的模型，但通常是有用的。
- en: 'But, in other contexts, it can be very important to know why the models make
    the predictions that they do. This includes high-stakes environments, such as
    in medicine and security. It also includes environments where we need to ensure
    there are no biases in the models related to race, gender or other protected classes.
    It’s important, as well, in environments that are audited: where it’s necessary
    to understand the models to determine they are performing as they should.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，在其他一些情况下，了解模型做出预测的原因可能非常重要。这包括高风险环境，如医疗和安全领域。它还包括我们需要确保模型在种族、性别或其他受保护群体方面没有偏见的环境。它在审计环境中也很重要：在这些环境中，需要理解模型，以确保它们的表现符合预期。
- en: Even in these cases, it is sometimes possible to use black-box models (such
    as boosted models, neural networks, Random Forests and so on) and then perform
    what is called post-hoc analysis. This provides an explanation, after the fact,
    of why the model likely predicted as it did. This is the field of Explainable
    AI (XAI), which uses techniques such as proxy models, feature importances (e.g.
    SHAP), counterfactuals, or ALE plots. These are very useful tools, but, everything
    else equal, it is preferable to have a model that is interpretable in the first
    place, at least where possible. XAI methods are very useful, but they do have
    limitations.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在这些情况下，有时也可以使用黑箱模型（例如，提升模型、神经网络、随机森林等），然后进行所谓的事后分析。这种方法在事后提供了一个解释，说明为什么模型做出了这样的预测。这就是可解释人工智能（XAI）领域，它使用代理模型、特征重要性（例如SHAP）、反事实推理或ALE图等技术。这些都是非常有用的工具，但在其他条件相同的情况下，最好还是有一个一开始就可解释的模型，至少在可能的情况下是这样。XAI方法非常有用，但它们确实有局限性。
- en: With proxy models, we train a model that is interpretable (for example, a shallow
    decision tree) to learn the behavior of the black-box model. This can provide
    some level of explanation, but will not always be accurate and will provide only
    approximate explanations.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 使用代理模型时，我们训练一个可解释的模型（例如，一个浅层决策树），让它学习黑箱模型的行为。这可以提供一定程度的解释，但并不总是准确的，只能提供近似的解释。
- en: Feature importances are also quite useful, but indicate only what the relevant
    features are, not how they relate to the prediction, or how they interact with
    each other to form the prediction. They also have no ability to determine if the
    model will work reasonably with unseen data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 特征重要性也非常有用，但它们仅能指示哪些特征是相关的，并不能说明这些特征如何与预测相关，或者它们如何相互作用来形成预测。它们也无法判断模型在未见数据上的表现是否合理。
- en: 'With interpretable models, we do not have these issues. The model is itself
    comprehensible and we can know exactly why it makes each prediction. The problem,
    though, is: interpretable models can have lower accuracy then black-box models.
    Most interpretable models, for most problems, will not be competitive with boosted
    models or neural networks. For any given problem, it may be necessary to try several
    interpretable models before an interpretable model of sufficient accuracy can
    be found, if any can be.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于可解释模型，我们没有这些问题。模型本身是可以理解的，我们可以确切地知道它为何做出每个预测。然而，问题是：可解释模型的准确性可能低于黑箱模型。对于大多数问题来说，大多数可解释模型的表现无法与提升模型或神经网络竞争。对于任何给定的问题，可能需要尝试几个可解释模型，才能找到一个准确性足够的可解释模型，如果有的话。
- en: 'There are a number of interpretable models available today, but unfortunately,
    very few. Among these are decision trees, rules lists (and rule sets), GAMs (Generalized
    Additive Models, such as Explainable Boosted Machines), and linear/logistic regression.
    These can each be useful where they work well, but the options are limited. The
    implication is: it can be impossible for many projects to find an interpretable
    model that performs satisfactorily. There can be real benefits in having more
    options available.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 今天有许多可解释模型，但不幸的是，选择非常有限。其中包括决策树、规则列表（和规则集）、GAMs（广义加性模型，如可解释提升机器）以及线性/逻辑回归。这些模型在适用的情况下是有用的，但选择有限。这意味着：对于许多项目来说，找到一个表现令人满意的可解释模型可能是不可能的。如果有更多的选择，可能会带来真正的好处。
- en: We introduce here another interpretable model, called ikNN, or interpretable
    k Nearest Neighbors. This is based on an ensemble of 2d kNN models. While the
    idea is straightforward, it is also surprisingly effective. And quite interpretable.
    While it is not competitive in terms of accuracy with state of the art models
    for prediction on tabular data such as CatBoost, it can often provide accuracy
    that is close and that is sufficient for the problem. It is also quite competitive
    with decision trees and other existing interpretable models.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里介绍另一个可解释的模型，称为ikNN，或可解释的k最近邻。这个模型基于一组2D kNN模型。虽然这个想法非常直接，但它也出奇的有效，并且相当可解释。尽管它在准确性方面无法与最先进的表格数据预测模型（如CatBoost）竞争，但它通常能提供接近的准确性，这对于某些问题来说已经足够。它在决策树和其他现有可解释模型中也具有较强的竞争力。
- en: Interestingly, it also appears to have stronger accuracy than plain kNN models.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，它的准确性似乎比普通的kNN模型更强。
- en: 'The main page for the project is: [https://github.com/Brett-Kennedy/ikNN](https://github.com/Brett-Kennedy/ikNN)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 项目的主页是：[https://github.com/Brett-Kennedy/ikNN](https://github.com/Brett-Kennedy/ikNN)
- en: The project defines a single class called iKNNClassifier. This can be included
    in any project copying the interpretable_knn.py file and importing it. It provides
    an interface consistent with scikit-learn classifiers. That is, we generally simply
    need to create an instance, call fit(), and call predict(), similar to using Random
    Forest or other scikit-learn models.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目定义了一个名为 iKNNClassifier 的类。任何项目只需复制 interpretable_knn.py 文件并导入即可包含此类。它提供了与
    scikit-learn 分类器一致的接口。也就是说，我们通常只需创建一个实例，调用 fit()，然后调用 predict()，类似于使用随机森林或其他 scikit-learn
    模型。
- en: 'Using, under the hood, an ensemble of 2d kNN’s provides a number of advantages.
    One is the normal advantage we always see with ensembling: we get more reliable
    predictions than when relying on a single model.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在后台使用二维 kNN 集成方法提供了许多优势。其中一个是我们在集成方法中通常看到的优势：相比依赖单一模型，我们能够获得更可靠的预测。
- en: Another is that 2d spaces are straightforward to visualize. The model currently
    requires numeric input (as is the case with kNN), so all categorical features
    need to be encoded, but once this is done, every 2d space can be visualized as
    a scatter plot. This provides a high degree of interpretability.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个优势是二维空间易于可视化。该模型目前需要数值型输入（就像 kNN 模型一样），因此所有类别特征需要进行编码，但完成编码后，每个二维空间都可以像散点图一样进行可视化。这提供了较高的可解释性。
- en: And, it’s possible to determine the most relevant 2d spaces for each prediction,
    which allows us to present a small number of plots for each record. This allows
    fairly simple as well as complete visual explanations for each record.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，可以确定每个预测最相关的二维空间，这使我们能够为每个记录展示少量的图表。这为每个记录提供了相当简单且完整的可视化解释。
- en: ikNN is, then, an interesting model, as it is based on ensembling, but actually
    increases interpretability, while the opposite is more often the case.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，ikNN 是一个有趣的模型，因为它基于集成方法，但实际上提高了可解释性，而相反的情况更为常见。
- en: Standard kNN Classifiers
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标准 kNN 分类器
- en: 'kNN models are less-used than many others, as they are not usually as accurate
    as boosted models or neural networks, or as interpretable as decision trees. They
    are, though, still widely used. They work based on an intuitive idea: the class
    of an item can be predicted based on the class of most of the items that are most
    similar to it.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: kNN 模型比许多其他模型使用得较少，因为它们通常不如提升模型或神经网络准确，也不如决策树具有可解释性。然而，它们仍然被广泛使用。它们基于一个直观的思想：一个项目的类别可以基于与其最相似的项目的类别来预测。
- en: For example, if we look at the iris dataset (as is used in an example below),
    we have three classes, representing three types of iris. If we collect another
    sample of iris and wish to predict which of the three types of iris it is, we
    can look at the most similar, say, 10 records from the training data, determine
    what their classes are, and take the most common of these.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们查看鸢尾花数据集（如下例所示），我们有三类，代表三种鸢尾花。如果我们收集另一个鸢尾花样本并希望预测它属于哪一种鸢尾花，我们可以查看训练数据中最相似的10条记录，确定它们的类别，并选择这些记录中最常见的类别。
- en: In this example, we chose 10 to be the number of nearest neighbors to use to
    estimate the class of each record, but other values may be used. This is specified
    as a hyperparameter (the k parameter) with kNN and ikNN models. We wish set k
    so as to use to a reasonable number of similar records. If we use too few, the
    results may be unstable (each prediction is based on very few other records).
    If we use too many, the results may be based on some other records that aren’t
    that similar.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们选择10作为用于估计每条记录类别的最近邻数量，但也可以使用其他值。这作为超参数（k 参数）在 kNN 和 ikNN 模型中进行指定。我们希望设置
    k 以使用合理数量的相似记录。如果使用的记录太少，结果可能不稳定（每个预测基于非常少的记录）。如果使用的记录太多，结果可能会基于一些不太相似的记录。
- en: We also need a way to determine which are the most similar items. For this,
    at least by default, we use the Euclidean distance. If the dataset has 20 features
    and we use k=10, then we find the closest 10 points in the 20-dimensional space,
    based on their Euclidean distances.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一种方法来确定哪些是最相似的项目。为此，至少默认情况下，我们使用欧几里得距离。如果数据集有20个特征，并且我们使用 k=10，那么我们将根据它们的欧几里得距离在20维空间中找到最接近的10个点。
- en: Predicting for one record, we would find the 10 closest records from the training
    data and see what their classes are. If 8 of the 10 are class Setosa (one of the
    3 types of iris), then we can assume this row is most likely also Setosa, or at
    least this is the best guess we can make.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一条记录的预测，我们会找到训练数据中离它最近的10条记录，并查看它们的类别。如果10条记录中有8条是Setosa类别（鸢尾花的三种类型之一），那么我们可以假设这条记录很可能也是Setosa，或者至少这是我们能做出的最佳猜测。
- en: One issue with this is, it breaks down when there are many features, due to
    what’s called *the curse of dimensionality*. An interesting property of high-dimensional
    spaces is that with enough features, distances between the points start to become
    meaningless.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题在于，当特征很多时，算法会出现问题，这就是所谓的*维度灾难*。高维空间的一个有趣特性是，当特征足够多时，点与点之间的距离开始变得没有意义。
- en: kNN also uses all features equally, though some may be much more predictive
    of the target than others. The distances between points, being based on Euclidean
    (or sometimes Manhattan or other distance metrics) are calculated considering
    all features equally. This is simple, but not always the most effective, given
    many features may be irrelevant to the target. Assuming some feature selection
    has been performed, this is less likely, but the relevance of the features will
    still not be equal.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: kNN还会平等地使用所有特征，尽管有些特征对目标的预测能力可能远远超过其他特征。点之间的距离是基于欧几里得（有时是曼哈顿或其他距离度量）计算的，考虑所有特征是相等的。这种方法简单，但并不总是最有效的，因为许多特征可能与目标无关。如果已经进行了特征选择，这种情况会有所减少，但特征的相关性仍然不等同。
- en: And, the predictions made by kNN predictors are uninterpretable. The algorithm
    is quite intelligible, but the predictions can be difficult to understand. It’s
    possible to list the k nearest neighbors for any record, which provides some insight
    into the prediction made for that record, but it’s difficult to see why a given
    set of records are the most similar, particularly where there are many features.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，kNN预测器做出的预测是不可解释的。算法本身非常直观，但预测结果可能难以理解。虽然可以列出任何记录的k个最近邻，这为预测提供了一些见解，但很难理解为什么某一组记录是最相似的，尤其是在特征很多的情况下。
- en: The ikNN Algorithm
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ikNN算法
- en: The ikNN model first takes each pair of features and creates a standard 2d kNN
    classifier using these features. So, if a table has 10 features, this creates
    (10 choose 2), or 45 models, one for each unique pair of features.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ikNN模型首先会取每一对特征，并使用这些特征创建一个标准的二维kNN分类器。所以，如果一个表格有10个特征，这将创建(10选2)，即45个模型，每个模型对应一对独特的特征。
- en: It then assesses their accuracies with respect to predicting the target column
    using the training data. Given this, the ikNN model determines the predictive
    power of each 2d subspace. In the case of 45 2d models, some will be more predictive
    than others. To make a prediction, the 2d subspaces known to be most predictive
    are used, optionally weighted by their predictive power on the training data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，模型会根据使用训练数据来预测目标列的准确度进行评估。基于这一点，ikNN模型确定每个二维子空间的预测能力。在45个二维模型中，有些会比其他的更具预测力。为了进行预测，会使用已知最具预测力的二维子空间，且可以根据它们在训练数据上的预测能力进行加权。
- en: Further, at inference, the purity of the set of nearest neighbors around a given
    row within each 2d space may be considered, allowing the model to weight more
    heavily both the subspaces proven to be more predictive with training data and
    the subspaces that appear to be the most consistent in their prediction with respect
    to the current instance.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在推理时，可能会考虑每个二维空间中给定行的最近邻集的纯度，从而使模型能够更加重视那些在训练数据中证明最具预测性的子空间，以及那些在当前实例预测中表现出最高一致性的子空间。
- en: Consider two subspaces and a point shown here as a star. In both cases, we can
    find the set of k points closest to the point. Here we draw a green circle around
    the star, though the set of points do not actually form a circle (though there
    is a radius to the kth nearest neighbor that effectively defines a neighborhood).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑两个子空间和一个在这里显示为星形的点。在这两种情况下，我们都可以找到离该点最近的k个点。这里我们画了一个绿色圆圈围绕星形，尽管这些点集并不实际形成一个圆形（尽管第k个最近邻的半径实际上定义了一个邻域）。
- en: '![](../Images/8c88aef07e3d9300cff2dcf6367e5b1f.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c88aef07e3d9300cff2dcf6367e5b1f.png)'
- en: 'These plots each represent a pair of features. In the case of the left plot,
    there is very high consistency among the neighbors of the star: they are entirely
    red. In the right plot, there is no little consistency among the neigbhors: some
    are red and some are blue. The first pair of features appears to be more predictive
    of the record than the second pair of features, which ikNN takes advantage of.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图每个代表一对特征。在左侧的图中，星号邻居之间具有非常高的一致性：它们完全是红色的。在右侧的图中，邻居之间几乎没有一致性：有些是红色的，有些是蓝色的。第一对特征似乎比第二对特征更能预测记录，ikNN
    正是利用了这一点。
- en: This approach allows the model to consider the influence all input features,
    but weigh them in a manner that magnifies the influence of more predictive features,
    and diminishes the influence of less-predictive features.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法使得模型能够考虑所有输入特征的影响，但以一种方式加权，使得更具预测性的特征的影响得到放大，而较少预测性的特征的影响则被削弱。
- en: Example
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例
- en: We first demonstrate ikNN with a toy dataset, specifically the iris dataset.
    We load in the data, do a train-test split, and make predictions on the test set.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用一个玩具数据集来演示 ikNN，具体来说是鸢尾花数据集。我们加载数据，进行训练-测试集拆分，并在测试集上进行预测。
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: For prediction, this is all that is required. But, ikNN also provides tools
    for understanding the model, specifically the graph_model() and graph_predictions()
    APIs.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于预测，这是所需的一切。但 ikNN 还提供了理解模型的工具，具体来说是 `graph_model()` 和 `graph_predictions()`
    API。
- en: 'For an example of graph_model():'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 `graph_model()` 的示例：
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/12d6dfed1bd65f1421b38f373615b451.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12d6dfed1bd65f1421b38f373615b451.png)'
- en: This provides a quick overview of the dataspace, plotting, by default, five
    2d spaces. The dots show the classes of the training data. The background color
    shows the predictions made by the 2d kNN for each region of the 2d space.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供了数据空间的快速概览，默认情况下绘制了五个二维空间。点显示了训练数据的类别。背景颜色显示了 2D kNN 对每个二维空间区域所做的预测。
- en: 'The graph_predictions() API will explain a specific row, for example:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`graph_predictions()` API 将解释特定的行，例如：'
- en: '![](../Images/09682e05b2511bea57fee9ebb839f0ef.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09682e05b2511bea57fee9ebb839f0ef.png)'
- en: Here, the row being explained (Row 0) is shown as a red star. Again, by default,
    five plots are used by default, but for simplicity, this uses just two. In both
    plots, we can see where Row 0 is located relative to the training data and the
    predictions made by the 2D kNN for this 2D space.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，被解释的行（第 0 行）以红色星号显示。再次强调，默认情况下使用五个图，但为了简化，这里仅使用了两个。在这两个图中，我们可以看到第 0 行相对于训练数据的位置，以及
    2D kNN 对该 2D 空间所做的预测。
- en: Visualizations
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化
- en: Although it is configurable, by default only five 2d spaces are used by each
    ikNN prediction. This ensures the prediction times are fast and the visualizations
    simple. It also means that the visualizations are showing the true predictions,
    not a simplification of the predictions, ensuring the predictions are completely
    interpretable
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然是可配置的，但默认情况下，每个 ikNN 预测仅使用五个二维空间。这确保了预测速度较快且可视化简单。这也意味着这些可视化展示了真实的预测，而不是对预测的简化，确保了预测的完全可解释性。
- en: For most datasets, for most rows, all or almost all 2d spaces tend to agree
    on the prediction. However, where the predictions are incorrect, it may be useful
    to examine more 2d plots in order to better tune the hyperparameters to suit the
    current dataset.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数数据集的大多数行，所有或几乎所有的二维空间在预测上趋于一致。然而，在预测不准确的地方，可能需要检查更多的二维图，以便更好地调节超参数，使其更适应当前的数据集。
- en: Accuracy Tests
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准确性测试
- en: A set of tests were performed using a random set of 100 classification datasets
    from OpenML. Comparing the F1 (macro) scores of standard kNN and ikNN models,
    ikNN had higher scores for 58 datasets and kNN for 42.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 OpenML 上的 100 个分类数据集的随机集合进行了多组测试。在比较标准 kNN 和 ikNN 模型的 F1（宏观）得分时，ikNN 在 58
    个数据集上得分较高，而 kNN 在 42 个数据集上得分较高。
- en: ikNN’s do even a bit better when performing grid search to search for the best
    hyperparameters. After doing this for both models on all 100 datasets, ikNN performed
    the best in 76 of the 100 cases. It also tends to have smaller gaps between the
    train and test scores, suggesting more stable models than standard kNN models.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ikNN 在执行网格搜索以寻找最佳超参数时，表现得更好。在对所有 100 个数据集的两个模型进行网格搜索后，ikNN 在 100 个案例中有 76 个表现最好。它还倾向于在训练和测试分数之间有较小的差距，表明它比标准
    kNN 模型更加稳定。
- en: ikNN models can be somewhat slower, but they tend to still be considerably faster
    than boosted models, and still very fast, typically taking well under a minute
    for training, usually only seconds.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ikNN 模型可能稍微慢一些，但它们通常比提升型模型快得多，而且依然非常快速，通常只需不到一分钟的训练时间，通常仅需几秒钟。
- en: The github page provides some more examples and analysis of the accuracy.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub 页面提供了一些更多的示例和准确性的分析。
- en: Conclusions
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: While ikNN is likely not the strongest model where accuracy is the primary goal
    (though, as with any model, it can be on occasion), it is likely a model that
    should be tried where an interpretable model is necessary.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 ikNN 可能不是在准确性为主要目标时最强的模型（尽管像任何模型一样，它偶尔也能做到），但在需要可解释性模型的场合，它很可能是一个应该尝试的模型。
- en: This page provided the basic information necessary to use the tool. It simply
    necessary to download the .py file ([https://github.com/Brett-Kennedy/ikNN/blob/main/ikNN/interpretable_knn.py](https://github.com/Brett-Kennedy/ikNN/blob/main/ikNN/interpretable_knn.py)),
    import it into your code, create an instance, train and predict, and (where desired),
    call graph_predictions() to view the explanations for any records you wish.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 本页面提供了使用该工具所需的基本信息。只需下载 .py 文件（[https://github.com/Brett-Kennedy/ikNN/blob/main/ikNN/interpretable_knn.py](https://github.com/Brett-Kennedy/ikNN/blob/main/ikNN/interpretable_knn.py)），将其导入到你的代码中，创建实例，进行训练和预测，并在需要时调用
    graph_predictions() 来查看你希望查看的任何记录的解释。
- en: All images are by author.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 所有图片均由作者提供。
