<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Segmenting Water in Satellite Images Using PaliGemma</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Segmenting Water in Satellite Images Using PaliGemma</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/segmenting-water-in-satellite-images-using-paligemma-b172dc0cf55d?source=collection_archive---------2-----------------------#2024-12-29">https://towardsdatascience.com/segmenting-water-in-satellite-images-using-paligemma-b172dc0cf55d?source=collection_archive---------2-----------------------#2024-12-29</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="c27d" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Some insights on using Google’s latest Vision Language Model</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://anamabo3.medium.com/?source=post_page---byline--b172dc0cf55d--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Dr. Carmen Adriana Martínez Barbosa" class="l ep by dd de cx" src="../Images/caad66f044af1131e17dc28ea2f48863.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*NFhDEeUmBdi6gegYQbCMZQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--b172dc0cf55d--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://anamabo3.medium.com/?source=post_page---byline--b172dc0cf55d--------------------------------" rel="noopener follow">Dr. Carmen Adriana Martínez Barbosa</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--b172dc0cf55d--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Dec 29, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj mk"><img src="../Images/e9d032d36ebb9f252e75c1a95d5b64cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*aG0wm_9RuieOSxFYwxqN2g.png"/></div><figcaption class="ms mt mu mi mj mv mw bf b bg z dx">Hutt Lagoon, Australia. Depending on the season, time of day, and cloud coverage, this lake changes from red to pink or purple. Source: Google Maps.</figcaption></figure></div></div></div><div class="ab cb mx my mz na" role="separator"><span class="nb by bm nc nd ne"/><span class="nb by bm nc nd ne"/><span class="nb by bm nc nd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="c9c3" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Multimodal models are architectures that simultaneously integrate and process different data types, such as text, images, and audio. Some examples include CLIP and DALL-E from OpenAI, both released in 2021. CLIP understands images and text jointly, allowing it to perform tasks like zero-shot image classification. DALL-E, on the other hand, generates images from textual descriptions, allowing the automation and enhancement of creative processes in gaming, advertising, and literature, among other sectors.</p><p id="619e" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Visual language models (VLMs) are a special case of multimodal models. VLMs generate language based on visual inputs. One prominent example is Paligemma, which Google introduced in May 2024. Paligemma can be used for Visual Question Answering, object detection, and image segmentation.</p><p id="925f" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Some blog posts explore the capabilities of Paligemma in object detection, such as this excellent read from Roboflow:</p><div class="ob oc od oe of og"><a href="https://blog.roboflow.com/how-to-fine-tune-paligemma/?source=post_page-----b172dc0cf55d--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="oh ab ig"><div class="oi ab co cb oj ok"><h2 class="bf fr hw z io ol iq ir om it iv fp bk">Fine-tune PaliGemma for Object Detection with Custom Data</h2><div class="on l"><h3 class="bf b hw z io ol iq ir om it iv dx">Learn how to fine-tune the PaliGemma multimodal model to detect custom objects.</h3></div><div class="oo l"><p class="bf b dy z io ol iq ir om it iv dx">blog.roboflow.com</p></div></div><div class="op l"><div class="oq l or os ot op ou lq og"/></div></div></a></div><p id="7d9f" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">However, by the time I wrote this blog, the existing documentation on preparing data to use Paligemma for object segmentation was vague. That is why I wanted to evaluate whether it is easy to use Paligemma for this task. Here, I share my experience.</p><h1 id="a469" class="ov ow fq bf ox oy oz gq pa pb pc gt pd pe pf pg ph pi pj pk pl pm pn po pp pq bk">Brief introduction of Paligemma</h1><p id="40d2" class="pw-post-body-paragraph nf ng fq nh b go pr nj nk gr ps nm nn no pt nq nr ns pu nu nv nw pv ny nz oa fj bk">Before going into detail on the use case, let’s briefly revisit the inner workings of Paligemma.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pw"><img src="../Images/e3f8adc404fa7c8dd20c873399e6032a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*cO0kQhmvh0iYdWga5jdmaA.png"/></div><figcaption class="ms mt mu mi mj mv mw bf b bg z dx">Architecture of Paligemma2. Source: <a class="af px" href="https://arxiv.org/abs/2412.03555" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2412.03555</a></figcaption></figure><p id="fe1b" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Paligemma combines a <a class="af px" href="https://arxiv.org/abs/2303.15343" rel="noopener ugc nofollow" target="_blank">SigLIP-So400m vision encoder</a> with a <a class="af px" href="https://developers.googleblog.com/en/gemma-explained-overview-gemma-model-family-architectures/" rel="noopener ugc nofollow" target="_blank">Gemma language model</a> to process images and text (see figure above). In the new version of Paligemma released in December of this year, the vision encoder can preprocess images at three different resolutions: 224px, 448px, or 896px. The vision encoder preprocesses an image and outputs a sequence of image tokens, which are linearly combined with input text tokens. This combination of tokens is further processed by the Gemma language model, which outputs text tokens. The Gemma model has different sizes, from 2B to 27B parameters.</p><p id="0fc0" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">An example of model output is shown in the following figure.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj py"><img src="../Images/bb5b62fcee9bc621f472c12acb4655ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*eZpxhyVuoaS6A7AB-OiSjQ.png"/></div><figcaption class="ms mt mu mi mj mv mw bf b bg z dx">Example of an object segmentation output. Source: <a class="af px" href="https://arxiv.org/abs/2412.03555" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2412.03555</a></figcaption></figure><p id="8958" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">The Paligemma model was trained on various datasets such as <a class="af px" href="https://paperswithcode.com/dataset/webli" rel="noopener ugc nofollow" target="_blank">WebLi</a>, <a class="af px" href="https://storage.googleapis.com/openimages/web/index.html" rel="noopener ugc nofollow" target="_blank">openImages</a>, <a class="af px" href="https://github.com/google-research-datasets/wit" rel="noopener ugc nofollow" target="_blank">WIT</a>, and others (see this <a class="af px" href="https://www.kaggle.com/models/google/paligemma" rel="noopener ugc nofollow" target="_blank">Kaggle blog</a> for more details). This means that Paligemma can identify objects without fine-tuning. However, such abilities are limited. That’s why Google recommends fine-tuning Paligemma in domain-specific use cases.</p><h2 id="a6c7" class="pz ow fq bf ox qa qb qc pa qd qe qf pd no qg qh qi ns qj qk ql nw qm qn qo qp bk">Input format</h2><p id="8a40" class="pw-post-body-paragraph nf ng fq nh b go pr nj nk gr ps nm nn no pt nq nr ns pu nu nv nw pv ny nz oa fj bk">To fine-tune Paligemma, the input data needs to be in JSONL format. A dataset in JSONL format has each line as a separate JSON object, like a list of individual records. Each JSON object contains the following keys:</p><p id="5028" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk"><strong class="nh fr">Image:</strong> The image’s name.</p><p id="0553" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk"><strong class="nh fr">Prefix: </strong>This specifies the task you want the model to perform.</p><p id="965d" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk"><strong class="nh fr">Suffix:</strong> This provides the ground truth the model learns to make predictions.</p><p id="bf04" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Depending on the task, you must change the JSON object's prefix and suffix accordingly. Here are some examples:</p><ul class=""><li id="33ae" class="nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa qq qr qs bk"><strong class="nh fr">Image captioning:</strong></li></ul><pre class="ml mm mn mo mp qt qu qv bp qw bb bk"><span id="9ce0" class="qx ow fq qu b bg qy qz l ra rb">{"image": "some_filename.png", <br/> "prefix": "caption en" (To indicate that the model should generate an English caption for an image),<br/> "suffix": "This is an image of a big, white boat traveling in the ocean."<br/>}</span></pre><ul class=""><li id="f5e6" class="nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa qq qr qs bk"><strong class="nh fr">Question answering:</strong></li></ul><pre class="ml mm mn mo mp qt qu qv bp qw bb bk"><span id="a13a" class="qx ow fq qu b bg qy qz l ra rb">{"image": "another_filename.jpg", <br/> "prefix": "How many people are in the image?",<br/> "suffix": "ten"<br/>}</span></pre><ul class=""><li id="ffe7" class="nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa qq qr qs bk"><strong class="nh fr">Object detection:</strong></li></ul><pre class="ml mm mn mo mp qt qu qv bp qw bb bk"><span id="84cb" class="qx ow fq qu b bg qy qz l ra rb">{"image": "filename.jpeg", <br/> "prefix": "detect airplane",<br/> "suffix": "&lt;loc0055&gt;&lt;loc0115&gt;&lt;loc1023&gt;&lt;loc1023&gt; airplane" (four corner bounding box coords)<br/>}</span></pre><p id="c69f" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">If you have several categories to be detected, add a semicolon (;) among each category in the prefix and suffix.</p><p id="ce0b" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">A complete and clear explanation of how to prepare the data for object detection in Paligemma can be found in <a class="af px" href="https://blog.roboflow.com/how-to-fine-tune-paligemma/" rel="noopener ugc nofollow" target="_blank">this Roboflow post</a>.</p><ul class=""><li id="52e4" class="nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa qq qr qs bk"><strong class="nh fr">Image segmentation:</strong></li></ul><pre class="ml mm mn mo mp qt qu qv bp qw bb bk"><span id="cd85" class="qx ow fq qu b bg qy qz l ra rb">{"image": "filename.jpeg", <br/> "prefix": "detect airplane",<br/> "suffix": "&lt;loc0055&gt;&lt;loc0115&gt;&lt;loc1023&gt;&lt;loc1023&gt;&lt;seg063&gt;&lt;seg108&gt;&lt;seg045&gt;&lt;seg028&gt;&lt;seg056&gt;&lt;seg052&gt;&lt;seg114&gt;&lt;seg005&gt;&lt;seg042&gt;&lt;seg023&gt;&lt;seg084&gt;&lt;seg064&gt;&lt;seg086&gt;&lt;seg077&gt;&lt;seg090&gt;&lt;seg054&gt; airplane" <br/>}</span></pre><p id="1513" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Note that for segmentation, apart from the object’s bounding box coordinates, you need to specify 16 extra segmentation tokens representing a mask that fits within the bounding box. According to <a class="af px" href="https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/paligemma/README.md#tokenizer" rel="noopener ugc nofollow" target="_blank">Google’s Big Vision repository</a>, those tokens are codewords with 128 entries (&lt;seg000&gt;…&lt;seg127&gt;). How do we obtain these values? In my personal experience, it was challenging and frustrating to get them without proper documentation. But I’ll give more details later.</p><p id="bb63" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">If you are interested in learning more about Paligemma, I recommend these blogs:</p><div class="ob oc od oe of og"><a href="https://huggingface.co/blog/paligemma2?source=post_page-----b172dc0cf55d--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="oh ab ig"><div class="oi ab co cb oj ok"><h2 class="bf fr hw z io ol iq ir om it iv fp bk">Welcome PaliGemma 2 — New vision language models by Google</h2><div class="on l"><h3 class="bf b hw z io ol iq ir om it iv dx">We’re on a journey to advance and democratize artificial intelligence through open source and open science.</h3></div><div class="oo l"><p class="bf b dy z io ol iq ir om it iv dx">huggingface.co</p></div></div><div class="op l"><div class="rc l or os ot op ou lq og"/></div></div></a></div><div class="ob oc od oe of og"><a href="https://www.datature.io/blog/introducing-paligemma-googles-latest-visual-language-model?source=post_page-----b172dc0cf55d--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="oh ab ig"><div class="oi ab co cb oj ok"><h2 class="bf fr hw z io ol iq ir om it iv fp bk">Introducing PaliGemma: Google's Latest Visual Language Model</h2><div class="on l"><h3 class="bf b hw z io ol iq ir om it iv dx">PaliGemma pushes the boundaries for efficient multi-modality in Visual Language Models through task-specific finetuning…</h3></div><div class="oo l"><p class="bf b dy z io ol iq ir om it iv dx">www.datature.io</p></div></div><div class="op l"><div class="rd l or os ot op ou lq og"/></div></div></a></div><h1 id="b059" class="ov ow fq bf ox oy oz gq pa pb pc gt pd pe pf pg ph pi pj pk pl pm pn po pp pq bk">Satellite images of water bodies</h1><p id="3150" class="pw-post-body-paragraph nf ng fq nh b go pr nj nk gr ps nm nn no pt nq nr ns pu nu nv nw pv ny nz oa fj bk">As mentioned above, Paligemma was trained on different datasets. Therefore, this model is expected to be good at segmenting “traditional” objects such as cars, people, or animals. But what about segmenting objects in satellite images? This question led me to explore Paligemma’s capabilities for segmenting water in satellite images.</p><p id="b52c" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Kaggle’s <a class="af px" href="https://www.kaggle.com/datasets/franciscoescobar/satellite-images-of-water-bodies" rel="noopener ugc nofollow" target="_blank">Satellite Image of Water Bodies dataset</a> is suitable for this purpose. This dataset contains 2841 images with their corresponding masks.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="rf rg ed rh bh ri"><div class="mi mj re"><img src="../Images/ad8e9458fd86cd6d05b75a1c60a1563f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*erw_D-Y0KxKSfvy5pafoJg.png"/></div></div><figcaption class="ms mt mu mi mj mv mw bf b bg z dx">Here's an example of the water bodies dataset: The RGB image is shown on the left, while the corresponding mask appears on the right.</figcaption></figure><p id="bd0c" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Some masks in this dataset were incorrect, and others needed further preprocessing. Faulty examples include masks with all values set to water, while only a small portion was present in the original image. Other masks did not correspond to their RGB images. When an image is rotated, some masks make these areas appear as if they have water.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj rj"><img src="../Images/b74083bafddf71b84cdbbc2f3cfd00f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*UQQCzEH0soY6T7N_989EQQ.png"/></div><figcaption class="ms mt mu mi mj mv mw bf b bg z dx">Example of a rotated mask. When reading this image in Python, the area outside the image appears as it would have water. In this case, image rotation is needed to correct this mask. Image made by the author.</figcaption></figure><p id="dd97" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Given these data limitations, I selected a sample of 164 images for which the masks did not have any of the problems mentioned above. This set of images is used to fine-tune Paligemma.</p><h2 id="1ddb" class="pz ow fq bf ox qa qb qc pa qd qe qf pd no qg qh qi ns qj qk ql nw qm qn qo qp bk">Preparing the JSONL dataset</h2><p id="4926" class="pw-post-body-paragraph nf ng fq nh b go pr nj nk gr ps nm nn no pt nq nr ns pu nu nv nw pv ny nz oa fj bk">As explained in the previous section, Paligemma needs entries that represent the object’s bounding box coordinates in normalized image-space (&lt;loc0000&gt;…&lt;loc1023&gt;) plus an extra 16 segmentation tokens representing 128 different codewords (&lt;seg000&gt;…&lt;seg127&gt;). Obtaining the bounding box coordinates in the desired format was easy, thanks to <a class="af px" href="https://blog.roboflow.com/how-to-fine-tune-paligemma/" rel="noopener ugc nofollow" target="_blank">Roboflow’s explanation</a>. But how do we obtain the 128 codewords from the masks? There was no clear documentation or examples in the Big Vision repository that I could use for my use case. I naively thought that the process of creating the segmentation tokens was similar to that of making the bounding boxes. However, this led to an incorrect representation of the water masks, which led to wrong prediction results.</p><p id="d8ff" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">By the time I wrote this blog (beginning of December), Google announced the second version of Paligemma. Following this event, Roboflow published <a class="af px" href="https://blog.roboflow.com/fine-tune-paligemma-2/" rel="noopener ugc nofollow" target="_blank">a nice overview</a> of preparing data to fine-tune Paligemma2 for different applications, including image segmentation. I use part of their code to finally obtain the correct segmentation codewords. What was my mistake? Well, first of all, the masks need to be resized to a tensor of shape [None, 64, 64, 1] and then use a pre-trained variational auto-encoder (VAE) to convert annotation masks into text labels. Although the usage of a VAE model was briefly mentioned in the Big Vision repository, there is no explanation or examples on how to use it.</p><p id="a7c4" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">The workflow I use to prepare the data to fine-tune Paligemma is shown below:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="rf rg ed rh bh ri"><div class="mi mj rk"><img src="../Images/31298a3389159effa1b9eff062b5b8cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iCXuZzahO_9iGwTNQmmAfQ.png"/></div></div><figcaption class="ms mt mu mi mj mv mw bf b bg z dx">Steps to convert one original mask from the filtered <a class="af px" href="https://www.kaggle.com/datasets/franciscoescobar/satellite-images-of-water-bodies" rel="noopener ugc nofollow" target="_blank">water bodies dataset</a> to a JSON object. This process is repeated over the 164 images of the train set and the 21 images of the test dataset to build the JSONL dataset.</figcaption></figure><p id="abca" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">As observed, the number of steps needed to prepare the data for Paligemma is large, so I don’t share code snippets here. However, if you want to explore the code, you can visit <a class="af px" href="https://github.com/anamabo/SegmentWaterWithPaligemma" rel="noopener ugc nofollow" target="_blank">this GitHub repository</a>. The script <em class="rl">convert.py</em> has all the steps mentioned in the workflow shown above. I also added the selected images so you can play with this script immediately.</p><p id="8d06" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">When preprocessing the segmentation codewords back to segmentation masks, we note how these masks cover the water bodies in the images:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="rf rg ed rh bh ri"><div class="mi mj rm"><img src="../Images/21e69594fd0053603c25be1b6e07aaba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AGnPsJFen8ykU6yoaEVqFA.png"/></div></div><figcaption class="ms mt mu mi mj mv mw bf b bg z dx">Resulting masks when decoding the segmentation codewords in the train set. Image made by the author using <a class="af px" href="https://github.com/anamabo/SegmentWaterWithPaligemma/blob/main/finetune_paligemma_for_segmentation.ipynb" rel="noopener ugc nofollow" target="_blank">this Notebook</a>.</figcaption></figure><h1 id="53e9" class="ov ow fq bf ox oy oz gq pa pb pc gt pd pe pf pg ph pi pj pk pl pm pn po pp pq bk">How is Paligemma at segmenting water in satellite images?</h1><p id="4eda" class="pw-post-body-paragraph nf ng fq nh b go pr nj nk gr ps nm nn no pt nq nr ns pu nu nv nw pv ny nz oa fj bk">Before fine-tuning Paligemma, I tried its segmentation capabilities on the models uploaded to Hugging Face. This platform ha<a class="af px" href="https://huggingface.co/spaces/big-vision/paligemma" rel="noopener ugc nofollow" target="_blank">s a demo</a> where you can upload images and interact with different Paligemma models.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="rf rg ed rh bh ri"><div class="mi mj rn"><img src="../Images/13a159f2e23495fa1218fc60692ec45e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UOf1poS5PE64cBpz9IzdmA.gif"/></div></div><figcaption class="ms mt mu mi mj mv mw bf b bg z dx">Default Paligemma model at segmenting water in satellite images.</figcaption></figure><p id="c784" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">The current version of Paligemma is generally good at segmenting water in satellite images, but it’s not perfect. Let’s see if we can improve these results!</p><p id="b6bd" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">There are two ways to fine-tune Paligemma, either through <a class="af px" href="https://huggingface.co/blog/paligemma#using-transformers-1" rel="noopener ugc nofollow" target="_blank">Hugging Face’s Transformer library</a> or by using Big Vision and JAX. I went for this last option. Big Vision provides a <a class="af px" href="https://colab.research.google.com/github/google-research/big_vision/blob/main/big_vision/configs/proj/paligemma/finetune_paligemma.ipynb" rel="noopener ugc nofollow" target="_blank">Colab notebook</a>, which I modified for my use case. You can open it by going to my <a class="af px" href="https://github.com/anamabo/SegmentWaterWithPaligemma?tab=readme-ov-file" rel="noopener ugc nofollow" target="_blank">GitHub repository</a>:</p><div class="ob oc od oe of og"><a href="https://github.com/anamabo/SegmentWaterWithPaligemma/blob/main/finetune_paligemma_for_segmentation.ipynb?source=post_page-----b172dc0cf55d--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="oh ab ig"><div class="oi ab co cb oj ok"><h2 class="bf fr hw z io ol iq ir om it iv fp bk">SegmentWaterWithPaligemma/finetune_paligemma_for_segmentation.ipynb at main ·…</h2><div class="on l"><h3 class="bf b hw z io ol iq ir om it iv dx">Segmentation of water in Satellite images using Paligemma …</h3></div><div class="oo l"><p class="bf b dy z io ol iq ir om it iv dx">github.com</p></div></div><div class="op l"><div class="ro l or os ot op ou lq og"/></div></div></a></div><p id="8284" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">I used a <em class="rl">batch size</em> of 8 and a <em class="rl">learning rate</em> of 0.003. I ran the training loop twice, which translates to 158 training steps. The total running time using a T4 GPU machine was 24 minutes.</p><p id="f54a" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">The results were not as expected. Paligemma did not produce predictions in some images, and in others, the resulting masks were far from the ground truth. I also obtained segmentation codewords with more than 16 tokens in two images.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj rp"><img src="../Images/f9ba415d9a14a051d7c5168dee08e1d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*1jdVzbu4xmFoSa89umsjrw.png"/></div><figcaption class="ms mt mu mi mj mv mw bf b bg z dx">Results of the fine-tuning where there were predictions. Image made by the author.</figcaption></figure><p id="ae70" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">It’s worth mentioning that I use the first Paligemma version. Perhaps the results are improved when using Paligemma2 or by tweaking the batch size or learning rate further. In any case, these experiments are out of the scope of this blog.</p><p id="c3f3" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">The demo results show that the default Paligemma model is better at segmenting water than my finetuned model. In my opinion, UNET is a better architecture if the aim is to build a model specialized in segmenting objects. For more information on how to train such a model, you can read my previous blog post:</p><div class="ob oc od oe of og"><a rel="noopener follow" target="_blank" href="/detecting-clouds-with-ai-b553e6576af6?source=post_page-----b172dc0cf55d--------------------------------"><div class="oh ab ig"><div class="oi ab co cb oj ok"><h2 class="bf fr hw z io ol iq ir om it iv fp bk">Detecting Clouds with AI</h2><div class="on l"><h3 class="bf b hw z io ol iq ir om it iv dx">From Random Forest to YOLO: Comparing different algorithms for cloud segmentation in satellite Images.</h3></div><div class="oo l"><p class="bf b dy z io ol iq ir om it iv dx">towardsdatascience.com</p></div></div><div class="op l"><div class="rq l or os ot op ou lq og"/></div></div></a></div><h2 id="a58f" class="pz ow fq bf ox qa qb qc pa qd qe qf pd no qg qh qi ns qj qk ql nw qm qn qo qp bk">Other limitations:</h2><p id="b7b5" class="pw-post-body-paragraph nf ng fq nh b go pr nj nk gr ps nm nn no pt nq nr ns pu nu nv nw pv ny nz oa fj bk">I want to mention some other challenges I encountered when fine-tuning Paligemma using Big Vision and JAX.</p><ul class=""><li id="5541" class="nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa qq qr qs bk">Setting up different model configurations is difficult because there’s still little documentation on those parameters.</li><li id="f413" class="nf ng fq nh b go rr nj nk gr rs nm nn no rt nq nr ns ru nu nv nw rv ny nz oa qq qr qs bk">The first version of Paligemma has been trained to handle images of different aspect ratios resized to 224x224. Make sure to resize your input images with this size only. This will prevent raising exceptions.</li><li id="fb1f" class="nf ng fq nh b go rr nj nk gr rs nm nn no rt nq nr ns ru nu nv nw rv ny nz oa qq qr qs bk">When fine-tuning with Big Vision and JAX, You might have JAX GPU-related problems. Ways to overcome this issue are:</li></ul><p id="75d1" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">a. Reducing the samples in your training and validation datasets.</p><p id="f547" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">b. Increasing the batch size from 8 to 16 or higher.</p><ul class=""><li id="afe5" class="nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa qq qr qs bk">The fine-tuned model has a size of ~ 5GB. Make sure to have enough space in your Drive to store it.</li></ul><h1 id="bf0a" class="ov ow fq bf ox oy oz gq pa pb pc gt pd pe pf pg ph pi pj pk pl pm pn po pp pq bk">Takeaway messages</h1><p id="c6f6" class="pw-post-body-paragraph nf ng fq nh b go pr nj nk gr ps nm nn no pt nq nr ns pu nu nv nw pv ny nz oa fj bk">Discovering a new AI model is exciting, especially in this age of multimodal algorithms transforming our society. However, working with state-of-the-art models can sometimes be challenging due to the lack of available documentation. Therefore, the launch of a new AI model should be accompanied by comprehensive documentation to ensure its smooth and widespread adoption, especially among professionals who are still inexperienced in this area.</p><p id="07f2" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Despite the difficulties I encountered fine-tuning Paligemma, the current pre-trained models are powerful at doing zero-shot object detection and image segmentation, which can be used for many applications, including assisted ML labeling.</p><p id="6eed" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Are you using Paligemma in your Computer Vision projects? Share your experience fine-tuning this model in the comments!</p><p id="623d" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">I hope you enjoyed this post. Once more, thanks for reading!</p><p id="5425" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">You can contact me via LinkedIn at:</p><p id="ad88" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk"><a class="af px" href="https://www.linkedin.com/in/camartinezbarbosa/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/camartinezbarbosa/</a></p><p id="3a17" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk"><em class="rl">Acknowledgments: I want to thank José Celis-Gil for all the fruitful discussions on data preprocessing and modeling.</em></p></div></div></div></div>    
</body>
</html>