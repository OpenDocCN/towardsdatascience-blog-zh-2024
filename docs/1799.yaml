- en: Forecasting US GDP using Machine Learning and Mathematics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/forecasting-us-gdp-using-machine-learning-and-mathematics-62f3f794d690?source=collection_archive---------2-----------------------#2024-07-24](https://towardsdatascience.com/forecasting-us-gdp-using-machine-learning-and-mathematics-62f3f794d690?source=collection_archive---------2-----------------------#2024-07-24)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What can we learn from this modern problem?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@dmongia_35626?source=post_page---byline--62f3f794d690--------------------------------)[![Dron
    Mongia](../Images/97b524f351f2dbb489cb9dc6fc340fce.png)](https://medium.com/@dmongia_35626?source=post_page---byline--62f3f794d690--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--62f3f794d690--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--62f3f794d690--------------------------------)
    [Dron Mongia](https://medium.com/@dmongia_35626?source=post_page---byline--62f3f794d690--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--62f3f794d690--------------------------------)
    ·14 min read·Jul 24, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3fbd13ca97f54686d59cd4faecac6b41.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Igor Omilaev](https://unsplash.com/@omilaev?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '**Motivation: why would we want to forecast US GDP?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GDP is a very strong metric of a country’s economic well-being; therefore, making
    forecasts of the measurement highly sought after. Policymakers and legislators,
    for example, may want to have a rough forecast of the trends regarding the country’s
    GDP prior to passing a new bill or law. Researchers and economists will also consider
    these forecasts for various endeavors in both academic and industrial settings.
  prefs: []
  type: TYPE_NORMAL
- en: '**Procedure: how can we approach this problem?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Forecasting GDP, similarly to many other time series problems, follows a general
    workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Using the integrated FRED (Federal Reserve Economic Data) library and API, we
    will create our features by constructing a data frame composed of US GDP along
    with some other metrics that are closely related (GDP = Consumption + Investment
    + Govt. Spending + Net Export)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using a variety of statistical tests and analyses, we will explore the nuances
    of our data in order to better understand the underlying relationships between
    features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we will utilize a variety of statistical and machine-learning models
    to conclude which approach can lead us to the most accurate and efficient forecast.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Along all of these steps, we will delve into the nuances of the underlying mathematical
    backbone that supports our tests and models.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1: Feature Creation**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To construct our dataset for this project, we will be utilizing the FRED (Federal
    Reserve Economic Data) API which is the premier application to gather economic
    data. Note that to use this data, one must register an account on the FRED website
    and request a custom API key.
  prefs: []
  type: TYPE_NORMAL
- en: Each time series on the website is connected to a specific character string
    (for example GDP is linked to ‘GDP’, Net Export to ‘NETEXP’, etc.). This is important
    because when we make a call for each of our features, we need to make sure that
    we specify the correct character string to go along with it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keeping this in mind, lets now construct our data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that since we have defined functions as opposed to static chunks of
    code, we are free to expand our list of features for further testing. Running
    this code, our resulting data frame is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b7ae9f89eac9b86b0cb20261a2c07e7f.png)'
  prefs: []
  type: TYPE_IMG
- en: (final dataset)
  prefs: []
  type: TYPE_NORMAL
- en: We notice that our dataset starts from the 1960s, giving us a fairly broad historical
    context. In addition, looking at the shape of the data frame, we have 1285 instances
    of actual economic data to work with, a number that is not necessarily small but
    not big either. These observations will come into play during our modeling phase.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2: Exploratory Data Analysis**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that our dataset is initialized, we can begin visualizing and conducting
    tests to gather some insights into the behavior of our data and how our features
    relate to one another.
  prefs: []
  type: TYPE_NORMAL
- en: '**Visualization (Line plot):**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first approach to analyzing this dataset is to simply graph each feature
    on the same plot in order to catch some patterns. We can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the code, we get the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f2b62eec9e69b6fe2b12d21217107e35.png)'
  prefs: []
  type: TYPE_IMG
- en: (features plotted against one another)
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the graph, we notice below that some of the features resemble GDP
    far more than others. For instance, GDP and PCE follow almost the exact same trend
    while NETEXP shares no visible similarities. Though it may be tempting, we can
    not yet begin selecting and removing certain features before conducting more exploratory
    tests.
  prefs: []
  type: TYPE_NORMAL
- en: '**ADF (Augmented Dickey-Fuller) Test:**'
  prefs: []
  type: TYPE_NORMAL
- en: The ADF (Augmented Dickey-Fuller) Test evaluates the stationarity of a particular
    time series by checking for the presence of a unit root, a characteristic that
    defines a time series as nonstationarity. Stationarity essentially means that
    a time series has a constant mean and variance. This is important to test because
    many popular forecasting methods (including ones we will use in our modeling phase)
    require stationarity to function properly.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fc808d998fca289a59d232b4a07aec12.png)'
  prefs: []
  type: TYPE_IMG
- en: Formula for Unit Root
  prefs: []
  type: TYPE_NORMAL
- en: 'Although we can determine the stationarity for most of these time series just
    by looking at the graph, doing the testing is still beneficial because we will
    likely reuse it in later parts of the forecast. Using the Statsmodel library we
    write:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'giving us the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/974ea0b3207fbfe0a4af91f4957f2d55.png)'
  prefs: []
  type: TYPE_IMG
- en: (ADF Test results)
  prefs: []
  type: TYPE_NORMAL
- en: The numbers we are interested from this test are the P-values. A P-value close
    to zero (equal to or less than 0.05) implies stationarity while a value closer
    to 1 implies nonstationarity. We can see that all of our time series features
    are highly nonstationary due to their statistically insignificant p-values, in
    other words, we are unable to reject the null hypothesis for the absence of a
    unit root. Below is a simple visual representation of the test for one of our
    features. The red dotted line represents the P-value where we would be able to
    determine stationarity for the time series feature, and the blue box represents
    the P-value where the feature is currently.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4abee4a8fd2bf4e883c33c276457b596.png)'
  prefs: []
  type: TYPE_IMG
- en: (ADF visualization for NETEXP)
  prefs: []
  type: TYPE_NORMAL
- en: '**VIF (Variance Inflation Factor) Test:**'
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of finding the Variance Inflation Factor of each feature is to check
    for multicollinearity, or the degree of correlation the predictors share with
    one another. High multicollinearity is not necessarily detrimental to our forecast,
    however, it can make it much harder for us to determine the individual effect
    of each feature time series for the prediction, thus hurting the interpretability
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, the calculation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/83a0be71deb630d58bab2a36e5bf4094.png)'
  prefs: []
  type: TYPE_IMG
- en: (Variance Inflation Factor of predictor)
  prefs: []
  type: TYPE_NORMAL
- en: 'with *X*j representing our selected predictor and *R*²j is the coefficient
    of determination for our specific predictor. Applying this calculation to our
    data, we arrive at the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d78b31f4b9919259c798c16fc5f0fabb.png)'
  prefs: []
  type: TYPE_IMG
- en: (VIF scores for each feature)
  prefs: []
  type: TYPE_NORMAL
- en: Evidently, our predictors are very closely linked to one another. A VIF score
    greater than 5 implies multicollinearity, and the scores our features achieved
    far exceed this amount. Predictably, PCE by far had the highest score which makes
    sense given how its shape on the line plot resembled many of the other features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Modeling'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have looked thoroughly through our data to better understand the
    relationships and characteristics of each feature, we will begin to make modifications
    to our dataset in order to prepare it for modeling.
  prefs: []
  type: TYPE_NORMAL
- en: '**Differencing to achieve stationarity**'
  prefs: []
  type: TYPE_NORMAL
- en: To begin modeling we need to first ensure our data is stationary. we can achieve
    this using a technique called differencing, which essentially transforms the raw
    data using a mathematical formula similar to the tests above.
  prefs: []
  type: TYPE_NORMAL
- en: 'The concept is defined mathematically as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aed0be843d2dfa1b1249170a84df02fb.png)'
  prefs: []
  type: TYPE_IMG
- en: (First Order Differencing equation)
  prefs: []
  type: TYPE_NORMAL
- en: This makes it so we are removing the nonlinear trends from the features, resulting
    in a constant series. In other words, we are taking values from our time series
    and calculating the change which occurred following the previous point.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can implement this concept in our dataset and check the results from the
    previously used ADF test with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'running this results in:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/13297aef25c3f1abca6c69b392b360b8.png)'
  prefs: []
  type: TYPE_IMG
- en: (ADF test for differenced data)
  prefs: []
  type: TYPE_NORMAL
- en: 'We notice that our new p-values are less than 0.05, meaning that we can now
    reject the null hypothesis that our dataset is nonstationary. Taking a look at
    the graph of the new dataset proves this assertion:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4cd4f014b03f09bde08b6f998f15fe92.png)'
  prefs: []
  type: TYPE_IMG
- en: (Graph of Differenced Data)
  prefs: []
  type: TYPE_NORMAL
- en: We see how all of our time series are now centered around 0 with the mean and
    variance remaining constant. In other words, our data now visibly demonstrates
    characteristics of a stationary system.
  prefs: []
  type: TYPE_NORMAL
- en: '**VAR (Vector Auto Regression) Model**'
  prefs: []
  type: TYPE_NORMAL
- en: The first step of the VAR model is performing the **Granger Causality Test**
    which will tell us which of our features are statistically significant to our
    prediction. The test indicates to us if a lagged version of a specific time series
    can help us predict our target time series, however not necessarily that one time
    series causes the other (note that causation in the context of statistics is a
    far more difficult concept to prove).
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the StatsModels library, we can apply the test as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the code results in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d926ffb0542cc4ebe11520d5e637eddd.png)'
  prefs: []
  type: TYPE_IMG
- en: (Sample of Granger Causality for two features)
  prefs: []
  type: TYPE_NORMAL
- en: Here we are just looking for a single lag for each feature that has statistically
    significant p-values(>.05). So for example, since on the first lag both NETEXP
    and GovTotExp, we will consider both these features for our VAR model. Personal
    consumption expenditures arguably did not make this cut-off (see notebook), however,
    the sixth lag is so close that I decided to keep it in. Our next step is to create
    our VAR model now that we have decided that all of our features are significant
    from the Granger Causality Test.
  prefs: []
  type: TYPE_NORMAL
- en: 'VAR (Vector Auto Regression) is a model which can leverage different time series
    to gauge patterns and determine a flexible forecast. Mathematically, the model
    is defined by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f11eaeef138234215f2348bda28faf5a.png)'
  prefs: []
  type: TYPE_IMG
- en: (Vector Auto Regression Model)
  prefs: []
  type: TYPE_NORMAL
- en: 'Where *Y*t is some time series at a particular time t and *A*p is a determined
    coefficient matrix. We are essentially using the lagged values of a time series
    (and in our case other time series) to make a prediction for *Y*t. Knowing this,
    we can now apply this algorithm to the data_diff dataset and evaluate the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2712377799137a622265b5ed812aaf0a.png)'
  prefs: []
  type: TYPE_IMG
- en: (Evaluation Metrics)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b483d05226c2be735c416b43cb10f10c.png)'
  prefs: []
  type: TYPE_IMG
- en: (Actual vs Forecasted GDP for VAR)
  prefs: []
  type: TYPE_NORMAL
- en: Looking at this forecast, we can clearly see that despite missing the mark quite
    heavily on both evaluation metrics used (MAE and MAPE), our model visually was
    not too inaccurate barring the outliers caused by the pandemic. We managed to
    stay on the testing line for the most part from 2018–2019 and from 2022–2024,
    however, the global events following obviously threw in some unpredictability
    which affected the model’s ability to precisely judge the trends.
  prefs: []
  type: TYPE_NORMAL
- en: '**VECM (Vector Error Correction Model)**'
  prefs: []
  type: TYPE_NORMAL
- en: 'VECM (Vector Error Correction Model) is similar to VAR, albeit with a few key
    differences. Unlike VAR, VECM does not rely on stationarity so differencing and
    normalizing the time series will not be necessary. VECM also assumes **cointegration**,
    or long-term equilibrium between the time series. Mathematically, we define the
    model as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5619e8ab39353fb30a31818414ce003f.png)'
  prefs: []
  type: TYPE_IMG
- en: (VECM model equation)
  prefs: []
  type: TYPE_NORMAL
- en: 'This equation is similar to the VAR equation, with Π being a coefficient matrix
    which is the product of two other matrices, along with taking the sum of lagged
    versions of our time series *Y*t. Remembering to fit the model on our original
    (not difference) dataset, we achieve the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b80ed2ebd8453a3fe0fd50f270e4e3f2.png)'
  prefs: []
  type: TYPE_IMG
- en: (Actual vs Forecasted GDP for VECM)
  prefs: []
  type: TYPE_NORMAL
- en: Though it is hard to compare to our VAR model to this one given that we are
    now using nonstationary data, we can still deduce both by the error metric and
    the visualization that this model was not able to accurately capture the trends
    in this forecast. With this, it is fair to say that we can rule out traditional
    statistical methods for approaching this problem.
  prefs: []
  type: TYPE_NORMAL
- en: '**Machine Learning forecasting**'
  prefs: []
  type: TYPE_NORMAL
- en: When deciding on a machine learning approach to model this problem, we want
    to keep in mind the amount of data that we are working with. Prior to creating
    lagged columns, our dataset has a total of 1275 observations across all time-series.
    This means that using more complex approaches, such as LSTMs or gradient boosting,
    are perhaps unnecessary as we can use a more simple model to receive the same
    amount of accuracy and far more interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: '**Train-Test Split**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Train-test splits for time series problems differ slightly from splits in traditional
    regression or classification tasks (Note we also used the train-test split in
    our VAR and VECM models, however, it feels more appropriate to address in the
    Machine Learning section). We can perform our Train-Test split on our differenced
    data with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Here it is imperative that we do not shuffle around our data, since that would
    mean we are training our model on data from the future which in turn will cause
    data leakages.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/de5a1ce10a9912a4601563441503a514.png)'
  prefs: []
  type: TYPE_IMG
- en: example of train-test split on time series data
  prefs: []
  type: TYPE_NORMAL
- en: Also in comparison, notice that we are training over a very large portion (90
    percent) of the data whereas typically we would train over 75 percent in a common
    regression task. This is because practically, we are not actually concerned with
    forecasting over a large time frame. Realistically even forecasting over several
    years is not probable for this task given the general unpredictability that comes
    with real-world time series data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Random Forests**'
  prefs: []
  type: TYPE_NORMAL
- en: Remembering our VIF test from earlier, we know our features are highly correlated
    with one another. This partially plays into the decision to choose random forests
    as one of our machine-learning models. decision trees make binary choices between
    features, meaning that theoretically our features being highly correlated should
    not be detrimental to our model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eca013bc75fccede71560849246b59d1.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of a traditional binary decision tree that builds random forests models
  prefs: []
  type: TYPE_NORMAL
- en: To add on, random forest is generally a very strong model being robust to overfitting
    from the stochastic nature of how the trees are computed. Each tree uses a random
    subset of the total feature space, meaning that certain features are unlikely
    to dominate the model. Following the construction of the individual trees, the
    results are averaged in order to make a final prediction using every individual
    learner.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can implement the model to our dataset with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'running this gives us the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3ca1b24217fb26ad0ef1b59f43daaf68.png)'
  prefs: []
  type: TYPE_IMG
- en: (Evaluation Metrics for Random Forests)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c0688d7c0e77bc7292b383b26984a3e3.png)'
  prefs: []
  type: TYPE_IMG
- en: (Actual vs Forecasted GDP for Random Forests)
  prefs: []
  type: TYPE_NORMAL
- en: We can see that Random Forests was able to produce our best forecast yet, attaining
    better error metrics than our attempts at VAR and VECM. Perhaps most impressively,
    visually we can see that our model was almost perfectly encapsulating the data
    from 2017–2019, just prior to encountering the outliers.
  prefs: []
  type: TYPE_NORMAL
- en: '**K Nearest Neighbors**'
  prefs: []
  type: TYPE_NORMAL
- en: KNN (K-Nearest-Neighbors) was one final approach we will attempt. Part of the
    reasoning for why we choose this specific model is due to the feature-to-observation
    ratio. KNN is a distanced based algorithm that we are dealing with data which
    has a low amount of feature space comparative to the number of observations.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the model, we must first select a hyperparameter *k* which defines the
    number of neighbors our data gets mapped to. A higher *k* value insinuates a more
    biased model while a lower *k* value insinuates a more overfit model. We can choose
    the optimal one with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this code gives us:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/99f8d61e42cf3e26d8b022ba6d1a7d7d.png)'
  prefs: []
  type: TYPE_IMG
- en: (accuracy comparing different values of k)
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that our best accuracy measurements are achieved when *k*=2, following
    that value the model becomes too biased with increasing values of *k*. knowing
    this, we can now apply the model to our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'resulting in:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/79f5cb7098825c4ddd54c80da4a7f5e2.png)'
  prefs: []
  type: TYPE_IMG
- en: (Evaluation metrics for KNN)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/76927c9c5d7fd1281174d717c802f2ac.png)'
  prefs: []
  type: TYPE_IMG
- en: (Actual vs Forecasted GDP for KNN)
  prefs: []
  type: TYPE_NORMAL
- en: We can see KNN in its own right performed very well. Despite being outperformed
    slightly in terms of error metrics compared to Random Forests, visually the model
    performed about the same and arguably captured the period before the pandemic
    from 2018–2019 even better than Random Forests.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusions**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Looking at all of our models, we can see the one which performed the best was
    Random Forests. This is most likely due to Random Forests for the most part being
    a very strong predictive model that can be fit to a variety of datasets. In general,
    the machine learning algorithms far outperformed the traditional statistical methods.
    Perhaps this can be explained by the fact that VAR and VECM both require a great
    amount of historical background data to work optimally, something which we did
    not have much of given that our data came out in quarterly intervals. There also
    may be something to be said about how both the machine learning models used were
    nonparametric. These models often are governed by fewer assumptions than their
    counterparts and therefore may be more flexible to unique problem sets like the
    one here. Below is our final best prediction, removing the differencing transformation
    we previously used to fit the models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f43369d43765369d17baa7ef30ebb2c5.png)'
  prefs: []
  type: TYPE_IMG
- en: (Actual vs Forecasted GDP for Random Forests (not differenced))
  prefs: []
  type: TYPE_NORMAL
- en: Challenges and Areas of Improvement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By far the greatest challenge regarding this forecasting problem was handling
    the massive outlier caused by the pandemic along with the following instability
    caused by it. Our methods for forecasting obviously can not predict that this
    would occur, ultimately decreasing our accuracy for each approach. Had our goal
    been to forecast the previous decade, our models would most likely have a much
    easier time finding and predicting trends. In terms of improvement and further
    research, I think a possible solution would be to perform some sort of normalization
    and outlier smoothing technique on the time interval from 2020–2024, and then
    evaluate our fully trained model on new quarterly data that comes in. In addition,
    it may be beneficial to incorporate new features that have a heavy influence on
    GDP such as quarterly inflation and personal asset evaluations.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For traditional statistical methods- [https://link.springer.com/book/10.1007/978-1-4842-7150-6](https://link.springer.com/book/10.1007/978-1-4842-7150-6)
    , [https://www.statsmodels.org/stable/generated/statsmodels.tsa.vector_ar.vecm.VECM.html](https://www.statsmodels.org/stable/generated/statsmodels.tsa.vector_ar.vecm.VECM.html)
  prefs: []
  type: TYPE_NORMAL
- en: For machine learning methods — [https://www.statlearning.com/](https://www.statlearning.com/)
  prefs: []
  type: TYPE_NORMAL
- en: For dataset — [https://fred.stlouisfed.org/docs/api/fred/](https://fred.stlouisfed.org/docs/api/fred/)
  prefs: []
  type: TYPE_NORMAL
- en: FRED provides licensed, free-to-access datasets for any user who owns an API
    key, read more here — [https://fredhelp.stlouisfed.org/fred/about/about-fred/what-is-fred/](https://fredhelp.stlouisfed.org/fred/about/about-fred/what-is-fred/)
  prefs: []
  type: TYPE_NORMAL
- en: All pictures not specifically given credit in the caption belong to me.
  prefs: []
  type: TYPE_NORMAL
- en: Notebook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: please note that in order to run this notebook you must create an account on
    the FRED website, request an API key, and paste said key into the second cell
    of the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/Dronmong/GDP-Forecast](https://github.com/Dronmong/GDP-Forecast)'
  prefs: []
  type: TYPE_NORMAL
