<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Let There Be Light! Diffusion Models and the Future of Relighting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Let There Be Light! Diffusion Models and the Future of Relighting</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/let-there-be-light-diffusion-models-and-the-future-of-relighting-03af12b8e86c?source=collection_archive---------3-----------------------#2024-11-04">https://towardsdatascience.com/let-there-be-light-diffusion-models-and-the-future-of-relighting-03af12b8e86c?source=collection_archive---------3-----------------------#2024-11-04</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="3fbb" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Discover how cutting-edge diffusion models tackle relighting, harmonization, and shadow removal in this in-depth blog on scene editing</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@darth_gera?source=post_page---byline--03af12b8e86c--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Pulkit Gera" class="l ep by dd de cx" src="../Images/c7e840a79628d71320b8c2c63277df69.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/0*w-ZAyuzUZUkCQ7R0."/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--03af12b8e86c--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@darth_gera?source=post_page---byline--03af12b8e86c--------------------------------" rel="noopener follow">Pulkit Gera</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--03af12b8e86c--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">15 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 4, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">4</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/7a8e3f4981f22fe3485a083e12fbd430.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yCBvQRVlDMP4WZqB"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@brianaitk0001?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Brian Aitkenhead</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="608d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Relighting is the task of rendering a scene under a specified target lighting condition, given an input scene. This is a crucial task in computer vision and graphics. However, it is an ill-posed problem, because the appearance of an object in a scene results from a complex interplay between factors like the light source, the geometry, and the material properties of the surface. These interactions create ambiguities. For instance, given a photograph of a scene, is a dark spot on an object due to a shadow cast by lighting or is the material itself dark in color? Distinguishing between these factors is key to effective relighting.</p><p id="e525" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this blog post we discuss how different papers are tackling the problem of relighting via diffusion models. Relighting encompases a variety of subproblems including simple lighting adjustments, image harmonization, shadow removal and intrinsic decomposition. These areas are essential for refining scene edits such as balancing color and shadow across composited images or decoupling material and lighting properties. We will first introduce the problem of relighting and briefly discuss Diffusion models and ControlNets. We will then discuss different approaches that solve the problem of relighting in different types of scenes ranging from single objects to portraits to large scenes.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="280b" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Solving Relighting</h1><p id="0264" class="pw-post-body-paragraph nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny fj bk">The goal is to decompose the scene into its fundamental components such as geometry, material, and light interactions and model them parametrically. Once solved then we can change it according to our preference. The appearance of a point in the scene can be described by the rendering equation as follows:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pi"><img src="../Images/eebcbafb661916da1ec0d2ffb59eb303.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0LD2SnpJPonyqxvn"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Rendering Equation from <a class="af nc" href="https://twitter.com/levork/status/609603797258600448" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="1169" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Most methods aim to solve for each single component of the rendering equation. Once solved, then we can perform relighting and material editing. Since the lighting term L is on both sides, this equation cannot be evaluated analytically and is either solved via Monte Carlo methods or approximation based approaches.</p><p id="8968" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">An alternate approach is data-driven learning, where instead of explicitly modeling the scene properties it directly learns from data. For example, instead of fitting a parametric function, a network can learn the material properties of the surface from data. Data-driven approaches have proven to be more powerful than parametric approaches. However they require a huge amount of high quality data which is really hard to collect especially for lighting and material estimation tasks.</p><figure class="mm mn mo mp mq mr"><div class="pj io l ed"><div class="pk pl l"/></div></figure><figure class="mm mn mo mp mq mr"><div class="pj io l ed"><div class="pk pl l"/></div></figure><p id="fa6e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Datasets for lighting and material estimation are rare as they require expensive, complex setups such as light stages to capture detailed lighting interactions. These setups are accessible to only a few organizations, limiting the availability of data for training and evaluation. There are no full-body ground truth light stage datasets publicly available which further highlights this challenge.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="e1b9" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Diffusion Models</h1><p id="b14f" class="pw-post-body-paragraph nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny fj bk">Computer vision has experienced a significant transformation with the advent of pre-training on vast amounts of image and video data available online. This has led to the development of foundation models, which serve as powerful general-purpose models that can be fine-tuned for a wide range of specific tasks. Diffusion models work by learning to model the underlying data distribution from independent samples, gradually reversing a noise-adding process to generate realistic data. By leveraging their ability to generate high-quality samples from learned distributions, diffusion models have become essential tools for solving a diverse set of generative tasks.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pm"><img src="../Images/285e147fe9962066256db92eaaf48792.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/0*gVtpoG1hR1dpPSwx"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Latent Diffusion Models from <a class="af nc" href="https://arxiv.org/abs/2112.10752" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="0f93" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">One of the most prominent examples of this is <a class="af nc" href="https://github.com/CompVis/stable-diffusion" rel="noopener ugc nofollow" target="_blank">Stable Diffusion</a>(SD), which was trained on the large-scale LAION-5B dataset that consists of 5 billion image text pairs. It has encoded a wealth of general knowledge about visual concepts making it suitable for fine-tuning for specific tasks. It has learnt fundamental relationships and associations during training such as chairs having 4 legs or recognizing structure of cars. This intrinsic understanding has allowed Stable Diffusion to generate highly coherent and realistic images and be used for fine tuning to predict other modalities. Based on this idea, the question arises if we can leverage pretrained SD to solve the problem of scene relighting.</p><p id="de93" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">So how do we fine-tune LDMs? A naive approach is to do transfer learning with LDMs. This would be freezing early layers (which capture general features) and fine tuning the model on the specific task. While this approach has been used by some papers such as <a class="af nc" href="https://arxiv.org/abs/2312.02970" rel="noopener ugc nofollow" target="_blank">Alchemist</a> (for Material Transfer), it requires a large amount of paired data for the model to generalize well. Another drawback to this approach is the risk of catastrophic forgetting, where the model losses the knowledge gained during pretraining. This would limit its capability on generalizing across various conditions.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pn"><img src="../Images/6faeb78a6bd8b3bd8fda521767bfddbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/0*DUjUi2WVQHR527kH"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">ControlNet figure from <a class="af nc" href="https://arxiv.org/abs/2302.05543" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="9d18" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Another approach to fine-tuning such large models is by introducing a ControlNet. Here, a copy of the network is made and the weights of the original network are frozen. During training only the duplicate network weights are updated and the conditioning signal is passed as input to the duplicate network. The original network continues to leverage its pretrained knowledge.</p><p id="c210" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">While this increases the memory footprint, the advantage is that we dont lose the generalization capabilities acquired from training on large scale datasets. It ensures that it retains its ability to generate high-quality outputs across a wide range of prompts while learning the task specific relationships needed for the current task.</p><p id="87ac" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Additionally it helps the model learn robust and meaningful connections between control input and the desired output. By decoupling the control network from the core model, it avoids the risk of overfitting or catastrophic forgetting. It also needs significantly less paired data to train.</p><p id="d2de" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">While there are other techniques for fine-tuning foundational models — such as LoRA (Low-Rank Adaptation) and others — we will focus on the two methods discussed: traditional transfer learning and ControlNet. These approaches are particularly relevant for understanding how various papers have tackled image-based relighting using diffusion models.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="6bf2" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">DiLightNet</h1><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk po"><img src="../Images/e7cc7d02681474f33ee17d588721a584.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pxS_3xjyO8-OuYnw"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><a class="af nc" href="https://arxiv.org/pdf/2409.13690" rel="noopener ugc nofollow" target="_blank">DiLightNet:Fine-grained Lighting Control for Diffusion-based Image Generation</a></figcaption></figure><p id="4219" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Introduction</strong></p><p id="70a6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This work proposes fine grained control over relighting of an input image. The input image can either be generated or given as input. Further it can also change the material of the object based on the text prompt. The objective is to exert fine-grained control on the effects of lighting.</p><p id="7cc8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Method</strong></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pp"><img src="../Images/d543f869aa1291a2ee5fba0a36d7d8d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wxVRVYESbnUVVD9b"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Method figure from <a class="af nc" href="https://arxiv.org/pdf/2409.13690" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="1263" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Given an input image, the following preprocessing steps are applied:</p><ol class=""><li id="357a" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pq pr ps bk">Estimate background and depth map using off the shelf SOTA models.</li><li id="9b11" class="nd ne fq nf b go pt nh ni gr pu nk nl nm pv no np nq pw ns nt nu px nw nx ny pq pr ps bk">Extract mesh by triangulating the depth map</li><li id="febb" class="nd ne fq nf b go pt nh ni gr pu nk nl nm pv no np nq pw ns nt nu px nw nx ny pq pr ps bk">Generate 4 different radiance cues images. Radiance cues images are created by assigning the extracted mesh different materials and rendering them under target lighting. The radiance cues images act as basis for encoding lighting effects such as specular, shadows and global illumination.</li></ol><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk py"><img src="../Images/0f10a43072a7b2914b326255e305f274.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yRrGSluLUSH0W4cY"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">ControlNet inputs from <a class="af nc" href="https://arxiv.org/pdf/2409.13690" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="987f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Once these images are generated, they train a ControlNet module. The input image and the mask are passed through an encoder decoder network which outputs a 12 channel feature map. This is then multiplied with the radiance cues images that are channel wise concatenated together. Thus during training, the noisy target image is denoised with this custom 12 channel image as conditioning signal.</p><p id="d5c8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Additionally an appearance seed is provided to procure consistent appearance under different illumination. Without it the network renders a different interpretation of light-matter interaction. Additionally one can provide more cues via text to alter the appearance such as by adding “plastic/shiny metallic” to change the material of the generated image.</p><p id="9cbe" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Implementation</strong></p><p id="23d9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The dataset was curated using 25K synthetic objects from Objaverse. Each object was rendered from 4 unique views and lit with 12 different lighting conditions ranging from point source lighting, multiple point source, environment maps and area lights. For training, the radiance cues were rendered in blender.</p><p id="0d24" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The ControlNet module uses stable diffusion v2.1 as base pretrained model to refine. Training took roughly 30 hours on 8x NVIDIA V100 GPUs. Training data was rendered in Blender at 512x512 resolution.</p><p id="0aa3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Results</strong></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pz"><img src="../Images/b1acd57738d769c1ed1202bac18da10b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*T_J3R0BABa7MLnFq"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">DiLightNet results from <a class="af nc" href="https://arxiv.org/pdf/2409.13690" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="8c4c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This figure shows the provisional image as reference and the corresponding target lighting under which the object is relit.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qa"><img src="../Images/256c63b5832c8e8f8e26644606908b40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fBjC_pAYUmHEZRyr"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">DiLightNet results from <a class="af nc" href="https://arxiv.org/pdf/2409.13690" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="1eec" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This figure shows how the text prompt can be used to change the material of the object.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qb"><img src="../Images/6979caf7f09e042cc803b9e03051a7ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VD3NlQXwRttcd7wy"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">DiLightNet results from <a class="af nc" href="https://arxiv.org/pdf/2409.13690" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="aa63" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This figure shows more results of AI generated provisional images that are then rendered under different input environment light conditions.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qa"><img src="../Images/35a2d76fa45cbe4185b49d99c505f004.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*B_g6J7PT3kd9vhuF"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">DiLightNet results from <a class="af nc" href="https://arxiv.org/pdf/2409.13690" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="1713" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This figure shows the different solutions the network comes up to resolve light interaction if the appearance seed is not fixed.</p><p id="6c68" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Limitations</strong></p><p id="2bba" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Due to training on synthetic objects, the method is not very good with real images and works much better with AI generated provisional images. Additionally the material light interaction might not follow the intention of the prompt. Since it relies on depth maps for generating radiance cues, it may fail to get satisfactory results. Finally generating a rotating light video may not result in consistent results.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="c04d" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Neural Gaffer</h1><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qc"><img src="../Images/72883b2854c2037fa5466a9ccac7d95b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*MrTDmDiXxFDk5u2u"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><a class="af nc" href="https://arxiv.org/pdf/2406.07520" rel="noopener ugc nofollow" target="_blank">Neural Gaffer: Relighting Any Object via Diffusion</a></figcaption></figure><p id="c8e4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Introduction</strong></p><p id="0b00" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This work proposes an end to end 2D relighting diffusion model. This model learns physical priors from synthetic dataset featuring physically based materials and HDR environment maps. It can be further used to relight multiple views and be used to create a 3D representation of the scene.</p><p id="0f30" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Method</strong></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qd"><img src="../Images/f7f7dd9fd071a66d99d238ea8a5efe35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FnadB5Ibedb3jqI7"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Neural Gaffer method figure from <a class="af nc" href="https://arxiv.org/pdf/2406.07520" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="a3bc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Given an image and a target HDR environment map, the goal is to learn a model that can synthesize a relit version of the image which here is a single object. This is achieved by adopting a pre-trained <a class="af nc" href="https://arxiv.org/abs/2303.11328" rel="noopener ugc nofollow" target="_blank">Zero-1-to-3</a> model. Zero-1-to-3 is a diffusion model that is conditioned on view direction to render novel views of an input image. They discard its novel view synthesis components. To incorporate lighting conditions, they concatenate input image and environment map encodings with the denoising latent.</p><p id="c8c2" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The input HDR environment map E is split into two components: E_l, a tone-mapped LDR representation capturing lighting details in low-intensity regions, and E_h, a log-normalized map preserving information across the full spectrum. Together, these provide the network with a balanced representation of the energy spectrum, ensuring accurate relighting without the generated output appearing washed out due to extreme brightness.</p><p id="f80c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Additionally the CLIP embedding of the input image is also passed as input. Thus the input to the model is the Input Image, LDR Image, Normalized HDR Image and CLIP embedding of Image all conditioning the denoising network. This network is then used as prior for further 3D object relighting.</p><p id="7d86" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Implementation</strong></p><p id="2985" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The model is trained on a custom Relit Objaverse Dataset that consists of 90K objects. For each object there are 204 images that are rendered under different lighting conditions and viewpoints. In total, the dataset consists of 18.4 M images at resolution 512x512.</p><p id="459b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The model is finetuned from Zero-1-to-3’s checkpoint and only the denoising network is finetined. The input environment map is downsampled to 256x256 resolution. The model is trained on 8 A6000 GPUs for 5 days. Further downstream tasks such as text-based relighting and object insertion can be achieved.</p><p id="6ed4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Results</strong></p><p id="0e1f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">They show comparisons with different backgrounds and comparisons with other works such as DilightNet and <a class="af nc" href="https://github.com/lllyasviel/IC-Light" rel="noopener ugc nofollow" target="_blank">IC-Light</a>.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pn"><img src="../Images/6babf1b85e8818a8e3044f9702c99353.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/0*8xFL9AIVSt2OBwrt"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Neural Gaffer results from <a class="af nc" href="https://arxiv.org/pdf/2406.07520" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="5d36" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This figure compares the relighting results of their method with IC-Light, another ControlNet based method. Their method can produce consistent lighting and color with the rotating environment map.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qe"><img src="../Images/64961cf772f654a103aa0aae7fc76b72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Gm-OJQgWia_zBxAq"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Neural Gaffer results from <a class="af nc" href="https://arxiv.org/pdf/2406.07520" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="a7b9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This figure compares the relighting results of their method with DiLightnet, another ControlNet based method. Their method can produce specular highlights and accurate colors.</p><p id="7ff1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Limitations</strong></p><p id="4b42" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">A major limitation is that it only produces low image resolution (256x256). Additionally it only works on objects and performs poorly for portrait relighting.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="ac6e" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Relightful Harmonization</h1><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qf"><img src="../Images/9c8aa09f65174b1a0080250bfb7ec656.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pcamZS-R84GUCBaL"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><a class="af nc" href="https://arxiv.org/pdf/2312.06886" rel="noopener ugc nofollow" target="_blank">Relightful Harmonization: Lighting-aware Portrait Background Replacement</a></figcaption></figure><p id="fdb7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Introduction</strong></p><p id="5757" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Image Harmonization is the process of aligning the color and lighting features of the foreground subject with the background to make it a plausible composition. This work proposes a diffusion based approach to solve the task.</p><p id="4ee3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Method</strong></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qf"><img src="../Images/41e5d56268af57453ecb1588651d890b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_oZY3ySkyQB2nohw"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Method figure from <a class="af nc" href="https://arxiv.org/pdf/2312.06886" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="89cc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Given an input composite image, alpha mask and a target background, the goal is to predict a relit portrait image. This is achieved by training a ControlNet to predict the Harmonized image output.</p><p id="1c74" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the first stage, we train a background control net model that takes the composite image and target background as input and outputs a relit portrait image. During training, the denoising network takes the noisy target image concatenated with composite image and predicts the noise. The background is provided as conditioning via the control net. Since background image by itself are LDR, they do not provide sufficient signals for relighting purposes.</p><p id="26c6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the second stage, an environment map control net model is trained. The HDR environment map provide lot more signals for relighting and this gives lot better results. However at test time, the users only provide LDR backgrounds. Thus, to bridge this gap, the 2 control net models are aligned with each other.</p><p id="2bb9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Finally more data is generated using the environment map ControlNet model and then the background ControlNet model is finetuned to generate more photo realistic results.</p><p id="4827" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Implementation</strong></p><p id="25f9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The dataset used for training consists of 400k image pair samples that were curated using 100 lightstage. In the third stage additional 200k synthetic samples were generated for finetuning for photorealism.</p><p id="f550" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The model is finetuned from InstructPix2PIx checkpoint The model is trained on 8 A100 GPUs at 512x512 resolution.</p><p id="bd0a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Results</strong></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qf"><img src="../Images/006be013588b7d6c75686bc67f409787.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vKiB_hPo-hnh9bHR"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Relightful Harmonization results from <a class="af nc" href="https://arxiv.org/pdf/2312.06886" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="11fb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This figure shows how the method neutralizes pronounced shadows in input which are usually hard to remove. On the left is input and right is relit image.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qg"><img src="../Images/da10eb947f17e6da62edf5df1e3deb56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0kCvigeeP1pKp2_J"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Relightful Harmonization results from <a class="af nc" href="https://arxiv.org/pdf/2312.06886" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qh"><img src="../Images/a9af9f87c920b8c7b77cc5b1662515a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8EonsCR8XqpNt6CY"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Relightful Harmonization results from <a class="af nc" href="https://arxiv.org/pdf/2312.06886" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="5029" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The figures show results on real world test subjects. Their method is able to remove shadows and make the composition more plausible compared to other methods.</p><p id="cf9c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Limitations</strong></p><p id="8d1b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">While this method is able to plausibly relight the subject, it is not great at identity preservation and struggles in maintaining color of the clothes or hair. Further it may struggle to eliminate shadow properly. Also it does not estimate albedo which is crucial for complex light interactions.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="5ca4" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Multi-Illumination Synthesis</h1><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qi"><img src="../Images/44a7b9666c182fb5958fd12b808554b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Kd3ScMm0Gj52oBND"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><a class="af nc" href="https://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/content/paper.pdf" rel="noopener ugc nofollow" target="_blank">A Diffusion Approach to Radiance Field Relighting using Multi-Illumination Synthesis</a></figcaption></figure><p id="4b04" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Introduction</strong></p><p id="c4c8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This work proposes a 2D relighting diffusion model that is further used to relight a radiance field of a scene. It first trains a ControlNet model to predict the scene under novel light directions. Then this model is used to generate more data which is eventually used to fit a relightable radiance field. We discuss the 2D relighting model in this section.</p><p id="4088" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Method</strong></p><p id="777c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Given a set of images X_i with corresponding depth map D (that is calculated via off the shelf methods), and light direction l_i the goal is to predict the scene under light direction l_j. During training, the input to the denoising network is X_i under random illumination, depth map D concatenated with noisy target image X_j. The light direction is encoded with 4th order SH and conditioned via ControlNet model.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qf"><img src="../Images/26f19d3cf8ca01a8e59224fade3490e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*I81uPyGZawr-ijz_"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Method figure from <a class="af nc" href="https://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/content/paper.pdf" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="a133" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Although this leads to decent results, there are some significant problems. It is unable to preserve colors and leads to loss in contrast. Additionally it produces distorted edges. To resolve this, they color-match the predictions to input image to compensate for color difference. This is done by converting the image to LAB space and then channel normalization. The loss is then taken between ground-truth and denoised output. To preserve edges, the decoder was pretrained on image inpainting tasks which was useful in preserving edges. This network is then used to create corresponding scene under novel light directions which is further used to create a relightable radiance field representation.</p><p id="ae24" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Implementation</strong></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qj"><img src="../Images/0c11081146d4da1f30d2b2a03b504318.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3vjUpKqAC2ls3opo"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Inference figure of 2D relighting module from <a class="af nc" href="https://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/content/paper.pdf" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="8fa1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The method was developed upon Multi-Illumination dataset. It consists of 1000 real scenes of indoor scenes captured under 25 lighting directions. The images also consist of a diffuse and a metallic sphere ball that is useful for obtaining the light direction in world coordinates. Additionally some more scenes were rendered in Blender. The network was trained on images at resolution 1536x1024 and training consisted of 18 non-front facing light directions on 1015 indoor scenes.</p><p id="94c7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The ControlNet module was trained using Stable Diffusion v2.1 model as backbone. It was trained on multiple A6000 GPUs for 150K iterations.</p><p id="2eaa" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Results</strong></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qk"><img src="../Images/867838e946028a45ca45db6b61e66920.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ULVmnxsHOBJxIN2d"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Results figure from <a class="af nc" href="https://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/content/paper.pdf" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="f7ed" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Here the diffuse spheres show the test time light directions. As can be seen, the method can render plausible relighting results</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ql"><img src="../Images/80b2fbbb5c5b9468ed12257070d043dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*MYnafFtIoT1N2hTM"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Results figure from <a class="af nc" href="https://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/content/paper.pdf" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="aab6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This figure shows how with the changing light direction, the specular highlights and shadows are moving as evident on the shiny highlight on the kettle.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qm"><img src="../Images/039969c08ca765eef247b61ee7b07b17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yAy8RI3eSS5YMrpg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Results figure from <a class="af nc" href="https://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/content/paper.pdf" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="a32b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This figure compares results with other relightable radiance field methods. Their method clearly preserves color and contrast much better compared to other methods.</p><p id="d559" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Limitations</strong></p><p id="2988" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The method does not enforce physical accuracy and can produce incorrect shadows. Additionally it also struggles to completely remove shadows in a fully accurate way. Also it does work reasonably for out of distribution scenes where the variance in lighting is not much.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="c033" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">LightIt</h1><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qn"><img src="../Images/81d3e65b89db90f7fabcde14396167cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7rKekzuj2wDTeuns"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><a class="af nc" href="https://arxiv.org/pdf/2403.10615" rel="noopener ugc nofollow" target="_blank">LightIt: Illumination Modeling and Control for Diffusion Models</a></figcaption></figure><p id="5480" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Introduction</strong></p><p id="c1bb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This work proposes a single view shading estimation method to generate a paired image and its corresponding direct light shading. This shading can then be used to guide the generation of the scene and relight a scene. They approach the problem as an intrinsic decomposition problem where the scene can be split into Reflectance and Shading. We will discuss the relighting component here.</p><p id="0d31" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Method</strong></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qo"><img src="../Images/36add64e8ec06b00ddd78eb38969ff54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*GgZj3WjlPpg02iiS"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Method figure from <a class="af nc" href="https://arxiv.org/pdf/2403.10615" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="e883" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Given an input image, its corresponding surface normal, text conditioning and a target direct shading image, they generate a relit stylized image. This is achieved by training a ControlNet module.</p><p id="1f62" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">During training, the noisy target image is passed to the denoising network along with text conditioning. The normal and target direct shading image are concatenated and passed through a Residual Control Encoder. The feature map is then used to condition the network. Additionally its also reconstructed back via Residual Control Decoder to regularize the training</p><p id="2948" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Implementation</strong></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qp"><img src="../Images/695a3799c65c3f1855b0ba28d22aa248.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-WPVeQ2kueArzxAN"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Dataset figure from <a class="af nc" href="https://arxiv.org/pdf/2403.10615" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="4c3e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The dataset consists of Outdoor Laval Dataset which consist of outdoor real world HDR panoramas. From these images, 250 512x512 images are cropped and various camera effects are applied. The dataset consists of 51250 samples of LDR images and text prompts along with estimated normal and shading maps. The normals maps were estimated from depth maps that were estimated using off the shelf estimators.</p><p id="4d0c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The ControlNet module was finetuned from stable diffusion v1.5. The network was trained for two epochs. Other training details are not shared.</p><p id="f757" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Results</strong></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qq"><img src="../Images/314ec873f97a6984c1734c417181c8fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*cAPrqbNSihVtB0Wy"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Results figure from <a class="af nc" href="https://arxiv.org/pdf/2403.10615" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="0288" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This figure shows that the generated images feature consistent lighting aligned with target shading for custom stylized text prompts. This is different from other papers discussed whose sole focus is on photorealism.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qr"><img src="../Images/b7603273635fd4716981670c651f25b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*szvp7-3lCC4uPCRF"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Results figure from <a class="af nc" href="https://arxiv.org/pdf/2403.10615" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="8ecb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This figure shows identity preservation under different lighting conditions.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qs"><img src="../Images/b84d91833607c18321534c4c1f3e02e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*k-9p2jo1hB9KaTnQ"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Results figure from <a class="af nc" href="https://arxiv.org/pdf/2403.10615" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="2e64" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This figure shows results on different styles and scenes under changing lighting conditions.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qt"><img src="../Images/b3364a0266714b2402a305762025eb1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/0*b-iuxlGBAe--JO2V"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Results figure from <a class="af nc" href="https://arxiv.org/pdf/2403.10615" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="312e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This figure compares relighting with another method. Utilizing the diffusion prior helps with generalization and resolving shading disambiguation.</p><p id="2b6c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Limitations</strong></p><p id="88e7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Since this method assumes directional lighting, it enables tracing rays in arbitrary direction. It requires shading cues to generate images which are non trivial to obtain. Further their method does not work for portraits and indoor scenes.</p><h1 id="5116" class="oh oi fq bf oj ok qu gq om on qv gt op oq qw os ot ou qx ow ox oy qy pa pb pc bk">Takeaways</h1><p id="3346" class="pw-post-body-paragraph nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny fj bk">We have discussed a non-exhaustive list of papers that leverage 2D diffusion models for relighting purposes. We explored different ways to condition Diffusion models for relighting ranging from radiance cues, direct shading images, light directions and environment maps. Most of these methods show results on synthetic datasets and dont generalize well to out of distribution datasets. There are more papers coming everyday and the base models are also improving. Recently <a class="af nc" href="https://github.com/lllyasviel/IC-Light/discussions/98" rel="noopener ugc nofollow" target="_blank">IC-Light2</a> was released which is a ControlNet model based upon Flux models. It will be interesting which direction it takes as maintaining identities is tricky.</p><p id="cd8c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">References:</p><ol class=""><li id="76ae" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pq pr ps bk"><a class="af nc" href="https://github.com/lllyasviel/IC-Light" rel="noopener ugc nofollow" target="_blank">GitHub — lllyasviel/IC-Light: More relighting!</a></li><li id="563d" class="nd ne fq nf b go pt nh ni gr pu nk nl nm pv no np nq pw ns nt nu px nw nx ny pq pr ps bk"><a class="af nc" href="https://illuminerf.github.io/" rel="noopener ugc nofollow" target="_blank">IllumiNeRF — 3D Relighting without Inverse Rendering</a></li><li id="5d61" class="nd ne fq nf b go pt nh ni gr pu nk nl nm pv no np nq pw ns nt nu px nw nx ny pq pr ps bk"><a class="af nc" href="https://neural-gaffer.github.io/" rel="noopener ugc nofollow" target="_blank">Neural Gaffer</a></li><li id="e375" class="nd ne fq nf b go pt nh ni gr pu nk nl nm pv no np nq pw ns nt nu px nw nx ny pq pr ps bk"><a class="af nc" href="https://dilightnet.github.io/" rel="noopener ugc nofollow" target="_blank">DiLightNet: Fine-grained Lighting Control for Diffusion-based Image Generation</a></li><li id="395b" class="nd ne fq nf b go pt nh ni gr pu nk nl nm pv no np nq pw ns nt nu px nw nx ny pq pr ps bk"><a class="af nc" href="https://arxiv.org/pdf/2312.06886" rel="noopener ugc nofollow" target="_blank">Relightful Harmonization</a></li><li id="6699" class="nd ne fq nf b go pt nh ni gr pu nk nl nm pv no np nq pw ns nt nu px nw nx ny pq pr ps bk"><a class="af nc" href="https://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/" rel="noopener ugc nofollow" target="_blank">A Diffusion Approach to Radiance Field Relighting using Multi-Illumination Synthesis</a></li><li id="2c76" class="nd ne fq nf b go pt nh ni gr pu nk nl nm pv no np nq pw ns nt nu px nw nx ny pq pr ps bk"><a class="af nc" href="https://theaisummer.com/diffusion-models/" rel="noopener ugc nofollow" target="_blank">How diffusion models work: the math from scratch | AI Summer</a></li><li id="4292" class="nd ne fq nf b go pt nh ni gr pu nk nl nm pv no np nq pw ns nt nu px nw nx ny pq pr ps bk"><a class="af nc" href="https://arxiv.org/pdf/2403.18103" rel="noopener ugc nofollow" target="_blank">Tutorial on Diffusion Models for Imaging and Vision</a></li><li id="603f" class="nd ne fq nf b go pt nh ni gr pu nk nl nm pv no np nq pw ns nt nu px nw nx ny pq pr ps bk"><a class="af nc" href="https://www.youtube.com/watch?v=a4Yfz2FxXiY" rel="noopener ugc nofollow" target="_blank">Diffusion models from scratch in PyTorch</a></li><li id="c035" class="nd ne fq nf b go pt nh ni gr pu nk nl nm pv no np nq pw ns nt nu px nw nx ny pq pr ps bk"><a class="af nc" href="https://www.youtube.com/watch?v=S_il77Ttrmg" rel="noopener ugc nofollow" target="_blank">Diffusion Models — Live Coding Tutorial</a></li><li id="ec4e" class="nd ne fq nf b go pt nh ni gr pu nk nl nm pv no np nq pw ns nt nu px nw nx ny pq pr ps bk"><a class="af nc" href="https://www.youtube.com/watch?v=HoKDTa5jHvg" rel="noopener ugc nofollow" target="_blank">Diffusion Models | Paper Explanation | Math Explained</a></li><li id="19ca" class="nd ne fq nf b go pt nh ni gr pu nk nl nm pv no np nq pw ns nt nu px nw nx ny pq pr ps bk"><a class="af nc" href="https://www.youtube.com/watch?v=i2qSxMVeVLI" rel="noopener ugc nofollow" target="_blank">How I Understand Diffusion Models</a> by Prof Jia Bin Huang</li><li id="4147" class="nd ne fq nf b go pt nh ni gr pu nk nl nm pv no np nq pw ns nt nu px nw nx ny pq pr ps bk"><a class="af nc" href="https://www.youtube.com/watch?v=H45lF4sUgiE" rel="noopener ugc nofollow" target="_blank">Denoising Diffusion Probabilistic Models | DDPM Explained</a> Good intuition of math of diffusion models</li></ol></div></div></div></div>    
</body>
</html>