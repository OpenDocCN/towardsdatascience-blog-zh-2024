- en: Can Generative AI Lead to AI Collapse?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/can-generative-ai-lead-to-ai-collapse-481966259d23?source=collection_archive---------6-----------------------#2024-07-30](https://towardsdatascience.com/can-generative-ai-lead-to-ai-collapse-481966259d23?source=collection_archive---------6-----------------------#2024-07-30)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '|LLM|GENERATIVE AI|MODEL COLLAPSE|'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AI eating its own tail: the risk of model collapse in generative systems'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://salvatore-raieli.medium.com/?source=post_page---byline--481966259d23--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page---byline--481966259d23--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--481966259d23--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--481966259d23--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page---byline--481966259d23--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--481966259d23--------------------------------)
    ·9 min read·Jul 30, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b8b486423beca38a2f4ae4b4a4db6a7c.png)'
  prefs: []
  type: TYPE_IMG
- en: image created by the author using AI
  prefs: []
  type: TYPE_NORMAL
- en: “Civilizations die from suicide, not by murder.” — Arnold Toynbee
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Large language models (LLMs)](https://github.com/SalvatoreRa/tutorial/blob/main/artificial%20intelligence/FAQ.md#:~:text=What%20is%20a%20Large%20Language%20Model%20(LLM)%3F)
    are generally trained in an unsupervised manner on a huge amount of text. This
    text is obtained by crawling the Internet. This text was written by humans, however,
    that may soon not be the case.'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/a-requiem-for-the-transformer-297e6f14e189?source=post_page-----481966259d23--------------------------------)
    [## A Requiem for the Transformer?'
  prefs: []
  type: TYPE_NORMAL
- en: Will be the transformer the model leading us to artificial general intelligence?
    Or will be replaced?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/a-requiem-for-the-transformer-297e6f14e189?source=post_page-----481966259d23--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are data-hungry by definition, and the datasets used are getting bigger
    and bigger. According to the [scaling law](https://github.com/SalvatoreRa/tutorial/blob/main/artificial%20intelligence/FAQ.md#:~:text=What%20does%20it%20mean%20emergent%20properties%3F%20what%20it%20is%20the%20scaling%20law%3F)
    [2] to improve performance one must increase both the number of parameters and
    the number of training tokens (with the latter considered the most important factor).
  prefs: []
  type: TYPE_NORMAL
- en: These datasets contain data produced by humans, however, some studies show that
    this is a limited resource. Humans also do not produce data at the same scale
    as we do, as we are increasing consumption [through LLM training](https://venturebeat.com/ai/ais-hunger-games-a-lucrative-data-market-is-exploding-to-feed-insatiable-llms-the-ai-beat/).
    One study…
  prefs: []
  type: TYPE_NORMAL
