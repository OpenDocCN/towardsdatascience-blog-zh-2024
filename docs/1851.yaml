- en: Can Generative AI Lead to AI Collapse?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成性AI能引发AI崩溃吗？
- en: 原文：[https://towardsdatascience.com/can-generative-ai-lead-to-ai-collapse-481966259d23?source=collection_archive---------6-----------------------#2024-07-30](https://towardsdatascience.com/can-generative-ai-lead-to-ai-collapse-481966259d23?source=collection_archive---------6-----------------------#2024-07-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/can-generative-ai-lead-to-ai-collapse-481966259d23?source=collection_archive---------6-----------------------#2024-07-30](https://towardsdatascience.com/can-generative-ai-lead-to-ai-collapse-481966259d23?source=collection_archive---------6-----------------------#2024-07-30)
- en: '|LLM|GENERATIVE AI|MODEL COLLAPSE|'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '|LLM|生成性AI|模型崩溃|'
- en: 'AI eating its own tail: the risk of model collapse in generative systems'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AI吞噬自己的尾巴：生成系统中的模型崩溃风险
- en: '[](https://salvatore-raieli.medium.com/?source=post_page---byline--481966259d23--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page---byline--481966259d23--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--481966259d23--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--481966259d23--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page---byline--481966259d23--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://salvatore-raieli.medium.com/?source=post_page---byline--481966259d23--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page---byline--481966259d23--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--481966259d23--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--481966259d23--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page---byline--481966259d23--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--481966259d23--------------------------------)
    ·9 min read·Jul 30, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--481966259d23--------------------------------)
    ·阅读时长9分钟·2024年7月30日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/b8b486423beca38a2f4ae4b4a4db6a7c.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b8b486423beca38a2f4ae4b4a4db6a7c.png)'
- en: image created by the author using AI
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图片由作者使用AI生成
- en: “Civilizations die from suicide, not by murder.” — Arnold Toynbee
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “文明的灭亡来自自杀，而非谋杀。” — 阿诺德·汤因比
- en: '[Large language models (LLMs)](https://github.com/SalvatoreRa/tutorial/blob/main/artificial%20intelligence/FAQ.md#:~:text=What%20is%20a%20Large%20Language%20Model%20(LLM)%3F)
    are generally trained in an unsupervised manner on a huge amount of text. This
    text is obtained by crawling the Internet. This text was written by humans, however,
    that may soon not be the case.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[大规模语言模型（LLMs）](https://github.com/SalvatoreRa/tutorial/blob/main/artificial%20intelligence/FAQ.md#:~:text=What%20is%20a%20Large%20Language%20Model%20(LLM)%3F)通常是在大量文本数据上以无监督方式进行训练的。这些文本通过爬取互联网获得。虽然这些文本最初是由人类编写的，但这种情况很快可能会发生变化。'
- en: '[](/a-requiem-for-the-transformer-297e6f14e189?source=post_page-----481966259d23--------------------------------)
    [## A Requiem for the Transformer?'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/a-requiem-for-the-transformer-297e6f14e189?source=post_page-----481966259d23--------------------------------)
    [## 《变换器的安魂曲》？'
- en: Will be the transformer the model leading us to artificial general intelligence?
    Or will be replaced?
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 变换器模型是否将引领我们走向人工通用智能？还是会被替代？
- en: towardsdatascience.com](/a-requiem-for-the-transformer-297e6f14e189?source=post_page-----481966259d23--------------------------------)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/a-requiem-for-the-transformer-297e6f14e189?source=post_page-----481966259d23--------------------------------)
- en: LLMs are data-hungry by definition, and the datasets used are getting bigger
    and bigger. According to the [scaling law](https://github.com/SalvatoreRa/tutorial/blob/main/artificial%20intelligence/FAQ.md#:~:text=What%20does%20it%20mean%20emergent%20properties%3F%20what%20it%20is%20the%20scaling%20law%3F)
    [2] to improve performance one must increase both the number of parameters and
    the number of training tokens (with the latter considered the most important factor).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs本质上是数据饥渴型的，而且用于训练的数据集越来越大。根据[扩展法则](https://github.com/SalvatoreRa/tutorial/blob/main/artificial%20intelligence/FAQ.md#:~:text=What%20does%20it%20mean%20emergent%20properties%3F%20what%20it%20is%20the%20scaling%20law%3F)
    [2]，为了提高性能，必须增加模型的参数数量和训练标记的数量（后者被认为是最重要的因素）。
- en: These datasets contain data produced by humans, however, some studies show that
    this is a limited resource. Humans also do not produce data at the same scale
    as we do, as we are increasing consumption [through LLM training](https://venturebeat.com/ai/ais-hunger-games-a-lucrative-data-market-is-exploding-to-feed-insatiable-llms-the-ai-beat/).
    One study…
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据集包含由人类生成的数据，然而，一些研究表明这是一种有限的资源。人类生成的数据量也不像我们这样庞大，因为我们正在通过[LLM训练](https://venturebeat.com/ai/ais-hunger-games-a-lucrative-data-market-is-exploding-to-feed-insatiable-llms-the-ai-beat/)增加数据消耗。一项研究…
