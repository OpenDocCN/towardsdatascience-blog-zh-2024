<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Is Less More? Do Deep Learning Forecasting Models Need Feature Reduction?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Is Less More? Do Deep Learning Forecasting Models Need Feature Reduction?</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/is-less-more-do-deep-learning-forecasting-models-need-feature-reduction-25d8968ac15c?source=collection_archive---------7-----------------------#2024-09-30">https://towardsdatascience.com/is-less-more-do-deep-learning-forecasting-models-need-feature-reduction-25d8968ac15c?source=collection_archive---------7-----------------------#2024-09-30</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="695a" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">To curate, or not to curate, that is the question</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://ostiguyphilippe.medium.com/?source=post_page---byline--25d8968ac15c--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Philippe Ostiguy, M. Sc." class="l ep by dd de cx" src="../Images/8b292bc1baa848a0c5de821dc9576534.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*0PKMcQqbfyqyiAsmEfDcAg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--25d8968ac15c--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://ostiguyphilippe.medium.com/?source=post_page---byline--25d8968ac15c--------------------------------" rel="noopener follow">Philippe Ostiguy, M. Sc.</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--25d8968ac15c--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 30, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/d06437fd932d3128b235a26000f217c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0y0-x4_SfqpNH3A7aX50GA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">AI image created on MidJourney V6.1 by the author.</figcaption></figure><p id="fe62" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Time series forecasting is a powerful tool in data science, offering insights into future trends based on historical patterns. In our previous article, we explored <a class="af nx" href="https://levelup.gitconnected.com/want-to-decrease-your-models-prediction-errors-by-20-follow-this-simple-trick-97354102098e" rel="noopener ugc nofollow" target="_blank">how making your time series stationary automatically</a> can significantly enhance model performance. But stationarity is just one piece of the puzzle. As we continue to refine our forecasting models, another crucial question arises: how do we handle the multitude of features our data may present?</p><p id="2b27" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">As you work with time series data, you’ll often find yourself with many potential features to include in your model. While it’s tempting to use all available data, adding more features isn’t always better. It can make your model more complex and slower to train, without necessarily improving its performance.</p><p id="fd5e" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">You might be wondering: is it important to simplify the feature set and what are the techniques available out there? That’s exactly what we’ll discuss in this article.</p><p id="fe24" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Here’s a quick summary of what will be covered:</p><ul class=""><li id="5bbf" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw ny nz oa bk"><strong class="nd fr">Feature reduction in time series</strong> — <em class="ob">We’ll explain the concept of feature reduction in time series analysis and why it matters.</em></li><li id="e575" class="nb nc fq nd b go oc nf ng gr od ni nj nk oe nm nn no of nq nr ns og nu nv nw ny nz oa bk"><strong class="nd fr">Practical implementation guide </strong>— <em class="ob">Using Python, we’ll walk through evaluating and selecting features for your time series model, providing hands-on tools to optimize your approach. We’ll also assess whether trimming down features is necessary for our forecasting models.</em></li></ul><p id="2df3" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Once you’re familiar with techniques like stationarity and feature reduction as explained in this article, and you’re looking to elevate your models even further? Check out this article on <a class="af nx" href="https://medium.com/@ostiguyphilippe/enhancing-deep-learning-model-evaluation-for-stock-market-forecasting-bea30b905b80" rel="noopener">using a custom validation loss</a> in your deep learning model to get better stock forecasts — it’s a great next step!</p><h1 id="a83f" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Feature reduction in time series : a simple explanation</h1><p id="4aea" class="pw-post-body-paragraph nb nc fq nd b go pd nf ng gr pe ni nj nk pf nm nn no pg nq nr ns ph nu nv nw fj bk">Feature reduction is like cleaning up your workspace to make it easier to find what you need. In time series analysis, it means cutting down the number of input variables (features) that your model uses to make predictions. The goal is to simplify the model while retaining its predictive power. This is important because too many and correlated features can make your model complicated, slow, and less accurate.</p><p id="7cf1" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Specifically, simplifying the feature set can :</p><ul class=""><li id="fd7e" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw ny nz oa bk"><strong class="nd fr">Reduced complexity: </strong>Fewer features means a simpler model, which is often faster to train and use.</li><li id="8e10" class="nb nc fq nd b go oc nf ng gr od ni nj nk oe nm nn no of nq nr ns og nu nv nw ny nz oa bk"><strong class="nd fr">Improved generalization</strong>: By removing noise, eliminating correlated features and focusing on key information, it helps the model to learn the true underlying patterns rather than memorizing redundant information. This enhances the model’s ability to generalize its predictions to different datasets.</li><li id="7c70" class="nb nc fq nd b go oc nf ng gr od ni nj nk oe nm nn no of nq nr ns og nu nv nw ny nz oa bk"><strong class="nd fr">Easier interpretation</strong>: A model with fewer features is often easier for humans to understand and explain.</li><li id="0f60" class="nb nc fq nd b go oc nf ng gr od ni nj nk oe nm nn no of nq nr ns og nu nv nw ny nz oa bk"><strong class="nd fr">Computational efficiency</strong>: Fewer features requires less memory and processing power, which can be crucial for large datasets or real-time applications.</li></ul><p id="9480" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">It’s also important to note that most time series packages in Python for forecasting don’t perform feature reduction automatically. This is a step you typically need to handle on your own before using these packages.</p><p id="f704" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">To better understand these concepts, let’s walk through a practical example using real-world daily data from the Federal Reserve Economic Data (FRED) database. We’ll skip the data retrieval process here, as we’ve already covered how to get free and reliable data from the FRED API in a <a class="af nx" href="https://levelup.gitconnected.com/get-free-and-reliable-financial-market-data-machine-learning-ready-246e59b00cea" rel="noopener ugc nofollow" target="_blank">previous article</a>. You can get the data we’ll be using <a class="af nx" href="https://github.com/philippe-ostiguy/free-fin-data" rel="noopener ugc nofollow" target="_blank">this script</a>. Once you’ve fetched the data :</p><ul class=""><li id="a85a" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw ny nz oa bk">Create a<code class="cx pi pj pk pl b">data</code> directory in your current directory</li></ul><pre class="ml mm mn mo mp pm pl pn bp po bb bk"><span id="8934" class="pp oi fq pl b bg pq pr l ps pt">mkdir -p /path/to/current_directory/data</span></pre><ul class=""><li id="0915" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw ny nz oa bk">Copy the data in your directory</li></ul><pre class="ml mm mn mo mp pm pl pn bp po bb bk"><span id="8686" class="pp oi fq pl b bg pq pr l ps pt">cp -R /path/to/fetcher_directory /path/to/current_directory/data</span></pre><p id="b914" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Now that we have our data, let’s dive into our feature reduction example.</p><p id="f53f" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">We’ve previously demonstrated how to clean the daily data fetched from the FRED API in <a class="af nx" href="https://levelup.gitconnected.com/get-free-and-reliable-financial-market-data-machine-learning-ready-246e59b00cea" rel="noopener ugc nofollow" target="_blank">another article</a>, so we’ll skip that process here and use the <code class="cx pi pj pk pl b">processed_dataframes</code>(list of dataframes) that resulted from the steps outlined in that article.</p><pre class="ml mm mn mo mp pm pl pn bp po bb bk"><span id="0d44" class="pp oi fq pl b bg pq pr l ps pt">import pandas as pd<br/>import os<br/>import warnings<br/>warnings.filterwarnings("ignore")<br/><br/>def is_sp500(df):<br/>    last_date = df['ds'].max()<br/>    last_value = float(df.loc[df['ds'] == last_date, 'value'].iloc[0])<br/>    return 5400 &lt;= last_value &lt;= 5650<br/><br/>initial_model_train = None<br/>for i, df in enumerate(processed_dataframes):<br/>    if df['value'].isna().any():<br/>        continue<br/>    if is_sp500(df):<br/>        initial_model_train = df<br/>        break<br/><br/>TRAIN_SIZE = .90<br/>START_DATE = '2018-10-01'<br/>END_DATE = '2024-09-05'<br/>initial_model_train = initial_model_train.rename(columns={'value': 'price'}).reset_index(drop=True)<br/>initial_model_train['unique_id'] = 'SPY'<br/>initial_model_train['price'] = initial_model_train['price'].astype(float)<br/>initial_model_train['y'] = initial_model_train['price'].pct_change()<br/><br/>initial_model_train = initial_model_train[initial_model_train['ds'] &gt; START_DATE].reset_index(drop=True)<br/>combined_df_all = pd.concat([df.drop(columns=['ds']) for df in processed_dataframes], axis=1)<br/>combined_df_all.columns = [f'value_{i}' for i in range(len(processed_dataframes))]<br/>rows_to_keep = len(initial_model_train)<br/>combined_df_all = combined_df_all.iloc[-rows_to_keep:].reset_index(drop=True)<br/><br/>train_size = int(len(initial_model_train)*TRAIN_SIZE)<br/>initial_model_test = initial_model_train[train_size:]<br/>initial_model_train = initial_model_train[:train_size]<br/>combined_df_test = combined_df_all[train_size:]<br/>combined_df_train = combined_df_all[:train_size]</span></pre><p id="dee5" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">You might be wondering why we’ve divided our data into training and testing sets? The reason is to ensure that there is no data leakage before applying any transformation or reduction technique.</p><p id="03a9" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><code class="cx pi pj pk pl b">initial_model_data</code> contains the S&amp;P 500 daily prices (stored originally <code class="cx pi pj pk pl b">processed_dataframes</code>), which will be the data we’ll be trying to forecast.</p><p id="cbde" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Then, we need to ensure our data is stationary. For a detailed explanation on how to automatically make your data stationary and improve your model by 20%, refer to <a class="af nx" href="https://levelup.gitconnected.com/want-to-decrease-your-models-prediction-errors-by-20-follow-this-simple-trick-97354102098e" rel="noopener ugc nofollow" target="_blank">this article</a>.</p><pre class="ml mm mn mo mp pm pl pn bp po bb bk"><span id="b2c7" class="pp oi fq pl b bg pq pr l ps pt">import numpy as np<br/>from statsmodels.tsa.stattools import adfuller<br/>P_VALUE = 0.05<br/><br/>def replace_inf_nan(series):<br/>    if np.isnan(series.iloc[0]) or np.isinf(series.iloc[0]):<br/>        series.iloc[0] = 0<br/>    mask = np.isinf(series) | np.isnan(series)<br/>    series = series.copy()<br/>    series[mask] = np.nan<br/>    series = series.ffill()<br/>    return series<br/><br/>def safe_convert_to_numeric(series):<br/>    return pd.to_numeric(series, errors='coerce')<br/><br/>tempo_df = pd.DataFrame()<br/>stationary_df_train = pd.DataFrame()<br/>stationary_df_test = pd.DataFrame()<br/><br/>value_columns = [col for col in combined_df_all.columns if col.startswith('value_')]<br/><br/>transformations = ['first_diff', 'pct_change', 'log', 'identity']<br/><br/>def get_first_diff(numeric_series):<br/>    return replace_inf_nan(numeric_series.diff())<br/><br/>def get_pct_change(numeric_series):<br/>    return replace_inf_nan(numeric_series.pct_change())<br/><br/>def get_log_transform(numeric_series):<br/>    return replace_inf_nan(np.log(numeric_series.replace(0, np.nan)))<br/><br/>def get_identity(numeric_series):<br/>    return numeric_series<br/><br/>for index, val_col in enumerate(value_columns):<br/>    numeric_series = safe_convert_to_numeric(combined_df_train[val_col])<br/>    numeric_series_all = safe_convert_to_numeric(combined_df_all[val_col])<br/><br/>    if numeric_series.isna().all():<br/>        continue<br/><br/>    valid_transformations = []<br/><br/>    tempo_df['first_diff'] = get_first_diff(numeric_series)<br/>    tempo_df['pct_change'] = get_pct_change(numeric_series)<br/>    tempo_df['log'] = get_log_transform(numeric_series)<br/>    tempo_df['identity'] = get_identity(numeric_series)<br/><br/>    for transfo in transformations:<br/>        tempo_df[transfo] = replace_inf_nan(tempo_df[transfo])<br/>        series = tempo_df[transfo].dropna()<br/><br/>        if len(series) &gt; 1 and not (series == series.iloc[0]).all():<br/>            result = adfuller(series)<br/>            if result[1] &lt; P_VALUE:<br/>                valid_transformations.append((transfo, result[0], result[1]))<br/><br/>    if valid_transformations:<br/>        if any(transfo == 'identity' for transfo, _, _ in valid_transformations):<br/>            chosen_transfo = 'identity'<br/>        else:<br/>            chosen_transfo = min(valid_transformations, key=lambda x: x[1])[0]<br/><br/>        if chosen_transfo == 'first_diff':<br/>            stationary_df_train[val_col] = get_first_diff(numeric_series_all)<br/>        elif chosen_transfo == 'pct_change':<br/>            stationary_df_train[val_col] = get_pct_change(numeric_series_all)<br/>        elif chosen_transfo == 'log':<br/>            stationary_df_train[val_col] = get_log_transform(numeric_series_all)<br/>        else:<br/>            stationary_df_train[val_col] = get_identity(numeric_series_all)<br/><br/>    else:<br/>        print(f"No valid transformation found for {val_col}")<br/><br/><br/>stationary_df_test = stationary_df_train[train_size:]<br/>stationary_df_train = stationary_df_train[:train_size]<br/><br/>initial_model_train = initial_model_train.iloc[1:].reset_index(drop=True)<br/>stationary_df_train = stationary_df_train.iloc[1:].reset_index(drop=True)<br/>last_train_index = stationary_df_train.index[-1]<br/>stationary_df_test = stationary_df_test.loc[last_train_index + 1:].reset_index(drop=True)<br/>initial_model_test = initial_model_test.loc[last_train_index + 1:].reset_index(drop=True)</span></pre><p id="432e" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Then, we will count the number of variables that have at least a 95% correlation coefficient with another variable.</p><pre class="ml mm mn mo mp pm pl pn bp po bb bk"><span id="c67f" class="pp oi fq pl b bg pq pr l ps pt">CORR_COFF = .95<br/>corr_matrix = stationary_df_train.corr().abs()<br/>mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)<br/>high_corr = corr_matrix.where(mask).stack()<br/>high_corr = high_corr[high_corr &gt;= CORR_COFF]<br/>unique_cols = set(high_corr.index.get_level_values(0)) | set(high_corr.index.get_level_values(1))<br/>num_high_corr_cols = len(unique_cols)<br/><br/>print(f"\n{num_high_corr_cols}/{stationary_df_train.shape[1]} variables have ≥{int(CORR_COFF*100)}% "<br/>      f"correlation with another variable.\n")</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pu"><img src="../Images/3e88d562d5f155546e490822fecc2ca8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*gUIfWVGxIncUVED3TUJkFw.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Number of variables highly correlated. Image by the author.</figcaption></figure><p id="739c" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Having 260 out of 438 variables that have a correlation of 95% or more with at least another variable can be an issue. It indicates significant multicollinearity in the dataset. This redundancy can lead to several issues:</p><ul class=""><li id="50ae" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw ny nz oa bk">It complicates the model without adding substantial new information</li><li id="f489" class="nb nc fq nd b go oc nf ng gr od ni nj nk oe nm nn no of nq nr ns og nu nv nw ny nz oa bk">Potentially causes instability in coefficient estimates</li><li id="ecdf" class="nb nc fq nd b go oc nf ng gr od ni nj nk oe nm nn no of nq nr ns og nu nv nw ny nz oa bk">Increases the risk of overfitting</li><li id="2cfa" class="nb nc fq nd b go oc nf ng gr od ni nj nk oe nm nn no of nq nr ns og nu nv nw ny nz oa bk">Makes interpretation of individual variable impacts challenging</li></ul><h1 id="e9f4" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Feature evaluation and selection</h1><p id="25a4" class="pw-post-body-paragraph nb nc fq nd b go pd nf ng gr pe ni nj nk pf nm nn no pg nq nr ns ph nu nv nw fj bk">We understand that feature reduction can be important, but how do we perform it? Which techniques should we use? These are the questions we’ll explore now.</p><p id="9677" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The first technique we’ll examine is Principal Component Analysis (PCA). It’s a common and an effective dimensionality reduction technique. PCA identifies linear relationships between features and retains the principal components that explain a predetermined percentage of the variance in the original dataset. In our use case, we’ve set the <code class="cx pi pj pk pl b">EXPLAINED_VARIANCE</code> threshold to 90%.</p><pre class="ml mm mn mo mp pm pl pn bp po bb bk"><span id="579f" class="pp oi fq pl b bg pq pr l ps pt">from sklearn.preprocessing import StandardScaler<br/>from sklearn.decomposition import PCA<br/>EXPLAINED_VARIANCE = .9<br/>MIN_VARIANCE = 1e-10<br/><br/>X_train = stationary_df_train.values<br/>scaler = StandardScaler()<br/>X_train_scaled = scaler.fit_transform(X_train)<br/>pca = PCA(n_components=EXPLAINED_VARIANCE, svd_solver='full')<br/>X_train_pca = pca.fit_transform(X_train_scaled)<br/><br/>components_to_keep = pca.explained_variance_ &gt; MIN_VARIANCE<br/>X_train_pca = X_train_pca[:, components_to_keep]<br/><br/>pca_df_train = pd.DataFrame(<br/>    X_train_pca,<br/>    columns=[f'PC{i+1}' for i in range(X_train_pca.shape[1])]<br/>)<br/><br/>X_test = stationary_df_test.values<br/>X_test_scaled = scaler.transform(X_test)<br/>X_test_pca = pca.transform(X_test_scaled)<br/><br/>X_test_pca = X_test_pca[:, components_to_keep]<br/><br/>pca_df_test = pd.DataFrame(<br/>    X_test_pca,<br/>    columns=[f'PC{i+1}' for i in range(X_test_pca.shape[1])]<br/>)<br/><br/>print(f"\nOriginal number of features: {stationary_df_train.shape[1]}")<br/>print(f"Number of components after PCA: {pca_df_train.shape[1]}\n")</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pv"><img src="../Images/5544416f6efe78886454deea283a782d.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*3Xu2lRwIHF147OIScX-LRg.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Feature reduction using Principal Component Analysis. Image by the author.</figcaption></figure><p id="8717" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">It’s an impressive: only 76 components out of 438 features remaining after the reduction while keeping 90% of the variance explained! Now let’s move to a non-linear reduction technique.</p></div></div></div><div class="ab cb pw px py pz" role="separator"><span class="qa by bm qb qc qd"/><span class="qa by bm qb qc qd"/><span class="qa by bm qb qc"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="c4b0" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The <a class="af nx" href="https://arxiv.org/pdf/1912.09363.pdf" rel="noopener ugc nofollow" target="_blank">Temporal Fusion Transformers (TFT)</a> is an advanced model for time series forecasting. It includes the Variable Selection Network (VSN), which is a key component of the model. It’s specifically designed to automatically identify and focus on the most relevant features within a dataset. It achieves this by assigning learned weights to each input variable, effectively highlighting which features contribute most to the predictive task.</p><p id="54ce" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">This VSN-based approach will be our second reduction technique. We’ll implement it using <a class="af nx" href="https://pytorch-forecasting.readthedocs.io/en/stable/" rel="noopener ugc nofollow" target="_blank">PyTorch Forecasting</a>, which allows us to leverage the Variable Selection Network from the TFT model.</p><p id="2a38" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">We’ll use a basic configuration. Our goal isn’t to create the highest-performing model possible, but rather to identify the most relevant features while using minimal resources.</p><pre class="ml mm mn mo mp pm pl pn bp po bb bk"><span id="a9fe" class="pp oi fq pl b bg pq pr l ps pt">from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet<br/>from pytorch_forecasting.metrics import QuantileLoss<br/>from lightning.pytorch.callbacks import EarlyStopping<br/>import lightning.pytorch as pl<br/>import torch<br/><br/>pl.seed_everything(42)<br/>max_encoder_length = 32<br/>max_prediction_length = 1<br/>VAL_SIZE = .2<br/>VARIABLES_IMPORTANCE = .8<br/>model_data_feature_sel = initial_model_train.join(stationary_df_train)<br/>model_data_feature_sel = model_data_feature_sel.join(pca_df_train)<br/>model_data_feature_sel['price'] = model_data_feature_sel['price'].astype(float)<br/>model_data_feature_sel['y'] = model_data_feature_sel['price'].pct_change()<br/>model_data_feature_sel = model_data_feature_sel.iloc[1:].reset_index(drop=True)<br/><br/>model_data_feature_sel['group'] = 'spy'<br/>model_data_feature_sel['time_idx'] = range(len(model_data_feature_sel))<br/><br/>train_size_vsn = int((1-VAL_SIZE)*len(model_data_feature_sel))<br/>train_data_feature = model_data_feature_sel[:train_size_vsn]<br/>val_data_feature = model_data_feature_sel[train_size_vsn:]<br/>unknown_reals_origin = [col for col in model_data_feature_sel.columns if col.startswith('value_')] + ['y']<br/><br/>timeseries_config = {<br/>    "time_idx": "time_idx",<br/>    "target": "y",<br/>    "group_ids": ["group"],<br/>    "max_encoder_length": max_encoder_length,<br/>    "max_prediction_length": max_prediction_length,<br/>    "time_varying_unknown_reals": unknown_reals_origin,<br/>    "add_relative_time_idx": True,<br/>    "add_target_scales": True,<br/>    "add_encoder_length": True<br/>}<br/><br/><br/>training_ts = TimeSeriesDataSet(<br/>    train_data_feature,<br/>    **timeseries_config<br/>)</span></pre><p id="291d" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The <code class="cx pi pj pk pl b">VARIABLES_IMPORTANCE</code> threshold is set to 0.8, which means we'll retain features in the top 80th percentile of importance as determined by the Variable Selection Network (VSN). For more information about the Temporal Fusion Transformers (TFT) and its parameters, please refer to the <a class="af nx" href="https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.models.temporal_fusion_transformer.TemporalFusionTransformer.html#pytorch_forecasting.models.temporal_fusion_transformer.TemporalFusionTransformer" rel="noopener ugc nofollow" target="_blank">documentation</a>.</p><p id="c69f" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Next, we’ll train the TFT model.</p><pre class="ml mm mn mo mp pm pl pn bp po bb bk"><span id="aecd" class="pp oi fq pl b bg pq pr l ps pt">if torch.cuda.is_available():<br/>    accelerator = 'gpu'<br/>    num_workers = 2<br/>else :<br/>    accelerator = 'auto'<br/>    num_workers = 0<br/><br/>validation = TimeSeriesDataSet.from_dataset(training_ts, val_data_feature, predict=True, stop_randomization=True)<br/>train_dataloader = training_ts.to_dataloader(train=True, batch_size=64, num_workers=num_workers)<br/>val_dataloader = validation.to_dataloader(train=False, batch_size=64*5, num_workers=num_workers)<br/><br/>tft = TemporalFusionTransformer.from_dataset(<br/>    training_ts,<br/>    learning_rate=0.03,<br/>    hidden_size=16,<br/>    attention_head_size=2,<br/>    dropout=0.1,<br/>    loss=QuantileLoss()<br/>)<br/><br/>early_stop_callback = EarlyStopping(monitor="val_loss", min_delta=1e-5, patience=5, verbose=False, mode="min")<br/><br/>trainer = pl.Trainer(max_epochs=20,  accelerator=accelerator, gradient_clip_val=.5, callbacks=[early_stop_callback])<br/>trainer.fit(<br/>    tft,<br/>    train_dataloaders=train_dataloader,<br/>    val_dataloaders=val_dataloader<br/><br/>)</span></pre><p id="701f" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">We intentionally set <code class="cx pi pj pk pl b">max_epochs=20</code> so the model doesn’t train too long. Additionally, we implemented an <code class="cx pi pj pk pl b">early_stop_callback</code> that halts training if the model shows no improvement for 5 consecutive epochs (<code class="cx pi pj pk pl b">patience=5</code>).</p><p id="9b10" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Finally, using the best model obtained, we select the 80th percentile of the most important features as determined by the VSN.</p><pre class="ml mm mn mo mp pm pl pn bp po bb bk"><span id="b53f" class="pp oi fq pl b bg pq pr l ps pt">best_model_path = trainer.checkpoint_callback.best_model_path<br/>best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)<br/><br/>raw_predictions = best_tft.predict(val_dataloader, mode="raw", return_x=True)<br/><br/><br/>def get_top_encoder_variables(best_tft,interpretation):<br/>    encoder_importances = interpretation["encoder_variables"]<br/>    sorted_importances, indices = torch.sort(encoder_importances, descending=True)<br/>    cumulative_importances = torch.cumsum(sorted_importances, dim=0)<br/>    threshold_index = torch.where(cumulative_importances &gt; VARIABLES_IMPORTANCE)[0][0]<br/>    top_variables = [best_tft.encoder_variables[i] for i in indices[:threshold_index+1]]<br/>    if 'relative_time_idx' in top_variables:<br/>        top_variables.remove('relative_time_idx')<br/>    return top_variables<br/><br/>interpretation= best_tft.interpret_output(raw_predictions.output, reduction="sum")<br/>top_encoder_vars = get_top_encoder_variables(best_tft,interpretation)<br/><br/>print(f"\nOriginal number of features: {stationary_df_train.shape[1]}")<br/>print(f"Number of features after Variable Selection Network (VSN): {len(top_encoder_vars)}\n")</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qe"><img src="../Images/752a7a4f43f1726c1de6b2236a68e5c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*KaWlCudPQLZoxLuY3JPOUQ.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Feature reduction using Variable Selection Network. Image by the author.</figcaption></figure><p id="7ee1" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The original dataset contained 438 features, which were then reduced to 1 feature only after applying the VSN method! This drastic reduction suggests several possibilities:</p><ol class=""><li id="3911" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw qf nz oa bk">Many of the original features may have been redundant.</li><li id="b855" class="nb nc fq nd b go oc nf ng gr od ni nj nk oe nm nn no of nq nr ns og nu nv nw qf nz oa bk">The feature selection process may have oversimplified the data.</li><li id="9b87" class="nb nc fq nd b go oc nf ng gr od ni nj nk oe nm nn no of nq nr ns og nu nv nw qf nz oa bk">Using only the target variable’s historical values (autoregressive approach) might perform as well as, or possibly better than, models incorporating exogenous variables.</li></ol><h1 id="f82e" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Evaluating feature reduction techniques</h1><p id="9fd8" class="pw-post-body-paragraph nb nc fq nd b go pd nf ng gr pe ni nj nk pf nm nn no pg nq nr ns ph nu nv nw fj bk">In this final section, we compare out reduction techniques applied to our model. Each method is tested while maintaining identical model configurations, varying only the features subjected to reduction.</p><p id="d0fa" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">We’ll use <a class="af nx" href="https://arxiv.org/pdf/2304.08424.pdf" rel="noopener ugc nofollow" target="_blank">TiDE</a>, a small state-of-the-art Transformer-based model. We’ll use the implementation provided by <a class="af nx" href="https://nixtlaverse.nixtla.io/neuralforecast/models.tide.html" rel="noopener ugc nofollow" target="_blank">NeuralForecast</a>. Any model from NeuralForecast <a class="af nx" href="https://nixtlaverse.nixtla.io/neuralforecast/docs/capabilities/overview.html" rel="noopener ugc nofollow" target="_blank">here</a> would work as long as it allows exogenous historical variables.</p><p id="aaaa" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">We’ll train and test two models using daily SPY (S&amp;P 500 ETF) data. Both models will have the same:</p><ol class=""><li id="fce4" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw qf nz oa bk">Train-test split ratio</li><li id="0cc3" class="nb nc fq nd b go oc nf ng gr od ni nj nk oe nm nn no of nq nr ns og nu nv nw qf nz oa bk">Hyperparameters</li><li id="0bd0" class="nb nc fq nd b go oc nf ng gr od ni nj nk oe nm nn no of nq nr ns og nu nv nw qf nz oa bk">Single time series (SPY)</li><li id="0329" class="nb nc fq nd b go oc nf ng gr od ni nj nk oe nm nn no of nq nr ns og nu nv nw qf nz oa bk">Forecasting horizon of 1 step ahead</li></ol><p id="328e" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The only difference between the models will be the feature reduction technique. That’s it!</p><ol class=""><li id="6d08" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw qf nz oa bk">First model: Original features (no feature reduction)</li><li id="b4cc" class="nb nc fq nd b go oc nf ng gr od ni nj nk oe nm nn no of nq nr ns og nu nv nw qf nz oa bk">Second model: Feature reduction using PCA</li><li id="bb32" class="nb nc fq nd b go oc nf ng gr od ni nj nk oe nm nn no of nq nr ns og nu nv nw qf nz oa bk">Third model: Feature reduction using VSN</li></ol><p id="512c" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">This setup allows us to isolate the impact of each feature reduction technique on model performance.</p><p id="5495" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">First we train the 3 models with the same configuration except for the features.</p><pre class="ml mm mn mo mp pm pl pn bp po bb bk"><span id="0370" class="pp oi fq pl b bg pq pr l ps pt">from neuralforecast.models import TiDE<br/>from neuralforecast import NeuralForecast<br/><br/>train_data = initial_model_train.join(stationary_df_train)<br/>train_data = train_data.join(pca_df_train)<br/>test_data = initial_model_test.join(stationary_df_test)<br/>test_data = test_data.join(pca_df_test)<br/><br/>hist_exog_list_origin = [col for col in train_data.columns if col.startswith('value_')] + ['y']<br/>hist_exog_list_pca = [col for col in train_data.columns if col.startswith('PC')] + ['y']<br/>hist_exog_list_vsn = top_encoder_vars<br/><br/><br/>tide_params = {<br/>    "h": 1,<br/>    "input_size": 32,<br/>    "scaler_type": "robust",<br/>    "max_steps": 500,<br/>    "val_check_steps": 20,<br/>    "early_stop_patience_steps": 5<br/>}<br/><br/>model_original = TiDE(<br/>    **tide_params,<br/>    hist_exog_list=hist_exog_list_origin,<br/>)<br/><br/><br/>model_pca = TiDE(<br/>    **tide_params,<br/>    hist_exog_list=hist_exog_list_pca,<br/>)<br/><br/>model_vsn = TiDE(<br/>    **tide_params,<br/>    hist_exog_list=hist_exog_list_vsn,<br/>)<br/><br/>nf = NeuralForecast(<br/>    models=[model_original, model_pca, model_vsn],<br/>    freq='D'<br/>)<br/><br/>val_size = int(train_size*VAL_SIZE)<br/>nf.fit(df=train_data,val_size=val_size,use_init_models=True)</span></pre><p id="c0d2" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Then, we make the predictions.</p><pre class="ml mm mn mo mp pm pl pn bp po bb bk"><span id="19e4" class="pp oi fq pl b bg pq pr l ps pt">from tabulate import tabulate<br/>y_hat_test_ret = pd.DataFrame()<br/>current_train_data = train_data.copy()<br/><br/>y_hat_ret = nf.predict(current_train_data)<br/>y_hat_test_ret = pd.concat([y_hat_test_ret, y_hat_ret.iloc[[-1]]])<br/><br/>for i in range(len(test_data) - 1):<br/>    combined_data = pd.concat([current_train_data, test_data.iloc[[i]]])<br/>    y_hat_ret = nf.predict(combined_data)<br/>    y_hat_test_ret = pd.concat([y_hat_test_ret, y_hat_ret.iloc[[-1]]])<br/>    current_train_data = combined_data<br/><br/>predicted_returns_original = y_hat_test_ret['TiDE'].values<br/>predicted_returns_pca = y_hat_test_ret['TiDE1'].values<br/>predicted_returns_vsn = y_hat_test_ret['TiDE2'].values<br/><br/>predicted_prices_original = []<br/>predicted_prices_pca = []<br/>predicted_prices_vsn = []<br/><br/>for i in range(len(predicted_returns_pca)):<br/>    if i == 0:<br/>        last_true_price = train_data['price'].iloc[-1]<br/>    else:<br/>        last_true_price = test_data['price'].iloc[i-1]<br/>    predicted_prices_original.append(last_true_price * (1 + predicted_returns_original[i]))<br/>    predicted_prices_pca.append(last_true_price * (1 + predicted_returns_pca[i]))<br/>    predicted_prices_vsn.append(last_true_price * (1 + predicted_returns_vsn[i]))<br/><br/>true_values = test_data['price']<br/>methods = ['Original','PCA', 'VSN']<br/>predicted_prices = [predicted_prices_original,predicted_prices_pca, predicted_prices_vsn]<br/><br/>results = []<br/><br/>for method, prices in zip(methods, predicted_prices):<br/>    mse = np.mean((np.array(prices) - true_values)**2)<br/>    rmse = np.sqrt(mse)<br/>    mae = np.mean(np.abs(np.array(prices) - true_values))<br/><br/>    results.append([method, mse, rmse, mae])<br/><br/>headers = ["Method", "MSE", "RMSE", "MAE"]<br/>table = tabulate(results, headers=headers, floatfmt=".4f", tablefmt="grid")<br/><br/>print("\nPrediction Errors Comparison:")<br/>print(table)<br/><br/>with open("prediction_errors_comparison.txt", "w") as f:<br/>    f.write("Prediction Errors Comparison:\n")<br/>    f.write(table)</span></pre><p id="0d2f" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">We forecast the daily returns using the model, then convert these back to prices. This approach allows us to calculate prediction errors using prices and compare the actual prices to the forecasted prices in a plot.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qg"><img src="../Images/269abd534c5abdfe8194df39136fd090.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*OSAX6ovPRF0tNzPjzd0VwA.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Comparison of prediction errors with different feature reduction techniques. Image by the author.</figcaption></figure><p id="3b95" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The similar performance of the TiDE model across both original and reduced feature sets reveals a crucial insight: feature reduction did not lead to improved predictions as one might expect. This suggests potential key issues:</p><ul class=""><li id="d81a" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw ny nz oa bk">Information loss: despite aiming to preserve essential data, dimensionality reduction techniques discarded information relevant to the prediction task, explaining the lack of improvement with fewer features.</li><li id="9789" class="nb nc fq nd b go oc nf ng gr od ni nj nk oe nm nn no of nq nr ns og nu nv nw ny nz oa bk">Generalization struggles: consistent performance across feature sets indicates the model’s difficulty in capturing underlying patterns, regardless of feature count.</li><li id="e94d" class="nb nc fq nd b go oc nf ng gr od ni nj nk oe nm nn no of nq nr ns og nu nv nw ny nz oa bk">Complexity overkill: similar results with fewer features suggest TiDE’s sophisticated architecture may be unnecessarily complex. A simpler model, like ARIMA, could potentially perform just as well.</li></ul><p id="ce37" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Then, let’s examine the chart to see if we can observe any significant differences among the three forecasting methods and the actual prices.</p><pre class="ml mm mn mo mp pm pl pn bp po bb bk"><span id="a867" class="pp oi fq pl b bg pq pr l ps pt">import matplotlib.pyplot as plt<br/><br/>plt.figure(figsize=(12, 6))<br/>plt.plot(train_data['ds'], train_data['price'], label='Training Data', color='blue')<br/>plt.plot(test_data['ds'], true_values, label='True Prices', color='green')<br/>plt.plot(test_data['ds'], predicted_prices_original, label='Predicted Prices', color='red')<br/>plt.legend()<br/>plt.title('SPY Price Forecast Using All Original Feature')<br/>plt.xlabel('Date')<br/>plt.ylabel('SPY Price')<br/>plt.savefig('spy_forecast_chart_original.png', dpi=300, bbox_inches='tight')<br/>plt.close()<br/><br/>plt.figure(figsize=(12, 6))<br/>plt.plot(train_data['ds'], train_data['price'], label='Training Data', color='blue')<br/>plt.plot(test_data['ds'], true_values, label='True Prices', color='green')<br/>plt.plot(test_data['ds'], predicted_prices_pca, label='Predicted Prices', color='red')<br/>plt.legend()<br/>plt.title('SPY Price Forecast Using PCA Dimensionality Reduction')<br/>plt.xlabel('Date')<br/>plt.ylabel('SPY Price')<br/>plt.savefig('spy_forecast_chart_pca.png', dpi=300, bbox_inches='tight')<br/>plt.close()<br/><br/>plt.figure(figsize=(12, 6))<br/>plt.plot(train_data['ds'], train_data['price'], label='Training Data', color='blue')<br/>plt.plot(test_data['ds'], true_values, label='True Prices', color='green')<br/>plt.plot(test_data['ds'], predicted_prices_vsn, label='Predicted Prices', color='red')<br/>plt.legend()<br/>plt.title('SPY Price Forecast Using VSN')<br/>plt.xlabel('Date')<br/>plt.ylabel('SPY Price')<br/>plt.savefig('spy_forecast_chart_vsn.png', dpi=300, bbox_inches='tight')<br/>plt.close()</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qh"><img src="../Images/198959fd7771ea78751d923086b0820d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IYtbTDBFh-LamkDDdmEwhw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">SPY price forecast using all original features. Image created by the author.</figcaption></figure><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qh"><img src="../Images/12c100de5f3352051765fbd2db5be6e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5_8poZN42CkS7cQTxtLMxA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">SPY price forecast using PCA. Image created by the author.</figcaption></figure><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qh"><img src="../Images/ba26bb6bbb1727805ee38cd8788db4cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8cygY-s3F-c33JA1YmUVdw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">SPY price forecast using VSN. Image created by the author.</figcaption></figure><p id="ac98" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The difference between true and predicted prices appears consistent across all three models, with no noticeable variation in performance between them.</p><h1 id="f437" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Conclusion</h1><p id="fc47" class="pw-post-body-paragraph nb nc fq nd b go pd nf ng gr pe ni nj nk pf nm nn no pg nq nr ns ph nu nv nw fj bk">We did it! We explored the importance of feature reduction in time series analysis and provided a practical implementation guide:</p><ul class=""><li id="d046" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw ny nz oa bk">Feature reduction aims to simplify models while maintaining predictive power. Benefits include reduced complexity, improved generalization, easier interpretation, and computational efficiency.</li><li id="1f40" class="nb nc fq nd b go oc nf ng gr od ni nj nk oe nm nn no of nq nr ns og nu nv nw ny nz oa bk">We demonstrated two reduction techniques using FRED data:</li></ul><ol class=""><li id="573f" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw qf nz oa bk">Principal Component Analysis (PCA), a linear dimensionality reduction method, reduced features from 438 to 76 while retaining 90% of explained variance.</li><li id="d69d" class="nb nc fq nd b go oc nf ng gr od ni nj nk oe nm nn no of nq nr ns og nu nv nw qf nz oa bk">Variable Selection Network (VSN) from the Temporal Fusion Transformers, a non-linear approach, drastically reduced features to just 1 using an 80th percentile importance threshold.</li></ol><ul class=""><li id="757d" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw ny nz oa bk">Evaluation using TiDE models showed similar performance across original and reduced feature sets, suggesting feature reduction may not always improve forecasting performance. This could be due to information loss during reduction, the model’s difficulty in capturing underlying patterns, or the possibility that a simpler model might be equally effective for this particular forecasting task.</li></ul><p id="47af" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">On a final note, we didn’t explore all feature reduction techniques, such as SHAP (SHapley Additive exPlanations), which provides a unified measure of feature importance across various model types. Even if we didn’t improve our model, it’s still better to perform feature curation and compare performance across different reduction methods. This approach helps ensure you’re not discarding valuable information while optimizing your model’s efficiency and interpretability.</p><p id="c4c7" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In future articles, we’ll apply these feature reduction techniques to more complex models, comparing their impact on performance and interpretability. Stay tuned!</p><p id="f0ee" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Ready to put these concepts into action? You can find the complete code implementation <a class="af nx" href="https://github.com/philippe-ostiguy/feature-reduction-ts" rel="noopener ugc nofollow" target="_blank">here</a>.</p><h1 id="d35f" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Liked this article? Show your support!</h1><p id="bba7" class="pw-post-body-paragraph nb nc fq nd b go pd nf ng gr pe ni nj nk pf nm nn no pg nq nr ns ph nu nv nw fj bk">👏 Clap it up to 50 times</p><p id="78e3" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">🤝 Send me a <a class="af nx" href="https://www.linkedin.com/in/philippe-ostiguy/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a> connection request to stay in touch</p><p id="43cf" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="ob">Your support means everything!</em> 🙏</p></div></div></div></div>    
</body>
</html>