- en: Graph Neural Networks Part 2\. Graph Attention Networks vs. Graph Convolutional
    Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/graph-neural-networks-part-2-graph-attention-networks-vs-gcns-029efd7a1d92?source=collection_archive---------3-----------------------#2024-10-08](https://towardsdatascience.com/graph-neural-networks-part-2-graph-attention-networks-vs-gcns-029efd7a1d92?source=collection_archive---------3-----------------------#2024-10-08)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/416f94719e2ed6517b37cc618d4bfa4c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created with Dall·E by the author.
  prefs: []
  type: TYPE_NORMAL
- en: A model that pays attention to your graph
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://hennie-de-harder.medium.com/?source=post_page---byline--029efd7a1d92--------------------------------)[![Hennie
    de Harder](../Images/20a1be0ed2ac4b535397973ad6148e6b.png)](https://hennie-de-harder.medium.com/?source=post_page---byline--029efd7a1d92--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--029efd7a1d92--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--029efd7a1d92--------------------------------)
    [Hennie de Harder](https://hennie-de-harder.medium.com/?source=post_page---byline--029efd7a1d92--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--029efd7a1d92--------------------------------)
    ·8 min read·Oct 8, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '**Welcome to the second post about GNN architectures! In the previous post,
    we saw a staggering improvement in accuracy on the Cora dataset by incorporating
    the graph structure in the model using a Graph Convolutional Network (GCN). This
    post explains Graph Attention Networks (GATs), another fundamental architecture
    of graph neural networks. Can we improve the accuracy even further with a GAT?**'
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s talk about the difference between GATs and GCNs. Then let’s train
    a GAT and compare the accuracy with the GCN and basic neural network.
  prefs: []
  type: TYPE_NORMAL
- en: This blog post is part of a series. Are you new to GNNs? I recommend you to
    start with [the first post](https://medium.com/towards-data-science/graph-neural-networks-part-1-graph-convolutional-networks-explained-9c6aaa8a406e),
    which explains graphs, neural networks, the dataset, and GCNs.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Graph Attention Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In my previous post, we saw a GCN in action. Let’s take it a step further and
    look at Graph Attention Networks (GATs). As you might remember, GCNs treat all
    neighbors equally. For GATs, this is different. GATs allow the model to learn
    different importance (attention) scores for different neighbors. They aggregate
    neighbor information by using attention mechanisms (this might ring a bell…
  prefs: []
  type: TYPE_NORMAL
