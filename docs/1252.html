<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Are GPTs Good Embedding Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Are GPTs Good Embedding Models</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/are-gpts-good-embedding-models-28d8ef6f3f63?source=collection_archive---------0-----------------------#2024-05-18">https://towardsdatascience.com/are-gpts-good-embedding-models-28d8ef6f3f63?source=collection_archive---------0-----------------------#2024-05-18</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="c23f" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A surprising experiment to show that the devil is in the details</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@yuchengtsai84?source=post_page---byline--28d8ef6f3f63--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Yu-Cheng Tsai" class="l ep by dd de cx" src="../Images/c0ec2d4b9fea512040c8e6e0250670fc.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*zGsnjC43H_diWUwS5LNb_A.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--28d8ef6f3f63--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@yuchengtsai84?source=post_page---byline--28d8ef6f3f63--------------------------------" rel="noopener follow">Yu-Cheng Tsai</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--28d8ef6f3f63--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">6 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 18, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/0f89f4b2c267496534e57792622439db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Dl6DVZB7MC2DxHzwCxgig.jpeg"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by the author using DALL-E</figcaption></figure><p id="ecee" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">With the growing number of embedding models available, choosing the right one for your machine learning applications can be challenging. Fortunately, the <a class="af nx" href="https://huggingface.co/spaces/mteb/leaderboard" rel="noopener ugc nofollow" target="_blank">MTEB leaderboard</a> provides a comprehensive range of ranking metrics for various natural language processing tasks.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ny"><img src="../Images/608efe4c9a370b4f9a08332196481dfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T9bp19voxIYWgU6FH_ltgA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Top 5 embedding models from the <a class="af nx" href="https://huggingface.co/spaces/mteb/leaderboard" rel="noopener ugc nofollow" target="_blank">MTEB leaderboard</a> as of May 17th, 2024</figcaption></figure><p id="5a07" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">When you visit the site, you’ll notice that the top five embedding models are Generative Pre-trained Transformers (GPTs). This might lead you to think that GPT models are the best for embeddings. But is this really true? Let’s conduct an experiment to find out.</p><h1 id="c61c" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">GPT Embeddings</h1><p id="f5f4" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">Embeddings are tensor representation of texts, that converts text token IDs and projects them into a tensor space.</p><p id="b661" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">By inputting text into a neural network model and performing a forward pass, you can obtain embedding vectors. However, the actual process is a bit more complex. Let’s break it down step by step:</p><ol class=""><li id="0183" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pa pb pc bk">Convert the text into token IDs</li><li id="b0a2" class="nb nc fq nd b go pd nf ng gr pe ni nj nk pf nm nn no pg nq nr ns ph nu nv nw pa pb pc bk">Pass the token IDs into a neural network</li><li id="cae2" class="nb nc fq nd b go pd nf ng gr pe ni nj nk pf nm nn no pg nq nr ns ph nu nv nw pa pb pc bk">Return the outputs of the neural network</li></ol><p id="2007" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In the first step, I am going to use a tokenizer to achieve it. <code class="cx pi pj pk pl b">model_inputs</code> is the tensor representation of the text content, <code class="cx pi pj pk pl b">"some questions."</code> .</p><pre class="ml mm mn mo mp pm pl pn bp po bb bk"><span id="5e1b" class="pp oa fq pl b bg pq pr l ps pt">from transformers import AutoTokenizer<br/><br/>tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1")<br/><br/>messages = [<br/>        {<br/>            "role": "user",<br/>            "content": "some questions.",<br/>        },<br/>]<br/><br/>encodeds = tokenizer.apply_chat_template(messages, return_tensors="pt")<br/>model_inputs = encodeds.to("cuda")</span></pre><p id="6428" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The second step is straightforward, forward-passing the <code class="cx pi pj pk pl b">model_inputs</code> into a neural network. The logits of generated tokens can be accessed via <code class="cx pi pj pk pl b">.logits</code>. <code class="cx pi pj pk pl b">torch.no_grad()</code> means I don’t want the model weights to be updated because the model is in inference mode.</p><pre class="ml mm mn mo mp pm pl pn bp po bb bk"><span id="8305" class="pp oa fq pl b bg pq pr l ps pt">import torch<br/><br/>with torch.no_grad():<br/>    return model(model_inputs).logits</span></pre><p id="67ea" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The third step is a bit tricky. GPT models are decoder-only, and their token generation is autoregressive. In simple terms, the last token of a completed sentence has seen all the preceding tokens in the sentence. Therefore, the output of the last token contains all the affinity scores (attentions) from the preceding tokens.</p><blockquote class="pu"><p id="daf9" class="pv pw fq bf px py pz qa qb qc qd nw dx">Bingo! You are most interested in the last token because of the attention mechanism in the transformers.</p></blockquote><p id="e641" class="pw-post-body-paragraph nb nc fq nd b go qe nf ng gr qf ni nj nk qg nm nn no qh nq nr ns qi nu nv nw fj bk">The output dimension of the GPTs implemented in Hugging Face is (batch size, input token size, number of vocabulary). To get the last token output of all the batches, I can perform a tensor slice.</p><pre class="ml mm mn mo mp pm pl pn bp po bb bk"><span id="d955" class="pp oa fq pl b bg pq pr l ps pt">import torch<br/>with torch.no_grad():<br/>    return model(model_inputs).logits[:, -1, :]</span></pre><h1 id="1c56" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Quality of these GPT Embeddings</h1><p id="89a4" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">To measure the quality of these GPT embeddings, you can use <a class="af nx" href="https://pytorch.org/docs/stable/generated/torch.nn.CosineSimilarity.html" rel="noopener ugc nofollow" target="_blank">cosine similarity.</a> The higher the cosine similarity, the closer the semantic meaning of the sentences.</p><pre class="ml mm mn mo mp pm pl pn bp po bb bk"><span id="7355" class="pp oa fq pl b bg pq pr l ps pt">import torch<br/>def compute_cosine_similarity(vec1, vec2):<br/>    cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)<br/>    return cos(vec1, vec2)</span></pre><p id="1754" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Let’s create some util functions that allows us to loop through list of question and answer pairs and see the result. <a class="af nx" href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1" rel="noopener ugc nofollow" target="_blank">Mistral 7b v0.1 instruct</a> , one of the great open-sourced models, is used for this experiment.</p><pre class="ml mm mn mo mp pm pl pn bp po bb bk"><span id="d4bd" class="pp oa fq pl b bg pq pr l ps pt">import torch<br/>from termcolor import colored<br/>from transformers import AutoModelForCausalLM, AutoTokenizer<br/><br/>model = AutoModelForCausalLM.from_pretrained(<br/>    "mistralai/Mistral-7B-Instruct-v0.1"<br/>)<br/><br/>tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1")<br/><br/>def generate_last_token_embeddings(question):<br/>    messages = [<br/>        {<br/>            "role": "user",<br/>            "content": question,<br/>        },<br/>    ]<br/>    encodeds = tokenizer.apply_chat_template(messages, return_tensors="pt")<br/>    model_inputs = encodeds.to("cuda")<br/>    with torch.no_grad():<br/>        return model(model_inputs).logits[:, -1, :]<br/><br/>def get_similarities(questions, answers):<br/>    for question in questions:<br/>        for answer in answers:<br/>            q_embedding, a_embedding = (<br/>                generate_last_token_embeddings(question),<br/>                generate_last_token_embeddings(answer),<br/>            )<br/>            similarity = compute_cosine_similarity(q_embedding, a_embedding)<br/>            print(colored(f"question: {question} and ans: {answer}", "green"))<br/>            print(colored(f"result: {similarity}", "blue"))<br/><br/>questions = ["Where is the headquarter of OpenAI?", "What is GPU?"]<br/>answers = [<br/>    "OpenAI is based at San Francisco.",<br/>    "A graphics processing unit (GPU) is an electronic circuit that can perform mathematical calculations quickly",<br/>]<br/>get_similarities(questions, answers)</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qj"><img src="../Images/c99fb95672048c42a47ba01ae6599ec2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SvnGTZmNHCDniBw_kGJ8OQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Cosine similarities for mistral 7b v0.1 instruct (Image by the author)</figcaption></figure><h1 id="8c91" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Results and Observations</h1><p id="07de" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">For the first question and answer pair:</p><ul class=""><li id="88c1" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw qk pb pc bk">Question: “What is the headquarter of OpenAI?”</li><li id="1c69" class="nb nc fq nd b go pd nf ng gr pe ni nj nk pf nm nn no pg nq nr ns ph nu nv nw qk pb pc bk">Answer: “OpenAI is based at San Francisco.”</li><li id="604e" class="nb nc fq nd b go pd nf ng gr pe ni nj nk pf nm nn no pg nq nr ns ph nu nv nw qk pb pc bk">Cosine Similarity: 0.96</li></ul><p id="1a15" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">For the second question and answer pair:</p><ul class=""><li id="8637" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw qk pb pc bk">Question: “What is GPU?”</li><li id="03fa" class="nb nc fq nd b go pd nf ng gr pe ni nj nk pf nm nn no pg nq nr ns ph nu nv nw qk pb pc bk">Answer: “A graphics processing unit (GPU) is an electronic circuit that can perform mathematical calculations quickly.”</li><li id="ad79" class="nb nc fq nd b go pd nf ng gr pe ni nj nk pf nm nn no pg nq nr ns ph nu nv nw qk pb pc bk">Cosine Similarity: 0.94</li></ul><p id="96d9" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">For an irrelevant pair:</p><ul class=""><li id="240b" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw qk pb pc bk">Question: “Where is the headquarter of OpenAI?”</li><li id="90b9" class="nb nc fq nd b go pd nf ng gr pe ni nj nk pf nm nn no pg nq nr ns ph nu nv nw qk pb pc bk">Answer: “A graphics processing unit (GPU) is an electronic circuit that can perform mathematical calculations quickly.”</li><li id="c6ea" class="nb nc fq nd b go pd nf ng gr pe ni nj nk pf nm nn no pg nq nr ns ph nu nv nw qk pb pc bk">Cosine Similarity: 0.90</li></ul><p id="82a1" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">For the worst pair:</p><ul class=""><li id="5d0a" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw qk pb pc bk">Question: “What is GPU?”</li><li id="bb40" class="nb nc fq nd b go pd nf ng gr pe ni nj nk pf nm nn no pg nq nr ns ph nu nv nw qk pb pc bk">Answer: “OpenAI is based at San Francisco.”</li><li id="4a88" class="nb nc fq nd b go pd nf ng gr pe ni nj nk pf nm nn no pg nq nr ns ph nu nv nw qk pb pc bk">Cosine Similarity: 0.93</li></ul><p id="1686" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">These results suggest that using GPT models, in this case, the mistral 7b instruct v0.1, as embedding models may not yield great results in terms of distinguishing between relevant and irrelevant pairs. But why are GPT models still among the top 5 embedding models?</p><h1 id="0143" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Contrastive Loss Comes to the Rescue</h1><pre class="ml mm mn mo mp pm pl pn bp po bb bk"><span id="8248" class="pp oa fq pl b bg pq pr l ps pt">tokenizer = AutoTokenizer.from_pretrained("intfloat/e5-mistral-7b-instruct")<br/>model = AutoModelForCausalLM.from_pretrained(<br/>  "intfloat/e5-mistral-7b-instruct"<br/>)</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ql"><img src="../Images/a7d9806157a8c611ddf1ee437d3e12be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D5Y4kJCItheLJK3jKnbPgQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Cosine similarities for <code class="cx pi pj pk pl b">e5-mistral-7b-instruct (Image by the author)</code></figcaption></figure><p id="5537" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Repeating the same evaluation procedure with a different model, <code class="cx pi pj pk pl b">e<a class="af nx" href="http://intfloat/e5-mistral-7b-instruct" rel="noopener ugc nofollow" target="_blank">5-mistral-7b-instruct</a></code>, which is one of the top open-sourced models from the MTEB leaderboard and fine-tuned from mistral 7b instruct, I discover that the cosine similarity for the relevant question and pairs are 0.88 and 0.84 for OpenAI and GPU questions, respectively. For the irrelevant question and answer pairs, the similarity drops to 0.56 and 0.67. This findings suggests <code class="cx pi pj pk pl b">e5-mistral-7b-instruct</code> is a much-improved model for embeddings. What makes such an improvement?</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qm"><img src="../Images/08fb21f2da6dcfa55fa80bdea3f81999.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GVLrxCF0w-BekP_OM7ldfg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx"><a class="af nx" rel="noopener" target="_blank" href="/contrastive-loss-explaned-159f2d4a87ec">The contrastive loss function</a></figcaption></figure><p id="725f" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Delving into the <a class="af nx" href="https://arxiv.org/pdf/2401.00368" rel="noopener ugc nofollow" target="_blank">paper</a> behind <code class="cx pi pj pk pl b">e5-mistral-7b-instruct</code>, the key is the use of <a class="af nx" href="https://lilianweng.github.io/posts/2021-05-31-contrastive/" rel="noopener ugc nofollow" target="_blank">contrastive loss </a>to further fine tune the mistral model.</p><blockquote class="pu"><p id="71d4" class="pv pw fq bf px py pz qa qb qc qd nw dx">Unlike GPTs that are trained or further fine-tuned using <a class="af nx" href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html" rel="noopener ugc nofollow" target="_blank">cross-entropy loss</a> of predicted tokens and labeled tokens, contrastive loss aims to maximize the distance between negative pairs and minimize the distance between the positive pairs.</p></blockquote><p id="96d6" class="pw-post-body-paragraph nb nc fq nd b go qe nf ng gr qf ni nj nk qg nm nn no qh nq nr ns qi nu nv nw fj bk">This <a class="af nx" rel="noopener" target="_blank" href="/contrastive-loss-explaned-159f2d4a87ec">blog post </a>covers this concept in greater details. The <code class="cx pi pj pk pl b">sim</code> function calculates the cosine distance between two vectors. For contrastive loss, the denominators represent the cosine distance between positive examples and negative examples. The rationale behind contrastive loss is that we want similar vectors to be as close to 1 as possible, since log(1) = 0 represents the optimal loss.</p><h1 id="5ab2" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Conclusion</h1><p id="5730" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">In this post, I have highlighted a common pitfall of using GPTs as embedding models without fine-tuning. My evaluation suggests that fine-tuning GPTs with contrastive loss, the embeddings can be more meaningful and discriminative. By understanding the strengths and limitations of GPT models, and leveraging customized loss like contrastive loss, you can make more informed decisions when selecting and utilizing embedding models for your machine learning projects. I hope this post helps you choose GPTs models wisely for your applications and look forward to hearing your feedback! :)</p></div></div></div><div class="ab cb qn qo qp qq" role="separator"><span class="qr by bm qs qt qu"/><span class="qr by bm qs qt qu"/><span class="qr by bm qs qt"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="2cbe" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">If you are interested in fine-tuning LLMs at scale, I have another related post that can help you achieve it. :)</p><div class="qv qw qx qy qz ra"><a href="https://medium.com/sage-ai/fine-tuning-large-language-models-a-guide-into-distributed-parallel-training-with-deepspeed-ray-784914926a17?source=post_page-----28d8ef6f3f63--------------------------------" rel="noopener follow" target="_blank"><div class="rb ab ig"><div class="rc ab co cb rd re"><h2 class="bf fr hw z io rf iq ir rg it iv fp bk">Fine-Tuning Large Language Models: A Guide into Distributed Parallel Training with DeepSpeed, Ray…</h2><div class="rh l"><h3 class="bf b hw z io rf iq ir rg it iv dx">Path to Open-source LLMs</h3></div><div class="ri l"><p class="bf b dy z io rf iq ir rg it iv dx">medium.com</p></div></div><div class="rj l"><div class="rk l rl rm rn rj ro lq ra"/></div></div></a></div><p id="1339" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">I also have another post related to the scaling law of LLMs. It provides additional context on why language models are getting larger and explains the underlying reasons? Happy learning and sharing! Cheers.</p><div class="qv qw qx qy qz ra"><a href="https://medium.com/sage-ai/demystify-transformers-a-comprehensive-guide-to-scaling-laws-attention-mechanism-fine-tuning-fffb62fc2552?source=post_page-----28d8ef6f3f63--------------------------------" rel="noopener follow" target="_blank"><div class="rb ab ig"><div class="rc ab co cb rd re"><h2 class="bf fr hw z io rf iq ir rg it iv fp bk">Demystify Transformers: A Comprehensive Guide to Scaling Laws</h2><div class="rh l"><h3 class="bf b hw z io rf iq ir rg it iv dx">Unpacking Transformer Technologies and Scaling Strategies</h3></div><div class="ri l"><p class="bf b dy z io rf iq ir rg it iv dx">medium.com</p></div></div><div class="rj l"><div class="rp l rl rm rn rj ro lq ra"/></div></div></a></div></div></div></div></div>    
</body>
</html>