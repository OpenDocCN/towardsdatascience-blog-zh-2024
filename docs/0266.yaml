- en: Fraud Detection with Generative Adversarial Nets (GANs)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用生成对抗网络（GANs）进行欺诈检测
- en: 原文：[https://towardsdatascience.com/fraud-detection-with-generative-adversarial-nets-gans-26bea360870d?source=collection_archive---------5-----------------------#2024-01-29](https://towardsdatascience.com/fraud-detection-with-generative-adversarial-nets-gans-26bea360870d?source=collection_archive---------5-----------------------#2024-01-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/fraud-detection-with-generative-adversarial-nets-gans-26bea360870d?source=collection_archive---------5-----------------------#2024-01-29](https://towardsdatascience.com/fraud-detection-with-generative-adversarial-nets-gans-26bea360870d?source=collection_archive---------5-----------------------#2024-01-29)
- en: Application of GANs for data augmentation to adjust an imbalanced dataset
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用GANs进行数据增强以调整不平衡数据集
- en: '[](https://deeporigami.medium.com/?source=post_page---byline--26bea360870d--------------------------------)[![Michio
    Suginoo](../Images/15e4a70d17d163889cc902bf4409931a.png)](https://deeporigami.medium.com/?source=post_page---byline--26bea360870d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--26bea360870d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--26bea360870d--------------------------------)
    [Michio Suginoo](https://deeporigami.medium.com/?source=post_page---byline--26bea360870d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://deeporigami.medium.com/?source=post_page---byline--26bea360870d--------------------------------)[![Michio
    Suginoo](../Images/15e4a70d17d163889cc902bf4409931a.png)](https://deeporigami.medium.com/?source=post_page---byline--26bea360870d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--26bea360870d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--26bea360870d--------------------------------)
    [Michio Suginoo](https://deeporigami.medium.com/?source=post_page---byline--26bea360870d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--26bea360870d--------------------------------)
    ·18 min read·Jan 29, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--26bea360870d--------------------------------)
    ·阅读时间：18分钟·2024年1月29日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/b701f889a034bd2b3b6ca7bfd994721c.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b701f889a034bd2b3b6ca7bfd994721c.png)'
- en: Photo by [Brett Jordan](https://unsplash.com/@brett_jordan?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Brett Jordan](https://unsplash.com/@brett_jordan?utm_source=medium&utm_medium=referral)于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '[“Generative Adversarial Nets](https://arxiv.org/abs/1406.2661)” (GANs) demonstrated
    outstanding performance in generating realistic synthetic data which are indistinguishable
    from the real data in the past. Unfortunately, GANs caught the public’s attention
    because of its unethical applications, [deepfakes](https://www.technologyreview.com/2018/08/17/240305/fake-america-great-again/)
    (Knight, 2018).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[“生成对抗网络](https://arxiv.org/abs/1406.2661)”（GANs）在过去展示了生成逼真合成数据的出色表现，这些数据与真实数据几乎无法区分。不幸的是，GANs因其不道德的应用，尤其是[深度伪造](https://www.technologyreview.com/2018/08/17/240305/fake-america-great-again/)（Knight，2018），而引起了公众的关注。'
- en: This article illustrates a case with a good motive in the application of GANs
    in the context of fraud detection.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本文阐述了GANs在欺诈检测领域应用的一个有良好动机的案例。
- en: Fraud detection is an application of binary classification prediction. Fraud
    cases, which account for only a small fraction of the transaction universe, constitute
    a minority class that makes the dataset highly imbalanced. In general, the resulting
    model tends to be biased towards the majority class and tends to underfit to the
    minority class. Thus, the less balanced the dataset, the poorer the performance
    of the classification predictor would be.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 欺诈检测是一个二分类预测应用。欺诈案件仅占交易总量的一小部分，构成了一个少数类别，使得数据集高度不平衡。通常，生成的模型往往偏向于多数类，并且容易对少数类欠拟合。因此，数据集不平衡的程度越高，分类预测器的表现就会越差。
- en: My motive here is to use GANs as a data augmentation tool in an attempt to address
    this classical problem of fraud detection associated with the imbalanced dataset.
    More specifically, GANs can generate realistic synthetic data of the minority
    fraud class and transform the imbalanced dataset perfectly balanced.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我的动机是在尝试解决与不平衡数据集相关的经典欺诈检测问题时，将GANs作为数据增强工具来使用。更具体地说，GANs可以生成少数欺诈类的逼真合成数据，并使不平衡的数据集达到完美平衡。
- en: 'And, I am hoping that this sophisticated algorithm could materially contribute
    to the performance of fraud detection. In other words, my initial expectation
    is: the better sophisticated algorithm, the better performance.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: A relevant question is if the use of GANs will guarantee a promising improvement
    in the performance of fraud detection and satisfy my motive. Let’s see.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '***Introduction***'
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In principle, fraud detection is an application of binary classification algorithm:
    to classify each transaction whether it is a fraud case or not.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Fraud cases account for only a small fraction of the transaction universe. In
    general, fraud cases constitute the minority class, thus, make the dataset highly
    imbalanced.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: The fewer fraud cases, the more sound the transaction system would be.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Very simple and intuitive.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Paradoxically, that sound condition was one of the primary reasons that made
    fraud detection challenging in the past, if not impossible. It is simply because
    it was difficult for a classification algorithm to learn the probability distribution
    of the minority class of fraud.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: In general, the more balanced the dataset, the better the performance of the
    classification predictor. In other words, the less balanced (or the more imbalanced)
    the dataset, the poorer the performance of classifier.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: 'This paints the classical problem of fraud detection: a binary classification
    application with highly imbalanced dataset.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: In this setting, we can use Generative Adversarial Nets (GANs) as a data augmentation
    tool to generate realistic synthetic data of the minority fraud class to transform
    the entire dataset more balanced in an attempt to improve the performance of the
    classifier model of fraud detection.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'This article is divided into the following sections:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'Section 1: Algorithm Overview: Bi-level Optimization Architecture of GANs'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Section 2: Fraud Dataset'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Section 3: Python Code breakdown of GANs for data augmentation'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Section 4: Fraud Detection Overview (Benchmark Scenario vs GANs Scenario)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Section 5: Conclusion'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, I will primarily focus on the topic of GANs (both the algorithm and
    the code). For the remaining topics of the model development other than GANs,
    such as data preprocessing and classifier algorithm, I will only outline the process
    and refrain from going into their details. In this context, this article assumes
    that the readers have a basic knowledge about the binary classifier algorithm
    (especially, Ensemble Classifier that I selected for fraud detection) as well
    as general understanding of data cleaning and preprocessing.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'For the detailed code, the readers are welcome to access the following link:
    [https://github.com/deeporigami/Portfolio/blob/6538fcaad1bf58c5f63d6320ca477fa867edb1df/GAN_FraudDetection_Medium_2.ipynb](https://github.com/deeporigami/Portfolio/blob/6538fcaad1bf58c5f63d6320ca477fa867edb1df/GAN_FraudDetection_Medium_2.ipynb)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '***Section 1: Algorithm Overview: Bi-level Optimization Architecture of GANs***'
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'GANs is a special type of generative algorithm. As its name suggests, Generative
    Adversarial Nets (GANs) is composed of two neural networks: the generative network
    (the generator) and the adversarial network (the discriminator). GANs pits these
    two agents against each other to engage in a competition, where the generator
    attempts to generate realistic synthetic data and the discriminator to distinguish
    the synthetic data from the real data.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: GAN是一种特殊类型的生成算法。正如其名称所示，生成对抗网络（GAN）由两个神经网络组成：生成网络（生成器）和对抗网络（判别器）。GAN将这两个代理放在一起进行竞争，其中生成器试图生成逼真的合成数据，而判别器则试图区分合成数据和真实数据。
- en: 'The original GANs was introduced in a seminal paper: “[Generative Adversarial
    Nets](https://arxiv.org/abs/1406.2661)” (Goodfellow, et al., Generative Adversarial
    Nets, 2014). The co-authors of the original GANs portrayed GANs with a counterfeiter-police
    analogy: an iterative game, where the generator acts as a counterfeiter and the
    discriminator plays the role of the police to detect the counterfeit that the
    generator forged.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的GAN在一篇开创性的论文中提出：[“生成对抗网络”（Generative Adversarial Nets](https://arxiv.org/abs/1406.2661))（Goodfellow等人，生成对抗网络，2014年）。原始GAN的联合作者用伪造者-警察的类比来描述GAN：这是一场迭代博弈，其中生成器扮演伪造者，判别器扮演警察的角色，检测生成器伪造的假货。
- en: The original GANs was innovative in a sense that it addressed and overcame conventional
    difficulties in training deep generative algorithm in the past. And as its core,
    it was designed with bi-level optimization framework with an equilibrium seeking
    objective setting (vs maximum likelihood oriented objective setting).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 原始GAN具有创新性，因为它解决并克服了过去训练深度生成算法的常见困难。而且，作为其核心，它采用了二级优化框架，具有寻求平衡的目标设置（与最大似然目标设置相对）。
- en: Ever since, many variant architectures of GANs have been explored. As a precaution,
    this article refers solely to the prototype architecture of the original GANs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从那时起，已经探索了许多GAN的变体架构。作为一种预防措施，本文仅参考原始GAN的原型架构。
- en: '***Generator and Discriminator***'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '***生成器与判别器***'
- en: Repeatedly, in the architecture of GANs, the two neural networks — the generator
    and the discriminator — compete against each other. In this context, the competition
    takes place through the iteration of forward propagation and backward propagation
    (according to the general framework of neural networks).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在GAN架构中，这两个神经网络——生成器和判别器——相互竞争。具体来说，这种竞争通过前向传播和反向传播的迭代进行（遵循神经网络的一般框架）。
- en: 'On one hand, it is straight-forward that the discriminator is a binary classifier
    by design: it classifies whether each sample is real (label: 1) or fake/synthetic
    (label:0). And the discriminator is fed with both the real samples and the synthetic
    samples during the forward propagation. Then, during the backpropagation, it learns
    to detect the synthetic data from the mixed data feed.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，显而易见，判别器本质上是一个二分类器：它将每个样本分类为真实（标签：1）或伪造/合成（标签：0）。判别器在前向传播过程中接收真实样本和合成样本。然后，在反向传播过程中，它学习从混合数据中检测合成数据。
- en: On the other hand, the generator is a noise distribution by design. The generator
    is fed with the real samples during the forward propagation. Then, during the
    backward propagation, the generator learns the probability distribution of the
    real data in order to better simulate its synthetic samples.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，生成器在设计上是一个噪声分布。生成器在前向传播过程中接收真实样本。然后，在反向传播过程中，生成器学习真实数据的概率分布，以便更好地模拟其合成样本。
- en: And these two agents are trained alternately via “bi-level optimization” framework.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个代理通过“二级优化”框架交替训练。
- en: '***Bi-level Training Mechanism (bi-level optimization method)***'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '***二级训练机制（二级优化方法）***'
- en: In the original GAN paper, in order to train these two agents that pursue their
    diametrically opposite objectives, the co-authors designed a “bi-level optimization
    (training)” architecture, in which one internal training block (training of the
    discriminator) is nested within another high-level training block (training of
    the generator).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始的GAN论文中，为了训练这两个目标完全对立的代理，联合作者设计了一个“二级优化（训练）”架构，其中一个内部训练模块（判别器的训练）被嵌套在另一个高级训练模块（生成器的训练）中。
- en: The image below illustrates the structure of “bi-level optimization” in the
    nested training loops. The discriminator is trained within the nested inner loop,
    while the generator is trained in the main loop at the higher level.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了“二级优化”在嵌套训练循环中的结构。判别器在嵌套的内部循环中进行训练，而生成器在更高层级的主循环中进行训练。
- en: '![](../Images/ee36b648bf82a7b9e95df5ddb35f87b9.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee36b648bf82a7b9e95df5ddb35f87b9.png)'
- en: Image by Author
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: And GANs trains these two agents alternately in this bi-level training architecture
    (Goodfellow, et al., Generative Adversarial Nets, 2014, p. 3). In other words,
    while training one agent during the alternation, we need to freeze the learning
    process of the other agent (Goodfellow I. , 2015, p. 3).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: GAN通过这种二级训练架构交替训练这两个代理（Goodfellow等，生成对抗网络，2014年，第3页）。换句话说，在交替训练一个代理的过程中，我们需要冻结另一个代理的学习过程（Goodfellow
    I.，2015年，第3页）。
- en: '***Mini-Max Optimization Objective***'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '***极小极大优化目标***'
- en: In addition to the “bi-level optimization” mechanism which enables the alternate
    training of these two agents, another unique feature that differentiates GANs
    from the conventional prototype of neural network is its mini-max optimization
    objective. Simply put, in contrast to the conventional maximum seeking approach
    (such as maximum-likelihood) , GANs pursues an equilibrium-seeking optimization
    objective.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使这两个代理可以交替训练的“二级优化”机制外，GAN与传统神经网络原型的另一个独特特征是其极小极大优化目标。简单来说，与传统的最大化方法（例如最大似然）不同，GAN追求的是一个寻求平衡的优化目标。
- en: What is an equilibrium-seeking optimization objective?
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是寻求平衡的优化目标？
- en: Let’s break it down.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步解析。
- en: 'GANs’ two agents have two diametrically opposite objectives. While the discriminator,
    as a binary classifier, aims at maximizing the probability of correctly classifying
    the mixture of the real samples and the synthetic samples, the generator’s objective
    is to minimize the probability that the discriminator correctly classifies the
    synthetic data: simply because the generator needs to fool the discriminator.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: GAN的两个代理有着截然相反的目标。判别器作为一个二分类器，旨在最大化正确分类真实样本和合成样本混合体的概率，而生成器的目标则是最小化判别器正确分类合成数据的概率：因为生成器需要欺骗判别器。
- en: In this context, the co-authors of the original GANs called the overall objective
    a “***minimax game***”. (Goodfellow, et al., 2014, p. 3)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下，原始GAN的合著者称整体目标为“***极小极大博弈***”。（Goodfellow等，2014年，第3页）
- en: 'Overall, the ultimate ***mini-max optimization objective*** of GANs is not
    to search for the global maximum/minimum of either of these objective functions.
    Instead, it is set to seek an equilibrium point which can be interpreted as:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，GAN的最终***极小极大优化目标***不是寻找这些目标函数的全局最大值/最小值。而是设定为寻求一个平衡点，可以理解为：
- en: “a saddle point that is a local maximum for the classifier and a local minimum
    for the generator” (Goodfellow I. , 2015, p. 2)
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “一个鞍点，对于分类器来说是局部最大值，对于生成器来说是局部最小值”（Goodfellow I.，2015年，第2页）
- en: where neither of agents can improve their performance any longer.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其中，任何一个代理都无法再提高其性能。
- en: where the synthetic data that the generator learned to create has become realistic
    enough to fool the discriminator.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其中，生成器学会创造的合成数据已经足够真实，能够欺骗判别器。
- en: 'And the equilibrium point could be conceptually represented by the probability
    of random guessing, 0.5 (50%), for the discriminator: D(z) => 0.5 .'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 平衡点可以在概念上通过判别器的随机猜测概率0.5（50%）来表示：D(z) => 0.5。
- en: Let’s transcribe the conceptual framework of GANs’ minimax optimization in terms
    of their objective functions.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们根据GAN的目标函数来转述其极小极大优化的概念框架。
- en: 'The objective of the discriminator is to maximize the objective function in
    the following image:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器的目标是最大化下图中的目标函数：
- en: '![](../Images/5b3ace53f4fcc19374406302839ec188.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b3ace53f4fcc19374406302839ec188.png)'
- en: Image by Author
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: 'In order to resolve a potential saturation issue, they converted the second
    term of the original log-likelihood objective function for the generator as follows
    and recommended to maximize the converted version as the generator’s objective:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决可能的饱和问题，他们将生成器原始对数似然目标函数中的第二项转换如下，并建议将转换后的版本作为生成器的目标来最大化：
- en: '![](../Images/1047088989096cf0cee24f1e93409952.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1047088989096cf0cee24f1e93409952.png)'
- en: Image by Author
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: Overall, the architecture of GANs’ “bi-level optimization” can be translated
    in to the following algorithm.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，GANs的“二级优化”架构可以转化为以下算法。
- en: '![](../Images/1c7d25fd63b2c1395701db01c688802a.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c7d25fd63b2c1395701db01c688802a.png)'
- en: Image by Author
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自作者
- en: 'For more details about the algorithmic design of GANs, please read another
    article of mine: [Mini-Max Optimization Design of Generative Adversarial Nets](/mini-max-optimization-design-of-generative-adversarial-networks-gan-dc1b9ea44a02)
    .'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 有关GANs算法设计的更多细节，请阅读我另一篇文章：[生成对抗网络的极小极大优化设计](/mini-max-optimization-design-of-generative-adversarial-networks-gan-dc1b9ea44a02)。
- en: Now, let’s move on to the actual coding with a dataset.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始使用数据集进行实际编码。
- en: In order to highlight GANs algorithm, I will primarily focus on the code of
    GANs here and only outline the rest of the process.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了突出GANs算法，我将在这里主要聚焦于GANs的代码，并仅概述其余过程。
- en: 'Section 2: Fraud Dataset'
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2部分：欺诈数据集
- en: 'For fraud detection, I selected the following dataset of credit card transactions
    from Kaggle: [https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行欺诈检测，我从Kaggle选择了以下信用卡交易数据集：[https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)
- en: 'Data License: [Database Contents License (DbCL) v1.0](https://opendatacommons.org/licenses/dbcl/1-0/)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 数据许可：[数据库内容许可（DbCL）v1.0](https://opendatacommons.org/licenses/dbcl/1-0/)
- en: Here is a summary of the dataset.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这是数据集的总结。
- en: The dataset contains 284,807 transactions. In the dataset, we have only 492
    fraud cases (including 29 duplicated cases).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含284,807笔交易。在该数据集中，我们只有492个欺诈案例（其中包括29个重复案例）。
- en: Since the fraud class accounts for only 0.172% of all transactions, it constitutes
    an extremely small minority class. This dataset is an appropriate one for illustrating
    the classical problem of fraud detection associated with the imbalanced dataset.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 由于欺诈类别仅占所有交易的0.172%，它构成了一个极其小的少数类。这个数据集非常适合用来说明与不平衡数据集相关的经典欺诈检测问题。
- en: 'It has the following 30 features:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 它包含以下30个特征：
- en: 'V1, V2, … V28: 28 principal components obtained by PCA. The source of the data
    is not disclosed for the privacy protection purpose.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: V1, V2, … V28：通过PCA获得的28个主成分。数据来源未公开，以保护隐私。
- en: '‘Time’: the seconds elapsed between each transaction and the first transaction
    of the dataset.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ‘Time’：每笔交易与数据集第一笔交易之间经过的秒数。
- en: '‘Amount’: the amount of the transaction.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ‘Amount’：交易金额。
- en: The label is set as ‘Class’.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 标签设置为“Class”。
- en: '‘Class’: 1 in case of fraud; and 0 otherwise.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ‘Class’：如果是欺诈，则为1；否则为0。
- en: '***Data Preprocessing: Feature Selection***'
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '***数据预处理：特征选择***'
- en: 'Since the dataset has already been pretty much, if not perfectly, cleaned,
    I only had to do few things for the data cleaning: elimination of duplicated data
    and removal of outliers.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集已经相当干净（如果不是完全干净的话），我只需做一些数据清理工作：去除重复数据和剔除异常值。
- en: Thereafter, given 30 features in the dataset, I decided to run the feature selection
    to reduce the number of the features by eliminating less important features before
    the training process. I selected the built-in [***feature importance score***](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel)of
    the scikit-learn Random Forest Classifier to estimate the scores of all the 30
    features.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，鉴于数据集中有30个特征，我决定进行特征选择，通过剔除不太重要的特征来减少特征数量，准备训练过程。我选择了scikit-learn随机森林分类器的内置[***特征重要性评分***](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel)，用来估算所有30个特征的评分。
- en: The following chart displays the summary of the result. If interested in the
    detailed process, please visit my code listed above.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了结果的摘要。如果您对详细过程感兴趣，请访问我上面列出的代码。
- en: '![](../Images/2ce0c2fdd9ede86e0b09d75dea23a6e0.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ce0c2fdd9ede86e0b09d75dea23a6e0.png)'
- en: Image by Author
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自作者
- en: Based on the results displayed in the bar chart above, I made my subjective
    judgement to select the top 6 features for the analysis and remove all the remaining
    insignificant features from the model building process.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上面条形图中显示的结果，我做出了主观判断，选择了6个最重要的特征进行分析，并从模型构建过程中移除了其余不重要的特征。
- en: Here is the selected top 6 important features.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这是选定的6个重要特征。
- en: '![](../Images/69b736742bf84f1f6d3608082ba86018.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69b736742bf84f1f6d3608082ba86018.png)'
- en: Image by Author
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自作者
- en: 'For the model building purpose going forward, I focused on these 6 selected
    features. After the data preprocessing, we have the working dataframe, df, of
    the following shape:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续的模型构建中，我专注于这6个选择的特征。在数据预处理之后，我们得到了如下形状的工作数据框df：
- en: df.shape = (282513, 7)
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: df.shape = (282513, 7)
- en: Hopefully, the feature selection would reduce the complexity of the resulting
    model and stabilize its performance, while retaining critical information for
    optimizing a binary classifier.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 希望特征选择能够减少最终模型的复杂性并稳定其性能，同时保留优化二分类器所需的关键信息。
- en: '***Scenario 3:*** Code breakdown of GANs for data augmentation'
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '***场景3：*** GANs数据增强代码解析'
- en: Finally, it’s time for us to use GANs for data augmentation.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，是时候使用GANs进行数据增强了。
- en: '***So how many synthetic data do we need to create?***'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '***那么我们需要创建多少合成数据呢？***'
- en: First of all, our interest for the data augmentation is only for the model training.
    Since the test dataset is out-of-sample data, we want to preserve the original
    form of the test dataset. Secondly, because our intention is to transform the
    imbalanced dataset perfectly, we do not want to augment the majority class of
    non-fraud cases.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们对数据增强的兴趣仅限于模型训练。由于测试数据集是样本外数据，我们希望保留测试数据集的原始形式。其次，因为我们的目标是完美地转换不平衡数据集，我们不想增加非欺诈类别的多数类数据。
- en: Simply put, we want to augment only the train dataset of the minority fraud
    class, nothing else.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，我们只希望增加少数欺诈类的训练数据集，而不是其他任何数据。
- en: Now, let’s split the working dataframe into the train dataset and the test dataset
    in 80/20 ratio, using a stratified data split method.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用分层数据拆分方法，将工作数据框拆分为80/20比例的训练数据集和测试数据集。
- en: '[PRE0]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As a result, the shape of the train dataset is as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 结果，训练数据集的形状如下：
- en: train_df.shape = (226010, 7)
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: train_df.shape = (226010, 7)
- en: Let’s see the composition (the fraud cases and the non-fraud cases) of the train
    dataset.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看训练数据集的组成（欺诈案例和非欺诈案例）。
- en: '[PRE1]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This tells us that the train dataset (226,010) is comprised of 225,632 non-fraud
    data and 378 fraud data. In other words, the difference between them is 225,254\.
    This number is the number of the synthetic fraud data (*num_synthetic_samples*)
    that we need to augment in order to perfectly match the numbers of these two classes
    within the train dataset: as a reminder, we do preserve the original test dataset.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们，训练数据集（226,010）由225,632个非欺诈数据和378个欺诈数据组成。换句话说，它们之间的差异是225,254。这个数字是我们需要增加的合成欺诈数据（*num_synthetic_samples*）的数量，以便在训练数据集中完美地匹配这两个类别的数量：提醒一下，我们保留了原始的测试数据集。
- en: '***Next, let’s code GANs.***'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '***接下来，让我们编写GANs代码。***'
- en: 'First, let’s create custom functions to determine the two agents: the discriminator
    and the generator.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建自定义函数来确定两个代理：判别器和生成器。
- en: 'For the generator, I create a noise distribution function, *build_generator()*,
    which requires two parameters: *latent_dim* (the dimension of the noise) as the
    shape of its input; and the shape of its output, *output_dim*, which corresponds
    to the number of the features.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生成器，我创建了一个噪声分布函数*build_generator()*，它需要两个参数：*latent_dim*（噪声的维度）作为输入的形状；以及输出的形状*output_dim*，即特征的数量。
- en: '[PRE2]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For the discriminator, I create a custom function *build_discriminator()* that
    takes *input_dim*, which corresponds to the number of the features.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对于判别器，我创建了一个自定义函数*build_discriminator()*，它需要一个*input_dim*，即特征的数量。
- en: '[PRE3]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then, we can call these function to create the generator and the discriminator.
    Here, for the generator I arbitrarily set *latent_dim* to be 32: you can try other
    value here, if you like.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以调用这些函数来创建生成器和判别器。在这里，对于生成器，我随意将*latent_dim*设置为32：你可以尝试其他值。
- en: '[PRE4]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: At this stage, we need to compile the discriminator, which is going to be nested
    in the main (higher) optimization loop later. And we can compile the discriminator
    with the following argument setting.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一阶段，我们需要编译判别器，稍后它将嵌套在主（更高层次的）优化循环中。我们可以通过以下参数设置来编译判别器。
- en: 'the loss function of the discriminator: the generic cross-entropy loss function
    for a binary classifier'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 判别器的损失函数：二分类器的通用交叉熵损失函数
- en: 'the evaluation metrics: precision and recall.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估指标：精确度和召回率。
- en: '[PRE5]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: For the generator, we will compile it when we construct the main (upper) optimization
    loop.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生成器，我们将在构建主（上层）优化循环时进行编译。
- en: 'At this stage, we can define the custom objective function for the generator
    as follows. Remember, the recommended objective was to maximize the following
    formula:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们可以定义生成器的自定义目标函数如下。记住，推荐的目标是最大化以下公式：
- en: '![](../Images/1047088989096cf0cee24f1e93409952.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1047088989096cf0cee24f1e93409952.png)'
- en: Image by Author
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: '[PRE6]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Above, the negative sign is required, since the loss function by default is
    designed to be minimized.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 上面提到的负号是必须的，因为默认情况下，损失函数是设计为最小化的。
- en: Then, we can construct the main (upper) loop, *build_GANs(generator, discriminator),*
    of the bi-level optimization architecture. In this main loop, we compile the generator
    implicitly. In this context, we need to use the custom objective function of the
    generator, *generator_loss_log_d*, when we compile the main loop.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以构建双层优化架构的主（上层）循环，*build_GANs(generator, discriminator)*。在这个主循环中，我们隐式地编译生成器。在这种情况下，我们在编译主循环时需要使用生成器的自定义目标函数
    *generator_loss_log_d*。
- en: As aforementioned, we need to freeze the discriminator when we train the generator.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们在训练生成器时需要冻结判别器。
- en: '[PRE7]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: At the last line above, *gan* calls *build_gan()* in order to implement the
    batch training below, using [Keras’ model*.train_on_batch()* method](https://keras.io/api/models/model_training_apis/)*.*
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面最后一行，*gan* 调用了 *build_gan()*，以便执行下面的批量训练，使用 [Keras 的 model.train_on_batch()
    方法](https://keras.io/api/models/model_training_apis/)。
- en: As a reminder, while we train the discriminator, we need to freeze the training
    of the generator; and while we train the generator, we need to freeze the training
    of the discriminator.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒一下，在训练判别器时，我们需要冻结生成器的训练；而在训练生成器时，我们需要冻结判别器的训练。
- en: Here is the batch training code incorporating the alternating training process
    of these two agents under the bi-level optimization framework.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是结合了交替训练过程的批量训练代码，这两个代理在双层优化框架下进行训练。
- en: '[PRE8]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Here, I have a quick question for you.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我有一个快速的问题要问你。
- en: Below we have an excerpt associated with the generator training from the code
    above.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我们展示了与上述代码中的生成器训练相关的一个代码片段。
- en: Can you explain what this code is doing?
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 你能解释一下这段代码的作用吗？
- en: '[PRE9]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In the first line, *noise* generates the synthetic data. In the second line,
    *valid_labels* assigns the label of the synthetic data.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一行，*noise* 生成了合成数据；在第二行，*valid_labels* 分配了合成数据的标签。
- en: Why do we need to label it with 1, which is supposed to be the label for the
    real data? Didn’t you find the code counter-intuitive?
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们需要用 1 来标注它，1 本应是实际数据的标签？难道你没有觉得这段代码让人有些困惑吗？
- en: Ladies and gentlemen, welcome to the world of counterfeiters.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 各位女士们，先生们，欢迎来到伪造者的世界。
- en: This is the labeling magic that trains the generator to create samples that
    can fool the discriminator.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这是标签化的魔法，它训练生成器生成能够欺骗判别器的样本。
- en: Now, let’s use the trained generator to create the synthetic data for the minority
    fraud class.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用训练好的生成器来创建少数欺诈类的合成数据。
- en: '[PRE10]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Finally, the synthetic data is created.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，合成数据已经创建完成。
- en: In the next section, we can combine this synthetic fraud data with the original
    train dataset to make the entire train dataset perfectly balanced. I hope that
    the perfectly balanced training dataset would improve the performance of the fraud
    detection classification model.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们可以将这些合成的欺诈数据与原始训练数据集结合，确保整个训练数据集达到完美平衡。我希望这个完美平衡的训练数据集能提高欺诈检测分类模型的性能。
- en: 'Section 4: Fraud Detection Overview (with and without GANs data augmentation)'
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4节：欺诈检测概述（有无 GAN 数据增强）
- en: Repeatedly, the use of GANs in this project is exclusively for data augmentation,
    but not for classification.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 一再强调，本项目中使用 GAN 的唯一目的是数据增强，而不是分类。
- en: First of all, we would need the benchmark model as the basis of the comparison
    in order for us to evaluate the improvement made by the GANs based data augmentation
    on the performance of the fraud detection model.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要基准模型作为比较的基础，以便我们评估基于 GAN 数据增强的欺诈检测模型性能的提升。
- en: 'As a binary classifier algorithm, I selected Ensemble Method for building the
    fraud detection model. As the benchmark scenario, I developed a fraud detection
    model only with the original imbalanced dataset: thus, without data augmentation.
    Then, for the second scenario with data augmentation by GANs, I can train the
    same algorithm with the perfectly balanced train dataset, which contains the synthetic
    fraud data created by GANs.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个二分类算法，我选择了集成方法来构建欺诈检测模型。作为基准场景，我仅使用原始不平衡数据集来开发欺诈检测模型：也就是没有数据增强。然后，在第二个使用GAN进行数据增强的场景中，我可以使用包含GAN生成的合成欺诈数据的完美平衡训练数据集来训练相同的算法。
- en: 'Benchmark Scenario: Ensemble Classifier without data augmentation'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基准场景：没有数据增强的集成分类器
- en: 'GANs Scenario: Ensemble Classifier with data augmentation by GANs'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GAN场景：使用GAN进行数据增强的集成分类器
- en: '***Benchmark Scenario: Ensemble without data augmentation***'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '***基准场景：没有数据增强的集成***'
- en: 'Next, let’s define the benchmark scenario (without data augmentation). I decided
    to select Ensemble Classifier: voting method as the meta learner with the following
    3 base learners.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们定义基准场景（没有数据增强）。我决定选择集成分类器：投票法作为元学习器，使用以下3个基础学习器。
- en: Gradient Boosting
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升
- en: Decision Tree
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树
- en: Random Forest
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林
- en: 'Since the original dataset is highly imbalanced, rather than accuracy I shall
    select evaluation metrics from the following 3 options: precision, recall, and
    F1-Score.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 由于原始数据集高度不平衡，除了准确率之外，我将从以下三个选项中选择评估指标：精确率、召回率和F1-Score。
- en: The following custom function, *ensemble_training(X_train, y_train)*, defines
    the training and validation process.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 以下自定义函数，*ensemble_training(X_train, y_train)*，定义了训练和验证过程。
- en: '[PRE11]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The next function block, *ensemble_evaluations(meta_learner, X_train, y_train,
    X_test, y_test)*, calculates the performance evaluation metrics at the meta learner
    level.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个函数模块，*ensemble_evaluations(meta_learner, X_train, y_train, X_test, y_test)*，在元学习器层面计算性能评估指标。
- en: '[PRE13]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Below, let’s look at the performance of the benchmark Ensemble Classifier.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看一下基准集成分类器的表现。
- en: '[PRE15]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: At the meta-learner level, the benchmark model generated F1-Score at a reasonable
    level of 0.8372.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在元学习器层面，基准模型生成的F1-Score在合理的水平0.8372。
- en: Next, let’s move on to the scenario with data augmentation using GANs . We want
    to see if the performance of the scenario with GAN can outperform the benchmark
    scenario.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们进入使用GAN进行数据增强的场景。我们想看看使用GAN的场景是否能超越基准场景的表现。
- en: '***GANs Scenario: Fraud Detection with data augmentation by GANs***'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '***GAN场景：使用GAN进行数据增强的欺诈检测***'
- en: Finally, we have constructed a perfectly balanced dataset by combining the original
    imbalanced train dataset (both non-fraud and fraud cases), *train_df*, and the
    synthetic fraud dataset generated by GANs, *fake_df*. Here, we will preserve the
    test dataset as original by not involving it in this process.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们通过将原始不平衡训练数据集（包括非欺诈和欺诈案例）、*train_df* 和通过GAN生成的合成欺诈数据集 *fake_df* 结合起来，构建了一个完美平衡的数据集。在这里，我们将原始测试数据集保留不变，不参与这个过程。
- en: '[PRE16]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We will train the same ensemble method with the mixed balanced dataset to see
    if it will outperform the benchmark model.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用混合的平衡数据集训练相同的集成方法，以查看它是否能够超越基准模型。
- en: Now, we need to split the mixed balanced dataset into the features and the label.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要将混合平衡数据集分割成特征和标签。
- en: '[PRE17]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Remember, when I ran the benchmark scenario earlier, I already defined the necessary
    custom function blocks to train and evaluate the ensemble classifier. I can use
    those custom functions here as well to train the same Ensemble algorithm with
    the combined balanced data.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，当我之前运行基准场景时，我已经定义了必要的自定义函数模块来训练和评估集成分类器。我也可以在这里使用这些自定义函数来训练相同的集成算法，使用合成的平衡数据集。
- en: We can pass the features and the label (X_mixed, y_mixed) into the custom Ensemble
    Classifier function ensemble_training().
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将特征和标签（X_mixed, y_mixed）传入自定义的集成分类器函数ensemble_training()。
- en: '[PRE18]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Finally, we can evaluate the model with the test dataset.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用测试数据集来评估模型。
- en: '[PRE19]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Here is the result.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是结果。
- en: '[PRE20]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '***Conclusion***'
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '***结论***'
- en: Finally, we can assess whether the data augmentation by GANs improved the performance
    of the classifier, as I expected.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以评估通过GAN进行数据增强是否提高了分类器的表现，正如我所预期的那样。
- en: Let’s compare the evaluation metrics between the benchmark scenario and GANs
    scenario.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较基准场景和GAN场景的评估指标。
- en: Here is the result from the benchmark scenario.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这是来自基准场景的结果。
- en: '[PRE21]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Here is the result from GANs scenario.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: When we review the evaluation results on the training dataset, clearly GANs
    scenario outperformed the benchmark scenario over all the three evaluation metrics.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, when we focus on the results on the out-of-sample test data,
    GANs scenario outperformed the benchmark scenario only for precision (Benchmark:
    0.935 vs GANs Scenario: 0.9714): it failed do so for recall and F1-Score (Benchmark:
    0.7579; 0.8372 vs GANs Scenario: 0.7158; 0.8242).'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: A higher precision means that the model’s prediction of fraud cases did include
    less proportion of non-fraud cases than the benchmark scenario.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A lower recall means that the model failed to detect certain varieties of the
    actual fraud cases.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These two comparisons indicate: while the data augmentation by GANs was successful
    in simulating the realistic fraud data within the training dataset, it has failed
    to capture the diversity of the actual fraud cases included in the out-of-sample
    test dataset.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: GANs was too good in simulating the particular probability distribution of the
    train data. Ironically, as a result, the use of GANs as the data augmentation
    tool, accounting for overfitting to the train data, resulted in a poor generalization
    of the resulting fraud detection (classification) model.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Paradoxically, this particular example made a counter-intuitive case that a
    better sophisticated algorithm might not necessarily guarantee a better performance
    when compared with simpler conventional algorithms.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, we could also take into account of another unintended consequence,
    wasteful carbon footprint: adding energy demanding algorithms into your model
    development could increase the carbon footprint in the use of the machine learning
    in our daily life. This case could illustrate an example of an unnecessarily wasteful
    case which wasted energy unnecessarily without delivering a better performance.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Here I leave you some links regarding energy consumption of machine learning.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[https://spectrum.ieee.org/ai-energy-consumption](https://spectrum.ieee.org/ai-energy-consumption)'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.cell.com/joule/fulltext/S2542-4351(23)00365-3?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS2542435123003653%3Fshowall%3Dtrue](https://www.cell.com/joule/fulltext/S2542-4351(23)00365-3?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS2542435123003653%3Fshowall%3Dtrue)'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Today, we have many variants of GANs. In the future article, I would like to
    explore other variants of GANs to see if any variant can capture a wider diversity
    of the original samples so that it can improve the performance of a fraud detector.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Michio Suginoo
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: REFERENCE
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Borji, A. (2018, 10 24). *Pros and Cons of GAN Evaluation Measures.* Retrieved
    from ArXvi: [https://arxiv.org/abs/1802.03446](https://arxiv.org/abs/1802.03446)'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goodfellow, I. (2015, 5 21). *On distinguishability criteria for estimating
    generative models.* Retrieved from ArXiv: [https://arxiv.org/abs/1412.6515](https://arxiv.org/abs/1412.6515)'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D.,
    Ozairy, S., . . . Bengioz, Y. (2014, 6 10). *Generative Adversarial Nets.* Retrieved
    from arXiv: [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D.,
    Ozairy, S., . . . Bengioz, Y. (2014, 6 10). *Generative Adversarial Networks.*
    Retrieved from arXiv: [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Knight, W. (2018, 8 17). *Fake America great again.* Retrieved from MIT Technology
    Review: [https://www.technologyreview.com/2018/08/17/240305/fake-america-great-again/](https://www.technologyreview.com/2018/08/17/240305/fake-america-great-again/)'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Suginoo, M. (2024, 1 13). *Mini-Max Optimization Design of Generative Adversarial
    Networks (GAN).* Retrieved from Towards Data Science: [https://towardsdatascience.com/mini-max-optimization-design-of-generative-adversarial-networks-gan-dc1b9ea44a02](/mini-max-optimization-design-of-generative-adversarial-networks-gan-dc1b9ea44a02)'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
