- en: 'Transforming Data Quality: Automating SQL Testing for Faster, Smarter Analytics'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/transforming-data-quality-automating-sql-testing-for-faster-smarter-analytics-6da431493570?source=collection_archive---------0-----------------------#2024-10-26](https://towardsdatascience.com/transforming-data-quality-automating-sql-testing-for-faster-smarter-analytics-6da431493570?source=collection_archive---------0-----------------------#2024-10-26)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to test the quality of SQL and resultant dataset against the business question
    to increase trust with customers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@hello.akashm?source=post_page---byline--6da431493570--------------------------------)[![Akash
    Mukherjee](../Images/b083a1dd6d07935792eae58edd563ebf.png)](https://medium.com/@hello.akashm?source=post_page---byline--6da431493570--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6da431493570--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6da431493570--------------------------------)
    [Akash Mukherjee](https://medium.com/@hello.akashm?source=post_page---byline--6da431493570--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6da431493570--------------------------------)
    ·11 min read·Oct 26, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4436d79ec654db529fca075e12919af5.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Caspar Camille Rubin](https://unsplash.com/@casparrubin?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/macbook-pro-with-images-of-computer-language-codes-fPkvU7RDmCo?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to software development, there are plenty of automated testing
    tools and frameworks to rely on. But for analytics teams, manual testing and data
    quality assurance (QA) are still the norm. Too often, it’s the customer or business
    team who first spots issues with data quality or completeness, rather than the
    analytics team.
  prefs: []
  type: TYPE_NORMAL
- en: That’s where automation can make a huge difference. By setting up an automated
    system with scripts to run data quality tests at scale, you can keep things running
    fast without sacrificing the accuracy or completeness of your data.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this gets trickier when business questions are vague or open-ended.
    In those cases, a mix of rule-based logic and large language models (LLMs) can
    really help — allowing you to generate scenarios and run automated checks. In
    this tutorial, we’ll walk through how to build an automated testing system that
    evaluates and scores the quality of your data and SQL queries, even when the business
    questions are written in plain English.
  prefs: []
  type: TYPE_NORMAL
- en: What You’ll Need Before We Start
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To follow along with this tutorial, make sure you have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A solid understanding of databases and SQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experience with Python for API calls and handling data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to GPT-4 API tokens
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A dataset of business questions for testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing the System Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To build an automated QA system for evaluating SQL queries, the architecture
    must integrate rule-based logic, LLM validation, and automated scoring. This setup
    is perfect for handling those open-ended business questions, letting you scale
    your testing beyond manual processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Key components include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Query Ingestion Engine**: Where SQL queries are received and executed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluation Module**: Combines static rules with LLM-based to validate the
    results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scoring System**: Grades the results based on different user roles like Data
    Scientists, Business Leaders, and End Users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The architecture includes a feedback loop that logs issue types–things like
    missing data, wrong granularity, or slow performance. This information get stored
    in a centralized database, so you can keep optimizing the system over time. We
    will use Python for scripting, SQL for tracking backend issues, and OpenAI’s LLM
    for interpreting natural language inputs. By scheduling these tests to run regularly,
    you’ll maintain consistent data quality and scalability, while also fine-tuning
    query performance to align with business goals.
  prefs: []
  type: TYPE_NORMAL
- en: The diagram below shows how data flows through the system — from SQL ingestion
    to automated testing, scoring, and issue tracking — so you can maintain high data
    quality at scale.
  prefs: []
  type: TYPE_NORMAL
- en: In the end, this system doesn’t just catch errors — it drives continuous improvement
    and keeps your technical execution aligned with business objectives.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/64922a96612de5060deab240c1c68a64.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by the author: Diagram created to illustrate technical architecture'
  prefs: []
  type: TYPE_NORMAL
- en: Tutorial
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Step 1: Prepare Dataset of Test Questions & Answers'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To get started, collect real business questions that your internal teams or
    customers frequently ask the analytics team. Many of these might be ad-hoc data
    requests, so by having a variety of questions on hand you can make sure your testing
    is relevant. Here are a few examples to get you going:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Question #1**: “How many of our Pro Plan users are converting from a trial?”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Question #2**: “How many new users did we bring on in June 2024?”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Question #3**: “What products are trending right now?”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Question #4**: “What’s the current sales volume for our top products?”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 2: Building Your Evaluation & Scoring Criteria'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**2a: Define Your Graders**'
  prefs: []
  type: TYPE_NORMAL
- en: 'For thorough testing, set up graders from different perspectives to cover all
    bases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**End User**: Focuses on usability and clarity. Is the result easy to interpret?
    Does it address the original business question directly?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data Scientist**: Evaluates technical accuracy and completeness. Are all
    the necessary datasets included? Is the analysis detailed and reproducible?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Business Leader**: Looks for alignment with strategic goals. Does the output
    support decision-making in line with business objectives?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2b: Define Scoring Criteria**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each grader should assess queries based on specific factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy**: Does the query provide the right answer? Are any data points
    missing or misinterpreted?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Relevance**: Does the output contain all the necessary data while excluding
    irrelevant information?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logic**: Is the query well-structured? Are joins, filters, and aggregations
    applied correctly?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficiency**: Is the query optimized for performance without extra complexity
    or delays?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2c: Track and Log Issue Types**'
  prefs: []
  type: TYPE_NORMAL
- en: To cover all bases, it’s important to log common issues that arise during query
    execution. This makes it easier to tag and run automated evaluations later on.
  prefs: []
  type: TYPE_NORMAL
- en: '**Wrong Granularity**: Data is returned at an incorrect level of detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Excessive Columns**: The result includes unnecessary fields, creating clutter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missing Data**: Critical data is missing from the output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Incorrect Values**: Calculations or values are wrong.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance Issues**: The query runs inefficiently, taking too long to execute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]json'
  prefs: []
  type: TYPE_NORMAL
- en: '{{'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"endUser": {{"overallScore": "", "criteriaScores": {{"accuracy": "", "relevance":
    "", "logic": "", "efficiency": ""}}, "issueTags": [], "otherObservations": []}},'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"dataScientist": {{"overallScore": "", "criteriaScores": {{"accuracy": "",
    "relevance": "", "logic": "", "efficiency": ""}}, "issueTags": [], "otherObservations":
    []}},'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"businessLeader": {{"overallScore": "", "criteriaScores": {{"accuracy": "",
    "relevance": "", "logic": "", "efficiency": ""}}, "issueTags": [], "otherObservations":
    []}}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '}}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Step 3: Automate the Testing'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**3a: Loop Through the Questions**'
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve gathered your business questions, set up a loop to feed each question,
    its related SQL query, and the results into your evaluation function. This lets
    you automate the entire evaluation process, making sure that each query is scored
    consistently.
  prefs: []
  type: TYPE_NORMAL
- en: '**3b: Schedule Regular Runs**'
  prefs: []
  type: TYPE_NORMAL
- en: Automate the testing process by scheduling the script to run regularly — ideally
    after each data refresh or query update. This keeps the testing in sync with your
    data, catching any issues as soon as they arise.
  prefs: []
  type: TYPE_NORMAL
- en: '**3c: Log Scores, Tags, and Observations in a Database**'
  prefs: []
  type: TYPE_NORMAL
- en: For each test run, log all scores, issue tags, and observations in a structured
    database. Use the Python script to populate a table (e.g., **issue_catalog**)
    with the relevant data. This gives you a history of evaluations to track trends,
    pinpoint frequent issues, and optimize future testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Reporting Test Outcomes'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**4a: Pivot & Group by Scores**'
  prefs: []
  type: TYPE_NORMAL
- en: Leverage SQL queries or BI tools to create pivot tables that group your results
    by overall scores and specific criteria like accuracy, relevance, logic, and efficiency.
    This helps you spot trends in performance and figure out which areas need the
    most attention.
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate an overall score for each query across all graders, use a weighted
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: Overall Score = w1​×Accuracy + w2​×Relevance + w3​×Logic + w4​×Efficiency
  prefs: []
  type: TYPE_NORMAL
- en: Where w1​, w2​, w3​, w4​ are the weights assigned to each scoring criterion.
    The sum of these weights should equal 1 for normalization.
  prefs: []
  type: TYPE_NORMAL
- en: For example, you might assign higher weight to **Accuracy** for Data Scientists
    and higher weight to **Relevance** for Business Leaders, depending on their priorities.
  prefs: []
  type: TYPE_NORMAL
- en: '**4b: Highlight Top Issues**'
  prefs: []
  type: TYPE_NORMAL
- en: Identify the most frequent and critical issues — things like **missing data**,
    **wrong granularity**, or **performance inefficiencies**. Provide a detailed report
    that breaks down how often these issues occur and which types of queries are most
    affected.
  prefs: []
  type: TYPE_NORMAL
- en: Focus on patterns that could lead to more significant errors if left unaddressed.
    For example, highlight cases where data quality issues might have skewed decision-making
    or slowed down business processes.
  prefs: []
  type: TYPE_NORMAL
- en: Prioritize the issues that need immediate action, such as those affecting query
    performance or accuracy in key datasets, and outline clear next steps to resolve
    them.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b2484b2780ed989ca27648b77d518b00.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by the author: Chart Created using Sample Test Data'
  prefs: []
  type: TYPE_NORMAL
- en: '**4c: Analyze Variance of Graders**'
  prefs: []
  type: TYPE_NORMAL
- en: Look closely at any discrepancies between scores from different graders (End
    User, Data Scientist, Business Leader). Large differences can reveal potential
    misalignments between the technical execution and business objectives.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if a query scores high in technical accuracy but low in relevance
    to the business question, this signals a gap in translating data insights into
    actionable outcomes. Similarly, if the End User finds the results hard to interpret,
    but the Data Scientist finds them technically sound, it may point to communication
    or presentation issues.
  prefs: []
  type: TYPE_NORMAL
- en: By tracking these differences, you can better align the analytics process with
    both technical precision and business value, keeping all stakeholders satisfied.
  prefs: []
  type: TYPE_NORMAL
- en: 'To quantify this variance, you can calculate the variance of the graders’ scores.
    First, define the individual scores as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**S-EndUser​:** The overall score from the End User.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**S-DataScientist​:** The overall score from the Data Scientist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**S-BusinessLeader**​: The overall score from the Business Leader.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The mean score **μ** across the three graders can be calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: μ = (S-EndUser​ + S-DataScientist​ + S-BusinessLeader​​) / 3
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, calculate the variance **σ²**, which is the average of the squared differences
    between each grader’s score and the mean score. The formula for variance is:'
  prefs: []
  type: TYPE_NORMAL
- en: σ**²** = (S-EndUser − μ)**²** + (S-DataScientist − μ)**² +** (S-BusinessLeader
    − μ)**²**/ 3
  prefs: []
  type: TYPE_NORMAL
- en: By calculating this variance, you can objectively measure how much the graders’
    scores differ.
  prefs: []
  type: TYPE_NORMAL
- en: Large variances suggest that one or more graders perceive the quality of the
    query or relevance differently, which may indicate a need for better alignment
    between technical output and business needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 5: Create a Feedback Loop'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**5a: Pinpoint Key Issues**'
  prefs: []
  type: TYPE_NORMAL
- en: Throughout your testing process, you’ll likely notice certain issues cropping
    up repeatedly. These might include **missing data**, **incorrect values**, **wrong
    granularity**, or **performance inefficiencies**. It’s important to not only log
    these issues but also categorize and prioritize them.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if critical data is missing, that should be addressed immediately,
    while performance tweaks can be considered as longer-term optimizations. By focusing
    on the most impactful and recurring problems, you’ll be able to improve data quality
    and tackle the root causes more effectively.
  prefs: []
  type: TYPE_NORMAL
- en: '**5b: Refine Your SQL Queries**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you’ve identified the recurring issues, it’s time to update your SQL
    queries to resolve them. This involves refining query logic to achieve accurate
    joins, filters, and aggregations. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: If you encounter **wrong granularity**, adjust the query to aggregate data at
    the appropriate level.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For **missing data**, make sure all relevant tables are joined correctly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there are **performance problems**, simplify the query, add indexes, or use
    more efficient SQL functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal here is to translate the feedback you’ve logged into tangible improvements
    in your SQL code, making your future queries more precise, relevant, and efficient.
  prefs: []
  type: TYPE_NORMAL
- en: '**5c: Re-Test for Validation**'
  prefs: []
  type: TYPE_NORMAL
- en: Once your queries have been optimized, re-run the tests to verify the improvements.
    Automating this step ensures that your updated queries are consistently evaluated
    against new data or business questions. Running the tests again allows you to
    confirm that your changes have fixed the issues and improved overall data quality.
    It also helps confirm that your SQL queries are fully aligned with business needs,
    which can enable quicker and more accurate insights. If any new issues arise,
    simply feed them back into the loop for continuous improvement.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example Code for Automating the Feedback Loop**'
  prefs: []
  type: TYPE_NORMAL
- en: 'To automate this feedback loop, here is a Python script that processes multiple
    test cases (including business questions, SQL queries, and results), evaluates
    them using OpenAI’s API, and stores the results in a database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Setting Up the Issue Catalog Table**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The *issue_catalog* table serves as the main repository for storing detailed
    test results, giving you a clear way to track query performance and flag issues
    over time. By using JSONB format for storing issue tags and observations, you
    gain flexibility, allowing you to log complex information without needing to update
    the database schema frequently. Here’s the SQL code for setting it up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**What This Feedback Loop Accomplishes**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Continuous Improvement**: By keeping track of issues over time, you’ll be
    able to fine-tune your SQL queries and steadily boost their quality. Each test
    run delivers actionable insights, and by targeting the most frequent problems,
    your system becomes more efficient and resilient with every pass.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data Quality Assurance**: Running tests regularly on updated SQL queries
    helps you verify that they handle new data and test cases correctly. This ongoing
    process shows whether your adjustments are truly improving data quality and keeping
    everything aligned with business needs, lowering the risk of future issues.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Alignment with Business Needs**: Sorting issues based on who raised them
    — whether it’s an End User, Data Scientist, or Business Leader — lets you zero
    in on improvements that matter to both technical accuracy and business relevance.
    Over time, this builds a system where technical efforts directly support meaningful
    business insights.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Scalable Testing and Optimization**: This approach scales smoothly as you
    add more test cases. As your issue catalog expands, patterns emerge, making it
    easier to fine-tune queries that affect a wide range of business questions. With
    each iteration, your testing framework gets stronger, driving continuous improvements
    in data quality at scale.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Automating SQL testing is a game-changer for analytics teams, helping them catch
    data issues early and resolve them with precision. By setting up a structured
    feedback loop that combines rule-based logic with LLMs, you can scale testing
    to handle even the most complex business questions.
  prefs: []
  type: TYPE_NORMAL
- en: This approach not only sharpens data accuracy but also keeps your insights aligned
    with business goals. The future of analytics depends on this balance between automation
    and insight — are you ready to make that leap?
  prefs: []
  type: TYPE_NORMAL
