- en: 'Gradient Boosting Regressor, Explained: A Visual Guide with Code Examples'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/gradient-boosting-regressor-explained-a-visual-guide-with-code-examples-c098d1ae425c?source=collection_archive---------1-----------------------#2024-11-14](https://towardsdatascience.com/gradient-boosting-regressor-explained-a-visual-guide-with-code-examples-c098d1ae425c?source=collection_archive---------1-----------------------#2024-11-14)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ENSEMBLE LEARNING
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fitting to errors one booster stage at a time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--c098d1ae425c--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--c098d1ae425c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c098d1ae425c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c098d1ae425c--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--c098d1ae425c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c098d1ae425c--------------------------------)
    ·11 min read·Nov 14, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef?source=post_page-----c098d1ae425c--------------------------------)
    [## Decision Tree Regressor, Explained: A Visual Guide with Code Examples'
  prefs: []
  type: TYPE_NORMAL
- en: Trimming branches smartly with Cost-Complexity Pruning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef?source=post_page-----c098d1ae425c--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Of course, in machine learning, we want our predictions spot on. We started
    with [simple decision trees](https://medium.com/towards-data-science/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e)
    — they worked okay. Then came [Random Forests](https://medium.com/towards-data-science/random-forest-explained-a-visual-guide-with-code-examples-9f736a6e1b3c)
    and [AdaBoost](https://medium.com/towards-data-science/adaboost-classifier-explained-a-visual-guide-with-code-examples-fc0f25326d7b),
    which did better. But Gradient Boosting? That was a game-changer, making predictions
    way more accurate.
  prefs: []
  type: TYPE_NORMAL
- en: 'They said, “What makes Gradient Boosting work so well is actually simple: it
    builds models one after another, where each new model focuses on fixing the mistakes
    of all previous models combined. This way of fixing errors step by step is what
    makes it special.” I thought it’s really gonna be that simple but *every time*
    I look up Gradient Boosting, trying to understand how it works, I see the same
    thing: rows and rows of complex math formulas and ugly charts that somehow drive
    me insane. Just try it.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s put a stop to this and break it down in a way that actually makes sense.
    We’ll visually navigate through the training steps of Gradient Boosting, focusing
    on a regression case — a simpler scenario than classification — so we can avoid
    the confusing math. Like a multi-stage rocket shedding unnecessary weight to reach
    orbit, we’ll blast away those prediction errors one residual at a time.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2d6078ab2a677250978f6a47f6460221.png)'
  prefs: []
  type: TYPE_IMG
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Gradient Boosting is an ensemble machine learning technique that builds a series
    of decision trees, each aimed at correcting the errors of the previous ones. Unlike
    AdaBoost, which uses shallow trees, Gradient Boosting uses deeper trees as its
    weak learners. Each new tree focuses on minimizing the residual errors — the differences
    between actual and predicted values — rather than learning directly from the original
    targets.
  prefs: []
  type: TYPE_NORMAL
- en: For regression tasks, Gradient Boosting adds trees one after another with each
    new tree is trained to reduce the remaining errors by addressing the current residual
    errors. The final prediction is made by adding up the outputs from all the trees.
  prefs: []
  type: TYPE_NORMAL
- en: The model’s strength comes from its additive learning process — while each tree
    focuses on correcting the remaining errors in the ensemble, the sequential combination
    creates a powerful predictor that **progressively reduces the overall prediction
    error** by focusing on the parts of the problem where the model still struggles.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3ac141fa57a3f0707e680165024c850a.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradient Boosting is part of the boosting family of algorithms because it builds
    trees sequentially, with each new tree trying to correct the errors of its predecessors.
    However, unlike other boosting methods, Gradient Boosting approaches the problem
    from an optimization perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset Used
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Throughout this article, we’ll focus on the classic golf dataset as an example
    for regression. While Gradient Boosting can handle both regression and classification
    tasks effectively, we’ll concentrate on the simpler task which in this case is
    the regression — predicting the number of players who will show up to play golf
    based on weather conditions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dfe988f34df90e6d6bc95d67f04271db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Columns: ‘Overcast (one-hot-encoded into 3 columns)’, ’Temperature’ (in Fahrenheit),
    ‘Humidity’ (in %), ‘Windy’ (Yes/No) and ‘Number of Players’ (target feature)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Main Mechanism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here’s how Gradient Boosting works:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Initialize Model:** Start with a simple prediction, typically the mean of
    target values.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Iterative Learning:** For a set number of iterations, compute the residuals,
    train a decision tree to predict these residuals, and add the new tree’s predictions
    (scaled by the learning rate) to the running total.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Build Trees on Residuals:** Each new tree focuses on the remaining errors
    from all previous iterations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Final Prediction:** Sum up all tree contributions (scaled by the learning
    rate) and the initial prediction.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/5aebdd14999e70e5924a74ce7ef351b5.png)'
  prefs: []
  type: TYPE_IMG
- en: A Gradient Boosting Regressor starts with an average prediction and improves
    it through multiple trees, each one fixing the previous trees’ mistakes in small
    steps, until reaching the final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Training Steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll follow the standard gradient boosting approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.0\. Set Model Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before building any trees, we need set the core parameters that control the
    learning process:'
  prefs: []
  type: TYPE_NORMAL
- en: · the number of trees (typically 100, but we’ll choose 50) to build sequentially,
  prefs: []
  type: TYPE_NORMAL
- en: · the learning rate (typically 0.1), and
  prefs: []
  type: TYPE_NORMAL
- en: · the maximum depth of each tree (typically 3)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/11972f0214d401ffdf105a994791fc6b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A tree diagram showing our key settings: each tree will have 3 levels, and
    we’ll create 50 of them while moving forward in small steps of 0.1.'
  prefs: []
  type: TYPE_NORMAL
- en: For the First Tree
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.0 Make an initial prediction for the label. This is typically the mean (just
    like [a dummy prediction](/dummy-regressor-explained-a-visual-guide-with-code-examples-for-beginners-4007c3d16629).)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0583a69027986dee2fda863439108614.png)'
  prefs: []
  type: TYPE_IMG
- en: To start our predictions, we use the average value (37.43) of all our training
    data as the first guess for every case.
  prefs: []
  type: TYPE_NORMAL
- en: '2.1\. Calculate temporary residual (or pseudo-residuals):'
  prefs: []
  type: TYPE_NORMAL
- en: residual = actual value — predicted value
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5f135dbb9d7034726d771919188d31f6.png)'
  prefs: []
  type: TYPE_IMG
- en: Calculating the initial residuals by subtracting the mean prediction (37.43)
    from each target value in our training set.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Build a decision tree to **predict these residuals.** The tree building
    steps are exactly the same as in the [regression tree](https://medium.com/towards-data-science/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/77364ffaaeca757af50cfa5a758e12b1.png)'
  prefs: []
  type: TYPE_IMG
- en: The first decision tree begins its training by searching for patterns in our
    features that can best predict the calculated residuals from our initial mean
    prediction.
  prefs: []
  type: TYPE_NORMAL
- en: a. Calculate initial MSE (Mean Squared Error) for the root node
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/44736650f521e040f1c4b17edd948edc.png)'
  prefs: []
  type: TYPE_IMG
- en: Just like in regular regression trees, we calculate the Mean Squared Error (MSE),
    but this time we’re measuring the spread of residuals (around zero) instead of
    actual values (around their mean).
  prefs: []
  type: TYPE_NORMAL
- en: 'b. For each feature:'
  prefs: []
  type: TYPE_NORMAL
- en: · Sort data by feature values
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b2f725b3c389f832749b26326e214e85.png)'
  prefs: []
  type: TYPE_IMG
- en: For each feature in our dataset, we sort its values and find potential split
    points between them, just as we would in a standard decision tree, to determine
    the best way to divide our residuals.
  prefs: []
  type: TYPE_NORMAL
- en: '· For each possible split point:'
  prefs: []
  type: TYPE_NORMAL
- en: ·· Split samples into left and right groups
  prefs: []
  type: TYPE_NORMAL
- en: ·· Calculate MSE for both groups
  prefs: []
  type: TYPE_NORMAL
- en: ·· Calculate MSE reduction for this split
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7082a68606950c5a393205a4e8694ad0.png)'
  prefs: []
  type: TYPE_IMG
- en: Similar to a regular regression tree, we evaluate each split by calculating
    the weighted MSE of both groups, but here we’re measuring how well the split groups
    similar residuals rather than similar target values.
  prefs: []
  type: TYPE_NORMAL
- en: c. Pick the split that gives the largest MSE reduction
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/26c4db7c3842cc6336ef543ffe94653d.png)![](../Images/164adc4d983357b7bd928394075c5cbb.png)'
  prefs: []
  type: TYPE_IMG
- en: The tree makes its first split using the “rain” feature at value 0.5, dividing
    samples into two groups based on their residuals — this first decision will be
    refined by additional splits at deeper levels.
  prefs: []
  type: TYPE_NORMAL
- en: d. Continue splitting until reaching maximum depth or minimum samples per leaf.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dfb7d12a8ebb84f85be346d2ab27e286.png)'
  prefs: []
  type: TYPE_IMG
- en: After three levels of splitting on different features, our first tree has created
    eight distinct groups, each with its own prediction for the residuals.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3\. Calculate Leaf Values
  prefs: []
  type: TYPE_NORMAL
- en: For each leaf, find the **mean of residuals**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fbb6b98089b4529f42201e77bbdeca1d.png)'
  prefs: []
  type: TYPE_IMG
- en: Each leaf in our first tree contains an average of the residuals in that group
    — these values will be used to adjust and improve our initial mean prediction
    of 37.43.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4\. Update Predictions
  prefs: []
  type: TYPE_NORMAL
- en: · For each data point in the **training dataset**, determine which leaf it falls
    into based on the new tree.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/df18786126f2894b4b244d4fde2b9192.png)'
  prefs: []
  type: TYPE_IMG
- en: Running our training data through the first tree, each sample follows its own
    path based on weather features to get its predicted residual value, which will
    help correct our initial prediction.
  prefs: []
  type: TYPE_NORMAL
- en: · Multiply the new tree’s predictions by the learning rate and add these scaled
    predictions to the current model’s predictions. This will be the updated prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/76a4aa18c7ed15a837c54901efb09dcc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our model updates its predictions by taking small steps: it adds just 10% (our
    learning rate of 0.1) of each predicted residual to our initial prediction of
    37.43, creating slightly improved predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: For the Second Tree
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1\. Calculate new residuals based on current model
  prefs: []
  type: TYPE_NORMAL
- en: a. Compute the difference between the target and current predictions.
  prefs: []
  type: TYPE_NORMAL
- en: These residuals will be a bit different from the first iteration.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2843e92a51f3b6bd97696bf603c17128.png)'
  prefs: []
  type: TYPE_IMG
- en: After updating our predictions with the first tree, we calculate new residuals
    — notice how they’re slightly smaller than the original ones, showing our predictions
    are gradually improving.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Build a new tree to predict these residuals. Same process as first tree,
    but targeting new residuals.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bd50d352c28d235ec429312a6fc54dd7.png)'
  prefs: []
  type: TYPE_IMG
- en: Starting our second tree to predict the new, smaller residuals — we’ll use the
    same tree-building process as before, but now we’re trying to catch the errors
    our first tree missed
  prefs: []
  type: TYPE_NORMAL
- en: 2.3\. Calculate the mean residuals for each leaf
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b5226feb595f93476c22f6f3acfb4488.png)'
  prefs: []
  type: TYPE_IMG
- en: The second tree follows an identical structure to our first tree with the same
    weather features and split points, but with smaller values in its leaves — showing
    we’re fine-tuning the remaining errors.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4\. Update model predictions
  prefs: []
  type: TYPE_NORMAL
- en: · Multiply the new tree’s predictions by the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: · Add the new scaled tree predictions to the running total.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/269aeb6fbb665e5f8d830fff8afb22f0.png)'
  prefs: []
  type: TYPE_IMG
- en: After running our data through the second tree, we again take small steps with
    our 0.1 learning rate to update predictions, and calculate new residuals that
    are even smaller than before — our model is gradually learning the patterns.
  prefs: []
  type: TYPE_NORMAL
- en: For the Third Tree onwards
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Repeat Steps 2.1–2.3 for remaining iterations. Note that each tree sees different
    residuals.
  prefs: []
  type: TYPE_NORMAL
- en: · Trees progressively focus on harder-to-predict patterns
  prefs: []
  type: TYPE_NORMAL
- en: · Learning rate prevents overfitting by limiting each tree’s contribution
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c3bec57f3cfc4d79ab75c562246a8250.png)'
  prefs: []
  type: TYPE_IMG
- en: As we build more trees, notice how the split points slowly shift and the residual
    values in the leaves get smaller — by tree 50, we’re making tiny adjustments using
    different combinations of features compared to our first trees.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/8a98aab772c6bdcf801764e343cb4c17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Visualization from scikit-learn shows how our gradient boosting trees evolve:
    from Tree 1 making large splits with big prediction values, to Tree 50 making
    refined splits with tiny adjustments — each tree focuses on correcting the remaining
    errors from previous trees.'
  prefs: []
  type: TYPE_NORMAL
- en: Testing Step
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For predicting:'
  prefs: []
  type: TYPE_NORMAL
- en: a. Start with the initial prediction (the average number of players)
  prefs: []
  type: TYPE_NORMAL
- en: b. Run the input through each tree to get its predicted adjustment
  prefs: []
  type: TYPE_NORMAL
- en: c. Scale each tree’s prediction by the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: d. Add all these adjustments to the initial prediction
  prefs: []
  type: TYPE_NORMAL
- en: e. The sum directly gives us the predicted number of players
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b7ce876ba3cdfad96da7c7e0415989f.png)'
  prefs: []
  type: TYPE_IMG
- en: When predicting on unseen data, each tree contributes its small prediction,
    starting from 5.57 in Tree 1 down to 0.008 in Tree 50 — all these predictions
    are scaled by our 0.1 learning rate and added to our base prediction of 37.43
    to get the final answer.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Step
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After building all the trees, we can evaluate the test set.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/32381bf34c6c5780246b6047af2863bd.png)'
  prefs: []
  type: TYPE_IMG
- en: Our gradient boosting model achieves an RMSE of 4.785, quite an improvement
    over [a single regression tree’s 5.27](https://medium.com/towards-data-science/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef)
    — showing how combining many small corrections leads to better predictions than
    one complex tree!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Key Parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the key parameters for Gradient Boosting, particularly in `scikit-learn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_depth`: The depth of trees used to model residuals. Unlike AdaBoost which
    uses stumps, Gradient Boosting works better with deeper trees (typically 3-8 levels).
    Deeper trees capture more complex patterns but risk overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_estimators`: The number of trees to be used (typically 100-1000). More trees
    usually improve performance when paired with a small learning rate.'
  prefs: []
  type: TYPE_NORMAL
- en: '`learning_rate`: Also called "shrinkage", this scales each tree''s contribution
    (typically 0.01-0.1). Smaller values require more trees but often give better
    results by making the learning process more fine-grained.'
  prefs: []
  type: TYPE_NORMAL
- en: '`subsample`: The fraction of samples used to train each tree (typically 0.5-0.8).
    This optional feature adds randomness that can improve robustness and reduce overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These parameters work together: a small learning rate needs more trees, while
    deeper trees might need a smaller learning rate to avoid overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: Key differences from AdaBoost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Both AdaBoost and Gradient Boosting are boosting algorithms, but the way they
    learn from their mistakes are different. Here are the key differences:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_depth` is typically higher (3-8) in Gradient Boosting, while AdaBoost
    prefers stumps.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: No `sample_weight` updates because Gradient Boosting uses residuals instead
    of sample weighting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `learning_rate` is typically much smaller (0.01-0.1) compared to AdaBoost's
    larger values (0.1-1.0).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initial prediction starts from the mean while AdaBoost starts from zero.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Trees are combined through simple addition rather than weighted voting, making
    each tree’s contribution more straightforward.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Optional `subsample` parameter adds randomness, a feature not present in standard
    AdaBoost.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pros & Cons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pros:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Step-by-Step Error Fixing:** In Gradient Boosting, each new tree focuses
    on correcting the mistakes made by the previous ones. This makes the model better
    at improving its predictions in areas where it was previously wrong.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexible Error Measures:** Unlike AdaBoost, Gradient Boosting can optimize
    different types of error measurements (like mean absolute error, mean squared
    error, or others). This makes it adaptable to various kinds of problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High Accuracy:** By using more detailed trees and carefully controlling the
    learning rate, Gradient Boosting often provides more accurate results than other
    algorithms, especially for well-structured data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cons:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Risk of Overfitting:** The use of deeper trees and the sequential building
    process can cause the model to fit the training data too closely, which may reduce
    its performance on new data. This requires careful tuning of tree depth, learning
    rate, and the number of trees.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Slow Training Process:** Like AdaBoost, trees must be built one after another,
    making it slower to train compared to algorithms that can build trees in parallel,
    like Random Forest. Each tree relies on the errors of the previous ones.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High Memory Use:** The need for deeper and more numerous trees means Gradient
    Boosting can consume more memory than simpler boosting methods such as AdaBoost.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sensitive to Settings:** The effectiveness of Gradient Boosting heavily depends
    on finding the right combination of learning rate, tree depth, and number of trees,
    which can be more complex and time-consuming than tuning simpler algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Final Remarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gradient Boosting is a major improvement in boosting algorithms. This success
    has led to popular versions like XGBoost and LightGBM, which are widely used in
    machine learning competitions and real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: While Gradient Boosting requires more careful tuning than simpler algorithms
    — especially when adjusting the depth of decision trees, the learning rate, and
    the number of trees — it is very flexible and powerful. This makes it a top choice
    for problems with structured data.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Boosting can handle complex relationships that simpler methods like
    AdaBoost might miss. Its continued popularity and ongoing improvements show that
    the approach of using gradients and building models step-by-step remains highly
    important in modern machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 🌟 Gradient Boosting Regressor Code Summarized
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a detailed explanation of the [GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)
    and its implementation in scikit-learn, readers can refer to the official documentation,
    which provides comprehensive information on its usage and parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Technical Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article uses Python 3.7 and scikit-learn 1.6\. While the concepts discussed
    are generally applicable, specific code implementations may vary slightly with
    different versions.
  prefs: []
  type: TYPE_NORMAL
- en: About the Illustrations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unless otherwise noted, all images are created by the author, incorporating
    licensed design elements from Canva Pro.
  prefs: []
  type: TYPE_NORMAL
- en: '𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝙀𝙣𝙨𝙚𝙢𝙗𝙡𝙚 𝙇𝙚𝙖𝙧𝙣𝙞𝙣𝙜 𝙝𝙚𝙧𝙚:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----c098d1ae425c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----c098d1ae425c--------------------------------)4
    stories![](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----c098d1ae425c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Regression Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----c098d1ae425c--------------------------------)5
    stories![A cartoon doll with pigtails and a pink hat. This “dummy” doll, with
    its basic design and heart-adorned shirt, visually represents the concept of a
    dummy regressor in machine. Just as this toy-like figure is a simplified, static
    representation of a person, a dummy regressor is a basic models serve as baselines
    for more sophisticated analyses.](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)'
  prefs: []
  type: TYPE_NORMAL
