- en: 'Gradient Boosting Regressor, Explained: A Visual Guide with Code Examples'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度提升回归器详解：带有代码示例的视觉指南
- en: 原文：[https://towardsdatascience.com/gradient-boosting-regressor-explained-a-visual-guide-with-code-examples-c098d1ae425c?source=collection_archive---------1-----------------------#2024-11-14](https://towardsdatascience.com/gradient-boosting-regressor-explained-a-visual-guide-with-code-examples-c098d1ae425c?source=collection_archive---------1-----------------------#2024-11-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/gradient-boosting-regressor-explained-a-visual-guide-with-code-examples-c098d1ae425c?source=collection_archive---------1-----------------------#2024-11-14](https://towardsdatascience.com/gradient-boosting-regressor-explained-a-visual-guide-with-code-examples-c098d1ae425c?source=collection_archive---------1-----------------------#2024-11-14)
- en: ENSEMBLE LEARNING
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成学习
- en: Fitting to errors one booster stage at a time
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一次一个增强阶段地拟合误差
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--c098d1ae425c--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--c098d1ae425c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c098d1ae425c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c098d1ae425c--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--c098d1ae425c--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@samybaladram?source=post_page---byline--c098d1ae425c--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--c098d1ae425c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c098d1ae425c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c098d1ae425c--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--c098d1ae425c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c098d1ae425c--------------------------------)
    ·11 min read·Nov 14, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c098d1ae425c--------------------------------)
    ·阅读时间 11分钟·2024年11月14日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef?source=post_page-----c098d1ae425c--------------------------------)
    [## Decision Tree Regressor, Explained: A Visual Guide with Code Examples'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef?source=post_page-----c098d1ae425c--------------------------------)
    [## 决策树回归器详解：带有代码示例的视觉指南'
- en: Trimming branches smartly with Cost-Complexity Pruning
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过成本复杂度修剪智能地修剪分支
- en: towardsdatascience.com](/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef?source=post_page-----c098d1ae425c--------------------------------)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef?source=post_page-----c098d1ae425c--------------------------------)
- en: Of course, in machine learning, we want our predictions spot on. We started
    with [simple decision trees](https://medium.com/towards-data-science/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e)
    — they worked okay. Then came [Random Forests](https://medium.com/towards-data-science/random-forest-explained-a-visual-guide-with-code-examples-9f736a6e1b3c)
    and [AdaBoost](https://medium.com/towards-data-science/adaboost-classifier-explained-a-visual-guide-with-code-examples-fc0f25326d7b),
    which did better. But Gradient Boosting? That was a game-changer, making predictions
    way more accurate.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在机器学习中，我们希望我们的预测非常准确。我们从[简单的决策树](https://medium.com/towards-data-science/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e)开始——它们还不错。然后出现了[随机森林](https://medium.com/towards-data-science/random-forest-explained-a-visual-guide-with-code-examples-9f736a6e1b3c)和[AdaBoost](https://medium.com/towards-data-science/adaboost-classifier-explained-a-visual-guide-with-code-examples-fc0f25326d7b)，效果更好。但梯度提升呢？那简直是一个游戏规则的改变，使得预测变得更加准确。
- en: 'They said, “What makes Gradient Boosting work so well is actually simple: it
    builds models one after another, where each new model focuses on fixing the mistakes
    of all previous models combined. This way of fixing errors step by step is what
    makes it special.” I thought it’s really gonna be that simple but *every time*
    I look up Gradient Boosting, trying to understand how it works, I see the same
    thing: rows and rows of complex math formulas and ugly charts that somehow drive
    me insane. Just try it.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 他们说：“使得梯度提升如此有效的原因其实很简单：它逐个构建模型，每个新模型都专注于修正所有之前模型的错误。这样逐步修正错误的方式就是它特别之处。”我本以为这真的会这么简单，但*每次*我查阅梯度提升，试图理解它是如何工作的时，我看到的却是相同的内容：一行行复杂的数学公式和令人头疼的图表，这些东西总是让我抓狂。试试看吧。
- en: Let’s put a stop to this and break it down in a way that actually makes sense.
    We’ll visually navigate through the training steps of Gradient Boosting, focusing
    on a regression case — a simpler scenario than classification — so we can avoid
    the confusing math. Like a multi-stage rocket shedding unnecessary weight to reach
    orbit, we’ll blast away those prediction errors one residual at a time.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们终结这一点，并以一种实际有意义的方式来分解它。我们将通过梯度提升的训练步骤进行可视化导航，专注于回归案例——这是一个比分类更简单的场景——这样我们就可以避免混淆的数学计算。就像多级火箭为了达到轨道而丢弃不必要的重量一样，我们将一项一项地消除那些预测误差。
- en: '![](../Images/2d6078ab2a677250978f6a47f6460221.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d6078ab2a677250978f6a47f6460221.png)'
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 所有视觉图像：作者使用 Canva Pro 创建。优化为移动设备显示；在桌面上可能显示过大。
- en: Definition
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义
- en: Gradient Boosting is an ensemble machine learning technique that builds a series
    of decision trees, each aimed at correcting the errors of the previous ones. Unlike
    AdaBoost, which uses shallow trees, Gradient Boosting uses deeper trees as its
    weak learners. Each new tree focuses on minimizing the residual errors — the differences
    between actual and predicted values — rather than learning directly from the original
    targets.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升是一种集成机器学习技术，它构建一系列决策树，每棵树旨在纠正前一棵树的错误。与使用浅层树的 AdaBoost 不同，梯度提升使用更深的树作为其弱学习器。每棵新树的目标是最小化残差误差——实际值与预测值之间的差异——而不是直接从原始目标中学习。
- en: For regression tasks, Gradient Boosting adds trees one after another with each
    new tree is trained to reduce the remaining errors by addressing the current residual
    errors. The final prediction is made by adding up the outputs from all the trees.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归任务，梯度提升逐一添加决策树，每棵新树的训练目标是通过解决当前的残差误差来减少剩余的错误。最终的预测是通过将所有树的输出相加得到的。
- en: The model’s strength comes from its additive learning process — while each tree
    focuses on correcting the remaining errors in the ensemble, the sequential combination
    creates a powerful predictor that **progressively reduces the overall prediction
    error** by focusing on the parts of the problem where the model still struggles.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的优势来自于其加法学习过程——每棵树专注于纠正集成中的剩余错误，而顺序组合的方式使得它成为一个强大的预测器，通过专注于模型仍然难以处理的问题部分，**逐步减少整体预测误差**。
- en: '![](../Images/3ac141fa57a3f0707e680165024c850a.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ac141fa57a3f0707e680165024c850a.png)'
- en: Gradient Boosting is part of the boosting family of algorithms because it builds
    trees sequentially, with each new tree trying to correct the errors of its predecessors.
    However, unlike other boosting methods, Gradient Boosting approaches the problem
    from an optimization perspective.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升是提升家族算法的一部分，因为它是逐一构建树的，每棵新树尝试纠正前一棵树的错误。然而，与其他提升方法不同，梯度提升从优化的角度来解决问题。
- en: Dataset Used
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用的数据集
- en: Throughout this article, we’ll focus on the classic golf dataset as an example
    for regression. While Gradient Boosting can handle both regression and classification
    tasks effectively, we’ll concentrate on the simpler task which in this case is
    the regression — predicting the number of players who will show up to play golf
    based on weather conditions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将以经典的高尔夫数据集作为回归的例子。尽管梯度提升可以有效地处理回归和分类任务，但我们将专注于更简单的回归任务——根据天气条件预测来打高尔夫的玩家人数。
- en: '![](../Images/dfe988f34df90e6d6bc95d67f04271db.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dfe988f34df90e6d6bc95d67f04271db.png)'
- en: 'Columns: ‘Overcast (one-hot-encoded into 3 columns)’, ’Temperature’ (in Fahrenheit),
    ‘Humidity’ (in %), ‘Windy’ (Yes/No) and ‘Number of Players’ (target feature)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 列：‘阴天（通过一热编码转换为 3 列）’，‘温度’（华氏度），‘湿度’（百分比），‘有风’（是/否）和‘玩家人数’（目标特征）
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Main Mechanism
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主要机制
- en: 'Here’s how Gradient Boosting works:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是梯度提升的工作原理：
- en: '**Initialize Model:** Start with a simple prediction, typically the mean of
    target values.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**初始化模型：** 从一个简单的预测开始，通常是目标值的平均值。'
- en: '**Iterative Learning:** For a set number of iterations, compute the residuals,
    train a decision tree to predict these residuals, and add the new tree’s predictions
    (scaled by the learning rate) to the running total.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**迭代学习：** 在设定的迭代次数内，计算残差，训练一棵决策树来预测这些残差，并将新树的预测结果（按学习率缩放）添加到运行总和中。'
- en: '**Build Trees on Residuals:** Each new tree focuses on the remaining errors
    from all previous iterations.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**在残差上构建决策树：** 每棵新树专注于所有前期迭代中的剩余误差。'
- en: '**Final Prediction:** Sum up all tree contributions (scaled by the learning
    rate) and the initial prediction.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**最终预测：** 汇总所有树的贡献（按学习率缩放）和初始预测。'
- en: '![](../Images/5aebdd14999e70e5924a74ce7ef351b5.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5aebdd14999e70e5924a74ce7ef351b5.png)'
- en: A Gradient Boosting Regressor starts with an average prediction and improves
    it through multiple trees, each one fixing the previous trees’ mistakes in small
    steps, until reaching the final prediction.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升回归模型从平均预测开始，通过多棵树进行改进，每棵树都在小步修正前一棵树的错误，直到达到最终预测。
- en: Training Steps
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练步骤
- en: 'We’ll follow the standard gradient boosting approach:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遵循标准的梯度提升方法：
- en: '1.0\. Set Model Parameters:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 1.0\. 设置模型参数：
- en: 'Before building any trees, we need set the core parameters that control the
    learning process:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建任何树之前，我们需要设置控制学习过程的核心参数：
- en: · the number of trees (typically 100, but we’ll choose 50) to build sequentially,
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: · 树的数量（通常为100，但我们选择50）按顺序构建，
- en: · the learning rate (typically 0.1), and
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: · 学习率（通常为0.1），以及
- en: · the maximum depth of each tree (typically 3)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: · 每棵树的最大深度（通常为3）
- en: '![](../Images/11972f0214d401ffdf105a994791fc6b.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11972f0214d401ffdf105a994791fc6b.png)'
- en: 'A tree diagram showing our key settings: each tree will have 3 levels, and
    we’ll create 50 of them while moving forward in small steps of 0.1.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一棵树的图示，展示了我们的关键设置：每棵树将有3个层级，我们将创建50棵树，并在每次迭代中以0.1的小步前进。
- en: For the First Tree
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对于第一棵树
- en: 2.0 Make an initial prediction for the label. This is typically the mean (just
    like [a dummy prediction](/dummy-regressor-explained-a-visual-guide-with-code-examples-for-beginners-4007c3d16629).)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 2.0 对标签进行初始预测。通常这是均值（就像是[a dummy prediction](/dummy-regressor-explained-a-visual-guide-with-code-examples-for-beginners-4007c3d16629)一样。）
- en: '![](../Images/0583a69027986dee2fda863439108614.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0583a69027986dee2fda863439108614.png)'
- en: To start our predictions, we use the average value (37.43) of all our training
    data as the first guess for every case.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始我们的预测，我们使用所有训练数据的平均值（37.43）作为每个案例的第一次猜测。
- en: '2.1\. Calculate temporary residual (or pseudo-residuals):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 2.1\. 计算临时残差（或伪残差）：
- en: residual = actual value — predicted value
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 残差 = 实际值 — 预测值
- en: '![](../Images/5f135dbb9d7034726d771919188d31f6.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5f135dbb9d7034726d771919188d31f6.png)'
- en: Calculating the initial residuals by subtracting the mean prediction (37.43)
    from each target value in our training set.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 通过从每个目标值中减去均值预测（37.43）来计算初始残差。
- en: 2.2\. Build a decision tree to **predict these residuals.** The tree building
    steps are exactly the same as in the [regression tree](https://medium.com/towards-data-science/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 2.2\. 构建决策树以**预测这些残差**。树的构建步骤与[回归树](https://medium.com/towards-data-science/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef)完全相同。
- en: '![](../Images/77364ffaaeca757af50cfa5a758e12b1.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77364ffaaeca757af50cfa5a758e12b1.png)'
- en: The first decision tree begins its training by searching for patterns in our
    features that can best predict the calculated residuals from our initial mean
    prediction.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 第一棵决策树开始训练时，通过寻找特征中的模式，来预测我们初步均值预测的计算残差。
- en: a. Calculate initial MSE (Mean Squared Error) for the root node
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: a. 计算根节点的初始均方误差（MSE）
- en: '![](../Images/44736650f521e040f1c4b17edd948edc.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44736650f521e040f1c4b17edd948edc.png)'
- en: Just like in regular regression trees, we calculate the Mean Squared Error (MSE),
    but this time we’re measuring the spread of residuals (around zero) instead of
    actual values (around their mean).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 就像常规回归树一样，我们计算均方误差（MSE），但这次我们测量的是残差的分布（围绕零），而不是实际值的分布（围绕它们的均值）。
- en: 'b. For each feature:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: b. 对每个特征：
- en: · Sort data by feature values
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: · 按特征值对数据进行排序
- en: '![](../Images/b2f725b3c389f832749b26326e214e85.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2f725b3c389f832749b26326e214e85.png)'
- en: For each feature in our dataset, we sort its values and find potential split
    points between them, just as we would in a standard decision tree, to determine
    the best way to divide our residuals.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据集中的每个特征，我们对其值进行排序并找到潜在的分裂点，正如我们在标准决策树中所做的那样，来确定最好的方式来划分我们的残差。
- en: '· For each possible split point:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: · 对每个可能的分裂点：
- en: ·· Split samples into left and right groups
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ·· 将样本分为左组和右组
- en: ·· Calculate MSE for both groups
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ·· 计算两个组的均方误差（MSE）
- en: ·· Calculate MSE reduction for this split
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ·· 计算这个分裂的均方误差（MSE）减少量
- en: '![](../Images/7082a68606950c5a393205a4e8694ad0.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7082a68606950c5a393205a4e8694ad0.png)'
- en: Similar to a regular regression tree, we evaluate each split by calculating
    the weighted MSE of both groups, but here we’re measuring how well the split groups
    similar residuals rather than similar target values.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于常规的回归树，我们通过计算两组的加权均方误差（MSE）来评估每次划分，但这里我们衡量的是划分后的组如何聚集相似的残差，而不是相似的目标值。
- en: c. Pick the split that gives the largest MSE reduction
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: c. 选择能够带来最大MSE降低的分裂
- en: '![](../Images/26c4db7c3842cc6336ef543ffe94653d.png)![](../Images/164adc4d983357b7bd928394075c5cbb.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26c4db7c3842cc6336ef543ffe94653d.png)![](../Images/164adc4d983357b7bd928394075c5cbb.png)'
- en: The tree makes its first split using the “rain” feature at value 0.5, dividing
    samples into two groups based on their residuals — this first decision will be
    refined by additional splits at deeper levels.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 树通过使用“rain”特征（值为0.5）进行第一次分裂，基于残差将样本分成两组——这个第一次决策将在更深层次的进一步分裂中得到精炼。
- en: d. Continue splitting until reaching maximum depth or minimum samples per leaf.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: d. 继续分裂，直到达到最大深度或每个叶子的最小样本数。
- en: '![](../Images/dfb7d12a8ebb84f85be346d2ab27e286.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dfb7d12a8ebb84f85be346d2ab27e286.png)'
- en: After three levels of splitting on different features, our first tree has created
    eight distinct groups, each with its own prediction for the residuals.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 经历了三层基于不同特征的分裂后，我们的第一棵树创建了八个不同的组，每个组都有自己的残差预测值。
- en: 2.3\. Calculate Leaf Values
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 2.3\. 计算叶子节点值
- en: For each leaf, find the **mean of residuals**.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个叶子节点，计算**残差的均值**。
- en: '![](../Images/fbb6b98089b4529f42201e77bbdeca1d.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fbb6b98089b4529f42201e77bbdeca1d.png)'
- en: Each leaf in our first tree contains an average of the residuals in that group
    — these values will be used to adjust and improve our initial mean prediction
    of 37.43.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 第一棵树的每个叶子节点包含该组残差的平均值——这些值将用于调整和改善我们最初的37.43的预测。
- en: 2.4\. Update Predictions
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 2.4\. 更新预测
- en: · For each data point in the **training dataset**, determine which leaf it falls
    into based on the new tree.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: · 对于**训练数据集**中的每个数据点，基于新树确定它属于哪个叶子节点。
- en: '![](../Images/df18786126f2894b4b244d4fde2b9192.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df18786126f2894b4b244d4fde2b9192.png)'
- en: Running our training data through the first tree, each sample follows its own
    path based on weather features to get its predicted residual value, which will
    help correct our initial prediction.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们的训练数据通过第一棵树运行时，每个样本根据天气特征沿着自己的路径获取预测残差值，这将帮助修正我们最初的预测。
- en: · Multiply the new tree’s predictions by the learning rate and add these scaled
    predictions to the current model’s predictions. This will be the updated prediction.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: · 将新树的预测结果乘以学习率，然后将这些缩放后的预测值加到当前模型的预测结果中。这将是更新后的预测。
- en: '![](../Images/76a4aa18c7ed15a837c54901efb09dcc.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/76a4aa18c7ed15a837c54901efb09dcc.png)'
- en: 'Our model updates its predictions by taking small steps: it adds just 10% (our
    learning rate of 0.1) of each predicted residual to our initial prediction of
    37.43, creating slightly improved predictions.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型通过采取小步进来更新预测：它只将每个预测残差的10%（学习率为0.1）加到我们最初的37.43预测值上，从而得到稍微改进的预测。
- en: For the Second Tree
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对于第二棵树
- en: 2.1\. Calculate new residuals based on current model
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 2.1\. 基于当前模型计算新的残差
- en: a. Compute the difference between the target and current predictions.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: a. 计算目标预测值与当前预测值之间的差异。
- en: These residuals will be a bit different from the first iteration.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这些残差与第一次迭代的残差会略有不同。
- en: '![](../Images/2843e92a51f3b6bd97696bf603c17128.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2843e92a51f3b6bd97696bf603c17128.png)'
- en: After updating our predictions with the first tree, we calculate new residuals
    — notice how they’re slightly smaller than the original ones, showing our predictions
    are gradually improving.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 更新了第一棵树的预测后，我们计算新的残差——注意到它们比原来的残差稍微小一些，显示我们的预测逐渐得到了改善。
- en: 2.2\. Build a new tree to predict these residuals. Same process as first tree,
    but targeting new residuals.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 2.2\. 构建一棵新树来预测这些残差。过程与第一棵树相同，但目标是新的残差。
- en: '![](../Images/bd50d352c28d235ec429312a6fc54dd7.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd50d352c28d235ec429312a6fc54dd7.png)'
- en: Starting our second tree to predict the new, smaller residuals — we’ll use the
    same tree-building process as before, but now we’re trying to catch the errors
    our first tree missed
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 启动我们的第二棵树来预测新的、更小的残差——我们将使用与之前相同的树构建过程，但这次我们试图捕捉第一棵树遗漏的错误。
- en: 2.3\. Calculate the mean residuals for each leaf
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 2.3\. 计算每个叶子节点的均值残差
- en: '![](../Images/b5226feb595f93476c22f6f3acfb4488.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5226feb595f93476c22f6f3acfb4488.png)'
- en: The second tree follows an identical structure to our first tree with the same
    weather features and split points, but with smaller values in its leaves — showing
    we’re fine-tuning the remaining errors.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 第二棵树与第一棵树的结构相同，使用相同的天气特征和分裂点，但其叶节点的值较小——这表明我们正在微调剩余的误差。
- en: 2.4\. Update model predictions
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 2.4\. 更新模型预测
- en: · Multiply the new tree’s predictions by the learning rate.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: · 将新树的预测乘以学习率。
- en: · Add the new scaled tree predictions to the running total.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: · 将新缩放过的树预测加到当前总和中。
- en: '![](../Images/269aeb6fbb665e5f8d830fff8afb22f0.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/269aeb6fbb665e5f8d830fff8afb22f0.png)'
- en: After running our data through the second tree, we again take small steps with
    our 0.1 learning rate to update predictions, and calculate new residuals that
    are even smaller than before — our model is gradually learning the patterns.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据通过第二棵树后，我们再次以0.1的学习率做出小步调整以更新预测，并计算出比之前更小的残差——我们的模型正在逐渐学习模式。
- en: For the Third Tree onwards
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从第三棵树开始
- en: Repeat Steps 2.1–2.3 for remaining iterations. Note that each tree sees different
    residuals.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对剩余的迭代重复步骤2.1–2.3。注意，每棵树看到的残差不同。
- en: · Trees progressively focus on harder-to-predict patterns
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: · 决策树逐渐专注于更难预测的模式
- en: · Learning rate prevents overfitting by limiting each tree’s contribution
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: · 学习率通过限制每棵树的贡献来防止过拟合
- en: '![](../Images/c3bec57f3cfc4d79ab75c562246a8250.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3bec57f3cfc4d79ab75c562246a8250.png)'
- en: As we build more trees, notice how the split points slowly shift and the residual
    values in the leaves get smaller — by tree 50, we’re making tiny adjustments using
    different combinations of features compared to our first trees.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们构建更多的树，注意分裂点如何逐渐变化，叶节点中的残差值变得更小——到第50棵树时，我们通过不同的特征组合进行微调，调整的幅度比最初的树要小。
- en: '[PRE1]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/8a98aab772c6bdcf801764e343cb4c17.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a98aab772c6bdcf801764e343cb4c17.png)'
- en: 'Visualization from scikit-learn shows how our gradient boosting trees evolve:
    from Tree 1 making large splits with big prediction values, to Tree 50 making
    refined splits with tiny adjustments — each tree focuses on correcting the remaining
    errors from previous trees.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 来自scikit-learn的可视化展示了我们的梯度提升树如何演变：从树1进行大范围的分裂并给出大预测值，到树50进行精细的分裂并做出微小的调整——每棵树都专注于修正前面树所产生的剩余误差。
- en: Testing Step
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试步骤
- en: 'For predicting:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 预测时：
- en: a. Start with the initial prediction (the average number of players)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: a. 从初始预测开始（玩家的平均数量）
- en: b. Run the input through each tree to get its predicted adjustment
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: b. 将输入数据传递给每棵树以获得其预测的调整值
- en: c. Scale each tree’s prediction by the learning rate.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: c. 按照学习率缩放每棵树的预测值。
- en: d. Add all these adjustments to the initial prediction
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: d. 将所有这些调整添加到初始预测中
- en: e. The sum directly gives us the predicted number of players
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: e. 这些和直接给出我们预测的玩家数量
- en: '![](../Images/4b7ce876ba3cdfad96da7c7e0415989f.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b7ce876ba3cdfad96da7c7e0415989f.png)'
- en: When predicting on unseen data, each tree contributes its small prediction,
    starting from 5.57 in Tree 1 down to 0.008 in Tree 50 — all these predictions
    are scaled by our 0.1 learning rate and added to our base prediction of 37.43
    to get the final answer.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在对未见数据进行预测时，每棵树都会贡献一个小的预测值，从树1的5.57开始，到树50的0.008——所有这些预测都被我们的0.1学习率进行缩放，并加到我们的基础预测值37.43上，得到最终的答案。
- en: Evaluation Step
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估步骤
- en: After building all the trees, we can evaluate the test set.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 构建所有树后，我们可以评估测试集。
- en: '![](../Images/32381bf34c6c5780246b6047af2863bd.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/32381bf34c6c5780246b6047af2863bd.png)'
- en: Our gradient boosting model achieves an RMSE of 4.785, quite an improvement
    over [a single regression tree’s 5.27](https://medium.com/towards-data-science/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef)
    — showing how combining many small corrections leads to better predictions than
    one complex tree!
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的梯度提升模型达到了4.785的RMSE，相较于[a棵回归树的5.27](https://medium.com/towards-data-science/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef)有了显著的提升——这表明将多个小的调整组合起来，比单棵复杂的树更能做出准确的预测！
- en: '[PRE2]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Key Parameters
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键参数
- en: 'Here are the key parameters for Gradient Boosting, particularly in `scikit-learn`:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是梯度提升中的关键参数，特别是在`scikit-learn`中：
- en: '`max_depth`: The depth of trees used to model residuals. Unlike AdaBoost which
    uses stumps, Gradient Boosting works better with deeper trees (typically 3-8 levels).
    Deeper trees capture more complex patterns but risk overfitting.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_depth`：用于建模残差的树的深度。与使用树桩的AdaBoost不同，梯度提升在深层树（通常为3-8层）上效果更好。深层树能够捕捉更复杂的模式，但也有过拟合的风险。'
- en: '`n_estimators`: The number of trees to be used (typically 100-1000). More trees
    usually improve performance when paired with a small learning rate.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_estimators`：要使用的树的数量（通常为 100-1000）。当与较小的学习率配对时，更多的树通常能提高性能。'
- en: '`learning_rate`: Also called "shrinkage", this scales each tree''s contribution
    (typically 0.01-0.1). Smaller values require more trees but often give better
    results by making the learning process more fine-grained.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`learning_rate`：也称为“收缩”，用于缩放每棵树的贡献（通常为 0.01-0.1）。较小的值需要更多的树，但通过使学习过程更精细化，通常能获得更好的结果。'
- en: '`subsample`: The fraction of samples used to train each tree (typically 0.5-0.8).
    This optional feature adds randomness that can improve robustness and reduce overfitting.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`subsample`：用于训练每棵树的样本比例（通常为 0.5-0.8）。这个可选特性增加了随机性，可以提高鲁棒性并减少过拟合。'
- en: 'These parameters work together: a small learning rate needs more trees, while
    deeper trees might need a smaller learning rate to avoid overfitting.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数是相互配合工作的：较小的学习率需要更多的树，而较深的树可能需要较小的学习率以避免过拟合。
- en: Key differences from AdaBoost
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与 AdaBoost 的关键区别
- en: 'Both AdaBoost and Gradient Boosting are boosting algorithms, but the way they
    learn from their mistakes are different. Here are the key differences:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost 和 Gradient Boosting 都是提升算法，但它们从错误中学习的方式不同。以下是它们的关键区别：
- en: '`max_depth` is typically higher (3-8) in Gradient Boosting, while AdaBoost
    prefers stumps.'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`max_depth` 通常在 Gradient Boosting 中较高（3-8），而 AdaBoost 更倾向于使用树桩。'
- en: No `sample_weight` updates because Gradient Boosting uses residuals instead
    of sample weighting.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 没有 `sample_weight` 更新，因为 Gradient Boosting 使用残差而不是样本加权。
- en: The `learning_rate` is typically much smaller (0.01-0.1) compared to AdaBoost's
    larger values (0.1-1.0).
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`learning_rate` 通常比 AdaBoost 的较大值（0.1-1.0）小得多（0.01-0.1）。'
- en: Initial prediction starts from the mean while AdaBoost starts from zero.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始预测从均值开始，而 AdaBoost 从零开始。
- en: Trees are combined through simple addition rather than weighted voting, making
    each tree’s contribution more straightforward.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 树是通过简单的加法而不是加权投票来组合的，这使得每棵树的贡献更加直观。
- en: Optional `subsample` parameter adds randomness, a feature not present in standard
    AdaBoost.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可选的 `subsample` 参数增加了随机性，这是标准 AdaBoost 所没有的特性。
- en: Pros & Cons
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优点与缺点
- en: 'Pros:'
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优点：
- en: '**Step-by-Step Error Fixing:** In Gradient Boosting, each new tree focuses
    on correcting the mistakes made by the previous ones. This makes the model better
    at improving its predictions in areas where it was previously wrong.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逐步错误修正：** 在 Gradient Boosting 中，每棵新树专注于修正前一棵树的错误。这使得模型在之前错误的区域更好地改进预测。'
- en: '**Flexible Error Measures:** Unlike AdaBoost, Gradient Boosting can optimize
    different types of error measurements (like mean absolute error, mean squared
    error, or others). This makes it adaptable to various kinds of problems.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活的误差度量：** 与 AdaBoost 不同，Gradient Boosting 可以优化不同类型的误差度量（如平均绝对误差、均方误差等）。这使得它可以适应各种问题。'
- en: '**High Accuracy:** By using more detailed trees and carefully controlling the
    learning rate, Gradient Boosting often provides more accurate results than other
    algorithms, especially for well-structured data.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高准确度：** 通过使用更详细的树并仔细控制学习率，Gradient Boosting 往往能提供比其他算法更准确的结果，尤其是对于结构良好的数据。'
- en: 'Cons:'
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺点：
- en: '**Risk of Overfitting:** The use of deeper trees and the sequential building
    process can cause the model to fit the training data too closely, which may reduce
    its performance on new data. This requires careful tuning of tree depth, learning
    rate, and the number of trees.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过拟合的风险：** 使用更深的树和顺序构建过程可能导致模型过度拟合训练数据，从而降低在新数据上的表现。这需要仔细调整树的深度、学习率和树的数量。'
- en: '**Slow Training Process:** Like AdaBoost, trees must be built one after another,
    making it slower to train compared to algorithms that can build trees in parallel,
    like Random Forest. Each tree relies on the errors of the previous ones.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练过程缓慢：** 和 AdaBoost 一样，树必须一个接一个地构建，因此相比于可以并行构建树的算法（如随机森林），训练速度较慢。每棵树都依赖于前一棵树的错误。'
- en: '**High Memory Use:** The need for deeper and more numerous trees means Gradient
    Boosting can consume more memory than simpler boosting methods such as AdaBoost.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高内存使用：** 由于需要更深和更多的树，Gradient Boosting 的内存消耗可能比像 AdaBoost 这样的简单提升方法更高。'
- en: '**Sensitive to Settings:** The effectiveness of Gradient Boosting heavily depends
    on finding the right combination of learning rate, tree depth, and number of trees,
    which can be more complex and time-consuming than tuning simpler algorithms.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对设置敏感：** 梯度提升的有效性在很大程度上取决于找到合适的学习率、树的深度和树的数量的组合，这可能比调优简单算法更复杂且耗时。'
- en: Final Remarks
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结语
- en: Gradient Boosting is a major improvement in boosting algorithms. This success
    has led to popular versions like XGBoost and LightGBM, which are widely used in
    machine learning competitions and real-world applications.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升（Gradient Boosting）是提升算法的一项重要改进。这一成功催生了像XGBoost和LightGBM这样的流行版本，它们在机器学习竞赛和实际应用中得到了广泛使用。
- en: While Gradient Boosting requires more careful tuning than simpler algorithms
    — especially when adjusting the depth of decision trees, the learning rate, and
    the number of trees — it is very flexible and powerful. This makes it a top choice
    for problems with structured data.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管梯度提升比简单算法需要更精细的调优——尤其是在调整决策树深度、学习率和树的数量时——它非常灵活且强大。这使得它成为结构化数据问题的首选方法。
- en: Gradient Boosting can handle complex relationships that simpler methods like
    AdaBoost might miss. Its continued popularity and ongoing improvements show that
    the approach of using gradients and building models step-by-step remains highly
    important in modern machine learning.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升能够处理简单方法（如AdaBoost）可能忽视的复杂关系。其持续的流行和不断的改进表明，使用梯度并逐步构建模型的方法在现代机器学习中依然极为重要。
- en: 🌟 Gradient Boosting Regressor Code Summarized
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🌟 梯度提升回归器代码总结
- en: '[PRE3]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Further Reading
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: For a detailed explanation of the [GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)
    and its implementation in scikit-learn, readers can refer to the official documentation,
    which provides comprehensive information on its usage and parameters.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 对于[梯度提升回归器（GradientBoostingRegressor）](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)及其在scikit-learn中的实现的详细解释，读者可以参考官方文档，该文档提供了有关其使用和参数的全面信息。
- en: Technical Environment
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术环境
- en: This article uses Python 3.7 and scikit-learn 1.6\. While the concepts discussed
    are generally applicable, specific code implementations may vary slightly with
    different versions.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 本文使用的是Python 3.7和scikit-learn 1.6版本。虽然所讨论的概念具有普遍适用性，但具体的代码实现可能会因版本不同而略有差异。
- en: About the Illustrations
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于插图
- en: Unless otherwise noted, all images are created by the author, incorporating
    licensed design elements from Canva Pro.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图片均由作者创作，包含来自Canva Pro的授权设计元素。
- en: '𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝙀𝙣𝙨𝙚𝙢𝙗𝙡𝙚 𝙇𝙚𝙖𝙧𝙣𝙞𝙣𝙜 𝙝𝙚𝙧𝙚:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝙀𝙣𝙨𝙚𝙢𝙗𝙡𝙚 𝙇𝙚𝙖𝙧𝙣𝙞𝙣𝙜 𝙝𝙚𝙧𝙚:'
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----c098d1ae425c--------------------------------)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----c098d1ae425c--------------------------------)'
- en: Ensemble Learning
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成学习
- en: '[View list](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----c098d1ae425c--------------------------------)4
    stories![](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----c098d1ae425c--------------------------------)4篇故事![](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)'
- en: '𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:'
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----c098d1ae425c--------------------------------)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----c098d1ae425c--------------------------------)'
- en: Regression Algorithms
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归算法
- en: '[View list](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----c098d1ae425c--------------------------------)5
    stories![A cartoon doll with pigtails and a pink hat. This “dummy” doll, with
    its basic design and heart-adorned shirt, visually represents the concept of a
    dummy regressor in machine. Just as this toy-like figure is a simplified, static
    representation of a person, a dummy regressor is a basic models serve as baselines
    for more sophisticated analyses.](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----c098d1ae425c--------------------------------)5个故事![一只戴着粉色帽子、扎着双马尾的卡通娃娃。这个“假人”娃娃，凭借其简化的设计和心形装饰的衣服，形象地表现了机器学习中的“假回归器”概念。就像这个玩具般的角色是一个简化的、静态的人物表现一样，假回归器也是一个基础模型，作为更复杂分析的基准。](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)'
